Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.789670705795288 = 1.9522897005081177 + 0.1 * 8.373809814453125
Epoch 0, val loss: 1.9590022563934326
Epoch 10, training loss: 2.779528856277466 = 1.9421718120574951 + 0.1 * 8.373570442199707
Epoch 10, val loss: 1.948914647102356
Epoch 20, training loss: 2.767009735107422 = 1.9297906160354614 + 0.1 * 8.3721923828125
Epoch 20, val loss: 1.936554193496704
Epoch 30, training loss: 2.7487308979034424 = 1.9124865531921387 + 0.1 * 8.362442970275879
Epoch 30, val loss: 1.9192522764205933
Epoch 40, training loss: 2.717195749282837 = 1.8868200778961182 + 0.1 * 8.303755760192871
Epoch 40, val loss: 1.8939241170883179
Epoch 50, training loss: 2.645357608795166 = 1.8514822721481323 + 0.1 * 7.9387526512146
Epoch 50, val loss: 1.8604137897491455
Epoch 60, training loss: 2.5463850498199463 = 1.8125017881393433 + 0.1 * 7.338832855224609
Epoch 60, val loss: 1.8260188102722168
Epoch 70, training loss: 2.4781181812286377 = 1.7732841968536377 + 0.1 * 7.048338890075684
Epoch 70, val loss: 1.7920987606048584
Epoch 80, training loss: 2.4198553562164307 = 1.7321257591247559 + 0.1 * 6.877296447753906
Epoch 80, val loss: 1.757262110710144
Epoch 90, training loss: 2.359933376312256 = 1.6828383207321167 + 0.1 * 6.770951747894287
Epoch 90, val loss: 1.714431881904602
Epoch 100, training loss: 2.287673234939575 = 1.6162315607070923 + 0.1 * 6.71441650390625
Epoch 100, val loss: 1.6578377485275269
Epoch 110, training loss: 2.197192907333374 = 1.5290701389312744 + 0.1 * 6.68122673034668
Epoch 110, val loss: 1.5858982801437378
Epoch 120, training loss: 2.0888805389404297 = 1.4233263731002808 + 0.1 * 6.655541896820068
Epoch 120, val loss: 1.4989131689071655
Epoch 130, training loss: 1.9707987308502197 = 1.3080741167068481 + 0.1 * 6.627245903015137
Epoch 130, val loss: 1.4046354293823242
Epoch 140, training loss: 1.8536843061447144 = 1.19411039352417 + 0.1 * 6.595738887786865
Epoch 140, val loss: 1.312680721282959
Epoch 150, training loss: 1.7461037635803223 = 1.088796854019165 + 0.1 * 6.573068618774414
Epoch 150, val loss: 1.2304381132125854
Epoch 160, training loss: 1.6510231494903564 = 0.997080385684967 + 0.1 * 6.539428234100342
Epoch 160, val loss: 1.1605150699615479
Epoch 170, training loss: 1.5671840906143188 = 0.9157823324203491 + 0.1 * 6.514017581939697
Epoch 170, val loss: 1.099889874458313
Epoch 180, training loss: 1.4921252727508545 = 0.8431857824325562 + 0.1 * 6.489395618438721
Epoch 180, val loss: 1.0467108488082886
Epoch 190, training loss: 1.4245364665985107 = 0.7777340412139893 + 0.1 * 6.468023300170898
Epoch 190, val loss: 0.9994167685508728
Epoch 200, training loss: 1.3661720752716064 = 0.7184756398200989 + 0.1 * 6.476964473724365
Epoch 200, val loss: 0.9575187563896179
Epoch 210, training loss: 1.310584306716919 = 0.6660726070404053 + 0.1 * 6.44511604309082
Epoch 210, val loss: 0.9221328496932983
Epoch 220, training loss: 1.260551929473877 = 0.618313193321228 + 0.1 * 6.42238712310791
Epoch 220, val loss: 0.8918743133544922
Epoch 230, training loss: 1.216808795928955 = 0.5737117528915405 + 0.1 * 6.430969715118408
Epoch 230, val loss: 0.8658323884010315
Epoch 240, training loss: 1.171933650970459 = 0.5320277214050293 + 0.1 * 6.3990583419799805
Epoch 240, val loss: 0.8434261679649353
Epoch 250, training loss: 1.130775809288025 = 0.49224624037742615 + 0.1 * 6.385295867919922
Epoch 250, val loss: 0.8235887289047241
Epoch 260, training loss: 1.093573808670044 = 0.45426416397094727 + 0.1 * 6.393095970153809
Epoch 260, val loss: 0.8062641024589539
Epoch 270, training loss: 1.055396556854248 = 0.41840529441833496 + 0.1 * 6.369912147521973
Epoch 270, val loss: 0.791544497013092
Epoch 280, training loss: 1.021378993988037 = 0.3843950033187866 + 0.1 * 6.369840621948242
Epoch 280, val loss: 0.7791053652763367
Epoch 290, training loss: 0.9877374172210693 = 0.3523852229118347 + 0.1 * 6.353521823883057
Epoch 290, val loss: 0.7690504193305969
Epoch 300, training loss: 0.9567607641220093 = 0.3222615122795105 + 0.1 * 6.344992637634277
Epoch 300, val loss: 0.7612050771713257
Epoch 310, training loss: 0.9275404214859009 = 0.2940228581428528 + 0.1 * 6.335175514221191
Epoch 310, val loss: 0.7556684613227844
Epoch 320, training loss: 0.9017254114151001 = 0.2679421603679657 + 0.1 * 6.337831974029541
Epoch 320, val loss: 0.7526922225952148
Epoch 330, training loss: 0.8767889142036438 = 0.24415045976638794 + 0.1 * 6.326384544372559
Epoch 330, val loss: 0.7522592544555664
Epoch 340, training loss: 0.8545113205909729 = 0.22248204052448273 + 0.1 * 6.320292949676514
Epoch 340, val loss: 0.7542215585708618
Epoch 350, training loss: 0.8340792655944824 = 0.202859565615654 + 0.1 * 6.312196731567383
Epoch 350, val loss: 0.7584059834480286
Epoch 360, training loss: 0.8184371590614319 = 0.18529875576496124 + 0.1 * 6.331384181976318
Epoch 360, val loss: 0.7644472122192383
Epoch 370, training loss: 0.7997321486473083 = 0.16964781284332275 + 0.1 * 6.300843238830566
Epoch 370, val loss: 0.7722014784812927
Epoch 380, training loss: 0.7853651642799377 = 0.1555868238210678 + 0.1 * 6.297783374786377
Epoch 380, val loss: 0.7814316749572754
Epoch 390, training loss: 0.7731668949127197 = 0.14294737577438354 + 0.1 * 6.302195072174072
Epoch 390, val loss: 0.791960597038269
Epoch 400, training loss: 0.7606695294380188 = 0.13159717619419098 + 0.1 * 6.2907233238220215
Epoch 400, val loss: 0.8035910129547119
Epoch 410, training loss: 0.7506723999977112 = 0.12136262655258179 + 0.1 * 6.293097496032715
Epoch 410, val loss: 0.8160710334777832
Epoch 420, training loss: 0.7406156659126282 = 0.1121310368180275 + 0.1 * 6.28484582901001
Epoch 420, val loss: 0.8293036818504333
Epoch 430, training loss: 0.7326797842979431 = 0.1037750244140625 + 0.1 * 6.289047718048096
Epoch 430, val loss: 0.8430216908454895
Epoch 440, training loss: 0.7235121130943298 = 0.09620345383882523 + 0.1 * 6.2730865478515625
Epoch 440, val loss: 0.8572359085083008
Epoch 450, training loss: 0.7169405817985535 = 0.08930325508117676 + 0.1 * 6.276372909545898
Epoch 450, val loss: 0.8718332052230835
Epoch 460, training loss: 0.7098435759544373 = 0.08301550149917603 + 0.1 * 6.268280506134033
Epoch 460, val loss: 0.8866936564445496
Epoch 470, training loss: 0.7045843601226807 = 0.07729297131299973 + 0.1 * 6.272913455963135
Epoch 470, val loss: 0.9016372561454773
Epoch 480, training loss: 0.6987829804420471 = 0.07208605855703354 + 0.1 * 6.266969203948975
Epoch 480, val loss: 0.9167279005050659
Epoch 490, training loss: 0.6927305459976196 = 0.06730189174413681 + 0.1 * 6.254286766052246
Epoch 490, val loss: 0.9319413900375366
Epoch 500, training loss: 0.6879319548606873 = 0.06289178133010864 + 0.1 * 6.250401496887207
Epoch 500, val loss: 0.9473620057106018
Epoch 510, training loss: 0.684913694858551 = 0.058829911053180695 + 0.1 * 6.260838031768799
Epoch 510, val loss: 0.9627758860588074
Epoch 520, training loss: 0.6804457306861877 = 0.05510484054684639 + 0.1 * 6.253408432006836
Epoch 520, val loss: 0.9782031178474426
Epoch 530, training loss: 0.6758722066879272 = 0.051669519394636154 + 0.1 * 6.242026329040527
Epoch 530, val loss: 0.9935559034347534
Epoch 540, training loss: 0.67228764295578 = 0.04850756749510765 + 0.1 * 6.237800598144531
Epoch 540, val loss: 1.008703589439392
Epoch 550, training loss: 0.6698682904243469 = 0.04559928923845291 + 0.1 * 6.242690086364746
Epoch 550, val loss: 1.0237979888916016
Epoch 560, training loss: 0.6670300364494324 = 0.042911168187856674 + 0.1 * 6.2411885261535645
Epoch 560, val loss: 1.0386626720428467
Epoch 570, training loss: 0.6630339622497559 = 0.040429890155792236 + 0.1 * 6.226040363311768
Epoch 570, val loss: 1.0533065795898438
Epoch 580, training loss: 0.6623215079307556 = 0.03813600912690163 + 0.1 * 6.241854667663574
Epoch 580, val loss: 1.067720890045166
Epoch 590, training loss: 0.6584402322769165 = 0.03602157160639763 + 0.1 * 6.224186420440674
Epoch 590, val loss: 1.0817943811416626
Epoch 600, training loss: 0.6561228632926941 = 0.03406517952680588 + 0.1 * 6.220576763153076
Epoch 600, val loss: 1.095610499382019
Epoch 610, training loss: 0.6547221541404724 = 0.03225146606564522 + 0.1 * 6.224706649780273
Epoch 610, val loss: 1.109156847000122
Epoch 620, training loss: 0.6524198651313782 = 0.030572647228837013 + 0.1 * 6.218472003936768
Epoch 620, val loss: 1.1224685907363892
Epoch 630, training loss: 0.6510618925094604 = 0.02901572361588478 + 0.1 * 6.220461368560791
Epoch 630, val loss: 1.1353905200958252
Epoch 640, training loss: 0.6492717862129211 = 0.027571959421038628 + 0.1 * 6.216998100280762
Epoch 640, val loss: 1.1480920314788818
Epoch 650, training loss: 0.6467699408531189 = 0.026228182017803192 + 0.1 * 6.205417633056641
Epoch 650, val loss: 1.1604682207107544
Epoch 660, training loss: 0.6457923650741577 = 0.024975601583719254 + 0.1 * 6.208167552947998
Epoch 660, val loss: 1.1726610660552979
Epoch 670, training loss: 0.644515335559845 = 0.023808352649211884 + 0.1 * 6.2070698738098145
Epoch 670, val loss: 1.184485912322998
Epoch 680, training loss: 0.6429092884063721 = 0.02272108942270279 + 0.1 * 6.201881408691406
Epoch 680, val loss: 1.196030855178833
Epoch 690, training loss: 0.6418628096580505 = 0.02170696295797825 + 0.1 * 6.201558589935303
Epoch 690, val loss: 1.207381248474121
Epoch 700, training loss: 0.6398084759712219 = 0.020758140832185745 + 0.1 * 6.190503120422363
Epoch 700, val loss: 1.2184553146362305
Epoch 710, training loss: 0.639542281627655 = 0.019869999960064888 + 0.1 * 6.196722984313965
Epoch 710, val loss: 1.2292572259902954
Epoch 720, training loss: 0.6382172703742981 = 0.019038645550608635 + 0.1 * 6.191786289215088
Epoch 720, val loss: 1.239869475364685
Epoch 730, training loss: 0.6372833251953125 = 0.01825842633843422 + 0.1 * 6.190248489379883
Epoch 730, val loss: 1.2501413822174072
Epoch 740, training loss: 0.6371065378189087 = 0.017527710646390915 + 0.1 * 6.195788383483887
Epoch 740, val loss: 1.2602952718734741
Epoch 750, training loss: 0.635529100894928 = 0.016841363161802292 + 0.1 * 6.186877250671387
Epoch 750, val loss: 1.2701228857040405
Epoch 760, training loss: 0.6336668729782104 = 0.016196520999073982 + 0.1 * 6.174703121185303
Epoch 760, val loss: 1.2798603773117065
Epoch 770, training loss: 0.6338768601417542 = 0.01558719016611576 + 0.1 * 6.182896137237549
Epoch 770, val loss: 1.2894197702407837
Epoch 780, training loss: 0.6325461268424988 = 0.015012996271252632 + 0.1 * 6.1753315925598145
Epoch 780, val loss: 1.298692226409912
Epoch 790, training loss: 0.6325780153274536 = 0.01447176281362772 + 0.1 * 6.1810622215271
Epoch 790, val loss: 1.307801604270935
Epoch 800, training loss: 0.6310614347457886 = 0.013961787335574627 + 0.1 * 6.170996189117432
Epoch 800, val loss: 1.3167169094085693
Epoch 810, training loss: 0.6303816437721252 = 0.013478484936058521 + 0.1 * 6.169031620025635
Epoch 810, val loss: 1.325518012046814
Epoch 820, training loss: 0.6311336755752563 = 0.013019949197769165 + 0.1 * 6.1811370849609375
Epoch 820, val loss: 1.3341333866119385
Epoch 830, training loss: 0.6296736001968384 = 0.012586460448801517 + 0.1 * 6.170871257781982
Epoch 830, val loss: 1.342491626739502
Epoch 840, training loss: 0.629490315914154 = 0.012175587937235832 + 0.1 * 6.173147678375244
Epoch 840, val loss: 1.3507634401321411
Epoch 850, training loss: 0.6280633807182312 = 0.011786130256950855 + 0.1 * 6.1627726554870605
Epoch 850, val loss: 1.3589046001434326
Epoch 860, training loss: 0.6284602284431458 = 0.011415595188736916 + 0.1 * 6.170446395874023
Epoch 860, val loss: 1.366873025894165
Epoch 870, training loss: 0.6273376941680908 = 0.01106355246156454 + 0.1 * 6.162741184234619
Epoch 870, val loss: 1.3746757507324219
Epoch 880, training loss: 0.6263593435287476 = 0.010728253051638603 + 0.1 * 6.15631103515625
Epoch 880, val loss: 1.3824108839035034
Epoch 890, training loss: 0.6266209483146667 = 0.010409137234091759 + 0.1 * 6.162117958068848
Epoch 890, val loss: 1.3898805379867554
Epoch 900, training loss: 0.6252079606056213 = 0.010105893015861511 + 0.1 * 6.151020526885986
Epoch 900, val loss: 1.397291898727417
Epoch 910, training loss: 0.6252911686897278 = 0.009816523641347885 + 0.1 * 6.154746055603027
Epoch 910, val loss: 1.40462327003479
Epoch 920, training loss: 0.6244939565658569 = 0.009540023282170296 + 0.1 * 6.149539470672607
Epoch 920, val loss: 1.4117474555969238
Epoch 930, training loss: 0.6242315173149109 = 0.009276405908167362 + 0.1 * 6.1495513916015625
Epoch 930, val loss: 1.418779969215393
Epoch 940, training loss: 0.6236139535903931 = 0.009023587219417095 + 0.1 * 6.14590311050415
Epoch 940, val loss: 1.425686001777649
Epoch 950, training loss: 0.6243281364440918 = 0.0087819779291749 + 0.1 * 6.15546178817749
Epoch 950, val loss: 1.43246328830719
Epoch 960, training loss: 0.623268187046051 = 0.008551015518605709 + 0.1 * 6.147171974182129
Epoch 960, val loss: 1.4390223026275635
Epoch 970, training loss: 0.6226489543914795 = 0.00833063293248415 + 0.1 * 6.143182754516602
Epoch 970, val loss: 1.4456311464309692
Epoch 980, training loss: 0.6227996349334717 = 0.008119091391563416 + 0.1 * 6.146805286407471
Epoch 980, val loss: 1.4521211385726929
Epoch 990, training loss: 0.621330976486206 = 0.007915819995105267 + 0.1 * 6.134151935577393
Epoch 990, val loss: 1.4583567380905151
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.3579
Flip ASR: 0.2489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7936344146728516 = 1.9562513828277588 + 0.1 * 8.373830795288086
Epoch 0, val loss: 1.962228775024414
Epoch 10, training loss: 2.7832603454589844 = 1.9458962678909302 + 0.1 * 8.373641014099121
Epoch 10, val loss: 1.9518380165100098
Epoch 20, training loss: 2.7703309059143066 = 1.9330809116363525 + 0.1 * 8.3725004196167
Epoch 20, val loss: 1.9384765625
Epoch 30, training loss: 2.751157760620117 = 1.9147658348083496 + 0.1 * 8.363919258117676
Epoch 30, val loss: 1.9190638065338135
Epoch 40, training loss: 2.718719005584717 = 1.8873780965805054 + 0.1 * 8.313407897949219
Epoch 40, val loss: 1.890297770500183
Epoch 50, training loss: 2.652672290802002 = 1.8500317335128784 + 0.1 * 8.02640438079834
Epoch 50, val loss: 1.8528069257736206
Epoch 60, training loss: 2.563380718231201 = 1.8102117776870728 + 0.1 * 7.53169059753418
Epoch 60, val loss: 1.8155558109283447
Epoch 70, training loss: 2.4831671714782715 = 1.774133563041687 + 0.1 * 7.090336322784424
Epoch 70, val loss: 1.7835773229599
Epoch 80, training loss: 2.421884775161743 = 1.7413522005081177 + 0.1 * 6.805325031280518
Epoch 80, val loss: 1.7545262575149536
Epoch 90, training loss: 2.368898391723633 = 1.6995623111724854 + 0.1 * 6.693361759185791
Epoch 90, val loss: 1.7164651155471802
Epoch 100, training loss: 2.3056204319000244 = 1.6424168348312378 + 0.1 * 6.632036209106445
Epoch 100, val loss: 1.6669849157333374
Epoch 110, training loss: 2.227491855621338 = 1.5678528547286987 + 0.1 * 6.5963897705078125
Epoch 110, val loss: 1.6054631471633911
Epoch 120, training loss: 2.1359128952026367 = 1.4785951375961304 + 0.1 * 6.573177814483643
Epoch 120, val loss: 1.533441185951233
Epoch 130, training loss: 2.03997540473938 = 1.3839571475982666 + 0.1 * 6.560182571411133
Epoch 130, val loss: 1.4576835632324219
Epoch 140, training loss: 1.9447836875915527 = 1.2897199392318726 + 0.1 * 6.550637245178223
Epoch 140, val loss: 1.3831700086593628
Epoch 150, training loss: 1.8517048358917236 = 1.1977657079696655 + 0.1 * 6.53939151763916
Epoch 150, val loss: 1.3127045631408691
Epoch 160, training loss: 1.7635059356689453 = 1.1106511354446411 + 0.1 * 6.528547286987305
Epoch 160, val loss: 1.2467423677444458
Epoch 170, training loss: 1.6807093620300293 = 1.0290203094482422 + 0.1 * 6.516889572143555
Epoch 170, val loss: 1.184951663017273
Epoch 180, training loss: 1.6035670042037964 = 0.9528849124908447 + 0.1 * 6.5068206787109375
Epoch 180, val loss: 1.1279027462005615
Epoch 190, training loss: 1.5321016311645508 = 0.8825948238372803 + 0.1 * 6.495067596435547
Epoch 190, val loss: 1.0755255222320557
Epoch 200, training loss: 1.4648246765136719 = 0.8165421485900879 + 0.1 * 6.482824325561523
Epoch 200, val loss: 1.0262393951416016
Epoch 210, training loss: 1.4021143913269043 = 0.7534257769584656 + 0.1 * 6.486886024475098
Epoch 210, val loss: 0.9790910482406616
Epoch 220, training loss: 1.3409817218780518 = 0.6946702003479004 + 0.1 * 6.463114261627197
Epoch 220, val loss: 0.9353780746459961
Epoch 230, training loss: 1.2848814725875854 = 0.6394668817520142 + 0.1 * 6.454145908355713
Epoch 230, val loss: 0.8943835496902466
Epoch 240, training loss: 1.232013463973999 = 0.5875742435455322 + 0.1 * 6.444391250610352
Epoch 240, val loss: 0.8565840721130371
Epoch 250, training loss: 1.1824097633361816 = 0.5388481020927429 + 0.1 * 6.435616970062256
Epoch 250, val loss: 0.8220179080963135
Epoch 260, training loss: 1.1359319686889648 = 0.4930781126022339 + 0.1 * 6.428538799285889
Epoch 260, val loss: 0.7908364534378052
Epoch 270, training loss: 1.0917415618896484 = 0.44962289929389954 + 0.1 * 6.421186447143555
Epoch 270, val loss: 0.7625955939292908
Epoch 280, training loss: 1.0499839782714844 = 0.4085920453071594 + 0.1 * 6.413918972015381
Epoch 280, val loss: 0.7374061942100525
Epoch 290, training loss: 1.0106843709945679 = 0.37035033106803894 + 0.1 * 6.4033403396606445
Epoch 290, val loss: 0.7152246832847595
Epoch 300, training loss: 0.9747124910354614 = 0.33494237065315247 + 0.1 * 6.397700786590576
Epoch 300, val loss: 0.6957607269287109
Epoch 310, training loss: 0.941699743270874 = 0.30249306559562683 + 0.1 * 6.392066478729248
Epoch 310, val loss: 0.679039478302002
Epoch 320, training loss: 0.9111690521240234 = 0.2729291617870331 + 0.1 * 6.38239860534668
Epoch 320, val loss: 0.6646934151649475
Epoch 330, training loss: 0.883802592754364 = 0.246151402592659 + 0.1 * 6.376512050628662
Epoch 330, val loss: 0.6525875329971313
Epoch 340, training loss: 0.8601765632629395 = 0.22211679816246033 + 0.1 * 6.3805975914001465
Epoch 340, val loss: 0.6427512168884277
Epoch 350, training loss: 0.8364009857177734 = 0.2005736529827118 + 0.1 * 6.358273029327393
Epoch 350, val loss: 0.6347870826721191
Epoch 360, training loss: 0.8164966106414795 = 0.18118266761302948 + 0.1 * 6.3531389236450195
Epoch 360, val loss: 0.628724992275238
Epoch 370, training loss: 0.800327718257904 = 0.1638059765100479 + 0.1 * 6.365217208862305
Epoch 370, val loss: 0.6244500279426575
Epoch 380, training loss: 0.7828067541122437 = 0.148392915725708 + 0.1 * 6.344138145446777
Epoch 380, val loss: 0.6216632127761841
Epoch 390, training loss: 0.7692650556564331 = 0.13470320403575897 + 0.1 * 6.34561824798584
Epoch 390, val loss: 0.6204127669334412
Epoch 400, training loss: 0.7560779452323914 = 0.12250151485204697 + 0.1 * 6.335764408111572
Epoch 400, val loss: 0.620408833026886
Epoch 410, training loss: 0.7444823980331421 = 0.1116318628191948 + 0.1 * 6.328505039215088
Epoch 410, val loss: 0.6215782165527344
Epoch 420, training loss: 0.734062910079956 = 0.10186272859573364 + 0.1 * 6.3220014572143555
Epoch 420, val loss: 0.6236511468887329
Epoch 430, training loss: 0.7253460884094238 = 0.09313635528087616 + 0.1 * 6.322096824645996
Epoch 430, val loss: 0.6264362335205078
Epoch 440, training loss: 0.7166076898574829 = 0.08514660596847534 + 0.1 * 6.314610481262207
Epoch 440, val loss: 0.6299290657043457
Epoch 450, training loss: 0.7095987200737 = 0.07775647938251495 + 0.1 * 6.318422317504883
Epoch 450, val loss: 0.6339550018310547
Epoch 460, training loss: 0.7014985084533691 = 0.07080969214439392 + 0.1 * 6.306887626647949
Epoch 460, val loss: 0.6382758617401123
Epoch 470, training loss: 0.695786714553833 = 0.06427715718746185 + 0.1 * 6.3150954246521
Epoch 470, val loss: 0.6428765058517456
Epoch 480, training loss: 0.6884165406227112 = 0.05822187662124634 + 0.1 * 6.301946640014648
Epoch 480, val loss: 0.6476553678512573
Epoch 490, training loss: 0.6820036172866821 = 0.052788689732551575 + 0.1 * 6.292149066925049
Epoch 490, val loss: 0.6526613831520081
Epoch 500, training loss: 0.6772564053535461 = 0.04810447245836258 + 0.1 * 6.2915191650390625
Epoch 500, val loss: 0.6580303311347961
Epoch 510, training loss: 0.6722844243049622 = 0.044052980840206146 + 0.1 * 6.282314300537109
Epoch 510, val loss: 0.6638349294662476
Epoch 520, training loss: 0.6688570976257324 = 0.04051458463072777 + 0.1 * 6.2834248542785645
Epoch 520, val loss: 0.6698313355445862
Epoch 530, training loss: 0.665209174156189 = 0.03739126771688461 + 0.1 * 6.278179168701172
Epoch 530, val loss: 0.6759610772132874
Epoch 540, training loss: 0.6620072722434998 = 0.03462483733892441 + 0.1 * 6.273824214935303
Epoch 540, val loss: 0.6821667551994324
Epoch 550, training loss: 0.658759593963623 = 0.03215581923723221 + 0.1 * 6.266037464141846
Epoch 550, val loss: 0.6884082555770874
Epoch 560, training loss: 0.6585233807563782 = 0.029938669875264168 + 0.1 * 6.285846710205078
Epoch 560, val loss: 0.6947415471076965
Epoch 570, training loss: 0.6542898416519165 = 0.027956735342741013 + 0.1 * 6.263330936431885
Epoch 570, val loss: 0.701084315776825
Epoch 580, training loss: 0.652599036693573 = 0.0261736661195755 + 0.1 * 6.264253616333008
Epoch 580, val loss: 0.7074876427650452
Epoch 590, training loss: 0.6501200199127197 = 0.024569358676671982 + 0.1 * 6.255506992340088
Epoch 590, val loss: 0.7138975262641907
Epoch 600, training loss: 0.6482298970222473 = 0.023116687312722206 + 0.1 * 6.251132011413574
Epoch 600, val loss: 0.7202576398849487
Epoch 610, training loss: 0.6474823355674744 = 0.02179693430662155 + 0.1 * 6.2568535804748535
Epoch 610, val loss: 0.7266546487808228
Epoch 620, training loss: 0.646179735660553 = 0.020601117983460426 + 0.1 * 6.255786418914795
Epoch 620, val loss: 0.733019232749939
Epoch 630, training loss: 0.6438008546829224 = 0.01951194740831852 + 0.1 * 6.242888927459717
Epoch 630, val loss: 0.7392474412918091
Epoch 640, training loss: 0.6428031921386719 = 0.018513726070523262 + 0.1 * 6.242894172668457
Epoch 640, val loss: 0.7454989552497864
Epoch 650, training loss: 0.6414279937744141 = 0.01759585365653038 + 0.1 * 6.238321781158447
Epoch 650, val loss: 0.7516088485717773
Epoch 660, training loss: 0.6408685445785522 = 0.016750020906329155 + 0.1 * 6.241185188293457
Epoch 660, val loss: 0.7576828002929688
Epoch 670, training loss: 0.6398767232894897 = 0.01596982404589653 + 0.1 * 6.239068984985352
Epoch 670, val loss: 0.7636605501174927
Epoch 680, training loss: 0.6380649209022522 = 0.015249102376401424 + 0.1 * 6.228158473968506
Epoch 680, val loss: 0.7694925665855408
Epoch 690, training loss: 0.6369609236717224 = 0.014577583409845829 + 0.1 * 6.2238335609436035
Epoch 690, val loss: 0.7753140926361084
Epoch 700, training loss: 0.637127697467804 = 0.013951677829027176 + 0.1 * 6.231760025024414
Epoch 700, val loss: 0.7810826897621155
Epoch 710, training loss: 0.6354438066482544 = 0.013373102992773056 + 0.1 * 6.220707416534424
Epoch 710, val loss: 0.7866194248199463
Epoch 720, training loss: 0.6350383758544922 = 0.012834062799811363 + 0.1 * 6.222043037414551
Epoch 720, val loss: 0.7920482158660889
Epoch 730, training loss: 0.6337578892707825 = 0.012328062206506729 + 0.1 * 6.214297771453857
Epoch 730, val loss: 0.7975031733512878
Epoch 740, training loss: 0.6325969696044922 = 0.011855218559503555 + 0.1 * 6.207417011260986
Epoch 740, val loss: 0.802782416343689
Epoch 750, training loss: 0.6326300501823425 = 0.011410795152187347 + 0.1 * 6.212192535400391
Epoch 750, val loss: 0.8080084919929504
Epoch 760, training loss: 0.6324681639671326 = 0.01099346112459898 + 0.1 * 6.214746952056885
Epoch 760, val loss: 0.8132063150405884
Epoch 770, training loss: 0.6316294074058533 = 0.010603164322674274 + 0.1 * 6.210261821746826
Epoch 770, val loss: 0.8182064294815063
Epoch 780, training loss: 0.6302268505096436 = 0.01023593358695507 + 0.1 * 6.199909210205078
Epoch 780, val loss: 0.8231184482574463
Epoch 790, training loss: 0.6307916641235352 = 0.009888668544590473 + 0.1 * 6.2090301513671875
Epoch 790, val loss: 0.8280609250068665
Epoch 800, training loss: 0.6286433935165405 = 0.009561192244291306 + 0.1 * 6.190822124481201
Epoch 800, val loss: 0.8328719139099121
Epoch 810, training loss: 0.6285200119018555 = 0.009252817369997501 + 0.1 * 6.192671298980713
Epoch 810, val loss: 0.8375403881072998
Epoch 820, training loss: 0.6283276677131653 = 0.008959426544606686 + 0.1 * 6.193682670593262
Epoch 820, val loss: 0.8422932028770447
Epoch 830, training loss: 0.6276934146881104 = 0.008683049120008945 + 0.1 * 6.190103530883789
Epoch 830, val loss: 0.8468325138092041
Epoch 840, training loss: 0.6288609504699707 = 0.008420320227742195 + 0.1 * 6.204405784606934
Epoch 840, val loss: 0.8513385057449341
Epoch 850, training loss: 0.6266410946846008 = 0.008171453140676022 + 0.1 * 6.184696674346924
Epoch 850, val loss: 0.8558176755905151
Epoch 860, training loss: 0.6256533861160278 = 0.007936389185488224 + 0.1 * 6.177169322967529
Epoch 860, val loss: 0.860139787197113
Epoch 870, training loss: 0.6286640167236328 = 0.007711121346801519 + 0.1 * 6.209528923034668
Epoch 870, val loss: 0.8644667863845825
Epoch 880, training loss: 0.6250911355018616 = 0.00749561982229352 + 0.1 * 6.175954818725586
Epoch 880, val loss: 0.8687623739242554
Epoch 890, training loss: 0.624869704246521 = 0.007292463444173336 + 0.1 * 6.175772190093994
Epoch 890, val loss: 0.8728206753730774
Epoch 900, training loss: 0.6263220310211182 = 0.007098392583429813 + 0.1 * 6.192236423492432
Epoch 900, val loss: 0.8769122958183289
Epoch 910, training loss: 0.6244939565658569 = 0.006911329925060272 + 0.1 * 6.175826072692871
Epoch 910, val loss: 0.8809924721717834
Epoch 920, training loss: 0.6237661242485046 = 0.0067349933087825775 + 0.1 * 6.170311450958252
Epoch 920, val loss: 0.8848744630813599
Epoch 930, training loss: 0.6232320666313171 = 0.006565442308783531 + 0.1 * 6.166666030883789
Epoch 930, val loss: 0.8887410759925842
Epoch 940, training loss: 0.623062252998352 = 0.00640273280441761 + 0.1 * 6.166594982147217
Epoch 940, val loss: 0.8926342725753784
Epoch 950, training loss: 0.6224576830863953 = 0.00624629994854331 + 0.1 * 6.162113666534424
Epoch 950, val loss: 0.8964751362800598
Epoch 960, training loss: 0.6236510276794434 = 0.006096940021961927 + 0.1 * 6.175540447235107
Epoch 960, val loss: 0.9002078175544739
Epoch 970, training loss: 0.622739315032959 = 0.0059532709419727325 + 0.1 * 6.167860507965088
Epoch 970, val loss: 0.9039068222045898
Epoch 980, training loss: 0.6219848394393921 = 0.0058161914348602295 + 0.1 * 6.161686420440674
Epoch 980, val loss: 0.9075676798820496
Epoch 990, training loss: 0.6219367980957031 = 0.005684192292392254 + 0.1 * 6.162525653839111
Epoch 990, val loss: 0.9111331105232239
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.4280
Flip ASR: 0.3644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7759673595428467 = 1.9385818243026733 + 0.1 * 8.373855590820312
Epoch 0, val loss: 1.938293695449829
Epoch 10, training loss: 2.7667250633239746 = 1.9293559789657593 + 0.1 * 8.373689651489258
Epoch 10, val loss: 1.9294004440307617
Epoch 20, training loss: 2.754995346069336 = 1.9177147150039673 + 0.1 * 8.372806549072266
Epoch 20, val loss: 1.9175448417663574
Epoch 30, training loss: 2.737684726715088 = 1.9010233879089355 + 0.1 * 8.366612434387207
Epoch 30, val loss: 1.8999731540679932
Epoch 40, training loss: 2.708907127380371 = 1.876220941543579 + 0.1 * 8.326862335205078
Epoch 40, val loss: 1.8737913370132446
Epoch 50, training loss: 2.6489598751068115 = 1.8421592712402344 + 0.1 * 8.06800651550293
Epoch 50, val loss: 1.8396153450012207
Epoch 60, training loss: 2.556765556335449 = 1.8041192293167114 + 0.1 * 7.526463508605957
Epoch 60, val loss: 1.8044184446334839
Epoch 70, training loss: 2.4831509590148926 = 1.7695591449737549 + 0.1 * 7.135919094085693
Epoch 70, val loss: 1.7743375301361084
Epoch 80, training loss: 2.4208498001098633 = 1.7320078611373901 + 0.1 * 6.888419151306152
Epoch 80, val loss: 1.7418845891952515
Epoch 90, training loss: 2.3602981567382812 = 1.682255506515503 + 0.1 * 6.780425548553467
Epoch 90, val loss: 1.6988801956176758
Epoch 100, training loss: 2.288710355758667 = 1.616806149482727 + 0.1 * 6.71904182434082
Epoch 100, val loss: 1.6434526443481445
Epoch 110, training loss: 2.2034525871276855 = 1.5353916883468628 + 0.1 * 6.68060827255249
Epoch 110, val loss: 1.576542615890503
Epoch 120, training loss: 2.109175205230713 = 1.4433221817016602 + 0.1 * 6.658528804779053
Epoch 120, val loss: 1.5022941827774048
Epoch 130, training loss: 2.0120046138763428 = 1.3472298383712769 + 0.1 * 6.647747993469238
Epoch 130, val loss: 1.4262895584106445
Epoch 140, training loss: 1.9132810831069946 = 1.249444603919983 + 0.1 * 6.638364791870117
Epoch 140, val loss: 1.351254940032959
Epoch 150, training loss: 1.812234878540039 = 1.149501919746399 + 0.1 * 6.627330303192139
Epoch 150, val loss: 1.275178074836731
Epoch 160, training loss: 1.710073471069336 = 1.0486657619476318 + 0.1 * 6.614076614379883
Epoch 160, val loss: 1.1976147890090942
Epoch 170, training loss: 1.6105594635009766 = 0.9501215219497681 + 0.1 * 6.604378700256348
Epoch 170, val loss: 1.1208782196044922
Epoch 180, training loss: 1.5165884494781494 = 0.8577539920806885 + 0.1 * 6.588343620300293
Epoch 180, val loss: 1.0492082834243774
Epoch 190, training loss: 1.4307358264923096 = 0.7730388045310974 + 0.1 * 6.57697057723999
Epoch 190, val loss: 0.9841568470001221
Epoch 200, training loss: 1.354050874710083 = 0.6968945264816284 + 0.1 * 6.571563720703125
Epoch 200, val loss: 0.927377462387085
Epoch 210, training loss: 1.285754680633545 = 0.6297174692153931 + 0.1 * 6.5603718757629395
Epoch 210, val loss: 0.8802470564842224
Epoch 220, training loss: 1.2253729104995728 = 0.5698985457420349 + 0.1 * 6.554743766784668
Epoch 220, val loss: 0.8416198492050171
Epoch 230, training loss: 1.1707775592803955 = 0.5159531831741333 + 0.1 * 6.548243999481201
Epoch 230, val loss: 0.8104490041732788
Epoch 240, training loss: 1.121963620185852 = 0.4670577049255371 + 0.1 * 6.54905891418457
Epoch 240, val loss: 0.7863048315048218
Epoch 250, training loss: 1.0763643980026245 = 0.4224366545677185 + 0.1 * 6.53927755355835
Epoch 250, val loss: 0.7684034705162048
Epoch 260, training loss: 1.0338349342346191 = 0.380677193403244 + 0.1 * 6.5315775871276855
Epoch 260, val loss: 0.7558695673942566
Epoch 270, training loss: 0.9937637448310852 = 0.34121114015579224 + 0.1 * 6.52552604675293
Epoch 270, val loss: 0.7479594349861145
Epoch 280, training loss: 0.956274151802063 = 0.30429908633232117 + 0.1 * 6.519750595092773
Epoch 280, val loss: 0.7443446516990662
Epoch 290, training loss: 0.9216139316558838 = 0.270276814699173 + 0.1 * 6.513370513916016
Epoch 290, val loss: 0.7444992065429688
Epoch 300, training loss: 0.890180766582489 = 0.2394675761461258 + 0.1 * 6.507132053375244
Epoch 300, val loss: 0.7478820085525513
Epoch 310, training loss: 0.8620948791503906 = 0.21228604018688202 + 0.1 * 6.498088359832764
Epoch 310, val loss: 0.7543164491653442
Epoch 320, training loss: 0.8375197649002075 = 0.18848881125450134 + 0.1 * 6.490309715270996
Epoch 320, val loss: 0.7634627223014832
Epoch 330, training loss: 0.8167764544487 = 0.16786092519760132 + 0.1 * 6.489155292510986
Epoch 330, val loss: 0.7747198939323425
Epoch 340, training loss: 0.7975701689720154 = 0.1500147432088852 + 0.1 * 6.4755539894104
Epoch 340, val loss: 0.7876583933830261
Epoch 350, training loss: 0.780568540096283 = 0.13453078269958496 + 0.1 * 6.4603776931762695
Epoch 350, val loss: 0.802021324634552
Epoch 360, training loss: 0.7668169736862183 = 0.12103131413459778 + 0.1 * 6.457856178283691
Epoch 360, val loss: 0.8172874450683594
Epoch 370, training loss: 0.7534065246582031 = 0.1092400848865509 + 0.1 * 6.441664695739746
Epoch 370, val loss: 0.8334035873413086
Epoch 380, training loss: 0.7430529594421387 = 0.09888216853141785 + 0.1 * 6.441708087921143
Epoch 380, val loss: 0.8500482439994812
Epoch 390, training loss: 0.7327425479888916 = 0.08977751433849335 + 0.1 * 6.429649829864502
Epoch 390, val loss: 0.8669573664665222
Epoch 400, training loss: 0.7237510681152344 = 0.08172613382339478 + 0.1 * 6.420248985290527
Epoch 400, val loss: 0.8841636776924133
Epoch 410, training loss: 0.7165771126747131 = 0.07459338754415512 + 0.1 * 6.419837474822998
Epoch 410, val loss: 0.9013246297836304
Epoch 420, training loss: 0.7085191011428833 = 0.06827806681394577 + 0.1 * 6.40241003036499
Epoch 420, val loss: 0.9184824824333191
Epoch 430, training loss: 0.7018887996673584 = 0.06263979524374008 + 0.1 * 6.392490386962891
Epoch 430, val loss: 0.9357184171676636
Epoch 440, training loss: 0.6961966156959534 = 0.05758774280548096 + 0.1 * 6.3860883712768555
Epoch 440, val loss: 0.9526750445365906
Epoch 450, training loss: 0.6905434131622314 = 0.05306127667427063 + 0.1 * 6.374821186065674
Epoch 450, val loss: 0.9696386456489563
Epoch 460, training loss: 0.687079668045044 = 0.04898727312684059 + 0.1 * 6.380923748016357
Epoch 460, val loss: 0.9863368272781372
Epoch 470, training loss: 0.6818366050720215 = 0.04533389210700989 + 0.1 * 6.365026950836182
Epoch 470, val loss: 1.0028023719787598
Epoch 480, training loss: 0.6800929307937622 = 0.04204360768198967 + 0.1 * 6.380492687225342
Epoch 480, val loss: 1.0187915563583374
Epoch 490, training loss: 0.6739112138748169 = 0.03908516839146614 + 0.1 * 6.348260402679443
Epoch 490, val loss: 1.0347039699554443
Epoch 500, training loss: 0.6708229780197144 = 0.03640437126159668 + 0.1 * 6.344185829162598
Epoch 500, val loss: 1.0501551628112793
Epoch 510, training loss: 0.6702420711517334 = 0.0339679978787899 + 0.1 * 6.362740993499756
Epoch 510, val loss: 1.0652716159820557
Epoch 520, training loss: 0.665095329284668 = 0.031765490770339966 + 0.1 * 6.333298206329346
Epoch 520, val loss: 1.080138087272644
Epoch 530, training loss: 0.6623648405075073 = 0.029760245233774185 + 0.1 * 6.326045989990234
Epoch 530, val loss: 1.0945689678192139
Epoch 540, training loss: 0.6609382033348083 = 0.027928832918405533 + 0.1 * 6.3300933837890625
Epoch 540, val loss: 1.108851671218872
Epoch 550, training loss: 0.6585181355476379 = 0.02625923417508602 + 0.1 * 6.322588920593262
Epoch 550, val loss: 1.1224843263626099
Epoch 560, training loss: 0.6562957167625427 = 0.024734972044825554 + 0.1 * 6.31560754776001
Epoch 560, val loss: 1.1360664367675781
Epoch 570, training loss: 0.6546510457992554 = 0.023336082696914673 + 0.1 * 6.3131489753723145
Epoch 570, val loss: 1.149011492729187
Epoch 580, training loss: 0.6525318026542664 = 0.022058064118027687 + 0.1 * 6.304737091064453
Epoch 580, val loss: 1.1618958711624146
Epoch 590, training loss: 0.6535564661026001 = 0.020879356190562248 + 0.1 * 6.326771259307861
Epoch 590, val loss: 1.17424476146698
Epoch 600, training loss: 0.6489909887313843 = 0.019799157977104187 + 0.1 * 6.29191780090332
Epoch 600, val loss: 1.1862047910690308
Epoch 610, training loss: 0.6481450796127319 = 0.018804611638188362 + 0.1 * 6.293404579162598
Epoch 610, val loss: 1.1980676651000977
Epoch 620, training loss: 0.6489023566246033 = 0.017882974818348885 + 0.1 * 6.3101935386657715
Epoch 620, val loss: 1.2095979452133179
Epoch 630, training loss: 0.6454192399978638 = 0.01702992618083954 + 0.1 * 6.283892631530762
Epoch 630, val loss: 1.2202938795089722
Epoch 640, training loss: 0.6444925665855408 = 0.016244882717728615 + 0.1 * 6.282476425170898
Epoch 640, val loss: 1.2313241958618164
Epoch 650, training loss: 0.6431390047073364 = 0.015512374229729176 + 0.1 * 6.276266574859619
Epoch 650, val loss: 1.2416232824325562
Epoch 660, training loss: 0.6421272158622742 = 0.014834870584309101 + 0.1 * 6.272922992706299
Epoch 660, val loss: 1.2521246671676636
Epoch 670, training loss: 0.6421953439712524 = 0.01420016773045063 + 0.1 * 6.279951572418213
Epoch 670, val loss: 1.2619043588638306
Epoch 680, training loss: 0.6391809582710266 = 0.013610613532364368 + 0.1 * 6.255703449249268
Epoch 680, val loss: 1.2718932628631592
Epoch 690, training loss: 0.6394718885421753 = 0.01305907592177391 + 0.1 * 6.2641282081604
Epoch 690, val loss: 1.2814414501190186
Epoch 700, training loss: 0.638335108757019 = 0.012541077099740505 + 0.1 * 6.257940292358398
Epoch 700, val loss: 1.2904999256134033
Epoch 710, training loss: 0.6367995142936707 = 0.012059232220053673 + 0.1 * 6.247403144836426
Epoch 710, val loss: 1.29988431930542
Epoch 720, training loss: 0.638290524482727 = 0.011603784747421741 + 0.1 * 6.266867637634277
Epoch 720, val loss: 1.3086462020874023
Epoch 730, training loss: 0.6353740692138672 = 0.011176805943250656 + 0.1 * 6.24197244644165
Epoch 730, val loss: 1.317205548286438
Epoch 740, training loss: 0.6359964609146118 = 0.010776380077004433 + 0.1 * 6.252200603485107
Epoch 740, val loss: 1.3259278535842896
Epoch 750, training loss: 0.6341844201087952 = 0.010397647507488728 + 0.1 * 6.23786735534668
Epoch 750, val loss: 1.3340193033218384
Epoch 760, training loss: 0.632737934589386 = 0.010043248534202576 + 0.1 * 6.226946830749512
Epoch 760, val loss: 1.3424464464187622
Epoch 770, training loss: 0.6330481767654419 = 0.009705494157969952 + 0.1 * 6.233426570892334
Epoch 770, val loss: 1.350403904914856
Epoch 780, training loss: 0.632699191570282 = 0.009386308491230011 + 0.1 * 6.233128547668457
Epoch 780, val loss: 1.3578414916992188
Epoch 790, training loss: 0.6315553188323975 = 0.009086228907108307 + 0.1 * 6.2246904373168945
Epoch 790, val loss: 1.3657810688018799
Epoch 800, training loss: 0.6300471425056458 = 0.008799925446510315 + 0.1 * 6.212472438812256
Epoch 800, val loss: 1.3732173442840576
Epoch 810, training loss: 0.6302300095558167 = 0.008528225123882294 + 0.1 * 6.217017650604248
Epoch 810, val loss: 1.3803597688674927
Epoch 820, training loss: 0.6290901899337769 = 0.008271628990769386 + 0.1 * 6.20818567276001
Epoch 820, val loss: 1.3876475095748901
Epoch 830, training loss: 0.6294302940368652 = 0.008027027361094952 + 0.1 * 6.2140326499938965
Epoch 830, val loss: 1.3947241306304932
Epoch 840, training loss: 0.6282413005828857 = 0.007793842349201441 + 0.1 * 6.204474449157715
Epoch 840, val loss: 1.4015737771987915
Epoch 850, training loss: 0.6308073997497559 = 0.007571665104478598 + 0.1 * 6.232357501983643
Epoch 850, val loss: 1.4082869291305542
Epoch 860, training loss: 0.6266929507255554 = 0.007358899340033531 + 0.1 * 6.193340301513672
Epoch 860, val loss: 1.4145457744598389
Epoch 870, training loss: 0.6276648044586182 = 0.007158549036830664 + 0.1 * 6.205061912536621
Epoch 870, val loss: 1.4214228391647339
Epoch 880, training loss: 0.6259426474571228 = 0.0069651007652282715 + 0.1 * 6.189775466918945
Epoch 880, val loss: 1.427551507949829
Epoch 890, training loss: 0.6253247857093811 = 0.006782040931284428 + 0.1 * 6.185427188873291
Epoch 890, val loss: 1.4341260194778442
Epoch 900, training loss: 0.625813364982605 = 0.006605566944926977 + 0.1 * 6.192078113555908
Epoch 900, val loss: 1.440210223197937
Epoch 910, training loss: 0.6252012848854065 = 0.006436311639845371 + 0.1 * 6.187649726867676
Epoch 910, val loss: 1.4461185932159424
Epoch 920, training loss: 0.6242354512214661 = 0.006275232415646315 + 0.1 * 6.179602146148682
Epoch 920, val loss: 1.4521058797836304
Epoch 930, training loss: 0.6243299841880798 = 0.006120145320892334 + 0.1 * 6.182098388671875
Epoch 930, val loss: 1.4580457210540771
Epoch 940, training loss: 0.6232960820198059 = 0.0059713637456297874 + 0.1 * 6.17324686050415
Epoch 940, val loss: 1.4637210369110107
Epoch 950, training loss: 0.6242018342018127 = 0.0058282529935240746 + 0.1 * 6.1837358474731445
Epoch 950, val loss: 1.4692387580871582
Epoch 960, training loss: 0.6228180527687073 = 0.005691892001777887 + 0.1 * 6.171261787414551
Epoch 960, val loss: 1.474753499031067
Epoch 970, training loss: 0.6244431138038635 = 0.0055608199909329414 + 0.1 * 6.1888227462768555
Epoch 970, val loss: 1.4803104400634766
Epoch 980, training loss: 0.6224446296691895 = 0.005433429032564163 + 0.1 * 6.170111656188965
Epoch 980, val loss: 1.4854031801223755
Epoch 990, training loss: 0.6219222545623779 = 0.005312854889780283 + 0.1 * 6.166093826293945
Epoch 990, val loss: 1.4909021854400635
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.9041
Flip ASR: 0.8889/225 nodes
The final ASR:0.56335, 0.24262, Accuracy:0.80370, 0.02688
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9594])
updated graph: torch.Size([2, 10670])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98032, 0.00870, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.786473035812378 = 1.9490966796875 + 0.1 * 8.373763084411621
Epoch 0, val loss: 1.9569218158721924
Epoch 10, training loss: 2.7767815589904785 = 1.9394385814666748 + 0.1 * 8.373428344726562
Epoch 10, val loss: 1.9476608037948608
Epoch 20, training loss: 2.7644925117492676 = 1.9273645877838135 + 0.1 * 8.371278762817383
Epoch 20, val loss: 1.935678482055664
Epoch 30, training loss: 2.7456767559051514 = 1.910170555114746 + 0.1 * 8.355061531066895
Epoch 30, val loss: 1.918049693107605
Epoch 40, training loss: 2.710832357406616 = 1.88462233543396 + 0.1 * 8.262099266052246
Epoch 40, val loss: 1.8918355703353882
Epoch 50, training loss: 2.620990753173828 = 1.8516874313354492 + 0.1 * 7.6930317878723145
Epoch 50, val loss: 1.859697937965393
Epoch 60, training loss: 2.5389339923858643 = 1.8169759511947632 + 0.1 * 7.219580173492432
Epoch 60, val loss: 1.8280655145645142
Epoch 70, training loss: 2.470381021499634 = 1.782480001449585 + 0.1 * 6.87900972366333
Epoch 70, val loss: 1.7982593774795532
Epoch 80, training loss: 2.418508291244507 = 1.7493958473205566 + 0.1 * 6.691123962402344
Epoch 80, val loss: 1.7706332206726074
Epoch 90, training loss: 2.370652914047241 = 1.7110140323638916 + 0.1 * 6.59638786315918
Epoch 90, val loss: 1.7373665571212769
Epoch 100, training loss: 2.3149876594543457 = 1.6593518257141113 + 0.1 * 6.55635929107666
Epoch 100, val loss: 1.693951964378357
Epoch 110, training loss: 2.2447195053100586 = 1.5917503833770752 + 0.1 * 6.529691219329834
Epoch 110, val loss: 1.6390570402145386
Epoch 120, training loss: 2.1582040786743164 = 1.5073732137680054 + 0.1 * 6.508309841156006
Epoch 120, val loss: 1.5711278915405273
Epoch 130, training loss: 2.0599327087402344 = 1.4107829332351685 + 0.1 * 6.491497993469238
Epoch 130, val loss: 1.4928799867630005
Epoch 140, training loss: 1.9549801349639893 = 1.3074088096618652 + 0.1 * 6.475712776184082
Epoch 140, val loss: 1.409118413925171
Epoch 150, training loss: 1.8475165367126465 = 1.201434850692749 + 0.1 * 6.460815906524658
Epoch 150, val loss: 1.323349118232727
Epoch 160, training loss: 1.7413172721862793 = 1.0961692333221436 + 0.1 * 6.451480865478516
Epoch 160, val loss: 1.2377108335494995
Epoch 170, training loss: 1.6393824815750122 = 0.9952820539474487 + 0.1 * 6.441004276275635
Epoch 170, val loss: 1.1571563482284546
Epoch 180, training loss: 1.5438897609710693 = 0.9003814458847046 + 0.1 * 6.435083389282227
Epoch 180, val loss: 1.0826592445373535
Epoch 190, training loss: 1.4548722505569458 = 0.8122717142105103 + 0.1 * 6.4260053634643555
Epoch 190, val loss: 1.0148639678955078
Epoch 200, training loss: 1.3727374076843262 = 0.7309556603431702 + 0.1 * 6.417816638946533
Epoch 200, val loss: 0.9527322053909302
Epoch 210, training loss: 1.2982566356658936 = 0.6571658849716187 + 0.1 * 6.410907745361328
Epoch 210, val loss: 0.8972172141075134
Epoch 220, training loss: 1.2326700687408447 = 0.5916324853897095 + 0.1 * 6.41037654876709
Epoch 220, val loss: 0.8493978977203369
Epoch 230, training loss: 1.1740097999572754 = 0.5347457528114319 + 0.1 * 6.392641067504883
Epoch 230, val loss: 0.8100881576538086
Epoch 240, training loss: 1.123534917831421 = 0.4846462607383728 + 0.1 * 6.388886451721191
Epoch 240, val loss: 0.7778991460800171
Epoch 250, training loss: 1.0773614645004272 = 0.4397396743297577 + 0.1 * 6.376217365264893
Epoch 250, val loss: 0.7521740198135376
Epoch 260, training loss: 1.0358585119247437 = 0.39778006076812744 + 0.1 * 6.380784511566162
Epoch 260, val loss: 0.7306557297706604
Epoch 270, training loss: 0.9936015009880066 = 0.35790932178497314 + 0.1 * 6.356921672821045
Epoch 270, val loss: 0.7126818299293518
Epoch 280, training loss: 0.954712986946106 = 0.31949153542518616 + 0.1 * 6.3522138595581055
Epoch 280, val loss: 0.6975094676017761
Epoch 290, training loss: 0.9180964231491089 = 0.2833753526210785 + 0.1 * 6.347210884094238
Epoch 290, val loss: 0.6853269934654236
Epoch 300, training loss: 0.8838396072387695 = 0.2502456605434418 + 0.1 * 6.335938930511475
Epoch 300, val loss: 0.6761860251426697
Epoch 310, training loss: 0.852935791015625 = 0.22044570744037628 + 0.1 * 6.324901103973389
Epoch 310, val loss: 0.6701632738113403
Epoch 320, training loss: 0.8277577757835388 = 0.194453164935112 + 0.1 * 6.333045959472656
Epoch 320, val loss: 0.6671215891838074
Epoch 330, training loss: 0.8037111759185791 = 0.1722613275051117 + 0.1 * 6.314497947692871
Epoch 330, val loss: 0.667019248008728
Epoch 340, training loss: 0.7837652564048767 = 0.1533082276582718 + 0.1 * 6.304570198059082
Epoch 340, val loss: 0.6691246628761292
Epoch 350, training loss: 0.7683589458465576 = 0.13717399537563324 + 0.1 * 6.311849117279053
Epoch 350, val loss: 0.6730690002441406
Epoch 360, training loss: 0.7526599168777466 = 0.1233648806810379 + 0.1 * 6.29295015335083
Epoch 360, val loss: 0.6786990761756897
Epoch 370, training loss: 0.7400794625282288 = 0.11143440008163452 + 0.1 * 6.286450386047363
Epoch 370, val loss: 0.6854090094566345
Epoch 380, training loss: 0.7293459177017212 = 0.10110462456941605 + 0.1 * 6.282413005828857
Epoch 380, val loss: 0.6932856440544128
Epoch 390, training loss: 0.7193931937217712 = 0.09202885627746582 + 0.1 * 6.2736430168151855
Epoch 390, val loss: 0.7017228603363037
Epoch 400, training loss: 0.7120240926742554 = 0.08400345593690872 + 0.1 * 6.280206203460693
Epoch 400, val loss: 0.7108911871910095
Epoch 410, training loss: 0.7036136388778687 = 0.07690723985433578 + 0.1 * 6.267064094543457
Epoch 410, val loss: 0.7205426096916199
Epoch 420, training loss: 0.6962447166442871 = 0.07059884071350098 + 0.1 * 6.256458759307861
Epoch 420, val loss: 0.7305516600608826
Epoch 430, training loss: 0.6901117563247681 = 0.06498033553361893 + 0.1 * 6.251314163208008
Epoch 430, val loss: 0.7406264543533325
Epoch 440, training loss: 0.6845104098320007 = 0.059978630393743515 + 0.1 * 6.2453179359436035
Epoch 440, val loss: 0.7509313821792603
Epoch 450, training loss: 0.6801906824111938 = 0.055485647171735764 + 0.1 * 6.247049808502197
Epoch 450, val loss: 0.7610574960708618
Epoch 460, training loss: 0.6749062538146973 = 0.05144912376999855 + 0.1 * 6.23457145690918
Epoch 460, val loss: 0.7712361812591553
Epoch 470, training loss: 0.6713423728942871 = 0.04780535399913788 + 0.1 * 6.235369682312012
Epoch 470, val loss: 0.7812383770942688
Epoch 480, training loss: 0.6669455766677856 = 0.04451233521103859 + 0.1 * 6.224332332611084
Epoch 480, val loss: 0.7911427021026611
Epoch 490, training loss: 0.66449373960495 = 0.04152964428067207 + 0.1 * 6.229640960693359
Epoch 490, val loss: 0.8009344935417175
Epoch 500, training loss: 0.6616794466972351 = 0.03882048651576042 + 0.1 * 6.2285895347595215
Epoch 500, val loss: 0.8104010820388794
Epoch 510, training loss: 0.6578506827354431 = 0.036359645426273346 + 0.1 * 6.21491003036499
Epoch 510, val loss: 0.8199054002761841
Epoch 520, training loss: 0.6552791595458984 = 0.034110177308321 + 0.1 * 6.211689472198486
Epoch 520, val loss: 0.8291298747062683
Epoch 530, training loss: 0.6524398326873779 = 0.032055385410785675 + 0.1 * 6.2038445472717285
Epoch 530, val loss: 0.8382894992828369
Epoch 540, training loss: 0.6513226628303528 = 0.0301726795732975 + 0.1 * 6.2114996910095215
Epoch 540, val loss: 0.8473583459854126
Epoch 550, training loss: 0.6475978493690491 = 0.02844390459358692 + 0.1 * 6.1915388107299805
Epoch 550, val loss: 0.8562401533126831
Epoch 560, training loss: 0.6475409865379333 = 0.026854494586586952 + 0.1 * 6.206864356994629
Epoch 560, val loss: 0.8650771975517273
Epoch 570, training loss: 0.6453979015350342 = 0.025391031056642532 + 0.1 * 6.200068473815918
Epoch 570, val loss: 0.8737947344779968
Epoch 580, training loss: 0.6430400013923645 = 0.024042606353759766 + 0.1 * 6.189973831176758
Epoch 580, val loss: 0.882299542427063
Epoch 590, training loss: 0.6410158276557922 = 0.022799121215939522 + 0.1 * 6.182166576385498
Epoch 590, val loss: 0.8908277153968811
Epoch 600, training loss: 0.6397527456283569 = 0.021646317094564438 + 0.1 * 6.181064128875732
Epoch 600, val loss: 0.8991121053695679
Epoch 610, training loss: 0.6392647624015808 = 0.020578650757670403 + 0.1 * 6.186861038208008
Epoch 610, val loss: 0.9072496294975281
Epoch 620, training loss: 0.6372223496437073 = 0.019591333344578743 + 0.1 * 6.176310062408447
Epoch 620, val loss: 0.9153603315353394
Epoch 630, training loss: 0.6363660097122192 = 0.01867549866437912 + 0.1 * 6.176904678344727
Epoch 630, val loss: 0.9233574867248535
Epoch 640, training loss: 0.6349096894264221 = 0.017821550369262695 + 0.1 * 6.170881271362305
Epoch 640, val loss: 0.9310399293899536
Epoch 650, training loss: 0.6347931623458862 = 0.017028311267495155 + 0.1 * 6.177648067474365
Epoch 650, val loss: 0.9387832283973694
Epoch 660, training loss: 0.6336542963981628 = 0.01628713682293892 + 0.1 * 6.173671245574951
Epoch 660, val loss: 0.9463089108467102
Epoch 670, training loss: 0.6318750977516174 = 0.015595334582030773 + 0.1 * 6.162797451019287
Epoch 670, val loss: 0.9537057280540466
Epoch 680, training loss: 0.6308952569961548 = 0.014948530122637749 + 0.1 * 6.159466743469238
Epoch 680, val loss: 0.9609793424606323
Epoch 690, training loss: 0.6300008893013 = 0.01434371992945671 + 0.1 * 6.156571388244629
Epoch 690, val loss: 0.9682217240333557
Epoch 700, training loss: 0.6293705701828003 = 0.01377531886100769 + 0.1 * 6.155951976776123
Epoch 700, val loss: 0.9752293229103088
Epoch 710, training loss: 0.6308653950691223 = 0.01323970127850771 + 0.1 * 6.176257133483887
Epoch 710, val loss: 0.9818719029426575
Epoch 720, training loss: 0.6277832984924316 = 0.012741138227283955 + 0.1 * 6.150421619415283
Epoch 720, val loss: 0.9888612031936646
Epoch 730, training loss: 0.6271733045578003 = 0.012271802872419357 + 0.1 * 6.149014949798584
Epoch 730, val loss: 0.9955928325653076
Epoch 740, training loss: 0.627113938331604 = 0.011826936155557632 + 0.1 * 6.152870178222656
Epoch 740, val loss: 1.001996397972107
Epoch 750, training loss: 0.6261547803878784 = 0.011407321318984032 + 0.1 * 6.147474765777588
Epoch 750, val loss: 1.0082534551620483
Epoch 760, training loss: 0.6258580684661865 = 0.011013583280146122 + 0.1 * 6.148444652557373
Epoch 760, val loss: 1.0147217512130737
Epoch 770, training loss: 0.626365602016449 = 0.01064015831798315 + 0.1 * 6.157254695892334
Epoch 770, val loss: 1.0208683013916016
Epoch 780, training loss: 0.6243985295295715 = 0.010286944918334484 + 0.1 * 6.141116142272949
Epoch 780, val loss: 1.0269099473953247
Epoch 790, training loss: 0.6236247420310974 = 0.009952783584594727 + 0.1 * 6.136719703674316
Epoch 790, val loss: 1.0329430103302002
Epoch 800, training loss: 0.6246484518051147 = 0.009634565562009811 + 0.1 * 6.150138854980469
Epoch 800, val loss: 1.038722276687622
Epoch 810, training loss: 0.6229329109191895 = 0.009332932531833649 + 0.1 * 6.1359992027282715
Epoch 810, val loss: 1.0445129871368408
Epoch 820, training loss: 0.6224116683006287 = 0.009046525694429874 + 0.1 * 6.133650779724121
Epoch 820, val loss: 1.050239086151123
Epoch 830, training loss: 0.6222997903823853 = 0.008772825822234154 + 0.1 * 6.1352691650390625
Epoch 830, val loss: 1.0556282997131348
Epoch 840, training loss: 0.6215668320655823 = 0.008513806387782097 + 0.1 * 6.130529880523682
Epoch 840, val loss: 1.0611932277679443
Epoch 850, training loss: 0.6209201216697693 = 0.008267332799732685 + 0.1 * 6.126527786254883
Epoch 850, val loss: 1.0666882991790771
Epoch 860, training loss: 0.6212890148162842 = 0.00803246721625328 + 0.1 * 6.132565498352051
Epoch 860, val loss: 1.0720850229263306
Epoch 870, training loss: 0.6203041672706604 = 0.007807142101228237 + 0.1 * 6.124969959259033
Epoch 870, val loss: 1.0771887302398682
Epoch 880, training loss: 0.6210588812828064 = 0.007593323476612568 + 0.1 * 6.134655475616455
Epoch 880, val loss: 1.082448959350586
Epoch 890, training loss: 0.6195018887519836 = 0.007387995254248381 + 0.1 * 6.121139049530029
Epoch 890, val loss: 1.0874167680740356
Epoch 900, training loss: 0.6199215650558472 = 0.007193008903414011 + 0.1 * 6.127285480499268
Epoch 900, val loss: 1.0925406217575073
Epoch 910, training loss: 0.6190659403800964 = 0.007005349267274141 + 0.1 * 6.120605945587158
Epoch 910, val loss: 1.097425103187561
Epoch 920, training loss: 0.6181268692016602 = 0.006825738586485386 + 0.1 * 6.113010883331299
Epoch 920, val loss: 1.1022342443466187
Epoch 930, training loss: 0.61844801902771 = 0.006654432043433189 + 0.1 * 6.117935657501221
Epoch 930, val loss: 1.1071031093597412
Epoch 940, training loss: 0.618912398815155 = 0.0064892154186964035 + 0.1 * 6.124231815338135
Epoch 940, val loss: 1.1117209196090698
Epoch 950, training loss: 0.6173806190490723 = 0.006330766715109348 + 0.1 * 6.110498428344727
Epoch 950, val loss: 1.1163325309753418
Epoch 960, training loss: 0.619198203086853 = 0.006178864277899265 + 0.1 * 6.13019323348999
Epoch 960, val loss: 1.1208114624023438
Epoch 970, training loss: 0.6173078417778015 = 0.0060333735309541225 + 0.1 * 6.1127448081970215
Epoch 970, val loss: 1.1253985166549683
Epoch 980, training loss: 0.6173190474510193 = 0.005894608795642853 + 0.1 * 6.11424446105957
Epoch 980, val loss: 1.1300033330917358
Epoch 990, training loss: 0.6162769794464111 = 0.005760187283158302 + 0.1 * 6.105167865753174
Epoch 990, val loss: 1.1343004703521729
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6236
Flip ASR: 0.5467/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.765109062194824 = 1.927734136581421 + 0.1 * 8.373747825622559
Epoch 0, val loss: 1.9156700372695923
Epoch 10, training loss: 2.755755662918091 = 1.9184304475784302 + 0.1 * 8.373252868652344
Epoch 10, val loss: 1.906244158744812
Epoch 20, training loss: 2.7439746856689453 = 1.9069403409957886 + 0.1 * 8.370343208312988
Epoch 20, val loss: 1.8939820528030396
Epoch 30, training loss: 2.7263195514678955 = 1.8908071517944336 + 0.1 * 8.355124473571777
Epoch 30, val loss: 1.8762280941009521
Epoch 40, training loss: 2.6921157836914062 = 1.8671073913574219 + 0.1 * 8.250083923339844
Epoch 40, val loss: 1.8502142429351807
Epoch 50, training loss: 2.5737709999084473 = 1.837222695350647 + 0.1 * 7.36548376083374
Epoch 50, val loss: 1.8192120790481567
Epoch 60, training loss: 2.5103402137756348 = 1.8069102764129639 + 0.1 * 7.034299850463867
Epoch 60, val loss: 1.7895359992980957
Epoch 70, training loss: 2.455686569213867 = 1.773156762123108 + 0.1 * 6.825297832489014
Epoch 70, val loss: 1.7587045431137085
Epoch 80, training loss: 2.404005289077759 = 1.7359505891799927 + 0.1 * 6.680546283721924
Epoch 80, val loss: 1.7263813018798828
Epoch 90, training loss: 2.3504812717437744 = 1.6894502639770508 + 0.1 * 6.61030912399292
Epoch 90, val loss: 1.6868534088134766
Epoch 100, training loss: 2.2845280170440674 = 1.62660813331604 + 0.1 * 6.579199314117432
Epoch 100, val loss: 1.6340936422348022
Epoch 110, training loss: 2.2017531394958496 = 1.5455948114395142 + 0.1 * 6.561582088470459
Epoch 110, val loss: 1.5681818723678589
Epoch 120, training loss: 2.104797124862671 = 1.4499001502990723 + 0.1 * 6.5489702224731445
Epoch 120, val loss: 1.491904854774475
Epoch 130, training loss: 2.0014493465423584 = 1.3476277589797974 + 0.1 * 6.5382161140441895
Epoch 130, val loss: 1.4138463735580444
Epoch 140, training loss: 1.8986308574676514 = 1.2460541725158691 + 0.1 * 6.5257673263549805
Epoch 140, val loss: 1.3409411907196045
Epoch 150, training loss: 1.8002057075500488 = 1.1491316556930542 + 0.1 * 6.510740756988525
Epoch 150, val loss: 1.2742574214935303
Epoch 160, training loss: 1.707656741142273 = 1.0581592321395874 + 0.1 * 6.4949750900268555
Epoch 160, val loss: 1.21214759349823
Epoch 170, training loss: 1.6214410066604614 = 0.9733964800834656 + 0.1 * 6.48044490814209
Epoch 170, val loss: 1.153953194618225
Epoch 180, training loss: 1.5424401760101318 = 0.8953980207443237 + 0.1 * 6.470420837402344
Epoch 180, val loss: 1.0994534492492676
Epoch 190, training loss: 1.4697890281677246 = 0.8238256573677063 + 0.1 * 6.459632873535156
Epoch 190, val loss: 1.0487369298934937
Epoch 200, training loss: 1.4020105600357056 = 0.7568934559822083 + 0.1 * 6.451170921325684
Epoch 200, val loss: 1.0009840726852417
Epoch 210, training loss: 1.3380208015441895 = 0.694040834903717 + 0.1 * 6.439798831939697
Epoch 210, val loss: 0.9566210508346558
Epoch 220, training loss: 1.277498483657837 = 0.6344248056411743 + 0.1 * 6.430736064910889
Epoch 220, val loss: 0.9148346185684204
Epoch 230, training loss: 1.220059871673584 = 0.5778332352638245 + 0.1 * 6.422266960144043
Epoch 230, val loss: 0.8754590749740601
Epoch 240, training loss: 1.1672348976135254 = 0.5247675180435181 + 0.1 * 6.424673557281494
Epoch 240, val loss: 0.8390177488327026
Epoch 250, training loss: 1.1176578998565674 = 0.47673022747039795 + 0.1 * 6.409277439117432
Epoch 250, val loss: 0.8075951337814331
Epoch 260, training loss: 1.0733436346054077 = 0.433247834444046 + 0.1 * 6.400958061218262
Epoch 260, val loss: 0.7810044288635254
Epoch 270, training loss: 1.0331727266311646 = 0.39390286803245544 + 0.1 * 6.392698764801025
Epoch 270, val loss: 0.7590876221656799
Epoch 280, training loss: 0.9970628023147583 = 0.35842886567115784 + 0.1 * 6.38633918762207
Epoch 280, val loss: 0.7416177988052368
Epoch 290, training loss: 0.9647690057754517 = 0.3268270194530487 + 0.1 * 6.379419803619385
Epoch 290, val loss: 0.72779381275177
Epoch 300, training loss: 0.9354730844497681 = 0.2981124222278595 + 0.1 * 6.3736066818237305
Epoch 300, val loss: 0.7170650959014893
Epoch 310, training loss: 0.9085512757301331 = 0.27173125743865967 + 0.1 * 6.368200302124023
Epoch 310, val loss: 0.7087234854698181
Epoch 320, training loss: 0.8845728635787964 = 0.2473108172416687 + 0.1 * 6.372620105743408
Epoch 320, val loss: 0.7025020122528076
Epoch 330, training loss: 0.860767662525177 = 0.2247820347547531 + 0.1 * 6.359856128692627
Epoch 330, val loss: 0.6979755163192749
Epoch 340, training loss: 0.8392142653465271 = 0.203740656375885 + 0.1 * 6.354735851287842
Epoch 340, val loss: 0.6948085427284241
Epoch 350, training loss: 0.8188672065734863 = 0.1840423345565796 + 0.1 * 6.348248481750488
Epoch 350, val loss: 0.6929299235343933
Epoch 360, training loss: 0.7999674081802368 = 0.1656709611415863 + 0.1 * 6.3429646492004395
Epoch 360, val loss: 0.6920925974845886
Epoch 370, training loss: 0.7832512855529785 = 0.14885058999061584 + 0.1 * 6.3440070152282715
Epoch 370, val loss: 0.6923664212226868
Epoch 380, training loss: 0.7670710682868958 = 0.13369067013263702 + 0.1 * 6.333804130554199
Epoch 380, val loss: 0.6933843493461609
Epoch 390, training loss: 0.7529028058052063 = 0.12009561061859131 + 0.1 * 6.3280720710754395
Epoch 390, val loss: 0.6954834461212158
Epoch 400, training loss: 0.7430432438850403 = 0.10799436271190643 + 0.1 * 6.350488662719727
Epoch 400, val loss: 0.6984477043151855
Epoch 410, training loss: 0.7297632098197937 = 0.09738828241825104 + 0.1 * 6.32374906539917
Epoch 410, val loss: 0.7020741105079651
Epoch 420, training loss: 0.7193114757537842 = 0.08804040402173996 + 0.1 * 6.312710285186768
Epoch 420, val loss: 0.7063640356063843
Epoch 430, training loss: 0.71038818359375 = 0.07975494116544724 + 0.1 * 6.306332111358643
Epoch 430, val loss: 0.7111445665359497
Epoch 440, training loss: 0.7039883136749268 = 0.07239219546318054 + 0.1 * 6.315960884094238
Epoch 440, val loss: 0.7163437008857727
Epoch 450, training loss: 0.6957095861434937 = 0.06586822122335434 + 0.1 * 6.2984137535095215
Epoch 450, val loss: 0.7217739820480347
Epoch 460, training loss: 0.6889909505844116 = 0.06003449112176895 + 0.1 * 6.28956413269043
Epoch 460, val loss: 0.7274424433708191
Epoch 470, training loss: 0.6838767528533936 = 0.05481757968664169 + 0.1 * 6.290591716766357
Epoch 470, val loss: 0.7332302331924438
Epoch 480, training loss: 0.6785951256752014 = 0.05020894110202789 + 0.1 * 6.2838616371154785
Epoch 480, val loss: 0.7390796542167664
Epoch 490, training loss: 0.6745689511299133 = 0.046174705028533936 + 0.1 * 6.283942222595215
Epoch 490, val loss: 0.7449366450309753
Epoch 500, training loss: 0.6697978973388672 = 0.04262740537524223 + 0.1 * 6.27170467376709
Epoch 500, val loss: 0.7511181235313416
Epoch 510, training loss: 0.6689959168434143 = 0.03949121758341789 + 0.1 * 6.295046806335449
Epoch 510, val loss: 0.7575851678848267
Epoch 520, training loss: 0.6638829708099365 = 0.036718931049108505 + 0.1 * 6.271640777587891
Epoch 520, val loss: 0.7641079425811768
Epoch 530, training loss: 0.6600141525268555 = 0.034242209047079086 + 0.1 * 6.25771951675415
Epoch 530, val loss: 0.7705641984939575
Epoch 540, training loss: 0.6578982472419739 = 0.032015182077884674 + 0.1 * 6.2588300704956055
Epoch 540, val loss: 0.7769506573677063
Epoch 550, training loss: 0.6562550067901611 = 0.03001195192337036 + 0.1 * 6.262430191040039
Epoch 550, val loss: 0.7832611203193665
Epoch 560, training loss: 0.6526315212249756 = 0.028209378942847252 + 0.1 * 6.244221210479736
Epoch 560, val loss: 0.7895379066467285
Epoch 570, training loss: 0.6515569090843201 = 0.02657165750861168 + 0.1 * 6.249852657318115
Epoch 570, val loss: 0.795719563961029
Epoch 580, training loss: 0.6492600440979004 = 0.025080975145101547 + 0.1 * 6.241790771484375
Epoch 580, val loss: 0.8016794919967651
Epoch 590, training loss: 0.6475024819374084 = 0.023716358467936516 + 0.1 * 6.237861156463623
Epoch 590, val loss: 0.8076236844062805
Epoch 600, training loss: 0.6462072730064392 = 0.022466449066996574 + 0.1 * 6.237408638000488
Epoch 600, val loss: 0.8133757710456848
Epoch 610, training loss: 0.644309401512146 = 0.021320411935448647 + 0.1 * 6.229889869689941
Epoch 610, val loss: 0.8191259503364563
Epoch 620, training loss: 0.6431488394737244 = 0.02026301622390747 + 0.1 * 6.22885799407959
Epoch 620, val loss: 0.8247247934341431
Epoch 630, training loss: 0.6418129801750183 = 0.019285906106233597 + 0.1 * 6.2252702713012695
Epoch 630, val loss: 0.8301866054534912
Epoch 640, training loss: 0.6410911679267883 = 0.018382934853434563 + 0.1 * 6.227082252502441
Epoch 640, val loss: 0.8355331420898438
Epoch 650, training loss: 0.6391967535018921 = 0.017543189227581024 + 0.1 * 6.216535568237305
Epoch 650, val loss: 0.8407331109046936
Epoch 660, training loss: 0.6393941044807434 = 0.016764981672167778 + 0.1 * 6.226291179656982
Epoch 660, val loss: 0.8458495736122131
Epoch 670, training loss: 0.6375265121459961 = 0.016041090711951256 + 0.1 * 6.214853763580322
Epoch 670, val loss: 0.8508684635162354
Epoch 680, training loss: 0.6360278725624084 = 0.015367076732218266 + 0.1 * 6.206607818603516
Epoch 680, val loss: 0.8557304739952087
Epoch 690, training loss: 0.6346864700317383 = 0.014733836986124516 + 0.1 * 6.199525833129883
Epoch 690, val loss: 0.8605448007583618
Epoch 700, training loss: 0.6353253722190857 = 0.01413847878575325 + 0.1 * 6.211869239807129
Epoch 700, val loss: 0.8652872443199158
Epoch 710, training loss: 0.6340557336807251 = 0.013579180464148521 + 0.1 * 6.204765319824219
Epoch 710, val loss: 0.8698742985725403
Epoch 720, training loss: 0.6325471997261047 = 0.01305629126727581 + 0.1 * 6.19490909576416
Epoch 720, val loss: 0.8744133114814758
Epoch 730, training loss: 0.6323617696762085 = 0.012563600204885006 + 0.1 * 6.197981834411621
Epoch 730, val loss: 0.878868579864502
Epoch 740, training loss: 0.6310508251190186 = 0.012096958234906197 + 0.1 * 6.189538478851318
Epoch 740, val loss: 0.8831142783164978
Epoch 750, training loss: 0.6314653158187866 = 0.011655803769826889 + 0.1 * 6.198094844818115
Epoch 750, val loss: 0.8873573541641235
Epoch 760, training loss: 0.6301292777061462 = 0.011239586398005486 + 0.1 * 6.188896656036377
Epoch 760, val loss: 0.8914812803268433
Epoch 770, training loss: 0.629376232624054 = 0.010845993645489216 + 0.1 * 6.185302257537842
Epoch 770, val loss: 0.89555424451828
Epoch 780, training loss: 0.6295709013938904 = 0.010472907684743404 + 0.1 * 6.190979957580566
Epoch 780, val loss: 0.8994884490966797
Epoch 790, training loss: 0.6284439563751221 = 0.01011821161955595 + 0.1 * 6.183257579803467
Epoch 790, val loss: 0.9033321738243103
Epoch 800, training loss: 0.6273449063301086 = 0.00978313572704792 + 0.1 * 6.175617694854736
Epoch 800, val loss: 0.9072375297546387
Epoch 810, training loss: 0.6287391781806946 = 0.009465986862778664 + 0.1 * 6.192731857299805
Epoch 810, val loss: 0.9109551310539246
Epoch 820, training loss: 0.6264581680297852 = 0.00916536245495081 + 0.1 * 6.172928333282471
Epoch 820, val loss: 0.9145547151565552
Epoch 830, training loss: 0.626076877117157 = 0.008881055749952793 + 0.1 * 6.1719584465026855
Epoch 830, val loss: 0.9181601405143738
Epoch 840, training loss: 0.6261544823646545 = 0.008609403856098652 + 0.1 * 6.175450325012207
Epoch 840, val loss: 0.9215865731239319
Epoch 850, training loss: 0.6252304911613464 = 0.00835275836288929 + 0.1 * 6.1687774658203125
Epoch 850, val loss: 0.9250576496124268
Epoch 860, training loss: 0.624618411064148 = 0.008108507841825485 + 0.1 * 6.165099143981934
Epoch 860, val loss: 0.9284751415252686
Epoch 870, training loss: 0.6244240999221802 = 0.007875405251979828 + 0.1 * 6.165487289428711
Epoch 870, val loss: 0.9317981004714966
Epoch 880, training loss: 0.6246486902236938 = 0.007652637083083391 + 0.1 * 6.1699604988098145
Epoch 880, val loss: 0.9351054430007935
Epoch 890, training loss: 0.6233678460121155 = 0.0074399057775735855 + 0.1 * 6.1592793464660645
Epoch 890, val loss: 0.9384397268295288
Epoch 900, training loss: 0.6236943006515503 = 0.007237620186060667 + 0.1 * 6.164566993713379
Epoch 900, val loss: 0.9417091012001038
Epoch 910, training loss: 0.6241239309310913 = 0.007043343503028154 + 0.1 * 6.17080545425415
Epoch 910, val loss: 0.9449002742767334
Epoch 920, training loss: 0.6225475668907166 = 0.006857878994196653 + 0.1 * 6.156897068023682
Epoch 920, val loss: 0.9480175971984863
Epoch 930, training loss: 0.6225011944770813 = 0.0066810231655836105 + 0.1 * 6.158201694488525
Epoch 930, val loss: 0.9510819911956787
Epoch 940, training loss: 0.6216332316398621 = 0.006510615814477205 + 0.1 * 6.151226043701172
Epoch 940, val loss: 0.9541234970092773
Epoch 950, training loss: 0.6229810118675232 = 0.006347834598273039 + 0.1 * 6.166331768035889
Epoch 950, val loss: 0.9570847153663635
Epoch 960, training loss: 0.6208075284957886 = 0.006190945859998465 + 0.1 * 6.146165370941162
Epoch 960, val loss: 0.9601193070411682
Epoch 970, training loss: 0.6203618049621582 = 0.0060414099134504795 + 0.1 * 6.1432037353515625
Epoch 970, val loss: 0.9630618691444397
Epoch 980, training loss: 0.6201609969139099 = 0.005897733848541975 + 0.1 * 6.142632484436035
Epoch 980, val loss: 0.9659143686294556
Epoch 990, training loss: 0.6201669573783875 = 0.005759703926742077 + 0.1 * 6.14407205581665
Epoch 990, val loss: 0.9687213897705078
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.4391
Flip ASR: 0.3822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7902286052703857 = 1.9528512954711914 + 0.1 * 8.373773574829102
Epoch 0, val loss: 1.9578651189804077
Epoch 10, training loss: 2.779846429824829 = 1.9424992799758911 + 0.1 * 8.3734712600708
Epoch 10, val loss: 1.9478391408920288
Epoch 20, training loss: 2.766964912414551 = 1.9298102855682373 + 0.1 * 8.371545791625977
Epoch 20, val loss: 1.9350793361663818
Epoch 30, training loss: 2.747795343399048 = 1.912198543548584 + 0.1 * 8.35596752166748
Epoch 30, val loss: 1.9170035123825073
Epoch 40, training loss: 2.710554838180542 = 1.8865923881530762 + 0.1 * 8.239624977111816
Epoch 40, val loss: 1.890820860862732
Epoch 50, training loss: 2.6118342876434326 = 1.8538881540298462 + 0.1 * 7.579460620880127
Epoch 50, val loss: 1.8585736751556396
Epoch 60, training loss: 2.541689395904541 = 1.8199783563613892 + 0.1 * 7.217109203338623
Epoch 60, val loss: 1.8264273405075073
Epoch 70, training loss: 2.479795217514038 = 1.7849336862564087 + 0.1 * 6.948615550994873
Epoch 70, val loss: 1.7943105697631836
Epoch 80, training loss: 2.424910545349121 = 1.7490791082382202 + 0.1 * 6.758315086364746
Epoch 80, val loss: 1.7622617483139038
Epoch 90, training loss: 2.37031888961792 = 1.7057104110717773 + 0.1 * 6.646085262298584
Epoch 90, val loss: 1.7221392393112183
Epoch 100, training loss: 2.3058180809020996 = 1.6471765041351318 + 0.1 * 6.586416244506836
Epoch 100, val loss: 1.6686874628067017
Epoch 110, training loss: 2.224259853363037 = 1.5702078342437744 + 0.1 * 6.540520668029785
Epoch 110, val loss: 1.6020420789718628
Epoch 120, training loss: 2.1269290447235107 = 1.476328730583191 + 0.1 * 6.506002426147461
Epoch 120, val loss: 1.523978590965271
Epoch 130, training loss: 2.024176597595215 = 1.3756731748580933 + 0.1 * 6.48503303527832
Epoch 130, val loss: 1.443331241607666
Epoch 140, training loss: 1.9234418869018555 = 1.2765353918075562 + 0.1 * 6.469065189361572
Epoch 140, val loss: 1.3676040172576904
Epoch 150, training loss: 1.8288688659667969 = 1.1835392713546753 + 0.1 * 6.4532952308654785
Epoch 150, val loss: 1.2986353635787964
Epoch 160, training loss: 1.7430838346481323 = 1.0989247560501099 + 0.1 * 6.441590785980225
Epoch 160, val loss: 1.237113118171692
Epoch 170, training loss: 1.6674489974975586 = 1.0246037244796753 + 0.1 * 6.42845344543457
Epoch 170, val loss: 1.1846826076507568
Epoch 180, training loss: 1.6013367176055908 = 0.9592784643173218 + 0.1 * 6.420582294464111
Epoch 180, val loss: 1.1402831077575684
Epoch 190, training loss: 1.542752981185913 = 0.9017840027809143 + 0.1 * 6.409689426422119
Epoch 190, val loss: 1.1028801202774048
Epoch 200, training loss: 1.4885120391845703 = 0.848473072052002 + 0.1 * 6.400389671325684
Epoch 200, val loss: 1.0688573122024536
Epoch 210, training loss: 1.4357471466064453 = 0.7965799570083618 + 0.1 * 6.391672611236572
Epoch 210, val loss: 1.0358250141143799
Epoch 220, training loss: 1.3827033042907715 = 0.7442607879638672 + 0.1 * 6.384424686431885
Epoch 220, val loss: 1.0020989179611206
Epoch 230, training loss: 1.3286620378494263 = 0.6910368800163269 + 0.1 * 6.376251697540283
Epoch 230, val loss: 0.967661440372467
Epoch 240, training loss: 1.2741751670837402 = 0.6372031569480896 + 0.1 * 6.369720458984375
Epoch 240, val loss: 0.9327148199081421
Epoch 250, training loss: 1.2201544046401978 = 0.5830656886100769 + 0.1 * 6.370887279510498
Epoch 250, val loss: 0.8976740837097168
Epoch 260, training loss: 1.1657652854919434 = 0.5297681093215942 + 0.1 * 6.359971046447754
Epoch 260, val loss: 0.8629937767982483
Epoch 270, training loss: 1.1129518747329712 = 0.4779989421367645 + 0.1 * 6.349528789520264
Epoch 270, val loss: 0.8295327425003052
Epoch 280, training loss: 1.0621957778930664 = 0.42814359068870544 + 0.1 * 6.340522289276123
Epoch 280, val loss: 0.7981888651847839
Epoch 290, training loss: 1.0152318477630615 = 0.3809654712677002 + 0.1 * 6.342663764953613
Epoch 290, val loss: 0.7699955701828003
Epoch 300, training loss: 0.9710680246353149 = 0.33759117126464844 + 0.1 * 6.334768295288086
Epoch 300, val loss: 0.7464157938957214
Epoch 310, training loss: 0.930607795715332 = 0.2981708347797394 + 0.1 * 6.32436990737915
Epoch 310, val loss: 0.7281067371368408
Epoch 320, training loss: 0.8940390348434448 = 0.26284319162368774 + 0.1 * 6.311958312988281
Epoch 320, val loss: 0.7148017883300781
Epoch 330, training loss: 0.8645902276039124 = 0.23217235505580902 + 0.1 * 6.324178695678711
Epoch 330, val loss: 0.7059435844421387
Epoch 340, training loss: 0.8357362747192383 = 0.20608319342136383 + 0.1 * 6.296530723571777
Epoch 340, val loss: 0.7012366652488708
Epoch 350, training loss: 0.8142321109771729 = 0.18382403254508972 + 0.1 * 6.304080963134766
Epoch 350, val loss: 0.6999430060386658
Epoch 360, training loss: 0.7932999134063721 = 0.16487063467502594 + 0.1 * 6.284292697906494
Epoch 360, val loss: 0.7012414336204529
Epoch 370, training loss: 0.7764224410057068 = 0.14852704107761383 + 0.1 * 6.278954029083252
Epoch 370, val loss: 0.7045722603797913
Epoch 380, training loss: 0.7630789279937744 = 0.13431094586849213 + 0.1 * 6.287679672241211
Epoch 380, val loss: 0.7095633745193481
Epoch 390, training loss: 0.7486959099769592 = 0.12194395065307617 + 0.1 * 6.267519474029541
Epoch 390, val loss: 0.715787410736084
Epoch 400, training loss: 0.7381014227867126 = 0.11108624190092087 + 0.1 * 6.270151615142822
Epoch 400, val loss: 0.7230039834976196
Epoch 410, training loss: 0.7284311652183533 = 0.10152013599872589 + 0.1 * 6.269110202789307
Epoch 410, val loss: 0.7310358285903931
Epoch 420, training loss: 0.7185357213020325 = 0.09305299073457718 + 0.1 * 6.25482702255249
Epoch 420, val loss: 0.7396479845046997
Epoch 430, training loss: 0.7101401090621948 = 0.08550683408975601 + 0.1 * 6.24633264541626
Epoch 430, val loss: 0.748897135257721
Epoch 440, training loss: 0.7045493721961975 = 0.07876589149236679 + 0.1 * 6.2578349113464355
Epoch 440, val loss: 0.7584041357040405
Epoch 450, training loss: 0.6979184150695801 = 0.0727439746260643 + 0.1 * 6.251744747161865
Epoch 450, val loss: 0.768286406993866
Epoch 460, training loss: 0.6908180117607117 = 0.06732919067144394 + 0.1 * 6.234888553619385
Epoch 460, val loss: 0.7783230543136597
Epoch 470, training loss: 0.6857241988182068 = 0.062443483620882034 + 0.1 * 6.23280668258667
Epoch 470, val loss: 0.7884573340415955
Epoch 480, training loss: 0.6807587146759033 = 0.058030083775520325 + 0.1 * 6.227286338806152
Epoch 480, val loss: 0.7986298203468323
Epoch 490, training loss: 0.6767275929450989 = 0.05403002351522446 + 0.1 * 6.226975917816162
Epoch 490, val loss: 0.8087558746337891
Epoch 500, training loss: 0.674906313419342 = 0.05040210857987404 + 0.1 * 6.245041847229004
Epoch 500, val loss: 0.8187944889068604
Epoch 510, training loss: 0.6685853004455566 = 0.04710916802287102 + 0.1 * 6.214760780334473
Epoch 510, val loss: 0.8287752270698547
Epoch 520, training loss: 0.6667318940162659 = 0.04410321265459061 + 0.1 * 6.226286888122559
Epoch 520, val loss: 0.8386505246162415
Epoch 530, training loss: 0.6622276902198792 = 0.04136122390627861 + 0.1 * 6.208664417266846
Epoch 530, val loss: 0.848430335521698
Epoch 540, training loss: 0.6599181890487671 = 0.03884980082511902 + 0.1 * 6.210684299468994
Epoch 540, val loss: 0.8580997586250305
Epoch 550, training loss: 0.6564713716506958 = 0.03654531389474869 + 0.1 * 6.199260234832764
Epoch 550, val loss: 0.867569625377655
Epoch 560, training loss: 0.6544938087463379 = 0.03443382680416107 + 0.1 * 6.200599670410156
Epoch 560, val loss: 0.876976728439331
Epoch 570, training loss: 0.6515451669692993 = 0.032487958669662476 + 0.1 * 6.190572261810303
Epoch 570, val loss: 0.8861644864082336
Epoch 580, training loss: 0.6491711735725403 = 0.03069309890270233 + 0.1 * 6.184780597686768
Epoch 580, val loss: 0.8953184485435486
Epoch 590, training loss: 0.6486169099807739 = 0.02902994491159916 + 0.1 * 6.195869445800781
Epoch 590, val loss: 0.9042434096336365
Epoch 600, training loss: 0.6456230282783508 = 0.027493301779031754 + 0.1 * 6.181297302246094
Epoch 600, val loss: 0.9129878878593445
Epoch 610, training loss: 0.6458911895751953 = 0.02607632614672184 + 0.1 * 6.198148727416992
Epoch 610, val loss: 0.9216861128807068
Epoch 620, training loss: 0.6423478722572327 = 0.024763045832514763 + 0.1 * 6.175848007202148
Epoch 620, val loss: 0.9301294684410095
Epoch 630, training loss: 0.6405398845672607 = 0.023545175790786743 + 0.1 * 6.169947147369385
Epoch 630, val loss: 0.9385464191436768
Epoch 640, training loss: 0.6390921473503113 = 0.02240968495607376 + 0.1 * 6.166824817657471
Epoch 640, val loss: 0.9466458559036255
Epoch 650, training loss: 0.6375523209571838 = 0.021356165409088135 + 0.1 * 6.161961555480957
Epoch 650, val loss: 0.954846203327179
Epoch 660, training loss: 0.637416660785675 = 0.020371172577142715 + 0.1 * 6.170454502105713
Epoch 660, val loss: 0.9627728462219238
Epoch 670, training loss: 0.635226309299469 = 0.019452925771474838 + 0.1 * 6.15773344039917
Epoch 670, val loss: 0.9705613851547241
Epoch 680, training loss: 0.6344582438468933 = 0.018595680594444275 + 0.1 * 6.158625602722168
Epoch 680, val loss: 0.9782922863960266
Epoch 690, training loss: 0.6334277391433716 = 0.01779274456202984 + 0.1 * 6.1563496589660645
Epoch 690, val loss: 0.9857407212257385
Epoch 700, training loss: 0.6323395371437073 = 0.017041904851794243 + 0.1 * 6.152976036071777
Epoch 700, val loss: 0.993134081363678
Epoch 710, training loss: 0.6309517621994019 = 0.016338659450411797 + 0.1 * 6.1461310386657715
Epoch 710, val loss: 1.0004351139068604
Epoch 720, training loss: 0.6314978003501892 = 0.01567721553146839 + 0.1 * 6.158205509185791
Epoch 720, val loss: 1.0075483322143555
Epoch 730, training loss: 0.6298861503601074 = 0.015056891366839409 + 0.1 * 6.148292064666748
Epoch 730, val loss: 1.0145710706710815
Epoch 740, training loss: 0.6279791593551636 = 0.014473319053649902 + 0.1 * 6.135058403015137
Epoch 740, val loss: 1.021433711051941
Epoch 750, training loss: 0.6284785270690918 = 0.013923954218626022 + 0.1 * 6.145545482635498
Epoch 750, val loss: 1.0282114744186401
Epoch 760, training loss: 0.6281651854515076 = 0.013405736535787582 + 0.1 * 6.147594451904297
Epoch 760, val loss: 1.0347256660461426
Epoch 770, training loss: 0.6263662576675415 = 0.012918376363813877 + 0.1 * 6.13447904586792
Epoch 770, val loss: 1.041345477104187
Epoch 780, training loss: 0.6274346113204956 = 0.012457450851798058 + 0.1 * 6.149771690368652
Epoch 780, val loss: 1.0477045774459839
Epoch 790, training loss: 0.6256710290908813 = 0.012021619826555252 + 0.1 * 6.136494159698486
Epoch 790, val loss: 1.0538579225540161
Epoch 800, training loss: 0.6245993971824646 = 0.011610162444412708 + 0.1 * 6.129892349243164
Epoch 800, val loss: 1.0601067543029785
Epoch 810, training loss: 0.6248736381530762 = 0.011219123378396034 + 0.1 * 6.136544704437256
Epoch 810, val loss: 1.0661002397537231
Epoch 820, training loss: 0.623066246509552 = 0.010848626494407654 + 0.1 * 6.122176170349121
Epoch 820, val loss: 1.0720865726470947
Epoch 830, training loss: 0.623399555683136 = 0.010496556758880615 + 0.1 * 6.129029750823975
Epoch 830, val loss: 1.077891230583191
Epoch 840, training loss: 0.6224629878997803 = 0.010162321850657463 + 0.1 * 6.123006820678711
Epoch 840, val loss: 1.0836198329925537
Epoch 850, training loss: 0.6237273216247559 = 0.009844977408647537 + 0.1 * 6.138823509216309
Epoch 850, val loss: 1.0892682075500488
Epoch 860, training loss: 0.6221671104431152 = 0.009542330168187618 + 0.1 * 6.126247406005859
Epoch 860, val loss: 1.094695806503296
Epoch 870, training loss: 0.6208459138870239 = 0.00925513170659542 + 0.1 * 6.115908145904541
Epoch 870, val loss: 1.1001640558242798
Epoch 880, training loss: 0.6216282844543457 = 0.008981498889625072 + 0.1 * 6.126467227935791
Epoch 880, val loss: 1.1055116653442383
Epoch 890, training loss: 0.6203078627586365 = 0.008720554411411285 + 0.1 * 6.115872859954834
Epoch 890, val loss: 1.1107522249221802
Epoch 900, training loss: 0.6201523542404175 = 0.008472112938761711 + 0.1 * 6.11680269241333
Epoch 900, val loss: 1.1159878969192505
Epoch 910, training loss: 0.6202732920646667 = 0.008234233595430851 + 0.1 * 6.12039041519165
Epoch 910, val loss: 1.1210463047027588
Epoch 920, training loss: 0.6192507147789001 = 0.008007281459867954 + 0.1 * 6.112433910369873
Epoch 920, val loss: 1.1261382102966309
Epoch 930, training loss: 0.6206469535827637 = 0.007790311239659786 + 0.1 * 6.128566265106201
Epoch 930, val loss: 1.1311136484146118
Epoch 940, training loss: 0.6186628937721252 = 0.00758239533752203 + 0.1 * 6.110805034637451
Epoch 940, val loss: 1.135871410369873
Epoch 950, training loss: 0.6179168224334717 = 0.0073846192099153996 + 0.1 * 6.105321884155273
Epoch 950, val loss: 1.1408462524414062
Epoch 960, training loss: 0.6184085011482239 = 0.0071945711970329285 + 0.1 * 6.1121392250061035
Epoch 960, val loss: 1.1455891132354736
Epoch 970, training loss: 0.6174432635307312 = 0.007012071553617716 + 0.1 * 6.104311943054199
Epoch 970, val loss: 1.1501632928848267
Epoch 980, training loss: 0.618066132068634 = 0.006837222259491682 + 0.1 * 6.112288951873779
Epoch 980, val loss: 1.1547741889953613
Epoch 990, training loss: 0.6168175935745239 = 0.006669121328741312 + 0.1 * 6.101484775543213
Epoch 990, val loss: 1.1592425107955933
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8007
Flip ASR: 0.7644/225 nodes
The final ASR:0.62116, 0.14764, Accuracy:0.82469, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9466])
updated graph: torch.Size([2, 10508])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00758, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7651946544647217 = 1.9278076887130737 + 0.1 * 8.373868942260742
Epoch 0, val loss: 1.9241172075271606
Epoch 10, training loss: 2.7554075717926025 = 1.9180374145507812 + 0.1 * 8.373702049255371
Epoch 10, val loss: 1.915086030960083
Epoch 20, training loss: 2.743316888809204 = 1.9060373306274414 + 0.1 * 8.372795104980469
Epoch 20, val loss: 1.9036575555801392
Epoch 30, training loss: 2.7259066104888916 = 1.889324426651001 + 0.1 * 8.36582088470459
Epoch 30, val loss: 1.8873909711837769
Epoch 40, training loss: 2.696502685546875 = 1.8649979829788208 + 0.1 * 8.315047264099121
Epoch 40, val loss: 1.864009976387024
Epoch 50, training loss: 2.626941204071045 = 1.8325644731521606 + 0.1 * 7.943768501281738
Epoch 50, val loss: 1.8342139720916748
Epoch 60, training loss: 2.5508503913879395 = 1.797778844833374 + 0.1 * 7.530716419219971
Epoch 60, val loss: 1.8043032884597778
Epoch 70, training loss: 2.4784791469573975 = 1.7652570009231567 + 0.1 * 7.1322221755981445
Epoch 70, val loss: 1.7779953479766846
Epoch 80, training loss: 2.4167683124542236 = 1.7293375730514526 + 0.1 * 6.874307632446289
Epoch 80, val loss: 1.7480717897415161
Epoch 90, training loss: 2.356509208679199 = 1.6783615350723267 + 0.1 * 6.781476020812988
Epoch 90, val loss: 1.7043129205703735
Epoch 100, training loss: 2.282339096069336 = 1.6076469421386719 + 0.1 * 6.746922016143799
Epoch 100, val loss: 1.6453429460525513
Epoch 110, training loss: 2.1890616416931152 = 1.5167529582977295 + 0.1 * 6.723085880279541
Epoch 110, val loss: 1.5719290971755981
Epoch 120, training loss: 2.081148624420166 = 1.411159873008728 + 0.1 * 6.699887275695801
Epoch 120, val loss: 1.4875174760818481
Epoch 130, training loss: 1.9681322574615479 = 1.3004552125930786 + 0.1 * 6.6767706871032715
Epoch 130, val loss: 1.4009878635406494
Epoch 140, training loss: 1.8582406044006348 = 1.1930081844329834 + 0.1 * 6.652324676513672
Epoch 140, val loss: 1.3193727731704712
Epoch 150, training loss: 1.758797287940979 = 1.0957216024398804 + 0.1 * 6.630756855010986
Epoch 150, val loss: 1.247341275215149
Epoch 160, training loss: 1.6749122142791748 = 1.0132986307144165 + 0.1 * 6.616135597229004
Epoch 160, val loss: 1.1885216236114502
Epoch 170, training loss: 1.6041197776794434 = 0.9439880847930908 + 0.1 * 6.601317405700684
Epoch 170, val loss: 1.1403802633285522
Epoch 180, training loss: 1.540900707244873 = 0.882310688495636 + 0.1 * 6.58590030670166
Epoch 180, val loss: 1.0983210802078247
Epoch 190, training loss: 1.4801669120788574 = 0.8228339552879333 + 0.1 * 6.573328971862793
Epoch 190, val loss: 1.0571492910385132
Epoch 200, training loss: 1.4185259342193604 = 0.762775719165802 + 0.1 * 6.557501316070557
Epoch 200, val loss: 1.0146511793136597
Epoch 210, training loss: 1.3551979064941406 = 0.7008875012397766 + 0.1 * 6.543103218078613
Epoch 210, val loss: 0.9692821502685547
Epoch 220, training loss: 1.291017770767212 = 0.6380353569984436 + 0.1 * 6.529823303222656
Epoch 220, val loss: 0.9223514199256897
Epoch 230, training loss: 1.228834867477417 = 0.5766987204551697 + 0.1 * 6.521361827850342
Epoch 230, val loss: 0.8769708871841431
Epoch 240, training loss: 1.1705467700958252 = 0.5191996693611145 + 0.1 * 6.513470649719238
Epoch 240, val loss: 0.8362395763397217
Epoch 250, training loss: 1.1159462928771973 = 0.46548736095428467 + 0.1 * 6.504590034484863
Epoch 250, val loss: 0.8005166053771973
Epoch 260, training loss: 1.0650964975357056 = 0.4153331220149994 + 0.1 * 6.497633934020996
Epoch 260, val loss: 0.7700571417808533
Epoch 270, training loss: 1.0179609060287476 = 0.3683381974697113 + 0.1 * 6.496226787567139
Epoch 270, val loss: 0.7444755434989929
Epoch 280, training loss: 0.973700761795044 = 0.324644535779953 + 0.1 * 6.490562438964844
Epoch 280, val loss: 0.7236740589141846
Epoch 290, training loss: 0.9340943098068237 = 0.2846006453037262 + 0.1 * 6.494935989379883
Epoch 290, val loss: 0.7077338695526123
Epoch 300, training loss: 0.8973090648651123 = 0.24898196756839752 + 0.1 * 6.48327112197876
Epoch 300, val loss: 0.6967496871948242
Epoch 310, training loss: 0.866385817527771 = 0.2178104966878891 + 0.1 * 6.485753059387207
Epoch 310, val loss: 0.6902303099632263
Epoch 320, training loss: 0.8384227156639099 = 0.19105452299118042 + 0.1 * 6.473681926727295
Epoch 320, val loss: 0.6879215240478516
Epoch 330, training loss: 0.81495201587677 = 0.16818447411060333 + 0.1 * 6.467675685882568
Epoch 330, val loss: 0.6890383958816528
Epoch 340, training loss: 0.7948254346847534 = 0.1486484557390213 + 0.1 * 6.461770057678223
Epoch 340, val loss: 0.6929164528846741
Epoch 350, training loss: 0.7786322832107544 = 0.13193140923976898 + 0.1 * 6.467008590698242
Epoch 350, val loss: 0.6989179253578186
Epoch 360, training loss: 0.7630951404571533 = 0.11763051152229309 + 0.1 * 6.454646110534668
Epoch 360, val loss: 0.7065709233283997
Epoch 370, training loss: 0.7499449253082275 = 0.10525776445865631 + 0.1 * 6.446871757507324
Epoch 370, val loss: 0.7153013944625854
Epoch 380, training loss: 0.7387340068817139 = 0.0944765955209732 + 0.1 * 6.4425740242004395
Epoch 380, val loss: 0.7248605489730835
Epoch 390, training loss: 0.7290505170822144 = 0.08505937457084656 + 0.1 * 6.439910888671875
Epoch 390, val loss: 0.7350152730941772
Epoch 400, training loss: 0.7198574542999268 = 0.07683893293142319 + 0.1 * 6.430185317993164
Epoch 400, val loss: 0.7454204559326172
Epoch 410, training loss: 0.7114367485046387 = 0.06960881501436234 + 0.1 * 6.41827917098999
Epoch 410, val loss: 0.7560634016990662
Epoch 420, training loss: 0.7057358026504517 = 0.06321806460618973 + 0.1 * 6.425177097320557
Epoch 420, val loss: 0.7668432593345642
Epoch 430, training loss: 0.6982119679450989 = 0.0575820617377758 + 0.1 * 6.406298637390137
Epoch 430, val loss: 0.7776649594306946
Epoch 440, training loss: 0.6921531558036804 = 0.052589692175388336 + 0.1 * 6.395634174346924
Epoch 440, val loss: 0.7884155511856079
Epoch 450, training loss: 0.6869720220565796 = 0.04816119372844696 + 0.1 * 6.388107776641846
Epoch 450, val loss: 0.7990736365318298
Epoch 460, training loss: 0.6821670532226562 = 0.04422518238425255 + 0.1 * 6.379418849945068
Epoch 460, val loss: 0.8096859455108643
Epoch 470, training loss: 0.6804118156433105 = 0.04073335975408554 + 0.1 * 6.396784782409668
Epoch 470, val loss: 0.8198420405387878
Epoch 480, training loss: 0.6746469140052795 = 0.037639912217855453 + 0.1 * 6.370069980621338
Epoch 480, val loss: 0.8299764394760132
Epoch 490, training loss: 0.6707227826118469 = 0.034870754927396774 + 0.1 * 6.358520030975342
Epoch 490, val loss: 0.8398113250732422
Epoch 500, training loss: 0.6703023314476013 = 0.03238390013575554 + 0.1 * 6.379183769226074
Epoch 500, val loss: 0.8493557572364807
Epoch 510, training loss: 0.6657458543777466 = 0.03015582449734211 + 0.1 * 6.355899810791016
Epoch 510, val loss: 0.8587940335273743
Epoch 520, training loss: 0.6624518036842346 = 0.028145214542746544 + 0.1 * 6.3430657386779785
Epoch 520, val loss: 0.8679760694503784
Epoch 530, training loss: 0.6615756154060364 = 0.026324521750211716 + 0.1 * 6.352510929107666
Epoch 530, val loss: 0.87692791223526
Epoch 540, training loss: 0.6584345102310181 = 0.024677682667970657 + 0.1 * 6.3375678062438965
Epoch 540, val loss: 0.8857225179672241
Epoch 550, training loss: 0.6575243473052979 = 0.023179542273283005 + 0.1 * 6.343448162078857
Epoch 550, val loss: 0.894307017326355
Epoch 560, training loss: 0.6541141271591187 = 0.021815989166498184 + 0.1 * 6.322980880737305
Epoch 560, val loss: 0.9026076793670654
Epoch 570, training loss: 0.6528632640838623 = 0.020571794360876083 + 0.1 * 6.3229146003723145
Epoch 570, val loss: 0.9108224511146545
Epoch 580, training loss: 0.6517331004142761 = 0.01943109929561615 + 0.1 * 6.323019981384277
Epoch 580, val loss: 0.9188084006309509
Epoch 590, training loss: 0.6499724984169006 = 0.018385417759418488 + 0.1 * 6.315870761871338
Epoch 590, val loss: 0.9265734553337097
Epoch 600, training loss: 0.6497898101806641 = 0.017424819990992546 + 0.1 * 6.323649883270264
Epoch 600, val loss: 0.9341400265693665
Epoch 610, training loss: 0.6472427248954773 = 0.016542864963412285 + 0.1 * 6.3069987297058105
Epoch 610, val loss: 0.9415871500968933
Epoch 620, training loss: 0.6462266445159912 = 0.01572820171713829 + 0.1 * 6.3049845695495605
Epoch 620, val loss: 0.9488567113876343
Epoch 630, training loss: 0.644996702671051 = 0.014975572004914284 + 0.1 * 6.300211429595947
Epoch 630, val loss: 0.955810546875
Epoch 640, training loss: 0.6440351605415344 = 0.014280206523835659 + 0.1 * 6.297549247741699
Epoch 640, val loss: 0.9628294706344604
Epoch 650, training loss: 0.6448892951011658 = 0.013634322211146355 + 0.1 * 6.312549591064453
Epoch 650, val loss: 0.9695696830749512
Epoch 660, training loss: 0.6425039768218994 = 0.013034377247095108 + 0.1 * 6.2946953773498535
Epoch 660, val loss: 0.9760591983795166
Epoch 670, training loss: 0.6405554413795471 = 0.012476861476898193 + 0.1 * 6.28078556060791
Epoch 670, val loss: 0.9825655221939087
Epoch 680, training loss: 0.6414676904678345 = 0.011955182999372482 + 0.1 * 6.2951250076293945
Epoch 680, val loss: 0.9887812733650208
Epoch 690, training loss: 0.6395674347877502 = 0.011469149962067604 + 0.1 * 6.280982971191406
Epoch 690, val loss: 0.9949204921722412
Epoch 700, training loss: 0.638150155544281 = 0.011013924144208431 + 0.1 * 6.2713623046875
Epoch 700, val loss: 1.0008975267410278
Epoch 710, training loss: 0.6382922530174255 = 0.010588156059384346 + 0.1 * 6.277040958404541
Epoch 710, val loss: 1.006805419921875
Epoch 720, training loss: 0.6368104815483093 = 0.01018917839974165 + 0.1 * 6.2662129402160645
Epoch 720, val loss: 1.0125417709350586
Epoch 730, training loss: 0.6376697421073914 = 0.009813863784074783 + 0.1 * 6.278558731079102
Epoch 730, val loss: 1.0182266235351562
Epoch 740, training loss: 0.6361410617828369 = 0.009459865279495716 + 0.1 * 6.266811847686768
Epoch 740, val loss: 1.02354896068573
Epoch 750, training loss: 0.6343225836753845 = 0.009128272533416748 + 0.1 * 6.251943111419678
Epoch 750, val loss: 1.0290950536727905
Epoch 760, training loss: 0.6374443769454956 = 0.008814540691673756 + 0.1 * 6.2862982749938965
Epoch 760, val loss: 1.0342904329299927
Epoch 770, training loss: 0.6337409019470215 = 0.00851853284984827 + 0.1 * 6.252223491668701
Epoch 770, val loss: 1.0392862558364868
Epoch 780, training loss: 0.6324569582939148 = 0.008240078575909138 + 0.1 * 6.242168426513672
Epoch 780, val loss: 1.0444443225860596
Epoch 790, training loss: 0.6342871785163879 = 0.007976099848747253 + 0.1 * 6.263110637664795
Epoch 790, val loss: 1.0493550300598145
Epoch 800, training loss: 0.6321578025817871 = 0.0077245584689080715 + 0.1 * 6.2443318367004395
Epoch 800, val loss: 1.0540586709976196
Epoch 810, training loss: 0.6306590437889099 = 0.007487766910344362 + 0.1 * 6.231712341308594
Epoch 810, val loss: 1.0589303970336914
Epoch 820, training loss: 0.6319790482521057 = 0.007262432482093573 + 0.1 * 6.247166156768799
Epoch 820, val loss: 1.063591718673706
Epoch 830, training loss: 0.6307608485221863 = 0.007047497201710939 + 0.1 * 6.237133502960205
Epoch 830, val loss: 1.068138599395752
Epoch 840, training loss: 0.6319741010665894 = 0.006843480281531811 + 0.1 * 6.251306056976318
Epoch 840, val loss: 1.0725953578948975
Epoch 850, training loss: 0.6293683648109436 = 0.006649272050708532 + 0.1 * 6.227190971374512
Epoch 850, val loss: 1.0768533945083618
Epoch 860, training loss: 0.6288245320320129 = 0.006464882288128138 + 0.1 * 6.223596096038818
Epoch 860, val loss: 1.0813236236572266
Epoch 870, training loss: 0.6289148926734924 = 0.0062888613902032375 + 0.1 * 6.226260185241699
Epoch 870, val loss: 1.085446834564209
Epoch 880, training loss: 0.6275990009307861 = 0.006120364181697369 + 0.1 * 6.214786052703857
Epoch 880, val loss: 1.0895462036132812
Epoch 890, training loss: 0.629124104976654 = 0.005959261674433947 + 0.1 * 6.2316484451293945
Epoch 890, val loss: 1.093489170074463
Epoch 900, training loss: 0.627633810043335 = 0.005805579479783773 + 0.1 * 6.218282222747803
Epoch 900, val loss: 1.0973641872406006
Epoch 910, training loss: 0.6262511014938354 = 0.005659447517246008 + 0.1 * 6.205916404724121
Epoch 910, val loss: 1.1013449430465698
Epoch 920, training loss: 0.6267167925834656 = 0.005519140046089888 + 0.1 * 6.211976528167725
Epoch 920, val loss: 1.1052271127700806
Epoch 930, training loss: 0.6268419027328491 = 0.005384177900850773 + 0.1 * 6.214576721191406
Epoch 930, val loss: 1.1088765859603882
Epoch 940, training loss: 0.6252050995826721 = 0.005255057476460934 + 0.1 * 6.199500560760498
Epoch 940, val loss: 1.1125047206878662
Epoch 950, training loss: 0.6254289746284485 = 0.0051315645687282085 + 0.1 * 6.202974319458008
Epoch 950, val loss: 1.116182804107666
Epoch 960, training loss: 0.6251501441001892 = 0.005012554582208395 + 0.1 * 6.201375484466553
Epoch 960, val loss: 1.1196444034576416
Epoch 970, training loss: 0.6251558065414429 = 0.004898387007415295 + 0.1 * 6.202574253082275
Epoch 970, val loss: 1.1231329441070557
Epoch 980, training loss: 0.6235002875328064 = 0.004789087921380997 + 0.1 * 6.1871113777160645
Epoch 980, val loss: 1.1265791654586792
Epoch 990, training loss: 0.62655109167099 = 0.004683853592723608 + 0.1 * 6.218672275543213
Epoch 990, val loss: 1.1299649477005005
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6716
Flip ASR: 0.6133/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7851734161376953 = 1.9477852582931519 + 0.1 * 8.373880386352539
Epoch 0, val loss: 1.9488215446472168
Epoch 10, training loss: 2.7751803398132324 = 1.937803864479065 + 0.1 * 8.373764991760254
Epoch 10, val loss: 1.9383949041366577
Epoch 20, training loss: 2.7634823322296143 = 1.926175594329834 + 0.1 * 8.373067855834961
Epoch 20, val loss: 1.9257794618606567
Epoch 30, training loss: 2.747345447540283 = 1.9105287790298462 + 0.1 * 8.368165969848633
Epoch 30, val loss: 1.9086785316467285
Epoch 40, training loss: 2.721198320388794 = 1.8880027532577515 + 0.1 * 8.331954956054688
Epoch 40, val loss: 1.8841397762298584
Epoch 50, training loss: 2.6564955711364746 = 1.8566040992736816 + 0.1 * 7.998913764953613
Epoch 50, val loss: 1.8512537479400635
Epoch 60, training loss: 2.5582263469696045 = 1.821991205215454 + 0.1 * 7.362350940704346
Epoch 60, val loss: 1.8169004917144775
Epoch 70, training loss: 2.494349241256714 = 1.7869454622268677 + 0.1 * 7.074038505554199
Epoch 70, val loss: 1.7834128141403198
Epoch 80, training loss: 2.4358458518981934 = 1.7494044303894043 + 0.1 * 6.864415168762207
Epoch 80, val loss: 1.749385118484497
Epoch 90, training loss: 2.381568431854248 = 1.707585096359253 + 0.1 * 6.739832401275635
Epoch 90, val loss: 1.7123855352401733
Epoch 100, training loss: 2.3209450244903564 = 1.6524547338485718 + 0.1 * 6.684902191162109
Epoch 100, val loss: 1.6638652086257935
Epoch 110, training loss: 2.24465274810791 = 1.5787701606750488 + 0.1 * 6.658824443817139
Epoch 110, val loss: 1.6010628938674927
Epoch 120, training loss: 2.1482295989990234 = 1.483852505683899 + 0.1 * 6.643771648406982
Epoch 120, val loss: 1.5214875936508179
Epoch 130, training loss: 2.034569501876831 = 1.3708726167678833 + 0.1 * 6.636968612670898
Epoch 130, val loss: 1.4283593893051147
Epoch 140, training loss: 1.9142379760742188 = 1.2509056329727173 + 0.1 * 6.633324146270752
Epoch 140, val loss: 1.3315067291259766
Epoch 150, training loss: 1.7980854511260986 = 1.1352804899215698 + 0.1 * 6.628048896789551
Epoch 150, val loss: 1.2398481369018555
Epoch 160, training loss: 1.693181037902832 = 1.0312482118606567 + 0.1 * 6.61932897567749
Epoch 160, val loss: 1.159346103668213
Epoch 170, training loss: 1.6019513607025146 = 0.9409703016281128 + 0.1 * 6.609810829162598
Epoch 170, val loss: 1.0911880731582642
Epoch 180, training loss: 1.5225505828857422 = 0.8627511858940125 + 0.1 * 6.59799337387085
Epoch 180, val loss: 1.0336030721664429
Epoch 190, training loss: 1.4503203630447388 = 0.7916614413261414 + 0.1 * 6.586589336395264
Epoch 190, val loss: 0.9818090200424194
Epoch 200, training loss: 1.3827385902404785 = 0.7246856093406677 + 0.1 * 6.58052921295166
Epoch 200, val loss: 0.932783842086792
Epoch 210, training loss: 1.3184819221496582 = 0.6618728637695312 + 0.1 * 6.566090106964111
Epoch 210, val loss: 0.8866665959358215
Epoch 220, training loss: 1.2578699588775635 = 0.6024600863456726 + 0.1 * 6.554099082946777
Epoch 220, val loss: 0.8429093360900879
Epoch 230, training loss: 1.2006926536560059 = 0.5464434027671814 + 0.1 * 6.542492866516113
Epoch 230, val loss: 0.8020886778831482
Epoch 240, training loss: 1.148569107055664 = 0.49378225207328796 + 0.1 * 6.547868251800537
Epoch 240, val loss: 0.7649074792861938
Epoch 250, training loss: 1.0975744724273682 = 0.444876492023468 + 0.1 * 6.526979446411133
Epoch 250, val loss: 0.7319875359535217
Epoch 260, training loss: 1.0503753423690796 = 0.3988379240036011 + 0.1 * 6.515374183654785
Epoch 260, val loss: 0.7025696635246277
Epoch 270, training loss: 1.0064740180969238 = 0.3554420471191406 + 0.1 * 6.510319709777832
Epoch 270, val loss: 0.6768080592155457
Epoch 280, training loss: 0.9651254415512085 = 0.3150489926338196 + 0.1 * 6.5007643699646
Epoch 280, val loss: 0.6548485159873962
Epoch 290, training loss: 0.9267473220825195 = 0.2775951325893402 + 0.1 * 6.49152135848999
Epoch 290, val loss: 0.6365712881088257
Epoch 300, training loss: 0.8923803567886353 = 0.24330320954322815 + 0.1 * 6.490771770477295
Epoch 300, val loss: 0.6218854188919067
Epoch 310, training loss: 0.8612090349197388 = 0.21266311407089233 + 0.1 * 6.485459327697754
Epoch 310, val loss: 0.610868513584137
Epoch 320, training loss: 0.8329271674156189 = 0.1855417937040329 + 0.1 * 6.473853588104248
Epoch 320, val loss: 0.6031558513641357
Epoch 330, training loss: 0.8085761070251465 = 0.16175109148025513 + 0.1 * 6.468250274658203
Epoch 330, val loss: 0.598482608795166
Epoch 340, training loss: 0.7881763577461243 = 0.1413823813199997 + 0.1 * 6.467939376831055
Epoch 340, val loss: 0.5966091156005859
Epoch 350, training loss: 0.7699647545814514 = 0.1242133304476738 + 0.1 * 6.457513809204102
Epoch 350, val loss: 0.5967530608177185
Epoch 360, training loss: 0.7544233202934265 = 0.10968594998121262 + 0.1 * 6.447373390197754
Epoch 360, val loss: 0.5988760590553284
Epoch 370, training loss: 0.7452123761177063 = 0.09735190868377686 + 0.1 * 6.478604316711426
Epoch 370, val loss: 0.6025341749191284
Epoch 380, training loss: 0.7305530309677124 = 0.08698577433824539 + 0.1 * 6.435672283172607
Epoch 380, val loss: 0.6072131395339966
Epoch 390, training loss: 0.7208542823791504 = 0.07817111164331436 + 0.1 * 6.426831245422363
Epoch 390, val loss: 0.6131718158721924
Epoch 400, training loss: 0.7126530408859253 = 0.07060437649488449 + 0.1 * 6.4204864501953125
Epoch 400, val loss: 0.620007336139679
Epoch 410, training loss: 0.7074641585350037 = 0.06405331194400787 + 0.1 * 6.434108734130859
Epoch 410, val loss: 0.627377450466156
Epoch 420, training loss: 0.699701726436615 = 0.058394499123096466 + 0.1 * 6.413072109222412
Epoch 420, val loss: 0.6352481842041016
Epoch 430, training loss: 0.6939019560813904 = 0.05345173552632332 + 0.1 * 6.4045023918151855
Epoch 430, val loss: 0.6434201002120972
Epoch 440, training loss: 0.6894361972808838 = 0.04909570515155792 + 0.1 * 6.403404712677002
Epoch 440, val loss: 0.6517040729522705
Epoch 450, training loss: 0.6850513219833374 = 0.04524875804781914 + 0.1 * 6.3980255126953125
Epoch 450, val loss: 0.660062849521637
Epoch 460, training loss: 0.6803474426269531 = 0.041831351816654205 + 0.1 * 6.385160446166992
Epoch 460, val loss: 0.6684696078300476
Epoch 470, training loss: 0.6777476072311401 = 0.03877675160765648 + 0.1 * 6.389708518981934
Epoch 470, val loss: 0.6768995523452759
Epoch 480, training loss: 0.6737363338470459 = 0.036035049706697464 + 0.1 * 6.377012729644775
Epoch 480, val loss: 0.6851374506950378
Epoch 490, training loss: 0.6704638600349426 = 0.03356824070215225 + 0.1 * 6.368955612182617
Epoch 490, val loss: 0.6933683753013611
Epoch 500, training loss: 0.6689177751541138 = 0.031353287398815155 + 0.1 * 6.375644683837891
Epoch 500, val loss: 0.7015329599380493
Epoch 510, training loss: 0.6656202077865601 = 0.029372969642281532 + 0.1 * 6.362472057342529
Epoch 510, val loss: 0.7094573378562927
Epoch 520, training loss: 0.6628368496894836 = 0.02757725678384304 + 0.1 * 6.352595806121826
Epoch 520, val loss: 0.7173629999160767
Epoch 530, training loss: 0.6612584590911865 = 0.025937432423233986 + 0.1 * 6.353209972381592
Epoch 530, val loss: 0.7250970005989075
Epoch 540, training loss: 0.6593875885009766 = 0.024445952847599983 + 0.1 * 6.349416255950928
Epoch 540, val loss: 0.7325825095176697
Epoch 550, training loss: 0.6568624973297119 = 0.023091336712241173 + 0.1 * 6.337711811065674
Epoch 550, val loss: 0.7399884462356567
Epoch 560, training loss: 0.6552265286445618 = 0.021848520264029503 + 0.1 * 6.333780288696289
Epoch 560, val loss: 0.7472723722457886
Epoch 570, training loss: 0.6544922590255737 = 0.020707588642835617 + 0.1 * 6.337846279144287
Epoch 570, val loss: 0.7543460130691528
Epoch 580, training loss: 0.6515183448791504 = 0.01966138556599617 + 0.1 * 6.318569660186768
Epoch 580, val loss: 0.7612424492835999
Epoch 590, training loss: 0.6505348086357117 = 0.018694238737225533 + 0.1 * 6.318405628204346
Epoch 590, val loss: 0.7679873704910278
Epoch 600, training loss: 0.6486212015151978 = 0.017798760905861855 + 0.1 * 6.308224201202393
Epoch 600, val loss: 0.7746687531471252
Epoch 610, training loss: 0.6480535268783569 = 0.016968687996268272 + 0.1 * 6.310848712921143
Epoch 610, val loss: 0.7811840772628784
Epoch 620, training loss: 0.6504074335098267 = 0.016198743134737015 + 0.1 * 6.3420867919921875
Epoch 620, val loss: 0.7875910997390747
Epoch 630, training loss: 0.6456124782562256 = 0.015487844124436378 + 0.1 * 6.301246166229248
Epoch 630, val loss: 0.7937701344490051
Epoch 640, training loss: 0.643952488899231 = 0.014826241880655289 + 0.1 * 6.291262626647949
Epoch 640, val loss: 0.7998222708702087
Epoch 650, training loss: 0.6427338123321533 = 0.014205021783709526 + 0.1 * 6.285287857055664
Epoch 650, val loss: 0.8058323860168457
Epoch 660, training loss: 0.6426169276237488 = 0.013621080666780472 + 0.1 * 6.2899580001831055
Epoch 660, val loss: 0.811738908290863
Epoch 670, training loss: 0.6467151641845703 = 0.013071811757981777 + 0.1 * 6.336432933807373
Epoch 670, val loss: 0.817470133304596
Epoch 680, training loss: 0.6406580209732056 = 0.012560014612972736 + 0.1 * 6.280980110168457
Epoch 680, val loss: 0.8230671882629395
Epoch 690, training loss: 0.6391515135765076 = 0.012080331332981586 + 0.1 * 6.270711898803711
Epoch 690, val loss: 0.8285430073738098
Epoch 700, training loss: 0.6384857892990112 = 0.011626092717051506 + 0.1 * 6.26859712600708
Epoch 700, val loss: 0.8340322971343994
Epoch 710, training loss: 0.6386982202529907 = 0.011195686645805836 + 0.1 * 6.275025367736816
Epoch 710, val loss: 0.8392959237098694
Epoch 720, training loss: 0.6370508670806885 = 0.010793746449053288 + 0.1 * 6.262570858001709
Epoch 720, val loss: 0.8445553183555603
Epoch 730, training loss: 0.6358296871185303 = 0.010414550080895424 + 0.1 * 6.254151344299316
Epoch 730, val loss: 0.8497318029403687
Epoch 740, training loss: 0.6389476656913757 = 0.010054830461740494 + 0.1 * 6.288928508758545
Epoch 740, val loss: 0.8548115491867065
Epoch 750, training loss: 0.6351760625839233 = 0.009715990163385868 + 0.1 * 6.254600524902344
Epoch 750, val loss: 0.8598079681396484
Epoch 760, training loss: 0.6347722411155701 = 0.009398583322763443 + 0.1 * 6.25373649597168
Epoch 760, val loss: 0.8646750450134277
Epoch 770, training loss: 0.6337262392044067 = 0.009097444824874401 + 0.1 * 6.246287822723389
Epoch 770, val loss: 0.8695045113563538
Epoch 780, training loss: 0.63381427526474 = 0.008812463842332363 + 0.1 * 6.2500176429748535
Epoch 780, val loss: 0.8742426633834839
Epoch 790, training loss: 0.6321617364883423 = 0.008541778661310673 + 0.1 * 6.236199855804443
Epoch 790, val loss: 0.8789676427841187
Epoch 800, training loss: 0.6327299475669861 = 0.008285274729132652 + 0.1 * 6.244446754455566
Epoch 800, val loss: 0.8835936188697815
Epoch 810, training loss: 0.6308557391166687 = 0.008041542023420334 + 0.1 * 6.228141784667969
Epoch 810, val loss: 0.8881546258926392
Epoch 820, training loss: 0.630835235118866 = 0.007810316514223814 + 0.1 * 6.230249404907227
Epoch 820, val loss: 0.8926047086715698
Epoch 830, training loss: 0.630169153213501 = 0.007589080836623907 + 0.1 * 6.225800514221191
Epoch 830, val loss: 0.8970063924789429
Epoch 840, training loss: 0.6318285465240479 = 0.007379243616014719 + 0.1 * 6.244493007659912
Epoch 840, val loss: 0.9013882875442505
Epoch 850, training loss: 0.6294182538986206 = 0.0071790204383432865 + 0.1 * 6.2223920822143555
Epoch 850, val loss: 0.9055768251419067
Epoch 860, training loss: 0.6290684938430786 = 0.006988909561187029 + 0.1 * 6.22079610824585
Epoch 860, val loss: 0.909744381904602
Epoch 870, training loss: 0.6293572187423706 = 0.006805882323533297 + 0.1 * 6.225512981414795
Epoch 870, val loss: 0.9139075875282288
Epoch 880, training loss: 0.6277199983596802 = 0.00663033127784729 + 0.1 * 6.2108964920043945
Epoch 880, val loss: 0.9179422855377197
Epoch 890, training loss: 0.6277016401290894 = 0.006462540477514267 + 0.1 * 6.212390899658203
Epoch 890, val loss: 0.9219071865081787
Epoch 900, training loss: 0.6285639405250549 = 0.006301019340753555 + 0.1 * 6.222629070281982
Epoch 900, val loss: 0.925948441028595
Epoch 910, training loss: 0.6271917223930359 = 0.006147276144474745 + 0.1 * 6.210444450378418
Epoch 910, val loss: 0.9298218488693237
Epoch 920, training loss: 0.626541256904602 = 0.006000962108373642 + 0.1 * 6.2054033279418945
Epoch 920, val loss: 0.9336428642272949
Epoch 930, training loss: 0.6261888742446899 = 0.005859977565705776 + 0.1 * 6.203289031982422
Epoch 930, val loss: 0.9373964071273804
Epoch 940, training loss: 0.6252912878990173 = 0.005723724607378244 + 0.1 * 6.195675373077393
Epoch 940, val loss: 0.9411398768424988
Epoch 950, training loss: 0.6278125047683716 = 0.005593214184045792 + 0.1 * 6.222192764282227
Epoch 950, val loss: 0.9448157548904419
Epoch 960, training loss: 0.6248749494552612 = 0.005467545241117477 + 0.1 * 6.1940741539001465
Epoch 960, val loss: 0.9484934210777283
Epoch 970, training loss: 0.6256887316703796 = 0.0053475406020879745 + 0.1 * 6.20341157913208
Epoch 970, val loss: 0.952124834060669
Epoch 980, training loss: 0.624460756778717 = 0.00523179117590189 + 0.1 * 6.192289352416992
Epoch 980, val loss: 0.9555951356887817
Epoch 990, training loss: 0.6252856850624084 = 0.005120621994137764 + 0.1 * 6.201650142669678
Epoch 990, val loss: 0.9591163992881775
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8007
Flip ASR: 0.7600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.8010058403015137 = 1.9636197090148926 + 0.1 * 8.373861312866211
Epoch 0, val loss: 1.9734569787979126
Epoch 10, training loss: 2.7906899452209473 = 1.9533190727233887 + 0.1 * 8.373708724975586
Epoch 10, val loss: 1.9630399942398071
Epoch 20, training loss: 2.777881622314453 = 1.9406163692474365 + 0.1 * 8.372651100158691
Epoch 20, val loss: 1.9499850273132324
Epoch 30, training loss: 2.7591280937194824 = 1.9227317571640015 + 0.1 * 8.363964080810547
Epoch 30, val loss: 1.931344747543335
Epoch 40, training loss: 2.7265243530273438 = 1.8961477279663086 + 0.1 * 8.303766250610352
Epoch 40, val loss: 1.9036976099014282
Epoch 50, training loss: 2.654888868331909 = 1.860045075416565 + 0.1 * 7.9484381675720215
Epoch 50, val loss: 1.867786169052124
Epoch 60, training loss: 2.5773537158966064 = 1.821087121963501 + 0.1 * 7.5626654624938965
Epoch 60, val loss: 1.8315755128860474
Epoch 70, training loss: 2.4928903579711914 = 1.7868021726608276 + 0.1 * 7.060880661010742
Epoch 70, val loss: 1.8018836975097656
Epoch 80, training loss: 2.4354844093322754 = 1.75582754611969 + 0.1 * 6.796567440032959
Epoch 80, val loss: 1.77480947971344
Epoch 90, training loss: 2.387807846069336 = 1.7165954113006592 + 0.1 * 6.712122917175293
Epoch 90, val loss: 1.7391870021820068
Epoch 100, training loss: 2.330048084259033 = 1.6635841131210327 + 0.1 * 6.664639472961426
Epoch 100, val loss: 1.6929441690444946
Epoch 110, training loss: 2.2562928199768066 = 1.5936721563339233 + 0.1 * 6.626205921173096
Epoch 110, val loss: 1.6346598863601685
Epoch 120, training loss: 2.1677188873291016 = 1.5077168941497803 + 0.1 * 6.600019454956055
Epoch 120, val loss: 1.564387559890747
Epoch 130, training loss: 2.0699729919433594 = 1.4116246700286865 + 0.1 * 6.583483695983887
Epoch 130, val loss: 1.4865411520004272
Epoch 140, training loss: 1.968674659729004 = 1.311722993850708 + 0.1 * 6.569515705108643
Epoch 140, val loss: 1.4068231582641602
Epoch 150, training loss: 1.8661856651306152 = 1.2098712921142578 + 0.1 * 6.563143730163574
Epoch 150, val loss: 1.3274863958358765
Epoch 160, training loss: 1.7632651329040527 = 1.1079072952270508 + 0.1 * 6.553577899932861
Epoch 160, val loss: 1.2480255365371704
Epoch 170, training loss: 1.6619147062301636 = 1.0072882175445557 + 0.1 * 6.5462646484375
Epoch 170, val loss: 1.1695574522018433
Epoch 180, training loss: 1.5662434101104736 = 0.912098228931427 + 0.1 * 6.541452407836914
Epoch 180, val loss: 1.0953001976013184
Epoch 190, training loss: 1.4780023097991943 = 0.8248235583305359 + 0.1 * 6.531787395477295
Epoch 190, val loss: 1.0279279947280884
Epoch 200, training loss: 1.3977360725402832 = 0.7453494071960449 + 0.1 * 6.523866653442383
Epoch 200, val loss: 0.9669546484947205
Epoch 210, training loss: 1.3270237445831299 = 0.6748947501182556 + 0.1 * 6.521289348602295
Epoch 210, val loss: 0.9143394827842712
Epoch 220, training loss: 1.2629464864730835 = 0.6126319766044617 + 0.1 * 6.50314474105835
Epoch 220, val loss: 0.869507372379303
Epoch 230, training loss: 1.2058912515640259 = 0.5568056106567383 + 0.1 * 6.490856170654297
Epoch 230, val loss: 0.8315929770469666
Epoch 240, training loss: 1.1550724506378174 = 0.5070374608039856 + 0.1 * 6.480350017547607
Epoch 240, val loss: 0.800584077835083
Epoch 250, training loss: 1.1090883016586304 = 0.4624366760253906 + 0.1 * 6.466516017913818
Epoch 250, val loss: 0.775739848613739
Epoch 260, training loss: 1.0676411390304565 = 0.421888530254364 + 0.1 * 6.457525730133057
Epoch 260, val loss: 0.7560313940048218
Epoch 270, training loss: 1.028780460357666 = 0.3849525451660156 + 0.1 * 6.438279628753662
Epoch 270, val loss: 0.7408537864685059
Epoch 280, training loss: 0.9944490194320679 = 0.35090965032577515 + 0.1 * 6.435393333435059
Epoch 280, val loss: 0.728995680809021
Epoch 290, training loss: 0.9611964821815491 = 0.3195028305053711 + 0.1 * 6.41693639755249
Epoch 290, val loss: 0.7200300097465515
Epoch 300, training loss: 0.9315307140350342 = 0.29013803601264954 + 0.1 * 6.41392707824707
Epoch 300, val loss: 0.7130228877067566
Epoch 310, training loss: 0.9031237363815308 = 0.2626758813858032 + 0.1 * 6.404478549957275
Epoch 310, val loss: 0.7077468037605286
Epoch 320, training loss: 0.8758389949798584 = 0.23693835735321045 + 0.1 * 6.3890061378479
Epoch 320, val loss: 0.7038964629173279
Epoch 330, training loss: 0.8524746894836426 = 0.21283464133739471 + 0.1 * 6.396400451660156
Epoch 330, val loss: 0.7011804580688477
Epoch 340, training loss: 0.8284398317337036 = 0.19059127569198608 + 0.1 * 6.378485202789307
Epoch 340, val loss: 0.6997286677360535
Epoch 350, training loss: 0.8066683411598206 = 0.1701926440000534 + 0.1 * 6.3647565841674805
Epoch 350, val loss: 0.6993462443351746
Epoch 360, training loss: 0.7939513921737671 = 0.15167412161827087 + 0.1 * 6.4227728843688965
Epoch 360, val loss: 0.6999477744102478
Epoch 370, training loss: 0.7709961533546448 = 0.13526153564453125 + 0.1 * 6.357346057891846
Epoch 370, val loss: 0.701488196849823
Epoch 380, training loss: 0.7549876570701599 = 0.12070629745721817 + 0.1 * 6.342813491821289
Epoch 380, val loss: 0.7042380571365356
Epoch 390, training loss: 0.7412911653518677 = 0.10784081369638443 + 0.1 * 6.334503173828125
Epoch 390, val loss: 0.7079944610595703
Epoch 400, training loss: 0.7311114072799683 = 0.09656919538974762 + 0.1 * 6.345422267913818
Epoch 400, val loss: 0.7125966548919678
Epoch 410, training loss: 0.7194114327430725 = 0.08677370846271515 + 0.1 * 6.3263773918151855
Epoch 410, val loss: 0.7180808186531067
Epoch 420, training loss: 0.7096107006072998 = 0.07821550965309143 + 0.1 * 6.3139519691467285
Epoch 420, val loss: 0.7242957353591919
Epoch 430, training loss: 0.7016820311546326 = 0.07074762135744095 + 0.1 * 6.309344291687012
Epoch 430, val loss: 0.7310006618499756
Epoch 440, training loss: 0.694412112236023 = 0.06426092237234116 + 0.1 * 6.301512241363525
Epoch 440, val loss: 0.7381609082221985
Epoch 450, training loss: 0.688218891620636 = 0.05857960879802704 + 0.1 * 6.296392440795898
Epoch 450, val loss: 0.7456833720207214
Epoch 460, training loss: 0.6843675971031189 = 0.05358659476041794 + 0.1 * 6.307809829711914
Epoch 460, val loss: 0.753429114818573
Epoch 470, training loss: 0.6779811382293701 = 0.04919373244047165 + 0.1 * 6.2878737449646
Epoch 470, val loss: 0.76132732629776
Epoch 480, training loss: 0.6760236024856567 = 0.045312948524951935 + 0.1 * 6.3071064949035645
Epoch 480, val loss: 0.769393801689148
Epoch 490, training loss: 0.67060786485672 = 0.04188501462340355 + 0.1 * 6.287228584289551
Epoch 490, val loss: 0.7773622870445251
Epoch 500, training loss: 0.6656782031059265 = 0.03883649781346321 + 0.1 * 6.268416881561279
Epoch 500, val loss: 0.7853817939758301
Epoch 510, training loss: 0.6646108627319336 = 0.036104828119277954 + 0.1 * 6.285060405731201
Epoch 510, val loss: 0.7933082580566406
Epoch 520, training loss: 0.6607402563095093 = 0.033661410212516785 + 0.1 * 6.270788192749023
Epoch 520, val loss: 0.8011212348937988
Epoch 530, training loss: 0.6578238010406494 = 0.031461313366889954 + 0.1 * 6.263624668121338
Epoch 530, val loss: 0.8088903427124023
Epoch 540, training loss: 0.6564273238182068 = 0.029475023970007896 + 0.1 * 6.269522666931152
Epoch 540, val loss: 0.816519021987915
Epoch 550, training loss: 0.653148889541626 = 0.02767876908183098 + 0.1 * 6.254700660705566
Epoch 550, val loss: 0.8240232467651367
Epoch 560, training loss: 0.6511770486831665 = 0.026048144325613976 + 0.1 * 6.251289367675781
Epoch 560, val loss: 0.8314908742904663
Epoch 570, training loss: 0.6491955518722534 = 0.024560391902923584 + 0.1 * 6.24635124206543
Epoch 570, val loss: 0.8387500047683716
Epoch 580, training loss: 0.648032009601593 = 0.023200014606118202 + 0.1 * 6.248319625854492
Epoch 580, val loss: 0.8459336757659912
Epoch 590, training loss: 0.6455495357513428 = 0.02195635810494423 + 0.1 * 6.235931396484375
Epoch 590, val loss: 0.8529415130615234
Epoch 600, training loss: 0.6450310349464417 = 0.020812978968024254 + 0.1 * 6.242180347442627
Epoch 600, val loss: 0.8598784804344177
Epoch 610, training loss: 0.6425474882125854 = 0.019761506468057632 + 0.1 * 6.2278594970703125
Epoch 610, val loss: 0.8666927814483643
Epoch 620, training loss: 0.6420897245407104 = 0.018790781497955322 + 0.1 * 6.232989311218262
Epoch 620, val loss: 0.8733606338500977
Epoch 630, training loss: 0.6402410864830017 = 0.01789223589003086 + 0.1 * 6.2234883308410645
Epoch 630, val loss: 0.8799197673797607
Epoch 640, training loss: 0.6411370038986206 = 0.017060955986380577 + 0.1 * 6.240760326385498
Epoch 640, val loss: 0.8862873315811157
Epoch 650, training loss: 0.637603759765625 = 0.016292979940772057 + 0.1 * 6.213107585906982
Epoch 650, val loss: 0.8925988674163818
Epoch 660, training loss: 0.6362882852554321 = 0.015578212216496468 + 0.1 * 6.2071003913879395
Epoch 660, val loss: 0.8988186120986938
Epoch 670, training loss: 0.6380788683891296 = 0.014909564517438412 + 0.1 * 6.231692790985107
Epoch 670, val loss: 0.9048676490783691
Epoch 680, training loss: 0.6348593235015869 = 0.01428698468953371 + 0.1 * 6.205723285675049
Epoch 680, val loss: 0.9107891917228699
Epoch 690, training loss: 0.6344941854476929 = 0.013708935119211674 + 0.1 * 6.207852363586426
Epoch 690, val loss: 0.9166830778121948
Epoch 700, training loss: 0.6335610151290894 = 0.013165481388568878 + 0.1 * 6.203955173492432
Epoch 700, val loss: 0.9225029349327087
Epoch 710, training loss: 0.6333219408988953 = 0.012654326856136322 + 0.1 * 6.206676006317139
Epoch 710, val loss: 0.9281222224235535
Epoch 720, training loss: 0.6315867900848389 = 0.012176438234746456 + 0.1 * 6.194103240966797
Epoch 720, val loss: 0.9336162805557251
Epoch 730, training loss: 0.6318238377571106 = 0.011726763099431992 + 0.1 * 6.200970649719238
Epoch 730, val loss: 0.9391574859619141
Epoch 740, training loss: 0.6300368309020996 = 0.011302907951176167 + 0.1 * 6.1873393058776855
Epoch 740, val loss: 0.944474458694458
Epoch 750, training loss: 0.6301798224449158 = 0.01090389583259821 + 0.1 * 6.192759037017822
Epoch 750, val loss: 0.9497100710868835
Epoch 760, training loss: 0.6294978260993958 = 0.01052682101726532 + 0.1 * 6.189709663391113
Epoch 760, val loss: 0.9548442959785461
Epoch 770, training loss: 0.629023015499115 = 0.010171490721404552 + 0.1 * 6.1885151863098145
Epoch 770, val loss: 0.9598857760429382
Epoch 780, training loss: 0.6285676956176758 = 0.009835723787546158 + 0.1 * 6.187319755554199
Epoch 780, val loss: 0.9649929404258728
Epoch 790, training loss: 0.6276425123214722 = 0.009516512043774128 + 0.1 * 6.181259632110596
Epoch 790, val loss: 0.9698247909545898
Epoch 800, training loss: 0.6277834177017212 = 0.00921523105353117 + 0.1 * 6.1856818199157715
Epoch 800, val loss: 0.9746798872947693
Epoch 810, training loss: 0.6260828375816345 = 0.008930087089538574 + 0.1 * 6.17152738571167
Epoch 810, val loss: 0.9794329404830933
Epoch 820, training loss: 0.6274077892303467 = 0.008657963946461678 + 0.1 * 6.187497615814209
Epoch 820, val loss: 0.9841923713684082
Epoch 830, training loss: 0.6268439888954163 = 0.008399329148232937 + 0.1 * 6.184446811676025
Epoch 830, val loss: 0.9887713193893433
Epoch 840, training loss: 0.6250172853469849 = 0.008153827860951424 + 0.1 * 6.168634414672852
Epoch 840, val loss: 0.9933079481124878
Epoch 850, training loss: 0.6246486902236938 = 0.007921063341200352 + 0.1 * 6.167276382446289
Epoch 850, val loss: 0.9977760314941406
Epoch 860, training loss: 0.6246622800827026 = 0.007698446977883577 + 0.1 * 6.169637680053711
Epoch 860, val loss: 1.0022106170654297
Epoch 870, training loss: 0.6233978271484375 = 0.007485433481633663 + 0.1 * 6.159123420715332
Epoch 870, val loss: 1.0065007209777832
Epoch 880, training loss: 0.624179482460022 = 0.007282733451575041 + 0.1 * 6.168967247009277
Epoch 880, val loss: 1.0107407569885254
Epoch 890, training loss: 0.6240328550338745 = 0.007088422775268555 + 0.1 * 6.1694440841674805
Epoch 890, val loss: 1.0149110555648804
Epoch 900, training loss: 0.6236282587051392 = 0.006902656983584166 + 0.1 * 6.1672563552856445
Epoch 900, val loss: 1.0190894603729248
Epoch 910, training loss: 0.6220826506614685 = 0.006725569721311331 + 0.1 * 6.153571128845215
Epoch 910, val loss: 1.0231032371520996
Epoch 920, training loss: 0.6219756007194519 = 0.006556044798344374 + 0.1 * 6.154195785522461
Epoch 920, val loss: 1.027154803276062
Epoch 930, training loss: 0.6211432218551636 = 0.006393084302544594 + 0.1 * 6.147501468658447
Epoch 930, val loss: 1.0310709476470947
Epoch 940, training loss: 0.6215266585350037 = 0.006236707791686058 + 0.1 * 6.152899265289307
Epoch 940, val loss: 1.0349315404891968
Epoch 950, training loss: 0.6217333674430847 = 0.0060871560126543045 + 0.1 * 6.156461715698242
Epoch 950, val loss: 1.0387532711029053
Epoch 960, training loss: 0.6202646493911743 = 0.005943415220826864 + 0.1 * 6.14321231842041
Epoch 960, val loss: 1.0424649715423584
Epoch 970, training loss: 0.6197482347488403 = 0.005805992521345615 + 0.1 * 6.139422416687012
Epoch 970, val loss: 1.0461424589157104
Epoch 980, training loss: 0.6197070479393005 = 0.0056738597340881824 + 0.1 * 6.140331745147705
Epoch 980, val loss: 1.0498440265655518
Epoch 990, training loss: 0.6216155886650085 = 0.00554595747962594 + 0.1 * 6.160696029663086
Epoch 990, val loss: 1.0534591674804688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.8782
Flip ASR: 0.8533/225 nodes
The final ASR:0.78352, 0.08524, Accuracy:0.81975, 0.01522
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10592])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7731494903564453 = 1.9357603788375854 + 0.1 * 8.373889923095703
Epoch 0, val loss: 1.928242564201355
Epoch 10, training loss: 2.763319969177246 = 1.9259449243545532 + 0.1 * 8.373751640319824
Epoch 10, val loss: 1.9187387228012085
Epoch 20, training loss: 2.750702381134033 = 1.9134138822555542 + 0.1 * 8.372885704040527
Epoch 20, val loss: 1.9059346914291382
Epoch 30, training loss: 2.732113838195801 = 1.89552640914917 + 0.1 * 8.365874290466309
Epoch 30, val loss: 1.8872079849243164
Epoch 40, training loss: 2.7007977962493896 = 1.8690191507339478 + 0.1 * 8.317787170410156
Epoch 40, val loss: 1.8597317934036255
Epoch 50, training loss: 2.6376044750213623 = 1.833561658859253 + 0.1 * 8.040427207946777
Epoch 50, val loss: 1.8252437114715576
Epoch 60, training loss: 2.5634100437164307 = 1.7951433658599854 + 0.1 * 7.682666778564453
Epoch 60, val loss: 1.7917057275772095
Epoch 70, training loss: 2.4913108348846436 = 1.7578606605529785 + 0.1 * 7.334501266479492
Epoch 70, val loss: 1.7613917589187622
Epoch 80, training loss: 2.4169154167175293 = 1.714984655380249 + 0.1 * 7.019306659698486
Epoch 80, val loss: 1.7258734703063965
Epoch 90, training loss: 2.34424090385437 = 1.6594337224960327 + 0.1 * 6.848071575164795
Epoch 90, val loss: 1.6790673732757568
Epoch 100, training loss: 2.262579917907715 = 1.58656644821167 + 0.1 * 6.760135650634766
Epoch 100, val loss: 1.617854118347168
Epoch 110, training loss: 2.1710360050201416 = 1.4994032382965088 + 0.1 * 6.716327667236328
Epoch 110, val loss: 1.5454338788986206
Epoch 120, training loss: 2.0753605365753174 = 1.4066907167434692 + 0.1 * 6.6866984367370605
Epoch 120, val loss: 1.4713886976242065
Epoch 130, training loss: 1.9806320667266846 = 1.3147342205047607 + 0.1 * 6.658977508544922
Epoch 130, val loss: 1.402800440788269
Epoch 140, training loss: 1.888230562210083 = 1.2249963283538818 + 0.1 * 6.63234281539917
Epoch 140, val loss: 1.3401159048080444
Epoch 150, training loss: 1.8002510070800781 = 1.1389923095703125 + 0.1 * 6.61258602142334
Epoch 150, val loss: 1.2828168869018555
Epoch 160, training loss: 1.717970371246338 = 1.058025598526001 + 0.1 * 6.599448204040527
Epoch 160, val loss: 1.230297565460205
Epoch 170, training loss: 1.6420440673828125 = 0.9832242727279663 + 0.1 * 6.588197708129883
Epoch 170, val loss: 1.1825289726257324
Epoch 180, training loss: 1.571664810180664 = 0.9137704968452454 + 0.1 * 6.578943729400635
Epoch 180, val loss: 1.1380037069320679
Epoch 190, training loss: 1.5050554275512695 = 0.8479688167572021 + 0.1 * 6.570866584777832
Epoch 190, val loss: 1.0952427387237549
Epoch 200, training loss: 1.4417753219604492 = 0.7850075364112854 + 0.1 * 6.567677974700928
Epoch 200, val loss: 1.0536530017852783
Epoch 210, training loss: 1.3804426193237305 = 0.7242832779884338 + 0.1 * 6.561594009399414
Epoch 210, val loss: 1.01288902759552
Epoch 220, training loss: 1.3209295272827148 = 0.6652193069458008 + 0.1 * 6.557102680206299
Epoch 220, val loss: 0.9727179408073425
Epoch 230, training loss: 1.2630352973937988 = 0.6077437996864319 + 0.1 * 6.552915573120117
Epoch 230, val loss: 0.9333739876747131
Epoch 240, training loss: 1.2075382471084595 = 0.5519554615020752 + 0.1 * 6.555827617645264
Epoch 240, val loss: 0.8951138257980347
Epoch 250, training loss: 1.15329909324646 = 0.49870675802230835 + 0.1 * 6.545923709869385
Epoch 250, val loss: 0.8596283197402954
Epoch 260, training loss: 1.1024571657180786 = 0.4482661187648773 + 0.1 * 6.541910648345947
Epoch 260, val loss: 0.827762246131897
Epoch 270, training loss: 1.0550343990325928 = 0.4008663594722748 + 0.1 * 6.541680812835693
Epoch 270, val loss: 0.8004894256591797
Epoch 280, training loss: 1.0104235410690308 = 0.3568309247493744 + 0.1 * 6.535926342010498
Epoch 280, val loss: 0.7784359455108643
Epoch 290, training loss: 0.9683916568756104 = 0.31574633717536926 + 0.1 * 6.526452541351318
Epoch 290, val loss: 0.761340856552124
Epoch 300, training loss: 0.9298899173736572 = 0.2777019739151001 + 0.1 * 6.521879196166992
Epoch 300, val loss: 0.7491583228111267
Epoch 310, training loss: 0.8949843049049377 = 0.24321848154067993 + 0.1 * 6.517658233642578
Epoch 310, val loss: 0.7415571212768555
Epoch 320, training loss: 0.8637325167655945 = 0.21263659000396729 + 0.1 * 6.510959148406982
Epoch 320, val loss: 0.7383058071136475
Epoch 330, training loss: 0.8361214399337769 = 0.1860000193119049 + 0.1 * 6.501214027404785
Epoch 330, val loss: 0.7387232780456543
Epoch 340, training loss: 0.8149447441101074 = 0.16315415501594543 + 0.1 * 6.517906188964844
Epoch 340, val loss: 0.7418261766433716
Epoch 350, training loss: 0.7932878732681274 = 0.1437835693359375 + 0.1 * 6.49504280090332
Epoch 350, val loss: 0.7468068599700928
Epoch 360, training loss: 0.7757536172866821 = 0.1271725744009018 + 0.1 * 6.485810279846191
Epoch 360, val loss: 0.7534288763999939
Epoch 370, training loss: 0.7608468532562256 = 0.11288975179195404 + 0.1 * 6.4795708656311035
Epoch 370, val loss: 0.7613655924797058
Epoch 380, training loss: 0.7478584051132202 = 0.1005612313747406 + 0.1 * 6.472971439361572
Epoch 380, val loss: 0.7700562477111816
Epoch 390, training loss: 0.7365930676460266 = 0.08987821638584137 + 0.1 * 6.467148303985596
Epoch 390, val loss: 0.7794726490974426
Epoch 400, training loss: 0.7319735288619995 = 0.08059488981962204 + 0.1 * 6.513786315917969
Epoch 400, val loss: 0.7891990542411804
Epoch 410, training loss: 0.7192631959915161 = 0.07260580360889435 + 0.1 * 6.466573715209961
Epoch 410, val loss: 0.79911869764328
Epoch 420, training loss: 0.7113710641860962 = 0.06564684957265854 + 0.1 * 6.457242012023926
Epoch 420, val loss: 0.8086802363395691
Epoch 430, training loss: 0.7044023275375366 = 0.05952445790171623 + 0.1 * 6.4487786293029785
Epoch 430, val loss: 0.8185627460479736
Epoch 440, training loss: 0.698308527469635 = 0.05411602556705475 + 0.1 * 6.441925048828125
Epoch 440, val loss: 0.8284536004066467
Epoch 450, training loss: 0.6969555616378784 = 0.04933130368590355 + 0.1 * 6.476242542266846
Epoch 450, val loss: 0.8381561040878296
Epoch 460, training loss: 0.6894902586936951 = 0.04513972997665405 + 0.1 * 6.44350528717041
Epoch 460, val loss: 0.8479870557785034
Epoch 470, training loss: 0.6844078302383423 = 0.04143389314413071 + 0.1 * 6.429739475250244
Epoch 470, val loss: 0.8573742508888245
Epoch 480, training loss: 0.6798890829086304 = 0.03814239054918289 + 0.1 * 6.417466640472412
Epoch 480, val loss: 0.8669636845588684
Epoch 490, training loss: 0.6797717213630676 = 0.03521209582686424 + 0.1 * 6.445596218109131
Epoch 490, val loss: 0.8763787746429443
Epoch 500, training loss: 0.6736131906509399 = 0.03261738643050194 + 0.1 * 6.4099578857421875
Epoch 500, val loss: 0.885898768901825
Epoch 510, training loss: 0.6699169278144836 = 0.030300328508019447 + 0.1 * 6.39616584777832
Epoch 510, val loss: 0.8950364589691162
Epoch 520, training loss: 0.6686861515045166 = 0.02822527289390564 + 0.1 * 6.404608249664307
Epoch 520, val loss: 0.9039807915687561
Epoch 530, training loss: 0.6648959517478943 = 0.026377689093351364 + 0.1 * 6.385182857513428
Epoch 530, val loss: 0.9130584597587585
Epoch 540, training loss: 0.6624928712844849 = 0.02471410669386387 + 0.1 * 6.3777875900268555
Epoch 540, val loss: 0.9215648770332336
Epoch 550, training loss: 0.6598268747329712 = 0.02320605330169201 + 0.1 * 6.366208076477051
Epoch 550, val loss: 0.9301950931549072
Epoch 560, training loss: 0.660637617111206 = 0.02183421328663826 + 0.1 * 6.388034343719482
Epoch 560, val loss: 0.9383962750434875
Epoch 570, training loss: 0.6563912034034729 = 0.02059115469455719 + 0.1 * 6.3580002784729
Epoch 570, val loss: 0.9468007683753967
Epoch 580, training loss: 0.6546574831008911 = 0.019456781446933746 + 0.1 * 6.352006435394287
Epoch 580, val loss: 0.9547112584114075
Epoch 590, training loss: 0.6530587673187256 = 0.01841922476887703 + 0.1 * 6.346395492553711
Epoch 590, val loss: 0.9625235199928284
Epoch 600, training loss: 0.6519101858139038 = 0.017467139288783073 + 0.1 * 6.344430446624756
Epoch 600, val loss: 0.9701539278030396
Epoch 610, training loss: 0.6539568901062012 = 0.016591133549809456 + 0.1 * 6.373657703399658
Epoch 610, val loss: 0.9775973558425903
Epoch 620, training loss: 0.6487721800804138 = 0.015788309276103973 + 0.1 * 6.329838275909424
Epoch 620, val loss: 0.9847846031188965
Epoch 630, training loss: 0.646690309047699 = 0.015048310160636902 + 0.1 * 6.31641960144043
Epoch 630, val loss: 0.9919370412826538
Epoch 640, training loss: 0.6458946466445923 = 0.014361097477376461 + 0.1 * 6.315335273742676
Epoch 640, val loss: 0.9987675547599792
Epoch 650, training loss: 0.6458185911178589 = 0.013722209259867668 + 0.1 * 6.3209638595581055
Epoch 650, val loss: 1.0055269002914429
Epoch 660, training loss: 0.6435162425041199 = 0.01312969159334898 + 0.1 * 6.3038649559021
Epoch 660, val loss: 1.0121901035308838
Epoch 670, training loss: 0.6426750421524048 = 0.012578352354466915 + 0.1 * 6.300966739654541
Epoch 670, val loss: 1.0188480615615845
Epoch 680, training loss: 0.641241192817688 = 0.0120635274797678 + 0.1 * 6.291776657104492
Epoch 680, val loss: 1.0249640941619873
Epoch 690, training loss: 0.6406207084655762 = 0.011582942679524422 + 0.1 * 6.290377140045166
Epoch 690, val loss: 1.0313020944595337
Epoch 700, training loss: 0.642187774181366 = 0.011132542043924332 + 0.1 * 6.31055212020874
Epoch 700, val loss: 1.037307620048523
Epoch 710, training loss: 0.6387200355529785 = 0.01071125827729702 + 0.1 * 6.280087947845459
Epoch 710, val loss: 1.0431272983551025
Epoch 720, training loss: 0.6393592357635498 = 0.010316253639757633 + 0.1 * 6.290429592132568
Epoch 720, val loss: 1.049108624458313
Epoch 730, training loss: 0.6371203064918518 = 0.009944619610905647 + 0.1 * 6.271756649017334
Epoch 730, val loss: 1.0545564889907837
Epoch 740, training loss: 0.6375009417533875 = 0.009594911709427834 + 0.1 * 6.279059886932373
Epoch 740, val loss: 1.060186505317688
Epoch 750, training loss: 0.6360869407653809 = 0.009264996275305748 + 0.1 * 6.268219470977783
Epoch 750, val loss: 1.0656750202178955
Epoch 760, training loss: 0.6352531313896179 = 0.00895295012742281 + 0.1 * 6.263001441955566
Epoch 760, val loss: 1.0708811283111572
Epoch 770, training loss: 0.6348309516906738 = 0.008658683858811855 + 0.1 * 6.261722087860107
Epoch 770, val loss: 1.0761308670043945
Epoch 780, training loss: 0.6349449753761292 = 0.008380522951483727 + 0.1 * 6.265644550323486
Epoch 780, val loss: 1.0814262628555298
Epoch 790, training loss: 0.633583128452301 = 0.008116825483739376 + 0.1 * 6.25466251373291
Epoch 790, val loss: 1.0861533880233765
Epoch 800, training loss: 0.6330282688140869 = 0.007867852225899696 + 0.1 * 6.251603603363037
Epoch 800, val loss: 1.0912513732910156
Epoch 810, training loss: 0.6332959532737732 = 0.007631122134625912 + 0.1 * 6.256648063659668
Epoch 810, val loss: 1.0960164070129395
Epoch 820, training loss: 0.631754994392395 = 0.007406305521726608 + 0.1 * 6.2434868812561035
Epoch 820, val loss: 1.100672960281372
Epoch 830, training loss: 0.633240818977356 = 0.0071925329975783825 + 0.1 * 6.2604827880859375
Epoch 830, val loss: 1.1053142547607422
Epoch 840, training loss: 0.6308731436729431 = 0.006989232264459133 + 0.1 * 6.238839149475098
Epoch 840, val loss: 1.1098606586456299
Epoch 850, training loss: 0.6313747763633728 = 0.00679561635479331 + 0.1 * 6.245790958404541
Epoch 850, val loss: 1.1142843961715698
Epoch 860, training loss: 0.6304422616958618 = 0.006611022166907787 + 0.1 * 6.238311767578125
Epoch 860, val loss: 1.1185779571533203
Epoch 870, training loss: 0.6311308741569519 = 0.006434994284063578 + 0.1 * 6.2469587326049805
Epoch 870, val loss: 1.1229156255722046
Epoch 880, training loss: 0.6313093304634094 = 0.006266681477427483 + 0.1 * 6.250426769256592
Epoch 880, val loss: 1.1270546913146973
Epoch 890, training loss: 0.6282042264938354 = 0.0061063338071107864 + 0.1 * 6.220978736877441
Epoch 890, val loss: 1.131222128868103
Epoch 900, training loss: 0.6310154795646667 = 0.0059530409052968025 + 0.1 * 6.250624656677246
Epoch 900, val loss: 1.1354063749313354
Epoch 910, training loss: 0.6284401416778564 = 0.005805792286992073 + 0.1 * 6.226343154907227
Epoch 910, val loss: 1.139272689819336
Epoch 920, training loss: 0.6281667351722717 = 0.005665026139467955 + 0.1 * 6.225017070770264
Epoch 920, val loss: 1.1433175802230835
Epoch 930, training loss: 0.62699294090271 = 0.005529941990971565 + 0.1 * 6.214630126953125
Epoch 930, val loss: 1.1470518112182617
Epoch 940, training loss: 0.6263009309768677 = 0.0054005347192287445 + 0.1 * 6.209003925323486
Epoch 940, val loss: 1.1510136127471924
Epoch 950, training loss: 0.6284772157669067 = 0.005275919567793608 + 0.1 * 6.23201322555542
Epoch 950, val loss: 1.1546729803085327
Epoch 960, training loss: 0.626185417175293 = 0.005156387109309435 + 0.1 * 6.21028995513916
Epoch 960, val loss: 1.158280611038208
Epoch 970, training loss: 0.6255026459693909 = 0.005041881930083036 + 0.1 * 6.2046074867248535
Epoch 970, val loss: 1.1621391773223877
Epoch 980, training loss: 0.6269567608833313 = 0.0049311756156384945 + 0.1 * 6.2202558517456055
Epoch 980, val loss: 1.1655834913253784
Epoch 990, training loss: 0.6255283951759338 = 0.004824974574148655 + 0.1 * 6.207033634185791
Epoch 990, val loss: 1.1690489053726196
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7122
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.776904582977295 = 1.9395148754119873 + 0.1 * 8.373896598815918
Epoch 0, val loss: 1.9439997673034668
Epoch 10, training loss: 2.768061637878418 = 1.9306806325912476 + 0.1 * 8.373809814453125
Epoch 10, val loss: 1.9350500106811523
Epoch 20, training loss: 2.7573766708374023 = 1.9200439453125 + 0.1 * 8.373326301574707
Epoch 20, val loss: 1.9237521886825562
Epoch 30, training loss: 2.742262125015259 = 1.9053049087524414 + 0.1 * 8.369571685791016
Epoch 30, val loss: 1.9075809717178345
Epoch 40, training loss: 2.717604875564575 = 1.8835053443908691 + 0.1 * 8.340995788574219
Epoch 40, val loss: 1.8832818269729614
Epoch 50, training loss: 2.67120099067688 = 1.8522496223449707 + 0.1 * 8.189513206481934
Epoch 50, val loss: 1.8494879007339478
Epoch 60, training loss: 2.5769734382629395 = 1.8145956993103027 + 0.1 * 7.623775959014893
Epoch 60, val loss: 1.8111354112625122
Epoch 70, training loss: 2.5024142265319824 = 1.7759910821914673 + 0.1 * 7.264230728149414
Epoch 70, val loss: 1.7744916677474976
Epoch 80, training loss: 2.42533016204834 = 1.7340824604034424 + 0.1 * 6.912476062774658
Epoch 80, val loss: 1.7361911535263062
Epoch 90, training loss: 2.3589541912078857 = 1.6826984882354736 + 0.1 * 6.762556076049805
Epoch 90, val loss: 1.6904059648513794
Epoch 100, training loss: 2.288032293319702 = 1.6155465841293335 + 0.1 * 6.724856376647949
Epoch 100, val loss: 1.6323087215423584
Epoch 110, training loss: 2.1997451782226562 = 1.5293446779251099 + 0.1 * 6.704004287719727
Epoch 110, val loss: 1.5596851110458374
Epoch 120, training loss: 2.093611478805542 = 1.425054669380188 + 0.1 * 6.685567855834961
Epoch 120, val loss: 1.4734923839569092
Epoch 130, training loss: 1.975779414176941 = 1.3086328506469727 + 0.1 * 6.6714653968811035
Epoch 130, val loss: 1.378752589225769
Epoch 140, training loss: 1.8550083637237549 = 1.1888154745101929 + 0.1 * 6.661928653717041
Epoch 140, val loss: 1.2825368642807007
Epoch 150, training loss: 1.7392921447753906 = 1.073669672012329 + 0.1 * 6.656224727630615
Epoch 150, val loss: 1.1910408735275269
Epoch 160, training loss: 1.6329984664916992 = 0.9676494598388672 + 0.1 * 6.65349006652832
Epoch 160, val loss: 1.1068238019943237
Epoch 170, training loss: 1.536651372909546 = 0.8713856935501099 + 0.1 * 6.652657508850098
Epoch 170, val loss: 1.0302238464355469
Epoch 180, training loss: 1.4484537839889526 = 0.7832649350166321 + 0.1 * 6.651888370513916
Epoch 180, val loss: 0.9605326056480408
Epoch 190, training loss: 1.3672534227371216 = 0.7022302150726318 + 0.1 * 6.650231838226318
Epoch 190, val loss: 0.8975392580032349
Epoch 200, training loss: 1.2931978702545166 = 0.6284728646278381 + 0.1 * 6.647249221801758
Epoch 200, val loss: 0.8417897820472717
Epoch 210, training loss: 1.227112054824829 = 0.5627022981643677 + 0.1 * 6.644096851348877
Epoch 210, val loss: 0.7943567633628845
Epoch 220, training loss: 1.1688014268875122 = 0.5049218535423279 + 0.1 * 6.638795375823975
Epoch 220, val loss: 0.7557340264320374
Epoch 230, training loss: 1.117397665977478 = 0.454166054725647 + 0.1 * 6.6323161125183105
Epoch 230, val loss: 0.7251977920532227
Epoch 240, training loss: 1.071833610534668 = 0.40932905673980713 + 0.1 * 6.6250457763671875
Epoch 240, val loss: 0.7019209861755371
Epoch 250, training loss: 1.0313472747802734 = 0.3695150315761566 + 0.1 * 6.618322372436523
Epoch 250, val loss: 0.6847028136253357
Epoch 260, training loss: 0.994604229927063 = 0.33383074402809143 + 0.1 * 6.6077351570129395
Epoch 260, val loss: 0.6717970371246338
Epoch 270, training loss: 0.9612131714820862 = 0.3013867139816284 + 0.1 * 6.598264694213867
Epoch 270, val loss: 0.6621299386024475
Epoch 280, training loss: 0.9323766231536865 = 0.2714811861515045 + 0.1 * 6.608954429626465
Epoch 280, val loss: 0.6547384858131409
Epoch 290, training loss: 0.9027382731437683 = 0.2440488189458847 + 0.1 * 6.5868940353393555
Epoch 290, val loss: 0.6493415832519531
Epoch 300, training loss: 0.8764646649360657 = 0.21844494342803955 + 0.1 * 6.580197334289551
Epoch 300, val loss: 0.6454015374183655
Epoch 310, training loss: 0.8521162271499634 = 0.19462956488132477 + 0.1 * 6.574866771697998
Epoch 310, val loss: 0.6432091593742371
Epoch 320, training loss: 0.8299487233161926 = 0.1729542762041092 + 0.1 * 6.569944381713867
Epoch 320, val loss: 0.642861545085907
Epoch 330, training loss: 0.8103430271148682 = 0.15378692746162415 + 0.1 * 6.565560817718506
Epoch 330, val loss: 0.6447807550430298
Epoch 340, training loss: 0.7933461666107178 = 0.13724377751350403 + 0.1 * 6.561024188995361
Epoch 340, val loss: 0.6489031314849854
Epoch 350, training loss: 0.780218243598938 = 0.12312313914299011 + 0.1 * 6.570950508117676
Epoch 350, val loss: 0.6549065709114075
Epoch 360, training loss: 0.7666990756988525 = 0.11115511506795883 + 0.1 * 6.555439472198486
Epoch 360, val loss: 0.6624565124511719
Epoch 370, training loss: 0.7555462718009949 = 0.10088231414556503 + 0.1 * 6.546639442443848
Epoch 370, val loss: 0.6711065173149109
Epoch 380, training loss: 0.7459325194358826 = 0.09195796400308609 + 0.1 * 6.539745330810547
Epoch 380, val loss: 0.6807028651237488
Epoch 390, training loss: 0.7373486161231995 = 0.08412326872348785 + 0.1 * 6.532253265380859
Epoch 390, val loss: 0.6910539865493774
Epoch 400, training loss: 0.7296433448791504 = 0.0771981030702591 + 0.1 * 6.524452209472656
Epoch 400, val loss: 0.7020356059074402
Epoch 410, training loss: 0.722775399684906 = 0.07106508314609528 + 0.1 * 6.51710319519043
Epoch 410, val loss: 0.713142991065979
Epoch 420, training loss: 0.7164365649223328 = 0.06557217985391617 + 0.1 * 6.508643627166748
Epoch 420, val loss: 0.7245609164237976
Epoch 430, training loss: 0.7104237079620361 = 0.06061691418290138 + 0.1 * 6.498067855834961
Epoch 430, val loss: 0.7362743616104126
Epoch 440, training loss: 0.7052602171897888 = 0.05613544210791588 + 0.1 * 6.49124813079834
Epoch 440, val loss: 0.7480141520500183
Epoch 450, training loss: 0.7003532648086548 = 0.05209837481379509 + 0.1 * 6.482548713684082
Epoch 450, val loss: 0.7594712972640991
Epoch 460, training loss: 0.6958674788475037 = 0.048425134271383286 + 0.1 * 6.474423408508301
Epoch 460, val loss: 0.770786702632904
Epoch 470, training loss: 0.6913540363311768 = 0.04506761208176613 + 0.1 * 6.462864398956299
Epoch 470, val loss: 0.7820623517036438
Epoch 480, training loss: 0.6888013482093811 = 0.04198751971125603 + 0.1 * 6.468137741088867
Epoch 480, val loss: 0.7931552529335022
Epoch 490, training loss: 0.6839511394500732 = 0.03916430100798607 + 0.1 * 6.447868347167969
Epoch 490, val loss: 0.8038679957389832
Epoch 500, training loss: 0.6811556816101074 = 0.03656608983874321 + 0.1 * 6.445895671844482
Epoch 500, val loss: 0.8144270777702332
Epoch 510, training loss: 0.6781582832336426 = 0.034169360995292664 + 0.1 * 6.439888954162598
Epoch 510, val loss: 0.8246488571166992
Epoch 520, training loss: 0.6748385429382324 = 0.03193911537528038 + 0.1 * 6.428994655609131
Epoch 520, val loss: 0.8344482183456421
Epoch 530, training loss: 0.6720642447471619 = 0.029868831858038902 + 0.1 * 6.421954154968262
Epoch 530, val loss: 0.8439689874649048
Epoch 540, training loss: 0.6708850860595703 = 0.02796359919011593 + 0.1 * 6.429214954376221
Epoch 540, val loss: 0.8532933592796326
Epoch 550, training loss: 0.6672808527946472 = 0.026211043819785118 + 0.1 * 6.410697937011719
Epoch 550, val loss: 0.8621165752410889
Epoch 560, training loss: 0.6647339463233948 = 0.024595556780695915 + 0.1 * 6.401383876800537
Epoch 560, val loss: 0.8708302974700928
Epoch 570, training loss: 0.6626808047294617 = 0.02310398779809475 + 0.1 * 6.395767688751221
Epoch 570, val loss: 0.8793203234672546
Epoch 580, training loss: 0.6618965268135071 = 0.021729953587055206 + 0.1 * 6.401665687561035
Epoch 580, val loss: 0.8876904249191284
Epoch 590, training loss: 0.6592413187026978 = 0.02046870067715645 + 0.1 * 6.387725830078125
Epoch 590, val loss: 0.895520031452179
Epoch 600, training loss: 0.657069742679596 = 0.01930818147957325 + 0.1 * 6.377615451812744
Epoch 600, val loss: 0.9034218788146973
Epoch 610, training loss: 0.6563755869865417 = 0.018239500001072884 + 0.1 * 6.3813605308532715
Epoch 610, val loss: 0.9110815525054932
Epoch 620, training loss: 0.6547266244888306 = 0.017258085310459137 + 0.1 * 6.374684810638428
Epoch 620, val loss: 0.9185119271278381
Epoch 630, training loss: 0.6529048085212708 = 0.0163596048951149 + 0.1 * 6.365451812744141
Epoch 630, val loss: 0.9256991744041443
Epoch 640, training loss: 0.6514286398887634 = 0.01553011778742075 + 0.1 * 6.358985424041748
Epoch 640, val loss: 0.9328505992889404
Epoch 650, training loss: 0.65019690990448 = 0.014757315628230572 + 0.1 * 6.354395866394043
Epoch 650, val loss: 0.9399101138114929
Epoch 660, training loss: 0.6489105224609375 = 0.014038149267435074 + 0.1 * 6.348723411560059
Epoch 660, val loss: 0.9468422532081604
Epoch 670, training loss: 0.647814929485321 = 0.013375301845371723 + 0.1 * 6.344396114349365
Epoch 670, val loss: 0.9534131288528442
Epoch 680, training loss: 0.6466540098190308 = 0.012761303223669529 + 0.1 * 6.338927268981934
Epoch 680, val loss: 0.9600493311882019
Epoch 690, training loss: 0.6463255286216736 = 0.012191745452582836 + 0.1 * 6.341337203979492
Epoch 690, val loss: 0.9665841460227966
Epoch 700, training loss: 0.6456202864646912 = 0.011666876263916492 + 0.1 * 6.339533805847168
Epoch 700, val loss: 0.9726570248603821
Epoch 710, training loss: 0.6439650654792786 = 0.011179864406585693 + 0.1 * 6.32785177230835
Epoch 710, val loss: 0.9788715243339539
Epoch 720, training loss: 0.6430663466453552 = 0.010724755935370922 + 0.1 * 6.323415756225586
Epoch 720, val loss: 0.9848219156265259
Epoch 730, training loss: 0.6442456841468811 = 0.010298154316842556 + 0.1 * 6.339475631713867
Epoch 730, val loss: 0.9907424449920654
Epoch 740, training loss: 0.641548752784729 = 0.009899796918034554 + 0.1 * 6.316489219665527
Epoch 740, val loss: 0.9965883493423462
Epoch 750, training loss: 0.6410753130912781 = 0.009525518864393234 + 0.1 * 6.315497398376465
Epoch 750, val loss: 1.002261757850647
Epoch 760, training loss: 0.6401001214981079 = 0.009174383245408535 + 0.1 * 6.3092570304870605
Epoch 760, val loss: 1.0078555345535278
Epoch 770, training loss: 0.6397481560707092 = 0.008844587951898575 + 0.1 * 6.309035301208496
Epoch 770, val loss: 1.0132721662521362
Epoch 780, training loss: 0.6387419104576111 = 0.008534951135516167 + 0.1 * 6.302069187164307
Epoch 780, val loss: 1.0185997486114502
Epoch 790, training loss: 0.6386216878890991 = 0.008243011310696602 + 0.1 * 6.303786754608154
Epoch 790, val loss: 1.0238205194473267
Epoch 800, training loss: 0.6373087763786316 = 0.007967864163219929 + 0.1 * 6.2934088706970215
Epoch 800, val loss: 1.0288351774215698
Epoch 810, training loss: 0.6386092305183411 = 0.00770818954333663 + 0.1 * 6.309010028839111
Epoch 810, val loss: 1.033806324005127
Epoch 820, training loss: 0.636143147945404 = 0.007463294547051191 + 0.1 * 6.286798477172852
Epoch 820, val loss: 1.0386850833892822
Epoch 830, training loss: 0.6359251141548157 = 0.007231035269796848 + 0.1 * 6.286940574645996
Epoch 830, val loss: 1.0434188842773438
Epoch 840, training loss: 0.6361245512962341 = 0.007010539993643761 + 0.1 * 6.291140079498291
Epoch 840, val loss: 1.048142910003662
Epoch 850, training loss: 0.6347169280052185 = 0.0068020327016711235 + 0.1 * 6.279149055480957
Epoch 850, val loss: 1.0525562763214111
Epoch 860, training loss: 0.6335536241531372 = 0.0066036321222782135 + 0.1 * 6.2694993019104
Epoch 860, val loss: 1.0571075677871704
Epoch 870, training loss: 0.6333271265029907 = 0.006415024399757385 + 0.1 * 6.269121170043945
Epoch 870, val loss: 1.0615756511688232
Epoch 880, training loss: 0.6327957510948181 = 0.0062364330515265465 + 0.1 * 6.2655930519104
Epoch 880, val loss: 1.0657706260681152
Epoch 890, training loss: 0.6327949166297913 = 0.006067030131816864 + 0.1 * 6.267278671264648
Epoch 890, val loss: 1.0699803829193115
Epoch 900, training loss: 0.6320478916168213 = 0.005905436351895332 + 0.1 * 6.2614240646362305
Epoch 900, val loss: 1.0741740465164185
Epoch 910, training loss: 0.6320227384567261 = 0.005751364398747683 + 0.1 * 6.262713432312012
Epoch 910, val loss: 1.0781424045562744
Epoch 920, training loss: 0.6305480003356934 = 0.0056050498969852924 + 0.1 * 6.249429702758789
Epoch 920, val loss: 1.082033395767212
Epoch 930, training loss: 0.6301254630088806 = 0.005465082358568907 + 0.1 * 6.246603488922119
Epoch 930, val loss: 1.0858485698699951
Epoch 940, training loss: 0.6296449303627014 = 0.0053306506015360355 + 0.1 * 6.243143081665039
Epoch 940, val loss: 1.0896885395050049
Epoch 950, training loss: 0.6289819478988647 = 0.005202129948884249 + 0.1 * 6.23779821395874
Epoch 950, val loss: 1.0933526754379272
Epoch 960, training loss: 0.6292937994003296 = 0.005078716669231653 + 0.1 * 6.24215030670166
Epoch 960, val loss: 1.0970723628997803
Epoch 970, training loss: 0.6293898820877075 = 0.004960371647030115 + 0.1 * 6.244295120239258
Epoch 970, val loss: 1.1006019115447998
Epoch 980, training loss: 0.6284783482551575 = 0.004847052972763777 + 0.1 * 6.2363128662109375
Epoch 980, val loss: 1.1042282581329346
Epoch 990, training loss: 0.6284788846969604 = 0.004738413728773594 + 0.1 * 6.237404823303223
Epoch 990, val loss: 1.1075875759124756
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.7196
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.790635585784912 = 1.953245759010315 + 0.1 * 8.37389850616455
Epoch 0, val loss: 1.9487133026123047
Epoch 10, training loss: 2.7804830074310303 = 1.9430984258651733 + 0.1 * 8.373845100402832
Epoch 10, val loss: 1.9393258094787598
Epoch 20, training loss: 2.7685818672180176 = 1.9312388896942139 + 0.1 * 8.373428344726562
Epoch 20, val loss: 1.9279025793075562
Epoch 30, training loss: 2.7520198822021484 = 1.9150493144989014 + 0.1 * 8.369705200195312
Epoch 30, val loss: 1.9119318723678589
Epoch 40, training loss: 2.7249698638916016 = 1.8914470672607422 + 0.1 * 8.33522891998291
Epoch 40, val loss: 1.888588547706604
Epoch 50, training loss: 2.6663312911987305 = 1.858233094215393 + 0.1 * 8.080981254577637
Epoch 50, val loss: 1.8568166494369507
Epoch 60, training loss: 2.5828347206115723 = 1.8188446760177612 + 0.1 * 7.639900207519531
Epoch 60, val loss: 1.8206663131713867
Epoch 70, training loss: 2.507120132446289 = 1.7778465747833252 + 0.1 * 7.292735576629639
Epoch 70, val loss: 1.783302664756775
Epoch 80, training loss: 2.433687210083008 = 1.735970377922058 + 0.1 * 6.977168083190918
Epoch 80, val loss: 1.7453086376190186
Epoch 90, training loss: 2.3681602478027344 = 1.686564326286316 + 0.1 * 6.815959453582764
Epoch 90, val loss: 1.6999015808105469
Epoch 100, training loss: 2.297593832015991 = 1.6216182708740234 + 0.1 * 6.759755611419678
Epoch 100, val loss: 1.6423341035842896
Epoch 110, training loss: 2.212451934814453 = 1.5390466451644897 + 0.1 * 6.734053134918213
Epoch 110, val loss: 1.5730230808258057
Epoch 120, training loss: 2.1114320755004883 = 1.4404670000076294 + 0.1 * 6.709650993347168
Epoch 120, val loss: 1.4917269945144653
Epoch 130, training loss: 2.0006163120269775 = 1.331749439239502 + 0.1 * 6.688668727874756
Epoch 130, val loss: 1.4033126831054688
Epoch 140, training loss: 1.8869917392730713 = 1.2196743488311768 + 0.1 * 6.673172950744629
Epoch 140, val loss: 1.3144114017486572
Epoch 150, training loss: 1.7745424509048462 = 1.1080958843231201 + 0.1 * 6.664465427398682
Epoch 150, val loss: 1.2283217906951904
Epoch 160, training loss: 1.6655418872833252 = 0.9995031356811523 + 0.1 * 6.66038703918457
Epoch 160, val loss: 1.1458027362823486
Epoch 170, training loss: 1.562164068222046 = 0.8962896466255188 + 0.1 * 6.658744812011719
Epoch 170, val loss: 1.0672703981399536
Epoch 180, training loss: 1.4669933319091797 = 0.8013027310371399 + 0.1 * 6.656905174255371
Epoch 180, val loss: 0.9948079586029053
Epoch 190, training loss: 1.3808834552764893 = 0.7156015634536743 + 0.1 * 6.65281867980957
Epoch 190, val loss: 0.9297976493835449
Epoch 200, training loss: 1.3036167621612549 = 0.638763964176178 + 0.1 * 6.6485276222229
Epoch 200, val loss: 0.8725618124008179
Epoch 210, training loss: 1.234189748764038 = 0.5698838233947754 + 0.1 * 6.6430583000183105
Epoch 210, val loss: 0.8234007358551025
Epoch 220, training loss: 1.1715781688690186 = 0.5078766942024231 + 0.1 * 6.637014389038086
Epoch 220, val loss: 0.781912088394165
Epoch 230, training loss: 1.1145868301391602 = 0.4519554376602173 + 0.1 * 6.626314640045166
Epoch 230, val loss: 0.7475815415382385
Epoch 240, training loss: 1.0631794929504395 = 0.4013870358467102 + 0.1 * 6.61792516708374
Epoch 240, val loss: 0.7194622159004211
Epoch 250, training loss: 1.0165107250213623 = 0.3560580313205719 + 0.1 * 6.604526519775391
Epoch 250, val loss: 0.6966624855995178
Epoch 260, training loss: 0.9741069078445435 = 0.3151671588420868 + 0.1 * 6.589396953582764
Epoch 260, val loss: 0.6779383420944214
Epoch 270, training loss: 0.9359517097473145 = 0.27857762575149536 + 0.1 * 6.5737409591674805
Epoch 270, val loss: 0.6631952524185181
Epoch 280, training loss: 0.9020509719848633 = 0.24623015522956848 + 0.1 * 6.5582075119018555
Epoch 280, val loss: 0.6520464420318604
Epoch 290, training loss: 0.8715412020683289 = 0.21753810346126556 + 0.1 * 6.540030479431152
Epoch 290, val loss: 0.6439985632896423
Epoch 300, training loss: 0.8493727445602417 = 0.19222715497016907 + 0.1 * 6.571455955505371
Epoch 300, val loss: 0.6389078497886658
Epoch 310, training loss: 0.8229553699493408 = 0.17046120762825012 + 0.1 * 6.524941444396973
Epoch 310, val loss: 0.6367488503456116
Epoch 320, training loss: 0.8018295764923096 = 0.1515205353498459 + 0.1 * 6.503089904785156
Epoch 320, val loss: 0.6368148326873779
Epoch 330, training loss: 0.7838333249092102 = 0.13496814668178558 + 0.1 * 6.488651752471924
Epoch 330, val loss: 0.6390308737754822
Epoch 340, training loss: 0.7692265510559082 = 0.12057740986347198 + 0.1 * 6.4864912033081055
Epoch 340, val loss: 0.6430121064186096
Epoch 350, training loss: 0.7551183700561523 = 0.1080952137708664 + 0.1 * 6.470231533050537
Epoch 350, val loss: 0.6483057737350464
Epoch 360, training loss: 0.7427602410316467 = 0.09716612100601196 + 0.1 * 6.455941200256348
Epoch 360, val loss: 0.6547330617904663
Epoch 370, training loss: 0.7323467135429382 = 0.08755868673324585 + 0.1 * 6.447880268096924
Epoch 370, val loss: 0.6620054244995117
Epoch 380, training loss: 0.7243825197219849 = 0.07915180921554565 + 0.1 * 6.452306747436523
Epoch 380, val loss: 0.6699008941650391
Epoch 390, training loss: 0.7149590849876404 = 0.07180905342102051 + 0.1 * 6.43149995803833
Epoch 390, val loss: 0.6782901883125305
Epoch 400, training loss: 0.708489179611206 = 0.06532737612724304 + 0.1 * 6.4316182136535645
Epoch 400, val loss: 0.6869978308677673
Epoch 410, training loss: 0.7016788721084595 = 0.05961024761199951 + 0.1 * 6.4206862449646
Epoch 410, val loss: 0.6960130929946899
Epoch 420, training loss: 0.6954203844070435 = 0.054542940109968185 + 0.1 * 6.408774375915527
Epoch 420, val loss: 0.7051821947097778
Epoch 430, training loss: 0.6917656064033508 = 0.05003643408417702 + 0.1 * 6.417291641235352
Epoch 430, val loss: 0.7145001888275146
Epoch 440, training loss: 0.6857594847679138 = 0.04603799805045128 + 0.1 * 6.397214889526367
Epoch 440, val loss: 0.7239417433738708
Epoch 450, training loss: 0.6822583079338074 = 0.04247738793492317 + 0.1 * 6.397809028625488
Epoch 450, val loss: 0.7332214117050171
Epoch 460, training loss: 0.6782611608505249 = 0.039305202662944794 + 0.1 * 6.389559268951416
Epoch 460, val loss: 0.742607593536377
Epoch 470, training loss: 0.6745092272758484 = 0.0364544540643692 + 0.1 * 6.380548000335693
Epoch 470, val loss: 0.751815915107727
Epoch 480, training loss: 0.6734260320663452 = 0.033886849880218506 + 0.1 * 6.395391464233398
Epoch 480, val loss: 0.760901927947998
Epoch 490, training loss: 0.6689077019691467 = 0.031583741307258606 + 0.1 * 6.373239517211914
Epoch 490, val loss: 0.7700051665306091
Epoch 500, training loss: 0.6660256385803223 = 0.029499739408493042 + 0.1 * 6.365259170532227
Epoch 500, val loss: 0.7789077758789062
Epoch 510, training loss: 0.6651268601417542 = 0.02760837972164154 + 0.1 * 6.375184535980225
Epoch 510, val loss: 0.7876371741294861
Epoch 520, training loss: 0.6615163683891296 = 0.025894595310091972 + 0.1 * 6.356217861175537
Epoch 520, val loss: 0.7963264584541321
Epoch 530, training loss: 0.6593434810638428 = 0.024331549182534218 + 0.1 * 6.350119113922119
Epoch 530, val loss: 0.8048423528671265
Epoch 540, training loss: 0.6626743674278259 = 0.022901032119989395 + 0.1 * 6.397732734680176
Epoch 540, val loss: 0.8130996227264404
Epoch 550, training loss: 0.6569703817367554 = 0.021603409200906754 + 0.1 * 6.3536696434021
Epoch 550, val loss: 0.8212324976921082
Epoch 560, training loss: 0.6550527811050415 = 0.020414866507053375 + 0.1 * 6.346378803253174
Epoch 560, val loss: 0.8292914032936096
Epoch 570, training loss: 0.65261310338974 = 0.019323019310832024 + 0.1 * 6.3329010009765625
Epoch 570, val loss: 0.8370035886764526
Epoch 580, training loss: 0.6507408618927002 = 0.01831795461475849 + 0.1 * 6.3242292404174805
Epoch 580, val loss: 0.8447645306587219
Epoch 590, training loss: 0.6508025527000427 = 0.017388848587870598 + 0.1 * 6.334137439727783
Epoch 590, val loss: 0.852270781993866
Epoch 600, training loss: 0.6486384272575378 = 0.016533000394701958 + 0.1 * 6.321053981781006
Epoch 600, val loss: 0.859566867351532
Epoch 610, training loss: 0.6469255685806274 = 0.01574057899415493 + 0.1 * 6.311850070953369
Epoch 610, val loss: 0.8668639659881592
Epoch 620, training loss: 0.6473766565322876 = 0.015004180371761322 + 0.1 * 6.323724746704102
Epoch 620, val loss: 0.8738610148429871
Epoch 630, training loss: 0.6455633640289307 = 0.014323665760457516 + 0.1 * 6.312397003173828
Epoch 630, val loss: 0.8807315826416016
Epoch 640, training loss: 0.6439026594161987 = 0.013691677711904049 + 0.1 * 6.302109718322754
Epoch 640, val loss: 0.8874655365943909
Epoch 650, training loss: 0.6427986025810242 = 0.013102376833558083 + 0.1 * 6.296962261199951
Epoch 650, val loss: 0.894132137298584
Epoch 660, training loss: 0.642461359500885 = 0.012551638297736645 + 0.1 * 6.299097061157227
Epoch 660, val loss: 0.9005164504051208
Epoch 670, training loss: 0.6415483355522156 = 0.01203826628625393 + 0.1 * 6.295100212097168
Epoch 670, val loss: 0.9069437980651855
Epoch 680, training loss: 0.6407073140144348 = 0.011556657962501049 + 0.1 * 6.291506290435791
Epoch 680, val loss: 0.9131736755371094
Epoch 690, training loss: 0.6430221199989319 = 0.011105109937489033 + 0.1 * 6.319169998168945
Epoch 690, val loss: 0.9192683100700378
Epoch 700, training loss: 0.6385644674301147 = 0.010682421736419201 + 0.1 * 6.278820514678955
Epoch 700, val loss: 0.9251490235328674
Epoch 710, training loss: 0.639400839805603 = 0.010286062024533749 + 0.1 * 6.2911481857299805
Epoch 710, val loss: 0.9310145378112793
Epoch 720, training loss: 0.6369202136993408 = 0.00991304125636816 + 0.1 * 6.270071983337402
Epoch 720, val loss: 0.936763346195221
Epoch 730, training loss: 0.6375249624252319 = 0.009561233222484589 + 0.1 * 6.279636859893799
Epoch 730, val loss: 0.9423732757568359
Epoch 740, training loss: 0.6355290412902832 = 0.009229511022567749 + 0.1 * 6.262994766235352
Epoch 740, val loss: 0.9478369355201721
Epoch 750, training loss: 0.6357452273368835 = 0.008916331455111504 + 0.1 * 6.268289089202881
Epoch 750, val loss: 0.9532229900360107
Epoch 760, training loss: 0.6343178153038025 = 0.008621353656053543 + 0.1 * 6.256964683532715
Epoch 760, val loss: 0.9584997296333313
Epoch 770, training loss: 0.6354097723960876 = 0.008341745473444462 + 0.1 * 6.2706804275512695
Epoch 770, val loss: 0.9637263417243958
Epoch 780, training loss: 0.6332289576530457 = 0.008076867088675499 + 0.1 * 6.251521110534668
Epoch 780, val loss: 0.9688225388526917
Epoch 790, training loss: 0.6356059908866882 = 0.00782519206404686 + 0.1 * 6.277807712554932
Epoch 790, val loss: 0.9737970232963562
Epoch 800, training loss: 0.6331272125244141 = 0.007587578613311052 + 0.1 * 6.255396366119385
Epoch 800, val loss: 0.978515625
Epoch 810, training loss: 0.6324481964111328 = 0.007361569441854954 + 0.1 * 6.250865936279297
Epoch 810, val loss: 0.9834611415863037
Epoch 820, training loss: 0.6309201121330261 = 0.007146264426410198 + 0.1 * 6.237738609313965
Epoch 820, val loss: 0.9881221055984497
Epoch 830, training loss: 0.6314722299575806 = 0.006941724102944136 + 0.1 * 6.245305061340332
Epoch 830, val loss: 0.9927892684936523
Epoch 840, training loss: 0.6308372616767883 = 0.006746589671820402 + 0.1 * 6.240906715393066
Epoch 840, val loss: 0.9972237944602966
Epoch 850, training loss: 0.6307341456413269 = 0.006561026442795992 + 0.1 * 6.241730690002441
Epoch 850, val loss: 1.0016790628433228
Epoch 860, training loss: 0.6294364929199219 = 0.006383738946169615 + 0.1 * 6.230527877807617
Epoch 860, val loss: 1.0061416625976562
Epoch 870, training loss: 0.6306698322296143 = 0.006213795859366655 + 0.1 * 6.244560241699219
Epoch 870, val loss: 1.0104825496673584
Epoch 880, training loss: 0.6291147470474243 = 0.006051948759704828 + 0.1 * 6.23062801361084
Epoch 880, val loss: 1.0144903659820557
Epoch 890, training loss: 0.6291785836219788 = 0.00589764816686511 + 0.1 * 6.232809066772461
Epoch 890, val loss: 1.0186824798583984
Epoch 900, training loss: 0.6282486915588379 = 0.005749550648033619 + 0.1 * 6.224991321563721
Epoch 900, val loss: 1.0227688550949097
Epoch 910, training loss: 0.6269496083259583 = 0.005608222912997007 + 0.1 * 6.213414192199707
Epoch 910, val loss: 1.0267767906188965
Epoch 920, training loss: 0.6272051930427551 = 0.005472563672810793 + 0.1 * 6.217326641082764
Epoch 920, val loss: 1.0307927131652832
Epoch 930, training loss: 0.6277600526809692 = 0.005342135205864906 + 0.1 * 6.224179267883301
Epoch 930, val loss: 1.034679651260376
Epoch 940, training loss: 0.6268118619918823 = 0.005217067431658506 + 0.1 * 6.21594762802124
Epoch 940, val loss: 1.0383397340774536
Epoch 950, training loss: 0.6262969970703125 = 0.005097619723528624 + 0.1 * 6.21199369430542
Epoch 950, val loss: 1.0421051979064941
Epoch 960, training loss: 0.6262593269348145 = 0.0049823131412267685 + 0.1 * 6.212769985198975
Epoch 960, val loss: 1.0458388328552246
Epoch 970, training loss: 0.6250259876251221 = 0.004871616140007973 + 0.1 * 6.201543807983398
Epoch 970, val loss: 1.0495012998580933
Epoch 980, training loss: 0.6262663006782532 = 0.00476488284766674 + 0.1 * 6.215014457702637
Epoch 980, val loss: 1.05307137966156
Epoch 990, training loss: 0.6255896091461182 = 0.004662320017814636 + 0.1 * 6.209272861480713
Epoch 990, val loss: 1.0565215349197388
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8229
Flip ASR: 0.7911/225 nodes
The final ASR:0.75154, 0.05054, Accuracy:0.81975, 0.01492
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98524, 0.00904, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.774901866912842 = 1.937513828277588 + 0.1 * 8.373880386352539
Epoch 0, val loss: 1.940985083580017
Epoch 10, training loss: 2.765592575073242 = 1.9282134771347046 + 0.1 * 8.37378978729248
Epoch 10, val loss: 1.9323745965957642
Epoch 20, training loss: 2.754267930984497 = 1.916943907737732 + 0.1 * 8.37324047088623
Epoch 20, val loss: 1.9215655326843262
Epoch 30, training loss: 2.73824143409729 = 1.9013707637786865 + 0.1 * 8.368706703186035
Epoch 30, val loss: 1.906495213508606
Epoch 40, training loss: 2.711862564086914 = 1.8785954713821411 + 0.1 * 8.332671165466309
Epoch 40, val loss: 1.884700059890747
Epoch 50, training loss: 2.657252550125122 = 1.8466672897338867 + 0.1 * 8.105852127075195
Epoch 50, val loss: 1.8551702499389648
Epoch 60, training loss: 2.5682373046875 = 1.807762861251831 + 0.1 * 7.604743957519531
Epoch 60, val loss: 1.8203092813491821
Epoch 70, training loss: 2.5036873817443848 = 1.7676516771316528 + 0.1 * 7.360357284545898
Epoch 70, val loss: 1.7856659889221191
Epoch 80, training loss: 2.4381768703460693 = 1.7246696949005127 + 0.1 * 7.13507080078125
Epoch 80, val loss: 1.7488304376602173
Epoch 90, training loss: 2.3664121627807617 = 1.6694096326828003 + 0.1 * 6.970024108886719
Epoch 90, val loss: 1.70151948928833
Epoch 100, training loss: 2.2834370136260986 = 1.5954086780548096 + 0.1 * 6.880282402038574
Epoch 100, val loss: 1.6386653184890747
Epoch 110, training loss: 2.186995029449463 = 1.5038353204727173 + 0.1 * 6.831597328186035
Epoch 110, val loss: 1.5643882751464844
Epoch 120, training loss: 2.080125570297241 = 1.4019917249679565 + 0.1 * 6.781338214874268
Epoch 120, val loss: 1.4791643619537354
Epoch 130, training loss: 1.9713685512542725 = 1.29840886592865 + 0.1 * 6.729596138000488
Epoch 130, val loss: 1.3939743041992188
Epoch 140, training loss: 1.8655614852905273 = 1.197336196899414 + 0.1 * 6.682253360748291
Epoch 140, val loss: 1.3125364780426025
Epoch 150, training loss: 1.764983057975769 = 1.1006046533584595 + 0.1 * 6.643784046173096
Epoch 150, val loss: 1.2373945713043213
Epoch 160, training loss: 1.6709377765655518 = 1.0085538625717163 + 0.1 * 6.623838901519775
Epoch 160, val loss: 1.1673896312713623
Epoch 170, training loss: 1.5827820301055908 = 0.9224443435668945 + 0.1 * 6.6033759117126465
Epoch 170, val loss: 1.1025480031967163
Epoch 180, training loss: 1.5006158351898193 = 0.8415631055831909 + 0.1 * 6.590526580810547
Epoch 180, val loss: 1.0420304536819458
Epoch 190, training loss: 1.4249255657196045 = 0.7663743495941162 + 0.1 * 6.585511684417725
Epoch 190, val loss: 0.9871225953102112
Epoch 200, training loss: 1.3551076650619507 = 0.6975216269493103 + 0.1 * 6.575860023498535
Epoch 200, val loss: 0.9393817186355591
Epoch 210, training loss: 1.2906196117401123 = 0.6334412097930908 + 0.1 * 6.571783542633057
Epoch 210, val loss: 0.8978060483932495
Epoch 220, training loss: 1.2296514511108398 = 0.5725769996643066 + 0.1 * 6.57074499130249
Epoch 220, val loss: 0.8611682057380676
Epoch 230, training loss: 1.1714820861816406 = 0.5147420763969421 + 0.1 * 6.567399978637695
Epoch 230, val loss: 0.8288755416870117
Epoch 240, training loss: 1.1161768436431885 = 0.46011579036712646 + 0.1 * 6.560610771179199
Epoch 240, val loss: 0.8000835180282593
Epoch 250, training loss: 1.065080165863037 = 0.40942439436912537 + 0.1 * 6.556557655334473
Epoch 250, val loss: 0.7748439311981201
Epoch 260, training loss: 1.0189253091812134 = 0.3635670840740204 + 0.1 * 6.553581714630127
Epoch 260, val loss: 0.7541444897651672
Epoch 270, training loss: 0.9778853058815002 = 0.3228563070297241 + 0.1 * 6.550289630889893
Epoch 270, val loss: 0.7384517788887024
Epoch 280, training loss: 0.9420332908630371 = 0.2867777645587921 + 0.1 * 6.552554607391357
Epoch 280, val loss: 0.727729082107544
Epoch 290, training loss: 0.9090060591697693 = 0.25499534606933594 + 0.1 * 6.540106773376465
Epoch 290, val loss: 0.7218374013900757
Epoch 300, training loss: 0.8800908327102661 = 0.22677505016326904 + 0.1 * 6.533157825469971
Epoch 300, val loss: 0.7200515270233154
Epoch 310, training loss: 0.8546379804611206 = 0.20168931782245636 + 0.1 * 6.529486656188965
Epoch 310, val loss: 0.7216300368309021
Epoch 320, training loss: 0.8317172527313232 = 0.1796647608280182 + 0.1 * 6.520524501800537
Epoch 320, val loss: 0.7259796261787415
Epoch 330, training loss: 0.8118836879730225 = 0.1604471355676651 + 0.1 * 6.5143656730651855
Epoch 330, val loss: 0.7324236631393433
Epoch 340, training loss: 0.793899655342102 = 0.1436404585838318 + 0.1 * 6.502592086791992
Epoch 340, val loss: 0.7406035661697388
Epoch 350, training loss: 0.7797103524208069 = 0.12899671494960785 + 0.1 * 6.507136344909668
Epoch 350, val loss: 0.7500084638595581
Epoch 360, training loss: 0.7647157311439514 = 0.11624554544687271 + 0.1 * 6.484701633453369
Epoch 360, val loss: 0.760378360748291
Epoch 370, training loss: 0.7530858516693115 = 0.10499195754528046 + 0.1 * 6.48093843460083
Epoch 370, val loss: 0.7713974714279175
Epoch 380, training loss: 0.7418532371520996 = 0.095057912170887 + 0.1 * 6.467953205108643
Epoch 380, val loss: 0.7827695608139038
Epoch 390, training loss: 0.7321917414665222 = 0.0862368792295456 + 0.1 * 6.459548473358154
Epoch 390, val loss: 0.7944132685661316
Epoch 400, training loss: 0.7234277129173279 = 0.07835730165243149 + 0.1 * 6.450704097747803
Epoch 400, val loss: 0.8064225316047668
Epoch 410, training loss: 0.717812180519104 = 0.07130523771047592 + 0.1 * 6.46506929397583
Epoch 410, val loss: 0.8185081481933594
Epoch 420, training loss: 0.7097463607788086 = 0.06504985690116882 + 0.1 * 6.446965217590332
Epoch 420, val loss: 0.8307051062583923
Epoch 430, training loss: 0.7025602459907532 = 0.05947379395365715 + 0.1 * 6.430864334106445
Epoch 430, val loss: 0.8428370356559753
Epoch 440, training loss: 0.6971921920776367 = 0.05449679493904114 + 0.1 * 6.4269537925720215
Epoch 440, val loss: 0.8548987507820129
Epoch 450, training loss: 0.6923186182975769 = 0.05006909370422363 + 0.1 * 6.422494888305664
Epoch 450, val loss: 0.8670160174369812
Epoch 460, training loss: 0.6875725388526917 = 0.04611578211188316 + 0.1 * 6.414567947387695
Epoch 460, val loss: 0.8788198232650757
Epoch 470, training loss: 0.6828213930130005 = 0.042582932859659195 + 0.1 * 6.402384281158447
Epoch 470, val loss: 0.8905743360519409
Epoch 480, training loss: 0.681505024433136 = 0.039419468492269516 + 0.1 * 6.420855522155762
Epoch 480, val loss: 0.901922881603241
Epoch 490, training loss: 0.675038754940033 = 0.036600179970264435 + 0.1 * 6.384385585784912
Epoch 490, val loss: 0.9132922291755676
Epoch 500, training loss: 0.6718814373016357 = 0.03406735509634018 + 0.1 * 6.378140926361084
Epoch 500, val loss: 0.9243203401565552
Epoch 510, training loss: 0.6705177426338196 = 0.03178255632519722 + 0.1 * 6.3873515129089355
Epoch 510, val loss: 0.9349521398544312
Epoch 520, training loss: 0.6658395528793335 = 0.029721835628151894 + 0.1 * 6.36117696762085
Epoch 520, val loss: 0.9455491304397583
Epoch 530, training loss: 0.663541316986084 = 0.027852501720190048 + 0.1 * 6.356888294219971
Epoch 530, val loss: 0.9558732509613037
Epoch 540, training loss: 0.6624981760978699 = 0.02615736424922943 + 0.1 * 6.363407611846924
Epoch 540, val loss: 0.9655780792236328
Epoch 550, training loss: 0.6590963006019592 = 0.024619106203317642 + 0.1 * 6.344771862030029
Epoch 550, val loss: 0.975566029548645
Epoch 560, training loss: 0.6567098498344421 = 0.023211242631077766 + 0.1 * 6.334985733032227
Epoch 560, val loss: 0.9850403070449829
Epoch 570, training loss: 0.6572666764259338 = 0.021918296813964844 + 0.1 * 6.3534836769104
Epoch 570, val loss: 0.9943990111351013
Epoch 580, training loss: 0.6538723111152649 = 0.02073516510426998 + 0.1 * 6.331371307373047
Epoch 580, val loss: 1.0032316446304321
Epoch 590, training loss: 0.6514566540718079 = 0.01964784413576126 + 0.1 * 6.318088054656982
Epoch 590, val loss: 1.0122981071472168
Epoch 600, training loss: 0.6527318358421326 = 0.01864379271864891 + 0.1 * 6.340880393981934
Epoch 600, val loss: 1.0208278894424438
Epoch 610, training loss: 0.6489505767822266 = 0.0177180003374815 + 0.1 * 6.312325477600098
Epoch 610, val loss: 1.0290905237197876
Epoch 620, training loss: 0.647243082523346 = 0.016862094402313232 + 0.1 * 6.303809642791748
Epoch 620, val loss: 1.0374963283538818
Epoch 630, training loss: 0.6468374133110046 = 0.016067197546362877 + 0.1 * 6.30770206451416
Epoch 630, val loss: 1.0455317497253418
Epoch 640, training loss: 0.6445023417472839 = 0.015328485518693924 + 0.1 * 6.291738510131836
Epoch 640, val loss: 1.0533556938171387
Epoch 650, training loss: 0.6453520655632019 = 0.01464033406227827 + 0.1 * 6.307116985321045
Epoch 650, val loss: 1.061049461364746
Epoch 660, training loss: 0.644524872303009 = 0.013999703340232372 + 0.1 * 6.305251598358154
Epoch 660, val loss: 1.0685769319534302
Epoch 670, training loss: 0.6426813006401062 = 0.01340249739587307 + 0.1 * 6.292787551879883
Epoch 670, val loss: 1.0756949186325073
Epoch 680, training loss: 0.6404743194580078 = 0.01284661516547203 + 0.1 * 6.2762770652771
Epoch 680, val loss: 1.0830940008163452
Epoch 690, training loss: 0.6399301886558533 = 0.012325306423008442 + 0.1 * 6.2760491371154785
Epoch 690, val loss: 1.0901427268981934
Epoch 700, training loss: 0.6391943693161011 = 0.011835704557597637 + 0.1 * 6.273586750030518
Epoch 700, val loss: 1.0969102382659912
Epoch 710, training loss: 0.6382573246955872 = 0.011376265436410904 + 0.1 * 6.268810272216797
Epoch 710, val loss: 1.1034640073776245
Epoch 720, training loss: 0.6374059319496155 = 0.010945637710392475 + 0.1 * 6.264603137969971
Epoch 720, val loss: 1.1100654602050781
Epoch 730, training loss: 0.6366732120513916 = 0.010540604591369629 + 0.1 * 6.261325836181641
Epoch 730, val loss: 1.1166788339614868
Epoch 740, training loss: 0.6367496252059937 = 0.010157976299524307 + 0.1 * 6.265915870666504
Epoch 740, val loss: 1.1227314472198486
Epoch 750, training loss: 0.6354056000709534 = 0.009798159822821617 + 0.1 * 6.25607442855835
Epoch 750, val loss: 1.1288230419158936
Epoch 760, training loss: 0.6345986127853394 = 0.009458721615374088 + 0.1 * 6.251399040222168
Epoch 760, val loss: 1.135048508644104
Epoch 770, training loss: 0.6341910362243652 = 0.009136947803199291 + 0.1 * 6.250540256500244
Epoch 770, val loss: 1.1407808065414429
Epoch 780, training loss: 0.6350938677787781 = 0.008832952938973904 + 0.1 * 6.262609004974365
Epoch 780, val loss: 1.1464647054672241
Epoch 790, training loss: 0.6335150003433228 = 0.008545614778995514 + 0.1 * 6.249693393707275
Epoch 790, val loss: 1.1520928144454956
Epoch 800, training loss: 0.6329399347305298 = 0.008273306302726269 + 0.1 * 6.246666431427002
Epoch 800, val loss: 1.157806396484375
Epoch 810, training loss: 0.6323336362838745 = 0.008014370687305927 + 0.1 * 6.243192195892334
Epoch 810, val loss: 1.1630618572235107
Epoch 820, training loss: 0.6325722932815552 = 0.007768821436911821 + 0.1 * 6.248034954071045
Epoch 820, val loss: 1.1684229373931885
Epoch 830, training loss: 0.6314084529876709 = 0.007535561453551054 + 0.1 * 6.2387285232543945
Epoch 830, val loss: 1.1736348867416382
Epoch 840, training loss: 0.630109429359436 = 0.00731346569955349 + 0.1 * 6.227959156036377
Epoch 840, val loss: 1.1786855459213257
Epoch 850, training loss: 0.6308377981185913 = 0.007102374918758869 + 0.1 * 6.237354278564453
Epoch 850, val loss: 1.1837631464004517
Epoch 860, training loss: 0.6291627287864685 = 0.006901062559336424 + 0.1 * 6.222616195678711
Epoch 860, val loss: 1.188557505607605
Epoch 870, training loss: 0.6293444037437439 = 0.0067096189595758915 + 0.1 * 6.22634744644165
Epoch 870, val loss: 1.193540334701538
Epoch 880, training loss: 0.6291705965995789 = 0.006526603829115629 + 0.1 * 6.226439952850342
Epoch 880, val loss: 1.1983709335327148
Epoch 890, training loss: 0.6280163526535034 = 0.0063515035435557365 + 0.1 * 6.216648101806641
Epoch 890, val loss: 1.202815294265747
Epoch 900, training loss: 0.630095899105072 = 0.0061846026219427586 + 0.1 * 6.239112854003906
Epoch 900, val loss: 1.2073787450790405
Epoch 910, training loss: 0.6273396015167236 = 0.006025323644280434 + 0.1 * 6.2131428718566895
Epoch 910, val loss: 1.2118873596191406
Epoch 920, training loss: 0.627581000328064 = 0.005872947163879871 + 0.1 * 6.217080116271973
Epoch 920, val loss: 1.2164429426193237
Epoch 930, training loss: 0.6294165849685669 = 0.005726659204810858 + 0.1 * 6.236899375915527
Epoch 930, val loss: 1.2205003499984741
Epoch 940, training loss: 0.6270337104797363 = 0.005586985498666763 + 0.1 * 6.2144670486450195
Epoch 940, val loss: 1.224627137184143
Epoch 950, training loss: 0.6265803575515747 = 0.00545385479927063 + 0.1 * 6.211264610290527
Epoch 950, val loss: 1.2290878295898438
Epoch 960, training loss: 0.625685453414917 = 0.005325503181666136 + 0.1 * 6.203599452972412
Epoch 960, val loss: 1.2333636283874512
Epoch 970, training loss: 0.6252386569976807 = 0.005201976280659437 + 0.1 * 6.200366497039795
Epoch 970, val loss: 1.2373461723327637
Epoch 980, training loss: 0.6248034834861755 = 0.005083109717816114 + 0.1 * 6.197204113006592
Epoch 980, val loss: 1.2410303354263306
Epoch 990, training loss: 0.6250839829444885 = 0.004969190806150436 + 0.1 * 6.20114803314209
Epoch 990, val loss: 1.2449043989181519
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.6273
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.775634288787842 = 1.9382426738739014 + 0.1 * 8.373915672302246
Epoch 0, val loss: 1.9400668144226074
Epoch 10, training loss: 2.7657289505004883 = 1.928346037864685 + 0.1 * 8.373827934265137
Epoch 10, val loss: 1.9297010898590088
Epoch 20, training loss: 2.7533440589904785 = 1.9160093069076538 + 0.1 * 8.373346328735352
Epoch 20, val loss: 1.9165089130401611
Epoch 30, training loss: 2.7354447841644287 = 1.8985066413879395 + 0.1 * 8.36938190460205
Epoch 30, val loss: 1.897476315498352
Epoch 40, training loss: 2.7063207626342773 = 1.872429609298706 + 0.1 * 8.338912010192871
Epoch 40, val loss: 1.8691818714141846
Epoch 50, training loss: 2.6518161296844482 = 1.8360586166381836 + 0.1 * 8.157574653625488
Epoch 50, val loss: 1.8312448263168335
Epoch 60, training loss: 2.5617363452911377 = 1.7945235967636108 + 0.1 * 7.672128200531006
Epoch 60, val loss: 1.790929913520813
Epoch 70, training loss: 2.4878416061401367 = 1.7533386945724487 + 0.1 * 7.345028877258301
Epoch 70, val loss: 1.75360906124115
Epoch 80, training loss: 2.4105446338653564 = 1.7070741653442383 + 0.1 * 7.034703731536865
Epoch 80, val loss: 1.7119050025939941
Epoch 90, training loss: 2.3359487056732178 = 1.6462295055389404 + 0.1 * 6.897192478179932
Epoch 90, val loss: 1.657759428024292
Epoch 100, training loss: 2.2501416206359863 = 1.5667784214019775 + 0.1 * 6.833631992340088
Epoch 100, val loss: 1.5896493196487427
Epoch 110, training loss: 2.151960849761963 = 1.4729770421981812 + 0.1 * 6.789838790893555
Epoch 110, val loss: 1.5124871730804443
Epoch 120, training loss: 2.049557685852051 = 1.3735038042068481 + 0.1 * 6.760538101196289
Epoch 120, val loss: 1.434328317642212
Epoch 130, training loss: 1.9486074447631836 = 1.2748923301696777 + 0.1 * 6.737151145935059
Epoch 130, val loss: 1.3602519035339355
Epoch 140, training loss: 1.8504059314727783 = 1.1788932085037231 + 0.1 * 6.715127468109131
Epoch 140, val loss: 1.2905123233795166
Epoch 150, training loss: 1.7537219524383545 = 1.0841933488845825 + 0.1 * 6.695285797119141
Epoch 150, val loss: 1.222210168838501
Epoch 160, training loss: 1.6573686599731445 = 0.9893417358398438 + 0.1 * 6.680269718170166
Epoch 160, val loss: 1.1537983417510986
Epoch 170, training loss: 1.561828851699829 = 0.894842803478241 + 0.1 * 6.669860363006592
Epoch 170, val loss: 1.0849705934524536
Epoch 180, training loss: 1.4703184366226196 = 0.8041251301765442 + 0.1 * 6.661932945251465
Epoch 180, val loss: 1.0181019306182861
Epoch 190, training loss: 1.3864566087722778 = 0.7211190462112427 + 0.1 * 6.653375625610352
Epoch 190, val loss: 0.9565712213516235
Epoch 200, training loss: 1.3121802806854248 = 0.647935152053833 + 0.1 * 6.64245080947876
Epoch 200, val loss: 0.9033292531967163
Epoch 210, training loss: 1.2481410503387451 = 0.5848522186279297 + 0.1 * 6.632887363433838
Epoch 210, val loss: 0.8595939874649048
Epoch 220, training loss: 1.1922379732131958 = 0.5303573608398438 + 0.1 * 6.618805885314941
Epoch 220, val loss: 0.8243362903594971
Epoch 230, training loss: 1.1430246829986572 = 0.4824107587337494 + 0.1 * 6.606139183044434
Epoch 230, val loss: 0.7955535054206848
Epoch 240, training loss: 1.0996193885803223 = 0.4396746754646301 + 0.1 * 6.599446773529053
Epoch 240, val loss: 0.7722505927085876
Epoch 250, training loss: 1.059602975845337 = 0.4010997712612152 + 0.1 * 6.585031986236572
Epoch 250, val loss: 0.7531952261924744
Epoch 260, training loss: 1.0234899520874023 = 0.3658674359321594 + 0.1 * 6.576225757598877
Epoch 260, val loss: 0.7379945516586304
Epoch 270, training loss: 0.9905348420143127 = 0.3337247967720032 + 0.1 * 6.568100452423096
Epoch 270, val loss: 0.7260980606079102
Epoch 280, training loss: 0.9602397084236145 = 0.30407726764678955 + 0.1 * 6.561624050140381
Epoch 280, val loss: 0.71697598695755
Epoch 290, training loss: 0.9322943687438965 = 0.276767760515213 + 0.1 * 6.5552659034729
Epoch 290, val loss: 0.7107763290405273
Epoch 300, training loss: 0.9069845080375671 = 0.25195181369781494 + 0.1 * 6.550326824188232
Epoch 300, val loss: 0.7072039246559143
Epoch 310, training loss: 0.8836238384246826 = 0.22957804799079895 + 0.1 * 6.540457248687744
Epoch 310, val loss: 0.7059110403060913
Epoch 320, training loss: 0.8629266619682312 = 0.20942626893520355 + 0.1 * 6.535003662109375
Epoch 320, val loss: 0.706790566444397
Epoch 330, training loss: 0.8441803455352783 = 0.19140711426734924 + 0.1 * 6.5277323722839355
Epoch 330, val loss: 0.7095196843147278
Epoch 340, training loss: 0.8272296190261841 = 0.1752859652042389 + 0.1 * 6.519436836242676
Epoch 340, val loss: 0.713851273059845
Epoch 350, training loss: 0.8125537037849426 = 0.16076509654521942 + 0.1 * 6.517885684967041
Epoch 350, val loss: 0.7197048664093018
Epoch 360, training loss: 0.7984957695007324 = 0.1477595567703247 + 0.1 * 6.507361888885498
Epoch 360, val loss: 0.7263689041137695
Epoch 370, training loss: 0.7853599786758423 = 0.13602659106254578 + 0.1 * 6.4933342933654785
Epoch 370, val loss: 0.7339439392089844
Epoch 380, training loss: 0.7745393514633179 = 0.1253649741411209 + 0.1 * 6.491744041442871
Epoch 380, val loss: 0.7425156235694885
Epoch 390, training loss: 0.7629842162132263 = 0.11564259976148605 + 0.1 * 6.473415851593018
Epoch 390, val loss: 0.7515735030174255
Epoch 400, training loss: 0.7536419630050659 = 0.10673550516366959 + 0.1 * 6.469064235687256
Epoch 400, val loss: 0.7610607743263245
Epoch 410, training loss: 0.7450815439224243 = 0.0985395610332489 + 0.1 * 6.465419292449951
Epoch 410, val loss: 0.7711536288261414
Epoch 420, training loss: 0.7385784983634949 = 0.0910516232252121 + 0.1 * 6.475268363952637
Epoch 420, val loss: 0.7813040018081665
Epoch 430, training loss: 0.7287411093711853 = 0.08431440591812134 + 0.1 * 6.4442667961120605
Epoch 430, val loss: 0.7915891408920288
Epoch 440, training loss: 0.7219159603118896 = 0.07822861522436142 + 0.1 * 6.436873435974121
Epoch 440, val loss: 0.8023595809936523
Epoch 450, training loss: 0.7150039672851562 = 0.0726873129606247 + 0.1 * 6.423166275024414
Epoch 450, val loss: 0.8134506940841675
Epoch 460, training loss: 0.7091200351715088 = 0.06765659898519516 + 0.1 * 6.414634704589844
Epoch 460, val loss: 0.8250000476837158
Epoch 470, training loss: 0.704670786857605 = 0.06312482059001923 + 0.1 * 6.415459156036377
Epoch 470, val loss: 0.8367640972137451
Epoch 480, training loss: 0.6991919875144958 = 0.05906002223491669 + 0.1 * 6.40131950378418
Epoch 480, val loss: 0.8485687375068665
Epoch 490, training loss: 0.6968374252319336 = 0.05537876486778259 + 0.1 * 6.414586067199707
Epoch 490, val loss: 0.8604151010513306
Epoch 500, training loss: 0.6905065774917603 = 0.05203810706734657 + 0.1 * 6.3846845626831055
Epoch 500, val loss: 0.8722022771835327
Epoch 510, training loss: 0.6870891451835632 = 0.048972055315971375 + 0.1 * 6.381170749664307
Epoch 510, val loss: 0.8838809728622437
Epoch 520, training loss: 0.6863362193107605 = 0.04616163671016693 + 0.1 * 6.401745319366455
Epoch 520, val loss: 0.8953631520271301
Epoch 530, training loss: 0.680302619934082 = 0.04357440769672394 + 0.1 * 6.367281913757324
Epoch 530, val loss: 0.906802237033844
Epoch 540, training loss: 0.6765627264976501 = 0.04117344692349434 + 0.1 * 6.3538923263549805
Epoch 540, val loss: 0.9180976152420044
Epoch 550, training loss: 0.6751957535743713 = 0.03893379494547844 + 0.1 * 6.362619400024414
Epoch 550, val loss: 0.9293015003204346
Epoch 560, training loss: 0.6714510917663574 = 0.036854714155197144 + 0.1 * 6.345963478088379
Epoch 560, val loss: 0.940394937992096
Epoch 570, training loss: 0.6685442924499512 = 0.034903690218925476 + 0.1 * 6.3364057540893555
Epoch 570, val loss: 0.9513533711433411
Epoch 580, training loss: 0.6665282249450684 = 0.03306962922215462 + 0.1 * 6.334586143493652
Epoch 580, val loss: 0.9620760083198547
Epoch 590, training loss: 0.6632956266403198 = 0.031327445060014725 + 0.1 * 6.319681644439697
Epoch 590, val loss: 0.9727707505226135
Epoch 600, training loss: 0.6617986559867859 = 0.029639258980751038 + 0.1 * 6.321593761444092
Epoch 600, val loss: 0.983342707157135
Epoch 610, training loss: 0.6594272255897522 = 0.027983393520116806 + 0.1 * 6.314438343048096
Epoch 610, val loss: 0.99357670545578
Epoch 620, training loss: 0.6575853228569031 = 0.026379669085144997 + 0.1 * 6.312056541442871
Epoch 620, val loss: 1.003667950630188
Epoch 630, training loss: 0.6553106307983398 = 0.02482140250504017 + 0.1 * 6.304892063140869
Epoch 630, val loss: 1.0137006044387817
Epoch 640, training loss: 0.653629720211029 = 0.02327270247042179 + 0.1 * 6.30357027053833
Epoch 640, val loss: 1.023561716079712
Epoch 650, training loss: 0.653250515460968 = 0.021651284769177437 + 0.1 * 6.31599235534668
Epoch 650, val loss: 1.033115029335022
Epoch 660, training loss: 0.6487147212028503 = 0.02003372274339199 + 0.1 * 6.286809921264648
Epoch 660, val loss: 1.0423444509506226
Epoch 670, training loss: 0.6462674140930176 = 0.018498459830880165 + 0.1 * 6.277689456939697
Epoch 670, val loss: 1.051546335220337
Epoch 680, training loss: 0.6452579498291016 = 0.017132248729467392 + 0.1 * 6.281257152557373
Epoch 680, val loss: 1.0604106187820435
Epoch 690, training loss: 0.6431598663330078 = 0.016008922830224037 + 0.1 * 6.271509170532227
Epoch 690, val loss: 1.0693845748901367
Epoch 700, training loss: 0.6424906253814697 = 0.015075582079589367 + 0.1 * 6.274150371551514
Epoch 700, val loss: 1.078538417816162
Epoch 710, training loss: 0.6412078738212585 = 0.014268157072365284 + 0.1 * 6.269396781921387
Epoch 710, val loss: 1.0875126123428345
Epoch 720, training loss: 0.6401335597038269 = 0.01355891115963459 + 0.1 * 6.265746116638184
Epoch 720, val loss: 1.096139907836914
Epoch 730, training loss: 0.6383023262023926 = 0.012926170602440834 + 0.1 * 6.2537617683410645
Epoch 730, val loss: 1.1045966148376465
Epoch 740, training loss: 0.6379327774047852 = 0.012352565303444862 + 0.1 * 6.255801677703857
Epoch 740, val loss: 1.1127121448516846
Epoch 750, training loss: 0.6373851299285889 = 0.011827750131487846 + 0.1 * 6.255573272705078
Epoch 750, val loss: 1.120866298675537
Epoch 760, training loss: 0.6371783018112183 = 0.011344079859554768 + 0.1 * 6.258342266082764
Epoch 760, val loss: 1.128722071647644
Epoch 770, training loss: 0.6352739930152893 = 0.010898158885538578 + 0.1 * 6.243758201599121
Epoch 770, val loss: 1.136221170425415
Epoch 780, training loss: 0.6341696977615356 = 0.010483942925930023 + 0.1 * 6.236857891082764
Epoch 780, val loss: 1.1437803506851196
Epoch 790, training loss: 0.6349534392356873 = 0.010096743702888489 + 0.1 * 6.2485671043396
Epoch 790, val loss: 1.1511056423187256
Epoch 800, training loss: 0.6339850425720215 = 0.009735013358294964 + 0.1 * 6.242500305175781
Epoch 800, val loss: 1.1582443714141846
Epoch 810, training loss: 0.6337634325027466 = 0.009395770728588104 + 0.1 * 6.24367618560791
Epoch 810, val loss: 1.1652841567993164
Epoch 820, training loss: 0.6325085759162903 = 0.009077680297195911 + 0.1 * 6.234309196472168
Epoch 820, val loss: 1.172207236289978
Epoch 830, training loss: 0.6315358281135559 = 0.008777623996138573 + 0.1 * 6.227581977844238
Epoch 830, val loss: 1.1790202856063843
Epoch 840, training loss: 0.632271409034729 = 0.00849539041519165 + 0.1 * 6.237760066986084
Epoch 840, val loss: 1.1856571435928345
Epoch 850, training loss: 0.6314338445663452 = 0.008228524588048458 + 0.1 * 6.232052803039551
Epoch 850, val loss: 1.192122220993042
Epoch 860, training loss: 0.6291312575340271 = 0.007977016270160675 + 0.1 * 6.211542129516602
Epoch 860, val loss: 1.1984914541244507
Epoch 870, training loss: 0.6287115812301636 = 0.007738998159766197 + 0.1 * 6.209725856781006
Epoch 870, val loss: 1.204757809638977
Epoch 880, training loss: 0.6299837231636047 = 0.007512266281992197 + 0.1 * 6.224714279174805
Epoch 880, val loss: 1.210999608039856
Epoch 890, training loss: 0.6290625929832458 = 0.007297806907445192 + 0.1 * 6.217647552490234
Epoch 890, val loss: 1.2168267965316772
Epoch 900, training loss: 0.6280193328857422 = 0.007093831431120634 + 0.1 * 6.209254741668701
Epoch 900, val loss: 1.2228422164916992
Epoch 910, training loss: 0.6270801424980164 = 0.0068992143496870995 + 0.1 * 6.201808929443359
Epoch 910, val loss: 1.2286256551742554
Epoch 920, training loss: 0.6268218755722046 = 0.0067144036293029785 + 0.1 * 6.201074600219727
Epoch 920, val loss: 1.2343428134918213
Epoch 930, training loss: 0.6271989345550537 = 0.0065373932011425495 + 0.1 * 6.206615447998047
Epoch 930, val loss: 1.2399065494537354
Epoch 940, training loss: 0.6258298754692078 = 0.006369562353938818 + 0.1 * 6.194602966308594
Epoch 940, val loss: 1.2454636096954346
Epoch 950, training loss: 0.6262282133102417 = 0.006208773702383041 + 0.1 * 6.200194358825684
Epoch 950, val loss: 1.2509546279907227
Epoch 960, training loss: 0.6251181364059448 = 0.006054810713976622 + 0.1 * 6.1906328201293945
Epoch 960, val loss: 1.2563239336013794
Epoch 970, training loss: 0.6244630813598633 = 0.005907418671995401 + 0.1 * 6.185556411743164
Epoch 970, val loss: 1.2616220712661743
Epoch 980, training loss: 0.6244834661483765 = 0.005766137037426233 + 0.1 * 6.187172889709473
Epoch 980, val loss: 1.2668206691741943
Epoch 990, training loss: 0.6241912841796875 = 0.005630820989608765 + 0.1 * 6.185604095458984
Epoch 990, val loss: 1.271911859512329
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6974
Flip ASR: 0.6533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7743072509765625 = 1.9369210004806519 + 0.1 * 8.373862266540527
Epoch 0, val loss: 1.9330717325210571
Epoch 10, training loss: 2.7640652656555176 = 1.9267023801803589 + 0.1 * 8.373627662658691
Epoch 10, val loss: 1.922663927078247
Epoch 20, training loss: 2.751621961593628 = 1.914415955543518 + 0.1 * 8.37205982208252
Epoch 20, val loss: 1.9094921350479126
Epoch 30, training loss: 2.7335593700408936 = 1.8973851203918457 + 0.1 * 8.36174201965332
Epoch 30, val loss: 1.8906971216201782
Epoch 40, training loss: 2.7028777599334717 = 1.8725999593734741 + 0.1 * 8.302777290344238
Epoch 40, val loss: 1.8634285926818848
Epoch 50, training loss: 2.639261245727539 = 1.8390706777572632 + 0.1 * 8.001906394958496
Epoch 50, val loss: 1.8282334804534912
Epoch 60, training loss: 2.549978494644165 = 1.7995189428329468 + 0.1 * 7.504594802856445
Epoch 60, val loss: 1.7900112867355347
Epoch 70, training loss: 2.480121612548828 = 1.7583624124526978 + 0.1 * 7.217591762542725
Epoch 70, val loss: 1.7534157037734985
Epoch 80, training loss: 2.420224666595459 = 1.713261604309082 + 0.1 * 7.0696306228637695
Epoch 80, val loss: 1.7156165838241577
Epoch 90, training loss: 2.3548245429992676 = 1.659287452697754 + 0.1 * 6.955371856689453
Epoch 90, val loss: 1.6704156398773193
Epoch 100, training loss: 2.2755422592163086 = 1.5883361101150513 + 0.1 * 6.872061729431152
Epoch 100, val loss: 1.612056851387024
Epoch 110, training loss: 2.1811647415161133 = 1.5004491806030273 + 0.1 * 6.807156085968018
Epoch 110, val loss: 1.5437841415405273
Epoch 120, training loss: 2.0804128646850586 = 1.4037604331970215 + 0.1 * 6.766523361206055
Epoch 120, val loss: 1.4709172248840332
Epoch 130, training loss: 1.9837141036987305 = 1.309256672859192 + 0.1 * 6.744573593139648
Epoch 130, val loss: 1.4029206037521362
Epoch 140, training loss: 1.8936662673950195 = 1.2212512493133545 + 0.1 * 6.724150657653809
Epoch 140, val loss: 1.3411219120025635
Epoch 150, training loss: 1.8069627285003662 = 1.1369640827178955 + 0.1 * 6.699986934661865
Epoch 150, val loss: 1.282469630241394
Epoch 160, training loss: 1.7191624641418457 = 1.0516785383224487 + 0.1 * 6.674839019775391
Epoch 160, val loss: 1.223797082901001
Epoch 170, training loss: 1.6281405687332153 = 0.962558388710022 + 0.1 * 6.655821800231934
Epoch 170, val loss: 1.1630429029464722
Epoch 180, training loss: 1.5367238521575928 = 0.8720961213111877 + 0.1 * 6.646277904510498
Epoch 180, val loss: 1.1020004749298096
Epoch 190, training loss: 1.4499119520187378 = 0.7860289216041565 + 0.1 * 6.638830184936523
Epoch 190, val loss: 1.044112205505371
Epoch 200, training loss: 1.3714516162872314 = 0.7084618806838989 + 0.1 * 6.629896640777588
Epoch 200, val loss: 0.9928080439567566
Epoch 210, training loss: 1.3023645877838135 = 0.6404432058334351 + 0.1 * 6.6192145347595215
Epoch 210, val loss: 0.9500790238380432
Epoch 220, training loss: 1.2415404319763184 = 0.5806921720504761 + 0.1 * 6.60848331451416
Epoch 220, val loss: 0.9148730039596558
Epoch 230, training loss: 1.1869730949401855 = 0.5272471308708191 + 0.1 * 6.597259521484375
Epoch 230, val loss: 0.8861440420150757
Epoch 240, training loss: 1.1369341611862183 = 0.47835370898246765 + 0.1 * 6.5858049392700195
Epoch 240, val loss: 0.8634049296379089
Epoch 250, training loss: 1.0912151336669922 = 0.43386438488960266 + 0.1 * 6.573506832122803
Epoch 250, val loss: 0.8466214537620544
Epoch 260, training loss: 1.0498079061508179 = 0.39333590865135193 + 0.1 * 6.564720153808594
Epoch 260, val loss: 0.8346588015556335
Epoch 270, training loss: 1.0118211507797241 = 0.3561856746673584 + 0.1 * 6.556354522705078
Epoch 270, val loss: 0.8261693716049194
Epoch 280, training loss: 0.9768457412719727 = 0.32193121314048767 + 0.1 * 6.549145698547363
Epoch 280, val loss: 0.8202989101409912
Epoch 290, training loss: 0.9445900321006775 = 0.29009056091308594 + 0.1 * 6.544994354248047
Epoch 290, val loss: 0.8159536719322205
Epoch 300, training loss: 0.9136892557144165 = 0.26000848412513733 + 0.1 * 6.536807060241699
Epoch 300, val loss: 0.812205970287323
Epoch 310, training loss: 0.8849974870681763 = 0.23153862357139587 + 0.1 * 6.53458833694458
Epoch 310, val loss: 0.8090572357177734
Epoch 320, training loss: 0.8580785393714905 = 0.2048717886209488 + 0.1 * 6.532067775726318
Epoch 320, val loss: 0.8064630627632141
Epoch 330, training loss: 0.8328107595443726 = 0.18020404875278473 + 0.1 * 6.52606725692749
Epoch 330, val loss: 0.8045616745948792
Epoch 340, training loss: 0.8101561665534973 = 0.15812939405441284 + 0.1 * 6.520267486572266
Epoch 340, val loss: 0.804214596748352
Epoch 350, training loss: 0.7902991771697998 = 0.13875874876976013 + 0.1 * 6.515404224395752
Epoch 350, val loss: 0.8056083917617798
Epoch 360, training loss: 0.7736485004425049 = 0.12199220061302185 + 0.1 * 6.516563415527344
Epoch 360, val loss: 0.8091140389442444
Epoch 370, training loss: 0.7582725882530212 = 0.10762635618448257 + 0.1 * 6.506462097167969
Epoch 370, val loss: 0.8139982223510742
Epoch 380, training loss: 0.7463144659996033 = 0.09528131783008575 + 0.1 * 6.510331630706787
Epoch 380, val loss: 0.8199331164360046
Epoch 390, training loss: 0.7335541844367981 = 0.08469923585653305 + 0.1 * 6.488549709320068
Epoch 390, val loss: 0.8269789814949036
Epoch 400, training loss: 0.7240437865257263 = 0.07557296007871628 + 0.1 * 6.484708309173584
Epoch 400, val loss: 0.8346377611160278
Epoch 410, training loss: 0.7164049744606018 = 0.06770169734954834 + 0.1 * 6.487032890319824
Epoch 410, val loss: 0.842915415763855
Epoch 420, training loss: 0.7085183262825012 = 0.06094326078891754 + 0.1 * 6.47575044631958
Epoch 420, val loss: 0.8514794707298279
Epoch 430, training loss: 0.7008937001228333 = 0.055083371698856354 + 0.1 * 6.458102703094482
Epoch 430, val loss: 0.8603236079216003
Epoch 440, training loss: 0.6966255307197571 = 0.04998760297894478 + 0.1 * 6.466379165649414
Epoch 440, val loss: 0.8693231344223022
Epoch 450, training loss: 0.6903094053268433 = 0.04556678235530853 + 0.1 * 6.447425842285156
Epoch 450, val loss: 0.878407895565033
Epoch 460, training loss: 0.6849629282951355 = 0.04169955104589462 + 0.1 * 6.432633876800537
Epoch 460, val loss: 0.887451171875
Epoch 470, training loss: 0.681469738483429 = 0.03830709680914879 + 0.1 * 6.431626319885254
Epoch 470, val loss: 0.8963351249694824
Epoch 480, training loss: 0.6765840649604797 = 0.03531065210700035 + 0.1 * 6.412734508514404
Epoch 480, val loss: 0.9051730632781982
Epoch 490, training loss: 0.6732761859893799 = 0.0326579287648201 + 0.1 * 6.406182765960693
Epoch 490, val loss: 0.9138477444648743
Epoch 500, training loss: 0.6726392507553101 = 0.03030405566096306 + 0.1 * 6.423351764678955
Epoch 500, val loss: 0.9223437309265137
Epoch 510, training loss: 0.6674111485481262 = 0.028214460238814354 + 0.1 * 6.391966819763184
Epoch 510, val loss: 0.9306451678276062
Epoch 520, training loss: 0.6648162603378296 = 0.026343509554862976 + 0.1 * 6.384727478027344
Epoch 520, val loss: 0.9388853907585144
Epoch 530, training loss: 0.662682056427002 = 0.024654092267155647 + 0.1 * 6.380279541015625
Epoch 530, val loss: 0.9469018578529358
Epoch 540, training loss: 0.6605907678604126 = 0.02312176115810871 + 0.1 * 6.37468957901001
Epoch 540, val loss: 0.9548091292381287
Epoch 550, training loss: 0.660376250743866 = 0.021735232323408127 + 0.1 * 6.386409759521484
Epoch 550, val loss: 0.9624540209770203
Epoch 560, training loss: 0.6571899652481079 = 0.020478976890444756 + 0.1 * 6.367109775543213
Epoch 560, val loss: 0.9700672030448914
Epoch 570, training loss: 0.655020534992218 = 0.019331330433487892 + 0.1 * 6.356892108917236
Epoch 570, val loss: 0.9774689674377441
Epoch 580, training loss: 0.6550545692443848 = 0.018280811607837677 + 0.1 * 6.367737770080566
Epoch 580, val loss: 0.9845851063728333
Epoch 590, training loss: 0.6528142094612122 = 0.017322544008493423 + 0.1 * 6.354916572570801
Epoch 590, val loss: 0.9916200041770935
Epoch 600, training loss: 0.6513525247573853 = 0.016441889107227325 + 0.1 * 6.349106311798096
Epoch 600, val loss: 0.9985370635986328
Epoch 610, training loss: 0.6495577692985535 = 0.015629064291715622 + 0.1 * 6.339287281036377
Epoch 610, val loss: 1.0052378177642822
Epoch 620, training loss: 0.6529265642166138 = 0.014877722598612309 + 0.1 * 6.380488395690918
Epoch 620, val loss: 1.0118250846862793
Epoch 630, training loss: 0.6483609080314636 = 0.014189144596457481 + 0.1 * 6.341717720031738
Epoch 630, val loss: 1.0181416273117065
Epoch 640, training loss: 0.6463994979858398 = 0.013552486896514893 + 0.1 * 6.328469753265381
Epoch 640, val loss: 1.0245356559753418
Epoch 650, training loss: 0.6452183127403259 = 0.012960131280124187 + 0.1 * 6.322581768035889
Epoch 650, val loss: 1.0306962728500366
Epoch 660, training loss: 0.6457070708274841 = 0.012407795526087284 + 0.1 * 6.3329925537109375
Epoch 660, val loss: 1.0366404056549072
Epoch 670, training loss: 0.6446056365966797 = 0.011893306858837605 + 0.1 * 6.327122688293457
Epoch 670, val loss: 1.0425664186477661
Epoch 680, training loss: 0.6434631943702698 = 0.011412721127271652 + 0.1 * 6.320504188537598
Epoch 680, val loss: 1.0483561754226685
Epoch 690, training loss: 0.6414135694503784 = 0.01096406951546669 + 0.1 * 6.304494380950928
Epoch 690, val loss: 1.0539882183074951
Epoch 700, training loss: 0.6407116055488586 = 0.010544363409280777 + 0.1 * 6.301672458648682
Epoch 700, val loss: 1.0595896244049072
Epoch 710, training loss: 0.6407845616340637 = 0.010150575079023838 + 0.1 * 6.306340217590332
Epoch 710, val loss: 1.0649665594100952
Epoch 720, training loss: 0.6392269134521484 = 0.00978260301053524 + 0.1 * 6.294443130493164
Epoch 720, val loss: 1.0702874660491943
Epoch 730, training loss: 0.639072835445404 = 0.009436340071260929 + 0.1 * 6.296364784240723
Epoch 730, val loss: 1.0754762887954712
Epoch 740, training loss: 0.6387407183647156 = 0.00911047961562872 + 0.1 * 6.296302318572998
Epoch 740, val loss: 1.0805753469467163
Epoch 750, training loss: 0.6393123269081116 = 0.00880217831581831 + 0.1 * 6.30510139465332
Epoch 750, val loss: 1.0855913162231445
Epoch 760, training loss: 0.637115478515625 = 0.008512084372341633 + 0.1 * 6.286034107208252
Epoch 760, val loss: 1.090383529663086
Epoch 770, training loss: 0.6358404159545898 = 0.008237411268055439 + 0.1 * 6.27603006362915
Epoch 770, val loss: 1.09522545337677
Epoch 780, training loss: 0.6381908059120178 = 0.007976993918418884 + 0.1 * 6.302137851715088
Epoch 780, val loss: 1.0998975038528442
Epoch 790, training loss: 0.635053813457489 = 0.007731518242508173 + 0.1 * 6.273222923278809
Epoch 790, val loss: 1.1044522523880005
Epoch 800, training loss: 0.6343456506729126 = 0.007498553488403559 + 0.1 * 6.268470764160156
Epoch 800, val loss: 1.1090425252914429
Epoch 810, training loss: 0.6344920992851257 = 0.007277216296643019 + 0.1 * 6.272149085998535
Epoch 810, val loss: 1.1134001016616821
Epoch 820, training loss: 0.6339768171310425 = 0.007067709695547819 + 0.1 * 6.2690911293029785
Epoch 820, val loss: 1.1176587343215942
Epoch 830, training loss: 0.6335634589195251 = 0.006868309807032347 + 0.1 * 6.266951560974121
Epoch 830, val loss: 1.1219499111175537
Epoch 840, training loss: 0.6319530010223389 = 0.00667791161686182 + 0.1 * 6.252750396728516
Epoch 840, val loss: 1.1260963678359985
Epoch 850, training loss: 0.6328654885292053 = 0.006496416870504618 + 0.1 * 6.26369047164917
Epoch 850, val loss: 1.1301212310791016
Epoch 860, training loss: 0.631236732006073 = 0.006324070505797863 + 0.1 * 6.249126434326172
Epoch 860, val loss: 1.1341496706008911
Epoch 870, training loss: 0.6316550374031067 = 0.006159121636301279 + 0.1 * 6.2549591064453125
Epoch 870, val loss: 1.1380712985992432
Epoch 880, training loss: 0.6300483345985413 = 0.006002048961818218 + 0.1 * 6.240462303161621
Epoch 880, val loss: 1.1418603658676147
Epoch 890, training loss: 0.6296610832214355 = 0.0058521367609500885 + 0.1 * 6.238089084625244
Epoch 890, val loss: 1.1456832885742188
Epoch 900, training loss: 0.6293409466743469 = 0.005708543583750725 + 0.1 * 6.236324310302734
Epoch 900, val loss: 1.1493974924087524
Epoch 910, training loss: 0.6312690377235413 = 0.005570997949689627 + 0.1 * 6.2569804191589355
Epoch 910, val loss: 1.153058409690857
Epoch 920, training loss: 0.6298869848251343 = 0.005439632572233677 + 0.1 * 6.244473457336426
Epoch 920, val loss: 1.156552791595459
Epoch 930, training loss: 0.6291285753250122 = 0.005313552916049957 + 0.1 * 6.238150119781494
Epoch 930, val loss: 1.1601088047027588
Epoch 940, training loss: 0.6266530156135559 = 0.005192689597606659 + 0.1 * 6.214602947235107
Epoch 940, val loss: 1.163540005683899
Epoch 950, training loss: 0.6276831030845642 = 0.005076628644019365 + 0.1 * 6.226064682006836
Epoch 950, val loss: 1.1669498682022095
Epoch 960, training loss: 0.6272948980331421 = 0.004964789841324091 + 0.1 * 6.223300933837891
Epoch 960, val loss: 1.1702861785888672
Epoch 970, training loss: 0.6261641979217529 = 0.004857254214584827 + 0.1 * 6.213069438934326
Epoch 970, val loss: 1.1735213994979858
Epoch 980, training loss: 0.6282631158828735 = 0.004753787536174059 + 0.1 * 6.235093116760254
Epoch 980, val loss: 1.1767358779907227
Epoch 990, training loss: 0.6262260675430298 = 0.00465431809425354 + 0.1 * 6.21571683883667
Epoch 990, val loss: 1.1799182891845703
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.7122
Flip ASR: 0.6711/225 nodes
The final ASR:0.67897, 0.03702, Accuracy:0.79259, 0.00524
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10552])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83210, 0.01222
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7692928314208984 = 1.9319003820419312 + 0.1 * 8.373924255371094
Epoch 0, val loss: 1.932470440864563
Epoch 10, training loss: 2.759657144546509 = 1.9222749471664429 + 0.1 * 8.373821258544922
Epoch 10, val loss: 1.9228919744491577
Epoch 20, training loss: 2.747422218322754 = 1.910103678703308 + 0.1 * 8.373184204101562
Epoch 20, val loss: 1.9102727174758911
Epoch 30, training loss: 2.72938871383667 = 1.8926424980163574 + 0.1 * 8.367463111877441
Epoch 30, val loss: 1.892027497291565
Epoch 40, training loss: 2.699484348297119 = 1.8667426109313965 + 0.1 * 8.32741641998291
Epoch 40, val loss: 1.8651881217956543
Epoch 50, training loss: 2.6438241004943848 = 1.8318074941635132 + 0.1 * 8.12016487121582
Epoch 50, val loss: 1.8310679197311401
Epoch 60, training loss: 2.5689339637756348 = 1.79331374168396 + 0.1 * 7.756202697753906
Epoch 60, val loss: 1.7969754934310913
Epoch 70, training loss: 2.5008625984191895 = 1.755833625793457 + 0.1 * 7.450288772583008
Epoch 70, val loss: 1.7664158344268799
Epoch 80, training loss: 2.4291794300079346 = 1.7123888731002808 + 0.1 * 7.167905330657959
Epoch 80, val loss: 1.7305608987808228
Epoch 90, training loss: 2.3557815551757812 = 1.6558541059494019 + 0.1 * 6.999273777008057
Epoch 90, val loss: 1.6820077896118164
Epoch 100, training loss: 2.271752119064331 = 1.5831704139709473 + 0.1 * 6.885817050933838
Epoch 100, val loss: 1.620576024055481
Epoch 110, training loss: 2.1761667728424072 = 1.4936771392822266 + 0.1 * 6.824896335601807
Epoch 110, val loss: 1.5485926866531372
Epoch 120, training loss: 2.0721187591552734 = 1.3932679891586304 + 0.1 * 6.788507461547852
Epoch 120, val loss: 1.467921495437622
Epoch 130, training loss: 1.9656245708465576 = 1.2895830869674683 + 0.1 * 6.7604146003723145
Epoch 130, val loss: 1.3854402303695679
Epoch 140, training loss: 1.8605992794036865 = 1.1874284744262695 + 0.1 * 6.731707572937012
Epoch 140, val loss: 1.3051046133041382
Epoch 150, training loss: 1.759268045425415 = 1.088985562324524 + 0.1 * 6.702825546264648
Epoch 150, val loss: 1.2271543741226196
Epoch 160, training loss: 1.6640410423278809 = 0.9953098893165588 + 0.1 * 6.687312126159668
Epoch 160, val loss: 1.1536433696746826
Epoch 170, training loss: 1.5748201608657837 = 0.9074575304985046 + 0.1 * 6.67362642288208
Epoch 170, val loss: 1.0861930847167969
Epoch 180, training loss: 1.490593433380127 = 0.8237650990486145 + 0.1 * 6.668283939361572
Epoch 180, val loss: 1.0226138830184937
Epoch 190, training loss: 1.4106812477111816 = 0.7446737289428711 + 0.1 * 6.660075664520264
Epoch 190, val loss: 0.9631467461585999
Epoch 200, training loss: 1.3367295265197754 = 0.6714105010032654 + 0.1 * 6.653189659118652
Epoch 200, val loss: 0.9090268015861511
Epoch 210, training loss: 1.2710727453231812 = 0.6055288910865784 + 0.1 * 6.655438423156738
Epoch 210, val loss: 0.8623751401901245
Epoch 220, training loss: 1.2115333080291748 = 0.5476729273796082 + 0.1 * 6.638604164123535
Epoch 220, val loss: 0.824266254901886
Epoch 230, training loss: 1.159440040588379 = 0.4964693486690521 + 0.1 * 6.629706859588623
Epoch 230, val loss: 0.7933148145675659
Epoch 240, training loss: 1.1118403673171997 = 0.4501638412475586 + 0.1 * 6.616765022277832
Epoch 240, val loss: 0.7678313255310059
Epoch 250, training loss: 1.0686097145080566 = 0.4072456657886505 + 0.1 * 6.613640785217285
Epoch 250, val loss: 0.7461906671524048
Epoch 260, training loss: 1.025960087776184 = 0.3666413426399231 + 0.1 * 6.59318733215332
Epoch 260, val loss: 0.7274457812309265
Epoch 270, training loss: 0.9851396083831787 = 0.32748961448669434 + 0.1 * 6.576499938964844
Epoch 270, val loss: 0.7109206914901733
Epoch 280, training loss: 0.9464845657348633 = 0.2901364862918854 + 0.1 * 6.563480377197266
Epoch 280, val loss: 0.6969623565673828
Epoch 290, training loss: 0.9104990363121033 = 0.2554922103881836 + 0.1 * 6.550068378448486
Epoch 290, val loss: 0.6860226392745972
Epoch 300, training loss: 0.8776274919509888 = 0.2238888144493103 + 0.1 * 6.537386894226074
Epoch 300, val loss: 0.6782657504081726
Epoch 310, training loss: 0.8481650352478027 = 0.19572216272354126 + 0.1 * 6.524428844451904
Epoch 310, val loss: 0.6739413142204285
Epoch 320, training loss: 0.8228699564933777 = 0.17124144732952118 + 0.1 * 6.516284942626953
Epoch 320, val loss: 0.6730807423591614
Epoch 330, training loss: 0.8015865087509155 = 0.15045686066150665 + 0.1 * 6.51129674911499
Epoch 330, val loss: 0.6749811768531799
Epoch 340, training loss: 0.7818139791488647 = 0.13266336917877197 + 0.1 * 6.491506099700928
Epoch 340, val loss: 0.6794992685317993
Epoch 350, training loss: 0.7652261853218079 = 0.11737022548913956 + 0.1 * 6.4785590171813965
Epoch 350, val loss: 0.6862801313400269
Epoch 360, training loss: 0.7510831356048584 = 0.10419642925262451 + 0.1 * 6.46886682510376
Epoch 360, val loss: 0.6947108507156372
Epoch 370, training loss: 0.740405261516571 = 0.09292932599782944 + 0.1 * 6.474759578704834
Epoch 370, val loss: 0.7042965888977051
Epoch 380, training loss: 0.7287256717681885 = 0.08331158757209778 + 0.1 * 6.454140663146973
Epoch 380, val loss: 0.714531660079956
Epoch 390, training loss: 0.7200953960418701 = 0.07499412447214127 + 0.1 * 6.451012134552002
Epoch 390, val loss: 0.7253000140190125
Epoch 400, training loss: 0.7111937403678894 = 0.067779041826725 + 0.1 * 6.434146404266357
Epoch 400, val loss: 0.7365575432777405
Epoch 410, training loss: 0.7082484364509583 = 0.06148657575249672 + 0.1 * 6.467618465423584
Epoch 410, val loss: 0.748012363910675
Epoch 420, training loss: 0.6984810829162598 = 0.05601721256971359 + 0.1 * 6.424638748168945
Epoch 420, val loss: 0.7593628168106079
Epoch 430, training loss: 0.6923791170120239 = 0.051217541098594666 + 0.1 * 6.411615371704102
Epoch 430, val loss: 0.7706770300865173
Epoch 440, training loss: 0.6875645518302917 = 0.04697896167635918 + 0.1 * 6.405856132507324
Epoch 440, val loss: 0.7819408774375916
Epoch 450, training loss: 0.6860144138336182 = 0.043226081877946854 + 0.1 * 6.427883625030518
Epoch 450, val loss: 0.793123185634613
Epoch 460, training loss: 0.6795523166656494 = 0.03991558402776718 + 0.1 * 6.39636754989624
Epoch 460, val loss: 0.8039443492889404
Epoch 470, training loss: 0.6756445169448853 = 0.036965612322092056 + 0.1 * 6.386788845062256
Epoch 470, val loss: 0.8146068453788757
Epoch 480, training loss: 0.6724470257759094 = 0.03431914001703262 + 0.1 * 6.3812785148620605
Epoch 480, val loss: 0.8251222372055054
Epoch 490, training loss: 0.6702495813369751 = 0.03194595128297806 + 0.1 * 6.383036136627197
Epoch 490, val loss: 0.8353983163833618
Epoch 500, training loss: 0.66828852891922 = 0.02983195334672928 + 0.1 * 6.384565830230713
Epoch 500, val loss: 0.8452478051185608
Epoch 510, training loss: 0.6649667620658875 = 0.027924280613660812 + 0.1 * 6.370424747467041
Epoch 510, val loss: 0.854911744594574
Epoch 520, training loss: 0.6624683141708374 = 0.026188939809799194 + 0.1 * 6.362793445587158
Epoch 520, val loss: 0.8643829226493835
Epoch 530, training loss: 0.6635405421257019 = 0.024605758488178253 + 0.1 * 6.389347553253174
Epoch 530, val loss: 0.8737348914146423
Epoch 540, training loss: 0.6587713956832886 = 0.023167796432971954 + 0.1 * 6.3560357093811035
Epoch 540, val loss: 0.8827905654907227
Epoch 550, training loss: 0.6571911573410034 = 0.02185460738837719 + 0.1 * 6.353365898132324
Epoch 550, val loss: 0.8916635513305664
Epoch 560, training loss: 0.6562134623527527 = 0.020648930221796036 + 0.1 * 6.355645179748535
Epoch 560, val loss: 0.9004012942314148
Epoch 570, training loss: 0.6550566554069519 = 0.01954393833875656 + 0.1 * 6.355126857757568
Epoch 570, val loss: 0.9089249968528748
Epoch 580, training loss: 0.6522798538208008 = 0.018528085201978683 + 0.1 * 6.337517261505127
Epoch 580, val loss: 0.9172174334526062
Epoch 590, training loss: 0.6518364548683167 = 0.01758953556418419 + 0.1 * 6.342469215393066
Epoch 590, val loss: 0.9254021644592285
Epoch 600, training loss: 0.6516029238700867 = 0.01672685518860817 + 0.1 * 6.348760604858398
Epoch 600, val loss: 0.9333023428916931
Epoch 610, training loss: 0.6494813561439514 = 0.015932921320199966 + 0.1 * 6.335484027862549
Epoch 610, val loss: 0.940971851348877
Epoch 620, training loss: 0.6478084921836853 = 0.015194810926914215 + 0.1 * 6.326136589050293
Epoch 620, val loss: 0.94853675365448
Epoch 630, training loss: 0.6471080183982849 = 0.014507957734167576 + 0.1 * 6.326000690460205
Epoch 630, val loss: 0.955960214138031
Epoch 640, training loss: 0.6469075679779053 = 0.013869299553334713 + 0.1 * 6.330382823944092
Epoch 640, val loss: 0.9631831645965576
Epoch 650, training loss: 0.645158052444458 = 0.013276626355946064 + 0.1 * 6.318813800811768
Epoch 650, val loss: 0.9702160954475403
Epoch 660, training loss: 0.6442572474479675 = 0.012722890824079514 + 0.1 * 6.315343379974365
Epoch 660, val loss: 0.977164626121521
Epoch 670, training loss: 0.6435745358467102 = 0.012204534374177456 + 0.1 * 6.313700199127197
Epoch 670, val loss: 0.9839423298835754
Epoch 680, training loss: 0.644983172416687 = 0.011719834059476852 + 0.1 * 6.3326334953308105
Epoch 680, val loss: 0.9905023574829102
Epoch 690, training loss: 0.6422815918922424 = 0.011267771944403648 + 0.1 * 6.310137748718262
Epoch 690, val loss: 0.9969897270202637
Epoch 700, training loss: 0.6407113075256348 = 0.010843110270798206 + 0.1 * 6.29868221282959
Epoch 700, val loss: 1.0032358169555664
Epoch 710, training loss: 0.6414608955383301 = 0.010442756116390228 + 0.1 * 6.310181617736816
Epoch 710, val loss: 1.0094619989395142
Epoch 720, training loss: 0.6388623714447021 = 0.01006677933037281 + 0.1 * 6.2879557609558105
Epoch 720, val loss: 1.015552282333374
Epoch 730, training loss: 0.6397241353988647 = 0.009712671861052513 + 0.1 * 6.300114154815674
Epoch 730, val loss: 1.0215072631835938
Epoch 740, training loss: 0.6380518674850464 = 0.009378860704600811 + 0.1 * 6.28672981262207
Epoch 740, val loss: 1.0273628234863281
Epoch 750, training loss: 0.6418622136116028 = 0.009064089506864548 + 0.1 * 6.327981472015381
Epoch 750, val loss: 1.0331010818481445
Epoch 760, training loss: 0.6374627351760864 = 0.008767488412559032 + 0.1 * 6.286952495574951
Epoch 760, val loss: 1.038620948791504
Epoch 770, training loss: 0.6367762088775635 = 0.008487033657729626 + 0.1 * 6.282891273498535
Epoch 770, val loss: 1.0440263748168945
Epoch 780, training loss: 0.6346430778503418 = 0.008221066556870937 + 0.1 * 6.264219760894775
Epoch 780, val loss: 1.0494099855422974
Epoch 790, training loss: 0.6373584866523743 = 0.007968331687152386 + 0.1 * 6.293901443481445
Epoch 790, val loss: 1.0546749830245972
Epoch 800, training loss: 0.6338372826576233 = 0.0077284760773181915 + 0.1 * 6.261087894439697
Epoch 800, val loss: 1.0598081350326538
Epoch 810, training loss: 0.6359389424324036 = 0.007501129060983658 + 0.1 * 6.2843780517578125
Epoch 810, val loss: 1.0649101734161377
Epoch 820, training loss: 0.6333461403846741 = 0.007285077590495348 + 0.1 * 6.260610580444336
Epoch 820, val loss: 1.0699098110198975
Epoch 830, training loss: 0.634002149105072 = 0.007079387549310923 + 0.1 * 6.269227504730225
Epoch 830, val loss: 1.0747489929199219
Epoch 840, training loss: 0.6315567493438721 = 0.006883467547595501 + 0.1 * 6.246732711791992
Epoch 840, val loss: 1.0795011520385742
Epoch 850, training loss: 0.6341860294342041 = 0.006696063093841076 + 0.1 * 6.274899959564209
Epoch 850, val loss: 1.0841662883758545
Epoch 860, training loss: 0.6317716240882874 = 0.00651754392310977 + 0.1 * 6.252540588378906
Epoch 860, val loss: 1.088895320892334
Epoch 870, training loss: 0.6314842700958252 = 0.006346960086375475 + 0.1 * 6.251373291015625
Epoch 870, val loss: 1.0934178829193115
Epoch 880, training loss: 0.6310420036315918 = 0.006184035912156105 + 0.1 * 6.248579502105713
Epoch 880, val loss: 1.0979160070419312
Epoch 890, training loss: 0.630261242389679 = 0.006028240080922842 + 0.1 * 6.242330074310303
Epoch 890, val loss: 1.1022876501083374
Epoch 900, training loss: 0.6300397515296936 = 0.0058785188011825085 + 0.1 * 6.241612434387207
Epoch 900, val loss: 1.1066137552261353
Epoch 910, training loss: 0.6284353137016296 = 0.005735096987336874 + 0.1 * 6.227002143859863
Epoch 910, val loss: 1.1109380722045898
Epoch 920, training loss: 0.6287428140640259 = 0.005597781855612993 + 0.1 * 6.23145055770874
Epoch 920, val loss: 1.1150836944580078
Epoch 930, training loss: 0.6280815005302429 = 0.0054662046022713184 + 0.1 * 6.226152420043945
Epoch 930, val loss: 1.1192611455917358
Epoch 940, training loss: 0.6305598616600037 = 0.005337350536137819 + 0.1 * 6.252224922180176
Epoch 940, val loss: 1.1232751607894897
Epoch 950, training loss: 0.6281017065048218 = 0.0052179922349750996 + 0.1 * 6.228837013244629
Epoch 950, val loss: 1.1274019479751587
Epoch 960, training loss: 0.6298992037773132 = 0.005100398324429989 + 0.1 * 6.247988224029541
Epoch 960, val loss: 1.1313027143478394
Epoch 970, training loss: 0.6270930171012878 = 0.0049846661277115345 + 0.1 * 6.221083164215088
Epoch 970, val loss: 1.1350605487823486
Epoch 980, training loss: 0.6261667013168335 = 0.004876186605542898 + 0.1 * 6.212904930114746
Epoch 980, val loss: 1.1389739513397217
Epoch 990, training loss: 0.6265383362770081 = 0.0047708535566926 + 0.1 * 6.217674255371094
Epoch 990, val loss: 1.1427984237670898
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5720
Flip ASR: 0.4889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.776625633239746 = 1.9392365217208862 + 0.1 * 8.373889923095703
Epoch 0, val loss: 1.9262959957122803
Epoch 10, training loss: 2.766209602355957 = 1.928838849067688 + 0.1 * 8.37370777130127
Epoch 10, val loss: 1.9159003496170044
Epoch 20, training loss: 2.753091335296631 = 1.915837049484253 + 0.1 * 8.372542381286621
Epoch 20, val loss: 1.902439832687378
Epoch 30, training loss: 2.733849048614502 = 1.8975462913513184 + 0.1 * 8.363027572631836
Epoch 30, val loss: 1.8830403089523315
Epoch 40, training loss: 2.7009825706481934 = 1.8707692623138428 + 0.1 * 8.302132606506348
Epoch 40, val loss: 1.85483717918396
Epoch 50, training loss: 2.6311354637145996 = 1.835145354270935 + 0.1 * 7.959901809692383
Epoch 50, val loss: 1.8192085027694702
Epoch 60, training loss: 2.561168670654297 = 1.7964112758636475 + 0.1 * 7.647572994232178
Epoch 60, val loss: 1.7838900089263916
Epoch 70, training loss: 2.4856910705566406 = 1.7589936256408691 + 0.1 * 7.266973972320557
Epoch 70, val loss: 1.7515842914581299
Epoch 80, training loss: 2.414849281311035 = 1.7159054279327393 + 0.1 * 6.989438533782959
Epoch 80, val loss: 1.7140846252441406
Epoch 90, training loss: 2.346111536026001 = 1.6601014137268066 + 0.1 * 6.860100269317627
Epoch 90, val loss: 1.6653025150299072
Epoch 100, training loss: 2.2674460411071777 = 1.5880069732666016 + 0.1 * 6.794389724731445
Epoch 100, val loss: 1.6031074523925781
Epoch 110, training loss: 2.1763062477111816 = 1.5017820596694946 + 0.1 * 6.745243072509766
Epoch 110, val loss: 1.5308667421340942
Epoch 120, training loss: 2.078998327255249 = 1.40837562084198 + 0.1 * 6.706227779388428
Epoch 120, val loss: 1.4546549320220947
Epoch 130, training loss: 1.9809601306915283 = 1.3124920129776 + 0.1 * 6.684681415557861
Epoch 130, val loss: 1.3797552585601807
Epoch 140, training loss: 1.8814442157745361 = 1.213959813117981 + 0.1 * 6.674844741821289
Epoch 140, val loss: 1.3055615425109863
Epoch 150, training loss: 1.779219150543213 = 1.112228512763977 + 0.1 * 6.669907093048096
Epoch 150, val loss: 1.2297145128250122
Epoch 160, training loss: 1.6760046482086182 = 1.0089795589447021 + 0.1 * 6.670251369476318
Epoch 160, val loss: 1.1524198055267334
Epoch 170, training loss: 1.5758326053619385 = 0.9092564582824707 + 0.1 * 6.665761947631836
Epoch 170, val loss: 1.0786203145980835
Epoch 180, training loss: 1.4825072288513184 = 0.816246509552002 + 0.1 * 6.662606716156006
Epoch 180, val loss: 1.0116301774978638
Epoch 190, training loss: 1.39780592918396 = 0.7319496870040894 + 0.1 * 6.658562183380127
Epoch 190, val loss: 0.9534192085266113
Epoch 200, training loss: 1.3232402801513672 = 0.6575965881347656 + 0.1 * 6.656436920166016
Epoch 200, val loss: 0.9053441882133484
Epoch 210, training loss: 1.257420301437378 = 0.5926792621612549 + 0.1 * 6.647410869598389
Epoch 210, val loss: 0.8665863871574402
Epoch 220, training loss: 1.199828863143921 = 0.5359346866607666 + 0.1 * 6.638942241668701
Epoch 220, val loss: 0.8361058235168457
Epoch 230, training loss: 1.1499238014221191 = 0.48681339621543884 + 0.1 * 6.631103515625
Epoch 230, val loss: 0.8132767677307129
Epoch 240, training loss: 1.1062074899673462 = 0.44427844882011414 + 0.1 * 6.619290828704834
Epoch 240, val loss: 0.7974336743354797
Epoch 250, training loss: 1.0678093433380127 = 0.4068382680416107 + 0.1 * 6.609710216522217
Epoch 250, val loss: 0.787067711353302
Epoch 260, training loss: 1.0329699516296387 = 0.37358108162879944 + 0.1 * 6.593888759613037
Epoch 260, val loss: 0.7814477682113647
Epoch 270, training loss: 1.0016815662384033 = 0.34340769052505493 + 0.1 * 6.582738399505615
Epoch 270, val loss: 0.7792652249336243
Epoch 280, training loss: 0.9723354578018188 = 0.31562894582748413 + 0.1 * 6.567065238952637
Epoch 280, val loss: 0.7795826196670532
Epoch 290, training loss: 0.9451314806938171 = 0.28933262825012207 + 0.1 * 6.55798864364624
Epoch 290, val loss: 0.7818638682365417
Epoch 300, training loss: 0.9195914268493652 = 0.26395314931869507 + 0.1 * 6.556382656097412
Epoch 300, val loss: 0.7862064838409424
Epoch 310, training loss: 0.8940598964691162 = 0.2393219769001007 + 0.1 * 6.547379493713379
Epoch 310, val loss: 0.7913115620613098
Epoch 320, training loss: 0.8688401579856873 = 0.21520887315273285 + 0.1 * 6.536312580108643
Epoch 320, val loss: 0.7965549230575562
Epoch 330, training loss: 0.8464585542678833 = 0.19274002313613892 + 0.1 * 6.537185192108154
Epoch 330, val loss: 0.8044559955596924
Epoch 340, training loss: 0.8253844380378723 = 0.17301571369171143 + 0.1 * 6.52368688583374
Epoch 340, val loss: 0.8153724670410156
Epoch 350, training loss: 0.8074541091918945 = 0.15569746494293213 + 0.1 * 6.517566204071045
Epoch 350, val loss: 0.8269387483596802
Epoch 360, training loss: 0.7914868593215942 = 0.1403304785490036 + 0.1 * 6.511563777923584
Epoch 360, val loss: 0.8385223746299744
Epoch 370, training loss: 0.780197262763977 = 0.12676264345645905 + 0.1 * 6.534346103668213
Epoch 370, val loss: 0.8504504561424255
Epoch 380, training loss: 0.7646337747573853 = 0.11483756452798843 + 0.1 * 6.49796199798584
Epoch 380, val loss: 0.8625341057777405
Epoch 390, training loss: 0.7531507611274719 = 0.1042274758219719 + 0.1 * 6.489232540130615
Epoch 390, val loss: 0.874703586101532
Epoch 400, training loss: 0.7451562881469727 = 0.09477201104164124 + 0.1 * 6.503842353820801
Epoch 400, val loss: 0.8870599269866943
Epoch 410, training loss: 0.7336841821670532 = 0.08641903102397919 + 0.1 * 6.472651481628418
Epoch 410, val loss: 0.8993338942527771
Epoch 420, training loss: 0.7249683141708374 = 0.07897944003343582 + 0.1 * 6.459888458251953
Epoch 420, val loss: 0.9117164611816406
Epoch 430, training loss: 0.7188478708267212 = 0.07236587256193161 + 0.1 * 6.46481990814209
Epoch 430, val loss: 0.9239590167999268
Epoch 440, training loss: 0.7112654447555542 = 0.06650586426258087 + 0.1 * 6.447595596313477
Epoch 440, val loss: 0.9360439777374268
Epoch 450, training loss: 0.7042790651321411 = 0.06126967445015907 + 0.1 * 6.430093765258789
Epoch 450, val loss: 0.9480847120285034
Epoch 460, training loss: 0.7003299593925476 = 0.056592486798763275 + 0.1 * 6.437374591827393
Epoch 460, val loss: 0.9599396586418152
Epoch 470, training loss: 0.6942906975746155 = 0.05242550000548363 + 0.1 * 6.418651580810547
Epoch 470, val loss: 0.9715178608894348
Epoch 480, training loss: 0.690708339214325 = 0.04867963865399361 + 0.1 * 6.420286655426025
Epoch 480, val loss: 0.9829614758491516
Epoch 490, training loss: 0.6844007968902588 = 0.04531829431653023 + 0.1 * 6.390824794769287
Epoch 490, val loss: 0.9941530823707581
Epoch 500, training loss: 0.683099627494812 = 0.04227450489997864 + 0.1 * 6.40825080871582
Epoch 500, val loss: 1.0051662921905518
Epoch 510, training loss: 0.677384078502655 = 0.03952930495142937 + 0.1 * 6.378547191619873
Epoch 510, val loss: 1.0157907009124756
Epoch 520, training loss: 0.6732273697853088 = 0.03703871741890907 + 0.1 * 6.361886024475098
Epoch 520, val loss: 1.0262969732284546
Epoch 530, training loss: 0.6710911989212036 = 0.03477213904261589 + 0.1 * 6.363190174102783
Epoch 530, val loss: 1.0363895893096924
Epoch 540, training loss: 0.6680523753166199 = 0.03271173685789108 + 0.1 * 6.3534064292907715
Epoch 540, val loss: 1.0463365316390991
Epoch 550, training loss: 0.6646740436553955 = 0.03082096576690674 + 0.1 * 6.338530540466309
Epoch 550, val loss: 1.0560872554779053
Epoch 560, training loss: 0.6640220880508423 = 0.029083658009767532 + 0.1 * 6.349384307861328
Epoch 560, val loss: 1.0656291246414185
Epoch 570, training loss: 0.6606184840202332 = 0.027490537613630295 + 0.1 * 6.331279277801514
Epoch 570, val loss: 1.0748857259750366
Epoch 580, training loss: 0.6603975892066956 = 0.026023266837000847 + 0.1 * 6.343742847442627
Epoch 580, val loss: 1.083905816078186
Epoch 590, training loss: 0.6559115648269653 = 0.024671191349625587 + 0.1 * 6.312403678894043
Epoch 590, val loss: 1.0927419662475586
Epoch 600, training loss: 0.6546911597251892 = 0.02341843582689762 + 0.1 * 6.312727451324463
Epoch 600, val loss: 1.1013132333755493
Epoch 610, training loss: 0.6521115899085999 = 0.02225739322602749 + 0.1 * 6.298542022705078
Epoch 610, val loss: 1.1097575426101685
Epoch 620, training loss: 0.652047872543335 = 0.021176913753151894 + 0.1 * 6.308709144592285
Epoch 620, val loss: 1.1179776191711426
Epoch 630, training loss: 0.6487798094749451 = 0.02017568238079548 + 0.1 * 6.286041259765625
Epoch 630, val loss: 1.1260390281677246
Epoch 640, training loss: 0.6498441100120544 = 0.019244620576500893 + 0.1 * 6.305994987487793
Epoch 640, val loss: 1.1338233947753906
Epoch 650, training loss: 0.6464468240737915 = 0.018382154405117035 + 0.1 * 6.280646800994873
Epoch 650, val loss: 1.1414566040039062
Epoch 660, training loss: 0.6457032561302185 = 0.017576534301042557 + 0.1 * 6.281267166137695
Epoch 660, val loss: 1.1490181684494019
Epoch 670, training loss: 0.6436634063720703 = 0.016821423545479774 + 0.1 * 6.2684197425842285
Epoch 670, val loss: 1.1563841104507446
Epoch 680, training loss: 0.6429002285003662 = 0.016113482415676117 + 0.1 * 6.267867565155029
Epoch 680, val loss: 1.1636219024658203
Epoch 690, training loss: 0.6452606320381165 = 0.01545108575373888 + 0.1 * 6.298095226287842
Epoch 690, val loss: 1.170614242553711
Epoch 700, training loss: 0.6415742039680481 = 0.014830122701823711 + 0.1 * 6.2674407958984375
Epoch 700, val loss: 1.177545428276062
Epoch 710, training loss: 0.6395063400268555 = 0.014247430488467216 + 0.1 * 6.252589225769043
Epoch 710, val loss: 1.184338092803955
Epoch 720, training loss: 0.6392449736595154 = 0.013698192313313484 + 0.1 * 6.255467414855957
Epoch 720, val loss: 1.190995216369629
Epoch 730, training loss: 0.6375573873519897 = 0.01318103726953268 + 0.1 * 6.243763446807861
Epoch 730, val loss: 1.1975626945495605
Epoch 740, training loss: 0.6408656239509583 = 0.012693026103079319 + 0.1 * 6.281725883483887
Epoch 740, val loss: 1.2039415836334229
Epoch 750, training loss: 0.6360421776771545 = 0.012234249152243137 + 0.1 * 6.238079071044922
Epoch 750, val loss: 1.210138201713562
Epoch 760, training loss: 0.6352425217628479 = 0.011800801381468773 + 0.1 * 6.234416961669922
Epoch 760, val loss: 1.216360092163086
Epoch 770, training loss: 0.635719895362854 = 0.011390292085707188 + 0.1 * 6.243296146392822
Epoch 770, val loss: 1.2224332094192505
Epoch 780, training loss: 0.6345271468162537 = 0.011001568287611008 + 0.1 * 6.235255241394043
Epoch 780, val loss: 1.2283515930175781
Epoch 790, training loss: 0.6327775716781616 = 0.010633419267833233 + 0.1 * 6.221441745758057
Epoch 790, val loss: 1.2341605424880981
Epoch 800, training loss: 0.633807361125946 = 0.01028425619006157 + 0.1 * 6.235230922698975
Epoch 800, val loss: 1.2399500608444214
Epoch 810, training loss: 0.6334196329116821 = 0.009952649474143982 + 0.1 * 6.2346696853637695
Epoch 810, val loss: 1.2455193996429443
Epoch 820, training loss: 0.6321107745170593 = 0.009638066403567791 + 0.1 * 6.224727153778076
Epoch 820, val loss: 1.2509737014770508
Epoch 830, training loss: 0.631203830242157 = 0.009339452721178532 + 0.1 * 6.218643665313721
Epoch 830, val loss: 1.2564746141433716
Epoch 840, training loss: 0.631328821182251 = 0.009054134599864483 + 0.1 * 6.222746849060059
Epoch 840, val loss: 1.2618224620819092
Epoch 850, training loss: 0.6329501867294312 = 0.008783147670328617 + 0.1 * 6.24167013168335
Epoch 850, val loss: 1.2670023441314697
Epoch 860, training loss: 0.6289086937904358 = 0.008525236509740353 + 0.1 * 6.203834056854248
Epoch 860, val loss: 1.2720305919647217
Epoch 870, training loss: 0.6291891932487488 = 0.008279879577457905 + 0.1 * 6.20909309387207
Epoch 870, val loss: 1.2771304845809937
Epoch 880, training loss: 0.6277338266372681 = 0.008045287802815437 + 0.1 * 6.196885108947754
Epoch 880, val loss: 1.282157301902771
Epoch 890, training loss: 0.6304090023040771 = 0.007820731960237026 + 0.1 * 6.225882530212402
Epoch 890, val loss: 1.2870888710021973
Epoch 900, training loss: 0.6289219260215759 = 0.007606037426739931 + 0.1 * 6.213159084320068
Epoch 900, val loss: 1.291883111000061
Epoch 910, training loss: 0.6283227205276489 = 0.007401397451758385 + 0.1 * 6.209212779998779
Epoch 910, val loss: 1.2965281009674072
Epoch 920, training loss: 0.6269679069519043 = 0.007206002250313759 + 0.1 * 6.19761848449707
Epoch 920, val loss: 1.3011951446533203
Epoch 930, training loss: 0.626473605632782 = 0.007018459960818291 + 0.1 * 6.194551467895508
Epoch 930, val loss: 1.305885672569275
Epoch 940, training loss: 0.6257973313331604 = 0.006838507484644651 + 0.1 * 6.1895880699157715
Epoch 940, val loss: 1.310354232788086
Epoch 950, training loss: 0.6257889866828918 = 0.006666330620646477 + 0.1 * 6.191226482391357
Epoch 950, val loss: 1.3148818016052246
Epoch 960, training loss: 0.6267836093902588 = 0.006500869058072567 + 0.1 * 6.202827453613281
Epoch 960, val loss: 1.3192766904830933
Epoch 970, training loss: 0.6261597275733948 = 0.006342272274196148 + 0.1 * 6.198174476623535
Epoch 970, val loss: 1.3236092329025269
Epoch 980, training loss: 0.6249560713768005 = 0.0061898790299892426 + 0.1 * 6.187661647796631
Epoch 980, val loss: 1.32785964012146
Epoch 990, training loss: 0.6251595616340637 = 0.006043760105967522 + 0.1 * 6.191158294677734
Epoch 990, val loss: 1.332207202911377
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.799492120742798 = 1.9620994329452515 + 0.1 * 8.373927116394043
Epoch 0, val loss: 1.9638760089874268
Epoch 10, training loss: 2.7884340286254883 = 1.9510538578033447 + 0.1 * 8.373802185058594
Epoch 10, val loss: 1.9532109498977661
Epoch 20, training loss: 2.7746775150299072 = 1.9373756647109985 + 0.1 * 8.373018264770508
Epoch 20, val loss: 1.9394956827163696
Epoch 30, training loss: 2.7550323009490967 = 1.9183449745178223 + 0.1 * 8.366872787475586
Epoch 30, val loss: 1.9200209379196167
Epoch 40, training loss: 2.7236123085021973 = 1.8904966115951538 + 0.1 * 8.331157684326172
Epoch 40, val loss: 1.8916972875595093
Epoch 50, training loss: 2.6684823036193848 = 1.8519673347473145 + 0.1 * 8.165149688720703
Epoch 50, val loss: 1.8542261123657227
Epoch 60, training loss: 2.5872879028320312 = 1.8070179224014282 + 0.1 * 7.802700042724609
Epoch 60, val loss: 1.8126766681671143
Epoch 70, training loss: 2.519481658935547 = 1.7641215324401855 + 0.1 * 7.553600311279297
Epoch 70, val loss: 1.774972677230835
Epoch 80, training loss: 2.4407799243927 = 1.721675157546997 + 0.1 * 7.1910481452941895
Epoch 80, val loss: 1.7356067895889282
Epoch 90, training loss: 2.3671936988830566 = 1.667614459991455 + 0.1 * 6.995791912078857
Epoch 90, val loss: 1.6851170063018799
Epoch 100, training loss: 2.288867235183716 = 1.5975476503372192 + 0.1 * 6.9131951332092285
Epoch 100, val loss: 1.6245959997177124
Epoch 110, training loss: 2.1968512535095215 = 1.510197639465332 + 0.1 * 6.866536617279053
Epoch 110, val loss: 1.5533567667007446
Epoch 120, training loss: 2.0933494567871094 = 1.4097319841384888 + 0.1 * 6.8361735343933105
Epoch 120, val loss: 1.4724370241165161
Epoch 130, training loss: 1.9864652156829834 = 1.3050366640090942 + 0.1 * 6.814284801483154
Epoch 130, val loss: 1.3890968561172485
Epoch 140, training loss: 1.881742238998413 = 1.2019070386886597 + 0.1 * 6.798352241516113
Epoch 140, val loss: 1.3076163530349731
Epoch 150, training loss: 1.7798404693603516 = 1.1013864278793335 + 0.1 * 6.784539699554443
Epoch 150, val loss: 1.2288728952407837
Epoch 160, training loss: 1.681241750717163 = 1.0041941404342651 + 0.1 * 6.7704758644104
Epoch 160, val loss: 1.1524907350540161
Epoch 170, training loss: 1.5865411758422852 = 0.9110283851623535 + 0.1 * 6.755128383636475
Epoch 170, val loss: 1.0793596506118774
Epoch 180, training loss: 1.496496558189392 = 0.822349488735199 + 0.1 * 6.7414703369140625
Epoch 180, val loss: 1.0106314420700073
Epoch 190, training loss: 1.4122556447982788 = 0.7390726208686829 + 0.1 * 6.73183012008667
Epoch 190, val loss: 0.9470444917678833
Epoch 200, training loss: 1.3342468738555908 = 0.6620367169380188 + 0.1 * 6.72210168838501
Epoch 200, val loss: 0.8888720273971558
Epoch 210, training loss: 1.2643802165985107 = 0.5930463075637817 + 0.1 * 6.713338375091553
Epoch 210, val loss: 0.8379978537559509
Epoch 220, training loss: 1.2038278579711914 = 0.5331650972366333 + 0.1 * 6.706627368927002
Epoch 220, val loss: 0.7961171865463257
Epoch 230, training loss: 1.151507019996643 = 0.4818202555179596 + 0.1 * 6.6968674659729
Epoch 230, val loss: 0.76332026720047
Epoch 240, training loss: 1.1058152914047241 = 0.4369397461414337 + 0.1 * 6.688755035400391
Epoch 240, val loss: 0.7382038235664368
Epoch 250, training loss: 1.063998818397522 = 0.3962494730949402 + 0.1 * 6.677493095397949
Epoch 250, val loss: 0.7189471125602722
Epoch 260, training loss: 1.0248351097106934 = 0.35806071758270264 + 0.1 * 6.667743682861328
Epoch 260, val loss: 0.7040268182754517
Epoch 270, training loss: 0.9870249629020691 = 0.321727991104126 + 0.1 * 6.652969837188721
Epoch 270, val loss: 0.6926701068878174
Epoch 280, training loss: 0.9521292448043823 = 0.2874099910259247 + 0.1 * 6.647192001342773
Epoch 280, val loss: 0.6847364902496338
Epoch 290, training loss: 0.9186035990715027 = 0.25565314292907715 + 0.1 * 6.629504680633545
Epoch 290, val loss: 0.6796485781669617
Epoch 300, training loss: 0.888844907283783 = 0.22673465311527252 + 0.1 * 6.621102809906006
Epoch 300, val loss: 0.677351176738739
Epoch 310, training loss: 0.8620727062225342 = 0.200791135430336 + 0.1 * 6.612815856933594
Epoch 310, val loss: 0.6776010990142822
Epoch 320, training loss: 0.8379981517791748 = 0.17771194875240326 + 0.1 * 6.6028618812561035
Epoch 320, val loss: 0.6802501082420349
Epoch 330, training loss: 0.8171822428703308 = 0.15729333460330963 + 0.1 * 6.598888874053955
Epoch 330, val loss: 0.6847665309906006
Epoch 340, training loss: 0.798700213432312 = 0.13936945796012878 + 0.1 * 6.5933074951171875
Epoch 340, val loss: 0.6910931468009949
Epoch 350, training loss: 0.7823165059089661 = 0.12368039786815643 + 0.1 * 6.586360931396484
Epoch 350, val loss: 0.6988058686256409
Epoch 360, training loss: 0.7681035399436951 = 0.11000607162714005 + 0.1 * 6.580974578857422
Epoch 360, val loss: 0.7080255150794983
Epoch 370, training loss: 0.7561917901039124 = 0.09816140681505203 + 0.1 * 6.58030366897583
Epoch 370, val loss: 0.7182819843292236
Epoch 380, training loss: 0.7447731494903564 = 0.08791656792163849 + 0.1 * 6.568565845489502
Epoch 380, val loss: 0.7294434309005737
Epoch 390, training loss: 0.7352710962295532 = 0.07900144159793854 + 0.1 * 6.56269645690918
Epoch 390, val loss: 0.7413612604141235
Epoch 400, training loss: 0.7280514240264893 = 0.07123178243637085 + 0.1 * 6.5681962966918945
Epoch 400, val loss: 0.7538920640945435
Epoch 410, training loss: 0.7200759649276733 = 0.06448047608137131 + 0.1 * 6.555954933166504
Epoch 410, val loss: 0.7666003108024597
Epoch 420, training loss: 0.7131759524345398 = 0.05856333300471306 + 0.1 * 6.546125888824463
Epoch 420, val loss: 0.7795067429542542
Epoch 430, training loss: 0.7081769108772278 = 0.053354740142822266 + 0.1 * 6.548221588134766
Epoch 430, val loss: 0.7924929261207581
Epoch 440, training loss: 0.702278196811676 = 0.04877345636487007 + 0.1 * 6.5350470542907715
Epoch 440, val loss: 0.8055359125137329
Epoch 450, training loss: 0.6974582076072693 = 0.04472186788916588 + 0.1 * 6.527363300323486
Epoch 450, val loss: 0.8185476064682007
Epoch 460, training loss: 0.693763256072998 = 0.0411362498998642 + 0.1 * 6.526269912719727
Epoch 460, val loss: 0.8313339352607727
Epoch 470, training loss: 0.6903162002563477 = 0.03798312321305275 + 0.1 * 6.5233306884765625
Epoch 470, val loss: 0.8438882231712341
Epoch 480, training loss: 0.6864499449729919 = 0.03517382591962814 + 0.1 * 6.512761116027832
Epoch 480, val loss: 0.8562042117118835
Epoch 490, training loss: 0.6836965680122375 = 0.03265056759119034 + 0.1 * 6.5104594230651855
Epoch 490, val loss: 0.8683697581291199
Epoch 500, training loss: 0.6804797649383545 = 0.03038887493312359 + 0.1 * 6.500908374786377
Epoch 500, val loss: 0.8802857398986816
Epoch 510, training loss: 0.6775385737419128 = 0.028354108333587646 + 0.1 * 6.491844654083252
Epoch 510, val loss: 0.892045259475708
Epoch 520, training loss: 0.6758440732955933 = 0.02651076950132847 + 0.1 * 6.493332862854004
Epoch 520, val loss: 0.9035200476646423
Epoch 530, training loss: 0.6730070114135742 = 0.024845635518431664 + 0.1 * 6.481613636016846
Epoch 530, val loss: 0.9147581458091736
Epoch 540, training loss: 0.6709463000297546 = 0.02333512343466282 + 0.1 * 6.476111888885498
Epoch 540, val loss: 0.9258032441139221
Epoch 550, training loss: 0.6695054769515991 = 0.021961797028779984 + 0.1 * 6.475436687469482
Epoch 550, val loss: 0.9364780783653259
Epoch 560, training loss: 0.6669496297836304 = 0.020712826400995255 + 0.1 * 6.462367534637451
Epoch 560, val loss: 0.9469972252845764
Epoch 570, training loss: 0.6644904017448425 = 0.01956675760447979 + 0.1 * 6.449235916137695
Epoch 570, val loss: 0.957184374332428
Epoch 580, training loss: 0.6642274856567383 = 0.01851094700396061 + 0.1 * 6.457165241241455
Epoch 580, val loss: 0.9672244787216187
Epoch 590, training loss: 0.6634358167648315 = 0.01754722185432911 + 0.1 * 6.458885669708252
Epoch 590, val loss: 0.9768908619880676
Epoch 600, training loss: 0.6599052548408508 = 0.016665255650877953 + 0.1 * 6.432399749755859
Epoch 600, val loss: 0.9865365624427795
Epoch 610, training loss: 0.6584517955780029 = 0.015847044065594673 + 0.1 * 6.4260478019714355
Epoch 610, val loss: 0.9957827925682068
Epoch 620, training loss: 0.6572445034980774 = 0.015086255967617035 + 0.1 * 6.4215826988220215
Epoch 620, val loss: 1.004868984222412
Epoch 630, training loss: 0.6562910079956055 = 0.014379816129803658 + 0.1 * 6.419112205505371
Epoch 630, val loss: 1.0138455629348755
Epoch 640, training loss: 0.6546475887298584 = 0.013726283796131611 + 0.1 * 6.409212589263916
Epoch 640, val loss: 1.0224528312683105
Epoch 650, training loss: 0.6540878415107727 = 0.013120751827955246 + 0.1 * 6.409670829772949
Epoch 650, val loss: 1.0309845209121704
Epoch 660, training loss: 0.6523165702819824 = 0.012554251588881016 + 0.1 * 6.397623538970947
Epoch 660, val loss: 1.0392968654632568
Epoch 670, training loss: 0.6550657749176025 = 0.01202400028705597 + 0.1 * 6.430417537689209
Epoch 670, val loss: 1.04738450050354
Epoch 680, training loss: 0.650947093963623 = 0.011531864292919636 + 0.1 * 6.39415168762207
Epoch 680, val loss: 1.0550862550735474
Epoch 690, training loss: 0.6494125127792358 = 0.011073571629822254 + 0.1 * 6.383388996124268
Epoch 690, val loss: 1.06292724609375
Epoch 700, training loss: 0.6483517289161682 = 0.010641290806233883 + 0.1 * 6.37710428237915
Epoch 700, val loss: 1.0704104900360107
Epoch 710, training loss: 0.6468634605407715 = 0.010235407389700413 + 0.1 * 6.3662800788879395
Epoch 710, val loss: 1.0776304006576538
Epoch 720, training loss: 0.6458171606063843 = 0.009854771196842194 + 0.1 * 6.359623432159424
Epoch 720, val loss: 1.084811806678772
Epoch 730, training loss: 0.645535409450531 = 0.009495179168879986 + 0.1 * 6.3604021072387695
Epoch 730, val loss: 1.0918948650360107
Epoch 740, training loss: 0.6445686221122742 = 0.009156744927167892 + 0.1 * 6.354118347167969
Epoch 740, val loss: 1.09870445728302
Epoch 750, training loss: 0.6446342468261719 = 0.008838227950036526 + 0.1 * 6.357959747314453
Epoch 750, val loss: 1.1054775714874268
Epoch 760, training loss: 0.6439418792724609 = 0.008536647073924541 + 0.1 * 6.3540520668029785
Epoch 760, val loss: 1.111950159072876
Epoch 770, training loss: 0.6422246694564819 = 0.008252019062638283 + 0.1 * 6.339726448059082
Epoch 770, val loss: 1.1184346675872803
Epoch 780, training loss: 0.6424887180328369 = 0.007981927134096622 + 0.1 * 6.345067501068115
Epoch 780, val loss: 1.1246874332427979
Epoch 790, training loss: 0.6407408118247986 = 0.007726322393864393 + 0.1 * 6.330144882202148
Epoch 790, val loss: 1.1308495998382568
Epoch 800, training loss: 0.6405141353607178 = 0.007483064197003841 + 0.1 * 6.330310344696045
Epoch 800, val loss: 1.1369487047195435
Epoch 810, training loss: 0.6401273608207703 = 0.007252057548612356 + 0.1 * 6.3287529945373535
Epoch 810, val loss: 1.142800211906433
Epoch 820, training loss: 0.6400396823883057 = 0.0070328121073544025 + 0.1 * 6.330068588256836
Epoch 820, val loss: 1.1486972570419312
Epoch 830, training loss: 0.639461100101471 = 0.00682411901652813 + 0.1 * 6.326369762420654
Epoch 830, val loss: 1.1542906761169434
Epoch 840, training loss: 0.6389092206954956 = 0.006625906098634005 + 0.1 * 6.322833061218262
Epoch 840, val loss: 1.1598891019821167
Epoch 850, training loss: 0.6385974884033203 = 0.0064363786950707436 + 0.1 * 6.321610927581787
Epoch 850, val loss: 1.1653410196304321
Epoch 860, training loss: 0.6373660564422607 = 0.006256300490349531 + 0.1 * 6.311097621917725
Epoch 860, val loss: 1.1707303524017334
Epoch 870, training loss: 0.6367921233177185 = 0.006083745509386063 + 0.1 * 6.307084083557129
Epoch 870, val loss: 1.176027536392212
Epoch 880, training loss: 0.6371390223503113 = 0.0059187947772443295 + 0.1 * 6.312201976776123
Epoch 880, val loss: 1.1811959743499756
Epoch 890, training loss: 0.6360911726951599 = 0.00576153676956892 + 0.1 * 6.303296089172363
Epoch 890, val loss: 1.186247706413269
Epoch 900, training loss: 0.6360390186309814 = 0.005611167289316654 + 0.1 * 6.304278373718262
Epoch 900, val loss: 1.191286325454712
Epoch 910, training loss: 0.6354866623878479 = 0.00546671450138092 + 0.1 * 6.300199508666992
Epoch 910, val loss: 1.1961551904678345
Epoch 920, training loss: 0.6360196471214294 = 0.005328897386789322 + 0.1 * 6.3069071769714355
Epoch 920, val loss: 1.200907826423645
Epoch 930, training loss: 0.6351244449615479 = 0.005196734797209501 + 0.1 * 6.299276828765869
Epoch 930, val loss: 1.2056279182434082
Epoch 940, training loss: 0.635671854019165 = 0.005069921724498272 + 0.1 * 6.306019306182861
Epoch 940, val loss: 1.210333228111267
Epoch 950, training loss: 0.6344398856163025 = 0.004948384594172239 + 0.1 * 6.294914722442627
Epoch 950, val loss: 1.2148009538650513
Epoch 960, training loss: 0.6337206363677979 = 0.004831728059798479 + 0.1 * 6.288888931274414
Epoch 960, val loss: 1.2193208932876587
Epoch 970, training loss: 0.6345062851905823 = 0.004719412419945002 + 0.1 * 6.297868251800537
Epoch 970, val loss: 1.2236970663070679
Epoch 980, training loss: 0.6332444548606873 = 0.00461207889020443 + 0.1 * 6.286323547363281
Epoch 980, val loss: 1.2279767990112305
Epoch 990, training loss: 0.6344292759895325 = 0.004508629906922579 + 0.1 * 6.299206733703613
Epoch 990, val loss: 1.2322481870651245
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8266
Flip ASR: 0.7956/225 nodes
The final ASR:0.74416, 0.12180, Accuracy:0.81235, 0.01823
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11702])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10646])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98155, 0.01044, Accuracy:0.83086, 0.00761
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7941911220550537 = 1.9568108320236206 + 0.1 * 8.37380313873291
Epoch 0, val loss: 1.9552490711212158
Epoch 10, training loss: 2.7838807106018066 = 1.9465179443359375 + 0.1 * 8.373626708984375
Epoch 10, val loss: 1.94562828540802
Epoch 20, training loss: 2.7713661193847656 = 1.9341351985931396 + 0.1 * 8.372308731079102
Epoch 20, val loss: 1.9336570501327515
Epoch 30, training loss: 2.7529709339141846 = 1.9167840480804443 + 0.1 * 8.361867904663086
Epoch 30, val loss: 1.916579008102417
Epoch 40, training loss: 2.7209091186523438 = 1.8910506963729858 + 0.1 * 8.298583984375
Epoch 40, val loss: 1.8912103176116943
Epoch 50, training loss: 2.65313720703125 = 1.856225848197937 + 0.1 * 7.969113826751709
Epoch 50, val loss: 1.8580350875854492
Epoch 60, training loss: 2.5758721828460693 = 1.8171110153198242 + 0.1 * 7.587611198425293
Epoch 60, val loss: 1.8227136135101318
Epoch 70, training loss: 2.492396593093872 = 1.780523419380188 + 0.1 * 7.11873197555542
Epoch 70, val loss: 1.7916849851608276
Epoch 80, training loss: 2.4286489486694336 = 1.7457804679870605 + 0.1 * 6.828685283660889
Epoch 80, val loss: 1.7629557847976685
Epoch 90, training loss: 2.375134229660034 = 1.7028995752334595 + 0.1 * 6.722346782684326
Epoch 90, val loss: 1.7256113290786743
Epoch 100, training loss: 2.3117454051971436 = 1.6452112197875977 + 0.1 * 6.665341377258301
Epoch 100, val loss: 1.6759510040283203
Epoch 110, training loss: 2.2340612411499023 = 1.5713329315185547 + 0.1 * 6.62728214263916
Epoch 110, val loss: 1.6145838499069214
Epoch 120, training loss: 2.1450588703155518 = 1.4845162630081177 + 0.1 * 6.6054253578186035
Epoch 120, val loss: 1.5437536239624023
Epoch 130, training loss: 2.050420045852661 = 1.391194462776184 + 0.1 * 6.592255592346191
Epoch 130, val loss: 1.469522476196289
Epoch 140, training loss: 1.953597068786621 = 1.2953239679336548 + 0.1 * 6.5827317237854
Epoch 140, val loss: 1.3951847553253174
Epoch 150, training loss: 1.8552379608154297 = 1.1973100900650024 + 0.1 * 6.579278469085693
Epoch 150, val loss: 1.3221538066864014
Epoch 160, training loss: 1.7567942142486572 = 1.09949791431427 + 0.1 * 6.572962284088135
Epoch 160, val loss: 1.2509713172912598
Epoch 170, training loss: 1.659348964691162 = 1.002585530281067 + 0.1 * 6.567634582519531
Epoch 170, val loss: 1.1809592247009277
Epoch 180, training loss: 1.5649313926696777 = 0.9086849689483643 + 0.1 * 6.562463760375977
Epoch 180, val loss: 1.1129618883132935
Epoch 190, training loss: 1.4778141975402832 = 0.8218171000480652 + 0.1 * 6.559970855712891
Epoch 190, val loss: 1.0498348474502563
Epoch 200, training loss: 1.400299310684204 = 0.745069682598114 + 0.1 * 6.552296161651611
Epoch 200, val loss: 0.9948035478591919
Epoch 210, training loss: 1.332602620124817 = 0.6780787110328674 + 0.1 * 6.545238971710205
Epoch 210, val loss: 0.9481946229934692
Epoch 220, training loss: 1.2730231285095215 = 0.6191847920417786 + 0.1 * 6.538383960723877
Epoch 220, val loss: 0.9091178178787231
Epoch 230, training loss: 1.2190263271331787 = 0.5657274127006531 + 0.1 * 6.532989501953125
Epoch 230, val loss: 0.8751069903373718
Epoch 240, training loss: 1.1677261590957642 = 0.5150541067123413 + 0.1 * 6.5267205238342285
Epoch 240, val loss: 0.8438263535499573
Epoch 250, training loss: 1.1177749633789062 = 0.4660857319831848 + 0.1 * 6.516892910003662
Epoch 250, val loss: 0.8146926164627075
Epoch 260, training loss: 1.0694072246551514 = 0.4184533953666687 + 0.1 * 6.509538650512695
Epoch 260, val loss: 0.7877687811851501
Epoch 270, training loss: 1.022018551826477 = 0.3726462721824646 + 0.1 * 6.493722915649414
Epoch 270, val loss: 0.7638072967529297
Epoch 280, training loss: 0.978340744972229 = 0.3292170763015747 + 0.1 * 6.491236686706543
Epoch 280, val loss: 0.7431367635726929
Epoch 290, training loss: 0.9365134835243225 = 0.289331316947937 + 0.1 * 6.471821308135986
Epoch 290, val loss: 0.7262502908706665
Epoch 300, training loss: 0.9010016322135925 = 0.2534449100494385 + 0.1 * 6.47556734085083
Epoch 300, val loss: 0.7133427262306213
Epoch 310, training loss: 0.8675906658172607 = 0.2221393585205078 + 0.1 * 6.454513072967529
Epoch 310, val loss: 0.7045168280601501
Epoch 320, training loss: 0.8396557569503784 = 0.19510339200496674 + 0.1 * 6.445523262023926
Epoch 320, val loss: 0.6996590495109558
Epoch 330, training loss: 0.8162438869476318 = 0.1719987690448761 + 0.1 * 6.442450523376465
Epoch 330, val loss: 0.6985324025154114
Epoch 340, training loss: 0.7952055931091309 = 0.15235021710395813 + 0.1 * 6.428553581237793
Epoch 340, val loss: 0.7002989053726196
Epoch 350, training loss: 0.780291736125946 = 0.13553853332996368 + 0.1 * 6.4475321769714355
Epoch 350, val loss: 0.7046787142753601
Epoch 360, training loss: 0.7629818916320801 = 0.12126324325799942 + 0.1 * 6.417186260223389
Epoch 360, val loss: 0.7105931043624878
Epoch 370, training loss: 0.7496255040168762 = 0.1089785173535347 + 0.1 * 6.406469821929932
Epoch 370, val loss: 0.718048095703125
Epoch 380, training loss: 0.7408820986747742 = 0.09829097241163254 + 0.1 * 6.425910949707031
Epoch 380, val loss: 0.7266786098480225
Epoch 390, training loss: 0.7296352982521057 = 0.08903290331363678 + 0.1 * 6.406023979187012
Epoch 390, val loss: 0.7358713150024414
Epoch 400, training loss: 0.7192820310592651 = 0.08091418445110321 + 0.1 * 6.383678436279297
Epoch 400, val loss: 0.7456381916999817
Epoch 410, training loss: 0.7111116051673889 = 0.07373329252004623 + 0.1 * 6.373783111572266
Epoch 410, val loss: 0.7559099793434143
Epoch 420, training loss: 0.7064850330352783 = 0.06736353784799576 + 0.1 * 6.391214847564697
Epoch 420, val loss: 0.7665038704872131
Epoch 430, training loss: 0.6980112791061401 = 0.061758704483509064 + 0.1 * 6.362525939941406
Epoch 430, val loss: 0.7770928144454956
Epoch 440, training loss: 0.6922516226768494 = 0.05676053464412689 + 0.1 * 6.354910850524902
Epoch 440, val loss: 0.7878568172454834
Epoch 450, training loss: 0.6892862319946289 = 0.05227929353713989 + 0.1 * 6.3700690269470215
Epoch 450, val loss: 0.7987746000289917
Epoch 460, training loss: 0.6816080212593079 = 0.04826877638697624 + 0.1 * 6.333392143249512
Epoch 460, val loss: 0.8096611499786377
Epoch 470, training loss: 0.6816398501396179 = 0.04465926066040993 + 0.1 * 6.369805812835693
Epoch 470, val loss: 0.8205251693725586
Epoch 480, training loss: 0.674203097820282 = 0.04142244532704353 + 0.1 * 6.32780647277832
Epoch 480, val loss: 0.8312903642654419
Epoch 490, training loss: 0.6698898077011108 = 0.038496095687150955 + 0.1 * 6.313937187194824
Epoch 490, val loss: 0.8419801592826843
Epoch 500, training loss: 0.6663853526115417 = 0.035839058458805084 + 0.1 * 6.30546236038208
Epoch 500, val loss: 0.8525947332382202
Epoch 510, training loss: 0.666845977306366 = 0.03342285379767418 + 0.1 * 6.334231376647949
Epoch 510, val loss: 0.863081693649292
Epoch 520, training loss: 0.6612509489059448 = 0.03124186396598816 + 0.1 * 6.300090789794922
Epoch 520, val loss: 0.8733342885971069
Epoch 530, training loss: 0.6581845283508301 = 0.02926255390048027 + 0.1 * 6.289219856262207
Epoch 530, val loss: 0.8833633065223694
Epoch 540, training loss: 0.6560220122337341 = 0.027458682656288147 + 0.1 * 6.285633087158203
Epoch 540, val loss: 0.8931277394294739
Epoch 550, training loss: 0.6549449563026428 = 0.025819044560194016 + 0.1 * 6.291258811950684
Epoch 550, val loss: 0.9027070999145508
Epoch 560, training loss: 0.6522606611251831 = 0.024316567927598953 + 0.1 * 6.279440879821777
Epoch 560, val loss: 0.912085235118866
Epoch 570, training loss: 0.6499053835868835 = 0.02294103614985943 + 0.1 * 6.269643306732178
Epoch 570, val loss: 0.9211786985397339
Epoch 580, training loss: 0.6487138867378235 = 0.021682830527424812 + 0.1 * 6.270309925079346
Epoch 580, val loss: 0.930084764957428
Epoch 590, training loss: 0.6470716595649719 = 0.020524287596344948 + 0.1 * 6.26547384262085
Epoch 590, val loss: 0.9387593865394592
Epoch 600, training loss: 0.6464704871177673 = 0.019458284601569176 + 0.1 * 6.2701215744018555
Epoch 600, val loss: 0.9472193121910095
Epoch 610, training loss: 0.6439216732978821 = 0.01847792975604534 + 0.1 * 6.254437446594238
Epoch 610, val loss: 0.9554574489593506
Epoch 620, training loss: 0.6436777114868164 = 0.017570333555340767 + 0.1 * 6.261074066162109
Epoch 620, val loss: 0.9635931849479675
Epoch 630, training loss: 0.6429637670516968 = 0.01673046126961708 + 0.1 * 6.262332439422607
Epoch 630, val loss: 0.9713667035102844
Epoch 640, training loss: 0.6401752233505249 = 0.01595483161509037 + 0.1 * 6.242203712463379
Epoch 640, val loss: 0.9790464043617249
Epoch 650, training loss: 0.6398642659187317 = 0.015232793986797333 + 0.1 * 6.246314525604248
Epoch 650, val loss: 0.986544668674469
Epoch 660, training loss: 0.6380061507225037 = 0.014560399577021599 + 0.1 * 6.234457492828369
Epoch 660, val loss: 0.9938464164733887
Epoch 670, training loss: 0.6377549767494202 = 0.013934274204075336 + 0.1 * 6.23820686340332
Epoch 670, val loss: 1.0009596347808838
Epoch 680, training loss: 0.6363990306854248 = 0.013351990841329098 + 0.1 * 6.230470180511475
Epoch 680, val loss: 1.0079677104949951
Epoch 690, training loss: 0.6359794735908508 = 0.012808257713913918 + 0.1 * 6.2317118644714355
Epoch 690, val loss: 1.0148086547851562
Epoch 700, training loss: 0.6347946524620056 = 0.012297551147639751 + 0.1 * 6.224970817565918
Epoch 700, val loss: 1.0213791131973267
Epoch 710, training loss: 0.6337403655052185 = 0.01182053703814745 + 0.1 * 6.219197750091553
Epoch 710, val loss: 1.027907133102417
Epoch 720, training loss: 0.6335825324058533 = 0.011371797882020473 + 0.1 * 6.222107410430908
Epoch 720, val loss: 1.0342291593551636
Epoch 730, training loss: 0.6326067447662354 = 0.010951806791126728 + 0.1 * 6.216549396514893
Epoch 730, val loss: 1.040427327156067
Epoch 740, training loss: 0.6336852312088013 = 0.010557036846876144 + 0.1 * 6.231281757354736
Epoch 740, val loss: 1.0464600324630737
Epoch 750, training loss: 0.6313230395317078 = 0.010184193961322308 + 0.1 * 6.21138858795166
Epoch 750, val loss: 1.0524030923843384
Epoch 760, training loss: 0.6301881670951843 = 0.009833072312176228 + 0.1 * 6.203550815582275
Epoch 760, val loss: 1.0582183599472046
Epoch 770, training loss: 0.6300359964370728 = 0.00950070284307003 + 0.1 * 6.205352783203125
Epoch 770, val loss: 1.063955307006836
Epoch 780, training loss: 0.6297141313552856 = 0.009185700677335262 + 0.1 * 6.205284595489502
Epoch 780, val loss: 1.0694986581802368
Epoch 790, training loss: 0.6288701891899109 = 0.008889446035027504 + 0.1 * 6.199807167053223
Epoch 790, val loss: 1.0750114917755127
Epoch 800, training loss: 0.6300709247589111 = 0.008608300238847733 + 0.1 * 6.214626312255859
Epoch 800, val loss: 1.080358624458313
Epoch 810, training loss: 0.6276759505271912 = 0.008341078646481037 + 0.1 * 6.193348407745361
Epoch 810, val loss: 1.0856069326400757
Epoch 820, training loss: 0.6283140182495117 = 0.008088694885373116 + 0.1 * 6.2022528648376465
Epoch 820, val loss: 1.0908006429672241
Epoch 830, training loss: 0.6266648173332214 = 0.007848157547414303 + 0.1 * 6.188166618347168
Epoch 830, val loss: 1.0958374738693237
Epoch 840, training loss: 0.6282770037651062 = 0.00761921051889658 + 0.1 * 6.206577777862549
Epoch 840, val loss: 1.1008317470550537
Epoch 850, training loss: 0.6266463398933411 = 0.00740124611184001 + 0.1 * 6.192451000213623
Epoch 850, val loss: 1.1056416034698486
Epoch 860, training loss: 0.6258959770202637 = 0.007194318808615208 + 0.1 * 6.187016010284424
Epoch 860, val loss: 1.1104828119277954
Epoch 870, training loss: 0.6250910758972168 = 0.006996654439717531 + 0.1 * 6.180944442749023
Epoch 870, val loss: 1.115177869796753
Epoch 880, training loss: 0.6263163089752197 = 0.006808354519307613 + 0.1 * 6.195079326629639
Epoch 880, val loss: 1.1198393106460571
Epoch 890, training loss: 0.6253173351287842 = 0.006627654191106558 + 0.1 * 6.186896324157715
Epoch 890, val loss: 1.1243091821670532
Epoch 900, training loss: 0.6237517595291138 = 0.006456098053604364 + 0.1 * 6.172956466674805
Epoch 900, val loss: 1.1288535594940186
Epoch 910, training loss: 0.6252330541610718 = 0.006291303317993879 + 0.1 * 6.189416885375977
Epoch 910, val loss: 1.1333143711090088
Epoch 920, training loss: 0.6239421367645264 = 0.006132961716502905 + 0.1 * 6.178091526031494
Epoch 920, val loss: 1.1375867128372192
Epoch 930, training loss: 0.6231207847595215 = 0.005982246249914169 + 0.1 * 6.171385765075684
Epoch 930, val loss: 1.1419199705123901
Epoch 940, training loss: 0.6230832934379578 = 0.005837754812091589 + 0.1 * 6.172455310821533
Epoch 940, val loss: 1.1461595296859741
Epoch 950, training loss: 0.6227509379386902 = 0.005698530003428459 + 0.1 * 6.1705241203308105
Epoch 950, val loss: 1.1503344774246216
Epoch 960, training loss: 0.6223332285881042 = 0.005565142259001732 + 0.1 * 6.167680263519287
Epoch 960, val loss: 1.1544548273086548
Epoch 970, training loss: 0.6217857003211975 = 0.0054373531602323055 + 0.1 * 6.163483142852783
Epoch 970, val loss: 1.158552646636963
Epoch 980, training loss: 0.6220775842666626 = 0.005314245820045471 + 0.1 * 6.167633533477783
Epoch 980, val loss: 1.1625862121582031
Epoch 990, training loss: 0.6216153502464294 = 0.005195388570427895 + 0.1 * 6.164199352264404
Epoch 990, val loss: 1.1664763689041138
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6199
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7952067852020264 = 1.9578274488449097 + 0.1 * 8.373793601989746
Epoch 0, val loss: 1.9652364253997803
Epoch 10, training loss: 2.7847328186035156 = 1.9473751783370972 + 0.1 * 8.373576164245605
Epoch 10, val loss: 1.9537900686264038
Epoch 20, training loss: 2.7720797061920166 = 1.934861183166504 + 0.1 * 8.372185707092285
Epoch 20, val loss: 1.9396899938583374
Epoch 30, training loss: 2.7536041736602783 = 1.9174937009811401 + 0.1 * 8.361104965209961
Epoch 30, val loss: 1.9199374914169312
Epoch 40, training loss: 2.719407320022583 = 1.8919910192489624 + 0.1 * 8.274162292480469
Epoch 40, val loss: 1.8911123275756836
Epoch 50, training loss: 2.624392032623291 = 1.8589938879013062 + 0.1 * 7.653980255126953
Epoch 50, val loss: 1.8554294109344482
Epoch 60, training loss: 2.540619373321533 = 1.8247995376586914 + 0.1 * 7.158198356628418
Epoch 60, val loss: 1.8193961381912231
Epoch 70, training loss: 2.474928140640259 = 1.787365436553955 + 0.1 * 6.875627040863037
Epoch 70, val loss: 1.7809630632400513
Epoch 80, training loss: 2.4218809604644775 = 1.7481157779693604 + 0.1 * 6.737651824951172
Epoch 80, val loss: 1.7427624464035034
Epoch 90, training loss: 2.3739051818847656 = 1.7058429718017578 + 0.1 * 6.6806230545043945
Epoch 90, val loss: 1.7038389444351196
Epoch 100, training loss: 2.3180317878723145 = 1.6522200107574463 + 0.1 * 6.658116340637207
Epoch 100, val loss: 1.6564052104949951
Epoch 110, training loss: 2.2473669052124023 = 1.5823583602905273 + 0.1 * 6.650084018707275
Epoch 110, val loss: 1.5972120761871338
Epoch 120, training loss: 2.1587321758270264 = 1.4942790269851685 + 0.1 * 6.644532203674316
Epoch 120, val loss: 1.5239624977111816
Epoch 130, training loss: 2.055251121520996 = 1.3912726640701294 + 0.1 * 6.6397833824157715
Epoch 130, val loss: 1.4394150972366333
Epoch 140, training loss: 1.944338321685791 = 1.2808592319488525 + 0.1 * 6.634789943695068
Epoch 140, val loss: 1.3506778478622437
Epoch 150, training loss: 1.8321534395217896 = 1.169363260269165 + 0.1 * 6.627901554107666
Epoch 150, val loss: 1.2632715702056885
Epoch 160, training loss: 1.7233045101165771 = 1.0617070198059082 + 0.1 * 6.615973949432373
Epoch 160, val loss: 1.1803900003433228
Epoch 170, training loss: 1.6214128732681274 = 0.9615586996078491 + 0.1 * 6.598541736602783
Epoch 170, val loss: 1.1034247875213623
Epoch 180, training loss: 1.5303301811218262 = 0.8716660737991333 + 0.1 * 6.586641311645508
Epoch 180, val loss: 1.0348752737045288
Epoch 190, training loss: 1.4491082429885864 = 0.7925410270690918 + 0.1 * 6.565671920776367
Epoch 190, val loss: 0.9747233986854553
Epoch 200, training loss: 1.3771837949752808 = 0.7220766544342041 + 0.1 * 6.5510711669921875
Epoch 200, val loss: 0.920805037021637
Epoch 210, training loss: 1.3131794929504395 = 0.6592727303504944 + 0.1 * 6.53906774520874
Epoch 210, val loss: 0.87310791015625
Epoch 220, training loss: 1.254809021949768 = 0.6025555729866028 + 0.1 * 6.522534370422363
Epoch 220, val loss: 0.8315660357475281
Epoch 230, training loss: 1.2022218704223633 = 0.5505478382110596 + 0.1 * 6.516739368438721
Epoch 230, val loss: 0.7955049276351929
Epoch 240, training loss: 1.1518490314483643 = 0.5022137761116028 + 0.1 * 6.496352672576904
Epoch 240, val loss: 0.7636563777923584
Epoch 250, training loss: 1.10539710521698 = 0.45618119835853577 + 0.1 * 6.492158889770508
Epoch 250, val loss: 0.7350371479988098
Epoch 260, training loss: 1.0597351789474487 = 0.412593275308609 + 0.1 * 6.471419334411621
Epoch 260, val loss: 0.7098948359489441
Epoch 270, training loss: 1.0166029930114746 = 0.37080517411231995 + 0.1 * 6.457977771759033
Epoch 270, val loss: 0.687420129776001
Epoch 280, training loss: 0.9760838747024536 = 0.3313159644603729 + 0.1 * 6.447678565979004
Epoch 280, val loss: 0.6677390336990356
Epoch 290, training loss: 0.9378443956375122 = 0.29455873370170593 + 0.1 * 6.43285608291626
Epoch 290, val loss: 0.6508804559707642
Epoch 300, training loss: 0.9026791453361511 = 0.26044392585754395 + 0.1 * 6.422352313995361
Epoch 300, val loss: 0.6364919543266296
Epoch 310, training loss: 0.8709584474563599 = 0.22941060364246368 + 0.1 * 6.415478229522705
Epoch 310, val loss: 0.6250498294830322
Epoch 320, training loss: 0.8430812358856201 = 0.2021302878856659 + 0.1 * 6.409509181976318
Epoch 320, val loss: 0.6169227957725525
Epoch 330, training loss: 0.8177552223205566 = 0.17814119160175323 + 0.1 * 6.396140098571777
Epoch 330, val loss: 0.6116357445716858
Epoch 340, training loss: 0.7953334450721741 = 0.15731702744960785 + 0.1 * 6.380163669586182
Epoch 340, val loss: 0.6093502640724182
Epoch 350, training loss: 0.7766659259796143 = 0.13941198587417603 + 0.1 * 6.372539043426514
Epoch 350, val loss: 0.6094922423362732
Epoch 360, training loss: 0.7605730891227722 = 0.12410972267389297 + 0.1 * 6.364633083343506
Epoch 360, val loss: 0.6118444204330444
Epoch 370, training loss: 0.7468016743659973 = 0.11111034452915192 + 0.1 * 6.3569135665893555
Epoch 370, val loss: 0.61595219373703
Epoch 380, training loss: 0.7344993352890015 = 0.09992719441652298 + 0.1 * 6.345721244812012
Epoch 380, val loss: 0.6214216947555542
Epoch 390, training loss: 0.7297834157943726 = 0.09025052934885025 + 0.1 * 6.395328521728516
Epoch 390, val loss: 0.6278347373008728
Epoch 400, training loss: 0.7161333560943604 = 0.08195605874061584 + 0.1 * 6.341772556304932
Epoch 400, val loss: 0.6348912119865417
Epoch 410, training loss: 0.7072670459747314 = 0.07472202926874161 + 0.1 * 6.3254499435424805
Epoch 410, val loss: 0.6424393057823181
Epoch 420, training loss: 0.7005107402801514 = 0.06833624094724655 + 0.1 * 6.321744918823242
Epoch 420, val loss: 0.6503774523735046
Epoch 430, training loss: 0.6953185200691223 = 0.0627216175198555 + 0.1 * 6.3259687423706055
Epoch 430, val loss: 0.6585223078727722
Epoch 440, training loss: 0.6884421706199646 = 0.057768628001213074 + 0.1 * 6.306735038757324
Epoch 440, val loss: 0.6668460369110107
Epoch 450, training loss: 0.6839672923088074 = 0.0533546581864357 + 0.1 * 6.306126117706299
Epoch 450, val loss: 0.6752946376800537
Epoch 460, training loss: 0.6796972155570984 = 0.049420278519392014 + 0.1 * 6.302769184112549
Epoch 460, val loss: 0.683754563331604
Epoch 470, training loss: 0.6753143072128296 = 0.04589912295341492 + 0.1 * 6.294151782989502
Epoch 470, val loss: 0.692154049873352
Epoch 480, training loss: 0.6727292537689209 = 0.04273374751210213 + 0.1 * 6.299954891204834
Epoch 480, val loss: 0.7005188465118408
Epoch 490, training loss: 0.6682248115539551 = 0.03989480435848236 + 0.1 * 6.283299922943115
Epoch 490, val loss: 0.7088067531585693
Epoch 500, training loss: 0.6648346781730652 = 0.037323229014873505 + 0.1 * 6.2751145362854
Epoch 500, val loss: 0.7170133590698242
Epoch 510, training loss: 0.664738118648529 = 0.03498419374227524 + 0.1 * 6.297539234161377
Epoch 510, val loss: 0.7250666618347168
Epoch 520, training loss: 0.6598789095878601 = 0.03286539390683174 + 0.1 * 6.270134925842285
Epoch 520, val loss: 0.7330315113067627
Epoch 530, training loss: 0.6570009589195251 = 0.030933726578950882 + 0.1 * 6.260672092437744
Epoch 530, val loss: 0.7408705353736877
Epoch 540, training loss: 0.6558307409286499 = 0.02916562557220459 + 0.1 * 6.266651153564453
Epoch 540, val loss: 0.7485345005989075
Epoch 550, training loss: 0.653617262840271 = 0.02754615806043148 + 0.1 * 6.260710716247559
Epoch 550, val loss: 0.7560372352600098
Epoch 560, training loss: 0.6515306234359741 = 0.0260589811950922 + 0.1 * 6.254716396331787
Epoch 560, val loss: 0.7634044289588928
Epoch 570, training loss: 0.6497822403907776 = 0.024692432954907417 + 0.1 * 6.250898361206055
Epoch 570, val loss: 0.7706237435340881
Epoch 580, training loss: 0.6474774479866028 = 0.023431172594428062 + 0.1 * 6.240462303161621
Epoch 580, val loss: 0.7777243256568909
Epoch 590, training loss: 0.6459032297134399 = 0.02226799726486206 + 0.1 * 6.236352443695068
Epoch 590, val loss: 0.7847334146499634
Epoch 600, training loss: 0.647038459777832 = 0.021188851445913315 + 0.1 * 6.258495807647705
Epoch 600, val loss: 0.7916046977043152
Epoch 610, training loss: 0.6440161466598511 = 0.020189806818962097 + 0.1 * 6.238263130187988
Epoch 610, val loss: 0.7982739210128784
Epoch 620, training loss: 0.6421780586242676 = 0.019264204427599907 + 0.1 * 6.229138374328613
Epoch 620, val loss: 0.804833710193634
Epoch 630, training loss: 0.6407645344734192 = 0.018403897061944008 + 0.1 * 6.223606109619141
Epoch 630, val loss: 0.8113011717796326
Epoch 640, training loss: 0.6402602195739746 = 0.017599817365407944 + 0.1 * 6.226603984832764
Epoch 640, val loss: 0.8176242113113403
Epoch 650, training loss: 0.6397073268890381 = 0.016846930608153343 + 0.1 * 6.228603839874268
Epoch 650, val loss: 0.8236908316612244
Epoch 660, training loss: 0.6372932195663452 = 0.01614677906036377 + 0.1 * 6.2114644050598145
Epoch 660, val loss: 0.8298383355140686
Epoch 670, training loss: 0.63874751329422 = 0.015488842502236366 + 0.1 * 6.23258638381958
Epoch 670, val loss: 0.8358264565467834
Epoch 680, training loss: 0.6357160806655884 = 0.014872882515192032 + 0.1 * 6.208431720733643
Epoch 680, val loss: 0.8415952324867249
Epoch 690, training loss: 0.635293185710907 = 0.014297764748334885 + 0.1 * 6.209953784942627
Epoch 690, val loss: 0.8473794460296631
Epoch 700, training loss: 0.6341919898986816 = 0.013755171559751034 + 0.1 * 6.204368591308594
Epoch 700, val loss: 0.8530305624008179
Epoch 710, training loss: 0.6361491084098816 = 0.01324416883289814 + 0.1 * 6.2290496826171875
Epoch 710, val loss: 0.8585243225097656
Epoch 720, training loss: 0.6325504779815674 = 0.012762725353240967 + 0.1 * 6.197877407073975
Epoch 720, val loss: 0.8639254570007324
Epoch 730, training loss: 0.6317247748374939 = 0.012309410609304905 + 0.1 * 6.194153308868408
Epoch 730, val loss: 0.869300365447998
Epoch 740, training loss: 0.6315582990646362 = 0.0118794534355402 + 0.1 * 6.196788311004639
Epoch 740, val loss: 0.8745241165161133
Epoch 750, training loss: 0.6318477392196655 = 0.011474013328552246 + 0.1 * 6.203737258911133
Epoch 750, val loss: 0.8796315789222717
Epoch 760, training loss: 0.629520058631897 = 0.011091114953160286 + 0.1 * 6.184289455413818
Epoch 760, val loss: 0.8847129344940186
Epoch 770, training loss: 0.6297584772109985 = 0.010727486573159695 + 0.1 * 6.190310001373291
Epoch 770, val loss: 0.8897204399108887
Epoch 780, training loss: 0.6290621161460876 = 0.010381152853369713 + 0.1 * 6.186809539794922
Epoch 780, val loss: 0.8944977521896362
Epoch 790, training loss: 0.6284182071685791 = 0.010053466074168682 + 0.1 * 6.183647632598877
Epoch 790, val loss: 0.8992698192596436
Epoch 800, training loss: 0.6280526518821716 = 0.009742473252117634 + 0.1 * 6.183101654052734
Epoch 800, val loss: 0.9039698839187622
Epoch 810, training loss: 0.6270514130592346 = 0.00944533385336399 + 0.1 * 6.176061153411865
Epoch 810, val loss: 0.9085243344306946
Epoch 820, training loss: 0.6272655725479126 = 0.009163773618638515 + 0.1 * 6.1810173988342285
Epoch 820, val loss: 0.9130226969718933
Epoch 830, training loss: 0.6258506178855896 = 0.00889546051621437 + 0.1 * 6.169551372528076
Epoch 830, val loss: 0.9174686074256897
Epoch 840, training loss: 0.6258823275566101 = 0.008640200830996037 + 0.1 * 6.172420978546143
Epoch 840, val loss: 0.9218533635139465
Epoch 850, training loss: 0.6259007453918457 = 0.008395085111260414 + 0.1 * 6.175056457519531
Epoch 850, val loss: 0.9261120557785034
Epoch 860, training loss: 0.6271212697029114 = 0.008160212077200413 + 0.1 * 6.189610004425049
Epoch 860, val loss: 0.9302920699119568
Epoch 870, training loss: 0.6250758767127991 = 0.007936977781355381 + 0.1 * 6.171389102935791
Epoch 870, val loss: 0.9343869686126709
Epoch 880, training loss: 0.6239115595817566 = 0.007724728900939226 + 0.1 * 6.161868572235107
Epoch 880, val loss: 0.9385034441947937
Epoch 890, training loss: 0.6244061589241028 = 0.0075213308446109295 + 0.1 * 6.168848037719727
Epoch 890, val loss: 0.9424651861190796
Epoch 900, training loss: 0.6232665181159973 = 0.007326950319111347 + 0.1 * 6.159395694732666
Epoch 900, val loss: 0.9463893175125122
Epoch 910, training loss: 0.6239362955093384 = 0.007140839472413063 + 0.1 * 6.167954444885254
Epoch 910, val loss: 0.9502581357955933
Epoch 920, training loss: 0.6227786540985107 = 0.006962329149246216 + 0.1 * 6.158163070678711
Epoch 920, val loss: 0.954027533531189
Epoch 930, training loss: 0.6222083568572998 = 0.0067917718552052975 + 0.1 * 6.154165744781494
Epoch 930, val loss: 0.9577807188034058
Epoch 940, training loss: 0.6214766502380371 = 0.006628499832004309 + 0.1 * 6.148481369018555
Epoch 940, val loss: 0.9615215063095093
Epoch 950, training loss: 0.6228620409965515 = 0.006471198983490467 + 0.1 * 6.163908004760742
Epoch 950, val loss: 0.9651812314987183
Epoch 960, training loss: 0.6210671067237854 = 0.006319711916148663 + 0.1 * 6.1474738121032715
Epoch 960, val loss: 0.9687339663505554
Epoch 970, training loss: 0.6218740940093994 = 0.006174725946038961 + 0.1 * 6.156993389129639
Epoch 970, val loss: 0.9723167419433594
Epoch 980, training loss: 0.6209286451339722 = 0.0060343388468027115 + 0.1 * 6.148942947387695
Epoch 980, val loss: 0.9757924675941467
Epoch 990, training loss: 0.6219311356544495 = 0.005900323390960693 + 0.1 * 6.160307884216309
Epoch 990, val loss: 0.9792829751968384
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6568
Flip ASR: 0.6044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7866835594177246 = 1.9493075609207153 + 0.1 * 8.373761177062988
Epoch 0, val loss: 1.9524825811386108
Epoch 10, training loss: 2.776700019836426 = 1.9393670558929443 + 0.1 * 8.373330116271973
Epoch 10, val loss: 1.9425338506698608
Epoch 20, training loss: 2.7641494274139404 = 1.927090048789978 + 0.1 * 8.370594024658203
Epoch 20, val loss: 1.9297775030136108
Epoch 30, training loss: 2.7454237937927246 = 1.9097797870635986 + 0.1 * 8.356439590454102
Epoch 30, val loss: 1.9113699197769165
Epoch 40, training loss: 2.7125539779663086 = 1.8841524124145508 + 0.1 * 8.284016609191895
Epoch 40, val loss: 1.884190320968628
Epoch 50, training loss: 2.639502763748169 = 1.850558876991272 + 0.1 * 7.889438629150391
Epoch 50, val loss: 1.8500771522521973
Epoch 60, training loss: 2.5553555488586426 = 1.8144935369491577 + 0.1 * 7.4086198806762695
Epoch 60, val loss: 1.8144851922988892
Epoch 70, training loss: 2.4838004112243652 = 1.7760028839111328 + 0.1 * 7.077974796295166
Epoch 70, val loss: 1.7771869897842407
Epoch 80, training loss: 2.4274024963378906 = 1.7365968227386475 + 0.1 * 6.908057689666748
Epoch 80, val loss: 1.7406750917434692
Epoch 90, training loss: 2.36979603767395 = 1.6920886039733887 + 0.1 * 6.777073860168457
Epoch 90, val loss: 1.6999456882476807
Epoch 100, training loss: 2.3032898902893066 = 1.6331695318222046 + 0.1 * 6.701203346252441
Epoch 100, val loss: 1.6481536626815796
Epoch 110, training loss: 2.2220346927642822 = 1.5555673837661743 + 0.1 * 6.6646728515625
Epoch 110, val loss: 1.5834144353866577
Epoch 120, training loss: 2.1247737407684326 = 1.4602763652801514 + 0.1 * 6.644972801208496
Epoch 120, val loss: 1.5061382055282593
Epoch 130, training loss: 2.021134853363037 = 1.3580660820007324 + 0.1 * 6.6306867599487305
Epoch 130, val loss: 1.426526427268982
Epoch 140, training loss: 1.9202685356140137 = 1.2586681842803955 + 0.1 * 6.61600399017334
Epoch 140, val loss: 1.3524112701416016
Epoch 150, training loss: 1.826453685760498 = 1.1667828559875488 + 0.1 * 6.596707820892334
Epoch 150, val loss: 1.2866854667663574
Epoch 160, training loss: 1.7436316013336182 = 1.0853501558303833 + 0.1 * 6.5828142166137695
Epoch 160, val loss: 1.2296172380447388
Epoch 170, training loss: 1.6712970733642578 = 1.0149435997009277 + 0.1 * 6.563534259796143
Epoch 170, val loss: 1.1807632446289062
Epoch 180, training loss: 1.6090877056121826 = 0.954219400882721 + 0.1 * 6.54868221282959
Epoch 180, val loss: 1.1402409076690674
Epoch 190, training loss: 1.5550928115844727 = 0.901329755783081 + 0.1 * 6.537630081176758
Epoch 190, val loss: 1.1059341430664062
Epoch 200, training loss: 1.5055153369903564 = 0.8531766533851624 + 0.1 * 6.523387432098389
Epoch 200, val loss: 1.07517671585083
Epoch 210, training loss: 1.4574652910232544 = 0.8056890964508057 + 0.1 * 6.517761707305908
Epoch 210, val loss: 1.043824553489685
Epoch 220, training loss: 1.4064288139343262 = 0.756027340888977 + 0.1 * 6.50401496887207
Epoch 220, val loss: 1.0095374584197998
Epoch 230, training loss: 1.3514142036437988 = 0.7021010518074036 + 0.1 * 6.493130683898926
Epoch 230, val loss: 0.9704738855361938
Epoch 240, training loss: 1.2923133373260498 = 0.6441076993942261 + 0.1 * 6.482056617736816
Epoch 240, val loss: 0.9269392490386963
Epoch 250, training loss: 1.2310835123062134 = 0.5840264558792114 + 0.1 * 6.4705705642700195
Epoch 250, val loss: 0.8808925151824951
Epoch 260, training loss: 1.1719110012054443 = 0.5248588919639587 + 0.1 * 6.470520973205566
Epoch 260, val loss: 0.8357807993888855
Epoch 270, training loss: 1.1144455671310425 = 0.4694216549396515 + 0.1 * 6.4502387046813965
Epoch 270, val loss: 0.7950241565704346
Epoch 280, training loss: 1.0616410970687866 = 0.4177587032318115 + 0.1 * 6.438823699951172
Epoch 280, val loss: 0.7592726945877075
Epoch 290, training loss: 1.0134282112121582 = 0.3698365092277527 + 0.1 * 6.435917377471924
Epoch 290, val loss: 0.7286289930343628
Epoch 300, training loss: 0.9675841331481934 = 0.3258720636367798 + 0.1 * 6.417120456695557
Epoch 300, val loss: 0.7027204632759094
Epoch 310, training loss: 0.9265139102935791 = 0.2853916585445404 + 0.1 * 6.411222457885742
Epoch 310, val loss: 0.6805768013000488
Epoch 320, training loss: 0.8890621662139893 = 0.24890318512916565 + 0.1 * 6.401589393615723
Epoch 320, val loss: 0.6622899174690247
Epoch 330, training loss: 0.8558971881866455 = 0.2166469395160675 + 0.1 * 6.392502784729004
Epoch 330, val loss: 0.6479854583740234
Epoch 340, training loss: 0.8288213610649109 = 0.18840481340885162 + 0.1 * 6.404165267944336
Epoch 340, val loss: 0.637654721736908
Epoch 350, training loss: 0.8023544549942017 = 0.16435343027114868 + 0.1 * 6.38001012802124
Epoch 350, val loss: 0.6312150955200195
Epoch 360, training loss: 0.7809573411941528 = 0.14398425817489624 + 0.1 * 6.3697309494018555
Epoch 360, val loss: 0.6283231973648071
Epoch 370, training loss: 0.7627951502799988 = 0.12694914638996124 + 0.1 * 6.358459949493408
Epoch 370, val loss: 0.6284817457199097
Epoch 380, training loss: 0.747218906879425 = 0.1126781702041626 + 0.1 * 6.345407485961914
Epoch 380, val loss: 0.6310157179832458
Epoch 390, training loss: 0.7343324422836304 = 0.10059025138616562 + 0.1 * 6.337421894073486
Epoch 390, val loss: 0.6353740692138672
Epoch 400, training loss: 0.7271417379379272 = 0.0902923196554184 + 0.1 * 6.368494033813477
Epoch 400, val loss: 0.6411038041114807
Epoch 410, training loss: 0.713849663734436 = 0.0815519466996193 + 0.1 * 6.322977066040039
Epoch 410, val loss: 0.6476122736930847
Epoch 420, training loss: 0.7056338787078857 = 0.07402605563402176 + 0.1 * 6.316078186035156
Epoch 420, val loss: 0.6547377705574036
Epoch 430, training loss: 0.6986727118492126 = 0.06748686730861664 + 0.1 * 6.311858177185059
Epoch 430, val loss: 0.6621573567390442
Epoch 440, training loss: 0.6918781995773315 = 0.06177061051130295 + 0.1 * 6.3010759353637695
Epoch 440, val loss: 0.6697664856910706
Epoch 450, training loss: 0.6856203675270081 = 0.05674262344837189 + 0.1 * 6.2887773513793945
Epoch 450, val loss: 0.6775093674659729
Epoch 460, training loss: 0.6814177632331848 = 0.05230897665023804 + 0.1 * 6.291087627410889
Epoch 460, val loss: 0.6851704716682434
Epoch 470, training loss: 0.6790580153465271 = 0.048360880464315414 + 0.1 * 6.306971073150635
Epoch 470, val loss: 0.6928696036338806
Epoch 480, training loss: 0.672785758972168 = 0.044856227934360504 + 0.1 * 6.279294967651367
Epoch 480, val loss: 0.700448215007782
Epoch 490, training loss: 0.6686809659004211 = 0.04172443971037865 + 0.1 * 6.269565105438232
Epoch 490, val loss: 0.7079417109489441
Epoch 500, training loss: 0.6653732061386108 = 0.038900356739759445 + 0.1 * 6.26472806930542
Epoch 500, val loss: 0.7153448462486267
Epoch 510, training loss: 0.6631679534912109 = 0.036352984607219696 + 0.1 * 6.268149375915527
Epoch 510, val loss: 0.722611129283905
Epoch 520, training loss: 0.659046471118927 = 0.034049294888973236 + 0.1 * 6.249971866607666
Epoch 520, val loss: 0.7297971248626709
Epoch 530, training loss: 0.6573700308799744 = 0.03195057436823845 + 0.1 * 6.254194259643555
Epoch 530, val loss: 0.7368876338005066
Epoch 540, training loss: 0.6554601788520813 = 0.030040409415960312 + 0.1 * 6.25419807434082
Epoch 540, val loss: 0.7438842058181763
Epoch 550, training loss: 0.6526933908462524 = 0.02830410934984684 + 0.1 * 6.243892669677734
Epoch 550, val loss: 0.7506881952285767
Epoch 560, training loss: 0.650682270526886 = 0.026715852320194244 + 0.1 * 6.239663600921631
Epoch 560, val loss: 0.7573682069778442
Epoch 570, training loss: 0.6483607292175293 = 0.025256086140871048 + 0.1 * 6.231046199798584
Epoch 570, val loss: 0.7639086842536926
Epoch 580, training loss: 0.6488407254219055 = 0.023911822587251663 + 0.1 * 6.249288558959961
Epoch 580, val loss: 0.7703079581260681
Epoch 590, training loss: 0.6456524729728699 = 0.02267506532371044 + 0.1 * 6.229774475097656
Epoch 590, val loss: 0.7766158580780029
Epoch 600, training loss: 0.6454653739929199 = 0.021534709259867668 + 0.1 * 6.239306926727295
Epoch 600, val loss: 0.7827755212783813
Epoch 610, training loss: 0.6427618265151978 = 0.020483626052737236 + 0.1 * 6.222782135009766
Epoch 610, val loss: 0.7887906432151794
Epoch 620, training loss: 0.6415993571281433 = 0.01950983516871929 + 0.1 * 6.220894813537598
Epoch 620, val loss: 0.7946557402610779
Epoch 630, training loss: 0.6401832699775696 = 0.018604984506964684 + 0.1 * 6.215782642364502
Epoch 630, val loss: 0.8004453778266907
Epoch 640, training loss: 0.6387568116188049 = 0.017765900120139122 + 0.1 * 6.209908962249756
Epoch 640, val loss: 0.806052029132843
Epoch 650, training loss: 0.6383702158927917 = 0.016983043402433395 + 0.1 * 6.213871479034424
Epoch 650, val loss: 0.8115872740745544
Epoch 660, training loss: 0.6364637017250061 = 0.016251709312200546 + 0.1 * 6.202120304107666
Epoch 660, val loss: 0.8170019388198853
Epoch 670, training loss: 0.6356901526451111 = 0.015569215640425682 + 0.1 * 6.20120906829834
Epoch 670, val loss: 0.8222916126251221
Epoch 680, training loss: 0.6351177096366882 = 0.01492928434163332 + 0.1 * 6.201883792877197
Epoch 680, val loss: 0.8275011777877808
Epoch 690, training loss: 0.6340453624725342 = 0.014327405951917171 + 0.1 * 6.197179317474365
Epoch 690, val loss: 0.8325446248054504
Epoch 700, training loss: 0.6330602169036865 = 0.013763264752924442 + 0.1 * 6.192969799041748
Epoch 700, val loss: 0.8375723361968994
Epoch 710, training loss: 0.6325069069862366 = 0.013233994133770466 + 0.1 * 6.1927289962768555
Epoch 710, val loss: 0.8424893021583557
Epoch 720, training loss: 0.6325268745422363 = 0.012735830619931221 + 0.1 * 6.197910308837891
Epoch 720, val loss: 0.8473153114318848
Epoch 730, training loss: 0.6307061910629272 = 0.012265443801879883 + 0.1 * 6.1844072341918945
Epoch 730, val loss: 0.8520504832267761
Epoch 740, training loss: 0.6312324404716492 = 0.011824147775769234 + 0.1 * 6.194082736968994
Epoch 740, val loss: 0.8566948175430298
Epoch 750, training loss: 0.6292523741722107 = 0.011404871009290218 + 0.1 * 6.1784749031066895
Epoch 750, val loss: 0.8612684607505798
Epoch 760, training loss: 0.6290454864501953 = 0.01100881677120924 + 0.1 * 6.1803669929504395
Epoch 760, val loss: 0.8658125400543213
Epoch 770, training loss: 0.6277878284454346 = 0.010634229518473148 + 0.1 * 6.171535491943359
Epoch 770, val loss: 0.8702548146247864
Epoch 780, training loss: 0.6278180480003357 = 0.010278245434165001 + 0.1 * 6.175397872924805
Epoch 780, val loss: 0.874637246131897
Epoch 790, training loss: 0.6274595856666565 = 0.00994086917489767 + 0.1 * 6.175187110900879
Epoch 790, val loss: 0.8789471387863159
Epoch 800, training loss: 0.6270806789398193 = 0.00962140318006277 + 0.1 * 6.174592971801758
Epoch 800, val loss: 0.8832190632820129
Epoch 810, training loss: 0.6262685656547546 = 0.009319182485342026 + 0.1 * 6.169493675231934
Epoch 810, val loss: 0.8874454498291016
Epoch 820, training loss: 0.6257292628288269 = 0.009031157940626144 + 0.1 * 6.166980743408203
Epoch 820, val loss: 0.8915987014770508
Epoch 830, training loss: 0.625687301158905 = 0.008757560513913631 + 0.1 * 6.169297695159912
Epoch 830, val loss: 0.8956873416900635
Epoch 840, training loss: 0.6249432563781738 = 0.008496847003698349 + 0.1 * 6.164464473724365
Epoch 840, val loss: 0.8996963500976562
Epoch 850, training loss: 0.6264951825141907 = 0.008248159661889076 + 0.1 * 6.182470321655273
Epoch 850, val loss: 0.9036798477172852
Epoch 860, training loss: 0.6242036819458008 = 0.008011313155293465 + 0.1 * 6.161923408508301
Epoch 860, val loss: 0.9075990319252014
Epoch 870, training loss: 0.6236304640769958 = 0.007785387337207794 + 0.1 * 6.158450603485107
Epoch 870, val loss: 0.9114647507667542
Epoch 880, training loss: 0.6226450204849243 = 0.007569900713860989 + 0.1 * 6.150751113891602
Epoch 880, val loss: 0.9152594208717346
Epoch 890, training loss: 0.6222453117370605 = 0.007364619057625532 + 0.1 * 6.148806571960449
Epoch 890, val loss: 0.9190028309822083
Epoch 900, training loss: 0.623334527015686 = 0.007168755400925875 + 0.1 * 6.161657333374023
Epoch 900, val loss: 0.9226962327957153
Epoch 910, training loss: 0.6221619844436646 = 0.0069807651452720165 + 0.1 * 6.1518120765686035
Epoch 910, val loss: 0.9263037443161011
Epoch 920, training loss: 0.6221048831939697 = 0.006801423616707325 + 0.1 * 6.153034687042236
Epoch 920, val loss: 0.9298357367515564
Epoch 930, training loss: 0.6212874054908752 = 0.006628965958952904 + 0.1 * 6.1465840339660645
Epoch 930, val loss: 0.9333567023277283
Epoch 940, training loss: 0.6231908798217773 = 0.006463608238846064 + 0.1 * 6.167273044586182
Epoch 940, val loss: 0.9368305802345276
Epoch 950, training loss: 0.6206789612770081 = 0.006305591203272343 + 0.1 * 6.143733501434326
Epoch 950, val loss: 0.9402437806129456
Epoch 960, training loss: 0.6202253699302673 = 0.006154509726911783 + 0.1 * 6.140707969665527
Epoch 960, val loss: 0.9435946941375732
Epoch 970, training loss: 0.6209262013435364 = 0.006008523516356945 + 0.1 * 6.149177074432373
Epoch 970, val loss: 0.9469339847564697
Epoch 980, training loss: 0.6207639575004578 = 0.005868298467248678 + 0.1 * 6.148956298828125
Epoch 980, val loss: 0.9502344131469727
Epoch 990, training loss: 0.6191783547401428 = 0.005733960308134556 + 0.1 * 6.134443759918213
Epoch 990, val loss: 0.9534881711006165
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6900
Flip ASR: 0.6578/225 nodes
The final ASR:0.65560, 0.02864, Accuracy:0.81852, 0.00524
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7602899074554443 = 1.9229079484939575 + 0.1 * 8.373819351196289
Epoch 0, val loss: 1.9218388795852661
Epoch 10, training loss: 2.75148868560791 = 1.9141240119934082 + 0.1 * 8.373645782470703
Epoch 10, val loss: 1.913100004196167
Epoch 20, training loss: 2.740473508834839 = 1.9031906127929688 + 0.1 * 8.37282943725586
Epoch 20, val loss: 1.9021795988082886
Epoch 30, training loss: 2.7244467735290527 = 1.8876943588256836 + 0.1 * 8.367524147033691
Epoch 30, val loss: 1.8867086172103882
Epoch 40, training loss: 2.6974868774414062 = 1.8645482063293457 + 0.1 * 8.329385757446289
Epoch 40, val loss: 1.8637615442276
Epoch 50, training loss: 2.6396806240081787 = 1.8324326276779175 + 0.1 * 8.072479248046875
Epoch 50, val loss: 1.8333947658538818
Epoch 60, training loss: 2.5383150577545166 = 1.795028805732727 + 0.1 * 7.432862758636475
Epoch 60, val loss: 1.7999982833862305
Epoch 70, training loss: 2.4719183444976807 = 1.7564860582351685 + 0.1 * 7.154322147369385
Epoch 70, val loss: 1.767751693725586
Epoch 80, training loss: 2.4079062938690186 = 1.7121766805648804 + 0.1 * 6.957295894622803
Epoch 80, val loss: 1.7310622930526733
Epoch 90, training loss: 2.3408641815185547 = 1.6551026105880737 + 0.1 * 6.8576154708862305
Epoch 90, val loss: 1.6826928853988647
Epoch 100, training loss: 2.2606303691864014 = 1.5796667337417603 + 0.1 * 6.80963659286499
Epoch 100, val loss: 1.6189262866973877
Epoch 110, training loss: 2.1637678146362305 = 1.4862390756607056 + 0.1 * 6.775287628173828
Epoch 110, val loss: 1.542272925376892
Epoch 120, training loss: 2.0552525520324707 = 1.3811475038528442 + 0.1 * 6.741049766540527
Epoch 120, val loss: 1.457196831703186
Epoch 130, training loss: 1.9431705474853516 = 1.272322416305542 + 0.1 * 6.708480358123779
Epoch 130, val loss: 1.369899034500122
Epoch 140, training loss: 1.8359031677246094 = 1.1680233478546143 + 0.1 * 6.678798675537109
Epoch 140, val loss: 1.287627100944519
Epoch 150, training loss: 1.7396223545074463 = 1.0744627714157104 + 0.1 * 6.651596546173096
Epoch 150, val loss: 1.216006875038147
Epoch 160, training loss: 1.6572597026824951 = 0.9945352077484131 + 0.1 * 6.62724494934082
Epoch 160, val loss: 1.1567286252975464
Epoch 170, training loss: 1.5874346494674683 = 0.9264715313911438 + 0.1 * 6.609631061553955
Epoch 170, val loss: 1.1081149578094482
Epoch 180, training loss: 1.5256325006484985 = 0.8658849596977234 + 0.1 * 6.597475051879883
Epoch 180, val loss: 1.0660459995269775
Epoch 190, training loss: 1.4664485454559326 = 0.8073866963386536 + 0.1 * 6.590617656707764
Epoch 190, val loss: 1.0246706008911133
Epoch 200, training loss: 1.4059767723083496 = 0.747399091720581 + 0.1 * 6.585775852203369
Epoch 200, val loss: 0.9812248945236206
Epoch 210, training loss: 1.3432366847991943 = 0.6850625872612 + 0.1 * 6.581740856170654
Epoch 210, val loss: 0.9347241520881653
Epoch 220, training loss: 1.2805347442626953 = 0.6223682165145874 + 0.1 * 6.581665992736816
Epoch 220, val loss: 0.8877754211425781
Epoch 230, training loss: 1.2207159996032715 = 0.5633012056350708 + 0.1 * 6.574148178100586
Epoch 230, val loss: 0.8439722061157227
Epoch 240, training loss: 1.1669039726257324 = 0.5101357698440552 + 0.1 * 6.56768274307251
Epoch 240, val loss: 0.8055752515792847
Epoch 250, training loss: 1.11968994140625 = 0.46359768509864807 + 0.1 * 6.560922622680664
Epoch 250, val loss: 0.7740020155906677
Epoch 260, training loss: 1.0784974098205566 = 0.4228270351886749 + 0.1 * 6.556703567504883
Epoch 260, val loss: 0.7489585876464844
Epoch 270, training loss: 1.0409550666809082 = 0.3862801492214203 + 0.1 * 6.546749114990234
Epoch 270, val loss: 0.7292090058326721
Epoch 280, training loss: 1.00612473487854 = 0.3522726893424988 + 0.1 * 6.538520812988281
Epoch 280, val loss: 0.7131341695785522
Epoch 290, training loss: 0.9733234643936157 = 0.3199833333492279 + 0.1 * 6.533401012420654
Epoch 290, val loss: 0.6997027397155762
Epoch 300, training loss: 0.9412551522254944 = 0.2890056371688843 + 0.1 * 6.522495269775391
Epoch 300, val loss: 0.6883454918861389
Epoch 310, training loss: 0.9106117486953735 = 0.259247750043869 + 0.1 * 6.513639450073242
Epoch 310, val loss: 0.6784621477127075
Epoch 320, training loss: 0.8816788196563721 = 0.2310100793838501 + 0.1 * 6.506687164306641
Epoch 320, val loss: 0.6701242327690125
Epoch 330, training loss: 0.8552999496459961 = 0.20488184690475464 + 0.1 * 6.504180908203125
Epoch 330, val loss: 0.6632817983627319
Epoch 340, training loss: 0.8309437036514282 = 0.18135687708854675 + 0.1 * 6.495867729187012
Epoch 340, val loss: 0.6584673523902893
Epoch 350, training loss: 0.8091773986816406 = 0.16043460369110107 + 0.1 * 6.487427711486816
Epoch 350, val loss: 0.6554585695266724
Epoch 360, training loss: 0.7910192012786865 = 0.14206771552562714 + 0.1 * 6.4895148277282715
Epoch 360, val loss: 0.6543630957603455
Epoch 370, training loss: 0.7736434936523438 = 0.12614527344703674 + 0.1 * 6.474982261657715
Epoch 370, val loss: 0.6550662517547607
Epoch 380, training loss: 0.7592266201972961 = 0.11229167133569717 + 0.1 * 6.469349384307861
Epoch 380, val loss: 0.6573655605316162
Epoch 390, training loss: 0.7467272281646729 = 0.10021190345287323 + 0.1 * 6.465152740478516
Epoch 390, val loss: 0.6611329913139343
Epoch 400, training loss: 0.7357245087623596 = 0.08969897776842117 + 0.1 * 6.460255146026611
Epoch 400, val loss: 0.6661316752433777
Epoch 410, training loss: 0.7259825468063354 = 0.08052808791399002 + 0.1 * 6.454544544219971
Epoch 410, val loss: 0.6721296906471252
Epoch 420, training loss: 0.7174651026725769 = 0.07248397171497345 + 0.1 * 6.4498114585876465
Epoch 420, val loss: 0.6789982318878174
Epoch 430, training loss: 0.7096882462501526 = 0.06543682515621185 + 0.1 * 6.442513942718506
Epoch 430, val loss: 0.6864978671073914
Epoch 440, training loss: 0.7032820582389832 = 0.059249408543109894 + 0.1 * 6.44032621383667
Epoch 440, val loss: 0.6944742798805237
Epoch 450, training loss: 0.6976834535598755 = 0.053806155920028687 + 0.1 * 6.438772678375244
Epoch 450, val loss: 0.7028465270996094
Epoch 460, training loss: 0.6922280788421631 = 0.049021296203136444 + 0.1 * 6.43206787109375
Epoch 460, val loss: 0.7114821672439575
Epoch 470, training loss: 0.687455415725708 = 0.04479558393359184 + 0.1 * 6.42659854888916
Epoch 470, val loss: 0.7202390432357788
Epoch 480, training loss: 0.6844203472137451 = 0.0410531684756279 + 0.1 * 6.433671474456787
Epoch 480, val loss: 0.7291721105575562
Epoch 490, training loss: 0.6798228621482849 = 0.03775133937597275 + 0.1 * 6.42071533203125
Epoch 490, val loss: 0.7380870580673218
Epoch 500, training loss: 0.6763771176338196 = 0.03481399640440941 + 0.1 * 6.41563081741333
Epoch 500, val loss: 0.7469931244850159
Epoch 510, training loss: 0.6735994219779968 = 0.03219486400485039 + 0.1 * 6.414045333862305
Epoch 510, val loss: 0.755935549736023
Epoch 520, training loss: 0.6707483530044556 = 0.02985532581806183 + 0.1 * 6.40893030166626
Epoch 520, val loss: 0.7647771835327148
Epoch 530, training loss: 0.6698591113090515 = 0.027756957337260246 + 0.1 * 6.421021938323975
Epoch 530, val loss: 0.7735036015510559
Epoch 540, training loss: 0.6660553812980652 = 0.025883812457323074 + 0.1 * 6.4017157554626465
Epoch 540, val loss: 0.7821431756019592
Epoch 550, training loss: 0.6637537479400635 = 0.02419501356780529 + 0.1 * 6.39558744430542
Epoch 550, val loss: 0.7906247973442078
Epoch 560, training loss: 0.6617375612258911 = 0.022663623094558716 + 0.1 * 6.390739440917969
Epoch 560, val loss: 0.7990895509719849
Epoch 570, training loss: 0.6598979234695435 = 0.021270139142870903 + 0.1 * 6.386277675628662
Epoch 570, val loss: 0.8074011206626892
Epoch 580, training loss: 0.6604443192481995 = 0.020001238211989403 + 0.1 * 6.404430866241455
Epoch 580, val loss: 0.8154836297035217
Epoch 590, training loss: 0.6570375561714172 = 0.018860943615436554 + 0.1 * 6.381766319274902
Epoch 590, val loss: 0.8234041929244995
Epoch 600, training loss: 0.6553652286529541 = 0.017826247960329056 + 0.1 * 6.375389575958252
Epoch 600, val loss: 0.8310343027114868
Epoch 610, training loss: 0.6539191007614136 = 0.016876831650733948 + 0.1 * 6.370422840118408
Epoch 610, val loss: 0.8387039303779602
Epoch 620, training loss: 0.6524817943572998 = 0.016000453382730484 + 0.1 * 6.364813327789307
Epoch 620, val loss: 0.8461471199989319
Epoch 630, training loss: 0.6517463326454163 = 0.01519029587507248 + 0.1 * 6.365560054779053
Epoch 630, val loss: 0.853500485420227
Epoch 640, training loss: 0.6520307660102844 = 0.014444883912801743 + 0.1 * 6.375858783721924
Epoch 640, val loss: 0.8605040311813354
Epoch 650, training loss: 0.6496845483779907 = 0.013762871734797955 + 0.1 * 6.359216213226318
Epoch 650, val loss: 0.8675199151039124
Epoch 660, training loss: 0.6478538513183594 = 0.013130489736795425 + 0.1 * 6.347233295440674
Epoch 660, val loss: 0.8743302226066589
Epoch 670, training loss: 0.6475029587745667 = 0.012542132288217545 + 0.1 * 6.349608421325684
Epoch 670, val loss: 0.8809921145439148
Epoch 680, training loss: 0.6457651853561401 = 0.011996676214039326 + 0.1 * 6.3376851081848145
Epoch 680, val loss: 0.887511134147644
Epoch 690, training loss: 0.6456208229064941 = 0.011488961055874825 + 0.1 * 6.341318130493164
Epoch 690, val loss: 0.8938792943954468
Epoch 700, training loss: 0.6443579196929932 = 0.011014054529368877 + 0.1 * 6.333438396453857
Epoch 700, val loss: 0.9001602530479431
Epoch 710, training loss: 0.6437429189682007 = 0.010572507977485657 + 0.1 * 6.331704139709473
Epoch 710, val loss: 0.9061383008956909
Epoch 720, training loss: 0.6424756050109863 = 0.010161935351788998 + 0.1 * 6.323136329650879
Epoch 720, val loss: 0.9121992588043213
Epoch 730, training loss: 0.6422206163406372 = 0.009777461178600788 + 0.1 * 6.324431419372559
Epoch 730, val loss: 0.9181126356124878
Epoch 740, training loss: 0.6418139338493347 = 0.00941560510545969 + 0.1 * 6.3239827156066895
Epoch 740, val loss: 0.9238068461418152
Epoch 750, training loss: 0.6396421194076538 = 0.00907727051526308 + 0.1 * 6.305648326873779
Epoch 750, val loss: 0.9294980764389038
Epoch 760, training loss: 0.6397095322608948 = 0.008758613839745522 + 0.1 * 6.30950927734375
Epoch 760, val loss: 0.9350579977035522
Epoch 770, training loss: 0.6407750844955444 = 0.00845795962959528 + 0.1 * 6.323171138763428
Epoch 770, val loss: 0.9401776790618896
Epoch 780, training loss: 0.6378388404846191 = 0.008178633637726307 + 0.1 * 6.29660177230835
Epoch 780, val loss: 0.9455512166023254
Epoch 790, training loss: 0.6367294788360596 = 0.007915527559816837 + 0.1 * 6.288139343261719
Epoch 790, val loss: 0.9507354497909546
Epoch 800, training loss: 0.6358889937400818 = 0.00766541063785553 + 0.1 * 6.282236099243164
Epoch 800, val loss: 0.9558761119842529
Epoch 810, training loss: 0.6359942555427551 = 0.007426464464515448 + 0.1 * 6.285677909851074
Epoch 810, val loss: 0.9607436656951904
Epoch 820, training loss: 0.6350579857826233 = 0.0072004119865596294 + 0.1 * 6.278575420379639
Epoch 820, val loss: 0.9655802249908447
Epoch 830, training loss: 0.6371558308601379 = 0.006987215951085091 + 0.1 * 6.301685810089111
Epoch 830, val loss: 0.9704518914222717
Epoch 840, training loss: 0.6339264512062073 = 0.006783838849514723 + 0.1 * 6.271425724029541
Epoch 840, val loss: 0.9750235080718994
Epoch 850, training loss: 0.6336110830307007 = 0.006592821329832077 + 0.1 * 6.2701826095581055
Epoch 850, val loss: 0.9797555208206177
Epoch 860, training loss: 0.6331205368041992 = 0.006409902125597 + 0.1 * 6.267106533050537
Epoch 860, val loss: 0.9842676520347595
Epoch 870, training loss: 0.631758987903595 = 0.006234942469745874 + 0.1 * 6.255240440368652
Epoch 870, val loss: 0.9886764883995056
Epoch 880, training loss: 0.6333107352256775 = 0.0060687074437737465 + 0.1 * 6.2724199295043945
Epoch 880, val loss: 0.9930950999259949
Epoch 890, training loss: 0.6314008831977844 = 0.0059091076254844666 + 0.1 * 6.254917621612549
Epoch 890, val loss: 0.9972613453865051
Epoch 900, training loss: 0.6321852803230286 = 0.005757959559559822 + 0.1 * 6.264272689819336
Epoch 900, val loss: 1.0014793872833252
Epoch 910, training loss: 0.6298913955688477 = 0.005613590124994516 + 0.1 * 6.242778301239014
Epoch 910, val loss: 1.0055954456329346
Epoch 920, training loss: 0.6299030780792236 = 0.00547584006562829 + 0.1 * 6.244272232055664
Epoch 920, val loss: 1.0097287893295288
Epoch 930, training loss: 0.6296724081039429 = 0.005343042314052582 + 0.1 * 6.243293762207031
Epoch 930, val loss: 1.0135324001312256
Epoch 940, training loss: 0.6287044882774353 = 0.005216831807047129 + 0.1 * 6.23487663269043
Epoch 940, val loss: 1.017520785331726
Epoch 950, training loss: 0.6287358999252319 = 0.005096121225506067 + 0.1 * 6.2363972663879395
Epoch 950, val loss: 1.0214043855667114
Epoch 960, training loss: 0.6289445757865906 = 0.0049796211533248425 + 0.1 * 6.239649772644043
Epoch 960, val loss: 1.0250943899154663
Epoch 970, training loss: 0.6279048323631287 = 0.004867876414209604 + 0.1 * 6.230369567871094
Epoch 970, val loss: 1.0288501977920532
Epoch 980, training loss: 0.626909077167511 = 0.00476068165153265 + 0.1 * 6.2214837074279785
Epoch 980, val loss: 1.0324634313583374
Epoch 990, training loss: 0.6262233853340149 = 0.004658512305468321 + 0.1 * 6.215648651123047
Epoch 990, val loss: 1.0361171960830688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5978
Flip ASR: 0.5244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7849955558776855 = 1.947618007659912 + 0.1 * 8.37377643585205
Epoch 0, val loss: 1.9461846351623535
Epoch 10, training loss: 2.774801254272461 = 1.9374468326568604 + 0.1 * 8.373543739318848
Epoch 10, val loss: 1.935144066810608
Epoch 20, training loss: 2.7619848251342773 = 1.9247699975967407 + 0.1 * 8.372148513793945
Epoch 20, val loss: 1.921102523803711
Epoch 30, training loss: 2.742506504058838 = 1.9062938690185547 + 0.1 * 8.362127304077148
Epoch 30, val loss: 1.9004708528518677
Epoch 40, training loss: 2.708946704864502 = 1.878246545791626 + 0.1 * 8.307000160217285
Epoch 40, val loss: 1.8695210218429565
Epoch 50, training loss: 2.6337838172912598 = 1.8406003713607788 + 0.1 * 7.9318342208862305
Epoch 50, val loss: 1.8298048973083496
Epoch 60, training loss: 2.5409908294677734 = 1.7983946800231934 + 0.1 * 7.425962448120117
Epoch 60, val loss: 1.7877180576324463
Epoch 70, training loss: 2.468136787414551 = 1.756382942199707 + 0.1 * 7.117538928985596
Epoch 70, val loss: 1.7478171586990356
Epoch 80, training loss: 2.410715103149414 = 1.7135061025619507 + 0.1 * 6.9720892906188965
Epoch 80, val loss: 1.7096781730651855
Epoch 90, training loss: 2.347909450531006 = 1.6610575914382935 + 0.1 * 6.868518352508545
Epoch 90, val loss: 1.664000153541565
Epoch 100, training loss: 2.2728421688079834 = 1.5924874544143677 + 0.1 * 6.8035478591918945
Epoch 100, val loss: 1.6068273782730103
Epoch 110, training loss: 2.1826186180114746 = 1.5064113140106201 + 0.1 * 6.762073040008545
Epoch 110, val loss: 1.5378413200378418
Epoch 120, training loss: 2.0823974609375 = 1.408772349357605 + 0.1 * 6.736251354217529
Epoch 120, val loss: 1.4615720510482788
Epoch 130, training loss: 1.9812860488891602 = 1.3097103834152222 + 0.1 * 6.715756416320801
Epoch 130, val loss: 1.3873156309127808
Epoch 140, training loss: 1.8839949369430542 = 1.2150115966796875 + 0.1 * 6.689833164215088
Epoch 140, val loss: 1.3205218315124512
Epoch 150, training loss: 1.7910349369049072 = 1.1249628067016602 + 0.1 * 6.660721302032471
Epoch 150, val loss: 1.259804368019104
Epoch 160, training loss: 1.702026605606079 = 1.038385272026062 + 0.1 * 6.636412620544434
Epoch 160, val loss: 1.2014235258102417
Epoch 170, training loss: 1.6170704364776611 = 0.9550780057907104 + 0.1 * 6.619924545288086
Epoch 170, val loss: 1.1455879211425781
Epoch 180, training loss: 1.537644624710083 = 0.8767987489700317 + 0.1 * 6.608458518981934
Epoch 180, val loss: 1.0934101343154907
Epoch 190, training loss: 1.465245246887207 = 0.8054308891296387 + 0.1 * 6.598143100738525
Epoch 190, val loss: 1.0463471412658691
Epoch 200, training loss: 1.3998188972473145 = 0.7411478757858276 + 0.1 * 6.586709976196289
Epoch 200, val loss: 1.0043816566467285
Epoch 210, training loss: 1.341566801071167 = 0.6840330958366394 + 0.1 * 6.5753374099731445
Epoch 210, val loss: 0.968353807926178
Epoch 220, training loss: 1.2901108264923096 = 0.6333296895027161 + 0.1 * 6.567811965942383
Epoch 220, val loss: 0.9380714297294617
Epoch 230, training loss: 1.242708444595337 = 0.5870254039764404 + 0.1 * 6.556829929351807
Epoch 230, val loss: 0.9120407104492188
Epoch 240, training loss: 1.1972382068634033 = 0.5425891876220703 + 0.1 * 6.546490669250488
Epoch 240, val loss: 0.8885750770568848
Epoch 250, training loss: 1.152794599533081 = 0.49919965863227844 + 0.1 * 6.535948753356934
Epoch 250, val loss: 0.8675251603126526
Epoch 260, training loss: 1.1089047193527222 = 0.45660051703453064 + 0.1 * 6.523041725158691
Epoch 260, val loss: 0.8490524291992188
Epoch 270, training loss: 1.066523790359497 = 0.41522374749183655 + 0.1 * 6.513000011444092
Epoch 270, val loss: 0.8336206078529358
Epoch 280, training loss: 1.0259082317352295 = 0.37606731057167053 + 0.1 * 6.498408794403076
Epoch 280, val loss: 0.8222194314002991
Epoch 290, training loss: 0.9891188144683838 = 0.3397091031074524 + 0.1 * 6.494096755981445
Epoch 290, val loss: 0.8145866990089417
Epoch 300, training loss: 0.9547437429428101 = 0.30682215094566345 + 0.1 * 6.4792160987854
Epoch 300, val loss: 0.8101853132247925
Epoch 310, training loss: 0.9231829643249512 = 0.2771361768245697 + 0.1 * 6.46046781539917
Epoch 310, val loss: 0.8084908723831177
Epoch 320, training loss: 0.8950805068016052 = 0.2500694990158081 + 0.1 * 6.450109958648682
Epoch 320, val loss: 0.8089579939842224
Epoch 330, training loss: 0.8698636293411255 = 0.2254796028137207 + 0.1 * 6.443840026855469
Epoch 330, val loss: 0.8112736940383911
Epoch 340, training loss: 0.8464128971099854 = 0.20333121716976166 + 0.1 * 6.430816650390625
Epoch 340, val loss: 0.8150460720062256
Epoch 350, training loss: 0.8256922960281372 = 0.18344423174858093 + 0.1 * 6.422480583190918
Epoch 350, val loss: 0.8198774456977844
Epoch 360, training loss: 0.8093352317810059 = 0.16569308936595917 + 0.1 * 6.4364213943481445
Epoch 360, val loss: 0.8258681297302246
Epoch 370, training loss: 0.7910047769546509 = 0.15006521344184875 + 0.1 * 6.409395217895508
Epoch 370, val loss: 0.832998514175415
Epoch 380, training loss: 0.7762755751609802 = 0.1362028270959854 + 0.1 * 6.40072774887085
Epoch 380, val loss: 0.8412256240844727
Epoch 390, training loss: 0.7642147541046143 = 0.1238003671169281 + 0.1 * 6.404143333435059
Epoch 390, val loss: 0.8502324223518372
Epoch 400, training loss: 0.7515960931777954 = 0.1127120777964592 + 0.1 * 6.3888397216796875
Epoch 400, val loss: 0.8599197268486023
Epoch 410, training loss: 0.7412370443344116 = 0.1027175784111023 + 0.1 * 6.385194301605225
Epoch 410, val loss: 0.8701134324073792
Epoch 420, training loss: 0.7314663529396057 = 0.09365645796060562 + 0.1 * 6.378098487854004
Epoch 420, val loss: 0.880754292011261
Epoch 430, training loss: 0.721352219581604 = 0.08543244004249573 + 0.1 * 6.35919713973999
Epoch 430, val loss: 0.8916289210319519
Epoch 440, training loss: 0.7132464051246643 = 0.0779758095741272 + 0.1 * 6.352705955505371
Epoch 440, val loss: 0.9027644991874695
Epoch 450, training loss: 0.7064806818962097 = 0.07126269489526749 + 0.1 * 6.352179527282715
Epoch 450, val loss: 0.9139713048934937
Epoch 460, training loss: 0.6993470191955566 = 0.065291628241539 + 0.1 * 6.3405537605285645
Epoch 460, val loss: 0.9252063632011414
Epoch 470, training loss: 0.6933386325836182 = 0.059982866048812866 + 0.1 * 6.333558082580566
Epoch 470, val loss: 0.9365993142127991
Epoch 480, training loss: 0.6876571774482727 = 0.055253103375434875 + 0.1 * 6.32404088973999
Epoch 480, val loss: 0.9481167197227478
Epoch 490, training loss: 0.6829512715339661 = 0.051013801246881485 + 0.1 * 6.319374084472656
Epoch 490, val loss: 0.9598556160926819
Epoch 500, training loss: 0.6811568737030029 = 0.047189775854349136 + 0.1 * 6.3396711349487305
Epoch 500, val loss: 0.971631646156311
Epoch 510, training loss: 0.6744835376739502 = 0.04375871643424034 + 0.1 * 6.307248115539551
Epoch 510, val loss: 0.9835323691368103
Epoch 520, training loss: 0.670551598072052 = 0.040651578456163406 + 0.1 * 6.299000263214111
Epoch 520, val loss: 0.9952627420425415
Epoch 530, training loss: 0.6675734519958496 = 0.03781917691230774 + 0.1 * 6.297542572021484
Epoch 530, val loss: 1.0070527791976929
Epoch 540, training loss: 0.6649816632270813 = 0.035247936844825745 + 0.1 * 6.297337055206299
Epoch 540, val loss: 1.018593668937683
Epoch 550, training loss: 0.661751389503479 = 0.03292366862297058 + 0.1 * 6.2882771492004395
Epoch 550, val loss: 1.0302244424819946
Epoch 560, training loss: 0.6585603952407837 = 0.030807387083768845 + 0.1 * 6.277530193328857
Epoch 560, val loss: 1.0415267944335938
Epoch 570, training loss: 0.656537652015686 = 0.028875062242150307 + 0.1 * 6.276625633239746
Epoch 570, val loss: 1.0527645349502563
Epoch 580, training loss: 0.6559545993804932 = 0.027109118178486824 + 0.1 * 6.288454532623291
Epoch 580, val loss: 1.0638830661773682
Epoch 590, training loss: 0.6524751782417297 = 0.02549470029771328 + 0.1 * 6.269804954528809
Epoch 590, val loss: 1.0746686458587646
Epoch 600, training loss: 0.6498169302940369 = 0.024020254611968994 + 0.1 * 6.2579665184021
Epoch 600, val loss: 1.0853309631347656
Epoch 610, training loss: 0.6495910882949829 = 0.022663265466690063 + 0.1 * 6.269278049468994
Epoch 610, val loss: 1.0958489179611206
Epoch 620, training loss: 0.6464968323707581 = 0.021416615694761276 + 0.1 * 6.250802040100098
Epoch 620, val loss: 1.1061770915985107
Epoch 630, training loss: 0.6453497409820557 = 0.020267752930521965 + 0.1 * 6.250820159912109
Epoch 630, val loss: 1.1163015365600586
Epoch 640, training loss: 0.6430297493934631 = 0.01920805498957634 + 0.1 * 6.238216876983643
Epoch 640, val loss: 1.1260759830474854
Epoch 650, training loss: 0.6419581770896912 = 0.018231304362416267 + 0.1 * 6.237268447875977
Epoch 650, val loss: 1.1358420848846436
Epoch 660, training loss: 0.6414793729782104 = 0.017327191308140755 + 0.1 * 6.241521835327148
Epoch 660, val loss: 1.145259976387024
Epoch 670, training loss: 0.6411947011947632 = 0.01649252511560917 + 0.1 * 6.247021198272705
Epoch 670, val loss: 1.154543399810791
Epoch 680, training loss: 0.6385753154754639 = 0.015718773007392883 + 0.1 * 6.228565216064453
Epoch 680, val loss: 1.1636230945587158
Epoch 690, training loss: 0.6386508941650391 = 0.01499977707862854 + 0.1 * 6.236510753631592
Epoch 690, val loss: 1.1724752187728882
Epoch 700, training loss: 0.6365946531295776 = 0.01433174405246973 + 0.1 * 6.222629070281982
Epoch 700, val loss: 1.1810872554779053
Epoch 710, training loss: 0.636191189289093 = 0.013710036873817444 + 0.1 * 6.224811553955078
Epoch 710, val loss: 1.1895434856414795
Epoch 720, training loss: 0.6348747611045837 = 0.013128809630870819 + 0.1 * 6.217459201812744
Epoch 720, val loss: 1.1978883743286133
Epoch 730, training loss: 0.6342533826828003 = 0.012585794553160667 + 0.1 * 6.216675758361816
Epoch 730, val loss: 1.2058002948760986
Epoch 740, training loss: 0.6337363719940186 = 0.012079079635441303 + 0.1 * 6.216573238372803
Epoch 740, val loss: 1.2138049602508545
Epoch 750, training loss: 0.6328033208847046 = 0.01160468440502882 + 0.1 * 6.211986541748047
Epoch 750, val loss: 1.2215250730514526
Epoch 760, training loss: 0.6322839856147766 = 0.01115976832807064 + 0.1 * 6.211242198944092
Epoch 760, val loss: 1.2290751934051514
Epoch 770, training loss: 0.6305725574493408 = 0.0107415234670043 + 0.1 * 6.198309898376465
Epoch 770, val loss: 1.2365128993988037
Epoch 780, training loss: 0.6310233473777771 = 0.010347133502364159 + 0.1 * 6.206762313842773
Epoch 780, val loss: 1.2437907457351685
Epoch 790, training loss: 0.630547046661377 = 0.009975416585803032 + 0.1 * 6.205716133117676
Epoch 790, val loss: 1.2509047985076904
Epoch 800, training loss: 0.6295889019966125 = 0.009625374339520931 + 0.1 * 6.199635028839111
Epoch 800, val loss: 1.2578942775726318
Epoch 810, training loss: 0.6288727521896362 = 0.009294466115534306 + 0.1 * 6.1957831382751465
Epoch 810, val loss: 1.2646480798721313
Epoch 820, training loss: 0.629252016544342 = 0.008981820195913315 + 0.1 * 6.202702045440674
Epoch 820, val loss: 1.2712631225585938
Epoch 830, training loss: 0.6272373795509338 = 0.008685583248734474 + 0.1 * 6.18551778793335
Epoch 830, val loss: 1.2777559757232666
Epoch 840, training loss: 0.6276538968086243 = 0.008405487984418869 + 0.1 * 6.192483901977539
Epoch 840, val loss: 1.2841639518737793
Epoch 850, training loss: 0.6267171502113342 = 0.008139554411172867 + 0.1 * 6.1857757568359375
Epoch 850, val loss: 1.2902978658676147
Epoch 860, training loss: 0.6266593337059021 = 0.007887405343353748 + 0.1 * 6.187718868255615
Epoch 860, val loss: 1.2962470054626465
Epoch 870, training loss: 0.6259087920188904 = 0.007648255676031113 + 0.1 * 6.182605266571045
Epoch 870, val loss: 1.3023172616958618
Epoch 880, training loss: 0.6259771585464478 = 0.007420728448778391 + 0.1 * 6.1855645179748535
Epoch 880, val loss: 1.3079510927200317
Epoch 890, training loss: 0.624379575252533 = 0.0072043840773403645 + 0.1 * 6.171751499176025
Epoch 890, val loss: 1.3136651515960693
Epoch 900, training loss: 0.6244488954544067 = 0.006998509168624878 + 0.1 * 6.174504280090332
Epoch 900, val loss: 1.3191629648208618
Epoch 910, training loss: 0.6237594485282898 = 0.0068025593645870686 + 0.1 * 6.1695685386657715
Epoch 910, val loss: 1.3246465921401978
Epoch 920, training loss: 0.6240149140357971 = 0.0066148159094154835 + 0.1 * 6.174001216888428
Epoch 920, val loss: 1.329984426498413
Epoch 930, training loss: 0.6229433417320251 = 0.006435707677155733 + 0.1 * 6.16507625579834
Epoch 930, val loss: 1.3351658582687378
Epoch 940, training loss: 0.6234006285667419 = 0.006264643743634224 + 0.1 * 6.171360015869141
Epoch 940, val loss: 1.340348720550537
Epoch 950, training loss: 0.6230164170265198 = 0.006101179867982864 + 0.1 * 6.16915225982666
Epoch 950, val loss: 1.3454530239105225
Epoch 960, training loss: 0.6220248937606812 = 0.0059447032399475574 + 0.1 * 6.160801410675049
Epoch 960, val loss: 1.3504620790481567
Epoch 970, training loss: 0.6212575435638428 = 0.00579488929361105 + 0.1 * 6.154626369476318
Epoch 970, val loss: 1.3553801774978638
Epoch 980, training loss: 0.6225191950798035 = 0.00565159460529685 + 0.1 * 6.168675899505615
Epoch 980, val loss: 1.3602485656738281
Epoch 990, training loss: 0.6219910979270935 = 0.005514081567525864 + 0.1 * 6.164770126342773
Epoch 990, val loss: 1.3648031949996948
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.8118
Flip ASR: 0.7778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.8017802238464355 = 1.9644055366516113 + 0.1 * 8.373746871948242
Epoch 0, val loss: 1.9604525566101074
Epoch 10, training loss: 2.79059100151062 = 1.9532511234283447 + 0.1 * 8.373398780822754
Epoch 10, val loss: 1.9488577842712402
Epoch 20, training loss: 2.776642084121704 = 1.9395259618759155 + 0.1 * 8.371160507202148
Epoch 20, val loss: 1.9344310760498047
Epoch 30, training loss: 2.7556698322296143 = 1.9201101064682007 + 0.1 * 8.355597496032715
Epoch 30, val loss: 1.9139550924301147
Epoch 40, training loss: 2.718513011932373 = 1.8911625146865845 + 0.1 * 8.273506164550781
Epoch 40, val loss: 1.8840364217758179
Epoch 50, training loss: 2.6367390155792236 = 1.852411150932312 + 0.1 * 7.8432793617248535
Epoch 50, val loss: 1.8461294174194336
Epoch 60, training loss: 2.5536210536956787 = 1.811407446861267 + 0.1 * 7.422136306762695
Epoch 60, val loss: 1.8087824583053589
Epoch 70, training loss: 2.47566556930542 = 1.773736596107483 + 0.1 * 7.019289493560791
Epoch 70, val loss: 1.777208685874939
Epoch 80, training loss: 2.4161553382873535 = 1.7390042543411255 + 0.1 * 6.771510124206543
Epoch 80, val loss: 1.7495512962341309
Epoch 90, training loss: 2.3612184524536133 = 1.6938029527664185 + 0.1 * 6.674155235290527
Epoch 90, val loss: 1.711320400238037
Epoch 100, training loss: 2.295517921447754 = 1.6319433450698853 + 0.1 * 6.635744571685791
Epoch 100, val loss: 1.659256935119629
Epoch 110, training loss: 2.2137763500213623 = 1.5530357360839844 + 0.1 * 6.607405662536621
Epoch 110, val loss: 1.595000147819519
Epoch 120, training loss: 2.122706890106201 = 1.4643452167510986 + 0.1 * 6.583616256713867
Epoch 120, val loss: 1.5250835418701172
Epoch 130, training loss: 2.031999111175537 = 1.375535488128662 + 0.1 * 6.564636707305908
Epoch 130, val loss: 1.4580477476119995
Epoch 140, training loss: 1.945020318031311 = 1.2899341583251953 + 0.1 * 6.550861358642578
Epoch 140, val loss: 1.396053671836853
Epoch 150, training loss: 1.8593114614486694 = 1.2055140733718872 + 0.1 * 6.537973880767822
Epoch 150, val loss: 1.3370263576507568
Epoch 160, training loss: 1.7746336460113525 = 1.1219850778579712 + 0.1 * 6.526485919952393
Epoch 160, val loss: 1.2789146900177002
Epoch 170, training loss: 1.6917392015457153 = 1.0405510663986206 + 0.1 * 6.511881351470947
Epoch 170, val loss: 1.221476674079895
Epoch 180, training loss: 1.6127688884735107 = 0.9628482460975647 + 0.1 * 6.49920654296875
Epoch 180, val loss: 1.1657625436782837
Epoch 190, training loss: 1.5389788150787354 = 0.8899946808815002 + 0.1 * 6.489841461181641
Epoch 190, val loss: 1.1136218309402466
Epoch 200, training loss: 1.4693281650543213 = 0.8210359215736389 + 0.1 * 6.482922077178955
Epoch 200, val loss: 1.0641742944717407
Epoch 210, training loss: 1.4017188549041748 = 0.7542733550071716 + 0.1 * 6.474454402923584
Epoch 210, val loss: 1.0159978866577148
Epoch 220, training loss: 1.3357350826263428 = 0.6889535188674927 + 0.1 * 6.46781587600708
Epoch 220, val loss: 0.96839439868927
Epoch 230, training loss: 1.2715046405792236 = 0.6253514289855957 + 0.1 * 6.461532115936279
Epoch 230, val loss: 0.9219831228256226
Epoch 240, training loss: 1.2099474668502808 = 0.5644536018371582 + 0.1 * 6.4549384117126465
Epoch 240, val loss: 0.878043532371521
Epoch 250, training loss: 1.152752161026001 = 0.5073297619819641 + 0.1 * 6.4542236328125
Epoch 250, val loss: 0.837841808795929
Epoch 260, training loss: 1.0992748737335205 = 0.4548361599445343 + 0.1 * 6.444387435913086
Epoch 260, val loss: 0.802703320980072
Epoch 270, training loss: 1.0504837036132812 = 0.4068353772163391 + 0.1 * 6.436483383178711
Epoch 270, val loss: 0.7729189395904541
Epoch 280, training loss: 1.0067682266235352 = 0.3635503053665161 + 0.1 * 6.432178497314453
Epoch 280, val loss: 0.7484803199768066
Epoch 290, training loss: 0.9676705598831177 = 0.3248457610607147 + 0.1 * 6.428248405456543
Epoch 290, val loss: 0.7288840413093567
Epoch 300, training loss: 0.9312906265258789 = 0.2897636890411377 + 0.1 * 6.415269374847412
Epoch 300, val loss: 0.7125813961029053
Epoch 310, training loss: 0.9000750780105591 = 0.25794047117233276 + 0.1 * 6.4213457107543945
Epoch 310, val loss: 0.6991181969642639
Epoch 320, training loss: 0.8691602349281311 = 0.22933155298233032 + 0.1 * 6.398286819458008
Epoch 320, val loss: 0.6885401010513306
Epoch 330, training loss: 0.8431528210639954 = 0.20360326766967773 + 0.1 * 6.395495414733887
Epoch 330, val loss: 0.6803947687149048
Epoch 340, training loss: 0.820429801940918 = 0.1806463599205017 + 0.1 * 6.397834300994873
Epoch 340, val loss: 0.6744433641433716
Epoch 350, training loss: 0.7977921366691589 = 0.16032327711582184 + 0.1 * 6.374688625335693
Epoch 350, val loss: 0.6705788373947144
Epoch 360, training loss: 0.7805386781692505 = 0.14234037697315216 + 0.1 * 6.381982803344727
Epoch 360, val loss: 0.6683209538459778
Epoch 370, training loss: 0.763597846031189 = 0.12660761177539825 + 0.1 * 6.36990213394165
Epoch 370, val loss: 0.6677393317222595
Epoch 380, training loss: 0.748014509677887 = 0.11288564652204514 + 0.1 * 6.351288318634033
Epoch 380, val loss: 0.6686553359031677
Epoch 390, training loss: 0.7384754419326782 = 0.10092529654502869 + 0.1 * 6.3755011558532715
Epoch 390, val loss: 0.6707945466041565
Epoch 400, training loss: 0.7252605557441711 = 0.09059322625398636 + 0.1 * 6.346673011779785
Epoch 400, val loss: 0.6740920543670654
Epoch 410, training loss: 0.7148522734642029 = 0.08160950988531113 + 0.1 * 6.332427501678467
Epoch 410, val loss: 0.678228497505188
Epoch 420, training loss: 0.7068989872932434 = 0.07376614958047867 + 0.1 * 6.33132791519165
Epoch 420, val loss: 0.683164656162262
Epoch 430, training loss: 0.6994070410728455 = 0.0669577494263649 + 0.1 * 6.324492454528809
Epoch 430, val loss: 0.6885643601417542
Epoch 440, training loss: 0.6927255392074585 = 0.061039362102746964 + 0.1 * 6.316861152648926
Epoch 440, val loss: 0.6946396827697754
Epoch 450, training loss: 0.6867161989212036 = 0.055833104997873306 + 0.1 * 6.308830738067627
Epoch 450, val loss: 0.7008982300758362
Epoch 460, training loss: 0.6817924976348877 = 0.05123227462172508 + 0.1 * 6.305602073669434
Epoch 460, val loss: 0.7074706554412842
Epoch 470, training loss: 0.6780388951301575 = 0.04716574773192406 + 0.1 * 6.3087310791015625
Epoch 470, val loss: 0.7141640186309814
Epoch 480, training loss: 0.6722899079322815 = 0.043568748980760574 + 0.1 * 6.2872114181518555
Epoch 480, val loss: 0.721045196056366
Epoch 490, training loss: 0.6708627939224243 = 0.04036375880241394 + 0.1 * 6.304990291595459
Epoch 490, val loss: 0.7278896570205688
Epoch 500, training loss: 0.6661545038223267 = 0.03751632571220398 + 0.1 * 6.286381721496582
Epoch 500, val loss: 0.7347970008850098
Epoch 510, training loss: 0.6625056862831116 = 0.03495746850967407 + 0.1 * 6.275482177734375
Epoch 510, val loss: 0.7417113184928894
Epoch 520, training loss: 0.6601409316062927 = 0.032645922154188156 + 0.1 * 6.274949550628662
Epoch 520, val loss: 0.7485875487327576
Epoch 530, training loss: 0.657614529132843 = 0.030560504645109177 + 0.1 * 6.270540237426758
Epoch 530, val loss: 0.7553146481513977
Epoch 540, training loss: 0.6551019549369812 = 0.028676524758338928 + 0.1 * 6.264254093170166
Epoch 540, val loss: 0.7621395587921143
Epoch 550, training loss: 0.6542576551437378 = 0.02696102485060692 + 0.1 * 6.272965908050537
Epoch 550, val loss: 0.7688054442405701
Epoch 560, training loss: 0.6512470245361328 = 0.025401156395673752 + 0.1 * 6.258458614349365
Epoch 560, val loss: 0.7753318548202515
Epoch 570, training loss: 0.6491882801055908 = 0.023977434262633324 + 0.1 * 6.252108097076416
Epoch 570, val loss: 0.7818275690078735
Epoch 580, training loss: 0.6474815011024475 = 0.022674227133393288 + 0.1 * 6.248073101043701
Epoch 580, val loss: 0.7881621718406677
Epoch 590, training loss: 0.6455985307693481 = 0.02148035541176796 + 0.1 * 6.241181373596191
Epoch 590, val loss: 0.79441237449646
Epoch 600, training loss: 0.6452691555023193 = 0.020382249727845192 + 0.1 * 6.2488694190979
Epoch 600, val loss: 0.8005372881889343
Epoch 610, training loss: 0.6428582072257996 = 0.01937052793800831 + 0.1 * 6.2348761558532715
Epoch 610, val loss: 0.8065954446792603
Epoch 620, training loss: 0.6426348686218262 = 0.01843493990600109 + 0.1 * 6.24199914932251
Epoch 620, val loss: 0.812490701675415
Epoch 630, training loss: 0.6405635476112366 = 0.017569927498698235 + 0.1 * 6.229936122894287
Epoch 630, val loss: 0.8183010816574097
Epoch 640, training loss: 0.6400856971740723 = 0.016768312081694603 + 0.1 * 6.233173370361328
Epoch 640, val loss: 0.8240043520927429
Epoch 650, training loss: 0.6375576257705688 = 0.01602683775126934 + 0.1 * 6.215307712554932
Epoch 650, val loss: 0.8296000361442566
Epoch 660, training loss: 0.6366218328475952 = 0.015335961245000362 + 0.1 * 6.212858200073242
Epoch 660, val loss: 0.835157573223114
Epoch 670, training loss: 0.6367903351783752 = 0.014688795432448387 + 0.1 * 6.221015453338623
Epoch 670, val loss: 0.8405707478523254
Epoch 680, training loss: 0.6344456672668457 = 0.014085781760513783 + 0.1 * 6.203598976135254
Epoch 680, val loss: 0.8458858132362366
Epoch 690, training loss: 0.6352499127388 = 0.013521839864552021 + 0.1 * 6.217280387878418
Epoch 690, val loss: 0.8511707782745361
Epoch 700, training loss: 0.6336835622787476 = 0.012992984615266323 + 0.1 * 6.206905841827393
Epoch 700, val loss: 0.8562756180763245
Epoch 710, training loss: 0.6339548230171204 = 0.012498744763433933 + 0.1 * 6.214560508728027
Epoch 710, val loss: 0.8613401055335999
Epoch 720, training loss: 0.6316930651664734 = 0.012034747749567032 + 0.1 * 6.196582794189453
Epoch 720, val loss: 0.866283655166626
Epoch 730, training loss: 0.6327095031738281 = 0.0115970429033041 + 0.1 * 6.211124420166016
Epoch 730, val loss: 0.8711633086204529
Epoch 740, training loss: 0.6304225921630859 = 0.011185362935066223 + 0.1 * 6.1923723220825195
Epoch 740, val loss: 0.8759602308273315
Epoch 750, training loss: 0.6303897500038147 = 0.010797069407999516 + 0.1 * 6.195926666259766
Epoch 750, val loss: 0.8807001113891602
Epoch 760, training loss: 0.6291429400444031 = 0.010429788380861282 + 0.1 * 6.187131404876709
Epoch 760, val loss: 0.8853100538253784
Epoch 770, training loss: 0.6294982433319092 = 0.010082054883241653 + 0.1 * 6.194161415100098
Epoch 770, val loss: 0.8898308873176575
Epoch 780, training loss: 0.6278280019760132 = 0.009755085222423077 + 0.1 * 6.180729389190674
Epoch 780, val loss: 0.8942915201187134
Epoch 790, training loss: 0.6271460056304932 = 0.009445736184716225 + 0.1 * 6.177002429962158
Epoch 790, val loss: 0.8987440466880798
Epoch 800, training loss: 0.6283703446388245 = 0.00915040448307991 + 0.1 * 6.192199230194092
Epoch 800, val loss: 0.9030629396438599
Epoch 810, training loss: 0.6257795095443726 = 0.00887118000537157 + 0.1 * 6.169083118438721
Epoch 810, val loss: 0.9072564244270325
Epoch 820, training loss: 0.625632107257843 = 0.008606805466115475 + 0.1 * 6.170253276824951
Epoch 820, val loss: 0.9114530086517334
Epoch 830, training loss: 0.6260242462158203 = 0.0083537632599473 + 0.1 * 6.176704406738281
Epoch 830, val loss: 0.9155965447425842
Epoch 840, training loss: 0.6255614757537842 = 0.00811215303838253 + 0.1 * 6.174493312835693
Epoch 840, val loss: 0.9195965528488159
Epoch 850, training loss: 0.625254213809967 = 0.007882748730480671 + 0.1 * 6.1737141609191895
Epoch 850, val loss: 0.923518717288971
Epoch 860, training loss: 0.6254533529281616 = 0.007664158008992672 + 0.1 * 6.177891731262207
Epoch 860, val loss: 0.927428662776947
Epoch 870, training loss: 0.6240100264549255 = 0.007455854676663876 + 0.1 * 6.165542125701904
Epoch 870, val loss: 0.931246280670166
Epoch 880, training loss: 0.6237421631813049 = 0.007256775628775358 + 0.1 * 6.164854049682617
Epoch 880, val loss: 0.9350104331970215
Epoch 890, training loss: 0.6240025162696838 = 0.007065907586365938 + 0.1 * 6.169366359710693
Epoch 890, val loss: 0.9387251734733582
Epoch 900, training loss: 0.6231850385665894 = 0.006884029600769281 + 0.1 * 6.1630096435546875
Epoch 900, val loss: 0.9424381852149963
Epoch 910, training loss: 0.6223599314689636 = 0.006709800101816654 + 0.1 * 6.156501293182373
Epoch 910, val loss: 0.9460678696632385
Epoch 920, training loss: 0.622164249420166 = 0.006542529910802841 + 0.1 * 6.156217575073242
Epoch 920, val loss: 0.9496578574180603
Epoch 930, training loss: 0.6227860450744629 = 0.006382687482982874 + 0.1 * 6.16403341293335
Epoch 930, val loss: 0.9532172083854675
Epoch 940, training loss: 0.6226715445518494 = 0.0062283542938530445 + 0.1 * 6.164432048797607
Epoch 940, val loss: 0.9566826820373535
Epoch 950, training loss: 0.6212169528007507 = 0.006081858184188604 + 0.1 * 6.151350498199463
Epoch 950, val loss: 0.9600694179534912
Epoch 960, training loss: 0.621659517288208 = 0.005941151175647974 + 0.1 * 6.157183647155762
Epoch 960, val loss: 0.9634740948677063
Epoch 970, training loss: 0.620716392993927 = 0.005805648397654295 + 0.1 * 6.149106979370117
Epoch 970, val loss: 0.9668739438056946
Epoch 980, training loss: 0.6200901865959167 = 0.005675169639289379 + 0.1 * 6.1441497802734375
Epoch 980, val loss: 0.9701674580574036
Epoch 990, training loss: 0.6225156784057617 = 0.005549713037908077 + 0.1 * 6.169659614562988
Epoch 990, val loss: 0.9734127521514893
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8708
Flip ASR: 0.8444/225 nodes
The final ASR:0.76015, 0.11731, Accuracy:0.81481, 0.01318
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10520])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.801377773284912 = 1.9639892578125 + 0.1 * 8.373884201049805
Epoch 0, val loss: 1.9563066959381104
Epoch 10, training loss: 2.7899091243743896 = 1.9525318145751953 + 0.1 * 8.373773574829102
Epoch 10, val loss: 1.945733904838562
Epoch 20, training loss: 2.7758395671844482 = 1.9385204315185547 + 0.1 * 8.373191833496094
Epoch 20, val loss: 1.9323545694351196
Epoch 30, training loss: 2.7559356689453125 = 1.9190722703933716 + 0.1 * 8.368635177612305
Epoch 30, val loss: 1.913488745689392
Epoch 40, training loss: 2.7240138053894043 = 1.8907698392868042 + 0.1 * 8.332438468933105
Epoch 40, val loss: 1.8862329721450806
Epoch 50, training loss: 2.655221700668335 = 1.853233814239502 + 0.1 * 8.019878387451172
Epoch 50, val loss: 1.851934552192688
Epoch 60, training loss: 2.575000286102295 = 1.8171594142913818 + 0.1 * 7.578409671783447
Epoch 60, val loss: 1.8214792013168335
Epoch 70, training loss: 2.4956424236297607 = 1.7848892211914062 + 0.1 * 7.107532501220703
Epoch 70, val loss: 1.7961448431015015
Epoch 80, training loss: 2.440622329711914 = 1.7551904916763306 + 0.1 * 6.854318141937256
Epoch 80, val loss: 1.7721620798110962
Epoch 90, training loss: 2.3924360275268555 = 1.718136191368103 + 0.1 * 6.742997169494629
Epoch 90, val loss: 1.740342140197754
Epoch 100, training loss: 2.3351480960845947 = 1.6686358451843262 + 0.1 * 6.665122032165527
Epoch 100, val loss: 1.6981284618377686
Epoch 110, training loss: 2.2640254497528076 = 1.6034674644470215 + 0.1 * 6.605579376220703
Epoch 110, val loss: 1.643273949623108
Epoch 120, training loss: 2.18129825592041 = 1.5245848894119263 + 0.1 * 6.56713342666626
Epoch 120, val loss: 1.5776704549789429
Epoch 130, training loss: 2.0963854789733887 = 1.4418621063232422 + 0.1 * 6.545234680175781
Epoch 130, val loss: 1.5105600357055664
Epoch 140, training loss: 2.015728235244751 = 1.3626806735992432 + 0.1 * 6.530475616455078
Epoch 140, val loss: 1.4498305320739746
Epoch 150, training loss: 1.9385592937469482 = 1.2868258953094482 + 0.1 * 6.517333030700684
Epoch 150, val loss: 1.3952347040176392
Epoch 160, training loss: 1.8632638454437256 = 1.2131633758544922 + 0.1 * 6.501003742218018
Epoch 160, val loss: 1.344252347946167
Epoch 170, training loss: 1.7893624305725098 = 1.1391874551773071 + 0.1 * 6.501750469207764
Epoch 170, val loss: 1.2934937477111816
Epoch 180, training loss: 1.7134490013122559 = 1.0659394264221191 + 0.1 * 6.475095748901367
Epoch 180, val loss: 1.2438143491744995
Epoch 190, training loss: 1.6377618312835693 = 0.991981029510498 + 0.1 * 6.4578070640563965
Epoch 190, val loss: 1.1943353414535522
Epoch 200, training loss: 1.5674912929534912 = 0.9185354709625244 + 0.1 * 6.489558696746826
Epoch 200, val loss: 1.1460033655166626
Epoch 210, training loss: 1.4951286315917969 = 0.8506576418876648 + 0.1 * 6.4447102546691895
Epoch 210, val loss: 1.1030821800231934
Epoch 220, training loss: 1.4318313598632812 = 0.7889126539230347 + 0.1 * 6.429187774658203
Epoch 220, val loss: 1.0660215616226196
Epoch 230, training loss: 1.3747786283493042 = 0.7331967949867249 + 0.1 * 6.415818214416504
Epoch 230, val loss: 1.0355069637298584
Epoch 240, training loss: 1.3233846426010132 = 0.6826837062835693 + 0.1 * 6.407009124755859
Epoch 240, val loss: 1.0107359886169434
Epoch 250, training loss: 1.2764323949813843 = 0.6363356709480286 + 0.1 * 6.400967121124268
Epoch 250, val loss: 0.9905567169189453
Epoch 260, training loss: 1.2312092781066895 = 0.5920842885971069 + 0.1 * 6.391249656677246
Epoch 260, val loss: 0.9728224277496338
Epoch 270, training loss: 1.1882445812225342 = 0.5491176247596741 + 0.1 * 6.391269207000732
Epoch 270, val loss: 0.9561795592308044
Epoch 280, training loss: 1.144730806350708 = 0.5069568753242493 + 0.1 * 6.377739429473877
Epoch 280, val loss: 0.9398704171180725
Epoch 290, training loss: 1.1029047966003418 = 0.46570393443107605 + 0.1 * 6.372008323669434
Epoch 290, val loss: 0.9235537648200989
Epoch 300, training loss: 1.0629613399505615 = 0.42608052492141724 + 0.1 * 6.368807792663574
Epoch 300, val loss: 0.9080576300621033
Epoch 310, training loss: 1.0274536609649658 = 0.38849371671676636 + 0.1 * 6.389599800109863
Epoch 310, val loss: 0.8938964605331421
Epoch 320, training loss: 0.9897176623344421 = 0.35374879837036133 + 0.1 * 6.3596882820129395
Epoch 320, val loss: 0.8822948336601257
Epoch 330, training loss: 0.9566144943237305 = 0.3216303288936615 + 0.1 * 6.349841594696045
Epoch 330, val loss: 0.8735978007316589
Epoch 340, training loss: 0.9268206357955933 = 0.2920907437801361 + 0.1 * 6.347299098968506
Epoch 340, val loss: 0.8679769039154053
Epoch 350, training loss: 0.9001891613006592 = 0.2649725675582886 + 0.1 * 6.352165699005127
Epoch 350, val loss: 0.8652469515800476
Epoch 360, training loss: 0.8732693195343018 = 0.24001184105873108 + 0.1 * 6.332574367523193
Epoch 360, val loss: 0.8650117516517639
Epoch 370, training loss: 0.851549506187439 = 0.2169465720653534 + 0.1 * 6.346028804779053
Epoch 370, val loss: 0.8668220043182373
Epoch 380, training loss: 0.8282302618026733 = 0.19578976929187775 + 0.1 * 6.324405193328857
Epoch 380, val loss: 0.870255172252655
Epoch 390, training loss: 0.8077713847160339 = 0.17638081312179565 + 0.1 * 6.313905715942383
Epoch 390, val loss: 0.8749447464942932
Epoch 400, training loss: 0.7902078628540039 = 0.15867258608341217 + 0.1 * 6.315352916717529
Epoch 400, val loss: 0.8806155920028687
Epoch 410, training loss: 0.7736617922782898 = 0.1426471471786499 + 0.1 * 6.310146331787109
Epoch 410, val loss: 0.8873074054718018
Epoch 420, training loss: 0.758408784866333 = 0.1280316412448883 + 0.1 * 6.303771495819092
Epoch 420, val loss: 0.8945840001106262
Epoch 430, training loss: 0.7459734678268433 = 0.11477833241224289 + 0.1 * 6.311951160430908
Epoch 430, val loss: 0.9024828672409058
Epoch 440, training loss: 0.7320302128791809 = 0.10303889960050583 + 0.1 * 6.289913177490234
Epoch 440, val loss: 0.9109243750572205
Epoch 450, training loss: 0.7214174866676331 = 0.09283311665058136 + 0.1 * 6.285843849182129
Epoch 450, val loss: 0.9201001524925232
Epoch 460, training loss: 0.7118732333183289 = 0.08396123349666595 + 0.1 * 6.279119491577148
Epoch 460, val loss: 0.9300758838653564
Epoch 470, training loss: 0.7043442130088806 = 0.07620032131671906 + 0.1 * 6.281438827514648
Epoch 470, val loss: 0.940477728843689
Epoch 480, training loss: 0.6964740753173828 = 0.06938949972391129 + 0.1 * 6.270845413208008
Epoch 480, val loss: 0.9513513445854187
Epoch 490, training loss: 0.6905854940414429 = 0.06337828189134598 + 0.1 * 6.272071838378906
Epoch 490, val loss: 0.9623435735702515
Epoch 500, training loss: 0.6850826740264893 = 0.05806230008602142 + 0.1 * 6.270203590393066
Epoch 500, val loss: 0.9736576676368713
Epoch 510, training loss: 0.6787899732589722 = 0.053344860672950745 + 0.1 * 6.254451274871826
Epoch 510, val loss: 0.9850482940673828
Epoch 520, training loss: 0.6759693622589111 = 0.049144744873046875 + 0.1 * 6.268246173858643
Epoch 520, val loss: 0.9963780045509338
Epoch 530, training loss: 0.6710094809532166 = 0.045415230095386505 + 0.1 * 6.255942344665527
Epoch 530, val loss: 1.0078741312026978
Epoch 540, training loss: 0.6665064096450806 = 0.04208242893218994 + 0.1 * 6.244239807128906
Epoch 540, val loss: 1.019205093383789
Epoch 550, training loss: 0.6653873920440674 = 0.039089493453502655 + 0.1 * 6.262978553771973
Epoch 550, val loss: 1.0304791927337646
Epoch 560, training loss: 0.6605029702186584 = 0.036405086517333984 + 0.1 * 6.240978717803955
Epoch 560, val loss: 1.0416591167449951
Epoch 570, training loss: 0.657002329826355 = 0.03398892283439636 + 0.1 * 6.2301344871521
Epoch 570, val loss: 1.0527549982070923
Epoch 580, training loss: 0.6559181809425354 = 0.03180108219385147 + 0.1 * 6.241170883178711
Epoch 580, val loss: 1.0636221170425415
Epoch 590, training loss: 0.6532413363456726 = 0.02982090599834919 + 0.1 * 6.234204292297363
Epoch 590, val loss: 1.074143648147583
Epoch 600, training loss: 0.6507927179336548 = 0.0280255526304245 + 0.1 * 6.2276716232299805
Epoch 600, val loss: 1.0846824645996094
Epoch 610, training loss: 0.6482065916061401 = 0.026388460770249367 + 0.1 * 6.218181133270264
Epoch 610, val loss: 1.0947808027267456
Epoch 620, training loss: 0.6466922760009766 = 0.024895837530493736 + 0.1 * 6.217964172363281
Epoch 620, val loss: 1.1049230098724365
Epoch 630, training loss: 0.6455116868019104 = 0.023526282981038094 + 0.1 * 6.21985387802124
Epoch 630, val loss: 1.1147031784057617
Epoch 640, training loss: 0.6435017585754395 = 0.022270694375038147 + 0.1 * 6.212310791015625
Epoch 640, val loss: 1.1244146823883057
Epoch 650, training loss: 0.6426914930343628 = 0.021118367090821266 + 0.1 * 6.215731143951416
Epoch 650, val loss: 1.1337116956710815
Epoch 660, training loss: 0.6404565572738647 = 0.020060038194060326 + 0.1 * 6.203964710235596
Epoch 660, val loss: 1.1431020498275757
Epoch 670, training loss: 0.639108419418335 = 0.01908213645219803 + 0.1 * 6.200263023376465
Epoch 670, val loss: 1.1521624326705933
Epoch 680, training loss: 0.6387943029403687 = 0.018176021054387093 + 0.1 * 6.206182956695557
Epoch 680, val loss: 1.1609926223754883
Epoch 690, training loss: 0.6382055282592773 = 0.017337879166007042 + 0.1 * 6.208676815032959
Epoch 690, val loss: 1.169715404510498
Epoch 700, training loss: 0.6365844011306763 = 0.016560647636651993 + 0.1 * 6.200237274169922
Epoch 700, val loss: 1.1780749559402466
Epoch 710, training loss: 0.6348569989204407 = 0.01583980955183506 + 0.1 * 6.190171718597412
Epoch 710, val loss: 1.1865179538726807
Epoch 720, training loss: 0.6347870826721191 = 0.01516651175916195 + 0.1 * 6.1962056159973145
Epoch 720, val loss: 1.194614052772522
Epoch 730, training loss: 0.6334251761436462 = 0.014538293704390526 + 0.1 * 6.188868999481201
Epoch 730, val loss: 1.2025572061538696
Epoch 740, training loss: 0.6334362626075745 = 0.013951667584478855 + 0.1 * 6.194845676422119
Epoch 740, val loss: 1.2103381156921387
Epoch 750, training loss: 0.6320284605026245 = 0.013403119519352913 + 0.1 * 6.186253547668457
Epoch 750, val loss: 1.2180745601654053
Epoch 760, training loss: 0.6320263147354126 = 0.012888472527265549 + 0.1 * 6.191378116607666
Epoch 760, val loss: 1.2256643772125244
Epoch 770, training loss: 0.6304420232772827 = 0.012404642067849636 + 0.1 * 6.1803741455078125
Epoch 770, val loss: 1.232768177986145
Epoch 780, training loss: 0.6308004260063171 = 0.011951965279877186 + 0.1 * 6.188484191894531
Epoch 780, val loss: 1.240174412727356
Epoch 790, training loss: 0.6291032433509827 = 0.011524367146193981 + 0.1 * 6.175788402557373
Epoch 790, val loss: 1.2470299005508423
Epoch 800, training loss: 0.6314997673034668 = 0.011122710071504116 + 0.1 * 6.203770637512207
Epoch 800, val loss: 1.2540807723999023
Epoch 810, training loss: 0.6283600926399231 = 0.010743227787315845 + 0.1 * 6.176168918609619
Epoch 810, val loss: 1.2605760097503662
Epoch 820, training loss: 0.6269095540046692 = 0.010386541485786438 + 0.1 * 6.1652302742004395
Epoch 820, val loss: 1.2674469947814941
Epoch 830, training loss: 0.62669438123703 = 0.01004734355956316 + 0.1 * 6.166470527648926
Epoch 830, val loss: 1.2739965915679932
Epoch 840, training loss: 0.6269422769546509 = 0.009724676609039307 + 0.1 * 6.172175884246826
Epoch 840, val loss: 1.2803131341934204
Epoch 850, training loss: 0.6258708834648132 = 0.00941986683756113 + 0.1 * 6.164510250091553
Epoch 850, val loss: 1.286724328994751
Epoch 860, training loss: 0.6256824135780334 = 0.009129229933023453 + 0.1 * 6.165531635284424
Epoch 860, val loss: 1.292720913887024
Epoch 870, training loss: 0.6257359385490417 = 0.008854877203702927 + 0.1 * 6.1688103675842285
Epoch 870, val loss: 1.2988864183425903
Epoch 880, training loss: 0.6241527199745178 = 0.008594337850809097 + 0.1 * 6.15558385848999
Epoch 880, val loss: 1.3049027919769287
Epoch 890, training loss: 0.6239684820175171 = 0.00834628939628601 + 0.1 * 6.156222343444824
Epoch 890, val loss: 1.3108339309692383
Epoch 900, training loss: 0.6243589520454407 = 0.008108818903565407 + 0.1 * 6.162501335144043
Epoch 900, val loss: 1.31643807888031
Epoch 910, training loss: 0.6235171556472778 = 0.007882999256253242 + 0.1 * 6.156341552734375
Epoch 910, val loss: 1.3220083713531494
Epoch 920, training loss: 0.6242702007293701 = 0.007668821141123772 + 0.1 * 6.166014194488525
Epoch 920, val loss: 1.3276838064193726
Epoch 930, training loss: 0.6229914426803589 = 0.007463546935468912 + 0.1 * 6.15527868270874
Epoch 930, val loss: 1.3330528736114502
Epoch 940, training loss: 0.6220186352729797 = 0.0072683109901845455 + 0.1 * 6.147502899169922
Epoch 940, val loss: 1.3385400772094727
Epoch 950, training loss: 0.6228913068771362 = 0.007080561015754938 + 0.1 * 6.158107280731201
Epoch 950, val loss: 1.3437808752059937
Epoch 960, training loss: 0.6225706338882446 = 0.006900342646986246 + 0.1 * 6.156702518463135
Epoch 960, val loss: 1.3486727476119995
Epoch 970, training loss: 0.6223644018173218 = 0.006728976033627987 + 0.1 * 6.1563544273376465
Epoch 970, val loss: 1.3537906408309937
Epoch 980, training loss: 0.6211628913879395 = 0.006565032992511988 + 0.1 * 6.1459784507751465
Epoch 980, val loss: 1.358925461769104
Epoch 990, training loss: 0.621984601020813 = 0.006406836677342653 + 0.1 * 6.155777454376221
Epoch 990, val loss: 1.3636295795440674
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.796858787536621 = 1.959472417831421 + 0.1 * 8.373863220214844
Epoch 0, val loss: 1.9679954051971436
Epoch 10, training loss: 2.7865452766418457 = 1.949171543121338 + 0.1 * 8.373737335205078
Epoch 10, val loss: 1.958038568496704
Epoch 20, training loss: 2.7735605239868164 = 1.9362670183181763 + 0.1 * 8.372934341430664
Epoch 20, val loss: 1.9450138807296753
Epoch 30, training loss: 2.7543792724609375 = 1.917710781097412 + 0.1 * 8.366683959960938
Epoch 30, val loss: 1.9257482290267944
Epoch 40, training loss: 2.7226483821868896 = 1.8895093202590942 + 0.1 * 8.331390380859375
Epoch 40, val loss: 1.896589994430542
Epoch 50, training loss: 2.6649889945983887 = 1.8504525423049927 + 0.1 * 8.145363807678223
Epoch 50, val loss: 1.85825514793396
Epoch 60, training loss: 2.5970044136047363 = 1.8067941665649414 + 0.1 * 7.902102947235107
Epoch 60, val loss: 1.818359136581421
Epoch 70, training loss: 2.5386109352111816 = 1.7670825719833374 + 0.1 * 7.715283393859863
Epoch 70, val loss: 1.784214735031128
Epoch 80, training loss: 2.466148853302002 = 1.7267003059387207 + 0.1 * 7.394485950469971
Epoch 80, val loss: 1.7489858865737915
Epoch 90, training loss: 2.3825764656066895 = 1.6782780885696411 + 0.1 * 7.042983055114746
Epoch 90, val loss: 1.708385944366455
Epoch 100, training loss: 2.2967844009399414 = 1.6164195537567139 + 0.1 * 6.803648471832275
Epoch 100, val loss: 1.6589434146881104
Epoch 110, training loss: 2.210517406463623 = 1.5386961698532104 + 0.1 * 6.7182135581970215
Epoch 110, val loss: 1.5958184003829956
Epoch 120, training loss: 2.121307611465454 = 1.454888939857483 + 0.1 * 6.664187431335449
Epoch 120, val loss: 1.5306226015090942
Epoch 130, training loss: 2.0359108448028564 = 1.3730381727218628 + 0.1 * 6.628726005554199
Epoch 130, val loss: 1.4688360691070557
Epoch 140, training loss: 1.953507423400879 = 1.2930915355682373 + 0.1 * 6.604159355163574
Epoch 140, val loss: 1.4101382493972778
Epoch 150, training loss: 1.87173593044281 = 1.2133389711380005 + 0.1 * 6.583969593048096
Epoch 150, val loss: 1.3527331352233887
Epoch 160, training loss: 1.7897632122039795 = 1.1328606605529785 + 0.1 * 6.569025039672852
Epoch 160, val loss: 1.2938181161880493
Epoch 170, training loss: 1.710137963294983 = 1.054054617881775 + 0.1 * 6.56083345413208
Epoch 170, val loss: 1.237163782119751
Epoch 180, training loss: 1.634420394897461 = 0.9797295331954956 + 0.1 * 6.546908855438232
Epoch 180, val loss: 1.1841912269592285
Epoch 190, training loss: 1.5626909732818604 = 0.9091047644615173 + 0.1 * 6.535861492156982
Epoch 190, val loss: 1.1334705352783203
Epoch 200, training loss: 1.4943252801895142 = 0.8417862057685852 + 0.1 * 6.525390625
Epoch 200, val loss: 1.0847445726394653
Epoch 210, training loss: 1.4282357692718506 = 0.7767375707626343 + 0.1 * 6.514981269836426
Epoch 210, val loss: 1.037779688835144
Epoch 220, training loss: 1.3647384643554688 = 0.7138454914093018 + 0.1 * 6.50892972946167
Epoch 220, val loss: 0.9931154847145081
Epoch 230, training loss: 1.3041157722473145 = 0.6541815996170044 + 0.1 * 6.499342441558838
Epoch 230, val loss: 0.9522125124931335
Epoch 240, training loss: 1.2463984489440918 = 0.5973645448684692 + 0.1 * 6.4903388023376465
Epoch 240, val loss: 0.9154255390167236
Epoch 250, training loss: 1.1911590099334717 = 0.5430409908294678 + 0.1 * 6.481179714202881
Epoch 250, val loss: 0.8824094533920288
Epoch 260, training loss: 1.1395533084869385 = 0.49164342880249023 + 0.1 * 6.479099273681641
Epoch 260, val loss: 0.8536915183067322
Epoch 270, training loss: 1.0899245738983154 = 0.44340959191322327 + 0.1 * 6.465149402618408
Epoch 270, val loss: 0.8291054368019104
Epoch 280, training loss: 1.0436761379241943 = 0.39793112874031067 + 0.1 * 6.4574503898620605
Epoch 280, val loss: 0.8081569671630859
Epoch 290, training loss: 1.0012155771255493 = 0.3555181920528412 + 0.1 * 6.456973552703857
Epoch 290, val loss: 0.7913001775741577
Epoch 300, training loss: 0.9623546600341797 = 0.3164225220680237 + 0.1 * 6.45932149887085
Epoch 300, val loss: 0.7783923745155334
Epoch 310, training loss: 0.9246616363525391 = 0.280988484621048 + 0.1 * 6.436731338500977
Epoch 310, val loss: 0.7691267728805542
Epoch 320, training loss: 0.8916344046592712 = 0.24900390207767487 + 0.1 * 6.426305294036865
Epoch 320, val loss: 0.7628196477890015
Epoch 330, training loss: 0.8650166392326355 = 0.2202380746603012 + 0.1 * 6.447785377502441
Epoch 330, val loss: 0.7589389085769653
Epoch 340, training loss: 0.8361251354217529 = 0.19489581882953644 + 0.1 * 6.412292957305908
Epoch 340, val loss: 0.7576587200164795
Epoch 350, training loss: 0.8127085566520691 = 0.17249085009098053 + 0.1 * 6.402177333831787
Epoch 350, val loss: 0.7584376335144043
Epoch 360, training loss: 0.7926101088523865 = 0.15285290777683258 + 0.1 * 6.397572040557861
Epoch 360, val loss: 0.7608814835548401
Epoch 370, training loss: 0.7750686407089233 = 0.1358422189950943 + 0.1 * 6.392264366149902
Epoch 370, val loss: 0.7651520371437073
Epoch 380, training loss: 0.7589016556739807 = 0.12102969735860825 + 0.1 * 6.378719806671143
Epoch 380, val loss: 0.7707972526550293
Epoch 390, training loss: 0.7454483509063721 = 0.10814463347196579 + 0.1 * 6.373036861419678
Epoch 390, val loss: 0.7774519920349121
Epoch 400, training loss: 0.735141396522522 = 0.09691544622182846 + 0.1 * 6.382259845733643
Epoch 400, val loss: 0.7849499583244324
Epoch 410, training loss: 0.7234091758728027 = 0.08709356933832169 + 0.1 * 6.363155841827393
Epoch 410, val loss: 0.7926945090293884
Epoch 420, training loss: 0.7133499979972839 = 0.07842447608709335 + 0.1 * 6.349255084991455
Epoch 420, val loss: 0.8007537722587585
Epoch 430, training loss: 0.7077742218971252 = 0.0707787349820137 + 0.1 * 6.369955062866211
Epoch 430, val loss: 0.80924391746521
Epoch 440, training loss: 0.6980457305908203 = 0.06410249322652817 + 0.1 * 6.339432716369629
Epoch 440, val loss: 0.8179425597190857
Epoch 450, training loss: 0.6919161677360535 = 0.05827195942401886 + 0.1 * 6.336441993713379
Epoch 450, val loss: 0.8270296454429626
Epoch 460, training loss: 0.6874771118164062 = 0.053166743367910385 + 0.1 * 6.343103408813477
Epoch 460, val loss: 0.8362839818000793
Epoch 470, training loss: 0.6808739900588989 = 0.048709359019994736 + 0.1 * 6.321646213531494
Epoch 470, val loss: 0.8455820679664612
Epoch 480, training loss: 0.678565263748169 = 0.044794417917728424 + 0.1 * 6.337707996368408
Epoch 480, val loss: 0.8547767996788025
Epoch 490, training loss: 0.6725896596908569 = 0.041348401457071304 + 0.1 * 6.312412261962891
Epoch 490, val loss: 0.8640632629394531
Epoch 500, training loss: 0.6686940789222717 = 0.03828277438879013 + 0.1 * 6.304113388061523
Epoch 500, val loss: 0.873133659362793
Epoch 510, training loss: 0.6692808866500854 = 0.03553740680217743 + 0.1 * 6.337434768676758
Epoch 510, val loss: 0.8821240663528442
Epoch 520, training loss: 0.6630439162254333 = 0.03308736905455589 + 0.1 * 6.299565315246582
Epoch 520, val loss: 0.8908787369728088
Epoch 530, training loss: 0.6596353054046631 = 0.030884819105267525 + 0.1 * 6.28750467300415
Epoch 530, val loss: 0.8996861577033997
Epoch 540, training loss: 0.6576870083808899 = 0.028889870271086693 + 0.1 * 6.287971019744873
Epoch 540, val loss: 0.9082656502723694
Epoch 550, training loss: 0.6576831340789795 = 0.027080882340669632 + 0.1 * 6.3060221672058105
Epoch 550, val loss: 0.9166622757911682
Epoch 560, training loss: 0.6535487771034241 = 0.025448184460401535 + 0.1 * 6.281005382537842
Epoch 560, val loss: 0.924984872341156
Epoch 570, training loss: 0.6506678462028503 = 0.023961130529642105 + 0.1 * 6.267066955566406
Epoch 570, val loss: 0.9332061409950256
Epoch 580, training loss: 0.6509627103805542 = 0.022597944363951683 + 0.1 * 6.283647537231445
Epoch 580, val loss: 0.9411542415618896
Epoch 590, training loss: 0.648604691028595 = 0.02135024406015873 + 0.1 * 6.272543907165527
Epoch 590, val loss: 0.9489967823028564
Epoch 600, training loss: 0.6473448872566223 = 0.02020430751144886 + 0.1 * 6.2714056968688965
Epoch 600, val loss: 0.9567675590515137
Epoch 610, training loss: 0.6449674367904663 = 0.019151596352458 + 0.1 * 6.2581586837768555
Epoch 610, val loss: 0.9643456339836121
Epoch 620, training loss: 0.6444061994552612 = 0.018180545419454575 + 0.1 * 6.262256145477295
Epoch 620, val loss: 0.9718296527862549
Epoch 630, training loss: 0.6422073245048523 = 0.01728418655693531 + 0.1 * 6.249230861663818
Epoch 630, val loss: 0.9791786670684814
Epoch 640, training loss: 0.6426376104354858 = 0.016454370692372322 + 0.1 * 6.261832237243652
Epoch 640, val loss: 0.9863924384117126
Epoch 650, training loss: 0.6407368183135986 = 0.015686096623539925 + 0.1 * 6.25050687789917
Epoch 650, val loss: 0.993423342704773
Epoch 660, training loss: 0.6399231553077698 = 0.014973069541156292 + 0.1 * 6.249500751495361
Epoch 660, val loss: 1.0005148649215698
Epoch 670, training loss: 0.6389133930206299 = 0.014308447949588299 + 0.1 * 6.246049404144287
Epoch 670, val loss: 1.007152795791626
Epoch 680, training loss: 0.6369272470474243 = 0.013691025786101818 + 0.1 * 6.232362270355225
Epoch 680, val loss: 1.0139135122299194
Epoch 690, training loss: 0.6371045708656311 = 0.013114111497998238 + 0.1 * 6.239904403686523
Epoch 690, val loss: 1.0206044912338257
Epoch 700, training loss: 0.6359512209892273 = 0.012574481777846813 + 0.1 * 6.233767032623291
Epoch 700, val loss: 1.0268923044204712
Epoch 710, training loss: 0.63486248254776 = 0.012070494703948498 + 0.1 * 6.227920055389404
Epoch 710, val loss: 1.0333595275878906
Epoch 720, training loss: 0.6342998743057251 = 0.011597308330237865 + 0.1 * 6.227025508880615
Epoch 720, val loss: 1.0395278930664062
Epoch 730, training loss: 0.6331605315208435 = 0.011153674684464931 + 0.1 * 6.220067977905273
Epoch 730, val loss: 1.0457217693328857
Epoch 740, training loss: 0.6362431049346924 = 0.010736816562712193 + 0.1 * 6.255062580108643
Epoch 740, val loss: 1.0516430139541626
Epoch 750, training loss: 0.6314796805381775 = 0.01034538447856903 + 0.1 * 6.211342811584473
Epoch 750, val loss: 1.057540774345398
Epoch 760, training loss: 0.6316904425621033 = 0.009976987726986408 + 0.1 * 6.217134475708008
Epoch 760, val loss: 1.0635722875595093
Epoch 770, training loss: 0.6303632259368896 = 0.009628190658986568 + 0.1 * 6.207350254058838
Epoch 770, val loss: 1.0690585374832153
Epoch 780, training loss: 0.6300230026245117 = 0.009299579076468945 + 0.1 * 6.207233905792236
Epoch 780, val loss: 1.0748229026794434
Epoch 790, training loss: 0.6302070617675781 = 0.0089883366599679 + 0.1 * 6.21218729019165
Epoch 790, val loss: 1.0801818370819092
Epoch 800, training loss: 0.6296730637550354 = 0.008695016615092754 + 0.1 * 6.209780693054199
Epoch 800, val loss: 1.085604190826416
Epoch 810, training loss: 0.6285014748573303 = 0.008417013101279736 + 0.1 * 6.200844764709473
Epoch 810, val loss: 1.0911564826965332
Epoch 820, training loss: 0.6293126344680786 = 0.008152157068252563 + 0.1 * 6.211605072021484
Epoch 820, val loss: 1.0963120460510254
Epoch 830, training loss: 0.6274868249893188 = 0.007901504635810852 + 0.1 * 6.195853233337402
Epoch 830, val loss: 1.101314902305603
Epoch 840, training loss: 0.62770676612854 = 0.007663562428206205 + 0.1 * 6.200431823730469
Epoch 840, val loss: 1.1065387725830078
Epoch 850, training loss: 0.6274781227111816 = 0.007436628453433514 + 0.1 * 6.200414657592773
Epoch 850, val loss: 1.1114370822906494
Epoch 860, training loss: 0.6261885762214661 = 0.007220841478556395 + 0.1 * 6.1896772384643555
Epoch 860, val loss: 1.116305947303772
Epoch 870, training loss: 0.625863254070282 = 0.00701594864949584 + 0.1 * 6.188473224639893
Epoch 870, val loss: 1.1213027238845825
Epoch 880, training loss: 0.6260513067245483 = 0.006819650065153837 + 0.1 * 6.192316055297852
Epoch 880, val loss: 1.1259208917617798
Epoch 890, training loss: 0.6260124444961548 = 0.006632741075009108 + 0.1 * 6.193796634674072
Epoch 890, val loss: 1.1306180953979492
Epoch 900, training loss: 0.6249305605888367 = 0.006454537622630596 + 0.1 * 6.184760093688965
Epoch 900, val loss: 1.135279893875122
Epoch 910, training loss: 0.6248477697372437 = 0.006283945869654417 + 0.1 * 6.185638427734375
Epoch 910, val loss: 1.1398980617523193
Epoch 920, training loss: 0.6247966885566711 = 0.006120618898421526 + 0.1 * 6.186760425567627
Epoch 920, val loss: 1.1443381309509277
Epoch 930, training loss: 0.6237644553184509 = 0.005964323878288269 + 0.1 * 6.1780009269714355
Epoch 930, val loss: 1.1486140489578247
Epoch 940, training loss: 0.6240209341049194 = 0.005815462209284306 + 0.1 * 6.18205451965332
Epoch 940, val loss: 1.153127670288086
Epoch 950, training loss: 0.6232053637504578 = 0.00567214097827673 + 0.1 * 6.175332069396973
Epoch 950, val loss: 1.1572781801223755
Epoch 960, training loss: 0.6231269836425781 = 0.0055354381911456585 + 0.1 * 6.175915241241455
Epoch 960, val loss: 1.1616570949554443
Epoch 970, training loss: 0.6243664026260376 = 0.005403741262853146 + 0.1 * 6.189626216888428
Epoch 970, val loss: 1.1657553911209106
Epoch 980, training loss: 0.6225774884223938 = 0.005277469754219055 + 0.1 * 6.173000335693359
Epoch 980, val loss: 1.1698968410491943
Epoch 990, training loss: 0.6233422756195068 = 0.005156281404197216 + 0.1 * 6.181859970092773
Epoch 990, val loss: 1.1741225719451904
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.6000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.3358
Flip ASR: 0.2578/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7925946712493896 = 1.9552042484283447 + 0.1 * 8.37390422821045
Epoch 0, val loss: 1.9609662294387817
Epoch 10, training loss: 2.7818243503570557 = 1.944444179534912 + 0.1 * 8.373802185058594
Epoch 10, val loss: 1.9493234157562256
Epoch 20, training loss: 2.7684719562530518 = 1.9311391115188599 + 0.1 * 8.373329162597656
Epoch 20, val loss: 1.9345519542694092
Epoch 30, training loss: 2.7493250370025635 = 1.9123315811157227 + 0.1 * 8.369935035705566
Epoch 30, val loss: 1.9133180379867554
Epoch 40, training loss: 2.7188832759857178 = 1.8844287395477295 + 0.1 * 8.344545364379883
Epoch 40, val loss: 1.8818122148513794
Epoch 50, training loss: 2.6625871658325195 = 1.8452942371368408 + 0.1 * 8.172927856445312
Epoch 50, val loss: 1.838881015777588
Epoch 60, training loss: 2.562235116958618 = 1.801012396812439 + 0.1 * 7.612226486206055
Epoch 60, val loss: 1.7929974794387817
Epoch 70, training loss: 2.4969630241394043 = 1.7594141960144043 + 0.1 * 7.375489234924316
Epoch 70, val loss: 1.7527217864990234
Epoch 80, training loss: 2.4341468811035156 = 1.7137086391448975 + 0.1 * 7.204381465911865
Epoch 80, val loss: 1.711097002029419
Epoch 90, training loss: 2.3582847118377686 = 1.6556205749511719 + 0.1 * 7.02664041519165
Epoch 90, val loss: 1.6596791744232178
Epoch 100, training loss: 2.270704507827759 = 1.5794825553894043 + 0.1 * 6.912220001220703
Epoch 100, val loss: 1.5930976867675781
Epoch 110, training loss: 2.1725900173187256 = 1.4866634607315063 + 0.1 * 6.85926628112793
Epoch 110, val loss: 1.5131340026855469
Epoch 120, training loss: 2.0706844329833984 = 1.3881396055221558 + 0.1 * 6.825448989868164
Epoch 120, val loss: 1.4328944683074951
Epoch 130, training loss: 1.9727253913879395 = 1.292966365814209 + 0.1 * 6.7975897789001465
Epoch 130, val loss: 1.3622972965240479
Epoch 140, training loss: 1.8801175355911255 = 1.2035261392593384 + 0.1 * 6.765913963317871
Epoch 140, val loss: 1.3012880086898804
Epoch 150, training loss: 1.7925459146499634 = 1.1197668313980103 + 0.1 * 6.727790832519531
Epoch 150, val loss: 1.2470118999481201
Epoch 160, training loss: 1.7119646072387695 = 1.0421960353851318 + 0.1 * 6.697685241699219
Epoch 160, val loss: 1.197126865386963
Epoch 170, training loss: 1.637952446937561 = 0.9716481566429138 + 0.1 * 6.663043022155762
Epoch 170, val loss: 1.1505541801452637
Epoch 180, training loss: 1.570317268371582 = 0.9063706994056702 + 0.1 * 6.63946533203125
Epoch 180, val loss: 1.1065934896469116
Epoch 190, training loss: 1.5068087577819824 = 0.845037043094635 + 0.1 * 6.617717266082764
Epoch 190, val loss: 1.0641248226165771
Epoch 200, training loss: 1.4474903345108032 = 0.7871689200401306 + 0.1 * 6.603214263916016
Epoch 200, val loss: 1.0229519605636597
Epoch 210, training loss: 1.3916258811950684 = 0.7327960133552551 + 0.1 * 6.588298797607422
Epoch 210, val loss: 0.9832856059074402
Epoch 220, training loss: 1.3398785591125488 = 0.6821632385253906 + 0.1 * 6.577153205871582
Epoch 220, val loss: 0.9460293650627136
Epoch 230, training loss: 1.2919718027114868 = 0.6358386278152466 + 0.1 * 6.561331748962402
Epoch 230, val loss: 0.9122494459152222
Epoch 240, training loss: 1.247635006904602 = 0.5929328203201294 + 0.1 * 6.547021865844727
Epoch 240, val loss: 0.8821269273757935
Epoch 250, training loss: 1.2062053680419922 = 0.5520795583724976 + 0.1 * 6.541257381439209
Epoch 250, val loss: 0.8548440337181091
Epoch 260, training loss: 1.1659884452819824 = 0.5124817490577698 + 0.1 * 6.535067081451416
Epoch 260, val loss: 0.8299955129623413
Epoch 270, training loss: 1.1247409582138062 = 0.47287413477897644 + 0.1 * 6.518667697906494
Epoch 270, val loss: 0.8063902258872986
Epoch 280, training loss: 1.0838468074798584 = 0.4326831102371216 + 0.1 * 6.5116376876831055
Epoch 280, val loss: 0.7840365171432495
Epoch 290, training loss: 1.044874668121338 = 0.3923059403896332 + 0.1 * 6.525687217712402
Epoch 290, val loss: 0.7633072733879089
Epoch 300, training loss: 1.0032660961151123 = 0.3531205952167511 + 0.1 * 6.5014543533325195
Epoch 300, val loss: 0.7452477216720581
Epoch 310, training loss: 0.9658192992210388 = 0.31599563360214233 + 0.1 * 6.498236656188965
Epoch 310, val loss: 0.7302906513214111
Epoch 320, training loss: 0.9318079948425293 = 0.28171974420547485 + 0.1 * 6.500882625579834
Epoch 320, val loss: 0.7189838290214539
Epoch 330, training loss: 0.899603009223938 = 0.2509867548942566 + 0.1 * 6.486162185668945
Epoch 330, val loss: 0.7114498019218445
Epoch 340, training loss: 0.8718315958976746 = 0.22373509407043457 + 0.1 * 6.480964660644531
Epoch 340, val loss: 0.7074356079101562
Epoch 350, training loss: 0.8472719192504883 = 0.19974851608276367 + 0.1 * 6.475234031677246
Epoch 350, val loss: 0.7065253853797913
Epoch 360, training loss: 0.8274984955787659 = 0.17886273562908173 + 0.1 * 6.48635721206665
Epoch 360, val loss: 0.7082704305648804
Epoch 370, training loss: 0.8074886798858643 = 0.16081365942955017 + 0.1 * 6.466750621795654
Epoch 370, val loss: 0.712360143661499
Epoch 380, training loss: 0.7908831834793091 = 0.14505016803741455 + 0.1 * 6.458330154418945
Epoch 380, val loss: 0.7182998061180115
Epoch 390, training loss: 0.7766391634941101 = 0.131226047873497 + 0.1 * 6.45413064956665
Epoch 390, val loss: 0.7257205247879028
Epoch 400, training loss: 0.7631696462631226 = 0.11908622831106186 + 0.1 * 6.440834045410156
Epoch 400, val loss: 0.7343762516975403
Epoch 410, training loss: 0.7513283491134644 = 0.10830050706863403 + 0.1 * 6.430278301239014
Epoch 410, val loss: 0.7439826130867004
Epoch 420, training loss: 0.7422691583633423 = 0.09867030382156372 + 0.1 * 6.435988426208496
Epoch 420, val loss: 0.7543134689331055
Epoch 430, training loss: 0.7334424257278442 = 0.09010844677686691 + 0.1 * 6.433339595794678
Epoch 430, val loss: 0.7652304768562317
Epoch 440, training loss: 0.7235584259033203 = 0.08247967064380646 + 0.1 * 6.410787582397461
Epoch 440, val loss: 0.7765465378761292
Epoch 450, training loss: 0.7153836488723755 = 0.07562797516584396 + 0.1 * 6.397556781768799
Epoch 450, val loss: 0.7882370352745056
Epoch 460, training loss: 0.7101348638534546 = 0.06943580508232117 + 0.1 * 6.4069905281066895
Epoch 460, val loss: 0.8000731468200684
Epoch 470, training loss: 0.7030681371688843 = 0.06390386819839478 + 0.1 * 6.3916425704956055
Epoch 470, val loss: 0.8119165897369385
Epoch 480, training loss: 0.6971445083618164 = 0.05894717574119568 + 0.1 * 6.3819732666015625
Epoch 480, val loss: 0.8238102197647095
Epoch 490, training loss: 0.6914849281311035 = 0.054461024701595306 + 0.1 * 6.3702392578125
Epoch 490, val loss: 0.8357215523719788
Epoch 500, training loss: 0.6869688034057617 = 0.05041387677192688 + 0.1 * 6.365548610687256
Epoch 500, val loss: 0.8476104736328125
Epoch 510, training loss: 0.682163655757904 = 0.04675953462719917 + 0.1 * 6.35404109954834
Epoch 510, val loss: 0.8595172762870789
Epoch 520, training loss: 0.6812023520469666 = 0.04345104843378067 + 0.1 * 6.377512454986572
Epoch 520, val loss: 0.8714026808738708
Epoch 530, training loss: 0.6744537353515625 = 0.040462616831064224 + 0.1 * 6.339910984039307
Epoch 530, val loss: 0.8830167651176453
Epoch 540, training loss: 0.6710062026977539 = 0.037752170115709305 + 0.1 * 6.332540512084961
Epoch 540, val loss: 0.8946395516395569
Epoch 550, training loss: 0.6679158210754395 = 0.035279542207717896 + 0.1 * 6.326362609863281
Epoch 550, val loss: 0.9061647653579712
Epoch 560, training loss: 0.6666203737258911 = 0.03303167223930359 + 0.1 * 6.3358869552612305
Epoch 560, val loss: 0.9175333380699158
Epoch 570, training loss: 0.6626104712486267 = 0.03099210187792778 + 0.1 * 6.316183567047119
Epoch 570, val loss: 0.9286116361618042
Epoch 580, training loss: 0.66197669506073 = 0.029122978448867798 + 0.1 * 6.328536510467529
Epoch 580, val loss: 0.9396214485168457
Epoch 590, training loss: 0.6581381559371948 = 0.02741185761988163 + 0.1 * 6.307262420654297
Epoch 590, val loss: 0.950124204158783
Epoch 600, training loss: 0.6566023230552673 = 0.02584005519747734 + 0.1 * 6.307622909545898
Epoch 600, val loss: 0.9606763124465942
Epoch 610, training loss: 0.6539101004600525 = 0.024399900808930397 + 0.1 * 6.295102119445801
Epoch 610, val loss: 0.9709717631340027
Epoch 620, training loss: 0.6544957160949707 = 0.02307075448334217 + 0.1 * 6.314249515533447
Epoch 620, val loss: 0.9811015725135803
Epoch 630, training loss: 0.6506794095039368 = 0.021857937797904015 + 0.1 * 6.288214683532715
Epoch 630, val loss: 0.9909815192222595
Epoch 640, training loss: 0.6495115756988525 = 0.020738817751407623 + 0.1 * 6.287727355957031
Epoch 640, val loss: 1.0007351636886597
Epoch 650, training loss: 0.6483069658279419 = 0.019706936553120613 + 0.1 * 6.285999774932861
Epoch 650, val loss: 1.0101474523544312
Epoch 660, training loss: 0.6460086703300476 = 0.018750589340925217 + 0.1 * 6.272580623626709
Epoch 660, val loss: 1.0194975137710571
Epoch 670, training loss: 0.6451488733291626 = 0.017864862456917763 + 0.1 * 6.272839546203613
Epoch 670, val loss: 1.0286681652069092
Epoch 680, training loss: 0.6437492966651917 = 0.017045600339770317 + 0.1 * 6.2670369148254395
Epoch 680, val loss: 1.0375537872314453
Epoch 690, training loss: 0.6449717879295349 = 0.016281871125102043 + 0.1 * 6.286899089813232
Epoch 690, val loss: 1.0463489294052124
Epoch 700, training loss: 0.6421864628791809 = 0.015572633594274521 + 0.1 * 6.266138553619385
Epoch 700, val loss: 1.0548526048660278
Epoch 710, training loss: 0.6424263119697571 = 0.014910003170371056 + 0.1 * 6.275163173675537
Epoch 710, val loss: 1.0632421970367432
Epoch 720, training loss: 0.6395484209060669 = 0.014292853884398937 + 0.1 * 6.252555847167969
Epoch 720, val loss: 1.0714504718780518
Epoch 730, training loss: 0.6402338743209839 = 0.013713576830923557 + 0.1 * 6.265202522277832
Epoch 730, val loss: 1.079483151435852
Epoch 740, training loss: 0.6385002732276917 = 0.013171224854886532 + 0.1 * 6.25329065322876
Epoch 740, val loss: 1.087372064590454
Epoch 750, training loss: 0.6367096304893494 = 0.012662343680858612 + 0.1 * 6.240472793579102
Epoch 750, val loss: 1.095138669013977
Epoch 760, training loss: 0.6377905607223511 = 0.012182925827801228 + 0.1 * 6.256075859069824
Epoch 760, val loss: 1.1027926206588745
Epoch 770, training loss: 0.6365368962287903 = 0.011732871644198895 + 0.1 * 6.248040199279785
Epoch 770, val loss: 1.1101768016815186
Epoch 780, training loss: 0.6351499557495117 = 0.011310243979096413 + 0.1 * 6.238397121429443
Epoch 780, val loss: 1.1174826622009277
Epoch 790, training loss: 0.6353686451911926 = 0.010911024175584316 + 0.1 * 6.2445759773254395
Epoch 790, val loss: 1.1246910095214844
Epoch 800, training loss: 0.6340423822402954 = 0.01053505390882492 + 0.1 * 6.235073089599609
Epoch 800, val loss: 1.1316931247711182
Epoch 810, training loss: 0.633638322353363 = 0.010179795324802399 + 0.1 * 6.234584808349609
Epoch 810, val loss: 1.1385725736618042
Epoch 820, training loss: 0.6331703662872314 = 0.009843123145401478 + 0.1 * 6.233272552490234
Epoch 820, val loss: 1.1453611850738525
Epoch 830, training loss: 0.6320722699165344 = 0.009525250643491745 + 0.1 * 6.225470066070557
Epoch 830, val loss: 1.1520423889160156
Epoch 840, training loss: 0.6325371861457825 = 0.00922332052141428 + 0.1 * 6.233138084411621
Epoch 840, val loss: 1.1586029529571533
Epoch 850, training loss: 0.6306538581848145 = 0.008936767466366291 + 0.1 * 6.217170715332031
Epoch 850, val loss: 1.16502845287323
Epoch 860, training loss: 0.6327658891677856 = 0.00866477657109499 + 0.1 * 6.241011142730713
Epoch 860, val loss: 1.1713954210281372
Epoch 870, training loss: 0.630203127861023 = 0.008405993692576885 + 0.1 * 6.217971324920654
Epoch 870, val loss: 1.1774951219558716
Epoch 880, training loss: 0.6296775937080383 = 0.008162260055541992 + 0.1 * 6.215153217315674
Epoch 880, val loss: 1.1835169792175293
Epoch 890, training loss: 0.6306040287017822 = 0.007927695289254189 + 0.1 * 6.2267632484436035
Epoch 890, val loss: 1.1894457340240479
Epoch 900, training loss: 0.628760576248169 = 0.007706410717219114 + 0.1 * 6.210541725158691
Epoch 900, val loss: 1.1952881813049316
Epoch 910, training loss: 0.6291642189025879 = 0.00749426893889904 + 0.1 * 6.216699123382568
Epoch 910, val loss: 1.2010482549667358
Epoch 920, training loss: 0.6284171342849731 = 0.007291042245924473 + 0.1 * 6.211260795593262
Epoch 920, val loss: 1.206697702407837
Epoch 930, training loss: 0.6280653476715088 = 0.007098158821463585 + 0.1 * 6.209671974182129
Epoch 930, val loss: 1.2121998071670532
Epoch 940, training loss: 0.6271413564682007 = 0.0069124735891819 + 0.1 * 6.202288627624512
Epoch 940, val loss: 1.2176507711410522
Epoch 950, training loss: 0.6274973750114441 = 0.006735882256180048 + 0.1 * 6.207614898681641
Epoch 950, val loss: 1.2230629920959473
Epoch 960, training loss: 0.6269279718399048 = 0.006566372700035572 + 0.1 * 6.203616142272949
Epoch 960, val loss: 1.2283411026000977
Epoch 970, training loss: 0.6264442205429077 = 0.0064043509773910046 + 0.1 * 6.2003984451293945
Epoch 970, val loss: 1.2335388660430908
Epoch 980, training loss: 0.6261007785797119 = 0.006249558180570602 + 0.1 * 6.198512077331543
Epoch 980, val loss: 1.2386424541473389
Epoch 990, training loss: 0.6260789036750793 = 0.006100297439843416 + 0.1 * 6.1997857093811035
Epoch 990, val loss: 1.2436621189117432
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.5720
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.769944429397583 = 1.932555079460144 + 0.1 * 8.373893737792969
Epoch 0, val loss: 1.9354792833328247
Epoch 10, training loss: 2.759798049926758 = 1.922424077987671 + 0.1 * 8.373740196228027
Epoch 10, val loss: 1.9248162508010864
Epoch 20, training loss: 2.7468297481536865 = 1.9095407724380493 + 0.1 * 8.372889518737793
Epoch 20, val loss: 1.9108848571777344
Epoch 30, training loss: 2.727962017059326 = 1.89141845703125 + 0.1 * 8.365435600280762
Epoch 30, val loss: 1.891239881515503
Epoch 40, training loss: 2.6956663131713867 = 1.8652571439743042 + 0.1 * 8.304091453552246
Epoch 40, val loss: 1.863364815711975
Epoch 50, training loss: 2.6242194175720215 = 1.8315136432647705 + 0.1 * 7.927056312561035
Epoch 50, val loss: 1.829723596572876
Epoch 60, training loss: 2.5518171787261963 = 1.7980159521102905 + 0.1 * 7.538012504577637
Epoch 60, val loss: 1.7992496490478516
Epoch 70, training loss: 2.474155902862549 = 1.765812873840332 + 0.1 * 7.083429336547852
Epoch 70, val loss: 1.7711101770401
Epoch 80, training loss: 2.4095449447631836 = 1.7284605503082275 + 0.1 * 6.810844421386719
Epoch 80, val loss: 1.7387596368789673
Epoch 90, training loss: 2.350140333175659 = 1.6775412559509277 + 0.1 * 6.7259907722473145
Epoch 90, val loss: 1.694885015487671
Epoch 100, training loss: 2.2776589393615723 = 1.6086626052856445 + 0.1 * 6.689962387084961
Epoch 100, val loss: 1.6366719007492065
Epoch 110, training loss: 2.190372943878174 = 1.523648977279663 + 0.1 * 6.667240142822266
Epoch 110, val loss: 1.5665327310562134
Epoch 120, training loss: 2.0952234268188477 = 1.4300707578659058 + 0.1 * 6.651526927947998
Epoch 120, val loss: 1.4914122819900513
Epoch 130, training loss: 1.9993302822113037 = 1.335312843322754 + 0.1 * 6.640174388885498
Epoch 130, val loss: 1.419472336769104
Epoch 140, training loss: 1.906644344329834 = 1.2438257932662964 + 0.1 * 6.628185749053955
Epoch 140, val loss: 1.3554588556289673
Epoch 150, training loss: 1.8183741569519043 = 1.1566001176834106 + 0.1 * 6.617740631103516
Epoch 150, val loss: 1.2979025840759277
Epoch 160, training loss: 1.734007477760315 = 1.0735726356506348 + 0.1 * 6.604348182678223
Epoch 160, val loss: 1.2435436248779297
Epoch 170, training loss: 1.6530158519744873 = 0.9939309358596802 + 0.1 * 6.590848922729492
Epoch 170, val loss: 1.1910055875778198
Epoch 180, training loss: 1.5754046440124512 = 0.9177833795547485 + 0.1 * 6.576212406158447
Epoch 180, val loss: 1.1400824785232544
Epoch 190, training loss: 1.5010125637054443 = 0.8448582291603088 + 0.1 * 6.561542987823486
Epoch 190, val loss: 1.0896676778793335
Epoch 200, training loss: 1.4304604530334473 = 0.7755573391914368 + 0.1 * 6.549031734466553
Epoch 200, val loss: 1.0401639938354492
Epoch 210, training loss: 1.3628038167953491 = 0.7102903723716736 + 0.1 * 6.525134086608887
Epoch 210, val loss: 0.9928603768348694
Epoch 220, training loss: 1.3004531860351562 = 0.6492131352424622 + 0.1 * 6.512399673461914
Epoch 220, val loss: 0.9481714367866516
Epoch 230, training loss: 1.2421817779541016 = 0.5929179191589355 + 0.1 * 6.49263858795166
Epoch 230, val loss: 0.9080327153205872
Epoch 240, training loss: 1.1885571479797363 = 0.5413388013839722 + 0.1 * 6.472184181213379
Epoch 240, val loss: 0.8729562759399414
Epoch 250, training loss: 1.1399831771850586 = 0.4941169321537018 + 0.1 * 6.458662509918213
Epoch 250, val loss: 0.8434355854988098
Epoch 260, training loss: 1.0954495668411255 = 0.4507220983505249 + 0.1 * 6.447274684906006
Epoch 260, val loss: 0.8192080855369568
Epoch 270, training loss: 1.0531808137893677 = 0.4101319909095764 + 0.1 * 6.430488109588623
Epoch 270, val loss: 0.7992565631866455
Epoch 280, training loss: 1.013401746749878 = 0.3714848756790161 + 0.1 * 6.419167995452881
Epoch 280, val loss: 0.7823994159698486
Epoch 290, training loss: 0.9750632643699646 = 0.3342699408531189 + 0.1 * 6.407933235168457
Epoch 290, val loss: 0.7676157355308533
Epoch 300, training loss: 0.9384124279022217 = 0.29819631576538086 + 0.1 * 6.402161121368408
Epoch 300, val loss: 0.7539980411529541
Epoch 310, training loss: 0.9034096002578735 = 0.2637811601161957 + 0.1 * 6.396284103393555
Epoch 310, val loss: 0.741744875907898
Epoch 320, training loss: 0.8707879185676575 = 0.2313794493675232 + 0.1 * 6.394084453582764
Epoch 320, val loss: 0.7309861183166504
Epoch 330, training loss: 0.8403853178024292 = 0.2020379900932312 + 0.1 * 6.383472919464111
Epoch 330, val loss: 0.7227433323860168
Epoch 340, training loss: 0.8145528435707092 = 0.17648614943027496 + 0.1 * 6.380667209625244
Epoch 340, val loss: 0.7177583575248718
Epoch 350, training loss: 0.7919611930847168 = 0.15490411221981049 + 0.1 * 6.370570659637451
Epoch 350, val loss: 0.7161621451377869
Epoch 360, training loss: 0.7749965786933899 = 0.13692598044872284 + 0.1 * 6.380705833435059
Epoch 360, val loss: 0.7174150943756104
Epoch 370, training loss: 0.7576414346694946 = 0.12204950302839279 + 0.1 * 6.355919361114502
Epoch 370, val loss: 0.7208139896392822
Epoch 380, training loss: 0.744186282157898 = 0.10959688574075699 + 0.1 * 6.345893383026123
Epoch 380, val loss: 0.7257305383682251
Epoch 390, training loss: 0.7337211966514587 = 0.09907429665327072 + 0.1 * 6.346468925476074
Epoch 390, val loss: 0.731698215007782
Epoch 400, training loss: 0.723806619644165 = 0.09010828286409378 + 0.1 * 6.3369832038879395
Epoch 400, val loss: 0.7383106350898743
Epoch 410, training loss: 0.7148466110229492 = 0.08234032988548279 + 0.1 * 6.3250627517700195
Epoch 410, val loss: 0.745368242263794
Epoch 420, training loss: 0.7084898352622986 = 0.07551924139261246 + 0.1 * 6.329705715179443
Epoch 420, val loss: 0.7527874708175659
Epoch 430, training loss: 0.7017596364021301 = 0.06951325386762619 + 0.1 * 6.322463512420654
Epoch 430, val loss: 0.7604146003723145
Epoch 440, training loss: 0.6955428123474121 = 0.0641724169254303 + 0.1 * 6.313704013824463
Epoch 440, val loss: 0.7680625319480896
Epoch 450, training loss: 0.6907438635826111 = 0.0593705028295517 + 0.1 * 6.3137335777282715
Epoch 450, val loss: 0.7757933139801025
Epoch 460, training loss: 0.6867494583129883 = 0.055052366107702255 + 0.1 * 6.316971302032471
Epoch 460, val loss: 0.7834964990615845
Epoch 470, training loss: 0.6807843446731567 = 0.05115317925810814 + 0.1 * 6.296311378479004
Epoch 470, val loss: 0.7911190390586853
Epoch 480, training loss: 0.6759669780731201 = 0.04760506749153137 + 0.1 * 6.283618450164795
Epoch 480, val loss: 0.7986962795257568
Epoch 490, training loss: 0.675994873046875 = 0.044368937611579895 + 0.1 * 6.316259384155273
Epoch 490, val loss: 0.8062436580657959
Epoch 500, training loss: 0.6701505780220032 = 0.041429463773965836 + 0.1 * 6.287210941314697
Epoch 500, val loss: 0.813647449016571
Epoch 510, training loss: 0.6659183502197266 = 0.0387415811419487 + 0.1 * 6.271767616271973
Epoch 510, val loss: 0.8209035396575928
Epoch 520, training loss: 0.6654263138771057 = 0.036273129284381866 + 0.1 * 6.291531562805176
Epoch 520, val loss: 0.8281156420707703
Epoch 530, training loss: 0.6604800820350647 = 0.03401719778776169 + 0.1 * 6.264628887176514
Epoch 530, val loss: 0.8352029323577881
Epoch 540, training loss: 0.6582726836204529 = 0.031949739903211594 + 0.1 * 6.2632293701171875
Epoch 540, val loss: 0.8421254754066467
Epoch 550, training loss: 0.6575238108634949 = 0.030046159401535988 + 0.1 * 6.274776458740234
Epoch 550, val loss: 0.8489269614219666
Epoch 560, training loss: 0.6532432436943054 = 0.028299570083618164 + 0.1 * 6.249436378479004
Epoch 560, val loss: 0.855646014213562
Epoch 570, training loss: 0.6513281464576721 = 0.026691298931837082 + 0.1 * 6.246368408203125
Epoch 570, val loss: 0.8622151613235474
Epoch 580, training loss: 0.6496235728263855 = 0.02520545944571495 + 0.1 * 6.244181156158447
Epoch 580, val loss: 0.8686732053756714
Epoch 590, training loss: 0.6482563018798828 = 0.023832013830542564 + 0.1 * 6.2442426681518555
Epoch 590, val loss: 0.8750365376472473
Epoch 600, training loss: 0.6474723219871521 = 0.022566480562090874 + 0.1 * 6.249058246612549
Epoch 600, val loss: 0.8812445998191833
Epoch 610, training loss: 0.6450941562652588 = 0.0213989969342947 + 0.1 * 6.2369513511657715
Epoch 610, val loss: 0.8872995972633362
Epoch 620, training loss: 0.643329918384552 = 0.020316950976848602 + 0.1 * 6.230129718780518
Epoch 620, val loss: 0.893175482749939
Epoch 630, training loss: 0.642963171005249 = 0.019311388954520226 + 0.1 * 6.236517906188965
Epoch 630, val loss: 0.8989610075950623
Epoch 640, training loss: 0.6406814455986023 = 0.018380198627710342 + 0.1 * 6.2230119705200195
Epoch 640, val loss: 0.9046287536621094
Epoch 650, training loss: 0.6400852799415588 = 0.01751408725976944 + 0.1 * 6.225711345672607
Epoch 650, val loss: 0.9101504683494568
Epoch 660, training loss: 0.639106035232544 = 0.016707099974155426 + 0.1 * 6.223989009857178
Epoch 660, val loss: 0.9155349731445312
Epoch 670, training loss: 0.6391544938087463 = 0.015955690294504166 + 0.1 * 6.231987953186035
Epoch 670, val loss: 0.9208173751831055
Epoch 680, training loss: 0.6370660662651062 = 0.015255680307745934 + 0.1 * 6.218103885650635
Epoch 680, val loss: 0.9259921908378601
Epoch 690, training loss: 0.6369852423667908 = 0.0146007826551795 + 0.1 * 6.223844528198242
Epoch 690, val loss: 0.9310265779495239
Epoch 700, training loss: 0.6355251669883728 = 0.013987830840051174 + 0.1 * 6.2153730392456055
Epoch 700, val loss: 0.9359806776046753
Epoch 710, training loss: 0.6343774795532227 = 0.013415174558758736 + 0.1 * 6.209622859954834
Epoch 710, val loss: 0.9408199787139893
Epoch 720, training loss: 0.6354440450668335 = 0.012878775596618652 + 0.1 * 6.225652694702148
Epoch 720, val loss: 0.9455766677856445
Epoch 730, training loss: 0.6330776214599609 = 0.012374951504170895 + 0.1 * 6.207026958465576
Epoch 730, val loss: 0.9502131938934326
Epoch 740, training loss: 0.6328431367874146 = 0.01190213579684496 + 0.1 * 6.209409713745117
Epoch 740, val loss: 0.9547691941261292
Epoch 750, training loss: 0.6311003565788269 = 0.011456964537501335 + 0.1 * 6.1964335441589355
Epoch 750, val loss: 0.9592220783233643
Epoch 760, training loss: 0.6314437389373779 = 0.011037238873541355 + 0.1 * 6.204064846038818
Epoch 760, val loss: 0.9635825753211975
Epoch 770, training loss: 0.63199383020401 = 0.010641032829880714 + 0.1 * 6.213527679443359
Epoch 770, val loss: 0.9679065942764282
Epoch 780, training loss: 0.6301828026771545 = 0.010267945006489754 + 0.1 * 6.199148178100586
Epoch 780, val loss: 0.9721330404281616
Epoch 790, training loss: 0.6291085481643677 = 0.009916294366121292 + 0.1 * 6.191922187805176
Epoch 790, val loss: 0.9762554168701172
Epoch 800, training loss: 0.6290509700775146 = 0.009582825936377048 + 0.1 * 6.194681644439697
Epoch 800, val loss: 0.9802908301353455
Epoch 810, training loss: 0.6278529763221741 = 0.009266492910683155 + 0.1 * 6.1858649253845215
Epoch 810, val loss: 0.9842348694801331
Epoch 820, training loss: 0.627414882183075 = 0.008967367932200432 + 0.1 * 6.184475421905518
Epoch 820, val loss: 0.9881101250648499
Epoch 830, training loss: 0.6274756193161011 = 0.008683597669005394 + 0.1 * 6.187919616699219
Epoch 830, val loss: 0.9919335246086121
Epoch 840, training loss: 0.6276536583900452 = 0.008414479903876781 + 0.1 * 6.192391395568848
Epoch 840, val loss: 0.9956788420677185
Epoch 850, training loss: 0.6261522769927979 = 0.008158879354596138 + 0.1 * 6.179934024810791
Epoch 850, val loss: 0.9993265867233276
Epoch 860, training loss: 0.6270400881767273 = 0.007915341295301914 + 0.1 * 6.191247463226318
Epoch 860, val loss: 1.0029118061065674
Epoch 870, training loss: 0.6264203786849976 = 0.007683353032916784 + 0.1 * 6.187370300292969
Epoch 870, val loss: 1.006465196609497
Epoch 880, training loss: 0.6258370876312256 = 0.007462804671376944 + 0.1 * 6.183742523193359
Epoch 880, val loss: 1.0099585056304932
Epoch 890, training loss: 0.6245942115783691 = 0.0072528100572526455 + 0.1 * 6.1734137535095215
Epoch 890, val loss: 1.0133583545684814
Epoch 900, training loss: 0.6265866160392761 = 0.007052148226648569 + 0.1 * 6.195344924926758
Epoch 900, val loss: 1.0166664123535156
Epoch 910, training loss: 0.62434983253479 = 0.006861040834337473 + 0.1 * 6.1748881340026855
Epoch 910, val loss: 1.0199538469314575
Epoch 920, training loss: 0.6242507696151733 = 0.006678140256553888 + 0.1 * 6.175725936889648
Epoch 920, val loss: 1.0231921672821045
Epoch 930, training loss: 0.624640703201294 = 0.006503087468445301 + 0.1 * 6.181375980377197
Epoch 930, val loss: 1.0263527631759644
Epoch 940, training loss: 0.6231310963630676 = 0.0063359709456563 + 0.1 * 6.1679511070251465
Epoch 940, val loss: 1.0294792652130127
Epoch 950, training loss: 0.6230534315109253 = 0.006175843067467213 + 0.1 * 6.168776035308838
Epoch 950, val loss: 1.0325385332107544
Epoch 960, training loss: 0.622170090675354 = 0.006022306624799967 + 0.1 * 6.161477565765381
Epoch 960, val loss: 1.035534381866455
Epoch 970, training loss: 0.6233576536178589 = 0.005875060334801674 + 0.1 * 6.174826145172119
Epoch 970, val loss: 1.0384892225265503
Epoch 980, training loss: 0.6229608654975891 = 0.005734077654778957 + 0.1 * 6.172267436981201
Epoch 980, val loss: 1.0414303541183472
Epoch 990, training loss: 0.6211066842079163 = 0.005598899908363819 + 0.1 * 6.1550774574279785
Epoch 990, val loss: 1.0443382263183594
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7749
Flip ASR: 0.7333/225 nodes
The final ASR:0.56089, 0.17944, Accuracy:0.80617, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9556])
updated graph: torch.Size([2, 10602])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7831709384918213 = 1.9457844495773315 + 0.1 * 8.37386417388916
Epoch 0, val loss: 1.9505640268325806
Epoch 10, training loss: 2.772251605987549 = 1.9348806142807007 + 0.1 * 8.373710632324219
Epoch 10, val loss: 1.9399235248565674
Epoch 20, training loss: 2.7583224773406982 = 1.9210442304611206 + 0.1 * 8.372782707214355
Epoch 20, val loss: 1.9259480237960815
Epoch 30, training loss: 2.737847328186035 = 1.9012997150421143 + 0.1 * 8.365474700927734
Epoch 30, val loss: 1.9056828022003174
Epoch 40, training loss: 2.7049851417541504 = 1.8721892833709717 + 0.1 * 8.327959060668945
Epoch 40, val loss: 1.8763110637664795
Epoch 50, training loss: 2.6496036052703857 = 1.834367275238037 + 0.1 * 8.152362823486328
Epoch 50, val loss: 1.8407701253890991
Epoch 60, training loss: 2.5768609046936035 = 1.7947142124176025 + 0.1 * 7.821465969085693
Epoch 60, val loss: 1.80724036693573
Epoch 70, training loss: 2.515629291534424 = 1.755861759185791 + 0.1 * 7.59767484664917
Epoch 70, val loss: 1.7755157947540283
Epoch 80, training loss: 2.4398951530456543 = 1.7090435028076172 + 0.1 * 7.308516979217529
Epoch 80, val loss: 1.7362934350967407
Epoch 90, training loss: 2.3615612983703613 = 1.6492185592651367 + 0.1 * 7.1234259605407715
Epoch 90, val loss: 1.6878433227539062
Epoch 100, training loss: 2.2760086059570312 = 1.5730321407318115 + 0.1 * 7.029765605926514
Epoch 100, val loss: 1.6252421140670776
Epoch 110, training loss: 2.177717685699463 = 1.4840142726898193 + 0.1 * 6.937033653259277
Epoch 110, val loss: 1.5519250631332397
Epoch 120, training loss: 2.074553966522217 = 1.3902770280838013 + 0.1 * 6.842769145965576
Epoch 120, val loss: 1.4772894382476807
Epoch 130, training loss: 1.9729864597320557 = 1.2950783967971802 + 0.1 * 6.779081344604492
Epoch 130, val loss: 1.4029157161712646
Epoch 140, training loss: 1.871000051498413 = 1.1973952054977417 + 0.1 * 6.736047744750977
Epoch 140, val loss: 1.3278425931930542
Epoch 150, training loss: 1.7684459686279297 = 1.0976598262786865 + 0.1 * 6.70786190032959
Epoch 150, val loss: 1.2521300315856934
Epoch 160, training loss: 1.6691514253616333 = 0.9997233152389526 + 0.1 * 6.694281101226807
Epoch 160, val loss: 1.179099440574646
Epoch 170, training loss: 1.5767457485198975 = 0.9087358117103577 + 0.1 * 6.680098533630371
Epoch 170, val loss: 1.113473653793335
Epoch 180, training loss: 1.4937281608581543 = 0.8270502090454102 + 0.1 * 6.666779518127441
Epoch 180, val loss: 1.0562865734100342
Epoch 190, training loss: 1.4217908382415771 = 0.7564928531646729 + 0.1 * 6.652979850769043
Epoch 190, val loss: 1.0090981721878052
Epoch 200, training loss: 1.3604211807250977 = 0.696583092212677 + 0.1 * 6.638380527496338
Epoch 200, val loss: 0.9712120890617371
Epoch 210, training loss: 1.307374119758606 = 0.6451243162155151 + 0.1 * 6.622498035430908
Epoch 210, val loss: 0.9414215087890625
Epoch 220, training loss: 1.2598094940185547 = 0.5989806652069092 + 0.1 * 6.608288288116455
Epoch 220, val loss: 0.9173211455345154
Epoch 230, training loss: 1.215008020401001 = 0.5558357834815979 + 0.1 * 6.591722011566162
Epoch 230, val loss: 0.8969317674636841
Epoch 240, training loss: 1.1723467111587524 = 0.5146732330322266 + 0.1 * 6.57673454284668
Epoch 240, val loss: 0.8793469071388245
Epoch 250, training loss: 1.1328197717666626 = 0.47601738572120667 + 0.1 * 6.568023681640625
Epoch 250, val loss: 0.864616334438324
Epoch 260, training loss: 1.0959761142730713 = 0.44047844409942627 + 0.1 * 6.554976463317871
Epoch 260, val loss: 0.8532613515853882
Epoch 270, training loss: 1.0621258020401 = 0.40784952044487 + 0.1 * 6.542762756347656
Epoch 270, val loss: 0.8452737331390381
Epoch 280, training loss: 1.031035304069519 = 0.3776126205921173 + 0.1 * 6.534226894378662
Epoch 280, val loss: 0.8401082754135132
Epoch 290, training loss: 1.0017714500427246 = 0.34898942708969116 + 0.1 * 6.527820110321045
Epoch 290, val loss: 0.8373444676399231
Epoch 300, training loss: 0.9725714921951294 = 0.320776104927063 + 0.1 * 6.517953872680664
Epoch 300, val loss: 0.8357712030410767
Epoch 310, training loss: 0.9436144828796387 = 0.29246386885643005 + 0.1 * 6.511506080627441
Epoch 310, val loss: 0.8350973129272461
Epoch 320, training loss: 0.9145937561988831 = 0.26412689685821533 + 0.1 * 6.504668235778809
Epoch 320, val loss: 0.8354867100715637
Epoch 330, training loss: 0.8862545490264893 = 0.23621787130832672 + 0.1 * 6.500366687774658
Epoch 330, val loss: 0.8372599482536316
Epoch 340, training loss: 0.8592065572738647 = 0.20958851277828217 + 0.1 * 6.496180057525635
Epoch 340, val loss: 0.8408573865890503
Epoch 350, training loss: 0.8349642157554626 = 0.18511433899402618 + 0.1 * 6.498498916625977
Epoch 350, val loss: 0.8465898036956787
Epoch 360, training loss: 0.8122552633285522 = 0.1632935106754303 + 0.1 * 6.489617824554443
Epoch 360, val loss: 0.8546480536460876
Epoch 370, training loss: 0.7922334671020508 = 0.14410361647605896 + 0.1 * 6.481297969818115
Epoch 370, val loss: 0.8649564981460571
Epoch 380, training loss: 0.7771456837654114 = 0.12741361558437347 + 0.1 * 6.497320652008057
Epoch 380, val loss: 0.8771611452102661
Epoch 390, training loss: 0.7610995173454285 = 0.11311744153499603 + 0.1 * 6.479820728302002
Epoch 390, val loss: 0.8906933665275574
Epoch 400, training loss: 0.7472115159034729 = 0.10081767290830612 + 0.1 * 6.463938236236572
Epoch 400, val loss: 0.9053840637207031
Epoch 410, training loss: 0.7377932071685791 = 0.09019427001476288 + 0.1 * 6.475988864898682
Epoch 410, val loss: 0.9212282299995422
Epoch 420, training loss: 0.7267652750015259 = 0.0810571163892746 + 0.1 * 6.457081317901611
Epoch 420, val loss: 0.9376580715179443
Epoch 430, training loss: 0.7178165316581726 = 0.07314088195562363 + 0.1 * 6.446756362915039
Epoch 430, val loss: 0.9544957876205444
Epoch 440, training loss: 0.710667610168457 = 0.06626807898283005 + 0.1 * 6.443994998931885
Epoch 440, val loss: 0.971570611000061
Epoch 450, training loss: 0.7044044733047485 = 0.06030551344156265 + 0.1 * 6.440989017486572
Epoch 450, val loss: 0.9886570572853088
Epoch 460, training loss: 0.6977585554122925 = 0.05507365241646767 + 0.1 * 6.426848888397217
Epoch 460, val loss: 1.0056198835372925
Epoch 470, training loss: 0.6972503066062927 = 0.050458867102861404 + 0.1 * 6.467914581298828
Epoch 470, val loss: 1.0223888158798218
Epoch 480, training loss: 0.6890733242034912 = 0.04641754552721977 + 0.1 * 6.426558017730713
Epoch 480, val loss: 1.0385793447494507
Epoch 490, training loss: 0.6838605403900146 = 0.04283953830599785 + 0.1 * 6.410210132598877
Epoch 490, val loss: 1.0543104410171509
Epoch 500, training loss: 0.6799310445785522 = 0.03964672610163689 + 0.1 * 6.402842998504639
Epoch 500, val loss: 1.069887638092041
Epoch 510, training loss: 0.676360547542572 = 0.036788102239370346 + 0.1 * 6.395724296569824
Epoch 510, val loss: 1.084836721420288
Epoch 520, training loss: 0.6740108132362366 = 0.03422824293375015 + 0.1 * 6.397825241088867
Epoch 520, val loss: 1.0996240377426147
Epoch 530, training loss: 0.67121422290802 = 0.031931161880493164 + 0.1 * 6.3928303718566895
Epoch 530, val loss: 1.1135647296905518
Epoch 540, training loss: 0.6675263047218323 = 0.029859986156225204 + 0.1 * 6.3766632080078125
Epoch 540, val loss: 1.127327799797058
Epoch 550, training loss: 0.6654329299926758 = 0.02798108570277691 + 0.1 * 6.374517917633057
Epoch 550, val loss: 1.1407525539398193
Epoch 560, training loss: 0.663189709186554 = 0.026277629658579826 + 0.1 * 6.369121074676514
Epoch 560, val loss: 1.1536781787872314
Epoch 570, training loss: 0.6610579490661621 = 0.02472946234047413 + 0.1 * 6.363284587860107
Epoch 570, val loss: 1.1663984060287476
Epoch 580, training loss: 0.6606652140617371 = 0.023317977786064148 + 0.1 * 6.373472213745117
Epoch 580, val loss: 1.178598165512085
Epoch 590, training loss: 0.6574429273605347 = 0.0220300555229187 + 0.1 * 6.354128360748291
Epoch 590, val loss: 1.1906745433807373
Epoch 600, training loss: 0.6567046642303467 = 0.020847974345088005 + 0.1 * 6.358566761016846
Epoch 600, val loss: 1.2023019790649414
Epoch 610, training loss: 0.6553649306297302 = 0.019765054807066917 + 0.1 * 6.355998516082764
Epoch 610, val loss: 1.2135779857635498
Epoch 620, training loss: 0.6518186926841736 = 0.01877005770802498 + 0.1 * 6.330486297607422
Epoch 620, val loss: 1.2246497869491577
Epoch 630, training loss: 0.6511430144309998 = 0.017852777615189552 + 0.1 * 6.332902431488037
Epoch 630, val loss: 1.2355703115463257
Epoch 640, training loss: 0.6514614224433899 = 0.017005527392029762 + 0.1 * 6.344559192657471
Epoch 640, val loss: 1.2459516525268555
Epoch 650, training loss: 0.6477827429771423 = 0.016222694888710976 + 0.1 * 6.315600395202637
Epoch 650, val loss: 1.2562966346740723
Epoch 660, training loss: 0.6469482779502869 = 0.01549599226564169 + 0.1 * 6.3145222663879395
Epoch 660, val loss: 1.2663792371749878
Epoch 670, training loss: 0.6477408409118652 = 0.014820312149822712 + 0.1 * 6.32920503616333
Epoch 670, val loss: 1.2761293649673462
Epoch 680, training loss: 0.6456586122512817 = 0.014191248454153538 + 0.1 * 6.31467342376709
Epoch 680, val loss: 1.285772681236267
Epoch 690, training loss: 0.645566463470459 = 0.013605439104139805 + 0.1 * 6.319610118865967
Epoch 690, val loss: 1.295127272605896
Epoch 700, training loss: 0.6430608630180359 = 0.013058875687420368 + 0.1 * 6.30001974105835
Epoch 700, val loss: 1.3043372631072998
Epoch 710, training loss: 0.6418756246566772 = 0.012547024525702 + 0.1 * 6.293285846710205
Epoch 710, val loss: 1.313395619392395
Epoch 720, training loss: 0.6427336931228638 = 0.012066535651683807 + 0.1 * 6.306671619415283
Epoch 720, val loss: 1.3221945762634277
Epoch 730, training loss: 0.6409310698509216 = 0.011616514064371586 + 0.1 * 6.293145179748535
Epoch 730, val loss: 1.3308351039886475
Epoch 740, training loss: 0.6395297050476074 = 0.011193518526852131 + 0.1 * 6.283361911773682
Epoch 740, val loss: 1.3394578695297241
Epoch 750, training loss: 0.6391028165817261 = 0.010794814676046371 + 0.1 * 6.283079624176025
Epoch 750, val loss: 1.347434163093567
Epoch 760, training loss: 0.6382782459259033 = 0.01042089518159628 + 0.1 * 6.278573036193848
Epoch 760, val loss: 1.3556779623031616
Epoch 770, training loss: 0.637320876121521 = 0.010067755356431007 + 0.1 * 6.272531032562256
Epoch 770, val loss: 1.3638396263122559
Epoch 780, training loss: 0.6364212036132812 = 0.009733036160469055 + 0.1 * 6.266881942749023
Epoch 780, val loss: 1.371465802192688
Epoch 790, training loss: 0.6364306807518005 = 0.009417066350579262 + 0.1 * 6.270135879516602
Epoch 790, val loss: 1.3791872262954712
Epoch 800, training loss: 0.6348491907119751 = 0.009117784909904003 + 0.1 * 6.2573137283325195
Epoch 800, val loss: 1.386780858039856
Epoch 810, training loss: 0.6363446116447449 = 0.008833913132548332 + 0.1 * 6.275106906890869
Epoch 810, val loss: 1.394296646118164
Epoch 820, training loss: 0.6345603466033936 = 0.008564294315874577 + 0.1 * 6.259960174560547
Epoch 820, val loss: 1.4014801979064941
Epoch 830, training loss: 0.6348309516906738 = 0.008308015763759613 + 0.1 * 6.265229225158691
Epoch 830, val loss: 1.4085355997085571
Epoch 840, training loss: 0.6338871717453003 = 0.008065011352300644 + 0.1 * 6.258221626281738
Epoch 840, val loss: 1.4156572818756104
Epoch 850, training loss: 0.6323386430740356 = 0.007833651266992092 + 0.1 * 6.245049476623535
Epoch 850, val loss: 1.4224117994308472
Epoch 860, training loss: 0.6320247650146484 = 0.007613737601786852 + 0.1 * 6.244110107421875
Epoch 860, val loss: 1.429353952407837
Epoch 870, training loss: 0.6334706544876099 = 0.0074034216813743114 + 0.1 * 6.260672092437744
Epoch 870, val loss: 1.4360744953155518
Epoch 880, training loss: 0.6317974328994751 = 0.007202310021966696 + 0.1 * 6.245951175689697
Epoch 880, val loss: 1.4423861503601074
Epoch 890, training loss: 0.6306548714637756 = 0.0070111812092363834 + 0.1 * 6.23643684387207
Epoch 890, val loss: 1.4490480422973633
Epoch 900, training loss: 0.6317666172981262 = 0.0068278186954557896 + 0.1 * 6.249387741088867
Epoch 900, val loss: 1.4553781747817993
Epoch 910, training loss: 0.6318642497062683 = 0.006652666721493006 + 0.1 * 6.252115726470947
Epoch 910, val loss: 1.4613752365112305
Epoch 920, training loss: 0.6294453740119934 = 0.006485683377832174 + 0.1 * 6.2295966148376465
Epoch 920, val loss: 1.4676485061645508
Epoch 930, training loss: 0.6293034553527832 = 0.0063255769200623035 + 0.1 * 6.229778289794922
Epoch 930, val loss: 1.473880648612976
Epoch 940, training loss: 0.6288179159164429 = 0.006171238608658314 + 0.1 * 6.226466655731201
Epoch 940, val loss: 1.4795557260513306
Epoch 950, training loss: 0.6288657784461975 = 0.006024000234901905 + 0.1 * 6.22841739654541
Epoch 950, val loss: 1.48548424243927
Epoch 960, training loss: 0.629029393196106 = 0.005882653407752514 + 0.1 * 6.231467247009277
Epoch 960, val loss: 1.4912446737289429
Epoch 970, training loss: 0.6277018785476685 = 0.005747245624661446 + 0.1 * 6.219545841217041
Epoch 970, val loss: 1.4967800378799438
Epoch 980, training loss: 0.6278426647186279 = 0.0056174201890826225 + 0.1 * 6.222251892089844
Epoch 980, val loss: 1.5026321411132812
Epoch 990, training loss: 0.6268219947814941 = 0.0054922811686992645 + 0.1 * 6.213296890258789
Epoch 990, val loss: 1.5082345008850098
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6568
Flip ASR: 0.5911/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.778874635696411 = 1.941487431526184 + 0.1 * 8.373871803283691
Epoch 0, val loss: 1.934972882270813
Epoch 10, training loss: 2.7694146633148193 = 1.9320417642593384 + 0.1 * 8.37372875213623
Epoch 10, val loss: 1.925730586051941
Epoch 20, training loss: 2.757946014404297 = 1.9206589460372925 + 0.1 * 8.372870445251465
Epoch 20, val loss: 1.9141407012939453
Epoch 30, training loss: 2.741440773010254 = 1.9048070907592773 + 0.1 * 8.366336822509766
Epoch 30, val loss: 1.897724986076355
Epoch 40, training loss: 2.7140541076660156 = 1.8816027641296387 + 0.1 * 8.324512481689453
Epoch 40, val loss: 1.8736960887908936
Epoch 50, training loss: 2.658160924911499 = 1.8493661880493164 + 0.1 * 8.087946891784668
Epoch 50, val loss: 1.8418914079666138
Epoch 60, training loss: 2.5788402557373047 = 1.8115140199661255 + 0.1 * 7.673262596130371
Epoch 60, val loss: 1.8068480491638184
Epoch 70, training loss: 2.4989829063415527 = 1.7732797861099243 + 0.1 * 7.2570319175720215
Epoch 70, val loss: 1.771859884262085
Epoch 80, training loss: 2.4319324493408203 = 1.7319437265396118 + 0.1 * 6.9998860359191895
Epoch 80, val loss: 1.7339951992034912
Epoch 90, training loss: 2.367306709289551 = 1.6811922788619995 + 0.1 * 6.86114501953125
Epoch 90, val loss: 1.6876596212387085
Epoch 100, training loss: 2.293539524078369 = 1.614047646522522 + 0.1 * 6.794919967651367
Epoch 100, val loss: 1.6281338930130005
Epoch 110, training loss: 2.202857732772827 = 1.5276389122009277 + 0.1 * 6.752187252044678
Epoch 110, val loss: 1.5554026365280151
Epoch 120, training loss: 2.097811222076416 = 1.4260163307189941 + 0.1 * 6.7179484367370605
Epoch 120, val loss: 1.4729549884796143
Epoch 130, training loss: 1.988325834274292 = 1.3188130855560303 + 0.1 * 6.695127487182617
Epoch 130, val loss: 1.3882663249969482
Epoch 140, training loss: 1.8811335563659668 = 1.2129356861114502 + 0.1 * 6.681978225708008
Epoch 140, val loss: 1.3070123195648193
Epoch 150, training loss: 1.7783339023590088 = 1.1112390756607056 + 0.1 * 6.6709489822387695
Epoch 150, val loss: 1.2305278778076172
Epoch 160, training loss: 1.6816871166229248 = 1.015519380569458 + 0.1 * 6.661677837371826
Epoch 160, val loss: 1.158852458000183
Epoch 170, training loss: 1.5929265022277832 = 0.9274390339851379 + 0.1 * 6.654874324798584
Epoch 170, val loss: 1.0935739278793335
Epoch 180, training loss: 1.511188268661499 = 0.846804678440094 + 0.1 * 6.64383602142334
Epoch 180, val loss: 1.0342413187026978
Epoch 190, training loss: 1.4357699155807495 = 0.7722828388214111 + 0.1 * 6.634870529174805
Epoch 190, val loss: 0.9797335267066956
Epoch 200, training loss: 1.3659199476242065 = 0.704200029373169 + 0.1 * 6.617198944091797
Epoch 200, val loss: 0.9307276606559753
Epoch 210, training loss: 1.302244782447815 = 0.6420334577560425 + 0.1 * 6.602113246917725
Epoch 210, val loss: 0.8873881697654724
Epoch 220, training loss: 1.2441139221191406 = 0.5851064324378967 + 0.1 * 6.590074062347412
Epoch 220, val loss: 0.8498477339744568
Epoch 230, training loss: 1.1906640529632568 = 0.5333614349365234 + 0.1 * 6.573026180267334
Epoch 230, val loss: 0.8181113004684448
Epoch 240, training loss: 1.1419565677642822 = 0.48588186502456665 + 0.1 * 6.560747146606445
Epoch 240, val loss: 0.7911049127578735
Epoch 250, training loss: 1.0971567630767822 = 0.44210702180862427 + 0.1 * 6.550497055053711
Epoch 250, val loss: 0.7683863639831543
Epoch 260, training loss: 1.055689811706543 = 0.4019845724105835 + 0.1 * 6.537052154541016
Epoch 260, val loss: 0.7499080300331116
Epoch 270, training loss: 1.0179897546768188 = 0.3650471866130829 + 0.1 * 6.529425621032715
Epoch 270, val loss: 0.7354273796081543
Epoch 280, training loss: 0.9831401109695435 = 0.33138060569763184 + 0.1 * 6.517594814300537
Epoch 280, val loss: 0.7250327467918396
Epoch 290, training loss: 0.951484739780426 = 0.3005656599998474 + 0.1 * 6.509190559387207
Epoch 290, val loss: 0.7182542085647583
Epoch 300, training loss: 0.9217718839645386 = 0.27254316210746765 + 0.1 * 6.4922871589660645
Epoch 300, val loss: 0.7150693535804749
Epoch 310, training loss: 0.8946833610534668 = 0.24690282344818115 + 0.1 * 6.477805137634277
Epoch 310, val loss: 0.7147417664527893
Epoch 320, training loss: 0.8716280460357666 = 0.2234714776277542 + 0.1 * 6.481565952301025
Epoch 320, val loss: 0.7168115973472595
Epoch 330, training loss: 0.8485940098762512 = 0.2024155706167221 + 0.1 * 6.4617838859558105
Epoch 330, val loss: 0.7212297320365906
Epoch 340, training loss: 0.8288302421569824 = 0.18361885845661163 + 0.1 * 6.452113628387451
Epoch 340, val loss: 0.7275327444076538
Epoch 350, training loss: 0.8113464117050171 = 0.1669880896806717 + 0.1 * 6.4435834884643555
Epoch 350, val loss: 0.7353678941726685
Epoch 360, training loss: 0.7958691716194153 = 0.1523086279630661 + 0.1 * 6.435605049133301
Epoch 360, val loss: 0.7445141077041626
Epoch 370, training loss: 0.7817205190658569 = 0.13939431309700012 + 0.1 * 6.423262119293213
Epoch 370, val loss: 0.754584789276123
Epoch 380, training loss: 0.769256055355072 = 0.1279572993516922 + 0.1 * 6.41298770904541
Epoch 380, val loss: 0.7652769684791565
Epoch 390, training loss: 0.75910484790802 = 0.11773627251386642 + 0.1 * 6.413685321807861
Epoch 390, val loss: 0.7765218019485474
Epoch 400, training loss: 0.7485532760620117 = 0.108587346971035 + 0.1 * 6.399659156799316
Epoch 400, val loss: 0.7881103754043579
Epoch 410, training loss: 0.7396541833877563 = 0.10035107284784317 + 0.1 * 6.393030643463135
Epoch 410, val loss: 0.7998944520950317
Epoch 420, training loss: 0.7327228784561157 = 0.0928935557603836 + 0.1 * 6.398293495178223
Epoch 420, val loss: 0.8118330240249634
Epoch 430, training loss: 0.7238932847976685 = 0.08615577965974808 + 0.1 * 6.37737512588501
Epoch 430, val loss: 0.8235630989074707
Epoch 440, training loss: 0.7178735733032227 = 0.080033078789711 + 0.1 * 6.3784050941467285
Epoch 440, val loss: 0.8353846669197083
Epoch 450, training loss: 0.7109960317611694 = 0.07445164769887924 + 0.1 * 6.365443706512451
Epoch 450, val loss: 0.8472459316253662
Epoch 460, training loss: 0.7062972187995911 = 0.06935795396566391 + 0.1 * 6.3693928718566895
Epoch 460, val loss: 0.8590006232261658
Epoch 470, training loss: 0.7001138925552368 = 0.06470027565956116 + 0.1 * 6.3541364669799805
Epoch 470, val loss: 0.8707799911499023
Epoch 480, training loss: 0.6968563199043274 = 0.06041517108678818 + 0.1 * 6.364411354064941
Epoch 480, val loss: 0.8825777173042297
Epoch 490, training loss: 0.6907806992530823 = 0.056470394134521484 + 0.1 * 6.343102931976318
Epoch 490, val loss: 0.8943051695823669
Epoch 500, training loss: 0.6865763068199158 = 0.052831631153821945 + 0.1 * 6.337446212768555
Epoch 500, val loss: 0.9060077667236328
Epoch 510, training loss: 0.6831245422363281 = 0.04948115721344948 + 0.1 * 6.3364338874816895
Epoch 510, val loss: 0.9175350069999695
Epoch 520, training loss: 0.6786168813705444 = 0.046409424394369125 + 0.1 * 6.3220744132995605
Epoch 520, val loss: 0.9287979602813721
Epoch 530, training loss: 0.6759268045425415 = 0.04358171299099922 + 0.1 * 6.323450565338135
Epoch 530, val loss: 0.9398931860923767
Epoch 540, training loss: 0.6726288199424744 = 0.04097677767276764 + 0.1 * 6.3165202140808105
Epoch 540, val loss: 0.9507679343223572
Epoch 550, training loss: 0.6703199148178101 = 0.038572657853364944 + 0.1 * 6.317472457885742
Epoch 550, val loss: 0.9614276885986328
Epoch 560, training loss: 0.6672859191894531 = 0.036359645426273346 + 0.1 * 6.309262275695801
Epoch 560, val loss: 0.9717192649841309
Epoch 570, training loss: 0.6651742458343506 = 0.03431801125407219 + 0.1 * 6.3085618019104
Epoch 570, val loss: 0.9818666577339172
Epoch 580, training loss: 0.6620411276817322 = 0.03243051841855049 + 0.1 * 6.296105861663818
Epoch 580, val loss: 0.991709291934967
Epoch 590, training loss: 0.6608901619911194 = 0.030668139457702637 + 0.1 * 6.302220344543457
Epoch 590, val loss: 1.001355528831482
Epoch 600, training loss: 0.6578721404075623 = 0.0290231853723526 + 0.1 * 6.28848934173584
Epoch 600, val loss: 1.010687232017517
Epoch 610, training loss: 0.6556984186172485 = 0.027496183291077614 + 0.1 * 6.282022476196289
Epoch 610, val loss: 1.0198278427124023
Epoch 620, training loss: 0.6549164056777954 = 0.026073969900608063 + 0.1 * 6.288424015045166
Epoch 620, val loss: 1.0287104845046997
Epoch 630, training loss: 0.6530453562736511 = 0.024747608229517937 + 0.1 * 6.282977104187012
Epoch 630, val loss: 1.0374864339828491
Epoch 640, training loss: 0.6515393853187561 = 0.023512138053774834 + 0.1 * 6.280272483825684
Epoch 640, val loss: 1.0459537506103516
Epoch 650, training loss: 0.6496233940124512 = 0.022359464317560196 + 0.1 * 6.27263879776001
Epoch 650, val loss: 1.0543428659439087
Epoch 660, training loss: 0.6501206159591675 = 0.021285219117999077 + 0.1 * 6.28835391998291
Epoch 660, val loss: 1.0623911619186401
Epoch 670, training loss: 0.6462633609771729 = 0.02028750814497471 + 0.1 * 6.259758472442627
Epoch 670, val loss: 1.0703290700912476
Epoch 680, training loss: 0.6458413004875183 = 0.019357608631253242 + 0.1 * 6.26483678817749
Epoch 680, val loss: 1.0781792402267456
Epoch 690, training loss: 0.6437682509422302 = 0.018484992906451225 + 0.1 * 6.252832412719727
Epoch 690, val loss: 1.0857182741165161
Epoch 700, training loss: 0.642457127571106 = 0.017660178244113922 + 0.1 * 6.247969150543213
Epoch 700, val loss: 1.0932456254959106
Epoch 710, training loss: 0.6420422792434692 = 0.01688281260430813 + 0.1 * 6.251594066619873
Epoch 710, val loss: 1.1006087064743042
Epoch 720, training loss: 0.6404526233673096 = 0.016151750460267067 + 0.1 * 6.243008613586426
Epoch 720, val loss: 1.107789158821106
Epoch 730, training loss: 0.6400200724601746 = 0.015464798547327518 + 0.1 * 6.245553016662598
Epoch 730, val loss: 1.1148278713226318
Epoch 740, training loss: 0.6399775147438049 = 0.01482322160154581 + 0.1 * 6.251543045043945
Epoch 740, val loss: 1.121690273284912
Epoch 750, training loss: 0.6388083696365356 = 0.014222965575754642 + 0.1 * 6.245853900909424
Epoch 750, val loss: 1.128517746925354
Epoch 760, training loss: 0.6370349526405334 = 0.013658298179507256 + 0.1 * 6.233766555786133
Epoch 760, val loss: 1.1352062225341797
Epoch 770, training loss: 0.6366910934448242 = 0.013126283884048462 + 0.1 * 6.235648155212402
Epoch 770, val loss: 1.1417465209960938
Epoch 780, training loss: 0.6352218985557556 = 0.012626559473574162 + 0.1 * 6.225953102111816
Epoch 780, val loss: 1.1481597423553467
Epoch 790, training loss: 0.6352118849754333 = 0.012153923511505127 + 0.1 * 6.230579376220703
Epoch 790, val loss: 1.15450918674469
Epoch 800, training loss: 0.6350691914558411 = 0.0117076076567173 + 0.1 * 6.233615875244141
Epoch 800, val loss: 1.160717487335205
Epoch 810, training loss: 0.6343805193901062 = 0.011286604218184948 + 0.1 * 6.230938911437988
Epoch 810, val loss: 1.166745901107788
Epoch 820, training loss: 0.6330317258834839 = 0.010889220051467419 + 0.1 * 6.2214250564575195
Epoch 820, val loss: 1.172752857208252
Epoch 830, training loss: 0.6324108242988586 = 0.010511965490877628 + 0.1 * 6.218987941741943
Epoch 830, val loss: 1.1786549091339111
Epoch 840, training loss: 0.6320217847824097 = 0.01015514973551035 + 0.1 * 6.2186665534973145
Epoch 840, val loss: 1.1844159364700317
Epoch 850, training loss: 0.6322646141052246 = 0.009817060083150864 + 0.1 * 6.224475383758545
Epoch 850, val loss: 1.1900582313537598
Epoch 860, training loss: 0.631385087966919 = 0.009496496990323067 + 0.1 * 6.218885898590088
Epoch 860, val loss: 1.195597529411316
Epoch 870, training loss: 0.6298469305038452 = 0.009193331003189087 + 0.1 * 6.206535816192627
Epoch 870, val loss: 1.201101541519165
Epoch 880, training loss: 0.630268394947052 = 0.008905073627829552 + 0.1 * 6.2136335372924805
Epoch 880, val loss: 1.206556797027588
Epoch 890, training loss: 0.6302676796913147 = 0.008630669675767422 + 0.1 * 6.21636962890625
Epoch 890, val loss: 1.2117749452590942
Epoch 900, training loss: 0.6287230253219604 = 0.00837071891874075 + 0.1 * 6.2035231590271
Epoch 900, val loss: 1.2169687747955322
Epoch 910, training loss: 0.6283371448516846 = 0.0081236707046628 + 0.1 * 6.202134132385254
Epoch 910, val loss: 1.2220964431762695
Epoch 920, training loss: 0.6272763609886169 = 0.007888535037636757 + 0.1 * 6.193877696990967
Epoch 920, val loss: 1.2271877527236938
Epoch 930, training loss: 0.6285462975502014 = 0.007664128672331572 + 0.1 * 6.2088212966918945
Epoch 930, val loss: 1.2321888208389282
Epoch 940, training loss: 0.6270299553871155 = 0.007449035532772541 + 0.1 * 6.195809364318848
Epoch 940, val loss: 1.2369643449783325
Epoch 950, training loss: 0.6264519691467285 = 0.007244989275932312 + 0.1 * 6.1920695304870605
Epoch 950, val loss: 1.241811752319336
Epoch 960, training loss: 0.6279352903366089 = 0.007049239240586758 + 0.1 * 6.208860397338867
Epoch 960, val loss: 1.2465325593948364
Epoch 970, training loss: 0.6255639791488647 = 0.006862074602395296 + 0.1 * 6.187018394470215
Epoch 970, val loss: 1.251123070716858
Epoch 980, training loss: 0.6248350143432617 = 0.00668306415900588 + 0.1 * 6.181519508361816
Epoch 980, val loss: 1.2557164430618286
Epoch 990, training loss: 0.6265543103218079 = 0.006511323153972626 + 0.1 * 6.200429916381836
Epoch 990, val loss: 1.2601954936981201
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8155
Flip ASR: 0.7778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.773137331008911 = 1.935748815536499 + 0.1 * 8.373884201049805
Epoch 0, val loss: 1.9297561645507812
Epoch 10, training loss: 2.7633843421936035 = 1.9260072708129883 + 0.1 * 8.373769760131836
Epoch 10, val loss: 1.9196221828460693
Epoch 20, training loss: 2.751708507537842 = 1.9144057035446167 + 0.1 * 8.373026847839355
Epoch 20, val loss: 1.907399296760559
Epoch 30, training loss: 2.735201120376587 = 1.898545503616333 + 0.1 * 8.366556167602539
Epoch 30, val loss: 1.890767216682434
Epoch 40, training loss: 2.7074344158172607 = 1.8754169940948486 + 0.1 * 8.320174217224121
Epoch 40, val loss: 1.8669594526290894
Epoch 50, training loss: 2.6461381912231445 = 1.8439724445343018 + 0.1 * 8.021657943725586
Epoch 50, val loss: 1.8360693454742432
Epoch 60, training loss: 2.5694940090179443 = 1.807957410812378 + 0.1 * 7.615366458892822
Epoch 60, val loss: 1.80311918258667
Epoch 70, training loss: 2.4870078563690186 = 1.7715750932693481 + 0.1 * 7.154327392578125
Epoch 70, val loss: 1.771487832069397
Epoch 80, training loss: 2.421473741531372 = 1.7308064699172974 + 0.1 * 6.90667200088501
Epoch 80, val loss: 1.7370359897613525
Epoch 90, training loss: 2.358393430709839 = 1.6789581775665283 + 0.1 * 6.794352054595947
Epoch 90, val loss: 1.6936849355697632
Epoch 100, training loss: 2.2851860523223877 = 1.6102913618087769 + 0.1 * 6.748946666717529
Epoch 100, val loss: 1.6361347436904907
Epoch 110, training loss: 2.196136236190796 = 1.523235559463501 + 0.1 * 6.729006767272949
Epoch 110, val loss: 1.5641785860061646
Epoch 120, training loss: 2.0913562774658203 = 1.4199368953704834 + 0.1 * 6.714193344116211
Epoch 120, val loss: 1.480836033821106
Epoch 130, training loss: 1.9768052101135254 = 1.3061548471450806 + 0.1 * 6.706502914428711
Epoch 130, val loss: 1.3899019956588745
Epoch 140, training loss: 1.8594332933425903 = 1.1893606185913086 + 0.1 * 6.700726509094238
Epoch 140, val loss: 1.2982330322265625
Epoch 150, training loss: 1.7458558082580566 = 1.0763086080551147 + 0.1 * 6.69547176361084
Epoch 150, val loss: 1.2113064527511597
Epoch 160, training loss: 1.6409286260604858 = 0.9720067977905273 + 0.1 * 6.689218044281006
Epoch 160, val loss: 1.132500171661377
Epoch 170, training loss: 1.5466065406799316 = 0.8783981800079346 + 0.1 * 6.682084083557129
Epoch 170, val loss: 1.0633784532546997
Epoch 180, training loss: 1.461845874786377 = 0.7944939136505127 + 0.1 * 6.673519611358643
Epoch 180, val loss: 1.0026496648788452
Epoch 190, training loss: 1.3843461275100708 = 0.7180284261703491 + 0.1 * 6.663177013397217
Epoch 190, val loss: 0.9484198689460754
Epoch 200, training loss: 1.313089370727539 = 0.6478076577186584 + 0.1 * 6.652816295623779
Epoch 200, val loss: 0.9001656174659729
Epoch 210, training loss: 1.2471661567687988 = 0.5829594731330872 + 0.1 * 6.642066955566406
Epoch 210, val loss: 0.8575151562690735
Epoch 220, training loss: 1.1865379810333252 = 0.5231863856315613 + 0.1 * 6.633516311645508
Epoch 220, val loss: 0.8207253217697144
Epoch 230, training loss: 1.1301558017730713 = 0.4675493538379669 + 0.1 * 6.626064300537109
Epoch 230, val loss: 0.7891010046005249
Epoch 240, training loss: 1.0778459310531616 = 0.4157622754573822 + 0.1 * 6.6208367347717285
Epoch 240, val loss: 0.7624130249023438
Epoch 250, training loss: 1.0294520854949951 = 0.3680061995983124 + 0.1 * 6.614459037780762
Epoch 250, val loss: 0.740486741065979
Epoch 260, training loss: 0.984932005405426 = 0.32439351081848145 + 0.1 * 6.605384826660156
Epoch 260, val loss: 0.7226880788803101
Epoch 270, training loss: 0.9464660286903381 = 0.285179078578949 + 0.1 * 6.6128692626953125
Epoch 270, val loss: 0.7089220285415649
Epoch 280, training loss: 0.9104735851287842 = 0.2508646547794342 + 0.1 * 6.596088886260986
Epoch 280, val loss: 0.6989759802818298
Epoch 290, training loss: 0.8799459934234619 = 0.22092178463935852 + 0.1 * 6.5902419090271
Epoch 290, val loss: 0.692317008972168
Epoch 300, training loss: 0.8537939786911011 = 0.1949322670698166 + 0.1 * 6.588616847991943
Epoch 300, val loss: 0.6886472105979919
Epoch 310, training loss: 0.8308778405189514 = 0.17260026931762695 + 0.1 * 6.582775592803955
Epoch 310, val loss: 0.6875706911087036
Epoch 320, training loss: 0.8101086616516113 = 0.1533280313014984 + 0.1 * 6.567806720733643
Epoch 320, val loss: 0.6886953115463257
Epoch 330, training loss: 0.7944005727767944 = 0.13662365078926086 + 0.1 * 6.577768802642822
Epoch 330, val loss: 0.6916377544403076
Epoch 340, training loss: 0.7778775691986084 = 0.1222018152475357 + 0.1 * 6.55675745010376
Epoch 340, val loss: 0.6959841251373291
Epoch 350, training loss: 0.7641609907150269 = 0.10962068289518356 + 0.1 * 6.545403003692627
Epoch 350, val loss: 0.7016218304634094
Epoch 360, training loss: 0.7516565322875977 = 0.09861928969621658 + 0.1 * 6.530372142791748
Epoch 360, val loss: 0.7081741094589233
Epoch 370, training loss: 0.7423214912414551 = 0.08898314833641052 + 0.1 * 6.533382892608643
Epoch 370, val loss: 0.7154608964920044
Epoch 380, training loss: 0.7316837906837463 = 0.08054529130458832 + 0.1 * 6.511384963989258
Epoch 380, val loss: 0.7233555316925049
Epoch 390, training loss: 0.7231478095054626 = 0.07312034070491791 + 0.1 * 6.500274658203125
Epoch 390, val loss: 0.7316115498542786
Epoch 400, training loss: 0.7170288562774658 = 0.06658416241407394 + 0.1 * 6.504446983337402
Epoch 400, val loss: 0.7402452826499939
Epoch 410, training loss: 0.708743155002594 = 0.06081351265311241 + 0.1 * 6.479296684265137
Epoch 410, val loss: 0.7489928603172302
Epoch 420, training loss: 0.7046957612037659 = 0.05570787563920021 + 0.1 * 6.4898786544799805
Epoch 420, val loss: 0.7578164339065552
Epoch 430, training loss: 0.6971704363822937 = 0.05119869485497475 + 0.1 * 6.459717273712158
Epoch 430, val loss: 0.7665383815765381
Epoch 440, training loss: 0.6925392150878906 = 0.04717376455664635 + 0.1 * 6.453654766082764
Epoch 440, val loss: 0.7754086256027222
Epoch 450, training loss: 0.6880304217338562 = 0.043564293533563614 + 0.1 * 6.4446611404418945
Epoch 450, val loss: 0.7842019200325012
Epoch 460, training loss: 0.6848615407943726 = 0.04033348336815834 + 0.1 * 6.445280075073242
Epoch 460, val loss: 0.7928723096847534
Epoch 470, training loss: 0.6802847385406494 = 0.03744695335626602 + 0.1 * 6.428377628326416
Epoch 470, val loss: 0.8014097213745117
Epoch 480, training loss: 0.6771671772003174 = 0.034848857671022415 + 0.1 * 6.423182964324951
Epoch 480, val loss: 0.8098866939544678
Epoch 490, training loss: 0.6737326383590698 = 0.032505277544260025 + 0.1 * 6.412273406982422
Epoch 490, val loss: 0.8181585669517517
Epoch 500, training loss: 0.6715033054351807 = 0.03039676509797573 + 0.1 * 6.411065578460693
Epoch 500, val loss: 0.8261770009994507
Epoch 510, training loss: 0.6692213416099548 = 0.028489571064710617 + 0.1 * 6.407317161560059
Epoch 510, val loss: 0.8341503143310547
Epoch 520, training loss: 0.6667622327804565 = 0.026755739003419876 + 0.1 * 6.400064468383789
Epoch 520, val loss: 0.8419110774993896
Epoch 530, training loss: 0.66483473777771 = 0.02517751045525074 + 0.1 * 6.396572113037109
Epoch 530, val loss: 0.8495616316795349
Epoch 540, training loss: 0.6625540256500244 = 0.023739317432045937 + 0.1 * 6.388146877288818
Epoch 540, val loss: 0.856890857219696
Epoch 550, training loss: 0.6602506637573242 = 0.022425608709454536 + 0.1 * 6.378250598907471
Epoch 550, val loss: 0.8642916679382324
Epoch 560, training loss: 0.658984363079071 = 0.021220093593001366 + 0.1 * 6.377642631530762
Epoch 560, val loss: 0.8713532090187073
Epoch 570, training loss: 0.6558141708374023 = 0.020114468410611153 + 0.1 * 6.356997013092041
Epoch 570, val loss: 0.8783298134803772
Epoch 580, training loss: 0.6550537347793579 = 0.01909501850605011 + 0.1 * 6.3595871925354
Epoch 580, val loss: 0.8852159380912781
Epoch 590, training loss: 0.6538543105125427 = 0.018154138699173927 + 0.1 * 6.357001304626465
Epoch 590, val loss: 0.8917792439460754
Epoch 600, training loss: 0.6518009305000305 = 0.017287131398916245 + 0.1 * 6.345138072967529
Epoch 600, val loss: 0.8982424139976501
Epoch 610, training loss: 0.64996737241745 = 0.0164861548691988 + 0.1 * 6.334812164306641
Epoch 610, val loss: 0.904649555683136
Epoch 620, training loss: 0.6503193974494934 = 0.015740500763058662 + 0.1 * 6.345788478851318
Epoch 620, val loss: 0.9109828472137451
Epoch 630, training loss: 0.6485949754714966 = 0.01504677627235651 + 0.1 * 6.335482120513916
Epoch 630, val loss: 0.9170224070549011
Epoch 640, training loss: 0.6474601030349731 = 0.014400295913219452 + 0.1 * 6.330597877502441
Epoch 640, val loss: 0.9230567216873169
Epoch 650, training loss: 0.647999107837677 = 0.0137980617582798 + 0.1 * 6.342010498046875
Epoch 650, val loss: 0.9289758205413818
Epoch 660, training loss: 0.6450363993644714 = 0.013236280530691147 + 0.1 * 6.318000793457031
Epoch 660, val loss: 0.9346326589584351
Epoch 670, training loss: 0.6435377597808838 = 0.01271036546677351 + 0.1 * 6.3082733154296875
Epoch 670, val loss: 0.940406322479248
Epoch 680, training loss: 0.6423748135566711 = 0.012216172181069851 + 0.1 * 6.301586627960205
Epoch 680, val loss: 0.9459196329116821
Epoch 690, training loss: 0.6430712342262268 = 0.011751812882721424 + 0.1 * 6.3131937980651855
Epoch 690, val loss: 0.9514285326004028
Epoch 700, training loss: 0.6410110592842102 = 0.011316658928990364 + 0.1 * 6.2969441413879395
Epoch 700, val loss: 0.9567437171936035
Epoch 710, training loss: 0.6405130624771118 = 0.010906663723289967 + 0.1 * 6.2960638999938965
Epoch 710, val loss: 0.9620628356933594
Epoch 720, training loss: 0.6407468914985657 = 0.01051984541118145 + 0.1 * 6.30226993560791
Epoch 720, val loss: 0.9672287702560425
Epoch 730, training loss: 0.638817548751831 = 0.010156414471566677 + 0.1 * 6.286611557006836
Epoch 730, val loss: 0.9722056984901428
Epoch 740, training loss: 0.6383103132247925 = 0.009813925251364708 + 0.1 * 6.284964084625244
Epoch 740, val loss: 0.977297306060791
Epoch 750, training loss: 0.6379393339157104 = 0.009489080868661404 + 0.1 * 6.2845025062561035
Epoch 750, val loss: 0.982144832611084
Epoch 760, training loss: 0.636117696762085 = 0.009181173518300056 + 0.1 * 6.269364833831787
Epoch 760, val loss: 0.9870621562004089
Epoch 770, training loss: 0.6368733048439026 = 0.008889020420610905 + 0.1 * 6.279842853546143
Epoch 770, val loss: 0.9916324019432068
Epoch 780, training loss: 0.635625958442688 = 0.008613526821136475 + 0.1 * 6.2701239585876465
Epoch 780, val loss: 0.9963362812995911
Epoch 790, training loss: 0.6341160535812378 = 0.008352065458893776 + 0.1 * 6.2576398849487305
Epoch 790, val loss: 1.0009816884994507
Epoch 800, training loss: 0.634181559085846 = 0.008102517575025558 + 0.1 * 6.2607903480529785
Epoch 800, val loss: 1.0053726434707642
Epoch 810, training loss: 0.6337167024612427 = 0.007865667343139648 + 0.1 * 6.258510112762451
Epoch 810, val loss: 1.0097578763961792
Epoch 820, training loss: 0.6334088444709778 = 0.007640406489372253 + 0.1 * 6.257684230804443
Epoch 820, val loss: 1.0141656398773193
Epoch 830, training loss: 0.6329457759857178 = 0.0074259936809539795 + 0.1 * 6.255197525024414
Epoch 830, val loss: 1.0184383392333984
Epoch 840, training loss: 0.631214439868927 = 0.007221028208732605 + 0.1 * 6.239933967590332
Epoch 840, val loss: 1.0226587057113647
Epoch 850, training loss: 0.6355229020118713 = 0.00702501880005002 + 0.1 * 6.284978866577148
Epoch 850, val loss: 1.0268136262893677
Epoch 860, training loss: 0.6315815448760986 = 0.006838241592049599 + 0.1 * 6.247432708740234
Epoch 860, val loss: 1.0308146476745605
Epoch 870, training loss: 0.6301979422569275 = 0.006660249084234238 + 0.1 * 6.235377311706543
Epoch 870, val loss: 1.034980297088623
Epoch 880, training loss: 0.6303333044052124 = 0.00648958096280694 + 0.1 * 6.238437175750732
Epoch 880, val loss: 1.038988709449768
Epoch 890, training loss: 0.6297603845596313 = 0.006325519643723965 + 0.1 * 6.234348297119141
Epoch 890, val loss: 1.0427137613296509
Epoch 900, training loss: 0.6298485994338989 = 0.006169569678604603 + 0.1 * 6.236790180206299
Epoch 900, val loss: 1.0466388463974
Epoch 910, training loss: 0.6285942792892456 = 0.006019672378897667 + 0.1 * 6.225746154785156
Epoch 910, val loss: 1.0504459142684937
Epoch 920, training loss: 0.6300047039985657 = 0.005875553470104933 + 0.1 * 6.241291522979736
Epoch 920, val loss: 1.0541818141937256
Epoch 930, training loss: 0.6272013783454895 = 0.0057374234311282635 + 0.1 * 6.214639186859131
Epoch 930, val loss: 1.0578734874725342
Epoch 940, training loss: 0.6266757845878601 = 0.005604953039437532 + 0.1 * 6.210707664489746
Epoch 940, val loss: 1.0615800619125366
Epoch 950, training loss: 0.6289258003234863 = 0.005477221682667732 + 0.1 * 6.234485626220703
Epoch 950, val loss: 1.0651466846466064
Epoch 960, training loss: 0.6267279982566833 = 0.005354057531803846 + 0.1 * 6.213739395141602
Epoch 960, val loss: 1.068595290184021
Epoch 970, training loss: 0.6265687346458435 = 0.005236792843788862 + 0.1 * 6.213319301605225
Epoch 970, val loss: 1.0721123218536377
Epoch 980, training loss: 0.625689685344696 = 0.0051236399449408054 + 0.1 * 6.205660343170166
Epoch 980, val loss: 1.0755823850631714
Epoch 990, training loss: 0.6265807151794434 = 0.005014156457036734 + 0.1 * 6.215665340423584
Epoch 990, val loss: 1.0789872407913208
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9041
Flip ASR: 0.8844/225 nodes
The final ASR:0.79213, 0.10228, Accuracy:0.80864, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9558])
updated graph: torch.Size([2, 10610])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.780306816101074 = 1.9429188966751099 + 0.1 * 8.373878479003906
Epoch 0, val loss: 1.9448769092559814
Epoch 10, training loss: 2.7702693939208984 = 1.932895541191101 + 0.1 * 8.373738288879395
Epoch 10, val loss: 1.9348716735839844
Epoch 20, training loss: 2.7577719688415527 = 1.9205094575881958 + 0.1 * 8.372624397277832
Epoch 20, val loss: 1.9223777055740356
Epoch 30, training loss: 2.739316940307617 = 1.9030578136444092 + 0.1 * 8.362590789794922
Epoch 30, val loss: 1.9047901630401611
Epoch 40, training loss: 2.7056937217712402 = 1.8772104978561401 + 0.1 * 8.284832000732422
Epoch 40, val loss: 1.8790178298950195
Epoch 50, training loss: 2.6297473907470703 = 1.842810034751892 + 0.1 * 7.869373798370361
Epoch 50, val loss: 1.8462618589401245
Epoch 60, training loss: 2.5686025619506836 = 1.8061672449111938 + 0.1 * 7.62435245513916
Epoch 60, val loss: 1.8135201930999756
Epoch 70, training loss: 2.492891311645508 = 1.7722806930541992 + 0.1 * 7.206106185913086
Epoch 70, val loss: 1.7849866151809692
Epoch 80, training loss: 2.42966628074646 = 1.7382407188415527 + 0.1 * 6.914255619049072
Epoch 80, val loss: 1.7564448118209839
Epoch 90, training loss: 2.3719582557678223 = 1.6929363012313843 + 0.1 * 6.790220737457275
Epoch 90, val loss: 1.7177499532699585
Epoch 100, training loss: 2.302499294281006 = 1.6313297748565674 + 0.1 * 6.711694240570068
Epoch 100, val loss: 1.6660804748535156
Epoch 110, training loss: 2.2185487747192383 = 1.5527540445327759 + 0.1 * 6.657947063446045
Epoch 110, val loss: 1.6013942956924438
Epoch 120, training loss: 2.121910810470581 = 1.4602880477905273 + 0.1 * 6.616226673126221
Epoch 120, val loss: 1.5261162519454956
Epoch 130, training loss: 2.0204319953918457 = 1.3614362478256226 + 0.1 * 6.589958667755127
Epoch 130, val loss: 1.4471328258514404
Epoch 140, training loss: 1.9173188209533691 = 1.2604072093963623 + 0.1 * 6.569116115570068
Epoch 140, val loss: 1.3671133518218994
Epoch 150, training loss: 1.8162808418273926 = 1.1602333784103394 + 0.1 * 6.5604753494262695
Epoch 150, val loss: 1.2886840105056763
Epoch 160, training loss: 1.7209832668304443 = 1.0667948722839355 + 0.1 * 6.54188346862793
Epoch 160, val loss: 1.2168047428131104
Epoch 170, training loss: 1.6346378326416016 = 0.9815914034843445 + 0.1 * 6.5304646492004395
Epoch 170, val loss: 1.1516649723052979
Epoch 180, training loss: 1.5579125881195068 = 0.9059361219406128 + 0.1 * 6.519763946533203
Epoch 180, val loss: 1.0948482751846313
Epoch 190, training loss: 1.4869194030761719 = 0.8363767862319946 + 0.1 * 6.505425930023193
Epoch 190, val loss: 1.0430774688720703
Epoch 200, training loss: 1.419118881225586 = 0.7698127627372742 + 0.1 * 6.49306058883667
Epoch 200, val loss: 0.9935749173164368
Epoch 210, training loss: 1.3558082580566406 = 0.7056045532226562 + 0.1 * 6.5020365715026855
Epoch 210, val loss: 0.9461231827735901
Epoch 220, training loss: 1.2935810089111328 = 0.6460337042808533 + 0.1 * 6.475472450256348
Epoch 220, val loss: 0.9029064774513245
Epoch 230, training loss: 1.237779974937439 = 0.591951310634613 + 0.1 * 6.458286762237549
Epoch 230, val loss: 0.8652567267417908
Epoch 240, training loss: 1.1900169849395752 = 0.5442349910736084 + 0.1 * 6.457819938659668
Epoch 240, val loss: 0.8342525959014893
Epoch 250, training loss: 1.1473321914672852 = 0.5033042430877686 + 0.1 * 6.440279483795166
Epoch 250, val loss: 0.8103575110435486
Epoch 260, training loss: 1.1100049018859863 = 0.46761614084243774 + 0.1 * 6.423886775970459
Epoch 260, val loss: 0.7920694351196289
Epoch 270, training loss: 1.0780065059661865 = 0.4355596601963043 + 0.1 * 6.424468040466309
Epoch 270, val loss: 0.777912974357605
Epoch 280, training loss: 1.046478509902954 = 0.4058949649333954 + 0.1 * 6.405835151672363
Epoch 280, val loss: 0.7665433287620544
Epoch 290, training loss: 1.0174145698547363 = 0.3773379921913147 + 0.1 * 6.400765419006348
Epoch 290, val loss: 0.7568466067314148
Epoch 300, training loss: 0.9879776835441589 = 0.3491792678833008 + 0.1 * 6.387983798980713
Epoch 300, val loss: 0.7481635808944702
Epoch 310, training loss: 0.9589220285415649 = 0.3207341432571411 + 0.1 * 6.381878852844238
Epoch 310, val loss: 0.73996502161026
Epoch 320, training loss: 0.9299745559692383 = 0.2920459508895874 + 0.1 * 6.37928581237793
Epoch 320, val loss: 0.7323330640792847
Epoch 330, training loss: 0.9009835124015808 = 0.2637394070625305 + 0.1 * 6.372440814971924
Epoch 330, val loss: 0.7254136800765991
Epoch 340, training loss: 0.8741791844367981 = 0.23629818856716156 + 0.1 * 6.378809452056885
Epoch 340, val loss: 0.7194287180900574
Epoch 350, training loss: 0.8467617630958557 = 0.21055513620376587 + 0.1 * 6.362066268920898
Epoch 350, val loss: 0.7146598696708679
Epoch 360, training loss: 0.8224031329154968 = 0.1870250105857849 + 0.1 * 6.353781223297119
Epoch 360, val loss: 0.71133953332901
Epoch 370, training loss: 0.803460419178009 = 0.1660441756248474 + 0.1 * 6.374162197113037
Epoch 370, val loss: 0.709747850894928
Epoch 380, training loss: 0.782766580581665 = 0.14784611761569977 + 0.1 * 6.3492045402526855
Epoch 380, val loss: 0.7098395824432373
Epoch 390, training loss: 0.7657591104507446 = 0.1321519911289215 + 0.1 * 6.336071491241455
Epoch 390, val loss: 0.7116407155990601
Epoch 400, training loss: 0.752089262008667 = 0.1186719760298729 + 0.1 * 6.33417272567749
Epoch 400, val loss: 0.7149474620819092
Epoch 410, training loss: 0.7398349046707153 = 0.10713621973991394 + 0.1 * 6.326986312866211
Epoch 410, val loss: 0.7195981740951538
Epoch 420, training loss: 0.729301393032074 = 0.0971665233373642 + 0.1 * 6.321348667144775
Epoch 420, val loss: 0.7252369523048401
Epoch 430, training loss: 0.7207657098770142 = 0.08850305527448654 + 0.1 * 6.32262659072876
Epoch 430, val loss: 0.7316355109214783
Epoch 440, training loss: 0.7124229669570923 = 0.08095019310712814 + 0.1 * 6.314727306365967
Epoch 440, val loss: 0.7386014461517334
Epoch 450, training loss: 0.705210268497467 = 0.07430914789438248 + 0.1 * 6.309010982513428
Epoch 450, val loss: 0.7459450960159302
Epoch 460, training loss: 0.6988943219184875 = 0.06843971461057663 + 0.1 * 6.304545879364014
Epoch 460, val loss: 0.7535763382911682
Epoch 470, training loss: 0.6942797303199768 = 0.06320415437221527 + 0.1 * 6.310755729675293
Epoch 470, val loss: 0.7613996863365173
Epoch 480, training loss: 0.6891568303108215 = 0.05852792039513588 + 0.1 * 6.306288719177246
Epoch 480, val loss: 0.7692990303039551
Epoch 490, training loss: 0.6830978393554688 = 0.054324448108673096 + 0.1 * 6.287734031677246
Epoch 490, val loss: 0.777282178401947
Epoch 500, training loss: 0.6802294850349426 = 0.05051988735795021 + 0.1 * 6.297095775604248
Epoch 500, val loss: 0.7852600812911987
Epoch 510, training loss: 0.6759581565856934 = 0.0470748245716095 + 0.1 * 6.288832664489746
Epoch 510, val loss: 0.7932347059249878
Epoch 520, training loss: 0.6717571020126343 = 0.043939489871263504 + 0.1 * 6.278176307678223
Epoch 520, val loss: 0.8011878728866577
Epoch 530, training loss: 0.6688317656517029 = 0.041068580001592636 + 0.1 * 6.277631759643555
Epoch 530, val loss: 0.8091385960578918
Epoch 540, training loss: 0.6666816473007202 = 0.03844446688890457 + 0.1 * 6.282371997833252
Epoch 540, val loss: 0.8169087171554565
Epoch 550, training loss: 0.6627135276794434 = 0.036040615290403366 + 0.1 * 6.26672887802124
Epoch 550, val loss: 0.8247472047805786
Epoch 560, training loss: 0.6605189442634583 = 0.03383321315050125 + 0.1 * 6.266857147216797
Epoch 560, val loss: 0.8324425220489502
Epoch 570, training loss: 0.6571646928787231 = 0.03180595859885216 + 0.1 * 6.25358772277832
Epoch 570, val loss: 0.8400852680206299
Epoch 580, training loss: 0.6562620997428894 = 0.029937701299786568 + 0.1 * 6.263244152069092
Epoch 580, val loss: 0.8476737141609192
Epoch 590, training loss: 0.6541736721992493 = 0.028217654675245285 + 0.1 * 6.259559631347656
Epoch 590, val loss: 0.8551660180091858
Epoch 600, training loss: 0.6521270275115967 = 0.026634179055690765 + 0.1 * 6.2549285888671875
Epoch 600, val loss: 0.8626092672348022
Epoch 610, training loss: 0.6500334143638611 = 0.025173258036375046 + 0.1 * 6.24860143661499
Epoch 610, val loss: 0.8699455857276917
Epoch 620, training loss: 0.6480676531791687 = 0.02382132038474083 + 0.1 * 6.2424635887146
Epoch 620, val loss: 0.877233624458313
Epoch 630, training loss: 0.6471466422080994 = 0.022570960223674774 + 0.1 * 6.24575662612915
Epoch 630, val loss: 0.8843449354171753
Epoch 640, training loss: 0.6459531784057617 = 0.02141747437417507 + 0.1 * 6.245356559753418
Epoch 640, val loss: 0.8913801312446594
Epoch 650, training loss: 0.6441391706466675 = 0.02034994401037693 + 0.1 * 6.237892150878906
Epoch 650, val loss: 0.8983138203620911
Epoch 660, training loss: 0.6440737843513489 = 0.01935778558254242 + 0.1 * 6.247159957885742
Epoch 660, val loss: 0.9050765633583069
Epoch 670, training loss: 0.6421492099761963 = 0.01843653805553913 + 0.1 * 6.237126350402832
Epoch 670, val loss: 0.911735475063324
Epoch 680, training loss: 0.6410501003265381 = 0.017578398808836937 + 0.1 * 6.234716415405273
Epoch 680, val loss: 0.9183579087257385
Epoch 690, training loss: 0.6402294635772705 = 0.016779206693172455 + 0.1 * 6.234502792358398
Epoch 690, val loss: 0.9247543215751648
Epoch 700, training loss: 0.639028787612915 = 0.01603519171476364 + 0.1 * 6.229935646057129
Epoch 700, val loss: 0.9311228394508362
Epoch 710, training loss: 0.6384118795394897 = 0.015339547768235207 + 0.1 * 6.230722904205322
Epoch 710, val loss: 0.9373359680175781
Epoch 720, training loss: 0.6365371942520142 = 0.014689958654344082 + 0.1 * 6.218472003936768
Epoch 720, val loss: 0.9434748888015747
Epoch 730, training loss: 0.638038694858551 = 0.014081581495702267 + 0.1 * 6.239570617675781
Epoch 730, val loss: 0.9494917988777161
Epoch 740, training loss: 0.6356320381164551 = 0.013512786477804184 + 0.1 * 6.221192359924316
Epoch 740, val loss: 0.9552907347679138
Epoch 750, training loss: 0.6337246894836426 = 0.01297935750335455 + 0.1 * 6.207453727722168
Epoch 750, val loss: 0.9610777497291565
Epoch 760, training loss: 0.6352573037147522 = 0.012477586977183819 + 0.1 * 6.227797031402588
Epoch 760, val loss: 0.966722846031189
Epoch 770, training loss: 0.6331377625465393 = 0.012006005272269249 + 0.1 * 6.211317539215088
Epoch 770, val loss: 0.9722455143928528
Epoch 780, training loss: 0.633607804775238 = 0.01156136766076088 + 0.1 * 6.22046422958374
Epoch 780, val loss: 0.9777024984359741
Epoch 790, training loss: 0.6321826577186584 = 0.011142611503601074 + 0.1 * 6.210400104522705
Epoch 790, val loss: 0.9829965233802795
Epoch 800, training loss: 0.6312832832336426 = 0.010748211294412613 + 0.1 * 6.205350875854492
Epoch 800, val loss: 0.9882436990737915
Epoch 810, training loss: 0.6307183504104614 = 0.010375532321631908 + 0.1 * 6.203427791595459
Epoch 810, val loss: 0.9933663606643677
Epoch 820, training loss: 0.63084477186203 = 0.010022832080721855 + 0.1 * 6.208219528198242
Epoch 820, val loss: 0.9983805418014526
Epoch 830, training loss: 0.6292644143104553 = 0.009689156897366047 + 0.1 * 6.1957526206970215
Epoch 830, val loss: 1.003312110900879
Epoch 840, training loss: 0.6293508410453796 = 0.009373249486088753 + 0.1 * 6.199775695800781
Epoch 840, val loss: 1.0081641674041748
Epoch 850, training loss: 0.6284500956535339 = 0.009073500521481037 + 0.1 * 6.193765640258789
Epoch 850, val loss: 1.0129327774047852
Epoch 860, training loss: 0.6297712922096252 = 0.008789400570094585 + 0.1 * 6.2098188400268555
Epoch 860, val loss: 1.017601728439331
Epoch 870, training loss: 0.6272968053817749 = 0.008519531227648258 + 0.1 * 6.187772750854492
Epoch 870, val loss: 1.022157907485962
Epoch 880, training loss: 0.6285607814788818 = 0.008262936025857925 + 0.1 * 6.202978610992432
Epoch 880, val loss: 1.0267038345336914
Epoch 890, training loss: 0.6267480850219727 = 0.008018981665372849 + 0.1 * 6.187291145324707
Epoch 890, val loss: 1.0310739278793335
Epoch 900, training loss: 0.6258012056350708 = 0.007786593399941921 + 0.1 * 6.180146217346191
Epoch 900, val loss: 1.0354918241500854
Epoch 910, training loss: 0.6270658373832703 = 0.007564629428088665 + 0.1 * 6.195012092590332
Epoch 910, val loss: 1.0397939682006836
Epoch 920, training loss: 0.6257874965667725 = 0.007353365886956453 + 0.1 * 6.184340953826904
Epoch 920, val loss: 1.0439525842666626
Epoch 930, training loss: 0.6248242259025574 = 0.00715209124609828 + 0.1 * 6.176721572875977
Epoch 930, val loss: 1.0481007099151611
Epoch 940, training loss: 0.6258594393730164 = 0.006959648337215185 + 0.1 * 6.188997745513916
Epoch 940, val loss: 1.0522079467773438
Epoch 950, training loss: 0.6248348951339722 = 0.006775563582777977 + 0.1 * 6.180593013763428
Epoch 950, val loss: 1.0561599731445312
Epoch 960, training loss: 0.6241869330406189 = 0.006599729880690575 + 0.1 * 6.175871849060059
Epoch 960, val loss: 1.0600626468658447
Epoch 970, training loss: 0.6231289505958557 = 0.006431246642023325 + 0.1 * 6.1669769287109375
Epoch 970, val loss: 1.0639550685882568
Epoch 980, training loss: 0.6242341995239258 = 0.006269508507102728 + 0.1 * 6.179646968841553
Epoch 980, val loss: 1.067750334739685
Epoch 990, training loss: 0.6226696968078613 = 0.0061147622764110565 + 0.1 * 6.165549278259277
Epoch 990, val loss: 1.0714802742004395
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5203
Flip ASR: 0.4267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7823679447174072 = 1.9449758529663086 + 0.1 * 8.373920440673828
Epoch 0, val loss: 1.9360954761505127
Epoch 10, training loss: 2.77197265625 = 1.934590220451355 + 0.1 * 8.373824119567871
Epoch 10, val loss: 1.9259008169174194
Epoch 20, training loss: 2.759004592895508 = 1.9216742515563965 + 0.1 * 8.37330436706543
Epoch 20, val loss: 1.9128119945526123
Epoch 30, training loss: 2.7405049800872803 = 1.9035977125167847 + 0.1 * 8.369072914123535
Epoch 30, val loss: 1.8942184448242188
Epoch 40, training loss: 2.7106244564056396 = 1.8770253658294678 + 0.1 * 8.335989952087402
Epoch 40, val loss: 1.866923213005066
Epoch 50, training loss: 2.652784824371338 = 1.8404440879821777 + 0.1 * 8.123408317565918
Epoch 50, val loss: 1.8308312892913818
Epoch 60, training loss: 2.5722885131835938 = 1.800668716430664 + 0.1 * 7.716196537017822
Epoch 60, val loss: 1.7944426536560059
Epoch 70, training loss: 2.4913601875305176 = 1.7632782459259033 + 0.1 * 7.280818462371826
Epoch 70, val loss: 1.7617038488388062
Epoch 80, training loss: 2.4227280616760254 = 1.721464991569519 + 0.1 * 7.012630939483643
Epoch 80, val loss: 1.7245529890060425
Epoch 90, training loss: 2.357194185256958 = 1.6678234338760376 + 0.1 * 6.893707752227783
Epoch 90, val loss: 1.6774847507476807
Epoch 100, training loss: 2.279534339904785 = 1.596535325050354 + 0.1 * 6.829991340637207
Epoch 100, val loss: 1.6157177686691284
Epoch 110, training loss: 2.186206817626953 = 1.507137417793274 + 0.1 * 6.7906928062438965
Epoch 110, val loss: 1.5397459268569946
Epoch 120, training loss: 2.0838029384613037 = 1.4074698686599731 + 0.1 * 6.763329982757568
Epoch 120, val loss: 1.457642912864685
Epoch 130, training loss: 1.9810295104980469 = 1.3070420026779175 + 0.1 * 6.739874839782715
Epoch 130, val loss: 1.3776345252990723
Epoch 140, training loss: 1.8827035427093506 = 1.2109302282333374 + 0.1 * 6.717732906341553
Epoch 140, val loss: 1.3043427467346191
Epoch 150, training loss: 1.7892584800720215 = 1.1192854642868042 + 0.1 * 6.69973087310791
Epoch 150, val loss: 1.235611081123352
Epoch 160, training loss: 1.6995892524719238 = 1.0307650566101074 + 0.1 * 6.688241958618164
Epoch 160, val loss: 1.1693618297576904
Epoch 170, training loss: 1.6124155521392822 = 0.9448854923248291 + 0.1 * 6.675300598144531
Epoch 170, val loss: 1.1052495241165161
Epoch 180, training loss: 1.5289443731307983 = 0.8623952269554138 + 0.1 * 6.665491104125977
Epoch 180, val loss: 1.0440253019332886
Epoch 190, training loss: 1.451215147972107 = 0.7859876155853271 + 0.1 * 6.652275085449219
Epoch 190, val loss: 0.9879105091094971
Epoch 200, training loss: 1.381387710571289 = 0.7174686789512634 + 0.1 * 6.639189720153809
Epoch 200, val loss: 0.9390364289283752
Epoch 210, training loss: 1.3204355239868164 = 0.6573298573493958 + 0.1 * 6.631056308746338
Epoch 210, val loss: 0.89835125207901
Epoch 220, training loss: 1.265862226486206 = 0.6040654182434082 + 0.1 * 6.6179680824279785
Epoch 220, val loss: 0.864961564540863
Epoch 230, training loss: 1.2165257930755615 = 0.5554303526878357 + 0.1 * 6.6109538078308105
Epoch 230, val loss: 0.8373249173164368
Epoch 240, training loss: 1.1702675819396973 = 0.5099786520004272 + 0.1 * 6.602889060974121
Epoch 240, val loss: 0.8139732480049133
Epoch 250, training loss: 1.1266765594482422 = 0.4669073522090912 + 0.1 * 6.597692012786865
Epoch 250, val loss: 0.7939773797988892
Epoch 260, training loss: 1.0854382514953613 = 0.42597973346710205 + 0.1 * 6.59458589553833
Epoch 260, val loss: 0.776885449886322
Epoch 270, training loss: 1.0456037521362305 = 0.38726475834846497 + 0.1 * 6.5833892822265625
Epoch 270, val loss: 0.7624124884605408
Epoch 280, training loss: 1.0103051662445068 = 0.35062968730926514 + 0.1 * 6.59675407409668
Epoch 280, val loss: 0.7502425312995911
Epoch 290, training loss: 0.9738837480545044 = 0.31657788157463074 + 0.1 * 6.573058605194092
Epoch 290, val loss: 0.740706205368042
Epoch 300, training loss: 0.9413430094718933 = 0.28479212522506714 + 0.1 * 6.565508842468262
Epoch 300, val loss: 0.7337130904197693
Epoch 310, training loss: 0.910596489906311 = 0.25513049960136414 + 0.1 * 6.554659366607666
Epoch 310, val loss: 0.7294862270355225
Epoch 320, training loss: 0.8835753798484802 = 0.2278115600347519 + 0.1 * 6.557638168334961
Epoch 320, val loss: 0.7277578711509705
Epoch 330, training loss: 0.8572103381156921 = 0.2032311111688614 + 0.1 * 6.539792537689209
Epoch 330, val loss: 0.7285068035125732
Epoch 340, training loss: 0.8351762294769287 = 0.18133459985256195 + 0.1 * 6.538415908813477
Epoch 340, val loss: 0.7314859628677368
Epoch 350, training loss: 0.8145880699157715 = 0.1622578203678131 + 0.1 * 6.52330207824707
Epoch 350, val loss: 0.7364112138748169
Epoch 360, training loss: 0.7965177297592163 = 0.1457112729549408 + 0.1 * 6.508064270019531
Epoch 360, val loss: 0.7430984973907471
Epoch 370, training loss: 0.7856848835945129 = 0.13139642775058746 + 0.1 * 6.542884826660156
Epoch 370, val loss: 0.7512478232383728
Epoch 380, training loss: 0.7680703401565552 = 0.1191357746720314 + 0.1 * 6.489345073699951
Epoch 380, val loss: 0.7601321339607239
Epoch 390, training loss: 0.7562071084976196 = 0.10845938324928284 + 0.1 * 6.477476596832275
Epoch 390, val loss: 0.7699040770530701
Epoch 400, training loss: 0.745785653591156 = 0.09903910011053085 + 0.1 * 6.467465877532959
Epoch 400, val loss: 0.7803654074668884
Epoch 410, training loss: 0.7366969585418701 = 0.09067616611719131 + 0.1 * 6.460207462310791
Epoch 410, val loss: 0.7912152409553528
Epoch 420, training loss: 0.7279787063598633 = 0.08321219682693481 + 0.1 * 6.447664737701416
Epoch 420, val loss: 0.802402913570404
Epoch 430, training loss: 0.7206257581710815 = 0.07645365595817566 + 0.1 * 6.441720485687256
Epoch 430, val loss: 0.8139395117759705
Epoch 440, training loss: 0.7135648727416992 = 0.07032998651266098 + 0.1 * 6.432348728179932
Epoch 440, val loss: 0.825704038143158
Epoch 450, training loss: 0.7077584266662598 = 0.0647488385438919 + 0.1 * 6.43009614944458
Epoch 450, val loss: 0.8377785682678223
Epoch 460, training loss: 0.701823353767395 = 0.059643782675266266 + 0.1 * 6.42179536819458
Epoch 460, val loss: 0.8500291109085083
Epoch 470, training loss: 0.697136402130127 = 0.0549643337726593 + 0.1 * 6.421720504760742
Epoch 470, val loss: 0.8624709248542786
Epoch 480, training loss: 0.6921640634536743 = 0.05067691206932068 + 0.1 * 6.414871692657471
Epoch 480, val loss: 0.87502521276474
Epoch 490, training loss: 0.687349259853363 = 0.046746060252189636 + 0.1 * 6.406031608581543
Epoch 490, val loss: 0.8876325488090515
Epoch 500, training loss: 0.6822714805603027 = 0.043160416185855865 + 0.1 * 6.391110420227051
Epoch 500, val loss: 0.9003868103027344
Epoch 510, training loss: 0.6790035963058472 = 0.03989391773939133 + 0.1 * 6.391096591949463
Epoch 510, val loss: 0.9132251739501953
Epoch 520, training loss: 0.6756389737129211 = 0.03693877533078194 + 0.1 * 6.3870015144348145
Epoch 520, val loss: 0.9259434938430786
Epoch 530, training loss: 0.6734829545021057 = 0.03429480642080307 + 0.1 * 6.391881465911865
Epoch 530, val loss: 0.9385705590248108
Epoch 540, training loss: 0.669094443321228 = 0.031923189759254456 + 0.1 * 6.371712684631348
Epoch 540, val loss: 0.9511080980300903
Epoch 550, training loss: 0.6685349941253662 = 0.029777904972434044 + 0.1 * 6.387570858001709
Epoch 550, val loss: 0.9634535312652588
Epoch 560, training loss: 0.6634155511856079 = 0.027842814102768898 + 0.1 * 6.355727195739746
Epoch 560, val loss: 0.9753810167312622
Epoch 570, training loss: 0.6615078449249268 = 0.026091869920492172 + 0.1 * 6.354159832000732
Epoch 570, val loss: 0.9871541261672974
Epoch 580, training loss: 0.6590434312820435 = 0.024505555629730225 + 0.1 * 6.345378875732422
Epoch 580, val loss: 0.9985323548316956
Epoch 590, training loss: 0.6575441956520081 = 0.023066136986017227 + 0.1 * 6.344779968261719
Epoch 590, val loss: 1.0097016096115112
Epoch 600, training loss: 0.6557246446609497 = 0.02175205573439598 + 0.1 * 6.339725494384766
Epoch 600, val loss: 1.0205433368682861
Epoch 610, training loss: 0.6537853479385376 = 0.020550459623336792 + 0.1 * 6.3323493003845215
Epoch 610, val loss: 1.0311022996902466
Epoch 620, training loss: 0.6538479328155518 = 0.019450316205620766 + 0.1 * 6.343976020812988
Epoch 620, val loss: 1.0413284301757812
Epoch 630, training loss: 0.6507110595703125 = 0.018443617969751358 + 0.1 * 6.322673797607422
Epoch 630, val loss: 1.051222324371338
Epoch 640, training loss: 0.6490515470504761 = 0.017515968531370163 + 0.1 * 6.3153557777404785
Epoch 640, val loss: 1.0609534978866577
Epoch 650, training loss: 0.6532424092292786 = 0.01665790006518364 + 0.1 * 6.365845203399658
Epoch 650, val loss: 1.0704119205474854
Epoch 660, training loss: 0.6465916633605957 = 0.015867752954363823 + 0.1 * 6.307238578796387
Epoch 660, val loss: 1.0794754028320312
Epoch 670, training loss: 0.6464582681655884 = 0.01513404119759798 + 0.1 * 6.313242435455322
Epoch 670, val loss: 1.0884912014007568
Epoch 680, training loss: 0.643923282623291 = 0.014452029019594193 + 0.1 * 6.294712543487549
Epoch 680, val loss: 1.0972100496292114
Epoch 690, training loss: 0.6445355415344238 = 0.013818591833114624 + 0.1 * 6.307169437408447
Epoch 690, val loss: 1.1057641506195068
Epoch 700, training loss: 0.6420069932937622 = 0.013226202689111233 + 0.1 * 6.287807941436768
Epoch 700, val loss: 1.1141160726547241
Epoch 710, training loss: 0.6425009369850159 = 0.012673314660787582 + 0.1 * 6.298275947570801
Epoch 710, val loss: 1.1223136186599731
Epoch 720, training loss: 0.6419758796691895 = 0.012156936340034008 + 0.1 * 6.298189163208008
Epoch 720, val loss: 1.1302847862243652
Epoch 730, training loss: 0.6407509446144104 = 0.011674368754029274 + 0.1 * 6.290765762329102
Epoch 730, val loss: 1.1380616426467896
Epoch 740, training loss: 0.6392856240272522 = 0.01122272852808237 + 0.1 * 6.280628681182861
Epoch 740, val loss: 1.145716667175293
Epoch 750, training loss: 0.6399243474006653 = 0.01079814787954092 + 0.1 * 6.291261672973633
Epoch 750, val loss: 1.1531901359558105
Epoch 760, training loss: 0.6390349864959717 = 0.010399547405540943 + 0.1 * 6.2863545417785645
Epoch 760, val loss: 1.1604832410812378
Epoch 770, training loss: 0.6377771496772766 = 0.010025168769061565 + 0.1 * 6.277520179748535
Epoch 770, val loss: 1.1676701307296753
Epoch 780, training loss: 0.6370029449462891 = 0.009672456420958042 + 0.1 * 6.2733049392700195
Epoch 780, val loss: 1.1746742725372314
Epoch 790, training loss: 0.6356288194656372 = 0.009338892064988613 + 0.1 * 6.262898921966553
Epoch 790, val loss: 1.1815435886383057
Epoch 800, training loss: 0.6366727352142334 = 0.00902507547289133 + 0.1 * 6.2764763832092285
Epoch 800, val loss: 1.1882879734039307
Epoch 810, training loss: 0.6355978846549988 = 0.008727073669433594 + 0.1 * 6.268708229064941
Epoch 810, val loss: 1.1949031352996826
Epoch 820, training loss: 0.6340540647506714 = 0.00844560656696558 + 0.1 * 6.256084442138672
Epoch 820, val loss: 1.2013484239578247
Epoch 830, training loss: 0.6336654424667358 = 0.008178992196917534 + 0.1 * 6.25486421585083
Epoch 830, val loss: 1.207688570022583
Epoch 840, training loss: 0.6359397172927856 = 0.007925426587462425 + 0.1 * 6.280142784118652
Epoch 840, val loss: 1.2138465642929077
Epoch 850, training loss: 0.6332259178161621 = 0.007687326520681381 + 0.1 * 6.255385875701904
Epoch 850, val loss: 1.2199671268463135
Epoch 860, training loss: 0.6320869326591492 = 0.0074593923054635525 + 0.1 * 6.246274948120117
Epoch 860, val loss: 1.2260119915008545
Epoch 870, training loss: 0.6331775784492493 = 0.007242660503834486 + 0.1 * 6.2593488693237305
Epoch 870, val loss: 1.2319132089614868
Epoch 880, training loss: 0.6335979104042053 = 0.007036678493022919 + 0.1 * 6.2656121253967285
Epoch 880, val loss: 1.237671136856079
Epoch 890, training loss: 0.6314118504524231 = 0.0068407366052269936 + 0.1 * 6.245710849761963
Epoch 890, val loss: 1.2432681322097778
Epoch 900, training loss: 0.6302070617675781 = 0.006653692573308945 + 0.1 * 6.235533714294434
Epoch 900, val loss: 1.2489700317382812
Epoch 910, training loss: 0.6304674744606018 = 0.006474503315985203 + 0.1 * 6.239929676055908
Epoch 910, val loss: 1.254400610923767
Epoch 920, training loss: 0.629982054233551 = 0.006304732523858547 + 0.1 * 6.2367730140686035
Epoch 920, val loss: 1.259759783744812
Epoch 930, training loss: 0.6294728517532349 = 0.0061418889090418816 + 0.1 * 6.233309268951416
Epoch 930, val loss: 1.265059232711792
Epoch 940, training loss: 0.6304588317871094 = 0.005986443255096674 + 0.1 * 6.244723796844482
Epoch 940, val loss: 1.270259141921997
Epoch 950, training loss: 0.6287107467651367 = 0.0058377571403980255 + 0.1 * 6.228730201721191
Epoch 950, val loss: 1.275329351425171
Epoch 960, training loss: 0.6280880570411682 = 0.00569497887045145 + 0.1 * 6.223930358886719
Epoch 960, val loss: 1.280442714691162
Epoch 970, training loss: 0.628896176815033 = 0.005558005068451166 + 0.1 * 6.233381748199463
Epoch 970, val loss: 1.285409688949585
Epoch 980, training loss: 0.6274703145027161 = 0.00542679987847805 + 0.1 * 6.220434665679932
Epoch 980, val loss: 1.290211796760559
Epoch 990, training loss: 0.6268675327301025 = 0.005301037337630987 + 0.1 * 6.215664863586426
Epoch 990, val loss: 1.2950495481491089
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7749
Flip ASR: 0.7333/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.773548126220703 = 1.936157464981079 + 0.1 * 8.373905181884766
Epoch 0, val loss: 1.9373096227645874
Epoch 10, training loss: 2.764057159423828 = 1.9266760349273682 + 0.1 * 8.373809814453125
Epoch 10, val loss: 1.9279814958572388
Epoch 20, training loss: 2.7523481845855713 = 1.9150198698043823 + 0.1 * 8.373283386230469
Epoch 20, val loss: 1.9163563251495361
Epoch 30, training loss: 2.7357332706451416 = 1.8987996578216553 + 0.1 * 8.369336128234863
Epoch 30, val loss: 1.900259017944336
Epoch 40, training loss: 2.709219455718994 = 1.8749475479125977 + 0.1 * 8.342720031738281
Epoch 40, val loss: 1.8769701719284058
Epoch 50, training loss: 2.661698818206787 = 1.8416671752929688 + 0.1 * 8.200316429138184
Epoch 50, val loss: 1.8456884622573853
Epoch 60, training loss: 2.576578140258789 = 1.802459955215454 + 0.1 * 7.741181373596191
Epoch 60, val loss: 1.8104794025421143
Epoch 70, training loss: 2.5043227672576904 = 1.7620362043380737 + 0.1 * 7.422865390777588
Epoch 70, val loss: 1.7751545906066895
Epoch 80, training loss: 2.42995548248291 = 1.7167059183120728 + 0.1 * 7.1324944496154785
Epoch 80, val loss: 1.7355612516403198
Epoch 90, training loss: 2.35575532913208 = 1.658925175666809 + 0.1 * 6.9683003425598145
Epoch 90, val loss: 1.6858241558074951
Epoch 100, training loss: 2.272160291671753 = 1.5831248760223389 + 0.1 * 6.890354156494141
Epoch 100, val loss: 1.6208851337432861
Epoch 110, training loss: 2.1714038848876953 = 1.487853765487671 + 0.1 * 6.835500717163086
Epoch 110, val loss: 1.5408673286437988
Epoch 120, training loss: 2.0570130348205566 = 1.3771566152572632 + 0.1 * 6.798563003540039
Epoch 120, val loss: 1.4493523836135864
Epoch 130, training loss: 1.9356215000152588 = 1.258152723312378 + 0.1 * 6.77468729019165
Epoch 130, val loss: 1.3519325256347656
Epoch 140, training loss: 1.8149783611297607 = 1.1389747858047485 + 0.1 * 6.760035991668701
Epoch 140, val loss: 1.2557750940322876
Epoch 150, training loss: 1.7014920711517334 = 1.0264092683792114 + 0.1 * 6.750828266143799
Epoch 150, val loss: 1.166407823562622
Epoch 160, training loss: 1.5982089042663574 = 0.9236882328987122 + 0.1 * 6.745206832885742
Epoch 160, val loss: 1.0858012437820435
Epoch 170, training loss: 1.5049669742584229 = 0.8306927680969238 + 0.1 * 6.742741584777832
Epoch 170, val loss: 1.0139421224594116
Epoch 180, training loss: 1.420332431793213 = 0.746355414390564 + 0.1 * 6.739770412445068
Epoch 180, val loss: 0.949872612953186
Epoch 190, training loss: 1.3436914682388306 = 0.6699385046958923 + 0.1 * 6.737529277801514
Epoch 190, val loss: 0.8932824730873108
Epoch 200, training loss: 1.2749197483062744 = 0.6014480590820312 + 0.1 * 6.734716415405273
Epoch 200, val loss: 0.8446738123893738
Epoch 210, training loss: 1.2135521173477173 = 0.5404139757156372 + 0.1 * 6.731381416320801
Epoch 210, val loss: 0.8046243190765381
Epoch 220, training loss: 1.1584949493408203 = 0.4857628643512726 + 0.1 * 6.727321147918701
Epoch 220, val loss: 0.7723296284675598
Epoch 230, training loss: 1.108278751373291 = 0.43601590394973755 + 0.1 * 6.722629070281982
Epoch 230, val loss: 0.7465727925300598
Epoch 240, training loss: 1.061797857284546 = 0.390048623085022 + 0.1 * 6.717491626739502
Epoch 240, val loss: 0.7257770895957947
Epoch 250, training loss: 1.0188171863555908 = 0.3475358486175537 + 0.1 * 6.712812900543213
Epoch 250, val loss: 0.7091690301895142
Epoch 260, training loss: 0.9790282249450684 = 0.30849701166152954 + 0.1 * 6.7053117752075195
Epoch 260, val loss: 0.6962437629699707
Epoch 270, training loss: 0.942925751209259 = 0.27308183908462524 + 0.1 * 6.698439121246338
Epoch 270, val loss: 0.6869092583656311
Epoch 280, training loss: 0.9106841087341309 = 0.24155482649803162 + 0.1 * 6.6912922859191895
Epoch 280, val loss: 0.6810585260391235
Epoch 290, training loss: 0.882015585899353 = 0.2137380987405777 + 0.1 * 6.682775020599365
Epoch 290, val loss: 0.6783181428909302
Epoch 300, training loss: 0.8567037582397461 = 0.18956322968006134 + 0.1 * 6.67140531539917
Epoch 300, val loss: 0.6784588098526001
Epoch 310, training loss: 0.8356266021728516 = 0.1687193363904953 + 0.1 * 6.669072151184082
Epoch 310, val loss: 0.6810175776481628
Epoch 320, training loss: 0.8155235648155212 = 0.1508692055940628 + 0.1 * 6.646543502807617
Epoch 320, val loss: 0.6854758858680725
Epoch 330, training loss: 0.798816978931427 = 0.13541381061077118 + 0.1 * 6.634031295776367
Epoch 330, val loss: 0.6915158629417419
Epoch 340, training loss: 0.7840290069580078 = 0.12197186052799225 + 0.1 * 6.620571613311768
Epoch 340, val loss: 0.6988068222999573
Epoch 350, training loss: 0.7711277604103088 = 0.11020339280366898 + 0.1 * 6.609243392944336
Epoch 350, val loss: 0.7069483995437622
Epoch 360, training loss: 0.7592423558235168 = 0.09982544183731079 + 0.1 * 6.5941691398620605
Epoch 360, val loss: 0.7158609628677368
Epoch 370, training loss: 0.7499434351921082 = 0.09063177555799484 + 0.1 * 6.593116283416748
Epoch 370, val loss: 0.725296139717102
Epoch 380, training loss: 0.7409407496452332 = 0.08252058178186417 + 0.1 * 6.584201335906982
Epoch 380, val loss: 0.7350721955299377
Epoch 390, training loss: 0.7318556308746338 = 0.07531502842903137 + 0.1 * 6.565406322479248
Epoch 390, val loss: 0.7450906038284302
Epoch 400, training loss: 0.72530597448349 = 0.06887195259332657 + 0.1 * 6.564340114593506
Epoch 400, val loss: 0.755395233631134
Epoch 410, training loss: 0.7184882760047913 = 0.06312710046768188 + 0.1 * 6.553611755371094
Epoch 410, val loss: 0.7659081220626831
Epoch 420, training loss: 0.7123304009437561 = 0.05799567326903343 + 0.1 * 6.543347358703613
Epoch 420, val loss: 0.7763333916664124
Epoch 430, training loss: 0.7067917585372925 = 0.05340772494673729 + 0.1 * 6.533840179443359
Epoch 430, val loss: 0.7867846488952637
Epoch 440, training loss: 0.7019321322441101 = 0.049313321709632874 + 0.1 * 6.526187896728516
Epoch 440, val loss: 0.7971136569976807
Epoch 450, training loss: 0.6977741122245789 = 0.04563411697745323 + 0.1 * 6.521399974822998
Epoch 450, val loss: 0.8073521256446838
Epoch 460, training loss: 0.6939536333084106 = 0.04233168065547943 + 0.1 * 6.516219139099121
Epoch 460, val loss: 0.817554771900177
Epoch 470, training loss: 0.6898787021636963 = 0.03935938701033592 + 0.1 * 6.505192756652832
Epoch 470, val loss: 0.8274438977241516
Epoch 480, training loss: 0.686785876750946 = 0.03668318688869476 + 0.1 * 6.501026630401611
Epoch 480, val loss: 0.8370742797851562
Epoch 490, training loss: 0.6828787922859192 = 0.03428637608885765 + 0.1 * 6.485924243927002
Epoch 490, val loss: 0.846599280834198
Epoch 500, training loss: 0.6798263192176819 = 0.032108940184116364 + 0.1 * 6.477173328399658
Epoch 500, val loss: 0.855651319026947
Epoch 510, training loss: 0.6772210001945496 = 0.030120566487312317 + 0.1 * 6.471004486083984
Epoch 510, val loss: 0.8648377656936646
Epoch 520, training loss: 0.6775769591331482 = 0.02830987609922886 + 0.1 * 6.49267053604126
Epoch 520, val loss: 0.8735488057136536
Epoch 530, training loss: 0.6722851395606995 = 0.02666269987821579 + 0.1 * 6.45622444152832
Epoch 530, val loss: 0.8822550177574158
Epoch 540, training loss: 0.6703442335128784 = 0.02514973282814026 + 0.1 * 6.451944351196289
Epoch 540, val loss: 0.8906316161155701
Epoch 550, training loss: 0.6691485643386841 = 0.023757556453347206 + 0.1 * 6.453909873962402
Epoch 550, val loss: 0.8988733887672424
Epoch 560, training loss: 0.6671093702316284 = 0.02247657999396324 + 0.1 * 6.446328163146973
Epoch 560, val loss: 0.9070695638656616
Epoch 570, training loss: 0.6647449135780334 = 0.021294716745615005 + 0.1 * 6.434501647949219
Epoch 570, val loss: 0.9149824380874634
Epoch 580, training loss: 0.6646801829338074 = 0.020204558968544006 + 0.1 * 6.444756031036377
Epoch 580, val loss: 0.9226392507553101
Epoch 590, training loss: 0.6616337299346924 = 0.019199363887310028 + 0.1 * 6.424343585968018
Epoch 590, val loss: 0.9303428530693054
Epoch 600, training loss: 0.6598461270332336 = 0.018268704414367676 + 0.1 * 6.415773868560791
Epoch 600, val loss: 0.9376187324523926
Epoch 610, training loss: 0.6582279801368713 = 0.017405137419700623 + 0.1 * 6.408227920532227
Epoch 610, val loss: 0.9447136521339417
Epoch 620, training loss: 0.6595261693000793 = 0.016604308038949966 + 0.1 * 6.429218769073486
Epoch 620, val loss: 0.9516944289207458
Epoch 630, training loss: 0.6558928489685059 = 0.01586189679801464 + 0.1 * 6.4003095626831055
Epoch 630, val loss: 0.9585336446762085
Epoch 640, training loss: 0.6547694206237793 = 0.01516792643815279 + 0.1 * 6.39601469039917
Epoch 640, val loss: 0.9650738835334778
Epoch 650, training loss: 0.6583571434020996 = 0.01451912522315979 + 0.1 * 6.438379764556885
Epoch 650, val loss: 0.9715203046798706
Epoch 660, training loss: 0.652184247970581 = 0.013913865201175213 + 0.1 * 6.38270378112793
Epoch 660, val loss: 0.9778088331222534
Epoch 670, training loss: 0.6517869830131531 = 0.013347717002034187 + 0.1 * 6.384392261505127
Epoch 670, val loss: 0.9840813279151917
Epoch 680, training loss: 0.6505761742591858 = 0.01281752623617649 + 0.1 * 6.377586364746094
Epoch 680, val loss: 0.9898854494094849
Epoch 690, training loss: 0.6489971280097961 = 0.012319422326982021 + 0.1 * 6.366776943206787
Epoch 690, val loss: 0.9959412813186646
Epoch 700, training loss: 0.648005485534668 = 0.011849721893668175 + 0.1 * 6.361557483673096
Epoch 700, val loss: 1.001649022102356
Epoch 710, training loss: 0.6526695489883423 = 0.01140720583498478 + 0.1 * 6.412623405456543
Epoch 710, val loss: 1.0071804523468018
Epoch 720, training loss: 0.6469694972038269 = 0.01099301129579544 + 0.1 * 6.35976505279541
Epoch 720, val loss: 1.0125715732574463
Epoch 730, training loss: 0.6462843418121338 = 0.010603481903672218 + 0.1 * 6.356808662414551
Epoch 730, val loss: 1.0180060863494873
Epoch 740, training loss: 0.6449382305145264 = 0.010233760811388493 + 0.1 * 6.347044467926025
Epoch 740, val loss: 1.0231506824493408
Epoch 750, training loss: 0.6446017026901245 = 0.009883535094559193 + 0.1 * 6.347181797027588
Epoch 750, val loss: 1.0282564163208008
Epoch 760, training loss: 0.6445851922035217 = 0.009552083909511566 + 0.1 * 6.3503313064575195
Epoch 760, val loss: 1.0332640409469604
Epoch 770, training loss: 0.6448249220848083 = 0.009238522499799728 + 0.1 * 6.355863571166992
Epoch 770, val loss: 1.0380619764328003
Epoch 780, training loss: 0.6423270106315613 = 0.008941705338656902 + 0.1 * 6.333852767944336
Epoch 780, val loss: 1.0427898168563843
Epoch 790, training loss: 0.6416592001914978 = 0.008661147207021713 + 0.1 * 6.329980373382568
Epoch 790, val loss: 1.047550082206726
Epoch 800, training loss: 0.6413828134536743 = 0.008394179865717888 + 0.1 * 6.329885959625244
Epoch 800, val loss: 1.0519851446151733
Epoch 810, training loss: 0.640516996383667 = 0.008141572587192059 + 0.1 * 6.32375431060791
Epoch 810, val loss: 1.0564805269241333
Epoch 820, training loss: 0.6390924453735352 = 0.007900253869593143 + 0.1 * 6.311922073364258
Epoch 820, val loss: 1.0609129667282104
Epoch 830, training loss: 0.6389009356498718 = 0.007669893559068441 + 0.1 * 6.312310695648193
Epoch 830, val loss: 1.0652340650558472
Epoch 840, training loss: 0.639128565788269 = 0.007450547534972429 + 0.1 * 6.316780090332031
Epoch 840, val loss: 1.0694310665130615
Epoch 850, training loss: 0.6385296583175659 = 0.0072411890141665936 + 0.1 * 6.31288480758667
Epoch 850, val loss: 1.0737000703811646
Epoch 860, training loss: 0.6377031803131104 = 0.007041626609861851 + 0.1 * 6.306615352630615
Epoch 860, val loss: 1.0777312517166138
Epoch 870, training loss: 0.6367354393005371 = 0.006851533427834511 + 0.1 * 6.2988386154174805
Epoch 870, val loss: 1.0818017721176147
Epoch 880, training loss: 0.6377691030502319 = 0.006669447757303715 + 0.1 * 6.3109965324401855
Epoch 880, val loss: 1.0858384370803833
Epoch 890, training loss: 0.6356071829795837 = 0.00649575749412179 + 0.1 * 6.29111385345459
Epoch 890, val loss: 1.0896594524383545
Epoch 900, training loss: 0.635026752948761 = 0.006329323630779982 + 0.1 * 6.2869744300842285
Epoch 900, val loss: 1.0935487747192383
Epoch 910, training loss: 0.6392037868499756 = 0.006169645115733147 + 0.1 * 6.330341339111328
Epoch 910, val loss: 1.097288966178894
Epoch 920, training loss: 0.6337813138961792 = 0.006016713101416826 + 0.1 * 6.277645587921143
Epoch 920, val loss: 1.100953459739685
Epoch 930, training loss: 0.6346995234489441 = 0.005870350636541843 + 0.1 * 6.288291931152344
Epoch 930, val loss: 1.104733943939209
Epoch 940, training loss: 0.6337324976921082 = 0.00572990020737052 + 0.1 * 6.280025959014893
Epoch 940, val loss: 1.1081827878952026
Epoch 950, training loss: 0.6322664618492126 = 0.00559548195451498 + 0.1 * 6.266709327697754
Epoch 950, val loss: 1.1117769479751587
Epoch 960, training loss: 0.6348531246185303 = 0.005465811584144831 + 0.1 * 6.293873310089111
Epoch 960, val loss: 1.115296483039856
Epoch 970, training loss: 0.6317657828330994 = 0.005341202486306429 + 0.1 * 6.264245510101318
Epoch 970, val loss: 1.1186401844024658
Epoch 980, training loss: 0.6317746043205261 = 0.005221674218773842 + 0.1 * 6.265529155731201
Epoch 980, val loss: 1.1220800876617432
Epoch 990, training loss: 0.6312947273254395 = 0.005106413271278143 + 0.1 * 6.26188325881958
Epoch 990, val loss: 1.1254315376281738
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8745
Flip ASR: 0.8489/225 nodes
The final ASR:0.72325, 0.14916, Accuracy:0.81605, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10580])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98032, 0.00870, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.781881332397461 = 1.9445000886917114 + 0.1 * 8.373811721801758
Epoch 0, val loss: 1.9508808851242065
Epoch 10, training loss: 2.771907329559326 = 1.9345588684082031 + 0.1 * 8.37348461151123
Epoch 10, val loss: 1.9407403469085693
Epoch 20, training loss: 2.758899211883545 = 1.9217133522033691 + 0.1 * 8.371859550476074
Epoch 20, val loss: 1.9275273084640503
Epoch 30, training loss: 2.739267587661743 = 1.9032007455825806 + 0.1 * 8.360669136047363
Epoch 30, val loss: 1.9086893796920776
Epoch 40, training loss: 2.702785015106201 = 1.8756366968154907 + 0.1 * 8.271482467651367
Epoch 40, val loss: 1.8814926147460938
Epoch 50, training loss: 2.586961269378662 = 1.840641736984253 + 0.1 * 7.463194847106934
Epoch 50, val loss: 1.8493092060089111
Epoch 60, training loss: 2.5170810222625732 = 1.8075605630874634 + 0.1 * 7.0952043533325195
Epoch 60, val loss: 1.8206697702407837
Epoch 70, training loss: 2.4626054763793945 = 1.7726256847381592 + 0.1 * 6.899798393249512
Epoch 70, val loss: 1.790581226348877
Epoch 80, training loss: 2.4145052433013916 = 1.7347116470336914 + 0.1 * 6.797935962677002
Epoch 80, val loss: 1.7578753232955933
Epoch 90, training loss: 2.3618719577789307 = 1.6868401765823364 + 0.1 * 6.750317096710205
Epoch 90, val loss: 1.715745210647583
Epoch 100, training loss: 2.2948060035705566 = 1.622568964958191 + 0.1 * 6.722370624542236
Epoch 100, val loss: 1.6604689359664917
Epoch 110, training loss: 2.2098169326782227 = 1.5398833751678467 + 0.1 * 6.69933557510376
Epoch 110, val loss: 1.5916781425476074
Epoch 120, training loss: 2.1083860397338867 = 1.440877079963684 + 0.1 * 6.675090312957764
Epoch 120, val loss: 1.5101345777511597
Epoch 130, training loss: 1.9963157176971436 = 1.3313143253326416 + 0.1 * 6.650014400482178
Epoch 130, val loss: 1.4199633598327637
Epoch 140, training loss: 1.879669427871704 = 1.2174673080444336 + 0.1 * 6.622020244598389
Epoch 140, val loss: 1.3276079893112183
Epoch 150, training loss: 1.7660598754882812 = 1.1060457229614258 + 0.1 * 6.600140571594238
Epoch 150, val loss: 1.2396039962768555
Epoch 160, training loss: 1.662381887435913 = 1.0047928094863892 + 0.1 * 6.575891494750977
Epoch 160, val loss: 1.1610934734344482
Epoch 170, training loss: 1.570122480392456 = 0.9147160053253174 + 0.1 * 6.554065227508545
Epoch 170, val loss: 1.0920846462249756
Epoch 180, training loss: 1.4900398254394531 = 0.836510956287384 + 0.1 * 6.5352888107299805
Epoch 180, val loss: 1.0334285497665405
Epoch 190, training loss: 1.420084834098816 = 0.7686240673065186 + 0.1 * 6.5146074295043945
Epoch 190, val loss: 0.9845230579376221
Epoch 200, training loss: 1.3589659929275513 = 0.7086958289146423 + 0.1 * 6.502701759338379
Epoch 200, val loss: 0.9434240460395813
Epoch 210, training loss: 1.3033182621002197 = 0.655392587184906 + 0.1 * 6.479257106781006
Epoch 210, val loss: 0.909459114074707
Epoch 220, training loss: 1.2535203695297241 = 0.6067256331443787 + 0.1 * 6.467947483062744
Epoch 220, val loss: 0.8812329173088074
Epoch 230, training loss: 1.2065389156341553 = 0.5615372657775879 + 0.1 * 6.450016021728516
Epoch 230, val loss: 0.8579127192497253
Epoch 240, training loss: 1.1641440391540527 = 0.5191220045089722 + 0.1 * 6.450220108032227
Epoch 240, val loss: 0.8386876583099365
Epoch 250, training loss: 1.1224958896636963 = 0.4796585738658905 + 0.1 * 6.428372859954834
Epoch 250, val loss: 0.8237608671188354
Epoch 260, training loss: 1.08432137966156 = 0.4424874484539032 + 0.1 * 6.418338775634766
Epoch 260, val loss: 0.8120710849761963
Epoch 270, training loss: 1.0492185354232788 = 0.4076043963432312 + 0.1 * 6.416141510009766
Epoch 270, val loss: 0.8035342693328857
Epoch 280, training loss: 1.0151735544204712 = 0.3750450313091278 + 0.1 * 6.401285648345947
Epoch 280, val loss: 0.7984827160835266
Epoch 290, training loss: 0.9842787981033325 = 0.34425175189971924 + 0.1 * 6.400270462036133
Epoch 290, val loss: 0.7958622574806213
Epoch 300, training loss: 0.953350841999054 = 0.31523215770721436 + 0.1 * 6.381186485290527
Epoch 300, val loss: 0.7959031462669373
Epoch 310, training loss: 0.9254709482192993 = 0.2876645624637604 + 0.1 * 6.378063678741455
Epoch 310, val loss: 0.7982288599014282
Epoch 320, training loss: 0.8995515704154968 = 0.2616114616394043 + 0.1 * 6.379400730133057
Epoch 320, val loss: 0.8024798035621643
Epoch 330, training loss: 0.8734340667724609 = 0.23724158108234406 + 0.1 * 6.361924648284912
Epoch 330, val loss: 0.80889892578125
Epoch 340, training loss: 0.8508644104003906 = 0.21458156406879425 + 0.1 * 6.362828731536865
Epoch 340, val loss: 0.8170196413993835
Epoch 350, training loss: 0.8289702534675598 = 0.19381661713123322 + 0.1 * 6.351536273956299
Epoch 350, val loss: 0.826555073261261
Epoch 360, training loss: 0.810791552066803 = 0.1749413162469864 + 0.1 * 6.358502388000488
Epoch 360, val loss: 0.8372809886932373
Epoch 370, training loss: 0.7915769219398499 = 0.1580856293439865 + 0.1 * 6.3349127769470215
Epoch 370, val loss: 0.8488480448722839
Epoch 380, training loss: 0.7757973670959473 = 0.14300282299518585 + 0.1 * 6.327945232391357
Epoch 380, val loss: 0.8611339926719666
Epoch 390, training loss: 0.7617567777633667 = 0.12946678698062897 + 0.1 * 6.32289981842041
Epoch 390, val loss: 0.8740167617797852
Epoch 400, training loss: 0.7509390115737915 = 0.11733775585889816 + 0.1 * 6.336012363433838
Epoch 400, val loss: 0.8872049450874329
Epoch 410, training loss: 0.7375442385673523 = 0.10655339062213898 + 0.1 * 6.309908390045166
Epoch 410, val loss: 0.9006847143173218
Epoch 420, training loss: 0.7277160286903381 = 0.09694350510835648 + 0.1 * 6.307725429534912
Epoch 420, val loss: 0.9141193628311157
Epoch 430, training loss: 0.7193126678466797 = 0.08839064091444016 + 0.1 * 6.309219837188721
Epoch 430, val loss: 0.9276042580604553
Epoch 440, training loss: 0.7109387516975403 = 0.0808173343539238 + 0.1 * 6.301214218139648
Epoch 440, val loss: 0.9409461617469788
Epoch 450, training loss: 0.7030020356178284 = 0.07409106940031052 + 0.1 * 6.289109706878662
Epoch 450, val loss: 0.9542078971862793
Epoch 460, training loss: 0.6964015364646912 = 0.0680830329656601 + 0.1 * 6.283185005187988
Epoch 460, val loss: 0.9671987891197205
Epoch 470, training loss: 0.6909198760986328 = 0.0627162829041481 + 0.1 * 6.282035827636719
Epoch 470, val loss: 0.979911744594574
Epoch 480, training loss: 0.6857287287712097 = 0.057931672781705856 + 0.1 * 6.277970790863037
Epoch 480, val loss: 0.9926065802574158
Epoch 490, training loss: 0.6809712648391724 = 0.05363287031650543 + 0.1 * 6.273384094238281
Epoch 490, val loss: 1.0048662424087524
Epoch 500, training loss: 0.6776990294456482 = 0.04975925758481026 + 0.1 * 6.279397487640381
Epoch 500, val loss: 1.016913652420044
Epoch 510, training loss: 0.6735459566116333 = 0.04628397524356842 + 0.1 * 6.272619724273682
Epoch 510, val loss: 1.0285756587982178
Epoch 520, training loss: 0.6693249344825745 = 0.04314747080206871 + 0.1 * 6.261775016784668
Epoch 520, val loss: 1.0402213335037231
Epoch 530, training loss: 0.665480375289917 = 0.04029613360762596 + 0.1 * 6.251842021942139
Epoch 530, val loss: 1.051484227180481
Epoch 540, training loss: 0.6635894775390625 = 0.03769615292549133 + 0.1 * 6.258932590484619
Epoch 540, val loss: 1.0626444816589355
Epoch 550, training loss: 0.660957932472229 = 0.03532879799604416 + 0.1 * 6.256291389465332
Epoch 550, val loss: 1.0733157396316528
Epoch 560, training loss: 0.6578952074050903 = 0.03316850587725639 + 0.1 * 6.247267246246338
Epoch 560, val loss: 1.0841487646102905
Epoch 570, training loss: 0.6553313136100769 = 0.031189080327749252 + 0.1 * 6.241422176361084
Epoch 570, val loss: 1.0944877862930298
Epoch 580, training loss: 0.6550419330596924 = 0.029374167323112488 + 0.1 * 6.256677627563477
Epoch 580, val loss: 1.1046265363693237
Epoch 590, training loss: 0.650920569896698 = 0.02771305851638317 + 0.1 * 6.232075214385986
Epoch 590, val loss: 1.1148000955581665
Epoch 600, training loss: 0.6487699747085571 = 0.026184286922216415 + 0.1 * 6.225856781005859
Epoch 600, val loss: 1.124638319015503
Epoch 610, training loss: 0.6470415592193604 = 0.02477213181555271 + 0.1 * 6.222694396972656
Epoch 610, val loss: 1.134347915649414
Epoch 620, training loss: 0.6468102931976318 = 0.023466436192393303 + 0.1 * 6.233438491821289
Epoch 620, val loss: 1.1436173915863037
Epoch 630, training loss: 0.6440852284431458 = 0.022263435646891594 + 0.1 * 6.218217849731445
Epoch 630, val loss: 1.153090238571167
Epoch 640, training loss: 0.6449716687202454 = 0.02114957571029663 + 0.1 * 6.238220691680908
Epoch 640, val loss: 1.1620705127716064
Epoch 650, training loss: 0.6420565247535706 = 0.020118247717618942 + 0.1 * 6.2193827629089355
Epoch 650, val loss: 1.1709080934524536
Epoch 660, training loss: 0.6414967775344849 = 0.019163040444254875 + 0.1 * 6.223337650299072
Epoch 660, val loss: 1.1797438859939575
Epoch 670, training loss: 0.6388521194458008 = 0.018274158239364624 + 0.1 * 6.205779075622559
Epoch 670, val loss: 1.1882150173187256
Epoch 680, training loss: 0.6384444832801819 = 0.01744663342833519 + 0.1 * 6.2099785804748535
Epoch 680, val loss: 1.196661114692688
Epoch 690, training loss: 0.6375190019607544 = 0.01667504943907261 + 0.1 * 6.208439350128174
Epoch 690, val loss: 1.204899787902832
Epoch 700, training loss: 0.6364747285842896 = 0.015954554080963135 + 0.1 * 6.205201625823975
Epoch 700, val loss: 1.212894320487976
Epoch 710, training loss: 0.6368497014045715 = 0.015282697044312954 + 0.1 * 6.215670108795166
Epoch 710, val loss: 1.2208620309829712
Epoch 720, training loss: 0.6348758339881897 = 0.014653345569968224 + 0.1 * 6.202225208282471
Epoch 720, val loss: 1.2285897731781006
Epoch 730, training loss: 0.633783757686615 = 0.014065292663872242 + 0.1 * 6.1971845626831055
Epoch 730, val loss: 1.2361572980880737
Epoch 740, training loss: 0.6325291991233826 = 0.013514343649148941 + 0.1 * 6.19014835357666
Epoch 740, val loss: 1.2437177896499634
Epoch 750, training loss: 0.6337938904762268 = 0.012994893826544285 + 0.1 * 6.2079901695251465
Epoch 750, val loss: 1.2507604360580444
Epoch 760, training loss: 0.6313438415527344 = 0.01250719279050827 + 0.1 * 6.188366413116455
Epoch 760, val loss: 1.2578309774398804
Epoch 770, training loss: 0.6310067772865295 = 0.012048358097672462 + 0.1 * 6.189584255218506
Epoch 770, val loss: 1.2650034427642822
Epoch 780, training loss: 0.63055819272995 = 0.011614873073995113 + 0.1 * 6.1894330978393555
Epoch 780, val loss: 1.271713137626648
Epoch 790, training loss: 0.6292595267295837 = 0.011205707676708698 + 0.1 * 6.180537700653076
Epoch 790, val loss: 1.2779650688171387
Epoch 800, training loss: 0.6294208765029907 = 0.01082269474864006 + 0.1 * 6.185981273651123
Epoch 800, val loss: 1.2848049402236938
Epoch 810, training loss: 0.6281773447990417 = 0.010459893383085728 + 0.1 * 6.177174091339111
Epoch 810, val loss: 1.2913898229599
Epoch 820, training loss: 0.6273407340049744 = 0.010115365497767925 + 0.1 * 6.172253608703613
Epoch 820, val loss: 1.2976179122924805
Epoch 830, training loss: 0.6285152435302734 = 0.009787599556148052 + 0.1 * 6.1872758865356445
Epoch 830, val loss: 1.3036432266235352
Epoch 840, training loss: 0.6270496845245361 = 0.009477468207478523 + 0.1 * 6.175721645355225
Epoch 840, val loss: 1.3096829652786255
Epoch 850, training loss: 0.6287432312965393 = 0.00918322429060936 + 0.1 * 6.1956000328063965
Epoch 850, val loss: 1.3156605958938599
Epoch 860, training loss: 0.6260498762130737 = 0.008903654292225838 + 0.1 * 6.171462059020996
Epoch 860, val loss: 1.3213344812393188
Epoch 870, training loss: 0.6260999441146851 = 0.008638746105134487 + 0.1 * 6.174612045288086
Epoch 870, val loss: 1.3271600008010864
Epoch 880, training loss: 0.6248001456260681 = 0.00838611088693142 + 0.1 * 6.164140224456787
Epoch 880, val loss: 1.3326600790023804
Epoch 890, training loss: 0.6255311965942383 = 0.008145986124873161 + 0.1 * 6.17385196685791
Epoch 890, val loss: 1.3381974697113037
Epoch 900, training loss: 0.6243981122970581 = 0.007916283793747425 + 0.1 * 6.164818286895752
Epoch 900, val loss: 1.3435217142105103
Epoch 910, training loss: 0.6236687898635864 = 0.007696790155023336 + 0.1 * 6.159719944000244
Epoch 910, val loss: 1.348670482635498
Epoch 920, training loss: 0.6232831478118896 = 0.007488118018954992 + 0.1 * 6.157949924468994
Epoch 920, val loss: 1.3540246486663818
Epoch 930, training loss: 0.6246023178100586 = 0.00728833582252264 + 0.1 * 6.173139572143555
Epoch 930, val loss: 1.3592106103897095
Epoch 940, training loss: 0.6237286329269409 = 0.0070966011844575405 + 0.1 * 6.166320323944092
Epoch 940, val loss: 1.3638582229614258
Epoch 950, training loss: 0.6242468357086182 = 0.006914233323186636 + 0.1 * 6.173325538635254
Epoch 950, val loss: 1.3689398765563965
Epoch 960, training loss: 0.6220911145210266 = 0.006739619188010693 + 0.1 * 6.153514862060547
Epoch 960, val loss: 1.3736793994903564
Epoch 970, training loss: 0.6221188306808472 = 0.006572688464075327 + 0.1 * 6.15546178817749
Epoch 970, val loss: 1.3785456418991089
Epoch 980, training loss: 0.622471034526825 = 0.006412129383534193 + 0.1 * 6.16058874130249
Epoch 980, val loss: 1.3832999467849731
Epoch 990, training loss: 0.6210741400718689 = 0.006257684901356697 + 0.1 * 6.14816427230835
Epoch 990, val loss: 1.387592077255249
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.5535
Flip ASR: 0.4756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.787607431411743 = 1.9502320289611816 + 0.1 * 8.373754501342773
Epoch 0, val loss: 1.948828101158142
Epoch 10, training loss: 2.7768852710723877 = 1.9395394325256348 + 0.1 * 8.373457908630371
Epoch 10, val loss: 1.9371721744537354
Epoch 20, training loss: 2.7635293006896973 = 1.926353096961975 + 0.1 * 8.3717622756958
Epoch 20, val loss: 1.9225009679794312
Epoch 30, training loss: 2.7438831329345703 = 1.9078644514083862 + 0.1 * 8.360187530517578
Epoch 30, val loss: 1.9018667936325073
Epoch 40, training loss: 2.710021734237671 = 1.8808943033218384 + 0.1 * 8.291274070739746
Epoch 40, val loss: 1.8723174333572388
Epoch 50, training loss: 2.636202812194824 = 1.8461620807647705 + 0.1 * 7.9004082679748535
Epoch 50, val loss: 1.8361468315124512
Epoch 60, training loss: 2.5641708374023438 = 1.8086363077163696 + 0.1 * 7.555344581604004
Epoch 60, val loss: 1.7984635829925537
Epoch 70, training loss: 2.491041421890259 = 1.7710078954696655 + 0.1 * 7.20033597946167
Epoch 70, val loss: 1.7630391120910645
Epoch 80, training loss: 2.4254918098449707 = 1.7348744869232178 + 0.1 * 6.906172275543213
Epoch 80, val loss: 1.7309551239013672
Epoch 90, training loss: 2.3655600547790527 = 1.6898430585861206 + 0.1 * 6.7571702003479
Epoch 90, val loss: 1.690801739692688
Epoch 100, training loss: 2.297384738922119 = 1.6280462741851807 + 0.1 * 6.693383693695068
Epoch 100, val loss: 1.636716604232788
Epoch 110, training loss: 2.2132680416107178 = 1.5481764078140259 + 0.1 * 6.650916576385498
Epoch 110, val loss: 1.5692782402038574
Epoch 120, training loss: 2.118385076522827 = 1.4557234048843384 + 0.1 * 6.62661600112915
Epoch 120, val loss: 1.492578387260437
Epoch 130, training loss: 2.0206069946289062 = 1.3592833280563354 + 0.1 * 6.6132378578186035
Epoch 130, val loss: 1.4156713485717773
Epoch 140, training loss: 1.9238097667694092 = 1.2638624906539917 + 0.1 * 6.599472999572754
Epoch 140, val loss: 1.343889594078064
Epoch 150, training loss: 1.829502820968628 = 1.1710197925567627 + 0.1 * 6.584829807281494
Epoch 150, val loss: 1.278775691986084
Epoch 160, training loss: 1.740030288696289 = 1.0826616287231445 + 0.1 * 6.573686599731445
Epoch 160, val loss: 1.2188365459442139
Epoch 170, training loss: 1.6502416133880615 = 0.994056761264801 + 0.1 * 6.561848163604736
Epoch 170, val loss: 1.158397912979126
Epoch 180, training loss: 1.5587034225463867 = 0.9038509130477905 + 0.1 * 6.548525810241699
Epoch 180, val loss: 1.0960307121276855
Epoch 190, training loss: 1.46942138671875 = 0.8156150579452515 + 0.1 * 6.5380635261535645
Epoch 190, val loss: 1.0340538024902344
Epoch 200, training loss: 1.3865246772766113 = 0.7341152429580688 + 0.1 * 6.524094581604004
Epoch 200, val loss: 0.9769237637519836
Epoch 210, training loss: 1.3132576942443848 = 0.6622574329376221 + 0.1 * 6.5100016593933105
Epoch 210, val loss: 0.9275254607200623
Epoch 220, training loss: 1.2513463497161865 = 0.6014063358306885 + 0.1 * 6.499400615692139
Epoch 220, val loss: 0.8881555795669556
Epoch 230, training loss: 1.2000619173049927 = 0.5493057370185852 + 0.1 * 6.507561683654785
Epoch 230, val loss: 0.8569970726966858
Epoch 240, training loss: 1.151573896408081 = 0.5042111277580261 + 0.1 * 6.473628044128418
Epoch 240, val loss: 0.8328543305397034
Epoch 250, training loss: 1.109503984451294 = 0.4632772207260132 + 0.1 * 6.462267875671387
Epoch 250, val loss: 0.8132878541946411
Epoch 260, training loss: 1.073662281036377 = 0.42480653524398804 + 0.1 * 6.488556861877441
Epoch 260, val loss: 0.7970411777496338
Epoch 270, training loss: 1.0326942205429077 = 0.3886806070804596 + 0.1 * 6.440135955810547
Epoch 270, val loss: 0.7839420437812805
Epoch 280, training loss: 0.9965918064117432 = 0.35398465394973755 + 0.1 * 6.426071643829346
Epoch 280, val loss: 0.7733845710754395
Epoch 290, training loss: 0.9636061191558838 = 0.32056310772895813 + 0.1 * 6.430429935455322
Epoch 290, val loss: 0.7653142809867859
Epoch 300, training loss: 0.9304726123809814 = 0.2891605794429779 + 0.1 * 6.413119792938232
Epoch 300, val loss: 0.759589672088623
Epoch 310, training loss: 0.8999383449554443 = 0.2602413594722748 + 0.1 * 6.396969795227051
Epoch 310, val loss: 0.7561312317848206
Epoch 320, training loss: 0.8723272085189819 = 0.2338428497314453 + 0.1 * 6.384843349456787
Epoch 320, val loss: 0.7547281384468079
Epoch 330, training loss: 0.8480152487754822 = 0.2102196365594864 + 0.1 * 6.377955913543701
Epoch 330, val loss: 0.7555135488510132
Epoch 340, training loss: 0.8264464735984802 = 0.1892959624528885 + 0.1 * 6.371505260467529
Epoch 340, val loss: 0.7582037448883057
Epoch 350, training loss: 0.8074315786361694 = 0.17092260718345642 + 0.1 * 6.3650898933410645
Epoch 350, val loss: 0.7625064253807068
Epoch 360, training loss: 0.7896560430526733 = 0.15487642586231232 + 0.1 * 6.3477959632873535
Epoch 360, val loss: 0.7682782411575317
Epoch 370, training loss: 0.7755310535430908 = 0.14070717990398407 + 0.1 * 6.348238468170166
Epoch 370, val loss: 0.7752404808998108
Epoch 380, training loss: 0.7611981630325317 = 0.12818829715251923 + 0.1 * 6.3300981521606445
Epoch 380, val loss: 0.7830314040184021
Epoch 390, training loss: 0.7513547539710999 = 0.11706479638814926 + 0.1 * 6.342899799346924
Epoch 390, val loss: 0.7917138338088989
Epoch 400, training loss: 0.739315390586853 = 0.10718782246112823 + 0.1 * 6.32127571105957
Epoch 400, val loss: 0.8009199500083923
Epoch 410, training loss: 0.7290540337562561 = 0.09836049377918243 + 0.1 * 6.3069353103637695
Epoch 410, val loss: 0.8105419874191284
Epoch 420, training loss: 0.7220047116279602 = 0.09040247648954391 + 0.1 * 6.316021919250488
Epoch 420, val loss: 0.8205218315124512
Epoch 430, training loss: 0.7122935652732849 = 0.08323681354522705 + 0.1 * 6.290567398071289
Epoch 430, val loss: 0.8307381868362427
Epoch 440, training loss: 0.7066855430603027 = 0.07675991207361221 + 0.1 * 6.299255847930908
Epoch 440, val loss: 0.8411632180213928
Epoch 450, training loss: 0.6990503072738647 = 0.07090204209089279 + 0.1 * 6.281482219696045
Epoch 450, val loss: 0.8517983555793762
Epoch 460, training loss: 0.694566547870636 = 0.06558408588171005 + 0.1 * 6.289824962615967
Epoch 460, val loss: 0.8624653816223145
Epoch 470, training loss: 0.6893174052238464 = 0.060764145106077194 + 0.1 * 6.285532474517822
Epoch 470, val loss: 0.8731237649917603
Epoch 480, training loss: 0.6833608150482178 = 0.056391529738903046 + 0.1 * 6.269692420959473
Epoch 480, val loss: 0.8838626742362976
Epoch 490, training loss: 0.6783611178398132 = 0.052403923124074936 + 0.1 * 6.259571552276611
Epoch 490, val loss: 0.8945428729057312
Epoch 500, training loss: 0.6745463013648987 = 0.0487607978284359 + 0.1 * 6.257855415344238
Epoch 500, val loss: 0.9052149057388306
Epoch 510, training loss: 0.6709040403366089 = 0.04543885216116905 + 0.1 * 6.25465202331543
Epoch 510, val loss: 0.915873646736145
Epoch 520, training loss: 0.6695026159286499 = 0.04240897297859192 + 0.1 * 6.270936489105225
Epoch 520, val loss: 0.9264887571334839
Epoch 530, training loss: 0.6648151278495789 = 0.03964974358677864 + 0.1 * 6.251653671264648
Epoch 530, val loss: 0.9369980096817017
Epoch 540, training loss: 0.6614777445793152 = 0.037130456417798996 + 0.1 * 6.243472576141357
Epoch 540, val loss: 0.9474421143531799
Epoch 550, training loss: 0.6596668362617493 = 0.03482339531183243 + 0.1 * 6.248434066772461
Epoch 550, val loss: 0.957806408405304
Epoch 560, training loss: 0.6564095616340637 = 0.03271496668457985 + 0.1 * 6.236946105957031
Epoch 560, val loss: 0.9679951071739197
Epoch 570, training loss: 0.6538659334182739 = 0.03078126721084118 + 0.1 * 6.230846405029297
Epoch 570, val loss: 0.9781845808029175
Epoch 580, training loss: 0.651820719242096 = 0.02900882437825203 + 0.1 * 6.228118896484375
Epoch 580, val loss: 0.9881069660186768
Epoch 590, training loss: 0.6501762866973877 = 0.027384307235479355 + 0.1 * 6.227920055389404
Epoch 590, val loss: 0.9980077743530273
Epoch 600, training loss: 0.6474066376686096 = 0.025890033692121506 + 0.1 * 6.215165615081787
Epoch 600, val loss: 1.007631778717041
Epoch 610, training loss: 0.6464883089065552 = 0.02451670542359352 + 0.1 * 6.2197160720825195
Epoch 610, val loss: 1.0170996189117432
Epoch 620, training loss: 0.645413875579834 = 0.023248277604579926 + 0.1 * 6.221656322479248
Epoch 620, val loss: 1.0263363122940063
Epoch 630, training loss: 0.642730712890625 = 0.022079987451434135 + 0.1 * 6.206506729125977
Epoch 630, val loss: 1.0355000495910645
Epoch 640, training loss: 0.6426382660865784 = 0.02099578268826008 + 0.1 * 6.216424465179443
Epoch 640, val loss: 1.044451117515564
Epoch 650, training loss: 0.6411404609680176 = 0.019990837201476097 + 0.1 * 6.211495876312256
Epoch 650, val loss: 1.0531116724014282
Epoch 660, training loss: 0.6388806104660034 = 0.019060403108596802 + 0.1 * 6.198202133178711
Epoch 660, val loss: 1.0617626905441284
Epoch 670, training loss: 0.6385590434074402 = 0.018194206058979034 + 0.1 * 6.203648090362549
Epoch 670, val loss: 1.0701837539672852
Epoch 680, training loss: 0.6373561024665833 = 0.017386972904205322 + 0.1 * 6.199691295623779
Epoch 680, val loss: 1.0783981084823608
Epoch 690, training loss: 0.6368023157119751 = 0.01663295179605484 + 0.1 * 6.201693534851074
Epoch 690, val loss: 1.0864883661270142
Epoch 700, training loss: 0.6349614262580872 = 0.015928221866488457 + 0.1 * 6.190331935882568
Epoch 700, val loss: 1.0943958759307861
Epoch 710, training loss: 0.6349499821662903 = 0.015268449671566486 + 0.1 * 6.196815013885498
Epoch 710, val loss: 1.1021355390548706
Epoch 720, training loss: 0.6338557004928589 = 0.014653090387582779 + 0.1 * 6.192026138305664
Epoch 720, val loss: 1.1097350120544434
Epoch 730, training loss: 0.6324048042297363 = 0.01407544780522585 + 0.1 * 6.183293342590332
Epoch 730, val loss: 1.1172547340393066
Epoch 740, training loss: 0.6324684023857117 = 0.013531848788261414 + 0.1 * 6.189365386962891
Epoch 740, val loss: 1.1245782375335693
Epoch 750, training loss: 0.6311990022659302 = 0.01302012987434864 + 0.1 * 6.181788444519043
Epoch 750, val loss: 1.1316975355148315
Epoch 760, training loss: 0.6309694051742554 = 0.012540181167423725 + 0.1 * 6.184292316436768
Epoch 760, val loss: 1.1388267278671265
Epoch 770, training loss: 0.6296424865722656 = 0.012087398208677769 + 0.1 * 6.17555046081543
Epoch 770, val loss: 1.1457377672195435
Epoch 780, training loss: 0.6288849115371704 = 0.01165988203138113 + 0.1 * 6.172250270843506
Epoch 780, val loss: 1.1524966955184937
Epoch 790, training loss: 0.6292269825935364 = 0.011255156248807907 + 0.1 * 6.179718017578125
Epoch 790, val loss: 1.1590685844421387
Epoch 800, training loss: 0.6281043887138367 = 0.01087296660989523 + 0.1 * 6.172314167022705
Epoch 800, val loss: 1.165540337562561
Epoch 810, training loss: 0.6287747025489807 = 0.010511791333556175 + 0.1 * 6.182628631591797
Epoch 810, val loss: 1.1719709634780884
Epoch 820, training loss: 0.6269834041595459 = 0.010170118883252144 + 0.1 * 6.168132781982422
Epoch 820, val loss: 1.1782214641571045
Epoch 830, training loss: 0.6263530254364014 = 0.009845572523772717 + 0.1 * 6.165074348449707
Epoch 830, val loss: 1.1843386888504028
Epoch 840, training loss: 0.626754641532898 = 0.009536824189126492 + 0.1 * 6.172178268432617
Epoch 840, val loss: 1.1902885437011719
Epoch 850, training loss: 0.6260624527931213 = 0.009244098328053951 + 0.1 * 6.16818380355835
Epoch 850, val loss: 1.1961779594421387
Epoch 860, training loss: 0.6257637143135071 = 0.008965160697698593 + 0.1 * 6.167985916137695
Epoch 860, val loss: 1.201988935470581
Epoch 870, training loss: 0.6255509853363037 = 0.008699605241417885 + 0.1 * 6.168513774871826
Epoch 870, val loss: 1.2075999975204468
Epoch 880, training loss: 0.6245132684707642 = 0.008447459898889065 + 0.1 * 6.16065788269043
Epoch 880, val loss: 1.2132352590560913
Epoch 890, training loss: 0.6253913044929504 = 0.00820652674883604 + 0.1 * 6.171847343444824
Epoch 890, val loss: 1.2186942100524902
Epoch 900, training loss: 0.623724639415741 = 0.007977262139320374 + 0.1 * 6.157473564147949
Epoch 900, val loss: 1.2240647077560425
Epoch 910, training loss: 0.6224989295005798 = 0.007758038584142923 + 0.1 * 6.147408962249756
Epoch 910, val loss: 1.2293668985366821
Epoch 920, training loss: 0.6254861950874329 = 0.0075479429215192795 + 0.1 * 6.179382801055908
Epoch 920, val loss: 1.2345354557037354
Epoch 930, training loss: 0.6225991249084473 = 0.007347282487899065 + 0.1 * 6.152518272399902
Epoch 930, val loss: 1.2395799160003662
Epoch 940, training loss: 0.6219258308410645 = 0.007155958563089371 + 0.1 * 6.147698402404785
Epoch 940, val loss: 1.2446616888046265
Epoch 950, training loss: 0.6215924620628357 = 0.006972793489694595 + 0.1 * 6.146196365356445
Epoch 950, val loss: 1.2496980428695679
Epoch 960, training loss: 0.6216141581535339 = 0.0067965262569487095 + 0.1 * 6.148176193237305
Epoch 960, val loss: 1.2545011043548584
Epoch 970, training loss: 0.6213966012001038 = 0.006627876311540604 + 0.1 * 6.1476874351501465
Epoch 970, val loss: 1.2592930793762207
Epoch 980, training loss: 0.6220957040786743 = 0.006465767975896597 + 0.1 * 6.156299114227295
Epoch 980, val loss: 1.2639572620391846
Epoch 990, training loss: 0.6205256581306458 = 0.006310911383479834 + 0.1 * 6.142147064208984
Epoch 990, val loss: 1.268594741821289
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7491
Flip ASR: 0.7156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.8024234771728516 = 1.9650518894195557 + 0.1 * 8.373714447021484
Epoch 0, val loss: 1.9589483737945557
Epoch 10, training loss: 2.7914376258850098 = 1.9541176557540894 + 0.1 * 8.373198509216309
Epoch 10, val loss: 1.9473541975021362
Epoch 20, training loss: 2.777407169342041 = 1.94041109085083 + 0.1 * 8.36996078491211
Epoch 20, val loss: 1.9326398372650146
Epoch 30, training loss: 2.755640745162964 = 1.9208838939666748 + 0.1 * 8.347567558288574
Epoch 30, val loss: 1.911468267440796
Epoch 40, training loss: 2.710099220275879 = 1.8924108743667603 + 0.1 * 8.176884651184082
Epoch 40, val loss: 1.8810478448867798
Epoch 50, training loss: 2.606193780899048 = 1.857465147972107 + 0.1 * 7.487286567687988
Epoch 50, val loss: 1.8454170227050781
Epoch 60, training loss: 2.5293197631835938 = 1.820351481437683 + 0.1 * 7.089684009552002
Epoch 60, val loss: 1.8086016178131104
Epoch 70, training loss: 2.466641426086426 = 1.7795789241790771 + 0.1 * 6.8706254959106445
Epoch 70, val loss: 1.7694389820098877
Epoch 80, training loss: 2.412980556488037 = 1.7388674020767212 + 0.1 * 6.741132736206055
Epoch 80, val loss: 1.7329998016357422
Epoch 90, training loss: 2.3621480464935303 = 1.6955817937850952 + 0.1 * 6.665663242340088
Epoch 90, val loss: 1.6960690021514893
Epoch 100, training loss: 2.300780773162842 = 1.6387519836425781 + 0.1 * 6.620288372039795
Epoch 100, val loss: 1.64903724193573
Epoch 110, training loss: 2.223050117492676 = 1.5641493797302246 + 0.1 * 6.589006423950195
Epoch 110, val loss: 1.5896316766738892
Epoch 120, training loss: 2.1281116008758545 = 1.4718432426452637 + 0.1 * 6.562682628631592
Epoch 120, val loss: 1.5180240869522095
Epoch 130, training loss: 2.022498607635498 = 1.3684024810791016 + 0.1 * 6.540960311889648
Epoch 130, val loss: 1.439184546470642
Epoch 140, training loss: 1.9133338928222656 = 1.2609552145004272 + 0.1 * 6.523787021636963
Epoch 140, val loss: 1.3587883710861206
Epoch 150, training loss: 1.8040478229522705 = 1.1529805660247803 + 0.1 * 6.510672569274902
Epoch 150, val loss: 1.2791534662246704
Epoch 160, training loss: 1.6960358619689941 = 1.0459842681884766 + 0.1 * 6.500516414642334
Epoch 160, val loss: 1.201606035232544
Epoch 170, training loss: 1.5942021608352661 = 0.9456271529197693 + 0.1 * 6.485750198364258
Epoch 170, val loss: 1.1304470300674438
Epoch 180, training loss: 1.5031774044036865 = 0.8552969098091125 + 0.1 * 6.478805065155029
Epoch 180, val loss: 1.0687260627746582
Epoch 190, training loss: 1.4260696172714233 = 0.7792589664459229 + 0.1 * 6.468106269836426
Epoch 190, val loss: 1.0193736553192139
Epoch 200, training loss: 1.3611316680908203 = 0.7157493233680725 + 0.1 * 6.453823089599609
Epoch 200, val loss: 0.9798831343650818
Epoch 210, training loss: 1.3063867092132568 = 0.6610330939292908 + 0.1 * 6.453536510467529
Epoch 210, val loss: 0.9466103315353394
Epoch 220, training loss: 1.2560608386993408 = 0.6128011345863342 + 0.1 * 6.432596206665039
Epoch 220, val loss: 0.9194095134735107
Epoch 230, training loss: 1.2123291492462158 = 0.5685248374938965 + 0.1 * 6.438042640686035
Epoch 230, val loss: 0.8970903158187866
Epoch 240, training loss: 1.1685694456100464 = 0.5272826552391052 + 0.1 * 6.412868022918701
Epoch 240, val loss: 0.8782054781913757
Epoch 250, training loss: 1.128437876701355 = 0.4879225194454193 + 0.1 * 6.405153751373291
Epoch 250, val loss: 0.8618057370185852
Epoch 260, training loss: 1.0898443460464478 = 0.4504918158054352 + 0.1 * 6.39352560043335
Epoch 260, val loss: 0.8478507995605469
Epoch 270, training loss: 1.0531110763549805 = 0.414827436208725 + 0.1 * 6.382836818695068
Epoch 270, val loss: 0.8356806039810181
Epoch 280, training loss: 1.0190681219100952 = 0.3805829584598541 + 0.1 * 6.384850978851318
Epoch 280, val loss: 0.8251185417175293
Epoch 290, training loss: 0.9852725863456726 = 0.34786301851272583 + 0.1 * 6.374095439910889
Epoch 290, val loss: 0.8160372376441956
Epoch 300, training loss: 0.9533326625823975 = 0.31678780913352966 + 0.1 * 6.365448951721191
Epoch 300, val loss: 0.8084779381752014
Epoch 310, training loss: 0.9234403967857361 = 0.2875846028327942 + 0.1 * 6.35855770111084
Epoch 310, val loss: 0.8024431467056274
Epoch 320, training loss: 0.8966403007507324 = 0.26028570532798767 + 0.1 * 6.3635454177856445
Epoch 320, val loss: 0.7981943488121033
Epoch 330, training loss: 0.8699974417686462 = 0.23539066314697266 + 0.1 * 6.346067905426025
Epoch 330, val loss: 0.7958654761314392
Epoch 340, training loss: 0.8462605476379395 = 0.21287253499031067 + 0.1 * 6.333879470825195
Epoch 340, val loss: 0.7956210970878601
Epoch 350, training loss: 0.8266842365264893 = 0.19267389178276062 + 0.1 * 6.3401031494140625
Epoch 350, val loss: 0.7972556948661804
Epoch 360, training loss: 0.8070557117462158 = 0.1747893989086151 + 0.1 * 6.322663307189941
Epoch 360, val loss: 0.8005932569503784
Epoch 370, training loss: 0.7913726568222046 = 0.15896473824977875 + 0.1 * 6.3240790367126465
Epoch 370, val loss: 0.8054659962654114
Epoch 380, training loss: 0.7766637802124023 = 0.14511191844940186 + 0.1 * 6.315518379211426
Epoch 380, val loss: 0.8113648891448975
Epoch 390, training loss: 0.7633166909217834 = 0.13296212255954742 + 0.1 * 6.303545951843262
Epoch 390, val loss: 0.8182314038276672
Epoch 400, training loss: 0.7534142136573792 = 0.12223421782255173 + 0.1 * 6.311800003051758
Epoch 400, val loss: 0.82558673620224
Epoch 410, training loss: 0.7429425716400146 = 0.11275879293680191 + 0.1 * 6.30183744430542
Epoch 410, val loss: 0.8333610892295837
Epoch 420, training loss: 0.7331516742706299 = 0.10429199784994125 + 0.1 * 6.288597106933594
Epoch 420, val loss: 0.8415102362632751
Epoch 430, training loss: 0.7255334258079529 = 0.09665138274431229 + 0.1 * 6.288820266723633
Epoch 430, val loss: 0.8497713804244995
Epoch 440, training loss: 0.7191389203071594 = 0.08974967151880264 + 0.1 * 6.293891906738281
Epoch 440, val loss: 0.8580837845802307
Epoch 450, training loss: 0.7116252779960632 = 0.08345671743154526 + 0.1 * 6.281685829162598
Epoch 450, val loss: 0.8663927316665649
Epoch 460, training loss: 0.7047759294509888 = 0.07766887545585632 + 0.1 * 6.271070957183838
Epoch 460, val loss: 0.8747484683990479
Epoch 470, training loss: 0.7019466757774353 = 0.07231546938419342 + 0.1 * 6.29631233215332
Epoch 470, val loss: 0.8830769658088684
Epoch 480, training loss: 0.6949125528335571 = 0.06732099503278732 + 0.1 * 6.275915622711182
Epoch 480, val loss: 0.8911076188087463
Epoch 490, training loss: 0.6882702708244324 = 0.06258173286914825 + 0.1 * 6.256885528564453
Epoch 490, val loss: 0.8991352915763855
Epoch 500, training loss: 0.6835120916366577 = 0.05805609002709389 + 0.1 * 6.254559516906738
Epoch 500, val loss: 0.9070356488227844
Epoch 510, training loss: 0.6785670518875122 = 0.05375779792666435 + 0.1 * 6.2480926513671875
Epoch 510, val loss: 0.914919912815094
Epoch 520, training loss: 0.6755126714706421 = 0.04975280165672302 + 0.1 * 6.257598876953125
Epoch 520, val loss: 0.9229018092155457
Epoch 530, training loss: 0.670445442199707 = 0.04609403386712074 + 0.1 * 6.243513584136963
Epoch 530, val loss: 0.9311779141426086
Epoch 540, training loss: 0.6663655042648315 = 0.04280794411897659 + 0.1 * 6.2355756759643555
Epoch 540, val loss: 0.940001904964447
Epoch 550, training loss: 0.6669027805328369 = 0.03986293822526932 + 0.1 * 6.270398139953613
Epoch 550, val loss: 0.9490740895271301
Epoch 560, training loss: 0.6603454351425171 = 0.03724472597241402 + 0.1 * 6.231007099151611
Epoch 560, val loss: 0.9582175016403198
Epoch 570, training loss: 0.658778727054596 = 0.03488924726843834 + 0.1 * 6.238894939422607
Epoch 570, val loss: 0.9672867655754089
Epoch 580, training loss: 0.6549862623214722 = 0.032761100679636 + 0.1 * 6.2222514152526855
Epoch 580, val loss: 0.976104736328125
Epoch 590, training loss: 0.656309962272644 = 0.030824245885014534 + 0.1 * 6.254857063293457
Epoch 590, val loss: 0.9847961664199829
Epoch 600, training loss: 0.6515663862228394 = 0.029067721217870712 + 0.1 * 6.224987030029297
Epoch 600, val loss: 0.9933053255081177
Epoch 610, training loss: 0.6494590640068054 = 0.02746240794658661 + 0.1 * 6.219966411590576
Epoch 610, val loss: 1.0016734600067139
Epoch 620, training loss: 0.6478219032287598 = 0.02598688006401062 + 0.1 * 6.218350410461426
Epoch 620, val loss: 1.0099977254867554
Epoch 630, training loss: 0.6458015441894531 = 0.02462881989777088 + 0.1 * 6.211727142333984
Epoch 630, val loss: 1.01819908618927
Epoch 640, training loss: 0.6456012725830078 = 0.023379141464829445 + 0.1 * 6.2222208976745605
Epoch 640, val loss: 1.026314616203308
Epoch 650, training loss: 0.6428436040878296 = 0.022225569933652878 + 0.1 * 6.206180572509766
Epoch 650, val loss: 1.0342199802398682
Epoch 660, training loss: 0.6430687308311462 = 0.021152926608920097 + 0.1 * 6.219158172607422
Epoch 660, val loss: 1.0420948266983032
Epoch 670, training loss: 0.6405786871910095 = 0.020156489685177803 + 0.1 * 6.204221725463867
Epoch 670, val loss: 1.0498151779174805
Epoch 680, training loss: 0.6389275789260864 = 0.019234513863921165 + 0.1 * 6.196930408477783
Epoch 680, val loss: 1.0573511123657227
Epoch 690, training loss: 0.6414432525634766 = 0.0183724295347929 + 0.1 * 6.230708122253418
Epoch 690, val loss: 1.0648232698440552
Epoch 700, training loss: 0.6376374363899231 = 0.017573770135641098 + 0.1 * 6.200636386871338
Epoch 700, val loss: 1.0721068382263184
Epoch 710, training loss: 0.6357303857803345 = 0.0168302059173584 + 0.1 * 6.189001560211182
Epoch 710, val loss: 1.0792292356491089
Epoch 720, training loss: 0.6351758241653442 = 0.016133379191160202 + 0.1 * 6.19042444229126
Epoch 720, val loss: 1.086342453956604
Epoch 730, training loss: 0.6346582174301147 = 0.015480258502066135 + 0.1 * 6.191779136657715
Epoch 730, val loss: 1.0933018922805786
Epoch 740, training loss: 0.6330121755599976 = 0.014870659448206425 + 0.1 * 6.18141508102417
Epoch 740, val loss: 1.1000378131866455
Epoch 750, training loss: 0.6347886919975281 = 0.014296948909759521 + 0.1 * 6.2049174308776855
Epoch 750, val loss: 1.1067863702774048
Epoch 760, training loss: 0.6323351263999939 = 0.013758323155343533 + 0.1 * 6.185768127441406
Epoch 760, val loss: 1.1133110523223877
Epoch 770, training loss: 0.6318390369415283 = 0.013250824064016342 + 0.1 * 6.185882091522217
Epoch 770, val loss: 1.1196820735931396
Epoch 780, training loss: 0.6300089359283447 = 0.01277181413024664 + 0.1 * 6.1723713874816895
Epoch 780, val loss: 1.1260647773742676
Epoch 790, training loss: 0.6294930577278137 = 0.01232239045202732 + 0.1 * 6.171706676483154
Epoch 790, val loss: 1.13214111328125
Epoch 800, training loss: 0.6300281882286072 = 0.011896887794137001 + 0.1 * 6.181312561035156
Epoch 800, val loss: 1.1382761001586914
Epoch 810, training loss: 0.6287842392921448 = 0.011494013480842113 + 0.1 * 6.1729021072387695
Epoch 810, val loss: 1.1443005800247192
Epoch 820, training loss: 0.6280152201652527 = 0.011114105582237244 + 0.1 * 6.169010639190674
Epoch 820, val loss: 1.1501593589782715
Epoch 830, training loss: 0.6270176768302917 = 0.010753873735666275 + 0.1 * 6.162638187408447
Epoch 830, val loss: 1.1559350490570068
Epoch 840, training loss: 0.6272311210632324 = 0.010411703027784824 + 0.1 * 6.16819429397583
Epoch 840, val loss: 1.161657691001892
Epoch 850, training loss: 0.6272782683372498 = 0.0100904181599617 + 0.1 * 6.171878337860107
Epoch 850, val loss: 1.1671191453933716
Epoch 860, training loss: 0.6257977485656738 = 0.009784869849681854 + 0.1 * 6.160128593444824
Epoch 860, val loss: 1.1725356578826904
Epoch 870, training loss: 0.6258993148803711 = 0.009494408033788204 + 0.1 * 6.164048671722412
Epoch 870, val loss: 1.1778593063354492
Epoch 880, training loss: 0.6267794370651245 = 0.009216777980327606 + 0.1 * 6.175626277923584
Epoch 880, val loss: 1.1831660270690918
Epoch 890, training loss: 0.624942421913147 = 0.008953830227255821 + 0.1 * 6.159885883331299
Epoch 890, val loss: 1.1883212327957153
Epoch 900, training loss: 0.6240063905715942 = 0.00870215892791748 + 0.1 * 6.153042316436768
Epoch 900, val loss: 1.1934348344802856
Epoch 910, training loss: 0.6247802376747131 = 0.008461659774184227 + 0.1 * 6.1631855964660645
Epoch 910, val loss: 1.1985418796539307
Epoch 920, training loss: 0.6235363483428955 = 0.008232664316892624 + 0.1 * 6.153037071228027
Epoch 920, val loss: 1.203424096107483
Epoch 930, training loss: 0.6231483221054077 = 0.008014178834855556 + 0.1 * 6.151340961456299
Epoch 930, val loss: 1.2082802057266235
Epoch 940, training loss: 0.6235799193382263 = 0.007804482709616423 + 0.1 * 6.157754421234131
Epoch 940, val loss: 1.2131414413452148
Epoch 950, training loss: 0.6223257184028625 = 0.007604462560266256 + 0.1 * 6.147212505340576
Epoch 950, val loss: 1.2178610563278198
Epoch 960, training loss: 0.6232874989509583 = 0.007412401959300041 + 0.1 * 6.158751010894775
Epoch 960, val loss: 1.2225749492645264
Epoch 970, training loss: 0.6218945980072021 = 0.0072283367626369 + 0.1 * 6.146662712097168
Epoch 970, val loss: 1.2271764278411865
Epoch 980, training loss: 0.6214603781700134 = 0.00705269630998373 + 0.1 * 6.144076347351074
Epoch 980, val loss: 1.2317042350769043
Epoch 990, training loss: 0.621874213218689 = 0.006882977206259966 + 0.1 * 6.149912357330322
Epoch 990, val loss: 1.2362672090530396
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6900
Flip ASR: 0.6444/225 nodes
The final ASR:0.66421, 0.08190, Accuracy:0.80123, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10506])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98032, 0.01141, Accuracy:0.83210, 0.00924
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7968974113464355 = 1.9595227241516113 + 0.1 * 8.373746871948242
Epoch 0, val loss: 1.9615135192871094
Epoch 10, training loss: 2.7865242958068848 = 1.94917631149292 + 0.1 * 8.373478889465332
Epoch 10, val loss: 1.9516538381576538
Epoch 20, training loss: 2.7734603881835938 = 1.936265468597412 + 0.1 * 8.371949195861816
Epoch 20, val loss: 1.938944935798645
Epoch 30, training loss: 2.7539403438568115 = 1.9178155660629272 + 0.1 * 8.361248016357422
Epoch 30, val loss: 1.920533537864685
Epoch 40, training loss: 2.720376968383789 = 1.890230417251587 + 0.1 * 8.30146598815918
Epoch 40, val loss: 1.8934438228607178
Epoch 50, training loss: 2.6501526832580566 = 1.8533127307891846 + 0.1 * 7.9683990478515625
Epoch 50, val loss: 1.8592897653579712
Epoch 60, training loss: 2.568878650665283 = 1.8123528957366943 + 0.1 * 7.565256118774414
Epoch 60, val loss: 1.8235641717910767
Epoch 70, training loss: 2.487874984741211 = 1.773681402206421 + 0.1 * 7.141936302185059
Epoch 70, val loss: 1.7919728755950928
Epoch 80, training loss: 2.4214062690734863 = 1.7383382320404053 + 0.1 * 6.830679416656494
Epoch 80, val loss: 1.7624447345733643
Epoch 90, training loss: 2.3647594451904297 = 1.6949326992034912 + 0.1 * 6.698266983032227
Epoch 90, val loss: 1.722644329071045
Epoch 100, training loss: 2.301957368850708 = 1.637305498123169 + 0.1 * 6.646518707275391
Epoch 100, val loss: 1.6716582775115967
Epoch 110, training loss: 2.224801540374756 = 1.56330406665802 + 0.1 * 6.614974021911621
Epoch 110, val loss: 1.6095739603042603
Epoch 120, training loss: 2.13506817817688 = 1.475178599357605 + 0.1 * 6.598896503448486
Epoch 120, val loss: 1.5371354818344116
Epoch 130, training loss: 2.0386054515838623 = 1.379616379737854 + 0.1 * 6.589890956878662
Epoch 130, val loss: 1.4599934816360474
Epoch 140, training loss: 1.9398689270019531 = 1.2816762924194336 + 0.1 * 6.581926345825195
Epoch 140, val loss: 1.3816853761672974
Epoch 150, training loss: 1.8391883373260498 = 1.1819862127304077 + 0.1 * 6.572020530700684
Epoch 150, val loss: 1.3019585609436035
Epoch 160, training loss: 1.737412452697754 = 1.0814038515090942 + 0.1 * 6.560085296630859
Epoch 160, val loss: 1.2207213640213013
Epoch 170, training loss: 1.6384243965148926 = 0.983313262462616 + 0.1 * 6.551111221313477
Epoch 170, val loss: 1.141579031944275
Epoch 180, training loss: 1.545573115348816 = 0.8915906548500061 + 0.1 * 6.539824485778809
Epoch 180, val loss: 1.0693624019622803
Epoch 190, training loss: 1.4596056938171387 = 0.8065426349639893 + 0.1 * 6.530630588531494
Epoch 190, val loss: 1.003925085067749
Epoch 200, training loss: 1.3809473514556885 = 0.7279770374298096 + 0.1 * 6.529703617095947
Epoch 200, val loss: 0.9454696178436279
Epoch 210, training loss: 1.3082506656646729 = 0.6564275026321411 + 0.1 * 6.5182318687438965
Epoch 210, val loss: 0.8941648602485657
Epoch 220, training loss: 1.2423102855682373 = 0.5912047624588013 + 0.1 * 6.5110554695129395
Epoch 220, val loss: 0.8498398661613464
Epoch 230, training loss: 1.1824393272399902 = 0.5316244959831238 + 0.1 * 6.508148670196533
Epoch 230, val loss: 0.8119811415672302
Epoch 240, training loss: 1.1274322271347046 = 0.4774218499660492 + 0.1 * 6.500103950500488
Epoch 240, val loss: 0.7804614305496216
Epoch 250, training loss: 1.0769531726837158 = 0.42751264572143555 + 0.1 * 6.494405746459961
Epoch 250, val loss: 0.7539957761764526
Epoch 260, training loss: 1.0298607349395752 = 0.3811347484588623 + 0.1 * 6.4872589111328125
Epoch 260, val loss: 0.7316852807998657
Epoch 270, training loss: 0.9860021471977234 = 0.33780622482299805 + 0.1 * 6.481958866119385
Epoch 270, val loss: 0.7129221558570862
Epoch 280, training loss: 0.9449539184570312 = 0.2975834012031555 + 0.1 * 6.473705291748047
Epoch 280, val loss: 0.6973930597305298
Epoch 290, training loss: 0.9073335528373718 = 0.26069021224975586 + 0.1 * 6.466433048248291
Epoch 290, val loss: 0.6852058172225952
Epoch 300, training loss: 0.8735696077346802 = 0.22757521271705627 + 0.1 * 6.4599432945251465
Epoch 300, val loss: 0.6764126420021057
Epoch 310, training loss: 0.8440658450126648 = 0.19871844351291656 + 0.1 * 6.453474044799805
Epoch 310, val loss: 0.6708625555038452
Epoch 320, training loss: 0.8189041614532471 = 0.174000546336174 + 0.1 * 6.449036121368408
Epoch 320, val loss: 0.6686310172080994
Epoch 330, training loss: 0.7975093126296997 = 0.1531454473733902 + 0.1 * 6.443638801574707
Epoch 330, val loss: 0.6691069006919861
Epoch 340, training loss: 0.7790510654449463 = 0.13556194305419922 + 0.1 * 6.434891223907471
Epoch 340, val loss: 0.6717619895935059
Epoch 350, training loss: 0.76363205909729 = 0.1206517219543457 + 0.1 * 6.429803371429443
Epoch 350, val loss: 0.6762192845344543
Epoch 360, training loss: 0.7513248324394226 = 0.10799376666545868 + 0.1 * 6.433310508728027
Epoch 360, val loss: 0.6820099353790283
Epoch 370, training loss: 0.7390207648277283 = 0.09721381217241287 + 0.1 * 6.418069362640381
Epoch 370, val loss: 0.6885455846786499
Epoch 380, training loss: 0.7287526726722717 = 0.0878906399011612 + 0.1 * 6.408620357513428
Epoch 380, val loss: 0.6957219243049622
Epoch 390, training loss: 0.7207708954811096 = 0.07974850386381149 + 0.1 * 6.410223484039307
Epoch 390, val loss: 0.7032869458198547
Epoch 400, training loss: 0.7125975489616394 = 0.0726379007101059 + 0.1 * 6.399596214294434
Epoch 400, val loss: 0.711132824420929
Epoch 410, training loss: 0.7062495946884155 = 0.06640397757291794 + 0.1 * 6.39845609664917
Epoch 410, val loss: 0.7190132141113281
Epoch 420, training loss: 0.6991726756095886 = 0.06090308725833893 + 0.1 * 6.382696151733398
Epoch 420, val loss: 0.7269111275672913
Epoch 430, training loss: 0.6931958794593811 = 0.05599573999643326 + 0.1 * 6.3720011711120605
Epoch 430, val loss: 0.7349034547805786
Epoch 440, training loss: 0.6900309920310974 = 0.05160830542445183 + 0.1 * 6.384226322174072
Epoch 440, val loss: 0.7430261969566345
Epoch 450, training loss: 0.6840776801109314 = 0.04771512746810913 + 0.1 * 6.363625526428223
Epoch 450, val loss: 0.7510308027267456
Epoch 460, training loss: 0.6795592308044434 = 0.04421631991863251 + 0.1 * 6.353428840637207
Epoch 460, val loss: 0.7589001655578613
Epoch 470, training loss: 0.6778368949890137 = 0.04105836898088455 + 0.1 * 6.3677849769592285
Epoch 470, val loss: 0.766918957233429
Epoch 480, training loss: 0.6727141737937927 = 0.038223739713430405 + 0.1 * 6.34490442276001
Epoch 480, val loss: 0.774861216545105
Epoch 490, training loss: 0.6686075329780579 = 0.03565634414553642 + 0.1 * 6.329511642456055
Epoch 490, val loss: 0.7825490236282349
Epoch 500, training loss: 0.666043221950531 = 0.03332391753792763 + 0.1 * 6.327192783355713
Epoch 500, val loss: 0.7902764678001404
Epoch 510, training loss: 0.6629923582077026 = 0.0312042273581028 + 0.1 * 6.317881107330322
Epoch 510, val loss: 0.7979675531387329
Epoch 520, training loss: 0.6617165207862854 = 0.029271269217133522 + 0.1 * 6.324452877044678
Epoch 520, val loss: 0.8054894208908081
Epoch 530, training loss: 0.6579018831253052 = 0.027512166649103165 + 0.1 * 6.303896903991699
Epoch 530, val loss: 0.812932014465332
Epoch 540, training loss: 0.6561505198478699 = 0.02590051107108593 + 0.1 * 6.302500247955322
Epoch 540, val loss: 0.8201200366020203
Epoch 550, training loss: 0.652701735496521 = 0.02443186193704605 + 0.1 * 6.282698154449463
Epoch 550, val loss: 0.8273676037788391
Epoch 560, training loss: 0.6507449150085449 = 0.023082947358489037 + 0.1 * 6.2766194343566895
Epoch 560, val loss: 0.8343117237091064
Epoch 570, training loss: 0.6508728861808777 = 0.021842336282134056 + 0.1 * 6.290305137634277
Epoch 570, val loss: 0.8411180377006531
Epoch 580, training loss: 0.6471225619316101 = 0.02070624567568302 + 0.1 * 6.264163017272949
Epoch 580, val loss: 0.8479148745536804
Epoch 590, training loss: 0.6456537246704102 = 0.019654642790555954 + 0.1 * 6.259990692138672
Epoch 590, val loss: 0.8543543815612793
Epoch 600, training loss: 0.6440730094909668 = 0.01868053339421749 + 0.1 * 6.25392484664917
Epoch 600, val loss: 0.8608326315879822
Epoch 610, training loss: 0.6425764560699463 = 0.017781579867005348 + 0.1 * 6.24794864654541
Epoch 610, val loss: 0.8672690987586975
Epoch 620, training loss: 0.6420673727989197 = 0.01694614812731743 + 0.1 * 6.251212120056152
Epoch 620, val loss: 0.8733603358268738
Epoch 630, training loss: 0.6402329802513123 = 0.0161683801561594 + 0.1 * 6.240645885467529
Epoch 630, val loss: 0.8794502019882202
Epoch 640, training loss: 0.6392244100570679 = 0.015446598641574383 + 0.1 * 6.2377777099609375
Epoch 640, val loss: 0.8855523467063904
Epoch 650, training loss: 0.6386133432388306 = 0.014771940186619759 + 0.1 * 6.2384138107299805
Epoch 650, val loss: 0.8913434147834778
Epoch 660, training loss: 0.6367242932319641 = 0.014142759144306183 + 0.1 * 6.225815296173096
Epoch 660, val loss: 0.8970863819122314
Epoch 670, training loss: 0.6364913582801819 = 0.013554534874856472 + 0.1 * 6.229368209838867
Epoch 670, val loss: 0.9027577638626099
Epoch 680, training loss: 0.6378767490386963 = 0.013002444989979267 + 0.1 * 6.248742580413818
Epoch 680, val loss: 0.9082581400871277
Epoch 690, training loss: 0.6340757012367249 = 0.01248656865209341 + 0.1 * 6.215891361236572
Epoch 690, val loss: 0.9137731194496155
Epoch 700, training loss: 0.6342893242835999 = 0.01200366485863924 + 0.1 * 6.222856521606445
Epoch 700, val loss: 0.9190635681152344
Epoch 710, training loss: 0.6322420239448547 = 0.011548968963325024 + 0.1 * 6.206930160522461
Epoch 710, val loss: 0.9242270588874817
Epoch 720, training loss: 0.6316211223602295 = 0.011122513562440872 + 0.1 * 6.204986095428467
Epoch 720, val loss: 0.929388165473938
Epoch 730, training loss: 0.6316277980804443 = 0.010719386860728264 + 0.1 * 6.2090840339660645
Epoch 730, val loss: 0.9343775510787964
Epoch 740, training loss: 0.6304711699485779 = 0.010339095257222652 + 0.1 * 6.201320648193359
Epoch 740, val loss: 0.9393936395645142
Epoch 750, training loss: 0.6297858953475952 = 0.009981430135667324 + 0.1 * 6.198044300079346
Epoch 750, val loss: 0.9443214535713196
Epoch 760, training loss: 0.6294946074485779 = 0.0096428282558918 + 0.1 * 6.198517322540283
Epoch 760, val loss: 0.94903564453125
Epoch 770, training loss: 0.6300907135009766 = 0.009322451427578926 + 0.1 * 6.207682132720947
Epoch 770, val loss: 0.9537386298179626
Epoch 780, training loss: 0.6281907558441162 = 0.009019190445542336 + 0.1 * 6.191715240478516
Epoch 780, val loss: 0.9583820700645447
Epoch 790, training loss: 0.6272919774055481 = 0.00873261597007513 + 0.1 * 6.185593605041504
Epoch 790, val loss: 0.9629687070846558
Epoch 800, training loss: 0.6297452449798584 = 0.00846009235829115 + 0.1 * 6.212851524353027
Epoch 800, val loss: 0.9673985242843628
Epoch 810, training loss: 0.6264916658401489 = 0.008200528100132942 + 0.1 * 6.182911396026611
Epoch 810, val loss: 0.9718380570411682
Epoch 820, training loss: 0.626173734664917 = 0.007955355569720268 + 0.1 * 6.182183265686035
Epoch 820, val loss: 0.9762226343154907
Epoch 830, training loss: 0.6254799962043762 = 0.007721183821558952 + 0.1 * 6.177587985992432
Epoch 830, val loss: 0.9804272651672363
Epoch 840, training loss: 0.6251307129859924 = 0.007498410064727068 + 0.1 * 6.176322937011719
Epoch 840, val loss: 0.9846564531326294
Epoch 850, training loss: 0.624352216720581 = 0.007286697160452604 + 0.1 * 6.170655250549316
Epoch 850, val loss: 0.9887896776199341
Epoch 860, training loss: 0.6239825487136841 = 0.00708401296287775 + 0.1 * 6.168985366821289
Epoch 860, val loss: 0.9928436279296875
Epoch 870, training loss: 0.6236245036125183 = 0.006889997981488705 + 0.1 * 6.16734504699707
Epoch 870, val loss: 0.9968773722648621
Epoch 880, training loss: 0.6242020130157471 = 0.006704298779368401 + 0.1 * 6.1749773025512695
Epoch 880, val loss: 1.0008398294448853
Epoch 890, training loss: 0.6226927042007446 = 0.006527627818286419 + 0.1 * 6.161650657653809
Epoch 890, val loss: 1.0048080682754517
Epoch 900, training loss: 0.6219254732131958 = 0.006359150167554617 + 0.1 * 6.155663013458252
Epoch 900, val loss: 1.0086205005645752
Epoch 910, training loss: 0.6218336224555969 = 0.0061971768736839294 + 0.1 * 6.156364440917969
Epoch 910, val loss: 1.0122637748718262
Epoch 920, training loss: 0.6213067770004272 = 0.006042574532330036 + 0.1 * 6.152641773223877
Epoch 920, val loss: 1.0159974098205566
Epoch 930, training loss: 0.623295247554779 = 0.005894704721868038 + 0.1 * 6.174005031585693
Epoch 930, val loss: 1.0196597576141357
Epoch 940, training loss: 0.6216027736663818 = 0.005752267315983772 + 0.1 * 6.158504962921143
Epoch 940, val loss: 1.023193597793579
Epoch 950, training loss: 0.62079918384552 = 0.005616342183202505 + 0.1 * 6.151827812194824
Epoch 950, val loss: 1.0267722606658936
Epoch 960, training loss: 0.6211287379264832 = 0.005485278554260731 + 0.1 * 6.156434535980225
Epoch 960, val loss: 1.0301929712295532
Epoch 970, training loss: 0.6199682950973511 = 0.0053595686331391335 + 0.1 * 6.146087169647217
Epoch 970, val loss: 1.0336499214172363
Epoch 980, training loss: 0.6203400492668152 = 0.005238778423517942 + 0.1 * 6.151012420654297
Epoch 980, val loss: 1.0369880199432373
Epoch 990, training loss: 0.6194493174552917 = 0.005122209899127483 + 0.1 * 6.143270969390869
Epoch 990, val loss: 1.0403366088867188
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7769503593444824 = 1.9395642280578613 + 0.1 * 8.373862266540527
Epoch 0, val loss: 1.9352387189865112
Epoch 10, training loss: 2.7669620513916016 = 1.9295868873596191 + 0.1 * 8.37375259399414
Epoch 10, val loss: 1.9263312816619873
Epoch 20, training loss: 2.7549073696136475 = 1.9176005125045776 + 0.1 * 8.373068809509277
Epoch 20, val loss: 1.9155205488204956
Epoch 30, training loss: 2.737889289855957 = 1.9011411666870117 + 0.1 * 8.36748218536377
Epoch 30, val loss: 1.900696039199829
Epoch 40, training loss: 2.709536552429199 = 1.8772518634796143 + 0.1 * 8.322846412658691
Epoch 40, val loss: 1.8795380592346191
Epoch 50, training loss: 2.641732931137085 = 1.844990849494934 + 0.1 * 7.967421054840088
Epoch 50, val loss: 1.8519253730773926
Epoch 60, training loss: 2.5708956718444824 = 1.810298204421997 + 0.1 * 7.605975151062012
Epoch 60, val loss: 1.8238294124603271
Epoch 70, training loss: 2.4925129413604736 = 1.775899052619934 + 0.1 * 7.166139125823975
Epoch 70, val loss: 1.7970330715179443
Epoch 80, training loss: 2.428056001663208 = 1.7408620119094849 + 0.1 * 6.871940612792969
Epoch 80, val loss: 1.7692370414733887
Epoch 90, training loss: 2.372133255004883 = 1.6955009698867798 + 0.1 * 6.766323089599609
Epoch 90, val loss: 1.7293187379837036
Epoch 100, training loss: 2.303370952606201 = 1.6330609321594238 + 0.1 * 6.703099727630615
Epoch 100, val loss: 1.674436092376709
Epoch 110, training loss: 2.2177555561065674 = 1.551071286201477 + 0.1 * 6.666841983795166
Epoch 110, val loss: 1.6065902709960938
Epoch 120, training loss: 2.1174404621124268 = 1.4532133340835571 + 0.1 * 6.642270565032959
Epoch 120, val loss: 1.5270721912384033
Epoch 130, training loss: 2.0102765560150146 = 1.3481241464614868 + 0.1 * 6.621523857116699
Epoch 130, val loss: 1.442448377609253
Epoch 140, training loss: 1.9025561809539795 = 1.2424414157867432 + 0.1 * 6.601147174835205
Epoch 140, val loss: 1.359181523323059
Epoch 150, training loss: 1.8011670112609863 = 1.142322301864624 + 0.1 * 6.588447093963623
Epoch 150, val loss: 1.281929850578308
Epoch 160, training loss: 1.705512523651123 = 1.048511266708374 + 0.1 * 6.570013046264648
Epoch 160, val loss: 1.2110871076583862
Epoch 170, training loss: 1.616865634918213 = 0.9607333540916443 + 0.1 * 6.5613226890563965
Epoch 170, val loss: 1.1456170082092285
Epoch 180, training loss: 1.5348682403564453 = 0.8806366920471191 + 0.1 * 6.5423150062561035
Epoch 180, val loss: 1.0873090028762817
Epoch 190, training loss: 1.4606099128723145 = 0.8070559501647949 + 0.1 * 6.535539150238037
Epoch 190, val loss: 1.0345879793167114
Epoch 200, training loss: 1.391357421875 = 0.7393484115600586 + 0.1 * 6.520089149475098
Epoch 200, val loss: 0.9872528910636902
Epoch 210, training loss: 1.3275117874145508 = 0.6760231852531433 + 0.1 * 6.514885902404785
Epoch 210, val loss: 0.9436323046684265
Epoch 220, training loss: 1.266308307647705 = 0.6166571378707886 + 0.1 * 6.496510982513428
Epoch 220, val loss: 0.9038221836090088
Epoch 230, training loss: 1.209718942642212 = 0.5605365633964539 + 0.1 * 6.491823196411133
Epoch 230, val loss: 0.8670558333396912
Epoch 240, training loss: 1.1561226844787598 = 0.5086222887039185 + 0.1 * 6.475003242492676
Epoch 240, val loss: 0.8342098593711853
Epoch 250, training loss: 1.107743501663208 = 0.4610801339149475 + 0.1 * 6.466633319854736
Epoch 250, val loss: 0.8057907223701477
Epoch 260, training loss: 1.064862608909607 = 0.4180918037891388 + 0.1 * 6.467708110809326
Epoch 260, val loss: 0.7819415926933289
Epoch 270, training loss: 1.0236694812774658 = 0.3794199824333191 + 0.1 * 6.442495346069336
Epoch 270, val loss: 0.7625195384025574
Epoch 280, training loss: 0.9874716401100159 = 0.34416407346725464 + 0.1 * 6.433075428009033
Epoch 280, val loss: 0.7462564706802368
Epoch 290, training loss: 0.9562745094299316 = 0.3119717240333557 + 0.1 * 6.443027496337891
Epoch 290, val loss: 0.7326273322105408
Epoch 300, training loss: 0.9241887927055359 = 0.2825682759284973 + 0.1 * 6.416204929351807
Epoch 300, val loss: 0.7214036583900452
Epoch 310, training loss: 0.8966802954673767 = 0.2555568814277649 + 0.1 * 6.411233901977539
Epoch 310, val loss: 0.7117525339126587
Epoch 320, training loss: 0.8716332912445068 = 0.23090966045856476 + 0.1 * 6.407236576080322
Epoch 320, val loss: 0.7040397524833679
Epoch 330, training loss: 0.84813392162323 = 0.20840153098106384 + 0.1 * 6.397324085235596
Epoch 330, val loss: 0.6978265643119812
Epoch 340, training loss: 0.8275338411331177 = 0.18807686865329742 + 0.1 * 6.3945698738098145
Epoch 340, val loss: 0.6932833194732666
Epoch 350, training loss: 0.807537317276001 = 0.16975682973861694 + 0.1 * 6.377804756164551
Epoch 350, val loss: 0.6903190612792969
Epoch 360, training loss: 0.793086051940918 = 0.15321771800518036 + 0.1 * 6.398683071136475
Epoch 360, val loss: 0.6887595653533936
Epoch 370, training loss: 0.7751510143280029 = 0.13842663168907166 + 0.1 * 6.36724328994751
Epoch 370, val loss: 0.6886774897575378
Epoch 380, training loss: 0.761381208896637 = 0.12516415119171143 + 0.1 * 6.362170219421387
Epoch 380, val loss: 0.689858078956604
Epoch 390, training loss: 0.7495980858802795 = 0.11329192668199539 + 0.1 * 6.363061428070068
Epoch 390, val loss: 0.6921912431716919
Epoch 400, training loss: 0.7379767298698425 = 0.10273100435733795 + 0.1 * 6.352457523345947
Epoch 400, val loss: 0.6955671310424805
Epoch 410, training loss: 0.7296140789985657 = 0.09333807229995728 + 0.1 * 6.362760066986084
Epoch 410, val loss: 0.6998438239097595
Epoch 420, training loss: 0.7188851833343506 = 0.08502180129289627 + 0.1 * 6.338634014129639
Epoch 420, val loss: 0.7047972679138184
Epoch 430, training loss: 0.7105066776275635 = 0.0776311531662941 + 0.1 * 6.328754901885986
Epoch 430, val loss: 0.7104254364967346
Epoch 440, training loss: 0.7044804692268372 = 0.0710456520318985 + 0.1 * 6.334348201751709
Epoch 440, val loss: 0.7165622115135193
Epoch 450, training loss: 0.6974270939826965 = 0.06519322097301483 + 0.1 * 6.322338581085205
Epoch 450, val loss: 0.723158597946167
Epoch 460, training loss: 0.6913896799087524 = 0.05997329577803612 + 0.1 * 6.314163684844971
Epoch 460, val loss: 0.7300617694854736
Epoch 470, training loss: 0.6869802474975586 = 0.05531246215105057 + 0.1 * 6.316678047180176
Epoch 470, val loss: 0.7372018694877625
Epoch 480, training loss: 0.681458592414856 = 0.051159344613552094 + 0.1 * 6.302992343902588
Epoch 480, val loss: 0.744535505771637
Epoch 490, training loss: 0.6776301860809326 = 0.04742962121963501 + 0.1 * 6.302005767822266
Epoch 490, val loss: 0.7520145773887634
Epoch 500, training loss: 0.6743692755699158 = 0.04407934471964836 + 0.1 * 6.30289888381958
Epoch 500, val loss: 0.7595415711402893
Epoch 510, training loss: 0.669760525226593 = 0.04106788709759712 + 0.1 * 6.28692626953125
Epoch 510, val loss: 0.7671173810958862
Epoch 520, training loss: 0.6662680506706238 = 0.0383438915014267 + 0.1 * 6.279241561889648
Epoch 520, val loss: 0.7747218608856201
Epoch 530, training loss: 0.6647048592567444 = 0.03587168827652931 + 0.1 * 6.288331508636475
Epoch 530, val loss: 0.7822701334953308
Epoch 540, training loss: 0.6613232493400574 = 0.03363736346364021 + 0.1 * 6.276858806610107
Epoch 540, val loss: 0.789777934551239
Epoch 550, training loss: 0.658297061920166 = 0.03160187602043152 + 0.1 * 6.266951560974121
Epoch 550, val loss: 0.7972357869148254
Epoch 560, training loss: 0.6582003831863403 = 0.02973981574177742 + 0.1 * 6.284605979919434
Epoch 560, val loss: 0.8046175837516785
Epoch 570, training loss: 0.653853178024292 = 0.02804064378142357 + 0.1 * 6.258125305175781
Epoch 570, val loss: 0.8119409680366516
Epoch 580, training loss: 0.6527960300445557 = 0.026481445878744125 + 0.1 * 6.263145923614502
Epoch 580, val loss: 0.8191563487052917
Epoch 590, training loss: 0.6505867838859558 = 0.025050820782780647 + 0.1 * 6.255359649658203
Epoch 590, val loss: 0.8262397050857544
Epoch 600, training loss: 0.6486013531684875 = 0.02373730204999447 + 0.1 * 6.248640537261963
Epoch 600, val loss: 0.8331696391105652
Epoch 610, training loss: 0.647787868976593 = 0.022531040012836456 + 0.1 * 6.252568244934082
Epoch 610, val loss: 0.8400582075119019
Epoch 620, training loss: 0.6465272307395935 = 0.021414509043097496 + 0.1 * 6.251127243041992
Epoch 620, val loss: 0.8468295335769653
Epoch 630, training loss: 0.6442880630493164 = 0.020381063222885132 + 0.1 * 6.239070415496826
Epoch 630, val loss: 0.8534718751907349
Epoch 640, training loss: 0.6439245343208313 = 0.01942131109535694 + 0.1 * 6.245031833648682
Epoch 640, val loss: 0.8600013256072998
Epoch 650, training loss: 0.6431061029434204 = 0.018530547618865967 + 0.1 * 6.245755195617676
Epoch 650, val loss: 0.8664346933364868
Epoch 660, training loss: 0.6405493021011353 = 0.017702728509902954 + 0.1 * 6.228466033935547
Epoch 660, val loss: 0.8727433085441589
Epoch 670, training loss: 0.6399431824684143 = 0.016932541504502296 + 0.1 * 6.230106353759766
Epoch 670, val loss: 0.8789721727371216
Epoch 680, training loss: 0.6390116810798645 = 0.016212832182645798 + 0.1 * 6.227988243103027
Epoch 680, val loss: 0.8850184679031372
Epoch 690, training loss: 0.6377988457679749 = 0.015541110187768936 + 0.1 * 6.222577095031738
Epoch 690, val loss: 0.8910002708435059
Epoch 700, training loss: 0.6363286375999451 = 0.014912430197000504 + 0.1 * 6.2141618728637695
Epoch 700, val loss: 0.8969407677650452
Epoch 710, training loss: 0.6368721127510071 = 0.014321446418762207 + 0.1 * 6.225506782531738
Epoch 710, val loss: 0.9026708602905273
Epoch 720, training loss: 0.6353449821472168 = 0.01376782450824976 + 0.1 * 6.215771198272705
Epoch 720, val loss: 0.9083486795425415
Epoch 730, training loss: 0.6340564489364624 = 0.013248573057353497 + 0.1 * 6.208078861236572
Epoch 730, val loss: 0.9138984680175781
Epoch 740, training loss: 0.6333067417144775 = 0.012760215438902378 + 0.1 * 6.205464839935303
Epoch 740, val loss: 0.9193869233131409
Epoch 750, training loss: 0.6325848698616028 = 0.012299084104597569 + 0.1 * 6.202857971191406
Epoch 750, val loss: 0.9247896075248718
Epoch 760, training loss: 0.6345821022987366 = 0.011862658895552158 + 0.1 * 6.227194309234619
Epoch 760, val loss: 0.9300257563591003
Epoch 770, training loss: 0.6322442889213562 = 0.011452192440629005 + 0.1 * 6.207920551300049
Epoch 770, val loss: 0.9352455139160156
Epoch 780, training loss: 0.6313314437866211 = 0.011065518483519554 + 0.1 * 6.202658653259277
Epoch 780, val loss: 0.9404138326644897
Epoch 790, training loss: 0.6299206614494324 = 0.010698608122766018 + 0.1 * 6.192220211029053
Epoch 790, val loss: 0.9454262852668762
Epoch 800, training loss: 0.6303942799568176 = 0.01035046111792326 + 0.1 * 6.200438022613525
Epoch 800, val loss: 0.950411856174469
Epoch 810, training loss: 0.629971981048584 = 0.010019851848483086 + 0.1 * 6.199521541595459
Epoch 810, val loss: 0.9552252888679504
Epoch 820, training loss: 0.629260241985321 = 0.00970672257244587 + 0.1 * 6.195535182952881
Epoch 820, val loss: 0.9600421190261841
Epoch 830, training loss: 0.6288893222808838 = 0.009410503320395947 + 0.1 * 6.194787979125977
Epoch 830, val loss: 0.9648321270942688
Epoch 840, training loss: 0.6277298331260681 = 0.009128077886998653 + 0.1 * 6.186017036437988
Epoch 840, val loss: 0.9694931507110596
Epoch 850, training loss: 0.627887487411499 = 0.008858656510710716 + 0.1 * 6.190288066864014
Epoch 850, val loss: 0.9740619659423828
Epoch 860, training loss: 0.6267122626304626 = 0.008602400310337543 + 0.1 * 6.181098937988281
Epoch 860, val loss: 0.9785627722740173
Epoch 870, training loss: 0.6260436177253723 = 0.00835852324962616 + 0.1 * 6.17685079574585
Epoch 870, val loss: 0.9830767512321472
Epoch 880, training loss: 0.62611985206604 = 0.00812538806349039 + 0.1 * 6.17994499206543
Epoch 880, val loss: 0.9874785542488098
Epoch 890, training loss: 0.6275469660758972 = 0.007902359589934349 + 0.1 * 6.196445941925049
Epoch 890, val loss: 0.9918063879013062
Epoch 900, training loss: 0.6252279281616211 = 0.0076895360834896564 + 0.1 * 6.175383567810059
Epoch 900, val loss: 0.9959787726402283
Epoch 910, training loss: 0.6247456669807434 = 0.007487091701477766 + 0.1 * 6.172585487365723
Epoch 910, val loss: 1.000220537185669
Epoch 920, training loss: 0.624351441860199 = 0.007293122820556164 + 0.1 * 6.170583248138428
Epoch 920, val loss: 1.004281759262085
Epoch 930, training loss: 0.6238394975662231 = 0.007107592653483152 + 0.1 * 6.167318820953369
Epoch 930, val loss: 1.0083531141281128
Epoch 940, training loss: 0.624769389629364 = 0.006929043680429459 + 0.1 * 6.178403377532959
Epoch 940, val loss: 1.0123546123504639
Epoch 950, training loss: 0.6243175864219666 = 0.006757865194231272 + 0.1 * 6.175597190856934
Epoch 950, val loss: 1.0162824392318726
Epoch 960, training loss: 0.6230912804603577 = 0.0065938387997448444 + 0.1 * 6.164974689483643
Epoch 960, val loss: 1.0201488733291626
Epoch 970, training loss: 0.6228798031806946 = 0.006436713505536318 + 0.1 * 6.164431095123291
Epoch 970, val loss: 1.0239897966384888
Epoch 980, training loss: 0.6228452920913696 = 0.006285365670919418 + 0.1 * 6.165599346160889
Epoch 980, val loss: 1.027718424797058
Epoch 990, training loss: 0.6216318607330322 = 0.006140371318906546 + 0.1 * 6.154914855957031
Epoch 990, val loss: 1.0314083099365234
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.5904
Flip ASR: 0.5156/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7921531200408936 = 1.9547702074050903 + 0.1 * 8.373828887939453
Epoch 0, val loss: 1.9663933515548706
Epoch 10, training loss: 2.7820591926574707 = 1.9446914196014404 + 0.1 * 8.373676300048828
Epoch 10, val loss: 1.9567233324050903
Epoch 20, training loss: 2.7694971561431885 = 1.9322057962417603 + 0.1 * 8.37291431427002
Epoch 20, val loss: 1.9445914030075073
Epoch 30, training loss: 2.751415491104126 = 1.9146263599395752 + 0.1 * 8.367891311645508
Epoch 30, val loss: 1.9273403882980347
Epoch 40, training loss: 2.721619129180908 = 1.8881683349609375 + 0.1 * 8.334507942199707
Epoch 40, val loss: 1.9014793634414673
Epoch 50, training loss: 2.6620492935180664 = 1.8509292602539062 + 0.1 * 8.111199378967285
Epoch 50, val loss: 1.8657869100570679
Epoch 60, training loss: 2.574115037918091 = 1.8091386556625366 + 0.1 * 7.649764060974121
Epoch 60, val loss: 1.8260585069656372
Epoch 70, training loss: 2.489408016204834 = 1.767648696899414 + 0.1 * 7.217593669891357
Epoch 70, val loss: 1.7858614921569824
Epoch 80, training loss: 2.419036865234375 = 1.7245651483535767 + 0.1 * 6.9447174072265625
Epoch 80, val loss: 1.7449150085449219
Epoch 90, training loss: 2.352581262588501 = 1.6710634231567383 + 0.1 * 6.8151774406433105
Epoch 90, val loss: 1.6940412521362305
Epoch 100, training loss: 2.275219678878784 = 1.598925232887268 + 0.1 * 6.762944221496582
Epoch 100, val loss: 1.6299011707305908
Epoch 110, training loss: 2.1824893951416016 = 1.509643793106079 + 0.1 * 6.728456974029541
Epoch 110, val loss: 1.5561816692352295
Epoch 120, training loss: 2.0822930335998535 = 1.412105679512024 + 0.1 * 6.701873779296875
Epoch 120, val loss: 1.4768949747085571
Epoch 130, training loss: 1.9854422807693481 = 1.3173927068710327 + 0.1 * 6.680495738983154
Epoch 130, val loss: 1.4027113914489746
Epoch 140, training loss: 1.8958474397659302 = 1.229852557182312 + 0.1 * 6.659948825836182
Epoch 140, val loss: 1.3366807699203491
Epoch 150, training loss: 1.8140894174575806 = 1.1498467922210693 + 0.1 * 6.642426013946533
Epoch 150, val loss: 1.278934359550476
Epoch 160, training loss: 1.7382874488830566 = 1.0761529207229614 + 0.1 * 6.621345043182373
Epoch 160, val loss: 1.2274219989776611
Epoch 170, training loss: 1.6657299995422363 = 1.00568425655365 + 0.1 * 6.600457191467285
Epoch 170, val loss: 1.178918480873108
Epoch 180, training loss: 1.5954961776733398 = 0.9374269247055054 + 0.1 * 6.580691814422607
Epoch 180, val loss: 1.1317613124847412
Epoch 190, training loss: 1.5291543006896973 = 0.8717191219329834 + 0.1 * 6.574350833892822
Epoch 190, val loss: 1.0858416557312012
Epoch 200, training loss: 1.46318519115448 = 0.8075815439224243 + 0.1 * 6.556036472320557
Epoch 200, val loss: 1.0406361818313599
Epoch 210, training loss: 1.397037148475647 = 0.7426401972770691 + 0.1 * 6.54396915435791
Epoch 210, val loss: 0.9943504333496094
Epoch 220, training loss: 1.330296277999878 = 0.6765116453170776 + 0.1 * 6.53784704208374
Epoch 220, val loss: 0.9472839832305908
Epoch 230, training loss: 1.262784481048584 = 0.6101607084274292 + 0.1 * 6.526237487792969
Epoch 230, val loss: 0.9011485576629639
Epoch 240, training loss: 1.196732521057129 = 0.5452006459236145 + 0.1 * 6.515318393707275
Epoch 240, val loss: 0.8578362464904785
Epoch 250, training loss: 1.135095477104187 = 0.4842952489852905 + 0.1 * 6.508002281188965
Epoch 250, val loss: 0.8202841877937317
Epoch 260, training loss: 1.0795748233795166 = 0.4294610619544983 + 0.1 * 6.5011372566223145
Epoch 260, val loss: 0.7894275188446045
Epoch 270, training loss: 1.0313451290130615 = 0.38117703795433044 + 0.1 * 6.501680850982666
Epoch 270, val loss: 0.7654439210891724
Epoch 280, training loss: 0.9894109964370728 = 0.3398648202419281 + 0.1 * 6.495461463928223
Epoch 280, val loss: 0.7486956119537354
Epoch 290, training loss: 0.9527488946914673 = 0.30452123284339905 + 0.1 * 6.482276439666748
Epoch 290, val loss: 0.7378717064857483
Epoch 300, training loss: 0.921318531036377 = 0.273926705121994 + 0.1 * 6.4739179611206055
Epoch 300, val loss: 0.7315261960029602
Epoch 310, training loss: 0.8941112756729126 = 0.2471364289522171 + 0.1 * 6.469748497009277
Epoch 310, val loss: 0.7287282347679138
Epoch 320, training loss: 0.8704044222831726 = 0.22356504201889038 + 0.1 * 6.468393802642822
Epoch 320, val loss: 0.7283690571784973
Epoch 330, training loss: 0.8485115766525269 = 0.20266121625900269 + 0.1 * 6.458503723144531
Epoch 330, val loss: 0.7301129698753357
Epoch 340, training loss: 0.8292233347892761 = 0.18392403423786163 + 0.1 * 6.452992916107178
Epoch 340, val loss: 0.7334774136543274
Epoch 350, training loss: 0.8118491768836975 = 0.16710631549358368 + 0.1 * 6.447428226470947
Epoch 350, val loss: 0.738199770450592
Epoch 360, training loss: 0.7962086200714111 = 0.15200437605381012 + 0.1 * 6.442042350769043
Epoch 360, val loss: 0.7440938353538513
Epoch 370, training loss: 0.7819664478302002 = 0.13833525776863098 + 0.1 * 6.436312198638916
Epoch 370, val loss: 0.7511138916015625
Epoch 380, training loss: 0.7701377272605896 = 0.12594389915466309 + 0.1 * 6.4419379234313965
Epoch 380, val loss: 0.7591356039047241
Epoch 390, training loss: 0.7576987147331238 = 0.1148340031504631 + 0.1 * 6.428647041320801
Epoch 390, val loss: 0.7682552933692932
Epoch 400, training loss: 0.7471112608909607 = 0.10482189804315567 + 0.1 * 6.422893524169922
Epoch 400, val loss: 0.7781602740287781
Epoch 410, training loss: 0.7379700541496277 = 0.09574557840824127 + 0.1 * 6.422244548797607
Epoch 410, val loss: 0.7887709140777588
Epoch 420, training loss: 0.7294813394546509 = 0.08757039904594421 + 0.1 * 6.419108867645264
Epoch 420, val loss: 0.799827516078949
Epoch 430, training loss: 0.7203214764595032 = 0.08016457408666611 + 0.1 * 6.401569366455078
Epoch 430, val loss: 0.8113541007041931
Epoch 440, training loss: 0.7134532332420349 = 0.07344325631856918 + 0.1 * 6.400099754333496
Epoch 440, val loss: 0.8230834007263184
Epoch 450, training loss: 0.7070767879486084 = 0.06739826500415802 + 0.1 * 6.396785259246826
Epoch 450, val loss: 0.8349769115447998
Epoch 460, training loss: 0.7002686262130737 = 0.06195820868015289 + 0.1 * 6.38310432434082
Epoch 460, val loss: 0.8470295667648315
Epoch 470, training loss: 0.6973411440849304 = 0.057053275406360626 + 0.1 * 6.402878761291504
Epoch 470, val loss: 0.8589515686035156
Epoch 480, training loss: 0.6901385188102722 = 0.0526672825217247 + 0.1 * 6.3747124671936035
Epoch 480, val loss: 0.8708101511001587
Epoch 490, training loss: 0.6852720975875854 = 0.0487220399081707 + 0.1 * 6.365500450134277
Epoch 490, val loss: 0.882570743560791
Epoch 500, training loss: 0.681206464767456 = 0.045164432376623154 + 0.1 * 6.360420227050781
Epoch 500, val loss: 0.8943607211112976
Epoch 510, training loss: 0.6776139736175537 = 0.04195408150553703 + 0.1 * 6.356598854064941
Epoch 510, val loss: 0.9059603810310364
Epoch 520, training loss: 0.6746962070465088 = 0.03905966877937317 + 0.1 * 6.356364727020264
Epoch 520, val loss: 0.9172611832618713
Epoch 530, training loss: 0.6706010699272156 = 0.036460183560848236 + 0.1 * 6.341409206390381
Epoch 530, val loss: 0.9286454916000366
Epoch 540, training loss: 0.6682301759719849 = 0.034111473709344864 + 0.1 * 6.3411865234375
Epoch 540, val loss: 0.9394372701644897
Epoch 550, training loss: 0.6651847958564758 = 0.031998343765735626 + 0.1 * 6.331864356994629
Epoch 550, val loss: 0.9501662850379944
Epoch 560, training loss: 0.6627047657966614 = 0.03007473796606064 + 0.1 * 6.326300621032715
Epoch 560, val loss: 0.9605520963668823
Epoch 570, training loss: 0.6632220149040222 = 0.028317652642726898 + 0.1 * 6.349043846130371
Epoch 570, val loss: 0.9707026481628418
Epoch 580, training loss: 0.6587381958961487 = 0.026722051203250885 + 0.1 * 6.320160865783691
Epoch 580, val loss: 0.9806927442550659
Epoch 590, training loss: 0.656454861164093 = 0.02525644563138485 + 0.1 * 6.311984062194824
Epoch 590, val loss: 0.9904787540435791
Epoch 600, training loss: 0.6548182964324951 = 0.023910898715257645 + 0.1 * 6.3090739250183105
Epoch 600, val loss: 1.000035285949707
Epoch 610, training loss: 0.6529995799064636 = 0.022671151906251907 + 0.1 * 6.303284168243408
Epoch 610, val loss: 1.0094438791275024
Epoch 620, training loss: 0.6523905992507935 = 0.021529128775000572 + 0.1 * 6.308614253997803
Epoch 620, val loss: 1.018406629562378
Epoch 630, training loss: 0.6496857404708862 = 0.02047864720225334 + 0.1 * 6.2920708656311035
Epoch 630, val loss: 1.0274726152420044
Epoch 640, training loss: 0.6490378379821777 = 0.019506296142935753 + 0.1 * 6.295315742492676
Epoch 640, val loss: 1.0362660884857178
Epoch 650, training loss: 0.6484599709510803 = 0.01860249973833561 + 0.1 * 6.298574924468994
Epoch 650, val loss: 1.0447570085525513
Epoch 660, training loss: 0.6467419862747192 = 0.01776621863245964 + 0.1 * 6.289757251739502
Epoch 660, val loss: 1.0530827045440674
Epoch 670, training loss: 0.6454229354858398 = 0.016987238079309464 + 0.1 * 6.284356594085693
Epoch 670, val loss: 1.0613281726837158
Epoch 680, training loss: 0.6456394195556641 = 0.016260966658592224 + 0.1 * 6.293784141540527
Epoch 680, val loss: 1.0692222118377686
Epoch 690, training loss: 0.6432622075080872 = 0.0155843710526824 + 0.1 * 6.276778697967529
Epoch 690, val loss: 1.077007532119751
Epoch 700, training loss: 0.6426882147789001 = 0.014953447505831718 + 0.1 * 6.277347564697266
Epoch 700, val loss: 1.0847253799438477
Epoch 710, training loss: 0.6411821246147156 = 0.014360753819346428 + 0.1 * 6.268213748931885
Epoch 710, val loss: 1.092106580734253
Epoch 720, training loss: 0.6403307318687439 = 0.013805853202939034 + 0.1 * 6.2652482986450195
Epoch 720, val loss: 1.099509596824646
Epoch 730, training loss: 0.6395546793937683 = 0.013283018954098225 + 0.1 * 6.262716293334961
Epoch 730, val loss: 1.1065694093704224
Epoch 740, training loss: 0.6386404037475586 = 0.012793399393558502 + 0.1 * 6.258469581604004
Epoch 740, val loss: 1.113631248474121
Epoch 750, training loss: 0.6377003788948059 = 0.012332053855061531 + 0.1 * 6.253682613372803
Epoch 750, val loss: 1.1206148862838745
Epoch 760, training loss: 0.6380182504653931 = 0.011896086856722832 + 0.1 * 6.261221885681152
Epoch 760, val loss: 1.1272135972976685
Epoch 770, training loss: 0.6369521021842957 = 0.01148542482405901 + 0.1 * 6.254666805267334
Epoch 770, val loss: 1.1337817907333374
Epoch 780, training loss: 0.6356419324874878 = 0.011099036782979965 + 0.1 * 6.245428562164307
Epoch 780, val loss: 1.1403529644012451
Epoch 790, training loss: 0.6353244185447693 = 0.010732397437095642 + 0.1 * 6.245919704437256
Epoch 790, val loss: 1.146709680557251
Epoch 800, training loss: 0.635714590549469 = 0.010385369881987572 + 0.1 * 6.253291606903076
Epoch 800, val loss: 1.1528435945510864
Epoch 810, training loss: 0.6339185237884521 = 0.010054916143417358 + 0.1 * 6.238636016845703
Epoch 810, val loss: 1.1588149070739746
Epoch 820, training loss: 0.6329120993614197 = 0.009743326343595982 + 0.1 * 6.231687545776367
Epoch 820, val loss: 1.1648879051208496
Epoch 830, training loss: 0.6338437795639038 = 0.009446327574551105 + 0.1 * 6.243974685668945
Epoch 830, val loss: 1.1706475019454956
Epoch 840, training loss: 0.6322980523109436 = 0.009164434857666492 + 0.1 * 6.2313361167907715
Epoch 840, val loss: 1.176345705986023
Epoch 850, training loss: 0.6328572630882263 = 0.008896466344594955 + 0.1 * 6.239607810974121
Epoch 850, val loss: 1.1820181608200073
Epoch 860, training loss: 0.6311869025230408 = 0.008640876039862633 + 0.1 * 6.225460052490234
Epoch 860, val loss: 1.1873610019683838
Epoch 870, training loss: 0.6310562491416931 = 0.008398494683206081 + 0.1 * 6.2265777587890625
Epoch 870, val loss: 1.1928845643997192
Epoch 880, training loss: 0.6308639049530029 = 0.00816636998206377 + 0.1 * 6.226975440979004
Epoch 880, val loss: 1.1980512142181396
Epoch 890, training loss: 0.6301897764205933 = 0.007945198565721512 + 0.1 * 6.222445964813232
Epoch 890, val loss: 1.2032326459884644
Epoch 900, training loss: 0.6299389004707336 = 0.007733860518783331 + 0.1 * 6.222050189971924
Epoch 900, val loss: 1.2083221673965454
Epoch 910, training loss: 0.6290440559387207 = 0.007531745824962854 + 0.1 * 6.215122699737549
Epoch 910, val loss: 1.213278889656067
Epoch 920, training loss: 0.6296881437301636 = 0.007337914314121008 + 0.1 * 6.223502159118652
Epoch 920, val loss: 1.2182083129882812
Epoch 930, training loss: 0.6291796565055847 = 0.007151727564632893 + 0.1 * 6.220279216766357
Epoch 930, val loss: 1.2229305505752563
Epoch 940, training loss: 0.628478467464447 = 0.006974241696298122 + 0.1 * 6.215042591094971
Epoch 940, val loss: 1.2277545928955078
Epoch 950, training loss: 0.6283420324325562 = 0.006803396623581648 + 0.1 * 6.215386390686035
Epoch 950, val loss: 1.2323154211044312
Epoch 960, training loss: 0.6282545328140259 = 0.006640404462814331 + 0.1 * 6.216141700744629
Epoch 960, val loss: 1.2368831634521484
Epoch 970, training loss: 0.6277185678482056 = 0.006483522709459066 + 0.1 * 6.212350845336914
Epoch 970, val loss: 1.2413407564163208
Epoch 980, training loss: 0.6267309784889221 = 0.006333404686301947 + 0.1 * 6.203975677490234
Epoch 980, val loss: 1.2458659410476685
Epoch 990, training loss: 0.6266245245933533 = 0.006188433617353439 + 0.1 * 6.204360485076904
Epoch 990, val loss: 1.2501518726348877
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.6089
Flip ASR: 0.5511/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7872986793518066 = 1.9499133825302124 + 0.1 * 8.37385368347168
Epoch 0, val loss: 1.948246955871582
Epoch 10, training loss: 2.776625394821167 = 1.939263105392456 + 0.1 * 8.373621940612793
Epoch 10, val loss: 1.9378389120101929
Epoch 20, training loss: 2.7631447315216064 = 1.9259068965911865 + 0.1 * 8.3723783493042
Epoch 20, val loss: 1.9244742393493652
Epoch 30, training loss: 2.7437052726745605 = 1.9072386026382446 + 0.1 * 8.364666938781738
Epoch 30, val loss: 1.905493140220642
Epoch 40, training loss: 2.7123665809631348 = 1.8799898624420166 + 0.1 * 8.323765754699707
Epoch 40, val loss: 1.8780841827392578
Epoch 50, training loss: 2.6523146629333496 = 1.8435813188552856 + 0.1 * 8.087332725524902
Epoch 50, val loss: 1.8431986570358276
Epoch 60, training loss: 2.5697951316833496 = 1.8048007488250732 + 0.1 * 7.649944305419922
Epoch 60, val loss: 1.8081088066101074
Epoch 70, training loss: 2.4876203536987305 = 1.7683627605438232 + 0.1 * 7.192574977874756
Epoch 70, val loss: 1.774971842765808
Epoch 80, training loss: 2.422595977783203 = 1.731131911277771 + 0.1 * 6.914639472961426
Epoch 80, val loss: 1.7405176162719727
Epoch 90, training loss: 2.3656668663024902 = 1.6840072870254517 + 0.1 * 6.816595077514648
Epoch 90, val loss: 1.6968166828155518
Epoch 100, training loss: 2.2986156940460205 = 1.6218011379241943 + 0.1 * 6.768145561218262
Epoch 100, val loss: 1.6415058374404907
Epoch 110, training loss: 2.2173564434051514 = 1.544127345085144 + 0.1 * 6.732290267944336
Epoch 110, val loss: 1.5757514238357544
Epoch 120, training loss: 2.1268301010131836 = 1.456256628036499 + 0.1 * 6.705735206604004
Epoch 120, val loss: 1.5023303031921387
Epoch 130, training loss: 2.033999443054199 = 1.3655143976211548 + 0.1 * 6.684850215911865
Epoch 130, val loss: 1.428625464439392
Epoch 140, training loss: 1.9408373832702637 = 1.274214506149292 + 0.1 * 6.666229248046875
Epoch 140, val loss: 1.3556361198425293
Epoch 150, training loss: 1.8470160961151123 = 1.1823235750198364 + 0.1 * 6.64692497253418
Epoch 150, val loss: 1.2853649854660034
Epoch 160, training loss: 1.7538864612579346 = 1.0910592079162598 + 0.1 * 6.628272533416748
Epoch 160, val loss: 1.2170708179473877
Epoch 170, training loss: 1.6654741764068604 = 1.003839373588562 + 0.1 * 6.616347312927246
Epoch 170, val loss: 1.1534754037857056
Epoch 180, training loss: 1.5808770656585693 = 0.9204946756362915 + 0.1 * 6.603824615478516
Epoch 180, val loss: 1.0935554504394531
Epoch 190, training loss: 1.500760793685913 = 0.841769278049469 + 0.1 * 6.589914321899414
Epoch 190, val loss: 1.037837266921997
Epoch 200, training loss: 1.4258553981781006 = 0.7680702805519104 + 0.1 * 6.577851295471191
Epoch 200, val loss: 0.9860384464263916
Epoch 210, training loss: 1.3573617935180664 = 0.7004783153533936 + 0.1 * 6.5688347816467285
Epoch 210, val loss: 0.9389986395835876
Epoch 220, training loss: 1.2949392795562744 = 0.6392666697502136 + 0.1 * 6.556725978851318
Epoch 220, val loss: 0.8974596858024597
Epoch 230, training loss: 1.2401254177093506 = 0.5841352939605713 + 0.1 * 6.559900283813477
Epoch 230, val loss: 0.8617504835128784
Epoch 240, training loss: 1.1896986961364746 = 0.5355428457260132 + 0.1 * 6.541558265686035
Epoch 240, val loss: 0.8324217200279236
Epoch 250, training loss: 1.1455987691879272 = 0.49211013317108154 + 0.1 * 6.534886360168457
Epoch 250, val loss: 0.8086702227592468
Epoch 260, training loss: 1.1056225299835205 = 0.45303085446357727 + 0.1 * 6.52591609954834
Epoch 260, val loss: 0.7899678945541382
Epoch 270, training loss: 1.0693488121032715 = 0.4175242483615875 + 0.1 * 6.518245220184326
Epoch 270, val loss: 0.7752947211265564
Epoch 280, training loss: 1.0376962423324585 = 0.3850666284561157 + 0.1 * 6.526296138763428
Epoch 280, val loss: 0.7637906670570374
Epoch 290, training loss: 1.0060696601867676 = 0.3551427125930786 + 0.1 * 6.509270191192627
Epoch 290, val loss: 0.7547670602798462
Epoch 300, training loss: 0.9766830205917358 = 0.326908141374588 + 0.1 * 6.497748851776123
Epoch 300, val loss: 0.7475987672805786
Epoch 310, training loss: 0.9517800807952881 = 0.3000048100948334 + 0.1 * 6.517752647399902
Epoch 310, val loss: 0.7416766881942749
Epoch 320, training loss: 0.9231677055358887 = 0.2745615541934967 + 0.1 * 6.486061096191406
Epoch 320, val loss: 0.7368423938751221
Epoch 330, training loss: 0.8982679843902588 = 0.250308096408844 + 0.1 * 6.479598522186279
Epoch 330, val loss: 0.732936680316925
Epoch 340, training loss: 0.8742750883102417 = 0.22718743979930878 + 0.1 * 6.470876216888428
Epoch 340, val loss: 0.7297974228858948
Epoch 350, training loss: 0.8529014587402344 = 0.20526942610740662 + 0.1 * 6.476319789886475
Epoch 350, val loss: 0.727560818195343
Epoch 360, training loss: 0.8305547833442688 = 0.1847737431526184 + 0.1 * 6.457810401916504
Epoch 360, val loss: 0.7262499332427979
Epoch 370, training loss: 0.8125334978103638 = 0.16567613184452057 + 0.1 * 6.468573570251465
Epoch 370, val loss: 0.7259299159049988
Epoch 380, training loss: 0.7928974032402039 = 0.1481931209564209 + 0.1 * 6.447042465209961
Epoch 380, val loss: 0.7266398072242737
Epoch 390, training loss: 0.7757056951522827 = 0.13226482272148132 + 0.1 * 6.434408187866211
Epoch 390, val loss: 0.7285630106925964
Epoch 400, training loss: 0.7606742978096008 = 0.11785898357629776 + 0.1 * 6.428152561187744
Epoch 400, val loss: 0.7317266464233398
Epoch 410, training loss: 0.7471184134483337 = 0.10500933974981308 + 0.1 * 6.421090602874756
Epoch 410, val loss: 0.7362100481987
Epoch 420, training loss: 0.7378417253494263 = 0.09371323883533478 + 0.1 * 6.441285133361816
Epoch 420, val loss: 0.7419739365577698
Epoch 430, training loss: 0.7241707444190979 = 0.08389578014612198 + 0.1 * 6.402749061584473
Epoch 430, val loss: 0.7488057613372803
Epoch 440, training loss: 0.7150651216506958 = 0.0753425732254982 + 0.1 * 6.397225379943848
Epoch 440, val loss: 0.75660640001297
Epoch 450, training loss: 0.7073093056678772 = 0.06789026409387589 + 0.1 * 6.394189834594727
Epoch 450, val loss: 0.7651082277297974
Epoch 460, training loss: 0.6998278498649597 = 0.06141401827335358 + 0.1 * 6.384138584136963
Epoch 460, val loss: 0.7740273475646973
Epoch 470, training loss: 0.6934482455253601 = 0.0557667575776577 + 0.1 * 6.376814842224121
Epoch 470, val loss: 0.7832909226417542
Epoch 480, training loss: 0.6882951855659485 = 0.050834134221076965 + 0.1 * 6.374610424041748
Epoch 480, val loss: 0.7927150130271912
Epoch 490, training loss: 0.6839233636856079 = 0.046507805585861206 + 0.1 * 6.3741559982299805
Epoch 490, val loss: 0.8021768927574158
Epoch 500, training loss: 0.6783621907234192 = 0.04271157830953598 + 0.1 * 6.35650634765625
Epoch 500, val loss: 0.8115895986557007
Epoch 510, training loss: 0.6749504208564758 = 0.039350856095552444 + 0.1 * 6.355995178222656
Epoch 510, val loss: 0.8211389183998108
Epoch 520, training loss: 0.670813798904419 = 0.03636841103434563 + 0.1 * 6.344453811645508
Epoch 520, val loss: 0.8303686380386353
Epoch 530, training loss: 0.6686425805091858 = 0.03371672332286835 + 0.1 * 6.3492584228515625
Epoch 530, val loss: 0.8396595120429993
Epoch 540, training loss: 0.6646989583969116 = 0.031352173537015915 + 0.1 * 6.333467960357666
Epoch 540, val loss: 0.8486096858978271
Epoch 550, training loss: 0.6619566679000854 = 0.029232775792479515 + 0.1 * 6.3272385597229
Epoch 550, val loss: 0.8575722575187683
Epoch 560, training loss: 0.6587837934494019 = 0.027327852323651314 + 0.1 * 6.314559459686279
Epoch 560, val loss: 0.8661966323852539
Epoch 570, training loss: 0.6580994725227356 = 0.02560698240995407 + 0.1 * 6.324924468994141
Epoch 570, val loss: 0.8747342228889465
Epoch 580, training loss: 0.6575357913970947 = 0.024050822481513023 + 0.1 * 6.3348493576049805
Epoch 580, val loss: 0.8830191493034363
Epoch 590, training loss: 0.653142511844635 = 0.022643892094492912 + 0.1 * 6.304986476898193
Epoch 590, val loss: 0.8911629915237427
Epoch 600, training loss: 0.6512895822525024 = 0.02136244997382164 + 0.1 * 6.299271583557129
Epoch 600, val loss: 0.8991612195968628
Epoch 610, training loss: 0.6501116156578064 = 0.02019120380282402 + 0.1 * 6.299203872680664
Epoch 610, val loss: 0.9070232510566711
Epoch 620, training loss: 0.6485535502433777 = 0.019122285768389702 + 0.1 * 6.294312477111816
Epoch 620, val loss: 0.9145947694778442
Epoch 630, training loss: 0.6491770148277283 = 0.018139012157917023 + 0.1 * 6.310379981994629
Epoch 630, val loss: 0.9220737218856812
Epoch 640, training loss: 0.645453691482544 = 0.017236312851309776 + 0.1 * 6.2821736335754395
Epoch 640, val loss: 0.9292998909950256
Epoch 650, training loss: 0.6452062129974365 = 0.01640322618186474 + 0.1 * 6.28803014755249
Epoch 650, val loss: 0.9363179802894592
Epoch 660, training loss: 0.6431223750114441 = 0.01563335955142975 + 0.1 * 6.274890422821045
Epoch 660, val loss: 0.9432816505432129
Epoch 670, training loss: 0.6441543698310852 = 0.014919883571565151 + 0.1 * 6.292344570159912
Epoch 670, val loss: 0.9499731063842773
Epoch 680, training loss: 0.6415844559669495 = 0.014257019385695457 + 0.1 * 6.2732744216918945
Epoch 680, val loss: 0.9565983414649963
Epoch 690, training loss: 0.6397860050201416 = 0.01364429946988821 + 0.1 * 6.261417388916016
Epoch 690, val loss: 0.9629747271537781
Epoch 700, training loss: 0.6406388878822327 = 0.01307092234492302 + 0.1 * 6.275679111480713
Epoch 700, val loss: 0.9693520069122314
Epoch 710, training loss: 0.6380153894424438 = 0.012537969276309013 + 0.1 * 6.25477409362793
Epoch 710, val loss: 0.9754340052604675
Epoch 720, training loss: 0.637916624546051 = 0.012040648609399796 + 0.1 * 6.258759498596191
Epoch 720, val loss: 0.9813733100891113
Epoch 730, training loss: 0.6366884708404541 = 0.0115735474973917 + 0.1 * 6.2511491775512695
Epoch 730, val loss: 0.9875019192695618
Epoch 740, training loss: 0.6359952688217163 = 0.011137775145471096 + 0.1 * 6.248574733734131
Epoch 740, val loss: 0.9931038022041321
Epoch 750, training loss: 0.634965181350708 = 0.010727924294769764 + 0.1 * 6.242372035980225
Epoch 750, val loss: 0.9988303780555725
Epoch 760, training loss: 0.635094940662384 = 0.010340833105146885 + 0.1 * 6.2475409507751465
Epoch 760, val loss: 1.0044925212860107
Epoch 770, training loss: 0.6351442933082581 = 0.009976294822990894 + 0.1 * 6.25167989730835
Epoch 770, val loss: 1.010056495666504
Epoch 780, training loss: 0.6332235932350159 = 0.009633983485400677 + 0.1 * 6.235896110534668
Epoch 780, val loss: 1.0153366327285767
Epoch 790, training loss: 0.6326935291290283 = 0.009310811758041382 + 0.1 * 6.233826637268066
Epoch 790, val loss: 1.0205882787704468
Epoch 800, training loss: 0.632480263710022 = 0.009005448780953884 + 0.1 * 6.234748363494873
Epoch 800, val loss: 1.0258008241653442
Epoch 810, training loss: 0.6334367990493774 = 0.008716746233403683 + 0.1 * 6.2472004890441895
Epoch 810, val loss: 1.030917763710022
Epoch 820, training loss: 0.6318055987358093 = 0.008441941812634468 + 0.1 * 6.233636856079102
Epoch 820, val loss: 1.0358307361602783
Epoch 830, training loss: 0.6301558017730713 = 0.008184202946722507 + 0.1 * 6.2197160720825195
Epoch 830, val loss: 1.0406794548034668
Epoch 840, training loss: 0.6319777965545654 = 0.007938175462186337 + 0.1 * 6.240396499633789
Epoch 840, val loss: 1.0455752611160278
Epoch 850, training loss: 0.6297773718833923 = 0.007704052608460188 + 0.1 * 6.220732688903809
Epoch 850, val loss: 1.0503638982772827
Epoch 860, training loss: 0.6296787858009338 = 0.007482095621526241 + 0.1 * 6.221966743469238
Epoch 860, val loss: 1.0548869371414185
Epoch 870, training loss: 0.6286335587501526 = 0.007270435802638531 + 0.1 * 6.2136311531066895
Epoch 870, val loss: 1.0595036745071411
Epoch 880, training loss: 0.6284743547439575 = 0.007068532984703779 + 0.1 * 6.2140583992004395
Epoch 880, val loss: 1.0640342235565186
Epoch 890, training loss: 0.628979504108429 = 0.006875651888549328 + 0.1 * 6.221038818359375
Epoch 890, val loss: 1.0685746669769287
Epoch 900, training loss: 0.6278746128082275 = 0.006691729184240103 + 0.1 * 6.211828708648682
Epoch 900, val loss: 1.0729008913040161
Epoch 910, training loss: 0.627561092376709 = 0.006516527384519577 + 0.1 * 6.210445404052734
Epoch 910, val loss: 1.077175498008728
Epoch 920, training loss: 0.6272421479225159 = 0.006348844617605209 + 0.1 * 6.208932876586914
Epoch 920, val loss: 1.0814521312713623
Epoch 930, training loss: 0.6277791857719421 = 0.006188171915709972 + 0.1 * 6.2159104347229
Epoch 930, val loss: 1.0855573415756226
Epoch 940, training loss: 0.6255201697349548 = 0.0060347761027514935 + 0.1 * 6.194853782653809
Epoch 940, val loss: 1.0896549224853516
Epoch 950, training loss: 0.6264598369598389 = 0.005887799896299839 + 0.1 * 6.2057204246521
Epoch 950, val loss: 1.0936857461929321
Epoch 960, training loss: 0.6250961422920227 = 0.005746862851083279 + 0.1 * 6.193492889404297
Epoch 960, val loss: 1.09765625
Epoch 970, training loss: 0.625697910785675 = 0.0056113190948963165 + 0.1 * 6.200866222381592
Epoch 970, val loss: 1.1016058921813965
Epoch 980, training loss: 0.6247749924659729 = 0.00548184011131525 + 0.1 * 6.192931652069092
Epoch 980, val loss: 1.1054370403289795
Epoch 990, training loss: 0.6263878345489502 = 0.005357932299375534 + 0.1 * 6.210299015045166
Epoch 990, val loss: 1.1091951131820679
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.5129
Flip ASR: 0.4844/225 nodes
The final ASR:0.57073, 0.04157, Accuracy:0.83210, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9536])
updated graph: torch.Size([2, 10614])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.98278, 0.00696, Accuracy:0.83457, 0.00924
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.77123761177063 = 1.9338499307632446 + 0.1 * 8.373876571655273
Epoch 0, val loss: 1.922511100769043
Epoch 10, training loss: 2.7612695693969727 = 1.923893690109253 + 0.1 * 8.373757362365723
Epoch 10, val loss: 1.9137648344039917
Epoch 20, training loss: 2.7490346431732178 = 1.9117270708084106 + 0.1 * 8.373076438903809
Epoch 20, val loss: 1.9027942419052124
Epoch 30, training loss: 2.7314705848693848 = 1.8946704864501953 + 0.1 * 8.368001937866211
Epoch 30, val loss: 1.8871431350708008
Epoch 40, training loss: 2.703108787536621 = 1.8696725368499756 + 0.1 * 8.334362030029297
Epoch 40, val loss: 1.864366054534912
Epoch 50, training loss: 2.6481189727783203 = 1.835728645324707 + 0.1 * 8.12390422821045
Epoch 50, val loss: 1.834783911705017
Epoch 60, training loss: 2.5707664489746094 = 1.7969104051589966 + 0.1 * 7.738560676574707
Epoch 60, val loss: 1.8021544218063354
Epoch 70, training loss: 2.492725372314453 = 1.756089448928833 + 0.1 * 7.366359710693359
Epoch 70, val loss: 1.7671655416488647
Epoch 80, training loss: 2.416551113128662 = 1.7098736763000488 + 0.1 * 7.066773414611816
Epoch 80, val loss: 1.7279847860336304
Epoch 90, training loss: 2.338538885116577 = 1.6512926816940308 + 0.1 * 6.872462749481201
Epoch 90, val loss: 1.6771512031555176
Epoch 100, training loss: 2.2535054683685303 = 1.5734833478927612 + 0.1 * 6.800220489501953
Epoch 100, val loss: 1.6091868877410889
Epoch 110, training loss: 2.155311107635498 = 1.4799182415008545 + 0.1 * 6.753927230834961
Epoch 110, val loss: 1.5302529335021973
Epoch 120, training loss: 2.052502393722534 = 1.3804330825805664 + 0.1 * 6.720693588256836
Epoch 120, val loss: 1.4485602378845215
Epoch 130, training loss: 1.9535996913909912 = 1.2838330268859863 + 0.1 * 6.697665691375732
Epoch 130, val loss: 1.372666358947754
Epoch 140, training loss: 1.8625339269638062 = 1.194511890411377 + 0.1 * 6.680220127105713
Epoch 140, val loss: 1.3065077066421509
Epoch 150, training loss: 1.778223991394043 = 1.1113908290863037 + 0.1 * 6.668332099914551
Epoch 150, val loss: 1.2480361461639404
Epoch 160, training loss: 1.6965935230255127 = 1.0313023328781128 + 0.1 * 6.652911186218262
Epoch 160, val loss: 1.192995548248291
Epoch 170, training loss: 1.614936351776123 = 0.9512109756469727 + 0.1 * 6.637253761291504
Epoch 170, val loss: 1.1377427577972412
Epoch 180, training loss: 1.5336331129074097 = 0.8712684512138367 + 0.1 * 6.623646259307861
Epoch 180, val loss: 1.0819941759109497
Epoch 190, training loss: 1.4540379047393799 = 0.7928820252418518 + 0.1 * 6.6115593910217285
Epoch 190, val loss: 1.0270190238952637
Epoch 200, training loss: 1.376420497894287 = 0.7162678837776184 + 0.1 * 6.601526737213135
Epoch 200, val loss: 0.9728020429611206
Epoch 210, training loss: 1.301248550415039 = 0.6421201229095459 + 0.1 * 6.591283321380615
Epoch 210, val loss: 0.9196920394897461
Epoch 220, training loss: 1.229968547821045 = 0.5713231563568115 + 0.1 * 6.586454391479492
Epoch 220, val loss: 0.8691553473472595
Epoch 230, training loss: 1.1635138988494873 = 0.5054879784584045 + 0.1 * 6.580259799957275
Epoch 230, val loss: 0.8240495324134827
Epoch 240, training loss: 1.100921630859375 = 0.4440767168998718 + 0.1 * 6.568448543548584
Epoch 240, val loss: 0.7853196859359741
Epoch 250, training loss: 1.04328191280365 = 0.38738465309143066 + 0.1 * 6.558972358703613
Epoch 250, val loss: 0.7538633942604065
Epoch 260, training loss: 0.9926631450653076 = 0.3361067771911621 + 0.1 * 6.565563678741455
Epoch 260, val loss: 0.7297959327697754
Epoch 270, training loss: 0.9457385540008545 = 0.29131338000297546 + 0.1 * 6.544251918792725
Epoch 270, val loss: 0.7128303050994873
Epoch 280, training loss: 0.9054248929023743 = 0.25259435176849365 + 0.1 * 6.528305530548096
Epoch 280, val loss: 0.7018258571624756
Epoch 290, training loss: 0.8719340562820435 = 0.21975599229335785 + 0.1 * 6.521780490875244
Epoch 290, val loss: 0.6959312558174133
Epoch 300, training loss: 0.84303879737854 = 0.19238628447055817 + 0.1 * 6.506525039672852
Epoch 300, val loss: 0.6945304870605469
Epoch 310, training loss: 0.8185842037200928 = 0.16941231489181519 + 0.1 * 6.491718769073486
Epoch 310, val loss: 0.6966774463653564
Epoch 320, training loss: 0.8054305911064148 = 0.150041401386261 + 0.1 * 6.553891658782959
Epoch 320, val loss: 0.70135098695755
Epoch 330, training loss: 0.781489908695221 = 0.13396859169006348 + 0.1 * 6.475213050842285
Epoch 330, val loss: 0.7080920338630676
Epoch 340, training loss: 0.7670689821243286 = 0.12028010189533234 + 0.1 * 6.467888832092285
Epoch 340, val loss: 0.7159322500228882
Epoch 350, training loss: 0.7536121010780334 = 0.10843218863010406 + 0.1 * 6.451799392700195
Epoch 350, val loss: 0.7253094911575317
Epoch 360, training loss: 0.7427762150764465 = 0.09808172285556793 + 0.1 * 6.446944713592529
Epoch 360, val loss: 0.7356299757957458
Epoch 370, training loss: 0.7346587777137756 = 0.08905047178268433 + 0.1 * 6.456082820892334
Epoch 370, val loss: 0.7465347051620483
Epoch 380, training loss: 0.724730372428894 = 0.08115008473396301 + 0.1 * 6.435802459716797
Epoch 380, val loss: 0.7579919695854187
Epoch 390, training loss: 0.7172797322273254 = 0.07416331022977829 + 0.1 * 6.431163787841797
Epoch 390, val loss: 0.7697725892066956
Epoch 400, training loss: 0.7099183201789856 = 0.06797230988740921 + 0.1 * 6.419460296630859
Epoch 400, val loss: 0.7817109823226929
Epoch 410, training loss: 0.703399121761322 = 0.06245843693614006 + 0.1 * 6.409406661987305
Epoch 410, val loss: 0.7938234806060791
Epoch 420, training loss: 0.6979776620864868 = 0.05752706527709961 + 0.1 * 6.404505729675293
Epoch 420, val loss: 0.8060498237609863
Epoch 430, training loss: 0.6943082213401794 = 0.05312590301036835 + 0.1 * 6.41182279586792
Epoch 430, val loss: 0.8181051015853882
Epoch 440, training loss: 0.6894412040710449 = 0.04920610040426254 + 0.1 * 6.402350902557373
Epoch 440, val loss: 0.8300295472145081
Epoch 450, training loss: 0.6845049262046814 = 0.04567434638738632 + 0.1 * 6.3883056640625
Epoch 450, val loss: 0.8417553901672363
Epoch 460, training loss: 0.6814848780632019 = 0.04248408228158951 + 0.1 * 6.390007495880127
Epoch 460, val loss: 0.8533104658126831
Epoch 470, training loss: 0.6776270270347595 = 0.039611656218767166 + 0.1 * 6.380153656005859
Epoch 470, val loss: 0.8647058010101318
Epoch 480, training loss: 0.6741343140602112 = 0.03700985386967659 + 0.1 * 6.37124490737915
Epoch 480, val loss: 0.8758149743080139
Epoch 490, training loss: 0.6720802187919617 = 0.034643255174160004 + 0.1 * 6.3743696212768555
Epoch 490, val loss: 0.8867336511611938
Epoch 500, training loss: 0.6687540411949158 = 0.03249240666627884 + 0.1 * 6.362616062164307
Epoch 500, val loss: 0.8973948359489441
Epoch 510, training loss: 0.6666019558906555 = 0.030530590564012527 + 0.1 * 6.360713958740234
Epoch 510, val loss: 0.9078080058097839
Epoch 520, training loss: 0.6644009351730347 = 0.028740670531988144 + 0.1 * 6.356602668762207
Epoch 520, val loss: 0.9180346727371216
Epoch 530, training loss: 0.6613729000091553 = 0.027103345841169357 + 0.1 * 6.342695236206055
Epoch 530, val loss: 0.9279441833496094
Epoch 540, training loss: 0.6589205265045166 = 0.025599775835871696 + 0.1 * 6.333207607269287
Epoch 540, val loss: 0.9377269148826599
Epoch 550, training loss: 0.6604750156402588 = 0.024214327335357666 + 0.1 * 6.362607002258301
Epoch 550, val loss: 0.947227418422699
Epoch 560, training loss: 0.6553930640220642 = 0.02294158935546875 + 0.1 * 6.324514389038086
Epoch 560, val loss: 0.9565129280090332
Epoch 570, training loss: 0.654965877532959 = 0.021767405793070793 + 0.1 * 6.331984996795654
Epoch 570, val loss: 0.9656409621238708
Epoch 580, training loss: 0.6513040661811829 = 0.020683428272604942 + 0.1 * 6.306206703186035
Epoch 580, val loss: 0.9744014143943787
Epoch 590, training loss: 0.6519025564193726 = 0.019679216668009758 + 0.1 * 6.3222336769104
Epoch 590, val loss: 0.9831380248069763
Epoch 600, training loss: 0.6496109366416931 = 0.01874713972210884 + 0.1 * 6.308637619018555
Epoch 600, val loss: 0.9916157126426697
Epoch 610, training loss: 0.6478779911994934 = 0.017880283296108246 + 0.1 * 6.2999773025512695
Epoch 610, val loss: 0.9999520778656006
Epoch 620, training loss: 0.6451596021652222 = 0.017072541639208794 + 0.1 * 6.28087043762207
Epoch 620, val loss: 1.0080584287643433
Epoch 630, training loss: 0.6448612809181213 = 0.01631917990744114 + 0.1 * 6.285420894622803
Epoch 630, val loss: 1.0160824060440063
Epoch 640, training loss: 0.6447670459747314 = 0.015615541487932205 + 0.1 * 6.291515350341797
Epoch 640, val loss: 1.0237644910812378
Epoch 650, training loss: 0.641812264919281 = 0.014959761872887611 + 0.1 * 6.268525123596191
Epoch 650, val loss: 1.0314710140228271
Epoch 660, training loss: 0.6424293518066406 = 0.014343555085361004 + 0.1 * 6.280858039855957
Epoch 660, val loss: 1.0390105247497559
Epoch 670, training loss: 0.6407390832901001 = 0.013765583746135235 + 0.1 * 6.269734859466553
Epoch 670, val loss: 1.0462195873260498
Epoch 680, training loss: 0.6402090191841125 = 0.013223730958998203 + 0.1 * 6.269853115081787
Epoch 680, val loss: 1.053508996963501
Epoch 690, training loss: 0.6411599516868591 = 0.01271282508969307 + 0.1 * 6.284471035003662
Epoch 690, val loss: 1.0605624914169312
Epoch 700, training loss: 0.638047993183136 = 0.012233045883476734 + 0.1 * 6.25814962387085
Epoch 700, val loss: 1.0674041509628296
Epoch 710, training loss: 0.6372960209846497 = 0.011779914610087872 + 0.1 * 6.255160808563232
Epoch 710, val loss: 1.0743212699890137
Epoch 720, training loss: 0.6372562050819397 = 0.011352911591529846 + 0.1 * 6.259033203125
Epoch 720, val loss: 1.0808714628219604
Epoch 730, training loss: 0.6348715424537659 = 0.010949373245239258 + 0.1 * 6.239221572875977
Epoch 730, val loss: 1.0874003171920776
Epoch 740, training loss: 0.6347982883453369 = 0.010567966848611832 + 0.1 * 6.242303371429443
Epoch 740, val loss: 1.093895673751831
Epoch 750, training loss: 0.6341567039489746 = 0.01020737923681736 + 0.1 * 6.239493370056152
Epoch 750, val loss: 1.1001375913619995
Epoch 760, training loss: 0.6343744993209839 = 0.00986463576555252 + 0.1 * 6.24509859085083
Epoch 760, val loss: 1.1063759326934814
Epoch 770, training loss: 0.6334004402160645 = 0.009539592079818249 + 0.1 * 6.238608360290527
Epoch 770, val loss: 1.1124433279037476
Epoch 780, training loss: 0.632177472114563 = 0.00923214852809906 + 0.1 * 6.229453086853027
Epoch 780, val loss: 1.1184747219085693
Epoch 790, training loss: 0.6337000131607056 = 0.008938726037740707 + 0.1 * 6.247612476348877
Epoch 790, val loss: 1.1243367195129395
Epoch 800, training loss: 0.6308448910713196 = 0.008661109022796154 + 0.1 * 6.221837997436523
Epoch 800, val loss: 1.129995346069336
Epoch 810, training loss: 0.6335671544075012 = 0.008396870456635952 + 0.1 * 6.251702785491943
Epoch 810, val loss: 1.13564932346344
Epoch 820, training loss: 0.6308823823928833 = 0.008146408013999462 + 0.1 * 6.227359771728516
Epoch 820, val loss: 1.1411970853805542
Epoch 830, training loss: 0.6296593546867371 = 0.007907073013484478 + 0.1 * 6.217522621154785
Epoch 830, val loss: 1.146713376045227
Epoch 840, training loss: 0.6294768452644348 = 0.007678634487092495 + 0.1 * 6.217981815338135
Epoch 840, val loss: 1.1520631313323975
Epoch 850, training loss: 0.6293693780899048 = 0.007460759021341801 + 0.1 * 6.219085693359375
Epoch 850, val loss: 1.157360315322876
Epoch 860, training loss: 0.6283205151557922 = 0.0072523304261267185 + 0.1 * 6.210681438446045
Epoch 860, val loss: 1.1625455617904663
Epoch 870, training loss: 0.6290589570999146 = 0.007053484208881855 + 0.1 * 6.220054626464844
Epoch 870, val loss: 1.1676632165908813
Epoch 880, training loss: 0.6286038756370544 = 0.006863523740321398 + 0.1 * 6.217403411865234
Epoch 880, val loss: 1.1726348400115967
Epoch 890, training loss: 0.6273199915885925 = 0.006681522820144892 + 0.1 * 6.206384658813477
Epoch 890, val loss: 1.177648663520813
Epoch 900, training loss: 0.6282904744148254 = 0.006506763398647308 + 0.1 * 6.217837333679199
Epoch 900, val loss: 1.1824699640274048
Epoch 910, training loss: 0.6271255016326904 = 0.006340350955724716 + 0.1 * 6.207851409912109
Epoch 910, val loss: 1.1872094869613647
Epoch 920, training loss: 0.6265122294425964 = 0.006180048454552889 + 0.1 * 6.20332145690918
Epoch 920, val loss: 1.192009687423706
Epoch 930, training loss: 0.6270468235015869 = 0.006026608869433403 + 0.1 * 6.210201740264893
Epoch 930, val loss: 1.1966615915298462
Epoch 940, training loss: 0.6261730790138245 = 0.0058792755007743835 + 0.1 * 6.202938079833984
Epoch 940, val loss: 1.2010239362716675
Epoch 950, training loss: 0.6255853772163391 = 0.0057390532456338406 + 0.1 * 6.198463439941406
Epoch 950, val loss: 1.2055420875549316
Epoch 960, training loss: 0.6255826354026794 = 0.005602882709354162 + 0.1 * 6.1997971534729
Epoch 960, val loss: 1.210031509399414
Epoch 970, training loss: 0.6246330738067627 = 0.005472364369779825 + 0.1 * 6.1916069984436035
Epoch 970, val loss: 1.2143909931182861
Epoch 980, training loss: 0.6245015859603882 = 0.005346585065126419 + 0.1 * 6.191549777984619
Epoch 980, val loss: 1.2186437845230103
Epoch 990, training loss: 0.6254765391349792 = 0.005226087290793657 + 0.1 * 6.202504634857178
Epoch 990, val loss: 1.2228918075561523
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.5867
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7962985038757324 = 1.9589101076126099 + 0.1 * 8.373883247375488
Epoch 0, val loss: 1.9652043581008911
Epoch 10, training loss: 2.7855353355407715 = 1.9481821060180664 + 0.1 * 8.373531341552734
Epoch 10, val loss: 1.952865481376648
Epoch 20, training loss: 2.7722787857055664 = 1.9351489543914795 + 0.1 * 8.371298789978027
Epoch 20, val loss: 1.9370540380477905
Epoch 30, training loss: 2.7534286975860596 = 1.9171048402786255 + 0.1 * 8.363238334655762
Epoch 30, val loss: 1.9145606756210327
Epoch 40, training loss: 2.723005533218384 = 1.891234278678894 + 0.1 * 8.317712783813477
Epoch 40, val loss: 1.882503628730774
Epoch 50, training loss: 2.664262056350708 = 1.8565304279327393 + 0.1 * 8.077316284179688
Epoch 50, val loss: 1.8421236276626587
Epoch 60, training loss: 2.573211669921875 = 1.8170512914657593 + 0.1 * 7.561604022979736
Epoch 60, val loss: 1.799942135810852
Epoch 70, training loss: 2.493656635284424 = 1.775984287261963 + 0.1 * 7.176724433898926
Epoch 70, val loss: 1.7597953081130981
Epoch 80, training loss: 2.4314873218536377 = 1.733049988746643 + 0.1 * 6.984374046325684
Epoch 80, val loss: 1.7213941812515259
Epoch 90, training loss: 2.372856378555298 = 1.6868754625320435 + 0.1 * 6.859808444976807
Epoch 90, val loss: 1.6824430227279663
Epoch 100, training loss: 2.3065645694732666 = 1.6262205839157104 + 0.1 * 6.803439140319824
Epoch 100, val loss: 1.6320075988769531
Epoch 110, training loss: 2.224848747253418 = 1.5468655824661255 + 0.1 * 6.779831886291504
Epoch 110, val loss: 1.5672552585601807
Epoch 120, training loss: 2.126469850540161 = 1.4503257274627686 + 0.1 * 6.761440753936768
Epoch 120, val loss: 1.489729404449463
Epoch 130, training loss: 2.019496440887451 = 1.3446766138076782 + 0.1 * 6.748197078704834
Epoch 130, val loss: 1.405418872833252
Epoch 140, training loss: 1.9127302169799805 = 1.2389507293701172 + 0.1 * 6.737795352935791
Epoch 140, val loss: 1.3234288692474365
Epoch 150, training loss: 1.8106913566589355 = 1.1379122734069824 + 0.1 * 6.7277913093566895
Epoch 150, val loss: 1.2467283010482788
Epoch 160, training loss: 1.7137775421142578 = 1.0419176816940308 + 0.1 * 6.718599319458008
Epoch 160, val loss: 1.1743382215499878
Epoch 170, training loss: 1.6216623783111572 = 0.9506791234016418 + 0.1 * 6.709833145141602
Epoch 170, val loss: 1.1045703887939453
Epoch 180, training loss: 1.535841464996338 = 0.8651637434959412 + 0.1 * 6.706777572631836
Epoch 180, val loss: 1.0381948947906494
Epoch 190, training loss: 1.457215666770935 = 0.7881237864494324 + 0.1 * 6.690918445587158
Epoch 190, val loss: 0.9781002402305603
Epoch 200, training loss: 1.3875430822372437 = 0.7195709943771362 + 0.1 * 6.679720878601074
Epoch 200, val loss: 0.9252104759216309
Epoch 210, training loss: 1.3247926235198975 = 0.65838223695755 + 0.1 * 6.664104461669922
Epoch 210, val loss: 0.8788955807685852
Epoch 220, training loss: 1.2685309648513794 = 0.6028016209602356 + 0.1 * 6.657293319702148
Epoch 220, val loss: 0.8384128212928772
Epoch 230, training loss: 1.2154595851898193 = 0.5518926382064819 + 0.1 * 6.635670185089111
Epoch 230, val loss: 0.8035092949867249
Epoch 240, training loss: 1.1662988662719727 = 0.5043109059333801 + 0.1 * 6.619879722595215
Epoch 240, val loss: 0.7732295989990234
Epoch 250, training loss: 1.1202418804168701 = 0.45982030034065247 + 0.1 * 6.604215145111084
Epoch 250, val loss: 0.7471652626991272
Epoch 260, training loss: 1.077946424484253 = 0.4187498986721039 + 0.1 * 6.591965198516846
Epoch 260, val loss: 0.7250627279281616
Epoch 270, training loss: 1.0386650562286377 = 0.38048189878463745 + 0.1 * 6.581831932067871
Epoch 270, val loss: 0.7068305611610413
Epoch 280, training loss: 1.0014020204544067 = 0.3448425233364105 + 0.1 * 6.565594673156738
Epoch 280, val loss: 0.6915908455848694
Epoch 290, training loss: 0.9672195911407471 = 0.31171876192092896 + 0.1 * 6.5550079345703125
Epoch 290, val loss: 0.6788010597229004
Epoch 300, training loss: 0.9356169104576111 = 0.28114956617355347 + 0.1 * 6.544673442840576
Epoch 300, val loss: 0.6681341528892517
Epoch 310, training loss: 0.9058877229690552 = 0.2529139518737793 + 0.1 * 6.52973747253418
Epoch 310, val loss: 0.6597649455070496
Epoch 320, training loss: 0.8813563585281372 = 0.2271862030029297 + 0.1 * 6.541701316833496
Epoch 320, val loss: 0.654107391834259
Epoch 330, training loss: 0.8554847836494446 = 0.20433799922466278 + 0.1 * 6.511467456817627
Epoch 330, val loss: 0.6508371233940125
Epoch 340, training loss: 0.8345907926559448 = 0.1841193288564682 + 0.1 * 6.504714488983154
Epoch 340, val loss: 0.6499882340431213
Epoch 350, training loss: 0.8162710070610046 = 0.16634969413280487 + 0.1 * 6.499212741851807
Epoch 350, val loss: 0.6514319777488708
Epoch 360, training loss: 0.7992520928382874 = 0.15082348883152008 + 0.1 * 6.484286308288574
Epoch 360, val loss: 0.6548007130622864
Epoch 370, training loss: 0.785076916217804 = 0.13719762861728668 + 0.1 * 6.478792667388916
Epoch 370, val loss: 0.6598364114761353
Epoch 380, training loss: 0.7736276388168335 = 0.12525275349617004 + 0.1 * 6.483748912811279
Epoch 380, val loss: 0.6660356521606445
Epoch 390, training loss: 0.7609678506851196 = 0.1147705614566803 + 0.1 * 6.461972236633301
Epoch 390, val loss: 0.6731014847755432
Epoch 400, training loss: 0.7527037262916565 = 0.1054944172501564 + 0.1 * 6.472092628479004
Epoch 400, val loss: 0.6807608604431152
Epoch 410, training loss: 0.7412175536155701 = 0.09727794677019119 + 0.1 * 6.439395904541016
Epoch 410, val loss: 0.6888512969017029
Epoch 420, training loss: 0.7345827221870422 = 0.08992446213960648 + 0.1 * 6.446582317352295
Epoch 420, val loss: 0.6971400380134583
Epoch 430, training loss: 0.7272052764892578 = 0.0833272635936737 + 0.1 * 6.438779830932617
Epoch 430, val loss: 0.7054437398910522
Epoch 440, training loss: 0.7193395495414734 = 0.07735465466976166 + 0.1 * 6.419848918914795
Epoch 440, val loss: 0.713797926902771
Epoch 450, training loss: 0.7131204605102539 = 0.07191137969493866 + 0.1 * 6.41209077835083
Epoch 450, val loss: 0.7221115231513977
Epoch 460, training loss: 0.707209587097168 = 0.06693681329488754 + 0.1 * 6.4027276039123535
Epoch 460, val loss: 0.7303889989852905
Epoch 470, training loss: 0.7018961310386658 = 0.06237034127116203 + 0.1 * 6.395257472991943
Epoch 470, val loss: 0.7385409474372864
Epoch 480, training loss: 0.6965100765228271 = 0.05817399173974991 + 0.1 * 6.383360385894775
Epoch 480, val loss: 0.7466250658035278
Epoch 490, training loss: 0.6943907141685486 = 0.05429063364863396 + 0.1 * 6.401000499725342
Epoch 490, val loss: 0.7546628713607788
Epoch 500, training loss: 0.6883440613746643 = 0.05073424056172371 + 0.1 * 6.376098155975342
Epoch 500, val loss: 0.7626281976699829
Epoch 510, training loss: 0.6849991083145142 = 0.04749048873782158 + 0.1 * 6.375086307525635
Epoch 510, val loss: 0.7706212401390076
Epoch 520, training loss: 0.6806759834289551 = 0.04454508423805237 + 0.1 * 6.361309051513672
Epoch 520, val loss: 0.778619110584259
Epoch 530, training loss: 0.6776388883590698 = 0.041862014681100845 + 0.1 * 6.357768535614014
Epoch 530, val loss: 0.7865570187568665
Epoch 540, training loss: 0.6743141412734985 = 0.0394119918346405 + 0.1 * 6.3490214347839355
Epoch 540, val loss: 0.7943522334098816
Epoch 550, training loss: 0.6714694499969482 = 0.03717968612909317 + 0.1 * 6.342897415161133
Epoch 550, val loss: 0.8020281791687012
Epoch 560, training loss: 0.6682683229446411 = 0.0351371131837368 + 0.1 * 6.3313117027282715
Epoch 560, val loss: 0.8095623850822449
Epoch 570, training loss: 0.6659517884254456 = 0.033265866339206696 + 0.1 * 6.326858997344971
Epoch 570, val loss: 0.8170207738876343
Epoch 580, training loss: 0.66446453332901 = 0.0315401516854763 + 0.1 * 6.3292436599731445
Epoch 580, val loss: 0.8243356943130493
Epoch 590, training loss: 0.6643980145454407 = 0.029947232455015182 + 0.1 * 6.344507694244385
Epoch 590, val loss: 0.8314886093139648
Epoch 600, training loss: 0.6598040461540222 = 0.028477532789111137 + 0.1 * 6.313264846801758
Epoch 600, val loss: 0.838585615158081
Epoch 610, training loss: 0.6574054956436157 = 0.027111461386084557 + 0.1 * 6.3029398918151855
Epoch 610, val loss: 0.8455904722213745
Epoch 620, training loss: 0.6579203009605408 = 0.025838704779744148 + 0.1 * 6.320815563201904
Epoch 620, val loss: 0.8524867296218872
Epoch 630, training loss: 0.6558430194854736 = 0.02465878054499626 + 0.1 * 6.31184196472168
Epoch 630, val loss: 0.8591154217720032
Epoch 640, training loss: 0.6532667279243469 = 0.02356484718620777 + 0.1 * 6.297018527984619
Epoch 640, val loss: 0.8657145500183105
Epoch 650, training loss: 0.6517283320426941 = 0.022539878264069557 + 0.1 * 6.291884422302246
Epoch 650, val loss: 0.8721714615821838
Epoch 660, training loss: 0.6506756544113159 = 0.021581830456852913 + 0.1 * 6.290937900543213
Epoch 660, val loss: 0.878578782081604
Epoch 670, training loss: 0.6488765478134155 = 0.02068035863339901 + 0.1 * 6.281961917877197
Epoch 670, val loss: 0.8848837614059448
Epoch 680, training loss: 0.6482353806495667 = 0.019831927493214607 + 0.1 * 6.284034252166748
Epoch 680, val loss: 0.8910176753997803
Epoch 690, training loss: 0.64717698097229 = 0.019037019461393356 + 0.1 * 6.281399726867676
Epoch 690, val loss: 0.8970759510993958
Epoch 700, training loss: 0.6465717554092407 = 0.018286550417542458 + 0.1 * 6.2828521728515625
Epoch 700, val loss: 0.9030060768127441
Epoch 710, training loss: 0.6493244767189026 = 0.01757897064089775 + 0.1 * 6.317455291748047
Epoch 710, val loss: 0.9088047742843628
Epoch 720, training loss: 0.6436952352523804 = 0.016914047300815582 + 0.1 * 6.267811298370361
Epoch 720, val loss: 0.9144601225852966
Epoch 730, training loss: 0.6427700519561768 = 0.01628497801721096 + 0.1 * 6.26485013961792
Epoch 730, val loss: 0.9201011657714844
Epoch 740, training loss: 0.6415984630584717 = 0.01568756252527237 + 0.1 * 6.259108543395996
Epoch 740, val loss: 0.925621509552002
Epoch 750, training loss: 0.6417716145515442 = 0.015121519565582275 + 0.1 * 6.266500949859619
Epoch 750, val loss: 0.9309801459312439
Epoch 760, training loss: 0.64007169008255 = 0.014586145058274269 + 0.1 * 6.254855155944824
Epoch 760, val loss: 0.9362261891365051
Epoch 770, training loss: 0.6394855380058289 = 0.014078115113079548 + 0.1 * 6.2540740966796875
Epoch 770, val loss: 0.9414833188056946
Epoch 780, training loss: 0.6385790705680847 = 0.013594872318208218 + 0.1 * 6.249842166900635
Epoch 780, val loss: 0.9465843439102173
Epoch 790, training loss: 0.6382802724838257 = 0.013135295361280441 + 0.1 * 6.2514495849609375
Epoch 790, val loss: 0.9516107439994812
Epoch 800, training loss: 0.6384747624397278 = 0.012698455713689327 + 0.1 * 6.257762908935547
Epoch 800, val loss: 0.9564727544784546
Epoch 810, training loss: 0.6368660926818848 = 0.012283838354051113 + 0.1 * 6.245822429656982
Epoch 810, val loss: 0.9613507390022278
Epoch 820, training loss: 0.6361225247383118 = 0.011887764558196068 + 0.1 * 6.242347240447998
Epoch 820, val loss: 0.9660894274711609
Epoch 830, training loss: 0.6354207992553711 = 0.011510930024087429 + 0.1 * 6.23909854888916
Epoch 830, val loss: 0.9707065224647522
Epoch 840, training loss: 0.6349905133247375 = 0.011151430197060108 + 0.1 * 6.238390922546387
Epoch 840, val loss: 0.9753528833389282
Epoch 850, training loss: 0.6344894170761108 = 0.010806702077388763 + 0.1 * 6.236827373504639
Epoch 850, val loss: 0.9797903299331665
Epoch 860, training loss: 0.6341770887374878 = 0.010480090044438839 + 0.1 * 6.236969470977783
Epoch 860, val loss: 0.9842036366462708
Epoch 870, training loss: 0.6328729391098022 = 0.010167324915528297 + 0.1 * 6.22705602645874
Epoch 870, val loss: 0.9885642528533936
Epoch 880, training loss: 0.6331300139427185 = 0.009868125431239605 + 0.1 * 6.23261833190918
Epoch 880, val loss: 0.9928719997406006
Epoch 890, training loss: 0.6351683735847473 = 0.00958190206438303 + 0.1 * 6.25586462020874
Epoch 890, val loss: 0.9970289468765259
Epoch 900, training loss: 0.6311986446380615 = 0.00930857378989458 + 0.1 * 6.218900680541992
Epoch 900, val loss: 1.0011171102523804
Epoch 910, training loss: 0.6313949227333069 = 0.009046421386301517 + 0.1 * 6.223484516143799
Epoch 910, val loss: 1.005215048789978
Epoch 920, training loss: 0.6316280961036682 = 0.00879478920251131 + 0.1 * 6.228332996368408
Epoch 920, val loss: 1.009198546409607
Epoch 930, training loss: 0.6311896443367004 = 0.008554225787520409 + 0.1 * 6.226354122161865
Epoch 930, val loss: 1.013031244277954
Epoch 940, training loss: 0.630108118057251 = 0.008324919268488884 + 0.1 * 6.217831611633301
Epoch 940, val loss: 1.0168859958648682
Epoch 950, training loss: 0.6308003067970276 = 0.008104192093014717 + 0.1 * 6.2269606590271
Epoch 950, val loss: 1.0207115411758423
Epoch 960, training loss: 0.6289746165275574 = 0.007892822846770287 + 0.1 * 6.210817337036133
Epoch 960, val loss: 1.02437162399292
Epoch 970, training loss: 0.6289001107215881 = 0.0076904296875 + 0.1 * 6.212096691131592
Epoch 970, val loss: 1.0280370712280273
Epoch 980, training loss: 0.6292366981506348 = 0.007495118770748377 + 0.1 * 6.217415809631348
Epoch 980, val loss: 1.0316245555877686
Epoch 990, training loss: 0.6284517645835876 = 0.0073081208392977715 + 0.1 * 6.2114362716674805
Epoch 990, val loss: 1.0351663827896118
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5277
Flip ASR: 0.4933/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.783970355987549 = 1.946581482887268 + 0.1 * 8.373888969421387
Epoch 0, val loss: 1.9493896961212158
Epoch 10, training loss: 2.7743146419525146 = 1.9369356632232666 + 0.1 * 8.373788833618164
Epoch 10, val loss: 1.9398019313812256
Epoch 20, training loss: 2.7622580528259277 = 1.9249321222305298 + 0.1 * 8.373258590698242
Epoch 20, val loss: 1.9274985790252686
Epoch 30, training loss: 2.744697332382202 = 1.9077516794204712 + 0.1 * 8.36945629119873
Epoch 30, val loss: 1.9096053838729858
Epoch 40, training loss: 2.7162885665893555 = 1.8818119764328003 + 0.1 * 8.344766616821289
Epoch 40, val loss: 1.8825887441635132
Epoch 50, training loss: 2.666666030883789 = 1.8448506593704224 + 0.1 * 8.218154907226562
Epoch 50, val loss: 1.8457684516906738
Epoch 60, training loss: 2.5819976329803467 = 1.8019322156906128 + 0.1 * 7.800654888153076
Epoch 60, val loss: 1.8066331148147583
Epoch 70, training loss: 2.503694534301758 = 1.7623956203460693 + 0.1 * 7.412989616394043
Epoch 70, val loss: 1.771938681602478
Epoch 80, training loss: 2.4291937351226807 = 1.7188040018081665 + 0.1 * 7.1038970947265625
Epoch 80, val loss: 1.7327451705932617
Epoch 90, training loss: 2.3542819023132324 = 1.6631139516830444 + 0.1 * 6.911679267883301
Epoch 90, val loss: 1.6826480627059937
Epoch 100, training loss: 2.2727017402648926 = 1.5904828310012817 + 0.1 * 6.822189807891846
Epoch 100, val loss: 1.6197348833084106
Epoch 110, training loss: 2.179333448410034 = 1.501481056213379 + 0.1 * 6.7785234451293945
Epoch 110, val loss: 1.5462546348571777
Epoch 120, training loss: 2.0770411491394043 = 1.4012991189956665 + 0.1 * 6.757420063018799
Epoch 120, val loss: 1.4659959077835083
Epoch 130, training loss: 1.971443772315979 = 1.2972816228866577 + 0.1 * 6.741621494293213
Epoch 130, val loss: 1.3850449323654175
Epoch 140, training loss: 1.8665695190429688 = 1.1938096284866333 + 0.1 * 6.727599143981934
Epoch 140, val loss: 1.3067107200622559
Epoch 150, training loss: 1.7660608291625977 = 1.0945378541946411 + 0.1 * 6.715230464935303
Epoch 150, val loss: 1.2323979139328003
Epoch 160, training loss: 1.6737908124923706 = 1.0033713579177856 + 0.1 * 6.70419454574585
Epoch 160, val loss: 1.1650681495666504
Epoch 170, training loss: 1.5917303562164307 = 0.9222227931022644 + 0.1 * 6.695075035095215
Epoch 170, val loss: 1.1060949563980103
Epoch 180, training loss: 1.5170373916625977 = 0.8488029837608337 + 0.1 * 6.682344436645508
Epoch 180, val loss: 1.0531303882598877
Epoch 190, training loss: 1.4466352462768555 = 0.7797818183898926 + 0.1 * 6.668534278869629
Epoch 190, val loss: 1.003670334815979
Epoch 200, training loss: 1.3794739246368408 = 0.7138984203338623 + 0.1 * 6.655754566192627
Epoch 200, val loss: 0.9564495086669922
Epoch 210, training loss: 1.314962387084961 = 0.6512759327888489 + 0.1 * 6.63686466217041
Epoch 210, val loss: 0.9119143486022949
Epoch 220, training loss: 1.2535078525543213 = 0.5914923548698425 + 0.1 * 6.62015438079834
Epoch 220, val loss: 0.8698959946632385
Epoch 230, training loss: 1.19580078125 = 0.534870982170105 + 0.1 * 6.609298229217529
Epoch 230, val loss: 0.8313049674034119
Epoch 240, training loss: 1.14108145236969 = 0.48164647817611694 + 0.1 * 6.5943498611450195
Epoch 240, val loss: 0.7967654466629028
Epoch 250, training loss: 1.0904558897018433 = 0.4312966465950012 + 0.1 * 6.591592311859131
Epoch 250, val loss: 0.7661938071250916
Epoch 260, training loss: 1.041784644126892 = 0.3843364119529724 + 0.1 * 6.574482440948486
Epoch 260, val loss: 0.7397654056549072
Epoch 270, training loss: 0.9973633885383606 = 0.34062039852142334 + 0.1 * 6.567430019378662
Epoch 270, val loss: 0.7169979810714722
Epoch 280, training loss: 0.9565670490264893 = 0.3008559048175812 + 0.1 * 6.557111740112305
Epoch 280, val loss: 0.6979092955589294
Epoch 290, training loss: 0.9205176830291748 = 0.2651664912700653 + 0.1 * 6.553511619567871
Epoch 290, val loss: 0.6823474168777466
Epoch 300, training loss: 0.8877465724945068 = 0.23337000608444214 + 0.1 * 6.543765544891357
Epoch 300, val loss: 0.6697880625724792
Epoch 310, training loss: 0.8595392107963562 = 0.20536547899246216 + 0.1 * 6.541737079620361
Epoch 310, val loss: 0.6604151129722595
Epoch 320, training loss: 0.8351831436157227 = 0.1812298744916916 + 0.1 * 6.539532661437988
Epoch 320, val loss: 0.6541515588760376
Epoch 330, training loss: 0.8139249682426453 = 0.1604655534029007 + 0.1 * 6.5345940589904785
Epoch 330, val loss: 0.6506553888320923
Epoch 340, training loss: 0.7947807312011719 = 0.14268431067466736 + 0.1 * 6.520964622497559
Epoch 340, val loss: 0.6495911478996277
Epoch 350, training loss: 0.7784411311149597 = 0.12738002836704254 + 0.1 * 6.510611057281494
Epoch 350, val loss: 0.6506389379501343
Epoch 360, training loss: 0.7635250091552734 = 0.11411724239587784 + 0.1 * 6.494077205657959
Epoch 360, val loss: 0.653571605682373
Epoch 370, training loss: 0.7509968280792236 = 0.10261474549770355 + 0.1 * 6.48382043838501
Epoch 370, val loss: 0.6579623818397522
Epoch 380, training loss: 0.7412893772125244 = 0.09274942427873611 + 0.1 * 6.4853997230529785
Epoch 380, val loss: 0.6635990738868713
Epoch 390, training loss: 0.7335126399993896 = 0.08415123075246811 + 0.1 * 6.493614196777344
Epoch 390, val loss: 0.669816792011261
Epoch 400, training loss: 0.7228813171386719 = 0.07666905969381332 + 0.1 * 6.462122440338135
Epoch 400, val loss: 0.6768374443054199
Epoch 410, training loss: 0.7148052453994751 = 0.0700952485203743 + 0.1 * 6.447099685668945
Epoch 410, val loss: 0.6843188405036926
Epoch 420, training loss: 0.7080289125442505 = 0.06426795572042465 + 0.1 * 6.437609672546387
Epoch 420, val loss: 0.6921038031578064
Epoch 430, training loss: 0.7026416659355164 = 0.05910991504788399 + 0.1 * 6.435317516326904
Epoch 430, val loss: 0.6999147534370422
Epoch 440, training loss: 0.6964945197105408 = 0.05454898998141289 + 0.1 * 6.419455528259277
Epoch 440, val loss: 0.7079964876174927
Epoch 450, training loss: 0.6921122670173645 = 0.05048449710011482 + 0.1 * 6.416277885437012
Epoch 450, val loss: 0.7160677909851074
Epoch 460, training loss: 0.6878097057342529 = 0.04685310274362564 + 0.1 * 6.4095659255981445
Epoch 460, val loss: 0.7238626480102539
Epoch 470, training loss: 0.6830386519432068 = 0.04359443113207817 + 0.1 * 6.394442081451416
Epoch 470, val loss: 0.7319278120994568
Epoch 480, training loss: 0.6794307827949524 = 0.04066765680909157 + 0.1 * 6.387630939483643
Epoch 480, val loss: 0.7394757866859436
Epoch 490, training loss: 0.6756467223167419 = 0.038037579506635666 + 0.1 * 6.376091003417969
Epoch 490, val loss: 0.7472938299179077
Epoch 500, training loss: 0.6727252006530762 = 0.03565376251935959 + 0.1 * 6.37071418762207
Epoch 500, val loss: 0.7549329400062561
Epoch 510, training loss: 0.670070469379425 = 0.03348376229405403 + 0.1 * 6.3658671379089355
Epoch 510, val loss: 0.7622455358505249
Epoch 520, training loss: 0.6671653985977173 = 0.03150768205523491 + 0.1 * 6.356577396392822
Epoch 520, val loss: 0.7697563767433167
Epoch 530, training loss: 0.6648934483528137 = 0.02970592863857746 + 0.1 * 6.351874828338623
Epoch 530, val loss: 0.7767876386642456
Epoch 540, training loss: 0.6614326238632202 = 0.028059987351298332 + 0.1 * 6.333726406097412
Epoch 540, val loss: 0.7840163707733154
Epoch 550, training loss: 0.6613891124725342 = 0.026548026129603386 + 0.1 * 6.348410606384277
Epoch 550, val loss: 0.7911109328269958
Epoch 560, training loss: 0.6584905385971069 = 0.025154951959848404 + 0.1 * 6.333355903625488
Epoch 560, val loss: 0.7979406118392944
Epoch 570, training loss: 0.6556696891784668 = 0.023870373144745827 + 0.1 * 6.3179931640625
Epoch 570, val loss: 0.8046447038650513
Epoch 580, training loss: 0.654248833656311 = 0.022683750838041306 + 0.1 * 6.315650939941406
Epoch 580, val loss: 0.8113656640052795
Epoch 590, training loss: 0.6533822417259216 = 0.021584738045930862 + 0.1 * 6.317975044250488
Epoch 590, val loss: 0.8178358674049377
Epoch 600, training loss: 0.6520599126815796 = 0.020566178485751152 + 0.1 * 6.314937591552734
Epoch 600, val loss: 0.8242699503898621
Epoch 610, training loss: 0.6496214866638184 = 0.019620517268776894 + 0.1 * 6.300009250640869
Epoch 610, val loss: 0.8305200934410095
Epoch 620, training loss: 0.648774266242981 = 0.018739208579063416 + 0.1 * 6.300350189208984
Epoch 620, val loss: 0.8368096947669983
Epoch 630, training loss: 0.6472537517547607 = 0.017917221412062645 + 0.1 * 6.293365001678467
Epoch 630, val loss: 0.8428736925125122
Epoch 640, training loss: 0.6453878879547119 = 0.01715012639760971 + 0.1 * 6.28237771987915
Epoch 640, val loss: 0.8486140370368958
Epoch 650, training loss: 0.6443212032318115 = 0.016434134915471077 + 0.1 * 6.278870582580566
Epoch 650, val loss: 0.8545864224433899
Epoch 660, training loss: 0.6437022089958191 = 0.015763284638524055 + 0.1 * 6.279389381408691
Epoch 660, val loss: 0.8602041006088257
Epoch 670, training loss: 0.6421313285827637 = 0.015135008841753006 + 0.1 * 6.269963264465332
Epoch 670, val loss: 0.8658510446548462
Epoch 680, training loss: 0.6420120000839233 = 0.014543941244482994 + 0.1 * 6.274680137634277
Epoch 680, val loss: 0.8712817430496216
Epoch 690, training loss: 0.6406377553939819 = 0.013988162390887737 + 0.1 * 6.266496181488037
Epoch 690, val loss: 0.8768101930618286
Epoch 700, training loss: 0.6422518491744995 = 0.013464036397635937 + 0.1 * 6.287878036499023
Epoch 700, val loss: 0.8821159601211548
Epoch 710, training loss: 0.6386677026748657 = 0.012971797026693821 + 0.1 * 6.256958961486816
Epoch 710, val loss: 0.887284517288208
Epoch 720, training loss: 0.6376357078552246 = 0.012505711987614632 + 0.1 * 6.251299858093262
Epoch 720, val loss: 0.8925678730010986
Epoch 730, training loss: 0.6368268728256226 = 0.012064745649695396 + 0.1 * 6.247621059417725
Epoch 730, val loss: 0.8975759744644165
Epoch 740, training loss: 0.6388523578643799 = 0.011648125946521759 + 0.1 * 6.272042274475098
Epoch 740, val loss: 0.9024331569671631
Epoch 750, training loss: 0.6359363198280334 = 0.011256192810833454 + 0.1 * 6.246800899505615
Epoch 750, val loss: 0.9073470234870911
Epoch 760, training loss: 0.6342563629150391 = 0.010884569957852364 + 0.1 * 6.233717918395996
Epoch 760, val loss: 0.9123368263244629
Epoch 770, training loss: 0.6352702975273132 = 0.010531148873269558 + 0.1 * 6.247391700744629
Epoch 770, val loss: 0.9172046184539795
Epoch 780, training loss: 0.634056806564331 = 0.01019524596631527 + 0.1 * 6.2386155128479
Epoch 780, val loss: 0.9216612577438354
Epoch 790, training loss: 0.632686972618103 = 0.00987690594047308 + 0.1 * 6.228100776672363
Epoch 790, val loss: 0.9262554049491882
Epoch 800, training loss: 0.6320361495018005 = 0.009574076160788536 + 0.1 * 6.224620819091797
Epoch 800, val loss: 0.930891215801239
Epoch 810, training loss: 0.6330816149711609 = 0.009286214597523212 + 0.1 * 6.2379536628723145
Epoch 810, val loss: 0.9353287816047668
Epoch 820, training loss: 0.6311013102531433 = 0.009011495858430862 + 0.1 * 6.220897674560547
Epoch 820, val loss: 0.9396631121635437
Epoch 830, training loss: 0.6318584084510803 = 0.008749625645577908 + 0.1 * 6.231087684631348
Epoch 830, val loss: 0.9439929723739624
Epoch 840, training loss: 0.6296905279159546 = 0.008500082418322563 + 0.1 * 6.211904048919678
Epoch 840, val loss: 0.9482493996620178
Epoch 850, training loss: 0.630813717842102 = 0.008261153474450111 + 0.1 * 6.225525856018066
Epoch 850, val loss: 0.9525359272956848
Epoch 860, training loss: 0.6294942498207092 = 0.008033182471990585 + 0.1 * 6.214610576629639
Epoch 860, val loss: 0.956585705280304
Epoch 870, training loss: 0.6320798993110657 = 0.007815337739884853 + 0.1 * 6.242645263671875
Epoch 870, val loss: 0.9606055617332458
Epoch 880, training loss: 0.6292905807495117 = 0.007607744541019201 + 0.1 * 6.216827869415283
Epoch 880, val loss: 0.9645025134086609
Epoch 890, training loss: 0.6280470490455627 = 0.0074092671275138855 + 0.1 * 6.2063775062561035
Epoch 890, val loss: 0.9686329960823059
Epoch 900, training loss: 0.6282896399497986 = 0.007218318060040474 + 0.1 * 6.2107133865356445
Epoch 900, val loss: 0.9726155400276184
Epoch 910, training loss: 0.6282684206962585 = 0.0070353164337575436 + 0.1 * 6.2123308181762695
Epoch 910, val loss: 0.9763898849487305
Epoch 920, training loss: 0.6264945864677429 = 0.0068597993813455105 + 0.1 * 6.196347713470459
Epoch 920, val loss: 0.9800877571105957
Epoch 930, training loss: 0.6274634003639221 = 0.006691284012049437 + 0.1 * 6.20772123336792
Epoch 930, val loss: 0.9839041233062744
Epoch 940, training loss: 0.6269839406013489 = 0.006529573816806078 + 0.1 * 6.204543590545654
Epoch 940, val loss: 0.9876223802566528
Epoch 950, training loss: 0.6255967020988464 = 0.0063744205981493 + 0.1 * 6.192222595214844
Epoch 950, val loss: 0.9911572337150574
Epoch 960, training loss: 0.626560628414154 = 0.006225057411938906 + 0.1 * 6.20335578918457
Epoch 960, val loss: 0.9947670102119446
Epoch 970, training loss: 0.6255127191543579 = 0.006081591825932264 + 0.1 * 6.194311141967773
Epoch 970, val loss: 0.9982834458351135
Epoch 980, training loss: 0.6261717081069946 = 0.005943490192294121 + 0.1 * 6.202282428741455
Epoch 980, val loss: 1.001802921295166
Epoch 990, training loss: 0.6256539821624756 = 0.005810259375721216 + 0.1 * 6.198437213897705
Epoch 990, val loss: 1.005123496055603
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8819
Flip ASR: 0.8578/225 nodes
The final ASR:0.66544, 0.15496, Accuracy:0.83086, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11646])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10610])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83457, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7750701904296875 = 1.937683343887329 + 0.1 * 8.373867988586426
Epoch 0, val loss: 1.9314186573028564
Epoch 10, training loss: 2.76491117477417 = 1.9275341033935547 + 0.1 * 8.373770713806152
Epoch 10, val loss: 1.9215927124023438
Epoch 20, training loss: 2.752652645111084 = 1.915337085723877 + 0.1 * 8.37315559387207
Epoch 20, val loss: 1.9098411798477173
Epoch 30, training loss: 2.7354512214660645 = 1.898612380027771 + 0.1 * 8.368388175964355
Epoch 30, val loss: 1.8938714265823364
Epoch 40, training loss: 2.707977294921875 = 1.8741633892059326 + 0.1 * 8.338138580322266
Epoch 40, val loss: 1.8710963726043701
Epoch 50, training loss: 2.658963680267334 = 1.8402132987976074 + 0.1 * 8.187504768371582
Epoch 50, val loss: 1.8410215377807617
Epoch 60, training loss: 2.583308219909668 = 1.799479603767395 + 0.1 * 7.838284969329834
Epoch 60, val loss: 1.8068814277648926
Epoch 70, training loss: 2.521272897720337 = 1.7543684244155884 + 0.1 * 7.669044494628906
Epoch 70, val loss: 1.7698559761047363
Epoch 80, training loss: 2.4524683952331543 = 1.7029623985290527 + 0.1 * 7.495059013366699
Epoch 80, val loss: 1.727002501487732
Epoch 90, training loss: 2.365081310272217 = 1.6396632194519043 + 0.1 * 7.254181861877441
Epoch 90, val loss: 1.6734873056411743
Epoch 100, training loss: 2.2570269107818604 = 1.5610401630401611 + 0.1 * 6.959866523742676
Epoch 100, val loss: 1.6068694591522217
Epoch 110, training loss: 2.151038885116577 = 1.4702441692352295 + 0.1 * 6.807946681976318
Epoch 110, val loss: 1.5334599018096924
Epoch 120, training loss: 2.0523810386657715 = 1.3784854412078857 + 0.1 * 6.738954544067383
Epoch 120, val loss: 1.460519790649414
Epoch 130, training loss: 1.9654276371002197 = 1.2934032678604126 + 0.1 * 6.720243453979492
Epoch 130, val loss: 1.3940644264221191
Epoch 140, training loss: 1.8860714435577393 = 1.2155537605285645 + 0.1 * 6.705177307128906
Epoch 140, val loss: 1.3356895446777344
Epoch 150, training loss: 1.8131648302078247 = 1.1434764862060547 + 0.1 * 6.696883201599121
Epoch 150, val loss: 1.2826604843139648
Epoch 160, training loss: 1.742734432220459 = 1.073504090309143 + 0.1 * 6.692303657531738
Epoch 160, val loss: 1.231786847114563
Epoch 170, training loss: 1.671097993850708 = 1.0025091171264648 + 0.1 * 6.685888290405273
Epoch 170, val loss: 1.1804012060165405
Epoch 180, training loss: 1.5972373485565186 = 0.9291316270828247 + 0.1 * 6.681056499481201
Epoch 180, val loss: 1.126282811164856
Epoch 190, training loss: 1.5219234228134155 = 0.8544265627861023 + 0.1 * 6.674968242645264
Epoch 190, val loss: 1.0706415176391602
Epoch 200, training loss: 1.447131633758545 = 0.7802009582519531 + 0.1 * 6.669307231903076
Epoch 200, val loss: 1.015225887298584
Epoch 210, training loss: 1.3740286827087402 = 0.7082017064094543 + 0.1 * 6.658270359039307
Epoch 210, val loss: 0.9619734883308411
Epoch 220, training loss: 1.303848385810852 = 0.639100968837738 + 0.1 * 6.6474738121032715
Epoch 220, val loss: 0.9115981459617615
Epoch 230, training loss: 1.2375469207763672 = 0.5738373398780823 + 0.1 * 6.6370954513549805
Epoch 230, val loss: 0.864861011505127
Epoch 240, training loss: 1.1760551929473877 = 0.5134426951408386 + 0.1 * 6.626124858856201
Epoch 240, val loss: 0.8229265213012695
Epoch 250, training loss: 1.1196039915084839 = 0.4579305350780487 + 0.1 * 6.616734981536865
Epoch 250, val loss: 0.7862532734870911
Epoch 260, training loss: 1.0681090354919434 = 0.40706533193588257 + 0.1 * 6.610437393188477
Epoch 260, val loss: 0.7548443078994751
Epoch 270, training loss: 1.020533561706543 = 0.36044609546661377 + 0.1 * 6.600873947143555
Epoch 270, val loss: 0.728541374206543
Epoch 280, training loss: 0.976607084274292 = 0.3172636926174164 + 0.1 * 6.593433380126953
Epoch 280, val loss: 0.7060920000076294
Epoch 290, training loss: 0.9364051818847656 = 0.2777212858200073 + 0.1 * 6.586838722229004
Epoch 290, val loss: 0.6873565912246704
Epoch 300, training loss: 0.899831235408783 = 0.2420673817396164 + 0.1 * 6.577638149261475
Epoch 300, val loss: 0.6725091934204102
Epoch 310, training loss: 0.8678046464920044 = 0.21051834523677826 + 0.1 * 6.57286262512207
Epoch 310, val loss: 0.6617591381072998
Epoch 320, training loss: 0.8391983509063721 = 0.18327733874320984 + 0.1 * 6.559210300445557
Epoch 320, val loss: 0.6550949811935425
Epoch 330, training loss: 0.8149102926254272 = 0.15995639562606812 + 0.1 * 6.549539089202881
Epoch 330, val loss: 0.6522359848022461
Epoch 340, training loss: 0.7945709228515625 = 0.14013442397117615 + 0.1 * 6.544364929199219
Epoch 340, val loss: 0.6524293422698975
Epoch 350, training loss: 0.7765816450119019 = 0.12329407781362534 + 0.1 * 6.532876014709473
Epoch 350, val loss: 0.655178964138031
Epoch 360, training loss: 0.7617977261543274 = 0.10888107866048813 + 0.1 * 6.5291666984558105
Epoch 360, val loss: 0.6597358584403992
Epoch 370, training loss: 0.7489051222801208 = 0.09654409438371658 + 0.1 * 6.5236101150512695
Epoch 370, val loss: 0.6657213568687439
Epoch 380, training loss: 0.7369824647903442 = 0.08592169731855392 + 0.1 * 6.510607719421387
Epoch 380, val loss: 0.6725628972053528
Epoch 390, training loss: 0.7283387780189514 = 0.07672011107206345 + 0.1 * 6.516186714172363
Epoch 390, val loss: 0.6800907850265503
Epoch 400, training loss: 0.7190585136413574 = 0.06875599920749664 + 0.1 * 6.503025054931641
Epoch 400, val loss: 0.6880974769592285
Epoch 410, training loss: 0.7117539644241333 = 0.06180733069777489 + 0.1 * 6.499466419219971
Epoch 410, val loss: 0.6964622139930725
Epoch 420, training loss: 0.70540851354599 = 0.055753644555807114 + 0.1 * 6.496548652648926
Epoch 420, val loss: 0.7048922777175903
Epoch 430, training loss: 0.6987720727920532 = 0.05046605318784714 + 0.1 * 6.483059883117676
Epoch 430, val loss: 0.7135154008865356
Epoch 440, training loss: 0.6944819688796997 = 0.04581887647509575 + 0.1 * 6.486630916595459
Epoch 440, val loss: 0.7222757935523987
Epoch 450, training loss: 0.6889321208000183 = 0.04176060110330582 + 0.1 * 6.471714973449707
Epoch 450, val loss: 0.7308408617973328
Epoch 460, training loss: 0.6847109794616699 = 0.038195956498384476 + 0.1 * 6.465149879455566
Epoch 460, val loss: 0.7394632697105408
Epoch 470, training loss: 0.6828185319900513 = 0.03504623845219612 + 0.1 * 6.477723121643066
Epoch 470, val loss: 0.7480029463768005
Epoch 480, training loss: 0.6790897846221924 = 0.03227897733449936 + 0.1 * 6.468108177185059
Epoch 480, val loss: 0.756129801273346
Epoch 490, training loss: 0.6745758652687073 = 0.029827844351530075 + 0.1 * 6.447480201721191
Epoch 490, val loss: 0.7641977667808533
Epoch 500, training loss: 0.6717283129692078 = 0.0276411771774292 + 0.1 * 6.440871238708496
Epoch 500, val loss: 0.772148072719574
Epoch 510, training loss: 0.6694623827934265 = 0.02568650245666504 + 0.1 * 6.437758922576904
Epoch 510, val loss: 0.7797096371650696
Epoch 520, training loss: 0.667290985584259 = 0.023940540850162506 + 0.1 * 6.433504104614258
Epoch 520, val loss: 0.7873034477233887
Epoch 530, training loss: 0.6684449911117554 = 0.02237406000494957 + 0.1 * 6.460709571838379
Epoch 530, val loss: 0.7945297956466675
Epoch 540, training loss: 0.662928581237793 = 0.020972561091184616 + 0.1 * 6.419560432434082
Epoch 540, val loss: 0.801470935344696
Epoch 550, training loss: 0.6604960560798645 = 0.01970730535686016 + 0.1 * 6.4078874588012695
Epoch 550, val loss: 0.8083794713020325
Epoch 560, training loss: 0.6580321788787842 = 0.0185592882335186 + 0.1 * 6.394729137420654
Epoch 560, val loss: 0.8148453235626221
Epoch 570, training loss: 0.6564638614654541 = 0.01751604676246643 + 0.1 * 6.3894782066345215
Epoch 570, val loss: 0.8214171528816223
Epoch 580, training loss: 0.6555251479148865 = 0.01656791754066944 + 0.1 * 6.389571666717529
Epoch 580, val loss: 0.8275443315505981
Epoch 590, training loss: 0.6538978219032288 = 0.015702692791819572 + 0.1 * 6.381951332092285
Epoch 590, val loss: 0.8337055444717407
Epoch 600, training loss: 0.6521007418632507 = 0.014908491633832455 + 0.1 * 6.371922492980957
Epoch 600, val loss: 0.8396979570388794
Epoch 610, training loss: 0.652198314666748 = 0.014179494231939316 + 0.1 * 6.380188465118408
Epoch 610, val loss: 0.8451788425445557
Epoch 620, training loss: 0.6491434574127197 = 0.01351232174783945 + 0.1 * 6.356311321258545
Epoch 620, val loss: 0.8508874177932739
Epoch 630, training loss: 0.6479172110557556 = 0.012893457897007465 + 0.1 * 6.3502373695373535
Epoch 630, val loss: 0.8564705848693848
Epoch 640, training loss: 0.6468109488487244 = 0.012319588102400303 + 0.1 * 6.344913482666016
Epoch 640, val loss: 0.8615838289260864
Epoch 650, training loss: 0.6449176669120789 = 0.011789289303123951 + 0.1 * 6.331284046173096
Epoch 650, val loss: 0.8667186498641968
Epoch 660, training loss: 0.6445661187171936 = 0.011296880431473255 + 0.1 * 6.332692623138428
Epoch 660, val loss: 0.8718767166137695
Epoch 670, training loss: 0.6464007496833801 = 0.010838172398507595 + 0.1 * 6.355625629425049
Epoch 670, val loss: 0.876808226108551
Epoch 680, training loss: 0.6417216062545776 = 0.010411065071821213 + 0.1 * 6.31310510635376
Epoch 680, val loss: 0.8815019130706787
Epoch 690, training loss: 0.6410287022590637 = 0.01001090370118618 + 0.1 * 6.310177803039551
Epoch 690, val loss: 0.8863683938980103
Epoch 700, training loss: 0.6402372717857361 = 0.009635587222874165 + 0.1 * 6.30601692199707
Epoch 700, val loss: 0.8909462690353394
Epoch 710, training loss: 0.6402767300605774 = 0.009284596890211105 + 0.1 * 6.3099212646484375
Epoch 710, val loss: 0.895403265953064
Epoch 720, training loss: 0.6389875411987305 = 0.008955301716923714 + 0.1 * 6.300322532653809
Epoch 720, val loss: 0.8999515175819397
Epoch 730, training loss: 0.6371603608131409 = 0.008644395507872105 + 0.1 * 6.285159587860107
Epoch 730, val loss: 0.9044014811515808
Epoch 740, training loss: 0.6382452845573425 = 0.008350285701453686 + 0.1 * 6.298949718475342
Epoch 740, val loss: 0.908627986907959
Epoch 750, training loss: 0.6370280981063843 = 0.008074833080172539 + 0.1 * 6.289532661437988
Epoch 750, val loss: 0.9127528667449951
Epoch 760, training loss: 0.6368216276168823 = 0.00781523808836937 + 0.1 * 6.290063858032227
Epoch 760, val loss: 0.9170380234718323
Epoch 770, training loss: 0.6346498131752014 = 0.0075686099007725716 + 0.1 * 6.270812034606934
Epoch 770, val loss: 0.9211487770080566
Epoch 780, training loss: 0.6343553066253662 = 0.007334554567933083 + 0.1 * 6.27020788192749
Epoch 780, val loss: 0.9250593781471252
Epoch 790, training loss: 0.6330050826072693 = 0.00711318664252758 + 0.1 * 6.2589192390441895
Epoch 790, val loss: 0.928966760635376
Epoch 800, training loss: 0.6352454423904419 = 0.006902353372424841 + 0.1 * 6.283431053161621
Epoch 800, val loss: 0.9328536987304688
Epoch 810, training loss: 0.632835328578949 = 0.006703547202050686 + 0.1 * 6.261317729949951
Epoch 810, val loss: 0.9366627335548401
Epoch 820, training loss: 0.6322122812271118 = 0.006513436324894428 + 0.1 * 6.256988525390625
Epoch 820, val loss: 0.9404594898223877
Epoch 830, training loss: 0.6325100064277649 = 0.006332428194582462 + 0.1 * 6.261775493621826
Epoch 830, val loss: 0.9440389275550842
Epoch 840, training loss: 0.6302332282066345 = 0.006160444114357233 + 0.1 * 6.24072790145874
Epoch 840, val loss: 0.9476626515388489
Epoch 850, training loss: 0.6330851316452026 = 0.005995923653244972 + 0.1 * 6.270892143249512
Epoch 850, val loss: 0.951298713684082
Epoch 860, training loss: 0.6299749612808228 = 0.005839542485773563 + 0.1 * 6.241354465484619
Epoch 860, val loss: 0.9547569155693054
Epoch 870, training loss: 0.629202127456665 = 0.0056899129413068295 + 0.1 * 6.235122203826904
Epoch 870, val loss: 0.9582560062408447
Epoch 880, training loss: 0.6338499188423157 = 0.005546098574995995 + 0.1 * 6.283038139343262
Epoch 880, val loss: 0.9617251753807068
Epoch 890, training loss: 0.6285609006881714 = 0.005409387871623039 + 0.1 * 6.231515407562256
Epoch 890, val loss: 0.9648478031158447
Epoch 900, training loss: 0.6285091638565063 = 0.005278897006064653 + 0.1 * 6.232302665710449
Epoch 900, val loss: 0.9682237505912781
Epoch 910, training loss: 0.6285626888275146 = 0.00515325553715229 + 0.1 * 6.234094619750977
Epoch 910, val loss: 0.971501350402832
Epoch 920, training loss: 0.6268892288208008 = 0.0050327349454164505 + 0.1 * 6.218564987182617
Epoch 920, val loss: 0.9747049808502197
Epoch 930, training loss: 0.6275098323822021 = 0.004916641861200333 + 0.1 * 6.225931644439697
Epoch 930, val loss: 0.9777551889419556
Epoch 940, training loss: 0.6262422800064087 = 0.004805718548595905 + 0.1 * 6.214365005493164
Epoch 940, val loss: 0.9807989001274109
Epoch 950, training loss: 0.6263907551765442 = 0.004698980133980513 + 0.1 * 6.216917514801025
Epoch 950, val loss: 0.9839511513710022
Epoch 960, training loss: 0.6268235445022583 = 0.004596163053065538 + 0.1 * 6.222273826599121
Epoch 960, val loss: 0.9869339466094971
Epoch 970, training loss: 0.6259356737136841 = 0.004497552290558815 + 0.1 * 6.214381217956543
Epoch 970, val loss: 0.9897645115852356
Epoch 980, training loss: 0.6260359883308411 = 0.004402757156640291 + 0.1 * 6.21633243560791
Epoch 980, val loss: 0.9927893877029419
Epoch 990, training loss: 0.6248194575309753 = 0.004310933407396078 + 0.1 * 6.205085277557373
Epoch 990, val loss: 0.9956120252609253
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.6531
Flip ASR: 0.5822/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7646985054016113 = 1.9273117780685425 + 0.1 * 8.373867988586426
Epoch 0, val loss: 1.9231232404708862
Epoch 10, training loss: 2.755342721939087 = 1.9179707765579224 + 0.1 * 8.373720169067383
Epoch 10, val loss: 1.9137043952941895
Epoch 20, training loss: 2.74338960647583 = 1.9060850143432617 + 0.1 * 8.373045921325684
Epoch 20, val loss: 1.9012082815170288
Epoch 30, training loss: 2.7257091999053955 = 1.888861060142517 + 0.1 * 8.368480682373047
Epoch 30, val loss: 1.8828096389770508
Epoch 40, training loss: 2.697096347808838 = 1.8632874488830566 + 0.1 * 8.338088035583496
Epoch 40, val loss: 1.8558512926101685
Epoch 50, training loss: 2.641568660736084 = 1.8287070989608765 + 0.1 * 8.128615379333496
Epoch 50, val loss: 1.821252703666687
Epoch 60, training loss: 2.553445339202881 = 1.7914448976516724 + 0.1 * 7.6200056076049805
Epoch 60, val loss: 1.785847783088684
Epoch 70, training loss: 2.4798953533172607 = 1.7544505596160889 + 0.1 * 7.254446983337402
Epoch 70, val loss: 1.7511687278747559
Epoch 80, training loss: 2.410093307495117 = 1.7117090225219727 + 0.1 * 6.983843803405762
Epoch 80, val loss: 1.7120355367660522
Epoch 90, training loss: 2.339937210083008 = 1.6566308736801147 + 0.1 * 6.833063125610352
Epoch 90, val loss: 1.6621577739715576
Epoch 100, training loss: 2.260150671005249 = 1.582749366760254 + 0.1 * 6.774013042449951
Epoch 100, val loss: 1.596046805381775
Epoch 110, training loss: 2.165207862854004 = 1.4903677701950073 + 0.1 * 6.748401641845703
Epoch 110, val loss: 1.5163601636886597
Epoch 120, training loss: 2.060816764831543 = 1.3878355026245117 + 0.1 * 6.7298126220703125
Epoch 120, val loss: 1.4319472312927246
Epoch 130, training loss: 1.9556405544281006 = 1.284030556678772 + 0.1 * 6.716099262237549
Epoch 130, val loss: 1.3501994609832764
Epoch 140, training loss: 1.853345513343811 = 1.1828778982162476 + 0.1 * 6.704676151275635
Epoch 140, val loss: 1.2736823558807373
Epoch 150, training loss: 1.7548569440841675 = 1.0855333805084229 + 0.1 * 6.693235397338867
Epoch 150, val loss: 1.2026464939117432
Epoch 160, training loss: 1.660536289215088 = 0.9925159215927124 + 0.1 * 6.680202960968018
Epoch 160, val loss: 1.1358826160430908
Epoch 170, training loss: 1.5705633163452148 = 0.9038557410240173 + 0.1 * 6.667075157165527
Epoch 170, val loss: 1.0734257698059082
Epoch 180, training loss: 1.48576819896698 = 0.8203025460243225 + 0.1 * 6.654656410217285
Epoch 180, val loss: 1.0152678489685059
Epoch 190, training loss: 1.4065983295440674 = 0.7423409223556519 + 0.1 * 6.642573356628418
Epoch 190, val loss: 0.9617869257926941
Epoch 200, training loss: 1.3347140550613403 = 0.6706681251525879 + 0.1 * 6.640459060668945
Epoch 200, val loss: 0.9135212302207947
Epoch 210, training loss: 1.2693496942520142 = 0.6066970229148865 + 0.1 * 6.626526832580566
Epoch 210, val loss: 0.8719947338104248
Epoch 220, training loss: 1.2116563320159912 = 0.5498329401016235 + 0.1 * 6.6182332038879395
Epoch 220, val loss: 0.8368521928787231
Epoch 230, training loss: 1.1602540016174316 = 0.4991331696510315 + 0.1 * 6.611207962036133
Epoch 230, val loss: 0.8079083561897278
Epoch 240, training loss: 1.1141659021377563 = 0.4534439742565155 + 0.1 * 6.607219219207764
Epoch 240, val loss: 0.7843484282493591
Epoch 250, training loss: 1.071460485458374 = 0.4116036295890808 + 0.1 * 6.598567962646484
Epoch 250, val loss: 0.7652941346168518
Epoch 260, training loss: 1.0317656993865967 = 0.3726412057876587 + 0.1 * 6.591244220733643
Epoch 260, val loss: 0.7490972876548767
Epoch 270, training loss: 0.9966937303543091 = 0.33678489923477173 + 0.1 * 6.599088191986084
Epoch 270, val loss: 0.736117422580719
Epoch 280, training loss: 0.9615164399147034 = 0.3039165735244751 + 0.1 * 6.575998783111572
Epoch 280, val loss: 0.7251191735267639
Epoch 290, training loss: 0.9304558038711548 = 0.2736339569091797 + 0.1 * 6.568218231201172
Epoch 290, val loss: 0.716469407081604
Epoch 300, training loss: 0.9022420048713684 = 0.2459602802991867 + 0.1 * 6.562817096710205
Epoch 300, val loss: 0.7101086378097534
Epoch 310, training loss: 0.8767327070236206 = 0.22108493745326996 + 0.1 * 6.5564775466918945
Epoch 310, val loss: 0.7060388922691345
Epoch 320, training loss: 0.8526922464370728 = 0.19876807928085327 + 0.1 * 6.539241790771484
Epoch 320, val loss: 0.704379141330719
Epoch 330, training loss: 0.8316271305084229 = 0.17874202132225037 + 0.1 * 6.52885103225708
Epoch 330, val loss: 0.704994261264801
Epoch 340, training loss: 0.8135687708854675 = 0.16091501712799072 + 0.1 * 6.5265374183654785
Epoch 340, val loss: 0.7075576186180115
Epoch 350, training loss: 0.7964373230934143 = 0.14516840875148773 + 0.1 * 6.512689113616943
Epoch 350, val loss: 0.711836576461792
Epoch 360, training loss: 0.783420741558075 = 0.13117371499538422 + 0.1 * 6.522469997406006
Epoch 360, val loss: 0.7178465127944946
Epoch 370, training loss: 0.7684667706489563 = 0.11881744116544724 + 0.1 * 6.496493339538574
Epoch 370, val loss: 0.7251103520393372
Epoch 380, training loss: 0.7559497952461243 = 0.10784385353326797 + 0.1 * 6.4810590744018555
Epoch 380, val loss: 0.7335586547851562
Epoch 390, training loss: 0.7464348673820496 = 0.09812097251415253 + 0.1 * 6.483138561248779
Epoch 390, val loss: 0.7429636716842651
Epoch 400, training loss: 0.7357991933822632 = 0.08956333994865417 + 0.1 * 6.462357997894287
Epoch 400, val loss: 0.7527191638946533
Epoch 410, training loss: 0.7273334860801697 = 0.08196486532688141 + 0.1 * 6.453686237335205
Epoch 410, val loss: 0.7632545232772827
Epoch 420, training loss: 0.7196098566055298 = 0.07520809769630432 + 0.1 * 6.4440178871154785
Epoch 420, val loss: 0.7743405103683472
Epoch 430, training loss: 0.7136964797973633 = 0.06921033561229706 + 0.1 * 6.444860935211182
Epoch 430, val loss: 0.7854354977607727
Epoch 440, training loss: 0.7067307829856873 = 0.0638575404882431 + 0.1 * 6.428731918334961
Epoch 440, val loss: 0.7967817783355713
Epoch 450, training loss: 0.7018727660179138 = 0.05905008688569069 + 0.1 * 6.428226470947266
Epoch 450, val loss: 0.8083486557006836
Epoch 460, training loss: 0.6965603828430176 = 0.05472511425614357 + 0.1 * 6.4183526039123535
Epoch 460, val loss: 0.8200111389160156
Epoch 470, training loss: 0.6935495734214783 = 0.050824396312236786 + 0.1 * 6.42725133895874
Epoch 470, val loss: 0.8317817449569702
Epoch 480, training loss: 0.6876064538955688 = 0.04730420932173729 + 0.1 * 6.403022289276123
Epoch 480, val loss: 0.8432888388633728
Epoch 490, training loss: 0.6853669881820679 = 0.044111672788858414 + 0.1 * 6.412553310394287
Epoch 490, val loss: 0.8549866080284119
Epoch 500, training loss: 0.6802718043327332 = 0.041218601167201996 + 0.1 * 6.39053201675415
Epoch 500, val loss: 0.8664654493331909
Epoch 510, training loss: 0.6766600012779236 = 0.03857692703604698 + 0.1 * 6.380830764770508
Epoch 510, val loss: 0.8779363632202148
Epoch 520, training loss: 0.6736372709274292 = 0.036149896681308746 + 0.1 * 6.374873638153076
Epoch 520, val loss: 0.8892607688903809
Epoch 530, training loss: 0.672990083694458 = 0.033928513526916504 + 0.1 * 6.390615463256836
Epoch 530, val loss: 0.9007259011268616
Epoch 540, training loss: 0.6688305735588074 = 0.03189878165721893 + 0.1 * 6.369317531585693
Epoch 540, val loss: 0.9116041660308838
Epoch 550, training loss: 0.6651058793067932 = 0.030026104301214218 + 0.1 * 6.350797653198242
Epoch 550, val loss: 0.9226744174957275
Epoch 560, training loss: 0.6640880703926086 = 0.028295913711190224 + 0.1 * 6.357921123504639
Epoch 560, val loss: 0.9336153864860535
Epoch 570, training loss: 0.6623547673225403 = 0.02670109272003174 + 0.1 * 6.356536865234375
Epoch 570, val loss: 0.9443953633308411
Epoch 580, training loss: 0.6593242287635803 = 0.025234099477529526 + 0.1 * 6.3409013748168945
Epoch 580, val loss: 0.9548284411430359
Epoch 590, training loss: 0.657849907875061 = 0.02387971058487892 + 0.1 * 6.3397016525268555
Epoch 590, val loss: 0.9650887846946716
Epoch 600, training loss: 0.6558419466018677 = 0.022627122700214386 + 0.1 * 6.33214807510376
Epoch 600, val loss: 0.975128710269928
Epoch 610, training loss: 0.6547368168830872 = 0.02146877534687519 + 0.1 * 6.3326802253723145
Epoch 610, val loss: 0.9850644469261169
Epoch 620, training loss: 0.6519878506660461 = 0.020397238433361053 + 0.1 * 6.315906047821045
Epoch 620, val loss: 0.9946643114089966
Epoch 630, training loss: 0.6501424312591553 = 0.019405709579586983 + 0.1 * 6.307366847991943
Epoch 630, val loss: 1.004037857055664
Epoch 640, training loss: 0.6496961712837219 = 0.018487725406885147 + 0.1 * 6.312084197998047
Epoch 640, val loss: 1.01311457157135
Epoch 650, training loss: 0.648004949092865 = 0.017636064440011978 + 0.1 * 6.3036885261535645
Epoch 650, val loss: 1.0218982696533203
Epoch 660, training loss: 0.6468786001205444 = 0.016843244433403015 + 0.1 * 6.300353050231934
Epoch 660, val loss: 1.0305708646774292
Epoch 670, training loss: 0.6461936235427856 = 0.016106365248560905 + 0.1 * 6.300872325897217
Epoch 670, val loss: 1.0389277935028076
Epoch 680, training loss: 0.6443582773208618 = 0.015418732538819313 + 0.1 * 6.289395332336426
Epoch 680, val loss: 1.0470657348632812
Epoch 690, training loss: 0.6439302563667297 = 0.014777286909520626 + 0.1 * 6.291529178619385
Epoch 690, val loss: 1.0549802780151367
Epoch 700, training loss: 0.6420380473136902 = 0.014178200624883175 + 0.1 * 6.278598308563232
Epoch 700, val loss: 1.0628119707107544
Epoch 710, training loss: 0.6412876844406128 = 0.013617169111967087 + 0.1 * 6.276705265045166
Epoch 710, val loss: 1.0702595710754395
Epoch 720, training loss: 0.6407225728034973 = 0.013090466149151325 + 0.1 * 6.276320457458496
Epoch 720, val loss: 1.077767014503479
Epoch 730, training loss: 0.6396142840385437 = 0.01259563211351633 + 0.1 * 6.270186424255371
Epoch 730, val loss: 1.0850645303726196
Epoch 740, training loss: 0.6397811770439148 = 0.01212957315146923 + 0.1 * 6.276515960693359
Epoch 740, val loss: 1.0922271013259888
Epoch 750, training loss: 0.63787841796875 = 0.011693608947098255 + 0.1 * 6.261847972869873
Epoch 750, val loss: 1.0991020202636719
Epoch 760, training loss: 0.6380725502967834 = 0.011281956918537617 + 0.1 * 6.2679057121276855
Epoch 760, val loss: 1.105840802192688
Epoch 770, training loss: 0.636492908000946 = 0.010893084108829498 + 0.1 * 6.255998134613037
Epoch 770, val loss: 1.1125264167785645
Epoch 780, training loss: 0.6369103789329529 = 0.010525827296078205 + 0.1 * 6.263845443725586
Epoch 780, val loss: 1.1190249919891357
Epoch 790, training loss: 0.634706437587738 = 0.010178455151617527 + 0.1 * 6.245279788970947
Epoch 790, val loss: 1.1254324913024902
Epoch 800, training loss: 0.6342297792434692 = 0.009849793277680874 + 0.1 * 6.243799686431885
Epoch 800, val loss: 1.1316012144088745
Epoch 810, training loss: 0.6349375247955322 = 0.009537743404507637 + 0.1 * 6.253997802734375
Epoch 810, val loss: 1.137811303138733
Epoch 820, training loss: 0.6339278817176819 = 0.009242150001227856 + 0.1 * 6.246857166290283
Epoch 820, val loss: 1.143842339515686
Epoch 830, training loss: 0.6327745914459229 = 0.008962451480329037 + 0.1 * 6.238121032714844
Epoch 830, val loss: 1.1495611667633057
Epoch 840, training loss: 0.631921648979187 = 0.008696152828633785 + 0.1 * 6.232254981994629
Epoch 840, val loss: 1.1553912162780762
Epoch 850, training loss: 0.6313925385475159 = 0.008442509919404984 + 0.1 * 6.229499816894531
Epoch 850, val loss: 1.161094069480896
Epoch 860, training loss: 0.6313436627388 = 0.008200651034712791 + 0.1 * 6.2314300537109375
Epoch 860, val loss: 1.1665958166122437
Epoch 870, training loss: 0.6310680508613586 = 0.007969988510012627 + 0.1 * 6.23098087310791
Epoch 870, val loss: 1.1722339391708374
Epoch 880, training loss: 0.6308993101119995 = 0.007750442251563072 + 0.1 * 6.231488227844238
Epoch 880, val loss: 1.1775171756744385
Epoch 890, training loss: 0.629805326461792 = 0.007540903519839048 + 0.1 * 6.222644329071045
Epoch 890, val loss: 1.1828230619430542
Epoch 900, training loss: 0.6288732886314392 = 0.00734062260016799 + 0.1 * 6.21532678604126
Epoch 900, val loss: 1.1879277229309082
Epoch 910, training loss: 0.6306549906730652 = 0.00714851776137948 + 0.1 * 6.235064506530762
Epoch 910, val loss: 1.1930288076400757
Epoch 920, training loss: 0.6285789012908936 = 0.006964807398617268 + 0.1 * 6.2161407470703125
Epoch 920, val loss: 1.1981123685836792
Epoch 930, training loss: 0.6279497742652893 = 0.006788764614611864 + 0.1 * 6.211609840393066
Epoch 930, val loss: 1.2029714584350586
Epoch 940, training loss: 0.6267518997192383 = 0.006620015483349562 + 0.1 * 6.201318740844727
Epoch 940, val loss: 1.2079144716262817
Epoch 950, training loss: 0.6285142302513123 = 0.006458558142185211 + 0.1 * 6.220556259155273
Epoch 950, val loss: 1.2126630544662476
Epoch 960, training loss: 0.6274843811988831 = 0.006303665693849325 + 0.1 * 6.211806774139404
Epoch 960, val loss: 1.217415690422058
Epoch 970, training loss: 0.6269607543945312 = 0.006155140697956085 + 0.1 * 6.2080559730529785
Epoch 970, val loss: 1.221912145614624
Epoch 980, training loss: 0.6257500648498535 = 0.00601210119202733 + 0.1 * 6.1973795890808105
Epoch 980, val loss: 1.2265599966049194
Epoch 990, training loss: 0.6264723539352417 = 0.005875021684914827 + 0.1 * 6.205973148345947
Epoch 990, val loss: 1.2309856414794922
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9188
Flip ASR: 0.9022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.784064531326294 = 1.946675419807434 + 0.1 * 8.373891830444336
Epoch 0, val loss: 1.9420592784881592
Epoch 10, training loss: 2.774256706237793 = 1.9368733167648315 + 0.1 * 8.373834609985352
Epoch 10, val loss: 1.93194580078125
Epoch 20, training loss: 2.762953042984009 = 1.9256045818328857 + 0.1 * 8.373483657836914
Epoch 20, val loss: 1.9203357696533203
Epoch 30, training loss: 2.747560977935791 = 1.9104392528533936 + 0.1 * 8.3712158203125
Epoch 30, val loss: 1.9048792123794556
Epoch 40, training loss: 2.7234573364257812 = 1.8880586624145508 + 0.1 * 8.353985786437988
Epoch 40, val loss: 1.8824797868728638
Epoch 50, training loss: 2.680041790008545 = 1.855391263961792 + 0.1 * 8.246504783630371
Epoch 50, val loss: 1.850839376449585
Epoch 60, training loss: 2.589108467102051 = 1.814741611480713 + 0.1 * 7.743669033050537
Epoch 60, val loss: 1.814756155014038
Epoch 70, training loss: 2.5114057064056396 = 1.7752740383148193 + 0.1 * 7.361317157745361
Epoch 70, val loss: 1.7818593978881836
Epoch 80, training loss: 2.437680721282959 = 1.7352879047393799 + 0.1 * 7.023929119110107
Epoch 80, val loss: 1.74870765209198
Epoch 90, training loss: 2.373279094696045 = 1.6869169473648071 + 0.1 * 6.863620758056641
Epoch 90, val loss: 1.7075697183609009
Epoch 100, training loss: 2.302607536315918 = 1.6232784986495972 + 0.1 * 6.793290138244629
Epoch 100, val loss: 1.65440034866333
Epoch 110, training loss: 2.216460704803467 = 1.54083251953125 + 0.1 * 6.756281852722168
Epoch 110, val loss: 1.587244987487793
Epoch 120, training loss: 2.1139211654663086 = 1.4403870105743408 + 0.1 * 6.735341548919678
Epoch 120, val loss: 1.505865216255188
Epoch 130, training loss: 2.0007951259613037 = 1.329007863998413 + 0.1 * 6.717872142791748
Epoch 130, val loss: 1.4158955812454224
Epoch 140, training loss: 1.8869686126708984 = 1.216680645942688 + 0.1 * 6.702878952026367
Epoch 140, val loss: 1.3247623443603516
Epoch 150, training loss: 1.7788503170013428 = 1.1097564697265625 + 0.1 * 6.690938472747803
Epoch 150, val loss: 1.2378615140914917
Epoch 160, training loss: 1.6786515712738037 = 1.0105394124984741 + 0.1 * 6.681122303009033
Epoch 160, val loss: 1.15777587890625
Epoch 170, training loss: 1.5863983631134033 = 0.9192292094230652 + 0.1 * 6.671691417694092
Epoch 170, val loss: 1.085922122001648
Epoch 180, training loss: 1.5018181800842285 = 0.8358035683631897 + 0.1 * 6.660146236419678
Epoch 180, val loss: 1.0224554538726807
Epoch 190, training loss: 1.4241094589233398 = 0.759242594242096 + 0.1 * 6.648667812347412
Epoch 190, val loss: 0.9653925895690918
Epoch 200, training loss: 1.353074312210083 = 0.6894533634185791 + 0.1 * 6.636209011077881
Epoch 200, val loss: 0.9146048426628113
Epoch 210, training loss: 1.2882893085479736 = 0.6260295510292053 + 0.1 * 6.622596740722656
Epoch 210, val loss: 0.8698336482048035
Epoch 220, training loss: 1.2293200492858887 = 0.5683245062828064 + 0.1 * 6.609955787658691
Epoch 220, val loss: 0.8305931091308594
Epoch 230, training loss: 1.1759341955184937 = 0.5158669352531433 + 0.1 * 6.600672721862793
Epoch 230, val loss: 0.7968951463699341
Epoch 240, training loss: 1.1260712146759033 = 0.4672825336456299 + 0.1 * 6.587886810302734
Epoch 240, val loss: 0.7675117254257202
Epoch 250, training loss: 1.0806574821472168 = 0.4218447208404541 + 0.1 * 6.588127613067627
Epoch 250, val loss: 0.7420334815979004
Epoch 260, training loss: 1.037585735321045 = 0.3799664378166199 + 0.1 * 6.576192378997803
Epoch 260, val loss: 0.7206780910491943
Epoch 270, training loss: 0.9976739287376404 = 0.3410215377807617 + 0.1 * 6.566524028778076
Epoch 270, val loss: 0.7026816010475159
Epoch 280, training loss: 0.9603898525238037 = 0.30458441376686096 + 0.1 * 6.558054447174072
Epoch 280, val loss: 0.6879458427429199
Epoch 290, training loss: 0.925668478012085 = 0.2704926133155823 + 0.1 * 6.551758766174316
Epoch 290, val loss: 0.6760258674621582
Epoch 300, training loss: 0.8934189677238464 = 0.2388288378715515 + 0.1 * 6.545901298522949
Epoch 300, val loss: 0.6669284105300903
Epoch 310, training loss: 0.8641250729560852 = 0.209821879863739 + 0.1 * 6.543031692504883
Epoch 310, val loss: 0.6604321002960205
Epoch 320, training loss: 0.8370754718780518 = 0.18347008526325226 + 0.1 * 6.536053657531738
Epoch 320, val loss: 0.6563363671302795
Epoch 330, training loss: 0.8128774762153625 = 0.15995655953884125 + 0.1 * 6.529209136962891
Epoch 330, val loss: 0.654788076877594
Epoch 340, training loss: 0.7917704582214355 = 0.1394694745540619 + 0.1 * 6.52301025390625
Epoch 340, val loss: 0.6557132601737976
Epoch 350, training loss: 0.7740738391876221 = 0.12196484208106995 + 0.1 * 6.521089553833008
Epoch 350, val loss: 0.6590976119041443
Epoch 360, training loss: 0.7589179277420044 = 0.10714767873287201 + 0.1 * 6.517702102661133
Epoch 360, val loss: 0.664341390132904
Epoch 370, training loss: 0.7458274364471436 = 0.09465654194355011 + 0.1 * 6.511708736419678
Epoch 370, val loss: 0.6713050007820129
Epoch 380, training loss: 0.7342218160629272 = 0.0840580090880394 + 0.1 * 6.501638412475586
Epoch 380, val loss: 0.6794883608818054
Epoch 390, training loss: 0.7256721258163452 = 0.07503053545951843 + 0.1 * 6.506415367126465
Epoch 390, val loss: 0.6885490417480469
Epoch 400, training loss: 0.7171433568000793 = 0.06735307723283768 + 0.1 * 6.497902870178223
Epoch 400, val loss: 0.6983195543289185
Epoch 410, training loss: 0.7095397710800171 = 0.06075922027230263 + 0.1 * 6.487805366516113
Epoch 410, val loss: 0.7082651853561401
Epoch 420, training loss: 0.7033355236053467 = 0.05505587160587311 + 0.1 * 6.482796669006348
Epoch 420, val loss: 0.7184815406799316
Epoch 430, training loss: 0.6982475519180298 = 0.05012327805161476 + 0.1 * 6.481242656707764
Epoch 430, val loss: 0.7288159132003784
Epoch 440, training loss: 0.693023681640625 = 0.0458252876996994 + 0.1 * 6.471983432769775
Epoch 440, val loss: 0.7389804124832153
Epoch 450, training loss: 0.6894323825836182 = 0.04205954074859619 + 0.1 * 6.473728179931641
Epoch 450, val loss: 0.749053418636322
Epoch 460, training loss: 0.6849818825721741 = 0.03875427320599556 + 0.1 * 6.462275981903076
Epoch 460, val loss: 0.7589347958564758
Epoch 470, training loss: 0.6815466284751892 = 0.03582334145903587 + 0.1 * 6.457232475280762
Epoch 470, val loss: 0.768531858921051
Epoch 480, training loss: 0.6791664958000183 = 0.03321696072816849 + 0.1 * 6.4594950675964355
Epoch 480, val loss: 0.7780344486236572
Epoch 490, training loss: 0.6747044920921326 = 0.030894244089722633 + 0.1 * 6.438102722167969
Epoch 490, val loss: 0.7871937155723572
Epoch 500, training loss: 0.6732688546180725 = 0.02881333790719509 + 0.1 * 6.444554805755615
Epoch 500, val loss: 0.7961660027503967
Epoch 510, training loss: 0.6689050793647766 = 0.026960041373968124 + 0.1 * 6.419450283050537
Epoch 510, val loss: 0.8047910928726196
Epoch 520, training loss: 0.6662948727607727 = 0.02529252879321575 + 0.1 * 6.4100236892700195
Epoch 520, val loss: 0.8132094740867615
Epoch 530, training loss: 0.6682314872741699 = 0.02378014661371708 + 0.1 * 6.444512844085693
Epoch 530, val loss: 0.8214269876480103
Epoch 540, training loss: 0.6642966866493225 = 0.022413721308112144 + 0.1 * 6.418829441070557
Epoch 540, val loss: 0.8291983008384705
Epoch 550, training loss: 0.6604365110397339 = 0.021172724664211273 + 0.1 * 6.392638206481934
Epoch 550, val loss: 0.8369421362876892
Epoch 560, training loss: 0.6580926775932312 = 0.020034076645970345 + 0.1 * 6.38058614730835
Epoch 560, val loss: 0.844353199005127
Epoch 570, training loss: 0.6577748656272888 = 0.01898832991719246 + 0.1 * 6.3878655433654785
Epoch 570, val loss: 0.8515771627426147
Epoch 580, training loss: 0.65578293800354 = 0.018032323569059372 + 0.1 * 6.377506256103516
Epoch 580, val loss: 0.8586724400520325
Epoch 590, training loss: 0.6529479026794434 = 0.01715189591050148 + 0.1 * 6.357959747314453
Epoch 590, val loss: 0.865554690361023
Epoch 600, training loss: 0.6571698188781738 = 0.016335278749465942 + 0.1 * 6.408344745635986
Epoch 600, val loss: 0.8721157908439636
Epoch 610, training loss: 0.6513821482658386 = 0.015585246495902538 + 0.1 * 6.357968807220459
Epoch 610, val loss: 0.8786678314208984
Epoch 620, training loss: 0.648753821849823 = 0.014891748316586018 + 0.1 * 6.338620662689209
Epoch 620, val loss: 0.8850638270378113
Epoch 630, training loss: 0.6487433910369873 = 0.014244597405195236 + 0.1 * 6.344987392425537
Epoch 630, val loss: 0.8911912441253662
Epoch 640, training loss: 0.6478652954101562 = 0.013641930185258389 + 0.1 * 6.342233180999756
Epoch 640, val loss: 0.8971819281578064
Epoch 650, training loss: 0.6467278599739075 = 0.01308029517531395 + 0.1 * 6.336475372314453
Epoch 650, val loss: 0.9031214118003845
Epoch 660, training loss: 0.6440085172653198 = 0.012558098882436752 + 0.1 * 6.314504146575928
Epoch 660, val loss: 0.908926248550415
Epoch 670, training loss: 0.645007312297821 = 0.012069357559084892 + 0.1 * 6.329379558563232
Epoch 670, val loss: 0.9145628809928894
Epoch 680, training loss: 0.6436500549316406 = 0.011611294001340866 + 0.1 * 6.320387363433838
Epoch 680, val loss: 0.9200129508972168
Epoch 690, training loss: 0.6416488885879517 = 0.011181319132447243 + 0.1 * 6.304675579071045
Epoch 690, val loss: 0.9252997040748596
Epoch 700, training loss: 0.6401854753494263 = 0.010778791271150112 + 0.1 * 6.294066905975342
Epoch 700, val loss: 0.9306377172470093
Epoch 710, training loss: 0.6412380933761597 = 0.01039846520870924 + 0.1 * 6.308396339416504
Epoch 710, val loss: 0.9358254671096802
Epoch 720, training loss: 0.6399703025817871 = 0.010038967244327068 + 0.1 * 6.299313068389893
Epoch 720, val loss: 0.9407727122306824
Epoch 730, training loss: 0.6382579207420349 = 0.00970104243606329 + 0.1 * 6.285568714141846
Epoch 730, val loss: 0.9457958936691284
Epoch 740, training loss: 0.6380741596221924 = 0.009383446536958218 + 0.1 * 6.286907196044922
Epoch 740, val loss: 0.9506179094314575
Epoch 750, training loss: 0.6380352973937988 = 0.009082643315196037 + 0.1 * 6.289526462554932
Epoch 750, val loss: 0.9554648995399475
Epoch 760, training loss: 0.6352927088737488 = 0.008796685375273228 + 0.1 * 6.264959812164307
Epoch 760, val loss: 0.9600648283958435
Epoch 770, training loss: 0.6357446908950806 = 0.008525533601641655 + 0.1 * 6.272191047668457
Epoch 770, val loss: 0.9645951986312866
Epoch 780, training loss: 0.6362901926040649 = 0.008268221281468868 + 0.1 * 6.280219554901123
Epoch 780, val loss: 0.9691063761711121
Epoch 790, training loss: 0.6342650055885315 = 0.008024240843951702 + 0.1 * 6.2624077796936035
Epoch 790, val loss: 0.9734398722648621
Epoch 800, training loss: 0.6336382627487183 = 0.007791843265295029 + 0.1 * 6.258464336395264
Epoch 800, val loss: 0.9777333736419678
Epoch 810, training loss: 0.6329902410507202 = 0.007571160327643156 + 0.1 * 6.254190444946289
Epoch 810, val loss: 0.9819352626800537
Epoch 820, training loss: 0.6325020790100098 = 0.007361165713518858 + 0.1 * 6.25140905380249
Epoch 820, val loss: 0.9860841035842896
Epoch 830, training loss: 0.633590817451477 = 0.0071607958525419235 + 0.1 * 6.264300346374512
Epoch 830, val loss: 0.9900745749473572
Epoch 840, training loss: 0.6325435638427734 = 0.00696930754929781 + 0.1 * 6.255742073059082
Epoch 840, val loss: 0.9941250085830688
Epoch 850, training loss: 0.6327682733535767 = 0.00678667426109314 + 0.1 * 6.2598161697387695
Epoch 850, val loss: 0.997977614402771
Epoch 860, training loss: 0.6300570964813232 = 0.00661216676235199 + 0.1 * 6.2344489097595215
Epoch 860, val loss: 1.001867651939392
Epoch 870, training loss: 0.6299182772636414 = 0.006445121485739946 + 0.1 * 6.234731674194336
Epoch 870, val loss: 1.0056583881378174
Epoch 880, training loss: 0.6301746964454651 = 0.00628502294421196 + 0.1 * 6.23889684677124
Epoch 880, val loss: 1.0093618631362915
Epoch 890, training loss: 0.629095733165741 = 0.006132230628281832 + 0.1 * 6.229634761810303
Epoch 890, val loss: 1.0130832195281982
Epoch 900, training loss: 0.6292323470115662 = 0.005985597614198923 + 0.1 * 6.232467174530029
Epoch 900, val loss: 1.0166999101638794
Epoch 910, training loss: 0.6295138001441956 = 0.005844326224178076 + 0.1 * 6.2366943359375
Epoch 910, val loss: 1.0202422142028809
Epoch 920, training loss: 0.6270862221717834 = 0.005708926357328892 + 0.1 * 6.213772773742676
Epoch 920, val loss: 1.0236709117889404
Epoch 930, training loss: 0.628492534160614 = 0.005579216871410608 + 0.1 * 6.229133129119873
Epoch 930, val loss: 1.027087688446045
Epoch 940, training loss: 0.6269259452819824 = 0.005454107653349638 + 0.1 * 6.214718341827393
Epoch 940, val loss: 1.030540108680725
Epoch 950, training loss: 0.6293392181396484 = 0.005334228277206421 + 0.1 * 6.240050315856934
Epoch 950, val loss: 1.0338799953460693
Epoch 960, training loss: 0.6259943246841431 = 0.005218425765633583 + 0.1 * 6.20775842666626
Epoch 960, val loss: 1.0370864868164062
Epoch 970, training loss: 0.6260153651237488 = 0.0051074582152068615 + 0.1 * 6.209078788757324
Epoch 970, val loss: 1.0403159856796265
Epoch 980, training loss: 0.6261632442474365 = 0.005000153556466103 + 0.1 * 6.211630344390869
Epoch 980, val loss: 1.043467402458191
Epoch 990, training loss: 0.6253596544265747 = 0.004897302947938442 + 0.1 * 6.204623699188232
Epoch 990, val loss: 1.046600103378296
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8635
Flip ASR: 0.8356/225 nodes
The final ASR:0.81181, 0.11445, Accuracy:0.82469, 0.01397
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00460, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7822368144989014 = 1.944844365119934 + 0.1 * 8.373924255371094
Epoch 0, val loss: 1.937974214553833
Epoch 10, training loss: 2.772575855255127 = 1.9351929426193237 + 0.1 * 8.373828887939453
Epoch 10, val loss: 1.9289846420288086
Epoch 20, training loss: 2.760486364364624 = 1.9231526851654053 + 0.1 * 8.373335838317871
Epoch 20, val loss: 1.9173427820205688
Epoch 30, training loss: 2.7430806159973145 = 1.9061882495880127 + 0.1 * 8.36892318725586
Epoch 30, val loss: 1.9006292819976807
Epoch 40, training loss: 2.7145252227783203 = 1.8808938264846802 + 0.1 * 8.33631420135498
Epoch 40, val loss: 1.8759329319000244
Epoch 50, training loss: 2.661189556121826 = 1.845923662185669 + 0.1 * 8.152658462524414
Epoch 50, val loss: 1.8435893058776855
Epoch 60, training loss: 2.5952279567718506 = 1.8057546615600586 + 0.1 * 7.89473295211792
Epoch 60, val loss: 1.809372901916504
Epoch 70, training loss: 2.5359272956848145 = 1.7655869722366333 + 0.1 * 7.703402519226074
Epoch 70, val loss: 1.77742600440979
Epoch 80, training loss: 2.4618358612060547 = 1.7220274209976196 + 0.1 * 7.3980841636657715
Epoch 80, val loss: 1.74171781539917
Epoch 90, training loss: 2.378849506378174 = 1.6681448221206665 + 0.1 * 7.1070475578308105
Epoch 90, val loss: 1.6954092979431152
Epoch 100, training loss: 2.2953386306762695 = 1.5991096496582031 + 0.1 * 6.962290287017822
Epoch 100, val loss: 1.6355981826782227
Epoch 110, training loss: 2.2065505981445312 = 1.5167039632797241 + 0.1 * 6.898466110229492
Epoch 110, val loss: 1.5664030313491821
Epoch 120, training loss: 2.116712808609009 = 1.430896282196045 + 0.1 * 6.858165264129639
Epoch 120, val loss: 1.4982988834381104
Epoch 130, training loss: 2.0296318531036377 = 1.347314476966858 + 0.1 * 6.8231730461120605
Epoch 130, val loss: 1.4330998659133911
Epoch 140, training loss: 1.9424816370010376 = 1.2636319398880005 + 0.1 * 6.788496971130371
Epoch 140, val loss: 1.3693773746490479
Epoch 150, training loss: 1.8534033298492432 = 1.177042007446289 + 0.1 * 6.763613224029541
Epoch 150, val loss: 1.3044471740722656
Epoch 160, training loss: 1.7622336149215698 = 1.0879032611846924 + 0.1 * 6.743303298950195
Epoch 160, val loss: 1.2385398149490356
Epoch 170, training loss: 1.6690526008605957 = 0.9958804845809937 + 0.1 * 6.731720924377441
Epoch 170, val loss: 1.170338749885559
Epoch 180, training loss: 1.5755372047424316 = 0.9031479358673096 + 0.1 * 6.723891735076904
Epoch 180, val loss: 1.1016792058944702
Epoch 190, training loss: 1.4839293956756592 = 0.8125102519989014 + 0.1 * 6.71419095993042
Epoch 190, val loss: 1.0340352058410645
Epoch 200, training loss: 1.396648645401001 = 0.7263879776000977 + 0.1 * 6.702605724334717
Epoch 200, val loss: 0.9700387716293335
Epoch 210, training loss: 1.3168039321899414 = 0.6471700668334961 + 0.1 * 6.696338176727295
Epoch 210, val loss: 0.9116683602333069
Epoch 220, training loss: 1.243835210800171 = 0.5758097767829895 + 0.1 * 6.680253505706787
Epoch 220, val loss: 0.8605172038078308
Epoch 230, training loss: 1.1782721281051636 = 0.5113679766654968 + 0.1 * 6.669041156768799
Epoch 230, val loss: 0.8162428736686707
Epoch 240, training loss: 1.1192939281463623 = 0.4531738758087158 + 0.1 * 6.661199569702148
Epoch 240, val loss: 0.7788134813308716
Epoch 250, training loss: 1.0671271085739136 = 0.40072891116142273 + 0.1 * 6.6639814376831055
Epoch 250, val loss: 0.7482008337974548
Epoch 260, training loss: 1.0184071063995361 = 0.35383713245391846 + 0.1 * 6.645700454711914
Epoch 260, val loss: 0.7240394353866577
Epoch 270, training loss: 0.9750621318817139 = 0.31167975068092346 + 0.1 * 6.633823871612549
Epoch 270, val loss: 0.7049425840377808
Epoch 280, training loss: 0.9372528791427612 = 0.27387797832489014 + 0.1 * 6.633749008178711
Epoch 280, val loss: 0.690441906452179
Epoch 290, training loss: 0.9020959734916687 = 0.24058102071285248 + 0.1 * 6.615149021148682
Epoch 290, val loss: 0.6804187893867493
Epoch 300, training loss: 0.871853768825531 = 0.21124613285064697 + 0.1 * 6.606076240539551
Epoch 300, val loss: 0.6743569374084473
Epoch 310, training loss: 0.8450761437416077 = 0.18550002574920654 + 0.1 * 6.595761299133301
Epoch 310, val loss: 0.6721224784851074
Epoch 320, training loss: 0.8237950801849365 = 0.1633135974407196 + 0.1 * 6.6048150062561035
Epoch 320, val loss: 0.6732082962989807
Epoch 330, training loss: 0.8023492097854614 = 0.1444823294878006 + 0.1 * 6.57866907119751
Epoch 330, val loss: 0.6770672798156738
Epoch 340, training loss: 0.7860750555992126 = 0.12833549082279205 + 0.1 * 6.577395439147949
Epoch 340, val loss: 0.6829833388328552
Epoch 350, training loss: 0.7704514265060425 = 0.11446937918663025 + 0.1 * 6.559820175170898
Epoch 350, val loss: 0.6905451416969299
Epoch 360, training loss: 0.7576711773872375 = 0.10252722352743149 + 0.1 * 6.5514397621154785
Epoch 360, val loss: 0.699305534362793
Epoch 370, training loss: 0.7454506158828735 = 0.09217765182256699 + 0.1 * 6.532729625701904
Epoch 370, val loss: 0.7090374827384949
Epoch 380, training loss: 0.7361006140708923 = 0.08318501710891724 + 0.1 * 6.529155731201172
Epoch 380, val loss: 0.7194088101387024
Epoch 390, training loss: 0.725984513759613 = 0.07533816248178482 + 0.1 * 6.506463050842285
Epoch 390, val loss: 0.7302643656730652
Epoch 400, training loss: 0.7189977169036865 = 0.0684632658958435 + 0.1 * 6.505344390869141
Epoch 400, val loss: 0.7414003610610962
Epoch 410, training loss: 0.7110562324523926 = 0.06241951510310173 + 0.1 * 6.4863667488098145
Epoch 410, val loss: 0.7528892159461975
Epoch 420, training loss: 0.7055232524871826 = 0.05706483870744705 + 0.1 * 6.484583854675293
Epoch 420, val loss: 0.7644571661949158
Epoch 430, training loss: 0.6985116600990295 = 0.05231879651546478 + 0.1 * 6.461928367614746
Epoch 430, val loss: 0.7762495875358582
Epoch 440, training loss: 0.6954559087753296 = 0.048092592507600784 + 0.1 * 6.4736328125
Epoch 440, val loss: 0.7878387570381165
Epoch 450, training loss: 0.6892577409744263 = 0.044336289167404175 + 0.1 * 6.449214935302734
Epoch 450, val loss: 0.7995468378067017
Epoch 460, training loss: 0.6873636841773987 = 0.04097354784607887 + 0.1 * 6.463901519775391
Epoch 460, val loss: 0.8109012246131897
Epoch 470, training loss: 0.6817519664764404 = 0.03796849399805069 + 0.1 * 6.4378342628479
Epoch 470, val loss: 0.8222767114639282
Epoch 480, training loss: 0.6778628826141357 = 0.0352669283747673 + 0.1 * 6.42595911026001
Epoch 480, val loss: 0.833451509475708
Epoch 490, training loss: 0.675119936466217 = 0.03282786160707474 + 0.1 * 6.4229207038879395
Epoch 490, val loss: 0.8444371223449707
Epoch 500, training loss: 0.6730852127075195 = 0.03062947653234005 + 0.1 * 6.424557685852051
Epoch 500, val loss: 0.8551726341247559
Epoch 510, training loss: 0.6699691414833069 = 0.028643963858485222 + 0.1 * 6.413251876831055
Epoch 510, val loss: 0.8658196330070496
Epoch 520, training loss: 0.6680785417556763 = 0.026844652369618416 + 0.1 * 6.412338733673096
Epoch 520, val loss: 0.8760258555412292
Epoch 530, training loss: 0.6657247543334961 = 0.025214318186044693 + 0.1 * 6.405104160308838
Epoch 530, val loss: 0.8861119747161865
Epoch 540, training loss: 0.6653977036476135 = 0.023727897554636 + 0.1 * 6.416697978973389
Epoch 540, val loss: 0.895873486995697
Epoch 550, training loss: 0.6620570421218872 = 0.022376827895641327 + 0.1 * 6.396801948547363
Epoch 550, val loss: 0.9053639769554138
Epoch 560, training loss: 0.6602450609207153 = 0.02114129438996315 + 0.1 * 6.391037464141846
Epoch 560, val loss: 0.9146897196769714
Epoch 570, training loss: 0.6587713956832886 = 0.020007070153951645 + 0.1 * 6.387643337249756
Epoch 570, val loss: 0.9236996173858643
Epoch 580, training loss: 0.6566354632377625 = 0.018965715542435646 + 0.1 * 6.376697063446045
Epoch 580, val loss: 0.9326566457748413
Epoch 590, training loss: 0.6578963994979858 = 0.01800641417503357 + 0.1 * 6.398899555206299
Epoch 590, val loss: 0.9412018656730652
Epoch 600, training loss: 0.6532256603240967 = 0.017123546451330185 + 0.1 * 6.361021518707275
Epoch 600, val loss: 0.949694037437439
Epoch 610, training loss: 0.6530399322509766 = 0.0163081306964159 + 0.1 * 6.367318153381348
Epoch 610, val loss: 0.9578409790992737
Epoch 620, training loss: 0.6510599851608276 = 0.015555706806480885 + 0.1 * 6.355042934417725
Epoch 620, val loss: 0.9659289121627808
Epoch 630, training loss: 0.6511006355285645 = 0.014855454675853252 + 0.1 * 6.362452030181885
Epoch 630, val loss: 0.9738278985023499
Epoch 640, training loss: 0.6492004990577698 = 0.01420360617339611 + 0.1 * 6.349968910217285
Epoch 640, val loss: 0.9813390374183655
Epoch 650, training loss: 0.6493244171142578 = 0.013598043471574783 + 0.1 * 6.357263565063477
Epoch 650, val loss: 0.9889081120491028
Epoch 660, training loss: 0.6468843221664429 = 0.013031491078436375 + 0.1 * 6.338528156280518
Epoch 660, val loss: 0.9961047172546387
Epoch 670, training loss: 0.6461986303329468 = 0.012504534795880318 + 0.1 * 6.336940765380859
Epoch 670, val loss: 1.0033060312271118
Epoch 680, training loss: 0.6443322896957397 = 0.012009541504085064 + 0.1 * 6.3232269287109375
Epoch 680, val loss: 1.0102907419204712
Epoch 690, training loss: 0.6442796587944031 = 0.011546310968697071 + 0.1 * 6.327333450317383
Epoch 690, val loss: 1.0171834230422974
Epoch 700, training loss: 0.6444191932678223 = 0.011111326515674591 + 0.1 * 6.333078861236572
Epoch 700, val loss: 1.0237555503845215
Epoch 710, training loss: 0.6422427892684937 = 0.010704043321311474 + 0.1 * 6.31538724899292
Epoch 710, val loss: 1.0303575992584229
Epoch 720, training loss: 0.6423395276069641 = 0.010320330038666725 + 0.1 * 6.320192337036133
Epoch 720, val loss: 1.0367602109909058
Epoch 730, training loss: 0.6407243013381958 = 0.009959050454199314 + 0.1 * 6.307652473449707
Epoch 730, val loss: 1.043128490447998
Epoch 740, training loss: 0.6412724256515503 = 0.009617479518055916 + 0.1 * 6.316549301147461
Epoch 740, val loss: 1.0492160320281982
Epoch 750, training loss: 0.6404725909233093 = 0.009295860305428505 + 0.1 * 6.311767101287842
Epoch 750, val loss: 1.0553085803985596
Epoch 760, training loss: 0.6409873962402344 = 0.008990760892629623 + 0.1 * 6.3199663162231445
Epoch 760, val loss: 1.0612589120864868
Epoch 770, training loss: 0.6380253434181213 = 0.008702866733074188 + 0.1 * 6.293224811553955
Epoch 770, val loss: 1.0670173168182373
Epoch 780, training loss: 0.6396210193634033 = 0.008429848589003086 + 0.1 * 6.3119120597839355
Epoch 780, val loss: 1.0728763341903687
Epoch 790, training loss: 0.6373330354690552 = 0.008170721121132374 + 0.1 * 6.291623115539551
Epoch 790, val loss: 1.0783288478851318
Epoch 800, training loss: 0.636198878288269 = 0.007925333455204964 + 0.1 * 6.282735347747803
Epoch 800, val loss: 1.0839155912399292
Epoch 810, training loss: 0.6360279321670532 = 0.007691083010286093 + 0.1 * 6.2833685874938965
Epoch 810, val loss: 1.0892740488052368
Epoch 820, training loss: 0.6355909109115601 = 0.007469519507139921 + 0.1 * 6.281213760375977
Epoch 820, val loss: 1.0945061445236206
Epoch 830, training loss: 0.6354241371154785 = 0.0072577293030917645 + 0.1 * 6.2816643714904785
Epoch 830, val loss: 1.0998139381408691
Epoch 840, training loss: 0.6358441710472107 = 0.007056022994220257 + 0.1 * 6.287881851196289
Epoch 840, val loss: 1.1048237085342407
Epoch 850, training loss: 0.6333538293838501 = 0.006863987073302269 + 0.1 * 6.26489782333374
Epoch 850, val loss: 1.1098310947418213
Epoch 860, training loss: 0.6343146562576294 = 0.0066805495880544186 + 0.1 * 6.276340961456299
Epoch 860, val loss: 1.11493718624115
Epoch 870, training loss: 0.6326941847801208 = 0.0065050809644162655 + 0.1 * 6.261890888214111
Epoch 870, val loss: 1.1196345090866089
Epoch 880, training loss: 0.6333242058753967 = 0.006337583065032959 + 0.1 * 6.269865989685059
Epoch 880, val loss: 1.1245299577713013
Epoch 890, training loss: 0.6319263577461243 = 0.006176937837153673 + 0.1 * 6.25749397277832
Epoch 890, val loss: 1.129172682762146
Epoch 900, training loss: 0.6316923499107361 = 0.006023443769663572 + 0.1 * 6.256688594818115
Epoch 900, val loss: 1.133840799331665
Epoch 910, training loss: 0.6343013048171997 = 0.005876017268747091 + 0.1 * 6.284252643585205
Epoch 910, val loss: 1.138392448425293
Epoch 920, training loss: 0.631074845790863 = 0.00573465833440423 + 0.1 * 6.253401279449463
Epoch 920, val loss: 1.1428791284561157
Epoch 930, training loss: 0.6315422654151917 = 0.005599351599812508 + 0.1 * 6.25942850112915
Epoch 930, val loss: 1.1473917961120605
Epoch 940, training loss: 0.6306189894676208 = 0.005469219759106636 + 0.1 * 6.251497745513916
Epoch 940, val loss: 1.1516886949539185
Epoch 950, training loss: 0.6290616989135742 = 0.005344487726688385 + 0.1 * 6.237171649932861
Epoch 950, val loss: 1.1559782028198242
Epoch 960, training loss: 0.6303718686103821 = 0.005224079824984074 + 0.1 * 6.2514777183532715
Epoch 960, val loss: 1.1602678298950195
Epoch 970, training loss: 0.6282004714012146 = 0.005108233541250229 + 0.1 * 6.230922222137451
Epoch 970, val loss: 1.1643551588058472
Epoch 980, training loss: 0.6290922164916992 = 0.004996898118406534 + 0.1 * 6.240952968597412
Epoch 980, val loss: 1.1684752702713013
Epoch 990, training loss: 0.6276564002037048 = 0.004889884497970343 + 0.1 * 6.227664947509766
Epoch 990, val loss: 1.1725808382034302
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5646
Flip ASR: 0.4756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7851881980895996 = 1.94779372215271 + 0.1 * 8.373943328857422
Epoch 0, val loss: 1.9498528242111206
Epoch 10, training loss: 2.7759766578674316 = 1.9385861158370972 + 0.1 * 8.373906135559082
Epoch 10, val loss: 1.9406225681304932
Epoch 20, training loss: 2.765134334564209 = 1.9277640581130981 + 0.1 * 8.373703956604004
Epoch 20, val loss: 1.929179310798645
Epoch 30, training loss: 2.750192642211914 = 1.9129544496536255 + 0.1 * 8.372382164001465
Epoch 30, val loss: 1.9131449460983276
Epoch 40, training loss: 2.727283000946045 = 1.891195297241211 + 0.1 * 8.360877990722656
Epoch 40, val loss: 1.8895061016082764
Epoch 50, training loss: 2.688469886779785 = 1.8592709302902222 + 0.1 * 8.291990280151367
Epoch 50, val loss: 1.8556870222091675
Epoch 60, training loss: 2.6202754974365234 = 1.8186675310134888 + 0.1 * 8.01607894897461
Epoch 60, val loss: 1.8156731128692627
Epoch 70, training loss: 2.5546483993530273 = 1.7756140232086182 + 0.1 * 7.790343761444092
Epoch 70, val loss: 1.7760539054870605
Epoch 80, training loss: 2.4685568809509277 = 1.7309669256210327 + 0.1 * 7.375899314880371
Epoch 80, val loss: 1.7359111309051514
Epoch 90, training loss: 2.3845953941345215 = 1.676587462425232 + 0.1 * 7.080079555511475
Epoch 90, val loss: 1.6882226467132568
Epoch 100, training loss: 2.3001062870025635 = 1.6064722537994385 + 0.1 * 6.936339855194092
Epoch 100, val loss: 1.6281538009643555
Epoch 110, training loss: 2.2019338607788086 = 1.5188820362091064 + 0.1 * 6.83051872253418
Epoch 110, val loss: 1.553168535232544
Epoch 120, training loss: 2.0963542461395264 = 1.4190090894699097 + 0.1 * 6.77345085144043
Epoch 120, val loss: 1.4703036546707153
Epoch 130, training loss: 1.989556074142456 = 1.3154546022415161 + 0.1 * 6.741014003753662
Epoch 130, val loss: 1.3901851177215576
Epoch 140, training loss: 1.888852596282959 = 1.2171900272369385 + 0.1 * 6.716624736785889
Epoch 140, val loss: 1.3185762166976929
Epoch 150, training loss: 1.7964365482330322 = 1.1266772747039795 + 0.1 * 6.697591781616211
Epoch 150, val loss: 1.2546197175979614
Epoch 160, training loss: 1.7119781970977783 = 1.044344425201416 + 0.1 * 6.676338195800781
Epoch 160, val loss: 1.1968941688537598
Epoch 170, training loss: 1.6350531578063965 = 0.9692189693450928 + 0.1 * 6.658341884613037
Epoch 170, val loss: 1.143619418144226
Epoch 180, training loss: 1.5655722618103027 = 0.90086829662323 + 0.1 * 6.64703893661499
Epoch 180, val loss: 1.09478759765625
Epoch 190, training loss: 1.5003368854522705 = 0.8374158143997192 + 0.1 * 6.62921142578125
Epoch 190, val loss: 1.0494545698165894
Epoch 200, training loss: 1.4381561279296875 = 0.7766590118408203 + 0.1 * 6.61497163772583
Epoch 200, val loss: 1.0062005519866943
Epoch 210, training loss: 1.3784703016281128 = 0.7183007597923279 + 0.1 * 6.601695537567139
Epoch 210, val loss: 0.9652137160301208
Epoch 220, training loss: 1.3211023807525635 = 0.6621793508529663 + 0.1 * 6.589230060577393
Epoch 220, val loss: 0.927117109298706
Epoch 230, training loss: 1.2669055461883545 = 0.6088082194328308 + 0.1 * 6.5809736251831055
Epoch 230, val loss: 0.8929634690284729
Epoch 240, training loss: 1.2151143550872803 = 0.5584420561790466 + 0.1 * 6.566723346710205
Epoch 240, val loss: 0.8632784485816956
Epoch 250, training loss: 1.1668906211853027 = 0.5109502077102661 + 0.1 * 6.559403896331787
Epoch 250, val loss: 0.8378435373306274
Epoch 260, training loss: 1.121537685394287 = 0.4664146304130554 + 0.1 * 6.5512309074401855
Epoch 260, val loss: 0.81634122133255
Epoch 270, training loss: 1.0788265466690063 = 0.4245753288269043 + 0.1 * 6.542511940002441
Epoch 270, val loss: 0.7977163791656494
Epoch 280, training loss: 1.041062593460083 = 0.3855428993701935 + 0.1 * 6.555197238922119
Epoch 280, val loss: 0.7815956473350525
Epoch 290, training loss: 1.002687931060791 = 0.3497060239315033 + 0.1 * 6.529818534851074
Epoch 290, val loss: 0.7679434418678284
Epoch 300, training loss: 0.9688158631324768 = 0.31657570600509644 + 0.1 * 6.522401332855225
Epoch 300, val loss: 0.7568216323852539
Epoch 310, training loss: 0.9387228488922119 = 0.28595542907714844 + 0.1 * 6.527674198150635
Epoch 310, val loss: 0.7482151985168457
Epoch 320, training loss: 0.9087004661560059 = 0.2579632103443146 + 0.1 * 6.5073723793029785
Epoch 320, val loss: 0.7422013282775879
Epoch 330, training loss: 0.8836329579353333 = 0.232401505112648 + 0.1 * 6.512314796447754
Epoch 330, val loss: 0.7383902668952942
Epoch 340, training loss: 0.8586350679397583 = 0.2092454433441162 + 0.1 * 6.493896007537842
Epoch 340, val loss: 0.7365731000900269
Epoch 350, training loss: 0.8379629254341125 = 0.18821458518505096 + 0.1 * 6.497483253479004
Epoch 350, val loss: 0.7363221049308777
Epoch 360, training loss: 0.8177827596664429 = 0.16927048563957214 + 0.1 * 6.4851226806640625
Epoch 360, val loss: 0.7375867962837219
Epoch 370, training loss: 0.7994919419288635 = 0.1522306352853775 + 0.1 * 6.4726128578186035
Epoch 370, val loss: 0.7400301694869995
Epoch 380, training loss: 0.7842139005661011 = 0.13692167401313782 + 0.1 * 6.472921848297119
Epoch 380, val loss: 0.7433806657791138
Epoch 390, training loss: 0.7694197297096252 = 0.12326953560113907 + 0.1 * 6.461501598358154
Epoch 390, val loss: 0.7475533485412598
Epoch 400, training loss: 0.7557123899459839 = 0.11107519268989563 + 0.1 * 6.446371555328369
Epoch 400, val loss: 0.7523576617240906
Epoch 410, training loss: 0.7456955313682556 = 0.10025303810834885 + 0.1 * 6.454424858093262
Epoch 410, val loss: 0.7576997876167297
Epoch 420, training loss: 0.7348697781562805 = 0.09066766500473022 + 0.1 * 6.442020893096924
Epoch 420, val loss: 0.7635323405265808
Epoch 430, training loss: 0.7249595522880554 = 0.08216279745101929 + 0.1 * 6.427967548370361
Epoch 430, val loss: 0.7695887088775635
Epoch 440, training loss: 0.7171539664268494 = 0.07460487633943558 + 0.1 * 6.425490856170654
Epoch 440, val loss: 0.7760394811630249
Epoch 450, training loss: 0.7094413042068481 = 0.06789528578519821 + 0.1 * 6.415460109710693
Epoch 450, val loss: 0.7827800512313843
Epoch 460, training loss: 0.7029090523719788 = 0.06185609847307205 + 0.1 * 6.410529613494873
Epoch 460, val loss: 0.7895547747612
Epoch 470, training loss: 0.6964435577392578 = 0.05639754980802536 + 0.1 * 6.4004597663879395
Epoch 470, val loss: 0.7965916395187378
Epoch 480, training loss: 0.6908405423164368 = 0.051429808139801025 + 0.1 * 6.394107341766357
Epoch 480, val loss: 0.8036415576934814
Epoch 490, training loss: 0.6865472793579102 = 0.04689823463559151 + 0.1 * 6.396490097045898
Epoch 490, val loss: 0.8107067346572876
Epoch 500, training loss: 0.6799870729446411 = 0.04270017519593239 + 0.1 * 6.37286901473999
Epoch 500, val loss: 0.8179576992988586
Epoch 510, training loss: 0.675721287727356 = 0.03892214968800545 + 0.1 * 6.3679914474487305
Epoch 510, val loss: 0.8249789476394653
Epoch 520, training loss: 0.6738281846046448 = 0.035597171634435654 + 0.1 * 6.382310390472412
Epoch 520, val loss: 0.8322874903678894
Epoch 530, training loss: 0.6689985394477844 = 0.03272204473614693 + 0.1 * 6.362764835357666
Epoch 530, val loss: 0.8398426175117493
Epoch 540, training loss: 0.6671443581581116 = 0.030222641304135323 + 0.1 * 6.3692169189453125
Epoch 540, val loss: 0.8475090861320496
Epoch 550, training loss: 0.6639352440834045 = 0.028046241030097008 + 0.1 * 6.358890056610107
Epoch 550, val loss: 0.8550228476524353
Epoch 560, training loss: 0.6608332991600037 = 0.026132594794034958 + 0.1 * 6.3470072746276855
Epoch 560, val loss: 0.8627066612243652
Epoch 570, training loss: 0.6594109535217285 = 0.0244334414601326 + 0.1 * 6.3497748374938965
Epoch 570, val loss: 0.8700924515724182
Epoch 580, training loss: 0.655962347984314 = 0.022914323955774307 + 0.1 * 6.330480098724365
Epoch 580, val loss: 0.8775247931480408
Epoch 590, training loss: 0.6543020009994507 = 0.021546633914113045 + 0.1 * 6.3275532722473145
Epoch 590, val loss: 0.884844958782196
Epoch 600, training loss: 0.652858555316925 = 0.0203107763081789 + 0.1 * 6.3254780769348145
Epoch 600, val loss: 0.8918909430503845
Epoch 610, training loss: 0.6511760354042053 = 0.019192568957805634 + 0.1 * 6.319834232330322
Epoch 610, val loss: 0.8988764882087708
Epoch 620, training loss: 0.6494406461715698 = 0.01817324385046959 + 0.1 * 6.312674045562744
Epoch 620, val loss: 0.9057319760322571
Epoch 630, training loss: 0.6488166451454163 = 0.017238276079297066 + 0.1 * 6.315783977508545
Epoch 630, val loss: 0.9122689366340637
Epoch 640, training loss: 0.6469611525535583 = 0.01638125069439411 + 0.1 * 6.305798530578613
Epoch 640, val loss: 0.9188370704650879
Epoch 650, training loss: 0.6474474668502808 = 0.015592585317790508 + 0.1 * 6.318548679351807
Epoch 650, val loss: 0.9251832365989685
Epoch 660, training loss: 0.6457595825195312 = 0.014864970929920673 + 0.1 * 6.308945655822754
Epoch 660, val loss: 0.9313225150108337
Epoch 670, training loss: 0.6440073251724243 = 0.01419154368340969 + 0.1 * 6.298157215118408
Epoch 670, val loss: 0.937380850315094
Epoch 680, training loss: 0.6430960893630981 = 0.013566178269684315 + 0.1 * 6.2952985763549805
Epoch 680, val loss: 0.9433026909828186
Epoch 690, training loss: 0.6412471532821655 = 0.012985180132091045 + 0.1 * 6.282619953155518
Epoch 690, val loss: 0.949030876159668
Epoch 700, training loss: 0.6405461430549622 = 0.01244481559842825 + 0.1 * 6.281013488769531
Epoch 700, val loss: 0.9547087550163269
Epoch 710, training loss: 0.6421996355056763 = 0.011939514428377151 + 0.1 * 6.302600860595703
Epoch 710, val loss: 0.9601901769638062
Epoch 720, training loss: 0.6392954587936401 = 0.011468669399619102 + 0.1 * 6.278267860412598
Epoch 720, val loss: 0.9655444025993347
Epoch 730, training loss: 0.6391159892082214 = 0.011027657426893711 + 0.1 * 6.280883312225342
Epoch 730, val loss: 0.9708319902420044
Epoch 740, training loss: 0.6382784843444824 = 0.010613876394927502 + 0.1 * 6.276646137237549
Epoch 740, val loss: 0.9760092496871948
Epoch 750, training loss: 0.6371011734008789 = 0.010224767960608006 + 0.1 * 6.268764019012451
Epoch 750, val loss: 0.980941891670227
Epoch 760, training loss: 0.6357905268669128 = 0.009859707206487656 + 0.1 * 6.259308338165283
Epoch 760, val loss: 0.9859300255775452
Epoch 770, training loss: 0.6372720003128052 = 0.009515116922557354 + 0.1 * 6.277568817138672
Epoch 770, val loss: 0.990765392780304
Epoch 780, training loss: 0.6350077986717224 = 0.009190073236823082 + 0.1 * 6.258177280426025
Epoch 780, val loss: 0.9954135417938232
Epoch 790, training loss: 0.6355947256088257 = 0.008883563801646233 + 0.1 * 6.267111778259277
Epoch 790, val loss: 1.0000742673873901
Epoch 800, training loss: 0.6339445114135742 = 0.008594128303229809 + 0.1 * 6.253503322601318
Epoch 800, val loss: 1.00454580783844
Epoch 810, training loss: 0.633363664150238 = 0.008319946005940437 + 0.1 * 6.250436782836914
Epoch 810, val loss: 1.0090407133102417
Epoch 820, training loss: 0.6354565024375916 = 0.00805951189249754 + 0.1 * 6.273970127105713
Epoch 820, val loss: 1.0133589506149292
Epoch 830, training loss: 0.6330377459526062 = 0.00781260710209608 + 0.1 * 6.252251148223877
Epoch 830, val loss: 1.0175648927688599
Epoch 840, training loss: 0.6329770684242249 = 0.007578949443995953 + 0.1 * 6.25398063659668
Epoch 840, val loss: 1.0217993259429932
Epoch 850, training loss: 0.6316052079200745 = 0.007356463931500912 + 0.1 * 6.24248743057251
Epoch 850, val loss: 1.025909185409546
Epoch 860, training loss: 0.632175624370575 = 0.007144700735807419 + 0.1 * 6.250309467315674
Epoch 860, val loss: 1.0299285650253296
Epoch 870, training loss: 0.6309295892715454 = 0.006942817475646734 + 0.1 * 6.239867687225342
Epoch 870, val loss: 1.0338623523712158
Epoch 880, training loss: 0.6302964091300964 = 0.006750660948455334 + 0.1 * 6.235457420349121
Epoch 880, val loss: 1.0377905368804932
Epoch 890, training loss: 0.6299983859062195 = 0.006567215081304312 + 0.1 * 6.234311580657959
Epoch 890, val loss: 1.041617751121521
Epoch 900, training loss: 0.6298131346702576 = 0.006391976028680801 + 0.1 * 6.234210968017578
Epoch 900, val loss: 1.045337200164795
Epoch 910, training loss: 0.6290652751922607 = 0.006224480923265219 + 0.1 * 6.228408336639404
Epoch 910, val loss: 1.0490652322769165
Epoch 920, training loss: 0.6303045153617859 = 0.006064167711883783 + 0.1 * 6.242403507232666
Epoch 920, val loss: 1.052631139755249
Epoch 930, training loss: 0.6280903220176697 = 0.005911362823098898 + 0.1 * 6.221789360046387
Epoch 930, val loss: 1.0561866760253906
Epoch 940, training loss: 0.6293867230415344 = 0.005765259265899658 + 0.1 * 6.236214637756348
Epoch 940, val loss: 1.0597392320632935
Epoch 950, training loss: 0.6291345357894897 = 0.005624932236969471 + 0.1 * 6.235095977783203
Epoch 950, val loss: 1.0631279945373535
Epoch 960, training loss: 0.6270250678062439 = 0.005490697454661131 + 0.1 * 6.215343475341797
Epoch 960, val loss: 1.066506266593933
Epoch 970, training loss: 0.6279310584068298 = 0.005361608695238829 + 0.1 * 6.225694179534912
Epoch 970, val loss: 1.0698732137680054
Epoch 980, training loss: 0.6277044415473938 = 0.0052372757345438 + 0.1 * 6.224671363830566
Epoch 980, val loss: 1.0731337070465088
Epoch 990, training loss: 0.6273306012153625 = 0.005118191707879305 + 0.1 * 6.222124099731445
Epoch 990, val loss: 1.0763823986053467
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.782418727874756 = 1.9450267553329468 + 0.1 * 8.373918533325195
Epoch 0, val loss: 1.9493862390518188
Epoch 10, training loss: 2.77233624458313 = 1.9349539279937744 + 0.1 * 8.373822212219238
Epoch 10, val loss: 1.9393380880355835
Epoch 20, training loss: 2.7598958015441895 = 1.922566294670105 + 0.1 * 8.37329387664795
Epoch 20, val loss: 1.9267964363098145
Epoch 30, training loss: 2.74223256111145 = 1.9053454399108887 + 0.1 * 8.368870735168457
Epoch 30, val loss: 1.9091577529907227
Epoch 40, training loss: 2.713759422302246 = 1.8800578117370605 + 0.1 * 8.337015151977539
Epoch 40, val loss: 1.8834378719329834
Epoch 50, training loss: 2.6604132652282715 = 1.8451452255249023 + 0.1 * 8.152680397033691
Epoch 50, val loss: 1.8494638204574585
Epoch 60, training loss: 2.5811984539031982 = 1.8049532175064087 + 0.1 * 7.762452125549316
Epoch 60, val loss: 1.8125869035720825
Epoch 70, training loss: 2.506420135498047 = 1.766531229019165 + 0.1 * 7.398889064788818
Epoch 70, val loss: 1.7787516117095947
Epoch 80, training loss: 2.440216064453125 = 1.7266631126403809 + 0.1 * 7.135528087615967
Epoch 80, val loss: 1.7431011199951172
Epoch 90, training loss: 2.372354030609131 = 1.676135778427124 + 0.1 * 6.962183475494385
Epoch 90, val loss: 1.6980550289154053
Epoch 100, training loss: 2.294400215148926 = 1.6105830669403076 + 0.1 * 6.838172435760498
Epoch 100, val loss: 1.6417065858840942
Epoch 110, training loss: 2.2052078247070312 = 1.5289968252182007 + 0.1 * 6.76210880279541
Epoch 110, val loss: 1.5728644132614136
Epoch 120, training loss: 2.1087241172790527 = 1.4354228973388672 + 0.1 * 6.733013153076172
Epoch 120, val loss: 1.4929627180099487
Epoch 130, training loss: 2.0076286792755127 = 1.3357309103012085 + 0.1 * 6.718976974487305
Epoch 130, val loss: 1.408779501914978
Epoch 140, training loss: 1.9026167392730713 = 1.2315701246261597 + 0.1 * 6.710465908050537
Epoch 140, val loss: 1.3213616609573364
Epoch 150, training loss: 1.793074607849121 = 1.1226593255996704 + 0.1 * 6.7041521072387695
Epoch 150, val loss: 1.2307665348052979
Epoch 160, training loss: 1.6806224584579468 = 1.0108197927474976 + 0.1 * 6.698026657104492
Epoch 160, val loss: 1.1386573314666748
Epoch 170, training loss: 1.569840908050537 = 0.9007081985473633 + 0.1 * 6.69132661819458
Epoch 170, val loss: 1.0491923093795776
Epoch 180, training loss: 1.4651143550872803 = 0.7966830730438232 + 0.1 * 6.6843132972717285
Epoch 180, val loss: 0.9674579501152039
Epoch 190, training loss: 1.3712295293807983 = 0.7034973502159119 + 0.1 * 6.677321434020996
Epoch 190, val loss: 0.8979929089546204
Epoch 200, training loss: 1.2887811660766602 = 0.6221297979354858 + 0.1 * 6.666513919830322
Epoch 200, val loss: 0.842054545879364
Epoch 210, training loss: 1.2172658443450928 = 0.5514717698097229 + 0.1 * 6.657939910888672
Epoch 210, val loss: 0.7977832555770874
Epoch 220, training loss: 1.1560368537902832 = 0.4900591969490051 + 0.1 * 6.659775733947754
Epoch 220, val loss: 0.7633083462715149
Epoch 230, training loss: 1.1012296676635742 = 0.43657660484313965 + 0.1 * 6.646529674530029
Epoch 230, val loss: 0.7366264462471008
Epoch 240, training loss: 1.052403211593628 = 0.3884260952472687 + 0.1 * 6.639770984649658
Epoch 240, val loss: 0.7149838209152222
Epoch 250, training loss: 1.007599115371704 = 0.34437277913093567 + 0.1 * 6.63226318359375
Epoch 250, val loss: 0.6973727345466614
Epoch 260, training loss: 0.9665367603302002 = 0.30393388867378235 + 0.1 * 6.626028060913086
Epoch 260, val loss: 0.6832911968231201
Epoch 270, training loss: 0.9299067258834839 = 0.2671463191509247 + 0.1 * 6.6276044845581055
Epoch 270, val loss: 0.6726099848747253
Epoch 280, training loss: 0.8958407640457153 = 0.23451434075832367 + 0.1 * 6.613264083862305
Epoch 280, val loss: 0.6652458906173706
Epoch 290, training loss: 0.8667129278182983 = 0.20587971806526184 + 0.1 * 6.60833215713501
Epoch 290, val loss: 0.6609316468238831
Epoch 300, training loss: 0.8411739468574524 = 0.18106609582901 + 0.1 * 6.601078510284424
Epoch 300, val loss: 0.6594343781471252
Epoch 310, training loss: 0.8194262981414795 = 0.1599227786064148 + 0.1 * 6.595035076141357
Epoch 310, val loss: 0.6603407859802246
Epoch 320, training loss: 0.800757110118866 = 0.14201004803180695 + 0.1 * 6.587470531463623
Epoch 320, val loss: 0.6635215878486633
Epoch 330, training loss: 0.7843594551086426 = 0.12670183181762695 + 0.1 * 6.576576232910156
Epoch 330, val loss: 0.6685842871665955
Epoch 340, training loss: 0.7705221176147461 = 0.11359009146690369 + 0.1 * 6.5693206787109375
Epoch 340, val loss: 0.6750972270965576
Epoch 350, training loss: 0.758826494216919 = 0.1022980585694313 + 0.1 * 6.56528377532959
Epoch 350, val loss: 0.682682991027832
Epoch 360, training loss: 0.7479292154312134 = 0.0925087183713913 + 0.1 * 6.55420446395874
Epoch 360, val loss: 0.6910647749900818
Epoch 370, training loss: 0.7376804947853088 = 0.08397691696882248 + 0.1 * 6.5370354652404785
Epoch 370, val loss: 0.6999704241752625
Epoch 380, training loss: 0.7299926280975342 = 0.07649016380310059 + 0.1 * 6.535024642944336
Epoch 380, val loss: 0.7092916369438171
Epoch 390, training loss: 0.7217127084732056 = 0.06990844756364822 + 0.1 * 6.51804256439209
Epoch 390, val loss: 0.7187850475311279
Epoch 400, training loss: 0.7149159908294678 = 0.06406117230653763 + 0.1 * 6.508548259735107
Epoch 400, val loss: 0.7285342216491699
Epoch 410, training loss: 0.7101994156837463 = 0.05885959789156914 + 0.1 * 6.513398170471191
Epoch 410, val loss: 0.7381848096847534
Epoch 420, training loss: 0.703387439250946 = 0.054225124418735504 + 0.1 * 6.4916229248046875
Epoch 420, val loss: 0.747939944267273
Epoch 430, training loss: 0.6985160112380981 = 0.050062697380781174 + 0.1 * 6.484532833099365
Epoch 430, val loss: 0.7577523589134216
Epoch 440, training loss: 0.6956359148025513 = 0.046325672417879105 + 0.1 * 6.493102073669434
Epoch 440, val loss: 0.7673197984695435
Epoch 450, training loss: 0.6903274655342102 = 0.04297136142849922 + 0.1 * 6.473560810089111
Epoch 450, val loss: 0.7768386602401733
Epoch 460, training loss: 0.6886868476867676 = 0.039938732981681824 + 0.1 * 6.487480640411377
Epoch 460, val loss: 0.7863022089004517
Epoch 470, training loss: 0.6826730370521545 = 0.03720128908753395 + 0.1 * 6.454717636108398
Epoch 470, val loss: 0.7956414818763733
Epoch 480, training loss: 0.6791621446609497 = 0.034720949828624725 + 0.1 * 6.444411754608154
Epoch 480, val loss: 0.8048471808433533
Epoch 490, training loss: 0.6769329309463501 = 0.03246390447020531 + 0.1 * 6.444690227508545
Epoch 490, val loss: 0.8139145374298096
Epoch 500, training loss: 0.6742337942123413 = 0.03041725978255272 + 0.1 * 6.438165187835693
Epoch 500, val loss: 0.8228768706321716
Epoch 510, training loss: 0.6720752716064453 = 0.028553813695907593 + 0.1 * 6.435214996337891
Epoch 510, val loss: 0.8316360116004944
Epoch 520, training loss: 0.6689655184745789 = 0.02685568854212761 + 0.1 * 6.421097755432129
Epoch 520, val loss: 0.8402020335197449
Epoch 530, training loss: 0.6654694676399231 = 0.02530323900282383 + 0.1 * 6.4016618728637695
Epoch 530, val loss: 0.8486369848251343
Epoch 540, training loss: 0.6632711291313171 = 0.023882606998085976 + 0.1 * 6.393884658813477
Epoch 540, val loss: 0.8568755388259888
Epoch 550, training loss: 0.6614791750907898 = 0.022579098120331764 + 0.1 * 6.38900089263916
Epoch 550, val loss: 0.8649789690971375
Epoch 560, training loss: 0.6621794104576111 = 0.021380245685577393 + 0.1 * 6.407991409301758
Epoch 560, val loss: 0.8728229403495789
Epoch 570, training loss: 0.6578001379966736 = 0.020283617079257965 + 0.1 * 6.375164985656738
Epoch 570, val loss: 0.8804581165313721
Epoch 580, training loss: 0.655929684638977 = 0.01927068457007408 + 0.1 * 6.3665900230407715
Epoch 580, val loss: 0.8879493474960327
Epoch 590, training loss: 0.6559215784072876 = 0.018329299986362457 + 0.1 * 6.375922679901123
Epoch 590, val loss: 0.8953197598457336
Epoch 600, training loss: 0.6529170274734497 = 0.017456719651818275 + 0.1 * 6.354602813720703
Epoch 600, val loss: 0.9025430083274841
Epoch 610, training loss: 0.6533367037773132 = 0.01664574071764946 + 0.1 * 6.366909503936768
Epoch 610, val loss: 0.9096086621284485
Epoch 620, training loss: 0.6501341462135315 = 0.015892671421170235 + 0.1 * 6.342414379119873
Epoch 620, val loss: 0.916388213634491
Epoch 630, training loss: 0.6485424637794495 = 0.015192514285445213 + 0.1 * 6.333499431610107
Epoch 630, val loss: 0.9231093525886536
Epoch 640, training loss: 0.648780345916748 = 0.014538537710905075 + 0.1 * 6.342418193817139
Epoch 640, val loss: 0.9296287894248962
Epoch 650, training loss: 0.6463199257850647 = 0.013929655775427818 + 0.1 * 6.323902606964111
Epoch 650, val loss: 0.9360087513923645
Epoch 660, training loss: 0.646018385887146 = 0.013359281234443188 + 0.1 * 6.326590538024902
Epoch 660, val loss: 0.942306637763977
Epoch 670, training loss: 0.6471837162971497 = 0.012823139317333698 + 0.1 * 6.3436055183410645
Epoch 670, val loss: 0.9484564661979675
Epoch 680, training loss: 0.6438709497451782 = 0.012321184389293194 + 0.1 * 6.315497875213623
Epoch 680, val loss: 0.9544487595558167
Epoch 690, training loss: 0.6425825357437134 = 0.011850113049149513 + 0.1 * 6.307323932647705
Epoch 690, val loss: 0.9603753089904785
Epoch 700, training loss: 0.6428287029266357 = 0.011405975557863712 + 0.1 * 6.3142266273498535
Epoch 700, val loss: 0.9661539793014526
Epoch 710, training loss: 0.6408640146255493 = 0.010989279486238956 + 0.1 * 6.2987470626831055
Epoch 710, val loss: 0.9718079566955566
Epoch 720, training loss: 0.6428641080856323 = 0.01059612538665533 + 0.1 * 6.3226799964904785
Epoch 720, val loss: 0.9774046540260315
Epoch 730, training loss: 0.6406901478767395 = 0.010225321166217327 + 0.1 * 6.304648399353027
Epoch 730, val loss: 0.9827530384063721
Epoch 740, training loss: 0.639448344707489 = 0.009875253774225712 + 0.1 * 6.295731067657471
Epoch 740, val loss: 0.9881520867347717
Epoch 750, training loss: 0.6392841935157776 = 0.009542680345475674 + 0.1 * 6.297414779663086
Epoch 750, val loss: 0.993392288684845
Epoch 760, training loss: 0.6396692991256714 = 0.009227998554706573 + 0.1 * 6.304412841796875
Epoch 760, val loss: 0.9985522031784058
Epoch 770, training loss: 0.6381650567054749 = 0.008930539712309837 + 0.1 * 6.292344570159912
Epoch 770, val loss: 1.0035892724990845
Epoch 780, training loss: 0.6365221738815308 = 0.008649053052067757 + 0.1 * 6.278730869293213
Epoch 780, val loss: 1.0085391998291016
Epoch 790, training loss: 0.6376144886016846 = 0.008380965329706669 + 0.1 * 6.292335033416748
Epoch 790, val loss: 1.013397455215454
Epoch 800, training loss: 0.635909914970398 = 0.008126583881676197 + 0.1 * 6.277833461761475
Epoch 800, val loss: 1.0182145833969116
Epoch 810, training loss: 0.6347172260284424 = 0.007884694263339043 + 0.1 * 6.268325328826904
Epoch 810, val loss: 1.0228878259658813
Epoch 820, training loss: 0.6349071264266968 = 0.007654339540749788 + 0.1 * 6.27252721786499
Epoch 820, val loss: 1.0275447368621826
Epoch 830, training loss: 0.6337374448776245 = 0.007434932515025139 + 0.1 * 6.263025283813477
Epoch 830, val loss: 1.0321426391601562
Epoch 840, training loss: 0.6326445937156677 = 0.0072260694578289986 + 0.1 * 6.254185199737549
Epoch 840, val loss: 1.0366286039352417
Epoch 850, training loss: 0.6356814503669739 = 0.00702616898342967 + 0.1 * 6.286552429199219
Epoch 850, val loss: 1.0410817861557007
Epoch 860, training loss: 0.6329829692840576 = 0.006835749838501215 + 0.1 * 6.261471748352051
Epoch 860, val loss: 1.0453966856002808
Epoch 870, training loss: 0.6327733993530273 = 0.006653983145952225 + 0.1 * 6.261194229125977
Epoch 870, val loss: 1.0496749877929688
Epoch 880, training loss: 0.6325432658195496 = 0.006479798350483179 + 0.1 * 6.260634422302246
Epoch 880, val loss: 1.0539129972457886
Epoch 890, training loss: 0.6315599679946899 = 0.006313695106655359 + 0.1 * 6.252462863922119
Epoch 890, val loss: 1.058121919631958
Epoch 900, training loss: 0.6317171454429626 = 0.006153918337076902 + 0.1 * 6.255631923675537
Epoch 900, val loss: 1.0622665882110596
Epoch 910, training loss: 0.6320305466651917 = 0.0060006664134562016 + 0.1 * 6.260299205780029
Epoch 910, val loss: 1.066391944885254
Epoch 920, training loss: 0.6301107406616211 = 0.005854160990566015 + 0.1 * 6.242565155029297
Epoch 920, val loss: 1.0702909231185913
Epoch 930, training loss: 0.6299939155578613 = 0.005714129190891981 + 0.1 * 6.2427978515625
Epoch 930, val loss: 1.0742698907852173
Epoch 940, training loss: 0.6306655406951904 = 0.005579262971878052 + 0.1 * 6.250862121582031
Epoch 940, val loss: 1.0782067775726318
Epoch 950, training loss: 0.628937304019928 = 0.005449424032121897 + 0.1 * 6.2348785400390625
Epoch 950, val loss: 1.0819867849349976
Epoch 960, training loss: 0.6291005611419678 = 0.005325064994394779 + 0.1 * 6.237754821777344
Epoch 960, val loss: 1.0857713222503662
Epoch 970, training loss: 0.6283912658691406 = 0.00520567549392581 + 0.1 * 6.231855869293213
Epoch 970, val loss: 1.089520812034607
Epoch 980, training loss: 0.6302921175956726 = 0.005090457387268543 + 0.1 * 6.252016067504883
Epoch 980, val loss: 1.0932673215866089
Epoch 990, training loss: 0.627952516078949 = 0.004979543853551149 + 0.1 * 6.229729652404785
Epoch 990, val loss: 1.096838355064392
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9262
Flip ASR: 0.9111/225 nodes
The final ASR:0.82780, 0.18809, Accuracy:0.81852, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10618])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00758, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.788607120513916 = 1.9512301683425903 + 0.1 * 8.37376880645752
Epoch 0, val loss: 1.9539439678192139
Epoch 10, training loss: 2.778343677520752 = 1.940997838973999 + 0.1 * 8.373456954956055
Epoch 10, val loss: 1.9443689584732056
Epoch 20, training loss: 2.765010356903076 = 1.9278367757797241 + 0.1 * 8.371735572814941
Epoch 20, val loss: 1.9314141273498535
Epoch 30, training loss: 2.7449259757995605 = 1.9089527130126953 + 0.1 * 8.359732627868652
Epoch 30, val loss: 1.9123191833496094
Epoch 40, training loss: 2.7084434032440186 = 1.8809078931808472 + 0.1 * 8.275355339050293
Epoch 40, val loss: 1.884013056755066
Epoch 50, training loss: 2.6184606552124023 = 1.8452987670898438 + 0.1 * 7.7316179275512695
Epoch 50, val loss: 1.8498084545135498
Epoch 60, training loss: 2.536379098892212 = 1.8120572566986084 + 0.1 * 7.243217468261719
Epoch 60, val loss: 1.8196165561676025
Epoch 70, training loss: 2.471238613128662 = 1.7797235250473022 + 0.1 * 6.915150165557861
Epoch 70, val loss: 1.7917531728744507
Epoch 80, training loss: 2.423194169998169 = 1.745888352394104 + 0.1 * 6.773058891296387
Epoch 80, val loss: 1.763265609741211
Epoch 90, training loss: 2.3728890419006348 = 1.7026170492172241 + 0.1 * 6.702721118927002
Epoch 90, val loss: 1.7264387607574463
Epoch 100, training loss: 2.310206413269043 = 1.645139217376709 + 0.1 * 6.650670528411865
Epoch 100, val loss: 1.6779558658599854
Epoch 110, training loss: 2.2315287590026855 = 1.5705267190933228 + 0.1 * 6.610019207000732
Epoch 110, val loss: 1.615921974182129
Epoch 120, training loss: 2.1387383937835693 = 1.4806383848190308 + 0.1 * 6.581000804901123
Epoch 120, val loss: 1.5419142246246338
Epoch 130, training loss: 2.0389809608459473 = 1.3831454515457153 + 0.1 * 6.558355808258057
Epoch 130, val loss: 1.463696837425232
Epoch 140, training loss: 1.937546730041504 = 1.2833733558654785 + 0.1 * 6.5417327880859375
Epoch 140, val loss: 1.3854585886001587
Epoch 150, training loss: 1.8368014097213745 = 1.1838617324829102 + 0.1 * 6.5293965339660645
Epoch 150, val loss: 1.3089017868041992
Epoch 160, training loss: 1.739757776260376 = 1.0880409479141235 + 0.1 * 6.517168045043945
Epoch 160, val loss: 1.2357635498046875
Epoch 170, training loss: 1.6498504877090454 = 0.9994800686836243 + 0.1 * 6.503704071044922
Epoch 170, val loss: 1.1692109107971191
Epoch 180, training loss: 1.5686379671096802 = 0.9197154641151428 + 0.1 * 6.489224910736084
Epoch 180, val loss: 1.1102410554885864
Epoch 190, training loss: 1.4958720207214355 = 0.8481065034866333 + 0.1 * 6.477655410766602
Epoch 190, val loss: 1.0574538707733154
Epoch 200, training loss: 1.4305405616760254 = 0.7841241955757141 + 0.1 * 6.464162826538086
Epoch 200, val loss: 1.0109331607818604
Epoch 210, training loss: 1.3718650341033936 = 0.726702094078064 + 0.1 * 6.451629638671875
Epoch 210, val loss: 0.969921886920929
Epoch 220, training loss: 1.3196914196014404 = 0.6751269698143005 + 0.1 * 6.445644855499268
Epoch 220, val loss: 0.9344235062599182
Epoch 230, training loss: 1.2712446451187134 = 0.6282842755317688 + 0.1 * 6.429603576660156
Epoch 230, val loss: 0.9038985371589661
Epoch 240, training loss: 1.2259674072265625 = 0.5843549966812134 + 0.1 * 6.416123390197754
Epoch 240, val loss: 0.8768174052238464
Epoch 250, training loss: 1.1839408874511719 = 0.5426753759384155 + 0.1 * 6.412655353546143
Epoch 250, val loss: 0.8528112173080444
Epoch 260, training loss: 1.1424460411071777 = 0.5029171705245972 + 0.1 * 6.395289421081543
Epoch 260, val loss: 0.8317921161651611
Epoch 270, training loss: 1.1038522720336914 = 0.46481138467788696 + 0.1 * 6.390408992767334
Epoch 270, val loss: 0.8136090040206909
Epoch 280, training loss: 1.0666300058364868 = 0.4285673499107361 + 0.1 * 6.380626678466797
Epoch 280, val loss: 0.7988021373748779
Epoch 290, training loss: 1.0307334661483765 = 0.3936906158924103 + 0.1 * 6.370428085327148
Epoch 290, val loss: 0.7866806983947754
Epoch 300, training loss: 0.9993383884429932 = 0.3599108159542084 + 0.1 * 6.394275665283203
Epoch 300, val loss: 0.7768603563308716
Epoch 310, training loss: 0.9647485017776489 = 0.3276028633117676 + 0.1 * 6.371456146240234
Epoch 310, val loss: 0.76930171251297
Epoch 320, training loss: 0.9327249526977539 = 0.29676470160484314 + 0.1 * 6.359601974487305
Epoch 320, val loss: 0.7632957696914673
Epoch 330, training loss: 0.9023926258087158 = 0.26762792468070984 + 0.1 * 6.347647190093994
Epoch 330, val loss: 0.7589741945266724
Epoch 340, training loss: 0.8747464418411255 = 0.24028708040714264 + 0.1 * 6.344593524932861
Epoch 340, val loss: 0.7558508515357971
Epoch 350, training loss: 0.8496089577674866 = 0.21518145501613617 + 0.1 * 6.344274520874023
Epoch 350, val loss: 0.754364550113678
Epoch 360, training loss: 0.8259120583534241 = 0.19243796169757843 + 0.1 * 6.334741115570068
Epoch 360, val loss: 0.7545008063316345
Epoch 370, training loss: 0.8050262928009033 = 0.17210105061531067 + 0.1 * 6.329252243041992
Epoch 370, val loss: 0.7560160160064697
Epoch 380, training loss: 0.7865394949913025 = 0.15407787263393402 + 0.1 * 6.324615955352783
Epoch 380, val loss: 0.7589032649993896
Epoch 390, training loss: 0.7704614400863647 = 0.13815966248512268 + 0.1 * 6.323017120361328
Epoch 390, val loss: 0.7630292773246765
Epoch 400, training loss: 0.7553044557571411 = 0.12409136444330215 + 0.1 * 6.312130451202393
Epoch 400, val loss: 0.7681583166122437
Epoch 410, training loss: 0.7420157790184021 = 0.11165563762187958 + 0.1 * 6.303601264953613
Epoch 410, val loss: 0.7740429639816284
Epoch 420, training loss: 0.7306756377220154 = 0.10063780844211578 + 0.1 * 6.300378322601318
Epoch 420, val loss: 0.7805746793746948
Epoch 430, training loss: 0.7218451499938965 = 0.09081867337226868 + 0.1 * 6.3102641105651855
Epoch 430, val loss: 0.7875556945800781
Epoch 440, training loss: 0.7110835909843445 = 0.08210238069295883 + 0.1 * 6.289812088012695
Epoch 440, val loss: 0.7947433590888977
Epoch 450, training loss: 0.7032924294471741 = 0.07434714585542679 + 0.1 * 6.28945255279541
Epoch 450, val loss: 0.8022836446762085
Epoch 460, training loss: 0.6961098909378052 = 0.06744368374347687 + 0.1 * 6.2866621017456055
Epoch 460, val loss: 0.8099152445793152
Epoch 470, training loss: 0.689525842666626 = 0.06130928173661232 + 0.1 * 6.28216552734375
Epoch 470, val loss: 0.817659318447113
Epoch 480, training loss: 0.6836276054382324 = 0.05584997311234474 + 0.1 * 6.27777624130249
Epoch 480, val loss: 0.8255448937416077
Epoch 490, training loss: 0.6789473295211792 = 0.05098665505647659 + 0.1 * 6.279606819152832
Epoch 490, val loss: 0.8334924578666687
Epoch 500, training loss: 0.6735453009605408 = 0.046663496643304825 + 0.1 * 6.26881742477417
Epoch 500, val loss: 0.841422438621521
Epoch 510, training loss: 0.6692377328872681 = 0.04281944781541824 + 0.1 * 6.2641825675964355
Epoch 510, val loss: 0.8493927121162415
Epoch 520, training loss: 0.6654607057571411 = 0.03939511626958847 + 0.1 * 6.260655879974365
Epoch 520, val loss: 0.8572589159011841
Epoch 530, training loss: 0.6620904803276062 = 0.03635147586464882 + 0.1 * 6.257389545440674
Epoch 530, val loss: 0.8651959300041199
Epoch 540, training loss: 0.659033477306366 = 0.03363380953669548 + 0.1 * 6.2539963722229
Epoch 540, val loss: 0.8728619813919067
Epoch 550, training loss: 0.6557556986808777 = 0.03121383674442768 + 0.1 * 6.245418548583984
Epoch 550, val loss: 0.8806881904602051
Epoch 560, training loss: 0.6541969776153564 = 0.029039207845926285 + 0.1 * 6.251577854156494
Epoch 560, val loss: 0.8882767558097839
Epoch 570, training loss: 0.650959312915802 = 0.02708371914923191 + 0.1 * 6.23875617980957
Epoch 570, val loss: 0.8958148956298828
Epoch 580, training loss: 0.6508170366287231 = 0.02531876601278782 + 0.1 * 6.254982948303223
Epoch 580, val loss: 0.9032036662101746
Epoch 590, training loss: 0.6473639607429504 = 0.023728573694825172 + 0.1 * 6.236353874206543
Epoch 590, val loss: 0.9104107618331909
Epoch 600, training loss: 0.6451037526130676 = 0.022291084751486778 + 0.1 * 6.228126525878906
Epoch 600, val loss: 0.9177106618881226
Epoch 610, training loss: 0.6440179944038391 = 0.02098424732685089 + 0.1 * 6.230337619781494
Epoch 610, val loss: 0.9246514439582825
Epoch 620, training loss: 0.6426692605018616 = 0.019798872992396355 + 0.1 * 6.22870397567749
Epoch 620, val loss: 0.9315947890281677
Epoch 630, training loss: 0.6405444145202637 = 0.018717044964432716 + 0.1 * 6.218273639678955
Epoch 630, val loss: 0.938348650932312
Epoch 640, training loss: 0.6394630074501038 = 0.017726195976138115 + 0.1 * 6.217368125915527
Epoch 640, val loss: 0.9451185464859009
Epoch 650, training loss: 0.6386351585388184 = 0.016814546659588814 + 0.1 * 6.21820592880249
Epoch 650, val loss: 0.9517005085945129
Epoch 660, training loss: 0.6392291784286499 = 0.01597691886126995 + 0.1 * 6.232522487640381
Epoch 660, val loss: 0.9580366611480713
Epoch 670, training loss: 0.6360170245170593 = 0.015206913463771343 + 0.1 * 6.20810079574585
Epoch 670, val loss: 0.9642873406410217
Epoch 680, training loss: 0.6354221105575562 = 0.014497298747301102 + 0.1 * 6.209248065948486
Epoch 680, val loss: 0.9705929160118103
Epoch 690, training loss: 0.6343541741371155 = 0.013838040642440319 + 0.1 * 6.205161094665527
Epoch 690, val loss: 0.9765890836715698
Epoch 700, training loss: 0.6349010467529297 = 0.013226890005171299 + 0.1 * 6.216741561889648
Epoch 700, val loss: 0.9824011921882629
Epoch 710, training loss: 0.6326646208763123 = 0.01266183890402317 + 0.1 * 6.200027942657471
Epoch 710, val loss: 0.9881913065910339
Epoch 720, training loss: 0.6313294768333435 = 0.012134697288274765 + 0.1 * 6.191947937011719
Epoch 720, val loss: 0.9940523505210876
Epoch 730, training loss: 0.6324927806854248 = 0.011641551740467548 + 0.1 * 6.208511829376221
Epoch 730, val loss: 0.9996392130851746
Epoch 740, training loss: 0.630466639995575 = 0.011180037632584572 + 0.1 * 6.19286584854126
Epoch 740, val loss: 1.0050249099731445
Epoch 750, training loss: 0.6296142339706421 = 0.01074968557804823 + 0.1 * 6.188645362854004
Epoch 750, val loss: 1.0106078386306763
Epoch 760, training loss: 0.6294199228286743 = 0.010344418697059155 + 0.1 * 6.1907548904418945
Epoch 760, val loss: 1.0157592296600342
Epoch 770, training loss: 0.6286136507987976 = 0.009966766461730003 + 0.1 * 6.186468601226807
Epoch 770, val loss: 1.0210044384002686
Epoch 780, training loss: 0.6283127069473267 = 0.009610557928681374 + 0.1 * 6.187021732330322
Epoch 780, val loss: 1.0261762142181396
Epoch 790, training loss: 0.6272616386413574 = 0.00927530787885189 + 0.1 * 6.179862976074219
Epoch 790, val loss: 1.0312508344650269
Epoch 800, training loss: 0.6268897652626038 = 0.008958879858255386 + 0.1 * 6.179308891296387
Epoch 800, val loss: 1.036060094833374
Epoch 810, training loss: 0.6258171796798706 = 0.008661407046020031 + 0.1 * 6.171557903289795
Epoch 810, val loss: 1.0409302711486816
Epoch 820, training loss: 0.6254666447639465 = 0.008379564620554447 + 0.1 * 6.170870780944824
Epoch 820, val loss: 1.0457755327224731
Epoch 830, training loss: 0.6265719532966614 = 0.00811201985925436 + 0.1 * 6.18459939956665
Epoch 830, val loss: 1.0503884553909302
Epoch 840, training loss: 0.6250708103179932 = 0.007859648205339909 + 0.1 * 6.172111511230469
Epoch 840, val loss: 1.0548865795135498
Epoch 850, training loss: 0.6241092085838318 = 0.007620792370289564 + 0.1 * 6.164883613586426
Epoch 850, val loss: 1.059561014175415
Epoch 860, training loss: 0.6242438554763794 = 0.007393076084554195 + 0.1 * 6.1685075759887695
Epoch 860, val loss: 1.063929557800293
Epoch 870, training loss: 0.6239069104194641 = 0.007177454419434071 + 0.1 * 6.167294502258301
Epoch 870, val loss: 1.0682188272476196
Epoch 880, training loss: 0.6235295534133911 = 0.006972627714276314 + 0.1 * 6.165569305419922
Epoch 880, val loss: 1.0724868774414062
Epoch 890, training loss: 0.6231403946876526 = 0.006777869537472725 + 0.1 * 6.163625240325928
Epoch 890, val loss: 1.0767539739608765
Epoch 900, training loss: 0.6225074529647827 = 0.006592142395675182 + 0.1 * 6.159152984619141
Epoch 900, val loss: 1.0808608531951904
Epoch 910, training loss: 0.6226462721824646 = 0.006415356416255236 + 0.1 * 6.162309169769287
Epoch 910, val loss: 1.084998369216919
Epoch 920, training loss: 0.6231030821800232 = 0.00624535558745265 + 0.1 * 6.168577194213867
Epoch 920, val loss: 1.0888707637786865
Epoch 930, training loss: 0.6218900084495544 = 0.006084247026592493 + 0.1 * 6.15805721282959
Epoch 930, val loss: 1.0927319526672363
Epoch 940, training loss: 0.621026337146759 = 0.0059302086010575294 + 0.1 * 6.150961399078369
Epoch 940, val loss: 1.0967680215835571
Epoch 950, training loss: 0.6216315627098083 = 0.0057825506664812565 + 0.1 * 6.158490180969238
Epoch 950, val loss: 1.1006945371627808
Epoch 960, training loss: 0.621070146560669 = 0.005640310235321522 + 0.1 * 6.154298305511475
Epoch 960, val loss: 1.1041259765625
Epoch 970, training loss: 0.6210206747055054 = 0.005505131091922522 + 0.1 * 6.155155181884766
Epoch 970, val loss: 1.107890248298645
Epoch 980, training loss: 0.6206409931182861 = 0.0053754583932459354 + 0.1 * 6.152655124664307
Epoch 980, val loss: 1.1115580797195435
Epoch 990, training loss: 0.6221012473106384 = 0.005250908900052309 + 0.1 * 6.168503284454346
Epoch 990, val loss: 1.1151293516159058
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.4576
Flip ASR: 0.3644/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.790210723876953 = 1.9528264999389648 + 0.1 * 8.3738431930542
Epoch 0, val loss: 1.9483070373535156
Epoch 10, training loss: 2.779195547103882 = 1.9418281316757202 + 0.1 * 8.373674392700195
Epoch 10, val loss: 1.9374192953109741
Epoch 20, training loss: 2.764869213104248 = 1.9275832176208496 + 0.1 * 8.372859954833984
Epoch 20, val loss: 1.9229426383972168
Epoch 30, training loss: 2.7438297271728516 = 1.9070827960968018 + 0.1 * 8.36746883392334
Epoch 30, val loss: 1.9019197225570679
Epoch 40, training loss: 2.7106308937072754 = 1.87678861618042 + 0.1 * 8.338423728942871
Epoch 40, val loss: 1.8715027570724487
Epoch 50, training loss: 2.6551578044891357 = 1.8362962007522583 + 0.1 * 8.188615798950195
Epoch 50, val loss: 1.8331154584884644
Epoch 60, training loss: 2.577788829803467 = 1.793112874031067 + 0.1 * 7.84675931930542
Epoch 60, val loss: 1.79521644115448
Epoch 70, training loss: 2.508906364440918 = 1.7538416385650635 + 0.1 * 7.550647258758545
Epoch 70, val loss: 1.7620055675506592
Epoch 80, training loss: 2.4305310249328613 = 1.7127904891967773 + 0.1 * 7.177404880523682
Epoch 80, val loss: 1.7265925407409668
Epoch 90, training loss: 2.3570945262908936 = 1.6587278842926025 + 0.1 * 6.98366641998291
Epoch 90, val loss: 1.6793568134307861
Epoch 100, training loss: 2.2717058658599854 = 1.5877711772918701 + 0.1 * 6.839345932006836
Epoch 100, val loss: 1.6193047761917114
Epoch 110, training loss: 2.1792399883270264 = 1.5034303665161133 + 0.1 * 6.7580952644348145
Epoch 110, val loss: 1.550504207611084
Epoch 120, training loss: 2.087639331817627 = 1.4160120487213135 + 0.1 * 6.716272830963135
Epoch 120, val loss: 1.4812120199203491
Epoch 130, training loss: 2.000457286834717 = 1.331903100013733 + 0.1 * 6.685542106628418
Epoch 130, val loss: 1.4172648191452026
Epoch 140, training loss: 1.917860507965088 = 1.2514383792877197 + 0.1 * 6.664220333099365
Epoch 140, val loss: 1.3598636388778687
Epoch 150, training loss: 1.8400225639343262 = 1.175734519958496 + 0.1 * 6.642879486083984
Epoch 150, val loss: 1.3081365823745728
Epoch 160, training loss: 1.766927719116211 = 1.1049001216888428 + 0.1 * 6.620275497436523
Epoch 160, val loss: 1.2610453367233276
Epoch 170, training loss: 1.7002313137054443 = 1.0396302938461304 + 0.1 * 6.606009483337402
Epoch 170, val loss: 1.2191193103790283
Epoch 180, training loss: 1.640775442123413 = 0.9815031290054321 + 0.1 * 6.592723846435547
Epoch 180, val loss: 1.182661533355713
Epoch 190, training loss: 1.5866200923919678 = 0.9288978576660156 + 0.1 * 6.5772223472595215
Epoch 190, val loss: 1.1498671770095825
Epoch 200, training loss: 1.5362112522125244 = 0.879588782787323 + 0.1 * 6.566225051879883
Epoch 200, val loss: 1.1189332008361816
Epoch 210, training loss: 1.486938238143921 = 0.8313784003257751 + 0.1 * 6.555598735809326
Epoch 210, val loss: 1.0882773399353027
Epoch 220, training loss: 1.4365613460540771 = 0.7819042205810547 + 0.1 * 6.546571254730225
Epoch 220, val loss: 1.0560667514801025
Epoch 230, training loss: 1.3835012912750244 = 0.7295024991035461 + 0.1 * 6.539987087249756
Epoch 230, val loss: 1.0210062265396118
Epoch 240, training loss: 1.327904462814331 = 0.6750869750976562 + 0.1 * 6.528175354003906
Epoch 240, val loss: 0.9838809370994568
Epoch 250, training loss: 1.2713261842727661 = 0.6196776032447815 + 0.1 * 6.516485691070557
Epoch 250, val loss: 0.9458383917808533
Epoch 260, training loss: 1.2155640125274658 = 0.5642250776290894 + 0.1 * 6.513389587402344
Epoch 260, val loss: 0.9084882736206055
Epoch 270, training loss: 1.1612480878829956 = 0.5103591084480286 + 0.1 * 6.508889675140381
Epoch 270, val loss: 0.8736105561256409
Epoch 280, training loss: 1.1078846454620361 = 0.4584828317165375 + 0.1 * 6.494018077850342
Epoch 280, val loss: 0.8421675562858582
Epoch 290, training loss: 1.057064175605774 = 0.40861353278160095 + 0.1 * 6.484506607055664
Epoch 290, val loss: 0.8144518136978149
Epoch 300, training loss: 1.0109184980392456 = 0.3615778684616089 + 0.1 * 6.493406295776367
Epoch 300, val loss: 0.7905657887458801
Epoch 310, training loss: 0.9657613635063171 = 0.3187404274940491 + 0.1 * 6.470209121704102
Epoch 310, val loss: 0.7716296315193176
Epoch 320, training loss: 0.926331639289856 = 0.2804226279258728 + 0.1 * 6.459090232849121
Epoch 320, val loss: 0.7572475075721741
Epoch 330, training loss: 0.8936609029769897 = 0.2469453066587448 + 0.1 * 6.467155933380127
Epoch 330, val loss: 0.7475524544715881
Epoch 340, training loss: 0.8631194233894348 = 0.21827924251556396 + 0.1 * 6.448401927947998
Epoch 340, val loss: 0.742343544960022
Epoch 350, training loss: 0.837755024433136 = 0.19376520812511444 + 0.1 * 6.4398980140686035
Epoch 350, val loss: 0.7407912015914917
Epoch 360, training loss: 0.8168287873268127 = 0.17284561693668365 + 0.1 * 6.439831733703613
Epoch 360, val loss: 0.7424130439758301
Epoch 370, training loss: 0.7974856495857239 = 0.1549927443265915 + 0.1 * 6.424928665161133
Epoch 370, val loss: 0.7464333176612854
Epoch 380, training loss: 0.782424807548523 = 0.1396133452653885 + 0.1 * 6.428114414215088
Epoch 380, val loss: 0.752465009689331
Epoch 390, training loss: 0.7667308449745178 = 0.12627965211868286 + 0.1 * 6.40451192855835
Epoch 390, val loss: 0.7600406408309937
Epoch 400, training loss: 0.754410982131958 = 0.11457664519548416 + 0.1 * 6.398343563079834
Epoch 400, val loss: 0.7687941193580627
Epoch 410, training loss: 0.7430889010429382 = 0.10425031185150146 + 0.1 * 6.388385772705078
Epoch 410, val loss: 0.7785302400588989
Epoch 420, training loss: 0.7392469048500061 = 0.09510619193315506 + 0.1 * 6.441407203674316
Epoch 420, val loss: 0.7888813018798828
Epoch 430, training loss: 0.7248857021331787 = 0.08704176545143127 + 0.1 * 6.378439426422119
Epoch 430, val loss: 0.799725353717804
Epoch 440, training loss: 0.7169045805931091 = 0.079841747879982 + 0.1 * 6.370628356933594
Epoch 440, val loss: 0.8109318017959595
Epoch 450, training loss: 0.712009072303772 = 0.07338418066501617 + 0.1 * 6.38624906539917
Epoch 450, val loss: 0.8225237131118774
Epoch 460, training loss: 0.7030478715896606 = 0.06760402023792267 + 0.1 * 6.354438304901123
Epoch 460, val loss: 0.8344829082489014
Epoch 470, training loss: 0.6976597309112549 = 0.062397561967372894 + 0.1 * 6.352621555328369
Epoch 470, val loss: 0.8466390371322632
Epoch 480, training loss: 0.693426251411438 = 0.05770201236009598 + 0.1 * 6.357242107391357
Epoch 480, val loss: 0.8588628768920898
Epoch 490, training loss: 0.6883102059364319 = 0.053474415093660355 + 0.1 * 6.348357677459717
Epoch 490, val loss: 0.871211588382721
Epoch 500, training loss: 0.6829826235771179 = 0.049663443118333817 + 0.1 * 6.333191871643066
Epoch 500, val loss: 0.8834366202354431
Epoch 510, training loss: 0.6794164776802063 = 0.04622088372707367 + 0.1 * 6.331955432891846
Epoch 510, val loss: 0.8957183957099915
Epoch 520, training loss: 0.6770280003547668 = 0.043103210628032684 + 0.1 * 6.339247703552246
Epoch 520, val loss: 0.9078197479248047
Epoch 530, training loss: 0.6729881763458252 = 0.04028700292110443 + 0.1 * 6.327011585235596
Epoch 530, val loss: 0.9197148084640503
Epoch 540, training loss: 0.6688657999038696 = 0.03773140162229538 + 0.1 * 6.311343669891357
Epoch 540, val loss: 0.9315888285636902
Epoch 550, training loss: 0.6687779426574707 = 0.035404086112976074 + 0.1 * 6.333738327026367
Epoch 550, val loss: 0.9431160688400269
Epoch 560, training loss: 0.663323163986206 = 0.033292289823293686 + 0.1 * 6.300308704376221
Epoch 560, val loss: 0.9545609951019287
Epoch 570, training loss: 0.6619048714637756 = 0.03136139363050461 + 0.1 * 6.305434703826904
Epoch 570, val loss: 0.9658234715461731
Epoch 580, training loss: 0.6592992544174194 = 0.029594002291560173 + 0.1 * 6.297052383422852
Epoch 580, val loss: 0.9767119288444519
Epoch 590, training loss: 0.6569501161575317 = 0.027975060045719147 + 0.1 * 6.289750099182129
Epoch 590, val loss: 0.9873745441436768
Epoch 600, training loss: 0.6553195118904114 = 0.026493437588214874 + 0.1 * 6.288260459899902
Epoch 600, val loss: 0.9979692101478577
Epoch 610, training loss: 0.6530612111091614 = 0.02512628585100174 + 0.1 * 6.279349327087402
Epoch 610, val loss: 1.008325457572937
Epoch 620, training loss: 0.6529944539070129 = 0.02386154979467392 + 0.1 * 6.2913289070129395
Epoch 620, val loss: 1.018401026725769
Epoch 630, training loss: 0.6497378349304199 = 0.022694876417517662 + 0.1 * 6.270429611206055
Epoch 630, val loss: 1.02830171585083
Epoch 640, training loss: 0.6492823362350464 = 0.021614478901028633 + 0.1 * 6.276678085327148
Epoch 640, val loss: 1.0381110906600952
Epoch 650, training loss: 0.6484695076942444 = 0.020611904561519623 + 0.1 * 6.278575897216797
Epoch 650, val loss: 1.0475502014160156
Epoch 660, training loss: 0.6459535956382751 = 0.01968369260430336 + 0.1 * 6.262698650360107
Epoch 660, val loss: 1.056741714477539
Epoch 670, training loss: 0.6443190574645996 = 0.018820995464920998 + 0.1 * 6.254980564117432
Epoch 670, val loss: 1.0658782720565796
Epoch 680, training loss: 0.6447829008102417 = 0.01801488548517227 + 0.1 * 6.267679691314697
Epoch 680, val loss: 1.0746819972991943
Epoch 690, training loss: 0.6426541209220886 = 0.01726350747048855 + 0.1 * 6.253905773162842
Epoch 690, val loss: 1.083302617073059
Epoch 700, training loss: 0.6411944627761841 = 0.016561497002840042 + 0.1 * 6.246329307556152
Epoch 700, val loss: 1.0918611288070679
Epoch 710, training loss: 0.6430652141571045 = 0.01590169221162796 + 0.1 * 6.271635055541992
Epoch 710, val loss: 1.1001744270324707
Epoch 720, training loss: 0.6392661333084106 = 0.015284384600818157 + 0.1 * 6.2398176193237305
Epoch 720, val loss: 1.1082404851913452
Epoch 730, training loss: 0.6386834979057312 = 0.014706348069012165 + 0.1 * 6.239771366119385
Epoch 730, val loss: 1.1163369417190552
Epoch 740, training loss: 0.6374297738075256 = 0.014160695485770702 + 0.1 * 6.232690811157227
Epoch 740, val loss: 1.1241085529327393
Epoch 750, training loss: 0.6364691257476807 = 0.01364812534302473 + 0.1 * 6.228209495544434
Epoch 750, val loss: 1.131945013999939
Epoch 760, training loss: 0.6375712752342224 = 0.013164001516997814 + 0.1 * 6.244072914123535
Epoch 760, val loss: 1.1395082473754883
Epoch 770, training loss: 0.6353771686553955 = 0.012705947272479534 + 0.1 * 6.226712226867676
Epoch 770, val loss: 1.146822214126587
Epoch 780, training loss: 0.6350589394569397 = 0.012274608947336674 + 0.1 * 6.227842807769775
Epoch 780, val loss: 1.1542928218841553
Epoch 790, training loss: 0.6347219944000244 = 0.01186496764421463 + 0.1 * 6.228570461273193
Epoch 790, val loss: 1.1612664461135864
Epoch 800, training loss: 0.6333647966384888 = 0.011479062959551811 + 0.1 * 6.218857288360596
Epoch 800, val loss: 1.16840660572052
Epoch 810, training loss: 0.6323497891426086 = 0.011112939566373825 + 0.1 * 6.212368488311768
Epoch 810, val loss: 1.175429344177246
Epoch 820, training loss: 0.6320522427558899 = 0.01076467428356409 + 0.1 * 6.212875843048096
Epoch 820, val loss: 1.1822201013565063
Epoch 830, training loss: 0.6312458515167236 = 0.010433164425194263 + 0.1 * 6.208127021789551
Epoch 830, val loss: 1.1888353824615479
Epoch 840, training loss: 0.632167398929596 = 0.010118420235812664 + 0.1 * 6.220489501953125
Epoch 840, val loss: 1.1954288482666016
Epoch 850, training loss: 0.6306014060974121 = 0.0098190912976861 + 0.1 * 6.207823276519775
Epoch 850, val loss: 1.2018312215805054
Epoch 860, training loss: 0.6303507089614868 = 0.009534948505461216 + 0.1 * 6.208157539367676
Epoch 860, val loss: 1.2083876132965088
Epoch 870, training loss: 0.6289354562759399 = 0.009263142943382263 + 0.1 * 6.196722984313965
Epoch 870, val loss: 1.2146929502487183
Epoch 880, training loss: 0.6294158697128296 = 0.009003693237900734 + 0.1 * 6.2041215896606445
Epoch 880, val loss: 1.2209583520889282
Epoch 890, training loss: 0.6288864612579346 = 0.008755591697990894 + 0.1 * 6.201308727264404
Epoch 890, val loss: 1.2269949913024902
Epoch 900, training loss: 0.6284449100494385 = 0.008519230410456657 + 0.1 * 6.199256420135498
Epoch 900, val loss: 1.2330830097198486
Epoch 910, training loss: 0.6277651786804199 = 0.00829364638775587 + 0.1 * 6.19471549987793
Epoch 910, val loss: 1.2390234470367432
Epoch 920, training loss: 0.6272374987602234 = 0.008077881298959255 + 0.1 * 6.191596031188965
Epoch 920, val loss: 1.2449723482131958
Epoch 930, training loss: 0.6281803250312805 = 0.007870624773204327 + 0.1 * 6.203096866607666
Epoch 930, val loss: 1.250728726387024
Epoch 940, training loss: 0.6267998218536377 = 0.007672101259231567 + 0.1 * 6.191277027130127
Epoch 940, val loss: 1.2563146352767944
Epoch 950, training loss: 0.6260131597518921 = 0.007482202723622322 + 0.1 * 6.185308933258057
Epoch 950, val loss: 1.262047529220581
Epoch 960, training loss: 0.6268682479858398 = 0.007299724500626326 + 0.1 * 6.195685386657715
Epoch 960, val loss: 1.2676178216934204
Epoch 970, training loss: 0.62608802318573 = 0.007124362047761679 + 0.1 * 6.18963623046875
Epoch 970, val loss: 1.2730827331542969
Epoch 980, training loss: 0.6251069903373718 = 0.006956466007977724 + 0.1 * 6.18150520324707
Epoch 980, val loss: 1.2784879207611084
Epoch 990, training loss: 0.6251317858695984 = 0.006794859655201435 + 0.1 * 6.183369159698486
Epoch 990, val loss: 1.2837814092636108
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6716
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7750020027160645 = 1.9376236200332642 + 0.1 * 8.373783111572266
Epoch 0, val loss: 1.9351885318756104
Epoch 10, training loss: 2.7642807960510254 = 1.9269381761550903 + 0.1 * 8.37342643737793
Epoch 10, val loss: 1.923934817314148
Epoch 20, training loss: 2.750779151916504 = 1.913626790046692 + 0.1 * 8.371522903442383
Epoch 20, val loss: 1.9095922708511353
Epoch 30, training loss: 2.7307498455047607 = 1.8946119546890259 + 0.1 * 8.36137866973877
Epoch 30, val loss: 1.888836145401001
Epoch 40, training loss: 2.6972732543945312 = 1.866969347000122 + 0.1 * 8.303038597106934
Epoch 40, val loss: 1.8592274188995361
Epoch 50, training loss: 2.6271698474884033 = 1.8312402963638306 + 0.1 * 7.959295272827148
Epoch 50, val loss: 1.8227375745773315
Epoch 60, training loss: 2.5454933643341064 = 1.7914533615112305 + 0.1 * 7.540399074554443
Epoch 60, val loss: 1.7845755815505981
Epoch 70, training loss: 2.472895860671997 = 1.7512669563293457 + 0.1 * 7.216288089752197
Epoch 70, val loss: 1.7485865354537964
Epoch 80, training loss: 2.410191297531128 = 1.7104588747024536 + 0.1 * 6.997323513031006
Epoch 80, val loss: 1.7138116359710693
Epoch 90, training loss: 2.3484420776367188 = 1.660771131515503 + 0.1 * 6.876708984375
Epoch 90, val loss: 1.6711872816085815
Epoch 100, training loss: 2.2757110595703125 = 1.5943280458450317 + 0.1 * 6.8138298988342285
Epoch 100, val loss: 1.6158250570297241
Epoch 110, training loss: 2.1866908073425293 = 1.510145664215088 + 0.1 * 6.765451431274414
Epoch 110, val loss: 1.5491611957550049
Epoch 120, training loss: 2.086104154586792 = 1.413124442100525 + 0.1 * 6.72979736328125
Epoch 120, val loss: 1.4727723598480225
Epoch 130, training loss: 1.9828428030014038 = 1.3122464418411255 + 0.1 * 6.705963611602783
Epoch 130, val loss: 1.394365906715393
Epoch 140, training loss: 1.8828058242797852 = 1.2142021656036377 + 0.1 * 6.686036109924316
Epoch 140, val loss: 1.318873643875122
Epoch 150, training loss: 1.7868561744689941 = 1.1203068494796753 + 0.1 * 6.665493011474609
Epoch 150, val loss: 1.2482506036758423
Epoch 160, training loss: 1.6949230432510376 = 1.0299705266952515 + 0.1 * 6.649525165557861
Epoch 160, val loss: 1.181585669517517
Epoch 170, training loss: 1.605506181716919 = 0.9419517517089844 + 0.1 * 6.635544300079346
Epoch 170, val loss: 1.116575002670288
Epoch 180, training loss: 1.518244981765747 = 0.856128990650177 + 0.1 * 6.621159553527832
Epoch 180, val loss: 1.0525063276290894
Epoch 190, training loss: 1.4367889165878296 = 0.7757453322410583 + 0.1 * 6.610435485839844
Epoch 190, val loss: 0.9932501912117004
Epoch 200, training loss: 1.3631727695465088 = 0.7032822966575623 + 0.1 * 6.598904609680176
Epoch 200, val loss: 0.9414589405059814
Epoch 210, training loss: 1.2980560064315796 = 0.6394014954566956 + 0.1 * 6.586544990539551
Epoch 210, val loss: 0.8975396752357483
Epoch 220, training loss: 1.2407660484313965 = 0.5831184387207031 + 0.1 * 6.576475620269775
Epoch 220, val loss: 0.8611331582069397
Epoch 230, training loss: 1.189857006072998 = 0.532833456993103 + 0.1 * 6.570234775543213
Epoch 230, val loss: 0.8312947750091553
Epoch 240, training loss: 1.143223524093628 = 0.4878748655319214 + 0.1 * 6.5534868240356445
Epoch 240, val loss: 0.807270884513855
Epoch 250, training loss: 1.1019225120544434 = 0.44704774022102356 + 0.1 * 6.5487470626831055
Epoch 250, val loss: 0.7878679633140564
Epoch 260, training loss: 1.0627878904342651 = 0.40992188453674316 + 0.1 * 6.528659820556641
Epoch 260, val loss: 0.7725266814231873
Epoch 270, training loss: 1.027383804321289 = 0.375745952129364 + 0.1 * 6.516378402709961
Epoch 270, val loss: 0.7601510882377625
Epoch 280, training loss: 0.9948862195014954 = 0.34422773122787476 + 0.1 * 6.506584644317627
Epoch 280, val loss: 0.7502229809761047
Epoch 290, training loss: 0.9643303751945496 = 0.3149298429489136 + 0.1 * 6.49400520324707
Epoch 290, val loss: 0.7420430779457092
Epoch 300, training loss: 0.9361122846603394 = 0.28748390078544617 + 0.1 * 6.486284255981445
Epoch 300, val loss: 0.7351978421211243
Epoch 310, training loss: 0.9091500639915466 = 0.2618139982223511 + 0.1 * 6.473360538482666
Epoch 310, val loss: 0.7292476296424866
Epoch 320, training loss: 0.8845030069351196 = 0.23782728612422943 + 0.1 * 6.466756820678711
Epoch 320, val loss: 0.724260151386261
Epoch 330, training loss: 0.8611890077590942 = 0.21562333405017853 + 0.1 * 6.4556565284729
Epoch 330, val loss: 0.720093846321106
Epoch 340, training loss: 0.8402303457260132 = 0.19506549835205078 + 0.1 * 6.451648235321045
Epoch 340, val loss: 0.716704249382019
Epoch 350, training loss: 0.8202215433120728 = 0.17633463442325592 + 0.1 * 6.438868999481201
Epoch 350, val loss: 0.7144246697425842
Epoch 360, training loss: 0.8025155067443848 = 0.15940049290657043 + 0.1 * 6.431149482727051
Epoch 360, val loss: 0.713148832321167
Epoch 370, training loss: 0.7875375151634216 = 0.14412355422973633 + 0.1 * 6.434139251708984
Epoch 370, val loss: 0.7129000425338745
Epoch 380, training loss: 0.7719943523406982 = 0.1304233968257904 + 0.1 * 6.415709018707275
Epoch 380, val loss: 0.7136334180831909
Epoch 390, training loss: 0.7585843801498413 = 0.11816494166851044 + 0.1 * 6.404193878173828
Epoch 390, val loss: 0.715343713760376
Epoch 400, training loss: 0.7467788457870483 = 0.10724249482154846 + 0.1 * 6.395363807678223
Epoch 400, val loss: 0.7178510427474976
Epoch 410, training loss: 0.7369263172149658 = 0.09753400832414627 + 0.1 * 6.393923282623291
Epoch 410, val loss: 0.7210950255393982
Epoch 420, training loss: 0.7270314693450928 = 0.0889194905757904 + 0.1 * 6.381119251251221
Epoch 420, val loss: 0.724840521812439
Epoch 430, training loss: 0.7182040810585022 = 0.08122003078460693 + 0.1 * 6.369840145111084
Epoch 430, val loss: 0.7290988564491272
Epoch 440, training loss: 0.7112545967102051 = 0.07433657348155975 + 0.1 * 6.369180202484131
Epoch 440, val loss: 0.7337318658828735
Epoch 450, training loss: 0.7037023305892944 = 0.06819310784339905 + 0.1 * 6.3550920486450195
Epoch 450, val loss: 0.7386707067489624
Epoch 460, training loss: 0.6994699239730835 = 0.06269048154354095 + 0.1 * 6.367794036865234
Epoch 460, val loss: 0.7438696026802063
Epoch 470, training loss: 0.6925228238105774 = 0.05777575075626373 + 0.1 * 6.347470760345459
Epoch 470, val loss: 0.7491270303726196
Epoch 480, training loss: 0.6867536306381226 = 0.05336481332778931 + 0.1 * 6.333888053894043
Epoch 480, val loss: 0.7545903325080872
Epoch 490, training loss: 0.6836119890213013 = 0.04939745366573334 + 0.1 * 6.342144966125488
Epoch 490, val loss: 0.7600038051605225
Epoch 500, training loss: 0.6785939335823059 = 0.04584231227636337 + 0.1 * 6.327516078948975
Epoch 500, val loss: 0.7655420303344727
Epoch 510, training loss: 0.6739253401756287 = 0.04263053834438324 + 0.1 * 6.312947750091553
Epoch 510, val loss: 0.7711371779441833
Epoch 520, training loss: 0.6710062026977539 = 0.03971877321600914 + 0.1 * 6.3128743171691895
Epoch 520, val loss: 0.7767006158828735
Epoch 530, training loss: 0.6683493852615356 = 0.037080422043800354 + 0.1 * 6.312689781188965
Epoch 530, val loss: 0.7822169065475464
Epoch 540, training loss: 0.6641532182693481 = 0.034694038331508636 + 0.1 * 6.294591903686523
Epoch 540, val loss: 0.787742018699646
Epoch 550, training loss: 0.6613543033599854 = 0.0325203500688076 + 0.1 * 6.288339614868164
Epoch 550, val loss: 0.7932928204536438
Epoch 560, training loss: 0.6606372594833374 = 0.030535226687788963 + 0.1 * 6.30102014541626
Epoch 560, val loss: 0.7986806035041809
Epoch 570, training loss: 0.6576133370399475 = 0.02872459404170513 + 0.1 * 6.288887023925781
Epoch 570, val loss: 0.8040547966957092
Epoch 580, training loss: 0.6551883220672607 = 0.027069268748164177 + 0.1 * 6.281190395355225
Epoch 580, val loss: 0.8094606995582581
Epoch 590, training loss: 0.6534993648529053 = 0.025547439232468605 + 0.1 * 6.279519081115723
Epoch 590, val loss: 0.8147405385971069
Epoch 600, training loss: 0.6511313915252686 = 0.024151861667633057 + 0.1 * 6.2697954177856445
Epoch 600, val loss: 0.819922149181366
Epoch 610, training loss: 0.6493046283721924 = 0.022868704050779343 + 0.1 * 6.264359474182129
Epoch 610, val loss: 0.8251285552978516
Epoch 620, training loss: 0.648878812789917 = 0.021684056147933006 + 0.1 * 6.271947383880615
Epoch 620, val loss: 0.8302035331726074
Epoch 630, training loss: 0.6470867395401001 = 0.020591840147972107 + 0.1 * 6.264948844909668
Epoch 630, val loss: 0.835168182849884
Epoch 640, training loss: 0.6445273160934448 = 0.01957983523607254 + 0.1 * 6.249475002288818
Epoch 640, val loss: 0.840169370174408
Epoch 650, training loss: 0.6452882289886475 = 0.018642256036400795 + 0.1 * 6.266459941864014
Epoch 650, val loss: 0.845068633556366
Epoch 660, training loss: 0.6428579688072205 = 0.017771834507584572 + 0.1 * 6.250861167907715
Epoch 660, val loss: 0.849822998046875
Epoch 670, training loss: 0.64069002866745 = 0.016963722184300423 + 0.1 * 6.237262725830078
Epoch 670, val loss: 0.8545833826065063
Epoch 680, training loss: 0.640644907951355 = 0.016209430992603302 + 0.1 * 6.244354724884033
Epoch 680, val loss: 0.8592303991317749
Epoch 690, training loss: 0.6390585899353027 = 0.015506410971283913 + 0.1 * 6.2355217933654785
Epoch 690, val loss: 0.8637466430664062
Epoch 700, training loss: 0.6376867294311523 = 0.01484945509582758 + 0.1 * 6.228372097015381
Epoch 700, val loss: 0.8683121204376221
Epoch 710, training loss: 0.6385239958763123 = 0.014234999194741249 + 0.1 * 6.242889404296875
Epoch 710, val loss: 0.872843861579895
Epoch 720, training loss: 0.6371085047721863 = 0.013660927303135395 + 0.1 * 6.234476089477539
Epoch 720, val loss: 0.8772124648094177
Epoch 730, training loss: 0.6350841522216797 = 0.013122202828526497 + 0.1 * 6.219619274139404
Epoch 730, val loss: 0.8815431594848633
Epoch 740, training loss: 0.6348427534103394 = 0.012617537751793861 + 0.1 * 6.222252368927002
Epoch 740, val loss: 0.8858113884925842
Epoch 750, training loss: 0.6334132552146912 = 0.012142403982579708 + 0.1 * 6.212707996368408
Epoch 750, val loss: 0.889990508556366
Epoch 760, training loss: 0.6340784430503845 = 0.011695191264152527 + 0.1 * 6.223832130432129
Epoch 760, val loss: 0.8941383361816406
Epoch 770, training loss: 0.6320650577545166 = 0.011274023912847042 + 0.1 * 6.207910060882568
Epoch 770, val loss: 0.8982412815093994
Epoch 780, training loss: 0.6342949867248535 = 0.010877181775867939 + 0.1 * 6.234177589416504
Epoch 780, val loss: 0.902252733707428
Epoch 790, training loss: 0.6319822669029236 = 0.010503116063773632 + 0.1 * 6.214791297912598
Epoch 790, val loss: 0.9062159061431885
Epoch 800, training loss: 0.6306577324867249 = 0.01015029288828373 + 0.1 * 6.205074310302734
Epoch 800, val loss: 0.9100973010063171
Epoch 810, training loss: 0.6297873258590698 = 0.009816383011639118 + 0.1 * 6.199708938598633
Epoch 810, val loss: 0.9138818383216858
Epoch 820, training loss: 0.6292316317558289 = 0.009499486535787582 + 0.1 * 6.19732141494751
Epoch 820, val loss: 0.9175812602043152
Epoch 830, training loss: 0.630757749080658 = 0.009198752231895924 + 0.1 * 6.215590000152588
Epoch 830, val loss: 0.9213337898254395
Epoch 840, training loss: 0.6286970376968384 = 0.008912920951843262 + 0.1 * 6.197841167449951
Epoch 840, val loss: 0.9249404072761536
Epoch 850, training loss: 0.62970370054245 = 0.008641818538308144 + 0.1 * 6.210618495941162
Epoch 850, val loss: 0.9285880327224731
Epoch 860, training loss: 0.6273791193962097 = 0.008385088294744492 + 0.1 * 6.189940452575684
Epoch 860, val loss: 0.9320452213287354
Epoch 870, training loss: 0.626746416091919 = 0.008140661753714085 + 0.1 * 6.186057090759277
Epoch 870, val loss: 0.9355170130729675
Epoch 880, training loss: 0.626592218875885 = 0.007907032035291195 + 0.1 * 6.186851978302002
Epoch 880, val loss: 0.9389066696166992
Epoch 890, training loss: 0.6269527673721313 = 0.007684360258281231 + 0.1 * 6.192684173583984
Epoch 890, val loss: 0.9422321915626526
Epoch 900, training loss: 0.6267312169075012 = 0.00747208297252655 + 0.1 * 6.192591190338135
Epoch 900, val loss: 0.9455538392066956
Epoch 910, training loss: 0.6258366107940674 = 0.0072697605937719345 + 0.1 * 6.185668468475342
Epoch 910, val loss: 0.9488511681556702
Epoch 920, training loss: 0.6250055432319641 = 0.007076771464198828 + 0.1 * 6.179287433624268
Epoch 920, val loss: 0.9520856142044067
Epoch 930, training loss: 0.6251698732376099 = 0.006892357021570206 + 0.1 * 6.182774543762207
Epoch 930, val loss: 0.9552276730537415
Epoch 940, training loss: 0.6238523721694946 = 0.006715753581374884 + 0.1 * 6.171365737915039
Epoch 940, val loss: 0.9583730101585388
Epoch 950, training loss: 0.6259348392486572 = 0.006546420976519585 + 0.1 * 6.193883895874023
Epoch 950, val loss: 0.9614790081977844
Epoch 960, training loss: 0.6240225434303284 = 0.006383529398590326 + 0.1 * 6.176389694213867
Epoch 960, val loss: 0.964459240436554
Epoch 970, training loss: 0.6250878572463989 = 0.006228149868547916 + 0.1 * 6.188597202301025
Epoch 970, val loss: 0.9674830436706543
Epoch 980, training loss: 0.6228364109992981 = 0.0060790772549808025 + 0.1 * 6.167572975158691
Epoch 980, val loss: 0.9704502820968628
Epoch 990, training loss: 0.6232775449752808 = 0.0059359315782785416 + 0.1 * 6.1734161376953125
Epoch 990, val loss: 0.9733691215515137
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7638
Flip ASR: 0.7333/225 nodes
The final ASR:0.63100, 0.12829, Accuracy:0.81111, 0.00907
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9542])
updated graph: torch.Size([2, 10594])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7651562690734863 = 1.927783727645874 + 0.1 * 8.373724937438965
Epoch 0, val loss: 1.919854760169983
Epoch 10, training loss: 2.7557036876678467 = 1.9183681011199951 + 0.1 * 8.3733549118042
Epoch 10, val loss: 1.910872220993042
Epoch 20, training loss: 2.7437117099761963 = 1.9065920114517212 + 0.1 * 8.371197700500488
Epoch 20, val loss: 1.8993152379989624
Epoch 30, training loss: 2.7256431579589844 = 1.8899298906326294 + 0.1 * 8.357131958007812
Epoch 30, val loss: 1.8828959465026855
Epoch 40, training loss: 2.6936256885528564 = 1.8655316829681396 + 0.1 * 8.280940055847168
Epoch 40, val loss: 1.8595391511917114
Epoch 50, training loss: 2.6217339038848877 = 1.833949327468872 + 0.1 * 7.8778462409973145
Epoch 50, val loss: 1.8314144611358643
Epoch 60, training loss: 2.5523529052734375 = 1.8000684976577759 + 0.1 * 7.522844314575195
Epoch 60, val loss: 1.8038275241851807
Epoch 70, training loss: 2.4785146713256836 = 1.7692232131958008 + 0.1 * 7.092913627624512
Epoch 70, val loss: 1.7804065942764282
Epoch 80, training loss: 2.417703151702881 = 1.7364529371261597 + 0.1 * 6.812503337860107
Epoch 80, val loss: 1.755029559135437
Epoch 90, training loss: 2.361262559890747 = 1.6913546323776245 + 0.1 * 6.699079513549805
Epoch 90, val loss: 1.717134714126587
Epoch 100, training loss: 2.293902635574341 = 1.6288005113601685 + 0.1 * 6.651020526885986
Epoch 100, val loss: 1.66512930393219
Epoch 110, training loss: 2.2129030227661133 = 1.550187587738037 + 0.1 * 6.627153396606445
Epoch 110, val loss: 1.6023650169372559
Epoch 120, training loss: 2.1234474182128906 = 1.462345838546753 + 0.1 * 6.611014366149902
Epoch 120, val loss: 1.5321369171142578
Epoch 130, training loss: 2.031975507736206 = 1.3721824884414673 + 0.1 * 6.597929954528809
Epoch 130, val loss: 1.462218999862671
Epoch 140, training loss: 1.9405555725097656 = 1.2820836305618286 + 0.1 * 6.584719657897949
Epoch 140, val loss: 1.3941730260849
Epoch 150, training loss: 1.8492810726165771 = 1.1921707391738892 + 0.1 * 6.571103572845459
Epoch 150, val loss: 1.3267663717269897
Epoch 160, training loss: 1.7605986595153809 = 1.1041507720947266 + 0.1 * 6.564478874206543
Epoch 160, val loss: 1.261889100074768
Epoch 170, training loss: 1.6731230020523071 = 1.0178173780441284 + 0.1 * 6.553056240081787
Epoch 170, val loss: 1.1991208791732788
Epoch 180, training loss: 1.586400032043457 = 0.9320809245109558 + 0.1 * 6.543191432952881
Epoch 180, val loss: 1.1365482807159424
Epoch 190, training loss: 1.5021297931671143 = 0.8489956259727478 + 0.1 * 6.531341075897217
Epoch 190, val loss: 1.0754953622817993
Epoch 200, training loss: 1.4241323471069336 = 0.7715526223182678 + 0.1 * 6.525797367095947
Epoch 200, val loss: 1.017957091331482
Epoch 210, training loss: 1.3532755374908447 = 0.7022795677185059 + 0.1 * 6.5099592208862305
Epoch 210, val loss: 0.9667401313781738
Epoch 220, training loss: 1.290076732635498 = 0.6402905583381653 + 0.1 * 6.497861862182617
Epoch 220, val loss: 0.9214110970497131
Epoch 230, training loss: 1.2331713438034058 = 0.5845469236373901 + 0.1 * 6.486244201660156
Epoch 230, val loss: 0.8818607926368713
Epoch 240, training loss: 1.1806533336639404 = 0.5332071185112 + 0.1 * 6.474462509155273
Epoch 240, val loss: 0.8467189073562622
Epoch 250, training loss: 1.132957935333252 = 0.48489004373550415 + 0.1 * 6.480678081512451
Epoch 250, val loss: 0.8154367208480835
Epoch 260, training loss: 1.084903597831726 = 0.43981900811195374 + 0.1 * 6.450845241546631
Epoch 260, val loss: 0.7887566089630127
Epoch 270, training loss: 1.0414600372314453 = 0.39738038182258606 + 0.1 * 6.4407958984375
Epoch 270, val loss: 0.7661588191986084
Epoch 280, training loss: 1.000579595565796 = 0.35756203532218933 + 0.1 * 6.43017578125
Epoch 280, val loss: 0.7475492358207703
Epoch 290, training loss: 0.9625519514083862 = 0.32079508900642395 + 0.1 * 6.417568206787109
Epoch 290, val loss: 0.732691764831543
Epoch 300, training loss: 0.9286219477653503 = 0.28675657510757446 + 0.1 * 6.41865348815918
Epoch 300, val loss: 0.7202572822570801
Epoch 310, training loss: 0.8947476744651794 = 0.25558018684387207 + 0.1 * 6.391674518585205
Epoch 310, val loss: 0.7099867463111877
Epoch 320, training loss: 0.8665791153907776 = 0.22699545323848724 + 0.1 * 6.395836353302002
Epoch 320, val loss: 0.7014667987823486
Epoch 330, training loss: 0.8388912081718445 = 0.20124725997447968 + 0.1 * 6.376439094543457
Epoch 330, val loss: 0.6946365833282471
Epoch 340, training loss: 0.8141961097717285 = 0.17820078134536743 + 0.1 * 6.3599534034729
Epoch 340, val loss: 0.6892946362495422
Epoch 350, training loss: 0.7953460216522217 = 0.1577337682247162 + 0.1 * 6.376121997833252
Epoch 350, val loss: 0.6856480836868286
Epoch 360, training loss: 0.775659441947937 = 0.13997474312782288 + 0.1 * 6.356846809387207
Epoch 360, val loss: 0.6836479306221008
Epoch 370, training loss: 0.7581764459609985 = 0.12455406039953232 + 0.1 * 6.33622407913208
Epoch 370, val loss: 0.6831517815589905
Epoch 380, training loss: 0.7441772222518921 = 0.1111195981502533 + 0.1 * 6.330576419830322
Epoch 380, val loss: 0.6841341853141785
Epoch 390, training loss: 0.731853723526001 = 0.09947255253791809 + 0.1 * 6.323812007904053
Epoch 390, val loss: 0.6864009499549866
Epoch 400, training loss: 0.720707893371582 = 0.08939948678016663 + 0.1 * 6.313083648681641
Epoch 400, val loss: 0.6897509098052979
Epoch 410, training loss: 0.7112113833427429 = 0.08069176971912384 + 0.1 * 6.305196285247803
Epoch 410, val loss: 0.6940957307815552
Epoch 420, training loss: 0.7033905982971191 = 0.07309675216674805 + 0.1 * 6.302938461303711
Epoch 420, val loss: 0.6991724371910095
Epoch 430, training loss: 0.6959515810012817 = 0.06647121161222458 + 0.1 * 6.294803619384766
Epoch 430, val loss: 0.7049047350883484
Epoch 440, training loss: 0.6898736357688904 = 0.0606636218726635 + 0.1 * 6.292099952697754
Epoch 440, val loss: 0.7111347317695618
Epoch 450, training loss: 0.6853964924812317 = 0.055553633719682693 + 0.1 * 6.298428535461426
Epoch 450, val loss: 0.717670202255249
Epoch 460, training loss: 0.6791146397590637 = 0.05105920135974884 + 0.1 * 6.280554294586182
Epoch 460, val loss: 0.7245068550109863
Epoch 470, training loss: 0.673937976360321 = 0.047074198722839355 + 0.1 * 6.268637657165527
Epoch 470, val loss: 0.7315337061882019
Epoch 480, training loss: 0.6711206436157227 = 0.043527401983737946 + 0.1 * 6.2759318351745605
Epoch 480, val loss: 0.7386037111282349
Epoch 490, training loss: 0.6669740676879883 = 0.040373485535383224 + 0.1 * 6.266005516052246
Epoch 490, val loss: 0.7458280324935913
Epoch 500, training loss: 0.6640915870666504 = 0.03754445165395737 + 0.1 * 6.265471458435059
Epoch 500, val loss: 0.7530039548873901
Epoch 510, training loss: 0.6608548760414124 = 0.03500786051154137 + 0.1 * 6.258470058441162
Epoch 510, val loss: 0.7601912617683411
Epoch 520, training loss: 0.6574838161468506 = 0.03272681310772896 + 0.1 * 6.247570037841797
Epoch 520, val loss: 0.7673299312591553
Epoch 530, training loss: 0.6548689603805542 = 0.030667772516608238 + 0.1 * 6.242011547088623
Epoch 530, val loss: 0.7745047807693481
Epoch 540, training loss: 0.6529356837272644 = 0.02879597432911396 + 0.1 * 6.241396903991699
Epoch 540, val loss: 0.7815958857536316
Epoch 550, training loss: 0.6503499150276184 = 0.027093833312392235 + 0.1 * 6.232560634613037
Epoch 550, val loss: 0.7884972095489502
Epoch 560, training loss: 0.6490795016288757 = 0.025547031313180923 + 0.1 * 6.235324859619141
Epoch 560, val loss: 0.7955117225646973
Epoch 570, training loss: 0.6481581926345825 = 0.024129310622811317 + 0.1 * 6.240288257598877
Epoch 570, val loss: 0.8023141026496887
Epoch 580, training loss: 0.6461552381515503 = 0.022832604125142097 + 0.1 * 6.2332258224487305
Epoch 580, val loss: 0.8090047836303711
Epoch 590, training loss: 0.6437810659408569 = 0.021643303334712982 + 0.1 * 6.221377849578857
Epoch 590, val loss: 0.8157213926315308
Epoch 600, training loss: 0.6424480080604553 = 0.020547006279230118 + 0.1 * 6.2190093994140625
Epoch 600, val loss: 0.8221011161804199
Epoch 610, training loss: 0.6417651176452637 = 0.019540969282388687 + 0.1 * 6.222241401672363
Epoch 610, val loss: 0.8285650610923767
Epoch 620, training loss: 0.6398093700408936 = 0.018608003854751587 + 0.1 * 6.212013244628906
Epoch 620, val loss: 0.8348862528800964
Epoch 630, training loss: 0.6384628415107727 = 0.0177425816655159 + 0.1 * 6.207202434539795
Epoch 630, val loss: 0.8409828543663025
Epoch 640, training loss: 0.6376741528511047 = 0.01694100722670555 + 0.1 * 6.20733118057251
Epoch 640, val loss: 0.8471182584762573
Epoch 650, training loss: 0.6369039416313171 = 0.016193892806768417 + 0.1 * 6.2071003913879395
Epoch 650, val loss: 0.8530077934265137
Epoch 660, training loss: 0.6354138255119324 = 0.015499697998166084 + 0.1 * 6.199141025543213
Epoch 660, val loss: 0.8589298725128174
Epoch 670, training loss: 0.6348314881324768 = 0.01485036127269268 + 0.1 * 6.199810981750488
Epoch 670, val loss: 0.8647451996803284
Epoch 680, training loss: 0.6349093914031982 = 0.014241455122828484 + 0.1 * 6.206679344177246
Epoch 680, val loss: 0.8703356981277466
Epoch 690, training loss: 0.6332650780677795 = 0.013673636130988598 + 0.1 * 6.195914268493652
Epoch 690, val loss: 0.8758347034454346
Epoch 700, training loss: 0.6322361826896667 = 0.013142337091267109 + 0.1 * 6.190938472747803
Epoch 700, val loss: 0.8813208341598511
Epoch 710, training loss: 0.6314569115638733 = 0.012642023153603077 + 0.1 * 6.188148498535156
Epoch 710, val loss: 0.8866949677467346
Epoch 720, training loss: 0.6309630870819092 = 0.012170608155429363 + 0.1 * 6.187924385070801
Epoch 720, val loss: 0.8917432427406311
Epoch 730, training loss: 0.6306591033935547 = 0.011730005033314228 + 0.1 * 6.189290523529053
Epoch 730, val loss: 0.8969513177871704
Epoch 740, training loss: 0.6295862197875977 = 0.011314853094518185 + 0.1 * 6.182713508605957
Epoch 740, val loss: 0.9021034240722656
Epoch 750, training loss: 0.6291933655738831 = 0.010921342298388481 + 0.1 * 6.182720184326172
Epoch 750, val loss: 0.9068682789802551
Epoch 760, training loss: 0.6288116574287415 = 0.010552067309617996 + 0.1 * 6.182596206665039
Epoch 760, val loss: 0.9117441177368164
Epoch 770, training loss: 0.6278970837593079 = 0.010203221812844276 + 0.1 * 6.176938056945801
Epoch 770, val loss: 0.9165921807289124
Epoch 780, training loss: 0.628358781337738 = 0.00987109262496233 + 0.1 * 6.184876918792725
Epoch 780, val loss: 0.921244740486145
Epoch 790, training loss: 0.6274328231811523 = 0.009556621313095093 + 0.1 * 6.1787614822387695
Epoch 790, val loss: 0.9257502555847168
Epoch 800, training loss: 0.6261152625083923 = 0.009259442798793316 + 0.1 * 6.168558120727539
Epoch 800, val loss: 0.9303687810897827
Epoch 810, training loss: 0.6259084939956665 = 0.008976317942142487 + 0.1 * 6.169321537017822
Epoch 810, val loss: 0.9348973631858826
Epoch 820, training loss: 0.6251788139343262 = 0.008706610649824142 + 0.1 * 6.164721965789795
Epoch 820, val loss: 0.9392262101173401
Epoch 830, training loss: 0.624549925327301 = 0.008450545370578766 + 0.1 * 6.160993576049805
Epoch 830, val loss: 0.9435653686523438
Epoch 840, training loss: 0.6265570521354675 = 0.00820644199848175 + 0.1 * 6.183506011962891
Epoch 840, val loss: 0.9478336572647095
Epoch 850, training loss: 0.6250630021095276 = 0.007973372004926205 + 0.1 * 6.170896053314209
Epoch 850, val loss: 0.9518683552742004
Epoch 860, training loss: 0.623146116733551 = 0.007753266487270594 + 0.1 * 6.153928279876709
Epoch 860, val loss: 0.9560922980308533
Epoch 870, training loss: 0.6239790320396423 = 0.007542516570538282 + 0.1 * 6.164365291595459
Epoch 870, val loss: 0.9602183103561401
Epoch 880, training loss: 0.622840404510498 = 0.00734011922031641 + 0.1 * 6.155002593994141
Epoch 880, val loss: 0.9640643000602722
Epoch 890, training loss: 0.6241872310638428 = 0.0071476297453045845 + 0.1 * 6.170396327972412
Epoch 890, val loss: 0.9679767489433289
Epoch 900, training loss: 0.6222059726715088 = 0.006963464431464672 + 0.1 * 6.1524248123168945
Epoch 900, val loss: 0.9717316627502441
Epoch 910, training loss: 0.6216765642166138 = 0.006787725258618593 + 0.1 * 6.148888111114502
Epoch 910, val loss: 0.9756134152412415
Epoch 920, training loss: 0.6219053268432617 = 0.0066184429451823235 + 0.1 * 6.152868747711182
Epoch 920, val loss: 0.9792213439941406
Epoch 930, training loss: 0.6217527389526367 = 0.006456701084971428 + 0.1 * 6.152960300445557
Epoch 930, val loss: 0.9829189777374268
Epoch 940, training loss: 0.6217442750930786 = 0.006301692686975002 + 0.1 * 6.154425621032715
Epoch 940, val loss: 0.9865131378173828
Epoch 950, training loss: 0.6202065944671631 = 0.006152805406600237 + 0.1 * 6.140537738800049
Epoch 950, val loss: 0.9901250600814819
Epoch 960, training loss: 0.6208527088165283 = 0.006009839475154877 + 0.1 * 6.148428916931152
Epoch 960, val loss: 0.9936851263046265
Epoch 970, training loss: 0.6198513507843018 = 0.005871576722711325 + 0.1 * 6.139797210693359
Epoch 970, val loss: 0.9970225095748901
Epoch 980, training loss: 0.619133472442627 = 0.005739275831729174 + 0.1 * 6.133941650390625
Epoch 980, val loss: 1.0005115270614624
Epoch 990, training loss: 0.6208723187446594 = 0.005611802451312542 + 0.1 * 6.152605056762695
Epoch 990, val loss: 1.0039616823196411
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5351
Flip ASR: 0.4444/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7716386318206787 = 1.9342691898345947 + 0.1 * 8.37369441986084
Epoch 0, val loss: 1.936212420463562
Epoch 10, training loss: 2.7621726989746094 = 1.9248297214508057 + 0.1 * 8.373428344726562
Epoch 10, val loss: 1.92603600025177
Epoch 20, training loss: 2.7507405281066895 = 1.9135648012161255 + 0.1 * 8.371758460998535
Epoch 20, val loss: 1.9135165214538574
Epoch 30, training loss: 2.734200954437256 = 1.898146390914917 + 0.1 * 8.360544204711914
Epoch 30, val loss: 1.8962466716766357
Epoch 40, training loss: 2.703569173812866 = 1.8756049871444702 + 0.1 * 8.279642105102539
Epoch 40, val loss: 1.871107578277588
Epoch 50, training loss: 2.59433913230896 = 1.8467066287994385 + 0.1 * 7.476324558258057
Epoch 50, val loss: 1.8403019905090332
Epoch 60, training loss: 2.5173654556274414 = 1.8182440996170044 + 0.1 * 6.991213798522949
Epoch 60, val loss: 1.8115084171295166
Epoch 70, training loss: 2.467116355895996 = 1.7866525650024414 + 0.1 * 6.804637908935547
Epoch 70, val loss: 1.7803059816360474
Epoch 80, training loss: 2.4223406314849854 = 1.7513326406478882 + 0.1 * 6.710080623626709
Epoch 80, val loss: 1.746504545211792
Epoch 90, training loss: 2.3752083778381348 = 1.7102628946304321 + 0.1 * 6.649456024169922
Epoch 90, val loss: 1.7098382711410522
Epoch 100, training loss: 2.3164820671081543 = 1.6552942991256714 + 0.1 * 6.61187744140625
Epoch 100, val loss: 1.6630977392196655
Epoch 110, training loss: 2.240391969680786 = 1.5816601514816284 + 0.1 * 6.58731746673584
Epoch 110, val loss: 1.603022575378418
Epoch 120, training loss: 2.1455821990966797 = 1.4886393547058105 + 0.1 * 6.56942892074585
Epoch 120, val loss: 1.5283231735229492
Epoch 130, training loss: 2.0379180908203125 = 1.3819680213928223 + 0.1 * 6.5595011711120605
Epoch 130, val loss: 1.4444140195846558
Epoch 140, training loss: 1.9250550270080566 = 1.270067811012268 + 0.1 * 6.549871921539307
Epoch 140, val loss: 1.3581691980361938
Epoch 150, training loss: 1.8120019435882568 = 1.1582871675491333 + 0.1 * 6.537147045135498
Epoch 150, val loss: 1.2744261026382446
Epoch 160, training loss: 1.7021782398223877 = 1.0498234033584595 + 0.1 * 6.5235490798950195
Epoch 160, val loss: 1.1928437948226929
Epoch 170, training loss: 1.5994691848754883 = 0.9486127495765686 + 0.1 * 6.508563995361328
Epoch 170, val loss: 1.1155751943588257
Epoch 180, training loss: 1.5044472217559814 = 0.8550558686256409 + 0.1 * 6.493913650512695
Epoch 180, val loss: 1.0440019369125366
Epoch 190, training loss: 1.4176902770996094 = 0.769343912601471 + 0.1 * 6.483462810516357
Epoch 190, val loss: 0.9796565175056458
Epoch 200, training loss: 1.338245153427124 = 0.6919248104095459 + 0.1 * 6.4632039070129395
Epoch 200, val loss: 0.9226711988449097
Epoch 210, training loss: 1.2685401439666748 = 0.622390866279602 + 0.1 * 6.461493015289307
Epoch 210, val loss: 0.8725864887237549
Epoch 220, training loss: 1.2052911520004272 = 0.5620177388191223 + 0.1 * 6.43273401260376
Epoch 220, val loss: 0.8304440975189209
Epoch 230, training loss: 1.1511645317077637 = 0.5094849467277527 + 0.1 * 6.4167962074279785
Epoch 230, val loss: 0.795650839805603
Epoch 240, training loss: 1.1061029434204102 = 0.46356236934661865 + 0.1 * 6.425405025482178
Epoch 240, val loss: 0.7677608132362366
Epoch 250, training loss: 1.062610387802124 = 0.42317235469818115 + 0.1 * 6.394380569458008
Epoch 250, val loss: 0.7459635138511658
Epoch 260, training loss: 1.0247756242752075 = 0.38614675402641296 + 0.1 * 6.386288166046143
Epoch 260, val loss: 0.7282807230949402
Epoch 270, training loss: 0.9908819198608398 = 0.35142841935157776 + 0.1 * 6.394535064697266
Epoch 270, val loss: 0.7138938307762146
Epoch 280, training loss: 0.9561284780502319 = 0.31887087225914 + 0.1 * 6.3725762367248535
Epoch 280, val loss: 0.7022998332977295
Epoch 290, training loss: 0.9243850708007812 = 0.28816041350364685 + 0.1 * 6.362246513366699
Epoch 290, val loss: 0.6930791139602661
Epoch 300, training loss: 0.8949251174926758 = 0.2592022716999054 + 0.1 * 6.357227802276611
Epoch 300, val loss: 0.6858236789703369
Epoch 310, training loss: 0.8671019673347473 = 0.2325706034898758 + 0.1 * 6.345313549041748
Epoch 310, val loss: 0.6808690428733826
Epoch 320, training loss: 0.8425068855285645 = 0.20839598774909973 + 0.1 * 6.341108798980713
Epoch 320, val loss: 0.6780494451522827
Epoch 330, training loss: 0.8220878839492798 = 0.18686191737651825 + 0.1 * 6.352259159088135
Epoch 330, val loss: 0.6774908304214478
Epoch 340, training loss: 0.8006882071495056 = 0.16816116869449615 + 0.1 * 6.325270652770996
Epoch 340, val loss: 0.6789966821670532
Epoch 350, training loss: 0.7840516567230225 = 0.15186510980129242 + 0.1 * 6.321865081787109
Epoch 350, val loss: 0.6823967099189758
Epoch 360, training loss: 0.7705273032188416 = 0.1377110332250595 + 0.1 * 6.32816219329834
Epoch 360, val loss: 0.6875305771827698
Epoch 370, training loss: 0.756255567073822 = 0.1255093216896057 + 0.1 * 6.307462215423584
Epoch 370, val loss: 0.693953275680542
Epoch 380, training loss: 0.7454480528831482 = 0.11489716917276382 + 0.1 * 6.305508613586426
Epoch 380, val loss: 0.7013980150222778
Epoch 390, training loss: 0.7354710102081299 = 0.1056094691157341 + 0.1 * 6.298614978790283
Epoch 390, val loss: 0.7096942067146301
Epoch 400, training loss: 0.72846919298172 = 0.09746641665697098 + 0.1 * 6.310027599334717
Epoch 400, val loss: 0.7185637354850769
Epoch 410, training loss: 0.7193210124969482 = 0.09031564742326736 + 0.1 * 6.290053367614746
Epoch 410, val loss: 0.7279829382896423
Epoch 420, training loss: 0.7127736210823059 = 0.08394104987382889 + 0.1 * 6.288325786590576
Epoch 420, val loss: 0.7376944422721863
Epoch 430, training loss: 0.7065064311027527 = 0.07826048880815506 + 0.1 * 6.282459259033203
Epoch 430, val loss: 0.7474988698959351
Epoch 440, training loss: 0.7013599872589111 = 0.07318389415740967 + 0.1 * 6.2817606925964355
Epoch 440, val loss: 0.7574364542961121
Epoch 450, training loss: 0.6959511041641235 = 0.06862115859985352 + 0.1 * 6.273299217224121
Epoch 450, val loss: 0.767434298992157
Epoch 460, training loss: 0.6909211277961731 = 0.0644865557551384 + 0.1 * 6.264345645904541
Epoch 460, val loss: 0.7774450778961182
Epoch 470, training loss: 0.6869917511940002 = 0.06072264537215233 + 0.1 * 6.262691020965576
Epoch 470, val loss: 0.7873682975769043
Epoch 480, training loss: 0.6831772327423096 = 0.0572914257645607 + 0.1 * 6.258857727050781
Epoch 480, val loss: 0.7972199320793152
Epoch 490, training loss: 0.6803926825523376 = 0.0541372075676918 + 0.1 * 6.262554168701172
Epoch 490, val loss: 0.8069531917572021
Epoch 500, training loss: 0.6763868927955627 = 0.05124055594205856 + 0.1 * 6.251462936401367
Epoch 500, val loss: 0.8165605664253235
Epoch 510, training loss: 0.6730572581291199 = 0.048568401485681534 + 0.1 * 6.2448883056640625
Epoch 510, val loss: 0.8260939121246338
Epoch 520, training loss: 0.6727320551872253 = 0.04608314111828804 + 0.1 * 6.266489028930664
Epoch 520, val loss: 0.8354946374893188
Epoch 530, training loss: 0.6672185659408569 = 0.04378446564078331 + 0.1 * 6.234341144561768
Epoch 530, val loss: 0.8447042107582092
Epoch 540, training loss: 0.6650952100753784 = 0.04164012894034386 + 0.1 * 6.234550952911377
Epoch 540, val loss: 0.8538682460784912
Epoch 550, training loss: 0.6630309224128723 = 0.039620526134967804 + 0.1 * 6.234103679656982
Epoch 550, val loss: 0.8628741502761841
Epoch 560, training loss: 0.6616151332855225 = 0.037719134241342545 + 0.1 * 6.238959789276123
Epoch 560, val loss: 0.871581494808197
Epoch 570, training loss: 0.6581364870071411 = 0.0359356552362442 + 0.1 * 6.222008228302002
Epoch 570, val loss: 0.8802025318145752
Epoch 580, training loss: 0.6571294665336609 = 0.03425735980272293 + 0.1 * 6.2287211418151855
Epoch 580, val loss: 0.8887056708335876
Epoch 590, training loss: 0.6547955274581909 = 0.03267484903335571 + 0.1 * 6.2212066650390625
Epoch 590, val loss: 0.8970536589622498
Epoch 600, training loss: 0.6532718539237976 = 0.031186778098344803 + 0.1 * 6.220850944519043
Epoch 600, val loss: 0.9053542613983154
Epoch 610, training loss: 0.6515061855316162 = 0.02977851592004299 + 0.1 * 6.217276096343994
Epoch 610, val loss: 0.9135944247245789
Epoch 620, training loss: 0.649616003036499 = 0.02844509296119213 + 0.1 * 6.211709499359131
Epoch 620, val loss: 0.9215537309646606
Epoch 630, training loss: 0.6477698683738708 = 0.027177782729268074 + 0.1 * 6.205920219421387
Epoch 630, val loss: 0.929408848285675
Epoch 640, training loss: 0.6482160091400146 = 0.025972113013267517 + 0.1 * 6.222438812255859
Epoch 640, val loss: 0.9371820688247681
Epoch 650, training loss: 0.645363450050354 = 0.024828866124153137 + 0.1 * 6.205345630645752
Epoch 650, val loss: 0.9448216557502747
Epoch 660, training loss: 0.6442558169364929 = 0.023746004328131676 + 0.1 * 6.205097675323486
Epoch 660, val loss: 0.9524238705635071
Epoch 670, training loss: 0.6424639225006104 = 0.022716665640473366 + 0.1 * 6.19747257232666
Epoch 670, val loss: 0.959792971611023
Epoch 680, training loss: 0.6411047577857971 = 0.02174472250044346 + 0.1 * 6.193600177764893
Epoch 680, val loss: 0.9672040939331055
Epoch 690, training loss: 0.6420426964759827 = 0.020818285644054413 + 0.1 * 6.212244033813477
Epoch 690, val loss: 0.9743682742118835
Epoch 700, training loss: 0.6394698619842529 = 0.019940899685025215 + 0.1 * 6.195289134979248
Epoch 700, val loss: 0.9813705086708069
Epoch 710, training loss: 0.6379607915878296 = 0.01910991221666336 + 0.1 * 6.1885085105896
Epoch 710, val loss: 0.9884710311889648
Epoch 720, training loss: 0.6373817324638367 = 0.018320268020033836 + 0.1 * 6.190614700317383
Epoch 720, val loss: 0.9953267574310303
Epoch 730, training loss: 0.6363688111305237 = 0.017568176612257957 + 0.1 * 6.188006401062012
Epoch 730, val loss: 1.0021592378616333
Epoch 740, training loss: 0.635158121585846 = 0.016848193481564522 + 0.1 * 6.183099269866943
Epoch 740, val loss: 1.0088855028152466
Epoch 750, training loss: 0.6341303586959839 = 0.0161573626101017 + 0.1 * 6.179729461669922
Epoch 750, val loss: 1.0155216455459595
Epoch 760, training loss: 0.6333764791488647 = 0.015494734048843384 + 0.1 * 6.178817272186279
Epoch 760, val loss: 1.0220781564712524
Epoch 770, training loss: 0.6325341463088989 = 0.014859010465443134 + 0.1 * 6.176751136779785
Epoch 770, val loss: 1.0284113883972168
Epoch 780, training loss: 0.6316534280776978 = 0.014255880378186703 + 0.1 * 6.173975467681885
Epoch 780, val loss: 1.0348937511444092
Epoch 790, training loss: 0.6323771476745605 = 0.013683930039405823 + 0.1 * 6.18693208694458
Epoch 790, val loss: 1.0412572622299194
Epoch 800, training loss: 0.6306061148643494 = 0.0131516233086586 + 0.1 * 6.174544811248779
Epoch 800, val loss: 1.0473766326904297
Epoch 810, training loss: 0.6294177770614624 = 0.012654928490519524 + 0.1 * 6.167628765106201
Epoch 810, val loss: 1.0535857677459717
Epoch 820, training loss: 0.6303720474243164 = 0.012181969359517097 + 0.1 * 6.181900501251221
Epoch 820, val loss: 1.0594853162765503
Epoch 830, training loss: 0.6289299130439758 = 0.01173480600118637 + 0.1 * 6.1719512939453125
Epoch 830, val loss: 1.0654224157333374
Epoch 840, training loss: 0.6283385753631592 = 0.011309471912682056 + 0.1 * 6.170291423797607
Epoch 840, val loss: 1.0713083744049072
Epoch 850, training loss: 0.6272201538085938 = 0.01090692076832056 + 0.1 * 6.163132190704346
Epoch 850, val loss: 1.0770097970962524
Epoch 860, training loss: 0.627170205116272 = 0.010525415651500225 + 0.1 * 6.16644811630249
Epoch 860, val loss: 1.0825639963150024
Epoch 870, training loss: 0.6255634427070618 = 0.010163480415940285 + 0.1 * 6.153999328613281
Epoch 870, val loss: 1.0881210565567017
Epoch 880, training loss: 0.6268672347068787 = 0.009820963256061077 + 0.1 * 6.1704630851745605
Epoch 880, val loss: 1.0935763120651245
Epoch 890, training loss: 0.6250612735748291 = 0.009497708640992641 + 0.1 * 6.155635833740234
Epoch 890, val loss: 1.0988972187042236
Epoch 900, training loss: 0.62578284740448 = 0.009191409684717655 + 0.1 * 6.165914535522461
Epoch 900, val loss: 1.1042187213897705
Epoch 910, training loss: 0.6242047548294067 = 0.008899591863155365 + 0.1 * 6.153051853179932
Epoch 910, val loss: 1.1094266176223755
Epoch 920, training loss: 0.6238758563995361 = 0.008621763437986374 + 0.1 * 6.152541160583496
Epoch 920, val loss: 1.1145344972610474
Epoch 930, training loss: 0.624459981918335 = 0.008356771431863308 + 0.1 * 6.161032199859619
Epoch 930, val loss: 1.1194305419921875
Epoch 940, training loss: 0.6236016750335693 = 0.00810602679848671 + 0.1 * 6.154956340789795
Epoch 940, val loss: 1.1243540048599243
Epoch 950, training loss: 0.6226683855056763 = 0.007866732776165009 + 0.1 * 6.148016452789307
Epoch 950, val loss: 1.1291903257369995
Epoch 960, training loss: 0.6218867897987366 = 0.007639843970537186 + 0.1 * 6.14246940612793
Epoch 960, val loss: 1.1340141296386719
Epoch 970, training loss: 0.6227233409881592 = 0.007424418348819017 + 0.1 * 6.152988910675049
Epoch 970, val loss: 1.1387380361557007
Epoch 980, training loss: 0.6217575073242188 = 0.007219215389341116 + 0.1 * 6.145382881164551
Epoch 980, val loss: 1.1432602405548096
Epoch 990, training loss: 0.621281087398529 = 0.007024185732007027 + 0.1 * 6.142569065093994
Epoch 990, val loss: 1.1477525234222412
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8192
Flip ASR: 0.7867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.801036834716797 = 1.9636634588241577 + 0.1 * 8.373732566833496
Epoch 0, val loss: 1.9609730243682861
Epoch 10, training loss: 2.7896652221679688 = 1.952331304550171 + 0.1 * 8.37333869934082
Epoch 10, val loss: 1.9494540691375732
Epoch 20, training loss: 2.7752156257629395 = 1.9381049871444702 + 0.1 * 8.371105194091797
Epoch 20, val loss: 1.9343758821487427
Epoch 30, training loss: 2.7532424926757812 = 1.9174326658248901 + 0.1 * 8.358098983764648
Epoch 30, val loss: 1.9120004177093506
Epoch 40, training loss: 2.7140355110168457 = 1.8863550424575806 + 0.1 * 8.27680492401123
Epoch 40, val loss: 1.878713607788086
Epoch 50, training loss: 2.6183249950408936 = 1.8462036848068237 + 0.1 * 7.7212138175964355
Epoch 50, val loss: 1.8383102416992188
Epoch 60, training loss: 2.5296173095703125 = 1.8070703744888306 + 0.1 * 7.22546911239624
Epoch 60, val loss: 1.8014240264892578
Epoch 70, training loss: 2.4608542919158936 = 1.769771933555603 + 0.1 * 6.910822868347168
Epoch 70, val loss: 1.767433524131775
Epoch 80, training loss: 2.4105119705200195 = 1.7330515384674072 + 0.1 * 6.7746052742004395
Epoch 80, val loss: 1.735372543334961
Epoch 90, training loss: 2.357276201248169 = 1.6880948543548584 + 0.1 * 6.691812515258789
Epoch 90, val loss: 1.6961040496826172
Epoch 100, training loss: 2.292701482772827 = 1.6284878253936768 + 0.1 * 6.642136096954346
Epoch 100, val loss: 1.6457558870315552
Epoch 110, training loss: 2.2127323150634766 = 1.5516718626022339 + 0.1 * 6.610604286193848
Epoch 110, val loss: 1.583746075630188
Epoch 120, training loss: 2.1207728385925293 = 1.4613932371139526 + 0.1 * 6.593794822692871
Epoch 120, val loss: 1.5134587287902832
Epoch 130, training loss: 2.0256054401397705 = 1.3674561977386475 + 0.1 * 6.581492900848389
Epoch 130, val loss: 1.4436155557632446
Epoch 140, training loss: 1.9315335750579834 = 1.2748706340789795 + 0.1 * 6.566628932952881
Epoch 140, val loss: 1.3783769607543945
Epoch 150, training loss: 1.8390452861785889 = 1.1835532188415527 + 0.1 * 6.554920196533203
Epoch 150, val loss: 1.3169430494308472
Epoch 160, training loss: 1.7472145557403564 = 1.0938607454299927 + 0.1 * 6.533538341522217
Epoch 160, val loss: 1.255765676498413
Epoch 170, training loss: 1.6559109687805176 = 1.004643201828003 + 0.1 * 6.51267671585083
Epoch 170, val loss: 1.194762945175171
Epoch 180, training loss: 1.568770408630371 = 0.9187260866165161 + 0.1 * 6.5004425048828125
Epoch 180, val loss: 1.1366204023361206
Epoch 190, training loss: 1.4876782894134521 = 0.8393080234527588 + 0.1 * 6.483702659606934
Epoch 190, val loss: 1.0852408409118652
Epoch 200, training loss: 1.4154834747314453 = 0.7689321041107178 + 0.1 * 6.465512752532959
Epoch 200, val loss: 1.0419976711273193
Epoch 210, training loss: 1.352892279624939 = 0.7076584696769714 + 0.1 * 6.452338218688965
Epoch 210, val loss: 1.006714940071106
Epoch 220, training loss: 1.2979536056518555 = 0.6539409756660461 + 0.1 * 6.440126895904541
Epoch 220, val loss: 0.9773271083831787
Epoch 230, training loss: 1.2495185136795044 = 0.6065934896469116 + 0.1 * 6.429250240325928
Epoch 230, val loss: 0.9530431628227234
Epoch 240, training loss: 1.20517897605896 = 0.5638937950134277 + 0.1 * 6.4128522872924805
Epoch 240, val loss: 0.9329179525375366
Epoch 250, training loss: 1.1656702756881714 = 0.5244260430335999 + 0.1 * 6.412442207336426
Epoch 250, val loss: 0.916265606880188
Epoch 260, training loss: 1.127343773841858 = 0.4878466725349426 + 0.1 * 6.394970893859863
Epoch 260, val loss: 0.9028804898262024
Epoch 270, training loss: 1.0920284986495972 = 0.453475683927536 + 0.1 * 6.385528087615967
Epoch 270, val loss: 0.8921173214912415
Epoch 280, training loss: 1.0590885877609253 = 0.42117711901664734 + 0.1 * 6.379115104675293
Epoch 280, val loss: 0.8839366436004639
Epoch 290, training loss: 1.0282169580459595 = 0.3910004794597626 + 0.1 * 6.372165203094482
Epoch 290, val loss: 0.878208339214325
Epoch 300, training loss: 0.999333381652832 = 0.3625669777393341 + 0.1 * 6.367664337158203
Epoch 300, val loss: 0.8744839429855347
Epoch 310, training loss: 0.9720858335494995 = 0.33598315715789795 + 0.1 * 6.361026763916016
Epoch 310, val loss: 0.8726597428321838
Epoch 320, training loss: 0.9457759857177734 = 0.3109837770462036 + 0.1 * 6.347921848297119
Epoch 320, val loss: 0.8718659281730652
Epoch 330, training loss: 0.9230581521987915 = 0.2873409688472748 + 0.1 * 6.357171535491943
Epoch 330, val loss: 0.8718041181564331
Epoch 340, training loss: 0.8994052410125732 = 0.26501134037971497 + 0.1 * 6.343939304351807
Epoch 340, val loss: 0.87202388048172
Epoch 350, training loss: 0.8771765232086182 = 0.24379685521125793 + 0.1 * 6.33379602432251
Epoch 350, val loss: 0.8724597692489624
Epoch 360, training loss: 0.8580459952354431 = 0.22352127730846405 + 0.1 * 6.3452467918396
Epoch 360, val loss: 0.8728330135345459
Epoch 370, training loss: 0.8362478017807007 = 0.2042359858751297 + 0.1 * 6.320118427276611
Epoch 370, val loss: 0.8729837536811829
Epoch 380, training loss: 0.8176118731498718 = 0.18565255403518677 + 0.1 * 6.3195929527282715
Epoch 380, val loss: 0.8730451464653015
Epoch 390, training loss: 0.7989209294319153 = 0.16794408857822418 + 0.1 * 6.3097686767578125
Epoch 390, val loss: 0.8733510375022888
Epoch 400, training loss: 0.7817054390907288 = 0.15129196643829346 + 0.1 * 6.304134845733643
Epoch 400, val loss: 0.8744306564331055
Epoch 410, training loss: 0.7669743895530701 = 0.13591736555099487 + 0.1 * 6.310570240020752
Epoch 410, val loss: 0.8764984607696533
Epoch 420, training loss: 0.7530674934387207 = 0.12212848663330078 + 0.1 * 6.309390068054199
Epoch 420, val loss: 0.8796163201332092
Epoch 430, training loss: 0.7391374111175537 = 0.10991300642490387 + 0.1 * 6.292243957519531
Epoch 430, val loss: 0.8840208649635315
Epoch 440, training loss: 0.7284144163131714 = 0.0990343689918518 + 0.1 * 6.293800354003906
Epoch 440, val loss: 0.8895630836486816
Epoch 450, training loss: 0.7184353470802307 = 0.0893222838640213 + 0.1 * 6.291130542755127
Epoch 450, val loss: 0.8960322141647339
Epoch 460, training loss: 0.70942622423172 = 0.08070693165063858 + 0.1 * 6.287192344665527
Epoch 460, val loss: 0.9034931063652039
Epoch 470, training loss: 0.7006403207778931 = 0.07312337309122086 + 0.1 * 6.2751688957214355
Epoch 470, val loss: 0.9116390943527222
Epoch 480, training loss: 0.6933621764183044 = 0.06644272804260254 + 0.1 * 6.26919412612915
Epoch 480, val loss: 0.9205785393714905
Epoch 490, training loss: 0.6872060894966125 = 0.06057688221335411 + 0.1 * 6.266292095184326
Epoch 490, val loss: 0.9299677014350891
Epoch 500, training loss: 0.6815640330314636 = 0.0554201565682888 + 0.1 * 6.261438369750977
Epoch 500, val loss: 0.9396228194236755
Epoch 510, training loss: 0.676554262638092 = 0.05086136981844902 + 0.1 * 6.25692892074585
Epoch 510, val loss: 0.9494364261627197
Epoch 520, training loss: 0.6728759407997131 = 0.04682166874408722 + 0.1 * 6.260542869567871
Epoch 520, val loss: 0.9591699838638306
Epoch 530, training loss: 0.6682616472244263 = 0.04323688521981239 + 0.1 * 6.250247478485107
Epoch 530, val loss: 0.9689343571662903
Epoch 540, training loss: 0.6651527881622314 = 0.04003280773758888 + 0.1 * 6.251199722290039
Epoch 540, val loss: 0.978588879108429
Epoch 550, training loss: 0.661393940448761 = 0.03716767951846123 + 0.1 * 6.242262363433838
Epoch 550, val loss: 0.9881150722503662
Epoch 560, training loss: 0.6598713397979736 = 0.034597042948007584 + 0.1 * 6.252742767333984
Epoch 560, val loss: 0.9974820613861084
Epoch 570, training loss: 0.6555338501930237 = 0.03229416534304619 + 0.1 * 6.232396602630615
Epoch 570, val loss: 1.0065780878067017
Epoch 580, training loss: 0.653060257434845 = 0.030219364911317825 + 0.1 * 6.2284088134765625
Epoch 580, val loss: 1.015533685684204
Epoch 590, training loss: 0.6520392894744873 = 0.028338778764009476 + 0.1 * 6.23700475692749
Epoch 590, val loss: 1.024293303489685
Epoch 600, training loss: 0.6500760912895203 = 0.02663634903728962 + 0.1 * 6.2343974113464355
Epoch 600, val loss: 1.0328614711761475
Epoch 610, training loss: 0.6468971967697144 = 0.025087712332606316 + 0.1 * 6.218094825744629
Epoch 610, val loss: 1.0412412881851196
Epoch 620, training loss: 0.645506739616394 = 0.023672739043831825 + 0.1 * 6.218339920043945
Epoch 620, val loss: 1.049450397491455
Epoch 630, training loss: 0.6472046375274658 = 0.022375887259840965 + 0.1 * 6.248287677764893
Epoch 630, val loss: 1.0575145483016968
Epoch 640, training loss: 0.6445567011833191 = 0.021193265914916992 + 0.1 * 6.233633995056152
Epoch 640, val loss: 1.0653481483459473
Epoch 650, training loss: 0.6412947773933411 = 0.02011014334857464 + 0.1 * 6.211846351623535
Epoch 650, val loss: 1.0730299949645996
Epoch 660, training loss: 0.6393306255340576 = 0.01911318488419056 + 0.1 * 6.202174186706543
Epoch 660, val loss: 1.0805201530456543
Epoch 670, training loss: 0.6381301879882812 = 0.01819029450416565 + 0.1 * 6.199398994445801
Epoch 670, val loss: 1.0878559350967407
Epoch 680, training loss: 0.6414936184883118 = 0.017333785071969032 + 0.1 * 6.241598129272461
Epoch 680, val loss: 1.095035433769226
Epoch 690, training loss: 0.6362438201904297 = 0.016542384400963783 + 0.1 * 6.197014331817627
Epoch 690, val loss: 1.1020617485046387
Epoch 700, training loss: 0.6354144811630249 = 0.01580924354493618 + 0.1 * 6.196052074432373
Epoch 700, val loss: 1.1089692115783691
Epoch 710, training loss: 0.6351488828659058 = 0.015126880258321762 + 0.1 * 6.200220108032227
Epoch 710, val loss: 1.1157057285308838
Epoch 720, training loss: 0.6332129240036011 = 0.014492428861558437 + 0.1 * 6.187204360961914
Epoch 720, val loss: 1.1222915649414062
Epoch 730, training loss: 0.6331511735916138 = 0.013899505138397217 + 0.1 * 6.192516326904297
Epoch 730, val loss: 1.1287251710891724
Epoch 740, training loss: 0.6319119930267334 = 0.013344560749828815 + 0.1 * 6.185673713684082
Epoch 740, val loss: 1.1349934339523315
Epoch 750, training loss: 0.6314685940742493 = 0.012827188707888126 + 0.1 * 6.186413764953613
Epoch 750, val loss: 1.14115571975708
Epoch 760, training loss: 0.6307650208473206 = 0.012341215275228024 + 0.1 * 6.184237957000732
Epoch 760, val loss: 1.147161841392517
Epoch 770, training loss: 0.6307202577590942 = 0.011885330080986023 + 0.1 * 6.18834924697876
Epoch 770, val loss: 1.1530680656433105
Epoch 780, training loss: 0.6298868656158447 = 0.011455822736024857 + 0.1 * 6.184309959411621
Epoch 780, val loss: 1.1588131189346313
Epoch 790, training loss: 0.6289560198783875 = 0.011051700450479984 + 0.1 * 6.179042816162109
Epoch 790, val loss: 1.1644595861434937
Epoch 800, training loss: 0.6287630200386047 = 0.010671443305909634 + 0.1 * 6.180915832519531
Epoch 800, val loss: 1.1700087785720825
Epoch 810, training loss: 0.6296898722648621 = 0.010312208905816078 + 0.1 * 6.193776607513428
Epoch 810, val loss: 1.1754355430603027
Epoch 820, training loss: 0.6273772120475769 = 0.009972824715077877 + 0.1 * 6.174044132232666
Epoch 820, val loss: 1.1806977987289429
Epoch 830, training loss: 0.6267758011817932 = 0.009652741253376007 + 0.1 * 6.171230316162109
Epoch 830, val loss: 1.1859525442123413
Epoch 840, training loss: 0.6266523599624634 = 0.009348070248961449 + 0.1 * 6.1730427742004395
Epoch 840, val loss: 1.1910593509674072
Epoch 850, training loss: 0.6257258653640747 = 0.00905989482998848 + 0.1 * 6.166659355163574
Epoch 850, val loss: 1.1960914134979248
Epoch 860, training loss: 0.6254773736000061 = 0.008786795660853386 + 0.1 * 6.166905879974365
Epoch 860, val loss: 1.2010619640350342
Epoch 870, training loss: 0.6248152256011963 = 0.008526629768311977 + 0.1 * 6.162886142730713
Epoch 870, val loss: 1.205942988395691
Epoch 880, training loss: 0.6238542795181274 = 0.008279488421976566 + 0.1 * 6.155747890472412
Epoch 880, val loss: 1.2107412815093994
Epoch 890, training loss: 0.6248804926872253 = 0.008044088259339333 + 0.1 * 6.16836404800415
Epoch 890, val loss: 1.2154486179351807
Epoch 900, training loss: 0.6231186985969543 = 0.007820024155080318 + 0.1 * 6.152987003326416
Epoch 900, val loss: 1.2200294733047485
Epoch 910, training loss: 0.6243468523025513 = 0.0076070548966526985 + 0.1 * 6.167397975921631
Epoch 910, val loss: 1.2245864868164062
Epoch 920, training loss: 0.6222371459007263 = 0.0074036759324371815 + 0.1 * 6.148334503173828
Epoch 920, val loss: 1.2290277481079102
Epoch 930, training loss: 0.6227420568466187 = 0.0072097815573215485 + 0.1 * 6.155322551727295
Epoch 930, val loss: 1.233429193496704
Epoch 940, training loss: 0.623123824596405 = 0.007023634854704142 + 0.1 * 6.161001682281494
Epoch 940, val loss: 1.237736701965332
Epoch 950, training loss: 0.6212905049324036 = 0.0068461233749985695 + 0.1 * 6.144443511962891
Epoch 950, val loss: 1.2419629096984863
Epoch 960, training loss: 0.6210592985153198 = 0.0066758920438587666 + 0.1 * 6.143834114074707
Epoch 960, val loss: 1.2461581230163574
Epoch 970, training loss: 0.6212620735168457 = 0.00651198485866189 + 0.1 * 6.147500991821289
Epoch 970, val loss: 1.2502403259277344
Epoch 980, training loss: 0.6202303171157837 = 0.00635563675314188 + 0.1 * 6.138746738433838
Epoch 980, val loss: 1.2542794942855835
Epoch 990, training loss: 0.6202546954154968 = 0.006206256803125143 + 0.1 * 6.14048433303833
Epoch 990, val loss: 1.258310317993164
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.7934
Flip ASR: 0.7511/225 nodes
The final ASR:0.71587, 0.12829, Accuracy:0.80494, 0.02014
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11540])
remove edge: torch.Size([2, 9592])
updated graph: torch.Size([2, 10576])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00348, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7733898162841797 = 1.936001181602478 + 0.1 * 8.373885154724121
Epoch 0, val loss: 1.9336292743682861
Epoch 10, training loss: 2.7638189792633057 = 1.9264401197433472 + 0.1 * 8.373788833618164
Epoch 10, val loss: 1.9246389865875244
Epoch 20, training loss: 2.751904010772705 = 1.9145686626434326 + 0.1 * 8.373353958129883
Epoch 20, val loss: 1.9129101037979126
Epoch 30, training loss: 2.734748125076294 = 1.8976624011993408 + 0.1 * 8.370857238769531
Epoch 30, val loss: 1.8960275650024414
Epoch 40, training loss: 2.708189010620117 = 1.8727331161499023 + 0.1 * 8.354557991027832
Epoch 40, val loss: 1.8715901374816895
Epoch 50, training loss: 2.6637649536132812 = 1.8383713960647583 + 0.1 * 8.253934860229492
Epoch 50, val loss: 1.8398349285125732
Epoch 60, training loss: 2.57069993019104 = 1.7999032735824585 + 0.1 * 7.707965850830078
Epoch 60, val loss: 1.8068522214889526
Epoch 70, training loss: 2.4973342418670654 = 1.7648229598999023 + 0.1 * 7.325112819671631
Epoch 70, val loss: 1.7779945135116577
Epoch 80, training loss: 2.432994842529297 = 1.7256032228469849 + 0.1 * 7.073917388916016
Epoch 80, val loss: 1.7447564601898193
Epoch 90, training loss: 2.3683547973632812 = 1.6743215322494507 + 0.1 * 6.940333366394043
Epoch 90, val loss: 1.700385332107544
Epoch 100, training loss: 2.2927627563476562 = 1.6068753004074097 + 0.1 * 6.8588738441467285
Epoch 100, val loss: 1.6430214643478394
Epoch 110, training loss: 2.2049572467803955 = 1.5245596170425415 + 0.1 * 6.803976058959961
Epoch 110, val loss: 1.5735033750534058
Epoch 120, training loss: 2.1107358932495117 = 1.4340806007385254 + 0.1 * 6.7665534019470215
Epoch 120, val loss: 1.4967485666275024
Epoch 130, training loss: 2.0142903327941895 = 1.3396391868591309 + 0.1 * 6.746511459350586
Epoch 130, val loss: 1.4178017377853394
Epoch 140, training loss: 1.9152443408966064 = 1.2419105768203735 + 0.1 * 6.733337879180908
Epoch 140, val loss: 1.3381216526031494
Epoch 150, training loss: 1.8137013912200928 = 1.1414804458618164 + 0.1 * 6.722209453582764
Epoch 150, val loss: 1.2573773860931396
Epoch 160, training loss: 1.7113282680511475 = 1.0402904748916626 + 0.1 * 6.710377216339111
Epoch 160, val loss: 1.178066372871399
Epoch 170, training loss: 1.6111605167388916 = 0.9412844181060791 + 0.1 * 6.698760509490967
Epoch 170, val loss: 1.1018401384353638
Epoch 180, training loss: 1.5171680450439453 = 0.8484420776367188 + 0.1 * 6.687258720397949
Epoch 180, val loss: 1.031873106956482
Epoch 190, training loss: 1.4325239658355713 = 0.7645986080169678 + 0.1 * 6.679252624511719
Epoch 190, val loss: 0.9705377817153931
Epoch 200, training loss: 1.3582360744476318 = 0.6914714574813843 + 0.1 * 6.667645454406738
Epoch 200, val loss: 0.9190213680267334
Epoch 210, training loss: 1.2939531803131104 = 0.6280558109283447 + 0.1 * 6.658973217010498
Epoch 210, val loss: 0.8761112093925476
Epoch 220, training loss: 1.2375320196151733 = 0.572711169719696 + 0.1 * 6.648208141326904
Epoch 220, val loss: 0.8402159214019775
Epoch 230, training loss: 1.187964916229248 = 0.523881733417511 + 0.1 * 6.640830993652344
Epoch 230, val loss: 0.8099178075790405
Epoch 240, training loss: 1.1435775756835938 = 0.48042917251586914 + 0.1 * 6.631484031677246
Epoch 240, val loss: 0.7845644354820251
Epoch 250, training loss: 1.1030175685882568 = 0.44100359082221985 + 0.1 * 6.620140075683594
Epoch 250, val loss: 0.7631688117980957
Epoch 260, training loss: 1.0663931369781494 = 0.404570072889328 + 0.1 * 6.61823034286499
Epoch 260, val loss: 0.7451608777046204
Epoch 270, training loss: 1.0306432247161865 = 0.37055903673171997 + 0.1 * 6.600842475891113
Epoch 270, val loss: 0.7302284240722656
Epoch 280, training loss: 0.9973817467689514 = 0.3384307622909546 + 0.1 * 6.589509963989258
Epoch 280, val loss: 0.7178248167037964
Epoch 290, training loss: 0.9664217829704285 = 0.30820757150650024 + 0.1 * 6.582141876220703
Epoch 290, val loss: 0.7078625559806824
Epoch 300, training loss: 0.9369280338287354 = 0.27984967827796936 + 0.1 * 6.570783615112305
Epoch 300, val loss: 0.7001129984855652
Epoch 310, training loss: 0.9111641645431519 = 0.25343868136405945 + 0.1 * 6.577254772186279
Epoch 310, val loss: 0.69464111328125
Epoch 320, training loss: 0.8851029872894287 = 0.22916559875011444 + 0.1 * 6.55937385559082
Epoch 320, val loss: 0.6912546157836914
Epoch 330, training loss: 0.8615849018096924 = 0.2066824734210968 + 0.1 * 6.549023628234863
Epoch 330, val loss: 0.6897520422935486
Epoch 340, training loss: 0.840103030204773 = 0.185886949300766 + 0.1 * 6.542160987854004
Epoch 340, val loss: 0.6899620890617371
Epoch 350, training loss: 0.8202991485595703 = 0.166704922914505 + 0.1 * 6.535942077636719
Epoch 350, val loss: 0.6914999485015869
Epoch 360, training loss: 0.8018349409103394 = 0.14911124110221863 + 0.1 * 6.527236461639404
Epoch 360, val loss: 0.6944074630737305
Epoch 370, training loss: 0.7857512831687927 = 0.13313663005828857 + 0.1 * 6.526146411895752
Epoch 370, val loss: 0.6984773874282837
Epoch 380, training loss: 0.7705609202384949 = 0.1188773512840271 + 0.1 * 6.516835689544678
Epoch 380, val loss: 0.7033805847167969
Epoch 390, training loss: 0.7572781443595886 = 0.10617164522409439 + 0.1 * 6.511064529418945
Epoch 390, val loss: 0.709261417388916
Epoch 400, training loss: 0.7449229955673218 = 0.09488978981971741 + 0.1 * 6.500332355499268
Epoch 400, val loss: 0.7156682014465332
Epoch 410, training loss: 0.7352402210235596 = 0.08489543944597244 + 0.1 * 6.503447532653809
Epoch 410, val loss: 0.7225464582443237
Epoch 420, training loss: 0.7247831225395203 = 0.07610631734132767 + 0.1 * 6.486767768859863
Epoch 420, val loss: 0.7297496199607849
Epoch 430, training loss: 0.716063916683197 = 0.0683663934469223 + 0.1 * 6.476974964141846
Epoch 430, val loss: 0.7372453212738037
Epoch 440, training loss: 0.7085920572280884 = 0.06154373288154602 + 0.1 * 6.470483303070068
Epoch 440, val loss: 0.7450437545776367
Epoch 450, training loss: 0.702920138835907 = 0.05554152652621269 + 0.1 * 6.473786354064941
Epoch 450, val loss: 0.7529915571212769
Epoch 460, training loss: 0.6972143650054932 = 0.05028031766414642 + 0.1 * 6.4693403244018555
Epoch 460, val loss: 0.7611098289489746
Epoch 470, training loss: 0.6910029649734497 = 0.045673444867134094 + 0.1 * 6.4532952308654785
Epoch 470, val loss: 0.7693526148796082
Epoch 480, training loss: 0.6860381364822388 = 0.04161699116230011 + 0.1 * 6.444211483001709
Epoch 480, val loss: 0.7775973081588745
Epoch 490, training loss: 0.6821936964988708 = 0.03803443908691406 + 0.1 * 6.441592693328857
Epoch 490, val loss: 0.785873532295227
Epoch 500, training loss: 0.6782869100570679 = 0.03487010300159454 + 0.1 * 6.434167861938477
Epoch 500, val loss: 0.7941724061965942
Epoch 510, training loss: 0.674765408039093 = 0.03207198157906532 + 0.1 * 6.426933765411377
Epoch 510, val loss: 0.8024044632911682
Epoch 520, training loss: 0.6718343496322632 = 0.02958802506327629 + 0.1 * 6.4224629402160645
Epoch 520, val loss: 0.8105065822601318
Epoch 530, training loss: 0.6690111756324768 = 0.027385126799345016 + 0.1 * 6.416260719299316
Epoch 530, val loss: 0.8183364868164062
Epoch 540, training loss: 0.666591227054596 = 0.025421207770705223 + 0.1 * 6.411700248718262
Epoch 540, val loss: 0.8261274695396423
Epoch 550, training loss: 0.6649035215377808 = 0.023662567138671875 + 0.1 * 6.41240930557251
Epoch 550, val loss: 0.8336710929870605
Epoch 560, training loss: 0.6619660258293152 = 0.022087691351771355 + 0.1 * 6.398783206939697
Epoch 560, val loss: 0.8411093950271606
Epoch 570, training loss: 0.6614574790000916 = 0.020666399970650673 + 0.1 * 6.407910346984863
Epoch 570, val loss: 0.8482903242111206
Epoch 580, training loss: 0.6582878232002258 = 0.019388563930988312 + 0.1 * 6.388992786407471
Epoch 580, val loss: 0.8551902174949646
Epoch 590, training loss: 0.6566025614738464 = 0.018236655741930008 + 0.1 * 6.3836588859558105
Epoch 590, val loss: 0.8620982766151428
Epoch 600, training loss: 0.6553168296813965 = 0.017187872901558876 + 0.1 * 6.381289005279541
Epoch 600, val loss: 0.8687061071395874
Epoch 610, training loss: 0.6529482007026672 = 0.01623358577489853 + 0.1 * 6.367146015167236
Epoch 610, val loss: 0.8751579523086548
Epoch 620, training loss: 0.6536248326301575 = 0.015360034070909023 + 0.1 * 6.382647514343262
Epoch 620, val loss: 0.8814137578010559
Epoch 630, training loss: 0.6498331427574158 = 0.01456163078546524 + 0.1 * 6.352715015411377
Epoch 630, val loss: 0.8875856995582581
Epoch 640, training loss: 0.6496409773826599 = 0.01383066177368164 + 0.1 * 6.358103275299072
Epoch 640, val loss: 0.8936330080032349
Epoch 650, training loss: 0.6478879451751709 = 0.013155815191566944 + 0.1 * 6.347321033477783
Epoch 650, val loss: 0.8994743824005127
Epoch 660, training loss: 0.6482987999916077 = 0.01253254059702158 + 0.1 * 6.357662677764893
Epoch 660, val loss: 0.905127763748169
Epoch 670, training loss: 0.6458673477172852 = 0.011959201656281948 + 0.1 * 6.339081287384033
Epoch 670, val loss: 0.9107155799865723
Epoch 680, training loss: 0.6443752646446228 = 0.011429611593484879 + 0.1 * 6.329456329345703
Epoch 680, val loss: 0.9162207245826721
Epoch 690, training loss: 0.6448851823806763 = 0.010936344042420387 + 0.1 * 6.339488506317139
Epoch 690, val loss: 0.9214820265769958
Epoch 700, training loss: 0.6430132389068604 = 0.010479354299604893 + 0.1 * 6.325338840484619
Epoch 700, val loss: 0.9266313314437866
Epoch 710, training loss: 0.6429967284202576 = 0.01005450077354908 + 0.1 * 6.3294219970703125
Epoch 710, val loss: 0.9317386746406555
Epoch 720, training loss: 0.6413459777832031 = 0.009656606242060661 + 0.1 * 6.316893577575684
Epoch 720, val loss: 0.9365533590316772
Epoch 730, training loss: 0.6407543420791626 = 0.009286340326070786 + 0.1 * 6.314680099487305
Epoch 730, val loss: 0.9413586258888245
Epoch 740, training loss: 0.6402422785758972 = 0.008937864564359188 + 0.1 * 6.313044548034668
Epoch 740, val loss: 0.9460583925247192
Epoch 750, training loss: 0.6401759386062622 = 0.008611765690147877 + 0.1 * 6.3156418800354
Epoch 750, val loss: 0.9506322741508484
Epoch 760, training loss: 0.6380333304405212 = 0.008306033909320831 + 0.1 * 6.297272682189941
Epoch 760, val loss: 0.9551647305488586
Epoch 770, training loss: 0.6371789574623108 = 0.008018597960472107 + 0.1 * 6.291603088378906
Epoch 770, val loss: 0.959632158279419
Epoch 780, training loss: 0.6387364864349365 = 0.007746533956378698 + 0.1 * 6.309899806976318
Epoch 780, val loss: 0.9639190435409546
Epoch 790, training loss: 0.6357146501541138 = 0.00749099999666214 + 0.1 * 6.282236576080322
Epoch 790, val loss: 0.9681180715560913
Epoch 800, training loss: 0.6360616087913513 = 0.0072501725517213345 + 0.1 * 6.288114070892334
Epoch 800, val loss: 0.97227942943573
Epoch 810, training loss: 0.6346458196640015 = 0.007021571509540081 + 0.1 * 6.276242733001709
Epoch 810, val loss: 0.9763472676277161
Epoch 820, training loss: 0.6338720917701721 = 0.006805909797549248 + 0.1 * 6.2706618309021
Epoch 820, val loss: 0.9803286790847778
Epoch 830, training loss: 0.6345575451850891 = 0.006600797176361084 + 0.1 * 6.279567241668701
Epoch 830, val loss: 0.9842191338539124
Epoch 840, training loss: 0.6338340640068054 = 0.006406191270798445 + 0.1 * 6.27427864074707
Epoch 840, val loss: 0.9880557656288147
Epoch 850, training loss: 0.6325457692146301 = 0.006220837589353323 + 0.1 * 6.263249397277832
Epoch 850, val loss: 0.991706132888794
Epoch 860, training loss: 0.6326595544815063 = 0.006046242546290159 + 0.1 * 6.2661333084106445
Epoch 860, val loss: 0.9954752326011658
Epoch 870, training loss: 0.6322888135910034 = 0.005879661533981562 + 0.1 * 6.264091491699219
Epoch 870, val loss: 0.9991389513015747
Epoch 880, training loss: 0.6313174962997437 = 0.005720313172787428 + 0.1 * 6.255971908569336
Epoch 880, val loss: 1.0026531219482422
Epoch 890, training loss: 0.6313528418540955 = 0.005568433087319136 + 0.1 * 6.257843971252441
Epoch 890, val loss: 1.0061287879943848
Epoch 900, training loss: 0.6321471333503723 = 0.005423254333436489 + 0.1 * 6.267238616943359
Epoch 900, val loss: 1.0096014738082886
Epoch 910, training loss: 0.6305420994758606 = 0.005284193903207779 + 0.1 * 6.252579212188721
Epoch 910, val loss: 1.0128484964370728
Epoch 920, training loss: 0.6314359903335571 = 0.005152028053998947 + 0.1 * 6.262839317321777
Epoch 920, val loss: 1.0162001848220825
Epoch 930, training loss: 0.6298993229866028 = 0.005025585647672415 + 0.1 * 6.248737335205078
Epoch 930, val loss: 1.0194168090820312
Epoch 940, training loss: 0.6292139291763306 = 0.004905189853161573 + 0.1 * 6.2430877685546875
Epoch 940, val loss: 1.022603988647461
Epoch 950, training loss: 0.6297088861465454 = 0.004789124242961407 + 0.1 * 6.249197483062744
Epoch 950, val loss: 1.0257315635681152
Epoch 960, training loss: 0.6285381317138672 = 0.00467805378139019 + 0.1 * 6.238600730895996
Epoch 960, val loss: 1.028772234916687
Epoch 970, training loss: 0.6280530691146851 = 0.00457131490111351 + 0.1 * 6.2348175048828125
Epoch 970, val loss: 1.0318187475204468
Epoch 980, training loss: 0.6282954812049866 = 0.004468619357794523 + 0.1 * 6.2382683753967285
Epoch 980, val loss: 1.0347962379455566
Epoch 990, training loss: 0.6275357604026794 = 0.004370547831058502 + 0.1 * 6.231651782989502
Epoch 990, val loss: 1.0377705097198486
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7196
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7904837131500244 = 1.953097939491272 + 0.1 * 8.373858451843262
Epoch 0, val loss: 1.950287103652954
Epoch 10, training loss: 2.780155897140503 = 1.942782998085022 + 0.1 * 8.37372875213623
Epoch 10, val loss: 1.9399515390396118
Epoch 20, training loss: 2.767428398132324 = 1.9301183223724365 + 0.1 * 8.373100280761719
Epoch 20, val loss: 1.9273096323013306
Epoch 30, training loss: 2.749462366104126 = 1.91255521774292 + 0.1 * 8.369071960449219
Epoch 30, val loss: 1.9097685813903809
Epoch 40, training loss: 2.721001148223877 = 1.8867547512054443 + 0.1 * 8.342462539672852
Epoch 40, val loss: 1.8843880891799927
Epoch 50, training loss: 2.6628105640411377 = 1.8506333827972412 + 0.1 * 8.121771812438965
Epoch 50, val loss: 1.8502956628799438
Epoch 60, training loss: 2.573237419128418 = 1.8111889362335205 + 0.1 * 7.620484352111816
Epoch 60, val loss: 1.8146123886108398
Epoch 70, training loss: 2.4927237033843994 = 1.7726694345474243 + 0.1 * 7.200543403625488
Epoch 70, val loss: 1.7809571027755737
Epoch 80, training loss: 2.427659273147583 = 1.7344706058502197 + 0.1 * 6.931886672973633
Epoch 80, val loss: 1.7477960586547852
Epoch 90, training loss: 2.3662941455841064 = 1.6855864524841309 + 0.1 * 6.8070759773254395
Epoch 90, val loss: 1.703170895576477
Epoch 100, training loss: 2.292652130126953 = 1.618455410003662 + 0.1 * 6.741966724395752
Epoch 100, val loss: 1.6427921056747437
Epoch 110, training loss: 2.2014007568359375 = 1.5315780639648438 + 0.1 * 6.6982269287109375
Epoch 110, val loss: 1.5678694248199463
Epoch 120, training loss: 2.0977137088775635 = 1.4308176040649414 + 0.1 * 6.668960094451904
Epoch 120, val loss: 1.483059287071228
Epoch 130, training loss: 1.9905774593353271 = 1.326187252998352 + 0.1 * 6.64390230178833
Epoch 130, val loss: 1.398210048675537
Epoch 140, training loss: 1.8866238594055176 = 1.2245409488677979 + 0.1 * 6.620828151702881
Epoch 140, val loss: 1.3183846473693848
Epoch 150, training loss: 1.7906361818313599 = 1.1303269863128662 + 0.1 * 6.603091716766357
Epoch 150, val loss: 1.2466480731964111
Epoch 160, training loss: 1.7003204822540283 = 1.0418134927749634 + 0.1 * 6.58506965637207
Epoch 160, val loss: 1.1804920434951782
Epoch 170, training loss: 1.6159367561340332 = 0.9581533670425415 + 0.1 * 6.577834606170654
Epoch 170, val loss: 1.1180261373519897
Epoch 180, training loss: 1.5368890762329102 = 0.8806366920471191 + 0.1 * 6.562524318695068
Epoch 180, val loss: 1.0603877305984497
Epoch 190, training loss: 1.4631996154785156 = 0.808221161365509 + 0.1 * 6.549785137176514
Epoch 190, val loss: 1.0066032409667969
Epoch 200, training loss: 1.396561861038208 = 0.7405779361724854 + 0.1 * 6.559839248657227
Epoch 200, val loss: 0.9575051665306091
Epoch 210, training loss: 1.3328707218170166 = 0.6792011260986328 + 0.1 * 6.536696434020996
Epoch 210, val loss: 0.9149084091186523
Epoch 220, training loss: 1.2754936218261719 = 0.6229913234710693 + 0.1 * 6.525022983551025
Epoch 220, val loss: 0.8786261677742004
Epoch 230, training loss: 1.2231508493423462 = 0.5716378092765808 + 0.1 * 6.515130519866943
Epoch 230, val loss: 0.8489776849746704
Epoch 240, training loss: 1.1787452697753906 = 0.5250590443611145 + 0.1 * 6.536862373352051
Epoch 240, val loss: 0.8257741928100586
Epoch 250, training loss: 1.1334580183029175 = 0.48364129662513733 + 0.1 * 6.498167514801025
Epoch 250, val loss: 0.808800458908081
Epoch 260, training loss: 1.0951002836227417 = 0.445873498916626 + 0.1 * 6.492267608642578
Epoch 260, val loss: 0.796064019203186
Epoch 270, training loss: 1.0590052604675293 = 0.4108179807662964 + 0.1 * 6.481873512268066
Epoch 270, val loss: 0.7865713834762573
Epoch 280, training loss: 1.0252383947372437 = 0.37797239422798157 + 0.1 * 6.472659587860107
Epoch 280, val loss: 0.7792806029319763
Epoch 290, training loss: 0.9938850402832031 = 0.347115695476532 + 0.1 * 6.467693328857422
Epoch 290, val loss: 0.7735821604728699
Epoch 300, training loss: 0.9642836451530457 = 0.3177003860473633 + 0.1 * 6.465832710266113
Epoch 300, val loss: 0.768811047077179
Epoch 310, training loss: 0.9344291090965271 = 0.28936153650283813 + 0.1 * 6.4506754875183105
Epoch 310, val loss: 0.7644454836845398
Epoch 320, training loss: 0.9055641293525696 = 0.2616906762123108 + 0.1 * 6.438734531402588
Epoch 320, val loss: 0.760443389415741
Epoch 330, training loss: 0.880916178226471 = 0.23465488851070404 + 0.1 * 6.462612628936768
Epoch 330, val loss: 0.7567645311355591
Epoch 340, training loss: 0.8521095514297485 = 0.20892630517482758 + 0.1 * 6.431832313537598
Epoch 340, val loss: 0.7536441683769226
Epoch 350, training loss: 0.8266507387161255 = 0.18502214550971985 + 0.1 * 6.416285514831543
Epoch 350, val loss: 0.7516149878501892
Epoch 360, training loss: 0.8055871725082397 = 0.16356858611106873 + 0.1 * 6.420185565948486
Epoch 360, val loss: 0.7510864734649658
Epoch 370, training loss: 0.7855648994445801 = 0.14494925737380981 + 0.1 * 6.406156539916992
Epoch 370, val loss: 0.7522021532058716
Epoch 380, training loss: 0.7689999341964722 = 0.1290706992149353 + 0.1 * 6.3992919921875
Epoch 380, val loss: 0.7549734711647034
Epoch 390, training loss: 0.7546501159667969 = 0.1155758649110794 + 0.1 * 6.390742778778076
Epoch 390, val loss: 0.7592369914054871
Epoch 400, training loss: 0.7425273060798645 = 0.10406623780727386 + 0.1 * 6.384610652923584
Epoch 400, val loss: 0.764757513999939
Epoch 410, training loss: 0.731810450553894 = 0.09417875856161118 + 0.1 * 6.376317024230957
Epoch 410, val loss: 0.7713555097579956
Epoch 420, training loss: 0.7227314710617065 = 0.08560232818126678 + 0.1 * 6.371291160583496
Epoch 420, val loss: 0.7789355516433716
Epoch 430, training loss: 0.7151656150817871 = 0.07811170071363449 + 0.1 * 6.37053918838501
Epoch 430, val loss: 0.7873093485832214
Epoch 440, training loss: 0.7067457437515259 = 0.07153133302927017 + 0.1 * 6.35214376449585
Epoch 440, val loss: 0.796324610710144
Epoch 450, training loss: 0.7004275321960449 = 0.06571561843156815 + 0.1 * 6.347118854522705
Epoch 450, val loss: 0.8058829307556152
Epoch 460, training loss: 0.6949359178543091 = 0.0605461411178112 + 0.1 * 6.343897342681885
Epoch 460, val loss: 0.8158630728721619
Epoch 470, training loss: 0.6894703507423401 = 0.055927518755197525 + 0.1 * 6.335428237915039
Epoch 470, val loss: 0.8261722326278687
Epoch 480, training loss: 0.6850230693817139 = 0.05178355798125267 + 0.1 * 6.332395076751709
Epoch 480, val loss: 0.8367238640785217
Epoch 490, training loss: 0.6805082559585571 = 0.048052940517663956 + 0.1 * 6.3245530128479
Epoch 490, val loss: 0.8473609089851379
Epoch 500, training loss: 0.6762572526931763 = 0.04468837380409241 + 0.1 * 6.315688610076904
Epoch 500, val loss: 0.8580036759376526
Epoch 510, training loss: 0.6727164387702942 = 0.04163723438978195 + 0.1 * 6.310791969299316
Epoch 510, val loss: 0.8686419725418091
Epoch 520, training loss: 0.6710405945777893 = 0.038867369294166565 + 0.1 * 6.321732044219971
Epoch 520, val loss: 0.8791351318359375
Epoch 530, training loss: 0.6668354868888855 = 0.036361657083034515 + 0.1 * 6.304738521575928
Epoch 530, val loss: 0.88941890001297
Epoch 540, training loss: 0.6630717515945435 = 0.034077420830726624 + 0.1 * 6.289943218231201
Epoch 540, val loss: 0.899622917175293
Epoch 550, training loss: 0.6616832613945007 = 0.03198744356632233 + 0.1 * 6.2969584465026855
Epoch 550, val loss: 0.909690797328949
Epoch 560, training loss: 0.6579106450080872 = 0.03007684461772442 + 0.1 * 6.278337478637695
Epoch 560, val loss: 0.9195823669433594
Epoch 570, training loss: 0.6565642356872559 = 0.028322430327534676 + 0.1 * 6.2824177742004395
Epoch 570, val loss: 0.9293098449707031
Epoch 580, training loss: 0.6540988087654114 = 0.02671254798769951 + 0.1 * 6.273862838745117
Epoch 580, val loss: 0.9387639760971069
Epoch 590, training loss: 0.652746319770813 = 0.02523372881114483 + 0.1 * 6.275125503540039
Epoch 590, val loss: 0.9479935169219971
Epoch 600, training loss: 0.6504996418952942 = 0.02387031726539135 + 0.1 * 6.266293525695801
Epoch 600, val loss: 0.9570598602294922
Epoch 610, training loss: 0.6500838398933411 = 0.022610142827033997 + 0.1 * 6.2747368812561035
Epoch 610, val loss: 0.9659206867218018
Epoch 620, training loss: 0.6474528908729553 = 0.02144908532500267 + 0.1 * 6.260037899017334
Epoch 620, val loss: 0.9744411706924438
Epoch 630, training loss: 0.6455965042114258 = 0.02037528157234192 + 0.1 * 6.2522125244140625
Epoch 630, val loss: 0.9827737212181091
Epoch 640, training loss: 0.6461279988288879 = 0.01937784254550934 + 0.1 * 6.267501354217529
Epoch 640, val loss: 0.9909775257110596
Epoch 650, training loss: 0.6433579921722412 = 0.01845494844019413 + 0.1 * 6.249030113220215
Epoch 650, val loss: 0.9988443851470947
Epoch 660, training loss: 0.6418662071228027 = 0.01759728230535984 + 0.1 * 6.24268913269043
Epoch 660, val loss: 1.0066224336624146
Epoch 670, training loss: 0.6409530639648438 = 0.016797691583633423 + 0.1 * 6.241553783416748
Epoch 670, val loss: 1.014182448387146
Epoch 680, training loss: 0.6396297216415405 = 0.016054168343544006 + 0.1 * 6.235755443572998
Epoch 680, val loss: 1.0215290784835815
Epoch 690, training loss: 0.6397027969360352 = 0.015361147932708263 + 0.1 * 6.243416786193848
Epoch 690, val loss: 1.0286855697631836
Epoch 700, training loss: 0.6378383040428162 = 0.014713013544678688 + 0.1 * 6.231253147125244
Epoch 700, val loss: 1.0356864929199219
Epoch 710, training loss: 0.6368768215179443 = 0.014106497168540955 + 0.1 * 6.227703094482422
Epoch 710, val loss: 1.042508840560913
Epoch 720, training loss: 0.6369513273239136 = 0.013536880724132061 + 0.1 * 6.23414421081543
Epoch 720, val loss: 1.0492178201675415
Epoch 730, training loss: 0.6349061727523804 = 0.013004017062485218 + 0.1 * 6.219021797180176
Epoch 730, val loss: 1.0557329654693604
Epoch 740, training loss: 0.6373579502105713 = 0.01250384096056223 + 0.1 * 6.248540878295898
Epoch 740, val loss: 1.0621675252914429
Epoch 750, training loss: 0.6344168782234192 = 0.012033907696604729 + 0.1 * 6.223829746246338
Epoch 750, val loss: 1.0683794021606445
Epoch 760, training loss: 0.6337289214134216 = 0.011592842638492584 + 0.1 * 6.221360683441162
Epoch 760, val loss: 1.074522614479065
Epoch 770, training loss: 0.6324543952941895 = 0.011175677180290222 + 0.1 * 6.212786674499512
Epoch 770, val loss: 1.0804808139801025
Epoch 780, training loss: 0.6315444707870483 = 0.010783376172184944 + 0.1 * 6.207610607147217
Epoch 780, val loss: 1.0863386392593384
Epoch 790, training loss: 0.6314428448677063 = 0.010411199182271957 + 0.1 * 6.210316181182861
Epoch 790, val loss: 1.092090129852295
Epoch 800, training loss: 0.6311492919921875 = 0.010059724561870098 + 0.1 * 6.210895538330078
Epoch 800, val loss: 1.0976715087890625
Epoch 810, training loss: 0.6301547884941101 = 0.009727218188345432 + 0.1 * 6.204275608062744
Epoch 810, val loss: 1.103187084197998
Epoch 820, training loss: 0.62932950258255 = 0.009412772953510284 + 0.1 * 6.199167251586914
Epoch 820, val loss: 1.1086227893829346
Epoch 830, training loss: 0.6290735602378845 = 0.009113484993577003 + 0.1 * 6.199600696563721
Epoch 830, val loss: 1.1139386892318726
Epoch 840, training loss: 0.6285184621810913 = 0.008830999955534935 + 0.1 * 6.196874141693115
Epoch 840, val loss: 1.1191754341125488
Epoch 850, training loss: 0.6292334198951721 = 0.00856201071292162 + 0.1 * 6.206713676452637
Epoch 850, val loss: 1.1243565082550049
Epoch 860, training loss: 0.6272901296615601 = 0.008306173607707024 + 0.1 * 6.189839839935303
Epoch 860, val loss: 1.129414439201355
Epoch 870, training loss: 0.6286886930465698 = 0.008063029497861862 + 0.1 * 6.206256866455078
Epoch 870, val loss: 1.134432077407837
Epoch 880, training loss: 0.6266698837280273 = 0.007831419818103313 + 0.1 * 6.188385009765625
Epoch 880, val loss: 1.1392927169799805
Epoch 890, training loss: 0.6258792877197266 = 0.007611074019223452 + 0.1 * 6.182681560516357
Epoch 890, val loss: 1.144141435623169
Epoch 900, training loss: 0.6265453696250916 = 0.0074000488966703415 + 0.1 * 6.191452980041504
Epoch 900, val loss: 1.1489431858062744
Epoch 910, training loss: 0.6261865496635437 = 0.007197599392384291 + 0.1 * 6.189889430999756
Epoch 910, val loss: 1.1535676717758179
Epoch 920, training loss: 0.6253023147583008 = 0.00700599467381835 + 0.1 * 6.182962894439697
Epoch 920, val loss: 1.1581549644470215
Epoch 930, training loss: 0.6249566674232483 = 0.0068224286660552025 + 0.1 * 6.181342601776123
Epoch 930, val loss: 1.1626925468444824
Epoch 940, training loss: 0.6255419254302979 = 0.006646253168582916 + 0.1 * 6.188956260681152
Epoch 940, val loss: 1.1671020984649658
Epoch 950, training loss: 0.6244049668312073 = 0.006478286348283291 + 0.1 * 6.179266452789307
Epoch 950, val loss: 1.171437382698059
Epoch 960, training loss: 0.6234741806983948 = 0.006317439489066601 + 0.1 * 6.171567440032959
Epoch 960, val loss: 1.1757785081863403
Epoch 970, training loss: 0.6250734925270081 = 0.006162620149552822 + 0.1 * 6.189108371734619
Epoch 970, val loss: 1.1800248622894287
Epoch 980, training loss: 0.6236238479614258 = 0.006013940088450909 + 0.1 * 6.1760993003845215
Epoch 980, val loss: 1.184170126914978
Epoch 990, training loss: 0.6241808533668518 = 0.005871342495083809 + 0.1 * 6.1830949783325195
Epoch 990, val loss: 1.1883071660995483
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7232
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7731151580810547 = 1.9357333183288574 + 0.1 * 8.373819351196289
Epoch 0, val loss: 1.9386987686157227
Epoch 10, training loss: 2.7639970779418945 = 1.9266319274902344 + 0.1 * 8.373650550842285
Epoch 10, val loss: 1.9293832778930664
Epoch 20, training loss: 2.752687454223633 = 1.9154300689697266 + 0.1 * 8.372574806213379
Epoch 20, val loss: 1.9177829027175903
Epoch 30, training loss: 2.7362380027770996 = 1.899707555770874 + 0.1 * 8.365304946899414
Epoch 30, val loss: 1.901641845703125
Epoch 40, training loss: 2.708498477935791 = 1.876469373703003 + 0.1 * 8.320290565490723
Epoch 40, val loss: 1.8781647682189941
Epoch 50, training loss: 2.648890256881714 = 1.8446944952011108 + 0.1 * 8.041956901550293
Epoch 50, val loss: 1.847556233406067
Epoch 60, training loss: 2.571807384490967 = 1.8095518350601196 + 0.1 * 7.622555255889893
Epoch 60, val loss: 1.815362811088562
Epoch 70, training loss: 2.4895098209381104 = 1.7748017311096191 + 0.1 * 7.14708137512207
Epoch 70, val loss: 1.7838560342788696
Epoch 80, training loss: 2.427424430847168 = 1.7369279861450195 + 0.1 * 6.904964447021484
Epoch 80, val loss: 1.7504230737686157
Epoch 90, training loss: 2.3676204681396484 = 1.6885402202606201 + 0.1 * 6.790801048278809
Epoch 90, val loss: 1.7086091041564941
Epoch 100, training loss: 2.2973997592926025 = 1.6235169172286987 + 0.1 * 6.738828659057617
Epoch 100, val loss: 1.653842806816101
Epoch 110, training loss: 2.2084474563598633 = 1.537632703781128 + 0.1 * 6.70814847946167
Epoch 110, val loss: 1.5830451250076294
Epoch 120, training loss: 2.1007637977600098 = 1.432093858718872 + 0.1 * 6.6866984367370605
Epoch 120, val loss: 1.497623085975647
Epoch 130, training loss: 1.981527328491211 = 1.3147183656692505 + 0.1 * 6.668089389801025
Epoch 130, val loss: 1.403841257095337
Epoch 140, training loss: 1.8634693622589111 = 1.1985005140304565 + 0.1 * 6.649689197540283
Epoch 140, val loss: 1.3128283023834229
Epoch 150, training loss: 1.7570509910583496 = 1.093797206878662 + 0.1 * 6.632537841796875
Epoch 150, val loss: 1.2318464517593384
Epoch 160, training loss: 1.6625468730926514 = 1.001634120941162 + 0.1 * 6.609127998352051
Epoch 160, val loss: 1.160853624343872
Epoch 170, training loss: 1.5824291706085205 = 0.9226675629615784 + 0.1 * 6.597616195678711
Epoch 170, val loss: 1.1004389524459839
Epoch 180, training loss: 1.509354591369629 = 0.8514673113822937 + 0.1 * 6.5788726806640625
Epoch 180, val loss: 1.0460965633392334
Epoch 190, training loss: 1.4391112327575684 = 0.7831380367279053 + 0.1 * 6.559731960296631
Epoch 190, val loss: 0.9936567544937134
Epoch 200, training loss: 1.3708267211914062 = 0.7164500951766968 + 0.1 * 6.543765544891357
Epoch 200, val loss: 0.9427032470703125
Epoch 210, training loss: 1.3065111637115479 = 0.6519544720649719 + 0.1 * 6.545566082000732
Epoch 210, val loss: 0.8944146633148193
Epoch 220, training loss: 1.2433854341506958 = 0.5914186239242554 + 0.1 * 6.519668102264404
Epoch 220, val loss: 0.8513189554214478
Epoch 230, training loss: 1.1854593753814697 = 0.5348559617996216 + 0.1 * 6.5060343742370605
Epoch 230, val loss: 0.8140135407447815
Epoch 240, training loss: 1.133060097694397 = 0.482608824968338 + 0.1 * 6.504512310028076
Epoch 240, val loss: 0.7831562757492065
Epoch 250, training loss: 1.0835986137390137 = 0.4348631501197815 + 0.1 * 6.487354278564453
Epoch 250, val loss: 0.7588340640068054
Epoch 260, training loss: 1.0393059253692627 = 0.3914698362350464 + 0.1 * 6.478360176086426
Epoch 260, val loss: 0.7402384281158447
Epoch 270, training loss: 0.9992305040359497 = 0.35231998562812805 + 0.1 * 6.469104766845703
Epoch 270, val loss: 0.7266585230827332
Epoch 280, training loss: 0.9638183116912842 = 0.31721988320350647 + 0.1 * 6.465984344482422
Epoch 280, val loss: 0.7174891829490662
Epoch 290, training loss: 0.9316191077232361 = 0.2861306071281433 + 0.1 * 6.454885005950928
Epoch 290, val loss: 0.7122775316238403
Epoch 300, training loss: 0.9052860736846924 = 0.25852760672569275 + 0.1 * 6.46758508682251
Epoch 300, val loss: 0.7104731798171997
Epoch 310, training loss: 0.8778361678123474 = 0.23423899710178375 + 0.1 * 6.435971736907959
Epoch 310, val loss: 0.7117352485656738
Epoch 320, training loss: 0.8552054762840271 = 0.2126733660697937 + 0.1 * 6.425321102142334
Epoch 320, val loss: 0.7155855894088745
Epoch 330, training loss: 0.8367022275924683 = 0.19348131120204926 + 0.1 * 6.432209014892578
Epoch 330, val loss: 0.7218033075332642
Epoch 340, training loss: 0.8177709579467773 = 0.17656539380550385 + 0.1 * 6.412055492401123
Epoch 340, val loss: 0.7298029065132141
Epoch 350, training loss: 0.8021920919418335 = 0.1615876406431198 + 0.1 * 6.4060444831848145
Epoch 350, val loss: 0.7394595742225647
Epoch 360, training loss: 0.7879884243011475 = 0.14823918044567108 + 0.1 * 6.397491931915283
Epoch 360, val loss: 0.7503274083137512
Epoch 370, training loss: 0.7752900719642639 = 0.13634735345840454 + 0.1 * 6.389427185058594
Epoch 370, val loss: 0.7619957327842712
Epoch 380, training loss: 0.7640121579170227 = 0.125727117061615 + 0.1 * 6.382850170135498
Epoch 380, val loss: 0.7743966579437256
Epoch 390, training loss: 0.7535781264305115 = 0.11618994921445847 + 0.1 * 6.373881816864014
Epoch 390, val loss: 0.7871052622795105
Epoch 400, training loss: 0.7446199655532837 = 0.10759922116994858 + 0.1 * 6.370207786560059
Epoch 400, val loss: 0.8001341223716736
Epoch 410, training loss: 0.7354801297187805 = 0.09981448948383331 + 0.1 * 6.356656551361084
Epoch 410, val loss: 0.8134399652481079
Epoch 420, training loss: 0.7286341190338135 = 0.09274125844240189 + 0.1 * 6.358928680419922
Epoch 420, val loss: 0.8266814947128296
Epoch 430, training loss: 0.7206971645355225 = 0.08630889654159546 + 0.1 * 6.3438825607299805
Epoch 430, val loss: 0.8399502038955688
Epoch 440, training loss: 0.7141444087028503 = 0.08043103665113449 + 0.1 * 6.337133407592773
Epoch 440, val loss: 0.8532578945159912
Epoch 450, training loss: 0.7083272933959961 = 0.07502514868974686 + 0.1 * 6.333021640777588
Epoch 450, val loss: 0.8665329813957214
Epoch 460, training loss: 0.70311439037323 = 0.07005184143781662 + 0.1 * 6.330625057220459
Epoch 460, val loss: 0.87960284948349
Epoch 470, training loss: 0.6972839832305908 = 0.06547529250383377 + 0.1 * 6.318087100982666
Epoch 470, val loss: 0.8927832245826721
Epoch 480, training loss: 0.6924264430999756 = 0.06124019995331764 + 0.1 * 6.311862468719482
Epoch 480, val loss: 0.9057280421257019
Epoch 490, training loss: 0.6896297335624695 = 0.05732589587569237 + 0.1 * 6.323038101196289
Epoch 490, val loss: 0.9182601571083069
Epoch 500, training loss: 0.6844292879104614 = 0.0537167452275753 + 0.1 * 6.307125568389893
Epoch 500, val loss: 0.9308273792266846
Epoch 510, training loss: 0.6800267696380615 = 0.05036427453160286 + 0.1 * 6.296625137329102
Epoch 510, val loss: 0.9431878924369812
Epoch 520, training loss: 0.6775239706039429 = 0.04724988713860512 + 0.1 * 6.302740573883057
Epoch 520, val loss: 0.9552390575408936
Epoch 530, training loss: 0.6734591126441956 = 0.04436398670077324 + 0.1 * 6.290951251983643
Epoch 530, val loss: 0.9672088623046875
Epoch 540, training loss: 0.6708771586418152 = 0.041678719222545624 + 0.1 * 6.2919840812683105
Epoch 540, val loss: 0.9790153503417969
Epoch 550, training loss: 0.6687859892845154 = 0.03918513283133507 + 0.1 * 6.296008586883545
Epoch 550, val loss: 0.9904804229736328
Epoch 560, training loss: 0.6642132997512817 = 0.03687505051493645 + 0.1 * 6.273382186889648
Epoch 560, val loss: 1.0017942190170288
Epoch 570, training loss: 0.6616907715797424 = 0.03472880274057388 + 0.1 * 6.269619464874268
Epoch 570, val loss: 1.0129690170288086
Epoch 580, training loss: 0.6592018008232117 = 0.03273182362318039 + 0.1 * 6.264699459075928
Epoch 580, val loss: 1.023926854133606
Epoch 590, training loss: 0.6572564244270325 = 0.030875174328684807 + 0.1 * 6.26381254196167
Epoch 590, val loss: 1.0345509052276611
Epoch 600, training loss: 0.6549280285835266 = 0.029154658317565918 + 0.1 * 6.2577338218688965
Epoch 600, val loss: 1.0452303886413574
Epoch 610, training loss: 0.6539443731307983 = 0.027556372806429863 + 0.1 * 6.263879776000977
Epoch 610, val loss: 1.0556201934814453
Epoch 620, training loss: 0.6529996395111084 = 0.02607397735118866 + 0.1 * 6.269256591796875
Epoch 620, val loss: 1.0657614469528198
Epoch 630, training loss: 0.6503493785858154 = 0.024700745940208435 + 0.1 * 6.256485939025879
Epoch 630, val loss: 1.075797200202942
Epoch 640, training loss: 0.6474185585975647 = 0.023427335545420647 + 0.1 * 6.239912509918213
Epoch 640, val loss: 1.0856835842132568
Epoch 650, training loss: 0.646897554397583 = 0.022242724895477295 + 0.1 * 6.246548175811768
Epoch 650, val loss: 1.095341682434082
Epoch 660, training loss: 0.6480059623718262 = 0.02114148624241352 + 0.1 * 6.2686448097229
Epoch 660, val loss: 1.1047557592391968
Epoch 670, training loss: 0.6438474655151367 = 0.02012256160378456 + 0.1 * 6.237248420715332
Epoch 670, val loss: 1.1139732599258423
Epoch 680, training loss: 0.6420579552650452 = 0.019175276160240173 + 0.1 * 6.228826522827148
Epoch 680, val loss: 1.123242735862732
Epoch 690, training loss: 0.6419317722320557 = 0.01829296350479126 + 0.1 * 6.236387729644775
Epoch 690, val loss: 1.132119059562683
Epoch 700, training loss: 0.6415063738822937 = 0.017471564933657646 + 0.1 * 6.240347862243652
Epoch 700, val loss: 1.1407960653305054
Epoch 710, training loss: 0.6393623352050781 = 0.0167083702981472 + 0.1 * 6.226539611816406
Epoch 710, val loss: 1.1494781970977783
Epoch 720, training loss: 0.6375992298126221 = 0.01599491760134697 + 0.1 * 6.216042995452881
Epoch 720, val loss: 1.1580045223236084
Epoch 730, training loss: 0.6372788548469543 = 0.015327148139476776 + 0.1 * 6.219516754150391
Epoch 730, val loss: 1.1662914752960205
Epoch 740, training loss: 0.6378238201141357 = 0.014702643267810345 + 0.1 * 6.231212139129639
Epoch 740, val loss: 1.17428719997406
Epoch 750, training loss: 0.6357997059822083 = 0.014118104241788387 + 0.1 * 6.216815948486328
Epoch 750, val loss: 1.1822744607925415
Epoch 760, training loss: 0.6352224946022034 = 0.013571389019489288 + 0.1 * 6.216510772705078
Epoch 760, val loss: 1.1900334358215332
Epoch 770, training loss: 0.6331153512001038 = 0.013058063574135303 + 0.1 * 6.200572490692139
Epoch 770, val loss: 1.197752594947815
Epoch 780, training loss: 0.6329208612442017 = 0.012574924156069756 + 0.1 * 6.2034592628479
Epoch 780, val loss: 1.2053865194320679
Epoch 790, training loss: 0.6323217153549194 = 0.012119472958147526 + 0.1 * 6.202022075653076
Epoch 790, val loss: 1.2126578092575073
Epoch 800, training loss: 0.6319301128387451 = 0.011690636165440083 + 0.1 * 6.202394962310791
Epoch 800, val loss: 1.2197256088256836
Epoch 810, training loss: 0.6312052011489868 = 0.011287347413599491 + 0.1 * 6.199178695678711
Epoch 810, val loss: 1.2269564867019653
Epoch 820, training loss: 0.6300165057182312 = 0.010906078852713108 + 0.1 * 6.191103935241699
Epoch 820, val loss: 1.2340282201766968
Epoch 830, training loss: 0.6300191879272461 = 0.01054432149976492 + 0.1 * 6.194748878479004
Epoch 830, val loss: 1.240951418876648
Epoch 840, training loss: 0.6296351552009583 = 0.010201257653534412 + 0.1 * 6.194338798522949
Epoch 840, val loss: 1.2474806308746338
Epoch 850, training loss: 0.6296972036361694 = 0.009876751340925694 + 0.1 * 6.198204517364502
Epoch 850, val loss: 1.2540662288665771
Epoch 860, training loss: 0.6290648579597473 = 0.009569552727043629 + 0.1 * 6.194953441619873
Epoch 860, val loss: 1.2605481147766113
Epoch 870, training loss: 0.6269468665122986 = 0.009278136305510998 + 0.1 * 6.176687240600586
Epoch 870, val loss: 1.2669428586959839
Epoch 880, training loss: 0.626910924911499 = 0.00900138821452856 + 0.1 * 6.179095268249512
Epoch 880, val loss: 1.2733726501464844
Epoch 890, training loss: 0.6272579431533813 = 0.008737023919820786 + 0.1 * 6.185208797454834
Epoch 890, val loss: 1.2793779373168945
Epoch 900, training loss: 0.6262070536613464 = 0.008485451340675354 + 0.1 * 6.177216053009033
Epoch 900, val loss: 1.2854235172271729
Epoch 910, training loss: 0.6258543729782104 = 0.008246057666838169 + 0.1 * 6.176083087921143
Epoch 910, val loss: 1.2914701700210571
Epoch 920, training loss: 0.6262471675872803 = 0.00801706314086914 + 0.1 * 6.182301044464111
Epoch 920, val loss: 1.2973469495773315
Epoch 930, training loss: 0.6265423893928528 = 0.007798586040735245 + 0.1 * 6.187437534332275
Epoch 930, val loss: 1.303003191947937
Epoch 940, training loss: 0.6241482496261597 = 0.007589838933199644 + 0.1 * 6.165584087371826
Epoch 940, val loss: 1.3085858821868896
Epoch 950, training loss: 0.6239853501319885 = 0.00739013496786356 + 0.1 * 6.165952205657959
Epoch 950, val loss: 1.3142857551574707
Epoch 960, training loss: 0.6244260668754578 = 0.007199116982519627 + 0.1 * 6.172269344329834
Epoch 960, val loss: 1.3197873830795288
Epoch 970, training loss: 0.6242113709449768 = 0.00701561477035284 + 0.1 * 6.171957969665527
Epoch 970, val loss: 1.325071930885315
Epoch 980, training loss: 0.6230583786964417 = 0.006840085610747337 + 0.1 * 6.162182807922363
Epoch 980, val loss: 1.3304331302642822
Epoch 990, training loss: 0.6234016418457031 = 0.006671571172773838 + 0.1 * 6.167300701141357
Epoch 990, val loss: 1.3357617855072021
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8413
Flip ASR: 0.8089/225 nodes
The final ASR:0.76138, 0.05655, Accuracy:0.80988, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9478])
updated graph: torch.Size([2, 10516])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97294, 0.00174, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.786860466003418 = 1.9494763612747192 + 0.1 * 8.373842239379883
Epoch 0, val loss: 1.9428482055664062
Epoch 10, training loss: 2.7760212421417236 = 1.9386529922485352 + 0.1 * 8.373682022094727
Epoch 10, val loss: 1.9321558475494385
Epoch 20, training loss: 2.762552261352539 = 1.9252820014953613 + 0.1 * 8.372701644897461
Epoch 20, val loss: 1.918888807296753
Epoch 30, training loss: 2.7427151203155518 = 1.906278133392334 + 0.1 * 8.36436939239502
Epoch 30, val loss: 1.9001919031143188
Epoch 40, training loss: 2.7087318897247314 = 1.8780759572982788 + 0.1 * 8.306558609008789
Epoch 40, val loss: 1.8731375932693481
Epoch 50, training loss: 2.634053945541382 = 1.840649127960205 + 0.1 * 7.934047698974609
Epoch 50, val loss: 1.8394395112991333
Epoch 60, training loss: 2.5664725303649902 = 1.800108551979065 + 0.1 * 7.663639545440674
Epoch 60, val loss: 1.8051667213439941
Epoch 70, training loss: 2.4852073192596436 = 1.7607431411743164 + 0.1 * 7.244640827178955
Epoch 70, val loss: 1.7737805843353271
Epoch 80, training loss: 2.412734031677246 = 1.7197306156158447 + 0.1 * 6.93003511428833
Epoch 80, val loss: 1.7397509813308716
Epoch 90, training loss: 2.343641519546509 = 1.6653790473937988 + 0.1 * 6.782624244689941
Epoch 90, val loss: 1.6931562423706055
Epoch 100, training loss: 2.2615208625793457 = 1.5933865308761597 + 0.1 * 6.681344032287598
Epoch 100, val loss: 1.6337436437606812
Epoch 110, training loss: 2.1696841716766357 = 1.5075106620788574 + 0.1 * 6.621735095977783
Epoch 110, val loss: 1.5658555030822754
Epoch 120, training loss: 2.0764870643615723 = 1.4176177978515625 + 0.1 * 6.5886921882629395
Epoch 120, val loss: 1.49619460105896
Epoch 130, training loss: 1.988494873046875 = 1.3322089910507202 + 0.1 * 6.562859535217285
Epoch 130, val loss: 1.432737946510315
Epoch 140, training loss: 1.9073567390441895 = 1.2532633543014526 + 0.1 * 6.5409345626831055
Epoch 140, val loss: 1.3774631023406982
Epoch 150, training loss: 1.8332860469818115 = 1.1811981201171875 + 0.1 * 6.520878791809082
Epoch 150, val loss: 1.3295985460281372
Epoch 160, training loss: 1.764116644859314 = 1.1141067743301392 + 0.1 * 6.500098705291748
Epoch 160, val loss: 1.2867376804351807
Epoch 170, training loss: 1.6973958015441895 = 1.0487712621688843 + 0.1 * 6.486246109008789
Epoch 170, val loss: 1.2454969882965088
Epoch 180, training loss: 1.6316373348236084 = 0.9850269556045532 + 0.1 * 6.4661030769348145
Epoch 180, val loss: 1.2046048641204834
Epoch 190, training loss: 1.568256139755249 = 0.922620415687561 + 0.1 * 6.456357002258301
Epoch 190, val loss: 1.1638332605361938
Epoch 200, training loss: 1.505039095878601 = 0.8611173629760742 + 0.1 * 6.4392170906066895
Epoch 200, val loss: 1.122857689857483
Epoch 210, training loss: 1.4428470134735107 = 0.7991580367088318 + 0.1 * 6.4368896484375
Epoch 210, val loss: 1.0807149410247803
Epoch 220, training loss: 1.3794703483581543 = 0.7370161414146423 + 0.1 * 6.424541473388672
Epoch 220, val loss: 1.0382345914840698
Epoch 230, training loss: 1.315975546836853 = 0.6744721531867981 + 0.1 * 6.41503381729126
Epoch 230, val loss: 0.9955296516418457
Epoch 240, training loss: 1.2529890537261963 = 0.6124350428581238 + 0.1 * 6.405539512634277
Epoch 240, val loss: 0.9540702700614929
Epoch 250, training loss: 1.1929411888122559 = 0.5523138046264648 + 0.1 * 6.40627384185791
Epoch 250, val loss: 0.9155305027961731
Epoch 260, training loss: 1.134690284729004 = 0.49559128284454346 + 0.1 * 6.390990734100342
Epoch 260, val loss: 0.8815023899078369
Epoch 270, training loss: 1.0814069509506226 = 0.44300001859664917 + 0.1 * 6.384069442749023
Epoch 270, val loss: 0.8526594042778015
Epoch 280, training loss: 1.0334904193878174 = 0.39516156911849976 + 0.1 * 6.383288383483887
Epoch 280, val loss: 0.829359233379364
Epoch 290, training loss: 0.9889593720436096 = 0.35203349590301514 + 0.1 * 6.369258880615234
Epoch 290, val loss: 0.8115526437759399
Epoch 300, training loss: 0.9518843293190002 = 0.3136463761329651 + 0.1 * 6.382379531860352
Epoch 300, val loss: 0.7988523840904236
Epoch 310, training loss: 0.9154226779937744 = 0.2799382507801056 + 0.1 * 6.354844093322754
Epoch 310, val loss: 0.791083574295044
Epoch 320, training loss: 0.8897616267204285 = 0.250180721282959 + 0.1 * 6.395809173583984
Epoch 320, val loss: 0.7874113321304321
Epoch 330, training loss: 0.8590061664581299 = 0.2241840362548828 + 0.1 * 6.348221302032471
Epoch 330, val loss: 0.7875237464904785
Epoch 340, training loss: 0.8347763419151306 = 0.20129375159740448 + 0.1 * 6.33482551574707
Epoch 340, val loss: 0.7904515862464905
Epoch 350, training loss: 0.8143715858459473 = 0.18113474547863007 + 0.1 * 6.33236837387085
Epoch 350, val loss: 0.7957653403282166
Epoch 360, training loss: 0.7966218590736389 = 0.1634330153465271 + 0.1 * 6.331888198852539
Epoch 360, val loss: 0.802855908870697
Epoch 370, training loss: 0.7803823947906494 = 0.14786361157894135 + 0.1 * 6.325187683105469
Epoch 370, val loss: 0.811156690120697
Epoch 380, training loss: 0.7664690613746643 = 0.13414053618907928 + 0.1 * 6.323285102844238
Epoch 380, val loss: 0.8205020427703857
Epoch 390, training loss: 0.753265917301178 = 0.12200664728879929 + 0.1 * 6.312592506408691
Epoch 390, val loss: 0.8304406404495239
Epoch 400, training loss: 0.7419456839561462 = 0.11125259101390839 + 0.1 * 6.3069305419921875
Epoch 400, val loss: 0.8408526182174683
Epoch 410, training loss: 0.7318552732467651 = 0.1016760990023613 + 0.1 * 6.301791667938232
Epoch 410, val loss: 0.8516212105751038
Epoch 420, training loss: 0.723231315612793 = 0.09313111752271652 + 0.1 * 6.301002025604248
Epoch 420, val loss: 0.8624200820922852
Epoch 430, training loss: 0.7143426537513733 = 0.08552727848291397 + 0.1 * 6.288153171539307
Epoch 430, val loss: 0.8735804557800293
Epoch 440, training loss: 0.7076306939125061 = 0.07870029658079147 + 0.1 * 6.289303779602051
Epoch 440, val loss: 0.8845752477645874
Epoch 450, training loss: 0.7014631628990173 = 0.07257046550512314 + 0.1 * 6.28892707824707
Epoch 450, val loss: 0.8957472443580627
Epoch 460, training loss: 0.6958041787147522 = 0.06706535816192627 + 0.1 * 6.287387847900391
Epoch 460, val loss: 0.9069510102272034
Epoch 470, training loss: 0.6894099712371826 = 0.0621085949242115 + 0.1 * 6.273014068603516
Epoch 470, val loss: 0.9180439710617065
Epoch 480, training loss: 0.6848447322845459 = 0.05762728676199913 + 0.1 * 6.272174835205078
Epoch 480, val loss: 0.9292863607406616
Epoch 490, training loss: 0.6812103986740112 = 0.05356749892234802 + 0.1 * 6.276429176330566
Epoch 490, val loss: 0.9403851628303528
Epoch 500, training loss: 0.6773889660835266 = 0.049891337752342224 + 0.1 * 6.274975776672363
Epoch 500, val loss: 0.9514934420585632
Epoch 510, training loss: 0.6730977296829224 = 0.04655936360359192 + 0.1 * 6.265383243560791
Epoch 510, val loss: 0.9624539017677307
Epoch 520, training loss: 0.6697896122932434 = 0.043540116399526596 + 0.1 * 6.2624945640563965
Epoch 520, val loss: 0.9732427597045898
Epoch 530, training loss: 0.6656301021575928 = 0.04079483076930046 + 0.1 * 6.248352527618408
Epoch 530, val loss: 0.9841275811195374
Epoch 540, training loss: 0.6632080674171448 = 0.038284312933683395 + 0.1 * 6.249237537384033
Epoch 540, val loss: 0.9946919679641724
Epoch 550, training loss: 0.661300539970398 = 0.035991594195365906 + 0.1 * 6.25308895111084
Epoch 550, val loss: 1.005108118057251
Epoch 560, training loss: 0.6592212915420532 = 0.03390050679445267 + 0.1 * 6.253207683563232
Epoch 560, val loss: 1.0155513286590576
Epoch 570, training loss: 0.6563505530357361 = 0.031983375549316406 + 0.1 * 6.243671417236328
Epoch 570, val loss: 1.0257402658462524
Epoch 580, training loss: 0.6542564034461975 = 0.030221575871109962 + 0.1 * 6.240347862243652
Epoch 580, val loss: 1.0357755422592163
Epoch 590, training loss: 0.6535490155220032 = 0.028599681332707405 + 0.1 * 6.24949312210083
Epoch 590, val loss: 1.0457355976104736
Epoch 600, training loss: 0.6513128280639648 = 0.02710670232772827 + 0.1 * 6.242061138153076
Epoch 600, val loss: 1.055341362953186
Epoch 610, training loss: 0.6485471725463867 = 0.025730665773153305 + 0.1 * 6.2281646728515625
Epoch 610, val loss: 1.0649083852767944
Epoch 620, training loss: 0.6476112604141235 = 0.024457760155200958 + 0.1 * 6.231534957885742
Epoch 620, val loss: 1.0743627548217773
Epoch 630, training loss: 0.6465731263160706 = 0.023277411237359047 + 0.1 * 6.232956886291504
Epoch 630, val loss: 1.083560824394226
Epoch 640, training loss: 0.6447639465332031 = 0.02218109741806984 + 0.1 * 6.225828170776367
Epoch 640, val loss: 1.092599868774414
Epoch 650, training loss: 0.6453457474708557 = 0.02116117998957634 + 0.1 * 6.24184513092041
Epoch 650, val loss: 1.101442813873291
Epoch 660, training loss: 0.6429303288459778 = 0.020215358585119247 + 0.1 * 6.227149963378906
Epoch 660, val loss: 1.1100486516952515
Epoch 670, training loss: 0.6408727169036865 = 0.01933291181921959 + 0.1 * 6.21539831161499
Epoch 670, val loss: 1.1188111305236816
Epoch 680, training loss: 0.6416429281234741 = 0.018505766987800598 + 0.1 * 6.231371879577637
Epoch 680, val loss: 1.1271551847457886
Epoch 690, training loss: 0.6393265724182129 = 0.017734574154019356 + 0.1 * 6.2159199714660645
Epoch 690, val loss: 1.1352051496505737
Epoch 700, training loss: 0.6383040547370911 = 0.017011461779475212 + 0.1 * 6.212925910949707
Epoch 700, val loss: 1.143497109413147
Epoch 710, training loss: 0.6370009779930115 = 0.01633358746767044 + 0.1 * 6.206673622131348
Epoch 710, val loss: 1.1512162685394287
Epoch 720, training loss: 0.6361897587776184 = 0.015696639195084572 + 0.1 * 6.204931259155273
Epoch 720, val loss: 1.159067988395691
Epoch 730, training loss: 0.6354762315750122 = 0.01509714312851429 + 0.1 * 6.203790664672852
Epoch 730, val loss: 1.1666914224624634
Epoch 740, training loss: 0.6359063982963562 = 0.014532799832522869 + 0.1 * 6.213736057281494
Epoch 740, val loss: 1.1741552352905273
Epoch 750, training loss: 0.6334617137908936 = 0.014000882394611835 + 0.1 * 6.194608211517334
Epoch 750, val loss: 1.1814227104187012
Epoch 760, training loss: 0.6347888708114624 = 0.013499375432729721 + 0.1 * 6.212894916534424
Epoch 760, val loss: 1.1887011528015137
Epoch 770, training loss: 0.6326987743377686 = 0.013025031425058842 + 0.1 * 6.196737289428711
Epoch 770, val loss: 1.1956787109375
Epoch 780, training loss: 0.6332753300666809 = 0.012576957233250141 + 0.1 * 6.206984043121338
Epoch 780, val loss: 1.202663779258728
Epoch 790, training loss: 0.6314604878425598 = 0.012153399176895618 + 0.1 * 6.193070888519287
Epoch 790, val loss: 1.2093905210494995
Epoch 800, training loss: 0.6315287351608276 = 0.011750648729503155 + 0.1 * 6.197780609130859
Epoch 800, val loss: 1.216108798980713
Epoch 810, training loss: 0.630212664604187 = 0.011369404383003712 + 0.1 * 6.188432693481445
Epoch 810, val loss: 1.2226568460464478
Epoch 820, training loss: 0.6298566460609436 = 0.011005987413227558 + 0.1 * 6.188506603240967
Epoch 820, val loss: 1.2291072607040405
Epoch 830, training loss: 0.62815260887146 = 0.01066102460026741 + 0.1 * 6.174915790557861
Epoch 830, val loss: 1.235402226448059
Epoch 840, training loss: 0.6298425197601318 = 0.010332359932363033 + 0.1 * 6.195101737976074
Epoch 840, val loss: 1.2417197227478027
Epoch 850, training loss: 0.6283626556396484 = 0.010019577108323574 + 0.1 * 6.1834306716918945
Epoch 850, val loss: 1.2475203275680542
Epoch 860, training loss: 0.627400279045105 = 0.009723154827952385 + 0.1 * 6.176771640777588
Epoch 860, val loss: 1.2536065578460693
Epoch 870, training loss: 0.6276059746742249 = 0.009440293535590172 + 0.1 * 6.181656360626221
Epoch 870, val loss: 1.2596179246902466
Epoch 880, training loss: 0.6274698972702026 = 0.009170004166662693 + 0.1 * 6.1829986572265625
Epoch 880, val loss: 1.2653465270996094
Epoch 890, training loss: 0.6256014704704285 = 0.00891205109655857 + 0.1 * 6.166894435882568
Epoch 890, val loss: 1.2709407806396484
Epoch 900, training loss: 0.6265438199043274 = 0.008665193803608418 + 0.1 * 6.178785800933838
Epoch 900, val loss: 1.2766951322555542
Epoch 910, training loss: 0.6262747049331665 = 0.008429452776908875 + 0.1 * 6.178452491760254
Epoch 910, val loss: 1.282020092010498
Epoch 920, training loss: 0.625618577003479 = 0.00820443406701088 + 0.1 * 6.174140930175781
Epoch 920, val loss: 1.2875349521636963
Epoch 930, training loss: 0.6245085597038269 = 0.00798782892525196 + 0.1 * 6.1652069091796875
Epoch 930, val loss: 1.2930006980895996
Epoch 940, training loss: 0.6246955990791321 = 0.007780676707625389 + 0.1 * 6.169149398803711
Epoch 940, val loss: 1.2979892492294312
Epoch 950, training loss: 0.6244427561759949 = 0.007582466118037701 + 0.1 * 6.168602466583252
Epoch 950, val loss: 1.3032286167144775
Epoch 960, training loss: 0.6249452233314514 = 0.00739250797778368 + 0.1 * 6.175527095794678
Epoch 960, val loss: 1.3083837032318115
Epoch 970, training loss: 0.623199462890625 = 0.007209830917418003 + 0.1 * 6.159896373748779
Epoch 970, val loss: 1.3133882284164429
Epoch 980, training loss: 0.6238542795181274 = 0.007034303620457649 + 0.1 * 6.16819953918457
Epoch 980, val loss: 1.3184702396392822
Epoch 990, training loss: 0.6241117715835571 = 0.006865184288471937 + 0.1 * 6.1724653244018555
Epoch 990, val loss: 1.3232192993164062
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7196
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.785733461380005 = 1.9483438730239868 + 0.1 * 8.373896598815918
Epoch 0, val loss: 1.9519153833389282
Epoch 10, training loss: 2.7752065658569336 = 1.9378275871276855 + 0.1 * 8.373788833618164
Epoch 10, val loss: 1.9407565593719482
Epoch 20, training loss: 2.7620158195495605 = 1.9246861934661865 + 0.1 * 8.373295783996582
Epoch 20, val loss: 1.9265192747116089
Epoch 30, training loss: 2.7427382469177246 = 1.9057518243789673 + 0.1 * 8.369864463806152
Epoch 30, val loss: 1.905889630317688
Epoch 40, training loss: 2.7120728492736816 = 1.8774546384811401 + 0.1 * 8.346181869506836
Epoch 40, val loss: 1.8754069805145264
Epoch 50, training loss: 2.660853862762451 = 1.8385319709777832 + 0.1 * 8.22321891784668
Epoch 50, val loss: 1.8353294134140015
Epoch 60, training loss: 2.5831825733184814 = 1.7954902648925781 + 0.1 * 7.876923084259033
Epoch 60, val loss: 1.7944636344909668
Epoch 70, training loss: 2.51291823387146 = 1.754080891609192 + 0.1 * 7.588372707366943
Epoch 70, val loss: 1.7569661140441895
Epoch 80, training loss: 2.4336111545562744 = 1.7066181898117065 + 0.1 * 7.269928932189941
Epoch 80, val loss: 1.7151731252670288
Epoch 90, training loss: 2.3482391834259033 = 1.6475749015808105 + 0.1 * 7.006643295288086
Epoch 90, val loss: 1.664157509803772
Epoch 100, training loss: 2.2572154998779297 = 1.5709189176559448 + 0.1 * 6.8629655838012695
Epoch 100, val loss: 1.5970643758773804
Epoch 110, training loss: 2.158597946166992 = 1.4797273874282837 + 0.1 * 6.7887043952941895
Epoch 110, val loss: 1.5198849439620972
Epoch 120, training loss: 2.059597969055176 = 1.3842731714248657 + 0.1 * 6.753249168395996
Epoch 120, val loss: 1.4431166648864746
Epoch 130, training loss: 1.9641343355178833 = 1.2911417484283447 + 0.1 * 6.729925632476807
Epoch 130, val loss: 1.3724262714385986
Epoch 140, training loss: 1.8732490539550781 = 1.2020419836044312 + 0.1 * 6.712070941925049
Epoch 140, val loss: 1.3086867332458496
Epoch 150, training loss: 1.7881276607513428 = 1.1182925701141357 + 0.1 * 6.698350429534912
Epoch 150, val loss: 1.250954270362854
Epoch 160, training loss: 1.7079576253890991 = 1.0394338369369507 + 0.1 * 6.685237884521484
Epoch 160, val loss: 1.1974424123764038
Epoch 170, training loss: 1.6308658123016357 = 0.9637405276298523 + 0.1 * 6.671252727508545
Epoch 170, val loss: 1.1470295190811157
Epoch 180, training loss: 1.556505560874939 = 0.8904802203178406 + 0.1 * 6.660253524780273
Epoch 180, val loss: 1.0974606275558472
Epoch 190, training loss: 1.4851505756378174 = 0.8201445937156677 + 0.1 * 6.650060176849365
Epoch 190, val loss: 1.049478530883789
Epoch 200, training loss: 1.4171353578567505 = 0.7532135844230652 + 0.1 * 6.639217853546143
Epoch 200, val loss: 1.0030665397644043
Epoch 210, training loss: 1.3542661666870117 = 0.6908372640609741 + 0.1 * 6.634288787841797
Epoch 210, val loss: 0.9594666361808777
Epoch 220, training loss: 1.2969180345535278 = 0.6344615817070007 + 0.1 * 6.6245646476745605
Epoch 220, val loss: 0.9202782511711121
Epoch 230, training loss: 1.2453237771987915 = 0.5835770964622498 + 0.1 * 6.617466926574707
Epoch 230, val loss: 0.8853888511657715
Epoch 240, training loss: 1.1988554000854492 = 0.5377833843231201 + 0.1 * 6.610719680786133
Epoch 240, val loss: 0.8551589250564575
Epoch 250, training loss: 1.156928300857544 = 0.4965009391307831 + 0.1 * 6.604273796081543
Epoch 250, val loss: 0.8297937512397766
Epoch 260, training loss: 1.1180442571640015 = 0.45869413018226624 + 0.1 * 6.593501091003418
Epoch 260, val loss: 0.8090003132820129
Epoch 270, training loss: 1.0829071998596191 = 0.4235820770263672 + 0.1 * 6.593250274658203
Epoch 270, val loss: 0.7922019958496094
Epoch 280, training loss: 1.0482935905456543 = 0.3905094265937805 + 0.1 * 6.5778422355651855
Epoch 280, val loss: 0.778919517993927
Epoch 290, training loss: 1.0157066583633423 = 0.35843226313591003 + 0.1 * 6.572743892669678
Epoch 290, val loss: 0.767928957939148
Epoch 300, training loss: 0.9836199879646301 = 0.32718801498413086 + 0.1 * 6.564319610595703
Epoch 300, val loss: 0.7588645815849304
Epoch 310, training loss: 0.9518669247627258 = 0.29652613401412964 + 0.1 * 6.553407669067383
Epoch 310, val loss: 0.7511143684387207
Epoch 320, training loss: 0.9210801720619202 = 0.26652222871780396 + 0.1 * 6.545579433441162
Epoch 320, val loss: 0.7446501851081848
Epoch 330, training loss: 0.8919647336006165 = 0.2376497983932495 + 0.1 * 6.543149471282959
Epoch 330, val loss: 0.7391418814659119
Epoch 340, training loss: 0.864346444606781 = 0.21071042120456696 + 0.1 * 6.536360263824463
Epoch 340, val loss: 0.7350854873657227
Epoch 350, training loss: 0.838914692401886 = 0.18618158996105194 + 0.1 * 6.5273308753967285
Epoch 350, val loss: 0.7325505018234253
Epoch 360, training loss: 0.8171305656433105 = 0.16435745358467102 + 0.1 * 6.527731418609619
Epoch 360, val loss: 0.7323619723320007
Epoch 370, training loss: 0.7976294159889221 = 0.14559035003185272 + 0.1 * 6.520390510559082
Epoch 370, val loss: 0.7349959015846252
Epoch 380, training loss: 0.7806953191757202 = 0.1296537071466446 + 0.1 * 6.510416030883789
Epoch 380, val loss: 0.7401964664459229
Epoch 390, training loss: 0.7672808766365051 = 0.11618777364492416 + 0.1 * 6.510931015014648
Epoch 390, val loss: 0.7476361393928528
Epoch 400, training loss: 0.7549104690551758 = 0.10484107583761215 + 0.1 * 6.5006937980651855
Epoch 400, val loss: 0.7567977905273438
Epoch 410, training loss: 0.7439479827880859 = 0.09525244683027267 + 0.1 * 6.486955165863037
Epoch 410, val loss: 0.7672035098075867
Epoch 420, training loss: 0.7358653545379639 = 0.08706951141357422 + 0.1 * 6.4879584312438965
Epoch 420, val loss: 0.7783339619636536
Epoch 430, training loss: 0.7278051376342773 = 0.08007081598043442 + 0.1 * 6.4773430824279785
Epoch 430, val loss: 0.7898815870285034
Epoch 440, training loss: 0.7207078337669373 = 0.07396715879440308 + 0.1 * 6.467406749725342
Epoch 440, val loss: 0.8016183376312256
Epoch 450, training loss: 0.7160031795501709 = 0.06859226524829865 + 0.1 * 6.474109172821045
Epoch 450, val loss: 0.8135207295417786
Epoch 460, training loss: 0.7094436883926392 = 0.06386084109544754 + 0.1 * 6.455828666687012
Epoch 460, val loss: 0.8253329992294312
Epoch 470, training loss: 0.7043609619140625 = 0.059655219316482544 + 0.1 * 6.447056770324707
Epoch 470, val loss: 0.8368895649909973
Epoch 480, training loss: 0.7004129886627197 = 0.05589110031723976 + 0.1 * 6.445218563079834
Epoch 480, val loss: 0.8485002517700195
Epoch 490, training loss: 0.6957805156707764 = 0.052508462220430374 + 0.1 * 6.432720184326172
Epoch 490, val loss: 0.8598029613494873
Epoch 500, training loss: 0.6916876435279846 = 0.0494580939412117 + 0.1 * 6.422295570373535
Epoch 500, val loss: 0.8707607984542847
Epoch 510, training loss: 0.6887401938438416 = 0.0466923862695694 + 0.1 * 6.420478343963623
Epoch 510, val loss: 0.8815879225730896
Epoch 520, training loss: 0.6857445240020752 = 0.04417497292160988 + 0.1 * 6.415695667266846
Epoch 520, val loss: 0.8920682668685913
Epoch 530, training loss: 0.6824020147323608 = 0.04187292978167534 + 0.1 * 6.4052910804748535
Epoch 530, val loss: 0.9022819399833679
Epoch 540, training loss: 0.6797914505004883 = 0.03975430876016617 + 0.1 * 6.400371074676514
Epoch 540, val loss: 0.9121432304382324
Epoch 550, training loss: 0.6765995621681213 = 0.037800922989845276 + 0.1 * 6.387986660003662
Epoch 550, val loss: 0.9218946695327759
Epoch 560, training loss: 0.674078106880188 = 0.03598589822649956 + 0.1 * 6.380922317504883
Epoch 560, val loss: 0.9313280582427979
Epoch 570, training loss: 0.6721149682998657 = 0.03429572284221649 + 0.1 * 6.37819242477417
Epoch 570, val loss: 0.9404128193855286
Epoch 580, training loss: 0.6696544885635376 = 0.03272528946399689 + 0.1 * 6.369292259216309
Epoch 580, val loss: 0.9494260549545288
Epoch 590, training loss: 0.6680943965911865 = 0.031251903623342514 + 0.1 * 6.368424892425537
Epoch 590, val loss: 0.9580982327461243
Epoch 600, training loss: 0.6665511727333069 = 0.029873760417103767 + 0.1 * 6.36677360534668
Epoch 600, val loss: 0.9666421413421631
Epoch 610, training loss: 0.6640351414680481 = 0.028574995696544647 + 0.1 * 6.3546013832092285
Epoch 610, val loss: 0.9749676585197449
Epoch 620, training loss: 0.6637901067733765 = 0.027340369299054146 + 0.1 * 6.364497661590576
Epoch 620, val loss: 0.9830214977264404
Epoch 630, training loss: 0.6609602570533752 = 0.026170216500759125 + 0.1 * 6.347900390625
Epoch 630, val loss: 0.9909372925758362
Epoch 640, training loss: 0.6597620844841003 = 0.025060806423425674 + 0.1 * 6.347012519836426
Epoch 640, val loss: 0.9987996220588684
Epoch 650, training loss: 0.6581974625587463 = 0.023999188095331192 + 0.1 * 6.341982364654541
Epoch 650, val loss: 1.0063846111297607
Epoch 660, training loss: 0.6563287377357483 = 0.02297009341418743 + 0.1 * 6.333586692810059
Epoch 660, val loss: 1.014052152633667
Epoch 670, training loss: 0.6554508805274963 = 0.02197093516588211 + 0.1 * 6.334799289703369
Epoch 670, val loss: 1.0213873386383057
Epoch 680, training loss: 0.6555606126785278 = 0.021011188626289368 + 0.1 * 6.345494270324707
Epoch 680, val loss: 1.0286850929260254
Epoch 690, training loss: 0.6524022221565247 = 0.020085930824279785 + 0.1 * 6.323163032531738
Epoch 690, val loss: 1.035780906677246
Epoch 700, training loss: 0.6509258151054382 = 0.019191095605492592 + 0.1 * 6.317347049713135
Epoch 700, val loss: 1.0427361726760864
Epoch 710, training loss: 0.6495988368988037 = 0.01832158863544464 + 0.1 * 6.312772274017334
Epoch 710, val loss: 1.049708604812622
Epoch 720, training loss: 0.6495852470397949 = 0.017467699944972992 + 0.1 * 6.321175575256348
Epoch 720, val loss: 1.0565379858016968
Epoch 730, training loss: 0.6480150818824768 = 0.016635224223136902 + 0.1 * 6.313798427581787
Epoch 730, val loss: 1.0633958578109741
Epoch 740, training loss: 0.6476633548736572 = 0.015825564041733742 + 0.1 * 6.31837797164917
Epoch 740, val loss: 1.0701762437820435
Epoch 750, training loss: 0.6454344987869263 = 0.015040203928947449 + 0.1 * 6.303942680358887
Epoch 750, val loss: 1.0768365859985352
Epoch 760, training loss: 0.6453841924667358 = 0.01429777778685093 + 0.1 * 6.310863971710205
Epoch 760, val loss: 1.0832959413528442
Epoch 770, training loss: 0.6440178751945496 = 0.013579795137047768 + 0.1 * 6.304380893707275
Epoch 770, val loss: 1.089652419090271
Epoch 780, training loss: 0.6421957015991211 = 0.012905647046864033 + 0.1 * 6.292900562286377
Epoch 780, val loss: 1.0962696075439453
Epoch 790, training loss: 0.6417197585105896 = 0.012267429381608963 + 0.1 * 6.294523239135742
Epoch 790, val loss: 1.102749228477478
Epoch 800, training loss: 0.6399934887886047 = 0.011671437881886959 + 0.1 * 6.283220291137695
Epoch 800, val loss: 1.1093266010284424
Epoch 810, training loss: 0.6419190168380737 = 0.011121004819869995 + 0.1 * 6.307980537414551
Epoch 810, val loss: 1.1157971620559692
Epoch 820, training loss: 0.6388143301010132 = 0.0105831827968359 + 0.1 * 6.28231143951416
Epoch 820, val loss: 1.1223583221435547
Epoch 830, training loss: 0.6383404731750488 = 0.010060158558189869 + 0.1 * 6.282803535461426
Epoch 830, val loss: 1.1288336515426636
Epoch 840, training loss: 0.6372856497764587 = 0.0095711974427104 + 0.1 * 6.277144432067871
Epoch 840, val loss: 1.1350575685501099
Epoch 850, training loss: 0.6369328498840332 = 0.009133855812251568 + 0.1 * 6.277989864349365
Epoch 850, val loss: 1.1411552429199219
Epoch 860, training loss: 0.6361764073371887 = 0.008743871003389359 + 0.1 * 6.274324893951416
Epoch 860, val loss: 1.1476953029632568
Epoch 870, training loss: 0.6356827616691589 = 0.008395451121032238 + 0.1 * 6.2728729248046875
Epoch 870, val loss: 1.154133677482605
Epoch 880, training loss: 0.6345381140708923 = 0.00807531364262104 + 0.1 * 6.264627933502197
Epoch 880, val loss: 1.1602948904037476
Epoch 890, training loss: 0.6343124508857727 = 0.007782639004290104 + 0.1 * 6.265297889709473
Epoch 890, val loss: 1.1665663719177246
Epoch 900, training loss: 0.6340950727462769 = 0.007509754505008459 + 0.1 * 6.265852928161621
Epoch 900, val loss: 1.1727240085601807
Epoch 910, training loss: 0.6325836777687073 = 0.007255405653268099 + 0.1 * 6.25328254699707
Epoch 910, val loss: 1.1787383556365967
Epoch 920, training loss: 0.6329247951507568 = 0.007019347976893187 + 0.1 * 6.259054183959961
Epoch 920, val loss: 1.184452772140503
Epoch 930, training loss: 0.6322526931762695 = 0.006800387054681778 + 0.1 * 6.254522800445557
Epoch 930, val loss: 1.1903458833694458
Epoch 940, training loss: 0.6343610286712646 = 0.006596386432647705 + 0.1 * 6.277646541595459
Epoch 940, val loss: 1.1959967613220215
Epoch 950, training loss: 0.6318356394767761 = 0.006403304636478424 + 0.1 * 6.2543230056762695
Epoch 950, val loss: 1.201550006866455
Epoch 960, training loss: 0.6317681670188904 = 0.006223056931048632 + 0.1 * 6.25545072555542
Epoch 960, val loss: 1.2071318626403809
Epoch 970, training loss: 0.6318122148513794 = 0.006051552016288042 + 0.1 * 6.257606506347656
Epoch 970, val loss: 1.2123944759368896
Epoch 980, training loss: 0.6302874088287354 = 0.005890040658414364 + 0.1 * 6.243973731994629
Epoch 980, val loss: 1.2176662683486938
Epoch 990, training loss: 0.6310559511184692 = 0.005735993850976229 + 0.1 * 6.253199577331543
Epoch 990, val loss: 1.2229249477386475
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7749
Flip ASR: 0.7333/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7810800075531006 = 1.943690538406372 + 0.1 * 8.373893737792969
Epoch 0, val loss: 1.934956431388855
Epoch 10, training loss: 2.7715237140655518 = 1.934141993522644 + 0.1 * 8.373817443847656
Epoch 10, val loss: 1.9263968467712402
Epoch 20, training loss: 2.7600274085998535 = 1.9226856231689453 + 0.1 * 8.373417854309082
Epoch 20, val loss: 1.9158912897109985
Epoch 30, training loss: 2.744014263153076 = 1.9069528579711914 + 0.1 * 8.370613098144531
Epoch 30, val loss: 1.9012466669082642
Epoch 40, training loss: 2.7183146476745605 = 1.883729338645935 + 0.1 * 8.34585189819336
Epoch 40, val loss: 1.8796970844268799
Epoch 50, training loss: 2.6625051498413086 = 1.8504987955093384 + 0.1 * 8.120062828063965
Epoch 50, val loss: 1.850058674812317
Epoch 60, training loss: 2.5800046920776367 = 1.8127472400665283 + 0.1 * 7.672574043273926
Epoch 60, val loss: 1.8177541494369507
Epoch 70, training loss: 2.501915454864502 = 1.777363657951355 + 0.1 * 7.245519161224365
Epoch 70, val loss: 1.7868733406066895
Epoch 80, training loss: 2.4341373443603516 = 1.7421352863311768 + 0.1 * 6.920021057128906
Epoch 80, val loss: 1.755861520767212
Epoch 90, training loss: 2.373955726623535 = 1.699284553527832 + 0.1 * 6.746710777282715
Epoch 90, val loss: 1.716589331626892
Epoch 100, training loss: 2.306520700454712 = 1.6395832300186157 + 0.1 * 6.669374465942383
Epoch 100, val loss: 1.6624482870101929
Epoch 110, training loss: 2.2244739532470703 = 1.5616952180862427 + 0.1 * 6.6277875900268555
Epoch 110, val loss: 1.596367597579956
Epoch 120, training loss: 2.131551504135132 = 1.4714335203170776 + 0.1 * 6.601180076599121
Epoch 120, val loss: 1.5209465026855469
Epoch 130, training loss: 2.037276268005371 = 1.3786745071411133 + 0.1 * 6.5860185623168945
Epoch 130, val loss: 1.4463789463043213
Epoch 140, training loss: 1.9467368125915527 = 1.289441466331482 + 0.1 * 6.572952747344971
Epoch 140, val loss: 1.3760735988616943
Epoch 150, training loss: 1.859508752822876 = 1.2033333778381348 + 0.1 * 6.561753749847412
Epoch 150, val loss: 1.3109859228134155
Epoch 160, training loss: 1.7754261493682861 = 1.1197642087936401 + 0.1 * 6.556619644165039
Epoch 160, val loss: 1.249667763710022
Epoch 170, training loss: 1.6926121711730957 = 1.037706732749939 + 0.1 * 6.54905366897583
Epoch 170, val loss: 1.19102144241333
Epoch 180, training loss: 1.610032081604004 = 0.9547776579856873 + 0.1 * 6.552544116973877
Epoch 180, val loss: 1.1322659254074097
Epoch 190, training loss: 1.5266938209533691 = 0.8722008466720581 + 0.1 * 6.544930458068848
Epoch 190, val loss: 1.0734690427780151
Epoch 200, training loss: 1.444690465927124 = 0.7904142141342163 + 0.1 * 6.542762279510498
Epoch 200, val loss: 1.0150686502456665
Epoch 210, training loss: 1.364851474761963 = 0.7107431888580322 + 0.1 * 6.541082382202148
Epoch 210, val loss: 0.9585981369018555
Epoch 220, training loss: 1.289960265159607 = 0.6349233984947205 + 0.1 * 6.550368785858154
Epoch 220, val loss: 0.905830442905426
Epoch 230, training loss: 1.219154953956604 = 0.5654165148735046 + 0.1 * 6.537384033203125
Epoch 230, val loss: 0.8592134118080139
Epoch 240, training loss: 1.1573139429092407 = 0.5025416016578674 + 0.1 * 6.547723293304443
Epoch 240, val loss: 0.8194442987442017
Epoch 250, training loss: 1.1004661321640015 = 0.446775883436203 + 0.1 * 6.536901950836182
Epoch 250, val loss: 0.7871177792549133
Epoch 260, training loss: 1.0502527952194214 = 0.39715492725372314 + 0.1 * 6.530978679656982
Epoch 260, val loss: 0.7610162496566772
Epoch 270, training loss: 1.0056722164154053 = 0.3530384600162506 + 0.1 * 6.526337623596191
Epoch 270, val loss: 0.7400088906288147
Epoch 280, training loss: 0.9663865566253662 = 0.3139914870262146 + 0.1 * 6.523950576782227
Epoch 280, val loss: 0.7233010530471802
Epoch 290, training loss: 0.9317967891693115 = 0.27973389625549316 + 0.1 * 6.520628929138184
Epoch 290, val loss: 0.7107746601104736
Epoch 300, training loss: 0.9007196426391602 = 0.24954478442668915 + 0.1 * 6.511748313903809
Epoch 300, val loss: 0.7013124823570251
Epoch 310, training loss: 0.8736305236816406 = 0.22269654273986816 + 0.1 * 6.509339809417725
Epoch 310, val loss: 0.6946399807929993
Epoch 320, training loss: 0.8501060009002686 = 0.198904350399971 + 0.1 * 6.512016773223877
Epoch 320, val loss: 0.689949095249176
Epoch 330, training loss: 0.8273966312408447 = 0.17787382006645203 + 0.1 * 6.495228290557861
Epoch 330, val loss: 0.6875145435333252
Epoch 340, training loss: 0.8076543807983398 = 0.1591406762599945 + 0.1 * 6.48513650894165
Epoch 340, val loss: 0.6865836977958679
Epoch 350, training loss: 0.7906028628349304 = 0.1424606591463089 + 0.1 * 6.481421947479248
Epoch 350, val loss: 0.6872604489326477
Epoch 360, training loss: 0.7744070887565613 = 0.12775152921676636 + 0.1 * 6.466555595397949
Epoch 360, val loss: 0.6890923976898193
Epoch 370, training loss: 0.760412871837616 = 0.1147446557879448 + 0.1 * 6.456682205200195
Epoch 370, val loss: 0.6922067403793335
Epoch 380, training loss: 0.7516836524009705 = 0.10321499407291412 + 0.1 * 6.484686851501465
Epoch 380, val loss: 0.6963818073272705
Epoch 390, training loss: 0.7383413910865784 = 0.09313618391752243 + 0.1 * 6.452051639556885
Epoch 390, val loss: 0.7012563943862915
Epoch 400, training loss: 0.7277563214302063 = 0.08426143229007721 + 0.1 * 6.434948921203613
Epoch 400, val loss: 0.706952691078186
Epoch 410, training loss: 0.719260573387146 = 0.07640809565782547 + 0.1 * 6.428524494171143
Epoch 410, val loss: 0.7133831977844238
Epoch 420, training loss: 0.7126705646514893 = 0.06949923932552338 + 0.1 * 6.431713104248047
Epoch 420, val loss: 0.7202425599098206
Epoch 430, training loss: 0.7045142650604248 = 0.06342677772045135 + 0.1 * 6.410874366760254
Epoch 430, val loss: 0.727654218673706
Epoch 440, training loss: 0.6986169815063477 = 0.05804641172289848 + 0.1 * 6.405705451965332
Epoch 440, val loss: 0.7353498935699463
Epoch 450, training loss: 0.6937404870986938 = 0.053286876529455185 + 0.1 * 6.404536247253418
Epoch 450, val loss: 0.7431167364120483
Epoch 460, training loss: 0.6887125968933105 = 0.04909360781311989 + 0.1 * 6.3961896896362305
Epoch 460, val loss: 0.751284658908844
Epoch 470, training loss: 0.6839575171470642 = 0.045360349118709564 + 0.1 * 6.385971546173096
Epoch 470, val loss: 0.7594903707504272
Epoch 480, training loss: 0.6797496676445007 = 0.042024772614240646 + 0.1 * 6.377248764038086
Epoch 480, val loss: 0.7676446437835693
Epoch 490, training loss: 0.6761295199394226 = 0.039038438349962234 + 0.1 * 6.37091064453125
Epoch 490, val loss: 0.7759910821914673
Epoch 500, training loss: 0.6750164031982422 = 0.03635629639029503 + 0.1 * 6.386601448059082
Epoch 500, val loss: 0.7842040061950684
Epoch 510, training loss: 0.6698513627052307 = 0.03395078331232071 + 0.1 * 6.359005451202393
Epoch 510, val loss: 0.7924518585205078
Epoch 520, training loss: 0.6675506234169006 = 0.03177886828780174 + 0.1 * 6.357717514038086
Epoch 520, val loss: 0.8006311058998108
Epoch 530, training loss: 0.6653949022293091 = 0.029816560447216034 + 0.1 * 6.355782985687256
Epoch 530, val loss: 0.8084504008293152
Epoch 540, training loss: 0.6622372269630432 = 0.02803729474544525 + 0.1 * 6.341999053955078
Epoch 540, val loss: 0.8165336847305298
Epoch 550, training loss: 0.6602311730384827 = 0.026418142020702362 + 0.1 * 6.338129997253418
Epoch 550, val loss: 0.8242017030715942
Epoch 560, training loss: 0.6578893065452576 = 0.024943843483924866 + 0.1 * 6.32945442199707
Epoch 560, val loss: 0.8319019675254822
Epoch 570, training loss: 0.6558170318603516 = 0.023589206859469414 + 0.1 * 6.322278022766113
Epoch 570, val loss: 0.8395018577575684
Epoch 580, training loss: 0.6598014235496521 = 0.022344406694173813 + 0.1 * 6.374569892883301
Epoch 580, val loss: 0.8467708230018616
Epoch 590, training loss: 0.653505265712738 = 0.021206360310316086 + 0.1 * 6.322988986968994
Epoch 590, val loss: 0.8539714813232422
Epoch 600, training loss: 0.6512491106987 = 0.020155416801571846 + 0.1 * 6.310936450958252
Epoch 600, val loss: 0.8612617254257202
Epoch 610, training loss: 0.6497039794921875 = 0.019187794998288155 + 0.1 * 6.305161476135254
Epoch 610, val loss: 0.8680844902992249
Epoch 620, training loss: 0.6485714316368103 = 0.01828792504966259 + 0.1 * 6.302835464477539
Epoch 620, val loss: 0.8750254511833191
Epoch 630, training loss: 0.647156298160553 = 0.01745588332414627 + 0.1 * 6.297004222869873
Epoch 630, val loss: 0.8817277550697327
Epoch 640, training loss: 0.6453360319137573 = 0.016680369153618813 + 0.1 * 6.286556720733643
Epoch 640, val loss: 0.8883228898048401
Epoch 650, training loss: 0.6456021666526794 = 0.0159604474902153 + 0.1 * 6.296416759490967
Epoch 650, val loss: 0.8947230577468872
Epoch 660, training loss: 0.6438701748847961 = 0.015288576483726501 + 0.1 * 6.285815715789795
Epoch 660, val loss: 0.9010499715805054
Epoch 670, training loss: 0.6420674920082092 = 0.014659766107797623 + 0.1 * 6.274077415466309
Epoch 670, val loss: 0.9073674082756042
Epoch 680, training loss: 0.6417582631111145 = 0.014072836376726627 + 0.1 * 6.276854038238525
Epoch 680, val loss: 0.9132574796676636
Epoch 690, training loss: 0.6406092047691345 = 0.013523825444281101 + 0.1 * 6.270853519439697
Epoch 690, val loss: 0.9192860722541809
Epoch 700, training loss: 0.6413108110427856 = 0.013009234331548214 + 0.1 * 6.283015251159668
Epoch 700, val loss: 0.9251610636711121
Epoch 710, training loss: 0.639079749584198 = 0.012524518184363842 + 0.1 * 6.265552043914795
Epoch 710, val loss: 0.9307231903076172
Epoch 720, training loss: 0.6385528445243835 = 0.012070060707628727 + 0.1 * 6.264827728271484
Epoch 720, val loss: 0.9363154768943787
Epoch 730, training loss: 0.6368857622146606 = 0.011641099117696285 + 0.1 * 6.25244665145874
Epoch 730, val loss: 0.9418562650680542
Epoch 740, training loss: 0.6390383839607239 = 0.011236757971346378 + 0.1 * 6.278015613555908
Epoch 740, val loss: 0.947286069393158
Epoch 750, training loss: 0.635934054851532 = 0.01085521001368761 + 0.1 * 6.25078821182251
Epoch 750, val loss: 0.9524062275886536
Epoch 760, training loss: 0.6356945037841797 = 0.010492971166968346 + 0.1 * 6.252015113830566
Epoch 760, val loss: 0.9576500654220581
Epoch 770, training loss: 0.6347119212150574 = 0.010150752030313015 + 0.1 * 6.245611667633057
Epoch 770, val loss: 0.9626196026802063
Epoch 780, training loss: 0.6365213394165039 = 0.009826172143220901 + 0.1 * 6.266951560974121
Epoch 780, val loss: 0.9677149057388306
Epoch 790, training loss: 0.633530855178833 = 0.009519515559077263 + 0.1 * 6.240113258361816
Epoch 790, val loss: 0.9723639488220215
Epoch 800, training loss: 0.6327470541000366 = 0.009228138253092766 + 0.1 * 6.235189437866211
Epoch 800, val loss: 0.9772046208381653
Epoch 810, training loss: 0.6325768232345581 = 0.008952192962169647 + 0.1 * 6.236246109008789
Epoch 810, val loss: 0.9819172024726868
Epoch 820, training loss: 0.6325666904449463 = 0.008687645196914673 + 0.1 * 6.238790035247803
Epoch 820, val loss: 0.986503541469574
Epoch 830, training loss: 0.6313955783843994 = 0.008436509408056736 + 0.1 * 6.229590892791748
Epoch 830, val loss: 0.9909626245498657
Epoch 840, training loss: 0.6301578283309937 = 0.008197258226573467 + 0.1 * 6.219605922698975
Epoch 840, val loss: 0.9955260157585144
Epoch 850, training loss: 0.6332641839981079 = 0.007968864403665066 + 0.1 * 6.252953052520752
Epoch 850, val loss: 0.9998332858085632
Epoch 860, training loss: 0.6298606991767883 = 0.007750113029032946 + 0.1 * 6.221106052398682
Epoch 860, val loss: 1.004013180732727
Epoch 870, training loss: 0.6289015412330627 = 0.0075422851368784904 + 0.1 * 6.213592052459717
Epoch 870, val loss: 1.0084004402160645
Epoch 880, training loss: 0.6306039094924927 = 0.007343647535890341 + 0.1 * 6.232602596282959
Epoch 880, val loss: 1.0126045942306519
Epoch 890, training loss: 0.6286506652832031 = 0.007153048645704985 + 0.1 * 6.2149763107299805
Epoch 890, val loss: 1.016544222831726
Epoch 900, training loss: 0.6274502277374268 = 0.006971017457544804 + 0.1 * 6.204792022705078
Epoch 900, val loss: 1.020695447921753
Epoch 910, training loss: 0.6286290884017944 = 0.006796370726078749 + 0.1 * 6.218327045440674
Epoch 910, val loss: 1.024658441543579
Epoch 920, training loss: 0.6264988780021667 = 0.006628410890698433 + 0.1 * 6.198704719543457
Epoch 920, val loss: 1.028327465057373
Epoch 930, training loss: 0.6262481212615967 = 0.006468294188380241 + 0.1 * 6.197798252105713
Epoch 930, val loss: 1.0322750806808472
Epoch 940, training loss: 0.6274557709693909 = 0.0063142613507807255 + 0.1 * 6.211414813995361
Epoch 940, val loss: 1.0360490083694458
Epoch 950, training loss: 0.625961422920227 = 0.006166011095046997 + 0.1 * 6.197954177856445
Epoch 950, val loss: 1.0396537780761719
Epoch 960, training loss: 0.6263730525970459 = 0.0060242218896746635 + 0.1 * 6.203487873077393
Epoch 960, val loss: 1.0433266162872314
Epoch 970, training loss: 0.6249973177909851 = 0.005887176841497421 + 0.1 * 6.191101551055908
Epoch 970, val loss: 1.0469653606414795
Epoch 980, training loss: 0.6248254776000977 = 0.005755794234573841 + 0.1 * 6.190696716308594
Epoch 980, val loss: 1.0505200624465942
Epoch 990, training loss: 0.6253774166107178 = 0.005628654733300209 + 0.1 * 6.1974873542785645
Epoch 990, val loss: 1.0540268421173096
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9373
Flip ASR: 0.9244/225 nodes
The final ASR:0.81058, 0.09239, Accuracy:0.81111, 0.01889
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9438])
updated graph: torch.Size([2, 10472])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.99016, 0.01141, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.79443359375 = 1.9570482969284058 + 0.1 * 8.373851776123047
Epoch 0, val loss: 1.9627798795700073
Epoch 10, training loss: 2.7840464115142822 = 1.9466782808303833 + 0.1 * 8.373682022094727
Epoch 10, val loss: 1.9521794319152832
Epoch 20, training loss: 2.7713303565979004 = 1.9340587854385376 + 0.1 * 8.372716903686523
Epoch 20, val loss: 1.9389464855194092
Epoch 30, training loss: 2.753082275390625 = 1.9165090322494507 + 0.1 * 8.36573314666748
Epoch 30, val loss: 1.9204111099243164
Epoch 40, training loss: 2.722357749938965 = 1.8904014825820923 + 0.1 * 8.319561958312988
Epoch 40, val loss: 1.8932019472122192
Epoch 50, training loss: 2.653494358062744 = 1.8543387651443481 + 0.1 * 7.991556167602539
Epoch 50, val loss: 1.8575810194015503
Epoch 60, training loss: 2.5656578540802 = 1.8160947561264038 + 0.1 * 7.495630741119385
Epoch 60, val loss: 1.822502851486206
Epoch 70, training loss: 2.4877030849456787 = 1.7818164825439453 + 0.1 * 7.058866024017334
Epoch 70, val loss: 1.7928617000579834
Epoch 80, training loss: 2.429760456085205 = 1.7478512525558472 + 0.1 * 6.819092273712158
Epoch 80, val loss: 1.7649072408676147
Epoch 90, training loss: 2.3785476684570312 = 1.7064756155014038 + 0.1 * 6.720720291137695
Epoch 90, val loss: 1.7298212051391602
Epoch 100, training loss: 2.3183374404907227 = 1.6508760452270508 + 0.1 * 6.674612998962402
Epoch 100, val loss: 1.682224154472351
Epoch 110, training loss: 2.242889404296875 = 1.5786733627319336 + 0.1 * 6.642160415649414
Epoch 110, val loss: 1.622074842453003
Epoch 120, training loss: 2.152977228164673 = 1.491373896598816 + 0.1 * 6.616032600402832
Epoch 120, val loss: 1.5501644611358643
Epoch 130, training loss: 2.0545952320098877 = 1.3951079845428467 + 0.1 * 6.594871997833252
Epoch 130, val loss: 1.471788763999939
Epoch 140, training loss: 1.9527604579925537 = 1.2952039241790771 + 0.1 * 6.575565338134766
Epoch 140, val loss: 1.392502784729004
Epoch 150, training loss: 1.8510761260986328 = 1.1954632997512817 + 0.1 * 6.55612850189209
Epoch 150, val loss: 1.316353440284729
Epoch 160, training loss: 1.75498628616333 = 1.1008459329605103 + 0.1 * 6.541403770446777
Epoch 160, val loss: 1.246399998664856
Epoch 170, training loss: 1.6668339967727661 = 1.014580249786377 + 0.1 * 6.5225372314453125
Epoch 170, val loss: 1.1842656135559082
Epoch 180, training loss: 1.5877511501312256 = 0.936328649520874 + 0.1 * 6.514224052429199
Epoch 180, val loss: 1.128831386566162
Epoch 190, training loss: 1.516037940979004 = 0.8665945529937744 + 0.1 * 6.494433403015137
Epoch 190, val loss: 1.0803780555725098
Epoch 200, training loss: 1.450585126876831 = 0.8024470806121826 + 0.1 * 6.481380462646484
Epoch 200, val loss: 1.035752534866333
Epoch 210, training loss: 1.389845848083496 = 0.7424324154853821 + 0.1 * 6.474133491516113
Epoch 210, val loss: 0.9949433207511902
Epoch 220, training loss: 1.3325350284576416 = 0.6865257024765015 + 0.1 * 6.460093021392822
Epoch 220, val loss: 0.9581170082092285
Epoch 230, training loss: 1.2797207832336426 = 0.6340275406837463 + 0.1 * 6.45693302154541
Epoch 230, val loss: 0.9248287081718445
Epoch 240, training loss: 1.229435920715332 = 0.5850009918212891 + 0.1 * 6.4443488121032715
Epoch 240, val loss: 0.8955237865447998
Epoch 250, training loss: 1.182781457901001 = 0.5389695763587952 + 0.1 * 6.438119411468506
Epoch 250, val loss: 0.8694719076156616
Epoch 260, training loss: 1.1378108263015747 = 0.4954914450645447 + 0.1 * 6.423193454742432
Epoch 260, val loss: 0.846714437007904
Epoch 270, training loss: 1.097368597984314 = 0.45390835404396057 + 0.1 * 6.4346022605896
Epoch 270, val loss: 0.8265276551246643
Epoch 280, training loss: 1.055420160293579 = 0.41426369547843933 + 0.1 * 6.411564350128174
Epoch 280, val loss: 0.8090121150016785
Epoch 290, training loss: 1.0169434547424316 = 0.3765682578086853 + 0.1 * 6.403751373291016
Epoch 290, val loss: 0.7939360737800598
Epoch 300, training loss: 0.9821968674659729 = 0.3416749835014343 + 0.1 * 6.405218601226807
Epoch 300, val loss: 0.781815767288208
Epoch 310, training loss: 0.9484528303146362 = 0.30987805128097534 + 0.1 * 6.385747909545898
Epoch 310, val loss: 0.772818922996521
Epoch 320, training loss: 0.9186878204345703 = 0.281128466129303 + 0.1 * 6.375593185424805
Epoch 320, val loss: 0.7672730684280396
Epoch 330, training loss: 0.8930717706680298 = 0.255228728055954 + 0.1 * 6.378430366516113
Epoch 330, val loss: 0.7649374008178711
Epoch 340, training loss: 0.8683670163154602 = 0.23205269873142242 + 0.1 * 6.363142967224121
Epoch 340, val loss: 0.7654032111167908
Epoch 350, training loss: 0.8468199372291565 = 0.21107923984527588 + 0.1 * 6.357407093048096
Epoch 350, val loss: 0.768082857131958
Epoch 360, training loss: 0.8280201554298401 = 0.1919487714767456 + 0.1 * 6.360713481903076
Epoch 360, val loss: 0.7726601362228394
Epoch 370, training loss: 0.8090521097183228 = 0.17455893754959106 + 0.1 * 6.344931602478027
Epoch 370, val loss: 0.7786859273910522
Epoch 380, training loss: 0.7921949625015259 = 0.15867993235588074 + 0.1 * 6.335149765014648
Epoch 380, val loss: 0.7858622074127197
Epoch 390, training loss: 0.7798190116882324 = 0.14423200488090515 + 0.1 * 6.355869770050049
Epoch 390, val loss: 0.7941192388534546
Epoch 400, training loss: 0.7640665769577026 = 0.13122306764125824 + 0.1 * 6.328434944152832
Epoch 400, val loss: 0.8031832575798035
Epoch 410, training loss: 0.75129234790802 = 0.11946864426136017 + 0.1 * 6.3182373046875
Epoch 410, val loss: 0.8127158880233765
Epoch 420, training loss: 0.741325855255127 = 0.1088641807436943 + 0.1 * 6.324616432189941
Epoch 420, val loss: 0.822754442691803
Epoch 430, training loss: 0.7304354906082153 = 0.09934753179550171 + 0.1 * 6.310879230499268
Epoch 430, val loss: 0.8330507874488831
Epoch 440, training loss: 0.7211100459098816 = 0.09082179516553879 + 0.1 * 6.302882194519043
Epoch 440, val loss: 0.843194842338562
Epoch 450, training loss: 0.7127283215522766 = 0.08319360762834549 + 0.1 * 6.295347213745117
Epoch 450, val loss: 0.8535217046737671
Epoch 460, training loss: 0.7052948474884033 = 0.07633265107870102 + 0.1 * 6.289621829986572
Epoch 460, val loss: 0.8636059165000916
Epoch 470, training loss: 0.6998370289802551 = 0.07015492022037506 + 0.1 * 6.296821117401123
Epoch 470, val loss: 0.8737142086029053
Epoch 480, training loss: 0.6943549513816833 = 0.06462383270263672 + 0.1 * 6.297310829162598
Epoch 480, val loss: 0.8835091590881348
Epoch 490, training loss: 0.6878413558006287 = 0.0596691258251667 + 0.1 * 6.281722068786621
Epoch 490, val loss: 0.8932424783706665
Epoch 500, training loss: 0.6847038269042969 = 0.05520416423678398 + 0.1 * 6.294996738433838
Epoch 500, val loss: 0.9026838541030884
Epoch 510, training loss: 0.6777804493904114 = 0.05118931457400322 + 0.1 * 6.26591157913208
Epoch 510, val loss: 0.9121476411819458
Epoch 520, training loss: 0.6741481423377991 = 0.047559283673763275 + 0.1 * 6.265888690948486
Epoch 520, val loss: 0.9213531613349915
Epoch 530, training loss: 0.6709074974060059 = 0.04428127780556679 + 0.1 * 6.266262054443359
Epoch 530, val loss: 0.9303472638130188
Epoch 540, training loss: 0.6667875051498413 = 0.04132380336523056 + 0.1 * 6.254637241363525
Epoch 540, val loss: 0.9394108057022095
Epoch 550, training loss: 0.6645411849021912 = 0.03863484784960747 + 0.1 * 6.259063243865967
Epoch 550, val loss: 0.9481844305992126
Epoch 560, training loss: 0.6616310477256775 = 0.03619169071316719 + 0.1 * 6.254393577575684
Epoch 560, val loss: 0.9568132162094116
Epoch 570, training loss: 0.6586145162582397 = 0.03396419435739517 + 0.1 * 6.246502876281738
Epoch 570, val loss: 0.9654210805892944
Epoch 580, training loss: 0.6560531854629517 = 0.031928543001413345 + 0.1 * 6.241246223449707
Epoch 580, val loss: 0.9735730290412903
Epoch 590, training loss: 0.6541429758071899 = 0.03006867691874504 + 0.1 * 6.2407426834106445
Epoch 590, val loss: 0.9818730354309082
Epoch 600, training loss: 0.6523625254631042 = 0.028364447876811028 + 0.1 * 6.239980697631836
Epoch 600, val loss: 0.9897249341011047
Epoch 610, training loss: 0.6493731737136841 = 0.02680121175944805 + 0.1 * 6.225719451904297
Epoch 610, val loss: 0.9976592063903809
Epoch 620, training loss: 0.6480404138565063 = 0.025357812643051147 + 0.1 * 6.226825714111328
Epoch 620, val loss: 1.0053657293319702
Epoch 630, training loss: 0.6465665102005005 = 0.024022908881306648 + 0.1 * 6.225435733795166
Epoch 630, val loss: 1.0128628015518188
Epoch 640, training loss: 0.6456114053726196 = 0.022788967937231064 + 0.1 * 6.228224754333496
Epoch 640, val loss: 1.0202656984329224
Epoch 650, training loss: 0.6446030735969543 = 0.021647684276103973 + 0.1 * 6.229553699493408
Epoch 650, val loss: 1.027524471282959
Epoch 660, training loss: 0.6422962546348572 = 0.020591817796230316 + 0.1 * 6.217044353485107
Epoch 660, val loss: 1.0345652103424072
Epoch 670, training loss: 0.6440882682800293 = 0.0196109339594841 + 0.1 * 6.2447733879089355
Epoch 670, val loss: 1.0415312051773071
Epoch 680, training loss: 0.6401782631874084 = 0.018703510984778404 + 0.1 * 6.214747428894043
Epoch 680, val loss: 1.0482138395309448
Epoch 690, training loss: 0.6384940147399902 = 0.01785835064947605 + 0.1 * 6.206356525421143
Epoch 690, val loss: 1.0550190210342407
Epoch 700, training loss: 0.6375294923782349 = 0.01706908456981182 + 0.1 * 6.204603672027588
Epoch 700, val loss: 1.0615283250808716
Epoch 710, training loss: 0.6369258761405945 = 0.01633177325129509 + 0.1 * 6.2059407234191895
Epoch 710, val loss: 1.0677509307861328
Epoch 720, training loss: 0.6355286836624146 = 0.015643034130334854 + 0.1 * 6.198856353759766
Epoch 720, val loss: 1.0740598440170288
Epoch 730, training loss: 0.6351320743560791 = 0.014997498132288456 + 0.1 * 6.201345920562744
Epoch 730, val loss: 1.0802868604660034
Epoch 740, training loss: 0.636044442653656 = 0.014391802251338959 + 0.1 * 6.216526508331299
Epoch 740, val loss: 1.0860610008239746
Epoch 750, training loss: 0.6334681510925293 = 0.01382527593523264 + 0.1 * 6.196428298950195
Epoch 750, val loss: 1.0919510126113892
Epoch 760, training loss: 0.6323116421699524 = 0.013292892836034298 + 0.1 * 6.190187454223633
Epoch 760, val loss: 1.0978463888168335
Epoch 770, training loss: 0.633701741695404 = 0.012791368179023266 + 0.1 * 6.209103107452393
Epoch 770, val loss: 1.1032686233520508
Epoch 780, training loss: 0.6311134696006775 = 0.012319342233240604 + 0.1 * 6.187941551208496
Epoch 780, val loss: 1.108741283416748
Epoch 790, training loss: 0.6303547024726868 = 0.011874768882989883 + 0.1 * 6.184798717498779
Epoch 790, val loss: 1.1143006086349487
Epoch 800, training loss: 0.6302058100700378 = 0.011453579179942608 + 0.1 * 6.1875224113464355
Epoch 800, val loss: 1.1196088790893555
Epoch 810, training loss: 0.6290183663368225 = 0.011055628769099712 + 0.1 * 6.179626941680908
Epoch 810, val loss: 1.1247119903564453
Epoch 820, training loss: 0.6289806962013245 = 0.010679700411856174 + 0.1 * 6.183009624481201
Epoch 820, val loss: 1.1298894882202148
Epoch 830, training loss: 0.6283677816390991 = 0.010323173366487026 + 0.1 * 6.180446147918701
Epoch 830, val loss: 1.1348059177398682
Epoch 840, training loss: 0.628075361251831 = 0.009985656477510929 + 0.1 * 6.180896759033203
Epoch 840, val loss: 1.1397018432617188
Epoch 850, training loss: 0.6298389434814453 = 0.009666532278060913 + 0.1 * 6.201724052429199
Epoch 850, val loss: 1.1446682214736938
Epoch 860, training loss: 0.626894474029541 = 0.009363108314573765 + 0.1 * 6.175313472747803
Epoch 860, val loss: 1.1493366956710815
Epoch 870, training loss: 0.6260355710983276 = 0.009075288660824299 + 0.1 * 6.169602870941162
Epoch 870, val loss: 1.154159665107727
Epoch 880, training loss: 0.6269169449806213 = 0.008800849318504333 + 0.1 * 6.181160926818848
Epoch 880, val loss: 1.1586854457855225
Epoch 890, training loss: 0.6259263157844543 = 0.008539609611034393 + 0.1 * 6.1738667488098145
Epoch 890, val loss: 1.163153052330017
Epoch 900, training loss: 0.6271479725837708 = 0.008291248232126236 + 0.1 * 6.188567161560059
Epoch 900, val loss: 1.1676318645477295
Epoch 910, training loss: 0.624845564365387 = 0.008054165169596672 + 0.1 * 6.167914390563965
Epoch 910, val loss: 1.1719132661819458
Epoch 920, training loss: 0.6241697669029236 = 0.007828561589121819 + 0.1 * 6.163412094116211
Epoch 920, val loss: 1.1763843297958374
Epoch 930, training loss: 0.6247733235359192 = 0.007612504530698061 + 0.1 * 6.171607971191406
Epoch 930, val loss: 1.1806062459945679
Epoch 940, training loss: 0.6246181130409241 = 0.007405835669487715 + 0.1 * 6.172122955322266
Epoch 940, val loss: 1.1847244501113892
Epoch 950, training loss: 0.6237782835960388 = 0.007208577357232571 + 0.1 * 6.165696620941162
Epoch 950, val loss: 1.1887468099594116
Epoch 960, training loss: 0.6227331757545471 = 0.007020312827080488 + 0.1 * 6.157128810882568
Epoch 960, val loss: 1.192832589149475
Epoch 970, training loss: 0.622454047203064 = 0.00684006791561842 + 0.1 * 6.156139850616455
Epoch 970, val loss: 1.1969419717788696
Epoch 980, training loss: 0.6228041648864746 = 0.0066666919738054276 + 0.1 * 6.161374568939209
Epoch 980, val loss: 1.2008960247039795
Epoch 990, training loss: 0.6218409538269043 = 0.006499797105789185 + 0.1 * 6.153410911560059
Epoch 990, val loss: 1.2045505046844482
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.5609
Flip ASR: 0.4844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.780836582183838 = 1.9434518814086914 + 0.1 * 8.373846054077148
Epoch 0, val loss: 1.9404267072677612
Epoch 10, training loss: 2.7704029083251953 = 1.933056116104126 + 0.1 * 8.373467445373535
Epoch 10, val loss: 1.9295573234558105
Epoch 20, training loss: 2.7577271461486816 = 1.9205286502838135 + 0.1 * 8.37198543548584
Epoch 20, val loss: 1.9157805442810059
Epoch 30, training loss: 2.739713191986084 = 1.903239369392395 + 0.1 * 8.364738464355469
Epoch 30, val loss: 1.8962002992630005
Epoch 40, training loss: 2.709317684173584 = 1.8782070875167847 + 0.1 * 8.31110668182373
Epoch 40, val loss: 1.8681588172912598
Epoch 50, training loss: 2.632606029510498 = 1.8448030948638916 + 0.1 * 7.878028392791748
Epoch 50, val loss: 1.8327476978302002
Epoch 60, training loss: 2.5528130531311035 = 1.8091998100280762 + 0.1 * 7.436131477355957
Epoch 60, val loss: 1.7969281673431396
Epoch 70, training loss: 2.48646879196167 = 1.7709592580795288 + 0.1 * 7.155095100402832
Epoch 70, val loss: 1.7611627578735352
Epoch 80, training loss: 2.4253923892974854 = 1.7304691076278687 + 0.1 * 6.94923210144043
Epoch 80, val loss: 1.726168155670166
Epoch 90, training loss: 2.366760492324829 = 1.682828664779663 + 0.1 * 6.839317321777344
Epoch 90, val loss: 1.684706449508667
Epoch 100, training loss: 2.2950592041015625 = 1.6185420751571655 + 0.1 * 6.765172004699707
Epoch 100, val loss: 1.6282073259353638
Epoch 110, training loss: 2.2071924209594727 = 1.5364068746566772 + 0.1 * 6.707854747772217
Epoch 110, val loss: 1.5597002506256104
Epoch 120, training loss: 2.109683036804199 = 1.4428625106811523 + 0.1 * 6.668204307556152
Epoch 120, val loss: 1.4849224090576172
Epoch 130, training loss: 2.013951301574707 = 1.3495808839797974 + 0.1 * 6.643705368041992
Epoch 130, val loss: 1.412890076637268
Epoch 140, training loss: 1.9259006977081299 = 1.2637273073196411 + 0.1 * 6.62173318862915
Epoch 140, val loss: 1.34908127784729
Epoch 150, training loss: 1.847102403640747 = 1.186599850654602 + 0.1 * 6.605025291442871
Epoch 150, val loss: 1.2944495677947998
Epoch 160, training loss: 1.7732510566711426 = 1.1144404411315918 + 0.1 * 6.58810567855835
Epoch 160, val loss: 1.2440621852874756
Epoch 170, training loss: 1.700425624847412 = 1.0428810119628906 + 0.1 * 6.575445175170898
Epoch 170, val loss: 1.1939592361450195
Epoch 180, training loss: 1.6260684728622437 = 0.9699456691741943 + 0.1 * 6.561227798461914
Epoch 180, val loss: 1.1424070596694946
Epoch 190, training loss: 1.5503058433532715 = 0.8955333232879639 + 0.1 * 6.547725200653076
Epoch 190, val loss: 1.0889579057693481
Epoch 200, training loss: 1.4770822525024414 = 0.8233175277709961 + 0.1 * 6.537647724151611
Epoch 200, val loss: 1.0371047258377075
Epoch 210, training loss: 1.4085159301757812 = 0.7568674087524414 + 0.1 * 6.516485691070557
Epoch 210, val loss: 0.9900332093238831
Epoch 220, training loss: 1.3484547138214111 = 0.697723388671875 + 0.1 * 6.507312774658203
Epoch 220, val loss: 0.9490651488304138
Epoch 230, training loss: 1.296565294265747 = 0.6469671726226807 + 0.1 * 6.495980262756348
Epoch 230, val loss: 0.9156074523925781
Epoch 240, training loss: 1.2500605583190918 = 0.6019216179847717 + 0.1 * 6.481388568878174
Epoch 240, val loss: 0.887363612651825
Epoch 250, training loss: 1.2076213359832764 = 0.5608968734741211 + 0.1 * 6.4672441482543945
Epoch 250, val loss: 0.8635337352752686
Epoch 260, training loss: 1.171266794204712 = 0.5228760838508606 + 0.1 * 6.483906269073486
Epoch 260, val loss: 0.8434256315231323
Epoch 270, training loss: 1.1332191228866577 = 0.48777976632118225 + 0.1 * 6.45439338684082
Epoch 270, val loss: 0.8266357183456421
Epoch 280, training loss: 1.099036693572998 = 0.45426905155181885 + 0.1 * 6.447676658630371
Epoch 280, val loss: 0.8122218251228333
Epoch 290, training loss: 1.0645947456359863 = 0.4222903251647949 + 0.1 * 6.423043727874756
Epoch 290, val loss: 0.8001280426979065
Epoch 300, training loss: 1.033089280128479 = 0.39138880372047424 + 0.1 * 6.4170050621032715
Epoch 300, val loss: 0.7901726961135864
Epoch 310, training loss: 1.0035161972045898 = 0.3617640435695648 + 0.1 * 6.4175214767456055
Epoch 310, val loss: 0.7825802564620972
Epoch 320, training loss: 0.9746469259262085 = 0.3339698016643524 + 0.1 * 6.406771183013916
Epoch 320, val loss: 0.7777249813079834
Epoch 330, training loss: 0.947628378868103 = 0.3085486590862274 + 0.1 * 6.3907976150512695
Epoch 330, val loss: 0.7757012248039246
Epoch 340, training loss: 0.9246912002563477 = 0.28561243414878845 + 0.1 * 6.3907880783081055
Epoch 340, val loss: 0.7760712504386902
Epoch 350, training loss: 0.9024585485458374 = 0.2650851905345917 + 0.1 * 6.3737335205078125
Epoch 350, val loss: 0.7785483598709106
Epoch 360, training loss: 0.8839752674102783 = 0.24633543193340302 + 0.1 * 6.376398086547852
Epoch 360, val loss: 0.7824153304100037
Epoch 370, training loss: 0.8653382062911987 = 0.22885815799236298 + 0.1 * 6.364799976348877
Epoch 370, val loss: 0.7869914174079895
Epoch 380, training loss: 0.8472909927368164 = 0.21223337948322296 + 0.1 * 6.350575923919678
Epoch 380, val loss: 0.7920079231262207
Epoch 390, training loss: 0.8313066363334656 = 0.19627749919891357 + 0.1 * 6.3502912521362305
Epoch 390, val loss: 0.7973591685295105
Epoch 400, training loss: 0.8168109059333801 = 0.18114210665225983 + 0.1 * 6.356688022613525
Epoch 400, val loss: 0.8028849363327026
Epoch 410, training loss: 0.8005958199501038 = 0.1669187992811203 + 0.1 * 6.336770057678223
Epoch 410, val loss: 0.8086552023887634
Epoch 420, training loss: 0.7861393690109253 = 0.15366430580615997 + 0.1 * 6.324750900268555
Epoch 420, val loss: 0.8147708177566528
Epoch 430, training loss: 0.7753767967224121 = 0.14152128994464874 + 0.1 * 6.338554859161377
Epoch 430, val loss: 0.8214409947395325
Epoch 440, training loss: 0.7624973058700562 = 0.13056908547878265 + 0.1 * 6.319282054901123
Epoch 440, val loss: 0.8286342620849609
Epoch 450, training loss: 0.7523634433746338 = 0.12071117758750916 + 0.1 * 6.316522121429443
Epoch 450, val loss: 0.8363602757453918
Epoch 460, training loss: 0.7424314022064209 = 0.11185619980096817 + 0.1 * 6.305751800537109
Epoch 460, val loss: 0.8445618748664856
Epoch 470, training loss: 0.7336786389350891 = 0.10387376695871353 + 0.1 * 6.298048496246338
Epoch 470, val loss: 0.8531676530838013
Epoch 480, training loss: 0.7277867197990417 = 0.09664589911699295 + 0.1 * 6.311408042907715
Epoch 480, val loss: 0.8621982336044312
Epoch 490, training loss: 0.7198882699012756 = 0.09009375423192978 + 0.1 * 6.297945022583008
Epoch 490, val loss: 0.8714691400527954
Epoch 500, training loss: 0.7130036950111389 = 0.08412610739469528 + 0.1 * 6.28877592086792
Epoch 500, val loss: 0.8810785412788391
Epoch 510, training loss: 0.7066583633422852 = 0.07867049425840378 + 0.1 * 6.279878616333008
Epoch 510, val loss: 0.8908478021621704
Epoch 520, training loss: 0.7016103863716125 = 0.07365049421787262 + 0.1 * 6.279598712921143
Epoch 520, val loss: 0.9006592035293579
Epoch 530, training loss: 0.6961073875427246 = 0.06903213262557983 + 0.1 * 6.270752429962158
Epoch 530, val loss: 0.9107082486152649
Epoch 540, training loss: 0.6922533512115479 = 0.06475947052240372 + 0.1 * 6.274938583374023
Epoch 540, val loss: 0.9207720756530762
Epoch 550, training loss: 0.6880106329917908 = 0.06081875413656235 + 0.1 * 6.271918773651123
Epoch 550, val loss: 0.9307307600975037
Epoch 560, training loss: 0.6850733757019043 = 0.057219747453927994 + 0.1 * 6.278536319732666
Epoch 560, val loss: 0.9405523538589478
Epoch 570, training loss: 0.6795937418937683 = 0.053967222571372986 + 0.1 * 6.256264686584473
Epoch 570, val loss: 0.9501072764396667
Epoch 580, training loss: 0.6765106320381165 = 0.050993576645851135 + 0.1 * 6.255170822143555
Epoch 580, val loss: 0.9595574140548706
Epoch 590, training loss: 0.6730395555496216 = 0.048277441412210464 + 0.1 * 6.247621059417725
Epoch 590, val loss: 0.9692184925079346
Epoch 600, training loss: 0.6727340221405029 = 0.045791905373334885 + 0.1 * 6.269421100616455
Epoch 600, val loss: 0.9784566164016724
Epoch 610, training loss: 0.6678462624549866 = 0.043525516986846924 + 0.1 * 6.2432074546813965
Epoch 610, val loss: 0.9875264763832092
Epoch 620, training loss: 0.665479838848114 = 0.04144148901104927 + 0.1 * 6.240383148193359
Epoch 620, val loss: 0.9966090321540833
Epoch 630, training loss: 0.6633482575416565 = 0.03950643539428711 + 0.1 * 6.238418102264404
Epoch 630, val loss: 1.0054789781570435
Epoch 640, training loss: 0.6614450216293335 = 0.03769417479634285 + 0.1 * 6.237508773803711
Epoch 640, val loss: 1.0141741037368774
Epoch 650, training loss: 0.6599161028862 = 0.036002568900585175 + 0.1 * 6.239135265350342
Epoch 650, val loss: 1.0228124856948853
Epoch 660, training loss: 0.6576518416404724 = 0.03442750126123428 + 0.1 * 6.232243061065674
Epoch 660, val loss: 1.0312187671661377
Epoch 670, training loss: 0.6576943397521973 = 0.0329560711979866 + 0.1 * 6.247382640838623
Epoch 670, val loss: 1.0394052267074585
Epoch 680, training loss: 0.654419481754303 = 0.03158196434378624 + 0.1 * 6.228375434875488
Epoch 680, val loss: 1.047499418258667
Epoch 690, training loss: 0.6525604128837585 = 0.030291657894849777 + 0.1 * 6.222687244415283
Epoch 690, val loss: 1.0555272102355957
Epoch 700, training loss: 0.6528663635253906 = 0.029076118022203445 + 0.1 * 6.237902641296387
Epoch 700, val loss: 1.0632177591323853
Epoch 710, training loss: 0.6491116285324097 = 0.027926236391067505 + 0.1 * 6.211853981018066
Epoch 710, val loss: 1.0708534717559814
Epoch 720, training loss: 0.647784948348999 = 0.02682427316904068 + 0.1 * 6.209606170654297
Epoch 720, val loss: 1.0783740282058716
Epoch 730, training loss: 0.64728182554245 = 0.025769684463739395 + 0.1 * 6.215120792388916
Epoch 730, val loss: 1.0855858325958252
Epoch 740, training loss: 0.6459865570068359 = 0.02476269192993641 + 0.1 * 6.212238311767578
Epoch 740, val loss: 1.0927492380142212
Epoch 750, training loss: 0.6448877453804016 = 0.023801445960998535 + 0.1 * 6.210862636566162
Epoch 750, val loss: 1.099698781967163
Epoch 760, training loss: 0.6435271501541138 = 0.02287793904542923 + 0.1 * 6.206491947174072
Epoch 760, val loss: 1.1067081689834595
Epoch 770, training loss: 0.6436380743980408 = 0.021983597427606583 + 0.1 * 6.216544151306152
Epoch 770, val loss: 1.1135635375976562
Epoch 780, training loss: 0.6412140727043152 = 0.021120550110936165 + 0.1 * 6.200934886932373
Epoch 780, val loss: 1.120168924331665
Epoch 790, training loss: 0.6411132216453552 = 0.020282290875911713 + 0.1 * 6.208309173583984
Epoch 790, val loss: 1.1266900300979614
Epoch 800, training loss: 0.6387512683868408 = 0.019475024193525314 + 0.1 * 6.19276237487793
Epoch 800, val loss: 1.133091926574707
Epoch 810, training loss: 0.6382710337638855 = 0.01870056241750717 + 0.1 * 6.195704936981201
Epoch 810, val loss: 1.1394206285476685
Epoch 820, training loss: 0.6379949450492859 = 0.017959721386432648 + 0.1 * 6.200352191925049
Epoch 820, val loss: 1.1454575061798096
Epoch 830, training loss: 0.6364728212356567 = 0.017252139747142792 + 0.1 * 6.192206382751465
Epoch 830, val loss: 1.151523232460022
Epoch 840, training loss: 0.6358895301818848 = 0.016569383442401886 + 0.1 * 6.193201541900635
Epoch 840, val loss: 1.1575103998184204
Epoch 850, training loss: 0.6340766549110413 = 0.01590864360332489 + 0.1 * 6.181679725646973
Epoch 850, val loss: 1.1632665395736694
Epoch 860, training loss: 0.6342034935951233 = 0.015266994945704937 + 0.1 * 6.189364433288574
Epoch 860, val loss: 1.16912043094635
Epoch 870, training loss: 0.6328818798065186 = 0.0146400798112154 + 0.1 * 6.182417869567871
Epoch 870, val loss: 1.1745860576629639
Epoch 880, training loss: 0.6320478916168213 = 0.014027928002178669 + 0.1 * 6.180199146270752
Epoch 880, val loss: 1.1801483631134033
Epoch 890, training loss: 0.6338757276535034 = 0.013432538136839867 + 0.1 * 6.204431533813477
Epoch 890, val loss: 1.1855666637420654
Epoch 900, training loss: 0.6309012770652771 = 0.0128748444840312 + 0.1 * 6.180263996124268
Epoch 900, val loss: 1.190704345703125
Epoch 910, training loss: 0.6303355097770691 = 0.012362525798380375 + 0.1 * 6.179729461669922
Epoch 910, val loss: 1.1959894895553589
Epoch 920, training loss: 0.6290851831436157 = 0.01189336646348238 + 0.1 * 6.171917915344238
Epoch 920, val loss: 1.2011111974716187
Epoch 930, training loss: 0.6303646564483643 = 0.011457993648946285 + 0.1 * 6.189066410064697
Epoch 930, val loss: 1.206162452697754
Epoch 940, training loss: 0.6282243728637695 = 0.011050031520426273 + 0.1 * 6.171742916107178
Epoch 940, val loss: 1.2110605239868164
Epoch 950, training loss: 0.6276369690895081 = 0.010665211826562881 + 0.1 * 6.169717311859131
Epoch 950, val loss: 1.2160348892211914
Epoch 960, training loss: 0.6273398995399475 = 0.010300887748599052 + 0.1 * 6.1703901290893555
Epoch 960, val loss: 1.2207850217819214
Epoch 970, training loss: 0.6270710825920105 = 0.00995601061731577 + 0.1 * 6.1711506843566895
Epoch 970, val loss: 1.2255322933197021
Epoch 980, training loss: 0.6274757981300354 = 0.009627852588891983 + 0.1 * 6.178479194641113
Epoch 980, val loss: 1.230170726776123
Epoch 990, training loss: 0.6258614659309387 = 0.009315120056271553 + 0.1 * 6.165462970733643
Epoch 990, val loss: 1.2346115112304688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.4096
Flip ASR: 0.4000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7682301998138428 = 1.9308478832244873 + 0.1 * 8.373822212219238
Epoch 0, val loss: 1.9297704696655273
Epoch 10, training loss: 2.758821964263916 = 1.9214612245559692 + 0.1 * 8.373607635498047
Epoch 10, val loss: 1.9211738109588623
Epoch 20, training loss: 2.7472000122070312 = 1.9099599123001099 + 0.1 * 8.372400283813477
Epoch 20, val loss: 1.9103375673294067
Epoch 30, training loss: 2.7302956581115723 = 1.893921136856079 + 0.1 * 8.363744735717773
Epoch 30, val loss: 1.8949720859527588
Epoch 40, training loss: 2.700979471206665 = 1.870419979095459 + 0.1 * 8.305594444274902
Epoch 40, val loss: 1.872801661491394
Epoch 50, training loss: 2.6322596073150635 = 1.8386081457138062 + 0.1 * 7.9365153312683105
Epoch 50, val loss: 1.8439948558807373
Epoch 60, training loss: 2.556180238723755 = 1.802871823310852 + 0.1 * 7.533083915710449
Epoch 60, val loss: 1.8126163482666016
Epoch 70, training loss: 2.4860787391662598 = 1.766445517539978 + 0.1 * 7.196331024169922
Epoch 70, val loss: 1.7807368040084839
Epoch 80, training loss: 2.4216885566711426 = 1.7268428802490234 + 0.1 * 6.948456287384033
Epoch 80, val loss: 1.7456045150756836
Epoch 90, training loss: 2.353642463684082 = 1.6747782230377197 + 0.1 * 6.788641452789307
Epoch 90, val loss: 1.6974824666976929
Epoch 100, training loss: 2.275625228881836 = 1.60493004322052 + 0.1 * 6.706950664520264
Epoch 100, val loss: 1.6351038217544556
Epoch 110, training loss: 2.184098720550537 = 1.5179176330566406 + 0.1 * 6.661810874938965
Epoch 110, val loss: 1.560282826423645
Epoch 120, training loss: 2.085479259490967 = 1.422031283378601 + 0.1 * 6.6344804763793945
Epoch 120, val loss: 1.4788790941238403
Epoch 130, training loss: 1.9862589836120605 = 1.3248822689056396 + 0.1 * 6.613766193389893
Epoch 130, val loss: 1.3989676237106323
Epoch 140, training loss: 1.8913096189498901 = 1.2312324047088623 + 0.1 * 6.600771903991699
Epoch 140, val loss: 1.3254921436309814
Epoch 150, training loss: 1.801946997642517 = 1.1428852081298828 + 0.1 * 6.590617656707764
Epoch 150, val loss: 1.2590739727020264
Epoch 160, training loss: 1.7154722213745117 = 1.0567536354064941 + 0.1 * 6.587185859680176
Epoch 160, val loss: 1.1962804794311523
Epoch 170, training loss: 1.6278557777404785 = 0.9702960252761841 + 0.1 * 6.575597763061523
Epoch 170, val loss: 1.134020209312439
Epoch 180, training loss: 1.539109468460083 = 0.8823766112327576 + 0.1 * 6.567327976226807
Epoch 180, val loss: 1.07053804397583
Epoch 190, training loss: 1.4518349170684814 = 0.7957479357719421 + 0.1 * 6.560870170593262
Epoch 190, val loss: 1.007519006729126
Epoch 200, training loss: 1.3692142963409424 = 0.714403510093689 + 0.1 * 6.548107624053955
Epoch 200, val loss: 0.9484363794326782
Epoch 210, training loss: 1.2948235273361206 = 0.6408134698867798 + 0.1 * 6.540100574493408
Epoch 210, val loss: 0.8960186243057251
Epoch 220, training loss: 1.229257583618164 = 0.5764964818954468 + 0.1 * 6.527610778808594
Epoch 220, val loss: 0.8519691228866577
Epoch 230, training loss: 1.1719584465026855 = 0.5203185081481934 + 0.1 * 6.516399383544922
Epoch 230, val loss: 0.8154853582382202
Epoch 240, training loss: 1.1216570138931274 = 0.4710997939109802 + 0.1 * 6.5055718421936035
Epoch 240, val loss: 0.7858545780181885
Epoch 250, training loss: 1.0760349035263062 = 0.42672058939933777 + 0.1 * 6.493142604827881
Epoch 250, val loss: 0.760822594165802
Epoch 260, training loss: 1.0347307920455933 = 0.3856838047504425 + 0.1 * 6.490469455718994
Epoch 260, val loss: 0.7393811941146851
Epoch 270, training loss: 0.9954858422279358 = 0.34737950563430786 + 0.1 * 6.481063365936279
Epoch 270, val loss: 0.7210426926612854
Epoch 280, training loss: 0.9584349393844604 = 0.3113429844379425 + 0.1 * 6.470919609069824
Epoch 280, val loss: 0.7052561044692993
Epoch 290, training loss: 0.9234271049499512 = 0.27734437584877014 + 0.1 * 6.460827350616455
Epoch 290, val loss: 0.6917450428009033
Epoch 300, training loss: 0.8928524255752563 = 0.2457384318113327 + 0.1 * 6.471139907836914
Epoch 300, val loss: 0.6806035041809082
Epoch 310, training loss: 0.8630707263946533 = 0.21734097599983215 + 0.1 * 6.457296848297119
Epoch 310, val loss: 0.6720252633094788
Epoch 320, training loss: 0.8361455798149109 = 0.19201797246932983 + 0.1 * 6.4412760734558105
Epoch 320, val loss: 0.6656525135040283
Epoch 330, training loss: 0.8132854104042053 = 0.1696823686361313 + 0.1 * 6.43602991104126
Epoch 330, val loss: 0.6615085601806641
Epoch 340, training loss: 0.7937700748443604 = 0.15028288960456848 + 0.1 * 6.434871196746826
Epoch 340, val loss: 0.6595863699913025
Epoch 350, training loss: 0.775770902633667 = 0.13355594873428345 + 0.1 * 6.422149181365967
Epoch 350, val loss: 0.6597053408622742
Epoch 360, training loss: 0.7631898522377014 = 0.11907992511987686 + 0.1 * 6.441099166870117
Epoch 360, val loss: 0.6616154313087463
Epoch 370, training loss: 0.7478514909744263 = 0.10664152354001999 + 0.1 * 6.412099361419678
Epoch 370, val loss: 0.6648081541061401
Epoch 380, training loss: 0.7356424331665039 = 0.09583988785743713 + 0.1 * 6.3980255126953125
Epoch 380, val loss: 0.6692078709602356
Epoch 390, training loss: 0.7320635914802551 = 0.08640275150537491 + 0.1 * 6.456608295440674
Epoch 390, val loss: 0.6745479106903076
Epoch 400, training loss: 0.7171851992607117 = 0.07826804369688034 + 0.1 * 6.389171123504639
Epoch 400, val loss: 0.6805211901664734
Epoch 410, training loss: 0.709764838218689 = 0.07114525884389877 + 0.1 * 6.386195659637451
Epoch 410, val loss: 0.6869122982025146
Epoch 420, training loss: 0.7019940614700317 = 0.06485196948051453 + 0.1 * 6.371420383453369
Epoch 420, val loss: 0.6937806010246277
Epoch 430, training loss: 0.6957137584686279 = 0.059264976531267166 + 0.1 * 6.364488124847412
Epoch 430, val loss: 0.7010112404823303
Epoch 440, training loss: 0.6910372376441956 = 0.05430232733488083 + 0.1 * 6.367348670959473
Epoch 440, val loss: 0.7084124684333801
Epoch 450, training loss: 0.6860882639884949 = 0.049922239035367966 + 0.1 * 6.361660480499268
Epoch 450, val loss: 0.7158359289169312
Epoch 460, training loss: 0.68097323179245 = 0.04601479321718216 + 0.1 * 6.349584579467773
Epoch 460, val loss: 0.7232440114021301
Epoch 470, training loss: 0.6770998239517212 = 0.042511481791734695 + 0.1 * 6.345883369445801
Epoch 470, val loss: 0.7307084202766418
Epoch 480, training loss: 0.6741460561752319 = 0.039381153881549835 + 0.1 * 6.347649097442627
Epoch 480, val loss: 0.7381554245948792
Epoch 490, training loss: 0.670069694519043 = 0.0365799181163311 + 0.1 * 6.334897994995117
Epoch 490, val loss: 0.7455185055732727
Epoch 500, training loss: 0.668473482131958 = 0.03405246138572693 + 0.1 * 6.344209671020508
Epoch 500, val loss: 0.7527409195899963
Epoch 510, training loss: 0.6653250455856323 = 0.03178112953901291 + 0.1 * 6.3354387283325195
Epoch 510, val loss: 0.7598616480827332
Epoch 520, training loss: 0.6614673137664795 = 0.029728250578045845 + 0.1 * 6.317390441894531
Epoch 520, val loss: 0.7669259309768677
Epoch 530, training loss: 0.6594715118408203 = 0.027861779555678368 + 0.1 * 6.316097259521484
Epoch 530, val loss: 0.7738317847251892
Epoch 540, training loss: 0.6577531695365906 = 0.026166200637817383 + 0.1 * 6.3158698081970215
Epoch 540, val loss: 0.7806445360183716
Epoch 550, training loss: 0.6552861928939819 = 0.024624479934573174 + 0.1 * 6.306617259979248
Epoch 550, val loss: 0.7873416543006897
Epoch 560, training loss: 0.6560028791427612 = 0.02321355789899826 + 0.1 * 6.327893257141113
Epoch 560, val loss: 0.793846070766449
Epoch 570, training loss: 0.6521837711334229 = 0.021928707137703896 + 0.1 * 6.302550315856934
Epoch 570, val loss: 0.8001682162284851
Epoch 580, training loss: 0.6501659154891968 = 0.02075071446597576 + 0.1 * 6.294151782989502
Epoch 580, val loss: 0.806411623954773
Epoch 590, training loss: 0.6517527103424072 = 0.019666263833642006 + 0.1 * 6.320864200592041
Epoch 590, val loss: 0.8124578595161438
Epoch 600, training loss: 0.6477401256561279 = 0.01866990514099598 + 0.1 * 6.290701866149902
Epoch 600, val loss: 0.8183590173721313
Epoch 610, training loss: 0.6462889909744263 = 0.01775069162249565 + 0.1 * 6.2853827476501465
Epoch 610, val loss: 0.8241767883300781
Epoch 620, training loss: 0.6452555656433105 = 0.016899894922971725 + 0.1 * 6.283556938171387
Epoch 620, val loss: 0.8297938704490662
Epoch 630, training loss: 0.6444265246391296 = 0.016112668439745903 + 0.1 * 6.283138275146484
Epoch 630, val loss: 0.8353433012962341
Epoch 640, training loss: 0.642824649810791 = 0.015382050536572933 + 0.1 * 6.274425983428955
Epoch 640, val loss: 0.840804934501648
Epoch 650, training loss: 0.6427488327026367 = 0.01470169983804226 + 0.1 * 6.280470848083496
Epoch 650, val loss: 0.8461295962333679
Epoch 660, training loss: 0.6418952345848083 = 0.014068101532757282 + 0.1 * 6.278270721435547
Epoch 660, val loss: 0.8512662053108215
Epoch 670, training loss: 0.6400087475776672 = 0.013478856533765793 + 0.1 * 6.265298843383789
Epoch 670, val loss: 0.8564096093177795
Epoch 680, training loss: 0.6398695707321167 = 0.012927209958434105 + 0.1 * 6.269423007965088
Epoch 680, val loss: 0.8613618016242981
Epoch 690, training loss: 0.6386240720748901 = 0.012412844225764275 + 0.1 * 6.262112140655518
Epoch 690, val loss: 0.8661990165710449
Epoch 700, training loss: 0.6380804181098938 = 0.0119307991117239 + 0.1 * 6.261495590209961
Epoch 700, val loss: 0.8709820508956909
Epoch 710, training loss: 0.6363494992256165 = 0.011478322558104992 + 0.1 * 6.248712062835693
Epoch 710, val loss: 0.8755844235420227
Epoch 720, training loss: 0.6358340382575989 = 0.011053420603275299 + 0.1 * 6.247806072235107
Epoch 720, val loss: 0.880156934261322
Epoch 730, training loss: 0.6363946795463562 = 0.010652809403836727 + 0.1 * 6.257418632507324
Epoch 730, val loss: 0.8846357464790344
Epoch 740, training loss: 0.6348997950553894 = 0.010275996290147305 + 0.1 * 6.246237754821777
Epoch 740, val loss: 0.8890019655227661
Epoch 750, training loss: 0.6343207359313965 = 0.009921032935380936 + 0.1 * 6.243997097015381
Epoch 750, val loss: 0.893332839012146
Epoch 760, training loss: 0.6344423890113831 = 0.009584960527718067 + 0.1 * 6.248574256896973
Epoch 760, val loss: 0.8975439667701721
Epoch 770, training loss: 0.6331421732902527 = 0.009267086163163185 + 0.1 * 6.238750457763672
Epoch 770, val loss: 0.9016169905662537
Epoch 780, training loss: 0.6335083842277527 = 0.008966593071818352 + 0.1 * 6.245418071746826
Epoch 780, val loss: 0.905652642250061
Epoch 790, training loss: 0.6320253610610962 = 0.008682187646627426 + 0.1 * 6.233431816101074
Epoch 790, val loss: 0.9096026420593262
Epoch 800, training loss: 0.6315235495567322 = 0.008412417955696583 + 0.1 * 6.2311110496521
Epoch 800, val loss: 0.913501501083374
Epoch 810, training loss: 0.6319686770439148 = 0.008155368268489838 + 0.1 * 6.238133430480957
Epoch 810, val loss: 0.9172773361206055
Epoch 820, training loss: 0.6306844353675842 = 0.00791182555258274 + 0.1 * 6.227725982666016
Epoch 820, val loss: 0.9209845662117004
Epoch 830, training loss: 0.6310743689537048 = 0.007680571638047695 + 0.1 * 6.233937740325928
Epoch 830, val loss: 0.9246322512626648
Epoch 840, training loss: 0.6293352842330933 = 0.007461380213499069 + 0.1 * 6.218739032745361
Epoch 840, val loss: 0.9282197952270508
Epoch 850, training loss: 0.6296104192733765 = 0.0072520701214671135 + 0.1 * 6.223583221435547
Epoch 850, val loss: 0.931713879108429
Epoch 860, training loss: 0.6296560168266296 = 0.007051969412714243 + 0.1 * 6.226040840148926
Epoch 860, val loss: 0.9351054430007935
Epoch 870, training loss: 0.6279539465904236 = 0.006861106492578983 + 0.1 * 6.210928440093994
Epoch 870, val loss: 0.9384596943855286
Epoch 880, training loss: 0.628182590007782 = 0.006679012440145016 + 0.1 * 6.215035915374756
Epoch 880, val loss: 0.9418177008628845
Epoch 890, training loss: 0.6282832622528076 = 0.00650490028783679 + 0.1 * 6.217783451080322
Epoch 890, val loss: 0.9450693726539612
Epoch 900, training loss: 0.6275250315666199 = 0.006338306702673435 + 0.1 * 6.211867332458496
Epoch 900, val loss: 0.9482496380805969
Epoch 910, training loss: 0.6269673705101013 = 0.0061793215572834015 + 0.1 * 6.20788049697876
Epoch 910, val loss: 0.9514151215553284
Epoch 920, training loss: 0.6266160011291504 = 0.006026595365256071 + 0.1 * 6.2058939933776855
Epoch 920, val loss: 0.9545249938964844
Epoch 930, training loss: 0.6296480894088745 = 0.005879989359527826 + 0.1 * 6.237680912017822
Epoch 930, val loss: 0.9575249552726746
Epoch 940, training loss: 0.6262201070785522 = 0.005739999003708363 + 0.1 * 6.204801082611084
Epoch 940, val loss: 0.9605113863945007
Epoch 950, training loss: 0.6253361105918884 = 0.005605753045529127 + 0.1 * 6.197303295135498
Epoch 950, val loss: 0.9634766578674316
Epoch 960, training loss: 0.6288025379180908 = 0.0054763262160122395 + 0.1 * 6.233261585235596
Epoch 960, val loss: 0.9663711190223694
Epoch 970, training loss: 0.6247132420539856 = 0.005352415610104799 + 0.1 * 6.193607807159424
Epoch 970, val loss: 0.9691473841667175
Epoch 980, training loss: 0.6241123080253601 = 0.005233324598520994 + 0.1 * 6.188789367675781
Epoch 980, val loss: 0.9719939231872559
Epoch 990, training loss: 0.6254401206970215 = 0.005118404049426317 + 0.1 * 6.203216552734375
Epoch 990, val loss: 0.9747354984283447
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9410
Flip ASR: 0.9289/225 nodes
The final ASR:0.63715, 0.22353, Accuracy:0.80494, 0.03089
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9448])
updated graph: torch.Size([2, 10478])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00758, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7628448009490967 = 1.9254525899887085 + 0.1 * 8.373921394348145
Epoch 0, val loss: 1.9193233251571655
Epoch 10, training loss: 2.7536094188690186 = 1.9162237644195557 + 0.1 * 8.373856544494629
Epoch 10, val loss: 1.9111095666885376
Epoch 20, training loss: 2.7422728538513184 = 1.9049243927001953 + 0.1 * 8.373483657836914
Epoch 20, val loss: 1.900773525238037
Epoch 30, training loss: 2.7259960174560547 = 1.8889349699020386 + 0.1 * 8.370610237121582
Epoch 30, val loss: 1.8859895467758179
Epoch 40, training loss: 2.7000648975372314 = 1.8652604818344116 + 0.1 * 8.348043441772461
Epoch 40, val loss: 1.864469289779663
Epoch 50, training loss: 2.6517364978790283 = 1.8323196172714233 + 0.1 * 8.194169044494629
Epoch 50, val loss: 1.8360892534255981
Epoch 60, training loss: 2.584010601043701 = 1.7945168018341064 + 0.1 * 7.894936561584473
Epoch 60, val loss: 1.8049938678741455
Epoch 70, training loss: 2.5091655254364014 = 1.7546887397766113 + 0.1 * 7.544767379760742
Epoch 70, val loss: 1.770607352256775
Epoch 80, training loss: 2.42647647857666 = 1.7091035842895508 + 0.1 * 7.173727989196777
Epoch 80, val loss: 1.7298325300216675
Epoch 90, training loss: 2.347198486328125 = 1.650153398513794 + 0.1 * 6.970451354980469
Epoch 90, val loss: 1.6786437034606934
Epoch 100, training loss: 2.2583281993865967 = 1.5721086263656616 + 0.1 * 6.86219596862793
Epoch 100, val loss: 1.6116666793823242
Epoch 110, training loss: 2.157968282699585 = 1.4785103797912598 + 0.1 * 6.794579029083252
Epoch 110, val loss: 1.5333284139633179
Epoch 120, training loss: 2.0525901317596436 = 1.377514123916626 + 0.1 * 6.750760555267334
Epoch 120, val loss: 1.451549768447876
Epoch 130, training loss: 1.9486851692199707 = 1.2765045166015625 + 0.1 * 6.721806049346924
Epoch 130, val loss: 1.3725014925003052
Epoch 140, training loss: 1.8511264324188232 = 1.181323766708374 + 0.1 * 6.698025703430176
Epoch 140, val loss: 1.3005635738372803
Epoch 150, training loss: 1.7618108987808228 = 1.093883752822876 + 0.1 * 6.679271221160889
Epoch 150, val loss: 1.2366790771484375
Epoch 160, training loss: 1.6798362731933594 = 1.0137720108032227 + 0.1 * 6.660642147064209
Epoch 160, val loss: 1.1792237758636475
Epoch 170, training loss: 1.6045331954956055 = 0.9392988681793213 + 0.1 * 6.652342796325684
Epoch 170, val loss: 1.1258800029754639
Epoch 180, training loss: 1.5331308841705322 = 0.8695493936538696 + 0.1 * 6.635814666748047
Epoch 180, val loss: 1.0762373208999634
Epoch 190, training loss: 1.464553952217102 = 0.8021332621574402 + 0.1 * 6.62420654296875
Epoch 190, val loss: 1.0281121730804443
Epoch 200, training loss: 1.3980827331542969 = 0.7359011173248291 + 0.1 * 6.621816635131836
Epoch 200, val loss: 0.9814926981925964
Epoch 210, training loss: 1.333873987197876 = 0.6723517179489136 + 0.1 * 6.615223407745361
Epoch 210, val loss: 0.938409149646759
Epoch 220, training loss: 1.2713996171951294 = 0.6116732358932495 + 0.1 * 6.597263813018799
Epoch 220, val loss: 0.89942866563797
Epoch 230, training loss: 1.2128950357437134 = 0.5542643070220947 + 0.1 * 6.586307048797607
Epoch 230, val loss: 0.8658062815666199
Epoch 240, training loss: 1.159592866897583 = 0.5009886622428894 + 0.1 * 6.5860419273376465
Epoch 240, val loss: 0.8386291265487671
Epoch 250, training loss: 1.109004259109497 = 0.4523460268974304 + 0.1 * 6.566582679748535
Epoch 250, val loss: 0.8184264302253723
Epoch 260, training loss: 1.0648643970489502 = 0.40781155228614807 + 0.1 * 6.570528507232666
Epoch 260, val loss: 0.8043809533119202
Epoch 270, training loss: 1.0226494073867798 = 0.3673822581768036 + 0.1 * 6.552671909332275
Epoch 270, val loss: 0.7953357696533203
Epoch 280, training loss: 0.9847791194915771 = 0.3306415379047394 + 0.1 * 6.541376113891602
Epoch 280, val loss: 0.7899370193481445
Epoch 290, training loss: 0.9497848153114319 = 0.2970466613769531 + 0.1 * 6.527381420135498
Epoch 290, val loss: 0.7871466875076294
Epoch 300, training loss: 0.9192898273468018 = 0.2662729024887085 + 0.1 * 6.5301690101623535
Epoch 300, val loss: 0.7864989638328552
Epoch 310, training loss: 0.8891265988349915 = 0.23836050927639008 + 0.1 * 6.507660865783691
Epoch 310, val loss: 0.7877203822135925
Epoch 320, training loss: 0.8643977046012878 = 0.2130994200706482 + 0.1 * 6.5129828453063965
Epoch 320, val loss: 0.7905451059341431
Epoch 330, training loss: 0.8396926522254944 = 0.19047580659389496 + 0.1 * 6.492167949676514
Epoch 330, val loss: 0.7949658632278442
Epoch 340, training loss: 0.8190225958824158 = 0.17035436630249023 + 0.1 * 6.486681938171387
Epoch 340, val loss: 0.8006573915481567
Epoch 350, training loss: 0.8002274632453918 = 0.1526040881872177 + 0.1 * 6.47623348236084
Epoch 350, val loss: 0.8076696991920471
Epoch 360, training loss: 0.7834888696670532 = 0.13690105080604553 + 0.1 * 6.465878486633301
Epoch 360, val loss: 0.8159366846084595
Epoch 370, training loss: 0.7690712809562683 = 0.12305816262960434 + 0.1 * 6.4601311683654785
Epoch 370, val loss: 0.8252574801445007
Epoch 380, training loss: 0.7579756379127502 = 0.11090441048145294 + 0.1 * 6.470712184906006
Epoch 380, val loss: 0.8354430198669434
Epoch 390, training loss: 0.745280921459198 = 0.10027290880680084 + 0.1 * 6.450079917907715
Epoch 390, val loss: 0.8461982011795044
Epoch 400, training loss: 0.7357412576675415 = 0.09092873334884644 + 0.1 * 6.44812536239624
Epoch 400, val loss: 0.8574703335762024
Epoch 410, training loss: 0.7263310551643372 = 0.08272801339626312 + 0.1 * 6.43602991104126
Epoch 410, val loss: 0.8691425323486328
Epoch 420, training loss: 0.7186213731765747 = 0.07550182193517685 + 0.1 * 6.4311957359313965
Epoch 420, val loss: 0.8809908628463745
Epoch 430, training loss: 0.7101319432258606 = 0.06912510097026825 + 0.1 * 6.410068035125732
Epoch 430, val loss: 0.8930150270462036
Epoch 440, training loss: 0.7059921622276306 = 0.06347106397151947 + 0.1 * 6.425210952758789
Epoch 440, val loss: 0.9049981236457825
Epoch 450, training loss: 0.6986589431762695 = 0.058468617498874664 + 0.1 * 6.401902675628662
Epoch 450, val loss: 0.9170394539833069
Epoch 460, training loss: 0.6940585374832153 = 0.05401357635855675 + 0.1 * 6.400449752807617
Epoch 460, val loss: 0.928838312625885
Epoch 470, training loss: 0.689094066619873 = 0.050033751875162125 + 0.1 * 6.390603065490723
Epoch 470, val loss: 0.9407080411911011
Epoch 480, training loss: 0.6844648122787476 = 0.046468585729599 + 0.1 * 6.379961967468262
Epoch 480, val loss: 0.9521399736404419
Epoch 490, training loss: 0.6810309290885925 = 0.0432746447622776 + 0.1 * 6.377562999725342
Epoch 490, val loss: 0.9637053608894348
Epoch 500, training loss: 0.678131639957428 = 0.04039015248417854 + 0.1 * 6.377414703369141
Epoch 500, val loss: 0.9748213291168213
Epoch 510, training loss: 0.674024760723114 = 0.03777860850095749 + 0.1 * 6.362461090087891
Epoch 510, val loss: 0.9860283732414246
Epoch 520, training loss: 0.6710265278816223 = 0.03540428727865219 + 0.1 * 6.356222629547119
Epoch 520, val loss: 0.9968950152397156
Epoch 530, training loss: 0.6683794260025024 = 0.03324766084551811 + 0.1 * 6.351317882537842
Epoch 530, val loss: 1.007541298866272
Epoch 540, training loss: 0.6665058135986328 = 0.0312918946146965 + 0.1 * 6.352138996124268
Epoch 540, val loss: 1.0178142786026
Epoch 550, training loss: 0.6631758213043213 = 0.029512198641896248 + 0.1 * 6.336636066436768
Epoch 550, val loss: 1.0280860662460327
Epoch 560, training loss: 0.6612106561660767 = 0.027879446744918823 + 0.1 * 6.333312511444092
Epoch 560, val loss: 1.0379899740219116
Epoch 570, training loss: 0.6599740386009216 = 0.026376191526651382 + 0.1 * 6.3359785079956055
Epoch 570, val loss: 1.0477904081344604
Epoch 580, training loss: 0.658246636390686 = 0.02499096468091011 + 0.1 * 6.33255672454834
Epoch 580, val loss: 1.057350754737854
Epoch 590, training loss: 0.6597575545310974 = 0.023715177550911903 + 0.1 * 6.360423564910889
Epoch 590, val loss: 1.0667246580123901
Epoch 600, training loss: 0.653688371181488 = 0.022540070116519928 + 0.1 * 6.3114824295043945
Epoch 600, val loss: 1.0758551359176636
Epoch 610, training loss: 0.6524239778518677 = 0.021452022716403008 + 0.1 * 6.309719562530518
Epoch 610, val loss: 1.0848686695098877
Epoch 620, training loss: 0.6518111824989319 = 0.020441628992557526 + 0.1 * 6.313694953918457
Epoch 620, val loss: 1.09343683719635
Epoch 630, training loss: 0.6497247219085693 = 0.019505033269524574 + 0.1 * 6.302196979522705
Epoch 630, val loss: 1.1021841764450073
Epoch 640, training loss: 0.6503372192382812 = 0.01863071694970131 + 0.1 * 6.3170647621154785
Epoch 640, val loss: 1.11056649684906
Epoch 650, training loss: 0.6474396586418152 = 0.017817988991737366 + 0.1 * 6.2962164878845215
Epoch 650, val loss: 1.11873197555542
Epoch 660, training loss: 0.6474472880363464 = 0.017059601843357086 + 0.1 * 6.3038763999938965
Epoch 660, val loss: 1.1269004344940186
Epoch 670, training loss: 0.6453672051429749 = 0.01634989120066166 + 0.1 * 6.290172576904297
Epoch 670, val loss: 1.134756088256836
Epoch 680, training loss: 0.6447612643241882 = 0.0156867615878582 + 0.1 * 6.290745258331299
Epoch 680, val loss: 1.142320156097412
Epoch 690, training loss: 0.6431570649147034 = 0.015065857209265232 + 0.1 * 6.280911922454834
Epoch 690, val loss: 1.15000319480896
Epoch 700, training loss: 0.6423864364624023 = 0.014481537975370884 + 0.1 * 6.279048919677734
Epoch 700, val loss: 1.1573817729949951
Epoch 710, training loss: 0.6420836448669434 = 0.013931936584413052 + 0.1 * 6.281517028808594
Epoch 710, val loss: 1.1644866466522217
Epoch 720, training loss: 0.640625536441803 = 0.013416401110589504 + 0.1 * 6.272090911865234
Epoch 720, val loss: 1.1716970205307007
Epoch 730, training loss: 0.6401426792144775 = 0.012929138727486134 + 0.1 * 6.272135257720947
Epoch 730, val loss: 1.1786845922470093
Epoch 740, training loss: 0.639251708984375 = 0.012469575740396976 + 0.1 * 6.267821311950684
Epoch 740, val loss: 1.1853300333023071
Epoch 750, training loss: 0.6409779191017151 = 0.012036480940878391 + 0.1 * 6.289413928985596
Epoch 750, val loss: 1.192078709602356
Epoch 760, training loss: 0.6374882459640503 = 0.01162742916494608 + 0.1 * 6.258607864379883
Epoch 760, val loss: 1.1984970569610596
Epoch 770, training loss: 0.6365360617637634 = 0.011240479536354542 + 0.1 * 6.252955436706543
Epoch 770, val loss: 1.205004096031189
Epoch 780, training loss: 0.6361556053161621 = 0.01087294053286314 + 0.1 * 6.25282621383667
Epoch 780, val loss: 1.211218237876892
Epoch 790, training loss: 0.6363588571548462 = 0.010524416342377663 + 0.1 * 6.258344650268555
Epoch 790, val loss: 1.2174067497253418
Epoch 800, training loss: 0.6352303624153137 = 0.01019272767007351 + 0.1 * 6.250376224517822
Epoch 800, val loss: 1.2234431505203247
Epoch 810, training loss: 0.6347602009773254 = 0.009878297336399555 + 0.1 * 6.248819351196289
Epoch 810, val loss: 1.2292109727859497
Epoch 820, training loss: 0.6341245174407959 = 0.009579747915267944 + 0.1 * 6.245447158813477
Epoch 820, val loss: 1.2351635694503784
Epoch 830, training loss: 0.6347152590751648 = 0.009295448660850525 + 0.1 * 6.254197597503662
Epoch 830, val loss: 1.2408775091171265
Epoch 840, training loss: 0.6334555745124817 = 0.00902438722550869 + 0.1 * 6.244311809539795
Epoch 840, val loss: 1.2463728189468384
Epoch 850, training loss: 0.6344521045684814 = 0.008765927515923977 + 0.1 * 6.256861686706543
Epoch 850, val loss: 1.2519471645355225
Epoch 860, training loss: 0.6319260597229004 = 0.008519801311194897 + 0.1 * 6.234062194824219
Epoch 860, val loss: 1.257238507270813
Epoch 870, training loss: 0.6316260099411011 = 0.008284364826977253 + 0.1 * 6.2334160804748535
Epoch 870, val loss: 1.2625892162322998
Epoch 880, training loss: 0.633065938949585 = 0.008058842271566391 + 0.1 * 6.250071048736572
Epoch 880, val loss: 1.2676677703857422
Epoch 890, training loss: 0.6306254267692566 = 0.007844416424632072 + 0.1 * 6.227810382843018
Epoch 890, val loss: 1.2725920677185059
Epoch 900, training loss: 0.6305080056190491 = 0.007639731280505657 + 0.1 * 6.228682994842529
Epoch 900, val loss: 1.277790904045105
Epoch 910, training loss: 0.6314391493797302 = 0.0074429623782634735 + 0.1 * 6.239962100982666
Epoch 910, val loss: 1.2826695442199707
Epoch 920, training loss: 0.6294967532157898 = 0.007254020776599646 + 0.1 * 6.2224273681640625
Epoch 920, val loss: 1.2874795198440552
Epoch 930, training loss: 0.6299055218696594 = 0.007072984240949154 + 0.1 * 6.228325366973877
Epoch 930, val loss: 1.292212724685669
Epoch 940, training loss: 0.6287316679954529 = 0.006899371277540922 + 0.1 * 6.218323230743408
Epoch 940, val loss: 1.2968608140945435
Epoch 950, training loss: 0.627897322177887 = 0.00673262495547533 + 0.1 * 6.211646556854248
Epoch 950, val loss: 1.3016302585601807
Epoch 960, training loss: 0.6305541396141052 = 0.00657207565382123 + 0.1 * 6.23982048034668
Epoch 960, val loss: 1.306059718132019
Epoch 970, training loss: 0.6275226473808289 = 0.0064179920591413975 + 0.1 * 6.21104621887207
Epoch 970, val loss: 1.3104088306427002
Epoch 980, training loss: 0.6295101046562195 = 0.006270650774240494 + 0.1 * 6.232394218444824
Epoch 980, val loss: 1.3149137496948242
Epoch 990, training loss: 0.6269801259040833 = 0.0061286562122404575 + 0.1 * 6.208514213562012
Epoch 990, val loss: 1.3191289901733398
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.6642
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7797253131866455 = 1.9423385858535767 + 0.1 * 8.373867988586426
Epoch 0, val loss: 1.9404665231704712
Epoch 10, training loss: 2.7697930335998535 = 1.9324222803115845 + 0.1 * 8.37370777130127
Epoch 10, val loss: 1.9306073188781738
Epoch 20, training loss: 2.7571187019348145 = 1.919848084449768 + 0.1 * 8.372705459594727
Epoch 20, val loss: 1.9176249504089355
Epoch 30, training loss: 2.738467216491699 = 1.9018996953964233 + 0.1 * 8.365674018859863
Epoch 30, val loss: 1.8986269235610962
Epoch 40, training loss: 2.708165407180786 = 1.875304102897644 + 0.1 * 8.32861328125
Epoch 40, val loss: 1.8704038858413696
Epoch 50, training loss: 2.6528284549713135 = 1.8387086391448975 + 0.1 * 8.14119815826416
Epoch 50, val loss: 1.8329129219055176
Epoch 60, training loss: 2.5765998363494873 = 1.7951862812042236 + 0.1 * 7.81413459777832
Epoch 60, val loss: 1.7899466753005981
Epoch 70, training loss: 2.493705987930298 = 1.7485404014587402 + 0.1 * 7.451655864715576
Epoch 70, val loss: 1.745115876197815
Epoch 80, training loss: 2.418720245361328 = 1.6976875066757202 + 0.1 * 7.210328578948975
Epoch 80, val loss: 1.6986011266708374
Epoch 90, training loss: 2.3381106853485107 = 1.635999083518982 + 0.1 * 7.021115779876709
Epoch 90, val loss: 1.6438984870910645
Epoch 100, training loss: 2.246914863586426 = 1.555842399597168 + 0.1 * 6.9107232093811035
Epoch 100, val loss: 1.575560450553894
Epoch 110, training loss: 2.145831823348999 = 1.4607264995574951 + 0.1 * 6.851052284240723
Epoch 110, val loss: 1.5002896785736084
Epoch 120, training loss: 2.0440242290496826 = 1.3630802631378174 + 0.1 * 6.809439659118652
Epoch 120, val loss: 1.427068829536438
Epoch 130, training loss: 1.949052095413208 = 1.2721011638641357 + 0.1 * 6.769509792327881
Epoch 130, val loss: 1.3628095388412476
Epoch 140, training loss: 1.8632173538208008 = 1.189548373222351 + 0.1 * 6.736689567565918
Epoch 140, val loss: 1.3078309297561646
Epoch 150, training loss: 1.7865079641342163 = 1.115687370300293 + 0.1 * 6.708205699920654
Epoch 150, val loss: 1.2594444751739502
Epoch 160, training loss: 1.7157261371612549 = 1.0475221872329712 + 0.1 * 6.682040214538574
Epoch 160, val loss: 1.2155314683914185
Epoch 170, training loss: 1.6513948440551758 = 0.9846168160438538 + 0.1 * 6.667779922485352
Epoch 170, val loss: 1.1754777431488037
Epoch 180, training loss: 1.5918843746185303 = 0.9267871975898743 + 0.1 * 6.65097188949585
Epoch 180, val loss: 1.1380047798156738
Epoch 190, training loss: 1.5362117290496826 = 0.8721739649772644 + 0.1 * 6.640377998352051
Epoch 190, val loss: 1.1017301082611084
Epoch 200, training loss: 1.482011079788208 = 0.8188490271568298 + 0.1 * 6.631619930267334
Epoch 200, val loss: 1.0660191774368286
Epoch 210, training loss: 1.428094744682312 = 0.7655428647994995 + 0.1 * 6.625518798828125
Epoch 210, val loss: 1.0305488109588623
Epoch 220, training loss: 1.3736780881881714 = 0.7122284173965454 + 0.1 * 6.61449670791626
Epoch 220, val loss: 0.9952797293663025
Epoch 230, training loss: 1.3202519416809082 = 0.659242570400238 + 0.1 * 6.61009407043457
Epoch 230, val loss: 0.9600521326065063
Epoch 240, training loss: 1.2678680419921875 = 0.6076972484588623 + 0.1 * 6.601707935333252
Epoch 240, val loss: 0.9257773160934448
Epoch 250, training loss: 1.217698097229004 = 0.5583163499832153 + 0.1 * 6.593817710876465
Epoch 250, val loss: 0.8931131958961487
Epoch 260, training loss: 1.170441746711731 = 0.5115127563476562 + 0.1 * 6.589289665222168
Epoch 260, val loss: 0.8627805113792419
Epoch 270, training loss: 1.125277042388916 = 0.4676496386528015 + 0.1 * 6.576273441314697
Epoch 270, val loss: 0.8354966044425964
Epoch 280, training loss: 1.084348440170288 = 0.42711830139160156 + 0.1 * 6.572301387786865
Epoch 280, val loss: 0.8115206360816956
Epoch 290, training loss: 1.045935034751892 = 0.3900656998157501 + 0.1 * 6.558692932128906
Epoch 290, val loss: 0.7909414768218994
Epoch 300, training loss: 1.0115028619766235 = 0.3562156856060028 + 0.1 * 6.5528717041015625
Epoch 300, val loss: 0.7736641764640808
Epoch 310, training loss: 0.9802761077880859 = 0.32541945576667786 + 0.1 * 6.548565864562988
Epoch 310, val loss: 0.759725034236908
Epoch 320, training loss: 0.9511265158653259 = 0.2973076105117798 + 0.1 * 6.538188934326172
Epoch 320, val loss: 0.7488267421722412
Epoch 330, training loss: 0.9245550632476807 = 0.27156707644462585 + 0.1 * 6.529880046844482
Epoch 330, val loss: 0.7408453822135925
Epoch 340, training loss: 0.9003250598907471 = 0.24800458550453186 + 0.1 * 6.523204803466797
Epoch 340, val loss: 0.735434889793396
Epoch 350, training loss: 0.8787481784820557 = 0.22640946507453918 + 0.1 * 6.5233869552612305
Epoch 350, val loss: 0.7323707938194275
Epoch 360, training loss: 0.8573770523071289 = 0.2067158818244934 + 0.1 * 6.506611347198486
Epoch 360, val loss: 0.7311978936195374
Epoch 370, training loss: 0.8392120599746704 = 0.18868768215179443 + 0.1 * 6.50524377822876
Epoch 370, val loss: 0.7318944334983826
Epoch 380, training loss: 0.8216809630393982 = 0.1723250299692154 + 0.1 * 6.49355936050415
Epoch 380, val loss: 0.7341359853744507
Epoch 390, training loss: 0.8060320019721985 = 0.1575133353471756 + 0.1 * 6.485186576843262
Epoch 390, val loss: 0.7376976013183594
Epoch 400, training loss: 0.7928541898727417 = 0.14419908821582794 + 0.1 * 6.486551284790039
Epoch 400, val loss: 0.7423658967018127
Epoch 410, training loss: 0.7792472839355469 = 0.13224810361862183 + 0.1 * 6.469991683959961
Epoch 410, val loss: 0.7480111122131348
Epoch 420, training loss: 0.7676756978034973 = 0.12152118980884552 + 0.1 * 6.461544990539551
Epoch 420, val loss: 0.7544199228286743
Epoch 430, training loss: 0.7577032446861267 = 0.11186377704143524 + 0.1 * 6.458394527435303
Epoch 430, val loss: 0.7614161372184753
Epoch 440, training loss: 0.7484022378921509 = 0.10318733006715775 + 0.1 * 6.452148914337158
Epoch 440, val loss: 0.7689716815948486
Epoch 450, training loss: 0.7394590973854065 = 0.09539282321929932 + 0.1 * 6.440662384033203
Epoch 450, val loss: 0.7768847346305847
Epoch 460, training loss: 0.7317676544189453 = 0.08839986473321915 + 0.1 * 6.433677673339844
Epoch 460, val loss: 0.7851319909095764
Epoch 470, training loss: 0.7265809774398804 = 0.08209844678640366 + 0.1 * 6.444825649261475
Epoch 470, val loss: 0.793441891670227
Epoch 480, training loss: 0.7190210223197937 = 0.07641684263944626 + 0.1 * 6.426041603088379
Epoch 480, val loss: 0.8018255829811096
Epoch 490, training loss: 0.7140697836875916 = 0.07125768810510635 + 0.1 * 6.4281206130981445
Epoch 490, val loss: 0.8102833032608032
Epoch 500, training loss: 0.7073638439178467 = 0.06656313687562943 + 0.1 * 6.4080071449279785
Epoch 500, val loss: 0.8187810182571411
Epoch 510, training loss: 0.7046877145767212 = 0.06227008253335953 + 0.1 * 6.42417573928833
Epoch 510, val loss: 0.8273798823356628
Epoch 520, training loss: 0.6988638639450073 = 0.058321595191955566 + 0.1 * 6.405422687530518
Epoch 520, val loss: 0.8358680605888367
Epoch 530, training loss: 0.6939921975135803 = 0.05467701703310013 + 0.1 * 6.393151760101318
Epoch 530, val loss: 0.8444122672080994
Epoch 540, training loss: 0.692069411277771 = 0.051289744675159454 + 0.1 * 6.407796382904053
Epoch 540, val loss: 0.85291987657547
Epoch 550, training loss: 0.6875176429748535 = 0.048149220645427704 + 0.1 * 6.393683910369873
Epoch 550, val loss: 0.8612605333328247
Epoch 560, training loss: 0.6829915046691895 = 0.04520203173160553 + 0.1 * 6.377894878387451
Epoch 560, val loss: 0.8694952130317688
Epoch 570, training loss: 0.6798908114433289 = 0.042441584169864655 + 0.1 * 6.3744916915893555
Epoch 570, val loss: 0.8776779770851135
Epoch 580, training loss: 0.6769831776618958 = 0.03984500840306282 + 0.1 * 6.371381759643555
Epoch 580, val loss: 0.8856719732284546
Epoch 590, training loss: 0.6740195751190186 = 0.037403374910354614 + 0.1 * 6.366162300109863
Epoch 590, val loss: 0.8936825394630432
Epoch 600, training loss: 0.6758597493171692 = 0.03509841114282608 + 0.1 * 6.407613277435303
Epoch 600, val loss: 0.9014354944229126
Epoch 610, training loss: 0.6688536405563354 = 0.03296134993433952 + 0.1 * 6.358922481536865
Epoch 610, val loss: 0.9090701341629028
Epoch 620, training loss: 0.6667557954788208 = 0.03098529763519764 + 0.1 * 6.3577046394348145
Epoch 620, val loss: 0.9168122410774231
Epoch 630, training loss: 0.6645055413246155 = 0.02918209508061409 + 0.1 * 6.35323429107666
Epoch 630, val loss: 0.9244420528411865
Epoch 640, training loss: 0.6623619198799133 = 0.02752934768795967 + 0.1 * 6.348325729370117
Epoch 640, val loss: 0.9321615695953369
Epoch 650, training loss: 0.660681962966919 = 0.026009948924183846 + 0.1 * 6.346719741821289
Epoch 650, val loss: 0.9399272203445435
Epoch 660, training loss: 0.6595012545585632 = 0.024605102837085724 + 0.1 * 6.348961353302002
Epoch 660, val loss: 0.9474111795425415
Epoch 670, training loss: 0.6590958833694458 = 0.023316213861107826 + 0.1 * 6.357796669006348
Epoch 670, val loss: 0.954744815826416
Epoch 680, training loss: 0.6554244756698608 = 0.02213040553033352 + 0.1 * 6.332940578460693
Epoch 680, val loss: 0.9619396924972534
Epoch 690, training loss: 0.6545259952545166 = 0.0210309699177742 + 0.1 * 6.3349504470825195
Epoch 690, val loss: 0.9690958857536316
Epoch 700, training loss: 0.6529548168182373 = 0.020008796826004982 + 0.1 * 6.329460144042969
Epoch 700, val loss: 0.9761158227920532
Epoch 710, training loss: 0.6516998410224915 = 0.01905764266848564 + 0.1 * 6.326422214508057
Epoch 710, val loss: 0.9829805493354797
Epoch 720, training loss: 0.6505665183067322 = 0.018171044066548347 + 0.1 * 6.3239545822143555
Epoch 720, val loss: 0.9896085262298584
Epoch 730, training loss: 0.6490744352340698 = 0.017344994470477104 + 0.1 * 6.317294120788574
Epoch 730, val loss: 0.9962091445922852
Epoch 740, training loss: 0.6489710807800293 = 0.01657470315694809 + 0.1 * 6.3239641189575195
Epoch 740, val loss: 1.0027443170547485
Epoch 750, training loss: 0.6472881436347961 = 0.01585603691637516 + 0.1 * 6.314321041107178
Epoch 750, val loss: 1.0091054439544678
Epoch 760, training loss: 0.6469849348068237 = 0.015184594318270683 + 0.1 * 6.3180036544799805
Epoch 760, val loss: 1.0153228044509888
Epoch 770, training loss: 0.6452013254165649 = 0.01455586776137352 + 0.1 * 6.306454658508301
Epoch 770, val loss: 1.0214147567749023
Epoch 780, training loss: 0.6450181603431702 = 0.013963477686047554 + 0.1 * 6.310546875
Epoch 780, val loss: 1.027457356452942
Epoch 790, training loss: 0.644443929195404 = 0.013409871608018875 + 0.1 * 6.310340404510498
Epoch 790, val loss: 1.033370852470398
Epoch 800, training loss: 0.6431074142456055 = 0.01288701593875885 + 0.1 * 6.302204132080078
Epoch 800, val loss: 1.0390396118164062
Epoch 810, training loss: 0.6415427923202515 = 0.012397456914186478 + 0.1 * 6.2914533615112305
Epoch 810, val loss: 1.0446857213974
Epoch 820, training loss: 0.6414797902107239 = 0.011938190087676048 + 0.1 * 6.29541540145874
Epoch 820, val loss: 1.050331473350525
Epoch 830, training loss: 0.6402899026870728 = 0.011502914130687714 + 0.1 * 6.287869930267334
Epoch 830, val loss: 1.0557314157485962
Epoch 840, training loss: 0.6403784155845642 = 0.011091961525380611 + 0.1 * 6.292864799499512
Epoch 840, val loss: 1.0610911846160889
Epoch 850, training loss: 0.6389884352684021 = 0.010703932493925095 + 0.1 * 6.2828450202941895
Epoch 850, val loss: 1.0663485527038574
Epoch 860, training loss: 0.6408025622367859 = 0.010337541811168194 + 0.1 * 6.30465030670166
Epoch 860, val loss: 1.0715060234069824
Epoch 870, training loss: 0.6378600597381592 = 0.009989171288907528 + 0.1 * 6.2787089347839355
Epoch 870, val loss: 1.076474905014038
Epoch 880, training loss: 0.6374409794807434 = 0.009660362266004086 + 0.1 * 6.277806282043457
Epoch 880, val loss: 1.0814716815948486
Epoch 890, training loss: 0.6359579563140869 = 0.009348385035991669 + 0.1 * 6.2660956382751465
Epoch 890, val loss: 1.0864038467407227
Epoch 900, training loss: 0.6361526250839233 = 0.009051777422428131 + 0.1 * 6.271008014678955
Epoch 900, val loss: 1.0912007093429565
Epoch 910, training loss: 0.6352132558822632 = 0.008770255371928215 + 0.1 * 6.264430046081543
Epoch 910, val loss: 1.0959534645080566
Epoch 920, training loss: 0.6361442804336548 = 0.008501678705215454 + 0.1 * 6.276425838470459
Epoch 920, val loss: 1.1005101203918457
Epoch 930, training loss: 0.6347407698631287 = 0.008247144520282745 + 0.1 * 6.264936447143555
Epoch 930, val loss: 1.1050896644592285
Epoch 940, training loss: 0.6336711049079895 = 0.008004290983080864 + 0.1 * 6.256667613983154
Epoch 940, val loss: 1.1095713376998901
Epoch 950, training loss: 0.6335562467575073 = 0.007772638462483883 + 0.1 * 6.257835865020752
Epoch 950, val loss: 1.1140190362930298
Epoch 960, training loss: 0.632816731929779 = 0.007551277056336403 + 0.1 * 6.252654552459717
Epoch 960, val loss: 1.1183196306228638
Epoch 970, training loss: 0.633960485458374 = 0.007340193726122379 + 0.1 * 6.266202926635742
Epoch 970, val loss: 1.122649073600769
Epoch 980, training loss: 0.6347267031669617 = 0.007137893233448267 + 0.1 * 6.275887966156006
Epoch 980, val loss: 1.126766562461853
Epoch 990, training loss: 0.6311805248260498 = 0.006945355329662561 + 0.1 * 6.242351531982422
Epoch 990, val loss: 1.1309181451797485
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.4834
Flip ASR: 0.4222/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7826335430145264 = 1.945239782333374 + 0.1 * 8.373936653137207
Epoch 0, val loss: 1.9417426586151123
Epoch 10, training loss: 2.77280592918396 = 1.935418725013733 + 0.1 * 8.373872756958008
Epoch 10, val loss: 1.931879997253418
Epoch 20, training loss: 2.7607908248901367 = 1.9234333038330078 + 0.1 * 8.373576164245605
Epoch 20, val loss: 1.9190834760665894
Epoch 30, training loss: 2.743751287460327 = 1.9066098928451538 + 0.1 * 8.371414184570312
Epoch 30, val loss: 1.9004647731781006
Epoch 40, training loss: 2.716846466064453 = 1.8816170692443848 + 0.1 * 8.352293014526367
Epoch 40, val loss: 1.8726561069488525
Epoch 50, training loss: 2.6670942306518555 = 1.8459420204162598 + 0.1 * 8.211523056030273
Epoch 50, val loss: 1.8346847295761108
Epoch 60, training loss: 2.5900635719299316 = 1.804032325744629 + 0.1 * 7.860311985015869
Epoch 60, val loss: 1.793776512145996
Epoch 70, training loss: 2.503541946411133 = 1.763392686843872 + 0.1 * 7.401491641998291
Epoch 70, val loss: 1.7574878931045532
Epoch 80, training loss: 2.427668571472168 = 1.7220851182937622 + 0.1 * 7.05583381652832
Epoch 80, val loss: 1.7208616733551025
Epoch 90, training loss: 2.3627700805664062 = 1.6726744174957275 + 0.1 * 6.9009552001953125
Epoch 90, val loss: 1.6774837970733643
Epoch 100, training loss: 2.2916033267974854 = 1.6088590621948242 + 0.1 * 6.827442169189453
Epoch 100, val loss: 1.6240242719650269
Epoch 110, training loss: 2.205167293548584 = 1.5263417959213257 + 0.1 * 6.78825569152832
Epoch 110, val loss: 1.5573832988739014
Epoch 120, training loss: 2.103344440460205 = 1.4269835948944092 + 0.1 * 6.763609409332275
Epoch 120, val loss: 1.478251338005066
Epoch 130, training loss: 1.9918663501739502 = 1.317509412765503 + 0.1 * 6.743569850921631
Epoch 130, val loss: 1.3926100730895996
Epoch 140, training loss: 1.8775482177734375 = 1.2049281597137451 + 0.1 * 6.726201057434082
Epoch 140, val loss: 1.3066000938415527
Epoch 150, training loss: 1.764899730682373 = 1.0946701765060425 + 0.1 * 6.702295780181885
Epoch 150, val loss: 1.2233130931854248
Epoch 160, training loss: 1.6576216220855713 = 0.9890273213386536 + 0.1 * 6.685943126678467
Epoch 160, val loss: 1.14283287525177
Epoch 170, training loss: 1.5576863288879395 = 0.8911349773406982 + 0.1 * 6.66551399230957
Epoch 170, val loss: 1.0683971643447876
Epoch 180, training loss: 1.466431736946106 = 0.8018526434898376 + 0.1 * 6.6457905769348145
Epoch 180, val loss: 1.001037359237671
Epoch 190, training loss: 1.3840121030807495 = 0.7210901379585266 + 0.1 * 6.6292195320129395
Epoch 190, val loss: 0.9408315420150757
Epoch 200, training loss: 1.3096617460250854 = 0.6482558250427246 + 0.1 * 6.614058971405029
Epoch 200, val loss: 0.8874179124832153
Epoch 210, training loss: 1.2428183555603027 = 0.582581639289856 + 0.1 * 6.6023664474487305
Epoch 210, val loss: 0.8403728604316711
Epoch 220, training loss: 1.1807336807250977 = 0.5217793583869934 + 0.1 * 6.589543342590332
Epoch 220, val loss: 0.7983764410018921
Epoch 230, training loss: 1.1242306232452393 = 0.46547359228134155 + 0.1 * 6.587570667266846
Epoch 230, val loss: 0.7615962624549866
Epoch 240, training loss: 1.0712209939956665 = 0.41396766901016235 + 0.1 * 6.572533130645752
Epoch 240, val loss: 0.7303939461708069
Epoch 250, training loss: 1.0240204334259033 = 0.3673474192619324 + 0.1 * 6.566729545593262
Epoch 250, val loss: 0.7044917345046997
Epoch 260, training loss: 0.9805091619491577 = 0.32553669810295105 + 0.1 * 6.549724578857422
Epoch 260, val loss: 0.6835792660713196
Epoch 270, training loss: 0.9426208734512329 = 0.28834885358810425 + 0.1 * 6.542719841003418
Epoch 270, val loss: 0.6672705411911011
Epoch 280, training loss: 0.909424901008606 = 0.2556261718273163 + 0.1 * 6.537987232208252
Epoch 280, val loss: 0.6552935838699341
Epoch 290, training loss: 0.8791059255599976 = 0.22689051926136017 + 0.1 * 6.522154331207275
Epoch 290, val loss: 0.6470825672149658
Epoch 300, training loss: 0.854140043258667 = 0.20167973637580872 + 0.1 * 6.52460241317749
Epoch 300, val loss: 0.6419110894203186
Epoch 310, training loss: 0.8305467367172241 = 0.17980501055717468 + 0.1 * 6.507417678833008
Epoch 310, val loss: 0.6394419074058533
Epoch 320, training loss: 0.8101428747177124 = 0.16066214442253113 + 0.1 * 6.494807720184326
Epoch 320, val loss: 0.6391298174858093
Epoch 330, training loss: 0.7926160097122192 = 0.14397969841957092 + 0.1 * 6.486362457275391
Epoch 330, val loss: 0.6403084993362427
Epoch 340, training loss: 0.7766517400741577 = 0.1294819861650467 + 0.1 * 6.4716973304748535
Epoch 340, val loss: 0.6429345011711121
Epoch 350, training loss: 0.7657660245895386 = 0.11682886630296707 + 0.1 * 6.489371299743652
Epoch 350, val loss: 0.6466085314750671
Epoch 360, training loss: 0.7520875334739685 = 0.10584151744842529 + 0.1 * 6.462460041046143
Epoch 360, val loss: 0.6510969400405884
Epoch 370, training loss: 0.7401936054229736 = 0.09621424227952957 + 0.1 * 6.439793586730957
Epoch 370, val loss: 0.656220555305481
Epoch 380, training loss: 0.7314212918281555 = 0.08776845782995224 + 0.1 * 6.436528205871582
Epoch 380, val loss: 0.6618077754974365
Epoch 390, training loss: 0.7235753536224365 = 0.08036188781261444 + 0.1 * 6.43213415145874
Epoch 390, val loss: 0.6677574515342712
Epoch 400, training loss: 0.7177750468254089 = 0.07380412518978119 + 0.1 * 6.439709186553955
Epoch 400, val loss: 0.6739949584007263
Epoch 410, training loss: 0.7094458341598511 = 0.06799343228340149 + 0.1 * 6.414524078369141
Epoch 410, val loss: 0.68043053150177
Epoch 420, training loss: 0.7032389044761658 = 0.06280576437711716 + 0.1 * 6.404331207275391
Epoch 420, val loss: 0.6871253848075867
Epoch 430, training loss: 0.6992995142936707 = 0.058151863515377045 + 0.1 * 6.4114766120910645
Epoch 430, val loss: 0.69389408826828
Epoch 440, training loss: 0.6938820481300354 = 0.05397844314575195 + 0.1 * 6.399035930633545
Epoch 440, val loss: 0.7007870674133301
Epoch 450, training loss: 0.6892623901367188 = 0.05023528262972832 + 0.1 * 6.390271186828613
Epoch 450, val loss: 0.7076612114906311
Epoch 460, training loss: 0.6839864253997803 = 0.046847887337207794 + 0.1 * 6.37138557434082
Epoch 460, val loss: 0.7145910859107971
Epoch 470, training loss: 0.6817018985748291 = 0.043770454823970795 + 0.1 * 6.379314422607422
Epoch 470, val loss: 0.7215297222137451
Epoch 480, training loss: 0.6777118444442749 = 0.040977418422698975 + 0.1 * 6.367344379425049
Epoch 480, val loss: 0.728484034538269
Epoch 490, training loss: 0.6750524044036865 = 0.0384313240647316 + 0.1 * 6.366210460662842
Epoch 490, val loss: 0.7354130148887634
Epoch 500, training loss: 0.6711069345474243 = 0.0361100509762764 + 0.1 * 6.349968910217285
Epoch 500, val loss: 0.7422938346862793
Epoch 510, training loss: 0.6690993905067444 = 0.033981647342443466 + 0.1 * 6.35117769241333
Epoch 510, val loss: 0.7491235733032227
Epoch 520, training loss: 0.6654919981956482 = 0.03202898055315018 + 0.1 * 6.334630012512207
Epoch 520, val loss: 0.7559181451797485
Epoch 530, training loss: 0.6674129962921143 = 0.03023894503712654 + 0.1 * 6.371740341186523
Epoch 530, val loss: 0.7626324892044067
Epoch 540, training loss: 0.66136634349823 = 0.028593922033905983 + 0.1 * 6.327724456787109
Epoch 540, val loss: 0.769223690032959
Epoch 550, training loss: 0.6600244641304016 = 0.027075834572315216 + 0.1 * 6.32948637008667
Epoch 550, val loss: 0.7757812142372131
Epoch 560, training loss: 0.6581346988677979 = 0.025669557973742485 + 0.1 * 6.32465124130249
Epoch 560, val loss: 0.7822281122207642
Epoch 570, training loss: 0.6556814312934875 = 0.02436971291899681 + 0.1 * 6.313117027282715
Epoch 570, val loss: 0.7885913848876953
Epoch 580, training loss: 0.6543564200401306 = 0.023163754492998123 + 0.1 * 6.31192684173584
Epoch 580, val loss: 0.7949106097221375
Epoch 590, training loss: 0.6540701389312744 = 0.02204313315451145 + 0.1 * 6.32027006149292
Epoch 590, val loss: 0.8011176586151123
Epoch 600, training loss: 0.6508185267448425 = 0.02100367844104767 + 0.1 * 6.2981486320495605
Epoch 600, val loss: 0.8071655631065369
Epoch 610, training loss: 0.650261402130127 = 0.02003767527639866 + 0.1 * 6.302237510681152
Epoch 610, val loss: 0.8131691217422485
Epoch 620, training loss: 0.6480628252029419 = 0.019134631380438805 + 0.1 * 6.289281845092773
Epoch 620, val loss: 0.8190367221832275
Epoch 630, training loss: 0.6474031805992126 = 0.018291650339961052 + 0.1 * 6.2911152839660645
Epoch 630, val loss: 0.8248240947723389
Epoch 640, training loss: 0.6457963585853577 = 0.017503023147583008 + 0.1 * 6.282933235168457
Epoch 640, val loss: 0.8305051326751709
Epoch 650, training loss: 0.6455507874488831 = 0.016765711829066277 + 0.1 * 6.287850379943848
Epoch 650, val loss: 0.8361053466796875
Epoch 660, training loss: 0.6432009935379028 = 0.01607557386159897 + 0.1 * 6.271254539489746
Epoch 660, val loss: 0.8416603803634644
Epoch 670, training loss: 0.6435312628746033 = 0.015425859950482845 + 0.1 * 6.2810540199279785
Epoch 670, val loss: 0.847080647945404
Epoch 680, training loss: 0.6427280306816101 = 0.014816118404269218 + 0.1 * 6.27911901473999
Epoch 680, val loss: 0.8524221181869507
Epoch 690, training loss: 0.6407231688499451 = 0.014244195073843002 + 0.1 * 6.264789581298828
Epoch 690, val loss: 0.8576646447181702
Epoch 700, training loss: 0.6413036584854126 = 0.013705029152333736 + 0.1 * 6.275986194610596
Epoch 700, val loss: 0.8629041910171509
Epoch 710, training loss: 0.6392912268638611 = 0.013195591978728771 + 0.1 * 6.260956287384033
Epoch 710, val loss: 0.8680052161216736
Epoch 720, training loss: 0.6399055123329163 = 0.012716200202703476 + 0.1 * 6.271893501281738
Epoch 720, val loss: 0.873035192489624
Epoch 730, training loss: 0.6384649872779846 = 0.012262297794222832 + 0.1 * 6.262026786804199
Epoch 730, val loss: 0.8779482841491699
Epoch 740, training loss: 0.6376094818115234 = 0.011833855882287025 + 0.1 * 6.257756233215332
Epoch 740, val loss: 0.8828287720680237
Epoch 750, training loss: 0.6360863447189331 = 0.011428020894527435 + 0.1 * 6.246583461761475
Epoch 750, val loss: 0.8876358270645142
Epoch 760, training loss: 0.6364193558692932 = 0.011042936705052853 + 0.1 * 6.253763675689697
Epoch 760, val loss: 0.8923869729042053
Epoch 770, training loss: 0.6360534429550171 = 0.01067852508276701 + 0.1 * 6.253748893737793
Epoch 770, val loss: 0.8970209360122681
Epoch 780, training loss: 0.6340864300727844 = 0.010333389975130558 + 0.1 * 6.237529754638672
Epoch 780, val loss: 0.9015543460845947
Epoch 790, training loss: 0.6335957050323486 = 0.010005622170865536 + 0.1 * 6.23590087890625
Epoch 790, val loss: 0.9060749411582947
Epoch 800, training loss: 0.6341273188591003 = 0.00969312060624361 + 0.1 * 6.2443413734436035
Epoch 800, val loss: 0.9105121493339539
Epoch 810, training loss: 0.6322380900382996 = 0.009396005421876907 + 0.1 * 6.228420734405518
Epoch 810, val loss: 0.9148532152175903
Epoch 820, training loss: 0.6322417259216309 = 0.009113194420933723 + 0.1 * 6.231285095214844
Epoch 820, val loss: 0.9191564917564392
Epoch 830, training loss: 0.6328518986701965 = 0.00884308386594057 + 0.1 * 6.240087985992432
Epoch 830, val loss: 0.9234147667884827
Epoch 840, training loss: 0.6323860287666321 = 0.008586490526795387 + 0.1 * 6.237995624542236
Epoch 840, val loss: 0.927565336227417
Epoch 850, training loss: 0.6305087804794312 = 0.008341378532350063 + 0.1 * 6.221673965454102
Epoch 850, val loss: 0.931667149066925
Epoch 860, training loss: 0.6308114528656006 = 0.008107272908091545 + 0.1 * 6.227041721343994
Epoch 860, val loss: 0.9357481002807617
Epoch 870, training loss: 0.6295634508132935 = 0.007883332669734955 + 0.1 * 6.216801166534424
Epoch 870, val loss: 0.9397046566009521
Epoch 880, training loss: 0.6314016580581665 = 0.00766915874555707 + 0.1 * 6.237325191497803
Epoch 880, val loss: 0.9436414241790771
Epoch 890, training loss: 0.629344642162323 = 0.007464650087058544 + 0.1 * 6.218799591064453
Epoch 890, val loss: 0.9475079774856567
Epoch 900, training loss: 0.6304728388786316 = 0.0072690523229539394 + 0.1 * 6.2320380210876465
Epoch 900, val loss: 0.9513173699378967
Epoch 910, training loss: 0.6275311708450317 = 0.00708145322278142 + 0.1 * 6.20449686050415
Epoch 910, val loss: 0.9550609588623047
Epoch 920, training loss: 0.6280595064163208 = 0.00690189516171813 + 0.1 * 6.211575984954834
Epoch 920, val loss: 0.9587779641151428
Epoch 930, training loss: 0.6292948126792908 = 0.006729365326464176 + 0.1 * 6.225654125213623
Epoch 930, val loss: 0.962460458278656
Epoch 940, training loss: 0.6265636086463928 = 0.006564479786902666 + 0.1 * 6.199991226196289
Epoch 940, val loss: 0.9660055637359619
Epoch 950, training loss: 0.6262691020965576 = 0.006405702792108059 + 0.1 * 6.198633670806885
Epoch 950, val loss: 0.9696005582809448
Epoch 960, training loss: 0.6264342069625854 = 0.0062527903355658054 + 0.1 * 6.2018141746521
Epoch 960, val loss: 0.9731699824333191
Epoch 970, training loss: 0.627022385597229 = 0.0061059072613716125 + 0.1 * 6.209164619445801
Epoch 970, val loss: 0.9766080379486084
Epoch 980, training loss: 0.6263408064842224 = 0.0059652794152498245 + 0.1 * 6.203754901885986
Epoch 980, val loss: 0.9799848794937134
Epoch 990, training loss: 0.6269468069076538 = 0.005829724483191967 + 0.1 * 6.211170673370361
Epoch 990, val loss: 0.9834193587303162
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9336
Flip ASR: 0.9200/225 nodes
The final ASR:0.69373, 0.18497, Accuracy:0.81111, 0.02283
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11646])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10606])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97540, 0.00348, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7934794425964355 = 1.95609450340271 + 0.1 * 8.373847961425781
Epoch 0, val loss: 1.946263313293457
Epoch 10, training loss: 2.783238172531128 = 1.9458625316619873 + 0.1 * 8.37375545501709
Epoch 10, val loss: 1.9368447065353394
Epoch 20, training loss: 2.7710278034210205 = 1.9337084293365479 + 0.1 * 8.373193740844727
Epoch 20, val loss: 1.9253637790679932
Epoch 30, training loss: 2.753955364227295 = 1.9169946908950806 + 0.1 * 8.369606018066406
Epoch 30, val loss: 1.9092967510223389
Epoch 40, training loss: 2.726619243621826 = 1.8923988342285156 + 0.1 * 8.342203140258789
Epoch 40, val loss: 1.8856165409088135
Epoch 50, training loss: 2.6677281856536865 = 1.8572524785995483 + 0.1 * 8.104756355285645
Epoch 50, val loss: 1.8529702425003052
Epoch 60, training loss: 2.5613768100738525 = 1.8186858892440796 + 0.1 * 7.426909446716309
Epoch 60, val loss: 1.8193230628967285
Epoch 70, training loss: 2.4814791679382324 = 1.7833495140075684 + 0.1 * 6.981295108795166
Epoch 70, val loss: 1.7897523641586304
Epoch 80, training loss: 2.425273895263672 = 1.74827241897583 + 0.1 * 6.770014762878418
Epoch 80, val loss: 1.761648416519165
Epoch 90, training loss: 2.375304937362671 = 1.7075101137161255 + 0.1 * 6.677948474884033
Epoch 90, val loss: 1.7272897958755493
Epoch 100, training loss: 2.3162691593170166 = 1.6531397104263306 + 0.1 * 6.631294250488281
Epoch 100, val loss: 1.6807644367218018
Epoch 110, training loss: 2.2424447536468506 = 1.5819127559661865 + 0.1 * 6.605319023132324
Epoch 110, val loss: 1.6219961643218994
Epoch 120, training loss: 2.1530158519744873 = 1.4941105842590332 + 0.1 * 6.589052200317383
Epoch 120, val loss: 1.5507590770721436
Epoch 130, training loss: 2.0545380115509033 = 1.3965623378753662 + 0.1 * 6.579756259918213
Epoch 130, val loss: 1.4731041193008423
Epoch 140, training loss: 1.9536303281784058 = 1.2962684631347656 + 0.1 * 6.573618412017822
Epoch 140, val loss: 1.3963963985443115
Epoch 150, training loss: 1.8535616397857666 = 1.1969000101089478 + 0.1 * 6.566617012023926
Epoch 150, val loss: 1.323487401008606
Epoch 160, training loss: 1.7561695575714111 = 1.1003241539001465 + 0.1 * 6.558453559875488
Epoch 160, val loss: 1.2535425424575806
Epoch 170, training loss: 1.6645770072937012 = 1.0092902183532715 + 0.1 * 6.552867889404297
Epoch 170, val loss: 1.188271403312683
Epoch 180, training loss: 1.5785889625549316 = 0.9241142272949219 + 0.1 * 6.544746398925781
Epoch 180, val loss: 1.1268943548202515
Epoch 190, training loss: 1.4968125820159912 = 0.8427612781524658 + 0.1 * 6.540513038635254
Epoch 190, val loss: 1.0680596828460693
Epoch 200, training loss: 1.417036771774292 = 0.7640705704689026 + 0.1 * 6.529662132263184
Epoch 200, val loss: 1.0107439756393433
Epoch 210, training loss: 1.3394774198532104 = 0.6873406767845154 + 0.1 * 6.52136754989624
Epoch 210, val loss: 0.9552544355392456
Epoch 220, training loss: 1.2651309967041016 = 0.61370849609375 + 0.1 * 6.514225006103516
Epoch 220, val loss: 0.902830183506012
Epoch 230, training loss: 1.1961371898651123 = 0.5453813672065735 + 0.1 * 6.507558345794678
Epoch 230, val loss: 0.8563746809959412
Epoch 240, training loss: 1.1332993507385254 = 0.48279115557670593 + 0.1 * 6.505082130432129
Epoch 240, val loss: 0.8169687390327454
Epoch 250, training loss: 1.075453519821167 = 0.42636290192604065 + 0.1 * 6.49090576171875
Epoch 250, val loss: 0.7852897644042969
Epoch 260, training loss: 1.023552656173706 = 0.375219464302063 + 0.1 * 6.483332633972168
Epoch 260, val loss: 0.7606521844863892
Epoch 270, training loss: 0.9768456220626831 = 0.32867565751075745 + 0.1 * 6.481698989868164
Epoch 270, val loss: 0.7420761585235596
Epoch 280, training loss: 0.9344043731689453 = 0.28679561614990234 + 0.1 * 6.47608757019043
Epoch 280, val loss: 0.7290658354759216
Epoch 290, training loss: 0.8958605527877808 = 0.24961282312870026 + 0.1 * 6.462477207183838
Epoch 290, val loss: 0.7212159633636475
Epoch 300, training loss: 0.8628725409507751 = 0.21695534884929657 + 0.1 * 6.459171772003174
Epoch 300, val loss: 0.7175460457801819
Epoch 310, training loss: 0.8336893916130066 = 0.18885648250579834 + 0.1 * 6.448328971862793
Epoch 310, val loss: 0.7179491519927979
Epoch 320, training loss: 0.8090546131134033 = 0.16490891575813293 + 0.1 * 6.4414567947387695
Epoch 320, val loss: 0.7215048670768738
Epoch 330, training loss: 0.7877539992332458 = 0.14464378356933594 + 0.1 * 6.431102275848389
Epoch 330, val loss: 0.7278399467468262
Epoch 340, training loss: 0.7704728841781616 = 0.1274760216474533 + 0.1 * 6.429968357086182
Epoch 340, val loss: 0.7361615896224976
Epoch 350, training loss: 0.7546021938323975 = 0.11297549307346344 + 0.1 * 6.416266918182373
Epoch 350, val loss: 0.7461332082748413
Epoch 360, training loss: 0.7412736415863037 = 0.10059192776679993 + 0.1 * 6.4068169593811035
Epoch 360, val loss: 0.7572135925292969
Epoch 370, training loss: 0.7298965454101562 = 0.08997133374214172 + 0.1 * 6.399251937866211
Epoch 370, val loss: 0.7691271305084229
Epoch 380, training loss: 0.719943642616272 = 0.08084256947040558 + 0.1 * 6.391010761260986
Epoch 380, val loss: 0.7817990779876709
Epoch 390, training loss: 0.7110952734947205 = 0.07292728871107101 + 0.1 * 6.381679534912109
Epoch 390, val loss: 0.7948276400566101
Epoch 400, training loss: 0.7052659392356873 = 0.06607415527105331 + 0.1 * 6.391918182373047
Epoch 400, val loss: 0.8076969981193542
Epoch 410, training loss: 0.6975097060203552 = 0.06014467775821686 + 0.1 * 6.373650550842285
Epoch 410, val loss: 0.8208163380622864
Epoch 420, training loss: 0.690949022769928 = 0.0549410842359066 + 0.1 * 6.360079765319824
Epoch 420, val loss: 0.833791196346283
Epoch 430, training loss: 0.6873791217803955 = 0.050366103649139404 + 0.1 * 6.3701300621032715
Epoch 430, val loss: 0.8464250564575195
Epoch 440, training loss: 0.6808410882949829 = 0.046335190534591675 + 0.1 * 6.345058917999268
Epoch 440, val loss: 0.8591728806495667
Epoch 450, training loss: 0.676697313785553 = 0.04275065287947655 + 0.1 * 6.339466571807861
Epoch 450, val loss: 0.8715302348136902
Epoch 460, training loss: 0.6747727990150452 = 0.039552055299282074 + 0.1 * 6.352207660675049
Epoch 460, val loss: 0.8837036490440369
Epoch 470, training loss: 0.6696150898933411 = 0.036706287413835526 + 0.1 * 6.32908821105957
Epoch 470, val loss: 0.8956508636474609
Epoch 480, training loss: 0.6662677526473999 = 0.03415738791227341 + 0.1 * 6.321103572845459
Epoch 480, val loss: 0.9073618650436401
Epoch 490, training loss: 0.6640409827232361 = 0.03186662122607231 + 0.1 * 6.321743488311768
Epoch 490, val loss: 0.9185322523117065
Epoch 500, training loss: 0.6611325144767761 = 0.029809337109327316 + 0.1 * 6.313231468200684
Epoch 500, val loss: 0.9297020435333252
Epoch 510, training loss: 0.6589851975440979 = 0.027946755290031433 + 0.1 * 6.310384273529053
Epoch 510, val loss: 0.9404339790344238
Epoch 520, training loss: 0.6563100218772888 = 0.026258716359734535 + 0.1 * 6.300512790679932
Epoch 520, val loss: 0.9508530497550964
Epoch 530, training loss: 0.6540560126304626 = 0.02472599595785141 + 0.1 * 6.293300151824951
Epoch 530, val loss: 0.9611098766326904
Epoch 540, training loss: 0.6531656384468079 = 0.02332456037402153 + 0.1 * 6.298410892486572
Epoch 540, val loss: 0.9710040092468262
Epoch 550, training loss: 0.6510631442070007 = 0.022048087790608406 + 0.1 * 6.290150165557861
Epoch 550, val loss: 0.980504035949707
Epoch 560, training loss: 0.6490419507026672 = 0.02088255062699318 + 0.1 * 6.2815937995910645
Epoch 560, val loss: 0.9900496602058411
Epoch 570, training loss: 0.6478110551834106 = 0.019808931276202202 + 0.1 * 6.2800211906433105
Epoch 570, val loss: 0.9990061521530151
Epoch 580, training loss: 0.6456652283668518 = 0.018824193626642227 + 0.1 * 6.268409729003906
Epoch 580, val loss: 1.0080119371414185
Epoch 590, training loss: 0.646587610244751 = 0.01791277527809143 + 0.1 * 6.28674840927124
Epoch 590, val loss: 1.016662836074829
Epoch 600, training loss: 0.6432437896728516 = 0.017071640118956566 + 0.1 * 6.261721134185791
Epoch 600, val loss: 1.0249814987182617
Epoch 610, training loss: 0.642187237739563 = 0.01629546284675598 + 0.1 * 6.258917331695557
Epoch 610, val loss: 1.0334157943725586
Epoch 620, training loss: 0.6414496898651123 = 0.015570670366287231 + 0.1 * 6.258789539337158
Epoch 620, val loss: 1.0414515733718872
Epoch 630, training loss: 0.6399716138839722 = 0.014895959757268429 + 0.1 * 6.25075626373291
Epoch 630, val loss: 1.0492374897003174
Epoch 640, training loss: 0.6410148739814758 = 0.014268026687204838 + 0.1 * 6.267468452453613
Epoch 640, val loss: 1.0569745302200317
Epoch 650, training loss: 0.6381223201751709 = 0.01368299126625061 + 0.1 * 6.2443928718566895
Epoch 650, val loss: 1.0644299983978271
Epoch 660, training loss: 0.6385037302970886 = 0.013136162422597408 + 0.1 * 6.253675937652588
Epoch 660, val loss: 1.0719212293624878
Epoch 670, training loss: 0.6375262141227722 = 0.012621549889445305 + 0.1 * 6.249046802520752
Epoch 670, val loss: 1.078730821609497
Epoch 680, training loss: 0.6355609893798828 = 0.012144394218921661 + 0.1 * 6.234165668487549
Epoch 680, val loss: 1.0859662294387817
Epoch 690, training loss: 0.6346344947814941 = 0.011693884618580341 + 0.1 * 6.229405879974365
Epoch 690, val loss: 1.0928988456726074
Epoch 700, training loss: 0.6356991529464722 = 0.011268547736108303 + 0.1 * 6.2443060874938965
Epoch 700, val loss: 1.0994884967803955
Epoch 710, training loss: 0.6340770721435547 = 0.01086871325969696 + 0.1 * 6.232083320617676
Epoch 710, val loss: 1.1060755252838135
Epoch 720, training loss: 0.6330868005752563 = 0.010492400266230106 + 0.1 * 6.2259440422058105
Epoch 720, val loss: 1.1124495267868042
Epoch 730, training loss: 0.6321301460266113 = 0.010138199664652348 + 0.1 * 6.219919204711914
Epoch 730, val loss: 1.118933081626892
Epoch 740, training loss: 0.6323033571243286 = 0.009802216663956642 + 0.1 * 6.225011348724365
Epoch 740, val loss: 1.125120759010315
Epoch 750, training loss: 0.6305436491966248 = 0.009483790956437588 + 0.1 * 6.210598468780518
Epoch 750, val loss: 1.1311025619506836
Epoch 760, training loss: 0.6306618452072144 = 0.009183496236801147 + 0.1 * 6.214783191680908
Epoch 760, val loss: 1.1372286081314087
Epoch 770, training loss: 0.6317521929740906 = 0.008897320367395878 + 0.1 * 6.228549003601074
Epoch 770, val loss: 1.1430190801620483
Epoch 780, training loss: 0.6295801401138306 = 0.00862630270421505 + 0.1 * 6.209538459777832
Epoch 780, val loss: 1.1488254070281982
Epoch 790, training loss: 0.6296547055244446 = 0.008369873277842999 + 0.1 * 6.21284818649292
Epoch 790, val loss: 1.15464186668396
Epoch 800, training loss: 0.6285700798034668 = 0.008123897016048431 + 0.1 * 6.204462051391602
Epoch 800, val loss: 1.1600937843322754
Epoch 810, training loss: 0.6279810070991516 = 0.007892066612839699 + 0.1 * 6.2008891105651855
Epoch 810, val loss: 1.165759563446045
Epoch 820, training loss: 0.6288354992866516 = 0.007669354323297739 + 0.1 * 6.2116618156433105
Epoch 820, val loss: 1.1709997653961182
Epoch 830, training loss: 0.6269358992576599 = 0.007458909414708614 + 0.1 * 6.194769859313965
Epoch 830, val loss: 1.1763395071029663
Epoch 840, training loss: 0.6265124678611755 = 0.007259204052388668 + 0.1 * 6.192532539367676
Epoch 840, val loss: 1.1818368434906006
Epoch 850, training loss: 0.6269447207450867 = 0.007066693622618914 + 0.1 * 6.198780536651611
Epoch 850, val loss: 1.1868822574615479
Epoch 860, training loss: 0.6257538795471191 = 0.006882864516228437 + 0.1 * 6.1887102127075195
Epoch 860, val loss: 1.1918790340423584
Epoch 870, training loss: 0.6250816583633423 = 0.006708648055791855 + 0.1 * 6.183730125427246
Epoch 870, val loss: 1.1971232891082764
Epoch 880, training loss: 0.6258775591850281 = 0.006540106609463692 + 0.1 * 6.193374156951904
Epoch 880, val loss: 1.201951026916504
Epoch 890, training loss: 0.62535160779953 = 0.006378507707268 + 0.1 * 6.189730644226074
Epoch 890, val loss: 1.2066010236740112
Epoch 900, training loss: 0.6242778301239014 = 0.006225354038178921 + 0.1 * 6.180524826049805
Epoch 900, val loss: 1.2114779949188232
Epoch 910, training loss: 0.6246264576911926 = 0.006077758967876434 + 0.1 * 6.185486793518066
Epoch 910, val loss: 1.2161511182785034
Epoch 920, training loss: 0.6234185099601746 = 0.005936042871326208 + 0.1 * 6.1748247146606445
Epoch 920, val loss: 1.2207340002059937
Epoch 930, training loss: 0.6236967444419861 = 0.005800493992865086 + 0.1 * 6.178962707519531
Epoch 930, val loss: 1.2253509759902954
Epoch 940, training loss: 0.6227096319198608 = 0.005669106729328632 + 0.1 * 6.17040491104126
Epoch 940, val loss: 1.2297954559326172
Epoch 950, training loss: 0.62325519323349 = 0.005542517174035311 + 0.1 * 6.177126884460449
Epoch 950, val loss: 1.2340087890625
Epoch 960, training loss: 0.622298002243042 = 0.0054219383746385574 + 0.1 * 6.168760776519775
Epoch 960, val loss: 1.2384520769119263
Epoch 970, training loss: 0.6226247549057007 = 0.005306062288582325 + 0.1 * 6.173186779022217
Epoch 970, val loss: 1.2427823543548584
Epoch 980, training loss: 0.6214653849601746 = 0.005193551070988178 + 0.1 * 6.162718296051025
Epoch 980, val loss: 1.2468791007995605
Epoch 990, training loss: 0.6220505833625793 = 0.005085783079266548 + 0.1 * 6.169648170471191
Epoch 990, val loss: 1.251096487045288
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.6679
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7834651470184326 = 1.94608473777771 + 0.1 * 8.37380313873291
Epoch 0, val loss: 1.939666509628296
Epoch 10, training loss: 2.7738256454467773 = 1.9364713430404663 + 0.1 * 8.373543739318848
Epoch 10, val loss: 1.9299687147140503
Epoch 20, training loss: 2.761836528778076 = 1.924649953842163 + 0.1 * 8.371864318847656
Epoch 20, val loss: 1.9179306030273438
Epoch 30, training loss: 2.7439124584198 = 1.907910704612732 + 0.1 * 8.360017776489258
Epoch 30, val loss: 1.9010502099990845
Epoch 40, training loss: 2.7119288444519043 = 1.8829739093780518 + 0.1 * 8.289548873901367
Epoch 40, val loss: 1.8765932321548462
Epoch 50, training loss: 2.637335777282715 = 1.8502389192581177 + 0.1 * 7.870967388153076
Epoch 50, val loss: 1.8463919162750244
Epoch 60, training loss: 2.557877540588379 = 1.8159599304199219 + 0.1 * 7.419177055358887
Epoch 60, val loss: 1.8161786794662476
Epoch 70, training loss: 2.481235980987549 = 1.7832040786743164 + 0.1 * 6.980319976806641
Epoch 70, val loss: 1.7887399196624756
Epoch 80, training loss: 2.4281444549560547 = 1.7522132396697998 + 0.1 * 6.759311199188232
Epoch 80, val loss: 1.762860894203186
Epoch 90, training loss: 2.3794524669647217 = 1.7131696939468384 + 0.1 * 6.66282844543457
Epoch 90, val loss: 1.7282429933547974
Epoch 100, training loss: 2.3216981887817383 = 1.659628987312317 + 0.1 * 6.620692729949951
Epoch 100, val loss: 1.6821625232696533
Epoch 110, training loss: 2.2484350204467773 = 1.5888993740081787 + 0.1 * 6.595356464385986
Epoch 110, val loss: 1.6242525577545166
Epoch 120, training loss: 2.1600751876831055 = 1.5024895668029785 + 0.1 * 6.575854778289795
Epoch 120, val loss: 1.5540467500686646
Epoch 130, training loss: 2.0620615482330322 = 1.406308650970459 + 0.1 * 6.557528018951416
Epoch 130, val loss: 1.475996732711792
Epoch 140, training loss: 1.9620237350463867 = 1.3080419301986694 + 0.1 * 6.539817810058594
Epoch 140, val loss: 1.3969730138778687
Epoch 150, training loss: 1.8645946979522705 = 1.2121927738189697 + 0.1 * 6.524019718170166
Epoch 150, val loss: 1.3201098442077637
Epoch 160, training loss: 1.7740478515625 = 1.1229029893875122 + 0.1 * 6.511448383331299
Epoch 160, val loss: 1.2487576007843018
Epoch 170, training loss: 1.6929349899291992 = 1.0433475971221924 + 0.1 * 6.49587345123291
Epoch 170, val loss: 1.1871342658996582
Epoch 180, training loss: 1.6211230754852295 = 0.9725267887115479 + 0.1 * 6.4859619140625
Epoch 180, val loss: 1.133679986000061
Epoch 190, training loss: 1.5557260513305664 = 0.9080629944801331 + 0.1 * 6.476631164550781
Epoch 190, val loss: 1.0863734483718872
Epoch 200, training loss: 1.4926199913024902 = 0.8458481431007385 + 0.1 * 6.46771764755249
Epoch 200, val loss: 1.040934681892395
Epoch 210, training loss: 1.4292761087417603 = 0.7829806208610535 + 0.1 * 6.462954998016357
Epoch 210, val loss: 0.9945988655090332
Epoch 220, training loss: 1.3642688989639282 = 0.718578040599823 + 0.1 * 6.456908702850342
Epoch 220, val loss: 0.9464753866195679
Epoch 230, training loss: 1.3001415729522705 = 0.6543119549751282 + 0.1 * 6.458296298980713
Epoch 230, val loss: 0.898367702960968
Epoch 240, training loss: 1.236541986465454 = 0.5919080376625061 + 0.1 * 6.446339130401611
Epoch 240, val loss: 0.8521354794502258
Epoch 250, training loss: 1.1773788928985596 = 0.5327609181404114 + 0.1 * 6.44618034362793
Epoch 250, val loss: 0.8093165159225464
Epoch 260, training loss: 1.1226751804351807 = 0.47865819931030273 + 0.1 * 6.4401702880859375
Epoch 260, val loss: 0.7719359397888184
Epoch 270, training loss: 1.0726863145828247 = 0.4296550154685974 + 0.1 * 6.430312633514404
Epoch 270, val loss: 0.7404718995094299
Epoch 280, training loss: 1.0281143188476562 = 0.38493824005126953 + 0.1 * 6.431759834289551
Epoch 280, val loss: 0.7143990993499756
Epoch 290, training loss: 0.9855811595916748 = 0.343816339969635 + 0.1 * 6.4176483154296875
Epoch 290, val loss: 0.6923165917396545
Epoch 300, training loss: 0.946901798248291 = 0.3054790198802948 + 0.1 * 6.4142279624938965
Epoch 300, val loss: 0.6733652949333191
Epoch 310, training loss: 0.9108315706253052 = 0.2701122760772705 + 0.1 * 6.407192707061768
Epoch 310, val loss: 0.6573130488395691
Epoch 320, training loss: 0.8774588704109192 = 0.23783785104751587 + 0.1 * 6.396210193634033
Epoch 320, val loss: 0.6439585089683533
Epoch 330, training loss: 0.8473638892173767 = 0.2087116241455078 + 0.1 * 6.3865227699279785
Epoch 330, val loss: 0.6332349181175232
Epoch 340, training loss: 0.8226380348205566 = 0.18299666047096252 + 0.1 * 6.396413326263428
Epoch 340, val loss: 0.6253189444541931
Epoch 350, training loss: 0.7984656095504761 = 0.16084854304790497 + 0.1 * 6.3761701583862305
Epoch 350, val loss: 0.6201825141906738
Epoch 360, training loss: 0.7805933356285095 = 0.14185883104801178 + 0.1 * 6.387344837188721
Epoch 360, val loss: 0.6174675822257996
Epoch 370, training loss: 0.7622764110565186 = 0.12575778365135193 + 0.1 * 6.3651862144470215
Epoch 370, val loss: 0.6170572638511658
Epoch 380, training loss: 0.7473251819610596 = 0.11203007400035858 + 0.1 * 6.3529510498046875
Epoch 380, val loss: 0.6186042428016663
Epoch 390, training loss: 0.7355790734291077 = 0.10026009380817413 + 0.1 * 6.353189945220947
Epoch 390, val loss: 0.6215279698371887
Epoch 400, training loss: 0.7243033647537231 = 0.09015092998743057 + 0.1 * 6.341524124145508
Epoch 400, val loss: 0.6256586909294128
Epoch 410, training loss: 0.7168400287628174 = 0.08139470219612122 + 0.1 * 6.354453086853027
Epoch 410, val loss: 0.630661129951477
Epoch 420, training loss: 0.7078250050544739 = 0.0738261491060257 + 0.1 * 6.339988708496094
Epoch 420, val loss: 0.6362124681472778
Epoch 430, training loss: 0.6998963356018066 = 0.06721004843711853 + 0.1 * 6.3268632888793945
Epoch 430, val loss: 0.6423209309577942
Epoch 440, training loss: 0.6932297348976135 = 0.061368655413389206 + 0.1 * 6.318610668182373
Epoch 440, val loss: 0.6486616134643555
Epoch 450, training loss: 0.6888288855552673 = 0.056211892515420914 + 0.1 * 6.326169967651367
Epoch 450, val loss: 0.6553597450256348
Epoch 460, training loss: 0.6830491423606873 = 0.05166073143482208 + 0.1 * 6.313884258270264
Epoch 460, val loss: 0.6620738506317139
Epoch 470, training loss: 0.6781260371208191 = 0.04761315882205963 + 0.1 * 6.305129051208496
Epoch 470, val loss: 0.6688881516456604
Epoch 480, training loss: 0.6737496256828308 = 0.044002946466207504 + 0.1 * 6.29746675491333
Epoch 480, val loss: 0.6757017970085144
Epoch 490, training loss: 0.6704251766204834 = 0.040757521986961365 + 0.1 * 6.296676158905029
Epoch 490, val loss: 0.6825864911079407
Epoch 500, training loss: 0.668464183807373 = 0.03784805163741112 + 0.1 * 6.306160926818848
Epoch 500, val loss: 0.689346432685852
Epoch 510, training loss: 0.6642521619796753 = 0.035242050886154175 + 0.1 * 6.290100574493408
Epoch 510, val loss: 0.6961175799369812
Epoch 520, training loss: 0.6605963706970215 = 0.0328829288482666 + 0.1 * 6.277134418487549
Epoch 520, val loss: 0.7027421593666077
Epoch 530, training loss: 0.6590862274169922 = 0.030742209404706955 + 0.1 * 6.283440113067627
Epoch 530, val loss: 0.7092728018760681
Epoch 540, training loss: 0.6583163142204285 = 0.028803253546357155 + 0.1 * 6.295130729675293
Epoch 540, val loss: 0.7158317565917969
Epoch 550, training loss: 0.6545462608337402 = 0.027051419019699097 + 0.1 * 6.274948596954346
Epoch 550, val loss: 0.7221752405166626
Epoch 560, training loss: 0.6518776416778564 = 0.025457680225372314 + 0.1 * 6.264199733734131
Epoch 560, val loss: 0.7284160256385803
Epoch 570, training loss: 0.6493285894393921 = 0.023999374359846115 + 0.1 * 6.253291606903076
Epoch 570, val loss: 0.7345422506332397
Epoch 580, training loss: 0.6482461094856262 = 0.022662518545985222 + 0.1 * 6.255836009979248
Epoch 580, val loss: 0.7405339479446411
Epoch 590, training loss: 0.6467419862747192 = 0.021443814039230347 + 0.1 * 6.252982139587402
Epoch 590, val loss: 0.7464959025382996
Epoch 600, training loss: 0.646481454372406 = 0.020324403420090675 + 0.1 * 6.261570453643799
Epoch 600, val loss: 0.7523505091667175
Epoch 610, training loss: 0.643890380859375 = 0.019292695447802544 + 0.1 * 6.245976448059082
Epoch 610, val loss: 0.7579625844955444
Epoch 620, training loss: 0.6437722444534302 = 0.018344204872846603 + 0.1 * 6.254280090332031
Epoch 620, val loss: 0.7635459899902344
Epoch 630, training loss: 0.641076922416687 = 0.017466021701693535 + 0.1 * 6.236109256744385
Epoch 630, val loss: 0.7690522074699402
Epoch 640, training loss: 0.6398188471794128 = 0.016655441373586655 + 0.1 * 6.231634140014648
Epoch 640, val loss: 0.7744420766830444
Epoch 650, training loss: 0.6389381885528564 = 0.01590040884912014 + 0.1 * 6.230377197265625
Epoch 650, val loss: 0.7797076106071472
Epoch 660, training loss: 0.6386054754257202 = 0.01519985031336546 + 0.1 * 6.234055995941162
Epoch 660, val loss: 0.7848248481750488
Epoch 670, training loss: 0.6370494365692139 = 0.01454741507768631 + 0.1 * 6.225020408630371
Epoch 670, val loss: 0.7898572087287903
Epoch 680, training loss: 0.6358222961425781 = 0.013942662626504898 + 0.1 * 6.218796253204346
Epoch 680, val loss: 0.7948490977287292
Epoch 690, training loss: 0.6349108219146729 = 0.013375213369727135 + 0.1 * 6.21535587310791
Epoch 690, val loss: 0.7996931076049805
Epoch 700, training loss: 0.6341280341148376 = 0.012844179756939411 + 0.1 * 6.212838172912598
Epoch 700, val loss: 0.8044142723083496
Epoch 710, training loss: 0.6327759623527527 = 0.012348485179245472 + 0.1 * 6.204274654388428
Epoch 710, val loss: 0.809133768081665
Epoch 720, training loss: 0.6333218216896057 = 0.011882162652909756 + 0.1 * 6.2143964767456055
Epoch 720, val loss: 0.8137967586517334
Epoch 730, training loss: 0.6316942572593689 = 0.011442937888205051 + 0.1 * 6.202512741088867
Epoch 730, val loss: 0.8182848691940308
Epoch 740, training loss: 0.631252110004425 = 0.01103109773248434 + 0.1 * 6.202209949493408
Epoch 740, val loss: 0.8227115869522095
Epoch 750, training loss: 0.6303648352622986 = 0.010644182562828064 + 0.1 * 6.197206020355225
Epoch 750, val loss: 0.8271276950836182
Epoch 760, training loss: 0.6312083005905151 = 0.010277850553393364 + 0.1 * 6.209304332733154
Epoch 760, val loss: 0.8314092755317688
Epoch 770, training loss: 0.6289604306221008 = 0.009930942207574844 + 0.1 * 6.1902947425842285
Epoch 770, val loss: 0.8355576992034912
Epoch 780, training loss: 0.6284774541854858 = 0.009606278501451015 + 0.1 * 6.188711643218994
Epoch 780, val loss: 0.8397618532180786
Epoch 790, training loss: 0.6271904706954956 = 0.00929713062942028 + 0.1 * 6.178933143615723
Epoch 790, val loss: 0.8438655734062195
Epoch 800, training loss: 0.6290941834449768 = 0.0090033495798707 + 0.1 * 6.200908184051514
Epoch 800, val loss: 0.8478543162345886
Epoch 810, training loss: 0.6265568137168884 = 0.008724458515644073 + 0.1 * 6.178323745727539
Epoch 810, val loss: 0.8517919778823853
Epoch 820, training loss: 0.626461386680603 = 0.008461895398795605 + 0.1 * 6.179994583129883
Epoch 820, val loss: 0.8557167649269104
Epoch 830, training loss: 0.6263749599456787 = 0.008210374973714352 + 0.1 * 6.18164587020874
Epoch 830, val loss: 0.8595048189163208
Epoch 840, training loss: 0.6250946521759033 = 0.00797195266932249 + 0.1 * 6.171226978302002
Epoch 840, val loss: 0.8632145524024963
Epoch 850, training loss: 0.6241570115089417 = 0.007746448274701834 + 0.1 * 6.164105415344238
Epoch 850, val loss: 0.8669058680534363
Epoch 860, training loss: 0.626255214214325 = 0.007529573980718851 + 0.1 * 6.187256336212158
Epoch 860, val loss: 0.8705597519874573
Epoch 870, training loss: 0.6236361265182495 = 0.007321671117097139 + 0.1 * 6.163144588470459
Epoch 870, val loss: 0.8741506338119507
Epoch 880, training loss: 0.6232928037643433 = 0.00712610874325037 + 0.1 * 6.1616668701171875
Epoch 880, val loss: 0.8777315616607666
Epoch 890, training loss: 0.6234564781188965 = 0.00693837134167552 + 0.1 * 6.165180683135986
Epoch 890, val loss: 0.8812664747238159
Epoch 900, training loss: 0.6232532858848572 = 0.006757455412298441 + 0.1 * 6.164958477020264
Epoch 900, val loss: 0.8847097158432007
Epoch 910, training loss: 0.6225936412811279 = 0.006585210561752319 + 0.1 * 6.1600847244262695
Epoch 910, val loss: 0.8881369233131409
Epoch 920, training loss: 0.6221020817756653 = 0.006420500110834837 + 0.1 * 6.156816005706787
Epoch 920, val loss: 0.8915517926216125
Epoch 930, training loss: 0.6239851117134094 = 0.006261478643864393 + 0.1 * 6.177236557006836
Epoch 930, val loss: 0.894966185092926
Epoch 940, training loss: 0.6216959953308105 = 0.006109705660492182 + 0.1 * 6.155862808227539
Epoch 940, val loss: 0.8982788920402527
Epoch 950, training loss: 0.6214349865913391 = 0.005965737160295248 + 0.1 * 6.154692649841309
Epoch 950, val loss: 0.9015973210334778
Epoch 960, training loss: 0.622124433517456 = 0.005826165433973074 + 0.1 * 6.16298246383667
Epoch 960, val loss: 0.9048591256141663
Epoch 970, training loss: 0.6202913522720337 = 0.00569169269874692 + 0.1 * 6.145996570587158
Epoch 970, val loss: 0.9080203175544739
Epoch 980, training loss: 0.6210166215896606 = 0.0055631836876273155 + 0.1 * 6.154534339904785
Epoch 980, val loss: 0.9111667275428772
Epoch 990, training loss: 0.6203399300575256 = 0.00543883815407753 + 0.1 * 6.14901065826416
Epoch 990, val loss: 0.9142976999282837
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6052
Flip ASR: 0.5244/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7955355644226074 = 1.9581719636917114 + 0.1 * 8.373637199401855
Epoch 0, val loss: 1.9677820205688477
Epoch 10, training loss: 2.784281015396118 = 1.9470268487930298 + 0.1 * 8.372541427612305
Epoch 10, val loss: 1.9559202194213867
Epoch 20, training loss: 2.7699568271636963 = 1.9330909252166748 + 0.1 * 8.368658065795898
Epoch 20, val loss: 1.9402945041656494
Epoch 30, training loss: 2.749375820159912 = 1.9134585857391357 + 0.1 * 8.359171867370605
Epoch 30, val loss: 1.917937994003296
Epoch 40, training loss: 2.71610426902771 = 1.8851341009140015 + 0.1 * 8.309701919555664
Epoch 40, val loss: 1.8863763809204102
Epoch 50, training loss: 2.6528892517089844 = 1.8485682010650635 + 0.1 * 8.043210983276367
Epoch 50, val loss: 1.8479995727539062
Epoch 60, training loss: 2.5675313472747803 = 1.8090919256210327 + 0.1 * 7.584394454956055
Epoch 60, val loss: 1.809064507484436
Epoch 70, training loss: 2.4825799465179443 = 1.771216869354248 + 0.1 * 7.113630771636963
Epoch 70, val loss: 1.7726373672485352
Epoch 80, training loss: 2.4204227924346924 = 1.7336349487304688 + 0.1 * 6.8678789138793945
Epoch 80, val loss: 1.7378230094909668
Epoch 90, training loss: 2.3681068420410156 = 1.6907877922058105 + 0.1 * 6.773190498352051
Epoch 90, val loss: 1.6988062858581543
Epoch 100, training loss: 2.3075766563415527 = 1.6338231563568115 + 0.1 * 6.737534046173096
Epoch 100, val loss: 1.6492644548416138
Epoch 110, training loss: 2.230288028717041 = 1.559083342552185 + 0.1 * 6.712047100067139
Epoch 110, val loss: 1.5878815650939941
Epoch 120, training loss: 2.136608123779297 = 1.4670195579528809 + 0.1 * 6.695886611938477
Epoch 120, val loss: 1.5141521692276
Epoch 130, training loss: 2.031050205230713 = 1.3625344038009644 + 0.1 * 6.685158729553223
Epoch 130, val loss: 1.4282848834991455
Epoch 140, training loss: 1.9194023609161377 = 1.25174880027771 + 0.1 * 6.676535129547119
Epoch 140, val loss: 1.3391433954238892
Epoch 150, training loss: 1.8052515983581543 = 1.13837468624115 + 0.1 * 6.668769359588623
Epoch 150, val loss: 1.2499524354934692
Epoch 160, training loss: 1.6917309761047363 = 1.0257445573806763 + 0.1 * 6.6598639488220215
Epoch 160, val loss: 1.1631355285644531
Epoch 170, training loss: 1.5848748683929443 = 0.9191935658454895 + 0.1 * 6.656813144683838
Epoch 170, val loss: 1.081834077835083
Epoch 180, training loss: 1.4887317419052124 = 0.8245524764060974 + 0.1 * 6.6417927742004395
Epoch 180, val loss: 1.0099908113479614
Epoch 190, training loss: 1.4057588577270508 = 0.7428337931632996 + 0.1 * 6.629250526428223
Epoch 190, val loss: 0.9493820667266846
Epoch 200, training loss: 1.3348791599273682 = 0.6735275387763977 + 0.1 * 6.613516330718994
Epoch 200, val loss: 0.9006710052490234
Epoch 210, training loss: 1.2738535404205322 = 0.6142619848251343 + 0.1 * 6.595916271209717
Epoch 210, val loss: 0.8626300096511841
Epoch 220, training loss: 1.2202237844467163 = 0.5622049570083618 + 0.1 * 6.580188274383545
Epoch 220, val loss: 0.8335076570510864
Epoch 230, training loss: 1.1711444854736328 = 0.5146329998970032 + 0.1 * 6.565114974975586
Epoch 230, val loss: 0.8108877539634705
Epoch 240, training loss: 1.125616431236267 = 0.4701070189476013 + 0.1 * 6.555094242095947
Epoch 240, val loss: 0.7925459742546082
Epoch 250, training loss: 1.0827399492263794 = 0.42855092883110046 + 0.1 * 6.5418901443481445
Epoch 250, val loss: 0.7773581147193909
Epoch 260, training loss: 1.0422301292419434 = 0.38956689834594727 + 0.1 * 6.526631832122803
Epoch 260, val loss: 0.7645536065101624
Epoch 270, training loss: 1.0041186809539795 = 0.35279974341392517 + 0.1 * 6.51318883895874
Epoch 270, val loss: 0.7536633014678955
Epoch 280, training loss: 0.9688891172409058 = 0.31813469529151917 + 0.1 * 6.507543563842773
Epoch 280, val loss: 0.7446110844612122
Epoch 290, training loss: 0.9354978799819946 = 0.2860230505466461 + 0.1 * 6.494748115539551
Epoch 290, val loss: 0.7376238703727722
Epoch 300, training loss: 0.9042632579803467 = 0.2564735412597656 + 0.1 * 6.4778971672058105
Epoch 300, val loss: 0.7329333424568176
Epoch 310, training loss: 0.8762673139572144 = 0.22932842373847961 + 0.1 * 6.469388961791992
Epoch 310, val loss: 0.7305052876472473
Epoch 320, training loss: 0.8501241207122803 = 0.20475439727306366 + 0.1 * 6.4536967277526855
Epoch 320, val loss: 0.7305669188499451
Epoch 330, training loss: 0.8299123048782349 = 0.18279632925987244 + 0.1 * 6.471159934997559
Epoch 330, val loss: 0.733007550239563
Epoch 340, training loss: 0.8064674139022827 = 0.16346213221549988 + 0.1 * 6.430053234100342
Epoch 340, val loss: 0.7374377846717834
Epoch 350, training loss: 0.7878785133361816 = 0.1463504433631897 + 0.1 * 6.415280818939209
Epoch 350, val loss: 0.7435134649276733
Epoch 360, training loss: 0.7723163366317749 = 0.13130328059196472 + 0.1 * 6.410130023956299
Epoch 360, val loss: 0.7511768937110901
Epoch 370, training loss: 0.7582806944847107 = 0.11814408004283905 + 0.1 * 6.401365756988525
Epoch 370, val loss: 0.7598710656166077
Epoch 380, training loss: 0.7460667490959167 = 0.10659242421388626 + 0.1 * 6.394742965698242
Epoch 380, val loss: 0.7695404887199402
Epoch 390, training loss: 0.7340132594108582 = 0.09647312015295029 + 0.1 * 6.375401496887207
Epoch 390, val loss: 0.7799230217933655
Epoch 400, training loss: 0.7264944314956665 = 0.08756343275308609 + 0.1 * 6.389309406280518
Epoch 400, val loss: 0.7907868027687073
Epoch 410, training loss: 0.7160041928291321 = 0.07972544431686401 + 0.1 * 6.362787246704102
Epoch 410, val loss: 0.8018835186958313
Epoch 420, training loss: 0.7083612084388733 = 0.07278227806091309 + 0.1 * 6.3557891845703125
Epoch 420, val loss: 0.8131971955299377
Epoch 430, training loss: 0.7013090252876282 = 0.06662427634000778 + 0.1 * 6.3468475341796875
Epoch 430, val loss: 0.8245587944984436
Epoch 440, training loss: 0.6951878666877747 = 0.061136797070503235 + 0.1 * 6.340510845184326
Epoch 440, val loss: 0.8358929753303528
Epoch 450, training loss: 0.6903034448623657 = 0.05625038966536522 + 0.1 * 6.3405303955078125
Epoch 450, val loss: 0.8471173048019409
Epoch 460, training loss: 0.6849023103713989 = 0.05190004035830498 + 0.1 * 6.330022811889648
Epoch 460, val loss: 0.8581272959709167
Epoch 470, training loss: 0.6794059872627258 = 0.04799259454011917 + 0.1 * 6.314134120941162
Epoch 470, val loss: 0.8689929246902466
Epoch 480, training loss: 0.6787868738174438 = 0.04447619989514351 + 0.1 * 6.343106746673584
Epoch 480, val loss: 0.8797081112861633
Epoch 490, training loss: 0.6726207733154297 = 0.04132850840687752 + 0.1 * 6.312922477722168
Epoch 490, val loss: 0.8902307152748108
Epoch 500, training loss: 0.6690274477005005 = 0.038502298295497894 + 0.1 * 6.305251598358154
Epoch 500, val loss: 0.9005810022354126
Epoch 510, training loss: 0.6653394103050232 = 0.03595125302672386 + 0.1 * 6.293881416320801
Epoch 510, val loss: 0.9107053875923157
Epoch 520, training loss: 0.6635569334030151 = 0.03364340215921402 + 0.1 * 6.299135208129883
Epoch 520, val loss: 0.9206523895263672
Epoch 530, training loss: 0.6605627536773682 = 0.031548529863357544 + 0.1 * 6.290142059326172
Epoch 530, val loss: 0.9303862452507019
Epoch 540, training loss: 0.6590008735656738 = 0.029640940949320793 + 0.1 * 6.293599605560303
Epoch 540, val loss: 0.9399591684341431
Epoch 550, training loss: 0.6559875011444092 = 0.027904653921723366 + 0.1 * 6.280828475952148
Epoch 550, val loss: 0.9493290781974792
Epoch 560, training loss: 0.6539115309715271 = 0.02631758153438568 + 0.1 * 6.275939464569092
Epoch 560, val loss: 0.958508312702179
Epoch 570, training loss: 0.6516572833061218 = 0.024867195636034012 + 0.1 * 6.2679009437561035
Epoch 570, val loss: 0.967484176158905
Epoch 580, training loss: 0.649716854095459 = 0.023541100323200226 + 0.1 * 6.2617573738098145
Epoch 580, val loss: 0.9761632680892944
Epoch 590, training loss: 0.6478683948516846 = 0.02231929637491703 + 0.1 * 6.255490779876709
Epoch 590, val loss: 0.984695315361023
Epoch 600, training loss: 0.6484092473983765 = 0.021190796047449112 + 0.1 * 6.272184371948242
Epoch 600, val loss: 0.9931035041809082
Epoch 610, training loss: 0.6455956697463989 = 0.020151644945144653 + 0.1 * 6.254439830780029
Epoch 610, val loss: 1.0013139247894287
Epoch 620, training loss: 0.6441076993942261 = 0.01919199712574482 + 0.1 * 6.249156951904297
Epoch 620, val loss: 1.0093438625335693
Epoch 630, training loss: 0.6423598527908325 = 0.018301965668797493 + 0.1 * 6.240578651428223
Epoch 630, val loss: 1.0171819925308228
Epoch 640, training loss: 0.6431803703308105 = 0.017474118620157242 + 0.1 * 6.2570624351501465
Epoch 640, val loss: 1.0248833894729614
Epoch 650, training loss: 0.6401394605636597 = 0.016704870387911797 + 0.1 * 6.23434591293335
Epoch 650, val loss: 1.0324037075042725
Epoch 660, training loss: 0.6397027969360352 = 0.015988947823643684 + 0.1 * 6.237138271331787
Epoch 660, val loss: 1.039778470993042
Epoch 670, training loss: 0.6372572779655457 = 0.01532017346471548 + 0.1 * 6.2193708419799805
Epoch 670, val loss: 1.0470103025436401
Epoch 680, training loss: 0.6386877298355103 = 0.01469525694847107 + 0.1 * 6.239924907684326
Epoch 680, val loss: 1.0540847778320312
Epoch 690, training loss: 0.6371872425079346 = 0.014109402894973755 + 0.1 * 6.230777740478516
Epoch 690, val loss: 1.0609804391860962
Epoch 700, training loss: 0.6354331374168396 = 0.01356299128383398 + 0.1 * 6.218701362609863
Epoch 700, val loss: 1.0677329301834106
Epoch 710, training loss: 0.636064887046814 = 0.013048223219811916 + 0.1 * 6.230166435241699
Epoch 710, val loss: 1.0743299722671509
Epoch 720, training loss: 0.633493959903717 = 0.012565044686198235 + 0.1 * 6.209289073944092
Epoch 720, val loss: 1.0807980298995972
Epoch 730, training loss: 0.6329559087753296 = 0.012110779993236065 + 0.1 * 6.208451271057129
Epoch 730, val loss: 1.087158203125
Epoch 740, training loss: 0.6321186423301697 = 0.011681802570819855 + 0.1 * 6.2043681144714355
Epoch 740, val loss: 1.0933884382247925
Epoch 750, training loss: 0.6313068270683289 = 0.01127671543508768 + 0.1 * 6.200300693511963
Epoch 750, val loss: 1.0995075702667236
Epoch 760, training loss: 0.6328495740890503 = 0.01089335698634386 + 0.1 * 6.21956205368042
Epoch 760, val loss: 1.1055808067321777
Epoch 770, training loss: 0.6305913329124451 = 0.010532590560615063 + 0.1 * 6.200586795806885
Epoch 770, val loss: 1.1114426851272583
Epoch 780, training loss: 0.6303106546401978 = 0.010191615670919418 + 0.1 * 6.20119047164917
Epoch 780, val loss: 1.1172735691070557
Epoch 790, training loss: 0.6286025047302246 = 0.009867697954177856 + 0.1 * 6.187347888946533
Epoch 790, val loss: 1.1229759454727173
Epoch 800, training loss: 0.6288501024246216 = 0.009560802020132542 + 0.1 * 6.192892551422119
Epoch 800, val loss: 1.128550410270691
Epoch 810, training loss: 0.6297880411148071 = 0.009268510155379772 + 0.1 * 6.20519495010376
Epoch 810, val loss: 1.134099006652832
Epoch 820, training loss: 0.6283858418464661 = 0.008991288021206856 + 0.1 * 6.193945407867432
Epoch 820, val loss: 1.1395188570022583
Epoch 830, training loss: 0.6269654631614685 = 0.008728939108550549 + 0.1 * 6.182365417480469
Epoch 830, val loss: 1.14484703540802
Epoch 840, training loss: 0.6284154057502747 = 0.008478318341076374 + 0.1 * 6.199370384216309
Epoch 840, val loss: 1.1500608921051025
Epoch 850, training loss: 0.6258572936058044 = 0.008239072747528553 + 0.1 * 6.176181793212891
Epoch 850, val loss: 1.1552097797393799
Epoch 860, training loss: 0.6265928745269775 = 0.008011380210518837 + 0.1 * 6.185814380645752
Epoch 860, val loss: 1.160278558731079
Epoch 870, training loss: 0.6258463859558105 = 0.00779252452775836 + 0.1 * 6.180538654327393
Epoch 870, val loss: 1.165310263633728
Epoch 880, training loss: 0.6258546113967896 = 0.00758448988199234 + 0.1 * 6.182701110839844
Epoch 880, val loss: 1.170249104499817
Epoch 890, training loss: 0.6257304549217224 = 0.007385757751762867 + 0.1 * 6.183447360992432
Epoch 890, val loss: 1.1751066446304321
Epoch 900, training loss: 0.6239381432533264 = 0.007195133715867996 + 0.1 * 6.167430400848389
Epoch 900, val loss: 1.1799170970916748
Epoch 910, training loss: 0.6245261430740356 = 0.007013359107077122 + 0.1 * 6.1751275062561035
Epoch 910, val loss: 1.184615135192871
Epoch 920, training loss: 0.6233384013175964 = 0.0068384865298867226 + 0.1 * 6.164999008178711
Epoch 920, val loss: 1.1892437934875488
Epoch 930, training loss: 0.6236935257911682 = 0.006670747883617878 + 0.1 * 6.170227527618408
Epoch 930, val loss: 1.1938334703445435
Epoch 940, training loss: 0.6233952045440674 = 0.006510098930448294 + 0.1 * 6.168850898742676
Epoch 940, val loss: 1.1983505487442017
Epoch 950, training loss: 0.6227061152458191 = 0.006355943623930216 + 0.1 * 6.163501739501953
Epoch 950, val loss: 1.202771544456482
Epoch 960, training loss: 0.6236345171928406 = 0.0062079112976789474 + 0.1 * 6.174266338348389
Epoch 960, val loss: 1.2071253061294556
Epoch 970, training loss: 0.6227874159812927 = 0.0060651483945548534 + 0.1 * 6.16722297668457
Epoch 970, val loss: 1.211483359336853
Epoch 980, training loss: 0.6224417686462402 = 0.005928731057792902 + 0.1 * 6.165130138397217
Epoch 980, val loss: 1.2157167196273804
Epoch 990, training loss: 0.6216164827346802 = 0.005797322373837233 + 0.1 * 6.158191204071045
Epoch 990, val loss: 1.2199078798294067
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7823
Flip ASR: 0.7556/225 nodes
The final ASR:0.68512, 0.07333, Accuracy:0.80988, 0.01772
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9426])
updated graph: torch.Size([2, 10454])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98401, 0.00969, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.784377098083496 = 1.9469937086105347 + 0.1 * 8.373833656311035
Epoch 0, val loss: 1.9427437782287598
Epoch 10, training loss: 2.774857997894287 = 1.9374897480010986 + 0.1 * 8.37368106842041
Epoch 10, val loss: 1.9335236549377441
Epoch 20, training loss: 2.7627789974212646 = 1.925480604171753 + 0.1 * 8.3729829788208
Epoch 20, val loss: 1.921433925628662
Epoch 30, training loss: 2.745163917541504 = 1.9083060026168823 + 0.1 * 8.36857795715332
Epoch 30, val loss: 1.9039828777313232
Epoch 40, training loss: 2.7162861824035645 = 1.8825477361679077 + 0.1 * 8.337383270263672
Epoch 40, val loss: 1.8782262802124023
Epoch 50, training loss: 2.6568241119384766 = 1.8464053869247437 + 0.1 * 8.104186058044434
Epoch 50, val loss: 1.8440645933151245
Epoch 60, training loss: 2.549323081970215 = 1.808103084564209 + 0.1 * 7.412200450897217
Epoch 60, val loss: 1.8112179040908813
Epoch 70, training loss: 2.4799747467041016 = 1.7741973400115967 + 0.1 * 7.057775020599365
Epoch 70, val loss: 1.7831549644470215
Epoch 80, training loss: 2.4256932735443115 = 1.7392957210540771 + 0.1 * 6.863976001739502
Epoch 80, val loss: 1.7550266981124878
Epoch 90, training loss: 2.371859312057495 = 1.6951243877410889 + 0.1 * 6.7673492431640625
Epoch 90, val loss: 1.7174352407455444
Epoch 100, training loss: 2.3074517250061035 = 1.636149287223816 + 0.1 * 6.713025093078613
Epoch 100, val loss: 1.6671648025512695
Epoch 110, training loss: 2.2266573905944824 = 1.5599533319473267 + 0.1 * 6.667039394378662
Epoch 110, val loss: 1.603702187538147
Epoch 120, training loss: 2.1334946155548096 = 1.470442295074463 + 0.1 * 6.63052225112915
Epoch 120, val loss: 1.531260371208191
Epoch 130, training loss: 2.0365490913391113 = 1.376185655593872 + 0.1 * 6.603633880615234
Epoch 130, val loss: 1.4579893350601196
Epoch 140, training loss: 1.9408771991729736 = 1.2828140258789062 + 0.1 * 6.580630779266357
Epoch 140, val loss: 1.388664722442627
Epoch 150, training loss: 1.8480370044708252 = 1.1925686597824097 + 0.1 * 6.554682731628418
Epoch 150, val loss: 1.3243837356567383
Epoch 160, training loss: 1.7606709003448486 = 1.1076632738113403 + 0.1 * 6.53007698059082
Epoch 160, val loss: 1.2650208473205566
Epoch 170, training loss: 1.6814312934875488 = 1.0305169820785522 + 0.1 * 6.509143352508545
Epoch 170, val loss: 1.2113245725631714
Epoch 180, training loss: 1.6108410358428955 = 0.9611659646034241 + 0.1 * 6.496750831604004
Epoch 180, val loss: 1.1634373664855957
Epoch 190, training loss: 1.5470311641693115 = 0.8994566202163696 + 0.1 * 6.475745677947998
Epoch 190, val loss: 1.1214509010314941
Epoch 200, training loss: 1.4882853031158447 = 0.8419919610023499 + 0.1 * 6.4629340171813965
Epoch 200, val loss: 1.0823832750320435
Epoch 210, training loss: 1.4315438270568848 = 0.7868610620498657 + 0.1 * 6.446828365325928
Epoch 210, val loss: 1.0449970960617065
Epoch 220, training loss: 1.3759753704071045 = 0.7324725985527039 + 0.1 * 6.435027599334717
Epoch 220, val loss: 1.0084227323532104
Epoch 230, training loss: 1.32138991355896 = 0.6784560680389404 + 0.1 * 6.429337501525879
Epoch 230, val loss: 0.973075807094574
Epoch 240, training loss: 1.2671180963516235 = 0.6252140998840332 + 0.1 * 6.419039726257324
Epoch 240, val loss: 0.9399790167808533
Epoch 250, training loss: 1.214268684387207 = 0.5726005434989929 + 0.1 * 6.416680812835693
Epoch 250, val loss: 0.9093246459960938
Epoch 260, training loss: 1.1619632244110107 = 0.5213252305984497 + 0.1 * 6.406379699707031
Epoch 260, val loss: 0.8819674253463745
Epoch 270, training loss: 1.1111853122711182 = 0.47137534618377686 + 0.1 * 6.398099422454834
Epoch 270, val loss: 0.8576812148094177
Epoch 280, training loss: 1.0628774166107178 = 0.4237590730190277 + 0.1 * 6.391183853149414
Epoch 280, val loss: 0.8371750712394714
Epoch 290, training loss: 1.0190403461456299 = 0.3799727261066437 + 0.1 * 6.390676021575928
Epoch 290, val loss: 0.821641206741333
Epoch 300, training loss: 0.9788482785224915 = 0.34075427055358887 + 0.1 * 6.380939960479736
Epoch 300, val loss: 0.8111920952796936
Epoch 310, training loss: 0.9434177875518799 = 0.3058963119983673 + 0.1 * 6.375214576721191
Epoch 310, val loss: 0.8051081299781799
Epoch 320, training loss: 0.9122916460037231 = 0.27515909075737 + 0.1 * 6.3713250160217285
Epoch 320, val loss: 0.8026477694511414
Epoch 330, training loss: 0.8844904899597168 = 0.24800381064414978 + 0.1 * 6.364866256713867
Epoch 330, val loss: 0.8032175302505493
Epoch 340, training loss: 0.8596094846725464 = 0.22360605001449585 + 0.1 * 6.360034465789795
Epoch 340, val loss: 0.8055501580238342
Epoch 350, training loss: 0.8366909027099609 = 0.20135292410850525 + 0.1 * 6.353379726409912
Epoch 350, val loss: 0.8095155358314514
Epoch 360, training loss: 0.8155091404914856 = 0.18073271214962006 + 0.1 * 6.347764015197754
Epoch 360, val loss: 0.8146710991859436
Epoch 370, training loss: 0.7961015105247498 = 0.16174350678920746 + 0.1 * 6.3435797691345215
Epoch 370, val loss: 0.8209412693977356
Epoch 380, training loss: 0.7778444886207581 = 0.14449022710323334 + 0.1 * 6.333542346954346
Epoch 380, val loss: 0.8286049962043762
Epoch 390, training loss: 0.7620372772216797 = 0.1290767788887024 + 0.1 * 6.3296051025390625
Epoch 390, val loss: 0.837656557559967
Epoch 400, training loss: 0.7479638457298279 = 0.1155555322766304 + 0.1 * 6.32408332824707
Epoch 400, val loss: 0.8480840921401978
Epoch 410, training loss: 0.7379891276359558 = 0.10381816327571869 + 0.1 * 6.341709136962891
Epoch 410, val loss: 0.8593032956123352
Epoch 420, training loss: 0.7255520224571228 = 0.09375376999378204 + 0.1 * 6.3179826736450195
Epoch 420, val loss: 0.8718321919441223
Epoch 430, training loss: 0.7156367301940918 = 0.08504042029380798 + 0.1 * 6.305962562561035
Epoch 430, val loss: 0.8847624063491821
Epoch 440, training loss: 0.7072984576225281 = 0.07742990553379059 + 0.1 * 6.298685550689697
Epoch 440, val loss: 0.8985949754714966
Epoch 450, training loss: 0.7005710601806641 = 0.0707395151257515 + 0.1 * 6.298315525054932
Epoch 450, val loss: 0.912743866443634
Epoch 460, training loss: 0.6955055594444275 = 0.06485772132873535 + 0.1 * 6.306478023529053
Epoch 460, val loss: 0.9270451068878174
Epoch 470, training loss: 0.6885996460914612 = 0.05967913568019867 + 0.1 * 6.289205074310303
Epoch 470, val loss: 0.9415516257286072
Epoch 480, training loss: 0.6837916374206543 = 0.05506880208849907 + 0.1 * 6.287228107452393
Epoch 480, val loss: 0.9559317231178284
Epoch 490, training loss: 0.678460419178009 = 0.050953276455402374 + 0.1 * 6.275071144104004
Epoch 490, val loss: 0.9704502820968628
Epoch 500, training loss: 0.6741933822631836 = 0.04726222902536392 + 0.1 * 6.26931095123291
Epoch 500, val loss: 0.9846423268318176
Epoch 510, training loss: 0.6715527772903442 = 0.04394041746854782 + 0.1 * 6.276123523712158
Epoch 510, val loss: 0.9988144636154175
Epoch 520, training loss: 0.6682655215263367 = 0.04094908758997917 + 0.1 * 6.27316427230835
Epoch 520, val loss: 1.0128247737884521
Epoch 530, training loss: 0.6648635864257812 = 0.03825308009982109 + 0.1 * 6.2661051750183105
Epoch 530, val loss: 1.0262826681137085
Epoch 540, training loss: 0.660926103591919 = 0.0358150415122509 + 0.1 * 6.251110076904297
Epoch 540, val loss: 1.039940357208252
Epoch 550, training loss: 0.65944904088974 = 0.033592693507671356 + 0.1 * 6.25856351852417
Epoch 550, val loss: 1.0528604984283447
Epoch 560, training loss: 0.6565027236938477 = 0.03156858682632446 + 0.1 * 6.2493414878845215
Epoch 560, val loss: 1.0658981800079346
Epoch 570, training loss: 0.6543464660644531 = 0.029715554788708687 + 0.1 * 6.246309280395508
Epoch 570, val loss: 1.0785506963729858
Epoch 580, training loss: 0.6526471972465515 = 0.0280208308249712 + 0.1 * 6.24626350402832
Epoch 580, val loss: 1.0907056331634521
Epoch 590, training loss: 0.6503406763076782 = 0.026468932628631592 + 0.1 * 6.238717079162598
Epoch 590, val loss: 1.1031876802444458
Epoch 600, training loss: 0.6497309803962708 = 0.02503948099911213 + 0.1 * 6.246914863586426
Epoch 600, val loss: 1.1148511171340942
Epoch 610, training loss: 0.6467987298965454 = 0.023723453283309937 + 0.1 * 6.230752468109131
Epoch 610, val loss: 1.1265672445297241
Epoch 620, training loss: 0.6455653309822083 = 0.022507531568408012 + 0.1 * 6.23057746887207
Epoch 620, val loss: 1.1381406784057617
Epoch 630, training loss: 0.6434720754623413 = 0.02138170227408409 + 0.1 * 6.2209038734436035
Epoch 630, val loss: 1.1490195989608765
Epoch 640, training loss: 0.6421395540237427 = 0.020341571420431137 + 0.1 * 6.217979907989502
Epoch 640, val loss: 1.160150408744812
Epoch 650, training loss: 0.6422202587127686 = 0.019374212250113487 + 0.1 * 6.228460788726807
Epoch 650, val loss: 1.1706730127334595
Epoch 660, training loss: 0.6403664350509644 = 0.018476752564311028 + 0.1 * 6.218896865844727
Epoch 660, val loss: 1.1810766458511353
Epoch 670, training loss: 0.6395385265350342 = 0.01764131523668766 + 0.1 * 6.218972206115723
Epoch 670, val loss: 1.1915874481201172
Epoch 680, training loss: 0.637283980846405 = 0.01686214841902256 + 0.1 * 6.204217910766602
Epoch 680, val loss: 1.2011759281158447
Epoch 690, training loss: 0.6363446712493896 = 0.01613529771566391 + 0.1 * 6.202094078063965
Epoch 690, val loss: 1.2112993001937866
Epoch 700, training loss: 0.6371480822563171 = 0.015454266220331192 + 0.1 * 6.216938018798828
Epoch 700, val loss: 1.220833659172058
Epoch 710, training loss: 0.6350917816162109 = 0.014817253686487675 + 0.1 * 6.20274543762207
Epoch 710, val loss: 1.2300336360931396
Epoch 720, training loss: 0.6346557140350342 = 0.014221507124602795 + 0.1 * 6.204342365264893
Epoch 720, val loss: 1.2395117282867432
Epoch 730, training loss: 0.6332429051399231 = 0.01366291381418705 + 0.1 * 6.195799350738525
Epoch 730, val loss: 1.2483264207839966
Epoch 740, training loss: 0.6323221921920776 = 0.013137531466782093 + 0.1 * 6.19184684753418
Epoch 740, val loss: 1.2572590112686157
Epoch 750, training loss: 0.6316885948181152 = 0.012642496265470982 + 0.1 * 6.190460681915283
Epoch 750, val loss: 1.2657053470611572
Epoch 760, training loss: 0.6309309601783752 = 0.01217686664313078 + 0.1 * 6.1875410079956055
Epoch 760, val loss: 1.2740098237991333
Epoch 770, training loss: 0.6298956274986267 = 0.011738820001482964 + 0.1 * 6.181567668914795
Epoch 770, val loss: 1.2825864553451538
Epoch 780, training loss: 0.6295046210289001 = 0.011324573308229446 + 0.1 * 6.181800365447998
Epoch 780, val loss: 1.290539026260376
Epoch 790, training loss: 0.6295433640480042 = 0.010933052748441696 + 0.1 * 6.186103343963623
Epoch 790, val loss: 1.2985427379608154
Epoch 800, training loss: 0.6279309988021851 = 0.010563150979578495 + 0.1 * 6.173678398132324
Epoch 800, val loss: 1.3061991930007935
Epoch 810, training loss: 0.6289747953414917 = 0.010213227942585945 + 0.1 * 6.187615394592285
Epoch 810, val loss: 1.3139151334762573
Epoch 820, training loss: 0.6273727416992188 = 0.009881303645670414 + 0.1 * 6.1749138832092285
Epoch 820, val loss: 1.3210439682006836
Epoch 830, training loss: 0.6273585557937622 = 0.009567747823894024 + 0.1 * 6.177907943725586
Epoch 830, val loss: 1.328652262687683
Epoch 840, training loss: 0.6258082985877991 = 0.009269500151276588 + 0.1 * 6.165388107299805
Epoch 840, val loss: 1.3356152772903442
Epoch 850, training loss: 0.6255998015403748 = 0.008986476808786392 + 0.1 * 6.166133403778076
Epoch 850, val loss: 1.3428932428359985
Epoch 860, training loss: 0.6255457401275635 = 0.008716711774468422 + 0.1 * 6.168290138244629
Epoch 860, val loss: 1.3497984409332275
Epoch 870, training loss: 0.6246495842933655 = 0.008459432050585747 + 0.1 * 6.161901473999023
Epoch 870, val loss: 1.356500267982483
Epoch 880, training loss: 0.6252689957618713 = 0.00821452122181654 + 0.1 * 6.170544624328613
Epoch 880, val loss: 1.3628791570663452
Epoch 890, training loss: 0.6239590048789978 = 0.007981088943779469 + 0.1 * 6.1597795486450195
Epoch 890, val loss: 1.3693619966506958
Epoch 900, training loss: 0.6228458881378174 = 0.00775891961529851 + 0.1 * 6.150869846343994
Epoch 900, val loss: 1.376006007194519
Epoch 910, training loss: 0.6248528361320496 = 0.007545982021838427 + 0.1 * 6.173068046569824
Epoch 910, val loss: 1.3822119235992432
Epoch 920, training loss: 0.6224855780601501 = 0.007342071272432804 + 0.1 * 6.151434421539307
Epoch 920, val loss: 1.3879660367965698
Epoch 930, training loss: 0.6229690909385681 = 0.0071480548940598965 + 0.1 * 6.158210277557373
Epoch 930, val loss: 1.394387125968933
Epoch 940, training loss: 0.6222087144851685 = 0.006962059997022152 + 0.1 * 6.152466297149658
Epoch 940, val loss: 1.4001232385635376
Epoch 950, training loss: 0.6211754083633423 = 0.006784132681787014 + 0.1 * 6.1439127922058105
Epoch 950, val loss: 1.4061301946640015
Epoch 960, training loss: 0.6219222545623779 = 0.006613546516746283 + 0.1 * 6.1530866622924805
Epoch 960, val loss: 1.4119552373886108
Epoch 970, training loss: 0.6206300258636475 = 0.006449618376791477 + 0.1 * 6.141803741455078
Epoch 970, val loss: 1.4174026250839233
Epoch 980, training loss: 0.6203821301460266 = 0.006292583420872688 + 0.1 * 6.140895366668701
Epoch 980, val loss: 1.4229376316070557
Epoch 990, training loss: 0.6202861666679382 = 0.00614192383363843 + 0.1 * 6.14144229888916
Epoch 990, val loss: 1.428452968597412
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.797375440597534 = 1.959994912147522 + 0.1 * 8.373805046081543
Epoch 0, val loss: 1.9586585760116577
Epoch 10, training loss: 2.7860636711120605 = 1.9487087726593018 + 0.1 * 8.373547554016113
Epoch 10, val loss: 1.9474403858184814
Epoch 20, training loss: 2.7719364166259766 = 1.9347363710403442 + 0.1 * 8.372001647949219
Epoch 20, val loss: 1.9336414337158203
Epoch 30, training loss: 2.7509565353393555 = 1.9148778915405273 + 0.1 * 8.360786437988281
Epoch 30, val loss: 1.9140952825546265
Epoch 40, training loss: 2.7141971588134766 = 1.885676622390747 + 0.1 * 8.28520393371582
Epoch 40, val loss: 1.8858548402786255
Epoch 50, training loss: 2.6277880668640137 = 1.8478553295135498 + 0.1 * 7.799326419830322
Epoch 50, val loss: 1.8510602712631226
Epoch 60, training loss: 2.5457229614257812 = 1.8107502460479736 + 0.1 * 7.349726676940918
Epoch 60, val loss: 1.8195942640304565
Epoch 70, training loss: 2.4710195064544678 = 1.7756462097167969 + 0.1 * 6.953732967376709
Epoch 70, val loss: 1.7918462753295898
Epoch 80, training loss: 2.4181735515594482 = 1.7397987842559814 + 0.1 * 6.783748149871826
Epoch 80, val loss: 1.762755274772644
Epoch 90, training loss: 2.36590576171875 = 1.695283055305481 + 0.1 * 6.7062273025512695
Epoch 90, val loss: 1.7237576246261597
Epoch 100, training loss: 2.302882671356201 = 1.6365724802017212 + 0.1 * 6.663103103637695
Epoch 100, val loss: 1.673080563545227
Epoch 110, training loss: 2.2238614559173584 = 1.5601922273635864 + 0.1 * 6.636692523956299
Epoch 110, val loss: 1.6096092462539673
Epoch 120, training loss: 2.1306040287017822 = 1.4684641361236572 + 0.1 * 6.621399402618408
Epoch 120, val loss: 1.5355041027069092
Epoch 130, training loss: 2.028960704803467 = 1.3680167198181152 + 0.1 * 6.609440326690674
Epoch 130, val loss: 1.4549157619476318
Epoch 140, training loss: 1.9255313873291016 = 1.2658334970474243 + 0.1 * 6.59697961807251
Epoch 140, val loss: 1.37382972240448
Epoch 150, training loss: 1.823311448097229 = 1.165146827697754 + 0.1 * 6.581645965576172
Epoch 150, val loss: 1.2943097352981567
Epoch 160, training loss: 1.726789951324463 = 1.0695645809173584 + 0.1 * 6.572253704071045
Epoch 160, val loss: 1.2192825078964233
Epoch 170, training loss: 1.637345314025879 = 0.9815555214881897 + 0.1 * 6.557897567749023
Epoch 170, val loss: 1.150795817375183
Epoch 180, training loss: 1.5552654266357422 = 0.9008346796035767 + 0.1 * 6.544306755065918
Epoch 180, val loss: 1.088496446609497
Epoch 190, training loss: 1.4801303148269653 = 0.8264328837394714 + 0.1 * 6.53697395324707
Epoch 190, val loss: 1.0314102172851562
Epoch 200, training loss: 1.4097272157669067 = 0.7571460604667664 + 0.1 * 6.525811672210693
Epoch 200, val loss: 0.9786921739578247
Epoch 210, training loss: 1.3440577983856201 = 0.6918168067932129 + 0.1 * 6.5224103927612305
Epoch 210, val loss: 0.9291288256645203
Epoch 220, training loss: 1.282451868057251 = 0.6312419176101685 + 0.1 * 6.512098789215088
Epoch 220, val loss: 0.8844213485717773
Epoch 230, training loss: 1.2256536483764648 = 0.575221598148346 + 0.1 * 6.504319667816162
Epoch 230, val loss: 0.8443356156349182
Epoch 240, training loss: 1.1733288764953613 = 0.5237572193145752 + 0.1 * 6.4957170486450195
Epoch 240, val loss: 0.80954509973526
Epoch 250, training loss: 1.1252543926239014 = 0.4765360653400421 + 0.1 * 6.4871826171875
Epoch 250, val loss: 0.7804222106933594
Epoch 260, training loss: 1.0820627212524414 = 0.43297576904296875 + 0.1 * 6.49086856842041
Epoch 260, val loss: 0.7566779255867004
Epoch 270, training loss: 1.040451169013977 = 0.39315465092658997 + 0.1 * 6.472964763641357
Epoch 270, val loss: 0.7380689382553101
Epoch 280, training loss: 1.0027518272399902 = 0.3564552366733551 + 0.1 * 6.462965488433838
Epoch 280, val loss: 0.7235293984413147
Epoch 290, training loss: 0.9702962636947632 = 0.3227492868900299 + 0.1 * 6.475469589233398
Epoch 290, val loss: 0.7126836180686951
Epoch 300, training loss: 0.9372909069061279 = 0.29226312041282654 + 0.1 * 6.450277805328369
Epoch 300, val loss: 0.7054357528686523
Epoch 310, training loss: 0.9089197516441345 = 0.2644740343093872 + 0.1 * 6.444457054138184
Epoch 310, val loss: 0.7007887363433838
Epoch 320, training loss: 0.8833452463150024 = 0.23910930752754211 + 0.1 * 6.442359447479248
Epoch 320, val loss: 0.6985810399055481
Epoch 330, training loss: 0.8592177629470825 = 0.21607644855976105 + 0.1 * 6.431413173675537
Epoch 330, val loss: 0.6980423331260681
Epoch 340, training loss: 0.8376139402389526 = 0.19510327279567719 + 0.1 * 6.425106525421143
Epoch 340, val loss: 0.6990693807601929
Epoch 350, training loss: 0.8180298209190369 = 0.17603258788585663 + 0.1 * 6.419971942901611
Epoch 350, val loss: 0.7015231847763062
Epoch 360, training loss: 0.8001502752304077 = 0.1587844341993332 + 0.1 * 6.413658142089844
Epoch 360, val loss: 0.7053543925285339
Epoch 370, training loss: 0.7845994234085083 = 0.1431598663330078 + 0.1 * 6.414395332336426
Epoch 370, val loss: 0.7101308107376099
Epoch 380, training loss: 0.7698498368263245 = 0.12913508713245392 + 0.1 * 6.407147407531738
Epoch 380, val loss: 0.7157624959945679
Epoch 390, training loss: 0.7562315464019775 = 0.1165822446346283 + 0.1 * 6.3964924812316895
Epoch 390, val loss: 0.7220817804336548
Epoch 400, training loss: 0.7443204522132874 = 0.10535614937543869 + 0.1 * 6.389642715454102
Epoch 400, val loss: 0.7289707064628601
Epoch 410, training loss: 0.7347554564476013 = 0.09538926929235458 + 0.1 * 6.393661975860596
Epoch 410, val loss: 0.736176073551178
Epoch 420, training loss: 0.7243033647537231 = 0.08657548576593399 + 0.1 * 6.3772783279418945
Epoch 420, val loss: 0.743554413318634
Epoch 430, training loss: 0.715786874294281 = 0.07875566929578781 + 0.1 * 6.370311737060547
Epoch 430, val loss: 0.7510409355163574
Epoch 440, training loss: 0.7105799913406372 = 0.07181546837091446 + 0.1 * 6.3876447677612305
Epoch 440, val loss: 0.7585946917533875
Epoch 450, training loss: 0.7013764977455139 = 0.06568986177444458 + 0.1 * 6.356866359710693
Epoch 450, val loss: 0.7661675214767456
Epoch 460, training loss: 0.695952296257019 = 0.06025990471243858 + 0.1 * 6.356924057006836
Epoch 460, val loss: 0.773559033870697
Epoch 470, training loss: 0.6899307370185852 = 0.05543508380651474 + 0.1 * 6.344956398010254
Epoch 470, val loss: 0.7810648679733276
Epoch 480, training loss: 0.6883060932159424 = 0.051128312945365906 + 0.1 * 6.371777534484863
Epoch 480, val loss: 0.7884374260902405
Epoch 490, training loss: 0.6816659569740295 = 0.047295890748500824 + 0.1 * 6.343700885772705
Epoch 490, val loss: 0.7956347465515137
Epoch 500, training loss: 0.6767585277557373 = 0.04386630281805992 + 0.1 * 6.328922271728516
Epoch 500, val loss: 0.8027766346931458
Epoch 510, training loss: 0.6739735007286072 = 0.04077698662877083 + 0.1 * 6.331965446472168
Epoch 510, val loss: 0.8097516894340515
Epoch 520, training loss: 0.6708104610443115 = 0.03799667954444885 + 0.1 * 6.328137397766113
Epoch 520, val loss: 0.8166561722755432
Epoch 530, training loss: 0.6665284633636475 = 0.03549002483487129 + 0.1 * 6.310384273529053
Epoch 530, val loss: 0.8234977722167969
Epoch 540, training loss: 0.6659122109413147 = 0.03321526572108269 + 0.1 * 6.326969146728516
Epoch 540, val loss: 0.8301693797111511
Epoch 550, training loss: 0.6610966920852661 = 0.031151020899415016 + 0.1 * 6.29945707321167
Epoch 550, val loss: 0.8368028998374939
Epoch 560, training loss: 0.658724844455719 = 0.029269838705658913 + 0.1 * 6.29455041885376
Epoch 560, val loss: 0.8433806300163269
Epoch 570, training loss: 0.6571203470230103 = 0.027548138052225113 + 0.1 * 6.295722007751465
Epoch 570, val loss: 0.8497401475906372
Epoch 580, training loss: 0.6545168161392212 = 0.025972407311201096 + 0.1 * 6.2854437828063965
Epoch 580, val loss: 0.8561930656433105
Epoch 590, training loss: 0.6548445820808411 = 0.024525459855794907 + 0.1 * 6.303191184997559
Epoch 590, val loss: 0.8622668981552124
Epoch 600, training loss: 0.6510971188545227 = 0.023204313591122627 + 0.1 * 6.278927803039551
Epoch 600, val loss: 0.8684489130973816
Epoch 610, training loss: 0.6491696834564209 = 0.02199125476181507 + 0.1 * 6.27178430557251
Epoch 610, val loss: 0.874525785446167
Epoch 620, training loss: 0.6471741795539856 = 0.020867586135864258 + 0.1 * 6.263065814971924
Epoch 620, val loss: 0.8804003596305847
Epoch 630, training loss: 0.6473414897918701 = 0.019827064126729965 + 0.1 * 6.275144100189209
Epoch 630, val loss: 0.886311948299408
Epoch 640, training loss: 0.644503116607666 = 0.0188649483025074 + 0.1 * 6.256381988525391
Epoch 640, val loss: 0.89215487241745
Epoch 650, training loss: 0.6450774073600769 = 0.01797286793589592 + 0.1 * 6.271045207977295
Epoch 650, val loss: 0.8978805541992188
Epoch 660, training loss: 0.6432069540023804 = 0.01714332029223442 + 0.1 * 6.260636329650879
Epoch 660, val loss: 0.9034243226051331
Epoch 670, training loss: 0.6412139534950256 = 0.016372671350836754 + 0.1 * 6.248412609100342
Epoch 670, val loss: 0.9090501070022583
Epoch 680, training loss: 0.6411826014518738 = 0.015652678906917572 + 0.1 * 6.2552995681762695
Epoch 680, val loss: 0.9145077466964722
Epoch 690, training loss: 0.6397911906242371 = 0.01498036552220583 + 0.1 * 6.24810791015625
Epoch 690, val loss: 0.9196606874465942
Epoch 700, training loss: 0.6377626061439514 = 0.014354832470417023 + 0.1 * 6.234077453613281
Epoch 700, val loss: 0.925050675868988
Epoch 710, training loss: 0.6381467580795288 = 0.013768084347248077 + 0.1 * 6.243786811828613
Epoch 710, val loss: 0.9302022457122803
Epoch 720, training loss: 0.6371228098869324 = 0.013218226842582226 + 0.1 * 6.2390456199646
Epoch 720, val loss: 0.9351673722267151
Epoch 730, training loss: 0.6358476281166077 = 0.0127035453915596 + 0.1 * 6.231441020965576
Epoch 730, val loss: 0.9400647282600403
Epoch 740, training loss: 0.634904146194458 = 0.012220931239426136 + 0.1 * 6.226832389831543
Epoch 740, val loss: 0.9449815154075623
Epoch 750, training loss: 0.6342061758041382 = 0.011766667477786541 + 0.1 * 6.224394798278809
Epoch 750, val loss: 0.9498628973960876
Epoch 760, training loss: 0.6343430280685425 = 0.011337063275277615 + 0.1 * 6.230059623718262
Epoch 760, val loss: 0.9544709324836731
Epoch 770, training loss: 0.6333124041557312 = 0.010932561941444874 + 0.1 * 6.2237982749938965
Epoch 770, val loss: 0.9589681029319763
Epoch 780, training loss: 0.6346091628074646 = 0.010552451014518738 + 0.1 * 6.240566730499268
Epoch 780, val loss: 0.9635327458381653
Epoch 790, training loss: 0.6315459609031677 = 0.01019358728080988 + 0.1 * 6.213523864746094
Epoch 790, val loss: 0.9680105447769165
Epoch 800, training loss: 0.6308221817016602 = 0.009854696691036224 + 0.1 * 6.209674835205078
Epoch 800, val loss: 0.9724509716033936
Epoch 810, training loss: 0.6313713192939758 = 0.00953204557299614 + 0.1 * 6.218392848968506
Epoch 810, val loss: 0.9766425490379333
Epoch 820, training loss: 0.629935622215271 = 0.009226709604263306 + 0.1 * 6.207088947296143
Epoch 820, val loss: 0.9808590412139893
Epoch 830, training loss: 0.629197895526886 = 0.008937660604715347 + 0.1 * 6.202602386474609
Epoch 830, val loss: 0.9851588606834412
Epoch 840, training loss: 0.629429817199707 = 0.00866217166185379 + 0.1 * 6.207676410675049
Epoch 840, val loss: 0.9892493486404419
Epoch 850, training loss: 0.6295743584632874 = 0.008399661630392075 + 0.1 * 6.211746692657471
Epoch 850, val loss: 0.993156373500824
Epoch 860, training loss: 0.6284573078155518 = 0.008151335641741753 + 0.1 * 6.203059196472168
Epoch 860, val loss: 0.9971709251403809
Epoch 870, training loss: 0.6281259059906006 = 0.00791572779417038 + 0.1 * 6.202101707458496
Epoch 870, val loss: 1.0011707544326782
Epoch 880, training loss: 0.626327633857727 = 0.0076905665919184685 + 0.1 * 6.186370849609375
Epoch 880, val loss: 1.0049704313278198
Epoch 890, training loss: 0.6266798973083496 = 0.00747637078166008 + 0.1 * 6.19203519821167
Epoch 890, val loss: 1.0088164806365967
Epoch 900, training loss: 0.6259002089500427 = 0.007270998787134886 + 0.1 * 6.1862921714782715
Epoch 900, val loss: 1.0125447511672974
Epoch 910, training loss: 0.6266579627990723 = 0.007074207067489624 + 0.1 * 6.195837020874023
Epoch 910, val loss: 1.0159742832183838
Epoch 920, training loss: 0.6251738667488098 = 0.006887332070618868 + 0.1 * 6.182865142822266
Epoch 920, val loss: 1.0196583271026611
Epoch 930, training loss: 0.6259679794311523 = 0.006709309294819832 + 0.1 * 6.192586421966553
Epoch 930, val loss: 1.0233519077301025
Epoch 940, training loss: 0.6250972747802734 = 0.006538265850394964 + 0.1 * 6.185589790344238
Epoch 940, val loss: 1.0267279148101807
Epoch 950, training loss: 0.6245742440223694 = 0.006374476011842489 + 0.1 * 6.181997776031494
Epoch 950, val loss: 1.0300710201263428
Epoch 960, training loss: 0.6241106986999512 = 0.006218209397047758 + 0.1 * 6.178925037384033
Epoch 960, val loss: 1.0335044860839844
Epoch 970, training loss: 0.6250338554382324 = 0.006067706737667322 + 0.1 * 6.189661502838135
Epoch 970, val loss: 1.03679358959198
Epoch 980, training loss: 0.6232988238334656 = 0.005923361051827669 + 0.1 * 6.1737542152404785
Epoch 980, val loss: 1.0399725437164307
Epoch 990, training loss: 0.622876763343811 = 0.005785675719380379 + 0.1 * 6.170910358428955
Epoch 990, val loss: 1.0433218479156494
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.786227226257324 = 1.9488362073898315 + 0.1 * 8.373910903930664
Epoch 0, val loss: 1.9436711072921753
Epoch 10, training loss: 2.77647066116333 = 1.9390904903411865 + 0.1 * 8.373800277709961
Epoch 10, val loss: 1.9341564178466797
Epoch 20, training loss: 2.764423370361328 = 1.9271107912063599 + 0.1 * 8.373126983642578
Epoch 20, val loss: 1.9225735664367676
Epoch 30, training loss: 2.7469241619110107 = 1.91023850440979 + 0.1 * 8.36685562133789
Epoch 30, val loss: 1.9066270589828491
Epoch 40, training loss: 2.717092752456665 = 1.8850133419036865 + 0.1 * 8.320793151855469
Epoch 40, val loss: 1.8834103345870972
Epoch 50, training loss: 2.6549532413482666 = 1.850196361541748 + 0.1 * 8.047568321228027
Epoch 50, val loss: 1.8529584407806396
Epoch 60, training loss: 2.587930917739868 = 1.809340000152588 + 0.1 * 7.785909652709961
Epoch 60, val loss: 1.8189226388931274
Epoch 70, training loss: 2.513705253601074 = 1.767846703529358 + 0.1 * 7.4585862159729
Epoch 70, val loss: 1.7853543758392334
Epoch 80, training loss: 2.4347269535064697 = 1.725567102432251 + 0.1 * 7.0915985107421875
Epoch 80, val loss: 1.75102961063385
Epoch 90, training loss: 2.3625223636627197 = 1.6744327545166016 + 0.1 * 6.88089656829834
Epoch 90, val loss: 1.7074593305587769
Epoch 100, training loss: 2.2883806228637695 = 1.6078280210494995 + 0.1 * 6.805526256561279
Epoch 100, val loss: 1.6504257917404175
Epoch 110, training loss: 2.20300555229187 = 1.5267342329025269 + 0.1 * 6.762713432312012
Epoch 110, val loss: 1.5827312469482422
Epoch 120, training loss: 2.11049485206604 = 1.4377284049987793 + 0.1 * 6.727663516998291
Epoch 120, val loss: 1.5099399089813232
Epoch 130, training loss: 2.014965772628784 = 1.3451817035675049 + 0.1 * 6.697839736938477
Epoch 130, val loss: 1.4360471963882446
Epoch 140, training loss: 1.9171125888824463 = 1.2500698566436768 + 0.1 * 6.670426368713379
Epoch 140, val loss: 1.3599416017532349
Epoch 150, training loss: 1.8168749809265137 = 1.1517493724822998 + 0.1 * 6.651256561279297
Epoch 150, val loss: 1.2816194295883179
Epoch 160, training loss: 1.7169973850250244 = 1.053959846496582 + 0.1 * 6.630375385284424
Epoch 160, val loss: 1.2044416666030884
Epoch 170, training loss: 1.621506690979004 = 0.959662675857544 + 0.1 * 6.618439674377441
Epoch 170, val loss: 1.1308610439300537
Epoch 180, training loss: 1.532894253730774 = 0.8721407055854797 + 0.1 * 6.607535362243652
Epoch 180, val loss: 1.0639307498931885
Epoch 190, training loss: 1.4528329372406006 = 0.7931240200996399 + 0.1 * 6.59708833694458
Epoch 190, val loss: 1.00454580783844
Epoch 200, training loss: 1.3801767826080322 = 0.7216780185699463 + 0.1 * 6.584986686706543
Epoch 200, val loss: 0.9509270787239075
Epoch 210, training loss: 1.3136825561523438 = 0.6562193632125854 + 0.1 * 6.574631214141846
Epoch 210, val loss: 0.9025481939315796
Epoch 220, training loss: 1.2533469200134277 = 0.5962468385696411 + 0.1 * 6.571001052856445
Epoch 220, val loss: 0.8588705658912659
Epoch 230, training loss: 1.198045015335083 = 0.5422140955924988 + 0.1 * 6.558309555053711
Epoch 230, val loss: 0.8215088248252869
Epoch 240, training loss: 1.1466774940490723 = 0.4925287365913391 + 0.1 * 6.541486740112305
Epoch 240, val loss: 0.7893322706222534
Epoch 250, training loss: 1.0996730327606201 = 0.44666194915771484 + 0.1 * 6.5301103591918945
Epoch 250, val loss: 0.7624120712280273
Epoch 260, training loss: 1.0565736293792725 = 0.4044185280799866 + 0.1 * 6.521551132202148
Epoch 260, val loss: 0.7406518459320068
Epoch 270, training loss: 1.0173938274383545 = 0.3659117817878723 + 0.1 * 6.514820098876953
Epoch 270, val loss: 0.7233509421348572
Epoch 280, training loss: 0.9805623888969421 = 0.33040881156921387 + 0.1 * 6.501535415649414
Epoch 280, val loss: 0.7092583775520325
Epoch 290, training loss: 0.9504615068435669 = 0.29753270745277405 + 0.1 * 6.529287338256836
Epoch 290, val loss: 0.6980074048042297
Epoch 300, training loss: 0.9166158437728882 = 0.26751527190208435 + 0.1 * 6.491005897521973
Epoch 300, val loss: 0.6897203326225281
Epoch 310, training loss: 0.8875536322593689 = 0.23984843492507935 + 0.1 * 6.477051734924316
Epoch 310, val loss: 0.6837316155433655
Epoch 320, training loss: 0.8611842393875122 = 0.2143336832523346 + 0.1 * 6.468504905700684
Epoch 320, val loss: 0.6802712678909302
Epoch 330, training loss: 0.8377525210380554 = 0.19108782708644867 + 0.1 * 6.466646671295166
Epoch 330, val loss: 0.6790936589241028
Epoch 340, training loss: 0.8155856132507324 = 0.1703532189130783 + 0.1 * 6.4523234367370605
Epoch 340, val loss: 0.6802829504013062
Epoch 350, training loss: 0.7964945435523987 = 0.15185461938381195 + 0.1 * 6.446399211883545
Epoch 350, val loss: 0.6833469867706299
Epoch 360, training loss: 0.7795803546905518 = 0.1354859471321106 + 0.1 * 6.440944194793701
Epoch 360, val loss: 0.6882454752922058
Epoch 370, training loss: 0.7641372680664062 = 0.12113386392593384 + 0.1 * 6.430034160614014
Epoch 370, val loss: 0.6948308348655701
Epoch 380, training loss: 0.7514606714248657 = 0.10852121561765671 + 0.1 * 6.429394245147705
Epoch 380, val loss: 0.7024977803230286
Epoch 390, training loss: 0.7390567064285278 = 0.09751269221305847 + 0.1 * 6.415439605712891
Epoch 390, val loss: 0.7109708786010742
Epoch 400, training loss: 0.7299660444259644 = 0.08788863569498062 + 0.1 * 6.420773506164551
Epoch 400, val loss: 0.7199850678443909
Epoch 410, training loss: 0.720076322555542 = 0.07949163764715195 + 0.1 * 6.405847072601318
Epoch 410, val loss: 0.72922682762146
Epoch 420, training loss: 0.7118071913719177 = 0.0721188336610794 + 0.1 * 6.396883487701416
Epoch 420, val loss: 0.7385926246643066
Epoch 430, training loss: 0.7055697441101074 = 0.06563147902488708 + 0.1 * 6.399382591247559
Epoch 430, val loss: 0.7478609681129456
Epoch 440, training loss: 0.698796272277832 = 0.05993063375353813 + 0.1 * 6.3886566162109375
Epoch 440, val loss: 0.7571180462837219
Epoch 450, training loss: 0.6931357979774475 = 0.05488194152712822 + 0.1 * 6.382538795471191
Epoch 450, val loss: 0.766191303730011
Epoch 460, training loss: 0.6902271509170532 = 0.050395164638757706 + 0.1 * 6.398319721221924
Epoch 460, val loss: 0.7752347588539124
Epoch 470, training loss: 0.6841920018196106 = 0.04640982300043106 + 0.1 * 6.377821445465088
Epoch 470, val loss: 0.7840809226036072
Epoch 480, training loss: 0.6804224252700806 = 0.04285019636154175 + 0.1 * 6.3757219314575195
Epoch 480, val loss: 0.7928359508514404
Epoch 490, training loss: 0.677172064781189 = 0.03967226669192314 + 0.1 * 6.374998092651367
Epoch 490, val loss: 0.8012721538543701
Epoch 500, training loss: 0.6728404760360718 = 0.03682418912649155 + 0.1 * 6.360162734985352
Epoch 500, val loss: 0.8096935749053955
Epoch 510, training loss: 0.6699005365371704 = 0.03425593301653862 + 0.1 * 6.356445789337158
Epoch 510, val loss: 0.817844808101654
Epoch 520, training loss: 0.6675897836685181 = 0.03193807601928711 + 0.1 * 6.3565168380737305
Epoch 520, val loss: 0.8258517384529114
Epoch 530, training loss: 0.6669191122055054 = 0.02984643168747425 + 0.1 * 6.370726585388184
Epoch 530, val loss: 0.8336684107780457
Epoch 540, training loss: 0.6629627346992493 = 0.027958527207374573 + 0.1 * 6.35004186630249
Epoch 540, val loss: 0.8412546515464783
Epoch 550, training loss: 0.6603804230690002 = 0.026243308559060097 + 0.1 * 6.341371059417725
Epoch 550, val loss: 0.848743200302124
Epoch 560, training loss: 0.6584958434104919 = 0.02467586100101471 + 0.1 * 6.338199615478516
Epoch 560, val loss: 0.8560649752616882
Epoch 570, training loss: 0.6579124331474304 = 0.023248011246323586 + 0.1 * 6.346643924713135
Epoch 570, val loss: 0.8630046844482422
Epoch 580, training loss: 0.6555803418159485 = 0.021948931738734245 + 0.1 * 6.3363142013549805
Epoch 580, val loss: 0.8700965642929077
Epoch 590, training loss: 0.6534186601638794 = 0.020755723118782043 + 0.1 * 6.326629161834717
Epoch 590, val loss: 0.8768891096115112
Epoch 600, training loss: 0.6519542932510376 = 0.01966029591858387 + 0.1 * 6.322939872741699
Epoch 600, val loss: 0.8834125995635986
Epoch 610, training loss: 0.6509445309638977 = 0.018653549253940582 + 0.1 * 6.322909355163574
Epoch 610, val loss: 0.8900197148323059
Epoch 620, training loss: 0.6507697105407715 = 0.01772458292543888 + 0.1 * 6.330451011657715
Epoch 620, val loss: 0.8962857723236084
Epoch 630, training loss: 0.6483578681945801 = 0.016866568475961685 + 0.1 * 6.314912796020508
Epoch 630, val loss: 0.9025431871414185
Epoch 640, training loss: 0.6488659381866455 = 0.016070634126663208 + 0.1 * 6.327952861785889
Epoch 640, val loss: 0.9086763858795166
Epoch 650, training loss: 0.6462875604629517 = 0.015332401730120182 + 0.1 * 6.30955171585083
Epoch 650, val loss: 0.914516806602478
Epoch 660, training loss: 0.6447076201438904 = 0.014645585790276527 + 0.1 * 6.3006205558776855
Epoch 660, val loss: 0.9204579591751099
Epoch 670, training loss: 0.6452330350875854 = 0.014005856588482857 + 0.1 * 6.312271595001221
Epoch 670, val loss: 0.9259858727455139
Epoch 680, training loss: 0.6434009075164795 = 0.013412847183644772 + 0.1 * 6.299880504608154
Epoch 680, val loss: 0.9316362738609314
Epoch 690, training loss: 0.6425309181213379 = 0.012858717702329159 + 0.1 * 6.296722412109375
Epoch 690, val loss: 0.9371328353881836
Epoch 700, training loss: 0.6411799788475037 = 0.012339983135461807 + 0.1 * 6.288399696350098
Epoch 700, val loss: 0.942370593547821
Epoch 710, training loss: 0.6421726942062378 = 0.011855094693601131 + 0.1 * 6.303176403045654
Epoch 710, val loss: 0.9476242065429688
Epoch 720, training loss: 0.6403648853302002 = 0.011401716619729996 + 0.1 * 6.2896318435668945
Epoch 720, val loss: 0.9527364373207092
Epoch 730, training loss: 0.6385848522186279 = 0.010975128971040249 + 0.1 * 6.276097297668457
Epoch 730, val loss: 0.957811713218689
Epoch 740, training loss: 0.6381016373634338 = 0.010573876090347767 + 0.1 * 6.275277614593506
Epoch 740, val loss: 0.9626457095146179
Epoch 750, training loss: 0.6380707621574402 = 0.010197141207754612 + 0.1 * 6.278735637664795
Epoch 750, val loss: 0.9675022959709167
Epoch 760, training loss: 0.640116274356842 = 0.009841332212090492 + 0.1 * 6.3027496337890625
Epoch 760, val loss: 0.9722180366516113
Epoch 770, training loss: 0.6366710066795349 = 0.009506872855126858 + 0.1 * 6.271641254425049
Epoch 770, val loss: 0.976854681968689
Epoch 780, training loss: 0.636307418346405 = 0.009190069511532784 + 0.1 * 6.271173477172852
Epoch 780, val loss: 0.9815406203269958
Epoch 790, training loss: 0.6366103291511536 = 0.008889934979379177 + 0.1 * 6.2772040367126465
Epoch 790, val loss: 0.9859880208969116
Epoch 800, training loss: 0.6350444555282593 = 0.00860607996582985 + 0.1 * 6.264383792877197
Epoch 800, val loss: 0.9903628826141357
Epoch 810, training loss: 0.6347728967666626 = 0.008336752653121948 + 0.1 * 6.264361381530762
Epoch 810, val loss: 0.994748592376709
Epoch 820, training loss: 0.6341332793235779 = 0.008081887848675251 + 0.1 * 6.260513782501221
Epoch 820, val loss: 0.9989513158798218
Epoch 830, training loss: 0.6332284212112427 = 0.007839621976017952 + 0.1 * 6.25388765335083
Epoch 830, val loss: 1.0031183958053589
Epoch 840, training loss: 0.6318784952163696 = 0.007609485182911158 + 0.1 * 6.242690086364746
Epoch 840, val loss: 1.0072461366653442
Epoch 850, training loss: 0.6330289840698242 = 0.007390278857201338 + 0.1 * 6.256387233734131
Epoch 850, val loss: 1.0113862752914429
Epoch 860, training loss: 0.6329196691513062 = 0.007181019056588411 + 0.1 * 6.257386207580566
Epoch 860, val loss: 1.0152279138565063
Epoch 870, training loss: 0.631220817565918 = 0.0069826398976147175 + 0.1 * 6.242382049560547
Epoch 870, val loss: 1.019102931022644
Epoch 880, training loss: 0.6312234401702881 = 0.006793628912419081 + 0.1 * 6.244297981262207
Epoch 880, val loss: 1.0230709314346313
Epoch 890, training loss: 0.6304202079772949 = 0.0066125113517045975 + 0.1 * 6.238076686859131
Epoch 890, val loss: 1.0267977714538574
Epoch 900, training loss: 0.631212055683136 = 0.00644028652459383 + 0.1 * 6.24771785736084
Epoch 900, val loss: 1.0305845737457275
Epoch 910, training loss: 0.6290507316589355 = 0.0062748948112130165 + 0.1 * 6.227758407592773
Epoch 910, val loss: 1.034092903137207
Epoch 920, training loss: 0.627997875213623 = 0.006117620971053839 + 0.1 * 6.218802452087402
Epoch 920, val loss: 1.0378029346466064
Epoch 930, training loss: 0.6297712922096252 = 0.005966407246887684 + 0.1 * 6.238048553466797
Epoch 930, val loss: 1.041405439376831
Epoch 940, training loss: 0.6275296211242676 = 0.00582154979929328 + 0.1 * 6.217080593109131
Epoch 940, val loss: 1.044866919517517
Epoch 950, training loss: 0.6275268793106079 = 0.005682885646820068 + 0.1 * 6.218440055847168
Epoch 950, val loss: 1.048409342765808
Epoch 960, training loss: 0.6308844089508057 = 0.0055490280501544476 + 0.1 * 6.253353595733643
Epoch 960, val loss: 1.0518784523010254
Epoch 970, training loss: 0.6284896731376648 = 0.005420951638370752 + 0.1 * 6.230687618255615
Epoch 970, val loss: 1.0550968647003174
Epoch 980, training loss: 0.6271165609359741 = 0.005298160482198 + 0.1 * 6.218183994293213
Epoch 980, val loss: 1.058523416519165
Epoch 990, training loss: 0.6270846724510193 = 0.005179747007787228 + 0.1 * 6.219048976898193
Epoch 990, val loss: 1.061875343322754
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6052
Flip ASR: 0.5333/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.797600269317627 = 1.9602160453796387 + 0.1 * 8.373842239379883
Epoch 0, val loss: 1.9524140357971191
Epoch 10, training loss: 2.7857351303100586 = 1.9483917951583862 + 0.1 * 8.373433113098145
Epoch 10, val loss: 1.93972909450531
Epoch 20, training loss: 2.7705929279327393 = 1.9334540367126465 + 0.1 * 8.371389389038086
Epoch 20, val loss: 1.9226346015930176
Epoch 30, training loss: 2.7483654022216797 = 1.9123367071151733 + 0.1 * 8.360285758972168
Epoch 30, val loss: 1.8975423574447632
Epoch 40, training loss: 2.7114176750183105 = 1.8823689222335815 + 0.1 * 8.290488243103027
Epoch 40, val loss: 1.8627108335494995
Epoch 50, training loss: 2.629847288131714 = 1.8445686101913452 + 0.1 * 7.852786064147949
Epoch 50, val loss: 1.822218656539917
Epoch 60, training loss: 2.554792881011963 = 1.8044326305389404 + 0.1 * 7.503602981567383
Epoch 60, val loss: 1.7833654880523682
Epoch 70, training loss: 2.4849889278411865 = 1.7631003856658936 + 0.1 * 7.218885898590088
Epoch 70, val loss: 1.7467050552368164
Epoch 80, training loss: 2.422421455383301 = 1.7204701900482178 + 0.1 * 7.019513130187988
Epoch 80, val loss: 1.7125146389007568
Epoch 90, training loss: 2.3600339889526367 = 1.669930100440979 + 0.1 * 6.9010396003723145
Epoch 90, val loss: 1.6707104444503784
Epoch 100, training loss: 2.286503314971924 = 1.602675199508667 + 0.1 * 6.838281631469727
Epoch 100, val loss: 1.6141269207000732
Epoch 110, training loss: 2.1989805698394775 = 1.5202157497406006 + 0.1 * 6.7876482009887695
Epoch 110, val loss: 1.5483920574188232
Epoch 120, training loss: 2.106234073638916 = 1.4317599534988403 + 0.1 * 6.744742393493652
Epoch 120, val loss: 1.48111891746521
Epoch 130, training loss: 2.018805742263794 = 1.3474308252334595 + 0.1 * 6.713748455047607
Epoch 130, val loss: 1.4208003282546997
Epoch 140, training loss: 1.9401804208755493 = 1.2712818384170532 + 0.1 * 6.688985824584961
Epoch 140, val loss: 1.3698190450668335
Epoch 150, training loss: 1.8694748878479004 = 1.2027003765106201 + 0.1 * 6.667744159698486
Epoch 150, val loss: 1.3266103267669678
Epoch 160, training loss: 1.8052558898925781 = 1.1397088766098022 + 0.1 * 6.6554694175720215
Epoch 160, val loss: 1.287413477897644
Epoch 170, training loss: 1.7429150342941284 = 1.0787630081176758 + 0.1 * 6.641520023345947
Epoch 170, val loss: 1.2489919662475586
Epoch 180, training loss: 1.680638074874878 = 1.0176745653152466 + 0.1 * 6.629634380340576
Epoch 180, val loss: 1.2096638679504395
Epoch 190, training loss: 1.618518590927124 = 0.9567516446113586 + 0.1 * 6.617668628692627
Epoch 190, val loss: 1.1692651510238647
Epoch 200, training loss: 1.5565268993377686 = 0.8961552381515503 + 0.1 * 6.6037163734436035
Epoch 200, val loss: 1.1275920867919922
Epoch 210, training loss: 1.4955602884292603 = 0.8360050320625305 + 0.1 * 6.595552444458008
Epoch 210, val loss: 1.0849593877792358
Epoch 220, training loss: 1.4337042570114136 = 0.7761542201042175 + 0.1 * 6.575500011444092
Epoch 220, val loss: 1.041635274887085
Epoch 230, training loss: 1.3719878196716309 = 0.7155824899673462 + 0.1 * 6.564053535461426
Epoch 230, val loss: 0.997576117515564
Epoch 240, training loss: 1.3108739852905273 = 0.6555604934692383 + 0.1 * 6.553134441375732
Epoch 240, val loss: 0.9548329710960388
Epoch 250, training loss: 1.251917839050293 = 0.5975891351699829 + 0.1 * 6.543287754058838
Epoch 250, val loss: 0.91534823179245
Epoch 260, training loss: 1.1976702213287354 = 0.5428764820098877 + 0.1 * 6.54793643951416
Epoch 260, val loss: 0.8809570074081421
Epoch 270, training loss: 1.1453120708465576 = 0.49242889881134033 + 0.1 * 6.528831481933594
Epoch 270, val loss: 0.8526843786239624
Epoch 280, training loss: 1.097696304321289 = 0.44592857360839844 + 0.1 * 6.517676830291748
Epoch 280, val loss: 0.8299354314804077
Epoch 290, training loss: 1.0540578365325928 = 0.4031537175178528 + 0.1 * 6.509040355682373
Epoch 290, val loss: 0.8118233680725098
Epoch 300, training loss: 1.0138472318649292 = 0.36366716027259827 + 0.1 * 6.501800537109375
Epoch 300, val loss: 0.7970345616340637
Epoch 310, training loss: 0.9773502349853516 = 0.3270237147808075 + 0.1 * 6.503265380859375
Epoch 310, val loss: 0.7849734425544739
Epoch 320, training loss: 0.9417611360549927 = 0.29323282837867737 + 0.1 * 6.4852824211120605
Epoch 320, val loss: 0.7750871181488037
Epoch 330, training loss: 0.9094932675361633 = 0.2621837258338928 + 0.1 * 6.473095417022705
Epoch 330, val loss: 0.7673224806785583
Epoch 340, training loss: 0.8808883428573608 = 0.23392051458358765 + 0.1 * 6.4696784019470215
Epoch 340, val loss: 0.7615911364555359
Epoch 350, training loss: 0.8547959327697754 = 0.2087026685476303 + 0.1 * 6.46093225479126
Epoch 350, val loss: 0.7576133012771606
Epoch 360, training loss: 0.8309844136238098 = 0.18644537031650543 + 0.1 * 6.445390701293945
Epoch 360, val loss: 0.7556048631668091
Epoch 370, training loss: 0.8111013174057007 = 0.16690972447395325 + 0.1 * 6.441915512084961
Epoch 370, val loss: 0.7555740475654602
Epoch 380, training loss: 0.7922861576080322 = 0.1499212086200714 + 0.1 * 6.423648834228516
Epoch 380, val loss: 0.7570080161094666
Epoch 390, training loss: 0.776671290397644 = 0.13511434197425842 + 0.1 * 6.415569305419922
Epoch 390, val loss: 0.7598868012428284
Epoch 400, training loss: 0.7640626430511475 = 0.12218936532735825 + 0.1 * 6.4187331199646
Epoch 400, val loss: 0.763741672039032
Epoch 410, training loss: 0.7507922649383545 = 0.11092488467693329 + 0.1 * 6.3986735343933105
Epoch 410, val loss: 0.7685507535934448
Epoch 420, training loss: 0.7396489381790161 = 0.10103015601634979 + 0.1 * 6.386187553405762
Epoch 420, val loss: 0.7739948034286499
Epoch 430, training loss: 0.7308676242828369 = 0.09228160977363586 + 0.1 * 6.385859966278076
Epoch 430, val loss: 0.779994547367096
Epoch 440, training loss: 0.7217271327972412 = 0.08453842997550964 + 0.1 * 6.371886730194092
Epoch 440, val loss: 0.7863702178001404
Epoch 450, training loss: 0.7143242955207825 = 0.07769428938627243 + 0.1 * 6.366299629211426
Epoch 450, val loss: 0.7926828861236572
Epoch 460, training loss: 0.7075960040092468 = 0.07160177081823349 + 0.1 * 6.359942436218262
Epoch 460, val loss: 0.79971843957901
Epoch 470, training loss: 0.7010979652404785 = 0.06612604111433029 + 0.1 * 6.349719047546387
Epoch 470, val loss: 0.8065518140792847
Epoch 480, training loss: 0.6964555382728577 = 0.06118573993444443 + 0.1 * 6.352697849273682
Epoch 480, val loss: 0.8136481642723083
Epoch 490, training loss: 0.6914089322090149 = 0.05676090344786644 + 0.1 * 6.346479892730713
Epoch 490, val loss: 0.8207280039787292
Epoch 500, training loss: 0.6858587265014648 = 0.052737046033144 + 0.1 * 6.331216335296631
Epoch 500, val loss: 0.8277938961982727
Epoch 510, training loss: 0.6840574741363525 = 0.049107953906059265 + 0.1 * 6.349494934082031
Epoch 510, val loss: 0.8347605466842651
Epoch 520, training loss: 0.6777557134628296 = 0.045806486159563065 + 0.1 * 6.319492340087891
Epoch 520, val loss: 0.8417794108390808
Epoch 530, training loss: 0.6745643615722656 = 0.042812149971723557 + 0.1 * 6.317521572113037
Epoch 530, val loss: 0.8486794829368591
Epoch 540, training loss: 0.6712945699691772 = 0.04010060429573059 + 0.1 * 6.311939239501953
Epoch 540, val loss: 0.8555486798286438
Epoch 550, training loss: 0.6681283116340637 = 0.03763790801167488 + 0.1 * 6.304903984069824
Epoch 550, val loss: 0.8624000549316406
Epoch 560, training loss: 0.6670556664466858 = 0.035392921417951584 + 0.1 * 6.316627025604248
Epoch 560, val loss: 0.8690289855003357
Epoch 570, training loss: 0.6629095077514648 = 0.03335044905543327 + 0.1 * 6.295590400695801
Epoch 570, val loss: 0.875540018081665
Epoch 580, training loss: 0.6610034704208374 = 0.03148600086569786 + 0.1 * 6.295175075531006
Epoch 580, val loss: 0.8821038007736206
Epoch 590, training loss: 0.6582115888595581 = 0.029773380607366562 + 0.1 * 6.284382343292236
Epoch 590, val loss: 0.8885306715965271
Epoch 600, training loss: 0.6582403779029846 = 0.028202330693602562 + 0.1 * 6.300380229949951
Epoch 600, val loss: 0.8947858810424805
Epoch 610, training loss: 0.6550687551498413 = 0.0267623458057642 + 0.1 * 6.283063888549805
Epoch 610, val loss: 0.9010694622993469
Epoch 620, training loss: 0.6544252634048462 = 0.025436177849769592 + 0.1 * 6.289890766143799
Epoch 620, val loss: 0.9072586894035339
Epoch 630, training loss: 0.6519162654876709 = 0.024203667417168617 + 0.1 * 6.277125835418701
Epoch 630, val loss: 0.9131386876106262
Epoch 640, training loss: 0.6496641635894775 = 0.023039421066641808 + 0.1 * 6.266247749328613
Epoch 640, val loss: 0.9191197156906128
Epoch 650, training loss: 0.6498540639877319 = 0.021945666521787643 + 0.1 * 6.279084205627441
Epoch 650, val loss: 0.924827516078949
Epoch 660, training loss: 0.6469264626502991 = 0.02092619054019451 + 0.1 * 6.260002613067627
Epoch 660, val loss: 0.9303921461105347
Epoch 670, training loss: 0.6464546322822571 = 0.020006751641631126 + 0.1 * 6.2644782066345215
Epoch 670, val loss: 0.9359689950942993
Epoch 680, training loss: 0.6447769999504089 = 0.01914169453084469 + 0.1 * 6.25635290145874
Epoch 680, val loss: 0.9414998888969421
Epoch 690, training loss: 0.645726203918457 = 0.018342340365052223 + 0.1 * 6.273838520050049
Epoch 690, val loss: 0.9469650387763977
Epoch 700, training loss: 0.6440900564193726 = 0.01759498566389084 + 0.1 * 6.264950752258301
Epoch 700, val loss: 0.9519713521003723
Epoch 710, training loss: 0.6419641375541687 = 0.016894659027457237 + 0.1 * 6.250694751739502
Epoch 710, val loss: 0.9571101665496826
Epoch 720, training loss: 0.6414739489555359 = 0.01623734086751938 + 0.1 * 6.252366065979004
Epoch 720, val loss: 0.9621570706367493
Epoch 730, training loss: 0.6393953561782837 = 0.015618041157722473 + 0.1 * 6.237773418426514
Epoch 730, val loss: 0.967048704624176
Epoch 740, training loss: 0.6386827826499939 = 0.015035288408398628 + 0.1 * 6.236474990844727
Epoch 740, val loss: 0.9719521999359131
Epoch 750, training loss: 0.6390311121940613 = 0.01448540948331356 + 0.1 * 6.245456695556641
Epoch 750, val loss: 0.9766830801963806
Epoch 760, training loss: 0.6377952098846436 = 0.013966959901154041 + 0.1 * 6.238282203674316
Epoch 760, val loss: 0.9812670946121216
Epoch 770, training loss: 0.6369460225105286 = 0.013477703556418419 + 0.1 * 6.234683036804199
Epoch 770, val loss: 0.9857956767082214
Epoch 780, training loss: 0.6358935236930847 = 0.013010761700570583 + 0.1 * 6.228827953338623
Epoch 780, val loss: 0.9903855919837952
Epoch 790, training loss: 0.6373448967933655 = 0.012569574639201164 + 0.1 * 6.247753143310547
Epoch 790, val loss: 0.9947509765625
Epoch 800, training loss: 0.6353293061256409 = 0.012154849246144295 + 0.1 * 6.231744289398193
Epoch 800, val loss: 0.9990189075469971
Epoch 810, training loss: 0.6345176696777344 = 0.011762461625039577 + 0.1 * 6.22755241394043
Epoch 810, val loss: 1.0032938718795776
Epoch 820, training loss: 0.6335718631744385 = 0.01139009278267622 + 0.1 * 6.221817493438721
Epoch 820, val loss: 1.0075840950012207
Epoch 830, training loss: 0.6358829736709595 = 0.011036443524062634 + 0.1 * 6.248465061187744
Epoch 830, val loss: 1.0116890668869019
Epoch 840, training loss: 0.6324467658996582 = 0.010699605569243431 + 0.1 * 6.217471122741699
Epoch 840, val loss: 1.0156716108322144
Epoch 850, training loss: 0.6334550380706787 = 0.010380097664892673 + 0.1 * 6.230749607086182
Epoch 850, val loss: 1.0197285413742065
Epoch 860, training loss: 0.6318223476409912 = 0.010075347498059273 + 0.1 * 6.217470169067383
Epoch 860, val loss: 1.023608684539795
Epoch 870, training loss: 0.6308054327964783 = 0.009785640984773636 + 0.1 * 6.210197448730469
Epoch 870, val loss: 1.0275243520736694
Epoch 880, training loss: 0.6302947998046875 = 0.009509162977337837 + 0.1 * 6.20785665512085
Epoch 880, val loss: 1.03128981590271
Epoch 890, training loss: 0.630327582359314 = 0.009245787747204304 + 0.1 * 6.210817813873291
Epoch 890, val loss: 1.035078525543213
Epoch 900, training loss: 0.6299424171447754 = 0.008990490809082985 + 0.1 * 6.209519386291504
Epoch 900, val loss: 1.0387872457504272
Epoch 910, training loss: 0.6290573477745056 = 0.008743936195969582 + 0.1 * 6.203134059906006
Epoch 910, val loss: 1.0424981117248535
Epoch 920, training loss: 0.6284118294715881 = 0.008507301099598408 + 0.1 * 6.199045181274414
Epoch 920, val loss: 1.0461426973342896
Epoch 930, training loss: 0.6285673379898071 = 0.008281280286610126 + 0.1 * 6.202860355377197
Epoch 930, val loss: 1.049724817276001
Epoch 940, training loss: 0.6285592317581177 = 0.008065146394073963 + 0.1 * 6.2049407958984375
Epoch 940, val loss: 1.0532183647155762
Epoch 950, training loss: 0.6276692748069763 = 0.007858557626605034 + 0.1 * 6.19810676574707
Epoch 950, val loss: 1.0566705465316772
Epoch 960, training loss: 0.6282201409339905 = 0.0076613337732851505 + 0.1 * 6.205587863922119
Epoch 960, val loss: 1.0600813627243042
Epoch 970, training loss: 0.6263471841812134 = 0.007473128382116556 + 0.1 * 6.1887407302856445
Epoch 970, val loss: 1.063402533531189
Epoch 980, training loss: 0.6261189579963684 = 0.007293154951184988 + 0.1 * 6.188257694244385
Epoch 980, val loss: 1.0667709112167358
Epoch 990, training loss: 0.625276505947113 = 0.0071196709759533405 + 0.1 * 6.181568622589111
Epoch 990, val loss: 1.0701041221618652
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.1771
Flip ASR: 0.2000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.8051857948303223 = 1.9677987098693848 + 0.1 * 8.373869895935059
Epoch 0, val loss: 1.9751118421554565
Epoch 10, training loss: 2.792940378189087 = 1.955572247505188 + 0.1 * 8.373682022094727
Epoch 10, val loss: 1.9620078802108765
Epoch 20, training loss: 2.7769479751586914 = 1.9396929740905762 + 0.1 * 8.372550964355469
Epoch 20, val loss: 1.9447182416915894
Epoch 30, training loss: 2.752774238586426 = 1.9164704084396362 + 0.1 * 8.363039016723633
Epoch 30, val loss: 1.9194724559783936
Epoch 40, training loss: 2.7129502296447754 = 1.8820040225982666 + 0.1 * 8.309460639953613
Epoch 40, val loss: 1.8827838897705078
Epoch 50, training loss: 2.63995361328125 = 1.8371530771255493 + 0.1 * 8.028006553649902
Epoch 50, val loss: 1.8379805088043213
Epoch 60, training loss: 2.567959785461426 = 1.790464162826538 + 0.1 * 7.774956703186035
Epoch 60, val loss: 1.7957574129104614
Epoch 70, training loss: 2.4960217475891113 = 1.750494122505188 + 0.1 * 7.4552764892578125
Epoch 70, val loss: 1.761641263961792
Epoch 80, training loss: 2.4157776832580566 = 1.7064582109451294 + 0.1 * 7.093193531036377
Epoch 80, val loss: 1.7234368324279785
Epoch 90, training loss: 2.340050220489502 = 1.6489808559417725 + 0.1 * 6.910693168640137
Epoch 90, val loss: 1.674773097038269
Epoch 100, training loss: 2.259009838104248 = 1.5753792524337769 + 0.1 * 6.836307048797607
Epoch 100, val loss: 1.6141449213027954
Epoch 110, training loss: 2.1675984859466553 = 1.4899070262908936 + 0.1 * 6.776914596557617
Epoch 110, val loss: 1.5460108518600464
Epoch 120, training loss: 2.0745537281036377 = 1.4008398056030273 + 0.1 * 6.7371392250061035
Epoch 120, val loss: 1.478010892868042
Epoch 130, training loss: 1.9840703010559082 = 1.3130745887756348 + 0.1 * 6.709957122802734
Epoch 130, val loss: 1.4149283170700073
Epoch 140, training loss: 1.8962407112121582 = 1.2273424863815308 + 0.1 * 6.688981533050537
Epoch 140, val loss: 1.3545886278152466
Epoch 150, training loss: 1.8097140789031982 = 1.1425458192825317 + 0.1 * 6.671681880950928
Epoch 150, val loss: 1.2953014373779297
Epoch 160, training loss: 1.7254188060760498 = 1.0596462488174438 + 0.1 * 6.6577253341674805
Epoch 160, val loss: 1.235726237297058
Epoch 170, training loss: 1.644542932510376 = 0.9800235033035278 + 0.1 * 6.6451945304870605
Epoch 170, val loss: 1.1774349212646484
Epoch 180, training loss: 1.5660642385482788 = 0.9032081365585327 + 0.1 * 6.628561019897461
Epoch 180, val loss: 1.1211261749267578
Epoch 190, training loss: 1.4908117055892944 = 0.829745888710022 + 0.1 * 6.610658168792725
Epoch 190, val loss: 1.069268822669983
Epoch 200, training loss: 1.42111074924469 = 0.761464536190033 + 0.1 * 6.596461772918701
Epoch 200, val loss: 1.0233765840530396
Epoch 210, training loss: 1.3566114902496338 = 0.6992567777633667 + 0.1 * 6.573546886444092
Epoch 210, val loss: 0.9835149645805359
Epoch 220, training loss: 1.2999275922775269 = 0.6425515413284302 + 0.1 * 6.573760509490967
Epoch 220, val loss: 0.9482148289680481
Epoch 230, training loss: 1.2457921504974365 = 0.5915358662605286 + 0.1 * 6.542562484741211
Epoch 230, val loss: 0.9182206988334656
Epoch 240, training loss: 1.1972551345825195 = 0.5446133613586426 + 0.1 * 6.526417255401611
Epoch 240, val loss: 0.8922462463378906
Epoch 250, training loss: 1.152260661125183 = 0.5007472038269043 + 0.1 * 6.515134334564209
Epoch 250, val loss: 0.8704774975776672
Epoch 260, training loss: 1.1088368892669678 = 0.45919978618621826 + 0.1 * 6.496371746063232
Epoch 260, val loss: 0.8527050614356995
Epoch 270, training loss: 1.0686349868774414 = 0.4194924831390381 + 0.1 * 6.491425514221191
Epoch 270, val loss: 0.8388171792030334
Epoch 280, training loss: 1.0288299322128296 = 0.38129526376724243 + 0.1 * 6.475346565246582
Epoch 280, val loss: 0.8284905552864075
Epoch 290, training loss: 0.9906985759735107 = 0.3447073996067047 + 0.1 * 6.459911823272705
Epoch 290, val loss: 0.8212503790855408
Epoch 300, training loss: 0.9552218914031982 = 0.3099115490913391 + 0.1 * 6.453103065490723
Epoch 300, val loss: 0.8168503642082214
Epoch 310, training loss: 0.9216523766517639 = 0.27754199504852295 + 0.1 * 6.441103458404541
Epoch 310, val loss: 0.8152803778648376
Epoch 320, training loss: 0.8912041187286377 = 0.24782687425613403 + 0.1 * 6.433772087097168
Epoch 320, val loss: 0.8160369396209717
Epoch 330, training loss: 0.8643772006034851 = 0.2210526317358017 + 0.1 * 6.4332451820373535
Epoch 330, val loss: 0.8192670345306396
Epoch 340, training loss: 0.8380724191665649 = 0.1974823921918869 + 0.1 * 6.405900001525879
Epoch 340, val loss: 0.824678897857666
Epoch 350, training loss: 0.81686931848526 = 0.17690615355968475 + 0.1 * 6.399631500244141
Epoch 350, val loss: 0.8323269486427307
Epoch 360, training loss: 0.7991304397583008 = 0.1590588390827179 + 0.1 * 6.4007158279418945
Epoch 360, val loss: 0.8417946100234985
Epoch 370, training loss: 0.7811258435249329 = 0.1435161978006363 + 0.1 * 6.376096725463867
Epoch 370, val loss: 0.8527193069458008
Epoch 380, training loss: 0.7668673396110535 = 0.12983064353466034 + 0.1 * 6.37036657333374
Epoch 380, val loss: 0.8649519085884094
Epoch 390, training loss: 0.7552305459976196 = 0.11777608096599579 + 0.1 * 6.374544143676758
Epoch 390, val loss: 0.8779576420783997
Epoch 400, training loss: 0.7427651882171631 = 0.10715515166521072 + 0.1 * 6.356100559234619
Epoch 400, val loss: 0.8916850686073303
Epoch 410, training loss: 0.7329751253128052 = 0.09771577268838882 + 0.1 * 6.352593421936035
Epoch 410, val loss: 0.9058952927589417
Epoch 420, training loss: 0.7237768173217773 = 0.0893140584230423 + 0.1 * 6.344627380371094
Epoch 420, val loss: 0.9203395843505859
Epoch 430, training loss: 0.7158991098403931 = 0.0818151906132698 + 0.1 * 6.340839385986328
Epoch 430, val loss: 0.93495112657547
Epoch 440, training loss: 0.7104928493499756 = 0.07510818541049957 + 0.1 * 6.353846549987793
Epoch 440, val loss: 0.9496273398399353
Epoch 450, training loss: 0.7017427086830139 = 0.06912030279636383 + 0.1 * 6.326223850250244
Epoch 450, val loss: 0.9641218185424805
Epoch 460, training loss: 0.696368932723999 = 0.0637429729104042 + 0.1 * 6.326259136199951
Epoch 460, val loss: 0.9784733653068542
Epoch 470, training loss: 0.692611038684845 = 0.05890065059065819 + 0.1 * 6.337103843688965
Epoch 470, val loss: 0.992705762386322
Epoch 480, training loss: 0.6866272687911987 = 0.05454687401652336 + 0.1 * 6.320803642272949
Epoch 480, val loss: 1.0066685676574707
Epoch 490, training loss: 0.6819064021110535 = 0.05060914531350136 + 0.1 * 6.312972068786621
Epoch 490, val loss: 1.0204089879989624
Epoch 500, training loss: 0.677604615688324 = 0.047036562114953995 + 0.1 * 6.305680274963379
Epoch 500, val loss: 1.0339497327804565
Epoch 510, training loss: 0.6748260855674744 = 0.04379429295659065 + 0.1 * 6.3103179931640625
Epoch 510, val loss: 1.0471863746643066
Epoch 520, training loss: 0.6715532541275024 = 0.040856242179870605 + 0.1 * 6.306970119476318
Epoch 520, val loss: 1.0601904392242432
Epoch 530, training loss: 0.6671920418739319 = 0.03818069025874138 + 0.1 * 6.29011344909668
Epoch 530, val loss: 1.072969913482666
Epoch 540, training loss: 0.666630208492279 = 0.035738006234169006 + 0.1 * 6.308921813964844
Epoch 540, val loss: 1.0854414701461792
Epoch 550, training loss: 0.6628124713897705 = 0.03351002186536789 + 0.1 * 6.29302453994751
Epoch 550, val loss: 1.097631573677063
Epoch 560, training loss: 0.6605932116508484 = 0.031471025198698044 + 0.1 * 6.291222095489502
Epoch 560, val loss: 1.109578251838684
Epoch 570, training loss: 0.6576327681541443 = 0.029602529481053352 + 0.1 * 6.28030252456665
Epoch 570, val loss: 1.121263027191162
Epoch 580, training loss: 0.659183919429779 = 0.02788762003183365 + 0.1 * 6.312962532043457
Epoch 580, val loss: 1.1326566934585571
Epoch 590, training loss: 0.6537425518035889 = 0.02631504461169243 + 0.1 * 6.274275302886963
Epoch 590, val loss: 1.1437981128692627
Epoch 600, training loss: 0.6513372659683228 = 0.024867035448551178 + 0.1 * 6.264701843261719
Epoch 600, val loss: 1.1546618938446045
Epoch 610, training loss: 0.6540253758430481 = 0.023530662059783936 + 0.1 * 6.3049468994140625
Epoch 610, val loss: 1.165231466293335
Epoch 620, training loss: 0.648509681224823 = 0.022299937903881073 + 0.1 * 6.262097358703613
Epoch 620, val loss: 1.1754635572433472
Epoch 630, training loss: 0.6475059390068054 = 0.021163206547498703 + 0.1 * 6.263427257537842
Epoch 630, val loss: 1.1855993270874023
Epoch 640, training loss: 0.6452507376670837 = 0.02011032961308956 + 0.1 * 6.251404285430908
Epoch 640, val loss: 1.1953245401382446
Epoch 650, training loss: 0.6438710689544678 = 0.019134603440761566 + 0.1 * 6.247364521026611
Epoch 650, val loss: 1.204981803894043
Epoch 660, training loss: 0.6438280940055847 = 0.01822659559547901 + 0.1 * 6.256014347076416
Epoch 660, val loss: 1.2143586874008179
Epoch 670, training loss: 0.6426219344139099 = 0.01738184504210949 + 0.1 * 6.252400875091553
Epoch 670, val loss: 1.223497748374939
Epoch 680, training loss: 0.6408849358558655 = 0.016594721004366875 + 0.1 * 6.2429022789001465
Epoch 680, val loss: 1.2323741912841797
Epoch 690, training loss: 0.6435521841049194 = 0.015860741958022118 + 0.1 * 6.276914596557617
Epoch 690, val loss: 1.2410978078842163
Epoch 700, training loss: 0.6397655010223389 = 0.015177802182734013 + 0.1 * 6.245876789093018
Epoch 700, val loss: 1.2494791746139526
Epoch 710, training loss: 0.6378230452537537 = 0.01453994307667017 + 0.1 * 6.232831001281738
Epoch 710, val loss: 1.2578920125961304
Epoch 720, training loss: 0.6373217105865479 = 0.013942062854766846 + 0.1 * 6.2337965965271
Epoch 720, val loss: 1.2659348249435425
Epoch 730, training loss: 0.6364304423332214 = 0.013381252996623516 + 0.1 * 6.230491638183594
Epoch 730, val loss: 1.2738101482391357
Epoch 740, training loss: 0.6370601058006287 = 0.012855186127126217 + 0.1 * 6.242049217224121
Epoch 740, val loss: 1.2814104557037354
Epoch 750, training loss: 0.634668231010437 = 0.012361068278551102 + 0.1 * 6.223071575164795
Epoch 750, val loss: 1.2890478372573853
Epoch 760, training loss: 0.6338626742362976 = 0.011896204203367233 + 0.1 * 6.219664573669434
Epoch 760, val loss: 1.296494722366333
Epoch 770, training loss: 0.6329337358474731 = 0.011457698419690132 + 0.1 * 6.2147603034973145
Epoch 770, val loss: 1.3035999536514282
Epoch 780, training loss: 0.6356546878814697 = 0.011044986546039581 + 0.1 * 6.246097087860107
Epoch 780, val loss: 1.3106019496917725
Epoch 790, training loss: 0.6315038204193115 = 0.010656225495040417 + 0.1 * 6.208475589752197
Epoch 790, val loss: 1.3175561428070068
Epoch 800, training loss: 0.6311289668083191 = 0.010288693942129612 + 0.1 * 6.208402156829834
Epoch 800, val loss: 1.3244458436965942
Epoch 810, training loss: 0.6326215863227844 = 0.00994039885699749 + 0.1 * 6.22681188583374
Epoch 810, val loss: 1.3310401439666748
Epoch 820, training loss: 0.6317171454429626 = 0.009610936045646667 + 0.1 * 6.221061706542969
Epoch 820, val loss: 1.3371840715408325
Epoch 830, training loss: 0.629456102848053 = 0.00929939839988947 + 0.1 * 6.201566696166992
Epoch 830, val loss: 1.3437632322311401
Epoch 840, training loss: 0.6308120489120483 = 0.009003782644867897 + 0.1 * 6.218082904815674
Epoch 840, val loss: 1.3498926162719727
Epoch 850, training loss: 0.6289302110671997 = 0.008723437786102295 + 0.1 * 6.202067852020264
Epoch 850, val loss: 1.356031060218811
Epoch 860, training loss: 0.6280532479286194 = 0.008456332609057426 + 0.1 * 6.195969104766846
Epoch 860, val loss: 1.361967921257019
Epoch 870, training loss: 0.6281341314315796 = 0.008201907388865948 + 0.1 * 6.19932222366333
Epoch 870, val loss: 1.3678309917449951
Epoch 880, training loss: 0.6290295124053955 = 0.007959417998790741 + 0.1 * 6.210700988769531
Epoch 880, val loss: 1.3735275268554688
Epoch 890, training loss: 0.6274978518486023 = 0.00772841926664114 + 0.1 * 6.197694301605225
Epoch 890, val loss: 1.3790168762207031
Epoch 900, training loss: 0.6275680065155029 = 0.007509045768529177 + 0.1 * 6.200589179992676
Epoch 900, val loss: 1.384781837463379
Epoch 910, training loss: 0.6260157227516174 = 0.007299043238162994 + 0.1 * 6.187166213989258
Epoch 910, val loss: 1.3900283575057983
Epoch 920, training loss: 0.6269817352294922 = 0.007099214941263199 + 0.1 * 6.198825359344482
Epoch 920, val loss: 1.3954578638076782
Epoch 930, training loss: 0.6254145503044128 = 0.006908100098371506 + 0.1 * 6.185064315795898
Epoch 930, val loss: 1.4006184339523315
Epoch 940, training loss: 0.625790536403656 = 0.0067253075540065765 + 0.1 * 6.190652370452881
Epoch 940, val loss: 1.405733346939087
Epoch 950, training loss: 0.6247028708457947 = 0.006550289690494537 + 0.1 * 6.181525230407715
Epoch 950, val loss: 1.410778522491455
Epoch 960, training loss: 0.6257883310317993 = 0.006382620427757502 + 0.1 * 6.194056510925293
Epoch 960, val loss: 1.4158117771148682
Epoch 970, training loss: 0.625248372554779 = 0.006222010590136051 + 0.1 * 6.190263271331787
Epoch 970, val loss: 1.4204483032226562
Epoch 980, training loss: 0.6239528059959412 = 0.0060682399198412895 + 0.1 * 6.178845405578613
Epoch 980, val loss: 1.4253432750701904
Epoch 990, training loss: 0.6241071224212646 = 0.005920625291764736 + 0.1 * 6.1818647384643555
Epoch 990, val loss: 1.4299976825714111
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.6679
Flip ASR: 0.6178/225 nodes
The final ASR:0.48339, 0.21808, Accuracy:0.80617, 0.01492
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9526])
updated graph: torch.Size([2, 10562])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98278, 0.00460, Accuracy:0.83580, 0.00972
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.770993947982788 = 1.9336103200912476 + 0.1 * 8.373835563659668
Epoch 0, val loss: 1.9224785566329956
Epoch 10, training loss: 2.7618703842163086 = 1.924500823020935 + 0.1 * 8.373696327209473
Epoch 10, val loss: 1.9140546321868896
Epoch 20, training loss: 2.750948429107666 = 1.9136685132980347 + 0.1 * 8.372798919677734
Epoch 20, val loss: 1.9037439823150635
Epoch 30, training loss: 2.735308885574341 = 1.8987576961517334 + 0.1 * 8.365510940551758
Epoch 30, val loss: 1.8893494606018066
Epoch 40, training loss: 2.7071776390075684 = 1.8766776323318481 + 0.1 * 8.304999351501465
Epoch 40, val loss: 1.868145227432251
Epoch 50, training loss: 2.6346821784973145 = 1.846093773841858 + 0.1 * 7.88588285446167
Epoch 50, val loss: 1.840437650680542
Epoch 60, training loss: 2.5577054023742676 = 1.8133107423782349 + 0.1 * 7.4439473152160645
Epoch 60, val loss: 1.812811017036438
Epoch 70, training loss: 2.4801690578460693 = 1.7814021110534668 + 0.1 * 6.987669944763184
Epoch 70, val loss: 1.7875245809555054
Epoch 80, training loss: 2.4228010177612305 = 1.7473926544189453 + 0.1 * 6.754082679748535
Epoch 80, val loss: 1.7615582942962646
Epoch 90, training loss: 2.372375965118408 = 1.704673171043396 + 0.1 * 6.677028656005859
Epoch 90, val loss: 1.7276339530944824
Epoch 100, training loss: 2.31143856048584 = 1.647239089012146 + 0.1 * 6.641995429992676
Epoch 100, val loss: 1.6798747777938843
Epoch 110, training loss: 2.2334659099578857 = 1.572305679321289 + 0.1 * 6.611602783203125
Epoch 110, val loss: 1.6181286573410034
Epoch 120, training loss: 2.1390299797058105 = 1.4804452657699585 + 0.1 * 6.585848331451416
Epoch 120, val loss: 1.543915867805481
Epoch 130, training loss: 2.0342214107513428 = 1.3773436546325684 + 0.1 * 6.568776607513428
Epoch 130, val loss: 1.4612456560134888
Epoch 140, training loss: 1.9254645109176636 = 1.2697592973709106 + 0.1 * 6.557052135467529
Epoch 140, val loss: 1.376193881034851
Epoch 150, training loss: 1.8196742534637451 = 1.164893388748169 + 0.1 * 6.547807693481445
Epoch 150, val loss: 1.2939724922180176
Epoch 160, training loss: 1.7205111980438232 = 1.0665812492370605 + 0.1 * 6.539299488067627
Epoch 160, val loss: 1.2166635990142822
Epoch 170, training loss: 1.6297638416290283 = 0.9768256545066833 + 0.1 * 6.529381275177002
Epoch 170, val loss: 1.1468037366867065
Epoch 180, training loss: 1.548429250717163 = 0.8956943154335022 + 0.1 * 6.52734899520874
Epoch 180, val loss: 1.084084391593933
Epoch 190, training loss: 1.4725923538208008 = 0.8209984302520752 + 0.1 * 6.515939712524414
Epoch 190, val loss: 1.027174949645996
Epoch 200, training loss: 1.4004572629928589 = 0.7493589520454407 + 0.1 * 6.510982990264893
Epoch 200, val loss: 0.9729307889938354
Epoch 210, training loss: 1.3302638530731201 = 0.6799648404121399 + 0.1 * 6.502989768981934
Epoch 210, val loss: 0.9205899834632874
Epoch 220, training loss: 1.2637171745300293 = 0.6136926412582397 + 0.1 * 6.500246047973633
Epoch 220, val loss: 0.871446430683136
Epoch 230, training loss: 1.2021448612213135 = 0.5526758432388306 + 0.1 * 6.494690418243408
Epoch 230, val loss: 0.8277345895767212
Epoch 240, training loss: 1.1468422412872314 = 0.4982118010520935 + 0.1 * 6.486303806304932
Epoch 240, val loss: 0.7910802364349365
Epoch 250, training loss: 1.0978294610977173 = 0.4506055414676666 + 0.1 * 6.472239017486572
Epoch 250, val loss: 0.7617284655570984
Epoch 260, training loss: 1.054849624633789 = 0.4086637496948242 + 0.1 * 6.461857795715332
Epoch 260, val loss: 0.7384424209594727
Epoch 270, training loss: 1.0179691314697266 = 0.3716300129890442 + 0.1 * 6.463390827178955
Epoch 270, val loss: 0.7201915979385376
Epoch 280, training loss: 0.9834709167480469 = 0.3392554223537445 + 0.1 * 6.442154884338379
Epoch 280, val loss: 0.7065364122390747
Epoch 290, training loss: 0.9535799026489258 = 0.31040510535240173 + 0.1 * 6.431747913360596
Epoch 290, val loss: 0.696228563785553
Epoch 300, training loss: 0.9264035224914551 = 0.284610778093338 + 0.1 * 6.417927265167236
Epoch 300, val loss: 0.6891375780105591
Epoch 310, training loss: 0.9019807577133179 = 0.2612505257129669 + 0.1 * 6.407302379608154
Epoch 310, val loss: 0.6847088932991028
Epoch 320, training loss: 0.8805228471755981 = 0.23970967531204224 + 0.1 * 6.4081315994262695
Epoch 320, val loss: 0.6824471950531006
Epoch 330, training loss: 0.8576648235321045 = 0.21960701048374176 + 0.1 * 6.38057804107666
Epoch 330, val loss: 0.6819358468055725
Epoch 340, training loss: 0.8416574001312256 = 0.20051389932632446 + 0.1 * 6.411435127258301
Epoch 340, val loss: 0.6828500032424927
Epoch 350, training loss: 0.819757878780365 = 0.182529017329216 + 0.1 * 6.372288227081299
Epoch 350, val loss: 0.685078501701355
Epoch 360, training loss: 0.8004440069198608 = 0.16553732752799988 + 0.1 * 6.349067211151123
Epoch 360, val loss: 0.6886023879051208
Epoch 370, training loss: 0.7843747735023499 = 0.14970290660858154 + 0.1 * 6.3467183113098145
Epoch 370, val loss: 0.6933579444885254
Epoch 380, training loss: 0.7696635127067566 = 0.13538634777069092 + 0.1 * 6.342771530151367
Epoch 380, val loss: 0.6991330981254578
Epoch 390, training loss: 0.7555184364318848 = 0.12257224321365356 + 0.1 * 6.329461574554443
Epoch 390, val loss: 0.7058302760124207
Epoch 400, training loss: 0.7454939484596252 = 0.11110619455575943 + 0.1 * 6.34387731552124
Epoch 400, val loss: 0.7134010791778564
Epoch 410, training loss: 0.7323772311210632 = 0.10090117156505585 + 0.1 * 6.314760208129883
Epoch 410, val loss: 0.721662163734436
Epoch 420, training loss: 0.7240539789199829 = 0.09178939461708069 + 0.1 * 6.322645664215088
Epoch 420, val loss: 0.7305010557174683
Epoch 430, training loss: 0.7145510315895081 = 0.08368039131164551 + 0.1 * 6.308706283569336
Epoch 430, val loss: 0.7396504878997803
Epoch 440, training loss: 0.7067996859550476 = 0.07644344866275787 + 0.1 * 6.303562164306641
Epoch 440, val loss: 0.749154806137085
Epoch 450, training loss: 0.699275016784668 = 0.06996278464794159 + 0.1 * 6.293121814727783
Epoch 450, val loss: 0.7588309049606323
Epoch 460, training loss: 0.6948215961456299 = 0.06417067348957062 + 0.1 * 6.306509494781494
Epoch 460, val loss: 0.7685776352882385
Epoch 470, training loss: 0.6880666613578796 = 0.05903104320168495 + 0.1 * 6.290355682373047
Epoch 470, val loss: 0.7782166600227356
Epoch 480, training loss: 0.6829060912132263 = 0.05442676320672035 + 0.1 * 6.284792900085449
Epoch 480, val loss: 0.7878789901733398
Epoch 490, training loss: 0.6779290437698364 = 0.05029555782675743 + 0.1 * 6.2763352394104
Epoch 490, val loss: 0.7973834872245789
Epoch 500, training loss: 0.6735935211181641 = 0.04658785089850426 + 0.1 * 6.27005672454834
Epoch 500, val loss: 0.8067712187767029
Epoch 510, training loss: 0.6697031855583191 = 0.04325098171830177 + 0.1 * 6.264522075653076
Epoch 510, val loss: 0.8159947991371155
Epoch 520, training loss: 0.6663016080856323 = 0.04024004563689232 + 0.1 * 6.260615348815918
Epoch 520, val loss: 0.8250352144241333
Epoch 530, training loss: 0.6644101142883301 = 0.03751692920923233 + 0.1 * 6.2689313888549805
Epoch 530, val loss: 0.8339746594429016
Epoch 540, training loss: 0.6607381105422974 = 0.035057298839092255 + 0.1 * 6.256807804107666
Epoch 540, val loss: 0.842634916305542
Epoch 550, training loss: 0.6583045721054077 = 0.03282942995429039 + 0.1 * 6.254751682281494
Epoch 550, val loss: 0.8511420488357544
Epoch 560, training loss: 0.6556047797203064 = 0.03081420250236988 + 0.1 * 6.247905254364014
Epoch 560, val loss: 0.8593461513519287
Epoch 570, training loss: 0.6532630920410156 = 0.02897348441183567 + 0.1 * 6.24289608001709
Epoch 570, val loss: 0.867476224899292
Epoch 580, training loss: 0.654478907585144 = 0.027287263423204422 + 0.1 * 6.271916389465332
Epoch 580, val loss: 0.8754376769065857
Epoch 590, training loss: 0.6502436995506287 = 0.025752319023013115 + 0.1 * 6.2449140548706055
Epoch 590, val loss: 0.8831464052200317
Epoch 600, training loss: 0.6477993726730347 = 0.02434484288096428 + 0.1 * 6.2345452308654785
Epoch 600, val loss: 0.8906946778297424
Epoch 610, training loss: 0.6466501951217651 = 0.02304837666451931 + 0.1 * 6.23601770401001
Epoch 610, val loss: 0.8981304168701172
Epoch 620, training loss: 0.6451090574264526 = 0.02185409888625145 + 0.1 * 6.23254919052124
Epoch 620, val loss: 0.9053574800491333
Epoch 630, training loss: 0.6433399319648743 = 0.020753249526023865 + 0.1 * 6.225866794586182
Epoch 630, val loss: 0.9124358892440796
Epoch 640, training loss: 0.6417431831359863 = 0.019735148176550865 + 0.1 * 6.220080375671387
Epoch 640, val loss: 0.9193928837776184
Epoch 650, training loss: 0.6427185535430908 = 0.018792418763041496 + 0.1 * 6.239261627197266
Epoch 650, val loss: 0.9261335730552673
Epoch 660, training loss: 0.6397126913070679 = 0.01792151667177677 + 0.1 * 6.217911720275879
Epoch 660, val loss: 0.9327093958854675
Epoch 670, training loss: 0.6388460993766785 = 0.0171127300709486 + 0.1 * 6.217333793640137
Epoch 670, val loss: 0.9391406178474426
Epoch 680, training loss: 0.638659656047821 = 0.016359273344278336 + 0.1 * 6.22300386428833
Epoch 680, val loss: 0.9454314708709717
Epoch 690, training loss: 0.6364659070968628 = 0.015656420961022377 + 0.1 * 6.208095073699951
Epoch 690, val loss: 0.9515708684921265
Epoch 700, training loss: 0.6374080181121826 = 0.01500067301094532 + 0.1 * 6.2240729331970215
Epoch 700, val loss: 0.9576153755187988
Epoch 710, training loss: 0.634757936000824 = 0.01438874565064907 + 0.1 * 6.2036919593811035
Epoch 710, val loss: 0.9634804129600525
Epoch 720, training loss: 0.6342511177062988 = 0.01381521113216877 + 0.1 * 6.20435905456543
Epoch 720, val loss: 0.9692821502685547
Epoch 730, training loss: 0.6344205141067505 = 0.013276792131364346 + 0.1 * 6.211436748504639
Epoch 730, val loss: 0.9749102592468262
Epoch 740, training loss: 0.6334919929504395 = 0.012771649286150932 + 0.1 * 6.207203388214111
Epoch 740, val loss: 0.9804701209068298
Epoch 750, training loss: 0.6322831511497498 = 0.01229678001254797 + 0.1 * 6.199863433837891
Epoch 750, val loss: 0.9858972430229187
Epoch 760, training loss: 0.632179856300354 = 0.01184968650341034 + 0.1 * 6.203301429748535
Epoch 760, val loss: 0.9912135601043701
Epoch 770, training loss: 0.6320617198944092 = 0.01142826210707426 + 0.1 * 6.206334590911865
Epoch 770, val loss: 0.9964542984962463
Epoch 780, training loss: 0.6310799717903137 = 0.011030813679099083 + 0.1 * 6.200491905212402
Epoch 780, val loss: 1.0015509128570557
Epoch 790, training loss: 0.6301763653755188 = 0.01065651886165142 + 0.1 * 6.195198059082031
Epoch 790, val loss: 1.0066003799438477
Epoch 800, training loss: 0.6286442875862122 = 0.010301848873496056 + 0.1 * 6.183424472808838
Epoch 800, val loss: 1.0115143060684204
Epoch 810, training loss: 0.6305536031723022 = 0.009965360164642334 + 0.1 * 6.2058820724487305
Epoch 810, val loss: 1.0163893699645996
Epoch 820, training loss: 0.6278826594352722 = 0.00964739266782999 + 0.1 * 6.182352542877197
Epoch 820, val loss: 1.0211092233657837
Epoch 830, training loss: 0.6281740069389343 = 0.009345926344394684 + 0.1 * 6.1882805824279785
Epoch 830, val loss: 1.02580726146698
Epoch 840, training loss: 0.6276015043258667 = 0.009059333242475986 + 0.1 * 6.185421943664551
Epoch 840, val loss: 1.030361533164978
Epoch 850, training loss: 0.626379668712616 = 0.00878741592168808 + 0.1 * 6.175922393798828
Epoch 850, val loss: 1.034860372543335
Epoch 860, training loss: 0.6257791519165039 = 0.008528590202331543 + 0.1 * 6.1725053787231445
Epoch 860, val loss: 1.0392872095108032
Epoch 870, training loss: 0.6273095607757568 = 0.008281177841126919 + 0.1 * 6.19028377532959
Epoch 870, val loss: 1.0436580181121826
Epoch 880, training loss: 0.6269814968109131 = 0.008045962080359459 + 0.1 * 6.18935489654541
Epoch 880, val loss: 1.0479212999343872
Epoch 890, training loss: 0.6255603432655334 = 0.007822537794709206 + 0.1 * 6.177378177642822
Epoch 890, val loss: 1.0521485805511475
Epoch 900, training loss: 0.6241191625595093 = 0.007609446067363024 + 0.1 * 6.165097236633301
Epoch 900, val loss: 1.0562357902526855
Epoch 910, training loss: 0.6263501048088074 = 0.007405285257846117 + 0.1 * 6.189448356628418
Epoch 910, val loss: 1.0603303909301758
Epoch 920, training loss: 0.6243633031845093 = 0.007210414391011 + 0.1 * 6.1715288162231445
Epoch 920, val loss: 1.064322590827942
Epoch 930, training loss: 0.6231101155281067 = 0.007023775018751621 + 0.1 * 6.160863399505615
Epoch 930, val loss: 1.068253517150879
Epoch 940, training loss: 0.6237674951553345 = 0.006844949442893267 + 0.1 * 6.169225692749023
Epoch 940, val loss: 1.072135090827942
Epoch 950, training loss: 0.623531699180603 = 0.0066739646717906 + 0.1 * 6.168577194213867
Epoch 950, val loss: 1.075965166091919
Epoch 960, training loss: 0.6217249035835266 = 0.006510142236948013 + 0.1 * 6.1521477699279785
Epoch 960, val loss: 1.0797288417816162
Epoch 970, training loss: 0.6228329539299011 = 0.006352548953145742 + 0.1 * 6.164803981781006
Epoch 970, val loss: 1.0834418535232544
Epoch 980, training loss: 0.6213286519050598 = 0.006201272830367088 + 0.1 * 6.151273727416992
Epoch 980, val loss: 1.0871084928512573
Epoch 990, training loss: 0.6227424740791321 = 0.006056192796677351 + 0.1 * 6.166862487792969
Epoch 990, val loss: 1.090692400932312
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6052
Flip ASR: 0.5244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7915992736816406 = 1.9542146921157837 + 0.1 * 8.373845100402832
Epoch 0, val loss: 1.961725115776062
Epoch 10, training loss: 2.781215190887451 = 1.9438650608062744 + 0.1 * 8.373501777648926
Epoch 10, val loss: 1.9502127170562744
Epoch 20, training loss: 2.7684707641601562 = 1.9313291311264038 + 0.1 * 8.371416091918945
Epoch 20, val loss: 1.9359806776046753
Epoch 30, training loss: 2.7498748302459717 = 1.9139248132705688 + 0.1 * 8.359500885009766
Epoch 30, val loss: 1.9162219762802124
Epoch 40, training loss: 2.7167651653289795 = 1.8884690999984741 + 0.1 * 8.282960891723633
Epoch 40, val loss: 1.8876374959945679
Epoch 50, training loss: 2.6412007808685303 = 1.8538358211517334 + 0.1 * 7.873649597167969
Epoch 50, val loss: 1.8504633903503418
Epoch 60, training loss: 2.5545082092285156 = 1.8157321214675903 + 0.1 * 7.387761116027832
Epoch 60, val loss: 1.8119624853134155
Epoch 70, training loss: 2.478694438934326 = 1.7778315544128418 + 0.1 * 7.00862979888916
Epoch 70, val loss: 1.7742007970809937
Epoch 80, training loss: 2.418713092803955 = 1.735615849494934 + 0.1 * 6.830973148345947
Epoch 80, val loss: 1.7340210676193237
Epoch 90, training loss: 2.361266613006592 = 1.6858928203582764 + 0.1 * 6.753737449645996
Epoch 90, val loss: 1.688643455505371
Epoch 100, training loss: 2.2922747135162354 = 1.6201273202896118 + 0.1 * 6.721473693847656
Epoch 100, val loss: 1.631139874458313
Epoch 110, training loss: 2.2063357830047607 = 1.5365777015686035 + 0.1 * 6.697579860687256
Epoch 110, val loss: 1.5608022212982178
Epoch 120, training loss: 2.1070008277893066 = 1.439054012298584 + 0.1 * 6.679469108581543
Epoch 120, val loss: 1.480449914932251
Epoch 130, training loss: 2.002685785293579 = 1.335878610610962 + 0.1 * 6.66807222366333
Epoch 130, val loss: 1.3970181941986084
Epoch 140, training loss: 1.8979401588439941 = 1.2320629358291626 + 0.1 * 6.658771514892578
Epoch 140, val loss: 1.3170608282089233
Epoch 150, training loss: 1.79416823387146 = 1.1291038990020752 + 0.1 * 6.6506428718566895
Epoch 150, val loss: 1.240598201751709
Epoch 160, training loss: 1.6919022798538208 = 1.0277312994003296 + 0.1 * 6.641709804534912
Epoch 160, val loss: 1.1656965017318726
Epoch 170, training loss: 1.592494010925293 = 0.9290364384651184 + 0.1 * 6.634575843811035
Epoch 170, val loss: 1.092637538909912
Epoch 180, training loss: 1.4987125396728516 = 0.8366875052452087 + 0.1 * 6.620250225067139
Epoch 180, val loss: 1.0239747762680054
Epoch 190, training loss: 1.4140573740005493 = 0.7533146739006042 + 0.1 * 6.60742712020874
Epoch 190, val loss: 0.9623443484306335
Epoch 200, training loss: 1.3396729230880737 = 0.6807088851928711 + 0.1 * 6.589640140533447
Epoch 200, val loss: 0.9098799824714661
Epoch 210, training loss: 1.2745647430419922 = 0.617157518863678 + 0.1 * 6.574071407318115
Epoch 210, val loss: 0.8658223748207092
Epoch 220, training loss: 1.216753602027893 = 0.560441792011261 + 0.1 * 6.563117980957031
Epoch 220, val loss: 0.8290417790412903
Epoch 230, training loss: 1.163470983505249 = 0.5088574886322021 + 0.1 * 6.5461344718933105
Epoch 230, val loss: 0.7983871102333069
Epoch 240, training loss: 1.1145294904708862 = 0.46086356043815613 + 0.1 * 6.536658763885498
Epoch 240, val loss: 0.7718671560287476
Epoch 250, training loss: 1.0681174993515015 = 0.41601642966270447 + 0.1 * 6.521010398864746
Epoch 250, val loss: 0.7488044500350952
Epoch 260, training loss: 1.0266671180725098 = 0.37398529052734375 + 0.1 * 6.526817321777344
Epoch 260, val loss: 0.7280988097190857
Epoch 270, training loss: 0.9860354661941528 = 0.33575308322906494 + 0.1 * 6.502823829650879
Epoch 270, val loss: 0.7107229828834534
Epoch 280, training loss: 0.9499056935310364 = 0.3004356622695923 + 0.1 * 6.494699954986572
Epoch 280, val loss: 0.6957957148551941
Epoch 290, training loss: 0.9160884618759155 = 0.26770463585853577 + 0.1 * 6.4838385581970215
Epoch 290, val loss: 0.6835962533950806
Epoch 300, training loss: 0.8861328363418579 = 0.23776701092720032 + 0.1 * 6.483658313751221
Epoch 300, val loss: 0.6737760901451111
Epoch 310, training loss: 0.8580983877182007 = 0.21094052493572235 + 0.1 * 6.471578598022461
Epoch 310, val loss: 0.6665910482406616
Epoch 320, training loss: 0.8328042030334473 = 0.1870173066854477 + 0.1 * 6.457868576049805
Epoch 320, val loss: 0.6617830395698547
Epoch 330, training loss: 0.8123440742492676 = 0.16605669260025024 + 0.1 * 6.462873458862305
Epoch 330, val loss: 0.659348726272583
Epoch 340, training loss: 0.7922050952911377 = 0.14796429872512817 + 0.1 * 6.442408084869385
Epoch 340, val loss: 0.6592844724655151
Epoch 350, training loss: 0.7763305902481079 = 0.13232438266277313 + 0.1 * 6.440061569213867
Epoch 350, val loss: 0.661210298538208
Epoch 360, training loss: 0.7613848447799683 = 0.11890460550785065 + 0.1 * 6.424802303314209
Epoch 360, val loss: 0.6649196147918701
Epoch 370, training loss: 0.7491673827171326 = 0.10735023766756058 + 0.1 * 6.418171405792236
Epoch 370, val loss: 0.6700157523155212
Epoch 380, training loss: 0.7378897666931152 = 0.0973239317536354 + 0.1 * 6.405657768249512
Epoch 380, val loss: 0.6763861179351807
Epoch 390, training loss: 0.7287019491195679 = 0.08854541927576065 + 0.1 * 6.401565074920654
Epoch 390, val loss: 0.6835921406745911
Epoch 400, training loss: 0.7204044461250305 = 0.08085130900144577 + 0.1 * 6.395531177520752
Epoch 400, val loss: 0.6914588809013367
Epoch 410, training loss: 0.7125919461250305 = 0.0740751400589943 + 0.1 * 6.385167598724365
Epoch 410, val loss: 0.6997922658920288
Epoch 420, training loss: 0.7057219743728638 = 0.06808486580848694 + 0.1 * 6.376371383666992
Epoch 420, val loss: 0.7083084583282471
Epoch 430, training loss: 0.6989490985870361 = 0.06278063356876373 + 0.1 * 6.361684799194336
Epoch 430, val loss: 0.717082679271698
Epoch 440, training loss: 0.6945362091064453 = 0.058043062686920166 + 0.1 * 6.364931106567383
Epoch 440, val loss: 0.7259304523468018
Epoch 450, training loss: 0.6887557506561279 = 0.053805138915777206 + 0.1 * 6.349506378173828
Epoch 450, val loss: 0.7347432374954224
Epoch 460, training loss: 0.6845331192016602 = 0.04999824985861778 + 0.1 * 6.345348834991455
Epoch 460, val loss: 0.7436197996139526
Epoch 470, training loss: 0.6810639500617981 = 0.04657257720828056 + 0.1 * 6.344913482666016
Epoch 470, val loss: 0.7523778080940247
Epoch 480, training loss: 0.6765169501304626 = 0.04347512871026993 + 0.1 * 6.330417633056641
Epoch 480, val loss: 0.7611103057861328
Epoch 490, training loss: 0.6753748655319214 = 0.0406741201877594 + 0.1 * 6.3470072746276855
Epoch 490, val loss: 0.7695662379264832
Epoch 500, training loss: 0.6704429984092712 = 0.03813720494508743 + 0.1 * 6.323057651519775
Epoch 500, val loss: 0.7781203389167786
Epoch 510, training loss: 0.6688506007194519 = 0.035824596881866455 + 0.1 * 6.330259799957275
Epoch 510, val loss: 0.7863485217094421
Epoch 520, training loss: 0.6652370691299438 = 0.03372354805469513 + 0.1 * 6.315135478973389
Epoch 520, val loss: 0.7945617437362671
Epoch 530, training loss: 0.6619545817375183 = 0.03179686889052391 + 0.1 * 6.301577091217041
Epoch 530, val loss: 0.8026305437088013
Epoch 540, training loss: 0.6594085693359375 = 0.030027173459529877 + 0.1 * 6.293814182281494
Epoch 540, val loss: 0.8104429841041565
Epoch 550, training loss: 0.6595112085342407 = 0.0284019336104393 + 0.1 * 6.311092853546143
Epoch 550, val loss: 0.8182902336120605
Epoch 560, training loss: 0.6554727554321289 = 0.026907773688435555 + 0.1 * 6.285649299621582
Epoch 560, val loss: 0.8259173631668091
Epoch 570, training loss: 0.6538795828819275 = 0.025528663769364357 + 0.1 * 6.283508777618408
Epoch 570, val loss: 0.8334638476371765
Epoch 580, training loss: 0.6544454097747803 = 0.024249514564871788 + 0.1 * 6.301959037780762
Epoch 580, val loss: 0.8408668637275696
Epoch 590, training loss: 0.6501015424728394 = 0.023067660629749298 + 0.1 * 6.270338535308838
Epoch 590, val loss: 0.8480478525161743
Epoch 600, training loss: 0.6489919424057007 = 0.021972108632326126 + 0.1 * 6.270197868347168
Epoch 600, val loss: 0.8552361130714417
Epoch 610, training loss: 0.6477274894714355 = 0.02095252461731434 + 0.1 * 6.267749309539795
Epoch 610, val loss: 0.8621826767921448
Epoch 620, training loss: 0.6463189721107483 = 0.02000570110976696 + 0.1 * 6.263132572174072
Epoch 620, val loss: 0.8689669966697693
Epoch 630, training loss: 0.644216001033783 = 0.019123954698443413 + 0.1 * 6.250920295715332
Epoch 630, val loss: 0.875749409198761
Epoch 640, training loss: 0.6441390514373779 = 0.01829870045185089 + 0.1 * 6.258403301239014
Epoch 640, val loss: 0.8823204636573792
Epoch 650, training loss: 0.6429082751274109 = 0.017526842653751373 + 0.1 * 6.253814220428467
Epoch 650, val loss: 0.8886891603469849
Epoch 660, training loss: 0.6408873200416565 = 0.01680552028119564 + 0.1 * 6.240817546844482
Epoch 660, val loss: 0.8950036764144897
Epoch 670, training loss: 0.6393810510635376 = 0.016129039227962494 + 0.1 * 6.23252010345459
Epoch 670, val loss: 0.9012840986251831
Epoch 680, training loss: 0.6406459212303162 = 0.015491980127990246 + 0.1 * 6.25153923034668
Epoch 680, val loss: 0.9073331356048584
Epoch 690, training loss: 0.6386538147926331 = 0.014894750900566578 + 0.1 * 6.237590789794922
Epoch 690, val loss: 0.9131996035575867
Epoch 700, training loss: 0.6368660926818848 = 0.014333558268845081 + 0.1 * 6.225325584411621
Epoch 700, val loss: 0.9190624952316284
Epoch 710, training loss: 0.6374367475509644 = 0.013804330490529537 + 0.1 * 6.236324310302734
Epoch 710, val loss: 0.9248161315917969
Epoch 720, training loss: 0.6356937885284424 = 0.013305376283824444 + 0.1 * 6.223884105682373
Epoch 720, val loss: 0.9303752183914185
Epoch 730, training loss: 0.636654257774353 = 0.01283454429358244 + 0.1 * 6.238196849822998
Epoch 730, val loss: 0.9358770251274109
Epoch 740, training loss: 0.6338258981704712 = 0.01238939631730318 + 0.1 * 6.214364528656006
Epoch 740, val loss: 0.9412298798561096
Epoch 750, training loss: 0.6348005533218384 = 0.011967646889388561 + 0.1 * 6.228328704833984
Epoch 750, val loss: 0.9466052651405334
Epoch 760, training loss: 0.6328752040863037 = 0.011568192392587662 + 0.1 * 6.213069915771484
Epoch 760, val loss: 0.951731264591217
Epoch 770, training loss: 0.6342650651931763 = 0.011189178563654423 + 0.1 * 6.230759143829346
Epoch 770, val loss: 0.9569096565246582
Epoch 780, training loss: 0.6317406892776489 = 0.010829749517142773 + 0.1 * 6.209109306335449
Epoch 780, val loss: 0.9617609977722168
Epoch 790, training loss: 0.6308922171592712 = 0.010490014217793941 + 0.1 * 6.20402193069458
Epoch 790, val loss: 0.9667547345161438
Epoch 800, training loss: 0.6301950812339783 = 0.010166225954890251 + 0.1 * 6.20028829574585
Epoch 800, val loss: 0.9716278910636902
Epoch 810, training loss: 0.6291712522506714 = 0.009857933968305588 + 0.1 * 6.1931328773498535
Epoch 810, val loss: 0.9762576818466187
Epoch 820, training loss: 0.6285381317138672 = 0.009564794600009918 + 0.1 * 6.189733028411865
Epoch 820, val loss: 0.9809650778770447
Epoch 830, training loss: 0.6297124028205872 = 0.00928483810275793 + 0.1 * 6.204275131225586
Epoch 830, val loss: 0.9855602383613586
Epoch 840, training loss: 0.6280187964439392 = 0.00901786144822836 + 0.1 * 6.190009117126465
Epoch 840, val loss: 0.9900179505348206
Epoch 850, training loss: 0.626889169216156 = 0.008763551712036133 + 0.1 * 6.18125581741333
Epoch 850, val loss: 0.9944549798965454
Epoch 860, training loss: 0.6284527778625488 = 0.00852054450660944 + 0.1 * 6.19932222366333
Epoch 860, val loss: 0.9989128112792969
Epoch 870, training loss: 0.6262416243553162 = 0.008287684991955757 + 0.1 * 6.179539203643799
Epoch 870, val loss: 1.0030601024627686
Epoch 880, training loss: 0.6285477876663208 = 0.008065779693424702 + 0.1 * 6.204820156097412
Epoch 880, val loss: 1.0072543621063232
Epoch 890, training loss: 0.6258533000946045 = 0.007853461429476738 + 0.1 * 6.179998397827148
Epoch 890, val loss: 1.0113886594772339
Epoch 900, training loss: 0.6250133514404297 = 0.007650204934179783 + 0.1 * 6.17363166809082
Epoch 900, val loss: 1.0155534744262695
Epoch 910, training loss: 0.6247888207435608 = 0.007454895880073309 + 0.1 * 6.173339366912842
Epoch 910, val loss: 1.0194847583770752
Epoch 920, training loss: 0.624090313911438 = 0.007267671171575785 + 0.1 * 6.16822624206543
Epoch 920, val loss: 1.0233798027038574
Epoch 930, training loss: 0.6248738169670105 = 0.007087788078933954 + 0.1 * 6.177860260009766
Epoch 930, val loss: 1.0272903442382812
Epoch 940, training loss: 0.6232524514198303 = 0.006915018893778324 + 0.1 * 6.163373947143555
Epoch 940, val loss: 1.0311490297317505
Epoch 950, training loss: 0.6246039867401123 = 0.006748993415385485 + 0.1 * 6.178549766540527
Epoch 950, val loss: 1.0349358320236206
Epoch 960, training loss: 0.6224927306175232 = 0.0065893991850316525 + 0.1 * 6.159033298492432
Epoch 960, val loss: 1.0385195016860962
Epoch 970, training loss: 0.6231482028961182 = 0.00643653841689229 + 0.1 * 6.167116641998291
Epoch 970, val loss: 1.0422090291976929
Epoch 980, training loss: 0.6219210028648376 = 0.006289070937782526 + 0.1 * 6.1563191413879395
Epoch 980, val loss: 1.0458812713623047
Epoch 990, training loss: 0.6237832903862 = 0.006146801635622978 + 0.1 * 6.176364898681641
Epoch 990, val loss: 1.0493930578231812
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7934
Flip ASR: 0.7600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7830357551574707 = 1.9456626176834106 + 0.1 * 8.373730659484863
Epoch 0, val loss: 1.9429099559783936
Epoch 10, training loss: 2.7722673416137695 = 1.9349950551986694 + 0.1 * 8.372721672058105
Epoch 10, val loss: 1.9306484460830688
Epoch 20, training loss: 2.758505344390869 = 1.9217126369476318 + 0.1 * 8.367925643920898
Epoch 20, val loss: 1.9143853187561035
Epoch 30, training loss: 2.7378740310668945 = 1.902825951576233 + 0.1 * 8.350481033325195
Epoch 30, val loss: 1.8905025720596313
Epoch 40, training loss: 2.701829671859741 = 1.8766260147094727 + 0.1 * 8.252037048339844
Epoch 40, val loss: 1.8590548038482666
Epoch 50, training loss: 2.62282657623291 = 1.8455126285552979 + 0.1 * 7.773139953613281
Epoch 50, val loss: 1.8264018297195435
Epoch 60, training loss: 2.5479018688201904 = 1.8118574619293213 + 0.1 * 7.360443592071533
Epoch 60, val loss: 1.7948558330535889
Epoch 70, training loss: 2.482842445373535 = 1.7740967273712158 + 0.1 * 7.087456226348877
Epoch 70, val loss: 1.7615818977355957
Epoch 80, training loss: 2.4233462810516357 = 1.7322133779525757 + 0.1 * 6.911329746246338
Epoch 80, val loss: 1.7272026538848877
Epoch 90, training loss: 2.364664077758789 = 1.683696985244751 + 0.1 * 6.809670925140381
Epoch 90, val loss: 1.688642978668213
Epoch 100, training loss: 2.295356273651123 = 1.6200947761535645 + 0.1 * 6.752614974975586
Epoch 100, val loss: 1.6377842426300049
Epoch 110, training loss: 2.207627534866333 = 1.5364657640457153 + 0.1 * 6.711617946624756
Epoch 110, val loss: 1.571541666984558
Epoch 120, training loss: 2.1014444828033447 = 1.4342323541641235 + 0.1 * 6.672121524810791
Epoch 120, val loss: 1.4930472373962402
Epoch 130, training loss: 1.984825611114502 = 1.3213555812835693 + 0.1 * 6.634700775146484
Epoch 130, val loss: 1.4086823463439941
Epoch 140, training loss: 1.866926670074463 = 1.2064218521118164 + 0.1 * 6.605047225952148
Epoch 140, val loss: 1.3244225978851318
Epoch 150, training loss: 1.7548539638519287 = 1.0969033241271973 + 0.1 * 6.579505443572998
Epoch 150, val loss: 1.2452058792114258
Epoch 160, training loss: 1.6506471633911133 = 0.9938160181045532 + 0.1 * 6.568312168121338
Epoch 160, val loss: 1.1715880632400513
Epoch 170, training loss: 1.5556577444076538 = 0.9018460512161255 + 0.1 * 6.538116931915283
Epoch 170, val loss: 1.107258915901184
Epoch 180, training loss: 1.472806453704834 = 0.820836067199707 + 0.1 * 6.519704341888428
Epoch 180, val loss: 1.0516343116760254
Epoch 190, training loss: 1.4019286632537842 = 0.7505251169204712 + 0.1 * 6.514035701751709
Epoch 190, val loss: 1.0041146278381348
Epoch 200, training loss: 1.339354157447815 = 0.6907082796096802 + 0.1 * 6.486458778381348
Epoch 200, val loss: 0.9668464064598083
Epoch 210, training loss: 1.2872939109802246 = 0.6387534737586975 + 0.1 * 6.485404014587402
Epoch 210, val loss: 0.937321126461029
Epoch 220, training loss: 1.2404001951217651 = 0.5927875638008118 + 0.1 * 6.476126194000244
Epoch 220, val loss: 0.914820671081543
Epoch 230, training loss: 1.1965382099151611 = 0.5512353181838989 + 0.1 * 6.453029155731201
Epoch 230, val loss: 0.8975033164024353
Epoch 240, training loss: 1.1572444438934326 = 0.512798011302948 + 0.1 * 6.444464683532715
Epoch 240, val loss: 0.8838807940483093
Epoch 250, training loss: 1.1206098794937134 = 0.4769172966480255 + 0.1 * 6.436925888061523
Epoch 250, val loss: 0.872820258140564
Epoch 260, training loss: 1.0863494873046875 = 0.4433479607105255 + 0.1 * 6.430014610290527
Epoch 260, val loss: 0.8641185760498047
Epoch 270, training loss: 1.0536589622497559 = 0.41165322065353394 + 0.1 * 6.4200568199157715
Epoch 270, val loss: 0.8572296500205994
Epoch 280, training loss: 1.0254731178283691 = 0.38167789578437805 + 0.1 * 6.437952518463135
Epoch 280, val loss: 0.8522526621818542
Epoch 290, training loss: 0.9953895807266235 = 0.3536204397678375 + 0.1 * 6.417691707611084
Epoch 290, val loss: 0.8490270972251892
Epoch 300, training loss: 0.9676052927970886 = 0.32709765434265137 + 0.1 * 6.405076503753662
Epoch 300, val loss: 0.8474855422973633
Epoch 310, training loss: 0.9433577656745911 = 0.3020309805870056 + 0.1 * 6.413267612457275
Epoch 310, val loss: 0.8474884033203125
Epoch 320, training loss: 0.9179295301437378 = 0.2786461412906647 + 0.1 * 6.392834186553955
Epoch 320, val loss: 0.8489962220191956
Epoch 330, training loss: 0.895764946937561 = 0.2568081319332123 + 0.1 * 6.389568328857422
Epoch 330, val loss: 0.8519957065582275
Epoch 340, training loss: 0.8748469352722168 = 0.23650191724300385 + 0.1 * 6.383450031280518
Epoch 340, val loss: 0.8561730980873108
Epoch 350, training loss: 0.8551232814788818 = 0.2176901400089264 + 0.1 * 6.374330997467041
Epoch 350, val loss: 0.8610256314277649
Epoch 360, training loss: 0.8370535969734192 = 0.20021109282970428 + 0.1 * 6.368424892425537
Epoch 360, val loss: 0.8663756847381592
Epoch 370, training loss: 0.8210096955299377 = 0.18386675417423248 + 0.1 * 6.371429443359375
Epoch 370, val loss: 0.8718180060386658
Epoch 380, training loss: 0.8042468428611755 = 0.16836529970169067 + 0.1 * 6.3588151931762695
Epoch 380, val loss: 0.8766450881958008
Epoch 390, training loss: 0.7902128100395203 = 0.15332049131393433 + 0.1 * 6.368923187255859
Epoch 390, val loss: 0.8804233074188232
Epoch 400, training loss: 0.7727634906768799 = 0.13833771646022797 + 0.1 * 6.344257354736328
Epoch 400, val loss: 0.8837379217147827
Epoch 410, training loss: 0.757585346698761 = 0.12376890331506729 + 0.1 * 6.338164329528809
Epoch 410, val loss: 0.8885534405708313
Epoch 420, training loss: 0.7468420267105103 = 0.11128164827823639 + 0.1 * 6.3556036949157715
Epoch 420, val loss: 0.897878885269165
Epoch 430, training loss: 0.7343990206718445 = 0.10086215287446976 + 0.1 * 6.335368633270264
Epoch 430, val loss: 0.9109817743301392
Epoch 440, training loss: 0.72425776720047 = 0.09183858335018158 + 0.1 * 6.324191570281982
Epoch 440, val loss: 0.92621910572052
Epoch 450, training loss: 0.715569019317627 = 0.08382204920053482 + 0.1 * 6.317469596862793
Epoch 450, val loss: 0.9418133497238159
Epoch 460, training loss: 0.7095021605491638 = 0.07664427906274796 + 0.1 * 6.328578472137451
Epoch 460, val loss: 0.9575408697128296
Epoch 470, training loss: 0.7010707855224609 = 0.07022582739591599 + 0.1 * 6.3084492683410645
Epoch 470, val loss: 0.9733597636222839
Epoch 480, training loss: 0.6969442367553711 = 0.06445694714784622 + 0.1 * 6.324872970581055
Epoch 480, val loss: 0.9890832901000977
Epoch 490, training loss: 0.6889857649803162 = 0.05928848683834076 + 0.1 * 6.296972274780273
Epoch 490, val loss: 1.004651665687561
Epoch 500, training loss: 0.6845672130584717 = 0.05464057996869087 + 0.1 * 6.2992658615112305
Epoch 500, val loss: 1.0198851823806763
Epoch 510, training loss: 0.6795681118965149 = 0.050459124147892 + 0.1 * 6.2910895347595215
Epoch 510, val loss: 1.0349698066711426
Epoch 520, training loss: 0.6755149960517883 = 0.04668821394443512 + 0.1 * 6.288267612457275
Epoch 520, val loss: 1.049600601196289
Epoch 530, training loss: 0.6713181138038635 = 0.04328807443380356 + 0.1 * 6.280300617218018
Epoch 530, val loss: 1.0639444589614868
Epoch 540, training loss: 0.6684760451316833 = 0.04021601006388664 + 0.1 * 6.282599925994873
Epoch 540, val loss: 1.0778244733810425
Epoch 550, training loss: 0.6648123860359192 = 0.03743772953748703 + 0.1 * 6.273746490478516
Epoch 550, val loss: 1.09136164188385
Epoch 560, training loss: 0.664279043674469 = 0.03491891175508499 + 0.1 * 6.293601036071777
Epoch 560, val loss: 1.104448676109314
Epoch 570, training loss: 0.6594406366348267 = 0.03263753280043602 + 0.1 * 6.268030643463135
Epoch 570, val loss: 1.1171857118606567
Epoch 580, training loss: 0.6565431952476501 = 0.03056364506483078 + 0.1 * 6.259795188903809
Epoch 580, val loss: 1.12946355342865
Epoch 590, training loss: 0.6543075442314148 = 0.028676046058535576 + 0.1 * 6.256315231323242
Epoch 590, val loss: 1.1414074897766113
Epoch 600, training loss: 0.6515534520149231 = 0.026957141235470772 + 0.1 * 6.245963096618652
Epoch 600, val loss: 1.1529490947723389
Epoch 610, training loss: 0.6501448154449463 = 0.02538270875811577 + 0.1 * 6.247621059417725
Epoch 610, val loss: 1.1642229557037354
Epoch 620, training loss: 0.6502094864845276 = 0.02393939346075058 + 0.1 * 6.262701034545898
Epoch 620, val loss: 1.1751667261123657
Epoch 630, training loss: 0.6463035345077515 = 0.022618526592850685 + 0.1 * 6.236849784851074
Epoch 630, val loss: 1.1857430934906006
Epoch 640, training loss: 0.6470004320144653 = 0.021402768790721893 + 0.1 * 6.25597620010376
Epoch 640, val loss: 1.1961066722869873
Epoch 650, training loss: 0.6445962190628052 = 0.020285669714212418 + 0.1 * 6.243105411529541
Epoch 650, val loss: 1.2061067819595337
Epoch 660, training loss: 0.6427007913589478 = 0.019254926592111588 + 0.1 * 6.2344584465026855
Epoch 660, val loss: 1.2158386707305908
Epoch 670, training loss: 0.642524778842926 = 0.01830139011144638 + 0.1 * 6.242234230041504
Epoch 670, val loss: 1.225345253944397
Epoch 680, training loss: 0.6403465270996094 = 0.017420368269085884 + 0.1 * 6.229261875152588
Epoch 680, val loss: 1.2346404790878296
Epoch 690, training loss: 0.6390181183815002 = 0.016601907089352608 + 0.1 * 6.224161624908447
Epoch 690, val loss: 1.243613600730896
Epoch 700, training loss: 0.6379215121269226 = 0.01584126241505146 + 0.1 * 6.2208027839660645
Epoch 700, val loss: 1.2525060176849365
Epoch 710, training loss: 0.6377879977226257 = 0.015133311040699482 + 0.1 * 6.226546764373779
Epoch 710, val loss: 1.2610702514648438
Epoch 720, training loss: 0.6374439001083374 = 0.014475545845925808 + 0.1 * 6.229683876037598
Epoch 720, val loss: 1.2694520950317383
Epoch 730, training loss: 0.6354730725288391 = 0.013863351196050644 + 0.1 * 6.216097354888916
Epoch 730, val loss: 1.2775261402130127
Epoch 740, training loss: 0.6350200772285461 = 0.013291450217366219 + 0.1 * 6.217286109924316
Epoch 740, val loss: 1.2854235172271729
Epoch 750, training loss: 0.6333356499671936 = 0.01275547593832016 + 0.1 * 6.205801963806152
Epoch 750, val loss: 1.2931991815567017
Epoch 760, training loss: 0.6340063214302063 = 0.01225270889699459 + 0.1 * 6.217535972595215
Epoch 760, val loss: 1.3008105754852295
Epoch 770, training loss: 0.6330604553222656 = 0.011781701818108559 + 0.1 * 6.212787628173828
Epoch 770, val loss: 1.308279275894165
Epoch 780, training loss: 0.6316328644752502 = 0.011339276097714901 + 0.1 * 6.202936172485352
Epoch 780, val loss: 1.3155244588851929
Epoch 790, training loss: 0.6312962174415588 = 0.010922953486442566 + 0.1 * 6.203732490539551
Epoch 790, val loss: 1.3226594924926758
Epoch 800, training loss: 0.6315309405326843 = 0.01053127832710743 + 0.1 * 6.209996223449707
Epoch 800, val loss: 1.3296433687210083
Epoch 810, training loss: 0.6293029189109802 = 0.010162359103560448 + 0.1 * 6.191405296325684
Epoch 810, val loss: 1.3364311456680298
Epoch 820, training loss: 0.6294192671775818 = 0.009814008139073849 + 0.1 * 6.196052074432373
Epoch 820, val loss: 1.3430728912353516
Epoch 830, training loss: 0.6295932531356812 = 0.00948524009436369 + 0.1 * 6.201079845428467
Epoch 830, val loss: 1.3496003150939941
Epoch 840, training loss: 0.6285001039505005 = 0.009173846803605556 + 0.1 * 6.193262100219727
Epoch 840, val loss: 1.3560150861740112
Epoch 850, training loss: 0.6291182637214661 = 0.008879143744707108 + 0.1 * 6.202391147613525
Epoch 850, val loss: 1.362185001373291
Epoch 860, training loss: 0.6268812417984009 = 0.00860064197331667 + 0.1 * 6.182806015014648
Epoch 860, val loss: 1.368292212486267
Epoch 870, training loss: 0.6260940432548523 = 0.008335823193192482 + 0.1 * 6.177581787109375
Epoch 870, val loss: 1.3742833137512207
Epoch 880, training loss: 0.6267764568328857 = 0.008083810098469257 + 0.1 * 6.186926364898682
Epoch 880, val loss: 1.3801733255386353
Epoch 890, training loss: 0.6252162456512451 = 0.007844062522053719 + 0.1 * 6.1737213134765625
Epoch 890, val loss: 1.3859611749649048
Epoch 900, training loss: 0.6251307725906372 = 0.0076162624172866344 + 0.1 * 6.175144672393799
Epoch 900, val loss: 1.391619324684143
Epoch 910, training loss: 0.6256728768348694 = 0.007399027235805988 + 0.1 * 6.182738304138184
Epoch 910, val loss: 1.3971800804138184
Epoch 920, training loss: 0.6256993412971497 = 0.007192079909145832 + 0.1 * 6.185072422027588
Epoch 920, val loss: 1.4026135206222534
Epoch 930, training loss: 0.6243535280227661 = 0.006995331030339003 + 0.1 * 6.173582077026367
Epoch 930, val loss: 1.4079681634902954
Epoch 940, training loss: 0.6248373985290527 = 0.006807397585362196 + 0.1 * 6.180300235748291
Epoch 940, val loss: 1.4132081270217896
Epoch 950, training loss: 0.6243783831596375 = 0.006627371534705162 + 0.1 * 6.177509784698486
Epoch 950, val loss: 1.4183731079101562
Epoch 960, training loss: 0.6226600408554077 = 0.006455560680478811 + 0.1 * 6.162045001983643
Epoch 960, val loss: 1.4234288930892944
Epoch 970, training loss: 0.623161792755127 = 0.006290993187576532 + 0.1 * 6.168707847595215
Epoch 970, val loss: 1.428436040878296
Epoch 980, training loss: 0.6235643625259399 = 0.006133031100034714 + 0.1 * 6.174313545227051
Epoch 980, val loss: 1.4333373308181763
Epoch 990, training loss: 0.6219033598899841 = 0.005982104688882828 + 0.1 * 6.159212589263916
Epoch 990, val loss: 1.4381513595581055
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.8708
Flip ASR: 0.8444/225 nodes
The final ASR:0.75646, 0.11156, Accuracy:0.81111, 0.01684
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11548])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10480])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.97786, 0.01044, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7990283966064453 = 1.9616364240646362 + 0.1 * 8.373920440673828
Epoch 0, val loss: 1.9677505493164062
Epoch 10, training loss: 2.788270950317383 = 1.9508858919143677 + 0.1 * 8.373849868774414
Epoch 10, val loss: 1.957037091255188
Epoch 20, training loss: 2.775301456451416 = 1.9379591941833496 + 0.1 * 8.373422622680664
Epoch 20, val loss: 1.944143533706665
Epoch 30, training loss: 2.756748676300049 = 1.9197808504104614 + 0.1 * 8.369677543640137
Epoch 30, val loss: 1.925964593887329
Epoch 40, training loss: 2.726539134979248 = 1.892521619796753 + 0.1 * 8.340173721313477
Epoch 40, val loss: 1.8988138437271118
Epoch 50, training loss: 2.6709518432617188 = 1.8540873527526855 + 0.1 * 8.168645858764648
Epoch 50, val loss: 1.8621160984039307
Epoch 60, training loss: 2.5984959602355957 = 1.810908317565918 + 0.1 * 7.8758769035339355
Epoch 60, val loss: 1.824121117591858
Epoch 70, training loss: 2.52872896194458 = 1.7737054824829102 + 0.1 * 7.550235271453857
Epoch 70, val loss: 1.7934885025024414
Epoch 80, training loss: 2.447150230407715 = 1.739660620689392 + 0.1 * 7.074894905090332
Epoch 80, val loss: 1.763512372970581
Epoch 90, training loss: 2.3802249431610107 = 1.6961677074432373 + 0.1 * 6.840572357177734
Epoch 90, val loss: 1.7252482175827026
Epoch 100, training loss: 2.3126707077026367 = 1.6370604038238525 + 0.1 * 6.756102561950684
Epoch 100, val loss: 1.6763172149658203
Epoch 110, training loss: 2.2306346893310547 = 1.5605099201202393 + 0.1 * 6.701247215270996
Epoch 110, val loss: 1.6144014596939087
Epoch 120, training loss: 2.1372342109680176 = 1.470982313156128 + 0.1 * 6.662518501281738
Epoch 120, val loss: 1.542405366897583
Epoch 130, training loss: 2.0400335788726807 = 1.3763861656188965 + 0.1 * 6.636474132537842
Epoch 130, val loss: 1.4680907726287842
Epoch 140, training loss: 1.944472074508667 = 1.283886432647705 + 0.1 * 6.605855941772461
Epoch 140, val loss: 1.3959406614303589
Epoch 150, training loss: 1.8515137434005737 = 1.193310260772705 + 0.1 * 6.582034587860107
Epoch 150, val loss: 1.3259938955307007
Epoch 160, training loss: 1.7631645202636719 = 1.106810212135315 + 0.1 * 6.56354284286499
Epoch 160, val loss: 1.2590981721878052
Epoch 170, training loss: 1.680676817893982 = 1.025933861732483 + 0.1 * 6.54742956161499
Epoch 170, val loss: 1.1972417831420898
Epoch 180, training loss: 1.6060385704040527 = 0.9514464139938354 + 0.1 * 6.54592227935791
Epoch 180, val loss: 1.1410386562347412
Epoch 190, training loss: 1.5351685285568237 = 0.8827071785926819 + 0.1 * 6.524613380432129
Epoch 190, val loss: 1.0903379917144775
Epoch 200, training loss: 1.4683160781860352 = 0.8164057731628418 + 0.1 * 6.519102096557617
Epoch 200, val loss: 1.0422017574310303
Epoch 210, training loss: 1.402958869934082 = 0.7518306970596313 + 0.1 * 6.511281967163086
Epoch 210, val loss: 0.9963675141334534
Epoch 220, training loss: 1.3404150009155273 = 0.6893709897994995 + 0.1 * 6.510440349578857
Epoch 220, val loss: 0.9536194801330566
Epoch 230, training loss: 1.2806493043899536 = 0.6306544542312622 + 0.1 * 6.499948501586914
Epoch 230, val loss: 0.9154018759727478
Epoch 240, training loss: 1.2257575988769531 = 0.5762367248535156 + 0.1 * 6.495209217071533
Epoch 240, val loss: 0.8828439116477966
Epoch 250, training loss: 1.1753095388412476 = 0.5265658497810364 + 0.1 * 6.487436771392822
Epoch 250, val loss: 0.8565182685852051
Epoch 260, training loss: 1.1295140981674194 = 0.48153987526893616 + 0.1 * 6.47974157333374
Epoch 260, val loss: 0.8360887169837952
Epoch 270, training loss: 1.0885591506958008 = 0.44081610441207886 + 0.1 * 6.477430820465088
Epoch 270, val loss: 0.8210422992706299
Epoch 280, training loss: 1.0517370700836182 = 0.40418219566345215 + 0.1 * 6.475549221038818
Epoch 280, val loss: 0.8103662133216858
Epoch 290, training loss: 1.0173341035842896 = 0.3710583746433258 + 0.1 * 6.462757110595703
Epoch 290, val loss: 0.8032346963882446
Epoch 300, training loss: 0.9856314063072205 = 0.3406350016593933 + 0.1 * 6.4499640464782715
Epoch 300, val loss: 0.7988261580467224
Epoch 310, training loss: 0.9571999907493591 = 0.312441349029541 + 0.1 * 6.447586536407471
Epoch 310, val loss: 0.7963130474090576
Epoch 320, training loss: 0.930419921875 = 0.2861829698085785 + 0.1 * 6.4423699378967285
Epoch 320, val loss: 0.7953701019287109
Epoch 330, training loss: 0.9044708013534546 = 0.26144227385520935 + 0.1 * 6.430285453796387
Epoch 330, val loss: 0.7953507900238037
Epoch 340, training loss: 0.8806222677230835 = 0.2379230260848999 + 0.1 * 6.426992416381836
Epoch 340, val loss: 0.7962007522583008
Epoch 350, training loss: 0.8575471043586731 = 0.21560253202915192 + 0.1 * 6.419445991516113
Epoch 350, val loss: 0.7977769374847412
Epoch 360, training loss: 0.8357729911804199 = 0.19450335204601288 + 0.1 * 6.412696361541748
Epoch 360, val loss: 0.7997366786003113
Epoch 370, training loss: 0.8167165517807007 = 0.174684077501297 + 0.1 * 6.420324802398682
Epoch 370, val loss: 0.8021586537361145
Epoch 380, training loss: 0.7971526384353638 = 0.1564837396144867 + 0.1 * 6.406689167022705
Epoch 380, val loss: 0.805190920829773
Epoch 390, training loss: 0.7798038125038147 = 0.14003147184848785 + 0.1 * 6.39772367477417
Epoch 390, val loss: 0.8089026212692261
Epoch 400, training loss: 0.76491379737854 = 0.1253693848848343 + 0.1 * 6.395443916320801
Epoch 400, val loss: 0.8133145570755005
Epoch 410, training loss: 0.7512038946151733 = 0.1124582588672638 + 0.1 * 6.387455940246582
Epoch 410, val loss: 0.8185209631919861
Epoch 420, training loss: 0.7388821840286255 = 0.10114964097738266 + 0.1 * 6.377325534820557
Epoch 420, val loss: 0.8244083523750305
Epoch 430, training loss: 0.7310138940811157 = 0.09129350632429123 + 0.1 * 6.3972039222717285
Epoch 430, val loss: 0.8308328986167908
Epoch 440, training loss: 0.7202532291412354 = 0.08275303244590759 + 0.1 * 6.375001430511475
Epoch 440, val loss: 0.8377928733825684
Epoch 450, training loss: 0.7117143869400024 = 0.07530206441879272 + 0.1 * 6.364123344421387
Epoch 450, val loss: 0.8451825380325317
Epoch 460, training loss: 0.7043394446372986 = 0.06876815110445023 + 0.1 * 6.355712890625
Epoch 460, val loss: 0.8529199957847595
Epoch 470, training loss: 0.6983177661895752 = 0.06300830841064453 + 0.1 * 6.353094577789307
Epoch 470, val loss: 0.8610367178916931
Epoch 480, training loss: 0.6933082938194275 = 0.057904358953237534 + 0.1 * 6.354039192199707
Epoch 480, val loss: 0.8693318963050842
Epoch 490, training loss: 0.6879661083221436 = 0.05337808281183243 + 0.1 * 6.345880031585693
Epoch 490, val loss: 0.877720057964325
Epoch 500, training loss: 0.6828482151031494 = 0.049337223172187805 + 0.1 * 6.335110187530518
Epoch 500, val loss: 0.886206865310669
Epoch 510, training loss: 0.6804178357124329 = 0.045719344168901443 + 0.1 * 6.34698486328125
Epoch 510, val loss: 0.894744336605072
Epoch 520, training loss: 0.674660325050354 = 0.042477693408727646 + 0.1 * 6.321826457977295
Epoch 520, val loss: 0.9032129645347595
Epoch 530, training loss: 0.6713469624519348 = 0.039556555449962616 + 0.1 * 6.317903995513916
Epoch 530, val loss: 0.9117842316627502
Epoch 540, training loss: 0.6679554581642151 = 0.0369088388979435 + 0.1 * 6.3104658126831055
Epoch 540, val loss: 0.9202543497085571
Epoch 550, training loss: 0.6654964685440063 = 0.0345059372484684 + 0.1 * 6.309905052185059
Epoch 550, val loss: 0.9285058975219727
Epoch 560, training loss: 0.6635077595710754 = 0.03232730180025101 + 0.1 * 6.31180477142334
Epoch 560, val loss: 0.9368506669998169
Epoch 570, training loss: 0.6607540249824524 = 0.03034362941980362 + 0.1 * 6.304103374481201
Epoch 570, val loss: 0.9449623227119446
Epoch 580, training loss: 0.6584469079971313 = 0.028532076627016068 + 0.1 * 6.299148082733154
Epoch 580, val loss: 0.9529249668121338
Epoch 590, training loss: 0.656707227230072 = 0.026870742440223694 + 0.1 * 6.298364639282227
Epoch 590, val loss: 0.9607523679733276
Epoch 600, training loss: 0.6546037197113037 = 0.025345806032419205 + 0.1 * 6.292579174041748
Epoch 600, val loss: 0.9684963822364807
Epoch 610, training loss: 0.6534226536750793 = 0.023941729217767715 + 0.1 * 6.294808864593506
Epoch 610, val loss: 0.9760985374450684
Epoch 620, training loss: 0.6525284647941589 = 0.02265007048845291 + 0.1 * 6.298783779144287
Epoch 620, val loss: 0.9835395812988281
Epoch 630, training loss: 0.6496954560279846 = 0.02146037295460701 + 0.1 * 6.282351016998291
Epoch 630, val loss: 0.9907497763633728
Epoch 640, training loss: 0.6483809351921082 = 0.02036292478442192 + 0.1 * 6.280179977416992
Epoch 640, val loss: 0.9979014992713928
Epoch 650, training loss: 0.6485708951950073 = 0.01934674382209778 + 0.1 * 6.292241096496582
Epoch 650, val loss: 1.0047528743743896
Epoch 660, training loss: 0.6458417177200317 = 0.01840817928314209 + 0.1 * 6.2743353843688965
Epoch 660, val loss: 1.0115597248077393
Epoch 670, training loss: 0.6453951001167297 = 0.017534460872411728 + 0.1 * 6.278606414794922
Epoch 670, val loss: 1.0182054042816162
Epoch 680, training loss: 0.6432734131813049 = 0.016722725704312325 + 0.1 * 6.265506744384766
Epoch 680, val loss: 1.0246350765228271
Epoch 690, training loss: 0.6428563594818115 = 0.015967875719070435 + 0.1 * 6.268884181976318
Epoch 690, val loss: 1.030996322631836
Epoch 700, training loss: 0.6411281824111938 = 0.015263088978827 + 0.1 * 6.258650779724121
Epoch 700, val loss: 1.0371742248535156
Epoch 710, training loss: 0.641420841217041 = 0.014605027623474598 + 0.1 * 6.268157958984375
Epoch 710, val loss: 1.0432385206222534
Epoch 720, training loss: 0.639899492263794 = 0.013989455997943878 + 0.1 * 6.259100437164307
Epoch 720, val loss: 1.0491161346435547
Epoch 730, training loss: 0.6399203538894653 = 0.013413632288575172 + 0.1 * 6.265067100524902
Epoch 730, val loss: 1.0548919439315796
Epoch 740, training loss: 0.637994647026062 = 0.012873743660748005 + 0.1 * 6.251208782196045
Epoch 740, val loss: 1.0605770349502563
Epoch 750, training loss: 0.6377291679382324 = 0.012366781942546368 + 0.1 * 6.253623962402344
Epoch 750, val loss: 1.0661695003509521
Epoch 760, training loss: 0.6360960006713867 = 0.011888811364769936 + 0.1 * 6.242071628570557
Epoch 760, val loss: 1.0715371370315552
Epoch 770, training loss: 0.6382383704185486 = 0.011440248228609562 + 0.1 * 6.267981052398682
Epoch 770, val loss: 1.0768483877182007
Epoch 780, training loss: 0.635642409324646 = 0.011017292737960815 + 0.1 * 6.246251583099365
Epoch 780, val loss: 1.0819851160049438
Epoch 790, training loss: 0.6341708302497864 = 0.010619456879794598 + 0.1 * 6.235513687133789
Epoch 790, val loss: 1.0871458053588867
Epoch 800, training loss: 0.6352514624595642 = 0.010241632349789143 + 0.1 * 6.25009822845459
Epoch 800, val loss: 1.0921969413757324
Epoch 810, training loss: 0.6335158348083496 = 0.009886346757411957 + 0.1 * 6.236294746398926
Epoch 810, val loss: 1.0969486236572266
Epoch 820, training loss: 0.6329817771911621 = 0.009550957940518856 + 0.1 * 6.234308242797852
Epoch 820, val loss: 1.1017647981643677
Epoch 830, training loss: 0.6330896615982056 = 0.009232545271515846 + 0.1 * 6.2385711669921875
Epoch 830, val loss: 1.1065019369125366
Epoch 840, training loss: 0.6318953633308411 = 0.008930759504437447 + 0.1 * 6.2296462059021
Epoch 840, val loss: 1.1110081672668457
Epoch 850, training loss: 0.6310064792633057 = 0.008645582012832165 + 0.1 * 6.223608493804932
Epoch 850, val loss: 1.1155565977096558
Epoch 860, training loss: 0.6312916278839111 = 0.008374272845685482 + 0.1 * 6.229173183441162
Epoch 860, val loss: 1.1200660467147827
Epoch 870, training loss: 0.6313084363937378 = 0.008115631528198719 + 0.1 * 6.231927871704102
Epoch 870, val loss: 1.1242142915725708
Epoch 880, training loss: 0.6298439502716064 = 0.007871522568166256 + 0.1 * 6.219724178314209
Epoch 880, val loss: 1.1284767389297485
Epoch 890, training loss: 0.6310948133468628 = 0.007638887502253056 + 0.1 * 6.234559059143066
Epoch 890, val loss: 1.1325069665908813
Epoch 900, training loss: 0.6295672655105591 = 0.007418068125844002 + 0.1 * 6.221491813659668
Epoch 900, val loss: 1.1365951299667358
Epoch 910, training loss: 0.6284233927726746 = 0.007207821123301983 + 0.1 * 6.212155818939209
Epoch 910, val loss: 1.1406534910202026
Epoch 920, training loss: 0.6296543478965759 = 0.0070061953738331795 + 0.1 * 6.2264814376831055
Epoch 920, val loss: 1.1446970701217651
Epoch 930, training loss: 0.6291865706443787 = 0.006813196465373039 + 0.1 * 6.223733901977539
Epoch 930, val loss: 1.1484034061431885
Epoch 940, training loss: 0.627600908279419 = 0.006630131974816322 + 0.1 * 6.209707736968994
Epoch 940, val loss: 1.1522092819213867
Epoch 950, training loss: 0.6272292137145996 = 0.006454494781792164 + 0.1 * 6.207747459411621
Epoch 950, val loss: 1.155964970588684
Epoch 960, training loss: 0.6273696422576904 = 0.006286233197897673 + 0.1 * 6.210834503173828
Epoch 960, val loss: 1.159701943397522
Epoch 970, training loss: 0.629444420337677 = 0.00612531928345561 + 0.1 * 6.233190536499023
Epoch 970, val loss: 1.1632399559020996
Epoch 980, training loss: 0.6263381838798523 = 0.00597142381593585 + 0.1 * 6.203667640686035
Epoch 980, val loss: 1.1666488647460938
Epoch 990, training loss: 0.6257230639457703 = 0.00582477729767561 + 0.1 * 6.1989827156066895
Epoch 990, val loss: 1.170228123664856
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7883403301239014 = 1.9509601593017578 + 0.1 * 8.373802185058594
Epoch 0, val loss: 1.953413724899292
Epoch 10, training loss: 2.7782702445983887 = 1.9409139156341553 + 0.1 * 8.373563766479492
Epoch 10, val loss: 1.9440562725067139
Epoch 20, training loss: 2.7655234336853027 = 1.9283071756362915 + 0.1 * 8.372163772583008
Epoch 20, val loss: 1.9318583011627197
Epoch 30, training loss: 2.746246337890625 = 1.9101064205169678 + 0.1 * 8.36139965057373
Epoch 30, val loss: 1.9139951467514038
Epoch 40, training loss: 2.7103123664855957 = 1.8824782371520996 + 0.1 * 8.278340339660645
Epoch 40, val loss: 1.8872557878494263
Epoch 50, training loss: 2.625373125076294 = 1.845849871635437 + 0.1 * 7.795232772827148
Epoch 50, val loss: 1.8538250923156738
Epoch 60, training loss: 2.541604518890381 = 1.8091689348220825 + 0.1 * 7.324356555938721
Epoch 60, val loss: 1.8219811916351318
Epoch 70, training loss: 2.4656145572662354 = 1.7747578620910645 + 0.1 * 6.908566474914551
Epoch 70, val loss: 1.7937803268432617
Epoch 80, training loss: 2.4170291423797607 = 1.7414823770523071 + 0.1 * 6.755466938018799
Epoch 80, val loss: 1.765927791595459
Epoch 90, training loss: 2.3658130168914795 = 1.6981815099716187 + 0.1 * 6.6763153076171875
Epoch 90, val loss: 1.7267359495162964
Epoch 100, training loss: 2.301469326019287 = 1.639176368713379 + 0.1 * 6.622930526733398
Epoch 100, val loss: 1.6765499114990234
Epoch 110, training loss: 2.221522092819214 = 1.563055157661438 + 0.1 * 6.58466911315918
Epoch 110, val loss: 1.615253210067749
Epoch 120, training loss: 2.1302924156188965 = 1.4742499589920044 + 0.1 * 6.5604248046875
Epoch 120, val loss: 1.5437077283859253
Epoch 130, training loss: 2.0361459255218506 = 1.382094383239746 + 0.1 * 6.540514945983887
Epoch 130, val loss: 1.4701951742172241
Epoch 140, training loss: 1.9451452493667603 = 1.2927863597869873 + 0.1 * 6.52358865737915
Epoch 140, val loss: 1.399733304977417
Epoch 150, training loss: 1.8590278625488281 = 1.2084530591964722 + 0.1 * 6.505748271942139
Epoch 150, val loss: 1.3350776433944702
Epoch 160, training loss: 1.776681900024414 = 1.1280337572097778 + 0.1 * 6.486481189727783
Epoch 160, val loss: 1.2746723890304565
Epoch 170, training loss: 1.7001311779022217 = 1.0519022941589355 + 0.1 * 6.4822893142700195
Epoch 170, val loss: 1.2187093496322632
Epoch 180, training loss: 1.6271860599517822 = 0.9809300899505615 + 0.1 * 6.462559223175049
Epoch 180, val loss: 1.1680516004562378
Epoch 190, training loss: 1.5573772192001343 = 0.9127330183982849 + 0.1 * 6.446441650390625
Epoch 190, val loss: 1.119041919708252
Epoch 200, training loss: 1.4912667274475098 = 0.8461029529571533 + 0.1 * 6.451636791229248
Epoch 200, val loss: 1.0706493854522705
Epoch 210, training loss: 1.424454689025879 = 0.7815214991569519 + 0.1 * 6.429332256317139
Epoch 210, val loss: 1.0234938859939575
Epoch 220, training loss: 1.3600023984909058 = 0.7177228331565857 + 0.1 * 6.42279577255249
Epoch 220, val loss: 0.9761354923248291
Epoch 230, training loss: 1.2973134517669678 = 0.6553860902786255 + 0.1 * 6.4192728996276855
Epoch 230, val loss: 0.9297007918357849
Epoch 240, training loss: 1.2372050285339355 = 0.5967469811439514 + 0.1 * 6.404581069946289
Epoch 240, val loss: 0.8866602182388306
Epoch 250, training loss: 1.1826398372650146 = 0.542826235294342 + 0.1 * 6.398136138916016
Epoch 250, val loss: 0.8481336236000061
Epoch 260, training loss: 1.1359825134277344 = 0.493947297334671 + 0.1 * 6.420351982116699
Epoch 260, val loss: 0.8147267699241638
Epoch 270, training loss: 1.0894594192504883 = 0.45033401250839233 + 0.1 * 6.39125394821167
Epoch 270, val loss: 0.7866740822792053
Epoch 280, training loss: 1.0478256940841675 = 0.4105186462402344 + 0.1 * 6.373070240020752
Epoch 280, val loss: 0.7625482678413391
Epoch 290, training loss: 1.0118376016616821 = 0.37342754006385803 + 0.1 * 6.384100437164307
Epoch 290, val loss: 0.7417001724243164
Epoch 300, training loss: 0.9758453369140625 = 0.33915528655052185 + 0.1 * 6.36690092086792
Epoch 300, val loss: 0.7239256501197815
Epoch 310, training loss: 0.9430165886878967 = 0.30746138095855713 + 0.1 * 6.3555521965026855
Epoch 310, val loss: 0.7089521884918213
Epoch 320, training loss: 0.9147834777832031 = 0.2782743275165558 + 0.1 * 6.365090847015381
Epoch 320, val loss: 0.6967127919197083
Epoch 330, training loss: 0.8865806460380554 = 0.2518184185028076 + 0.1 * 6.347621917724609
Epoch 330, val loss: 0.6873447895050049
Epoch 340, training loss: 0.8612191677093506 = 0.22767946124076843 + 0.1 * 6.335397243499756
Epoch 340, val loss: 0.6806674599647522
Epoch 350, training loss: 0.8417279720306396 = 0.20548135042190552 + 0.1 * 6.362466335296631
Epoch 350, val loss: 0.6763659715652466
Epoch 360, training loss: 0.8188841938972473 = 0.1852671504020691 + 0.1 * 6.336170196533203
Epoch 360, val loss: 0.6742324233055115
Epoch 370, training loss: 0.7983412146568298 = 0.16674429178237915 + 0.1 * 6.315968990325928
Epoch 370, val loss: 0.6740480065345764
Epoch 380, training loss: 0.7809360027313232 = 0.1497880220413208 + 0.1 * 6.311479568481445
Epoch 380, val loss: 0.6755375862121582
Epoch 390, training loss: 0.7653076648712158 = 0.13443930447101593 + 0.1 * 6.3086838722229
Epoch 390, val loss: 0.6785828471183777
Epoch 400, training loss: 0.7508370280265808 = 0.12077023088932037 + 0.1 * 6.300668239593506
Epoch 400, val loss: 0.6828385591506958
Epoch 410, training loss: 0.7397448420524597 = 0.10865889489650726 + 0.1 * 6.310859203338623
Epoch 410, val loss: 0.688171923160553
Epoch 420, training loss: 0.7279276847839355 = 0.09804067015647888 + 0.1 * 6.298869609832764
Epoch 420, val loss: 0.694317638874054
Epoch 430, training loss: 0.7174156308174133 = 0.08874117583036423 + 0.1 * 6.286744594573975
Epoch 430, val loss: 0.701216995716095
Epoch 440, training loss: 0.7085456252098083 = 0.08057083934545517 + 0.1 * 6.27974796295166
Epoch 440, val loss: 0.7086529731750488
Epoch 450, training loss: 0.7017263770103455 = 0.0733930841088295 + 0.1 * 6.283332347869873
Epoch 450, val loss: 0.7164501547813416
Epoch 460, training loss: 0.6955615282058716 = 0.06708312779664993 + 0.1 * 6.284783840179443
Epoch 460, val loss: 0.7244263887405396
Epoch 470, training loss: 0.6894399523735046 = 0.06152556091547012 + 0.1 * 6.279143333435059
Epoch 470, val loss: 0.7324786186218262
Epoch 480, training loss: 0.6835659146308899 = 0.056611716747283936 + 0.1 * 6.2695417404174805
Epoch 480, val loss: 0.7405256628990173
Epoch 490, training loss: 0.6778852343559265 = 0.052247483283281326 + 0.1 * 6.256377696990967
Epoch 490, val loss: 0.748551070690155
Epoch 500, training loss: 0.6744147539138794 = 0.04834593087434769 + 0.1 * 6.260688304901123
Epoch 500, val loss: 0.7565044164657593
Epoch 510, training loss: 0.6697712540626526 = 0.04485258832573891 + 0.1 * 6.249186038970947
Epoch 510, val loss: 0.7643570303916931
Epoch 520, training loss: 0.6668592691421509 = 0.04171689972281456 + 0.1 * 6.2514238357543945
Epoch 520, val loss: 0.7720892429351807
Epoch 530, training loss: 0.6629206538200378 = 0.03889570012688637 + 0.1 * 6.240249156951904
Epoch 530, val loss: 0.7796614766120911
Epoch 540, training loss: 0.6601265072822571 = 0.03634382039308548 + 0.1 * 6.237826824188232
Epoch 540, val loss: 0.7871147394180298
Epoch 550, training loss: 0.65916508436203 = 0.03402970731258392 + 0.1 * 6.251353740692139
Epoch 550, val loss: 0.7944065928459167
Epoch 560, training loss: 0.6556341648101807 = 0.03193647786974907 + 0.1 * 6.236976623535156
Epoch 560, val loss: 0.8015375137329102
Epoch 570, training loss: 0.6530042886734009 = 0.030027057975530624 + 0.1 * 6.229772567749023
Epoch 570, val loss: 0.8085463643074036
Epoch 580, training loss: 0.6511313319206238 = 0.028284551575779915 + 0.1 * 6.2284674644470215
Epoch 580, val loss: 0.8153384327888489
Epoch 590, training loss: 0.649262011051178 = 0.0266961008310318 + 0.1 * 6.225658893585205
Epoch 590, val loss: 0.8220661282539368
Epoch 600, training loss: 0.648039698600769 = 0.025237472727894783 + 0.1 * 6.228021621704102
Epoch 600, val loss: 0.8286635279655457
Epoch 610, training loss: 0.645468533039093 = 0.02389700524508953 + 0.1 * 6.215715408325195
Epoch 610, val loss: 0.835101842880249
Epoch 620, training loss: 0.6448599696159363 = 0.022661449387669563 + 0.1 * 6.221985340118408
Epoch 620, val loss: 0.8414226174354553
Epoch 630, training loss: 0.6436038613319397 = 0.02152087725698948 + 0.1 * 6.220829963684082
Epoch 630, val loss: 0.8476183414459229
Epoch 640, training loss: 0.6416561007499695 = 0.020466692745685577 + 0.1 * 6.2118940353393555
Epoch 640, val loss: 0.8536770939826965
Epoch 650, training loss: 0.6401534676551819 = 0.019491542130708694 + 0.1 * 6.2066192626953125
Epoch 650, val loss: 0.8596230149269104
Epoch 660, training loss: 0.639403760433197 = 0.0185837559401989 + 0.1 * 6.208199977874756
Epoch 660, val loss: 0.8654478192329407
Epoch 670, training loss: 0.6394639015197754 = 0.017741261050105095 + 0.1 * 6.217226505279541
Epoch 670, val loss: 0.8711275458335876
Epoch 680, training loss: 0.6367111206054688 = 0.016961829736828804 + 0.1 * 6.197493076324463
Epoch 680, val loss: 0.8766568303108215
Epoch 690, training loss: 0.6362648010253906 = 0.01623404584825039 + 0.1 * 6.200307369232178
Epoch 690, val loss: 0.8821074366569519
Epoch 700, training loss: 0.634940505027771 = 0.015551291406154633 + 0.1 * 6.193892002105713
Epoch 700, val loss: 0.8874440789222717
Epoch 710, training loss: 0.6337083578109741 = 0.014911838807165623 + 0.1 * 6.187964916229248
Epoch 710, val loss: 0.8927181959152222
Epoch 720, training loss: 0.634758710861206 = 0.014307092875242233 + 0.1 * 6.204516410827637
Epoch 720, val loss: 0.8979084491729736
Epoch 730, training loss: 0.6322430372238159 = 0.013748057186603546 + 0.1 * 6.18494987487793
Epoch 730, val loss: 0.9030373096466064
Epoch 740, training loss: 0.6326172351837158 = 0.01321484800428152 + 0.1 * 6.194024085998535
Epoch 740, val loss: 0.9079468846321106
Epoch 750, training loss: 0.6310414671897888 = 0.012716497294604778 + 0.1 * 6.1832499504089355
Epoch 750, val loss: 0.9128099083900452
Epoch 760, training loss: 0.6307345628738403 = 0.012249577790498734 + 0.1 * 6.184849739074707
Epoch 760, val loss: 0.9175912141799927
Epoch 770, training loss: 0.6294755935668945 = 0.011808935552835464 + 0.1 * 6.176666259765625
Epoch 770, val loss: 0.9222719669342041
Epoch 780, training loss: 0.6309027671813965 = 0.011391513980925083 + 0.1 * 6.195112228393555
Epoch 780, val loss: 0.9268827438354492
Epoch 790, training loss: 0.6286532282829285 = 0.010997611097991467 + 0.1 * 6.17655611038208
Epoch 790, val loss: 0.9314415454864502
Epoch 800, training loss: 0.6283528208732605 = 0.010625189170241356 + 0.1 * 6.177276134490967
Epoch 800, val loss: 0.935920774936676
Epoch 810, training loss: 0.6273176074028015 = 0.010271542705595493 + 0.1 * 6.1704607009887695
Epoch 810, val loss: 0.9403746128082275
Epoch 820, training loss: 0.6277828216552734 = 0.009937616065144539 + 0.1 * 6.1784515380859375
Epoch 820, val loss: 0.9446973204612732
Epoch 830, training loss: 0.6263059973716736 = 0.009621399454772472 + 0.1 * 6.166845798492432
Epoch 830, val loss: 0.9489273428916931
Epoch 840, training loss: 0.6266971230506897 = 0.009320713579654694 + 0.1 * 6.173763751983643
Epoch 840, val loss: 0.9531378746032715
Epoch 850, training loss: 0.6257361769676208 = 0.009034852497279644 + 0.1 * 6.167013168334961
Epoch 850, val loss: 0.9571979641914368
Epoch 860, training loss: 0.6257707476615906 = 0.008763155899941921 + 0.1 * 6.1700758934021
Epoch 860, val loss: 0.9612637162208557
Epoch 870, training loss: 0.6240615248680115 = 0.008504727855324745 + 0.1 * 6.155567646026611
Epoch 870, val loss: 0.965232789516449
Epoch 880, training loss: 0.6248487830162048 = 0.008258410729467869 + 0.1 * 6.165903568267822
Epoch 880, val loss: 0.9691582322120667
Epoch 890, training loss: 0.6251393556594849 = 0.008022751659154892 + 0.1 * 6.17116641998291
Epoch 890, val loss: 0.9729817509651184
Epoch 900, training loss: 0.6240965723991394 = 0.007798743434250355 + 0.1 * 6.162978172302246
Epoch 900, val loss: 0.9768016934394836
Epoch 910, training loss: 0.6228724718093872 = 0.007585492450743914 + 0.1 * 6.152869701385498
Epoch 910, val loss: 0.9805231094360352
Epoch 920, training loss: 0.622982919216156 = 0.007381608709692955 + 0.1 * 6.156013011932373
Epoch 920, val loss: 0.9842013716697693
Epoch 930, training loss: 0.6237846612930298 = 0.007186120841652155 + 0.1 * 6.165985584259033
Epoch 930, val loss: 0.9878330826759338
Epoch 940, training loss: 0.6229469180107117 = 0.006999478675425053 + 0.1 * 6.159473896026611
Epoch 940, val loss: 0.9914335012435913
Epoch 950, training loss: 0.6214750409126282 = 0.006821185816079378 + 0.1 * 6.146538257598877
Epoch 950, val loss: 0.9949642419815063
Epoch 960, training loss: 0.621107280254364 = 0.00665058009326458 + 0.1 * 6.144567012786865
Epoch 960, val loss: 0.9984571933746338
Epoch 970, training loss: 0.620937168598175 = 0.006486216560006142 + 0.1 * 6.144509315490723
Epoch 970, val loss: 1.0019174814224243
Epoch 980, training loss: 0.6207919716835022 = 0.006328053306788206 + 0.1 * 6.144639015197754
Epoch 980, val loss: 1.005320429801941
Epoch 990, training loss: 0.6206768751144409 = 0.0061768097802996635 + 0.1 * 6.145000457763672
Epoch 990, val loss: 1.0087205171585083
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7935128211975098 = 1.9561353921890259 + 0.1 * 8.373773574829102
Epoch 0, val loss: 1.9589619636535645
Epoch 10, training loss: 2.7825465202331543 = 1.9451922178268433 + 0.1 * 8.373543739318848
Epoch 10, val loss: 1.9493186473846436
Epoch 20, training loss: 2.7688651084899902 = 1.931634783744812 + 0.1 * 8.37230396270752
Epoch 20, val loss: 1.9369784593582153
Epoch 30, training loss: 2.7489073276519775 = 1.9125514030456543 + 0.1 * 8.36355972290039
Epoch 30, val loss: 1.919234037399292
Epoch 40, training loss: 2.715451240539551 = 1.884161353111267 + 0.1 * 8.312897682189941
Epoch 40, val loss: 1.8929059505462646
Epoch 50, training loss: 2.6477785110473633 = 1.8460185527801514 + 0.1 * 8.017599105834961
Epoch 50, val loss: 1.8586161136627197
Epoch 60, training loss: 2.563870668411255 = 1.8027952909469604 + 0.1 * 7.610754013061523
Epoch 60, val loss: 1.820878505706787
Epoch 70, training loss: 2.495464324951172 = 1.7602816820144653 + 0.1 * 7.351825714111328
Epoch 70, val loss: 1.7843658924102783
Epoch 80, training loss: 2.4263882637023926 = 1.7180213928222656 + 0.1 * 7.083669662475586
Epoch 80, val loss: 1.7473664283752441
Epoch 90, training loss: 2.3526251316070557 = 1.6647658348083496 + 0.1 * 6.878592491149902
Epoch 90, val loss: 1.6990394592285156
Epoch 100, training loss: 2.2735862731933594 = 1.5965311527252197 + 0.1 * 6.7705512046813965
Epoch 100, val loss: 1.6400164365768433
Epoch 110, training loss: 2.188990354537964 = 1.5178242921829224 + 0.1 * 6.711659908294678
Epoch 110, val loss: 1.5752114057540894
Epoch 120, training loss: 2.106584072113037 = 1.439501166343689 + 0.1 * 6.670830249786377
Epoch 120, val loss: 1.5113736391067505
Epoch 130, training loss: 2.031010627746582 = 1.3663289546966553 + 0.1 * 6.646815299987793
Epoch 130, val loss: 1.453795075416565
Epoch 140, training loss: 1.9590973854064941 = 1.296475887298584 + 0.1 * 6.62621545791626
Epoch 140, val loss: 1.3998653888702393
Epoch 150, training loss: 1.8875198364257812 = 1.2268728017807007 + 0.1 * 6.606470108032227
Epoch 150, val loss: 1.3466094732284546
Epoch 160, training loss: 1.815264344215393 = 1.1559654474258423 + 0.1 * 6.592988967895508
Epoch 160, val loss: 1.2933375835418701
Epoch 170, training loss: 1.7411243915557861 = 1.0833821296691895 + 0.1 * 6.577422618865967
Epoch 170, val loss: 1.2409045696258545
Epoch 180, training loss: 1.6638355255126953 = 1.0071226358413696 + 0.1 * 6.567129611968994
Epoch 180, val loss: 1.1862590312957764
Epoch 190, training loss: 1.5831868648529053 = 0.9272713661193848 + 0.1 * 6.559154987335205
Epoch 190, val loss: 1.1289821863174438
Epoch 200, training loss: 1.501906394958496 = 0.8467742204666138 + 0.1 * 6.551321983337402
Epoch 200, val loss: 1.071155071258545
Epoch 210, training loss: 1.4230031967163086 = 0.7688033580780029 + 0.1 * 6.541998386383057
Epoch 210, val loss: 1.015313982963562
Epoch 220, training loss: 1.3494625091552734 = 0.69599848985672 + 0.1 * 6.534640312194824
Epoch 220, val loss: 0.9636751413345337
Epoch 230, training loss: 1.2824947834014893 = 0.6302199959754944 + 0.1 * 6.5227484703063965
Epoch 230, val loss: 0.9178652763366699
Epoch 240, training loss: 1.2222168445587158 = 0.5713033676147461 + 0.1 * 6.509134292602539
Epoch 240, val loss: 0.8777246475219727
Epoch 250, training loss: 1.1686999797821045 = 0.5187035202980042 + 0.1 * 6.499963760375977
Epoch 250, val loss: 0.8432467579841614
Epoch 260, training loss: 1.1207246780395508 = 0.4720143675804138 + 0.1 * 6.487102508544922
Epoch 260, val loss: 0.8145626187324524
Epoch 270, training loss: 1.0771775245666504 = 0.429556667804718 + 0.1 * 6.476208686828613
Epoch 270, val loss: 0.7904810309410095
Epoch 280, training loss: 1.037546157836914 = 0.3903542757034302 + 0.1 * 6.47191858291626
Epoch 280, val loss: 0.770755410194397
Epoch 290, training loss: 0.9997790455818176 = 0.3538053631782532 + 0.1 * 6.4597368240356445
Epoch 290, val loss: 0.7551527619361877
Epoch 300, training loss: 0.9642613530158997 = 0.319244384765625 + 0.1 * 6.450169563293457
Epoch 300, val loss: 0.742851734161377
Epoch 310, training loss: 0.9314383268356323 = 0.2866658866405487 + 0.1 * 6.44772481918335
Epoch 310, val loss: 0.733428418636322
Epoch 320, training loss: 0.900814414024353 = 0.2564479410648346 + 0.1 * 6.44366455078125
Epoch 320, val loss: 0.7269565463066101
Epoch 330, training loss: 0.8716015815734863 = 0.22880050539970398 + 0.1 * 6.428010940551758
Epoch 330, val loss: 0.7229366302490234
Epoch 340, training loss: 0.8467921614646912 = 0.20377232134342194 + 0.1 * 6.4301981925964355
Epoch 340, val loss: 0.7212432026863098
Epoch 350, training loss: 0.8231720328330994 = 0.18156151473522186 + 0.1 * 6.416104793548584
Epoch 350, val loss: 0.7217797040939331
Epoch 360, training loss: 0.8030358552932739 = 0.1619834452867508 + 0.1 * 6.410523891448975
Epoch 360, val loss: 0.7241130471229553
Epoch 370, training loss: 0.7853615880012512 = 0.14481449127197266 + 0.1 * 6.405470848083496
Epoch 370, val loss: 0.7280879616737366
Epoch 380, training loss: 0.7698855400085449 = 0.12983396649360657 + 0.1 * 6.400515556335449
Epoch 380, val loss: 0.7335975170135498
Epoch 390, training loss: 0.75840163230896 = 0.11670471727848053 + 0.1 * 6.416969299316406
Epoch 390, val loss: 0.7402210831642151
Epoch 400, training loss: 0.7447331547737122 = 0.10527350008487701 + 0.1 * 6.394596576690674
Epoch 400, val loss: 0.7479493618011475
Epoch 410, training loss: 0.733636736869812 = 0.09524113684892654 + 0.1 * 6.383955955505371
Epoch 410, val loss: 0.7563414573669434
Epoch 420, training loss: 0.7238852381706238 = 0.08640432357788086 + 0.1 * 6.3748087882995605
Epoch 420, val loss: 0.7654189467430115
Epoch 430, training loss: 0.7184861302375793 = 0.07861165702342987 + 0.1 * 6.398744583129883
Epoch 430, val loss: 0.7749467492103577
Epoch 440, training loss: 0.7087079882621765 = 0.07178781926631927 + 0.1 * 6.36920166015625
Epoch 440, val loss: 0.7847480773925781
Epoch 450, training loss: 0.7023276090621948 = 0.06576164066791534 + 0.1 * 6.365659713745117
Epoch 450, val loss: 0.7945731282234192
Epoch 460, training loss: 0.6964329481124878 = 0.060446687042713165 + 0.1 * 6.359862804412842
Epoch 460, val loss: 0.8044189214706421
Epoch 470, training loss: 0.6907477378845215 = 0.055732376873493195 + 0.1 * 6.35015344619751
Epoch 470, val loss: 0.8143168687820435
Epoch 480, training loss: 0.6884775161743164 = 0.051524460315704346 + 0.1 * 6.36953067779541
Epoch 480, val loss: 0.8240756392478943
Epoch 490, training loss: 0.6817419528961182 = 0.04777530953288078 + 0.1 * 6.33966588973999
Epoch 490, val loss: 0.8338915705680847
Epoch 500, training loss: 0.6777270436286926 = 0.044407252222299576 + 0.1 * 6.333197593688965
Epoch 500, val loss: 0.8433932662010193
Epoch 510, training loss: 0.6741853952407837 = 0.04136870801448822 + 0.1 * 6.328166484832764
Epoch 510, val loss: 0.8529477119445801
Epoch 520, training loss: 0.6712123155593872 = 0.03862342983484268 + 0.1 * 6.325888633728027
Epoch 520, val loss: 0.8621712923049927
Epoch 530, training loss: 0.6680581569671631 = 0.036144960671663284 + 0.1 * 6.319131851196289
Epoch 530, val loss: 0.8714563250541687
Epoch 540, training loss: 0.665640652179718 = 0.033893026411533356 + 0.1 * 6.31747579574585
Epoch 540, val loss: 0.8803650736808777
Epoch 550, training loss: 0.6628141403198242 = 0.03184732794761658 + 0.1 * 6.309667587280273
Epoch 550, val loss: 0.8892782926559448
Epoch 560, training loss: 0.6609622836112976 = 0.029977615922689438 + 0.1 * 6.3098464012146
Epoch 560, val loss: 0.8979291915893555
Epoch 570, training loss: 0.6580154895782471 = 0.028269557282328606 + 0.1 * 6.297459602355957
Epoch 570, val loss: 0.9065265655517578
Epoch 580, training loss: 0.6566942930221558 = 0.02669922262430191 + 0.1 * 6.299950122833252
Epoch 580, val loss: 0.9148126840591431
Epoch 590, training loss: 0.6552270650863647 = 0.025260167196393013 + 0.1 * 6.29966926574707
Epoch 590, val loss: 0.9230895638465881
Epoch 600, training loss: 0.6523112654685974 = 0.023937920108437538 + 0.1 * 6.283732891082764
Epoch 600, val loss: 0.9312432408332825
Epoch 610, training loss: 0.6507453918457031 = 0.022715358063578606 + 0.1 * 6.280300617218018
Epoch 610, val loss: 0.9391698241233826
Epoch 620, training loss: 0.6517013311386108 = 0.021582694724202156 + 0.1 * 6.3011860847473145
Epoch 620, val loss: 0.9469237923622131
Epoch 630, training loss: 0.6480958461761475 = 0.02053818479180336 + 0.1 * 6.275576114654541
Epoch 630, val loss: 0.9547669291496277
Epoch 640, training loss: 0.6465607285499573 = 0.0195687934756279 + 0.1 * 6.269919395446777
Epoch 640, val loss: 0.9623799920082092
Epoch 650, training loss: 0.646075427532196 = 0.018664848059415817 + 0.1 * 6.274106025695801
Epoch 650, val loss: 0.9697228074073792
Epoch 660, training loss: 0.644669771194458 = 0.017824338749051094 + 0.1 * 6.268454074859619
Epoch 660, val loss: 0.9770146012306213
Epoch 670, training loss: 0.6428909301757812 = 0.01704340986907482 + 0.1 * 6.258474826812744
Epoch 670, val loss: 0.9843999743461609
Epoch 680, training loss: 0.6430253386497498 = 0.016311978921294212 + 0.1 * 6.2671332359313965
Epoch 680, val loss: 0.9913811683654785
Epoch 690, training loss: 0.6405027508735657 = 0.015629757195711136 + 0.1 * 6.248729705810547
Epoch 690, val loss: 0.9984016418457031
Epoch 700, training loss: 0.6399031281471252 = 0.01499136071652174 + 0.1 * 6.249117851257324
Epoch 700, val loss: 1.0052554607391357
Epoch 710, training loss: 0.6390529870986938 = 0.01439196988940239 + 0.1 * 6.246610164642334
Epoch 710, val loss: 1.0119837522506714
Epoch 720, training loss: 0.6380813717842102 = 0.013829528354108334 + 0.1 * 6.242518424987793
Epoch 720, val loss: 1.0185586214065552
Epoch 730, training loss: 0.6369874477386475 = 0.013302809558808804 + 0.1 * 6.236846446990967
Epoch 730, val loss: 1.025146484375
Epoch 740, training loss: 0.6359139680862427 = 0.0128072714433074 + 0.1 * 6.231067180633545
Epoch 740, val loss: 1.0315006971359253
Epoch 750, training loss: 0.635841429233551 = 0.01234161015599966 + 0.1 * 6.234997749328613
Epoch 750, val loss: 1.0379400253295898
Epoch 760, training loss: 0.63502037525177 = 0.011900031939148903 + 0.1 * 6.231203556060791
Epoch 760, val loss: 1.0439331531524658
Epoch 770, training loss: 0.6345210671424866 = 0.011484727263450623 + 0.1 * 6.230363368988037
Epoch 770, val loss: 1.0501058101654053
Epoch 780, training loss: 0.6331373453140259 = 0.011091620661318302 + 0.1 * 6.220457077026367
Epoch 780, val loss: 1.055991768836975
Epoch 790, training loss: 0.6329004764556885 = 0.010720771737396717 + 0.1 * 6.221796989440918
Epoch 790, val loss: 1.061950922012329
Epoch 800, training loss: 0.6324186325073242 = 0.010367877781391144 + 0.1 * 6.220507621765137
Epoch 800, val loss: 1.067459225654602
Epoch 810, training loss: 0.6309814453125 = 0.010035963729023933 + 0.1 * 6.209454536437988
Epoch 810, val loss: 1.0733661651611328
Epoch 820, training loss: 0.631046712398529 = 0.009720767848193645 + 0.1 * 6.213259696960449
Epoch 820, val loss: 1.0789624452590942
Epoch 830, training loss: 0.6296146512031555 = 0.009419802576303482 + 0.1 * 6.201948642730713
Epoch 830, val loss: 1.084274172782898
Epoch 840, training loss: 0.6309323310852051 = 0.009135373868048191 + 0.1 * 6.2179694175720215
Epoch 840, val loss: 1.0898147821426392
Epoch 850, training loss: 0.6291706562042236 = 0.008863513357937336 + 0.1 * 6.203071594238281
Epoch 850, val loss: 1.0949654579162598
Epoch 860, training loss: 0.6288606524467468 = 0.008606581948697567 + 0.1 * 6.202540874481201
Epoch 860, val loss: 1.1004462242126465
Epoch 870, training loss: 0.6287999749183655 = 0.008360489271581173 + 0.1 * 6.204394817352295
Epoch 870, val loss: 1.1054785251617432
Epoch 880, training loss: 0.6291953921318054 = 0.008124953135848045 + 0.1 * 6.210704326629639
Epoch 880, val loss: 1.1102564334869385
Epoch 890, training loss: 0.6273762583732605 = 0.007901773788034916 + 0.1 * 6.19474458694458
Epoch 890, val loss: 1.115410566329956
Epoch 900, training loss: 0.6268925666809082 = 0.007688998244702816 + 0.1 * 6.192035675048828
Epoch 900, val loss: 1.1203663349151611
Epoch 910, training loss: 0.6266798973083496 = 0.007484229747205973 + 0.1 * 6.191956520080566
Epoch 910, val loss: 1.1248507499694824
Epoch 920, training loss: 0.62572181224823 = 0.007289994973689318 + 0.1 * 6.1843180656433105
Epoch 920, val loss: 1.1297707557678223
Epoch 930, training loss: 0.6257445216178894 = 0.007103719748556614 + 0.1 * 6.186407566070557
Epoch 930, val loss: 1.1344635486602783
Epoch 940, training loss: 0.6260756850242615 = 0.006924147717654705 + 0.1 * 6.19151496887207
Epoch 940, val loss: 1.138813853263855
Epoch 950, training loss: 0.624907910823822 = 0.006752992980182171 + 0.1 * 6.181549549102783
Epoch 950, val loss: 1.1434088945388794
Epoch 960, training loss: 0.6248146891593933 = 0.006588958203792572 + 0.1 * 6.182257175445557
Epoch 960, val loss: 1.1478842496871948
Epoch 970, training loss: 0.623745858669281 = 0.006431283429265022 + 0.1 * 6.173145771026611
Epoch 970, val loss: 1.1521806716918945
Epoch 980, training loss: 0.6242849230766296 = 0.006280563306063414 + 0.1 * 6.180043697357178
Epoch 980, val loss: 1.1566622257232666
Epoch 990, training loss: 0.6238729953765869 = 0.006133852526545525 + 0.1 * 6.177391052246094
Epoch 990, val loss: 1.1604623794555664
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6162
Flip ASR: 0.5378/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7834341526031494 = 1.946059226989746 + 0.1 * 8.373748779296875
Epoch 0, val loss: 1.9476919174194336
Epoch 10, training loss: 2.7728662490844727 = 1.9355502128601074 + 0.1 * 8.373161315917969
Epoch 10, val loss: 1.9362390041351318
Epoch 20, training loss: 2.759139060974121 = 1.9220999479293823 + 0.1 * 8.370391845703125
Epoch 20, val loss: 1.921220064163208
Epoch 30, training loss: 2.7387197017669678 = 1.902969479560852 + 0.1 * 8.357501983642578
Epoch 30, val loss: 1.8997271060943604
Epoch 40, training loss: 2.703504800796509 = 1.8749805688858032 + 0.1 * 8.285243034362793
Epoch 40, val loss: 1.8690721988677979
Epoch 50, training loss: 2.6316943168640137 = 1.838136911392212 + 0.1 * 7.935574531555176
Epoch 50, val loss: 1.831073522567749
Epoch 60, training loss: 2.546552896499634 = 1.798464298248291 + 0.1 * 7.480886459350586
Epoch 60, val loss: 1.7923238277435303
Epoch 70, training loss: 2.4658896923065186 = 1.759926676750183 + 0.1 * 7.059629917144775
Epoch 70, val loss: 1.7557679414749146
Epoch 80, training loss: 2.406048536300659 = 1.7190272808074951 + 0.1 * 6.870212078094482
Epoch 80, val loss: 1.7190566062927246
Epoch 90, training loss: 2.3457889556884766 = 1.6698646545410156 + 0.1 * 6.759242057800293
Epoch 90, val loss: 1.6761599779129028
Epoch 100, training loss: 2.274240016937256 = 1.6045769453048706 + 0.1 * 6.696630954742432
Epoch 100, val loss: 1.621519684791565
Epoch 110, training loss: 2.188088893890381 = 1.5219556093215942 + 0.1 * 6.661333084106445
Epoch 110, val loss: 1.5554858446121216
Epoch 120, training loss: 2.091871500015259 = 1.4282336235046387 + 0.1 * 6.636377811431885
Epoch 120, val loss: 1.481887698173523
Epoch 130, training loss: 1.9940252304077148 = 1.3328200578689575 + 0.1 * 6.612051010131836
Epoch 130, val loss: 1.407930850982666
Epoch 140, training loss: 1.8975086212158203 = 1.2385271787643433 + 0.1 * 6.58981466293335
Epoch 140, val loss: 1.3363176584243774
Epoch 150, training loss: 1.8022631406784058 = 1.1452878713607788 + 0.1 * 6.5697526931762695
Epoch 150, val loss: 1.2653297185897827
Epoch 160, training loss: 1.7078375816345215 = 1.0526785850524902 + 0.1 * 6.5515899658203125
Epoch 160, val loss: 1.1947928667068481
Epoch 170, training loss: 1.6141719818115234 = 0.9609038233757019 + 0.1 * 6.532681465148926
Epoch 170, val loss: 1.125578761100769
Epoch 180, training loss: 1.5252381563186646 = 0.8731110692024231 + 0.1 * 6.521270751953125
Epoch 180, val loss: 1.0610666275024414
Epoch 190, training loss: 1.4444339275360107 = 0.793685793876648 + 0.1 * 6.507480621337891
Epoch 190, val loss: 1.0042428970336914
Epoch 200, training loss: 1.3726650476455688 = 0.7232495546340942 + 0.1 * 6.494154930114746
Epoch 200, val loss: 0.955899178981781
Epoch 210, training loss: 1.3096909523010254 = 0.6613244414329529 + 0.1 * 6.4836649894714355
Epoch 210, val loss: 0.916195273399353
Epoch 220, training loss: 1.2554328441619873 = 0.607298731803894 + 0.1 * 6.48134183883667
Epoch 220, val loss: 0.8844504356384277
Epoch 230, training loss: 1.206324577331543 = 0.5598109364509583 + 0.1 * 6.465136528015137
Epoch 230, val loss: 0.8592872023582458
Epoch 240, training loss: 1.1627118587493896 = 0.5169421434402466 + 0.1 * 6.457696914672852
Epoch 240, val loss: 0.8386338949203491
Epoch 250, training loss: 1.1232647895812988 = 0.4777677059173584 + 0.1 * 6.454969882965088
Epoch 250, val loss: 0.8228991031646729
Epoch 260, training loss: 1.0857222080230713 = 0.4416472315788269 + 0.1 * 6.440749168395996
Epoch 260, val loss: 0.8109580278396606
Epoch 270, training loss: 1.0516464710235596 = 0.4077087640762329 + 0.1 * 6.4393768310546875
Epoch 270, val loss: 0.8021376132965088
Epoch 280, training loss: 1.0179758071899414 = 0.37558847665786743 + 0.1 * 6.4238739013671875
Epoch 280, val loss: 0.7963176965713501
Epoch 290, training loss: 0.9858980178833008 = 0.3442579209804535 + 0.1 * 6.416401386260986
Epoch 290, val loss: 0.7929126024246216
Epoch 300, training loss: 0.9584813714027405 = 0.3128306269645691 + 0.1 * 6.456507205963135
Epoch 300, val loss: 0.7910580039024353
Epoch 310, training loss: 0.923901379108429 = 0.2825213074684143 + 0.1 * 6.4138007164001465
Epoch 310, val loss: 0.7912954092025757
Epoch 320, training loss: 0.895142674446106 = 0.25509315729141235 + 0.1 * 6.4004950523376465
Epoch 320, val loss: 0.7941998243331909
Epoch 330, training loss: 0.8701198697090149 = 0.23084087669849396 + 0.1 * 6.392789840698242
Epoch 330, val loss: 0.8008813261985779
Epoch 340, training loss: 0.8479632139205933 = 0.20905040204524994 + 0.1 * 6.389127731323242
Epoch 340, val loss: 0.810009241104126
Epoch 350, training loss: 0.82707679271698 = 0.18933124840259552 + 0.1 * 6.377455711364746
Epoch 350, val loss: 0.8209757208824158
Epoch 360, training loss: 0.8086023330688477 = 0.17151109874248505 + 0.1 * 6.370912075042725
Epoch 360, val loss: 0.8334212303161621
Epoch 370, training loss: 0.7921794652938843 = 0.1555638164281845 + 0.1 * 6.366156101226807
Epoch 370, val loss: 0.8471159934997559
Epoch 380, training loss: 0.7773449420928955 = 0.14133577048778534 + 0.1 * 6.360091209411621
Epoch 380, val loss: 0.8619616627693176
Epoch 390, training loss: 0.7650842666625977 = 0.12858912348747253 + 0.1 * 6.3649516105651855
Epoch 390, val loss: 0.8778199553489685
Epoch 400, training loss: 0.7526231408119202 = 0.11721991747617722 + 0.1 * 6.354032039642334
Epoch 400, val loss: 0.8944464921951294
Epoch 410, training loss: 0.7415719032287598 = 0.10704828053712845 + 0.1 * 6.345236301422119
Epoch 410, val loss: 0.9117097854614258
Epoch 420, training loss: 0.731802225112915 = 0.0979367047548294 + 0.1 * 6.3386549949646
Epoch 420, val loss: 0.9293104410171509
Epoch 430, training loss: 0.7237258553504944 = 0.08973131328821182 + 0.1 * 6.339945316314697
Epoch 430, val loss: 0.9472705721855164
Epoch 440, training loss: 0.7157068252563477 = 0.0823647528886795 + 0.1 * 6.333420753479004
Epoch 440, val loss: 0.9652020335197449
Epoch 450, training loss: 0.7080190181732178 = 0.07577328383922577 + 0.1 * 6.322457313537598
Epoch 450, val loss: 0.9828279614448547
Epoch 460, training loss: 0.7017970085144043 = 0.06984862685203552 + 0.1 * 6.319483280181885
Epoch 460, val loss: 1.0002261400222778
Epoch 470, training loss: 0.6975084543228149 = 0.06450865417718887 + 0.1 * 6.329997539520264
Epoch 470, val loss: 1.0172733068466187
Epoch 480, training loss: 0.690485954284668 = 0.059707123786211014 + 0.1 * 6.307788372039795
Epoch 480, val loss: 1.0338438749313354
Epoch 490, training loss: 0.6861487030982971 = 0.05536658316850662 + 0.1 * 6.307821273803711
Epoch 490, val loss: 1.0499918460845947
Epoch 500, training loss: 0.6813896298408508 = 0.051437679678201675 + 0.1 * 6.2995195388793945
Epoch 500, val loss: 1.0656874179840088
Epoch 510, training loss: 0.677422821521759 = 0.04787563532590866 + 0.1 * 6.295471668243408
Epoch 510, val loss: 1.0810325145721436
Epoch 520, training loss: 0.6775341033935547 = 0.0446363240480423 + 0.1 * 6.328977584838867
Epoch 520, val loss: 1.0959203243255615
Epoch 530, training loss: 0.6714564561843872 = 0.04170794039964676 + 0.1 * 6.2974853515625
Epoch 530, val loss: 1.110296368598938
Epoch 540, training loss: 0.6672241687774658 = 0.039034727960824966 + 0.1 * 6.281894207000732
Epoch 540, val loss: 1.1242390871047974
Epoch 550, training loss: 0.6648112535476685 = 0.03658684715628624 + 0.1 * 6.2822442054748535
Epoch 550, val loss: 1.1380219459533691
Epoch 560, training loss: 0.6620647311210632 = 0.034347835928201675 + 0.1 * 6.2771687507629395
Epoch 560, val loss: 1.1513208150863647
Epoch 570, training loss: 0.6604717969894409 = 0.03230049088597298 + 0.1 * 6.281713008880615
Epoch 570, val loss: 1.1642940044403076
Epoch 580, training loss: 0.6566914319992065 = 0.030420292168855667 + 0.1 * 6.262711048126221
Epoch 580, val loss: 1.1769273281097412
Epoch 590, training loss: 0.6550514698028564 = 0.02869437262415886 + 0.1 * 6.263570785522461
Epoch 590, val loss: 1.1891660690307617
Epoch 600, training loss: 0.6527438163757324 = 0.027113482356071472 + 0.1 * 6.256302833557129
Epoch 600, val loss: 1.201148271560669
Epoch 610, training loss: 0.6515786647796631 = 0.025653904303908348 + 0.1 * 6.259247303009033
Epoch 610, val loss: 1.2127610445022583
Epoch 620, training loss: 0.6497010588645935 = 0.024305101484060287 + 0.1 * 6.2539591789245605
Epoch 620, val loss: 1.2240850925445557
Epoch 630, training loss: 0.6483547687530518 = 0.023057971149683 + 0.1 * 6.252967357635498
Epoch 630, val loss: 1.2351597547531128
Epoch 640, training loss: 0.646645188331604 = 0.02190394513309002 + 0.1 * 6.24741268157959
Epoch 640, val loss: 1.2459096908569336
Epoch 650, training loss: 0.6449007391929626 = 0.020834431052207947 + 0.1 * 6.240663051605225
Epoch 650, val loss: 1.2564815282821655
Epoch 660, training loss: 0.6462428569793701 = 0.019839413464069366 + 0.1 * 6.264034271240234
Epoch 660, val loss: 1.2666670083999634
Epoch 670, training loss: 0.643047571182251 = 0.018915055319666862 + 0.1 * 6.2413249015808105
Epoch 670, val loss: 1.2765557765960693
Epoch 680, training loss: 0.6411843299865723 = 0.01805693469941616 + 0.1 * 6.231274127960205
Epoch 680, val loss: 1.2862730026245117
Epoch 690, training loss: 0.6393181681632996 = 0.017256755381822586 + 0.1 * 6.220613956451416
Epoch 690, val loss: 1.295746922492981
Epoch 700, training loss: 0.640053927898407 = 0.016508005559444427 + 0.1 * 6.235459327697754
Epoch 700, val loss: 1.305005669593811
Epoch 710, training loss: 0.6390441656112671 = 0.015807967633008957 + 0.1 * 6.232361793518066
Epoch 710, val loss: 1.3140596151351929
Epoch 720, training loss: 0.6369744539260864 = 0.01515379548072815 + 0.1 * 6.21820592880249
Epoch 720, val loss: 1.3228188753128052
Epoch 730, training loss: 0.6370019316673279 = 0.01454081665724516 + 0.1 * 6.224611282348633
Epoch 730, val loss: 1.331373691558838
Epoch 740, training loss: 0.6346955895423889 = 0.013965019024908543 + 0.1 * 6.207305431365967
Epoch 740, val loss: 1.3397506475448608
Epoch 750, training loss: 0.635065495967865 = 0.013423544354736805 + 0.1 * 6.216419219970703
Epoch 750, val loss: 1.347944736480713
Epoch 760, training loss: 0.6335757970809937 = 0.012914853170514107 + 0.1 * 6.20660924911499
Epoch 760, val loss: 1.3558111190795898
Epoch 770, training loss: 0.6327313184738159 = 0.012438071891665459 + 0.1 * 6.202932357788086
Epoch 770, val loss: 1.3636195659637451
Epoch 780, training loss: 0.6317914128303528 = 0.011988372541964054 + 0.1 * 6.1980299949646
Epoch 780, val loss: 1.3711973428726196
Epoch 790, training loss: 0.6317391991615295 = 0.011563027277588844 + 0.1 * 6.201761722564697
Epoch 790, val loss: 1.378753423690796
Epoch 800, training loss: 0.6301652193069458 = 0.011159708723425865 + 0.1 * 6.190054893493652
Epoch 800, val loss: 1.386000156402588
Epoch 810, training loss: 0.6324612498283386 = 0.010779310949146748 + 0.1 * 6.2168192863464355
Epoch 810, val loss: 1.393093466758728
Epoch 820, training loss: 0.6290426850318909 = 0.01042055431753397 + 0.1 * 6.186221122741699
Epoch 820, val loss: 1.400092601776123
Epoch 830, training loss: 0.6282417178153992 = 0.01008117850869894 + 0.1 * 6.181605339050293
Epoch 830, val loss: 1.4070136547088623
Epoch 840, training loss: 0.6294998526573181 = 0.009758046828210354 + 0.1 * 6.197417736053467
Epoch 840, val loss: 1.4137243032455444
Epoch 850, training loss: 0.6277238726615906 = 0.009450345300137997 + 0.1 * 6.182735443115234
Epoch 850, val loss: 1.4203152656555176
Epoch 860, training loss: 0.6276872754096985 = 0.009158380329608917 + 0.1 * 6.185288906097412
Epoch 860, val loss: 1.426756739616394
Epoch 870, training loss: 0.6270169019699097 = 0.008880581706762314 + 0.1 * 6.181363105773926
Epoch 870, val loss: 1.433076024055481
Epoch 880, training loss: 0.627321183681488 = 0.008616343140602112 + 0.1 * 6.187048435211182
Epoch 880, val loss: 1.4392057657241821
Epoch 890, training loss: 0.6258152723312378 = 0.008364226669073105 + 0.1 * 6.174510478973389
Epoch 890, val loss: 1.4451522827148438
Epoch 900, training loss: 0.6252025365829468 = 0.0081251859664917 + 0.1 * 6.170773506164551
Epoch 900, val loss: 1.4510794878005981
Epoch 910, training loss: 0.6255995631217957 = 0.007896548137068748 + 0.1 * 6.177030086517334
Epoch 910, val loss: 1.4568270444869995
Epoch 920, training loss: 0.6244124174118042 = 0.007677909452468157 + 0.1 * 6.16734504699707
Epoch 920, val loss: 1.4624943733215332
Epoch 930, training loss: 0.624426007270813 = 0.007469110656529665 + 0.1 * 6.1695685386657715
Epoch 930, val loss: 1.4680465459823608
Epoch 940, training loss: 0.6234892010688782 = 0.007269321475178003 + 0.1 * 6.162198543548584
Epoch 940, val loss: 1.4735243320465088
Epoch 950, training loss: 0.6251487731933594 = 0.007078172173351049 + 0.1 * 6.180706024169922
Epoch 950, val loss: 1.4788904190063477
Epoch 960, training loss: 0.6235983371734619 = 0.006895435508340597 + 0.1 * 6.167028903961182
Epoch 960, val loss: 1.4841210842132568
Epoch 970, training loss: 0.6236189603805542 = 0.0067205606028437614 + 0.1 * 6.1689839363098145
Epoch 970, val loss: 1.4893602132797241
Epoch 980, training loss: 0.6228822469711304 = 0.006552806589752436 + 0.1 * 6.163294315338135
Epoch 980, val loss: 1.4944056272506714
Epoch 990, training loss: 0.6224257349967957 = 0.006391976028680801 + 0.1 * 6.160337448120117
Epoch 990, val loss: 1.499443769454956
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.8708
Flip ASR: 0.8444/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.779348373413086 = 1.9419711828231812 + 0.1 * 8.373771667480469
Epoch 0, val loss: 1.928964376449585
Epoch 10, training loss: 2.768613338470459 = 1.9312611818313599 + 0.1 * 8.37352180480957
Epoch 10, val loss: 1.919215202331543
Epoch 20, training loss: 2.7554259300231934 = 1.9182065725326538 + 0.1 * 8.372194290161133
Epoch 20, val loss: 1.9071533679962158
Epoch 30, training loss: 2.7365782260894775 = 1.9002571105957031 + 0.1 * 8.363210678100586
Epoch 30, val loss: 1.8903594017028809
Epoch 40, training loss: 2.7052390575408936 = 1.87428617477417 + 0.1 * 8.309528350830078
Epoch 40, val loss: 1.8664506673812866
Epoch 50, training loss: 2.642855167388916 = 1.8397865295410156 + 0.1 * 8.030686378479004
Epoch 50, val loss: 1.836140513420105
Epoch 60, training loss: 2.5537235736846924 = 1.802321434020996 + 0.1 * 7.514020919799805
Epoch 60, val loss: 1.8049466609954834
Epoch 70, training loss: 2.4735233783721924 = 1.767499566078186 + 0.1 * 7.060238361358643
Epoch 70, val loss: 1.7765216827392578
Epoch 80, training loss: 2.4116735458374023 = 1.7303862571716309 + 0.1 * 6.812873363494873
Epoch 80, val loss: 1.7451014518737793
Epoch 90, training loss: 2.3551485538482666 = 1.6812881231307983 + 0.1 * 6.738604545593262
Epoch 90, val loss: 1.7010180950164795
Epoch 100, training loss: 2.2859997749328613 = 1.6162593364715576 + 0.1 * 6.697404384613037
Epoch 100, val loss: 1.644308090209961
Epoch 110, training loss: 2.202075481414795 = 1.5350167751312256 + 0.1 * 6.670587539672852
Epoch 110, val loss: 1.5772595405578613
Epoch 120, training loss: 2.1088743209838867 = 1.4434826374053955 + 0.1 * 6.6539154052734375
Epoch 120, val loss: 1.5035799741744995
Epoch 130, training loss: 2.013079881668091 = 1.3488399982452393 + 0.1 * 6.642399311065674
Epoch 130, val loss: 1.4298697710037231
Epoch 140, training loss: 1.9181069135665894 = 1.2549015283584595 + 0.1 * 6.632053852081299
Epoch 140, val loss: 1.3587923049926758
Epoch 150, training loss: 1.8229541778564453 = 1.160815954208374 + 0.1 * 6.621381759643555
Epoch 150, val loss: 1.2876944541931152
Epoch 160, training loss: 1.7257063388824463 = 1.0648672580718994 + 0.1 * 6.608391284942627
Epoch 160, val loss: 1.2142784595489502
Epoch 170, training loss: 1.62746000289917 = 0.9678016901016235 + 0.1 * 6.596582412719727
Epoch 170, val loss: 1.1402190923690796
Epoch 180, training loss: 1.5315892696380615 = 0.8735685348510742 + 0.1 * 6.580207824707031
Epoch 180, val loss: 1.0690652132034302
Epoch 190, training loss: 1.4416254758834839 = 0.7848376035690308 + 0.1 * 6.567878723144531
Epoch 190, val loss: 1.003308892250061
Epoch 200, training loss: 1.3596603870391846 = 0.7039433717727661 + 0.1 * 6.557169437408447
Epoch 200, val loss: 0.9447203278541565
Epoch 210, training loss: 1.2856450080871582 = 0.6311062574386597 + 0.1 * 6.545386791229248
Epoch 210, val loss: 0.8932358622550964
Epoch 220, training loss: 1.2200316190719604 = 0.5662947297096252 + 0.1 * 6.5373687744140625
Epoch 220, val loss: 0.8494366407394409
Epoch 230, training loss: 1.1624782085418701 = 0.5098091959953308 + 0.1 * 6.526689529418945
Epoch 230, val loss: 0.8137809634208679
Epoch 240, training loss: 1.1107263565063477 = 0.4594694674015045 + 0.1 * 6.512568473815918
Epoch 240, val loss: 0.7845665216445923
Epoch 250, training loss: 1.064413070678711 = 0.41415128111839294 + 0.1 * 6.502618312835693
Epoch 250, val loss: 0.7609466314315796
Epoch 260, training loss: 1.0217978954315186 = 0.37291035056114197 + 0.1 * 6.488875865936279
Epoch 260, val loss: 0.7421865463256836
Epoch 270, training loss: 0.9825536012649536 = 0.33467239141464233 + 0.1 * 6.478812217712402
Epoch 270, val loss: 0.7267786264419556
Epoch 280, training loss: 0.9474083185195923 = 0.29905298352241516 + 0.1 * 6.483552932739258
Epoch 280, val loss: 0.7138572335243225
Epoch 290, training loss: 0.9129370450973511 = 0.2663900852203369 + 0.1 * 6.4654693603515625
Epoch 290, val loss: 0.7033888101577759
Epoch 300, training loss: 0.8812522888183594 = 0.23637893795967102 + 0.1 * 6.448733329772949
Epoch 300, val loss: 0.6947874426841736
Epoch 310, training loss: 0.8548648357391357 = 0.2091216742992401 + 0.1 * 6.457431793212891
Epoch 310, val loss: 0.6883267760276794
Epoch 320, training loss: 0.8286539316177368 = 0.18501494824886322 + 0.1 * 6.436389446258545
Epoch 320, val loss: 0.684065043926239
Epoch 330, training loss: 0.806027352809906 = 0.16381549835205078 + 0.1 * 6.422118663787842
Epoch 330, val loss: 0.6818645596504211
Epoch 340, training loss: 0.7871549129486084 = 0.1452443152666092 + 0.1 * 6.4191060066223145
Epoch 340, val loss: 0.6819038391113281
Epoch 350, training loss: 0.7694797515869141 = 0.1291668862104416 + 0.1 * 6.403128623962402
Epoch 350, val loss: 0.683783769607544
Epoch 360, training loss: 0.7549788355827332 = 0.11522377282381058 + 0.1 * 6.397550582885742
Epoch 360, val loss: 0.6873180270195007
Epoch 370, training loss: 0.7423258423805237 = 0.10310733318328857 + 0.1 * 6.392185211181641
Epoch 370, val loss: 0.6924261450767517
Epoch 380, training loss: 0.7316319942474365 = 0.09263543784618378 + 0.1 * 6.389965057373047
Epoch 380, val loss: 0.6985285878181458
Epoch 390, training loss: 0.7207839488983154 = 0.08348789811134338 + 0.1 * 6.372960090637207
Epoch 390, val loss: 0.705470085144043
Epoch 400, training loss: 0.7126396894454956 = 0.07547105848789215 + 0.1 * 6.3716864585876465
Epoch 400, val loss: 0.7132129669189453
Epoch 410, training loss: 0.7051097750663757 = 0.06845979392528534 + 0.1 * 6.366499423980713
Epoch 410, val loss: 0.72125244140625
Epoch 420, training loss: 0.6983649730682373 = 0.062318600714206696 + 0.1 * 6.360463619232178
Epoch 420, val loss: 0.7295608520507812
Epoch 430, training loss: 0.6928394436836243 = 0.05690401792526245 + 0.1 * 6.359354019165039
Epoch 430, val loss: 0.7380416989326477
Epoch 440, training loss: 0.6865996718406677 = 0.05215052515268326 + 0.1 * 6.344491004943848
Epoch 440, val loss: 0.7465160489082336
Epoch 450, training loss: 0.6821897029876709 = 0.047955699265003204 + 0.1 * 6.342339992523193
Epoch 450, val loss: 0.7548442482948303
Epoch 460, training loss: 0.679909348487854 = 0.04422202706336975 + 0.1 * 6.35687255859375
Epoch 460, val loss: 0.7632365226745605
Epoch 470, training loss: 0.6741544008255005 = 0.04091155529022217 + 0.1 * 6.332428455352783
Epoch 470, val loss: 0.7715317606925964
Epoch 480, training loss: 0.6700217127799988 = 0.03795141354203224 + 0.1 * 6.320703029632568
Epoch 480, val loss: 0.7796237468719482
Epoch 490, training loss: 0.667699933052063 = 0.0352938249707222 + 0.1 * 6.324060916900635
Epoch 490, val loss: 0.7877366542816162
Epoch 500, training loss: 0.6645331978797913 = 0.03291071951389313 + 0.1 * 6.316225051879883
Epoch 500, val loss: 0.7957033514976501
Epoch 510, training loss: 0.6623255610466003 = 0.03076195903122425 + 0.1 * 6.315636157989502
Epoch 510, val loss: 0.8034688830375671
Epoch 520, training loss: 0.6597644686698914 = 0.028823351487517357 + 0.1 * 6.30941104888916
Epoch 520, val loss: 0.8111627101898193
Epoch 530, training loss: 0.6566749215126038 = 0.027072669938206673 + 0.1 * 6.296022415161133
Epoch 530, val loss: 0.8186548948287964
Epoch 540, training loss: 0.6556226015090942 = 0.0254785418510437 + 0.1 * 6.301440715789795
Epoch 540, val loss: 0.8259791731834412
Epoch 550, training loss: 0.6528841257095337 = 0.024025382474064827 + 0.1 * 6.28858757019043
Epoch 550, val loss: 0.8332715630531311
Epoch 560, training loss: 0.6512118577957153 = 0.022698024287819862 + 0.1 * 6.285138130187988
Epoch 560, val loss: 0.8403902649879456
Epoch 570, training loss: 0.6507033705711365 = 0.021481595933437347 + 0.1 * 6.29221773147583
Epoch 570, val loss: 0.8473619222640991
Epoch 580, training loss: 0.6479722261428833 = 0.02036757953464985 + 0.1 * 6.276046276092529
Epoch 580, val loss: 0.8542446494102478
Epoch 590, training loss: 0.6471116542816162 = 0.01934107393026352 + 0.1 * 6.277706146240234
Epoch 590, val loss: 0.8609041571617126
Epoch 600, training loss: 0.6448675394058228 = 0.018396146595478058 + 0.1 * 6.264713764190674
Epoch 600, val loss: 0.8675057888031006
Epoch 610, training loss: 0.6438261866569519 = 0.01752307265996933 + 0.1 * 6.263031005859375
Epoch 610, val loss: 0.8739335536956787
Epoch 620, training loss: 0.6455458998680115 = 0.016712205484509468 + 0.1 * 6.288337230682373
Epoch 620, val loss: 0.8802909255027771
Epoch 630, training loss: 0.6416717171669006 = 0.015962284058332443 + 0.1 * 6.257093906402588
Epoch 630, val loss: 0.8865147829055786
Epoch 640, training loss: 0.6407557129859924 = 0.015265967696905136 + 0.1 * 6.254897594451904
Epoch 640, val loss: 0.8925055861473083
Epoch 650, training loss: 0.6392248868942261 = 0.014615057036280632 + 0.1 * 6.246098041534424
Epoch 650, val loss: 0.8984532356262207
Epoch 660, training loss: 0.6403422951698303 = 0.014006280340254307 + 0.1 * 6.263360023498535
Epoch 660, val loss: 0.9044075608253479
Epoch 670, training loss: 0.6386228203773499 = 0.013437541201710701 + 0.1 * 6.251852512359619
Epoch 670, val loss: 0.9101945161819458
Epoch 680, training loss: 0.6365868449211121 = 0.012908742763102055 + 0.1 * 6.236780643463135
Epoch 680, val loss: 0.9157694578170776
Epoch 690, training loss: 0.6353185176849365 = 0.01241192314773798 + 0.1 * 6.229065418243408
Epoch 690, val loss: 0.9212016463279724
Epoch 700, training loss: 0.639218807220459 = 0.011943389661610126 + 0.1 * 6.272754192352295
Epoch 700, val loss: 0.9267226457595825
Epoch 710, training loss: 0.6337056159973145 = 0.011503512971103191 + 0.1 * 6.222020626068115
Epoch 710, val loss: 0.9321349263191223
Epoch 720, training loss: 0.6337705850601196 = 0.011091240681707859 + 0.1 * 6.22679328918457
Epoch 720, val loss: 0.9373223185539246
Epoch 730, training loss: 0.6343890428543091 = 0.01070110872387886 + 0.1 * 6.236879348754883
Epoch 730, val loss: 0.9424511194229126
Epoch 740, training loss: 0.6332875490188599 = 0.010332261212170124 + 0.1 * 6.229552745819092
Epoch 740, val loss: 0.947539746761322
Epoch 750, training loss: 0.6317088603973389 = 0.00998518243432045 + 0.1 * 6.217236518859863
Epoch 750, val loss: 0.9525104761123657
Epoch 760, training loss: 0.6310800909996033 = 0.009657938964664936 + 0.1 * 6.214221477508545
Epoch 760, val loss: 0.957343339920044
Epoch 770, training loss: 0.6328687071800232 = 0.009347671642899513 + 0.1 * 6.235209941864014
Epoch 770, val loss: 0.962089478969574
Epoch 780, training loss: 0.6309312582015991 = 0.009052298031747341 + 0.1 * 6.218789577484131
Epoch 780, val loss: 0.966793954372406
Epoch 790, training loss: 0.6296754479408264 = 0.008773990906774998 + 0.1 * 6.209014415740967
Epoch 790, val loss: 0.971433162689209
Epoch 800, training loss: 0.6313801407814026 = 0.008508449420332909 + 0.1 * 6.228716850280762
Epoch 800, val loss: 0.9759625196456909
Epoch 810, training loss: 0.628433108329773 = 0.008255401626229286 + 0.1 * 6.201776504516602
Epoch 810, val loss: 0.9804211854934692
Epoch 820, training loss: 0.6283717155456543 = 0.008016216568648815 + 0.1 * 6.203554630279541
Epoch 820, val loss: 0.9847735166549683
Epoch 830, training loss: 0.6281449198722839 = 0.007787513080984354 + 0.1 * 6.203573703765869
Epoch 830, val loss: 0.9890177249908447
Epoch 840, training loss: 0.6273142695426941 = 0.007570039015263319 + 0.1 * 6.197442054748535
Epoch 840, val loss: 0.9932293891906738
Epoch 850, training loss: 0.6263130307197571 = 0.007362276781350374 + 0.1 * 6.189507007598877
Epoch 850, val loss: 0.9973656535148621
Epoch 860, training loss: 0.6265560388565063 = 0.007163854781538248 + 0.1 * 6.1939215660095215
Epoch 860, val loss: 1.0014547109603882
Epoch 870, training loss: 0.6258817315101624 = 0.0069737667217850685 + 0.1 * 6.189079761505127
Epoch 870, val loss: 1.0054659843444824
Epoch 880, training loss: 0.6262551546096802 = 0.006792963482439518 + 0.1 * 6.194622039794922
Epoch 880, val loss: 1.0094304084777832
Epoch 890, training loss: 0.6254382729530334 = 0.006619217339903116 + 0.1 * 6.188190460205078
Epoch 890, val loss: 1.013319730758667
Epoch 900, training loss: 0.6253914833068848 = 0.0064531792886555195 + 0.1 * 6.189383029937744
Epoch 900, val loss: 1.0171805620193481
Epoch 910, training loss: 0.6247270107269287 = 0.006293225102126598 + 0.1 * 6.184337615966797
Epoch 910, val loss: 1.0209773778915405
Epoch 920, training loss: 0.6237833499908447 = 0.006141457706689835 + 0.1 * 6.176418304443359
Epoch 920, val loss: 1.0246808528900146
Epoch 930, training loss: 0.6255278587341309 = 0.005995423998683691 + 0.1 * 6.195323944091797
Epoch 930, val loss: 1.028304934501648
Epoch 940, training loss: 0.6237106323242188 = 0.005854735616594553 + 0.1 * 6.178558826446533
Epoch 940, val loss: 1.0319066047668457
Epoch 950, training loss: 0.6226514577865601 = 0.005720224697142839 + 0.1 * 6.169312477111816
Epoch 950, val loss: 1.0354454517364502
Epoch 960, training loss: 0.6239126920700073 = 0.005590586457401514 + 0.1 * 6.183221340179443
Epoch 960, val loss: 1.038886547088623
Epoch 970, training loss: 0.6224419474601746 = 0.00546537758782506 + 0.1 * 6.169765472412109
Epoch 970, val loss: 1.0423543453216553
Epoch 980, training loss: 0.62279212474823 = 0.00534489331766963 + 0.1 * 6.174472332000732
Epoch 980, val loss: 1.0457185506820679
Epoch 990, training loss: 0.6213974952697754 = 0.005229511298239231 + 0.1 * 6.161679267883301
Epoch 990, val loss: 1.0490014553070068
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9004
Flip ASR: 0.8800/225 nodes
The final ASR:0.79582, 0.12755, Accuracy:0.81481, 0.02117
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11674])
remove edge: torch.Size([2, 9398])
updated graph: torch.Size([2, 10516])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98893, 0.00797, Accuracy:0.82963, 0.00302
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7798619270324707 = 1.9424762725830078 + 0.1 * 8.373856544494629
Epoch 0, val loss: 1.9491934776306152
Epoch 10, training loss: 2.7701916694641113 = 1.9328210353851318 + 0.1 * 8.373706817626953
Epoch 10, val loss: 1.9397910833358765
Epoch 20, training loss: 2.7581844329833984 = 1.9208879470825195 + 0.1 * 8.372964859008789
Epoch 20, val loss: 1.927694320678711
Epoch 30, training loss: 2.740786075592041 = 1.9039973020553589 + 0.1 * 8.367888450622559
Epoch 30, val loss: 1.910089135169983
Epoch 40, training loss: 2.7119994163513184 = 1.8789489269256592 + 0.1 * 8.330503463745117
Epoch 40, val loss: 1.8838844299316406
Epoch 50, training loss: 2.6442832946777344 = 1.8451237678527832 + 0.1 * 7.991594314575195
Epoch 50, val loss: 1.8503074645996094
Epoch 60, training loss: 2.5444350242614746 = 1.812434196472168 + 0.1 * 7.320006847381592
Epoch 60, val loss: 1.8200067281723022
Epoch 70, training loss: 2.4692842960357666 = 1.7809886932373047 + 0.1 * 6.882956504821777
Epoch 70, val loss: 1.7912400960922241
Epoch 80, training loss: 2.4193739891052246 = 1.747164011001587 + 0.1 * 6.722099304199219
Epoch 80, val loss: 1.761546015739441
Epoch 90, training loss: 2.3709182739257812 = 1.704776644706726 + 0.1 * 6.661415100097656
Epoch 90, val loss: 1.7250611782073975
Epoch 100, training loss: 2.3104360103607178 = 1.6476069688796997 + 0.1 * 6.628291130065918
Epoch 100, val loss: 1.6763166189193726
Epoch 110, training loss: 2.2338192462921143 = 1.5732771158218384 + 0.1 * 6.6054205894470215
Epoch 110, val loss: 1.6134825944900513
Epoch 120, training loss: 2.1433568000793457 = 1.4844732284545898 + 0.1 * 6.5888352394104
Epoch 120, val loss: 1.539908528327942
Epoch 130, training loss: 2.0462231636047363 = 1.3884081840515137 + 0.1 * 6.578149795532227
Epoch 130, val loss: 1.4635957479476929
Epoch 140, training loss: 1.946319818496704 = 1.2896647453308105 + 0.1 * 6.566551208496094
Epoch 140, val loss: 1.3877887725830078
Epoch 150, training loss: 1.8470029830932617 = 1.1918339729309082 + 0.1 * 6.551690578460693
Epoch 150, val loss: 1.3151756525039673
Epoch 160, training loss: 1.751899003982544 = 1.097861886024475 + 0.1 * 6.540370941162109
Epoch 160, val loss: 1.2468048334121704
Epoch 170, training loss: 1.6647815704345703 = 1.012544870376587 + 0.1 * 6.522366046905518
Epoch 170, val loss: 1.1854982376098633
Epoch 180, training loss: 1.5875204801559448 = 0.9361909031867981 + 0.1 * 6.513295650482178
Epoch 180, val loss: 1.1314648389816284
Epoch 190, training loss: 1.5171515941619873 = 0.8672419786453247 + 0.1 * 6.499096870422363
Epoch 190, val loss: 1.0839098691940308
Epoch 200, training loss: 1.449930191040039 = 0.8010843396186829 + 0.1 * 6.488458633422852
Epoch 200, val loss: 1.0394210815429688
Epoch 210, training loss: 1.3828214406967163 = 0.7346949577331543 + 0.1 * 6.481264591217041
Epoch 210, val loss: 0.9958382844924927
Epoch 220, training loss: 1.3147752285003662 = 0.6673840880393982 + 0.1 * 6.473911285400391
Epoch 220, val loss: 0.9525384902954102
Epoch 230, training loss: 1.2468247413635254 = 0.6003795266151428 + 0.1 * 6.464452743530273
Epoch 230, val loss: 0.9106261730194092
Epoch 240, training loss: 1.1818242073059082 = 0.535882294178009 + 0.1 * 6.459419250488281
Epoch 240, val loss: 0.8718940615653992
Epoch 250, training loss: 1.1228179931640625 = 0.4761391580104828 + 0.1 * 6.4667887687683105
Epoch 250, val loss: 0.8384087085723877
Epoch 260, training loss: 1.0670925378799438 = 0.42263635993003845 + 0.1 * 6.444561958312988
Epoch 260, val loss: 0.8115149140357971
Epoch 270, training loss: 1.0201644897460938 = 0.3753877878189087 + 0.1 * 6.4477667808532715
Epoch 270, val loss: 0.7912692427635193
Epoch 280, training loss: 0.9771526455879211 = 0.33416128158569336 + 0.1 * 6.429913520812988
Epoch 280, val loss: 0.7772811055183411
Epoch 290, training loss: 0.9400515556335449 = 0.29792454838752747 + 0.1 * 6.42126989364624
Epoch 290, val loss: 0.7686043977737427
Epoch 300, training loss: 0.9086261987686157 = 0.26603975892066956 + 0.1 * 6.4258646965026855
Epoch 300, val loss: 0.7645059823989868
Epoch 310, training loss: 0.8784950375556946 = 0.23806393146514893 + 0.1 * 6.404311180114746
Epoch 310, val loss: 0.7641154527664185
Epoch 320, training loss: 0.8541945219039917 = 0.21328121423721313 + 0.1 * 6.409132957458496
Epoch 320, val loss: 0.7667312622070312
Epoch 330, training loss: 0.8305414319038391 = 0.19141703844070435 + 0.1 * 6.391243934631348
Epoch 330, val loss: 0.771921694278717
Epoch 340, training loss: 0.8102501034736633 = 0.172053262591362 + 0.1 * 6.381968021392822
Epoch 340, val loss: 0.779315173625946
Epoch 350, training loss: 0.7935165762901306 = 0.15496516227722168 + 0.1 * 6.385514259338379
Epoch 350, val loss: 0.7884406447410583
Epoch 360, training loss: 0.7768203020095825 = 0.14001303911209106 + 0.1 * 6.368072509765625
Epoch 360, val loss: 0.7989204525947571
Epoch 370, training loss: 0.7633877992630005 = 0.12682414054870605 + 0.1 * 6.365636348724365
Epoch 370, val loss: 0.8105514645576477
Epoch 380, training loss: 0.7505269050598145 = 0.11518842726945877 + 0.1 * 6.353384494781494
Epoch 380, val loss: 0.8230651021003723
Epoch 390, training loss: 0.7407016158103943 = 0.10488641262054443 + 0.1 * 6.358151912689209
Epoch 390, val loss: 0.836294412612915
Epoch 400, training loss: 0.730405867099762 = 0.0957786962389946 + 0.1 * 6.346271514892578
Epoch 400, val loss: 0.8499277234077454
Epoch 410, training loss: 0.7227324843406677 = 0.0876934677362442 + 0.1 * 6.3503899574279785
Epoch 410, val loss: 0.8639654517173767
Epoch 420, training loss: 0.7136920690536499 = 0.08051924407482147 + 0.1 * 6.331727981567383
Epoch 420, val loss: 0.8781533241271973
Epoch 430, training loss: 0.7071287631988525 = 0.0741112008690834 + 0.1 * 6.330175399780273
Epoch 430, val loss: 0.8925504088401794
Epoch 440, training loss: 0.7005600929260254 = 0.06837991625070572 + 0.1 * 6.321801662445068
Epoch 440, val loss: 0.9069112539291382
Epoch 450, training loss: 0.6943805813789368 = 0.06324038654565811 + 0.1 * 6.311401844024658
Epoch 450, val loss: 0.9213765859603882
Epoch 460, training loss: 0.6914684772491455 = 0.058617573231458664 + 0.1 * 6.3285088539123535
Epoch 460, val loss: 0.935707688331604
Epoch 470, training loss: 0.6854676008224487 = 0.05447286367416382 + 0.1 * 6.309947490692139
Epoch 470, val loss: 0.9498340487480164
Epoch 480, training loss: 0.6807058453559875 = 0.05073796957731247 + 0.1 * 6.299678802490234
Epoch 480, val loss: 0.9638192653656006
Epoch 490, training loss: 0.6766281723976135 = 0.04735017940402031 + 0.1 * 6.292779922485352
Epoch 490, val loss: 0.977676272392273
Epoch 500, training loss: 0.6743172407150269 = 0.04426954686641693 + 0.1 * 6.300476551055908
Epoch 500, val loss: 0.9913936853408813
Epoch 510, training loss: 0.6714481711387634 = 0.041475020349025726 + 0.1 * 6.299731731414795
Epoch 510, val loss: 1.0048383474349976
Epoch 520, training loss: 0.6680396199226379 = 0.03893503174185753 + 0.1 * 6.291046142578125
Epoch 520, val loss: 1.0180070400238037
Epoch 530, training loss: 0.664545476436615 = 0.036619771271944046 + 0.1 * 6.279256820678711
Epoch 530, val loss: 1.030978798866272
Epoch 540, training loss: 0.6619653701782227 = 0.03449619933962822 + 0.1 * 6.274691581726074
Epoch 540, val loss: 1.043770432472229
Epoch 550, training loss: 0.6614816188812256 = 0.032546959817409515 + 0.1 * 6.289346694946289
Epoch 550, val loss: 1.0564237833023071
Epoch 560, training loss: 0.6577847003936768 = 0.030763622373342514 + 0.1 * 6.2702107429504395
Epoch 560, val loss: 1.0686315298080444
Epoch 570, training loss: 0.6557440161705017 = 0.02912859618663788 + 0.1 * 6.266153812408447
Epoch 570, val loss: 1.0806787014007568
Epoch 580, training loss: 0.654748260974884 = 0.027617355808615685 + 0.1 * 6.2713093757629395
Epoch 580, val loss: 1.0924735069274902
Epoch 590, training loss: 0.6521698236465454 = 0.026224063709378242 + 0.1 * 6.259457111358643
Epoch 590, val loss: 1.104009985923767
Epoch 600, training loss: 0.6509389281272888 = 0.024935198947787285 + 0.1 * 6.260037422180176
Epoch 600, val loss: 1.1153658628463745
Epoch 610, training loss: 0.6490082740783691 = 0.023739632219076157 + 0.1 * 6.252686500549316
Epoch 610, val loss: 1.126466155052185
Epoch 620, training loss: 0.6468998789787292 = 0.022632023319602013 + 0.1 * 6.242678642272949
Epoch 620, val loss: 1.137315273284912
Epoch 630, training loss: 0.6460813879966736 = 0.021600468084216118 + 0.1 * 6.244809150695801
Epoch 630, val loss: 1.1480158567428589
Epoch 640, training loss: 0.6449049711227417 = 0.020637813955545425 + 0.1 * 6.242671489715576
Epoch 640, val loss: 1.158460259437561
Epoch 650, training loss: 0.6450996994972229 = 0.01974213868379593 + 0.1 * 6.253575325012207
Epoch 650, val loss: 1.168724775314331
Epoch 660, training loss: 0.6433462500572205 = 0.018906215205788612 + 0.1 * 6.2444000244140625
Epoch 660, val loss: 1.1786664724349976
Epoch 670, training loss: 0.6410465240478516 = 0.018126726150512695 + 0.1 * 6.229197978973389
Epoch 670, val loss: 1.1884132623672485
Epoch 680, training loss: 0.6410762071609497 = 0.01739470474421978 + 0.1 * 6.236814498901367
Epoch 680, val loss: 1.1979976892471313
Epoch 690, training loss: 0.6397690773010254 = 0.016706887632608414 + 0.1 * 6.230621814727783
Epoch 690, val loss: 1.2073838710784912
Epoch 700, training loss: 0.6385747194290161 = 0.016064411029219627 + 0.1 * 6.22510290145874
Epoch 700, val loss: 1.2166097164154053
Epoch 710, training loss: 0.6376805901527405 = 0.01545802690088749 + 0.1 * 6.222225189208984
Epoch 710, val loss: 1.2256014347076416
Epoch 720, training loss: 0.6360012888908386 = 0.014889376237988472 + 0.1 * 6.211119174957275
Epoch 720, val loss: 1.2344200611114502
Epoch 730, training loss: 0.6361750960350037 = 0.014351669698953629 + 0.1 * 6.218234062194824
Epoch 730, val loss: 1.2431116104125977
Epoch 740, training loss: 0.6357629895210266 = 0.013843193650245667 + 0.1 * 6.219197750091553
Epoch 740, val loss: 1.2517094612121582
Epoch 750, training loss: 0.634705126285553 = 0.013365146704018116 + 0.1 * 6.213399887084961
Epoch 750, val loss: 1.2600868940353394
Epoch 760, training loss: 0.6340188980102539 = 0.012914187274873257 + 0.1 * 6.211047172546387
Epoch 760, val loss: 1.2682939767837524
Epoch 770, training loss: 0.6326861381530762 = 0.012486190535128117 + 0.1 * 6.201999187469482
Epoch 770, val loss: 1.2763493061065674
Epoch 780, training loss: 0.6335104703903198 = 0.012080362997949123 + 0.1 * 6.214301109313965
Epoch 780, val loss: 1.2842971086502075
Epoch 790, training loss: 0.6316483020782471 = 0.01169491931796074 + 0.1 * 6.199533939361572
Epoch 790, val loss: 1.2921600341796875
Epoch 800, training loss: 0.6307367086410522 = 0.011330801993608475 + 0.1 * 6.194059371948242
Epoch 800, val loss: 1.2998881340026855
Epoch 810, training loss: 0.6311792135238647 = 0.010983934625983238 + 0.1 * 6.2019524574279785
Epoch 810, val loss: 1.307479739189148
Epoch 820, training loss: 0.631134033203125 = 0.010652887634932995 + 0.1 * 6.204811096191406
Epoch 820, val loss: 1.3149726390838623
Epoch 830, training loss: 0.6298522353172302 = 0.010339969769120216 + 0.1 * 6.195122241973877
Epoch 830, val loss: 1.3222492933273315
Epoch 840, training loss: 0.6293915510177612 = 0.01004173792898655 + 0.1 * 6.193498134613037
Epoch 840, val loss: 1.3294801712036133
Epoch 850, training loss: 0.6290632486343384 = 0.009755960665643215 + 0.1 * 6.19307279586792
Epoch 850, val loss: 1.336594581604004
Epoch 860, training loss: 0.6285528540611267 = 0.009484187699854374 + 0.1 * 6.1906867027282715
Epoch 860, val loss: 1.3435865640640259
Epoch 870, training loss: 0.6269344091415405 = 0.009225134737789631 + 0.1 * 6.177092552185059
Epoch 870, val loss: 1.350480079650879
Epoch 880, training loss: 0.6265937089920044 = 0.00897733774036169 + 0.1 * 6.176163673400879
Epoch 880, val loss: 1.3572362661361694
Epoch 890, training loss: 0.627885639667511 = 0.008739599026739597 + 0.1 * 6.191460609436035
Epoch 890, val loss: 1.3639415502548218
Epoch 900, training loss: 0.6277110576629639 = 0.00851150881499052 + 0.1 * 6.191995620727539
Epoch 900, val loss: 1.3706130981445312
Epoch 910, training loss: 0.6252132058143616 = 0.008294209837913513 + 0.1 * 6.169189453125
Epoch 910, val loss: 1.3771506547927856
Epoch 920, training loss: 0.6255185008049011 = 0.00808641780167818 + 0.1 * 6.174320220947266
Epoch 920, val loss: 1.3835852146148682
Epoch 930, training loss: 0.625191330909729 = 0.007885926403105259 + 0.1 * 6.173053741455078
Epoch 930, val loss: 1.3899613618850708
Epoch 940, training loss: 0.6257359385490417 = 0.007693840190768242 + 0.1 * 6.180420875549316
Epoch 940, val loss: 1.3962382078170776
Epoch 950, training loss: 0.6243321895599365 = 0.007509326562285423 + 0.1 * 6.1682281494140625
Epoch 950, val loss: 1.4024784564971924
Epoch 960, training loss: 0.624643862247467 = 0.007332333829253912 + 0.1 * 6.173115253448486
Epoch 960, val loss: 1.4086027145385742
Epoch 970, training loss: 0.6239312291145325 = 0.007161212153732777 + 0.1 * 6.167700290679932
Epoch 970, val loss: 1.414700984954834
Epoch 980, training loss: 0.623409628868103 = 0.006997826509177685 + 0.1 * 6.164117813110352
Epoch 980, val loss: 1.4206812381744385
Epoch 990, training loss: 0.6229627728462219 = 0.006840809714049101 + 0.1 * 6.161219596862793
Epoch 990, val loss: 1.426550269126892
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5830
Flip ASR: 0.5200/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7815909385681152 = 1.9442024230957031 + 0.1 * 8.373886108398438
Epoch 0, val loss: 1.9438420534133911
Epoch 10, training loss: 2.7706375122070312 = 1.933260202407837 + 0.1 * 8.373771667480469
Epoch 10, val loss: 1.9333773851394653
Epoch 20, training loss: 2.756801128387451 = 1.9194756746292114 + 0.1 * 8.373254776000977
Epoch 20, val loss: 1.9198212623596191
Epoch 30, training loss: 2.7370071411132812 = 1.9000505208969116 + 0.1 * 8.369566917419434
Epoch 30, val loss: 1.9004884958267212
Epoch 40, training loss: 2.7058117389678955 = 1.8717875480651855 + 0.1 * 8.340242385864258
Epoch 40, val loss: 1.8728489875793457
Epoch 50, training loss: 2.6467039585113525 = 1.8347338438034058 + 0.1 * 8.11970043182373
Epoch 50, val loss: 1.8386608362197876
Epoch 60, training loss: 2.567955255508423 = 1.79820716381073 + 0.1 * 7.697481632232666
Epoch 60, val loss: 1.8068926334381104
Epoch 70, training loss: 2.4923574924468994 = 1.766294002532959 + 0.1 * 7.260634422302246
Epoch 70, val loss: 1.7796354293823242
Epoch 80, training loss: 2.4238123893737793 = 1.7341610193252563 + 0.1 * 6.896513938903809
Epoch 80, val loss: 1.7507500648498535
Epoch 90, training loss: 2.367414951324463 = 1.689259648323059 + 0.1 * 6.781553745269775
Epoch 90, val loss: 1.7107738256454468
Epoch 100, training loss: 2.2990055084228516 = 1.6270582675933838 + 0.1 * 6.719470977783203
Epoch 100, val loss: 1.6581650972366333
Epoch 110, training loss: 2.216379165649414 = 1.5485031604766846 + 0.1 * 6.678761005401611
Epoch 110, val loss: 1.5931212902069092
Epoch 120, training loss: 2.126603603363037 = 1.4611313343048096 + 0.1 * 6.654723644256592
Epoch 120, val loss: 1.5208847522735596
Epoch 130, training loss: 2.0358855724334717 = 1.3723018169403076 + 0.1 * 6.635836601257324
Epoch 130, val loss: 1.448814868927002
Epoch 140, training loss: 1.9475021362304688 = 1.2858633995056152 + 0.1 * 6.616387367248535
Epoch 140, val loss: 1.3804759979248047
Epoch 150, training loss: 1.8620195388793945 = 1.2023652791976929 + 0.1 * 6.596541881561279
Epoch 150, val loss: 1.316853404045105
Epoch 160, training loss: 1.7797200679779053 = 1.1218342781066895 + 0.1 * 6.578857898712158
Epoch 160, val loss: 1.2571934461593628
Epoch 170, training loss: 1.697737216949463 = 1.0411832332611084 + 0.1 * 6.565540313720703
Epoch 170, val loss: 1.197740912437439
Epoch 180, training loss: 1.615112543106079 = 0.9596302509307861 + 0.1 * 6.554823398590088
Epoch 180, val loss: 1.1376103162765503
Epoch 190, training loss: 1.5315563678741455 = 0.8774526119232178 + 0.1 * 6.5410380363464355
Epoch 190, val loss: 1.0767881870269775
Epoch 200, training loss: 1.449539065361023 = 0.7963757514953613 + 0.1 * 6.531632900238037
Epoch 200, val loss: 1.0172611474990845
Epoch 210, training loss: 1.3710410594940186 = 0.719043493270874 + 0.1 * 6.519975185394287
Epoch 210, val loss: 0.9613754749298096
Epoch 220, training loss: 1.2980678081512451 = 0.6478736996650696 + 0.1 * 6.5019402503967285
Epoch 220, val loss: 0.9116715788841248
Epoch 230, training loss: 1.2333500385284424 = 0.583659827709198 + 0.1 * 6.496901512145996
Epoch 230, val loss: 0.8691273927688599
Epoch 240, training loss: 1.1748627424240112 = 0.5269796252250671 + 0.1 * 6.4788312911987305
Epoch 240, val loss: 0.8344806432723999
Epoch 250, training loss: 1.1231372356414795 = 0.4767197370529175 + 0.1 * 6.464174270629883
Epoch 250, val loss: 0.8068303465843201
Epoch 260, training loss: 1.076837182044983 = 0.4318854808807373 + 0.1 * 6.449516773223877
Epoch 260, val loss: 0.785408079624176
Epoch 270, training loss: 1.0349133014678955 = 0.39123520255088806 + 0.1 * 6.4367804527282715
Epoch 270, val loss: 0.7688992619514465
Epoch 280, training loss: 0.9979103803634644 = 0.3540182411670685 + 0.1 * 6.438920974731445
Epoch 280, val loss: 0.7563526034355164
Epoch 290, training loss: 0.9617550373077393 = 0.3199273943901062 + 0.1 * 6.418276309967041
Epoch 290, val loss: 0.7472429275512695
Epoch 300, training loss: 0.9297028183937073 = 0.288473904132843 + 0.1 * 6.412289142608643
Epoch 300, val loss: 0.7406905889511108
Epoch 310, training loss: 0.8997673392295837 = 0.25967174768447876 + 0.1 * 6.400955677032471
Epoch 310, val loss: 0.7365058064460754
Epoch 320, training loss: 0.8723467588424683 = 0.23327648639678955 + 0.1 * 6.390702724456787
Epoch 320, val loss: 0.734126627445221
Epoch 330, training loss: 0.8492793440818787 = 0.20940954983234406 + 0.1 * 6.398697853088379
Epoch 330, val loss: 0.7335461378097534
Epoch 340, training loss: 0.8254854679107666 = 0.18807020783424377 + 0.1 * 6.374152183532715
Epoch 340, val loss: 0.7347094416618347
Epoch 350, training loss: 0.8076790571212769 = 0.16899389028549194 + 0.1 * 6.3868513107299805
Epoch 350, val loss: 0.7375096678733826
Epoch 360, training loss: 0.7903584241867065 = 0.15214017033576965 + 0.1 * 6.3821821212768555
Epoch 360, val loss: 0.7417548894882202
Epoch 370, training loss: 0.7721545100212097 = 0.13725197315216064 + 0.1 * 6.349025249481201
Epoch 370, val loss: 0.7473415732383728
Epoch 380, training loss: 0.7581598162651062 = 0.1240106076002121 + 0.1 * 6.34149169921875
Epoch 380, val loss: 0.7540918588638306
Epoch 390, training loss: 0.7493157386779785 = 0.11222895979881287 + 0.1 * 6.370867729187012
Epoch 390, val loss: 0.7617639899253845
Epoch 400, training loss: 0.7358124256134033 = 0.10182935744524002 + 0.1 * 6.3398308753967285
Epoch 400, val loss: 0.7699747085571289
Epoch 410, training loss: 0.72478848695755 = 0.09259843826293945 + 0.1 * 6.321900367736816
Epoch 410, val loss: 0.7786374688148499
Epoch 420, training loss: 0.7172297835350037 = 0.08438032120466232 + 0.1 * 6.328494548797607
Epoch 420, val loss: 0.7875779867172241
Epoch 430, training loss: 0.7084516286849976 = 0.07707399129867554 + 0.1 * 6.313776016235352
Epoch 430, val loss: 0.796604573726654
Epoch 440, training loss: 0.7011443972587585 = 0.070548415184021 + 0.1 * 6.305959701538086
Epoch 440, val loss: 0.8057149052619934
Epoch 450, training loss: 0.6952961087226868 = 0.06472203880548477 + 0.1 * 6.3057403564453125
Epoch 450, val loss: 0.8147653937339783
Epoch 460, training loss: 0.6888123750686646 = 0.05953472852706909 + 0.1 * 6.292776107788086
Epoch 460, val loss: 0.8237295150756836
Epoch 470, training loss: 0.6844074726104736 = 0.05488105118274689 + 0.1 * 6.29526424407959
Epoch 470, val loss: 0.8325964212417603
Epoch 480, training loss: 0.6794086694717407 = 0.05070246756076813 + 0.1 * 6.287062168121338
Epoch 480, val loss: 0.8414145112037659
Epoch 490, training loss: 0.6758567094802856 = 0.04695293307304382 + 0.1 * 6.289037227630615
Epoch 490, val loss: 0.8500558137893677
Epoch 500, training loss: 0.6708313822746277 = 0.04358350485563278 + 0.1 * 6.2724785804748535
Epoch 500, val loss: 0.8586925268173218
Epoch 510, training loss: 0.6681134700775146 = 0.04053729772567749 + 0.1 * 6.275761604309082
Epoch 510, val loss: 0.8672557473182678
Epoch 520, training loss: 0.6654232144355774 = 0.03779016435146332 + 0.1 * 6.276330471038818
Epoch 520, val loss: 0.8756633996963501
Epoch 530, training loss: 0.6613972187042236 = 0.03530824929475784 + 0.1 * 6.260889530181885
Epoch 530, val loss: 0.8840821981430054
Epoch 540, training loss: 0.6590185761451721 = 0.03305337578058243 + 0.1 * 6.2596516609191895
Epoch 540, val loss: 0.892385721206665
Epoch 550, training loss: 0.6565245389938354 = 0.03100411221385002 + 0.1 * 6.255204200744629
Epoch 550, val loss: 0.9005697965621948
Epoch 560, training loss: 0.6539076566696167 = 0.0291383508592844 + 0.1 * 6.247693061828613
Epoch 560, val loss: 0.9086891412734985
Epoch 570, training loss: 0.652381181716919 = 0.02743152529001236 + 0.1 * 6.249495983123779
Epoch 570, val loss: 0.916614294052124
Epoch 580, training loss: 0.6528909206390381 = 0.025870857760310173 + 0.1 * 6.270200252532959
Epoch 580, val loss: 0.9244292974472046
Epoch 590, training loss: 0.6485458016395569 = 0.02444317191839218 + 0.1 * 6.241026401519775
Epoch 590, val loss: 0.9320268630981445
Epoch 600, training loss: 0.6465495824813843 = 0.02313380502164364 + 0.1 * 6.234157562255859
Epoch 600, val loss: 0.9395919442176819
Epoch 610, training loss: 0.6451866626739502 = 0.021926535293459892 + 0.1 * 6.232601165771484
Epoch 610, val loss: 0.946911633014679
Epoch 620, training loss: 0.6436899900436401 = 0.020815609022974968 + 0.1 * 6.228743553161621
Epoch 620, val loss: 0.9542223811149597
Epoch 630, training loss: 0.6450058817863464 = 0.01978662796318531 + 0.1 * 6.25219202041626
Epoch 630, val loss: 0.9613155722618103
Epoch 640, training loss: 0.6410955786705017 = 0.018835512921214104 + 0.1 * 6.222600936889648
Epoch 640, val loss: 0.9682527780532837
Epoch 650, training loss: 0.6396693587303162 = 0.017954420298337936 + 0.1 * 6.217149257659912
Epoch 650, val loss: 0.9751614332199097
Epoch 660, training loss: 0.6384821534156799 = 0.017132921144366264 + 0.1 * 6.213491916656494
Epoch 660, val loss: 0.9819245338439941
Epoch 670, training loss: 0.6386308670043945 = 0.016367344185709953 + 0.1 * 6.222635269165039
Epoch 670, val loss: 0.9884474873542786
Epoch 680, training loss: 0.6372276544570923 = 0.015655167400836945 + 0.1 * 6.215724945068359
Epoch 680, val loss: 0.9949478507041931
Epoch 690, training loss: 0.6363069415092468 = 0.014991051517426968 + 0.1 * 6.213159084320068
Epoch 690, val loss: 1.0011646747589111
Epoch 700, training loss: 0.6351313591003418 = 0.014372513629496098 + 0.1 * 6.207588195800781
Epoch 700, val loss: 1.0074291229248047
Epoch 710, training loss: 0.6346226930618286 = 0.013792124576866627 + 0.1 * 6.208305835723877
Epoch 710, val loss: 1.013410210609436
Epoch 720, training loss: 0.6332575678825378 = 0.013249141164124012 + 0.1 * 6.2000837326049805
Epoch 720, val loss: 1.019346833229065
Epoch 730, training loss: 0.6324092149734497 = 0.012738330289721489 + 0.1 * 6.196708679199219
Epoch 730, val loss: 1.0251309871673584
Epoch 740, training loss: 0.6321296691894531 = 0.012256932444870472 + 0.1 * 6.198727607727051
Epoch 740, val loss: 1.030701756477356
Epoch 750, training loss: 0.6316691040992737 = 0.011805180460214615 + 0.1 * 6.198639392852783
Epoch 750, val loss: 1.0362498760223389
Epoch 760, training loss: 0.6306906938552856 = 0.011379873380064964 + 0.1 * 6.193107604980469
Epoch 760, val loss: 1.0417091846466064
Epoch 770, training loss: 0.6300184726715088 = 0.0109790600836277 + 0.1 * 6.190393924713135
Epoch 770, val loss: 1.0470391511917114
Epoch 780, training loss: 0.6303112506866455 = 0.010599656961858273 + 0.1 * 6.197115898132324
Epoch 780, val loss: 1.0522056818008423
Epoch 790, training loss: 0.6288688778877258 = 0.010241158306598663 + 0.1 * 6.186277389526367
Epoch 790, val loss: 1.0573127269744873
Epoch 800, training loss: 0.62931889295578 = 0.009901972487568855 + 0.1 * 6.194168567657471
Epoch 800, val loss: 1.0622981786727905
Epoch 810, training loss: 0.6278991103172302 = 0.009580821730196476 + 0.1 * 6.183182716369629
Epoch 810, val loss: 1.0671799182891846
Epoch 820, training loss: 0.6277571320533752 = 0.009277073666453362 + 0.1 * 6.184800624847412
Epoch 820, val loss: 1.072062373161316
Epoch 830, training loss: 0.6263737678527832 = 0.00898757390677929 + 0.1 * 6.173861980438232
Epoch 830, val loss: 1.0767831802368164
Epoch 840, training loss: 0.6279543042182922 = 0.008712868206202984 + 0.1 * 6.192413806915283
Epoch 840, val loss: 1.0814461708068848
Epoch 850, training loss: 0.6263442039489746 = 0.00845104455947876 + 0.1 * 6.17893123626709
Epoch 850, val loss: 1.0859524011611938
Epoch 860, training loss: 0.626383900642395 = 0.008202889002859592 + 0.1 * 6.181809902191162
Epoch 860, val loss: 1.0904592275619507
Epoch 870, training loss: 0.6249769330024719 = 0.007965628989040852 + 0.1 * 6.1701130867004395
Epoch 870, val loss: 1.0948392152786255
Epoch 880, training loss: 0.6257632374763489 = 0.0077399141155183315 + 0.1 * 6.180233001708984
Epoch 880, val loss: 1.0991530418395996
Epoch 890, training loss: 0.624290406703949 = 0.007524620275944471 + 0.1 * 6.167657852172852
Epoch 890, val loss: 1.1033530235290527
Epoch 900, training loss: 0.6239136457443237 = 0.0073193940334022045 + 0.1 * 6.165942668914795
Epoch 900, val loss: 1.1076042652130127
Epoch 910, training loss: 0.6244404911994934 = 0.007122852373868227 + 0.1 * 6.173176288604736
Epoch 910, val loss: 1.1116169691085815
Epoch 920, training loss: 0.6231057643890381 = 0.006935397163033485 + 0.1 * 6.161703586578369
Epoch 920, val loss: 1.1156505346298218
Epoch 930, training loss: 0.6224028468132019 = 0.0067564998753368855 + 0.1 * 6.156463623046875
Epoch 930, val loss: 1.1196752786636353
Epoch 940, training loss: 0.6236708164215088 = 0.006584601476788521 + 0.1 * 6.170862197875977
Epoch 940, val loss: 1.1236037015914917
Epoch 950, training loss: 0.6225763559341431 = 0.006419094279408455 + 0.1 * 6.161571979522705
Epoch 950, val loss: 1.1272772550582886
Epoch 960, training loss: 0.6222660541534424 = 0.006261762231588364 + 0.1 * 6.160042762756348
Epoch 960, val loss: 1.1311272382736206
Epoch 970, training loss: 0.6225822567939758 = 0.006110059563070536 + 0.1 * 6.164721965789795
Epoch 970, val loss: 1.1348103284835815
Epoch 980, training loss: 0.6212220788002014 = 0.0059648798778653145 + 0.1 * 6.152572154998779
Epoch 980, val loss: 1.1384979486465454
Epoch 990, training loss: 0.6214338541030884 = 0.005825199652463198 + 0.1 * 6.156085968017578
Epoch 990, val loss: 1.1421070098876953
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7786
Flip ASR: 0.7333/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.8021349906921387 = 1.9647507667541504 + 0.1 * 8.373842239379883
Epoch 0, val loss: 1.9640620946884155
Epoch 10, training loss: 2.790815591812134 = 1.9534722566604614 + 0.1 * 8.373433113098145
Epoch 10, val loss: 1.9522523880004883
Epoch 20, training loss: 2.777019500732422 = 1.9398430585861206 + 0.1 * 8.371764183044434
Epoch 20, val loss: 1.937632441520691
Epoch 30, training loss: 2.7578084468841553 = 1.9211012125015259 + 0.1 * 8.367072105407715
Epoch 30, val loss: 1.9174113273620605
Epoch 40, training loss: 2.7276222705841064 = 1.8938446044921875 + 0.1 * 8.337777137756348
Epoch 40, val loss: 1.888391375541687
Epoch 50, training loss: 2.6678576469421387 = 1.8564271926879883 + 0.1 * 8.114304542541504
Epoch 50, val loss: 1.8505189418792725
Epoch 60, training loss: 2.5757861137390137 = 1.8145283460617065 + 0.1 * 7.61257791519165
Epoch 60, val loss: 1.8109668493270874
Epoch 70, training loss: 2.500748634338379 = 1.7716257572174072 + 0.1 * 7.291229724884033
Epoch 70, val loss: 1.7724155187606812
Epoch 80, training loss: 2.4348912239074707 = 1.730596661567688 + 0.1 * 7.042945861816406
Epoch 80, val loss: 1.738288402557373
Epoch 90, training loss: 2.370150566101074 = 1.6867008209228516 + 0.1 * 6.834496974945068
Epoch 90, val loss: 1.7006031274795532
Epoch 100, training loss: 2.300870656967163 = 1.6279376745224 + 0.1 * 6.729330062866211
Epoch 100, val loss: 1.6502609252929688
Epoch 110, training loss: 2.2210445404052734 = 1.5527106523513794 + 0.1 * 6.683339595794678
Epoch 110, val loss: 1.5889512300491333
Epoch 120, training loss: 2.131129741668701 = 1.4657191038131714 + 0.1 * 6.654107570648193
Epoch 120, val loss: 1.5207666158676147
Epoch 130, training loss: 2.0416100025177 = 1.378292441368103 + 0.1 * 6.633176326751709
Epoch 130, val loss: 1.4555755853652954
Epoch 140, training loss: 1.9599131345748901 = 1.298463225364685 + 0.1 * 6.614499092102051
Epoch 140, val loss: 1.4001867771148682
Epoch 150, training loss: 1.8858740329742432 = 1.2262176275253296 + 0.1 * 6.596564769744873
Epoch 150, val loss: 1.3533802032470703
Epoch 160, training loss: 1.8174304962158203 = 1.1593290567398071 + 0.1 * 6.581015110015869
Epoch 160, val loss: 1.311374545097351
Epoch 170, training loss: 1.7508561611175537 = 1.0945861339569092 + 0.1 * 6.562700271606445
Epoch 170, val loss: 1.271285891532898
Epoch 180, training loss: 1.6843023300170898 = 1.0296143293380737 + 0.1 * 6.546879291534424
Epoch 180, val loss: 1.2318792343139648
Epoch 190, training loss: 1.6179139614105225 = 0.9642277956008911 + 0.1 * 6.536862373352051
Epoch 190, val loss: 1.1921437978744507
Epoch 200, training loss: 1.5508917570114136 = 0.8987814784049988 + 0.1 * 6.521102428436279
Epoch 200, val loss: 1.1508210897445679
Epoch 210, training loss: 1.4858561754226685 = 0.8335595726966858 + 0.1 * 6.522965908050537
Epoch 210, val loss: 1.1071518659591675
Epoch 220, training loss: 1.4203089475631714 = 0.7702428102493286 + 0.1 * 6.500661373138428
Epoch 220, val loss: 1.06215238571167
Epoch 230, training loss: 1.3587030172348022 = 0.7093961238861084 + 0.1 * 6.493068695068359
Epoch 230, val loss: 1.0176763534545898
Epoch 240, training loss: 1.3000829219818115 = 0.6517906188964844 + 0.1 * 6.48292350769043
Epoch 240, val loss: 0.9749776124954224
Epoch 250, training loss: 1.2440046072006226 = 0.5966587066650391 + 0.1 * 6.473458766937256
Epoch 250, val loss: 0.9346935153007507
Epoch 260, training loss: 1.1916673183441162 = 0.5440280437469482 + 0.1 * 6.476391792297363
Epoch 260, val loss: 0.8978952765464783
Epoch 270, training loss: 1.140028476715088 = 0.4939365088939667 + 0.1 * 6.460919380187988
Epoch 270, val loss: 0.8659906983375549
Epoch 280, training loss: 1.094597578048706 = 0.4461321234703064 + 0.1 * 6.484654903411865
Epoch 280, val loss: 0.8391285538673401
Epoch 290, training loss: 1.0461175441741943 = 0.40193235874176025 + 0.1 * 6.4418511390686035
Epoch 290, val loss: 0.8185631036758423
Epoch 300, training loss: 1.004934549331665 = 0.36165669560432434 + 0.1 * 6.432778835296631
Epoch 300, val loss: 0.8038405776023865
Epoch 310, training loss: 0.9694949388504028 = 0.32569554448127747 + 0.1 * 6.43799352645874
Epoch 310, val loss: 0.7945047616958618
Epoch 320, training loss: 0.9358899593353271 = 0.2943294942378998 + 0.1 * 6.415604114532471
Epoch 320, val loss: 0.7897237539291382
Epoch 330, training loss: 0.9072717428207397 = 0.2664981186389923 + 0.1 * 6.407736301422119
Epoch 330, val loss: 0.7879395484924316
Epoch 340, training loss: 0.8821876049041748 = 0.2414337545633316 + 0.1 * 6.407538414001465
Epoch 340, val loss: 0.7881839871406555
Epoch 350, training loss: 0.857804000377655 = 0.21874390542507172 + 0.1 * 6.390600681304932
Epoch 350, val loss: 0.7897621989250183
Epoch 360, training loss: 0.8366246223449707 = 0.19806668162345886 + 0.1 * 6.385579586029053
Epoch 360, val loss: 0.7924210429191589
Epoch 370, training loss: 0.8169757127761841 = 0.17923250794410706 + 0.1 * 6.377431392669678
Epoch 370, val loss: 0.7960386872291565
Epoch 380, training loss: 0.7991300821304321 = 0.16214647889137268 + 0.1 * 6.36983585357666
Epoch 380, val loss: 0.8005944490432739
Epoch 390, training loss: 0.7833431363105774 = 0.14672064781188965 + 0.1 * 6.366224765777588
Epoch 390, val loss: 0.8060838580131531
Epoch 400, training loss: 0.7696902751922607 = 0.13295121490955353 + 0.1 * 6.3673906326293945
Epoch 400, val loss: 0.8123810887336731
Epoch 410, training loss: 0.7554711103439331 = 0.12072892487049103 + 0.1 * 6.347422122955322
Epoch 410, val loss: 0.8196277618408203
Epoch 420, training loss: 0.7443527579307556 = 0.1098678782582283 + 0.1 * 6.3448486328125
Epoch 420, val loss: 0.827816367149353
Epoch 430, training loss: 0.7346245050430298 = 0.10027182847261429 + 0.1 * 6.343526840209961
Epoch 430, val loss: 0.8368275761604309
Epoch 440, training loss: 0.7253707647323608 = 0.0918017104268074 + 0.1 * 6.335690498352051
Epoch 440, val loss: 0.8466123938560486
Epoch 450, training loss: 0.717396080493927 = 0.08430323749780655 + 0.1 * 6.330927848815918
Epoch 450, val loss: 0.8569408059120178
Epoch 460, training loss: 0.7096937894821167 = 0.07764522731304169 + 0.1 * 6.3204851150512695
Epoch 460, val loss: 0.8677554130554199
Epoch 470, training loss: 0.7047526836395264 = 0.07170373946428299 + 0.1 * 6.330489635467529
Epoch 470, val loss: 0.8787845373153687
Epoch 480, training loss: 0.698154091835022 = 0.06641168147325516 + 0.1 * 6.3174238204956055
Epoch 480, val loss: 0.8900435566902161
Epoch 490, training loss: 0.6927565336227417 = 0.06166175752878189 + 0.1 * 6.310947418212891
Epoch 490, val loss: 0.9013745784759521
Epoch 500, training loss: 0.688921332359314 = 0.057367678731679916 + 0.1 * 6.3155364990234375
Epoch 500, val loss: 0.9127461314201355
Epoch 510, training loss: 0.683824360370636 = 0.053486015647649765 + 0.1 * 6.3033833503723145
Epoch 510, val loss: 0.9241483211517334
Epoch 520, training loss: 0.6797917485237122 = 0.04996214434504509 + 0.1 * 6.298295974731445
Epoch 520, val loss: 0.9354347586631775
Epoch 530, training loss: 0.6763694882392883 = 0.04675685986876488 + 0.1 * 6.296125888824463
Epoch 530, val loss: 0.9466571807861328
Epoch 540, training loss: 0.6735290884971619 = 0.04383628070354462 + 0.1 * 6.2969279289245605
Epoch 540, val loss: 0.9577877521514893
Epoch 550, training loss: 0.6701579689979553 = 0.04116392135620117 + 0.1 * 6.289940357208252
Epoch 550, val loss: 0.9687047600746155
Epoch 560, training loss: 0.6666122078895569 = 0.03872055932879448 + 0.1 * 6.278916358947754
Epoch 560, val loss: 0.9796406030654907
Epoch 570, training loss: 0.6644642949104309 = 0.03647150471806526 + 0.1 * 6.279927730560303
Epoch 570, val loss: 0.9903120398521423
Epoch 580, training loss: 0.6623898148536682 = 0.034402940422296524 + 0.1 * 6.279868125915527
Epoch 580, val loss: 1.0008152723312378
Epoch 590, training loss: 0.6595118045806885 = 0.03250127285718918 + 0.1 * 6.270105361938477
Epoch 590, val loss: 1.0112807750701904
Epoch 600, training loss: 0.6582334041595459 = 0.0307435542345047 + 0.1 * 6.274898529052734
Epoch 600, val loss: 1.0214191675186157
Epoch 610, training loss: 0.656785249710083 = 0.029122181236743927 + 0.1 * 6.276630401611328
Epoch 610, val loss: 1.0314968824386597
Epoch 620, training loss: 0.6540583968162537 = 0.02762071043252945 + 0.1 * 6.264376640319824
Epoch 620, val loss: 1.0412979125976562
Epoch 630, training loss: 0.6516786813735962 = 0.02623181790113449 + 0.1 * 6.2544684410095215
Epoch 630, val loss: 1.0509893894195557
Epoch 640, training loss: 0.6502997875213623 = 0.024940304458141327 + 0.1 * 6.253594398498535
Epoch 640, val loss: 1.0604568719863892
Epoch 650, training loss: 0.6494422554969788 = 0.023739691823720932 + 0.1 * 6.257025718688965
Epoch 650, val loss: 1.0696710348129272
Epoch 660, training loss: 0.6476389765739441 = 0.02262684516608715 + 0.1 * 6.250121593475342
Epoch 660, val loss: 1.078839659690857
Epoch 670, training loss: 0.6458792090415955 = 0.021588105708360672 + 0.1 * 6.242910861968994
Epoch 670, val loss: 1.0877165794372559
Epoch 680, training loss: 0.6445999145507812 = 0.02061830461025238 + 0.1 * 6.239815711975098
Epoch 680, val loss: 1.0965379476547241
Epoch 690, training loss: 0.6442975401878357 = 0.019710438326001167 + 0.1 * 6.245871067047119
Epoch 690, val loss: 1.1050920486450195
Epoch 700, training loss: 0.6421677470207214 = 0.018864603713154793 + 0.1 * 6.233030796051025
Epoch 700, val loss: 1.1135785579681396
Epoch 710, training loss: 0.6411224007606506 = 0.018073908984661102 + 0.1 * 6.230484962463379
Epoch 710, val loss: 1.1219264268875122
Epoch 720, training loss: 0.64043128490448 = 0.01733117178082466 + 0.1 * 6.231000900268555
Epoch 720, val loss: 1.1299717426300049
Epoch 730, training loss: 0.6405701041221619 = 0.016635892912745476 + 0.1 * 6.239341735839844
Epoch 730, val loss: 1.1380175352096558
Epoch 740, training loss: 0.6378762125968933 = 0.015982601791620255 + 0.1 * 6.218935489654541
Epoch 740, val loss: 1.1458804607391357
Epoch 750, training loss: 0.6376668810844421 = 0.015368716791272163 + 0.1 * 6.222981929779053
Epoch 750, val loss: 1.153630018234253
Epoch 760, training loss: 0.6367875933647156 = 0.01478912029415369 + 0.1 * 6.219984531402588
Epoch 760, val loss: 1.1610791683197021
Epoch 770, training loss: 0.635958731174469 = 0.014243646524846554 + 0.1 * 6.217150688171387
Epoch 770, val loss: 1.16851806640625
Epoch 780, training loss: 0.6343998908996582 = 0.01372991967946291 + 0.1 * 6.206699371337891
Epoch 780, val loss: 1.1758230924606323
Epoch 790, training loss: 0.6353464722633362 = 0.013243209570646286 + 0.1 * 6.221032619476318
Epoch 790, val loss: 1.1829125881195068
Epoch 800, training loss: 0.6341270804405212 = 0.01278378814458847 + 0.1 * 6.213433265686035
Epoch 800, val loss: 1.1898783445358276
Epoch 810, training loss: 0.6329902410507202 = 0.012349551543593407 + 0.1 * 6.206406593322754
Epoch 810, val loss: 1.1967930793762207
Epoch 820, training loss: 0.6317230463027954 = 0.011937429197132587 + 0.1 * 6.1978559494018555
Epoch 820, val loss: 1.2034060955047607
Epoch 830, training loss: 0.6321201920509338 = 0.011547418311238289 + 0.1 * 6.205727577209473
Epoch 830, val loss: 1.21011221408844
Epoch 840, training loss: 0.6306716203689575 = 0.011176647618412971 + 0.1 * 6.194949626922607
Epoch 840, val loss: 1.2164555788040161
Epoch 850, training loss: 0.6299285292625427 = 0.010826059617102146 + 0.1 * 6.1910247802734375
Epoch 850, val loss: 1.2228769063949585
Epoch 860, training loss: 0.6305140852928162 = 0.010491657070815563 + 0.1 * 6.200223922729492
Epoch 860, val loss: 1.2290890216827393
Epoch 870, training loss: 0.6288236379623413 = 0.010172666981816292 + 0.1 * 6.186509609222412
Epoch 870, val loss: 1.2351408004760742
Epoch 880, training loss: 0.6287181973457336 = 0.00987036433070898 + 0.1 * 6.188478469848633
Epoch 880, val loss: 1.241319179534912
Epoch 890, training loss: 0.628288745880127 = 0.009580690413713455 + 0.1 * 6.1870808601379395
Epoch 890, val loss: 1.247156023979187
Epoch 900, training loss: 0.6290203332901001 = 0.009304306469857693 + 0.1 * 6.197160243988037
Epoch 900, val loss: 1.2528729438781738
Epoch 910, training loss: 0.6271896362304688 = 0.009042073972523212 + 0.1 * 6.181475639343262
Epoch 910, val loss: 1.2587369680404663
Epoch 920, training loss: 0.6266994476318359 = 0.008791963569819927 + 0.1 * 6.179074287414551
Epoch 920, val loss: 1.264372706413269
Epoch 930, training loss: 0.6266388297080994 = 0.008552038110792637 + 0.1 * 6.180868148803711
Epoch 930, val loss: 1.2698880434036255
Epoch 940, training loss: 0.6253142952919006 = 0.008322782814502716 + 0.1 * 6.169915199279785
Epoch 940, val loss: 1.2753229141235352
Epoch 950, training loss: 0.626987874507904 = 0.00810336135327816 + 0.1 * 6.188844680786133
Epoch 950, val loss: 1.2807512283325195
Epoch 960, training loss: 0.6261380314826965 = 0.007892281748354435 + 0.1 * 6.182456970214844
Epoch 960, val loss: 1.2859963178634644
Epoch 970, training loss: 0.6247013807296753 = 0.007690852507948875 + 0.1 * 6.17010498046875
Epoch 970, val loss: 1.2911736965179443
Epoch 980, training loss: 0.6244070529937744 = 0.007498828694224358 + 0.1 * 6.1690826416015625
Epoch 980, val loss: 1.2964285612106323
Epoch 990, training loss: 0.6243948936462402 = 0.007314010057598352 + 0.1 * 6.170808792114258
Epoch 990, val loss: 1.3014382123947144
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.78106, 0.16271, Accuracy:0.81111, 0.00605
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11580])
remove edge: torch.Size([2, 9414])
updated graph: torch.Size([2, 10438])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98032, 0.00174, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.770963191986084 = 1.9335758686065674 + 0.1 * 8.373873710632324
Epoch 0, val loss: 1.927018642425537
Epoch 10, training loss: 2.760425329208374 = 1.923050045967102 + 0.1 * 8.37375259399414
Epoch 10, val loss: 1.9174031019210815
Epoch 20, training loss: 2.747194290161133 = 1.9098786115646362 + 0.1 * 8.37315559387207
Epoch 20, val loss: 1.9049867391586304
Epoch 30, training loss: 2.728264808654785 = 1.891392469406128 + 0.1 * 8.368722915649414
Epoch 30, val loss: 1.8874027729034424
Epoch 40, training loss: 2.6986398696899414 = 1.8646693229675293 + 0.1 * 8.339704513549805
Epoch 40, val loss: 1.8623756170272827
Epoch 50, training loss: 2.6490278244018555 = 1.8294808864593506 + 0.1 * 8.195467948913574
Epoch 50, val loss: 1.8314565420150757
Epoch 60, training loss: 2.5768680572509766 = 1.7909247875213623 + 0.1 * 7.859432697296143
Epoch 60, val loss: 1.8000236749649048
Epoch 70, training loss: 2.5155389308929443 = 1.7497658729553223 + 0.1 * 7.657730579376221
Epoch 70, val loss: 1.7649308443069458
Epoch 80, training loss: 2.439112424850464 = 1.6985670328140259 + 0.1 * 7.405454158782959
Epoch 80, val loss: 1.719772219657898
Epoch 90, training loss: 2.3460779190063477 = 1.635358452796936 + 0.1 * 7.107194423675537
Epoch 90, val loss: 1.6651153564453125
Epoch 100, training loss: 2.2433552742004395 = 1.556713342666626 + 0.1 * 6.86641788482666
Epoch 100, val loss: 1.5986547470092773
Epoch 110, training loss: 2.1438357830047607 = 1.464600682258606 + 0.1 * 6.7923502922058105
Epoch 110, val loss: 1.5225023031234741
Epoch 120, training loss: 2.0477304458618164 = 1.3725762367248535 + 0.1 * 6.751543045043945
Epoch 120, val loss: 1.451051950454712
Epoch 130, training loss: 1.9611250162124634 = 1.289104700088501 + 0.1 * 6.720202922821045
Epoch 130, val loss: 1.3903093338012695
Epoch 140, training loss: 1.8851511478424072 = 1.215314269065857 + 0.1 * 6.698368549346924
Epoch 140, val loss: 1.341903567314148
Epoch 150, training loss: 1.8161969184875488 = 1.1478770971298218 + 0.1 * 6.683197975158691
Epoch 150, val loss: 1.3011507987976074
Epoch 160, training loss: 1.7490766048431396 = 1.0826424360275269 + 0.1 * 6.664341926574707
Epoch 160, val loss: 1.2626162767410278
Epoch 170, training loss: 1.681082844734192 = 1.0162286758422852 + 0.1 * 6.648541450500488
Epoch 170, val loss: 1.2232242822647095
Epoch 180, training loss: 1.611372709274292 = 0.9478753209114075 + 0.1 * 6.634974479675293
Epoch 180, val loss: 1.1817899942398071
Epoch 190, training loss: 1.540982961654663 = 0.8785103559494019 + 0.1 * 6.62472677230835
Epoch 190, val loss: 1.1386198997497559
Epoch 200, training loss: 1.472179651260376 = 0.8100733757019043 + 0.1 * 6.6210618019104
Epoch 200, val loss: 1.0953859090805054
Epoch 210, training loss: 1.4046710729599 = 0.7432174682617188 + 0.1 * 6.614535808563232
Epoch 210, val loss: 1.0521706342697144
Epoch 220, training loss: 1.3386050462722778 = 0.6775878667831421 + 0.1 * 6.610171794891357
Epoch 220, val loss: 1.0093241930007935
Epoch 230, training loss: 1.273503303527832 = 0.6128594875335693 + 0.1 * 6.606437683105469
Epoch 230, val loss: 0.9678561687469482
Epoch 240, training loss: 1.2090766429901123 = 0.5488116145133972 + 0.1 * 6.602650165557861
Epoch 240, val loss: 0.9288838505744934
Epoch 250, training loss: 1.146371603012085 = 0.48577943444252014 + 0.1 * 6.605921268463135
Epoch 250, val loss: 0.8936813473701477
Epoch 260, training loss: 1.085030436515808 = 0.42547479271888733 + 0.1 * 6.595556735992432
Epoch 260, val loss: 0.8638741970062256
Epoch 270, training loss: 1.0280499458312988 = 0.3690880537033081 + 0.1 * 6.589619159698486
Epoch 270, val loss: 0.8398387432098389
Epoch 280, training loss: 0.9761159420013428 = 0.31797847151756287 + 0.1 * 6.5813751220703125
Epoch 280, val loss: 0.8220717310905457
Epoch 290, training loss: 0.930992603302002 = 0.27312615513801575 + 0.1 * 6.578664302825928
Epoch 290, val loss: 0.8102834224700928
Epoch 300, training loss: 0.8917316198348999 = 0.2350238561630249 + 0.1 * 6.56707763671875
Epoch 300, val loss: 0.803930938243866
Epoch 310, training loss: 0.8594472408294678 = 0.20316824316978455 + 0.1 * 6.562789440155029
Epoch 310, val loss: 0.8019097447395325
Epoch 320, training loss: 0.8313559293746948 = 0.17669259011745453 + 0.1 * 6.546633243560791
Epoch 320, val loss: 0.803409218788147
Epoch 330, training loss: 0.8080422878265381 = 0.15447059273719788 + 0.1 * 6.535717010498047
Epoch 330, val loss: 0.808038055896759
Epoch 340, training loss: 0.7892910242080688 = 0.135781928896904 + 0.1 * 6.535090446472168
Epoch 340, val loss: 0.8147580623626709
Epoch 350, training loss: 0.7721461057662964 = 0.11995938420295715 + 0.1 * 6.521867275238037
Epoch 350, val loss: 0.8230584859848022
Epoch 360, training loss: 0.7575319409370422 = 0.10638487339019775 + 0.1 * 6.511470317840576
Epoch 360, val loss: 0.83269864320755
Epoch 370, training loss: 0.7462599277496338 = 0.09467673301696777 + 0.1 * 6.51583194732666
Epoch 370, val loss: 0.843161404132843
Epoch 380, training loss: 0.7345009446144104 = 0.08462183177471161 + 0.1 * 6.498790740966797
Epoch 380, val loss: 0.8540650010108948
Epoch 390, training loss: 0.7250438928604126 = 0.07590266317129135 + 0.1 * 6.491412162780762
Epoch 390, val loss: 0.8653513789176941
Epoch 400, training loss: 0.7168090343475342 = 0.0682963952422142 + 0.1 * 6.485126495361328
Epoch 400, val loss: 0.876825213432312
Epoch 410, training loss: 0.7093995809555054 = 0.06166599318385124 + 0.1 * 6.477335453033447
Epoch 410, val loss: 0.8883762955665588
Epoch 420, training loss: 0.7028319239616394 = 0.055878423154354095 + 0.1 * 6.4695353507995605
Epoch 420, val loss: 0.8998761773109436
Epoch 430, training loss: 0.6988957524299622 = 0.05080640688538551 + 0.1 * 6.480893135070801
Epoch 430, val loss: 0.9111605882644653
Epoch 440, training loss: 0.69181227684021 = 0.04637490212917328 + 0.1 * 6.454373359680176
Epoch 440, val loss: 0.9225434064865112
Epoch 450, training loss: 0.6881144642829895 = 0.04247795045375824 + 0.1 * 6.45636510848999
Epoch 450, val loss: 0.9334632158279419
Epoch 460, training loss: 0.6834431290626526 = 0.03906060382723808 + 0.1 * 6.4438252449035645
Epoch 460, val loss: 0.9443743824958801
Epoch 470, training loss: 0.6802541613578796 = 0.03603794798254967 + 0.1 * 6.44216251373291
Epoch 470, val loss: 0.9546899795532227
Epoch 480, training loss: 0.676236093044281 = 0.03335833176970482 + 0.1 * 6.42877721786499
Epoch 480, val loss: 0.9650071859359741
Epoch 490, training loss: 0.6731149554252625 = 0.030965784564614296 + 0.1 * 6.421491622924805
Epoch 490, val loss: 0.9750186204910278
Epoch 500, training loss: 0.6724857687950134 = 0.02882847934961319 + 0.1 * 6.436572551727295
Epoch 500, val loss: 0.9845468997955322
Epoch 510, training loss: 0.6666684150695801 = 0.026922965422272682 + 0.1 * 6.397454738616943
Epoch 510, val loss: 0.9940798282623291
Epoch 520, training loss: 0.6644842624664307 = 0.025204841047525406 + 0.1 * 6.392794132232666
Epoch 520, val loss: 1.003047227859497
Epoch 530, training loss: 0.6625888347625732 = 0.023650480434298515 + 0.1 * 6.389383792877197
Epoch 530, val loss: 1.0116914510726929
Epoch 540, training loss: 0.6605269908905029 = 0.02224714122712612 + 0.1 * 6.382798671722412
Epoch 540, val loss: 1.0204561948776245
Epoch 550, training loss: 0.659942626953125 = 0.02096860483288765 + 0.1 * 6.389739990234375
Epoch 550, val loss: 1.0285913944244385
Epoch 560, training loss: 0.6561133861541748 = 0.01980593241751194 + 0.1 * 6.363074779510498
Epoch 560, val loss: 1.0367674827575684
Epoch 570, training loss: 0.6553084850311279 = 0.018741467967629433 + 0.1 * 6.3656697273254395
Epoch 570, val loss: 1.0445866584777832
Epoch 580, training loss: 0.6537755131721497 = 0.017765620723366737 + 0.1 * 6.360098838806152
Epoch 580, val loss: 1.052049160003662
Epoch 590, training loss: 0.6544978618621826 = 0.016869593411684036 + 0.1 * 6.376282215118408
Epoch 590, val loss: 1.059444546699524
Epoch 600, training loss: 0.6505638957023621 = 0.016046717762947083 + 0.1 * 6.345171928405762
Epoch 600, val loss: 1.0666943788528442
Epoch 610, training loss: 0.6491321325302124 = 0.015285640023648739 + 0.1 * 6.338465213775635
Epoch 610, val loss: 1.073664903640747
Epoch 620, training loss: 0.6490602493286133 = 0.014580580405890942 + 0.1 * 6.344796180725098
Epoch 620, val loss: 1.080337643623352
Epoch 630, training loss: 0.646834135055542 = 0.013927999883890152 + 0.1 * 6.329061508178711
Epoch 630, val loss: 1.0870829820632935
Epoch 640, training loss: 0.6449061632156372 = 0.013321209698915482 + 0.1 * 6.315849304199219
Epoch 640, val loss: 1.0936230421066284
Epoch 650, training loss: 0.6453648209571838 = 0.01275539305061102 + 0.1 * 6.326094150543213
Epoch 650, val loss: 1.0996683835983276
Epoch 660, training loss: 0.6432430148124695 = 0.012230227701365948 + 0.1 * 6.3101277351379395
Epoch 660, val loss: 1.1060256958007812
Epoch 670, training loss: 0.643132209777832 = 0.011738897301256657 + 0.1 * 6.3139328956604
Epoch 670, val loss: 1.1119011640548706
Epoch 680, training loss: 0.6424518823623657 = 0.011279028840363026 + 0.1 * 6.311728000640869
Epoch 680, val loss: 1.1178669929504395
Epoch 690, training loss: 0.6401196718215942 = 0.010849853977560997 + 0.1 * 6.292698383331299
Epoch 690, val loss: 1.1233261823654175
Epoch 700, training loss: 0.6392158269882202 = 0.010447253473103046 + 0.1 * 6.287685394287109
Epoch 700, val loss: 1.129120945930481
Epoch 710, training loss: 0.6395370960235596 = 0.010067309252917767 + 0.1 * 6.294698238372803
Epoch 710, val loss: 1.1344701051712036
Epoch 720, training loss: 0.6390447020530701 = 0.009710283949971199 + 0.1 * 6.293343544006348
Epoch 720, val loss: 1.139649510383606
Epoch 730, training loss: 0.6370225548744202 = 0.009374496527016163 + 0.1 * 6.276480674743652
Epoch 730, val loss: 1.1450464725494385
Epoch 740, training loss: 0.6374958157539368 = 0.009057141840457916 + 0.1 * 6.284386157989502
Epoch 740, val loss: 1.1500376462936401
Epoch 750, training loss: 0.6365355849266052 = 0.00875782873481512 + 0.1 * 6.277777194976807
Epoch 750, val loss: 1.1551507711410522
Epoch 760, training loss: 0.6348026990890503 = 0.008474715985357761 + 0.1 * 6.263279914855957
Epoch 760, val loss: 1.1598379611968994
Epoch 770, training loss: 0.6350882053375244 = 0.008207157254219055 + 0.1 * 6.268810749053955
Epoch 770, val loss: 1.164769172668457
Epoch 780, training loss: 0.6348699927330017 = 0.007952895946800709 + 0.1 * 6.269171237945557
Epoch 780, val loss: 1.1695138216018677
Epoch 790, training loss: 0.6355591416358948 = 0.007711406331509352 + 0.1 * 6.278477191925049
Epoch 790, val loss: 1.1739102602005005
Epoch 800, training loss: 0.6329368352890015 = 0.007482956629246473 + 0.1 * 6.254538536071777
Epoch 800, val loss: 1.1785264015197754
Epoch 810, training loss: 0.6344757080078125 = 0.007265252526849508 + 0.1 * 6.272104263305664
Epoch 810, val loss: 1.1830393075942993
Epoch 820, training loss: 0.6328394412994385 = 0.007058274000883102 + 0.1 * 6.257811546325684
Epoch 820, val loss: 1.1871715784072876
Epoch 830, training loss: 0.6328220367431641 = 0.006861275993287563 + 0.1 * 6.259607315063477
Epoch 830, val loss: 1.1916494369506836
Epoch 840, training loss: 0.6312587857246399 = 0.006673025898635387 + 0.1 * 6.245857238769531
Epoch 840, val loss: 1.1956483125686646
Epoch 850, training loss: 0.6324001550674438 = 0.006493747234344482 + 0.1 * 6.259064197540283
Epoch 850, val loss: 1.1999256610870361
Epoch 860, training loss: 0.6316691040992737 = 0.00632252125069499 + 0.1 * 6.2534661293029785
Epoch 860, val loss: 1.2037538290023804
Epoch 870, training loss: 0.630730152130127 = 0.006159266922622919 + 0.1 * 6.24570894241333
Epoch 870, val loss: 1.2077124118804932
Epoch 880, training loss: 0.629610538482666 = 0.006003002170473337 + 0.1 * 6.236074924468994
Epoch 880, val loss: 1.2118252515792847
Epoch 890, training loss: 0.6291974186897278 = 0.005852948408573866 + 0.1 * 6.233444690704346
Epoch 890, val loss: 1.2154011726379395
Epoch 900, training loss: 0.6288957595825195 = 0.005709459073841572 + 0.1 * 6.231862545013428
Epoch 900, val loss: 1.219253659248352
Epoch 910, training loss: 0.6302379965782166 = 0.005571574438363314 + 0.1 * 6.246664047241211
Epoch 910, val loss: 1.2229636907577515
Epoch 920, training loss: 0.6291870474815369 = 0.005440013483166695 + 0.1 * 6.2374701499938965
Epoch 920, val loss: 1.226399540901184
Epoch 930, training loss: 0.6279301047325134 = 0.005314202047884464 + 0.1 * 6.226158618927002
Epoch 930, val loss: 1.2300231456756592
Epoch 940, training loss: 0.628415584564209 = 0.005193231627345085 + 0.1 * 6.2322235107421875
Epoch 940, val loss: 1.2336736917495728
Epoch 950, training loss: 0.6269667744636536 = 0.0050765350461006165 + 0.1 * 6.218902111053467
Epoch 950, val loss: 1.2369459867477417
Epoch 960, training loss: 0.6266122460365295 = 0.004964475054293871 + 0.1 * 6.216477870941162
Epoch 960, val loss: 1.2404663562774658
Epoch 970, training loss: 0.6270054578781128 = 0.004856548272073269 + 0.1 * 6.221488952636719
Epoch 970, val loss: 1.2435023784637451
Epoch 980, training loss: 0.6263847351074219 = 0.004753143526613712 + 0.1 * 6.216315269470215
Epoch 980, val loss: 1.246962070465088
Epoch 990, training loss: 0.6255723237991333 = 0.004653147887438536 + 0.1 * 6.20919132232666
Epoch 990, val loss: 1.2502557039260864
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6900
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7733073234558105 = 1.9359184503555298 + 0.1 * 8.37388801574707
Epoch 0, val loss: 1.9389169216156006
Epoch 10, training loss: 2.763937473297119 = 1.9265594482421875 + 0.1 * 8.373781204223633
Epoch 10, val loss: 1.929686427116394
Epoch 20, training loss: 2.752157211303711 = 1.914829134941101 + 0.1 * 8.37328052520752
Epoch 20, val loss: 1.9176307916641235
Epoch 30, training loss: 2.7351162433624268 = 1.8981596231460571 + 0.1 * 8.369566917419434
Epoch 30, val loss: 1.9001649618148804
Epoch 40, training loss: 2.707550525665283 = 1.873611569404602 + 0.1 * 8.339388847351074
Epoch 40, val loss: 1.874393343925476
Epoch 50, training loss: 2.6476221084594727 = 1.8399337530136108 + 0.1 * 8.076884269714355
Epoch 50, val loss: 1.840061902999878
Epoch 60, training loss: 2.552140712738037 = 1.8033338785171509 + 0.1 * 7.488068580627441
Epoch 60, val loss: 1.8036764860153198
Epoch 70, training loss: 2.4791383743286133 = 1.7669624090194702 + 0.1 * 7.121758937835693
Epoch 70, val loss: 1.76750910282135
Epoch 80, training loss: 2.418520927429199 = 1.7259249687194824 + 0.1 * 6.925958633422852
Epoch 80, val loss: 1.7275538444519043
Epoch 90, training loss: 2.3571858406066895 = 1.6741803884506226 + 0.1 * 6.830053806304932
Epoch 90, val loss: 1.6788774728775024
Epoch 100, training loss: 2.2836508750915527 = 1.6049411296844482 + 0.1 * 6.7870965003967285
Epoch 100, val loss: 1.6168166399002075
Epoch 110, training loss: 2.1933298110961914 = 1.5170472860336304 + 0.1 * 6.762825012207031
Epoch 110, val loss: 1.542023777961731
Epoch 120, training loss: 2.088703155517578 = 1.4142168760299683 + 0.1 * 6.744863510131836
Epoch 120, val loss: 1.4574086666107178
Epoch 130, training loss: 1.9775521755218506 = 1.3042833805084229 + 0.1 * 6.7326884269714355
Epoch 130, val loss: 1.369546890258789
Epoch 140, training loss: 1.8677350282669067 = 1.1954147815704346 + 0.1 * 6.723202228546143
Epoch 140, val loss: 1.2852990627288818
Epoch 150, training loss: 1.7624547481536865 = 1.0909497737884521 + 0.1 * 6.715049743652344
Epoch 150, val loss: 1.206743836402893
Epoch 160, training loss: 1.6608977317810059 = 0.9901714324951172 + 0.1 * 6.70726203918457
Epoch 160, val loss: 1.1309236288070679
Epoch 170, training loss: 1.5626716613769531 = 0.8928120732307434 + 0.1 * 6.698596000671387
Epoch 170, val loss: 1.0573852062225342
Epoch 180, training loss: 1.4709187746047974 = 0.8020278811454773 + 0.1 * 6.688908576965332
Epoch 180, val loss: 0.9890772104263306
Epoch 190, training loss: 1.3884085416793823 = 0.7205964922904968 + 0.1 * 6.678120136260986
Epoch 190, val loss: 0.9287921786308289
Epoch 200, training loss: 1.316587209701538 = 0.6505683660507202 + 0.1 * 6.660188674926758
Epoch 200, val loss: 0.8795957565307617
Epoch 210, training loss: 1.2552064657211304 = 0.590826690196991 + 0.1 * 6.643797874450684
Epoch 210, val loss: 0.8410360217094421
Epoch 220, training loss: 1.2028675079345703 = 0.5398532152175903 + 0.1 * 6.630142688751221
Epoch 220, val loss: 0.8118218779563904
Epoch 230, training loss: 1.1563588380813599 = 0.49523109197616577 + 0.1 * 6.6112775802612305
Epoch 230, val loss: 0.7890074849128723
Epoch 240, training loss: 1.1150174140930176 = 0.4546724557876587 + 0.1 * 6.603449821472168
Epoch 240, val loss: 0.7702826261520386
Epoch 250, training loss: 1.0756707191467285 = 0.4173707365989685 + 0.1 * 6.582999229431152
Epoch 250, val loss: 0.7550247311592102
Epoch 260, training loss: 1.0392338037490845 = 0.38238900899887085 + 0.1 * 6.568448066711426
Epoch 260, val loss: 0.7428194880485535
Epoch 270, training loss: 1.00607168674469 = 0.34949347376823425 + 0.1 * 6.565781593322754
Epoch 270, val loss: 0.7335091829299927
Epoch 280, training loss: 0.9735592603683472 = 0.3189016282558441 + 0.1 * 6.546576023101807
Epoch 280, val loss: 0.7269495129585266
Epoch 290, training loss: 0.9445641040802002 = 0.29039841890335083 + 0.1 * 6.541656494140625
Epoch 290, val loss: 0.722537636756897
Epoch 300, training loss: 0.91664719581604 = 0.2636394798755646 + 0.1 * 6.530076503753662
Epoch 300, val loss: 0.7204667329788208
Epoch 310, training loss: 0.8917584419250488 = 0.23837777972221375 + 0.1 * 6.533806800842285
Epoch 310, val loss: 0.7205408215522766
Epoch 320, training loss: 0.86593097448349 = 0.21459288895130157 + 0.1 * 6.513381004333496
Epoch 320, val loss: 0.722458004951477
Epoch 330, training loss: 0.8428280353546143 = 0.19232651591300964 + 0.1 * 6.505014896392822
Epoch 330, val loss: 0.7260900139808655
Epoch 340, training loss: 0.8216661214828491 = 0.17192474007606506 + 0.1 * 6.4974141120910645
Epoch 340, val loss: 0.7313200831413269
Epoch 350, training loss: 0.8032800555229187 = 0.15379635989665985 + 0.1 * 6.494836807250977
Epoch 350, val loss: 0.7376925945281982
Epoch 360, training loss: 0.7872174978256226 = 0.13800016045570374 + 0.1 * 6.492172718048096
Epoch 360, val loss: 0.7451967000961304
Epoch 370, training loss: 0.7714964151382446 = 0.12425506114959717 + 0.1 * 6.472413539886475
Epoch 370, val loss: 0.7533931732177734
Epoch 380, training loss: 0.758224606513977 = 0.112164705991745 + 0.1 * 6.460598945617676
Epoch 380, val loss: 0.7620794177055359
Epoch 390, training loss: 0.7470139265060425 = 0.10144654661417007 + 0.1 * 6.455673694610596
Epoch 390, val loss: 0.7712892293930054
Epoch 400, training loss: 0.7366962432861328 = 0.09190361201763153 + 0.1 * 6.447926044464111
Epoch 400, val loss: 0.780703067779541
Epoch 410, training loss: 0.7283912897109985 = 0.08332987129688263 + 0.1 * 6.450613975524902
Epoch 410, val loss: 0.7903164625167847
Epoch 420, training loss: 0.7187332510948181 = 0.07561753690242767 + 0.1 * 6.431156635284424
Epoch 420, val loss: 0.8000949621200562
Epoch 430, training loss: 0.710779070854187 = 0.06869221478700638 + 0.1 * 6.420868396759033
Epoch 430, val loss: 0.810060441493988
Epoch 440, training loss: 0.7046430110931396 = 0.062495555728673935 + 0.1 * 6.421474456787109
Epoch 440, val loss: 0.8202192187309265
Epoch 450, training loss: 0.6985118389129639 = 0.05700001120567322 + 0.1 * 6.41511869430542
Epoch 450, val loss: 0.8303819298744202
Epoch 460, training loss: 0.692315399646759 = 0.052158962935209274 + 0.1 * 6.401564121246338
Epoch 460, val loss: 0.8407515287399292
Epoch 470, training loss: 0.6870326995849609 = 0.047889430075883865 + 0.1 * 6.391432762145996
Epoch 470, val loss: 0.851227879524231
Epoch 480, training loss: 0.6828547716140747 = 0.04412068799138069 + 0.1 * 6.387341022491455
Epoch 480, val loss: 0.8618119955062866
Epoch 490, training loss: 0.678264319896698 = 0.04076698049902916 + 0.1 * 6.374973773956299
Epoch 490, val loss: 0.8725308179855347
Epoch 500, training loss: 0.6780074238777161 = 0.03776848688721657 + 0.1 * 6.4023895263671875
Epoch 500, val loss: 0.8833358287811279
Epoch 510, training loss: 0.6728832721710205 = 0.03510434925556183 + 0.1 * 6.377789497375488
Epoch 510, val loss: 0.8940085768699646
Epoch 520, training loss: 0.6685001254081726 = 0.032714951783418655 + 0.1 * 6.357851505279541
Epoch 520, val loss: 0.9046265482902527
Epoch 530, training loss: 0.6669068336486816 = 0.03055535815656185 + 0.1 * 6.363514423370361
Epoch 530, val loss: 0.9150894284248352
Epoch 540, training loss: 0.6638982892036438 = 0.028601491823792458 + 0.1 * 6.352968215942383
Epoch 540, val loss: 0.9254322648048401
Epoch 550, training loss: 0.6609905958175659 = 0.0268282201141119 + 0.1 * 6.341623783111572
Epoch 550, val loss: 0.9355787038803101
Epoch 560, training loss: 0.6618476510047913 = 0.025217419490218163 + 0.1 * 6.366302490234375
Epoch 560, val loss: 0.9455282688140869
Epoch 570, training loss: 0.6570711731910706 = 0.023753102868795395 + 0.1 * 6.3331804275512695
Epoch 570, val loss: 0.9552885890007019
Epoch 580, training loss: 0.6548586487770081 = 0.022411640733480453 + 0.1 * 6.324470043182373
Epoch 580, val loss: 0.9648391008377075
Epoch 590, training loss: 0.6537224054336548 = 0.021181315183639526 + 0.1 * 6.325410842895508
Epoch 590, val loss: 0.9740989208221436
Epoch 600, training loss: 0.6540987491607666 = 0.020054500550031662 + 0.1 * 6.340442180633545
Epoch 600, val loss: 0.9832156896591187
Epoch 610, training loss: 0.6511794328689575 = 0.01902407594025135 + 0.1 * 6.321553707122803
Epoch 610, val loss: 0.992047905921936
Epoch 620, training loss: 0.6488035321235657 = 0.018071750178933144 + 0.1 * 6.307317733764648
Epoch 620, val loss: 1.000767707824707
Epoch 630, training loss: 0.647258460521698 = 0.017191417515277863 + 0.1 * 6.300670623779297
Epoch 630, val loss: 1.0091476440429688
Epoch 640, training loss: 0.64723801612854 = 0.016377538442611694 + 0.1 * 6.308604717254639
Epoch 640, val loss: 1.0173683166503906
Epoch 650, training loss: 0.6453671455383301 = 0.015622809529304504 + 0.1 * 6.297443389892578
Epoch 650, val loss: 1.0253676176071167
Epoch 660, training loss: 0.6453095078468323 = 0.014922283589839935 + 0.1 * 6.303872585296631
Epoch 660, val loss: 1.0331910848617554
Epoch 670, training loss: 0.6445055603981018 = 0.014271615073084831 + 0.1 * 6.302339553833008
Epoch 670, val loss: 1.0407758951187134
Epoch 680, training loss: 0.6426510810852051 = 0.013665092177689075 + 0.1 * 6.289859771728516
Epoch 680, val loss: 1.0482261180877686
Epoch 690, training loss: 0.6422213315963745 = 0.0131005784496665 + 0.1 * 6.291207313537598
Epoch 690, val loss: 1.0555087327957153
Epoch 700, training loss: 0.6398614645004272 = 0.012571532279253006 + 0.1 * 6.272899150848389
Epoch 700, val loss: 1.0626145601272583
Epoch 710, training loss: 0.6406969428062439 = 0.012075510807335377 + 0.1 * 6.286214351654053
Epoch 710, val loss: 1.069583535194397
Epoch 720, training loss: 0.6384273171424866 = 0.011611824855208397 + 0.1 * 6.268154621124268
Epoch 720, val loss: 1.0763163566589355
Epoch 730, training loss: 0.6385921835899353 = 0.011176832020282745 + 0.1 * 6.274153709411621
Epoch 730, val loss: 1.082973599433899
Epoch 740, training loss: 0.6368594169616699 = 0.010768148116767406 + 0.1 * 6.2609124183654785
Epoch 740, val loss: 1.089479684829712
Epoch 750, training loss: 0.6377589106559753 = 0.01038264948874712 + 0.1 * 6.273762226104736
Epoch 750, val loss: 1.0957609415054321
Epoch 760, training loss: 0.6363728642463684 = 0.010020543821156025 + 0.1 * 6.263523101806641
Epoch 760, val loss: 1.1019299030303955
Epoch 770, training loss: 0.6347475647926331 = 0.009677915833890438 + 0.1 * 6.250696659088135
Epoch 770, val loss: 1.1079859733581543
Epoch 780, training loss: 0.6344830989837646 = 0.009354213252663612 + 0.1 * 6.251288414001465
Epoch 780, val loss: 1.113887906074524
Epoch 790, training loss: 0.6341875195503235 = 0.00904853269457817 + 0.1 * 6.251389980316162
Epoch 790, val loss: 1.1196749210357666
Epoch 800, training loss: 0.6342945694923401 = 0.008758477866649628 + 0.1 * 6.2553606033325195
Epoch 800, val loss: 1.1253128051757812
Epoch 810, training loss: 0.6332271695137024 = 0.008484149351716042 + 0.1 * 6.247430324554443
Epoch 810, val loss: 1.1308754682540894
Epoch 820, training loss: 0.6322047710418701 = 0.008223428390920162 + 0.1 * 6.239813327789307
Epoch 820, val loss: 1.1362991333007812
Epoch 830, training loss: 0.6325719356536865 = 0.007976019755005836 + 0.1 * 6.245959281921387
Epoch 830, val loss: 1.1416419744491577
Epoch 840, training loss: 0.6312354207038879 = 0.00774088641628623 + 0.1 * 6.234944820404053
Epoch 840, val loss: 1.1468061208724976
Epoch 850, training loss: 0.6305333375930786 = 0.007516615558415651 + 0.1 * 6.230166912078857
Epoch 850, val loss: 1.151980996131897
Epoch 860, training loss: 0.6310891509056091 = 0.007302736397832632 + 0.1 * 6.237863540649414
Epoch 860, val loss: 1.156976342201233
Epoch 870, training loss: 0.6297029256820679 = 0.007099440786987543 + 0.1 * 6.226034641265869
Epoch 870, val loss: 1.1617997884750366
Epoch 880, training loss: 0.629479169845581 = 0.006906014401465654 + 0.1 * 6.225731372833252
Epoch 880, val loss: 1.1666067838668823
Epoch 890, training loss: 0.6287115812301636 = 0.006720914971083403 + 0.1 * 6.219906806945801
Epoch 890, val loss: 1.1714342832565308
Epoch 900, training loss: 0.6286135315895081 = 0.006543207913637161 + 0.1 * 6.220703125
Epoch 900, val loss: 1.176086187362671
Epoch 910, training loss: 0.6281761527061462 = 0.006373225711286068 + 0.1 * 6.218029022216797
Epoch 910, val loss: 1.1806151866912842
Epoch 920, training loss: 0.6277779936790466 = 0.006210847292095423 + 0.1 * 6.215671062469482
Epoch 920, val loss: 1.1850589513778687
Epoch 930, training loss: 0.6265278458595276 = 0.006055446341633797 + 0.1 * 6.204723358154297
Epoch 930, val loss: 1.1895017623901367
Epoch 940, training loss: 0.6291530132293701 = 0.005906662438064814 + 0.1 * 6.232463836669922
Epoch 940, val loss: 1.1938542127609253
Epoch 950, training loss: 0.6274603605270386 = 0.005764456000179052 + 0.1 * 6.216958522796631
Epoch 950, val loss: 1.1980479955673218
Epoch 960, training loss: 0.6259533762931824 = 0.0056276144459843636 + 0.1 * 6.2032575607299805
Epoch 960, val loss: 1.2022819519042969
Epoch 970, training loss: 0.6260034441947937 = 0.005496246740221977 + 0.1 * 6.205071449279785
Epoch 970, val loss: 1.2063781023025513
Epoch 980, training loss: 0.6254296898841858 = 0.0053699989803135395 + 0.1 * 6.200596332550049
Epoch 980, val loss: 1.210442066192627
Epoch 990, training loss: 0.6259622573852539 = 0.005248351953923702 + 0.1 * 6.207139015197754
Epoch 990, val loss: 1.2144954204559326
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.8413
Flip ASR: 0.8133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.785817861557007 = 1.9484248161315918 + 0.1 * 8.373930931091309
Epoch 0, val loss: 1.9460701942443848
Epoch 10, training loss: 2.7752819061279297 = 1.9378955364227295 + 0.1 * 8.373862266540527
Epoch 10, val loss: 1.9359756708145142
Epoch 20, training loss: 2.76224422454834 = 1.9248887300491333 + 0.1 * 8.373556137084961
Epoch 20, val loss: 1.922953724861145
Epoch 30, training loss: 2.743842124938965 = 1.9067065715789795 + 0.1 * 8.371354103088379
Epoch 30, val loss: 1.904317021369934
Epoch 40, training loss: 2.715385913848877 = 1.8800437450408936 + 0.1 * 8.353422164916992
Epoch 40, val loss: 1.8770217895507812
Epoch 50, training loss: 2.66910982131958 = 1.8428233861923218 + 0.1 * 8.262863159179688
Epoch 50, val loss: 1.840579867362976
Epoch 60, training loss: 2.5975587368011475 = 1.8011462688446045 + 0.1 * 7.9641242027282715
Epoch 60, val loss: 1.803565263748169
Epoch 70, training loss: 2.5381507873535156 = 1.763286828994751 + 0.1 * 7.748640537261963
Epoch 70, val loss: 1.7719677686691284
Epoch 80, training loss: 2.451503276824951 = 1.721158742904663 + 0.1 * 7.303445339202881
Epoch 80, val loss: 1.7356938123703003
Epoch 90, training loss: 2.3674070835113525 = 1.66846764087677 + 0.1 * 6.9893951416015625
Epoch 90, val loss: 1.6905149221420288
Epoch 100, training loss: 2.284308433532715 = 1.6001033782958984 + 0.1 * 6.8420515060424805
Epoch 100, val loss: 1.6319422721862793
Epoch 110, training loss: 2.1935882568359375 = 1.5159674882888794 + 0.1 * 6.77620792388916
Epoch 110, val loss: 1.561832070350647
Epoch 120, training loss: 2.0976924896240234 = 1.4239708185195923 + 0.1 * 6.737216472625732
Epoch 120, val loss: 1.489815592765808
Epoch 130, training loss: 2.0026235580444336 = 1.3315893411636353 + 0.1 * 6.710341453552246
Epoch 130, val loss: 1.4180065393447876
Epoch 140, training loss: 1.9078612327575684 = 1.2391796112060547 + 0.1 * 6.686816215515137
Epoch 140, val loss: 1.3467615842819214
Epoch 150, training loss: 1.8128113746643066 = 1.146459937095642 + 0.1 * 6.663514614105225
Epoch 150, val loss: 1.2755355834960938
Epoch 160, training loss: 1.7218619585037231 = 1.057408094406128 + 0.1 * 6.644538402557373
Epoch 160, val loss: 1.2082374095916748
Epoch 170, training loss: 1.6375432014465332 = 0.974026083946228 + 0.1 * 6.635170936584473
Epoch 170, val loss: 1.1460940837860107
Epoch 180, training loss: 1.5589582920074463 = 0.8968287706375122 + 0.1 * 6.6212944984436035
Epoch 180, val loss: 1.0896908044815063
Epoch 190, training loss: 1.4851311445236206 = 0.8237108588218689 + 0.1 * 6.614202976226807
Epoch 190, val loss: 1.0365604162216187
Epoch 200, training loss: 1.414431095123291 = 0.7535669803619385 + 0.1 * 6.608640670776367
Epoch 200, val loss: 0.9857829809188843
Epoch 210, training loss: 1.3476390838623047 = 0.6871193647384644 + 0.1 * 6.605197429656982
Epoch 210, val loss: 0.9374626874923706
Epoch 220, training loss: 1.285834550857544 = 0.6257816553115845 + 0.1 * 6.600528717041016
Epoch 220, val loss: 0.8933337926864624
Epoch 230, training loss: 1.2284129858016968 = 0.569063127040863 + 0.1 * 6.593498229980469
Epoch 230, val loss: 0.8531337380409241
Epoch 240, training loss: 1.175152063369751 = 0.5163954496383667 + 0.1 * 6.587566375732422
Epoch 240, val loss: 0.8172670006752014
Epoch 250, training loss: 1.125442385673523 = 0.46741026639938354 + 0.1 * 6.580321311950684
Epoch 250, val loss: 0.7859796285629272
Epoch 260, training loss: 1.0791375637054443 = 0.42155176401138306 + 0.1 * 6.575858116149902
Epoch 260, val loss: 0.7590147256851196
Epoch 270, training loss: 1.0349647998809814 = 0.3782503604888916 + 0.1 * 6.567144870758057
Epoch 270, val loss: 0.7357474565505981
Epoch 280, training loss: 0.9925953149795532 = 0.33713963627815247 + 0.1 * 6.554556846618652
Epoch 280, val loss: 0.7153187990188599
Epoch 290, training loss: 0.9536020755767822 = 0.29851096868515015 + 0.1 * 6.550910949707031
Epoch 290, val loss: 0.6975181102752686
Epoch 300, training loss: 0.9161243438720703 = 0.26269760727882385 + 0.1 * 6.534266948699951
Epoch 300, val loss: 0.6821801066398621
Epoch 310, training loss: 0.883711576461792 = 0.23003263771533966 + 0.1 * 6.536789417266846
Epoch 310, val loss: 0.6693387627601624
Epoch 320, training loss: 0.8527117967605591 = 0.20118553936481476 + 0.1 * 6.515262603759766
Epoch 320, val loss: 0.6590724587440491
Epoch 330, training loss: 0.8260948657989502 = 0.1759798526763916 + 0.1 * 6.501150131225586
Epoch 330, val loss: 0.6514654755592346
Epoch 340, training loss: 0.8039320707321167 = 0.15420731902122498 + 0.1 * 6.497247219085693
Epoch 340, val loss: 0.6465963125228882
Epoch 350, training loss: 0.784312903881073 = 0.13561944663524628 + 0.1 * 6.486934185028076
Epoch 350, val loss: 0.6442617774009705
Epoch 360, training loss: 0.7674877643585205 = 0.11975156515836716 + 0.1 * 6.477361679077148
Epoch 360, val loss: 0.6443569660186768
Epoch 370, training loss: 0.7552540898323059 = 0.10619686543941498 + 0.1 * 6.490571975708008
Epoch 370, val loss: 0.646634042263031
Epoch 380, training loss: 0.7410775423049927 = 0.09468569606542587 + 0.1 * 6.463918685913086
Epoch 380, val loss: 0.6507090330123901
Epoch 390, training loss: 0.7306703329086304 = 0.08481914550065994 + 0.1 * 6.458511829376221
Epoch 390, val loss: 0.6562156677246094
Epoch 400, training loss: 0.7221973538398743 = 0.07633601129055023 + 0.1 * 6.45861291885376
Epoch 400, val loss: 0.6628611087799072
Epoch 410, training loss: 0.7138283848762512 = 0.06902729719877243 + 0.1 * 6.448010444641113
Epoch 410, val loss: 0.6701568365097046
Epoch 420, training loss: 0.706628680229187 = 0.06268035620450974 + 0.1 * 6.439483165740967
Epoch 420, val loss: 0.6780110597610474
Epoch 430, training loss: 0.7069843411445618 = 0.05713746324181557 + 0.1 * 6.498468399047852
Epoch 430, val loss: 0.6862165331840515
Epoch 440, training loss: 0.6969040632247925 = 0.05234721302986145 + 0.1 * 6.445568084716797
Epoch 440, val loss: 0.6945501565933228
Epoch 450, training loss: 0.6903062462806702 = 0.04814787954092026 + 0.1 * 6.421583652496338
Epoch 450, val loss: 0.7028339505195618
Epoch 460, training loss: 0.6863434314727783 = 0.04442210495471954 + 0.1 * 6.41921329498291
Epoch 460, val loss: 0.7113136053085327
Epoch 470, training loss: 0.6840963959693909 = 0.04110370948910713 + 0.1 * 6.42992639541626
Epoch 470, val loss: 0.7197668552398682
Epoch 480, training loss: 0.6790242791175842 = 0.03814706951379776 + 0.1 * 6.408771514892578
Epoch 480, val loss: 0.7280650734901428
Epoch 490, training loss: 0.6766645908355713 = 0.03550107777118683 + 0.1 * 6.411634922027588
Epoch 490, val loss: 0.7362834811210632
Epoch 500, training loss: 0.6728274822235107 = 0.03313002735376358 + 0.1 * 6.396974086761475
Epoch 500, val loss: 0.7443705797195435
Epoch 510, training loss: 0.671027421951294 = 0.03098873794078827 + 0.1 * 6.400386333465576
Epoch 510, val loss: 0.7523472309112549
Epoch 520, training loss: 0.6685364246368408 = 0.029057113453745842 + 0.1 * 6.394793510437012
Epoch 520, val loss: 0.760151743888855
Epoch 530, training loss: 0.6664186120033264 = 0.02730802446603775 + 0.1 * 6.391105651855469
Epoch 530, val loss: 0.7677237391471863
Epoch 540, training loss: 0.6667617559432983 = 0.025715941563248634 + 0.1 * 6.410458087921143
Epoch 540, val loss: 0.7751915454864502
Epoch 550, training loss: 0.6623135805130005 = 0.02427021786570549 + 0.1 * 6.380433559417725
Epoch 550, val loss: 0.7824103832244873
Epoch 560, training loss: 0.6604894995689392 = 0.02294713817536831 + 0.1 * 6.375423431396484
Epoch 560, val loss: 0.789459764957428
Epoch 570, training loss: 0.6583532094955444 = 0.021733194589614868 + 0.1 * 6.366199970245361
Epoch 570, val loss: 0.7964091897010803
Epoch 580, training loss: 0.6618831753730774 = 0.02061752788722515 + 0.1 * 6.412656307220459
Epoch 580, val loss: 0.8031538724899292
Epoch 590, training loss: 0.6553688049316406 = 0.019595451653003693 + 0.1 * 6.357733726501465
Epoch 590, val loss: 0.8097243905067444
Epoch 600, training loss: 0.6548759937286377 = 0.018652329221367836 + 0.1 * 6.362236499786377
Epoch 600, val loss: 0.816160261631012
Epoch 610, training loss: 0.6530644297599792 = 0.017780093476176262 + 0.1 * 6.352843761444092
Epoch 610, val loss: 0.8223732709884644
Epoch 620, training loss: 0.6510177850723267 = 0.01697254739701748 + 0.1 * 6.340452194213867
Epoch 620, val loss: 0.8285569548606873
Epoch 630, training loss: 0.6498515605926514 = 0.016220131888985634 + 0.1 * 6.336313724517822
Epoch 630, val loss: 0.8345530033111572
Epoch 640, training loss: 0.6508861780166626 = 0.015519006177783012 + 0.1 * 6.353671550750732
Epoch 640, val loss: 0.8404935598373413
Epoch 650, training loss: 0.6495856046676636 = 0.014868201687932014 + 0.1 * 6.347173690795898
Epoch 650, val loss: 0.8461381196975708
Epoch 660, training loss: 0.646795928478241 = 0.014263160526752472 + 0.1 * 6.325327396392822
Epoch 660, val loss: 0.8517323136329651
Epoch 670, training loss: 0.6471303105354309 = 0.013695409521460533 + 0.1 * 6.334348678588867
Epoch 670, val loss: 0.8571616411209106
Epoch 680, training loss: 0.6451956033706665 = 0.013163972645998001 + 0.1 * 6.320316314697266
Epoch 680, val loss: 0.8626186847686768
Epoch 690, training loss: 0.6461852788925171 = 0.01266460306942463 + 0.1 * 6.335206508636475
Epoch 690, val loss: 0.8678725361824036
Epoch 700, training loss: 0.6422387957572937 = 0.012196537107229233 + 0.1 * 6.300422191619873
Epoch 700, val loss: 0.8730257749557495
Epoch 710, training loss: 0.6429661512374878 = 0.011756242252886295 + 0.1 * 6.312098979949951
Epoch 710, val loss: 0.8780921697616577
Epoch 720, training loss: 0.6415987610816956 = 0.011340895667672157 + 0.1 * 6.302578449249268
Epoch 720, val loss: 0.8830965161323547
Epoch 730, training loss: 0.6399403810501099 = 0.010949840769171715 + 0.1 * 6.289905071258545
Epoch 730, val loss: 0.8879842162132263
Epoch 740, training loss: 0.6393958926200867 = 0.0105799725279212 + 0.1 * 6.288159370422363
Epoch 740, val loss: 0.8928175568580627
Epoch 750, training loss: 0.6395320296287537 = 0.010230198502540588 + 0.1 * 6.293018341064453
Epoch 750, val loss: 0.8975702524185181
Epoch 760, training loss: 0.6378424167633057 = 0.00989887211471796 + 0.1 * 6.279435157775879
Epoch 760, val loss: 0.9022075533866882
Epoch 770, training loss: 0.6381229758262634 = 0.009584697894752026 + 0.1 * 6.285382270812988
Epoch 770, val loss: 0.9068068265914917
Epoch 780, training loss: 0.6368911862373352 = 0.009286710061132908 + 0.1 * 6.276044845581055
Epoch 780, val loss: 0.9113263487815857
Epoch 790, training loss: 0.6358565092086792 = 0.009004310704767704 + 0.1 * 6.268521785736084
Epoch 790, val loss: 0.915765643119812
Epoch 800, training loss: 0.6349231600761414 = 0.008735716342926025 + 0.1 * 6.261874198913574
Epoch 800, val loss: 0.9200969338417053
Epoch 810, training loss: 0.6346083879470825 = 0.008479920215904713 + 0.1 * 6.261284828186035
Epoch 810, val loss: 0.9243618845939636
Epoch 820, training loss: 0.6347767114639282 = 0.008236325345933437 + 0.1 * 6.2654032707214355
Epoch 820, val loss: 0.928578794002533
Epoch 830, training loss: 0.6330180764198303 = 0.00800405628979206 + 0.1 * 6.250140190124512
Epoch 830, val loss: 0.9327564239501953
Epoch 840, training loss: 0.6335362195968628 = 0.00778313260525465 + 0.1 * 6.25753116607666
Epoch 840, val loss: 0.9368123412132263
Epoch 850, training loss: 0.6321685314178467 = 0.007571761030703783 + 0.1 * 6.245967388153076
Epoch 850, val loss: 0.9408406615257263
Epoch 860, training loss: 0.6346045136451721 = 0.007368796970695257 + 0.1 * 6.272356986999512
Epoch 860, val loss: 0.9448257088661194
Epoch 870, training loss: 0.6321941018104553 = 0.007175098638981581 + 0.1 * 6.250190258026123
Epoch 870, val loss: 0.9486985802650452
Epoch 880, training loss: 0.6346194744110107 = 0.0069908942095935345 + 0.1 * 6.276285648345947
Epoch 880, val loss: 0.9525633454322815
Epoch 890, training loss: 0.6318334341049194 = 0.0068135447800159454 + 0.1 * 6.250198841094971
Epoch 890, val loss: 0.9563947916030884
Epoch 900, training loss: 0.6300817131996155 = 0.006644857116043568 + 0.1 * 6.234368324279785
Epoch 900, val loss: 0.960124135017395
Epoch 910, training loss: 0.6299296617507935 = 0.006481829099357128 + 0.1 * 6.23447847366333
Epoch 910, val loss: 0.9637906551361084
Epoch 920, training loss: 0.6316116452217102 = 0.006324445828795433 + 0.1 * 6.252871513366699
Epoch 920, val loss: 0.9674097299575806
Epoch 930, training loss: 0.6290713548660278 = 0.006174391135573387 + 0.1 * 6.228969573974609
Epoch 930, val loss: 0.9710694551467896
Epoch 940, training loss: 0.6289635896682739 = 0.006030535325407982 + 0.1 * 6.229330062866211
Epoch 940, val loss: 0.9746517539024353
Epoch 950, training loss: 0.6291113495826721 = 0.005892052780836821 + 0.1 * 6.2321929931640625
Epoch 950, val loss: 0.978136420249939
Epoch 960, training loss: 0.6278113722801208 = 0.005758311133831739 + 0.1 * 6.220530986785889
Epoch 960, val loss: 0.9815539121627808
Epoch 970, training loss: 0.6288164854049683 = 0.00562994834035635 + 0.1 * 6.231864929199219
Epoch 970, val loss: 0.9849575757980347
Epoch 980, training loss: 0.6282188892364502 = 0.005506056360900402 + 0.1 * 6.227128505706787
Epoch 980, val loss: 0.9883885979652405
Epoch 990, training loss: 0.6270927786827087 = 0.00538782449439168 + 0.1 * 6.2170491218566895
Epoch 990, val loss: 0.9916771650314331
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8155
Flip ASR: 0.7867/225 nodes
The final ASR:0.78229, 0.06608, Accuracy:0.79877, 0.00698
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11668])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10632])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7877559661865234 = 1.9503707885742188 + 0.1 * 8.373851776123047
Epoch 0, val loss: 1.9421699047088623
Epoch 10, training loss: 2.7774832248687744 = 1.9401153326034546 + 0.1 * 8.373678207397461
Epoch 10, val loss: 1.9328197240829468
Epoch 20, training loss: 2.764695644378662 = 1.927430272102356 + 0.1 * 8.372653007507324
Epoch 20, val loss: 1.920964241027832
Epoch 30, training loss: 2.7458600997924805 = 1.9094475507736206 + 0.1 * 8.364124298095703
Epoch 30, val loss: 1.9040769338607788
Epoch 40, training loss: 2.712754726409912 = 1.882624864578247 + 0.1 * 8.301299095153809
Epoch 40, val loss: 1.8793350458145142
Epoch 50, training loss: 2.6307215690612793 = 1.8463940620422363 + 0.1 * 7.843274116516113
Epoch 50, val loss: 1.8476576805114746
Epoch 60, training loss: 2.5618162155151367 = 1.8069828748703003 + 0.1 * 7.548333168029785
Epoch 60, val loss: 1.8150634765625
Epoch 70, training loss: 2.4857900142669678 = 1.7701568603515625 + 0.1 * 7.156330585479736
Epoch 70, val loss: 1.7859632968902588
Epoch 80, training loss: 2.422816753387451 = 1.7338491678237915 + 0.1 * 6.889675617218018
Epoch 80, val loss: 1.756744384765625
Epoch 90, training loss: 2.363844156265259 = 1.6871955394744873 + 0.1 * 6.766485691070557
Epoch 90, val loss: 1.715468168258667
Epoch 100, training loss: 2.294374465942383 = 1.623713731765747 + 0.1 * 6.706607341766357
Epoch 100, val loss: 1.6602656841278076
Epoch 110, training loss: 2.211392641067505 = 1.5442122220993042 + 0.1 * 6.6718034744262695
Epoch 110, val loss: 1.5949441194534302
Epoch 120, training loss: 2.1214518547058105 = 1.4569289684295654 + 0.1 * 6.645229816436768
Epoch 120, val loss: 1.5242295265197754
Epoch 130, training loss: 2.033663272857666 = 1.371355414390564 + 0.1 * 6.623077869415283
Epoch 130, val loss: 1.4577699899673462
Epoch 140, training loss: 1.951103925704956 = 1.2909462451934814 + 0.1 * 6.601576805114746
Epoch 140, val loss: 1.3979604244232178
Epoch 150, training loss: 1.873802661895752 = 1.2157894372940063 + 0.1 * 6.580131530761719
Epoch 150, val loss: 1.344878911972046
Epoch 160, training loss: 1.8017255067825317 = 1.1452919244766235 + 0.1 * 6.564335823059082
Epoch 160, val loss: 1.2972770929336548
Epoch 170, training loss: 1.7324169874191284 = 1.0779073238372803 + 0.1 * 6.545096397399902
Epoch 170, val loss: 1.2537800073623657
Epoch 180, training loss: 1.663831114768982 = 1.0108376741409302 + 0.1 * 6.529934406280518
Epoch 180, val loss: 1.2105623483657837
Epoch 190, training loss: 1.5955536365509033 = 0.9439154267311096 + 0.1 * 6.51638126373291
Epoch 190, val loss: 1.1671357154846191
Epoch 200, training loss: 1.528106451034546 = 0.877887487411499 + 0.1 * 6.5021891593933105
Epoch 200, val loss: 1.1235761642456055
Epoch 210, training loss: 1.4624927043914795 = 0.8129479885101318 + 0.1 * 6.495446681976318
Epoch 210, val loss: 1.0797415971755981
Epoch 220, training loss: 1.3987407684326172 = 0.7498993873596191 + 0.1 * 6.488414287567139
Epoch 220, val loss: 1.0360164642333984
Epoch 230, training loss: 1.3366422653198242 = 0.6892251372337341 + 0.1 * 6.474171161651611
Epoch 230, val loss: 0.9932087063789368
Epoch 240, training loss: 1.27803635597229 = 0.6311706304550171 + 0.1 * 6.46865701675415
Epoch 240, val loss: 0.9520083665847778
Epoch 250, training loss: 1.2224934101104736 = 0.5767319798469543 + 0.1 * 6.457614421844482
Epoch 250, val loss: 0.9147331118583679
Epoch 260, training loss: 1.1705743074417114 = 0.5258094668388367 + 0.1 * 6.447648525238037
Epoch 260, val loss: 0.8814151287078857
Epoch 270, training loss: 1.1222730875015259 = 0.4782201647758484 + 0.1 * 6.4405293464660645
Epoch 270, val loss: 0.8523886203765869
Epoch 280, training loss: 1.0778509378433228 = 0.433619886636734 + 0.1 * 6.442309856414795
Epoch 280, val loss: 0.8279598951339722
Epoch 290, training loss: 1.033818244934082 = 0.39090636372566223 + 0.1 * 6.429118633270264
Epoch 290, val loss: 0.8066918253898621
Epoch 300, training loss: 0.9913009405136108 = 0.3492831885814667 + 0.1 * 6.420176982879639
Epoch 300, val loss: 0.7880276441574097
Epoch 310, training loss: 0.9505170583724976 = 0.3089625835418701 + 0.1 * 6.415544509887695
Epoch 310, val loss: 0.7718029618263245
Epoch 320, training loss: 0.9119395017623901 = 0.2711997926235199 + 0.1 * 6.4073967933654785
Epoch 320, val loss: 0.7585197687149048
Epoch 330, training loss: 0.8778841495513916 = 0.2367798238992691 + 0.1 * 6.411043167114258
Epoch 330, val loss: 0.7482788562774658
Epoch 340, training loss: 0.8455703854560852 = 0.2064642459154129 + 0.1 * 6.391061305999756
Epoch 340, val loss: 0.7415114641189575
Epoch 350, training loss: 0.8190053701400757 = 0.1802331507205963 + 0.1 * 6.387722492218018
Epoch 350, val loss: 0.7380505800247192
Epoch 360, training loss: 0.7959218621253967 = 0.1579350233078003 + 0.1 * 6.379868507385254
Epoch 360, val loss: 0.7374725341796875
Epoch 370, training loss: 0.7764887809753418 = 0.1390758901834488 + 0.1 * 6.374128818511963
Epoch 370, val loss: 0.7394376993179321
Epoch 380, training loss: 0.7605476975440979 = 0.12303948402404785 + 0.1 * 6.375082015991211
Epoch 380, val loss: 0.743428111076355
Epoch 390, training loss: 0.7455852627754211 = 0.10947509855031967 + 0.1 * 6.3611016273498535
Epoch 390, val loss: 0.7487757802009583
Epoch 400, training loss: 0.7329456806182861 = 0.09791234135627747 + 0.1 * 6.350332736968994
Epoch 400, val loss: 0.7553710341453552
Epoch 410, training loss: 0.7242206931114197 = 0.08796634525060654 + 0.1 * 6.36254358291626
Epoch 410, val loss: 0.7627199292182922
Epoch 420, training loss: 0.7135356664657593 = 0.07942482084035873 + 0.1 * 6.341108798980713
Epoch 420, val loss: 0.7705411911010742
Epoch 430, training loss: 0.7056813836097717 = 0.0720142126083374 + 0.1 * 6.336671829223633
Epoch 430, val loss: 0.778931200504303
Epoch 440, training loss: 0.698274552822113 = 0.06555784493684769 + 0.1 * 6.32716703414917
Epoch 440, val loss: 0.7875786423683167
Epoch 450, training loss: 0.6928896307945251 = 0.05989646166563034 + 0.1 * 6.329931735992432
Epoch 450, val loss: 0.7964543104171753
Epoch 460, training loss: 0.6865401864051819 = 0.0549197643995285 + 0.1 * 6.316204071044922
Epoch 460, val loss: 0.8054928779602051
Epoch 470, training loss: 0.6814829707145691 = 0.05051747336983681 + 0.1 * 6.309654712677002
Epoch 470, val loss: 0.8147144913673401
Epoch 480, training loss: 0.6777004599571228 = 0.04659508913755417 + 0.1 * 6.31105375289917
Epoch 480, val loss: 0.8239491581916809
Epoch 490, training loss: 0.6752381324768066 = 0.04309596121311188 + 0.1 * 6.3214216232299805
Epoch 490, val loss: 0.8332045078277588
Epoch 500, training loss: 0.6698082089424133 = 0.03998555243015289 + 0.1 * 6.298226833343506
Epoch 500, val loss: 0.8423745036125183
Epoch 510, training loss: 0.6662567257881165 = 0.03719769045710564 + 0.1 * 6.290589809417725
Epoch 510, val loss: 0.851637601852417
Epoch 520, training loss: 0.662926435470581 = 0.03467681631445885 + 0.1 * 6.282496452331543
Epoch 520, val loss: 0.8606960773468018
Epoch 530, training loss: 0.6626492738723755 = 0.03239339590072632 + 0.1 * 6.302558898925781
Epoch 530, val loss: 0.8696634769439697
Epoch 540, training loss: 0.6579338908195496 = 0.030330179259181023 + 0.1 * 6.276037216186523
Epoch 540, val loss: 0.8786786794662476
Epoch 550, training loss: 0.6561062932014465 = 0.028453875333070755 + 0.1 * 6.276524066925049
Epoch 550, val loss: 0.8875423073768616
Epoch 560, training loss: 0.653363823890686 = 0.0267487782984972 + 0.1 * 6.26615047454834
Epoch 560, val loss: 0.8961138129234314
Epoch 570, training loss: 0.6522799730300903 = 0.025198619812726974 + 0.1 * 6.27081298828125
Epoch 570, val loss: 0.9047713279724121
Epoch 580, training loss: 0.6497651934623718 = 0.0237827580422163 + 0.1 * 6.259824275970459
Epoch 580, val loss: 0.9130792617797852
Epoch 590, training loss: 0.6479009389877319 = 0.02248883619904518 + 0.1 * 6.254120826721191
Epoch 590, val loss: 0.9214184880256653
Epoch 600, training loss: 0.64673912525177 = 0.021298374980688095 + 0.1 * 6.25440788269043
Epoch 600, val loss: 0.929538905620575
Epoch 610, training loss: 0.645588755607605 = 0.020202618092298508 + 0.1 * 6.253860950469971
Epoch 610, val loss: 0.9373574256896973
Epoch 620, training loss: 0.644381046295166 = 0.019196728244423866 + 0.1 * 6.251842975616455
Epoch 620, val loss: 0.9453160762786865
Epoch 630, training loss: 0.6431915760040283 = 0.018266474828124046 + 0.1 * 6.249250411987305
Epoch 630, val loss: 0.9530172944068909
Epoch 640, training loss: 0.6411893963813782 = 0.01740492880344391 + 0.1 * 6.237844944000244
Epoch 640, val loss: 0.9604195952415466
Epoch 650, training loss: 0.6409661173820496 = 0.016606083139777184 + 0.1 * 6.243600368499756
Epoch 650, val loss: 0.9678469896316528
Epoch 660, training loss: 0.6398870944976807 = 0.015863725915551186 + 0.1 * 6.240233898162842
Epoch 660, val loss: 0.9750452041625977
Epoch 670, training loss: 0.639310896396637 = 0.015173904597759247 + 0.1 * 6.241369724273682
Epoch 670, val loss: 0.9820278882980347
Epoch 680, training loss: 0.6376798152923584 = 0.014534196816384792 + 0.1 * 6.231456279754639
Epoch 680, val loss: 0.9890159368515015
Epoch 690, training loss: 0.6364611983299255 = 0.013937017880380154 + 0.1 * 6.225241661071777
Epoch 690, val loss: 0.9958551526069641
Epoch 700, training loss: 0.6369990706443787 = 0.013376918621361256 + 0.1 * 6.236221790313721
Epoch 700, val loss: 1.002474308013916
Epoch 710, training loss: 0.6354962587356567 = 0.012852354906499386 + 0.1 * 6.226438999176025
Epoch 710, val loss: 1.0087532997131348
Epoch 720, training loss: 0.6344319581985474 = 0.012363012880086899 + 0.1 * 6.220689296722412
Epoch 720, val loss: 1.0152313709259033
Epoch 730, training loss: 0.6339051127433777 = 0.011902119033038616 + 0.1 * 6.220029830932617
Epoch 730, val loss: 1.0214542150497437
Epoch 740, training loss: 0.6326766014099121 = 0.011468840762972832 + 0.1 * 6.212077617645264
Epoch 740, val loss: 1.027450680732727
Epoch 750, training loss: 0.6326085329055786 = 0.011061605997383595 + 0.1 * 6.215468883514404
Epoch 750, val loss: 1.0334569215774536
Epoch 760, training loss: 0.63116854429245 = 0.010677212849259377 + 0.1 * 6.204913139343262
Epoch 760, val loss: 1.039233684539795
Epoch 770, training loss: 0.631053626537323 = 0.01031543966382742 + 0.1 * 6.207381725311279
Epoch 770, val loss: 1.0450894832611084
Epoch 780, training loss: 0.6320087909698486 = 0.009972643107175827 + 0.1 * 6.220361232757568
Epoch 780, val loss: 1.050631046295166
Epoch 790, training loss: 0.6299580335617065 = 0.009648849256336689 + 0.1 * 6.203091621398926
Epoch 790, val loss: 1.056148886680603
Epoch 800, training loss: 0.6295361518859863 = 0.009342331439256668 + 0.1 * 6.201938152313232
Epoch 800, val loss: 1.0616225004196167
Epoch 810, training loss: 0.6287899017333984 = 0.009050937369465828 + 0.1 * 6.197389125823975
Epoch 810, val loss: 1.0668061971664429
Epoch 820, training loss: 0.6296638250350952 = 0.008775487542152405 + 0.1 * 6.208883285522461
Epoch 820, val loss: 1.0720412731170654
Epoch 830, training loss: 0.6275725364685059 = 0.008513929322361946 + 0.1 * 6.190586090087891
Epoch 830, val loss: 1.0771772861480713
Epoch 840, training loss: 0.6274377703666687 = 0.008265030570328236 + 0.1 * 6.191727638244629
Epoch 840, val loss: 1.0821983814239502
Epoch 850, training loss: 0.6270123720169067 = 0.008027566596865654 + 0.1 * 6.189847946166992
Epoch 850, val loss: 1.0870401859283447
Epoch 860, training loss: 0.6268426179885864 = 0.007801585365086794 + 0.1 * 6.190410137176514
Epoch 860, val loss: 1.0919204950332642
Epoch 870, training loss: 0.6275855898857117 = 0.007586099207401276 + 0.1 * 6.1999945640563965
Epoch 870, val loss: 1.0966497659683228
Epoch 880, training loss: 0.6265889406204224 = 0.00738039193674922 + 0.1 * 6.1920857429504395
Epoch 880, val loss: 1.1013375520706177
Epoch 890, training loss: 0.6248447299003601 = 0.007183975074440241 + 0.1 * 6.176607608795166
Epoch 890, val loss: 1.1058707237243652
Epoch 900, training loss: 0.6247460246086121 = 0.006996501702815294 + 0.1 * 6.177495002746582
Epoch 900, val loss: 1.1104379892349243
Epoch 910, training loss: 0.6256122589111328 = 0.00681693758815527 + 0.1 * 6.187952995300293
Epoch 910, val loss: 1.114884853363037
Epoch 920, training loss: 0.6238718032836914 = 0.00664505222812295 + 0.1 * 6.172267436981201
Epoch 920, val loss: 1.119109869003296
Epoch 930, training loss: 0.6239445805549622 = 0.00648081861436367 + 0.1 * 6.174637317657471
Epoch 930, val loss: 1.123525857925415
Epoch 940, training loss: 0.6239455342292786 = 0.006323155947029591 + 0.1 * 6.1762237548828125
Epoch 940, val loss: 1.1277623176574707
Epoch 950, training loss: 0.6232944130897522 = 0.006171895656734705 + 0.1 * 6.171225070953369
Epoch 950, val loss: 1.131880283355713
Epoch 960, training loss: 0.6246967911720276 = 0.006026804447174072 + 0.1 * 6.186699867248535
Epoch 960, val loss: 1.1359994411468506
Epoch 970, training loss: 0.6235866546630859 = 0.005887351930141449 + 0.1 * 6.176992893218994
Epoch 970, val loss: 1.1398850679397583
Epoch 980, training loss: 0.621874213218689 = 0.005753934383392334 + 0.1 * 6.161202430725098
Epoch 980, val loss: 1.1439687013626099
Epoch 990, training loss: 0.6235929727554321 = 0.005625660065561533 + 0.1 * 6.179673194885254
Epoch 990, val loss: 1.1479368209838867
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6384
Flip ASR: 0.5644/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7979841232299805 = 1.9605963230133057 + 0.1 * 8.373876571655273
Epoch 0, val loss: 1.9532793760299683
Epoch 10, training loss: 2.78760027885437 = 1.9502259492874146 + 0.1 * 8.373743057250977
Epoch 10, val loss: 1.9426854848861694
Epoch 20, training loss: 2.7746047973632812 = 1.9373112916946411 + 0.1 * 8.372936248779297
Epoch 20, val loss: 1.9288734197616577
Epoch 30, training loss: 2.755441904067993 = 1.9188053607940674 + 0.1 * 8.366364479064941
Epoch 30, val loss: 1.908563256263733
Epoch 40, training loss: 2.7231762409210205 = 1.890882968902588 + 0.1 * 8.322933197021484
Epoch 40, val loss: 1.8778594732284546
Epoch 50, training loss: 2.656512975692749 = 1.8516583442687988 + 0.1 * 8.04854679107666
Epoch 50, val loss: 1.8366599082946777
Epoch 60, training loss: 2.5731706619262695 = 1.8078060150146484 + 0.1 * 7.653645038604736
Epoch 60, val loss: 1.7944340705871582
Epoch 70, training loss: 2.485807418823242 = 1.7672699689865112 + 0.1 * 7.185375213623047
Epoch 70, val loss: 1.7579425573349
Epoch 80, training loss: 2.418023109436035 = 1.7249794006347656 + 0.1 * 6.930438041687012
Epoch 80, val loss: 1.7209820747375488
Epoch 90, training loss: 2.3568968772888184 = 1.67289137840271 + 0.1 * 6.840053558349609
Epoch 90, val loss: 1.6761956214904785
Epoch 100, training loss: 2.28446626663208 = 1.6047439575195312 + 0.1 * 6.797221660614014
Epoch 100, val loss: 1.6184492111206055
Epoch 110, training loss: 2.195605516433716 = 1.5196586847305298 + 0.1 * 6.759469032287598
Epoch 110, val loss: 1.5482804775238037
Epoch 120, training loss: 2.097776412963867 = 1.4252312183380127 + 0.1 * 6.725452423095703
Epoch 120, val loss: 1.474308729171753
Epoch 130, training loss: 2.0000946521759033 = 1.3303881883621216 + 0.1 * 6.6970648765563965
Epoch 130, val loss: 1.403841495513916
Epoch 140, training loss: 1.9057652950286865 = 1.2383514642715454 + 0.1 * 6.67413854598999
Epoch 140, val loss: 1.3380552530288696
Epoch 150, training loss: 1.814192771911621 = 1.1487605571746826 + 0.1 * 6.654322624206543
Epoch 150, val loss: 1.2757197618484497
Epoch 160, training loss: 1.7272746562957764 = 1.0636370182037354 + 0.1 * 6.636376857757568
Epoch 160, val loss: 1.215828776359558
Epoch 170, training loss: 1.645462989807129 = 0.983465313911438 + 0.1 * 6.6199774742126465
Epoch 170, val loss: 1.159383773803711
Epoch 180, training loss: 1.567820429801941 = 0.9081434607505798 + 0.1 * 6.596769332885742
Epoch 180, val loss: 1.1071711778640747
Epoch 190, training loss: 1.4966890811920166 = 0.8389104008674622 + 0.1 * 6.577786445617676
Epoch 190, val loss: 1.060953140258789
Epoch 200, training loss: 1.4316208362579346 = 0.7760584950447083 + 0.1 * 6.555622577667236
Epoch 200, val loss: 1.0203857421875
Epoch 210, training loss: 1.373928189277649 = 0.7196000814437866 + 0.1 * 6.543281078338623
Epoch 210, val loss: 0.9854850769042969
Epoch 220, training loss: 1.3218967914581299 = 0.669568657875061 + 0.1 * 6.523280620574951
Epoch 220, val loss: 0.9562296271324158
Epoch 230, training loss: 1.2754920721054077 = 0.6243321895599365 + 0.1 * 6.511598587036133
Epoch 230, val loss: 0.9309489130973816
Epoch 240, training loss: 1.2324864864349365 = 0.5825591683387756 + 0.1 * 6.499273777008057
Epoch 240, val loss: 0.9086739420890808
Epoch 250, training loss: 1.192347764968872 = 0.5433973670005798 + 0.1 * 6.489503860473633
Epoch 250, val loss: 0.8890162706375122
Epoch 260, training loss: 1.1549859046936035 = 0.5065500140190125 + 0.1 * 6.484358787536621
Epoch 260, val loss: 0.872164249420166
Epoch 270, training loss: 1.1195037364959717 = 0.4718204140663147 + 0.1 * 6.476833820343018
Epoch 270, val loss: 0.8582474589347839
Epoch 280, training loss: 1.0852808952331543 = 0.4389517903327942 + 0.1 * 6.463291645050049
Epoch 280, val loss: 0.8475944399833679
Epoch 290, training loss: 1.0530822277069092 = 0.40753400325775146 + 0.1 * 6.455482482910156
Epoch 290, val loss: 0.8401156663894653
Epoch 300, training loss: 1.0224647521972656 = 0.37753376364707947 + 0.1 * 6.449309825897217
Epoch 300, val loss: 0.835925817489624
Epoch 310, training loss: 0.9927092790603638 = 0.34874317049980164 + 0.1 * 6.439661502838135
Epoch 310, val loss: 0.8341103792190552
Epoch 320, training loss: 0.9646958112716675 = 0.3208906352519989 + 0.1 * 6.438051223754883
Epoch 320, val loss: 0.8340547680854797
Epoch 330, training loss: 0.9371422529220581 = 0.29411840438842773 + 0.1 * 6.430238246917725
Epoch 330, val loss: 0.8353871703147888
Epoch 340, training loss: 0.9108084440231323 = 0.2685445249080658 + 0.1 * 6.4226393699646
Epoch 340, val loss: 0.8377074003219604
Epoch 350, training loss: 0.8866802453994751 = 0.24453358352184296 + 0.1 * 6.42146635055542
Epoch 350, val loss: 0.8410393595695496
Epoch 360, training loss: 0.8630471229553223 = 0.22234098613262177 + 0.1 * 6.4070611000061035
Epoch 360, val loss: 0.8455793857574463
Epoch 370, training loss: 0.8419187068939209 = 0.2017613649368286 + 0.1 * 6.401573181152344
Epoch 370, val loss: 0.8509598970413208
Epoch 380, training loss: 0.8228984475135803 = 0.1828126311302185 + 0.1 * 6.400857925415039
Epoch 380, val loss: 0.8579925894737244
Epoch 390, training loss: 0.8046378493309021 = 0.165388822555542 + 0.1 * 6.392490386962891
Epoch 390, val loss: 0.8661829829216003
Epoch 400, training loss: 0.7893102765083313 = 0.14923910796642303 + 0.1 * 6.400711536407471
Epoch 400, val loss: 0.8754376173019409
Epoch 410, training loss: 0.7721301913261414 = 0.1340850442647934 + 0.1 * 6.380451202392578
Epoch 410, val loss: 0.8855313062667847
Epoch 420, training loss: 0.7564781308174133 = 0.11943952739238739 + 0.1 * 6.370385646820068
Epoch 420, val loss: 0.896726131439209
Epoch 430, training loss: 0.7436076998710632 = 0.10640616714954376 + 0.1 * 6.372015476226807
Epoch 430, val loss: 0.9090938568115234
Epoch 440, training loss: 0.7316835522651672 = 0.09570256620645523 + 0.1 * 6.359809398651123
Epoch 440, val loss: 0.9225426912307739
Epoch 450, training loss: 0.7245938181877136 = 0.08704523742198944 + 0.1 * 6.375485420227051
Epoch 450, val loss: 0.9369380474090576
Epoch 460, training loss: 0.7148919701576233 = 0.07957962900400162 + 0.1 * 6.353123188018799
Epoch 460, val loss: 0.952842652797699
Epoch 470, training loss: 0.7078173160552979 = 0.07302666455507278 + 0.1 * 6.347906589508057
Epoch 470, val loss: 0.9687139391899109
Epoch 480, training loss: 0.7033644914627075 = 0.06724251806735992 + 0.1 * 6.361219882965088
Epoch 480, val loss: 0.9840376377105713
Epoch 490, training loss: 0.696017861366272 = 0.062106989324092865 + 0.1 * 6.339108943939209
Epoch 490, val loss: 0.9996456503868103
Epoch 500, training loss: 0.6899547576904297 = 0.0575045682489872 + 0.1 * 6.3245015144348145
Epoch 500, val loss: 1.0152674913406372
Epoch 510, training loss: 0.6864938735961914 = 0.053357161581516266 + 0.1 * 6.331367015838623
Epoch 510, val loss: 1.0309075117111206
Epoch 520, training loss: 0.6821190118789673 = 0.049625325947999954 + 0.1 * 6.324936389923096
Epoch 520, val loss: 1.0463634729385376
Epoch 530, training loss: 0.6777331829071045 = 0.046250540763139725 + 0.1 * 6.314826488494873
Epoch 530, val loss: 1.0616897344589233
Epoch 540, training loss: 0.6748396754264832 = 0.04318871349096298 + 0.1 * 6.31650972366333
Epoch 540, val loss: 1.0767942667007446
Epoch 550, training loss: 0.6709569096565247 = 0.04041043296456337 + 0.1 * 6.305464267730713
Epoch 550, val loss: 1.0917270183563232
Epoch 560, training loss: 0.6684139966964722 = 0.03787364438176155 + 0.1 * 6.305403709411621
Epoch 560, val loss: 1.1063816547393799
Epoch 570, training loss: 0.6651521325111389 = 0.03556198999285698 + 0.1 * 6.295901298522949
Epoch 570, val loss: 1.1207882165908813
Epoch 580, training loss: 0.6627739667892456 = 0.033451154828071594 + 0.1 * 6.2932281494140625
Epoch 580, val loss: 1.1350229978561401
Epoch 590, training loss: 0.6608585119247437 = 0.03151167556643486 + 0.1 * 6.293468475341797
Epoch 590, val loss: 1.149017572402954
Epoch 600, training loss: 0.6581223011016846 = 0.029733510687947273 + 0.1 * 6.2838873863220215
Epoch 600, val loss: 1.1626591682434082
Epoch 610, training loss: 0.6563583612442017 = 0.028094887733459473 + 0.1 * 6.282634735107422
Epoch 610, val loss: 1.1761265993118286
Epoch 620, training loss: 0.6562312245368958 = 0.026586687192320824 + 0.1 * 6.296444892883301
Epoch 620, val loss: 1.1892279386520386
Epoch 630, training loss: 0.6528289914131165 = 0.025200022384524345 + 0.1 * 6.276289939880371
Epoch 630, val loss: 1.2021640539169312
Epoch 640, training loss: 0.6502922177314758 = 0.023918980732560158 + 0.1 * 6.263731956481934
Epoch 640, val loss: 1.2148306369781494
Epoch 650, training loss: 0.651567280292511 = 0.022730402648448944 + 0.1 * 6.2883687019348145
Epoch 650, val loss: 1.2272179126739502
Epoch 660, training loss: 0.6484111547470093 = 0.02163105271756649 + 0.1 * 6.267800807952881
Epoch 660, val loss: 1.239244818687439
Epoch 670, training loss: 0.6465002298355103 = 0.020610978826880455 + 0.1 * 6.258892059326172
Epoch 670, val loss: 1.2511029243469238
Epoch 680, training loss: 0.6463415026664734 = 0.01966048963367939 + 0.1 * 6.266810417175293
Epoch 680, val loss: 1.2627217769622803
Epoch 690, training loss: 0.6445671916007996 = 0.01877729594707489 + 0.1 * 6.257898807525635
Epoch 690, val loss: 1.273939609527588
Epoch 700, training loss: 0.6428995132446289 = 0.017953218892216682 + 0.1 * 6.249462604522705
Epoch 700, val loss: 1.2851330041885376
Epoch 710, training loss: 0.6425211429595947 = 0.01718270778656006 + 0.1 * 6.253384113311768
Epoch 710, val loss: 1.2959846258163452
Epoch 720, training loss: 0.6413922309875488 = 0.016461169347167015 + 0.1 * 6.24931001663208
Epoch 720, val loss: 1.3066340684890747
Epoch 730, training loss: 0.640668511390686 = 0.015787338837981224 + 0.1 * 6.2488112449646
Epoch 730, val loss: 1.3168132305145264
Epoch 740, training loss: 0.6385890245437622 = 0.015156456269323826 + 0.1 * 6.234325408935547
Epoch 740, val loss: 1.3271328210830688
Epoch 750, training loss: 0.6384628415107727 = 0.014563512988388538 + 0.1 * 6.238993167877197
Epoch 750, val loss: 1.3370497226715088
Epoch 760, training loss: 0.6374372839927673 = 0.014005783945322037 + 0.1 * 6.234314918518066
Epoch 760, val loss: 1.346875548362732
Epoch 770, training loss: 0.6374099254608154 = 0.013480709865689278 + 0.1 * 6.239291667938232
Epoch 770, val loss: 1.3562121391296387
Epoch 780, training loss: 0.6353380680084229 = 0.012987958267331123 + 0.1 * 6.223500728607178
Epoch 780, val loss: 1.365633487701416
Epoch 790, training loss: 0.6343375444412231 = 0.01252219919115305 + 0.1 * 6.21815299987793
Epoch 790, val loss: 1.3747717142105103
Epoch 800, training loss: 0.6352965831756592 = 0.012082072906196117 + 0.1 * 6.232144832611084
Epoch 800, val loss: 1.383722186088562
Epoch 810, training loss: 0.6338590979576111 = 0.011665585450828075 + 0.1 * 6.221935272216797
Epoch 810, val loss: 1.3924883604049683
Epoch 820, training loss: 0.6335647106170654 = 0.011272608302533627 + 0.1 * 6.222920894622803
Epoch 820, val loss: 1.401138186454773
Epoch 830, training loss: 0.6321290731430054 = 0.010899428278207779 + 0.1 * 6.212296485900879
Epoch 830, val loss: 1.4095481634140015
Epoch 840, training loss: 0.6320098638534546 = 0.010546408593654633 + 0.1 * 6.214634895324707
Epoch 840, val loss: 1.4179497957229614
Epoch 850, training loss: 0.6322873830795288 = 0.010210234671831131 + 0.1 * 6.220771312713623
Epoch 850, val loss: 1.4259207248687744
Epoch 860, training loss: 0.6314026713371277 = 0.00989244319498539 + 0.1 * 6.215102195739746
Epoch 860, val loss: 1.4340879917144775
Epoch 870, training loss: 0.6309254169464111 = 0.009589321911334991 + 0.1 * 6.213360786437988
Epoch 870, val loss: 1.4417880773544312
Epoch 880, training loss: 0.6299598813056946 = 0.009301756508648396 + 0.1 * 6.206581115722656
Epoch 880, val loss: 1.449567198753357
Epoch 890, training loss: 0.6283678412437439 = 0.009028147906064987 + 0.1 * 6.193397045135498
Epoch 890, val loss: 1.4569809436798096
Epoch 900, training loss: 0.6287554502487183 = 0.008767408318817616 + 0.1 * 6.199880123138428
Epoch 900, val loss: 1.4644818305969238
Epoch 910, training loss: 0.6284276843070984 = 0.008518287912011147 + 0.1 * 6.199093818664551
Epoch 910, val loss: 1.4716006517410278
Epoch 920, training loss: 0.6276601552963257 = 0.008280843496322632 + 0.1 * 6.193792819976807
Epoch 920, val loss: 1.4788899421691895
Epoch 930, training loss: 0.6281188726425171 = 0.00805397517979145 + 0.1 * 6.200648784637451
Epoch 930, val loss: 1.4858336448669434
Epoch 940, training loss: 0.626919686794281 = 0.007837587967514992 + 0.1 * 6.190820693969727
Epoch 940, val loss: 1.492692470550537
Epoch 950, training loss: 0.6263666152954102 = 0.007630310487002134 + 0.1 * 6.187363147735596
Epoch 950, val loss: 1.4995219707489014
Epoch 960, training loss: 0.6258654594421387 = 0.007431767415255308 + 0.1 * 6.184337139129639
Epoch 960, val loss: 1.5061030387878418
Epoch 970, training loss: 0.625514566898346 = 0.007241820450872183 + 0.1 * 6.182727336883545
Epoch 970, val loss: 1.5127326250076294
Epoch 980, training loss: 0.6255773305892944 = 0.007059544324874878 + 0.1 * 6.185177326202393
Epoch 980, val loss: 1.51904296875
Epoch 990, training loss: 0.6252481341362 = 0.006884839851409197 + 0.1 * 6.183633327484131
Epoch 990, val loss: 1.5253829956054688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.6716
Flip ASR: 0.6222/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7887141704559326 = 1.951326847076416 + 0.1 * 8.373872756958008
Epoch 0, val loss: 1.9467334747314453
Epoch 10, training loss: 2.778369665145874 = 1.9409984350204468 + 0.1 * 8.373711585998535
Epoch 10, val loss: 1.9360523223876953
Epoch 20, training loss: 2.765693187713623 = 1.9284120798110962 + 0.1 * 8.372812271118164
Epoch 20, val loss: 1.9228160381317139
Epoch 30, training loss: 2.747429370880127 = 1.910860538482666 + 0.1 * 8.365687370300293
Epoch 30, val loss: 1.904328465461731
Epoch 40, training loss: 2.7165753841400146 = 1.8849666118621826 + 0.1 * 8.316086769104004
Epoch 40, val loss: 1.877555251121521
Epoch 50, training loss: 2.6484808921813965 = 1.849871277809143 + 0.1 * 7.986096382141113
Epoch 50, val loss: 1.8432267904281616
Epoch 60, training loss: 2.5669710636138916 = 1.8098020553588867 + 0.1 * 7.571689605712891
Epoch 60, val loss: 1.806288719177246
Epoch 70, training loss: 2.4892213344573975 = 1.7703659534454346 + 0.1 * 7.188554286956787
Epoch 70, val loss: 1.7722841501235962
Epoch 80, training loss: 2.4224815368652344 = 1.7316877841949463 + 0.1 * 6.907936096191406
Epoch 80, val loss: 1.7405096292495728
Epoch 90, training loss: 2.362137794494629 = 1.6834794282913208 + 0.1 * 6.786582946777344
Epoch 90, val loss: 1.69923996925354
Epoch 100, training loss: 2.2919254302978516 = 1.618345022201538 + 0.1 * 6.735804557800293
Epoch 100, val loss: 1.6437268257141113
Epoch 110, training loss: 2.206895589828491 = 1.5363284349441528 + 0.1 * 6.705671787261963
Epoch 110, val loss: 1.5757251977920532
Epoch 120, training loss: 2.1115543842315674 = 1.443004846572876 + 0.1 * 6.685495376586914
Epoch 120, val loss: 1.499722957611084
Epoch 130, training loss: 2.0132040977478027 = 1.346105933189392 + 0.1 * 6.670981407165527
Epoch 130, val loss: 1.424095869064331
Epoch 140, training loss: 1.9160665273666382 = 1.2502570152282715 + 0.1 * 6.658094882965088
Epoch 140, val loss: 1.3516086339950562
Epoch 150, training loss: 1.8206093311309814 = 1.156099796295166 + 0.1 * 6.6450958251953125
Epoch 150, val loss: 1.2814832925796509
Epoch 160, training loss: 1.7261953353881836 = 1.0630306005477905 + 0.1 * 6.631648063659668
Epoch 160, val loss: 1.2121702432632446
Epoch 170, training loss: 1.6337146759033203 = 0.9715684652328491 + 0.1 * 6.621461391448975
Epoch 170, val loss: 1.1433888673782349
Epoch 180, training loss: 1.5439445972442627 = 0.8827404975891113 + 0.1 * 6.612041473388672
Epoch 180, val loss: 1.0752760171890259
Epoch 190, training loss: 1.4582397937774658 = 0.7977010011672974 + 0.1 * 6.6053876876831055
Epoch 190, val loss: 1.0101059675216675
Epoch 200, training loss: 1.3769806623458862 = 0.7173580527305603 + 0.1 * 6.596225738525391
Epoch 200, val loss: 0.9491284489631653
Epoch 210, training loss: 1.3009426593780518 = 0.6420198082923889 + 0.1 * 6.589229106903076
Epoch 210, val loss: 0.8945508003234863
Epoch 220, training loss: 1.229841947555542 = 0.5718169808387756 + 0.1 * 6.580250263214111
Epoch 220, val loss: 0.8465806841850281
Epoch 230, training loss: 1.1635401248931885 = 0.5058040618896484 + 0.1 * 6.5773606300354
Epoch 230, val loss: 0.8042517900466919
Epoch 240, training loss: 1.1005144119262695 = 0.4441572427749634 + 0.1 * 6.563571453094482
Epoch 240, val loss: 0.7671934366226196
Epoch 250, training loss: 1.0423011779785156 = 0.3866345286369324 + 0.1 * 6.556666374206543
Epoch 250, val loss: 0.7347959280014038
Epoch 260, training loss: 0.9894742965698242 = 0.33395299315452576 + 0.1 * 6.555213451385498
Epoch 260, val loss: 0.7073246836662292
Epoch 270, training loss: 0.9413015842437744 = 0.28719082474708557 + 0.1 * 6.541107177734375
Epoch 270, val loss: 0.6850447058677673
Epoch 280, training loss: 0.9005347490310669 = 0.24646688997745514 + 0.1 * 6.54067850112915
Epoch 280, val loss: 0.6676945686340332
Epoch 290, training loss: 0.8647159337997437 = 0.21198777854442596 + 0.1 * 6.527281284332275
Epoch 290, val loss: 0.6552187204360962
Epoch 300, training loss: 0.8346014022827148 = 0.18302664160728455 + 0.1 * 6.515747547149658
Epoch 300, val loss: 0.6473546028137207
Epoch 310, training loss: 0.8096000552177429 = 0.15885476768016815 + 0.1 * 6.507452487945557
Epoch 310, val loss: 0.6435622572898865
Epoch 320, training loss: 0.7907288074493408 = 0.1388869732618332 + 0.1 * 6.518417835235596
Epoch 320, val loss: 0.6431466937065125
Epoch 330, training loss: 0.7716742753982544 = 0.1224663108587265 + 0.1 * 6.492079257965088
Epoch 330, val loss: 0.6453352570533752
Epoch 340, training loss: 0.7570639848709106 = 0.10871925950050354 + 0.1 * 6.483447551727295
Epoch 340, val loss: 0.6494877338409424
Epoch 350, training loss: 0.7455406785011292 = 0.0971960797905922 + 0.1 * 6.483445644378662
Epoch 350, val loss: 0.6548141241073608
Epoch 360, training loss: 0.7338039875030518 = 0.08745798468589783 + 0.1 * 6.4634599685668945
Epoch 360, val loss: 0.6611022353172302
Epoch 370, training loss: 0.7242951393127441 = 0.07910414040088654 + 0.1 * 6.451910018920898
Epoch 370, val loss: 0.6680163145065308
Epoch 380, training loss: 0.7164319753646851 = 0.07189800590276718 + 0.1 * 6.445339679718018
Epoch 380, val loss: 0.6751663684844971
Epoch 390, training loss: 0.7090225219726562 = 0.06568079441785812 + 0.1 * 6.433417320251465
Epoch 390, val loss: 0.6825466156005859
Epoch 400, training loss: 0.703025221824646 = 0.06023270636796951 + 0.1 * 6.427925109863281
Epoch 400, val loss: 0.6900095343589783
Epoch 410, training loss: 0.6967883706092834 = 0.055454958230257034 + 0.1 * 6.413333892822266
Epoch 410, val loss: 0.6973505616188049
Epoch 420, training loss: 0.6909700036048889 = 0.05123547092080116 + 0.1 * 6.397345066070557
Epoch 420, val loss: 0.7047625780105591
Epoch 430, training loss: 0.6884818077087402 = 0.04746442660689354 + 0.1 * 6.410173416137695
Epoch 430, val loss: 0.7121953368186951
Epoch 440, training loss: 0.6830217838287354 = 0.0441012866795063 + 0.1 * 6.389204978942871
Epoch 440, val loss: 0.719485878944397
Epoch 450, training loss: 0.6790956854820251 = 0.04108402505517006 + 0.1 * 6.3801164627075195
Epoch 450, val loss: 0.7267526984214783
Epoch 460, training loss: 0.6752158403396606 = 0.03835909813642502 + 0.1 * 6.368566989898682
Epoch 460, val loss: 0.7339673042297363
Epoch 470, training loss: 0.6718609929084778 = 0.035896185785532 + 0.1 * 6.359647750854492
Epoch 470, val loss: 0.7410076260566711
Epoch 480, training loss: 0.6690559983253479 = 0.033658623695373535 + 0.1 * 6.353973865509033
Epoch 480, val loss: 0.7480669021606445
Epoch 490, training loss: 0.6664862036705017 = 0.031620822846889496 + 0.1 * 6.348653793334961
Epoch 490, val loss: 0.7549208402633667
Epoch 500, training loss: 0.6639564633369446 = 0.029763976112008095 + 0.1 * 6.341924667358398
Epoch 500, val loss: 0.7617202997207642
Epoch 510, training loss: 0.6616125702857971 = 0.028061518445611 + 0.1 * 6.33551025390625
Epoch 510, val loss: 0.7683435678482056
Epoch 520, training loss: 0.6601643562316895 = 0.026501232758164406 + 0.1 * 6.3366312980651855
Epoch 520, val loss: 0.7749338150024414
Epoch 530, training loss: 0.6567064523696899 = 0.02506583370268345 + 0.1 * 6.316405773162842
Epoch 530, val loss: 0.7813558578491211
Epoch 540, training loss: 0.6577234268188477 = 0.023740587756037712 + 0.1 * 6.3398284912109375
Epoch 540, val loss: 0.7876434922218323
Epoch 550, training loss: 0.6548493504524231 = 0.022524932399392128 + 0.1 * 6.323244094848633
Epoch 550, val loss: 0.7937487959861755
Epoch 560, training loss: 0.6520063877105713 = 0.021405989304184914 + 0.1 * 6.306003570556641
Epoch 560, val loss: 0.7999077439308167
Epoch 570, training loss: 0.6512832641601562 = 0.020367251709103584 + 0.1 * 6.309159755706787
Epoch 570, val loss: 0.8057218194007874
Epoch 580, training loss: 0.6491702795028687 = 0.01940428651869297 + 0.1 * 6.297659873962402
Epoch 580, val loss: 0.8115594387054443
Epoch 590, training loss: 0.6482821106910706 = 0.018509281799197197 + 0.1 * 6.297728538513184
Epoch 590, val loss: 0.8172131180763245
Epoch 600, training loss: 0.6466284990310669 = 0.017676912248134613 + 0.1 * 6.289515972137451
Epoch 600, val loss: 0.8227851986885071
Epoch 610, training loss: 0.6451257467269897 = 0.016899729147553444 + 0.1 * 6.282260417938232
Epoch 610, val loss: 0.8282203674316406
Epoch 620, training loss: 0.6430185437202454 = 0.016175365075469017 + 0.1 * 6.268431663513184
Epoch 620, val loss: 0.8335477113723755
Epoch 630, training loss: 0.6422561407089233 = 0.01549920067191124 + 0.1 * 6.267569541931152
Epoch 630, val loss: 0.8388018608093262
Epoch 640, training loss: 0.6434494853019714 = 0.014864039607346058 + 0.1 * 6.285854339599609
Epoch 640, val loss: 0.8439138531684875
Epoch 650, training loss: 0.6404628157615662 = 0.014269940555095673 + 0.1 * 6.261928558349609
Epoch 650, val loss: 0.8488757610321045
Epoch 660, training loss: 0.6416690349578857 = 0.013712861575186253 + 0.1 * 6.279561519622803
Epoch 660, val loss: 0.8537852168083191
Epoch 670, training loss: 0.6402978897094727 = 0.013190718367695808 + 0.1 * 6.271071434020996
Epoch 670, val loss: 0.8585267066955566
Epoch 680, training loss: 0.6376324892044067 = 0.012701590545475483 + 0.1 * 6.249309062957764
Epoch 680, val loss: 0.8632229566574097
Epoch 690, training loss: 0.6366835832595825 = 0.012239883653819561 + 0.1 * 6.244436740875244
Epoch 690, val loss: 0.8677922487258911
Epoch 700, training loss: 0.6375278830528259 = 0.011801696382462978 + 0.1 * 6.257261753082275
Epoch 700, val loss: 0.8722569346427917
Epoch 710, training loss: 0.6350607872009277 = 0.011388270184397697 + 0.1 * 6.236725330352783
Epoch 710, val loss: 0.8766181468963623
Epoch 720, training loss: 0.6354866027832031 = 0.010998900979757309 + 0.1 * 6.244876861572266
Epoch 720, val loss: 0.8810378909111023
Epoch 730, training loss: 0.6342679858207703 = 0.010629492811858654 + 0.1 * 6.236384868621826
Epoch 730, val loss: 0.8852267265319824
Epoch 740, training loss: 0.6341776251792908 = 0.010281391441822052 + 0.1 * 6.238962173461914
Epoch 740, val loss: 0.8894417881965637
Epoch 750, training loss: 0.6331132054328918 = 0.00995063316076994 + 0.1 * 6.231625556945801
Epoch 750, val loss: 0.8935297131538391
Epoch 760, training loss: 0.6333751678466797 = 0.009635604918003082 + 0.1 * 6.237395286560059
Epoch 760, val loss: 0.8974783420562744
Epoch 770, training loss: 0.6325377821922302 = 0.00933745689690113 + 0.1 * 6.2320027351379395
Epoch 770, val loss: 0.9013857841491699
Epoch 780, training loss: 0.6323637366294861 = 0.009053968824446201 + 0.1 * 6.233097553253174
Epoch 780, val loss: 0.9052607417106628
Epoch 790, training loss: 0.6309598684310913 = 0.00878466572612524 + 0.1 * 6.221752166748047
Epoch 790, val loss: 0.9090293645858765
Epoch 800, training loss: 0.631075382232666 = 0.008527666330337524 + 0.1 * 6.22547721862793
Epoch 800, val loss: 0.912777304649353
Epoch 810, training loss: 0.6300638318061829 = 0.008282486349344254 + 0.1 * 6.217813491821289
Epoch 810, val loss: 0.916374146938324
Epoch 820, training loss: 0.6295000314712524 = 0.008049427531659603 + 0.1 * 6.214505672454834
Epoch 820, val loss: 0.9200308918952942
Epoch 830, training loss: 0.6295304298400879 = 0.007825884036719799 + 0.1 * 6.217044830322266
Epoch 830, val loss: 0.9235522150993347
Epoch 840, training loss: 0.6294093132019043 = 0.007612710352987051 + 0.1 * 6.217966079711914
Epoch 840, val loss: 0.9269850254058838
Epoch 850, training loss: 0.6284922361373901 = 0.00740913487970829 + 0.1 * 6.2108306884765625
Epoch 850, val loss: 0.9303863644599915
Epoch 860, training loss: 0.6274698376655579 = 0.007215508259832859 + 0.1 * 6.202543258666992
Epoch 860, val loss: 0.9338138699531555
Epoch 870, training loss: 0.6275575757026672 = 0.007028963882476091 + 0.1 * 6.205286026000977
Epoch 870, val loss: 0.9371334314346313
Epoch 880, training loss: 0.6287882328033447 = 0.006849620491266251 + 0.1 * 6.219385623931885
Epoch 880, val loss: 0.9402790665626526
Epoch 890, training loss: 0.6270065903663635 = 0.006678830366581678 + 0.1 * 6.203277587890625
Epoch 890, val loss: 0.9435372352600098
Epoch 900, training loss: 0.6260694861412048 = 0.00651506707072258 + 0.1 * 6.1955437660217285
Epoch 900, val loss: 0.9467335939407349
Epoch 910, training loss: 0.6280032992362976 = 0.006356783676892519 + 0.1 * 6.216464996337891
Epoch 910, val loss: 0.949874997138977
Epoch 920, training loss: 0.6260888576507568 = 0.006204907316714525 + 0.1 * 6.19883918762207
Epoch 920, val loss: 0.9527984857559204
Epoch 930, training loss: 0.6249883770942688 = 0.006060358136892319 + 0.1 * 6.189280033111572
Epoch 930, val loss: 0.955863893032074
Epoch 940, training loss: 0.6259357929229736 = 0.0059205531142652035 + 0.1 * 6.200152397155762
Epoch 940, val loss: 0.958878219127655
Epoch 950, training loss: 0.625701367855072 = 0.005785967223346233 + 0.1 * 6.199154376983643
Epoch 950, val loss: 0.9617221355438232
Epoch 960, training loss: 0.6241106390953064 = 0.005657075438648462 + 0.1 * 6.184535503387451
Epoch 960, val loss: 0.9645907878875732
Epoch 970, training loss: 0.6239479780197144 = 0.005532918497920036 + 0.1 * 6.184150695800781
Epoch 970, val loss: 0.9674992561340332
Epoch 980, training loss: 0.6246080994606018 = 0.005412392783910036 + 0.1 * 6.191956996917725
Epoch 980, val loss: 0.9701938033103943
Epoch 990, training loss: 0.6237534284591675 = 0.005296660121530294 + 0.1 * 6.184567928314209
Epoch 990, val loss: 0.9728278517723083
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.7343
Flip ASR: 0.6889/225 nodes
The final ASR:0.68143, 0.03978, Accuracy:0.81358, 0.03148
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10550])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.97663, 0.01141, Accuracy:0.83580, 0.00698
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7955374717712402 = 1.9581462144851685 + 0.1 * 8.373912811279297
Epoch 0, val loss: 1.9602388143539429
Epoch 10, training loss: 2.784177780151367 = 1.9467977285385132 + 0.1 * 8.373801231384277
Epoch 10, val loss: 1.948378086090088
Epoch 20, training loss: 2.7699804306030273 = 1.9326590299606323 + 0.1 * 8.373212814331055
Epoch 20, val loss: 1.9334739446640015
Epoch 30, training loss: 2.7496275901794434 = 1.9127764701843262 + 0.1 * 8.368510246276855
Epoch 30, val loss: 1.9125094413757324
Epoch 40, training loss: 2.7173187732696533 = 1.8836373090744019 + 0.1 * 8.336814880371094
Epoch 40, val loss: 1.8823230266571045
Epoch 50, training loss: 2.6586413383483887 = 1.8445022106170654 + 0.1 * 8.14139175415039
Epoch 50, val loss: 1.8444156646728516
Epoch 60, training loss: 2.5920209884643555 = 1.8022823333740234 + 0.1 * 7.897385597229004
Epoch 60, val loss: 1.807854175567627
Epoch 70, training loss: 2.5156660079956055 = 1.7644269466400146 + 0.1 * 7.512389183044434
Epoch 70, val loss: 1.7789101600646973
Epoch 80, training loss: 2.4306883811950684 = 1.7245525121688843 + 0.1 * 7.061357498168945
Epoch 80, val loss: 1.7469712495803833
Epoch 90, training loss: 2.3595242500305176 = 1.6731759309768677 + 0.1 * 6.863483905792236
Epoch 90, val loss: 1.7040094137191772
Epoch 100, training loss: 2.2828404903411865 = 1.6039791107177734 + 0.1 * 6.788613796234131
Epoch 100, val loss: 1.646098256111145
Epoch 110, training loss: 2.1913115978240967 = 1.5168519020080566 + 0.1 * 6.744596481323242
Epoch 110, val loss: 1.5748122930526733
Epoch 120, training loss: 2.08882212638855 = 1.4169787168502808 + 0.1 * 6.718433380126953
Epoch 120, val loss: 1.4963093996047974
Epoch 130, training loss: 1.9819867610931396 = 1.3121436834335327 + 0.1 * 6.698431015014648
Epoch 130, val loss: 1.4159928560256958
Epoch 140, training loss: 1.8755450248718262 = 1.207664966583252 + 0.1 * 6.678800582885742
Epoch 140, val loss: 1.337220311164856
Epoch 150, training loss: 1.7751843929290771 = 1.1088831424713135 + 0.1 * 6.663012981414795
Epoch 150, val loss: 1.2624021768569946
Epoch 160, training loss: 1.6824839115142822 = 1.0176494121551514 + 0.1 * 6.648345470428467
Epoch 160, val loss: 1.1933937072753906
Epoch 170, training loss: 1.5963926315307617 = 0.9326924085617065 + 0.1 * 6.637002468109131
Epoch 170, val loss: 1.1291253566741943
Epoch 180, training loss: 1.5164352655410767 = 0.8540953993797302 + 0.1 * 6.623398303985596
Epoch 180, val loss: 1.07124924659729
Epoch 190, training loss: 1.441431999206543 = 0.7805401086807251 + 0.1 * 6.608919620513916
Epoch 190, val loss: 1.0186467170715332
Epoch 200, training loss: 1.3731498718261719 = 0.7116369009017944 + 0.1 * 6.615128993988037
Epoch 200, val loss: 0.9709433317184448
Epoch 210, training loss: 1.3080296516418457 = 0.6489959955215454 + 0.1 * 6.590336322784424
Epoch 210, val loss: 0.9293246865272522
Epoch 220, training loss: 1.2496486902236938 = 0.591606080532074 + 0.1 * 6.58042573928833
Epoch 220, val loss: 0.8928391933441162
Epoch 230, training loss: 1.1956408023834229 = 0.5389853119850159 + 0.1 * 6.566554069519043
Epoch 230, val loss: 0.8615525364875793
Epoch 240, training loss: 1.1472033262252808 = 0.49068039655685425 + 0.1 * 6.5652289390563965
Epoch 240, val loss: 0.835051417350769
Epoch 250, training loss: 1.1016077995300293 = 0.4467446804046631 + 0.1 * 6.548631191253662
Epoch 250, val loss: 0.8136268854141235
Epoch 260, training loss: 1.0598912239074707 = 0.40595632791519165 + 0.1 * 6.539348125457764
Epoch 260, val loss: 0.7955760955810547
Epoch 270, training loss: 1.02254319190979 = 0.3678531050682068 + 0.1 * 6.546900749206543
Epoch 270, val loss: 0.7805225253105164
Epoch 280, training loss: 0.9847699403762817 = 0.3324911296367645 + 0.1 * 6.5227885246276855
Epoch 280, val loss: 0.7684361338615417
Epoch 290, training loss: 0.950110912322998 = 0.2994571626186371 + 0.1 * 6.506537914276123
Epoch 290, val loss: 0.7585388422012329
Epoch 300, training loss: 0.9187948703765869 = 0.2686520516872406 + 0.1 * 6.50142765045166
Epoch 300, val loss: 0.7507996559143066
Epoch 310, training loss: 0.889457643032074 = 0.24038083851337433 + 0.1 * 6.490767955780029
Epoch 310, val loss: 0.7451252937316895
Epoch 320, training loss: 0.8630980253219604 = 0.21463248133659363 + 0.1 * 6.484654903411865
Epoch 320, val loss: 0.741584062576294
Epoch 330, training loss: 0.8390595316886902 = 0.1913313865661621 + 0.1 * 6.477281093597412
Epoch 330, val loss: 0.7402869462966919
Epoch 340, training loss: 0.8167821764945984 = 0.17059971392154694 + 0.1 * 6.461824417114258
Epoch 340, val loss: 0.7412033677101135
Epoch 350, training loss: 0.7998843193054199 = 0.15222467482089996 + 0.1 * 6.476596355438232
Epoch 350, val loss: 0.7441213130950928
Epoch 360, training loss: 0.7804664373397827 = 0.136201411485672 + 0.1 * 6.442650318145752
Epoch 360, val loss: 0.7490606904029846
Epoch 370, training loss: 0.765905499458313 = 0.1222304031252861 + 0.1 * 6.436750888824463
Epoch 370, val loss: 0.7557015419006348
Epoch 380, training loss: 0.7539504766464233 = 0.11009926348924637 + 0.1 * 6.438511848449707
Epoch 380, val loss: 0.7636916637420654
Epoch 390, training loss: 0.7411055564880371 = 0.09953875094652176 + 0.1 * 6.41566801071167
Epoch 390, val loss: 0.7728619575500488
Epoch 400, training loss: 0.7315891981124878 = 0.09029233455657959 + 0.1 * 6.412968635559082
Epoch 400, val loss: 0.782917857170105
Epoch 410, training loss: 0.7234956622123718 = 0.08221080154180527 + 0.1 * 6.412848949432373
Epoch 410, val loss: 0.7935015559196472
Epoch 420, training loss: 0.7157155871391296 = 0.07512225210666656 + 0.1 * 6.405933380126953
Epoch 420, val loss: 0.8045778274536133
Epoch 430, training loss: 0.7084150314331055 = 0.0688682273030281 + 0.1 * 6.395467758178711
Epoch 430, val loss: 0.8158350586891174
Epoch 440, training loss: 0.7014051079750061 = 0.06333458423614502 + 0.1 * 6.3807053565979
Epoch 440, val loss: 0.8273089528083801
Epoch 450, training loss: 0.695530354976654 = 0.058422401547431946 + 0.1 * 6.371079444885254
Epoch 450, val loss: 0.8387138843536377
Epoch 460, training loss: 0.6901741623878479 = 0.054037198424339294 + 0.1 * 6.361369609832764
Epoch 460, val loss: 0.8502326607704163
Epoch 470, training loss: 0.6891948580741882 = 0.050106946378946304 + 0.1 * 6.390879154205322
Epoch 470, val loss: 0.8615708351135254
Epoch 480, training loss: 0.6828087568283081 = 0.046587470918893814 + 0.1 * 6.362212657928467
Epoch 480, val loss: 0.8728020191192627
Epoch 490, training loss: 0.6783292889595032 = 0.04341446980834007 + 0.1 * 6.349148273468018
Epoch 490, val loss: 0.8839108943939209
Epoch 500, training loss: 0.6749511957168579 = 0.040542036294937134 + 0.1 * 6.344090938568115
Epoch 500, val loss: 0.8947510123252869
Epoch 510, training loss: 0.6711872816085815 = 0.037934571504592896 + 0.1 * 6.332527160644531
Epoch 510, val loss: 0.9056630730628967
Epoch 520, training loss: 0.6682192087173462 = 0.03556029126048088 + 0.1 * 6.326589107513428
Epoch 520, val loss: 0.9161788821220398
Epoch 530, training loss: 0.6658679246902466 = 0.033396463841199875 + 0.1 * 6.324714183807373
Epoch 530, val loss: 0.9267735481262207
Epoch 540, training loss: 0.6634112000465393 = 0.03141584247350693 + 0.1 * 6.319953441619873
Epoch 540, val loss: 0.9371135830879211
Epoch 550, training loss: 0.6639911532402039 = 0.02960299700498581 + 0.1 * 6.343881130218506
Epoch 550, val loss: 0.9472000598907471
Epoch 560, training loss: 0.6599701642990112 = 0.02794419787824154 + 0.1 * 6.3202595710754395
Epoch 560, val loss: 0.9571482539176941
Epoch 570, training loss: 0.6568325757980347 = 0.026417359709739685 + 0.1 * 6.304152011871338
Epoch 570, val loss: 0.9670162796974182
Epoch 580, training loss: 0.6550913453102112 = 0.025005951523780823 + 0.1 * 6.300854206085205
Epoch 580, val loss: 0.9766678214073181
Epoch 590, training loss: 0.6537591218948364 = 0.02370302565395832 + 0.1 * 6.30056095123291
Epoch 590, val loss: 0.985921323299408
Epoch 600, training loss: 0.6519378423690796 = 0.02249990403652191 + 0.1 * 6.294379234313965
Epoch 600, val loss: 0.9952605962753296
Epoch 610, training loss: 0.6525839567184448 = 0.021384768187999725 + 0.1 * 6.3119916915893555
Epoch 610, val loss: 1.004263162612915
Epoch 620, training loss: 0.6495728492736816 = 0.02035108208656311 + 0.1 * 6.29221773147583
Epoch 620, val loss: 1.0130342245101929
Epoch 630, training loss: 0.6469228267669678 = 0.019391845911741257 + 0.1 * 6.275310039520264
Epoch 630, val loss: 1.0217314958572388
Epoch 640, training loss: 0.647478461265564 = 0.01849660836160183 + 0.1 * 6.289818286895752
Epoch 640, val loss: 1.030174970626831
Epoch 650, training loss: 0.646173894405365 = 0.017662761732935905 + 0.1 * 6.285111427307129
Epoch 650, val loss: 1.0383191108703613
Epoch 660, training loss: 0.6452286243438721 = 0.01688675582408905 + 0.1 * 6.283418655395508
Epoch 660, val loss: 1.0465779304504395
Epoch 670, training loss: 0.6429497003555298 = 0.016162196174263954 + 0.1 * 6.267874717712402
Epoch 670, val loss: 1.0543135404586792
Epoch 680, training loss: 0.6418050527572632 = 0.015485713258385658 + 0.1 * 6.263193130493164
Epoch 680, val loss: 1.0622174739837646
Epoch 690, training loss: 0.6423293352127075 = 0.014851049520075321 + 0.1 * 6.274783134460449
Epoch 690, val loss: 1.0697402954101562
Epoch 700, training loss: 0.6399016380310059 = 0.014256162568926811 + 0.1 * 6.2564544677734375
Epoch 700, val loss: 1.0771695375442505
Epoch 710, training loss: 0.6401494741439819 = 0.013696861453354359 + 0.1 * 6.2645263671875
Epoch 710, val loss: 1.0844848155975342
Epoch 720, training loss: 0.639302134513855 = 0.013170881196856499 + 0.1 * 6.261312007904053
Epoch 720, val loss: 1.091652274131775
Epoch 730, training loss: 0.6391907930374146 = 0.012675509788095951 + 0.1 * 6.265152454376221
Epoch 730, val loss: 1.0986675024032593
Epoch 740, training loss: 0.6367859244346619 = 0.01220916211605072 + 0.1 * 6.245767116546631
Epoch 740, val loss: 1.1054686307907104
Epoch 750, training loss: 0.6362728476524353 = 0.011769089847803116 + 0.1 * 6.245037078857422
Epoch 750, val loss: 1.1122422218322754
Epoch 760, training loss: 0.6352401375770569 = 0.011353109031915665 + 0.1 * 6.238869667053223
Epoch 760, val loss: 1.1188113689422607
Epoch 770, training loss: 0.6370260715484619 = 0.010960416868329048 + 0.1 * 6.260656833648682
Epoch 770, val loss: 1.125233769416809
Epoch 780, training loss: 0.6345908045768738 = 0.01058940589427948 + 0.1 * 6.240013599395752
Epoch 780, val loss: 1.1314539909362793
Epoch 790, training loss: 0.6341122984886169 = 0.010238184593617916 + 0.1 * 6.238740921020508
Epoch 790, val loss: 1.1377780437469482
Epoch 800, training loss: 0.6325289607048035 = 0.009904596023261547 + 0.1 * 6.226243019104004
Epoch 800, val loss: 1.1438729763031006
Epoch 810, training loss: 0.6332857608795166 = 0.009587549604475498 + 0.1 * 6.236982345581055
Epoch 810, val loss: 1.1498533487319946
Epoch 820, training loss: 0.6341531276702881 = 0.009286466985940933 + 0.1 * 6.248666763305664
Epoch 820, val loss: 1.1557116508483887
Epoch 830, training loss: 0.6320299506187439 = 0.009000553749501705 + 0.1 * 6.230294227600098
Epoch 830, val loss: 1.161424160003662
Epoch 840, training loss: 0.6306098103523254 = 0.00872879009693861 + 0.1 * 6.218810081481934
Epoch 840, val loss: 1.1671396493911743
Epoch 850, training loss: 0.6331205368041992 = 0.008469643071293831 + 0.1 * 6.246508598327637
Epoch 850, val loss: 1.1727993488311768
Epoch 860, training loss: 0.6299048662185669 = 0.00822251196950674 + 0.1 * 6.216823101043701
Epoch 860, val loss: 1.1781138181686401
Epoch 870, training loss: 0.6293467283248901 = 0.007987100630998611 + 0.1 * 6.213596343994141
Epoch 870, val loss: 1.1835918426513672
Epoch 880, training loss: 0.6285935044288635 = 0.007762112654745579 + 0.1 * 6.208313465118408
Epoch 880, val loss: 1.188899278640747
Epoch 890, training loss: 0.6305218935012817 = 0.007547337561845779 + 0.1 * 6.229745388031006
Epoch 890, val loss: 1.193997859954834
Epoch 900, training loss: 0.628528892993927 = 0.007342248689383268 + 0.1 * 6.21186637878418
Epoch 900, val loss: 1.199029564857483
Epoch 910, training loss: 0.6279895901679993 = 0.007146468386054039 + 0.1 * 6.208430767059326
Epoch 910, val loss: 1.204233169555664
Epoch 920, training loss: 0.6279207468032837 = 0.006958410143852234 + 0.1 * 6.209623336791992
Epoch 920, val loss: 1.2090407609939575
Epoch 930, training loss: 0.6266723275184631 = 0.006778632290661335 + 0.1 * 6.198936462402344
Epoch 930, val loss: 1.213903546333313
Epoch 940, training loss: 0.6272287368774414 = 0.006606265436857939 + 0.1 * 6.20622444152832
Epoch 940, val loss: 1.218720555305481
Epoch 950, training loss: 0.6261337399482727 = 0.00644074659794569 + 0.1 * 6.196929931640625
Epoch 950, val loss: 1.2233489751815796
Epoch 960, training loss: 0.6263120174407959 = 0.006282282527536154 + 0.1 * 6.2002973556518555
Epoch 960, val loss: 1.2278860807418823
Epoch 970, training loss: 0.6253708004951477 = 0.006130393594503403 + 0.1 * 6.192403793334961
Epoch 970, val loss: 1.232494592666626
Epoch 980, training loss: 0.6257348656654358 = 0.005984194576740265 + 0.1 * 6.197506904602051
Epoch 980, val loss: 1.2371693849563599
Epoch 990, training loss: 0.6263335347175598 = 0.005842965561896563 + 0.1 * 6.2049055099487305
Epoch 990, val loss: 1.2413393259048462
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.5867
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.778319835662842 = 1.9409325122833252 + 0.1 * 8.373872756958008
Epoch 0, val loss: 1.9291630983352661
Epoch 10, training loss: 2.768665075302124 = 1.9312909841537476 + 0.1 * 8.373741149902344
Epoch 10, val loss: 1.9197380542755127
Epoch 20, training loss: 2.7567358016967773 = 1.9194389581680298 + 0.1 * 8.372968673706055
Epoch 20, val loss: 1.907776117324829
Epoch 30, training loss: 2.739309787750244 = 1.902613639831543 + 0.1 * 8.366961479187012
Epoch 30, val loss: 1.890581488609314
Epoch 40, training loss: 2.710556745529175 = 1.8774349689483643 + 0.1 * 8.331217765808105
Epoch 40, val loss: 1.8651624917984009
Epoch 50, training loss: 2.6585025787353516 = 1.8424392938613892 + 0.1 * 8.16063404083252
Epoch 50, val loss: 1.8315379619598389
Epoch 60, training loss: 2.5928359031677246 = 1.801250696182251 + 0.1 * 7.915853023529053
Epoch 60, val loss: 1.7944679260253906
Epoch 70, training loss: 2.5245320796966553 = 1.7597987651824951 + 0.1 * 7.647332191467285
Epoch 70, val loss: 1.7582175731658936
Epoch 80, training loss: 2.4396166801452637 = 1.7152425050735474 + 0.1 * 7.243741989135742
Epoch 80, val loss: 1.7170960903167725
Epoch 90, training loss: 2.362283229827881 = 1.6573787927627563 + 0.1 * 7.049044609069824
Epoch 90, val loss: 1.6639951467514038
Epoch 100, training loss: 2.277371406555176 = 1.5815945863723755 + 0.1 * 6.957769393920898
Epoch 100, val loss: 1.5978120565414429
Epoch 110, training loss: 2.180819511413574 = 1.4919054508209229 + 0.1 * 6.889141082763672
Epoch 110, val loss: 1.5219733715057373
Epoch 120, training loss: 2.0811469554901123 = 1.398270606994629 + 0.1 * 6.828763008117676
Epoch 120, val loss: 1.4453750848770142
Epoch 130, training loss: 1.9840714931488037 = 1.3067891597747803 + 0.1 * 6.772823333740234
Epoch 130, val loss: 1.374130368232727
Epoch 140, training loss: 1.8927823305130005 = 1.2191121578216553 + 0.1 * 6.736701488494873
Epoch 140, val loss: 1.309482455253601
Epoch 150, training loss: 1.8059161901474 = 1.1345744132995605 + 0.1 * 6.7134175300598145
Epoch 150, val loss: 1.2483484745025635
Epoch 160, training loss: 1.7188358306884766 = 1.049551248550415 + 0.1 * 6.692844867706299
Epoch 160, val loss: 1.187025785446167
Epoch 170, training loss: 1.6303825378417969 = 0.9629868865013123 + 0.1 * 6.673956871032715
Epoch 170, val loss: 1.125248670578003
Epoch 180, training loss: 1.542511224746704 = 0.8775880932807922 + 0.1 * 6.64923095703125
Epoch 180, val loss: 1.0642973184585571
Epoch 190, training loss: 1.4590798616409302 = 0.7965285778045654 + 0.1 * 6.625512599945068
Epoch 190, val loss: 1.0066324472427368
Epoch 200, training loss: 1.383885383605957 = 0.7233703136444092 + 0.1 * 6.60515022277832
Epoch 200, val loss: 0.955291211605072
Epoch 210, training loss: 1.3183209896087646 = 0.6594879627227783 + 0.1 * 6.588329792022705
Epoch 210, val loss: 0.9122298955917358
Epoch 220, training loss: 1.260947823524475 = 0.6039187908172607 + 0.1 * 6.5702900886535645
Epoch 220, val loss: 0.8767960071563721
Epoch 230, training loss: 1.213975191116333 = 0.5562403202056885 + 0.1 * 6.577348709106445
Epoch 230, val loss: 0.848696768283844
Epoch 240, training loss: 1.1698946952819824 = 0.5149151086807251 + 0.1 * 6.549795150756836
Epoch 240, val loss: 0.8267296552658081
Epoch 250, training loss: 1.1308841705322266 = 0.47771692276000977 + 0.1 * 6.53167200088501
Epoch 250, val loss: 0.80904620885849
Epoch 260, training loss: 1.0951277017593384 = 0.4434243142604828 + 0.1 * 6.517033576965332
Epoch 260, val loss: 0.7948932647705078
Epoch 270, training loss: 1.061898946762085 = 0.41132429242134094 + 0.1 * 6.505746364593506
Epoch 270, val loss: 0.7837769389152527
Epoch 280, training loss: 1.0317339897155762 = 0.3813404142856598 + 0.1 * 6.503935813903809
Epoch 280, val loss: 0.7753778100013733
Epoch 290, training loss: 1.0023987293243408 = 0.35369396209716797 + 0.1 * 6.48704719543457
Epoch 290, val loss: 0.7694439888000488
Epoch 300, training loss: 0.9758085012435913 = 0.32805711030960083 + 0.1 * 6.477513790130615
Epoch 300, val loss: 0.7657901048660278
Epoch 310, training loss: 0.9510446190834045 = 0.3042173981666565 + 0.1 * 6.4682722091674805
Epoch 310, val loss: 0.7641254663467407
Epoch 320, training loss: 0.9310070872306824 = 0.2820822596549988 + 0.1 * 6.489248275756836
Epoch 320, val loss: 0.7645606398582458
Epoch 330, training loss: 0.9079352617263794 = 0.2616923749446869 + 0.1 * 6.462428569793701
Epoch 330, val loss: 0.7667025327682495
Epoch 340, training loss: 0.8876413702964783 = 0.2427501678466797 + 0.1 * 6.448912143707275
Epoch 340, val loss: 0.7706525921821594
Epoch 350, training loss: 0.8710601329803467 = 0.2250465601682663 + 0.1 * 6.460135459899902
Epoch 350, val loss: 0.7761610746383667
Epoch 360, training loss: 0.851616621017456 = 0.20853270590305328 + 0.1 * 6.4308390617370605
Epoch 360, val loss: 0.7832822799682617
Epoch 370, training loss: 0.8356761336326599 = 0.1930263191461563 + 0.1 * 6.4264984130859375
Epoch 370, val loss: 0.7918282151222229
Epoch 380, training loss: 0.8252454400062561 = 0.17843161523342133 + 0.1 * 6.468137741088867
Epoch 380, val loss: 0.8020238876342773
Epoch 390, training loss: 0.8075525164604187 = 0.1649114042520523 + 0.1 * 6.426411151885986
Epoch 390, val loss: 0.8132827877998352
Epoch 400, training loss: 0.7933251261711121 = 0.15231017768383026 + 0.1 * 6.410149097442627
Epoch 400, val loss: 0.8256844878196716
Epoch 410, training loss: 0.7830724120140076 = 0.14054888486862183 + 0.1 * 6.425235271453857
Epoch 410, val loss: 0.8391821384429932
Epoch 420, training loss: 0.7696871757507324 = 0.12955570220947266 + 0.1 * 6.401314735412598
Epoch 420, val loss: 0.8535311222076416
Epoch 430, training loss: 0.7584837675094604 = 0.1192629337310791 + 0.1 * 6.392208099365234
Epoch 430, val loss: 0.8686639666557312
Epoch 440, training loss: 0.7485632300376892 = 0.10973722487688065 + 0.1 * 6.3882598876953125
Epoch 440, val loss: 0.8841546773910522
Epoch 450, training loss: 0.7407585382461548 = 0.1009167954325676 + 0.1 * 6.3984174728393555
Epoch 450, val loss: 0.9002761840820312
Epoch 460, training loss: 0.7309341430664062 = 0.09280639886856079 + 0.1 * 6.381277561187744
Epoch 460, val loss: 0.916171133518219
Epoch 470, training loss: 0.7225592136383057 = 0.08532650023698807 + 0.1 * 6.372326850891113
Epoch 470, val loss: 0.9324386119842529
Epoch 480, training loss: 0.7154173851013184 = 0.07848304510116577 + 0.1 * 6.369343280792236
Epoch 480, val loss: 0.9487876892089844
Epoch 490, training loss: 0.7086547017097473 = 0.07214159518480301 + 0.1 * 6.36513090133667
Epoch 490, val loss: 0.9648228287696838
Epoch 500, training loss: 0.7014913558959961 = 0.06619876623153687 + 0.1 * 6.352925777435303
Epoch 500, val loss: 0.9809064865112305
Epoch 510, training loss: 0.696212649345398 = 0.06056808680295944 + 0.1 * 6.3564453125
Epoch 510, val loss: 0.9970625042915344
Epoch 520, training loss: 0.6899024844169617 = 0.05525567755103111 + 0.1 * 6.346468448638916
Epoch 520, val loss: 1.013044834136963
Epoch 530, training loss: 0.6853494644165039 = 0.050451427698135376 + 0.1 * 6.34898042678833
Epoch 530, val loss: 1.0291093587875366
Epoch 540, training loss: 0.6815329790115356 = 0.046255797147750854 + 0.1 * 6.352772235870361
Epoch 540, val loss: 1.0451874732971191
Epoch 550, training loss: 0.6763948202133179 = 0.0427033007144928 + 0.1 * 6.336915493011475
Epoch 550, val loss: 1.060952067375183
Epoch 560, training loss: 0.673801839351654 = 0.039606932550668716 + 0.1 * 6.341948986053467
Epoch 560, val loss: 1.0764431953430176
Epoch 570, training loss: 0.6696355938911438 = 0.036877334117889404 + 0.1 * 6.327582359313965
Epoch 570, val loss: 1.091511845588684
Epoch 580, training loss: 0.6663401126861572 = 0.034445323050022125 + 0.1 * 6.318947792053223
Epoch 580, val loss: 1.1060574054718018
Epoch 590, training loss: 0.6651214361190796 = 0.03226388990879059 + 0.1 * 6.328575611114502
Epoch 590, val loss: 1.1201483011245728
Epoch 600, training loss: 0.6621014475822449 = 0.03029411844909191 + 0.1 * 6.318073272705078
Epoch 600, val loss: 1.1337226629257202
Epoch 610, training loss: 0.6608705520629883 = 0.028506793081760406 + 0.1 * 6.323637008666992
Epoch 610, val loss: 1.1469166278839111
Epoch 620, training loss: 0.6572486758232117 = 0.026879481971263885 + 0.1 * 6.303691864013672
Epoch 620, val loss: 1.1597356796264648
Epoch 630, training loss: 0.656582236289978 = 0.025388868525624275 + 0.1 * 6.311933517456055
Epoch 630, val loss: 1.1722030639648438
Epoch 640, training loss: 0.6555119752883911 = 0.02402248978614807 + 0.1 * 6.314894676208496
Epoch 640, val loss: 1.184380054473877
Epoch 650, training loss: 0.6527799963951111 = 0.022766929119825363 + 0.1 * 6.300130367279053
Epoch 650, val loss: 1.1962922811508179
Epoch 660, training loss: 0.6511491537094116 = 0.021609395742416382 + 0.1 * 6.295397758483887
Epoch 660, val loss: 1.2078932523727417
Epoch 670, training loss: 0.649875283241272 = 0.020540211349725723 + 0.1 * 6.2933502197265625
Epoch 670, val loss: 1.219225525856018
Epoch 680, training loss: 0.6477415561676025 = 0.019553454592823982 + 0.1 * 6.281880855560303
Epoch 680, val loss: 1.2302322387695312
Epoch 690, training loss: 0.6484603881835938 = 0.018637266010046005 + 0.1 * 6.298231601715088
Epoch 690, val loss: 1.2409873008728027
Epoch 700, training loss: 0.6460781097412109 = 0.01778864674270153 + 0.1 * 6.282894611358643
Epoch 700, val loss: 1.251558780670166
Epoch 710, training loss: 0.6456940174102783 = 0.016998838633298874 + 0.1 * 6.286952018737793
Epoch 710, val loss: 1.2616912126541138
Epoch 720, training loss: 0.6438624858856201 = 0.016263360157608986 + 0.1 * 6.275990962982178
Epoch 720, val loss: 1.271755576133728
Epoch 730, training loss: 0.6440792679786682 = 0.015575051307678223 + 0.1 * 6.285041809082031
Epoch 730, val loss: 1.281488299369812
Epoch 740, training loss: 0.6426173448562622 = 0.014931371435523033 + 0.1 * 6.276859283447266
Epoch 740, val loss: 1.2910581827163696
Epoch 750, training loss: 0.6409502029418945 = 0.014328958466649055 + 0.1 * 6.266212463378906
Epoch 750, val loss: 1.3004058599472046
Epoch 760, training loss: 0.6400265693664551 = 0.013763311319053173 + 0.1 * 6.262632369995117
Epoch 760, val loss: 1.3095792531967163
Epoch 770, training loss: 0.6406999826431274 = 0.013232195749878883 + 0.1 * 6.274677753448486
Epoch 770, val loss: 1.318610668182373
Epoch 780, training loss: 0.6414783596992493 = 0.012735114432871342 + 0.1 * 6.2874321937561035
Epoch 780, val loss: 1.327405571937561
Epoch 790, training loss: 0.6389022469520569 = 0.012270333245396614 + 0.1 * 6.266319274902344
Epoch 790, val loss: 1.3359235525131226
Epoch 800, training loss: 0.6373273134231567 = 0.011827703565359116 + 0.1 * 6.254995822906494
Epoch 800, val loss: 1.3441487550735474
Epoch 810, training loss: 0.6373640894889832 = 0.011410963721573353 + 0.1 * 6.259531497955322
Epoch 810, val loss: 1.3523982763290405
Epoch 820, training loss: 0.6367909908294678 = 0.01101716235280037 + 0.1 * 6.25773811340332
Epoch 820, val loss: 1.360461950302124
Epoch 830, training loss: 0.6350818872451782 = 0.01064571738243103 + 0.1 * 6.244361400604248
Epoch 830, val loss: 1.3684585094451904
Epoch 840, training loss: 0.6349337697029114 = 0.01029342133551836 + 0.1 * 6.246403217315674
Epoch 840, val loss: 1.3761520385742188
Epoch 850, training loss: 0.6345980763435364 = 0.009959098882973194 + 0.1 * 6.246389389038086
Epoch 850, val loss: 1.3837716579437256
Epoch 860, training loss: 0.6345447301864624 = 0.009644207544624805 + 0.1 * 6.24900484085083
Epoch 860, val loss: 1.3912231922149658
Epoch 870, training loss: 0.633913516998291 = 0.009343435056507587 + 0.1 * 6.245700836181641
Epoch 870, val loss: 1.398514986038208
Epoch 880, training loss: 0.6332176327705383 = 0.009057878516614437 + 0.1 * 6.241597652435303
Epoch 880, val loss: 1.4056357145309448
Epoch 890, training loss: 0.633609414100647 = 0.008786571212112904 + 0.1 * 6.248228549957275
Epoch 890, val loss: 1.412777304649353
Epoch 900, training loss: 0.6319240927696228 = 0.008528427220880985 + 0.1 * 6.233956813812256
Epoch 900, val loss: 1.419655442237854
Epoch 910, training loss: 0.6329689621925354 = 0.008282408118247986 + 0.1 * 6.246865272521973
Epoch 910, val loss: 1.426495909690857
Epoch 920, training loss: 0.6306943893432617 = 0.008048934862017632 + 0.1 * 6.226454257965088
Epoch 920, val loss: 1.4331508874893188
Epoch 930, training loss: 0.6315065622329712 = 0.007825564593076706 + 0.1 * 6.236810207366943
Epoch 930, val loss: 1.4396764039993286
Epoch 940, training loss: 0.630131721496582 = 0.007612104527652264 + 0.1 * 6.22519588470459
Epoch 940, val loss: 1.4461065530776978
Epoch 950, training loss: 0.633987545967102 = 0.007407528348267078 + 0.1 * 6.2657999992370605
Epoch 950, val loss: 1.452476978302002
Epoch 960, training loss: 0.6295318007469177 = 0.0072134556248784065 + 0.1 * 6.2231831550598145
Epoch 960, val loss: 1.4587218761444092
Epoch 970, training loss: 0.6282988786697388 = 0.0070270090363919735 + 0.1 * 6.212718486785889
Epoch 970, val loss: 1.4647250175476074
Epoch 980, training loss: 0.6306287050247192 = 0.006847709883004427 + 0.1 * 6.237809658050537
Epoch 980, val loss: 1.4707797765731812
Epoch 990, training loss: 0.62873774766922 = 0.006677027326077223 + 0.1 * 6.220607280731201
Epoch 990, val loss: 1.476755142211914
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6125
Flip ASR: 0.5644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.793045997619629 = 1.955674648284912 + 0.1 * 8.373712539672852
Epoch 0, val loss: 1.9532263278961182
Epoch 10, training loss: 2.780816078186035 = 1.943591833114624 + 0.1 * 8.372241020202637
Epoch 10, val loss: 1.9399895668029785
Epoch 20, training loss: 2.765763282775879 = 1.929003357887268 + 0.1 * 8.367598533630371
Epoch 20, val loss: 1.9226628541946411
Epoch 30, training loss: 2.745073080062866 = 1.9092179536819458 + 0.1 * 8.358551025390625
Epoch 30, val loss: 1.8987923860549927
Epoch 40, training loss: 2.714465618133545 = 1.8836588859558105 + 0.1 * 8.308067321777344
Epoch 40, val loss: 1.8706996440887451
Epoch 50, training loss: 2.66054105758667 = 1.8523083925247192 + 0.1 * 8.082326889038086
Epoch 50, val loss: 1.8402290344238281
Epoch 60, training loss: 2.597240447998047 = 1.8159185647964478 + 0.1 * 7.813218116760254
Epoch 60, val loss: 1.8080979585647583
Epoch 70, training loss: 2.5202372074127197 = 1.7790330648422241 + 0.1 * 7.412040710449219
Epoch 70, val loss: 1.7775543928146362
Epoch 80, training loss: 2.456850290298462 = 1.7416659593582153 + 0.1 * 7.1518425941467285
Epoch 80, val loss: 1.7468243837356567
Epoch 90, training loss: 2.397279977798462 = 1.7000296115875244 + 0.1 * 6.972503662109375
Epoch 90, val loss: 1.7117314338684082
Epoch 100, training loss: 2.332298755645752 = 1.645338535308838 + 0.1 * 6.869600772857666
Epoch 100, val loss: 1.6658440828323364
Epoch 110, training loss: 2.2522642612457275 = 1.5732030868530273 + 0.1 * 6.790611743927002
Epoch 110, val loss: 1.608298897743225
Epoch 120, training loss: 2.1623950004577637 = 1.4891806840896606 + 0.1 * 6.732141971588135
Epoch 120, val loss: 1.5439924001693726
Epoch 130, training loss: 2.075260639190674 = 1.4056499004364014 + 0.1 * 6.696107387542725
Epoch 130, val loss: 1.481441855430603
Epoch 140, training loss: 1.9965765476226807 = 1.3304517269134521 + 0.1 * 6.661247730255127
Epoch 140, val loss: 1.426559567451477
Epoch 150, training loss: 1.9244085550308228 = 1.260872483253479 + 0.1 * 6.6353607177734375
Epoch 150, val loss: 1.3761661052703857
Epoch 160, training loss: 1.8560538291931152 = 1.1946561336517334 + 0.1 * 6.613976001739502
Epoch 160, val loss: 1.328520655632019
Epoch 170, training loss: 1.7892961502075195 = 1.1297498941421509 + 0.1 * 6.595462322235107
Epoch 170, val loss: 1.2820208072662354
Epoch 180, training loss: 1.7235500812530518 = 1.0648694038391113 + 0.1 * 6.5868072509765625
Epoch 180, val loss: 1.2362345457077026
Epoch 190, training loss: 1.6568063497543335 = 1.0000354051589966 + 0.1 * 6.567709445953369
Epoch 190, val loss: 1.1912598609924316
Epoch 200, training loss: 1.5902118682861328 = 0.9345781803131104 + 0.1 * 6.556337356567383
Epoch 200, val loss: 1.145599126815796
Epoch 210, training loss: 1.5246045589447021 = 0.8696194887161255 + 0.1 * 6.549850940704346
Epoch 210, val loss: 1.0998159646987915
Epoch 220, training loss: 1.4617292881011963 = 0.8077231645584106 + 0.1 * 6.540061950683594
Epoch 220, val loss: 1.0558531284332275
Epoch 230, training loss: 1.40133798122406 = 0.7491658329963684 + 0.1 * 6.521721363067627
Epoch 230, val loss: 1.0141801834106445
Epoch 240, training loss: 1.345271110534668 = 0.6938850283622742 + 0.1 * 6.513860702514648
Epoch 240, val loss: 0.974818229675293
Epoch 250, training loss: 1.2924933433532715 = 0.6424780488014221 + 0.1 * 6.500152111053467
Epoch 250, val loss: 0.9390671849250793
Epoch 260, training loss: 1.2438410520553589 = 0.5945401787757874 + 0.1 * 6.493008613586426
Epoch 260, val loss: 0.9069034457206726
Epoch 270, training loss: 1.1977357864379883 = 0.5494570732116699 + 0.1 * 6.482786655426025
Epoch 270, val loss: 0.8780292868614197
Epoch 280, training loss: 1.1533417701721191 = 0.5066911578178406 + 0.1 * 6.466506481170654
Epoch 280, val loss: 0.8521137237548828
Epoch 290, training loss: 1.1122212409973145 = 0.46589186787605286 + 0.1 * 6.463293552398682
Epoch 290, val loss: 0.8289765119552612
Epoch 300, training loss: 1.0728607177734375 = 0.42738229036331177 + 0.1 * 6.454784870147705
Epoch 300, val loss: 0.8089073896408081
Epoch 310, training loss: 1.035595417022705 = 0.391048789024353 + 0.1 * 6.445466995239258
Epoch 310, val loss: 0.7916823029518127
Epoch 320, training loss: 1.0008859634399414 = 0.35695379972457886 + 0.1 * 6.439321517944336
Epoch 320, val loss: 0.7776858806610107
Epoch 330, training loss: 0.9682526588439941 = 0.3252740502357483 + 0.1 * 6.42978572845459
Epoch 330, val loss: 0.7666622996330261
Epoch 340, training loss: 0.9405704736709595 = 0.2960052490234375 + 0.1 * 6.445652008056641
Epoch 340, val loss: 0.7582582235336304
Epoch 350, training loss: 0.9108703136444092 = 0.26925504207611084 + 0.1 * 6.416152477264404
Epoch 350, val loss: 0.7526482343673706
Epoch 360, training loss: 0.8854051828384399 = 0.2447284460067749 + 0.1 * 6.40676736831665
Epoch 360, val loss: 0.7495495676994324
Epoch 370, training loss: 0.8617022037506104 = 0.22235184907913208 + 0.1 * 6.393503665924072
Epoch 370, val loss: 0.7488263249397278
Epoch 380, training loss: 0.8431599140167236 = 0.20192775130271912 + 0.1 * 6.412321090698242
Epoch 380, val loss: 0.7501271367073059
Epoch 390, training loss: 0.821212887763977 = 0.18339863419532776 + 0.1 * 6.3781418800354
Epoch 390, val loss: 0.7532914876937866
Epoch 400, training loss: 0.8044124841690063 = 0.16653819382190704 + 0.1 * 6.3787431716918945
Epoch 400, val loss: 0.7582170963287354
Epoch 410, training loss: 0.7903631925582886 = 0.15125982463359833 + 0.1 * 6.39103364944458
Epoch 410, val loss: 0.7646297812461853
Epoch 420, training loss: 0.7740303874015808 = 0.13754010200500488 + 0.1 * 6.364902496337891
Epoch 420, val loss: 0.7723199725151062
Epoch 430, training loss: 0.7613164186477661 = 0.12518171966075897 + 0.1 * 6.36134672164917
Epoch 430, val loss: 0.7810616493225098
Epoch 440, training loss: 0.7496650815010071 = 0.11412526667118073 + 0.1 * 6.355398178100586
Epoch 440, val loss: 0.7905062437057495
Epoch 450, training loss: 0.7393051385879517 = 0.10425280034542084 + 0.1 * 6.350522994995117
Epoch 450, val loss: 0.8005260825157166
Epoch 460, training loss: 0.7306219339370728 = 0.09545990079641342 + 0.1 * 6.351620197296143
Epoch 460, val loss: 0.8106616139411926
Epoch 470, training loss: 0.7217721343040466 = 0.08763387054204941 + 0.1 * 6.34138298034668
Epoch 470, val loss: 0.8209754228591919
Epoch 480, training loss: 0.7138590216636658 = 0.08064264059066772 + 0.1 * 6.3321638107299805
Epoch 480, val loss: 0.8311367034912109
Epoch 490, training loss: 0.7086626291275024 = 0.07438747584819794 + 0.1 * 6.3427510261535645
Epoch 490, val loss: 0.8411879539489746
Epoch 500, training loss: 0.7020037770271301 = 0.06880883127450943 + 0.1 * 6.331949234008789
Epoch 500, val loss: 0.8511186242103577
Epoch 510, training loss: 0.6952793598175049 = 0.06380375474691391 + 0.1 * 6.314755916595459
Epoch 510, val loss: 0.8609676957130432
Epoch 520, training loss: 0.6914629936218262 = 0.05928501486778259 + 0.1 * 6.321779251098633
Epoch 520, val loss: 0.8707963824272156
Epoch 530, training loss: 0.6865547895431519 = 0.05521222576498985 + 0.1 * 6.3134260177612305
Epoch 530, val loss: 0.8805457949638367
Epoch 540, training loss: 0.6820473670959473 = 0.05152829363942146 + 0.1 * 6.305190563201904
Epoch 540, val loss: 0.8903453946113586
Epoch 550, training loss: 0.6806638240814209 = 0.04818633198738098 + 0.1 * 6.324774742126465
Epoch 550, val loss: 0.9000139236450195
Epoch 560, training loss: 0.6744919419288635 = 0.04515798017382622 + 0.1 * 6.293339729309082
Epoch 560, val loss: 0.9096413850784302
Epoch 570, training loss: 0.6737833023071289 = 0.04239717498421669 + 0.1 * 6.313860893249512
Epoch 570, val loss: 0.9191508889198303
Epoch 580, training loss: 0.6692284345626831 = 0.03988009691238403 + 0.1 * 6.293483257293701
Epoch 580, val loss: 0.928584098815918
Epoch 590, training loss: 0.6670608520507812 = 0.03757227212190628 + 0.1 * 6.294885635375977
Epoch 590, val loss: 0.9379538297653198
Epoch 600, training loss: 0.6637689471244812 = 0.03545526787638664 + 0.1 * 6.283136367797852
Epoch 600, val loss: 0.9472023248672485
Epoch 610, training loss: 0.662129819393158 = 0.03350524976849556 + 0.1 * 6.286245346069336
Epoch 610, val loss: 0.9563975930213928
Epoch 620, training loss: 0.6607957482337952 = 0.031711556017398834 + 0.1 * 6.290841579437256
Epoch 620, val loss: 0.9652290344238281
Epoch 630, training loss: 0.6576170325279236 = 0.03006134368479252 + 0.1 * 6.275556564331055
Epoch 630, val loss: 0.9741450548171997
Epoch 640, training loss: 0.6565927267074585 = 0.02853400632739067 + 0.1 * 6.280587196350098
Epoch 640, val loss: 0.9827812910079956
Epoch 650, training loss: 0.653647780418396 = 0.02711835503578186 + 0.1 * 6.265294075012207
Epoch 650, val loss: 0.9913439750671387
Epoch 660, training loss: 0.6530892848968506 = 0.025797607377171516 + 0.1 * 6.272916793823242
Epoch 660, val loss: 0.9999112486839294
Epoch 670, training loss: 0.6513246297836304 = 0.024566102772951126 + 0.1 * 6.267585277557373
Epoch 670, val loss: 1.0082190036773682
Epoch 680, training loss: 0.6501105427742004 = 0.023418016731739044 + 0.1 * 6.26692533493042
Epoch 680, val loss: 1.0165411233901978
Epoch 690, training loss: 0.6491708755493164 = 0.022342726588249207 + 0.1 * 6.268280982971191
Epoch 690, val loss: 1.0246354341506958
Epoch 700, training loss: 0.6470724940299988 = 0.02133488655090332 + 0.1 * 6.257376194000244
Epoch 700, val loss: 1.0327602624893188
Epoch 710, training loss: 0.6466453671455383 = 0.020387761294841766 + 0.1 * 6.262576103210449
Epoch 710, val loss: 1.040739893913269
Epoch 720, training loss: 0.6452100872993469 = 0.01950116828083992 + 0.1 * 6.257089138031006
Epoch 720, val loss: 1.0485179424285889
Epoch 730, training loss: 0.6444011926651001 = 0.01867402344942093 + 0.1 * 6.257271766662598
Epoch 730, val loss: 1.0562857389450073
Epoch 740, training loss: 0.6420195698738098 = 0.017895812168717384 + 0.1 * 6.241237163543701
Epoch 740, val loss: 1.0638728141784668
Epoch 750, training loss: 0.6435728669166565 = 0.017161952331662178 + 0.1 * 6.264109134674072
Epoch 750, val loss: 1.071363091468811
Epoch 760, training loss: 0.6418063044548035 = 0.01647397316992283 + 0.1 * 6.253323554992676
Epoch 760, val loss: 1.0786457061767578
Epoch 770, training loss: 0.6399338841438293 = 0.01582762971520424 + 0.1 * 6.241062164306641
Epoch 770, val loss: 1.0859253406524658
Epoch 780, training loss: 0.6392290592193604 = 0.015217765234410763 + 0.1 * 6.240112781524658
Epoch 780, val loss: 1.0929900407791138
Epoch 790, training loss: 0.6380928158760071 = 0.014643588103353977 + 0.1 * 6.23449182510376
Epoch 790, val loss: 1.0999326705932617
Epoch 800, training loss: 0.6371915340423584 = 0.014101289212703705 + 0.1 * 6.230902671813965
Epoch 800, val loss: 1.1068638563156128
Epoch 810, training loss: 0.6372290253639221 = 0.013588721863925457 + 0.1 * 6.236402988433838
Epoch 810, val loss: 1.1135315895080566
Epoch 820, training loss: 0.6362285614013672 = 0.013104915618896484 + 0.1 * 6.231236457824707
Epoch 820, val loss: 1.1201653480529785
Epoch 830, training loss: 0.6362877488136292 = 0.012646639719605446 + 0.1 * 6.236411094665527
Epoch 830, val loss: 1.1266902685165405
Epoch 840, training loss: 0.6372203826904297 = 0.012211975641548634 + 0.1 * 6.250083923339844
Epoch 840, val loss: 1.1330864429473877
Epoch 850, training loss: 0.6338040828704834 = 0.011800974607467651 + 0.1 * 6.220030784606934
Epoch 850, val loss: 1.1393356323242188
Epoch 860, training loss: 0.6344239115715027 = 0.01141115091741085 + 0.1 * 6.230127811431885
Epoch 860, val loss: 1.145532488822937
Epoch 870, training loss: 0.6335846781730652 = 0.011040914803743362 + 0.1 * 6.225437641143799
Epoch 870, val loss: 1.151483178138733
Epoch 880, training loss: 0.6329817175865173 = 0.01068962924182415 + 0.1 * 6.222920894622803
Epoch 880, val loss: 1.1574867963790894
Epoch 890, training loss: 0.6321056485176086 = 0.010354697704315186 + 0.1 * 6.2175092697143555
Epoch 890, val loss: 1.1632921695709229
Epoch 900, training loss: 0.6319374442100525 = 0.010035735554993153 + 0.1 * 6.2190165519714355
Epoch 900, val loss: 1.1690622568130493
Epoch 910, training loss: 0.6320471167564392 = 0.009732025675475597 + 0.1 * 6.223150730133057
Epoch 910, val loss: 1.1746854782104492
Epoch 920, training loss: 0.6305360794067383 = 0.009442790411412716 + 0.1 * 6.210932731628418
Epoch 920, val loss: 1.1802361011505127
Epoch 930, training loss: 0.6295922994613647 = 0.009166570380330086 + 0.1 * 6.204257011413574
Epoch 930, val loss: 1.1857229471206665
Epoch 940, training loss: 0.629580020904541 = 0.008902657777071 + 0.1 * 6.206773281097412
Epoch 940, val loss: 1.1910332441329956
Epoch 950, training loss: 0.63236004114151 = 0.008650974370539188 + 0.1 * 6.237090587615967
Epoch 950, val loss: 1.1962409019470215
Epoch 960, training loss: 0.6286108493804932 = 0.008412480354309082 + 0.1 * 6.201983451843262
Epoch 960, val loss: 1.2012475728988647
Epoch 970, training loss: 0.6299329400062561 = 0.008184944279491901 + 0.1 * 6.217479705810547
Epoch 970, val loss: 1.2062979936599731
Epoch 980, training loss: 0.6277832984924316 = 0.00796655286103487 + 0.1 * 6.198166847229004
Epoch 980, val loss: 1.211167335510254
Epoch 990, training loss: 0.6287727355957031 = 0.00775702903047204 + 0.1 * 6.21015739440918
Epoch 990, val loss: 1.216070532798767
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6568
Flip ASR: 0.6533/225 nodes
The final ASR:0.61870, 0.02895, Accuracy:0.80494, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11526])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10462])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.83580, 0.00175
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7760491371154785 = 1.9386717081069946 + 0.1 * 8.373773574829102
Epoch 0, val loss: 1.9322736263275146
Epoch 10, training loss: 2.7658655643463135 = 1.928519606590271 + 0.1 * 8.373458862304688
Epoch 10, val loss: 1.923033595085144
Epoch 20, training loss: 2.752934455871582 = 1.9157510995864868 + 0.1 * 8.371834754943848
Epoch 20, val loss: 1.9110792875289917
Epoch 30, training loss: 2.7339062690734863 = 1.897774338722229 + 0.1 * 8.361319541931152
Epoch 30, val loss: 1.894013524055481
Epoch 40, training loss: 2.702040672302246 = 1.8716716766357422 + 0.1 * 8.303689002990723
Epoch 40, val loss: 1.8694778680801392
Epoch 50, training loss: 2.630039930343628 = 1.8383097648620605 + 0.1 * 7.917301654815674
Epoch 50, val loss: 1.8394882678985596
Epoch 60, training loss: 2.545454740524292 = 1.805942177772522 + 0.1 * 7.395124912261963
Epoch 60, val loss: 1.8126132488250732
Epoch 70, training loss: 2.474515676498413 = 1.7752137184143066 + 0.1 * 6.993020057678223
Epoch 70, val loss: 1.7883182764053345
Epoch 80, training loss: 2.425335645675659 = 1.7428529262542725 + 0.1 * 6.824827194213867
Epoch 80, val loss: 1.761935830116272
Epoch 90, training loss: 2.3759496212005615 = 1.7013864517211914 + 0.1 * 6.745632171630859
Epoch 90, val loss: 1.7261013984680176
Epoch 100, training loss: 2.3169751167297363 = 1.6468220949172974 + 0.1 * 6.701530456542969
Epoch 100, val loss: 1.6799818277359009
Epoch 110, training loss: 2.244346857070923 = 1.5771983861923218 + 0.1 * 6.671485424041748
Epoch 110, val loss: 1.6232675313949585
Epoch 120, training loss: 2.1590137481689453 = 1.493969440460205 + 0.1 * 6.6504435539245605
Epoch 120, val loss: 1.5553977489471436
Epoch 130, training loss: 2.065824508666992 = 1.402664303779602 + 0.1 * 6.631600856781006
Epoch 130, val loss: 1.4824178218841553
Epoch 140, training loss: 1.968693733215332 = 1.3072035312652588 + 0.1 * 6.614901065826416
Epoch 140, val loss: 1.4061254262924194
Epoch 150, training loss: 1.8677690029144287 = 1.2078458070755005 + 0.1 * 6.599232196807861
Epoch 150, val loss: 1.3275116682052612
Epoch 160, training loss: 1.7652244567871094 = 1.1060998439788818 + 0.1 * 6.591245651245117
Epoch 160, val loss: 1.2467440366744995
Epoch 170, training loss: 1.6651711463928223 = 1.0069239139556885 + 0.1 * 6.58247184753418
Epoch 170, val loss: 1.1683003902435303
Epoch 180, training loss: 1.5685718059539795 = 0.9113385081291199 + 0.1 * 6.572332859039307
Epoch 180, val loss: 1.0920202732086182
Epoch 190, training loss: 1.4774391651153564 = 0.8206313252449036 + 0.1 * 6.568078994750977
Epoch 190, val loss: 1.019303321838379
Epoch 200, training loss: 1.392056941986084 = 0.7363969683647156 + 0.1 * 6.556599140167236
Epoch 200, val loss: 0.9525710344314575
Epoch 210, training loss: 1.3145922422409058 = 0.659251868724823 + 0.1 * 6.553403854370117
Epoch 210, val loss: 0.892224907875061
Epoch 220, training loss: 1.2457993030548096 = 0.5916862487792969 + 0.1 * 6.541130542755127
Epoch 220, val loss: 0.8418080806732178
Epoch 230, training loss: 1.1866744756698608 = 0.5334746241569519 + 0.1 * 6.531998157501221
Epoch 230, val loss: 0.8014392256736755
Epoch 240, training loss: 1.1355079412460327 = 0.48368605971336365 + 0.1 * 6.518218994140625
Epoch 240, val loss: 0.7706872820854187
Epoch 250, training loss: 1.0912952423095703 = 0.4402732849121094 + 0.1 * 6.510219573974609
Epoch 250, val loss: 0.7470661401748657
Epoch 260, training loss: 1.0525469779968262 = 0.40135157108306885 + 0.1 * 6.511953830718994
Epoch 260, val loss: 0.7286665439605713
Epoch 270, training loss: 1.0156058073043823 = 0.36618244647979736 + 0.1 * 6.49423360824585
Epoch 270, val loss: 0.7138262391090393
Epoch 280, training loss: 0.9816915988922119 = 0.3338927626609802 + 0.1 * 6.477988243103027
Epoch 280, val loss: 0.7018798589706421
Epoch 290, training loss: 0.9513802528381348 = 0.3041289746761322 + 0.1 * 6.472512722015381
Epoch 290, val loss: 0.692523181438446
Epoch 300, training loss: 0.9226045608520508 = 0.27705734968185425 + 0.1 * 6.455471992492676
Epoch 300, val loss: 0.6856994032859802
Epoch 310, training loss: 0.8970756530761719 = 0.252448171377182 + 0.1 * 6.446274757385254
Epoch 310, val loss: 0.6814751625061035
Epoch 320, training loss: 0.8749532699584961 = 0.23010526597499847 + 0.1 * 6.448479652404785
Epoch 320, val loss: 0.6795138716697693
Epoch 330, training loss: 0.8525530695915222 = 0.20971782505512238 + 0.1 * 6.428352355957031
Epoch 330, val loss: 0.6797475814819336
Epoch 340, training loss: 0.8330985307693481 = 0.19074971973896027 + 0.1 * 6.423488140106201
Epoch 340, val loss: 0.6815600991249084
Epoch 350, training loss: 0.8141078948974609 = 0.172964945435524 + 0.1 * 6.411429405212402
Epoch 350, val loss: 0.684539258480072
Epoch 360, training loss: 0.7964254021644592 = 0.1561671644449234 + 0.1 * 6.40258264541626
Epoch 360, val loss: 0.6884403824806213
Epoch 370, training loss: 0.7797662019729614 = 0.14036960899829865 + 0.1 * 6.393966197967529
Epoch 370, val loss: 0.6931384205818176
Epoch 380, training loss: 0.7651971578598022 = 0.12578175961971283 + 0.1 * 6.394153594970703
Epoch 380, val loss: 0.6984462141990662
Epoch 390, training loss: 0.7505698204040527 = 0.11263295263051987 + 0.1 * 6.379368782043457
Epoch 390, val loss: 0.7043874263763428
Epoch 400, training loss: 0.7379642128944397 = 0.10094785690307617 + 0.1 * 6.370163440704346
Epoch 400, val loss: 0.7108574509620667
Epoch 410, training loss: 0.7273896932601929 = 0.09074941277503967 + 0.1 * 6.3664021492004395
Epoch 410, val loss: 0.7180050611495972
Epoch 420, training loss: 0.7176960706710815 = 0.08193489909172058 + 0.1 * 6.357612133026123
Epoch 420, val loss: 0.725683867931366
Epoch 430, training loss: 0.7096216082572937 = 0.07432267814874649 + 0.1 * 6.352989196777344
Epoch 430, val loss: 0.7338463664054871
Epoch 440, training loss: 0.7030717134475708 = 0.06776106357574463 + 0.1 * 6.353106498718262
Epoch 440, val loss: 0.7421970367431641
Epoch 450, training loss: 0.6956321001052856 = 0.06208471581339836 + 0.1 * 6.335473537445068
Epoch 450, val loss: 0.7507966756820679
Epoch 460, training loss: 0.6910265684127808 = 0.05712607875466347 + 0.1 * 6.3390045166015625
Epoch 460, val loss: 0.7595358490943909
Epoch 470, training loss: 0.6863011121749878 = 0.05277058109641075 + 0.1 * 6.335305690765381
Epoch 470, val loss: 0.7684220671653748
Epoch 480, training loss: 0.6812986731529236 = 0.04892990365624428 + 0.1 * 6.323687553405762
Epoch 480, val loss: 0.777174174785614
Epoch 490, training loss: 0.6769487857818604 = 0.04551207646727562 + 0.1 * 6.314366817474365
Epoch 490, val loss: 0.7859505414962769
Epoch 500, training loss: 0.674045741558075 = 0.04244444519281387 + 0.1 * 6.316012382507324
Epoch 500, val loss: 0.7947630882263184
Epoch 510, training loss: 0.6710364818572998 = 0.039699479937553406 + 0.1 * 6.3133697509765625
Epoch 510, val loss: 0.8034125566482544
Epoch 520, training loss: 0.6673984527587891 = 0.03722736984491348 + 0.1 * 6.301710605621338
Epoch 520, val loss: 0.8119741678237915
Epoch 530, training loss: 0.6638929843902588 = 0.034977663308382034 + 0.1 * 6.289153099060059
Epoch 530, val loss: 0.8204267024993896
Epoch 540, training loss: 0.6634318232536316 = 0.03292231261730194 + 0.1 * 6.3050947189331055
Epoch 540, val loss: 0.8288634419441223
Epoch 550, training loss: 0.6602059602737427 = 0.031047513708472252 + 0.1 * 6.291584014892578
Epoch 550, val loss: 0.8371319770812988
Epoch 560, training loss: 0.6573274731636047 = 0.029331693425774574 + 0.1 * 6.2799577713012695
Epoch 560, val loss: 0.845241367816925
Epoch 570, training loss: 0.6567559242248535 = 0.02775350958108902 + 0.1 * 6.2900238037109375
Epoch 570, val loss: 0.8531551957130432
Epoch 580, training loss: 0.6534759998321533 = 0.02629898488521576 + 0.1 * 6.271770000457764
Epoch 580, val loss: 0.8610390424728394
Epoch 590, training loss: 0.6516432762145996 = 0.024958698078989983 + 0.1 * 6.266846179962158
Epoch 590, val loss: 0.8687217831611633
Epoch 600, training loss: 0.6510647535324097 = 0.023716509342193604 + 0.1 * 6.273482322692871
Epoch 600, val loss: 0.8762343525886536
Epoch 610, training loss: 0.6485104560852051 = 0.022571053355932236 + 0.1 * 6.25939416885376
Epoch 610, val loss: 0.8836266398429871
Epoch 620, training loss: 0.6470797657966614 = 0.02150542661547661 + 0.1 * 6.255743503570557
Epoch 620, val loss: 0.8908652663230896
Epoch 630, training loss: 0.6465280055999756 = 0.02051275596022606 + 0.1 * 6.260152339935303
Epoch 630, val loss: 0.8980088233947754
Epoch 640, training loss: 0.6442947387695312 = 0.019588714465498924 + 0.1 * 6.247060298919678
Epoch 640, val loss: 0.9050304293632507
Epoch 650, training loss: 0.6451480388641357 = 0.01872509904205799 + 0.1 * 6.2642292976379395
Epoch 650, val loss: 0.911919355392456
Epoch 660, training loss: 0.6428196430206299 = 0.017920665442943573 + 0.1 * 6.248989582061768
Epoch 660, val loss: 0.9186291694641113
Epoch 670, training loss: 0.6414461731910706 = 0.017168644815683365 + 0.1 * 6.242774963378906
Epoch 670, val loss: 0.9252593517303467
Epoch 680, training loss: 0.6400085091590881 = 0.016464971005916595 + 0.1 * 6.235435485839844
Epoch 680, val loss: 0.9317296147346497
Epoch 690, training loss: 0.6391558647155762 = 0.015804240480065346 + 0.1 * 6.233516216278076
Epoch 690, val loss: 0.9380497336387634
Epoch 700, training loss: 0.6384258270263672 = 0.01518335286527872 + 0.1 * 6.232424736022949
Epoch 700, val loss: 0.9442967176437378
Epoch 710, training loss: 0.6368924975395203 = 0.014600788243114948 + 0.1 * 6.222916603088379
Epoch 710, val loss: 0.9504640698432922
Epoch 720, training loss: 0.6366645097732544 = 0.0140521926805377 + 0.1 * 6.226122856140137
Epoch 720, val loss: 0.9564907550811768
Epoch 730, training loss: 0.6351574659347534 = 0.013533910736441612 + 0.1 * 6.216235637664795
Epoch 730, val loss: 0.9623776078224182
Epoch 740, training loss: 0.6357646584510803 = 0.013045777566730976 + 0.1 * 6.227188587188721
Epoch 740, val loss: 0.9681981801986694
Epoch 750, training loss: 0.633983314037323 = 0.012584594078361988 + 0.1 * 6.213987350463867
Epoch 750, val loss: 0.9739046692848206
Epoch 760, training loss: 0.6334332227706909 = 0.012149096466600895 + 0.1 * 6.212841033935547
Epoch 760, val loss: 0.9795235991477966
Epoch 770, training loss: 0.6323719024658203 = 0.011736201122403145 + 0.1 * 6.206357002258301
Epoch 770, val loss: 0.9850263595581055
Epoch 780, training loss: 0.6328237056732178 = 0.011345773003995419 + 0.1 * 6.214779376983643
Epoch 780, val loss: 0.9904870986938477
Epoch 790, training loss: 0.6317423582077026 = 0.010975104756653309 + 0.1 * 6.207672119140625
Epoch 790, val loss: 0.9958248734474182
Epoch 800, training loss: 0.6322118043899536 = 0.0106238704174757 + 0.1 * 6.215878963470459
Epoch 800, val loss: 1.0010695457458496
Epoch 810, training loss: 0.6311311721801758 = 0.010289701633155346 + 0.1 * 6.208414554595947
Epoch 810, val loss: 1.006198763847351
Epoch 820, training loss: 0.6299412846565247 = 0.009974615648388863 + 0.1 * 6.199666500091553
Epoch 820, val loss: 1.0112708806991577
Epoch 830, training loss: 0.6300236582756042 = 0.009673817083239555 + 0.1 * 6.203497886657715
Epoch 830, val loss: 1.0162277221679688
Epoch 840, training loss: 0.6291417479515076 = 0.009386024437844753 + 0.1 * 6.19755744934082
Epoch 840, val loss: 1.021089792251587
Epoch 850, training loss: 0.6285462379455566 = 0.009112942963838577 + 0.1 * 6.194332599639893
Epoch 850, val loss: 1.0259066820144653
Epoch 860, training loss: 0.6278508305549622 = 0.00885260570794344 + 0.1 * 6.189981937408447
Epoch 860, val loss: 1.0307137966156006
Epoch 870, training loss: 0.6273114085197449 = 0.008603145368397236 + 0.1 * 6.187082290649414
Epoch 870, val loss: 1.0353569984436035
Epoch 880, training loss: 0.6269978284835815 = 0.008366037160158157 + 0.1 * 6.186317443847656
Epoch 880, val loss: 1.0400587320327759
Epoch 890, training loss: 0.6274246573448181 = 0.00813918374478817 + 0.1 * 6.192854404449463
Epoch 890, val loss: 1.044614553451538
Epoch 900, training loss: 0.6261438727378845 = 0.007921912707388401 + 0.1 * 6.182219505310059
Epoch 900, val loss: 1.0490350723266602
Epoch 910, training loss: 0.6278615593910217 = 0.007715747691690922 + 0.1 * 6.201457977294922
Epoch 910, val loss: 1.0533630847930908
Epoch 920, training loss: 0.625540018081665 = 0.007516607642173767 + 0.1 * 6.180233955383301
Epoch 920, val loss: 1.0576814413070679
Epoch 930, training loss: 0.6248210072517395 = 0.0073281461372971535 + 0.1 * 6.174928188323975
Epoch 930, val loss: 1.0619263648986816
Epoch 940, training loss: 0.6243761777877808 = 0.007147001102566719 + 0.1 * 6.1722917556762695
Epoch 940, val loss: 1.066088080406189
Epoch 950, training loss: 0.6255941390991211 = 0.006971797440201044 + 0.1 * 6.18622350692749
Epoch 950, val loss: 1.0701428651809692
Epoch 960, training loss: 0.6246052980422974 = 0.006803242024034262 + 0.1 * 6.17802095413208
Epoch 960, val loss: 1.0741959810256958
Epoch 970, training loss: 0.6235243082046509 = 0.006642184220254421 + 0.1 * 6.168821334838867
Epoch 970, val loss: 1.0782502889633179
Epoch 980, training loss: 0.6236162185668945 = 0.006487008184194565 + 0.1 * 6.171291828155518
Epoch 980, val loss: 1.0822052955627441
Epoch 990, training loss: 0.6242594718933105 = 0.006337764672935009 + 0.1 * 6.1792168617248535
Epoch 990, val loss: 1.0860694646835327
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6162
Flip ASR: 0.5378/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.785000801086426 = 1.9476203918457031 + 0.1 * 8.37380313873291
Epoch 0, val loss: 1.9521312713623047
Epoch 10, training loss: 2.775263547897339 = 1.9378976821899414 + 0.1 * 8.373658180236816
Epoch 10, val loss: 1.9416110515594482
Epoch 20, training loss: 2.763427972793579 = 1.9261481761932373 + 0.1 * 8.372797012329102
Epoch 20, val loss: 1.9286723136901855
Epoch 30, training loss: 2.746497631072998 = 1.9097627401351929 + 0.1 * 8.367347717285156
Epoch 30, val loss: 1.9106477499008179
Epoch 40, training loss: 2.718581199645996 = 1.8855396509170532 + 0.1 * 8.330415725708008
Epoch 40, val loss: 1.884318232536316
Epoch 50, training loss: 2.6551263332366943 = 1.851815104484558 + 0.1 * 8.033111572265625
Epoch 50, val loss: 1.8490203619003296
Epoch 60, training loss: 2.5612688064575195 = 1.8130244016647339 + 0.1 * 7.4824442863464355
Epoch 60, val loss: 1.8102580308914185
Epoch 70, training loss: 2.486632823944092 = 1.7741762399673462 + 0.1 * 7.124565601348877
Epoch 70, val loss: 1.7740087509155273
Epoch 80, training loss: 2.425990581512451 = 1.7342652082443237 + 0.1 * 6.917252540588379
Epoch 80, val loss: 1.7381396293640137
Epoch 90, training loss: 2.3688809871673584 = 1.6866592168807983 + 0.1 * 6.822216987609863
Epoch 90, val loss: 1.6943703889846802
Epoch 100, training loss: 2.298902988433838 = 1.6224110126495361 + 0.1 * 6.764919281005859
Epoch 100, val loss: 1.6366124153137207
Epoch 110, training loss: 2.2118210792541504 = 1.5392590761184692 + 0.1 * 6.725618839263916
Epoch 110, val loss: 1.5655497312545776
Epoch 120, training loss: 2.1087851524353027 = 1.4392834901809692 + 0.1 * 6.6950178146362305
Epoch 120, val loss: 1.4816508293151855
Epoch 130, training loss: 1.9988501071929932 = 1.3310537338256836 + 0.1 * 6.677963733673096
Epoch 130, val loss: 1.3919214010238647
Epoch 140, training loss: 1.889047622680664 = 1.2227365970611572 + 0.1 * 6.663110733032227
Epoch 140, val loss: 1.3055404424667358
Epoch 150, training loss: 1.7831617593765259 = 1.1186684370040894 + 0.1 * 6.644933223724365
Epoch 150, val loss: 1.2253525257110596
Epoch 160, training loss: 1.6848056316375732 = 1.0214427709579468 + 0.1 * 6.633628845214844
Epoch 160, val loss: 1.1527330875396729
Epoch 170, training loss: 1.5940310955047607 = 0.9327327013015747 + 0.1 * 6.612984657287598
Epoch 170, val loss: 1.0874568223953247
Epoch 180, training loss: 1.5106303691864014 = 0.8506619930267334 + 0.1 * 6.599682807922363
Epoch 180, val loss: 1.0274953842163086
Epoch 190, training loss: 1.4343535900115967 = 0.7752411961555481 + 0.1 * 6.591123104095459
Epoch 190, val loss: 0.9728108644485474
Epoch 200, training loss: 1.364420771598816 = 0.7069321274757385 + 0.1 * 6.574886322021484
Epoch 200, val loss: 0.9242092370986938
Epoch 210, training loss: 1.300889492034912 = 0.6447625160217285 + 0.1 * 6.561270236968994
Epoch 210, val loss: 0.880738377571106
Epoch 220, training loss: 1.244465708732605 = 0.5882648825645447 + 0.1 * 6.562008380889893
Epoch 220, val loss: 0.8427538871765137
Epoch 230, training loss: 1.191413402557373 = 0.5374600291252136 + 0.1 * 6.539533615112305
Epoch 230, val loss: 0.8102344274520874
Epoch 240, training loss: 1.1429624557495117 = 0.49020928144454956 + 0.1 * 6.527531623840332
Epoch 240, val loss: 0.781724750995636
Epoch 250, training loss: 1.0977060794830322 = 0.4458528757095337 + 0.1 * 6.518532752990723
Epoch 250, val loss: 0.7570714354515076
Epoch 260, training loss: 1.055141806602478 = 0.40485790371894836 + 0.1 * 6.502839088439941
Epoch 260, val loss: 0.7363524436950684
Epoch 270, training loss: 1.0155481100082397 = 0.3671014904975891 + 0.1 * 6.484466075897217
Epoch 270, val loss: 0.7192743420600891
Epoch 280, training loss: 0.9812250137329102 = 0.33265647292137146 + 0.1 * 6.485685348510742
Epoch 280, val loss: 0.705743670463562
Epoch 290, training loss: 0.9473251104354858 = 0.3018035292625427 + 0.1 * 6.455215930938721
Epoch 290, val loss: 0.6957101821899414
Epoch 300, training loss: 0.9211777448654175 = 0.27414670586586 + 0.1 * 6.470310211181641
Epoch 300, val loss: 0.6889923214912415
Epoch 310, training loss: 0.8936075568199158 = 0.24967174232006073 + 0.1 * 6.439357757568359
Epoch 310, val loss: 0.685415506362915
Epoch 320, training loss: 0.8701232671737671 = 0.2280355542898178 + 0.1 * 6.420876979827881
Epoch 320, val loss: 0.6844333410263062
Epoch 330, training loss: 0.8497703075408936 = 0.2088867574930191 + 0.1 * 6.408835411071777
Epoch 330, val loss: 0.6859521865844727
Epoch 340, training loss: 0.8313260078430176 = 0.1917760968208313 + 0.1 * 6.395499229431152
Epoch 340, val loss: 0.6894097924232483
Epoch 350, training loss: 0.8153420686721802 = 0.17639008164405823 + 0.1 * 6.389519691467285
Epoch 350, val loss: 0.6945332884788513
Epoch 360, training loss: 0.8005149960517883 = 0.16265714168548584 + 0.1 * 6.378578186035156
Epoch 360, val loss: 0.7010020613670349
Epoch 370, training loss: 0.7874171733856201 = 0.1502678096294403 + 0.1 * 6.371493339538574
Epoch 370, val loss: 0.7084688544273376
Epoch 380, training loss: 0.7751845717430115 = 0.1390671581029892 + 0.1 * 6.3611741065979
Epoch 380, val loss: 0.7171022891998291
Epoch 390, training loss: 0.7641054391860962 = 0.1288585662841797 + 0.1 * 6.352468490600586
Epoch 390, val loss: 0.7265878319740295
Epoch 400, training loss: 0.7539547085762024 = 0.11950715631246567 + 0.1 * 6.344475746154785
Epoch 400, val loss: 0.7367863655090332
Epoch 410, training loss: 0.7451599836349487 = 0.11086723208427429 + 0.1 * 6.342927932739258
Epoch 410, val loss: 0.7476330399513245
Epoch 420, training loss: 0.7359299063682556 = 0.10277102887630463 + 0.1 * 6.331588268280029
Epoch 420, val loss: 0.7590218186378479
Epoch 430, training loss: 0.7275161743164062 = 0.09501298516988754 + 0.1 * 6.3250322341918945
Epoch 430, val loss: 0.770915150642395
Epoch 440, training loss: 0.7206456661224365 = 0.08764482289552689 + 0.1 * 6.330008506774902
Epoch 440, val loss: 0.783132016658783
Epoch 450, training loss: 0.711784839630127 = 0.08050613105297089 + 0.1 * 6.31278657913208
Epoch 450, val loss: 0.7957298159599304
Epoch 460, training loss: 0.7046923041343689 = 0.07313329726457596 + 0.1 * 6.315589904785156
Epoch 460, val loss: 0.8084182143211365
Epoch 470, training loss: 0.695614755153656 = 0.06594489514827728 + 0.1 * 6.296698093414307
Epoch 470, val loss: 0.8201976418495178
Epoch 480, training loss: 0.6893560886383057 = 0.060082998126745224 + 0.1 * 6.292730808258057
Epoch 480, val loss: 0.8313932418823242
Epoch 490, training loss: 0.6851309537887573 = 0.05524632707238197 + 0.1 * 6.2988457679748535
Epoch 490, val loss: 0.844914436340332
Epoch 500, training loss: 0.6797513961791992 = 0.051031023263931274 + 0.1 * 6.287203311920166
Epoch 500, val loss: 0.8589524030685425
Epoch 510, training loss: 0.6806607246398926 = 0.04730074480175972 + 0.1 * 6.33359956741333
Epoch 510, val loss: 0.8728651404380798
Epoch 520, training loss: 0.6722553372383118 = 0.044014401733875275 + 0.1 * 6.282409191131592
Epoch 520, val loss: 0.8866739273071289
Epoch 530, training loss: 0.667911946773529 = 0.04107405245304108 + 0.1 * 6.268379211425781
Epoch 530, val loss: 0.9002317786216736
Epoch 540, training loss: 0.664284884929657 = 0.03842317312955856 + 0.1 * 6.258616924285889
Epoch 540, val loss: 0.9137557148933411
Epoch 550, training loss: 0.6640385985374451 = 0.03601603955030441 + 0.1 * 6.2802252769470215
Epoch 550, val loss: 0.927176833152771
Epoch 560, training loss: 0.6588999032974243 = 0.033829886466264725 + 0.1 * 6.250699996948242
Epoch 560, val loss: 0.9403496384620667
Epoch 570, training loss: 0.6581584811210632 = 0.0318378284573555 + 0.1 * 6.263206481933594
Epoch 570, val loss: 0.9534401297569275
Epoch 580, training loss: 0.6550073623657227 = 0.03001260571181774 + 0.1 * 6.249947547912598
Epoch 580, val loss: 0.9662176966667175
Epoch 590, training loss: 0.653078556060791 = 0.028338780626654625 + 0.1 * 6.247397422790527
Epoch 590, val loss: 0.9787933230400085
Epoch 600, training loss: 0.6514005661010742 = 0.026794737204909325 + 0.1 * 6.246058464050293
Epoch 600, val loss: 0.9912039041519165
Epoch 610, training loss: 0.6492783427238464 = 0.02537042461335659 + 0.1 * 6.239078998565674
Epoch 610, val loss: 1.00327467918396
Epoch 620, training loss: 0.6470309495925903 = 0.024057814851403236 + 0.1 * 6.22973108291626
Epoch 620, val loss: 1.0152380466461182
Epoch 630, training loss: 0.6466394662857056 = 0.022843873128294945 + 0.1 * 6.237955570220947
Epoch 630, val loss: 1.0269906520843506
Epoch 640, training loss: 0.6449902653694153 = 0.02171717956662178 + 0.1 * 6.232730388641357
Epoch 640, val loss: 1.0385102033615112
Epoch 650, training loss: 0.6435394287109375 = 0.020673617720603943 + 0.1 * 6.2286577224731445
Epoch 650, val loss: 1.0496853590011597
Epoch 660, training loss: 0.6419485211372375 = 0.01970731094479561 + 0.1 * 6.222411632537842
Epoch 660, val loss: 1.060839295387268
Epoch 670, training loss: 0.6409968137741089 = 0.018806755542755127 + 0.1 * 6.221900463104248
Epoch 670, val loss: 1.0717074871063232
Epoch 680, training loss: 0.6394538879394531 = 0.01796785369515419 + 0.1 * 6.214859962463379
Epoch 680, val loss: 1.0824921131134033
Epoch 690, training loss: 0.6385574340820312 = 0.017183583229780197 + 0.1 * 6.213738441467285
Epoch 690, val loss: 1.093001127243042
Epoch 700, training loss: 0.6401787400245667 = 0.016450362280011177 + 0.1 * 6.237283706665039
Epoch 700, val loss: 1.1032655239105225
Epoch 710, training loss: 0.6369973421096802 = 0.01576646789908409 + 0.1 * 6.212308883666992
Epoch 710, val loss: 1.113328456878662
Epoch 720, training loss: 0.6357162594795227 = 0.015126394107937813 + 0.1 * 6.205898284912109
Epoch 720, val loss: 1.1233277320861816
Epoch 730, training loss: 0.6344886422157288 = 0.014525055885314941 + 0.1 * 6.199635982513428
Epoch 730, val loss: 1.1330450773239136
Epoch 740, training loss: 0.6346747875213623 = 0.013960794545710087 + 0.1 * 6.20713996887207
Epoch 740, val loss: 1.1425957679748535
Epoch 750, training loss: 0.633105993270874 = 0.01343105360865593 + 0.1 * 6.196749210357666
Epoch 750, val loss: 1.1520500183105469
Epoch 760, training loss: 0.6330162882804871 = 0.012931794859468937 + 0.1 * 6.200844764709473
Epoch 760, val loss: 1.1612807512283325
Epoch 770, training loss: 0.6320439577102661 = 0.01246052235364914 + 0.1 * 6.195834159851074
Epoch 770, val loss: 1.1702518463134766
Epoch 780, training loss: 0.6325400471687317 = 0.012016226537525654 + 0.1 * 6.205237865447998
Epoch 780, val loss: 1.179089903831482
Epoch 790, training loss: 0.6307123303413391 = 0.01159753929823637 + 0.1 * 6.191148281097412
Epoch 790, val loss: 1.1878446340560913
Epoch 800, training loss: 0.6300938725471497 = 0.011201278306543827 + 0.1 * 6.1889262199401855
Epoch 800, val loss: 1.1964575052261353
Epoch 810, training loss: 0.6301618814468384 = 0.01082560047507286 + 0.1 * 6.193363189697266
Epoch 810, val loss: 1.204827070236206
Epoch 820, training loss: 0.6288045644760132 = 0.010469675064086914 + 0.1 * 6.183348655700684
Epoch 820, val loss: 1.2130898237228394
Epoch 830, training loss: 0.6290618777275085 = 0.01013204175978899 + 0.1 * 6.189298629760742
Epoch 830, val loss: 1.2212388515472412
Epoch 840, training loss: 0.6280030012130737 = 0.009812043979763985 + 0.1 * 6.181909084320068
Epoch 840, val loss: 1.2291467189788818
Epoch 850, training loss: 0.6293992400169373 = 0.00950844306498766 + 0.1 * 6.198907852172852
Epoch 850, val loss: 1.2369576692581177
Epoch 860, training loss: 0.6270691752433777 = 0.009220120497047901 + 0.1 * 6.17849063873291
Epoch 860, val loss: 1.2446660995483398
Epoch 870, training loss: 0.626022458076477 = 0.008946146816015244 + 0.1 * 6.1707634925842285
Epoch 870, val loss: 1.2523444890975952
Epoch 880, training loss: 0.6260403990745544 = 0.008684280328452587 + 0.1 * 6.173561096191406
Epoch 880, val loss: 1.2596648931503296
Epoch 890, training loss: 0.625394880771637 = 0.008435378782451153 + 0.1 * 6.169594764709473
Epoch 890, val loss: 1.2670211791992188
Epoch 900, training loss: 0.6255196928977966 = 0.008197644725441933 + 0.1 * 6.173220157623291
Epoch 900, val loss: 1.2742717266082764
Epoch 910, training loss: 0.6240817308425903 = 0.007970775477588177 + 0.1 * 6.161109924316406
Epoch 910, val loss: 1.2813881635665894
Epoch 920, training loss: 0.626274824142456 = 0.0077537852339446545 + 0.1 * 6.185210227966309
Epoch 920, val loss: 1.288379430770874
Epoch 930, training loss: 0.6240725517272949 = 0.0075464388355612755 + 0.1 * 6.165261268615723
Epoch 930, val loss: 1.2951252460479736
Epoch 940, training loss: 0.6233508586883545 = 0.007348570507019758 + 0.1 * 6.160022735595703
Epoch 940, val loss: 1.3020305633544922
Epoch 950, training loss: 0.6234890222549438 = 0.0071587529964745045 + 0.1 * 6.163302421569824
Epoch 950, val loss: 1.3086823225021362
Epoch 960, training loss: 0.6233368515968323 = 0.006977011449635029 + 0.1 * 6.16359806060791
Epoch 960, val loss: 1.3151849508285522
Epoch 970, training loss: 0.6225399971008301 = 0.006802971474826336 + 0.1 * 6.157370090484619
Epoch 970, val loss: 1.3217451572418213
Epoch 980, training loss: 0.6222743391990662 = 0.006636155303567648 + 0.1 * 6.156381607055664
Epoch 980, val loss: 1.3282098770141602
Epoch 990, training loss: 0.6220746040344238 = 0.0064757633954286575 + 0.1 * 6.155988693237305
Epoch 990, val loss: 1.334517002105713
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6421
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.777155876159668 = 1.9397807121276855 + 0.1 * 8.373750686645508
Epoch 0, val loss: 1.9454365968704224
Epoch 10, training loss: 2.766667366027832 = 1.9293437004089355 + 0.1 * 8.373237609863281
Epoch 10, val loss: 1.935171365737915
Epoch 20, training loss: 2.753300428390503 = 1.916224479675293 + 0.1 * 8.370759963989258
Epoch 20, val loss: 1.9216471910476685
Epoch 30, training loss: 2.73335862159729 = 1.897455096244812 + 0.1 * 8.35903549194336
Epoch 30, val loss: 1.901846170425415
Epoch 40, training loss: 2.698005199432373 = 1.8699241876602173 + 0.1 * 8.280811309814453
Epoch 40, val loss: 1.8729736804962158
Epoch 50, training loss: 2.614577054977417 = 1.8354530334472656 + 0.1 * 7.7912397384643555
Epoch 50, val loss: 1.8380939960479736
Epoch 60, training loss: 2.533726692199707 = 1.800396203994751 + 0.1 * 7.333304405212402
Epoch 60, val loss: 1.8026471138000488
Epoch 70, training loss: 2.4679081439971924 = 1.7631409168243408 + 0.1 * 7.047672271728516
Epoch 70, val loss: 1.7665024995803833
Epoch 80, training loss: 2.411022424697876 = 1.72340726852417 + 0.1 * 6.8761515617370605
Epoch 80, val loss: 1.7297264337539673
Epoch 90, training loss: 2.3519668579101562 = 1.6747949123382568 + 0.1 * 6.7717204093933105
Epoch 90, val loss: 1.6857255697250366
Epoch 100, training loss: 2.283478260040283 = 1.610405445098877 + 0.1 * 6.730727195739746
Epoch 100, val loss: 1.6297767162322998
Epoch 110, training loss: 2.200392007827759 = 1.5290944576263428 + 0.1 * 6.712975025177002
Epoch 110, val loss: 1.5627052783966064
Epoch 120, training loss: 2.1057345867156982 = 1.435565710067749 + 0.1 * 6.70168924331665
Epoch 120, val loss: 1.4876765012741089
Epoch 130, training loss: 2.0064523220062256 = 1.337604284286499 + 0.1 * 6.688480854034424
Epoch 130, val loss: 1.4124349355697632
Epoch 140, training loss: 1.9076402187347412 = 1.240497350692749 + 0.1 * 6.671428680419922
Epoch 140, val loss: 1.3396973609924316
Epoch 150, training loss: 1.8106122016906738 = 1.1456001996994019 + 0.1 * 6.650120258331299
Epoch 150, val loss: 1.2696434259414673
Epoch 160, training loss: 1.716360092163086 = 1.0529755353927612 + 0.1 * 6.633845329284668
Epoch 160, val loss: 1.200581669807434
Epoch 170, training loss: 1.626448392868042 = 0.9650683403015137 + 0.1 * 6.613800048828125
Epoch 170, val loss: 1.1340833902359009
Epoch 180, training loss: 1.5409002304077148 = 0.8815791010856628 + 0.1 * 6.593210697174072
Epoch 180, val loss: 1.0703716278076172
Epoch 190, training loss: 1.4608500003814697 = 0.8033344149589539 + 0.1 * 6.575155258178711
Epoch 190, val loss: 1.0101006031036377
Epoch 200, training loss: 1.3883183002471924 = 0.7327032685279846 + 0.1 * 6.556150913238525
Epoch 200, val loss: 0.9557697176933289
Epoch 210, training loss: 1.3232237100601196 = 0.6693694591522217 + 0.1 * 6.5385422706604
Epoch 210, val loss: 0.90746009349823
Epoch 220, training loss: 1.2646071910858154 = 0.6119216084480286 + 0.1 * 6.526855945587158
Epoch 220, val loss: 0.8652828335762024
Epoch 230, training loss: 1.2096147537231445 = 0.5590229034423828 + 0.1 * 6.505918979644775
Epoch 230, val loss: 0.8285601139068604
Epoch 240, training loss: 1.158379077911377 = 0.5085943341255188 + 0.1 * 6.497847557067871
Epoch 240, val loss: 0.7957574129104614
Epoch 250, training loss: 1.10763418674469 = 0.46007999777793884 + 0.1 * 6.475542068481445
Epoch 250, val loss: 0.7665225267410278
Epoch 260, training loss: 1.0595569610595703 = 0.4130408465862274 + 0.1 * 6.465161323547363
Epoch 260, val loss: 0.7407500147819519
Epoch 270, training loss: 1.0132036209106445 = 0.36822736263275146 + 0.1 * 6.449762344360352
Epoch 270, val loss: 0.7187573909759521
Epoch 280, training loss: 0.9704325199127197 = 0.3263287842273712 + 0.1 * 6.441036701202393
Epoch 280, val loss: 0.7009402513504028
Epoch 290, training loss: 0.9340403079986572 = 0.28791698813438416 + 0.1 * 6.461233615875244
Epoch 290, val loss: 0.6874446868896484
Epoch 300, training loss: 0.897334098815918 = 0.25410398840904236 + 0.1 * 6.4323015213012695
Epoch 300, val loss: 0.6782342791557312
Epoch 310, training loss: 0.8655501008033752 = 0.22452779114246368 + 0.1 * 6.410223007202148
Epoch 310, val loss: 0.6729322075843811
Epoch 320, training loss: 0.8396619558334351 = 0.19904127717018127 + 0.1 * 6.4062066078186035
Epoch 320, val loss: 0.6710644364356995
Epoch 330, training loss: 0.81681227684021 = 0.17737844586372375 + 0.1 * 6.394338607788086
Epoch 330, val loss: 0.6719934344291687
Epoch 340, training loss: 0.7999261617660522 = 0.1588822901248932 + 0.1 * 6.410438537597656
Epoch 340, val loss: 0.675457775592804
Epoch 350, training loss: 0.7800638675689697 = 0.14315544068813324 + 0.1 * 6.369083881378174
Epoch 350, val loss: 0.6806172728538513
Epoch 360, training loss: 0.7656266093254089 = 0.12962090969085693 + 0.1 * 6.3600568771362305
Epoch 360, val loss: 0.6872058510780334
Epoch 370, training loss: 0.7532570958137512 = 0.1179010272026062 + 0.1 * 6.353560447692871
Epoch 370, val loss: 0.6947633028030396
Epoch 380, training loss: 0.7423171997070312 = 0.10771554708480835 + 0.1 * 6.3460164070129395
Epoch 380, val loss: 0.7029051780700684
Epoch 390, training loss: 0.7351317405700684 = 0.09875670820474625 + 0.1 * 6.363749980926514
Epoch 390, val loss: 0.7116084694862366
Epoch 400, training loss: 0.7232770919799805 = 0.09088470786809921 + 0.1 * 6.3239240646362305
Epoch 400, val loss: 0.7203390002250671
Epoch 410, training loss: 0.7163734436035156 = 0.08385203778743744 + 0.1 * 6.32521390914917
Epoch 410, val loss: 0.7293527126312256
Epoch 420, training loss: 0.7094990611076355 = 0.07754635065793991 + 0.1 * 6.319526672363281
Epoch 420, val loss: 0.7384344339370728
Epoch 430, training loss: 0.7018380165100098 = 0.07185860723257065 + 0.1 * 6.2997941970825195
Epoch 430, val loss: 0.7475401759147644
Epoch 440, training loss: 0.6973209381103516 = 0.06669244915246964 + 0.1 * 6.306284427642822
Epoch 440, val loss: 0.7567422389984131
Epoch 450, training loss: 0.6914643049240112 = 0.06200375407934189 + 0.1 * 6.294605731964111
Epoch 450, val loss: 0.7658477425575256
Epoch 460, training loss: 0.6889668703079224 = 0.0577334500849247 + 0.1 * 6.312333583831787
Epoch 460, val loss: 0.7749430537223816
Epoch 470, training loss: 0.6815634369850159 = 0.05385725572705269 + 0.1 * 6.277061462402344
Epoch 470, val loss: 0.783899188041687
Epoch 480, training loss: 0.6775182485580444 = 0.050319258123636246 + 0.1 * 6.271989822387695
Epoch 480, val loss: 0.7927922010421753
Epoch 490, training loss: 0.6755775809288025 = 0.047071609646081924 + 0.1 * 6.285059452056885
Epoch 490, val loss: 0.8016979694366455
Epoch 500, training loss: 0.6710653901100159 = 0.04409682750701904 + 0.1 * 6.2696852684021
Epoch 500, val loss: 0.8105136156082153
Epoch 510, training loss: 0.6679355502128601 = 0.041368741542100906 + 0.1 * 6.265667915344238
Epoch 510, val loss: 0.8192311525344849
Epoch 520, training loss: 0.6641333103179932 = 0.038866762071847916 + 0.1 * 6.2526655197143555
Epoch 520, val loss: 0.8277897834777832
Epoch 530, training loss: 0.6614028215408325 = 0.03655952587723732 + 0.1 * 6.2484331130981445
Epoch 530, val loss: 0.8363398313522339
Epoch 540, training loss: 0.6602787375450134 = 0.034428443759679794 + 0.1 * 6.258502960205078
Epoch 540, val loss: 0.8448140621185303
Epoch 550, training loss: 0.657850444316864 = 0.03246642276644707 + 0.1 * 6.253840446472168
Epoch 550, val loss: 0.8532595634460449
Epoch 560, training loss: 0.6560090780258179 = 0.03066154010593891 + 0.1 * 6.253475189208984
Epoch 560, val loss: 0.8615027666091919
Epoch 570, training loss: 0.6523182988166809 = 0.028995288535952568 + 0.1 * 6.233230113983154
Epoch 570, val loss: 0.8696455359458923
Epoch 580, training loss: 0.6501736044883728 = 0.027452923357486725 + 0.1 * 6.227206707000732
Epoch 580, val loss: 0.8776085376739502
Epoch 590, training loss: 0.6513144373893738 = 0.02602076530456543 + 0.1 * 6.252936363220215
Epoch 590, val loss: 0.885540246963501
Epoch 600, training loss: 0.6475400924682617 = 0.024696867913007736 + 0.1 * 6.228431701660156
Epoch 600, val loss: 0.893284797668457
Epoch 610, training loss: 0.645483672618866 = 0.023470265790820122 + 0.1 * 6.2201337814331055
Epoch 610, val loss: 0.9009377360343933
Epoch 620, training loss: 0.6446470618247986 = 0.022330274805426598 + 0.1 * 6.223167419433594
Epoch 620, val loss: 0.9084501266479492
Epoch 630, training loss: 0.6446604132652283 = 0.021268391981720924 + 0.1 * 6.233920574188232
Epoch 630, val loss: 0.9159106612205505
Epoch 640, training loss: 0.6415470838546753 = 0.020283430814743042 + 0.1 * 6.2126359939575195
Epoch 640, val loss: 0.9231123924255371
Epoch 650, training loss: 0.6403397917747498 = 0.01936447247862816 + 0.1 * 6.209753036499023
Epoch 650, val loss: 0.9302074313163757
Epoch 660, training loss: 0.6403102278709412 = 0.018503893166780472 + 0.1 * 6.218062877655029
Epoch 660, val loss: 0.9372137784957886
Epoch 670, training loss: 0.6380465626716614 = 0.017699962481856346 + 0.1 * 6.203465938568115
Epoch 670, val loss: 0.9440606832504272
Epoch 680, training loss: 0.636618435382843 = 0.01694757491350174 + 0.1 * 6.196708679199219
Epoch 680, val loss: 0.950765073299408
Epoch 690, training loss: 0.6355422735214233 = 0.016241732984781265 + 0.1 * 6.193005084991455
Epoch 690, val loss: 0.9573827385902405
Epoch 700, training loss: 0.6349999904632568 = 0.015581302344799042 + 0.1 * 6.194186687469482
Epoch 700, val loss: 0.9637997150421143
Epoch 710, training loss: 0.6348210573196411 = 0.014961019158363342 + 0.1 * 6.1986002922058105
Epoch 710, val loss: 0.970184862613678
Epoch 720, training loss: 0.6333353519439697 = 0.014380053617060184 + 0.1 * 6.1895527839660645
Epoch 720, val loss: 0.9763445854187012
Epoch 730, training loss: 0.6323971152305603 = 0.013833095319569111 + 0.1 * 6.18563985824585
Epoch 730, val loss: 0.9824551939964294
Epoch 740, training loss: 0.6321804523468018 = 0.013316641561686993 + 0.1 * 6.188638210296631
Epoch 740, val loss: 0.988494873046875
Epoch 750, training loss: 0.631890058517456 = 0.012829331681132317 + 0.1 * 6.19060754776001
Epoch 750, val loss: 0.9944189190864563
Epoch 760, training loss: 0.6305320858955383 = 0.012369202449917793 + 0.1 * 6.181628704071045
Epoch 760, val loss: 1.000239372253418
Epoch 770, training loss: 0.6302368640899658 = 0.01193498820066452 + 0.1 * 6.183018684387207
Epoch 770, val loss: 1.005919337272644
Epoch 780, training loss: 0.6296546459197998 = 0.011523976922035217 + 0.1 * 6.181306838989258
Epoch 780, val loss: 1.0115625858306885
Epoch 790, training loss: 0.6281242370605469 = 0.011135024018585682 + 0.1 * 6.169891834259033
Epoch 790, val loss: 1.0170551538467407
Epoch 800, training loss: 0.6283154487609863 = 0.010766605846583843 + 0.1 * 6.175487995147705
Epoch 800, val loss: 1.0224603414535522
Epoch 810, training loss: 0.6270960569381714 = 0.010416785255074501 + 0.1 * 6.166792392730713
Epoch 810, val loss: 1.0278114080429077
Epoch 820, training loss: 0.627539873123169 = 0.010084462352097034 + 0.1 * 6.174553871154785
Epoch 820, val loss: 1.0330966711044312
Epoch 830, training loss: 0.6264259815216064 = 0.009770277887582779 + 0.1 * 6.1665568351745605
Epoch 830, val loss: 1.0382297039031982
Epoch 840, training loss: 0.6265063285827637 = 0.009472329169511795 + 0.1 * 6.170340061187744
Epoch 840, val loss: 1.043245553970337
Epoch 850, training loss: 0.625260055065155 = 0.009187410585582256 + 0.1 * 6.160726547241211
Epoch 850, val loss: 1.0483062267303467
Epoch 860, training loss: 0.6247748732566833 = 0.008917122147977352 + 0.1 * 6.1585774421691895
Epoch 860, val loss: 1.0531617403030396
Epoch 870, training loss: 0.6245672106742859 = 0.008658886887133121 + 0.1 * 6.159082889556885
Epoch 870, val loss: 1.0579992532730103
Epoch 880, training loss: 0.6237170100212097 = 0.008412093855440617 + 0.1 * 6.153048515319824
Epoch 880, val loss: 1.0627522468566895
Epoch 890, training loss: 0.6231727004051208 = 0.008175755850970745 + 0.1 * 6.149969577789307
Epoch 890, val loss: 1.0675245523452759
Epoch 900, training loss: 0.623090922832489 = 0.00795100536197424 + 0.1 * 6.1513991355896
Epoch 900, val loss: 1.072114109992981
Epoch 910, training loss: 0.6257408857345581 = 0.007736079394817352 + 0.1 * 6.180047988891602
Epoch 910, val loss: 1.0766764879226685
Epoch 920, training loss: 0.6229937076568604 = 0.007530407048761845 + 0.1 * 6.154633045196533
Epoch 920, val loss: 1.081217646598816
Epoch 930, training loss: 0.6232308745384216 = 0.007334493566304445 + 0.1 * 6.158963680267334
Epoch 930, val loss: 1.0855956077575684
Epoch 940, training loss: 0.6214843988418579 = 0.00714656338095665 + 0.1 * 6.143378734588623
Epoch 940, val loss: 1.0899605751037598
Epoch 950, training loss: 0.6222683191299438 = 0.006967037450522184 + 0.1 * 6.153012752532959
Epoch 950, val loss: 1.0941717624664307
Epoch 960, training loss: 0.6207939386367798 = 0.006793659180402756 + 0.1 * 6.140002727508545
Epoch 960, val loss: 1.09843111038208
Epoch 970, training loss: 0.6214934587478638 = 0.006627807393670082 + 0.1 * 6.148656368255615
Epoch 970, val loss: 1.1025549173355103
Epoch 980, training loss: 0.6208012104034424 = 0.006468163803219795 + 0.1 * 6.143330097198486
Epoch 980, val loss: 1.1066923141479492
Epoch 990, training loss: 0.6210060715675354 = 0.0063156685791909695 + 0.1 * 6.146903991699219
Epoch 990, val loss: 1.1106829643249512
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7491
Flip ASR: 0.7333/225 nodes
The final ASR:0.66913, 0.05751, Accuracy:0.82716, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10530])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97294, 0.00460, Accuracy:0.82963, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7736806869506836 = 1.9363034963607788 + 0.1 * 8.373772621154785
Epoch 0, val loss: 1.9299182891845703
Epoch 10, training loss: 2.7641124725341797 = 1.9267616271972656 + 0.1 * 8.373509407043457
Epoch 10, val loss: 1.9211262464523315
Epoch 20, training loss: 2.752093553543091 = 1.9148805141448975 + 0.1 * 8.372129440307617
Epoch 20, val loss: 1.9098628759384155
Epoch 30, training loss: 2.7342796325683594 = 1.898064374923706 + 0.1 * 8.362153053283691
Epoch 30, val loss: 1.893705129623413
Epoch 40, training loss: 2.701695203781128 = 1.872957468032837 + 0.1 * 8.287376403808594
Epoch 40, val loss: 1.8697372674942017
Epoch 50, training loss: 2.607900381088257 = 1.8395594358444214 + 0.1 * 7.683408737182617
Epoch 50, val loss: 1.8391363620758057
Epoch 60, training loss: 2.5283241271972656 = 1.8040406703948975 + 0.1 * 7.242835521697998
Epoch 60, val loss: 1.8083412647247314
Epoch 70, training loss: 2.4640281200408936 = 1.7681999206542969 + 0.1 * 6.958282470703125
Epoch 70, val loss: 1.7790319919586182
Epoch 80, training loss: 2.41125226020813 = 1.7292569875717163 + 0.1 * 6.819952011108398
Epoch 80, val loss: 1.747066855430603
Epoch 90, training loss: 2.3543074131011963 = 1.6804834604263306 + 0.1 * 6.73823881149292
Epoch 90, val loss: 1.7059446573257446
Epoch 100, training loss: 2.284635543823242 = 1.6162090301513672 + 0.1 * 6.684263706207275
Epoch 100, val loss: 1.6526739597320557
Epoch 110, training loss: 2.200021266937256 = 1.5355932712554932 + 0.1 * 6.644278526306152
Epoch 110, val loss: 1.5883058309555054
Epoch 120, training loss: 2.1034629344940186 = 1.4417401552200317 + 0.1 * 6.617227077484131
Epoch 120, val loss: 1.5114575624465942
Epoch 130, training loss: 1.9998488426208496 = 1.3401837348937988 + 0.1 * 6.596650123596191
Epoch 130, val loss: 1.4279755353927612
Epoch 140, training loss: 1.8930909633636475 = 1.2351526021957397 + 0.1 * 6.579383850097656
Epoch 140, val loss: 1.3410029411315918
Epoch 150, training loss: 1.787174940109253 = 1.1308200359344482 + 0.1 * 6.563549041748047
Epoch 150, val loss: 1.2566934823989868
Epoch 160, training loss: 1.6877696514129639 = 1.0325642824172974 + 0.1 * 6.552052974700928
Epoch 160, val loss: 1.179997205734253
Epoch 170, training loss: 1.5960900783538818 = 0.9424512386322021 + 0.1 * 6.536388874053955
Epoch 170, val loss: 1.1124584674835205
Epoch 180, training loss: 1.5110409259796143 = 0.8589392304420471 + 0.1 * 6.521016597747803
Epoch 180, val loss: 1.0520025491714478
Epoch 190, training loss: 1.4311589002609253 = 0.7805154323577881 + 0.1 * 6.506434440612793
Epoch 190, val loss: 0.9959088563919067
Epoch 200, training loss: 1.3554071187973022 = 0.706204354763031 + 0.1 * 6.492027759552002
Epoch 200, val loss: 0.9428341388702393
Epoch 210, training loss: 1.2838740348815918 = 0.6361614465713501 + 0.1 * 6.47712516784668
Epoch 210, val loss: 0.8927646279335022
Epoch 220, training loss: 1.2181363105773926 = 0.570999026298523 + 0.1 * 6.471373558044434
Epoch 220, val loss: 0.8468000888824463
Epoch 230, training loss: 1.1567561626434326 = 0.511601448059082 + 0.1 * 6.4515461921691895
Epoch 230, val loss: 0.8067458868026733
Epoch 240, training loss: 1.101387858390808 = 0.4572693109512329 + 0.1 * 6.441185474395752
Epoch 240, val loss: 0.7733029127120972
Epoch 250, training loss: 1.052126407623291 = 0.4082697033882141 + 0.1 * 6.438567638397217
Epoch 250, val loss: 0.7470686435699463
Epoch 260, training loss: 1.005610466003418 = 0.36349034309387207 + 0.1 * 6.421201705932617
Epoch 260, val loss: 0.7261479496955872
Epoch 270, training loss: 0.9632487297058105 = 0.32209691405296326 + 0.1 * 6.41151762008667
Epoch 270, val loss: 0.7092878222465515
Epoch 280, training loss: 0.9254828691482544 = 0.284359872341156 + 0.1 * 6.411229610443115
Epoch 280, val loss: 0.6959291100502014
Epoch 290, training loss: 0.890461802482605 = 0.25056397914886475 + 0.1 * 6.398978233337402
Epoch 290, val loss: 0.6859758496284485
Epoch 300, training loss: 0.859241783618927 = 0.22051310539245605 + 0.1 * 6.38728666305542
Epoch 300, val loss: 0.6790471076965332
Epoch 310, training loss: 0.8337250351905823 = 0.19428223371505737 + 0.1 * 6.39442777633667
Epoch 310, val loss: 0.6749771237373352
Epoch 320, training loss: 0.8093138933181763 = 0.17171819508075714 + 0.1 * 6.375957012176514
Epoch 320, val loss: 0.6736968159675598
Epoch 330, training loss: 0.7892364263534546 = 0.15221555531024933 + 0.1 * 6.370208740234375
Epoch 330, val loss: 0.6747336387634277
Epoch 340, training loss: 0.7718193531036377 = 0.13552942872047424 + 0.1 * 6.362898826599121
Epoch 340, val loss: 0.677612841129303
Epoch 350, training loss: 0.7566922307014465 = 0.12123978137969971 + 0.1 * 6.354524612426758
Epoch 350, val loss: 0.6824159622192383
Epoch 360, training loss: 0.7444062232971191 = 0.10883920639753342 + 0.1 * 6.355669975280762
Epoch 360, val loss: 0.6885109543800354
Epoch 370, training loss: 0.7321967482566833 = 0.09805507212877274 + 0.1 * 6.341416835784912
Epoch 370, val loss: 0.6958774924278259
Epoch 380, training loss: 0.7222065925598145 = 0.08862186968326569 + 0.1 * 6.3358473777771
Epoch 380, val loss: 0.704150378704071
Epoch 390, training loss: 0.7134714722633362 = 0.08037416636943817 + 0.1 * 6.330972671508789
Epoch 390, val loss: 0.7130990624427795
Epoch 400, training loss: 0.7049962878227234 = 0.07313147932291031 + 0.1 * 6.318647861480713
Epoch 400, val loss: 0.7226276993751526
Epoch 410, training loss: 0.7051841020584106 = 0.0667271763086319 + 0.1 * 6.38456916809082
Epoch 410, val loss: 0.7324972748756409
Epoch 420, training loss: 0.6914129257202148 = 0.061126045882701874 + 0.1 * 6.302868843078613
Epoch 420, val loss: 0.7425183653831482
Epoch 430, training loss: 0.6863830089569092 = 0.056145697832107544 + 0.1 * 6.302372932434082
Epoch 430, val loss: 0.7525732517242432
Epoch 440, training loss: 0.6807901859283447 = 0.05170401930809021 + 0.1 * 6.290861129760742
Epoch 440, val loss: 0.7627668976783752
Epoch 450, training loss: 0.680236279964447 = 0.04772547259926796 + 0.1 * 6.325107574462891
Epoch 450, val loss: 0.772890031337738
Epoch 460, training loss: 0.6731245517730713 = 0.04418380558490753 + 0.1 * 6.289407730102539
Epoch 460, val loss: 0.7828963398933411
Epoch 470, training loss: 0.6687225699424744 = 0.04100693762302399 + 0.1 * 6.277156352996826
Epoch 470, val loss: 0.792800784111023
Epoch 480, training loss: 0.6652718186378479 = 0.03814183548092842 + 0.1 * 6.271299362182617
Epoch 480, val loss: 0.8024559617042542
Epoch 490, training loss: 0.6630212664604187 = 0.035564206540584564 + 0.1 * 6.274570465087891
Epoch 490, val loss: 0.8121568560600281
Epoch 500, training loss: 0.6591976284980774 = 0.03323771059513092 + 0.1 * 6.259599208831787
Epoch 500, val loss: 0.8214889168739319
Epoch 510, training loss: 0.6569031476974487 = 0.031136592850089073 + 0.1 * 6.257665634155273
Epoch 510, val loss: 0.8308562636375427
Epoch 520, training loss: 0.6561634540557861 = 0.02922682650387287 + 0.1 * 6.269366264343262
Epoch 520, val loss: 0.8399240970611572
Epoch 530, training loss: 0.6528883576393127 = 0.027491968125104904 + 0.1 * 6.253963470458984
Epoch 530, val loss: 0.8489181399345398
Epoch 540, training loss: 0.6501055359840393 = 0.025908013805747032 + 0.1 * 6.2419753074646
Epoch 540, val loss: 0.8577045202255249
Epoch 550, training loss: 0.6484676599502563 = 0.024456918239593506 + 0.1 * 6.24010705947876
Epoch 550, val loss: 0.8662865161895752
Epoch 560, training loss: 0.6479888558387756 = 0.023129688575863838 + 0.1 * 6.248591423034668
Epoch 560, val loss: 0.8747207522392273
Epoch 570, training loss: 0.64557284116745 = 0.021912848576903343 + 0.1 * 6.236599445343018
Epoch 570, val loss: 0.8829972743988037
Epoch 580, training loss: 0.644477903842926 = 0.020792583003640175 + 0.1 * 6.236852645874023
Epoch 580, val loss: 0.8911203742027283
Epoch 590, training loss: 0.6422656178474426 = 0.01975642517209053 + 0.1 * 6.225091934204102
Epoch 590, val loss: 0.8989307284355164
Epoch 600, training loss: 0.6430094838142395 = 0.01880040019750595 + 0.1 * 6.242090702056885
Epoch 600, val loss: 0.9067391157150269
Epoch 610, training loss: 0.6400749087333679 = 0.017916252836585045 + 0.1 * 6.221586227416992
Epoch 610, val loss: 0.9142823815345764
Epoch 620, training loss: 0.6387101411819458 = 0.017095152288675308 + 0.1 * 6.216149806976318
Epoch 620, val loss: 0.9218189120292664
Epoch 630, training loss: 0.6381185054779053 = 0.01632959581911564 + 0.1 * 6.217889308929443
Epoch 630, val loss: 0.9289655089378357
Epoch 640, training loss: 0.6368916034698486 = 0.015618998557329178 + 0.1 * 6.21272611618042
Epoch 640, val loss: 0.9362586736679077
Epoch 650, training loss: 0.6356748342514038 = 0.014952843077480793 + 0.1 * 6.207220077514648
Epoch 650, val loss: 0.9431391358375549
Epoch 660, training loss: 0.6366788744926453 = 0.014331872574985027 + 0.1 * 6.2234697341918945
Epoch 660, val loss: 0.9500206708908081
Epoch 670, training loss: 0.6336871981620789 = 0.01375281997025013 + 0.1 * 6.199343681335449
Epoch 670, val loss: 0.956807017326355
Epoch 680, training loss: 0.6334511041641235 = 0.013210715726017952 + 0.1 * 6.202403545379639
Epoch 680, val loss: 0.9634439945220947
Epoch 690, training loss: 0.6321956515312195 = 0.012699611485004425 + 0.1 * 6.194960594177246
Epoch 690, val loss: 0.9698244333267212
Epoch 700, training loss: 0.6313433647155762 = 0.012220954522490501 + 0.1 * 6.191223621368408
Epoch 700, val loss: 0.9762789011001587
Epoch 710, training loss: 0.6307395100593567 = 0.01176705677062273 + 0.1 * 6.189724922180176
Epoch 710, val loss: 0.9823599457740784
Epoch 720, training loss: 0.6311432719230652 = 0.01134380605071783 + 0.1 * 6.197994709014893
Epoch 720, val loss: 0.9885662198066711
Epoch 730, training loss: 0.6300565004348755 = 0.010943826287984848 + 0.1 * 6.191126823425293
Epoch 730, val loss: 0.9944746494293213
Epoch 740, training loss: 0.6287575960159302 = 0.010567964054644108 + 0.1 * 6.181896209716797
Epoch 740, val loss: 1.0003838539123535
Epoch 750, training loss: 0.6292029023170471 = 0.01021165493875742 + 0.1 * 6.18991231918335
Epoch 750, val loss: 1.0061099529266357
Epoch 760, training loss: 0.6280595660209656 = 0.009873859584331512 + 0.1 * 6.181856632232666
Epoch 760, val loss: 1.0116875171661377
Epoch 770, training loss: 0.6275655031204224 = 0.009555915370583534 + 0.1 * 6.180095672607422
Epoch 770, val loss: 1.0173174142837524
Epoch 780, training loss: 0.6272059679031372 = 0.00925218965858221 + 0.1 * 6.179537773132324
Epoch 780, val loss: 1.0227504968643188
Epoch 790, training loss: 0.6261183619499207 = 0.008964765816926956 + 0.1 * 6.171535968780518
Epoch 790, val loss: 1.0280983448028564
Epoch 800, training loss: 0.6261938214302063 = 0.008690962567925453 + 0.1 * 6.175028324127197
Epoch 800, val loss: 1.0332727432250977
Epoch 810, training loss: 0.6249892115592957 = 0.008431716822087765 + 0.1 * 6.165574550628662
Epoch 810, val loss: 1.0384770631790161
Epoch 820, training loss: 0.625358521938324 = 0.008185061626136303 + 0.1 * 6.17173433303833
Epoch 820, val loss: 1.043643832206726
Epoch 830, training loss: 0.6249862909317017 = 0.007948508486151695 + 0.1 * 6.170377731323242
Epoch 830, val loss: 1.0484222173690796
Epoch 840, training loss: 0.6245046854019165 = 0.007725848350673914 + 0.1 * 6.167788505554199
Epoch 840, val loss: 1.0534942150115967
Epoch 850, training loss: 0.6232327818870544 = 0.00751275010406971 + 0.1 * 6.157200336456299
Epoch 850, val loss: 1.0583956241607666
Epoch 860, training loss: 0.6246109008789062 = 0.007308081723749638 + 0.1 * 6.173027992248535
Epoch 860, val loss: 1.0631040334701538
Epoch 870, training loss: 0.6228038668632507 = 0.007112280931323767 + 0.1 * 6.156915664672852
Epoch 870, val loss: 1.0677058696746826
Epoch 880, training loss: 0.6226239800453186 = 0.006926081608980894 + 0.1 * 6.156979084014893
Epoch 880, val loss: 1.0723963975906372
Epoch 890, training loss: 0.6222241520881653 = 0.006746659986674786 + 0.1 * 6.154775142669678
Epoch 890, val loss: 1.076879858970642
Epoch 900, training loss: 0.621964693069458 = 0.006575761828571558 + 0.1 * 6.153889179229736
Epoch 900, val loss: 1.081375002861023
Epoch 910, training loss: 0.6220030784606934 = 0.0064120907336473465 + 0.1 * 6.155910015106201
Epoch 910, val loss: 1.0858277082443237
Epoch 920, training loss: 0.6217289566993713 = 0.006254670210182667 + 0.1 * 6.15474271774292
Epoch 920, val loss: 1.0901840925216675
Epoch 930, training loss: 0.621247410774231 = 0.006104010157287121 + 0.1 * 6.151433944702148
Epoch 930, val loss: 1.0944507122039795
Epoch 940, training loss: 0.6202934980392456 = 0.005959047004580498 + 0.1 * 6.143344402313232
Epoch 940, val loss: 1.0986318588256836
Epoch 950, training loss: 0.6201848387718201 = 0.005820906721055508 + 0.1 * 6.143639087677002
Epoch 950, val loss: 1.1029008626937866
Epoch 960, training loss: 0.6214413046836853 = 0.005686973687261343 + 0.1 * 6.157543182373047
Epoch 960, val loss: 1.1069514751434326
Epoch 970, training loss: 0.6204756498336792 = 0.005558537784963846 + 0.1 * 6.149170875549316
Epoch 970, val loss: 1.1109602451324463
Epoch 980, training loss: 0.6199113130569458 = 0.005435147788375616 + 0.1 * 6.144761085510254
Epoch 980, val loss: 1.1148998737335205
Epoch 990, training loss: 0.6197489500045776 = 0.005316595546901226 + 0.1 * 6.144323348999023
Epoch 990, val loss: 1.118943214416504
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.791062831878662 = 1.9536789655685425 + 0.1 * 8.373838424682617
Epoch 0, val loss: 1.9566175937652588
Epoch 10, training loss: 2.780999183654785 = 1.9436259269714355 + 0.1 * 8.373732566833496
Epoch 10, val loss: 1.9466285705566406
Epoch 20, training loss: 2.7688357830047607 = 1.931525707244873 + 0.1 * 8.373101234436035
Epoch 20, val loss: 1.9345289468765259
Epoch 30, training loss: 2.751375436782837 = 1.9145009517669678 + 0.1 * 8.368743896484375
Epoch 30, val loss: 1.917359709739685
Epoch 40, training loss: 2.7226791381835938 = 1.8890644311904907 + 0.1 * 8.336148262023926
Epoch 40, val loss: 1.8917948007583618
Epoch 50, training loss: 2.6577796936035156 = 1.8532838821411133 + 0.1 * 8.044957160949707
Epoch 50, val loss: 1.8574848175048828
Epoch 60, training loss: 2.562349319458008 = 1.8138465881347656 + 0.1 * 7.4850263595581055
Epoch 60, val loss: 1.8220441341400146
Epoch 70, training loss: 2.4862098693847656 = 1.7764828205108643 + 0.1 * 7.097269058227539
Epoch 70, val loss: 1.789944052696228
Epoch 80, training loss: 2.4313817024230957 = 1.7394521236419678 + 0.1 * 6.919294357299805
Epoch 80, val loss: 1.7588618993759155
Epoch 90, training loss: 2.3761043548583984 = 1.6938272714614868 + 0.1 * 6.822770118713379
Epoch 90, val loss: 1.7193225622177124
Epoch 100, training loss: 2.3077831268310547 = 1.6325764656066895 + 0.1 * 6.752066135406494
Epoch 100, val loss: 1.6669636964797974
Epoch 110, training loss: 2.2240071296691895 = 1.5537782907485962 + 0.1 * 6.702287673950195
Epoch 110, val loss: 1.6019933223724365
Epoch 120, training loss: 2.1267831325531006 = 1.460509181022644 + 0.1 * 6.6627397537231445
Epoch 120, val loss: 1.526824712753296
Epoch 130, training loss: 2.024294137954712 = 1.3607957363128662 + 0.1 * 6.634984493255615
Epoch 130, val loss: 1.4479339122772217
Epoch 140, training loss: 1.9214200973510742 = 1.260252594947815 + 0.1 * 6.61167573928833
Epoch 140, val loss: 1.3701719045639038
Epoch 150, training loss: 1.8212144374847412 = 1.161914348602295 + 0.1 * 6.593000888824463
Epoch 150, val loss: 1.29647696018219
Epoch 160, training loss: 1.7292206287384033 = 1.0714389085769653 + 0.1 * 6.577816486358643
Epoch 160, val loss: 1.2303017377853394
Epoch 170, training loss: 1.6470801830291748 = 0.9905313849449158 + 0.1 * 6.565487861633301
Epoch 170, val loss: 1.1720881462097168
Epoch 180, training loss: 1.5737783908843994 = 0.9172762632369995 + 0.1 * 6.565020561218262
Epoch 180, val loss: 1.1203917264938354
Epoch 190, training loss: 1.504420280456543 = 0.8495379686355591 + 0.1 * 6.548823356628418
Epoch 190, val loss: 1.0731801986694336
Epoch 200, training loss: 1.4373674392700195 = 0.7833870649337769 + 0.1 * 6.5398030281066895
Epoch 200, val loss: 1.0268117189407349
Epoch 210, training loss: 1.3702548742294312 = 0.7167162299156189 + 0.1 * 6.535386085510254
Epoch 210, val loss: 0.9800929427146912
Epoch 220, training loss: 1.3034331798553467 = 0.6505423784255981 + 0.1 * 6.528907775878906
Epoch 220, val loss: 0.9341066479682922
Epoch 230, training loss: 1.2385631799697876 = 0.5865203142166138 + 0.1 * 6.520428657531738
Epoch 230, val loss: 0.890507161617279
Epoch 240, training loss: 1.1789802312850952 = 0.5263713598251343 + 0.1 * 6.526088714599609
Epoch 240, val loss: 0.8513920903205872
Epoch 250, training loss: 1.1229934692382812 = 0.47206249833106995 + 0.1 * 6.5093092918396
Epoch 250, val loss: 0.8190146088600159
Epoch 260, training loss: 1.0731135606765747 = 0.42315202951431274 + 0.1 * 6.49961519241333
Epoch 260, val loss: 0.7933652997016907
Epoch 270, training loss: 1.028425693511963 = 0.37904250621795654 + 0.1 * 6.493831157684326
Epoch 270, val loss: 0.7744379043579102
Epoch 280, training loss: 0.987878680229187 = 0.3393220603466034 + 0.1 * 6.485566139221191
Epoch 280, val loss: 0.7616314888000488
Epoch 290, training loss: 0.9509620666503906 = 0.30325475335121155 + 0.1 * 6.477072715759277
Epoch 290, val loss: 0.7537989020347595
Epoch 300, training loss: 0.919022798538208 = 0.2706983983516693 + 0.1 * 6.483243465423584
Epoch 300, val loss: 0.750124990940094
Epoch 310, training loss: 0.8882458806037903 = 0.2416277527809143 + 0.1 * 6.46618127822876
Epoch 310, val loss: 0.7499340772628784
Epoch 320, training loss: 0.8612414598464966 = 0.2155732363462448 + 0.1 * 6.456682205200195
Epoch 320, val loss: 0.7521020770072937
Epoch 330, training loss: 0.8384379744529724 = 0.1924368143081665 + 0.1 * 6.4600114822387695
Epoch 330, val loss: 0.756219744682312
Epoch 340, training loss: 0.8166406750679016 = 0.17202837765216827 + 0.1 * 6.446123123168945
Epoch 340, val loss: 0.7621392011642456
Epoch 350, training loss: 0.7982964515686035 = 0.15403057634830475 + 0.1 * 6.4426589012146
Epoch 350, val loss: 0.7695319652557373
Epoch 360, training loss: 0.781505823135376 = 0.13823416829109192 + 0.1 * 6.432716369628906
Epoch 360, val loss: 0.7782489657402039
Epoch 370, training loss: 0.7666168212890625 = 0.12433811277151108 + 0.1 * 6.422787189483643
Epoch 370, val loss: 0.7881051898002625
Epoch 380, training loss: 0.7539130449295044 = 0.11212939769029617 + 0.1 * 6.417836666107178
Epoch 380, val loss: 0.7987850904464722
Epoch 390, training loss: 0.7421430349349976 = 0.10142970830202103 + 0.1 * 6.407133102416992
Epoch 390, val loss: 0.8100829124450684
Epoch 400, training loss: 0.7323732376098633 = 0.0919731929898262 + 0.1 * 6.404000282287598
Epoch 400, val loss: 0.821804404258728
Epoch 410, training loss: 0.7238821387290955 = 0.08361079543828964 + 0.1 * 6.402713775634766
Epoch 410, val loss: 0.8339675068855286
Epoch 420, training loss: 0.7152788639068604 = 0.07619741559028625 + 0.1 * 6.390814781188965
Epoch 420, val loss: 0.8463218808174133
Epoch 430, training loss: 0.7105963230133057 = 0.06958181411027908 + 0.1 * 6.410145282745361
Epoch 430, val loss: 0.8587380647659302
Epoch 440, training loss: 0.7013811469078064 = 0.06369620561599731 + 0.1 * 6.376849174499512
Epoch 440, val loss: 0.8711813688278198
Epoch 450, training loss: 0.6950613260269165 = 0.058423012495040894 + 0.1 * 6.366383075714111
Epoch 450, val loss: 0.8834547996520996
Epoch 460, training loss: 0.6913960576057434 = 0.05367930978536606 + 0.1 * 6.377167701721191
Epoch 460, val loss: 0.8956671953201294
Epoch 470, training loss: 0.6851905584335327 = 0.04942242428660393 + 0.1 * 6.357680797576904
Epoch 470, val loss: 0.9078150391578674
Epoch 480, training loss: 0.6808414459228516 = 0.045585788786411285 + 0.1 * 6.352556228637695
Epoch 480, val loss: 0.9196053743362427
Epoch 490, training loss: 0.6771818399429321 = 0.04212388023734093 + 0.1 * 6.350579738616943
Epoch 490, val loss: 0.9311971664428711
Epoch 500, training loss: 0.6726979613304138 = 0.039007581770420074 + 0.1 * 6.3369035720825195
Epoch 500, val loss: 0.9424683451652527
Epoch 510, training loss: 0.669783353805542 = 0.03619174286723137 + 0.1 * 6.335915565490723
Epoch 510, val loss: 0.953579843044281
Epoch 520, training loss: 0.6665101647377014 = 0.03364790230989456 + 0.1 * 6.328622341156006
Epoch 520, val loss: 0.9645192623138428
Epoch 530, training loss: 0.6637758016586304 = 0.03135045990347862 + 0.1 * 6.324253559112549
Epoch 530, val loss: 0.9749805331230164
Epoch 540, training loss: 0.6616604924201965 = 0.02927413396537304 + 0.1 * 6.323863506317139
Epoch 540, val loss: 0.9855145215988159
Epoch 550, training loss: 0.6591925621032715 = 0.027394361793994904 + 0.1 * 6.317981719970703
Epoch 550, val loss: 0.9952777028083801
Epoch 560, training loss: 0.656449556350708 = 0.025691043585538864 + 0.1 * 6.3075852394104
Epoch 560, val loss: 1.0052820444107056
Epoch 570, training loss: 0.6544370055198669 = 0.024138670414686203 + 0.1 * 6.302983283996582
Epoch 570, val loss: 1.0148078203201294
Epoch 580, training loss: 0.6527884006500244 = 0.0227234847843647 + 0.1 * 6.3006486892700195
Epoch 580, val loss: 1.0240873098373413
Epoch 590, training loss: 0.6511855125427246 = 0.021432975307106972 + 0.1 * 6.297524929046631
Epoch 590, val loss: 1.0333539247512817
Epoch 600, training loss: 0.649634063243866 = 0.02025195024907589 + 0.1 * 6.293821334838867
Epoch 600, val loss: 1.0421589612960815
Epoch 610, training loss: 0.6477566361427307 = 0.019169913604855537 + 0.1 * 6.285867214202881
Epoch 610, val loss: 1.050771713256836
Epoch 620, training loss: 0.6467586159706116 = 0.01817678101360798 + 0.1 * 6.285818099975586
Epoch 620, val loss: 1.0592831373214722
Epoch 630, training loss: 0.6448967456817627 = 0.017263269051909447 + 0.1 * 6.276334285736084
Epoch 630, val loss: 1.0672975778579712
Epoch 640, training loss: 0.643149197101593 = 0.016420966014266014 + 0.1 * 6.267282485961914
Epoch 640, val loss: 1.075437307357788
Epoch 650, training loss: 0.6429929733276367 = 0.015641089528799057 + 0.1 * 6.2735185623168945
Epoch 650, val loss: 1.0832195281982422
Epoch 660, training loss: 0.6413815021514893 = 0.01491990964859724 + 0.1 * 6.264615535736084
Epoch 660, val loss: 1.0905616283416748
Epoch 670, training loss: 0.6405385732650757 = 0.014252766035497189 + 0.1 * 6.262857913970947
Epoch 670, val loss: 1.098213791847229
Epoch 680, training loss: 0.6406586766242981 = 0.013630986213684082 + 0.1 * 6.2702765464782715
Epoch 680, val loss: 1.1053169965744019
Epoch 690, training loss: 0.6382938623428345 = 0.013052737340331078 + 0.1 * 6.252410888671875
Epoch 690, val loss: 1.1122946739196777
Epoch 700, training loss: 0.6371456384658813 = 0.012515182606875896 + 0.1 * 6.246304512023926
Epoch 700, val loss: 1.1193912029266357
Epoch 710, training loss: 0.636634886264801 = 0.012011492624878883 + 0.1 * 6.246233940124512
Epoch 710, val loss: 1.1262168884277344
Epoch 720, training loss: 0.6369906067848206 = 0.011539087630808353 + 0.1 * 6.254514694213867
Epoch 720, val loss: 1.1327638626098633
Epoch 730, training loss: 0.6362223625183105 = 0.011096550151705742 + 0.1 * 6.251258373260498
Epoch 730, val loss: 1.138992190361023
Epoch 740, training loss: 0.6343558430671692 = 0.010684091597795486 + 0.1 * 6.236717700958252
Epoch 740, val loss: 1.1455312967300415
Epoch 750, training loss: 0.6350556015968323 = 0.010296770371496677 + 0.1 * 6.247588157653809
Epoch 750, val loss: 1.1516798734664917
Epoch 760, training loss: 0.6335484981536865 = 0.009932311251759529 + 0.1 * 6.236161231994629
Epoch 760, val loss: 1.1578096151351929
Epoch 770, training loss: 0.6321170926094055 = 0.009588579647243023 + 0.1 * 6.225285053253174
Epoch 770, val loss: 1.1638517379760742
Epoch 780, training loss: 0.6321672201156616 = 0.00926300697028637 + 0.1 * 6.229042053222656
Epoch 780, val loss: 1.1697237491607666
Epoch 790, training loss: 0.6308338046073914 = 0.008954531513154507 + 0.1 * 6.218792915344238
Epoch 790, val loss: 1.1752649545669556
Epoch 800, training loss: 0.6313810348510742 = 0.008663215674459934 + 0.1 * 6.22717809677124
Epoch 800, val loss: 1.180661678314209
Epoch 810, training loss: 0.6303389668464661 = 0.008389861322939396 + 0.1 * 6.2194905281066895
Epoch 810, val loss: 1.1862238645553589
Epoch 820, training loss: 0.6304710507392883 = 0.00813068263232708 + 0.1 * 6.2234039306640625
Epoch 820, val loss: 1.1918845176696777
Epoch 830, training loss: 0.630653977394104 = 0.007883192971348763 + 0.1 * 6.227707862854004
Epoch 830, val loss: 1.196563959121704
Epoch 840, training loss: 0.6288554668426514 = 0.007650913204997778 + 0.1 * 6.212045192718506
Epoch 840, val loss: 1.2020362615585327
Epoch 850, training loss: 0.6279751658439636 = 0.007429833989590406 + 0.1 * 6.205452919006348
Epoch 850, val loss: 1.2072840929031372
Epoch 860, training loss: 0.6278697848320007 = 0.007217946462333202 + 0.1 * 6.206518173217773
Epoch 860, val loss: 1.212302803993225
Epoch 870, training loss: 0.629715085029602 = 0.00701474491506815 + 0.1 * 6.22700309753418
Epoch 870, val loss: 1.2167388200759888
Epoch 880, training loss: 0.6267993450164795 = 0.0068223970010876656 + 0.1 * 6.199769496917725
Epoch 880, val loss: 1.2215358018875122
Epoch 890, training loss: 0.6264151930809021 = 0.006640339735895395 + 0.1 * 6.19774866104126
Epoch 890, val loss: 1.2265782356262207
Epoch 900, training loss: 0.6260650753974915 = 0.00646571209654212 + 0.1 * 6.195993423461914
Epoch 900, val loss: 1.2312675714492798
Epoch 910, training loss: 0.6266539096832275 = 0.0062976316548883915 + 0.1 * 6.2035627365112305
Epoch 910, val loss: 1.2356730699539185
Epoch 920, training loss: 0.6260260343551636 = 0.006137656979262829 + 0.1 * 6.198883533477783
Epoch 920, val loss: 1.2401070594787598
Epoch 930, training loss: 0.6253917217254639 = 0.005984633229672909 + 0.1 * 6.194071292877197
Epoch 930, val loss: 1.2446187734603882
Epoch 940, training loss: 0.6261513233184814 = 0.0058388314209878445 + 0.1 * 6.203125
Epoch 940, val loss: 1.2489938735961914
Epoch 950, training loss: 0.6245592832565308 = 0.0056978873908519745 + 0.1 * 6.1886138916015625
Epoch 950, val loss: 1.2531317472457886
Epoch 960, training loss: 0.6243261098861694 = 0.005564057733863592 + 0.1 * 6.187620639801025
Epoch 960, val loss: 1.2574944496154785
Epoch 970, training loss: 0.6238757371902466 = 0.005434924736618996 + 0.1 * 6.184408187866211
Epoch 970, val loss: 1.2617764472961426
Epoch 980, training loss: 0.623401403427124 = 0.005310385022312403 + 0.1 * 6.180910587310791
Epoch 980, val loss: 1.2657227516174316
Epoch 990, training loss: 0.6256703734397888 = 0.005191221833229065 + 0.1 * 6.20479154586792
Epoch 990, val loss: 1.2697663307189941
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.804281234741211 = 1.9668914079666138 + 0.1 * 8.37389850616455
Epoch 0, val loss: 1.9694627523422241
Epoch 10, training loss: 2.7935545444488525 = 1.956175684928894 + 0.1 * 8.373788833618164
Epoch 10, val loss: 1.9587655067443848
Epoch 20, training loss: 2.7804603576660156 = 1.943140983581543 + 0.1 * 8.37319278717041
Epoch 20, val loss: 1.945539951324463
Epoch 30, training loss: 2.7616593837738037 = 1.9247753620147705 + 0.1 * 8.368840217590332
Epoch 30, val loss: 1.9268925189971924
Epoch 40, training loss: 2.7312393188476562 = 1.8970576524734497 + 0.1 * 8.341816902160645
Epoch 40, val loss: 1.8991929292678833
Epoch 50, training loss: 2.6777992248535156 = 1.8576222658157349 + 0.1 * 8.20176887512207
Epoch 50, val loss: 1.8619803190231323
Epoch 60, training loss: 2.6028506755828857 = 1.8129935264587402 + 0.1 * 7.898571491241455
Epoch 60, val loss: 1.8235605955123901
Epoch 70, training loss: 2.5207786560058594 = 1.7748159170150757 + 0.1 * 7.459628582000732
Epoch 70, val loss: 1.791825294494629
Epoch 80, training loss: 2.4448351860046387 = 1.7375890016555786 + 0.1 * 7.0724616050720215
Epoch 80, val loss: 1.758981704711914
Epoch 90, training loss: 2.3780198097229004 = 1.6916005611419678 + 0.1 * 6.864193439483643
Epoch 90, val loss: 1.7179105281829834
Epoch 100, training loss: 2.3089139461517334 = 1.6313045024871826 + 0.1 * 6.776094913482666
Epoch 100, val loss: 1.6644783020019531
Epoch 110, training loss: 2.2265372276306152 = 1.5535327196121216 + 0.1 * 6.730044364929199
Epoch 110, val loss: 1.596587896347046
Epoch 120, training loss: 2.1309149265289307 = 1.4611260890960693 + 0.1 * 6.697887420654297
Epoch 120, val loss: 1.5193191766738892
Epoch 130, training loss: 2.02748441696167 = 1.3600866794586182 + 0.1 * 6.673976898193359
Epoch 130, val loss: 1.4360318183898926
Epoch 140, training loss: 1.9212019443511963 = 1.2557601928710938 + 0.1 * 6.654417037963867
Epoch 140, val loss: 1.3508951663970947
Epoch 150, training loss: 1.8145794868469238 = 1.1510745286941528 + 0.1 * 6.635048866271973
Epoch 150, val loss: 1.2654259204864502
Epoch 160, training loss: 1.7119719982147217 = 1.049546241760254 + 0.1 * 6.624258041381836
Epoch 160, val loss: 1.182931900024414
Epoch 170, training loss: 1.6153230667114258 = 0.9546687602996826 + 0.1 * 6.606542110443115
Epoch 170, val loss: 1.1060625314712524
Epoch 180, training loss: 1.5260887145996094 = 0.8666706681251526 + 0.1 * 6.594180583953857
Epoch 180, val loss: 1.035431981086731
Epoch 190, training loss: 1.4449577331542969 = 0.7867529988288879 + 0.1 * 6.582046985626221
Epoch 190, val loss: 0.9728361964225769
Epoch 200, training loss: 1.371769905090332 = 0.7146559953689575 + 0.1 * 6.571138858795166
Epoch 200, val loss: 0.9181774854660034
Epoch 210, training loss: 1.3063935041427612 = 0.6503984928131104 + 0.1 * 6.55994987487793
Epoch 210, val loss: 0.8721880912780762
Epoch 220, training loss: 1.248232364654541 = 0.5933995842933655 + 0.1 * 6.548328399658203
Epoch 220, val loss: 0.8340606689453125
Epoch 230, training loss: 1.197228193283081 = 0.5429983735084534 + 0.1 * 6.54229736328125
Epoch 230, val loss: 0.8027828335762024
Epoch 240, training loss: 1.1515743732452393 = 0.4988410174846649 + 0.1 * 6.527333736419678
Epoch 240, val loss: 0.7780070900917053
Epoch 250, training loss: 1.1114709377288818 = 0.4595940113067627 + 0.1 * 6.518769264221191
Epoch 250, val loss: 0.7582563757896423
Epoch 260, training loss: 1.0759024620056152 = 0.424299031496048 + 0.1 * 6.516034126281738
Epoch 260, val loss: 0.7423533201217651
Epoch 270, training loss: 1.0420513153076172 = 0.39191535115242004 + 0.1 * 6.501359939575195
Epoch 270, val loss: 0.7293246984481812
Epoch 280, training loss: 1.0114916563034058 = 0.36151739954948425 + 0.1 * 6.49974250793457
Epoch 280, val loss: 0.718212902545929
Epoch 290, training loss: 0.9816229343414307 = 0.33293548226356506 + 0.1 * 6.486874103546143
Epoch 290, val loss: 0.7089167237281799
Epoch 300, training loss: 0.9536473751068115 = 0.30575188994407654 + 0.1 * 6.478954792022705
Epoch 300, val loss: 0.7010196447372437
Epoch 310, training loss: 0.9279885292053223 = 0.27988752722740173 + 0.1 * 6.481010437011719
Epoch 310, val loss: 0.694602906703949
Epoch 320, training loss: 0.9026635885238647 = 0.25566932559013367 + 0.1 * 6.469943046569824
Epoch 320, val loss: 0.689761757850647
Epoch 330, training loss: 0.8795437812805176 = 0.23296546936035156 + 0.1 * 6.46578311920166
Epoch 330, val loss: 0.6864340305328369
Epoch 340, training loss: 0.858894944190979 = 0.2117101550102234 + 0.1 * 6.471848011016846
Epoch 340, val loss: 0.6849737763404846
Epoch 350, training loss: 0.8371095657348633 = 0.1920519471168518 + 0.1 * 6.450575828552246
Epoch 350, val loss: 0.6854044795036316
Epoch 360, training loss: 0.819257915019989 = 0.1739736646413803 + 0.1 * 6.4528422355651855
Epoch 360, val loss: 0.6875275373458862
Epoch 370, training loss: 0.8019081354141235 = 0.15748487412929535 + 0.1 * 6.44423246383667
Epoch 370, val loss: 0.6912760138511658
Epoch 380, training loss: 0.7862070798873901 = 0.14253276586532593 + 0.1 * 6.436742782592773
Epoch 380, val loss: 0.6965095400810242
Epoch 390, training loss: 0.7735822796821594 = 0.12909434735774994 + 0.1 * 6.444879055023193
Epoch 390, val loss: 0.7026573419570923
Epoch 400, training loss: 0.7602841854095459 = 0.11717396229505539 + 0.1 * 6.431102275848389
Epoch 400, val loss: 0.7097596526145935
Epoch 410, training loss: 0.7490304708480835 = 0.10663856565952301 + 0.1 * 6.423919200897217
Epoch 410, val loss: 0.7178248167037964
Epoch 420, training loss: 0.7386466860771179 = 0.09730547666549683 + 0.1 * 6.413412094116211
Epoch 420, val loss: 0.72675621509552
Epoch 430, training loss: 0.7318338751792908 = 0.08898951858282089 + 0.1 * 6.428443908691406
Epoch 430, val loss: 0.736217200756073
Epoch 440, training loss: 0.7221384048461914 = 0.08159284293651581 + 0.1 * 6.405455112457275
Epoch 440, val loss: 0.74611496925354
Epoch 450, training loss: 0.7166504859924316 = 0.07498227804899216 + 0.1 * 6.41668176651001
Epoch 450, val loss: 0.7562907934188843
Epoch 460, training loss: 0.7085525393486023 = 0.06905306875705719 + 0.1 * 6.394994735717773
Epoch 460, val loss: 0.7667874693870544
Epoch 470, training loss: 0.7026850581169128 = 0.06368767470121384 + 0.1 * 6.3899736404418945
Epoch 470, val loss: 0.7773135304450989
Epoch 480, training loss: 0.6974007487297058 = 0.058818355202674866 + 0.1 * 6.385823726654053
Epoch 480, val loss: 0.7880858182907104
Epoch 490, training loss: 0.6934501528739929 = 0.05440475046634674 + 0.1 * 6.390453815460205
Epoch 490, val loss: 0.798732578754425
Epoch 500, training loss: 0.6876543164253235 = 0.050409238785505295 + 0.1 * 6.372450351715088
Epoch 500, val loss: 0.809324324131012
Epoch 510, training loss: 0.6839359998703003 = 0.046773914247751236 + 0.1 * 6.3716206550598145
Epoch 510, val loss: 0.8197699785232544
Epoch 520, training loss: 0.6798348426818848 = 0.043466534465551376 + 0.1 * 6.363683223724365
Epoch 520, val loss: 0.8300952911376953
Epoch 530, training loss: 0.6762920618057251 = 0.04045835882425308 + 0.1 * 6.358336448669434
Epoch 530, val loss: 0.8403113484382629
Epoch 540, training loss: 0.6756319999694824 = 0.03771510720252991 + 0.1 * 6.379168510437012
Epoch 540, val loss: 0.8502385020256042
Epoch 550, training loss: 0.6702285408973694 = 0.03522079810500145 + 0.1 * 6.350077152252197
Epoch 550, val loss: 0.8600289821624756
Epoch 560, training loss: 0.6678916215896606 = 0.03294248878955841 + 0.1 * 6.349491596221924
Epoch 560, val loss: 0.8695836663246155
Epoch 570, training loss: 0.6645434498786926 = 0.030860302969813347 + 0.1 * 6.336831092834473
Epoch 570, val loss: 0.8789858222007751
Epoch 580, training loss: 0.6643437743186951 = 0.02895401604473591 + 0.1 * 6.353897571563721
Epoch 580, val loss: 0.8881732225418091
Epoch 590, training loss: 0.661535918712616 = 0.02721172757446766 + 0.1 * 6.3432416915893555
Epoch 590, val loss: 0.8969417214393616
Epoch 600, training loss: 0.6583424806594849 = 0.025617819279432297 + 0.1 * 6.327246189117432
Epoch 600, val loss: 0.9057164788246155
Epoch 610, training loss: 0.6566361784934998 = 0.024152733385562897 + 0.1 * 6.32483434677124
Epoch 610, val loss: 0.9141620993614197
Epoch 620, training loss: 0.6550054550170898 = 0.022805241867899895 + 0.1 * 6.322002410888672
Epoch 620, val loss: 0.9224397540092468
Epoch 630, training loss: 0.653218150138855 = 0.021565580740571022 + 0.1 * 6.316525936126709
Epoch 630, val loss: 0.9304500222206116
Epoch 640, training loss: 0.6533756256103516 = 0.02042376436293125 + 0.1 * 6.3295183181762695
Epoch 640, val loss: 0.938291072845459
Epoch 650, training loss: 0.6507948637008667 = 0.01937142014503479 + 0.1 * 6.314234733581543
Epoch 650, val loss: 0.9458710551261902
Epoch 660, training loss: 0.6485388875007629 = 0.018399184569716454 + 0.1 * 6.30139684677124
Epoch 660, val loss: 0.9533959031105042
Epoch 670, training loss: 0.6488966345787048 = 0.017498264089226723 + 0.1 * 6.313983917236328
Epoch 670, val loss: 0.9606208205223083
Epoch 680, training loss: 0.6466833353042603 = 0.016663722693920135 + 0.1 * 6.300196170806885
Epoch 680, val loss: 0.9676551222801208
Epoch 690, training loss: 0.6452813148498535 = 0.015888109803199768 + 0.1 * 6.29393196105957
Epoch 690, val loss: 0.9745898246765137
Epoch 700, training loss: 0.6443862318992615 = 0.015166986733675003 + 0.1 * 6.292192459106445
Epoch 700, val loss: 0.9811615943908691
Epoch 710, training loss: 0.6439023613929749 = 0.014498072676360607 + 0.1 * 6.294043064117432
Epoch 710, val loss: 0.9877581596374512
Epoch 720, training loss: 0.6433722376823425 = 0.013873467221856117 + 0.1 * 6.294987678527832
Epoch 720, val loss: 0.9941362142562866
Epoch 730, training loss: 0.6410860419273376 = 0.013289961032569408 + 0.1 * 6.277960300445557
Epoch 730, val loss: 1.0002574920654297
Epoch 740, training loss: 0.6408398747444153 = 0.01274403277784586 + 0.1 * 6.280958652496338
Epoch 740, val loss: 1.0063073635101318
Epoch 750, training loss: 0.6406406164169312 = 0.012233109213411808 + 0.1 * 6.284074783325195
Epoch 750, val loss: 1.0122169256210327
Epoch 760, training loss: 0.6390376687049866 = 0.011754619888961315 + 0.1 * 6.272830009460449
Epoch 760, val loss: 1.0179404020309448
Epoch 770, training loss: 0.6381668448448181 = 0.011306140571832657 + 0.1 * 6.268606662750244
Epoch 770, val loss: 1.0236790180206299
Epoch 780, training loss: 0.637019157409668 = 0.010883364826440811 + 0.1 * 6.261357307434082
Epoch 780, val loss: 1.0290489196777344
Epoch 790, training loss: 0.6364045143127441 = 0.010486231185495853 + 0.1 * 6.259182453155518
Epoch 790, val loss: 1.034457802772522
Epoch 800, training loss: 0.6377310752868652 = 0.010111387819051743 + 0.1 * 6.276196479797363
Epoch 800, val loss: 1.0396111011505127
Epoch 810, training loss: 0.6361874341964722 = 0.009759639389812946 + 0.1 * 6.264277458190918
Epoch 810, val loss: 1.0446828603744507
Epoch 820, training loss: 0.6348215937614441 = 0.009427198208868504 + 0.1 * 6.253943920135498
Epoch 820, val loss: 1.0497640371322632
Epoch 830, training loss: 0.6336401104927063 = 0.00911250151693821 + 0.1 * 6.245275974273682
Epoch 830, val loss: 1.0547122955322266
Epoch 840, training loss: 0.637516975402832 = 0.008813800290226936 + 0.1 * 6.287031650543213
Epoch 840, val loss: 1.0595909357070923
Epoch 850, training loss: 0.6338500380516052 = 0.008530697785317898 + 0.1 * 6.253193378448486
Epoch 850, val loss: 1.064089059829712
Epoch 860, training loss: 0.6324397921562195 = 0.00826355628669262 + 0.1 * 6.241762638092041
Epoch 860, val loss: 1.0686993598937988
Epoch 870, training loss: 0.6322195529937744 = 0.008010529913008213 + 0.1 * 6.242090225219727
Epoch 870, val loss: 1.0732417106628418
Epoch 880, training loss: 0.6325579285621643 = 0.007769642397761345 + 0.1 * 6.247882843017578
Epoch 880, val loss: 1.0777106285095215
Epoch 890, training loss: 0.6306617259979248 = 0.007540279068052769 + 0.1 * 6.2312140464782715
Epoch 890, val loss: 1.0820575952529907
Epoch 900, training loss: 0.6309130787849426 = 0.007321212440729141 + 0.1 * 6.235918045043945
Epoch 900, val loss: 1.0862318277359009
Epoch 910, training loss: 0.6323121190071106 = 0.007113024592399597 + 0.1 * 6.251990795135498
Epoch 910, val loss: 1.090332269668579
Epoch 920, training loss: 0.6304147243499756 = 0.00691479817032814 + 0.1 * 6.234999179840088
Epoch 920, val loss: 1.0944006443023682
Epoch 930, training loss: 0.6293886303901672 = 0.00672628590837121 + 0.1 * 6.22662353515625
Epoch 930, val loss: 1.098447561264038
Epoch 940, training loss: 0.6297425031661987 = 0.0065455990843474865 + 0.1 * 6.231968879699707
Epoch 940, val loss: 1.1024353504180908
Epoch 950, training loss: 0.6299246549606323 = 0.006372686475515366 + 0.1 * 6.2355194091796875
Epoch 950, val loss: 1.1062239408493042
Epoch 960, training loss: 0.6280341148376465 = 0.006207158323377371 + 0.1 * 6.218269348144531
Epoch 960, val loss: 1.1099573373794556
Epoch 970, training loss: 0.6293222904205322 = 0.0060485913418233395 + 0.1 * 6.232736587524414
Epoch 970, val loss: 1.1137242317199707
Epoch 980, training loss: 0.6288235187530518 = 0.0058967010118067265 + 0.1 * 6.2292680740356445
Epoch 980, val loss: 1.1174392700195312
Epoch 990, training loss: 0.6278268098831177 = 0.005750856827944517 + 0.1 * 6.220759868621826
Epoch 990, val loss: 1.1208685636520386
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5461
Flip ASR: 0.4578/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7790794372558594 = 1.941691517829895 + 0.1 * 8.373880386352539
Epoch 0, val loss: 1.9458457231521606
Epoch 10, training loss: 2.769001007080078 = 1.9316270351409912 + 0.1 * 8.373738288879395
Epoch 10, val loss: 1.9350727796554565
Epoch 20, training loss: 2.757194757461548 = 1.9199142456054688 + 0.1 * 8.372804641723633
Epoch 20, val loss: 1.9221582412719727
Epoch 30, training loss: 2.7409019470214844 = 1.9042094945907593 + 0.1 * 8.366924285888672
Epoch 30, val loss: 1.9044820070266724
Epoch 40, training loss: 2.7151541709899902 = 1.8817087411880493 + 0.1 * 8.334455490112305
Epoch 40, val loss: 1.8790831565856934
Epoch 50, training loss: 2.6683452129364014 = 1.8504754304885864 + 0.1 * 8.17869758605957
Epoch 50, val loss: 1.8452359437942505
Epoch 60, training loss: 2.601950168609619 = 1.8132758140563965 + 0.1 * 7.886743545532227
Epoch 60, val loss: 1.8068275451660156
Epoch 70, training loss: 2.521782875061035 = 1.7740345001220703 + 0.1 * 7.477484703063965
Epoch 70, val loss: 1.7677981853485107
Epoch 80, training loss: 2.4450557231903076 = 1.7322406768798828 + 0.1 * 7.128150939941406
Epoch 80, val loss: 1.7274692058563232
Epoch 90, training loss: 2.3769733905792236 = 1.6824195384979248 + 0.1 * 6.945537567138672
Epoch 90, val loss: 1.6815263032913208
Epoch 100, training loss: 2.3012325763702393 = 1.6174917221069336 + 0.1 * 6.837409019470215
Epoch 100, val loss: 1.6245661973953247
Epoch 110, training loss: 2.2120273113250732 = 1.5339651107788086 + 0.1 * 6.780622482299805
Epoch 110, val loss: 1.5532100200653076
Epoch 120, training loss: 2.111959457397461 = 1.4368062019348145 + 0.1 * 6.75153112411499
Epoch 120, val loss: 1.4725233316421509
Epoch 130, training loss: 2.009737253189087 = 1.3359684944152832 + 0.1 * 6.737687587738037
Epoch 130, val loss: 1.3904266357421875
Epoch 140, training loss: 1.9114735126495361 = 1.238403081893921 + 0.1 * 6.7307047843933105
Epoch 140, val loss: 1.312815546989441
Epoch 150, training loss: 1.8179662227630615 = 1.1453133821487427 + 0.1 * 6.726529121398926
Epoch 150, val loss: 1.2404069900512695
Epoch 160, training loss: 1.7275478839874268 = 1.054890751838684 + 0.1 * 6.726572036743164
Epoch 160, val loss: 1.1694693565368652
Epoch 170, training loss: 1.6379886865615845 = 0.9657167792320251 + 0.1 * 6.722718715667725
Epoch 170, val loss: 1.0989203453063965
Epoch 180, training loss: 1.5496394634246826 = 0.8777420520782471 + 0.1 * 6.718973159790039
Epoch 180, val loss: 1.0284379720687866
Epoch 190, training loss: 1.46451997756958 = 0.7930630445480347 + 0.1 * 6.714568614959717
Epoch 190, val loss: 0.9608294367790222
Epoch 200, training loss: 1.3851866722106934 = 0.7144086956977844 + 0.1 * 6.707779407501221
Epoch 200, val loss: 0.8996513485908508
Epoch 210, training loss: 1.3139700889587402 = 0.6436759233474731 + 0.1 * 6.702942371368408
Epoch 210, val loss: 0.8482955098152161
Epoch 220, training loss: 1.2507472038269043 = 0.5816279053688049 + 0.1 * 6.691192626953125
Epoch 220, val loss: 0.8082085847854614
Epoch 230, training loss: 1.1946455240249634 = 0.5268270373344421 + 0.1 * 6.678184986114502
Epoch 230, val loss: 0.778222918510437
Epoch 240, training loss: 1.144085168838501 = 0.477676659822464 + 0.1 * 6.6640849113464355
Epoch 240, val loss: 0.7560842037200928
Epoch 250, training loss: 1.0980725288391113 = 0.43304312229156494 + 0.1 * 6.650294780731201
Epoch 250, val loss: 0.739059567451477
Epoch 260, training loss: 1.056373119354248 = 0.39219167828559875 + 0.1 * 6.641814708709717
Epoch 260, val loss: 0.7244890332221985
Epoch 270, training loss: 1.0168061256408691 = 0.35449784994125366 + 0.1 * 6.623082637786865
Epoch 270, val loss: 0.7106317281723022
Epoch 280, training loss: 0.980318009853363 = 0.3192518949508667 + 0.1 * 6.610661029815674
Epoch 280, val loss: 0.70013827085495
Epoch 290, training loss: 0.9464237689971924 = 0.2863910496234894 + 0.1 * 6.600327491760254
Epoch 290, val loss: 0.6936848759651184
Epoch 300, training loss: 0.9152742624282837 = 0.25638332962989807 + 0.1 * 6.588908672332764
Epoch 300, val loss: 0.6895577907562256
Epoch 310, training loss: 0.8868518471717834 = 0.2292788028717041 + 0.1 * 6.575730323791504
Epoch 310, val loss: 0.687994122505188
Epoch 320, training loss: 0.8622540235519409 = 0.20504212379455566 + 0.1 * 6.572118759155273
Epoch 320, val loss: 0.6889615058898926
Epoch 330, training loss: 0.839280903339386 = 0.18380720913410187 + 0.1 * 6.554737091064453
Epoch 330, val loss: 0.6924356818199158
Epoch 340, training loss: 0.8208819031715393 = 0.16537372767925262 + 0.1 * 6.555081367492676
Epoch 340, val loss: 0.6982670426368713
Epoch 350, training loss: 0.8031796216964722 = 0.14949695765972137 + 0.1 * 6.536826133728027
Epoch 350, val loss: 0.7057726383209229
Epoch 360, training loss: 0.7886463403701782 = 0.13574478030204773 + 0.1 * 6.529016017913818
Epoch 360, val loss: 0.7148497104644775
Epoch 370, training loss: 0.776319146156311 = 0.12374135851860046 + 0.1 * 6.525777339935303
Epoch 370, val loss: 0.7253161072731018
Epoch 380, training loss: 0.7669767737388611 = 0.11324523389339447 + 0.1 * 6.537315368652344
Epoch 380, val loss: 0.7365062832832336
Epoch 390, training loss: 0.7550958395004272 = 0.10405988991260529 + 0.1 * 6.510359287261963
Epoch 390, val loss: 0.7479294538497925
Epoch 400, training loss: 0.7458738684654236 = 0.09588427096605301 + 0.1 * 6.499896049499512
Epoch 400, val loss: 0.7597506642341614
Epoch 410, training loss: 0.7375991344451904 = 0.08854193985462189 + 0.1 * 6.490571975708008
Epoch 410, val loss: 0.7719146013259888
Epoch 420, training loss: 0.7322355508804321 = 0.0819106176495552 + 0.1 * 6.503249168395996
Epoch 420, val loss: 0.7841466665267944
Epoch 430, training loss: 0.7239163517951965 = 0.07594714313745499 + 0.1 * 6.479691982269287
Epoch 430, val loss: 0.7961722016334534
Epoch 440, training loss: 0.7185034155845642 = 0.07053110748529434 + 0.1 * 6.47972297668457
Epoch 440, val loss: 0.8081384897232056
Epoch 450, training loss: 0.7125101685523987 = 0.06559132784605026 + 0.1 * 6.469188213348389
Epoch 450, val loss: 0.8198704719543457
Epoch 460, training loss: 0.7075188755989075 = 0.06108219176530838 + 0.1 * 6.464366912841797
Epoch 460, val loss: 0.8313971757888794
Epoch 470, training loss: 0.7033140063285828 = 0.05694976821541786 + 0.1 * 6.463642120361328
Epoch 470, val loss: 0.8427575826644897
Epoch 480, training loss: 0.6983518004417419 = 0.053155552595853806 + 0.1 * 6.451962471008301
Epoch 480, val loss: 0.8536607623100281
Epoch 490, training loss: 0.6947131752967834 = 0.04965725913643837 + 0.1 * 6.450558662414551
Epoch 490, val loss: 0.864539623260498
Epoch 500, training loss: 0.6904942989349365 = 0.0464068278670311 + 0.1 * 6.4408745765686035
Epoch 500, val loss: 0.8752437829971313
Epoch 510, training loss: 0.6865718364715576 = 0.04333192855119705 + 0.1 * 6.432398796081543
Epoch 510, val loss: 0.885809600353241
Epoch 520, training loss: 0.6838076114654541 = 0.04045029357075691 + 0.1 * 6.433573246002197
Epoch 520, val loss: 0.8960224390029907
Epoch 530, training loss: 0.6802640557289124 = 0.03773359954357147 + 0.1 * 6.425304412841797
Epoch 530, val loss: 0.9062235355377197
Epoch 540, training loss: 0.6765369176864624 = 0.03515520691871643 + 0.1 * 6.413816928863525
Epoch 540, val loss: 0.9161210060119629
Epoch 550, training loss: 0.6741949915885925 = 0.032721105962991714 + 0.1 * 6.414738655090332
Epoch 550, val loss: 0.9261006116867065
Epoch 560, training loss: 0.6709927916526794 = 0.03046107292175293 + 0.1 * 6.405317306518555
Epoch 560, val loss: 0.9358687996864319
Epoch 570, training loss: 0.6683946847915649 = 0.02837751992046833 + 0.1 * 6.400171279907227
Epoch 570, val loss: 0.945676863193512
Epoch 580, training loss: 0.6653031706809998 = 0.02643752656877041 + 0.1 * 6.388656139373779
Epoch 580, val loss: 0.955330491065979
Epoch 590, training loss: 0.663455069065094 = 0.024627503007650375 + 0.1 * 6.388275623321533
Epoch 590, val loss: 0.9647876620292664
Epoch 600, training loss: 0.6609530448913574 = 0.022941648960113525 + 0.1 * 6.3801140785217285
Epoch 600, val loss: 0.9743179678916931
Epoch 610, training loss: 0.6583209037780762 = 0.021408280357718468 + 0.1 * 6.369126319885254
Epoch 610, val loss: 0.9831887483596802
Epoch 620, training loss: 0.6570371985435486 = 0.02003822661936283 + 0.1 * 6.369989395141602
Epoch 620, val loss: 0.9921213984489441
Epoch 630, training loss: 0.6551886200904846 = 0.01880752108991146 + 0.1 * 6.363811016082764
Epoch 630, val loss: 1.0007280111312866
Epoch 640, training loss: 0.6534239053726196 = 0.017711088061332703 + 0.1 * 6.357127666473389
Epoch 640, val loss: 1.008929967880249
Epoch 650, training loss: 0.6535597443580627 = 0.01672227680683136 + 0.1 * 6.368374824523926
Epoch 650, val loss: 1.0169260501861572
Epoch 660, training loss: 0.6514606475830078 = 0.01582280918955803 + 0.1 * 6.356378078460693
Epoch 660, val loss: 1.0244871377944946
Epoch 670, training loss: 0.6511086225509644 = 0.01499897614121437 + 0.1 * 6.361096382141113
Epoch 670, val loss: 1.031844139099121
Epoch 680, training loss: 0.648285984992981 = 0.014247129671275616 + 0.1 * 6.340388774871826
Epoch 680, val loss: 1.039042592048645
Epoch 690, training loss: 0.6475303173065186 = 0.013560686260461807 + 0.1 * 6.339695930480957
Epoch 690, val loss: 1.0458587408065796
Epoch 700, training loss: 0.6469152569770813 = 0.012927744537591934 + 0.1 * 6.339874744415283
Epoch 700, val loss: 1.0526741743087769
Epoch 710, training loss: 0.6455921530723572 = 0.012346932664513588 + 0.1 * 6.332452297210693
Epoch 710, val loss: 1.0591093301773071
Epoch 720, training loss: 0.644421398639679 = 0.011810638010501862 + 0.1 * 6.326107025146484
Epoch 720, val loss: 1.0653489828109741
Epoch 730, training loss: 0.6428728699684143 = 0.011312853544950485 + 0.1 * 6.315600395202637
Epoch 730, val loss: 1.0716074705123901
Epoch 740, training loss: 0.6434035301208496 = 0.010851147584617138 + 0.1 * 6.325523853302002
Epoch 740, val loss: 1.0775814056396484
Epoch 750, training loss: 0.6432021856307983 = 0.01041980367153883 + 0.1 * 6.327824115753174
Epoch 750, val loss: 1.0834896564483643
Epoch 760, training loss: 0.6408957839012146 = 0.010018016211688519 + 0.1 * 6.308777809143066
Epoch 760, val loss: 1.089262843132019
Epoch 770, training loss: 0.6404088735580444 = 0.009644011966884136 + 0.1 * 6.307648181915283
Epoch 770, val loss: 1.0947599411010742
Epoch 780, training loss: 0.6403672099113464 = 0.00929185189306736 + 0.1 * 6.31075382232666
Epoch 780, val loss: 1.1002545356750488
Epoch 790, training loss: 0.6403133869171143 = 0.008960280567407608 + 0.1 * 6.313530921936035
Epoch 790, val loss: 1.1056321859359741
Epoch 800, training loss: 0.6375828385353088 = 0.008650224655866623 + 0.1 * 6.289325714111328
Epoch 800, val loss: 1.1108686923980713
Epoch 810, training loss: 0.6380576491355896 = 0.00835754256695509 + 0.1 * 6.297000885009766
Epoch 810, val loss: 1.115989089012146
Epoch 820, training loss: 0.6374616622924805 = 0.00808170810341835 + 0.1 * 6.29379940032959
Epoch 820, val loss: 1.120979905128479
Epoch 830, training loss: 0.6363986134529114 = 0.007822183892130852 + 0.1 * 6.285764694213867
Epoch 830, val loss: 1.1258583068847656
Epoch 840, training loss: 0.6363080143928528 = 0.00757589703425765 + 0.1 * 6.287321090698242
Epoch 840, val loss: 1.1306575536727905
Epoch 850, training loss: 0.6349029541015625 = 0.007343055680394173 + 0.1 * 6.275599002838135
Epoch 850, val loss: 1.1353901624679565
Epoch 860, training loss: 0.6351318955421448 = 0.007122460752725601 + 0.1 * 6.280094146728516
Epoch 860, val loss: 1.1399503946304321
Epoch 870, training loss: 0.6347901821136475 = 0.006912733893841505 + 0.1 * 6.278774738311768
Epoch 870, val loss: 1.144486427307129
Epoch 880, training loss: 0.6336640119552612 = 0.006714430637657642 + 0.1 * 6.269495964050293
Epoch 880, val loss: 1.1489088535308838
Epoch 890, training loss: 0.6331977248191833 = 0.0065254932269454 + 0.1 * 6.266721725463867
Epoch 890, val loss: 1.1533139944076538
Epoch 900, training loss: 0.632893443107605 = 0.006346600130200386 + 0.1 * 6.265468597412109
Epoch 900, val loss: 1.1574896574020386
Epoch 910, training loss: 0.6346142292022705 = 0.006175663322210312 + 0.1 * 6.284385681152344
Epoch 910, val loss: 1.1617311239242554
Epoch 920, training loss: 0.6323028206825256 = 0.006011949852108955 + 0.1 * 6.262908458709717
Epoch 920, val loss: 1.1658294200897217
Epoch 930, training loss: 0.634898841381073 = 0.0058567407540977 + 0.1 * 6.2904205322265625
Epoch 930, val loss: 1.169912576675415
Epoch 940, training loss: 0.6309288144111633 = 0.005707998294383287 + 0.1 * 6.252208232879639
Epoch 940, val loss: 1.1738591194152832
Epoch 950, training loss: 0.6304332613945007 = 0.0055663916282355785 + 0.1 * 6.248668193817139
Epoch 950, val loss: 1.1777267456054688
Epoch 960, training loss: 0.6304758787155151 = 0.005430522374808788 + 0.1 * 6.250453472137451
Epoch 960, val loss: 1.181518316268921
Epoch 970, training loss: 0.6298368573188782 = 0.005299943033605814 + 0.1 * 6.245368957519531
Epoch 970, val loss: 1.1853137016296387
Epoch 980, training loss: 0.6316799521446228 = 0.005175574216991663 + 0.1 * 6.26504373550415
Epoch 980, val loss: 1.1890085935592651
Epoch 990, training loss: 0.6306290626525879 = 0.005055392161011696 + 0.1 * 6.255736827850342
Epoch 990, val loss: 1.1927270889282227
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8007
Flip ASR: 0.7600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7766273021698 = 1.9392421245574951 + 0.1 * 8.373851776123047
Epoch 0, val loss: 1.9380642175674438
Epoch 10, training loss: 2.7665021419525146 = 1.9291409254074097 + 0.1 * 8.373611450195312
Epoch 10, val loss: 1.9277836084365845
Epoch 20, training loss: 2.753448963165283 = 1.9162367582321167 + 0.1 * 8.372122764587402
Epoch 20, val loss: 1.914372444152832
Epoch 30, training loss: 2.734250545501709 = 1.8981086015701294 + 0.1 * 8.361420631408691
Epoch 30, val loss: 1.8952709436416626
Epoch 40, training loss: 2.7017691135406494 = 1.8720849752426147 + 0.1 * 8.296841621398926
Epoch 40, val loss: 1.8683136701583862
Epoch 50, training loss: 2.627527952194214 = 1.8385978937149048 + 0.1 * 7.8892998695373535
Epoch 50, val loss: 1.835612416267395
Epoch 60, training loss: 2.5485546588897705 = 1.8029704093933105 + 0.1 * 7.455842018127441
Epoch 60, val loss: 1.8017597198486328
Epoch 70, training loss: 2.4724316596984863 = 1.7653522491455078 + 0.1 * 7.070793628692627
Epoch 70, val loss: 1.7659149169921875
Epoch 80, training loss: 2.411219835281372 = 1.721472144126892 + 0.1 * 6.897476673126221
Epoch 80, val loss: 1.725398063659668
Epoch 90, training loss: 2.34722638130188 = 1.6665786504745483 + 0.1 * 6.8064775466918945
Epoch 90, val loss: 1.676098108291626
Epoch 100, training loss: 2.2701258659362793 = 1.595199465751648 + 0.1 * 6.749263763427734
Epoch 100, val loss: 1.6135624647140503
Epoch 110, training loss: 2.179006814956665 = 1.508130431175232 + 0.1 * 6.708763599395752
Epoch 110, val loss: 1.5401145219802856
Epoch 120, training loss: 2.0819344520568848 = 1.4133003950119019 + 0.1 * 6.68634033203125
Epoch 120, val loss: 1.463469386100769
Epoch 130, training loss: 1.9860010147094727 = 1.318742275238037 + 0.1 * 6.672587871551514
Epoch 130, val loss: 1.3917425870895386
Epoch 140, training loss: 1.8933753967285156 = 1.2273398637771606 + 0.1 * 6.6603546142578125
Epoch 140, val loss: 1.326540470123291
Epoch 150, training loss: 1.8044712543487549 = 1.1391668319702148 + 0.1 * 6.653044700622559
Epoch 150, val loss: 1.2660374641418457
Epoch 160, training loss: 1.718940019607544 = 1.0552446842193604 + 0.1 * 6.636953353881836
Epoch 160, val loss: 1.2091782093048096
Epoch 170, training loss: 1.6364760398864746 = 0.9741416573524475 + 0.1 * 6.623343467712402
Epoch 170, val loss: 1.154213309288025
Epoch 180, training loss: 1.5572175979614258 = 0.8963297605514526 + 0.1 * 6.608878135681152
Epoch 180, val loss: 1.1014102697372437
Epoch 190, training loss: 1.4809329509735107 = 0.8216823935508728 + 0.1 * 6.592504978179932
Epoch 190, val loss: 1.0505614280700684
Epoch 200, training loss: 1.40867280960083 = 0.7507820725440979 + 0.1 * 6.5789079666137695
Epoch 200, val loss: 1.0017797946929932
Epoch 210, training loss: 1.3440885543823242 = 0.6863003969192505 + 0.1 * 6.577881336212158
Epoch 210, val loss: 0.9578534364700317
Epoch 220, training loss: 1.28402841091156 = 0.6286472678184509 + 0.1 * 6.553811073303223
Epoch 220, val loss: 0.919427752494812
Epoch 230, training loss: 1.2308459281921387 = 0.5767106413841248 + 0.1 * 6.541353225708008
Epoch 230, val loss: 0.8861968517303467
Epoch 240, training loss: 1.1823493242263794 = 0.5296032428741455 + 0.1 * 6.52746057510376
Epoch 240, val loss: 0.8577090501785278
Epoch 250, training loss: 1.13908052444458 = 0.4864823818206787 + 0.1 * 6.5259809494018555
Epoch 250, val loss: 0.8336371183395386
Epoch 260, training loss: 1.0968314409255981 = 0.44644173979759216 + 0.1 * 6.503896713256836
Epoch 260, val loss: 0.8130565881729126
Epoch 270, training loss: 1.0586268901824951 = 0.40850013494491577 + 0.1 * 6.501267910003662
Epoch 270, val loss: 0.7952814102172852
Epoch 280, training loss: 1.021308422088623 = 0.37248849868774414 + 0.1 * 6.488199234008789
Epoch 280, val loss: 0.780385434627533
Epoch 290, training loss: 0.9850339889526367 = 0.3376937210559845 + 0.1 * 6.47340202331543
Epoch 290, val loss: 0.7678856253623962
Epoch 300, training loss: 0.951579213142395 = 0.30413931608200073 + 0.1 * 6.474398612976074
Epoch 300, val loss: 0.7579590678215027
Epoch 310, training loss: 0.9189156293869019 = 0.27274784445762634 + 0.1 * 6.4616780281066895
Epoch 310, val loss: 0.7507967352867126
Epoch 320, training loss: 0.8890783786773682 = 0.24397101998329163 + 0.1 * 6.45107364654541
Epoch 320, val loss: 0.746650755405426
Epoch 330, training loss: 0.8635101914405823 = 0.2182742804288864 + 0.1 * 6.452358722686768
Epoch 330, val loss: 0.7454972863197327
Epoch 340, training loss: 0.8387730121612549 = 0.19564244151115417 + 0.1 * 6.431305408477783
Epoch 340, val loss: 0.7468788027763367
Epoch 350, training loss: 0.817732036113739 = 0.17574308812618256 + 0.1 * 6.419889450073242
Epoch 350, val loss: 0.7504240274429321
Epoch 360, training loss: 0.8010839819908142 = 0.15830755233764648 + 0.1 * 6.427763938903809
Epoch 360, val loss: 0.7556442022323608
Epoch 370, training loss: 0.7840357422828674 = 0.1430673599243164 + 0.1 * 6.409683704376221
Epoch 370, val loss: 0.7621515989303589
Epoch 380, training loss: 0.7703264951705933 = 0.12963056564331055 + 0.1 * 6.406959056854248
Epoch 380, val loss: 0.7695571184158325
Epoch 390, training loss: 0.7572777271270752 = 0.11776617169380188 + 0.1 * 6.395115375518799
Epoch 390, val loss: 0.777912974357605
Epoch 400, training loss: 0.7466927766799927 = 0.1073109433054924 + 0.1 * 6.393818378448486
Epoch 400, val loss: 0.7865595817565918
Epoch 410, training loss: 0.7358407378196716 = 0.09808908402919769 + 0.1 * 6.377516269683838
Epoch 410, val loss: 0.7957311868667603
Epoch 420, training loss: 0.7268818616867065 = 0.0898858979344368 + 0.1 * 6.369959831237793
Epoch 420, val loss: 0.8051882386207581
Epoch 430, training loss: 0.7203704118728638 = 0.08259761333465576 + 0.1 * 6.37772798538208
Epoch 430, val loss: 0.8145818114280701
Epoch 440, training loss: 0.7115023136138916 = 0.07611331343650818 + 0.1 * 6.353889465332031
Epoch 440, val loss: 0.8242298364639282
Epoch 450, training loss: 0.7050389051437378 = 0.07028594613075256 + 0.1 * 6.34752893447876
Epoch 450, val loss: 0.8338344097137451
Epoch 460, training loss: 0.7071372866630554 = 0.06503534317016602 + 0.1 * 6.421019077301025
Epoch 460, val loss: 0.84334397315979
Epoch 470, training loss: 0.6952858567237854 = 0.06034241244196892 + 0.1 * 6.3494343757629395
Epoch 470, val loss: 0.852803647518158
Epoch 480, training loss: 0.6891549825668335 = 0.05610319972038269 + 0.1 * 6.330517292022705
Epoch 480, val loss: 0.8621727228164673
Epoch 490, training loss: 0.6848695874214172 = 0.052246931940317154 + 0.1 * 6.326226234436035
Epoch 490, val loss: 0.8714751601219177
Epoch 500, training loss: 0.681035578250885 = 0.04874129965901375 + 0.1 * 6.322942733764648
Epoch 500, val loss: 0.8806115984916687
Epoch 510, training loss: 0.677638053894043 = 0.04555463418364525 + 0.1 * 6.320834159851074
Epoch 510, val loss: 0.8898110389709473
Epoch 520, training loss: 0.6755074858665466 = 0.04264353960752487 + 0.1 * 6.328639030456543
Epoch 520, val loss: 0.8987974524497986
Epoch 530, training loss: 0.6707305312156677 = 0.03998466953635216 + 0.1 * 6.307458877563477
Epoch 530, val loss: 0.9076529741287231
Epoch 540, training loss: 0.6686444282531738 = 0.03754749894142151 + 0.1 * 6.310969352722168
Epoch 540, val loss: 0.9163661003112793
Epoch 550, training loss: 0.6661823391914368 = 0.03531254455447197 + 0.1 * 6.3086981773376465
Epoch 550, val loss: 0.9251236915588379
Epoch 560, training loss: 0.6642924547195435 = 0.03326011821627617 + 0.1 * 6.310323238372803
Epoch 560, val loss: 0.9334917068481445
Epoch 570, training loss: 0.6613808870315552 = 0.031376395374536514 + 0.1 * 6.300044536590576
Epoch 570, val loss: 0.9418986439704895
Epoch 580, training loss: 0.6584132313728333 = 0.02963878959417343 + 0.1 * 6.287744522094727
Epoch 580, val loss: 0.9501477479934692
Epoch 590, training loss: 0.6606459021568298 = 0.028031280264258385 + 0.1 * 6.326146125793457
Epoch 590, val loss: 0.9581468105316162
Epoch 600, training loss: 0.6555275321006775 = 0.026552243158221245 + 0.1 * 6.28975248336792
Epoch 600, val loss: 0.9660454392433167
Epoch 610, training loss: 0.6528509855270386 = 0.025187674909830093 + 0.1 * 6.276633262634277
Epoch 610, val loss: 0.9738845229148865
Epoch 620, training loss: 0.6528987884521484 = 0.023919904604554176 + 0.1 * 6.289789199829102
Epoch 620, val loss: 0.9815002679824829
Epoch 630, training loss: 0.6500310897827148 = 0.022743502631783485 + 0.1 * 6.2728753089904785
Epoch 630, val loss: 0.9890648722648621
Epoch 640, training loss: 0.6489092707633972 = 0.02165030688047409 + 0.1 * 6.272589206695557
Epoch 640, val loss: 0.9964616298675537
Epoch 650, training loss: 0.6476562023162842 = 0.020632822066545486 + 0.1 * 6.270234107971191
Epoch 650, val loss: 1.0037124156951904
Epoch 660, training loss: 0.6456769704818726 = 0.019685037434101105 + 0.1 * 6.259919166564941
Epoch 660, val loss: 1.0108425617218018
Epoch 670, training loss: 0.6471055150032043 = 0.018799908459186554 + 0.1 * 6.283056259155273
Epoch 670, val loss: 1.0178337097167969
Epoch 680, training loss: 0.6442196369171143 = 0.017975151538848877 + 0.1 * 6.262444972991943
Epoch 680, val loss: 1.0246047973632812
Epoch 690, training loss: 0.6439812183380127 = 0.017202893272042274 + 0.1 * 6.267782688140869
Epoch 690, val loss: 1.0313950777053833
Epoch 700, training loss: 0.6418760418891907 = 0.01647929474711418 + 0.1 * 6.25396728515625
Epoch 700, val loss: 1.037940263748169
Epoch 710, training loss: 0.6411880254745483 = 0.015801385045051575 + 0.1 * 6.253866195678711
Epoch 710, val loss: 1.0444356203079224
Epoch 720, training loss: 0.6400924324989319 = 0.015165097080171108 + 0.1 * 6.249273300170898
Epoch 720, val loss: 1.0508280992507935
Epoch 730, training loss: 0.6404819488525391 = 0.014566825702786446 + 0.1 * 6.259150981903076
Epoch 730, val loss: 1.0571290254592896
Epoch 740, training loss: 0.6393009424209595 = 0.014005317352712154 + 0.1 * 6.252956390380859
Epoch 740, val loss: 1.0631012916564941
Epoch 750, training loss: 0.6383285522460938 = 0.013477387838065624 + 0.1 * 6.24851131439209
Epoch 750, val loss: 1.069229006767273
Epoch 760, training loss: 0.6361939907073975 = 0.012979581020772457 + 0.1 * 6.232144355773926
Epoch 760, val loss: 1.0750868320465088
Epoch 770, training loss: 0.6360437273979187 = 0.012509815394878387 + 0.1 * 6.2353386878967285
Epoch 770, val loss: 1.0810155868530273
Epoch 780, training loss: 0.634857714176178 = 0.01206593494862318 + 0.1 * 6.227917671203613
Epoch 780, val loss: 1.086669921875
Epoch 790, training loss: 0.6356856226921082 = 0.011646165512502193 + 0.1 * 6.240394592285156
Epoch 790, val loss: 1.0922868251800537
Epoch 800, training loss: 0.635031521320343 = 0.011248934082686901 + 0.1 * 6.237825393676758
Epoch 800, val loss: 1.097851276397705
Epoch 810, training loss: 0.6338982582092285 = 0.010873167775571346 + 0.1 * 6.230251312255859
Epoch 810, val loss: 1.1032286882400513
Epoch 820, training loss: 0.6326457858085632 = 0.010517110116779804 + 0.1 * 6.221286773681641
Epoch 820, val loss: 1.1086344718933105
Epoch 830, training loss: 0.6327129602432251 = 0.01017825398594141 + 0.1 * 6.22534704208374
Epoch 830, val loss: 1.1138890981674194
Epoch 840, training loss: 0.6314152479171753 = 0.009856438264250755 + 0.1 * 6.215587615966797
Epoch 840, val loss: 1.119001865386963
Epoch 850, training loss: 0.6321494579315186 = 0.0095510957762599 + 0.1 * 6.225983619689941
Epoch 850, val loss: 1.124058723449707
Epoch 860, training loss: 0.6312083005905151 = 0.009260507300496101 + 0.1 * 6.219477653503418
Epoch 860, val loss: 1.1292288303375244
Epoch 870, training loss: 0.6295956373214722 = 0.008983968757092953 + 0.1 * 6.206116676330566
Epoch 870, val loss: 1.1341472864151
Epoch 880, training loss: 0.6305027604103088 = 0.008720263838768005 + 0.1 * 6.217824459075928
Epoch 880, val loss: 1.1389498710632324
Epoch 890, training loss: 0.6294149160385132 = 0.008468958549201488 + 0.1 * 6.20945930480957
Epoch 890, val loss: 1.1437791585922241
Epoch 900, training loss: 0.6289529204368591 = 0.008228914812207222 + 0.1 * 6.207240104675293
Epoch 900, val loss: 1.1485551595687866
Epoch 910, training loss: 0.6281428933143616 = 0.007999567314982414 + 0.1 * 6.201433181762695
Epoch 910, val loss: 1.153035044670105
Epoch 920, training loss: 0.6281397342681885 = 0.007780978921800852 + 0.1 * 6.203587532043457
Epoch 920, val loss: 1.1576439142227173
Epoch 930, training loss: 0.6267367601394653 = 0.007571791764348745 + 0.1 * 6.191649436950684
Epoch 930, val loss: 1.1622554063796997
Epoch 940, training loss: 0.628370463848114 = 0.007371085230261087 + 0.1 * 6.209993839263916
Epoch 940, val loss: 1.1667226552963257
Epoch 950, training loss: 0.6272863745689392 = 0.007179188076406717 + 0.1 * 6.201071739196777
Epoch 950, val loss: 1.1709622144699097
Epoch 960, training loss: 0.626125693321228 = 0.0069961645640432835 + 0.1 * 6.191295146942139
Epoch 960, val loss: 1.1753557920455933
Epoch 970, training loss: 0.6261954307556152 = 0.006820362061262131 + 0.1 * 6.193750381469727
Epoch 970, val loss: 1.1797155141830444
Epoch 980, training loss: 0.6269335150718689 = 0.006651717238128185 + 0.1 * 6.202817916870117
Epoch 980, val loss: 1.1839126348495483
Epoch 990, training loss: 0.6253899335861206 = 0.006489601451903582 + 0.1 * 6.1890034675598145
Epoch 990, val loss: 1.1879571676254272
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.8376
Flip ASR: 0.8044/225 nodes
The final ASR:0.72817, 0.12960, Accuracy:0.80741, 0.01839
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9408])
updated graph: torch.Size([2, 10438])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98401, 0.00627, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7927277088165283 = 1.9553430080413818 + 0.1 * 8.373846054077148
Epoch 0, val loss: 1.9502346515655518
Epoch 10, training loss: 2.7818689346313477 = 1.944498896598816 + 0.1 * 8.373700141906738
Epoch 10, val loss: 1.93930184841156
Epoch 20, training loss: 2.768202066421509 = 1.9309173822402954 + 0.1 * 8.372846603393555
Epoch 20, val loss: 1.9253191947937012
Epoch 30, training loss: 2.747959613800049 = 1.911301612854004 + 0.1 * 8.36658000946045
Epoch 30, val loss: 1.9052784442901611
Epoch 40, training loss: 2.714604377746582 = 1.881835699081421 + 0.1 * 8.327686309814453
Epoch 40, val loss: 1.8760122060775757
Epoch 50, training loss: 2.6500301361083984 = 1.8420921564102173 + 0.1 * 8.07938003540039
Epoch 50, val loss: 1.8392544984817505
Epoch 60, training loss: 2.58307147026062 = 1.7999119758605957 + 0.1 * 7.831594944000244
Epoch 60, val loss: 1.8039212226867676
Epoch 70, training loss: 2.50455641746521 = 1.762197494506836 + 0.1 * 7.42358922958374
Epoch 70, val loss: 1.7741601467132568
Epoch 80, training loss: 2.4277610778808594 = 1.7221462726593018 + 0.1 * 7.056149005889893
Epoch 80, val loss: 1.7408583164215088
Epoch 90, training loss: 2.3555312156677246 = 1.6705576181411743 + 0.1 * 6.849734783172607
Epoch 90, val loss: 1.6971750259399414
Epoch 100, training loss: 2.2774741649627686 = 1.601628065109253 + 0.1 * 6.75846004486084
Epoch 100, val loss: 1.639479637145996
Epoch 110, training loss: 2.1880710124969482 = 1.5177347660064697 + 0.1 * 6.703361511230469
Epoch 110, val loss: 1.5693304538726807
Epoch 120, training loss: 2.0942556858062744 = 1.4266788959503174 + 0.1 * 6.675767421722412
Epoch 120, val loss: 1.4949904680252075
Epoch 130, training loss: 2.0012741088867188 = 1.335422396659851 + 0.1 * 6.658518314361572
Epoch 130, val loss: 1.4225897789001465
Epoch 140, training loss: 1.910982370376587 = 1.2466988563537598 + 0.1 * 6.642834663391113
Epoch 140, val loss: 1.3549479246139526
Epoch 150, training loss: 1.8248908519744873 = 1.161668062210083 + 0.1 * 6.632226943969727
Epoch 150, val loss: 1.2923083305358887
Epoch 160, training loss: 1.741586685180664 = 1.0798418521881104 + 0.1 * 6.617448806762695
Epoch 160, val loss: 1.2332828044891357
Epoch 170, training loss: 1.6593592166900635 = 0.9982717037200928 + 0.1 * 6.610875129699707
Epoch 170, val loss: 1.1750761270523071
Epoch 180, training loss: 1.5768215656280518 = 0.9171717166900635 + 0.1 * 6.596498012542725
Epoch 180, val loss: 1.1169239282608032
Epoch 190, training loss: 1.4957572221755981 = 0.8371719717979431 + 0.1 * 6.58585262298584
Epoch 190, val loss: 1.059325098991394
Epoch 200, training loss: 1.4186596870422363 = 0.7601735591888428 + 0.1 * 6.584860324859619
Epoch 200, val loss: 1.0043952465057373
Epoch 210, training loss: 1.3459854125976562 = 0.689077615737915 + 0.1 * 6.569077014923096
Epoch 210, val loss: 0.9553653001785278
Epoch 220, training loss: 1.2793152332305908 = 0.6243284940719604 + 0.1 * 6.549866676330566
Epoch 220, val loss: 0.912827730178833
Epoch 230, training loss: 1.2205755710601807 = 0.5663935542106628 + 0.1 * 6.5418195724487305
Epoch 230, val loss: 0.8778413534164429
Epoch 240, training loss: 1.1685128211975098 = 0.5156849026679993 + 0.1 * 6.528278350830078
Epoch 240, val loss: 0.850554347038269
Epoch 250, training loss: 1.121310830116272 = 0.47047367691993713 + 0.1 * 6.508370876312256
Epoch 250, val loss: 0.8293558359146118
Epoch 260, training loss: 1.082207202911377 = 0.429625928401947 + 0.1 * 6.525812149047852
Epoch 260, val loss: 0.812988817691803
Epoch 270, training loss: 1.0411169528961182 = 0.39275312423706055 + 0.1 * 6.48363733291626
Epoch 270, val loss: 0.8007674813270569
Epoch 280, training loss: 1.005691647529602 = 0.3587675392627716 + 0.1 * 6.469241142272949
Epoch 280, val loss: 0.791804313659668
Epoch 290, training loss: 0.973177433013916 = 0.32728615403175354 + 0.1 * 6.4589128494262695
Epoch 290, val loss: 0.7857028245925903
Epoch 300, training loss: 0.9434319734573364 = 0.29842516779899597 + 0.1 * 6.450068473815918
Epoch 300, val loss: 0.7823891043663025
Epoch 310, training loss: 0.9153389930725098 = 0.27173906564712524 + 0.1 * 6.435999393463135
Epoch 310, val loss: 0.7813593149185181
Epoch 320, training loss: 0.8913087248802185 = 0.24710118770599365 + 0.1 * 6.442075252532959
Epoch 320, val loss: 0.78257817029953
Epoch 330, training loss: 0.8666958808898926 = 0.22467927634716034 + 0.1 * 6.420165538787842
Epoch 330, val loss: 0.7858743071556091
Epoch 340, training loss: 0.8453354835510254 = 0.20425409078598022 + 0.1 * 6.410813808441162
Epoch 340, val loss: 0.7911084294319153
Epoch 350, training loss: 0.8265901207923889 = 0.18576185405254364 + 0.1 * 6.408282279968262
Epoch 350, val loss: 0.7979963421821594
Epoch 360, training loss: 0.8084877729415894 = 0.169046550989151 + 0.1 * 6.394412517547607
Epoch 360, val loss: 0.8063989877700806
Epoch 370, training loss: 0.7921305298805237 = 0.1538320779800415 + 0.1 * 6.382984638214111
Epoch 370, val loss: 0.8160802721977234
Epoch 380, training loss: 0.7787439823150635 = 0.14001095294952393 + 0.1 * 6.387330055236816
Epoch 380, val loss: 0.8267393708229065
Epoch 390, training loss: 0.7649551630020142 = 0.12752363085746765 + 0.1 * 6.3743157386779785
Epoch 390, val loss: 0.8382571935653687
Epoch 400, training loss: 0.7533692717552185 = 0.1161932721734047 + 0.1 * 6.37175989151001
Epoch 400, val loss: 0.8502848148345947
Epoch 410, training loss: 0.7423600554466248 = 0.10594525188207626 + 0.1 * 6.364148139953613
Epoch 410, val loss: 0.862709641456604
Epoch 420, training loss: 0.7315983772277832 = 0.09666045755147934 + 0.1 * 6.349379539489746
Epoch 420, val loss: 0.8754703402519226
Epoch 430, training loss: 0.7233564257621765 = 0.08825262635946274 + 0.1 * 6.351037979125977
Epoch 430, val loss: 0.8885025382041931
Epoch 440, training loss: 0.7155776023864746 = 0.0806930884718895 + 0.1 * 6.348845481872559
Epoch 440, val loss: 0.9014661312103271
Epoch 450, training loss: 0.7077090740203857 = 0.07392627745866776 + 0.1 * 6.337828159332275
Epoch 450, val loss: 0.9145249128341675
Epoch 460, training loss: 0.700537919998169 = 0.06782831251621246 + 0.1 * 6.327095985412598
Epoch 460, val loss: 0.9275475144386292
Epoch 470, training loss: 0.6943530440330505 = 0.06232163682579994 + 0.1 * 6.320313453674316
Epoch 470, val loss: 0.9406324625015259
Epoch 480, training loss: 0.6903584599494934 = 0.057353392243385315 + 0.1 * 6.330050468444824
Epoch 480, val loss: 0.9535252451896667
Epoch 490, training loss: 0.6851894855499268 = 0.05289628729224205 + 0.1 * 6.322932243347168
Epoch 490, val loss: 0.9663160443305969
Epoch 500, training loss: 0.6801446080207825 = 0.04888198524713516 + 0.1 * 6.312625885009766
Epoch 500, val loss: 0.978828489780426
Epoch 510, training loss: 0.6757673025131226 = 0.045263975858688354 + 0.1 * 6.305032730102539
Epoch 510, val loss: 0.9912539720535278
Epoch 520, training loss: 0.6730536222457886 = 0.04199625179171562 + 0.1 * 6.310573577880859
Epoch 520, val loss: 1.0033621788024902
Epoch 530, training loss: 0.6690436601638794 = 0.03904861584305763 + 0.1 * 6.299950122833252
Epoch 530, val loss: 1.0153124332427979
Epoch 540, training loss: 0.6660216450691223 = 0.036382436752319336 + 0.1 * 6.29639196395874
Epoch 540, val loss: 1.0269912481307983
Epoch 550, training loss: 0.6629944443702698 = 0.03396957740187645 + 0.1 * 6.290248394012451
Epoch 550, val loss: 1.0383843183517456
Epoch 560, training loss: 0.6603153347969055 = 0.031780585646629333 + 0.1 * 6.2853474617004395
Epoch 560, val loss: 1.0496034622192383
Epoch 570, training loss: 0.659476101398468 = 0.029787905514240265 + 0.1 * 6.296882152557373
Epoch 570, val loss: 1.0605394840240479
Epoch 580, training loss: 0.6568953990936279 = 0.02797861397266388 + 0.1 * 6.289167881011963
Epoch 580, val loss: 1.071161150932312
Epoch 590, training loss: 0.6537250280380249 = 0.026332329958677292 + 0.1 * 6.273926734924316
Epoch 590, val loss: 1.0816577672958374
Epoch 600, training loss: 0.6536964774131775 = 0.024822937324643135 + 0.1 * 6.288735389709473
Epoch 600, val loss: 1.0918067693710327
Epoch 610, training loss: 0.6509435176849365 = 0.023441899567842484 + 0.1 * 6.275015830993652
Epoch 610, val loss: 1.1016623973846436
Epoch 620, training loss: 0.6486028432846069 = 0.022175023332238197 + 0.1 * 6.264278411865234
Epoch 620, val loss: 1.111451268196106
Epoch 630, training loss: 0.647342324256897 = 0.021007627248764038 + 0.1 * 6.2633466720581055
Epoch 630, val loss: 1.1209371089935303
Epoch 640, training loss: 0.6453886032104492 = 0.01993083395063877 + 0.1 * 6.25457763671875
Epoch 640, val loss: 1.1300989389419556
Epoch 650, training loss: 0.6461150646209717 = 0.018937885761260986 + 0.1 * 6.2717719078063965
Epoch 650, val loss: 1.139203429222107
Epoch 660, training loss: 0.6430318355560303 = 0.018019624054431915 + 0.1 * 6.2501220703125
Epoch 660, val loss: 1.1479682922363281
Epoch 670, training loss: 0.6421536803245544 = 0.017169315367937088 + 0.1 * 6.249843597412109
Epoch 670, val loss: 1.1566563844680786
Epoch 680, training loss: 0.6406461596488953 = 0.01637808047235012 + 0.1 * 6.242680549621582
Epoch 680, val loss: 1.1650104522705078
Epoch 690, training loss: 0.6402161121368408 = 0.015642359852790833 + 0.1 * 6.245737552642822
Epoch 690, val loss: 1.1732275485992432
Epoch 700, training loss: 0.6396911144256592 = 0.014957218430936337 + 0.1 * 6.247339248657227
Epoch 700, val loss: 1.1813215017318726
Epoch 710, training loss: 0.6387828588485718 = 0.014318656176328659 + 0.1 * 6.2446417808532715
Epoch 710, val loss: 1.188959002494812
Epoch 720, training loss: 0.6378610730171204 = 0.013725376687943935 + 0.1 * 6.241356372833252
Epoch 720, val loss: 1.1966590881347656
Epoch 730, training loss: 0.6359618902206421 = 0.013169258832931519 + 0.1 * 6.227925777435303
Epoch 730, val loss: 1.2041593790054321
Epoch 740, training loss: 0.6353170275688171 = 0.01264718733727932 + 0.1 * 6.22669792175293
Epoch 740, val loss: 1.211455225944519
Epoch 750, training loss: 0.6369495987892151 = 0.012156213633716106 + 0.1 * 6.247933864593506
Epoch 750, val loss: 1.2185066938400269
Epoch 760, training loss: 0.6341950297355652 = 0.01169667486101389 + 0.1 * 6.224983215332031
Epoch 760, val loss: 1.2254650592803955
Epoch 770, training loss: 0.6342359185218811 = 0.011265555396676064 + 0.1 * 6.229703426361084
Epoch 770, val loss: 1.232378363609314
Epoch 780, training loss: 0.632405698299408 = 0.010859284549951553 + 0.1 * 6.215463638305664
Epoch 780, val loss: 1.23896324634552
Epoch 790, training loss: 0.6339366436004639 = 0.010476425290107727 + 0.1 * 6.234602451324463
Epoch 790, val loss: 1.2455389499664307
Epoch 800, training loss: 0.6316621899604797 = 0.010114072822034359 + 0.1 * 6.215480804443359
Epoch 800, val loss: 1.2519129514694214
Epoch 810, training loss: 0.6335452198982239 = 0.009772186167538166 + 0.1 * 6.237730026245117
Epoch 810, val loss: 1.2581146955490112
Epoch 820, training loss: 0.6306170225143433 = 0.009449035860598087 + 0.1 * 6.211679458618164
Epoch 820, val loss: 1.2642353773117065
Epoch 830, training loss: 0.6304677724838257 = 0.009143238887190819 + 0.1 * 6.213245391845703
Epoch 830, val loss: 1.2703179121017456
Epoch 840, training loss: 0.6298685073852539 = 0.008853062987327576 + 0.1 * 6.210154056549072
Epoch 840, val loss: 1.2760515213012695
Epoch 850, training loss: 0.6291812062263489 = 0.008578414097428322 + 0.1 * 6.206027507781982
Epoch 850, val loss: 1.28187096118927
Epoch 860, training loss: 0.628388524055481 = 0.008317191153764725 + 0.1 * 6.200713157653809
Epoch 860, val loss: 1.2874830961227417
Epoch 870, training loss: 0.6286553740501404 = 0.008068960160017014 + 0.1 * 6.205863952636719
Epoch 870, val loss: 1.2929697036743164
Epoch 880, training loss: 0.6274433135986328 = 0.007832594215869904 + 0.1 * 6.196107387542725
Epoch 880, val loss: 1.2983366250991821
Epoch 890, training loss: 0.6275469064712524 = 0.007608017884194851 + 0.1 * 6.1993889808654785
Epoch 890, val loss: 1.303758978843689
Epoch 900, training loss: 0.6284492015838623 = 0.0073930807411670685 + 0.1 * 6.210561275482178
Epoch 900, val loss: 1.3088738918304443
Epoch 910, training loss: 0.6264172196388245 = 0.0071887909434735775 + 0.1 * 6.192284107208252
Epoch 910, val loss: 1.3140044212341309
Epoch 920, training loss: 0.6264697909355164 = 0.006993631366640329 + 0.1 * 6.194761276245117
Epoch 920, val loss: 1.3190653324127197
Epoch 930, training loss: 0.6261033415794373 = 0.006807229015976191 + 0.1 * 6.1929612159729
Epoch 930, val loss: 1.323927402496338
Epoch 940, training loss: 0.6248770356178284 = 0.006628720089793205 + 0.1 * 6.182483196258545
Epoch 940, val loss: 1.3287547826766968
Epoch 950, training loss: 0.6260752081871033 = 0.006458288989961147 + 0.1 * 6.196169376373291
Epoch 950, val loss: 1.3335577249526978
Epoch 960, training loss: 0.6252034306526184 = 0.0062946355901658535 + 0.1 * 6.189087867736816
Epoch 960, val loss: 1.3381108045578003
Epoch 970, training loss: 0.6249529719352722 = 0.006138342432677746 + 0.1 * 6.188146591186523
Epoch 970, val loss: 1.3427541255950928
Epoch 980, training loss: 0.6250066757202148 = 0.005988209508359432 + 0.1 * 6.190184593200684
Epoch 980, val loss: 1.3472635746002197
Epoch 990, training loss: 0.6229718327522278 = 0.005844540428370237 + 0.1 * 6.1712727546691895
Epoch 990, val loss: 1.351658821105957
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5351
Flip ASR: 0.4489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7782201766967773 = 1.940830945968628 + 0.1 * 8.37389087677002
Epoch 0, val loss: 1.9479562044143677
Epoch 10, training loss: 2.7685587406158447 = 1.9311776161193848 + 0.1 * 8.373811721801758
Epoch 10, val loss: 1.9379057884216309
Epoch 20, training loss: 2.7568464279174805 = 1.9195033311843872 + 0.1 * 8.373431205749512
Epoch 20, val loss: 1.9256964921951294
Epoch 30, training loss: 2.74045467376709 = 1.903334617614746 + 0.1 * 8.371201515197754
Epoch 30, val loss: 1.9090114831924438
Epoch 40, training loss: 2.7151896953582764 = 1.8798065185546875 + 0.1 * 8.35383129119873
Epoch 40, val loss: 1.8850781917572021
Epoch 50, training loss: 2.668381690979004 = 1.8463252782821655 + 0.1 * 8.220564842224121
Epoch 50, val loss: 1.8521710634231567
Epoch 60, training loss: 2.56144642829895 = 1.805916428565979 + 0.1 * 7.555300235748291
Epoch 60, val loss: 1.814270257949829
Epoch 70, training loss: 2.4872026443481445 = 1.765325903892517 + 0.1 * 7.218766212463379
Epoch 70, val loss: 1.7753490209579468
Epoch 80, training loss: 2.4211578369140625 = 1.7187575101852417 + 0.1 * 7.024003505706787
Epoch 80, val loss: 1.7308976650238037
Epoch 90, training loss: 2.351261615753174 = 1.6592190265655518 + 0.1 * 6.920424461364746
Epoch 90, val loss: 1.6752841472625732
Epoch 100, training loss: 2.2649247646331787 = 1.5802854299545288 + 0.1 * 6.846392631530762
Epoch 100, val loss: 1.6043678522109985
Epoch 110, training loss: 2.16001033782959 = 1.479823350906372 + 0.1 * 6.801870822906494
Epoch 110, val loss: 1.5180150270462036
Epoch 120, training loss: 2.0416550636291504 = 1.3637359142303467 + 0.1 * 6.779190540313721
Epoch 120, val loss: 1.42133629322052
Epoch 130, training loss: 1.9213502407073975 = 1.2447973489761353 + 0.1 * 6.765529155731201
Epoch 130, val loss: 1.3250755071640015
Epoch 140, training loss: 1.8080990314483643 = 1.1326651573181152 + 0.1 * 6.75433874130249
Epoch 140, val loss: 1.237143635749817
Epoch 150, training loss: 1.7052510976791382 = 1.030859112739563 + 0.1 * 6.743919849395752
Epoch 150, val loss: 1.159099817276001
Epoch 160, training loss: 1.6125802993774414 = 0.9392685890197754 + 0.1 * 6.733117580413818
Epoch 160, val loss: 1.0900039672851562
Epoch 170, training loss: 1.5299897193908691 = 0.8578584790229797 + 0.1 * 6.721312999725342
Epoch 170, val loss: 1.0298216342926025
Epoch 180, training loss: 1.4566435813903809 = 0.7859678864479065 + 0.1 * 6.706756114959717
Epoch 180, val loss: 0.9780818223953247
Epoch 190, training loss: 1.3918113708496094 = 0.7229151725769043 + 0.1 * 6.688961982727051
Epoch 190, val loss: 0.9340575337409973
Epoch 200, training loss: 1.334895372390747 = 0.6680938005447388 + 0.1 * 6.668015003204346
Epoch 200, val loss: 0.897606372833252
Epoch 210, training loss: 1.284781575202942 = 0.6198638677597046 + 0.1 * 6.649177074432373
Epoch 210, val loss: 0.8676825761795044
Epoch 220, training loss: 1.2386529445648193 = 0.5761346817016602 + 0.1 * 6.625182151794434
Epoch 220, val loss: 0.8422874808311462
Epoch 230, training loss: 1.194970965385437 = 0.5347683429718018 + 0.1 * 6.602025985717773
Epoch 230, val loss: 0.8196868896484375
Epoch 240, training loss: 1.1540746688842773 = 0.494884729385376 + 0.1 * 6.5918989181518555
Epoch 240, val loss: 0.799538254737854
Epoch 250, training loss: 1.1129872798919678 = 0.45599478483200073 + 0.1 * 6.569924831390381
Epoch 250, val loss: 0.7818695902824402
Epoch 260, training loss: 1.073371410369873 = 0.4177513122558594 + 0.1 * 6.5562005043029785
Epoch 260, val loss: 0.7668571472167969
Epoch 270, training loss: 1.0351378917694092 = 0.38047462701797485 + 0.1 * 6.546633243560791
Epoch 270, val loss: 0.7548823952674866
Epoch 280, training loss: 0.99781334400177 = 0.3445946276187897 + 0.1 * 6.532186985015869
Epoch 280, val loss: 0.746029794216156
Epoch 290, training loss: 0.9631748199462891 = 0.31031736731529236 + 0.1 * 6.528573989868164
Epoch 290, val loss: 0.7398201823234558
Epoch 300, training loss: 0.9304821491241455 = 0.27825799584388733 + 0.1 * 6.522241592407227
Epoch 300, val loss: 0.7357219457626343
Epoch 310, training loss: 0.8998638987541199 = 0.2487947791814804 + 0.1 * 6.510690689086914
Epoch 310, val loss: 0.7336451411247253
Epoch 320, training loss: 0.8718990087509155 = 0.22197622060775757 + 0.1 * 6.499227523803711
Epoch 320, val loss: 0.7334609627723694
Epoch 330, training loss: 0.8475624322891235 = 0.19805565476417542 + 0.1 * 6.495067119598389
Epoch 330, val loss: 0.734768271446228
Epoch 340, training loss: 0.825330913066864 = 0.17695732414722443 + 0.1 * 6.483736038208008
Epoch 340, val loss: 0.7375101447105408
Epoch 350, training loss: 0.8065906763076782 = 0.1584276258945465 + 0.1 * 6.481630325317383
Epoch 350, val loss: 0.74177086353302
Epoch 360, training loss: 0.7889968156814575 = 0.14220646023750305 + 0.1 * 6.4679036140441895
Epoch 360, val loss: 0.7471678256988525
Epoch 370, training loss: 0.7745028734207153 = 0.12797142565250397 + 0.1 * 6.4653143882751465
Epoch 370, val loss: 0.7535467147827148
Epoch 380, training loss: 0.7613489627838135 = 0.11546535789966583 + 0.1 * 6.458836078643799
Epoch 380, val loss: 0.7608160376548767
Epoch 390, training loss: 0.7495012283325195 = 0.10449178516864777 + 0.1 * 6.450094699859619
Epoch 390, val loss: 0.7686620950698853
Epoch 400, training loss: 0.7386924028396606 = 0.0948149636387825 + 0.1 * 6.438774585723877
Epoch 400, val loss: 0.7770740389823914
Epoch 410, training loss: 0.7297074794769287 = 0.0862606093287468 + 0.1 * 6.434468746185303
Epoch 410, val loss: 0.7857741117477417
Epoch 420, training loss: 0.7206764221191406 = 0.07870176434516907 + 0.1 * 6.419745922088623
Epoch 420, val loss: 0.7948058247566223
Epoch 430, training loss: 0.7138576507568359 = 0.07195677608251572 + 0.1 * 6.419008731842041
Epoch 430, val loss: 0.803905189037323
Epoch 440, training loss: 0.7067980170249939 = 0.06592272222042084 + 0.1 * 6.408752918243408
Epoch 440, val loss: 0.8130092620849609
Epoch 450, training loss: 0.7017747163772583 = 0.0604996383190155 + 0.1 * 6.412750720977783
Epoch 450, val loss: 0.8221369981765747
Epoch 460, training loss: 0.6961219310760498 = 0.055629897862672806 + 0.1 * 6.4049201011657715
Epoch 460, val loss: 0.8312146663665771
Epoch 470, training loss: 0.6902656555175781 = 0.05126093700528145 + 0.1 * 6.390047073364258
Epoch 470, val loss: 0.8401163220405579
Epoch 480, training loss: 0.6861236691474915 = 0.047332655638456345 + 0.1 * 6.387909889221191
Epoch 480, val loss: 0.8490282297134399
Epoch 490, training loss: 0.6818035244941711 = 0.04379858821630478 + 0.1 * 6.380049705505371
Epoch 490, val loss: 0.8576568961143494
Epoch 500, training loss: 0.6783958673477173 = 0.04060860723257065 + 0.1 * 6.377872467041016
Epoch 500, val loss: 0.8663960099220276
Epoch 510, training loss: 0.67693030834198 = 0.03771936893463135 + 0.1 * 6.392109394073486
Epoch 510, val loss: 0.8749035596847534
Epoch 520, training loss: 0.6716586947441101 = 0.03511129319667816 + 0.1 * 6.365473747253418
Epoch 520, val loss: 0.8833261728286743
Epoch 530, training loss: 0.6694400906562805 = 0.03274531289935112 + 0.1 * 6.366947650909424
Epoch 530, val loss: 0.891772449016571
Epoch 540, training loss: 0.6664713621139526 = 0.030603649094700813 + 0.1 * 6.358676910400391
Epoch 540, val loss: 0.9000054597854614
Epoch 550, training loss: 0.663300096988678 = 0.028662987053394318 + 0.1 * 6.346371173858643
Epoch 550, val loss: 0.9082880616188049
Epoch 560, training loss: 0.664178192615509 = 0.026895621791481972 + 0.1 * 6.372825622558594
Epoch 560, val loss: 0.9163501858711243
Epoch 570, training loss: 0.6593201160430908 = 0.025289500132203102 + 0.1 * 6.340306282043457
Epoch 570, val loss: 0.9243625402450562
Epoch 580, training loss: 0.658308207988739 = 0.023817814886569977 + 0.1 * 6.344903469085693
Epoch 580, val loss: 0.9323428273200989
Epoch 590, training loss: 0.6568904519081116 = 0.022470060735940933 + 0.1 * 6.344203948974609
Epoch 590, val loss: 0.9399774670600891
Epoch 600, training loss: 0.6538893580436707 = 0.021235311403870583 + 0.1 * 6.326540470123291
Epoch 600, val loss: 0.9478165507316589
Epoch 610, training loss: 0.6517934799194336 = 0.020095553249120712 + 0.1 * 6.31697940826416
Epoch 610, val loss: 0.9553971886634827
Epoch 620, training loss: 0.6549069285392761 = 0.01904251240193844 + 0.1 * 6.358644485473633
Epoch 620, val loss: 0.9628053903579712
Epoch 630, training loss: 0.6501893997192383 = 0.01807909645140171 + 0.1 * 6.321102619171143
Epoch 630, val loss: 0.9700040221214294
Epoch 640, training loss: 0.6477723121643066 = 0.01719164103269577 + 0.1 * 6.305806636810303
Epoch 640, val loss: 0.9773353934288025
Epoch 650, training loss: 0.6510825157165527 = 0.016370093449950218 + 0.1 * 6.347124099731445
Epoch 650, val loss: 0.9843197464942932
Epoch 660, training loss: 0.6453242897987366 = 0.015613563358783722 + 0.1 * 6.297107219696045
Epoch 660, val loss: 0.9912136793136597
Epoch 670, training loss: 0.6441153287887573 = 0.014911334030330181 + 0.1 * 6.292039394378662
Epoch 670, val loss: 0.998209536075592
Epoch 680, training loss: 0.6447659730911255 = 0.014256689697504044 + 0.1 * 6.3050923347473145
Epoch 680, val loss: 1.0049687623977661
Epoch 690, training loss: 0.6436154246330261 = 0.013647885993123055 + 0.1 * 6.299674987792969
Epoch 690, val loss: 1.0114612579345703
Epoch 700, training loss: 0.6414327025413513 = 0.013082007877528667 + 0.1 * 6.283506870269775
Epoch 700, val loss: 1.018113613128662
Epoch 710, training loss: 0.6407719850540161 = 0.012551289051771164 + 0.1 * 6.2822065353393555
Epoch 710, val loss: 1.024553656578064
Epoch 720, training loss: 0.6418296098709106 = 0.012053662911057472 + 0.1 * 6.297759056091309
Epoch 720, val loss: 1.0307941436767578
Epoch 730, training loss: 0.6389680504798889 = 0.01158833410590887 + 0.1 * 6.273797035217285
Epoch 730, val loss: 1.036911129951477
Epoch 740, training loss: 0.6382824182510376 = 0.011152411811053753 + 0.1 * 6.271300315856934
Epoch 740, val loss: 1.0431195497512817
Epoch 750, training loss: 0.6371106505393982 = 0.010741885751485825 + 0.1 * 6.263687610626221
Epoch 750, val loss: 1.0490138530731201
Epoch 760, training loss: 0.6364578008651733 = 0.010355021804571152 + 0.1 * 6.261027812957764
Epoch 760, val loss: 1.0549790859222412
Epoch 770, training loss: 0.6376639008522034 = 0.009990397840738297 + 0.1 * 6.276734828948975
Epoch 770, val loss: 1.0604127645492554
Epoch 780, training loss: 0.6359460353851318 = 0.00964886974543333 + 0.1 * 6.2629714012146
Epoch 780, val loss: 1.0661022663116455
Epoch 790, training loss: 0.6362701654434204 = 0.009326170198619366 + 0.1 * 6.269440174102783
Epoch 790, val loss: 1.0718629360198975
Epoch 800, training loss: 0.6346551775932312 = 0.009020675905048847 + 0.1 * 6.256344795227051
Epoch 800, val loss: 1.0770862102508545
Epoch 810, training loss: 0.6333789229393005 = 0.008731446228921413 + 0.1 * 6.246474742889404
Epoch 810, val loss: 1.0825929641723633
Epoch 820, training loss: 0.6338645815849304 = 0.00845625251531601 + 0.1 * 6.254083633422852
Epoch 820, val loss: 1.0878417491912842
Epoch 830, training loss: 0.6326867341995239 = 0.008194991387426853 + 0.1 * 6.244917392730713
Epoch 830, val loss: 1.0927587747573853
Epoch 840, training loss: 0.6318199038505554 = 0.007948351092636585 + 0.1 * 6.238715648651123
Epoch 840, val loss: 1.0979408025741577
Epoch 850, training loss: 0.6312589049339294 = 0.007713320665061474 + 0.1 * 6.235455513000488
Epoch 850, val loss: 1.1030596494674683
Epoch 860, training loss: 0.6316556930541992 = 0.0074889748357236385 + 0.1 * 6.2416672706604
Epoch 860, val loss: 1.107965350151062
Epoch 870, training loss: 0.6308490633964539 = 0.0072750914841890335 + 0.1 * 6.235739231109619
Epoch 870, val loss: 1.1126837730407715
Epoch 880, training loss: 0.6304746866226196 = 0.007071950472891331 + 0.1 * 6.234026908874512
Epoch 880, val loss: 1.1173374652862549
Epoch 890, training loss: 0.6294572949409485 = 0.00687821488827467 + 0.1 * 6.225790977478027
Epoch 890, val loss: 1.1220847368240356
Epoch 900, training loss: 0.6300716400146484 = 0.006693027913570404 + 0.1 * 6.233786106109619
Epoch 900, val loss: 1.126670002937317
Epoch 910, training loss: 0.6284992098808289 = 0.006516218185424805 + 0.1 * 6.219829559326172
Epoch 910, val loss: 1.1311087608337402
Epoch 920, training loss: 0.6292398571968079 = 0.006347058340907097 + 0.1 * 6.2289276123046875
Epoch 920, val loss: 1.135623574256897
Epoch 930, training loss: 0.6279647350311279 = 0.0061850156635046005 + 0.1 * 6.21779727935791
Epoch 930, val loss: 1.1398842334747314
Epoch 940, training loss: 0.6297183632850647 = 0.0060301306657493114 + 0.1 * 6.236882209777832
Epoch 940, val loss: 1.1442084312438965
Epoch 950, training loss: 0.6275275349617004 = 0.005881723016500473 + 0.1 * 6.216457843780518
Epoch 950, val loss: 1.1482995748519897
Epoch 960, training loss: 0.6272274851799011 = 0.005739705171436071 + 0.1 * 6.214877605438232
Epoch 960, val loss: 1.1525827646255493
Epoch 970, training loss: 0.6267064213752747 = 0.005603079684078693 + 0.1 * 6.211033344268799
Epoch 970, val loss: 1.1566029787063599
Epoch 980, training loss: 0.6261993050575256 = 0.005472025368362665 + 0.1 * 6.207273006439209
Epoch 980, val loss: 1.160657525062561
Epoch 990, training loss: 0.6264532804489136 = 0.0053460001945495605 + 0.1 * 6.2110724449157715
Epoch 990, val loss: 1.1646544933319092
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7528
Flip ASR: 0.7067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.778918743133545 = 1.9415361881256104 + 0.1 * 8.373826026916504
Epoch 0, val loss: 1.9373431205749512
Epoch 10, training loss: 2.7685773372650146 = 1.9312118291854858 + 0.1 * 8.373655319213867
Epoch 10, val loss: 1.927677869796753
Epoch 20, training loss: 2.7557506561279297 = 1.918498158454895 + 0.1 * 8.372526168823242
Epoch 20, val loss: 1.9153521060943604
Epoch 30, training loss: 2.736992120742798 = 1.9006255865097046 + 0.1 * 8.363664627075195
Epoch 30, val loss: 1.8977925777435303
Epoch 40, training loss: 2.704890251159668 = 1.8743462562561035 + 0.1 * 8.305439949035645
Epoch 40, val loss: 1.8723623752593994
Epoch 50, training loss: 2.635791778564453 = 1.8401376008987427 + 0.1 * 7.956540584564209
Epoch 50, val loss: 1.8408981561660767
Epoch 60, training loss: 2.5756936073303223 = 1.8020424842834473 + 0.1 * 7.736511707305908
Epoch 60, val loss: 1.807205080986023
Epoch 70, training loss: 2.504859685897827 = 1.7647743225097656 + 0.1 * 7.400853157043457
Epoch 70, val loss: 1.7757174968719482
Epoch 80, training loss: 2.4287900924682617 = 1.7271904945373535 + 0.1 * 7.01599645614624
Epoch 80, val loss: 1.744037389755249
Epoch 90, training loss: 2.357597589492798 = 1.6777055263519287 + 0.1 * 6.79892110824585
Epoch 90, val loss: 1.7008087635040283
Epoch 100, training loss: 2.2832226753234863 = 1.6109992265701294 + 0.1 * 6.72223424911499
Epoch 100, val loss: 1.6437159776687622
Epoch 110, training loss: 2.1962978839874268 = 1.5287386178970337 + 0.1 * 6.67559289932251
Epoch 110, val loss: 1.5765775442123413
Epoch 120, training loss: 2.102720022201538 = 1.4388759136199951 + 0.1 * 6.638440132141113
Epoch 120, val loss: 1.501838207244873
Epoch 130, training loss: 2.0078697204589844 = 1.3471283912658691 + 0.1 * 6.607414245605469
Epoch 130, val loss: 1.428322196006775
Epoch 140, training loss: 1.914581537246704 = 1.2556599378585815 + 0.1 * 6.589215278625488
Epoch 140, val loss: 1.355757236480713
Epoch 150, training loss: 1.8232381343841553 = 1.166743516921997 + 0.1 * 6.56494665145874
Epoch 150, val loss: 1.2868494987487793
Epoch 160, training loss: 1.7349251508712769 = 1.0804280042648315 + 0.1 * 6.544971466064453
Epoch 160, val loss: 1.2207450866699219
Epoch 170, training loss: 1.6519603729248047 = 0.9988788962364197 + 0.1 * 6.530814170837402
Epoch 170, val loss: 1.1590468883514404
Epoch 180, training loss: 1.5749588012695312 = 0.9233721494674683 + 0.1 * 6.515867233276367
Epoch 180, val loss: 1.1028169393539429
Epoch 190, training loss: 1.502096176147461 = 0.851822018623352 + 0.1 * 6.502741813659668
Epoch 190, val loss: 1.0500433444976807
Epoch 200, training loss: 1.4331917762756348 = 0.7839801907539368 + 0.1 * 6.492115020751953
Epoch 200, val loss: 1.000556230545044
Epoch 210, training loss: 1.3678014278411865 = 0.7197577953338623 + 0.1 * 6.480435371398926
Epoch 210, val loss: 0.9542263746261597
Epoch 220, training loss: 1.306621789932251 = 0.6587646007537842 + 0.1 * 6.478572368621826
Epoch 220, val loss: 0.911194384098053
Epoch 230, training loss: 1.248377799987793 = 0.6023015975952148 + 0.1 * 6.460761070251465
Epoch 230, val loss: 0.8734399676322937
Epoch 240, training loss: 1.1972126960754395 = 0.5507386326789856 + 0.1 * 6.464739799499512
Epoch 240, val loss: 0.8417800664901733
Epoch 250, training loss: 1.1492986679077148 = 0.5050628781318665 + 0.1 * 6.442357063293457
Epoch 250, val loss: 0.8174387216567993
Epoch 260, training loss: 1.1073822975158691 = 0.4642413556575775 + 0.1 * 6.431408882141113
Epoch 260, val loss: 0.7995424866676331
Epoch 270, training loss: 1.069354772567749 = 0.4270387589931488 + 0.1 * 6.423159599304199
Epoch 270, val loss: 0.7867104411125183
Epoch 280, training loss: 1.034309983253479 = 0.39265361428260803 + 0.1 * 6.416563510894775
Epoch 280, val loss: 0.7777571082115173
Epoch 290, training loss: 1.001096248626709 = 0.36029207706451416 + 0.1 * 6.408041477203369
Epoch 290, val loss: 0.771381676197052
Epoch 300, training loss: 0.969982385635376 = 0.32949528098106384 + 0.1 * 6.404870986938477
Epoch 300, val loss: 0.7668144702911377
Epoch 310, training loss: 0.939069390296936 = 0.29997584223747253 + 0.1 * 6.39093542098999
Epoch 310, val loss: 0.7636628150939941
Epoch 320, training loss: 0.9104976058006287 = 0.27188992500305176 + 0.1 * 6.386076927185059
Epoch 320, val loss: 0.7618181705474854
Epoch 330, training loss: 0.882935643196106 = 0.2454499751329422 + 0.1 * 6.374856948852539
Epoch 330, val loss: 0.7614817023277283
Epoch 340, training loss: 0.8583230972290039 = 0.22097760438919067 + 0.1 * 6.373454570770264
Epoch 340, val loss: 0.7624960541725159
Epoch 350, training loss: 0.8345511555671692 = 0.19876915216445923 + 0.1 * 6.3578200340271
Epoch 350, val loss: 0.7650448083877563
Epoch 360, training loss: 0.8155007362365723 = 0.17879924178123474 + 0.1 * 6.367014408111572
Epoch 360, val loss: 0.7689784169197083
Epoch 370, training loss: 0.7968391180038452 = 0.16108927130699158 + 0.1 * 6.3574981689453125
Epoch 370, val loss: 0.7742782235145569
Epoch 380, training loss: 0.7798369526863098 = 0.14545226097106934 + 0.1 * 6.343846797943115
Epoch 380, val loss: 0.7809227705001831
Epoch 390, training loss: 0.7668108940124512 = 0.13160565495491028 + 0.1 * 6.352051734924316
Epoch 390, val loss: 0.788567066192627
Epoch 400, training loss: 0.7532346248626709 = 0.11942315101623535 + 0.1 * 6.3381147384643555
Epoch 400, val loss: 0.7970008850097656
Epoch 410, training loss: 0.7411537170410156 = 0.10864991694688797 + 0.1 * 6.325037956237793
Epoch 410, val loss: 0.8063146471977234
Epoch 420, training loss: 0.7335883975028992 = 0.09908813238143921 + 0.1 * 6.3450026512146
Epoch 420, val loss: 0.8162025809288025
Epoch 430, training loss: 0.7221242785453796 = 0.09063035994768143 + 0.1 * 6.314939022064209
Epoch 430, val loss: 0.826566219329834
Epoch 440, training loss: 0.7136791944503784 = 0.08309698849916458 + 0.1 * 6.305821895599365
Epoch 440, val loss: 0.8372536897659302
Epoch 450, training loss: 0.7061041593551636 = 0.07638614624738693 + 0.1 * 6.297179698944092
Epoch 450, val loss: 0.8482232093811035
Epoch 460, training loss: 0.6998765468597412 = 0.07040216773748398 + 0.1 * 6.29474401473999
Epoch 460, val loss: 0.8594910502433777
Epoch 470, training loss: 0.6948036551475525 = 0.06505496799945831 + 0.1 * 6.297486782073975
Epoch 470, val loss: 0.8706084489822388
Epoch 480, training loss: 0.6890854239463806 = 0.06027645245194435 + 0.1 * 6.288089752197266
Epoch 480, val loss: 0.8820178508758545
Epoch 490, training loss: 0.6835999488830566 = 0.055972468107938766 + 0.1 * 6.276274681091309
Epoch 490, val loss: 0.8932324647903442
Epoch 500, training loss: 0.684894859790802 = 0.05208718404173851 + 0.1 * 6.3280768394470215
Epoch 500, val loss: 0.9043514132499695
Epoch 510, training loss: 0.6766322255134583 = 0.048604533076286316 + 0.1 * 6.280276775360107
Epoch 510, val loss: 0.9153140783309937
Epoch 520, training loss: 0.6718874573707581 = 0.04545626416802406 + 0.1 * 6.264311790466309
Epoch 520, val loss: 0.9262368679046631
Epoch 530, training loss: 0.6685926914215088 = 0.04258875548839569 + 0.1 * 6.26003885269165
Epoch 530, val loss: 0.9369274377822876
Epoch 540, training loss: 0.666719913482666 = 0.03997402638196945 + 0.1 * 6.267458915710449
Epoch 540, val loss: 0.9474393129348755
Epoch 550, training loss: 0.6629977226257324 = 0.037588875740766525 + 0.1 * 6.254087924957275
Epoch 550, val loss: 0.9578793048858643
Epoch 560, training loss: 0.6616896390914917 = 0.035403456538915634 + 0.1 * 6.262862205505371
Epoch 560, val loss: 0.9681113958358765
Epoch 570, training loss: 0.6582075953483582 = 0.033400777727365494 + 0.1 * 6.248068332672119
Epoch 570, val loss: 0.9780442714691162
Epoch 580, training loss: 0.6558847427368164 = 0.03155842423439026 + 0.1 * 6.243262767791748
Epoch 580, val loss: 0.9879519939422607
Epoch 590, training loss: 0.6545109748840332 = 0.02985980547964573 + 0.1 * 6.246511936187744
Epoch 590, val loss: 0.9973037242889404
Epoch 600, training loss: 0.6521909832954407 = 0.028297243639826775 + 0.1 * 6.2389373779296875
Epoch 600, val loss: 1.006924033164978
Epoch 610, training loss: 0.6502824425697327 = 0.026846710592508316 + 0.1 * 6.2343573570251465
Epoch 610, val loss: 1.0161912441253662
Epoch 620, training loss: 0.650383710861206 = 0.025499671697616577 + 0.1 * 6.248839855194092
Epoch 620, val loss: 1.025158166885376
Epoch 630, training loss: 0.6469244956970215 = 0.024251557886600494 + 0.1 * 6.226728916168213
Epoch 630, val loss: 1.034059762954712
Epoch 640, training loss: 0.6457350850105286 = 0.023090744391083717 + 0.1 * 6.226443290710449
Epoch 640, val loss: 1.0429370403289795
Epoch 650, training loss: 0.6447796821594238 = 0.022007310763001442 + 0.1 * 6.227723598480225
Epoch 650, val loss: 1.051504135131836
Epoch 660, training loss: 0.6429021954536438 = 0.02099546045064926 + 0.1 * 6.219067573547363
Epoch 660, val loss: 1.0599602460861206
Epoch 670, training loss: 0.6422415971755981 = 0.02005024254322052 + 0.1 * 6.2219133377075195
Epoch 670, val loss: 1.0683488845825195
Epoch 680, training loss: 0.6412012577056885 = 0.01916562020778656 + 0.1 * 6.220355987548828
Epoch 680, val loss: 1.0765550136566162
Epoch 690, training loss: 0.6395416855812073 = 0.018338076770305634 + 0.1 * 6.2120361328125
Epoch 690, val loss: 1.084505558013916
Epoch 700, training loss: 0.6384435296058655 = 0.01756221428513527 + 0.1 * 6.208813190460205
Epoch 700, val loss: 1.0925929546356201
Epoch 710, training loss: 0.6394106149673462 = 0.016833122819662094 + 0.1 * 6.225774765014648
Epoch 710, val loss: 1.100142240524292
Epoch 720, training loss: 0.6369518637657166 = 0.016150958836078644 + 0.1 * 6.208008766174316
Epoch 720, val loss: 1.1078722476959229
Epoch 730, training loss: 0.6361488699913025 = 0.015509795397520065 + 0.1 * 6.206390857696533
Epoch 730, val loss: 1.115526556968689
Epoch 740, training loss: 0.6357199549674988 = 0.014904898591339588 + 0.1 * 6.208150386810303
Epoch 740, val loss: 1.1226671934127808
Epoch 750, training loss: 0.6343526840209961 = 0.014336660504341125 + 0.1 * 6.200160503387451
Epoch 750, val loss: 1.1300703287124634
Epoch 760, training loss: 0.63407301902771 = 0.013799436390399933 + 0.1 * 6.202735900878906
Epoch 760, val loss: 1.137251615524292
Epoch 770, training loss: 0.6329174637794495 = 0.01329184602946043 + 0.1 * 6.196256160736084
Epoch 770, val loss: 1.1442805528640747
Epoch 780, training loss: 0.6319603323936462 = 0.012811429798603058 + 0.1 * 6.191488742828369
Epoch 780, val loss: 1.1511611938476562
Epoch 790, training loss: 0.6315620541572571 = 0.012357351370155811 + 0.1 * 6.192046642303467
Epoch 790, val loss: 1.158042073249817
Epoch 800, training loss: 0.6331601738929749 = 0.011927439831197262 + 0.1 * 6.212327480316162
Epoch 800, val loss: 1.1647003889083862
Epoch 810, training loss: 0.6308541893959045 = 0.01152139250189066 + 0.1 * 6.193327903747559
Epoch 810, val loss: 1.1711682081222534
Epoch 820, training loss: 0.6304025650024414 = 0.011137183755636215 + 0.1 * 6.192653656005859
Epoch 820, val loss: 1.177888035774231
Epoch 830, training loss: 0.6295250654220581 = 0.010771440342068672 + 0.1 * 6.187535762786865
Epoch 830, val loss: 1.18417489528656
Epoch 840, training loss: 0.6286053657531738 = 0.01042398251593113 + 0.1 * 6.181813716888428
Epoch 840, val loss: 1.1905043125152588
Epoch 850, training loss: 0.6305274367332458 = 0.010093468241393566 + 0.1 * 6.204339981079102
Epoch 850, val loss: 1.196684718132019
Epoch 860, training loss: 0.6284547448158264 = 0.009779498912394047 + 0.1 * 6.1867523193359375
Epoch 860, val loss: 1.2027294635772705
Epoch 870, training loss: 0.6286247968673706 = 0.009480921551585197 + 0.1 * 6.191438674926758
Epoch 870, val loss: 1.2088993787765503
Epoch 880, training loss: 0.6269212961196899 = 0.009196344763040543 + 0.1 * 6.177249431610107
Epoch 880, val loss: 1.2146661281585693
Epoch 890, training loss: 0.6266705989837646 = 0.00892532430589199 + 0.1 * 6.177452564239502
Epoch 890, val loss: 1.2206264734268188
Epoch 900, training loss: 0.6276747584342957 = 0.008666103705763817 + 0.1 * 6.190086364746094
Epoch 900, val loss: 1.2263985872268677
Epoch 910, training loss: 0.6258953213691711 = 0.008419232442975044 + 0.1 * 6.174760818481445
Epoch 910, val loss: 1.231837511062622
Epoch 920, training loss: 0.6250602006912231 = 0.00818414706736803 + 0.1 * 6.168760776519775
Epoch 920, val loss: 1.2375987768173218
Epoch 930, training loss: 0.6252090930938721 = 0.007959016598761082 + 0.1 * 6.1725006103515625
Epoch 930, val loss: 1.2430425882339478
Epoch 940, training loss: 0.6247196197509766 = 0.007743917405605316 + 0.1 * 6.16975736618042
Epoch 940, val loss: 1.2484276294708252
Epoch 950, training loss: 0.6243005394935608 = 0.007537708152085543 + 0.1 * 6.167627811431885
Epoch 950, val loss: 1.2536202669143677
Epoch 960, training loss: 0.6239268779754639 = 0.0073412274941802025 + 0.1 * 6.165856838226318
Epoch 960, val loss: 1.2589412927627563
Epoch 970, training loss: 0.6235852241516113 = 0.007152448873966932 + 0.1 * 6.164327621459961
Epoch 970, val loss: 1.2642818689346313
Epoch 980, training loss: 0.624488890171051 = 0.0069710975512862206 + 0.1 * 6.175177574157715
Epoch 980, val loss: 1.2693308591842651
Epoch 990, training loss: 0.6228744387626648 = 0.00679710041731596 + 0.1 * 6.160773277282715
Epoch 990, val loss: 1.274107813835144
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7269
Flip ASR: 0.6844/225 nodes
The final ASR:0.67159, 0.09712, Accuracy:0.80617, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9456])
updated graph: torch.Size([2, 10530])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83827, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7669730186462402 = 1.9295850992202759 + 0.1 * 8.373880386352539
Epoch 0, val loss: 1.9304910898208618
Epoch 10, training loss: 2.7556681632995605 = 1.9182918071746826 + 0.1 * 8.373763084411621
Epoch 10, val loss: 1.9164923429489136
Epoch 20, training loss: 2.741528034210205 = 1.9042103290557861 + 0.1 * 8.373176574707031
Epoch 20, val loss: 1.898118257522583
Epoch 30, training loss: 2.7222204208374023 = 1.8853703737258911 + 0.1 * 8.368500709533691
Epoch 30, val loss: 1.8741023540496826
Epoch 40, training loss: 2.6946682929992676 = 1.8612197637557983 + 0.1 * 8.334484100341797
Epoch 40, val loss: 1.8462663888931274
Epoch 50, training loss: 2.641385078430176 = 1.831272840499878 + 0.1 * 8.101120948791504
Epoch 50, val loss: 1.8154749870300293
Epoch 60, training loss: 2.564436912536621 = 1.7976078987121582 + 0.1 * 7.6682891845703125
Epoch 60, val loss: 1.783735752105713
Epoch 70, training loss: 2.4897305965423584 = 1.7634532451629639 + 0.1 * 7.2627739906311035
Epoch 70, val loss: 1.7531652450561523
Epoch 80, training loss: 2.4255964756011963 = 1.726218819618225 + 0.1 * 6.993777275085449
Epoch 80, val loss: 1.7214384078979492
Epoch 90, training loss: 2.368741273880005 = 1.6812161207199097 + 0.1 * 6.875251770019531
Epoch 90, val loss: 1.6840713024139404
Epoch 100, training loss: 2.3024635314941406 = 1.6212987899780273 + 0.1 * 6.811646938323975
Epoch 100, val loss: 1.6340280771255493
Epoch 110, training loss: 2.2188222408294678 = 1.542522668838501 + 0.1 * 6.762994766235352
Epoch 110, val loss: 1.5693061351776123
Epoch 120, training loss: 2.117589235305786 = 1.445433497428894 + 0.1 * 6.721557140350342
Epoch 120, val loss: 1.4920254945755005
Epoch 130, training loss: 2.0049548149108887 = 1.3362714052200317 + 0.1 * 6.686834812164307
Epoch 130, val loss: 1.4062278270721436
Epoch 140, training loss: 1.8924182653427124 = 1.225964069366455 + 0.1 * 6.664541721343994
Epoch 140, val loss: 1.321641445159912
Epoch 150, training loss: 1.7877776622772217 = 1.1243493556976318 + 0.1 * 6.634283542633057
Epoch 150, val loss: 1.2459172010421753
Epoch 160, training loss: 1.6970887184143066 = 1.035287857055664 + 0.1 * 6.618007659912109
Epoch 160, val loss: 1.1808634996414185
Epoch 170, training loss: 1.6187543869018555 = 0.9594646096229553 + 0.1 * 6.592897891998291
Epoch 170, val loss: 1.1264746189117432
Epoch 180, training loss: 1.5497283935546875 = 0.8923066854476929 + 0.1 * 6.574217319488525
Epoch 180, val loss: 1.0787667036056519
Epoch 190, training loss: 1.4856913089752197 = 0.8298514485359192 + 0.1 * 6.5583977699279785
Epoch 190, val loss: 1.0347782373428345
Epoch 200, training loss: 1.4262151718139648 = 0.7696332335472107 + 0.1 * 6.565819263458252
Epoch 200, val loss: 0.9927868843078613
Epoch 210, training loss: 1.3641473054885864 = 0.7107197046279907 + 0.1 * 6.534276008605957
Epoch 210, val loss: 0.9518727660179138
Epoch 220, training loss: 1.3043556213378906 = 0.6522327065467834 + 0.1 * 6.521228313446045
Epoch 220, val loss: 0.9111549854278564
Epoch 230, training loss: 1.2458925247192383 = 0.5949179530143738 + 0.1 * 6.509746074676514
Epoch 230, val loss: 0.8718056678771973
Epoch 240, training loss: 1.1900395154953003 = 0.5398609638214111 + 0.1 * 6.5017852783203125
Epoch 240, val loss: 0.8352822661399841
Epoch 250, training loss: 1.1371090412139893 = 0.4879191815853119 + 0.1 * 6.491899013519287
Epoch 250, val loss: 0.8029820919036865
Epoch 260, training loss: 1.0890934467315674 = 0.438485324382782 + 0.1 * 6.506080627441406
Epoch 260, val loss: 0.7745417356491089
Epoch 270, training loss: 1.039425015449524 = 0.3918921649456024 + 0.1 * 6.475327968597412
Epoch 270, val loss: 0.7503066062927246
Epoch 280, training loss: 0.9952331185340881 = 0.3478440046310425 + 0.1 * 6.473890781402588
Epoch 280, val loss: 0.7297419905662537
Epoch 290, training loss: 0.9535792469978333 = 0.30702918767929077 + 0.1 * 6.465500354766846
Epoch 290, val loss: 0.7131443619728088
Epoch 300, training loss: 0.9152708053588867 = 0.2700798809528351 + 0.1 * 6.45190954208374
Epoch 300, val loss: 0.7003961205482483
Epoch 310, training loss: 0.8826838731765747 = 0.23761646449565887 + 0.1 * 6.450673580169678
Epoch 310, val loss: 0.691487729549408
Epoch 320, training loss: 0.853265106678009 = 0.20968662202358246 + 0.1 * 6.435784816741943
Epoch 320, val loss: 0.6861464381217957
Epoch 330, training loss: 0.8310279846191406 = 0.18584856390953064 + 0.1 * 6.451793670654297
Epoch 330, val loss: 0.6837778687477112
Epoch 340, training loss: 0.809397280216217 = 0.16581839323043823 + 0.1 * 6.435788631439209
Epoch 340, val loss: 0.6840572953224182
Epoch 350, training loss: 0.789705753326416 = 0.14871864020824432 + 0.1 * 6.409870624542236
Epoch 350, val loss: 0.6861516833305359
Epoch 360, training loss: 0.775087296962738 = 0.1339598298072815 + 0.1 * 6.411274433135986
Epoch 360, val loss: 0.6898468732833862
Epoch 370, training loss: 0.7621556520462036 = 0.12121189385652542 + 0.1 * 6.409437656402588
Epoch 370, val loss: 0.6947057843208313
Epoch 380, training loss: 0.7490922808647156 = 0.11012440174818039 + 0.1 * 6.389678478240967
Epoch 380, val loss: 0.7005624175071716
Epoch 390, training loss: 0.7393077611923218 = 0.10039084404706955 + 0.1 * 6.389169216156006
Epoch 390, val loss: 0.7071579098701477
Epoch 400, training loss: 0.7291409373283386 = 0.09180182963609695 + 0.1 * 6.373391151428223
Epoch 400, val loss: 0.7143478989601135
Epoch 410, training loss: 0.7229898571968079 = 0.08418121188879013 + 0.1 * 6.388086795806885
Epoch 410, val loss: 0.7220383882522583
Epoch 420, training loss: 0.713181734085083 = 0.07741018384695053 + 0.1 * 6.357715606689453
Epoch 420, val loss: 0.7300539612770081
Epoch 430, training loss: 0.7078408002853394 = 0.0713459923863411 + 0.1 * 6.36494779586792
Epoch 430, val loss: 0.7382705211639404
Epoch 440, training loss: 0.700324296951294 = 0.06590467691421509 + 0.1 * 6.34419584274292
Epoch 440, val loss: 0.7467854619026184
Epoch 450, training loss: 0.6975467801094055 = 0.060986220836639404 + 0.1 * 6.365605354309082
Epoch 450, val loss: 0.7554404139518738
Epoch 460, training loss: 0.6910710334777832 = 0.056544672697782516 + 0.1 * 6.345263481140137
Epoch 460, val loss: 0.7641883492469788
Epoch 470, training loss: 0.6865668296813965 = 0.052524130791425705 + 0.1 * 6.340426445007324
Epoch 470, val loss: 0.7729944586753845
Epoch 480, training loss: 0.6815645694732666 = 0.048873063176870346 + 0.1 * 6.3269147872924805
Epoch 480, val loss: 0.7817777395248413
Epoch 490, training loss: 0.6777293682098389 = 0.045553065836429596 + 0.1 * 6.321762561798096
Epoch 490, val loss: 0.7906836271286011
Epoch 500, training loss: 0.6739699244499207 = 0.042530354112386703 + 0.1 * 6.314395427703857
Epoch 500, val loss: 0.799501895904541
Epoch 510, training loss: 0.671610951423645 = 0.039782214909791946 + 0.1 * 6.318286895751953
Epoch 510, val loss: 0.8083025217056274
Epoch 520, training loss: 0.6672902703285217 = 0.03727740794420242 + 0.1 * 6.30012845993042
Epoch 520, val loss: 0.817086935043335
Epoch 530, training loss: 0.6653468012809753 = 0.03498629853129387 + 0.1 * 6.303604602813721
Epoch 530, val loss: 0.8257883787155151
Epoch 540, training loss: 0.6623004078865051 = 0.032894305884838104 + 0.1 * 6.294060707092285
Epoch 540, val loss: 0.8343832492828369
Epoch 550, training loss: 0.6596274375915527 = 0.030975664034485817 + 0.1 * 6.286518096923828
Epoch 550, val loss: 0.842955470085144
Epoch 560, training loss: 0.6597261428833008 = 0.029212532564997673 + 0.1 * 6.305136203765869
Epoch 560, val loss: 0.8513526320457458
Epoch 570, training loss: 0.6558957695960999 = 0.027599938213825226 + 0.1 * 6.282958030700684
Epoch 570, val loss: 0.8596653342247009
Epoch 580, training loss: 0.6542766690254211 = 0.026113860309123993 + 0.1 * 6.281628131866455
Epoch 580, val loss: 0.8678502440452576
Epoch 590, training loss: 0.6518111228942871 = 0.024740256369113922 + 0.1 * 6.270708084106445
Epoch 590, val loss: 0.8758598566055298
Epoch 600, training loss: 0.6516901850700378 = 0.023468326777219772 + 0.1 * 6.2822184562683105
Epoch 600, val loss: 0.8837805986404419
Epoch 610, training loss: 0.6499955058097839 = 0.02229141816496849 + 0.1 * 6.277040481567383
Epoch 610, val loss: 0.8914722800254822
Epoch 620, training loss: 0.6474957466125488 = 0.02120289020240307 + 0.1 * 6.2629289627075195
Epoch 620, val loss: 0.8991165161132812
Epoch 630, training loss: 0.6473602056503296 = 0.02019071951508522 + 0.1 * 6.271694660186768
Epoch 630, val loss: 0.9065905809402466
Epoch 640, training loss: 0.6455308794975281 = 0.019249888136982918 + 0.1 * 6.2628092765808105
Epoch 640, val loss: 0.9138914346694946
Epoch 650, training loss: 0.6456010341644287 = 0.01837228238582611 + 0.1 * 6.272287368774414
Epoch 650, val loss: 0.9211438894271851
Epoch 660, training loss: 0.6429486870765686 = 0.017553184181451797 + 0.1 * 6.2539544105529785
Epoch 660, val loss: 0.9281687140464783
Epoch 670, training loss: 0.6426404118537903 = 0.01678796485066414 + 0.1 * 6.258524417877197
Epoch 670, val loss: 0.9351975917816162
Epoch 680, training loss: 0.6406394839286804 = 0.016072111204266548 + 0.1 * 6.245673656463623
Epoch 680, val loss: 0.9420468211174011
Epoch 690, training loss: 0.6393610239028931 = 0.01540264580398798 + 0.1 * 6.239583492279053
Epoch 690, val loss: 0.9487854242324829
Epoch 700, training loss: 0.6389578580856323 = 0.014775487594306469 + 0.1 * 6.241823196411133
Epoch 700, val loss: 0.9554372429847717
Epoch 710, training loss: 0.6380190253257751 = 0.014186163432896137 + 0.1 * 6.238328456878662
Epoch 710, val loss: 0.9619574546813965
Epoch 720, training loss: 0.6373155117034912 = 0.01363211777061224 + 0.1 * 6.2368340492248535
Epoch 720, val loss: 0.9683552980422974
Epoch 730, training loss: 0.6363075971603394 = 0.013112548738718033 + 0.1 * 6.231950283050537
Epoch 730, val loss: 0.9746748208999634
Epoch 740, training loss: 0.636663556098938 = 0.012622062116861343 + 0.1 * 6.240414619445801
Epoch 740, val loss: 0.9808831214904785
Epoch 750, training loss: 0.6349760293960571 = 0.012160040438175201 + 0.1 * 6.228159427642822
Epoch 750, val loss: 0.9869532585144043
Epoch 760, training loss: 0.6348993182182312 = 0.0117240771651268 + 0.1 * 6.231752395629883
Epoch 760, val loss: 0.9929670691490173
Epoch 770, training loss: 0.6339751482009888 = 0.011312329210340977 + 0.1 * 6.226628303527832
Epoch 770, val loss: 0.9988134503364563
Epoch 780, training loss: 0.6325299143791199 = 0.010922899469733238 + 0.1 * 6.216070175170898
Epoch 780, val loss: 1.0046336650848389
Epoch 790, training loss: 0.6323775053024292 = 0.010553586296737194 + 0.1 * 6.218238830566406
Epoch 790, val loss: 1.0103495121002197
Epoch 800, training loss: 0.6333542466163635 = 0.010203508660197258 + 0.1 * 6.231507778167725
Epoch 800, val loss: 1.0158830881118774
Epoch 810, training loss: 0.6317846179008484 = 0.009872190654277802 + 0.1 * 6.2191243171691895
Epoch 810, val loss: 1.0214475393295288
Epoch 820, training loss: 0.6329287886619568 = 0.009558238089084625 + 0.1 * 6.233705043792725
Epoch 820, val loss: 1.0268778800964355
Epoch 830, training loss: 0.6302587985992432 = 0.009259267710149288 + 0.1 * 6.209994792938232
Epoch 830, val loss: 1.0321629047393799
Epoch 840, training loss: 0.6309067010879517 = 0.00897557195276022 + 0.1 * 6.219311237335205
Epoch 840, val loss: 1.0374878644943237
Epoch 850, training loss: 0.6293141841888428 = 0.008704972453415394 + 0.1 * 6.20609188079834
Epoch 850, val loss: 1.042590618133545
Epoch 860, training loss: 0.6288423538208008 = 0.008447719737887383 + 0.1 * 6.203946113586426
Epoch 860, val loss: 1.0477081537246704
Epoch 870, training loss: 0.6285004019737244 = 0.008202532306313515 + 0.1 * 6.202978610992432
Epoch 870, val loss: 1.0527148246765137
Epoch 880, training loss: 0.628004789352417 = 0.007968668825924397 + 0.1 * 6.200361251831055
Epoch 880, val loss: 1.057648777961731
Epoch 890, training loss: 0.6277750730514526 = 0.007745583076030016 + 0.1 * 6.2002949714660645
Epoch 890, val loss: 1.0625550746917725
Epoch 900, training loss: 0.6282215118408203 = 0.007532255258411169 + 0.1 * 6.206892490386963
Epoch 900, val loss: 1.0672913789749146
Epoch 910, training loss: 0.6271995306015015 = 0.007328654173761606 + 0.1 * 6.198709011077881
Epoch 910, val loss: 1.0720582008361816
Epoch 920, training loss: 0.6263814568519592 = 0.007133814040571451 + 0.1 * 6.19247579574585
Epoch 920, val loss: 1.076689600944519
Epoch 930, training loss: 0.6270730495452881 = 0.006947220303118229 + 0.1 * 6.201258182525635
Epoch 930, val loss: 1.0812956094741821
Epoch 940, training loss: 0.6254981756210327 = 0.006768576800823212 + 0.1 * 6.187295913696289
Epoch 940, val loss: 1.0858176946640015
Epoch 950, training loss: 0.6252889037132263 = 0.006597377359867096 + 0.1 * 6.186914920806885
Epoch 950, val loss: 1.0902774333953857
Epoch 960, training loss: 0.6248233914375305 = 0.006433040834963322 + 0.1 * 6.183903217315674
Epoch 960, val loss: 1.094628095626831
Epoch 970, training loss: 0.6251457333564758 = 0.006275794468820095 + 0.1 * 6.188699245452881
Epoch 970, val loss: 1.0989470481872559
Epoch 980, training loss: 0.6247462034225464 = 0.006124640349298716 + 0.1 * 6.186215877532959
Epoch 980, val loss: 1.1031914949417114
Epoch 990, training loss: 0.6249436736106873 = 0.00597976753488183 + 0.1 * 6.189639091491699
Epoch 990, val loss: 1.107394814491272
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.9520
Flip ASR: 0.9467/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7770285606384277 = 1.9396413564682007 + 0.1 * 8.373872756958008
Epoch 0, val loss: 1.9410380125045776
Epoch 10, training loss: 2.767307758331299 = 1.929930567741394 + 0.1 * 8.373772621154785
Epoch 10, val loss: 1.9319415092468262
Epoch 20, training loss: 2.755733013153076 = 1.918416142463684 + 0.1 * 8.3731689453125
Epoch 20, val loss: 1.9207428693771362
Epoch 30, training loss: 2.739516019821167 = 1.9026283025741577 + 0.1 * 8.368876457214355
Epoch 30, val loss: 1.9049279689788818
Epoch 40, training loss: 2.7133989334106445 = 1.8794933557510376 + 0.1 * 8.339056015014648
Epoch 40, val loss: 1.8815711736679077
Epoch 50, training loss: 2.6601972579956055 = 1.8469247817993164 + 0.1 * 8.132723808288574
Epoch 50, val loss: 1.849551796913147
Epoch 60, training loss: 2.5693960189819336 = 1.8091115951538086 + 0.1 * 7.602843284606934
Epoch 60, val loss: 1.8139402866363525
Epoch 70, training loss: 2.498049020767212 = 1.7682702541351318 + 0.1 * 7.297786712646484
Epoch 70, val loss: 1.7765003442764282
Epoch 80, training loss: 2.429657459259033 = 1.72281813621521 + 0.1 * 7.068394184112549
Epoch 80, val loss: 1.736299991607666
Epoch 90, training loss: 2.3592231273651123 = 1.6678862571716309 + 0.1 * 6.9133687019348145
Epoch 90, val loss: 1.6886473894119263
Epoch 100, training loss: 2.2800862789154053 = 1.5959510803222656 + 0.1 * 6.841351509094238
Epoch 100, val loss: 1.6264138221740723
Epoch 110, training loss: 2.183945655822754 = 1.5046586990356445 + 0.1 * 6.792868614196777
Epoch 110, val loss: 1.5495604276657104
Epoch 120, training loss: 2.072479009628296 = 1.3975298404693604 + 0.1 * 6.749491214752197
Epoch 120, val loss: 1.461924433708191
Epoch 130, training loss: 1.9549548625946045 = 1.2840888500213623 + 0.1 * 6.70866060256958
Epoch 130, val loss: 1.3691809177398682
Epoch 140, training loss: 1.844041109085083 = 1.1765698194503784 + 0.1 * 6.674713134765625
Epoch 140, val loss: 1.2827508449554443
Epoch 150, training loss: 1.7493852376937866 = 1.083464503288269 + 0.1 * 6.659207344055176
Epoch 150, val loss: 1.2093297243118286
Epoch 160, training loss: 1.6706668138504028 = 1.006964921951294 + 0.1 * 6.63701868057251
Epoch 160, val loss: 1.1512250900268555
Epoch 170, training loss: 1.6034221649169922 = 0.9413381814956665 + 0.1 * 6.620839595794678
Epoch 170, val loss: 1.1032389402389526
Epoch 180, training loss: 1.5427086353302002 = 0.8817223310470581 + 0.1 * 6.609862804412842
Epoch 180, val loss: 1.0615508556365967
Epoch 190, training loss: 1.4848384857177734 = 0.8251661062240601 + 0.1 * 6.5967230796813965
Epoch 190, val loss: 1.0235143899917603
Epoch 200, training loss: 1.4278523921966553 = 0.7693848609924316 + 0.1 * 6.584674835205078
Epoch 200, val loss: 0.9868600368499756
Epoch 210, training loss: 1.371823787689209 = 0.7145764827728271 + 0.1 * 6.57247257232666
Epoch 210, val loss: 0.9522523283958435
Epoch 220, training loss: 1.319091796875 = 0.6619865894317627 + 0.1 * 6.571052074432373
Epoch 220, val loss: 0.920877993106842
Epoch 230, training loss: 1.2684199810028076 = 0.6132122874259949 + 0.1 * 6.552077293395996
Epoch 230, val loss: 0.8946920037269592
Epoch 240, training loss: 1.221665382385254 = 0.5678117275238037 + 0.1 * 6.538536071777344
Epoch 240, val loss: 0.8734134435653687
Epoch 250, training loss: 1.1777307987213135 = 0.525216281414032 + 0.1 * 6.525144577026367
Epoch 250, val loss: 0.8561537861824036
Epoch 260, training loss: 1.1367026567459106 = 0.4847768247127533 + 0.1 * 6.51925802230835
Epoch 260, val loss: 0.8419429063796997
Epoch 270, training loss: 1.0969946384429932 = 0.44621968269348145 + 0.1 * 6.507750034332275
Epoch 270, val loss: 0.8299022316932678
Epoch 280, training loss: 1.0589406490325928 = 0.40913182497024536 + 0.1 * 6.498087406158447
Epoch 280, val loss: 0.8192868828773499
Epoch 290, training loss: 1.0225385427474976 = 0.37351515889167786 + 0.1 * 6.490233898162842
Epoch 290, val loss: 0.8096035718917847
Epoch 300, training loss: 0.988656759262085 = 0.33986833691596985 + 0.1 * 6.487884044647217
Epoch 300, val loss: 0.8010973930358887
Epoch 310, training loss: 0.9563642740249634 = 0.30829355120658875 + 0.1 * 6.48070764541626
Epoch 310, val loss: 0.7938652038574219
Epoch 320, training loss: 0.9264897704124451 = 0.2793504595756531 + 0.1 * 6.47139310836792
Epoch 320, val loss: 0.7883433699607849
Epoch 330, training loss: 0.8993854522705078 = 0.2531258463859558 + 0.1 * 6.4625959396362305
Epoch 330, val loss: 0.7848232984542847
Epoch 340, training loss: 0.8752522468566895 = 0.22954639792442322 + 0.1 * 6.457057952880859
Epoch 340, val loss: 0.7831985950469971
Epoch 350, training loss: 0.8554013967514038 = 0.20854075253009796 + 0.1 * 6.468606472015381
Epoch 350, val loss: 0.7836394906044006
Epoch 360, training loss: 0.8345047831535339 = 0.190008744597435 + 0.1 * 6.444960117340088
Epoch 360, val loss: 0.785955548286438
Epoch 370, training loss: 0.8179317712783813 = 0.17355293035507202 + 0.1 * 6.443788528442383
Epoch 370, val loss: 0.7899336218833923
Epoch 380, training loss: 0.8027212619781494 = 0.15902666747570038 + 0.1 * 6.43694543838501
Epoch 380, val loss: 0.795367419719696
Epoch 390, training loss: 0.7884891033172607 = 0.14610856771469116 + 0.1 * 6.423805236816406
Epoch 390, val loss: 0.8021716475486755
Epoch 400, training loss: 0.7766920924186707 = 0.13455845415592194 + 0.1 * 6.4213361740112305
Epoch 400, val loss: 0.810106098651886
Epoch 410, training loss: 0.766424834728241 = 0.12423361092805862 + 0.1 * 6.42191219329834
Epoch 410, val loss: 0.8188577890396118
Epoch 420, training loss: 0.7558375597000122 = 0.11495070904493332 + 0.1 * 6.408868312835693
Epoch 420, val loss: 0.8283608555793762
Epoch 430, training loss: 0.746491014957428 = 0.10651932656764984 + 0.1 * 6.399716377258301
Epoch 430, val loss: 0.8383764028549194
Epoch 440, training loss: 0.738358736038208 = 0.09880036115646362 + 0.1 * 6.395583629608154
Epoch 440, val loss: 0.8487956523895264
Epoch 450, training loss: 0.7313057780265808 = 0.09171182662248611 + 0.1 * 6.395939350128174
Epoch 450, val loss: 0.8592973351478577
Epoch 460, training loss: 0.7236681580543518 = 0.08520511537790298 + 0.1 * 6.38463020324707
Epoch 460, val loss: 0.8698665499687195
Epoch 470, training loss: 0.7168735265731812 = 0.07919999957084656 + 0.1 * 6.376735687255859
Epoch 470, val loss: 0.8804953098297119
Epoch 480, training loss: 0.7138963937759399 = 0.07361841946840286 + 0.1 * 6.402779579162598
Epoch 480, val loss: 0.8910346627235413
Epoch 490, training loss: 0.7063540816307068 = 0.0684666559100151 + 0.1 * 6.378873825073242
Epoch 490, val loss: 0.9016208648681641
Epoch 500, training loss: 0.7010281682014465 = 0.06371408700942993 + 0.1 * 6.373140811920166
Epoch 500, val loss: 0.911957859992981
Epoch 510, training loss: 0.695397138595581 = 0.0593562126159668 + 0.1 * 6.360409259796143
Epoch 510, val loss: 0.9224182963371277
Epoch 520, training loss: 0.6907868981361389 = 0.05530746281147003 + 0.1 * 6.354794502258301
Epoch 520, val loss: 0.932877242565155
Epoch 530, training loss: 0.6867738366127014 = 0.0515378899872303 + 0.1 * 6.352359771728516
Epoch 530, val loss: 0.9431561231613159
Epoch 540, training loss: 0.6830815672874451 = 0.04799879714846611 + 0.1 * 6.350827693939209
Epoch 540, val loss: 0.9532510042190552
Epoch 550, training loss: 0.6789640188217163 = 0.04457682743668556 + 0.1 * 6.3438720703125
Epoch 550, val loss: 0.9631385803222656
Epoch 560, training loss: 0.674771785736084 = 0.04118860512971878 + 0.1 * 6.335831642150879
Epoch 560, val loss: 0.9726898074150085
Epoch 570, training loss: 0.6737322807312012 = 0.03795141354203224 + 0.1 * 6.357808589935303
Epoch 570, val loss: 0.9817219972610474
Epoch 580, training loss: 0.6681797504425049 = 0.035078536719083786 + 0.1 * 6.33101224899292
Epoch 580, val loss: 0.9905153512954712
Epoch 590, training loss: 0.6658518314361572 = 0.03262701630592346 + 0.1 * 6.332248210906982
Epoch 590, val loss: 0.9990834593772888
Epoch 600, training loss: 0.6629467010498047 = 0.03051075153052807 + 0.1 * 6.32435941696167
Epoch 600, val loss: 1.0075160264968872
Epoch 610, training loss: 0.6609227657318115 = 0.028653301298618317 + 0.1 * 6.322694301605225
Epoch 610, val loss: 1.0158371925354004
Epoch 620, training loss: 0.6578323841094971 = 0.026997854933142662 + 0.1 * 6.308344841003418
Epoch 620, val loss: 1.02413809299469
Epoch 630, training loss: 0.6566450595855713 = 0.025497186928987503 + 0.1 * 6.311478614807129
Epoch 630, val loss: 1.0323649644851685
Epoch 640, training loss: 0.6542696356773376 = 0.02412930503487587 + 0.1 * 6.301403045654297
Epoch 640, val loss: 1.0405100584030151
Epoch 650, training loss: 0.6532215476036072 = 0.02287503518164158 + 0.1 * 6.303465366363525
Epoch 650, val loss: 1.0486599206924438
Epoch 660, training loss: 0.6515748500823975 = 0.0217201579362154 + 0.1 * 6.29854679107666
Epoch 660, val loss: 1.05666983127594
Epoch 670, training loss: 0.649776041507721 = 0.020655520260334015 + 0.1 * 6.291205406188965
Epoch 670, val loss: 1.064589023590088
Epoch 680, training loss: 0.649380624294281 = 0.0196719728410244 + 0.1 * 6.297086238861084
Epoch 680, val loss: 1.0722583532333374
Epoch 690, training loss: 0.6464431881904602 = 0.018762169405817986 + 0.1 * 6.276810169219971
Epoch 690, val loss: 1.0799182653427124
Epoch 700, training loss: 0.6453196406364441 = 0.017914386466145515 + 0.1 * 6.274052143096924
Epoch 700, val loss: 1.0873852968215942
Epoch 710, training loss: 0.6453840136528015 = 0.017123602330684662 + 0.1 * 6.282604217529297
Epoch 710, val loss: 1.094652533531189
Epoch 720, training loss: 0.6452853083610535 = 0.016386104747653008 + 0.1 * 6.288991928100586
Epoch 720, val loss: 1.101883053779602
Epoch 730, training loss: 0.6431268453598022 = 0.01569744013249874 + 0.1 * 6.274294376373291
Epoch 730, val loss: 1.1090480089187622
Epoch 740, training loss: 0.6415878534317017 = 0.015053054317831993 + 0.1 * 6.265347957611084
Epoch 740, val loss: 1.1160023212432861
Epoch 750, training loss: 0.641014575958252 = 0.014447085559368134 + 0.1 * 6.265674591064453
Epoch 750, val loss: 1.1227731704711914
Epoch 760, training loss: 0.6408798694610596 = 0.013878419063985348 + 0.1 * 6.27001428604126
Epoch 760, val loss: 1.1293716430664062
Epoch 770, training loss: 0.6389344930648804 = 0.013345956802368164 + 0.1 * 6.255885124206543
Epoch 770, val loss: 1.1359331607818604
Epoch 780, training loss: 0.6384363174438477 = 0.012845692224800587 + 0.1 * 6.255906105041504
Epoch 780, val loss: 1.1423530578613281
Epoch 790, training loss: 0.6380359530448914 = 0.012373722158372402 + 0.1 * 6.256621837615967
Epoch 790, val loss: 1.1486060619354248
Epoch 800, training loss: 0.6365089416503906 = 0.011928989551961422 + 0.1 * 6.245799541473389
Epoch 800, val loss: 1.1546415090560913
Epoch 810, training loss: 0.6364691257476807 = 0.011509031057357788 + 0.1 * 6.249600887298584
Epoch 810, val loss: 1.1607457399368286
Epoch 820, training loss: 0.6348166465759277 = 0.011111731640994549 + 0.1 * 6.237049102783203
Epoch 820, val loss: 1.166581392288208
Epoch 830, training loss: 0.6347708106040955 = 0.010735767893493176 + 0.1 * 6.240350723266602
Epoch 830, val loss: 1.1724061965942383
Epoch 840, training loss: 0.6351990699768066 = 0.010378705337643623 + 0.1 * 6.248203277587891
Epoch 840, val loss: 1.1780564785003662
Epoch 850, training loss: 0.6336652636528015 = 0.01004075538367033 + 0.1 * 6.236245155334473
Epoch 850, val loss: 1.1835366487503052
Epoch 860, training loss: 0.6331017017364502 = 0.009719640016555786 + 0.1 * 6.233820915222168
Epoch 860, val loss: 1.1889535188674927
Epoch 870, training loss: 0.6324636340141296 = 0.009414969012141228 + 0.1 * 6.2304863929748535
Epoch 870, val loss: 1.1942670345306396
Epoch 880, training loss: 0.6329529881477356 = 0.009125745855271816 + 0.1 * 6.238272190093994
Epoch 880, val loss: 1.19944429397583
Epoch 890, training loss: 0.6316705346107483 = 0.008850636892020702 + 0.1 * 6.228199005126953
Epoch 890, val loss: 1.204572081565857
Epoch 900, training loss: 0.6320725083351135 = 0.008588917553424835 + 0.1 * 6.234835624694824
Epoch 900, val loss: 1.2095451354980469
Epoch 910, training loss: 0.6308520436286926 = 0.008339759893715382 + 0.1 * 6.225122928619385
Epoch 910, val loss: 1.2143763303756714
Epoch 920, training loss: 0.6304488182067871 = 0.008102558553218842 + 0.1 * 6.2234625816345215
Epoch 920, val loss: 1.219202995300293
Epoch 930, training loss: 0.6298737525939941 = 0.00787618849426508 + 0.1 * 6.219975471496582
Epoch 930, val loss: 1.2238597869873047
Epoch 940, training loss: 0.6290522813796997 = 0.007659859489649534 + 0.1 * 6.213924407958984
Epoch 940, val loss: 1.2283754348754883
Epoch 950, training loss: 0.6290967464447021 = 0.00745340995490551 + 0.1 * 6.216433525085449
Epoch 950, val loss: 1.2329071760177612
Epoch 960, training loss: 0.6282132863998413 = 0.007255880627781153 + 0.1 * 6.209574222564697
Epoch 960, val loss: 1.2372641563415527
Epoch 970, training loss: 0.6287499070167542 = 0.007066943217068911 + 0.1 * 6.216829299926758
Epoch 970, val loss: 1.2416683435440063
Epoch 980, training loss: 0.6290949583053589 = 0.006885713431984186 + 0.1 * 6.222092628479004
Epoch 980, val loss: 1.2458480596542358
Epoch 990, training loss: 0.6273096203804016 = 0.006711834575980902 + 0.1 * 6.205977439880371
Epoch 990, val loss: 1.2500747442245483
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6347
Flip ASR: 0.5822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7825379371643066 = 1.9451459646224976 + 0.1 * 8.373920440673828
Epoch 0, val loss: 1.93514883518219
Epoch 10, training loss: 2.7716286182403564 = 1.9342454671859741 + 0.1 * 8.373831748962402
Epoch 10, val loss: 1.9246150255203247
Epoch 20, training loss: 2.7578372955322266 = 1.9204987287521362 + 0.1 * 8.373384475708008
Epoch 20, val loss: 1.9106931686401367
Epoch 30, training loss: 2.737907648086548 = 1.9009039402008057 + 0.1 * 8.370036125183105
Epoch 30, val loss: 1.8903822898864746
Epoch 40, training loss: 2.706951856613159 = 1.872119665145874 + 0.1 * 8.348321914672852
Epoch 40, val loss: 1.8607097864151
Epoch 50, training loss: 2.6566474437713623 = 1.8334535360336304 + 0.1 * 8.231939315795898
Epoch 50, val loss: 1.8230407238006592
Epoch 60, training loss: 2.592623472213745 = 1.792781949043274 + 0.1 * 7.998414516448975
Epoch 60, val loss: 1.7876683473587036
Epoch 70, training loss: 2.5258054733276367 = 1.7546100616455078 + 0.1 * 7.711954116821289
Epoch 70, val loss: 1.7566393613815308
Epoch 80, training loss: 2.438894271850586 = 1.7106059789657593 + 0.1 * 7.2828826904296875
Epoch 80, val loss: 1.7189395427703857
Epoch 90, training loss: 2.355663776397705 = 1.6547150611877441 + 0.1 * 7.009487152099609
Epoch 90, val loss: 1.6703519821166992
Epoch 100, training loss: 2.2687854766845703 = 1.5805577039718628 + 0.1 * 6.882277011871338
Epoch 100, val loss: 1.60502028465271
Epoch 110, training loss: 2.17232608795166 = 1.4913095235824585 + 0.1 * 6.810164451599121
Epoch 110, val loss: 1.5272376537322998
Epoch 120, training loss: 2.0720977783203125 = 1.3957545757293701 + 0.1 * 6.763432025909424
Epoch 120, val loss: 1.448490023612976
Epoch 130, training loss: 1.9723316431045532 = 1.2997065782546997 + 0.1 * 6.726250648498535
Epoch 130, val loss: 1.3741941452026367
Epoch 140, training loss: 1.8767585754394531 = 1.2068897485733032 + 0.1 * 6.698688507080078
Epoch 140, val loss: 1.3064583539962769
Epoch 150, training loss: 1.7877509593963623 = 1.1199439764022827 + 0.1 * 6.678070545196533
Epoch 150, val loss: 1.2444771528244019
Epoch 160, training loss: 1.7048275470733643 = 1.03886079788208 + 0.1 * 6.659666538238525
Epoch 160, val loss: 1.1872053146362305
Epoch 170, training loss: 1.6293858289718628 = 0.9638721346855164 + 0.1 * 6.655136585235596
Epoch 170, val loss: 1.1345467567443848
Epoch 180, training loss: 1.559157371520996 = 0.8956551551818848 + 0.1 * 6.635021209716797
Epoch 180, val loss: 1.0867568254470825
Epoch 190, training loss: 1.4940223693847656 = 0.8316158652305603 + 0.1 * 6.6240644454956055
Epoch 190, val loss: 1.0415998697280884
Epoch 200, training loss: 1.431363582611084 = 0.7699377536773682 + 0.1 * 6.614258766174316
Epoch 200, val loss: 0.9981684684753418
Epoch 210, training loss: 1.3715510368347168 = 0.7103186845779419 + 0.1 * 6.612323760986328
Epoch 210, val loss: 0.9562694430351257
Epoch 220, training loss: 1.3142943382263184 = 0.6542545557022095 + 0.1 * 6.600397109985352
Epoch 220, val loss: 0.9170033931732178
Epoch 230, training loss: 1.261070728302002 = 0.6019201874732971 + 0.1 * 6.591506004333496
Epoch 230, val loss: 0.8813171982765198
Epoch 240, training loss: 1.2114299535751343 = 0.5531914234161377 + 0.1 * 6.582385063171387
Epoch 240, val loss: 0.8489026427268982
Epoch 250, training loss: 1.164632797241211 = 0.5073449611663818 + 0.1 * 6.572877407073975
Epoch 250, val loss: 0.8197149634361267
Epoch 260, training loss: 1.1196569204330444 = 0.4636327922344208 + 0.1 * 6.560240745544434
Epoch 260, val loss: 0.793235719203949
Epoch 270, training loss: 1.0800405740737915 = 0.4215448796749115 + 0.1 * 6.584956645965576
Epoch 270, val loss: 0.7696961760520935
Epoch 280, training loss: 1.0364588499069214 = 0.3812483251094818 + 0.1 * 6.552104949951172
Epoch 280, val loss: 0.7494410872459412
Epoch 290, training loss: 0.9961947202682495 = 0.34218403697013855 + 0.1 * 6.540107250213623
Epoch 290, val loss: 0.7322508692741394
Epoch 300, training loss: 0.9581409692764282 = 0.3048621118068695 + 0.1 * 6.532788276672363
Epoch 300, val loss: 0.7182565331459045
Epoch 310, training loss: 0.9232057332992554 = 0.27026358246803284 + 0.1 * 6.529421806335449
Epoch 310, val loss: 0.7077862024307251
Epoch 320, training loss: 0.8905919790267944 = 0.23893706500530243 + 0.1 * 6.5165486335754395
Epoch 320, val loss: 0.7004857659339905
Epoch 330, training loss: 0.8621928095817566 = 0.2109588235616684 + 0.1 * 6.5123395919799805
Epoch 330, val loss: 0.6961202025413513
Epoch 340, training loss: 0.8364819884300232 = 0.18617033958435059 + 0.1 * 6.503116130828857
Epoch 340, val loss: 0.694290280342102
Epoch 350, training loss: 0.8140842914581299 = 0.16422797739505768 + 0.1 * 6.498563289642334
Epoch 350, val loss: 0.6944414377212524
Epoch 360, training loss: 0.7964637875556946 = 0.14509524405002594 + 0.1 * 6.513685703277588
Epoch 360, val loss: 0.696368396282196
Epoch 370, training loss: 0.7767868638038635 = 0.12868106365203857 + 0.1 * 6.481058120727539
Epoch 370, val loss: 0.6996402144432068
Epoch 380, training loss: 0.7610815167427063 = 0.11453746259212494 + 0.1 * 6.465440273284912
Epoch 380, val loss: 0.7042905688285828
Epoch 390, training loss: 0.7498782873153687 = 0.10236121714115143 + 0.1 * 6.475170612335205
Epoch 390, val loss: 0.7100901007652283
Epoch 400, training loss: 0.7383436560630798 = 0.0919208899140358 + 0.1 * 6.464227199554443
Epoch 400, val loss: 0.7168055772781372
Epoch 410, training loss: 0.7277750372886658 = 0.08294343948364258 + 0.1 * 6.448315620422363
Epoch 410, val loss: 0.7243228554725647
Epoch 420, training loss: 0.7185506224632263 = 0.0751972571015358 + 0.1 * 6.433533191680908
Epoch 420, val loss: 0.7322606444358826
Epoch 430, training loss: 0.7132573127746582 = 0.06845235824584961 + 0.1 * 6.448049545288086
Epoch 430, val loss: 0.740642786026001
Epoch 440, training loss: 0.7066559791564941 = 0.06257113069295883 + 0.1 * 6.440848350524902
Epoch 440, val loss: 0.7492899298667908
Epoch 450, training loss: 0.698662281036377 = 0.05742131546139717 + 0.1 * 6.41240930557251
Epoch 450, val loss: 0.7581332325935364
Epoch 460, training loss: 0.6928200125694275 = 0.052867792546749115 + 0.1 * 6.399522304534912
Epoch 460, val loss: 0.7671586275100708
Epoch 470, training loss: 0.6880530714988708 = 0.048824552446603775 + 0.1 * 6.392284870147705
Epoch 470, val loss: 0.7762464880943298
Epoch 480, training loss: 0.687021791934967 = 0.04522592946887016 + 0.1 * 6.417958736419678
Epoch 480, val loss: 0.7853509783744812
Epoch 490, training loss: 0.6804649233818054 = 0.0420190654695034 + 0.1 * 6.384458541870117
Epoch 490, val loss: 0.7944017648696899
Epoch 500, training loss: 0.6764479875564575 = 0.03913525119423866 + 0.1 * 6.373126983642578
Epoch 500, val loss: 0.8035330772399902
Epoch 510, training loss: 0.6732762455940247 = 0.03653567656874657 + 0.1 * 6.367405414581299
Epoch 510, val loss: 0.8124914765357971
Epoch 520, training loss: 0.670102596282959 = 0.0341825895011425 + 0.1 * 6.3592000007629395
Epoch 520, val loss: 0.8215438723564148
Epoch 530, training loss: 0.6705430150032043 = 0.03204592689871788 + 0.1 * 6.3849711418151855
Epoch 530, val loss: 0.8304319977760315
Epoch 540, training loss: 0.6650123000144958 = 0.03011282905936241 + 0.1 * 6.348994731903076
Epoch 540, val loss: 0.8391693234443665
Epoch 550, training loss: 0.6620782613754272 = 0.028348490595817566 + 0.1 * 6.337297439575195
Epoch 550, val loss: 0.8478574752807617
Epoch 560, training loss: 0.6613565683364868 = 0.02672763355076313 + 0.1 * 6.346289157867432
Epoch 560, val loss: 0.8564491271972656
Epoch 570, training loss: 0.6584512591362 = 0.02524225413799286 + 0.1 * 6.332089900970459
Epoch 570, val loss: 0.8648895025253296
Epoch 580, training loss: 0.6587573289871216 = 0.023876309394836426 + 0.1 * 6.348810195922852
Epoch 580, val loss: 0.8732964992523193
Epoch 590, training loss: 0.6549067497253418 = 0.0226197000592947 + 0.1 * 6.322870254516602
Epoch 590, val loss: 0.881560742855072
Epoch 600, training loss: 0.6536650061607361 = 0.02145989052951336 + 0.1 * 6.322051048278809
Epoch 600, val loss: 0.8897051215171814
Epoch 610, training loss: 0.6511561274528503 = 0.02038750611245632 + 0.1 * 6.307685852050781
Epoch 610, val loss: 0.897747814655304
Epoch 620, training loss: 0.6528589725494385 = 0.019394734874367714 + 0.1 * 6.334641933441162
Epoch 620, val loss: 0.9055782556533813
Epoch 630, training loss: 0.649116039276123 = 0.018475260585546494 + 0.1 * 6.306407928466797
Epoch 630, val loss: 0.9133732914924622
Epoch 640, training loss: 0.6481735706329346 = 0.017622489482164383 + 0.1 * 6.305510997772217
Epoch 640, val loss: 0.9210175275802612
Epoch 650, training loss: 0.6474863290786743 = 0.01682809181511402 + 0.1 * 6.306581974029541
Epoch 650, val loss: 0.9284797310829163
Epoch 660, training loss: 0.6464920043945312 = 0.016087360680103302 + 0.1 * 6.304046630859375
Epoch 660, val loss: 0.9358462691307068
Epoch 670, training loss: 0.6445983052253723 = 0.015396728180348873 + 0.1 * 6.29201602935791
Epoch 670, val loss: 0.9431659579277039
Epoch 680, training loss: 0.6451520323753357 = 0.014750097878277302 + 0.1 * 6.304018974304199
Epoch 680, val loss: 0.95036780834198
Epoch 690, training loss: 0.643657922744751 = 0.014144623652100563 + 0.1 * 6.295133113861084
Epoch 690, val loss: 0.9573214054107666
Epoch 700, training loss: 0.6428005695343018 = 0.013578259386122227 + 0.1 * 6.29222297668457
Epoch 700, val loss: 0.9642311334609985
Epoch 710, training loss: 0.640953779220581 = 0.013047860004007816 + 0.1 * 6.279058933258057
Epoch 710, val loss: 0.9710046648979187
Epoch 720, training loss: 0.6408650279045105 = 0.012548444792628288 + 0.1 * 6.28316593170166
Epoch 720, val loss: 0.9777675867080688
Epoch 730, training loss: 0.6394448280334473 = 0.012078099884092808 + 0.1 * 6.273667335510254
Epoch 730, val loss: 0.9842549562454224
Epoch 740, training loss: 0.6384077668190002 = 0.011635223403573036 + 0.1 * 6.267724990844727
Epoch 740, val loss: 0.990753710269928
Epoch 750, training loss: 0.6383659243583679 = 0.01121677365154028 + 0.1 * 6.271491527557373
Epoch 750, val loss: 0.9971670508384705
Epoch 760, training loss: 0.6377317309379578 = 0.010822736658155918 + 0.1 * 6.269090175628662
Epoch 760, val loss: 1.0033984184265137
Epoch 770, training loss: 0.6367687582969666 = 0.010451407171785831 + 0.1 * 6.2631731033325195
Epoch 770, val loss: 1.0095632076263428
Epoch 780, training loss: 0.635615885257721 = 0.010099291801452637 + 0.1 * 6.2551655769348145
Epoch 780, val loss: 1.0156441926956177
Epoch 790, training loss: 0.6354156732559204 = 0.009765060618519783 + 0.1 * 6.256505489349365
Epoch 790, val loss: 1.0216422080993652
Epoch 800, training loss: 0.636242151260376 = 0.00944790244102478 + 0.1 * 6.267941951751709
Epoch 800, val loss: 1.027538776397705
Epoch 810, training loss: 0.6347925066947937 = 0.009148052893579006 + 0.1 * 6.256444454193115
Epoch 810, val loss: 1.033244013786316
Epoch 820, training loss: 0.6332979202270508 = 0.008863145485520363 + 0.1 * 6.24434757232666
Epoch 820, val loss: 1.0389490127563477
Epoch 830, training loss: 0.6328325867652893 = 0.008593847043812275 + 0.1 * 6.242387294769287
Epoch 830, val loss: 1.0445492267608643
Epoch 840, training loss: 0.6324482560157776 = 0.008336490020155907 + 0.1 * 6.241117477416992
Epoch 840, val loss: 1.0500441789627075
Epoch 850, training loss: 0.6316757798194885 = 0.008090386167168617 + 0.1 * 6.235853672027588
Epoch 850, val loss: 1.0555016994476318
Epoch 860, training loss: 0.631827175617218 = 0.007855755276978016 + 0.1 * 6.2397141456604
Epoch 860, val loss: 1.060773253440857
Epoch 870, training loss: 0.6340673565864563 = 0.00763341598212719 + 0.1 * 6.264339447021484
Epoch 870, val loss: 1.0660046339035034
Epoch 880, training loss: 0.6303063631057739 = 0.007421558257192373 + 0.1 * 6.228847980499268
Epoch 880, val loss: 1.0712224245071411
Epoch 890, training loss: 0.6313637495040894 = 0.007218749262392521 + 0.1 * 6.241449356079102
Epoch 890, val loss: 1.07638680934906
Epoch 900, training loss: 0.6302886009216309 = 0.007024431601166725 + 0.1 * 6.232641696929932
Epoch 900, val loss: 1.0813608169555664
Epoch 910, training loss: 0.6296559572219849 = 0.006838694680482149 + 0.1 * 6.228172779083252
Epoch 910, val loss: 1.0862526893615723
Epoch 920, training loss: 0.629054844379425 = 0.006661285180598497 + 0.1 * 6.223935604095459
Epoch 920, val loss: 1.091139554977417
Epoch 930, training loss: 0.6285622715950012 = 0.006491055246442556 + 0.1 * 6.220712184906006
Epoch 930, val loss: 1.0959856510162354
Epoch 940, training loss: 0.6299616098403931 = 0.006327831652015448 + 0.1 * 6.236338138580322
Epoch 940, val loss: 1.1007732152938843
Epoch 950, training loss: 0.6281652450561523 = 0.006171743385493755 + 0.1 * 6.219934940338135
Epoch 950, val loss: 1.10537850856781
Epoch 960, training loss: 0.6290852427482605 = 0.006021969951689243 + 0.1 * 6.230632305145264
Epoch 960, val loss: 1.1100172996520996
Epoch 970, training loss: 0.6271578073501587 = 0.005878148600459099 + 0.1 * 6.212796211242676
Epoch 970, val loss: 1.1145572662353516
Epoch 980, training loss: 0.6270896196365356 = 0.005740078166127205 + 0.1 * 6.213495254516602
Epoch 980, val loss: 1.1190595626831055
Epoch 990, training loss: 0.6283734440803528 = 0.005607052240520716 + 0.1 * 6.227663516998291
Epoch 990, val loss: 1.1234462261199951
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9077
Flip ASR: 0.8889/225 nodes
The final ASR:0.83149, 0.14033, Accuracy:0.81235, 0.00761
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9460])
updated graph: torch.Size([2, 10482])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97786, 0.00797, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.771233081817627 = 1.9338585138320923 + 0.1 * 8.373746871948242
Epoch 0, val loss: 1.9123376607894897
Epoch 10, training loss: 2.7617740631103516 = 1.9244219064712524 + 0.1 * 8.37352180480957
Epoch 10, val loss: 1.903975009918213
Epoch 20, training loss: 2.749950885772705 = 1.912733793258667 + 0.1 * 8.372169494628906
Epoch 20, val loss: 1.893326759338379
Epoch 30, training loss: 2.7323741912841797 = 1.8960578441619873 + 0.1 * 8.363163948059082
Epoch 30, val loss: 1.8779793977737427
Epoch 40, training loss: 2.701911449432373 = 1.871130108833313 + 0.1 * 8.307814598083496
Epoch 40, val loss: 1.8554760217666626
Epoch 50, training loss: 2.6341817378997803 = 1.8374555110931396 + 0.1 * 7.967261791229248
Epoch 50, val loss: 1.8267825841903687
Epoch 60, training loss: 2.5519461631774902 = 1.801388144493103 + 0.1 * 7.505580902099609
Epoch 60, val loss: 1.7979686260223389
Epoch 70, training loss: 2.4748470783233643 = 1.764952540397644 + 0.1 * 7.098945140838623
Epoch 70, val loss: 1.770732045173645
Epoch 80, training loss: 2.4155259132385254 = 1.726212739944458 + 0.1 * 6.893131256103516
Epoch 80, val loss: 1.741040587425232
Epoch 90, training loss: 2.3552286624908447 = 1.6749662160873413 + 0.1 * 6.802624225616455
Epoch 90, val loss: 1.697480320930481
Epoch 100, training loss: 2.2808029651641846 = 1.6060291528701782 + 0.1 * 6.747738361358643
Epoch 100, val loss: 1.6389662027359009
Epoch 110, training loss: 2.1893515586853027 = 1.5189521312713623 + 0.1 * 6.703993797302246
Epoch 110, val loss: 1.5676695108413696
Epoch 120, training loss: 2.0867271423339844 = 1.419238567352295 + 0.1 * 6.6748857498168945
Epoch 120, val loss: 1.4872230291366577
Epoch 130, training loss: 1.9800779819488525 = 1.3147403001785278 + 0.1 * 6.653376579284668
Epoch 130, val loss: 1.4057813882827759
Epoch 140, training loss: 1.873950481414795 = 1.2104579210281372 + 0.1 * 6.6349263191223145
Epoch 140, val loss: 1.3261915445327759
Epoch 150, training loss: 1.7723443508148193 = 1.1104164123535156 + 0.1 * 6.619279861450195
Epoch 150, val loss: 1.2517313957214355
Epoch 160, training loss: 1.6794774532318115 = 1.018872857093811 + 0.1 * 6.606045246124268
Epoch 160, val loss: 1.1853692531585693
Epoch 170, training loss: 1.59482741355896 = 0.9353916645050049 + 0.1 * 6.594357490539551
Epoch 170, val loss: 1.1253516674041748
Epoch 180, training loss: 1.516146183013916 = 0.8577384948730469 + 0.1 * 6.584075927734375
Epoch 180, val loss: 1.0687692165374756
Epoch 190, training loss: 1.4402352571487427 = 0.782934844493866 + 0.1 * 6.573003768920898
Epoch 190, val loss: 1.0138270854949951
Epoch 200, training loss: 1.3670985698699951 = 0.7100844979286194 + 0.1 * 6.570140838623047
Epoch 200, val loss: 0.9601671099662781
Epoch 210, training loss: 1.296989917755127 = 0.6415576934814453 + 0.1 * 6.554322719573975
Epoch 210, val loss: 0.9107638597488403
Epoch 220, training loss: 1.2318100929260254 = 0.5776022672653198 + 0.1 * 6.542078018188477
Epoch 220, val loss: 0.8663011193275452
Epoch 230, training loss: 1.1725163459777832 = 0.5189688801765442 + 0.1 * 6.535473823547363
Epoch 230, val loss: 0.8283509612083435
Epoch 240, training loss: 1.1183366775512695 = 0.4662433862686157 + 0.1 * 6.520933151245117
Epoch 240, val loss: 0.7978317737579346
Epoch 250, training loss: 1.0693302154541016 = 0.4185316860675812 + 0.1 * 6.5079851150512695
Epoch 250, val loss: 0.7735675573348999
Epoch 260, training loss: 1.0272951126098633 = 0.37492138147354126 + 0.1 * 6.523736476898193
Epoch 260, val loss: 0.7543875575065613
Epoch 270, training loss: 0.9850224256515503 = 0.33546650409698486 + 0.1 * 6.495559215545654
Epoch 270, val loss: 0.739691436290741
Epoch 280, training loss: 0.9482982158660889 = 0.2993922531604767 + 0.1 * 6.4890594482421875
Epoch 280, val loss: 0.728776752948761
Epoch 290, training loss: 0.9147692322731018 = 0.2667004466056824 + 0.1 * 6.480687618255615
Epoch 290, val loss: 0.721557080745697
Epoch 300, training loss: 0.8842200040817261 = 0.23729102313518524 + 0.1 * 6.469289779663086
Epoch 300, val loss: 0.7176693677902222
Epoch 310, training loss: 0.8571153879165649 = 0.21113210916519165 + 0.1 * 6.459832668304443
Epoch 310, val loss: 0.7168716788291931
Epoch 320, training loss: 0.8346467018127441 = 0.18800893425941467 + 0.1 * 6.4663777351379395
Epoch 320, val loss: 0.7188461422920227
Epoch 330, training loss: 0.8123575448989868 = 0.16773170232772827 + 0.1 * 6.446258544921875
Epoch 330, val loss: 0.723240077495575
Epoch 340, training loss: 0.7943840026855469 = 0.14992466568946838 + 0.1 * 6.4445929527282715
Epoch 340, val loss: 0.7296784520149231
Epoch 350, training loss: 0.7786721587181091 = 0.13439889252185822 + 0.1 * 6.442732810974121
Epoch 350, val loss: 0.7378697991371155
Epoch 360, training loss: 0.7636532783508301 = 0.12086435407400131 + 0.1 * 6.427889347076416
Epoch 360, val loss: 0.7474341988563538
Epoch 370, training loss: 0.7523807287216187 = 0.10900766402482986 + 0.1 * 6.433730602264404
Epoch 370, val loss: 0.7580890655517578
Epoch 380, training loss: 0.7395443916320801 = 0.0986371785402298 + 0.1 * 6.409071922302246
Epoch 380, val loss: 0.769538938999176
Epoch 390, training loss: 0.73090660572052 = 0.0895065888762474 + 0.1 * 6.414000511169434
Epoch 390, val loss: 0.781593382358551
Epoch 400, training loss: 0.7208216786384583 = 0.08147436380386353 + 0.1 * 6.393473148345947
Epoch 400, val loss: 0.7940998673439026
Epoch 410, training loss: 0.7135178446769714 = 0.07436348497867584 + 0.1 * 6.391543388366699
Epoch 410, val loss: 0.8068297505378723
Epoch 420, training loss: 0.70587158203125 = 0.06805519759654999 + 0.1 * 6.3781633377075195
Epoch 420, val loss: 0.8199130296707153
Epoch 430, training loss: 0.6998689770698547 = 0.06242743507027626 + 0.1 * 6.374414920806885
Epoch 430, val loss: 0.8330143094062805
Epoch 440, training loss: 0.6937369704246521 = 0.057413842529058456 + 0.1 * 6.363231182098389
Epoch 440, val loss: 0.8463008999824524
Epoch 450, training loss: 0.6909475922584534 = 0.05291583761572838 + 0.1 * 6.380317687988281
Epoch 450, val loss: 0.8595378994941711
Epoch 460, training loss: 0.6840097904205322 = 0.048888806253671646 + 0.1 * 6.35120964050293
Epoch 460, val loss: 0.8727676272392273
Epoch 470, training loss: 0.6790130734443665 = 0.045263174921274185 + 0.1 * 6.337499141693115
Epoch 470, val loss: 0.8859212398529053
Epoch 480, training loss: 0.6764758825302124 = 0.04198744148015976 + 0.1 * 6.344884395599365
Epoch 480, val loss: 0.8989598751068115
Epoch 490, training loss: 0.6729004979133606 = 0.03904044255614281 + 0.1 * 6.3386006355285645
Epoch 490, val loss: 0.9117637872695923
Epoch 500, training loss: 0.6688454151153564 = 0.03638240694999695 + 0.1 * 6.324630260467529
Epoch 500, val loss: 0.9243679642677307
Epoch 510, training loss: 0.666114866733551 = 0.0339718721807003 + 0.1 * 6.321430206298828
Epoch 510, val loss: 0.9366254806518555
Epoch 520, training loss: 0.6630813479423523 = 0.031786222010850906 + 0.1 * 6.312951564788818
Epoch 520, val loss: 0.9486415386199951
Epoch 530, training loss: 0.6621240377426147 = 0.02979820780456066 + 0.1 * 6.323257923126221
Epoch 530, val loss: 0.9603505730628967
Epoch 540, training loss: 0.6585781574249268 = 0.027989985421299934 + 0.1 * 6.305881500244141
Epoch 540, val loss: 0.9716954231262207
Epoch 550, training loss: 0.6559963226318359 = 0.02634102664887905 + 0.1 * 6.296553134918213
Epoch 550, val loss: 0.9828105568885803
Epoch 560, training loss: 0.6541410684585571 = 0.024832021445035934 + 0.1 * 6.293090343475342
Epoch 560, val loss: 0.9934717416763306
Epoch 570, training loss: 0.6525062322616577 = 0.023455845192074776 + 0.1 * 6.29050350189209
Epoch 570, val loss: 1.0040186643600464
Epoch 580, training loss: 0.6513368487358093 = 0.022188978269696236 + 0.1 * 6.291478633880615
Epoch 580, val loss: 1.0141867399215698
Epoch 590, training loss: 0.6496564149856567 = 0.021024903282523155 + 0.1 * 6.286314964294434
Epoch 590, val loss: 1.0240341424942017
Epoch 600, training loss: 0.6473316550254822 = 0.019953297451138496 + 0.1 * 6.273783206939697
Epoch 600, val loss: 1.03378427028656
Epoch 610, training loss: 0.645801842212677 = 0.01896260678768158 + 0.1 * 6.268392086029053
Epoch 610, val loss: 1.0431180000305176
Epoch 620, training loss: 0.6456227898597717 = 0.018047573044896126 + 0.1 * 6.275752067565918
Epoch 620, val loss: 1.0523232221603394
Epoch 630, training loss: 0.6435019969940186 = 0.017200015485286713 + 0.1 * 6.263020038604736
Epoch 630, val loss: 1.0612170696258545
Epoch 640, training loss: 0.6424580812454224 = 0.016412531957030296 + 0.1 * 6.260455131530762
Epoch 640, val loss: 1.0699501037597656
Epoch 650, training loss: 0.6422248482704163 = 0.015680458396673203 + 0.1 * 6.265443801879883
Epoch 650, val loss: 1.0783758163452148
Epoch 660, training loss: 0.6402394771575928 = 0.014999810606241226 + 0.1 * 6.252396583557129
Epoch 660, val loss: 1.086701512336731
Epoch 670, training loss: 0.6389686465263367 = 0.01436377689242363 + 0.1 * 6.246048927307129
Epoch 670, val loss: 1.0947870016098022
Epoch 680, training loss: 0.6385987997055054 = 0.013769127428531647 + 0.1 * 6.24829626083374
Epoch 680, val loss: 1.1025397777557373
Epoch 690, training loss: 0.637007474899292 = 0.013215359300374985 + 0.1 * 6.237921237945557
Epoch 690, val loss: 1.1102412939071655
Epoch 700, training loss: 0.6364308595657349 = 0.01269499771296978 + 0.1 * 6.237358570098877
Epoch 700, val loss: 1.1177664995193481
Epoch 710, training loss: 0.6360176205635071 = 0.012205955572426319 + 0.1 * 6.23811674118042
Epoch 710, val loss: 1.1249797344207764
Epoch 720, training loss: 0.6354491710662842 = 0.01174876932054758 + 0.1 * 6.237003803253174
Epoch 720, val loss: 1.1321808099746704
Epoch 730, training loss: 0.6353638768196106 = 0.011317920871078968 + 0.1 * 6.240459442138672
Epoch 730, val loss: 1.1391651630401611
Epoch 740, training loss: 0.633186936378479 = 0.010912181809544563 + 0.1 * 6.222747325897217
Epoch 740, val loss: 1.145987868309021
Epoch 750, training loss: 0.6330555081367493 = 0.010529936291277409 + 0.1 * 6.225255489349365
Epoch 750, val loss: 1.152777075767517
Epoch 760, training loss: 0.6325809955596924 = 0.010167726315557957 + 0.1 * 6.224132537841797
Epoch 760, val loss: 1.1593517065048218
Epoch 770, training loss: 0.6311762928962708 = 0.009826074354350567 + 0.1 * 6.213502407073975
Epoch 770, val loss: 1.1658293008804321
Epoch 780, training loss: 0.6313671469688416 = 0.009502259083092213 + 0.1 * 6.218648910522461
Epoch 780, val loss: 1.1721222400665283
Epoch 790, training loss: 0.6304910778999329 = 0.009196008555591106 + 0.1 * 6.212950706481934
Epoch 790, val loss: 1.1782587766647339
Epoch 800, training loss: 0.6308215856552124 = 0.008906245231628418 + 0.1 * 6.21915340423584
Epoch 800, val loss: 1.1844245195388794
Epoch 810, training loss: 0.629548192024231 = 0.008630321361124516 + 0.1 * 6.209178447723389
Epoch 810, val loss: 1.1902354955673218
Epoch 820, training loss: 0.6285942196846008 = 0.00836971215903759 + 0.1 * 6.202245235443115
Epoch 820, val loss: 1.196131706237793
Epoch 830, training loss: 0.6284915804862976 = 0.00812071468681097 + 0.1 * 6.203708648681641
Epoch 830, val loss: 1.201783299446106
Epoch 840, training loss: 0.6280001401901245 = 0.00788432452827692 + 0.1 * 6.201158046722412
Epoch 840, val loss: 1.2073969841003418
Epoch 850, training loss: 0.6288915872573853 = 0.0076587446965277195 + 0.1 * 6.2123284339904785
Epoch 850, val loss: 1.212918758392334
Epoch 860, training loss: 0.6279349327087402 = 0.007443442940711975 + 0.1 * 6.2049150466918945
Epoch 860, val loss: 1.218261957168579
Epoch 870, training loss: 0.6258766055107117 = 0.007238783873617649 + 0.1 * 6.186378002166748
Epoch 870, val loss: 1.22360360622406
Epoch 880, training loss: 0.6268448233604431 = 0.007043126970529556 + 0.1 * 6.198017120361328
Epoch 880, val loss: 1.228882074356079
Epoch 890, training loss: 0.6257503032684326 = 0.006854821462184191 + 0.1 * 6.1889543533325195
Epoch 890, val loss: 1.2339189052581787
Epoch 900, training loss: 0.6258370876312256 = 0.006675554905086756 + 0.1 * 6.191615104675293
Epoch 900, val loss: 1.2388859987258911
Epoch 910, training loss: 0.625024676322937 = 0.006504429969936609 + 0.1 * 6.185202598571777
Epoch 910, val loss: 1.2438209056854248
Epoch 920, training loss: 0.6244208216667175 = 0.0063405707478523254 + 0.1 * 6.180802345275879
Epoch 920, val loss: 1.248565912246704
Epoch 930, training loss: 0.6238534450531006 = 0.0061842529103159904 + 0.1 * 6.17669153213501
Epoch 930, val loss: 1.2532917261123657
Epoch 940, training loss: 0.623210608959198 = 0.00603439612314105 + 0.1 * 6.171762466430664
Epoch 940, val loss: 1.2580487728118896
Epoch 950, training loss: 0.6242132782936096 = 0.0058897919952869415 + 0.1 * 6.183234691619873
Epoch 950, val loss: 1.2626503705978394
Epoch 960, training loss: 0.6244144439697266 = 0.005750803276896477 + 0.1 * 6.186636447906494
Epoch 960, val loss: 1.2670791149139404
Epoch 970, training loss: 0.6226463317871094 = 0.005617773160338402 + 0.1 * 6.170285224914551
Epoch 970, val loss: 1.271544337272644
Epoch 980, training loss: 0.6230927109718323 = 0.005489944946020842 + 0.1 * 6.176027774810791
Epoch 980, val loss: 1.2759737968444824
Epoch 990, training loss: 0.6219847202301025 = 0.005366617348045111 + 0.1 * 6.1661810874938965
Epoch 990, val loss: 1.28024423122406
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5424
Flip ASR: 0.4489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.760652542114258 = 1.9232741594314575 + 0.1 * 8.373785018920898
Epoch 0, val loss: 1.9235254526138306
Epoch 10, training loss: 2.750903844833374 = 1.913548231124878 + 0.1 * 8.373556137084961
Epoch 10, val loss: 1.9133800268173218
Epoch 20, training loss: 2.7387454509735107 = 1.9015012979507446 + 0.1 * 8.372442245483398
Epoch 20, val loss: 1.9007999897003174
Epoch 30, training loss: 2.7209959030151367 = 1.8844568729400635 + 0.1 * 8.365389823913574
Epoch 30, val loss: 1.8832470178604126
Epoch 40, training loss: 2.692075252532959 = 1.8597608804702759 + 0.1 * 8.32314395904541
Epoch 40, val loss: 1.8585714101791382
Epoch 50, training loss: 2.63265323638916 = 1.8272284269332886 + 0.1 * 8.054248809814453
Epoch 50, val loss: 1.8277992010116577
Epoch 60, training loss: 2.550304651260376 = 1.7926063537597656 + 0.1 * 7.5769829750061035
Epoch 60, val loss: 1.7958800792694092
Epoch 70, training loss: 2.4709177017211914 = 1.7571419477462769 + 0.1 * 7.137758255004883
Epoch 70, val loss: 1.763526439666748
Epoch 80, training loss: 2.4061594009399414 = 1.715682864189148 + 0.1 * 6.904765605926514
Epoch 80, val loss: 1.726395845413208
Epoch 90, training loss: 2.3389291763305664 = 1.6593343019485474 + 0.1 * 6.795947551727295
Epoch 90, val loss: 1.6758276224136353
Epoch 100, training loss: 2.2583656311035156 = 1.5850130319595337 + 0.1 * 6.733524799346924
Epoch 100, val loss: 1.6120725870132446
Epoch 110, training loss: 2.1668477058410645 = 1.4969524145126343 + 0.1 * 6.698951721191406
Epoch 110, val loss: 1.540568232536316
Epoch 120, training loss: 2.071516752243042 = 1.40412437915802 + 0.1 * 6.673924446105957
Epoch 120, val loss: 1.4662179946899414
Epoch 130, training loss: 1.9788285493850708 = 1.3140528202056885 + 0.1 * 6.647757053375244
Epoch 130, val loss: 1.3975995779037476
Epoch 140, training loss: 1.8914721012115479 = 1.2297413349151611 + 0.1 * 6.617306709289551
Epoch 140, val loss: 1.336512565612793
Epoch 150, training loss: 1.8103044033050537 = 1.151292324066162 + 0.1 * 6.590121269226074
Epoch 150, val loss: 1.2818334102630615
Epoch 160, training loss: 1.7315526008605957 = 1.074730396270752 + 0.1 * 6.568221569061279
Epoch 160, val loss: 1.2290494441986084
Epoch 170, training loss: 1.6527371406555176 = 0.9979625940322876 + 0.1 * 6.547746181488037
Epoch 170, val loss: 1.1752326488494873
Epoch 180, training loss: 1.575559377670288 = 0.9219767451286316 + 0.1 * 6.535825729370117
Epoch 180, val loss: 1.120851993560791
Epoch 190, training loss: 1.5008485317230225 = 0.8488214015960693 + 0.1 * 6.520271301269531
Epoch 190, val loss: 1.067946434020996
Epoch 200, training loss: 1.4314982891082764 = 0.7799895405769348 + 0.1 * 6.515088081359863
Epoch 200, val loss: 1.017829179763794
Epoch 210, training loss: 1.3671956062316895 = 0.7172666192054749 + 0.1 * 6.4992899894714355
Epoch 210, val loss: 0.9732005000114441
Epoch 220, training loss: 1.3085463047027588 = 0.6598771214485168 + 0.1 * 6.486690998077393
Epoch 220, val loss: 0.933972954750061
Epoch 230, training loss: 1.2557148933410645 = 0.6074033975601196 + 0.1 * 6.483114719390869
Epoch 230, val loss: 0.9004250764846802
Epoch 240, training loss: 1.2055860757827759 = 0.5587186217308044 + 0.1 * 6.468674182891846
Epoch 240, val loss: 0.872107744216919
Epoch 250, training loss: 1.1589044332504272 = 0.5120217800140381 + 0.1 * 6.4688262939453125
Epoch 250, val loss: 0.8471949696540833
Epoch 260, training loss: 1.1118619441986084 = 0.4668322801589966 + 0.1 * 6.450296878814697
Epoch 260, val loss: 0.8250561356544495
Epoch 270, training loss: 1.0678943395614624 = 0.42280882596969604 + 0.1 * 6.450855255126953
Epoch 270, val loss: 0.8052884340286255
Epoch 280, training loss: 1.023796558380127 = 0.38053351640701294 + 0.1 * 6.432631015777588
Epoch 280, val loss: 0.7881261706352234
Epoch 290, training loss: 0.9844638705253601 = 0.34051913022994995 + 0.1 * 6.439447402954102
Epoch 290, val loss: 0.7737473845481873
Epoch 300, training loss: 0.9453722238540649 = 0.3034529387950897 + 0.1 * 6.419192314147949
Epoch 300, val loss: 0.7628053426742554
Epoch 310, training loss: 0.9105644226074219 = 0.26935824751853943 + 0.1 * 6.4120612144470215
Epoch 310, val loss: 0.7550770044326782
Epoch 320, training loss: 0.8791395425796509 = 0.23861876130104065 + 0.1 * 6.405207633972168
Epoch 320, val loss: 0.75075763463974
Epoch 330, training loss: 0.8519481420516968 = 0.2114390730857849 + 0.1 * 6.40509033203125
Epoch 330, val loss: 0.7497419118881226
Epoch 340, training loss: 0.8272410035133362 = 0.18771736323833466 + 0.1 * 6.395236015319824
Epoch 340, val loss: 0.7516384720802307
Epoch 350, training loss: 0.8054749965667725 = 0.1670806109905243 + 0.1 * 6.383943557739258
Epoch 350, val loss: 0.756243884563446
Epoch 360, training loss: 0.7866119146347046 = 0.1491897851228714 + 0.1 * 6.374221324920654
Epoch 360, val loss: 0.7629473805427551
Epoch 370, training loss: 0.7710062861442566 = 0.13372159004211426 + 0.1 * 6.372846603393555
Epoch 370, val loss: 0.7714716196060181
Epoch 380, training loss: 0.7578385472297668 = 0.12028399854898453 + 0.1 * 6.375545024871826
Epoch 380, val loss: 0.7812291979789734
Epoch 390, training loss: 0.7441334128379822 = 0.10862333327531815 + 0.1 * 6.355100631713867
Epoch 390, val loss: 0.7920755743980408
Epoch 400, training loss: 0.7336492538452148 = 0.09839680045843124 + 0.1 * 6.352524757385254
Epoch 400, val loss: 0.803698718547821
Epoch 410, training loss: 0.7236018776893616 = 0.08938710391521454 + 0.1 * 6.342147350311279
Epoch 410, val loss: 0.8159598708152771
Epoch 420, training loss: 0.7165047526359558 = 0.08141694962978363 + 0.1 * 6.35087776184082
Epoch 420, val loss: 0.8286469578742981
Epoch 430, training loss: 0.707273542881012 = 0.07436905056238174 + 0.1 * 6.329044342041016
Epoch 430, val loss: 0.8415866494178772
Epoch 440, training loss: 0.7003422379493713 = 0.06811553984880447 + 0.1 * 6.322267055511475
Epoch 440, val loss: 0.8547753095626831
Epoch 450, training loss: 0.6950446367263794 = 0.062532439827919 + 0.1 * 6.325121879577637
Epoch 450, val loss: 0.8680834770202637
Epoch 460, training loss: 0.6885886788368225 = 0.05753910914063454 + 0.1 * 6.310495853424072
Epoch 460, val loss: 0.881482720375061
Epoch 470, training loss: 0.6844427585601807 = 0.05307498574256897 + 0.1 * 6.313677787780762
Epoch 470, val loss: 0.8946813344955444
Epoch 480, training loss: 0.6786686778068542 = 0.04909040778875351 + 0.1 * 6.295782566070557
Epoch 480, val loss: 0.9079271554946899
Epoch 490, training loss: 0.6753707528114319 = 0.04549987241625786 + 0.1 * 6.298708915710449
Epoch 490, val loss: 0.9209436774253845
Epoch 500, training loss: 0.6710299849510193 = 0.04226198419928551 + 0.1 * 6.287680149078369
Epoch 500, val loss: 0.9339264035224915
Epoch 510, training loss: 0.6682617664337158 = 0.03934350982308388 + 0.1 * 6.289182186126709
Epoch 510, val loss: 0.946580171585083
Epoch 520, training loss: 0.6646352410316467 = 0.03671778365969658 + 0.1 * 6.2791748046875
Epoch 520, val loss: 0.9591297507286072
Epoch 530, training loss: 0.6620684266090393 = 0.03433595970273018 + 0.1 * 6.277324199676514
Epoch 530, val loss: 0.9714033603668213
Epoch 540, training loss: 0.6592922210693359 = 0.03217419981956482 + 0.1 * 6.271179676055908
Epoch 540, val loss: 0.9833216071128845
Epoch 550, training loss: 0.6571490168571472 = 0.03021453320980072 + 0.1 * 6.269344329833984
Epoch 550, val loss: 0.9950599670410156
Epoch 560, training loss: 0.654552698135376 = 0.028423219919204712 + 0.1 * 6.261294841766357
Epoch 560, val loss: 1.0064176321029663
Epoch 570, training loss: 0.6553926467895508 = 0.026785241439938545 + 0.1 * 6.286073684692383
Epoch 570, val loss: 1.017525315284729
Epoch 580, training loss: 0.651279628276825 = 0.025291822850704193 + 0.1 * 6.259878158569336
Epoch 580, val loss: 1.0282753705978394
Epoch 590, training loss: 0.6498225927352905 = 0.02392357587814331 + 0.1 * 6.258990287780762
Epoch 590, val loss: 1.038830280303955
Epoch 600, training loss: 0.6475169658660889 = 0.0226606335490942 + 0.1 * 6.248563289642334
Epoch 600, val loss: 1.0490401983261108
Epoch 610, training loss: 0.647279679775238 = 0.021494736894965172 + 0.1 * 6.257849216461182
Epoch 610, val loss: 1.0591177940368652
Epoch 620, training loss: 0.6454334259033203 = 0.020419331267476082 + 0.1 * 6.25014066696167
Epoch 620, val loss: 1.0688507556915283
Epoch 630, training loss: 0.6452796459197998 = 0.01942659541964531 + 0.1 * 6.258530616760254
Epoch 630, val loss: 1.0783724784851074
Epoch 640, training loss: 0.642112672328949 = 0.018506821244955063 + 0.1 * 6.236058235168457
Epoch 640, val loss: 1.0876017808914185
Epoch 650, training loss: 0.6408406496047974 = 0.01765490509569645 + 0.1 * 6.2318572998046875
Epoch 650, val loss: 1.0966675281524658
Epoch 660, training loss: 0.642045259475708 = 0.016862494871020317 + 0.1 * 6.251827239990234
Epoch 660, val loss: 1.1054550409317017
Epoch 670, training loss: 0.6393755078315735 = 0.016125816851854324 + 0.1 * 6.232496738433838
Epoch 670, val loss: 1.114006757736206
Epoch 680, training loss: 0.6378822326660156 = 0.015440595336258411 + 0.1 * 6.224416255950928
Epoch 680, val loss: 1.1225237846374512
Epoch 690, training loss: 0.6372334957122803 = 0.014799238182604313 + 0.1 * 6.224342346191406
Epoch 690, val loss: 1.1307368278503418
Epoch 700, training loss: 0.6367234587669373 = 0.014199080877006054 + 0.1 * 6.22524356842041
Epoch 700, val loss: 1.1386313438415527
Epoch 710, training loss: 0.6356289982795715 = 0.013639725744724274 + 0.1 * 6.219892501831055
Epoch 710, val loss: 1.1465271711349487
Epoch 720, training loss: 0.6347433924674988 = 0.013114139437675476 + 0.1 * 6.216292381286621
Epoch 720, val loss: 1.1542102098464966
Epoch 730, training loss: 0.634219229221344 = 0.0126189598813653 + 0.1 * 6.216002464294434
Epoch 730, val loss: 1.1616135835647583
Epoch 740, training loss: 0.6332960724830627 = 0.012153981253504753 + 0.1 * 6.21142053604126
Epoch 740, val loss: 1.1689828634262085
Epoch 750, training loss: 0.6348636746406555 = 0.011715582571923733 + 0.1 * 6.231480598449707
Epoch 750, val loss: 1.1762073040008545
Epoch 760, training loss: 0.6326358318328857 = 0.011302176862955093 + 0.1 * 6.21333646774292
Epoch 760, val loss: 1.1830627918243408
Epoch 770, training loss: 0.6316717863082886 = 0.010913093574345112 + 0.1 * 6.207587242126465
Epoch 770, val loss: 1.1900274753570557
Epoch 780, training loss: 0.6310149431228638 = 0.010545160621404648 + 0.1 * 6.204698085784912
Epoch 780, val loss: 1.1967047452926636
Epoch 790, training loss: 0.6306434869766235 = 0.010196508839726448 + 0.1 * 6.204469680786133
Epoch 790, val loss: 1.2033179998397827
Epoch 800, training loss: 0.6299786567687988 = 0.009866268374025822 + 0.1 * 6.201123237609863
Epoch 800, val loss: 1.2097946405410767
Epoch 810, training loss: 0.630411684513092 = 0.009552772156894207 + 0.1 * 6.20858907699585
Epoch 810, val loss: 1.2161200046539307
Epoch 820, training loss: 0.6284878849983215 = 0.009255416691303253 + 0.1 * 6.192324638366699
Epoch 820, val loss: 1.2223042249679565
Epoch 830, training loss: 0.6280125975608826 = 0.008973808027803898 + 0.1 * 6.190387725830078
Epoch 830, val loss: 1.2284919023513794
Epoch 840, training loss: 0.6290802955627441 = 0.0087052583694458 + 0.1 * 6.203750133514404
Epoch 840, val loss: 1.234447717666626
Epoch 850, training loss: 0.6283953189849854 = 0.008449562825262547 + 0.1 * 6.19945764541626
Epoch 850, val loss: 1.2402573823928833
Epoch 860, training loss: 0.6268652081489563 = 0.00820687972009182 + 0.1 * 6.186583042144775
Epoch 860, val loss: 1.246073603630066
Epoch 870, training loss: 0.6270344257354736 = 0.007975373417139053 + 0.1 * 6.1905903816223145
Epoch 870, val loss: 1.2517577409744263
Epoch 880, training loss: 0.626372218132019 = 0.007754153106361628 + 0.1 * 6.186180591583252
Epoch 880, val loss: 1.2573628425598145
Epoch 890, training loss: 0.6266908645629883 = 0.007543153595179319 + 0.1 * 6.191477298736572
Epoch 890, val loss: 1.2627227306365967
Epoch 900, training loss: 0.6259912252426147 = 0.007341672200709581 + 0.1 * 6.186495304107666
Epoch 900, val loss: 1.2680631875991821
Epoch 910, training loss: 0.6247820854187012 = 0.0071497163735330105 + 0.1 * 6.176323890686035
Epoch 910, val loss: 1.27339506149292
Epoch 920, training loss: 0.6253170371055603 = 0.00696586212143302 + 0.1 * 6.183511734008789
Epoch 920, val loss: 1.2786660194396973
Epoch 930, training loss: 0.6243946552276611 = 0.006788990925997496 + 0.1 * 6.176056861877441
Epoch 930, val loss: 1.2836880683898926
Epoch 940, training loss: 0.6246592998504639 = 0.0066197034902870655 + 0.1 * 6.18039608001709
Epoch 940, val loss: 1.2886807918548584
Epoch 950, training loss: 0.6241352558135986 = 0.006457887589931488 + 0.1 * 6.176774024963379
Epoch 950, val loss: 1.2937474250793457
Epoch 960, training loss: 0.6241363286972046 = 0.006302235648036003 + 0.1 * 6.178340911865234
Epoch 960, val loss: 1.2985740900039673
Epoch 970, training loss: 0.6230592727661133 = 0.006153111346065998 + 0.1 * 6.169061660766602
Epoch 970, val loss: 1.303383708000183
Epoch 980, training loss: 0.6239613890647888 = 0.006009739823639393 + 0.1 * 6.179516315460205
Epoch 980, val loss: 1.3080700635910034
Epoch 990, training loss: 0.6226392984390259 = 0.005871937610208988 + 0.1 * 6.167673587799072
Epoch 990, val loss: 1.3126634359359741
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7675
Flip ASR: 0.7378/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.794462203979492 = 1.9570894241333008 + 0.1 * 8.373726844787598
Epoch 0, val loss: 1.959809422492981
Epoch 10, training loss: 2.7840843200683594 = 1.9467413425445557 + 0.1 * 8.373429298400879
Epoch 10, val loss: 1.9498631954193115
Epoch 20, training loss: 2.7716453075408936 = 1.9344871044158936 + 0.1 * 8.37158203125
Epoch 20, val loss: 1.937454342842102
Epoch 30, training loss: 2.753297805786133 = 1.9174033403396606 + 0.1 * 8.358945846557617
Epoch 30, val loss: 1.9197810888290405
Epoch 40, training loss: 2.719305992126465 = 1.8922293186187744 + 0.1 * 8.270766258239746
Epoch 40, val loss: 1.8939412832260132
Epoch 50, training loss: 2.621230363845825 = 1.8599538803100586 + 0.1 * 7.612765312194824
Epoch 50, val loss: 1.862389326095581
Epoch 60, training loss: 2.5397462844848633 = 1.8279638290405273 + 0.1 * 7.117825508117676
Epoch 60, val loss: 1.8313825130462646
Epoch 70, training loss: 2.478018283843994 = 1.7926607131958008 + 0.1 * 6.853574752807617
Epoch 70, val loss: 1.7983741760253906
Epoch 80, training loss: 2.4295570850372314 = 1.7563904523849487 + 0.1 * 6.731666564941406
Epoch 80, val loss: 1.7659152746200562
Epoch 90, training loss: 2.384044885635376 = 1.7172536849975586 + 0.1 * 6.667912483215332
Epoch 90, val loss: 1.730250597000122
Epoch 100, training loss: 2.328725576400757 = 1.6657241582870483 + 0.1 * 6.630013465881348
Epoch 100, val loss: 1.6832026243209839
Epoch 110, training loss: 2.2575106620788574 = 1.597174882888794 + 0.1 * 6.603358745574951
Epoch 110, val loss: 1.6238915920257568
Epoch 120, training loss: 2.169283390045166 = 1.5109074115753174 + 0.1 * 6.58375883102417
Epoch 120, val loss: 1.5515525341033936
Epoch 130, training loss: 2.0682740211486816 = 1.4113280773162842 + 0.1 * 6.569459438323975
Epoch 130, val loss: 1.4684561491012573
Epoch 140, training loss: 1.9620790481567383 = 1.3063026666641235 + 0.1 * 6.557764530181885
Epoch 140, val loss: 1.3834582567214966
Epoch 150, training loss: 1.8539974689483643 = 1.199438214302063 + 0.1 * 6.545592308044434
Epoch 150, val loss: 1.2981741428375244
Epoch 160, training loss: 1.746701717376709 = 1.0929985046386719 + 0.1 * 6.537032127380371
Epoch 160, val loss: 1.21543288230896
Epoch 170, training loss: 1.6422779560089111 = 0.9895456433296204 + 0.1 * 6.527323246002197
Epoch 170, val loss: 1.1366260051727295
Epoch 180, training loss: 1.5447520017623901 = 0.8922354578971863 + 0.1 * 6.525165557861328
Epoch 180, val loss: 1.0629533529281616
Epoch 190, training loss: 1.4561805725097656 = 0.8049200177192688 + 0.1 * 6.512604713439941
Epoch 190, val loss: 0.9976927638053894
Epoch 200, training loss: 1.3788137435913086 = 0.7275300025939941 + 0.1 * 6.512836933135986
Epoch 200, val loss: 0.941094160079956
Epoch 210, training loss: 1.3101178407669067 = 0.6600024700164795 + 0.1 * 6.501153469085693
Epoch 210, val loss: 0.893805980682373
Epoch 220, training loss: 1.2495536804199219 = 0.6003541946411133 + 0.1 * 6.4919939041137695
Epoch 220, val loss: 0.8544571995735168
Epoch 230, training loss: 1.1950082778930664 = 0.5464695692062378 + 0.1 * 6.485387325286865
Epoch 230, val loss: 0.8213043212890625
Epoch 240, training loss: 1.1454137563705444 = 0.49714016914367676 + 0.1 * 6.482735633850098
Epoch 240, val loss: 0.793332576751709
Epoch 250, training loss: 1.098412275314331 = 0.45112144947052 + 0.1 * 6.4729084968566895
Epoch 250, val loss: 0.7690315246582031
Epoch 260, training loss: 1.0547165870666504 = 0.4074855446815491 + 0.1 * 6.472310543060303
Epoch 260, val loss: 0.74753338098526
Epoch 270, training loss: 1.0119975805282593 = 0.36633992195129395 + 0.1 * 6.456576347351074
Epoch 270, val loss: 0.7286607027053833
Epoch 280, training loss: 0.9724065661430359 = 0.32751893997192383 + 0.1 * 6.44887638092041
Epoch 280, val loss: 0.7124406099319458
Epoch 290, training loss: 0.9363391399383545 = 0.2914787232875824 + 0.1 * 6.448604106903076
Epoch 290, val loss: 0.6992844343185425
Epoch 300, training loss: 0.9024924039840698 = 0.25884851813316345 + 0.1 * 6.43643856048584
Epoch 300, val loss: 0.6892709732055664
Epoch 310, training loss: 0.8719954490661621 = 0.22942230105400085 + 0.1 * 6.425731658935547
Epoch 310, val loss: 0.6827581524848938
Epoch 320, training loss: 0.8456813097000122 = 0.20322369039058685 + 0.1 * 6.4245758056640625
Epoch 320, val loss: 0.6796597838401794
Epoch 330, training loss: 0.8214194774627686 = 0.18029573559761047 + 0.1 * 6.411236763000488
Epoch 330, val loss: 0.6796154379844666
Epoch 340, training loss: 0.8002405166625977 = 0.16018979251384735 + 0.1 * 6.400506973266602
Epoch 340, val loss: 0.6822183132171631
Epoch 350, training loss: 0.7828668355941772 = 0.14268244802951813 + 0.1 * 6.401844024658203
Epoch 350, val loss: 0.686995267868042
Epoch 360, training loss: 0.7666257619857788 = 0.12754103541374207 + 0.1 * 6.390847682952881
Epoch 360, val loss: 0.6933190822601318
Epoch 370, training loss: 0.7518532276153564 = 0.11434905976057053 + 0.1 * 6.375041484832764
Epoch 370, val loss: 0.7010841369628906
Epoch 380, training loss: 0.7413796186447144 = 0.10283304750919342 + 0.1 * 6.385465621948242
Epoch 380, val loss: 0.7099903225898743
Epoch 390, training loss: 0.7290009260177612 = 0.09283983707427979 + 0.1 * 6.3616108894348145
Epoch 390, val loss: 0.7195218205451965
Epoch 400, training loss: 0.7195010781288147 = 0.0841081514954567 + 0.1 * 6.353929042816162
Epoch 400, val loss: 0.7296164631843567
Epoch 410, training loss: 0.7139760851860046 = 0.07644478231668472 + 0.1 * 6.375312805175781
Epoch 410, val loss: 0.7400641441345215
Epoch 420, training loss: 0.7038692235946655 = 0.06976194679737091 + 0.1 * 6.341073036193848
Epoch 420, val loss: 0.7505706548690796
Epoch 430, training loss: 0.6979314684867859 = 0.06388195604085922 + 0.1 * 6.3404951095581055
Epoch 430, val loss: 0.7612496018409729
Epoch 440, training loss: 0.6912577748298645 = 0.058682166039943695 + 0.1 * 6.325756072998047
Epoch 440, val loss: 0.771847665309906
Epoch 450, training loss: 0.6886038184165955 = 0.05405442789196968 + 0.1 * 6.345493793487549
Epoch 450, val loss: 0.7825475335121155
Epoch 460, training loss: 0.6811829805374146 = 0.04995483532547951 + 0.1 * 6.312281608581543
Epoch 460, val loss: 0.7930727005004883
Epoch 470, training loss: 0.6788986325263977 = 0.04629736766219139 + 0.1 * 6.32601261138916
Epoch 470, val loss: 0.8035454750061035
Epoch 480, training loss: 0.6740147471427917 = 0.04302787408232689 + 0.1 * 6.309868812561035
Epoch 480, val loss: 0.8137076497077942
Epoch 490, training loss: 0.6698320508003235 = 0.04008447751402855 + 0.1 * 6.297475337982178
Epoch 490, val loss: 0.8237501978874207
Epoch 500, training loss: 0.6688240170478821 = 0.037423718720674515 + 0.1 * 6.314002990722656
Epoch 500, val loss: 0.8336154222488403
Epoch 510, training loss: 0.665414571762085 = 0.03502592071890831 + 0.1 * 6.303886890411377
Epoch 510, val loss: 0.8432740569114685
Epoch 520, training loss: 0.661771297454834 = 0.032859187573194504 + 0.1 * 6.289120674133301
Epoch 520, val loss: 0.852631151676178
Epoch 530, training loss: 0.6591378450393677 = 0.030886825174093246 + 0.1 * 6.282510280609131
Epoch 530, val loss: 0.8618406057357788
Epoch 540, training loss: 0.6570967435836792 = 0.029087498784065247 + 0.1 * 6.280092716217041
Epoch 540, val loss: 0.870846152305603
Epoch 550, training loss: 0.6563451886177063 = 0.02744382992386818 + 0.1 * 6.289013385772705
Epoch 550, val loss: 0.8797033429145813
Epoch 560, training loss: 0.653308629989624 = 0.025940224528312683 + 0.1 * 6.273683547973633
Epoch 560, val loss: 0.8883800506591797
Epoch 570, training loss: 0.6511247754096985 = 0.02455911971628666 + 0.1 * 6.265656471252441
Epoch 570, val loss: 0.8968857526779175
Epoch 580, training loss: 0.6526186466217041 = 0.023285383358597755 + 0.1 * 6.293332576751709
Epoch 580, val loss: 0.905296802520752
Epoch 590, training loss: 0.6483402848243713 = 0.022116905078291893 + 0.1 * 6.262233734130859
Epoch 590, val loss: 0.913330078125
Epoch 600, training loss: 0.6466155052185059 = 0.02103811874985695 + 0.1 * 6.255773544311523
Epoch 600, val loss: 0.9212576150894165
Epoch 610, training loss: 0.6450033783912659 = 0.020036984235048294 + 0.1 * 6.249663829803467
Epoch 610, val loss: 0.9289506673812866
Epoch 620, training loss: 0.6452377438545227 = 0.01910574547946453 + 0.1 * 6.261320114135742
Epoch 620, val loss: 0.9365859627723694
Epoch 630, training loss: 0.6433120965957642 = 0.018243273720145226 + 0.1 * 6.250688552856445
Epoch 630, val loss: 0.9439933896064758
Epoch 640, training loss: 0.6425826549530029 = 0.01744057610630989 + 0.1 * 6.251420974731445
Epoch 640, val loss: 0.9512674808502197
Epoch 650, training loss: 0.6407614946365356 = 0.01669263280928135 + 0.1 * 6.240688323974609
Epoch 650, val loss: 0.9584508538246155
Epoch 660, training loss: 0.6396680474281311 = 0.01599694788455963 + 0.1 * 6.236711025238037
Epoch 660, val loss: 0.9652973413467407
Epoch 670, training loss: 0.638779878616333 = 0.015344424173235893 + 0.1 * 6.234354019165039
Epoch 670, val loss: 0.9721062779426575
Epoch 680, training loss: 0.6378430128097534 = 0.0147325424477458 + 0.1 * 6.231104373931885
Epoch 680, val loss: 0.9788615107536316
Epoch 690, training loss: 0.6368207335472107 = 0.014160261489450932 + 0.1 * 6.226604461669922
Epoch 690, val loss: 0.9853814244270325
Epoch 700, training loss: 0.6360926032066345 = 0.013622784987092018 + 0.1 * 6.224698543548584
Epoch 700, val loss: 0.9918185472488403
Epoch 710, training loss: 0.6356586813926697 = 0.013116548769176006 + 0.1 * 6.22542142868042
Epoch 710, val loss: 0.9981982111930847
Epoch 720, training loss: 0.6352142691612244 = 0.012640268541872501 + 0.1 * 6.225739479064941
Epoch 720, val loss: 1.0043352842330933
Epoch 730, training loss: 0.6349064707756042 = 0.01219057384878397 + 0.1 * 6.227158546447754
Epoch 730, val loss: 1.0105056762695312
Epoch 740, training loss: 0.6329131126403809 = 0.01176648959517479 + 0.1 * 6.211465835571289
Epoch 740, val loss: 1.016465187072754
Epoch 750, training loss: 0.6342934370040894 = 0.011366058140993118 + 0.1 * 6.229273319244385
Epoch 750, val loss: 1.0224074125289917
Epoch 760, training loss: 0.6324115991592407 = 0.010987809859216213 + 0.1 * 6.214237689971924
Epoch 760, val loss: 1.0281968116760254
Epoch 770, training loss: 0.6311608552932739 = 0.010630951263010502 + 0.1 * 6.205298900604248
Epoch 770, val loss: 1.0337942838668823
Epoch 780, training loss: 0.6322027444839478 = 0.010291540995240211 + 0.1 * 6.219112396240234
Epoch 780, val loss: 1.0393821001052856
Epoch 790, training loss: 0.6307079195976257 = 0.009969226084649563 + 0.1 * 6.207386493682861
Epoch 790, val loss: 1.0448240041732788
Epoch 800, training loss: 0.6300061941146851 = 0.009664395824074745 + 0.1 * 6.203417778015137
Epoch 800, val loss: 1.0501410961151123
Epoch 810, training loss: 0.6290711164474487 = 0.009373131208121777 + 0.1 * 6.196979522705078
Epoch 810, val loss: 1.055504322052002
Epoch 820, training loss: 0.6284670829772949 = 0.00909714587032795 + 0.1 * 6.193698883056641
Epoch 820, val loss: 1.0606104135513306
Epoch 830, training loss: 0.6287742257118225 = 0.008834175765514374 + 0.1 * 6.199400424957275
Epoch 830, val loss: 1.0657376050949097
Epoch 840, training loss: 0.628140389919281 = 0.008583499118685722 + 0.1 * 6.195569038391113
Epoch 840, val loss: 1.07071053981781
Epoch 850, training loss: 0.6284158229827881 = 0.008344017900526524 + 0.1 * 6.200718402862549
Epoch 850, val loss: 1.0757120847702026
Epoch 860, training loss: 0.6270012855529785 = 0.008115741424262524 + 0.1 * 6.188855171203613
Epoch 860, val loss: 1.080619215965271
Epoch 870, training loss: 0.6265158653259277 = 0.007898662239313126 + 0.1 * 6.186171531677246
Epoch 870, val loss: 1.0853627920150757
Epoch 880, training loss: 0.6267949342727661 = 0.007690340746194124 + 0.1 * 6.19104528427124
Epoch 880, val loss: 1.0901223421096802
Epoch 890, training loss: 0.6260231137275696 = 0.00749021302908659 + 0.1 * 6.185328960418701
Epoch 890, val loss: 1.0948503017425537
Epoch 900, training loss: 0.6253623366355896 = 0.007300183642655611 + 0.1 * 6.180621147155762
Epoch 900, val loss: 1.099397897720337
Epoch 910, training loss: 0.6267331838607788 = 0.007117120083421469 + 0.1 * 6.196160793304443
Epoch 910, val loss: 1.103967308998108
Epoch 920, training loss: 0.6249074339866638 = 0.0069414288736879826 + 0.1 * 6.179659843444824
Epoch 920, val loss: 1.1085554361343384
Epoch 930, training loss: 0.6245346069335938 = 0.00677361199632287 + 0.1 * 6.177609920501709
Epoch 930, val loss: 1.1128863096237183
Epoch 940, training loss: 0.6234352588653564 = 0.0066118729300796986 + 0.1 * 6.168233394622803
Epoch 940, val loss: 1.117293119430542
Epoch 950, training loss: 0.6248933672904968 = 0.006456881295889616 + 0.1 * 6.184365272521973
Epoch 950, val loss: 1.1216278076171875
Epoch 960, training loss: 0.6231582164764404 = 0.006307404022663832 + 0.1 * 6.168508052825928
Epoch 960, val loss: 1.1258517503738403
Epoch 970, training loss: 0.6239261031150818 = 0.0061644273810088634 + 0.1 * 6.177616596221924
Epoch 970, val loss: 1.1300421953201294
Epoch 980, training loss: 0.623157799243927 = 0.006026288028806448 + 0.1 * 6.1713151931762695
Epoch 980, val loss: 1.1342008113861084
Epoch 990, training loss: 0.6219948530197144 = 0.005894040688872337 + 0.1 * 6.161007881164551
Epoch 990, val loss: 1.1381548643112183
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8229
Flip ASR: 0.7867/225 nodes
The final ASR:0.71095, 0.12128, Accuracy:0.81975, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11626])
remove edge: torch.Size([2, 9448])
updated graph: torch.Size([2, 10518])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97417, 0.00000, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.784731149673462 = 1.9473603963851929 + 0.1 * 8.37370777130127
Epoch 0, val loss: 1.9391613006591797
Epoch 10, training loss: 2.7741801738739014 = 1.9368469715118408 + 0.1 * 8.373332023620605
Epoch 10, val loss: 1.9296151399612427
Epoch 20, training loss: 2.7607779502868652 = 1.923636794090271 + 0.1 * 8.37141227722168
Epoch 20, val loss: 1.917332649230957
Epoch 30, training loss: 2.740591526031494 = 1.9047185182571411 + 0.1 * 8.358729362487793
Epoch 30, val loss: 1.8995935916900635
Epoch 40, training loss: 2.7032454013824463 = 1.8765279054641724 + 0.1 * 8.26717472076416
Epoch 40, val loss: 1.8737226724624634
Epoch 50, training loss: 2.6118152141571045 = 1.8406063318252563 + 0.1 * 7.7120890617370605
Epoch 50, val loss: 1.8423575162887573
Epoch 60, training loss: 2.53131365776062 = 1.806183934211731 + 0.1 * 7.2512969970703125
Epoch 60, val loss: 1.8135113716125488
Epoch 70, training loss: 2.460644245147705 = 1.773807406425476 + 0.1 * 6.868368625640869
Epoch 70, val loss: 1.7880568504333496
Epoch 80, training loss: 2.4107773303985596 = 1.7398567199707031 + 0.1 * 6.709205150604248
Epoch 80, val loss: 1.7607380151748657
Epoch 90, training loss: 2.358896255493164 = 1.6945481300354004 + 0.1 * 6.64348030090332
Epoch 90, val loss: 1.7211089134216309
Epoch 100, training loss: 2.291987180709839 = 1.632703423500061 + 0.1 * 6.592838287353516
Epoch 100, val loss: 1.6685919761657715
Epoch 110, training loss: 2.21000337600708 = 1.5538772344589233 + 0.1 * 6.561260223388672
Epoch 110, val loss: 1.6054275035858154
Epoch 120, training loss: 2.1195359230041504 = 1.4652397632598877 + 0.1 * 6.542962074279785
Epoch 120, val loss: 1.5350348949432373
Epoch 130, training loss: 2.0301103591918945 = 1.3772035837173462 + 0.1 * 6.5290679931640625
Epoch 130, val loss: 1.4667088985443115
Epoch 140, training loss: 1.9446980953216553 = 1.293526530265808 + 0.1 * 6.511714935302734
Epoch 140, val loss: 1.402512788772583
Epoch 150, training loss: 1.8631060123443604 = 1.2139335870742798 + 0.1 * 6.491724491119385
Epoch 150, val loss: 1.3426867723464966
Epoch 160, training loss: 1.7868502140045166 = 1.1393624544143677 + 0.1 * 6.474878311157227
Epoch 160, val loss: 1.288589358329773
Epoch 170, training loss: 1.7145471572875977 = 1.068970799446106 + 0.1 * 6.45576286315918
Epoch 170, val loss: 1.239429235458374
Epoch 180, training loss: 1.6464838981628418 = 1.0015044212341309 + 0.1 * 6.449794292449951
Epoch 180, val loss: 1.1927895545959473
Epoch 190, training loss: 1.5819653272628784 = 0.9386402368545532 + 0.1 * 6.433250904083252
Epoch 190, val loss: 1.1502470970153809
Epoch 200, training loss: 1.52093505859375 = 0.8789335489273071 + 0.1 * 6.420015335083008
Epoch 200, val loss: 1.109725832939148
Epoch 210, training loss: 1.4615981578826904 = 0.8205419182777405 + 0.1 * 6.410562515258789
Epoch 210, val loss: 1.0697156190872192
Epoch 220, training loss: 1.404003620147705 = 0.7629389762878418 + 0.1 * 6.410646438598633
Epoch 220, val loss: 1.0300759077072144
Epoch 230, training loss: 1.3460664749145508 = 0.7061405181884766 + 0.1 * 6.399258613586426
Epoch 230, val loss: 0.990567684173584
Epoch 240, training loss: 1.2896431684494019 = 0.6502305865287781 + 0.1 * 6.394125461578369
Epoch 240, val loss: 0.9513653516769409
Epoch 250, training loss: 1.234656810760498 = 0.5959944128990173 + 0.1 * 6.386623859405518
Epoch 250, val loss: 0.9135328531265259
Epoch 260, training loss: 1.182334542274475 = 0.5436481237411499 + 0.1 * 6.386864185333252
Epoch 260, val loss: 0.8775889277458191
Epoch 270, training loss: 1.131725788116455 = 0.494301974773407 + 0.1 * 6.374238014221191
Epoch 270, val loss: 0.8450337052345276
Epoch 280, training loss: 1.0850733518600464 = 0.44824284315109253 + 0.1 * 6.36830472946167
Epoch 280, val loss: 0.8163238763809204
Epoch 290, training loss: 1.04263436794281 = 0.4053287208080292 + 0.1 * 6.373056411743164
Epoch 290, val loss: 0.791637659072876
Epoch 300, training loss: 1.0020084381103516 = 0.365527480840683 + 0.1 * 6.364809513092041
Epoch 300, val loss: 0.7709553837776184
Epoch 310, training loss: 0.9641345739364624 = 0.3285708725452423 + 0.1 * 6.355637073516846
Epoch 310, val loss: 0.7538563013076782
Epoch 320, training loss: 0.9290050268173218 = 0.2942987382411957 + 0.1 * 6.347063064575195
Epoch 320, val loss: 0.7402292490005493
Epoch 330, training loss: 0.8984262347221375 = 0.26265448331832886 + 0.1 * 6.357717514038086
Epoch 330, val loss: 0.7297137975692749
Epoch 340, training loss: 0.8688815832138062 = 0.2341475784778595 + 0.1 * 6.347340106964111
Epoch 340, val loss: 0.7224217057228088
Epoch 350, training loss: 0.8422961831092834 = 0.20875871181488037 + 0.1 * 6.33537483215332
Epoch 350, val loss: 0.7180506587028503
Epoch 360, training loss: 0.8216487169265747 = 0.18635055422782898 + 0.1 * 6.352981090545654
Epoch 360, val loss: 0.7165794372558594
Epoch 370, training loss: 0.7994959354400635 = 0.1668490320444107 + 0.1 * 6.3264689445495605
Epoch 370, val loss: 0.7176381349563599
Epoch 380, training loss: 0.782004177570343 = 0.14987976849079132 + 0.1 * 6.321244239807129
Epoch 380, val loss: 0.7206412553787231
Epoch 390, training loss: 0.7664792537689209 = 0.13510122895240784 + 0.1 * 6.313779830932617
Epoch 390, val loss: 0.7254756689071655
Epoch 400, training loss: 0.7543810606002808 = 0.12216775119304657 + 0.1 * 6.322132587432861
Epoch 400, val loss: 0.7316232919692993
Epoch 410, training loss: 0.7410564422607422 = 0.11087288707494736 + 0.1 * 6.301835536956787
Epoch 410, val loss: 0.7388104796409607
Epoch 420, training loss: 0.7309630513191223 = 0.10094115883111954 + 0.1 * 6.30021858215332
Epoch 420, val loss: 0.7468506693840027
Epoch 430, training loss: 0.7225887775421143 = 0.09213991463184357 + 0.1 * 6.304488658905029
Epoch 430, val loss: 0.7555714249610901
Epoch 440, training loss: 0.7135094404220581 = 0.08433260768651962 + 0.1 * 6.291768550872803
Epoch 440, val loss: 0.7647809982299805
Epoch 450, training loss: 0.7058435082435608 = 0.07736684381961823 + 0.1 * 6.284766674041748
Epoch 450, val loss: 0.7744119167327881
Epoch 460, training loss: 0.6995722651481628 = 0.07113800942897797 + 0.1 * 6.284342288970947
Epoch 460, val loss: 0.7841964364051819
Epoch 470, training loss: 0.6937291026115417 = 0.06556938588619232 + 0.1 * 6.281596660614014
Epoch 470, val loss: 0.7942438125610352
Epoch 480, training loss: 0.6878993511199951 = 0.06057009845972061 + 0.1 * 6.273292064666748
Epoch 480, val loss: 0.8044119477272034
Epoch 490, training loss: 0.6833667159080505 = 0.05605744943022728 + 0.1 * 6.273092269897461
Epoch 490, val loss: 0.8146236538887024
Epoch 500, training loss: 0.6798958778381348 = 0.05198696255683899 + 0.1 * 6.279089450836182
Epoch 500, val loss: 0.8247326612472534
Epoch 510, training loss: 0.674689531326294 = 0.048318177461624146 + 0.1 * 6.2637128829956055
Epoch 510, val loss: 0.8348924517631531
Epoch 520, training loss: 0.6709384322166443 = 0.04497997462749481 + 0.1 * 6.259584426879883
Epoch 520, val loss: 0.8448932766914368
Epoch 530, training loss: 0.6675130724906921 = 0.04195838049054146 + 0.1 * 6.255546569824219
Epoch 530, val loss: 0.8547070622444153
Epoch 540, training loss: 0.6638518571853638 = 0.039211712777614594 + 0.1 * 6.246401786804199
Epoch 540, val loss: 0.8645111918449402
Epoch 550, training loss: 0.6624870300292969 = 0.03671059384942055 + 0.1 * 6.257763862609863
Epoch 550, val loss: 0.8741228580474854
Epoch 560, training loss: 0.6597208380699158 = 0.034437719732522964 + 0.1 * 6.252830982208252
Epoch 560, val loss: 0.8835606575012207
Epoch 570, training loss: 0.6560717225074768 = 0.032368287444114685 + 0.1 * 6.237033843994141
Epoch 570, val loss: 0.89283287525177
Epoch 580, training loss: 0.654316246509552 = 0.030471595004200935 + 0.1 * 6.238446235656738
Epoch 580, val loss: 0.9019412994384766
Epoch 590, training loss: 0.6524049043655396 = 0.02873309887945652 + 0.1 * 6.23671817779541
Epoch 590, val loss: 0.9107402563095093
Epoch 600, training loss: 0.6499038934707642 = 0.027143241837620735 + 0.1 * 6.227606296539307
Epoch 600, val loss: 0.9195889234542847
Epoch 610, training loss: 0.649333655834198 = 0.025677675381302834 + 0.1 * 6.2365593910217285
Epoch 610, val loss: 0.9280077219009399
Epoch 620, training loss: 0.6464599370956421 = 0.024328133091330528 + 0.1 * 6.221318244934082
Epoch 620, val loss: 0.9364269375801086
Epoch 630, training loss: 0.6464449167251587 = 0.02308098040521145 + 0.1 * 6.233639240264893
Epoch 630, val loss: 0.9445958733558655
Epoch 640, training loss: 0.6444900631904602 = 0.021929128095507622 + 0.1 * 6.225608825683594
Epoch 640, val loss: 0.952532172203064
Epoch 650, training loss: 0.6422837376594543 = 0.020863179117441177 + 0.1 * 6.214205741882324
Epoch 650, val loss: 0.9604006409645081
Epoch 660, training loss: 0.6413857340812683 = 0.019873198121786118 + 0.1 * 6.21512508392334
Epoch 660, val loss: 0.9678930044174194
Epoch 670, training loss: 0.6397138833999634 = 0.018955618143081665 + 0.1 * 6.207581996917725
Epoch 670, val loss: 0.9755061268806458
Epoch 680, training loss: 0.6396130919456482 = 0.018100004643201828 + 0.1 * 6.21513032913208
Epoch 680, val loss: 0.9828173518180847
Epoch 690, training loss: 0.6385219097137451 = 0.01730305142700672 + 0.1 * 6.212188243865967
Epoch 690, val loss: 0.9898375272750854
Epoch 700, training loss: 0.6368239521980286 = 0.016561606898903847 + 0.1 * 6.202622890472412
Epoch 700, val loss: 0.9969377517700195
Epoch 710, training loss: 0.6360597610473633 = 0.01586722582578659 + 0.1 * 6.201925277709961
Epoch 710, val loss: 1.003844976425171
Epoch 720, training loss: 0.6348996162414551 = 0.015216439962387085 + 0.1 * 6.196831703186035
Epoch 720, val loss: 1.01041841506958
Epoch 730, training loss: 0.6346448659896851 = 0.014607729390263557 + 0.1 * 6.200371265411377
Epoch 730, val loss: 1.0170599222183228
Epoch 740, training loss: 0.6330181360244751 = 0.014036561362445354 + 0.1 * 6.189815998077393
Epoch 740, val loss: 1.0234333276748657
Epoch 750, training loss: 0.6332550644874573 = 0.013500339351594448 + 0.1 * 6.197547435760498
Epoch 750, val loss: 1.0297211408615112
Epoch 760, training loss: 0.6315279603004456 = 0.01299560721963644 + 0.1 * 6.185323238372803
Epoch 760, val loss: 1.0358761548995972
Epoch 770, training loss: 0.6326665878295898 = 0.0125200180336833 + 0.1 * 6.201465606689453
Epoch 770, val loss: 1.041914939880371
Epoch 780, training loss: 0.6312324404716492 = 0.012070941738784313 + 0.1 * 6.191614627838135
Epoch 780, val loss: 1.047650694847107
Epoch 790, training loss: 0.6302070021629333 = 0.011649006977677345 + 0.1 * 6.185579776763916
Epoch 790, val loss: 1.0536137819290161
Epoch 800, training loss: 0.6302470564842224 = 0.01124858669936657 + 0.1 * 6.1899847984313965
Epoch 800, val loss: 1.0591398477554321
Epoch 810, training loss: 0.628408670425415 = 0.010870084166526794 + 0.1 * 6.175385475158691
Epoch 810, val loss: 1.0646809339523315
Epoch 820, training loss: 0.6282174587249756 = 0.01051238365471363 + 0.1 * 6.177050590515137
Epoch 820, val loss: 1.070278525352478
Epoch 830, training loss: 0.6283029317855835 = 0.01017194427549839 + 0.1 * 6.181309700012207
Epoch 830, val loss: 1.0755419731140137
Epoch 840, training loss: 0.6277370452880859 = 0.009849049150943756 + 0.1 * 6.178879737854004
Epoch 840, val loss: 1.0806595087051392
Epoch 850, training loss: 0.6266906261444092 = 0.009543227963149548 + 0.1 * 6.171473979949951
Epoch 850, val loss: 1.0859673023223877
Epoch 860, training loss: 0.6262251138687134 = 0.009251781739294529 + 0.1 * 6.169733047485352
Epoch 860, val loss: 1.0910472869873047
Epoch 870, training loss: 0.6262714862823486 = 0.00897361058741808 + 0.1 * 6.17297887802124
Epoch 870, val loss: 1.0957762002944946
Epoch 880, training loss: 0.6257309913635254 = 0.008710113354027271 + 0.1 * 6.170208930969238
Epoch 880, val loss: 1.1007564067840576
Epoch 890, training loss: 0.625464916229248 = 0.008458657190203667 + 0.1 * 6.17006254196167
Epoch 890, val loss: 1.1054983139038086
Epoch 900, training loss: 0.6244967579841614 = 0.008219347335398197 + 0.1 * 6.162773609161377
Epoch 900, val loss: 1.1102111339569092
Epoch 910, training loss: 0.6237216591835022 = 0.007990257814526558 + 0.1 * 6.157314300537109
Epoch 910, val loss: 1.114876627922058
Epoch 920, training loss: 0.6254345178604126 = 0.007770662195980549 + 0.1 * 6.176638603210449
Epoch 920, val loss: 1.1192790269851685
Epoch 930, training loss: 0.6239420771598816 = 0.007560794707387686 + 0.1 * 6.163812637329102
Epoch 930, val loss: 1.1236763000488281
Epoch 940, training loss: 0.6243630051612854 = 0.007360887248069048 + 0.1 * 6.170021057128906
Epoch 940, val loss: 1.1281317472457886
Epoch 950, training loss: 0.6230406165122986 = 0.007169496733695269 + 0.1 * 6.158710956573486
Epoch 950, val loss: 1.1324636936187744
Epoch 960, training loss: 0.6227269172668457 = 0.006986082531511784 + 0.1 * 6.157408237457275
Epoch 960, val loss: 1.1366218328475952
Epoch 970, training loss: 0.6219208836555481 = 0.006810463033616543 + 0.1 * 6.151103973388672
Epoch 970, val loss: 1.140777587890625
Epoch 980, training loss: 0.6223739981651306 = 0.006642058026045561 + 0.1 * 6.15731954574585
Epoch 980, val loss: 1.1449211835861206
Epoch 990, training loss: 0.6216337084770203 = 0.00647982070222497 + 0.1 * 6.151538848876953
Epoch 990, val loss: 1.148786187171936
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5387
Flip ASR: 0.4489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7857894897460938 = 1.9484165906906128 + 0.1 * 8.373727798461914
Epoch 0, val loss: 1.9395883083343506
Epoch 10, training loss: 2.7747576236724854 = 1.9374183416366577 + 0.1 * 8.373392105102539
Epoch 10, val loss: 1.927821397781372
Epoch 20, training loss: 2.760962963104248 = 1.9238102436065674 + 0.1 * 8.371526718139648
Epoch 20, val loss: 1.9132962226867676
Epoch 30, training loss: 2.7408952713012695 = 1.9048904180526733 + 0.1 * 8.3600492477417
Epoch 30, val loss: 1.8930686712265015
Epoch 40, training loss: 2.707052230834961 = 1.877394676208496 + 0.1 * 8.296574592590332
Epoch 40, val loss: 1.8641036748886108
Epoch 50, training loss: 2.6283552646636963 = 1.8408790826797485 + 0.1 * 7.874762058258057
Epoch 50, val loss: 1.8278144598007202
Epoch 60, training loss: 2.542316436767578 = 1.8014448881149292 + 0.1 * 7.408714771270752
Epoch 60, val loss: 1.7907769680023193
Epoch 70, training loss: 2.4679527282714844 = 1.7609330415725708 + 0.1 * 7.070197105407715
Epoch 70, val loss: 1.7556451559066772
Epoch 80, training loss: 2.4035840034484863 = 1.719434380531311 + 0.1 * 6.841495037078857
Epoch 80, val loss: 1.7216241359710693
Epoch 90, training loss: 2.3405051231384277 = 1.665390968322754 + 0.1 * 6.75114107131958
Epoch 90, val loss: 1.674308180809021
Epoch 100, training loss: 2.263634443283081 = 1.5930120944976807 + 0.1 * 6.706223011016846
Epoch 100, val loss: 1.6106549501419067
Epoch 110, training loss: 2.1711173057556152 = 1.503141164779663 + 0.1 * 6.6797614097595215
Epoch 110, val loss: 1.536363124847412
Epoch 120, training loss: 2.069009304046631 = 1.402632236480713 + 0.1 * 6.663769245147705
Epoch 120, val loss: 1.4557018280029297
Epoch 130, training loss: 1.9646198749542236 = 1.2996692657470703 + 0.1 * 6.649505615234375
Epoch 130, val loss: 1.3756822347640991
Epoch 140, training loss: 1.8618841171264648 = 1.198674201965332 + 0.1 * 6.632099151611328
Epoch 140, val loss: 1.300247311592102
Epoch 150, training loss: 1.7634270191192627 = 1.1024649143218994 + 0.1 * 6.609621047973633
Epoch 150, val loss: 1.2293676137924194
Epoch 160, training loss: 1.6728181838989258 = 1.0140963792800903 + 0.1 * 6.587218761444092
Epoch 160, val loss: 1.1652591228485107
Epoch 170, training loss: 1.5915911197662354 = 0.9350841641426086 + 0.1 * 6.56506872177124
Epoch 170, val loss: 1.1098943948745728
Epoch 180, training loss: 1.5195209980010986 = 0.8650237917900085 + 0.1 * 6.544971942901611
Epoch 180, val loss: 1.0621954202651978
Epoch 190, training loss: 1.455779790878296 = 0.803497314453125 + 0.1 * 6.522824764251709
Epoch 190, val loss: 1.021985650062561
Epoch 200, training loss: 1.400423288345337 = 0.7497469186782837 + 0.1 * 6.506763458251953
Epoch 200, val loss: 0.9886891841888428
Epoch 210, training loss: 1.3520368337631226 = 0.7039640545845032 + 0.1 * 6.480727672576904
Epoch 210, val loss: 0.9621974229812622
Epoch 220, training loss: 1.3107047080993652 = 0.6641522645950317 + 0.1 * 6.465525150299072
Epoch 220, val loss: 0.9410070776939392
Epoch 230, training loss: 1.2740299701690674 = 0.6285985708236694 + 0.1 * 6.4543137550354
Epoch 230, val loss: 0.9238271713256836
Epoch 240, training loss: 1.2384549379348755 = 0.5951910018920898 + 0.1 * 6.432639122009277
Epoch 240, val loss: 0.9097605347633362
Epoch 250, training loss: 1.2065541744232178 = 0.5623701214790344 + 0.1 * 6.441839694976807
Epoch 250, val loss: 0.8972036838531494
Epoch 260, training loss: 1.170276165008545 = 0.5296667814254761 + 0.1 * 6.406093597412109
Epoch 260, val loss: 0.8851368427276611
Epoch 270, training loss: 1.1360054016113281 = 0.49648696184158325 + 0.1 * 6.395183563232422
Epoch 270, val loss: 0.8733468651771545
Epoch 280, training loss: 1.1027565002441406 = 0.4630416929721832 + 0.1 * 6.3971476554870605
Epoch 280, val loss: 0.8621216416358948
Epoch 290, training loss: 1.0678441524505615 = 0.42997726798057556 + 0.1 * 6.378669261932373
Epoch 290, val loss: 0.8522582054138184
Epoch 300, training loss: 1.0348200798034668 = 0.39768466353416443 + 0.1 * 6.371354103088379
Epoch 300, val loss: 0.8444480299949646
Epoch 310, training loss: 1.0032472610473633 = 0.3666585683822632 + 0.1 * 6.365886688232422
Epoch 310, val loss: 0.8391069173812866
Epoch 320, training loss: 0.9735959768295288 = 0.3374752402305603 + 0.1 * 6.361207008361816
Epoch 320, val loss: 0.8361760973930359
Epoch 330, training loss: 0.9458163380622864 = 0.3103247284889221 + 0.1 * 6.354916095733643
Epoch 330, val loss: 0.8353813886642456
Epoch 340, training loss: 0.9196689128875732 = 0.2851906716823578 + 0.1 * 6.344781875610352
Epoch 340, val loss: 0.8364844918251038
Epoch 350, training loss: 0.8967045545578003 = 0.26209941506385803 + 0.1 * 6.34605073928833
Epoch 350, val loss: 0.8390758037567139
Epoch 360, training loss: 0.874561607837677 = 0.24097347259521484 + 0.1 * 6.335881233215332
Epoch 360, val loss: 0.8432648181915283
Epoch 370, training loss: 0.8553221821784973 = 0.22158969938755035 + 0.1 * 6.337324619293213
Epoch 370, val loss: 0.8486720323562622
Epoch 380, training loss: 0.8360069990158081 = 0.20375332236289978 + 0.1 * 6.322536945343018
Epoch 380, val loss: 0.8554633855819702
Epoch 390, training loss: 0.819531261920929 = 0.18718110024929047 + 0.1 * 6.323501110076904
Epoch 390, val loss: 0.8632357120513916
Epoch 400, training loss: 0.8048163056373596 = 0.17193372547626495 + 0.1 * 6.328825950622559
Epoch 400, val loss: 0.8714946508407593
Epoch 410, training loss: 0.789505124092102 = 0.15798541903495789 + 0.1 * 6.315196514129639
Epoch 410, val loss: 0.8802531957626343
Epoch 420, training loss: 0.7767220735549927 = 0.14519251883029938 + 0.1 * 6.315295696258545
Epoch 420, val loss: 0.8896767497062683
Epoch 430, training loss: 0.7645770311355591 = 0.1335003674030304 + 0.1 * 6.310766696929932
Epoch 430, val loss: 0.8996034264564514
Epoch 440, training loss: 0.7528225779533386 = 0.12287187576293945 + 0.1 * 6.299506664276123
Epoch 440, val loss: 0.9094396829605103
Epoch 450, training loss: 0.7431026697158813 = 0.1132606789469719 + 0.1 * 6.29841947555542
Epoch 450, val loss: 0.9197579622268677
Epoch 460, training loss: 0.734734833240509 = 0.10453619062900543 + 0.1 * 6.301986217498779
Epoch 460, val loss: 0.9302971363067627
Epoch 470, training loss: 0.7260974645614624 = 0.09662522375583649 + 0.1 * 6.294722557067871
Epoch 470, val loss: 0.9413423538208008
Epoch 480, training loss: 0.718468964099884 = 0.08941183984279633 + 0.1 * 6.290571212768555
Epoch 480, val loss: 0.9527590274810791
Epoch 490, training loss: 0.7116007208824158 = 0.08281771838665009 + 0.1 * 6.287829875946045
Epoch 490, val loss: 0.9643599987030029
Epoch 500, training loss: 0.7057333588600159 = 0.07679199427366257 + 0.1 * 6.2894134521484375
Epoch 500, val loss: 0.9761270880699158
Epoch 510, training loss: 0.698664128780365 = 0.07128199189901352 + 0.1 * 6.2738213539123535
Epoch 510, val loss: 0.9879819750785828
Epoch 520, training loss: 0.6933314204216003 = 0.06621725112199783 + 0.1 * 6.271141529083252
Epoch 520, val loss: 0.9996833205223083
Epoch 530, training loss: 0.688781201839447 = 0.061567384749650955 + 0.1 * 6.2721381187438965
Epoch 530, val loss: 1.0112794637680054
Epoch 540, training loss: 0.6839472055435181 = 0.057313140481710434 + 0.1 * 6.266340255737305
Epoch 540, val loss: 1.0226988792419434
Epoch 550, training loss: 0.6795702576637268 = 0.0534178800880909 + 0.1 * 6.261523723602295
Epoch 550, val loss: 1.0342298746109009
Epoch 560, training loss: 0.6767146587371826 = 0.0498538538813591 + 0.1 * 6.268608093261719
Epoch 560, val loss: 1.0455042123794556
Epoch 570, training loss: 0.6726829409599304 = 0.04660538583993912 + 0.1 * 6.260775089263916
Epoch 570, val loss: 1.0567506551742554
Epoch 580, training loss: 0.6695981025695801 = 0.04363709315657616 + 0.1 * 6.259609699249268
Epoch 580, val loss: 1.0679517984390259
Epoch 590, training loss: 0.6659005284309387 = 0.04092421755194664 + 0.1 * 6.249763488769531
Epoch 590, val loss: 1.0790305137634277
Epoch 600, training loss: 0.6641669273376465 = 0.038437217473983765 + 0.1 * 6.257297515869141
Epoch 600, val loss: 1.0899207592010498
Epoch 610, training loss: 0.6611615419387817 = 0.03615957498550415 + 0.1 * 6.250019550323486
Epoch 610, val loss: 1.100874423980713
Epoch 620, training loss: 0.6576812267303467 = 0.03407185897231102 + 0.1 * 6.236093521118164
Epoch 620, val loss: 1.1115148067474365
Epoch 630, training loss: 0.6565384268760681 = 0.032151706516742706 + 0.1 * 6.24386739730835
Epoch 630, val loss: 1.122108817100525
Epoch 640, training loss: 0.6551810503005981 = 0.030388696119189262 + 0.1 * 6.247923374176025
Epoch 640, val loss: 1.132354974746704
Epoch 650, training loss: 0.652213454246521 = 0.028766484931111336 + 0.1 * 6.234469413757324
Epoch 650, val loss: 1.1427462100982666
Epoch 660, training loss: 0.6498566269874573 = 0.027259815484285355 + 0.1 * 6.225967884063721
Epoch 660, val loss: 1.1527742147445679
Epoch 670, training loss: 0.6484150290489197 = 0.025863662362098694 + 0.1 * 6.225513458251953
Epoch 670, val loss: 1.162723422050476
Epoch 680, training loss: 0.6469253301620483 = 0.02457006648182869 + 0.1 * 6.223552703857422
Epoch 680, val loss: 1.1724207401275635
Epoch 690, training loss: 0.6455096006393433 = 0.023373547941446304 + 0.1 * 6.221360206604004
Epoch 690, val loss: 1.182033896446228
Epoch 700, training loss: 0.645622968673706 = 0.02226237952709198 + 0.1 * 6.23360538482666
Epoch 700, val loss: 1.19122314453125
Epoch 710, training loss: 0.6430896520614624 = 0.021233856678009033 + 0.1 * 6.218557834625244
Epoch 710, val loss: 1.2004612684249878
Epoch 720, training loss: 0.6410902142524719 = 0.020276056602597237 + 0.1 * 6.208141326904297
Epoch 720, val loss: 1.2096004486083984
Epoch 730, training loss: 0.6400226354598999 = 0.019381284713745117 + 0.1 * 6.206413269042969
Epoch 730, val loss: 1.2185200452804565
Epoch 740, training loss: 0.6388633847236633 = 0.01854638196527958 + 0.1 * 6.203169822692871
Epoch 740, val loss: 1.2269251346588135
Epoch 750, training loss: 0.6381217837333679 = 0.01776770129799843 + 0.1 * 6.203540325164795
Epoch 750, val loss: 1.235535740852356
Epoch 760, training loss: 0.637891948223114 = 0.017037613317370415 + 0.1 * 6.208543300628662
Epoch 760, val loss: 1.2437591552734375
Epoch 770, training loss: 0.6361881494522095 = 0.016353951767086983 + 0.1 * 6.1983418464660645
Epoch 770, val loss: 1.2519946098327637
Epoch 780, training loss: 0.6352583169937134 = 0.01570955663919449 + 0.1 * 6.195487022399902
Epoch 780, val loss: 1.2600762844085693
Epoch 790, training loss: 0.6338791251182556 = 0.015103260055184364 + 0.1 * 6.187758445739746
Epoch 790, val loss: 1.2678037881851196
Epoch 800, training loss: 0.6346853375434875 = 0.0145333930850029 + 0.1 * 6.201519012451172
Epoch 800, val loss: 1.2755743265151978
Epoch 810, training loss: 0.6336787343025208 = 0.01399619784206152 + 0.1 * 6.1968255043029785
Epoch 810, val loss: 1.2829680442810059
Epoch 820, training loss: 0.6327142715454102 = 0.013489611446857452 + 0.1 * 6.192246437072754
Epoch 820, val loss: 1.290504813194275
Epoch 830, training loss: 0.6315356492996216 = 0.013011181727051735 + 0.1 * 6.185244560241699
Epoch 830, val loss: 1.2977678775787354
Epoch 840, training loss: 0.6307077407836914 = 0.012558769434690475 + 0.1 * 6.181489944458008
Epoch 840, val loss: 1.3050520420074463
Epoch 850, training loss: 0.6312955617904663 = 0.012131048366427422 + 0.1 * 6.19164514541626
Epoch 850, val loss: 1.3119481801986694
Epoch 860, training loss: 0.6298903822898865 = 0.0117282560095191 + 0.1 * 6.181621551513672
Epoch 860, val loss: 1.3187366724014282
Epoch 870, training loss: 0.6295738816261292 = 0.011346383020281792 + 0.1 * 6.18227481842041
Epoch 870, val loss: 1.32547926902771
Epoch 880, training loss: 0.6282112002372742 = 0.01098377164453268 + 0.1 * 6.172274112701416
Epoch 880, val loss: 1.3322762250900269
Epoch 890, training loss: 0.6279874444007874 = 0.010638918727636337 + 0.1 * 6.173485279083252
Epoch 890, val loss: 1.338743805885315
Epoch 900, training loss: 0.6289021372795105 = 0.010311245918273926 + 0.1 * 6.185908794403076
Epoch 900, val loss: 1.3450496196746826
Epoch 910, training loss: 0.6269523501396179 = 0.009999732486903667 + 0.1 * 6.169526100158691
Epoch 910, val loss: 1.3510617017745972
Epoch 920, training loss: 0.6261505484580994 = 0.009703905321657658 + 0.1 * 6.16446590423584
Epoch 920, val loss: 1.3573991060256958
Epoch 930, training loss: 0.62615966796875 = 0.00942085962742567 + 0.1 * 6.167387962341309
Epoch 930, val loss: 1.3634331226348877
Epoch 940, training loss: 0.6264918446540833 = 0.009150429628789425 + 0.1 * 6.1734137535095215
Epoch 940, val loss: 1.369139313697815
Epoch 950, training loss: 0.6254538297653198 = 0.008893443271517754 + 0.1 * 6.165604114532471
Epoch 950, val loss: 1.3748174905776978
Epoch 960, training loss: 0.6254697442054749 = 0.008648070506751537 + 0.1 * 6.168216228485107
Epoch 960, val loss: 1.3805269002914429
Epoch 970, training loss: 0.624557375907898 = 0.0084143141284585 + 0.1 * 6.161430358886719
Epoch 970, val loss: 1.386371374130249
Epoch 980, training loss: 0.6235989928245544 = 0.008190329186618328 + 0.1 * 6.154086589813232
Epoch 980, val loss: 1.3919674158096313
Epoch 990, training loss: 0.624530017375946 = 0.007976113818585873 + 0.1 * 6.165538787841797
Epoch 990, val loss: 1.397281289100647
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.1993
Flip ASR: 0.1956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.780198812484741 = 1.942815899848938 + 0.1 * 8.37382984161377
Epoch 0, val loss: 1.9338469505310059
Epoch 10, training loss: 2.768733024597168 = 1.931354284286499 + 0.1 * 8.373787879943848
Epoch 10, val loss: 1.9213067293167114
Epoch 20, training loss: 2.7545530796051025 = 1.9172115325927734 + 0.1 * 8.373414993286133
Epoch 20, val loss: 1.9045742750167847
Epoch 30, training loss: 2.734700918197632 = 1.8975592851638794 + 0.1 * 8.371416091918945
Epoch 30, val loss: 1.880683422088623
Epoch 40, training loss: 2.7065606117248535 = 1.8707948923110962 + 0.1 * 8.357656478881836
Epoch 40, val loss: 1.8502243757247925
Epoch 50, training loss: 2.662586212158203 = 1.836254358291626 + 0.1 * 8.26331901550293
Epoch 50, val loss: 1.8156695365905762
Epoch 60, training loss: 2.580165386199951 = 1.7952370643615723 + 0.1 * 7.849283695220947
Epoch 60, val loss: 1.7788540124893188
Epoch 70, training loss: 2.508114814758301 = 1.7526657581329346 + 0.1 * 7.5544915199279785
Epoch 70, val loss: 1.7432974576950073
Epoch 80, training loss: 2.430176019668579 = 1.7102769613265991 + 0.1 * 7.198990345001221
Epoch 80, val loss: 1.709372878074646
Epoch 90, training loss: 2.3559184074401855 = 1.6600370407104492 + 0.1 * 6.958812713623047
Epoch 90, val loss: 1.6670458316802979
Epoch 100, training loss: 2.2755627632141113 = 1.5904916524887085 + 0.1 * 6.850712299346924
Epoch 100, val loss: 1.6084167957305908
Epoch 110, training loss: 2.1799819469451904 = 1.5018922090530396 + 0.1 * 6.780897617340088
Epoch 110, val loss: 1.5377404689788818
Epoch 120, training loss: 2.077078104019165 = 1.4035866260528564 + 0.1 * 6.734915256500244
Epoch 120, val loss: 1.4621715545654297
Epoch 130, training loss: 1.9773626327514648 = 1.3064810037612915 + 0.1 * 6.7088165283203125
Epoch 130, val loss: 1.3919965028762817
Epoch 140, training loss: 1.8852119445800781 = 1.2166742086410522 + 0.1 * 6.68537712097168
Epoch 140, val loss: 1.3315613269805908
Epoch 150, training loss: 1.802560567855835 = 1.135703682899475 + 0.1 * 6.6685686111450195
Epoch 150, val loss: 1.278987169265747
Epoch 160, training loss: 1.7268426418304443 = 1.0623071193695068 + 0.1 * 6.645355224609375
Epoch 160, val loss: 1.2310950756072998
Epoch 170, training loss: 1.6552255153656006 = 0.9925114512443542 + 0.1 * 6.627140522003174
Epoch 170, val loss: 1.1851332187652588
Epoch 180, training loss: 1.5861752033233643 = 0.924768328666687 + 0.1 * 6.614068984985352
Epoch 180, val loss: 1.139577031135559
Epoch 190, training loss: 1.5184988975524902 = 0.8586437702178955 + 0.1 * 6.5985517501831055
Epoch 190, val loss: 1.0940355062484741
Epoch 200, training loss: 1.4514961242675781 = 0.7927457094192505 + 0.1 * 6.587503910064697
Epoch 200, val loss: 1.047317385673523
Epoch 210, training loss: 1.3840632438659668 = 0.7266788482666016 + 0.1 * 6.5738444328308105
Epoch 210, val loss: 0.9998341798782349
Epoch 220, training loss: 1.3181486129760742 = 0.6610356569290161 + 0.1 * 6.57112979888916
Epoch 220, val loss: 0.9525928497314453
Epoch 230, training loss: 1.254396677017212 = 0.5986926555633545 + 0.1 * 6.557040691375732
Epoch 230, val loss: 0.9089938998222351
Epoch 240, training loss: 1.1939849853515625 = 0.5404384732246399 + 0.1 * 6.535465717315674
Epoch 240, val loss: 0.8701356649398804
Epoch 250, training loss: 1.1383562088012695 = 0.48647958040237427 + 0.1 * 6.518766403198242
Epoch 250, val loss: 0.8371160626411438
Epoch 260, training loss: 1.0879400968551636 = 0.43693092465400696 + 0.1 * 6.510091781616211
Epoch 260, val loss: 0.8104669451713562
Epoch 270, training loss: 1.041073203086853 = 0.39206939935684204 + 0.1 * 6.49003791809082
Epoch 270, val loss: 0.7905201315879822
Epoch 280, training loss: 0.9986613392829895 = 0.35093891620635986 + 0.1 * 6.477223873138428
Epoch 280, val loss: 0.7756534814834595
Epoch 290, training loss: 0.9623384475708008 = 0.31352534890174866 + 0.1 * 6.488130569458008
Epoch 290, val loss: 0.7652493119239807
Epoch 300, training loss: 0.9269086122512817 = 0.2803950011730194 + 0.1 * 6.46513557434082
Epoch 300, val loss: 0.7593078017234802
Epoch 310, training loss: 0.8948174715042114 = 0.2507376968860626 + 0.1 * 6.440797805786133
Epoch 310, val loss: 0.7568600177764893
Epoch 320, training loss: 0.8678621053695679 = 0.22440385818481445 + 0.1 * 6.434582233428955
Epoch 320, val loss: 0.757878839969635
Epoch 330, training loss: 0.844349205493927 = 0.2012251615524292 + 0.1 * 6.431240081787109
Epoch 330, val loss: 0.761849045753479
Epoch 340, training loss: 0.8226603865623474 = 0.18095426261425018 + 0.1 * 6.417060852050781
Epoch 340, val loss: 0.7685705423355103
Epoch 350, training loss: 0.8062624931335449 = 0.1633172184228897 + 0.1 * 6.429452896118164
Epoch 350, val loss: 0.7771976590156555
Epoch 360, training loss: 0.787741482257843 = 0.14804472029209137 + 0.1 * 6.39696741104126
Epoch 360, val loss: 0.7872334122657776
Epoch 370, training loss: 0.7733556032180786 = 0.13465777039527893 + 0.1 * 6.3869781494140625
Epoch 370, val loss: 0.7982975244522095
Epoch 380, training loss: 0.7624729871749878 = 0.12285739928483963 + 0.1 * 6.396155834197998
Epoch 380, val loss: 0.8101203441619873
Epoch 390, training loss: 0.7497551441192627 = 0.11245565861463547 + 0.1 * 6.372994422912598
Epoch 390, val loss: 0.8224940299987793
Epoch 400, training loss: 0.739815890789032 = 0.10323844105005264 + 0.1 * 6.365774154663086
Epoch 400, val loss: 0.8349202871322632
Epoch 410, training loss: 0.7311432957649231 = 0.09502708911895752 + 0.1 * 6.361162185668945
Epoch 410, val loss: 0.8477277159690857
Epoch 420, training loss: 0.7230187654495239 = 0.0876716673374176 + 0.1 * 6.353471279144287
Epoch 420, val loss: 0.8605663776397705
Epoch 430, training loss: 0.7160463929176331 = 0.08106154203414917 + 0.1 * 6.34984827041626
Epoch 430, val loss: 0.873335599899292
Epoch 440, training loss: 0.7091111540794373 = 0.07508726418018341 + 0.1 * 6.34023904800415
Epoch 440, val loss: 0.8859466314315796
Epoch 450, training loss: 0.702854335308075 = 0.06967808306217194 + 0.1 * 6.331762790679932
Epoch 450, val loss: 0.8984430432319641
Epoch 460, training loss: 0.6999396085739136 = 0.06475319713354111 + 0.1 * 6.351864337921143
Epoch 460, val loss: 0.9106442928314209
Epoch 470, training loss: 0.6925213932991028 = 0.06027979776263237 + 0.1 * 6.322415828704834
Epoch 470, val loss: 0.9226447343826294
Epoch 480, training loss: 0.687721312046051 = 0.05618736520409584 + 0.1 * 6.315339088439941
Epoch 480, val loss: 0.9343354105949402
Epoch 490, training loss: 0.6834546327590942 = 0.05243824049830437 + 0.1 * 6.310163497924805
Epoch 490, val loss: 0.9456972479820251
Epoch 500, training loss: 0.6792523860931396 = 0.0490056537091732 + 0.1 * 6.302467346191406
Epoch 500, val loss: 0.9568370580673218
Epoch 510, training loss: 0.6759257912635803 = 0.045848336070775986 + 0.1 * 6.300774574279785
Epoch 510, val loss: 0.9676816463470459
Epoch 520, training loss: 0.6733192205429077 = 0.042944520711898804 + 0.1 * 6.303747177124023
Epoch 520, val loss: 0.97828608751297
Epoch 530, training loss: 0.6700343489646912 = 0.04028083756566048 + 0.1 * 6.297535419464111
Epoch 530, val loss: 0.988551139831543
Epoch 540, training loss: 0.6663926243782043 = 0.0378333181142807 + 0.1 * 6.285593032836914
Epoch 540, val loss: 0.9986523389816284
Epoch 550, training loss: 0.663671612739563 = 0.035573460161685944 + 0.1 * 6.280981063842773
Epoch 550, val loss: 1.0085304975509644
Epoch 560, training loss: 0.660851001739502 = 0.033489715307950974 + 0.1 * 6.273612976074219
Epoch 560, val loss: 1.0181764364242554
Epoch 570, training loss: 0.6591339111328125 = 0.031570736318826675 + 0.1 * 6.275631904602051
Epoch 570, val loss: 1.0276644229888916
Epoch 580, training loss: 0.6568000316619873 = 0.029802249744534492 + 0.1 * 6.269977569580078
Epoch 580, val loss: 1.0368728637695312
Epoch 590, training loss: 0.6541999578475952 = 0.028172142803668976 + 0.1 * 6.260278224945068
Epoch 590, val loss: 1.045920968055725
Epoch 600, training loss: 0.6530919075012207 = 0.026665523648262024 + 0.1 * 6.264264106750488
Epoch 600, val loss: 1.054675817489624
Epoch 610, training loss: 0.6511328220367432 = 0.02527311071753502 + 0.1 * 6.258597373962402
Epoch 610, val loss: 1.063254714012146
Epoch 620, training loss: 0.6488664150238037 = 0.023985467851161957 + 0.1 * 6.248809337615967
Epoch 620, val loss: 1.0715919733047485
Epoch 630, training loss: 0.6480361819267273 = 0.022792577743530273 + 0.1 * 6.25243616104126
Epoch 630, val loss: 1.0798548460006714
Epoch 640, training loss: 0.6457693576812744 = 0.021686401218175888 + 0.1 * 6.240829944610596
Epoch 640, val loss: 1.0879075527191162
Epoch 650, training loss: 0.644384503364563 = 0.020659152418375015 + 0.1 * 6.237253189086914
Epoch 650, val loss: 1.095766544342041
Epoch 660, training loss: 0.6430050730705261 = 0.01970432698726654 + 0.1 * 6.233007431030273
Epoch 660, val loss: 1.103455662727356
Epoch 670, training loss: 0.6429898142814636 = 0.018815752118825912 + 0.1 * 6.2417402267456055
Epoch 670, val loss: 1.1110097169876099
Epoch 680, training loss: 0.6412370800971985 = 0.01798713393509388 + 0.1 * 6.232499122619629
Epoch 680, val loss: 1.118314266204834
Epoch 690, training loss: 0.6393558979034424 = 0.017215708270668983 + 0.1 * 6.221401691436768
Epoch 690, val loss: 1.1255180835723877
Epoch 700, training loss: 0.6397113800048828 = 0.0164936613291502 + 0.1 * 6.232176780700684
Epoch 700, val loss: 1.1325653791427612
Epoch 710, training loss: 0.6381152868270874 = 0.015817862004041672 + 0.1 * 6.222973823547363
Epoch 710, val loss: 1.139372706413269
Epoch 720, training loss: 0.637628972530365 = 0.015184163115918636 + 0.1 * 6.224447727203369
Epoch 720, val loss: 1.1461924314498901
Epoch 730, training loss: 0.6353909373283386 = 0.01458938978612423 + 0.1 * 6.208015441894531
Epoch 730, val loss: 1.1528409719467163
Epoch 740, training loss: 0.6352763175964355 = 0.014029641635715961 + 0.1 * 6.212466716766357
Epoch 740, val loss: 1.1593486070632935
Epoch 750, training loss: 0.6344974040985107 = 0.013502287678420544 + 0.1 * 6.209950923919678
Epoch 750, val loss: 1.1656804084777832
Epoch 760, training loss: 0.63358074426651 = 0.01300677191466093 + 0.1 * 6.205739974975586
Epoch 760, val loss: 1.1718816757202148
Epoch 770, training loss: 0.6328449845314026 = 0.012539306655526161 + 0.1 * 6.203056335449219
Epoch 770, val loss: 1.1778007745742798
Epoch 780, training loss: 0.6312796473503113 = 0.01209965068846941 + 0.1 * 6.191800117492676
Epoch 780, val loss: 1.1838154792785645
Epoch 790, training loss: 0.6308160424232483 = 0.01168323215097189 + 0.1 * 6.191328048706055
Epoch 790, val loss: 1.189683198928833
Epoch 800, training loss: 0.6314135789871216 = 0.011288615874946117 + 0.1 * 6.201249122619629
Epoch 800, val loss: 1.1953758001327515
Epoch 810, training loss: 0.6319137215614319 = 0.010914886370301247 + 0.1 * 6.209988594055176
Epoch 810, val loss: 1.2009055614471436
Epoch 820, training loss: 0.6297915577888489 = 0.010560917668044567 + 0.1 * 6.192306041717529
Epoch 820, val loss: 1.2063945531845093
Epoch 830, training loss: 0.6291318535804749 = 0.010225355625152588 + 0.1 * 6.189064979553223
Epoch 830, val loss: 1.2118819952011108
Epoch 840, training loss: 0.629228949546814 = 0.00990601908415556 + 0.1 * 6.193229675292969
Epoch 840, val loss: 1.2171530723571777
Epoch 850, training loss: 0.6274778842926025 = 0.009603068232536316 + 0.1 * 6.17874813079834
Epoch 850, val loss: 1.2223560810089111
Epoch 860, training loss: 0.6273738145828247 = 0.00931459665298462 + 0.1 * 6.180592060089111
Epoch 860, val loss: 1.2275444269180298
Epoch 870, training loss: 0.6266234517097473 = 0.009039845317602158 + 0.1 * 6.175836086273193
Epoch 870, val loss: 1.2325836420059204
Epoch 880, training loss: 0.6271888613700867 = 0.00877835601568222 + 0.1 * 6.184105396270752
Epoch 880, val loss: 1.2375710010528564
Epoch 890, training loss: 0.6260840892791748 = 0.008528733626008034 + 0.1 * 6.175553798675537
Epoch 890, val loss: 1.2424412965774536
Epoch 900, training loss: 0.6260968446731567 = 0.008290872909128666 + 0.1 * 6.1780595779418945
Epoch 900, val loss: 1.2472702264785767
Epoch 910, training loss: 0.6248658895492554 = 0.008063736371695995 + 0.1 * 6.1680216789245605
Epoch 910, val loss: 1.2520828247070312
Epoch 920, training loss: 0.6252866983413696 = 0.007846316322684288 + 0.1 * 6.174403667449951
Epoch 920, val loss: 1.2566450834274292
Epoch 930, training loss: 0.6236862540245056 = 0.007638479582965374 + 0.1 * 6.160478115081787
Epoch 930, val loss: 1.2611665725708008
Epoch 940, training loss: 0.624241292476654 = 0.007439588196575642 + 0.1 * 6.16801643371582
Epoch 940, val loss: 1.2657190561294556
Epoch 950, training loss: 0.6230931282043457 = 0.007248996756970882 + 0.1 * 6.158441066741943
Epoch 950, val loss: 1.270114779472351
Epoch 960, training loss: 0.6248713135719299 = 0.007066252641379833 + 0.1 * 6.178050518035889
Epoch 960, val loss: 1.2744405269622803
Epoch 970, training loss: 0.6225078701972961 = 0.006890853866934776 + 0.1 * 6.15617036819458
Epoch 970, val loss: 1.2786445617675781
Epoch 980, training loss: 0.6227246522903442 = 0.006723026279360056 + 0.1 * 6.160016059875488
Epoch 980, val loss: 1.2829616069793701
Epoch 990, training loss: 0.6217585802078247 = 0.006561502814292908 + 0.1 * 6.151970386505127
Epoch 990, val loss: 1.2871196269989014
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.9077
Flip ASR: 0.8889/225 nodes
The final ASR:0.54859, 0.28932, Accuracy:0.80247, 0.01364
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10524])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.810161590576172 = 1.9727818965911865 + 0.1 * 8.373795509338379
Epoch 0, val loss: 1.970022201538086
Epoch 10, training loss: 2.799098491668701 = 1.9617365598678589 + 0.1 * 8.37362003326416
Epoch 10, val loss: 1.9600865840911865
Epoch 20, training loss: 2.7854180335998535 = 1.9481489658355713 + 0.1 * 8.372690200805664
Epoch 20, val loss: 1.9475871324539185
Epoch 30, training loss: 2.765223741531372 = 1.9285873174667358 + 0.1 * 8.366363525390625
Epoch 30, val loss: 1.9294203519821167
Epoch 40, training loss: 2.7310006618499756 = 1.8992137908935547 + 0.1 * 8.317869186401367
Epoch 40, val loss: 1.902308702468872
Epoch 50, training loss: 2.657097339630127 = 1.8590896129608154 + 0.1 * 7.980077266693115
Epoch 50, val loss: 1.866563320159912
Epoch 60, training loss: 2.5718884468078613 = 1.8180030584335327 + 0.1 * 7.538853645324707
Epoch 60, val loss: 1.8316917419433594
Epoch 70, training loss: 2.4859445095062256 = 1.7815769910812378 + 0.1 * 7.043675422668457
Epoch 70, val loss: 1.8033915758132935
Epoch 80, training loss: 2.4311232566833496 = 1.7503186464309692 + 0.1 * 6.808047294616699
Epoch 80, val loss: 1.7769039869308472
Epoch 90, training loss: 2.3810675144195557 = 1.710135817527771 + 0.1 * 6.709316253662109
Epoch 90, val loss: 1.7388322353363037
Epoch 100, training loss: 2.321023941040039 = 1.6550568342208862 + 0.1 * 6.659671306610107
Epoch 100, val loss: 1.689910888671875
Epoch 110, training loss: 2.2458598613739014 = 1.5833632946014404 + 0.1 * 6.624966144561768
Epoch 110, val loss: 1.6304967403411865
Epoch 120, training loss: 2.1572046279907227 = 1.4976880550384521 + 0.1 * 6.595164775848389
Epoch 120, val loss: 1.5589392185211182
Epoch 130, training loss: 2.064265727996826 = 1.4070801734924316 + 0.1 * 6.571856498718262
Epoch 130, val loss: 1.4841060638427734
Epoch 140, training loss: 1.9756340980529785 = 1.3201738595962524 + 0.1 * 6.55460262298584
Epoch 140, val loss: 1.4130949974060059
Epoch 150, training loss: 1.8894476890563965 = 1.235571026802063 + 0.1 * 6.538765907287598
Epoch 150, val loss: 1.3450895547866821
Epoch 160, training loss: 1.80340576171875 = 1.1511199474334717 + 0.1 * 6.522858142852783
Epoch 160, val loss: 1.2782220840454102
Epoch 170, training loss: 1.7174038887023926 = 1.066673755645752 + 0.1 * 6.50730037689209
Epoch 170, val loss: 1.2131893634796143
Epoch 180, training loss: 1.6326146125793457 = 0.9825544357299805 + 0.1 * 6.5006022453308105
Epoch 180, val loss: 1.1494033336639404
Epoch 190, training loss: 1.5502402782440186 = 0.9016476273536682 + 0.1 * 6.485926151275635
Epoch 190, val loss: 1.0885672569274902
Epoch 200, training loss: 1.4699177742004395 = 0.8226173520088196 + 0.1 * 6.47300386428833
Epoch 200, val loss: 1.0294909477233887
Epoch 210, training loss: 1.393904685974121 = 0.7468190789222717 + 0.1 * 6.470855236053467
Epoch 210, val loss: 0.9736732840538025
Epoch 220, training loss: 1.3223886489868164 = 0.6768530011177063 + 0.1 * 6.455355644226074
Epoch 220, val loss: 0.9237769246101379
Epoch 230, training loss: 1.2573292255401611 = 0.6129573583602905 + 0.1 * 6.443718910217285
Epoch 230, val loss: 0.8805719017982483
Epoch 240, training loss: 1.2010366916656494 = 0.5559231042861938 + 0.1 * 6.451136112213135
Epoch 240, val loss: 0.8450973033905029
Epoch 250, training loss: 1.1485148668289185 = 0.5057360529899597 + 0.1 * 6.427788257598877
Epoch 250, val loss: 0.8176562786102295
Epoch 260, training loss: 1.102595567703247 = 0.46069231629371643 + 0.1 * 6.419032096862793
Epoch 260, val loss: 0.7965561747550964
Epoch 270, training loss: 1.0612238645553589 = 0.42009592056274414 + 0.1 * 6.411279201507568
Epoch 270, val loss: 0.7806554436683655
Epoch 280, training loss: 1.0231449604034424 = 0.3832547664642334 + 0.1 * 6.398900985717773
Epoch 280, val loss: 0.7689721584320068
Epoch 290, training loss: 0.9886789321899414 = 0.3496176302433014 + 0.1 * 6.390613079071045
Epoch 290, val loss: 0.7607659697532654
Epoch 300, training loss: 0.9575482606887817 = 0.31898030638694763 + 0.1 * 6.385679244995117
Epoch 300, val loss: 0.7557308673858643
Epoch 310, training loss: 0.928196907043457 = 0.2909393012523651 + 0.1 * 6.3725762367248535
Epoch 310, val loss: 0.7535197138786316
Epoch 320, training loss: 0.9033834934234619 = 0.26524096727371216 + 0.1 * 6.381424903869629
Epoch 320, val loss: 0.7536616921424866
Epoch 330, training loss: 0.8778481483459473 = 0.24183820188045502 + 0.1 * 6.3600993156433105
Epoch 330, val loss: 0.7560637593269348
Epoch 340, training loss: 0.8550927639007568 = 0.2203662246465683 + 0.1 * 6.347265243530273
Epoch 340, val loss: 0.7604114413261414
Epoch 350, training loss: 0.8383616209030151 = 0.20065483450889587 + 0.1 * 6.377067565917969
Epoch 350, val loss: 0.7665964365005493
Epoch 360, training loss: 0.8168924450874329 = 0.1827266365289688 + 0.1 * 6.341658115386963
Epoch 360, val loss: 0.7744095325469971
Epoch 370, training loss: 0.7992518544197083 = 0.16636288166046143 + 0.1 * 6.3288893699646
Epoch 370, val loss: 0.7836943864822388
Epoch 380, training loss: 0.7852274775505066 = 0.15150882303714752 + 0.1 * 6.337186336517334
Epoch 380, val loss: 0.7941294312477112
Epoch 390, training loss: 0.770099401473999 = 0.1381182223558426 + 0.1 * 6.319811820983887
Epoch 390, val loss: 0.8056374788284302
Epoch 400, training loss: 0.7572019100189209 = 0.12601590156555176 + 0.1 * 6.311860084533691
Epoch 400, val loss: 0.8180055618286133
Epoch 410, training loss: 0.7468968629837036 = 0.11507126688957214 + 0.1 * 6.318256378173828
Epoch 410, val loss: 0.8310811519622803
Epoch 420, training loss: 0.7377299070358276 = 0.10522753745317459 + 0.1 * 6.325023651123047
Epoch 420, val loss: 0.8445885181427002
Epoch 430, training loss: 0.7263741493225098 = 0.09639403223991394 + 0.1 * 6.299800872802734
Epoch 430, val loss: 0.8584231734275818
Epoch 440, training loss: 0.7179509401321411 = 0.08843119442462921 + 0.1 * 6.295197486877441
Epoch 440, val loss: 0.8724655508995056
Epoch 450, training loss: 0.710303008556366 = 0.08125879615545273 + 0.1 * 6.29044246673584
Epoch 450, val loss: 0.8866073489189148
Epoch 460, training loss: 0.7035834789276123 = 0.07481973618268967 + 0.1 * 6.287637233734131
Epoch 460, val loss: 0.900680422782898
Epoch 470, training loss: 0.6975401639938354 = 0.06901568174362183 + 0.1 * 6.285244464874268
Epoch 470, val loss: 0.9146915674209595
Epoch 480, training loss: 0.6924329400062561 = 0.06378698348999023 + 0.1 * 6.286459445953369
Epoch 480, val loss: 0.9285674691200256
Epoch 490, training loss: 0.6864461898803711 = 0.05907057970762253 + 0.1 * 6.27375602722168
Epoch 490, val loss: 0.9422416687011719
Epoch 500, training loss: 0.6820499300956726 = 0.05480049178004265 + 0.1 * 6.272493839263916
Epoch 500, val loss: 0.9557327628135681
Epoch 510, training loss: 0.6777783036231995 = 0.05093969404697418 + 0.1 * 6.268385887145996
Epoch 510, val loss: 0.9689756035804749
Epoch 520, training loss: 0.6746007800102234 = 0.047445546835660934 + 0.1 * 6.271552562713623
Epoch 520, val loss: 0.9819505214691162
Epoch 530, training loss: 0.6705748438835144 = 0.04428361356258392 + 0.1 * 6.262911796569824
Epoch 530, val loss: 0.9945670962333679
Epoch 540, training loss: 0.6661716103553772 = 0.041412338614463806 + 0.1 * 6.247592449188232
Epoch 540, val loss: 1.0069087743759155
Epoch 550, training loss: 0.6644080877304077 = 0.03879135102033615 + 0.1 * 6.256167411804199
Epoch 550, val loss: 1.0190222263336182
Epoch 560, training loss: 0.6617692112922668 = 0.036406707018613815 + 0.1 * 6.25362491607666
Epoch 560, val loss: 1.0307672023773193
Epoch 570, training loss: 0.6586250066757202 = 0.03423430770635605 + 0.1 * 6.2439069747924805
Epoch 570, val loss: 1.0422343015670776
Epoch 580, training loss: 0.6559205055236816 = 0.03224162012338638 + 0.1 * 6.236788749694824
Epoch 580, val loss: 1.0534532070159912
Epoch 590, training loss: 0.6531766653060913 = 0.03041321597993374 + 0.1 * 6.227634429931641
Epoch 590, val loss: 1.0644352436065674
Epoch 600, training loss: 0.6547620892524719 = 0.028730662539601326 + 0.1 * 6.260313987731934
Epoch 600, val loss: 1.0751680135726929
Epoch 610, training loss: 0.6494436264038086 = 0.027186855673789978 + 0.1 * 6.222567558288574
Epoch 610, val loss: 1.0855923891067505
Epoch 620, training loss: 0.6487451791763306 = 0.02576444484293461 + 0.1 * 6.229806900024414
Epoch 620, val loss: 1.0957891941070557
Epoch 630, training loss: 0.6460919976234436 = 0.02444992959499359 + 0.1 * 6.216420650482178
Epoch 630, val loss: 1.1056914329528809
Epoch 640, training loss: 0.6475250720977783 = 0.023236829787492752 + 0.1 * 6.24288272857666
Epoch 640, val loss: 1.1153568029403687
Epoch 650, training loss: 0.6443386077880859 = 0.022114897146821022 + 0.1 * 6.2222371101379395
Epoch 650, val loss: 1.1247392892837524
Epoch 660, training loss: 0.6428012847900391 = 0.021076804026961327 + 0.1 * 6.217244625091553
Epoch 660, val loss: 1.1338942050933838
Epoch 670, training loss: 0.6412317752838135 = 0.02011091820895672 + 0.1 * 6.211208820343018
Epoch 670, val loss: 1.1428442001342773
Epoch 680, training loss: 0.6401057243347168 = 0.0192099679261446 + 0.1 * 6.208957195281982
Epoch 680, val loss: 1.1516239643096924
Epoch 690, training loss: 0.6396848559379578 = 0.018369439989328384 + 0.1 * 6.213154315948486
Epoch 690, val loss: 1.1602083444595337
Epoch 700, training loss: 0.6386101245880127 = 0.017584476619958878 + 0.1 * 6.210256099700928
Epoch 700, val loss: 1.1686118841171265
Epoch 710, training loss: 0.6373302340507507 = 0.016851862892508507 + 0.1 * 6.204783916473389
Epoch 710, val loss: 1.1767616271972656
Epoch 720, training loss: 0.6357311606407166 = 0.0161678995937109 + 0.1 * 6.195632457733154
Epoch 720, val loss: 1.1847550868988037
Epoch 730, training loss: 0.6354940533638 = 0.015525002032518387 + 0.1 * 6.199690341949463
Epoch 730, val loss: 1.192567229270935
Epoch 740, training loss: 0.6345279216766357 = 0.014920296147465706 + 0.1 * 6.196075916290283
Epoch 740, val loss: 1.200247883796692
Epoch 750, training loss: 0.6342962980270386 = 0.014352216385304928 + 0.1 * 6.199440956115723
Epoch 750, val loss: 1.207737922668457
Epoch 760, training loss: 0.6337443590164185 = 0.013819331303238869 + 0.1 * 6.199250221252441
Epoch 760, val loss: 1.2150497436523438
Epoch 770, training loss: 0.6315547823905945 = 0.013317979872226715 + 0.1 * 6.18236780166626
Epoch 770, val loss: 1.2222247123718262
Epoch 780, training loss: 0.6311715841293335 = 0.012844712473452091 + 0.1 * 6.183268070220947
Epoch 780, val loss: 1.229283094406128
Epoch 790, training loss: 0.6322099566459656 = 0.012396499514579773 + 0.1 * 6.198134422302246
Epoch 790, val loss: 1.236209750175476
Epoch 800, training loss: 0.6303837895393372 = 0.011973381973803043 + 0.1 * 6.184103965759277
Epoch 800, val loss: 1.2429758310317993
Epoch 810, training loss: 0.6304337978363037 = 0.011574303731322289 + 0.1 * 6.188594818115234
Epoch 810, val loss: 1.2496676445007324
Epoch 820, training loss: 0.6290319561958313 = 0.011195756494998932 + 0.1 * 6.178361415863037
Epoch 820, val loss: 1.25614595413208
Epoch 830, training loss: 0.6288178563117981 = 0.010837863199412823 + 0.1 * 6.179800033569336
Epoch 830, val loss: 1.2625846862792969
Epoch 840, training loss: 0.6280422806739807 = 0.01049629133194685 + 0.1 * 6.175459384918213
Epoch 840, val loss: 1.2688950300216675
Epoch 850, training loss: 0.6280034780502319 = 0.010172289796173573 + 0.1 * 6.178311824798584
Epoch 850, val loss: 1.275105357170105
Epoch 860, training loss: 0.6279535889625549 = 0.009864943102002144 + 0.1 * 6.180886268615723
Epoch 860, val loss: 1.2811596393585205
Epoch 870, training loss: 0.6266493201255798 = 0.009572251699864864 + 0.1 * 6.170770645141602
Epoch 870, val loss: 1.2871488332748413
Epoch 880, training loss: 0.6266639828681946 = 0.009293470531702042 + 0.1 * 6.173704624176025
Epoch 880, val loss: 1.2929641008377075
Epoch 890, training loss: 0.625575065612793 = 0.009028231725096703 + 0.1 * 6.165467739105225
Epoch 890, val loss: 1.298715591430664
Epoch 900, training loss: 0.6257510185241699 = 0.00877489522099495 + 0.1 * 6.1697611808776855
Epoch 900, val loss: 1.3043569326400757
Epoch 910, training loss: 0.6255369186401367 = 0.0085323266685009 + 0.1 * 6.170045375823975
Epoch 910, val loss: 1.3098952770233154
Epoch 920, training loss: 0.6243226528167725 = 0.008300792425870895 + 0.1 * 6.160218715667725
Epoch 920, val loss: 1.3153475522994995
Epoch 930, training loss: 0.6235997676849365 = 0.00808101799339056 + 0.1 * 6.155187129974365
Epoch 930, val loss: 1.3206760883331299
Epoch 940, training loss: 0.62334805727005 = 0.007869996130466461 + 0.1 * 6.154780387878418
Epoch 940, val loss: 1.3259668350219727
Epoch 950, training loss: 0.6241645216941833 = 0.0076671503484249115 + 0.1 * 6.164973735809326
Epoch 950, val loss: 1.3311493396759033
Epoch 960, training loss: 0.6236960291862488 = 0.0074729532934725285 + 0.1 * 6.162230968475342
Epoch 960, val loss: 1.3362172842025757
Epoch 970, training loss: 0.6243550181388855 = 0.007287207525223494 + 0.1 * 6.17067813873291
Epoch 970, val loss: 1.3412190675735474
Epoch 980, training loss: 0.6221071481704712 = 0.007109099067747593 + 0.1 * 6.149980068206787
Epoch 980, val loss: 1.346153974533081
Epoch 990, training loss: 0.6223626732826233 = 0.006938643753528595 + 0.1 * 6.154240131378174
Epoch 990, val loss: 1.351014256477356
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.4945
Flip ASR: 0.3956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7704968452453613 = 1.933112621307373 + 0.1 * 8.373842239379883
Epoch 0, val loss: 1.929992914199829
Epoch 10, training loss: 2.7610042095184326 = 1.9236353635787964 + 0.1 * 8.373687744140625
Epoch 10, val loss: 1.9201040267944336
Epoch 20, training loss: 2.7493667602539062 = 1.9120782613754272 + 0.1 * 8.372885704040527
Epoch 20, val loss: 1.9077668190002441
Epoch 30, training loss: 2.732760190963745 = 1.895949363708496 + 0.1 * 8.368108749389648
Epoch 30, val loss: 1.8902714252471924
Epoch 40, training loss: 2.705341339111328 = 1.872233510017395 + 0.1 * 8.331079483032227
Epoch 40, val loss: 1.8645678758621216
Epoch 50, training loss: 2.6280741691589355 = 1.8399673700332642 + 0.1 * 7.881069183349609
Epoch 50, val loss: 1.8307092189788818
Epoch 60, training loss: 2.535733222961426 = 1.8074572086334229 + 0.1 * 7.282761096954346
Epoch 60, val loss: 1.7972670793533325
Epoch 70, training loss: 2.4697864055633545 = 1.7731231451034546 + 0.1 * 6.966632843017578
Epoch 70, val loss: 1.7635618448257446
Epoch 80, training loss: 2.420314073562622 = 1.735580325126648 + 0.1 * 6.84733772277832
Epoch 80, val loss: 1.7289013862609863
Epoch 90, training loss: 2.369582176208496 = 1.6913577318191528 + 0.1 * 6.782243251800537
Epoch 90, val loss: 1.6899816989898682
Epoch 100, training loss: 2.3065850734710693 = 1.6331478357315063 + 0.1 * 6.734373092651367
Epoch 100, val loss: 1.6407424211502075
Epoch 110, training loss: 2.226353645324707 = 1.5562297105789185 + 0.1 * 6.701239109039307
Epoch 110, val loss: 1.5776138305664062
Epoch 120, training loss: 2.127443313598633 = 1.4594457149505615 + 0.1 * 6.679974555969238
Epoch 120, val loss: 1.498991847038269
Epoch 130, training loss: 2.015117883682251 = 1.3485584259033203 + 0.1 * 6.66559362411499
Epoch 130, val loss: 1.4098045825958252
Epoch 140, training loss: 1.897423267364502 = 1.2322865724563599 + 0.1 * 6.651367664337158
Epoch 140, val loss: 1.318067193031311
Epoch 150, training loss: 1.783219337463379 = 1.1194891929626465 + 0.1 * 6.637300968170166
Epoch 150, val loss: 1.2302063703536987
Epoch 160, training loss: 1.6780071258544922 = 1.0156553983688354 + 0.1 * 6.62351655960083
Epoch 160, val loss: 1.1503866910934448
Epoch 170, training loss: 1.5825352668762207 = 0.9216204881668091 + 0.1 * 6.609148025512695
Epoch 170, val loss: 1.079054355621338
Epoch 180, training loss: 1.4981621503829956 = 0.838442325592041 + 0.1 * 6.597198009490967
Epoch 180, val loss: 1.0172454118728638
Epoch 190, training loss: 1.424004316329956 = 0.7656235694885254 + 0.1 * 6.583807468414307
Epoch 190, val loss: 0.9648267030715942
Epoch 200, training loss: 1.3599324226379395 = 0.7013670802116394 + 0.1 * 6.585653305053711
Epoch 200, val loss: 0.9197929501533508
Epoch 210, training loss: 1.3014333248138428 = 0.6452871561050415 + 0.1 * 6.561461448669434
Epoch 210, val loss: 0.8824027180671692
Epoch 220, training loss: 1.2501945495605469 = 0.5952077507972717 + 0.1 * 6.549867153167725
Epoch 220, val loss: 0.8507997989654541
Epoch 230, training loss: 1.2034975290298462 = 0.5496222972869873 + 0.1 * 6.53875207901001
Epoch 230, val loss: 0.8236377239227295
Epoch 240, training loss: 1.1608185768127441 = 0.5076146125793457 + 0.1 * 6.532040119171143
Epoch 240, val loss: 0.8002222776412964
Epoch 250, training loss: 1.1214725971221924 = 0.4679456055164337 + 0.1 * 6.535269737243652
Epoch 250, val loss: 0.7794966697692871
Epoch 260, training loss: 1.0821017026901245 = 0.4305703341960907 + 0.1 * 6.515314102172852
Epoch 260, val loss: 0.7617025375366211
Epoch 270, training loss: 1.0449163913726807 = 0.39490988850593567 + 0.1 * 6.500064849853516
Epoch 270, val loss: 0.7465598583221436
Epoch 280, training loss: 1.0104303359985352 = 0.36096006631851196 + 0.1 * 6.49470329284668
Epoch 280, val loss: 0.7343546748161316
Epoch 290, training loss: 0.9774297475814819 = 0.32922619581222534 + 0.1 * 6.482035160064697
Epoch 290, val loss: 0.7252073884010315
Epoch 300, training loss: 0.9465662240982056 = 0.2995966970920563 + 0.1 * 6.4696946144104
Epoch 300, val loss: 0.7187818288803101
Epoch 310, training loss: 0.9198253154754639 = 0.2724320590496063 + 0.1 * 6.473932266235352
Epoch 310, val loss: 0.7149524688720703
Epoch 320, training loss: 0.8932164311408997 = 0.24783873558044434 + 0.1 * 6.453776836395264
Epoch 320, val loss: 0.7134385108947754
Epoch 330, training loss: 0.8721076846122742 = 0.22565104067325592 + 0.1 * 6.464566707611084
Epoch 330, val loss: 0.7140681147575378
Epoch 340, training loss: 0.8493468761444092 = 0.2058747112751007 + 0.1 * 6.434720993041992
Epoch 340, val loss: 0.7167366743087769
Epoch 350, training loss: 0.8305210471153259 = 0.18820492923259735 + 0.1 * 6.423161029815674
Epoch 350, val loss: 0.7213214635848999
Epoch 360, training loss: 0.8141083717346191 = 0.17239196598529816 + 0.1 * 6.417163848876953
Epoch 360, val loss: 0.7276179194450378
Epoch 370, training loss: 0.7993319630622864 = 0.15833012759685516 + 0.1 * 6.410017967224121
Epoch 370, val loss: 0.7351678609848022
Epoch 380, training loss: 0.7868428826332092 = 0.14581173658370972 + 0.1 * 6.410311222076416
Epoch 380, val loss: 0.7438003420829773
Epoch 390, training loss: 0.7735681533813477 = 0.13457749783992767 + 0.1 * 6.389906406402588
Epoch 390, val loss: 0.7532604932785034
Epoch 400, training loss: 0.7624704837799072 = 0.12441068142652512 + 0.1 * 6.3805975914001465
Epoch 400, val loss: 0.7633142471313477
Epoch 410, training loss: 0.7543621063232422 = 0.11514662206172943 + 0.1 * 6.392154693603516
Epoch 410, val loss: 0.7738425731658936
Epoch 420, training loss: 0.745768666267395 = 0.10671405494213104 + 0.1 * 6.390545845031738
Epoch 420, val loss: 0.7846072912216187
Epoch 430, training loss: 0.7363418936729431 = 0.09901662915945053 + 0.1 * 6.373252868652344
Epoch 430, val loss: 0.7953892946243286
Epoch 440, training loss: 0.7273392677307129 = 0.09191758930683136 + 0.1 * 6.354217052459717
Epoch 440, val loss: 0.8062722086906433
Epoch 450, training loss: 0.7212585210800171 = 0.08534213900566101 + 0.1 * 6.359163284301758
Epoch 450, val loss: 0.8170356750488281
Epoch 460, training loss: 0.7140461206436157 = 0.07925436645746231 + 0.1 * 6.347917079925537
Epoch 460, val loss: 0.8276976346969604
Epoch 470, training loss: 0.7071213722229004 = 0.07359131425619125 + 0.1 * 6.335300922393799
Epoch 470, val loss: 0.8382841944694519
Epoch 480, training loss: 0.703275203704834 = 0.06828168779611588 + 0.1 * 6.3499345779418945
Epoch 480, val loss: 0.8487551212310791
Epoch 490, training loss: 0.6966097354888916 = 0.0633305013179779 + 0.1 * 6.332791805267334
Epoch 490, val loss: 0.8590266704559326
Epoch 500, training loss: 0.6911351680755615 = 0.058697473257780075 + 0.1 * 6.324377059936523
Epoch 500, val loss: 0.8691614270210266
Epoch 510, training loss: 0.6865460276603699 = 0.05436774715781212 + 0.1 * 6.321783065795898
Epoch 510, val loss: 0.8792287111282349
Epoch 520, training loss: 0.6835927367210388 = 0.050387073308229446 + 0.1 * 6.332056522369385
Epoch 520, val loss: 0.8891615271568298
Epoch 530, training loss: 0.6772818565368652 = 0.04674432426691055 + 0.1 * 6.305375099182129
Epoch 530, val loss: 0.8990246057510376
Epoch 540, training loss: 0.6753481030464172 = 0.04341639578342438 + 0.1 * 6.319316864013672
Epoch 540, val loss: 0.9088321328163147
Epoch 550, training loss: 0.6713786125183105 = 0.04040690138936043 + 0.1 * 6.309717178344727
Epoch 550, val loss: 0.9185277223587036
Epoch 560, training loss: 0.6673611402511597 = 0.03768139332532883 + 0.1 * 6.296797275543213
Epoch 560, val loss: 0.9282272458076477
Epoch 570, training loss: 0.6642328500747681 = 0.035204965621232986 + 0.1 * 6.290278911590576
Epoch 570, val loss: 0.9378372430801392
Epoch 580, training loss: 0.6614512801170349 = 0.03295348957180977 + 0.1 * 6.284977912902832
Epoch 580, val loss: 0.9472079873085022
Epoch 590, training loss: 0.6595442295074463 = 0.03090522065758705 + 0.1 * 6.2863898277282715
Epoch 590, val loss: 0.9563865661621094
Epoch 600, training loss: 0.6570847034454346 = 0.029042884707450867 + 0.1 * 6.2804179191589355
Epoch 600, val loss: 0.9653419852256775
Epoch 610, training loss: 0.6554127335548401 = 0.027344511821866035 + 0.1 * 6.28068208694458
Epoch 610, val loss: 0.9741895794868469
Epoch 620, training loss: 0.6534224152565002 = 0.02578943409025669 + 0.1 * 6.276329517364502
Epoch 620, val loss: 0.9828839302062988
Epoch 630, training loss: 0.6517221927642822 = 0.02436394989490509 + 0.1 * 6.273582458496094
Epoch 630, val loss: 0.9912674427032471
Epoch 640, training loss: 0.6490221619606018 = 0.02305472269654274 + 0.1 * 6.259674072265625
Epoch 640, val loss: 0.9995411038398743
Epoch 650, training loss: 0.6490854620933533 = 0.021845323964953423 + 0.1 * 6.272400856018066
Epoch 650, val loss: 1.0076359510421753
Epoch 660, training loss: 0.6465834379196167 = 0.020728083327412605 + 0.1 * 6.2585530281066895
Epoch 660, val loss: 1.0155977010726929
Epoch 670, training loss: 0.6457133889198303 = 0.019692933186888695 + 0.1 * 6.260204315185547
Epoch 670, val loss: 1.023466944694519
Epoch 680, training loss: 0.6436250805854797 = 0.018731970340013504 + 0.1 * 6.248930931091309
Epoch 680, val loss: 1.0310815572738647
Epoch 690, training loss: 0.6424271464347839 = 0.017839420586824417 + 0.1 * 6.245877265930176
Epoch 690, val loss: 1.0385491847991943
Epoch 700, training loss: 0.6415786743164062 = 0.017010414972901344 + 0.1 * 6.245682716369629
Epoch 700, val loss: 1.0458455085754395
Epoch 710, training loss: 0.6410129070281982 = 0.0162483062595129 + 0.1 * 6.247646331787109
Epoch 710, val loss: 1.0527697801589966
Epoch 720, training loss: 0.6394502520561218 = 0.015533898025751114 + 0.1 * 6.239163398742676
Epoch 720, val loss: 1.0597176551818848
Epoch 730, training loss: 0.6383097171783447 = 0.014867760241031647 + 0.1 * 6.234419345855713
Epoch 730, val loss: 1.066480040550232
Epoch 740, training loss: 0.6373023986816406 = 0.014247400686144829 + 0.1 * 6.2305498123168945
Epoch 740, val loss: 1.0730129480361938
Epoch 750, training loss: 0.6364145278930664 = 0.013668241910636425 + 0.1 * 6.2274627685546875
Epoch 750, val loss: 1.0794498920440674
Epoch 760, training loss: 0.6357949376106262 = 0.013122551143169403 + 0.1 * 6.226723670959473
Epoch 760, val loss: 1.0857672691345215
Epoch 770, training loss: 0.637809693813324 = 0.012611143290996552 + 0.1 * 6.2519850730896
Epoch 770, val loss: 1.0919400453567505
Epoch 780, training loss: 0.6347019672393799 = 0.012130765244364738 + 0.1 * 6.225712299346924
Epoch 780, val loss: 1.0979102849960327
Epoch 790, training loss: 0.635504961013794 = 0.011679193004965782 + 0.1 * 6.238257884979248
Epoch 790, val loss: 1.1038264036178589
Epoch 800, training loss: 0.6333536505699158 = 0.011254795826971531 + 0.1 * 6.220988750457764
Epoch 800, val loss: 1.1095123291015625
Epoch 810, training loss: 0.6329471468925476 = 0.010854500345885754 + 0.1 * 6.220926761627197
Epoch 810, val loss: 1.1151785850524902
Epoch 820, training loss: 0.6312640309333801 = 0.010476737283170223 + 0.1 * 6.2078728675842285
Epoch 820, val loss: 1.120661973953247
Epoch 830, training loss: 0.6320676207542419 = 0.01011939998716116 + 0.1 * 6.219482421875
Epoch 830, val loss: 1.1261045932769775
Epoch 840, training loss: 0.6314051151275635 = 0.009780858643352985 + 0.1 * 6.21624231338501
Epoch 840, val loss: 1.1313488483428955
Epoch 850, training loss: 0.6313227415084839 = 0.009461585432291031 + 0.1 * 6.218611717224121
Epoch 850, val loss: 1.136465311050415
Epoch 860, training loss: 0.6294347643852234 = 0.009158496744930744 + 0.1 * 6.202762603759766
Epoch 860, val loss: 1.1415458917617798
Epoch 870, training loss: 0.6294382810592651 = 0.008871981874108315 + 0.1 * 6.205663204193115
Epoch 870, val loss: 1.1465402841567993
Epoch 880, training loss: 0.6304317116737366 = 0.008600093424320221 + 0.1 * 6.218316078186035
Epoch 880, val loss: 1.1514226198196411
Epoch 890, training loss: 0.628476619720459 = 0.008341224864125252 + 0.1 * 6.201353549957275
Epoch 890, val loss: 1.1562119722366333
Epoch 900, training loss: 0.6282758712768555 = 0.008095325902104378 + 0.1 * 6.201805114746094
Epoch 900, val loss: 1.1609224081039429
Epoch 910, training loss: 0.6287148594856262 = 0.007861251942813396 + 0.1 * 6.208536148071289
Epoch 910, val loss: 1.1654902696609497
Epoch 920, training loss: 0.6269764304161072 = 0.007637861650437117 + 0.1 * 6.193385601043701
Epoch 920, val loss: 1.1700598001480103
Epoch 930, training loss: 0.6287914514541626 = 0.007424484472721815 + 0.1 * 6.213669776916504
Epoch 930, val loss: 1.1744794845581055
Epoch 940, training loss: 0.6258519291877747 = 0.00722180400043726 + 0.1 * 6.186301231384277
Epoch 940, val loss: 1.17878258228302
Epoch 950, training loss: 0.6254808902740479 = 0.007027929648756981 + 0.1 * 6.184529781341553
Epoch 950, val loss: 1.1830768585205078
Epoch 960, training loss: 0.6260083913803101 = 0.006842543371021748 + 0.1 * 6.1916584968566895
Epoch 960, val loss: 1.1872761249542236
Epoch 970, training loss: 0.6252304911613464 = 0.006664953660219908 + 0.1 * 6.18565559387207
Epoch 970, val loss: 1.1913387775421143
Epoch 980, training loss: 0.625009298324585 = 0.0064951712265610695 + 0.1 * 6.185141086578369
Epoch 980, val loss: 1.1953587532043457
Epoch 990, training loss: 0.6248628497123718 = 0.0063326419331133366 + 0.1 * 6.185302257537842
Epoch 990, val loss: 1.1993498802185059
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.797740936279297 = 1.9603562355041504 + 0.1 * 8.373846054077148
Epoch 0, val loss: 1.962346076965332
Epoch 10, training loss: 2.7871615886688232 = 1.949790596961975 + 0.1 * 8.373710632324219
Epoch 10, val loss: 1.9517923593521118
Epoch 20, training loss: 2.7744336128234863 = 1.9371309280395508 + 0.1 * 8.373025894165039
Epoch 20, val loss: 1.9391287565231323
Epoch 30, training loss: 2.7564632892608643 = 1.9196258783340454 + 0.1 * 8.368374824523926
Epoch 30, val loss: 1.9218671321868896
Epoch 40, training loss: 2.727020263671875 = 1.8937674760818481 + 0.1 * 8.332527160644531
Epoch 40, val loss: 1.8969154357910156
Epoch 50, training loss: 2.6586759090423584 = 1.8580665588378906 + 0.1 * 8.006093978881836
Epoch 50, val loss: 1.8638108968734741
Epoch 60, training loss: 2.5677382946014404 = 1.8203765153884888 + 0.1 * 7.473618030548096
Epoch 60, val loss: 1.8302679061889648
Epoch 70, training loss: 2.489367961883545 = 1.7849093675613403 + 0.1 * 7.044587135314941
Epoch 70, val loss: 1.7998290061950684
Epoch 80, training loss: 2.4328560829162598 = 1.750317096710205 + 0.1 * 6.825389385223389
Epoch 80, val loss: 1.7702363729476929
Epoch 90, training loss: 2.383823871612549 = 1.7092502117156982 + 0.1 * 6.745735168457031
Epoch 90, val loss: 1.7329965829849243
Epoch 100, training loss: 2.324843406677246 = 1.65389883518219 + 0.1 * 6.709446430206299
Epoch 100, val loss: 1.6838459968566895
Epoch 110, training loss: 2.2492990493774414 = 1.5807064771652222 + 0.1 * 6.685926914215088
Epoch 110, val loss: 1.6232593059539795
Epoch 120, training loss: 2.155109405517578 = 1.488184928894043 + 0.1 * 6.669245719909668
Epoch 120, val loss: 1.5486009120941162
Epoch 130, training loss: 2.0474448204040527 = 1.3820093870162964 + 0.1 * 6.654355049133301
Epoch 130, val loss: 1.4623771905899048
Epoch 140, training loss: 1.9345452785491943 = 1.2710238695144653 + 0.1 * 6.635213375091553
Epoch 140, val loss: 1.3732028007507324
Epoch 150, training loss: 1.8226146697998047 = 1.1605595350265503 + 0.1 * 6.620550632476807
Epoch 150, val loss: 1.2844878435134888
Epoch 160, training loss: 1.7142035961151123 = 1.0545017719268799 + 0.1 * 6.597017288208008
Epoch 160, val loss: 1.199778437614441
Epoch 170, training loss: 1.6128427982330322 = 0.9540771842002869 + 0.1 * 6.587656497955322
Epoch 170, val loss: 1.1195765733718872
Epoch 180, training loss: 1.5205060243606567 = 0.8642083406448364 + 0.1 * 6.562976837158203
Epoch 180, val loss: 1.0491687059402466
Epoch 190, training loss: 1.4382526874542236 = 0.7842391729354858 + 0.1 * 6.540134429931641
Epoch 190, val loss: 0.9872532486915588
Epoch 200, training loss: 1.3683826923370361 = 0.7141455411911011 + 0.1 * 6.542370796203613
Epoch 200, val loss: 0.9341158866882324
Epoch 210, training loss: 1.305355429649353 = 0.6540766358375549 + 0.1 * 6.512787818908691
Epoch 210, val loss: 0.8899543881416321
Epoch 220, training loss: 1.2501946687698364 = 0.6012155413627625 + 0.1 * 6.489791393280029
Epoch 220, val loss: 0.8525881767272949
Epoch 230, training loss: 1.2009992599487305 = 0.5532816648483276 + 0.1 * 6.477176189422607
Epoch 230, val loss: 0.8201825618743896
Epoch 240, training loss: 1.1551008224487305 = 0.5090387463569641 + 0.1 * 6.460621356964111
Epoch 240, val loss: 0.7920635938644409
Epoch 250, training loss: 1.1121807098388672 = 0.46760082244873047 + 0.1 * 6.445799350738525
Epoch 250, val loss: 0.7672430276870728
Epoch 260, training loss: 1.0724620819091797 = 0.42886924743652344 + 0.1 * 6.4359283447265625
Epoch 260, val loss: 0.7459918856620789
Epoch 270, training loss: 1.0361409187316895 = 0.3924790024757385 + 0.1 * 6.436619281768799
Epoch 270, val loss: 0.7277414202690125
Epoch 280, training loss: 0.9997078776359558 = 0.3584620952606201 + 0.1 * 6.4124579429626465
Epoch 280, val loss: 0.712653636932373
Epoch 290, training loss: 0.9674088954925537 = 0.32659757137298584 + 0.1 * 6.4081130027771
Epoch 290, val loss: 0.7002764940261841
Epoch 300, training loss: 0.9363314509391785 = 0.29689979553222656 + 0.1 * 6.394316673278809
Epoch 300, val loss: 0.690468966960907
Epoch 310, training loss: 0.9081481695175171 = 0.26917335391044617 + 0.1 * 6.389748573303223
Epoch 310, val loss: 0.6827399730682373
Epoch 320, training loss: 0.8823007345199585 = 0.24353955686092377 + 0.1 * 6.387611389160156
Epoch 320, val loss: 0.6770344376564026
Epoch 330, training loss: 0.8577370047569275 = 0.21999548375606537 + 0.1 * 6.377415180206299
Epoch 330, val loss: 0.6730430722236633
Epoch 340, training loss: 0.8352774381637573 = 0.19845344126224518 + 0.1 * 6.368239879608154
Epoch 340, val loss: 0.6706830263137817
Epoch 350, training loss: 0.8151569366455078 = 0.1788608580827713 + 0.1 * 6.3629608154296875
Epoch 350, val loss: 0.6697542071342468
Epoch 360, training loss: 0.7971936464309692 = 0.1612090915441513 + 0.1 * 6.359845161437988
Epoch 360, val loss: 0.6701063513755798
Epoch 370, training loss: 0.7800408005714417 = 0.14537115395069122 + 0.1 * 6.346696376800537
Epoch 370, val loss: 0.6716139316558838
Epoch 380, training loss: 0.7657886147499084 = 0.1312481313943863 + 0.1 * 6.345405101776123
Epoch 380, val loss: 0.6741561889648438
Epoch 390, training loss: 0.7517876625061035 = 0.11872250586748123 + 0.1 * 6.330651760101318
Epoch 390, val loss: 0.6776226162910461
Epoch 400, training loss: 0.7413243055343628 = 0.10761090368032455 + 0.1 * 6.33713436126709
Epoch 400, val loss: 0.68196702003479
Epoch 410, training loss: 0.7296120524406433 = 0.09778691083192825 + 0.1 * 6.318251132965088
Epoch 410, val loss: 0.6869461536407471
Epoch 420, training loss: 0.7208305597305298 = 0.08908861130475998 + 0.1 * 6.317419052124023
Epoch 420, val loss: 0.6924930810928345
Epoch 430, training loss: 0.712598979473114 = 0.08139541000127792 + 0.1 * 6.31203556060791
Epoch 430, val loss: 0.6984304189682007
Epoch 440, training loss: 0.7061289548873901 = 0.07455866038799286 + 0.1 * 6.31570291519165
Epoch 440, val loss: 0.7046656012535095
Epoch 450, training loss: 0.7004640102386475 = 0.06849196553230286 + 0.1 * 6.319720268249512
Epoch 450, val loss: 0.7110864520072937
Epoch 460, training loss: 0.6928651928901672 = 0.06310569494962692 + 0.1 * 6.297595024108887
Epoch 460, val loss: 0.7176600098609924
Epoch 470, training loss: 0.6870895028114319 = 0.058285340666770935 + 0.1 * 6.288041114807129
Epoch 470, val loss: 0.7243669629096985
Epoch 480, training loss: 0.6834633946418762 = 0.053957656025886536 + 0.1 * 6.29505729675293
Epoch 480, val loss: 0.7311058640480042
Epoch 490, training loss: 0.6784992218017578 = 0.05007012188434601 + 0.1 * 6.2842912673950195
Epoch 490, val loss: 0.7378344535827637
Epoch 500, training loss: 0.674223005771637 = 0.04655827581882477 + 0.1 * 6.276647090911865
Epoch 500, val loss: 0.7446094751358032
Epoch 510, training loss: 0.6709668636322021 = 0.043376706540584564 + 0.1 * 6.275901794433594
Epoch 510, val loss: 0.7513008713722229
Epoch 520, training loss: 0.6682938933372498 = 0.04049206152558327 + 0.1 * 6.278018474578857
Epoch 520, val loss: 0.7579730153083801
Epoch 530, training loss: 0.664322018623352 = 0.037871748208999634 + 0.1 * 6.264502048492432
Epoch 530, val loss: 0.7645376324653625
Epoch 540, training loss: 0.6618230938911438 = 0.03547312691807747 + 0.1 * 6.263499736785889
Epoch 540, val loss: 0.7710599303245544
Epoch 550, training loss: 0.6587362885475159 = 0.03328453376889229 + 0.1 * 6.254517078399658
Epoch 550, val loss: 0.7775647044181824
Epoch 560, training loss: 0.65912926197052 = 0.03128102049231529 + 0.1 * 6.278482437133789
Epoch 560, val loss: 0.7839238047599792
Epoch 570, training loss: 0.65391606092453 = 0.0294509194791317 + 0.1 * 6.2446513175964355
Epoch 570, val loss: 0.7901678681373596
Epoch 580, training loss: 0.6529119610786438 = 0.027767451480031013 + 0.1 * 6.2514448165893555
Epoch 580, val loss: 0.7964031100273132
Epoch 590, training loss: 0.6512426137924194 = 0.026221709325909615 + 0.1 * 6.250209331512451
Epoch 590, val loss: 0.8025037050247192
Epoch 600, training loss: 0.649216890335083 = 0.024802420288324356 + 0.1 * 6.244144439697266
Epoch 600, val loss: 0.8084744811058044
Epoch 610, training loss: 0.6475034952163696 = 0.02349466271698475 + 0.1 * 6.240087985992432
Epoch 610, val loss: 0.8144466280937195
Epoch 620, training loss: 0.6467342376708984 = 0.02228291518986225 + 0.1 * 6.244513511657715
Epoch 620, val loss: 0.8203127980232239
Epoch 630, training loss: 0.6440744400024414 = 0.021163806319236755 + 0.1 * 6.2291059494018555
Epoch 630, val loss: 0.8260475397109985
Epoch 640, training loss: 0.6432077288627625 = 0.02012295462191105 + 0.1 * 6.2308478355407715
Epoch 640, val loss: 0.8317480683326721
Epoch 650, training loss: 0.641399085521698 = 0.01915740966796875 + 0.1 * 6.222416400909424
Epoch 650, val loss: 0.8373464941978455
Epoch 660, training loss: 0.6416875123977661 = 0.018259385600686073 + 0.1 * 6.234281063079834
Epoch 660, val loss: 0.8428588509559631
Epoch 670, training loss: 0.6398050785064697 = 0.01742657460272312 + 0.1 * 6.223785400390625
Epoch 670, val loss: 0.8482533097267151
Epoch 680, training loss: 0.6383687853813171 = 0.01665116846561432 + 0.1 * 6.2171759605407715
Epoch 680, val loss: 0.8535911440849304
Epoch 690, training loss: 0.6380824446678162 = 0.01592520996928215 + 0.1 * 6.221571922302246
Epoch 690, val loss: 0.8588152527809143
Epoch 700, training loss: 0.6364635825157166 = 0.015249011106789112 + 0.1 * 6.212145805358887
Epoch 700, val loss: 0.863916277885437
Epoch 710, training loss: 0.6355571150779724 = 0.014616006053984165 + 0.1 * 6.209410667419434
Epoch 710, val loss: 0.8689918518066406
Epoch 720, training loss: 0.6351339221000671 = 0.014023298397660255 + 0.1 * 6.211105823516846
Epoch 720, val loss: 0.8739809989929199
Epoch 730, training loss: 0.6344990730285645 = 0.0134670939296484 + 0.1 * 6.210319995880127
Epoch 730, val loss: 0.8788490295410156
Epoch 740, training loss: 0.6328453421592712 = 0.012943566776812077 + 0.1 * 6.199017524719238
Epoch 740, val loss: 0.8836290836334229
Epoch 750, training loss: 0.6334791779518127 = 0.012451319955289364 + 0.1 * 6.210278511047363
Epoch 750, val loss: 0.8883522748947144
Epoch 760, training loss: 0.6328298449516296 = 0.011989840306341648 + 0.1 * 6.208400249481201
Epoch 760, val loss: 0.8929715156555176
Epoch 770, training loss: 0.631655752658844 = 0.011554091237485409 + 0.1 * 6.201016902923584
Epoch 770, val loss: 0.8975274562835693
Epoch 780, training loss: 0.6312382817268372 = 0.011143527925014496 + 0.1 * 6.2009477615356445
Epoch 780, val loss: 0.9020469784736633
Epoch 790, training loss: 0.6298508048057556 = 0.010755789466202259 + 0.1 * 6.1909499168396
Epoch 790, val loss: 0.9064459800720215
Epoch 800, training loss: 0.6294702887535095 = 0.01038866676390171 + 0.1 * 6.1908159255981445
Epoch 800, val loss: 0.9108484387397766
Epoch 810, training loss: 0.6298688650131226 = 0.010041694156825542 + 0.1 * 6.19827127456665
Epoch 810, val loss: 0.9151290059089661
Epoch 820, training loss: 0.628902792930603 = 0.009713012725114822 + 0.1 * 6.191897869110107
Epoch 820, val loss: 0.9193097352981567
Epoch 830, training loss: 0.6302427053451538 = 0.009402209892868996 + 0.1 * 6.208405017852783
Epoch 830, val loss: 0.9234272837638855
Epoch 840, training loss: 0.6281055212020874 = 0.009107548743486404 + 0.1 * 6.189979553222656
Epoch 840, val loss: 0.9274731278419495
Epoch 850, training loss: 0.626875102519989 = 0.008827234618365765 + 0.1 * 6.180479049682617
Epoch 850, val loss: 0.9315276741981506
Epoch 860, training loss: 0.6270084977149963 = 0.008560056798160076 + 0.1 * 6.184484004974365
Epoch 860, val loss: 0.9355279207229614
Epoch 870, training loss: 0.6262528896331787 = 0.008306303061544895 + 0.1 * 6.1794657707214355
Epoch 870, val loss: 0.9393434524536133
Epoch 880, training loss: 0.6257725358009338 = 0.008064690977334976 + 0.1 * 6.1770782470703125
Epoch 880, val loss: 0.9431523680686951
Epoch 890, training loss: 0.6255176663398743 = 0.007833670824766159 + 0.1 * 6.176839828491211
Epoch 890, val loss: 0.9469748139381409
Epoch 900, training loss: 0.6251876950263977 = 0.007613725960254669 + 0.1 * 6.175739765167236
Epoch 900, val loss: 0.950688362121582
Epoch 910, training loss: 0.6248190402984619 = 0.0074032507836818695 + 0.1 * 6.174157619476318
Epoch 910, val loss: 0.9543670415878296
Epoch 920, training loss: 0.6269509196281433 = 0.007202643435448408 + 0.1 * 6.197483062744141
Epoch 920, val loss: 0.957987904548645
Epoch 930, training loss: 0.6248116493225098 = 0.007010750472545624 + 0.1 * 6.178009033203125
Epoch 930, val loss: 0.9614422917366028
Epoch 940, training loss: 0.6236605048179626 = 0.006827944424003363 + 0.1 * 6.168325424194336
Epoch 940, val loss: 0.9649015069007874
Epoch 950, training loss: 0.6247463226318359 = 0.006652413867413998 + 0.1 * 6.180938720703125
Epoch 950, val loss: 0.9683250188827515
Epoch 960, training loss: 0.6229145526885986 = 0.006485596764832735 + 0.1 * 6.164289474487305
Epoch 960, val loss: 0.9716401100158691
Epoch 970, training loss: 0.6231154799461365 = 0.0063246628269553185 + 0.1 * 6.167908191680908
Epoch 970, val loss: 0.9749547839164734
Epoch 980, training loss: 0.6229276657104492 = 0.006170846521854401 + 0.1 * 6.167567729949951
Epoch 980, val loss: 0.9781800508499146
Epoch 990, training loss: 0.6215918064117432 = 0.006023047026246786 + 0.1 * 6.15568733215332
Epoch 990, val loss: 0.9813427925109863
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8598
Flip ASR: 0.8311/225 nodes
The final ASR:0.78352, 0.21186, Accuracy:0.81852, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10514])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00460, Accuracy:0.83086, 0.00761
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7697129249572754 = 1.9323201179504395 + 0.1 * 8.37392807006836
Epoch 0, val loss: 1.931722640991211
Epoch 10, training loss: 2.7609524726867676 = 1.92356538772583 + 0.1 * 8.373870849609375
Epoch 10, val loss: 1.9236775636672974
Epoch 20, training loss: 2.7504076957702637 = 1.9130470752716064 + 0.1 * 8.37360668182373
Epoch 20, val loss: 1.9135398864746094
Epoch 30, training loss: 2.7357730865478516 = 1.898573637008667 + 0.1 * 8.371993064880371
Epoch 30, val loss: 1.899194598197937
Epoch 40, training loss: 2.713256597518921 = 1.877395749092102 + 0.1 * 8.35860824584961
Epoch 40, val loss: 1.878051996231079
Epoch 50, training loss: 2.6739001274108887 = 1.846897006034851 + 0.1 * 8.270031929016113
Epoch 50, val loss: 1.8484379053115845
Epoch 60, training loss: 2.591771125793457 = 1.8094819784164429 + 0.1 * 7.822892665863037
Epoch 60, val loss: 1.814712405204773
Epoch 70, training loss: 2.512319564819336 = 1.7711915969848633 + 0.1 * 7.411279201507568
Epoch 70, val loss: 1.7819520235061646
Epoch 80, training loss: 2.4356942176818848 = 1.7300031185150146 + 0.1 * 7.056910037994385
Epoch 80, val loss: 1.746376395225525
Epoch 90, training loss: 2.3698785305023193 = 1.6778534650802612 + 0.1 * 6.920251369476318
Epoch 90, val loss: 1.7010002136230469
Epoch 100, training loss: 2.293430805206299 = 1.607578992843628 + 0.1 * 6.858519077301025
Epoch 100, val loss: 1.6401792764663696
Epoch 110, training loss: 2.2000675201416016 = 1.5184803009033203 + 0.1 * 6.815872669219971
Epoch 110, val loss: 1.5653785467147827
Epoch 120, training loss: 2.094291925430298 = 1.4160200357437134 + 0.1 * 6.782718181610107
Epoch 120, val loss: 1.4806458950042725
Epoch 130, training loss: 1.984081506729126 = 1.3085497617721558 + 0.1 * 6.755317687988281
Epoch 130, val loss: 1.3932373523712158
Epoch 140, training loss: 1.8754218816757202 = 1.2025980949401855 + 0.1 * 6.728237628936768
Epoch 140, val loss: 1.3093639612197876
Epoch 150, training loss: 1.77314031124115 = 1.1028985977172852 + 0.1 * 6.702416896820068
Epoch 150, val loss: 1.2325255870819092
Epoch 160, training loss: 1.6810197830200195 = 1.0123704671859741 + 0.1 * 6.686493396759033
Epoch 160, val loss: 1.164960503578186
Epoch 170, training loss: 1.597869873046875 = 0.9308630228042603 + 0.1 * 6.67006778717041
Epoch 170, val loss: 1.1058837175369263
Epoch 180, training loss: 1.5216034650802612 = 0.8557887673377991 + 0.1 * 6.658146858215332
Epoch 180, val loss: 1.0524907112121582
Epoch 190, training loss: 1.4492759704589844 = 0.7844548225402832 + 0.1 * 6.648211479187012
Epoch 190, val loss: 1.0022984743118286
Epoch 200, training loss: 1.3795537948608398 = 0.7153283953666687 + 0.1 * 6.642253398895264
Epoch 200, val loss: 0.9540274143218994
Epoch 210, training loss: 1.3116344213485718 = 0.6480510830879211 + 0.1 * 6.635833263397217
Epoch 210, val loss: 0.9077417254447937
Epoch 220, training loss: 1.2463488578796387 = 0.5831213593482971 + 0.1 * 6.632274627685547
Epoch 220, val loss: 0.8641564846038818
Epoch 230, training loss: 1.1846487522125244 = 0.521662175655365 + 0.1 * 6.629866123199463
Epoch 230, val loss: 0.824752926826477
Epoch 240, training loss: 1.1276679039001465 = 0.4648909866809845 + 0.1 * 6.627769470214844
Epoch 240, val loss: 0.790756106376648
Epoch 250, training loss: 1.0755634307861328 = 0.4131312370300293 + 0.1 * 6.624320983886719
Epoch 250, val loss: 0.7623677849769592
Epoch 260, training loss: 1.0291892290115356 = 0.36665213108062744 + 0.1 * 6.625370979309082
Epoch 260, val loss: 0.7395294904708862
Epoch 270, training loss: 0.987569272518158 = 0.32549846172332764 + 0.1 * 6.620707988739014
Epoch 270, val loss: 0.7219148874282837
Epoch 280, training loss: 0.950884222984314 = 0.28913068771362305 + 0.1 * 6.61753511428833
Epoch 280, val loss: 0.7086865901947021
Epoch 290, training loss: 0.9185051918029785 = 0.2569276988506317 + 0.1 * 6.615774631500244
Epoch 290, val loss: 0.6990090012550354
Epoch 300, training loss: 0.8895680904388428 = 0.22825564444065094 + 0.1 * 6.613124370574951
Epoch 300, val loss: 0.6924043893814087
Epoch 310, training loss: 0.8635079860687256 = 0.20251666009426117 + 0.1 * 6.609912872314453
Epoch 310, val loss: 0.6879874467849731
Epoch 320, training loss: 0.8400675058364868 = 0.17923703789710999 + 0.1 * 6.608304023742676
Epoch 320, val loss: 0.6856495141983032
Epoch 330, training loss: 0.8185692429542542 = 0.15812088549137115 + 0.1 * 6.604483604431152
Epoch 330, val loss: 0.6850264668464661
Epoch 340, training loss: 0.799125075340271 = 0.13902002573013306 + 0.1 * 6.60105037689209
Epoch 340, val loss: 0.6857767105102539
Epoch 350, training loss: 0.7824277281761169 = 0.1219203919172287 + 0.1 * 6.605072975158691
Epoch 350, val loss: 0.6880912780761719
Epoch 360, training loss: 0.7664852738380432 = 0.10694493353366852 + 0.1 * 6.595403671264648
Epoch 360, val loss: 0.6918001770973206
Epoch 370, training loss: 0.7529585957527161 = 0.09399260580539703 + 0.1 * 6.589659690856934
Epoch 370, val loss: 0.6965668201446533
Epoch 380, training loss: 0.7412470579147339 = 0.08289927989244461 + 0.1 * 6.58347749710083
Epoch 380, val loss: 0.7027567625045776
Epoch 390, training loss: 0.73195481300354 = 0.07346536964178085 + 0.1 * 6.58489465713501
Epoch 390, val loss: 0.7099828124046326
Epoch 400, training loss: 0.7231176495552063 = 0.06551199406385422 + 0.1 * 6.576056480407715
Epoch 400, val loss: 0.7180971503257751
Epoch 410, training loss: 0.7154549360275269 = 0.05876867473125458 + 0.1 * 6.5668625831604
Epoch 410, val loss: 0.726666271686554
Epoch 420, training loss: 0.7090015411376953 = 0.05301203951239586 + 0.1 * 6.559895038604736
Epoch 420, val loss: 0.7356858253479004
Epoch 430, training loss: 0.7041178941726685 = 0.04807601869106293 + 0.1 * 6.560418605804443
Epoch 430, val loss: 0.7448926568031311
Epoch 440, training loss: 0.6991461515426636 = 0.043848518282175064 + 0.1 * 6.552976131439209
Epoch 440, val loss: 0.754361093044281
Epoch 450, training loss: 0.6944226622581482 = 0.04017223045229912 + 0.1 * 6.54250431060791
Epoch 450, val loss: 0.7634741067886353
Epoch 460, training loss: 0.6904513239860535 = 0.03694572299718857 + 0.1 * 6.535056114196777
Epoch 460, val loss: 0.7729995250701904
Epoch 470, training loss: 0.6876633167266846 = 0.03409333527088165 + 0.1 * 6.535699844360352
Epoch 470, val loss: 0.7823826670646667
Epoch 480, training loss: 0.6852404475212097 = 0.03158530965447426 + 0.1 * 6.536550998687744
Epoch 480, val loss: 0.7912654876708984
Epoch 490, training loss: 0.6815829873085022 = 0.02936653606593609 + 0.1 * 6.522164344787598
Epoch 490, val loss: 0.800248920917511
Epoch 500, training loss: 0.678700864315033 = 0.027382027357816696 + 0.1 * 6.513188362121582
Epoch 500, val loss: 0.8088809847831726
Epoch 510, training loss: 0.6760736703872681 = 0.02559165470302105 + 0.1 * 6.504819869995117
Epoch 510, val loss: 0.817596971988678
Epoch 520, training loss: 0.6746253371238708 = 0.023972509428858757 + 0.1 * 6.506528377532959
Epoch 520, val loss: 0.8260611295700073
Epoch 530, training loss: 0.6722023487091064 = 0.022511716932058334 + 0.1 * 6.496906280517578
Epoch 530, val loss: 0.8345446586608887
Epoch 540, training loss: 0.6702237725257874 = 0.021184366196393967 + 0.1 * 6.49039363861084
Epoch 540, val loss: 0.8426337838172913
Epoch 550, training loss: 0.669581949710846 = 0.019978459924459457 + 0.1 * 6.496034622192383
Epoch 550, val loss: 0.8503772020339966
Epoch 560, training loss: 0.6667240262031555 = 0.018884975463151932 + 0.1 * 6.478390693664551
Epoch 560, val loss: 0.8585562705993652
Epoch 570, training loss: 0.6649158000946045 = 0.017882103100419044 + 0.1 * 6.4703369140625
Epoch 570, val loss: 0.8660746216773987
Epoch 580, training loss: 0.6665840744972229 = 0.016958165913820267 + 0.1 * 6.496258735656738
Epoch 580, val loss: 0.8734869956970215
Epoch 590, training loss: 0.6627147197723389 = 0.016111696138978004 + 0.1 * 6.466030597686768
Epoch 590, val loss: 0.8808689713478088
Epoch 600, training loss: 0.6614845991134644 = 0.015332700684666634 + 0.1 * 6.46151876449585
Epoch 600, val loss: 0.8880491852760315
Epoch 610, training loss: 0.6595547795295715 = 0.014613095670938492 + 0.1 * 6.449416637420654
Epoch 610, val loss: 0.8948599696159363
Epoch 620, training loss: 0.6605309844017029 = 0.013945762068033218 + 0.1 * 6.4658522605896
Epoch 620, val loss: 0.9018499255180359
Epoch 630, training loss: 0.6569140553474426 = 0.013326769694685936 + 0.1 * 6.435873031616211
Epoch 630, val loss: 0.908541202545166
Epoch 640, training loss: 0.658004105091095 = 0.01275227777659893 + 0.1 * 6.452517986297607
Epoch 640, val loss: 0.9152669906616211
Epoch 650, training loss: 0.6548024415969849 = 0.012216810137033463 + 0.1 * 6.425856113433838
Epoch 650, val loss: 0.9216460585594177
Epoch 660, training loss: 0.6546052098274231 = 0.01171860471367836 + 0.1 * 6.428865909576416
Epoch 660, val loss: 0.9281370639801025
Epoch 670, training loss: 0.65315842628479 = 0.011251680552959442 + 0.1 * 6.4190673828125
Epoch 670, val loss: 0.9341754913330078
Epoch 680, training loss: 0.6519505977630615 = 0.010815203189849854 + 0.1 * 6.411354064941406
Epoch 680, val loss: 0.9403151869773865
Epoch 690, training loss: 0.6517674922943115 = 0.010405795648694038 + 0.1 * 6.413617134094238
Epoch 690, val loss: 0.9461484551429749
Epoch 700, training loss: 0.6505947709083557 = 0.010021776892244816 + 0.1 * 6.4057297706604
Epoch 700, val loss: 0.9519887566566467
Epoch 710, training loss: 0.6501482129096985 = 0.009661993943154812 + 0.1 * 6.404862403869629
Epoch 710, val loss: 0.9577441215515137
Epoch 720, training loss: 0.6497660279273987 = 0.009321946650743484 + 0.1 * 6.404440879821777
Epoch 720, val loss: 0.9630547165870667
Epoch 730, training loss: 0.6479164958000183 = 0.009003117680549622 + 0.1 * 6.389133930206299
Epoch 730, val loss: 0.96873939037323
Epoch 740, training loss: 0.6483068466186523 = 0.008703434839844704 + 0.1 * 6.396034240722656
Epoch 740, val loss: 0.9738767147064209
Epoch 750, training loss: 0.6465445756912231 = 0.008418897166848183 + 0.1 * 6.381256580352783
Epoch 750, val loss: 0.9790094494819641
Epoch 760, training loss: 0.6457434892654419 = 0.008149578236043453 + 0.1 * 6.37593936920166
Epoch 760, val loss: 0.9843607544898987
Epoch 770, training loss: 0.6450021266937256 = 0.007894924841821194 + 0.1 * 6.371071815490723
Epoch 770, val loss: 0.9893136620521545
Epoch 780, training loss: 0.6445268988609314 = 0.007654574234038591 + 0.1 * 6.368723392486572
Epoch 780, val loss: 0.9942151308059692
Epoch 790, training loss: 0.6450042128562927 = 0.0074272314086556435 + 0.1 * 6.37576961517334
Epoch 790, val loss: 0.9991714954376221
Epoch 800, training loss: 0.6436324715614319 = 0.007211644668132067 + 0.1 * 6.364208221435547
Epoch 800, val loss: 1.0036276578903198
Epoch 810, training loss: 0.642292857170105 = 0.00700529245659709 + 0.1 * 6.352875709533691
Epoch 810, val loss: 1.0084753036499023
Epoch 820, training loss: 0.6415292024612427 = 0.006809300277382135 + 0.1 * 6.347198963165283
Epoch 820, val loss: 1.0129488706588745
Epoch 830, training loss: 0.6413032412528992 = 0.0066229552030563354 + 0.1 * 6.346802711486816
Epoch 830, val loss: 1.017372965812683
Epoch 840, training loss: 0.6421273946762085 = 0.006445522420108318 + 0.1 * 6.356818675994873
Epoch 840, val loss: 1.0218687057495117
Epoch 850, training loss: 0.6414631009101868 = 0.006276158150285482 + 0.1 * 6.351869583129883
Epoch 850, val loss: 1.0260322093963623
Epoch 860, training loss: 0.6389229893684387 = 0.006115187890827656 + 0.1 * 6.328077793121338
Epoch 860, val loss: 1.0304453372955322
Epoch 870, training loss: 0.6409146785736084 = 0.005961253307759762 + 0.1 * 6.349534511566162
Epoch 870, val loss: 1.034638524055481
Epoch 880, training loss: 0.6382929682731628 = 0.005812936462461948 + 0.1 * 6.32480001449585
Epoch 880, val loss: 1.0384628772735596
Epoch 890, training loss: 0.6383771300315857 = 0.005672515369951725 + 0.1 * 6.327045917510986
Epoch 890, val loss: 1.0425375699996948
Epoch 900, training loss: 0.6373986005783081 = 0.005537607707083225 + 0.1 * 6.318609714508057
Epoch 900, val loss: 1.0465515851974487
Epoch 910, training loss: 0.6369327306747437 = 0.005408140365034342 + 0.1 * 6.315245628356934
Epoch 910, val loss: 1.0503747463226318
Epoch 920, training loss: 0.636549174785614 = 0.005283499602228403 + 0.1 * 6.312656879425049
Epoch 920, val loss: 1.0541914701461792
Epoch 930, training loss: 0.6364864706993103 = 0.005164272151887417 + 0.1 * 6.3132219314575195
Epoch 930, val loss: 1.0579509735107422
Epoch 940, training loss: 0.6349318623542786 = 0.005050521343946457 + 0.1 * 6.298813343048096
Epoch 940, val loss: 1.061649203300476
Epoch 950, training loss: 0.6349153518676758 = 0.004940399434417486 + 0.1 * 6.299749374389648
Epoch 950, val loss: 1.0652252435684204
Epoch 960, training loss: 0.635712206363678 = 0.00483466312289238 + 0.1 * 6.308775424957275
Epoch 960, val loss: 1.0686372518539429
Epoch 970, training loss: 0.6337798833847046 = 0.0047331638634204865 + 0.1 * 6.290467262268066
Epoch 970, val loss: 1.0722144842147827
Epoch 980, training loss: 0.6358902454376221 = 0.00463592866435647 + 0.1 * 6.312542915344238
Epoch 980, val loss: 1.0757626295089722
Epoch 990, training loss: 0.632904589176178 = 0.004541194532066584 + 0.1 * 6.283633708953857
Epoch 990, val loss: 1.0790164470672607
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.7601
Flip ASR: 0.7156/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7759628295898438 = 1.9385749101638794 + 0.1 * 8.373878479003906
Epoch 0, val loss: 1.9287534952163696
Epoch 10, training loss: 2.7657642364501953 = 1.9283909797668457 + 0.1 * 8.373733520507812
Epoch 10, val loss: 1.9191062450408936
Epoch 20, training loss: 2.753251075744629 = 1.915948748588562 + 0.1 * 8.373022079467773
Epoch 20, val loss: 1.9069329500198364
Epoch 30, training loss: 2.7357730865478516 = 1.8989622592926025 + 0.1 * 8.368106842041016
Epoch 30, val loss: 1.8899658918380737
Epoch 40, training loss: 2.708240270614624 = 1.8743541240692139 + 0.1 * 8.338861465454102
Epoch 40, val loss: 1.865523099899292
Epoch 50, training loss: 2.654672622680664 = 1.840436577796936 + 0.1 * 8.142361640930176
Epoch 50, val loss: 1.8331776857376099
Epoch 60, training loss: 2.5754053592681885 = 1.8005459308624268 + 0.1 * 7.748593807220459
Epoch 60, val loss: 1.79706871509552
Epoch 70, training loss: 2.495492696762085 = 1.7601596117019653 + 0.1 * 7.353331089019775
Epoch 70, val loss: 1.76106858253479
Epoch 80, training loss: 2.4236996173858643 = 1.7161396741867065 + 0.1 * 7.07559871673584
Epoch 80, val loss: 1.722148060798645
Epoch 90, training loss: 2.3553268909454346 = 1.6594924926757812 + 0.1 * 6.958343505859375
Epoch 90, val loss: 1.671144962310791
Epoch 100, training loss: 2.2714672088623047 = 1.5847983360290527 + 0.1 * 6.866687297821045
Epoch 100, val loss: 1.606303334236145
Epoch 110, training loss: 2.1729958057403564 = 1.492995023727417 + 0.1 * 6.800007343292236
Epoch 110, val loss: 1.5309568643569946
Epoch 120, training loss: 2.068373203277588 = 1.393009901046753 + 0.1 * 6.753633975982666
Epoch 120, val loss: 1.4507598876953125
Epoch 130, training loss: 1.9668781757354736 = 1.2950448989868164 + 0.1 * 6.718332767486572
Epoch 130, val loss: 1.3751384019851685
Epoch 140, training loss: 1.8728251457214355 = 1.203957200050354 + 0.1 * 6.6886796951293945
Epoch 140, val loss: 1.3075329065322876
Epoch 150, training loss: 1.786008358001709 = 1.1198869943618774 + 0.1 * 6.661214351654053
Epoch 150, val loss: 1.2456202507019043
Epoch 160, training loss: 1.7047133445739746 = 1.0409504175186157 + 0.1 * 6.63762903213501
Epoch 160, val loss: 1.1886801719665527
Epoch 170, training loss: 1.628385305404663 = 0.9666115045547485 + 0.1 * 6.617737293243408
Epoch 170, val loss: 1.136784553527832
Epoch 180, training loss: 1.5567069053649902 = 0.8958620429039001 + 0.1 * 6.608448505401611
Epoch 180, val loss: 1.0885618925094604
Epoch 190, training loss: 1.4879329204559326 = 0.8292587399482727 + 0.1 * 6.586741924285889
Epoch 190, val loss: 1.0442379713058472
Epoch 200, training loss: 1.4232597351074219 = 0.7662433385848999 + 0.1 * 6.570164203643799
Epoch 200, val loss: 1.0030910968780518
Epoch 210, training loss: 1.3632619380950928 = 0.70763099193573 + 0.1 * 6.556308746337891
Epoch 210, val loss: 0.9660208225250244
Epoch 220, training loss: 1.3092761039733887 = 0.6548546552658081 + 0.1 * 6.544214248657227
Epoch 220, val loss: 0.9341998100280762
Epoch 230, training loss: 1.2599201202392578 = 0.6069766879081726 + 0.1 * 6.5294342041015625
Epoch 230, val loss: 0.9065477252006531
Epoch 240, training loss: 1.2156500816345215 = 0.5632967352867126 + 0.1 * 6.523532867431641
Epoch 240, val loss: 0.882537305355072
Epoch 250, training loss: 1.1748409271240234 = 0.5235480070114136 + 0.1 * 6.512929439544678
Epoch 250, val loss: 0.8618907332420349
Epoch 260, training loss: 1.1361680030822754 = 0.4867955446243286 + 0.1 * 6.493724822998047
Epoch 260, val loss: 0.8438052535057068
Epoch 270, training loss: 1.1011139154434204 = 0.45251038670539856 + 0.1 * 6.486035346984863
Epoch 270, val loss: 0.8283435106277466
Epoch 280, training loss: 1.0684773921966553 = 0.42051962018013 + 0.1 * 6.479578018188477
Epoch 280, val loss: 0.8153207898139954
Epoch 290, training loss: 1.0369855165481567 = 0.3901650011539459 + 0.1 * 6.468204975128174
Epoch 290, val loss: 0.8044553399085999
Epoch 300, training loss: 1.0083860158920288 = 0.36110350489616394 + 0.1 * 6.472824573516846
Epoch 300, val loss: 0.7955384850502014
Epoch 310, training loss: 0.9790587425231934 = 0.33330121636390686 + 0.1 * 6.45757532119751
Epoch 310, val loss: 0.788372278213501
Epoch 320, training loss: 0.9516022205352783 = 0.30654212832450867 + 0.1 * 6.450601100921631
Epoch 320, val loss: 0.7826348543167114
Epoch 330, training loss: 0.924653172492981 = 0.2805773615837097 + 0.1 * 6.440757751464844
Epoch 330, val loss: 0.7779167890548706
Epoch 340, training loss: 0.8999468088150024 = 0.25547555088996887 + 0.1 * 6.444712162017822
Epoch 340, val loss: 0.7742774486541748
Epoch 350, training loss: 0.8744292259216309 = 0.23135390877723694 + 0.1 * 6.430752754211426
Epoch 350, val loss: 0.7717105150222778
Epoch 360, training loss: 0.8510926961898804 = 0.2083355337381363 + 0.1 * 6.427571773529053
Epoch 360, val loss: 0.7703376412391663
Epoch 370, training loss: 0.8286386132240295 = 0.18686844408512115 + 0.1 * 6.417701721191406
Epoch 370, val loss: 0.770528256893158
Epoch 380, training loss: 0.8083462715148926 = 0.1670464724302292 + 0.1 * 6.412997722625732
Epoch 380, val loss: 0.7722266912460327
Epoch 390, training loss: 0.7900843620300293 = 0.14887431263923645 + 0.1 * 6.412100315093994
Epoch 390, val loss: 0.7756017446517944
Epoch 400, training loss: 0.7735801339149475 = 0.13224466145038605 + 0.1 * 6.413354873657227
Epoch 400, val loss: 0.7805538177490234
Epoch 410, training loss: 0.756949782371521 = 0.1172068789601326 + 0.1 * 6.3974289894104
Epoch 410, val loss: 0.787265419960022
Epoch 420, training loss: 0.7423387765884399 = 0.10391823202371597 + 0.1 * 6.384204864501953
Epoch 420, val loss: 0.7956439256668091
Epoch 430, training loss: 0.7304165959358215 = 0.09266366809606552 + 0.1 * 6.377529621124268
Epoch 430, val loss: 0.8054139614105225
Epoch 440, training loss: 0.724972665309906 = 0.0831306129693985 + 0.1 * 6.418420314788818
Epoch 440, val loss: 0.8160820007324219
Epoch 450, training loss: 0.7129374146461487 = 0.0750340074300766 + 0.1 * 6.379034042358398
Epoch 450, val loss: 0.8265632390975952
Epoch 460, training loss: 0.7042846083641052 = 0.06801958382129669 + 0.1 * 6.362650394439697
Epoch 460, val loss: 0.8369662165641785
Epoch 470, training loss: 0.6981090903282166 = 0.06187652796506882 + 0.1 * 6.362325668334961
Epoch 470, val loss: 0.8476024270057678
Epoch 480, training loss: 0.6923073530197144 = 0.05645310878753662 + 0.1 * 6.358542442321777
Epoch 480, val loss: 0.858452320098877
Epoch 490, training loss: 0.6876500248908997 = 0.05163554474711418 + 0.1 * 6.36014461517334
Epoch 490, val loss: 0.8694679737091064
Epoch 500, training loss: 0.6819793581962585 = 0.04735712707042694 + 0.1 * 6.346221923828125
Epoch 500, val loss: 0.8803483843803406
Epoch 510, training loss: 0.6770320534706116 = 0.04354284703731537 + 0.1 * 6.3348917961120605
Epoch 510, val loss: 0.8911067247390747
Epoch 520, training loss: 0.6753728985786438 = 0.04014795646071434 + 0.1 * 6.3522491455078125
Epoch 520, val loss: 0.9017431139945984
Epoch 530, training loss: 0.6704341769218445 = 0.03714490309357643 + 0.1 * 6.332892894744873
Epoch 530, val loss: 0.9120311737060547
Epoch 540, training loss: 0.6671528816223145 = 0.03447387367486954 + 0.1 * 6.326789855957031
Epoch 540, val loss: 0.9221516847610474
Epoch 550, training loss: 0.6635400652885437 = 0.03208762779831886 + 0.1 * 6.314524173736572
Epoch 550, val loss: 0.9321078658103943
Epoch 560, training loss: 0.6642209887504578 = 0.02995258755981922 + 0.1 * 6.342683792114258
Epoch 560, val loss: 0.9418753981590271
Epoch 570, training loss: 0.6591833829879761 = 0.028043486177921295 + 0.1 * 6.311398506164551
Epoch 570, val loss: 0.9513745307922363
Epoch 580, training loss: 0.6571140885353088 = 0.02632082998752594 + 0.1 * 6.3079328536987305
Epoch 580, val loss: 0.9606550335884094
Epoch 590, training loss: 0.6556593179702759 = 0.024765271693468094 + 0.1 * 6.308940410614014
Epoch 590, val loss: 0.9696773886680603
Epoch 600, training loss: 0.6537946462631226 = 0.02335572987794876 + 0.1 * 6.304388999938965
Epoch 600, val loss: 0.9784700870513916
Epoch 610, training loss: 0.6521403789520264 = 0.02206890657544136 + 0.1 * 6.300714492797852
Epoch 610, val loss: 0.9870967268943787
Epoch 620, training loss: 0.6507279276847839 = 0.020892951637506485 + 0.1 * 6.298349380493164
Epoch 620, val loss: 0.995546817779541
Epoch 630, training loss: 0.6487540006637573 = 0.019813936203718185 + 0.1 * 6.289400100708008
Epoch 630, val loss: 1.0037751197814941
Epoch 640, training loss: 0.6478941440582275 = 0.018819646909832954 + 0.1 * 6.290745258331299
Epoch 640, val loss: 1.0118168592453003
Epoch 650, training loss: 0.6468937993049622 = 0.017902914434671402 + 0.1 * 6.2899088859558105
Epoch 650, val loss: 1.0196949243545532
Epoch 660, training loss: 0.6455312371253967 = 0.01705942302942276 + 0.1 * 6.284718036651611
Epoch 660, val loss: 1.027327299118042
Epoch 670, training loss: 0.6439170837402344 = 0.016280338168144226 + 0.1 * 6.2763671875
Epoch 670, val loss: 1.034765601158142
Epoch 680, training loss: 0.6428551077842712 = 0.015556339174509048 + 0.1 * 6.272987365722656
Epoch 680, val loss: 1.0420899391174316
Epoch 690, training loss: 0.6445773243904114 = 0.014881198294460773 + 0.1 * 6.296961307525635
Epoch 690, val loss: 1.0492370128631592
Epoch 700, training loss: 0.642353892326355 = 0.014254377223551273 + 0.1 * 6.280994892120361
Epoch 700, val loss: 1.056298017501831
Epoch 710, training loss: 0.6408674120903015 = 0.013669603504240513 + 0.1 * 6.27197790145874
Epoch 710, val loss: 1.0630927085876465
Epoch 720, training loss: 0.6407935619354248 = 0.013123145326972008 + 0.1 * 6.276703834533691
Epoch 720, val loss: 1.069860816001892
Epoch 730, training loss: 0.6397136449813843 = 0.012612273916602135 + 0.1 * 6.2710137367248535
Epoch 730, val loss: 1.0763804912567139
Epoch 740, training loss: 0.6394167542457581 = 0.012132696807384491 + 0.1 * 6.27284049987793
Epoch 740, val loss: 1.0828229188919067
Epoch 750, training loss: 0.6378995180130005 = 0.011682789772748947 + 0.1 * 6.262166976928711
Epoch 750, val loss: 1.0890827178955078
Epoch 760, training loss: 0.6374770998954773 = 0.011259458027780056 + 0.1 * 6.262176513671875
Epoch 760, val loss: 1.0952856540679932
Epoch 770, training loss: 0.6370660066604614 = 0.010860810056328773 + 0.1 * 6.262052059173584
Epoch 770, val loss: 1.1013875007629395
Epoch 780, training loss: 0.6364848017692566 = 0.010485133156180382 + 0.1 * 6.2599968910217285
Epoch 780, val loss: 1.1073131561279297
Epoch 790, training loss: 0.6350999474525452 = 0.010131935589015484 + 0.1 * 6.249680042266846
Epoch 790, val loss: 1.1131054162979126
Epoch 800, training loss: 0.6350625157356262 = 0.009797614067792892 + 0.1 * 6.252648830413818
Epoch 800, val loss: 1.118744969367981
Epoch 810, training loss: 0.6340450048446655 = 0.009481011889874935 + 0.1 * 6.245639801025391
Epoch 810, val loss: 1.1243705749511719
Epoch 820, training loss: 0.6331446170806885 = 0.00918092206120491 + 0.1 * 6.2396368980407715
Epoch 820, val loss: 1.1298141479492188
Epoch 830, training loss: 0.6330862641334534 = 0.008896146900951862 + 0.1 * 6.24190092086792
Epoch 830, val loss: 1.1351720094680786
Epoch 840, training loss: 0.6325175166130066 = 0.008626478724181652 + 0.1 * 6.23891019821167
Epoch 840, val loss: 1.1404691934585571
Epoch 850, training loss: 0.6325216889381409 = 0.008370017632842064 + 0.1 * 6.24151611328125
Epoch 850, val loss: 1.1456605195999146
Epoch 860, training loss: 0.6335490345954895 = 0.008125893771648407 + 0.1 * 6.2542314529418945
Epoch 860, val loss: 1.1507682800292969
Epoch 870, training loss: 0.6322056651115417 = 0.00789446197450161 + 0.1 * 6.243112087249756
Epoch 870, val loss: 1.1558438539505005
Epoch 880, training loss: 0.6303770542144775 = 0.00767425587400794 + 0.1 * 6.227027893066406
Epoch 880, val loss: 1.1607190370559692
Epoch 890, training loss: 0.630116879940033 = 0.0074640377424657345 + 0.1 * 6.226528167724609
Epoch 890, val loss: 1.1655216217041016
Epoch 900, training loss: 0.6291772127151489 = 0.007262806873768568 + 0.1 * 6.219143867492676
Epoch 900, val loss: 1.1703503131866455
Epoch 910, training loss: 0.6300942301750183 = 0.0070712389424443245 + 0.1 * 6.230229377746582
Epoch 910, val loss: 1.1749939918518066
Epoch 920, training loss: 0.6282890439033508 = 0.006888286210596561 + 0.1 * 6.214007377624512
Epoch 920, val loss: 1.179540753364563
Epoch 930, training loss: 0.6289253830909729 = 0.006713292095810175 + 0.1 * 6.222120761871338
Epoch 930, val loss: 1.1840177774429321
Epoch 940, training loss: 0.6286622881889343 = 0.0065452042035758495 + 0.1 * 6.221170902252197
Epoch 940, val loss: 1.1884697675704956
Epoch 950, training loss: 0.6288343667984009 = 0.006384298671036959 + 0.1 * 6.2245001792907715
Epoch 950, val loss: 1.1928471326828003
Epoch 960, training loss: 0.6285284161567688 = 0.006230191793292761 + 0.1 * 6.222982406616211
Epoch 960, val loss: 1.1972066164016724
Epoch 970, training loss: 0.6269550323486328 = 0.006082604173570871 + 0.1 * 6.208724498748779
Epoch 970, val loss: 1.2014011144638062
Epoch 980, training loss: 0.6270114779472351 = 0.005940665490925312 + 0.1 * 6.210707664489746
Epoch 980, val loss: 1.2055625915527344
Epoch 990, training loss: 0.6274359822273254 = 0.005804199259728193 + 0.1 * 6.216318130493164
Epoch 990, val loss: 1.209769368171692
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6421
Flip ASR: 0.5911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.799236297607422 = 1.9618512392044067 + 0.1 * 8.373851776123047
Epoch 0, val loss: 1.969054102897644
Epoch 10, training loss: 2.7886366844177246 = 1.9512660503387451 + 0.1 * 8.37370491027832
Epoch 10, val loss: 1.9583336114883423
Epoch 20, training loss: 2.775726079940796 = 1.9384468793869019 + 0.1 * 8.372791290283203
Epoch 20, val loss: 1.9453175067901611
Epoch 30, training loss: 2.7569663524627686 = 1.9203797578811646 + 0.1 * 8.365866661071777
Epoch 30, val loss: 1.926939845085144
Epoch 40, training loss: 2.7260913848876953 = 1.8936752080917358 + 0.1 * 8.3241605758667
Epoch 40, val loss: 1.900024652481079
Epoch 50, training loss: 2.6638903617858887 = 1.8567438125610352 + 0.1 * 8.071465492248535
Epoch 50, val loss: 1.8638687133789062
Epoch 60, training loss: 2.588749647140503 = 1.8131036758422852 + 0.1 * 7.756460189819336
Epoch 60, val loss: 1.8225399255752563
Epoch 70, training loss: 2.5067405700683594 = 1.7685574293136597 + 0.1 * 7.38183069229126
Epoch 70, val loss: 1.7814432382583618
Epoch 80, training loss: 2.4371297359466553 = 1.7243554592132568 + 0.1 * 7.127741813659668
Epoch 80, val loss: 1.7415684461593628
Epoch 90, training loss: 2.37544846534729 = 1.6709474325180054 + 0.1 * 7.045011043548584
Epoch 90, val loss: 1.6932252645492554
Epoch 100, training loss: 2.2985661029815674 = 1.6009002923965454 + 0.1 * 6.976657867431641
Epoch 100, val loss: 1.63186776638031
Epoch 110, training loss: 2.2028419971466064 = 1.5116716623306274 + 0.1 * 6.911702632904053
Epoch 110, val loss: 1.5560481548309326
Epoch 120, training loss: 2.0905985832214355 = 1.4047356843948364 + 0.1 * 6.85862922668457
Epoch 120, val loss: 1.4656726121902466
Epoch 130, training loss: 1.9700870513916016 = 1.2881914377212524 + 0.1 * 6.8189568519592285
Epoch 130, val loss: 1.3683972358703613
Epoch 140, training loss: 1.8498563766479492 = 1.1711254119873047 + 0.1 * 6.787309169769287
Epoch 140, val loss: 1.2714169025421143
Epoch 150, training loss: 1.7359840869903564 = 1.0602014064788818 + 0.1 * 6.757826328277588
Epoch 150, val loss: 1.18101966381073
Epoch 160, training loss: 1.6330952644348145 = 0.9595228433609009 + 0.1 * 6.735723972320557
Epoch 160, val loss: 1.1006686687469482
Epoch 170, training loss: 1.5403238534927368 = 0.8695374727249146 + 0.1 * 6.707863807678223
Epoch 170, val loss: 1.0304166078567505
Epoch 180, training loss: 1.456713318824768 = 0.787752091884613 + 0.1 * 6.68961238861084
Epoch 180, val loss: 0.9681848883628845
Epoch 190, training loss: 1.3804678916931152 = 0.713299572467804 + 0.1 * 6.671682357788086
Epoch 190, val loss: 0.9132770299911499
Epoch 200, training loss: 1.3111908435821533 = 0.645494282245636 + 0.1 * 6.6569647789001465
Epoch 200, val loss: 0.8656068444252014
Epoch 210, training loss: 1.2496984004974365 = 0.5842505693435669 + 0.1 * 6.654478549957275
Epoch 210, val loss: 0.8254508972167969
Epoch 220, training loss: 1.192967176437378 = 0.5292991399765015 + 0.1 * 6.636679649353027
Epoch 220, val loss: 0.7926152944564819
Epoch 230, training loss: 1.1416738033294678 = 0.4793923497200012 + 0.1 * 6.622814655303955
Epoch 230, val loss: 0.7655330300331116
Epoch 240, training loss: 1.0955169200897217 = 0.43397951126098633 + 0.1 * 6.615374565124512
Epoch 240, val loss: 0.7433558702468872
Epoch 250, training loss: 1.0529937744140625 = 0.39287978410720825 + 0.1 * 6.60114049911499
Epoch 250, val loss: 0.7256497144699097
Epoch 260, training loss: 1.014744520187378 = 0.3555891513824463 + 0.1 * 6.591554164886475
Epoch 260, val loss: 0.7116938233375549
Epoch 270, training loss: 0.9801026582717896 = 0.3219859302043915 + 0.1 * 6.581167697906494
Epoch 270, val loss: 0.7012696266174316
Epoch 280, training loss: 0.9483190178871155 = 0.291351854801178 + 0.1 * 6.569671630859375
Epoch 280, val loss: 0.6940819621086121
Epoch 290, training loss: 0.9191066026687622 = 0.2632758319377899 + 0.1 * 6.558307647705078
Epoch 290, val loss: 0.6897518038749695
Epoch 300, training loss: 0.8926548361778259 = 0.23775213956832886 + 0.1 * 6.549026966094971
Epoch 300, val loss: 0.6881564855575562
Epoch 310, training loss: 0.8685957193374634 = 0.21459555625915527 + 0.1 * 6.540001392364502
Epoch 310, val loss: 0.6888372302055359
Epoch 320, training loss: 0.8466628193855286 = 0.19342570006847382 + 0.1 * 6.5323710441589355
Epoch 320, val loss: 0.6919232606887817
Epoch 330, training loss: 0.8261349201202393 = 0.17413970828056335 + 0.1 * 6.519951820373535
Epoch 330, val loss: 0.6970621347427368
Epoch 340, training loss: 0.8082732558250427 = 0.15666542947292328 + 0.1 * 6.516077995300293
Epoch 340, val loss: 0.7040625214576721
Epoch 350, training loss: 0.7917882800102234 = 0.14092405140399933 + 0.1 * 6.508642196655273
Epoch 350, val loss: 0.7125130295753479
Epoch 360, training loss: 0.7771355509757996 = 0.1268170028924942 + 0.1 * 6.503185272216797
Epoch 360, val loss: 0.7223472595214844
Epoch 370, training loss: 0.7628145813941956 = 0.11429381370544434 + 0.1 * 6.485207557678223
Epoch 370, val loss: 0.7330024242401123
Epoch 380, training loss: 0.7509279251098633 = 0.10314599424600601 + 0.1 * 6.477818965911865
Epoch 380, val loss: 0.7445754408836365
Epoch 390, training loss: 0.7404354214668274 = 0.0932435616850853 + 0.1 * 6.47191858291626
Epoch 390, val loss: 0.7566168904304504
Epoch 400, training loss: 0.7318002581596375 = 0.0844925045967102 + 0.1 * 6.473077297210693
Epoch 400, val loss: 0.7689629197120667
Epoch 410, training loss: 0.7224554419517517 = 0.07678306102752686 + 0.1 * 6.456723690032959
Epoch 410, val loss: 0.781185507774353
Epoch 420, training loss: 0.7141806483268738 = 0.06995127350091934 + 0.1 * 6.442293167114258
Epoch 420, val loss: 0.7934804558753967
Epoch 430, training loss: 0.7100032567977905 = 0.06388729810714722 + 0.1 * 6.4611592292785645
Epoch 430, val loss: 0.8057093024253845
Epoch 440, training loss: 0.7018476724624634 = 0.05853500962257385 + 0.1 * 6.433126926422119
Epoch 440, val loss: 0.817466139793396
Epoch 450, training loss: 0.6975492238998413 = 0.053781189024448395 + 0.1 * 6.437679767608643
Epoch 450, val loss: 0.8291676640510559
Epoch 460, training loss: 0.6907705664634705 = 0.04955863207578659 + 0.1 * 6.412118911743164
Epoch 460, val loss: 0.84034663438797
Epoch 470, training loss: 0.6882288455963135 = 0.04578665643930435 + 0.1 * 6.424421787261963
Epoch 470, val loss: 0.8513959050178528
Epoch 480, training loss: 0.6817132234573364 = 0.04242152348160744 + 0.1 * 6.392916679382324
Epoch 480, val loss: 0.8619782328605652
Epoch 490, training loss: 0.6780245304107666 = 0.03939276561141014 + 0.1 * 6.386317253112793
Epoch 490, val loss: 0.8723445534706116
Epoch 500, training loss: 0.6742123961448669 = 0.03666062653064728 + 0.1 * 6.375517845153809
Epoch 500, val loss: 0.8824712634086609
Epoch 510, training loss: 0.6726177930831909 = 0.03419140726327896 + 0.1 * 6.384263515472412
Epoch 510, val loss: 0.892439067363739
Epoch 520, training loss: 0.6690461039543152 = 0.0319674126803875 + 0.1 * 6.370787143707275
Epoch 520, val loss: 0.9018766283988953
Epoch 530, training loss: 0.6663858294487 = 0.02995390072464943 + 0.1 * 6.364319324493408
Epoch 530, val loss: 0.9111920595169067
Epoch 540, training loss: 0.6632789969444275 = 0.028124041855335236 + 0.1 * 6.3515496253967285
Epoch 540, val loss: 0.9201055765151978
Epoch 550, training loss: 0.6619956493377686 = 0.026454275473952293 + 0.1 * 6.355413913726807
Epoch 550, val loss: 0.9289658069610596
Epoch 560, training loss: 0.6596959829330444 = 0.024934493005275726 + 0.1 * 6.347614765167236
Epoch 560, val loss: 0.9373490214347839
Epoch 570, training loss: 0.6577034592628479 = 0.023540863767266273 + 0.1 * 6.341626167297363
Epoch 570, val loss: 0.9456523656845093
Epoch 580, training loss: 0.6552302837371826 = 0.02226092666387558 + 0.1 * 6.329693794250488
Epoch 580, val loss: 0.9537121653556824
Epoch 590, training loss: 0.6553367972373962 = 0.021080950275063515 + 0.1 * 6.34255838394165
Epoch 590, val loss: 0.9616298079490662
Epoch 600, training loss: 0.6529663801193237 = 0.01999625377357006 + 0.1 * 6.3297014236450195
Epoch 600, val loss: 0.9693384170532227
Epoch 610, training loss: 0.6538599133491516 = 0.01899595744907856 + 0.1 * 6.348639488220215
Epoch 610, val loss: 0.9767466187477112
Epoch 620, training loss: 0.6503478288650513 = 0.01807413622736931 + 0.1 * 6.322736740112305
Epoch 620, val loss: 0.9840379357337952
Epoch 630, training loss: 0.6487621068954468 = 0.017220092937350273 + 0.1 * 6.315420150756836
Epoch 630, val loss: 0.9909898638725281
Epoch 640, training loss: 0.6471442580223083 = 0.016425585374236107 + 0.1 * 6.307186603546143
Epoch 640, val loss: 0.9979338645935059
Epoch 650, training loss: 0.6477483510971069 = 0.01568666659295559 + 0.1 * 6.320616722106934
Epoch 650, val loss: 1.0046519041061401
Epoch 660, training loss: 0.6451281905174255 = 0.014999683015048504 + 0.1 * 6.3012847900390625
Epoch 660, val loss: 1.0112096071243286
Epoch 670, training loss: 0.644935667514801 = 0.014358904212713242 + 0.1 * 6.305768013000488
Epoch 670, val loss: 1.0176522731781006
Epoch 680, training loss: 0.6430442333221436 = 0.01376081258058548 + 0.1 * 6.292834281921387
Epoch 680, val loss: 1.0240097045898438
Epoch 690, training loss: 0.641948938369751 = 0.013203191570937634 + 0.1 * 6.287457466125488
Epoch 690, val loss: 1.0300183296203613
Epoch 700, training loss: 0.6426253318786621 = 0.012680103071033955 + 0.1 * 6.29945182800293
Epoch 700, val loss: 1.0360417366027832
Epoch 710, training loss: 0.6406958699226379 = 0.01218962948769331 + 0.1 * 6.285062313079834
Epoch 710, val loss: 1.041792869567871
Epoch 720, training loss: 0.6415714025497437 = 0.011728495359420776 + 0.1 * 6.298429012298584
Epoch 720, val loss: 1.047584056854248
Epoch 730, training loss: 0.6385335326194763 = 0.011295640841126442 + 0.1 * 6.272378921508789
Epoch 730, val loss: 1.0531558990478516
Epoch 740, training loss: 0.6378787159919739 = 0.010887999087572098 + 0.1 * 6.269906997680664
Epoch 740, val loss: 1.0585752725601196
Epoch 750, training loss: 0.6416590809822083 = 0.010502816177904606 + 0.1 * 6.311562538146973
Epoch 750, val loss: 1.0640112161636353
Epoch 760, training loss: 0.6369391083717346 = 0.010141064412891865 + 0.1 * 6.267980098724365
Epoch 760, val loss: 1.0692713260650635
Epoch 770, training loss: 0.6365365982055664 = 0.009799524210393429 + 0.1 * 6.267370700836182
Epoch 770, val loss: 1.0742448568344116
Epoch 780, training loss: 0.6362012028694153 = 0.009475559927523136 + 0.1 * 6.267256259918213
Epoch 780, val loss: 1.079349398612976
Epoch 790, training loss: 0.6360935568809509 = 0.009169116616249084 + 0.1 * 6.26924467086792
Epoch 790, val loss: 1.0842574834823608
Epoch 800, training loss: 0.6347634196281433 = 0.008878606371581554 + 0.1 * 6.258848190307617
Epoch 800, val loss: 1.0890933275222778
Epoch 810, training loss: 0.6354590654373169 = 0.008603556081652641 + 0.1 * 6.2685546875
Epoch 810, val loss: 1.0937919616699219
Epoch 820, training loss: 0.6334074139595032 = 0.008342509157955647 + 0.1 * 6.2506489753723145
Epoch 820, val loss: 1.0984190702438354
Epoch 830, training loss: 0.6338638663291931 = 0.008094166405498981 + 0.1 * 6.257696628570557
Epoch 830, val loss: 1.1028908491134644
Epoch 840, training loss: 0.6322251558303833 = 0.007857762277126312 + 0.1 * 6.243674278259277
Epoch 840, val loss: 1.1073520183563232
Epoch 850, training loss: 0.633294403553009 = 0.007632113061845303 + 0.1 * 6.256622791290283
Epoch 850, val loss: 1.1117147207260132
Epoch 860, training loss: 0.6315155625343323 = 0.007416740991175175 + 0.1 * 6.240988254547119
Epoch 860, val loss: 1.116097331047058
Epoch 870, training loss: 0.6309409737586975 = 0.007211489602923393 + 0.1 * 6.237294673919678
Epoch 870, val loss: 1.1202993392944336
Epoch 880, training loss: 0.6320673823356628 = 0.00701582757756114 + 0.1 * 6.250515460968018
Epoch 880, val loss: 1.124469518661499
Epoch 890, training loss: 0.6324880123138428 = 0.006828872486948967 + 0.1 * 6.256591320037842
Epoch 890, val loss: 1.1286274194717407
Epoch 900, training loss: 0.6301440596580505 = 0.0066507053561508656 + 0.1 * 6.234933853149414
Epoch 900, val loss: 1.132523775100708
Epoch 910, training loss: 0.6295873522758484 = 0.0064802211709320545 + 0.1 * 6.231071472167969
Epoch 910, val loss: 1.1364283561706543
Epoch 920, training loss: 0.6291890740394592 = 0.006316398736089468 + 0.1 * 6.228726387023926
Epoch 920, val loss: 1.140265941619873
Epoch 930, training loss: 0.6291236281394958 = 0.00615910068154335 + 0.1 * 6.229644775390625
Epoch 930, val loss: 1.144165277481079
Epoch 940, training loss: 0.6289814710617065 = 0.006008585449308157 + 0.1 * 6.229728698730469
Epoch 940, val loss: 1.1479063034057617
Epoch 950, training loss: 0.6287437081336975 = 0.005864511243999004 + 0.1 * 6.2287917137146
Epoch 950, val loss: 1.15153968334198
Epoch 960, training loss: 0.6283478140830994 = 0.005725954193621874 + 0.1 * 6.226218223571777
Epoch 960, val loss: 1.155220866203308
Epoch 970, training loss: 0.6271151304244995 = 0.005592782981693745 + 0.1 * 6.215223789215088
Epoch 970, val loss: 1.1588133573532104
Epoch 980, training loss: 0.6282813549041748 = 0.005464998073875904 + 0.1 * 6.228163719177246
Epoch 980, val loss: 1.1623344421386719
Epoch 990, training loss: 0.6284944415092468 = 0.005342457443475723 + 0.1 * 6.2315192222595215
Epoch 990, val loss: 1.1658504009246826
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8413
Flip ASR: 0.8089/225 nodes
The final ASR:0.74785, 0.08181, Accuracy:0.82346, 0.01968
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10566])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7936620712280273 = 1.9562793970108032 + 0.1 * 8.373826026916504
Epoch 0, val loss: 1.9549237489700317
Epoch 10, training loss: 2.7830970287323 = 1.9457294940948486 + 0.1 * 8.373674392700195
Epoch 10, val loss: 1.9456285238265991
Epoch 20, training loss: 2.7705976963043213 = 1.9333263635635376 + 0.1 * 8.372714042663574
Epoch 20, val loss: 1.9343514442443848
Epoch 30, training loss: 2.752685070037842 = 1.9161430597305298 + 0.1 * 8.3654203414917
Epoch 30, val loss: 1.918412685394287
Epoch 40, training loss: 2.7224135398864746 = 1.8908644914627075 + 0.1 * 8.315489768981934
Epoch 40, val loss: 1.8950903415679932
Epoch 50, training loss: 2.652423858642578 = 1.8568556308746338 + 0.1 * 7.955682754516602
Epoch 50, val loss: 1.8648250102996826
Epoch 60, training loss: 2.57914137840271 = 1.8191704750061035 + 0.1 * 7.5997090339660645
Epoch 60, val loss: 1.8323029279708862
Epoch 70, training loss: 2.5009961128234863 = 1.7829772233963013 + 0.1 * 7.180187702178955
Epoch 70, val loss: 1.8016858100891113
Epoch 80, training loss: 2.4345974922180176 = 1.7490718364715576 + 0.1 * 6.855256080627441
Epoch 80, val loss: 1.7730602025985718
Epoch 90, training loss: 2.381883382797241 = 1.7084463834762573 + 0.1 * 6.734369277954102
Epoch 90, val loss: 1.7350704669952393
Epoch 100, training loss: 2.3200902938842773 = 1.6520018577575684 + 0.1 * 6.680883884429932
Epoch 100, val loss: 1.6823737621307373
Epoch 110, training loss: 2.24307918548584 = 1.5784060955047607 + 0.1 * 6.646730422973633
Epoch 110, val loss: 1.6182316541671753
Epoch 120, training loss: 2.152926206588745 = 1.4911028146743774 + 0.1 * 6.618234634399414
Epoch 120, val loss: 1.5439366102218628
Epoch 130, training loss: 2.058603525161743 = 1.39887273311615 + 0.1 * 6.597307205200195
Epoch 130, val loss: 1.4671276807785034
Epoch 140, training loss: 1.964468240737915 = 1.3063759803771973 + 0.1 * 6.5809221267700195
Epoch 140, val loss: 1.3920139074325562
Epoch 150, training loss: 1.8729616403579712 = 1.2161020040512085 + 0.1 * 6.568596363067627
Epoch 150, val loss: 1.3216395378112793
Epoch 160, training loss: 1.7829816341400146 = 1.127453088760376 + 0.1 * 6.555285930633545
Epoch 160, val loss: 1.2545373439788818
Epoch 170, training loss: 1.6937285661697388 = 1.0393866300582886 + 0.1 * 6.543419361114502
Epoch 170, val loss: 1.189018964767456
Epoch 180, training loss: 1.605385184288025 = 0.9522378444671631 + 0.1 * 6.531473159790039
Epoch 180, val loss: 1.1250299215316772
Epoch 190, training loss: 1.5198824405670166 = 0.8677085041999817 + 0.1 * 6.521738529205322
Epoch 190, val loss: 1.0637356042861938
Epoch 200, training loss: 1.4393196105957031 = 0.7879648208618164 + 0.1 * 6.513547420501709
Epoch 200, val loss: 1.0067415237426758
Epoch 210, training loss: 1.3652458190917969 = 0.7148341536521912 + 0.1 * 6.504117012023926
Epoch 210, val loss: 0.9557228088378906
Epoch 220, training loss: 1.297674536705017 = 0.6478198766708374 + 0.1 * 6.498546600341797
Epoch 220, val loss: 0.9103531241416931
Epoch 230, training loss: 1.2356772422790527 = 0.587050199508667 + 0.1 * 6.486269474029541
Epoch 230, val loss: 0.8709357380867004
Epoch 240, training loss: 1.1804370880126953 = 0.5316612124443054 + 0.1 * 6.487758636474609
Epoch 240, val loss: 0.837395966053009
Epoch 250, training loss: 1.1282058954238892 = 0.48156648874282837 + 0.1 * 6.466393947601318
Epoch 250, val loss: 0.8095806241035461
Epoch 260, training loss: 1.0811506509780884 = 0.435651034116745 + 0.1 * 6.454996585845947
Epoch 260, val loss: 0.7863225340843201
Epoch 270, training loss: 1.0384609699249268 = 0.3931356370449066 + 0.1 * 6.453252792358398
Epoch 270, val loss: 0.7668054699897766
Epoch 280, training loss: 0.9973388910293579 = 0.3538883328437805 + 0.1 * 6.434505462646484
Epoch 280, val loss: 0.7508235573768616
Epoch 290, training loss: 0.96085524559021 = 0.31748077273368835 + 0.1 * 6.433744430541992
Epoch 290, val loss: 0.7376475930213928
Epoch 300, training loss: 0.925763726234436 = 0.28397053480148315 + 0.1 * 6.41793155670166
Epoch 300, val loss: 0.7272337675094604
Epoch 310, training loss: 0.8943073749542236 = 0.25333908200263977 + 0.1 * 6.4096832275390625
Epoch 310, val loss: 0.7192766070365906
Epoch 320, training loss: 0.8682008981704712 = 0.22564920783042908 + 0.1 * 6.4255170822143555
Epoch 320, val loss: 0.7134478092193604
Epoch 330, training loss: 0.8401699662208557 = 0.2012222558259964 + 0.1 * 6.389477252960205
Epoch 330, val loss: 0.7096710801124573
Epoch 340, training loss: 0.8178781270980835 = 0.17970027029514313 + 0.1 * 6.381778717041016
Epoch 340, val loss: 0.7077997922897339
Epoch 350, training loss: 0.7995201349258423 = 0.16088472306728363 + 0.1 * 6.386353969573975
Epoch 350, val loss: 0.7079134583473206
Epoch 360, training loss: 0.7818824052810669 = 0.1445694863796234 + 0.1 * 6.373128890991211
Epoch 360, val loss: 0.709854006767273
Epoch 370, training loss: 0.7660826444625854 = 0.13037444651126862 + 0.1 * 6.357081890106201
Epoch 370, val loss: 0.7133516073226929
Epoch 380, training loss: 0.7528387904167175 = 0.11799146234989166 + 0.1 * 6.34847354888916
Epoch 380, val loss: 0.7182786464691162
Epoch 390, training loss: 0.7421865463256836 = 0.10717083513736725 + 0.1 * 6.350157260894775
Epoch 390, val loss: 0.7242980599403381
Epoch 400, training loss: 0.731611967086792 = 0.09767591953277588 + 0.1 * 6.339360237121582
Epoch 400, val loss: 0.7312285900115967
Epoch 410, training loss: 0.7222879528999329 = 0.08925735950469971 + 0.1 * 6.330306053161621
Epoch 410, val loss: 0.7389205098152161
Epoch 420, training loss: 0.7160758972167969 = 0.08178720623254776 + 0.1 * 6.342886447906494
Epoch 420, val loss: 0.7472549080848694
Epoch 430, training loss: 0.7068341970443726 = 0.07515764236450195 + 0.1 * 6.316765308380127
Epoch 430, val loss: 0.7559786438941956
Epoch 440, training loss: 0.6999168395996094 = 0.0692315623164177 + 0.1 * 6.306852340698242
Epoch 440, val loss: 0.7650459408760071
Epoch 450, training loss: 0.6949636936187744 = 0.06390969455242157 + 0.1 * 6.310539722442627
Epoch 450, val loss: 0.7743976712226868
Epoch 460, training loss: 0.690133810043335 = 0.05914768576622009 + 0.1 * 6.309860706329346
Epoch 460, val loss: 0.7838512659072876
Epoch 470, training loss: 0.6838691234588623 = 0.054873235523700714 + 0.1 * 6.289958953857422
Epoch 470, val loss: 0.7933743596076965
Epoch 480, training loss: 0.6824181079864502 = 0.05100811645388603 + 0.1 * 6.3140997886657715
Epoch 480, val loss: 0.8029197454452515
Epoch 490, training loss: 0.6759964227676392 = 0.04752466082572937 + 0.1 * 6.284718036651611
Epoch 490, val loss: 0.8123735189437866
Epoch 500, training loss: 0.6719611287117004 = 0.04436998441815376 + 0.1 * 6.275911331176758
Epoch 500, val loss: 0.8217973709106445
Epoch 510, training loss: 0.6684321761131287 = 0.04149412363767624 + 0.1 * 6.26938009262085
Epoch 510, val loss: 0.8311457633972168
Epoch 520, training loss: 0.6669743061065674 = 0.03887089714407921 + 0.1 * 6.281034469604492
Epoch 520, val loss: 0.8403685688972473
Epoch 530, training loss: 0.6629189252853394 = 0.03648703917860985 + 0.1 * 6.264318466186523
Epoch 530, val loss: 0.8494892120361328
Epoch 540, training loss: 0.6602950692176819 = 0.03430547937750816 + 0.1 * 6.2598958015441895
Epoch 540, val loss: 0.8584259748458862
Epoch 550, training loss: 0.6578904986381531 = 0.032306212931871414 + 0.1 * 6.255843162536621
Epoch 550, val loss: 0.8672003746032715
Epoch 560, training loss: 0.6569963097572327 = 0.03047192469239235 + 0.1 * 6.2652435302734375
Epoch 560, val loss: 0.875868022441864
Epoch 570, training loss: 0.6541860103607178 = 0.028787173330783844 + 0.1 * 6.253988265991211
Epoch 570, val loss: 0.8842793107032776
Epoch 580, training loss: 0.6518674492835999 = 0.027237508445978165 + 0.1 * 6.246298789978027
Epoch 580, val loss: 0.8925966024398804
Epoch 590, training loss: 0.6510183215141296 = 0.025804003700613976 + 0.1 * 6.252143383026123
Epoch 590, val loss: 0.9006471633911133
Epoch 600, training loss: 0.6486879587173462 = 0.024482036009430885 + 0.1 * 6.242059230804443
Epoch 600, val loss: 0.9086464643478394
Epoch 610, training loss: 0.6472113728523254 = 0.02325567416846752 + 0.1 * 6.239557266235352
Epoch 610, val loss: 0.9164491295814514
Epoch 620, training loss: 0.6451785564422607 = 0.02211698703467846 + 0.1 * 6.230615615844727
Epoch 620, val loss: 0.9241591691970825
Epoch 630, training loss: 0.6448217630386353 = 0.02105780690908432 + 0.1 * 6.237639427185059
Epoch 630, val loss: 0.9316874742507935
Epoch 640, training loss: 0.6436996459960938 = 0.020076042041182518 + 0.1 * 6.236236095428467
Epoch 640, val loss: 0.93907231092453
Epoch 650, training loss: 0.6418534517288208 = 0.01916278712451458 + 0.1 * 6.226906776428223
Epoch 650, val loss: 0.9463534951210022
Epoch 660, training loss: 0.6427498459815979 = 0.0183096956461668 + 0.1 * 6.244400978088379
Epoch 660, val loss: 0.9534415006637573
Epoch 670, training loss: 0.6401373744010925 = 0.01751384511590004 + 0.1 * 6.2262349128723145
Epoch 670, val loss: 0.9603865742683411
Epoch 680, training loss: 0.6388779878616333 = 0.0167704951018095 + 0.1 * 6.221075057983398
Epoch 680, val loss: 0.9672636985778809
Epoch 690, training loss: 0.6387233138084412 = 0.016073843464255333 + 0.1 * 6.226494789123535
Epoch 690, val loss: 0.9738885760307312
Epoch 700, training loss: 0.6365172266960144 = 0.015420977026224136 + 0.1 * 6.210962295532227
Epoch 700, val loss: 0.9804683923721313
Epoch 710, training loss: 0.6363409757614136 = 0.014807912521064281 + 0.1 * 6.215330123901367
Epoch 710, val loss: 0.9869561195373535
Epoch 720, training loss: 0.6355971693992615 = 0.0142305176705122 + 0.1 * 6.2136664390563965
Epoch 720, val loss: 0.9932910799980164
Epoch 730, training loss: 0.6349563002586365 = 0.01368728093802929 + 0.1 * 6.212690353393555
Epoch 730, val loss: 0.9994844794273376
Epoch 740, training loss: 0.633804440498352 = 0.013176719658076763 + 0.1 * 6.206276893615723
Epoch 740, val loss: 1.0056397914886475
Epoch 750, training loss: 0.6348509788513184 = 0.01269440446048975 + 0.1 * 6.2215657234191895
Epoch 750, val loss: 1.0116287469863892
Epoch 760, training loss: 0.6329101920127869 = 0.012239743955433369 + 0.1 * 6.206704139709473
Epoch 760, val loss: 1.0175291299819946
Epoch 770, training loss: 0.6315252780914307 = 0.011810749769210815 + 0.1 * 6.197145462036133
Epoch 770, val loss: 1.0233254432678223
Epoch 780, training loss: 0.6315929889678955 = 0.01140527706593275 + 0.1 * 6.201876640319824
Epoch 780, val loss: 1.029030442237854
Epoch 790, training loss: 0.6301456093788147 = 0.011021045967936516 + 0.1 * 6.191246032714844
Epoch 790, val loss: 1.0346015691757202
Epoch 800, training loss: 0.6313815116882324 = 0.010656044818460941 + 0.1 * 6.207254409790039
Epoch 800, val loss: 1.040088176727295
Epoch 810, training loss: 0.6304292678833008 = 0.010309726931154728 + 0.1 * 6.201195240020752
Epoch 810, val loss: 1.0454853773117065
Epoch 820, training loss: 0.6294952630996704 = 0.00998248066753149 + 0.1 * 6.195127964019775
Epoch 820, val loss: 1.0508813858032227
Epoch 830, training loss: 0.6289770007133484 = 0.009670982137322426 + 0.1 * 6.193060398101807
Epoch 830, val loss: 1.0561275482177734
Epoch 840, training loss: 0.6285010576248169 = 0.00937467347830534 + 0.1 * 6.191263675689697
Epoch 840, val loss: 1.0611767768859863
Epoch 850, training loss: 0.6275774240493774 = 0.009094047360122204 + 0.1 * 6.184833526611328
Epoch 850, val loss: 1.0663201808929443
Epoch 860, training loss: 0.6266427636146545 = 0.008826585486531258 + 0.1 * 6.17816162109375
Epoch 860, val loss: 1.0713136196136475
Epoch 870, training loss: 0.627152144908905 = 0.00857081450521946 + 0.1 * 6.1858134269714355
Epoch 870, val loss: 1.0761523246765137
Epoch 880, training loss: 0.6265512704849243 = 0.00832761824131012 + 0.1 * 6.182236671447754
Epoch 880, val loss: 1.0810034275054932
Epoch 890, training loss: 0.6259677410125732 = 0.00809516105800867 + 0.1 * 6.178725719451904
Epoch 890, val loss: 1.0858107805252075
Epoch 900, training loss: 0.6269044280052185 = 0.007872776128351688 + 0.1 * 6.190316677093506
Epoch 900, val loss: 1.0904117822647095
Epoch 910, training loss: 0.6249902248382568 = 0.00766034796833992 + 0.1 * 6.1732988357543945
Epoch 910, val loss: 1.0950276851654053
Epoch 920, training loss: 0.6247822642326355 = 0.007457826752215624 + 0.1 * 6.173243999481201
Epoch 920, val loss: 1.0996068716049194
Epoch 930, training loss: 0.624380350112915 = 0.007263498846441507 + 0.1 * 6.171168327331543
Epoch 930, val loss: 1.1040740013122559
Epoch 940, training loss: 0.6242374181747437 = 0.0070768240839242935 + 0.1 * 6.171606063842773
Epoch 940, val loss: 1.1084365844726562
Epoch 950, training loss: 0.6254856586456299 = 0.006898355670273304 + 0.1 * 6.185873031616211
Epoch 950, val loss: 1.1127514839172363
Epoch 960, training loss: 0.623859167098999 = 0.006727803964167833 + 0.1 * 6.171313285827637
Epoch 960, val loss: 1.1171035766601562
Epoch 970, training loss: 0.6228950619697571 = 0.00656424555927515 + 0.1 * 6.1633076667785645
Epoch 970, val loss: 1.1213704347610474
Epoch 980, training loss: 0.6247345209121704 = 0.006406966596841812 + 0.1 * 6.18327522277832
Epoch 980, val loss: 1.1254547834396362
Epoch 990, training loss: 0.6226793527603149 = 0.006255484186112881 + 0.1 * 6.164238452911377
Epoch 990, val loss: 1.129524827003479
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7343
Flip ASR: 0.6844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.796436309814453 = 1.959053635597229 + 0.1 * 8.373827934265137
Epoch 0, val loss: 1.961177110671997
Epoch 10, training loss: 2.7868199348449707 = 1.949454426765442 + 0.1 * 8.373656272888184
Epoch 10, val loss: 1.9517749547958374
Epoch 20, training loss: 2.7751760482788086 = 1.9379122257232666 + 0.1 * 8.372638702392578
Epoch 20, val loss: 1.9401708841323853
Epoch 30, training loss: 2.758549451828003 = 1.9220573902130127 + 0.1 * 8.364919662475586
Epoch 30, val loss: 1.9240275621414185
Epoch 40, training loss: 2.7298521995544434 = 1.898809790611267 + 0.1 * 8.3104248046875
Epoch 40, val loss: 1.9002456665039062
Epoch 50, training loss: 2.6688485145568848 = 1.8662116527557373 + 0.1 * 8.026368141174316
Epoch 50, val loss: 1.8676470518112183
Epoch 60, training loss: 2.594538688659668 = 1.8264427185058594 + 0.1 * 7.680959701538086
Epoch 60, val loss: 1.8293118476867676
Epoch 70, training loss: 2.510514259338379 = 1.7856706380844116 + 0.1 * 7.24843692779541
Epoch 70, val loss: 1.791643738746643
Epoch 80, training loss: 2.4406986236572266 = 1.7456169128417969 + 0.1 * 6.950816631317139
Epoch 80, val loss: 1.7561490535736084
Epoch 90, training loss: 2.3847532272338867 = 1.7004787921905518 + 0.1 * 6.842743873596191
Epoch 90, val loss: 1.715928554534912
Epoch 100, training loss: 2.318540334701538 = 1.6404155492782593 + 0.1 * 6.781247138977051
Epoch 100, val loss: 1.663451910018921
Epoch 110, training loss: 2.237110137939453 = 1.5634000301361084 + 0.1 * 6.737099647521973
Epoch 110, val loss: 1.5986047983169556
Epoch 120, training loss: 2.142821788787842 = 1.4721381664276123 + 0.1 * 6.7068352699279785
Epoch 120, val loss: 1.5240864753723145
Epoch 130, training loss: 2.0427770614624023 = 1.3744709491729736 + 0.1 * 6.683061122894287
Epoch 130, val loss: 1.4471676349639893
Epoch 140, training loss: 1.9429482221603394 = 1.2770590782165527 + 0.1 * 6.658891201019287
Epoch 140, val loss: 1.371760606765747
Epoch 150, training loss: 1.8476773500442505 = 1.1829419136047363 + 0.1 * 6.6473541259765625
Epoch 150, val loss: 1.2998101711273193
Epoch 160, training loss: 1.7581144571304321 = 1.0953216552734375 + 0.1 * 6.627927780151367
Epoch 160, val loss: 1.2333556413650513
Epoch 170, training loss: 1.6758918762207031 = 1.0143417119979858 + 0.1 * 6.615501880645752
Epoch 170, val loss: 1.1715670824050903
Epoch 180, training loss: 1.600416660308838 = 0.9398444890975952 + 0.1 * 6.605721950531006
Epoch 180, val loss: 1.1144438982009888
Epoch 190, training loss: 1.529394507408142 = 0.8703410029411316 + 0.1 * 6.590534687042236
Epoch 190, val loss: 1.0609958171844482
Epoch 200, training loss: 1.4626013040542603 = 0.8041700720787048 + 0.1 * 6.584312438964844
Epoch 200, val loss: 1.009890079498291
Epoch 210, training loss: 1.3977336883544922 = 0.7411152720451355 + 0.1 * 6.5661845207214355
Epoch 210, val loss: 0.961420476436615
Epoch 220, training loss: 1.3356196880340576 = 0.6803838610649109 + 0.1 * 6.552358150482178
Epoch 220, val loss: 0.9157006740570068
Epoch 230, training loss: 1.276906967163086 = 0.6226877570152283 + 0.1 * 6.542192459106445
Epoch 230, val loss: 0.8743942379951477
Epoch 240, training loss: 1.2211389541625977 = 0.5689411163330078 + 0.1 * 6.521978855133057
Epoch 240, val loss: 0.8392184376716614
Epoch 250, training loss: 1.1689350605010986 = 0.5178394317626953 + 0.1 * 6.510955810546875
Epoch 250, val loss: 0.8089666962623596
Epoch 260, training loss: 1.1187677383422852 = 0.4692992568016052 + 0.1 * 6.494685173034668
Epoch 260, val loss: 0.783237099647522
Epoch 270, training loss: 1.0713123083114624 = 0.4230200946331024 + 0.1 * 6.482922077178955
Epoch 270, val loss: 0.7613055109977722
Epoch 280, training loss: 1.0265291929244995 = 0.3791052997112274 + 0.1 * 6.474238872528076
Epoch 280, val loss: 0.7424308657646179
Epoch 290, training loss: 0.9828325510025024 = 0.3375750482082367 + 0.1 * 6.452575206756592
Epoch 290, val loss: 0.726539671421051
Epoch 300, training loss: 0.9453113079071045 = 0.2988007664680481 + 0.1 * 6.4651055335998535
Epoch 300, val loss: 0.7134477496147156
Epoch 310, training loss: 0.9067686200141907 = 0.2634456753730774 + 0.1 * 6.433229446411133
Epoch 310, val loss: 0.7035138010978699
Epoch 320, training loss: 0.8739898800849915 = 0.2315264195203781 + 0.1 * 6.4246344566345215
Epoch 320, val loss: 0.6964492201805115
Epoch 330, training loss: 0.8447218537330627 = 0.20338879525661469 + 0.1 * 6.413330078125
Epoch 330, val loss: 0.6922455430030823
Epoch 340, training loss: 0.8189128041267395 = 0.1790233999490738 + 0.1 * 6.398894309997559
Epoch 340, val loss: 0.6907033324241638
Epoch 350, training loss: 0.8011045455932617 = 0.15815764665603638 + 0.1 * 6.429469108581543
Epoch 350, val loss: 0.6914423108100891
Epoch 360, training loss: 0.7794352769851685 = 0.14054566621780396 + 0.1 * 6.3888959884643555
Epoch 360, val loss: 0.6941201090812683
Epoch 370, training loss: 0.7628578543663025 = 0.12547756731510162 + 0.1 * 6.37380313873291
Epoch 370, val loss: 0.6982346773147583
Epoch 380, training loss: 0.7495708465576172 = 0.11248661577701569 + 0.1 * 6.370842456817627
Epoch 380, val loss: 0.7038001418113708
Epoch 390, training loss: 0.7382912635803223 = 0.10128413140773773 + 0.1 * 6.370070934295654
Epoch 390, val loss: 0.7102624773979187
Epoch 400, training loss: 0.7267381548881531 = 0.09159261733293533 + 0.1 * 6.351455211639404
Epoch 400, val loss: 0.7175266742706299
Epoch 410, training loss: 0.7187243103981018 = 0.08312749862670898 + 0.1 * 6.355967998504639
Epoch 410, val loss: 0.7252858281135559
Epoch 420, training loss: 0.7104198336601257 = 0.07573048770427704 + 0.1 * 6.346893310546875
Epoch 420, val loss: 0.7335745096206665
Epoch 430, training loss: 0.7029318809509277 = 0.06923490762710571 + 0.1 * 6.336969375610352
Epoch 430, val loss: 0.7421844601631165
Epoch 440, training loss: 0.6962698101997375 = 0.06347769498825073 + 0.1 * 6.327920913696289
Epoch 440, val loss: 0.7509504556655884
Epoch 450, training loss: 0.6919803023338318 = 0.058363936841487885 + 0.1 * 6.336163520812988
Epoch 450, val loss: 0.7599071860313416
Epoch 460, training loss: 0.6862979531288147 = 0.05385838821530342 + 0.1 * 6.324395656585693
Epoch 460, val loss: 0.7687355279922485
Epoch 470, training loss: 0.6814756393432617 = 0.049849819391965866 + 0.1 * 6.316258430480957
Epoch 470, val loss: 0.7778216600418091
Epoch 480, training loss: 0.6764969825744629 = 0.04624772071838379 + 0.1 * 6.302492618560791
Epoch 480, val loss: 0.7868008017539978
Epoch 490, training loss: 0.6743929982185364 = 0.04300135746598244 + 0.1 * 6.313916206359863
Epoch 490, val loss: 0.7958431839942932
Epoch 500, training loss: 0.6701527833938599 = 0.040077611804008484 + 0.1 * 6.300751686096191
Epoch 500, val loss: 0.8048715591430664
Epoch 510, training loss: 0.6666456460952759 = 0.03743251413106918 + 0.1 * 6.292131423950195
Epoch 510, val loss: 0.8138052225112915
Epoch 520, training loss: 0.6656849384307861 = 0.03504136949777603 + 0.1 * 6.3064351081848145
Epoch 520, val loss: 0.822420060634613
Epoch 530, training loss: 0.6616228222846985 = 0.03288139030337334 + 0.1 * 6.287414073944092
Epoch 530, val loss: 0.8312082886695862
Epoch 540, training loss: 0.6590552926063538 = 0.030912643298506737 + 0.1 * 6.281426429748535
Epoch 540, val loss: 0.8397483825683594
Epoch 550, training loss: 0.6564906239509583 = 0.02911726012825966 + 0.1 * 6.273733139038086
Epoch 550, val loss: 0.8479750752449036
Epoch 560, training loss: 0.6546393632888794 = 0.027478570118546486 + 0.1 * 6.271607875823975
Epoch 560, val loss: 0.8563634157180786
Epoch 570, training loss: 0.6520625352859497 = 0.02597413770854473 + 0.1 * 6.260883808135986
Epoch 570, val loss: 0.8645612597465515
Epoch 580, training loss: 0.6530059576034546 = 0.024588648229837418 + 0.1 * 6.284173011779785
Epoch 580, val loss: 0.872528612613678
Epoch 590, training loss: 0.6495267748832703 = 0.023319508880376816 + 0.1 * 6.262073040008545
Epoch 590, val loss: 0.8802947998046875
Epoch 600, training loss: 0.6485316753387451 = 0.022148411720991135 + 0.1 * 6.263832092285156
Epoch 600, val loss: 0.8881344795227051
Epoch 610, training loss: 0.6462100744247437 = 0.02106647379696369 + 0.1 * 6.25143575668335
Epoch 610, val loss: 0.8955024480819702
Epoch 620, training loss: 0.6454280018806458 = 0.020065346732735634 + 0.1 * 6.253626346588135
Epoch 620, val loss: 0.9030332565307617
Epoch 630, training loss: 0.6465627551078796 = 0.01913355104625225 + 0.1 * 6.274291515350342
Epoch 630, val loss: 0.910301923751831
Epoch 640, training loss: 0.6435950398445129 = 0.01826965995132923 + 0.1 * 6.25325345993042
Epoch 640, val loss: 0.9172511100769043
Epoch 650, training loss: 0.6420043706893921 = 0.0174653809517622 + 0.1 * 6.245389938354492
Epoch 650, val loss: 0.9243733286857605
Epoch 660, training loss: 0.6422169208526611 = 0.016714150086045265 + 0.1 * 6.2550272941589355
Epoch 660, val loss: 0.9311094880104065
Epoch 670, training loss: 0.639218807220459 = 0.016012582927942276 + 0.1 * 6.232061862945557
Epoch 670, val loss: 0.9378185868263245
Epoch 680, training loss: 0.6384270191192627 = 0.015355709008872509 + 0.1 * 6.230712890625
Epoch 680, val loss: 0.9445310235023499
Epoch 690, training loss: 0.63919597864151 = 0.014739127829670906 + 0.1 * 6.24456787109375
Epoch 690, val loss: 0.950912356376648
Epoch 700, training loss: 0.6368123888969421 = 0.014162476174533367 + 0.1 * 6.226499080657959
Epoch 700, val loss: 0.9571954607963562
Epoch 710, training loss: 0.6364313960075378 = 0.013620576821267605 + 0.1 * 6.2281084060668945
Epoch 710, val loss: 0.9635260701179504
Epoch 720, training loss: 0.6356784701347351 = 0.013110395520925522 + 0.1 * 6.225680828094482
Epoch 720, val loss: 0.9694898724555969
Epoch 730, training loss: 0.6349136829376221 = 0.012630836106836796 + 0.1 * 6.222828388214111
Epoch 730, val loss: 0.9754913449287415
Epoch 740, training loss: 0.6341285705566406 = 0.01217881590127945 + 0.1 * 6.219497203826904
Epoch 740, val loss: 0.9814387559890747
Epoch 750, training loss: 0.633464515209198 = 0.011750895529985428 + 0.1 * 6.217136383056641
Epoch 750, val loss: 0.9871767163276672
Epoch 760, training loss: 0.6331720948219299 = 0.011346379294991493 + 0.1 * 6.218256950378418
Epoch 760, val loss: 0.9928267002105713
Epoch 770, training loss: 0.6341693997383118 = 0.010963060893118382 + 0.1 * 6.232062816619873
Epoch 770, val loss: 0.9983940124511719
Epoch 780, training loss: 0.6321948170661926 = 0.010600217618048191 + 0.1 * 6.215945720672607
Epoch 780, val loss: 1.0038301944732666
Epoch 790, training loss: 0.6330136656761169 = 0.010256445966660976 + 0.1 * 6.227571964263916
Epoch 790, val loss: 1.0091354846954346
Epoch 800, training loss: 0.630796492099762 = 0.00993118155747652 + 0.1 * 6.208652496337891
Epoch 800, val loss: 1.0143468379974365
Epoch 810, training loss: 0.6300430297851562 = 0.009622125886380672 + 0.1 * 6.204209327697754
Epoch 810, val loss: 1.0196503400802612
Epoch 820, training loss: 0.629823625087738 = 0.009327628649771214 + 0.1 * 6.204959869384766
Epoch 820, val loss: 1.0246562957763672
Epoch 830, training loss: 0.6294311285018921 = 0.009047704748809338 + 0.1 * 6.203834056854248
Epoch 830, val loss: 1.0296484231948853
Epoch 840, training loss: 0.6295424699783325 = 0.008780629374086857 + 0.1 * 6.20761775970459
Epoch 840, val loss: 1.0345796346664429
Epoch 850, training loss: 0.6284856200218201 = 0.008526136167347431 + 0.1 * 6.199594497680664
Epoch 850, val loss: 1.0393708944320679
Epoch 860, training loss: 0.6281879544258118 = 0.00828348658978939 + 0.1 * 6.199044704437256
Epoch 860, val loss: 1.044238567352295
Epoch 870, training loss: 0.6286091804504395 = 0.008051302284002304 + 0.1 * 6.205578804016113
Epoch 870, val loss: 1.048899531364441
Epoch 880, training loss: 0.6287112832069397 = 0.007830101065337658 + 0.1 * 6.2088117599487305
Epoch 880, val loss: 1.0534896850585938
Epoch 890, training loss: 0.627342939376831 = 0.007618661504238844 + 0.1 * 6.197242736816406
Epoch 890, val loss: 1.0579875707626343
Epoch 900, training loss: 0.6264362335205078 = 0.007417097222059965 + 0.1 * 6.190191268920898
Epoch 900, val loss: 1.062363862991333
Epoch 910, training loss: 0.6256875991821289 = 0.007224406581372023 + 0.1 * 6.184631824493408
Epoch 910, val loss: 1.066800832748413
Epoch 920, training loss: 0.6277489066123962 = 0.007039342541247606 + 0.1 * 6.207095146179199
Epoch 920, val loss: 1.0711185932159424
Epoch 930, training loss: 0.6253674626350403 = 0.0068615879863500595 + 0.1 * 6.185059070587158
Epoch 930, val loss: 1.0753358602523804
Epoch 940, training loss: 0.6249719262123108 = 0.0066911522299051285 + 0.1 * 6.182807445526123
Epoch 940, val loss: 1.0795342922210693
Epoch 950, training loss: 0.6255900859832764 = 0.006527296733111143 + 0.1 * 6.190627574920654
Epoch 950, val loss: 1.0837008953094482
Epoch 960, training loss: 0.6251679062843323 = 0.006370041985064745 + 0.1 * 6.187978267669678
Epoch 960, val loss: 1.0876127481460571
Epoch 970, training loss: 0.6243008971214294 = 0.006219499744474888 + 0.1 * 6.180813789367676
Epoch 970, val loss: 1.091673493385315
Epoch 980, training loss: 0.6247779130935669 = 0.0060746269300580025 + 0.1 * 6.187032699584961
Epoch 980, val loss: 1.0956300497055054
Epoch 990, training loss: 0.6240182518959045 = 0.0059354957193136215 + 0.1 * 6.180827617645264
Epoch 990, val loss: 1.099424123764038
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9336
Flip ASR: 0.9200/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.765842914581299 = 1.9284603595733643 + 0.1 * 8.373826026916504
Epoch 0, val loss: 1.9191458225250244
Epoch 10, training loss: 2.756420850753784 = 1.9190680980682373 + 0.1 * 8.373527526855469
Epoch 10, val loss: 1.9102699756622314
Epoch 20, training loss: 2.74444580078125 = 1.907288908958435 + 0.1 * 8.371567726135254
Epoch 20, val loss: 1.898748755455017
Epoch 30, training loss: 2.7262582778930664 = 1.8905547857284546 + 0.1 * 8.357034683227539
Epoch 30, val loss: 1.8819851875305176
Epoch 40, training loss: 2.690013885498047 = 1.865850567817688 + 0.1 * 8.241633415222168
Epoch 40, val loss: 1.857527494430542
Epoch 50, training loss: 2.6034886837005615 = 1.834733009338379 + 0.1 * 7.68755578994751
Epoch 50, val loss: 1.8283478021621704
Epoch 60, training loss: 2.524074077606201 = 1.80227530002594 + 0.1 * 7.217988967895508
Epoch 60, val loss: 1.797995686531067
Epoch 70, training loss: 2.4645886421203613 = 1.7658920288085938 + 0.1 * 6.986967086791992
Epoch 70, val loss: 1.764330506324768
Epoch 80, training loss: 2.4122474193573 = 1.7232310771942139 + 0.1 * 6.890163421630859
Epoch 80, val loss: 1.7262550592422485
Epoch 90, training loss: 2.3529956340789795 = 1.6704280376434326 + 0.1 * 6.825676441192627
Epoch 90, val loss: 1.6792455911636353
Epoch 100, training loss: 2.278587579727173 = 1.6009067296981812 + 0.1 * 6.776808261871338
Epoch 100, val loss: 1.61808180809021
Epoch 110, training loss: 2.1877143383026123 = 1.5143513679504395 + 0.1 * 6.733630180358887
Epoch 110, val loss: 1.5462849140167236
Epoch 120, training loss: 2.086277961730957 = 1.4161702394485474 + 0.1 * 6.701076984405518
Epoch 120, val loss: 1.4662315845489502
Epoch 130, training loss: 1.9798953533172607 = 1.3120814561843872 + 0.1 * 6.678138732910156
Epoch 130, val loss: 1.3833390474319458
Epoch 140, training loss: 1.8711845874786377 = 1.2049227952957153 + 0.1 * 6.6626176834106445
Epoch 140, val loss: 1.2994935512542725
Epoch 150, training loss: 1.7614030838012695 = 1.0968940258026123 + 0.1 * 6.645090103149414
Epoch 150, val loss: 1.21622633934021
Epoch 160, training loss: 1.6540789604187012 = 0.9910055994987488 + 0.1 * 6.630733013153076
Epoch 160, val loss: 1.1360889673233032
Epoch 170, training loss: 1.5561556816101074 = 0.894199788570404 + 0.1 * 6.619559288024902
Epoch 170, val loss: 1.065409541130066
Epoch 180, training loss: 1.4684311151504517 = 0.8079970479011536 + 0.1 * 6.604340553283691
Epoch 180, val loss: 1.0042935609817505
Epoch 190, training loss: 1.3908767700195312 = 0.7316542863845825 + 0.1 * 6.592225551605225
Epoch 190, val loss: 0.9530146718025208
Epoch 200, training loss: 1.325198769569397 = 0.6666091680526733 + 0.1 * 6.585896015167236
Epoch 200, val loss: 0.9125139713287354
Epoch 210, training loss: 1.2680561542510986 = 0.6113462448120117 + 0.1 * 6.567099571228027
Epoch 210, val loss: 0.8812929391860962
Epoch 220, training loss: 1.2188892364501953 = 0.563793957233429 + 0.1 * 6.550953388214111
Epoch 220, val loss: 0.8573494553565979
Epoch 230, training loss: 1.1759133338928223 = 0.5224469304084778 + 0.1 * 6.534664630889893
Epoch 230, val loss: 0.8385931253433228
Epoch 240, training loss: 1.140455722808838 = 0.4859473407268524 + 0.1 * 6.545083999633789
Epoch 240, val loss: 0.8242390155792236
Epoch 250, training loss: 1.1049069166183472 = 0.45354288816452026 + 0.1 * 6.5136399269104
Epoch 250, val loss: 0.8137319684028625
Epoch 260, training loss: 1.0725882053375244 = 0.4233787953853607 + 0.1 * 6.492093563079834
Epoch 260, val loss: 0.8056380748748779
Epoch 270, training loss: 1.0439823865890503 = 0.39424431324005127 + 0.1 * 6.49738073348999
Epoch 270, val loss: 0.7991008758544922
Epoch 280, training loss: 1.0142192840576172 = 0.3658950626850128 + 0.1 * 6.483242034912109
Epoch 280, val loss: 0.7938058972358704
Epoch 290, training loss: 0.9822503924369812 = 0.337735652923584 + 0.1 * 6.445147514343262
Epoch 290, val loss: 0.7894839644432068
Epoch 300, training loss: 0.9527379274368286 = 0.3095373213291168 + 0.1 * 6.432005882263184
Epoch 300, val loss: 0.7861951589584351
Epoch 310, training loss: 0.9257469177246094 = 0.2819700837135315 + 0.1 * 6.43776798248291
Epoch 310, val loss: 0.784241795539856
Epoch 320, training loss: 0.89796382188797 = 0.255715548992157 + 0.1 * 6.422482490539551
Epoch 320, val loss: 0.783480167388916
Epoch 330, training loss: 0.8721238970756531 = 0.23117728531360626 + 0.1 * 6.40946626663208
Epoch 330, val loss: 0.7842843532562256
Epoch 340, training loss: 0.851049542427063 = 0.20891276001930237 + 0.1 * 6.42136812210083
Epoch 340, val loss: 0.7863188982009888
Epoch 350, training loss: 0.8279052972793579 = 0.18929114937782288 + 0.1 * 6.386141777038574
Epoch 350, val loss: 0.789806067943573
Epoch 360, training loss: 0.8091335296630859 = 0.1721043884754181 + 0.1 * 6.370290756225586
Epoch 360, val loss: 0.7944746017456055
Epoch 370, training loss: 0.7952713966369629 = 0.15702387690544128 + 0.1 * 6.382474899291992
Epoch 370, val loss: 0.799980103969574
Epoch 380, training loss: 0.7809333205223083 = 0.14377468824386597 + 0.1 * 6.371586322784424
Epoch 380, val loss: 0.806171178817749
Epoch 390, training loss: 0.7674680948257446 = 0.13209837675094604 + 0.1 * 6.353697299957275
Epoch 390, val loss: 0.812956690788269
Epoch 400, training loss: 0.7558156251907349 = 0.12168733030557632 + 0.1 * 6.341282844543457
Epoch 400, val loss: 0.8201582431793213
Epoch 410, training loss: 0.7461774349212646 = 0.11235403269529343 + 0.1 * 6.338233947753906
Epoch 410, val loss: 0.8276532888412476
Epoch 420, training loss: 0.7395114302635193 = 0.10392867028713226 + 0.1 * 6.355827331542969
Epoch 420, val loss: 0.8353221416473389
Epoch 430, training loss: 0.729887068271637 = 0.09630758315324783 + 0.1 * 6.335794925689697
Epoch 430, val loss: 0.8430700898170471
Epoch 440, training loss: 0.7212914824485779 = 0.08935695141553879 + 0.1 * 6.319344997406006
Epoch 440, val loss: 0.8508095145225525
Epoch 450, training loss: 0.7165608406066895 = 0.0829736590385437 + 0.1 * 6.335871696472168
Epoch 450, val loss: 0.8585230112075806
Epoch 460, training loss: 0.7084230184555054 = 0.07709217816591263 + 0.1 * 6.313308238983154
Epoch 460, val loss: 0.8662571310997009
Epoch 470, training loss: 0.7041924595832825 = 0.0716516301035881 + 0.1 * 6.325407981872559
Epoch 470, val loss: 0.8738546371459961
Epoch 480, training loss: 0.696284830570221 = 0.06653466075658798 + 0.1 * 6.297502040863037
Epoch 480, val loss: 0.8812756538391113
Epoch 490, training loss: 0.6911342144012451 = 0.06162375211715698 + 0.1 * 6.295104503631592
Epoch 490, val loss: 0.8885931372642517
Epoch 500, training loss: 0.6859002113342285 = 0.05687195062637329 + 0.1 * 6.290282726287842
Epoch 500, val loss: 0.8957969546318054
Epoch 510, training loss: 0.6835722327232361 = 0.05227788910269737 + 0.1 * 6.312943458557129
Epoch 510, val loss: 0.9029869437217712
Epoch 520, training loss: 0.6760358810424805 = 0.047837771475315094 + 0.1 * 6.281980991363525
Epoch 520, val loss: 0.9105093479156494
Epoch 530, training loss: 0.6728641986846924 = 0.0437348373234272 + 0.1 * 6.291293144226074
Epoch 530, val loss: 0.9184551239013672
Epoch 540, training loss: 0.6673738360404968 = 0.040168263018131256 + 0.1 * 6.272055149078369
Epoch 540, val loss: 0.926888644695282
Epoch 550, training loss: 0.6641321182250977 = 0.03714798390865326 + 0.1 * 6.269841194152832
Epoch 550, val loss: 0.9360069036483765
Epoch 560, training loss: 0.6614193916320801 = 0.03455669805407524 + 0.1 * 6.268626689910889
Epoch 560, val loss: 0.945483922958374
Epoch 570, training loss: 0.6591445803642273 = 0.032284755259752274 + 0.1 * 6.2685980796813965
Epoch 570, val loss: 0.9551247954368591
Epoch 580, training loss: 0.6563243865966797 = 0.030266793444752693 + 0.1 * 6.260575294494629
Epoch 580, val loss: 0.9646281003952026
Epoch 590, training loss: 0.6544533967971802 = 0.02845292165875435 + 0.1 * 6.26000452041626
Epoch 590, val loss: 0.9740902185440063
Epoch 600, training loss: 0.6530992388725281 = 0.026808584108948708 + 0.1 * 6.262906551361084
Epoch 600, val loss: 0.9832289814949036
Epoch 610, training loss: 0.6503880620002747 = 0.025312084704637527 + 0.1 * 6.250760078430176
Epoch 610, val loss: 0.9922769069671631
Epoch 620, training loss: 0.64985191822052 = 0.023942861706018448 + 0.1 * 6.259089946746826
Epoch 620, val loss: 1.001190185546875
Epoch 630, training loss: 0.647374153137207 = 0.02268778532743454 + 0.1 * 6.24686336517334
Epoch 630, val loss: 1.0099507570266724
Epoch 640, training loss: 0.644943356513977 = 0.021535109728574753 + 0.1 * 6.2340826988220215
Epoch 640, val loss: 1.018600344657898
Epoch 650, training loss: 0.6456460356712341 = 0.02046944946050644 + 0.1 * 6.251766204833984
Epoch 650, val loss: 1.0270389318466187
Epoch 660, training loss: 0.6430254578590393 = 0.019485918805003166 + 0.1 * 6.235395431518555
Epoch 660, val loss: 1.0352530479431152
Epoch 670, training loss: 0.6434435844421387 = 0.018574446439743042 + 0.1 * 6.248691082000732
Epoch 670, val loss: 1.043396234512329
Epoch 680, training loss: 0.6405963897705078 = 0.017730537801980972 + 0.1 * 6.228658676147461
Epoch 680, val loss: 1.051276683807373
Epoch 690, training loss: 0.6386861801147461 = 0.01694565638899803 + 0.1 * 6.217405319213867
Epoch 690, val loss: 1.05915367603302
Epoch 700, training loss: 0.6394756436347961 = 0.016212191432714462 + 0.1 * 6.232634544372559
Epoch 700, val loss: 1.0668888092041016
Epoch 710, training loss: 0.6381202340126038 = 0.01552809588611126 + 0.1 * 6.225921154022217
Epoch 710, val loss: 1.0742592811584473
Epoch 720, training loss: 0.6375767588615417 = 0.01489061675965786 + 0.1 * 6.226861476898193
Epoch 720, val loss: 1.0816700458526611
Epoch 730, training loss: 0.6375060677528381 = 0.014294842258095741 + 0.1 * 6.232111930847168
Epoch 730, val loss: 1.088934063911438
Epoch 740, training loss: 0.6348369717597961 = 0.013737483881413937 + 0.1 * 6.210994720458984
Epoch 740, val loss: 1.0959612131118774
Epoch 750, training loss: 0.6346542239189148 = 0.013213167898356915 + 0.1 * 6.214410781860352
Epoch 750, val loss: 1.1029142141342163
Epoch 760, training loss: 0.6337218880653381 = 0.012719040736556053 + 0.1 * 6.210028648376465
Epoch 760, val loss: 1.1097291707992554
Epoch 770, training loss: 0.632927656173706 = 0.012254010885953903 + 0.1 * 6.2067365646362305
Epoch 770, val loss: 1.1163650751113892
Epoch 780, training loss: 0.6327294111251831 = 0.011816488578915596 + 0.1 * 6.2091288566589355
Epoch 780, val loss: 1.122809648513794
Epoch 790, training loss: 0.6313881278038025 = 0.011404060758650303 + 0.1 * 6.199840545654297
Epoch 790, val loss: 1.1292901039123535
Epoch 800, training loss: 0.6317339539527893 = 0.011013277806341648 + 0.1 * 6.207206726074219
Epoch 800, val loss: 1.1356446743011475
Epoch 810, training loss: 0.6304090023040771 = 0.010643325746059418 + 0.1 * 6.197656631469727
Epoch 810, val loss: 1.1417393684387207
Epoch 820, training loss: 0.6318500638008118 = 0.010293110273778439 + 0.1 * 6.215569496154785
Epoch 820, val loss: 1.1477580070495605
Epoch 830, training loss: 0.6305536031723022 = 0.00996239110827446 + 0.1 * 6.205912113189697
Epoch 830, val loss: 1.1536548137664795
Epoch 840, training loss: 0.6292901635169983 = 0.009649477899074554 + 0.1 * 6.196406841278076
Epoch 840, val loss: 1.1595432758331299
Epoch 850, training loss: 0.6282655000686646 = 0.009351849555969238 + 0.1 * 6.189136505126953
Epoch 850, val loss: 1.1652837991714478
Epoch 860, training loss: 0.6283934712409973 = 0.009067981503903866 + 0.1 * 6.1932549476623535
Epoch 860, val loss: 1.1708625555038452
Epoch 870, training loss: 0.627282977104187 = 0.00879812240600586 + 0.1 * 6.184848308563232
Epoch 870, val loss: 1.1763070821762085
Epoch 880, training loss: 0.6271787881851196 = 0.008541257120668888 + 0.1 * 6.186375141143799
Epoch 880, val loss: 1.181735873222351
Epoch 890, training loss: 0.6268841028213501 = 0.008296364918351173 + 0.1 * 6.185877323150635
Epoch 890, val loss: 1.1870918273925781
Epoch 900, training loss: 0.6265609860420227 = 0.0080627566203475 + 0.1 * 6.184981822967529
Epoch 900, val loss: 1.1923491954803467
Epoch 910, training loss: 0.6269404888153076 = 0.007839440368115902 + 0.1 * 6.191010475158691
Epoch 910, val loss: 1.1975212097167969
Epoch 920, training loss: 0.6255497336387634 = 0.007626776117831469 + 0.1 * 6.179229259490967
Epoch 920, val loss: 1.2024775743484497
Epoch 930, training loss: 0.6250960230827332 = 0.007423842791467905 + 0.1 * 6.176722049713135
Epoch 930, val loss: 1.2075018882751465
Epoch 940, training loss: 0.6264023780822754 = 0.007229179609566927 + 0.1 * 6.1917314529418945
Epoch 940, val loss: 1.2124340534210205
Epoch 950, training loss: 0.6243892908096313 = 0.007042984012514353 + 0.1 * 6.173462867736816
Epoch 950, val loss: 1.2172220945358276
Epoch 960, training loss: 0.624292254447937 = 0.006864702794700861 + 0.1 * 6.174274921417236
Epoch 960, val loss: 1.2220053672790527
Epoch 970, training loss: 0.6243812441825867 = 0.006693383678793907 + 0.1 * 6.176878929138184
Epoch 970, val loss: 1.2266933917999268
Epoch 980, training loss: 0.6228885054588318 = 0.006529225502163172 + 0.1 * 6.16359281539917
Epoch 980, val loss: 1.2312560081481934
Epoch 990, training loss: 0.6244044899940491 = 0.006371861323714256 + 0.1 * 6.180326461791992
Epoch 990, val loss: 1.2358323335647583
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.7159
Flip ASR: 0.6756/225 nodes
The final ASR:0.79459, 0.09857, Accuracy:0.80617, 0.01259
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11652])
remove edge: torch.Size([2, 9522])
updated graph: torch.Size([2, 10618])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7932982444763184 = 1.9559075832366943 + 0.1 * 8.373907089233398
Epoch 0, val loss: 1.9575159549713135
Epoch 10, training loss: 2.7834181785583496 = 1.9460370540618896 + 0.1 * 8.373809814453125
Epoch 10, val loss: 1.9480507373809814
Epoch 20, training loss: 2.771014451980591 = 1.9336872100830078 + 0.1 * 8.373271942138672
Epoch 20, val loss: 1.9357528686523438
Epoch 30, training loss: 2.752924919128418 = 1.9160023927688599 + 0.1 * 8.369226455688477
Epoch 30, val loss: 1.9177130460739136
Epoch 40, training loss: 2.7234585285186768 = 1.889259696006775 + 0.1 * 8.341988563537598
Epoch 40, val loss: 1.8903882503509521
Epoch 50, training loss: 2.6702120304107666 = 1.851730465888977 + 0.1 * 8.184815406799316
Epoch 50, val loss: 1.8537545204162598
Epoch 60, training loss: 2.600550889968872 = 1.8098665475845337 + 0.1 * 7.9068427085876465
Epoch 60, val loss: 1.8164552450180054
Epoch 70, training loss: 2.5221269130706787 = 1.7712026834487915 + 0.1 * 7.509241580963135
Epoch 70, val loss: 1.7845109701156616
Epoch 80, training loss: 2.442964792251587 = 1.7310727834701538 + 0.1 * 7.11892032623291
Epoch 80, val loss: 1.750780463218689
Epoch 90, training loss: 2.373594284057617 = 1.680822491645813 + 0.1 * 6.927718639373779
Epoch 90, val loss: 1.7078444957733154
Epoch 100, training loss: 2.2992842197418213 = 1.614576816558838 + 0.1 * 6.847074508666992
Epoch 100, val loss: 1.6510143280029297
Epoch 110, training loss: 2.2115330696105957 = 1.532293438911438 + 0.1 * 6.7923970222473145
Epoch 110, val loss: 1.58175790309906
Epoch 120, training loss: 2.1150319576263428 = 1.439370036125183 + 0.1 * 6.756619930267334
Epoch 120, val loss: 1.5066906213760376
Epoch 130, training loss: 2.015861749649048 = 1.3427993059158325 + 0.1 * 6.730623722076416
Epoch 130, val loss: 1.431601643562317
Epoch 140, training loss: 1.917231559753418 = 1.2464599609375 + 0.1 * 6.7077155113220215
Epoch 140, val loss: 1.3596535921096802
Epoch 150, training loss: 1.820979118347168 = 1.1522490978240967 + 0.1 * 6.687299728393555
Epoch 150, val loss: 1.2910206317901611
Epoch 160, training loss: 1.7310504913330078 = 1.0636132955551147 + 0.1 * 6.674371242523193
Epoch 160, val loss: 1.2278220653533936
Epoch 170, training loss: 1.6454296112060547 = 0.9794155955314636 + 0.1 * 6.660140037536621
Epoch 170, val loss: 1.1676474809646606
Epoch 180, training loss: 1.563623070716858 = 0.898850679397583 + 0.1 * 6.64772367477417
Epoch 180, val loss: 1.1104975938796997
Epoch 190, training loss: 1.4853419065475464 = 0.821560800075531 + 0.1 * 6.637811183929443
Epoch 190, val loss: 1.055136799812317
Epoch 200, training loss: 1.4097681045532227 = 0.7469840049743652 + 0.1 * 6.627840518951416
Epoch 200, val loss: 1.001643419265747
Epoch 210, training loss: 1.33626389503479 = 0.6745511889457703 + 0.1 * 6.617126941680908
Epoch 210, val loss: 0.9495866298675537
Epoch 220, training loss: 1.2665200233459473 = 0.6058191061019897 + 0.1 * 6.607009410858154
Epoch 220, val loss: 0.9007200002670288
Epoch 230, training loss: 1.2018277645111084 = 0.5422068238258362 + 0.1 * 6.596209526062012
Epoch 230, val loss: 0.8569647669792175
Epoch 240, training loss: 1.1426876783370972 = 0.48389503359794617 + 0.1 * 6.587926387786865
Epoch 240, val loss: 0.8188175559043884
Epoch 250, training loss: 1.0891168117523193 = 0.43124088644981384 + 0.1 * 6.578758716583252
Epoch 250, val loss: 0.7868003845214844
Epoch 260, training loss: 1.040381908416748 = 0.3834247589111328 + 0.1 * 6.5695719718933105
Epoch 260, val loss: 0.7607589960098267
Epoch 270, training loss: 0.9960358142852783 = 0.3396399915218353 + 0.1 * 6.563957691192627
Epoch 270, val loss: 0.7395076155662537
Epoch 280, training loss: 0.9552047252655029 = 0.29995623230934143 + 0.1 * 6.552484512329102
Epoch 280, val loss: 0.7231001257896423
Epoch 290, training loss: 0.9192407131195068 = 0.2643299400806427 + 0.1 * 6.549108028411865
Epoch 290, val loss: 0.7109624147415161
Epoch 300, training loss: 0.8862233757972717 = 0.2331390380859375 + 0.1 * 6.530843257904053
Epoch 300, val loss: 0.7030153274536133
Epoch 310, training loss: 0.858921468257904 = 0.2061464786529541 + 0.1 * 6.527749538421631
Epoch 310, val loss: 0.6988430619239807
Epoch 320, training loss: 0.83551025390625 = 0.1831226348876953 + 0.1 * 6.523876190185547
Epoch 320, val loss: 0.6978501081466675
Epoch 330, training loss: 0.813968300819397 = 0.16354967653751373 + 0.1 * 6.504186153411865
Epoch 330, val loss: 0.6995331048965454
Epoch 340, training loss: 0.7975587248802185 = 0.14678968489170074 + 0.1 * 6.5076904296875
Epoch 340, val loss: 0.7033814787864685
Epoch 350, training loss: 0.781039297580719 = 0.13244348764419556 + 0.1 * 6.485958099365234
Epoch 350, val loss: 0.7088105082511902
Epoch 360, training loss: 0.7686967253684998 = 0.1201172024011612 + 0.1 * 6.485795497894287
Epoch 360, val loss: 0.7154729962348938
Epoch 370, training loss: 0.7565374374389648 = 0.10942649096250534 + 0.1 * 6.471109390258789
Epoch 370, val loss: 0.7229648232460022
Epoch 380, training loss: 0.7462841868400574 = 0.10001255571842194 + 0.1 * 6.462716579437256
Epoch 380, val loss: 0.7312952876091003
Epoch 390, training loss: 0.7385870218276978 = 0.09164401888847351 + 0.1 * 6.469430446624756
Epoch 390, val loss: 0.740374743938446
Epoch 400, training loss: 0.7292588949203491 = 0.08423932641744614 + 0.1 * 6.450195789337158
Epoch 400, val loss: 0.7496424317359924
Epoch 410, training loss: 0.7216602563858032 = 0.07763085514307022 + 0.1 * 6.440293788909912
Epoch 410, val loss: 0.759194016456604
Epoch 420, training loss: 0.7148321866989136 = 0.07168520987033844 + 0.1 * 6.431469917297363
Epoch 420, val loss: 0.7690020203590393
Epoch 430, training loss: 0.7092227339744568 = 0.0663352832198143 + 0.1 * 6.428874492645264
Epoch 430, val loss: 0.7789044380187988
Epoch 440, training loss: 0.7032565474510193 = 0.06152418628334999 + 0.1 * 6.417323112487793
Epoch 440, val loss: 0.7888336181640625
Epoch 450, training loss: 0.6995088458061218 = 0.05716325342655182 + 0.1 * 6.423455715179443
Epoch 450, val loss: 0.7987963557243347
Epoch 460, training loss: 0.6948973536491394 = 0.053225088864564896 + 0.1 * 6.416722297668457
Epoch 460, val loss: 0.8086948990821838
Epoch 470, training loss: 0.6898699998855591 = 0.049650538712739944 + 0.1 * 6.402194499969482
Epoch 470, val loss: 0.8185091614723206
Epoch 480, training loss: 0.6862044334411621 = 0.046386994421482086 + 0.1 * 6.398173809051514
Epoch 480, val loss: 0.8283174633979797
Epoch 490, training loss: 0.6821673512458801 = 0.04341185837984085 + 0.1 * 6.38755464553833
Epoch 490, val loss: 0.8380938768386841
Epoch 500, training loss: 0.6795426607131958 = 0.040700603276491165 + 0.1 * 6.388420104980469
Epoch 500, val loss: 0.8476834297180176
Epoch 510, training loss: 0.6778414845466614 = 0.03821602091193199 + 0.1 * 6.396254539489746
Epoch 510, val loss: 0.8571922183036804
Epoch 520, training loss: 0.6743007898330688 = 0.03593966364860535 + 0.1 * 6.383610725402832
Epoch 520, val loss: 0.8665958046913147
Epoch 530, training loss: 0.670695960521698 = 0.03385318070650101 + 0.1 * 6.368427753448486
Epoch 530, val loss: 0.8758369088172913
Epoch 540, training loss: 0.6691321730613708 = 0.03193488344550133 + 0.1 * 6.371973037719727
Epoch 540, val loss: 0.8849784135818481
Epoch 550, training loss: 0.6660128831863403 = 0.03016703389585018 + 0.1 * 6.358458518981934
Epoch 550, val loss: 0.8940153121948242
Epoch 560, training loss: 0.6659553050994873 = 0.028534559532999992 + 0.1 * 6.374207496643066
Epoch 560, val loss: 0.9028846621513367
Epoch 570, training loss: 0.6631626486778259 = 0.027031220495700836 + 0.1 * 6.361313819885254
Epoch 570, val loss: 0.9116203188896179
Epoch 580, training loss: 0.6604818105697632 = 0.02564314939081669 + 0.1 * 6.348386287689209
Epoch 580, val loss: 0.9202088713645935
Epoch 590, training loss: 0.6596555709838867 = 0.024355603381991386 + 0.1 * 6.352999687194824
Epoch 590, val loss: 0.9286587238311768
Epoch 600, training loss: 0.6581128835678101 = 0.02316123992204666 + 0.1 * 6.34951639175415
Epoch 600, val loss: 0.9370567202568054
Epoch 610, training loss: 0.6557670831680298 = 0.022052401676774025 + 0.1 * 6.337146759033203
Epoch 610, val loss: 0.9451537728309631
Epoch 620, training loss: 0.6554834246635437 = 0.021020568907260895 + 0.1 * 6.34462833404541
Epoch 620, val loss: 0.9532193541526794
Epoch 630, training loss: 0.6537148356437683 = 0.020060105249285698 + 0.1 * 6.336546897888184
Epoch 630, val loss: 0.9610627293586731
Epoch 640, training loss: 0.6529698967933655 = 0.019165940582752228 + 0.1 * 6.338039398193359
Epoch 640, val loss: 0.9688067436218262
Epoch 650, training loss: 0.6503667235374451 = 0.018331380560994148 + 0.1 * 6.320353031158447
Epoch 650, val loss: 0.9763631224632263
Epoch 660, training loss: 0.6507002115249634 = 0.017549213021993637 + 0.1 * 6.331510066986084
Epoch 660, val loss: 0.983866810798645
Epoch 670, training loss: 0.6489729881286621 = 0.01681780256330967 + 0.1 * 6.32155179977417
Epoch 670, val loss: 0.9911558032035828
Epoch 680, training loss: 0.6496684551239014 = 0.016132883727550507 + 0.1 * 6.335355758666992
Epoch 680, val loss: 0.9983824491500854
Epoch 690, training loss: 0.6466786861419678 = 0.015490388497710228 + 0.1 * 6.311882495880127
Epoch 690, val loss: 1.0053478479385376
Epoch 700, training loss: 0.6449264287948608 = 0.014887566678225994 + 0.1 * 6.300388336181641
Epoch 700, val loss: 1.0123140811920166
Epoch 710, training loss: 0.6446225047111511 = 0.014317885041236877 + 0.1 * 6.303046226501465
Epoch 710, val loss: 1.0191607475280762
Epoch 720, training loss: 0.6459923386573792 = 0.013780934736132622 + 0.1 * 6.322113990783691
Epoch 720, val loss: 1.025743842124939
Epoch 730, training loss: 0.6430268883705139 = 0.01327710971236229 + 0.1 * 6.297497749328613
Epoch 730, val loss: 1.0323494672775269
Epoch 740, training loss: 0.6418612003326416 = 0.012802652083337307 + 0.1 * 6.290585517883301
Epoch 740, val loss: 1.0388118028640747
Epoch 750, training loss: 0.6432997584342957 = 0.012353191152215004 + 0.1 * 6.3094658851623535
Epoch 750, val loss: 1.0450654029846191
Epoch 760, training loss: 0.6410819292068481 = 0.011929349042475224 + 0.1 * 6.291525840759277
Epoch 760, val loss: 1.0512853860855103
Epoch 770, training loss: 0.6402463912963867 = 0.011527132242918015 + 0.1 * 6.287192344665527
Epoch 770, val loss: 1.0574110746383667
Epoch 780, training loss: 0.6417890787124634 = 0.011145385913550854 + 0.1 * 6.306436538696289
Epoch 780, val loss: 1.0633721351623535
Epoch 790, training loss: 0.6389676928520203 = 0.010784030891954899 + 0.1 * 6.28183650970459
Epoch 790, val loss: 1.0692403316497803
Epoch 800, training loss: 0.6381271481513977 = 0.01044115237891674 + 0.1 * 6.276860237121582
Epoch 800, val loss: 1.0750956535339355
Epoch 810, training loss: 0.638401448726654 = 0.010114973410964012 + 0.1 * 6.282864570617676
Epoch 810, val loss: 1.0807591676712036
Epoch 820, training loss: 0.6385570168495178 = 0.009804919362068176 + 0.1 * 6.287520885467529
Epoch 820, val loss: 1.086269736289978
Epoch 830, training loss: 0.6361435651779175 = 0.009510758332908154 + 0.1 * 6.266327857971191
Epoch 830, val loss: 1.0918374061584473
Epoch 840, training loss: 0.6356624364852905 = 0.009231256321072578 + 0.1 * 6.264311790466309
Epoch 840, val loss: 1.0972312688827515
Epoch 850, training loss: 0.6361854672431946 = 0.008963393978774548 + 0.1 * 6.272220611572266
Epoch 850, val loss: 1.1025679111480713
Epoch 860, training loss: 0.6355447173118591 = 0.008708844892680645 + 0.1 * 6.2683587074279785
Epoch 860, val loss: 1.107771873474121
Epoch 870, training loss: 0.6341187357902527 = 0.00846573431044817 + 0.1 * 6.256530284881592
Epoch 870, val loss: 1.11294424533844
Epoch 880, training loss: 0.6338799595832825 = 0.008234020322561264 + 0.1 * 6.2564592361450195
Epoch 880, val loss: 1.1180121898651123
Epoch 890, training loss: 0.6342566013336182 = 0.008011630736291409 + 0.1 * 6.262449741363525
Epoch 890, val loss: 1.1230113506317139
Epoch 900, training loss: 0.6331509351730347 = 0.007799337152391672 + 0.1 * 6.253515720367432
Epoch 900, val loss: 1.1278722286224365
Epoch 910, training loss: 0.632778525352478 = 0.007595976814627647 + 0.1 * 6.25182580947876
Epoch 910, val loss: 1.132784366607666
Epoch 920, training loss: 0.6335161924362183 = 0.0074007390066981316 + 0.1 * 6.2611541748046875
Epoch 920, val loss: 1.1375086307525635
Epoch 930, training loss: 0.6327502727508545 = 0.007214563898742199 + 0.1 * 6.255357265472412
Epoch 930, val loss: 1.1421297788619995
Epoch 940, training loss: 0.6311743259429932 = 0.0070358035154640675 + 0.1 * 6.241384983062744
Epoch 940, val loss: 1.1467914581298828
Epoch 950, training loss: 0.6307739019393921 = 0.0068647549487650394 + 0.1 * 6.239091873168945
Epoch 950, val loss: 1.1513205766677856
Epoch 960, training loss: 0.6310818195343018 = 0.006699450314044952 + 0.1 * 6.243823528289795
Epoch 960, val loss: 1.1557754278182983
Epoch 970, training loss: 0.6308966875076294 = 0.006541326176375151 + 0.1 * 6.243553638458252
Epoch 970, val loss: 1.1601793766021729
Epoch 980, training loss: 0.6298918724060059 = 0.006388470996171236 + 0.1 * 6.2350335121154785
Epoch 980, val loss: 1.1644659042358398
Epoch 990, training loss: 0.6302938461303711 = 0.006242799572646618 + 0.1 * 6.240509986877441
Epoch 990, val loss: 1.1687462329864502
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6937
Flip ASR: 0.6400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7952609062194824 = 1.9578745365142822 + 0.1 * 8.37386417388916
Epoch 0, val loss: 1.9567204713821411
Epoch 10, training loss: 2.7853026390075684 = 1.9479464292526245 + 0.1 * 8.373560905456543
Epoch 10, val loss: 1.9455912113189697
Epoch 20, training loss: 2.773285388946533 = 1.936128854751587 + 0.1 * 8.371565818786621
Epoch 20, val loss: 1.9314367771148682
Epoch 30, training loss: 2.756511688232422 = 1.9199532270431519 + 0.1 * 8.365583419799805
Epoch 30, val loss: 1.9113317728042603
Epoch 40, training loss: 2.7310473918914795 = 1.8968546390533447 + 0.1 * 8.341927528381348
Epoch 40, val loss: 1.8828766345977783
Epoch 50, training loss: 2.6855697631835938 = 1.8650329113006592 + 0.1 * 8.205368995666504
Epoch 50, val loss: 1.8458322286605835
Epoch 60, training loss: 2.6055524349212646 = 1.827424168586731 + 0.1 * 7.781283378601074
Epoch 60, val loss: 1.805482029914856
Epoch 70, training loss: 2.5356035232543945 = 1.7862701416015625 + 0.1 * 7.4933342933654785
Epoch 70, val loss: 1.7653199434280396
Epoch 80, training loss: 2.4773104190826416 = 1.7429616451263428 + 0.1 * 7.343486785888672
Epoch 80, val loss: 1.726682186126709
Epoch 90, training loss: 2.4131360054016113 = 1.6973567008972168 + 0.1 * 7.157792568206787
Epoch 90, val loss: 1.6882503032684326
Epoch 100, training loss: 2.3372504711151123 = 1.6400123834609985 + 0.1 * 6.972381114959717
Epoch 100, val loss: 1.6403497457504272
Epoch 110, training loss: 2.2484965324401855 = 1.5633230209350586 + 0.1 * 6.851733684539795
Epoch 110, val loss: 1.5768381357192993
Epoch 120, training loss: 2.144860029220581 = 1.4664580821990967 + 0.1 * 6.784019470214844
Epoch 120, val loss: 1.497879147529602
Epoch 130, training loss: 2.036041498184204 = 1.361682653427124 + 0.1 * 6.743587970733643
Epoch 130, val loss: 1.416002631187439
Epoch 140, training loss: 1.9346234798431396 = 1.2632964849472046 + 0.1 * 6.7132697105407715
Epoch 140, val loss: 1.343108892440796
Epoch 150, training loss: 1.8461003303527832 = 1.1768341064453125 + 0.1 * 6.692661285400391
Epoch 150, val loss: 1.2816259860992432
Epoch 160, training loss: 1.7681703567504883 = 1.1002731323242188 + 0.1 * 6.678971767425537
Epoch 160, val loss: 1.2270203828811646
Epoch 170, training loss: 1.6944546699523926 = 1.0277758836746216 + 0.1 * 6.666787147521973
Epoch 170, val loss: 1.1737635135650635
Epoch 180, training loss: 1.6217286586761475 = 0.9562204480171204 + 0.1 * 6.655082702636719
Epoch 180, val loss: 1.1199595928192139
Epoch 190, training loss: 1.5505726337432861 = 0.8853060007095337 + 0.1 * 6.652665615081787
Epoch 190, val loss: 1.066123604774475
Epoch 200, training loss: 1.4797265529632568 = 0.8159149289131165 + 0.1 * 6.638116359710693
Epoch 200, val loss: 1.0132535696029663
Epoch 210, training loss: 1.409717321395874 = 0.7468847036361694 + 0.1 * 6.628325462341309
Epoch 210, val loss: 0.9611678719520569
Epoch 220, training loss: 1.3396785259246826 = 0.6779667735099792 + 0.1 * 6.617116928100586
Epoch 220, val loss: 0.9099662899971008
Epoch 230, training loss: 1.2716381549835205 = 0.6096136569976807 + 0.1 * 6.620244026184082
Epoch 230, val loss: 0.8605602979660034
Epoch 240, training loss: 1.2037217617034912 = 0.5436822175979614 + 0.1 * 6.600395679473877
Epoch 240, val loss: 0.8150277137756348
Epoch 250, training loss: 1.1391733884811401 = 0.4807535707950592 + 0.1 * 6.584197998046875
Epoch 250, val loss: 0.7748448848724365
Epoch 260, training loss: 1.0790889263153076 = 0.4215702712535858 + 0.1 * 6.575186729431152
Epoch 260, val loss: 0.7410753965377808
Epoch 270, training loss: 1.0238276720046997 = 0.36788615584373474 + 0.1 * 6.559415340423584
Epoch 270, val loss: 0.715229868888855
Epoch 280, training loss: 0.9754220843315125 = 0.3203399181365967 + 0.1 * 6.550821781158447
Epoch 280, val loss: 0.6959267854690552
Epoch 290, training loss: 0.933315098285675 = 0.2788471579551697 + 0.1 * 6.544679164886475
Epoch 290, val loss: 0.6816869378089905
Epoch 300, training loss: 0.8975715637207031 = 0.2434615194797516 + 0.1 * 6.541100025177002
Epoch 300, val loss: 0.6715211272239685
Epoch 310, training loss: 0.8651493787765503 = 0.2131926268339157 + 0.1 * 6.519567012786865
Epoch 310, val loss: 0.6657934784889221
Epoch 320, training loss: 0.8384351134300232 = 0.18737484514713287 + 0.1 * 6.510602951049805
Epoch 320, val loss: 0.6641199588775635
Epoch 330, training loss: 0.8170626163482666 = 0.1654241532087326 + 0.1 * 6.516384601593018
Epoch 330, val loss: 0.6658639311790466
Epoch 340, training loss: 0.79660964012146 = 0.14679034054279327 + 0.1 * 6.49819278717041
Epoch 340, val loss: 0.6701135635375977
Epoch 350, training loss: 0.7808784246444702 = 0.13088811933994293 + 0.1 * 6.499903202056885
Epoch 350, val loss: 0.6759428977966309
Epoch 360, training loss: 0.7654718160629272 = 0.11728904396295547 + 0.1 * 6.481827259063721
Epoch 360, val loss: 0.6830360889434814
Epoch 370, training loss: 0.7531771063804626 = 0.10548568516969681 + 0.1 * 6.476914405822754
Epoch 370, val loss: 0.6911332011222839
Epoch 380, training loss: 0.7424046397209167 = 0.09519576281309128 + 0.1 * 6.47208833694458
Epoch 380, val loss: 0.6996138691902161
Epoch 390, training loss: 0.7319387793540955 = 0.08622455596923828 + 0.1 * 6.457141876220703
Epoch 390, val loss: 0.708318293094635
Epoch 400, training loss: 0.7243291735649109 = 0.07838070392608643 + 0.1 * 6.459484577178955
Epoch 400, val loss: 0.7170798778533936
Epoch 410, training loss: 0.7159324288368225 = 0.07153875380754471 + 0.1 * 6.443936824798584
Epoch 410, val loss: 0.7258262038230896
Epoch 420, training loss: 0.7091588377952576 = 0.0655006542801857 + 0.1 * 6.436581611633301
Epoch 420, val loss: 0.7346059679985046
Epoch 430, training loss: 0.7067040205001831 = 0.06013759225606918 + 0.1 * 6.465664386749268
Epoch 430, val loss: 0.7433332204818726
Epoch 440, training loss: 0.6978148221969604 = 0.05538485571742058 + 0.1 * 6.424299240112305
Epoch 440, val loss: 0.7519347071647644
Epoch 450, training loss: 0.693244218826294 = 0.05114205926656723 + 0.1 * 6.421021938323975
Epoch 450, val loss: 0.760632336139679
Epoch 460, training loss: 0.6895501613616943 = 0.04734966158866882 + 0.1 * 6.422004699707031
Epoch 460, val loss: 0.7691762447357178
Epoch 470, training loss: 0.6845700144767761 = 0.043948058038949966 + 0.1 * 6.406219482421875
Epoch 470, val loss: 0.7775861620903015
Epoch 480, training loss: 0.6815032362937927 = 0.04087899252772331 + 0.1 * 6.406242370605469
Epoch 480, val loss: 0.7859849333763123
Epoch 490, training loss: 0.6771661639213562 = 0.03811969235539436 + 0.1 * 6.390464782714844
Epoch 490, val loss: 0.7942890524864197
Epoch 500, training loss: 0.6753010153770447 = 0.035644788295030594 + 0.1 * 6.396562576293945
Epoch 500, val loss: 0.8022379279136658
Epoch 510, training loss: 0.671305775642395 = 0.03340141102671623 + 0.1 * 6.379043102264404
Epoch 510, val loss: 0.8102473616600037
Epoch 520, training loss: 0.6683963537216187 = 0.03135053813457489 + 0.1 * 6.370458126068115
Epoch 520, val loss: 0.8181737661361694
Epoch 530, training loss: 0.668533444404602 = 0.02947179228067398 + 0.1 * 6.390615940093994
Epoch 530, val loss: 0.8260177373886108
Epoch 540, training loss: 0.6651802062988281 = 0.027758166193962097 + 0.1 * 6.374220371246338
Epoch 540, val loss: 0.8337128758430481
Epoch 550, training loss: 0.6618056297302246 = 0.02619078941643238 + 0.1 * 6.3561482429504395
Epoch 550, val loss: 0.8412137627601624
Epoch 560, training loss: 0.6628671884536743 = 0.024752356112003326 + 0.1 * 6.381147861480713
Epoch 560, val loss: 0.8486683368682861
Epoch 570, training loss: 0.6590122580528259 = 0.02343294583261013 + 0.1 * 6.35579252243042
Epoch 570, val loss: 0.8557998538017273
Epoch 580, training loss: 0.6564756631851196 = 0.02221754752099514 + 0.1 * 6.342580795288086
Epoch 580, val loss: 0.8628517389297485
Epoch 590, training loss: 0.6555241942405701 = 0.021093076094985008 + 0.1 * 6.344311237335205
Epoch 590, val loss: 0.8698164224624634
Epoch 600, training loss: 0.6539870500564575 = 0.02005862630903721 + 0.1 * 6.3392839431762695
Epoch 600, val loss: 0.8764793872833252
Epoch 610, training loss: 0.6523072719573975 = 0.01910143718123436 + 0.1 * 6.332057952880859
Epoch 610, val loss: 0.8830715417861938
Epoch 620, training loss: 0.6516166925430298 = 0.0182096716016531 + 0.1 * 6.334069728851318
Epoch 620, val loss: 0.889614999294281
Epoch 630, training loss: 0.6500747203826904 = 0.017383430153131485 + 0.1 * 6.3269124031066895
Epoch 630, val loss: 0.89588862657547
Epoch 640, training loss: 0.6489154696464539 = 0.016612419858574867 + 0.1 * 6.323030471801758
Epoch 640, val loss: 0.9021233320236206
Epoch 650, training loss: 0.6476686596870422 = 0.015892211347818375 + 0.1 * 6.3177642822265625
Epoch 650, val loss: 0.9082220792770386
Epoch 660, training loss: 0.6471739411354065 = 0.015221127308905125 + 0.1 * 6.319528102874756
Epoch 660, val loss: 0.9142530560493469
Epoch 670, training loss: 0.6458273530006409 = 0.014597776345908642 + 0.1 * 6.312295913696289
Epoch 670, val loss: 0.9199748039245605
Epoch 680, training loss: 0.6437403559684753 = 0.014013036154210567 + 0.1 * 6.2972731590271
Epoch 680, val loss: 0.9257153272628784
Epoch 690, training loss: 0.6432337760925293 = 0.013462117873132229 + 0.1 * 6.2977166175842285
Epoch 690, val loss: 0.9313876032829285
Epoch 700, training loss: 0.6449865102767944 = 0.0129444170743227 + 0.1 * 6.32042121887207
Epoch 700, val loss: 0.9370273947715759
Epoch 710, training loss: 0.6424989104270935 = 0.012462371028959751 + 0.1 * 6.300365447998047
Epoch 710, val loss: 0.9423375725746155
Epoch 720, training loss: 0.6406952142715454 = 0.012008106335997581 + 0.1 * 6.28687047958374
Epoch 720, val loss: 0.9475502371788025
Epoch 730, training loss: 0.6433791518211365 = 0.011577713303267956 + 0.1 * 6.318014144897461
Epoch 730, val loss: 0.9527972340583801
Epoch 740, training loss: 0.6403974890708923 = 0.011170938611030579 + 0.1 * 6.29226541519165
Epoch 740, val loss: 0.9579204320907593
Epoch 750, training loss: 0.6385915279388428 = 0.010788625106215477 + 0.1 * 6.278028964996338
Epoch 750, val loss: 0.9628911018371582
Epoch 760, training loss: 0.6381001472473145 = 0.010425068438053131 + 0.1 * 6.276750564575195
Epoch 760, val loss: 0.9678768515586853
Epoch 770, training loss: 0.6370095014572144 = 0.010082092136144638 + 0.1 * 6.2692742347717285
Epoch 770, val loss: 0.9727015495300293
Epoch 780, training loss: 0.6378893256187439 = 0.009756782092154026 + 0.1 * 6.281325340270996
Epoch 780, val loss: 0.9774535298347473
Epoch 790, training loss: 0.6376879811286926 = 0.009447640739381313 + 0.1 * 6.282403469085693
Epoch 790, val loss: 0.9821513891220093
Epoch 800, training loss: 0.6367361545562744 = 0.00915508158504963 + 0.1 * 6.275810241699219
Epoch 800, val loss: 0.9867064356803894
Epoch 810, training loss: 0.6356257796287537 = 0.008876555599272251 + 0.1 * 6.267492294311523
Epoch 810, val loss: 0.9912598729133606
Epoch 820, training loss: 0.634827196598053 = 0.00861257966607809 + 0.1 * 6.26214599609375
Epoch 820, val loss: 0.9956287741661072
Epoch 830, training loss: 0.6355613470077515 = 0.008360056206583977 + 0.1 * 6.272013187408447
Epoch 830, val loss: 0.9999819993972778
Epoch 840, training loss: 0.634286105632782 = 0.008119895122945309 + 0.1 * 6.261662006378174
Epoch 840, val loss: 1.0043060779571533
Epoch 850, training loss: 0.6336197257041931 = 0.007891727611422539 + 0.1 * 6.257279872894287
Epoch 850, val loss: 1.0084586143493652
Epoch 860, training loss: 0.6339635848999023 = 0.0076730200089514256 + 0.1 * 6.262905597686768
Epoch 860, val loss: 1.0125819444656372
Epoch 870, training loss: 0.6327573657035828 = 0.007464174181222916 + 0.1 * 6.252931594848633
Epoch 870, val loss: 1.0166854858398438
Epoch 880, training loss: 0.6322894096374512 = 0.00726426811888814 + 0.1 * 6.250251293182373
Epoch 880, val loss: 1.0207149982452393
Epoch 890, training loss: 0.6314497590065002 = 0.007073350716382265 + 0.1 * 6.2437639236450195
Epoch 890, val loss: 1.0246367454528809
Epoch 900, training loss: 0.6331528425216675 = 0.006890349555760622 + 0.1 * 6.262624740600586
Epoch 900, val loss: 1.0284918546676636
Epoch 910, training loss: 0.6313338279724121 = 0.006714905146509409 + 0.1 * 6.246189594268799
Epoch 910, val loss: 1.0323688983917236
Epoch 920, training loss: 0.6327753663063049 = 0.006547365337610245 + 0.1 * 6.262279510498047
Epoch 920, val loss: 1.0361120700836182
Epoch 930, training loss: 0.6302134990692139 = 0.006386923138052225 + 0.1 * 6.238265514373779
Epoch 930, val loss: 1.0398110151290894
Epoch 940, training loss: 0.6299102902412415 = 0.006232770625501871 + 0.1 * 6.236774921417236
Epoch 940, val loss: 1.0434730052947998
Epoch 950, training loss: 0.6294442415237427 = 0.006084831431508064 + 0.1 * 6.233593940734863
Epoch 950, val loss: 1.0470635890960693
Epoch 960, training loss: 0.6299810409545898 = 0.005943254567682743 + 0.1 * 6.240377902984619
Epoch 960, val loss: 1.0505857467651367
Epoch 970, training loss: 0.6289032697677612 = 0.005805738270282745 + 0.1 * 6.230975151062012
Epoch 970, val loss: 1.0540987253189087
Epoch 980, training loss: 0.6286327242851257 = 0.00567433750256896 + 0.1 * 6.229583740234375
Epoch 980, val loss: 1.0575668811798096
Epoch 990, training loss: 0.6286136507987976 = 0.005547677632421255 + 0.1 * 6.230659484863281
Epoch 990, val loss: 1.0609397888183594
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7934
Flip ASR: 0.7556/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7746405601501465 = 1.9372495412826538 + 0.1 * 8.373908996582031
Epoch 0, val loss: 1.9396086931228638
Epoch 10, training loss: 2.7650058269500732 = 1.927623987197876 + 0.1 * 8.373818397521973
Epoch 10, val loss: 1.9298118352890015
Epoch 20, training loss: 2.7532799243927 = 1.9159530401229858 + 0.1 * 8.373269081115723
Epoch 20, val loss: 1.9178071022033691
Epoch 30, training loss: 2.7365481853485107 = 1.8996553421020508 + 0.1 * 8.368927955627441
Epoch 30, val loss: 1.9011815786361694
Epoch 40, training loss: 2.7097225189208984 = 1.8756709098815918 + 0.1 * 8.34051513671875
Epoch 40, val loss: 1.877288818359375
Epoch 50, training loss: 2.661048412322998 = 1.842864990234375 + 0.1 * 8.181835174560547
Epoch 50, val loss: 1.846260666847229
Epoch 60, training loss: 2.6018788814544678 = 1.8057563304901123 + 0.1 * 7.961225509643555
Epoch 60, val loss: 1.8134706020355225
Epoch 70, training loss: 2.532383918762207 = 1.769419550895691 + 0.1 * 7.629644393920898
Epoch 70, val loss: 1.7818520069122314
Epoch 80, training loss: 2.4478306770324707 = 1.730526328086853 + 0.1 * 7.173044681549072
Epoch 80, val loss: 1.745898962020874
Epoch 90, training loss: 2.3768839836120605 = 1.682846188545227 + 0.1 * 6.940378665924072
Epoch 90, val loss: 1.7022114992141724
Epoch 100, training loss: 2.30372953414917 = 1.6196805238723755 + 0.1 * 6.840489864349365
Epoch 100, val loss: 1.6463388204574585
Epoch 110, training loss: 2.2184884548187256 = 1.5396268367767334 + 0.1 * 6.7886152267456055
Epoch 110, val loss: 1.5771582126617432
Epoch 120, training loss: 2.123861312866211 = 1.4481241703033447 + 0.1 * 6.7573723793029785
Epoch 120, val loss: 1.5001626014709473
Epoch 130, training loss: 2.025282859802246 = 1.351908802986145 + 0.1 * 6.733739376068115
Epoch 130, val loss: 1.4200958013534546
Epoch 140, training loss: 1.9262373447418213 = 1.2546392679214478 + 0.1 * 6.715980052947998
Epoch 140, val loss: 1.3419263362884521
Epoch 150, training loss: 1.828112006187439 = 1.1579192876815796 + 0.1 * 6.701927185058594
Epoch 150, val loss: 1.2658276557922363
Epoch 160, training loss: 1.7300548553466797 = 1.0610957145690918 + 0.1 * 6.689591884613037
Epoch 160, val loss: 1.190176248550415
Epoch 170, training loss: 1.6325671672821045 = 0.9643557071685791 + 0.1 * 6.682115077972412
Epoch 170, val loss: 1.1146796941757202
Epoch 180, training loss: 1.53651762008667 = 0.8691776394844055 + 0.1 * 6.673399925231934
Epoch 180, val loss: 1.0412129163742065
Epoch 190, training loss: 1.4432882070541382 = 0.7767526507377625 + 0.1 * 6.665355682373047
Epoch 190, val loss: 0.9700519442558289
Epoch 200, training loss: 1.3565058708190918 = 0.6904333829879761 + 0.1 * 6.6607255935668945
Epoch 200, val loss: 0.9041209816932678
Epoch 210, training loss: 1.277911901473999 = 0.613060712814331 + 0.1 * 6.648512363433838
Epoch 210, val loss: 0.8467673063278198
Epoch 220, training loss: 1.2089049816131592 = 0.5447679162025452 + 0.1 * 6.641371250152588
Epoch 220, val loss: 0.7984738945960999
Epoch 230, training loss: 1.1484088897705078 = 0.48522835969924927 + 0.1 * 6.631804466247559
Epoch 230, val loss: 0.7592372298240662
Epoch 240, training loss: 1.0945714712142944 = 0.4327392280101776 + 0.1 * 6.618322849273682
Epoch 240, val loss: 0.7276202440261841
Epoch 250, training loss: 1.0467056035995483 = 0.38569405674934387 + 0.1 * 6.6101155281066895
Epoch 250, val loss: 0.701630175113678
Epoch 260, training loss: 1.0040321350097656 = 0.34337127208709717 + 0.1 * 6.606607913970947
Epoch 260, val loss: 0.6803655028343201
Epoch 270, training loss: 0.9650519490242004 = 0.30532974004745483 + 0.1 * 6.597221851348877
Epoch 270, val loss: 0.6631231904029846
Epoch 280, training loss: 0.9300355911254883 = 0.2708922028541565 + 0.1 * 6.591434001922607
Epoch 280, val loss: 0.6491926312446594
Epoch 290, training loss: 0.898931622505188 = 0.2398662567138672 + 0.1 * 6.590653419494629
Epoch 290, val loss: 0.6385154724121094
Epoch 300, training loss: 0.8705294728279114 = 0.2122594565153122 + 0.1 * 6.582699775695801
Epoch 300, val loss: 0.6309416890144348
Epoch 310, training loss: 0.8453038930892944 = 0.187801331281662 + 0.1 * 6.575026035308838
Epoch 310, val loss: 0.6260983347892761
Epoch 320, training loss: 0.8238473534584045 = 0.16639645397663116 + 0.1 * 6.574509143829346
Epoch 320, val loss: 0.6238389015197754
Epoch 330, training loss: 0.8041110634803772 = 0.14777153730392456 + 0.1 * 6.563395023345947
Epoch 330, val loss: 0.6238227486610413
Epoch 340, training loss: 0.7909093499183655 = 0.13154709339141846 + 0.1 * 6.59362268447876
Epoch 340, val loss: 0.6257241368293762
Epoch 350, training loss: 0.7728114128112793 = 0.11760631948709488 + 0.1 * 6.552050590515137
Epoch 350, val loss: 0.6290807127952576
Epoch 360, training loss: 0.76017826795578 = 0.10550490766763687 + 0.1 * 6.546733379364014
Epoch 360, val loss: 0.6338008046150208
Epoch 370, training loss: 0.7486380338668823 = 0.09493027627468109 + 0.1 * 6.5370774269104
Epoch 370, val loss: 0.6395758390426636
Epoch 380, training loss: 0.7388983964920044 = 0.08569332957267761 + 0.1 * 6.532050132751465
Epoch 380, val loss: 0.6460510492324829
Epoch 390, training loss: 0.7303934693336487 = 0.07763200253248215 + 0.1 * 6.527614593505859
Epoch 390, val loss: 0.6530868411064148
Epoch 400, training loss: 0.7222734689712524 = 0.07053154706954956 + 0.1 * 6.517419338226318
Epoch 400, val loss: 0.6605868339538574
Epoch 410, training loss: 0.7161008715629578 = 0.06428308039903641 + 0.1 * 6.5181779861450195
Epoch 410, val loss: 0.6683134436607361
Epoch 420, training loss: 0.7091023921966553 = 0.05876902863383293 + 0.1 * 6.503333568572998
Epoch 420, val loss: 0.6762831807136536
Epoch 430, training loss: 0.705306887626648 = 0.05386757478117943 + 0.1 * 6.514392852783203
Epoch 430, val loss: 0.6844115853309631
Epoch 440, training loss: 0.6989116668701172 = 0.04952317848801613 + 0.1 * 6.493884563446045
Epoch 440, val loss: 0.6926781535148621
Epoch 450, training loss: 0.6947503685951233 = 0.045649196952581406 + 0.1 * 6.491012096405029
Epoch 450, val loss: 0.7009586691856384
Epoch 460, training loss: 0.6901464462280273 = 0.04219154268503189 + 0.1 * 6.479548454284668
Epoch 460, val loss: 0.7092126607894897
Epoch 470, training loss: 0.6862916350364685 = 0.039083387702703476 + 0.1 * 6.472082614898682
Epoch 470, val loss: 0.7176159024238586
Epoch 480, training loss: 0.6828150749206543 = 0.03629002720117569 + 0.1 * 6.465250015258789
Epoch 480, val loss: 0.7258596420288086
Epoch 490, training loss: 0.6781688928604126 = 0.03377939760684967 + 0.1 * 6.443894863128662
Epoch 490, val loss: 0.7341296076774597
Epoch 500, training loss: 0.6752107739448547 = 0.03151540458202362 + 0.1 * 6.436953544616699
Epoch 500, val loss: 0.7422742247581482
Epoch 510, training loss: 0.673134446144104 = 0.02945888414978981 + 0.1 * 6.436755657196045
Epoch 510, val loss: 0.7504096627235413
Epoch 520, training loss: 0.671150803565979 = 0.027592835947871208 + 0.1 * 6.435579299926758
Epoch 520, val loss: 0.758365273475647
Epoch 530, training loss: 0.6681636571884155 = 0.025911493226885796 + 0.1 * 6.422521114349365
Epoch 530, val loss: 0.7660627961158752
Epoch 540, training loss: 0.6656166315078735 = 0.024379294365644455 + 0.1 * 6.412373065948486
Epoch 540, val loss: 0.773658275604248
Epoch 550, training loss: 0.6667565107345581 = 0.022972026839852333 + 0.1 * 6.437844276428223
Epoch 550, val loss: 0.7811208963394165
Epoch 560, training loss: 0.661815881729126 = 0.02168576419353485 + 0.1 * 6.40130090713501
Epoch 560, val loss: 0.7884455323219299
Epoch 570, training loss: 0.6598648428916931 = 0.020506707951426506 + 0.1 * 6.393581390380859
Epoch 570, val loss: 0.7957062721252441
Epoch 580, training loss: 0.6599466800689697 = 0.019418785348534584 + 0.1 * 6.405279159545898
Epoch 580, val loss: 0.8027551770210266
Epoch 590, training loss: 0.6570155620574951 = 0.018421920016407967 + 0.1 * 6.385936260223389
Epoch 590, val loss: 0.8096311688423157
Epoch 600, training loss: 0.6552926898002625 = 0.01750924065709114 + 0.1 * 6.377834320068359
Epoch 600, val loss: 0.8162528276443481
Epoch 610, training loss: 0.6543310284614563 = 0.01666233129799366 + 0.1 * 6.3766865730285645
Epoch 610, val loss: 0.8228227496147156
Epoch 620, training loss: 0.6523651480674744 = 0.01587553508579731 + 0.1 * 6.364896297454834
Epoch 620, val loss: 0.8292165398597717
Epoch 630, training loss: 0.652733564376831 = 0.015143901109695435 + 0.1 * 6.375896453857422
Epoch 630, val loss: 0.8355409502983093
Epoch 640, training loss: 0.651716947555542 = 0.014466771855950356 + 0.1 * 6.372501850128174
Epoch 640, val loss: 0.8416936993598938
Epoch 650, training loss: 0.6495401263237 = 0.013839194551110268 + 0.1 * 6.357009410858154
Epoch 650, val loss: 0.847641110420227
Epoch 660, training loss: 0.6478939056396484 = 0.01325319055467844 + 0.1 * 6.346406936645508
Epoch 660, val loss: 0.853514552116394
Epoch 670, training loss: 0.6481063961982727 = 0.012705368921160698 + 0.1 * 6.354010105133057
Epoch 670, val loss: 0.8592649698257446
Epoch 680, training loss: 0.6461662650108337 = 0.012192725203931332 + 0.1 * 6.339735507965088
Epoch 680, val loss: 0.8648093938827515
Epoch 690, training loss: 0.6482251286506653 = 0.011711735278367996 + 0.1 * 6.365134239196777
Epoch 690, val loss: 0.8703336119651794
Epoch 700, training loss: 0.6445975303649902 = 0.011261834762990475 + 0.1 * 6.333356857299805
Epoch 700, val loss: 0.8756951093673706
Epoch 710, training loss: 0.6442369818687439 = 0.010840828530490398 + 0.1 * 6.333961486816406
Epoch 710, val loss: 0.8809597492218018
Epoch 720, training loss: 0.6433217525482178 = 0.010444692336022854 + 0.1 * 6.328770637512207
Epoch 720, val loss: 0.8860642910003662
Epoch 730, training loss: 0.6422532200813293 = 0.010071013122797012 + 0.1 * 6.321821689605713
Epoch 730, val loss: 0.891101062297821
Epoch 740, training loss: 0.6424233317375183 = 0.009717960841953754 + 0.1 * 6.327054023742676
Epoch 740, val loss: 0.896069347858429
Epoch 750, training loss: 0.6405627131462097 = 0.009386694058775902 + 0.1 * 6.311759948730469
Epoch 750, val loss: 0.9008716940879822
Epoch 760, training loss: 0.6412026882171631 = 0.00907280296087265 + 0.1 * 6.321299076080322
Epoch 760, val loss: 0.9056592583656311
Epoch 770, training loss: 0.6384937763214111 = 0.008776196278631687 + 0.1 * 6.297175884246826
Epoch 770, val loss: 0.9102658033370972
Epoch 780, training loss: 0.6394743919372559 = 0.008495533838868141 + 0.1 * 6.309788703918457
Epoch 780, val loss: 0.9148598909378052
Epoch 790, training loss: 0.638783872127533 = 0.008229191415011883 + 0.1 * 6.305546760559082
Epoch 790, val loss: 0.9193641543388367
Epoch 800, training loss: 0.6379052996635437 = 0.007976220920681953 + 0.1 * 6.299290657043457
Epoch 800, val loss: 0.9237827062606812
Epoch 810, training loss: 0.6371771693229675 = 0.007737333420664072 + 0.1 * 6.294398307800293
Epoch 810, val loss: 0.9281295537948608
Epoch 820, training loss: 0.6388952136039734 = 0.007508446462452412 + 0.1 * 6.313867568969727
Epoch 820, val loss: 0.9324791431427002
Epoch 830, training loss: 0.636375367641449 = 0.007291546557098627 + 0.1 * 6.29083776473999
Epoch 830, val loss: 0.936627984046936
Epoch 840, training loss: 0.6360269784927368 = 0.007084968499839306 + 0.1 * 6.289420127868652
Epoch 840, val loss: 0.940808892250061
Epoch 850, training loss: 0.6347193717956543 = 0.006887516472488642 + 0.1 * 6.278318881988525
Epoch 850, val loss: 0.9448881149291992
Epoch 860, training loss: 0.634001612663269 = 0.006699850782752037 + 0.1 * 6.273017406463623
Epoch 860, val loss: 0.9488922357559204
Epoch 870, training loss: 0.6346484422683716 = 0.006520025432109833 + 0.1 * 6.281283855438232
Epoch 870, val loss: 0.9528778195381165
Epoch 880, training loss: 0.6337404847145081 = 0.00634798826649785 + 0.1 * 6.273924827575684
Epoch 880, val loss: 0.9567746520042419
Epoch 890, training loss: 0.6334705948829651 = 0.006183282472193241 + 0.1 * 6.2728729248046875
Epoch 890, val loss: 0.9606091380119324
Epoch 900, training loss: 0.632626473903656 = 0.006027738563716412 + 0.1 * 6.265986919403076
Epoch 900, val loss: 0.9643421173095703
Epoch 910, training loss: 0.6326586604118347 = 0.005877751857042313 + 0.1 * 6.26780891418457
Epoch 910, val loss: 0.9680219888687134
Epoch 920, training loss: 0.6315566301345825 = 0.005733856465667486 + 0.1 * 6.258227825164795
Epoch 920, val loss: 0.9716628193855286
Epoch 930, training loss: 0.6321722269058228 = 0.005595352966338396 + 0.1 * 6.265769004821777
Epoch 930, val loss: 0.9752832651138306
Epoch 940, training loss: 0.6339783668518066 = 0.005462203174829483 + 0.1 * 6.28516149520874
Epoch 940, val loss: 0.9788439869880676
Epoch 950, training loss: 0.6315892338752747 = 0.005335173103958368 + 0.1 * 6.262540817260742
Epoch 950, val loss: 0.9823344945907593
Epoch 960, training loss: 0.632348895072937 = 0.005212785676121712 + 0.1 * 6.271360874176025
Epoch 960, val loss: 0.9858085513114929
Epoch 970, training loss: 0.6303297877311707 = 0.005095917731523514 + 0.1 * 6.252338886260986
Epoch 970, val loss: 0.9892010688781738
Epoch 980, training loss: 0.6294848918914795 = 0.004982615355402231 + 0.1 * 6.245022773742676
Epoch 980, val loss: 0.992559015750885
Epoch 990, training loss: 0.6316700577735901 = 0.004873726516962051 + 0.1 * 6.26796293258667
Epoch 990, val loss: 0.9958570599555969
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9225
Flip ASR: 0.9067/225 nodes
The final ASR:0.80320, 0.09366, Accuracy:0.81605, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11564])
remove edge: torch.Size([2, 9584])
updated graph: torch.Size([2, 10592])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98401, 0.00920, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7806997299194336 = 1.9433200359344482 + 0.1 * 8.373796463012695
Epoch 0, val loss: 1.9387892484664917
Epoch 10, training loss: 2.7703468799591064 = 1.9329818487167358 + 0.1 * 8.373650550842285
Epoch 10, val loss: 1.9284778833389282
Epoch 20, training loss: 2.7581231594085693 = 1.9208430051803589 + 0.1 * 8.372800827026367
Epoch 20, val loss: 1.9162448644638062
Epoch 30, training loss: 2.7411417961120605 = 1.904404878616333 + 0.1 * 8.3673677444458
Epoch 30, val loss: 1.899667739868164
Epoch 40, training loss: 2.7135274410247803 = 1.8807646036148071 + 0.1 * 8.327629089355469
Epoch 40, val loss: 1.8760262727737427
Epoch 50, training loss: 2.643592596054077 = 1.8488367795944214 + 0.1 * 7.947558403015137
Epoch 50, val loss: 1.8455924987792969
Epoch 60, training loss: 2.541752338409424 = 1.8156895637512207 + 0.1 * 7.260628700256348
Epoch 60, val loss: 1.8162438869476318
Epoch 70, training loss: 2.476489543914795 = 1.7826967239379883 + 0.1 * 6.937927722930908
Epoch 70, val loss: 1.7870445251464844
Epoch 80, training loss: 2.4241387844085693 = 1.7447471618652344 + 0.1 * 6.793916702270508
Epoch 80, val loss: 1.755455493927002
Epoch 90, training loss: 2.3722221851348877 = 1.6994035243988037 + 0.1 * 6.728187084197998
Epoch 90, val loss: 1.7175543308258057
Epoch 100, training loss: 2.3089888095855713 = 1.6399991512298584 + 0.1 * 6.689896583557129
Epoch 100, val loss: 1.6676677465438843
Epoch 110, training loss: 2.230659008026123 = 1.563897967338562 + 0.1 * 6.6676106452941895
Epoch 110, val loss: 1.6044825315475464
Epoch 120, training loss: 2.138871908187866 = 1.4734028577804565 + 0.1 * 6.654690265655518
Epoch 120, val loss: 1.531417965888977
Epoch 130, training loss: 2.040092706680298 = 1.3755788803100586 + 0.1 * 6.645137310028076
Epoch 130, val loss: 1.4535386562347412
Epoch 140, training loss: 1.939093828201294 = 1.2758880853652954 + 0.1 * 6.6320576667785645
Epoch 140, val loss: 1.3759535551071167
Epoch 150, training loss: 1.8378207683563232 = 1.1765187978744507 + 0.1 * 6.6130194664001465
Epoch 150, val loss: 1.2998754978179932
Epoch 160, training loss: 1.7379858493804932 = 1.0791791677474976 + 0.1 * 6.588066577911377
Epoch 160, val loss: 1.2248926162719727
Epoch 170, training loss: 1.6425108909606934 = 0.9858985543251038 + 0.1 * 6.566123008728027
Epoch 170, val loss: 1.1537147760391235
Epoch 180, training loss: 1.5524210929870605 = 0.8987284302711487 + 0.1 * 6.53692626953125
Epoch 180, val loss: 1.0880281925201416
Epoch 190, training loss: 1.467872977256775 = 0.8166208267211914 + 0.1 * 6.512521266937256
Epoch 190, val loss: 1.026835560798645
Epoch 200, training loss: 1.388512372970581 = 0.7394074201583862 + 0.1 * 6.491048812866211
Epoch 200, val loss: 0.9703275561332703
Epoch 210, training loss: 1.3149237632751465 = 0.6670231819152832 + 0.1 * 6.479005336761475
Epoch 210, val loss: 0.9177237749099731
Epoch 220, training loss: 1.2452120780944824 = 0.5997411012649536 + 0.1 * 6.454710006713867
Epoch 220, val loss: 0.8696177005767822
Epoch 230, training loss: 1.1807528734207153 = 0.5369693040847778 + 0.1 * 6.437835693359375
Epoch 230, val loss: 0.8252776265144348
Epoch 240, training loss: 1.1219024658203125 = 0.47894108295440674 + 0.1 * 6.4296135902404785
Epoch 240, val loss: 0.7855175733566284
Epoch 250, training loss: 1.0671539306640625 = 0.42569899559020996 + 0.1 * 6.414548873901367
Epoch 250, val loss: 0.7511064410209656
Epoch 260, training loss: 1.0177662372589111 = 0.37696170806884766 + 0.1 * 6.408044338226318
Epoch 260, val loss: 0.7219650745391846
Epoch 270, training loss: 0.9726830124855042 = 0.3323773145675659 + 0.1 * 6.403056621551514
Epoch 270, val loss: 0.6978374719619751
Epoch 280, training loss: 0.9315136671066284 = 0.2920171022415161 + 0.1 * 6.394965648651123
Epoch 280, val loss: 0.6781573295593262
Epoch 290, training loss: 0.8945450782775879 = 0.2558394968509674 + 0.1 * 6.38705587387085
Epoch 290, val loss: 0.6627546548843384
Epoch 300, training loss: 0.8622384667396545 = 0.22380544245243073 + 0.1 * 6.3843302726745605
Epoch 300, val loss: 0.6513602137565613
Epoch 310, training loss: 0.8332862854003906 = 0.19602486491203308 + 0.1 * 6.372613906860352
Epoch 310, val loss: 0.6439079642295837
Epoch 320, training loss: 0.8093039989471436 = 0.17209525406360626 + 0.1 * 6.372087478637695
Epoch 320, val loss: 0.639742910861969
Epoch 330, training loss: 0.7884457111358643 = 0.1517532467842102 + 0.1 * 6.366924285888672
Epoch 330, val loss: 0.6385724544525146
Epoch 340, training loss: 0.7701465487480164 = 0.13452278077602386 + 0.1 * 6.356237411499023
Epoch 340, val loss: 0.6396840810775757
Epoch 350, training loss: 0.7566320300102234 = 0.11986196786165237 + 0.1 * 6.367700576782227
Epoch 350, val loss: 0.6426810622215271
Epoch 360, training loss: 0.743065595626831 = 0.10745196044445038 + 0.1 * 6.356135845184326
Epoch 360, val loss: 0.6470219492912292
Epoch 370, training loss: 0.7317123413085938 = 0.09685979038476944 + 0.1 * 6.348525047302246
Epoch 370, val loss: 0.6523611545562744
Epoch 380, training loss: 0.7219774723052979 = 0.08775248378515244 + 0.1 * 6.342249870300293
Epoch 380, val loss: 0.658418595790863
Epoch 390, training loss: 0.7134777307510376 = 0.07985744625329971 + 0.1 * 6.336202621459961
Epoch 390, val loss: 0.6649553179740906
Epoch 400, training loss: 0.7061497569084167 = 0.07297364622354507 + 0.1 * 6.331760883331299
Epoch 400, val loss: 0.6718244552612305
Epoch 410, training loss: 0.6998451948165894 = 0.06695357710123062 + 0.1 * 6.328916072845459
Epoch 410, val loss: 0.6789044141769409
Epoch 420, training loss: 0.6941128373146057 = 0.06163283437490463 + 0.1 * 6.32480001449585
Epoch 420, val loss: 0.6861557960510254
Epoch 430, training loss: 0.690950870513916 = 0.05692917853593826 + 0.1 * 6.340216636657715
Epoch 430, val loss: 0.6934706568717957
Epoch 440, training loss: 0.684404730796814 = 0.05276500806212425 + 0.1 * 6.316397190093994
Epoch 440, val loss: 0.7007647752761841
Epoch 450, training loss: 0.6799170970916748 = 0.04902844876050949 + 0.1 * 6.3088860511779785
Epoch 450, val loss: 0.708087682723999
Epoch 460, training loss: 0.6775872111320496 = 0.045664310455322266 + 0.1 * 6.3192291259765625
Epoch 460, val loss: 0.715431272983551
Epoch 470, training loss: 0.6731328368186951 = 0.04264331981539726 + 0.1 * 6.304895401000977
Epoch 470, val loss: 0.7226893305778503
Epoch 480, training loss: 0.6702322363853455 = 0.03990781307220459 + 0.1 * 6.303244113922119
Epoch 480, val loss: 0.7299156188964844
Epoch 490, training loss: 0.6675254702568054 = 0.037419043481349945 + 0.1 * 6.301064491271973
Epoch 490, val loss: 0.7370742559432983
Epoch 500, training loss: 0.6644287705421448 = 0.0351572185754776 + 0.1 * 6.292715549468994
Epoch 500, val loss: 0.7441652417182922
Epoch 510, training loss: 0.6630424857139587 = 0.03308466449379921 + 0.1 * 6.299577713012695
Epoch 510, val loss: 0.7511829733848572
Epoch 520, training loss: 0.6605104804039001 = 0.031189706176519394 + 0.1 * 6.29320764541626
Epoch 520, val loss: 0.758094072341919
Epoch 530, training loss: 0.6575879454612732 = 0.029454898089170456 + 0.1 * 6.281330585479736
Epoch 530, val loss: 0.7649227380752563
Epoch 540, training loss: 0.6588486433029175 = 0.027853405103087425 + 0.1 * 6.3099517822265625
Epoch 540, val loss: 0.7716168165206909
Epoch 550, training loss: 0.6541159749031067 = 0.02638470195233822 + 0.1 * 6.277312755584717
Epoch 550, val loss: 0.7782192230224609
Epoch 560, training loss: 0.6517848372459412 = 0.025032170116901398 + 0.1 * 6.267526626586914
Epoch 560, val loss: 0.7847187519073486
Epoch 570, training loss: 0.6511831283569336 = 0.023776134476065636 + 0.1 * 6.2740702629089355
Epoch 570, val loss: 0.7911671996116638
Epoch 580, training loss: 0.649535596370697 = 0.02261161245405674 + 0.1 * 6.269239902496338
Epoch 580, val loss: 0.797433614730835
Epoch 590, training loss: 0.6474004983901978 = 0.02153705060482025 + 0.1 * 6.258634090423584
Epoch 590, val loss: 0.8036011457443237
Epoch 600, training loss: 0.6484548449516296 = 0.020534168928861618 + 0.1 * 6.2792067527771
Epoch 600, val loss: 0.8097407817840576
Epoch 610, training loss: 0.6446382999420166 = 0.01960153318941593 + 0.1 * 6.250367641448975
Epoch 610, val loss: 0.815725564956665
Epoch 620, training loss: 0.6435844302177429 = 0.018738817423582077 + 0.1 * 6.248456001281738
Epoch 620, val loss: 0.8215668201446533
Epoch 630, training loss: 0.6450740694999695 = 0.017927052453160286 + 0.1 * 6.271470069885254
Epoch 630, val loss: 0.8272718191146851
Epoch 640, training loss: 0.641628086566925 = 0.01717066764831543 + 0.1 * 6.244574069976807
Epoch 640, val loss: 0.8329023718833923
Epoch 650, training loss: 0.6404781341552734 = 0.016465656459331512 + 0.1 * 6.240124702453613
Epoch 650, val loss: 0.8384629487991333
Epoch 660, training loss: 0.6413808465003967 = 0.015798449516296387 + 0.1 * 6.255824089050293
Epoch 660, val loss: 0.8438901305198669
Epoch 670, training loss: 0.6388231515884399 = 0.015173828229308128 + 0.1 * 6.236493110656738
Epoch 670, val loss: 0.8492476344108582
Epoch 680, training loss: 0.6375409960746765 = 0.014590082690119743 + 0.1 * 6.229508876800537
Epoch 680, val loss: 0.8546009063720703
Epoch 690, training loss: 0.637087345123291 = 0.014036945067346096 + 0.1 * 6.230503559112549
Epoch 690, val loss: 0.8598102927207947
Epoch 700, training loss: 0.6368477940559387 = 0.013516656123101711 + 0.1 * 6.233311176300049
Epoch 700, val loss: 0.8649536967277527
Epoch 710, training loss: 0.6362871527671814 = 0.013026398606598377 + 0.1 * 6.232607364654541
Epoch 710, val loss: 0.8700006604194641
Epoch 720, training loss: 0.6350940465927124 = 0.012563223950564861 + 0.1 * 6.225307941436768
Epoch 720, val loss: 0.8749876618385315
Epoch 730, training loss: 0.6342843770980835 = 0.012126467190682888 + 0.1 * 6.221579074859619
Epoch 730, val loss: 0.8798291683197021
Epoch 740, training loss: 0.6328370571136475 = 0.011712182313203812 + 0.1 * 6.211248397827148
Epoch 740, val loss: 0.8845874667167664
Epoch 750, training loss: 0.6326010227203369 = 0.011322522535920143 + 0.1 * 6.212785243988037
Epoch 750, val loss: 0.889303982257843
Epoch 760, training loss: 0.6333727240562439 = 0.010951471514999866 + 0.1 * 6.224212646484375
Epoch 760, val loss: 0.893866240978241
Epoch 770, training loss: 0.6321049332618713 = 0.010598150081932545 + 0.1 * 6.2150678634643555
Epoch 770, val loss: 0.8983996510505676
Epoch 780, training loss: 0.6311492323875427 = 0.010266555473208427 + 0.1 * 6.208827018737793
Epoch 780, val loss: 0.9029243588447571
Epoch 790, training loss: 0.6316815614700317 = 0.009949374943971634 + 0.1 * 6.217321872711182
Epoch 790, val loss: 0.9073155522346497
Epoch 800, training loss: 0.6305159330368042 = 0.009646235033869743 + 0.1 * 6.2086968421936035
Epoch 800, val loss: 0.9115559458732605
Epoch 810, training loss: 0.6294220685958862 = 0.009361693635582924 + 0.1 * 6.20060396194458
Epoch 810, val loss: 0.9158229231834412
Epoch 820, training loss: 0.6298171281814575 = 0.009088429622352123 + 0.1 * 6.207286834716797
Epoch 820, val loss: 0.9199931025505066
Epoch 830, training loss: 0.6283626556396484 = 0.00882820226252079 + 0.1 * 6.195343971252441
Epoch 830, val loss: 0.9240415692329407
Epoch 840, training loss: 0.6289446949958801 = 0.008579925633966923 + 0.1 * 6.203647613525391
Epoch 840, val loss: 0.9280744791030884
Epoch 850, training loss: 0.6276243925094604 = 0.008341029286384583 + 0.1 * 6.19283390045166
Epoch 850, val loss: 0.9320358037948608
Epoch 860, training loss: 0.627110481262207 = 0.008114864118397236 + 0.1 * 6.189955711364746
Epoch 860, val loss: 0.9359457492828369
Epoch 870, training loss: 0.6276964545249939 = 0.007897235453128815 + 0.1 * 6.197991847991943
Epoch 870, val loss: 0.9398384690284729
Epoch 880, training loss: 0.6266090273857117 = 0.007690409664064646 + 0.1 * 6.189186096191406
Epoch 880, val loss: 0.9436002373695374
Epoch 890, training loss: 0.6259860992431641 = 0.007492179051041603 + 0.1 * 6.184938907623291
Epoch 890, val loss: 0.9473708868026733
Epoch 900, training loss: 0.6261169910430908 = 0.00730071309953928 + 0.1 * 6.188162326812744
Epoch 900, val loss: 0.9510350227355957
Epoch 910, training loss: 0.6260491013526917 = 0.007118071895092726 + 0.1 * 6.189310550689697
Epoch 910, val loss: 0.9546833038330078
Epoch 920, training loss: 0.6251522898674011 = 0.006943884771317244 + 0.1 * 6.182084083557129
Epoch 920, val loss: 0.9583113193511963
Epoch 930, training loss: 0.6250277161598206 = 0.006776394322514534 + 0.1 * 6.182513236999512
Epoch 930, val loss: 0.9618632197380066
Epoch 940, training loss: 0.6246370077133179 = 0.006614606361836195 + 0.1 * 6.18022346496582
Epoch 940, val loss: 0.965298593044281
Epoch 950, training loss: 0.6261261105537415 = 0.006459344644099474 + 0.1 * 6.196667671203613
Epoch 950, val loss: 0.9687842726707458
Epoch 960, training loss: 0.6234151721000671 = 0.006309993099421263 + 0.1 * 6.171051979064941
Epoch 960, val loss: 0.9721980094909668
Epoch 970, training loss: 0.6234489679336548 = 0.006168275605887175 + 0.1 * 6.172807216644287
Epoch 970, val loss: 0.9756261706352234
Epoch 980, training loss: 0.6231753826141357 = 0.006030399352312088 + 0.1 * 6.171449661254883
Epoch 980, val loss: 0.9789031744003296
Epoch 990, training loss: 0.6243603229522705 = 0.005897597409784794 + 0.1 * 6.184627532958984
Epoch 990, val loss: 0.982144832611084
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.5793
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.77252197265625 = 1.9351364374160767 + 0.1 * 8.373856544494629
Epoch 0, val loss: 1.9365582466125488
Epoch 10, training loss: 2.76263165473938 = 1.9252620935440063 + 0.1 * 8.373696327209473
Epoch 10, val loss: 1.9261723756790161
Epoch 20, training loss: 2.7503442764282227 = 1.9130473136901855 + 0.1 * 8.372969627380371
Epoch 20, val loss: 1.9131739139556885
Epoch 30, training loss: 2.7326347827911377 = 1.8958221673965454 + 0.1 * 8.368125915527344
Epoch 30, val loss: 1.8947089910507202
Epoch 40, training loss: 2.7030692100524902 = 1.8704335689544678 + 0.1 * 8.326355934143066
Epoch 40, val loss: 1.8676741123199463
Epoch 50, training loss: 2.6267669200897217 = 1.8369240760803223 + 0.1 * 7.898428440093994
Epoch 50, val loss: 1.833829641342163
Epoch 60, training loss: 2.5331430435180664 = 1.8061341047286987 + 0.1 * 7.270088195800781
Epoch 60, val loss: 1.804680347442627
Epoch 70, training loss: 2.469247579574585 = 1.7760143280029297 + 0.1 * 6.9323320388793945
Epoch 70, val loss: 1.7772293090820312
Epoch 80, training loss: 2.4217960834503174 = 1.7432080507278442 + 0.1 * 6.785880088806152
Epoch 80, val loss: 1.7487208843231201
Epoch 90, training loss: 2.3729772567749023 = 1.7006784677505493 + 0.1 * 6.722989082336426
Epoch 90, val loss: 1.7127962112426758
Epoch 100, training loss: 2.309727668762207 = 1.6422350406646729 + 0.1 * 6.674925804138184
Epoch 100, val loss: 1.6645649671554565
Epoch 110, training loss: 2.229170083999634 = 1.5653321743011475 + 0.1 * 6.638378143310547
Epoch 110, val loss: 1.60141122341156
Epoch 120, training loss: 2.133443832397461 = 1.4722552299499512 + 0.1 * 6.611886024475098
Epoch 120, val loss: 1.5251131057739258
Epoch 130, training loss: 2.028292179107666 = 1.3690638542175293 + 0.1 * 6.592281818389893
Epoch 130, val loss: 1.44107186794281
Epoch 140, training loss: 1.918299913406372 = 1.260908603668213 + 0.1 * 6.573912620544434
Epoch 140, val loss: 1.3547461032867432
Epoch 150, training loss: 1.8060283660888672 = 1.1507768630981445 + 0.1 * 6.552515029907227
Epoch 150, val loss: 1.2677491903305054
Epoch 160, training loss: 1.6970930099487305 = 1.043668508529663 + 0.1 * 6.534244060516357
Epoch 160, val loss: 1.183663249015808
Epoch 170, training loss: 1.5987136363983154 = 0.9470218420028687 + 0.1 * 6.516918659210205
Epoch 170, val loss: 1.1092238426208496
Epoch 180, training loss: 1.5126972198486328 = 0.8631442785263062 + 0.1 * 6.495528697967529
Epoch 180, val loss: 1.046461820602417
Epoch 190, training loss: 1.4418426752090454 = 0.7943701148033142 + 0.1 * 6.474725246429443
Epoch 190, val loss: 0.9976951479911804
Epoch 200, training loss: 1.3852348327636719 = 0.7394595146179199 + 0.1 * 6.457752227783203
Epoch 200, val loss: 0.9617605805397034
Epoch 210, training loss: 1.3393821716308594 = 0.6955288052558899 + 0.1 * 6.438533306121826
Epoch 210, val loss: 0.9357610940933228
Epoch 220, training loss: 1.3014676570892334 = 0.6590632200241089 + 0.1 * 6.424044609069824
Epoch 220, val loss: 0.9165753126144409
Epoch 230, training loss: 1.2682509422302246 = 0.6271593570709229 + 0.1 * 6.410915851593018
Epoch 230, val loss: 0.9012882113456726
Epoch 240, training loss: 1.236485242843628 = 0.5973715782165527 + 0.1 * 6.391136169433594
Epoch 240, val loss: 0.8878061771392822
Epoch 250, training loss: 1.207640528678894 = 0.5676556825637817 + 0.1 * 6.399848461151123
Epoch 250, val loss: 0.874487578868866
Epoch 260, training loss: 1.1747589111328125 = 0.537123441696167 + 0.1 * 6.376354217529297
Epoch 260, val loss: 0.8605884313583374
Epoch 270, training loss: 1.141420602798462 = 0.5048866868019104 + 0.1 * 6.3653388023376465
Epoch 270, val loss: 0.8458628058433533
Epoch 280, training loss: 1.1065396070480347 = 0.47069400548934937 + 0.1 * 6.358455657958984
Epoch 280, val loss: 0.8300963640213013
Epoch 290, training loss: 1.0703582763671875 = 0.43516868352890015 + 0.1 * 6.351895809173584
Epoch 290, val loss: 0.814014196395874
Epoch 300, training loss: 1.0340354442596436 = 0.39941173791885376 + 0.1 * 6.346237659454346
Epoch 300, val loss: 0.7983514666557312
Epoch 310, training loss: 0.9997258186340332 = 0.36471518874168396 + 0.1 * 6.3501057624816895
Epoch 310, val loss: 0.7842389941215515
Epoch 320, training loss: 0.9651333093643188 = 0.33201828598976135 + 0.1 * 6.331150054931641
Epoch 320, val loss: 0.7722585201263428
Epoch 330, training loss: 0.9339617490768433 = 0.30136656761169434 + 0.1 * 6.32595157623291
Epoch 330, val loss: 0.7626482844352722
Epoch 340, training loss: 0.9065783023834229 = 0.27295857667922974 + 0.1 * 6.336197376251221
Epoch 340, val loss: 0.7555564045906067
Epoch 350, training loss: 0.8786541819572449 = 0.24693505465984344 + 0.1 * 6.317191123962402
Epoch 350, val loss: 0.7508506774902344
Epoch 360, training loss: 0.854064404964447 = 0.22301602363586426 + 0.1 * 6.310483932495117
Epoch 360, val loss: 0.7484388947486877
Epoch 370, training loss: 0.8328611254692078 = 0.20135588943958282 + 0.1 * 6.315052509307861
Epoch 370, val loss: 0.7483779191970825
Epoch 380, training loss: 0.8124147653579712 = 0.18196147680282593 + 0.1 * 6.304533004760742
Epoch 380, val loss: 0.7504168748855591
Epoch 390, training loss: 0.7962287664413452 = 0.16465313732624054 + 0.1 * 6.315756320953369
Epoch 390, val loss: 0.7544100880622864
Epoch 400, training loss: 0.7783851623535156 = 0.14923745393753052 + 0.1 * 6.291477203369141
Epoch 400, val loss: 0.759782075881958
Epoch 410, training loss: 0.7665883302688599 = 0.13554425537586212 + 0.1 * 6.310441017150879
Epoch 410, val loss: 0.7668973803520203
Epoch 420, training loss: 0.7512952089309692 = 0.12343892455101013 + 0.1 * 6.278563022613525
Epoch 420, val loss: 0.7751417756080627
Epoch 430, training loss: 0.7406244874000549 = 0.11269848793745041 + 0.1 * 6.27925968170166
Epoch 430, val loss: 0.7846882939338684
Epoch 440, training loss: 0.7306702733039856 = 0.1031595915555954 + 0.1 * 6.275106430053711
Epoch 440, val loss: 0.7951098680496216
Epoch 450, training loss: 0.7211877107620239 = 0.09462718665599823 + 0.1 * 6.2656049728393555
Epoch 450, val loss: 0.8061657547950745
Epoch 460, training loss: 0.7135753035545349 = 0.08692234754562378 + 0.1 * 6.266529560089111
Epoch 460, val loss: 0.8177698254585266
Epoch 470, training loss: 0.7060297727584839 = 0.07999325543642044 + 0.1 * 6.260365009307861
Epoch 470, val loss: 0.829841673374176
Epoch 480, training loss: 0.6995271444320679 = 0.07375087589025497 + 0.1 * 6.257762908935547
Epoch 480, val loss: 0.8421825170516968
Epoch 490, training loss: 0.6934057474136353 = 0.06805000454187393 + 0.1 * 6.253557205200195
Epoch 490, val loss: 0.8546749949455261
Epoch 500, training loss: 0.6874417066574097 = 0.06282751262187958 + 0.1 * 6.2461419105529785
Epoch 500, val loss: 0.867327868938446
Epoch 510, training loss: 0.6840686798095703 = 0.05805615335702896 + 0.1 * 6.260125160217285
Epoch 510, val loss: 0.8799713253974915
Epoch 520, training loss: 0.6775479316711426 = 0.05372317135334015 + 0.1 * 6.238247394561768
Epoch 520, val loss: 0.8925968408584595
Epoch 530, training loss: 0.6743411421775818 = 0.04978511482477188 + 0.1 * 6.2455596923828125
Epoch 530, val loss: 0.9052143096923828
Epoch 540, training loss: 0.6692968606948853 = 0.04622476175427437 + 0.1 * 6.2307209968566895
Epoch 540, val loss: 0.9176861047744751
Epoch 550, training loss: 0.6659270524978638 = 0.04300442710518837 + 0.1 * 6.229226112365723
Epoch 550, val loss: 0.9301033616065979
Epoch 560, training loss: 0.6626203656196594 = 0.04009680077433586 + 0.1 * 6.225235462188721
Epoch 560, val loss: 0.942449152469635
Epoch 570, training loss: 0.6615167856216431 = 0.0374789796769619 + 0.1 * 6.240377902984619
Epoch 570, val loss: 0.9546445608139038
Epoch 580, training loss: 0.656813383102417 = 0.035126421600580215 + 0.1 * 6.216869354248047
Epoch 580, val loss: 0.9665926098823547
Epoch 590, training loss: 0.6548826694488525 = 0.03299001604318619 + 0.1 * 6.218925952911377
Epoch 590, val loss: 0.9783833622932434
Epoch 600, training loss: 0.652652382850647 = 0.03104492463171482 + 0.1 * 6.216074466705322
Epoch 600, val loss: 0.9899715781211853
Epoch 610, training loss: 0.6501668095588684 = 0.02927454747259617 + 0.1 * 6.208922863006592
Epoch 610, val loss: 1.0014152526855469
Epoch 620, training loss: 0.6489134430885315 = 0.027653133496642113 + 0.1 * 6.212602615356445
Epoch 620, val loss: 1.012608528137207
Epoch 630, training loss: 0.6465036273002625 = 0.02616739459335804 + 0.1 * 6.203361988067627
Epoch 630, val loss: 1.0235451459884644
Epoch 640, training loss: 0.6456133127212524 = 0.024797610938549042 + 0.1 * 6.208156585693359
Epoch 640, val loss: 1.034285068511963
Epoch 650, training loss: 0.6442388296127319 = 0.023536089807748795 + 0.1 * 6.207027435302734
Epoch 650, val loss: 1.0448285341262817
Epoch 660, training loss: 0.6423146724700928 = 0.02237033285200596 + 0.1 * 6.199443340301514
Epoch 660, val loss: 1.05521821975708
Epoch 670, training loss: 0.6404610276222229 = 0.021290188655257225 + 0.1 * 6.191708087921143
Epoch 670, val loss: 1.0654511451721191
Epoch 680, training loss: 0.6401056051254272 = 0.02028726041316986 + 0.1 * 6.198183059692383
Epoch 680, val loss: 1.075510025024414
Epoch 690, training loss: 0.6391174793243408 = 0.019356025382876396 + 0.1 * 6.197614669799805
Epoch 690, val loss: 1.0853824615478516
Epoch 700, training loss: 0.6373177170753479 = 0.018489249050617218 + 0.1 * 6.188284873962402
Epoch 700, val loss: 1.095092535018921
Epoch 710, training loss: 0.6363609433174133 = 0.017681395635008812 + 0.1 * 6.186795234680176
Epoch 710, val loss: 1.1046415567398071
Epoch 720, training loss: 0.6356160044670105 = 0.01692722737789154 + 0.1 * 6.186887264251709
Epoch 720, val loss: 1.1139867305755615
Epoch 730, training loss: 0.6342849731445312 = 0.016223857179284096 + 0.1 * 6.1806111335754395
Epoch 730, val loss: 1.1231549978256226
Epoch 740, training loss: 0.6341232657432556 = 0.015564946457743645 + 0.1 * 6.185583591461182
Epoch 740, val loss: 1.1321609020233154
Epoch 750, training loss: 0.6328532099723816 = 0.014947428368031979 + 0.1 * 6.179057598114014
Epoch 750, val loss: 1.1409939527511597
Epoch 760, training loss: 0.6322211027145386 = 0.014367829076945782 + 0.1 * 6.178532600402832
Epoch 760, val loss: 1.149717926979065
Epoch 770, training loss: 0.6313916444778442 = 0.013821526430547237 + 0.1 * 6.175701141357422
Epoch 770, val loss: 1.1582071781158447
Epoch 780, training loss: 0.6303065419197083 = 0.013309351168572903 + 0.1 * 6.169971466064453
Epoch 780, val loss: 1.166562557220459
Epoch 790, training loss: 0.6302796006202698 = 0.012825467623770237 + 0.1 * 6.174540996551514
Epoch 790, val loss: 1.1747982501983643
Epoch 800, training loss: 0.629902184009552 = 0.012368378229439259 + 0.1 * 6.175337791442871
Epoch 800, val loss: 1.182843804359436
Epoch 810, training loss: 0.6284663081169128 = 0.011937499046325684 + 0.1 * 6.165287971496582
Epoch 810, val loss: 1.1907140016555786
Epoch 820, training loss: 0.6282374858856201 = 0.011530897580087185 + 0.1 * 6.1670660972595215
Epoch 820, val loss: 1.1985218524932861
Epoch 830, training loss: 0.6276891827583313 = 0.01114512886852026 + 0.1 * 6.165440559387207
Epoch 830, val loss: 1.206144094467163
Epoch 840, training loss: 0.6268726587295532 = 0.010781140998005867 + 0.1 * 6.160914897918701
Epoch 840, val loss: 1.2136553525924683
Epoch 850, training loss: 0.6274898052215576 = 0.010435177013278008 + 0.1 * 6.170546531677246
Epoch 850, val loss: 1.2210530042648315
Epoch 860, training loss: 0.6253750324249268 = 0.010106854140758514 + 0.1 * 6.152681350708008
Epoch 860, val loss: 1.2282967567443848
Epoch 870, training loss: 0.6253280639648438 = 0.009795447811484337 + 0.1 * 6.1553263664245605
Epoch 870, val loss: 1.2354602813720703
Epoch 880, training loss: 0.6255891919136047 = 0.009498669765889645 + 0.1 * 6.160905361175537
Epoch 880, val loss: 1.2425227165222168
Epoch 890, training loss: 0.6244476437568665 = 0.00921592302620411 + 0.1 * 6.152316570281982
Epoch 890, val loss: 1.2493815422058105
Epoch 900, training loss: 0.6238610744476318 = 0.008948549628257751 + 0.1 * 6.149125099182129
Epoch 900, val loss: 1.2561695575714111
Epoch 910, training loss: 0.6244028210639954 = 0.008692910894751549 + 0.1 * 6.157098770141602
Epoch 910, val loss: 1.2628329992294312
Epoch 920, training loss: 0.6236072182655334 = 0.00844825990498066 + 0.1 * 6.151589870452881
Epoch 920, val loss: 1.269354224205017
Epoch 930, training loss: 0.6233841180801392 = 0.008215618319809437 + 0.1 * 6.151684761047363
Epoch 930, val loss: 1.2757883071899414
Epoch 940, training loss: 0.6230174899101257 = 0.007993383333086967 + 0.1 * 6.150240898132324
Epoch 940, val loss: 1.2821460962295532
Epoch 950, training loss: 0.6216784715652466 = 0.007780348416417837 + 0.1 * 6.138980865478516
Epoch 950, val loss: 1.2884095907211304
Epoch 960, training loss: 0.6220616102218628 = 0.007576670963317156 + 0.1 * 6.1448493003845215
Epoch 960, val loss: 1.2946243286132812
Epoch 970, training loss: 0.6218029856681824 = 0.007380960509181023 + 0.1 * 6.144219875335693
Epoch 970, val loss: 1.3006871938705444
Epoch 980, training loss: 0.621353268623352 = 0.0071944319643080235 + 0.1 * 6.14158821105957
Epoch 980, val loss: 1.3066829442977905
Epoch 990, training loss: 0.6221861839294434 = 0.007015977054834366 + 0.1 * 6.151701927185059
Epoch 990, val loss: 1.3126099109649658
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.4000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.3764
Flip ASR: 0.3156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7713875770568848 = 1.934002161026001 + 0.1 * 8.37385368347168
Epoch 0, val loss: 1.9373207092285156
Epoch 10, training loss: 2.7608683109283447 = 1.9234992265701294 + 0.1 * 8.37369155883789
Epoch 10, val loss: 1.926727294921875
Epoch 20, training loss: 2.7477941513061523 = 1.9105128049850464 + 0.1 * 8.372814178466797
Epoch 20, val loss: 1.9137053489685059
Epoch 30, training loss: 2.7288320064544678 = 1.892163872718811 + 0.1 * 8.366682052612305
Epoch 30, val loss: 1.8955456018447876
Epoch 40, training loss: 2.6975111961364746 = 1.8654941320419312 + 0.1 * 8.320170402526855
Epoch 40, val loss: 1.8698276281356812
Epoch 50, training loss: 2.6248834133148193 = 1.8305927515029907 + 0.1 * 7.942905902862549
Epoch 50, val loss: 1.8374450206756592
Epoch 60, training loss: 2.5347673892974854 = 1.7960442304611206 + 0.1 * 7.387232303619385
Epoch 60, val loss: 1.8055776357650757
Epoch 70, training loss: 2.4709253311157227 = 1.7601486444473267 + 0.1 * 7.107766628265381
Epoch 70, val loss: 1.772209882736206
Epoch 80, training loss: 2.4157156944274902 = 1.7185477018356323 + 0.1 * 6.971680641174316
Epoch 80, val loss: 1.7340128421783447
Epoch 90, training loss: 2.3513171672821045 = 1.6647722721099854 + 0.1 * 6.865448474884033
Epoch 90, val loss: 1.68491792678833
Epoch 100, training loss: 2.271805763244629 = 1.5941461324691772 + 0.1 * 6.776597499847412
Epoch 100, val loss: 1.6247200965881348
Epoch 110, training loss: 2.1788437366485596 = 1.50722336769104 + 0.1 * 6.716202735900879
Epoch 110, val loss: 1.5544918775558472
Epoch 120, training loss: 2.078028678894043 = 1.410622000694275 + 0.1 * 6.67406702041626
Epoch 120, val loss: 1.477069616317749
Epoch 130, training loss: 1.9771941900253296 = 1.313185214996338 + 0.1 * 6.640089511871338
Epoch 130, val loss: 1.400227665901184
Epoch 140, training loss: 1.8824353218078613 = 1.2200534343719482 + 0.1 * 6.623819351196289
Epoch 140, val loss: 1.3283252716064453
Epoch 150, training loss: 1.793139934539795 = 1.1343965530395508 + 0.1 * 6.5874342918396
Epoch 150, val loss: 1.2639456987380981
Epoch 160, training loss: 1.707798957824707 = 1.0510066747665405 + 0.1 * 6.567923069000244
Epoch 160, val loss: 1.2018121480941772
Epoch 170, training loss: 1.6218767166137695 = 0.9666092991828918 + 0.1 * 6.552673816680908
Epoch 170, val loss: 1.1402523517608643
Epoch 180, training loss: 1.5363953113555908 = 0.88240647315979 + 0.1 * 6.53988790512085
Epoch 180, val loss: 1.0796830654144287
Epoch 190, training loss: 1.4542267322540283 = 0.8014622330665588 + 0.1 * 6.527644634246826
Epoch 190, val loss: 1.0220509767532349
Epoch 200, training loss: 1.378570318222046 = 0.7261665463447571 + 0.1 * 6.5240373611450195
Epoch 200, val loss: 0.9690129160881042
Epoch 210, training loss: 1.3097566366195679 = 0.6593235731124878 + 0.1 * 6.504330635070801
Epoch 210, val loss: 0.9233479499816895
Epoch 220, training loss: 1.2500035762786865 = 0.6008608937263489 + 0.1 * 6.491426944732666
Epoch 220, val loss: 0.8851304650306702
Epoch 230, training loss: 1.1994444131851196 = 0.550249457359314 + 0.1 * 6.491949558258057
Epoch 230, val loss: 0.8549689054489136
Epoch 240, training loss: 1.1537450551986694 = 0.5066037178039551 + 0.1 * 6.4714131355285645
Epoch 240, val loss: 0.832165002822876
Epoch 250, training loss: 1.1130521297454834 = 0.4675588607788086 + 0.1 * 6.454933166503906
Epoch 250, val loss: 0.8146947026252747
Epoch 260, training loss: 1.0775721073150635 = 0.4315658211708069 + 0.1 * 6.460062026977539
Epoch 260, val loss: 0.8015590310096741
Epoch 270, training loss: 1.0422859191894531 = 0.3980286121368408 + 0.1 * 6.442573070526123
Epoch 270, val loss: 0.7922108769416809
Epoch 280, training loss: 1.0087769031524658 = 0.36609041690826416 + 0.1 * 6.426865577697754
Epoch 280, val loss: 0.7852727174758911
Epoch 290, training loss: 0.9774489402770996 = 0.3355324864387512 + 0.1 * 6.419164657592773
Epoch 290, val loss: 0.7808095216751099
Epoch 300, training loss: 0.9475229978561401 = 0.3065012991428375 + 0.1 * 6.41021728515625
Epoch 300, val loss: 0.7787624001502991
Epoch 310, training loss: 0.9194021224975586 = 0.2793066203594208 + 0.1 * 6.4009552001953125
Epoch 310, val loss: 0.7791697978973389
Epoch 320, training loss: 0.8938686847686768 = 0.2541663348674774 + 0.1 * 6.397023677825928
Epoch 320, val loss: 0.7817061543464661
Epoch 330, training loss: 0.8697094917297363 = 0.23124603927135468 + 0.1 * 6.384634017944336
Epoch 330, val loss: 0.7862637042999268
Epoch 340, training loss: 0.8488813042640686 = 0.21053163707256317 + 0.1 * 6.383496284484863
Epoch 340, val loss: 0.7926830649375916
Epoch 350, training loss: 0.8310919404029846 = 0.19184543192386627 + 0.1 * 6.392465114593506
Epoch 350, val loss: 0.8007867336273193
Epoch 360, training loss: 0.8119275569915771 = 0.1750708818435669 + 0.1 * 6.368566513061523
Epoch 360, val loss: 0.810348629951477
Epoch 370, training loss: 0.7964906096458435 = 0.1599736362695694 + 0.1 * 6.365169525146484
Epoch 370, val loss: 0.8210580348968506
Epoch 380, training loss: 0.7820753455162048 = 0.1463748663663864 + 0.1 * 6.357004642486572
Epoch 380, val loss: 0.8329057693481445
Epoch 390, training loss: 0.7698366045951843 = 0.1340789794921875 + 0.1 * 6.357576370239258
Epoch 390, val loss: 0.8456820249557495
Epoch 400, training loss: 0.7582794427871704 = 0.12300652265548706 + 0.1 * 6.352728843688965
Epoch 400, val loss: 0.859200656414032
Epoch 410, training loss: 0.7475318312644958 = 0.11306191980838776 + 0.1 * 6.344698905944824
Epoch 410, val loss: 0.8732742071151733
Epoch 420, training loss: 0.7377830743789673 = 0.10411810129880905 + 0.1 * 6.336649417877197
Epoch 420, val loss: 0.887862503528595
Epoch 430, training loss: 0.7294428944587708 = 0.09602806717157364 + 0.1 * 6.334147930145264
Epoch 430, val loss: 0.9028444886207581
Epoch 440, training loss: 0.7212637066841125 = 0.0887046828866005 + 0.1 * 6.325589656829834
Epoch 440, val loss: 0.9180416464805603
Epoch 450, training loss: 0.7143005132675171 = 0.0820646733045578 + 0.1 * 6.322358131408691
Epoch 450, val loss: 0.9334642887115479
Epoch 460, training loss: 0.7105721235275269 = 0.07602474838495255 + 0.1 * 6.345473289489746
Epoch 460, val loss: 0.9490463733673096
Epoch 470, training loss: 0.7021414637565613 = 0.0705554410815239 + 0.1 * 6.315859794616699
Epoch 470, val loss: 0.9644519686698914
Epoch 480, training loss: 0.6966836452484131 = 0.06557664275169373 + 0.1 * 6.311070442199707
Epoch 480, val loss: 0.9798862934112549
Epoch 490, training loss: 0.6918169856071472 = 0.0610467828810215 + 0.1 * 6.30770206451416
Epoch 490, val loss: 0.9952489733695984
Epoch 500, training loss: 0.6877692937850952 = 0.056914880871772766 + 0.1 * 6.308544158935547
Epoch 500, val loss: 1.0104210376739502
Epoch 510, training loss: 0.6832382082939148 = 0.05314740911126137 + 0.1 * 6.300907611846924
Epoch 510, val loss: 1.0255672931671143
Epoch 520, training loss: 0.6800900101661682 = 0.0496978722512722 + 0.1 * 6.303921222686768
Epoch 520, val loss: 1.040510654449463
Epoch 530, training loss: 0.6755167841911316 = 0.04654319956898689 + 0.1 * 6.289735794067383
Epoch 530, val loss: 1.0552922487258911
Epoch 540, training loss: 0.6726305484771729 = 0.043650008738040924 + 0.1 * 6.289804935455322
Epoch 540, val loss: 1.0698312520980835
Epoch 550, training loss: 0.669674813747406 = 0.040997765958309174 + 0.1 * 6.286770820617676
Epoch 550, val loss: 1.0841516256332397
Epoch 560, training loss: 0.6665198802947998 = 0.03855830430984497 + 0.1 * 6.279615879058838
Epoch 560, val loss: 1.0981711149215698
Epoch 570, training loss: 0.6643407940864563 = 0.03631500527262688 + 0.1 * 6.280257701873779
Epoch 570, val loss: 1.1119675636291504
Epoch 580, training loss: 0.6610245704650879 = 0.03424767032265663 + 0.1 * 6.267768383026123
Epoch 580, val loss: 1.1255131959915161
Epoch 590, training loss: 0.661941647529602 = 0.03233646973967552 + 0.1 * 6.296051502227783
Epoch 590, val loss: 1.1387511491775513
Epoch 600, training loss: 0.6576781272888184 = 0.03057512640953064 + 0.1 * 6.271029949188232
Epoch 600, val loss: 1.151656150817871
Epoch 610, training loss: 0.6560211777687073 = 0.028942935168743134 + 0.1 * 6.270781993865967
Epoch 610, val loss: 1.1643682718276978
Epoch 620, training loss: 0.6537275314331055 = 0.02743261307477951 + 0.1 * 6.262948989868164
Epoch 620, val loss: 1.1767511367797852
Epoch 630, training loss: 0.6515076756477356 = 0.026026854291558266 + 0.1 * 6.254807949066162
Epoch 630, val loss: 1.1888587474822998
Epoch 640, training loss: 0.6505277156829834 = 0.024719657376408577 + 0.1 * 6.25808048248291
Epoch 640, val loss: 1.2007575035095215
Epoch 650, training loss: 0.648202121257782 = 0.023503577336668968 + 0.1 * 6.246984958648682
Epoch 650, val loss: 1.2124838829040527
Epoch 660, training loss: 0.6463775634765625 = 0.022369345650076866 + 0.1 * 6.240082263946533
Epoch 660, val loss: 1.2239623069763184
Epoch 670, training loss: 0.6459018588066101 = 0.021309535950422287 + 0.1 * 6.245923042297363
Epoch 670, val loss: 1.2351768016815186
Epoch 680, training loss: 0.6462600231170654 = 0.020318618044257164 + 0.1 * 6.259413719177246
Epoch 680, val loss: 1.2461636066436768
Epoch 690, training loss: 0.6434820890426636 = 0.019392644986510277 + 0.1 * 6.240893840789795
Epoch 690, val loss: 1.2568590641021729
Epoch 700, training loss: 0.642341136932373 = 0.01852591149508953 + 0.1 * 6.238152027130127
Epoch 700, val loss: 1.2674542665481567
Epoch 710, training loss: 0.6436229944229126 = 0.017715130001306534 + 0.1 * 6.259078502655029
Epoch 710, val loss: 1.2778416872024536
Epoch 720, training loss: 0.6394148468971252 = 0.016955208033323288 + 0.1 * 6.2245965003967285
Epoch 720, val loss: 1.2878351211547852
Epoch 730, training loss: 0.6377777457237244 = 0.01624155603349209 + 0.1 * 6.215362071990967
Epoch 730, val loss: 1.297715187072754
Epoch 740, training loss: 0.6373210549354553 = 0.015570628456771374 + 0.1 * 6.217504024505615
Epoch 740, val loss: 1.3074891567230225
Epoch 750, training loss: 0.6370561718940735 = 0.014939184300601482 + 0.1 * 6.221169948577881
Epoch 750, val loss: 1.31682288646698
Epoch 760, training loss: 0.6367504596710205 = 0.014344503171741962 + 0.1 * 6.224059104919434
Epoch 760, val loss: 1.3259119987487793
Epoch 770, training loss: 0.6351811289787292 = 0.013785197399556637 + 0.1 * 6.213959217071533
Epoch 770, val loss: 1.3349887132644653
Epoch 780, training loss: 0.6337956786155701 = 0.013258201070129871 + 0.1 * 6.205374717712402
Epoch 780, val loss: 1.343840479850769
Epoch 790, training loss: 0.6349043846130371 = 0.012760740704834461 + 0.1 * 6.221436023712158
Epoch 790, val loss: 1.3525525331497192
Epoch 800, training loss: 0.6328855752944946 = 0.01229184865951538 + 0.1 * 6.205936908721924
Epoch 800, val loss: 1.3609473705291748
Epoch 810, training loss: 0.6330894827842712 = 0.011848079040646553 + 0.1 * 6.212413787841797
Epoch 810, val loss: 1.3692042827606201
Epoch 820, training loss: 0.6318218111991882 = 0.011428629979491234 + 0.1 * 6.2039313316345215
Epoch 820, val loss: 1.377242922782898
Epoch 830, training loss: 0.6307186484336853 = 0.01103210635483265 + 0.1 * 6.196865081787109
Epoch 830, val loss: 1.3852092027664185
Epoch 840, training loss: 0.631700336933136 = 0.010656056925654411 + 0.1 * 6.210442543029785
Epoch 840, val loss: 1.3929706811904907
Epoch 850, training loss: 0.6296612024307251 = 0.01029958389699459 + 0.1 * 6.1936163902282715
Epoch 850, val loss: 1.4005202054977417
Epoch 860, training loss: 0.6293078064918518 = 0.009961037896573544 + 0.1 * 6.19346809387207
Epoch 860, val loss: 1.4079509973526
Epoch 870, training loss: 0.62889564037323 = 0.009639221243560314 + 0.1 * 6.192564487457275
Epoch 870, val loss: 1.4151400327682495
Epoch 880, training loss: 0.6298708915710449 = 0.009333470836281776 + 0.1 * 6.205373764038086
Epoch 880, val loss: 1.422186255455017
Epoch 890, training loss: 0.6279446482658386 = 0.009042942896485329 + 0.1 * 6.189017295837402
Epoch 890, val loss: 1.4291318655014038
Epoch 900, training loss: 0.6272416710853577 = 0.008767358027398586 + 0.1 * 6.1847429275512695
Epoch 900, val loss: 1.4359856843948364
Epoch 910, training loss: 0.6284489631652832 = 0.008504432626068592 + 0.1 * 6.199444770812988
Epoch 910, val loss: 1.4426052570343018
Epoch 920, training loss: 0.627082884311676 = 0.008253894746303558 + 0.1 * 6.188290119171143
Epoch 920, val loss: 1.44911789894104
Epoch 930, training loss: 0.6260203123092651 = 0.008014757186174393 + 0.1 * 6.180055618286133
Epoch 930, val loss: 1.4554420709609985
Epoch 940, training loss: 0.6252520084381104 = 0.007786081172525883 + 0.1 * 6.17465877532959
Epoch 940, val loss: 1.4617373943328857
Epoch 950, training loss: 0.6256245374679565 = 0.007567168213427067 + 0.1 * 6.180573463439941
Epoch 950, val loss: 1.4677879810333252
Epoch 960, training loss: 0.6256771087646484 = 0.007358282804489136 + 0.1 * 6.183187961578369
Epoch 960, val loss: 1.4738290309906006
Epoch 970, training loss: 0.6244134306907654 = 0.007158283609896898 + 0.1 * 6.17255163192749
Epoch 970, val loss: 1.479740858078003
Epoch 980, training loss: 0.623701274394989 = 0.006966570392251015 + 0.1 * 6.167346954345703
Epoch 980, val loss: 1.4855564832687378
Epoch 990, training loss: 0.6237306594848633 = 0.006782574579119682 + 0.1 * 6.169480800628662
Epoch 990, val loss: 1.4912197589874268
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7306
Flip ASR: 0.6889/225 nodes
The final ASR:0.56212, 0.14513, Accuracy:0.81111, 0.00000
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11564])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10462])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97417, 0.00522, Accuracy:0.83210, 0.01145
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7795486450195312 = 1.9421662092208862 + 0.1 * 8.373824119567871
Epoch 0, val loss: 1.939463496208191
Epoch 10, training loss: 2.7691242694854736 = 1.9317593574523926 + 0.1 * 8.373649597167969
Epoch 10, val loss: 1.9298739433288574
Epoch 20, training loss: 2.755923271179199 = 1.918634295463562 + 0.1 * 8.37289047241211
Epoch 20, val loss: 1.9174284934997559
Epoch 30, training loss: 2.737074613571167 = 1.9002517461776733 + 0.1 * 8.368228912353516
Epoch 30, val loss: 1.8996937274932861
Epoch 40, training loss: 2.706683874130249 = 1.873418927192688 + 0.1 * 8.332649230957031
Epoch 40, val loss: 1.8740211725234985
Epoch 50, training loss: 2.637470006942749 = 1.837884783744812 + 0.1 * 7.995852947235107
Epoch 50, val loss: 1.8418960571289062
Epoch 60, training loss: 2.5357894897460938 = 1.8051819801330566 + 0.1 * 7.3060760498046875
Epoch 60, val loss: 1.8148961067199707
Epoch 70, training loss: 2.4647445678710938 = 1.7767010927200317 + 0.1 * 6.880434513092041
Epoch 70, val loss: 1.7918168306350708
Epoch 80, training loss: 2.417072296142578 = 1.7451274394989014 + 0.1 * 6.719447612762451
Epoch 80, val loss: 1.7658435106277466
Epoch 90, training loss: 2.3678882122039795 = 1.7033467292785645 + 0.1 * 6.645415306091309
Epoch 90, val loss: 1.7299562692642212
Epoch 100, training loss: 2.307321786880493 = 1.6466010808944702 + 0.1 * 6.607206344604492
Epoch 100, val loss: 1.681544542312622
Epoch 110, training loss: 2.232393503189087 = 1.5745518207550049 + 0.1 * 6.57841682434082
Epoch 110, val loss: 1.6220110654830933
Epoch 120, training loss: 2.1482579708099365 = 1.4924834966659546 + 0.1 * 6.557744979858398
Epoch 120, val loss: 1.5553325414657593
Epoch 130, training loss: 2.0613245964050293 = 1.4071019887924194 + 0.1 * 6.542226791381836
Epoch 130, val loss: 1.4875978231430054
Epoch 140, training loss: 1.9744946956634521 = 1.321768879890442 + 0.1 * 6.52725887298584
Epoch 140, val loss: 1.4224004745483398
Epoch 150, training loss: 1.8880302906036377 = 1.2368329763412476 + 0.1 * 6.511972904205322
Epoch 150, val loss: 1.3600114583969116
Epoch 160, training loss: 1.8039772510528564 = 1.154363751411438 + 0.1 * 6.496134281158447
Epoch 160, val loss: 1.3014426231384277
Epoch 170, training loss: 1.7257983684539795 = 1.0773756504058838 + 0.1 * 6.484226226806641
Epoch 170, val loss: 1.2482390403747559
Epoch 180, training loss: 1.654724359512329 = 1.0076946020126343 + 0.1 * 6.470297336578369
Epoch 180, val loss: 1.2014673948287964
Epoch 190, training loss: 1.5907965898513794 = 0.9443261027336121 + 0.1 * 6.464704990386963
Epoch 190, val loss: 1.1589434146881104
Epoch 200, training loss: 1.5315055847167969 = 0.8859283328056335 + 0.1 * 6.455772876739502
Epoch 200, val loss: 1.1196457147598267
Epoch 210, training loss: 1.4731786251068115 = 0.8288406133651733 + 0.1 * 6.443379878997803
Epoch 210, val loss: 1.080443263053894
Epoch 220, training loss: 1.4131567478179932 = 0.7696914076805115 + 0.1 * 6.434653282165527
Epoch 220, val loss: 1.0383878946304321
Epoch 230, training loss: 1.3502246141433716 = 0.7073720097541809 + 0.1 * 6.428525924682617
Epoch 230, val loss: 0.992950439453125
Epoch 240, training loss: 1.2857801914215088 = 0.6435793042182922 + 0.1 * 6.422008991241455
Epoch 240, val loss: 0.945612907409668
Epoch 250, training loss: 1.223406195640564 = 0.5801171660423279 + 0.1 * 6.432889938354492
Epoch 250, val loss: 0.8987504839897156
Epoch 260, training loss: 1.160973310470581 = 0.5202556848526001 + 0.1 * 6.407176494598389
Epoch 260, val loss: 0.8563151359558105
Epoch 270, training loss: 1.1052255630493164 = 0.4652576744556427 + 0.1 * 6.399679183959961
Epoch 270, val loss: 0.8204318284988403
Epoch 280, training loss: 1.055147647857666 = 0.41576123237609863 + 0.1 * 6.393863677978516
Epoch 280, val loss: 0.792036235332489
Epoch 290, training loss: 1.009769082069397 = 0.37143486738204956 + 0.1 * 6.383342266082764
Epoch 290, val loss: 0.7700873017311096
Epoch 300, training loss: 0.9695339798927307 = 0.3315659761428833 + 0.1 * 6.379680156707764
Epoch 300, val loss: 0.7534416913986206
Epoch 310, training loss: 0.9326097965240479 = 0.29578256607055664 + 0.1 * 6.368272304534912
Epoch 310, val loss: 0.7411841750144958
Epoch 320, training loss: 0.901131272315979 = 0.26337310671806335 + 0.1 * 6.37758207321167
Epoch 320, val loss: 0.7324059009552002
Epoch 330, training loss: 0.8696346879005432 = 0.23422753810882568 + 0.1 * 6.354071617126465
Epoch 330, val loss: 0.7269507050514221
Epoch 340, training loss: 0.8427691459655762 = 0.20807810127735138 + 0.1 * 6.34691047668457
Epoch 340, val loss: 0.7244874835014343
Epoch 350, training loss: 0.8199620246887207 = 0.18493011593818665 + 0.1 * 6.350318431854248
Epoch 350, val loss: 0.7247676849365234
Epoch 360, training loss: 0.7982926964759827 = 0.16464102268218994 + 0.1 * 6.336516857147217
Epoch 360, val loss: 0.7276396155357361
Epoch 370, training loss: 0.7803776264190674 = 0.14687487483024597 + 0.1 * 6.335027694702148
Epoch 370, val loss: 0.7328454852104187
Epoch 380, training loss: 0.7635402679443359 = 0.1314968466758728 + 0.1 * 6.320434093475342
Epoch 380, val loss: 0.7399723529815674
Epoch 390, training loss: 0.7498080730438232 = 0.1181643158197403 + 0.1 * 6.316437721252441
Epoch 390, val loss: 0.7487948536872864
Epoch 400, training loss: 0.7391149997711182 = 0.10658721625804901 + 0.1 * 6.325277328491211
Epoch 400, val loss: 0.7588092684745789
Epoch 410, training loss: 0.7272833585739136 = 0.09652892500162125 + 0.1 * 6.307543754577637
Epoch 410, val loss: 0.7698315382003784
Epoch 420, training loss: 0.7187353372573853 = 0.08770651370286942 + 0.1 * 6.310287952423096
Epoch 420, val loss: 0.7814700603485107
Epoch 430, training loss: 0.7100352048873901 = 0.07996401190757751 + 0.1 * 6.300711631774902
Epoch 430, val loss: 0.7935739755630493
Epoch 440, training loss: 0.7023720741271973 = 0.07314574718475342 + 0.1 * 6.292263031005859
Epoch 440, val loss: 0.8057694435119629
Epoch 450, training loss: 0.6955544948577881 = 0.0671226978302002 + 0.1 * 6.284317970275879
Epoch 450, val loss: 0.8181310296058655
Epoch 460, training loss: 0.6899910569190979 = 0.061751838773489 + 0.1 * 6.2823920249938965
Epoch 460, val loss: 0.830405056476593
Epoch 470, training loss: 0.6852442026138306 = 0.056961458176374435 + 0.1 * 6.282826900482178
Epoch 470, val loss: 0.8427347540855408
Epoch 480, training loss: 0.6808657646179199 = 0.052682556211948395 + 0.1 * 6.281831741333008
Epoch 480, val loss: 0.8547642827033997
Epoch 490, training loss: 0.675278902053833 = 0.048872917890548706 + 0.1 * 6.264060020446777
Epoch 490, val loss: 0.866816520690918
Epoch 500, training loss: 0.6714211702346802 = 0.0454423725605011 + 0.1 * 6.2597880363464355
Epoch 500, val loss: 0.8785678148269653
Epoch 510, training loss: 0.6682108044624329 = 0.04234326258301735 + 0.1 * 6.2586750984191895
Epoch 510, val loss: 0.8903040289878845
Epoch 520, training loss: 0.6661711931228638 = 0.03954160958528519 + 0.1 * 6.266295909881592
Epoch 520, val loss: 0.9016561508178711
Epoch 530, training loss: 0.6618261933326721 = 0.037016283720731735 + 0.1 * 6.248099327087402
Epoch 530, val loss: 0.913008451461792
Epoch 540, training loss: 0.6609079241752625 = 0.034713029861450195 + 0.1 * 6.261948585510254
Epoch 540, val loss: 0.923900306224823
Epoch 550, training loss: 0.6569263339042664 = 0.03262149915099144 + 0.1 * 6.243048191070557
Epoch 550, val loss: 0.9347631931304932
Epoch 560, training loss: 0.6550021171569824 = 0.030707938596606255 + 0.1 * 6.242941379547119
Epoch 560, val loss: 0.9453116059303284
Epoch 570, training loss: 0.6527318954467773 = 0.028958549723029137 + 0.1 * 6.237733364105225
Epoch 570, val loss: 0.9555790424346924
Epoch 580, training loss: 0.6503890156745911 = 0.027362272143363953 + 0.1 * 6.23026704788208
Epoch 580, val loss: 0.9657443761825562
Epoch 590, training loss: 0.6488149166107178 = 0.02588837966322899 + 0.1 * 6.229265213012695
Epoch 590, val loss: 0.9755939841270447
Epoch 600, training loss: 0.6473109126091003 = 0.024532975628972054 + 0.1 * 6.227779388427734
Epoch 600, val loss: 0.9851658344268799
Epoch 610, training loss: 0.6453834176063538 = 0.023284409195184708 + 0.1 * 6.22098970413208
Epoch 610, val loss: 0.9946537613868713
Epoch 620, training loss: 0.6442165374755859 = 0.022127235308289528 + 0.1 * 6.220893383026123
Epoch 620, val loss: 1.0037976503372192
Epoch 630, training loss: 0.6429988741874695 = 0.02105901576578617 + 0.1 * 6.219398021697998
Epoch 630, val loss: 1.012893795967102
Epoch 640, training loss: 0.6421585083007812 = 0.020064309239387512 + 0.1 * 6.22094202041626
Epoch 640, val loss: 1.0214440822601318
Epoch 650, training loss: 0.6401487588882446 = 0.01915188506245613 + 0.1 * 6.2099690437316895
Epoch 650, val loss: 1.0301560163497925
Epoch 660, training loss: 0.6400818824768066 = 0.01829749159514904 + 0.1 * 6.217844009399414
Epoch 660, val loss: 1.0383508205413818
Epoch 670, training loss: 0.638276219367981 = 0.01750442385673523 + 0.1 * 6.207718372344971
Epoch 670, val loss: 1.0465087890625
Epoch 680, training loss: 0.6368550658226013 = 0.016761377453804016 + 0.1 * 6.200936794281006
Epoch 680, val loss: 1.0544676780700684
Epoch 690, training loss: 0.6357326507568359 = 0.016065483912825584 + 0.1 * 6.196671485900879
Epoch 690, val loss: 1.0621464252471924
Epoch 700, training loss: 0.6347709894180298 = 0.015417246147990227 + 0.1 * 6.193537712097168
Epoch 700, val loss: 1.06989586353302
Epoch 710, training loss: 0.6339454054832458 = 0.014805774204432964 + 0.1 * 6.191396236419678
Epoch 710, val loss: 1.0773499011993408
Epoch 720, training loss: 0.6351816058158875 = 0.014230940490961075 + 0.1 * 6.209506511688232
Epoch 720, val loss: 1.0845599174499512
Epoch 730, training loss: 0.6333103179931641 = 0.013694310560822487 + 0.1 * 6.196160316467285
Epoch 730, val loss: 1.0917731523513794
Epoch 740, training loss: 0.6321792006492615 = 0.01318947970867157 + 0.1 * 6.189897060394287
Epoch 740, val loss: 1.0987516641616821
Epoch 750, training loss: 0.6321523785591125 = 0.012714195065200329 + 0.1 * 6.1943817138671875
Epoch 750, val loss: 1.105661153793335
Epoch 760, training loss: 0.6305876970291138 = 0.012266119010746479 + 0.1 * 6.183215618133545
Epoch 760, val loss: 1.1123727560043335
Epoch 770, training loss: 0.63015216588974 = 0.011843277141451836 + 0.1 * 6.183088779449463
Epoch 770, val loss: 1.1190593242645264
Epoch 780, training loss: 0.629081666469574 = 0.01144077256321907 + 0.1 * 6.176408767700195
Epoch 780, val loss: 1.125378966331482
Epoch 790, training loss: 0.6308466792106628 = 0.011061816476285458 + 0.1 * 6.197848796844482
Epoch 790, val loss: 1.131715178489685
Epoch 800, training loss: 0.6283932328224182 = 0.010703074745833874 + 0.1 * 6.176901340484619
Epoch 800, val loss: 1.1379011869430542
Epoch 810, training loss: 0.6276209950447083 = 0.010363719426095486 + 0.1 * 6.172572612762451
Epoch 810, val loss: 1.1441174745559692
Epoch 820, training loss: 0.628183901309967 = 0.010038967244327068 + 0.1 * 6.1814494132995605
Epoch 820, val loss: 1.149931788444519
Epoch 830, training loss: 0.6267144083976746 = 0.009733621031045914 + 0.1 * 6.1698079109191895
Epoch 830, val loss: 1.1558687686920166
Epoch 840, training loss: 0.6265245079994202 = 0.009441748261451721 + 0.1 * 6.170827388763428
Epoch 840, val loss: 1.1616671085357666
Epoch 850, training loss: 0.6263482570648193 = 0.009162502363324165 + 0.1 * 6.171857833862305
Epoch 850, val loss: 1.1671686172485352
Epoch 860, training loss: 0.6265363693237305 = 0.008898144587874413 + 0.1 * 6.176382064819336
Epoch 860, val loss: 1.1726582050323486
Epoch 870, training loss: 0.6248056292533875 = 0.008646427653729916 + 0.1 * 6.16159200668335
Epoch 870, val loss: 1.1782124042510986
Epoch 880, training loss: 0.6251794695854187 = 0.008405686356127262 + 0.1 * 6.16773796081543
Epoch 880, val loss: 1.183583378791809
Epoch 890, training loss: 0.6243076324462891 = 0.008174998685717583 + 0.1 * 6.161325931549072
Epoch 890, val loss: 1.188618779182434
Epoch 900, training loss: 0.625632107257843 = 0.007956147193908691 + 0.1 * 6.176759719848633
Epoch 900, val loss: 1.193809151649475
Epoch 910, training loss: 0.6236125230789185 = 0.007746156770735979 + 0.1 * 6.158663749694824
Epoch 910, val loss: 1.1988400220870972
Epoch 920, training loss: 0.623023509979248 = 0.00754608865827322 + 0.1 * 6.154774188995361
Epoch 920, val loss: 1.2038867473602295
Epoch 930, training loss: 0.6244081258773804 = 0.007353086490184069 + 0.1 * 6.170550346374512
Epoch 930, val loss: 1.20869779586792
Epoch 940, training loss: 0.6224958896636963 = 0.007168331183493137 + 0.1 * 6.153275012969971
Epoch 940, val loss: 1.2133289575576782
Epoch 950, training loss: 0.6224162578582764 = 0.006992815528064966 + 0.1 * 6.154234409332275
Epoch 950, val loss: 1.2181795835494995
Epoch 960, training loss: 0.6215624809265137 = 0.0068222712725400925 + 0.1 * 6.147401809692383
Epoch 960, val loss: 1.222669005393982
Epoch 970, training loss: 0.6213890910148621 = 0.0066604819148778915 + 0.1 * 6.147286415100098
Epoch 970, val loss: 1.2273391485214233
Epoch 980, training loss: 0.6228087544441223 = 0.006503165699541569 + 0.1 * 6.163055896759033
Epoch 980, val loss: 1.2316358089447021
Epoch 990, training loss: 0.62136310338974 = 0.0063537852838635445 + 0.1 * 6.150093078613281
Epoch 990, val loss: 1.2360689640045166
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7011
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.800811767578125 = 1.9634335041046143 + 0.1 * 8.373781204223633
Epoch 0, val loss: 1.9629286527633667
Epoch 10, training loss: 2.790449619293213 = 1.9531022310256958 + 0.1 * 8.373475074768066
Epoch 10, val loss: 1.9526233673095703
Epoch 20, training loss: 2.7779858112335205 = 1.9408066272735596 + 0.1 * 8.371790885925293
Epoch 20, val loss: 1.9397507905960083
Epoch 30, training loss: 2.7602226734161377 = 1.9238498210906982 + 0.1 * 8.363728523254395
Epoch 30, val loss: 1.9212762117385864
Epoch 40, training loss: 2.7299656867980957 = 1.898823857307434 + 0.1 * 8.311418533325195
Epoch 40, val loss: 1.8937764167785645
Epoch 50, training loss: 2.6353390216827393 = 1.864382028579712 + 0.1 * 7.709569931030273
Epoch 50, val loss: 1.8576200008392334
Epoch 60, training loss: 2.5417141914367676 = 1.828161358833313 + 0.1 * 7.135529518127441
Epoch 60, val loss: 1.8212591409683228
Epoch 70, training loss: 2.4804508686065674 = 1.791110873222351 + 0.1 * 6.893399715423584
Epoch 70, val loss: 1.7852641344070435
Epoch 80, training loss: 2.429086208343506 = 1.7527774572372437 + 0.1 * 6.763086318969727
Epoch 80, val loss: 1.7507394552230835
Epoch 90, training loss: 2.3806285858154297 = 1.7119665145874023 + 0.1 * 6.686620235443115
Epoch 90, val loss: 1.7155005931854248
Epoch 100, training loss: 2.323026418685913 = 1.6579571962356567 + 0.1 * 6.650691509246826
Epoch 100, val loss: 1.6703970432281494
Epoch 110, training loss: 2.2494421005249023 = 1.5862525701522827 + 0.1 * 6.631895542144775
Epoch 110, val loss: 1.6120262145996094
Epoch 120, training loss: 2.1570186614990234 = 1.4954502582550049 + 0.1 * 6.615683078765869
Epoch 120, val loss: 1.538723111152649
Epoch 130, training loss: 2.050196409225464 = 1.3900107145309448 + 0.1 * 6.601856231689453
Epoch 130, val loss: 1.4545053243637085
Epoch 140, training loss: 1.93690824508667 = 1.2783113718032837 + 0.1 * 6.585968494415283
Epoch 140, val loss: 1.3668283224105835
Epoch 150, training loss: 1.8218622207641602 = 1.1651673316955566 + 0.1 * 6.566949367523193
Epoch 150, val loss: 1.2799171209335327
Epoch 160, training loss: 1.7090550661087036 = 1.0538830757141113 + 0.1 * 6.551719665527344
Epoch 160, val loss: 1.1951439380645752
Epoch 170, training loss: 1.6032123565673828 = 0.9501425623893738 + 0.1 * 6.530697345733643
Epoch 170, val loss: 1.118059515953064
Epoch 180, training loss: 1.508143424987793 = 0.8567706942558289 + 0.1 * 6.51372766494751
Epoch 180, val loss: 1.0512661933898926
Epoch 190, training loss: 1.4259051084518433 = 0.7758665084838867 + 0.1 * 6.500385761260986
Epoch 190, val loss: 0.9957566857337952
Epoch 200, training loss: 1.3553228378295898 = 0.7068468332290649 + 0.1 * 6.48475980758667
Epoch 200, val loss: 0.9512363076210022
Epoch 210, training loss: 1.2950730323791504 = 0.6469622254371643 + 0.1 * 6.481107711791992
Epoch 210, val loss: 0.9156328439712524
Epoch 220, training loss: 1.2419312000274658 = 0.5954341888427734 + 0.1 * 6.464970588684082
Epoch 220, val loss: 0.8876623511314392
Epoch 230, training loss: 1.1948575973510742 = 0.549660325050354 + 0.1 * 6.451972961425781
Epoch 230, val loss: 0.8649423122406006
Epoch 240, training loss: 1.151890754699707 = 0.5078523755073547 + 0.1 * 6.440384387969971
Epoch 240, val loss: 0.8463625907897949
Epoch 250, training loss: 1.1123093366622925 = 0.4690537750720978 + 0.1 * 6.432555198669434
Epoch 250, val loss: 0.8312917351722717
Epoch 260, training loss: 1.0759923458099365 = 0.43307068943977356 + 0.1 * 6.429215908050537
Epoch 260, val loss: 0.8189015984535217
Epoch 270, training loss: 1.0416593551635742 = 0.39960458874702454 + 0.1 * 6.420547962188721
Epoch 270, val loss: 0.8087632060050964
Epoch 280, training loss: 1.0104403495788574 = 0.3681866526603699 + 0.1 * 6.422537326812744
Epoch 280, val loss: 0.8008566498756409
Epoch 290, training loss: 0.9796111583709717 = 0.3389897644519806 + 0.1 * 6.406213760375977
Epoch 290, val loss: 0.7949458956718445
Epoch 300, training loss: 0.9513005614280701 = 0.31180739402770996 + 0.1 * 6.394931793212891
Epoch 300, val loss: 0.7909717559814453
Epoch 310, training loss: 0.9270458817481995 = 0.2866491675376892 + 0.1 * 6.403966903686523
Epoch 310, val loss: 0.7891198992729187
Epoch 320, training loss: 0.9019193649291992 = 0.26361283659935 + 0.1 * 6.383065700531006
Epoch 320, val loss: 0.7890499234199524
Epoch 330, training loss: 0.8802124261856079 = 0.24232476949691772 + 0.1 * 6.378876209259033
Epoch 330, val loss: 0.7907999753952026
Epoch 340, training loss: 0.8603422045707703 = 0.22260941565036774 + 0.1 * 6.377327919006348
Epoch 340, val loss: 0.7942080497741699
Epoch 350, training loss: 0.8402734398841858 = 0.2043381929397583 + 0.1 * 6.359352111816406
Epoch 350, val loss: 0.7989448308944702
Epoch 360, training loss: 0.8248317241668701 = 0.18736238777637482 + 0.1 * 6.374693393707275
Epoch 360, val loss: 0.8049025535583496
Epoch 370, training loss: 0.806334376335144 = 0.17160722613334656 + 0.1 * 6.347271919250488
Epoch 370, val loss: 0.8117138743400574
Epoch 380, training loss: 0.790879487991333 = 0.15698882937431335 + 0.1 * 6.338906764984131
Epoch 380, val loss: 0.8194675445556641
Epoch 390, training loss: 0.7798067331314087 = 0.143367737531662 + 0.1 * 6.364389419555664
Epoch 390, val loss: 0.8282617330551147
Epoch 400, training loss: 0.7652003169059753 = 0.13076502084732056 + 0.1 * 6.344352722167969
Epoch 400, val loss: 0.8376328945159912
Epoch 410, training loss: 0.7518919110298157 = 0.11906943470239639 + 0.1 * 6.328224182128906
Epoch 410, val loss: 0.8476032018661499
Epoch 420, training loss: 0.7399433255195618 = 0.10813025385141373 + 0.1 * 6.3181304931640625
Epoch 420, val loss: 0.8580648303031921
Epoch 430, training loss: 0.7292641401290894 = 0.09786614775657654 + 0.1 * 6.3139801025390625
Epoch 430, val loss: 0.8688303828239441
Epoch 440, training loss: 0.719290554523468 = 0.08849485218524933 + 0.1 * 6.307957172393799
Epoch 440, val loss: 0.880521297454834
Epoch 450, training loss: 0.7110122442245483 = 0.08036035299301147 + 0.1 * 6.3065185546875
Epoch 450, val loss: 0.8928630948066711
Epoch 460, training loss: 0.7027581930160522 = 0.07321784645318985 + 0.1 * 6.295403003692627
Epoch 460, val loss: 0.9064468145370483
Epoch 470, training loss: 0.6968374252319336 = 0.06690619885921478 + 0.1 * 6.299312114715576
Epoch 470, val loss: 0.9198581576347351
Epoch 480, training loss: 0.6896737813949585 = 0.06129985675215721 + 0.1 * 6.28373908996582
Epoch 480, val loss: 0.9328450560569763
Epoch 490, training loss: 0.6854813694953918 = 0.05631154403090477 + 0.1 * 6.291697978973389
Epoch 490, val loss: 0.9457123279571533
Epoch 500, training loss: 0.6797158718109131 = 0.05189216881990433 + 0.1 * 6.2782368659973145
Epoch 500, val loss: 0.9585477709770203
Epoch 510, training loss: 0.6750176548957825 = 0.04796301946043968 + 0.1 * 6.2705464363098145
Epoch 510, val loss: 0.9713355302810669
Epoch 520, training loss: 0.6717295050621033 = 0.04446272924542427 + 0.1 * 6.272667407989502
Epoch 520, val loss: 0.9838200211524963
Epoch 530, training loss: 0.6678003668785095 = 0.04134295880794525 + 0.1 * 6.26457405090332
Epoch 530, val loss: 0.9959414005279541
Epoch 540, training loss: 0.6644617319107056 = 0.03853432089090347 + 0.1 * 6.259273529052734
Epoch 540, val loss: 1.007783055305481
Epoch 550, training loss: 0.6612749695777893 = 0.03600165992975235 + 0.1 * 6.252732753753662
Epoch 550, val loss: 1.0192828178405762
Epoch 560, training loss: 0.6585708856582642 = 0.03370651230216026 + 0.1 * 6.24864387512207
Epoch 560, val loss: 1.0305405855178833
Epoch 570, training loss: 0.6568838953971863 = 0.031616441905498505 + 0.1 * 6.252674102783203
Epoch 570, val loss: 1.0414811372756958
Epoch 580, training loss: 0.6565804481506348 = 0.029715146869421005 + 0.1 * 6.268653392791748
Epoch 580, val loss: 1.052038550376892
Epoch 590, training loss: 0.6523533463478088 = 0.027986222878098488 + 0.1 * 6.24367094039917
Epoch 590, val loss: 1.0622774362564087
Epoch 600, training loss: 0.6496788859367371 = 0.02640141174197197 + 0.1 * 6.23277473449707
Epoch 600, val loss: 1.0722993612289429
Epoch 610, training loss: 0.6488737463951111 = 0.02494283951818943 + 0.1 * 6.239309310913086
Epoch 610, val loss: 1.0820345878601074
Epoch 620, training loss: 0.6462523937225342 = 0.023601096123456955 + 0.1 * 6.226512432098389
Epoch 620, val loss: 1.091486930847168
Epoch 630, training loss: 0.6462963223457336 = 0.022365717217326164 + 0.1 * 6.2393059730529785
Epoch 630, val loss: 1.100751519203186
Epoch 640, training loss: 0.643809974193573 = 0.021230779588222504 + 0.1 * 6.225791931152344
Epoch 640, val loss: 1.1097185611724854
Epoch 650, training loss: 0.6421607732772827 = 0.02017926424741745 + 0.1 * 6.219814777374268
Epoch 650, val loss: 1.1184839010238647
Epoch 660, training loss: 0.6412696838378906 = 0.01920170895755291 + 0.1 * 6.220679759979248
Epoch 660, val loss: 1.1270172595977783
Epoch 670, training loss: 0.6404978036880493 = 0.018294628709554672 + 0.1 * 6.222031593322754
Epoch 670, val loss: 1.1351733207702637
Epoch 680, training loss: 0.638461172580719 = 0.017453493550419807 + 0.1 * 6.210076808929443
Epoch 680, val loss: 1.143251895904541
Epoch 690, training loss: 0.6381152272224426 = 0.016671234741806984 + 0.1 * 6.214439868927002
Epoch 690, val loss: 1.1511412858963013
Epoch 700, training loss: 0.6365270614624023 = 0.015941046178340912 + 0.1 * 6.205860137939453
Epoch 700, val loss: 1.1587272882461548
Epoch 710, training loss: 0.6360189318656921 = 0.015259046107530594 + 0.1 * 6.207598686218262
Epoch 710, val loss: 1.1661268472671509
Epoch 720, training loss: 0.6343702673912048 = 0.014621823094785213 + 0.1 * 6.197484493255615
Epoch 720, val loss: 1.1734540462493896
Epoch 730, training loss: 0.6351536512374878 = 0.014025354757905006 + 0.1 * 6.211283206939697
Epoch 730, val loss: 1.1805014610290527
Epoch 740, training loss: 0.6330441832542419 = 0.013467459008097649 + 0.1 * 6.195767402648926
Epoch 740, val loss: 1.1874128580093384
Epoch 750, training loss: 0.6335691213607788 = 0.012943442910909653 + 0.1 * 6.206256866455078
Epoch 750, val loss: 1.1941816806793213
Epoch 760, training loss: 0.6316750049591064 = 0.01245172694325447 + 0.1 * 6.192232608795166
Epoch 760, val loss: 1.2006371021270752
Epoch 770, training loss: 0.6322978138923645 = 0.011988811194896698 + 0.1 * 6.203089714050293
Epoch 770, val loss: 1.207105278968811
Epoch 780, training loss: 0.6309537887573242 = 0.011553257703781128 + 0.1 * 6.194005012512207
Epoch 780, val loss: 1.2132121324539185
Epoch 790, training loss: 0.6300864815711975 = 0.011142612434923649 + 0.1 * 6.189438343048096
Epoch 790, val loss: 1.2194312810897827
Epoch 800, training loss: 0.6287091970443726 = 0.010754920542240143 + 0.1 * 6.179542541503906
Epoch 800, val loss: 1.2252886295318604
Epoch 810, training loss: 0.6281910538673401 = 0.010389456525444984 + 0.1 * 6.178016185760498
Epoch 810, val loss: 1.2311843633651733
Epoch 820, training loss: 0.6281089782714844 = 0.010042756795883179 + 0.1 * 6.180661678314209
Epoch 820, val loss: 1.2369319200515747
Epoch 830, training loss: 0.6276898980140686 = 0.009714268147945404 + 0.1 * 6.1797566413879395
Epoch 830, val loss: 1.2425552606582642
Epoch 840, training loss: 0.6274300813674927 = 0.00940245296806097 + 0.1 * 6.180275917053223
Epoch 840, val loss: 1.2479298114776611
Epoch 850, training loss: 0.6258641481399536 = 0.009107330814003944 + 0.1 * 6.167567729949951
Epoch 850, val loss: 1.2534053325653076
Epoch 860, training loss: 0.625795304775238 = 0.008826778270304203 + 0.1 * 6.169684886932373
Epoch 860, val loss: 1.2587776184082031
Epoch 870, training loss: 0.6263843774795532 = 0.00855958554893732 + 0.1 * 6.178247928619385
Epoch 870, val loss: 1.2638366222381592
Epoch 880, training loss: 0.624865710735321 = 0.008305780589580536 + 0.1 * 6.1655988693237305
Epoch 880, val loss: 1.2689608335494995
Epoch 890, training loss: 0.6253143548965454 = 0.00806440319865942 + 0.1 * 6.172499656677246
Epoch 890, val loss: 1.2740007638931274
Epoch 900, training loss: 0.6240965723991394 = 0.007834281772375107 + 0.1 * 6.162622928619385
Epoch 900, val loss: 1.2788784503936768
Epoch 910, training loss: 0.6258053779602051 = 0.00761465635150671 + 0.1 * 6.1819071769714355
Epoch 910, val loss: 1.2836405038833618
Epoch 920, training loss: 0.6230701208114624 = 0.007405238226056099 + 0.1 * 6.156649112701416
Epoch 920, val loss: 1.2882918119430542
Epoch 930, training loss: 0.6225689649581909 = 0.007205619942396879 + 0.1 * 6.1536335945129395
Epoch 930, val loss: 1.2930084466934204
Epoch 940, training loss: 0.6242845058441162 = 0.007014579139649868 + 0.1 * 6.172699451446533
Epoch 940, val loss: 1.2974141836166382
Epoch 950, training loss: 0.6219215393066406 = 0.006832420360296965 + 0.1 * 6.150890827178955
Epoch 950, val loss: 1.3018256425857544
Epoch 960, training loss: 0.6218559145927429 = 0.006658472120761871 + 0.1 * 6.151974201202393
Epoch 960, val loss: 1.306259274482727
Epoch 970, training loss: 0.6228405833244324 = 0.006491425447165966 + 0.1 * 6.163491725921631
Epoch 970, val loss: 1.3104287385940552
Epoch 980, training loss: 0.6220269799232483 = 0.006331440526992083 + 0.1 * 6.156955242156982
Epoch 980, val loss: 1.3145562410354614
Epoch 990, training loss: 0.621168315410614 = 0.0061783865094184875 + 0.1 * 6.149899482727051
Epoch 990, val loss: 1.3186380863189697
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5646
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.794888496398926 = 1.9575036764144897 + 0.1 * 8.373847961425781
Epoch 0, val loss: 1.9546432495117188
Epoch 10, training loss: 2.7842211723327637 = 1.946852445602417 + 0.1 * 8.373687744140625
Epoch 10, val loss: 1.9446178674697876
Epoch 20, training loss: 2.7704408168792725 = 1.9331412315368652 + 0.1 * 8.372995376586914
Epoch 20, val loss: 1.9314128160476685
Epoch 30, training loss: 2.75014066696167 = 1.9132733345031738 + 0.1 * 8.368674278259277
Epoch 30, val loss: 1.9120879173278809
Epoch 40, training loss: 2.7170891761779785 = 1.8835160732269287 + 0.1 * 8.335731506347656
Epoch 40, val loss: 1.8836193084716797
Epoch 50, training loss: 2.6497530937194824 = 1.8433341979980469 + 0.1 * 8.064189910888672
Epoch 50, val loss: 1.8470852375030518
Epoch 60, training loss: 2.553619384765625 = 1.8027758598327637 + 0.1 * 7.508434295654297
Epoch 60, val loss: 1.8122073411941528
Epoch 70, training loss: 2.4748895168304443 = 1.767514944076538 + 0.1 * 7.0737457275390625
Epoch 70, val loss: 1.7822685241699219
Epoch 80, training loss: 2.4127914905548096 = 1.7317829132080078 + 0.1 * 6.810086250305176
Epoch 80, val loss: 1.7514078617095947
Epoch 90, training loss: 2.3524765968322754 = 1.6843318939208984 + 0.1 * 6.681446552276611
Epoch 90, val loss: 1.7086364030838013
Epoch 100, training loss: 2.2820606231689453 = 1.6204391717910767 + 0.1 * 6.616214752197266
Epoch 100, val loss: 1.6533749103546143
Epoch 110, training loss: 2.1995320320129395 = 1.5408656597137451 + 0.1 * 6.58666467666626
Epoch 110, val loss: 1.5872392654418945
Epoch 120, training loss: 2.1114766597747803 = 1.4537523984909058 + 0.1 * 6.577241897583008
Epoch 120, val loss: 1.515590786933899
Epoch 130, training loss: 2.0240068435668945 = 1.36707603931427 + 0.1 * 6.56930685043335
Epoch 130, val loss: 1.4465699195861816
Epoch 140, training loss: 1.9400880336761475 = 1.2842390537261963 + 0.1 * 6.558488845825195
Epoch 140, val loss: 1.381891131401062
Epoch 150, training loss: 1.8603343963623047 = 1.2057017087936401 + 0.1 * 6.546327590942383
Epoch 150, val loss: 1.3222988843917847
Epoch 160, training loss: 1.785200595855713 = 1.131788969039917 + 0.1 * 6.534115314483643
Epoch 160, val loss: 1.2685904502868652
Epoch 170, training loss: 1.7131046056747437 = 1.0610147714614868 + 0.1 * 6.520898342132568
Epoch 170, val loss: 1.2185914516448975
Epoch 180, training loss: 1.6442174911499023 = 0.992966890335083 + 0.1 * 6.512505531311035
Epoch 180, val loss: 1.171226978302002
Epoch 190, training loss: 1.578458309173584 = 0.9289950132369995 + 0.1 * 6.494632720947266
Epoch 190, val loss: 1.1274288892745972
Epoch 200, training loss: 1.5165044069290161 = 0.8685358762741089 + 0.1 * 6.479685306549072
Epoch 200, val loss: 1.0864726305007935
Epoch 210, training loss: 1.4584012031555176 = 0.811286211013794 + 0.1 * 6.47114896774292
Epoch 210, val loss: 1.0483314990997314
Epoch 220, training loss: 1.4035286903381348 = 0.7575095295906067 + 0.1 * 6.46019172668457
Epoch 220, val loss: 1.0139740705490112
Epoch 230, training loss: 1.3506786823272705 = 0.7057304382324219 + 0.1 * 6.4494829177856445
Epoch 230, val loss: 0.9821024537086487
Epoch 240, training loss: 1.2997775077819824 = 0.6549801826477051 + 0.1 * 6.447972774505615
Epoch 240, val loss: 0.9522151350975037
Epoch 250, training loss: 1.248503565788269 = 0.6051292419433594 + 0.1 * 6.433743000030518
Epoch 250, val loss: 0.9243701100349426
Epoch 260, training loss: 1.1983553171157837 = 0.5555586814880371 + 0.1 * 6.427966117858887
Epoch 260, val loss: 0.8979426622390747
Epoch 270, training loss: 1.1487855911254883 = 0.5067652463912964 + 0.1 * 6.420202732086182
Epoch 270, val loss: 0.8734307885169983
Epoch 280, training loss: 1.1003427505493164 = 0.4591526687145233 + 0.1 * 6.411900997161865
Epoch 280, val loss: 0.8513603210449219
Epoch 290, training loss: 1.0543960332870483 = 0.41360554099082947 + 0.1 * 6.407904624938965
Epoch 290, val loss: 0.8325618505477905
Epoch 300, training loss: 1.0102100372314453 = 0.37066805362701416 + 0.1 * 6.395420551300049
Epoch 300, val loss: 0.8170937895774841
Epoch 310, training loss: 0.9712076187133789 = 0.33047473430633545 + 0.1 * 6.4073286056518555
Epoch 310, val loss: 0.8047297596931458
Epoch 320, training loss: 0.9321644306182861 = 0.2935749590396881 + 0.1 * 6.385894775390625
Epoch 320, val loss: 0.7953652143478394
Epoch 330, training loss: 0.8971174955368042 = 0.25964614748954773 + 0.1 * 6.374713897705078
Epoch 330, val loss: 0.7885767221450806
Epoch 340, training loss: 0.8673769235610962 = 0.22861914336681366 + 0.1 * 6.387577533721924
Epoch 340, val loss: 0.784332811832428
Epoch 350, training loss: 0.836810827255249 = 0.20076444745063782 + 0.1 * 6.360463619232178
Epoch 350, val loss: 0.7824952006340027
Epoch 360, training loss: 0.8126530647277832 = 0.17586824297904968 + 0.1 * 6.367847919464111
Epoch 360, val loss: 0.7828315496444702
Epoch 370, training loss: 0.7890011668205261 = 0.15413714945316315 + 0.1 * 6.348640441894531
Epoch 370, val loss: 0.7852959632873535
Epoch 380, training loss: 0.7690564393997192 = 0.13531820476055145 + 0.1 * 6.337381839752197
Epoch 380, val loss: 0.7896151542663574
Epoch 390, training loss: 0.7536576390266418 = 0.11908502876758575 + 0.1 * 6.345726013183594
Epoch 390, val loss: 0.7956897020339966
Epoch 400, training loss: 0.7382702231407166 = 0.1052841767668724 + 0.1 * 6.329860687255859
Epoch 400, val loss: 0.8031407594680786
Epoch 410, training loss: 0.7249643802642822 = 0.09351744502782822 + 0.1 * 6.314469337463379
Epoch 410, val loss: 0.811693012714386
Epoch 420, training loss: 0.7185114622116089 = 0.08345789462327957 + 0.1 * 6.350535869598389
Epoch 420, val loss: 0.8211214542388916
Epoch 430, training loss: 0.7064650654792786 = 0.07492848485708237 + 0.1 * 6.315365791320801
Epoch 430, val loss: 0.8308060765266418
Epoch 440, training loss: 0.6974280476570129 = 0.06762852519750595 + 0.1 * 6.297995567321777
Epoch 440, val loss: 0.840691864490509
Epoch 450, training loss: 0.690524697303772 = 0.0613383948802948 + 0.1 * 6.291862487792969
Epoch 450, val loss: 0.8507047891616821
Epoch 460, training loss: 0.6843845248222351 = 0.055892765522003174 + 0.1 * 6.28491735458374
Epoch 460, val loss: 0.8605932593345642
Epoch 470, training loss: 0.6787902116775513 = 0.05114014074206352 + 0.1 * 6.276500701904297
Epoch 470, val loss: 0.8704358339309692
Epoch 480, training loss: 0.674902617931366 = 0.046976618468761444 + 0.1 * 6.27925968170166
Epoch 480, val loss: 0.8801611065864563
Epoch 490, training loss: 0.6702906489372253 = 0.04333212226629257 + 0.1 * 6.269585132598877
Epoch 490, val loss: 0.8895700573921204
Epoch 500, training loss: 0.6679950952529907 = 0.04010462388396263 + 0.1 * 6.278904438018799
Epoch 500, val loss: 0.8988267183303833
Epoch 510, training loss: 0.6633806824684143 = 0.03724396228790283 + 0.1 * 6.261367321014404
Epoch 510, val loss: 0.9078066349029541
Epoch 520, training loss: 0.65996915102005 = 0.034693729132413864 + 0.1 * 6.252754211425781
Epoch 520, val loss: 0.9165955185890198
Epoch 530, training loss: 0.658033013343811 = 0.032401192933321 + 0.1 * 6.256318092346191
Epoch 530, val loss: 0.9252311587333679
Epoch 540, training loss: 0.6554821133613586 = 0.03033924475312233 + 0.1 * 6.251428604125977
Epoch 540, val loss: 0.9337158203125
Epoch 550, training loss: 0.6527648568153381 = 0.028477972373366356 + 0.1 * 6.242868900299072
Epoch 550, val loss: 0.941932737827301
Epoch 560, training loss: 0.6520750522613525 = 0.026789454743266106 + 0.1 * 6.252856254577637
Epoch 560, val loss: 0.9499848484992981
Epoch 570, training loss: 0.6490290760993958 = 0.025256581604480743 + 0.1 * 6.237724781036377
Epoch 570, val loss: 0.9577801823616028
Epoch 580, training loss: 0.646447479724884 = 0.023856360465288162 + 0.1 * 6.2259111404418945
Epoch 580, val loss: 0.9654667377471924
Epoch 590, training loss: 0.6484457850456238 = 0.022570960223674774 + 0.1 * 6.2587480545043945
Epoch 590, val loss: 0.9729917049407959
Epoch 600, training loss: 0.6441064476966858 = 0.021399185061454773 + 0.1 * 6.227072238922119
Epoch 600, val loss: 0.9803478121757507
Epoch 610, training loss: 0.642274796962738 = 0.02032139152288437 + 0.1 * 6.219533920288086
Epoch 610, val loss: 0.9874249696731567
Epoch 620, training loss: 0.6451433300971985 = 0.019326187670230865 + 0.1 * 6.258171081542969
Epoch 620, val loss: 0.9944140315055847
Epoch 630, training loss: 0.6395972371101379 = 0.0184088796377182 + 0.1 * 6.211883544921875
Epoch 630, val loss: 1.0011907815933228
Epoch 640, training loss: 0.6385143995285034 = 0.017558446153998375 + 0.1 * 6.209559440612793
Epoch 640, val loss: 1.0078582763671875
Epoch 650, training loss: 0.6374849677085876 = 0.016767745837569237 + 0.1 * 6.207172393798828
Epoch 650, val loss: 1.014369249343872
Epoch 660, training loss: 0.6372126340866089 = 0.016030235216021538 + 0.1 * 6.2118239402771
Epoch 660, val loss: 1.0208382606506348
Epoch 670, training loss: 0.6353007555007935 = 0.015349002555012703 + 0.1 * 6.199517250061035
Epoch 670, val loss: 1.027023196220398
Epoch 680, training loss: 0.6353711485862732 = 0.01471145823597908 + 0.1 * 6.206596851348877
Epoch 680, val loss: 1.0331182479858398
Epoch 690, training loss: 0.633389949798584 = 0.014115259051322937 + 0.1 * 6.192746639251709
Epoch 690, val loss: 1.0390338897705078
Epoch 700, training loss: 0.6342377662658691 = 0.013556525111198425 + 0.1 * 6.206812381744385
Epoch 700, val loss: 1.0448799133300781
Epoch 710, training loss: 0.6322827935218811 = 0.01303376629948616 + 0.1 * 6.192490100860596
Epoch 710, val loss: 1.0505603551864624
Epoch 720, training loss: 0.631338357925415 = 0.012543587945401669 + 0.1 * 6.187947750091553
Epoch 720, val loss: 1.0561139583587646
Epoch 730, training loss: 0.6315333843231201 = 0.012082393281161785 + 0.1 * 6.194509506225586
Epoch 730, val loss: 1.0614895820617676
Epoch 740, training loss: 0.6301939487457275 = 0.011646419763565063 + 0.1 * 6.185474872589111
Epoch 740, val loss: 1.0667897462844849
Epoch 750, training loss: 0.629585325717926 = 0.011238565668463707 + 0.1 * 6.183467388153076
Epoch 750, val loss: 1.0719993114471436
Epoch 760, training loss: 0.6323466897010803 = 0.010851356200873852 + 0.1 * 6.2149529457092285
Epoch 760, val loss: 1.0770498514175415
Epoch 770, training loss: 0.6284855604171753 = 0.010487276129424572 + 0.1 * 6.179982662200928
Epoch 770, val loss: 1.0820313692092896
Epoch 780, training loss: 0.6277481913566589 = 0.010143054649233818 + 0.1 * 6.176051139831543
Epoch 780, val loss: 1.0868840217590332
Epoch 790, training loss: 0.6293438673019409 = 0.0098161855712533 + 0.1 * 6.195276737213135
Epoch 790, val loss: 1.091671109199524
Epoch 800, training loss: 0.6261675953865051 = 0.00950611848384142 + 0.1 * 6.166614532470703
Epoch 800, val loss: 1.096373438835144
Epoch 810, training loss: 0.626116156578064 = 0.009212717413902283 + 0.1 * 6.169034004211426
Epoch 810, val loss: 1.100983738899231
Epoch 820, training loss: 0.6271213889122009 = 0.00893313717097044 + 0.1 * 6.181882381439209
Epoch 820, val loss: 1.1054974794387817
Epoch 830, training loss: 0.6258993744850159 = 0.008666904643177986 + 0.1 * 6.172324180603027
Epoch 830, val loss: 1.1100231409072876
Epoch 840, training loss: 0.6257895231246948 = 0.00841490924358368 + 0.1 * 6.173746109008789
Epoch 840, val loss: 1.1144570112228394
Epoch 850, training loss: 0.6244045495986938 = 0.008174083195626736 + 0.1 * 6.162304878234863
Epoch 850, val loss: 1.118764042854309
Epoch 860, training loss: 0.6242549419403076 = 0.00794533733278513 + 0.1 * 6.163095951080322
Epoch 860, val loss: 1.122987151145935
Epoch 870, training loss: 0.6232686042785645 = 0.007726847659796476 + 0.1 * 6.155417442321777
Epoch 870, val loss: 1.12711501121521
Epoch 880, training loss: 0.6252999305725098 = 0.007517078425735235 + 0.1 * 6.177828311920166
Epoch 880, val loss: 1.1312053203582764
Epoch 890, training loss: 0.6224462389945984 = 0.007318438962101936 + 0.1 * 6.151278018951416
Epoch 890, val loss: 1.135263204574585
Epoch 900, training loss: 0.6224494576454163 = 0.007128110621124506 + 0.1 * 6.153213024139404
Epoch 900, val loss: 1.1392649412155151
Epoch 910, training loss: 0.622528076171875 = 0.00694473460316658 + 0.1 * 6.1558332443237305
Epoch 910, val loss: 1.1431814432144165
Epoch 920, training loss: 0.6216670870780945 = 0.00677062151953578 + 0.1 * 6.1489644050598145
Epoch 920, val loss: 1.1470478773117065
Epoch 930, training loss: 0.6222707033157349 = 0.00660339929163456 + 0.1 * 6.156672477722168
Epoch 930, val loss: 1.1508307456970215
Epoch 940, training loss: 0.621271550655365 = 0.006442436948418617 + 0.1 * 6.148291110992432
Epoch 940, val loss: 1.1545391082763672
Epoch 950, training loss: 0.6216317415237427 = 0.006288215517997742 + 0.1 * 6.153435230255127
Epoch 950, val loss: 1.1582140922546387
Epoch 960, training loss: 0.6208117604255676 = 0.0061410292983055115 + 0.1 * 6.146707534790039
Epoch 960, val loss: 1.1618022918701172
Epoch 970, training loss: 0.6207060217857361 = 0.005999018903821707 + 0.1 * 6.147069931030273
Epoch 970, val loss: 1.1653605699539185
Epoch 980, training loss: 0.6202393770217896 = 0.005862264893949032 + 0.1 * 6.143771171569824
Epoch 980, val loss: 1.1687945127487183
Epoch 990, training loss: 0.6202637553215027 = 0.005730778910219669 + 0.1 * 6.145329475402832
Epoch 990, val loss: 1.172243356704712
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8635
Flip ASR: 0.8356/225 nodes
The final ASR:0.70972, 0.12217, Accuracy:0.82346, 0.01145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10576])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.806983232498169 = 1.9695910215377808 + 0.1 * 8.373921394348145
Epoch 0, val loss: 1.9653264284133911
Epoch 10, training loss: 2.7954232692718506 = 1.9580371379852295 + 0.1 * 8.373860359191895
Epoch 10, val loss: 1.9543590545654297
Epoch 20, training loss: 2.7811100482940674 = 1.9437470436096191 + 0.1 * 8.37363052368164
Epoch 20, val loss: 1.9409353733062744
Epoch 30, training loss: 2.760823965072632 = 1.9235817193984985 + 0.1 * 8.372422218322754
Epoch 30, val loss: 1.9223359823226929
Epoch 40, training loss: 2.729975461959839 = 1.893811821937561 + 0.1 * 8.361637115478516
Epoch 40, val loss: 1.8957375288009644
Epoch 50, training loss: 2.6794307231903076 = 1.852766752243042 + 0.1 * 8.266639709472656
Epoch 50, val loss: 1.8615715503692627
Epoch 60, training loss: 2.5828394889831543 = 1.81123948097229 + 0.1 * 7.716001033782959
Epoch 60, val loss: 1.8297338485717773
Epoch 70, training loss: 2.5005698204040527 = 1.779047966003418 + 0.1 * 7.215217590332031
Epoch 70, val loss: 1.8038074970245361
Epoch 80, training loss: 2.4382917881011963 = 1.7481368780136108 + 0.1 * 6.901548862457275
Epoch 80, val loss: 1.7749642133712769
Epoch 90, training loss: 2.3857581615448 = 1.7095698118209839 + 0.1 * 6.76188325881958
Epoch 90, val loss: 1.7394555807113647
Epoch 100, training loss: 2.3251237869262695 = 1.6580593585968018 + 0.1 * 6.6706438064575195
Epoch 100, val loss: 1.6956021785736084
Epoch 110, training loss: 2.2507266998291016 = 1.5900850296020508 + 0.1 * 6.606415271759033
Epoch 110, val loss: 1.6389803886413574
Epoch 120, training loss: 2.1665902137756348 = 1.5098793506622314 + 0.1 * 6.567107200622559
Epoch 120, val loss: 1.5739083290100098
Epoch 130, training loss: 2.083712339401245 = 1.430477261543274 + 0.1 * 6.532350063323975
Epoch 130, val loss: 1.5091034173965454
Epoch 140, training loss: 2.0092360973358154 = 1.3583298921585083 + 0.1 * 6.50906229019165
Epoch 140, val loss: 1.4505605697631836
Epoch 150, training loss: 1.9424011707305908 = 1.2941365242004395 + 0.1 * 6.4826459884643555
Epoch 150, val loss: 1.4002803564071655
Epoch 160, training loss: 1.8821861743927002 = 1.2354238033294678 + 0.1 * 6.467624187469482
Epoch 160, val loss: 1.355454921722412
Epoch 170, training loss: 1.8245267868041992 = 1.180154800415039 + 0.1 * 6.443719863891602
Epoch 170, val loss: 1.3141403198242188
Epoch 180, training loss: 1.7677721977233887 = 1.1247628927230835 + 0.1 * 6.4300923347473145
Epoch 180, val loss: 1.2737340927124023
Epoch 190, training loss: 1.7079126834869385 = 1.0660316944122314 + 0.1 * 6.41880989074707
Epoch 190, val loss: 1.2316644191741943
Epoch 200, training loss: 1.6415362358093262 = 1.0004218816757202 + 0.1 * 6.411143779754639
Epoch 200, val loss: 1.183944582939148
Epoch 210, training loss: 1.5678799152374268 = 0.9272801876068115 + 0.1 * 6.4059977531433105
Epoch 210, val loss: 1.130301833152771
Epoch 220, training loss: 1.4897171258926392 = 0.8486849665641785 + 0.1 * 6.410321235656738
Epoch 220, val loss: 1.0721908807754517
Epoch 230, training loss: 1.40911865234375 = 0.7699769139289856 + 0.1 * 6.391417026519775
Epoch 230, val loss: 1.0141006708145142
Epoch 240, training loss: 1.334282636642456 = 0.6959567070007324 + 0.1 * 6.38325834274292
Epoch 240, val loss: 0.9602657556533813
Epoch 250, training loss: 1.2700328826904297 = 0.6310863494873047 + 0.1 * 6.38946533203125
Epoch 250, val loss: 0.9148563146591187
Epoch 260, training loss: 1.2139885425567627 = 0.5769802331924438 + 0.1 * 6.370083332061768
Epoch 260, val loss: 0.8793971538543701
Epoch 270, training loss: 1.1701005697250366 = 0.5324011445045471 + 0.1 * 6.3769941329956055
Epoch 270, val loss: 0.852921187877655
Epoch 280, training loss: 1.1312296390533447 = 0.49574005603790283 + 0.1 * 6.354896068572998
Epoch 280, val loss: 0.8340960741043091
Epoch 290, training loss: 1.1001896858215332 = 0.46471935510635376 + 0.1 * 6.354702472686768
Epoch 290, val loss: 0.820654034614563
Epoch 300, training loss: 1.0718047618865967 = 0.43780529499053955 + 0.1 * 6.33999490737915
Epoch 300, val loss: 0.8109497427940369
Epoch 310, training loss: 1.0464719533920288 = 0.4133897125720978 + 0.1 * 6.330822467803955
Epoch 310, val loss: 0.8034496307373047
Epoch 320, training loss: 1.023542881011963 = 0.39011552929878235 + 0.1 * 6.334273338317871
Epoch 320, val loss: 0.7971391677856445
Epoch 330, training loss: 0.9994509816169739 = 0.3670462965965271 + 0.1 * 6.324046611785889
Epoch 330, val loss: 0.7913133502006531
Epoch 340, training loss: 0.9745081663131714 = 0.3432505428791046 + 0.1 * 6.3125762939453125
Epoch 340, val loss: 0.7856945991516113
Epoch 350, training loss: 0.9487740993499756 = 0.31805261969566345 + 0.1 * 6.307214260101318
Epoch 350, val loss: 0.7798930406570435
Epoch 360, training loss: 0.9236316680908203 = 0.2914559543132782 + 0.1 * 6.321756839752197
Epoch 360, val loss: 0.7740193605422974
Epoch 370, training loss: 0.8946520686149597 = 0.2641359567642212 + 0.1 * 6.305160999298096
Epoch 370, val loss: 0.7687893509864807
Epoch 380, training loss: 0.8661362528800964 = 0.23682445287704468 + 0.1 * 6.293118000030518
Epoch 380, val loss: 0.7641342878341675
Epoch 390, training loss: 0.8397919535636902 = 0.21052102744579315 + 0.1 * 6.292708873748779
Epoch 390, val loss: 0.7607309818267822
Epoch 400, training loss: 0.8172798156738281 = 0.186303973197937 + 0.1 * 6.309758186340332
Epoch 400, val loss: 0.7590644955635071
Epoch 410, training loss: 0.7939078211784363 = 0.16471095383167267 + 0.1 * 6.291968822479248
Epoch 410, val loss: 0.7595536112785339
Epoch 420, training loss: 0.7739362120628357 = 0.1457086056470871 + 0.1 * 6.282275676727295
Epoch 420, val loss: 0.7618985772132874
Epoch 430, training loss: 0.7580398321151733 = 0.1291319578886032 + 0.1 * 6.289078712463379
Epoch 430, val loss: 0.7659527063369751
Epoch 440, training loss: 0.7420622110366821 = 0.11483319848775864 + 0.1 * 6.272290229797363
Epoch 440, val loss: 0.771501898765564
Epoch 450, training loss: 0.7297402620315552 = 0.10252615809440613 + 0.1 * 6.2721405029296875
Epoch 450, val loss: 0.7782602906227112
Epoch 460, training loss: 0.719447135925293 = 0.09192220866680145 + 0.1 * 6.275249004364014
Epoch 460, val loss: 0.7859559059143066
Epoch 470, training loss: 0.7088430523872375 = 0.08278883248567581 + 0.1 * 6.260542392730713
Epoch 470, val loss: 0.794439435005188
Epoch 480, training loss: 0.700505256652832 = 0.07487280666828156 + 0.1 * 6.256324291229248
Epoch 480, val loss: 0.8035085201263428
Epoch 490, training loss: 0.6945152282714844 = 0.0680035650730133 + 0.1 * 6.2651166915893555
Epoch 490, val loss: 0.8129257559776306
Epoch 500, training loss: 0.6869641542434692 = 0.06204281747341156 + 0.1 * 6.249213218688965
Epoch 500, val loss: 0.8228140473365784
Epoch 510, training loss: 0.6814637184143066 = 0.05681857094168663 + 0.1 * 6.246451377868652
Epoch 510, val loss: 0.8327435255050659
Epoch 520, training loss: 0.67753666639328 = 0.05221645161509514 + 0.1 * 6.253201961517334
Epoch 520, val loss: 0.8428342938423157
Epoch 530, training loss: 0.6733362674713135 = 0.04815299063920975 + 0.1 * 6.251832962036133
Epoch 530, val loss: 0.85296630859375
Epoch 540, training loss: 0.6684349775314331 = 0.04454678297042847 + 0.1 * 6.238881587982178
Epoch 540, val loss: 0.8631424903869629
Epoch 550, training loss: 0.66492760181427 = 0.041331928223371506 + 0.1 * 6.235956192016602
Epoch 550, val loss: 0.8732811808586121
Epoch 560, training loss: 0.6639899015426636 = 0.038451362401247025 + 0.1 * 6.255384922027588
Epoch 560, val loss: 0.8832948803901672
Epoch 570, training loss: 0.6585214734077454 = 0.03587229922413826 + 0.1 * 6.226491928100586
Epoch 570, val loss: 0.8933529853820801
Epoch 580, training loss: 0.6557179689407349 = 0.033545296639204025 + 0.1 * 6.221726417541504
Epoch 580, val loss: 0.9032487273216248
Epoch 590, training loss: 0.6554234623908997 = 0.03143762797117233 + 0.1 * 6.239858150482178
Epoch 590, val loss: 0.9130240082740784
Epoch 600, training loss: 0.6515430212020874 = 0.029532596468925476 + 0.1 * 6.220103740692139
Epoch 600, val loss: 0.9226062893867493
Epoch 610, training loss: 0.6493747234344482 = 0.027800530195236206 + 0.1 * 6.2157416343688965
Epoch 610, val loss: 0.9321803450584412
Epoch 620, training loss: 0.648543119430542 = 0.02621828019618988 + 0.1 * 6.22324800491333
Epoch 620, val loss: 0.9414340257644653
Epoch 630, training loss: 0.6461714506149292 = 0.02477341517806053 + 0.1 * 6.213980197906494
Epoch 630, val loss: 0.9506919384002686
Epoch 640, training loss: 0.6448879837989807 = 0.023447945713996887 + 0.1 * 6.214400291442871
Epoch 640, val loss: 0.9597745537757874
Epoch 650, training loss: 0.6429603695869446 = 0.022228650748729706 + 0.1 * 6.207316875457764
Epoch 650, val loss: 0.9686399102210999
Epoch 660, training loss: 0.641823410987854 = 0.02110571227967739 + 0.1 * 6.207176685333252
Epoch 660, val loss: 0.9774212837219238
Epoch 670, training loss: 0.6420122981071472 = 0.020068269222974777 + 0.1 * 6.21943998336792
Epoch 670, val loss: 0.9859578609466553
Epoch 680, training loss: 0.6402289867401123 = 0.019111689180135727 + 0.1 * 6.211173057556152
Epoch 680, val loss: 0.994292676448822
Epoch 690, training loss: 0.6378859281539917 = 0.01822831854224205 + 0.1 * 6.196576118469238
Epoch 690, val loss: 1.0026719570159912
Epoch 700, training loss: 0.6368716359138489 = 0.017405007034540176 + 0.1 * 6.194665908813477
Epoch 700, val loss: 1.0107437372207642
Epoch 710, training loss: 0.6367539763450623 = 0.016638370230793953 + 0.1 * 6.201155662536621
Epoch 710, val loss: 1.0186424255371094
Epoch 720, training loss: 0.6349167227745056 = 0.01592518761754036 + 0.1 * 6.189915657043457
Epoch 720, val loss: 1.026389718055725
Epoch 730, training loss: 0.6338269710540771 = 0.015261293388903141 + 0.1 * 6.185656547546387
Epoch 730, val loss: 1.0341535806655884
Epoch 740, training loss: 0.6328571438789368 = 0.014639078639447689 + 0.1 * 6.182180404663086
Epoch 740, val loss: 1.0417068004608154
Epoch 750, training loss: 0.6349762082099915 = 0.014055398292839527 + 0.1 * 6.2092084884643555
Epoch 750, val loss: 1.049093246459961
Epoch 760, training loss: 0.6320767402648926 = 0.013509416952729225 + 0.1 * 6.185673236846924
Epoch 760, val loss: 1.0562859773635864
Epoch 770, training loss: 0.6312447190284729 = 0.012998645193874836 + 0.1 * 6.182460784912109
Epoch 770, val loss: 1.0635778903961182
Epoch 780, training loss: 0.6316889524459839 = 0.012516483664512634 + 0.1 * 6.1917243003845215
Epoch 780, val loss: 1.0704569816589355
Epoch 790, training loss: 0.6295463442802429 = 0.012065062299370766 + 0.1 * 6.1748127937316895
Epoch 790, val loss: 1.0773252248764038
Epoch 800, training loss: 0.6293894648551941 = 0.011639149859547615 + 0.1 * 6.177502632141113
Epoch 800, val loss: 1.08413565158844
Epoch 810, training loss: 0.628444254398346 = 0.011235572397708893 + 0.1 * 6.172086715698242
Epoch 810, val loss: 1.0906888246536255
Epoch 820, training loss: 0.6295583844184875 = 0.010854855179786682 + 0.1 * 6.18703556060791
Epoch 820, val loss: 1.0971933603286743
Epoch 830, training loss: 0.627763032913208 = 0.010495468974113464 + 0.1 * 6.172675132751465
Epoch 830, val loss: 1.103621482849121
Epoch 840, training loss: 0.6267295479774475 = 0.010155309922993183 + 0.1 * 6.165741920471191
Epoch 840, val loss: 1.1100208759307861
Epoch 850, training loss: 0.6275261044502258 = 0.009831762872636318 + 0.1 * 6.176943302154541
Epoch 850, val loss: 1.1161988973617554
Epoch 860, training loss: 0.6263575553894043 = 0.009525001049041748 + 0.1 * 6.168325424194336
Epoch 860, val loss: 1.12226402759552
Epoch 870, training loss: 0.626230776309967 = 0.009234537370502949 + 0.1 * 6.169961929321289
Epoch 870, val loss: 1.1283265352249146
Epoch 880, training loss: 0.6263791918754578 = 0.008958485908806324 + 0.1 * 6.1742072105407715
Epoch 880, val loss: 1.1341633796691895
Epoch 890, training loss: 0.6245328187942505 = 0.008696372620761395 + 0.1 * 6.158364295959473
Epoch 890, val loss: 1.139996886253357
Epoch 900, training loss: 0.6242969632148743 = 0.008447657339274883 + 0.1 * 6.1584930419921875
Epoch 900, val loss: 1.1458637714385986
Epoch 910, training loss: 0.6234655976295471 = 0.00820932351052761 + 0.1 * 6.152562141418457
Epoch 910, val loss: 1.151488184928894
Epoch 920, training loss: 0.624547004699707 = 0.007981681264936924 + 0.1 * 6.165653228759766
Epoch 920, val loss: 1.157016634941101
Epoch 930, training loss: 0.6244053244590759 = 0.00776404095813632 + 0.1 * 6.166412830352783
Epoch 930, val loss: 1.16232430934906
Epoch 940, training loss: 0.6230121850967407 = 0.007557283155620098 + 0.1 * 6.1545491218566895
Epoch 940, val loss: 1.1677513122558594
Epoch 950, training loss: 0.6234311461448669 = 0.007359663490206003 + 0.1 * 6.160714626312256
Epoch 950, val loss: 1.1731141805648804
Epoch 960, training loss: 0.6223672032356262 = 0.007169943302869797 + 0.1 * 6.15197229385376
Epoch 960, val loss: 1.1782053709030151
Epoch 970, training loss: 0.6219486594200134 = 0.006989262532442808 + 0.1 * 6.149594306945801
Epoch 970, val loss: 1.1834607124328613
Epoch 980, training loss: 0.6220941543579102 = 0.006815764121711254 + 0.1 * 6.152783393859863
Epoch 980, val loss: 1.1885149478912354
Epoch 990, training loss: 0.6212158799171448 = 0.006649185437709093 + 0.1 * 6.145666599273682
Epoch 990, val loss: 1.1934382915496826
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.6236
Flip ASR: 0.5467/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7954933643341064 = 1.9581061601638794 + 0.1 * 8.373871803283691
Epoch 0, val loss: 1.9686627388000488
Epoch 10, training loss: 2.785613536834717 = 1.9482375383377075 + 0.1 * 8.373761177062988
Epoch 10, val loss: 1.9586448669433594
Epoch 20, training loss: 2.7737340927124023 = 1.9364113807678223 + 0.1 * 8.373228073120117
Epoch 20, val loss: 1.9461382627487183
Epoch 30, training loss: 2.7569401264190674 = 1.9199246168136597 + 0.1 * 8.370155334472656
Epoch 30, val loss: 1.9281772375106812
Epoch 40, training loss: 2.7303414344787598 = 1.895550012588501 + 0.1 * 8.34791374206543
Epoch 40, val loss: 1.9014925956726074
Epoch 50, training loss: 2.6711483001708984 = 1.8605397939682007 + 0.1 * 8.106085777282715
Epoch 50, val loss: 1.86416757106781
Epoch 60, training loss: 2.552060842514038 = 1.8231290578842163 + 0.1 * 7.289317607879639
Epoch 60, val loss: 1.8265995979309082
Epoch 70, training loss: 2.4890496730804443 = 1.786521315574646 + 0.1 * 7.025283336639404
Epoch 70, val loss: 1.7891285419464111
Epoch 80, training loss: 2.435560703277588 = 1.7468550205230713 + 0.1 * 6.88705587387085
Epoch 80, val loss: 1.750516653060913
Epoch 90, training loss: 2.3836302757263184 = 1.704513430595398 + 0.1 * 6.791168212890625
Epoch 90, val loss: 1.711122989654541
Epoch 100, training loss: 2.3235862255096436 = 1.6497365236282349 + 0.1 * 6.73849630355835
Epoch 100, val loss: 1.6629663705825806
Epoch 110, training loss: 2.247201681137085 = 1.5768369436264038 + 0.1 * 6.703646659851074
Epoch 110, val loss: 1.6017569303512573
Epoch 120, training loss: 2.150806427001953 = 1.4829988479614258 + 0.1 * 6.678076267242432
Epoch 120, val loss: 1.5245481729507446
Epoch 130, training loss: 2.0366427898406982 = 1.3709659576416016 + 0.1 * 6.65676736831665
Epoch 130, val loss: 1.4328875541687012
Epoch 140, training loss: 1.9138617515563965 = 1.2504318952560425 + 0.1 * 6.634299278259277
Epoch 140, val loss: 1.3356281518936157
Epoch 150, training loss: 1.7933111190795898 = 1.1317919492721558 + 0.1 * 6.6151909828186035
Epoch 150, val loss: 1.2416495084762573
Epoch 160, training loss: 1.6835300922393799 = 1.0240395069122314 + 0.1 * 6.594905376434326
Epoch 160, val loss: 1.157967448234558
Epoch 170, training loss: 1.5865983963012695 = 0.9291499853134155 + 0.1 * 6.574483394622803
Epoch 170, val loss: 1.085763931274414
Epoch 180, training loss: 1.503448486328125 = 0.8480900526046753 + 0.1 * 6.553584575653076
Epoch 180, val loss: 1.0258320569992065
Epoch 190, training loss: 1.4327139854431152 = 0.7780384421348572 + 0.1 * 6.546755313873291
Epoch 190, val loss: 0.9759597182273865
Epoch 200, training loss: 1.369776964187622 = 0.7179322242736816 + 0.1 * 6.5184478759765625
Epoch 200, val loss: 0.9358543157577515
Epoch 210, training loss: 1.3163363933563232 = 0.6654813885688782 + 0.1 * 6.508550643920898
Epoch 210, val loss: 0.9034029841423035
Epoch 220, training loss: 1.2680387496948242 = 0.6188331842422485 + 0.1 * 6.492055892944336
Epoch 220, val loss: 0.8769089579582214
Epoch 230, training loss: 1.2235056161880493 = 0.5759353637695312 + 0.1 * 6.475702285766602
Epoch 230, val loss: 0.8544881939888
Epoch 240, training loss: 1.1834416389465332 = 0.5355836153030396 + 0.1 * 6.478580474853516
Epoch 240, val loss: 0.8354743123054504
Epoch 250, training loss: 1.1436774730682373 = 0.4975064992904663 + 0.1 * 6.461709499359131
Epoch 250, val loss: 0.8192357420921326
Epoch 260, training loss: 1.106416940689087 = 0.4611508250236511 + 0.1 * 6.45266056060791
Epoch 260, val loss: 0.8049516677856445
Epoch 270, training loss: 1.0708720684051514 = 0.4262208342552185 + 0.1 * 6.446511745452881
Epoch 270, val loss: 0.7924934029579163
Epoch 280, training loss: 1.0368787050247192 = 0.3927743136882782 + 0.1 * 6.441043853759766
Epoch 280, val loss: 0.7818573117256165
Epoch 290, training loss: 1.0045533180236816 = 0.3610270917415619 + 0.1 * 6.435262680053711
Epoch 290, val loss: 0.7729857563972473
Epoch 300, training loss: 0.9734199047088623 = 0.3311367332935333 + 0.1 * 6.422832012176514
Epoch 300, val loss: 0.7659215331077576
Epoch 310, training loss: 0.9455147981643677 = 0.3032730519771576 + 0.1 * 6.422417640686035
Epoch 310, val loss: 0.7606827020645142
Epoch 320, training loss: 0.9188462495803833 = 0.27773305773735046 + 0.1 * 6.411132335662842
Epoch 320, val loss: 0.7576127648353577
Epoch 330, training loss: 0.8947271108627319 = 0.25439414381980896 + 0.1 * 6.403329849243164
Epoch 330, val loss: 0.7565171122550964
Epoch 340, training loss: 0.8744257092475891 = 0.23319639265537262 + 0.1 * 6.412292957305908
Epoch 340, val loss: 0.7571871280670166
Epoch 350, training loss: 0.8534924983978271 = 0.21417292952537537 + 0.1 * 6.393195152282715
Epoch 350, val loss: 0.7596904635429382
Epoch 360, training loss: 0.8348021507263184 = 0.19692926108837128 + 0.1 * 6.37872838973999
Epoch 360, val loss: 0.76359623670578
Epoch 370, training loss: 0.8190964460372925 = 0.18128594756126404 + 0.1 * 6.3781046867370605
Epoch 370, val loss: 0.7688033580780029
Epoch 380, training loss: 0.803982675075531 = 0.1670839637517929 + 0.1 * 6.368987083435059
Epoch 380, val loss: 0.7750908732414246
Epoch 390, training loss: 0.7906220555305481 = 0.15417204797267914 + 0.1 * 6.364499568939209
Epoch 390, val loss: 0.7821866869926453
Epoch 400, training loss: 0.7794166207313538 = 0.14245033264160156 + 0.1 * 6.369662761688232
Epoch 400, val loss: 0.7896966338157654
Epoch 410, training loss: 0.7672922015190125 = 0.13180921971797943 + 0.1 * 6.354829788208008
Epoch 410, val loss: 0.79765385389328
Epoch 420, training loss: 0.756658673286438 = 0.12212148308753967 + 0.1 * 6.345371723175049
Epoch 420, val loss: 0.8060048222541809
Epoch 430, training loss: 0.7468733787536621 = 0.11327645927667618 + 0.1 * 6.335968971252441
Epoch 430, val loss: 0.8145437836647034
Epoch 440, training loss: 0.7395258545875549 = 0.10516778379678726 + 0.1 * 6.34358024597168
Epoch 440, val loss: 0.8231465220451355
Epoch 450, training loss: 0.7310079336166382 = 0.09774632006883621 + 0.1 * 6.332615852355957
Epoch 450, val loss: 0.8317612409591675
Epoch 460, training loss: 0.7227813005447388 = 0.09092720597982407 + 0.1 * 6.318541049957275
Epoch 460, val loss: 0.840414822101593
Epoch 470, training loss: 0.7159325480461121 = 0.08460047096014023 + 0.1 * 6.313320636749268
Epoch 470, val loss: 0.8489519357681274
Epoch 480, training loss: 0.7096454501152039 = 0.07872986793518066 + 0.1 * 6.309155464172363
Epoch 480, val loss: 0.8573594093322754
Epoch 490, training loss: 0.7038072943687439 = 0.07324445247650146 + 0.1 * 6.305628299713135
Epoch 490, val loss: 0.8659100532531738
Epoch 500, training loss: 0.6992623805999756 = 0.0680941641330719 + 0.1 * 6.311682224273682
Epoch 500, val loss: 0.8744134306907654
Epoch 510, training loss: 0.6927567720413208 = 0.06328265368938446 + 0.1 * 6.294740676879883
Epoch 510, val loss: 0.8827658295631409
Epoch 520, training loss: 0.6895760297775269 = 0.0587272085249424 + 0.1 * 6.308487892150879
Epoch 520, val loss: 0.8913756012916565
Epoch 530, training loss: 0.6839016079902649 = 0.0543893538415432 + 0.1 * 6.2951226234436035
Epoch 530, val loss: 0.8995091915130615
Epoch 540, training loss: 0.6783453822135925 = 0.05026102066040039 + 0.1 * 6.280843257904053
Epoch 540, val loss: 0.9080376625061035
Epoch 550, training loss: 0.6750525236129761 = 0.04638194665312767 + 0.1 * 6.286705493927002
Epoch 550, val loss: 0.9162169694900513
Epoch 560, training loss: 0.6705501079559326 = 0.04268506541848183 + 0.1 * 6.278650283813477
Epoch 560, val loss: 0.9248368144035339
Epoch 570, training loss: 0.6663541197776794 = 0.03926872834563255 + 0.1 * 6.270853519439697
Epoch 570, val loss: 0.9328552484512329
Epoch 580, training loss: 0.6633149981498718 = 0.03622988238930702 + 0.1 * 6.270850658416748
Epoch 580, val loss: 0.940623939037323
Epoch 590, training loss: 0.6595368981361389 = 0.0335678905248642 + 0.1 * 6.259689807891846
Epoch 590, val loss: 0.9488087892532349
Epoch 600, training loss: 0.6579081416130066 = 0.03121272288262844 + 0.1 * 6.266953945159912
Epoch 600, val loss: 0.9565147161483765
Epoch 610, training loss: 0.6563137769699097 = 0.02913069538772106 + 0.1 * 6.2718305587768555
Epoch 610, val loss: 0.9638598561286926
Epoch 620, training loss: 0.6521467566490173 = 0.02727675996720791 + 0.1 * 6.24869966506958
Epoch 620, val loss: 0.9716543555259705
Epoch 630, training loss: 0.6519454121589661 = 0.02560318261384964 + 0.1 * 6.26342248916626
Epoch 630, val loss: 0.9787310361862183
Epoch 640, training loss: 0.648668646812439 = 0.024082347750663757 + 0.1 * 6.24586296081543
Epoch 640, val loss: 0.9857344031333923
Epoch 650, training loss: 0.6464285254478455 = 0.022701231762766838 + 0.1 * 6.237273216247559
Epoch 650, val loss: 0.9924622774124146
Epoch 660, training loss: 0.646398663520813 = 0.021439211443066597 + 0.1 * 6.249594688415527
Epoch 660, val loss: 0.9989941716194153
Epoch 670, training loss: 0.6442314386367798 = 0.020283643156290054 + 0.1 * 6.23947811126709
Epoch 670, val loss: 1.0052119493484497
Epoch 680, training loss: 0.6431118249893188 = 0.019224733114242554 + 0.1 * 6.238870620727539
Epoch 680, val loss: 1.0113884210586548
Epoch 690, training loss: 0.6418168544769287 = 0.018249578773975372 + 0.1 * 6.235672950744629
Epoch 690, val loss: 1.017251968383789
Epoch 700, training loss: 0.6403694748878479 = 0.01734914630651474 + 0.1 * 6.230203628540039
Epoch 700, val loss: 1.0231077671051025
Epoch 710, training loss: 0.6385621428489685 = 0.016514500603079796 + 0.1 * 6.2204766273498535
Epoch 710, val loss: 1.0286945104599
Epoch 720, training loss: 0.6387421488761902 = 0.015735860913991928 + 0.1 * 6.230062961578369
Epoch 720, val loss: 1.0342081785202026
Epoch 730, training loss: 0.6372213363647461 = 0.015012308023869991 + 0.1 * 6.222090244293213
Epoch 730, val loss: 1.0393939018249512
Epoch 740, training loss: 0.6385708451271057 = 0.01434387732297182 + 0.1 * 6.242269515991211
Epoch 740, val loss: 1.0445417165756226
Epoch 750, training loss: 0.6347669959068298 = 0.01372696552425623 + 0.1 * 6.210400581359863
Epoch 750, val loss: 1.0495685338974
Epoch 760, training loss: 0.6342821717262268 = 0.01315375417470932 + 0.1 * 6.211284160614014
Epoch 760, val loss: 1.054733395576477
Epoch 770, training loss: 0.6337255239486694 = 0.012617847882211208 + 0.1 * 6.211076736450195
Epoch 770, val loss: 1.0594500303268433
Epoch 780, training loss: 0.6335024237632751 = 0.012117364443838596 + 0.1 * 6.213850498199463
Epoch 780, val loss: 1.0642484426498413
Epoch 790, training loss: 0.6325399279594421 = 0.011648903600871563 + 0.1 * 6.20890998840332
Epoch 790, val loss: 1.0687522888183594
Epoch 800, training loss: 0.6315338611602783 = 0.01121063344180584 + 0.1 * 6.203232288360596
Epoch 800, val loss: 1.0734236240386963
Epoch 810, training loss: 0.6306230425834656 = 0.010797951370477676 + 0.1 * 6.198250770568848
Epoch 810, val loss: 1.0778168439865112
Epoch 820, training loss: 0.6303515434265137 = 0.010409539565443993 + 0.1 * 6.199419975280762
Epoch 820, val loss: 1.0821399688720703
Epoch 830, training loss: 0.6316292881965637 = 0.010043025016784668 + 0.1 * 6.21586275100708
Epoch 830, val loss: 1.0862326622009277
Epoch 840, training loss: 0.6291624903678894 = 0.009698950685560703 + 0.1 * 6.194635391235352
Epoch 840, val loss: 1.0902682542800903
Epoch 850, training loss: 0.6286158561706543 = 0.009374751709401608 + 0.1 * 6.192410945892334
Epoch 850, val loss: 1.0945019721984863
Epoch 860, training loss: 0.6291709542274475 = 0.009067332372069359 + 0.1 * 6.201035976409912
Epoch 860, val loss: 1.0985430479049683
Epoch 870, training loss: 0.6284545660018921 = 0.008775739930570126 + 0.1 * 6.196788311004639
Epoch 870, val loss: 1.102064609527588
Epoch 880, training loss: 0.6273934245109558 = 0.008500877767801285 + 0.1 * 6.188925266265869
Epoch 880, val loss: 1.1059601306915283
Epoch 890, training loss: 0.6274637579917908 = 0.00823926366865635 + 0.1 * 6.192245006561279
Epoch 890, val loss: 1.1097619533538818
Epoch 900, training loss: 0.6261923313140869 = 0.007990879938006401 + 0.1 * 6.182013988494873
Epoch 900, val loss: 1.1133252382278442
Epoch 910, training loss: 0.6273981928825378 = 0.007754323072731495 + 0.1 * 6.196438789367676
Epoch 910, val loss: 1.1167502403259277
Epoch 920, training loss: 0.6254922151565552 = 0.007529913913458586 + 0.1 * 6.179623126983643
Epoch 920, val loss: 1.1202269792556763
Epoch 930, training loss: 0.6254088878631592 = 0.007316132541745901 + 0.1 * 6.180927753448486
Epoch 930, val loss: 1.123780608177185
Epoch 940, training loss: 0.6249402165412903 = 0.007111403159797192 + 0.1 * 6.178287506103516
Epoch 940, val loss: 1.1271034479141235
Epoch 950, training loss: 0.6272688508033752 = 0.0069159637205302715 + 0.1 * 6.20352840423584
Epoch 950, val loss: 1.1303755044937134
Epoch 960, training loss: 0.6248686909675598 = 0.0067291660234332085 + 0.1 * 6.181395053863525
Epoch 960, val loss: 1.1334319114685059
Epoch 970, training loss: 0.6236180067062378 = 0.006552030798047781 + 0.1 * 6.17065954208374
Epoch 970, val loss: 1.1367905139923096
Epoch 980, training loss: 0.6246475577354431 = 0.0063816262409091 + 0.1 * 6.182659149169922
Epoch 980, val loss: 1.1399407386779785
Epoch 990, training loss: 0.6231452822685242 = 0.006218286696821451 + 0.1 * 6.169269561767578
Epoch 990, val loss: 1.1428422927856445
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7454
Flip ASR: 0.7022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7877135276794434 = 1.9503237009048462 + 0.1 * 8.37389850616455
Epoch 0, val loss: 1.9518009424209595
Epoch 10, training loss: 2.777500629425049 = 1.9401178359985352 + 0.1 * 8.373828887939453
Epoch 10, val loss: 1.94195556640625
Epoch 20, training loss: 2.765277147293091 = 1.9279248714447021 + 0.1 * 8.373522758483887
Epoch 20, val loss: 1.9297277927398682
Epoch 30, training loss: 2.748201608657837 = 1.9110158681869507 + 0.1 * 8.371857643127441
Epoch 30, val loss: 1.9125723838806152
Epoch 40, training loss: 2.72196102142334 = 1.8861918449401855 + 0.1 * 8.357690811157227
Epoch 40, val loss: 1.8876742124557495
Epoch 50, training loss: 2.67244291305542 = 1.851143479347229 + 0.1 * 8.212993621826172
Epoch 50, val loss: 1.8539549112319946
Epoch 60, training loss: 2.56917667388916 = 1.814052700996399 + 0.1 * 7.551240921020508
Epoch 60, val loss: 1.8202917575836182
Epoch 70, training loss: 2.485783576965332 = 1.7821218967437744 + 0.1 * 7.03661584854126
Epoch 70, val loss: 1.791242241859436
Epoch 80, training loss: 2.432396650314331 = 1.749516248703003 + 0.1 * 6.828803539276123
Epoch 80, val loss: 1.7621508836746216
Epoch 90, training loss: 2.3849987983703613 = 1.7107514142990112 + 0.1 * 6.7424750328063965
Epoch 90, val loss: 1.7276240587234497
Epoch 100, training loss: 2.328672170639038 = 1.6596511602401733 + 0.1 * 6.690210819244385
Epoch 100, val loss: 1.6829655170440674
Epoch 110, training loss: 2.2583463191986084 = 1.5926133394241333 + 0.1 * 6.657330513000488
Epoch 110, val loss: 1.6265870332717896
Epoch 120, training loss: 2.1723263263702393 = 1.508923888206482 + 0.1 * 6.634023666381836
Epoch 120, val loss: 1.557620644569397
Epoch 130, training loss: 2.0752735137939453 = 1.4136946201324463 + 0.1 * 6.615787982940674
Epoch 130, val loss: 1.4803290367126465
Epoch 140, training loss: 1.9748568534851074 = 1.314738154411316 + 0.1 * 6.601186275482178
Epoch 140, val loss: 1.4030523300170898
Epoch 150, training loss: 1.8778576850891113 = 1.2190897464752197 + 0.1 * 6.5876784324646
Epoch 150, val loss: 1.3319718837738037
Epoch 160, training loss: 1.7856289148330688 = 1.128053069114685 + 0.1 * 6.575758457183838
Epoch 160, val loss: 1.2667864561080933
Epoch 170, training loss: 1.698475956916809 = 1.0426421165466309 + 0.1 * 6.558338165283203
Epoch 170, val loss: 1.2069776058197021
Epoch 180, training loss: 1.616642713546753 = 0.9620295166969299 + 0.1 * 6.546131610870361
Epoch 180, val loss: 1.1504985094070435
Epoch 190, training loss: 1.5404407978057861 = 0.8872894048690796 + 0.1 * 6.531514644622803
Epoch 190, val loss: 1.0974682569503784
Epoch 200, training loss: 1.4693132638931274 = 0.8179746866226196 + 0.1 * 6.513385772705078
Epoch 200, val loss: 1.0474588871002197
Epoch 210, training loss: 1.4046335220336914 = 0.7538160681724548 + 0.1 * 6.508173942565918
Epoch 210, val loss: 1.0009101629257202
Epoch 220, training loss: 1.3451509475708008 = 0.6959889531135559 + 0.1 * 6.491619110107422
Epoch 220, val loss: 0.9596092104911804
Epoch 230, training loss: 1.2909259796142578 = 0.6432279348373413 + 0.1 * 6.476981163024902
Epoch 230, val loss: 0.9234853386878967
Epoch 240, training loss: 1.240161418914795 = 0.5936763286590576 + 0.1 * 6.464849948883057
Epoch 240, val loss: 0.8913968205451965
Epoch 250, training loss: 1.191051959991455 = 0.5456438660621643 + 0.1 * 6.4540815353393555
Epoch 250, val loss: 0.8620073795318604
Epoch 260, training loss: 1.1423883438110352 = 0.4978320300579071 + 0.1 * 6.445563316345215
Epoch 260, val loss: 0.8341900110244751
Epoch 270, training loss: 1.093631386756897 = 0.4497017562389374 + 0.1 * 6.439295768737793
Epoch 270, val loss: 0.8075280785560608
Epoch 280, training loss: 1.045600414276123 = 0.4019117057323456 + 0.1 * 6.436886787414551
Epoch 280, val loss: 0.782776415348053
Epoch 290, training loss: 0.9982396364212036 = 0.35567042231559753 + 0.1 * 6.425692558288574
Epoch 290, val loss: 0.7609786987304688
Epoch 300, training loss: 0.9530839920043945 = 0.31180474162101746 + 0.1 * 6.412792205810547
Epoch 300, val loss: 0.7423943281173706
Epoch 310, training loss: 0.9129709005355835 = 0.27149203419685364 + 0.1 * 6.414788246154785
Epoch 310, val loss: 0.72763592004776
Epoch 320, training loss: 0.876246452331543 = 0.23601403832435608 + 0.1 * 6.4023237228393555
Epoch 320, val loss: 0.7170459628105164
Epoch 330, training loss: 0.8445779085159302 = 0.20538398623466492 + 0.1 * 6.391939163208008
Epoch 330, val loss: 0.7104451656341553
Epoch 340, training loss: 0.8180853128433228 = 0.1793300360441208 + 0.1 * 6.387552261352539
Epoch 340, val loss: 0.707323431968689
Epoch 350, training loss: 0.7949872016906738 = 0.1574055403470993 + 0.1 * 6.375816345214844
Epoch 350, val loss: 0.7073085904121399
Epoch 360, training loss: 0.7763655781745911 = 0.1389792412519455 + 0.1 * 6.373863220214844
Epoch 360, val loss: 0.709943413734436
Epoch 370, training loss: 0.7614681720733643 = 0.12337945401668549 + 0.1 * 6.380887031555176
Epoch 370, val loss: 0.714500904083252
Epoch 380, training loss: 0.7465836405754089 = 0.1101827621459961 + 0.1 * 6.364008903503418
Epoch 380, val loss: 0.7205395698547363
Epoch 390, training loss: 0.7340971231460571 = 0.09889620542526245 + 0.1 * 6.352008819580078
Epoch 390, val loss: 0.7278345227241516
Epoch 400, training loss: 0.7244564294815063 = 0.08915339410305023 + 0.1 * 6.353030204772949
Epoch 400, val loss: 0.7360840439796448
Epoch 410, training loss: 0.7152122259140015 = 0.08072821795940399 + 0.1 * 6.344839572906494
Epoch 410, val loss: 0.7448938488960266
Epoch 420, training loss: 0.7066729664802551 = 0.07339514046907425 + 0.1 * 6.332778453826904
Epoch 420, val loss: 0.754340410232544
Epoch 430, training loss: 0.7025451064109802 = 0.06696876138448715 + 0.1 * 6.3557634353637695
Epoch 430, val loss: 0.7641323804855347
Epoch 440, training loss: 0.6944500803947449 = 0.06134555861353874 + 0.1 * 6.331044673919678
Epoch 440, val loss: 0.774163007736206
Epoch 450, training loss: 0.6877768039703369 = 0.0563829131424427 + 0.1 * 6.313939094543457
Epoch 450, val loss: 0.7843730449676514
Epoch 460, training loss: 0.6863560080528259 = 0.051973868161439896 + 0.1 * 6.343821048736572
Epoch 460, val loss: 0.794735312461853
Epoch 470, training loss: 0.6784288287162781 = 0.048062823712825775 + 0.1 * 6.3036603927612305
Epoch 470, val loss: 0.805108904838562
Epoch 480, training loss: 0.674531877040863 = 0.04456641152501106 + 0.1 * 6.299654483795166
Epoch 480, val loss: 0.8154900074005127
Epoch 490, training loss: 0.6712720990180969 = 0.04142771661281586 + 0.1 * 6.298443794250488
Epoch 490, val loss: 0.8257856369018555
Epoch 500, training loss: 0.6679792404174805 = 0.03860953822731972 + 0.1 * 6.293696880340576
Epoch 500, val loss: 0.8361597657203674
Epoch 510, training loss: 0.6641188859939575 = 0.03606121614575386 + 0.1 * 6.280576229095459
Epoch 510, val loss: 0.8464268445968628
Epoch 520, training loss: 0.6657655239105225 = 0.03374871239066124 + 0.1 * 6.3201680183410645
Epoch 520, val loss: 0.8566199541091919
Epoch 530, training loss: 0.65980064868927 = 0.03165882080793381 + 0.1 * 6.281418323516846
Epoch 530, val loss: 0.8665624260902405
Epoch 540, training loss: 0.6569900512695312 = 0.02976130321621895 + 0.1 * 6.272287368774414
Epoch 540, val loss: 0.8764784336090088
Epoch 550, training loss: 0.6556274890899658 = 0.028026903048157692 + 0.1 * 6.276005744934082
Epoch 550, val loss: 0.886176347732544
Epoch 560, training loss: 0.6536245346069336 = 0.02644176594913006 + 0.1 * 6.271827697753906
Epoch 560, val loss: 0.8956140279769897
Epoch 570, training loss: 0.6514639854431152 = 0.02498987875878811 + 0.1 * 6.264740467071533
Epoch 570, val loss: 0.905032217502594
Epoch 580, training loss: 0.6508268713951111 = 0.023655429482460022 + 0.1 * 6.271714210510254
Epoch 580, val loss: 0.9141725301742554
Epoch 590, training loss: 0.6476501226425171 = 0.022429224103689194 + 0.1 * 6.252208709716797
Epoch 590, val loss: 0.9231608510017395
Epoch 600, training loss: 0.6461111903190613 = 0.02129986509680748 + 0.1 * 6.248113632202148
Epoch 600, val loss: 0.9320027828216553
Epoch 610, training loss: 0.6461749076843262 = 0.020254651084542274 + 0.1 * 6.259202003479004
Epoch 610, val loss: 0.9406697750091553
Epoch 620, training loss: 0.6442645788192749 = 0.01928659714758396 + 0.1 * 6.249780178070068
Epoch 620, val loss: 0.949089765548706
Epoch 630, training loss: 0.642426609992981 = 0.018392829224467278 + 0.1 * 6.24033784866333
Epoch 630, val loss: 0.9574061632156372
Epoch 640, training loss: 0.6407946944236755 = 0.017561420798301697 + 0.1 * 6.232332706451416
Epoch 640, val loss: 0.9655417203903198
Epoch 650, training loss: 0.6414703130722046 = 0.016786307096481323 + 0.1 * 6.246840476989746
Epoch 650, val loss: 0.9734621644020081
Epoch 660, training loss: 0.6398984789848328 = 0.01606428436934948 + 0.1 * 6.238341808319092
Epoch 660, val loss: 0.9812289476394653
Epoch 670, training loss: 0.6375960111618042 = 0.015391936525702477 + 0.1 * 6.22204065322876
Epoch 670, val loss: 0.9888911247253418
Epoch 680, training loss: 0.6372954845428467 = 0.014762441627681255 + 0.1 * 6.225329875946045
Epoch 680, val loss: 0.9963750839233398
Epoch 690, training loss: 0.63608717918396 = 0.014172021299600601 + 0.1 * 6.219151496887207
Epoch 690, val loss: 1.0036654472351074
Epoch 700, training loss: 0.6356918811798096 = 0.013618473894894123 + 0.1 * 6.220734119415283
Epoch 700, val loss: 1.0108267068862915
Epoch 710, training loss: 0.6345705986022949 = 0.01310085691511631 + 0.1 * 6.214697360992432
Epoch 710, val loss: 1.0178648233413696
Epoch 720, training loss: 0.6335552930831909 = 0.012614011764526367 + 0.1 * 6.209412574768066
Epoch 720, val loss: 1.024767518043518
Epoch 730, training loss: 0.6328096389770508 = 0.012154790572822094 + 0.1 * 6.20654821395874
Epoch 730, val loss: 1.0314096212387085
Epoch 740, training loss: 0.6320281028747559 = 0.011722883209586143 + 0.1 * 6.203052043914795
Epoch 740, val loss: 1.0380430221557617
Epoch 750, training loss: 0.6327948570251465 = 0.01131508033722639 + 0.1 * 6.214797496795654
Epoch 750, val loss: 1.04453444480896
Epoch 760, training loss: 0.6318280696868896 = 0.010929226875305176 + 0.1 * 6.208988189697266
Epoch 760, val loss: 1.050710678100586
Epoch 770, training loss: 0.6302648782730103 = 0.01056554913520813 + 0.1 * 6.196993350982666
Epoch 770, val loss: 1.0570131540298462
Epoch 780, training loss: 0.6301718354225159 = 0.010220881551504135 + 0.1 * 6.199509620666504
Epoch 780, val loss: 1.06315016746521
Epoch 790, training loss: 0.6302064061164856 = 0.009894049726426601 + 0.1 * 6.203123092651367
Epoch 790, val loss: 1.0690882205963135
Epoch 800, training loss: 0.6291002035140991 = 0.009583291597664356 + 0.1 * 6.195168972015381
Epoch 800, val loss: 1.0749468803405762
Epoch 810, training loss: 0.6296883821487427 = 0.009289226494729519 + 0.1 * 6.203991413116455
Epoch 810, val loss: 1.0807651281356812
Epoch 820, training loss: 0.6280841827392578 = 0.009009378030896187 + 0.1 * 6.19074821472168
Epoch 820, val loss: 1.086395502090454
Epoch 830, training loss: 0.6271123290061951 = 0.008743762038648129 + 0.1 * 6.183685302734375
Epoch 830, val loss: 1.0919930934906006
Epoch 840, training loss: 0.6277326941490173 = 0.008490318432450294 + 0.1 * 6.192423343658447
Epoch 840, val loss: 1.097524642944336
Epoch 850, training loss: 0.6262772083282471 = 0.008248131722211838 + 0.1 * 6.180290699005127
Epoch 850, val loss: 1.1027889251708984
Epoch 860, training loss: 0.6268995404243469 = 0.008017775602638721 + 0.1 * 6.188817501068115
Epoch 860, val loss: 1.1081360578536987
Epoch 870, training loss: 0.6260167956352234 = 0.007797477766871452 + 0.1 * 6.182192802429199
Epoch 870, val loss: 1.1132540702819824
Epoch 880, training loss: 0.625095546245575 = 0.007587919477373362 + 0.1 * 6.175076484680176
Epoch 880, val loss: 1.1184107065200806
Epoch 890, training loss: 0.6248456835746765 = 0.007386927027255297 + 0.1 * 6.174587726593018
Epoch 890, val loss: 1.1234396696090698
Epoch 900, training loss: 0.6242204904556274 = 0.007194201927632093 + 0.1 * 6.170262336730957
Epoch 900, val loss: 1.1283636093139648
Epoch 910, training loss: 0.6263566017150879 = 0.007010084111243486 + 0.1 * 6.193464756011963
Epoch 910, val loss: 1.1332080364227295
Epoch 920, training loss: 0.6238964200019836 = 0.006833536084741354 + 0.1 * 6.170628547668457
Epoch 920, val loss: 1.1379603147506714
Epoch 930, training loss: 0.6240338683128357 = 0.006665012799203396 + 0.1 * 6.1736884117126465
Epoch 930, val loss: 1.1426960229873657
Epoch 940, training loss: 0.6228832006454468 = 0.006503500510007143 + 0.1 * 6.163796424865723
Epoch 940, val loss: 1.1473450660705566
Epoch 950, training loss: 0.6235421299934387 = 0.006348119117319584 + 0.1 * 6.171939849853516
Epoch 950, val loss: 1.1518712043762207
Epoch 960, training loss: 0.6221511363983154 = 0.0061986977234482765 + 0.1 * 6.159524440765381
Epoch 960, val loss: 1.1562596559524536
Epoch 970, training loss: 0.6227242946624756 = 0.006055629812180996 + 0.1 * 6.166686534881592
Epoch 970, val loss: 1.160706877708435
Epoch 980, training loss: 0.6231833696365356 = 0.005917686503380537 + 0.1 * 6.172657012939453
Epoch 980, val loss: 1.164971947669983
Epoch 990, training loss: 0.6218689680099487 = 0.005785036366432905 + 0.1 * 6.160839080810547
Epoch 990, val loss: 1.169094204902649
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9520
Flip ASR: 0.9422/225 nodes
The final ASR:0.77368, 0.13556, Accuracy:0.81481, 0.01600
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10568])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.98032, 0.00348, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7677114009857178 = 1.9303226470947266 + 0.1 * 8.373887062072754
Epoch 0, val loss: 1.9294928312301636
Epoch 10, training loss: 2.7582151889801025 = 1.9208450317382812 + 0.1 * 8.373702049255371
Epoch 10, val loss: 1.920700192451477
Epoch 20, training loss: 2.7457895278930664 = 1.9085040092468262 + 0.1 * 8.372854232788086
Epoch 20, val loss: 1.908949375152588
Epoch 30, training loss: 2.7274692058563232 = 1.8908318281173706 + 0.1 * 8.366373062133789
Epoch 30, val loss: 1.8919247388839722
Epoch 40, training loss: 2.6968271732330322 = 1.864797830581665 + 0.1 * 8.320292472839355
Epoch 40, val loss: 1.867119312286377
Epoch 50, training loss: 2.631067991256714 = 1.8300994634628296 + 0.1 * 8.009684562683105
Epoch 50, val loss: 1.835833191871643
Epoch 60, training loss: 2.5464165210723877 = 1.7936493158340454 + 0.1 * 7.527671813964844
Epoch 60, val loss: 1.8040140867233276
Epoch 70, training loss: 2.47080659866333 = 1.758347511291504 + 0.1 * 7.12459135055542
Epoch 70, val loss: 1.7728910446166992
Epoch 80, training loss: 2.4049782752990723 = 1.717613697052002 + 0.1 * 6.873645305633545
Epoch 80, val loss: 1.737770915031433
Epoch 90, training loss: 2.3408336639404297 = 1.663177728652954 + 0.1 * 6.776560306549072
Epoch 90, val loss: 1.6913284063339233
Epoch 100, training loss: 2.26485013961792 = 1.5906829833984375 + 0.1 * 6.741670608520508
Epoch 100, val loss: 1.6299608945846558
Epoch 110, training loss: 2.171937942504883 = 1.4998393058776855 + 0.1 * 6.7209858894348145
Epoch 110, val loss: 1.5542399883270264
Epoch 120, training loss: 2.0655574798583984 = 1.395052433013916 + 0.1 * 6.705051422119141
Epoch 120, val loss: 1.4672824144363403
Epoch 130, training loss: 1.9539661407470703 = 1.2848089933395386 + 0.1 * 6.6915717124938965
Epoch 130, val loss: 1.377297043800354
Epoch 140, training loss: 1.844193935394287 = 1.176483154296875 + 0.1 * 6.677106857299805
Epoch 140, val loss: 1.289141058921814
Epoch 150, training loss: 1.7407488822937012 = 1.0747994184494019 + 0.1 * 6.659494400024414
Epoch 150, val loss: 1.2079259157180786
Epoch 160, training loss: 1.6474814414978027 = 0.9831938743591309 + 0.1 * 6.6428751945495605
Epoch 160, val loss: 1.1379300355911255
Epoch 170, training loss: 1.5627703666687012 = 0.9005818367004395 + 0.1 * 6.621885776519775
Epoch 170, val loss: 1.076480507850647
Epoch 180, training loss: 1.4847822189331055 = 0.8243050575256348 + 0.1 * 6.604772090911865
Epoch 180, val loss: 1.021114468574524
Epoch 190, training loss: 1.4134504795074463 = 0.7534356117248535 + 0.1 * 6.6001482009887695
Epoch 190, val loss: 0.9704321026802063
Epoch 200, training loss: 1.346156358718872 = 0.6889377236366272 + 0.1 * 6.5721869468688965
Epoch 200, val loss: 0.9253320693969727
Epoch 210, training loss: 1.2858463525772095 = 0.6299261450767517 + 0.1 * 6.559201717376709
Epoch 210, val loss: 0.8853381276130676
Epoch 220, training loss: 1.2304086685180664 = 0.5760565400123596 + 0.1 * 6.543520927429199
Epoch 220, val loss: 0.8506506085395813
Epoch 230, training loss: 1.1792323589324951 = 0.5263624787330627 + 0.1 * 6.5286993980407715
Epoch 230, val loss: 0.8205366134643555
Epoch 240, training loss: 1.133455514907837 = 0.4804666340351105 + 0.1 * 6.52988862991333
Epoch 240, val loss: 0.7946866154670715
Epoch 250, training loss: 1.088948369026184 = 0.4384143054485321 + 0.1 * 6.505341053009033
Epoch 250, val loss: 0.7732427716255188
Epoch 260, training loss: 1.0494929552078247 = 0.3999145030975342 + 0.1 * 6.495784282684326
Epoch 260, val loss: 0.755772054195404
Epoch 270, training loss: 1.0137503147125244 = 0.36501315236091614 + 0.1 * 6.48737096786499
Epoch 270, val loss: 0.7425497174263
Epoch 280, training loss: 0.9814032316207886 = 0.3337358236312866 + 0.1 * 6.4766740798950195
Epoch 280, val loss: 0.7331797480583191
Epoch 290, training loss: 0.9523708820343018 = 0.3055330216884613 + 0.1 * 6.46837854385376
Epoch 290, val loss: 0.7270655035972595
Epoch 300, training loss: 0.9261325001716614 = 0.2798272371292114 + 0.1 * 6.463052749633789
Epoch 300, val loss: 0.7236111164093018
Epoch 310, training loss: 0.9005178213119507 = 0.2559524476528168 + 0.1 * 6.445653915405273
Epoch 310, val loss: 0.7222127318382263
Epoch 320, training loss: 0.8786448836326599 = 0.2333042025566101 + 0.1 * 6.453406810760498
Epoch 320, val loss: 0.7225510478019714
Epoch 330, training loss: 0.8548510670661926 = 0.2118891328573227 + 0.1 * 6.429618835449219
Epoch 330, val loss: 0.7242459058761597
Epoch 340, training loss: 0.8352680802345276 = 0.19178378582000732 + 0.1 * 6.434842586517334
Epoch 340, val loss: 0.7270909547805786
Epoch 350, training loss: 0.8142914175987244 = 0.17313005030155182 + 0.1 * 6.411613464355469
Epoch 350, val loss: 0.7311486005783081
Epoch 360, training loss: 0.7963343262672424 = 0.15599755942821503 + 0.1 * 6.403367519378662
Epoch 360, val loss: 0.7362215518951416
Epoch 370, training loss: 0.7811869382858276 = 0.14053411781787872 + 0.1 * 6.406528472900391
Epoch 370, val loss: 0.741962194442749
Epoch 380, training loss: 0.7654115557670593 = 0.12671713531017303 + 0.1 * 6.386943817138672
Epoch 380, val loss: 0.7484145164489746
Epoch 390, training loss: 0.7533261775970459 = 0.11436254531145096 + 0.1 * 6.389636039733887
Epoch 390, val loss: 0.7554850578308105
Epoch 400, training loss: 0.7406579256057739 = 0.10340393334627151 + 0.1 * 6.37253999710083
Epoch 400, val loss: 0.763006865978241
Epoch 410, training loss: 0.730858564376831 = 0.09369167685508728 + 0.1 * 6.371668815612793
Epoch 410, val loss: 0.7709872126579285
Epoch 420, training loss: 0.7214863896369934 = 0.08510030806064606 + 0.1 * 6.363860607147217
Epoch 420, val loss: 0.779341459274292
Epoch 430, training loss: 0.7129614353179932 = 0.0775177851319313 + 0.1 * 6.354435920715332
Epoch 430, val loss: 0.7879438400268555
Epoch 440, training loss: 0.7062826752662659 = 0.0708276554942131 + 0.1 * 6.354549884796143
Epoch 440, val loss: 0.7967314124107361
Epoch 450, training loss: 0.6998074054718018 = 0.06490998715162277 + 0.1 * 6.348974227905273
Epoch 450, val loss: 0.8056157827377319
Epoch 460, training loss: 0.6944043636322021 = 0.05966857820749283 + 0.1 * 6.347357749938965
Epoch 460, val loss: 0.8144744634628296
Epoch 470, training loss: 0.6884837746620178 = 0.055012162774801254 + 0.1 * 6.334715843200684
Epoch 470, val loss: 0.8233148455619812
Epoch 480, training loss: 0.6832031607627869 = 0.050844255834817886 + 0.1 * 6.323588848114014
Epoch 480, val loss: 0.8321848511695862
Epoch 490, training loss: 0.6793847680091858 = 0.047108832746744156 + 0.1 * 6.32275915145874
Epoch 490, val loss: 0.8408947587013245
Epoch 500, training loss: 0.6763245463371277 = 0.04375658184289932 + 0.1 * 6.325679302215576
Epoch 500, val loss: 0.8495749831199646
Epoch 510, training loss: 0.6723678112030029 = 0.04073857516050339 + 0.1 * 6.3162922859191895
Epoch 510, val loss: 0.85805743932724
Epoch 520, training loss: 0.6697247624397278 = 0.03801141306757927 + 0.1 * 6.31713342666626
Epoch 520, val loss: 0.8664629459381104
Epoch 530, training loss: 0.6654234528541565 = 0.03554213047027588 + 0.1 * 6.2988128662109375
Epoch 530, val loss: 0.8747670650482178
Epoch 540, training loss: 0.6632878184318542 = 0.03330014646053314 + 0.1 * 6.299876689910889
Epoch 540, val loss: 0.8829502463340759
Epoch 550, training loss: 0.6606388688087463 = 0.03126202151179314 + 0.1 * 6.293767929077148
Epoch 550, val loss: 0.8909186720848083
Epoch 560, training loss: 0.6579362154006958 = 0.029405733570456505 + 0.1 * 6.285305023193359
Epoch 560, val loss: 0.8987768888473511
Epoch 570, training loss: 0.6576516628265381 = 0.027707070112228394 + 0.1 * 6.299446105957031
Epoch 570, val loss: 0.906549334526062
Epoch 580, training loss: 0.6551740765571594 = 0.026152916252613068 + 0.1 * 6.2902116775512695
Epoch 580, val loss: 0.9140620231628418
Epoch 590, training loss: 0.6520571112632751 = 0.024728283286094666 + 0.1 * 6.273287773132324
Epoch 590, val loss: 0.9214473366737366
Epoch 600, training loss: 0.6504242420196533 = 0.023418646305799484 + 0.1 * 6.270055770874023
Epoch 600, val loss: 0.9287692308425903
Epoch 610, training loss: 0.6498888731002808 = 0.02220979891717434 + 0.1 * 6.276790618896484
Epoch 610, val loss: 0.9359353184700012
Epoch 620, training loss: 0.6474969387054443 = 0.021095972508192062 + 0.1 * 6.264009475708008
Epoch 620, val loss: 0.9428680539131165
Epoch 630, training loss: 0.6471245884895325 = 0.020066924393177032 + 0.1 * 6.270576477050781
Epoch 630, val loss: 0.9497409462928772
Epoch 640, training loss: 0.6449754238128662 = 0.01911383867263794 + 0.1 * 6.258615970611572
Epoch 640, val loss: 0.9564478993415833
Epoch 650, training loss: 0.6448960304260254 = 0.018228353932499886 + 0.1 * 6.266676425933838
Epoch 650, val loss: 0.9630512595176697
Epoch 660, training loss: 0.6426866054534912 = 0.017405344173312187 + 0.1 * 6.25281286239624
Epoch 660, val loss: 0.9694844484329224
Epoch 670, training loss: 0.6414886116981506 = 0.016639752313494682 + 0.1 * 6.248488426208496
Epoch 670, val loss: 0.9758488535881042
Epoch 680, training loss: 0.6406069993972778 = 0.015925845131278038 + 0.1 * 6.246811389923096
Epoch 680, val loss: 0.9819204807281494
Epoch 690, training loss: 0.6400782465934753 = 0.015262292698025703 + 0.1 * 6.248159408569336
Epoch 690, val loss: 0.9879215955734253
Epoch 700, training loss: 0.6393453478813171 = 0.01464106235653162 + 0.1 * 6.247043132781982
Epoch 700, val loss: 0.9938459396362305
Epoch 710, training loss: 0.6371980309486389 = 0.014058176428079605 + 0.1 * 6.231398582458496
Epoch 710, val loss: 0.9996984601020813
Epoch 720, training loss: 0.6383381485939026 = 0.01351095736026764 + 0.1 * 6.248271942138672
Epoch 720, val loss: 1.005384087562561
Epoch 730, training loss: 0.6363233327865601 = 0.012997275218367577 + 0.1 * 6.233260631561279
Epoch 730, val loss: 1.0109062194824219
Epoch 740, training loss: 0.6367512941360474 = 0.012515117414295673 + 0.1 * 6.242361545562744
Epoch 740, val loss: 1.0164096355438232
Epoch 750, training loss: 0.6347659230232239 = 0.012060269713401794 + 0.1 * 6.22705602645874
Epoch 750, val loss: 1.0217339992523193
Epoch 760, training loss: 0.6349850296974182 = 0.011631911620497704 + 0.1 * 6.2335309982299805
Epoch 760, val loss: 1.0270081758499146
Epoch 770, training loss: 0.6336318254470825 = 0.011227631941437721 + 0.1 * 6.224041938781738
Epoch 770, val loss: 1.0321742296218872
Epoch 780, training loss: 0.6344428658485413 = 0.010845093056559563 + 0.1 * 6.235977649688721
Epoch 780, val loss: 1.037253499031067
Epoch 790, training loss: 0.6322978734970093 = 0.010483356192708015 + 0.1 * 6.21814489364624
Epoch 790, val loss: 1.0421639680862427
Epoch 800, training loss: 0.6328314542770386 = 0.010140842758119106 + 0.1 * 6.2269062995910645
Epoch 800, val loss: 1.047072172164917
Epoch 810, training loss: 0.6312223076820374 = 0.009815549477934837 + 0.1 * 6.214067459106445
Epoch 810, val loss: 1.0518310070037842
Epoch 820, training loss: 0.6310773491859436 = 0.009507598355412483 + 0.1 * 6.215697288513184
Epoch 820, val loss: 1.0564920902252197
Epoch 830, training loss: 0.630649983882904 = 0.009215936064720154 + 0.1 * 6.2143402099609375
Epoch 830, val loss: 1.0610893964767456
Epoch 840, training loss: 0.6301668286323547 = 0.008938347920775414 + 0.1 * 6.212284564971924
Epoch 840, val loss: 1.0656723976135254
Epoch 850, training loss: 0.6288643479347229 = 0.008674039505422115 + 0.1 * 6.201902866363525
Epoch 850, val loss: 1.0701674222946167
Epoch 860, training loss: 0.629528820514679 = 0.008421755395829678 + 0.1 * 6.211070537567139
Epoch 860, val loss: 1.0745803117752075
Epoch 870, training loss: 0.6276286244392395 = 0.008181109093129635 + 0.1 * 6.194475173950195
Epoch 870, val loss: 1.0788671970367432
Epoch 880, training loss: 0.6285780072212219 = 0.00795203447341919 + 0.1 * 6.206259727478027
Epoch 880, val loss: 1.0831364393234253
Epoch 890, training loss: 0.6270607709884644 = 0.007733156438916922 + 0.1 * 6.1932759284973145
Epoch 890, val loss: 1.087331771850586
Epoch 900, training loss: 0.6267985105514526 = 0.007524228189140558 + 0.1 * 6.192742347717285
Epoch 900, val loss: 1.0915369987487793
Epoch 910, training loss: 0.6272743344306946 = 0.007323665544390678 + 0.1 * 6.1995062828063965
Epoch 910, val loss: 1.0955759286880493
Epoch 920, training loss: 0.6264604926109314 = 0.007132247090339661 + 0.1 * 6.193282604217529
Epoch 920, val loss: 1.0995233058929443
Epoch 930, training loss: 0.6265291571617126 = 0.00694965198636055 + 0.1 * 6.195794582366943
Epoch 930, val loss: 1.103408694267273
Epoch 940, training loss: 0.625255823135376 = 0.006774705369025469 + 0.1 * 6.184811115264893
Epoch 940, val loss: 1.107318639755249
Epoch 950, training loss: 0.6265655755996704 = 0.006607025861740112 + 0.1 * 6.199585914611816
Epoch 950, val loss: 1.1111528873443604
Epoch 960, training loss: 0.6256107091903687 = 0.006445867009460926 + 0.1 * 6.191648006439209
Epoch 960, val loss: 1.1148731708526611
Epoch 970, training loss: 0.6248646378517151 = 0.006291302852332592 + 0.1 * 6.185732841491699
Epoch 970, val loss: 1.1185282468795776
Epoch 980, training loss: 0.6247735619544983 = 0.0061432053335011005 + 0.1 * 6.18630313873291
Epoch 980, val loss: 1.1222046613693237
Epoch 990, training loss: 0.6234855055809021 = 0.0060005164705216885 + 0.1 * 6.174849987030029
Epoch 990, val loss: 1.1258097887039185
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.4871
Flip ASR: 0.3956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.766179323196411 = 1.9287887811660767 + 0.1 * 8.373906135559082
Epoch 0, val loss: 1.9259451627731323
Epoch 10, training loss: 2.756394863128662 = 1.9190127849578857 + 0.1 * 8.373819351196289
Epoch 10, val loss: 1.9158194065093994
Epoch 20, training loss: 2.7440032958984375 = 1.9066637754440308 + 0.1 * 8.373394012451172
Epoch 20, val loss: 1.9027185440063477
Epoch 30, training loss: 2.7261624336242676 = 1.889137625694275 + 0.1 * 8.370247840881348
Epoch 30, val loss: 1.8839268684387207
Epoch 40, training loss: 2.6978023052215576 = 1.863338828086853 + 0.1 * 8.344635009765625
Epoch 40, val loss: 1.8564319610595703
Epoch 50, training loss: 2.6431570053100586 = 1.8277192115783691 + 0.1 * 8.154376983642578
Epoch 50, val loss: 1.8202002048492432
Epoch 60, training loss: 2.557976484298706 = 1.78764009475708 + 0.1 * 7.703364372253418
Epoch 60, val loss: 1.7815041542053223
Epoch 70, training loss: 2.479410171508789 = 1.7475953102111816 + 0.1 * 7.318149566650391
Epoch 70, val loss: 1.7434585094451904
Epoch 80, training loss: 2.407149314880371 = 1.7013269662857056 + 0.1 * 7.05822229385376
Epoch 80, val loss: 1.7007039785385132
Epoch 90, training loss: 2.331444025039673 = 1.6417087316513062 + 0.1 * 6.897353172302246
Epoch 90, val loss: 1.648229718208313
Epoch 100, training loss: 2.2466864585876465 = 1.5653032064437866 + 0.1 * 6.813831806182861
Epoch 100, val loss: 1.5828346014022827
Epoch 110, training loss: 2.151461124420166 = 1.4750555753707886 + 0.1 * 6.76405668258667
Epoch 110, val loss: 1.5069390535354614
Epoch 120, training loss: 2.0521156787872314 = 1.3790949583053589 + 0.1 * 6.7302069664001465
Epoch 120, val loss: 1.4287186861038208
Epoch 130, training loss: 1.9538253545761108 = 1.2830935716629028 + 0.1 * 6.70731782913208
Epoch 130, val loss: 1.3542189598083496
Epoch 140, training loss: 1.8583345413208008 = 1.1894378662109375 + 0.1 * 6.688965797424316
Epoch 140, val loss: 1.2836793661117554
Epoch 150, training loss: 1.7663953304290771 = 1.0992568731307983 + 0.1 * 6.671384334564209
Epoch 150, val loss: 1.216924786567688
Epoch 160, training loss: 1.6791681051254272 = 1.0138027667999268 + 0.1 * 6.653653144836426
Epoch 160, val loss: 1.1547558307647705
Epoch 170, training loss: 1.5958374738693237 = 0.9323232769966125 + 0.1 * 6.635141849517822
Epoch 170, val loss: 1.0954811573028564
Epoch 180, training loss: 1.5184255838394165 = 0.8552471995353699 + 0.1 * 6.631783962249756
Epoch 180, val loss: 1.0395170450210571
Epoch 190, training loss: 1.445199966430664 = 0.7845723032951355 + 0.1 * 6.606276988983154
Epoch 190, val loss: 0.9883363842964172
Epoch 200, training loss: 1.3787081241607666 = 0.7194603681564331 + 0.1 * 6.592477798461914
Epoch 200, val loss: 0.9414889216423035
Epoch 210, training loss: 1.3190243244171143 = 0.6599956750869751 + 0.1 * 6.590285778045654
Epoch 210, val loss: 0.8996318578720093
Epoch 220, training loss: 1.2638499736785889 = 0.606864869594574 + 0.1 * 6.569850444793701
Epoch 220, val loss: 0.8637918829917908
Epoch 230, training loss: 1.2152906656265259 = 0.5592347383499146 + 0.1 * 6.560559272766113
Epoch 230, val loss: 0.833357036113739
Epoch 240, training loss: 1.1716809272766113 = 0.5164466500282288 + 0.1 * 6.552341938018799
Epoch 240, val loss: 0.8081920146942139
Epoch 250, training loss: 1.1333882808685303 = 0.47835206985473633 + 0.1 * 6.550361633300781
Epoch 250, val loss: 0.7879632115364075
Epoch 260, training loss: 1.0980257987976074 = 0.44430026412010193 + 0.1 * 6.53725528717041
Epoch 260, val loss: 0.7723268270492554
Epoch 270, training loss: 1.0655326843261719 = 0.41285666823387146 + 0.1 * 6.526760578155518
Epoch 270, val loss: 0.7601912617683411
Epoch 280, training loss: 1.0353922843933105 = 0.3828413784503937 + 0.1 * 6.525509357452393
Epoch 280, val loss: 0.75063157081604
Epoch 290, training loss: 1.0046122074127197 = 0.3534274697303772 + 0.1 * 6.511846542358398
Epoch 290, val loss: 0.743259608745575
Epoch 300, training loss: 0.9740276336669922 = 0.32377317547798157 + 0.1 * 6.50254487991333
Epoch 300, val loss: 0.7374131679534912
Epoch 310, training loss: 0.9447658061981201 = 0.2939065098762512 + 0.1 * 6.5085930824279785
Epoch 310, val loss: 0.7330693006515503
Epoch 320, training loss: 0.9142085313796997 = 0.26468661427497864 + 0.1 * 6.495218753814697
Epoch 320, val loss: 0.7302778959274292
Epoch 330, training loss: 0.8855348825454712 = 0.23698028922080994 + 0.1 * 6.485545635223389
Epoch 330, val loss: 0.7291837930679321
Epoch 340, training loss: 0.85984867811203 = 0.2117457240819931 + 0.1 * 6.481029033660889
Epoch 340, val loss: 0.7299596071243286
Epoch 350, training loss: 0.8361599445343018 = 0.18934527039527893 + 0.1 * 6.468146324157715
Epoch 350, val loss: 0.7327036261558533
Epoch 360, training loss: 0.8166622519493103 = 0.16977542638778687 + 0.1 * 6.468868255615234
Epoch 360, val loss: 0.7372164130210876
Epoch 370, training loss: 0.7980943918228149 = 0.1528717577457428 + 0.1 * 6.452226161956787
Epoch 370, val loss: 0.743330717086792
Epoch 380, training loss: 0.7823590636253357 = 0.13811828196048737 + 0.1 * 6.442408084869385
Epoch 380, val loss: 0.7505520582199097
Epoch 390, training loss: 0.7707622647285461 = 0.12519101798534393 + 0.1 * 6.45571231842041
Epoch 390, val loss: 0.758907675743103
Epoch 400, training loss: 0.7567842602729797 = 0.11385143548250198 + 0.1 * 6.429327964782715
Epoch 400, val loss: 0.7680855989456177
Epoch 410, training loss: 0.7465042471885681 = 0.10382755845785141 + 0.1 * 6.426766872406006
Epoch 410, val loss: 0.7780114412307739
Epoch 420, training loss: 0.7378756403923035 = 0.09495322406291962 + 0.1 * 6.429224014282227
Epoch 420, val loss: 0.7882525324821472
Epoch 430, training loss: 0.7281786203384399 = 0.08707736432552338 + 0.1 * 6.411012172698975
Epoch 430, val loss: 0.7986971139907837
Epoch 440, training loss: 0.7200763821601868 = 0.08002031594514847 + 0.1 * 6.4005608558654785
Epoch 440, val loss: 0.8093087673187256
Epoch 450, training loss: 0.7142168283462524 = 0.07366519421339035 + 0.1 * 6.405516147613525
Epoch 450, val loss: 0.8198028206825256
Epoch 460, training loss: 0.7070037126541138 = 0.06794743239879608 + 0.1 * 6.390562534332275
Epoch 460, val loss: 0.8302003741264343
Epoch 470, training loss: 0.7025402188301086 = 0.0627497211098671 + 0.1 * 6.397904872894287
Epoch 470, val loss: 0.8403826951980591
Epoch 480, training loss: 0.6959062814712524 = 0.05802004039287567 + 0.1 * 6.378861904144287
Epoch 480, val loss: 0.8503437638282776
Epoch 490, training loss: 0.690967321395874 = 0.053693681955337524 + 0.1 * 6.372735977172852
Epoch 490, val loss: 0.8600231409072876
Epoch 500, training loss: 0.6859779357910156 = 0.04972483217716217 + 0.1 * 6.3625311851501465
Epoch 500, val loss: 0.8695920705795288
Epoch 510, training loss: 0.6826547980308533 = 0.046019647270441055 + 0.1 * 6.366351127624512
Epoch 510, val loss: 0.8789466023445129
Epoch 520, training loss: 0.6781295537948608 = 0.0425775945186615 + 0.1 * 6.3555192947387695
Epoch 520, val loss: 0.8879305720329285
Epoch 530, training loss: 0.6737464070320129 = 0.03941163793206215 + 0.1 * 6.343347072601318
Epoch 530, val loss: 0.8966805338859558
Epoch 540, training loss: 0.6714954376220703 = 0.036486972123384476 + 0.1 * 6.35008430480957
Epoch 540, val loss: 0.9052426218986511
Epoch 550, training loss: 0.6680564880371094 = 0.03381536528468132 + 0.1 * 6.342411041259766
Epoch 550, val loss: 0.9134160876274109
Epoch 560, training loss: 0.6642205715179443 = 0.03137904405593872 + 0.1 * 6.328415393829346
Epoch 560, val loss: 0.9216689467430115
Epoch 570, training loss: 0.6631029844284058 = 0.029155835509300232 + 0.1 * 6.339471340179443
Epoch 570, val loss: 0.9293270707130432
Epoch 580, training loss: 0.6596719026565552 = 0.02717195264995098 + 0.1 * 6.3249993324279785
Epoch 580, val loss: 0.9369693398475647
Epoch 590, training loss: 0.6567614674568176 = 0.025392774492502213 + 0.1 * 6.313686847686768
Epoch 590, val loss: 0.9445602893829346
Epoch 600, training loss: 0.6549285054206848 = 0.023793155327439308 + 0.1 * 6.3113532066345215
Epoch 600, val loss: 0.9519069194793701
Epoch 610, training loss: 0.6524442434310913 = 0.02234763838350773 + 0.1 * 6.300966262817383
Epoch 610, val loss: 0.9593521952629089
Epoch 620, training loss: 0.6529877185821533 = 0.021026428788900375 + 0.1 * 6.319612979888916
Epoch 620, val loss: 0.9666932225227356
Epoch 630, training loss: 0.6500309109687805 = 0.019825562834739685 + 0.1 * 6.302052974700928
Epoch 630, val loss: 0.9737845063209534
Epoch 640, training loss: 0.6476255059242249 = 0.018732266500592232 + 0.1 * 6.2889323234558105
Epoch 640, val loss: 0.9809125065803528
Epoch 650, training loss: 0.6484870314598083 = 0.017731713131070137 + 0.1 * 6.307552814483643
Epoch 650, val loss: 0.9877538681030273
Epoch 660, training loss: 0.6451438069343567 = 0.016819022595882416 + 0.1 * 6.283247947692871
Epoch 660, val loss: 0.9945354461669922
Epoch 670, training loss: 0.6439134478569031 = 0.015977831557393074 + 0.1 * 6.279355525970459
Epoch 670, val loss: 1.001397728919983
Epoch 680, training loss: 0.645498514175415 = 0.015200676396489143 + 0.1 * 6.302978038787842
Epoch 680, val loss: 1.0080065727233887
Epoch 690, training loss: 0.6419057250022888 = 0.014486553147435188 + 0.1 * 6.274191856384277
Epoch 690, val loss: 1.0142309665679932
Epoch 700, training loss: 0.6403405666351318 = 0.013828428462147713 + 0.1 * 6.265120983123779
Epoch 700, val loss: 1.020767092704773
Epoch 710, training loss: 0.6399646997451782 = 0.013216497376561165 + 0.1 * 6.267481803894043
Epoch 710, val loss: 1.0269436836242676
Epoch 720, training loss: 0.6382405757904053 = 0.012648413889110088 + 0.1 * 6.255921363830566
Epoch 720, val loss: 1.0330770015716553
Epoch 730, training loss: 0.6387166380882263 = 0.012118502520024776 + 0.1 * 6.265981197357178
Epoch 730, val loss: 1.0392271280288696
Epoch 740, training loss: 0.6373409032821655 = 0.011623566970229149 + 0.1 * 6.25717306137085
Epoch 740, val loss: 1.0450018644332886
Epoch 750, training loss: 0.6365634799003601 = 0.01116251852363348 + 0.1 * 6.25400972366333
Epoch 750, val loss: 1.0508923530578613
Epoch 760, training loss: 0.6350017786026001 = 0.01073065958917141 + 0.1 * 6.242711067199707
Epoch 760, val loss: 1.0566213130950928
Epoch 770, training loss: 0.6348307132720947 = 0.010325745679438114 + 0.1 * 6.245049953460693
Epoch 770, val loss: 1.0622951984405518
Epoch 780, training loss: 0.6356308460235596 = 0.009944137185811996 + 0.1 * 6.256866931915283
Epoch 780, val loss: 1.0678783655166626
Epoch 790, training loss: 0.6345601081848145 = 0.009584974497556686 + 0.1 * 6.249751091003418
Epoch 790, val loss: 1.0731953382492065
Epoch 800, training loss: 0.6324952840805054 = 0.009248192422091961 + 0.1 * 6.232470512390137
Epoch 800, val loss: 1.078452467918396
Epoch 810, training loss: 0.6326472759246826 = 0.008930557407438755 + 0.1 * 6.237166881561279
Epoch 810, val loss: 1.0838204622268677
Epoch 820, training loss: 0.6325355768203735 = 0.008630158379673958 + 0.1 * 6.239054203033447
Epoch 820, val loss: 1.0888704061508179
Epoch 830, training loss: 0.6310866475105286 = 0.008347440510988235 + 0.1 * 6.227391719818115
Epoch 830, val loss: 1.0938572883605957
Epoch 840, training loss: 0.6320051550865173 = 0.008079287596046925 + 0.1 * 6.239258289337158
Epoch 840, val loss: 1.0988699197769165
Epoch 850, training loss: 0.6297909021377563 = 0.007825025357306004 + 0.1 * 6.219658851623535
Epoch 850, val loss: 1.1037222146987915
Epoch 860, training loss: 0.6303808093070984 = 0.00758362514898181 + 0.1 * 6.22797155380249
Epoch 860, val loss: 1.108625888824463
Epoch 870, training loss: 0.630341112613678 = 0.00735439034178853 + 0.1 * 6.229867458343506
Epoch 870, val loss: 1.113184928894043
Epoch 880, training loss: 0.630043625831604 = 0.00713763851672411 + 0.1 * 6.22905969619751
Epoch 880, val loss: 1.117844820022583
Epoch 890, training loss: 0.6277427673339844 = 0.006931365933269262 + 0.1 * 6.208114147186279
Epoch 890, val loss: 1.1224555969238281
Epoch 900, training loss: 0.6279478669166565 = 0.0067347390577197075 + 0.1 * 6.212131023406982
Epoch 900, val loss: 1.1270889043807983
Epoch 910, training loss: 0.6282048225402832 = 0.0065467944368720055 + 0.1 * 6.216580390930176
Epoch 910, val loss: 1.131433129310608
Epoch 920, training loss: 0.6272616386413574 = 0.006368330214172602 + 0.1 * 6.208932876586914
Epoch 920, val loss: 1.1357048749923706
Epoch 930, training loss: 0.6281237602233887 = 0.006197764538228512 + 0.1 * 6.219259738922119
Epoch 930, val loss: 1.1400973796844482
Epoch 940, training loss: 0.6265706419944763 = 0.00603495491668582 + 0.1 * 6.205356597900391
Epoch 940, val loss: 1.1443533897399902
Epoch 950, training loss: 0.6284301280975342 = 0.005878947209566832 + 0.1 * 6.2255120277404785
Epoch 950, val loss: 1.148536205291748
Epoch 960, training loss: 0.6264951825141907 = 0.005730141885578632 + 0.1 * 6.207650184631348
Epoch 960, val loss: 1.152644395828247
Epoch 970, training loss: 0.6267159581184387 = 0.0055876863189041615 + 0.1 * 6.211282253265381
Epoch 970, val loss: 1.156767725944519
Epoch 980, training loss: 0.6252580881118774 = 0.005451040808111429 + 0.1 * 6.198070526123047
Epoch 980, val loss: 1.1607197523117065
Epoch 990, training loss: 0.6260736584663391 = 0.0053201643750071526 + 0.1 * 6.2075347900390625
Epoch 990, val loss: 1.164657711982727
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7969887256622314 = 1.9596095085144043 + 0.1 * 8.373791694641113
Epoch 0, val loss: 1.960901141166687
Epoch 10, training loss: 2.7844176292419434 = 1.9471772909164429 + 0.1 * 8.372404098510742
Epoch 10, val loss: 1.9463387727737427
Epoch 20, training loss: 2.768373727798462 = 1.9313915967941284 + 0.1 * 8.369820594787598
Epoch 20, val loss: 1.9264644384384155
Epoch 30, training loss: 2.746992349624634 = 1.9105604887008667 + 0.1 * 8.364317893981934
Epoch 30, val loss: 1.9005070924758911
Epoch 40, training loss: 2.7163596153259277 = 1.8838804960250854 + 0.1 * 8.324790000915527
Epoch 40, val loss: 1.8704328536987305
Epoch 50, training loss: 2.65718412399292 = 1.8495079278945923 + 0.1 * 8.076761245727539
Epoch 50, val loss: 1.8355836868286133
Epoch 60, training loss: 2.576530933380127 = 1.8104685544967651 + 0.1 * 7.6606245040893555
Epoch 60, val loss: 1.7990899085998535
Epoch 70, training loss: 2.495742082595825 = 1.7703921794891357 + 0.1 * 7.253498554229736
Epoch 70, val loss: 1.7630956172943115
Epoch 80, training loss: 2.428666591644287 = 1.7318273782730103 + 0.1 * 6.968390941619873
Epoch 80, val loss: 1.7299524545669556
Epoch 90, training loss: 2.370669364929199 = 1.689149022102356 + 0.1 * 6.815203666687012
Epoch 90, val loss: 1.6919918060302734
Epoch 100, training loss: 2.3064444065093994 = 1.6313929557800293 + 0.1 * 6.750514507293701
Epoch 100, val loss: 1.6414724588394165
Epoch 110, training loss: 2.225975275039673 = 1.5561877489089966 + 0.1 * 6.697874546051025
Epoch 110, val loss: 1.5794789791107178
Epoch 120, training loss: 2.133080005645752 = 1.4672316312789917 + 0.1 * 6.658482551574707
Epoch 120, val loss: 1.5078842639923096
Epoch 130, training loss: 2.0369341373443604 = 1.3740997314453125 + 0.1 * 6.62834358215332
Epoch 130, val loss: 1.4347858428955078
Epoch 140, training loss: 1.945957899093628 = 1.2854007482528687 + 0.1 * 6.60557222366333
Epoch 140, val loss: 1.3672727346420288
Epoch 150, training loss: 1.8628127574920654 = 1.2045639753341675 + 0.1 * 6.582488059997559
Epoch 150, val loss: 1.3078187704086304
Epoch 160, training loss: 1.7853524684906006 = 1.1287952661514282 + 0.1 * 6.565572261810303
Epoch 160, val loss: 1.2529922723770142
Epoch 170, training loss: 1.7107751369476318 = 1.0536096096038818 + 0.1 * 6.5716552734375
Epoch 170, val loss: 1.1993030309677124
Epoch 180, training loss: 1.632148027420044 = 0.9774739742279053 + 0.1 * 6.546740531921387
Epoch 180, val loss: 1.1446797847747803
Epoch 190, training loss: 1.5521091222763062 = 0.898681104183197 + 0.1 * 6.534279823303223
Epoch 190, val loss: 1.0872315168380737
Epoch 200, training loss: 1.4716899394989014 = 0.8192759156227112 + 0.1 * 6.524139404296875
Epoch 200, val loss: 1.0292222499847412
Epoch 210, training loss: 1.396435260772705 = 0.7433933019638062 + 0.1 * 6.530420303344727
Epoch 210, val loss: 0.9732478260993958
Epoch 220, training loss: 1.3269340991973877 = 0.6757548451423645 + 0.1 * 6.511791706085205
Epoch 220, val loss: 0.9238780736923218
Epoch 230, training loss: 1.2649519443511963 = 0.6149630546569824 + 0.1 * 6.499888896942139
Epoch 230, val loss: 0.8805567026138306
Epoch 240, training loss: 1.2090020179748535 = 0.5597191452980042 + 0.1 * 6.492828845977783
Epoch 240, val loss: 0.8434480428695679
Epoch 250, training loss: 1.1575806140899658 = 0.509391188621521 + 0.1 * 6.4818949699401855
Epoch 250, val loss: 0.8121268153190613
Epoch 260, training loss: 1.1104085445404053 = 0.4632912278175354 + 0.1 * 6.471172332763672
Epoch 260, val loss: 0.7871360182762146
Epoch 270, training loss: 1.069069504737854 = 0.4211682975292206 + 0.1 * 6.4790120124816895
Epoch 270, val loss: 0.767492413520813
Epoch 280, training loss: 1.0291292667388916 = 0.3831961750984192 + 0.1 * 6.4593305587768555
Epoch 280, val loss: 0.7525376081466675
Epoch 290, training loss: 0.994205117225647 = 0.3489268720149994 + 0.1 * 6.45278263092041
Epoch 290, val loss: 0.7413362264633179
Epoch 300, training loss: 0.9619227647781372 = 0.3178258538246155 + 0.1 * 6.440968990325928
Epoch 300, val loss: 0.7329674959182739
Epoch 310, training loss: 0.9323399066925049 = 0.28921738266944885 + 0.1 * 6.431225299835205
Epoch 310, val loss: 0.7263875007629395
Epoch 320, training loss: 0.905206024646759 = 0.26260679960250854 + 0.1 * 6.425992012023926
Epoch 320, val loss: 0.7213762402534485
Epoch 330, training loss: 0.879489541053772 = 0.23762623965740204 + 0.1 * 6.418632984161377
Epoch 330, val loss: 0.7176710963249207
Epoch 340, training loss: 0.8573498725891113 = 0.21438108384609222 + 0.1 * 6.4296875
Epoch 340, val loss: 0.7150431275367737
Epoch 350, training loss: 0.8336610198020935 = 0.19312284886837006 + 0.1 * 6.405381679534912
Epoch 350, val loss: 0.7135767936706543
Epoch 360, training loss: 0.8134252429008484 = 0.1738787442445755 + 0.1 * 6.395464897155762
Epoch 360, val loss: 0.713327169418335
Epoch 370, training loss: 0.7958827614784241 = 0.15668118000030518 + 0.1 * 6.39201545715332
Epoch 370, val loss: 0.7143221497535706
Epoch 380, training loss: 0.7795789241790771 = 0.14148764312267303 + 0.1 * 6.3809123039245605
Epoch 380, val loss: 0.7164399027824402
Epoch 390, training loss: 0.7679284811019897 = 0.12812867760658264 + 0.1 * 6.3979973793029785
Epoch 390, val loss: 0.7196195721626282
Epoch 400, training loss: 0.7532885074615479 = 0.11645086854696274 + 0.1 * 6.3683762550354
Epoch 400, val loss: 0.7237181067466736
Epoch 410, training loss: 0.7415263652801514 = 0.10617002099752426 + 0.1 * 6.3535637855529785
Epoch 410, val loss: 0.7286604046821594
Epoch 420, training loss: 0.7320954203605652 = 0.09708889573812485 + 0.1 * 6.350065231323242
Epoch 420, val loss: 0.7342868447303772
Epoch 430, training loss: 0.7233951091766357 = 0.0890536904335022 + 0.1 * 6.343413829803467
Epoch 430, val loss: 0.7404969334602356
Epoch 440, training loss: 0.7153199911117554 = 0.08189627528190613 + 0.1 * 6.334237575531006
Epoch 440, val loss: 0.7471094131469727
Epoch 450, training loss: 0.7105079889297485 = 0.07552729547023773 + 0.1 * 6.349806785583496
Epoch 450, val loss: 0.7539482116699219
Epoch 460, training loss: 0.702487051486969 = 0.06984680891036987 + 0.1 * 6.326402187347412
Epoch 460, val loss: 0.7610664367675781
Epoch 470, training loss: 0.6962535977363586 = 0.06473179161548615 + 0.1 * 6.315217971801758
Epoch 470, val loss: 0.7682744264602661
Epoch 480, training loss: 0.6917464733123779 = 0.060114432126283646 + 0.1 * 6.316320419311523
Epoch 480, val loss: 0.7755829691886902
Epoch 490, training loss: 0.6867079734802246 = 0.05595150962471962 + 0.1 * 6.3075642585754395
Epoch 490, val loss: 0.7829916477203369
Epoch 500, training loss: 0.6827658414840698 = 0.052172861993312836 + 0.1 * 6.305930137634277
Epoch 500, val loss: 0.7903649210929871
Epoch 510, training loss: 0.6793171763420105 = 0.04873846843838692 + 0.1 * 6.305786609649658
Epoch 510, val loss: 0.7976770997047424
Epoch 520, training loss: 0.6753131151199341 = 0.045616958290338516 + 0.1 * 6.296961307525635
Epoch 520, val loss: 0.8049158453941345
Epoch 530, training loss: 0.6721975803375244 = 0.04277126118540764 + 0.1 * 6.294262886047363
Epoch 530, val loss: 0.8121584057807922
Epoch 540, training loss: 0.668025016784668 = 0.04017017036676407 + 0.1 * 6.278548240661621
Epoch 540, val loss: 0.8192400336265564
Epoch 550, training loss: 0.6664706468582153 = 0.037785738706588745 + 0.1 * 6.286849498748779
Epoch 550, val loss: 0.8263301849365234
Epoch 560, training loss: 0.6636906266212463 = 0.03559795767068863 + 0.1 * 6.28092622756958
Epoch 560, val loss: 0.833244800567627
Epoch 570, training loss: 0.6608256101608276 = 0.033590853214263916 + 0.1 * 6.272347450256348
Epoch 570, val loss: 0.840146005153656
Epoch 580, training loss: 0.65963214635849 = 0.031743329018354416 + 0.1 * 6.278887748718262
Epoch 580, val loss: 0.8469129800796509
Epoch 590, training loss: 0.6562102437019348 = 0.03004065714776516 + 0.1 * 6.261695384979248
Epoch 590, val loss: 0.8536025881767273
Epoch 600, training loss: 0.6554660201072693 = 0.028466811403632164 + 0.1 * 6.269991874694824
Epoch 600, val loss: 0.860202968120575
Epoch 610, training loss: 0.6526026725769043 = 0.027012577280402184 + 0.1 * 6.255900859832764
Epoch 610, val loss: 0.8666943311691284
Epoch 620, training loss: 0.6521097421646118 = 0.025665702298283577 + 0.1 * 6.264440536499023
Epoch 620, val loss: 0.8730610609054565
Epoch 630, training loss: 0.6492771506309509 = 0.024418659508228302 + 0.1 * 6.248584747314453
Epoch 630, val loss: 0.8793675899505615
Epoch 640, training loss: 0.6503429412841797 = 0.023259006440639496 + 0.1 * 6.270839691162109
Epoch 640, val loss: 0.8855575323104858
Epoch 650, training loss: 0.6473830342292786 = 0.02218160219490528 + 0.1 * 6.252013683319092
Epoch 650, val loss: 0.8916213512420654
Epoch 660, training loss: 0.6452116370201111 = 0.021176861599087715 + 0.1 * 6.240347385406494
Epoch 660, val loss: 0.8976432681083679
Epoch 670, training loss: 0.6463737487792969 = 0.020238423720002174 + 0.1 * 6.261353492736816
Epoch 670, val loss: 0.9034643173217773
Epoch 680, training loss: 0.6438903212547302 = 0.019363755360245705 + 0.1 * 6.245265483856201
Epoch 680, val loss: 0.9092158675193787
Epoch 690, training loss: 0.6422851085662842 = 0.018546322360634804 + 0.1 * 6.237387657165527
Epoch 690, val loss: 0.914947509765625
Epoch 700, training loss: 0.6417986154556274 = 0.01777970790863037 + 0.1 * 6.240189075469971
Epoch 700, val loss: 0.9204848408699036
Epoch 710, training loss: 0.6398089528083801 = 0.01706039346754551 + 0.1 * 6.227485656738281
Epoch 710, val loss: 0.9260028004646301
Epoch 720, training loss: 0.639609694480896 = 0.01638380065560341 + 0.1 * 6.2322587966918945
Epoch 720, val loss: 0.9313760995864868
Epoch 730, training loss: 0.6386117935180664 = 0.015749216079711914 + 0.1 * 6.228625774383545
Epoch 730, val loss: 0.9366315603256226
Epoch 740, training loss: 0.6383454203605652 = 0.015152279287576675 + 0.1 * 6.231931209564209
Epoch 740, val loss: 0.9418882131576538
Epoch 750, training loss: 0.6364700794219971 = 0.014589473605155945 + 0.1 * 6.21880578994751
Epoch 750, val loss: 0.9469865560531616
Epoch 760, training loss: 0.6356509327888489 = 0.014058491215109825 + 0.1 * 6.215924263000488
Epoch 760, val loss: 0.9519460201263428
Epoch 770, training loss: 0.6347562670707703 = 0.013557679951190948 + 0.1 * 6.211986064910889
Epoch 770, val loss: 0.9569226503372192
Epoch 780, training loss: 0.6355658173561096 = 0.013083843514323235 + 0.1 * 6.224820137023926
Epoch 780, val loss: 0.9618415832519531
Epoch 790, training loss: 0.6338127255439758 = 0.01263554859906435 + 0.1 * 6.2117719650268555
Epoch 790, val loss: 0.9665730595588684
Epoch 800, training loss: 0.6332342028617859 = 0.012210787273943424 + 0.1 * 6.21023416519165
Epoch 800, val loss: 0.9712504148483276
Epoch 810, training loss: 0.632306694984436 = 0.011808844283223152 + 0.1 * 6.204977989196777
Epoch 810, val loss: 0.9759075045585632
Epoch 820, training loss: 0.6324355602264404 = 0.011426960118114948 + 0.1 * 6.210085391998291
Epoch 820, val loss: 0.9805252552032471
Epoch 830, training loss: 0.632562518119812 = 0.011063856072723866 + 0.1 * 6.214986324310303
Epoch 830, val loss: 0.9849275946617126
Epoch 840, training loss: 0.6307850480079651 = 0.01071965228766203 + 0.1 * 6.200653553009033
Epoch 840, val loss: 0.9893364906311035
Epoch 850, training loss: 0.6309241056442261 = 0.010392150841653347 + 0.1 * 6.205319404602051
Epoch 850, val loss: 0.9937654733657837
Epoch 860, training loss: 0.6314316391944885 = 0.010079852305352688 + 0.1 * 6.213517665863037
Epoch 860, val loss: 0.997950553894043
Epoch 870, training loss: 0.6292052865028381 = 0.00978298019617796 + 0.1 * 6.194222927093506
Epoch 870, val loss: 1.0021328926086426
Epoch 880, training loss: 0.6295784115791321 = 0.009499883279204369 + 0.1 * 6.200785160064697
Epoch 880, val loss: 1.006312608718872
Epoch 890, training loss: 0.6285492777824402 = 0.009229402057826519 + 0.1 * 6.193198204040527
Epoch 890, val loss: 1.0103768110275269
Epoch 900, training loss: 0.6283255815505981 = 0.00897078588604927 + 0.1 * 6.19354772567749
Epoch 900, val loss: 1.0143675804138184
Epoch 910, training loss: 0.6285360455513 = 0.008723692037165165 + 0.1 * 6.198123455047607
Epoch 910, val loss: 1.0183689594268799
Epoch 920, training loss: 0.6279064416885376 = 0.00848747044801712 + 0.1 * 6.194189548492432
Epoch 920, val loss: 1.0222022533416748
Epoch 930, training loss: 0.6267733573913574 = 0.008261855691671371 + 0.1 * 6.185114860534668
Epoch 930, val loss: 1.0260249376296997
Epoch 940, training loss: 0.6270352602005005 = 0.008046033792197704 + 0.1 * 6.189892292022705
Epoch 940, val loss: 1.0298885107040405
Epoch 950, training loss: 0.6261902451515198 = 0.007838869467377663 + 0.1 * 6.18351411819458
Epoch 950, val loss: 1.033564567565918
Epoch 960, training loss: 0.627526581287384 = 0.007640032563358545 + 0.1 * 6.1988654136657715
Epoch 960, val loss: 1.0372450351715088
Epoch 970, training loss: 0.625430166721344 = 0.0074496520683169365 + 0.1 * 6.179805278778076
Epoch 970, val loss: 1.0407943725585938
Epoch 980, training loss: 0.6258504986763 = 0.007267128210514784 + 0.1 * 6.185833930969238
Epoch 980, val loss: 1.0443943738937378
Epoch 990, training loss: 0.6243948936462402 = 0.00709157157689333 + 0.1 * 6.173032760620117
Epoch 990, val loss: 1.0479155778884888
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.82042, 0.23582, Accuracy:0.82222, 0.01048
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11666])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10618])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00758, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7782208919525146 = 1.94083571434021 + 0.1 * 8.373851776123047
Epoch 0, val loss: 1.9472674131393433
Epoch 10, training loss: 2.768874406814575 = 1.9315131902694702 + 0.1 * 8.373611450195312
Epoch 10, val loss: 1.9385408163070679
Epoch 20, training loss: 2.7566778659820557 = 1.9194644689559937 + 0.1 * 8.3721342086792
Epoch 20, val loss: 1.9269386529922485
Epoch 30, training loss: 2.7378149032592773 = 1.9017865657806396 + 0.1 * 8.360283851623535
Epoch 30, val loss: 1.909875512123108
Epoch 40, training loss: 2.7030255794525146 = 1.8750102519989014 + 0.1 * 8.280152320861816
Epoch 40, val loss: 1.8845242261886597
Epoch 50, training loss: 2.620075225830078 = 1.8396003246307373 + 0.1 * 7.804747581481934
Epoch 50, val loss: 1.8521223068237305
Epoch 60, training loss: 2.549562692642212 = 1.7991162538528442 + 0.1 * 7.504464149475098
Epoch 60, val loss: 1.8152832984924316
Epoch 70, training loss: 2.476283073425293 = 1.7575867176055908 + 0.1 * 7.186962127685547
Epoch 70, val loss: 1.7787169218063354
Epoch 80, training loss: 2.4105262756347656 = 1.7138692140579224 + 0.1 * 6.966571807861328
Epoch 80, val loss: 1.740523099899292
Epoch 90, training loss: 2.341944694519043 = 1.658247709274292 + 0.1 * 6.836968421936035
Epoch 90, val loss: 1.6893417835235596
Epoch 100, training loss: 2.2617580890655518 = 1.5853620767593384 + 0.1 * 6.7639594078063965
Epoch 100, val loss: 1.625325083732605
Epoch 110, training loss: 2.168592691421509 = 1.4967010021209717 + 0.1 * 6.718916893005371
Epoch 110, val loss: 1.5510140657424927
Epoch 120, training loss: 2.067319393157959 = 1.3984767198562622 + 0.1 * 6.688426494598389
Epoch 120, val loss: 1.4680663347244263
Epoch 130, training loss: 1.9635043144226074 = 1.297054648399353 + 0.1 * 6.664497375488281
Epoch 130, val loss: 1.3845710754394531
Epoch 140, training loss: 1.8605117797851562 = 1.1964936256408691 + 0.1 * 6.640181541442871
Epoch 140, val loss: 1.3034932613372803
Epoch 150, training loss: 1.7607295513153076 = 1.0987719297409058 + 0.1 * 6.619576454162598
Epoch 150, val loss: 1.2266011238098145
Epoch 160, training loss: 1.6651103496551514 = 1.0052961111068726 + 0.1 * 6.598142623901367
Epoch 160, val loss: 1.1536792516708374
Epoch 170, training loss: 1.5765821933746338 = 0.91826331615448 + 0.1 * 6.583189487457275
Epoch 170, val loss: 1.0870860815048218
Epoch 180, training loss: 1.4940333366394043 = 0.83744215965271 + 0.1 * 6.565912246704102
Epoch 180, val loss: 1.025907278060913
Epoch 190, training loss: 1.4187864065170288 = 0.7636171579360962 + 0.1 * 6.551692485809326
Epoch 190, val loss: 0.9717959761619568
Epoch 200, training loss: 1.3515944480895996 = 0.6975262761116028 + 0.1 * 6.540680885314941
Epoch 200, val loss: 0.9262956380844116
Epoch 210, training loss: 1.2911347150802612 = 0.6377676725387573 + 0.1 * 6.533670425415039
Epoch 210, val loss: 0.8882383108139038
Epoch 220, training loss: 1.2354762554168701 = 0.584050178527832 + 0.1 * 6.514260292053223
Epoch 220, val loss: 0.8570988178253174
Epoch 230, training loss: 1.1853361129760742 = 0.5351699590682983 + 0.1 * 6.5016608238220215
Epoch 230, val loss: 0.8314971327781677
Epoch 240, training loss: 1.1402395963668823 = 0.4903669059276581 + 0.1 * 6.498727321624756
Epoch 240, val loss: 0.8104275465011597
Epoch 250, training loss: 1.097517490386963 = 0.4493962526321411 + 0.1 * 6.481212139129639
Epoch 250, val loss: 0.7932857871055603
Epoch 260, training loss: 1.0582996606826782 = 0.4114193320274353 + 0.1 * 6.4688029289245605
Epoch 260, val loss: 0.7792202234268188
Epoch 270, training loss: 1.0232832431793213 = 0.37629419565200806 + 0.1 * 6.46989107131958
Epoch 270, val loss: 0.7679348587989807
Epoch 280, training loss: 0.9890270233154297 = 0.3439396023750305 + 0.1 * 6.450873851776123
Epoch 280, val loss: 0.7593927979469299
Epoch 290, training loss: 0.9582693576812744 = 0.3139703869819641 + 0.1 * 6.442989349365234
Epoch 290, val loss: 0.7532243728637695
Epoch 300, training loss: 0.9309636950492859 = 0.2863779067993164 + 0.1 * 6.445857524871826
Epoch 300, val loss: 0.7492743134498596
Epoch 310, training loss: 0.9037582874298096 = 0.2609463632106781 + 0.1 * 6.42811918258667
Epoch 310, val loss: 0.7473087310791016
Epoch 320, training loss: 0.878778874874115 = 0.23711490631103516 + 0.1 * 6.41663932800293
Epoch 320, val loss: 0.7469964623451233
Epoch 330, training loss: 0.8557944297790527 = 0.21454483270645142 + 0.1 * 6.412496089935303
Epoch 330, val loss: 0.7480958700180054
Epoch 340, training loss: 0.8349449634552002 = 0.19333437085151672 + 0.1 * 6.4161057472229
Epoch 340, val loss: 0.7503133416175842
Epoch 350, training loss: 0.8146830797195435 = 0.17374303936958313 + 0.1 * 6.40939998626709
Epoch 350, val loss: 0.7535675168037415
Epoch 360, training loss: 0.7950592041015625 = 0.15566107630729675 + 0.1 * 6.393980979919434
Epoch 360, val loss: 0.7578732967376709
Epoch 370, training loss: 0.7775888442993164 = 0.1391882300376892 + 0.1 * 6.384006023406982
Epoch 370, val loss: 0.763374388217926
Epoch 380, training loss: 0.7625378966331482 = 0.12447017431259155 + 0.1 * 6.380677223205566
Epoch 380, val loss: 0.7698883414268494
Epoch 390, training loss: 0.7495740652084351 = 0.11159303784370422 + 0.1 * 6.379809856414795
Epoch 390, val loss: 0.777368426322937
Epoch 400, training loss: 0.7370818853378296 = 0.1003827229142189 + 0.1 * 6.3669915199279785
Epoch 400, val loss: 0.7856156229972839
Epoch 410, training loss: 0.7271136045455933 = 0.09061302989721298 + 0.1 * 6.365005970001221
Epoch 410, val loss: 0.7945501208305359
Epoch 420, training loss: 0.7177395224571228 = 0.08210233598947525 + 0.1 * 6.3563714027404785
Epoch 420, val loss: 0.8041679263114929
Epoch 430, training loss: 0.7097126245498657 = 0.07465982437133789 + 0.1 * 6.350527763366699
Epoch 430, val loss: 0.8142434358596802
Epoch 440, training loss: 0.703444242477417 = 0.06812744587659836 + 0.1 * 6.353167533874512
Epoch 440, val loss: 0.8248491287231445
Epoch 450, training loss: 0.6972486972808838 = 0.06235627830028534 + 0.1 * 6.348924160003662
Epoch 450, val loss: 0.8357505798339844
Epoch 460, training loss: 0.6903263330459595 = 0.057239681482315063 + 0.1 * 6.330866813659668
Epoch 460, val loss: 0.8469627499580383
Epoch 470, training loss: 0.6861349940299988 = 0.05267499014735222 + 0.1 * 6.33459997177124
Epoch 470, val loss: 0.8583711981773376
Epoch 480, training loss: 0.6825031638145447 = 0.04859467223286629 + 0.1 * 6.339085102081299
Epoch 480, val loss: 0.8699278831481934
Epoch 490, training loss: 0.6765221953392029 = 0.04494210332632065 + 0.1 * 6.31580114364624
Epoch 490, val loss: 0.8814985752105713
Epoch 500, training loss: 0.6745283007621765 = 0.041653700172901154 + 0.1 * 6.3287458419799805
Epoch 500, val loss: 0.8931281566619873
Epoch 510, training loss: 0.6700100302696228 = 0.038701869547367096 + 0.1 * 6.31308126449585
Epoch 510, val loss: 0.9045147895812988
Epoch 520, training loss: 0.6661409139633179 = 0.0360388420522213 + 0.1 * 6.301020622253418
Epoch 520, val loss: 0.9158815741539001
Epoch 530, training loss: 0.6639470458030701 = 0.03362211212515831 + 0.1 * 6.303249359130859
Epoch 530, val loss: 0.9270620346069336
Epoch 540, training loss: 0.6610932350158691 = 0.03143235668540001 + 0.1 * 6.296608924865723
Epoch 540, val loss: 0.9380556344985962
Epoch 550, training loss: 0.6584296822547913 = 0.029446111992001534 + 0.1 * 6.289835453033447
Epoch 550, val loss: 0.9487749934196472
Epoch 560, training loss: 0.6562342643737793 = 0.027640629559755325 + 0.1 * 6.285935878753662
Epoch 560, val loss: 0.95928955078125
Epoch 570, training loss: 0.6559179425239563 = 0.025993812829256058 + 0.1 * 6.299241065979004
Epoch 570, val loss: 0.9694558382034302
Epoch 580, training loss: 0.6525712609291077 = 0.024493344128131866 + 0.1 * 6.280778884887695
Epoch 580, val loss: 0.9793978929519653
Epoch 590, training loss: 0.650812029838562 = 0.023117991164326668 + 0.1 * 6.276939868927002
Epoch 590, val loss: 0.9891112446784973
Epoch 600, training loss: 0.6488597393035889 = 0.02185562252998352 + 0.1 * 6.270040512084961
Epoch 600, val loss: 0.9985228180885315
Epoch 610, training loss: 0.6478080749511719 = 0.020694958046078682 + 0.1 * 6.2711310386657715
Epoch 610, val loss: 1.0077989101409912
Epoch 620, training loss: 0.6484004855155945 = 0.019627343863248825 + 0.1 * 6.287731170654297
Epoch 620, val loss: 1.0166302919387817
Epoch 630, training loss: 0.6452575922012329 = 0.018646957352757454 + 0.1 * 6.266106128692627
Epoch 630, val loss: 1.0253498554229736
Epoch 640, training loss: 0.6443027257919312 = 0.01774151436984539 + 0.1 * 6.2656121253967285
Epoch 640, val loss: 1.0338467359542847
Epoch 650, training loss: 0.6426695585250854 = 0.016901735216379166 + 0.1 * 6.257678031921387
Epoch 650, val loss: 1.042107105255127
Epoch 660, training loss: 0.6419014930725098 = 0.01612291857600212 + 0.1 * 6.257785320281982
Epoch 660, val loss: 1.050168514251709
Epoch 670, training loss: 0.6401486396789551 = 0.01539927814155817 + 0.1 * 6.247493743896484
Epoch 670, val loss: 1.0580430030822754
Epoch 680, training loss: 0.640595555305481 = 0.014725160785019398 + 0.1 * 6.258703708648682
Epoch 680, val loss: 1.0658016204833984
Epoch 690, training loss: 0.6398257613182068 = 0.014097457751631737 + 0.1 * 6.2572832107543945
Epoch 690, val loss: 1.0731996297836304
Epoch 700, training loss: 0.6381124258041382 = 0.01351204328238964 + 0.1 * 6.246004104614258
Epoch 700, val loss: 1.0805193185806274
Epoch 710, training loss: 0.6367690563201904 = 0.012965151108801365 + 0.1 * 6.238039016723633
Epoch 710, val loss: 1.0877050161361694
Epoch 720, training loss: 0.6368812322616577 = 0.012451788410544395 + 0.1 * 6.2442946434021
Epoch 720, val loss: 1.0947387218475342
Epoch 730, training loss: 0.6353530883789062 = 0.011970052495598793 + 0.1 * 6.233830451965332
Epoch 730, val loss: 1.101486325263977
Epoch 740, training loss: 0.6365875601768494 = 0.011518970131874084 + 0.1 * 6.250686168670654
Epoch 740, val loss: 1.1082338094711304
Epoch 750, training loss: 0.6342628002166748 = 0.011095710098743439 + 0.1 * 6.23167085647583
Epoch 750, val loss: 1.1146676540374756
Epoch 760, training loss: 0.633038341999054 = 0.010697819292545319 + 0.1 * 6.223404884338379
Epoch 760, val loss: 1.1211107969284058
Epoch 770, training loss: 0.6331391930580139 = 0.010321969166398048 + 0.1 * 6.2281718254089355
Epoch 770, val loss: 1.1273901462554932
Epoch 780, training loss: 0.6324670910835266 = 0.009966266341507435 + 0.1 * 6.225008487701416
Epoch 780, val loss: 1.1334829330444336
Epoch 790, training loss: 0.6316866874694824 = 0.009631025604903698 + 0.1 * 6.220556259155273
Epoch 790, val loss: 1.139411211013794
Epoch 800, training loss: 0.6318406462669373 = 0.009314329363405704 + 0.1 * 6.225262641906738
Epoch 800, val loss: 1.145371437072754
Epoch 810, training loss: 0.6304715871810913 = 0.009014186449348927 + 0.1 * 6.214573860168457
Epoch 810, val loss: 1.151181697845459
Epoch 820, training loss: 0.6310169696807861 = 0.00872902199625969 + 0.1 * 6.222878932952881
Epoch 820, val loss: 1.1568033695220947
Epoch 830, training loss: 0.6303508877754211 = 0.008459514938294888 + 0.1 * 6.218913555145264
Epoch 830, val loss: 1.1622967720031738
Epoch 840, training loss: 0.6295696496963501 = 0.008203425444662571 + 0.1 * 6.213662147521973
Epoch 840, val loss: 1.1677324771881104
Epoch 850, training loss: 0.6283701062202454 = 0.007960564456880093 + 0.1 * 6.204095363616943
Epoch 850, val loss: 1.1730283498764038
Epoch 860, training loss: 0.6284962892532349 = 0.007729637902230024 + 0.1 * 6.207666397094727
Epoch 860, val loss: 1.1783231496810913
Epoch 870, training loss: 0.6280185580253601 = 0.007509235758334398 + 0.1 * 6.2050933837890625
Epoch 870, val loss: 1.1834402084350586
Epoch 880, training loss: 0.6279420852661133 = 0.0072991871275007725 + 0.1 * 6.2064290046691895
Epoch 880, val loss: 1.1884950399398804
Epoch 890, training loss: 0.6272201538085938 = 0.007098651025444269 + 0.1 * 6.201214790344238
Epoch 890, val loss: 1.1934144496917725
Epoch 900, training loss: 0.6272953748703003 = 0.006907807197421789 + 0.1 * 6.203875541687012
Epoch 900, val loss: 1.198338508605957
Epoch 910, training loss: 0.6269969940185547 = 0.00672539509832859 + 0.1 * 6.202715873718262
Epoch 910, val loss: 1.2031351327896118
Epoch 920, training loss: 0.6255248785018921 = 0.006550956051796675 + 0.1 * 6.189739227294922
Epoch 920, val loss: 1.2078880071640015
Epoch 930, training loss: 0.6280269026756287 = 0.006383913103491068 + 0.1 * 6.216429710388184
Epoch 930, val loss: 1.2125109434127808
Epoch 940, training loss: 0.6259419918060303 = 0.0062242597341537476 + 0.1 * 6.197176933288574
Epoch 940, val loss: 1.216996669769287
Epoch 950, training loss: 0.6253283619880676 = 0.006071669049561024 + 0.1 * 6.192566871643066
Epoch 950, val loss: 1.2214674949645996
Epoch 960, training loss: 0.6258484125137329 = 0.005925574339926243 + 0.1 * 6.199228286743164
Epoch 960, val loss: 1.225923776626587
Epoch 970, training loss: 0.6250020861625671 = 0.005785264540463686 + 0.1 * 6.19216775894165
Epoch 970, val loss: 1.2301719188690186
Epoch 980, training loss: 0.6249836087226868 = 0.005650756880640984 + 0.1 * 6.193328380584717
Epoch 980, val loss: 1.2344801425933838
Epoch 990, training loss: 0.6237731575965881 = 0.0055214413441717625 + 0.1 * 6.182517051696777
Epoch 990, val loss: 1.2386550903320312
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5240
Flip ASR: 0.4311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.777663230895996 = 1.940273404121399 + 0.1 * 8.373897552490234
Epoch 0, val loss: 1.9428647756576538
Epoch 10, training loss: 2.7676095962524414 = 1.9302289485931396 + 0.1 * 8.37380599975586
Epoch 10, val loss: 1.9326494932174683
Epoch 20, training loss: 2.755518913269043 = 1.91819167137146 + 0.1 * 8.373270988464355
Epoch 20, val loss: 1.9199352264404297
Epoch 30, training loss: 2.738649368286133 = 1.901766300201416 + 0.1 * 8.368829727172852
Epoch 30, val loss: 1.902246356010437
Epoch 40, training loss: 2.7117910385131836 = 1.8780510425567627 + 0.1 * 8.337398529052734
Epoch 40, val loss: 1.8768059015274048
Epoch 50, training loss: 2.6610307693481445 = 1.8453896045684814 + 0.1 * 8.156412124633789
Epoch 50, val loss: 1.843230128288269
Epoch 60, training loss: 2.5804622173309326 = 1.8075566291809082 + 0.1 * 7.729055404663086
Epoch 60, val loss: 1.8072829246520996
Epoch 70, training loss: 2.503788948059082 = 1.768860936164856 + 0.1 * 7.349279403686523
Epoch 70, val loss: 1.7722349166870117
Epoch 80, training loss: 2.4343371391296387 = 1.7267661094665527 + 0.1 * 7.075711250305176
Epoch 80, val loss: 1.7361235618591309
Epoch 90, training loss: 2.3687639236450195 = 1.6727030277252197 + 0.1 * 6.9606099128723145
Epoch 90, val loss: 1.6895277500152588
Epoch 100, training loss: 2.291203737258911 = 1.6010692119598389 + 0.1 * 6.901345729827881
Epoch 100, val loss: 1.6265753507614136
Epoch 110, training loss: 2.1983842849731445 = 1.512221336364746 + 0.1 * 6.861629486083984
Epoch 110, val loss: 1.5506340265274048
Epoch 120, training loss: 2.0937790870666504 = 1.4105019569396973 + 0.1 * 6.8327717781066895
Epoch 120, val loss: 1.4652769565582275
Epoch 130, training loss: 1.9824713468551636 = 1.3013956546783447 + 0.1 * 6.810756683349609
Epoch 130, val loss: 1.3764479160308838
Epoch 140, training loss: 1.8676609992980957 = 1.1890432834625244 + 0.1 * 6.786177635192871
Epoch 140, val loss: 1.2878687381744385
Epoch 150, training loss: 1.753122329711914 = 1.0777838230133057 + 0.1 * 6.753384113311768
Epoch 150, val loss: 1.20211923122406
Epoch 160, training loss: 1.6442583799362183 = 0.9721254706382751 + 0.1 * 6.721329212188721
Epoch 160, val loss: 1.1216741800308228
Epoch 170, training loss: 1.5461281538009644 = 0.8762214183807373 + 0.1 * 6.699067115783691
Epoch 170, val loss: 1.049991488456726
Epoch 180, training loss: 1.4575252532958984 = 0.7895492315292358 + 0.1 * 6.679760456085205
Epoch 180, val loss: 0.9865496754646301
Epoch 190, training loss: 1.3787274360656738 = 0.7124913930892944 + 0.1 * 6.662359714508057
Epoch 190, val loss: 0.932662308216095
Epoch 200, training loss: 1.3089730739593506 = 0.6448389291763306 + 0.1 * 6.641340732574463
Epoch 200, val loss: 0.8885812163352966
Epoch 210, training loss: 1.2481582164764404 = 0.5854325294494629 + 0.1 * 6.627255916595459
Epoch 210, val loss: 0.8534455299377441
Epoch 220, training loss: 1.1944453716278076 = 0.5334522128105164 + 0.1 * 6.609930992126465
Epoch 220, val loss: 0.8261352181434631
Epoch 230, training loss: 1.1473625898361206 = 0.4875553250312805 + 0.1 * 6.598072528839111
Epoch 230, val loss: 0.8054841160774231
Epoch 240, training loss: 1.1052260398864746 = 0.4468379318714142 + 0.1 * 6.583881378173828
Epoch 240, val loss: 0.7903451919555664
Epoch 250, training loss: 1.067576289176941 = 0.41027167439460754 + 0.1 * 6.57304573059082
Epoch 250, val loss: 0.7794020771980286
Epoch 260, training loss: 1.033597707748413 = 0.3772304654121399 + 0.1 * 6.563671588897705
Epoch 260, val loss: 0.7717748880386353
Epoch 270, training loss: 1.0019166469573975 = 0.3470425307750702 + 0.1 * 6.548741340637207
Epoch 270, val loss: 0.7668940424919128
Epoch 280, training loss: 0.9740397930145264 = 0.3192042112350464 + 0.1 * 6.548355579376221
Epoch 280, val loss: 0.7641873955726624
Epoch 290, training loss: 0.9470193386077881 = 0.29351139068603516 + 0.1 * 6.535079479217529
Epoch 290, val loss: 0.7631567120552063
Epoch 300, training loss: 0.9213652014732361 = 0.2693232297897339 + 0.1 * 6.520419597625732
Epoch 300, val loss: 0.7634544372558594
Epoch 310, training loss: 0.8979085087776184 = 0.24648840725421906 + 0.1 * 6.5142011642456055
Epoch 310, val loss: 0.7648699879646301
Epoch 320, training loss: 0.8755250573158264 = 0.22498822212219238 + 0.1 * 6.505368232727051
Epoch 320, val loss: 0.7671512365341187
Epoch 330, training loss: 0.8541285991668701 = 0.20468053221702576 + 0.1 * 6.494480133056641
Epoch 330, val loss: 0.7703276872634888
Epoch 340, training loss: 0.8351403474807739 = 0.18565039336681366 + 0.1 * 6.494899272918701
Epoch 340, val loss: 0.7743642926216125
Epoch 350, training loss: 0.8166568875312805 = 0.16807615756988525 + 0.1 * 6.485807418823242
Epoch 350, val loss: 0.7790950536727905
Epoch 360, training loss: 0.7997398376464844 = 0.15194091200828552 + 0.1 * 6.477989673614502
Epoch 360, val loss: 0.7845485210418701
Epoch 370, training loss: 0.7836993336677551 = 0.13723021745681763 + 0.1 * 6.464691162109375
Epoch 370, val loss: 0.7906787991523743
Epoch 380, training loss: 0.7703028917312622 = 0.1239297166466713 + 0.1 * 6.463731288909912
Epoch 380, val loss: 0.7975096106529236
Epoch 390, training loss: 0.7569202184677124 = 0.11202120780944824 + 0.1 * 6.4489898681640625
Epoch 390, val loss: 0.8049235343933105
Epoch 400, training loss: 0.7467566728591919 = 0.10135683417320251 + 0.1 * 6.45399808883667
Epoch 400, val loss: 0.8128640651702881
Epoch 410, training loss: 0.7349631190299988 = 0.09184418618679047 + 0.1 * 6.431189060211182
Epoch 410, val loss: 0.8211292028427124
Epoch 420, training loss: 0.7252513766288757 = 0.08334114402532578 + 0.1 * 6.419102191925049
Epoch 420, val loss: 0.8298472166061401
Epoch 430, training loss: 0.7174555659294128 = 0.0757652297616005 + 0.1 * 6.416903018951416
Epoch 430, val loss: 0.8387448787689209
Epoch 440, training loss: 0.7092719078063965 = 0.06905314326286316 + 0.1 * 6.402187824249268
Epoch 440, val loss: 0.8476473093032837
Epoch 450, training loss: 0.7041657567024231 = 0.06307608634233475 + 0.1 * 6.4108967781066895
Epoch 450, val loss: 0.8566417098045349
Epoch 460, training loss: 0.6975072026252747 = 0.05776349455118179 + 0.1 * 6.397436618804932
Epoch 460, val loss: 0.8655228018760681
Epoch 470, training loss: 0.6912071108818054 = 0.05302129313349724 + 0.1 * 6.381857872009277
Epoch 470, val loss: 0.87440025806427
Epoch 480, training loss: 0.6872348785400391 = 0.048776719719171524 + 0.1 * 6.384581089019775
Epoch 480, val loss: 0.8832364082336426
Epoch 490, training loss: 0.6836032867431641 = 0.04499226436018944 + 0.1 * 6.386109828948975
Epoch 490, val loss: 0.8918969631195068
Epoch 500, training loss: 0.6781248450279236 = 0.04161467403173447 + 0.1 * 6.3651018142700195
Epoch 500, val loss: 0.9003266096115112
Epoch 510, training loss: 0.6734058260917664 = 0.0385742262005806 + 0.1 * 6.348315715789795
Epoch 510, val loss: 0.9086465239524841
Epoch 520, training loss: 0.6748635172843933 = 0.03582802042365074 + 0.1 * 6.390355110168457
Epoch 520, val loss: 0.9168587327003479
Epoch 530, training loss: 0.6672539710998535 = 0.033363014459609985 + 0.1 * 6.33890962600708
Epoch 530, val loss: 0.9248701333999634
Epoch 540, training loss: 0.6665164828300476 = 0.031137606129050255 + 0.1 * 6.35378885269165
Epoch 540, val loss: 0.9327435493469238
Epoch 550, training loss: 0.6628594398498535 = 0.029121950268745422 + 0.1 * 6.337374687194824
Epoch 550, val loss: 0.9404503107070923
Epoch 560, training loss: 0.6608276963233948 = 0.02729072794318199 + 0.1 * 6.33536958694458
Epoch 560, val loss: 0.9480458498001099
Epoch 570, training loss: 0.6586465835571289 = 0.025622855871915817 + 0.1 * 6.33023738861084
Epoch 570, val loss: 0.955470085144043
Epoch 580, training loss: 0.6561679244041443 = 0.024104639887809753 + 0.1 * 6.320632457733154
Epoch 580, val loss: 0.9627814292907715
Epoch 590, training loss: 0.6545289158821106 = 0.02271939441561699 + 0.1 * 6.3180952072143555
Epoch 590, val loss: 0.9699127078056335
Epoch 600, training loss: 0.6531428098678589 = 0.021450869739055634 + 0.1 * 6.316919326782227
Epoch 600, val loss: 0.976883053779602
Epoch 610, training loss: 0.651523232460022 = 0.020287947729229927 + 0.1 * 6.312353134155273
Epoch 610, val loss: 0.9837877154350281
Epoch 620, training loss: 0.6496356725692749 = 0.019222935661673546 + 0.1 * 6.304127216339111
Epoch 620, val loss: 0.9904855489730835
Epoch 630, training loss: 0.6492645144462585 = 0.01823926530778408 + 0.1 * 6.3102521896362305
Epoch 630, val loss: 0.9971101880073547
Epoch 640, training loss: 0.6488812565803528 = 0.01733207516372204 + 0.1 * 6.315491676330566
Epoch 640, val loss: 1.0036067962646484
Epoch 650, training loss: 0.6453391909599304 = 0.016495680436491966 + 0.1 * 6.288434982299805
Epoch 650, val loss: 1.0099362134933472
Epoch 660, training loss: 0.6447051763534546 = 0.01572086662054062 + 0.1 * 6.2898430824279785
Epoch 660, val loss: 1.0161526203155518
Epoch 670, training loss: 0.6447818279266357 = 0.015001335181295872 + 0.1 * 6.297804832458496
Epoch 670, val loss: 1.022281289100647
Epoch 680, training loss: 0.642599880695343 = 0.014334388077259064 + 0.1 * 6.282654762268066
Epoch 680, val loss: 1.0282459259033203
Epoch 690, training loss: 0.642792820930481 = 0.013713568449020386 + 0.1 * 6.290792465209961
Epoch 690, val loss: 1.0340973138809204
Epoch 700, training loss: 0.6410570740699768 = 0.01313550304621458 + 0.1 * 6.2792158126831055
Epoch 700, val loss: 1.0398286581039429
Epoch 710, training loss: 0.6387961506843567 = 0.01259505283087492 + 0.1 * 6.26201057434082
Epoch 710, val loss: 1.0454587936401367
Epoch 720, training loss: 0.639893114566803 = 0.012088553048670292 + 0.1 * 6.278045654296875
Epoch 720, val loss: 1.0510125160217285
Epoch 730, training loss: 0.6393123269081116 = 0.011616092175245285 + 0.1 * 6.276961803436279
Epoch 730, val loss: 1.0563918352127075
Epoch 740, training loss: 0.6374527812004089 = 0.011175071820616722 + 0.1 * 6.262776851654053
Epoch 740, val loss: 1.0616389513015747
Epoch 750, training loss: 0.6373514533042908 = 0.010759762488305569 + 0.1 * 6.26591682434082
Epoch 750, val loss: 1.0668116807937622
Epoch 760, training loss: 0.6371023058891296 = 0.010369286872446537 + 0.1 * 6.267330169677734
Epoch 760, val loss: 1.0718861818313599
Epoch 770, training loss: 0.6354816555976868 = 0.010001164861023426 + 0.1 * 6.254804611206055
Epoch 770, val loss: 1.0768965482711792
Epoch 780, training loss: 0.6342536807060242 = 0.009654018096625805 + 0.1 * 6.245996475219727
Epoch 780, val loss: 1.0817952156066895
Epoch 790, training loss: 0.6362494826316833 = 0.009326173923909664 + 0.1 * 6.269233226776123
Epoch 790, val loss: 1.0866289138793945
Epoch 800, training loss: 0.6337370872497559 = 0.009016710333526134 + 0.1 * 6.247203826904297
Epoch 800, val loss: 1.0913532972335815
Epoch 810, training loss: 0.6362323760986328 = 0.008724062703549862 + 0.1 * 6.275083065032959
Epoch 810, val loss: 1.0959969758987427
Epoch 820, training loss: 0.6315509080886841 = 0.008447435684502125 + 0.1 * 6.231034755706787
Epoch 820, val loss: 1.1004937887191772
Epoch 830, training loss: 0.6320769190788269 = 0.008185365237295628 + 0.1 * 6.238914966583252
Epoch 830, val loss: 1.1049553155899048
Epoch 840, training loss: 0.6318603157997131 = 0.00793613400310278 + 0.1 * 6.239241600036621
Epoch 840, val loss: 1.109377384185791
Epoch 850, training loss: 0.6319716572761536 = 0.007699528243392706 + 0.1 * 6.242721080780029
Epoch 850, val loss: 1.1136294603347778
Epoch 860, training loss: 0.6312603950500488 = 0.007475907914340496 + 0.1 * 6.237844944000244
Epoch 860, val loss: 1.1178665161132812
Epoch 870, training loss: 0.6306705474853516 = 0.0072623929008841515 + 0.1 * 6.234081268310547
Epoch 870, val loss: 1.12199866771698
Epoch 880, training loss: 0.6299894452095032 = 0.007058741524815559 + 0.1 * 6.229306697845459
Epoch 880, val loss: 1.126097321510315
Epoch 890, training loss: 0.629950761795044 = 0.006864238064736128 + 0.1 * 6.230865001678467
Epoch 890, val loss: 1.130120038986206
Epoch 900, training loss: 0.6291140913963318 = 0.006678929552435875 + 0.1 * 6.224351406097412
Epoch 900, val loss: 1.1340912580490112
Epoch 910, training loss: 0.6281446814537048 = 0.006502198986709118 + 0.1 * 6.216424465179443
Epoch 910, val loss: 1.1379609107971191
Epoch 920, training loss: 0.6286560893058777 = 0.006333347409963608 + 0.1 * 6.223227500915527
Epoch 920, val loss: 1.1417737007141113
Epoch 930, training loss: 0.6278940439224243 = 0.006171685177832842 + 0.1 * 6.217223167419434
Epoch 930, val loss: 1.1455328464508057
Epoch 940, training loss: 0.6276461482048035 = 0.006017054431140423 + 0.1 * 6.2162909507751465
Epoch 940, val loss: 1.1492562294006348
Epoch 950, training loss: 0.6278157830238342 = 0.005868581589311361 + 0.1 * 6.2194719314575195
Epoch 950, val loss: 1.1528871059417725
Epoch 960, training loss: 0.627435028553009 = 0.005726463161408901 + 0.1 * 6.217085361480713
Epoch 960, val loss: 1.1564743518829346
Epoch 970, training loss: 0.6260082721710205 = 0.0055903224274516106 + 0.1 * 6.204179286956787
Epoch 970, val loss: 1.1599926948547363
Epoch 980, training loss: 0.6279425024986267 = 0.005459662526845932 + 0.1 * 6.224828243255615
Epoch 980, val loss: 1.1634604930877686
Epoch 990, training loss: 0.6258229613304138 = 0.005333827808499336 + 0.1 * 6.204891204833984
Epoch 990, val loss: 1.1668715476989746
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6716
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7781286239624023 = 1.940743088722229 + 0.1 * 8.373855590820312
Epoch 0, val loss: 1.9487478733062744
Epoch 10, training loss: 2.767779588699341 = 1.9304076433181763 + 0.1 * 8.373720169067383
Epoch 10, val loss: 1.937666654586792
Epoch 20, training loss: 2.7549850940704346 = 1.9176950454711914 + 0.1 * 8.372900009155273
Epoch 20, val loss: 1.9239124059677124
Epoch 30, training loss: 2.7365880012512207 = 1.8999032974243164 + 0.1 * 8.36684799194336
Epoch 30, val loss: 1.9048482179641724
Epoch 40, training loss: 2.7066774368286133 = 1.873878002166748 + 0.1 * 8.327993392944336
Epoch 40, val loss: 1.8775360584259033
Epoch 50, training loss: 2.6461374759674072 = 1.8389338254928589 + 0.1 * 8.072035789489746
Epoch 50, val loss: 1.8428723812103271
Epoch 60, training loss: 2.561539649963379 = 1.800763726234436 + 0.1 * 7.607758045196533
Epoch 60, val loss: 1.8071889877319336
Epoch 70, training loss: 2.481172561645508 = 1.7632875442504883 + 0.1 * 7.178849220275879
Epoch 70, val loss: 1.772092342376709
Epoch 80, training loss: 2.414006471633911 = 1.720237135887146 + 0.1 * 6.937694072723389
Epoch 80, val loss: 1.7313722372055054
Epoch 90, training loss: 2.3469114303588867 = 1.6649632453918457 + 0.1 * 6.8194804191589355
Epoch 90, val loss: 1.6807845830917358
Epoch 100, training loss: 2.2690773010253906 = 1.5916551351547241 + 0.1 * 6.7742228507995605
Epoch 100, val loss: 1.6177195310592651
Epoch 110, training loss: 2.1751058101654053 = 1.50021493434906 + 0.1 * 6.748908519744873
Epoch 110, val loss: 1.54179847240448
Epoch 120, training loss: 2.0689167976379395 = 1.3955779075622559 + 0.1 * 6.733389854431152
Epoch 120, val loss: 1.4562780857086182
Epoch 130, training loss: 1.9566411972045898 = 1.284467339515686 + 0.1 * 6.721739292144775
Epoch 130, val loss: 1.3681093454360962
Epoch 140, training loss: 1.8432376384735107 = 1.171846866607666 + 0.1 * 6.7139081954956055
Epoch 140, val loss: 1.281472086906433
Epoch 150, training loss: 1.7311818599700928 = 1.060438871383667 + 0.1 * 6.707429885864258
Epoch 150, val loss: 1.1978275775909424
Epoch 160, training loss: 1.623989224433899 = 0.9538350701332092 + 0.1 * 6.701541423797607
Epoch 160, val loss: 1.1190812587738037
Epoch 170, training loss: 1.5251253843307495 = 0.8558037877082825 + 0.1 * 6.693215847015381
Epoch 170, val loss: 1.0477502346038818
Epoch 180, training loss: 1.4354091882705688 = 0.7670300602912903 + 0.1 * 6.683791160583496
Epoch 180, val loss: 0.9845804572105408
Epoch 190, training loss: 1.354175090789795 = 0.6870701909065247 + 0.1 * 6.671048641204834
Epoch 190, val loss: 0.929643988609314
Epoch 200, training loss: 1.2808763980865479 = 0.615090012550354 + 0.1 * 6.657863140106201
Epoch 200, val loss: 0.8831201791763306
Epoch 210, training loss: 1.21402907371521 = 0.5498359799385071 + 0.1 * 6.641930103302002
Epoch 210, val loss: 0.8437420725822449
Epoch 220, training loss: 1.1547083854675293 = 0.49032172560691833 + 0.1 * 6.643866062164307
Epoch 220, val loss: 0.810028076171875
Epoch 230, training loss: 1.0988070964813232 = 0.4369371831417084 + 0.1 * 6.618698596954346
Epoch 230, val loss: 0.7818204164505005
Epoch 240, training loss: 1.0492345094680786 = 0.388656884431839 + 0.1 * 6.605775833129883
Epoch 240, val loss: 0.7583984732627869
Epoch 250, training loss: 1.0050241947174072 = 0.34505727887153625 + 0.1 * 6.599669456481934
Epoch 250, val loss: 0.7400625348091125
Epoch 260, training loss: 0.9649769067764282 = 0.30639761686325073 + 0.1 * 6.5857930183410645
Epoch 260, val loss: 0.7263698577880859
Epoch 270, training loss: 0.9295470714569092 = 0.27221137285232544 + 0.1 * 6.573356628417969
Epoch 270, val loss: 0.7165024280548096
Epoch 280, training loss: 0.8987658023834229 = 0.24204643070697784 + 0.1 * 6.567193984985352
Epoch 280, val loss: 0.7099972367286682
Epoch 290, training loss: 0.871251106262207 = 0.2157445251941681 + 0.1 * 6.555066108703613
Epoch 290, val loss: 0.7065188884735107
Epoch 300, training loss: 0.8479334115982056 = 0.1927531659603119 + 0.1 * 6.551802158355713
Epoch 300, val loss: 0.7056053280830383
Epoch 310, training loss: 0.826041042804718 = 0.1727539300918579 + 0.1 * 6.532870769500732
Epoch 310, val loss: 0.7067522406578064
Epoch 320, training loss: 0.8066750168800354 = 0.15521609783172607 + 0.1 * 6.514588832855225
Epoch 320, val loss: 0.709608256816864
Epoch 330, training loss: 0.7913981080055237 = 0.1398218870162964 + 0.1 * 6.515761852264404
Epoch 330, val loss: 0.7138176560401917
Epoch 340, training loss: 0.7757925987243652 = 0.12639817595481873 + 0.1 * 6.4939446449279785
Epoch 340, val loss: 0.7190967202186584
Epoch 350, training loss: 0.7624397873878479 = 0.11451320350170135 + 0.1 * 6.4792656898498535
Epoch 350, val loss: 0.7254317998886108
Epoch 360, training loss: 0.7535358667373657 = 0.10396269708871841 + 0.1 * 6.495731353759766
Epoch 360, val loss: 0.7325014472007751
Epoch 370, training loss: 0.7409461736679077 = 0.09463337063789368 + 0.1 * 6.463128089904785
Epoch 370, val loss: 0.7401384711265564
Epoch 380, training loss: 0.7331936955451965 = 0.08631861954927444 + 0.1 * 6.468750953674316
Epoch 380, val loss: 0.7482796907424927
Epoch 390, training loss: 0.7228876948356628 = 0.07894428819417953 + 0.1 * 6.439434051513672
Epoch 390, val loss: 0.7567628026008606
Epoch 400, training loss: 0.7153980135917664 = 0.07234574854373932 + 0.1 * 6.430522918701172
Epoch 400, val loss: 0.7655945420265198
Epoch 410, training loss: 0.7128782272338867 = 0.06642987579107285 + 0.1 * 6.464483737945557
Epoch 410, val loss: 0.774605929851532
Epoch 420, training loss: 0.7035443782806396 = 0.06118541583418846 + 0.1 * 6.423589706420898
Epoch 420, val loss: 0.7836715579032898
Epoch 430, training loss: 0.6976214647293091 = 0.05648120865225792 + 0.1 * 6.411402702331543
Epoch 430, val loss: 0.7927629351615906
Epoch 440, training loss: 0.6922595500946045 = 0.052232056856155396 + 0.1 * 6.400274276733398
Epoch 440, val loss: 0.8019992113113403
Epoch 450, training loss: 0.6941356658935547 = 0.04839110001921654 + 0.1 * 6.4574456214904785
Epoch 450, val loss: 0.8111243844032288
Epoch 460, training loss: 0.6837093234062195 = 0.04496215283870697 + 0.1 * 6.387471675872803
Epoch 460, val loss: 0.8202073574066162
Epoch 470, training loss: 0.6804548501968384 = 0.041865378618240356 + 0.1 * 6.385894775390625
Epoch 470, val loss: 0.8291386961936951
Epoch 480, training loss: 0.676793098449707 = 0.039051640778779984 + 0.1 * 6.377414226531982
Epoch 480, val loss: 0.8380120992660522
Epoch 490, training loss: 0.6751689314842224 = 0.03649979829788208 + 0.1 * 6.386691093444824
Epoch 490, val loss: 0.8466877937316895
Epoch 500, training loss: 0.6705877780914307 = 0.03418786823749542 + 0.1 * 6.363998889923096
Epoch 500, val loss: 0.8552840948104858
Epoch 510, training loss: 0.6701952815055847 = 0.03207946941256523 + 0.1 * 6.381157875061035
Epoch 510, val loss: 0.8636917471885681
Epoch 520, training loss: 0.6653403043746948 = 0.030162544921040535 + 0.1 * 6.351777076721191
Epoch 520, val loss: 0.8718628883361816
Epoch 530, training loss: 0.6639587879180908 = 0.02840791642665863 + 0.1 * 6.355508327484131
Epoch 530, val loss: 0.8798949122428894
Epoch 540, training loss: 0.6612556576728821 = 0.026804378256201744 + 0.1 * 6.344512462615967
Epoch 540, val loss: 0.8877852559089661
Epoch 550, training loss: 0.6597765684127808 = 0.02533184550702572 + 0.1 * 6.344447135925293
Epoch 550, val loss: 0.8955370187759399
Epoch 560, training loss: 0.6576866507530212 = 0.02397606149315834 + 0.1 * 6.3371052742004395
Epoch 560, val loss: 0.903092086315155
Epoch 570, training loss: 0.6550649404525757 = 0.022726792842149734 + 0.1 * 6.323381423950195
Epoch 570, val loss: 0.9104794263839722
Epoch 580, training loss: 0.6534665822982788 = 0.021573323756456375 + 0.1 * 6.31893253326416
Epoch 580, val loss: 0.9176796674728394
Epoch 590, training loss: 0.6541568040847778 = 0.02050815336406231 + 0.1 * 6.336486339569092
Epoch 590, val loss: 0.9247699975967407
Epoch 600, training loss: 0.6520389318466187 = 0.019522149115800858 + 0.1 * 6.325168132781982
Epoch 600, val loss: 0.9316255450248718
Epoch 610, training loss: 0.6497460603713989 = 0.018608340993523598 + 0.1 * 6.311377048492432
Epoch 610, val loss: 0.9384545683860779
Epoch 620, training loss: 0.6512720584869385 = 0.017757458612322807 + 0.1 * 6.335145473480225
Epoch 620, val loss: 0.9450564980506897
Epoch 630, training loss: 0.6466556191444397 = 0.01696746051311493 + 0.1 * 6.296881198883057
Epoch 630, val loss: 0.9514462947845459
Epoch 640, training loss: 0.6455515623092651 = 0.016229990869760513 + 0.1 * 6.293215751647949
Epoch 640, val loss: 0.9578090906143188
Epoch 650, training loss: 0.6454513669013977 = 0.015538012608885765 + 0.1 * 6.29913330078125
Epoch 650, val loss: 0.9640422463417053
Epoch 660, training loss: 0.6456007957458496 = 0.014891449362039566 + 0.1 * 6.307093620300293
Epoch 660, val loss: 0.9700174927711487
Epoch 670, training loss: 0.6451108455657959 = 0.014287188649177551 + 0.1 * 6.308236598968506
Epoch 670, val loss: 0.9760312438011169
Epoch 680, training loss: 0.6421310901641846 = 0.013722109608352184 + 0.1 * 6.2840895652771
Epoch 680, val loss: 0.9818134307861328
Epoch 690, training loss: 0.6410962343215942 = 0.013191725127398968 + 0.1 * 6.279045104980469
Epoch 690, val loss: 0.9875583052635193
Epoch 700, training loss: 0.640795111656189 = 0.012691012583673 + 0.1 * 6.281040668487549
Epoch 700, val loss: 0.993096113204956
Epoch 710, training loss: 0.6399575471878052 = 0.012219014577567577 + 0.1 * 6.277385234832764
Epoch 710, val loss: 0.9984856247901917
Epoch 720, training loss: 0.6411074995994568 = 0.011775018647313118 + 0.1 * 6.2933244705200195
Epoch 720, val loss: 1.0038350820541382
Epoch 730, training loss: 0.6381511688232422 = 0.011357183568179607 + 0.1 * 6.267939567565918
Epoch 730, val loss: 1.0091283321380615
Epoch 740, training loss: 0.6376398205757141 = 0.010961795225739479 + 0.1 * 6.266780376434326
Epoch 740, val loss: 1.014341950416565
Epoch 750, training loss: 0.6404685378074646 = 0.010586748830974102 + 0.1 * 6.298818111419678
Epoch 750, val loss: 1.0193148851394653
Epoch 760, training loss: 0.6359383463859558 = 0.010233180597424507 + 0.1 * 6.257051467895508
Epoch 760, val loss: 1.0242390632629395
Epoch 770, training loss: 0.6355484127998352 = 0.009898362681269646 + 0.1 * 6.256500244140625
Epoch 770, val loss: 1.0291588306427002
Epoch 780, training loss: 0.6352469325065613 = 0.009579966776072979 + 0.1 * 6.256669521331787
Epoch 780, val loss: 1.0338886976242065
Epoch 790, training loss: 0.6346300840377808 = 0.00927801989018917 + 0.1 * 6.253520965576172
Epoch 790, val loss: 1.0385690927505493
Epoch 800, training loss: 0.6358165740966797 = 0.008991281501948833 + 0.1 * 6.268252372741699
Epoch 800, val loss: 1.0432403087615967
Epoch 810, training loss: 0.6335675716400146 = 0.008718310855329037 + 0.1 * 6.248492240905762
Epoch 810, val loss: 1.0475990772247314
Epoch 820, training loss: 0.6326916217803955 = 0.008459040895104408 + 0.1 * 6.242325782775879
Epoch 820, val loss: 1.0521562099456787
Epoch 830, training loss: 0.6328113079071045 = 0.008211194537580013 + 0.1 * 6.246001243591309
Epoch 830, val loss: 1.0565170049667358
Epoch 840, training loss: 0.6327135562896729 = 0.007974871434271336 + 0.1 * 6.247386932373047
Epoch 840, val loss: 1.0607163906097412
Epoch 850, training loss: 0.6325639486312866 = 0.007750627119094133 + 0.1 * 6.248133182525635
Epoch 850, val loss: 1.064959168434143
Epoch 860, training loss: 0.6309300661087036 = 0.007536616642028093 + 0.1 * 6.2339348793029785
Epoch 860, val loss: 1.0690635442733765
Epoch 870, training loss: 0.6317149996757507 = 0.007331721484661102 + 0.1 * 6.243832588195801
Epoch 870, val loss: 1.0731955766677856
Epoch 880, training loss: 0.6305336952209473 = 0.007135502528399229 + 0.1 * 6.233981609344482
Epoch 880, val loss: 1.0771666765213013
Epoch 890, training loss: 0.6300638914108276 = 0.0069478098303079605 + 0.1 * 6.231160640716553
Epoch 890, val loss: 1.08110511302948
Epoch 900, training loss: 0.6299737095832825 = 0.0067685553804039955 + 0.1 * 6.232051372528076
Epoch 900, val loss: 1.0850017070770264
Epoch 910, training loss: 0.6300128102302551 = 0.006597481667995453 + 0.1 * 6.2341532707214355
Epoch 910, val loss: 1.0887680053710938
Epoch 920, training loss: 0.6286993026733398 = 0.006433302070945501 + 0.1 * 6.222660064697266
Epoch 920, val loss: 1.0925860404968262
Epoch 930, training loss: 0.6278395652770996 = 0.0062753488309681416 + 0.1 * 6.215641975402832
Epoch 930, val loss: 1.0963430404663086
Epoch 940, training loss: 0.628786563873291 = 0.006123322062194347 + 0.1 * 6.226632595062256
Epoch 940, val loss: 1.0999338626861572
Epoch 950, training loss: 0.6281656622886658 = 0.005977381486445665 + 0.1 * 6.2218828201293945
Epoch 950, val loss: 1.1035267114639282
Epoch 960, training loss: 0.6287986636161804 = 0.00583744328469038 + 0.1 * 6.229612350463867
Epoch 960, val loss: 1.107027292251587
Epoch 970, training loss: 0.6269848942756653 = 0.00570320151746273 + 0.1 * 6.2128167152404785
Epoch 970, val loss: 1.110554814338684
Epoch 980, training loss: 0.6279404759407043 = 0.005573853384703398 + 0.1 * 6.223665714263916
Epoch 980, val loss: 1.114007830619812
Epoch 990, training loss: 0.6261804103851318 = 0.0054491483606398106 + 0.1 * 6.207312107086182
Epoch 990, val loss: 1.1173887252807617
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8044
Flip ASR: 0.7689/225 nodes
The final ASR:0.66667, 0.11454, Accuracy:0.81728, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11646])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10554])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00603, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8049678802490234 = 1.967576026916504 + 0.1 * 8.373918533325195
Epoch 0, val loss: 1.9645012617111206
Epoch 10, training loss: 2.794297695159912 = 1.9569116830825806 + 0.1 * 8.373860359191895
Epoch 10, val loss: 1.9540414810180664
Epoch 20, training loss: 2.7816648483276367 = 1.9443140029907227 + 0.1 * 8.373509407043457
Epoch 20, val loss: 1.9416385889053345
Epoch 30, training loss: 2.7643625736236572 = 1.9272782802581787 + 0.1 * 8.370841979980469
Epoch 30, val loss: 1.9250013828277588
Epoch 40, training loss: 2.7377023696899414 = 1.9025261402130127 + 0.1 * 8.351762771606445
Epoch 40, val loss: 1.901305079460144
Epoch 50, training loss: 2.6916956901550293 = 1.8669098615646362 + 0.1 * 8.247858047485352
Epoch 50, val loss: 1.8685165643692017
Epoch 60, training loss: 2.6194872856140137 = 1.8229647874832153 + 0.1 * 7.965225696563721
Epoch 60, val loss: 1.8299927711486816
Epoch 70, training loss: 2.5577211380004883 = 1.7770251035690308 + 0.1 * 7.8069610595703125
Epoch 70, val loss: 1.7917182445526123
Epoch 80, training loss: 2.4793567657470703 = 1.7340844869613647 + 0.1 * 7.45272159576416
Epoch 80, val loss: 1.7539595365524292
Epoch 90, training loss: 2.394273042678833 = 1.6847213506698608 + 0.1 * 7.095517635345459
Epoch 90, val loss: 1.7081419229507446
Epoch 100, training loss: 2.313736915588379 = 1.6210689544677734 + 0.1 * 6.926680564880371
Epoch 100, val loss: 1.652275562286377
Epoch 110, training loss: 2.2229089736938477 = 1.539976716041565 + 0.1 * 6.829322338104248
Epoch 110, val loss: 1.5825451612472534
Epoch 120, training loss: 2.1243629455566406 = 1.4465322494506836 + 0.1 * 6.7783074378967285
Epoch 120, val loss: 1.503657579421997
Epoch 130, training loss: 2.0233547687530518 = 1.3482706546783447 + 0.1 * 6.75084114074707
Epoch 130, val loss: 1.4218870401382446
Epoch 140, training loss: 1.9224739074707031 = 1.2490248680114746 + 0.1 * 6.734489440917969
Epoch 140, val loss: 1.3420156240463257
Epoch 150, training loss: 1.8225629329681396 = 1.1505836248397827 + 0.1 * 6.719793319702148
Epoch 150, val loss: 1.2646163702011108
Epoch 160, training loss: 1.723806381225586 = 1.052951455116272 + 0.1 * 6.708549499511719
Epoch 160, val loss: 1.1894659996032715
Epoch 170, training loss: 1.6277284622192383 = 0.9578747749328613 + 0.1 * 6.698535919189453
Epoch 170, val loss: 1.1177618503570557
Epoch 180, training loss: 1.5367460250854492 = 0.8678452968597412 + 0.1 * 6.689006805419922
Epoch 180, val loss: 1.0506842136383057
Epoch 190, training loss: 1.4527848958969116 = 0.784895122051239 + 0.1 * 6.678897380828857
Epoch 190, val loss: 0.9895144701004028
Epoch 200, training loss: 1.3763844966888428 = 0.7093096971511841 + 0.1 * 6.670748233795166
Epoch 200, val loss: 0.9340118765830994
Epoch 210, training loss: 1.3067185878753662 = 0.6404938101768494 + 0.1 * 6.662248134613037
Epoch 210, val loss: 0.8839951753616333
Epoch 220, training loss: 1.2426868677139282 = 0.5775442719459534 + 0.1 * 6.651425838470459
Epoch 220, val loss: 0.8390213847160339
Epoch 230, training loss: 1.1840757131576538 = 0.5198153257369995 + 0.1 * 6.642603874206543
Epoch 230, val loss: 0.7991694808006287
Epoch 240, training loss: 1.1303355693817139 = 0.4670954644680023 + 0.1 * 6.6324005126953125
Epoch 240, val loss: 0.7649558782577515
Epoch 250, training loss: 1.081476092338562 = 0.41891178488731384 + 0.1 * 6.625643253326416
Epoch 250, val loss: 0.7365790605545044
Epoch 260, training loss: 1.035398006439209 = 0.37485265731811523 + 0.1 * 6.605452537536621
Epoch 260, val loss: 0.7135795950889587
Epoch 270, training loss: 0.9944448471069336 = 0.33416303992271423 + 0.1 * 6.602817535400391
Epoch 270, val loss: 0.6948869824409485
Epoch 280, training loss: 0.9552315473556519 = 0.29677799344062805 + 0.1 * 6.584535121917725
Epoch 280, val loss: 0.6803016662597656
Epoch 290, training loss: 0.9190472364425659 = 0.2623409330844879 + 0.1 * 6.5670623779296875
Epoch 290, val loss: 0.668992817401886
Epoch 300, training loss: 0.8868287205696106 = 0.23142440617084503 + 0.1 * 6.554043292999268
Epoch 300, val loss: 0.6611524224281311
Epoch 310, training loss: 0.8584513068199158 = 0.2042342871427536 + 0.1 * 6.54217004776001
Epoch 310, val loss: 0.6565877795219421
Epoch 320, training loss: 0.8330629467964172 = 0.18038706481456757 + 0.1 * 6.526758670806885
Epoch 320, val loss: 0.6547073721885681
Epoch 330, training loss: 0.8122351169586182 = 0.15966784954071045 + 0.1 * 6.525672435760498
Epoch 330, val loss: 0.6556589007377625
Epoch 340, training loss: 0.7938004732131958 = 0.1419615000486374 + 0.1 * 6.5183892250061035
Epoch 340, val loss: 0.659038245677948
Epoch 350, training loss: 0.7764344811439514 = 0.12675346434116364 + 0.1 * 6.496809959411621
Epoch 350, val loss: 0.6643886566162109
Epoch 360, training loss: 0.7631314396858215 = 0.11360108852386475 + 0.1 * 6.495303153991699
Epoch 360, val loss: 0.6712920069694519
Epoch 370, training loss: 0.7501436471939087 = 0.10226862877607346 + 0.1 * 6.478749752044678
Epoch 370, val loss: 0.6795477867126465
Epoch 380, training loss: 0.7386853098869324 = 0.09236273914575577 + 0.1 * 6.463225364685059
Epoch 380, val loss: 0.6887246966362
Epoch 390, training loss: 0.7302998304367065 = 0.08364233374595642 + 0.1 * 6.4665751457214355
Epoch 390, val loss: 0.6988083720207214
Epoch 400, training loss: 0.7218177914619446 = 0.0759868398308754 + 0.1 * 6.458309173583984
Epoch 400, val loss: 0.7091169357299805
Epoch 410, training loss: 0.7135066986083984 = 0.06923259049654007 + 0.1 * 6.4427409172058105
Epoch 410, val loss: 0.7198842763900757
Epoch 420, training loss: 0.7069391012191772 = 0.06325429677963257 + 0.1 * 6.436847686767578
Epoch 420, val loss: 0.7306515574455261
Epoch 430, training loss: 0.7003603577613831 = 0.0579611174762249 + 0.1 * 6.423992156982422
Epoch 430, val loss: 0.7415876984596252
Epoch 440, training loss: 0.696094810962677 = 0.05324576422572136 + 0.1 * 6.428490161895752
Epoch 440, val loss: 0.7524059414863586
Epoch 450, training loss: 0.6910648941993713 = 0.04904685169458389 + 0.1 * 6.420180320739746
Epoch 450, val loss: 0.7632085680961609
Epoch 460, training loss: 0.6853402853012085 = 0.04531193524599075 + 0.1 * 6.400283336639404
Epoch 460, val loss: 0.7737521529197693
Epoch 470, training loss: 0.6825425028800964 = 0.04197590425610542 + 0.1 * 6.405665874481201
Epoch 470, val loss: 0.7843191623687744
Epoch 480, training loss: 0.6790414452552795 = 0.03899091109633446 + 0.1 * 6.400505065917969
Epoch 480, val loss: 0.7945755124092102
Epoch 490, training loss: 0.6753900647163391 = 0.03631201758980751 + 0.1 * 6.390779972076416
Epoch 490, val loss: 0.8047235012054443
Epoch 500, training loss: 0.672171950340271 = 0.03389924764633179 + 0.1 * 6.382726669311523
Epoch 500, val loss: 0.8144421577453613
Epoch 510, training loss: 0.6691158413887024 = 0.03172402083873749 + 0.1 * 6.373918056488037
Epoch 510, val loss: 0.8241317868232727
Epoch 520, training loss: 0.6663140654563904 = 0.02975212037563324 + 0.1 * 6.36561918258667
Epoch 520, val loss: 0.8335521817207336
Epoch 530, training loss: 0.663724958896637 = 0.027958521619439125 + 0.1 * 6.357664108276367
Epoch 530, val loss: 0.842711865901947
Epoch 540, training loss: 0.6670953035354614 = 0.026325590908527374 + 0.1 * 6.4076972007751465
Epoch 540, val loss: 0.8516198396682739
Epoch 550, training loss: 0.660171627998352 = 0.02484414167702198 + 0.1 * 6.353274345397949
Epoch 550, val loss: 0.8602380752563477
Epoch 560, training loss: 0.657755970954895 = 0.023489169776439667 + 0.1 * 6.342667579650879
Epoch 560, val loss: 0.8688420653343201
Epoch 570, training loss: 0.6573286652565002 = 0.022240247577428818 + 0.1 * 6.350884437561035
Epoch 570, val loss: 0.8771042823791504
Epoch 580, training loss: 0.655830442905426 = 0.021089639514684677 + 0.1 * 6.347408294677734
Epoch 580, val loss: 0.8849741816520691
Epoch 590, training loss: 0.6531573534011841 = 0.020031046122312546 + 0.1 * 6.331263065338135
Epoch 590, val loss: 0.8930515050888062
Epoch 600, training loss: 0.6545543074607849 = 0.01904938742518425 + 0.1 * 6.355048656463623
Epoch 600, val loss: 0.9008380174636841
Epoch 610, training loss: 0.6514663100242615 = 0.018142588436603546 + 0.1 * 6.333237171173096
Epoch 610, val loss: 0.9080828428268433
Epoch 620, training loss: 0.6497687101364136 = 0.017305707558989525 + 0.1 * 6.324629783630371
Epoch 620, val loss: 0.9156403541564941
Epoch 630, training loss: 0.6489360332489014 = 0.016523679718375206 + 0.1 * 6.324123859405518
Epoch 630, val loss: 0.9228217005729675
Epoch 640, training loss: 0.6484879851341248 = 0.015795711427927017 + 0.1 * 6.326922416687012
Epoch 640, val loss: 0.9296361207962036
Epoch 650, training loss: 0.6464638710021973 = 0.015117914415895939 + 0.1 * 6.3134589195251465
Epoch 650, val loss: 0.9366122484207153
Epoch 660, training loss: 0.646958589553833 = 0.014482631348073483 + 0.1 * 6.324759483337402
Epoch 660, val loss: 0.9433533549308777
Epoch 670, training loss: 0.6450240015983582 = 0.013890096917748451 + 0.1 * 6.311338901519775
Epoch 670, val loss: 0.9497779011726379
Epoch 680, training loss: 0.6437665820121765 = 0.013335350900888443 + 0.1 * 6.304312229156494
Epoch 680, val loss: 0.9562515616416931
Epoch 690, training loss: 0.6430665254592896 = 0.012813197448849678 + 0.1 * 6.302533149719238
Epoch 690, val loss: 0.9625835418701172
Epoch 700, training loss: 0.6430732607841492 = 0.012321065180003643 + 0.1 * 6.307521820068359
Epoch 700, val loss: 0.9686601161956787
Epoch 710, training loss: 0.6419554352760315 = 0.011860484257340431 + 0.1 * 6.300949573516846
Epoch 710, val loss: 0.9744694232940674
Epoch 720, training loss: 0.6410203576087952 = 0.011429143138229847 + 0.1 * 6.295912265777588
Epoch 720, val loss: 0.9805004000663757
Epoch 730, training loss: 0.6402451992034912 = 0.01102145854383707 + 0.1 * 6.292237758636475
Epoch 730, val loss: 0.986355721950531
Epoch 740, training loss: 0.6390720009803772 = 0.01063471008092165 + 0.1 * 6.284372806549072
Epoch 740, val loss: 0.9919772744178772
Epoch 750, training loss: 0.6391891241073608 = 0.010269111022353172 + 0.1 * 6.2891998291015625
Epoch 750, val loss: 0.9973352551460266
Epoch 760, training loss: 0.6384336948394775 = 0.009924493730068207 + 0.1 * 6.285091876983643
Epoch 760, val loss: 1.0027875900268555
Epoch 770, training loss: 0.6380314230918884 = 0.009598267264664173 + 0.1 * 6.284331321716309
Epoch 770, val loss: 1.0082619190216064
Epoch 780, training loss: 0.6403968334197998 = 0.009288343600928783 + 0.1 * 6.311084747314453
Epoch 780, val loss: 1.0133105516433716
Epoch 790, training loss: 0.6370300054550171 = 0.008996590971946716 + 0.1 * 6.280333995819092
Epoch 790, val loss: 1.0184192657470703
Epoch 800, training loss: 0.6359401345252991 = 0.008718849159777164 + 0.1 * 6.272212505340576
Epoch 800, val loss: 1.0235761404037476
Epoch 810, training loss: 0.6366663575172424 = 0.008453874848783016 + 0.1 * 6.282124996185303
Epoch 810, val loss: 1.0284830331802368
Epoch 820, training loss: 0.6353963613510132 = 0.008202177472412586 + 0.1 * 6.271941661834717
Epoch 820, val loss: 1.03324556350708
Epoch 830, training loss: 0.6364319920539856 = 0.007963037118315697 + 0.1 * 6.284689426422119
Epoch 830, val loss: 1.0380370616912842
Epoch 840, training loss: 0.6345672011375427 = 0.0077348132617771626 + 0.1 * 6.2683234214782715
Epoch 840, val loss: 1.042691946029663
Epoch 850, training loss: 0.6339837312698364 = 0.007517762947827578 + 0.1 * 6.264659404754639
Epoch 850, val loss: 1.0472315549850464
Epoch 860, training loss: 0.6336256265640259 = 0.007310975342988968 + 0.1 * 6.26314640045166
Epoch 860, val loss: 1.0518702268600464
Epoch 870, training loss: 0.6328438520431519 = 0.007112616673111916 + 0.1 * 6.257312297821045
Epoch 870, val loss: 1.0561602115631104
Epoch 880, training loss: 0.6322749257087708 = 0.006923517677932978 + 0.1 * 6.253514289855957
Epoch 880, val loss: 1.060516357421875
Epoch 890, training loss: 0.6333819031715393 = 0.006742557976394892 + 0.1 * 6.266393661499023
Epoch 890, val loss: 1.0647002458572388
Epoch 900, training loss: 0.6312849521636963 = 0.006570249330252409 + 0.1 * 6.247147083282471
Epoch 900, val loss: 1.0689202547073364
Epoch 910, training loss: 0.6321877241134644 = 0.006404884159564972 + 0.1 * 6.257828235626221
Epoch 910, val loss: 1.0731909275054932
Epoch 920, training loss: 0.6305033564567566 = 0.006245878525078297 + 0.1 * 6.242574691772461
Epoch 920, val loss: 1.0770517587661743
Epoch 930, training loss: 0.6304651498794556 = 0.006093898788094521 + 0.1 * 6.243712425231934
Epoch 930, val loss: 1.0810900926589966
Epoch 940, training loss: 0.6319767236709595 = 0.005947483703494072 + 0.1 * 6.2602925300598145
Epoch 940, val loss: 1.084965467453003
Epoch 950, training loss: 0.6295911073684692 = 0.005807411391288042 + 0.1 * 6.237836837768555
Epoch 950, val loss: 1.0888092517852783
Epoch 960, training loss: 0.6301134824752808 = 0.0056726327165961266 + 0.1 * 6.244408130645752
Epoch 960, val loss: 1.092692494392395
Epoch 970, training loss: 0.6290828585624695 = 0.005542761646211147 + 0.1 * 6.235401153564453
Epoch 970, val loss: 1.0963342189788818
Epoch 980, training loss: 0.6300066113471985 = 0.0054182386957108974 + 0.1 * 6.245883464813232
Epoch 980, val loss: 1.1000126600265503
Epoch 990, training loss: 0.629851758480072 = 0.005298546049743891 + 0.1 * 6.245532035827637
Epoch 990, val loss: 1.103643774986267
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7417
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.766509771347046 = 1.9291175603866577 + 0.1 * 8.373922348022461
Epoch 0, val loss: 1.9182196855545044
Epoch 10, training loss: 2.756556749343872 = 1.919175624847412 + 0.1 * 8.373810768127441
Epoch 10, val loss: 1.908717155456543
Epoch 20, training loss: 2.7440590858459473 = 1.9067333936691284 + 0.1 * 8.37325668334961
Epoch 20, val loss: 1.8965686559677124
Epoch 30, training loss: 2.7262396812438965 = 1.8893016576766968 + 0.1 * 8.369379997253418
Epoch 30, val loss: 1.8792688846588135
Epoch 40, training loss: 2.6981887817382812 = 1.8638627529144287 + 0.1 * 8.343259811401367
Epoch 40, val loss: 1.854231834411621
Epoch 50, training loss: 2.647794485092163 = 1.8290607929229736 + 0.1 * 8.187335968017578
Epoch 50, val loss: 1.8213450908660889
Epoch 60, training loss: 2.561169147491455 = 1.7894067764282227 + 0.1 * 7.71762228012085
Epoch 60, val loss: 1.7853187322616577
Epoch 70, training loss: 2.4875168800354004 = 1.7486265897750854 + 0.1 * 7.388904094696045
Epoch 70, val loss: 1.7479673624038696
Epoch 80, training loss: 2.4204840660095215 = 1.7001789808273315 + 0.1 * 7.203049659729004
Epoch 80, val loss: 1.7038310766220093
Epoch 90, training loss: 2.34517240524292 = 1.6374515295028687 + 0.1 * 7.077207565307617
Epoch 90, val loss: 1.6477911472320557
Epoch 100, training loss: 2.257045269012451 = 1.5562283992767334 + 0.1 * 7.008167266845703
Epoch 100, val loss: 1.5783997774124146
Epoch 110, training loss: 2.1574811935424805 = 1.4615652561187744 + 0.1 * 6.959157943725586
Epoch 110, val loss: 1.5019481182098389
Epoch 120, training loss: 2.0557122230529785 = 1.3641328811645508 + 0.1 * 6.915792942047119
Epoch 120, val loss: 1.4230307340621948
Epoch 130, training loss: 1.9584420919418335 = 1.2716789245605469 + 0.1 * 6.867631435394287
Epoch 130, val loss: 1.3506823778152466
Epoch 140, training loss: 1.8677818775177002 = 1.1860798597335815 + 0.1 * 6.817020893096924
Epoch 140, val loss: 1.2870233058929443
Epoch 150, training loss: 1.7829744815826416 = 1.1044447422027588 + 0.1 * 6.785297393798828
Epoch 150, val loss: 1.2272392511367798
Epoch 160, training loss: 1.6999034881591797 = 1.0240390300750732 + 0.1 * 6.7586445808410645
Epoch 160, val loss: 1.1693429946899414
Epoch 170, training loss: 1.616380214691162 = 0.9422937035560608 + 0.1 * 6.740864276885986
Epoch 170, val loss: 1.110371708869934
Epoch 180, training loss: 1.5338022708892822 = 0.861311137676239 + 0.1 * 6.724911212921143
Epoch 180, val loss: 1.0512439012527466
Epoch 190, training loss: 1.456465721130371 = 0.7856895923614502 + 0.1 * 6.707761287689209
Epoch 190, val loss: 0.9962734580039978
Epoch 200, training loss: 1.3881059885025024 = 0.7189829349517822 + 0.1 * 6.691230297088623
Epoch 200, val loss: 0.949040949344635
Epoch 210, training loss: 1.3288037776947021 = 0.6609835624694824 + 0.1 * 6.678201198577881
Epoch 210, val loss: 0.909702479839325
Epoch 220, training loss: 1.2764003276824951 = 0.6101475358009338 + 0.1 * 6.662527561187744
Epoch 220, val loss: 0.8776492476463318
Epoch 230, training loss: 1.228280782699585 = 0.5634719133377075 + 0.1 * 6.648088455200195
Epoch 230, val loss: 0.8501965999603271
Epoch 240, training loss: 1.1820085048675537 = 0.5184859037399292 + 0.1 * 6.635225296020508
Epoch 240, val loss: 0.8252871036529541
Epoch 250, training loss: 1.136815071105957 = 0.47408565878868103 + 0.1 * 6.627294063568115
Epoch 250, val loss: 0.8025482892990112
Epoch 260, training loss: 1.0912264585494995 = 0.4298754632472992 + 0.1 * 6.613509654998779
Epoch 260, val loss: 0.7820398807525635
Epoch 270, training loss: 1.0481433868408203 = 0.38627511262893677 + 0.1 * 6.618681907653809
Epoch 270, val loss: 0.7640745639801025
Epoch 280, training loss: 1.0045090913772583 = 0.34466126561164856 + 0.1 * 6.598477840423584
Epoch 280, val loss: 0.7493571639060974
Epoch 290, training loss: 0.9646992683410645 = 0.3056063950061798 + 0.1 * 6.590928077697754
Epoch 290, val loss: 0.7379429340362549
Epoch 300, training loss: 0.9280799627304077 = 0.2699073255062103 + 0.1 * 6.581726551055908
Epoch 300, val loss: 0.7301688194274902
Epoch 310, training loss: 0.8952717185020447 = 0.23835933208465576 + 0.1 * 6.5691237449646
Epoch 310, val loss: 0.7259025573730469
Epoch 320, training loss: 0.8676156997680664 = 0.21099933981895447 + 0.1 * 6.566163063049316
Epoch 320, val loss: 0.7252172827720642
Epoch 330, training loss: 0.8436095714569092 = 0.18743011355400085 + 0.1 * 6.561794281005859
Epoch 330, val loss: 0.7274407744407654
Epoch 340, training loss: 0.8213790059089661 = 0.16710937023162842 + 0.1 * 6.542696475982666
Epoch 340, val loss: 0.7320955991744995
Epoch 350, training loss: 0.8026728630065918 = 0.14943212270736694 + 0.1 * 6.532407283782959
Epoch 350, val loss: 0.738893985748291
Epoch 360, training loss: 0.7886173725128174 = 0.1341153234243393 + 0.1 * 6.54502010345459
Epoch 360, val loss: 0.7472050189971924
Epoch 370, training loss: 0.7729408740997314 = 0.12092987447977066 + 0.1 * 6.5201096534729
Epoch 370, val loss: 0.7568509578704834
Epoch 380, training loss: 0.7618355751037598 = 0.10940535366535187 + 0.1 * 6.5243024826049805
Epoch 380, val loss: 0.7673748731613159
Epoch 390, training loss: 0.7494226098060608 = 0.09928295761346817 + 0.1 * 6.501396179199219
Epoch 390, val loss: 0.7786561250686646
Epoch 400, training loss: 0.7403493523597717 = 0.09029041230678558 + 0.1 * 6.500589370727539
Epoch 400, val loss: 0.7902109622955322
Epoch 410, training loss: 0.7311149835586548 = 0.08227667212486267 + 0.1 * 6.488382816314697
Epoch 410, val loss: 0.8020233511924744
Epoch 420, training loss: 0.7237653136253357 = 0.07510624080896378 + 0.1 * 6.486590385437012
Epoch 420, val loss: 0.8139822483062744
Epoch 430, training loss: 0.7164267301559448 = 0.06873314082622528 + 0.1 * 6.476935386657715
Epoch 430, val loss: 0.8258340358734131
Epoch 440, training loss: 0.7089792490005493 = 0.06303735077381134 + 0.1 * 6.459418773651123
Epoch 440, val loss: 0.8377410769462585
Epoch 450, training loss: 0.7036972641944885 = 0.05791330337524414 + 0.1 * 6.457839488983154
Epoch 450, val loss: 0.849696695804596
Epoch 460, training loss: 0.6980985403060913 = 0.053318239748477936 + 0.1 * 6.447802543640137
Epoch 460, val loss: 0.8615724444389343
Epoch 470, training loss: 0.6929170489311218 = 0.04919234290719032 + 0.1 * 6.437246799468994
Epoch 470, val loss: 0.8734834790229797
Epoch 480, training loss: 0.689866304397583 = 0.04547843709588051 + 0.1 * 6.443878173828125
Epoch 480, val loss: 0.8853425979614258
Epoch 490, training loss: 0.6862940788269043 = 0.04213905334472656 + 0.1 * 6.441550254821777
Epoch 490, val loss: 0.8971332907676697
Epoch 500, training loss: 0.6813621520996094 = 0.0391375906765461 + 0.1 * 6.422245502471924
Epoch 500, val loss: 0.9087708592414856
Epoch 510, training loss: 0.6779875159263611 = 0.03642513230443001 + 0.1 * 6.415623664855957
Epoch 510, val loss: 0.920339822769165
Epoch 520, training loss: 0.6760909557342529 = 0.033973101526498795 + 0.1 * 6.421178340911865
Epoch 520, val loss: 0.9315111637115479
Epoch 530, training loss: 0.672730565071106 = 0.03175513818860054 + 0.1 * 6.409754276275635
Epoch 530, val loss: 0.9426972270011902
Epoch 540, training loss: 0.6703884601593018 = 0.029737563803792 + 0.1 * 6.406508445739746
Epoch 540, val loss: 0.9535804986953735
Epoch 550, training loss: 0.666624128818512 = 0.02790338546037674 + 0.1 * 6.387207508087158
Epoch 550, val loss: 0.9642297625541687
Epoch 560, training loss: 0.6654710173606873 = 0.026231126859784126 + 0.1 * 6.392398834228516
Epoch 560, val loss: 0.974718451499939
Epoch 570, training loss: 0.6637982726097107 = 0.024707013741135597 + 0.1 * 6.3909125328063965
Epoch 570, val loss: 0.9848557114601135
Epoch 580, training loss: 0.6610284447669983 = 0.02331460267305374 + 0.1 * 6.377138614654541
Epoch 580, val loss: 0.9948607087135315
Epoch 590, training loss: 0.6591172814369202 = 0.0220347810536623 + 0.1 * 6.370824813842773
Epoch 590, val loss: 1.0045878887176514
Epoch 600, training loss: 0.6586582064628601 = 0.020857568830251694 + 0.1 * 6.378006458282471
Epoch 600, val loss: 1.0139601230621338
Epoch 610, training loss: 0.6564348936080933 = 0.01977868750691414 + 0.1 * 6.3665618896484375
Epoch 610, val loss: 1.0232025384902954
Epoch 620, training loss: 0.6553035974502563 = 0.0187830850481987 + 0.1 * 6.365204811096191
Epoch 620, val loss: 1.0322344303131104
Epoch 630, training loss: 0.652664303779602 = 0.01786249503493309 + 0.1 * 6.348018169403076
Epoch 630, val loss: 1.0409226417541504
Epoch 640, training loss: 0.653438150882721 = 0.017008304595947266 + 0.1 * 6.364298343658447
Epoch 640, val loss: 1.0494650602340698
Epoch 650, training loss: 0.6507735848426819 = 0.016217129305005074 + 0.1 * 6.345564365386963
Epoch 650, val loss: 1.0577645301818848
Epoch 660, training loss: 0.6502717137336731 = 0.015483022667467594 + 0.1 * 6.347886562347412
Epoch 660, val loss: 1.0659599304199219
Epoch 670, training loss: 0.6485331654548645 = 0.014798838645219803 + 0.1 * 6.337343215942383
Epoch 670, val loss: 1.0739243030548096
Epoch 680, training loss: 0.648149311542511 = 0.01416107825934887 + 0.1 * 6.3398823738098145
Epoch 680, val loss: 1.0816810131072998
Epoch 690, training loss: 0.6477587223052979 = 0.013565360568463802 + 0.1 * 6.341933727264404
Epoch 690, val loss: 1.0892748832702637
Epoch 700, training loss: 0.64546138048172 = 0.013009168207645416 + 0.1 * 6.324521541595459
Epoch 700, val loss: 1.0967106819152832
Epoch 710, training loss: 0.6454222202301025 = 0.01248844899237156 + 0.1 * 6.329338073730469
Epoch 710, val loss: 1.1039488315582275
Epoch 720, training loss: 0.6437336802482605 = 0.012000536546111107 + 0.1 * 6.317330837249756
Epoch 720, val loss: 1.111039638519287
Epoch 730, training loss: 0.6436836123466492 = 0.011541914194822311 + 0.1 * 6.321416854858398
Epoch 730, val loss: 1.117929220199585
Epoch 740, training loss: 0.6415681838989258 = 0.011111662723124027 + 0.1 * 6.304564952850342
Epoch 740, val loss: 1.124670147895813
Epoch 750, training loss: 0.6407581567764282 = 0.01070624403655529 + 0.1 * 6.300518989562988
Epoch 750, val loss: 1.1313806772232056
Epoch 760, training loss: 0.6405678987503052 = 0.01032395288348198 + 0.1 * 6.3024396896362305
Epoch 760, val loss: 1.137773036956787
Epoch 770, training loss: 0.6395573616027832 = 0.009964183904230595 + 0.1 * 6.295931816101074
Epoch 770, val loss: 1.1441339254379272
Epoch 780, training loss: 0.6392718553543091 = 0.009624391794204712 + 0.1 * 6.296473979949951
Epoch 780, val loss: 1.1504496335983276
Epoch 790, training loss: 0.6382710933685303 = 0.00930287316441536 + 0.1 * 6.289682388305664
Epoch 790, val loss: 1.1564980745315552
Epoch 800, training loss: 0.6382309794425964 = 0.008998842909932137 + 0.1 * 6.292320728302002
Epoch 800, val loss: 1.1625006198883057
Epoch 810, training loss: 0.6374755501747131 = 0.008710509166121483 + 0.1 * 6.287650108337402
Epoch 810, val loss: 1.168336033821106
Epoch 820, training loss: 0.6366490125656128 = 0.00843724887818098 + 0.1 * 6.2821173667907715
Epoch 820, val loss: 1.1740654706954956
Epoch 830, training loss: 0.636361300945282 = 0.008177907206118107 + 0.1 * 6.281833648681641
Epoch 830, val loss: 1.1797575950622559
Epoch 840, training loss: 0.6361059546470642 = 0.007931127212941647 + 0.1 * 6.281748294830322
Epoch 840, val loss: 1.1852936744689941
Epoch 850, training loss: 0.6363598704338074 = 0.0076964539475739 + 0.1 * 6.2866339683532715
Epoch 850, val loss: 1.190651774406433
Epoch 860, training loss: 0.6363956928253174 = 0.0074735116213560104 + 0.1 * 6.289221286773682
Epoch 860, val loss: 1.1959909200668335
Epoch 870, training loss: 0.633600652217865 = 0.007261808030307293 + 0.1 * 6.263388156890869
Epoch 870, val loss: 1.2012125253677368
Epoch 880, training loss: 0.633089542388916 = 0.007059882860630751 + 0.1 * 6.260296821594238
Epoch 880, val loss: 1.2064428329467773
Epoch 890, training loss: 0.6333320140838623 = 0.006866399198770523 + 0.1 * 6.264656066894531
Epoch 890, val loss: 1.2115983963012695
Epoch 900, training loss: 0.6327877640724182 = 0.006681410595774651 + 0.1 * 6.261063575744629
Epoch 900, val loss: 1.2165058851242065
Epoch 910, training loss: 0.6331209540367126 = 0.006505247671157122 + 0.1 * 6.266157150268555
Epoch 910, val loss: 1.2214910984039307
Epoch 920, training loss: 0.6322721838951111 = 0.006336663383990526 + 0.1 * 6.259354591369629
Epoch 920, val loss: 1.2264271974563599
Epoch 930, training loss: 0.6315727829933167 = 0.006175088696181774 + 0.1 * 6.253976821899414
Epoch 930, val loss: 1.2311596870422363
Epoch 940, training loss: 0.6308890581130981 = 0.006020800210535526 + 0.1 * 6.248682498931885
Epoch 940, val loss: 1.2359094619750977
Epoch 950, training loss: 0.6302886605262756 = 0.005872584879398346 + 0.1 * 6.244160175323486
Epoch 950, val loss: 1.240671157836914
Epoch 960, training loss: 0.6330990195274353 = 0.00573005760088563 + 0.1 * 6.2736897468566895
Epoch 960, val loss: 1.2452940940856934
Epoch 970, training loss: 0.6296349763870239 = 0.005593474954366684 + 0.1 * 6.240414619445801
Epoch 970, val loss: 1.2495992183685303
Epoch 980, training loss: 0.6296377182006836 = 0.005463196896016598 + 0.1 * 6.2417449951171875
Epoch 980, val loss: 1.254176139831543
Epoch 990, training loss: 0.6315364241600037 = 0.005337986629456282 + 0.1 * 6.261983871459961
Epoch 990, val loss: 1.2586414813995361
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7343
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7801568508148193 = 1.9427626132965088 + 0.1 * 8.373941421508789
Epoch 0, val loss: 1.9461452960968018
Epoch 10, training loss: 2.7700767517089844 = 1.932689905166626 + 0.1 * 8.373867988586426
Epoch 10, val loss: 1.9362878799438477
Epoch 20, training loss: 2.757812738418579 = 1.920462965965271 + 0.1 * 8.373497009277344
Epoch 20, val loss: 1.924394130706787
Epoch 30, training loss: 2.740525722503662 = 1.9034934043884277 + 0.1 * 8.370322227478027
Epoch 30, val loss: 1.907892107963562
Epoch 40, training loss: 2.7129735946655273 = 1.8786355257034302 + 0.1 * 8.343381881713867
Epoch 40, val loss: 1.8840497732162476
Epoch 50, training loss: 2.660939931869507 = 1.8443872928619385 + 0.1 * 8.165526390075684
Epoch 50, val loss: 1.852647066116333
Epoch 60, training loss: 2.5978164672851562 = 1.8063329458236694 + 0.1 * 7.914835453033447
Epoch 60, val loss: 1.8196361064910889
Epoch 70, training loss: 2.5233941078186035 = 1.7708698511123657 + 0.1 * 7.525243759155273
Epoch 70, val loss: 1.7888896465301514
Epoch 80, training loss: 2.4461350440979004 = 1.7334657907485962 + 0.1 * 7.126691818237305
Epoch 80, val loss: 1.7540184259414673
Epoch 90, training loss: 2.3786797523498535 = 1.6857506036758423 + 0.1 * 6.929291725158691
Epoch 90, val loss: 1.7114059925079346
Epoch 100, training loss: 2.3047146797180176 = 1.621827244758606 + 0.1 * 6.828874111175537
Epoch 100, val loss: 1.655785322189331
Epoch 110, training loss: 2.2175846099853516 = 1.5414329767227173 + 0.1 * 6.761516571044922
Epoch 110, val loss: 1.5869946479797363
Epoch 120, training loss: 2.12117338180542 = 1.4494234323501587 + 0.1 * 6.717499256134033
Epoch 120, val loss: 1.512484073638916
Epoch 130, training loss: 2.0219838619232178 = 1.3530811071395874 + 0.1 * 6.689027309417725
Epoch 130, val loss: 1.4353811740875244
Epoch 140, training loss: 1.924533486366272 = 1.257901906967163 + 0.1 * 6.66631555557251
Epoch 140, val loss: 1.3597575426101685
Epoch 150, training loss: 1.8303766250610352 = 1.165867567062378 + 0.1 * 6.645089626312256
Epoch 150, val loss: 1.2877740859985352
Epoch 160, training loss: 1.7432410717010498 = 1.0797888040542603 + 0.1 * 6.634521961212158
Epoch 160, val loss: 1.2211904525756836
Epoch 170, training loss: 1.6654479503631592 = 1.0039151906967163 + 0.1 * 6.615326881408691
Epoch 170, val loss: 1.1642041206359863
Epoch 180, training loss: 1.5973234176635742 = 0.937288224697113 + 0.1 * 6.600351810455322
Epoch 180, val loss: 1.115272879600525
Epoch 190, training loss: 1.5358139276504517 = 0.8765906691551208 + 0.1 * 6.5922322273254395
Epoch 190, val loss: 1.0722808837890625
Epoch 200, training loss: 1.4775502681732178 = 0.8192775845527649 + 0.1 * 6.58272647857666
Epoch 200, val loss: 1.0328550338745117
Epoch 210, training loss: 1.4203364849090576 = 0.7630716562271118 + 0.1 * 6.572647571563721
Epoch 210, val loss: 0.9950664639472961
Epoch 220, training loss: 1.363863229751587 = 0.7072237730026245 + 0.1 * 6.566394805908203
Epoch 220, val loss: 0.9584240913391113
Epoch 230, training loss: 1.3083969354629517 = 0.6524736285209656 + 0.1 * 6.559232711791992
Epoch 230, val loss: 0.9238573312759399
Epoch 240, training loss: 1.2547560930252075 = 0.599408745765686 + 0.1 * 6.553473472595215
Epoch 240, val loss: 0.8926387429237366
Epoch 250, training loss: 1.2034579515457153 = 0.54850172996521 + 0.1 * 6.549561977386475
Epoch 250, val loss: 0.8654823899269104
Epoch 260, training loss: 1.1541141271591187 = 0.49976345896720886 + 0.1 * 6.543506622314453
Epoch 260, val loss: 0.84270179271698
Epoch 270, training loss: 1.106752872467041 = 0.4529334306716919 + 0.1 * 6.53819465637207
Epoch 270, val loss: 0.8234018683433533
Epoch 280, training loss: 1.061930775642395 = 0.40820956230163574 + 0.1 * 6.537211894989014
Epoch 280, val loss: 0.8072382211685181
Epoch 290, training loss: 1.0193331241607666 = 0.36611324548721313 + 0.1 * 6.532197952270508
Epoch 290, val loss: 0.7941924929618835
Epoch 300, training loss: 0.9800633192062378 = 0.326901376247406 + 0.1 * 6.531619071960449
Epoch 300, val loss: 0.784210205078125
Epoch 310, training loss: 0.9436327219009399 = 0.2912810742855072 + 0.1 * 6.5235161781311035
Epoch 310, val loss: 0.7775354385375977
Epoch 320, training loss: 0.9111199378967285 = 0.25933733582496643 + 0.1 * 6.517825603485107
Epoch 320, val loss: 0.7743725180625916
Epoch 330, training loss: 0.8826872706413269 = 0.23094479739665985 + 0.1 * 6.517424583435059
Epoch 330, val loss: 0.7743899822235107
Epoch 340, training loss: 0.8568544387817383 = 0.20607388019561768 + 0.1 * 6.507805347442627
Epoch 340, val loss: 0.7771099805831909
Epoch 350, training loss: 0.8341819643974304 = 0.1843278855085373 + 0.1 * 6.49854040145874
Epoch 350, val loss: 0.7822263836860657
Epoch 360, training loss: 0.8146669864654541 = 0.1652769297361374 + 0.1 * 6.493900299072266
Epoch 360, val loss: 0.789376437664032
Epoch 370, training loss: 0.7987133264541626 = 0.14863485097885132 + 0.1 * 6.500784873962402
Epoch 370, val loss: 0.7979857325553894
Epoch 380, training loss: 0.7823090553283691 = 0.13409852981567383 + 0.1 * 6.482105255126953
Epoch 380, val loss: 0.8076090216636658
Epoch 390, training loss: 0.7690611481666565 = 0.12130583822727203 + 0.1 * 6.477553367614746
Epoch 390, val loss: 0.8180631399154663
Epoch 400, training loss: 0.756727397441864 = 0.11001558601856232 + 0.1 * 6.467118263244629
Epoch 400, val loss: 0.8292496204376221
Epoch 410, training loss: 0.7460390329360962 = 0.0999879315495491 + 0.1 * 6.460511207580566
Epoch 410, val loss: 0.8407211303710938
Epoch 420, training loss: 0.7384451627731323 = 0.09107524901628494 + 0.1 * 6.47369909286499
Epoch 420, val loss: 0.8523976802825928
Epoch 430, training loss: 0.7285111546516418 = 0.08316574990749359 + 0.1 * 6.45345401763916
Epoch 430, val loss: 0.8639323711395264
Epoch 440, training loss: 0.7200512886047363 = 0.07608845829963684 + 0.1 * 6.4396281242370605
Epoch 440, val loss: 0.8755403161048889
Epoch 450, training loss: 0.7147061824798584 = 0.06973082572221756 + 0.1 * 6.449753761291504
Epoch 450, val loss: 0.8871195316314697
Epoch 460, training loss: 0.7070601582527161 = 0.06402646005153656 + 0.1 * 6.430336952209473
Epoch 460, val loss: 0.8984826803207397
Epoch 470, training loss: 0.7010995149612427 = 0.058885328471660614 + 0.1 * 6.4221415519714355
Epoch 470, val loss: 0.9098320007324219
Epoch 480, training loss: 0.6973366141319275 = 0.05425800383090973 + 0.1 * 6.4307861328125
Epoch 480, val loss: 0.9208055734634399
Epoch 490, training loss: 0.6916603446006775 = 0.05009704455733299 + 0.1 * 6.415632724761963
Epoch 490, val loss: 0.9318590760231018
Epoch 500, training loss: 0.6886152625083923 = 0.046334441751241684 + 0.1 * 6.4228081703186035
Epoch 500, val loss: 0.9426733255386353
Epoch 510, training loss: 0.6834672689437866 = 0.0429372638463974 + 0.1 * 6.405299663543701
Epoch 510, val loss: 0.9533663988113403
Epoch 520, training loss: 0.6798720955848694 = 0.039858732372522354 + 0.1 * 6.40013313293457
Epoch 520, val loss: 0.9638081789016724
Epoch 530, training loss: 0.6768897175788879 = 0.03707495704293251 + 0.1 * 6.398147106170654
Epoch 530, val loss: 0.9740662574768066
Epoch 540, training loss: 0.6724585294723511 = 0.03455314040184021 + 0.1 * 6.379054069519043
Epoch 540, val loss: 0.9842762351036072
Epoch 550, training loss: 0.6703407764434814 = 0.03225736692547798 + 0.1 * 6.380834102630615
Epoch 550, val loss: 0.9940629601478577
Epoch 560, training loss: 0.6676222681999207 = 0.03017062321305275 + 0.1 * 6.374516487121582
Epoch 560, val loss: 1.0039652585983276
Epoch 570, training loss: 0.6646205186843872 = 0.028267651796340942 + 0.1 * 6.363528251647949
Epoch 570, val loss: 1.013404369354248
Epoch 580, training loss: 0.6628035306930542 = 0.026530927047133446 + 0.1 * 6.362725734710693
Epoch 580, val loss: 1.0228551626205444
Epoch 590, training loss: 0.6604929566383362 = 0.024942893534898758 + 0.1 * 6.3555006980896
Epoch 590, val loss: 1.032003402709961
Epoch 600, training loss: 0.6584600806236267 = 0.02349013276398182 + 0.1 * 6.3496994972229
Epoch 600, val loss: 1.0411036014556885
Epoch 610, training loss: 0.6587665677070618 = 0.02215672843158245 + 0.1 * 6.366098403930664
Epoch 610, val loss: 1.0497239828109741
Epoch 620, training loss: 0.6554210782051086 = 0.020935969427227974 + 0.1 * 6.344851016998291
Epoch 620, val loss: 1.0582351684570312
Epoch 630, training loss: 0.6538968682289124 = 0.019812120124697685 + 0.1 * 6.340847492218018
Epoch 630, val loss: 1.0665751695632935
Epoch 640, training loss: 0.6519790291786194 = 0.01877492107450962 + 0.1 * 6.332040786743164
Epoch 640, val loss: 1.0747833251953125
Epoch 650, training loss: 0.6529726982116699 = 0.017816122621297836 + 0.1 * 6.351565837860107
Epoch 650, val loss: 1.0828737020492554
Epoch 660, training loss: 0.6506658792495728 = 0.016930771991610527 + 0.1 * 6.337351322174072
Epoch 660, val loss: 1.0905652046203613
Epoch 670, training loss: 0.6480337381362915 = 0.016113078221678734 + 0.1 * 6.319206237792969
Epoch 670, val loss: 1.0982478857040405
Epoch 680, training loss: 0.6490281820297241 = 0.015352161601185799 + 0.1 * 6.3367600440979
Epoch 680, val loss: 1.1056982278823853
Epoch 690, training loss: 0.6469758749008179 = 0.014646589756011963 + 0.1 * 6.3232927322387695
Epoch 690, val loss: 1.1129902601242065
Epoch 700, training loss: 0.6471869349479675 = 0.013990329578518867 + 0.1 * 6.331965923309326
Epoch 700, val loss: 1.12018620967865
Epoch 710, training loss: 0.6447466611862183 = 0.01337970420718193 + 0.1 * 6.313669681549072
Epoch 710, val loss: 1.127097725868225
Epoch 720, training loss: 0.6431171894073486 = 0.012810039333999157 + 0.1 * 6.30307149887085
Epoch 720, val loss: 1.133988380432129
Epoch 730, training loss: 0.6460338830947876 = 0.01227517519146204 + 0.1 * 6.337587356567383
Epoch 730, val loss: 1.14077889919281
Epoch 740, training loss: 0.6413782238960266 = 0.011775067076086998 + 0.1 * 6.296031475067139
Epoch 740, val loss: 1.1471415758132935
Epoch 750, training loss: 0.6427788734436035 = 0.011308509856462479 + 0.1 * 6.314703464508057
Epoch 750, val loss: 1.1535451412200928
Epoch 760, training loss: 0.6407495737075806 = 0.010869846679270267 + 0.1 * 6.298797130584717
Epoch 760, val loss: 1.1597440242767334
Epoch 770, training loss: 0.6393105983734131 = 0.010458193719387054 + 0.1 * 6.288524150848389
Epoch 770, val loss: 1.1658090353012085
Epoch 780, training loss: 0.6400904655456543 = 0.010069657117128372 + 0.1 * 6.30020809173584
Epoch 780, val loss: 1.1718460321426392
Epoch 790, training loss: 0.6382589340209961 = 0.009704162366688251 + 0.1 * 6.285547256469727
Epoch 790, val loss: 1.1776460409164429
Epoch 800, training loss: 0.6414433121681213 = 0.009359163232147694 + 0.1 * 6.320841312408447
Epoch 800, val loss: 1.1834352016448975
Epoch 810, training loss: 0.6373761892318726 = 0.009034345857799053 + 0.1 * 6.28341817855835
Epoch 810, val loss: 1.1889536380767822
Epoch 820, training loss: 0.6382079124450684 = 0.008729069493710995 + 0.1 * 6.294788837432861
Epoch 820, val loss: 1.1945103406906128
Epoch 830, training loss: 0.6356502771377563 = 0.008439747616648674 + 0.1 * 6.2721052169799805
Epoch 830, val loss: 1.1997798681259155
Epoch 840, training loss: 0.6361074447631836 = 0.008165399543941021 + 0.1 * 6.279420375823975
Epoch 840, val loss: 1.2050179243087769
Epoch 850, training loss: 0.6359166502952576 = 0.007905055768787861 + 0.1 * 6.280115604400635
Epoch 850, val loss: 1.2101390361785889
Epoch 860, training loss: 0.634405791759491 = 0.007657794281840324 + 0.1 * 6.26747989654541
Epoch 860, val loss: 1.2153080701828003
Epoch 870, training loss: 0.6344092488288879 = 0.007423198316246271 + 0.1 * 6.26986026763916
Epoch 870, val loss: 1.220260500907898
Epoch 880, training loss: 0.6338882446289062 = 0.007199901156127453 + 0.1 * 6.266883373260498
Epoch 880, val loss: 1.2251641750335693
Epoch 890, training loss: 0.6332804560661316 = 0.006988662760704756 + 0.1 * 6.262917995452881
Epoch 890, val loss: 1.2298418283462524
Epoch 900, training loss: 0.6331615447998047 = 0.006787546910345554 + 0.1 * 6.263740062713623
Epoch 900, val loss: 1.234588861465454
Epoch 910, training loss: 0.6321853399276733 = 0.006595219951122999 + 0.1 * 6.255901336669922
Epoch 910, val loss: 1.2391233444213867
Epoch 920, training loss: 0.6332484483718872 = 0.0064122057519853115 + 0.1 * 6.268362045288086
Epoch 920, val loss: 1.2437191009521484
Epoch 930, training loss: 0.6331220269203186 = 0.00623713294044137 + 0.1 * 6.268848896026611
Epoch 930, val loss: 1.2480357885360718
Epoch 940, training loss: 0.6317068934440613 = 0.006070658564567566 + 0.1 * 6.256361961364746
Epoch 940, val loss: 1.252381443977356
Epoch 950, training loss: 0.6313571929931641 = 0.005912123713642359 + 0.1 * 6.254450798034668
Epoch 950, val loss: 1.2567628622055054
Epoch 960, training loss: 0.6304171085357666 = 0.005759531166404486 + 0.1 * 6.246575355529785
Epoch 960, val loss: 1.2608011960983276
Epoch 970, training loss: 0.6296738982200623 = 0.00561455637216568 + 0.1 * 6.240592956542969
Epoch 970, val loss: 1.2649027109146118
Epoch 980, training loss: 0.6309528946876526 = 0.005475102458149195 + 0.1 * 6.254777908325195
Epoch 980, val loss: 1.2689670324325562
Epoch 990, training loss: 0.6301046013832092 = 0.005341656040400267 + 0.1 * 6.247629642486572
Epoch 990, val loss: 1.2727797031402588
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8155
Flip ASR: 0.7778/225 nodes
The final ASR:0.76384, 0.03665, Accuracy:0.81975, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9440])
updated graph: torch.Size([2, 10518])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00460, Accuracy:0.83333, 0.00000
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8076038360595703 = 1.9702280759811401 + 0.1 * 8.373756408691406
Epoch 0, val loss: 1.9667340517044067
Epoch 10, training loss: 2.796776056289673 = 1.9594297409057617 + 0.1 * 8.37346363067627
Epoch 10, val loss: 1.9564859867095947
Epoch 20, training loss: 2.7832179069519043 = 1.9460424184799194 + 0.1 * 8.371755599975586
Epoch 20, val loss: 1.9431601762771606
Epoch 30, training loss: 2.762653350830078 = 1.9267102479934692 + 0.1 * 8.359432220458984
Epoch 30, val loss: 1.9234517812728882
Epoch 40, training loss: 2.725409984588623 = 1.8974229097366333 + 0.1 * 8.27987003326416
Epoch 40, val loss: 1.893912672996521
Epoch 50, training loss: 2.640374183654785 = 1.859684705734253 + 0.1 * 7.806893825531006
Epoch 50, val loss: 1.8575390577316284
Epoch 60, training loss: 2.557440757751465 = 1.820702075958252 + 0.1 * 7.367385387420654
Epoch 60, val loss: 1.8220182657241821
Epoch 70, training loss: 2.477858781814575 = 1.7844494581222534 + 0.1 * 6.934092998504639
Epoch 70, val loss: 1.7922805547714233
Epoch 80, training loss: 2.426755905151367 = 1.7537249326705933 + 0.1 * 6.730310440063477
Epoch 80, val loss: 1.768568515777588
Epoch 90, training loss: 2.3812930583953857 = 1.7160327434539795 + 0.1 * 6.6526031494140625
Epoch 90, val loss: 1.7361834049224854
Epoch 100, training loss: 2.3255581855773926 = 1.6648234128952026 + 0.1 * 6.60734748840332
Epoch 100, val loss: 1.6915494203567505
Epoch 110, training loss: 2.254850387573242 = 1.5976747274398804 + 0.1 * 6.5717573165893555
Epoch 110, val loss: 1.6344032287597656
Epoch 120, training loss: 2.1706278324127197 = 1.5159928798675537 + 0.1 * 6.54634952545166
Epoch 120, val loss: 1.5658273696899414
Epoch 130, training loss: 2.080264091491699 = 1.4269179105758667 + 0.1 * 6.533463001251221
Epoch 130, val loss: 1.4926862716674805
Epoch 140, training loss: 1.9895727634429932 = 1.3379706144332886 + 0.1 * 6.516021251678467
Epoch 140, val loss: 1.4219316244125366
Epoch 150, training loss: 1.8990592956542969 = 1.2485555410385132 + 0.1 * 6.505037307739258
Epoch 150, val loss: 1.3537157773971558
Epoch 160, training loss: 1.8082200288772583 = 1.1594585180282593 + 0.1 * 6.48761510848999
Epoch 160, val loss: 1.287625789642334
Epoch 170, training loss: 1.7187780141830444 = 1.0710246562957764 + 0.1 * 6.477533340454102
Epoch 170, val loss: 1.222483515739441
Epoch 180, training loss: 1.632383108139038 = 0.9856953024864197 + 0.1 * 6.466878414154053
Epoch 180, val loss: 1.15996515750885
Epoch 190, training loss: 1.5493710041046143 = 0.903560221195221 + 0.1 * 6.458107948303223
Epoch 190, val loss: 1.0996692180633545
Epoch 200, training loss: 1.469955325126648 = 0.8251017332077026 + 0.1 * 6.448535919189453
Epoch 200, val loss: 1.041987657546997
Epoch 210, training loss: 1.3947696685791016 = 0.7509822845458984 + 0.1 * 6.437873840332031
Epoch 210, val loss: 0.9879364967346191
Epoch 220, training loss: 1.3263697624206543 = 0.6828542947769165 + 0.1 * 6.435154438018799
Epoch 220, val loss: 0.9392223954200745
Epoch 230, training loss: 1.264756441116333 = 0.623002827167511 + 0.1 * 6.417536735534668
Epoch 230, val loss: 0.8984957933425903
Epoch 240, training loss: 1.2121182680130005 = 0.571255624294281 + 0.1 * 6.408626556396484
Epoch 240, val loss: 0.8656406998634338
Epoch 250, training loss: 1.1677337884902954 = 0.5273362994194031 + 0.1 * 6.403975009918213
Epoch 250, val loss: 0.8404582738876343
Epoch 260, training loss: 1.1290795803070068 = 0.49002164602279663 + 0.1 * 6.3905792236328125
Epoch 260, val loss: 0.821763277053833
Epoch 270, training loss: 1.0954313278198242 = 0.4576364755630493 + 0.1 * 6.377947807312012
Epoch 270, val loss: 0.807625412940979
Epoch 280, training loss: 1.0674842596054077 = 0.4286891520023346 + 0.1 * 6.387951374053955
Epoch 280, val loss: 0.7968500256538391
Epoch 290, training loss: 1.038860559463501 = 0.4022187292575836 + 0.1 * 6.366418361663818
Epoch 290, val loss: 0.7885907292366028
Epoch 300, training loss: 1.0120259523391724 = 0.37680765986442566 + 0.1 * 6.352182865142822
Epoch 300, val loss: 0.7819967269897461
Epoch 310, training loss: 0.9862066507339478 = 0.35143977403640747 + 0.1 * 6.347668647766113
Epoch 310, val loss: 0.7765428423881531
Epoch 320, training loss: 0.960807204246521 = 0.3256201446056366 + 0.1 * 6.351870536804199
Epoch 320, val loss: 0.7716107368469238
Epoch 330, training loss: 0.9330370426177979 = 0.29915687441825867 + 0.1 * 6.338801860809326
Epoch 330, val loss: 0.7672666311264038
Epoch 340, training loss: 0.9047826528549194 = 0.2722381055355072 + 0.1 * 6.325445652008057
Epoch 340, val loss: 0.7632100582122803
Epoch 350, training loss: 0.8774858713150024 = 0.24546542763710022 + 0.1 * 6.32020378112793
Epoch 350, val loss: 0.7597262263298035
Epoch 360, training loss: 0.8517562747001648 = 0.21977512538433075 + 0.1 * 6.3198113441467285
Epoch 360, val loss: 0.7569742202758789
Epoch 370, training loss: 0.8270775079727173 = 0.1960587054491043 + 0.1 * 6.310187816619873
Epoch 370, val loss: 0.7555396556854248
Epoch 380, training loss: 0.8068444132804871 = 0.17467062175273895 + 0.1 * 6.321737766265869
Epoch 380, val loss: 0.7552785873413086
Epoch 390, training loss: 0.7865827679634094 = 0.15589077770709991 + 0.1 * 6.306920051574707
Epoch 390, val loss: 0.7561700344085693
Epoch 400, training loss: 0.7694606781005859 = 0.1395164132118225 + 0.1 * 6.299442768096924
Epoch 400, val loss: 0.7582483291625977
Epoch 410, training loss: 0.7569714188575745 = 0.12524308264255524 + 0.1 * 6.3172831535339355
Epoch 410, val loss: 0.7613115906715393
Epoch 420, training loss: 0.7417370676994324 = 0.11284656822681427 + 0.1 * 6.288905143737793
Epoch 420, val loss: 0.7653653025627136
Epoch 430, training loss: 0.7298498749732971 = 0.10200125724077225 + 0.1 * 6.278486251831055
Epoch 430, val loss: 0.7702236771583557
Epoch 440, training loss: 0.7203612327575684 = 0.09248185902833939 + 0.1 * 6.278793811798096
Epoch 440, val loss: 0.7756993174552917
Epoch 450, training loss: 0.7118709087371826 = 0.0841292291879654 + 0.1 * 6.277416706085205
Epoch 450, val loss: 0.7819061279296875
Epoch 460, training loss: 0.703368604183197 = 0.07675757259130478 + 0.1 * 6.266110420227051
Epoch 460, val loss: 0.7884967923164368
Epoch 470, training loss: 0.6964101195335388 = 0.07021832466125488 + 0.1 * 6.261917591094971
Epoch 470, val loss: 0.7956715226173401
Epoch 480, training loss: 0.6907408237457275 = 0.06437692046165466 + 0.1 * 6.263638496398926
Epoch 480, val loss: 0.8031392097473145
Epoch 490, training loss: 0.6856648921966553 = 0.05916237086057663 + 0.1 * 6.2650251388549805
Epoch 490, val loss: 0.8108950853347778
Epoch 500, training loss: 0.6796931028366089 = 0.0545024499297142 + 0.1 * 6.251906871795654
Epoch 500, val loss: 0.8189520239830017
Epoch 510, training loss: 0.675068736076355 = 0.05032195523381233 + 0.1 * 6.247467994689941
Epoch 510, val loss: 0.8270790576934814
Epoch 520, training loss: 0.6718738675117493 = 0.04657149687409401 + 0.1 * 6.253023624420166
Epoch 520, val loss: 0.8353862166404724
Epoch 530, training loss: 0.6667364239692688 = 0.04320196434855461 + 0.1 * 6.235344409942627
Epoch 530, val loss: 0.8437401056289673
Epoch 540, training loss: 0.6641767621040344 = 0.040161389857530594 + 0.1 * 6.240153789520264
Epoch 540, val loss: 0.8521588444709778
Epoch 550, training loss: 0.6613463759422302 = 0.0374167300760746 + 0.1 * 6.239295959472656
Epoch 550, val loss: 0.86057049036026
Epoch 560, training loss: 0.6578359603881836 = 0.034940410405397415 + 0.1 * 6.228955268859863
Epoch 560, val loss: 0.8690477609634399
Epoch 570, training loss: 0.6552322506904602 = 0.032692067325115204 + 0.1 * 6.225401401519775
Epoch 570, val loss: 0.8775234818458557
Epoch 580, training loss: 0.6533202528953552 = 0.030645744875073433 + 0.1 * 6.226745128631592
Epoch 580, val loss: 0.8857808709144592
Epoch 590, training loss: 0.6507961750030518 = 0.02878693863749504 + 0.1 * 6.220091819763184
Epoch 590, val loss: 0.8941218852996826
Epoch 600, training loss: 0.649229109287262 = 0.027091598138213158 + 0.1 * 6.221374988555908
Epoch 600, val loss: 0.9021964073181152
Epoch 610, training loss: 0.6466332674026489 = 0.02554679848253727 + 0.1 * 6.210864543914795
Epoch 610, val loss: 0.9103358387947083
Epoch 620, training loss: 0.6451658606529236 = 0.024128887802362442 + 0.1 * 6.21036958694458
Epoch 620, val loss: 0.9182265996932983
Epoch 630, training loss: 0.6429703235626221 = 0.022826600819826126 + 0.1 * 6.201436996459961
Epoch 630, val loss: 0.9258898496627808
Epoch 640, training loss: 0.642466127872467 = 0.021629901602864265 + 0.1 * 6.208361625671387
Epoch 640, val loss: 0.9335340261459351
Epoch 650, training loss: 0.6404807567596436 = 0.020527349784970284 + 0.1 * 6.199533939361572
Epoch 650, val loss: 0.940987765789032
Epoch 660, training loss: 0.6396948099136353 = 0.019510386511683464 + 0.1 * 6.201844215393066
Epoch 660, val loss: 0.9484419226646423
Epoch 670, training loss: 0.6400269269943237 = 0.018568996340036392 + 0.1 * 6.214579105377197
Epoch 670, val loss: 0.955589234828949
Epoch 680, training loss: 0.6375082731246948 = 0.017699014395475388 + 0.1 * 6.198092460632324
Epoch 680, val loss: 0.9626790285110474
Epoch 690, training loss: 0.6361144781112671 = 0.016892580315470695 + 0.1 * 6.192219257354736
Epoch 690, val loss: 0.9697220325469971
Epoch 700, training loss: 0.6346901059150696 = 0.016141479834914207 + 0.1 * 6.185486316680908
Epoch 700, val loss: 0.9765025973320007
Epoch 710, training loss: 0.6349728107452393 = 0.01544297393411398 + 0.1 * 6.195298194885254
Epoch 710, val loss: 0.9832943677902222
Epoch 720, training loss: 0.6329527497291565 = 0.014790531247854233 + 0.1 * 6.18162202835083
Epoch 720, val loss: 0.989922285079956
Epoch 730, training loss: 0.6322460770606995 = 0.014181775972247124 + 0.1 * 6.180642604827881
Epoch 730, val loss: 0.9964365363121033
Epoch 740, training loss: 0.631456196308136 = 0.013612089678645134 + 0.1 * 6.178441047668457
Epoch 740, val loss: 1.0027827024459839
Epoch 750, training loss: 0.6312833428382874 = 0.013078625313937664 + 0.1 * 6.182046890258789
Epoch 750, val loss: 1.0090010166168213
Epoch 760, training loss: 0.6296592354774475 = 0.012579252012073994 + 0.1 * 6.170799732208252
Epoch 760, val loss: 1.0151221752166748
Epoch 770, training loss: 0.6295920610427856 = 0.012110301293432713 + 0.1 * 6.1748175621032715
Epoch 770, val loss: 1.0212429761886597
Epoch 780, training loss: 0.6283193826675415 = 0.011668040417134762 + 0.1 * 6.166513442993164
Epoch 780, val loss: 1.0270916223526
Epoch 790, training loss: 0.628555178642273 = 0.011251633986830711 + 0.1 * 6.173035144805908
Epoch 790, val loss: 1.0329281091690063
Epoch 800, training loss: 0.6289711594581604 = 0.010858654975891113 + 0.1 * 6.181124687194824
Epoch 800, val loss: 1.0385346412658691
Epoch 810, training loss: 0.6272742748260498 = 0.010488873347640038 + 0.1 * 6.167853832244873
Epoch 810, val loss: 1.0440478324890137
Epoch 820, training loss: 0.6263075470924377 = 0.01014082133769989 + 0.1 * 6.1616668701171875
Epoch 820, val loss: 1.0496102571487427
Epoch 830, training loss: 0.6259411573410034 = 0.009810404852032661 + 0.1 * 6.161307334899902
Epoch 830, val loss: 1.0548996925354004
Epoch 840, training loss: 0.6253806948661804 = 0.009497951716184616 + 0.1 * 6.158827304840088
Epoch 840, val loss: 1.0602374076843262
Epoch 850, training loss: 0.6249922513961792 = 0.009201341308653355 + 0.1 * 6.157908916473389
Epoch 850, val loss: 1.0654706954956055
Epoch 860, training loss: 0.6244380474090576 = 0.008918450213968754 + 0.1 * 6.155195713043213
Epoch 860, val loss: 1.0704065561294556
Epoch 870, training loss: 0.6246738433837891 = 0.00865128729492426 + 0.1 * 6.1602253913879395
Epoch 870, val loss: 1.0753860473632812
Epoch 880, training loss: 0.6241955161094666 = 0.008397625759243965 + 0.1 * 6.157978534698486
Epoch 880, val loss: 1.0803221464157104
Epoch 890, training loss: 0.6229066252708435 = 0.008156209252774715 + 0.1 * 6.1475043296813965
Epoch 890, val loss: 1.0850353240966797
Epoch 900, training loss: 0.6228193640708923 = 0.007926725782454014 + 0.1 * 6.148926734924316
Epoch 900, val loss: 1.0898668766021729
Epoch 910, training loss: 0.6242313385009766 = 0.007707265671342611 + 0.1 * 6.16524076461792
Epoch 910, val loss: 1.094467043876648
Epoch 920, training loss: 0.6220591068267822 = 0.0074975090101361275 + 0.1 * 6.145616054534912
Epoch 920, val loss: 1.0989091396331787
Epoch 930, training loss: 0.6220542192459106 = 0.007298171985894442 + 0.1 * 6.1475605964660645
Epoch 930, val loss: 1.1034810543060303
Epoch 940, training loss: 0.6220801472663879 = 0.0071066138334572315 + 0.1 * 6.149735450744629
Epoch 940, val loss: 1.1076667308807373
Epoch 950, training loss: 0.6208134293556213 = 0.006923997309058905 + 0.1 * 6.138894557952881
Epoch 950, val loss: 1.1119993925094604
Epoch 960, training loss: 0.6215704679489136 = 0.006749507039785385 + 0.1 * 6.148209571838379
Epoch 960, val loss: 1.11630117893219
Epoch 970, training loss: 0.6207255125045776 = 0.00658166641369462 + 0.1 * 6.1414384841918945
Epoch 970, val loss: 1.1204067468643188
Epoch 980, training loss: 0.6201425790786743 = 0.0064214239828288555 + 0.1 * 6.137211322784424
Epoch 980, val loss: 1.1246058940887451
Epoch 990, training loss: 0.6205575466156006 = 0.006267111748456955 + 0.1 * 6.142903804779053
Epoch 990, val loss: 1.1286252737045288
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6199
Flip ASR: 0.5467/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.770747661590576 = 1.9333666563034058 + 0.1 * 8.373809814453125
Epoch 0, val loss: 1.9240561723709106
Epoch 10, training loss: 2.761040449142456 = 1.923680067062378 + 0.1 * 8.373602867126465
Epoch 10, val loss: 1.9149755239486694
Epoch 20, training loss: 2.7490508556365967 = 1.9117929935455322 + 0.1 * 8.372577667236328
Epoch 20, val loss: 1.9035133123397827
Epoch 30, training loss: 2.7316389083862305 = 1.8950653076171875 + 0.1 * 8.36573600769043
Epoch 30, val loss: 1.8872264623641968
Epoch 40, training loss: 2.700138568878174 = 1.8703440427780151 + 0.1 * 8.297944068908691
Epoch 40, val loss: 1.8634799718856812
Epoch 50, training loss: 2.602851629257202 = 1.837207555770874 + 0.1 * 7.656440734863281
Epoch 50, val loss: 1.8326665163040161
Epoch 60, training loss: 2.5328848361968994 = 1.8018391132354736 + 0.1 * 7.310457706451416
Epoch 60, val loss: 1.8004878759384155
Epoch 70, training loss: 2.4640185832977295 = 1.7635037899017334 + 0.1 * 7.005147457122803
Epoch 70, val loss: 1.7674915790557861
Epoch 80, training loss: 2.4028220176696777 = 1.720346212387085 + 0.1 * 6.824757099151611
Epoch 80, val loss: 1.7304190397262573
Epoch 90, training loss: 2.3370866775512695 = 1.6627854108810425 + 0.1 * 6.743013858795166
Epoch 90, val loss: 1.6778706312179565
Epoch 100, training loss: 2.2546536922454834 = 1.585857629776001 + 0.1 * 6.687960624694824
Epoch 100, val loss: 1.60891592502594
Epoch 110, training loss: 2.156731128692627 = 1.4920958280563354 + 0.1 * 6.646352767944336
Epoch 110, val loss: 1.5301657915115356
Epoch 120, training loss: 2.053840160369873 = 1.3921505212783813 + 0.1 * 6.616897106170654
Epoch 120, val loss: 1.4481537342071533
Epoch 130, training loss: 1.9578298330307007 = 1.2990753650665283 + 0.1 * 6.5875444412231445
Epoch 130, val loss: 1.3758331537246704
Epoch 140, training loss: 1.8755543231964111 = 1.2196134328842163 + 0.1 * 6.559409141540527
Epoch 140, val loss: 1.3178043365478516
Epoch 150, training loss: 1.804312825202942 = 1.1505907773971558 + 0.1 * 6.537220478057861
Epoch 150, val loss: 1.2693843841552734
Epoch 160, training loss: 1.7366235256195068 = 1.0859674215316772 + 0.1 * 6.506561279296875
Epoch 160, val loss: 1.2250454425811768
Epoch 170, training loss: 1.6687144041061401 = 1.0202385187149048 + 0.1 * 6.4847588539123535
Epoch 170, val loss: 1.1796700954437256
Epoch 180, training loss: 1.5990296602249146 = 0.9521591663360596 + 0.1 * 6.468704700469971
Epoch 180, val loss: 1.1317459344863892
Epoch 190, training loss: 1.5275030136108398 = 0.882279098033905 + 0.1 * 6.4522385597229
Epoch 190, val loss: 1.0817220211029053
Epoch 200, training loss: 1.4559752941131592 = 0.8120666742324829 + 0.1 * 6.439085960388184
Epoch 200, val loss: 1.031279444694519
Epoch 210, training loss: 1.3862385749816895 = 0.7435467839241028 + 0.1 * 6.42691707611084
Epoch 210, val loss: 0.9824221134185791
Epoch 220, training loss: 1.3210680484771729 = 0.6782751679420471 + 0.1 * 6.4279279708862305
Epoch 220, val loss: 0.9370480179786682
Epoch 230, training loss: 1.2595100402832031 = 0.6185953617095947 + 0.1 * 6.409145832061768
Epoch 230, val loss: 0.8974205255508423
Epoch 240, training loss: 1.2036919593811035 = 0.5638251304626465 + 0.1 * 6.3986687660217285
Epoch 240, val loss: 0.8634508848190308
Epoch 250, training loss: 1.1552081108093262 = 0.5135870575904846 + 0.1 * 6.416211128234863
Epoch 250, val loss: 0.8355612754821777
Epoch 260, training loss: 1.1070754528045654 = 0.4684842526912689 + 0.1 * 6.38591194152832
Epoch 260, val loss: 0.8140590190887451
Epoch 270, training loss: 1.0665806531906128 = 0.4278421103954315 + 0.1 * 6.387385368347168
Epoch 270, val loss: 0.7980373501777649
Epoch 280, training loss: 1.0277842283248901 = 0.39131808280944824 + 0.1 * 6.36466121673584
Epoch 280, val loss: 0.7867580652236938
Epoch 290, training loss: 0.9942973852157593 = 0.3582591116428375 + 0.1 * 6.360382556915283
Epoch 290, val loss: 0.7793242931365967
Epoch 300, training loss: 0.9639698266983032 = 0.32832273840904236 + 0.1 * 6.356470584869385
Epoch 300, val loss: 0.7753241658210754
Epoch 310, training loss: 0.9359132647514343 = 0.3009605407714844 + 0.1 * 6.349526882171631
Epoch 310, val loss: 0.7740477919578552
Epoch 320, training loss: 0.9114421606063843 = 0.2756556570529938 + 0.1 * 6.3578643798828125
Epoch 320, val loss: 0.7749136090278625
Epoch 330, training loss: 0.8869290351867676 = 0.2523774206638336 + 0.1 * 6.345516204833984
Epoch 330, val loss: 0.7771542072296143
Epoch 340, training loss: 0.8634562492370605 = 0.2308809459209442 + 0.1 * 6.325753211975098
Epoch 340, val loss: 0.780617892742157
Epoch 350, training loss: 0.8429712653160095 = 0.21099649369716644 + 0.1 * 6.319747447967529
Epoch 350, val loss: 0.7850943803787231
Epoch 360, training loss: 0.8259324431419373 = 0.19272464513778687 + 0.1 * 6.332077980041504
Epoch 360, val loss: 0.7902671694755554
Epoch 370, training loss: 0.8076156377792358 = 0.1761656552553177 + 0.1 * 6.314499855041504
Epoch 370, val loss: 0.7963826060295105
Epoch 380, training loss: 0.7913900017738342 = 0.1611655205488205 + 0.1 * 6.302244663238525
Epoch 380, val loss: 0.8031351566314697
Epoch 390, training loss: 0.7782383561134338 = 0.14761365950107574 + 0.1 * 6.306246757507324
Epoch 390, val loss: 0.8105162978172302
Epoch 400, training loss: 0.765131950378418 = 0.13547147810459137 + 0.1 * 6.296604633331299
Epoch 400, val loss: 0.8186250925064087
Epoch 410, training loss: 0.7541487812995911 = 0.12450452148914337 + 0.1 * 6.29644250869751
Epoch 410, val loss: 0.8271297812461853
Epoch 420, training loss: 0.7430897951126099 = 0.11461266875267029 + 0.1 * 6.28477144241333
Epoch 420, val loss: 0.8361941576004028
Epoch 430, training loss: 0.7336764335632324 = 0.1056704968214035 + 0.1 * 6.280059337615967
Epoch 430, val loss: 0.8456079959869385
Epoch 440, training loss: 0.7262489199638367 = 0.09757113456726074 + 0.1 * 6.286777496337891
Epoch 440, val loss: 0.8552829027175903
Epoch 450, training loss: 0.7183551788330078 = 0.09024063497781754 + 0.1 * 6.281145095825195
Epoch 450, val loss: 0.865062952041626
Epoch 460, training loss: 0.7103947997093201 = 0.08358421176671982 + 0.1 * 6.268105983734131
Epoch 460, val loss: 0.8749500513076782
Epoch 470, training loss: 0.7060483694076538 = 0.07751508802175522 + 0.1 * 6.285332679748535
Epoch 470, val loss: 0.8848971128463745
Epoch 480, training loss: 0.6977763772010803 = 0.07200564444065094 + 0.1 * 6.257707118988037
Epoch 480, val loss: 0.8946962952613831
Epoch 490, training loss: 0.6925442218780518 = 0.06698614358901978 + 0.1 * 6.255580902099609
Epoch 490, val loss: 0.9045145511627197
Epoch 500, training loss: 0.6897521615028381 = 0.06239287927746773 + 0.1 * 6.273592472076416
Epoch 500, val loss: 0.9142930507659912
Epoch 510, training loss: 0.6839302778244019 = 0.05820241570472717 + 0.1 * 6.257277965545654
Epoch 510, val loss: 0.9239194393157959
Epoch 520, training loss: 0.6787776350975037 = 0.05437181144952774 + 0.1 * 6.244058132171631
Epoch 520, val loss: 0.9334589242935181
Epoch 530, training loss: 0.6757056713104248 = 0.05085964873433113 + 0.1 * 6.248460292816162
Epoch 530, val loss: 0.942965030670166
Epoch 540, training loss: 0.6719547510147095 = 0.047645799815654755 + 0.1 * 6.243089199066162
Epoch 540, val loss: 0.9523320198059082
Epoch 550, training loss: 0.6681281924247742 = 0.04470594599843025 + 0.1 * 6.234222412109375
Epoch 550, val loss: 0.9616671204566956
Epoch 560, training loss: 0.6662743091583252 = 0.04201143980026245 + 0.1 * 6.242628574371338
Epoch 560, val loss: 0.9709062576293945
Epoch 570, training loss: 0.6627386212348938 = 0.03954392299056053 + 0.1 * 6.23194694519043
Epoch 570, val loss: 0.9801352024078369
Epoch 580, training loss: 0.6597834229469299 = 0.0372793935239315 + 0.1 * 6.225040435791016
Epoch 580, val loss: 0.989193856716156
Epoch 590, training loss: 0.6569303274154663 = 0.0351983979344368 + 0.1 * 6.217319488525391
Epoch 590, val loss: 0.9982813000679016
Epoch 600, training loss: 0.6560216546058655 = 0.033277712762355804 + 0.1 * 6.2274394035339355
Epoch 600, val loss: 1.0072439908981323
Epoch 610, training loss: 0.6549685001373291 = 0.031509146094322205 + 0.1 * 6.234593391418457
Epoch 610, val loss: 1.0159499645233154
Epoch 620, training loss: 0.6513612866401672 = 0.029880188405513763 + 0.1 * 6.214811325073242
Epoch 620, val loss: 1.0246723890304565
Epoch 630, training loss: 0.6497883796691895 = 0.028372814878821373 + 0.1 * 6.214155197143555
Epoch 630, val loss: 1.0332518815994263
Epoch 640, training loss: 0.6476166248321533 = 0.026975717395544052 + 0.1 * 6.206409454345703
Epoch 640, val loss: 1.0417064428329468
Epoch 650, training loss: 0.6460883021354675 = 0.02567788027226925 + 0.1 * 6.204104423522949
Epoch 650, val loss: 1.0499829053878784
Epoch 660, training loss: 0.6456066370010376 = 0.024472564458847046 + 0.1 * 6.211340427398682
Epoch 660, val loss: 1.0581576824188232
Epoch 670, training loss: 0.6446473598480225 = 0.023350665345788002 + 0.1 * 6.2129669189453125
Epoch 670, val loss: 1.066244125366211
Epoch 680, training loss: 0.6426737308502197 = 0.02230747602880001 + 0.1 * 6.203662872314453
Epoch 680, val loss: 1.0741029977798462
Epoch 690, training loss: 0.6410971283912659 = 0.021334683522582054 + 0.1 * 6.197624206542969
Epoch 690, val loss: 1.0819777250289917
Epoch 700, training loss: 0.6404601335525513 = 0.020424263551831245 + 0.1 * 6.200358867645264
Epoch 700, val loss: 1.0896425247192383
Epoch 710, training loss: 0.6390891075134277 = 0.0195713359862566 + 0.1 * 6.19517707824707
Epoch 710, val loss: 1.0971757173538208
Epoch 720, training loss: 0.639200747013092 = 0.018772250041365623 + 0.1 * 6.204285144805908
Epoch 720, val loss: 1.1045838594436646
Epoch 730, training loss: 0.6362701654434204 = 0.01802266389131546 + 0.1 * 6.1824750900268555
Epoch 730, val loss: 1.111791729927063
Epoch 740, training loss: 0.6363717913627625 = 0.01731841079890728 + 0.1 * 6.190533638000488
Epoch 740, val loss: 1.1189820766448975
Epoch 750, training loss: 0.634639322757721 = 0.016655346378684044 + 0.1 * 6.179840087890625
Epoch 750, val loss: 1.1259846687316895
Epoch 760, training loss: 0.6363320350646973 = 0.01603010855615139 + 0.1 * 6.203019142150879
Epoch 760, val loss: 1.1328850984573364
Epoch 770, training loss: 0.6338223814964294 = 0.015442516654729843 + 0.1 * 6.183798789978027
Epoch 770, val loss: 1.1395338773727417
Epoch 780, training loss: 0.6323071122169495 = 0.014888274483382702 + 0.1 * 6.174188137054443
Epoch 780, val loss: 1.1463282108306885
Epoch 790, training loss: 0.6342648267745972 = 0.014363436959683895 + 0.1 * 6.199014186859131
Epoch 790, val loss: 1.152819037437439
Epoch 800, training loss: 0.6321497559547424 = 0.013868157751858234 + 0.1 * 6.182816028594971
Epoch 800, val loss: 1.159226655960083
Epoch 810, training loss: 0.6307342648506165 = 0.01339971274137497 + 0.1 * 6.17334508895874
Epoch 810, val loss: 1.165596842765808
Epoch 820, training loss: 0.6301699876785278 = 0.012954683974385262 + 0.1 * 6.172152519226074
Epoch 820, val loss: 1.1718008518218994
Epoch 830, training loss: 0.6294689178466797 = 0.012532655149698257 + 0.1 * 6.169362545013428
Epoch 830, val loss: 1.1778879165649414
Epoch 840, training loss: 0.6290179491043091 = 0.012131886556744576 + 0.1 * 6.168860912322998
Epoch 840, val loss: 1.1838213205337524
Epoch 850, training loss: 0.6288036704063416 = 0.011752122081816196 + 0.1 * 6.170515537261963
Epoch 850, val loss: 1.1897556781768799
Epoch 860, training loss: 0.6278710961341858 = 0.01139137800782919 + 0.1 * 6.164796829223633
Epoch 860, val loss: 1.1955578327178955
Epoch 870, training loss: 0.6267783045768738 = 0.011047618463635445 + 0.1 * 6.157307147979736
Epoch 870, val loss: 1.201358675956726
Epoch 880, training loss: 0.6278513073921204 = 0.010719474405050278 + 0.1 * 6.171318054199219
Epoch 880, val loss: 1.206971526145935
Epoch 890, training loss: 0.6264124512672424 = 0.010407335124909878 + 0.1 * 6.160051345825195
Epoch 890, val loss: 1.2124710083007812
Epoch 900, training loss: 0.626209557056427 = 0.010110033676028252 + 0.1 * 6.160995006561279
Epoch 900, val loss: 1.2180373668670654
Epoch 910, training loss: 0.6250660419464111 = 0.009825882501900196 + 0.1 * 6.152401447296143
Epoch 910, val loss: 1.2233848571777344
Epoch 920, training loss: 0.6263375878334045 = 0.00955431442707777 + 0.1 * 6.167832374572754
Epoch 920, val loss: 1.2286947965621948
Epoch 930, training loss: 0.6252759099006653 = 0.009295132011175156 + 0.1 * 6.159808158874512
Epoch 930, val loss: 1.2338323593139648
Epoch 940, training loss: 0.6239179372787476 = 0.009047850035130978 + 0.1 * 6.148700714111328
Epoch 940, val loss: 1.239140272140503
Epoch 950, training loss: 0.6241496205329895 = 0.008810524828732014 + 0.1 * 6.153390884399414
Epoch 950, val loss: 1.2442734241485596
Epoch 960, training loss: 0.6226450800895691 = 0.008582846261560917 + 0.1 * 6.140622138977051
Epoch 960, val loss: 1.2491672039031982
Epoch 970, training loss: 0.6230615973472595 = 0.008364837616682053 + 0.1 * 6.14696741104126
Epoch 970, val loss: 1.2541804313659668
Epoch 980, training loss: 0.6245995759963989 = 0.00815522950142622 + 0.1 * 6.164443492889404
Epoch 980, val loss: 1.2589823007583618
Epoch 990, training loss: 0.6220865845680237 = 0.007954790256917477 + 0.1 * 6.141317844390869
Epoch 990, val loss: 1.2636042833328247
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6162
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.789961338043213 = 1.952580213546753 + 0.1 * 8.373809814453125
Epoch 0, val loss: 1.950072169303894
Epoch 10, training loss: 2.7804877758026123 = 1.9431215524673462 + 0.1 * 8.373661994934082
Epoch 10, val loss: 1.9411582946777344
Epoch 20, training loss: 2.768916130065918 = 1.9316354990005493 + 0.1 * 8.372807502746582
Epoch 20, val loss: 1.9298665523529053
Epoch 30, training loss: 2.752174139022827 = 1.9154736995697021 + 0.1 * 8.367003440856934
Epoch 30, val loss: 1.9134246110916138
Epoch 40, training loss: 2.723935842514038 = 1.8911572694778442 + 0.1 * 8.32778549194336
Epoch 40, val loss: 1.8885788917541504
Epoch 50, training loss: 2.6594622135162354 = 1.8572770357131958 + 0.1 * 8.021851539611816
Epoch 50, val loss: 1.8553898334503174
Epoch 60, training loss: 2.5688061714172363 = 1.821463704109192 + 0.1 * 7.473424911499023
Epoch 60, val loss: 1.8227014541625977
Epoch 70, training loss: 2.4960498809814453 = 1.788474440574646 + 0.1 * 7.075753688812256
Epoch 70, val loss: 1.7936134338378906
Epoch 80, training loss: 2.4393603801727295 = 1.756616234779358 + 0.1 * 6.827441692352295
Epoch 80, val loss: 1.7664508819580078
Epoch 90, training loss: 2.390378952026367 = 1.7193434238433838 + 0.1 * 6.710354328155518
Epoch 90, val loss: 1.7334210872650146
Epoch 100, training loss: 2.334868907928467 = 1.6676907539367676 + 0.1 * 6.67178201675415
Epoch 100, val loss: 1.6885861158370972
Epoch 110, training loss: 2.2640888690948486 = 1.5993380546569824 + 0.1 * 6.647508144378662
Epoch 110, val loss: 1.6323764324188232
Epoch 120, training loss: 2.1788299083709717 = 1.5162099599838257 + 0.1 * 6.626198768615723
Epoch 120, val loss: 1.5659984350204468
Epoch 130, training loss: 2.0870814323425293 = 1.4264366626739502 + 0.1 * 6.606447219848633
Epoch 130, val loss: 1.4950156211853027
Epoch 140, training loss: 1.994727611541748 = 1.3360768556594849 + 0.1 * 6.586507797241211
Epoch 140, val loss: 1.4265038967132568
Epoch 150, training loss: 1.9033758640289307 = 1.2465115785598755 + 0.1 * 6.568643093109131
Epoch 150, val loss: 1.3617033958435059
Epoch 160, training loss: 1.814758062362671 = 1.1593915224075317 + 0.1 * 6.5536651611328125
Epoch 160, val loss: 1.3002427816390991
Epoch 170, training loss: 1.7298685312271118 = 1.076064944267273 + 0.1 * 6.538035869598389
Epoch 170, val loss: 1.2418006658554077
Epoch 180, training loss: 1.6481151580810547 = 0.9954638481140137 + 0.1 * 6.526512145996094
Epoch 180, val loss: 1.1845296621322632
Epoch 190, training loss: 1.5701237916946411 = 0.9178067445755005 + 0.1 * 6.523170471191406
Epoch 190, val loss: 1.1284819841384888
Epoch 200, training loss: 1.4951298236846924 = 0.8442050218582153 + 0.1 * 6.509247303009033
Epoch 200, val loss: 1.0748214721679688
Epoch 210, training loss: 1.42288076877594 = 0.7727357745170593 + 0.1 * 6.501450061798096
Epoch 210, val loss: 1.0218232870101929
Epoch 220, training loss: 1.3516899347305298 = 0.7025373578071594 + 0.1 * 6.491525650024414
Epoch 220, val loss: 0.9691305756568909
Epoch 230, training loss: 1.2815299034118652 = 0.633145809173584 + 0.1 * 6.483841419219971
Epoch 230, val loss: 0.9169067144393921
Epoch 240, training loss: 1.213303565979004 = 0.565585196018219 + 0.1 * 6.477184295654297
Epoch 240, val loss: 0.8663949370384216
Epoch 250, training loss: 1.1499593257904053 = 0.5013775825500488 + 0.1 * 6.485817909240723
Epoch 250, val loss: 0.8193074464797974
Epoch 260, training loss: 1.0885610580444336 = 0.44289520382881165 + 0.1 * 6.456658840179443
Epoch 260, val loss: 0.7782651782035828
Epoch 270, training loss: 1.0345590114593506 = 0.3900357484817505 + 0.1 * 6.44523286819458
Epoch 270, val loss: 0.7434282898902893
Epoch 280, training loss: 0.9865319728851318 = 0.34249791502952576 + 0.1 * 6.440340518951416
Epoch 280, val loss: 0.7148537635803223
Epoch 290, training loss: 0.944098711013794 = 0.30074068903923035 + 0.1 * 6.433579921722412
Epoch 290, val loss: 0.6923488974571228
Epoch 300, training loss: 0.9063471555709839 = 0.2642659544944763 + 0.1 * 6.420812129974365
Epoch 300, val loss: 0.675111711025238
Epoch 310, training loss: 0.8745246529579163 = 0.23241285979747772 + 0.1 * 6.421117782592773
Epoch 310, val loss: 0.6622630953788757
Epoch 320, training loss: 0.8452440500259399 = 0.20509865880012512 + 0.1 * 6.401453495025635
Epoch 320, val loss: 0.6533440351486206
Epoch 330, training loss: 0.8224591016769409 = 0.18177573382854462 + 0.1 * 6.406833648681641
Epoch 330, val loss: 0.6478370428085327
Epoch 340, training loss: 0.8010357618331909 = 0.16206462681293488 + 0.1 * 6.389711380004883
Epoch 340, val loss: 0.6451635360717773
Epoch 350, training loss: 0.783355712890625 = 0.14528360962867737 + 0.1 * 6.380720615386963
Epoch 350, val loss: 0.6445903182029724
Epoch 360, training loss: 0.7696579694747925 = 0.130866140127182 + 0.1 * 6.387917995452881
Epoch 360, val loss: 0.6456254124641418
Epoch 370, training loss: 0.7547866106033325 = 0.1184934675693512 + 0.1 * 6.362931251525879
Epoch 370, val loss: 0.6480293869972229
Epoch 380, training loss: 0.7430101037025452 = 0.10768965631723404 + 0.1 * 6.353204250335693
Epoch 380, val loss: 0.6512870788574219
Epoch 390, training loss: 0.7337084412574768 = 0.0981665775179863 + 0.1 * 6.355418682098389
Epoch 390, val loss: 0.6553874611854553
Epoch 400, training loss: 0.7256889343261719 = 0.08976128697395325 + 0.1 * 6.359275817871094
Epoch 400, val loss: 0.6599878072738647
Epoch 410, training loss: 0.7162026762962341 = 0.08232103288173676 + 0.1 * 6.338816165924072
Epoch 410, val loss: 0.6651010513305664
Epoch 420, training loss: 0.7086206674575806 = 0.07567188888788223 + 0.1 * 6.329487323760986
Epoch 420, val loss: 0.670589804649353
Epoch 430, training loss: 0.7017245292663574 = 0.06971929967403412 + 0.1 * 6.320052146911621
Epoch 430, val loss: 0.6763787269592285
Epoch 440, training loss: 0.6972730159759521 = 0.06438162922859192 + 0.1 * 6.328913688659668
Epoch 440, val loss: 0.6824651956558228
Epoch 450, training loss: 0.6903601884841919 = 0.05957908183336258 + 0.1 * 6.307811260223389
Epoch 450, val loss: 0.6887586712837219
Epoch 460, training loss: 0.6872771978378296 = 0.05523965135216713 + 0.1 * 6.320374965667725
Epoch 460, val loss: 0.6952340602874756
Epoch 470, training loss: 0.6815850734710693 = 0.051329828798770905 + 0.1 * 6.302552223205566
Epoch 470, val loss: 0.7018371224403381
Epoch 480, training loss: 0.6770643591880798 = 0.047794755548238754 + 0.1 * 6.292695999145508
Epoch 480, val loss: 0.7086198329925537
Epoch 490, training loss: 0.6728382706642151 = 0.04458651319146156 + 0.1 * 6.282516956329346
Epoch 490, val loss: 0.7153511047363281
Epoch 500, training loss: 0.6699541807174683 = 0.04168269410729408 + 0.1 * 6.282714366912842
Epoch 500, val loss: 0.7222809195518494
Epoch 510, training loss: 0.667096734046936 = 0.03903316333889961 + 0.1 * 6.280635356903076
Epoch 510, val loss: 0.729123592376709
Epoch 520, training loss: 0.6653207540512085 = 0.036620743572711945 + 0.1 * 6.2870001792907715
Epoch 520, val loss: 0.7359387278556824
Epoch 530, training loss: 0.6617534160614014 = 0.03442683070898056 + 0.1 * 6.273265838623047
Epoch 530, val loss: 0.742929220199585
Epoch 540, training loss: 0.6589178442955017 = 0.03242133557796478 + 0.1 * 6.264965057373047
Epoch 540, val loss: 0.7495649456977844
Epoch 550, training loss: 0.6565436720848083 = 0.03058796189725399 + 0.1 * 6.259556770324707
Epoch 550, val loss: 0.7563320994377136
Epoch 560, training loss: 0.6542308926582336 = 0.028900524601340294 + 0.1 * 6.2533040046691895
Epoch 560, val loss: 0.7629263401031494
Epoch 570, training loss: 0.6533687710762024 = 0.027344491332769394 + 0.1 * 6.260242938995361
Epoch 570, val loss: 0.7694573402404785
Epoch 580, training loss: 0.652228832244873 = 0.02591022476553917 + 0.1 * 6.263186454772949
Epoch 580, val loss: 0.7759233713150024
Epoch 590, training loss: 0.6487482786178589 = 0.024592168629169464 + 0.1 * 6.241560935974121
Epoch 590, val loss: 0.7822968363761902
Epoch 600, training loss: 0.6479589343070984 = 0.023372730240225792 + 0.1 * 6.245862007141113
Epoch 600, val loss: 0.7885918021202087
Epoch 610, training loss: 0.6462143063545227 = 0.022242672741413116 + 0.1 * 6.239716529846191
Epoch 610, val loss: 0.794556200504303
Epoch 620, training loss: 0.6445415019989014 = 0.02119755931198597 + 0.1 * 6.2334394454956055
Epoch 620, val loss: 0.8006452322006226
Epoch 630, training loss: 0.6433600187301636 = 0.020224537700414658 + 0.1 * 6.231354713439941
Epoch 630, val loss: 0.806451141834259
Epoch 640, training loss: 0.6429647207260132 = 0.019320646300911903 + 0.1 * 6.236440181732178
Epoch 640, val loss: 0.8123214244842529
Epoch 650, training loss: 0.6409878730773926 = 0.01847809925675392 + 0.1 * 6.22509765625
Epoch 650, val loss: 0.8179388046264648
Epoch 660, training loss: 0.6404471397399902 = 0.01769224926829338 + 0.1 * 6.227548599243164
Epoch 660, val loss: 0.8235335350036621
Epoch 670, training loss: 0.6410840749740601 = 0.01695692539215088 + 0.1 * 6.241271495819092
Epoch 670, val loss: 0.8290472030639648
Epoch 680, training loss: 0.6385628581047058 = 0.016269292682409286 + 0.1 * 6.222935676574707
Epoch 680, val loss: 0.8344099521636963
Epoch 690, training loss: 0.6372339129447937 = 0.01562558487057686 + 0.1 * 6.21608304977417
Epoch 690, val loss: 0.8397554159164429
Epoch 700, training loss: 0.6370677947998047 = 0.015018777921795845 + 0.1 * 6.220490455627441
Epoch 700, val loss: 0.8448567986488342
Epoch 710, training loss: 0.6361921429634094 = 0.014449238777160645 + 0.1 * 6.217428684234619
Epoch 710, val loss: 0.8499259948730469
Epoch 720, training loss: 0.6348277926445007 = 0.013915165327489376 + 0.1 * 6.209125995635986
Epoch 720, val loss: 0.8549610376358032
Epoch 730, training loss: 0.6352688670158386 = 0.013410144485533237 + 0.1 * 6.2185869216918945
Epoch 730, val loss: 0.8598690629005432
Epoch 740, training loss: 0.6332707405090332 = 0.01293348241597414 + 0.1 * 6.203372478485107
Epoch 740, val loss: 0.8645936250686646
Epoch 750, training loss: 0.6322889924049377 = 0.012484444305300713 + 0.1 * 6.198045253753662
Epoch 750, val loss: 0.8694100975990295
Epoch 760, training loss: 0.633057177066803 = 0.012058124877512455 + 0.1 * 6.209990501403809
Epoch 760, val loss: 0.8739709854125977
Epoch 770, training loss: 0.6313678622245789 = 0.011655065231025219 + 0.1 * 6.197127819061279
Epoch 770, val loss: 0.8785250186920166
Epoch 780, training loss: 0.6315725445747375 = 0.011273670941591263 + 0.1 * 6.202988624572754
Epoch 780, val loss: 0.8830122351646423
Epoch 790, training loss: 0.6302995085716248 = 0.010911446996033192 + 0.1 * 6.193880081176758
Epoch 790, val loss: 0.8874085545539856
Epoch 800, training loss: 0.6303749680519104 = 0.010568328201770782 + 0.1 * 6.198066234588623
Epoch 800, val loss: 0.8917567729949951
Epoch 810, training loss: 0.6295359134674072 = 0.010241920128464699 + 0.1 * 6.192939758300781
Epoch 810, val loss: 0.8959569931030273
Epoch 820, training loss: 0.6283122301101685 = 0.009931798093020916 + 0.1 * 6.183804035186768
Epoch 820, val loss: 0.9001790881156921
Epoch 830, training loss: 0.6290088295936584 = 0.009635825641453266 + 0.1 * 6.193729877471924
Epoch 830, val loss: 0.9043048620223999
Epoch 840, training loss: 0.6284974813461304 = 0.009353586472570896 + 0.1 * 6.191438674926758
Epoch 840, val loss: 0.9082522988319397
Epoch 850, training loss: 0.6271178126335144 = 0.009085766971111298 + 0.1 * 6.1803202629089355
Epoch 850, val loss: 0.9122576117515564
Epoch 860, training loss: 0.6271395683288574 = 0.008829758502542973 + 0.1 * 6.183097839355469
Epoch 860, val loss: 0.916114091873169
Epoch 870, training loss: 0.6260234117507935 = 0.008586268872022629 + 0.1 * 6.174371242523193
Epoch 870, val loss: 0.9199586510658264
Epoch 880, training loss: 0.6257777810096741 = 0.008353808894753456 + 0.1 * 6.174239635467529
Epoch 880, val loss: 0.9238086938858032
Epoch 890, training loss: 0.625529944896698 = 0.008130332455039024 + 0.1 * 6.1739959716796875
Epoch 890, val loss: 0.927472710609436
Epoch 900, training loss: 0.6254475116729736 = 0.007916845381259918 + 0.1 * 6.17530632019043
Epoch 900, val loss: 0.9311384558677673
Epoch 910, training loss: 0.6250869631767273 = 0.007712308317422867 + 0.1 * 6.173746585845947
Epoch 910, val loss: 0.9347478151321411
Epoch 920, training loss: 0.6244674324989319 = 0.00751663139089942 + 0.1 * 6.16950798034668
Epoch 920, val loss: 0.9383549094200134
Epoch 930, training loss: 0.6243848204612732 = 0.007328941021114588 + 0.1 * 6.170558929443359
Epoch 930, val loss: 0.9419148564338684
Epoch 940, training loss: 0.6238505840301514 = 0.0071480125188827515 + 0.1 * 6.167025566101074
Epoch 940, val loss: 0.9452743530273438
Epoch 950, training loss: 0.6245276927947998 = 0.006975714582949877 + 0.1 * 6.175519943237305
Epoch 950, val loss: 0.9486897587776184
Epoch 960, training loss: 0.6226991415023804 = 0.006810347083956003 + 0.1 * 6.15888786315918
Epoch 960, val loss: 0.9520973563194275
Epoch 970, training loss: 0.6235154271125793 = 0.0066513665951788425 + 0.1 * 6.168640613555908
Epoch 970, val loss: 0.9553860425949097
Epoch 980, training loss: 0.6230151057243347 = 0.006498112343251705 + 0.1 * 6.165169715881348
Epoch 980, val loss: 0.9585379362106323
Epoch 990, training loss: 0.6221417784690857 = 0.006351122632622719 + 0.1 * 6.157906532287598
Epoch 990, val loss: 0.9618263244628906
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9225
Flip ASR: 0.9067/225 nodes
The final ASR:0.71956, 0.14352, Accuracy:0.82346, 0.01397
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10568])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.83457, 0.00924
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.769552707672119 = 1.932176947593689 + 0.1 * 8.373757362365723
Epoch 0, val loss: 1.925439715385437
Epoch 10, training loss: 2.7600951194763184 = 1.9227498769760132 + 0.1 * 8.373451232910156
Epoch 10, val loss: 1.9162663221359253
Epoch 20, training loss: 2.7480688095092773 = 1.910894513130188 + 0.1 * 8.371744155883789
Epoch 20, val loss: 1.9048278331756592
Epoch 30, training loss: 2.7299742698669434 = 1.89398193359375 + 0.1 * 8.359922409057617
Epoch 30, val loss: 1.8888667821884155
Epoch 40, training loss: 2.697355270385742 = 1.8689123392105103 + 0.1 * 8.284429550170898
Epoch 40, val loss: 1.865739345550537
Epoch 50, training loss: 2.619493007659912 = 1.8356592655181885 + 0.1 * 7.838338375091553
Epoch 50, val loss: 1.8357511758804321
Epoch 60, training loss: 2.5446064472198486 = 1.7978477478027344 + 0.1 * 7.467587471008301
Epoch 60, val loss: 1.8033479452133179
Epoch 70, training loss: 2.469205379486084 = 1.758573293685913 + 0.1 * 7.106321334838867
Epoch 70, val loss: 1.7715638875961304
Epoch 80, training loss: 2.407245635986328 = 1.7168056964874268 + 0.1 * 6.904399394989014
Epoch 80, val loss: 1.7376501560211182
Epoch 90, training loss: 2.341860771179199 = 1.6634315252304077 + 0.1 * 6.784292697906494
Epoch 90, val loss: 1.6900182962417603
Epoch 100, training loss: 2.265939235687256 = 1.594567894935608 + 0.1 * 6.713712215423584
Epoch 100, val loss: 1.6304856538772583
Epoch 110, training loss: 2.17850399017334 = 1.5121413469314575 + 0.1 * 6.663625717163086
Epoch 110, val loss: 1.5625379085540771
Epoch 120, training loss: 2.085379123687744 = 1.4221214056015015 + 0.1 * 6.632575988769531
Epoch 120, val loss: 1.4888938665390015
Epoch 130, training loss: 1.9913092851638794 = 1.329950213432312 + 0.1 * 6.613590717315674
Epoch 130, val loss: 1.4167612791061401
Epoch 140, training loss: 1.8980565071105957 = 1.238391637802124 + 0.1 * 6.5966477394104
Epoch 140, val loss: 1.3477495908737183
Epoch 150, training loss: 1.8076345920562744 = 1.1495733261108398 + 0.1 * 6.580611705780029
Epoch 150, val loss: 1.2827309370040894
Epoch 160, training loss: 1.723111867904663 = 1.0664576292037964 + 0.1 * 6.566542625427246
Epoch 160, val loss: 1.2234424352645874
Epoch 170, training loss: 1.6445971727371216 = 0.9892050623893738 + 0.1 * 6.553921222686768
Epoch 170, val loss: 1.1694809198379517
Epoch 180, training loss: 1.5708088874816895 = 0.9163864254951477 + 0.1 * 6.544223785400391
Epoch 180, val loss: 1.1195579767227173
Epoch 190, training loss: 1.4998559951782227 = 0.846691370010376 + 0.1 * 6.531646728515625
Epoch 190, val loss: 1.072740912437439
Epoch 200, training loss: 1.4304873943328857 = 0.7782425880432129 + 0.1 * 6.522448539733887
Epoch 200, val loss: 1.0272399187088013
Epoch 210, training loss: 1.3624930381774902 = 0.7108548283576965 + 0.1 * 6.516381740570068
Epoch 210, val loss: 0.9830767512321472
Epoch 220, training loss: 1.2957055568695068 = 0.6447057127952576 + 0.1 * 6.509997844696045
Epoch 220, val loss: 0.9406504034996033
Epoch 230, training loss: 1.2307517528533936 = 0.5802321434020996 + 0.1 * 6.5051960945129395
Epoch 230, val loss: 0.900740921497345
Epoch 240, training loss: 1.1685383319854736 = 0.5186581015586853 + 0.1 * 6.498802661895752
Epoch 240, val loss: 0.8646003007888794
Epoch 250, training loss: 1.1095750331878662 = 0.46036437153816223 + 0.1 * 6.492105960845947
Epoch 250, val loss: 0.8326951861381531
Epoch 260, training loss: 1.054844856262207 = 0.40631523728370667 + 0.1 * 6.48529577255249
Epoch 260, val loss: 0.8057276010513306
Epoch 270, training loss: 1.0075150728225708 = 0.3575912415981293 + 0.1 * 6.49923849105835
Epoch 270, val loss: 0.7840985059738159
Epoch 280, training loss: 0.9627497792243958 = 0.31505292654037476 + 0.1 * 6.476968288421631
Epoch 280, val loss: 0.7683096528053284
Epoch 290, training loss: 0.9243426322937012 = 0.27773118019104004 + 0.1 * 6.466114521026611
Epoch 290, val loss: 0.757540762424469
Epoch 300, training loss: 0.8908882141113281 = 0.2448764592409134 + 0.1 * 6.460117816925049
Epoch 300, val loss: 0.7512134313583374
Epoch 310, training loss: 0.8618778586387634 = 0.2160942405462265 + 0.1 * 6.457835674285889
Epoch 310, val loss: 0.7487323880195618
Epoch 320, training loss: 0.8362217545509338 = 0.19106614589691162 + 0.1 * 6.451556205749512
Epoch 320, val loss: 0.7496682405471802
Epoch 330, training loss: 0.8136022090911865 = 0.16921167075634003 + 0.1 * 6.443905353546143
Epoch 330, val loss: 0.7535881996154785
Epoch 340, training loss: 0.7942230701446533 = 0.1503334790468216 + 0.1 * 6.438896179199219
Epoch 340, val loss: 0.7598806619644165
Epoch 350, training loss: 0.7767224311828613 = 0.13407886028289795 + 0.1 * 6.426435470581055
Epoch 350, val loss: 0.7679470181465149
Epoch 360, training loss: 0.7619857788085938 = 0.1200237050652504 + 0.1 * 6.419620513916016
Epoch 360, val loss: 0.7775561809539795
Epoch 370, training loss: 0.7502053380012512 = 0.10786564648151398 + 0.1 * 6.423397064208984
Epoch 370, val loss: 0.7881681323051453
Epoch 380, training loss: 0.7381221055984497 = 0.09733860194683075 + 0.1 * 6.407835006713867
Epoch 380, val loss: 0.7994385957717896
Epoch 390, training loss: 0.7284154891967773 = 0.08813778311014175 + 0.1 * 6.402776718139648
Epoch 390, val loss: 0.8114268183708191
Epoch 400, training loss: 0.7200829386711121 = 0.08010997623205185 + 0.1 * 6.3997297286987305
Epoch 400, val loss: 0.8234997987747192
Epoch 410, training loss: 0.7116745710372925 = 0.07307799905538559 + 0.1 * 6.385965824127197
Epoch 410, val loss: 0.8356621265411377
Epoch 420, training loss: 0.7040495872497559 = 0.06685461848974228 + 0.1 * 6.371949672698975
Epoch 420, val loss: 0.8480328917503357
Epoch 430, training loss: 0.6982172131538391 = 0.061339255422353745 + 0.1 * 6.36877965927124
Epoch 430, val loss: 0.8601820468902588
Epoch 440, training loss: 0.6928191184997559 = 0.0564696229994297 + 0.1 * 6.363494873046875
Epoch 440, val loss: 0.8724578619003296
Epoch 450, training loss: 0.6875057220458984 = 0.052116759121418 + 0.1 * 6.353889465332031
Epoch 450, val loss: 0.8844776153564453
Epoch 460, training loss: 0.6835231184959412 = 0.048221420496702194 + 0.1 * 6.3530168533325195
Epoch 460, val loss: 0.8964833617210388
Epoch 470, training loss: 0.6782282590866089 = 0.04472862929105759 + 0.1 * 6.334996223449707
Epoch 470, val loss: 0.9083590507507324
Epoch 480, training loss: 0.6766961812973022 = 0.041579991579055786 + 0.1 * 6.351161479949951
Epoch 480, val loss: 0.9201383590698242
Epoch 490, training loss: 0.6714885830879211 = 0.038747649639844894 + 0.1 * 6.327408790588379
Epoch 490, val loss: 0.9316653609275818
Epoch 500, training loss: 0.6681758165359497 = 0.03618626669049263 + 0.1 * 6.319895267486572
Epoch 500, val loss: 0.9429471492767334
Epoch 510, training loss: 0.6652742624282837 = 0.033866021782159805 + 0.1 * 6.314082145690918
Epoch 510, val loss: 0.9538989067077637
Epoch 520, training loss: 0.6627019047737122 = 0.031774457544088364 + 0.1 * 6.309274196624756
Epoch 520, val loss: 0.9648340344429016
Epoch 530, training loss: 0.6601109504699707 = 0.02986590377986431 + 0.1 * 6.302450180053711
Epoch 530, val loss: 0.9752627015113831
Epoch 540, training loss: 0.6578615307807922 = 0.028118597343564034 + 0.1 * 6.297429084777832
Epoch 540, val loss: 0.9857312440872192
Epoch 550, training loss: 0.6562946438789368 = 0.026516202837228775 + 0.1 * 6.297784328460693
Epoch 550, val loss: 0.9957442879676819
Epoch 560, training loss: 0.6537752747535706 = 0.02505365200340748 + 0.1 * 6.287215709686279
Epoch 560, val loss: 1.0056051015853882
Epoch 570, training loss: 0.6516953706741333 = 0.023713406175374985 + 0.1 * 6.279819488525391
Epoch 570, val loss: 1.0152860879898071
Epoch 580, training loss: 0.6505565643310547 = 0.02247503772377968 + 0.1 * 6.2808146476745605
Epoch 580, val loss: 1.0245949029922485
Epoch 590, training loss: 0.6486318707466125 = 0.0213306974619627 + 0.1 * 6.273011684417725
Epoch 590, val loss: 1.0336698293685913
Epoch 600, training loss: 0.6467750072479248 = 0.020274953916668892 + 0.1 * 6.265000820159912
Epoch 600, val loss: 1.042691946029663
Epoch 610, training loss: 0.6477027535438538 = 0.019295012578368187 + 0.1 * 6.284077167510986
Epoch 610, val loss: 1.0513046979904175
Epoch 620, training loss: 0.6456895470619202 = 0.0183881763368845 + 0.1 * 6.273014068603516
Epoch 620, val loss: 1.0596610307693481
Epoch 630, training loss: 0.6432421207427979 = 0.01755053736269474 + 0.1 * 6.25691556930542
Epoch 630, val loss: 1.0680387020111084
Epoch 640, training loss: 0.6422998309135437 = 0.016767337918281555 + 0.1 * 6.255324840545654
Epoch 640, val loss: 1.0759520530700684
Epoch 650, training loss: 0.6422256231307983 = 0.01603800430893898 + 0.1 * 6.261876106262207
Epoch 650, val loss: 1.0836366415023804
Epoch 660, training loss: 0.639896810054779 = 0.015362124890089035 + 0.1 * 6.245346546173096
Epoch 660, val loss: 1.0914424657821655
Epoch 670, training loss: 0.6388934254646301 = 0.014727896079421043 + 0.1 * 6.241655349731445
Epoch 670, val loss: 1.0988434553146362
Epoch 680, training loss: 0.6393171548843384 = 0.014132391661405563 + 0.1 * 6.251847743988037
Epoch 680, val loss: 1.1060948371887207
Epoch 690, training loss: 0.6378324627876282 = 0.013573507778346539 + 0.1 * 6.242589473724365
Epoch 690, val loss: 1.113054633140564
Epoch 700, training loss: 0.6369354724884033 = 0.013050605542957783 + 0.1 * 6.238848686218262
Epoch 700, val loss: 1.1201486587524414
Epoch 710, training loss: 0.6357691287994385 = 0.012557354755699635 + 0.1 * 6.232118129730225
Epoch 710, val loss: 1.1268837451934814
Epoch 720, training loss: 0.6351717710494995 = 0.012094350531697273 + 0.1 * 6.23077392578125
Epoch 720, val loss: 1.1336281299591064
Epoch 730, training loss: 0.6348648071289062 = 0.01165633462369442 + 0.1 * 6.232084274291992
Epoch 730, val loss: 1.1398046016693115
Epoch 740, training loss: 0.6335439682006836 = 0.011246277950704098 + 0.1 * 6.222977161407471
Epoch 740, val loss: 1.1463875770568848
Epoch 750, training loss: 0.6344847679138184 = 0.010858091525733471 + 0.1 * 6.236266613006592
Epoch 750, val loss: 1.1525819301605225
Epoch 760, training loss: 0.6326709985733032 = 0.010490454733371735 + 0.1 * 6.221805095672607
Epoch 760, val loss: 1.158492922782898
Epoch 770, training loss: 0.6315760016441345 = 0.010144365951418877 + 0.1 * 6.214316368103027
Epoch 770, val loss: 1.1646074056625366
Epoch 780, training loss: 0.6318811178207397 = 0.009814456105232239 + 0.1 * 6.220666885375977
Epoch 780, val loss: 1.1703070402145386
Epoch 790, training loss: 0.6305897235870361 = 0.009501286782324314 + 0.1 * 6.210884094238281
Epoch 790, val loss: 1.1758590936660767
Epoch 800, training loss: 0.6301520466804504 = 0.009206188842654228 + 0.1 * 6.209458351135254
Epoch 800, val loss: 1.18155038356781
Epoch 810, training loss: 0.631478488445282 = 0.008924228139221668 + 0.1 * 6.2255425453186035
Epoch 810, val loss: 1.1868466138839722
Epoch 820, training loss: 0.6292976140975952 = 0.008656593039631844 + 0.1 * 6.206409931182861
Epoch 820, val loss: 1.192097544670105
Epoch 830, training loss: 0.6286425590515137 = 0.008403844200074673 + 0.1 * 6.20238733291626
Epoch 830, val loss: 1.1975411176681519
Epoch 840, training loss: 0.629620373249054 = 0.008161406964063644 + 0.1 * 6.214589595794678
Epoch 840, val loss: 1.2025529146194458
Epoch 850, training loss: 0.6291908025741577 = 0.00792958028614521 + 0.1 * 6.212612152099609
Epoch 850, val loss: 1.207403540611267
Epoch 860, training loss: 0.6277621984481812 = 0.0077102831564843655 + 0.1 * 6.20051908493042
Epoch 860, val loss: 1.212471604347229
Epoch 870, training loss: 0.6275542378425598 = 0.007500221487134695 + 0.1 * 6.200540065765381
Epoch 870, val loss: 1.217267394065857
Epoch 880, training loss: 0.6273260116577148 = 0.007299268618226051 + 0.1 * 6.200267314910889
Epoch 880, val loss: 1.22197425365448
Epoch 890, training loss: 0.6263414621353149 = 0.007107507903128862 + 0.1 * 6.1923394203186035
Epoch 890, val loss: 1.2265812158584595
Epoch 900, training loss: 0.6262822151184082 = 0.006924363784492016 + 0.1 * 6.193578243255615
Epoch 900, val loss: 1.2312043905258179
Epoch 910, training loss: 0.6263242959976196 = 0.006748415995389223 + 0.1 * 6.19575834274292
Epoch 910, val loss: 1.2356687784194946
Epoch 920, training loss: 0.6263077259063721 = 0.00658016512170434 + 0.1 * 6.197275638580322
Epoch 920, val loss: 1.240079402923584
Epoch 930, training loss: 0.6249586343765259 = 0.0064186775125563145 + 0.1 * 6.185399055480957
Epoch 930, val loss: 1.2443662881851196
Epoch 940, training loss: 0.6248528957366943 = 0.006264753174036741 + 0.1 * 6.1858811378479
Epoch 940, val loss: 1.2487709522247314
Epoch 950, training loss: 0.6249405145645142 = 0.006115851923823357 + 0.1 * 6.188246726989746
Epoch 950, val loss: 1.2528841495513916
Epoch 960, training loss: 0.624424934387207 = 0.005973394028842449 + 0.1 * 6.184514999389648
Epoch 960, val loss: 1.2570312023162842
Epoch 970, training loss: 0.6239230036735535 = 0.005836803000420332 + 0.1 * 6.180861949920654
Epoch 970, val loss: 1.2611933946609497
Epoch 980, training loss: 0.6231726408004761 = 0.005704953800886869 + 0.1 * 6.174676895141602
Epoch 980, val loss: 1.265125036239624
Epoch 990, training loss: 0.6238706111907959 = 0.005578208714723587 + 0.1 * 6.182923793792725
Epoch 990, val loss: 1.2688406705856323
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5387
Flip ASR: 0.4578/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.765894889831543 = 1.9285262823104858 + 0.1 * 8.373684883117676
Epoch 0, val loss: 1.9262880086898804
Epoch 10, training loss: 2.755559206008911 = 1.918288230895996 + 0.1 * 8.372709274291992
Epoch 10, val loss: 1.9143391847610474
Epoch 20, training loss: 2.742680788040161 = 1.9057903289794922 + 0.1 * 8.368904113769531
Epoch 20, val loss: 1.899006962776184
Epoch 30, training loss: 2.724256992340088 = 1.8886280059814453 + 0.1 * 8.35628890991211
Epoch 30, val loss: 1.8774877786636353
Epoch 40, training loss: 2.6915903091430664 = 1.864883303642273 + 0.1 * 8.267070770263672
Epoch 40, val loss: 1.8484954833984375
Epoch 50, training loss: 2.614497423171997 = 1.8358479738235474 + 0.1 * 7.78649377822876
Epoch 50, val loss: 1.8161567449569702
Epoch 60, training loss: 2.5396552085876465 = 1.806527853012085 + 0.1 * 7.331273078918457
Epoch 60, val loss: 1.786892056465149
Epoch 70, training loss: 2.470806837081909 = 1.7752480506896973 + 0.1 * 6.955586910247803
Epoch 70, val loss: 1.758121371269226
Epoch 80, training loss: 2.418532371520996 = 1.7407387495040894 + 0.1 * 6.777935028076172
Epoch 80, val loss: 1.7285449504852295
Epoch 90, training loss: 2.367210865020752 = 1.69997239112854 + 0.1 * 6.672384738922119
Epoch 90, val loss: 1.693129539489746
Epoch 100, training loss: 2.3037102222442627 = 1.643499493598938 + 0.1 * 6.602107048034668
Epoch 100, val loss: 1.6431488990783691
Epoch 110, training loss: 2.2231905460357666 = 1.567469835281372 + 0.1 * 6.557207107543945
Epoch 110, val loss: 1.5797746181488037
Epoch 120, training loss: 2.1266226768493652 = 1.4743471145629883 + 0.1 * 6.522756099700928
Epoch 120, val loss: 1.5039020776748657
Epoch 130, training loss: 2.0239782333374023 = 1.3735367059707642 + 0.1 * 6.504415512084961
Epoch 130, val loss: 1.4223695993423462
Epoch 140, training loss: 1.924403429031372 = 1.2765690088272095 + 0.1 * 6.478344917297363
Epoch 140, val loss: 1.3467519283294678
Epoch 150, training loss: 1.8325002193450928 = 1.1867270469665527 + 0.1 * 6.457731246948242
Epoch 150, val loss: 1.2790782451629639
Epoch 160, training loss: 1.7505483627319336 = 1.1062172651290894 + 0.1 * 6.4433112144470215
Epoch 160, val loss: 1.2206498384475708
Epoch 170, training loss: 1.675504446029663 = 1.0331428050994873 + 0.1 * 6.4236159324646
Epoch 170, val loss: 1.168768048286438
Epoch 180, training loss: 1.6072430610656738 = 0.9648709297180176 + 0.1 * 6.423720836639404
Epoch 180, val loss: 1.1214388608932495
Epoch 190, training loss: 1.5416573286056519 = 0.9012848138809204 + 0.1 * 6.4037251472473145
Epoch 190, val loss: 1.077530860900879
Epoch 200, training loss: 1.4785140752792358 = 0.8392894268035889 + 0.1 * 6.392246246337891
Epoch 200, val loss: 1.034700870513916
Epoch 210, training loss: 1.41511869430542 = 0.7767390012741089 + 0.1 * 6.383796215057373
Epoch 210, val loss: 0.9917052388191223
Epoch 220, training loss: 1.352388858795166 = 0.7143857479095459 + 0.1 * 6.380031585693359
Epoch 220, val loss: 0.9490863680839539
Epoch 230, training loss: 1.2907049655914307 = 0.6537250876426697 + 0.1 * 6.369798183441162
Epoch 230, val loss: 0.9082687497138977
Epoch 240, training loss: 1.2325716018676758 = 0.596319854259491 + 0.1 * 6.3625168800354
Epoch 240, val loss: 0.8716528415679932
Epoch 250, training loss: 1.1800272464752197 = 0.5441015362739563 + 0.1 * 6.359256267547607
Epoch 250, val loss: 0.8407369256019592
Epoch 260, training loss: 1.1327452659606934 = 0.49756094813346863 + 0.1 * 6.351842880249023
Epoch 260, val loss: 0.8168520927429199
Epoch 270, training loss: 1.0907788276672363 = 0.4562283158302307 + 0.1 * 6.345505714416504
Epoch 270, val loss: 0.798745334148407
Epoch 280, training loss: 1.0539846420288086 = 0.4191649854183197 + 0.1 * 6.348196506500244
Epoch 280, val loss: 0.7860646843910217
Epoch 290, training loss: 1.019567847251892 = 0.38566896319389343 + 0.1 * 6.338988304138184
Epoch 290, val loss: 0.7771459221839905
Epoch 300, training loss: 0.9875489473342896 = 0.35496246814727783 + 0.1 * 6.325864791870117
Epoch 300, val loss: 0.7712181806564331
Epoch 310, training loss: 0.959017276763916 = 0.32659393548965454 + 0.1 * 6.324233055114746
Epoch 310, val loss: 0.7676870822906494
Epoch 320, training loss: 0.9316291213035583 = 0.3003411293029785 + 0.1 * 6.312880039215088
Epoch 320, val loss: 0.7660250663757324
Epoch 330, training loss: 0.9087238311767578 = 0.2756844460964203 + 0.1 * 6.330393314361572
Epoch 330, val loss: 0.766391932964325
Epoch 340, training loss: 0.8833000659942627 = 0.2526300549507141 + 0.1 * 6.306699752807617
Epoch 340, val loss: 0.7681843042373657
Epoch 350, training loss: 0.8612591624259949 = 0.2310631424188614 + 0.1 * 6.301959991455078
Epoch 350, val loss: 0.7716557383537292
Epoch 360, training loss: 0.8415071368217468 = 0.21104203164577484 + 0.1 * 6.304650783538818
Epoch 360, val loss: 0.7763557434082031
Epoch 370, training loss: 0.8229085206985474 = 0.19262507557868958 + 0.1 * 6.3028340339660645
Epoch 370, val loss: 0.7821969985961914
Epoch 380, training loss: 0.8047574162483215 = 0.17586742341518402 + 0.1 * 6.2888994216918945
Epoch 380, val loss: 0.7891799807548523
Epoch 390, training loss: 0.7902454137802124 = 0.16064587235450745 + 0.1 * 6.295995235443115
Epoch 390, val loss: 0.796988308429718
Epoch 400, training loss: 0.7756215333938599 = 0.1468466967344284 + 0.1 * 6.287747859954834
Epoch 400, val loss: 0.8053906559944153
Epoch 410, training loss: 0.7623457908630371 = 0.13435260951519012 + 0.1 * 6.279931545257568
Epoch 410, val loss: 0.8143119215965271
Epoch 420, training loss: 0.7509528398513794 = 0.12298329919576645 + 0.1 * 6.279695510864258
Epoch 420, val loss: 0.82344651222229
Epoch 430, training loss: 0.74052894115448 = 0.11262059956789017 + 0.1 * 6.279083251953125
Epoch 430, val loss: 0.8326532244682312
Epoch 440, training loss: 0.7304078340530396 = 0.10317125916481018 + 0.1 * 6.272365093231201
Epoch 440, val loss: 0.8418971300125122
Epoch 450, training loss: 0.7211940884590149 = 0.0945313349366188 + 0.1 * 6.266627311706543
Epoch 450, val loss: 0.851142942905426
Epoch 460, training loss: 0.7126343250274658 = 0.0866481363773346 + 0.1 * 6.259861946105957
Epoch 460, val loss: 0.8603262305259705
Epoch 470, training loss: 0.7110735774040222 = 0.07943985611200333 + 0.1 * 6.316336631774902
Epoch 470, val loss: 0.8695827722549438
Epoch 480, training loss: 0.7001317143440247 = 0.0729031041264534 + 0.1 * 6.2722859382629395
Epoch 480, val loss: 0.8784441947937012
Epoch 490, training loss: 0.6925356388092041 = 0.06696681678295135 + 0.1 * 6.255688190460205
Epoch 490, val loss: 0.8873127698898315
Epoch 500, training loss: 0.6863778829574585 = 0.06157549470663071 + 0.1 * 6.248023986816406
Epoch 500, val loss: 0.8962204456329346
Epoch 510, training loss: 0.6813458204269409 = 0.05666455626487732 + 0.1 * 6.246812343597412
Epoch 510, val loss: 0.9050359725952148
Epoch 520, training loss: 0.6768797039985657 = 0.05221164599061012 + 0.1 * 6.24668025970459
Epoch 520, val loss: 0.9137235879898071
Epoch 530, training loss: 0.6722941994667053 = 0.04818538576364517 + 0.1 * 6.241087913513184
Epoch 530, val loss: 0.9223401546478271
Epoch 540, training loss: 0.6682366728782654 = 0.04453246295452118 + 0.1 * 6.23704195022583
Epoch 540, val loss: 0.9309601187705994
Epoch 550, training loss: 0.6653011441230774 = 0.041220393031835556 + 0.1 * 6.24080753326416
Epoch 550, val loss: 0.9394351840019226
Epoch 560, training loss: 0.6628602743148804 = 0.03822816163301468 + 0.1 * 6.246321201324463
Epoch 560, val loss: 0.9478791356086731
Epoch 570, training loss: 0.6589066982269287 = 0.03551913797855377 + 0.1 * 6.233875751495361
Epoch 570, val loss: 0.9561294913291931
Epoch 580, training loss: 0.6555978655815125 = 0.033059582114219666 + 0.1 * 6.225382328033447
Epoch 580, val loss: 0.9644408226013184
Epoch 590, training loss: 0.653288722038269 = 0.030819840729236603 + 0.1 * 6.224688529968262
Epoch 590, val loss: 0.9727112650871277
Epoch 600, training loss: 0.6509979963302612 = 0.028779329732060432 + 0.1 * 6.22218656539917
Epoch 600, val loss: 0.980904221534729
Epoch 610, training loss: 0.6487582325935364 = 0.02692040614783764 + 0.1 * 6.21837854385376
Epoch 610, val loss: 0.9890245199203491
Epoch 620, training loss: 0.6472360491752625 = 0.02522576041519642 + 0.1 * 6.220102787017822
Epoch 620, val loss: 0.9969924092292786
Epoch 630, training loss: 0.6449993252754211 = 0.023681513965129852 + 0.1 * 6.2131781578063965
Epoch 630, val loss: 1.0049697160720825
Epoch 640, training loss: 0.643354594707489 = 0.02226884476840496 + 0.1 * 6.210857391357422
Epoch 640, val loss: 1.0127748250961304
Epoch 650, training loss: 0.6427417397499084 = 0.020975954830646515 + 0.1 * 6.217657566070557
Epoch 650, val loss: 1.0205059051513672
Epoch 660, training loss: 0.640836238861084 = 0.019793981686234474 + 0.1 * 6.210422515869141
Epoch 660, val loss: 1.0280712842941284
Epoch 670, training loss: 0.6385641694068909 = 0.018711164593696594 + 0.1 * 6.198530197143555
Epoch 670, val loss: 1.0356124639511108
Epoch 680, training loss: 0.6376001834869385 = 0.01771486923098564 + 0.1 * 6.198853015899658
Epoch 680, val loss: 1.0430892705917358
Epoch 690, training loss: 0.6362056136131287 = 0.016796350479125977 + 0.1 * 6.194092750549316
Epoch 690, val loss: 1.0503941774368286
Epoch 700, training loss: 0.6352510452270508 = 0.015948878601193428 + 0.1 * 6.193021297454834
Epoch 700, val loss: 1.0576493740081787
Epoch 710, training loss: 0.634518027305603 = 0.015164408832788467 + 0.1 * 6.193535804748535
Epoch 710, val loss: 1.064756155014038
Epoch 720, training loss: 0.6351207494735718 = 0.014440377242863178 + 0.1 * 6.206803798675537
Epoch 720, val loss: 1.0717123746871948
Epoch 730, training loss: 0.6325514912605286 = 0.013770794495940208 + 0.1 * 6.187806606292725
Epoch 730, val loss: 1.0785554647445679
Epoch 740, training loss: 0.631146252155304 = 0.013149983249604702 + 0.1 * 6.179962635040283
Epoch 740, val loss: 1.0852885246276855
Epoch 750, training loss: 0.6316407322883606 = 0.012571748346090317 + 0.1 * 6.190689563751221
Epoch 750, val loss: 1.0919657945632935
Epoch 760, training loss: 0.6301332712173462 = 0.01203245110809803 + 0.1 * 6.181008338928223
Epoch 760, val loss: 1.0984675884246826
Epoch 770, training loss: 0.629703938961029 = 0.011529816314578056 + 0.1 * 6.181741237640381
Epoch 770, val loss: 1.1048638820648193
Epoch 780, training loss: 0.6292431354522705 = 0.011060889810323715 + 0.1 * 6.181822299957275
Epoch 780, val loss: 1.1111774444580078
Epoch 790, training loss: 0.6286890506744385 = 0.010623032227158546 + 0.1 * 6.180659770965576
Epoch 790, val loss: 1.1172834634780884
Epoch 800, training loss: 0.6271412968635559 = 0.010213874280452728 + 0.1 * 6.16927433013916
Epoch 800, val loss: 1.123313307762146
Epoch 810, training loss: 0.6272607445716858 = 0.00982881709933281 + 0.1 * 6.174319267272949
Epoch 810, val loss: 1.1292203664779663
Epoch 820, training loss: 0.6259034276008606 = 0.009467259980738163 + 0.1 * 6.164361476898193
Epoch 820, val loss: 1.135115146636963
Epoch 830, training loss: 0.6256115436553955 = 0.009127293713390827 + 0.1 * 6.164842128753662
Epoch 830, val loss: 1.1409492492675781
Epoch 840, training loss: 0.625670850276947 = 0.008806022815406322 + 0.1 * 6.168647766113281
Epoch 840, val loss: 1.1465835571289062
Epoch 850, training loss: 0.6246698498725891 = 0.008504724130034447 + 0.1 * 6.161651134490967
Epoch 850, val loss: 1.1521333456039429
Epoch 860, training loss: 0.6242207288742065 = 0.008220119401812553 + 0.1 * 6.160005569458008
Epoch 860, val loss: 1.157630205154419
Epoch 870, training loss: 0.6237528324127197 = 0.007951077073812485 + 0.1 * 6.158017158508301
Epoch 870, val loss: 1.162981629371643
Epoch 880, training loss: 0.6235510110855103 = 0.00769787235185504 + 0.1 * 6.158531188964844
Epoch 880, val loss: 1.168332576751709
Epoch 890, training loss: 0.6236050128936768 = 0.007457631640136242 + 0.1 * 6.161473751068115
Epoch 890, val loss: 1.1735872030258179
Epoch 900, training loss: 0.6227152943611145 = 0.007229977753013372 + 0.1 * 6.154852867126465
Epoch 900, val loss: 1.178762674331665
Epoch 910, training loss: 0.6233296990394592 = 0.007013488560914993 + 0.1 * 6.163161754608154
Epoch 910, val loss: 1.1838476657867432
Epoch 920, training loss: 0.6221151351928711 = 0.006807595491409302 + 0.1 * 6.153075218200684
Epoch 920, val loss: 1.1888409852981567
Epoch 930, training loss: 0.6216143369674683 = 0.0066122678108513355 + 0.1 * 6.150021076202393
Epoch 930, val loss: 1.193766474723816
Epoch 940, training loss: 0.6214649677276611 = 0.006426642183214426 + 0.1 * 6.150383472442627
Epoch 940, val loss: 1.1985876560211182
Epoch 950, training loss: 0.6208993196487427 = 0.006250263191759586 + 0.1 * 6.146490573883057
Epoch 950, val loss: 1.203330397605896
Epoch 960, training loss: 0.6203110218048096 = 0.006081936880946159 + 0.1 * 6.1422905921936035
Epoch 960, val loss: 1.2080731391906738
Epoch 970, training loss: 0.621412992477417 = 0.005920779425650835 + 0.1 * 6.154922008514404
Epoch 970, val loss: 1.2127530574798584
Epoch 980, training loss: 0.6197635531425476 = 0.005767097231000662 + 0.1 * 6.139964580535889
Epoch 980, val loss: 1.21722412109375
Epoch 990, training loss: 0.619780421257019 = 0.005621034186333418 + 0.1 * 6.141593933105469
Epoch 990, val loss: 1.2217129468917847
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.8044
Flip ASR: 0.7733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7900071144104004 = 1.9526286125183105 + 0.1 * 8.373784065246582
Epoch 0, val loss: 1.952249526977539
Epoch 10, training loss: 2.7791199684143066 = 1.941764235496521 + 0.1 * 8.373556137084961
Epoch 10, val loss: 1.9420706033706665
Epoch 20, training loss: 2.765338897705078 = 1.928106665611267 + 0.1 * 8.372321128845215
Epoch 20, val loss: 1.9286234378814697
Epoch 30, training loss: 2.7449514865875244 = 1.9085724353790283 + 0.1 * 8.363790512084961
Epoch 30, val loss: 1.909011960029602
Epoch 40, training loss: 2.7107653617858887 = 1.8795503377914429 + 0.1 * 8.312150001525879
Epoch 40, val loss: 1.880371332168579
Epoch 50, training loss: 2.641619920730591 = 1.8413039445877075 + 0.1 * 8.003159523010254
Epoch 50, val loss: 1.8448307514190674
Epoch 60, training loss: 2.5550734996795654 = 1.8028653860092163 + 0.1 * 7.522080421447754
Epoch 60, val loss: 1.811607003211975
Epoch 70, training loss: 2.47629714012146 = 1.7696794271469116 + 0.1 * 7.066176414489746
Epoch 70, val loss: 1.7836129665374756
Epoch 80, training loss: 2.415538787841797 = 1.7365678548812866 + 0.1 * 6.789709568023682
Epoch 80, val loss: 1.7554785013198853
Epoch 90, training loss: 2.362837791442871 = 1.6931322813034058 + 0.1 * 6.697055339813232
Epoch 90, val loss: 1.7172008752822876
Epoch 100, training loss: 2.299102783203125 = 1.634376049041748 + 0.1 * 6.6472673416137695
Epoch 100, val loss: 1.6668975353240967
Epoch 110, training loss: 2.222888231277466 = 1.561443567276001 + 0.1 * 6.614446640014648
Epoch 110, val loss: 1.6069027185440063
Epoch 120, training loss: 2.1392440795898438 = 1.4808416366577148 + 0.1 * 6.584023952484131
Epoch 120, val loss: 1.5407322645187378
Epoch 130, training loss: 2.055617332458496 = 1.3999674320220947 + 0.1 * 6.55649995803833
Epoch 130, val loss: 1.4752306938171387
Epoch 140, training loss: 1.9733457565307617 = 1.3200122117996216 + 0.1 * 6.5333356857299805
Epoch 140, val loss: 1.411189079284668
Epoch 150, training loss: 1.891979694366455 = 1.2407735586166382 + 0.1 * 6.512062072753906
Epoch 150, val loss: 1.3493223190307617
Epoch 160, training loss: 1.8092797994613647 = 1.160038709640503 + 0.1 * 6.492410659790039
Epoch 160, val loss: 1.2866921424865723
Epoch 170, training loss: 1.7251207828521729 = 1.0774768590927124 + 0.1 * 6.476439476013184
Epoch 170, val loss: 1.2231477499008179
Epoch 180, training loss: 1.6417946815490723 = 0.9957608580589294 + 0.1 * 6.460338592529297
Epoch 180, val loss: 1.1607786417007446
Epoch 190, training loss: 1.559553623199463 = 0.9147845506668091 + 0.1 * 6.447690010070801
Epoch 190, val loss: 1.0988879203796387
Epoch 200, training loss: 1.4794819355010986 = 0.8361915349960327 + 0.1 * 6.432904243469238
Epoch 200, val loss: 1.0390865802764893
Epoch 210, training loss: 1.4031398296356201 = 0.760832667350769 + 0.1 * 6.423070907592773
Epoch 210, val loss: 0.9824414253234863
Epoch 220, training loss: 1.3313379287719727 = 0.6904277205467224 + 0.1 * 6.409102439880371
Epoch 220, val loss: 0.9317535758018494
Epoch 230, training loss: 1.266683578491211 = 0.6260915994644165 + 0.1 * 6.405919075012207
Epoch 230, val loss: 0.8879628777503967
Epoch 240, training loss: 1.2071783542633057 = 0.568508505821228 + 0.1 * 6.3866987228393555
Epoch 240, val loss: 0.8521956205368042
Epoch 250, training loss: 1.155480146408081 = 0.5170536041259766 + 0.1 * 6.3842644691467285
Epoch 250, val loss: 0.8236223459243774
Epoch 260, training loss: 1.1091575622558594 = 0.47054505348205566 + 0.1 * 6.386125564575195
Epoch 260, val loss: 0.8010810017585754
Epoch 270, training loss: 1.0652649402618408 = 0.42806610465049744 + 0.1 * 6.371987819671631
Epoch 270, val loss: 0.7835801839828491
Epoch 280, training loss: 1.0244896411895752 = 0.38844090700149536 + 0.1 * 6.360487937927246
Epoch 280, val loss: 0.7696632742881775
Epoch 290, training loss: 0.9862303733825684 = 0.3512836694717407 + 0.1 * 6.349466800689697
Epoch 290, val loss: 0.7587910294532776
Epoch 300, training loss: 0.9507911205291748 = 0.31657418608665466 + 0.1 * 6.342169761657715
Epoch 300, val loss: 0.750504732131958
Epoch 310, training loss: 0.91957688331604 = 0.28470101952552795 + 0.1 * 6.348758697509766
Epoch 310, val loss: 0.7448289394378662
Epoch 320, training loss: 0.8896173238754272 = 0.25612756609916687 + 0.1 * 6.334897994995117
Epoch 320, val loss: 0.7420789003372192
Epoch 330, training loss: 0.863477885723114 = 0.23067186772823334 + 0.1 * 6.328060150146484
Epoch 330, val loss: 0.7419243454933167
Epoch 340, training loss: 0.8406829237937927 = 0.2082439512014389 + 0.1 * 6.324389457702637
Epoch 340, val loss: 0.7443618178367615
Epoch 350, training loss: 0.8205438256263733 = 0.1887599378824234 + 0.1 * 6.3178391456604
Epoch 350, val loss: 0.7491298913955688
Epoch 360, training loss: 0.8034995198249817 = 0.1717609018087387 + 0.1 * 6.317385673522949
Epoch 360, val loss: 0.7560083866119385
Epoch 370, training loss: 0.7880626916885376 = 0.15673419833183289 + 0.1 * 6.313284873962402
Epoch 370, val loss: 0.7645401358604431
Epoch 380, training loss: 0.7743266820907593 = 0.14342159032821655 + 0.1 * 6.309051036834717
Epoch 380, val loss: 0.7743247151374817
Epoch 390, training loss: 0.761903703212738 = 0.1315867155790329 + 0.1 * 6.3031697273254395
Epoch 390, val loss: 0.7850209474563599
Epoch 400, training loss: 0.751032829284668 = 0.12104319781064987 + 0.1 * 6.299896240234375
Epoch 400, val loss: 0.7964987754821777
Epoch 410, training loss: 0.7405019998550415 = 0.11160547286272049 + 0.1 * 6.288965225219727
Epoch 410, val loss: 0.8086035847663879
Epoch 420, training loss: 0.7315670847892761 = 0.1031339019536972 + 0.1 * 6.284331321716309
Epoch 420, val loss: 0.8211469650268555
Epoch 430, training loss: 0.7236495018005371 = 0.09552887082099915 + 0.1 * 6.281205654144287
Epoch 430, val loss: 0.8340011835098267
Epoch 440, training loss: 0.7162770628929138 = 0.08867866545915604 + 0.1 * 6.275983810424805
Epoch 440, val loss: 0.847158670425415
Epoch 450, training loss: 0.7098476886749268 = 0.0824790820479393 + 0.1 * 6.273685932159424
Epoch 450, val loss: 0.8604689836502075
Epoch 460, training loss: 0.7032799124717712 = 0.07686016708612442 + 0.1 * 6.26419734954834
Epoch 460, val loss: 0.873894214630127
Epoch 470, training loss: 0.6986756324768066 = 0.07175284624099731 + 0.1 * 6.269227981567383
Epoch 470, val loss: 0.8872494101524353
Epoch 480, training loss: 0.6931791305541992 = 0.06710798293352127 + 0.1 * 6.260711669921875
Epoch 480, val loss: 0.9006084203720093
Epoch 490, training loss: 0.6879137754440308 = 0.06286944448947906 + 0.1 * 6.250443458557129
Epoch 490, val loss: 0.9139637351036072
Epoch 500, training loss: 0.6842076778411865 = 0.05898476392030716 + 0.1 * 6.2522292137146
Epoch 500, val loss: 0.9271121025085449
Epoch 510, training loss: 0.6797550320625305 = 0.05543174967169762 + 0.1 * 6.243232727050781
Epoch 510, val loss: 0.9402303695678711
Epoch 520, training loss: 0.6764882802963257 = 0.05216997116804123 + 0.1 * 6.243183135986328
Epoch 520, val loss: 0.9532392621040344
Epoch 530, training loss: 0.6734472513198853 = 0.04916474223136902 + 0.1 * 6.242824554443359
Epoch 530, val loss: 0.9660468101501465
Epoch 540, training loss: 0.6700927019119263 = 0.046398695558309555 + 0.1 * 6.236939907073975
Epoch 540, val loss: 0.9786478877067566
Epoch 550, training loss: 0.6669678092002869 = 0.04385042190551758 + 0.1 * 6.231173515319824
Epoch 550, val loss: 0.9913036227226257
Epoch 560, training loss: 0.6639977097511292 = 0.041487615555524826 + 0.1 * 6.225100517272949
Epoch 560, val loss: 1.0036921501159668
Epoch 570, training loss: 0.6626703143119812 = 0.03929752856492996 + 0.1 * 6.233727931976318
Epoch 570, val loss: 1.0159491300582886
Epoch 580, training loss: 0.6596090197563171 = 0.03727097064256668 + 0.1 * 6.2233805656433105
Epoch 580, val loss: 1.028058648109436
Epoch 590, training loss: 0.6567760705947876 = 0.03539101779460907 + 0.1 * 6.213850498199463
Epoch 590, val loss: 1.0400279760360718
Epoch 600, training loss: 0.6547768712043762 = 0.033641066402196884 + 0.1 * 6.211358070373535
Epoch 600, val loss: 1.0518226623535156
Epoch 610, training loss: 0.6537322998046875 = 0.03201022744178772 + 0.1 * 6.217220306396484
Epoch 610, val loss: 1.0633544921875
Epoch 620, training loss: 0.6516217589378357 = 0.030493484809994698 + 0.1 * 6.211282253265381
Epoch 620, val loss: 1.0749210119247437
Epoch 630, training loss: 0.6498268842697144 = 0.02907562628388405 + 0.1 * 6.207512378692627
Epoch 630, val loss: 1.0861893892288208
Epoch 640, training loss: 0.6482883095741272 = 0.02775118686258793 + 0.1 * 6.205370903015137
Epoch 640, val loss: 1.0973336696624756
Epoch 650, training loss: 0.6477614045143127 = 0.026512810960412025 + 0.1 * 6.2124857902526855
Epoch 650, val loss: 1.1082347631454468
Epoch 660, training loss: 0.6452978253364563 = 0.025355219841003418 + 0.1 * 6.199426174163818
Epoch 660, val loss: 1.119056224822998
Epoch 670, training loss: 0.6437485814094543 = 0.02426740899682045 + 0.1 * 6.1948113441467285
Epoch 670, val loss: 1.129632830619812
Epoch 680, training loss: 0.6428675055503845 = 0.023245807737112045 + 0.1 * 6.196217060089111
Epoch 680, val loss: 1.139892816543579
Epoch 690, training loss: 0.6421392560005188 = 0.022289741784334183 + 0.1 * 6.198494911193848
Epoch 690, val loss: 1.1502927541732788
Epoch 700, training loss: 0.6394090056419373 = 0.021389897912740707 + 0.1 * 6.1801910400390625
Epoch 700, val loss: 1.160241723060608
Epoch 710, training loss: 0.6388799548149109 = 0.020544100552797318 + 0.1 * 6.183358192443848
Epoch 710, val loss: 1.1702226400375366
Epoch 720, training loss: 0.6391839385032654 = 0.019745616242289543 + 0.1 * 6.19438362121582
Epoch 720, val loss: 1.179991602897644
Epoch 730, training loss: 0.6371005177497864 = 0.01899261400103569 + 0.1 * 6.181078910827637
Epoch 730, val loss: 1.1895067691802979
Epoch 740, training loss: 0.6359569430351257 = 0.018282931298017502 + 0.1 * 6.176739692687988
Epoch 740, val loss: 1.1990348100662231
Epoch 750, training loss: 0.6359900832176208 = 0.01761060580611229 + 0.1 * 6.1837944984436035
Epoch 750, val loss: 1.2082443237304688
Epoch 760, training loss: 0.6348742842674255 = 0.016976024955511093 + 0.1 * 6.178982734680176
Epoch 760, val loss: 1.2173798084259033
Epoch 770, training loss: 0.6334548592567444 = 0.016376063227653503 + 0.1 * 6.170787811279297
Epoch 770, val loss: 1.2264666557312012
Epoch 780, training loss: 0.6329850554466248 = 0.015806855633854866 + 0.1 * 6.17178201675415
Epoch 780, val loss: 1.2352197170257568
Epoch 790, training loss: 0.6321467161178589 = 0.015268050134181976 + 0.1 * 6.168787002563477
Epoch 790, val loss: 1.2440862655639648
Epoch 800, training loss: 0.6325525045394897 = 0.01475598756223917 + 0.1 * 6.17796516418457
Epoch 800, val loss: 1.2525149583816528
Epoch 810, training loss: 0.631290078163147 = 0.014271220192313194 + 0.1 * 6.1701884269714355
Epoch 810, val loss: 1.2609080076217651
Epoch 820, training loss: 0.6301433444023132 = 0.013811673037707806 + 0.1 * 6.16331672668457
Epoch 820, val loss: 1.2693921327590942
Epoch 830, training loss: 0.6300340890884399 = 0.013373133726418018 + 0.1 * 6.166609287261963
Epoch 830, val loss: 1.2774029970169067
Epoch 840, training loss: 0.6286056041717529 = 0.012955942191183567 + 0.1 * 6.156496047973633
Epoch 840, val loss: 1.2854068279266357
Epoch 850, training loss: 0.6286991238594055 = 0.012559457682073116 + 0.1 * 6.161396503448486
Epoch 850, val loss: 1.2933746576309204
Epoch 860, training loss: 0.6277849674224854 = 0.012180627323687077 + 0.1 * 6.15604305267334
Epoch 860, val loss: 1.3010895252227783
Epoch 870, training loss: 0.6264395117759705 = 0.011820444837212563 + 0.1 * 6.146190643310547
Epoch 870, val loss: 1.3089326620101929
Epoch 880, training loss: 0.6274204850196838 = 0.011475074104964733 + 0.1 * 6.159453868865967
Epoch 880, val loss: 1.316429615020752
Epoch 890, training loss: 0.6262475848197937 = 0.01114561501890421 + 0.1 * 6.15101957321167
Epoch 890, val loss: 1.3239375352859497
Epoch 900, training loss: 0.6269327402114868 = 0.010831398889422417 + 0.1 * 6.161013126373291
Epoch 900, val loss: 1.3312710523605347
Epoch 910, training loss: 0.6253646016120911 = 0.010530617088079453 + 0.1 * 6.148339748382568
Epoch 910, val loss: 1.3385034799575806
Epoch 920, training loss: 0.6255428791046143 = 0.010244752280414104 + 0.1 * 6.152981281280518
Epoch 920, val loss: 1.3457348346710205
Epoch 930, training loss: 0.6238081455230713 = 0.009969553910195827 + 0.1 * 6.138386249542236
Epoch 930, val loss: 1.3527405261993408
Epoch 940, training loss: 0.6240738034248352 = 0.009706573560833931 + 0.1 * 6.143672466278076
Epoch 940, val loss: 1.3597955703735352
Epoch 950, training loss: 0.6238831877708435 = 0.009453420527279377 + 0.1 * 6.1442975997924805
Epoch 950, val loss: 1.366587519645691
Epoch 960, training loss: 0.6240095496177673 = 0.009211600758135319 + 0.1 * 6.147979259490967
Epoch 960, val loss: 1.3734381198883057
Epoch 970, training loss: 0.622708261013031 = 0.008979300037026405 + 0.1 * 6.137289047241211
Epoch 970, val loss: 1.3802814483642578
Epoch 980, training loss: 0.6237400770187378 = 0.008756193332374096 + 0.1 * 6.149838447570801
Epoch 980, val loss: 1.3868720531463623
Epoch 990, training loss: 0.6220523715019226 = 0.008541788905858994 + 0.1 * 6.135106086730957
Epoch 990, val loss: 1.3934282064437866
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7417
Flip ASR: 0.6933/225 nodes
The final ASR:0.69496, 0.11339, Accuracy:0.80988, 0.02058
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7883708477020264 = 1.9509844779968262 + 0.1 * 8.37386417388916
Epoch 0, val loss: 1.9508497714996338
Epoch 10, training loss: 2.777970552444458 = 1.9406005144119263 + 0.1 * 8.373700141906738
Epoch 10, val loss: 1.9412165880203247
Epoch 20, training loss: 2.7647576332092285 = 1.9274649620056152 + 0.1 * 8.372925758361816
Epoch 20, val loss: 1.928818941116333
Epoch 30, training loss: 2.7454090118408203 = 1.9086635112762451 + 0.1 * 8.367453575134277
Epoch 30, val loss: 1.9109524488449097
Epoch 40, training loss: 2.7127580642700195 = 1.8805692195892334 + 0.1 * 8.321887016296387
Epoch 40, val loss: 1.8845138549804688
Epoch 50, training loss: 2.638456344604492 = 1.8438085317611694 + 0.1 * 7.946478366851807
Epoch 50, val loss: 1.8517893552780151
Epoch 60, training loss: 2.541658639907837 = 1.810737133026123 + 0.1 * 7.3092145919799805
Epoch 60, val loss: 1.8240200281143188
Epoch 70, training loss: 2.4683024883270264 = 1.7825891971588135 + 0.1 * 6.857132434844971
Epoch 70, val loss: 1.8006535768508911
Epoch 80, training loss: 2.4238805770874023 = 1.754192590713501 + 0.1 * 6.696878433227539
Epoch 80, val loss: 1.776410460472107
Epoch 90, training loss: 2.379538059234619 = 1.7182800769805908 + 0.1 * 6.612579345703125
Epoch 90, val loss: 1.745070219039917
Epoch 100, training loss: 2.325756311416626 = 1.6699271202087402 + 0.1 * 6.558291435241699
Epoch 100, val loss: 1.7035000324249268
Epoch 110, training loss: 2.2596848011016846 = 1.6071630716323853 + 0.1 * 6.525217533111572
Epoch 110, val loss: 1.6512094736099243
Epoch 120, training loss: 2.1825313568115234 = 1.5320147275924683 + 0.1 * 6.505165100097656
Epoch 120, val loss: 1.5907306671142578
Epoch 130, training loss: 2.101853370666504 = 1.45220947265625 + 0.1 * 6.496438503265381
Epoch 130, val loss: 1.527091383934021
Epoch 140, training loss: 2.024152994155884 = 1.3750165700912476 + 0.1 * 6.491364479064941
Epoch 140, val loss: 1.4679936170578003
Epoch 150, training loss: 1.9481197595596313 = 1.3003054857254028 + 0.1 * 6.478142738342285
Epoch 150, val loss: 1.4113343954086304
Epoch 160, training loss: 1.8717411756515503 = 1.224452018737793 + 0.1 * 6.472891330718994
Epoch 160, val loss: 1.354902744293213
Epoch 170, training loss: 1.7933311462402344 = 1.1480122804641724 + 0.1 * 6.453187942504883
Epoch 170, val loss: 1.2988048791885376
Epoch 180, training loss: 1.716484546661377 = 1.072365641593933 + 0.1 * 6.441189765930176
Epoch 180, val loss: 1.2441902160644531
Epoch 190, training loss: 1.6427161693572998 = 1.0000911951065063 + 0.1 * 6.426249980926514
Epoch 190, val loss: 1.1925570964813232
Epoch 200, training loss: 1.5749006271362305 = 0.9327847361564636 + 0.1 * 6.421158313751221
Epoch 200, val loss: 1.1443233489990234
Epoch 210, training loss: 1.5133864879608154 = 0.8727248311042786 + 0.1 * 6.4066162109375
Epoch 210, val loss: 1.1019302606582642
Epoch 220, training loss: 1.4584276676177979 = 0.8187673687934875 + 0.1 * 6.396602630615234
Epoch 220, val loss: 1.0647996664047241
Epoch 230, training loss: 1.4097039699554443 = 0.7701761722564697 + 0.1 * 6.395277500152588
Epoch 230, val loss: 1.0332845449447632
Epoch 240, training loss: 1.363969087600708 = 0.726090133190155 + 0.1 * 6.37878942489624
Epoch 240, val loss: 1.006448745727539
Epoch 250, training loss: 1.3214073181152344 = 0.6843399405479431 + 0.1 * 6.370673656463623
Epoch 250, val loss: 0.9824342131614685
Epoch 260, training loss: 1.2795002460479736 = 0.643181562423706 + 0.1 * 6.363187313079834
Epoch 260, val loss: 0.9592991471290588
Epoch 270, training loss: 1.2395461797714233 = 0.6023412942886353 + 0.1 * 6.372048854827881
Epoch 270, val loss: 0.9366772770881653
Epoch 280, training loss: 1.1977189779281616 = 0.5626258254051208 + 0.1 * 6.350931644439697
Epoch 280, val loss: 0.9151423573493958
Epoch 290, training loss: 1.1596843004226685 = 0.5250865817070007 + 0.1 * 6.345976829528809
Epoch 290, val loss: 0.8956425786018372
Epoch 300, training loss: 1.1274793148040771 = 0.4912651479244232 + 0.1 * 6.362141132354736
Epoch 300, val loss: 0.8796912431716919
Epoch 310, training loss: 1.0952491760253906 = 0.4617749750614166 + 0.1 * 6.334742069244385
Epoch 310, val loss: 0.8681228160858154
Epoch 320, training loss: 1.0684239864349365 = 0.43587052822113037 + 0.1 * 6.325534343719482
Epoch 320, val loss: 0.8601888418197632
Epoch 330, training loss: 1.0456507205963135 = 0.41247931122779846 + 0.1 * 6.331713676452637
Epoch 330, val loss: 0.8552606701850891
Epoch 340, training loss: 1.0222790241241455 = 0.3905866742134094 + 0.1 * 6.316922664642334
Epoch 340, val loss: 0.8524380922317505
Epoch 350, training loss: 1.0007354021072388 = 0.36907070875167847 + 0.1 * 6.316646575927734
Epoch 350, val loss: 0.8511776328086853
Epoch 360, training loss: 0.9774763584136963 = 0.3469398617744446 + 0.1 * 6.305365085601807
Epoch 360, val loss: 0.8510532379150391
Epoch 370, training loss: 0.9567269086837769 = 0.32336512207984924 + 0.1 * 6.3336181640625
Epoch 370, val loss: 0.8518562316894531
Epoch 380, training loss: 0.9280561208724976 = 0.2981964945793152 + 0.1 * 6.298596382141113
Epoch 380, val loss: 0.8536564707756042
Epoch 390, training loss: 0.9009305238723755 = 0.271735817193985 + 0.1 * 6.2919464111328125
Epoch 390, val loss: 0.8568068742752075
Epoch 400, training loss: 0.8749062418937683 = 0.24480418860912323 + 0.1 * 6.30102014541626
Epoch 400, val loss: 0.8615463972091675
Epoch 410, training loss: 0.8477298021316528 = 0.21860620379447937 + 0.1 * 6.29123592376709
Epoch 410, val loss: 0.8679729700088501
Epoch 420, training loss: 0.8236973285675049 = 0.19411005079746246 + 0.1 * 6.295872688293457
Epoch 420, val loss: 0.8761844038963318
Epoch 430, training loss: 0.8007487058639526 = 0.17220290005207062 + 0.1 * 6.285458087921143
Epoch 430, val loss: 0.8861328959465027
Epoch 440, training loss: 0.7812005281448364 = 0.15323637425899506 + 0.1 * 6.279641151428223
Epoch 440, val loss: 0.8978000283241272
Epoch 450, training loss: 0.7659640312194824 = 0.13700756430625916 + 0.1 * 6.28956413269043
Epoch 450, val loss: 0.9109387397766113
Epoch 460, training loss: 0.750403642654419 = 0.12309803068637848 + 0.1 * 6.2730560302734375
Epoch 460, val loss: 0.9253796339035034
Epoch 470, training loss: 0.7386218309402466 = 0.11104011535644531 + 0.1 * 6.275816917419434
Epoch 470, val loss: 0.9404876232147217
Epoch 480, training loss: 0.7277250289916992 = 0.1005372703075409 + 0.1 * 6.271877288818359
Epoch 480, val loss: 0.9562209248542786
Epoch 490, training loss: 0.7170952558517456 = 0.09132663905620575 + 0.1 * 6.257686138153076
Epoch 490, val loss: 0.9724406003952026
Epoch 500, training loss: 0.7084667086601257 = 0.08319281041622162 + 0.1 * 6.252738952636719
Epoch 500, val loss: 0.9890415668487549
Epoch 510, training loss: 0.7016063928604126 = 0.07597504556179047 + 0.1 * 6.256313323974609
Epoch 510, val loss: 1.0058698654174805
Epoch 520, training loss: 0.6955426931381226 = 0.06956736743450165 + 0.1 * 6.2597527503967285
Epoch 520, val loss: 1.0224529504776
Epoch 530, training loss: 0.6887571215629578 = 0.06387700140476227 + 0.1 * 6.248801231384277
Epoch 530, val loss: 1.0392946004867554
Epoch 540, training loss: 0.6830068826675415 = 0.05878589674830437 + 0.1 * 6.242209434509277
Epoch 540, val loss: 1.055903673171997
Epoch 550, training loss: 0.6789113283157349 = 0.05421920120716095 + 0.1 * 6.246921062469482
Epoch 550, val loss: 1.0723460912704468
Epoch 560, training loss: 0.6747045516967773 = 0.050124213099479675 + 0.1 * 6.245802879333496
Epoch 560, val loss: 1.0884755849838257
Epoch 570, training loss: 0.6695728898048401 = 0.0464443564414978 + 0.1 * 6.231285095214844
Epoch 570, val loss: 1.1043701171875
Epoch 580, training loss: 0.6664237976074219 = 0.04313003271818161 + 0.1 * 6.232937335968018
Epoch 580, val loss: 1.1197741031646729
Epoch 590, training loss: 0.662642240524292 = 0.04013955965638161 + 0.1 * 6.225026607513428
Epoch 590, val loss: 1.1349200010299683
Epoch 600, training loss: 0.6605268120765686 = 0.03743637725710869 + 0.1 * 6.2309041023254395
Epoch 600, val loss: 1.1494914293289185
Epoch 610, training loss: 0.6573259234428406 = 0.03499377891421318 + 0.1 * 6.223321437835693
Epoch 610, val loss: 1.163908839225769
Epoch 620, training loss: 0.6542431712150574 = 0.03277204558253288 + 0.1 * 6.2147111892700195
Epoch 620, val loss: 1.1777504682540894
Epoch 630, training loss: 0.6530466079711914 = 0.030748015269637108 + 0.1 * 6.222985744476318
Epoch 630, val loss: 1.191245675086975
Epoch 640, training loss: 0.6511792540550232 = 0.028907625004649162 + 0.1 * 6.222716331481934
Epoch 640, val loss: 1.2040672302246094
Epoch 650, training loss: 0.6484817266464233 = 0.027232268825173378 + 0.1 * 6.212494373321533
Epoch 650, val loss: 1.2168852090835571
Epoch 660, training loss: 0.6464937925338745 = 0.025696801021695137 + 0.1 * 6.207969665527344
Epoch 660, val loss: 1.229117751121521
Epoch 670, training loss: 0.644747793674469 = 0.024288063868880272 + 0.1 * 6.204597473144531
Epoch 670, val loss: 1.2409080266952515
Epoch 680, training loss: 0.6429709792137146 = 0.022996028885245323 + 0.1 * 6.19974946975708
Epoch 680, val loss: 1.2525627613067627
Epoch 690, training loss: 0.6420758366584778 = 0.02180538699030876 + 0.1 * 6.202703952789307
Epoch 690, val loss: 1.2638206481933594
Epoch 700, training loss: 0.6408626437187195 = 0.020706389099359512 + 0.1 * 6.20156192779541
Epoch 700, val loss: 1.2748003005981445
Epoch 710, training loss: 0.6392354965209961 = 0.019691990688443184 + 0.1 * 6.1954345703125
Epoch 710, val loss: 1.2852863073349
Epoch 720, training loss: 0.6382555961608887 = 0.01875520497560501 + 0.1 * 6.195003986358643
Epoch 720, val loss: 1.2957996129989624
Epoch 730, training loss: 0.6368767619132996 = 0.01788494363427162 + 0.1 * 6.189918041229248
Epoch 730, val loss: 1.3059812784194946
Epoch 740, training loss: 0.6365063786506653 = 0.017074931412935257 + 0.1 * 6.194314002990723
Epoch 740, val loss: 1.3157724142074585
Epoch 750, training loss: 0.6361855864524841 = 0.016322650015354156 + 0.1 * 6.198628902435303
Epoch 750, val loss: 1.325468897819519
Epoch 760, training loss: 0.6343652606010437 = 0.015621698461472988 + 0.1 * 6.187435150146484
Epoch 760, val loss: 1.3347851037979126
Epoch 770, training loss: 0.6338461637496948 = 0.01496793981641531 + 0.1 * 6.188782215118408
Epoch 770, val loss: 1.3440667390823364
Epoch 780, training loss: 0.6330798864364624 = 0.014355788007378578 + 0.1 * 6.187241077423096
Epoch 780, val loss: 1.3527525663375854
Epoch 790, training loss: 0.6318338513374329 = 0.013785691931843758 + 0.1 * 6.180481433868408
Epoch 790, val loss: 1.3615652322769165
Epoch 800, training loss: 0.630662202835083 = 0.013249646872282028 + 0.1 * 6.174125671386719
Epoch 800, val loss: 1.3700616359710693
Epoch 810, training loss: 0.6309576630592346 = 0.012745496816933155 + 0.1 * 6.182121276855469
Epoch 810, val loss: 1.378353238105774
Epoch 820, training loss: 0.6316922903060913 = 0.012271600775420666 + 0.1 * 6.194206714630127
Epoch 820, val loss: 1.386227011680603
Epoch 830, training loss: 0.6290627121925354 = 0.011827541515231133 + 0.1 * 6.172351837158203
Epoch 830, val loss: 1.3942382335662842
Epoch 840, training loss: 0.6287431716918945 = 0.011409119702875614 + 0.1 * 6.173340320587158
Epoch 840, val loss: 1.4020466804504395
Epoch 850, training loss: 0.6287412047386169 = 0.011013377457857132 + 0.1 * 6.1772780418396
Epoch 850, val loss: 1.4094984531402588
Epoch 860, training loss: 0.6274689435958862 = 0.010640902444720268 + 0.1 * 6.168280124664307
Epoch 860, val loss: 1.4170233011245728
Epoch 870, training loss: 0.6266176104545593 = 0.010287858545780182 + 0.1 * 6.163297176361084
Epoch 870, val loss: 1.4243032932281494
Epoch 880, training loss: 0.6271765232086182 = 0.009953133761882782 + 0.1 * 6.172233581542969
Epoch 880, val loss: 1.431356430053711
Epoch 890, training loss: 0.6262902021408081 = 0.00963614508509636 + 0.1 * 6.166540145874023
Epoch 890, val loss: 1.4383313655853271
Epoch 900, training loss: 0.6254914402961731 = 0.009336352348327637 + 0.1 * 6.161550998687744
Epoch 900, val loss: 1.4452860355377197
Epoch 910, training loss: 0.6262348294258118 = 0.00905071385204792 + 0.1 * 6.171840667724609
Epoch 910, val loss: 1.451842188835144
Epoch 920, training loss: 0.6250050663948059 = 0.008780332282185555 + 0.1 * 6.162247180938721
Epoch 920, val loss: 1.458390235900879
Epoch 930, training loss: 0.6243763566017151 = 0.00852333102375269 + 0.1 * 6.158530235290527
Epoch 930, val loss: 1.4648849964141846
Epoch 940, training loss: 0.6238990426063538 = 0.008278487250208855 + 0.1 * 6.156205654144287
Epoch 940, val loss: 1.4710965156555176
Epoch 950, training loss: 0.6242201328277588 = 0.00804551225155592 + 0.1 * 6.161746025085449
Epoch 950, val loss: 1.4773129224777222
Epoch 960, training loss: 0.6233799457550049 = 0.00782320648431778 + 0.1 * 6.155567169189453
Epoch 960, val loss: 1.483380675315857
Epoch 970, training loss: 0.6233786940574646 = 0.007610721979290247 + 0.1 * 6.157679557800293
Epoch 970, val loss: 1.4892754554748535
Epoch 980, training loss: 0.6235989332199097 = 0.007407764904201031 + 0.1 * 6.161911487579346
Epoch 980, val loss: 1.4951633214950562
Epoch 990, training loss: 0.6232271790504456 = 0.007213617209345102 + 0.1 * 6.160135746002197
Epoch 990, val loss: 1.5006678104400635
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7519
Overall ASR: 0.5461
Flip ASR: 0.4756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7843503952026367 = 1.9469678401947021 + 0.1 * 8.373826026916504
Epoch 0, val loss: 1.9409260749816895
Epoch 10, training loss: 2.773207426071167 = 1.9358446598052979 + 0.1 * 8.373626708984375
Epoch 10, val loss: 1.9290814399719238
Epoch 20, training loss: 2.759711742401123 = 1.9224462509155273 + 0.1 * 8.37265396118164
Epoch 20, val loss: 1.91463303565979
Epoch 30, training loss: 2.7406702041625977 = 1.9040284156799316 + 0.1 * 8.366416931152344
Epoch 30, val loss: 1.8947768211364746
Epoch 40, training loss: 2.7098944187164307 = 1.8775246143341064 + 0.1 * 8.323698043823242
Epoch 40, val loss: 1.8667881488800049
Epoch 50, training loss: 2.6438469886779785 = 1.8427767753601074 + 0.1 * 8.010703086853027
Epoch 50, val loss: 1.8323240280151367
Epoch 60, training loss: 2.5484094619750977 = 1.8089016675949097 + 0.1 * 7.395078182220459
Epoch 60, val loss: 1.8016879558563232
Epoch 70, training loss: 2.4743568897247314 = 1.7784700393676758 + 0.1 * 6.958868980407715
Epoch 70, val loss: 1.7761861085891724
Epoch 80, training loss: 2.4250600337982178 = 1.747893214225769 + 0.1 * 6.77166748046875
Epoch 80, val loss: 1.7519361972808838
Epoch 90, training loss: 2.3788323402404785 = 1.7085686922073364 + 0.1 * 6.70263671875
Epoch 90, val loss: 1.7194864749908447
Epoch 100, training loss: 2.3210744857788086 = 1.655179738998413 + 0.1 * 6.6589484214782715
Epoch 100, val loss: 1.6747044324874878
Epoch 110, training loss: 2.2484753131866455 = 1.5857954025268555 + 0.1 * 6.626799583435059
Epoch 110, val loss: 1.6180293560028076
Epoch 120, training loss: 2.1625428199768066 = 1.502305269241333 + 0.1 * 6.60237455368042
Epoch 120, val loss: 1.5509792566299438
Epoch 130, training loss: 2.0702667236328125 = 1.4116425514221191 + 0.1 * 6.586240768432617
Epoch 130, val loss: 1.479292869567871
Epoch 140, training loss: 1.976710557937622 = 1.3195512294769287 + 0.1 * 6.571593761444092
Epoch 140, val loss: 1.409378170967102
Epoch 150, training loss: 1.8835806846618652 = 1.2280372381210327 + 0.1 * 6.555434703826904
Epoch 150, val loss: 1.3406131267547607
Epoch 160, training loss: 1.7930982112884521 = 1.1390856504440308 + 0.1 * 6.540125846862793
Epoch 160, val loss: 1.2738591432571411
Epoch 170, training loss: 1.7079135179519653 = 1.0553141832351685 + 0.1 * 6.525993347167969
Epoch 170, val loss: 1.2105273008346558
Epoch 180, training loss: 1.6260027885437012 = 0.9754043817520142 + 0.1 * 6.505984783172607
Epoch 180, val loss: 1.150423526763916
Epoch 190, training loss: 1.548956036567688 = 0.8994583487510681 + 0.1 * 6.49497652053833
Epoch 190, val loss: 1.0929557085037231
Epoch 200, training loss: 1.4752576351165771 = 0.8272759914398193 + 0.1 * 6.479816913604736
Epoch 200, val loss: 1.0389440059661865
Epoch 210, training loss: 1.4040398597717285 = 0.7574822902679443 + 0.1 * 6.465574741363525
Epoch 210, val loss: 0.9859404563903809
Epoch 220, training loss: 1.3386423587799072 = 0.6916657090187073 + 0.1 * 6.469765663146973
Epoch 220, val loss: 0.9365381002426147
Epoch 230, training loss: 1.2763469219207764 = 0.6317054629325867 + 0.1 * 6.446414947509766
Epoch 230, val loss: 0.8929535746574402
Epoch 240, training loss: 1.2195277214050293 = 0.5766473412513733 + 0.1 * 6.428802967071533
Epoch 240, val loss: 0.8548716306686401
Epoch 250, training loss: 1.1687729358673096 = 0.5262158513069153 + 0.1 * 6.425570011138916
Epoch 250, val loss: 0.8227272629737854
Epoch 260, training loss: 1.1207804679870605 = 0.4797866940498352 + 0.1 * 6.409937858581543
Epoch 260, val loss: 0.7961738109588623
Epoch 270, training loss: 1.0776060819625854 = 0.43636879324913025 + 0.1 * 6.412372589111328
Epoch 270, val loss: 0.7738074660301208
Epoch 280, training loss: 1.0360660552978516 = 0.39602765440940857 + 0.1 * 6.400384426116943
Epoch 280, val loss: 0.755190908908844
Epoch 290, training loss: 0.9964803457260132 = 0.35817134380340576 + 0.1 * 6.383090019226074
Epoch 290, val loss: 0.7389951348304749
Epoch 300, training loss: 0.9629565477371216 = 0.3224991261959076 + 0.1 * 6.404574394226074
Epoch 300, val loss: 0.7245885729789734
Epoch 310, training loss: 0.9278016090393066 = 0.28952136635780334 + 0.1 * 6.3828020095825195
Epoch 310, val loss: 0.7120355367660522
Epoch 320, training loss: 0.8954035639762878 = 0.25915277004241943 + 0.1 * 6.3625078201293945
Epoch 320, val loss: 0.7011949419975281
Epoch 330, training loss: 0.8672528862953186 = 0.23146337270736694 + 0.1 * 6.3578948974609375
Epoch 330, val loss: 0.6923559308052063
Epoch 340, training loss: 0.8418415188789368 = 0.20662440359592438 + 0.1 * 6.352170944213867
Epoch 340, val loss: 0.685502290725708
Epoch 350, training loss: 0.8196679353713989 = 0.18464672565460205 + 0.1 * 6.350212097167969
Epoch 350, val loss: 0.6807810068130493
Epoch 360, training loss: 0.7994112968444824 = 0.16525311768054962 + 0.1 * 6.34158182144165
Epoch 360, val loss: 0.6780123114585876
Epoch 370, training loss: 0.7838610410690308 = 0.14830341935157776 + 0.1 * 6.355576515197754
Epoch 370, val loss: 0.6769989132881165
Epoch 380, training loss: 0.7670779228210449 = 0.13353827595710754 + 0.1 * 6.335395812988281
Epoch 380, val loss: 0.6777549982070923
Epoch 390, training loss: 0.754274308681488 = 0.1205776184797287 + 0.1 * 6.336966514587402
Epoch 390, val loss: 0.6798747181892395
Epoch 400, training loss: 0.7417871952056885 = 0.10919460654258728 + 0.1 * 6.325926303863525
Epoch 400, val loss: 0.6832693815231323
Epoch 410, training loss: 0.7307272553443909 = 0.0991535633802414 + 0.1 * 6.315736770629883
Epoch 410, val loss: 0.6875959634780884
Epoch 420, training loss: 0.7224151492118835 = 0.09027228504419327 + 0.1 * 6.321428298950195
Epoch 420, val loss: 0.6928194761276245
Epoch 430, training loss: 0.7154561281204224 = 0.08241961151361465 + 0.1 * 6.33036470413208
Epoch 430, val loss: 0.6986550092697144
Epoch 440, training loss: 0.706074059009552 = 0.07547596842050552 + 0.1 * 6.305980682373047
Epoch 440, val loss: 0.7050255537033081
Epoch 450, training loss: 0.6998080015182495 = 0.06929902732372284 + 0.1 * 6.305089473724365
Epoch 450, val loss: 0.7118039131164551
Epoch 460, training loss: 0.693610668182373 = 0.06379992514848709 + 0.1 * 6.298107624053955
Epoch 460, val loss: 0.7188079357147217
Epoch 470, training loss: 0.6881899833679199 = 0.058894552290439606 + 0.1 * 6.292953968048096
Epoch 470, val loss: 0.7260384559631348
Epoch 480, training loss: 0.6837596893310547 = 0.054497331380844116 + 0.1 * 6.292623996734619
Epoch 480, val loss: 0.7334076762199402
Epoch 490, training loss: 0.6806600689888 = 0.05055537447333336 + 0.1 * 6.301046848297119
Epoch 490, val loss: 0.740669846534729
Epoch 500, training loss: 0.6755578517913818 = 0.04702061787247658 + 0.1 * 6.285372257232666
Epoch 500, val loss: 0.7480986714363098
Epoch 510, training loss: 0.6712801456451416 = 0.0438249409198761 + 0.1 * 6.2745513916015625
Epoch 510, val loss: 0.7554569244384766
Epoch 520, training loss: 0.6703208684921265 = 0.0409250482916832 + 0.1 * 6.2939581871032715
Epoch 520, val loss: 0.7628242373466492
Epoch 530, training loss: 0.6661970019340515 = 0.038297515362501144 + 0.1 * 6.278994560241699
Epoch 530, val loss: 0.7700793147087097
Epoch 540, training loss: 0.6628166437149048 = 0.03591153025627136 + 0.1 * 6.269051551818848
Epoch 540, val loss: 0.7773546576499939
Epoch 550, training loss: 0.6605249643325806 = 0.03372950479388237 + 0.1 * 6.267954349517822
Epoch 550, val loss: 0.7845581769943237
Epoch 560, training loss: 0.6584499478340149 = 0.03173528611660004 + 0.1 * 6.267146110534668
Epoch 560, val loss: 0.7915863394737244
Epoch 570, training loss: 0.6564095616340637 = 0.029911017045378685 + 0.1 * 6.264985084533691
Epoch 570, val loss: 0.7986758947372437
Epoch 580, training loss: 0.6554797291755676 = 0.028233055025339127 + 0.1 * 6.272466659545898
Epoch 580, val loss: 0.8056435585021973
Epoch 590, training loss: 0.6520594358444214 = 0.026689616963267326 + 0.1 * 6.253698348999023
Epoch 590, val loss: 0.8124530911445618
Epoch 600, training loss: 0.6503816246986389 = 0.025265837088227272 + 0.1 * 6.251158237457275
Epoch 600, val loss: 0.8192901015281677
Epoch 610, training loss: 0.6491087675094604 = 0.023946963250637054 + 0.1 * 6.251617908477783
Epoch 610, val loss: 0.8260078430175781
Epoch 620, training loss: 0.6482897400856018 = 0.02272714674472809 + 0.1 * 6.255626201629639
Epoch 620, val loss: 0.8326139450073242
Epoch 630, training loss: 0.6460152864456177 = 0.02159866690635681 + 0.1 * 6.244166374206543
Epoch 630, val loss: 0.8391835689544678
Epoch 640, training loss: 0.6454818844795227 = 0.020550059154629707 + 0.1 * 6.2493181228637695
Epoch 640, val loss: 0.8457064628601074
Epoch 650, training loss: 0.643728494644165 = 0.01957455277442932 + 0.1 * 6.241539478302002
Epoch 650, val loss: 0.8520469069480896
Epoch 660, training loss: 0.6430768370628357 = 0.01866883412003517 + 0.1 * 6.244080066680908
Epoch 660, val loss: 0.8583545088768005
Epoch 670, training loss: 0.6411819458007812 = 0.017825493589043617 + 0.1 * 6.233564376831055
Epoch 670, val loss: 0.8646171689033508
Epoch 680, training loss: 0.6412903070449829 = 0.017037004232406616 + 0.1 * 6.242533206939697
Epoch 680, val loss: 0.870715856552124
Epoch 690, training loss: 0.6408282518386841 = 0.016301631927490234 + 0.1 * 6.245265960693359
Epoch 690, val loss: 0.8766739964485168
Epoch 700, training loss: 0.639001727104187 = 0.015615610405802727 + 0.1 * 6.233861446380615
Epoch 700, val loss: 0.8825746774673462
Epoch 710, training loss: 0.6367993354797363 = 0.01497456431388855 + 0.1 * 6.218247413635254
Epoch 710, val loss: 0.8884426355361938
Epoch 720, training loss: 0.637100338935852 = 0.014371546916663647 + 0.1 * 6.227287769317627
Epoch 720, val loss: 0.8941721320152283
Epoch 730, training loss: 0.6364503502845764 = 0.013803922571241856 + 0.1 * 6.22646427154541
Epoch 730, val loss: 0.8997455835342407
Epoch 740, training loss: 0.6349301338195801 = 0.013272193260490894 + 0.1 * 6.216579437255859
Epoch 740, val loss: 0.9052160978317261
Epoch 750, training loss: 0.6344743967056274 = 0.01277353148907423 + 0.1 * 6.217008590698242
Epoch 750, val loss: 0.910706102848053
Epoch 760, training loss: 0.6341765522956848 = 0.012302963063120842 + 0.1 * 6.218736171722412
Epoch 760, val loss: 0.9160223007202148
Epoch 770, training loss: 0.6331930160522461 = 0.01185913197696209 + 0.1 * 6.213338851928711
Epoch 770, val loss: 0.9212355613708496
Epoch 780, training loss: 0.6319899559020996 = 0.011440074071288109 + 0.1 * 6.205498695373535
Epoch 780, val loss: 0.9262892007827759
Epoch 790, training loss: 0.6315891146659851 = 0.011046351864933968 + 0.1 * 6.205427646636963
Epoch 790, val loss: 0.9313831925392151
Epoch 800, training loss: 0.6307869553565979 = 0.010672986507415771 + 0.1 * 6.201139450073242
Epoch 800, val loss: 0.9362843036651611
Epoch 810, training loss: 0.6315938830375671 = 0.010320169851183891 + 0.1 * 6.212737083435059
Epoch 810, val loss: 0.9411458373069763
Epoch 820, training loss: 0.6295319199562073 = 0.009985207580029964 + 0.1 * 6.195466995239258
Epoch 820, val loss: 0.94582599401474
Epoch 830, training loss: 0.6293714046478271 = 0.00966843031346798 + 0.1 * 6.197030067443848
Epoch 830, val loss: 0.9505722522735596
Epoch 840, training loss: 0.629571795463562 = 0.009366356767714024 + 0.1 * 6.202054500579834
Epoch 840, val loss: 0.9550888538360596
Epoch 850, training loss: 0.6288017630577087 = 0.009080387651920319 + 0.1 * 6.197213172912598
Epoch 850, val loss: 0.9595809578895569
Epoch 860, training loss: 0.6283239126205444 = 0.008808552287518978 + 0.1 * 6.195153713226318
Epoch 860, val loss: 0.9640989899635315
Epoch 870, training loss: 0.6279359459877014 = 0.0085490383207798 + 0.1 * 6.193869113922119
Epoch 870, val loss: 0.9683623313903809
Epoch 880, training loss: 0.6270716786384583 = 0.008302630856633186 + 0.1 * 6.187690258026123
Epoch 880, val loss: 0.9726601839065552
Epoch 890, training loss: 0.6269109845161438 = 0.008067937567830086 + 0.1 * 6.188430309295654
Epoch 890, val loss: 0.9768168330192566
Epoch 900, training loss: 0.626785159111023 = 0.007845127955079079 + 0.1 * 6.1894001960754395
Epoch 900, val loss: 0.9809762239456177
Epoch 910, training loss: 0.6274116635322571 = 0.007631357759237289 + 0.1 * 6.197803020477295
Epoch 910, val loss: 0.9850480556488037
Epoch 920, training loss: 0.6258847117424011 = 0.0074271815828979015 + 0.1 * 6.18457555770874
Epoch 920, val loss: 0.9889858961105347
Epoch 930, training loss: 0.6253549456596375 = 0.007232094649225473 + 0.1 * 6.1812286376953125
Epoch 930, val loss: 0.992835283279419
Epoch 940, training loss: 0.6246494054794312 = 0.007047048769891262 + 0.1 * 6.176023483276367
Epoch 940, val loss: 0.9967278242111206
Epoch 950, training loss: 0.624428391456604 = 0.006869721692055464 + 0.1 * 6.175586223602295
Epoch 950, val loss: 1.0006353855133057
Epoch 960, training loss: 0.623799204826355 = 0.006699008867144585 + 0.1 * 6.171001434326172
Epoch 960, val loss: 1.004414677619934
Epoch 970, training loss: 0.6253714561462402 = 0.006534304469823837 + 0.1 * 6.188371658325195
Epoch 970, val loss: 1.0080759525299072
Epoch 980, training loss: 0.6233903765678406 = 0.006376804783940315 + 0.1 * 6.170135498046875
Epoch 980, val loss: 1.0116746425628662
Epoch 990, training loss: 0.6240859031677246 = 0.006226684898138046 + 0.1 * 6.178592205047607
Epoch 990, val loss: 1.0153659582138062
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5720
Flip ASR: 0.5067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.800743341445923 = 1.9633538722991943 + 0.1 * 8.373894691467285
Epoch 0, val loss: 1.9635595083236694
Epoch 10, training loss: 2.7903881072998047 = 1.9530062675476074 + 0.1 * 8.373818397521973
Epoch 10, val loss: 1.9537439346313477
Epoch 20, training loss: 2.777895927429199 = 1.9405527114868164 + 0.1 * 8.373431205749512
Epoch 20, val loss: 1.9415920972824097
Epoch 30, training loss: 2.7599852085113525 = 1.922874093055725 + 0.1 * 8.371110916137695
Epoch 30, val loss: 1.92399001121521
Epoch 40, training loss: 2.731405735015869 = 1.896177887916565 + 0.1 * 8.352279663085938
Epoch 40, val loss: 1.8974987268447876
Epoch 50, training loss: 2.677980422973633 = 1.8577207326889038 + 0.1 * 8.202596664428711
Epoch 50, val loss: 1.8608343601226807
Epoch 60, training loss: 2.583437204360962 = 1.8151746988296509 + 0.1 * 7.682625770568848
Epoch 60, val loss: 1.8229261636734009
Epoch 70, training loss: 2.490586757659912 = 1.7781968116760254 + 0.1 * 7.123899459838867
Epoch 70, val loss: 1.7911620140075684
Epoch 80, training loss: 2.428999662399292 = 1.7447069883346558 + 0.1 * 6.842926979064941
Epoch 80, val loss: 1.7616300582885742
Epoch 90, training loss: 2.3797099590301514 = 1.7037513256072998 + 0.1 * 6.759585380554199
Epoch 90, val loss: 1.7235523462295532
Epoch 100, training loss: 2.320199728012085 = 1.6490259170532227 + 0.1 * 6.711737632751465
Epoch 100, val loss: 1.6756013631820679
Epoch 110, training loss: 2.246283531188965 = 1.5778168439865112 + 0.1 * 6.684666633605957
Epoch 110, val loss: 1.6176589727401733
Epoch 120, training loss: 2.1572957038879395 = 1.4909790754318237 + 0.1 * 6.66316556930542
Epoch 120, val loss: 1.5484683513641357
Epoch 130, training loss: 2.059098958969116 = 1.3945773839950562 + 0.1 * 6.6452155113220215
Epoch 130, val loss: 1.4712144136428833
Epoch 140, training loss: 1.955568790435791 = 1.2927595376968384 + 0.1 * 6.628092288970947
Epoch 140, val loss: 1.3894374370574951
Epoch 150, training loss: 1.8488045930862427 = 1.1870063543319702 + 0.1 * 6.617982387542725
Epoch 150, val loss: 1.3049325942993164
Epoch 160, training loss: 1.7416293621063232 = 1.0815507173538208 + 0.1 * 6.600785732269287
Epoch 160, val loss: 1.2201505899429321
Epoch 170, training loss: 1.6359961032867432 = 0.9776034355163574 + 0.1 * 6.583925724029541
Epoch 170, val loss: 1.1356117725372314
Epoch 180, training loss: 1.5374367237091064 = 0.8801287412643433 + 0.1 * 6.5730791091918945
Epoch 180, val loss: 1.056015968322754
Epoch 190, training loss: 1.4492511749267578 = 0.7931498289108276 + 0.1 * 6.561013698577881
Epoch 190, val loss: 0.9851256012916565
Epoch 200, training loss: 1.3734056949615479 = 0.7178016901016235 + 0.1 * 6.556040287017822
Epoch 200, val loss: 0.924936056137085
Epoch 210, training loss: 1.3071401119232178 = 0.6532857418060303 + 0.1 * 6.538543224334717
Epoch 210, val loss: 0.8757711052894592
Epoch 220, training loss: 1.2487092018127441 = 0.5961663126945496 + 0.1 * 6.5254292488098145
Epoch 220, val loss: 0.8346437811851501
Epoch 230, training loss: 1.196242332458496 = 0.5441911816596985 + 0.1 * 6.520511627197266
Epoch 230, val loss: 0.7997637391090393
Epoch 240, training loss: 1.148228645324707 = 0.4964599609375 + 0.1 * 6.51768684387207
Epoch 240, val loss: 0.770172655582428
Epoch 250, training loss: 1.1017006635665894 = 0.45199117064476013 + 0.1 * 6.497095108032227
Epoch 250, val loss: 0.7447384595870972
Epoch 260, training loss: 1.0586464405059814 = 0.4099273681640625 + 0.1 * 6.487189769744873
Epoch 260, val loss: 0.7225161194801331
Epoch 270, training loss: 1.0204507112503052 = 0.3703536093235016 + 0.1 * 6.500970840454102
Epoch 270, val loss: 0.7034361958503723
Epoch 280, training loss: 0.9816979169845581 = 0.3340383768081665 + 0.1 * 6.476595401763916
Epoch 280, val loss: 0.6879593729972839
Epoch 290, training loss: 0.9470856189727783 = 0.30103054642677307 + 0.1 * 6.460550308227539
Epoch 290, val loss: 0.6757681369781494
Epoch 300, training loss: 0.9176649451255798 = 0.27131474018096924 + 0.1 * 6.463501930236816
Epoch 300, val loss: 0.6668460965156555
Epoch 310, training loss: 0.8892600536346436 = 0.244846910238266 + 0.1 * 6.444131851196289
Epoch 310, val loss: 0.6608179211616516
Epoch 320, training loss: 0.8648315072059631 = 0.2209373265504837 + 0.1 * 6.438941955566406
Epoch 320, val loss: 0.65729820728302
Epoch 330, training loss: 0.8418464660644531 = 0.19912247359752655 + 0.1 * 6.427239894866943
Epoch 330, val loss: 0.6559612154960632
Epoch 340, training loss: 0.8225647211074829 = 0.17905206978321075 + 0.1 * 6.435126781463623
Epoch 340, val loss: 0.6564269661903381
Epoch 350, training loss: 0.8015424013137817 = 0.16057313978672028 + 0.1 * 6.409692764282227
Epoch 350, val loss: 0.6586002707481384
Epoch 360, training loss: 0.7855267524719238 = 0.1436682790517807 + 0.1 * 6.41858434677124
Epoch 360, val loss: 0.6622132658958435
Epoch 370, training loss: 0.7676964998245239 = 0.1284545660018921 + 0.1 * 6.392419338226318
Epoch 370, val loss: 0.6669294834136963
Epoch 380, training loss: 0.7532536387443542 = 0.11489330232143402 + 0.1 * 6.383603096008301
Epoch 380, val loss: 0.6727309226989746
Epoch 390, training loss: 0.7404701113700867 = 0.10295792669057846 + 0.1 * 6.3751220703125
Epoch 390, val loss: 0.6793649792671204
Epoch 400, training loss: 0.7296121120452881 = 0.09255917370319366 + 0.1 * 6.3705291748046875
Epoch 400, val loss: 0.6865257620811462
Epoch 410, training loss: 0.7198564410209656 = 0.08351390808820724 + 0.1 * 6.363425254821777
Epoch 410, val loss: 0.694294810295105
Epoch 420, training loss: 0.7114070057868958 = 0.07565803080797195 + 0.1 * 6.357489585876465
Epoch 420, val loss: 0.7022914290428162
Epoch 430, training loss: 0.7039872407913208 = 0.06882044672966003 + 0.1 * 6.351668357849121
Epoch 430, val loss: 0.7106792330741882
Epoch 440, training loss: 0.6971111297607422 = 0.06284549087285995 + 0.1 * 6.34265661239624
Epoch 440, val loss: 0.719232976436615
Epoch 450, training loss: 0.6913853287696838 = 0.05759907513856888 + 0.1 * 6.337862491607666
Epoch 450, val loss: 0.7280287742614746
Epoch 460, training loss: 0.6857861280441284 = 0.052987463772296906 + 0.1 * 6.327986717224121
Epoch 460, val loss: 0.7367185950279236
Epoch 470, training loss: 0.6809911727905273 = 0.048902884125709534 + 0.1 * 6.320882797241211
Epoch 470, val loss: 0.7454420924186707
Epoch 480, training loss: 0.6772675514221191 = 0.04526988044381142 + 0.1 * 6.319976329803467
Epoch 480, val loss: 0.7541429400444031
Epoch 490, training loss: 0.6728954911231995 = 0.04203887656331062 + 0.1 * 6.308566093444824
Epoch 490, val loss: 0.7626065015792847
Epoch 500, training loss: 0.6718125343322754 = 0.039139214903116226 + 0.1 * 6.326733112335205
Epoch 500, val loss: 0.7710936665534973
Epoch 510, training loss: 0.6667116284370422 = 0.03653326258063316 + 0.1 * 6.301783561706543
Epoch 510, val loss: 0.7792656421661377
Epoch 520, training loss: 0.6635288000106812 = 0.03417707234621048 + 0.1 * 6.293517112731934
Epoch 520, val loss: 0.7874197959899902
Epoch 530, training loss: 0.6626765727996826 = 0.03203600272536278 + 0.1 * 6.306405544281006
Epoch 530, val loss: 0.7954935431480408
Epoch 540, training loss: 0.659858226776123 = 0.03009738214313984 + 0.1 * 6.297608375549316
Epoch 540, val loss: 0.8034806251525879
Epoch 550, training loss: 0.656462550163269 = 0.028333021327853203 + 0.1 * 6.281295299530029
Epoch 550, val loss: 0.8112512826919556
Epoch 560, training loss: 0.6550573706626892 = 0.026720045134425163 + 0.1 * 6.2833733558654785
Epoch 560, val loss: 0.8189553022384644
Epoch 570, training loss: 0.6531467437744141 = 0.02524513192474842 + 0.1 * 6.279016017913818
Epoch 570, val loss: 0.8265023827552795
Epoch 580, training loss: 0.6521618962287903 = 0.02388984151184559 + 0.1 * 6.282720565795898
Epoch 580, val loss: 0.8340644240379333
Epoch 590, training loss: 0.649156928062439 = 0.022646820172667503 + 0.1 * 6.265100479125977
Epoch 590, val loss: 0.8413028120994568
Epoch 600, training loss: 0.6482669711112976 = 0.02150128223001957 + 0.1 * 6.2676568031311035
Epoch 600, val loss: 0.8484886884689331
Epoch 610, training loss: 0.6461070775985718 = 0.020440733060240746 + 0.1 * 6.2566633224487305
Epoch 610, val loss: 0.8555836081504822
Epoch 620, training loss: 0.6458508968353271 = 0.019459422677755356 + 0.1 * 6.263914585113525
Epoch 620, val loss: 0.862461507320404
Epoch 630, training loss: 0.6438886523246765 = 0.01854880340397358 + 0.1 * 6.253398418426514
Epoch 630, val loss: 0.8692964315414429
Epoch 640, training loss: 0.6425091624259949 = 0.0177062526345253 + 0.1 * 6.248028755187988
Epoch 640, val loss: 0.8758853077888489
Epoch 650, training loss: 0.641556978225708 = 0.016920235008001328 + 0.1 * 6.246367454528809
Epoch 650, val loss: 0.8824483156204224
Epoch 660, training loss: 0.6419582366943359 = 0.016187617555260658 + 0.1 * 6.257706165313721
Epoch 660, val loss: 0.8889130353927612
Epoch 670, training loss: 0.6398046612739563 = 0.015506409108638763 + 0.1 * 6.242982387542725
Epoch 670, val loss: 0.8951525092124939
Epoch 680, training loss: 0.6386139392852783 = 0.01487058401107788 + 0.1 * 6.237433433532715
Epoch 680, val loss: 0.9013456106185913
Epoch 690, training loss: 0.6374959945678711 = 0.014272998087108135 + 0.1 * 6.232230186462402
Epoch 690, val loss: 0.907346248626709
Epoch 700, training loss: 0.6376879811286926 = 0.013711233623325825 + 0.1 * 6.239767551422119
Epoch 700, val loss: 0.9133459329605103
Epoch 710, training loss: 0.6362799406051636 = 0.01318373903632164 + 0.1 * 6.23096227645874
Epoch 710, val loss: 0.9192171692848206
Epoch 720, training loss: 0.6368338465690613 = 0.012687759473919868 + 0.1 * 6.241460800170898
Epoch 720, val loss: 0.9250590205192566
Epoch 730, training loss: 0.6343281865119934 = 0.012220442295074463 + 0.1 * 6.2210774421691895
Epoch 730, val loss: 0.9307265877723694
Epoch 740, training loss: 0.6333150863647461 = 0.011779740452766418 + 0.1 * 6.215353488922119
Epoch 740, val loss: 0.9362446665763855
Epoch 750, training loss: 0.6333755254745483 = 0.011362259276211262 + 0.1 * 6.220132350921631
Epoch 750, val loss: 0.9417679309844971
Epoch 760, training loss: 0.6323005557060242 = 0.010966393165290356 + 0.1 * 6.213341236114502
Epoch 760, val loss: 0.947219729423523
Epoch 770, training loss: 0.6329687833786011 = 0.010594680905342102 + 0.1 * 6.223741054534912
Epoch 770, val loss: 0.9524700045585632
Epoch 780, training loss: 0.6317089796066284 = 0.010243301279842854 + 0.1 * 6.214656829833984
Epoch 780, val loss: 0.9576108455657959
Epoch 790, training loss: 0.6301949620246887 = 0.00991020631045103 + 0.1 * 6.202847003936768
Epoch 790, val loss: 0.9626650214195251
Epoch 800, training loss: 0.6310898661613464 = 0.009594171307981014 + 0.1 * 6.214957237243652
Epoch 800, val loss: 0.967605710029602
Epoch 810, training loss: 0.6294128894805908 = 0.009293931536376476 + 0.1 * 6.2011895179748535
Epoch 810, val loss: 0.9724962115287781
Epoch 820, training loss: 0.6293609142303467 = 0.009008493274450302 + 0.1 * 6.203524112701416
Epoch 820, val loss: 0.9772422313690186
Epoch 830, training loss: 0.6287267804145813 = 0.008736838586628437 + 0.1 * 6.199899196624756
Epoch 830, val loss: 0.9819368720054626
Epoch 840, training loss: 0.6281379461288452 = 0.008478610776364803 + 0.1 * 6.196593284606934
Epoch 840, val loss: 0.9866025447845459
Epoch 850, training loss: 0.6273751854896545 = 0.008232291787862778 + 0.1 * 6.1914286613464355
Epoch 850, val loss: 0.9911458492279053
Epoch 860, training loss: 0.6282610297203064 = 0.007998228073120117 + 0.1 * 6.202627658843994
Epoch 860, val loss: 0.9956156611442566
Epoch 870, training loss: 0.6271706223487854 = 0.007773784454911947 + 0.1 * 6.193968296051025
Epoch 870, val loss: 1.0001047849655151
Epoch 880, training loss: 0.6270585656166077 = 0.00756082171574235 + 0.1 * 6.194977283477783
Epoch 880, val loss: 1.0043740272521973
Epoch 890, training loss: 0.6260198354721069 = 0.007357277907431126 + 0.1 * 6.186625003814697
Epoch 890, val loss: 1.0086337327957153
Epoch 900, training loss: 0.6255049109458923 = 0.007162503898143768 + 0.1 * 6.18342399597168
Epoch 900, val loss: 1.0128320455551147
Epoch 910, training loss: 0.6266665458679199 = 0.006975960917770863 + 0.1 * 6.196905612945557
Epoch 910, val loss: 1.016944408416748
Epoch 920, training loss: 0.6255005598068237 = 0.006797099485993385 + 0.1 * 6.187034606933594
Epoch 920, val loss: 1.0210976600646973
Epoch 930, training loss: 0.6253136992454529 = 0.006627079099416733 + 0.1 * 6.18686580657959
Epoch 930, val loss: 1.0250294208526611
Epoch 940, training loss: 0.6247164011001587 = 0.006463443394750357 + 0.1 * 6.182528972625732
Epoch 940, val loss: 1.0290502309799194
Epoch 950, training loss: 0.6242942810058594 = 0.006306936964392662 + 0.1 * 6.179873466491699
Epoch 950, val loss: 1.0329723358154297
Epoch 960, training loss: 0.624470591545105 = 0.006156729534268379 + 0.1 * 6.183138370513916
Epoch 960, val loss: 1.036784291267395
Epoch 970, training loss: 0.6244029402732849 = 0.006011981517076492 + 0.1 * 6.1839094161987305
Epoch 970, val loss: 1.0406513214111328
Epoch 980, training loss: 0.6233993172645569 = 0.005873433314263821 + 0.1 * 6.175258636474609
Epoch 980, val loss: 1.0444358587265015
Epoch 990, training loss: 0.6227888464927673 = 0.005740238353610039 + 0.1 * 6.170485973358154
Epoch 990, val loss: 1.0480835437774658
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8081
Flip ASR: 0.7689/225 nodes
The final ASR:0.64207, 0.11789, Accuracy:0.79877, 0.03331
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10560])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98647, 0.00870, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.785396099090576 = 1.9480100870132446 + 0.1 * 8.373861312866211
Epoch 0, val loss: 1.9474073648452759
Epoch 10, training loss: 2.774972915649414 = 1.937601923942566 + 0.1 * 8.373708724975586
Epoch 10, val loss: 1.9368122816085815
Epoch 20, training loss: 2.76204252243042 = 1.9247604608535767 + 0.1 * 8.372821807861328
Epoch 20, val loss: 1.923507809638977
Epoch 30, training loss: 2.7433888912200928 = 1.9067319631576538 + 0.1 * 8.366568565368652
Epoch 30, val loss: 1.9047424793243408
Epoch 40, training loss: 2.7133443355560303 = 1.8799543380737305 + 0.1 * 8.333900451660156
Epoch 40, val loss: 1.8774387836456299
Epoch 50, training loss: 2.6611342430114746 = 1.8436458110809326 + 0.1 * 8.174882888793945
Epoch 50, val loss: 1.8424650430679321
Epoch 60, training loss: 2.593533992767334 = 1.8030246496200562 + 0.1 * 7.905092716217041
Epoch 60, val loss: 1.8064377307891846
Epoch 70, training loss: 2.5300087928771973 = 1.7632417678833008 + 0.1 * 7.667671203613281
Epoch 70, val loss: 1.7740423679351807
Epoch 80, training loss: 2.452845573425293 = 1.7207629680633545 + 0.1 * 7.320826053619385
Epoch 80, val loss: 1.7390754222869873
Epoch 90, training loss: 2.3736720085144043 = 1.6681232452392578 + 0.1 * 7.055487155914307
Epoch 90, val loss: 1.694913625717163
Epoch 100, training loss: 2.2909553050994873 = 1.6002212762832642 + 0.1 * 6.907339572906494
Epoch 100, val loss: 1.6372179985046387
Epoch 110, training loss: 2.1998372077941895 = 1.5182139873504639 + 0.1 * 6.8162312507629395
Epoch 110, val loss: 1.5687651634216309
Epoch 120, training loss: 2.105792999267578 = 1.4277762174606323 + 0.1 * 6.780167579650879
Epoch 120, val loss: 1.4948794841766357
Epoch 130, training loss: 2.009826421737671 = 1.333888053894043 + 0.1 * 6.7593841552734375
Epoch 130, val loss: 1.4202042818069458
Epoch 140, training loss: 1.912442684173584 = 1.2378727197647095 + 0.1 * 6.745698928833008
Epoch 140, val loss: 1.3442041873931885
Epoch 150, training loss: 1.8142708539962769 = 1.1407489776611328 + 0.1 * 6.735218524932861
Epoch 150, val loss: 1.268842101097107
Epoch 160, training loss: 1.7176871299743652 = 1.0451717376708984 + 0.1 * 6.725154399871826
Epoch 160, val loss: 1.1957708597183228
Epoch 170, training loss: 1.6260581016540527 = 0.9542905688285828 + 0.1 * 6.717675685882568
Epoch 170, val loss: 1.1277854442596436
Epoch 180, training loss: 1.5408940315246582 = 0.8700351119041443 + 0.1 * 6.708589553833008
Epoch 180, val loss: 1.066129207611084
Epoch 190, training loss: 1.4614295959472656 = 0.7914711833000183 + 0.1 * 6.699583530426025
Epoch 190, val loss: 1.009419560432434
Epoch 200, training loss: 1.386327862739563 = 0.7172636389732361 + 0.1 * 6.6906418800354
Epoch 200, val loss: 0.9563215374946594
Epoch 210, training loss: 1.3143119812011719 = 0.6461355090141296 + 0.1 * 6.681764125823975
Epoch 210, val loss: 0.905901312828064
Epoch 220, training loss: 1.2440125942230225 = 0.5769493579864502 + 0.1 * 6.670632839202881
Epoch 220, val loss: 0.8572403788566589
Epoch 230, training loss: 1.1762402057647705 = 0.5101739168167114 + 0.1 * 6.660662651062012
Epoch 230, val loss: 0.8110339641571045
Epoch 240, training loss: 1.1133034229278564 = 0.4473586082458496 + 0.1 * 6.65944766998291
Epoch 240, val loss: 0.7690659165382385
Epoch 250, training loss: 1.0545194149017334 = 0.39047670364379883 + 0.1 * 6.640427112579346
Epoch 250, val loss: 0.7331700921058655
Epoch 260, training loss: 1.0043108463287354 = 0.3401842415332794 + 0.1 * 6.641265869140625
Epoch 260, val loss: 0.7038893103599548
Epoch 270, training loss: 0.9590840339660645 = 0.296913743019104 + 0.1 * 6.621702671051025
Epoch 270, val loss: 0.6811521053314209
Epoch 280, training loss: 0.9203890562057495 = 0.2596457898616791 + 0.1 * 6.607432842254639
Epoch 280, val loss: 0.6637720465660095
Epoch 290, training loss: 0.8868982791900635 = 0.22738055884838104 + 0.1 * 6.595177173614502
Epoch 290, val loss: 0.6509068012237549
Epoch 300, training loss: 0.858034610748291 = 0.1996786892414093 + 0.1 * 6.583559513092041
Epoch 300, val loss: 0.6418517231941223
Epoch 310, training loss: 0.8327924609184265 = 0.17584551870822906 + 0.1 * 6.569469451904297
Epoch 310, val loss: 0.635746419429779
Epoch 320, training loss: 0.8106239438056946 = 0.1551266312599182 + 0.1 * 6.554973125457764
Epoch 320, val loss: 0.6322201490402222
Epoch 330, training loss: 0.7930793762207031 = 0.13722726702690125 + 0.1 * 6.558521270751953
Epoch 330, val loss: 0.6308333873748779
Epoch 340, training loss: 0.776059627532959 = 0.1218445748090744 + 0.1 * 6.542150020599365
Epoch 340, val loss: 0.6314588189125061
Epoch 350, training loss: 0.7616170644760132 = 0.1086663156747818 + 0.1 * 6.529507637023926
Epoch 350, val loss: 0.6338782906532288
Epoch 360, training loss: 0.7490207552909851 = 0.09733425825834274 + 0.1 * 6.516864776611328
Epoch 360, val loss: 0.6376736760139465
Epoch 370, training loss: 0.738469660282135 = 0.0875241681933403 + 0.1 * 6.509454727172852
Epoch 370, val loss: 0.6426606178283691
Epoch 380, training loss: 0.7285563349723816 = 0.07902168482542038 + 0.1 * 6.495346546173096
Epoch 380, val loss: 0.6485417485237122
Epoch 390, training loss: 0.720767080783844 = 0.07163422554731369 + 0.1 * 6.491328239440918
Epoch 390, val loss: 0.6550574898719788
Epoch 400, training loss: 0.7144477367401123 = 0.06518182158470154 + 0.1 * 6.492658615112305
Epoch 400, val loss: 0.6620645523071289
Epoch 410, training loss: 0.7068332433700562 = 0.05952044203877449 + 0.1 * 6.473128318786621
Epoch 410, val loss: 0.6693604588508606
Epoch 420, training loss: 0.6996446251869202 = 0.05451766029000282 + 0.1 * 6.451269149780273
Epoch 420, val loss: 0.6768928170204163
Epoch 430, training loss: 0.6974589824676514 = 0.05010814964771271 + 0.1 * 6.473508358001709
Epoch 430, val loss: 0.6844044327735901
Epoch 440, training loss: 0.6907216906547546 = 0.04622987285256386 + 0.1 * 6.444917678833008
Epoch 440, val loss: 0.6922157406806946
Epoch 450, training loss: 0.6878687143325806 = 0.0427682027220726 + 0.1 * 6.451004981994629
Epoch 450, val loss: 0.6997267603874207
Epoch 460, training loss: 0.6818236112594604 = 0.03968511149287224 + 0.1 * 6.421384811401367
Epoch 460, val loss: 0.7073610424995422
Epoch 470, training loss: 0.6786808371543884 = 0.03692080080509186 + 0.1 * 6.417600631713867
Epoch 470, val loss: 0.7149388790130615
Epoch 480, training loss: 0.6765703558921814 = 0.034431617707014084 + 0.1 * 6.421387195587158
Epoch 480, val loss: 0.7223495244979858
Epoch 490, training loss: 0.6725735068321228 = 0.03219294175505638 + 0.1 * 6.403805732727051
Epoch 490, val loss: 0.7298635840415955
Epoch 500, training loss: 0.6699822545051575 = 0.03016653098165989 + 0.1 * 6.398157119750977
Epoch 500, val loss: 0.7370883226394653
Epoch 510, training loss: 0.6690087914466858 = 0.028334317728877068 + 0.1 * 6.406744480133057
Epoch 510, val loss: 0.7442230582237244
Epoch 520, training loss: 0.6637989282608032 = 0.02667752280831337 + 0.1 * 6.371213912963867
Epoch 520, val loss: 0.7511958479881287
Epoch 530, training loss: 0.6643591523170471 = 0.02516707219183445 + 0.1 * 6.391921043395996
Epoch 530, val loss: 0.7579397559165955
Epoch 540, training loss: 0.6608400344848633 = 0.02378791943192482 + 0.1 * 6.370521068572998
Epoch 540, val loss: 0.7644659876823425
Epoch 550, training loss: 0.6589210629463196 = 0.02252664603292942 + 0.1 * 6.363944053649902
Epoch 550, val loss: 0.7710868716239929
Epoch 560, training loss: 0.6572346687316895 = 0.021364301443099976 + 0.1 * 6.358703136444092
Epoch 560, val loss: 0.7773590683937073
Epoch 570, training loss: 0.6555913090705872 = 0.02029409632086754 + 0.1 * 6.352972030639648
Epoch 570, val loss: 0.7834814786911011
Epoch 580, training loss: 0.6542340517044067 = 0.019309453666210175 + 0.1 * 6.349246025085449
Epoch 580, val loss: 0.7896982431411743
Epoch 590, training loss: 0.652273952960968 = 0.01839625835418701 + 0.1 * 6.338776588439941
Epoch 590, val loss: 0.795559287071228
Epoch 600, training loss: 0.650916337966919 = 0.017550522461533546 + 0.1 * 6.333657741546631
Epoch 600, val loss: 0.8014078140258789
Epoch 610, training loss: 0.6487120389938354 = 0.016765620559453964 + 0.1 * 6.319464206695557
Epoch 610, val loss: 0.8070776462554932
Epoch 620, training loss: 0.6488468050956726 = 0.016034822911024094 + 0.1 * 6.32811975479126
Epoch 620, val loss: 0.8126036524772644
Epoch 630, training loss: 0.646713137626648 = 0.015355003997683525 + 0.1 * 6.313581466674805
Epoch 630, val loss: 0.8181020617485046
Epoch 640, training loss: 0.6455745100975037 = 0.014720392413437366 + 0.1 * 6.308541297912598
Epoch 640, val loss: 0.8233960270881653
Epoch 650, training loss: 0.6447353363037109 = 0.014127122238278389 + 0.1 * 6.306081771850586
Epoch 650, val loss: 0.8286764621734619
Epoch 660, training loss: 0.6431030035018921 = 0.01356966607272625 + 0.1 * 6.295333385467529
Epoch 660, val loss: 0.8338798880577087
Epoch 670, training loss: 0.643635094165802 = 0.013045219704508781 + 0.1 * 6.305899143218994
Epoch 670, val loss: 0.8388679027557373
Epoch 680, training loss: 0.6421293616294861 = 0.012553551234304905 + 0.1 * 6.29575777053833
Epoch 680, val loss: 0.8438265919685364
Epoch 690, training loss: 0.6447705030441284 = 0.012090923264622688 + 0.1 * 6.32679557800293
Epoch 690, val loss: 0.8487038612365723
Epoch 700, training loss: 0.6411140561103821 = 0.011656082235276699 + 0.1 * 6.29457950592041
Epoch 700, val loss: 0.8534725308418274
Epoch 710, training loss: 0.6397964954376221 = 0.011246254667639732 + 0.1 * 6.285501956939697
Epoch 710, val loss: 0.85821932554245
Epoch 720, training loss: 0.6398970484733582 = 0.01085731666535139 + 0.1 * 6.290396690368652
Epoch 720, val loss: 0.8628078103065491
Epoch 730, training loss: 0.6392302513122559 = 0.010491184890270233 + 0.1 * 6.287390232086182
Epoch 730, val loss: 0.8672344088554382
Epoch 740, training loss: 0.6379245519638062 = 0.010144869796931744 + 0.1 * 6.277796268463135
Epoch 740, val loss: 0.871757984161377
Epoch 750, training loss: 0.6378594636917114 = 0.009815757162868977 + 0.1 * 6.280436992645264
Epoch 750, val loss: 0.8761879801750183
Epoch 760, training loss: 0.6365026831626892 = 0.00950284581631422 + 0.1 * 6.269998550415039
Epoch 760, val loss: 0.8803309202194214
Epoch 770, training loss: 0.6373807787895203 = 0.009207258000969887 + 0.1 * 6.281735420227051
Epoch 770, val loss: 0.884623110294342
Epoch 780, training loss: 0.6362968683242798 = 0.008925450034439564 + 0.1 * 6.273714065551758
Epoch 780, val loss: 0.8886988759040833
Epoch 790, training loss: 0.6337553262710571 = 0.00865854136645794 + 0.1 * 6.250967979431152
Epoch 790, val loss: 0.892768144607544
Epoch 800, training loss: 0.6352798938751221 = 0.00840379111468792 + 0.1 * 6.268761157989502
Epoch 800, val loss: 0.8968044519424438
Epoch 810, training loss: 0.6351697444915771 = 0.008160564117133617 + 0.1 * 6.270091533660889
Epoch 810, val loss: 0.9006547927856445
Epoch 820, training loss: 0.6327157020568848 = 0.007929408922791481 + 0.1 * 6.247862815856934
Epoch 820, val loss: 0.9045242071151733
Epoch 830, training loss: 0.635266125202179 = 0.007708793971687555 + 0.1 * 6.275573253631592
Epoch 830, val loss: 0.9083834886550903
Epoch 840, training loss: 0.6322503089904785 = 0.007498191669583321 + 0.1 * 6.247520923614502
Epoch 840, val loss: 0.9120940566062927
Epoch 850, training loss: 0.632049560546875 = 0.0072967177256941795 + 0.1 * 6.247528076171875
Epoch 850, val loss: 0.915855348110199
Epoch 860, training loss: 0.6347750425338745 = 0.007103609386831522 + 0.1 * 6.276714324951172
Epoch 860, val loss: 0.9194603562355042
Epoch 870, training loss: 0.6304615139961243 = 0.006919404026120901 + 0.1 * 6.235421180725098
Epoch 870, val loss: 0.9229136109352112
Epoch 880, training loss: 0.629778265953064 = 0.006743482314050198 + 0.1 * 6.230347633361816
Epoch 880, val loss: 0.9265633821487427
Epoch 890, training loss: 0.6308914422988892 = 0.006573949009180069 + 0.1 * 6.2431745529174805
Epoch 890, val loss: 0.9299344420433044
Epoch 900, training loss: 0.6295393109321594 = 0.006412406452000141 + 0.1 * 6.231268882751465
Epoch 900, val loss: 0.9333277940750122
Epoch 910, training loss: 0.6284261345863342 = 0.006257424596697092 + 0.1 * 6.221687316894531
Epoch 910, val loss: 0.9367886185646057
Epoch 920, training loss: 0.6310002207756042 = 0.006107635330408812 + 0.1 * 6.248925685882568
Epoch 920, val loss: 0.9401293396949768
Epoch 930, training loss: 0.6284273862838745 = 0.005964051466435194 + 0.1 * 6.22463321685791
Epoch 930, val loss: 0.9432927370071411
Epoch 940, training loss: 0.6291091442108154 = 0.005826086271554232 + 0.1 * 6.232830047607422
Epoch 940, val loss: 0.9465470910072327
Epoch 950, training loss: 0.6277207136154175 = 0.005693370476365089 + 0.1 * 6.220273017883301
Epoch 950, val loss: 0.9498013257980347
Epoch 960, training loss: 0.6281203031539917 = 0.005565452855080366 + 0.1 * 6.225548267364502
Epoch 960, val loss: 0.952896237373352
Epoch 970, training loss: 0.6273301839828491 = 0.005442675668746233 + 0.1 * 6.218874931335449
Epoch 970, val loss: 0.9560502767562866
Epoch 980, training loss: 0.6286419034004211 = 0.005324061959981918 + 0.1 * 6.23317813873291
Epoch 980, val loss: 0.9590684771537781
Epoch 990, training loss: 0.6267796754837036 = 0.005210413131862879 + 0.1 * 6.215692520141602
Epoch 990, val loss: 0.9620677828788757
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6790
Flip ASR: 0.6133/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7922868728637695 = 1.9548990726470947 + 0.1 * 8.373876571655273
Epoch 0, val loss: 1.9569144248962402
Epoch 10, training loss: 2.781332492828369 = 1.9439632892608643 + 0.1 * 8.37369155883789
Epoch 10, val loss: 1.9452695846557617
Epoch 20, training loss: 2.7678515911102295 = 1.930610179901123 + 0.1 * 8.372414588928223
Epoch 20, val loss: 1.9309344291687012
Epoch 30, training loss: 2.7481937408447266 = 1.912021279335022 + 0.1 * 8.361724853515625
Epoch 30, val loss: 1.9110826253890991
Epoch 40, training loss: 2.7146525382995605 = 1.8848416805267334 + 0.1 * 8.298108100891113
Epoch 40, val loss: 1.8826438188552856
Epoch 50, training loss: 2.6470024585723877 = 1.8489772081375122 + 0.1 * 7.980252265930176
Epoch 50, val loss: 1.8472217321395874
Epoch 60, training loss: 2.5801711082458496 = 1.8076667785644531 + 0.1 * 7.725042819976807
Epoch 60, val loss: 1.808621883392334
Epoch 70, training loss: 2.502319574356079 = 1.7658233642578125 + 0.1 * 7.364962577819824
Epoch 70, val loss: 1.7714406251907349
Epoch 80, training loss: 2.4367334842681885 = 1.722090721130371 + 0.1 * 7.146427631378174
Epoch 80, val loss: 1.7336149215698242
Epoch 90, training loss: 2.3707406520843506 = 1.6675639152526855 + 0.1 * 7.031767845153809
Epoch 90, val loss: 1.685460090637207
Epoch 100, training loss: 2.2905282974243164 = 1.5961036682128906 + 0.1 * 6.944245338439941
Epoch 100, val loss: 1.6240448951721191
Epoch 110, training loss: 2.193480968475342 = 1.5075629949569702 + 0.1 * 6.859180450439453
Epoch 110, val loss: 1.5509005784988403
Epoch 120, training loss: 2.085442066192627 = 1.4065731763839722 + 0.1 * 6.788688659667969
Epoch 120, val loss: 1.46804940700531
Epoch 130, training loss: 1.9733843803405762 = 1.3001179695129395 + 0.1 * 6.732664585113525
Epoch 130, val loss: 1.3821333646774292
Epoch 140, training loss: 1.8628462553024292 = 1.1945524215698242 + 0.1 * 6.682938098907471
Epoch 140, val loss: 1.298122763633728
Epoch 150, training loss: 1.7585201263427734 = 1.0922518968582153 + 0.1 * 6.662681579589844
Epoch 150, val loss: 1.2175002098083496
Epoch 160, training loss: 1.6615136861801147 = 0.9993024468421936 + 0.1 * 6.622112274169922
Epoch 160, val loss: 1.1452850103378296
Epoch 170, training loss: 1.5738937854766846 = 0.9146419167518616 + 0.1 * 6.592518329620361
Epoch 170, val loss: 1.0807315111160278
Epoch 180, training loss: 1.4974309206008911 = 0.8390815854072571 + 0.1 * 6.583493232727051
Epoch 180, val loss: 1.0250407457351685
Epoch 190, training loss: 1.428072452545166 = 0.7730569243431091 + 0.1 * 6.550154685974121
Epoch 190, val loss: 0.9789480566978455
Epoch 200, training loss: 1.3689088821411133 = 0.7148540019989014 + 0.1 * 6.540549278259277
Epoch 200, val loss: 0.9406248927116394
Epoch 210, training loss: 1.3153373003005981 = 0.6640901565551758 + 0.1 * 6.5124711990356445
Epoch 210, val loss: 0.9101275205612183
Epoch 220, training loss: 1.2706871032714844 = 0.6192305088043213 + 0.1 * 6.5145649909973145
Epoch 220, val loss: 0.8857952952384949
Epoch 230, training loss: 1.2282601594924927 = 0.5799263119697571 + 0.1 * 6.483338356018066
Epoch 230, val loss: 0.8670179843902588
Epoch 240, training loss: 1.1917874813079834 = 0.5449684858322144 + 0.1 * 6.468190670013428
Epoch 240, val loss: 0.8524928092956543
Epoch 250, training loss: 1.1597046852111816 = 0.5136157870292664 + 0.1 * 6.4608893394470215
Epoch 250, val loss: 0.8417759537696838
Epoch 260, training loss: 1.1296651363372803 = 0.4852595627307892 + 0.1 * 6.444055557250977
Epoch 260, val loss: 0.8342504501342773
Epoch 270, training loss: 1.104568362236023 = 0.4588370621204376 + 0.1 * 6.457313060760498
Epoch 270, val loss: 0.8290430307388306
Epoch 280, training loss: 1.07563316822052 = 0.43374887108802795 + 0.1 * 6.418842315673828
Epoch 280, val loss: 0.8258880972862244
Epoch 290, training loss: 1.0502218008041382 = 0.4090104401111603 + 0.1 * 6.412113189697266
Epoch 290, val loss: 0.8241155743598938
Epoch 300, training loss: 1.0249381065368652 = 0.3841519355773926 + 0.1 * 6.407861709594727
Epoch 300, val loss: 0.8237317800521851
Epoch 310, training loss: 1.0012322664260864 = 0.3590092062950134 + 0.1 * 6.4222307205200195
Epoch 310, val loss: 0.8248061537742615
Epoch 320, training loss: 0.9724256992340088 = 0.3339096009731293 + 0.1 * 6.385160446166992
Epoch 320, val loss: 0.8273890614509583
Epoch 330, training loss: 0.946536660194397 = 0.3091367781162262 + 0.1 * 6.373998641967773
Epoch 330, val loss: 0.8314502835273743
Epoch 340, training loss: 0.9232721924781799 = 0.28509122133255005 + 0.1 * 6.381809711456299
Epoch 340, val loss: 0.8369556665420532
Epoch 350, training loss: 0.9020495414733887 = 0.2622673213481903 + 0.1 * 6.39782190322876
Epoch 350, val loss: 0.8434615731239319
Epoch 360, training loss: 0.876743733882904 = 0.24101416766643524 + 0.1 * 6.357295513153076
Epoch 360, val loss: 0.8511177897453308
Epoch 370, training loss: 0.8574469685554504 = 0.2213054746389389 + 0.1 * 6.361414432525635
Epoch 370, val loss: 0.8598600029945374
Epoch 380, training loss: 0.838283896446228 = 0.20314517617225647 + 0.1 * 6.351386547088623
Epoch 380, val loss: 0.8698267936706543
Epoch 390, training loss: 0.8214201927185059 = 0.18658895790576935 + 0.1 * 6.3483123779296875
Epoch 390, val loss: 0.880721926689148
Epoch 400, training loss: 0.8052763938903809 = 0.17153416574001312 + 0.1 * 6.337421894073486
Epoch 400, val loss: 0.8925285339355469
Epoch 410, training loss: 0.7914142608642578 = 0.1578739434480667 + 0.1 * 6.335402965545654
Epoch 410, val loss: 0.905192494392395
Epoch 420, training loss: 0.7796933650970459 = 0.14540472626686096 + 0.1 * 6.342886447906494
Epoch 420, val loss: 0.9185117483139038
Epoch 430, training loss: 0.7671064138412476 = 0.13392630219459534 + 0.1 * 6.33180046081543
Epoch 430, val loss: 0.9324414134025574
Epoch 440, training loss: 0.7555493116378784 = 0.12325040996074677 + 0.1 * 6.322988986968994
Epoch 440, val loss: 0.9468104839324951
Epoch 450, training loss: 0.7444595098495483 = 0.11310520023107529 + 0.1 * 6.313543319702148
Epoch 450, val loss: 0.9617603421211243
Epoch 460, training loss: 0.7345892190933228 = 0.10354752093553543 + 0.1 * 6.3104166984558105
Epoch 460, val loss: 0.9769303202629089
Epoch 470, training loss: 0.728132963180542 = 0.0950496569275856 + 0.1 * 6.330832481384277
Epoch 470, val loss: 0.9924818277359009
Epoch 480, training loss: 0.7185248732566833 = 0.08776038140058517 + 0.1 * 6.307644844055176
Epoch 480, val loss: 1.0081568956375122
Epoch 490, training loss: 0.7110190987586975 = 0.08141888678073883 + 0.1 * 6.296002388000488
Epoch 490, val loss: 1.0241005420684814
Epoch 500, training loss: 0.7053863406181335 = 0.07574523985385895 + 0.1 * 6.296411037445068
Epoch 500, val loss: 1.040024757385254
Epoch 510, training loss: 0.7002601623535156 = 0.07063215225934982 + 0.1 * 6.2962799072265625
Epoch 510, val loss: 1.055778980255127
Epoch 520, training loss: 0.6946425437927246 = 0.06599476933479309 + 0.1 * 6.286478042602539
Epoch 520, val loss: 1.0714199542999268
Epoch 530, training loss: 0.6899260878562927 = 0.0617683045566082 + 0.1 * 6.281578063964844
Epoch 530, val loss: 1.086756706237793
Epoch 540, training loss: 0.6858954429626465 = 0.05790359899401665 + 0.1 * 6.279918670654297
Epoch 540, val loss: 1.1020121574401855
Epoch 550, training loss: 0.6834463477134705 = 0.054356787353754044 + 0.1 * 6.290895462036133
Epoch 550, val loss: 1.117156982421875
Epoch 560, training loss: 0.6794556975364685 = 0.0511048398911953 + 0.1 * 6.28350830078125
Epoch 560, val loss: 1.1320158243179321
Epoch 570, training loss: 0.6748511791229248 = 0.04810769855976105 + 0.1 * 6.267434597015381
Epoch 570, val loss: 1.1468082666397095
Epoch 580, training loss: 0.6727346181869507 = 0.045337822288274765 + 0.1 * 6.27396821975708
Epoch 580, val loss: 1.1613383293151855
Epoch 590, training loss: 0.6697524785995483 = 0.04277515783905983 + 0.1 * 6.269773006439209
Epoch 590, val loss: 1.1757892370224
Epoch 600, training loss: 0.6658307909965515 = 0.04039818421006203 + 0.1 * 6.254325866699219
Epoch 600, val loss: 1.189985990524292
Epoch 610, training loss: 0.6637842655181885 = 0.038189951330423355 + 0.1 * 6.255943298339844
Epoch 610, val loss: 1.2040520906448364
Epoch 620, training loss: 0.6618149876594543 = 0.036136042326688766 + 0.1 * 6.256789684295654
Epoch 620, val loss: 1.2179970741271973
Epoch 630, training loss: 0.6590821146965027 = 0.03422428295016289 + 0.1 * 6.248578071594238
Epoch 630, val loss: 1.231691598892212
Epoch 640, training loss: 0.6571685075759888 = 0.03244081512093544 + 0.1 * 6.247276306152344
Epoch 640, val loss: 1.2453668117523193
Epoch 650, training loss: 0.6547645330429077 = 0.030774720013141632 + 0.1 * 6.239898204803467
Epoch 650, val loss: 1.2588433027267456
Epoch 660, training loss: 0.6537064909934998 = 0.029218968003988266 + 0.1 * 6.244874954223633
Epoch 660, val loss: 1.272079586982727
Epoch 670, training loss: 0.6511014699935913 = 0.027766529470682144 + 0.1 * 6.233349323272705
Epoch 670, val loss: 1.2852650880813599
Epoch 680, training loss: 0.6497600674629211 = 0.02640705369412899 + 0.1 * 6.233530044555664
Epoch 680, val loss: 1.2983938455581665
Epoch 690, training loss: 0.648349404335022 = 0.025135500356554985 + 0.1 * 6.232138633728027
Epoch 690, val loss: 1.3111680746078491
Epoch 700, training loss: 0.6471567749977112 = 0.02394530549645424 + 0.1 * 6.232114315032959
Epoch 700, val loss: 1.3240290880203247
Epoch 710, training loss: 0.6454519629478455 = 0.02283192053437233 + 0.1 * 6.226200103759766
Epoch 710, val loss: 1.3366296291351318
Epoch 720, training loss: 0.6457087993621826 = 0.02178851142525673 + 0.1 * 6.239202499389648
Epoch 720, val loss: 1.3490406274795532
Epoch 730, training loss: 0.6428060531616211 = 0.020810691639780998 + 0.1 * 6.219954013824463
Epoch 730, val loss: 1.3613688945770264
Epoch 740, training loss: 0.6411815881729126 = 0.019895261153578758 + 0.1 * 6.212862968444824
Epoch 740, val loss: 1.3735418319702148
Epoch 750, training loss: 0.6408095359802246 = 0.01903524249792099 + 0.1 * 6.217742919921875
Epoch 750, val loss: 1.3855032920837402
Epoch 760, training loss: 0.6404988169670105 = 0.018230460584163666 + 0.1 * 6.222683429718018
Epoch 760, val loss: 1.3972759246826172
Epoch 770, training loss: 0.6383623480796814 = 0.01747444085776806 + 0.1 * 6.208879470825195
Epoch 770, val loss: 1.4089466333389282
Epoch 780, training loss: 0.6370775699615479 = 0.01676386408507824 + 0.1 * 6.203136920928955
Epoch 780, val loss: 1.420419454574585
Epoch 790, training loss: 0.637405276298523 = 0.016095660626888275 + 0.1 * 6.2130961418151855
Epoch 790, val loss: 1.4317330121994019
Epoch 800, training loss: 0.636212944984436 = 0.015467069111764431 + 0.1 * 6.207458972930908
Epoch 800, val loss: 1.4426790475845337
Epoch 810, training loss: 0.6354188919067383 = 0.014875831082463264 + 0.1 * 6.205430507659912
Epoch 810, val loss: 1.4537442922592163
Epoch 820, training loss: 0.6339324116706848 = 0.01431785523891449 + 0.1 * 6.196145057678223
Epoch 820, val loss: 1.4645313024520874
Epoch 830, training loss: 0.6352735161781311 = 0.013791974633932114 + 0.1 * 6.214815139770508
Epoch 830, val loss: 1.4750384092330933
Epoch 840, training loss: 0.6331486105918884 = 0.013294829986989498 + 0.1 * 6.198537826538086
Epoch 840, val loss: 1.4854744672775269
Epoch 850, training loss: 0.6321648359298706 = 0.012825562618672848 + 0.1 * 6.193392753601074
Epoch 850, val loss: 1.4956374168395996
Epoch 860, training loss: 0.6320391893386841 = 0.012381251901388168 + 0.1 * 6.196579456329346
Epoch 860, val loss: 1.5055707693099976
Epoch 870, training loss: 0.6313925981521606 = 0.011962140910327435 + 0.1 * 6.194304943084717
Epoch 870, val loss: 1.51552152633667
Epoch 880, training loss: 0.6308788061141968 = 0.011563749052584171 + 0.1 * 6.193150043487549
Epoch 880, val loss: 1.5251933336257935
Epoch 890, training loss: 0.6293679475784302 = 0.011186639778316021 + 0.1 * 6.181812763214111
Epoch 890, val loss: 1.5348069667816162
Epoch 900, training loss: 0.6285590529441833 = 0.010828109458088875 + 0.1 * 6.177309036254883
Epoch 900, val loss: 1.5440183877944946
Epoch 910, training loss: 0.6287253499031067 = 0.010487732477486134 + 0.1 * 6.182375907897949
Epoch 910, val loss: 1.5532817840576172
Epoch 920, training loss: 0.6295148134231567 = 0.010164590552449226 + 0.1 * 6.193501949310303
Epoch 920, val loss: 1.562329888343811
Epoch 930, training loss: 0.6285014152526855 = 0.00985682662576437 + 0.1 * 6.186445713043213
Epoch 930, val loss: 1.571209192276001
Epoch 940, training loss: 0.6274792551994324 = 0.009564337320625782 + 0.1 * 6.179149150848389
Epoch 940, val loss: 1.5798261165618896
Epoch 950, training loss: 0.6272916197776794 = 0.0092853382229805 + 0.1 * 6.180062770843506
Epoch 950, val loss: 1.5883902311325073
Epoch 960, training loss: 0.6264346837997437 = 0.009019428864121437 + 0.1 * 6.174152374267578
Epoch 960, val loss: 1.5968049764633179
Epoch 970, training loss: 0.6261246800422668 = 0.008765594102442265 + 0.1 * 6.173591136932373
Epoch 970, val loss: 1.6051119565963745
Epoch 980, training loss: 0.6260848045349121 = 0.008523302152752876 + 0.1 * 6.175615310668945
Epoch 980, val loss: 1.6133036613464355
Epoch 990, training loss: 0.6256042122840881 = 0.008291182108223438 + 0.1 * 6.173130035400391
Epoch 990, val loss: 1.6212782859802246
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7630
Overall ASR: 0.9446
Flip ASR: 0.9333/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7964911460876465 = 1.9591031074523926 + 0.1 * 8.373880386352539
Epoch 0, val loss: 1.9581454992294312
Epoch 10, training loss: 2.7859926223754883 = 1.9486157894134521 + 0.1 * 8.373766899108887
Epoch 10, val loss: 1.9474419355392456
Epoch 20, training loss: 2.773273468017578 = 1.9359644651412964 + 0.1 * 8.373090744018555
Epoch 20, val loss: 1.933950424194336
Epoch 30, training loss: 2.755251169204712 = 1.9183285236358643 + 0.1 * 8.369226455688477
Epoch 30, val loss: 1.9145474433898926
Epoch 40, training loss: 2.7267885208129883 = 1.8923393487930298 + 0.1 * 8.34449291229248
Epoch 40, val loss: 1.8857673406600952
Epoch 50, training loss: 2.6769943237304688 = 1.8558354377746582 + 0.1 * 8.211587905883789
Epoch 50, val loss: 1.8468002080917358
Epoch 60, training loss: 2.596344232559204 = 1.812454342842102 + 0.1 * 7.838898181915283
Epoch 60, val loss: 1.8036223649978638
Epoch 70, training loss: 2.5039172172546387 = 1.7698391675949097 + 0.1 * 7.340780735015869
Epoch 70, val loss: 1.764525055885315
Epoch 80, training loss: 2.427241563796997 = 1.7268625497817993 + 0.1 * 7.003790378570557
Epoch 80, val loss: 1.7265499830245972
Epoch 90, training loss: 2.3640987873077393 = 1.6771730184555054 + 0.1 * 6.869258403778076
Epoch 90, val loss: 1.683998703956604
Epoch 100, training loss: 2.2940635681152344 = 1.6135845184326172 + 0.1 * 6.804790496826172
Epoch 100, val loss: 1.6310479640960693
Epoch 110, training loss: 2.2083287239074707 = 1.5316158533096313 + 0.1 * 6.767127990722656
Epoch 110, val loss: 1.5645349025726318
Epoch 120, training loss: 2.107377290725708 = 1.4329018592834473 + 0.1 * 6.744753360748291
Epoch 120, val loss: 1.4860254526138306
Epoch 130, training loss: 1.9984829425811768 = 1.325448989868164 + 0.1 * 6.7303385734558105
Epoch 130, val loss: 1.402402639389038
Epoch 140, training loss: 1.889244556427002 = 1.2175061702728271 + 0.1 * 6.717383861541748
Epoch 140, val loss: 1.3209863901138306
Epoch 150, training loss: 1.7848320007324219 = 1.1145561933517456 + 0.1 * 6.7027587890625
Epoch 150, val loss: 1.2449675798416138
Epoch 160, training loss: 1.690141201019287 = 1.021252989768982 + 0.1 * 6.688882350921631
Epoch 160, val loss: 1.176277756690979
Epoch 170, training loss: 1.6043756008148193 = 0.9368898868560791 + 0.1 * 6.6748576164245605
Epoch 170, val loss: 1.113420009613037
Epoch 180, training loss: 1.5233720541000366 = 0.8576784133911133 + 0.1 * 6.656936168670654
Epoch 180, val loss: 1.0545390844345093
Epoch 190, training loss: 1.4452753067016602 = 0.7815095782279968 + 0.1 * 6.6376566886901855
Epoch 190, val loss: 0.9989390969276428
Epoch 200, training loss: 1.3709113597869873 = 0.7084358334541321 + 0.1 * 6.624754428863525
Epoch 200, val loss: 0.947094202041626
Epoch 210, training loss: 1.300875186920166 = 0.6402435898780823 + 0.1 * 6.6063151359558105
Epoch 210, val loss: 0.9005626440048218
Epoch 220, training loss: 1.238041877746582 = 0.5780320763587952 + 0.1 * 6.60009765625
Epoch 220, val loss: 0.8596513271331787
Epoch 230, training loss: 1.182332158088684 = 0.5234974026679993 + 0.1 * 6.588347434997559
Epoch 230, val loss: 0.8258177042007446
Epoch 240, training loss: 1.1332876682281494 = 0.47635316848754883 + 0.1 * 6.569344997406006
Epoch 240, val loss: 0.7996156811714172
Epoch 250, training loss: 1.0905886888504028 = 0.4350789487361908 + 0.1 * 6.555097579956055
Epoch 250, val loss: 0.780086874961853
Epoch 260, training loss: 1.0554454326629639 = 0.39823442697525024 + 0.1 * 6.572109699249268
Epoch 260, val loss: 0.7655606865882874
Epoch 270, training loss: 1.0193332433700562 = 0.365050733089447 + 0.1 * 6.542825222015381
Epoch 270, val loss: 0.7545903325080872
Epoch 280, training loss: 0.9867953658103943 = 0.3342447280883789 + 0.1 * 6.525506019592285
Epoch 280, val loss: 0.746389627456665
Epoch 290, training loss: 0.9568912982940674 = 0.30513861775398254 + 0.1 * 6.517526149749756
Epoch 290, val loss: 0.7398895621299744
Epoch 300, training loss: 0.9283422231674194 = 0.2777576148509979 + 0.1 * 6.5058465003967285
Epoch 300, val loss: 0.7349764108657837
Epoch 310, training loss: 0.9023321270942688 = 0.252047061920166 + 0.1 * 6.502850532531738
Epoch 310, val loss: 0.7316362261772156
Epoch 320, training loss: 0.8805800676345825 = 0.22794076800346375 + 0.1 * 6.526392459869385
Epoch 320, val loss: 0.7298787832260132
Epoch 330, training loss: 0.8549845814704895 = 0.20582638680934906 + 0.1 * 6.491581439971924
Epoch 330, val loss: 0.7297519445419312
Epoch 340, training loss: 0.8336057662963867 = 0.18573030829429626 + 0.1 * 6.478754043579102
Epoch 340, val loss: 0.7316790819168091
Epoch 350, training loss: 0.8142827749252319 = 0.16786834597587585 + 0.1 * 6.464143753051758
Epoch 350, val loss: 0.7355744242668152
Epoch 360, training loss: 0.797744631767273 = 0.1521134227514267 + 0.1 * 6.4563117027282715
Epoch 360, val loss: 0.7413532137870789
Epoch 370, training loss: 0.7837660908699036 = 0.13825495541095734 + 0.1 * 6.455111503601074
Epoch 370, val loss: 0.7486050128936768
Epoch 380, training loss: 0.7695674896240234 = 0.1260790377855301 + 0.1 * 6.434884548187256
Epoch 380, val loss: 0.7567351460456848
Epoch 390, training loss: 0.759543240070343 = 0.11527938395738602 + 0.1 * 6.442638397216797
Epoch 390, val loss: 0.7655571699142456
Epoch 400, training loss: 0.74714595079422 = 0.10569636523723602 + 0.1 * 6.414495468139648
Epoch 400, val loss: 0.7749161124229431
Epoch 410, training loss: 0.7398802042007446 = 0.09713876247406006 + 0.1 * 6.427414417266846
Epoch 410, val loss: 0.7847456336021423
Epoch 420, training loss: 0.7293858528137207 = 0.08950833976268768 + 0.1 * 6.398775100708008
Epoch 420, val loss: 0.7949106097221375
Epoch 430, training loss: 0.7219169735908508 = 0.0826614573597908 + 0.1 * 6.392555236816406
Epoch 430, val loss: 0.8053426146507263
Epoch 440, training loss: 0.7159022688865662 = 0.07646975666284561 + 0.1 * 6.394324779510498
Epoch 440, val loss: 0.8161458373069763
Epoch 450, training loss: 0.709541916847229 = 0.07088698446750641 + 0.1 * 6.386549472808838
Epoch 450, val loss: 0.8271625638008118
Epoch 460, training loss: 0.7034294009208679 = 0.06583904474973679 + 0.1 * 6.375903606414795
Epoch 460, val loss: 0.8382804989814758
Epoch 470, training loss: 0.698020875453949 = 0.06124399974942207 + 0.1 * 6.367768287658691
Epoch 470, val loss: 0.8496785759925842
Epoch 480, training loss: 0.6928011178970337 = 0.0570564791560173 + 0.1 * 6.357446193695068
Epoch 480, val loss: 0.861156702041626
Epoch 490, training loss: 0.6896653771400452 = 0.05322182551026344 + 0.1 * 6.364435195922852
Epoch 490, val loss: 0.8727714419364929
Epoch 500, training loss: 0.6840175986289978 = 0.0497128888964653 + 0.1 * 6.343047142028809
Epoch 500, val loss: 0.8844853043556213
Epoch 510, training loss: 0.6806772351264954 = 0.04649754986166954 + 0.1 * 6.341796875
Epoch 510, val loss: 0.8962177038192749
Epoch 520, training loss: 0.6781796813011169 = 0.043540939688682556 + 0.1 * 6.346386909484863
Epoch 520, val loss: 0.9080076217651367
Epoch 530, training loss: 0.6731460690498352 = 0.04082690179347992 + 0.1 * 6.323191165924072
Epoch 530, val loss: 0.9198042154312134
Epoch 540, training loss: 0.6708176732063293 = 0.03832421824336052 + 0.1 * 6.324934005737305
Epoch 540, val loss: 0.9315674304962158
Epoch 550, training loss: 0.6677832007408142 = 0.03602124750614166 + 0.1 * 6.317619800567627
Epoch 550, val loss: 0.9432041049003601
Epoch 560, training loss: 0.6693788170814514 = 0.03390135616064072 + 0.1 * 6.354774475097656
Epoch 560, val loss: 0.9546524882316589
Epoch 570, training loss: 0.662415087223053 = 0.03195391967892647 + 0.1 * 6.304611682891846
Epoch 570, val loss: 0.965888261795044
Epoch 580, training loss: 0.6602421402931213 = 0.030157333239912987 + 0.1 * 6.300848007202148
Epoch 580, val loss: 0.9769130945205688
Epoch 590, training loss: 0.6580002903938293 = 0.028493545949459076 + 0.1 * 6.295067310333252
Epoch 590, val loss: 0.9877930879592896
Epoch 600, training loss: 0.6571704149246216 = 0.026953892782330513 + 0.1 * 6.302165508270264
Epoch 600, val loss: 0.998468816280365
Epoch 610, training loss: 0.6550607085227966 = 0.025531752035021782 + 0.1 * 6.295289039611816
Epoch 610, val loss: 1.0089362859725952
Epoch 620, training loss: 0.6521576046943665 = 0.024215465411543846 + 0.1 * 6.279421329498291
Epoch 620, val loss: 1.0192105770111084
Epoch 630, training loss: 0.6522595882415771 = 0.022997451946139336 + 0.1 * 6.292621612548828
Epoch 630, val loss: 1.0291554927825928
Epoch 640, training loss: 0.6496476531028748 = 0.02186841145157814 + 0.1 * 6.277792453765869
Epoch 640, val loss: 1.0389195680618286
Epoch 650, training loss: 0.6496042013168335 = 0.020819634199142456 + 0.1 * 6.287845611572266
Epoch 650, val loss: 1.0484193563461304
Epoch 660, training loss: 0.6470680832862854 = 0.019843704998493195 + 0.1 * 6.272243976593018
Epoch 660, val loss: 1.057700514793396
Epoch 670, training loss: 0.6459361910820007 = 0.018935665488243103 + 0.1 * 6.270005226135254
Epoch 670, val loss: 1.0667412281036377
Epoch 680, training loss: 0.6457514762878418 = 0.018090374767780304 + 0.1 * 6.276610851287842
Epoch 680, val loss: 1.0755008459091187
Epoch 690, training loss: 0.643553614616394 = 0.017300840467214584 + 0.1 * 6.2625274658203125
Epoch 690, val loss: 1.0840905904769897
Epoch 700, training loss: 0.6424762606620789 = 0.01656126044690609 + 0.1 * 6.25915002822876
Epoch 700, val loss: 1.0924502611160278
Epoch 710, training loss: 0.6412276029586792 = 0.015867404639720917 + 0.1 * 6.253601551055908
Epoch 710, val loss: 1.1006929874420166
Epoch 720, training loss: 0.6405254006385803 = 0.015216679312288761 + 0.1 * 6.253087043762207
Epoch 720, val loss: 1.108728051185608
Epoch 730, training loss: 0.6415915489196777 = 0.014605982229113579 + 0.1 * 6.269855499267578
Epoch 730, val loss: 1.1166021823883057
Epoch 740, training loss: 0.6400666236877441 = 0.01403376180678606 + 0.1 * 6.260328769683838
Epoch 740, val loss: 1.1242371797561646
Epoch 750, training loss: 0.6388043165206909 = 0.013496166095137596 + 0.1 * 6.253081321716309
Epoch 750, val loss: 1.131710171699524
Epoch 760, training loss: 0.6365553140640259 = 0.0129897091537714 + 0.1 * 6.235656261444092
Epoch 760, val loss: 1.1390458345413208
Epoch 770, training loss: 0.6368319392204285 = 0.01251122634857893 + 0.1 * 6.24320650100708
Epoch 770, val loss: 1.1462657451629639
Epoch 780, training loss: 0.6370392441749573 = 0.012060295790433884 + 0.1 * 6.249789714813232
Epoch 780, val loss: 1.1532930135726929
Epoch 790, training loss: 0.6359367370605469 = 0.01163521409034729 + 0.1 * 6.243014812469482
Epoch 790, val loss: 1.1602058410644531
Epoch 800, training loss: 0.634645402431488 = 0.011233782395720482 + 0.1 * 6.234116077423096
Epoch 800, val loss: 1.1669269800186157
Epoch 810, training loss: 0.6339647769927979 = 0.010855020955204964 + 0.1 * 6.23109769821167
Epoch 810, val loss: 1.1734346151351929
Epoch 820, training loss: 0.6334143280982971 = 0.010495537891983986 + 0.1 * 6.229187965393066
Epoch 820, val loss: 1.1798871755599976
Epoch 830, training loss: 0.6322330832481384 = 0.010154100134968758 + 0.1 * 6.220789432525635
Epoch 830, val loss: 1.1862050294876099
Epoch 840, training loss: 0.6317009925842285 = 0.009829830378293991 + 0.1 * 6.2187113761901855
Epoch 840, val loss: 1.1924104690551758
Epoch 850, training loss: 0.6316202878952026 = 0.009521283209323883 + 0.1 * 6.22098970413208
Epoch 850, val loss: 1.1985361576080322
Epoch 860, training loss: 0.632433295249939 = 0.009228676557540894 + 0.1 * 6.232046604156494
Epoch 860, val loss: 1.204512119293213
Epoch 870, training loss: 0.6303093433380127 = 0.00895093847066164 + 0.1 * 6.213583946228027
Epoch 870, val loss: 1.2103296518325806
Epoch 880, training loss: 0.6303284764289856 = 0.008687586523592472 + 0.1 * 6.2164082527160645
Epoch 880, val loss: 1.21599280834198
Epoch 890, training loss: 0.629527747631073 = 0.008435512892901897 + 0.1 * 6.2109222412109375
Epoch 890, val loss: 1.221616268157959
Epoch 900, training loss: 0.6284775137901306 = 0.008194957859814167 + 0.1 * 6.20282506942749
Epoch 900, val loss: 1.2270910739898682
Epoch 910, training loss: 0.6302540898323059 = 0.007965012453496456 + 0.1 * 6.222890377044678
Epoch 910, val loss: 1.2325377464294434
Epoch 920, training loss: 0.6287698149681091 = 0.007745773531496525 + 0.1 * 6.210239887237549
Epoch 920, val loss: 1.2378380298614502
Epoch 930, training loss: 0.6286490559577942 = 0.007536047138273716 + 0.1 * 6.211129665374756
Epoch 930, val loss: 1.2430778741836548
Epoch 940, training loss: 0.626725435256958 = 0.007335278671234846 + 0.1 * 6.193901062011719
Epoch 940, val loss: 1.2482391595840454
Epoch 950, training loss: 0.6271604895591736 = 0.007143029011785984 + 0.1 * 6.200174331665039
Epoch 950, val loss: 1.25332772731781
Epoch 960, training loss: 0.6262294054031372 = 0.006958866026252508 + 0.1 * 6.1927056312561035
Epoch 960, val loss: 1.258291244506836
Epoch 970, training loss: 0.6256774663925171 = 0.006782962940633297 + 0.1 * 6.188945293426514
Epoch 970, val loss: 1.2631878852844238
Epoch 980, training loss: 0.6269546747207642 = 0.006613689474761486 + 0.1 * 6.2034101486206055
Epoch 980, val loss: 1.2680164575576782
Epoch 990, training loss: 0.6254061460494995 = 0.0064515043050050735 + 0.1 * 6.189546585083008
Epoch 990, val loss: 1.2727348804473877
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7454
Flip ASR: 0.7111/225 nodes
The final ASR:0.78967, 0.11289, Accuracy:0.80494, 0.03029
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9546])
updated graph: torch.Size([2, 10612])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7790322303771973 = 1.941641926765442 + 0.1 * 8.373902320861816
Epoch 0, val loss: 1.9430372714996338
Epoch 10, training loss: 2.7689549922943115 = 1.9315729141235352 + 0.1 * 8.373820304870605
Epoch 10, val loss: 1.9331341981887817
Epoch 20, training loss: 2.756596565246582 = 1.9192540645599365 + 0.1 * 8.373424530029297
Epoch 20, val loss: 1.9205645322799683
Epoch 30, training loss: 2.739047050476074 = 1.901969313621521 + 0.1 * 8.370776176452637
Epoch 30, val loss: 1.902471899986267
Epoch 40, training loss: 2.7116458415985107 = 1.8765075206756592 + 0.1 * 8.351383209228516
Epoch 40, val loss: 1.8758916854858398
Epoch 50, training loss: 2.6642234325408936 = 1.8408714532852173 + 0.1 * 8.2335205078125
Epoch 50, val loss: 1.840248465538025
Epoch 60, training loss: 2.573359727859497 = 1.7990549802780151 + 0.1 * 7.74304723739624
Epoch 60, val loss: 1.8015493154525757
Epoch 70, training loss: 2.497629404067993 = 1.7563073635101318 + 0.1 * 7.413220405578613
Epoch 70, val loss: 1.7650642395019531
Epoch 80, training loss: 2.421403646469116 = 1.7093873023986816 + 0.1 * 7.1201629638671875
Epoch 80, val loss: 1.7258803844451904
Epoch 90, training loss: 2.3441028594970703 = 1.6509791612625122 + 0.1 * 6.931237697601318
Epoch 90, val loss: 1.676398515701294
Epoch 100, training loss: 2.259214401245117 = 1.5737375020980835 + 0.1 * 6.854769229888916
Epoch 100, val loss: 1.6098480224609375
Epoch 110, training loss: 2.1607911586761475 = 1.4797166585922241 + 0.1 * 6.810744762420654
Epoch 110, val loss: 1.5316029787063599
Epoch 120, training loss: 2.05474853515625 = 1.37680983543396 + 0.1 * 6.779387950897217
Epoch 120, val loss: 1.4508388042449951
Epoch 130, training loss: 1.9481189250946045 = 1.2728848457336426 + 0.1 * 6.752339839935303
Epoch 130, val loss: 1.373868703842163
Epoch 140, training loss: 1.845560073852539 = 1.1729657649993896 + 0.1 * 6.725943088531494
Epoch 140, val loss: 1.3032349348068237
Epoch 150, training loss: 1.7504723072052002 = 1.080194115638733 + 0.1 * 6.7027812004089355
Epoch 150, val loss: 1.238690733909607
Epoch 160, training loss: 1.6642630100250244 = 0.995663046836853 + 0.1 * 6.685999393463135
Epoch 160, val loss: 1.17899751663208
Epoch 170, training loss: 1.5870208740234375 = 0.9194205403327942 + 0.1 * 6.6760029792785645
Epoch 170, val loss: 1.1243304014205933
Epoch 180, training loss: 1.514951467514038 = 0.8485071659088135 + 0.1 * 6.664443492889404
Epoch 180, val loss: 1.0724693536758423
Epoch 190, training loss: 1.4454879760742188 = 0.7798295617103577 + 0.1 * 6.656583309173584
Epoch 190, val loss: 1.0211989879608154
Epoch 200, training loss: 1.3773921728134155 = 0.712436318397522 + 0.1 * 6.6495585441589355
Epoch 200, val loss: 0.9699788689613342
Epoch 210, training loss: 1.3118994235992432 = 0.6474359631538391 + 0.1 * 6.64463472366333
Epoch 210, val loss: 0.9204922318458557
Epoch 220, training loss: 1.2499099969863892 = 0.5861995816230774 + 0.1 * 6.637104034423828
Epoch 220, val loss: 0.8750011920928955
Epoch 230, training loss: 1.1921318769454956 = 0.5291295051574707 + 0.1 * 6.63002347946167
Epoch 230, val loss: 0.8352884650230408
Epoch 240, training loss: 1.1388347148895264 = 0.47653740644454956 + 0.1 * 6.622973442077637
Epoch 240, val loss: 0.8022516965866089
Epoch 250, training loss: 1.0896809101104736 = 0.42831337451934814 + 0.1 * 6.613675117492676
Epoch 250, val loss: 0.7761996984481812
Epoch 260, training loss: 1.044484257698059 = 0.38390710949897766 + 0.1 * 6.605771064758301
Epoch 260, val loss: 0.7558192014694214
Epoch 270, training loss: 1.002484917640686 = 0.3426755368709564 + 0.1 * 6.598093509674072
Epoch 270, val loss: 0.7395867109298706
Epoch 280, training loss: 0.9632927179336548 = 0.30431169271469116 + 0.1 * 6.589809894561768
Epoch 280, val loss: 0.726932168006897
Epoch 290, training loss: 0.9267336130142212 = 0.26873236894607544 + 0.1 * 6.580012321472168
Epoch 290, val loss: 0.7175078392028809
Epoch 300, training loss: 0.8953608274459839 = 0.23616011440753937 + 0.1 * 6.592007160186768
Epoch 300, val loss: 0.7112577557563782
Epoch 310, training loss: 0.8637017011642456 = 0.20725615322589874 + 0.1 * 6.564455509185791
Epoch 310, val loss: 0.7081204652786255
Epoch 320, training loss: 0.8376835584640503 = 0.1818609982728958 + 0.1 * 6.558225631713867
Epoch 320, val loss: 0.7080167531967163
Epoch 330, training loss: 0.8171193599700928 = 0.15983185172080994 + 0.1 * 6.572874546051025
Epoch 330, val loss: 0.7106804251670837
Epoch 340, training loss: 0.795755922794342 = 0.14112889766693115 + 0.1 * 6.546270370483398
Epoch 340, val loss: 0.7155736684799194
Epoch 350, training loss: 0.7787925601005554 = 0.12515698373317719 + 0.1 * 6.536355495452881
Epoch 350, val loss: 0.7222340703010559
Epoch 360, training loss: 0.7641236782073975 = 0.11144259572029114 + 0.1 * 6.526811122894287
Epoch 360, val loss: 0.7302998900413513
Epoch 370, training loss: 0.7534456849098206 = 0.09960662573575974 + 0.1 * 6.538390636444092
Epoch 370, val loss: 0.7393015623092651
Epoch 380, training loss: 0.7412375807762146 = 0.08948272466659546 + 0.1 * 6.517548561096191
Epoch 380, val loss: 0.7487649321556091
Epoch 390, training loss: 0.7317214012145996 = 0.08072201907634735 + 0.1 * 6.509993553161621
Epoch 390, val loss: 0.7584975957870483
Epoch 400, training loss: 0.7227762341499329 = 0.07305412739515305 + 0.1 * 6.497220993041992
Epoch 400, val loss: 0.7686778903007507
Epoch 410, training loss: 0.7168028354644775 = 0.06630869954824448 + 0.1 * 6.504941463470459
Epoch 410, val loss: 0.7789968848228455
Epoch 420, training loss: 0.7084017992019653 = 0.06039097532629967 + 0.1 * 6.480108261108398
Epoch 420, val loss: 0.7894030213356018
Epoch 430, training loss: 0.7038873434066772 = 0.05519180744886398 + 0.1 * 6.486955165863037
Epoch 430, val loss: 0.7996099591255188
Epoch 440, training loss: 0.69744473695755 = 0.05062364041805267 + 0.1 * 6.468210697174072
Epoch 440, val loss: 0.8098809123039246
Epoch 450, training loss: 0.692118227481842 = 0.04656308889389038 + 0.1 * 6.4555511474609375
Epoch 450, val loss: 0.8199859857559204
Epoch 460, training loss: 0.6889005303382874 = 0.04294612258672714 + 0.1 * 6.4595441818237305
Epoch 460, val loss: 0.830051064491272
Epoch 470, training loss: 0.6844369769096375 = 0.0397426038980484 + 0.1 * 6.446943759918213
Epoch 470, val loss: 0.8399494290351868
Epoch 480, training loss: 0.6799395084381104 = 0.036877427250146866 + 0.1 * 6.430620193481445
Epoch 480, val loss: 0.8495924472808838
Epoch 490, training loss: 0.6770237684249878 = 0.034307632595300674 + 0.1 * 6.427160739898682
Epoch 490, val loss: 0.8591235280036926
Epoch 500, training loss: 0.6735670566558838 = 0.03200014680624008 + 0.1 * 6.415668964385986
Epoch 500, val loss: 0.8684872388839722
Epoch 510, training loss: 0.6710372567176819 = 0.029913678765296936 + 0.1 * 6.411235809326172
Epoch 510, val loss: 0.8775801062583923
Epoch 520, training loss: 0.6679568290710449 = 0.028026331216096878 + 0.1 * 6.3993048667907715
Epoch 520, val loss: 0.8863381147384644
Epoch 530, training loss: 0.6660909056663513 = 0.026317773386836052 + 0.1 * 6.397731304168701
Epoch 530, val loss: 0.8950244188308716
Epoch 540, training loss: 0.6644795536994934 = 0.024765150621533394 + 0.1 * 6.397143840789795
Epoch 540, val loss: 0.9032142162322998
Epoch 550, training loss: 0.6614413857460022 = 0.02335446886718273 + 0.1 * 6.380868911743164
Epoch 550, val loss: 0.9113672375679016
Epoch 560, training loss: 0.6592920422554016 = 0.02206120267510414 + 0.1 * 6.372307777404785
Epoch 560, val loss: 0.9191967248916626
Epoch 570, training loss: 0.6592152714729309 = 0.020872827619314194 + 0.1 * 6.383424282073975
Epoch 570, val loss: 0.9267728328704834
Epoch 580, training loss: 0.6571388244628906 = 0.01978563889861107 + 0.1 * 6.373531818389893
Epoch 580, val loss: 0.9341480135917664
Epoch 590, training loss: 0.654895007610321 = 0.018785659223794937 + 0.1 * 6.361093521118164
Epoch 590, val loss: 0.941348135471344
Epoch 600, training loss: 0.654381275177002 = 0.01785997860133648 + 0.1 * 6.365212917327881
Epoch 600, val loss: 0.948348879814148
Epoch 610, training loss: 0.6525206565856934 = 0.017004741355776787 + 0.1 * 6.355159282684326
Epoch 610, val loss: 0.9551098942756653
Epoch 620, training loss: 0.6509230732917786 = 0.016212809830904007 + 0.1 * 6.347102165222168
Epoch 620, val loss: 0.9617974162101746
Epoch 630, training loss: 0.6512079238891602 = 0.015476137399673462 + 0.1 * 6.357317924499512
Epoch 630, val loss: 0.9682430624961853
Epoch 640, training loss: 0.6511411666870117 = 0.014791811816394329 + 0.1 * 6.363493919372559
Epoch 640, val loss: 0.9745213389396667
Epoch 650, training loss: 0.6477459669113159 = 0.01415630616247654 + 0.1 * 6.3358964920043945
Epoch 650, val loss: 0.9805858731269836
Epoch 660, training loss: 0.6465149521827698 = 0.013563375920057297 + 0.1 * 6.32951545715332
Epoch 660, val loss: 0.9866283535957336
Epoch 670, training loss: 0.6470470428466797 = 0.013007122091948986 + 0.1 * 6.340399265289307
Epoch 670, val loss: 0.9924543499946594
Epoch 680, training loss: 0.6454434990882874 = 0.012486998923122883 + 0.1 * 6.329565048217773
Epoch 680, val loss: 0.9979193806648254
Epoch 690, training loss: 0.6445567607879639 = 0.012002998031675816 + 0.1 * 6.325537204742432
Epoch 690, val loss: 1.0035576820373535
Epoch 700, training loss: 0.6433217525482178 = 0.011547207832336426 + 0.1 * 6.317745208740234
Epoch 700, val loss: 1.0090103149414062
Epoch 710, training loss: 0.6421836018562317 = 0.0111176036298275 + 0.1 * 6.310660362243652
Epoch 710, val loss: 1.0140879154205322
Epoch 720, training loss: 0.641370415687561 = 0.010714930482208729 + 0.1 * 6.306554794311523
Epoch 720, val loss: 1.0192351341247559
Epoch 730, training loss: 0.6418808698654175 = 0.010334356687963009 + 0.1 * 6.315464973449707
Epoch 730, val loss: 1.024221420288086
Epoch 740, training loss: 0.640522301197052 = 0.009975736029446125 + 0.1 * 6.3054656982421875
Epoch 740, val loss: 1.0291557312011719
Epoch 750, training loss: 0.6395324468612671 = 0.009637430310249329 + 0.1 * 6.2989501953125
Epoch 750, val loss: 1.0340113639831543
Epoch 760, training loss: 0.6407257914543152 = 0.009316477924585342 + 0.1 * 6.314092636108398
Epoch 760, val loss: 1.0386155843734741
Epoch 770, training loss: 0.6381580829620361 = 0.009014423936605453 + 0.1 * 6.291436195373535
Epoch 770, val loss: 1.0431292057037354
Epoch 780, training loss: 0.6380431652069092 = 0.008728454820811749 + 0.1 * 6.293147087097168
Epoch 780, val loss: 1.047763466835022
Epoch 790, training loss: 0.6371536254882812 = 0.00845608115196228 + 0.1 * 6.286974906921387
Epoch 790, val loss: 1.0520789623260498
Epoch 800, training loss: 0.6370788216590881 = 0.008198566734790802 + 0.1 * 6.288802623748779
Epoch 800, val loss: 1.0563843250274658
Epoch 810, training loss: 0.6359789967536926 = 0.007953725755214691 + 0.1 * 6.280252933502197
Epoch 810, val loss: 1.0606220960617065
Epoch 820, training loss: 0.6355261206626892 = 0.007721358444541693 + 0.1 * 6.278047561645508
Epoch 820, val loss: 1.0648349523544312
Epoch 830, training loss: 0.6364551782608032 = 0.00749940937384963 + 0.1 * 6.289557933807373
Epoch 830, val loss: 1.0688728094100952
Epoch 840, training loss: 0.634763240814209 = 0.007288617081940174 + 0.1 * 6.274745941162109
Epoch 840, val loss: 1.0728323459625244
Epoch 850, training loss: 0.6343950629234314 = 0.007087922189384699 + 0.1 * 6.2730712890625
Epoch 850, val loss: 1.0768072605133057
Epoch 860, training loss: 0.6352311968803406 = 0.006895930040627718 + 0.1 * 6.283352375030518
Epoch 860, val loss: 1.080726981163025
Epoch 870, training loss: 0.6336057186126709 = 0.006712591741234064 + 0.1 * 6.268930912017822
Epoch 870, val loss: 1.0844447612762451
Epoch 880, training loss: 0.6344156265258789 = 0.006537680048495531 + 0.1 * 6.27877950668335
Epoch 880, val loss: 1.0882008075714111
Epoch 890, training loss: 0.6329715847969055 = 0.006370509043335915 + 0.1 * 6.266010761260986
Epoch 890, val loss: 1.09174644947052
Epoch 900, training loss: 0.6329123973846436 = 0.006210687104612589 + 0.1 * 6.267016887664795
Epoch 900, val loss: 1.0954010486602783
Epoch 910, training loss: 0.6330362558364868 = 0.006057878024876118 + 0.1 * 6.269783973693848
Epoch 910, val loss: 1.0990058183670044
Epoch 920, training loss: 0.6310040354728699 = 0.005910636857151985 + 0.1 * 6.250933647155762
Epoch 920, val loss: 1.1023693084716797
Epoch 930, training loss: 0.6306975483894348 = 0.005770415998995304 + 0.1 * 6.249270915985107
Epoch 930, val loss: 1.1058276891708374
Epoch 940, training loss: 0.6312974691390991 = 0.005635433364659548 + 0.1 * 6.256619930267334
Epoch 940, val loss: 1.1092171669006348
Epoch 950, training loss: 0.6297963857650757 = 0.005505978595465422 + 0.1 * 6.242903709411621
Epoch 950, val loss: 1.112548828125
Epoch 960, training loss: 0.6303825378417969 = 0.0053816125728189945 + 0.1 * 6.250009536743164
Epoch 960, val loss: 1.115816354751587
Epoch 970, training loss: 0.6309017539024353 = 0.005262312013655901 + 0.1 * 6.256394386291504
Epoch 970, val loss: 1.1190026998519897
Epoch 980, training loss: 0.6288668513298035 = 0.005147501826286316 + 0.1 * 6.2371931076049805
Epoch 980, val loss: 1.1221297979354858
Epoch 990, training loss: 0.6292302012443542 = 0.005037242546677589 + 0.1 * 6.241929531097412
Epoch 990, val loss: 1.1253770589828491
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7417
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.783724308013916 = 1.9463378190994263 + 0.1 * 8.37386417388916
Epoch 0, val loss: 1.9435153007507324
Epoch 10, training loss: 2.7736682891845703 = 1.9362963438034058 + 0.1 * 8.37371826171875
Epoch 10, val loss: 1.9322103261947632
Epoch 20, training loss: 2.761319160461426 = 1.9240334033966064 + 0.1 * 8.372858047485352
Epoch 20, val loss: 1.9179013967514038
Epoch 30, training loss: 2.7439005374908447 = 1.9071002006530762 + 0.1 * 8.368003845214844
Epoch 30, val loss: 1.8977100849151611
Epoch 40, training loss: 2.7153356075286865 = 1.8824399709701538 + 0.1 * 8.32895565032959
Epoch 40, val loss: 1.8683241605758667
Epoch 50, training loss: 2.65274715423584 = 1.8486899137496948 + 0.1 * 8.040573120117188
Epoch 50, val loss: 1.8297544717788696
Epoch 60, training loss: 2.5620462894439697 = 1.8123815059661865 + 0.1 * 7.496647357940674
Epoch 60, val loss: 1.791286826133728
Epoch 70, training loss: 2.4806630611419678 = 1.7759859561920166 + 0.1 * 7.046771049499512
Epoch 70, val loss: 1.7543559074401855
Epoch 80, training loss: 2.422240972518921 = 1.7379274368286133 + 0.1 * 6.843135356903076
Epoch 80, val loss: 1.7194571495056152
Epoch 90, training loss: 2.3678674697875977 = 1.694820523262024 + 0.1 * 6.730470657348633
Epoch 90, val loss: 1.6833211183547974
Epoch 100, training loss: 2.3056752681732178 = 1.6381597518920898 + 0.1 * 6.6751556396484375
Epoch 100, val loss: 1.6368244886398315
Epoch 110, training loss: 2.228111743927002 = 1.5639995336532593 + 0.1 * 6.641122341156006
Epoch 110, val loss: 1.5762101411819458
Epoch 120, training loss: 2.1362504959106445 = 1.474839687347412 + 0.1 * 6.614107608795166
Epoch 120, val loss: 1.5043269395828247
Epoch 130, training loss: 2.037490129470825 = 1.3784785270690918 + 0.1 * 6.590115547180176
Epoch 130, val loss: 1.4291374683380127
Epoch 140, training loss: 1.9391257762908936 = 1.2823336124420166 + 0.1 * 6.567921161651611
Epoch 140, val loss: 1.3579376935958862
Epoch 150, training loss: 1.8447434902191162 = 1.1898009777069092 + 0.1 * 6.5494256019592285
Epoch 150, val loss: 1.291976809501648
Epoch 160, training loss: 1.7544299364089966 = 1.1008358001708984 + 0.1 * 6.535941123962402
Epoch 160, val loss: 1.2296172380447388
Epoch 170, training loss: 1.6671802997589111 = 1.015310287475586 + 0.1 * 6.518700122833252
Epoch 170, val loss: 1.1692644357681274
Epoch 180, training loss: 1.5836279392242432 = 0.9326427578926086 + 0.1 * 6.509852409362793
Epoch 180, val loss: 1.1094547510147095
Epoch 190, training loss: 1.5038542747497559 = 0.8539689779281616 + 0.1 * 6.498852252960205
Epoch 190, val loss: 1.0510262250900269
Epoch 200, training loss: 1.428800344467163 = 0.7798295021057129 + 0.1 * 6.48970890045166
Epoch 200, val loss: 0.9955446720123291
Epoch 210, training loss: 1.3605153560638428 = 0.712478756904602 + 0.1 * 6.4803667068481445
Epoch 210, val loss: 0.9456257224082947
Epoch 220, training loss: 1.2995836734771729 = 0.6525565385818481 + 0.1 * 6.470271110534668
Epoch 220, val loss: 0.9017659425735474
Epoch 230, training loss: 1.246061086654663 = 0.5989081263542175 + 0.1 * 6.471529006958008
Epoch 230, val loss: 0.8638556599617004
Epoch 240, training loss: 1.1962628364562988 = 0.5506571531295776 + 0.1 * 6.456056118011475
Epoch 240, val loss: 0.8316612243652344
Epoch 250, training loss: 1.1511802673339844 = 0.5060918927192688 + 0.1 * 6.450883388519287
Epoch 250, val loss: 0.8037108778953552
Epoch 260, training loss: 1.1088647842407227 = 0.4643290042877197 + 0.1 * 6.4453582763671875
Epoch 260, val loss: 0.7795695662498474
Epoch 270, training loss: 1.068139910697937 = 0.4248715341091156 + 0.1 * 6.43268346786499
Epoch 270, val loss: 0.75858473777771
Epoch 280, training loss: 1.031045913696289 = 0.3878198266029358 + 0.1 * 6.432260513305664
Epoch 280, val loss: 0.7404191493988037
Epoch 290, training loss: 0.9949480891227722 = 0.3532031178474426 + 0.1 * 6.417449474334717
Epoch 290, val loss: 0.7248528003692627
Epoch 300, training loss: 0.9624862670898438 = 0.32088497281074524 + 0.1 * 6.416012287139893
Epoch 300, val loss: 0.7118266820907593
Epoch 310, training loss: 0.9315825700759888 = 0.2910851538181305 + 0.1 * 6.40497350692749
Epoch 310, val loss: 0.7014034986495972
Epoch 320, training loss: 0.903028130531311 = 0.26346439123153687 + 0.1 * 6.395637035369873
Epoch 320, val loss: 0.6932218670845032
Epoch 330, training loss: 0.8769661784172058 = 0.2377805858850479 + 0.1 * 6.391855716705322
Epoch 330, val loss: 0.6872169375419617
Epoch 340, training loss: 0.853873610496521 = 0.21400712430477142 + 0.1 * 6.398664474487305
Epoch 340, val loss: 0.6830567717552185
Epoch 350, training loss: 0.8298380374908447 = 0.19219902157783508 + 0.1 * 6.37639045715332
Epoch 350, val loss: 0.6806620955467224
Epoch 360, training loss: 0.8102254271507263 = 0.17225845158100128 + 0.1 * 6.379669666290283
Epoch 360, val loss: 0.679827094078064
Epoch 370, training loss: 0.7906984686851501 = 0.1542595773935318 + 0.1 * 6.364388942718506
Epoch 370, val loss: 0.6802064776420593
Epoch 380, training loss: 0.7734599113464355 = 0.13818208873271942 + 0.1 * 6.35277795791626
Epoch 380, val loss: 0.6816989183425903
Epoch 390, training loss: 0.7603017687797546 = 0.1239924281835556 + 0.1 * 6.363093376159668
Epoch 390, val loss: 0.6838554739952087
Epoch 400, training loss: 0.7456056475639343 = 0.11163379997015 + 0.1 * 6.339718341827393
Epoch 400, val loss: 0.6864002346992493
Epoch 410, training loss: 0.7355579137802124 = 0.10082932561635971 + 0.1 * 6.347286224365234
Epoch 410, val loss: 0.6898773908615112
Epoch 420, training loss: 0.7247002124786377 = 0.09139564633369446 + 0.1 * 6.333045959472656
Epoch 420, val loss: 0.6939215660095215
Epoch 430, training loss: 0.7155899405479431 = 0.08313895016908646 + 0.1 * 6.324509620666504
Epoch 430, val loss: 0.6986949443817139
Epoch 440, training loss: 0.7077839970588684 = 0.0759018138051033 + 0.1 * 6.318821907043457
Epoch 440, val loss: 0.7038404941558838
Epoch 450, training loss: 0.7006670832633972 = 0.06956233829259872 + 0.1 * 6.311047554016113
Epoch 450, val loss: 0.709349513053894
Epoch 460, training loss: 0.6948376297950745 = 0.0639616921544075 + 0.1 * 6.3087592124938965
Epoch 460, val loss: 0.7150785326957703
Epoch 470, training loss: 0.6894674897193909 = 0.059008218348026276 + 0.1 * 6.304592609405518
Epoch 470, val loss: 0.7209264636039734
Epoch 480, training loss: 0.684072732925415 = 0.05460017919540405 + 0.1 * 6.29472541809082
Epoch 480, val loss: 0.7270111441612244
Epoch 490, training loss: 0.6828593611717224 = 0.05065598711371422 + 0.1 * 6.322033882141113
Epoch 490, val loss: 0.733199417591095
Epoch 500, training loss: 0.675899088382721 = 0.047132574021816254 + 0.1 * 6.287665367126465
Epoch 500, val loss: 0.7392992973327637
Epoch 510, training loss: 0.6720228791236877 = 0.04396258667111397 + 0.1 * 6.28060245513916
Epoch 510, val loss: 0.7454929947853088
Epoch 520, training loss: 0.6690319180488586 = 0.041097190231084824 + 0.1 * 6.2793474197387695
Epoch 520, val loss: 0.7517222166061401
Epoch 530, training loss: 0.6659601926803589 = 0.03849974647164345 + 0.1 * 6.274604320526123
Epoch 530, val loss: 0.7578257322311401
Epoch 540, training loss: 0.6644783020019531 = 0.03613928705453873 + 0.1 * 6.283389568328857
Epoch 540, val loss: 0.7639884948730469
Epoch 550, training loss: 0.660750150680542 = 0.03400362282991409 + 0.1 * 6.267465114593506
Epoch 550, val loss: 0.7700286507606506
Epoch 560, training loss: 0.6584550142288208 = 0.03205467015504837 + 0.1 * 6.264003276824951
Epoch 560, val loss: 0.7759743332862854
Epoch 570, training loss: 0.6561980247497559 = 0.030278237536549568 + 0.1 * 6.25919771194458
Epoch 570, val loss: 0.7818942070007324
Epoch 580, training loss: 0.6541598439216614 = 0.028647828847169876 + 0.1 * 6.255119800567627
Epoch 580, val loss: 0.7876309752464294
Epoch 590, training loss: 0.6546072363853455 = 0.02714705467224121 + 0.1 * 6.274601459503174
Epoch 590, val loss: 0.7934376001358032
Epoch 600, training loss: 0.6507550477981567 = 0.025763096287846565 + 0.1 * 6.249919414520264
Epoch 600, val loss: 0.7989864349365234
Epoch 610, training loss: 0.6495701670646667 = 0.024478770792484283 + 0.1 * 6.250913619995117
Epoch 610, val loss: 0.8045447468757629
Epoch 620, training loss: 0.6472718119621277 = 0.023298759013414383 + 0.1 * 6.239730358123779
Epoch 620, val loss: 0.8100297451019287
Epoch 630, training loss: 0.647148609161377 = 0.02220260538160801 + 0.1 * 6.249459743499756
Epoch 630, val loss: 0.8153135180473328
Epoch 640, training loss: 0.6444929838180542 = 0.0211841631680727 + 0.1 * 6.23308801651001
Epoch 640, val loss: 0.8206006288528442
Epoch 650, training loss: 0.6437758803367615 = 0.020232200622558594 + 0.1 * 6.23543643951416
Epoch 650, val loss: 0.8256080150604248
Epoch 660, training loss: 0.6425989270210266 = 0.019346676766872406 + 0.1 * 6.232522487640381
Epoch 660, val loss: 0.8307664394378662
Epoch 670, training loss: 0.6427397131919861 = 0.018519645556807518 + 0.1 * 6.24220085144043
Epoch 670, val loss: 0.8357744812965393
Epoch 680, training loss: 0.639888346195221 = 0.017749851569533348 + 0.1 * 6.2213850021362305
Epoch 680, val loss: 0.8407189249992371
Epoch 690, training loss: 0.6390092372894287 = 0.017028767615556717 + 0.1 * 6.219804763793945
Epoch 690, val loss: 0.8455169200897217
Epoch 700, training loss: 0.638995349407196 = 0.016349829733371735 + 0.1 * 6.226454734802246
Epoch 700, val loss: 0.8503431677818298
Epoch 710, training loss: 0.6372182369232178 = 0.01571374014019966 + 0.1 * 6.215044975280762
Epoch 710, val loss: 0.8549996614456177
Epoch 720, training loss: 0.6391499042510986 = 0.01511450670659542 + 0.1 * 6.240354061126709
Epoch 720, val loss: 0.8595876693725586
Epoch 730, training loss: 0.6361292600631714 = 0.014551540836691856 + 0.1 * 6.215777397155762
Epoch 730, val loss: 0.8641488552093506
Epoch 740, training loss: 0.6351053714752197 = 0.014021346345543861 + 0.1 * 6.210840225219727
Epoch 740, val loss: 0.8685204386711121
Epoch 750, training loss: 0.635156512260437 = 0.013520246371626854 + 0.1 * 6.216362476348877
Epoch 750, val loss: 0.8729479908943176
Epoch 760, training loss: 0.6334702372550964 = 0.013046765699982643 + 0.1 * 6.204234600067139
Epoch 760, val loss: 0.8772167563438416
Epoch 770, training loss: 0.6330762505531311 = 0.01259862631559372 + 0.1 * 6.204776287078857
Epoch 770, val loss: 0.8815088272094727
Epoch 780, training loss: 0.6327897906303406 = 0.012174852192401886 + 0.1 * 6.206149101257324
Epoch 780, val loss: 0.8856821060180664
Epoch 790, training loss: 0.631607711315155 = 0.011773146688938141 + 0.1 * 6.198345184326172
Epoch 790, val loss: 0.8897660970687866
Epoch 800, training loss: 0.6322411298751831 = 0.011391361244022846 + 0.1 * 6.208498001098633
Epoch 800, val loss: 0.8937610983848572
Epoch 810, training loss: 0.6304301023483276 = 0.011029204353690147 + 0.1 * 6.194008827209473
Epoch 810, val loss: 0.8978188633918762
Epoch 820, training loss: 0.6315642595291138 = 0.010684536769986153 + 0.1 * 6.208796977996826
Epoch 820, val loss: 0.9017146229743958
Epoch 830, training loss: 0.6295941472053528 = 0.01035727746784687 + 0.1 * 6.192368984222412
Epoch 830, val loss: 0.9056180119514465
Epoch 840, training loss: 0.6314001083374023 = 0.010046485811471939 + 0.1 * 6.213536262512207
Epoch 840, val loss: 0.9094176888465881
Epoch 850, training loss: 0.6289653182029724 = 0.009753200225532055 + 0.1 * 6.1921210289001465
Epoch 850, val loss: 0.9132280945777893
Epoch 860, training loss: 0.6282385587692261 = 0.009472773410379887 + 0.1 * 6.187657356262207
Epoch 860, val loss: 0.9168006777763367
Epoch 870, training loss: 0.6288989782333374 = 0.009206222370266914 + 0.1 * 6.196927547454834
Epoch 870, val loss: 0.9204825162887573
Epoch 880, training loss: 0.6276646852493286 = 0.008951609022915363 + 0.1 * 6.187130451202393
Epoch 880, val loss: 0.9241964221000671
Epoch 890, training loss: 0.6268671154975891 = 0.008708818815648556 + 0.1 * 6.181582450866699
Epoch 890, val loss: 0.9275952577590942
Epoch 900, training loss: 0.6278246641159058 = 0.00847629550844431 + 0.1 * 6.193483352661133
Epoch 900, val loss: 0.9311081171035767
Epoch 910, training loss: 0.6260731220245361 = 0.008253611624240875 + 0.1 * 6.178194999694824
Epoch 910, val loss: 0.934610903263092
Epoch 920, training loss: 0.6255806088447571 = 0.008040416985750198 + 0.1 * 6.17540168762207
Epoch 920, val loss: 0.9379372000694275
Epoch 930, training loss: 0.6255602836608887 = 0.00783565267920494 + 0.1 * 6.177246570587158
Epoch 930, val loss: 0.9413378238677979
Epoch 940, training loss: 0.6255325675010681 = 0.007640007883310318 + 0.1 * 6.178925514221191
Epoch 940, val loss: 0.9446681141853333
Epoch 950, training loss: 0.624813973903656 = 0.007452055811882019 + 0.1 * 6.173618793487549
Epoch 950, val loss: 0.9479482769966125
Epoch 960, training loss: 0.6238648295402527 = 0.007271344307810068 + 0.1 * 6.1659345626831055
Epoch 960, val loss: 0.9511054158210754
Epoch 970, training loss: 0.6248571276664734 = 0.007097502704709768 + 0.1 * 6.177596569061279
Epoch 970, val loss: 0.9542856812477112
Epoch 980, training loss: 0.6239045858383179 = 0.006930507719516754 + 0.1 * 6.169740200042725
Epoch 980, val loss: 0.957509458065033
Epoch 990, training loss: 0.6249884963035583 = 0.006769842933863401 + 0.1 * 6.182186603546143
Epoch 990, val loss: 0.9605335593223572
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.4133
Flip ASR: 0.3778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.8016278743743896 = 1.964247226715088 + 0.1 * 8.37380599975586
Epoch 0, val loss: 1.9707931280136108
Epoch 10, training loss: 2.7903456687927246 = 1.9529955387115479 + 0.1 * 8.373499870300293
Epoch 10, val loss: 1.9597339630126953
Epoch 20, training loss: 2.7757506370544434 = 1.9385820627212524 + 0.1 * 8.371686935424805
Epoch 20, val loss: 1.945006251335144
Epoch 30, training loss: 2.7537975311279297 = 1.9179996252059937 + 0.1 * 8.357978820800781
Epoch 30, val loss: 1.92354416847229
Epoch 40, training loss: 2.7144126892089844 = 1.8877313137054443 + 0.1 * 8.266814231872559
Epoch 40, val loss: 1.8920034170150757
Epoch 50, training loss: 2.6331427097320557 = 1.849307656288147 + 0.1 * 7.838350772857666
Epoch 50, val loss: 1.8540226221084595
Epoch 60, training loss: 2.549804449081421 = 1.809562087059021 + 0.1 * 7.402424335479736
Epoch 60, val loss: 1.81687593460083
Epoch 70, training loss: 2.475754499435425 = 1.7716424465179443 + 0.1 * 7.0411200523376465
Epoch 70, val loss: 1.7834609746932983
Epoch 80, training loss: 2.419370412826538 = 1.733999490737915 + 0.1 * 6.853709697723389
Epoch 80, val loss: 1.7513030767440796
Epoch 90, training loss: 2.3611485958099365 = 1.68836510181427 + 0.1 * 6.727835178375244
Epoch 90, val loss: 1.712492823600769
Epoch 100, training loss: 2.2934370040893555 = 1.6275962591171265 + 0.1 * 6.658406734466553
Epoch 100, val loss: 1.6622700691223145
Epoch 110, training loss: 2.212764263153076 = 1.5504813194274902 + 0.1 * 6.622830390930176
Epoch 110, val loss: 1.5996824502944946
Epoch 120, training loss: 2.12204647064209 = 1.4619476795196533 + 0.1 * 6.600988388061523
Epoch 120, val loss: 1.529346227645874
Epoch 130, training loss: 2.0273501873016357 = 1.369110107421875 + 0.1 * 6.582400798797607
Epoch 130, val loss: 1.4580081701278687
Epoch 140, training loss: 1.9310057163238525 = 1.27397620677948 + 0.1 * 6.570294380187988
Epoch 140, val loss: 1.38628351688385
Epoch 150, training loss: 1.8333683013916016 = 1.1783013343811035 + 0.1 * 6.550670146942139
Epoch 150, val loss: 1.3147410154342651
Epoch 160, training loss: 1.7349638938903809 = 1.0817341804504395 + 0.1 * 6.532296180725098
Epoch 160, val loss: 1.2418723106384277
Epoch 170, training loss: 1.6389796733856201 = 0.9866071343421936 + 0.1 * 6.523725509643555
Epoch 170, val loss: 1.1700297594070435
Epoch 180, training loss: 1.5485111474990845 = 0.8981058597564697 + 0.1 * 6.504052639007568
Epoch 180, val loss: 1.104438304901123
Epoch 190, training loss: 1.466768741607666 = 0.8178980946540833 + 0.1 * 6.488706588745117
Epoch 190, val loss: 1.047192931175232
Epoch 200, training loss: 1.3945162296295166 = 0.7470855116844177 + 0.1 * 6.474306583404541
Epoch 200, val loss: 1.0000393390655518
Epoch 210, training loss: 1.3320072889328003 = 0.685143232345581 + 0.1 * 6.468640327453613
Epoch 210, val loss: 0.9618120193481445
Epoch 220, training loss: 1.2755048274993896 = 0.630935788154602 + 0.1 * 6.445690631866455
Epoch 220, val loss: 0.9312911033630371
Epoch 230, training loss: 1.225574016571045 = 0.5824488401412964 + 0.1 * 6.431251525878906
Epoch 230, val loss: 0.9063324928283691
Epoch 240, training loss: 1.1806771755218506 = 0.5387505888938904 + 0.1 * 6.419266223907471
Epoch 240, val loss: 0.8863039016723633
Epoch 250, training loss: 1.1415067911148071 = 0.4993351399898529 + 0.1 * 6.421716213226318
Epoch 250, val loss: 0.8708066940307617
Epoch 260, training loss: 1.1040195226669312 = 0.4639524519443512 + 0.1 * 6.400670528411865
Epoch 260, val loss: 0.859929621219635
Epoch 270, training loss: 1.0706921815872192 = 0.4315957725048065 + 0.1 * 6.390964031219482
Epoch 270, val loss: 0.8527281880378723
Epoch 280, training loss: 1.0410484075546265 = 0.40207210183143616 + 0.1 * 6.3897624015808105
Epoch 280, val loss: 0.8489875793457031
Epoch 290, training loss: 1.0126458406448364 = 0.37520673871040344 + 0.1 * 6.374390602111816
Epoch 290, val loss: 0.8484494686126709
Epoch 300, training loss: 0.9872918128967285 = 0.3507463037967682 + 0.1 * 6.365455150604248
Epoch 300, val loss: 0.8506690859794617
Epoch 310, training loss: 0.9654364585876465 = 0.32859644293785095 + 0.1 * 6.368399620056152
Epoch 310, val loss: 0.8554437756538391
Epoch 320, training loss: 0.9457799792289734 = 0.30861496925354004 + 0.1 * 6.371649742126465
Epoch 320, val loss: 0.8624083995819092
Epoch 330, training loss: 0.9251497983932495 = 0.29060235619544983 + 0.1 * 6.345473766326904
Epoch 330, val loss: 0.8712387681007385
Epoch 340, training loss: 0.9074407815933228 = 0.27423447370529175 + 0.1 * 6.332062721252441
Epoch 340, val loss: 0.8817501664161682
Epoch 350, training loss: 0.8940083980560303 = 0.25931409001350403 + 0.1 * 6.346943378448486
Epoch 350, val loss: 0.8937485218048096
Epoch 360, training loss: 0.8787562847137451 = 0.24574534595012665 + 0.1 * 6.330109119415283
Epoch 360, val loss: 0.9068429470062256
Epoch 370, training loss: 0.8651962280273438 = 0.23325446248054504 + 0.1 * 6.3194169998168945
Epoch 370, val loss: 0.9207966923713684
Epoch 380, training loss: 0.8538072109222412 = 0.22155418992042542 + 0.1 * 6.322530269622803
Epoch 380, val loss: 0.9353089928627014
Epoch 390, training loss: 0.8411983847618103 = 0.2103719264268875 + 0.1 * 6.30826473236084
Epoch 390, val loss: 0.9500115513801575
Epoch 400, training loss: 0.829473078250885 = 0.19932693243026733 + 0.1 * 6.301461219787598
Epoch 400, val loss: 0.9646624326705933
Epoch 410, training loss: 0.8207563161849976 = 0.18800494074821472 + 0.1 * 6.327513694763184
Epoch 410, val loss: 0.9791684746742249
Epoch 420, training loss: 0.8056824803352356 = 0.17616039514541626 + 0.1 * 6.295220851898193
Epoch 420, val loss: 0.993277907371521
Epoch 430, training loss: 0.7932345867156982 = 0.1636706292629242 + 0.1 * 6.295639514923096
Epoch 430, val loss: 1.0069786310195923
Epoch 440, training loss: 0.7803058624267578 = 0.15082336962223053 + 0.1 * 6.294825077056885
Epoch 440, val loss: 1.0203031301498413
Epoch 450, training loss: 0.7662819027900696 = 0.1381046026945114 + 0.1 * 6.281772613525391
Epoch 450, val loss: 1.0334055423736572
Epoch 460, training loss: 0.7567287683486938 = 0.12595120072364807 + 0.1 * 6.307775020599365
Epoch 460, val loss: 1.0465787649154663
Epoch 470, training loss: 0.7425379753112793 = 0.11474217474460602 + 0.1 * 6.277957916259766
Epoch 470, val loss: 1.0600740909576416
Epoch 480, training loss: 0.731482982635498 = 0.10459224134683609 + 0.1 * 6.268907070159912
Epoch 480, val loss: 1.0741736888885498
Epoch 490, training loss: 0.7237676382064819 = 0.09550875425338745 + 0.1 * 6.282588958740234
Epoch 490, val loss: 1.0887738466262817
Epoch 500, training loss: 0.714189350605011 = 0.08748393505811691 + 0.1 * 6.267054080963135
Epoch 500, val loss: 1.1041046380996704
Epoch 510, training loss: 0.7068694829940796 = 0.0804242342710495 + 0.1 * 6.2644524574279785
Epoch 510, val loss: 1.1200159788131714
Epoch 520, training loss: 0.6998952031135559 = 0.07420123368501663 + 0.1 * 6.256939888000488
Epoch 520, val loss: 1.1365865468978882
Epoch 530, training loss: 0.6949259638786316 = 0.06867920607328415 + 0.1 * 6.262467384338379
Epoch 530, val loss: 1.153642177581787
Epoch 540, training loss: 0.6896138787269592 = 0.06376095116138458 + 0.1 * 6.258529186248779
Epoch 540, val loss: 1.170985221862793
Epoch 550, training loss: 0.6843925714492798 = 0.05935870110988617 + 0.1 * 6.250338554382324
Epoch 550, val loss: 1.1885024309158325
Epoch 560, training loss: 0.68165123462677 = 0.05538717657327652 + 0.1 * 6.262639999389648
Epoch 560, val loss: 1.2061491012573242
Epoch 570, training loss: 0.6761858463287354 = 0.051800601184368134 + 0.1 * 6.243852138519287
Epoch 570, val loss: 1.2236902713775635
Epoch 580, training loss: 0.6728525161743164 = 0.04853755608201027 + 0.1 * 6.243149280548096
Epoch 580, val loss: 1.2411266565322876
Epoch 590, training loss: 0.6689491271972656 = 0.045558709651231766 + 0.1 * 6.2339043617248535
Epoch 590, val loss: 1.2584242820739746
Epoch 600, training loss: 0.6666193604469299 = 0.04283085837960243 + 0.1 * 6.237884521484375
Epoch 600, val loss: 1.275464415550232
Epoch 610, training loss: 0.664947509765625 = 0.040334783494472504 + 0.1 * 6.246127605438232
Epoch 610, val loss: 1.292209506034851
Epoch 620, training loss: 0.6622098088264465 = 0.0380525104701519 + 0.1 * 6.241572856903076
Epoch 620, val loss: 1.3087313175201416
Epoch 630, training loss: 0.6593974828720093 = 0.03595651313662529 + 0.1 * 6.234409809112549
Epoch 630, val loss: 1.3247716426849365
Epoch 640, training loss: 0.6568496823310852 = 0.03402784839272499 + 0.1 * 6.228218078613281
Epoch 640, val loss: 1.3406394720077515
Epoch 650, training loss: 0.6539076566696167 = 0.03224603086709976 + 0.1 * 6.216615676879883
Epoch 650, val loss: 1.356015682220459
Epoch 660, training loss: 0.6523727774620056 = 0.030596697703003883 + 0.1 * 6.217761039733887
Epoch 660, val loss: 1.3711739778518677
Epoch 670, training loss: 0.6521337032318115 = 0.0290659312158823 + 0.1 * 6.230677604675293
Epoch 670, val loss: 1.3859518766403198
Epoch 680, training loss: 0.6487939953804016 = 0.027648139744997025 + 0.1 * 6.211458206176758
Epoch 680, val loss: 1.400342583656311
Epoch 690, training loss: 0.6472585201263428 = 0.026329752057790756 + 0.1 * 6.209287643432617
Epoch 690, val loss: 1.4144569635391235
Epoch 700, training loss: 0.6474217772483826 = 0.025102075189352036 + 0.1 * 6.223196983337402
Epoch 700, val loss: 1.4282480478286743
Epoch 710, training loss: 0.6459502577781677 = 0.023958923295140266 + 0.1 * 6.219913005828857
Epoch 710, val loss: 1.4415889978408813
Epoch 720, training loss: 0.6441119909286499 = 0.022894693538546562 + 0.1 * 6.212172985076904
Epoch 720, val loss: 1.4547345638275146
Epoch 730, training loss: 0.6428462266921997 = 0.021900376304984093 + 0.1 * 6.209458351135254
Epoch 730, val loss: 1.4676084518432617
Epoch 740, training loss: 0.6409091353416443 = 0.02096938155591488 + 0.1 * 6.199397087097168
Epoch 740, val loss: 1.4801641702651978
Epoch 750, training loss: 0.6403732895851135 = 0.020096683874726295 + 0.1 * 6.202765941619873
Epoch 750, val loss: 1.4924181699752808
Epoch 760, training loss: 0.6386229991912842 = 0.01927895098924637 + 0.1 * 6.1934404373168945
Epoch 760, val loss: 1.504451870918274
Epoch 770, training loss: 0.6398614645004272 = 0.018509596586227417 + 0.1 * 6.2135186195373535
Epoch 770, val loss: 1.5161856412887573
Epoch 780, training loss: 0.6376022100448608 = 0.017786748707294464 + 0.1 * 6.198154449462891
Epoch 780, val loss: 1.5276740789413452
Epoch 790, training loss: 0.6373148560523987 = 0.017107713967561722 + 0.1 * 6.202071189880371
Epoch 790, val loss: 1.5388764142990112
Epoch 800, training loss: 0.6356058120727539 = 0.01646837778389454 + 0.1 * 6.191373825073242
Epoch 800, val loss: 1.5498930215835571
Epoch 810, training loss: 0.6343519687652588 = 0.0158648993819952 + 0.1 * 6.184870719909668
Epoch 810, val loss: 1.5607155561447144
Epoch 820, training loss: 0.6346960663795471 = 0.01529424637556076 + 0.1 * 6.1940178871154785
Epoch 820, val loss: 1.5711907148361206
Epoch 830, training loss: 0.6340863108634949 = 0.014754779636859894 + 0.1 * 6.193315505981445
Epoch 830, val loss: 1.5815542936325073
Epoch 840, training loss: 0.6333193182945251 = 0.014245699159801006 + 0.1 * 6.190736293792725
Epoch 840, val loss: 1.5915305614471436
Epoch 850, training loss: 0.6319530606269836 = 0.013763433322310448 + 0.1 * 6.181896209716797
Epoch 850, val loss: 1.6014727354049683
Epoch 860, training loss: 0.6317625045776367 = 0.013306453824043274 + 0.1 * 6.184560775756836
Epoch 860, val loss: 1.6111642122268677
Epoch 870, training loss: 0.6307480931282043 = 0.012872793711721897 + 0.1 * 6.178752899169922
Epoch 870, val loss: 1.6207135915756226
Epoch 880, training loss: 0.6299616098403931 = 0.012460701167583466 + 0.1 * 6.175008773803711
Epoch 880, val loss: 1.629950761795044
Epoch 890, training loss: 0.6294015049934387 = 0.012069572694599628 + 0.1 * 6.173318862915039
Epoch 890, val loss: 1.6391723155975342
Epoch 900, training loss: 0.6287413239479065 = 0.011696594767272472 + 0.1 * 6.17044734954834
Epoch 900, val loss: 1.6481561660766602
Epoch 910, training loss: 0.6297467350959778 = 0.011341647244989872 + 0.1 * 6.184051036834717
Epoch 910, val loss: 1.6568560600280762
Epoch 920, training loss: 0.6284314393997192 = 0.011004369705915451 + 0.1 * 6.1742706298828125
Epoch 920, val loss: 1.6654858589172363
Epoch 930, training loss: 0.6287637948989868 = 0.010682891122996807 + 0.1 * 6.180809020996094
Epoch 930, val loss: 1.6738965511322021
Epoch 940, training loss: 0.6267125606536865 = 0.010376348160207272 + 0.1 * 6.1633620262146
Epoch 940, val loss: 1.6822041273117065
Epoch 950, training loss: 0.6277890205383301 = 0.01008332148194313 + 0.1 * 6.177056789398193
Epoch 950, val loss: 1.6904269456863403
Epoch 960, training loss: 0.626141369342804 = 0.009802578017115593 + 0.1 * 6.163387775421143
Epoch 960, val loss: 1.6983073949813843
Epoch 970, training loss: 0.6259078979492188 = 0.009535075165331364 + 0.1 * 6.1637282371521
Epoch 970, val loss: 1.706193447113037
Epoch 980, training loss: 0.6251602172851562 = 0.009278879500925541 + 0.1 * 6.158812999725342
Epoch 980, val loss: 1.7138869762420654
Epoch 990, training loss: 0.6251958012580872 = 0.0090336287394166 + 0.1 * 6.161621570587158
Epoch 990, val loss: 1.7214314937591553
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7593
Overall ASR: 0.7712
Flip ASR: 0.7244/225 nodes
The final ASR:0.64207, 0.16222, Accuracy:0.80370, 0.03157
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00460, Accuracy:0.82840, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.780325412750244 = 1.9429301023483276 + 0.1 * 8.373953819274902
Epoch 0, val loss: 1.9480823278427124
Epoch 10, training loss: 2.770205020904541 = 1.9328153133392334 + 0.1 * 8.373896598815918
Epoch 10, val loss: 1.9376033544540405
Epoch 20, training loss: 2.7571663856506348 = 1.9198005199432373 + 0.1 * 8.373658180236816
Epoch 20, val loss: 1.923934817314148
Epoch 30, training loss: 2.7382607460021973 = 1.9010508060455322 + 0.1 * 8.372097969055176
Epoch 30, val loss: 1.9041190147399902
Epoch 40, training loss: 2.709318161010742 = 1.8732500076293945 + 0.1 * 8.36068058013916
Epoch 40, val loss: 1.8751269578933716
Epoch 50, training loss: 2.666489839553833 = 1.8354517221450806 + 0.1 * 8.310381889343262
Epoch 50, val loss: 1.8379193544387817
Epoch 60, training loss: 2.603806734085083 = 1.7952743768692017 + 0.1 * 8.085323333740234
Epoch 60, val loss: 1.8025104999542236
Epoch 70, training loss: 2.5457096099853516 = 1.7572835683822632 + 0.1 * 7.884261608123779
Epoch 70, val loss: 1.771209955215454
Epoch 80, training loss: 2.4606168270111084 = 1.7115092277526855 + 0.1 * 7.4910759925842285
Epoch 80, val loss: 1.7324597835540771
Epoch 90, training loss: 2.374277353286743 = 1.6538211107254028 + 0.1 * 7.204561710357666
Epoch 90, val loss: 1.682948112487793
Epoch 100, training loss: 2.285168170928955 = 1.5796754360198975 + 0.1 * 7.054928302764893
Epoch 100, val loss: 1.619209885597229
Epoch 110, training loss: 2.185391664505005 = 1.4897595643997192 + 0.1 * 6.956320762634277
Epoch 110, val loss: 1.5421816110610962
Epoch 120, training loss: 2.0786449909210205 = 1.3897300958633423 + 0.1 * 6.889148712158203
Epoch 120, val loss: 1.4593051671981812
Epoch 130, training loss: 1.9707348346710205 = 1.2859936952590942 + 0.1 * 6.847411155700684
Epoch 130, val loss: 1.3756440877914429
Epoch 140, training loss: 1.8631623983383179 = 1.1811116933822632 + 0.1 * 6.820507049560547
Epoch 140, val loss: 1.2935140132904053
Epoch 150, training loss: 1.758897304534912 = 1.0787266492843628 + 0.1 * 6.801705837249756
Epoch 150, val loss: 1.2151832580566406
Epoch 160, training loss: 1.660559058189392 = 0.98219895362854 + 0.1 * 6.783600807189941
Epoch 160, val loss: 1.1414077281951904
Epoch 170, training loss: 1.569706916809082 = 0.8927035331726074 + 0.1 * 6.770033836364746
Epoch 170, val loss: 1.0734869241714478
Epoch 180, training loss: 1.4853250980377197 = 0.809573769569397 + 0.1 * 6.757513999938965
Epoch 180, val loss: 1.0107340812683105
Epoch 190, training loss: 1.407062292098999 = 0.7321711778640747 + 0.1 * 6.7489118576049805
Epoch 190, val loss: 0.9526412487030029
Epoch 200, training loss: 1.3353900909423828 = 0.6615569591522217 + 0.1 * 6.738331317901611
Epoch 200, val loss: 0.9010968208312988
Epoch 210, training loss: 1.2724766731262207 = 0.5988916754722595 + 0.1 * 6.735849380493164
Epoch 210, val loss: 0.8576316237449646
Epoch 220, training loss: 1.2174972295761108 = 0.5454953908920288 + 0.1 * 6.72001838684082
Epoch 220, val loss: 0.8236667513847351
Epoch 230, training loss: 1.1713166236877441 = 0.5002031326293945 + 0.1 * 6.71113395690918
Epoch 230, val loss: 0.7977688312530518
Epoch 240, training loss: 1.1316397190093994 = 0.4613611698150635 + 0.1 * 6.702785968780518
Epoch 240, val loss: 0.7785192131996155
Epoch 250, training loss: 1.0965855121612549 = 0.4271819591522217 + 0.1 * 6.69403600692749
Epoch 250, val loss: 0.7639374732971191
Epoch 260, training loss: 1.064760684967041 = 0.39596807956695557 + 0.1 * 6.687925815582275
Epoch 260, val loss: 0.7524920105934143
Epoch 270, training loss: 1.0337767601013184 = 0.3660680949687958 + 0.1 * 6.67708683013916
Epoch 270, val loss: 0.7428978681564331
Epoch 280, training loss: 1.0031858682632446 = 0.3362995684146881 + 0.1 * 6.668862819671631
Epoch 280, val loss: 0.734419584274292
Epoch 290, training loss: 0.973003089427948 = 0.30644381046295166 + 0.1 * 6.665592670440674
Epoch 290, val loss: 0.7270001173019409
Epoch 300, training loss: 0.9425120949745178 = 0.27716779708862305 + 0.1 * 6.653442859649658
Epoch 300, val loss: 0.7211306095123291
Epoch 310, training loss: 0.9138696193695068 = 0.2491883784532547 + 0.1 * 6.646812438964844
Epoch 310, val loss: 0.7172037363052368
Epoch 320, training loss: 0.8873801231384277 = 0.22330281138420105 + 0.1 * 6.640772819519043
Epoch 320, val loss: 0.7157583236694336
Epoch 330, training loss: 0.863401472568512 = 0.19992674887180328 + 0.1 * 6.63474702835083
Epoch 330, val loss: 0.7165814638137817
Epoch 340, training loss: 0.8418793082237244 = 0.17906789481639862 + 0.1 * 6.628113746643066
Epoch 340, val loss: 0.7194966077804565
Epoch 350, training loss: 0.8220438957214355 = 0.16057786345481873 + 0.1 * 6.614660739898682
Epoch 350, val loss: 0.7241855263710022
Epoch 360, training loss: 0.8050543069839478 = 0.14422409236431122 + 0.1 * 6.608302116394043
Epoch 360, val loss: 0.7305371165275574
Epoch 370, training loss: 0.7903774976730347 = 0.12984046339988708 + 0.1 * 6.605370044708252
Epoch 370, val loss: 0.7383688688278198
Epoch 380, training loss: 0.7764800786972046 = 0.11721774935722351 + 0.1 * 6.592623710632324
Epoch 380, val loss: 0.7474482655525208
Epoch 390, training loss: 0.7645329236984253 = 0.1060967892408371 + 0.1 * 6.5843610763549805
Epoch 390, val loss: 0.7576014995574951
Epoch 400, training loss: 0.7557442784309387 = 0.0962909385561943 + 0.1 * 6.5945329666137695
Epoch 400, val loss: 0.768683671951294
Epoch 410, training loss: 0.7448902726173401 = 0.08766759932041168 + 0.1 * 6.572226524353027
Epoch 410, val loss: 0.7803324460983276
Epoch 420, training loss: 0.7367309927940369 = 0.0800335630774498 + 0.1 * 6.566974639892578
Epoch 420, val loss: 0.7925465703010559
Epoch 430, training loss: 0.7288706302642822 = 0.07327036559581757 + 0.1 * 6.556002140045166
Epoch 430, val loss: 0.8051403164863586
Epoch 440, training loss: 0.7223873138427734 = 0.06724553555250168 + 0.1 * 6.551417350769043
Epoch 440, val loss: 0.8178662061691284
Epoch 450, training loss: 0.7168566584587097 = 0.06185988709330559 + 0.1 * 6.549967288970947
Epoch 450, val loss: 0.8307337760925293
Epoch 460, training loss: 0.7107782363891602 = 0.05703889578580856 + 0.1 * 6.537393093109131
Epoch 460, val loss: 0.8436033129692078
Epoch 470, training loss: 0.706886887550354 = 0.0527120865881443 + 0.1 * 6.541748046875
Epoch 470, val loss: 0.8563526272773743
Epoch 480, training loss: 0.7016245722770691 = 0.048834893852472305 + 0.1 * 6.527896404266357
Epoch 480, val loss: 0.8687921762466431
Epoch 490, training loss: 0.6975923180580139 = 0.04533571004867554 + 0.1 * 6.522565841674805
Epoch 490, val loss: 0.8810690641403198
Epoch 500, training loss: 0.6936101913452148 = 0.04216550290584564 + 0.1 * 6.51444673538208
Epoch 500, val loss: 0.8932844400405884
Epoch 510, training loss: 0.6904006600379944 = 0.03929052874445915 + 0.1 * 6.511101245880127
Epoch 510, val loss: 0.9052601456642151
Epoch 520, training loss: 0.687201201915741 = 0.036688193678855896 + 0.1 * 6.505129814147949
Epoch 520, val loss: 0.9168501496315002
Epoch 530, training loss: 0.6847053170204163 = 0.03431926295161247 + 0.1 * 6.503859996795654
Epoch 530, val loss: 0.9282801151275635
Epoch 540, training loss: 0.6831125617027283 = 0.032154787331819534 + 0.1 * 6.509577751159668
Epoch 540, val loss: 0.9396156668663025
Epoch 550, training loss: 0.6793301105499268 = 0.03018583171069622 + 0.1 * 6.491442680358887
Epoch 550, val loss: 0.9504871368408203
Epoch 560, training loss: 0.6771248579025269 = 0.028387343510985374 + 0.1 * 6.487374782562256
Epoch 560, val loss: 0.9611027240753174
Epoch 570, training loss: 0.6753265857696533 = 0.026736140251159668 + 0.1 * 6.485904216766357
Epoch 570, val loss: 0.9715576171875
Epoch 580, training loss: 0.6745526790618896 = 0.025218511000275612 + 0.1 * 6.49334192276001
Epoch 580, val loss: 0.9817761182785034
Epoch 590, training loss: 0.6722126007080078 = 0.02382754348218441 + 0.1 * 6.483850479125977
Epoch 590, val loss: 0.9916087985038757
Epoch 600, training loss: 0.6701382398605347 = 0.022545134648680687 + 0.1 * 6.475930690765381
Epoch 600, val loss: 1.001304268836975
Epoch 610, training loss: 0.6688448786735535 = 0.021361351013183594 + 0.1 * 6.474835395812988
Epoch 610, val loss: 1.010711908340454
Epoch 620, training loss: 0.6668950319290161 = 0.020267091691493988 + 0.1 * 6.46627950668335
Epoch 620, val loss: 1.0198898315429688
Epoch 630, training loss: 0.6651995182037354 = 0.01925542950630188 + 0.1 * 6.459440231323242
Epoch 630, val loss: 1.0288918018341064
Epoch 640, training loss: 0.6649361848831177 = 0.018318330869078636 + 0.1 * 6.466177940368652
Epoch 640, val loss: 1.0376638174057007
Epoch 650, training loss: 0.662296712398529 = 0.01744794473052025 + 0.1 * 6.448487758636475
Epoch 650, val loss: 1.0461382865905762
Epoch 660, training loss: 0.6616010069847107 = 0.016640176996588707 + 0.1 * 6.449607849121094
Epoch 660, val loss: 1.0544841289520264
Epoch 670, training loss: 0.6624682545661926 = 0.015886902809143066 + 0.1 * 6.465813159942627
Epoch 670, val loss: 1.0626044273376465
Epoch 680, training loss: 0.6596788763999939 = 0.015186639502644539 + 0.1 * 6.444921970367432
Epoch 680, val loss: 1.0704951286315918
Epoch 690, training loss: 0.6586862206459045 = 0.014532089233398438 + 0.1 * 6.4415411949157715
Epoch 690, val loss: 1.0782979726791382
Epoch 700, training loss: 0.6584389209747314 = 0.013921498320996761 + 0.1 * 6.445174217224121
Epoch 700, val loss: 1.0857486724853516
Epoch 710, training loss: 0.6563659906387329 = 0.013349955901503563 + 0.1 * 6.430160045623779
Epoch 710, val loss: 1.0931020975112915
Epoch 720, training loss: 0.6548893451690674 = 0.012814600020647049 + 0.1 * 6.420747756958008
Epoch 720, val loss: 1.1002862453460693
Epoch 730, training loss: 0.6537901163101196 = 0.012312645092606544 + 0.1 * 6.414774417877197
Epoch 730, val loss: 1.1072806119918823
Epoch 740, training loss: 0.6538718938827515 = 0.011840218678116798 + 0.1 * 6.420316696166992
Epoch 740, val loss: 1.114219069480896
Epoch 750, training loss: 0.6549137830734253 = 0.011395138688385487 + 0.1 * 6.43518590927124
Epoch 750, val loss: 1.1209731101989746
Epoch 760, training loss: 0.6519894003868103 = 0.010976924560964108 + 0.1 * 6.410124778747559
Epoch 760, val loss: 1.1274385452270508
Epoch 770, training loss: 0.6513949036598206 = 0.010583226568996906 + 0.1 * 6.408116817474365
Epoch 770, val loss: 1.1338303089141846
Epoch 780, training loss: 0.6501931548118591 = 0.010211024433374405 + 0.1 * 6.399820804595947
Epoch 780, val loss: 1.1401331424713135
Epoch 790, training loss: 0.6506117582321167 = 0.009860004298388958 + 0.1 * 6.407517910003662
Epoch 790, val loss: 1.1462938785552979
Epoch 800, training loss: 0.6495645642280579 = 0.00952711421996355 + 0.1 * 6.400374412536621
Epoch 800, val loss: 1.1522746086120605
Epoch 810, training loss: 0.6488942503929138 = 0.009212199598550797 + 0.1 * 6.396820545196533
Epoch 810, val loss: 1.1582131385803223
Epoch 820, training loss: 0.6477501392364502 = 0.008913080208003521 + 0.1 * 6.388370513916016
Epoch 820, val loss: 1.1639641523361206
Epoch 830, training loss: 0.6468408703804016 = 0.008630364201962948 + 0.1 * 6.382105350494385
Epoch 830, val loss: 1.16959547996521
Epoch 840, training loss: 0.6505337953567505 = 0.008362204767763615 + 0.1 * 6.42171573638916
Epoch 840, val loss: 1.1751312017440796
Epoch 850, training loss: 0.6459656953811646 = 0.008107360452413559 + 0.1 * 6.378582954406738
Epoch 850, val loss: 1.180495262145996
Epoch 860, training loss: 0.6455329060554504 = 0.007866214960813522 + 0.1 * 6.376667022705078
Epoch 860, val loss: 1.1858106851577759
Epoch 870, training loss: 0.6458720564842224 = 0.007636601105332375 + 0.1 * 6.382354259490967
Epoch 870, val loss: 1.191094160079956
Epoch 880, training loss: 0.6440376043319702 = 0.007416685111820698 + 0.1 * 6.366209506988525
Epoch 880, val loss: 1.196203589439392
Epoch 890, training loss: 0.6444209218025208 = 0.007207989227026701 + 0.1 * 6.372129440307617
Epoch 890, val loss: 1.2012556791305542
Epoch 900, training loss: 0.6434340476989746 = 0.007008638232946396 + 0.1 * 6.364253997802734
Epoch 900, val loss: 1.206221342086792
Epoch 910, training loss: 0.6451438069343567 = 0.006818179506808519 + 0.1 * 6.383255958557129
Epoch 910, val loss: 1.2111339569091797
Epoch 920, training loss: 0.6435587406158447 = 0.006635733880102634 + 0.1 * 6.369229793548584
Epoch 920, val loss: 1.2158650159835815
Epoch 930, training loss: 0.643853485584259 = 0.006462720688432455 + 0.1 * 6.373908042907715
Epoch 930, val loss: 1.2205052375793457
Epoch 940, training loss: 0.6413141489028931 = 0.006296894513070583 + 0.1 * 6.350172519683838
Epoch 940, val loss: 1.225091576576233
Epoch 950, training loss: 0.6409393548965454 = 0.006138489115983248 + 0.1 * 6.348008632659912
Epoch 950, val loss: 1.2296080589294434
Epoch 960, training loss: 0.6400540471076965 = 0.005986023228615522 + 0.1 * 6.340680122375488
Epoch 960, val loss: 1.2340497970581055
Epoch 970, training loss: 0.640378475189209 = 0.005840006750077009 + 0.1 * 6.34538459777832
Epoch 970, val loss: 1.2383928298950195
Epoch 980, training loss: 0.6402064561843872 = 0.005700059235095978 + 0.1 * 6.345064163208008
Epoch 980, val loss: 1.2426940202713013
Epoch 990, training loss: 0.6396515965461731 = 0.0055654654279351234 + 0.1 * 6.3408613204956055
Epoch 990, val loss: 1.2467596530914307
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6753
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.787177562713623 = 1.949786901473999 + 0.1 * 8.373906135559082
Epoch 0, val loss: 1.9505549669265747
Epoch 10, training loss: 2.777794122695923 = 1.940409541130066 + 0.1 * 8.373846054077148
Epoch 10, val loss: 1.9411978721618652
Epoch 20, training loss: 2.7666187286376953 = 1.9292727708816528 + 0.1 * 8.373458862304688
Epoch 20, val loss: 1.9295382499694824
Epoch 30, training loss: 2.7509398460388184 = 1.9138908386230469 + 0.1 * 8.370489120483398
Epoch 30, val loss: 1.9130380153656006
Epoch 40, training loss: 2.7257440090179443 = 1.8911513090133667 + 0.1 * 8.345926284790039
Epoch 40, val loss: 1.888593077659607
Epoch 50, training loss: 2.6734626293182373 = 1.8580995798110962 + 0.1 * 8.153630256652832
Epoch 50, val loss: 1.8541203737258911
Epoch 60, training loss: 2.598480224609375 = 1.8178421258926392 + 0.1 * 7.806381702423096
Epoch 60, val loss: 1.8140820264816284
Epoch 70, training loss: 2.510751724243164 = 1.7755844593048096 + 0.1 * 7.351672172546387
Epoch 70, val loss: 1.7725245952606201
Epoch 80, training loss: 2.436157703399658 = 1.7320674657821655 + 0.1 * 7.0409016609191895
Epoch 80, val loss: 1.730661392211914
Epoch 90, training loss: 2.3702986240386963 = 1.6836611032485962 + 0.1 * 6.866375923156738
Epoch 90, val loss: 1.685564637184143
Epoch 100, training loss: 2.299675464630127 = 1.6203066110610962 + 0.1 * 6.793687343597412
Epoch 100, val loss: 1.62929105758667
Epoch 110, training loss: 2.214926242828369 = 1.5389925241470337 + 0.1 * 6.759336471557617
Epoch 110, val loss: 1.561098337173462
Epoch 120, training loss: 2.117316246032715 = 1.443530797958374 + 0.1 * 6.73785400390625
Epoch 120, val loss: 1.4844157695770264
Epoch 130, training loss: 2.0148539543151855 = 1.3428056240081787 + 0.1 * 6.720481872558594
Epoch 130, val loss: 1.4066494703292847
Epoch 140, training loss: 1.9135794639587402 = 1.2433847188949585 + 0.1 * 6.70194673538208
Epoch 140, val loss: 1.333957552909851
Epoch 150, training loss: 1.8180572986602783 = 1.1492880582809448 + 0.1 * 6.687692165374756
Epoch 150, val loss: 1.2673654556274414
Epoch 160, training loss: 1.7293387651443481 = 1.0626435279846191 + 0.1 * 6.666952133178711
Epoch 160, val loss: 1.2069904804229736
Epoch 170, training loss: 1.6492621898651123 = 0.9841355681419373 + 0.1 * 6.651266574859619
Epoch 170, val loss: 1.152517318725586
Epoch 180, training loss: 1.5786190032958984 = 0.914528489112854 + 0.1 * 6.640904903411865
Epoch 180, val loss: 1.1056010723114014
Epoch 190, training loss: 1.512001395225525 = 0.8495509624481201 + 0.1 * 6.624504089355469
Epoch 190, val loss: 1.0619627237319946
Epoch 200, training loss: 1.4471900463104248 = 0.7859569787979126 + 0.1 * 6.612330436706543
Epoch 200, val loss: 1.0191930532455444
Epoch 210, training loss: 1.3819725513458252 = 0.7218973636627197 + 0.1 * 6.600750923156738
Epoch 210, val loss: 0.9755667448043823
Epoch 220, training loss: 1.3183709383010864 = 0.6581754088401794 + 0.1 * 6.601955413818359
Epoch 220, val loss: 0.9318979978561401
Epoch 230, training loss: 1.2546420097351074 = 0.5964000821113586 + 0.1 * 6.582418441772461
Epoch 230, val loss: 0.8900644779205322
Epoch 240, training loss: 1.1950477361679077 = 0.5379146337509155 + 0.1 * 6.571331024169922
Epoch 240, val loss: 0.8520031571388245
Epoch 250, training loss: 1.1419041156768799 = 0.4843011796474457 + 0.1 * 6.576029300689697
Epoch 250, val loss: 0.8193740844726562
Epoch 260, training loss: 1.0918543338775635 = 0.4363057315349579 + 0.1 * 6.555485725402832
Epoch 260, val loss: 0.7931622266769409
Epoch 270, training loss: 1.0469835996627808 = 0.39304372668266296 + 0.1 * 6.539398670196533
Epoch 270, val loss: 0.7720568776130676
Epoch 280, training loss: 1.006942629814148 = 0.35402289032936096 + 0.1 * 6.529196739196777
Epoch 280, val loss: 0.7551026940345764
Epoch 290, training loss: 0.9708311557769775 = 0.3188858926296234 + 0.1 * 6.519453048706055
Epoch 290, val loss: 0.7418469190597534
Epoch 300, training loss: 0.940376877784729 = 0.28738275170326233 + 0.1 * 6.529940605163574
Epoch 300, val loss: 0.7325316071510315
Epoch 310, training loss: 0.909435510635376 = 0.2589714825153351 + 0.1 * 6.504640579223633
Epoch 310, val loss: 0.7265220880508423
Epoch 320, training loss: 0.8824535012245178 = 0.23302127420902252 + 0.1 * 6.494322299957275
Epoch 320, val loss: 0.7229995727539062
Epoch 330, training loss: 0.857429027557373 = 0.2092607319355011 + 0.1 * 6.481682300567627
Epoch 330, val loss: 0.7213859558105469
Epoch 340, training loss: 0.8376317024230957 = 0.18759706616401672 + 0.1 * 6.500346660614014
Epoch 340, val loss: 0.7216559648513794
Epoch 350, training loss: 0.8156332969665527 = 0.16828009486198425 + 0.1 * 6.473532199859619
Epoch 350, val loss: 0.7236566543579102
Epoch 360, training loss: 0.7968040704727173 = 0.1511998474597931 + 0.1 * 6.4560418128967285
Epoch 360, val loss: 0.7274816036224365
Epoch 370, training loss: 0.7830252051353455 = 0.1361786276102066 + 0.1 * 6.468465805053711
Epoch 370, val loss: 0.7328957915306091
Epoch 380, training loss: 0.7665765285491943 = 0.12312334030866623 + 0.1 * 6.4345316886901855
Epoch 380, val loss: 0.7397314310073853
Epoch 390, training loss: 0.756708562374115 = 0.11173270642757416 + 0.1 * 6.449758052825928
Epoch 390, val loss: 0.7476163506507874
Epoch 400, training loss: 0.7442036867141724 = 0.10179200023412704 + 0.1 * 6.424117088317871
Epoch 400, val loss: 0.7564369440078735
Epoch 410, training loss: 0.7341400980949402 = 0.09307513386011124 + 0.1 * 6.410649299621582
Epoch 410, val loss: 0.7655582427978516
Epoch 420, training loss: 0.7282138466835022 = 0.08548319339752197 + 0.1 * 6.427306175231934
Epoch 420, val loss: 0.7752000689506531
Epoch 430, training loss: 0.7194115519523621 = 0.0788751021027565 + 0.1 * 6.405364036560059
Epoch 430, val loss: 0.785301923751831
Epoch 440, training loss: 0.7134265899658203 = 0.07302499562501907 + 0.1 * 6.404016017913818
Epoch 440, val loss: 0.7956040501594543
Epoch 450, training loss: 0.7059057950973511 = 0.0678177997469902 + 0.1 * 6.380879878997803
Epoch 450, val loss: 0.8060614466667175
Epoch 460, training loss: 0.7018324732780457 = 0.06312847882509232 + 0.1 * 6.387039661407471
Epoch 460, val loss: 0.8165318369865417
Epoch 470, training loss: 0.69676274061203 = 0.058880217373371124 + 0.1 * 6.3788251876831055
Epoch 470, val loss: 0.8267884254455566
Epoch 480, training loss: 0.6917495727539062 = 0.0550113283097744 + 0.1 * 6.367382526397705
Epoch 480, val loss: 0.8370214104652405
Epoch 490, training loss: 0.6870949268341064 = 0.05146804451942444 + 0.1 * 6.356268405914307
Epoch 490, val loss: 0.8469968438148499
Epoch 500, training loss: 0.6839177012443542 = 0.04821357503533363 + 0.1 * 6.357040882110596
Epoch 500, val loss: 0.8568438291549683
Epoch 510, training loss: 0.6800705790519714 = 0.04519403725862503 + 0.1 * 6.3487653732299805
Epoch 510, val loss: 0.8665525317192078
Epoch 520, training loss: 0.6780200004577637 = 0.04238424822688103 + 0.1 * 6.356357097625732
Epoch 520, val loss: 0.8761335611343384
Epoch 530, training loss: 0.6738042831420898 = 0.039767075330019 + 0.1 * 6.340371608734131
Epoch 530, val loss: 0.8854493498802185
Epoch 540, training loss: 0.6705642938613892 = 0.037289589643478394 + 0.1 * 6.332746505737305
Epoch 540, val loss: 0.8947112560272217
Epoch 550, training loss: 0.668738067150116 = 0.03494523838162422 + 0.1 * 6.337928295135498
Epoch 550, val loss: 0.9036170840263367
Epoch 560, training loss: 0.6655580997467041 = 0.03274784982204437 + 0.1 * 6.328102111816406
Epoch 560, val loss: 0.9122933745384216
Epoch 570, training loss: 0.6631284952163696 = 0.030707404017448425 + 0.1 * 6.3242106437683105
Epoch 570, val loss: 0.9209238290786743
Epoch 580, training loss: 0.6601074934005737 = 0.028835611417889595 + 0.1 * 6.312718868255615
Epoch 580, val loss: 0.9292569756507874
Epoch 590, training loss: 0.660700261592865 = 0.027117928490042686 + 0.1 * 6.3358235359191895
Epoch 590, val loss: 0.9376264810562134
Epoch 600, training loss: 0.657166600227356 = 0.025552675127983093 + 0.1 * 6.316138744354248
Epoch 600, val loss: 0.9458132982254028
Epoch 610, training loss: 0.6549639105796814 = 0.02412918582558632 + 0.1 * 6.308346748352051
Epoch 610, val loss: 0.9541105031967163
Epoch 620, training loss: 0.6528227925300598 = 0.02281917631626129 + 0.1 * 6.300036430358887
Epoch 620, val loss: 0.96192467212677
Epoch 630, training loss: 0.6513780355453491 = 0.02161138691008091 + 0.1 * 6.297666072845459
Epoch 630, val loss: 0.9699646830558777
Epoch 640, training loss: 0.6496202945709229 = 0.020500363782048225 + 0.1 * 6.291199207305908
Epoch 640, val loss: 0.9775688052177429
Epoch 650, training loss: 0.6490930914878845 = 0.0194769985973835 + 0.1 * 6.296160697937012
Epoch 650, val loss: 0.9850794076919556
Epoch 660, training loss: 0.6485837697982788 = 0.018533743917942047 + 0.1 * 6.30049991607666
Epoch 660, val loss: 0.9923563599586487
Epoch 670, training loss: 0.647497832775116 = 0.01766369119286537 + 0.1 * 6.298341274261475
Epoch 670, val loss: 0.9995625615119934
Epoch 680, training loss: 0.6455676555633545 = 0.016859032213687897 + 0.1 * 6.287086009979248
Epoch 680, val loss: 1.0064650774002075
Epoch 690, training loss: 0.6432828903198242 = 0.016110634431242943 + 0.1 * 6.271722316741943
Epoch 690, val loss: 1.0133931636810303
Epoch 700, training loss: 0.6431282758712769 = 0.015412037260830402 + 0.1 * 6.277162075042725
Epoch 700, val loss: 1.0200936794281006
Epoch 710, training loss: 0.6438264846801758 = 0.014760157093405724 + 0.1 * 6.290663242340088
Epoch 710, val loss: 1.0264233350753784
Epoch 720, training loss: 0.6410883665084839 = 0.014153271913528442 + 0.1 * 6.269350528717041
Epoch 720, val loss: 1.0329148769378662
Epoch 730, training loss: 0.6398298740386963 = 0.013581758365035057 + 0.1 * 6.262481212615967
Epoch 730, val loss: 1.0393192768096924
Epoch 740, training loss: 0.6390787959098816 = 0.013046692125499249 + 0.1 * 6.260321140289307
Epoch 740, val loss: 1.0454457998275757
Epoch 750, training loss: 0.6401503682136536 = 0.012544526718556881 + 0.1 * 6.276058673858643
Epoch 750, val loss: 1.0513163805007935
Epoch 760, training loss: 0.6380852460861206 = 0.01207450870424509 + 0.1 * 6.260107040405273
Epoch 760, val loss: 1.0572558641433716
Epoch 770, training loss: 0.6378821730613708 = 0.011632480658590794 + 0.1 * 6.262496471405029
Epoch 770, val loss: 1.0630877017974854
Epoch 780, training loss: 0.6362352967262268 = 0.011214785277843475 + 0.1 * 6.250205039978027
Epoch 780, val loss: 1.0686215162277222
Epoch 790, training loss: 0.6364610195159912 = 0.010822217911481857 + 0.1 * 6.256387710571289
Epoch 790, val loss: 1.0740545988082886
Epoch 800, training loss: 0.6352279186248779 = 0.010452157817780972 + 0.1 * 6.247757434844971
Epoch 800, val loss: 1.0794308185577393
Epoch 810, training loss: 0.634258508682251 = 0.010102595202624798 + 0.1 * 6.2415595054626465
Epoch 810, val loss: 1.084830403327942
Epoch 820, training loss: 0.634105920791626 = 0.009771296754479408 + 0.1 * 6.243346214294434
Epoch 820, val loss: 1.0901092290878296
Epoch 830, training loss: 0.6347454190254211 = 0.009457380510866642 + 0.1 * 6.252880096435547
Epoch 830, val loss: 1.095086693763733
Epoch 840, training loss: 0.6328447461128235 = 0.0091601787135005 + 0.1 * 6.23684549331665
Epoch 840, val loss: 1.1000630855560303
Epoch 850, training loss: 0.6322757005691528 = 0.008878769353032112 + 0.1 * 6.233969211578369
Epoch 850, val loss: 1.1050928831100464
Epoch 860, training loss: 0.6322464942932129 = 0.008610584773123264 + 0.1 * 6.236358642578125
Epoch 860, val loss: 1.1098744869232178
Epoch 870, training loss: 0.6315887570381165 = 0.008356214500963688 + 0.1 * 6.232325553894043
Epoch 870, val loss: 1.1144821643829346
Epoch 880, training loss: 0.6309318542480469 = 0.008114438503980637 + 0.1 * 6.228173732757568
Epoch 880, val loss: 1.119204044342041
Epoch 890, training loss: 0.6315985918045044 = 0.007883873768150806 + 0.1 * 6.237147331237793
Epoch 890, val loss: 1.1238430738449097
Epoch 900, training loss: 0.6308308839797974 = 0.00766435731202364 + 0.1 * 6.231664657592773
Epoch 900, val loss: 1.1281291246414185
Epoch 910, training loss: 0.6290892958641052 = 0.007454868406057358 + 0.1 * 6.216343879699707
Epoch 910, val loss: 1.1326265335083008
Epoch 920, training loss: 0.6295920014381409 = 0.007255103439092636 + 0.1 * 6.223369121551514
Epoch 920, val loss: 1.136964201927185
Epoch 930, training loss: 0.6292970776557922 = 0.0070645613595843315 + 0.1 * 6.222324848175049
Epoch 930, val loss: 1.1411489248275757
Epoch 940, training loss: 0.628971517086029 = 0.0068825664930045605 + 0.1 * 6.220889568328857
Epoch 940, val loss: 1.1453642845153809
Epoch 950, training loss: 0.6277143359184265 = 0.006708373781293631 + 0.1 * 6.21005916595459
Epoch 950, val loss: 1.1493656635284424
Epoch 960, training loss: 0.6278438568115234 = 0.006541602313518524 + 0.1 * 6.213022232055664
Epoch 960, val loss: 1.1535218954086304
Epoch 970, training loss: 0.6270235776901245 = 0.006380919832736254 + 0.1 * 6.206426620483398
Epoch 970, val loss: 1.1575241088867188
Epoch 980, training loss: 0.6266322135925293 = 0.006226041354238987 + 0.1 * 6.204061508178711
Epoch 980, val loss: 1.161242961883545
Epoch 990, training loss: 0.6270363330841064 = 0.006077664438635111 + 0.1 * 6.2095866203308105
Epoch 990, val loss: 1.1650549173355103
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7454
Flip ASR: 0.7022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.780757188796997 = 1.9433656930923462 + 0.1 * 8.37391471862793
Epoch 0, val loss: 1.9388463497161865
Epoch 10, training loss: 2.7711846828460693 = 1.9337996244430542 + 0.1 * 8.37385082244873
Epoch 10, val loss: 1.929688572883606
Epoch 20, training loss: 2.7597296237945557 = 1.9223765134811401 + 0.1 * 8.373530387878418
Epoch 20, val loss: 1.9182567596435547
Epoch 30, training loss: 2.7438368797302246 = 1.9066991806030273 + 0.1 * 8.371376991271973
Epoch 30, val loss: 1.9022241830825806
Epoch 40, training loss: 2.7193210124969482 = 1.8839101791381836 + 0.1 * 8.354107856750488
Epoch 40, val loss: 1.878947377204895
Epoch 50, training loss: 2.676631212234497 = 1.8515580892562866 + 0.1 * 8.250731468200684
Epoch 50, val loss: 1.8468502759933472
Epoch 60, training loss: 2.602158784866333 = 1.812009334564209 + 0.1 * 7.90149450302124
Epoch 60, val loss: 1.8100733757019043
Epoch 70, training loss: 2.5166172981262207 = 1.7733489274978638 + 0.1 * 7.432684898376465
Epoch 70, val loss: 1.7765122652053833
Epoch 80, training loss: 2.433724880218506 = 1.7331914901733398 + 0.1 * 7.005333423614502
Epoch 80, val loss: 1.7415964603424072
Epoch 90, training loss: 2.3712005615234375 = 1.6837189197540283 + 0.1 * 6.87481689453125
Epoch 90, val loss: 1.698730230331421
Epoch 100, training loss: 2.300781726837158 = 1.6189523935317993 + 0.1 * 6.818292617797852
Epoch 100, val loss: 1.6440857648849487
Epoch 110, training loss: 2.2142252922058105 = 1.5363317728042603 + 0.1 * 6.778934001922607
Epoch 110, val loss: 1.575985312461853
Epoch 120, training loss: 2.1129653453826904 = 1.4376081228256226 + 0.1 * 6.753571510314941
Epoch 120, val loss: 1.4959019422531128
Epoch 130, training loss: 2.002958297729492 = 1.3292564153671265 + 0.1 * 6.737017631530762
Epoch 130, val loss: 1.408611536026001
Epoch 140, training loss: 1.8909728527069092 = 1.2186970710754395 + 0.1 * 6.722757339477539
Epoch 140, val loss: 1.3217304944992065
Epoch 150, training loss: 1.7823288440704346 = 1.1116174459457397 + 0.1 * 6.7071146965026855
Epoch 150, val loss: 1.239738941192627
Epoch 160, training loss: 1.6816502809524536 = 1.012075662612915 + 0.1 * 6.695745944976807
Epoch 160, val loss: 1.1652437448501587
Epoch 170, training loss: 1.591239333152771 = 0.9234631061553955 + 0.1 * 6.677762031555176
Epoch 170, val loss: 1.099393606185913
Epoch 180, training loss: 1.5099619626998901 = 0.8436786532402039 + 0.1 * 6.662833213806152
Epoch 180, val loss: 1.0397553443908691
Epoch 190, training loss: 1.4371843338012695 = 0.7719703912734985 + 0.1 * 6.652139663696289
Epoch 190, val loss: 0.9862877726554871
Epoch 200, training loss: 1.371941328048706 = 0.7077713012695312 + 0.1 * 6.641700267791748
Epoch 200, val loss: 0.9388924837112427
Epoch 210, training loss: 1.3134112358093262 = 0.65032559633255 + 0.1 * 6.630856513977051
Epoch 210, val loss: 0.8979169130325317
Epoch 220, training loss: 1.2615132331848145 = 0.598710298538208 + 0.1 * 6.628028392791748
Epoch 220, val loss: 0.863452136516571
Epoch 230, training loss: 1.2127354145050049 = 0.5513176321983337 + 0.1 * 6.61417818069458
Epoch 230, val loss: 0.8344946503639221
Epoch 240, training loss: 1.166630506515503 = 0.5061125159263611 + 0.1 * 6.605179786682129
Epoch 240, val loss: 0.8095177412033081
Epoch 250, training loss: 1.122823715209961 = 0.46213409304618835 + 0.1 * 6.60689640045166
Epoch 250, val loss: 0.7877277731895447
Epoch 260, training loss: 1.0784361362457275 = 0.4194510877132416 + 0.1 * 6.589849948883057
Epoch 260, val loss: 0.7690595984458923
Epoch 270, training loss: 1.0361363887786865 = 0.377973347902298 + 0.1 * 6.581630229949951
Epoch 270, val loss: 0.7530791163444519
Epoch 280, training loss: 0.9957865476608276 = 0.33851468563079834 + 0.1 * 6.572718620300293
Epoch 280, val loss: 0.7399886250495911
Epoch 290, training loss: 0.9580035209655762 = 0.3015879690647125 + 0.1 * 6.564155101776123
Epoch 290, val loss: 0.7294965386390686
Epoch 300, training loss: 0.9227461814880371 = 0.26739051938056946 + 0.1 * 6.553555965423584
Epoch 300, val loss: 0.7216475009918213
Epoch 310, training loss: 0.8935896754264832 = 0.2363479882478714 + 0.1 * 6.57241678237915
Epoch 310, val loss: 0.7163505554199219
Epoch 320, training loss: 0.8635964393615723 = 0.20902693271636963 + 0.1 * 6.545694828033447
Epoch 320, val loss: 0.713506817817688
Epoch 330, training loss: 0.837651252746582 = 0.1850406974554062 + 0.1 * 6.5261054039001465
Epoch 330, val loss: 0.7129862308502197
Epoch 340, training loss: 0.8158286213874817 = 0.16408418118953705 + 0.1 * 6.517444133758545
Epoch 340, val loss: 0.7146826386451721
Epoch 350, training loss: 0.7978282570838928 = 0.14587855339050293 + 0.1 * 6.519496917724609
Epoch 350, val loss: 0.7182989120483398
Epoch 360, training loss: 0.780933678150177 = 0.13021385669708252 + 0.1 * 6.507197856903076
Epoch 360, val loss: 0.7235406637191772
Epoch 370, training loss: 0.7660987377166748 = 0.11662623286247253 + 0.1 * 6.494724750518799
Epoch 370, val loss: 0.7301682829856873
Epoch 380, training loss: 0.7559044361114502 = 0.10482586920261383 + 0.1 * 6.5107855796813965
Epoch 380, val loss: 0.7378399968147278
Epoch 390, training loss: 0.7426632642745972 = 0.09461712837219238 + 0.1 * 6.480461120605469
Epoch 390, val loss: 0.7461998462677002
Epoch 400, training loss: 0.7333272099494934 = 0.08570074290037155 + 0.1 * 6.476264476776123
Epoch 400, val loss: 0.7552483081817627
Epoch 410, training loss: 0.7236635684967041 = 0.0778871476650238 + 0.1 * 6.457764625549316
Epoch 410, val loss: 0.7647172808647156
Epoch 420, training loss: 0.7163759469985962 = 0.07097932696342468 + 0.1 * 6.453965663909912
Epoch 420, val loss: 0.7745165824890137
Epoch 430, training loss: 0.7099688053131104 = 0.06489840149879456 + 0.1 * 6.450704097747803
Epoch 430, val loss: 0.7844047546386719
Epoch 440, training loss: 0.7029542326927185 = 0.05952087417244911 + 0.1 * 6.434333324432373
Epoch 440, val loss: 0.7944177389144897
Epoch 450, training loss: 0.6971292495727539 = 0.054733313620090485 + 0.1 * 6.423958778381348
Epoch 450, val loss: 0.8044902086257935
Epoch 460, training loss: 0.6937601566314697 = 0.05048187077045441 + 0.1 * 6.432783126831055
Epoch 460, val loss: 0.8144392967224121
Epoch 470, training loss: 0.6902275681495667 = 0.0466984286904335 + 0.1 * 6.435290813446045
Epoch 470, val loss: 0.8243509531021118
Epoch 480, training loss: 0.6839282512664795 = 0.04332207515835762 + 0.1 * 6.40606164932251
Epoch 480, val loss: 0.8340440392494202
Epoch 490, training loss: 0.6801609992980957 = 0.0402851365506649 + 0.1 * 6.398758888244629
Epoch 490, val loss: 0.8435958027839661
Epoch 500, training loss: 0.6758556365966797 = 0.03755207732319832 + 0.1 * 6.383035659790039
Epoch 500, val loss: 0.8529651165008545
Epoch 510, training loss: 0.6733540296554565 = 0.035084452480077744 + 0.1 * 6.382696151733398
Epoch 510, val loss: 0.8621761202812195
Epoch 520, training loss: 0.6702351570129395 = 0.03284882754087448 + 0.1 * 6.373863220214844
Epoch 520, val loss: 0.871184229850769
Epoch 530, training loss: 0.6693934798240662 = 0.030816564336419106 + 0.1 * 6.385769367218018
Epoch 530, val loss: 0.8800913691520691
Epoch 540, training loss: 0.6663380861282349 = 0.02897064760327339 + 0.1 * 6.373673915863037
Epoch 540, val loss: 0.8887197375297546
Epoch 550, training loss: 0.664577305316925 = 0.027285240590572357 + 0.1 * 6.372920989990234
Epoch 550, val loss: 0.8972269296646118
Epoch 560, training loss: 0.6608818769454956 = 0.025744283571839333 + 0.1 * 6.351376056671143
Epoch 560, val loss: 0.9055078625679016
Epoch 570, training loss: 0.6599317789077759 = 0.0243314690887928 + 0.1 * 6.356003284454346
Epoch 570, val loss: 0.9136448502540588
Epoch 580, training loss: 0.657646656036377 = 0.023033076897263527 + 0.1 * 6.34613561630249
Epoch 580, val loss: 0.9215531349182129
Epoch 590, training loss: 0.6574306488037109 = 0.0218352060765028 + 0.1 * 6.355954170227051
Epoch 590, val loss: 0.9293579459190369
Epoch 600, training loss: 0.6549369096755981 = 0.020730871707201004 + 0.1 * 6.342060565948486
Epoch 600, val loss: 0.9370373487472534
Epoch 610, training loss: 0.65556401014328 = 0.019707655534148216 + 0.1 * 6.358563423156738
Epoch 610, val loss: 0.9445225596427917
Epoch 620, training loss: 0.651001513004303 = 0.018763553351163864 + 0.1 * 6.3223795890808105
Epoch 620, val loss: 0.9517643451690674
Epoch 630, training loss: 0.652557909488678 = 0.017885969951748848 + 0.1 * 6.346719264984131
Epoch 630, val loss: 0.9589474201202393
Epoch 640, training loss: 0.6493023037910461 = 0.017070647329092026 + 0.1 * 6.3223161697387695
Epoch 640, val loss: 0.9659481644630432
Epoch 650, training loss: 0.6487203240394592 = 0.0163117703050375 + 0.1 * 6.324085712432861
Epoch 650, val loss: 0.9728146195411682
Epoch 660, training loss: 0.647092342376709 = 0.015603885054588318 + 0.1 * 6.314884185791016
Epoch 660, val loss: 0.9795100688934326
Epoch 670, training loss: 0.6466948986053467 = 0.014940649271011353 + 0.1 * 6.31754207611084
Epoch 670, val loss: 0.9862042665481567
Epoch 680, training loss: 0.6459317803382874 = 0.01432120893150568 + 0.1 * 6.316105365753174
Epoch 680, val loss: 0.9926202893257141
Epoch 690, training loss: 0.6440960168838501 = 0.013741714879870415 + 0.1 * 6.303542613983154
Epoch 690, val loss: 0.9989835619926453
Epoch 700, training loss: 0.6439939737319946 = 0.013197120279073715 + 0.1 * 6.307968616485596
Epoch 700, val loss: 1.0053114891052246
Epoch 710, training loss: 0.6424349546432495 = 0.01268668845295906 + 0.1 * 6.297482490539551
Epoch 710, val loss: 1.0113499164581299
Epoch 720, training loss: 0.6428124904632568 = 0.012206378392875195 + 0.1 * 6.306060791015625
Epoch 720, val loss: 1.0174063444137573
Epoch 730, training loss: 0.6415998935699463 = 0.011753554455935955 + 0.1 * 6.298463344573975
Epoch 730, val loss: 1.0232685804367065
Epoch 740, training loss: 0.642152726650238 = 0.01132813561707735 + 0.1 * 6.308245658874512
Epoch 740, val loss: 1.0289949178695679
Epoch 750, training loss: 0.6396084427833557 = 0.010927309282124043 + 0.1 * 6.286810874938965
Epoch 750, val loss: 1.0345938205718994
Epoch 760, training loss: 0.6386033892631531 = 0.010548267513513565 + 0.1 * 6.280550956726074
Epoch 760, val loss: 1.0401643514633179
Epoch 770, training loss: 0.639500081539154 = 0.010189061053097248 + 0.1 * 6.293109893798828
Epoch 770, val loss: 1.0456387996673584
Epoch 780, training loss: 0.6387757658958435 = 0.009850242175161839 + 0.1 * 6.289255142211914
Epoch 780, val loss: 1.0509352684020996
Epoch 790, training loss: 0.6383927464485168 = 0.00952870398759842 + 0.1 * 6.28864049911499
Epoch 790, val loss: 1.056100606918335
Epoch 800, training loss: 0.6358973979949951 = 0.009224563837051392 + 0.1 * 6.266727924346924
Epoch 800, val loss: 1.0612223148345947
Epoch 810, training loss: 0.6371976733207703 = 0.008935711346566677 + 0.1 * 6.282619953155518
Epoch 810, val loss: 1.0663694143295288
Epoch 820, training loss: 0.6362653374671936 = 0.00866102334111929 + 0.1 * 6.276042938232422
Epoch 820, val loss: 1.0713014602661133
Epoch 830, training loss: 0.6352179050445557 = 0.008400051854550838 + 0.1 * 6.268178939819336
Epoch 830, val loss: 1.076111912727356
Epoch 840, training loss: 0.6347956657409668 = 0.008151145651936531 + 0.1 * 6.266444683074951
Epoch 840, val loss: 1.0809358358383179
Epoch 850, training loss: 0.6336045861244202 = 0.007914404384791851 + 0.1 * 6.256901741027832
Epoch 850, val loss: 1.0856142044067383
Epoch 860, training loss: 0.6339319348335266 = 0.00768912211060524 + 0.1 * 6.262428283691406
Epoch 860, val loss: 1.0902621746063232
Epoch 870, training loss: 0.6342825889587402 = 0.007473852019757032 + 0.1 * 6.268087387084961
Epoch 870, val loss: 1.0948352813720703
Epoch 880, training loss: 0.6324949860572815 = 0.007268527057021856 + 0.1 * 6.252264022827148
Epoch 880, val loss: 1.099267601966858
Epoch 890, training loss: 0.6329377889633179 = 0.00707212183624506 + 0.1 * 6.258656024932861
Epoch 890, val loss: 1.1037229299545288
Epoch 900, training loss: 0.6322874426841736 = 0.006884206552058458 + 0.1 * 6.254032135009766
Epoch 900, val loss: 1.1080527305603027
Epoch 910, training loss: 0.6318032741546631 = 0.0067046754993498325 + 0.1 * 6.250985622406006
Epoch 910, val loss: 1.1123119592666626
Epoch 920, training loss: 0.6313443779945374 = 0.0065327598713338375 + 0.1 * 6.248115539550781
Epoch 920, val loss: 1.1165045499801636
Epoch 930, training loss: 0.6303509473800659 = 0.006368981208652258 + 0.1 * 6.239819526672363
Epoch 930, val loss: 1.1205215454101562
Epoch 940, training loss: 0.6302535533905029 = 0.006211588624864817 + 0.1 * 6.240419864654541
Epoch 940, val loss: 1.12466561794281
Epoch 950, training loss: 0.6307061910629272 = 0.006060040555894375 + 0.1 * 6.246461391448975
Epoch 950, val loss: 1.128692626953125
Epoch 960, training loss: 0.6303004622459412 = 0.005914891138672829 + 0.1 * 6.243855953216553
Epoch 960, val loss: 1.1325631141662598
Epoch 970, training loss: 0.6295604705810547 = 0.005775527097284794 + 0.1 * 6.237849235534668
Epoch 970, val loss: 1.136359453201294
Epoch 980, training loss: 0.6289656758308411 = 0.005641853902488947 + 0.1 * 6.2332377433776855
Epoch 980, val loss: 1.140224575996399
Epoch 990, training loss: 0.6291322708129883 = 0.005513050593435764 + 0.1 * 6.236191749572754
Epoch 990, val loss: 1.143883228302002
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8303
Flip ASR: 0.7956/225 nodes
The final ASR:0.75031, 0.06337, Accuracy:0.81852, 0.01210
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9568])
updated graph: torch.Size([2, 10622])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.97048, 0.00603, Accuracy:0.83210, 0.00630
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7760932445526123 = 1.9387189149856567 + 0.1 * 8.373743057250977
Epoch 0, val loss: 1.9318022727966309
Epoch 10, training loss: 2.765993595123291 = 1.9286575317382812 + 0.1 * 8.373361587524414
Epoch 10, val loss: 1.9229241609573364
Epoch 20, training loss: 2.753180503845215 = 1.9160523414611816 + 0.1 * 8.371280670166016
Epoch 20, val loss: 1.911473035812378
Epoch 30, training loss: 2.733961820602417 = 1.898165225982666 + 0.1 * 8.357965469360352
Epoch 30, val loss: 1.8951972723007202
Epoch 40, training loss: 2.7001729011535645 = 1.8721754550933838 + 0.1 * 8.279974937438965
Epoch 40, val loss: 1.8722145557403564
Epoch 50, training loss: 2.623730182647705 = 1.8389415740966797 + 0.1 * 7.847886562347412
Epoch 50, val loss: 1.8440641164779663
Epoch 60, training loss: 2.547110080718994 = 1.8042980432510376 + 0.1 * 7.428121566772461
Epoch 60, val loss: 1.816064715385437
Epoch 70, training loss: 2.475581169128418 = 1.7725557088851929 + 0.1 * 7.030253887176514
Epoch 70, val loss: 1.7920129299163818
Epoch 80, training loss: 2.4236371517181396 = 1.7406973838806152 + 0.1 * 6.829396724700928
Epoch 80, val loss: 1.76591157913208
Epoch 90, training loss: 2.3693795204162598 = 1.69919753074646 + 0.1 * 6.701818943023682
Epoch 90, val loss: 1.728127360343933
Epoch 100, training loss: 2.3065593242645264 = 1.6425377130508423 + 0.1 * 6.640216827392578
Epoch 100, val loss: 1.679290533065796
Epoch 110, training loss: 2.2284436225891113 = 1.5683960914611816 + 0.1 * 6.6004743576049805
Epoch 110, val loss: 1.618788480758667
Epoch 120, training loss: 2.140533208847046 = 1.4831162691116333 + 0.1 * 6.574170112609863
Epoch 120, val loss: 1.5502665042877197
Epoch 130, training loss: 2.0520317554473877 = 1.3967479467391968 + 0.1 * 6.552838325500488
Epoch 130, val loss: 1.4820483922958374
Epoch 140, training loss: 1.9666965007781982 = 1.313216209411621 + 0.1 * 6.534802436828613
Epoch 140, val loss: 1.416914939880371
Epoch 150, training loss: 1.8836555480957031 = 1.2320115566253662 + 0.1 * 6.516439437866211
Epoch 150, val loss: 1.3549388647079468
Epoch 160, training loss: 1.8016257286071777 = 1.1513874530792236 + 0.1 * 6.502381801605225
Epoch 160, val loss: 1.2946845293045044
Epoch 170, training loss: 1.7192769050598145 = 1.0705596208572388 + 0.1 * 6.487173080444336
Epoch 170, val loss: 1.2356421947479248
Epoch 180, training loss: 1.6360541582107544 = 0.9887000918388367 + 0.1 * 6.473540782928467
Epoch 180, val loss: 1.176498293876648
Epoch 190, training loss: 1.5530554056167603 = 0.9061022996902466 + 0.1 * 6.469531059265137
Epoch 190, val loss: 1.1170247793197632
Epoch 200, training loss: 1.4724757671356201 = 0.8269146680831909 + 0.1 * 6.455611705780029
Epoch 200, val loss: 1.060376524925232
Epoch 210, training loss: 1.3971214294433594 = 0.7526968121528625 + 0.1 * 6.444246292114258
Epoch 210, val loss: 1.0076638460159302
Epoch 220, training loss: 1.3278069496154785 = 0.6843547224998474 + 0.1 * 6.4345221519470215
Epoch 220, val loss: 0.9604597091674805
Epoch 230, training loss: 1.2665109634399414 = 0.6226714253425598 + 0.1 * 6.438394546508789
Epoch 230, val loss: 0.9204299449920654
Epoch 240, training loss: 1.2093385457992554 = 0.5672611594200134 + 0.1 * 6.420773983001709
Epoch 240, val loss: 0.8878790140151978
Epoch 250, training loss: 1.1584692001342773 = 0.5165485143661499 + 0.1 * 6.419207572937012
Epoch 250, val loss: 0.8614998459815979
Epoch 260, training loss: 1.110888957977295 = 0.47041159868240356 + 0.1 * 6.404774188995361
Epoch 260, val loss: 0.8408054113388062
Epoch 270, training loss: 1.0680869817733765 = 0.4283435344696045 + 0.1 * 6.397434234619141
Epoch 270, val loss: 0.825377345085144
Epoch 280, training loss: 1.0294692516326904 = 0.3900759816169739 + 0.1 * 6.393933296203613
Epoch 280, val loss: 0.814703643321991
Epoch 290, training loss: 0.9940635561943054 = 0.35505878925323486 + 0.1 * 6.390047550201416
Epoch 290, val loss: 0.808242678642273
Epoch 300, training loss: 0.9610112309455872 = 0.32275938987731934 + 0.1 * 6.382518291473389
Epoch 300, val loss: 0.805025577545166
Epoch 310, training loss: 0.9297164678573608 = 0.29248636960983276 + 0.1 * 6.37230110168457
Epoch 310, val loss: 0.8043314814567566
Epoch 320, training loss: 0.9032430648803711 = 0.2639082074165344 + 0.1 * 6.393348693847656
Epoch 320, val loss: 0.8059489727020264
Epoch 330, training loss: 0.8739485740661621 = 0.23738369345664978 + 0.1 * 6.36564826965332
Epoch 330, val loss: 0.8094627261161804
Epoch 340, training loss: 0.848522424697876 = 0.2129480540752411 + 0.1 * 6.355743408203125
Epoch 340, val loss: 0.8147291541099548
Epoch 350, training loss: 0.8256770968437195 = 0.19080011546611786 + 0.1 * 6.348769664764404
Epoch 350, val loss: 0.8217665553092957
Epoch 360, training loss: 0.8054540753364563 = 0.17107373476028442 + 0.1 * 6.343803405761719
Epoch 360, val loss: 0.830325186252594
Epoch 370, training loss: 0.7876197695732117 = 0.153684601187706 + 0.1 * 6.339351177215576
Epoch 370, val loss: 0.8402737975120544
Epoch 380, training loss: 0.7733671069145203 = 0.13851206004619598 + 0.1 * 6.348550319671631
Epoch 380, val loss: 0.8512011766433716
Epoch 390, training loss: 0.7585065960884094 = 0.1253475397825241 + 0.1 * 6.331590175628662
Epoch 390, val loss: 0.8632075190544128
Epoch 400, training loss: 0.7461198568344116 = 0.1138123869895935 + 0.1 * 6.323074817657471
Epoch 400, val loss: 0.875771164894104
Epoch 410, training loss: 0.7351750135421753 = 0.10363955795764923 + 0.1 * 6.315354347229004
Epoch 410, val loss: 0.8888838887214661
Epoch 420, training loss: 0.7281575798988342 = 0.09462615102529526 + 0.1 * 6.3353142738342285
Epoch 420, val loss: 0.9022441506385803
Epoch 430, training loss: 0.7182583808898926 = 0.0866754949092865 + 0.1 * 6.315829277038574
Epoch 430, val loss: 0.9155983924865723
Epoch 440, training loss: 0.71015864610672 = 0.0796077772974968 + 0.1 * 6.305508613586426
Epoch 440, val loss: 0.9290099143981934
Epoch 450, training loss: 0.7039909958839417 = 0.07328325510025024 + 0.1 * 6.307077407836914
Epoch 450, val loss: 0.9424692392349243
Epoch 460, training loss: 0.6981933116912842 = 0.06761650741100311 + 0.1 * 6.30576753616333
Epoch 460, val loss: 0.9558270573616028
Epoch 470, training loss: 0.6922000646591187 = 0.06252221018075943 + 0.1 * 6.296778202056885
Epoch 470, val loss: 0.969190239906311
Epoch 480, training loss: 0.6876608729362488 = 0.0579233318567276 + 0.1 * 6.297375202178955
Epoch 480, val loss: 0.9822764992713928
Epoch 490, training loss: 0.6825805902481079 = 0.053769223392009735 + 0.1 * 6.288113594055176
Epoch 490, val loss: 0.9952712655067444
Epoch 500, training loss: 0.6781478524208069 = 0.0499965138733387 + 0.1 * 6.281513690948486
Epoch 500, val loss: 1.0080605745315552
Epoch 510, training loss: 0.6762611865997314 = 0.04656397923827171 + 0.1 * 6.296971797943115
Epoch 510, val loss: 1.0205447673797607
Epoch 520, training loss: 0.6718831658363342 = 0.043446656316518784 + 0.1 * 6.284364700317383
Epoch 520, val loss: 1.0327192544937134
Epoch 530, training loss: 0.6677671670913696 = 0.04060935974121094 + 0.1 * 6.271577835083008
Epoch 530, val loss: 1.0446817874908447
Epoch 540, training loss: 0.664563775062561 = 0.03801514953374863 + 0.1 * 6.265486240386963
Epoch 540, val loss: 1.0563163757324219
Epoch 550, training loss: 0.6630343198776245 = 0.03563734143972397 + 0.1 * 6.273970127105713
Epoch 550, val loss: 1.067686915397644
Epoch 560, training loss: 0.6601217985153198 = 0.03346084803342819 + 0.1 * 6.266609191894531
Epoch 560, val loss: 1.0786641836166382
Epoch 570, training loss: 0.6577524542808533 = 0.031466059386730194 + 0.1 * 6.262863636016846
Epoch 570, val loss: 1.0895390510559082
Epoch 580, training loss: 0.6554181575775146 = 0.029634147882461548 + 0.1 * 6.257840156555176
Epoch 580, val loss: 1.1000045537948608
Epoch 590, training loss: 0.6534575819969177 = 0.02795293554663658 + 0.1 * 6.255046367645264
Epoch 590, val loss: 1.1101399660110474
Epoch 600, training loss: 0.6511426568031311 = 0.02640487812459469 + 0.1 * 6.247377872467041
Epoch 600, val loss: 1.1201480627059937
Epoch 610, training loss: 0.650033175945282 = 0.024974433705210686 + 0.1 * 6.250587463378906
Epoch 610, val loss: 1.1298372745513916
Epoch 620, training loss: 0.6476192474365234 = 0.023651886731386185 + 0.1 * 6.239673614501953
Epoch 620, val loss: 1.1390858888626099
Epoch 630, training loss: 0.6462154388427734 = 0.022432204335927963 + 0.1 * 6.237832069396973
Epoch 630, val loss: 1.1484216451644897
Epoch 640, training loss: 0.6457589864730835 = 0.021299155429005623 + 0.1 * 6.244597911834717
Epoch 640, val loss: 1.1572201251983643
Epoch 650, training loss: 0.6432287693023682 = 0.02025211602449417 + 0.1 * 6.229766368865967
Epoch 650, val loss: 1.1660637855529785
Epoch 660, training loss: 0.6422675848007202 = 0.019277729094028473 + 0.1 * 6.229897975921631
Epoch 660, val loss: 1.1746526956558228
Epoch 670, training loss: 0.6404215693473816 = 0.018369700759649277 + 0.1 * 6.220518589019775
Epoch 670, val loss: 1.1828632354736328
Epoch 680, training loss: 0.639595627784729 = 0.017525790259242058 + 0.1 * 6.220698356628418
Epoch 680, val loss: 1.1911345720291138
Epoch 690, training loss: 0.6390016674995422 = 0.01673710346221924 + 0.1 * 6.222645282745361
Epoch 690, val loss: 1.1989800930023193
Epoch 700, training loss: 0.6381297707557678 = 0.016002275049686432 + 0.1 * 6.2212748527526855
Epoch 700, val loss: 1.2068382501602173
Epoch 710, training loss: 0.6374526619911194 = 0.015314118005335331 + 0.1 * 6.221385478973389
Epoch 710, val loss: 1.2143186330795288
Epoch 720, training loss: 0.6359771490097046 = 0.014671694487333298 + 0.1 * 6.213054656982422
Epoch 720, val loss: 1.221772313117981
Epoch 730, training loss: 0.6350004076957703 = 0.01406946312636137 + 0.1 * 6.2093095779418945
Epoch 730, val loss: 1.2291784286499023
Epoch 740, training loss: 0.634811282157898 = 0.013502772897481918 + 0.1 * 6.213085174560547
Epoch 740, val loss: 1.2361823320388794
Epoch 750, training loss: 0.6331396102905273 = 0.012971111573278904 + 0.1 * 6.201684951782227
Epoch 750, val loss: 1.2432153224945068
Epoch 760, training loss: 0.6335476040840149 = 0.012470617890357971 + 0.1 * 6.2107696533203125
Epoch 760, val loss: 1.2501003742218018
Epoch 770, training loss: 0.6322579979896545 = 0.011998984962701797 + 0.1 * 6.202589511871338
Epoch 770, val loss: 1.2567709684371948
Epoch 780, training loss: 0.632138192653656 = 0.01155469473451376 + 0.1 * 6.20583438873291
Epoch 780, val loss: 1.2632777690887451
Epoch 790, training loss: 0.6309556365013123 = 0.011136796325445175 + 0.1 * 6.198188304901123
Epoch 790, val loss: 1.2697547674179077
Epoch 800, training loss: 0.6309070587158203 = 0.010741961188614368 + 0.1 * 6.201651096343994
Epoch 800, val loss: 1.2760443687438965
Epoch 810, training loss: 0.6294480562210083 = 0.010369223542511463 + 0.1 * 6.190788269042969
Epoch 810, val loss: 1.2823225259780884
Epoch 820, training loss: 0.6300148367881775 = 0.010015798732638359 + 0.1 * 6.199990272521973
Epoch 820, val loss: 1.2884068489074707
Epoch 830, training loss: 0.6286430358886719 = 0.009680833667516708 + 0.1 * 6.189622402191162
Epoch 830, val loss: 1.294205904006958
Epoch 840, training loss: 0.6279693245887756 = 0.009364848025143147 + 0.1 * 6.186044692993164
Epoch 840, val loss: 1.300310730934143
Epoch 850, training loss: 0.6274037957191467 = 0.009063806384801865 + 0.1 * 6.1834001541137695
Epoch 850, val loss: 1.306066632270813
Epoch 860, training loss: 0.6286163330078125 = 0.008777381852269173 + 0.1 * 6.198389530181885
Epoch 860, val loss: 1.311592936515808
Epoch 870, training loss: 0.6269104480743408 = 0.008506431244313717 + 0.1 * 6.184040069580078
Epoch 870, val loss: 1.3171547651290894
Epoch 880, training loss: 0.6260195374488831 = 0.008249166421592236 + 0.1 * 6.177703380584717
Epoch 880, val loss: 1.322774052619934
Epoch 890, training loss: 0.627264678478241 = 0.008003339171409607 + 0.1 * 6.192613124847412
Epoch 890, val loss: 1.3280222415924072
Epoch 900, training loss: 0.6257664561271667 = 0.007769023068249226 + 0.1 * 6.17997407913208
Epoch 900, val loss: 1.3331578969955444
Epoch 910, training loss: 0.6247484087944031 = 0.007546673063188791 + 0.1 * 6.1720170974731445
Epoch 910, val loss: 1.3384674787521362
Epoch 920, training loss: 0.6246927976608276 = 0.007333716377615929 + 0.1 * 6.173591136932373
Epoch 920, val loss: 1.3434970378875732
Epoch 930, training loss: 0.624130129814148 = 0.007130612153559923 + 0.1 * 6.169994831085205
Epoch 930, val loss: 1.348554253578186
Epoch 940, training loss: 0.6236604452133179 = 0.006936453748494387 + 0.1 * 6.167240142822266
Epoch 940, val loss: 1.3533737659454346
Epoch 950, training loss: 0.6244816780090332 = 0.006751254200935364 + 0.1 * 6.177304267883301
Epoch 950, val loss: 1.3582541942596436
Epoch 960, training loss: 0.6234874129295349 = 0.00657389173284173 + 0.1 * 6.169135093688965
Epoch 960, val loss: 1.362829327583313
Epoch 970, training loss: 0.6229005455970764 = 0.006405324675142765 + 0.1 * 6.164951801300049
Epoch 970, val loss: 1.3676267862319946
Epoch 980, training loss: 0.6229960918426514 = 0.006243059877306223 + 0.1 * 6.167530059814453
Epoch 980, val loss: 1.3721654415130615
Epoch 990, training loss: 0.6225639581680298 = 0.0060874964110553265 + 0.1 * 6.164764404296875
Epoch 990, val loss: 1.3765801191329956
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.4834
Flip ASR: 0.3956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7923166751861572 = 1.954928994178772 + 0.1 * 8.373876571655273
Epoch 0, val loss: 1.9536964893341064
Epoch 10, training loss: 2.7818055152893066 = 1.9444301128387451 + 0.1 * 8.37375259399414
Epoch 10, val loss: 1.943099856376648
Epoch 20, training loss: 2.7687976360321045 = 1.9314684867858887 + 0.1 * 8.373291015625
Epoch 20, val loss: 1.9295498132705688
Epoch 30, training loss: 2.7505407333374023 = 1.9134631156921387 + 0.1 * 8.37077522277832
Epoch 30, val loss: 1.9102274179458618
Epoch 40, training loss: 2.722282886505127 = 1.8868886232376099 + 0.1 * 8.353943824768066
Epoch 40, val loss: 1.8816502094268799
Epoch 50, training loss: 2.673490285873413 = 1.8492438793182373 + 0.1 * 8.242463111877441
Epoch 50, val loss: 1.8427152633666992
Epoch 60, training loss: 2.5680510997772217 = 1.8064138889312744 + 0.1 * 7.6163716316223145
Epoch 60, val loss: 1.8019638061523438
Epoch 70, training loss: 2.4870269298553467 = 1.768928050994873 + 0.1 * 7.180988788604736
Epoch 70, val loss: 1.7684918642044067
Epoch 80, training loss: 2.425443649291992 = 1.731670618057251 + 0.1 * 6.93773078918457
Epoch 80, val loss: 1.7362016439437866
Epoch 90, training loss: 2.367692470550537 = 1.685210108757019 + 0.1 * 6.824824333190918
Epoch 90, val loss: 1.695367693901062
Epoch 100, training loss: 2.2994110584259033 = 1.6229205131530762 + 0.1 * 6.764904499053955
Epoch 100, val loss: 1.6414164304733276
Epoch 110, training loss: 2.2156174182891846 = 1.543074607849121 + 0.1 * 6.725427627563477
Epoch 110, val loss: 1.575335144996643
Epoch 120, training loss: 2.1199281215667725 = 1.4503288269042969 + 0.1 * 6.695992946624756
Epoch 120, val loss: 1.5006954669952393
Epoch 130, training loss: 2.020740509033203 = 1.352987289428711 + 0.1 * 6.6775312423706055
Epoch 130, val loss: 1.4237630367279053
Epoch 140, training loss: 1.9229540824890137 = 1.2569078207015991 + 0.1 * 6.660462379455566
Epoch 140, val loss: 1.3508083820343018
Epoch 150, training loss: 1.8280599117279053 = 1.1638015508651733 + 0.1 * 6.642584323883057
Epoch 150, val loss: 1.2810297012329102
Epoch 160, training loss: 1.7391856908798218 = 1.0760027170181274 + 0.1 * 6.631829738616943
Epoch 160, val loss: 1.2147729396820068
Epoch 170, training loss: 1.6580958366394043 = 0.997033417224884 + 0.1 * 6.61062479019165
Epoch 170, val loss: 1.155269742012024
Epoch 180, training loss: 1.5853807926177979 = 0.9257842898368835 + 0.1 * 6.595964431762695
Epoch 180, val loss: 1.1021003723144531
Epoch 190, training loss: 1.5178112983703613 = 0.8598156571388245 + 0.1 * 6.579955577850342
Epoch 190, val loss: 1.0535868406295776
Epoch 200, training loss: 1.4528255462646484 = 0.7963191866874695 + 0.1 * 6.565063953399658
Epoch 200, val loss: 1.0072567462921143
Epoch 210, training loss: 1.3897647857666016 = 0.7343940138816833 + 0.1 * 6.553707122802734
Epoch 210, val loss: 0.9625356197357178
Epoch 220, training loss: 1.3286151885986328 = 0.6742850542068481 + 0.1 * 6.543301105499268
Epoch 220, val loss: 0.919296383857727
Epoch 230, training loss: 1.269242525100708 = 0.6163119077682495 + 0.1 * 6.529305934906006
Epoch 230, val loss: 0.8779494166374207
Epoch 240, training loss: 1.2142422199249268 = 0.5618497729301453 + 0.1 * 6.523923873901367
Epoch 240, val loss: 0.8402990698814392
Epoch 250, training loss: 1.1634125709533691 = 0.5121585726737976 + 0.1 * 6.512539386749268
Epoch 250, val loss: 0.8074801564216614
Epoch 260, training loss: 1.11720871925354 = 0.4673256576061249 + 0.1 * 6.498830795288086
Epoch 260, val loss: 0.7795667052268982
Epoch 270, training loss: 1.078073263168335 = 0.4267776608467102 + 0.1 * 6.512956619262695
Epoch 270, val loss: 0.7560788989067078
Epoch 280, training loss: 1.0381587743759155 = 0.3904310166835785 + 0.1 * 6.4772772789001465
Epoch 280, val loss: 0.7369596362113953
Epoch 290, training loss: 1.0039288997650146 = 0.357223778963089 + 0.1 * 6.4670515060424805
Epoch 290, val loss: 0.7215796113014221
Epoch 300, training loss: 0.9743188619613647 = 0.3267695903778076 + 0.1 * 6.475492477416992
Epoch 300, val loss: 0.7095426321029663
Epoch 310, training loss: 0.9438756108283997 = 0.29881441593170166 + 0.1 * 6.4506120681762695
Epoch 310, val loss: 0.7003593444824219
Epoch 320, training loss: 0.9174846410751343 = 0.27300819754600525 + 0.1 * 6.444764614105225
Epoch 320, val loss: 0.6936372518539429
Epoch 330, training loss: 0.8925163149833679 = 0.2492588311433792 + 0.1 * 6.43257474899292
Epoch 330, val loss: 0.6890453696250916
Epoch 340, training loss: 0.869668185710907 = 0.22728393971920013 + 0.1 * 6.423842430114746
Epoch 340, val loss: 0.6865197420120239
Epoch 350, training loss: 0.8489671349525452 = 0.20719556510448456 + 0.1 * 6.417715549468994
Epoch 350, val loss: 0.6858200430870056
Epoch 360, training loss: 0.8293853402137756 = 0.18895964324474335 + 0.1 * 6.404256820678711
Epoch 360, val loss: 0.6866199970245361
Epoch 370, training loss: 0.8127958178520203 = 0.1724013090133667 + 0.1 * 6.403944969177246
Epoch 370, val loss: 0.6890546083450317
Epoch 380, training loss: 0.7971486449241638 = 0.15753908455371857 + 0.1 * 6.3960957527160645
Epoch 380, val loss: 0.6928007006645203
Epoch 390, training loss: 0.7827540040016174 = 0.1442817598581314 + 0.1 * 6.3847222328186035
Epoch 390, val loss: 0.6977845430374146
Epoch 400, training loss: 0.7713682055473328 = 0.13242340087890625 + 0.1 * 6.3894476890563965
Epoch 400, val loss: 0.7038145065307617
Epoch 410, training loss: 0.7589691877365112 = 0.12188513576984406 + 0.1 * 6.370840549468994
Epoch 410, val loss: 0.7106842398643494
Epoch 420, training loss: 0.7483106255531311 = 0.11246166378259659 + 0.1 * 6.358489036560059
Epoch 420, val loss: 0.7182679176330566
Epoch 430, training loss: 0.7395325303077698 = 0.10398714989423752 + 0.1 * 6.3554534912109375
Epoch 430, val loss: 0.7263951897621155
Epoch 440, training loss: 0.7326258420944214 = 0.0964064970612526 + 0.1 * 6.3621931076049805
Epoch 440, val loss: 0.7348508834838867
Epoch 450, training loss: 0.7241232991218567 = 0.08962352573871613 + 0.1 * 6.344997882843018
Epoch 450, val loss: 0.7436318397521973
Epoch 460, training loss: 0.7169922590255737 = 0.08350986987352371 + 0.1 * 6.334824085235596
Epoch 460, val loss: 0.7525362968444824
Epoch 470, training loss: 0.7107356190681458 = 0.0780089721083641 + 0.1 * 6.327266693115234
Epoch 470, val loss: 0.7615126371383667
Epoch 480, training loss: 0.705359697341919 = 0.0730724036693573 + 0.1 * 6.322873115539551
Epoch 480, val loss: 0.7705333828926086
Epoch 490, training loss: 0.7010664939880371 = 0.06859774887561798 + 0.1 * 6.324687480926514
Epoch 490, val loss: 0.7795504927635193
Epoch 500, training loss: 0.6962540149688721 = 0.06453613936901093 + 0.1 * 6.317178726196289
Epoch 500, val loss: 0.7885984182357788
Epoch 510, training loss: 0.6920900940895081 = 0.060829464346170425 + 0.1 * 6.312606334686279
Epoch 510, val loss: 0.7976765036582947
Epoch 520, training loss: 0.687777042388916 = 0.057447489351034164 + 0.1 * 6.303295612335205
Epoch 520, val loss: 0.8066267967224121
Epoch 530, training loss: 0.6842052936553955 = 0.05436497554183006 + 0.1 * 6.298403263092041
Epoch 530, val loss: 0.8154752254486084
Epoch 540, training loss: 0.6808191537857056 = 0.051541898399591446 + 0.1 * 6.29277229309082
Epoch 540, val loss: 0.8242998719215393
Epoch 550, training loss: 0.6781356334686279 = 0.04893939942121506 + 0.1 * 6.291962146759033
Epoch 550, val loss: 0.8329017162322998
Epoch 560, training loss: 0.6754369139671326 = 0.04653454199433327 + 0.1 * 6.289023399353027
Epoch 560, val loss: 0.8413872718811035
Epoch 570, training loss: 0.6723606586456299 = 0.044313061982393265 + 0.1 * 6.280475616455078
Epoch 570, val loss: 0.8498939871788025
Epoch 580, training loss: 0.669633686542511 = 0.04224345460534096 + 0.1 * 6.27390193939209
Epoch 580, val loss: 0.8582022786140442
Epoch 590, training loss: 0.6682969331741333 = 0.04032036289572716 + 0.1 * 6.279766082763672
Epoch 590, val loss: 0.8663532137870789
Epoch 600, training loss: 0.6660701036453247 = 0.03853124380111694 + 0.1 * 6.275388717651367
Epoch 600, val loss: 0.8745248913764954
Epoch 610, training loss: 0.663903534412384 = 0.03685727342963219 + 0.1 * 6.270462989807129
Epoch 610, val loss: 0.8824910521507263
Epoch 620, training loss: 0.6611936688423157 = 0.03528740257024765 + 0.1 * 6.259062767028809
Epoch 620, val loss: 0.8902559876441956
Epoch 630, training loss: 0.6603916883468628 = 0.033808521926403046 + 0.1 * 6.265831470489502
Epoch 630, val loss: 0.8978397250175476
Epoch 640, training loss: 0.6576749682426453 = 0.03240307420492172 + 0.1 * 6.252718925476074
Epoch 640, val loss: 0.9053995609283447
Epoch 650, training loss: 0.6564723253250122 = 0.03106754459440708 + 0.1 * 6.254047870635986
Epoch 650, val loss: 0.9126948118209839
Epoch 660, training loss: 0.6544970273971558 = 0.029786668717861176 + 0.1 * 6.247103691101074
Epoch 660, val loss: 0.9198665022850037
Epoch 670, training loss: 0.653130054473877 = 0.028567207977175713 + 0.1 * 6.245628356933594
Epoch 670, val loss: 0.9269561767578125
Epoch 680, training loss: 0.6520019173622131 = 0.027405422180891037 + 0.1 * 6.245964527130127
Epoch 680, val loss: 0.9339421391487122
Epoch 690, training loss: 0.650770902633667 = 0.026296323165297508 + 0.1 * 6.24474573135376
Epoch 690, val loss: 0.9406988024711609
Epoch 700, training loss: 0.6489635109901428 = 0.02524394355714321 + 0.1 * 6.2371954917907715
Epoch 700, val loss: 0.9475031495094299
Epoch 710, training loss: 0.6475510597229004 = 0.024234589189291 + 0.1 * 6.233164310455322
Epoch 710, val loss: 0.9541677832603455
Epoch 720, training loss: 0.6472045183181763 = 0.023244986310601234 + 0.1 * 6.239595413208008
Epoch 720, val loss: 0.9607298970222473
Epoch 730, training loss: 0.6453056335449219 = 0.02226226218044758 + 0.1 * 6.230433464050293
Epoch 730, val loss: 0.967125654220581
Epoch 740, training loss: 0.6452394723892212 = 0.02127346582710743 + 0.1 * 6.239659786224365
Epoch 740, val loss: 0.9732401371002197
Epoch 750, training loss: 0.6430583596229553 = 0.020235924050211906 + 0.1 * 6.228224754333496
Epoch 750, val loss: 0.9792565107345581
Epoch 760, training loss: 0.640900731086731 = 0.019167574122548103 + 0.1 * 6.217331409454346
Epoch 760, val loss: 0.9850959777832031
Epoch 770, training loss: 0.640481173992157 = 0.018109504133462906 + 0.1 * 6.223716735839844
Epoch 770, val loss: 0.9907879829406738
Epoch 780, training loss: 0.6386125683784485 = 0.017025556415319443 + 0.1 * 6.215870380401611
Epoch 780, val loss: 0.9965574145317078
Epoch 790, training loss: 0.6376312375068665 = 0.015857264399528503 + 0.1 * 6.217739582061768
Epoch 790, val loss: 1.0023043155670166
Epoch 800, training loss: 0.6364976763725281 = 0.0146422004327178 + 0.1 * 6.218554973602295
Epoch 800, val loss: 1.0079493522644043
Epoch 810, training loss: 0.634107232093811 = 0.013500578701496124 + 0.1 * 6.206066608428955
Epoch 810, val loss: 1.0136085748672485
Epoch 820, training loss: 0.6327799558639526 = 0.012602969072759151 + 0.1 * 6.2017693519592285
Epoch 820, val loss: 1.0197467803955078
Epoch 830, training loss: 0.633205771446228 = 0.011899332515895367 + 0.1 * 6.213064193725586
Epoch 830, val loss: 1.0263090133666992
Epoch 840, training loss: 0.631773054599762 = 0.011330450884997845 + 0.1 * 6.204426288604736
Epoch 840, val loss: 1.0329461097717285
Epoch 850, training loss: 0.6316768527030945 = 0.01084519736468792 + 0.1 * 6.208316326141357
Epoch 850, val loss: 1.0394419431686401
Epoch 860, training loss: 0.6297964453697205 = 0.010414435528218746 + 0.1 * 6.193819999694824
Epoch 860, val loss: 1.0454577207565308
Epoch 870, training loss: 0.6295788288116455 = 0.010026630014181137 + 0.1 * 6.195521831512451
Epoch 870, val loss: 1.051435947418213
Epoch 880, training loss: 0.6294403076171875 = 0.009669952094554901 + 0.1 * 6.1977033615112305
Epoch 880, val loss: 1.0572142601013184
Epoch 890, training loss: 0.6285459399223328 = 0.009340299293398857 + 0.1 * 6.192056655883789
Epoch 890, val loss: 1.062982439994812
Epoch 900, training loss: 0.6282452344894409 = 0.009033103473484516 + 0.1 * 6.192121505737305
Epoch 900, val loss: 1.068642258644104
Epoch 910, training loss: 0.6273634433746338 = 0.008745413273572922 + 0.1 * 6.186180114746094
Epoch 910, val loss: 1.074187994003296
Epoch 920, training loss: 0.6296697854995728 = 0.008475245907902718 + 0.1 * 6.211945056915283
Epoch 920, val loss: 1.0796749591827393
Epoch 930, training loss: 0.6271916627883911 = 0.008220815099775791 + 0.1 * 6.189708709716797
Epoch 930, val loss: 1.0849838256835938
Epoch 940, training loss: 0.6261707544326782 = 0.007982050999999046 + 0.1 * 6.181886672973633
Epoch 940, val loss: 1.0902905464172363
Epoch 950, training loss: 0.6267469525337219 = 0.0077547067776322365 + 0.1 * 6.189922332763672
Epoch 950, val loss: 1.0953755378723145
Epoch 960, training loss: 0.6252772212028503 = 0.007539729122072458 + 0.1 * 6.177374362945557
Epoch 960, val loss: 1.1003601551055908
Epoch 970, training loss: 0.6248533129692078 = 0.007337089627981186 + 0.1 * 6.175161838531494
Epoch 970, val loss: 1.1053345203399658
Epoch 980, training loss: 0.6253248453140259 = 0.007143120281398296 + 0.1 * 6.181817054748535
Epoch 980, val loss: 1.110121250152588
Epoch 990, training loss: 0.6242659687995911 = 0.00695898849517107 + 0.1 * 6.173069953918457
Epoch 990, val loss: 1.1148955821990967
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7786
Flip ASR: 0.7422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.804360866546631 = 1.966981053352356 + 0.1 * 8.373799324035645
Epoch 0, val loss: 1.9685288667678833
Epoch 10, training loss: 2.7933883666992188 = 1.9560296535491943 + 0.1 * 8.373586654663086
Epoch 10, val loss: 1.9576597213745117
Epoch 20, training loss: 2.780233383178711 = 1.9429969787597656 + 0.1 * 8.37236499786377
Epoch 20, val loss: 1.9446123838424683
Epoch 30, training loss: 2.7615466117858887 = 1.9251561164855957 + 0.1 * 8.36390495300293
Epoch 30, val loss: 1.9269542694091797
Epoch 40, training loss: 2.7292301654815674 = 1.8989742994308472 + 0.1 * 8.302558898925781
Epoch 40, val loss: 1.9015727043151855
Epoch 50, training loss: 2.6484274864196777 = 1.8636019229888916 + 0.1 * 7.848254680633545
Epoch 50, val loss: 1.8684427738189697
Epoch 60, training loss: 2.5576019287109375 = 1.8266538381576538 + 0.1 * 7.309479713439941
Epoch 60, val loss: 1.835394024848938
Epoch 70, training loss: 2.48063325881958 = 1.789287805557251 + 0.1 * 6.913454055786133
Epoch 70, val loss: 1.8032606840133667
Epoch 80, training loss: 2.4297702312469482 = 1.752912163734436 + 0.1 * 6.768581390380859
Epoch 80, val loss: 1.7726372480392456
Epoch 90, training loss: 2.382150888442993 = 1.7119512557983398 + 0.1 * 6.701996803283691
Epoch 90, val loss: 1.7352385520935059
Epoch 100, training loss: 2.323199987411499 = 1.6578582525253296 + 0.1 * 6.653417110443115
Epoch 100, val loss: 1.6860557794570923
Epoch 110, training loss: 2.2496538162231445 = 1.588004231452942 + 0.1 * 6.616495132446289
Epoch 110, val loss: 1.6263266801834106
Epoch 120, training loss: 2.1609299182891846 = 1.5016188621520996 + 0.1 * 6.593110084533691
Epoch 120, val loss: 1.5536352396011353
Epoch 130, training loss: 2.064103364944458 = 1.406559705734253 + 0.1 * 6.575437068939209
Epoch 130, val loss: 1.474414587020874
Epoch 140, training loss: 1.9663074016571045 = 1.3103018999099731 + 0.1 * 6.560055732727051
Epoch 140, val loss: 1.3957982063293457
Epoch 150, training loss: 1.8712551593780518 = 1.2171576023101807 + 0.1 * 6.540975093841553
Epoch 150, val loss: 1.3225500583648682
Epoch 160, training loss: 1.7804903984069824 = 1.127922534942627 + 0.1 * 6.5256781578063965
Epoch 160, val loss: 1.2540483474731445
Epoch 170, training loss: 1.693521499633789 = 1.0430892705917358 + 0.1 * 6.504321575164795
Epoch 170, val loss: 1.1903785467147827
Epoch 180, training loss: 1.612778902053833 = 0.9633522033691406 + 0.1 * 6.494266510009766
Epoch 180, val loss: 1.1324485540390015
Epoch 190, training loss: 1.537773847579956 = 0.8904121518135071 + 0.1 * 6.473616600036621
Epoch 190, val loss: 1.0817011594772339
Epoch 200, training loss: 1.4694855213165283 = 0.823773205280304 + 0.1 * 6.457122325897217
Epoch 200, val loss: 1.0375158786773682
Epoch 210, training loss: 1.4076098203659058 = 0.7626679539680481 + 0.1 * 6.449418544769287
Epoch 210, val loss: 0.9990535974502563
Epoch 220, training loss: 1.3495151996612549 = 0.7059271335601807 + 0.1 * 6.435880184173584
Epoch 220, val loss: 0.9648314118385315
Epoch 230, training loss: 1.2950289249420166 = 0.6520790457725525 + 0.1 * 6.42949914932251
Epoch 230, val loss: 0.9332302808761597
Epoch 240, training loss: 1.2420904636383057 = 0.6006214618682861 + 0.1 * 6.414689540863037
Epoch 240, val loss: 0.9036739468574524
Epoch 250, training loss: 1.1923656463623047 = 0.5511446595191956 + 0.1 * 6.4122090339660645
Epoch 250, val loss: 0.8758850693702698
Epoch 260, training loss: 1.1439722776412964 = 0.5043161511421204 + 0.1 * 6.396561145782471
Epoch 260, val loss: 0.8507030606269836
Epoch 270, training loss: 1.099920630455017 = 0.4606897830963135 + 0.1 * 6.392308235168457
Epoch 270, val loss: 0.8290346264839172
Epoch 280, training loss: 1.0589896440505981 = 0.42059680819511414 + 0.1 * 6.383928298950195
Epoch 280, val loss: 0.8115195631980896
Epoch 290, training loss: 1.0216840505599976 = 0.3837524950504303 + 0.1 * 6.37931489944458
Epoch 290, val loss: 0.7979891896247864
Epoch 300, training loss: 0.9865660667419434 = 0.34990760684013367 + 0.1 * 6.366584300994873
Epoch 300, val loss: 0.7880663275718689
Epoch 310, training loss: 0.9545328617095947 = 0.31867796182632446 + 0.1 * 6.358549118041992
Epoch 310, val loss: 0.7810763716697693
Epoch 320, training loss: 0.9247463941574097 = 0.28967365622520447 + 0.1 * 6.350727558135986
Epoch 320, val loss: 0.7765437960624695
Epoch 330, training loss: 0.8973289728164673 = 0.26277270913124084 + 0.1 * 6.34556245803833
Epoch 330, val loss: 0.7741817831993103
Epoch 340, training loss: 0.8737907409667969 = 0.23813708126544952 + 0.1 * 6.356536865234375
Epoch 340, val loss: 0.7739441990852356
Epoch 350, training loss: 0.8487275838851929 = 0.21584449708461761 + 0.1 * 6.328830718994141
Epoch 350, val loss: 0.7756351232528687
Epoch 360, training loss: 0.8279677629470825 = 0.19554322957992554 + 0.1 * 6.324245452880859
Epoch 360, val loss: 0.7791120409965515
Epoch 370, training loss: 0.8089048266410828 = 0.17711897194385529 + 0.1 * 6.317858695983887
Epoch 370, val loss: 0.7844557762145996
Epoch 380, training loss: 0.7921916842460632 = 0.1605258584022522 + 0.1 * 6.316658020019531
Epoch 380, val loss: 0.7914315462112427
Epoch 390, training loss: 0.7769458889961243 = 0.14574050903320312 + 0.1 * 6.312053680419922
Epoch 390, val loss: 0.7997726202011108
Epoch 400, training loss: 0.763047456741333 = 0.13251881301403046 + 0.1 * 6.305285930633545
Epoch 400, val loss: 0.8093546628952026
Epoch 410, training loss: 0.7527980208396912 = 0.120696060359478 + 0.1 * 6.321019649505615
Epoch 410, val loss: 0.819961667060852
Epoch 420, training loss: 0.740236759185791 = 0.11017964780330658 + 0.1 * 6.300570964813232
Epoch 420, val loss: 0.8312604427337646
Epoch 430, training loss: 0.7314311265945435 = 0.10079484432935715 + 0.1 * 6.306362628936768
Epoch 430, val loss: 0.8429616093635559
Epoch 440, training loss: 0.7211090922355652 = 0.09243398904800415 + 0.1 * 6.286750793457031
Epoch 440, val loss: 0.8549737334251404
Epoch 450, training loss: 0.7136657238006592 = 0.08493396639823914 + 0.1 * 6.287317752838135
Epoch 450, val loss: 0.8670834302902222
Epoch 460, training loss: 0.7056097388267517 = 0.07820267230272293 + 0.1 * 6.274070739746094
Epoch 460, val loss: 0.8792536854743958
Epoch 470, training loss: 0.7007198929786682 = 0.07213368266820908 + 0.1 * 6.285861968994141
Epoch 470, val loss: 0.8913325667381287
Epoch 480, training loss: 0.6951993107795715 = 0.06667643040418625 + 0.1 * 6.285228729248047
Epoch 480, val loss: 0.9032086133956909
Epoch 490, training loss: 0.6880793571472168 = 0.0617646723985672 + 0.1 * 6.263146877288818
Epoch 490, val loss: 0.9148253202438354
Epoch 500, training loss: 0.6836058497428894 = 0.05731761455535889 + 0.1 * 6.262882232666016
Epoch 500, val loss: 0.926299512386322
Epoch 510, training loss: 0.6797765493392944 = 0.05327963829040527 + 0.1 * 6.2649688720703125
Epoch 510, val loss: 0.9375316500663757
Epoch 520, training loss: 0.6751791834831238 = 0.049625955522060394 + 0.1 * 6.255532264709473
Epoch 520, val loss: 0.9484438300132751
Epoch 530, training loss: 0.6711863875389099 = 0.04631275311112404 + 0.1 * 6.248736381530762
Epoch 530, val loss: 0.9591774344444275
Epoch 540, training loss: 0.6679918169975281 = 0.0432874895632267 + 0.1 * 6.247042655944824
Epoch 540, val loss: 0.9696998000144958
Epoch 550, training loss: 0.6649748682975769 = 0.040525197982788086 + 0.1 * 6.2444963455200195
Epoch 550, val loss: 0.9799628257751465
Epoch 560, training loss: 0.6620868444442749 = 0.03800341486930847 + 0.1 * 6.240833759307861
Epoch 560, val loss: 0.9900861978530884
Epoch 570, training loss: 0.659996747970581 = 0.03569433093070984 + 0.1 * 6.243023872375488
Epoch 570, val loss: 0.9999051690101624
Epoch 580, training loss: 0.6580531001091003 = 0.03358209878206253 + 0.1 * 6.2447099685668945
Epoch 580, val loss: 1.0095250606536865
Epoch 590, training loss: 0.6543295383453369 = 0.031644172966480255 + 0.1 * 6.226853847503662
Epoch 590, val loss: 1.0189001560211182
Epoch 600, training loss: 0.6532403230667114 = 0.0298590287566185 + 0.1 * 6.2338128089904785
Epoch 600, val loss: 1.0280826091766357
Epoch 610, training loss: 0.6523557901382446 = 0.028215760365128517 + 0.1 * 6.241400241851807
Epoch 610, val loss: 1.0369949340820312
Epoch 620, training loss: 0.6495419144630432 = 0.02670593559741974 + 0.1 * 6.228359699249268
Epoch 620, val loss: 1.0457810163497925
Epoch 630, training loss: 0.6472422480583191 = 0.02530984953045845 + 0.1 * 6.219323635101318
Epoch 630, val loss: 1.054364800453186
Epoch 640, training loss: 0.6466612815856934 = 0.024016162380576134 + 0.1 * 6.226451396942139
Epoch 640, val loss: 1.0627158880233765
Epoch 650, training loss: 0.6450549960136414 = 0.022821296006441116 + 0.1 * 6.222336769104004
Epoch 650, val loss: 1.0708266496658325
Epoch 660, training loss: 0.6427866220474243 = 0.02171742171049118 + 0.1 * 6.210691928863525
Epoch 660, val loss: 1.078844428062439
Epoch 670, training loss: 0.6417223215103149 = 0.020688747987151146 + 0.1 * 6.2103352546691895
Epoch 670, val loss: 1.0866522789001465
Epoch 680, training loss: 0.6408811807632446 = 0.0197304617613554 + 0.1 * 6.211507320404053
Epoch 680, val loss: 1.0941975116729736
Epoch 690, training loss: 0.6389833092689514 = 0.018840419128537178 + 0.1 * 6.2014288902282715
Epoch 690, val loss: 1.1016956567764282
Epoch 700, training loss: 0.6395169496536255 = 0.01800917647778988 + 0.1 * 6.2150774002075195
Epoch 700, val loss: 1.109006404876709
Epoch 710, training loss: 0.6376026272773743 = 0.017233241349458694 + 0.1 * 6.203693866729736
Epoch 710, val loss: 1.116025686264038
Epoch 720, training loss: 0.6364253759384155 = 0.016509901732206345 + 0.1 * 6.199154853820801
Epoch 720, val loss: 1.1230653524398804
Epoch 730, training loss: 0.6361768245697021 = 0.015830760821700096 + 0.1 * 6.203460693359375
Epoch 730, val loss: 1.1298600435256958
Epoch 740, training loss: 0.6347272396087646 = 0.015193384140729904 + 0.1 * 6.195338249206543
Epoch 740, val loss: 1.1364006996154785
Epoch 750, training loss: 0.6340282559394836 = 0.014598321169614792 + 0.1 * 6.194299221038818
Epoch 750, val loss: 1.1429752111434937
Epoch 760, training loss: 0.6332916617393494 = 0.014037509448826313 + 0.1 * 6.192541599273682
Epoch 760, val loss: 1.1493650674819946
Epoch 770, training loss: 0.6327589750289917 = 0.013508979231119156 + 0.1 * 6.192500114440918
Epoch 770, val loss: 1.155587077140808
Epoch 780, training loss: 0.6333463788032532 = 0.013011577539145947 + 0.1 * 6.203348159790039
Epoch 780, val loss: 1.1617186069488525
Epoch 790, training loss: 0.6310375928878784 = 0.01254298072308302 + 0.1 * 6.184946060180664
Epoch 790, val loss: 1.167658805847168
Epoch 800, training loss: 0.630327582359314 = 0.01210106536746025 + 0.1 * 6.182265281677246
Epoch 800, val loss: 1.173579216003418
Epoch 810, training loss: 0.6301296353340149 = 0.011682400479912758 + 0.1 * 6.18447208404541
Epoch 810, val loss: 1.1792782545089722
Epoch 820, training loss: 0.6296384334564209 = 0.011286463588476181 + 0.1 * 6.1835198402404785
Epoch 820, val loss: 1.1848217248916626
Epoch 830, training loss: 0.6284776329994202 = 0.010912822559475899 + 0.1 * 6.175647735595703
Epoch 830, val loss: 1.1902960538864136
Epoch 840, training loss: 0.6285697221755981 = 0.010558109730482101 + 0.1 * 6.180115699768066
Epoch 840, val loss: 1.195617437362671
Epoch 850, training loss: 0.6288321018218994 = 0.010221893899142742 + 0.1 * 6.186101913452148
Epoch 850, val loss: 1.2008618116378784
Epoch 860, training loss: 0.6273968815803528 = 0.009903430938720703 + 0.1 * 6.174934387207031
Epoch 860, val loss: 1.2059544324874878
Epoch 870, training loss: 0.6270326375961304 = 0.009600683115422726 + 0.1 * 6.174319267272949
Epoch 870, val loss: 1.2110402584075928
Epoch 880, training loss: 0.6266769170761108 = 0.009312341921031475 + 0.1 * 6.173645973205566
Epoch 880, val loss: 1.2158974409103394
Epoch 890, training loss: 0.6260865330696106 = 0.009038640186190605 + 0.1 * 6.170478820800781
Epoch 890, val loss: 1.2207838296890259
Epoch 900, training loss: 0.6268219351768494 = 0.00877788569778204 + 0.1 * 6.180440425872803
Epoch 900, val loss: 1.2255005836486816
Epoch 910, training loss: 0.6251994967460632 = 0.008529209531843662 + 0.1 * 6.166702747344971
Epoch 910, val loss: 1.2301665544509888
Epoch 920, training loss: 0.6245620846748352 = 0.008292345330119133 + 0.1 * 6.1626973152160645
Epoch 920, val loss: 1.2347912788391113
Epoch 930, training loss: 0.6245436072349548 = 0.008065355941653252 + 0.1 * 6.164782524108887
Epoch 930, val loss: 1.2392573356628418
Epoch 940, training loss: 0.6244032979011536 = 0.007848633453249931 + 0.1 * 6.165546417236328
Epoch 940, val loss: 1.2436681985855103
Epoch 950, training loss: 0.6231573224067688 = 0.007641479838639498 + 0.1 * 6.155158042907715
Epoch 950, val loss: 1.248033046722412
Epoch 960, training loss: 0.6239745616912842 = 0.0074430122040212154 + 0.1 * 6.1653151512146
Epoch 960, val loss: 1.252256155014038
Epoch 970, training loss: 0.6234560012817383 = 0.007252693176269531 + 0.1 * 6.1620330810546875
Epoch 970, val loss: 1.256374478340149
Epoch 980, training loss: 0.6227242350578308 = 0.007071235682815313 + 0.1 * 6.156529426574707
Epoch 980, val loss: 1.2605160474777222
Epoch 990, training loss: 0.6224484443664551 = 0.006896819919347763 + 0.1 * 6.155516624450684
Epoch 990, val loss: 1.2644003629684448
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7601
Flip ASR: 0.7156/225 nodes
The final ASR:0.67405, 0.13502, Accuracy:0.80864, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11670])
remove edge: torch.Size([2, 9460])
updated graph: torch.Size([2, 10574])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.98278, 0.00758, Accuracy:0.83951, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7875425815582275 = 1.9501774311065674 + 0.1 * 8.373650550842285
Epoch 0, val loss: 1.9409220218658447
Epoch 10, training loss: 2.777362108230591 = 1.9400460720062256 + 0.1 * 8.373159408569336
Epoch 10, val loss: 1.9313596487045288
Epoch 20, training loss: 2.7643380165100098 = 1.9273096323013306 + 0.1 * 8.370283126831055
Epoch 20, val loss: 1.9189414978027344
Epoch 30, training loss: 2.7442216873168945 = 1.9092499017715454 + 0.1 * 8.34971809387207
Epoch 30, val loss: 1.9011818170547485
Epoch 40, training loss: 2.6954357624053955 = 1.8829032182693481 + 0.1 * 8.125325202941895
Epoch 40, val loss: 1.87587571144104
Epoch 50, training loss: 2.5819954872131348 = 1.8544644117355347 + 0.1 * 7.2753095626831055
Epoch 50, val loss: 1.8502650260925293
Epoch 60, training loss: 2.5121750831604004 = 1.8262239694595337 + 0.1 * 6.8595099449157715
Epoch 60, val loss: 1.8249915838241577
Epoch 70, training loss: 2.4623117446899414 = 1.793472409248352 + 0.1 * 6.688394069671631
Epoch 70, val loss: 1.7970383167266846
Epoch 80, training loss: 2.417442798614502 = 1.757230520248413 + 0.1 * 6.602123737335205
Epoch 80, val loss: 1.768283724784851
Epoch 90, training loss: 2.3729050159454346 = 1.7183630466461182 + 0.1 * 6.545420169830322
Epoch 90, val loss: 1.7381235361099243
Epoch 100, training loss: 2.318833827972412 = 1.667447566986084 + 0.1 * 6.513861656188965
Epoch 100, val loss: 1.6964746713638306
Epoch 110, training loss: 2.2484233379364014 = 1.5988819599151611 + 0.1 * 6.495413303375244
Epoch 110, val loss: 1.6400190591812134
Epoch 120, training loss: 2.159294366836548 = 1.5112106800079346 + 0.1 * 6.480836868286133
Epoch 120, val loss: 1.569333791732788
Epoch 130, training loss: 2.056196689605713 = 1.409424901008606 + 0.1 * 6.467716693878174
Epoch 130, val loss: 1.48915696144104
Epoch 140, training loss: 1.947629690170288 = 1.302382230758667 + 0.1 * 6.452474117279053
Epoch 140, val loss: 1.4052402973175049
Epoch 150, training loss: 1.8416457176208496 = 1.1979670524597168 + 0.1 * 6.436787128448486
Epoch 150, val loss: 1.32407546043396
Epoch 160, training loss: 1.7436614036560059 = 1.1014559268951416 + 0.1 * 6.422053813934326
Epoch 160, val loss: 1.2497358322143555
Epoch 170, training loss: 1.6535375118255615 = 1.0125248432159424 + 0.1 * 6.41012716293335
Epoch 170, val loss: 1.1815606355667114
Epoch 180, training loss: 1.5712971687316895 = 0.9317420125007629 + 0.1 * 6.395552158355713
Epoch 180, val loss: 1.1210991144180298
Epoch 190, training loss: 1.4947788715362549 = 0.8559197187423706 + 0.1 * 6.38859224319458
Epoch 190, val loss: 1.065796971321106
Epoch 200, training loss: 1.4222171306610107 = 0.7851006388664246 + 0.1 * 6.371164798736572
Epoch 200, val loss: 1.0157431364059448
Epoch 210, training loss: 1.3533084392547607 = 0.717496931552887 + 0.1 * 6.3581156730651855
Epoch 210, val loss: 0.9686177372932434
Epoch 220, training loss: 1.2885925769805908 = 0.6528640389442444 + 0.1 * 6.357285499572754
Epoch 220, val loss: 0.9243589639663696
Epoch 230, training loss: 1.2276065349578857 = 0.5934829115867615 + 0.1 * 6.341236114501953
Epoch 230, val loss: 0.8850061297416687
Epoch 240, training loss: 1.1722626686096191 = 0.5395127534866333 + 0.1 * 6.327498912811279
Epoch 240, val loss: 0.8508422374725342
Epoch 250, training loss: 1.1246286630630493 = 0.4910820424556732 + 0.1 * 6.335466384887695
Epoch 250, val loss: 0.822342574596405
Epoch 260, training loss: 1.0794850587844849 = 0.4483432471752167 + 0.1 * 6.311418056488037
Epoch 260, val loss: 0.7998231649398804
Epoch 270, training loss: 1.0401760339736938 = 0.4097335636615753 + 0.1 * 6.30442476272583
Epoch 270, val loss: 0.782014787197113
Epoch 280, training loss: 1.0055266618728638 = 0.3744034767150879 + 0.1 * 6.31123161315918
Epoch 280, val loss: 0.7683376669883728
Epoch 290, training loss: 0.9715428352355957 = 0.34199172258377075 + 0.1 * 6.295510768890381
Epoch 290, val loss: 0.7586584687232971
Epoch 300, training loss: 0.9405239820480347 = 0.3120538592338562 + 0.1 * 6.284701347351074
Epoch 300, val loss: 0.7521297335624695
Epoch 310, training loss: 0.9131038188934326 = 0.2846699655056 + 0.1 * 6.284337997436523
Epoch 310, val loss: 0.7486585378646851
Epoch 320, training loss: 0.8887490034103394 = 0.2598787546157837 + 0.1 * 6.288702487945557
Epoch 320, val loss: 0.7480872869491577
Epoch 330, training loss: 0.8650304675102234 = 0.23739106953144073 + 0.1 * 6.276393890380859
Epoch 330, val loss: 0.7495356202125549
Epoch 340, training loss: 0.8435150384902954 = 0.21675722301006317 + 0.1 * 6.267578125
Epoch 340, val loss: 0.7528234720230103
Epoch 350, training loss: 0.8248851299285889 = 0.19768454134464264 + 0.1 * 6.272006034851074
Epoch 350, val loss: 0.757388710975647
Epoch 360, training loss: 0.806325376033783 = 0.18006183207035065 + 0.1 * 6.262635231018066
Epoch 360, val loss: 0.763359785079956
Epoch 370, training loss: 0.7893640995025635 = 0.16373306512832642 + 0.1 * 6.256309986114502
Epoch 370, val loss: 0.7703667283058167
Epoch 380, training loss: 0.7737820744514465 = 0.1487697809934616 + 0.1 * 6.250122547149658
Epoch 380, val loss: 0.7784363627433777
Epoch 390, training loss: 0.7621515989303589 = 0.13522647321224213 + 0.1 * 6.269250869750977
Epoch 390, val loss: 0.787731945514679
Epoch 400, training loss: 0.7475327253341675 = 0.12313209474086761 + 0.1 * 6.244006156921387
Epoch 400, val loss: 0.7979324460029602
Epoch 410, training loss: 0.7358803153038025 = 0.1123616173863411 + 0.1 * 6.23518705368042
Epoch 410, val loss: 0.8091585636138916
Epoch 420, training loss: 0.7263471484184265 = 0.10279399901628494 + 0.1 * 6.235531330108643
Epoch 420, val loss: 0.8212128281593323
Epoch 430, training loss: 0.7185204029083252 = 0.09433271735906601 + 0.1 * 6.24187707901001
Epoch 430, val loss: 0.8338236212730408
Epoch 440, training loss: 0.7095375657081604 = 0.08686871826648712 + 0.1 * 6.226688385009766
Epoch 440, val loss: 0.8471150994300842
Epoch 450, training loss: 0.7030680775642395 = 0.08022365719079971 + 0.1 * 6.2284440994262695
Epoch 450, val loss: 0.8607965707778931
Epoch 460, training loss: 0.6960501670837402 = 0.07429873943328857 + 0.1 * 6.2175140380859375
Epoch 460, val loss: 0.8746464252471924
Epoch 470, training loss: 0.6904328465461731 = 0.06898874044418335 + 0.1 * 6.214440822601318
Epoch 470, val loss: 0.8888251185417175
Epoch 480, training loss: 0.6862819790840149 = 0.06420783698558807 + 0.1 * 6.220741271972656
Epoch 480, val loss: 0.9027865529060364
Epoch 490, training loss: 0.6810503005981445 = 0.059898193925619125 + 0.1 * 6.211521148681641
Epoch 490, val loss: 0.9170741438865662
Epoch 500, training loss: 0.6766862869262695 = 0.05598617345094681 + 0.1 * 6.207001209259033
Epoch 500, val loss: 0.9311434626579285
Epoch 510, training loss: 0.6732667088508606 = 0.052426502108573914 + 0.1 * 6.208401679992676
Epoch 510, val loss: 0.9450323581695557
Epoch 520, training loss: 0.6692361235618591 = 0.04919012635946274 + 0.1 * 6.200459957122803
Epoch 520, val loss: 0.9590021371841431
Epoch 530, training loss: 0.6659003496170044 = 0.04622562229633331 + 0.1 * 6.196747303009033
Epoch 530, val loss: 0.9726075530052185
Epoch 540, training loss: 0.6632959842681885 = 0.04350445792078972 + 0.1 * 6.197915077209473
Epoch 540, val loss: 0.9861156344413757
Epoch 550, training loss: 0.660167396068573 = 0.04100511968135834 + 0.1 * 6.191622257232666
Epoch 550, val loss: 0.9995147585868835
Epoch 560, training loss: 0.6577835083007812 = 0.03870462626218796 + 0.1 * 6.190788745880127
Epoch 560, val loss: 1.012577772140503
Epoch 570, training loss: 0.6548979878425598 = 0.036586713045835495 + 0.1 * 6.183112621307373
Epoch 570, val loss: 1.0256304740905762
Epoch 580, training loss: 0.654301643371582 = 0.034629326313734055 + 0.1 * 6.196722984313965
Epoch 580, val loss: 1.0384511947631836
Epoch 590, training loss: 0.6515207290649414 = 0.032821107655763626 + 0.1 * 6.186995983123779
Epoch 590, val loss: 1.0508370399475098
Epoch 600, training loss: 0.6492244005203247 = 0.03114849515259266 + 0.1 * 6.180758953094482
Epoch 600, val loss: 1.0633184909820557
Epoch 610, training loss: 0.647581934928894 = 0.02959253080189228 + 0.1 * 6.179893970489502
Epoch 610, val loss: 1.0753233432769775
Epoch 620, training loss: 0.6472824215888977 = 0.028147375211119652 + 0.1 * 6.191349983215332
Epoch 620, val loss: 1.087051510810852
Epoch 630, training loss: 0.6440775394439697 = 0.026808729395270348 + 0.1 * 6.172688007354736
Epoch 630, val loss: 1.0987269878387451
Epoch 640, training loss: 0.6424555778503418 = 0.025561993941664696 + 0.1 * 6.168935775756836
Epoch 640, val loss: 1.110145926475525
Epoch 650, training loss: 0.641566812992096 = 0.02439686469733715 + 0.1 * 6.171699523925781
Epoch 650, val loss: 1.1211987733840942
Epoch 660, training loss: 0.6397880911827087 = 0.023311417549848557 + 0.1 * 6.164766311645508
Epoch 660, val loss: 1.1322097778320312
Epoch 670, training loss: 0.6388442516326904 = 0.022294549271464348 + 0.1 * 6.165497303009033
Epoch 670, val loss: 1.1429280042648315
Epoch 680, training loss: 0.6390727162361145 = 0.021341705694794655 + 0.1 * 6.177309989929199
Epoch 680, val loss: 1.1532899141311646
Epoch 690, training loss: 0.6365063786506653 = 0.020450206473469734 + 0.1 * 6.1605610847473145
Epoch 690, val loss: 1.1637200117111206
Epoch 700, training loss: 0.6355044841766357 = 0.019612841308116913 + 0.1 * 6.158915996551514
Epoch 700, val loss: 1.1738466024398804
Epoch 710, training loss: 0.6347978711128235 = 0.01882469654083252 + 0.1 * 6.159731388092041
Epoch 710, val loss: 1.1836237907409668
Epoch 720, training loss: 0.6348548531532288 = 0.01808418147265911 + 0.1 * 6.167706489562988
Epoch 720, val loss: 1.193297266960144
Epoch 730, training loss: 0.6328811645507812 = 0.017387934029102325 + 0.1 * 6.154932022094727
Epoch 730, val loss: 1.202868103981018
Epoch 740, training loss: 0.6319549083709717 = 0.01673228293657303 + 0.1 * 6.152226448059082
Epoch 740, val loss: 1.2121925354003906
Epoch 750, training loss: 0.6319288015365601 = 0.016112228855490685 + 0.1 * 6.15816593170166
Epoch 750, val loss: 1.2211499214172363
Epoch 760, training loss: 0.6304671764373779 = 0.015527958981692791 + 0.1 * 6.149392127990723
Epoch 760, val loss: 1.230141282081604
Epoch 770, training loss: 0.6298058032989502 = 0.014975135214626789 + 0.1 * 6.148306846618652
Epoch 770, val loss: 1.2389177083969116
Epoch 780, training loss: 0.6289003491401672 = 0.0144511628895998 + 0.1 * 6.144492149353027
Epoch 780, val loss: 1.2474620342254639
Epoch 790, training loss: 0.6288127899169922 = 0.013955189846456051 + 0.1 * 6.148576259613037
Epoch 790, val loss: 1.2558033466339111
Epoch 800, training loss: 0.6277094483375549 = 0.013485892675817013 + 0.1 * 6.142235279083252
Epoch 800, val loss: 1.264206051826477
Epoch 810, training loss: 0.6271248459815979 = 0.013040417805314064 + 0.1 * 6.140843868255615
Epoch 810, val loss: 1.2724010944366455
Epoch 820, training loss: 0.6269952654838562 = 0.012616894207894802 + 0.1 * 6.1437835693359375
Epoch 820, val loss: 1.2802259922027588
Epoch 830, training loss: 0.6261782646179199 = 0.01221492886543274 + 0.1 * 6.1396331787109375
Epoch 830, val loss: 1.2881447076797485
Epoch 840, training loss: 0.6261523365974426 = 0.011832986027002335 + 0.1 * 6.143193244934082
Epoch 840, val loss: 1.2956205606460571
Epoch 850, training loss: 0.6250019669532776 = 0.011470247060060501 + 0.1 * 6.135316848754883
Epoch 850, val loss: 1.303254246711731
Epoch 860, training loss: 0.6241719722747803 = 0.01112497877329588 + 0.1 * 6.130469799041748
Epoch 860, val loss: 1.3108841180801392
Epoch 870, training loss: 0.6255152225494385 = 0.010794437490403652 + 0.1 * 6.147207736968994
Epoch 870, val loss: 1.3179845809936523
Epoch 880, training loss: 0.6236715912818909 = 0.010479331016540527 + 0.1 * 6.131922721862793
Epoch 880, val loss: 1.3249677419662476
Epoch 890, training loss: 0.6231806874275208 = 0.010179150849580765 + 0.1 * 6.1300153732299805
Epoch 890, val loss: 1.3322454690933228
Epoch 900, training loss: 0.6230440735816956 = 0.009891992434859276 + 0.1 * 6.131520748138428
Epoch 900, val loss: 1.3390628099441528
Epoch 910, training loss: 0.6226519346237183 = 0.009617281146347523 + 0.1 * 6.130346775054932
Epoch 910, val loss: 1.3457316160202026
Epoch 920, training loss: 0.6226300597190857 = 0.009354805573821068 + 0.1 * 6.132751941680908
Epoch 920, val loss: 1.3523885011672974
Epoch 930, training loss: 0.6213632822036743 = 0.009103445336222649 + 0.1 * 6.122598648071289
Epoch 930, val loss: 1.3588255643844604
Epoch 940, training loss: 0.6211313605308533 = 0.008863694034516811 + 0.1 * 6.122676849365234
Epoch 940, val loss: 1.3653161525726318
Epoch 950, training loss: 0.620559811592102 = 0.008633540943264961 + 0.1 * 6.119262218475342
Epoch 950, val loss: 1.371622085571289
Epoch 960, training loss: 0.6209419965744019 = 0.008412682451307774 + 0.1 * 6.125293254852295
Epoch 960, val loss: 1.3778239488601685
Epoch 970, training loss: 0.619905412197113 = 0.008200458250939846 + 0.1 * 6.117049217224121
Epoch 970, val loss: 1.3838740587234497
Epoch 980, training loss: 0.6201828718185425 = 0.007997028529644012 + 0.1 * 6.1218581199646
Epoch 980, val loss: 1.3899152278900146
Epoch 990, training loss: 0.6199986934661865 = 0.007801544852554798 + 0.1 * 6.121971607208252
Epoch 990, val loss: 1.3956594467163086
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8007
Flip ASR: 0.7600/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7862462997436523 = 1.9488695859909058 + 0.1 * 8.373766899108887
Epoch 0, val loss: 1.9440650939941406
Epoch 10, training loss: 2.7760353088378906 = 1.938687801361084 + 0.1 * 8.373475074768066
Epoch 10, val loss: 1.9328101873397827
Epoch 20, training loss: 2.7633304595947266 = 1.9261329174041748 + 0.1 * 8.371975898742676
Epoch 20, val loss: 1.9187034368515015
Epoch 30, training loss: 2.745023250579834 = 1.9086648225784302 + 0.1 * 8.363585472106934
Epoch 30, val loss: 1.898930311203003
Epoch 40, training loss: 2.713263988494873 = 1.8831552267074585 + 0.1 * 8.301087379455566
Epoch 40, val loss: 1.8705456256866455
Epoch 50, training loss: 2.6215310096740723 = 1.8489867448806763 + 0.1 * 7.725441932678223
Epoch 50, val loss: 1.8343712091445923
Epoch 60, training loss: 2.5333828926086426 = 1.814551830291748 + 0.1 * 7.188309192657471
Epoch 60, val loss: 1.7998932600021362
Epoch 70, training loss: 2.4700067043304443 = 1.779752492904663 + 0.1 * 6.9025421142578125
Epoch 70, val loss: 1.7663650512695312
Epoch 80, training loss: 2.4199647903442383 = 1.7440838813781738 + 0.1 * 6.758808135986328
Epoch 80, val loss: 1.7351899147033691
Epoch 90, training loss: 2.3707492351531982 = 1.704224705696106 + 0.1 * 6.665245056152344
Epoch 90, val loss: 1.7011665105819702
Epoch 100, training loss: 2.3119616508483887 = 1.6498994827270508 + 0.1 * 6.620620250701904
Epoch 100, val loss: 1.6549746990203857
Epoch 110, training loss: 2.237400531768799 = 1.5783323049545288 + 0.1 * 6.5906829833984375
Epoch 110, val loss: 1.595793604850769
Epoch 120, training loss: 2.147616147994995 = 1.4912686347961426 + 0.1 * 6.563474178314209
Epoch 120, val loss: 1.525724172592163
Epoch 130, training loss: 2.0502805709838867 = 1.3963109254837036 + 0.1 * 6.5396952629089355
Epoch 130, val loss: 1.4495402574539185
Epoch 140, training loss: 1.9509317874908447 = 1.2992078065872192 + 0.1 * 6.517240047454834
Epoch 140, val loss: 1.37363600730896
Epoch 150, training loss: 1.852064847946167 = 1.202541470527649 + 0.1 * 6.495234489440918
Epoch 150, val loss: 1.298838496208191
Epoch 160, training loss: 1.7523659467697144 = 1.1044210195541382 + 0.1 * 6.479449272155762
Epoch 160, val loss: 1.223814845085144
Epoch 170, training loss: 1.6533865928649902 = 1.007164716720581 + 0.1 * 6.462218284606934
Epoch 170, val loss: 1.1512612104415894
Epoch 180, training loss: 1.5568912029266357 = 0.912663459777832 + 0.1 * 6.442277908325195
Epoch 180, val loss: 1.0823482275009155
Epoch 190, training loss: 1.4675638675689697 = 0.8244752287864685 + 0.1 * 6.430886745452881
Epoch 190, val loss: 1.0191339254379272
Epoch 200, training loss: 1.3890736103057861 = 0.7469361424446106 + 0.1 * 6.421374797821045
Epoch 200, val loss: 0.9658320546150208
Epoch 210, training loss: 1.3216540813446045 = 0.680517315864563 + 0.1 * 6.411367416381836
Epoch 210, val loss: 0.9224886894226074
Epoch 220, training loss: 1.2652020454406738 = 0.6241421699523926 + 0.1 * 6.410598278045654
Epoch 220, val loss: 0.8880735635757446
Epoch 230, training loss: 1.215529441833496 = 0.5759375691413879 + 0.1 * 6.395919322967529
Epoch 230, val loss: 0.8613269329071045
Epoch 240, training loss: 1.172178030014038 = 0.5336658954620361 + 0.1 * 6.385120868682861
Epoch 240, val loss: 0.8405244946479797
Epoch 250, training loss: 1.1335338354110718 = 0.4959372580051422 + 0.1 * 6.375965595245361
Epoch 250, val loss: 0.8242887258529663
Epoch 260, training loss: 1.098442554473877 = 0.4617811143398285 + 0.1 * 6.366614818572998
Epoch 260, val loss: 0.8115305304527283
Epoch 270, training loss: 1.0675932168960571 = 0.43042245507240295 + 0.1 * 6.371706962585449
Epoch 270, val loss: 0.8014644980430603
Epoch 280, training loss: 1.0365279912948608 = 0.40138956904411316 + 0.1 * 6.35138463973999
Epoch 280, val loss: 0.7936331033706665
Epoch 290, training loss: 1.0086203813552856 = 0.37411853671073914 + 0.1 * 6.345017910003662
Epoch 290, val loss: 0.7873460650444031
Epoch 300, training loss: 0.9826793074607849 = 0.348365843296051 + 0.1 * 6.34313440322876
Epoch 300, val loss: 0.7824472188949585
Epoch 310, training loss: 0.9572640657424927 = 0.3238639533519745 + 0.1 * 6.334000587463379
Epoch 310, val loss: 0.7786758542060852
Epoch 320, training loss: 0.932968020439148 = 0.30049172043800354 + 0.1 * 6.32476282119751
Epoch 320, val loss: 0.7760844826698303
Epoch 330, training loss: 0.9108586311340332 = 0.27823853492736816 + 0.1 * 6.32620096206665
Epoch 330, val loss: 0.7748249173164368
Epoch 340, training loss: 0.8888024091720581 = 0.2572905719280243 + 0.1 * 6.315118789672852
Epoch 340, val loss: 0.7751334309577942
Epoch 350, training loss: 0.8693959712982178 = 0.23758773505687714 + 0.1 * 6.318082332611084
Epoch 350, val loss: 0.7770925164222717
Epoch 360, training loss: 0.8502695560455322 = 0.21917647123336792 + 0.1 * 6.3109307289123535
Epoch 360, val loss: 0.7806601524353027
Epoch 370, training loss: 0.8320141434669495 = 0.20205675065517426 + 0.1 * 6.29957389831543
Epoch 370, val loss: 0.7858279943466187
Epoch 380, training loss: 0.816322386264801 = 0.18620331585407257 + 0.1 * 6.3011908531188965
Epoch 380, val loss: 0.7922532558441162
Epoch 390, training loss: 0.8004634976387024 = 0.1714438647031784 + 0.1 * 6.290195941925049
Epoch 390, val loss: 0.8001458048820496
Epoch 400, training loss: 0.786941647529602 = 0.15773609280586243 + 0.1 * 6.292055606842041
Epoch 400, val loss: 0.8089749813079834
Epoch 410, training loss: 0.7728905081748962 = 0.14512842893600464 + 0.1 * 6.277620792388916
Epoch 410, val loss: 0.8189749121665955
Epoch 420, training loss: 0.7617917060852051 = 0.1333986222743988 + 0.1 * 6.28393030166626
Epoch 420, val loss: 0.8297732472419739
Epoch 430, training loss: 0.7504288554191589 = 0.12237001210451126 + 0.1 * 6.280588150024414
Epoch 430, val loss: 0.8412755131721497
Epoch 440, training loss: 0.7388221025466919 = 0.1119014322757721 + 0.1 * 6.269207000732422
Epoch 440, val loss: 0.8535825610160828
Epoch 450, training loss: 0.7287576794624329 = 0.1021968349814415 + 0.1 * 6.265608787536621
Epoch 450, val loss: 0.8664590120315552
Epoch 460, training loss: 0.7200918197631836 = 0.0934973880648613 + 0.1 * 6.265944004058838
Epoch 460, val loss: 0.8800716996192932
Epoch 470, training loss: 0.711862325668335 = 0.08576922863721848 + 0.1 * 6.260931015014648
Epoch 470, val loss: 0.8946616649627686
Epoch 480, training loss: 0.7044134140014648 = 0.07891247421503067 + 0.1 * 6.255009174346924
Epoch 480, val loss: 0.9099189043045044
Epoch 490, training loss: 0.6990488767623901 = 0.07281208038330078 + 0.1 * 6.2623677253723145
Epoch 490, val loss: 0.9255993962287903
Epoch 500, training loss: 0.6922281980514526 = 0.0673389658331871 + 0.1 * 6.248892307281494
Epoch 500, val loss: 0.9415380358695984
Epoch 510, training loss: 0.6877028346061707 = 0.06242158263921738 + 0.1 * 6.252812385559082
Epoch 510, val loss: 0.9576543569564819
Epoch 520, training loss: 0.6825774312019348 = 0.05800013616681099 + 0.1 * 6.245772838592529
Epoch 520, val loss: 0.9737886786460876
Epoch 530, training loss: 0.6770797967910767 = 0.05400532856583595 + 0.1 * 6.230744361877441
Epoch 530, val loss: 0.9899009466171265
Epoch 540, training loss: 0.6745331883430481 = 0.050372324883937836 + 0.1 * 6.241608619689941
Epoch 540, val loss: 1.005958080291748
Epoch 550, training loss: 0.6700483560562134 = 0.04706757143139839 + 0.1 * 6.2298078536987305
Epoch 550, val loss: 1.021958351135254
Epoch 560, training loss: 0.6685804128646851 = 0.044057849794626236 + 0.1 * 6.245225429534912
Epoch 560, val loss: 1.0378257036209106
Epoch 570, training loss: 0.6641914248466492 = 0.041324954479932785 + 0.1 * 6.228664398193359
Epoch 570, val loss: 1.053389310836792
Epoch 580, training loss: 0.6607967019081116 = 0.038834549486637115 + 0.1 * 6.219621181488037
Epoch 580, val loss: 1.0686346292495728
Epoch 590, training loss: 0.6587698459625244 = 0.03655208647251129 + 0.1 * 6.222177505493164
Epoch 590, val loss: 1.0836437940597534
Epoch 600, training loss: 0.6556466817855835 = 0.03445820510387421 + 0.1 * 6.211884498596191
Epoch 600, val loss: 1.0982738733291626
Epoch 610, training loss: 0.653594970703125 = 0.03253006935119629 + 0.1 * 6.210649013519287
Epoch 610, val loss: 1.1126344203948975
Epoch 620, training loss: 0.6521545648574829 = 0.03075948730111122 + 0.1 * 6.213950157165527
Epoch 620, val loss: 1.1267277002334595
Epoch 630, training loss: 0.6504168510437012 = 0.029134344309568405 + 0.1 * 6.212824821472168
Epoch 630, val loss: 1.1404038667678833
Epoch 640, training loss: 0.6481227874755859 = 0.027637291699647903 + 0.1 * 6.204854488372803
Epoch 640, val loss: 1.1536920070648193
Epoch 650, training loss: 0.6464396715164185 = 0.026252591982483864 + 0.1 * 6.201870918273926
Epoch 650, val loss: 1.1666327714920044
Epoch 660, training loss: 0.6446065902709961 = 0.024971172213554382 + 0.1 * 6.196353912353516
Epoch 660, val loss: 1.1792628765106201
Epoch 670, training loss: 0.6431012153625488 = 0.023782342672348022 + 0.1 * 6.193188190460205
Epoch 670, val loss: 1.1915993690490723
Epoch 680, training loss: 0.6428649425506592 = 0.022674934938549995 + 0.1 * 6.201900005340576
Epoch 680, val loss: 1.2036628723144531
Epoch 690, training loss: 0.6409134864807129 = 0.02164393849670887 + 0.1 * 6.192695140838623
Epoch 690, val loss: 1.2154980897903442
Epoch 700, training loss: 0.6402373313903809 = 0.020684340968728065 + 0.1 * 6.195529460906982
Epoch 700, val loss: 1.2270301580429077
Epoch 710, training loss: 0.6392052173614502 = 0.019788937643170357 + 0.1 * 6.194162845611572
Epoch 710, val loss: 1.2382882833480835
Epoch 720, training loss: 0.6372908353805542 = 0.01895345188677311 + 0.1 * 6.183373928070068
Epoch 720, val loss: 1.2492648363113403
Epoch 730, training loss: 0.6360410451889038 = 0.018171479925513268 + 0.1 * 6.178695201873779
Epoch 730, val loss: 1.260009765625
Epoch 740, training loss: 0.6360623836517334 = 0.017438391223549843 + 0.1 * 6.186239719390869
Epoch 740, val loss: 1.2705683708190918
Epoch 750, training loss: 0.6347706317901611 = 0.01675179786980152 + 0.1 * 6.180188179016113
Epoch 750, val loss: 1.2808609008789062
Epoch 760, training loss: 0.634611189365387 = 0.016107266768813133 + 0.1 * 6.185039043426514
Epoch 760, val loss: 1.2908939123153687
Epoch 770, training loss: 0.633040189743042 = 0.015501072630286217 + 0.1 * 6.175390720367432
Epoch 770, val loss: 1.3006988763809204
Epoch 780, training loss: 0.6327584385871887 = 0.014929685741662979 + 0.1 * 6.178287029266357
Epoch 780, val loss: 1.3102785348892212
Epoch 790, training loss: 0.6316535472869873 = 0.01439127791672945 + 0.1 * 6.172622203826904
Epoch 790, val loss: 1.3195874691009521
Epoch 800, training loss: 0.6311699151992798 = 0.013884037733078003 + 0.1 * 6.172859191894531
Epoch 800, val loss: 1.3287233114242554
Epoch 810, training loss: 0.6308932900428772 = 0.013404103927314281 + 0.1 * 6.174891471862793
Epoch 810, val loss: 1.3376191854476929
Epoch 820, training loss: 0.6300147771835327 = 0.012950871139764786 + 0.1 * 6.170638561248779
Epoch 820, val loss: 1.3463284969329834
Epoch 830, training loss: 0.6290570497512817 = 0.012521772645413876 + 0.1 * 6.165352821350098
Epoch 830, val loss: 1.3548599481582642
Epoch 840, training loss: 0.6282209753990173 = 0.012115569785237312 + 0.1 * 6.161053657531738
Epoch 840, val loss: 1.3632506132125854
Epoch 850, training loss: 0.6280856132507324 = 0.011729173362255096 + 0.1 * 6.163564205169678
Epoch 850, val loss: 1.3714518547058105
Epoch 860, training loss: 0.6282010078430176 = 0.011362086981534958 + 0.1 * 6.168389320373535
Epoch 860, val loss: 1.3795076608657837
Epoch 870, training loss: 0.6269956231117249 = 0.011013481765985489 + 0.1 * 6.159821033477783
Epoch 870, val loss: 1.387404441833496
Epoch 880, training loss: 0.6259990334510803 = 0.010682446882128716 + 0.1 * 6.153165340423584
Epoch 880, val loss: 1.395124077796936
Epoch 890, training loss: 0.62549889087677 = 0.010366980917751789 + 0.1 * 6.1513190269470215
Epoch 890, val loss: 1.402730107307434
Epoch 900, training loss: 0.6262949109077454 = 0.01006581075489521 + 0.1 * 6.162291049957275
Epoch 900, val loss: 1.4101871252059937
Epoch 910, training loss: 0.6261814832687378 = 0.009778723120689392 + 0.1 * 6.164027214050293
Epoch 910, val loss: 1.417513370513916
Epoch 920, training loss: 0.6245399713516235 = 0.009505301713943481 + 0.1 * 6.150346279144287
Epoch 920, val loss: 1.4247138500213623
Epoch 930, training loss: 0.6246154308319092 = 0.009244526736438274 + 0.1 * 6.1537089347839355
Epoch 930, val loss: 1.4317539930343628
Epoch 940, training loss: 0.6237309575080872 = 0.00899511855095625 + 0.1 * 6.147358417510986
Epoch 940, val loss: 1.4386481046676636
Epoch 950, training loss: 0.624972939491272 = 0.008756597526371479 + 0.1 * 6.162163257598877
Epoch 950, val loss: 1.4454542398452759
Epoch 960, training loss: 0.6232313513755798 = 0.008528582751750946 + 0.1 * 6.147027492523193
Epoch 960, val loss: 1.4520646333694458
Epoch 970, training loss: 0.6236660480499268 = 0.008310279808938503 + 0.1 * 6.153557777404785
Epoch 970, val loss: 1.4586445093154907
Epoch 980, training loss: 0.6223210692405701 = 0.008100807666778564 + 0.1 * 6.142202377319336
Epoch 980, val loss: 1.4650646448135376
Epoch 990, training loss: 0.6219652891159058 = 0.00790013000369072 + 0.1 * 6.140651702880859
Epoch 990, val loss: 1.4714264869689941
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.4760
Flip ASR: 0.4133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7940754890441895 = 1.9566985368728638 + 0.1 * 8.37376880645752
Epoch 0, val loss: 1.9531749486923218
Epoch 10, training loss: 2.7831063270568848 = 1.9457520246505737 + 0.1 * 8.373542785644531
Epoch 10, val loss: 1.9425616264343262
Epoch 20, training loss: 2.769609212875366 = 1.9323735237121582 + 0.1 * 8.372357368469238
Epoch 20, val loss: 1.9292060136795044
Epoch 30, training loss: 2.7500929832458496 = 1.913649559020996 + 0.1 * 8.364433288574219
Epoch 30, val loss: 1.91032874584198
Epoch 40, training loss: 2.717355728149414 = 1.8860245943069458 + 0.1 * 8.313310623168945
Epoch 40, val loss: 1.8827953338623047
Epoch 50, training loss: 2.6439943313598633 = 1.8485077619552612 + 0.1 * 7.954866886138916
Epoch 50, val loss: 1.8473464250564575
Epoch 60, training loss: 2.5530829429626465 = 1.8083807229995728 + 0.1 * 7.447022438049316
Epoch 60, val loss: 1.8112423419952393
Epoch 70, training loss: 2.481050729751587 = 1.7683751583099365 + 0.1 * 7.1267547607421875
Epoch 70, val loss: 1.7759512662887573
Epoch 80, training loss: 2.4151477813720703 = 1.7274715900421143 + 0.1 * 6.876762390136719
Epoch 80, val loss: 1.7417712211608887
Epoch 90, training loss: 2.3473684787750244 = 1.6761225461959839 + 0.1 * 6.712459087371826
Epoch 90, val loss: 1.6961122751235962
Epoch 100, training loss: 2.2717347145080566 = 1.6068849563598633 + 0.1 * 6.648497581481934
Epoch 100, val loss: 1.636637568473816
Epoch 110, training loss: 2.1841375827789307 = 1.5237399339675903 + 0.1 * 6.603976249694824
Epoch 110, val loss: 1.5700684785842896
Epoch 120, training loss: 2.0960726737976074 = 1.4392271041870117 + 0.1 * 6.568455219268799
Epoch 120, val loss: 1.5043748617172241
Epoch 130, training loss: 2.016515016555786 = 1.3624005317687988 + 0.1 * 6.541144371032715
Epoch 130, val loss: 1.4496288299560547
Epoch 140, training loss: 1.9458131790161133 = 1.2940027713775635 + 0.1 * 6.51810359954834
Epoch 140, val loss: 1.4046627283096313
Epoch 150, training loss: 1.88261079788208 = 1.2329775094985962 + 0.1 * 6.49633264541626
Epoch 150, val loss: 1.367128610610962
Epoch 160, training loss: 1.8243629932403564 = 1.1766287088394165 + 0.1 * 6.477343559265137
Epoch 160, val loss: 1.3339523077011108
Epoch 170, training loss: 1.7675361633300781 = 1.1213901042938232 + 0.1 * 6.461460113525391
Epoch 170, val loss: 1.3022819757461548
Epoch 180, training loss: 1.709263801574707 = 1.064391016960144 + 0.1 * 6.448728561401367
Epoch 180, val loss: 1.269892692565918
Epoch 190, training loss: 1.6478419303894043 = 1.0042178630828857 + 0.1 * 6.4362406730651855
Epoch 190, val loss: 1.2350001335144043
Epoch 200, training loss: 1.5846993923187256 = 0.9413056373596191 + 0.1 * 6.433937072753906
Epoch 200, val loss: 1.1971980333328247
Epoch 210, training loss: 1.519981026649475 = 0.8781887888908386 + 0.1 * 6.417922496795654
Epoch 210, val loss: 1.1579170227050781
Epoch 220, training loss: 1.4573566913604736 = 0.8158591389656067 + 0.1 * 6.414974689483643
Epoch 220, val loss: 1.1180680990219116
Epoch 230, training loss: 1.3957138061523438 = 0.7555010914802551 + 0.1 * 6.402126312255859
Epoch 230, val loss: 1.0785140991210938
Epoch 240, training loss: 1.3373401165008545 = 0.6977943778038025 + 0.1 * 6.395456790924072
Epoch 240, val loss: 1.0398882627487183
Epoch 250, training loss: 1.282862901687622 = 0.6440195441246033 + 0.1 * 6.388433933258057
Epoch 250, val loss: 1.0034998655319214
Epoch 260, training loss: 1.232208013534546 = 0.5942428708076477 + 0.1 * 6.379650592803955
Epoch 260, val loss: 0.9699796438217163
Epoch 270, training loss: 1.185410976409912 = 0.547498881816864 + 0.1 * 6.37912130355835
Epoch 270, val loss: 0.9387845396995544
Epoch 280, training loss: 1.1407525539398193 = 0.503384530544281 + 0.1 * 6.373680114746094
Epoch 280, val loss: 0.9107329845428467
Epoch 290, training loss: 1.0978556871414185 = 0.46073147654533386 + 0.1 * 6.371241569519043
Epoch 290, val loss: 0.8850216269493103
Epoch 300, training loss: 1.0552542209625244 = 0.41942334175109863 + 0.1 * 6.358307838439941
Epoch 300, val loss: 0.8619109988212585
Epoch 310, training loss: 1.0150598287582397 = 0.37936609983444214 + 0.1 * 6.356936931610107
Epoch 310, val loss: 0.8413580656051636
Epoch 320, training loss: 0.9762414693832397 = 0.341094434261322 + 0.1 * 6.351470470428467
Epoch 320, val loss: 0.8241691589355469
Epoch 330, training loss: 0.9392192363739014 = 0.3050626218318939 + 0.1 * 6.3415656089782715
Epoch 330, val loss: 0.8106829524040222
Epoch 340, training loss: 0.9055727124214172 = 0.2717438340187073 + 0.1 * 6.3382887840271
Epoch 340, val loss: 0.8011047840118408
Epoch 350, training loss: 0.8754079937934875 = 0.24157635867595673 + 0.1 * 6.338315963745117
Epoch 350, val loss: 0.7952486276626587
Epoch 360, training loss: 0.8486289978027344 = 0.21489645540714264 + 0.1 * 6.337325572967529
Epoch 360, val loss: 0.7927592992782593
Epoch 370, training loss: 0.8241041302680969 = 0.19175578653812408 + 0.1 * 6.323483467102051
Epoch 370, val loss: 0.7932794094085693
Epoch 380, training loss: 0.8039097189903259 = 0.17187805473804474 + 0.1 * 6.320316791534424
Epoch 380, val loss: 0.7962913513183594
Epoch 390, training loss: 0.787054181098938 = 0.1548331379890442 + 0.1 * 6.322210311889648
Epoch 390, val loss: 0.8012059330940247
Epoch 400, training loss: 0.7717941403388977 = 0.14007125794887543 + 0.1 * 6.3172287940979
Epoch 400, val loss: 0.8074451684951782
Epoch 410, training loss: 0.7580647468566895 = 0.12716537714004517 + 0.1 * 6.308993339538574
Epoch 410, val loss: 0.8144583106040955
Epoch 420, training loss: 0.7455256581306458 = 0.11574377119541168 + 0.1 * 6.297818660736084
Epoch 420, val loss: 0.822025716304779
Epoch 430, training loss: 0.7363178133964539 = 0.10553091019392014 + 0.1 * 6.307868957519531
Epoch 430, val loss: 0.8300928473472595
Epoch 440, training loss: 0.7253954410552979 = 0.0964088886976242 + 0.1 * 6.289865493774414
Epoch 440, val loss: 0.8381783366203308
Epoch 450, training loss: 0.7166375517845154 = 0.08821434527635574 + 0.1 * 6.284232139587402
Epoch 450, val loss: 0.8465198278427124
Epoch 460, training loss: 0.7088691592216492 = 0.08081186562776566 + 0.1 * 6.280572891235352
Epoch 460, val loss: 0.855071485042572
Epoch 470, training loss: 0.70186847448349 = 0.07412765175104141 + 0.1 * 6.277407646179199
Epoch 470, val loss: 0.8633694648742676
Epoch 480, training loss: 0.6954125761985779 = 0.06809987127780914 + 0.1 * 6.27312707901001
Epoch 480, val loss: 0.8718245029449463
Epoch 490, training loss: 0.690028190612793 = 0.06263668090105057 + 0.1 * 6.273914813995361
Epoch 490, val loss: 0.8801899552345276
Epoch 500, training loss: 0.6842038035392761 = 0.057691216468811035 + 0.1 * 6.265125751495361
Epoch 500, val loss: 0.8884607553482056
Epoch 510, training loss: 0.6794070601463318 = 0.05321454629302025 + 0.1 * 6.261925220489502
Epoch 510, val loss: 0.896606981754303
Epoch 520, training loss: 0.6747715473175049 = 0.04916449636220932 + 0.1 * 6.256070613861084
Epoch 520, val loss: 0.9048115015029907
Epoch 530, training loss: 0.6710458993911743 = 0.045489970594644547 + 0.1 * 6.255558967590332
Epoch 530, val loss: 0.9129138588905334
Epoch 540, training loss: 0.6676340699195862 = 0.04216378927230835 + 0.1 * 6.254702568054199
Epoch 540, val loss: 0.9207908511161804
Epoch 550, training loss: 0.6638715267181396 = 0.03915650397539139 + 0.1 * 6.247150421142578
Epoch 550, val loss: 0.9286371469497681
Epoch 560, training loss: 0.6612781882286072 = 0.03642399236559868 + 0.1 * 6.248541831970215
Epoch 560, val loss: 0.9363240599632263
Epoch 570, training loss: 0.6599277257919312 = 0.03394600749015808 + 0.1 * 6.259817600250244
Epoch 570, val loss: 0.9437105059623718
Epoch 580, training loss: 0.6556132435798645 = 0.031706444919109344 + 0.1 * 6.239068031311035
Epoch 580, val loss: 0.9510842561721802
Epoch 590, training loss: 0.6531342267990112 = 0.02966812252998352 + 0.1 * 6.234660625457764
Epoch 590, val loss: 0.9583559632301331
Epoch 600, training loss: 0.6508914828300476 = 0.02780962735414505 + 0.1 * 6.230818748474121
Epoch 600, val loss: 0.9654373526573181
Epoch 610, training loss: 0.6501357555389404 = 0.02611638605594635 + 0.1 * 6.240193843841553
Epoch 610, val loss: 0.9723819494247437
Epoch 620, training loss: 0.6473485827445984 = 0.024572499096393585 + 0.1 * 6.2277607917785645
Epoch 620, val loss: 0.9792231917381287
Epoch 630, training loss: 0.6455154418945312 = 0.023160045966506004 + 0.1 * 6.223553657531738
Epoch 630, val loss: 0.9860193133354187
Epoch 640, training loss: 0.6455909609794617 = 0.021862870082259178 + 0.1 * 6.23728084564209
Epoch 640, val loss: 0.9924781918525696
Epoch 650, training loss: 0.6424993872642517 = 0.020678265020251274 + 0.1 * 6.218210697174072
Epoch 650, val loss: 0.9989414811134338
Epoch 660, training loss: 0.6425396203994751 = 0.019589627161622047 + 0.1 * 6.229499816894531
Epoch 660, val loss: 1.0052752494812012
Epoch 670, training loss: 0.6404876112937927 = 0.01858796738088131 + 0.1 * 6.218996524810791
Epoch 670, val loss: 1.0113393068313599
Epoch 680, training loss: 0.6383118033409119 = 0.01766788214445114 + 0.1 * 6.206439018249512
Epoch 680, val loss: 1.0174964666366577
Epoch 690, training loss: 0.637200117111206 = 0.01681365817785263 + 0.1 * 6.203864574432373
Epoch 690, val loss: 1.0234942436218262
Epoch 700, training loss: 0.6374874711036682 = 0.016022346913814545 + 0.1 * 6.214651584625244
Epoch 700, val loss: 1.0293004512786865
Epoch 710, training loss: 0.6363930702209473 = 0.015289130620658398 + 0.1 * 6.211039066314697
Epoch 710, val loss: 1.0350209474563599
Epoch 720, training loss: 0.6343763470649719 = 0.014608923345804214 + 0.1 * 6.19767427444458
Epoch 720, val loss: 1.0406697988510132
Epoch 730, training loss: 0.6362836360931396 = 0.013976659625768661 + 0.1 * 6.223069667816162
Epoch 730, val loss: 1.0462294816970825
Epoch 740, training loss: 0.6329646110534668 = 0.01338734570890665 + 0.1 * 6.195772647857666
Epoch 740, val loss: 1.051516056060791
Epoch 750, training loss: 0.6321092844009399 = 0.012838802300393581 + 0.1 * 6.192705154418945
Epoch 750, val loss: 1.0569180250167847
Epoch 760, training loss: 0.632664680480957 = 0.012323904782533646 + 0.1 * 6.203407287597656
Epoch 760, val loss: 1.0620646476745605
Epoch 770, training loss: 0.6310466527938843 = 0.011842749081552029 + 0.1 * 6.1920390129089355
Epoch 770, val loss: 1.0671463012695312
Epoch 780, training loss: 0.6310335993766785 = 0.011391486041247845 + 0.1 * 6.196421146392822
Epoch 780, val loss: 1.0721958875656128
Epoch 790, training loss: 0.6296408772468567 = 0.01096649281680584 + 0.1 * 6.18674373626709
Epoch 790, val loss: 1.0770868062973022
Epoch 800, training loss: 0.6293624639511108 = 0.010568932630121708 + 0.1 * 6.1879353523254395
Epoch 800, val loss: 1.0820475816726685
Epoch 810, training loss: 0.6284918189048767 = 0.010193608701229095 + 0.1 * 6.182981967926025
Epoch 810, val loss: 1.0867732763290405
Epoch 820, training loss: 0.627767026424408 = 0.009840752929449081 + 0.1 * 6.179262638092041
Epoch 820, val loss: 1.0915535688400269
Epoch 830, training loss: 0.6281720995903015 = 0.00950723234564066 + 0.1 * 6.186648368835449
Epoch 830, val loss: 1.0962051153182983
Epoch 840, training loss: 0.6282440423965454 = 0.009191428311169147 + 0.1 * 6.190526008605957
Epoch 840, val loss: 1.1004819869995117
Epoch 850, training loss: 0.6262272000312805 = 0.008895331993699074 + 0.1 * 6.173318386077881
Epoch 850, val loss: 1.1049890518188477
Epoch 860, training loss: 0.6262157559394836 = 0.0086146779358387 + 0.1 * 6.176011085510254
Epoch 860, val loss: 1.109452247619629
Epoch 870, training loss: 0.6249207258224487 = 0.00834793783724308 + 0.1 * 6.1657280921936035
Epoch 870, val loss: 1.1135538816452026
Epoch 880, training loss: 0.6245064735412598 = 0.00809572171419859 + 0.1 * 6.164107322692871
Epoch 880, val loss: 1.117857813835144
Epoch 890, training loss: 0.6250790357589722 = 0.007855615578591824 + 0.1 * 6.172234535217285
Epoch 890, val loss: 1.1219847202301025
Epoch 900, training loss: 0.6251139640808105 = 0.007626505568623543 + 0.1 * 6.174874305725098
Epoch 900, val loss: 1.1259111166000366
Epoch 910, training loss: 0.6246543526649475 = 0.007409941870719194 + 0.1 * 6.1724443435668945
Epoch 910, val loss: 1.1298352479934692
Epoch 920, training loss: 0.6230821013450623 = 0.0072042192332446575 + 0.1 * 6.158778667449951
Epoch 920, val loss: 1.133804202079773
Epoch 930, training loss: 0.6227717399597168 = 0.00700813252478838 + 0.1 * 6.1576361656188965
Epoch 930, val loss: 1.1377485990524292
Epoch 940, training loss: 0.6236727833747864 = 0.006820213980972767 + 0.1 * 6.168525695800781
Epoch 940, val loss: 1.1414306163787842
Epoch 950, training loss: 0.6231467723846436 = 0.006640832405537367 + 0.1 * 6.1650590896606445
Epoch 950, val loss: 1.1450276374816895
Epoch 960, training loss: 0.6217459440231323 = 0.006470413412898779 + 0.1 * 6.152754783630371
Epoch 960, val loss: 1.1486374139785767
Epoch 970, training loss: 0.6211954951286316 = 0.006307495757937431 + 0.1 * 6.1488800048828125
Epoch 970, val loss: 1.1523786783218384
Epoch 980, training loss: 0.6226736903190613 = 0.006150905974209309 + 0.1 * 6.165227890014648
Epoch 980, val loss: 1.1559042930603027
Epoch 990, training loss: 0.6211226582527161 = 0.006000863388180733 + 0.1 * 6.151217937469482
Epoch 990, val loss: 1.159306287765503
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7159
Flip ASR: 0.6756/225 nodes
The final ASR:0.66421, 0.13751, Accuracy:0.80988, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11672])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10606])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7764499187469482 = 1.9390654563903809 + 0.1 * 8.373845100402832
Epoch 0, val loss: 1.938527226448059
Epoch 10, training loss: 2.766458034515381 = 1.9290881156921387 + 0.1 * 8.373698234558105
Epoch 10, val loss: 1.9288831949234009
Epoch 20, training loss: 2.753566265106201 = 1.9162620306015015 + 0.1 * 8.373041152954102
Epoch 20, val loss: 1.9160574674606323
Epoch 30, training loss: 2.734644651412964 = 1.8977510929107666 + 0.1 * 8.368934631347656
Epoch 30, val loss: 1.8973850011825562
Epoch 40, training loss: 2.70479154586792 = 1.8705363273620605 + 0.1 * 8.34255313873291
Epoch 40, val loss: 1.8704949617385864
Epoch 50, training loss: 2.6486899852752686 = 1.8343479633331299 + 0.1 * 8.14341926574707
Epoch 50, val loss: 1.8370203971862793
Epoch 60, training loss: 2.5576627254486084 = 1.796348214149475 + 0.1 * 7.61314582824707
Epoch 60, val loss: 1.80489182472229
Epoch 70, training loss: 2.4847514629364014 = 1.7625843286514282 + 0.1 * 7.221671104431152
Epoch 70, val loss: 1.7767918109893799
Epoch 80, training loss: 2.423064947128296 = 1.7254923582077026 + 0.1 * 6.975726127624512
Epoch 80, val loss: 1.7449650764465332
Epoch 90, training loss: 2.361760139465332 = 1.6757187843322754 + 0.1 * 6.86041259765625
Epoch 90, val loss: 1.7011489868164062
Epoch 100, training loss: 2.2899258136749268 = 1.6089129447937012 + 0.1 * 6.8101277351379395
Epoch 100, val loss: 1.6428810358047485
Epoch 110, training loss: 2.2023420333862305 = 1.526086688041687 + 0.1 * 6.762552261352539
Epoch 110, val loss: 1.5726861953735352
Epoch 120, training loss: 2.1051321029663086 = 1.432445764541626 + 0.1 * 6.726862907409668
Epoch 120, val loss: 1.4946556091308594
Epoch 130, training loss: 2.0044968128204346 = 1.333979845046997 + 0.1 * 6.705169200897217
Epoch 130, val loss: 1.4154661893844604
Epoch 140, training loss: 1.9023220539093018 = 1.2337461709976196 + 0.1 * 6.685759544372559
Epoch 140, val loss: 1.3374712467193604
Epoch 150, training loss: 1.8028342723846436 = 1.1353176832199097 + 0.1 * 6.675166606903076
Epoch 150, val loss: 1.2635550498962402
Epoch 160, training loss: 1.7094168663024902 = 1.0441884994506836 + 0.1 * 6.652283191680908
Epoch 160, val loss: 1.1968611478805542
Epoch 170, training loss: 1.6241347789764404 = 0.9599735736846924 + 0.1 * 6.641611576080322
Epoch 170, val loss: 1.1353769302368164
Epoch 180, training loss: 1.544607400894165 = 0.8811926245689392 + 0.1 * 6.634147644042969
Epoch 180, val loss: 1.0779603719711304
Epoch 190, training loss: 1.46859610080719 = 0.8064878582954407 + 0.1 * 6.621082305908203
Epoch 190, val loss: 1.024195671081543
Epoch 200, training loss: 1.3963450193405151 = 0.7346289753913879 + 0.1 * 6.617160320281982
Epoch 200, val loss: 0.973673939704895
Epoch 210, training loss: 1.3267104625701904 = 0.6665262579917908 + 0.1 * 6.601841449737549
Epoch 210, val loss: 0.9279954433441162
Epoch 220, training loss: 1.2623851299285889 = 0.6032324433326721 + 0.1 * 6.591527462005615
Epoch 220, val loss: 0.888776957988739
Epoch 230, training loss: 1.2052475214004517 = 0.5459588170051575 + 0.1 * 6.592886924743652
Epoch 230, val loss: 0.8576450943946838
Epoch 240, training loss: 1.1523131132125854 = 0.49534669518470764 + 0.1 * 6.5696635246276855
Epoch 240, val loss: 0.8350231051445007
Epoch 250, training loss: 1.1056509017944336 = 0.4501959979534149 + 0.1 * 6.554549217224121
Epoch 250, val loss: 0.8192840218544006
Epoch 260, training loss: 1.0642961263656616 = 0.4096395671367645 + 0.1 * 6.546565055847168
Epoch 260, val loss: 0.8086884617805481
Epoch 270, training loss: 1.0264172554016113 = 0.3732549548149109 + 0.1 * 6.531623363494873
Epoch 270, val loss: 0.8018152713775635
Epoch 280, training loss: 0.9924259185791016 = 0.34004679322242737 + 0.1 * 6.523791313171387
Epoch 280, val loss: 0.7976463437080383
Epoch 290, training loss: 0.9602186679840088 = 0.3096369802951813 + 0.1 * 6.50581693649292
Epoch 290, val loss: 0.7957637310028076
Epoch 300, training loss: 0.9314133524894714 = 0.2814584970474243 + 0.1 * 6.499548435211182
Epoch 300, val loss: 0.7959946393966675
Epoch 310, training loss: 0.9039633870124817 = 0.25491052865982056 + 0.1 * 6.490528583526611
Epoch 310, val loss: 0.7980086803436279
Epoch 320, training loss: 0.8783900737762451 = 0.22991497814655304 + 0.1 * 6.484750747680664
Epoch 320, val loss: 0.8016619086265564
Epoch 330, training loss: 0.8533719182014465 = 0.20648758113384247 + 0.1 * 6.46884298324585
Epoch 330, val loss: 0.8068451881408691
Epoch 340, training loss: 0.831931471824646 = 0.18473407626152039 + 0.1 * 6.471973896026611
Epoch 340, val loss: 0.813579261302948
Epoch 350, training loss: 0.8110411167144775 = 0.16496804356575012 + 0.1 * 6.460730075836182
Epoch 350, val loss: 0.821780800819397
Epoch 360, training loss: 0.7923269271850586 = 0.14737513661384583 + 0.1 * 6.449517726898193
Epoch 360, val loss: 0.8313992619514465
Epoch 370, training loss: 0.7764267325401306 = 0.13183023035526276 + 0.1 * 6.445964813232422
Epoch 370, val loss: 0.8421985507011414
Epoch 380, training loss: 0.7624560594558716 = 0.1181328296661377 + 0.1 * 6.44323205947876
Epoch 380, val loss: 0.8538219332695007
Epoch 390, training loss: 0.7488993406295776 = 0.1060284674167633 + 0.1 * 6.428708553314209
Epoch 390, val loss: 0.8660740852355957
Epoch 400, training loss: 0.7387239336967468 = 0.09529940038919449 + 0.1 * 6.4342451095581055
Epoch 400, val loss: 0.8788042068481445
Epoch 410, training loss: 0.7277646660804749 = 0.08582089096307755 + 0.1 * 6.419437408447266
Epoch 410, val loss: 0.8918011784553528
Epoch 420, training loss: 0.7183564305305481 = 0.07742023468017578 + 0.1 * 6.409361839294434
Epoch 420, val loss: 0.9051769375801086
Epoch 430, training loss: 0.7105860710144043 = 0.06999418139457703 + 0.1 * 6.405919075012207
Epoch 430, val loss: 0.9187935590744019
Epoch 440, training loss: 0.7031475901603699 = 0.06344667822122574 + 0.1 * 6.397008895874023
Epoch 440, val loss: 0.932457447052002
Epoch 450, training loss: 0.6991785168647766 = 0.05765310302376747 + 0.1 * 6.415253639221191
Epoch 450, val loss: 0.9462023973464966
Epoch 460, training loss: 0.6911324262619019 = 0.05255335941910744 + 0.1 * 6.385790824890137
Epoch 460, val loss: 0.9597349762916565
Epoch 470, training loss: 0.6862587332725525 = 0.048046812415122986 + 0.1 * 6.3821187019348145
Epoch 470, val loss: 0.9731580018997192
Epoch 480, training loss: 0.6834566593170166 = 0.04405412822961807 + 0.1 * 6.394024848937988
Epoch 480, val loss: 0.9865306615829468
Epoch 490, training loss: 0.6777939200401306 = 0.040522631257772446 + 0.1 * 6.3727126121521
Epoch 490, val loss: 0.9995188117027283
Epoch 500, training loss: 0.6741179823875427 = 0.03738926723599434 + 0.1 * 6.3672871589660645
Epoch 500, val loss: 1.0123471021652222
Epoch 510, training loss: 0.6721214056015015 = 0.034600112587213516 + 0.1 * 6.375212669372559
Epoch 510, val loss: 1.0249247550964355
Epoch 520, training loss: 0.6678532361984253 = 0.03211609274148941 + 0.1 * 6.3573713302612305
Epoch 520, val loss: 1.0370252132415771
Epoch 530, training loss: 0.6650523543357849 = 0.02989950217306614 + 0.1 * 6.351528644561768
Epoch 530, val loss: 1.0487840175628662
Epoch 540, training loss: 0.6629453301429749 = 0.0279096569865942 + 0.1 * 6.350356578826904
Epoch 540, val loss: 1.0601470470428467
Epoch 550, training loss: 0.659808337688446 = 0.02612413465976715 + 0.1 * 6.336842060089111
Epoch 550, val loss: 1.0711944103240967
Epoch 560, training loss: 0.6583489179611206 = 0.02451077289879322 + 0.1 * 6.338380813598633
Epoch 560, val loss: 1.0818800926208496
Epoch 570, training loss: 0.6556228399276733 = 0.023051682859659195 + 0.1 * 6.325711727142334
Epoch 570, val loss: 1.0923093557357788
Epoch 580, training loss: 0.6547250151634216 = 0.0217242743819952 + 0.1 * 6.330007076263428
Epoch 580, val loss: 1.102420449256897
Epoch 590, training loss: 0.6564606428146362 = 0.020516669377684593 + 0.1 * 6.359439373016357
Epoch 590, val loss: 1.112281084060669
Epoch 600, training loss: 0.6518710851669312 = 0.019417401403188705 + 0.1 * 6.3245368003845215
Epoch 600, val loss: 1.1216520071029663
Epoch 610, training loss: 0.6496252417564392 = 0.01841677352786064 + 0.1 * 6.312084674835205
Epoch 610, val loss: 1.130856990814209
Epoch 620, training loss: 0.6476410031318665 = 0.017494723200798035 + 0.1 * 6.301462650299072
Epoch 620, val loss: 1.1397981643676758
Epoch 630, training loss: 0.6471477150917053 = 0.016643313691020012 + 0.1 * 6.305044174194336
Epoch 630, val loss: 1.1486027240753174
Epoch 640, training loss: 0.6468169689178467 = 0.015855176374316216 + 0.1 * 6.309617519378662
Epoch 640, val loss: 1.1569966077804565
Epoch 650, training loss: 0.6441816091537476 = 0.015131056308746338 + 0.1 * 6.290505409240723
Epoch 650, val loss: 1.165276288986206
Epoch 660, training loss: 0.6448001861572266 = 0.014457674697041512 + 0.1 * 6.303424835205078
Epoch 660, val loss: 1.1732462644577026
Epoch 670, training loss: 0.6427109837532043 = 0.01383211649954319 + 0.1 * 6.288788795471191
Epoch 670, val loss: 1.1810239553451538
Epoch 680, training loss: 0.6419671177864075 = 0.013249521143734455 + 0.1 * 6.287176132202148
Epoch 680, val loss: 1.188620686531067
Epoch 690, training loss: 0.642429769039154 = 0.012706002220511436 + 0.1 * 6.297237873077393
Epoch 690, val loss: 1.1959354877471924
Epoch 700, training loss: 0.6400353312492371 = 0.01219987217336893 + 0.1 * 6.278354644775391
Epoch 700, val loss: 1.2030593156814575
Epoch 710, training loss: 0.6387564539909363 = 0.011727326549589634 + 0.1 * 6.270290851593018
Epoch 710, val loss: 1.2100801467895508
Epoch 720, training loss: 0.6388136744499207 = 0.01128256693482399 + 0.1 * 6.27531099319458
Epoch 720, val loss: 1.2168492078781128
Epoch 730, training loss: 0.6407740712165833 = 0.010865687392652035 + 0.1 * 6.299083709716797
Epoch 730, val loss: 1.2234188318252563
Epoch 740, training loss: 0.6368975639343262 = 0.010474832728505135 + 0.1 * 6.264226913452148
Epoch 740, val loss: 1.2298438549041748
Epoch 750, training loss: 0.6358282566070557 = 0.010108529590070248 + 0.1 * 6.25719690322876
Epoch 750, val loss: 1.2361925840377808
Epoch 760, training loss: 0.6374372243881226 = 0.009761802852153778 + 0.1 * 6.276753902435303
Epoch 760, val loss: 1.2423547506332397
Epoch 770, training loss: 0.6355673670768738 = 0.009433768689632416 + 0.1 * 6.261336326599121
Epoch 770, val loss: 1.2483031749725342
Epoch 780, training loss: 0.6337865591049194 = 0.00912487879395485 + 0.1 * 6.246616840362549
Epoch 780, val loss: 1.2541499137878418
Epoch 790, training loss: 0.6334450840950012 = 0.008833964355289936 + 0.1 * 6.2461113929748535
Epoch 790, val loss: 1.2599761486053467
Epoch 800, training loss: 0.6342781782150269 = 0.008557047694921494 + 0.1 * 6.257211208343506
Epoch 800, val loss: 1.2656008005142212
Epoch 810, training loss: 0.6322683095932007 = 0.008294498547911644 + 0.1 * 6.239737510681152
Epoch 810, val loss: 1.2710931301116943
Epoch 820, training loss: 0.6340239644050598 = 0.008045526221394539 + 0.1 * 6.25978422164917
Epoch 820, val loss: 1.27651846408844
Epoch 830, training loss: 0.6317777633666992 = 0.007808325346559286 + 0.1 * 6.239694118499756
Epoch 830, val loss: 1.2816417217254639
Epoch 840, training loss: 0.6305056810379028 = 0.0075843483209609985 + 0.1 * 6.229213237762451
Epoch 840, val loss: 1.286865472793579
Epoch 850, training loss: 0.631693422794342 = 0.0073701017536222935 + 0.1 * 6.2432332038879395
Epoch 850, val loss: 1.291886568069458
Epoch 860, training loss: 0.6300375461578369 = 0.007165806367993355 + 0.1 * 6.22871732711792
Epoch 860, val loss: 1.2967896461486816
Epoch 870, training loss: 0.6295362710952759 = 0.006971672177314758 + 0.1 * 6.225646018981934
Epoch 870, val loss: 1.3017241954803467
Epoch 880, training loss: 0.6304261088371277 = 0.006785850506275892 + 0.1 * 6.23640251159668
Epoch 880, val loss: 1.3065484762191772
Epoch 890, training loss: 0.628887414932251 = 0.006607460789382458 + 0.1 * 6.222799301147461
Epoch 890, val loss: 1.3111072778701782
Epoch 900, training loss: 0.6280267238616943 = 0.006438801530748606 + 0.1 * 6.215878963470459
Epoch 900, val loss: 1.3157973289489746
Epoch 910, training loss: 0.6290480494499207 = 0.006276826374232769 + 0.1 * 6.2277116775512695
Epoch 910, val loss: 1.3203811645507812
Epoch 920, training loss: 0.6274226903915405 = 0.00612090015783906 + 0.1 * 6.213017463684082
Epoch 920, val loss: 1.3247160911560059
Epoch 930, training loss: 0.6269197463989258 = 0.005972903687506914 + 0.1 * 6.209468364715576
Epoch 930, val loss: 1.3291527032852173
Epoch 940, training loss: 0.6288043260574341 = 0.005830536130815744 + 0.1 * 6.229737758636475
Epoch 940, val loss: 1.333451271057129
Epoch 950, training loss: 0.6272261142730713 = 0.005692807026207447 + 0.1 * 6.215333461761475
Epoch 950, val loss: 1.3374944925308228
Epoch 960, training loss: 0.6261997222900391 = 0.005562662146985531 + 0.1 * 6.2063703536987305
Epoch 960, val loss: 1.3416926860809326
Epoch 970, training loss: 0.62651526927948 = 0.005437266081571579 + 0.1 * 6.210780143737793
Epoch 970, val loss: 1.345839262008667
Epoch 980, training loss: 0.6252425909042358 = 0.005315438378602266 + 0.1 * 6.1992716789245605
Epoch 980, val loss: 1.3496489524841309
Epoch 990, training loss: 0.6256239414215088 = 0.005199686158448458 + 0.1 * 6.204242706298828
Epoch 990, val loss: 1.3536051511764526
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.4871
Flip ASR: 0.4000/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7800726890563965 = 1.9426900148391724 + 0.1 * 8.37382698059082
Epoch 0, val loss: 1.9443167448043823
Epoch 10, training loss: 2.7702391147613525 = 1.9328749179840088 + 0.1 * 8.373641014099121
Epoch 10, val loss: 1.933685541152954
Epoch 20, training loss: 2.7576866149902344 = 1.92041015625 + 0.1 * 8.372764587402344
Epoch 20, val loss: 1.919952154159546
Epoch 30, training loss: 2.7392351627349854 = 1.9024957418441772 + 0.1 * 8.36739444732666
Epoch 30, val loss: 1.9002478122711182
Epoch 40, training loss: 2.709228515625 = 1.8757659196853638 + 0.1 * 8.334627151489258
Epoch 40, val loss: 1.8712997436523438
Epoch 50, training loss: 2.6481800079345703 = 1.8393605947494507 + 0.1 * 8.088193893432617
Epoch 50, val loss: 1.8338866233825684
Epoch 60, training loss: 2.550879716873169 = 1.8015538454055786 + 0.1 * 7.493258953094482
Epoch 60, val loss: 1.7992732524871826
Epoch 70, training loss: 2.480091094970703 = 1.766280174255371 + 0.1 * 7.13810920715332
Epoch 70, val loss: 1.7682615518569946
Epoch 80, training loss: 2.4199371337890625 = 1.7280961275100708 + 0.1 * 6.918410301208496
Epoch 80, val loss: 1.7344695329666138
Epoch 90, training loss: 2.3602542877197266 = 1.6791307926177979 + 0.1 * 6.811235427856445
Epoch 90, val loss: 1.692604660987854
Epoch 100, training loss: 2.289216995239258 = 1.6145083904266357 + 0.1 * 6.747086048126221
Epoch 100, val loss: 1.6380677223205566
Epoch 110, training loss: 2.2031736373901367 = 1.5324448347091675 + 0.1 * 6.70728874206543
Epoch 110, val loss: 1.5691943168640137
Epoch 120, training loss: 2.1056573390960693 = 1.4375205039978027 + 0.1 * 6.68136739730835
Epoch 120, val loss: 1.4910995960235596
Epoch 130, training loss: 2.002979278564453 = 1.3373125791549683 + 0.1 * 6.656668186187744
Epoch 130, val loss: 1.4104983806610107
Epoch 140, training loss: 1.8987419605255127 = 1.235723614692688 + 0.1 * 6.630184173583984
Epoch 140, val loss: 1.3295897245407104
Epoch 150, training loss: 1.797126293182373 = 1.1367788314819336 + 0.1 * 6.603473663330078
Epoch 150, val loss: 1.2514441013336182
Epoch 160, training loss: 1.700768232345581 = 1.042710542678833 + 0.1 * 6.580577373504639
Epoch 160, val loss: 1.177515983581543
Epoch 170, training loss: 1.611488699913025 = 0.9554364085197449 + 0.1 * 6.560522556304932
Epoch 170, val loss: 1.109938144683838
Epoch 180, training loss: 1.5291759967803955 = 0.8748931288719177 + 0.1 * 6.5428290367126465
Epoch 180, val loss: 1.0489189624786377
Epoch 190, training loss: 1.4541027545928955 = 0.8010351657867432 + 0.1 * 6.530676364898682
Epoch 190, val loss: 0.9942594170570374
Epoch 200, training loss: 1.3856654167175293 = 0.7342901825904846 + 0.1 * 6.5137529373168945
Epoch 200, val loss: 0.9469418525695801
Epoch 210, training loss: 1.3248693943023682 = 0.6736903190612793 + 0.1 * 6.511790752410889
Epoch 210, val loss: 0.905944287776947
Epoch 220, training loss: 1.2688050270080566 = 0.6191796064376831 + 0.1 * 6.496253967285156
Epoch 220, val loss: 0.8720228672027588
Epoch 230, training loss: 1.217910885810852 = 0.5692763328552246 + 0.1 * 6.486345291137695
Epoch 230, val loss: 0.8440214991569519
Epoch 240, training loss: 1.1709339618682861 = 0.5231362581253052 + 0.1 * 6.4779767990112305
Epoch 240, val loss: 0.8208391070365906
Epoch 250, training loss: 1.1268787384033203 = 0.4800397753715515 + 0.1 * 6.468389987945557
Epoch 250, val loss: 0.801494836807251
Epoch 260, training loss: 1.0866187810897827 = 0.43966788053512573 + 0.1 * 6.469509124755859
Epoch 260, val loss: 0.7851747274398804
Epoch 270, training loss: 1.0476572513580322 = 0.40215423703193665 + 0.1 * 6.455029487609863
Epoch 270, val loss: 0.7719370126724243
Epoch 280, training loss: 1.011885643005371 = 0.36696648597717285 + 0.1 * 6.449191093444824
Epoch 280, val loss: 0.7610405683517456
Epoch 290, training loss: 0.9789705276489258 = 0.3341585099697113 + 0.1 * 6.448120594024658
Epoch 290, val loss: 0.7526705861091614
Epoch 300, training loss: 0.9472203850746155 = 0.3037068247795105 + 0.1 * 6.435135364532471
Epoch 300, val loss: 0.746676504611969
Epoch 310, training loss: 0.918146014213562 = 0.27540746331214905 + 0.1 * 6.427385330200195
Epoch 310, val loss: 0.7429046630859375
Epoch 320, training loss: 0.8924968838691711 = 0.2492731660604477 + 0.1 * 6.432237148284912
Epoch 320, val loss: 0.7412024736404419
Epoch 330, training loss: 0.8667234182357788 = 0.2252625823020935 + 0.1 * 6.414608478546143
Epoch 330, val loss: 0.7414730191230774
Epoch 340, training loss: 0.8438768982887268 = 0.20320892333984375 + 0.1 * 6.406679630279541
Epoch 340, val loss: 0.7435787320137024
Epoch 350, training loss: 0.8248183131217957 = 0.18326687812805176 + 0.1 * 6.4155144691467285
Epoch 350, val loss: 0.7474445700645447
Epoch 360, training loss: 0.8044706583023071 = 0.16546979546546936 + 0.1 * 6.390008449554443
Epoch 360, val loss: 0.7529277801513672
Epoch 370, training loss: 0.7882974743843079 = 0.14961378276348114 + 0.1 * 6.386836528778076
Epoch 370, val loss: 0.7599250674247742
Epoch 380, training loss: 0.7731983661651611 = 0.13561111688613892 + 0.1 * 6.375872611999512
Epoch 380, val loss: 0.7683509588241577
Epoch 390, training loss: 0.7607257962226868 = 0.12329733371734619 + 0.1 * 6.374284267425537
Epoch 390, val loss: 0.7780446410179138
Epoch 400, training loss: 0.7488961815834045 = 0.11242283880710602 + 0.1 * 6.3647332191467285
Epoch 400, val loss: 0.788834810256958
Epoch 410, training loss: 0.7408121824264526 = 0.10285432636737823 + 0.1 * 6.379578590393066
Epoch 410, val loss: 0.8003555536270142
Epoch 420, training loss: 0.7300685048103333 = 0.09444687515497208 + 0.1 * 6.356215953826904
Epoch 420, val loss: 0.8123784065246582
Epoch 430, training loss: 0.7227885127067566 = 0.08699077367782593 + 0.1 * 6.357977390289307
Epoch 430, val loss: 0.8248518109321594
Epoch 440, training loss: 0.7146157026290894 = 0.08038017898797989 + 0.1 * 6.342354774475098
Epoch 440, val loss: 0.8375352621078491
Epoch 450, training loss: 0.7087538838386536 = 0.07448243349790573 + 0.1 * 6.342714309692383
Epoch 450, val loss: 0.8503074645996094
Epoch 460, training loss: 0.7047755718231201 = 0.06921559572219849 + 0.1 * 6.355599880218506
Epoch 460, val loss: 0.8629858493804932
Epoch 470, training loss: 0.6976709961891174 = 0.06451287120580673 + 0.1 * 6.331581115722656
Epoch 470, val loss: 0.8754584193229675
Epoch 480, training loss: 0.692562460899353 = 0.06028193607926369 + 0.1 * 6.322805404663086
Epoch 480, val loss: 0.8877565860748291
Epoch 490, training loss: 0.6905614137649536 = 0.05644341558218002 + 0.1 * 6.341180324554443
Epoch 490, val loss: 0.8998174071311951
Epoch 500, training loss: 0.6849165558815002 = 0.05296808108687401 + 0.1 * 6.319484710693359
Epoch 500, val loss: 0.911557674407959
Epoch 510, training loss: 0.680611252784729 = 0.04979569837450981 + 0.1 * 6.308155536651611
Epoch 510, val loss: 0.9231316447257996
Epoch 520, training loss: 0.6778125166893005 = 0.04688333719968796 + 0.1 * 6.309291839599609
Epoch 520, val loss: 0.9344831705093384
Epoch 530, training loss: 0.6748914122581482 = 0.04420563951134682 + 0.1 * 6.306857585906982
Epoch 530, val loss: 0.9455791115760803
Epoch 540, training loss: 0.6724505424499512 = 0.04174530878663063 + 0.1 * 6.307052135467529
Epoch 540, val loss: 0.9564917683601379
Epoch 550, training loss: 0.6699326038360596 = 0.039456162601709366 + 0.1 * 6.3047637939453125
Epoch 550, val loss: 0.9671924710273743
Epoch 560, training loss: 0.6666223406791687 = 0.037316109985113144 + 0.1 * 6.293062210083008
Epoch 560, val loss: 0.9776608943939209
Epoch 570, training loss: 0.6647624373435974 = 0.035302240401506424 + 0.1 * 6.294601917266846
Epoch 570, val loss: 0.9879904389381409
Epoch 580, training loss: 0.6624768972396851 = 0.03340106084942818 + 0.1 * 6.29075813293457
Epoch 580, val loss: 0.9980931878089905
Epoch 590, training loss: 0.6602556705474854 = 0.03159737586975098 + 0.1 * 6.286582946777344
Epoch 590, val loss: 1.008083701133728
Epoch 600, training loss: 0.6584711670875549 = 0.029863737523555756 + 0.1 * 6.286074638366699
Epoch 600, val loss: 1.0179096460342407
Epoch 610, training loss: 0.6561565399169922 = 0.028184503316879272 + 0.1 * 6.279720306396484
Epoch 610, val loss: 1.0276302099227905
Epoch 620, training loss: 0.6560474634170532 = 0.026555027812719345 + 0.1 * 6.294924259185791
Epoch 620, val loss: 1.037224531173706
Epoch 630, training loss: 0.6526470184326172 = 0.02497529797255993 + 0.1 * 6.276717185974121
Epoch 630, val loss: 1.046756386756897
Epoch 640, training loss: 0.6517136693000793 = 0.02343989536166191 + 0.1 * 6.282737731933594
Epoch 640, val loss: 1.0562695264816284
Epoch 650, training loss: 0.6496381759643555 = 0.021980835124850273 + 0.1 * 6.276573657989502
Epoch 650, val loss: 1.0656462907791138
Epoch 660, training loss: 0.646978497505188 = 0.02061361074447632 + 0.1 * 6.263648986816406
Epoch 660, val loss: 1.0750765800476074
Epoch 670, training loss: 0.6462931632995605 = 0.019348159432411194 + 0.1 * 6.2694501876831055
Epoch 670, val loss: 1.084437370300293
Epoch 680, training loss: 0.6436792016029358 = 0.018189938738942146 + 0.1 * 6.254892349243164
Epoch 680, val loss: 1.093658447265625
Epoch 690, training loss: 0.6423912644386292 = 0.017151731997728348 + 0.1 * 6.252395153045654
Epoch 690, val loss: 1.102763295173645
Epoch 700, training loss: 0.6430983543395996 = 0.01622278429567814 + 0.1 * 6.2687554359436035
Epoch 700, val loss: 1.11166512966156
Epoch 710, training loss: 0.6404011845588684 = 0.015395162627100945 + 0.1 * 6.250060081481934
Epoch 710, val loss: 1.1203166246414185
Epoch 720, training loss: 0.6392549276351929 = 0.014648977667093277 + 0.1 * 6.246059417724609
Epoch 720, val loss: 1.1288517713546753
Epoch 730, training loss: 0.6390849947929382 = 0.013965871185064316 + 0.1 * 6.251191139221191
Epoch 730, val loss: 1.1371186971664429
Epoch 740, training loss: 0.6380823850631714 = 0.01334067340940237 + 0.1 * 6.247417449951172
Epoch 740, val loss: 1.1451221704483032
Epoch 750, training loss: 0.6366810202598572 = 0.012769060209393501 + 0.1 * 6.239119529724121
Epoch 750, val loss: 1.1530201435089111
Epoch 760, training loss: 0.6356973052024841 = 0.01223911251872778 + 0.1 * 6.23458194732666
Epoch 760, val loss: 1.1607459783554077
Epoch 770, training loss: 0.6373173594474792 = 0.01174518745392561 + 0.1 * 6.255721092224121
Epoch 770, val loss: 1.1682486534118652
Epoch 780, training loss: 0.6350842714309692 = 0.01128720585256815 + 0.1 * 6.237970352172852
Epoch 780, val loss: 1.1755832433700562
Epoch 790, training loss: 0.6339017152786255 = 0.010860143229365349 + 0.1 * 6.2304158210754395
Epoch 790, val loss: 1.182779312133789
Epoch 800, training loss: 0.6338026523590088 = 0.010459406301379204 + 0.1 * 6.233432292938232
Epoch 800, val loss: 1.1898181438446045
Epoch 810, training loss: 0.6330644488334656 = 0.010083729401230812 + 0.1 * 6.229806900024414
Epoch 810, val loss: 1.1966432332992554
Epoch 820, training loss: 0.6321594715118408 = 0.00973101332783699 + 0.1 * 6.224284648895264
Epoch 820, val loss: 1.2033864259719849
Epoch 830, training loss: 0.6322189569473267 = 0.00939815305173397 + 0.1 * 6.228207588195801
Epoch 830, val loss: 1.2099391222000122
Epoch 840, training loss: 0.6315808296203613 = 0.009085406549274921 + 0.1 * 6.224954128265381
Epoch 840, val loss: 1.2163665294647217
Epoch 850, training loss: 0.6308642029762268 = 0.008789675310254097 + 0.1 * 6.220745086669922
Epoch 850, val loss: 1.2227262258529663
Epoch 860, training loss: 0.6297537684440613 = 0.008509145118296146 + 0.1 * 6.212446212768555
Epoch 860, val loss: 1.2288535833358765
Epoch 870, training loss: 0.6294443607330322 = 0.008244308643043041 + 0.1 * 6.211999893188477
Epoch 870, val loss: 1.2349810600280762
Epoch 880, training loss: 0.6304159164428711 = 0.007992587983608246 + 0.1 * 6.224233150482178
Epoch 880, val loss: 1.240913987159729
Epoch 890, training loss: 0.6300535202026367 = 0.007754592224955559 + 0.1 * 6.222989559173584
Epoch 890, val loss: 1.2467625141143799
Epoch 900, training loss: 0.6288245916366577 = 0.0075282142497599125 + 0.1 * 6.212964057922363
Epoch 900, val loss: 1.2524361610412598
Epoch 910, training loss: 0.6279186606407166 = 0.007313855458050966 + 0.1 * 6.206048011779785
Epoch 910, val loss: 1.258077621459961
Epoch 920, training loss: 0.6278363466262817 = 0.007108889054507017 + 0.1 * 6.207274436950684
Epoch 920, val loss: 1.2635936737060547
Epoch 930, training loss: 0.6292563676834106 = 0.0069134421646595 + 0.1 * 6.223429203033447
Epoch 930, val loss: 1.2689108848571777
Epoch 940, training loss: 0.6271730065345764 = 0.006727330852299929 + 0.1 * 6.204456329345703
Epoch 940, val loss: 1.274173617362976
Epoch 950, training loss: 0.6268625259399414 = 0.006550428923219442 + 0.1 * 6.203120708465576
Epoch 950, val loss: 1.2794290781021118
Epoch 960, training loss: 0.6266542673110962 = 0.006380441132932901 + 0.1 * 6.202738285064697
Epoch 960, val loss: 1.2844693660736084
Epoch 970, training loss: 0.6260344386100769 = 0.00621827132999897 + 0.1 * 6.198161602020264
Epoch 970, val loss: 1.2894750833511353
Epoch 980, training loss: 0.6254922151565552 = 0.006063109263777733 + 0.1 * 6.194291114807129
Epoch 980, val loss: 1.2944018840789795
Epoch 990, training loss: 0.6254850029945374 = 0.00591434258967638 + 0.1 * 6.195706367492676
Epoch 990, val loss: 1.2991517782211304
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.5978
Flip ASR: 0.5244/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.784146785736084 = 1.9467567205429077 + 0.1 * 8.373900413513184
Epoch 0, val loss: 1.9488641023635864
Epoch 10, training loss: 2.774461507797241 = 1.9370783567428589 + 0.1 * 8.373831748962402
Epoch 10, val loss: 1.9397106170654297
Epoch 20, training loss: 2.762763261795044 = 1.9254101514816284 + 0.1 * 8.373530387878418
Epoch 20, val loss: 1.9282617568969727
Epoch 30, training loss: 2.7464358806610107 = 1.9092440605163574 + 0.1 * 8.371918678283691
Epoch 30, val loss: 1.9120886325836182
Epoch 40, training loss: 2.721194267272949 = 1.8853685855865479 + 0.1 * 8.358255386352539
Epoch 40, val loss: 1.888378381729126
Epoch 50, training loss: 2.6718671321868896 = 1.8514376878738403 + 0.1 * 8.204294204711914
Epoch 50, val loss: 1.8558640480041504
Epoch 60, training loss: 2.571214199066162 = 1.8143768310546875 + 0.1 * 7.56837272644043
Epoch 60, val loss: 1.822455644607544
Epoch 70, training loss: 2.50823974609375 = 1.7806529998779297 + 0.1 * 7.275867462158203
Epoch 70, val loss: 1.7930718660354614
Epoch 80, training loss: 2.447730302810669 = 1.7481780052185059 + 0.1 * 6.995522499084473
Epoch 80, val loss: 1.7656042575836182
Epoch 90, training loss: 2.390139579772949 = 1.709064245223999 + 0.1 * 6.8107523918151855
Epoch 90, val loss: 1.7303473949432373
Epoch 100, training loss: 2.331123113632202 = 1.6561485528945923 + 0.1 * 6.749746322631836
Epoch 100, val loss: 1.6832103729248047
Epoch 110, training loss: 2.258225917816162 = 1.586509346961975 + 0.1 * 6.717166423797607
Epoch 110, val loss: 1.6236120462417603
Epoch 120, training loss: 2.1715526580810547 = 1.5018545389175415 + 0.1 * 6.6969804763793945
Epoch 120, val loss: 1.5526762008666992
Epoch 130, training loss: 2.0774760246276855 = 1.4094047546386719 + 0.1 * 6.680711269378662
Epoch 130, val loss: 1.4761345386505127
Epoch 140, training loss: 1.9822793006896973 = 1.3162117004394531 + 0.1 * 6.660676002502441
Epoch 140, val loss: 1.4016108512878418
Epoch 150, training loss: 1.8903207778930664 = 1.226474404335022 + 0.1 * 6.638463497161865
Epoch 150, val loss: 1.332006812095642
Epoch 160, training loss: 1.804868459701538 = 1.1437511444091797 + 0.1 * 6.611173152923584
Epoch 160, val loss: 1.2701932191848755
Epoch 170, training loss: 1.725372314453125 = 1.0667682886123657 + 0.1 * 6.586040019989014
Epoch 170, val loss: 1.213984727859497
Epoch 180, training loss: 1.6515001058578491 = 0.9943008422851562 + 0.1 * 6.57199239730835
Epoch 180, val loss: 1.1617368459701538
Epoch 190, training loss: 1.5807123184204102 = 0.9261002540588379 + 0.1 * 6.546121120452881
Epoch 190, val loss: 1.1124883890151978
Epoch 200, training loss: 1.5146279335021973 = 0.8603346347808838 + 0.1 * 6.542933464050293
Epoch 200, val loss: 1.0644760131835938
Epoch 210, training loss: 1.4487879276275635 = 0.7969112396240234 + 0.1 * 6.5187668800354
Epoch 210, val loss: 1.0172853469848633
Epoch 220, training loss: 1.384962797164917 = 0.7345585823059082 + 0.1 * 6.504042148590088
Epoch 220, val loss: 0.9698308706283569
Epoch 230, training loss: 1.3238496780395508 = 0.6740697622299194 + 0.1 * 6.497799873352051
Epoch 230, val loss: 0.9236814379692078
Epoch 240, training loss: 1.2649496793746948 = 0.6168385744094849 + 0.1 * 6.4811110496521
Epoch 240, val loss: 0.8804056644439697
Epoch 250, training loss: 1.210647702217102 = 0.5639553666114807 + 0.1 * 6.466923236846924
Epoch 250, val loss: 0.8416635990142822
Epoch 260, training loss: 1.1609125137329102 = 0.5158740878105164 + 0.1 * 6.450384140014648
Epoch 260, val loss: 0.8086407780647278
Epoch 270, training loss: 1.1168742179870605 = 0.4720921814441681 + 0.1 * 6.447819709777832
Epoch 270, val loss: 0.7814059257507324
Epoch 280, training loss: 1.0750786066055298 = 0.4319209158420563 + 0.1 * 6.431577205657959
Epoch 280, val loss: 0.7596568465232849
Epoch 290, training loss: 1.03635835647583 = 0.394317626953125 + 0.1 * 6.420407295227051
Epoch 290, val loss: 0.7423079609870911
Epoch 300, training loss: 0.9995468258857727 = 0.3585844039916992 + 0.1 * 6.409624099731445
Epoch 300, val loss: 0.7284970879554749
Epoch 310, training loss: 0.9653429388999939 = 0.32481884956359863 + 0.1 * 6.405240535736084
Epoch 310, val loss: 0.7178763747215271
Epoch 320, training loss: 0.932224690914154 = 0.2934175133705139 + 0.1 * 6.388071537017822
Epoch 320, val loss: 0.710102379322052
Epoch 330, training loss: 0.9028787612915039 = 0.26471951603889465 + 0.1 * 6.381592750549316
Epoch 330, val loss: 0.7048603296279907
Epoch 340, training loss: 0.8766762614250183 = 0.23898302018642426 + 0.1 * 6.376932144165039
Epoch 340, val loss: 0.7019153237342834
Epoch 350, training loss: 0.8539260029792786 = 0.21582446992397308 + 0.1 * 6.381014823913574
Epoch 350, val loss: 0.7007229328155518
Epoch 360, training loss: 0.8304539918899536 = 0.19510644674301147 + 0.1 * 6.353475570678711
Epoch 360, val loss: 0.7011832594871521
Epoch 370, training loss: 0.8127384781837463 = 0.1765240877866745 + 0.1 * 6.362143516540527
Epoch 370, val loss: 0.7029117941856384
Epoch 380, training loss: 0.7938666939735413 = 0.160017192363739 + 0.1 * 6.338494777679443
Epoch 380, val loss: 0.7058578729629517
Epoch 390, training loss: 0.7787442207336426 = 0.14531609416007996 + 0.1 * 6.3342814445495605
Epoch 390, val loss: 0.7098805904388428
Epoch 400, training loss: 0.7650337219238281 = 0.13217265903949738 + 0.1 * 6.328610420227051
Epoch 400, val loss: 0.7148340344429016
Epoch 410, training loss: 0.7527865171432495 = 0.12048906832933426 + 0.1 * 6.32297420501709
Epoch 410, val loss: 0.7206152081489563
Epoch 420, training loss: 0.7417011260986328 = 0.11011641472578049 + 0.1 * 6.315846920013428
Epoch 420, val loss: 0.7270921468734741
Epoch 430, training loss: 0.7332605719566345 = 0.10087791830301285 + 0.1 * 6.323826313018799
Epoch 430, val loss: 0.7340354919433594
Epoch 440, training loss: 0.7229179739952087 = 0.09263894706964493 + 0.1 * 6.302789688110352
Epoch 440, val loss: 0.7415183782577515
Epoch 450, training loss: 0.7160398960113525 = 0.08525113761425018 + 0.1 * 6.307887077331543
Epoch 450, val loss: 0.749329686164856
Epoch 460, training loss: 0.707732617855072 = 0.078614242374897 + 0.1 * 6.291183948516846
Epoch 460, val loss: 0.757404625415802
Epoch 470, training loss: 0.7022926807403564 = 0.07262454181909561 + 0.1 * 6.296680927276611
Epoch 470, val loss: 0.7655695676803589
Epoch 480, training loss: 0.695829451084137 = 0.06722046434879303 + 0.1 * 6.286089897155762
Epoch 480, val loss: 0.7739343047142029
Epoch 490, training loss: 0.6906480193138123 = 0.062325168401002884 + 0.1 * 6.283228397369385
Epoch 490, val loss: 0.782392680644989
Epoch 500, training loss: 0.6873782873153687 = 0.057877734303474426 + 0.1 * 6.2950053215026855
Epoch 500, val loss: 0.7907978296279907
Epoch 510, training loss: 0.6811684370040894 = 0.05383286625146866 + 0.1 * 6.273355484008789
Epoch 510, val loss: 0.7993630170822144
Epoch 520, training loss: 0.6766775846481323 = 0.05014593154191971 + 0.1 * 6.265316486358643
Epoch 520, val loss: 0.8078316450119019
Epoch 530, training loss: 0.6743418574333191 = 0.046779174357652664 + 0.1 * 6.2756266593933105
Epoch 530, val loss: 0.8163552284240723
Epoch 540, training loss: 0.6711685657501221 = 0.0437106192111969 + 0.1 * 6.2745795249938965
Epoch 540, val loss: 0.8247696757316589
Epoch 550, training loss: 0.6668912172317505 = 0.0409218966960907 + 0.1 * 6.259693145751953
Epoch 550, val loss: 0.833084225654602
Epoch 560, training loss: 0.6642330288887024 = 0.038374319672584534 + 0.1 * 6.25858736038208
Epoch 560, val loss: 0.8414545059204102
Epoch 570, training loss: 0.6608608961105347 = 0.03603668510913849 + 0.1 * 6.248242378234863
Epoch 570, val loss: 0.8496053218841553
Epoch 580, training loss: 0.6594767570495605 = 0.03388958424329758 + 0.1 * 6.255871772766113
Epoch 580, val loss: 0.8577516078948975
Epoch 590, training loss: 0.6560786366462708 = 0.03192100301384926 + 0.1 * 6.241576194763184
Epoch 590, val loss: 0.8658435344696045
Epoch 600, training loss: 0.6543281078338623 = 0.030109254643321037 + 0.1 * 6.242188930511475
Epoch 600, val loss: 0.8738511204719543
Epoch 610, training loss: 0.6547876000404358 = 0.028440136462450027 + 0.1 * 6.263474941253662
Epoch 610, val loss: 0.8816654086112976
Epoch 620, training loss: 0.6508687138557434 = 0.026907892897725105 + 0.1 * 6.239607810974121
Epoch 620, val loss: 0.8894050717353821
Epoch 630, training loss: 0.6486449241638184 = 0.025493213906884193 + 0.1 * 6.231517314910889
Epoch 630, val loss: 0.8971554636955261
Epoch 640, training loss: 0.6474929451942444 = 0.024183006957173347 + 0.1 * 6.233098983764648
Epoch 640, val loss: 0.9046982526779175
Epoch 650, training loss: 0.6453983187675476 = 0.022970104590058327 + 0.1 * 6.224282264709473
Epoch 650, val loss: 0.9120674133300781
Epoch 660, training loss: 0.645390510559082 = 0.021845413371920586 + 0.1 * 6.2354512214660645
Epoch 660, val loss: 0.9193208813667297
Epoch 670, training loss: 0.6425508260726929 = 0.02080361545085907 + 0.1 * 6.217472076416016
Epoch 670, val loss: 0.9265580773353577
Epoch 680, training loss: 0.642503023147583 = 0.019834602251648903 + 0.1 * 6.2266845703125
Epoch 680, val loss: 0.9336922764778137
Epoch 690, training loss: 0.6406409740447998 = 0.018930744379758835 + 0.1 * 6.21710205078125
Epoch 690, val loss: 0.9405524134635925
Epoch 700, training loss: 0.6394987106323242 = 0.018089428544044495 + 0.1 * 6.21409273147583
Epoch 700, val loss: 0.947391152381897
Epoch 710, training loss: 0.6394855976104736 = 0.017304332926869392 + 0.1 * 6.2218122482299805
Epoch 710, val loss: 0.9541031718254089
Epoch 720, training loss: 0.6374152898788452 = 0.016570985317230225 + 0.1 * 6.208442687988281
Epoch 720, val loss: 0.960675060749054
Epoch 730, training loss: 0.6362805962562561 = 0.0158835519105196 + 0.1 * 6.203970432281494
Epoch 730, val loss: 0.9671474695205688
Epoch 740, training loss: 0.6377584934234619 = 0.015238003805279732 + 0.1 * 6.225204944610596
Epoch 740, val loss: 0.9734312891960144
Epoch 750, training loss: 0.6355735063552856 = 0.014633124694228172 + 0.1 * 6.209403991699219
Epoch 750, val loss: 0.9794827103614807
Epoch 760, training loss: 0.6342037320137024 = 0.014066740870475769 + 0.1 * 6.201369762420654
Epoch 760, val loss: 0.9857091903686523
Epoch 770, training loss: 0.6331940293312073 = 0.01353229209780693 + 0.1 * 6.196617126464844
Epoch 770, val loss: 0.9916976094245911
Epoch 780, training loss: 0.633075475692749 = 0.013028613291680813 + 0.1 * 6.20046854019165
Epoch 780, val loss: 0.9975182414054871
Epoch 790, training loss: 0.6320341229438782 = 0.012553553096950054 + 0.1 * 6.19480562210083
Epoch 790, val loss: 1.0032522678375244
Epoch 800, training loss: 0.6318581104278564 = 0.012105037458240986 + 0.1 * 6.197530269622803
Epoch 800, val loss: 1.0089161396026611
Epoch 810, training loss: 0.6306130290031433 = 0.011681915260851383 + 0.1 * 6.1893110275268555
Epoch 810, val loss: 1.014313817024231
Epoch 820, training loss: 0.6299402713775635 = 0.011281510815024376 + 0.1 * 6.186587810516357
Epoch 820, val loss: 1.0198249816894531
Epoch 830, training loss: 0.6303296089172363 = 0.010901917703449726 + 0.1 * 6.194276809692383
Epoch 830, val loss: 1.0251164436340332
Epoch 840, training loss: 0.6292794346809387 = 0.01054162997752428 + 0.1 * 6.1873779296875
Epoch 840, val loss: 1.0301845073699951
Epoch 850, training loss: 0.629061758518219 = 0.010200994089245796 + 0.1 * 6.188607692718506
Epoch 850, val loss: 1.0352739095687866
Epoch 860, training loss: 0.6280336380004883 = 0.009878049604594707 + 0.1 * 6.18155574798584
Epoch 860, val loss: 1.0403598546981812
Epoch 870, training loss: 0.627774715423584 = 0.009571188129484653 + 0.1 * 6.182034969329834
Epoch 870, val loss: 1.0452256202697754
Epoch 880, training loss: 0.6273912191390991 = 0.009279127232730389 + 0.1 * 6.181120872497559
Epoch 880, val loss: 1.050094723701477
Epoch 890, training loss: 0.6265540719032288 = 0.009001133032143116 + 0.1 * 6.175529479980469
Epoch 890, val loss: 1.05474054813385
Epoch 900, training loss: 0.6260321140289307 = 0.008736439980566502 + 0.1 * 6.172956466674805
Epoch 900, val loss: 1.0594372749328613
Epoch 910, training loss: 0.6254509091377258 = 0.008483782410621643 + 0.1 * 6.169671535491943
Epoch 910, val loss: 1.0640676021575928
Epoch 920, training loss: 0.6266189217567444 = 0.00824277475476265 + 0.1 * 6.183761119842529
Epoch 920, val loss: 1.0684906244277954
Epoch 930, training loss: 0.6254937648773193 = 0.008013187907636166 + 0.1 * 6.174805641174316
Epoch 930, val loss: 1.0728884935379028
Epoch 940, training loss: 0.6254919767379761 = 0.007794402074068785 + 0.1 * 6.176975727081299
Epoch 940, val loss: 1.0772219896316528
Epoch 950, training loss: 0.6244719624519348 = 0.00758554320782423 + 0.1 * 6.1688642501831055
Epoch 950, val loss: 1.0815284252166748
Epoch 960, training loss: 0.6237408518791199 = 0.007385712582617998 + 0.1 * 6.163550853729248
Epoch 960, val loss: 1.0857629776000977
Epoch 970, training loss: 0.6235736608505249 = 0.007193830329924822 + 0.1 * 6.1637983322143555
Epoch 970, val loss: 1.0898762941360474
Epoch 980, training loss: 0.6238493323326111 = 0.007009858265519142 + 0.1 * 6.168394565582275
Epoch 980, val loss: 1.093738079071045
Epoch 990, training loss: 0.6237367987632751 = 0.006834019441157579 + 0.1 * 6.169027328491211
Epoch 990, val loss: 1.0976660251617432
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7528
Flip ASR: 0.7156/225 nodes
The final ASR:0.61255, 0.10897, Accuracy:0.80494, 0.01772
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11580])
remove edge: torch.Size([2, 9414])
updated graph: torch.Size([2, 10438])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83457, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.780121326446533 = 1.942737340927124 + 0.1 * 8.373838424682617
Epoch 0, val loss: 1.9432402849197388
Epoch 10, training loss: 2.771043539047241 = 1.9336771965026855 + 0.1 * 8.373663902282715
Epoch 10, val loss: 1.934301733970642
Epoch 20, training loss: 2.759878635406494 = 1.9226161241531372 + 0.1 * 8.372624397277832
Epoch 20, val loss: 1.923274040222168
Epoch 30, training loss: 2.7434864044189453 = 1.9069448709487915 + 0.1 * 8.365415573120117
Epoch 30, val loss: 1.9076192378997803
Epoch 40, training loss: 2.716111898422241 = 1.883435606956482 + 0.1 * 8.326763153076172
Epoch 40, val loss: 1.884405255317688
Epoch 50, training loss: 2.662297248840332 = 1.8504921197891235 + 0.1 * 8.118050575256348
Epoch 50, val loss: 1.8532058000564575
Epoch 60, training loss: 2.584939956665039 = 1.8108494281768799 + 0.1 * 7.740903854370117
Epoch 60, val loss: 1.8175630569458008
Epoch 70, training loss: 2.5028138160705566 = 1.7700939178466797 + 0.1 * 7.327198505401611
Epoch 70, val loss: 1.781941533088684
Epoch 80, training loss: 2.43135404586792 = 1.7281419038772583 + 0.1 * 7.032121181488037
Epoch 80, val loss: 1.7456130981445312
Epoch 90, training loss: 2.3684334754943848 = 1.6764601469039917 + 0.1 * 6.919732570648193
Epoch 90, val loss: 1.6987905502319336
Epoch 100, training loss: 2.2941815853118896 = 1.6079102754592896 + 0.1 * 6.86271333694458
Epoch 100, val loss: 1.6370487213134766
Epoch 110, training loss: 2.2025954723358154 = 1.5206974744796753 + 0.1 * 6.8189802169799805
Epoch 110, val loss: 1.5621484518051147
Epoch 120, training loss: 2.096327066421509 = 1.417984127998352 + 0.1 * 6.7834296226501465
Epoch 120, val loss: 1.4752062559127808
Epoch 130, training loss: 1.9811543226242065 = 1.3055447340011597 + 0.1 * 6.756095886230469
Epoch 130, val loss: 1.3804954290390015
Epoch 140, training loss: 1.8629405498504639 = 1.1896601915359497 + 0.1 * 6.7328033447265625
Epoch 140, val loss: 1.2833422422409058
Epoch 150, training loss: 1.746671438217163 = 1.0755947828292847 + 0.1 * 6.710766315460205
Epoch 150, val loss: 1.1880520582199097
Epoch 160, training loss: 1.6396594047546387 = 0.9697291254997253 + 0.1 * 6.6993021965026855
Epoch 160, val loss: 1.1004912853240967
Epoch 170, training loss: 1.5438697338104248 = 0.8756563663482666 + 0.1 * 6.68213415145874
Epoch 170, val loss: 1.0236618518829346
Epoch 180, training loss: 1.459445834159851 = 0.7923365831375122 + 0.1 * 6.671092510223389
Epoch 180, val loss: 0.9574997425079346
Epoch 190, training loss: 1.3858647346496582 = 0.7197003960609436 + 0.1 * 6.661643028259277
Epoch 190, val loss: 0.9021650552749634
Epoch 200, training loss: 1.3208256959915161 = 0.6558924317359924 + 0.1 * 6.649332523345947
Epoch 200, val loss: 0.8565706014633179
Epoch 210, training loss: 1.2630760669708252 = 0.599102258682251 + 0.1 * 6.639737129211426
Epoch 210, val loss: 0.8192451000213623
Epoch 220, training loss: 1.21140456199646 = 0.5484704971313477 + 0.1 * 6.629339694976807
Epoch 220, val loss: 0.7892142534255981
Epoch 230, training loss: 1.164551019668579 = 0.5027051568031311 + 0.1 * 6.618457794189453
Epoch 230, val loss: 0.7650362849235535
Epoch 240, training loss: 1.1214165687561035 = 0.46081510186195374 + 0.1 * 6.606014251708984
Epoch 240, val loss: 0.7450703978538513
Epoch 250, training loss: 1.0820647478103638 = 0.4216252863407135 + 0.1 * 6.604394435882568
Epoch 250, val loss: 0.7280452847480774
Epoch 260, training loss: 1.0425074100494385 = 0.384358674287796 + 0.1 * 6.581486701965332
Epoch 260, val loss: 0.7131011486053467
Epoch 270, training loss: 1.0052746534347534 = 0.3482024371623993 + 0.1 * 6.570721626281738
Epoch 270, val loss: 0.6995314359664917
Epoch 280, training loss: 0.969597578048706 = 0.3133139908313751 + 0.1 * 6.562836170196533
Epoch 280, val loss: 0.6874514818191528
Epoch 290, training loss: 0.9356295466423035 = 0.28023505210876465 + 0.1 * 6.5539445877075195
Epoch 290, val loss: 0.6774340271949768
Epoch 300, training loss: 0.9045535326004028 = 0.2496751844882965 + 0.1 * 6.548782825469971
Epoch 300, val loss: 0.6696479916572571
Epoch 310, training loss: 0.8749134540557861 = 0.22205094993114471 + 0.1 * 6.528625011444092
Epoch 310, val loss: 0.6646112203598022
Epoch 320, training loss: 0.8512940406799316 = 0.19740653038024902 + 0.1 * 6.538875102996826
Epoch 320, val loss: 0.6621055603027344
Epoch 330, training loss: 0.8266409635543823 = 0.17579665780067444 + 0.1 * 6.508442401885986
Epoch 330, val loss: 0.6622262001037598
Epoch 340, training loss: 0.8084194660186768 = 0.15691113471984863 + 0.1 * 6.515083312988281
Epoch 340, val loss: 0.6646589040756226
Epoch 350, training loss: 0.7897188663482666 = 0.1404939591884613 + 0.1 * 6.49224853515625
Epoch 350, val loss: 0.6692786812782288
Epoch 360, training loss: 0.7751600742340088 = 0.1261739581823349 + 0.1 * 6.489861011505127
Epoch 360, val loss: 0.675672173500061
Epoch 370, training loss: 0.7610780000686646 = 0.11370126157999039 + 0.1 * 6.473767280578613
Epoch 370, val loss: 0.6837483644485474
Epoch 380, training loss: 0.7492016553878784 = 0.10278309881687164 + 0.1 * 6.46418571472168
Epoch 380, val loss: 0.6930120587348938
Epoch 390, training loss: 0.7394014596939087 = 0.0932110846042633 + 0.1 * 6.461904048919678
Epoch 390, val loss: 0.7032711505889893
Epoch 400, training loss: 0.7294008731842041 = 0.08479830622673035 + 0.1 * 6.446025371551514
Epoch 400, val loss: 0.7142580151557922
Epoch 410, training loss: 0.7228674292564392 = 0.07736682891845703 + 0.1 * 6.455005645751953
Epoch 410, val loss: 0.72567218542099
Epoch 420, training loss: 0.7151711583137512 = 0.07080663740634918 + 0.1 * 6.443645000457764
Epoch 420, val loss: 0.7374428510665894
Epoch 430, training loss: 0.7089126110076904 = 0.06497129797935486 + 0.1 * 6.439413070678711
Epoch 430, val loss: 0.7493277788162231
Epoch 440, training loss: 0.7026240229606628 = 0.059782300144433975 + 0.1 * 6.428416728973389
Epoch 440, val loss: 0.7612457275390625
Epoch 450, training loss: 0.6966274976730347 = 0.05514013022184372 + 0.1 * 6.4148736000061035
Epoch 450, val loss: 0.7732794880867004
Epoch 460, training loss: 0.6926519870758057 = 0.05097944289445877 + 0.1 * 6.4167256355285645
Epoch 460, val loss: 0.7852910757064819
Epoch 470, training loss: 0.6869570016860962 = 0.047248732298612595 + 0.1 * 6.397082805633545
Epoch 470, val loss: 0.7971510291099548
Epoch 480, training loss: 0.6841453909873962 = 0.04388544708490372 + 0.1 * 6.402599334716797
Epoch 480, val loss: 0.8089264631271362
Epoch 490, training loss: 0.6800435185432434 = 0.04085693880915642 + 0.1 * 6.391865253448486
Epoch 490, val loss: 0.8205602765083313
Epoch 500, training loss: 0.6765984892845154 = 0.03812025114893913 + 0.1 * 6.384781837463379
Epoch 500, val loss: 0.8319697380065918
Epoch 510, training loss: 0.6746014356613159 = 0.03563626855611801 + 0.1 * 6.389651775360107
Epoch 510, val loss: 0.8432653546333313
Epoch 520, training loss: 0.6706497669219971 = 0.03337859734892845 + 0.1 * 6.372711658477783
Epoch 520, val loss: 0.8543340563774109
Epoch 530, training loss: 0.6687056422233582 = 0.031324516981840134 + 0.1 * 6.3738112449646
Epoch 530, val loss: 0.8651717901229858
Epoch 540, training loss: 0.6655179262161255 = 0.029453197494149208 + 0.1 * 6.360647201538086
Epoch 540, val loss: 0.8757879137992859
Epoch 550, training loss: 0.6626921892166138 = 0.027743201702833176 + 0.1 * 6.349489688873291
Epoch 550, val loss: 0.8861067295074463
Epoch 560, training loss: 0.6608695387840271 = 0.02617722749710083 + 0.1 * 6.346922874450684
Epoch 560, val loss: 0.896323025226593
Epoch 570, training loss: 0.6591498255729675 = 0.024736614897847176 + 0.1 * 6.344131946563721
Epoch 570, val loss: 0.9062515497207642
Epoch 580, training loss: 0.6561622023582458 = 0.023410776630043983 + 0.1 * 6.327513694763184
Epoch 580, val loss: 0.9160476326942444
Epoch 590, training loss: 0.6568239331245422 = 0.022190187126398087 + 0.1 * 6.346336841583252
Epoch 590, val loss: 0.9255246520042419
Epoch 600, training loss: 0.6536945104598999 = 0.021069122478365898 + 0.1 * 6.326253890991211
Epoch 600, val loss: 0.9347931742668152
Epoch 610, training loss: 0.651742696762085 = 0.02003154158592224 + 0.1 * 6.317111015319824
Epoch 610, val loss: 0.9439203143119812
Epoch 620, training loss: 0.6515140533447266 = 0.019067618995904922 + 0.1 * 6.324463844299316
Epoch 620, val loss: 0.9528656005859375
Epoch 630, training loss: 0.649039089679718 = 0.01817309483885765 + 0.1 * 6.30866003036499
Epoch 630, val loss: 0.9616258144378662
Epoch 640, training loss: 0.6483597755432129 = 0.017340458929538727 + 0.1 * 6.310192584991455
Epoch 640, val loss: 0.970237672328949
Epoch 650, training loss: 0.6481620073318481 = 0.016565954312682152 + 0.1 * 6.315959930419922
Epoch 650, val loss: 0.9786412119865417
Epoch 660, training loss: 0.6460924744606018 = 0.015845419839024544 + 0.1 * 6.3024702072143555
Epoch 660, val loss: 0.986897349357605
Epoch 670, training loss: 0.6447818279266357 = 0.015171374194324017 + 0.1 * 6.296104907989502
Epoch 670, val loss: 0.994964063167572
Epoch 680, training loss: 0.6430960297584534 = 0.014541435055434704 + 0.1 * 6.285545825958252
Epoch 680, val loss: 1.002839207649231
Epoch 690, training loss: 0.6418349742889404 = 0.013952139765024185 + 0.1 * 6.278828144073486
Epoch 690, val loss: 1.0105787515640259
Epoch 700, training loss: 0.6414035558700562 = 0.013398898765444756 + 0.1 * 6.280046463012695
Epoch 700, val loss: 1.018129587173462
Epoch 710, training loss: 0.6418668627738953 = 0.012881094589829445 + 0.1 * 6.289857387542725
Epoch 710, val loss: 1.0255956649780273
Epoch 720, training loss: 0.640053927898407 = 0.0123941320925951 + 0.1 * 6.27659797668457
Epoch 720, val loss: 1.032861590385437
Epoch 730, training loss: 0.6389586925506592 = 0.011936170980334282 + 0.1 * 6.2702250480651855
Epoch 730, val loss: 1.0399943590164185
Epoch 740, training loss: 0.6381961703300476 = 0.011503848247230053 + 0.1 * 6.266922950744629
Epoch 740, val loss: 1.0470290184020996
Epoch 750, training loss: 0.6377872824668884 = 0.011096244677901268 + 0.1 * 6.266910552978516
Epoch 750, val loss: 1.053923487663269
Epoch 760, training loss: 0.6381732225418091 = 0.010710567235946655 + 0.1 * 6.2746262550354
Epoch 760, val loss: 1.060726523399353
Epoch 770, training loss: 0.6363686919212341 = 0.010346763767302036 + 0.1 * 6.260219097137451
Epoch 770, val loss: 1.0673805475234985
Epoch 780, training loss: 0.6353919506072998 = 0.010002425871789455 + 0.1 * 6.253895282745361
Epoch 780, val loss: 1.0739449262619019
Epoch 790, training loss: 0.636266827583313 = 0.009675703942775726 + 0.1 * 6.26591157913208
Epoch 790, val loss: 1.0804375410079956
Epoch 800, training loss: 0.6343045234680176 = 0.009366649203002453 + 0.1 * 6.249378681182861
Epoch 800, val loss: 1.0867406129837036
Epoch 810, training loss: 0.6335203051567078 = 0.009073067456483841 + 0.1 * 6.244472026824951
Epoch 810, val loss: 1.092999815940857
Epoch 820, training loss: 0.6355845332145691 = 0.008794070221483707 + 0.1 * 6.267904281616211
Epoch 820, val loss: 1.0992132425308228
Epoch 830, training loss: 0.6331704258918762 = 0.008528856560587883 + 0.1 * 6.246416091918945
Epoch 830, val loss: 1.105202317237854
Epoch 840, training loss: 0.6347168684005737 = 0.008277044631540775 + 0.1 * 6.264398097991943
Epoch 840, val loss: 1.1111141443252563
Epoch 850, training loss: 0.6324220299720764 = 0.008036723360419273 + 0.1 * 6.243852615356445
Epoch 850, val loss: 1.1169748306274414
Epoch 860, training loss: 0.6316691637039185 = 0.007808534428477287 + 0.1 * 6.2386064529418945
Epoch 860, val loss: 1.1227707862854004
Epoch 870, training loss: 0.6305245161056519 = 0.00759007828310132 + 0.1 * 6.229344367980957
Epoch 870, val loss: 1.128413438796997
Epoch 880, training loss: 0.6302651762962341 = 0.007382073439657688 + 0.1 * 6.228830814361572
Epoch 880, val loss: 1.1339890956878662
Epoch 890, training loss: 0.6314024329185486 = 0.007182963192462921 + 0.1 * 6.242194652557373
Epoch 890, val loss: 1.1395524740219116
Epoch 900, training loss: 0.6293735504150391 = 0.006992620881646872 + 0.1 * 6.223808765411377
Epoch 900, val loss: 1.145000696182251
Epoch 910, training loss: 0.6310546398162842 = 0.0068104756064713 + 0.1 * 6.242441654205322
Epoch 910, val loss: 1.1503485441207886
Epoch 920, training loss: 0.6296312212944031 = 0.006636139936745167 + 0.1 * 6.229950428009033
Epoch 920, val loss: 1.1556123495101929
Epoch 930, training loss: 0.628710925579071 = 0.006469192449003458 + 0.1 * 6.22241735458374
Epoch 930, val loss: 1.1608115434646606
Epoch 940, training loss: 0.6286795139312744 = 0.00630949018523097 + 0.1 * 6.223700046539307
Epoch 940, val loss: 1.1659884452819824
Epoch 950, training loss: 0.6274564862251282 = 0.006155725568532944 + 0.1 * 6.21300745010376
Epoch 950, val loss: 1.1710377931594849
Epoch 960, training loss: 0.6298401355743408 = 0.006008575204759836 + 0.1 * 6.238315582275391
Epoch 960, val loss: 1.1760585308074951
Epoch 970, training loss: 0.6269497871398926 = 0.005867125932127237 + 0.1 * 6.210826396942139
Epoch 970, val loss: 1.180915355682373
Epoch 980, training loss: 0.6275908350944519 = 0.005731800105422735 + 0.1 * 6.218590259552002
Epoch 980, val loss: 1.1858655214309692
Epoch 990, training loss: 0.6261947751045227 = 0.005601061973720789 + 0.1 * 6.205936908721924
Epoch 990, val loss: 1.1906163692474365
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.4686
Flip ASR: 0.3778/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7797975540161133 = 1.9424070119857788 + 0.1 * 8.373905181884766
Epoch 0, val loss: 1.9410723447799683
Epoch 10, training loss: 2.76932430267334 = 1.9319415092468262 + 0.1 * 8.373827934265137
Epoch 10, val loss: 1.9297605752944946
Epoch 20, training loss: 2.756439685821533 = 1.919089913368225 + 0.1 * 8.373498916625977
Epoch 20, val loss: 1.9154653549194336
Epoch 30, training loss: 2.738302707672119 = 1.901178002357483 + 0.1 * 8.371246337890625
Epoch 30, val loss: 1.8953417539596558
Epoch 40, training loss: 2.7101566791534424 = 1.8748489618301392 + 0.1 * 8.353076934814453
Epoch 40, val loss: 1.8660157918930054
Epoch 50, training loss: 2.662013053894043 = 1.838535189628601 + 0.1 * 8.234779357910156
Epoch 50, val loss: 1.8273464441299438
Epoch 60, training loss: 2.588352680206299 = 1.7981390953063965 + 0.1 * 7.902134418487549
Epoch 60, val loss: 1.7877224683761597
Epoch 70, training loss: 2.5084214210510254 = 1.7596666812896729 + 0.1 * 7.487546443939209
Epoch 70, val loss: 1.751961588859558
Epoch 80, training loss: 2.4307260513305664 = 1.7185249328613281 + 0.1 * 7.122010231018066
Epoch 80, val loss: 1.715111494064331
Epoch 90, training loss: 2.359344005584717 = 1.6658809185028076 + 0.1 * 6.934630393981934
Epoch 90, val loss: 1.6693291664123535
Epoch 100, training loss: 2.2787609100341797 = 1.5963088274002075 + 0.1 * 6.824521064758301
Epoch 100, val loss: 1.609806776046753
Epoch 110, training loss: 2.189429521560669 = 1.5136429071426392 + 0.1 * 6.7578654289245605
Epoch 110, val loss: 1.5415722131729126
Epoch 120, training loss: 2.0990514755249023 = 1.427492380142212 + 0.1 * 6.7155914306640625
Epoch 120, val loss: 1.4744770526885986
Epoch 130, training loss: 2.0129168033599854 = 1.3442444801330566 + 0.1 * 6.686723232269287
Epoch 130, val loss: 1.4144365787506104
Epoch 140, training loss: 1.9313760995864868 = 1.265059471130371 + 0.1 * 6.663166046142578
Epoch 140, val loss: 1.362013339996338
Epoch 150, training loss: 1.8564345836639404 = 1.1916112899780273 + 0.1 * 6.648232460021973
Epoch 150, val loss: 1.3163901567459106
Epoch 160, training loss: 1.7858474254608154 = 1.1224603652954102 + 0.1 * 6.633870601654053
Epoch 160, val loss: 1.2746169567108154
Epoch 170, training loss: 1.7198207378387451 = 1.0574133396148682 + 0.1 * 6.624073505401611
Epoch 170, val loss: 1.236153483390808
Epoch 180, training loss: 1.6578693389892578 = 0.9967242479324341 + 0.1 * 6.611450672149658
Epoch 180, val loss: 1.2014098167419434
Epoch 190, training loss: 1.5999325513839722 = 0.9394490122795105 + 0.1 * 6.604835033416748
Epoch 190, val loss: 1.1685938835144043
Epoch 200, training loss: 1.5441186428070068 = 0.884817361831665 + 0.1 * 6.593011856079102
Epoch 200, val loss: 1.1364260911941528
Epoch 210, training loss: 1.4902324676513672 = 0.8312854766845703 + 0.1 * 6.589469909667969
Epoch 210, val loss: 1.1029036045074463
Epoch 220, training loss: 1.4354474544525146 = 0.7779706716537476 + 0.1 * 6.574767112731934
Epoch 220, val loss: 1.0674748420715332
Epoch 230, training loss: 1.3805570602416992 = 0.7240176796913147 + 0.1 * 6.565393924713135
Epoch 230, val loss: 1.0296342372894287
Epoch 240, training loss: 1.3257951736450195 = 0.6702063679695129 + 0.1 * 6.5558881759643555
Epoch 240, val loss: 0.9902235269546509
Epoch 250, training loss: 1.2719900608062744 = 0.6175844073295593 + 0.1 * 6.544055938720703
Epoch 250, val loss: 0.9506872296333313
Epoch 260, training loss: 1.2211815118789673 = 0.567372739315033 + 0.1 * 6.538087368011475
Epoch 260, val loss: 0.9129428863525391
Epoch 270, training loss: 1.1733039617538452 = 0.5207809209823608 + 0.1 * 6.525230407714844
Epoch 270, val loss: 0.8783125877380371
Epoch 280, training loss: 1.130111813545227 = 0.478375107049942 + 0.1 * 6.517366409301758
Epoch 280, val loss: 0.8481345176696777
Epoch 290, training loss: 1.089919924736023 = 0.43948957324028015 + 0.1 * 6.504303455352783
Epoch 290, val loss: 0.8223586678504944
Epoch 300, training loss: 1.0525975227355957 = 0.40300145745277405 + 0.1 * 6.495960235595703
Epoch 300, val loss: 0.8007014989852905
Epoch 310, training loss: 1.0170562267303467 = 0.36850202083587646 + 0.1 * 6.485541343688965
Epoch 310, val loss: 0.7825218439102173
Epoch 320, training loss: 0.9831088781356812 = 0.33537670969963074 + 0.1 * 6.477321147918701
Epoch 320, val loss: 0.7671242952346802
Epoch 330, training loss: 0.9511761665344238 = 0.3034377992153168 + 0.1 * 6.477384090423584
Epoch 330, val loss: 0.7539536356925964
Epoch 340, training loss: 0.9186775088310242 = 0.27293092012405396 + 0.1 * 6.457465648651123
Epoch 340, val loss: 0.7426709532737732
Epoch 350, training loss: 0.8912584781646729 = 0.24407227337360382 + 0.1 * 6.471861839294434
Epoch 350, val loss: 0.7328264117240906
Epoch 360, training loss: 0.861485481262207 = 0.21716830134391785 + 0.1 * 6.443171501159668
Epoch 360, val loss: 0.7250503897666931
Epoch 370, training loss: 0.8366075754165649 = 0.1923988163471222 + 0.1 * 6.442087173461914
Epoch 370, val loss: 0.7194147109985352
Epoch 380, training loss: 0.8141167163848877 = 0.17003941535949707 + 0.1 * 6.440773010253906
Epoch 380, val loss: 0.7159892320632935
Epoch 390, training loss: 0.792279839515686 = 0.15019838511943817 + 0.1 * 6.420814037322998
Epoch 390, val loss: 0.7144676446914673
Epoch 400, training loss: 0.7758018374443054 = 0.13279175758361816 + 0.1 * 6.430100440979004
Epoch 400, val loss: 0.7148407697677612
Epoch 410, training loss: 0.7583019137382507 = 0.117886483669281 + 0.1 * 6.404154300689697
Epoch 410, val loss: 0.7168251276016235
Epoch 420, training loss: 0.7446491122245789 = 0.1051802784204483 + 0.1 * 6.394688129425049
Epoch 420, val loss: 0.7202470898628235
Epoch 430, training loss: 0.733437716960907 = 0.09429789334535599 + 0.1 * 6.391397953033447
Epoch 430, val loss: 0.724866509437561
Epoch 440, training loss: 0.7269025444984436 = 0.08498029410839081 + 0.1 * 6.419222354888916
Epoch 440, val loss: 0.7303565740585327
Epoch 450, training loss: 0.7143934965133667 = 0.07703619450330734 + 0.1 * 6.373572826385498
Epoch 450, val loss: 0.7365090250968933
Epoch 460, training loss: 0.7064444422721863 = 0.07016152143478394 + 0.1 * 6.362829208374023
Epoch 460, val loss: 0.7431795597076416
Epoch 470, training loss: 0.6997239589691162 = 0.06414090842008591 + 0.1 * 6.355830192565918
Epoch 470, val loss: 0.7503600120544434
Epoch 480, training loss: 0.6949286460876465 = 0.058874499052762985 + 0.1 * 6.360541343688965
Epoch 480, val loss: 0.7576757073402405
Epoch 490, training loss: 0.6886178851127625 = 0.05424569547176361 + 0.1 * 6.343721866607666
Epoch 490, val loss: 0.7652252316474915
Epoch 500, training loss: 0.6865108609199524 = 0.050133127719163895 + 0.1 * 6.3637776374816895
Epoch 500, val loss: 0.7728185653686523
Epoch 510, training loss: 0.6799377202987671 = 0.04647434875369072 + 0.1 * 6.334633827209473
Epoch 510, val loss: 0.7804254293441772
Epoch 520, training loss: 0.6768133044242859 = 0.04320431873202324 + 0.1 * 6.336089611053467
Epoch 520, val loss: 0.7880425453186035
Epoch 530, training loss: 0.6738185286521912 = 0.04026947170495987 + 0.1 * 6.3354902267456055
Epoch 530, val loss: 0.7955822944641113
Epoch 540, training loss: 0.6695995926856995 = 0.0376257561147213 + 0.1 * 6.319738388061523
Epoch 540, val loss: 0.8031294941902161
Epoch 550, training loss: 0.6667745113372803 = 0.035232752561569214 + 0.1 * 6.315417289733887
Epoch 550, val loss: 0.810546875
Epoch 560, training loss: 0.6640512943267822 = 0.03306416794657707 + 0.1 * 6.309871196746826
Epoch 560, val loss: 0.8179391026496887
Epoch 570, training loss: 0.6631792783737183 = 0.031090237200260162 + 0.1 * 6.320889949798584
Epoch 570, val loss: 0.8252096176147461
Epoch 580, training loss: 0.6597693562507629 = 0.029288530349731445 + 0.1 * 6.304808139801025
Epoch 580, val loss: 0.8323695063591003
Epoch 590, training loss: 0.6571670174598694 = 0.02764490433037281 + 0.1 * 6.295220851898193
Epoch 590, val loss: 0.8394166827201843
Epoch 600, training loss: 0.6558409333229065 = 0.02613689750432968 + 0.1 * 6.2970404624938965
Epoch 600, val loss: 0.8463346362113953
Epoch 610, training loss: 0.65318763256073 = 0.024750404059886932 + 0.1 * 6.284371852874756
Epoch 610, val loss: 0.8531366586685181
Epoch 620, training loss: 0.6531928777694702 = 0.02347196638584137 + 0.1 * 6.2972092628479
Epoch 620, val loss: 0.8598023653030396
Epoch 630, training loss: 0.6503303647041321 = 0.0222933292388916 + 0.1 * 6.280370235443115
Epoch 630, val loss: 0.8663913607597351
Epoch 640, training loss: 0.6502278447151184 = 0.021204814314842224 + 0.1 * 6.290229797363281
Epoch 640, val loss: 0.8727612495422363
Epoch 650, training loss: 0.6467866897583008 = 0.020198151469230652 + 0.1 * 6.265885353088379
Epoch 650, val loss: 0.8790746927261353
Epoch 660, training loss: 0.6459358334541321 = 0.019264446571469307 + 0.1 * 6.266713619232178
Epoch 660, val loss: 0.8852782845497131
Epoch 670, training loss: 0.6455326676368713 = 0.018395431339740753 + 0.1 * 6.271372318267822
Epoch 670, val loss: 0.8913251161575317
Epoch 680, training loss: 0.6442009210586548 = 0.017587833106517792 + 0.1 * 6.2661309242248535
Epoch 680, val loss: 0.8971786499023438
Epoch 690, training loss: 0.6419622302055359 = 0.01683586649596691 + 0.1 * 6.251263618469238
Epoch 690, val loss: 0.9030673503875732
Epoch 700, training loss: 0.6411325931549072 = 0.016130397096276283 + 0.1 * 6.250021934509277
Epoch 700, val loss: 0.9088303446769714
Epoch 710, training loss: 0.6424006223678589 = 0.015470167621970177 + 0.1 * 6.2693047523498535
Epoch 710, val loss: 0.9143695831298828
Epoch 720, training loss: 0.6397742033004761 = 0.01485426351428032 + 0.1 * 6.249199390411377
Epoch 720, val loss: 0.9198595881462097
Epoch 730, training loss: 0.6385140419006348 = 0.014275514520704746 + 0.1 * 6.242385387420654
Epoch 730, val loss: 0.9253786206245422
Epoch 740, training loss: 0.6374537944793701 = 0.01373050082474947 + 0.1 * 6.237232685089111
Epoch 740, val loss: 0.9306198358535767
Epoch 750, training loss: 0.6368805170059204 = 0.013218032196164131 + 0.1 * 6.236624717712402
Epoch 750, val loss: 0.9358737468719482
Epoch 760, training loss: 0.6363734602928162 = 0.012734911404550076 + 0.1 * 6.236385822296143
Epoch 760, val loss: 0.9409513473510742
Epoch 770, training loss: 0.635642409324646 = 0.012279830873012543 + 0.1 * 6.233625888824463
Epoch 770, val loss: 0.9460245966911316
Epoch 780, training loss: 0.6352792382240295 = 0.011849774047732353 + 0.1 * 6.234294414520264
Epoch 780, val loss: 0.9509496688842773
Epoch 790, training loss: 0.6340726613998413 = 0.011442700400948524 + 0.1 * 6.226299285888672
Epoch 790, val loss: 0.9558116793632507
Epoch 800, training loss: 0.6337583661079407 = 0.011057733558118343 + 0.1 * 6.227006435394287
Epoch 800, val loss: 0.9606596827507019
Epoch 810, training loss: 0.6331437230110168 = 0.010692923329770565 + 0.1 * 6.224508285522461
Epoch 810, val loss: 0.9652571082115173
Epoch 820, training loss: 0.632747232913971 = 0.010348539799451828 + 0.1 * 6.223986625671387
Epoch 820, val loss: 0.9699056148529053
Epoch 830, training loss: 0.6324755549430847 = 0.010020825080573559 + 0.1 * 6.224546909332275
Epoch 830, val loss: 0.9744622111320496
Epoch 840, training loss: 0.6331665515899658 = 0.00970940850675106 + 0.1 * 6.23457145690918
Epoch 840, val loss: 0.9789002537727356
Epoch 850, training loss: 0.6314153075218201 = 0.009414898231625557 + 0.1 * 6.220003604888916
Epoch 850, val loss: 0.9832018613815308
Epoch 860, training loss: 0.6303703188896179 = 0.009134183637797832 + 0.1 * 6.2123613357543945
Epoch 860, val loss: 0.9875643253326416
Epoch 870, training loss: 0.6300658583641052 = 0.008866546675562859 + 0.1 * 6.2119927406311035
Epoch 870, val loss: 0.9917408227920532
Epoch 880, training loss: 0.6293065547943115 = 0.008612052537500858 + 0.1 * 6.206944942474365
Epoch 880, val loss: 0.9958868026733398
Epoch 890, training loss: 0.6296004056930542 = 0.00836891308426857 + 0.1 * 6.212315082550049
Epoch 890, val loss: 0.999979555606842
Epoch 900, training loss: 0.6287804841995239 = 0.008136868476867676 + 0.1 * 6.2064361572265625
Epoch 900, val loss: 1.0040026903152466
Epoch 910, training loss: 0.6293408274650574 = 0.007915626280009747 + 0.1 * 6.21425199508667
Epoch 910, val loss: 1.007944107055664
Epoch 920, training loss: 0.6275869607925415 = 0.007703707087785006 + 0.1 * 6.198832035064697
Epoch 920, val loss: 1.0118674039840698
Epoch 930, training loss: 0.6305928230285645 = 0.007501283194869757 + 0.1 * 6.230915069580078
Epoch 930, val loss: 1.0156606435775757
Epoch 940, training loss: 0.6270180940628052 = 0.007308283355087042 + 0.1 * 6.1970977783203125
Epoch 940, val loss: 1.0193270444869995
Epoch 950, training loss: 0.6268120408058167 = 0.00712384469807148 + 0.1 * 6.1968817710876465
Epoch 950, val loss: 1.023091197013855
Epoch 960, training loss: 0.626094400882721 = 0.00694629130885005 + 0.1 * 6.191481113433838
Epoch 960, val loss: 1.0267221927642822
Epoch 970, training loss: 0.6277288794517517 = 0.006775971502065659 + 0.1 * 6.209528923034668
Epoch 970, val loss: 1.030293583869934
Epoch 980, training loss: 0.6259456872940063 = 0.0066129183396697044 + 0.1 * 6.193327903747559
Epoch 980, val loss: 1.0337296724319458
Epoch 990, training loss: 0.6255083084106445 = 0.006456219591200352 + 0.1 * 6.190521240234375
Epoch 990, val loss: 1.0372635126113892
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.3173
Flip ASR: 0.3378/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7897467613220215 = 1.952357292175293 + 0.1 * 8.373894691467285
Epoch 0, val loss: 1.9541938304901123
Epoch 10, training loss: 2.7795586585998535 = 1.9421777725219727 + 0.1 * 8.373809814453125
Epoch 10, val loss: 1.944127082824707
Epoch 20, training loss: 2.766997814178467 = 1.929667592048645 + 0.1 * 8.373302459716797
Epoch 20, val loss: 1.9316048622131348
Epoch 30, training loss: 2.7490124702453613 = 1.9120984077453613 + 0.1 * 8.369141578674316
Epoch 30, val loss: 1.9139928817749023
Epoch 40, training loss: 2.720113515853882 = 1.8861806392669678 + 0.1 * 8.33932876586914
Epoch 40, val loss: 1.8883440494537354
Epoch 50, training loss: 2.6661596298217773 = 1.8503586053848267 + 0.1 * 8.158010482788086
Epoch 50, val loss: 1.854430079460144
Epoch 60, training loss: 2.601548910140991 = 1.8095293045043945 + 0.1 * 7.920195579528809
Epoch 60, val loss: 1.818058729171753
Epoch 70, training loss: 2.5195000171661377 = 1.7711892127990723 + 0.1 * 7.483107089996338
Epoch 70, val loss: 1.785064697265625
Epoch 80, training loss: 2.4498865604400635 = 1.7313883304595947 + 0.1 * 7.184981346130371
Epoch 80, val loss: 1.7490190267562866
Epoch 90, training loss: 2.382009267807007 = 1.681437373161316 + 0.1 * 7.00571870803833
Epoch 90, val loss: 1.703942894935608
Epoch 100, training loss: 2.304436683654785 = 1.6165765523910522 + 0.1 * 6.878600120544434
Epoch 100, val loss: 1.6479084491729736
Epoch 110, training loss: 2.2146573066711426 = 1.5357720851898193 + 0.1 * 6.788852214813232
Epoch 110, val loss: 1.5787454843521118
Epoch 120, training loss: 2.118931531906128 = 1.445365071296692 + 0.1 * 6.735665321350098
Epoch 120, val loss: 1.5020700693130493
Epoch 130, training loss: 2.0206120014190674 = 1.3508033752441406 + 0.1 * 6.698086738586426
Epoch 130, val loss: 1.4234325885772705
Epoch 140, training loss: 1.9234158992767334 = 1.2561148405075073 + 0.1 * 6.673010349273682
Epoch 140, val loss: 1.3477163314819336
Epoch 150, training loss: 1.8288311958312988 = 1.1635617017745972 + 0.1 * 6.652695178985596
Epoch 150, val loss: 1.2748820781707764
Epoch 160, training loss: 1.7358508110046387 = 1.0719751119613647 + 0.1 * 6.638757228851318
Epoch 160, val loss: 1.204239010810852
Epoch 170, training loss: 1.6453568935394287 = 0.9831043481826782 + 0.1 * 6.622524738311768
Epoch 170, val loss: 1.1375929117202759
Epoch 180, training loss: 1.559081792831421 = 0.8982781767845154 + 0.1 * 6.608036518096924
Epoch 180, val loss: 1.0757596492767334
Epoch 190, training loss: 1.47996985912323 = 0.8199820518493652 + 0.1 * 6.599877834320068
Epoch 190, val loss: 1.0205849409103394
Epoch 200, training loss: 1.4083561897277832 = 0.7502847909927368 + 0.1 * 6.580714225769043
Epoch 200, val loss: 0.9738121628761292
Epoch 210, training loss: 1.3455817699432373 = 0.6883143186569214 + 0.1 * 6.57267427444458
Epoch 210, val loss: 0.9346342086791992
Epoch 220, training loss: 1.2892917394638062 = 0.6336988806724548 + 0.1 * 6.5559282302856445
Epoch 220, val loss: 0.9030527472496033
Epoch 230, training loss: 1.2396471500396729 = 0.5857382416725159 + 0.1 * 6.539088726043701
Epoch 230, val loss: 0.8782997131347656
Epoch 240, training loss: 1.1964296102523804 = 0.5437300205230713 + 0.1 * 6.526995658874512
Epoch 240, val loss: 0.859552264213562
Epoch 250, training loss: 1.1586344242095947 = 0.5069167613983154 + 0.1 * 6.517177104949951
Epoch 250, val loss: 0.8460427522659302
Epoch 260, training loss: 1.1239056587219238 = 0.47428709268569946 + 0.1 * 6.496184825897217
Epoch 260, val loss: 0.8368870615959167
Epoch 270, training loss: 1.0929436683654785 = 0.4446728825569153 + 0.1 * 6.482707500457764
Epoch 270, val loss: 0.8309495449066162
Epoch 280, training loss: 1.064610242843628 = 0.4171961545944214 + 0.1 * 6.474141597747803
Epoch 280, val loss: 0.8274651765823364
Epoch 290, training loss: 1.0376631021499634 = 0.39081212878227234 + 0.1 * 6.468509674072266
Epoch 290, val loss: 0.8255384564399719
Epoch 300, training loss: 1.0117195844650269 = 0.3652326166629791 + 0.1 * 6.464869022369385
Epoch 300, val loss: 0.8245699405670166
Epoch 310, training loss: 0.984468936920166 = 0.34014588594436646 + 0.1 * 6.443230152130127
Epoch 310, val loss: 0.8244003653526306
Epoch 320, training loss: 0.9589577913284302 = 0.31540408730506897 + 0.1 * 6.435536861419678
Epoch 320, val loss: 0.8247149586677551
Epoch 330, training loss: 0.9346266984939575 = 0.29129570722579956 + 0.1 * 6.433310031890869
Epoch 330, val loss: 0.8255617618560791
Epoch 340, training loss: 0.9104164838790894 = 0.26815173029899597 + 0.1 * 6.422647476196289
Epoch 340, val loss: 0.8267693519592285
Epoch 350, training loss: 0.8884094953536987 = 0.2461821734905243 + 0.1 * 6.422272682189941
Epoch 350, val loss: 0.8284590244293213
Epoch 360, training loss: 0.8666947484016418 = 0.2256556898355484 + 0.1 * 6.410390853881836
Epoch 360, val loss: 0.8307314515113831
Epoch 370, training loss: 0.8476923704147339 = 0.20657505095005035 + 0.1 * 6.411173343658447
Epoch 370, val loss: 0.8336242437362671
Epoch 380, training loss: 0.8297143578529358 = 0.1889982372522354 + 0.1 * 6.407161235809326
Epoch 380, val loss: 0.8371864557266235
Epoch 390, training loss: 0.8118909597396851 = 0.17292219400405884 + 0.1 * 6.389687538146973
Epoch 390, val loss: 0.841597855091095
Epoch 400, training loss: 0.7975389957427979 = 0.15821592509746552 + 0.1 * 6.393230438232422
Epoch 400, val loss: 0.8468849658966064
Epoch 410, training loss: 0.7846115827560425 = 0.14483799040317535 + 0.1 * 6.397736072540283
Epoch 410, val loss: 0.8528720736503601
Epoch 420, training loss: 0.7705302834510803 = 0.13273586332798004 + 0.1 * 6.377943992614746
Epoch 420, val loss: 0.8595916628837585
Epoch 430, training loss: 0.7581325769424438 = 0.12175367772579193 + 0.1 * 6.363788604736328
Epoch 430, val loss: 0.8670057058334351
Epoch 440, training loss: 0.7484166622161865 = 0.1117609441280365 + 0.1 * 6.366556644439697
Epoch 440, val loss: 0.8750878572463989
Epoch 450, training loss: 0.7385545969009399 = 0.10267563164234161 + 0.1 * 6.358789920806885
Epoch 450, val loss: 0.8836352229118347
Epoch 460, training loss: 0.7304665446281433 = 0.0944330096244812 + 0.1 * 6.360335350036621
Epoch 460, val loss: 0.8925012350082397
Epoch 470, training loss: 0.7213653922080994 = 0.08696969598531723 + 0.1 * 6.34395694732666
Epoch 470, val loss: 0.9017210006713867
Epoch 480, training loss: 0.7146428227424622 = 0.08016417175531387 + 0.1 * 6.344786167144775
Epoch 480, val loss: 0.9112879633903503
Epoch 490, training loss: 0.7081944942474365 = 0.07397846132516861 + 0.1 * 6.342159748077393
Epoch 490, val loss: 0.9209564328193665
Epoch 500, training loss: 0.7021747827529907 = 0.06835584342479706 + 0.1 * 6.338189125061035
Epoch 500, val loss: 0.93080073595047
Epoch 510, training loss: 0.6959957480430603 = 0.06323939561843872 + 0.1 * 6.327563285827637
Epoch 510, val loss: 0.9407258629798889
Epoch 520, training loss: 0.691091001033783 = 0.058571889996528625 + 0.1 * 6.325191020965576
Epoch 520, val loss: 0.9508596062660217
Epoch 530, training loss: 0.6867356300354004 = 0.05431515723466873 + 0.1 * 6.324204444885254
Epoch 530, val loss: 0.9607771039009094
Epoch 540, training loss: 0.6821379065513611 = 0.05044782906770706 + 0.1 * 6.316900730133057
Epoch 540, val loss: 0.9707401990890503
Epoch 550, training loss: 0.678068995475769 = 0.046922631561756134 + 0.1 * 6.311463356018066
Epoch 550, val loss: 0.9806491136550903
Epoch 560, training loss: 0.6755766868591309 = 0.04370053485035896 + 0.1 * 6.318761348724365
Epoch 560, val loss: 0.9904863238334656
Epoch 570, training loss: 0.671330451965332 = 0.040760572999715805 + 0.1 * 6.305698871612549
Epoch 570, val loss: 1.0001981258392334
Epoch 580, training loss: 0.6680092811584473 = 0.03807549551129341 + 0.1 * 6.299337387084961
Epoch 580, val loss: 1.0098168849945068
Epoch 590, training loss: 0.6651203036308289 = 0.03562404215335846 + 0.1 * 6.294962406158447
Epoch 590, val loss: 1.019272804260254
Epoch 600, training loss: 0.6630027890205383 = 0.03338120877742767 + 0.1 * 6.296215534210205
Epoch 600, val loss: 1.0286574363708496
Epoch 610, training loss: 0.6612045168876648 = 0.03133231773972511 + 0.1 * 6.298721790313721
Epoch 610, val loss: 1.0377848148345947
Epoch 620, training loss: 0.6578112244606018 = 0.029459215700626373 + 0.1 * 6.283519744873047
Epoch 620, val loss: 1.0467281341552734
Epoch 630, training loss: 0.6563273668289185 = 0.027742702513933182 + 0.1 * 6.285846710205078
Epoch 630, val loss: 1.0555987358093262
Epoch 640, training loss: 0.6536612510681152 = 0.026169534772634506 + 0.1 * 6.274917125701904
Epoch 640, val loss: 1.0642231702804565
Epoch 650, training loss: 0.6535002589225769 = 0.024725964292883873 + 0.1 * 6.287742614746094
Epoch 650, val loss: 1.0727143287658691
Epoch 660, training loss: 0.6513134837150574 = 0.02340151183307171 + 0.1 * 6.279119968414307
Epoch 660, val loss: 1.0808303356170654
Epoch 670, training loss: 0.6494976878166199 = 0.022183436900377274 + 0.1 * 6.273142337799072
Epoch 670, val loss: 1.088897943496704
Epoch 680, training loss: 0.6480376124382019 = 0.021060165017843246 + 0.1 * 6.269774436950684
Epoch 680, val loss: 1.09681236743927
Epoch 690, training loss: 0.6467307209968567 = 0.020021561533212662 + 0.1 * 6.267091751098633
Epoch 690, val loss: 1.1045767068862915
Epoch 700, training loss: 0.6461324095726013 = 0.019063208252191544 + 0.1 * 6.270691871643066
Epoch 700, val loss: 1.112060546875
Epoch 710, training loss: 0.6442897319793701 = 0.018174873664975166 + 0.1 * 6.261148452758789
Epoch 710, val loss: 1.1195162534713745
Epoch 720, training loss: 0.6437721848487854 = 0.017350807785987854 + 0.1 * 6.264214038848877
Epoch 720, val loss: 1.126818299293518
Epoch 730, training loss: 0.6427472829818726 = 0.016583984717726707 + 0.1 * 6.261632919311523
Epoch 730, val loss: 1.133920669555664
Epoch 740, training loss: 0.6409392952919006 = 0.015870561823248863 + 0.1 * 6.250687122344971
Epoch 740, val loss: 1.1408933401107788
Epoch 750, training loss: 0.6402611136436462 = 0.015205217525362968 + 0.1 * 6.250558853149414
Epoch 750, val loss: 1.1478312015533447
Epoch 760, training loss: 0.6400908827781677 = 0.014582525007426739 + 0.1 * 6.2550835609436035
Epoch 760, val loss: 1.1544485092163086
Epoch 770, training loss: 0.6384161114692688 = 0.014003228396177292 + 0.1 * 6.244128704071045
Epoch 770, val loss: 1.161004900932312
Epoch 780, training loss: 0.6386769413948059 = 0.013459427282214165 + 0.1 * 6.2521748542785645
Epoch 780, val loss: 1.1674989461898804
Epoch 790, training loss: 0.6376270055770874 = 0.012950500473380089 + 0.1 * 6.24676513671875
Epoch 790, val loss: 1.1738245487213135
Epoch 800, training loss: 0.6358846426010132 = 0.012471593916416168 + 0.1 * 6.234129905700684
Epoch 800, val loss: 1.1800408363342285
Epoch 810, training loss: 0.6359889507293701 = 0.012020918540656567 + 0.1 * 6.23967981338501
Epoch 810, val loss: 1.1861586570739746
Epoch 820, training loss: 0.6349325180053711 = 0.011596731841564178 + 0.1 * 6.233357906341553
Epoch 820, val loss: 1.1921113729476929
Epoch 830, training loss: 0.6341772675514221 = 0.011196129024028778 + 0.1 * 6.229811191558838
Epoch 830, val loss: 1.1980695724487305
Epoch 840, training loss: 0.6347495913505554 = 0.010817067697644234 + 0.1 * 6.239325523376465
Epoch 840, val loss: 1.2037618160247803
Epoch 850, training loss: 0.6337584853172302 = 0.010461331345140934 + 0.1 * 6.232971668243408
Epoch 850, val loss: 1.2092938423156738
Epoch 860, training loss: 0.6324294805526733 = 0.010123836807906628 + 0.1 * 6.223056316375732
Epoch 860, val loss: 1.2148771286010742
Epoch 870, training loss: 0.6327552199363708 = 0.009804069064557552 + 0.1 * 6.229511260986328
Epoch 870, val loss: 1.2203055620193481
Epoch 880, training loss: 0.631281852722168 = 0.009500861167907715 + 0.1 * 6.217809677124023
Epoch 880, val loss: 1.225549340248108
Epoch 890, training loss: 0.6307774186134338 = 0.009212845005095005 + 0.1 * 6.215645790100098
Epoch 890, val loss: 1.2308149337768555
Epoch 900, training loss: 0.6316434741020203 = 0.008939354680478573 + 0.1 * 6.227041244506836
Epoch 900, val loss: 1.2360044717788696
Epoch 910, training loss: 0.6302828788757324 = 0.008678902871906757 + 0.1 * 6.216039657592773
Epoch 910, val loss: 1.2409899234771729
Epoch 920, training loss: 0.6322367191314697 = 0.00843099970370531 + 0.1 * 6.238056659698486
Epoch 920, val loss: 1.2460283041000366
Epoch 930, training loss: 0.6295318603515625 = 0.008195067755877972 + 0.1 * 6.213367938995361
Epoch 930, val loss: 1.2508165836334229
Epoch 940, training loss: 0.6297494769096375 = 0.007970361039042473 + 0.1 * 6.2177910804748535
Epoch 940, val loss: 1.2556284666061401
Epoch 950, training loss: 0.6290985345840454 = 0.007756026927381754 + 0.1 * 6.2134246826171875
Epoch 950, val loss: 1.2603657245635986
Epoch 960, training loss: 0.6277565956115723 = 0.0075508044101297855 + 0.1 * 6.2020583152771
Epoch 960, val loss: 1.2650246620178223
Epoch 970, training loss: 0.6281223893165588 = 0.007354879751801491 + 0.1 * 6.207675457000732
Epoch 970, val loss: 1.269683599472046
Epoch 980, training loss: 0.627855658531189 = 0.007167062256485224 + 0.1 * 6.206886291503906
Epoch 980, val loss: 1.274246335029602
Epoch 990, training loss: 0.6269791126251221 = 0.0069873761385679245 + 0.1 * 6.199916839599609
Epoch 990, val loss: 1.2787123918533325
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8081
Flip ASR: 0.7689/225 nodes
The final ASR:0.53137, 0.20521, Accuracy:0.81605, 0.01062
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10596])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00460, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.788806438446045 = 1.9514213800430298 + 0.1 * 8.373849868774414
Epoch 0, val loss: 1.9514586925506592
Epoch 10, training loss: 2.778561592102051 = 1.9411935806274414 + 0.1 * 8.373679161071777
Epoch 10, val loss: 1.9420349597930908
Epoch 20, training loss: 2.7660114765167236 = 1.9287413358688354 + 0.1 * 8.372700691223145
Epoch 20, val loss: 1.9302070140838623
Epoch 30, training loss: 2.747934341430664 = 1.911392331123352 + 0.1 * 8.365421295166016
Epoch 30, val loss: 1.9133447408676147
Epoch 40, training loss: 2.7180559635162354 = 1.8858307600021362 + 0.1 * 8.32225227355957
Epoch 40, val loss: 1.8884721994400024
Epoch 50, training loss: 2.6589560508728027 = 1.8506860733032227 + 0.1 * 8.0826997756958
Epoch 50, val loss: 1.8556134700775146
Epoch 60, training loss: 2.5873470306396484 = 1.810450792312622 + 0.1 * 7.768962383270264
Epoch 60, val loss: 1.8206757307052612
Epoch 70, training loss: 2.510533094406128 = 1.7716460227966309 + 0.1 * 7.388870716094971
Epoch 70, val loss: 1.789125919342041
Epoch 80, training loss: 2.4424901008605957 = 1.7339730262756348 + 0.1 * 7.085170269012451
Epoch 80, val loss: 1.7591885328292847
Epoch 90, training loss: 2.3804397583007812 = 1.6870540380477905 + 0.1 * 6.933857440948486
Epoch 90, val loss: 1.719050407409668
Epoch 100, training loss: 2.310485601425171 = 1.6247773170471191 + 0.1 * 6.857082843780518
Epoch 100, val loss: 1.6662638187408447
Epoch 110, training loss: 2.226346492767334 = 1.5461803674697876 + 0.1 * 6.801661014556885
Epoch 110, val loss: 1.6025729179382324
Epoch 120, training loss: 2.1314899921417236 = 1.4558610916137695 + 0.1 * 6.756289005279541
Epoch 120, val loss: 1.5293644666671753
Epoch 130, training loss: 2.031935691833496 = 1.359805703163147 + 0.1 * 6.721299648284912
Epoch 130, val loss: 1.4522109031677246
Epoch 140, training loss: 1.930591344833374 = 1.2613917589187622 + 0.1 * 6.691995143890381
Epoch 140, val loss: 1.3737678527832031
Epoch 150, training loss: 1.8311108350753784 = 1.1639912128448486 + 0.1 * 6.671195983886719
Epoch 150, val loss: 1.2966959476470947
Epoch 160, training loss: 1.7348299026489258 = 1.0706580877304077 + 0.1 * 6.641718864440918
Epoch 160, val loss: 1.2233270406723022
Epoch 170, training loss: 1.643744945526123 = 0.9817612171173096 + 0.1 * 6.619837284088135
Epoch 170, val loss: 1.1540864706039429
Epoch 180, training loss: 1.5578036308288574 = 0.8981389999389648 + 0.1 * 6.596645832061768
Epoch 180, val loss: 1.089743971824646
Epoch 190, training loss: 1.4784663915634155 = 0.8200374245643616 + 0.1 * 6.58428955078125
Epoch 190, val loss: 1.0302174091339111
Epoch 200, training loss: 1.4041028022766113 = 0.7482287287712097 + 0.1 * 6.558740615844727
Epoch 200, val loss: 0.9765336513519287
Epoch 210, training loss: 1.3370420932769775 = 0.6825042963027954 + 0.1 * 6.5453782081604
Epoch 210, val loss: 0.9286068677902222
Epoch 220, training loss: 1.2757010459899902 = 0.6232643723487854 + 0.1 * 6.5243659019470215
Epoch 220, val loss: 0.8872972726821899
Epoch 230, training loss: 1.222306489944458 = 0.5709996223449707 + 0.1 * 6.513069152832031
Epoch 230, val loss: 0.8531094193458557
Epoch 240, training loss: 1.1754469871520996 = 0.5259297490119934 + 0.1 * 6.495172500610352
Epoch 240, val loss: 0.8265895843505859
Epoch 250, training loss: 1.1356544494628906 = 0.4871429204940796 + 0.1 * 6.4851155281066895
Epoch 250, val loss: 0.8066884875297546
Epoch 260, training loss: 1.0997166633605957 = 0.45351144671440125 + 0.1 * 6.462052345275879
Epoch 260, val loss: 0.7923904657363892
Epoch 270, training loss: 1.0697760581970215 = 0.423245906829834 + 0.1 * 6.465301990509033
Epoch 270, val loss: 0.7816799879074097
Epoch 280, training loss: 1.0399625301361084 = 0.3953762650489807 + 0.1 * 6.44586181640625
Epoch 280, val loss: 0.7732290625572205
Epoch 290, training loss: 1.0118964910507202 = 0.3689238131046295 + 0.1 * 6.429726600646973
Epoch 290, val loss: 0.7661342024803162
Epoch 300, training loss: 0.9861325025558472 = 0.34342941641807556 + 0.1 * 6.42703104019165
Epoch 300, val loss: 0.7598341703414917
Epoch 310, training loss: 0.9616103172302246 = 0.31904518604278564 + 0.1 * 6.4256510734558105
Epoch 310, val loss: 0.754319429397583
Epoch 320, training loss: 0.9360149502754211 = 0.2958611845970154 + 0.1 * 6.4015374183654785
Epoch 320, val loss: 0.7493963837623596
Epoch 330, training loss: 0.9136266708374023 = 0.2738517224788666 + 0.1 * 6.397748947143555
Epoch 330, val loss: 0.7450051307678223
Epoch 340, training loss: 0.8926849365234375 = 0.25300222635269165 + 0.1 * 6.39682674407959
Epoch 340, val loss: 0.7411183714866638
Epoch 350, training loss: 0.8713171482086182 = 0.23305276036262512 + 0.1 * 6.382643222808838
Epoch 350, val loss: 0.7376821637153625
Epoch 360, training loss: 0.8510580658912659 = 0.21378476917743683 + 0.1 * 6.372733116149902
Epoch 360, val loss: 0.7344357371330261
Epoch 370, training loss: 0.8325897455215454 = 0.19518914818763733 + 0.1 * 6.3740057945251465
Epoch 370, val loss: 0.7314239144325256
Epoch 380, training loss: 0.8132991790771484 = 0.17713196575641632 + 0.1 * 6.361672401428223
Epoch 380, val loss: 0.7286692261695862
Epoch 390, training loss: 0.7952798008918762 = 0.15975672006607056 + 0.1 * 6.355230808258057
Epoch 390, val loss: 0.7265049815177917
Epoch 400, training loss: 0.7809607982635498 = 0.1433994024991989 + 0.1 * 6.375613689422607
Epoch 400, val loss: 0.7251362800598145
Epoch 410, training loss: 0.763662576675415 = 0.1284307837486267 + 0.1 * 6.352317810058594
Epoch 410, val loss: 0.7250348329544067
Epoch 420, training loss: 0.7516258955001831 = 0.1149548813700676 + 0.1 * 6.3667097091674805
Epoch 420, val loss: 0.7264590263366699
Epoch 430, training loss: 0.7371947765350342 = 0.10305654257535934 + 0.1 * 6.341382026672363
Epoch 430, val loss: 0.7294716238975525
Epoch 440, training loss: 0.7256616950035095 = 0.09263014048337936 + 0.1 * 6.330315113067627
Epoch 440, val loss: 0.733953058719635
Epoch 450, training loss: 0.716261625289917 = 0.08355706930160522 + 0.1 * 6.327045440673828
Epoch 450, val loss: 0.7396726012229919
Epoch 460, training loss: 0.708533763885498 = 0.07568658143281937 + 0.1 * 6.328471660614014
Epoch 460, val loss: 0.7463176846504211
Epoch 470, training loss: 0.7009180784225464 = 0.06884922832250595 + 0.1 * 6.320688724517822
Epoch 470, val loss: 0.7536731958389282
Epoch 480, training loss: 0.6944854259490967 = 0.06288250535726547 + 0.1 * 6.316029071807861
Epoch 480, val loss: 0.7614229321479797
Epoch 490, training loss: 0.6898382306098938 = 0.05764993652701378 + 0.1 * 6.321883201599121
Epoch 490, val loss: 0.7694878578186035
Epoch 500, training loss: 0.6840456128120422 = 0.05304243788123131 + 0.1 * 6.310031890869141
Epoch 500, val loss: 0.7777014970779419
Epoch 510, training loss: 0.6802964806556702 = 0.048953160643577576 + 0.1 * 6.313432693481445
Epoch 510, val loss: 0.7859711647033691
Epoch 520, training loss: 0.6751360893249512 = 0.04531312361359596 + 0.1 * 6.298229694366455
Epoch 520, val loss: 0.7942318320274353
Epoch 530, training loss: 0.6720612645149231 = 0.04205184429883957 + 0.1 * 6.300094127655029
Epoch 530, val loss: 0.8024540543556213
Epoch 540, training loss: 0.6681055426597595 = 0.03912236914038658 + 0.1 * 6.28983211517334
Epoch 540, val loss: 0.8105795383453369
Epoch 550, training loss: 0.6647313833236694 = 0.036478858441114426 + 0.1 * 6.282525062561035
Epoch 550, val loss: 0.8186129927635193
Epoch 560, training loss: 0.6621185541152954 = 0.03408413007855415 + 0.1 * 6.280344009399414
Epoch 560, val loss: 0.8265284895896912
Epoch 570, training loss: 0.6597158908843994 = 0.0319124199450016 + 0.1 * 6.278034210205078
Epoch 570, val loss: 0.8342734575271606
Epoch 580, training loss: 0.6575930118560791 = 0.029936522245407104 + 0.1 * 6.276564598083496
Epoch 580, val loss: 0.8418344855308533
Epoch 590, training loss: 0.6562203764915466 = 0.028130708262324333 + 0.1 * 6.2808966636657715
Epoch 590, val loss: 0.8493037223815918
Epoch 600, training loss: 0.6531786322593689 = 0.026484036818146706 + 0.1 * 6.266946315765381
Epoch 600, val loss: 0.8566012382507324
Epoch 610, training loss: 0.6512236595153809 = 0.024977510794997215 + 0.1 * 6.262461185455322
Epoch 610, val loss: 0.8637263178825378
Epoch 620, training loss: 0.6495062112808228 = 0.023591861128807068 + 0.1 * 6.259143352508545
Epoch 620, val loss: 0.870749294757843
Epoch 630, training loss: 0.647964596748352 = 0.022314663976430893 + 0.1 * 6.256499290466309
Epoch 630, val loss: 0.8775425553321838
Epoch 640, training loss: 0.6466162800788879 = 0.021137841045856476 + 0.1 * 6.254784107208252
Epoch 640, val loss: 0.8843048810958862
Epoch 650, training loss: 0.6458935737609863 = 0.020053870975971222 + 0.1 * 6.258397102355957
Epoch 650, val loss: 0.8908429145812988
Epoch 660, training loss: 0.643896758556366 = 0.019053446128964424 + 0.1 * 6.248432636260986
Epoch 660, val loss: 0.8972353339195251
Epoch 670, training loss: 0.6427686214447021 = 0.01812872104346752 + 0.1 * 6.24639892578125
Epoch 670, val loss: 0.9034351706504822
Epoch 680, training loss: 0.6425557136535645 = 0.017270714044570923 + 0.1 * 6.25285005569458
Epoch 680, val loss: 0.9095509648323059
Epoch 690, training loss: 0.6409506797790527 = 0.016474632546305656 + 0.1 * 6.244760513305664
Epoch 690, val loss: 0.9154766798019409
Epoch 700, training loss: 0.6407507061958313 = 0.015735555440187454 + 0.1 * 6.25015115737915
Epoch 700, val loss: 0.9213433861732483
Epoch 710, training loss: 0.6379817724227905 = 0.01504786778241396 + 0.1 * 6.229339122772217
Epoch 710, val loss: 0.9270405173301697
Epoch 720, training loss: 0.6376606822013855 = 0.014406226575374603 + 0.1 * 6.232544422149658
Epoch 720, val loss: 0.9326185584068298
Epoch 730, training loss: 0.6381170749664307 = 0.01380570512264967 + 0.1 * 6.243113994598389
Epoch 730, val loss: 0.9381325840950012
Epoch 740, training loss: 0.6359426975250244 = 0.013244853354990482 + 0.1 * 6.226978302001953
Epoch 740, val loss: 0.9435171484947205
Epoch 750, training loss: 0.6353006362915039 = 0.012719282880425453 + 0.1 * 6.225813388824463
Epoch 750, val loss: 0.948784589767456
Epoch 760, training loss: 0.6369957327842712 = 0.012225979939103127 + 0.1 * 6.247697353363037
Epoch 760, val loss: 0.9540265202522278
Epoch 770, training loss: 0.6342114806175232 = 0.011764313094317913 + 0.1 * 6.224471569061279
Epoch 770, val loss: 0.9591006636619568
Epoch 780, training loss: 0.6327241063117981 = 0.011330273933708668 + 0.1 * 6.2139387130737305
Epoch 780, val loss: 0.9640939235687256
Epoch 790, training loss: 0.6329009532928467 = 0.01092090830206871 + 0.1 * 6.2198004722595215
Epoch 790, val loss: 0.9690210223197937
Epoch 800, training loss: 0.632948637008667 = 0.010534348897635937 + 0.1 * 6.224143028259277
Epoch 800, val loss: 0.9737794995307922
Epoch 810, training loss: 0.6314018368721008 = 0.010171081870794296 + 0.1 * 6.212307929992676
Epoch 810, val loss: 0.9785014986991882
Epoch 820, training loss: 0.6322983503341675 = 0.009827887639403343 + 0.1 * 6.224704265594482
Epoch 820, val loss: 0.9831148386001587
Epoch 830, training loss: 0.630538821220398 = 0.009503836743533611 + 0.1 * 6.2103495597839355
Epoch 830, val loss: 0.9877025485038757
Epoch 840, training loss: 0.6301356554031372 = 0.009197412058711052 + 0.1 * 6.2093825340271
Epoch 840, val loss: 0.9920936822891235
Epoch 850, training loss: 0.628908634185791 = 0.008906761184334755 + 0.1 * 6.200018405914307
Epoch 850, val loss: 0.9964634776115417
Epoch 860, training loss: 0.6299040913581848 = 0.008630511350929737 + 0.1 * 6.212735652923584
Epoch 860, val loss: 1.0007672309875488
Epoch 870, training loss: 0.6282376050949097 = 0.008368694223463535 + 0.1 * 6.1986894607543945
Epoch 870, val loss: 1.0049790143966675
Epoch 880, training loss: 0.6309828162193298 = 0.008120049722492695 + 0.1 * 6.228627681732178
Epoch 880, val loss: 1.009129524230957
Epoch 890, training loss: 0.6286027431488037 = 0.007883934304118156 + 0.1 * 6.207188129425049
Epoch 890, val loss: 1.0132482051849365
Epoch 900, training loss: 0.6277633905410767 = 0.0076593984849750996 + 0.1 * 6.201040267944336
Epoch 900, val loss: 1.017225980758667
Epoch 910, training loss: 0.6268517374992371 = 0.007445069961249828 + 0.1 * 6.194066047668457
Epoch 910, val loss: 1.0211899280548096
Epoch 920, training loss: 0.6270526647567749 = 0.007240940351039171 + 0.1 * 6.198116779327393
Epoch 920, val loss: 1.0250399112701416
Epoch 930, training loss: 0.6262163519859314 = 0.007045485079288483 + 0.1 * 6.191708087921143
Epoch 930, val loss: 1.028855800628662
Epoch 940, training loss: 0.6284762620925903 = 0.006858864799141884 + 0.1 * 6.2161736488342285
Epoch 940, val loss: 1.0326746702194214
Epoch 950, training loss: 0.6262518763542175 = 0.0066802590154111385 + 0.1 * 6.19571590423584
Epoch 950, val loss: 1.0363315343856812
Epoch 960, training loss: 0.6264774799346924 = 0.006510094273835421 + 0.1 * 6.199673652648926
Epoch 960, val loss: 1.0400255918502808
Epoch 970, training loss: 0.6245575547218323 = 0.006346605252474546 + 0.1 * 6.182109832763672
Epoch 970, val loss: 1.0436265468597412
Epoch 980, training loss: 0.6250636577606201 = 0.006190381478518248 + 0.1 * 6.188733100891113
Epoch 980, val loss: 1.0471535921096802
Epoch 990, training loss: 0.6243022084236145 = 0.006040372420102358 + 0.1 * 6.182618141174316
Epoch 990, val loss: 1.0506675243377686
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6199
Flip ASR: 0.5467/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.786813735961914 = 1.949427604675293 + 0.1 * 8.373860359191895
Epoch 0, val loss: 1.949514389038086
Epoch 10, training loss: 2.776620864868164 = 1.9392467737197876 + 0.1 * 8.373741149902344
Epoch 10, val loss: 1.9390026330947876
Epoch 20, training loss: 2.764157295227051 = 1.9268521070480347 + 0.1 * 8.373052597045898
Epoch 20, val loss: 1.9261865615844727
Epoch 30, training loss: 2.7463557720184326 = 1.9095803499221802 + 0.1 * 8.367754936218262
Epoch 30, val loss: 1.9081802368164062
Epoch 40, training loss: 2.716123342514038 = 1.8842509984970093 + 0.1 * 8.31872272491455
Epoch 40, val loss: 1.881882905960083
Epoch 50, training loss: 2.635040283203125 = 1.8504208326339722 + 0.1 * 7.846193313598633
Epoch 50, val loss: 1.8482403755187988
Epoch 60, training loss: 2.545487880706787 = 1.8157649040222168 + 0.1 * 7.2972307205200195
Epoch 60, val loss: 1.8146864175796509
Epoch 70, training loss: 2.4759039878845215 = 1.7801140546798706 + 0.1 * 6.9578986167907715
Epoch 70, val loss: 1.7807732820510864
Epoch 80, training loss: 2.4194111824035645 = 1.7406419515609741 + 0.1 * 6.787691593170166
Epoch 80, val loss: 1.744109034538269
Epoch 90, training loss: 2.3637146949768066 = 1.691486120223999 + 0.1 * 6.72228479385376
Epoch 90, val loss: 1.6986063718795776
Epoch 100, training loss: 2.294390916824341 = 1.625909447669983 + 0.1 * 6.684813976287842
Epoch 100, val loss: 1.6387144327163696
Epoch 110, training loss: 2.2078065872192383 = 1.5419566631317139 + 0.1 * 6.6584978103637695
Epoch 110, val loss: 1.5648016929626465
Epoch 120, training loss: 2.108020544052124 = 1.444139003753662 + 0.1 * 6.638814926147461
Epoch 120, val loss: 1.4819241762161255
Epoch 130, training loss: 2.00529408454895 = 1.342881441116333 + 0.1 * 6.6241254806518555
Epoch 130, val loss: 1.399294376373291
Epoch 140, training loss: 1.9074375629425049 = 1.2464399337768555 + 0.1 * 6.609975814819336
Epoch 140, val loss: 1.3252766132354736
Epoch 150, training loss: 1.8173158168792725 = 1.1573554277420044 + 0.1 * 6.599604606628418
Epoch 150, val loss: 1.2605817317962646
Epoch 160, training loss: 1.7327739000320435 = 1.0741653442382812 + 0.1 * 6.586085319519043
Epoch 160, val loss: 1.2018922567367554
Epoch 170, training loss: 1.649573802947998 = 0.9920568466186523 + 0.1 * 6.575170040130615
Epoch 170, val loss: 1.1439026594161987
Epoch 180, training loss: 1.5652759075164795 = 0.9092245697975159 + 0.1 * 6.560513019561768
Epoch 180, val loss: 1.0850040912628174
Epoch 190, training loss: 1.4818379878997803 = 0.8267345428466797 + 0.1 * 6.551034450531006
Epoch 190, val loss: 1.02570641040802
Epoch 200, training loss: 1.4023150205612183 = 0.7484665513038635 + 0.1 * 6.538484573364258
Epoch 200, val loss: 0.9690372943878174
Epoch 210, training loss: 1.3297181129455566 = 0.6767401695251465 + 0.1 * 6.529778957366943
Epoch 210, val loss: 0.9176611304283142
Epoch 220, training loss: 1.2651088237762451 = 0.6137545108795166 + 0.1 * 6.513542652130127
Epoch 220, val loss: 0.8734994530677795
Epoch 230, training loss: 1.2098314762115479 = 0.5597333908081055 + 0.1 * 6.500980854034424
Epoch 230, val loss: 0.8372512459754944
Epoch 240, training loss: 1.1623308658599854 = 0.5133600831031799 + 0.1 * 6.489707946777344
Epoch 240, val loss: 0.8082117438316345
Epoch 250, training loss: 1.1224243640899658 = 0.4730232357978821 + 0.1 * 6.4940104484558105
Epoch 250, val loss: 0.7851981520652771
Epoch 260, training loss: 1.0847692489624023 = 0.43787989020347595 + 0.1 * 6.468893051147461
Epoch 260, val loss: 0.7671228647232056
Epoch 270, training loss: 1.0513006448745728 = 0.4055987000465393 + 0.1 * 6.457019329071045
Epoch 270, val loss: 0.7522460222244263
Epoch 280, training loss: 1.0197288990020752 = 0.37492209672927856 + 0.1 * 6.448068618774414
Epoch 280, val loss: 0.7395753264427185
Epoch 290, training loss: 0.9894463419914246 = 0.34527772665023804 + 0.1 * 6.441686153411865
Epoch 290, val loss: 0.7286831140518188
Epoch 300, training loss: 0.9598118662834167 = 0.31672531366348267 + 0.1 * 6.430865287780762
Epoch 300, val loss: 0.7193823456764221
Epoch 310, training loss: 0.931512713432312 = 0.2892093360424042 + 0.1 * 6.423033714294434
Epoch 310, val loss: 0.7115880846977234
Epoch 320, training loss: 0.9069675207138062 = 0.2632347643375397 + 0.1 * 6.437326908111572
Epoch 320, val loss: 0.7054447531700134
Epoch 330, training loss: 0.8807603716850281 = 0.23926974833011627 + 0.1 * 6.4149065017700195
Epoch 330, val loss: 0.7010247111320496
Epoch 340, training loss: 0.8573959469795227 = 0.21730439364910126 + 0.1 * 6.400915145874023
Epoch 340, val loss: 0.6983110308647156
Epoch 350, training loss: 0.8404809236526489 = 0.1973290890455246 + 0.1 * 6.431518077850342
Epoch 350, val loss: 0.6972168684005737
Epoch 360, training loss: 0.8187386393547058 = 0.17940570414066315 + 0.1 * 6.39332914352417
Epoch 360, val loss: 0.6974895596504211
Epoch 370, training loss: 0.8016767501831055 = 0.16317665576934814 + 0.1 * 6.385000705718994
Epoch 370, val loss: 0.6987923383712769
Epoch 380, training loss: 0.7866560816764832 = 0.14848190546035767 + 0.1 * 6.381741523742676
Epoch 380, val loss: 0.7010274529457092
Epoch 390, training loss: 0.7722827196121216 = 0.13520154356956482 + 0.1 * 6.370811939239502
Epoch 390, val loss: 0.7040411233901978
Epoch 400, training loss: 0.7597653269767761 = 0.12318430095911026 + 0.1 * 6.365809917449951
Epoch 400, val loss: 0.7076793313026428
Epoch 410, training loss: 0.749243438243866 = 0.11239653825759888 + 0.1 * 6.368468761444092
Epoch 410, val loss: 0.7118064761161804
Epoch 420, training loss: 0.7383193969726562 = 0.102757029235363 + 0.1 * 6.355623722076416
Epoch 420, val loss: 0.7165194749832153
Epoch 430, training loss: 0.7298281788825989 = 0.09407699853181839 + 0.1 * 6.3575119972229
Epoch 430, val loss: 0.7216082811355591
Epoch 440, training loss: 0.721052885055542 = 0.08626635372638702 + 0.1 * 6.347865104675293
Epoch 440, val loss: 0.7271599173545837
Epoch 450, training loss: 0.7136171460151672 = 0.07922135293483734 + 0.1 * 6.343957424163818
Epoch 450, val loss: 0.7331503629684448
Epoch 460, training loss: 0.7074364423751831 = 0.07287630438804626 + 0.1 * 6.3456010818481445
Epoch 460, val loss: 0.7394410967826843
Epoch 470, training loss: 0.7003086805343628 = 0.06718774884939194 + 0.1 * 6.331209182739258
Epoch 470, val loss: 0.74588942527771
Epoch 480, training loss: 0.694538414478302 = 0.062066152691841125 + 0.1 * 6.324722766876221
Epoch 480, val loss: 0.7525625228881836
Epoch 490, training loss: 0.6901124119758606 = 0.05743355676531792 + 0.1 * 6.326788425445557
Epoch 490, val loss: 0.7594049572944641
Epoch 500, training loss: 0.6848710179328918 = 0.05325528234243393 + 0.1 * 6.316157341003418
Epoch 500, val loss: 0.7663559317588806
Epoch 510, training loss: 0.6807769536972046 = 0.04947619140148163 + 0.1 * 6.313007354736328
Epoch 510, val loss: 0.77334064245224
Epoch 520, training loss: 0.678663432598114 = 0.046052269637584686 + 0.1 * 6.326111316680908
Epoch 520, val loss: 0.7803369760513306
Epoch 530, training loss: 0.6729772090911865 = 0.04295244440436363 + 0.1 * 6.300247669219971
Epoch 530, val loss: 0.7872962355613708
Epoch 540, training loss: 0.6702556610107422 = 0.04013558104634285 + 0.1 * 6.301200866699219
Epoch 540, val loss: 0.7942517399787903
Epoch 550, training loss: 0.6671966314315796 = 0.03757103905081749 + 0.1 * 6.296256065368652
Epoch 550, val loss: 0.8011103868484497
Epoch 560, training loss: 0.665425717830658 = 0.03524281084537506 + 0.1 * 6.301828861236572
Epoch 560, val loss: 0.807957649230957
Epoch 570, training loss: 0.6614368557929993 = 0.0331212654709816 + 0.1 * 6.28315544128418
Epoch 570, val loss: 0.8146328926086426
Epoch 580, training loss: 0.658955991268158 = 0.031181303784251213 + 0.1 * 6.27774715423584
Epoch 580, val loss: 0.8212791085243225
Epoch 590, training loss: 0.6585856676101685 = 0.029399549588561058 + 0.1 * 6.291861057281494
Epoch 590, val loss: 0.8278366923332214
Epoch 600, training loss: 0.6556642055511475 = 0.02776971086859703 + 0.1 * 6.278944969177246
Epoch 600, val loss: 0.8342293500900269
Epoch 610, training loss: 0.6546516418457031 = 0.026270240545272827 + 0.1 * 6.283813953399658
Epoch 610, val loss: 0.8405740261077881
Epoch 620, training loss: 0.6519565582275391 = 0.024891922250390053 + 0.1 * 6.270646095275879
Epoch 620, val loss: 0.8467679619789124
Epoch 630, training loss: 0.6510389447212219 = 0.023616423830389977 + 0.1 * 6.274224758148193
Epoch 630, val loss: 0.8528463840484619
Epoch 640, training loss: 0.6490419507026672 = 0.02243790589272976 + 0.1 * 6.266040325164795
Epoch 640, val loss: 0.8588227033615112
Epoch 650, training loss: 0.6476178169250488 = 0.02134547010064125 + 0.1 * 6.262722969055176
Epoch 650, val loss: 0.8647155165672302
Epoch 660, training loss: 0.6460568308830261 = 0.020334875211119652 + 0.1 * 6.257219314575195
Epoch 660, val loss: 0.8704769015312195
Epoch 670, training loss: 0.6454161405563354 = 0.019394705072045326 + 0.1 * 6.260214328765869
Epoch 670, val loss: 0.8761327862739563
Epoch 680, training loss: 0.6434780955314636 = 0.01852152869105339 + 0.1 * 6.249565601348877
Epoch 680, val loss: 0.8816698789596558
Epoch 690, training loss: 0.6429848670959473 = 0.01770646683871746 + 0.1 * 6.25278377532959
Epoch 690, val loss: 0.8871240019798279
Epoch 700, training loss: 0.6415760517120361 = 0.016946518793702126 + 0.1 * 6.24629545211792
Epoch 700, val loss: 0.89253169298172
Epoch 710, training loss: 0.6404697895050049 = 0.01623617298901081 + 0.1 * 6.242335796356201
Epoch 710, val loss: 0.8977572917938232
Epoch 720, training loss: 0.6399359703063965 = 0.015570794232189655 + 0.1 * 6.243651390075684
Epoch 720, val loss: 0.9029396772384644
Epoch 730, training loss: 0.6385118365287781 = 0.014947646297514439 + 0.1 * 6.2356414794921875
Epoch 730, val loss: 0.9079931378364563
Epoch 740, training loss: 0.6392945647239685 = 0.014362095855176449 + 0.1 * 6.249324798583984
Epoch 740, val loss: 0.9129875302314758
Epoch 750, training loss: 0.6373124718666077 = 0.013815092854201794 + 0.1 * 6.234973430633545
Epoch 750, val loss: 0.9178319573402405
Epoch 760, training loss: 0.6361861228942871 = 0.013299768790602684 + 0.1 * 6.228863716125488
Epoch 760, val loss: 0.9225870966911316
Epoch 770, training loss: 0.6355463266372681 = 0.012814239598810673 + 0.1 * 6.227321147918701
Epoch 770, val loss: 0.9273026585578918
Epoch 780, training loss: 0.6349788308143616 = 0.012356845661997795 + 0.1 * 6.22622013092041
Epoch 780, val loss: 0.9318673610687256
Epoch 790, training loss: 0.6341579556465149 = 0.01192418858408928 + 0.1 * 6.222337245941162
Epoch 790, val loss: 0.9364215135574341
Epoch 800, training loss: 0.6338673830032349 = 0.011514686979353428 + 0.1 * 6.223526954650879
Epoch 800, val loss: 0.9409003257751465
Epoch 810, training loss: 0.6341425180435181 = 0.011126447468996048 + 0.1 * 6.230160236358643
Epoch 810, val loss: 0.9452953338623047
Epoch 820, training loss: 0.6325384378433228 = 0.010760340839624405 + 0.1 * 6.217780590057373
Epoch 820, val loss: 0.9496105909347534
Epoch 830, training loss: 0.6319220066070557 = 0.01041311677545309 + 0.1 * 6.215088844299316
Epoch 830, val loss: 0.9538405537605286
Epoch 840, training loss: 0.6328429579734802 = 0.01008317805826664 + 0.1 * 6.227597713470459
Epoch 840, val loss: 0.9580543637275696
Epoch 850, training loss: 0.6314090490341187 = 0.009770253673195839 + 0.1 * 6.216387748718262
Epoch 850, val loss: 0.9621720910072327
Epoch 860, training loss: 0.6309328675270081 = 0.0094728609547019 + 0.1 * 6.214600086212158
Epoch 860, val loss: 0.9662242531776428
Epoch 870, training loss: 0.629841148853302 = 0.009189741685986519 + 0.1 * 6.206514358520508
Epoch 870, val loss: 0.9702209234237671
Epoch 880, training loss: 0.6301304697990417 = 0.008920819498598576 + 0.1 * 6.212096214294434
Epoch 880, val loss: 0.9741626977920532
Epoch 890, training loss: 0.6296590566635132 = 0.008663131855428219 + 0.1 * 6.209959506988525
Epoch 890, val loss: 0.9780264496803284
Epoch 900, training loss: 0.6288588643074036 = 0.008418443612754345 + 0.1 * 6.204403877258301
Epoch 900, val loss: 0.981823742389679
Epoch 910, training loss: 0.6288920640945435 = 0.008184605278074741 + 0.1 * 6.2070746421813965
Epoch 910, val loss: 0.9855475425720215
Epoch 920, training loss: 0.6280401945114136 = 0.007960764691233635 + 0.1 * 6.200794219970703
Epoch 920, val loss: 0.9891839027404785
Epoch 930, training loss: 0.6281027793884277 = 0.007747228257358074 + 0.1 * 6.203555583953857
Epoch 930, val loss: 0.9928060173988342
Epoch 940, training loss: 0.6268425583839417 = 0.007543311454355717 + 0.1 * 6.192992210388184
Epoch 940, val loss: 0.9963260293006897
Epoch 950, training loss: 0.6264382004737854 = 0.007348051760345697 + 0.1 * 6.190901279449463
Epoch 950, val loss: 0.9998074173927307
Epoch 960, training loss: 0.6274042725563049 = 0.007160548120737076 + 0.1 * 6.202437400817871
Epoch 960, val loss: 1.0032435655593872
Epoch 970, training loss: 0.6263724565505981 = 0.006980618461966515 + 0.1 * 6.193918704986572
Epoch 970, val loss: 1.0066006183624268
Epoch 980, training loss: 0.628365159034729 = 0.006809317972511053 + 0.1 * 6.2155585289001465
Epoch 980, val loss: 1.0099080801010132
Epoch 990, training loss: 0.6254247426986694 = 0.006644077133387327 + 0.1 * 6.187806606292725
Epoch 990, val loss: 1.0132062435150146
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6494
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.790679693222046 = 1.9532948732376099 + 0.1 * 8.373847961425781
Epoch 0, val loss: 1.95599365234375
Epoch 10, training loss: 2.7796101570129395 = 1.942240834236145 + 0.1 * 8.373693466186523
Epoch 10, val loss: 1.9443438053131104
Epoch 20, training loss: 2.7655539512634277 = 1.9282660484313965 + 0.1 * 8.372879981994629
Epoch 20, val loss: 1.9293330907821655
Epoch 30, training loss: 2.7449917793273926 = 1.908258318901062 + 0.1 * 8.367335319519043
Epoch 30, val loss: 1.9078502655029297
Epoch 40, training loss: 2.712024211883545 = 1.8786187171936035 + 0.1 * 8.334053993225098
Epoch 40, val loss: 1.876502513885498
Epoch 50, training loss: 2.650141477584839 = 1.837910532951355 + 0.1 * 8.122308731079102
Epoch 50, val loss: 1.8352909088134766
Epoch 60, training loss: 2.5532729625701904 = 1.7919307947158813 + 0.1 * 7.613422393798828
Epoch 60, val loss: 1.7918291091918945
Epoch 70, training loss: 2.47337007522583 = 1.7465828657150269 + 0.1 * 7.267872333526611
Epoch 70, val loss: 1.7518779039382935
Epoch 80, training loss: 2.4038825035095215 = 1.6978981494903564 + 0.1 * 7.059842109680176
Epoch 80, val loss: 1.7109637260437012
Epoch 90, training loss: 2.333859443664551 = 1.6347110271453857 + 0.1 * 6.991482734680176
Epoch 90, val loss: 1.6574548482894897
Epoch 100, training loss: 2.2465553283691406 = 1.5524746179580688 + 0.1 * 6.940805912017822
Epoch 100, val loss: 1.589442253112793
Epoch 110, training loss: 2.1427266597747803 = 1.4517399072647095 + 0.1 * 6.909868240356445
Epoch 110, val loss: 1.5087530612945557
Epoch 120, training loss: 2.0283310413360596 = 1.339481234550476 + 0.1 * 6.888497829437256
Epoch 120, val loss: 1.4210150241851807
Epoch 130, training loss: 1.9103386402130127 = 1.2236099243164062 + 0.1 * 6.867286682128906
Epoch 130, val loss: 1.3325432538986206
Epoch 140, training loss: 1.7937123775482178 = 1.1096343994140625 + 0.1 * 6.840780258178711
Epoch 140, val loss: 1.246656894683838
Epoch 150, training loss: 1.6828662157058716 = 1.0019071102142334 + 0.1 * 6.809590816497803
Epoch 150, val loss: 1.165661096572876
Epoch 160, training loss: 1.5823113918304443 = 0.9042396545410156 + 0.1 * 6.780717372894287
Epoch 160, val loss: 1.0931637287139893
Epoch 170, training loss: 1.495121955871582 = 0.8191299438476562 + 0.1 * 6.759920597076416
Epoch 170, val loss: 1.0316033363342285
Epoch 180, training loss: 1.4222536087036133 = 0.7477782964706421 + 0.1 * 6.744753360748291
Epoch 180, val loss: 0.9818150401115417
Epoch 190, training loss: 1.3599059581756592 = 0.6873451471328735 + 0.1 * 6.725607872009277
Epoch 190, val loss: 0.9410361051559448
Epoch 200, training loss: 1.3055475950241089 = 0.6346470713615417 + 0.1 * 6.709004878997803
Epoch 200, val loss: 0.9069936871528625
Epoch 210, training loss: 1.2561285495758057 = 0.5868898034095764 + 0.1 * 6.692387580871582
Epoch 210, val loss: 0.8782241344451904
Epoch 220, training loss: 1.208813190460205 = 0.5414830446243286 + 0.1 * 6.673302173614502
Epoch 220, val loss: 0.852975606918335
Epoch 230, training loss: 1.1628345251083374 = 0.49735257029533386 + 0.1 * 6.654819965362549
Epoch 230, val loss: 0.8304752707481384
Epoch 240, training loss: 1.1177303791046143 = 0.4541405439376831 + 0.1 * 6.635898113250732
Epoch 240, val loss: 0.8101231455802917
Epoch 250, training loss: 1.0739972591400146 = 0.41193440556526184 + 0.1 * 6.6206278800964355
Epoch 250, val loss: 0.7917333245277405
Epoch 260, training loss: 1.0319608449935913 = 0.3716033399105072 + 0.1 * 6.603574752807617
Epoch 260, val loss: 0.7757347822189331
Epoch 270, training loss: 0.9918918013572693 = 0.33338499069213867 + 0.1 * 6.585068225860596
Epoch 270, val loss: 0.7627053260803223
Epoch 280, training loss: 0.9549472332000732 = 0.2980744540691376 + 0.1 * 6.568727970123291
Epoch 280, val loss: 0.752868115901947
Epoch 290, training loss: 0.9210047721862793 = 0.26574409008026123 + 0.1 * 6.552606582641602
Epoch 290, val loss: 0.745993971824646
Epoch 300, training loss: 0.8900448083877563 = 0.2362370789051056 + 0.1 * 6.538077354431152
Epoch 300, val loss: 0.7422575950622559
Epoch 310, training loss: 0.8645957112312317 = 0.20958465337753296 + 0.1 * 6.550110340118408
Epoch 310, val loss: 0.7416374683380127
Epoch 320, training loss: 0.8383698463439941 = 0.1860499382019043 + 0.1 * 6.523199081420898
Epoch 320, val loss: 0.7437753677368164
Epoch 330, training loss: 0.8158149123191833 = 0.165349543094635 + 0.1 * 6.504653453826904
Epoch 330, val loss: 0.7488906383514404
Epoch 340, training loss: 0.7974609136581421 = 0.14731232821941376 + 0.1 * 6.501485347747803
Epoch 340, val loss: 0.7565767765045166
Epoch 350, training loss: 0.7804305553436279 = 0.1317211240530014 + 0.1 * 6.487093925476074
Epoch 350, val loss: 0.7664999961853027
Epoch 360, training loss: 0.7661104798316956 = 0.11822152137756348 + 0.1 * 6.478889465332031
Epoch 360, val loss: 0.7782152891159058
Epoch 370, training loss: 0.7529639005661011 = 0.10654038190841675 + 0.1 * 6.464235305786133
Epoch 370, val loss: 0.7911068797111511
Epoch 380, training loss: 0.7422481775283813 = 0.09638869017362595 + 0.1 * 6.458594799041748
Epoch 380, val loss: 0.8048900961875916
Epoch 390, training loss: 0.7314792275428772 = 0.08752132952213287 + 0.1 * 6.439579010009766
Epoch 390, val loss: 0.8192945122718811
Epoch 400, training loss: 0.7239078879356384 = 0.07973037660121918 + 0.1 * 6.441774845123291
Epoch 400, val loss: 0.8340215086936951
Epoch 410, training loss: 0.7152609825134277 = 0.0728897824883461 + 0.1 * 6.423711776733398
Epoch 410, val loss: 0.8487049341201782
Epoch 420, training loss: 0.7098691463470459 = 0.06682506203651428 + 0.1 * 6.430440425872803
Epoch 420, val loss: 0.8634181618690491
Epoch 430, training loss: 0.7031875252723694 = 0.06144158914685249 + 0.1 * 6.417459011077881
Epoch 430, val loss: 0.8779029846191406
Epoch 440, training loss: 0.6966413259506226 = 0.056628309190273285 + 0.1 * 6.400130271911621
Epoch 440, val loss: 0.8921501040458679
Epoch 450, training loss: 0.6933637857437134 = 0.052303772419691086 + 0.1 * 6.410599708557129
Epoch 450, val loss: 0.9062005281448364
Epoch 460, training loss: 0.6869085431098938 = 0.04842934384942055 + 0.1 * 6.384791374206543
Epoch 460, val loss: 0.9197032451629639
Epoch 470, training loss: 0.6829257011413574 = 0.04493927210569382 + 0.1 * 6.37986421585083
Epoch 470, val loss: 0.9329811334609985
Epoch 480, training loss: 0.6784274578094482 = 0.04178673028945923 + 0.1 * 6.3664069175720215
Epoch 480, val loss: 0.9459508657455444
Epoch 490, training loss: 0.6749495267868042 = 0.03893755376338959 + 0.1 * 6.360119342803955
Epoch 490, val loss: 0.9585606455802917
Epoch 500, training loss: 0.6737552285194397 = 0.03635001927614212 + 0.1 * 6.374052047729492
Epoch 500, val loss: 0.9709216952323914
Epoch 510, training loss: 0.6693618893623352 = 0.03400390222668648 + 0.1 * 6.353579521179199
Epoch 510, val loss: 0.9828929901123047
Epoch 520, training loss: 0.6667367219924927 = 0.0318681076169014 + 0.1 * 6.348686218261719
Epoch 520, val loss: 0.994606077671051
Epoch 530, training loss: 0.6639518737792969 = 0.029922880232334137 + 0.1 * 6.34028959274292
Epoch 530, val loss: 1.0060839653015137
Epoch 540, training loss: 0.6626260876655579 = 0.02814645878970623 + 0.1 * 6.344796180725098
Epoch 540, val loss: 1.017240047454834
Epoch 550, training loss: 0.6609246134757996 = 0.02652166225016117 + 0.1 * 6.344029426574707
Epoch 550, val loss: 1.0281258821487427
Epoch 560, training loss: 0.6574192047119141 = 0.02503320574760437 + 0.1 * 6.323860168457031
Epoch 560, val loss: 1.0388026237487793
Epoch 570, training loss: 0.6565368175506592 = 0.023663561791181564 + 0.1 * 6.328732013702393
Epoch 570, val loss: 1.0492347478866577
Epoch 580, training loss: 0.6542906165122986 = 0.022404128685593605 + 0.1 * 6.318864822387695
Epoch 580, val loss: 1.0593843460083008
Epoch 590, training loss: 0.6529147624969482 = 0.02124534733593464 + 0.1 * 6.3166937828063965
Epoch 590, val loss: 1.0692307949066162
Epoch 600, training loss: 0.6508376002311707 = 0.02017567865550518 + 0.1 * 6.306618690490723
Epoch 600, val loss: 1.0788167715072632
Epoch 610, training loss: 0.6495416164398193 = 0.01918644830584526 + 0.1 * 6.30355167388916
Epoch 610, val loss: 1.0882370471954346
Epoch 620, training loss: 0.6476776599884033 = 0.018271367996931076 + 0.1 * 6.294062614440918
Epoch 620, val loss: 1.0973153114318848
Epoch 630, training loss: 0.6485394835472107 = 0.017421074211597443 + 0.1 * 6.311183929443359
Epoch 630, val loss: 1.1063294410705566
Epoch 640, training loss: 0.6456180810928345 = 0.016631772741675377 + 0.1 * 6.289862632751465
Epoch 640, val loss: 1.114992380142212
Epoch 650, training loss: 0.6439645886421204 = 0.01589755155146122 + 0.1 * 6.280670166015625
Epoch 650, val loss: 1.1235344409942627
Epoch 660, training loss: 0.644554853439331 = 0.015212144702672958 + 0.1 * 6.293427467346191
Epoch 660, val loss: 1.1318845748901367
Epoch 670, training loss: 0.642452597618103 = 0.014573387801647186 + 0.1 * 6.278791904449463
Epoch 670, val loss: 1.139968991279602
Epoch 680, training loss: 0.6417446136474609 = 0.013975773938000202 + 0.1 * 6.277688026428223
Epoch 680, val loss: 1.147905945777893
Epoch 690, training loss: 0.6404595375061035 = 0.01341493334621191 + 0.1 * 6.270446300506592
Epoch 690, val loss: 1.1556239128112793
Epoch 700, training loss: 0.639598548412323 = 0.012889930978417397 + 0.1 * 6.267086029052734
Epoch 700, val loss: 1.1632447242736816
Epoch 710, training loss: 0.6385533809661865 = 0.012399046681821346 + 0.1 * 6.261543273925781
Epoch 710, val loss: 1.1705551147460938
Epoch 720, training loss: 0.637475311756134 = 0.011936980299651623 + 0.1 * 6.255383491516113
Epoch 720, val loss: 1.1778076887130737
Epoch 730, training loss: 0.6383623480796814 = 0.011500130407512188 + 0.1 * 6.268621921539307
Epoch 730, val loss: 1.1848698854446411
Epoch 740, training loss: 0.6367263197898865 = 0.011088868603110313 + 0.1 * 6.256374835968018
Epoch 740, val loss: 1.1917994022369385
Epoch 750, training loss: 0.6369572877883911 = 0.0107014374807477 + 0.1 * 6.262558460235596
Epoch 750, val loss: 1.1985803842544556
Epoch 760, training loss: 0.6350901126861572 = 0.010335988365113735 + 0.1 * 6.2475409507751465
Epoch 760, val loss: 1.2052162885665894
Epoch 770, training loss: 0.6339272260665894 = 0.00999004952609539 + 0.1 * 6.2393717765808105
Epoch 770, val loss: 1.2117507457733154
Epoch 780, training loss: 0.6350029110908508 = 0.009661579504609108 + 0.1 * 6.253413200378418
Epoch 780, val loss: 1.21819007396698
Epoch 790, training loss: 0.6336308121681213 = 0.009350394830107689 + 0.1 * 6.242803573608398
Epoch 790, val loss: 1.224438190460205
Epoch 800, training loss: 0.6323283314704895 = 0.00905656162649393 + 0.1 * 6.232717514038086
Epoch 800, val loss: 1.2306016683578491
Epoch 810, training loss: 0.6316463351249695 = 0.008778304792940617 + 0.1 * 6.228679656982422
Epoch 810, val loss: 1.236594796180725
Epoch 820, training loss: 0.6312613487243652 = 0.008513283915817738 + 0.1 * 6.227480888366699
Epoch 820, val loss: 1.2424858808517456
Epoch 830, training loss: 0.6315070390701294 = 0.008260524831712246 + 0.1 * 6.232464790344238
Epoch 830, val loss: 1.248336672782898
Epoch 840, training loss: 0.6312578320503235 = 0.008020023815333843 + 0.1 * 6.232377529144287
Epoch 840, val loss: 1.2539852857589722
Epoch 850, training loss: 0.6300691366195679 = 0.007790912874042988 + 0.1 * 6.222782135009766
Epoch 850, val loss: 1.2596139907836914
Epoch 860, training loss: 0.6299313306808472 = 0.007572414353489876 + 0.1 * 6.223588943481445
Epoch 860, val loss: 1.2651333808898926
Epoch 870, training loss: 0.6290857791900635 = 0.007363785523921251 + 0.1 * 6.217219829559326
Epoch 870, val loss: 1.2705239057540894
Epoch 880, training loss: 0.6285537481307983 = 0.007164433132857084 + 0.1 * 6.213893413543701
Epoch 880, val loss: 1.275844693183899
Epoch 890, training loss: 0.6292535662651062 = 0.0069736298173666 + 0.1 * 6.222799301147461
Epoch 890, val loss: 1.2810940742492676
Epoch 900, training loss: 0.6282809376716614 = 0.00679171085357666 + 0.1 * 6.214892387390137
Epoch 900, val loss: 1.2862595319747925
Epoch 910, training loss: 0.6275305151939392 = 0.006617491599172354 + 0.1 * 6.20913028717041
Epoch 910, val loss: 1.2913154363632202
Epoch 920, training loss: 0.6273266077041626 = 0.0064512984827160835 + 0.1 * 6.208752632141113
Epoch 920, val loss: 1.2962242364883423
Epoch 930, training loss: 0.6264055967330933 = 0.00629181694239378 + 0.1 * 6.201138019561768
Epoch 930, val loss: 1.3011400699615479
Epoch 940, training loss: 0.6265406608581543 = 0.006138255354017019 + 0.1 * 6.204023838043213
Epoch 940, val loss: 1.3059529066085815
Epoch 950, training loss: 0.6274681091308594 = 0.005991010926663876 + 0.1 * 6.214771270751953
Epoch 950, val loss: 1.3107091188430786
Epoch 960, training loss: 0.6256064772605896 = 0.00584983266890049 + 0.1 * 6.197566032409668
Epoch 960, val loss: 1.3154677152633667
Epoch 970, training loss: 0.6272410750389099 = 0.005714517552405596 + 0.1 * 6.215265274047852
Epoch 970, val loss: 1.3200777769088745
Epoch 980, training loss: 0.6249414682388306 = 0.00558421341702342 + 0.1 * 6.193572044372559
Epoch 980, val loss: 1.3246746063232422
Epoch 990, training loss: 0.6243263483047485 = 0.005459195002913475 + 0.1 * 6.188671588897705
Epoch 990, val loss: 1.329190731048584
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6458
Flip ASR: 0.5956/225 nodes
The final ASR:0.63838, 0.01313, Accuracy:0.80988, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10540])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98401, 0.00627, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7855427265167236 = 1.9481548070907593 + 0.1 * 8.373878479003906
Epoch 0, val loss: 1.9498636722564697
Epoch 10, training loss: 2.775763750076294 = 1.9383894205093384 + 0.1 * 8.373743057250977
Epoch 10, val loss: 1.940069556236267
Epoch 20, training loss: 2.7635908126831055 = 1.9262840747833252 + 0.1 * 8.373067855834961
Epoch 20, val loss: 1.927796483039856
Epoch 30, training loss: 2.7457804679870605 = 1.9090008735656738 + 0.1 * 8.367796897888184
Epoch 30, val loss: 1.9103045463562012
Epoch 40, training loss: 2.714956283569336 = 1.8827543258666992 + 0.1 * 8.322019577026367
Epoch 40, val loss: 1.8843302726745605
Epoch 50, training loss: 2.641345262527466 = 1.8469831943511963 + 0.1 * 7.943619728088379
Epoch 50, val loss: 1.8513952493667603
Epoch 60, training loss: 2.5539169311523438 = 1.8111186027526855 + 0.1 * 7.42798376083374
Epoch 60, val loss: 1.8195680379867554
Epoch 70, training loss: 2.479604721069336 = 1.7766577005386353 + 0.1 * 7.029469966888428
Epoch 70, val loss: 1.789852261543274
Epoch 80, training loss: 2.4222612380981445 = 1.7401834726333618 + 0.1 * 6.820777893066406
Epoch 80, val loss: 1.7584644556045532
Epoch 90, training loss: 2.3666763305664062 = 1.6946014165878296 + 0.1 * 6.720747947692871
Epoch 90, val loss: 1.7179628610610962
Epoch 100, training loss: 2.299769163131714 = 1.633080244064331 + 0.1 * 6.666888236999512
Epoch 100, val loss: 1.6637656688690186
Epoch 110, training loss: 2.2180798053741455 = 1.5554845333099365 + 0.1 * 6.625953197479248
Epoch 110, val loss: 1.5983383655548096
Epoch 120, training loss: 2.127021074295044 = 1.4680826663970947 + 0.1 * 6.589384078979492
Epoch 120, val loss: 1.5255130529403687
Epoch 130, training loss: 2.037996768951416 = 1.3818750381469727 + 0.1 * 6.56121826171875
Epoch 130, val loss: 1.4562534093856812
Epoch 140, training loss: 1.9546740055084229 = 1.3005636930465698 + 0.1 * 6.541102409362793
Epoch 140, val loss: 1.3926664590835571
Epoch 150, training loss: 1.8759844303131104 = 1.2238776683807373 + 0.1 * 6.521068096160889
Epoch 150, val loss: 1.3349673748016357
Epoch 160, training loss: 1.8001329898834229 = 1.1493587493896484 + 0.1 * 6.507742881774902
Epoch 160, val loss: 1.2797168493270874
Epoch 170, training loss: 1.7245047092437744 = 1.0751287937164307 + 0.1 * 6.4937591552734375
Epoch 170, val loss: 1.2253668308258057
Epoch 180, training loss: 1.6476550102233887 = 0.9993459582328796 + 0.1 * 6.483089923858643
Epoch 180, val loss: 1.1700828075408936
Epoch 190, training loss: 1.570190191268921 = 0.9221968054771423 + 0.1 * 6.479934215545654
Epoch 190, val loss: 1.1140929460525513
Epoch 200, training loss: 1.4927486181259155 = 0.8460680246353149 + 0.1 * 6.466805934906006
Epoch 200, val loss: 1.0583972930908203
Epoch 210, training loss: 1.4173145294189453 = 0.7719802856445312 + 0.1 * 6.453342914581299
Epoch 210, val loss: 1.0035629272460938
Epoch 220, training loss: 1.3466876745224 = 0.7016962170600891 + 0.1 * 6.449914455413818
Epoch 220, val loss: 0.9517533779144287
Epoch 230, training loss: 1.2805535793304443 = 0.6371446251869202 + 0.1 * 6.434089183807373
Epoch 230, val loss: 0.9046803116798401
Epoch 240, training loss: 1.2213176488876343 = 0.578705906867981 + 0.1 * 6.426117420196533
Epoch 240, val loss: 0.8637252449989319
Epoch 250, training loss: 1.1689367294311523 = 0.5267134308815002 + 0.1 * 6.4222331047058105
Epoch 250, val loss: 0.8294260501861572
Epoch 260, training loss: 1.1226451396942139 = 0.48097044229507446 + 0.1 * 6.416746616363525
Epoch 260, val loss: 0.8019709587097168
Epoch 270, training loss: 1.0807117223739624 = 0.44089189171791077 + 0.1 * 6.39819860458374
Epoch 270, val loss: 0.7810897827148438
Epoch 280, training loss: 1.0443280935287476 = 0.4054355323314667 + 0.1 * 6.388926029205322
Epoch 280, val loss: 0.7655280232429504
Epoch 290, training loss: 1.0113544464111328 = 0.3734554648399353 + 0.1 * 6.378989219665527
Epoch 290, val loss: 0.7545069456100464
Epoch 300, training loss: 0.9818183183670044 = 0.3440229594707489 + 0.1 * 6.377954006195068
Epoch 300, val loss: 0.7466732263565063
Epoch 310, training loss: 0.9527115821838379 = 0.3164670467376709 + 0.1 * 6.36244535446167
Epoch 310, val loss: 0.7416336536407471
Epoch 320, training loss: 0.9285026788711548 = 0.2902149260044098 + 0.1 * 6.382876873016357
Epoch 320, val loss: 0.7383903861045837
Epoch 330, training loss: 0.9006741046905518 = 0.26511651277542114 + 0.1 * 6.355576038360596
Epoch 330, val loss: 0.736544668674469
Epoch 340, training loss: 0.8772361278533936 = 0.2409142553806305 + 0.1 * 6.363218784332275
Epoch 340, val loss: 0.735680341720581
Epoch 350, training loss: 0.8520092368125916 = 0.2177705019712448 + 0.1 * 6.3423871994018555
Epoch 350, val loss: 0.735619306564331
Epoch 360, training loss: 0.8289571404457092 = 0.19587720930576324 + 0.1 * 6.330799102783203
Epoch 360, val loss: 0.7364081740379333
Epoch 370, training loss: 0.8108465671539307 = 0.17558890581130981 + 0.1 * 6.352576732635498
Epoch 370, val loss: 0.738237738609314
Epoch 380, training loss: 0.7911718487739563 = 0.1573595553636551 + 0.1 * 6.338122844696045
Epoch 380, val loss: 0.7412365674972534
Epoch 390, training loss: 0.7735713720321655 = 0.1412445604801178 + 0.1 * 6.323267936706543
Epoch 390, val loss: 0.7455745935440063
Epoch 400, training loss: 0.7586345672607422 = 0.1271011233329773 + 0.1 * 6.315334320068359
Epoch 400, val loss: 0.7512653470039368
Epoch 410, training loss: 0.7471091747283936 = 0.11475244909524918 + 0.1 * 6.323566913604736
Epoch 410, val loss: 0.7582947015762329
Epoch 420, training loss: 0.7364923357963562 = 0.10400988906621933 + 0.1 * 6.324824333190918
Epoch 420, val loss: 0.7664582133293152
Epoch 430, training loss: 0.7247028350830078 = 0.0946645736694336 + 0.1 * 6.300382614135742
Epoch 430, val loss: 0.7755385041236877
Epoch 440, training loss: 0.7169790267944336 = 0.0864572525024414 + 0.1 * 6.305217742919922
Epoch 440, val loss: 0.7853344678878784
Epoch 450, training loss: 0.708948016166687 = 0.07923264801502228 + 0.1 * 6.297153949737549
Epoch 450, val loss: 0.7956556677818298
Epoch 460, training loss: 0.7020449042320251 = 0.07282900810241699 + 0.1 * 6.292159080505371
Epoch 460, val loss: 0.8063725233078003
Epoch 470, training loss: 0.6965969800949097 = 0.06711553782224655 + 0.1 * 6.294814109802246
Epoch 470, val loss: 0.8173182606697083
Epoch 480, training loss: 0.6903120279312134 = 0.062005311250686646 + 0.1 * 6.283066749572754
Epoch 480, val loss: 0.8284953236579895
Epoch 490, training loss: 0.6856471300125122 = 0.057409729808568954 + 0.1 * 6.282374382019043
Epoch 490, val loss: 0.8397364020347595
Epoch 500, training loss: 0.6826321482658386 = 0.0532800629734993 + 0.1 * 6.293520927429199
Epoch 500, val loss: 0.8509737849235535
Epoch 510, training loss: 0.6766131520271301 = 0.04956750199198723 + 0.1 * 6.270456314086914
Epoch 510, val loss: 0.8621748089790344
Epoch 520, training loss: 0.6734866499900818 = 0.0462031327188015 + 0.1 * 6.2728352546691895
Epoch 520, val loss: 0.8732464909553528
Epoch 530, training loss: 0.6696417331695557 = 0.04314890503883362 + 0.1 * 6.264928340911865
Epoch 530, val loss: 0.8842073082923889
Epoch 540, training loss: 0.6676682829856873 = 0.040374670177698135 + 0.1 * 6.27293586730957
Epoch 540, val loss: 0.8950549364089966
Epoch 550, training loss: 0.6641348600387573 = 0.03784295171499252 + 0.1 * 6.262918949127197
Epoch 550, val loss: 0.9057533144950867
Epoch 560, training loss: 0.6612391471862793 = 0.03552854061126709 + 0.1 * 6.257105827331543
Epoch 560, val loss: 0.91628098487854
Epoch 570, training loss: 0.6608117818832397 = 0.03340836241841316 + 0.1 * 6.274034023284912
Epoch 570, val loss: 0.9264612793922424
Epoch 580, training loss: 0.6562345027923584 = 0.031470637768507004 + 0.1 * 6.24763822555542
Epoch 580, val loss: 0.936581015586853
Epoch 590, training loss: 0.6540260314941406 = 0.02968890592455864 + 0.1 * 6.243371486663818
Epoch 590, val loss: 0.9464764595031738
Epoch 600, training loss: 0.6536858081817627 = 0.02804640308022499 + 0.1 * 6.256393909454346
Epoch 600, val loss: 0.9560598731040955
Epoch 610, training loss: 0.6505717635154724 = 0.02653607912361622 + 0.1 * 6.2403564453125
Epoch 610, val loss: 0.9655811190605164
Epoch 620, training loss: 0.6492055058479309 = 0.02513701654970646 + 0.1 * 6.240684986114502
Epoch 620, val loss: 0.9748692512512207
Epoch 630, training loss: 0.6483477354049683 = 0.023841802030801773 + 0.1 * 6.245059013366699
Epoch 630, val loss: 0.9838823080062866
Epoch 640, training loss: 0.6464221477508545 = 0.022642090916633606 + 0.1 * 6.237800598144531
Epoch 640, val loss: 0.9928472638130188
Epoch 650, training loss: 0.6457644104957581 = 0.02152719907462597 + 0.1 * 6.242372512817383
Epoch 650, val loss: 1.0015710592269897
Epoch 660, training loss: 0.6441690921783447 = 0.02049250155687332 + 0.1 * 6.2367658615112305
Epoch 660, val loss: 1.01006281375885
Epoch 670, training loss: 0.642713189125061 = 0.019530214369297028 + 0.1 * 6.2318291664123535
Epoch 670, val loss: 1.0183773040771484
Epoch 680, training loss: 0.6419921517372131 = 0.018634896725416183 + 0.1 * 6.233572006225586
Epoch 680, val loss: 1.0265730619430542
Epoch 690, training loss: 0.6404088139533997 = 0.017799023538827896 + 0.1 * 6.226097583770752
Epoch 690, val loss: 1.034484624862671
Epoch 700, training loss: 0.640673041343689 = 0.01701951213181019 + 0.1 * 6.23653507232666
Epoch 700, val loss: 1.0423223972320557
Epoch 710, training loss: 0.638358473777771 = 0.01629164069890976 + 0.1 * 6.220668315887451
Epoch 710, val loss: 1.0498929023742676
Epoch 720, training loss: 0.6377356648445129 = 0.015609224326908588 + 0.1 * 6.221264362335205
Epoch 720, val loss: 1.0574570894241333
Epoch 730, training loss: 0.6372172236442566 = 0.014967205934226513 + 0.1 * 6.222500324249268
Epoch 730, val loss: 1.0648332834243774
Epoch 740, training loss: 0.6373637318611145 = 0.014364906586706638 + 0.1 * 6.229988098144531
Epoch 740, val loss: 1.0718920230865479
Epoch 750, training loss: 0.635176420211792 = 0.01380106806755066 + 0.1 * 6.213753700256348
Epoch 750, val loss: 1.078983187675476
Epoch 760, training loss: 0.634682297706604 = 0.013268320821225643 + 0.1 * 6.214139938354492
Epoch 760, val loss: 1.0859400033950806
Epoch 770, training loss: 0.6339594125747681 = 0.012767030857503414 + 0.1 * 6.211924076080322
Epoch 770, val loss: 1.0926417112350464
Epoch 780, training loss: 0.6355509757995605 = 0.012294531799852848 + 0.1 * 6.232564449310303
Epoch 780, val loss: 1.0992289781570435
Epoch 790, training loss: 0.6327306628227234 = 0.011850829236209393 + 0.1 * 6.208797931671143
Epoch 790, val loss: 1.1056643724441528
Epoch 800, training loss: 0.6321013569831848 = 0.011430356651544571 + 0.1 * 6.206709384918213
Epoch 800, val loss: 1.1120858192443848
Epoch 810, training loss: 0.6325528025627136 = 0.01103221531957388 + 0.1 * 6.215205669403076
Epoch 810, val loss: 1.1182390451431274
Epoch 820, training loss: 0.6313474178314209 = 0.010656457394361496 + 0.1 * 6.2069091796875
Epoch 820, val loss: 1.1243245601654053
Epoch 830, training loss: 0.6324052214622498 = 0.010299675166606903 + 0.1 * 6.221055030822754
Epoch 830, val loss: 1.1302679777145386
Epoch 840, training loss: 0.630210280418396 = 0.009963259100914001 + 0.1 * 6.202469825744629
Epoch 840, val loss: 1.1361113786697388
Epoch 850, training loss: 0.6292780041694641 = 0.009642079472541809 + 0.1 * 6.196359157562256
Epoch 850, val loss: 1.1419304609298706
Epoch 860, training loss: 0.6291122436523438 = 0.009337245486676693 + 0.1 * 6.197750091552734
Epoch 860, val loss: 1.1474976539611816
Epoch 870, training loss: 0.6282385587692261 = 0.009048650972545147 + 0.1 * 6.191899299621582
Epoch 870, val loss: 1.1530165672302246
Epoch 880, training loss: 0.6289108991622925 = 0.008773009292781353 + 0.1 * 6.201378345489502
Epoch 880, val loss: 1.1585224866867065
Epoch 890, training loss: 0.6276829242706299 = 0.008510957472026348 + 0.1 * 6.1917195320129395
Epoch 890, val loss: 1.16374933719635
Epoch 900, training loss: 0.6281009912490845 = 0.00826145987957716 + 0.1 * 6.198395252227783
Epoch 900, val loss: 1.1690311431884766
Epoch 910, training loss: 0.6264213919639587 = 0.008023788221180439 + 0.1 * 6.183975696563721
Epoch 910, val loss: 1.1740895509719849
Epoch 920, training loss: 0.6266684532165527 = 0.007797073572874069 + 0.1 * 6.188714027404785
Epoch 920, val loss: 1.1791737079620361
Epoch 930, training loss: 0.6269911527633667 = 0.007580191362649202 + 0.1 * 6.194108963012695
Epoch 930, val loss: 1.1841118335723877
Epoch 940, training loss: 0.6258757710456848 = 0.0073735397309064865 + 0.1 * 6.185022354125977
Epoch 940, val loss: 1.1889057159423828
Epoch 950, training loss: 0.6249376535415649 = 0.0071760970167815685 + 0.1 * 6.177615165710449
Epoch 950, val loss: 1.1936583518981934
Epoch 960, training loss: 0.6249406933784485 = 0.006987194996327162 + 0.1 * 6.179534912109375
Epoch 960, val loss: 1.1983721256256104
Epoch 970, training loss: 0.6263664364814758 = 0.00680580222979188 + 0.1 * 6.195606231689453
Epoch 970, val loss: 1.2029606103897095
Epoch 980, training loss: 0.6271978616714478 = 0.0066325729712843895 + 0.1 * 6.205652713775635
Epoch 980, val loss: 1.2073709964752197
Epoch 990, training loss: 0.6241124272346497 = 0.006467227358371019 + 0.1 * 6.176451683044434
Epoch 990, val loss: 1.2117319107055664
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.5351
Flip ASR: 0.4533/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.782794713973999 = 1.9454032182693481 + 0.1 * 8.37391471862793
Epoch 0, val loss: 1.9522051811218262
Epoch 10, training loss: 2.7733585834503174 = 1.9359725713729858 + 0.1 * 8.373859405517578
Epoch 10, val loss: 1.9423871040344238
Epoch 20, training loss: 2.7620675563812256 = 1.9247099161148071 + 0.1 * 8.373576164245605
Epoch 20, val loss: 1.9306172132492065
Epoch 30, training loss: 2.7464263439178467 = 1.9092272520065308 + 0.1 * 8.371991157531738
Epoch 30, val loss: 1.9145605564117432
Epoch 40, training loss: 2.7224295139312744 = 1.886412501335144 + 0.1 * 8.360170364379883
Epoch 40, val loss: 1.8913050889968872
Epoch 50, training loss: 2.681629180908203 = 1.8530526161193848 + 0.1 * 8.2857666015625
Epoch 50, val loss: 1.8581914901733398
Epoch 60, training loss: 2.592392921447754 = 1.8108776807785034 + 0.1 * 7.815151214599609
Epoch 60, val loss: 1.8178497552871704
Epoch 70, training loss: 2.5138020515441895 = 1.7675944566726685 + 0.1 * 7.462075233459473
Epoch 70, val loss: 1.7776180505752563
Epoch 80, training loss: 2.439887762069702 = 1.7220555543899536 + 0.1 * 7.178321838378906
Epoch 80, val loss: 1.7357063293457031
Epoch 90, training loss: 2.3685381412506104 = 1.66567063331604 + 0.1 * 7.028674125671387
Epoch 90, val loss: 1.6847202777862549
Epoch 100, training loss: 2.2875277996063232 = 1.5914756059646606 + 0.1 * 6.960522174835205
Epoch 100, val loss: 1.6195749044418335
Epoch 110, training loss: 2.1896400451660156 = 1.496699571609497 + 0.1 * 6.929405212402344
Epoch 110, val loss: 1.5390931367874146
Epoch 120, training loss: 2.075927734375 = 1.3843474388122559 + 0.1 * 6.915803909301758
Epoch 120, val loss: 1.4450323581695557
Epoch 130, training loss: 1.9539494514465332 = 1.2631028890609741 + 0.1 * 6.908465385437012
Epoch 130, val loss: 1.345287799835205
Epoch 140, training loss: 1.8321053981781006 = 1.1418322324752808 + 0.1 * 6.902731895446777
Epoch 140, val loss: 1.2464888095855713
Epoch 150, training loss: 1.7159383296966553 = 1.0264462232589722 + 0.1 * 6.894921779632568
Epoch 150, val loss: 1.1539939641952515
Epoch 160, training loss: 1.6080806255340576 = 0.9199193716049194 + 0.1 * 6.881613254547119
Epoch 160, val loss: 1.0699057579040527
Epoch 170, training loss: 1.5098124742507935 = 0.8239172101020813 + 0.1 * 6.858952522277832
Epoch 170, val loss: 0.9955753087997437
Epoch 180, training loss: 1.4215524196624756 = 0.738690972328186 + 0.1 * 6.828614234924316
Epoch 180, val loss: 0.9316593408584595
Epoch 190, training loss: 1.3433881998062134 = 0.6639896035194397 + 0.1 * 6.793985843658447
Epoch 190, val loss: 0.8785127997398376
Epoch 200, training loss: 1.273146152496338 = 0.5971183776855469 + 0.1 * 6.760278224945068
Epoch 200, val loss: 0.8341695666313171
Epoch 210, training loss: 1.210230827331543 = 0.5363792181015015 + 0.1 * 6.738515377044678
Epoch 210, val loss: 0.7973011136054993
Epoch 220, training loss: 1.1519891023635864 = 0.48026782274246216 + 0.1 * 6.717212677001953
Epoch 220, val loss: 0.765716552734375
Epoch 230, training loss: 1.098219871520996 = 0.42804405093193054 + 0.1 * 6.701757907867432
Epoch 230, val loss: 0.7385768890380859
Epoch 240, training loss: 1.0489935874938965 = 0.3801536560058594 + 0.1 * 6.688399314880371
Epoch 240, val loss: 0.716026246547699
Epoch 250, training loss: 1.0046370029449463 = 0.3371277451515198 + 0.1 * 6.675091743469238
Epoch 250, val loss: 0.6985422372817993
Epoch 260, training loss: 0.966222882270813 = 0.298801064491272 + 0.1 * 6.67421817779541
Epoch 260, val loss: 0.6858522891998291
Epoch 270, training loss: 0.9313150644302368 = 0.2652725279331207 + 0.1 * 6.660424709320068
Epoch 270, val loss: 0.6780207753181458
Epoch 280, training loss: 0.9001790881156921 = 0.23573778569698334 + 0.1 * 6.644412994384766
Epoch 280, val loss: 0.6742062568664551
Epoch 290, training loss: 0.8741112947463989 = 0.2096184939146042 + 0.1 * 6.644927978515625
Epoch 290, val loss: 0.6737584471702576
Epoch 300, training loss: 0.8489835262298584 = 0.18685300648212433 + 0.1 * 6.621305465698242
Epoch 300, val loss: 0.6759706139564514
Epoch 310, training loss: 0.8275394439697266 = 0.16687847673892975 + 0.1 * 6.60660982131958
Epoch 310, val loss: 0.680424153804779
Epoch 320, training loss: 0.8102381229400635 = 0.1492539346218109 + 0.1 * 6.609841823577881
Epoch 320, val loss: 0.686791181564331
Epoch 330, training loss: 0.7924982905387878 = 0.13377751410007477 + 0.1 * 6.587207794189453
Epoch 330, val loss: 0.6944760680198669
Epoch 340, training loss: 0.7768275737762451 = 0.12013310194015503 + 0.1 * 6.566944599151611
Epoch 340, val loss: 0.7033393383026123
Epoch 350, training loss: 0.7639733552932739 = 0.10811606049537659 + 0.1 * 6.558572769165039
Epoch 350, val loss: 0.7130700349807739
Epoch 360, training loss: 0.7519240379333496 = 0.0975978821516037 + 0.1 * 6.543261528015137
Epoch 360, val loss: 0.7234352827072144
Epoch 370, training loss: 0.7410829663276672 = 0.08836503326892853 + 0.1 * 6.52717924118042
Epoch 370, val loss: 0.7342483401298523
Epoch 380, training loss: 0.7326235175132751 = 0.08021573722362518 + 0.1 * 6.524077415466309
Epoch 380, val loss: 0.7454795837402344
Epoch 390, training loss: 0.7235667109489441 = 0.0730590671300888 + 0.1 * 6.505075931549072
Epoch 390, val loss: 0.7568115592002869
Epoch 400, training loss: 0.7167943716049194 = 0.06674110889434814 + 0.1 * 6.500532627105713
Epoch 400, val loss: 0.7683044075965881
Epoch 410, training loss: 0.7096253633499146 = 0.06118056923151016 + 0.1 * 6.484447479248047
Epoch 410, val loss: 0.7797175645828247
Epoch 420, training loss: 0.7033945322036743 = 0.05625338479876518 + 0.1 * 6.471411228179932
Epoch 420, val loss: 0.7909095287322998
Epoch 430, training loss: 0.698379635810852 = 0.051849450916051865 + 0.1 * 6.465301990509033
Epoch 430, val loss: 0.8022851347923279
Epoch 440, training loss: 0.6937612891197205 = 0.04791949689388275 + 0.1 * 6.4584174156188965
Epoch 440, val loss: 0.8133352994918823
Epoch 450, training loss: 0.6897671222686768 = 0.04439878463745117 + 0.1 * 6.453683376312256
Epoch 450, val loss: 0.824475884437561
Epoch 460, training loss: 0.684924840927124 = 0.04124525561928749 + 0.1 * 6.436795234680176
Epoch 460, val loss: 0.8352449536323547
Epoch 470, training loss: 0.6809160709381104 = 0.03840857371687889 + 0.1 * 6.425074577331543
Epoch 470, val loss: 0.8458077907562256
Epoch 480, training loss: 0.6787382960319519 = 0.03583919256925583 + 0.1 * 6.428990840911865
Epoch 480, val loss: 0.856322705745697
Epoch 490, training loss: 0.6751391887664795 = 0.03352011367678642 + 0.1 * 6.4161906242370605
Epoch 490, val loss: 0.8666059374809265
Epoch 500, training loss: 0.6718367338180542 = 0.0314084030687809 + 0.1 * 6.404283046722412
Epoch 500, val loss: 0.8767057061195374
Epoch 510, training loss: 0.6702803373336792 = 0.02947903424501419 + 0.1 * 6.408012866973877
Epoch 510, val loss: 0.8866846561431885
Epoch 520, training loss: 0.6678222417831421 = 0.0277214627712965 + 0.1 * 6.401007652282715
Epoch 520, val loss: 0.8965192437171936
Epoch 530, training loss: 0.6644156575202942 = 0.026119546964764595 + 0.1 * 6.382961273193359
Epoch 530, val loss: 0.9060490131378174
Epoch 540, training loss: 0.6637301445007324 = 0.024651698768138885 + 0.1 * 6.39078426361084
Epoch 540, val loss: 0.9153861403465271
Epoch 550, training loss: 0.6618984341621399 = 0.023306438699364662 + 0.1 * 6.38592004776001
Epoch 550, val loss: 0.9244754910469055
Epoch 560, training loss: 0.6589764356613159 = 0.02207549288868904 + 0.1 * 6.369009494781494
Epoch 560, val loss: 0.9333392381668091
Epoch 570, training loss: 0.657053530216217 = 0.020938394591212273 + 0.1 * 6.361151218414307
Epoch 570, val loss: 0.9420999884605408
Epoch 580, training loss: 0.6556801795959473 = 0.019885992631316185 + 0.1 * 6.357941627502441
Epoch 580, val loss: 0.9507075548171997
Epoch 590, training loss: 0.6562992930412292 = 0.018912406638264656 + 0.1 * 6.373868465423584
Epoch 590, val loss: 0.9590136408805847
Epoch 600, training loss: 0.6529311537742615 = 0.018016140908002853 + 0.1 * 6.349149703979492
Epoch 600, val loss: 0.9672107696533203
Epoch 610, training loss: 0.650802731513977 = 0.017182307317852974 + 0.1 * 6.3362040519714355
Epoch 610, val loss: 0.9752420783042908
Epoch 620, training loss: 0.6508899927139282 = 0.01640482246875763 + 0.1 * 6.344851970672607
Epoch 620, val loss: 0.9830994606018066
Epoch 630, training loss: 0.6487165689468384 = 0.015681307762861252 + 0.1 * 6.330352306365967
Epoch 630, val loss: 0.9908650517463684
Epoch 640, training loss: 0.6472681760787964 = 0.01500561460852623 + 0.1 * 6.322625637054443
Epoch 640, val loss: 0.9983503818511963
Epoch 650, training loss: 0.6489635109901428 = 0.014375526458024979 + 0.1 * 6.345880031585693
Epoch 650, val loss: 1.0058256387710571
Epoch 660, training loss: 0.6461024284362793 = 0.013787500560283661 + 0.1 * 6.32314920425415
Epoch 660, val loss: 1.012941598892212
Epoch 670, training loss: 0.6444153785705566 = 0.013235882855951786 + 0.1 * 6.311795234680176
Epoch 670, val loss: 1.0200742483139038
Epoch 680, training loss: 0.6442779898643494 = 0.012717525474727154 + 0.1 * 6.315604209899902
Epoch 680, val loss: 1.0269743204116821
Epoch 690, training loss: 0.6436079740524292 = 0.012232296168804169 + 0.1 * 6.313756465911865
Epoch 690, val loss: 1.0337594747543335
Epoch 700, training loss: 0.6414536237716675 = 0.01177640538662672 + 0.1 * 6.296772003173828
Epoch 700, val loss: 1.0403826236724854
Epoch 710, training loss: 0.6423410773277283 = 0.011346719227731228 + 0.1 * 6.309943675994873
Epoch 710, val loss: 1.046922206878662
Epoch 720, training loss: 0.6411831378936768 = 0.010943260043859482 + 0.1 * 6.302398681640625
Epoch 720, val loss: 1.0532580614089966
Epoch 730, training loss: 0.639481782913208 = 0.010561793111264706 + 0.1 * 6.289200305938721
Epoch 730, val loss: 1.0595004558563232
Epoch 740, training loss: 0.6395473480224609 = 0.010200049728155136 + 0.1 * 6.293473243713379
Epoch 740, val loss: 1.0656965970993042
Epoch 750, training loss: 0.6380640268325806 = 0.00985789205878973 + 0.1 * 6.2820611000061035
Epoch 750, val loss: 1.0716781616210938
Epoch 760, training loss: 0.6382614970207214 = 0.009534397162497044 + 0.1 * 6.287270545959473
Epoch 760, val loss: 1.0776655673980713
Epoch 770, training loss: 0.6381857991218567 = 0.009229032322764397 + 0.1 * 6.289567947387695
Epoch 770, val loss: 1.0834088325500488
Epoch 780, training loss: 0.6358652114868164 = 0.008939091116189957 + 0.1 * 6.269261360168457
Epoch 780, val loss: 1.089070439338684
Epoch 790, training loss: 0.6370867490768433 = 0.008663102984428406 + 0.1 * 6.284235954284668
Epoch 790, val loss: 1.0947096347808838
Epoch 800, training loss: 0.6367937922477722 = 0.008400600403547287 + 0.1 * 6.283931732177734
Epoch 800, val loss: 1.1001615524291992
Epoch 810, training loss: 0.6354586482048035 = 0.008151602931320667 + 0.1 * 6.273070335388184
Epoch 810, val loss: 1.1055659055709839
Epoch 820, training loss: 0.6346719861030579 = 0.007914548739790916 + 0.1 * 6.267574310302734
Epoch 820, val loss: 1.1108828783035278
Epoch 830, training loss: 0.6337438225746155 = 0.007688155397772789 + 0.1 * 6.260556697845459
Epoch 830, val loss: 1.1161744594573975
Epoch 840, training loss: 0.6353114247322083 = 0.007471596356481314 + 0.1 * 6.278398036956787
Epoch 840, val loss: 1.1213124990463257
Epoch 850, training loss: 0.633192777633667 = 0.007266840431839228 + 0.1 * 6.259259223937988
Epoch 850, val loss: 1.1263623237609863
Epoch 860, training loss: 0.6324324011802673 = 0.00707039050757885 + 0.1 * 6.253620147705078
Epoch 860, val loss: 1.1313642263412476
Epoch 870, training loss: 0.6347454786300659 = 0.006882349960505962 + 0.1 * 6.27863073348999
Epoch 870, val loss: 1.1362859010696411
Epoch 880, training loss: 0.6315955519676208 = 0.006702841725200415 + 0.1 * 6.248927116394043
Epoch 880, val loss: 1.1411523818969727
Epoch 890, training loss: 0.632565975189209 = 0.006531302817165852 + 0.1 * 6.260346412658691
Epoch 890, val loss: 1.1459002494812012
Epoch 900, training loss: 0.6310009360313416 = 0.006366963963955641 + 0.1 * 6.246339797973633
Epoch 900, val loss: 1.1505167484283447
Epoch 910, training loss: 0.6307423114776611 = 0.006209213752299547 + 0.1 * 6.245330810546875
Epoch 910, val loss: 1.1551506519317627
Epoch 920, training loss: 0.6295042037963867 = 0.006057782098650932 + 0.1 * 6.234464168548584
Epoch 920, val loss: 1.1596661806106567
Epoch 930, training loss: 0.629564106464386 = 0.005912747234106064 + 0.1 * 6.236513614654541
Epoch 930, val loss: 1.1641501188278198
Epoch 940, training loss: 0.6304476261138916 = 0.005773282144218683 + 0.1 * 6.246743202209473
Epoch 940, val loss: 1.168553352355957
Epoch 950, training loss: 0.6287136673927307 = 0.005639192182570696 + 0.1 * 6.2307448387146
Epoch 950, val loss: 1.1729047298431396
Epoch 960, training loss: 0.6295611262321472 = 0.005510337185114622 + 0.1 * 6.240508079528809
Epoch 960, val loss: 1.1772427558898926
Epoch 970, training loss: 0.6281424164772034 = 0.005386310629546642 + 0.1 * 6.227560997009277
Epoch 970, val loss: 1.1813610792160034
Epoch 980, training loss: 0.6285538673400879 = 0.005267007276415825 + 0.1 * 6.232868194580078
Epoch 980, val loss: 1.1855155229568481
Epoch 990, training loss: 0.6288060545921326 = 0.005152055993676186 + 0.1 * 6.236539840698242
Epoch 990, val loss: 1.1896766424179077
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8745
Flip ASR: 0.8489/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7935729026794434 = 1.9561834335327148 + 0.1 * 8.373894691467285
Epoch 0, val loss: 1.9641673564910889
Epoch 10, training loss: 2.7831709384918213 = 1.9457905292510986 + 0.1 * 8.37380313873291
Epoch 10, val loss: 1.9529796838760376
Epoch 20, training loss: 2.770751714706421 = 1.9334259033203125 + 0.1 * 8.373257637023926
Epoch 20, val loss: 1.9392354488372803
Epoch 30, training loss: 2.753371000289917 = 1.9163742065429688 + 0.1 * 8.36996841430664
Epoch 30, val loss: 1.9199057817459106
Epoch 40, training loss: 2.726047992706299 = 1.891161561012268 + 0.1 * 8.348865509033203
Epoch 40, val loss: 1.8914129734039307
Epoch 50, training loss: 2.677835464477539 = 1.8551524877548218 + 0.1 * 8.226829528808594
Epoch 50, val loss: 1.8520320653915405
Epoch 60, training loss: 2.5965919494628906 = 1.8128000497817993 + 0.1 * 7.837918281555176
Epoch 60, val loss: 1.808312177658081
Epoch 70, training loss: 2.5125083923339844 = 1.7710901498794556 + 0.1 * 7.414181232452393
Epoch 70, val loss: 1.7670618295669556
Epoch 80, training loss: 2.440520763397217 = 1.729933261871338 + 0.1 * 7.105874061584473
Epoch 80, val loss: 1.728850245475769
Epoch 90, training loss: 2.374566078186035 = 1.6824047565460205 + 0.1 * 6.92161226272583
Epoch 90, val loss: 1.6870887279510498
Epoch 100, training loss: 2.3036105632781982 = 1.6202070713043213 + 0.1 * 6.834034442901611
Epoch 100, val loss: 1.6351464986801147
Epoch 110, training loss: 2.2188735008239746 = 1.5399001836776733 + 0.1 * 6.789731979370117
Epoch 110, val loss: 1.5705335140228271
Epoch 120, training loss: 2.121514320373535 = 1.4448223114013672 + 0.1 * 6.76692008972168
Epoch 120, val loss: 1.495982050895691
Epoch 130, training loss: 2.0201570987701416 = 1.3449041843414307 + 0.1 * 6.752529621124268
Epoch 130, val loss: 1.4196842908859253
Epoch 140, training loss: 1.9204843044281006 = 1.2464299201965332 + 0.1 * 6.740543365478516
Epoch 140, val loss: 1.3472458124160767
Epoch 150, training loss: 1.8225231170654297 = 1.149417757987976 + 0.1 * 6.731053829193115
Epoch 150, val loss: 1.2771577835083008
Epoch 160, training loss: 1.7252371311187744 = 1.0533924102783203 + 0.1 * 6.718446731567383
Epoch 160, val loss: 1.2079637050628662
Epoch 170, training loss: 1.6281726360321045 = 0.9574958086013794 + 0.1 * 6.706768989562988
Epoch 170, val loss: 1.1387101411819458
Epoch 180, training loss: 1.533156156539917 = 0.8633482456207275 + 0.1 * 6.698078632354736
Epoch 180, val loss: 1.0716395378112793
Epoch 190, training loss: 1.4433510303497314 = 0.7750084400177002 + 0.1 * 6.683424949645996
Epoch 190, val loss: 1.0096712112426758
Epoch 200, training loss: 1.3610397577285767 = 0.6944889426231384 + 0.1 * 6.665507793426514
Epoch 200, val loss: 0.9536018371582031
Epoch 210, training loss: 1.2884128093719482 = 0.6233545541763306 + 0.1 * 6.650582313537598
Epoch 210, val loss: 0.9048115015029907
Epoch 220, training loss: 1.2252130508422852 = 0.5611664652824402 + 0.1 * 6.640465259552002
Epoch 220, val loss: 0.8634254336357117
Epoch 230, training loss: 1.167680263519287 = 0.5055303573608398 + 0.1 * 6.621499061584473
Epoch 230, val loss: 0.8277852535247803
Epoch 240, training loss: 1.1170297861099243 = 0.4552520215511322 + 0.1 * 6.6177778244018555
Epoch 240, val loss: 0.7974175810813904
Epoch 250, training loss: 1.0694432258605957 = 0.4099557101726532 + 0.1 * 6.594874858856201
Epoch 250, val loss: 0.7719710469245911
Epoch 260, training loss: 1.026172399520874 = 0.3683827519416809 + 0.1 * 6.577896595001221
Epoch 260, val loss: 0.7498413920402527
Epoch 270, training loss: 0.9862246513366699 = 0.3298282325267792 + 0.1 * 6.563964366912842
Epoch 270, val loss: 0.7301563620567322
Epoch 280, training loss: 0.9515879154205322 = 0.29459527134895325 + 0.1 * 6.5699262619018555
Epoch 280, val loss: 0.7134061455726624
Epoch 290, training loss: 0.9171353578567505 = 0.26292648911476135 + 0.1 * 6.542088985443115
Epoch 290, val loss: 0.699289083480835
Epoch 300, training loss: 0.8870267868041992 = 0.234122171998024 + 0.1 * 6.529046058654785
Epoch 300, val loss: 0.6875951290130615
Epoch 310, training loss: 0.8612440824508667 = 0.20813396573066711 + 0.1 * 6.531101226806641
Epoch 310, val loss: 0.678521454334259
Epoch 320, training loss: 0.8354734182357788 = 0.1852363646030426 + 0.1 * 6.502370357513428
Epoch 320, val loss: 0.6723136305809021
Epoch 330, training loss: 0.8140410780906677 = 0.16509293019771576 + 0.1 * 6.489481449127197
Epoch 330, val loss: 0.6688932776451111
Epoch 340, training loss: 0.8017802834510803 = 0.14748196303844452 + 0.1 * 6.542983055114746
Epoch 340, val loss: 0.6680485010147095
Epoch 350, training loss: 0.7796709537506104 = 0.13241681456565857 + 0.1 * 6.472540855407715
Epoch 350, val loss: 0.6695874333381653
Epoch 360, training loss: 0.7655142545700073 = 0.11937882751226425 + 0.1 * 6.4613542556762695
Epoch 360, val loss: 0.6728941202163696
Epoch 370, training loss: 0.7528729438781738 = 0.10798134654760361 + 0.1 * 6.448915958404541
Epoch 370, val loss: 0.6776725649833679
Epoch 380, training loss: 0.7424097061157227 = 0.09794362634420395 + 0.1 * 6.4446611404418945
Epoch 380, val loss: 0.6836817860603333
Epoch 390, training loss: 0.7332969903945923 = 0.08914805203676224 + 0.1 * 6.441489219665527
Epoch 390, val loss: 0.6905575394630432
Epoch 400, training loss: 0.7238600254058838 = 0.08141221851110458 + 0.1 * 6.424478054046631
Epoch 400, val loss: 0.6982016563415527
Epoch 410, training loss: 0.7180371880531311 = 0.07454060763120651 + 0.1 * 6.43496561050415
Epoch 410, val loss: 0.706283688545227
Epoch 420, training loss: 0.7095454931259155 = 0.06843487918376923 + 0.1 * 6.411106109619141
Epoch 420, val loss: 0.7147391438484192
Epoch 430, training loss: 0.7041300535202026 = 0.06297799199819565 + 0.1 * 6.411520957946777
Epoch 430, val loss: 0.7235218286514282
Epoch 440, training loss: 0.6987113356590271 = 0.05810920149087906 + 0.1 * 6.4060211181640625
Epoch 440, val loss: 0.7323930263519287
Epoch 450, training loss: 0.6933664679527283 = 0.05374665558338165 + 0.1 * 6.396198272705078
Epoch 450, val loss: 0.741408109664917
Epoch 460, training loss: 0.6906868815422058 = 0.0498172752559185 + 0.1 * 6.408696174621582
Epoch 460, val loss: 0.7504225373268127
Epoch 470, training loss: 0.685196578502655 = 0.04628617689013481 + 0.1 * 6.389103889465332
Epoch 470, val loss: 0.7594795227050781
Epoch 480, training loss: 0.6810464859008789 = 0.04309749975800514 + 0.1 * 6.379489898681641
Epoch 480, val loss: 0.7685455679893494
Epoch 490, training loss: 0.6774376034736633 = 0.04021201282739639 + 0.1 * 6.372255325317383
Epoch 490, val loss: 0.7775447368621826
Epoch 500, training loss: 0.674568772315979 = 0.037598781287670135 + 0.1 * 6.369699478149414
Epoch 500, val loss: 0.786533772945404
Epoch 510, training loss: 0.6709896326065063 = 0.03521708771586418 + 0.1 * 6.357725143432617
Epoch 510, val loss: 0.7954772114753723
Epoch 520, training loss: 0.6731487512588501 = 0.03304114192724228 + 0.1 * 6.401076316833496
Epoch 520, val loss: 0.8043373823165894
Epoch 530, training loss: 0.6671767830848694 = 0.031062984839081764 + 0.1 * 6.361138343811035
Epoch 530, val loss: 0.8130651116371155
Epoch 540, training loss: 0.664645254611969 = 0.029258890077471733 + 0.1 * 6.35386323928833
Epoch 540, val loss: 0.8216778039932251
Epoch 550, training loss: 0.6617690324783325 = 0.027605826035141945 + 0.1 * 6.341631889343262
Epoch 550, val loss: 0.8301489353179932
Epoch 560, training loss: 0.662564754486084 = 0.026084257289767265 + 0.1 * 6.364804744720459
Epoch 560, val loss: 0.8384904861450195
Epoch 570, training loss: 0.6585832238197327 = 0.024686958640813828 + 0.1 * 6.338962554931641
Epoch 570, val loss: 0.8466429710388184
Epoch 580, training loss: 0.6591819524765015 = 0.023397451266646385 + 0.1 * 6.357844829559326
Epoch 580, val loss: 0.8547446727752686
Epoch 590, training loss: 0.656069278717041 = 0.022210627794265747 + 0.1 * 6.338586330413818
Epoch 590, val loss: 0.8625656366348267
Epoch 600, training loss: 0.6529245972633362 = 0.021113064140081406 + 0.1 * 6.318115711212158
Epoch 600, val loss: 0.8703184723854065
Epoch 610, training loss: 0.6523631811141968 = 0.02009301446378231 + 0.1 * 6.322701454162598
Epoch 610, val loss: 0.8779124617576599
Epoch 620, training loss: 0.650926411151886 = 0.019146621227264404 + 0.1 * 6.317797660827637
Epoch 620, val loss: 0.8852629661560059
Epoch 630, training loss: 0.6495110392570496 = 0.018267856910824776 + 0.1 * 6.312431812286377
Epoch 630, val loss: 0.8925559520721436
Epoch 640, training loss: 0.6482837200164795 = 0.017449647188186646 + 0.1 * 6.308340549468994
Epoch 640, val loss: 0.8997148871421814
Epoch 650, training loss: 0.6486184597015381 = 0.016687463968992233 + 0.1 * 6.319310188293457
Epoch 650, val loss: 0.9066643118858337
Epoch 660, training loss: 0.6463453769683838 = 0.015975797548890114 + 0.1 * 6.3036956787109375
Epoch 660, val loss: 0.9134782552719116
Epoch 670, training loss: 0.6456666588783264 = 0.015309986658394337 + 0.1 * 6.3035664558410645
Epoch 670, val loss: 0.9201927185058594
Epoch 680, training loss: 0.6438536047935486 = 0.01468659844249487 + 0.1 * 6.291670322418213
Epoch 680, val loss: 0.9267125725746155
Epoch 690, training loss: 0.6434264779090881 = 0.014101400040090084 + 0.1 * 6.293251037597656
Epoch 690, val loss: 0.933154821395874
Epoch 700, training loss: 0.6429722309112549 = 0.013552455231547356 + 0.1 * 6.294198036193848
Epoch 700, val loss: 0.9394537806510925
Epoch 710, training loss: 0.6411520838737488 = 0.013036438263952732 + 0.1 * 6.281156539916992
Epoch 710, val loss: 0.9456303119659424
Epoch 720, training loss: 0.6405734419822693 = 0.012551623396575451 + 0.1 * 6.280218124389648
Epoch 720, val loss: 0.9516965746879578
Epoch 730, training loss: 0.6412659883499146 = 0.012094118632376194 + 0.1 * 6.291718482971191
Epoch 730, val loss: 0.9576389789581299
Epoch 740, training loss: 0.639516294002533 = 0.01166325993835926 + 0.1 * 6.278530120849609
Epoch 740, val loss: 0.9634304642677307
Epoch 750, training loss: 0.6388000845909119 = 0.011256242170929909 + 0.1 * 6.2754387855529785
Epoch 750, val loss: 0.9691616296768188
Epoch 760, training loss: 0.6375313401222229 = 0.01087126787751913 + 0.1 * 6.266600131988525
Epoch 760, val loss: 0.9747385382652283
Epoch 770, training loss: 0.637102484703064 = 0.010507299564778805 + 0.1 * 6.265952110290527
Epoch 770, val loss: 0.9802465438842773
Epoch 780, training loss: 0.6374000310897827 = 0.010161583311855793 + 0.1 * 6.272384166717529
Epoch 780, val loss: 0.985687792301178
Epoch 790, training loss: 0.6368616819381714 = 0.00983473565429449 + 0.1 * 6.270269870758057
Epoch 790, val loss: 0.9909611344337463
Epoch 800, training loss: 0.6351504325866699 = 0.00952488835901022 + 0.1 * 6.256255626678467
Epoch 800, val loss: 0.9961680173873901
Epoch 810, training loss: 0.634475588798523 = 0.009230886586010456 + 0.1 * 6.25244665145874
Epoch 810, val loss: 1.001291275024414
Epoch 820, training loss: 0.6358848810195923 = 0.008950541727244854 + 0.1 * 6.269343376159668
Epoch 820, val loss: 1.0063170194625854
Epoch 830, training loss: 0.6344156265258789 = 0.008684000931680202 + 0.1 * 6.257316589355469
Epoch 830, val loss: 1.011229157447815
Epoch 840, training loss: 0.632983922958374 = 0.008430934511125088 + 0.1 * 6.245529651641846
Epoch 840, val loss: 1.0160707235336304
Epoch 850, training loss: 0.632843554019928 = 0.008188911713659763 + 0.1 * 6.246546268463135
Epoch 850, val loss: 1.0208860635757446
Epoch 860, training loss: 0.6352138519287109 = 0.007957523688673973 + 0.1 * 6.2725629806518555
Epoch 860, val loss: 1.0255472660064697
Epoch 870, training loss: 0.6320832371711731 = 0.007737983949482441 + 0.1 * 6.243452548980713
Epoch 870, val loss: 1.030100703239441
Epoch 880, training loss: 0.6337187886238098 = 0.007528230082243681 + 0.1 * 6.261905193328857
Epoch 880, val loss: 1.0346758365631104
Epoch 890, training loss: 0.6312148571014404 = 0.007328033447265625 + 0.1 * 6.238868236541748
Epoch 890, val loss: 1.0390812158584595
Epoch 900, training loss: 0.6311861872673035 = 0.00713626854121685 + 0.1 * 6.240499019622803
Epoch 900, val loss: 1.0434578657150269
Epoch 910, training loss: 0.6305395364761353 = 0.006952477153390646 + 0.1 * 6.235870838165283
Epoch 910, val loss: 1.047767162322998
Epoch 920, training loss: 0.6309726238250732 = 0.006776173133403063 + 0.1 * 6.241964340209961
Epoch 920, val loss: 1.051963210105896
Epoch 930, training loss: 0.6296766400337219 = 0.006607867777347565 + 0.1 * 6.230687141418457
Epoch 930, val loss: 1.0560976266860962
Epoch 940, training loss: 0.6308873295783997 = 0.006446046754717827 + 0.1 * 6.244412422180176
Epoch 940, val loss: 1.0602251291275024
Epoch 950, training loss: 0.6296260356903076 = 0.006291018333286047 + 0.1 * 6.233349800109863
Epoch 950, val loss: 1.064255952835083
Epoch 960, training loss: 0.6289639472961426 = 0.006141949910670519 + 0.1 * 6.228219509124756
Epoch 960, val loss: 1.0682520866394043
Epoch 970, training loss: 0.6285192966461182 = 0.005998818203806877 + 0.1 * 6.225204944610596
Epoch 970, val loss: 1.0721794366836548
Epoch 980, training loss: 0.6287402510643005 = 0.005860586650669575 + 0.1 * 6.228796482086182
Epoch 980, val loss: 1.0760589838027954
Epoch 990, training loss: 0.6284707188606262 = 0.005728173069655895 + 0.1 * 6.2274250984191895
Epoch 990, val loss: 1.0798307657241821
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.79090, 0.18449, Accuracy:0.81605, 0.01944
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11564])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10496])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98647, 0.00870, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.77886700630188 = 1.9414843320846558 + 0.1 * 8.37382698059082
Epoch 0, val loss: 1.933671474456787
Epoch 10, training loss: 2.768739700317383 = 1.9313732385635376 + 0.1 * 8.373663902282715
Epoch 10, val loss: 1.923954963684082
Epoch 20, training loss: 2.755763053894043 = 1.9184720516204834 + 0.1 * 8.372910499572754
Epoch 20, val loss: 1.9116687774658203
Epoch 30, training loss: 2.736382484436035 = 1.8995815515518188 + 0.1 * 8.368009567260742
Epoch 30, val loss: 1.8941147327423096
Epoch 40, training loss: 2.7044689655303955 = 1.8710006475448608 + 0.1 * 8.33468246459961
Epoch 40, val loss: 1.8686652183532715
Epoch 50, training loss: 2.6447393894195557 = 1.8330250978469849 + 0.1 * 8.117143630981445
Epoch 50, val loss: 1.8369956016540527
Epoch 60, training loss: 2.5706520080566406 = 1.7941523790359497 + 0.1 * 7.764997482299805
Epoch 60, val loss: 1.806257963180542
Epoch 70, training loss: 2.493288516998291 = 1.7579536437988281 + 0.1 * 7.353349208831787
Epoch 70, val loss: 1.7777916193008423
Epoch 80, training loss: 2.417330503463745 = 1.7204129695892334 + 0.1 * 6.969174385070801
Epoch 80, val loss: 1.7459112405776978
Epoch 90, training loss: 2.350458860397339 = 1.6691759824752808 + 0.1 * 6.812828063964844
Epoch 90, val loss: 1.699905514717102
Epoch 100, training loss: 2.273343563079834 = 1.6004490852355957 + 0.1 * 6.728944301605225
Epoch 100, val loss: 1.6421191692352295
Epoch 110, training loss: 2.189279556274414 = 1.5219359397888184 + 0.1 * 6.673435688018799
Epoch 110, val loss: 1.5797406435012817
Epoch 120, training loss: 2.106417655944824 = 1.4428952932357788 + 0.1 * 6.635222911834717
Epoch 120, val loss: 1.5161265134811401
Epoch 130, training loss: 2.029798984527588 = 1.3685075044631958 + 0.1 * 6.612914085388184
Epoch 130, val loss: 1.4595322608947754
Epoch 140, training loss: 1.959383249282837 = 1.3010895252227783 + 0.1 * 6.582936763763428
Epoch 140, val loss: 1.4105044603347778
Epoch 150, training loss: 1.8940989971160889 = 1.2383767366409302 + 0.1 * 6.55722188949585
Epoch 150, val loss: 1.36619234085083
Epoch 160, training loss: 1.8340404033660889 = 1.1797856092453003 + 0.1 * 6.542548179626465
Epoch 160, val loss: 1.3276615142822266
Epoch 170, training loss: 1.7771782875061035 = 1.125157117843628 + 0.1 * 6.520212173461914
Epoch 170, val loss: 1.293631911277771
Epoch 180, training loss: 1.7216265201568604 = 1.0715546607971191 + 0.1 * 6.500718116760254
Epoch 180, val loss: 1.261116623878479
Epoch 190, training loss: 1.6667582988739014 = 1.0180659294128418 + 0.1 * 6.486922740936279
Epoch 190, val loss: 1.2282352447509766
Epoch 200, training loss: 1.6136891841888428 = 0.9662986397743225 + 0.1 * 6.47390604019165
Epoch 200, val loss: 1.1962529420852661
Epoch 210, training loss: 1.5629160404205322 = 0.9167266488075256 + 0.1 * 6.461894512176514
Epoch 210, val loss: 1.1649367809295654
Epoch 220, training loss: 1.5146973133087158 = 0.8683342337608337 + 0.1 * 6.4636311531066895
Epoch 220, val loss: 1.1333918571472168
Epoch 230, training loss: 1.4645140171051025 = 0.8197645545005798 + 0.1 * 6.447494983673096
Epoch 230, val loss: 1.100662112236023
Epoch 240, training loss: 1.4121606349945068 = 0.7685343623161316 + 0.1 * 6.436263084411621
Epoch 240, val loss: 1.064712643623352
Epoch 250, training loss: 1.3568625450134277 = 0.7135388851165771 + 0.1 * 6.433236598968506
Epoch 250, val loss: 1.025217890739441
Epoch 260, training loss: 1.2978042364120483 = 0.6555481553077698 + 0.1 * 6.422560691833496
Epoch 260, val loss: 0.9837542772293091
Epoch 270, training loss: 1.2388272285461426 = 0.596303403377533 + 0.1 * 6.425238609313965
Epoch 270, val loss: 0.9424224495887756
Epoch 280, training loss: 1.180080533027649 = 0.5389938950538635 + 0.1 * 6.4108662605285645
Epoch 280, val loss: 0.9049817323684692
Epoch 290, training loss: 1.127092719078064 = 0.48541003465652466 + 0.1 * 6.4168267250061035
Epoch 290, val loss: 0.8727770447731018
Epoch 300, training loss: 1.076265573501587 = 0.43649569153785706 + 0.1 * 6.397698402404785
Epoch 300, val loss: 0.8465688228607178
Epoch 310, training loss: 1.0325636863708496 = 0.391816645860672 + 0.1 * 6.407470226287842
Epoch 310, val loss: 0.8252707719802856
Epoch 320, training loss: 0.989959716796875 = 0.35123857855796814 + 0.1 * 6.387210845947266
Epoch 320, val loss: 0.8085514903068542
Epoch 330, training loss: 0.9529154300689697 = 0.3140786290168762 + 0.1 * 6.388367652893066
Epoch 330, val loss: 0.7954570651054382
Epoch 340, training loss: 0.9172768592834473 = 0.28027328848838806 + 0.1 * 6.370035171508789
Epoch 340, val loss: 0.7859542369842529
Epoch 350, training loss: 0.886181116104126 = 0.2496684193611145 + 0.1 * 6.365127086639404
Epoch 350, val loss: 0.7795736789703369
Epoch 360, training loss: 0.8583600521087646 = 0.2221377193927765 + 0.1 * 6.362222671508789
Epoch 360, val loss: 0.7759200930595398
Epoch 370, training loss: 0.8329395055770874 = 0.1977769434452057 + 0.1 * 6.351625919342041
Epoch 370, val loss: 0.774883508682251
Epoch 380, training loss: 0.8129830956459045 = 0.17642603814601898 + 0.1 * 6.365570068359375
Epoch 380, val loss: 0.7760915756225586
Epoch 390, training loss: 0.7916386127471924 = 0.1579468995332718 + 0.1 * 6.336917400360107
Epoch 390, val loss: 0.7793021202087402
Epoch 400, training loss: 0.7755488157272339 = 0.141899973154068 + 0.1 * 6.336487770080566
Epoch 400, val loss: 0.7841370701789856
Epoch 410, training loss: 0.7621066570281982 = 0.12800554931163788 + 0.1 * 6.341010570526123
Epoch 410, val loss: 0.7902992963790894
Epoch 420, training loss: 0.7483698129653931 = 0.11593416333198547 + 0.1 * 6.324356555938721
Epoch 420, val loss: 0.7975894212722778
Epoch 430, training loss: 0.738096296787262 = 0.105379618704319 + 0.1 * 6.327166557312012
Epoch 430, val loss: 0.8056577444076538
Epoch 440, training loss: 0.7276585698127747 = 0.0961475595831871 + 0.1 * 6.315110206604004
Epoch 440, val loss: 0.8143576383590698
Epoch 450, training loss: 0.7184922099113464 = 0.0879976823925972 + 0.1 * 6.30494499206543
Epoch 450, val loss: 0.8234730958938599
Epoch 460, training loss: 0.7116770148277283 = 0.08077357709407806 + 0.1 * 6.30903434753418
Epoch 460, val loss: 0.8327642679214478
Epoch 470, training loss: 0.7040226459503174 = 0.07438035309314728 + 0.1 * 6.296422958374023
Epoch 470, val loss: 0.8422817587852478
Epoch 480, training loss: 0.6975304484367371 = 0.06865408271551132 + 0.1 * 6.288763999938965
Epoch 480, val loss: 0.8517695069313049
Epoch 490, training loss: 0.6923964023590088 = 0.06349045038223267 + 0.1 * 6.289059162139893
Epoch 490, val loss: 0.8613582849502563
Epoch 500, training loss: 0.6872716546058655 = 0.05882050096988678 + 0.1 * 6.284511566162109
Epoch 500, val loss: 0.8709217309951782
Epoch 510, training loss: 0.6826434135437012 = 0.05460035055875778 + 0.1 * 6.280430316925049
Epoch 510, val loss: 0.8803409337997437
Epoch 520, training loss: 0.6781973242759705 = 0.05078767612576485 + 0.1 * 6.2740960121154785
Epoch 520, val loss: 0.8898701071739197
Epoch 530, training loss: 0.675236701965332 = 0.04731210321187973 + 0.1 * 6.2792463302612305
Epoch 530, val loss: 0.8991822600364685
Epoch 540, training loss: 0.6706736087799072 = 0.04414427652955055 + 0.1 * 6.265293121337891
Epoch 540, val loss: 0.9085254073143005
Epoch 550, training loss: 0.667198121547699 = 0.0412445142865181 + 0.1 * 6.259535789489746
Epoch 550, val loss: 0.9178540706634521
Epoch 560, training loss: 0.667282223701477 = 0.03858931362628937 + 0.1 * 6.286929130554199
Epoch 560, val loss: 0.9270871877670288
Epoch 570, training loss: 0.6616769433021545 = 0.03616815432906151 + 0.1 * 6.255087852478027
Epoch 570, val loss: 0.9362637996673584
Epoch 580, training loss: 0.6590675711631775 = 0.033950939774513245 + 0.1 * 6.251166343688965
Epoch 580, val loss: 0.9452762603759766
Epoch 590, training loss: 0.6578707098960876 = 0.031918566673994064 + 0.1 * 6.259521007537842
Epoch 590, val loss: 0.9541328549385071
Epoch 600, training loss: 0.6546361446380615 = 0.030061259865760803 + 0.1 * 6.245748996734619
Epoch 600, val loss: 0.9630241394042969
Epoch 610, training loss: 0.65377277135849 = 0.028353622183203697 + 0.1 * 6.2541913986206055
Epoch 610, val loss: 0.9717206954956055
Epoch 620, training loss: 0.6507362723350525 = 0.026786310598254204 + 0.1 * 6.239499568939209
Epoch 620, val loss: 0.9802916646003723
Epoch 630, training loss: 0.6488578915596008 = 0.025342555716633797 + 0.1 * 6.235152721405029
Epoch 630, val loss: 0.9888240098953247
Epoch 640, training loss: 0.648038387298584 = 0.024008767679333687 + 0.1 * 6.240296363830566
Epoch 640, val loss: 0.9971135854721069
Epoch 650, training loss: 0.6459972262382507 = 0.022779731079936028 + 0.1 * 6.232174873352051
Epoch 650, val loss: 1.0054783821105957
Epoch 660, training loss: 0.6444428563117981 = 0.021640174090862274 + 0.1 * 6.228026390075684
Epoch 660, val loss: 1.013602614402771
Epoch 670, training loss: 0.6435763835906982 = 0.02058534324169159 + 0.1 * 6.229909896850586
Epoch 670, val loss: 1.0215225219726562
Epoch 680, training loss: 0.6419569253921509 = 0.019608905538916588 + 0.1 * 6.223479747772217
Epoch 680, val loss: 1.029458999633789
Epoch 690, training loss: 0.6416870355606079 = 0.018700378015637398 + 0.1 * 6.2298665046691895
Epoch 690, val loss: 1.0371742248535156
Epoch 700, training loss: 0.6397270560264587 = 0.01785474829375744 + 0.1 * 6.218722820281982
Epoch 700, val loss: 1.0447007417678833
Epoch 710, training loss: 0.6383278369903564 = 0.017068177461624146 + 0.1 * 6.2125959396362305
Epoch 710, val loss: 1.0522688627243042
Epoch 720, training loss: 0.638689398765564 = 0.016331857070326805 + 0.1 * 6.223575592041016
Epoch 720, val loss: 1.059502363204956
Epoch 730, training loss: 0.6363914012908936 = 0.015646107494831085 + 0.1 * 6.207452774047852
Epoch 730, val loss: 1.066746711730957
Epoch 740, training loss: 0.635599672794342 = 0.015004063956439495 + 0.1 * 6.20595645904541
Epoch 740, val loss: 1.0738804340362549
Epoch 750, training loss: 0.6355079412460327 = 0.014401093125343323 + 0.1 * 6.211068630218506
Epoch 750, val loss: 1.080782175064087
Epoch 760, training loss: 0.6342113018035889 = 0.013835764490067959 + 0.1 * 6.203754901885986
Epoch 760, val loss: 1.0875983238220215
Epoch 770, training loss: 0.6331027746200562 = 0.013305733911693096 + 0.1 * 6.197970390319824
Epoch 770, val loss: 1.094377875328064
Epoch 780, training loss: 0.6336576342582703 = 0.012805831618607044 + 0.1 * 6.208518028259277
Epoch 780, val loss: 1.100921869277954
Epoch 790, training loss: 0.6328039169311523 = 0.012335630133748055 + 0.1 * 6.204682350158691
Epoch 790, val loss: 1.107378363609314
Epoch 800, training loss: 0.6311431527137756 = 0.0118931345641613 + 0.1 * 6.19249963760376
Epoch 800, val loss: 1.1137667894363403
Epoch 810, training loss: 0.6317310333251953 = 0.011475685983896255 + 0.1 * 6.2025532722473145
Epoch 810, val loss: 1.120044231414795
Epoch 820, training loss: 0.6300981044769287 = 0.011080401949584484 + 0.1 * 6.190176963806152
Epoch 820, val loss: 1.1260991096496582
Epoch 830, training loss: 0.6297535300254822 = 0.010707657784223557 + 0.1 * 6.190458297729492
Epoch 830, val loss: 1.1322190761566162
Epoch 840, training loss: 0.6289865970611572 = 0.01035355869680643 + 0.1 * 6.186330318450928
Epoch 840, val loss: 1.1379426717758179
Epoch 850, training loss: 0.6282607913017273 = 0.010020517744123936 + 0.1 * 6.182403087615967
Epoch 850, val loss: 1.1438963413238525
Epoch 860, training loss: 0.6294819712638855 = 0.00970411580055952 + 0.1 * 6.197778701782227
Epoch 860, val loss: 1.1495624780654907
Epoch 870, training loss: 0.6275151968002319 = 0.009403550997376442 + 0.1 * 6.181116580963135
Epoch 870, val loss: 1.1551215648651123
Epoch 880, training loss: 0.6269899606704712 = 0.009118529036641121 + 0.1 * 6.178713798522949
Epoch 880, val loss: 1.1607242822647095
Epoch 890, training loss: 0.6267966628074646 = 0.008845953270792961 + 0.1 * 6.179507255554199
Epoch 890, val loss: 1.166007399559021
Epoch 900, training loss: 0.6261114478111267 = 0.008587228134274483 + 0.1 * 6.175241947174072
Epoch 900, val loss: 1.171409010887146
Epoch 910, training loss: 0.6267271041870117 = 0.00834018923342228 + 0.1 * 6.183869361877441
Epoch 910, val loss: 1.1766815185546875
Epoch 920, training loss: 0.6253640651702881 = 0.008104212582111359 + 0.1 * 6.172598361968994
Epoch 920, val loss: 1.1817735433578491
Epoch 930, training loss: 0.6257968544960022 = 0.007879720069468021 + 0.1 * 6.179171085357666
Epoch 930, val loss: 1.1868265867233276
Epoch 940, training loss: 0.6249132752418518 = 0.007665704004466534 + 0.1 * 6.172475814819336
Epoch 940, val loss: 1.1918622255325317
Epoch 950, training loss: 0.624946117401123 = 0.007461379282176495 + 0.1 * 6.17484712600708
Epoch 950, val loss: 1.1966967582702637
Epoch 960, training loss: 0.6237506866455078 = 0.007266254164278507 + 0.1 * 6.164844036102295
Epoch 960, val loss: 1.2015167474746704
Epoch 970, training loss: 0.6246880292892456 = 0.007079405710101128 + 0.1 * 6.176085948944092
Epoch 970, val loss: 1.2063037157058716
Epoch 980, training loss: 0.6229889988899231 = 0.0068998076021671295 + 0.1 * 6.160891532897949
Epoch 980, val loss: 1.2109135389328003
Epoch 990, training loss: 0.6249357461929321 = 0.006728105712682009 + 0.1 * 6.1820759773254395
Epoch 990, val loss: 1.2155722379684448
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.4982
Flip ASR: 0.4089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7785420417785645 = 1.941163182258606 + 0.1 * 8.373788833618164
Epoch 0, val loss: 1.9424571990966797
Epoch 10, training loss: 2.7683944702148438 = 1.931034803390503 + 0.1 * 8.373595237731934
Epoch 10, val loss: 1.93248450756073
Epoch 20, training loss: 2.756042003631592 = 1.91878342628479 + 0.1 * 8.372584342956543
Epoch 20, val loss: 1.9200828075408936
Epoch 30, training loss: 2.738373279571533 = 1.9016915559768677 + 0.1 * 8.366816520690918
Epoch 30, val loss: 1.9025061130523682
Epoch 40, training loss: 2.7093422412872314 = 1.8767313957214355 + 0.1 * 8.3261079788208
Epoch 40, val loss: 1.8768330812454224
Epoch 50, training loss: 2.6368324756622314 = 1.842824101448059 + 0.1 * 7.940083026885986
Epoch 50, val loss: 1.843146800994873
Epoch 60, training loss: 2.5376460552215576 = 1.806491732597351 + 0.1 * 7.311542987823486
Epoch 60, val loss: 1.8071035146713257
Epoch 70, training loss: 2.466569662094116 = 1.7691974639892578 + 0.1 * 6.973722457885742
Epoch 70, val loss: 1.7701561450958252
Epoch 80, training loss: 2.408914804458618 = 1.7274857759475708 + 0.1 * 6.8142900466918945
Epoch 80, val loss: 1.7324386835098267
Epoch 90, training loss: 2.34879732131958 = 1.6764674186706543 + 0.1 * 6.723299980163574
Epoch 90, val loss: 1.6854268312454224
Epoch 100, training loss: 2.275906562805176 = 1.6085107326507568 + 0.1 * 6.673957824707031
Epoch 100, val loss: 1.6246390342712402
Epoch 110, training loss: 2.1858596801757812 = 1.521465539932251 + 0.1 * 6.6439409255981445
Epoch 110, val loss: 1.5518440008163452
Epoch 120, training loss: 2.0811386108398438 = 1.4189447164535522 + 0.1 * 6.6219401359558105
Epoch 120, val loss: 1.4674819707870483
Epoch 130, training loss: 1.9714252948760986 = 1.3110504150390625 + 0.1 * 6.603748798370361
Epoch 130, val loss: 1.3801608085632324
Epoch 140, training loss: 1.8661694526672363 = 1.2076207399368286 + 0.1 * 6.585486888885498
Epoch 140, val loss: 1.2982434034347534
Epoch 150, training loss: 1.7711511850357056 = 1.1143518686294556 + 0.1 * 6.5679931640625
Epoch 150, val loss: 1.2267836332321167
Epoch 160, training loss: 1.68638014793396 = 1.0310664176940918 + 0.1 * 6.553136825561523
Epoch 160, val loss: 1.164305567741394
Epoch 170, training loss: 1.6094152927398682 = 0.9555433988571167 + 0.1 * 6.538718223571777
Epoch 170, val loss: 1.1088448762893677
Epoch 180, training loss: 1.5394251346588135 = 0.8863511681556702 + 0.1 * 6.530739784240723
Epoch 180, val loss: 1.0589299201965332
Epoch 190, training loss: 1.4741957187652588 = 0.8225206136703491 + 0.1 * 6.516750335693359
Epoch 190, val loss: 1.0133967399597168
Epoch 200, training loss: 1.412628412246704 = 0.762382984161377 + 0.1 * 6.5024542808532715
Epoch 200, val loss: 0.9710308909416199
Epoch 210, training loss: 1.3552563190460205 = 0.7051658034324646 + 0.1 * 6.500905513763428
Epoch 210, val loss: 0.9313503503799438
Epoch 220, training loss: 1.2994606494903564 = 0.6510149240493774 + 0.1 * 6.484457969665527
Epoch 220, val loss: 0.8944324254989624
Epoch 230, training loss: 1.2486481666564941 = 0.6003937125205994 + 0.1 * 6.4825439453125
Epoch 230, val loss: 0.860967218875885
Epoch 240, training loss: 1.2009080648422241 = 0.5543118715286255 + 0.1 * 6.465961933135986
Epoch 240, val loss: 0.8317926526069641
Epoch 250, training loss: 1.1593513488769531 = 0.5128574371337891 + 0.1 * 6.464939117431641
Epoch 250, val loss: 0.8074451684951782
Epoch 260, training loss: 1.1206486225128174 = 0.4758075475692749 + 0.1 * 6.4484100341796875
Epoch 260, val loss: 0.7876386046409607
Epoch 270, training loss: 1.0858980417251587 = 0.44199877977371216 + 0.1 * 6.438992500305176
Epoch 270, val loss: 0.7720131278038025
Epoch 280, training loss: 1.0540813207626343 = 0.4104291796684265 + 0.1 * 6.436521530151367
Epoch 280, val loss: 0.7597735524177551
Epoch 290, training loss: 1.0230941772460938 = 0.3806793987751007 + 0.1 * 6.424148082733154
Epoch 290, val loss: 0.7506406903266907
Epoch 300, training loss: 0.9944714903831482 = 0.352095365524292 + 0.1 * 6.423760890960693
Epoch 300, val loss: 0.7440510392189026
Epoch 310, training loss: 0.9657876491546631 = 0.3247409164905548 + 0.1 * 6.410467147827148
Epoch 310, val loss: 0.7399783134460449
Epoch 320, training loss: 0.9391340017318726 = 0.29879501461982727 + 0.1 * 6.4033894538879395
Epoch 320, val loss: 0.7382413148880005
Epoch 330, training loss: 0.9142752885818481 = 0.2747124135494232 + 0.1 * 6.395628929138184
Epoch 330, val loss: 0.7387940883636475
Epoch 340, training loss: 0.8916959762573242 = 0.2526775896549225 + 0.1 * 6.390183925628662
Epoch 340, val loss: 0.7413592338562012
Epoch 350, training loss: 0.870846688747406 = 0.23254548013210297 + 0.1 * 6.383011817932129
Epoch 350, val loss: 0.7456250786781311
Epoch 360, training loss: 0.852397084236145 = 0.21416440606117249 + 0.1 * 6.382326602935791
Epoch 360, val loss: 0.7510635852813721
Epoch 370, training loss: 0.8345545530319214 = 0.1973041594028473 + 0.1 * 6.372503280639648
Epoch 370, val loss: 0.7573414444923401
Epoch 380, training loss: 0.8186620473861694 = 0.1817101538181305 + 0.1 * 6.369518280029297
Epoch 380, val loss: 0.7642273902893066
Epoch 390, training loss: 0.8039222359657288 = 0.16728907823562622 + 0.1 * 6.366331577301025
Epoch 390, val loss: 0.7716746926307678
Epoch 400, training loss: 0.7894244194030762 = 0.15397313237190247 + 0.1 * 6.3545122146606445
Epoch 400, val loss: 0.7795810103416443
Epoch 410, training loss: 0.7768553495407104 = 0.14170929789543152 + 0.1 * 6.3514604568481445
Epoch 410, val loss: 0.7878871560096741
Epoch 420, training loss: 0.7654086947441101 = 0.13048113882541656 + 0.1 * 6.349275588989258
Epoch 420, val loss: 0.796532392501831
Epoch 430, training loss: 0.7547838687896729 = 0.12022591382265091 + 0.1 * 6.345579624176025
Epoch 430, val loss: 0.8055084347724915
Epoch 440, training loss: 0.7443107962608337 = 0.1108384057879448 + 0.1 * 6.334723949432373
Epoch 440, val loss: 0.8147169351577759
Epoch 450, training loss: 0.7361458539962769 = 0.10224930942058563 + 0.1 * 6.338964939117432
Epoch 450, val loss: 0.824073314666748
Epoch 460, training loss: 0.728066623210907 = 0.09440314024686813 + 0.1 * 6.336634635925293
Epoch 460, val loss: 0.8334665298461914
Epoch 470, training loss: 0.7201544642448425 = 0.08725379407405853 + 0.1 * 6.329006671905518
Epoch 470, val loss: 0.8428066968917847
Epoch 480, training loss: 0.7126152515411377 = 0.08071041107177734 + 0.1 * 6.3190484046936035
Epoch 480, val loss: 0.8520750403404236
Epoch 490, training loss: 0.7062882781028748 = 0.07467757165431976 + 0.1 * 6.316106796264648
Epoch 490, val loss: 0.8614920377731323
Epoch 500, training loss: 0.7024257779121399 = 0.06910711526870728 + 0.1 * 6.333186626434326
Epoch 500, val loss: 0.8706880211830139
Epoch 510, training loss: 0.6946089267730713 = 0.0639692023396492 + 0.1 * 6.306397438049316
Epoch 510, val loss: 0.8797645568847656
Epoch 520, training loss: 0.6896863579750061 = 0.05921361595392227 + 0.1 * 6.304727077484131
Epoch 520, val loss: 0.8886624574661255
Epoch 530, training loss: 0.6856753826141357 = 0.054818294942379 + 0.1 * 6.308570861816406
Epoch 530, val loss: 0.8973115086555481
Epoch 540, training loss: 0.6804559826850891 = 0.05077445134520531 + 0.1 * 6.296815395355225
Epoch 540, val loss: 0.9057779908180237
Epoch 550, training loss: 0.6765686869621277 = 0.04706386476755142 + 0.1 * 6.295047760009766
Epoch 550, val loss: 0.913938581943512
Epoch 560, training loss: 0.6722177267074585 = 0.043675512075424194 + 0.1 * 6.285421848297119
Epoch 560, val loss: 0.9219351410865784
Epoch 570, training loss: 0.6691892743110657 = 0.040595002472400665 + 0.1 * 6.285942554473877
Epoch 570, val loss: 0.9297475218772888
Epoch 580, training loss: 0.6663610339164734 = 0.037803612649440765 + 0.1 * 6.285574436187744
Epoch 580, val loss: 0.9374560117721558
Epoch 590, training loss: 0.6656458973884583 = 0.035264384001493454 + 0.1 * 6.303814888000488
Epoch 590, val loss: 0.9451137781143188
Epoch 600, training loss: 0.6610112190246582 = 0.03296231850981712 + 0.1 * 6.280488967895508
Epoch 600, val loss: 0.952547550201416
Epoch 610, training loss: 0.6577714085578918 = 0.03087390586733818 + 0.1 * 6.268974781036377
Epoch 610, val loss: 0.9599927067756653
Epoch 620, training loss: 0.6556463241577148 = 0.028966499492526054 + 0.1 * 6.266798496246338
Epoch 620, val loss: 0.9672625660896301
Epoch 630, training loss: 0.6545353531837463 = 0.027218744158744812 + 0.1 * 6.273165702819824
Epoch 630, val loss: 0.9744755625724792
Epoch 640, training loss: 0.6516556739807129 = 0.025617728009819984 + 0.1 * 6.260379314422607
Epoch 640, val loss: 0.9815990328788757
Epoch 650, training loss: 0.6502143740653992 = 0.024147436022758484 + 0.1 * 6.260669231414795
Epoch 650, val loss: 0.9885692596435547
Epoch 660, training loss: 0.6489456295967102 = 0.022794386371970177 + 0.1 * 6.261512279510498
Epoch 660, val loss: 0.9954245090484619
Epoch 670, training loss: 0.6473247408866882 = 0.02154933661222458 + 0.1 * 6.257753849029541
Epoch 670, val loss: 1.002186894416809
Epoch 680, training loss: 0.6462538838386536 = 0.020403286442160606 + 0.1 * 6.2585062980651855
Epoch 680, val loss: 1.0088189840316772
Epoch 690, training loss: 0.6446073055267334 = 0.019346078857779503 + 0.1 * 6.252612590789795
Epoch 690, val loss: 1.0152431726455688
Epoch 700, training loss: 0.6432228088378906 = 0.018368961289525032 + 0.1 * 6.248538494110107
Epoch 700, val loss: 1.0216110944747925
Epoch 710, training loss: 0.6422412991523743 = 0.017466679215431213 + 0.1 * 6.247746467590332
Epoch 710, val loss: 1.0278372764587402
Epoch 720, training loss: 0.6415951251983643 = 0.016629088670015335 + 0.1 * 6.249660015106201
Epoch 720, val loss: 1.0339096784591675
Epoch 730, training loss: 0.6395661234855652 = 0.015849635004997253 + 0.1 * 6.237164497375488
Epoch 730, val loss: 1.0399023294448853
Epoch 740, training loss: 0.6398748755455017 = 0.015122906304895878 + 0.1 * 6.247519493103027
Epoch 740, val loss: 1.0458039045333862
Epoch 750, training loss: 0.6394609808921814 = 0.014446120709180832 + 0.1 * 6.250148296356201
Epoch 750, val loss: 1.0514776706695557
Epoch 760, training loss: 0.6373155117034912 = 0.013818362727761269 + 0.1 * 6.234971046447754
Epoch 760, val loss: 1.0572032928466797
Epoch 770, training loss: 0.6369187831878662 = 0.01323271356523037 + 0.1 * 6.236860752105713
Epoch 770, val loss: 1.0626939535140991
Epoch 780, training loss: 0.6351286172866821 = 0.012685028836131096 + 0.1 * 6.224436283111572
Epoch 780, val loss: 1.0680533647537231
Epoch 790, training loss: 0.6350829601287842 = 0.012170977890491486 + 0.1 * 6.229119777679443
Epoch 790, val loss: 1.073343276977539
Epoch 800, training loss: 0.6339713335037231 = 0.011688007973134518 + 0.1 * 6.222833156585693
Epoch 800, val loss: 1.078562617301941
Epoch 810, training loss: 0.6350472569465637 = 0.011234644800424576 + 0.1 * 6.238125801086426
Epoch 810, val loss: 1.0836085081100464
Epoch 820, training loss: 0.632770299911499 = 0.010810445994138718 + 0.1 * 6.219598770141602
Epoch 820, val loss: 1.0884639024734497
Epoch 830, training loss: 0.6320372819900513 = 0.010412169620394707 + 0.1 * 6.216251373291016
Epoch 830, val loss: 1.0933477878570557
Epoch 840, training loss: 0.6314690709114075 = 0.010036271996796131 + 0.1 * 6.214327812194824
Epoch 840, val loss: 1.0980558395385742
Epoch 850, training loss: 0.6326925754547119 = 0.009680905379354954 + 0.1 * 6.230116367340088
Epoch 850, val loss: 1.1026347875595093
Epoch 860, training loss: 0.630508303642273 = 0.009347228333353996 + 0.1 * 6.211610794067383
Epoch 860, val loss: 1.107113003730774
Epoch 870, training loss: 0.6301393508911133 = 0.009032566100358963 + 0.1 * 6.211068153381348
Epoch 870, val loss: 1.1115666627883911
Epoch 880, training loss: 0.6296136379241943 = 0.008734752424061298 + 0.1 * 6.2087883949279785
Epoch 880, val loss: 1.1158499717712402
Epoch 890, training loss: 0.6294257044792175 = 0.008453593589365482 + 0.1 * 6.209720611572266
Epoch 890, val loss: 1.1201058626174927
Epoch 900, training loss: 0.6282135844230652 = 0.008186648599803448 + 0.1 * 6.2002692222595215
Epoch 900, val loss: 1.1242250204086304
Epoch 910, training loss: 0.6278650760650635 = 0.007933157496154308 + 0.1 * 6.199319362640381
Epoch 910, val loss: 1.1282709836959839
Epoch 920, training loss: 0.627723217010498 = 0.007693082094192505 + 0.1 * 6.200301170349121
Epoch 920, val loss: 1.1322139501571655
Epoch 930, training loss: 0.627888023853302 = 0.007464803755283356 + 0.1 * 6.2042317390441895
Epoch 930, val loss: 1.1361416578292847
Epoch 940, training loss: 0.6275155544281006 = 0.00724794901907444 + 0.1 * 6.202676296234131
Epoch 940, val loss: 1.139888048171997
Epoch 950, training loss: 0.626743733882904 = 0.007041324395686388 + 0.1 * 6.197023868560791
Epoch 950, val loss: 1.1436352729797363
Epoch 960, training loss: 0.6261003017425537 = 0.006845009513199329 + 0.1 * 6.1925530433654785
Epoch 960, val loss: 1.1472629308700562
Epoch 970, training loss: 0.6258710622787476 = 0.006657926365733147 + 0.1 * 6.192131519317627
Epoch 970, val loss: 1.1508876085281372
Epoch 980, training loss: 0.6251671314239502 = 0.006479316856712103 + 0.1 * 6.186877727508545
Epoch 980, val loss: 1.1543984413146973
Epoch 990, training loss: 0.6266039609909058 = 0.006308834068477154 + 0.1 * 6.202950954437256
Epoch 990, val loss: 1.1578487157821655
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.6679
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7653088569641113 = 1.9279215335845947 + 0.1 * 8.373872756958008
Epoch 0, val loss: 1.9259978532791138
Epoch 10, training loss: 2.755952835083008 = 1.9185781478881836 + 0.1 * 8.373747825622559
Epoch 10, val loss: 1.9169926643371582
Epoch 20, training loss: 2.7443580627441406 = 1.9070441722869873 + 0.1 * 8.373137474060059
Epoch 20, val loss: 1.9055448770523071
Epoch 30, training loss: 2.7278690338134766 = 1.890828013420105 + 0.1 * 8.37040901184082
Epoch 30, val loss: 1.8889943361282349
Epoch 40, training loss: 2.7023580074310303 = 1.8669089078903198 + 0.1 * 8.354491233825684
Epoch 40, val loss: 1.8643975257873535
Epoch 50, training loss: 2.6567893028259277 = 1.833028793334961 + 0.1 * 8.237604141235352
Epoch 50, val loss: 1.830281376838684
Epoch 60, training loss: 2.5576233863830566 = 1.7936571836471558 + 0.1 * 7.639662265777588
Epoch 60, val loss: 1.7919195890426636
Epoch 70, training loss: 2.4756109714508057 = 1.7532671689987183 + 0.1 * 7.223438262939453
Epoch 70, val loss: 1.753636360168457
Epoch 80, training loss: 2.408766984939575 = 1.709812879562378 + 0.1 * 6.989540100097656
Epoch 80, val loss: 1.7153631448745728
Epoch 90, training loss: 2.344423294067383 = 1.6560776233673096 + 0.1 * 6.883456707000732
Epoch 90, val loss: 1.6690386533737183
Epoch 100, training loss: 2.267096519470215 = 1.5857408046722412 + 0.1 * 6.813558101654053
Epoch 100, val loss: 1.609958291053772
Epoch 110, training loss: 2.1734156608581543 = 1.4963394403457642 + 0.1 * 6.770761489868164
Epoch 110, val loss: 1.5378293991088867
Epoch 120, training loss: 2.0659501552581787 = 1.391528606414795 + 0.1 * 6.744215965270996
Epoch 120, val loss: 1.4549843072891235
Epoch 130, training loss: 1.9531184434890747 = 1.280714988708496 + 0.1 * 6.724034309387207
Epoch 130, val loss: 1.369984745979309
Epoch 140, training loss: 1.8442647457122803 = 1.173749566078186 + 0.1 * 6.705151081085205
Epoch 140, val loss: 1.2892874479293823
Epoch 150, training loss: 1.746352195739746 = 1.0766685009002686 + 0.1 * 6.696836471557617
Epoch 150, val loss: 1.2162258625030518
Epoch 160, training loss: 1.6605991125106812 = 0.9928651452064514 + 0.1 * 6.677339553833008
Epoch 160, val loss: 1.1533490419387817
Epoch 170, training loss: 1.5859705209732056 = 0.9192537665367126 + 0.1 * 6.6671671867370605
Epoch 170, val loss: 1.0989744663238525
Epoch 180, training loss: 1.5176219940185547 = 0.8520006537437439 + 0.1 * 6.656213760375977
Epoch 180, val loss: 1.0504577159881592
Epoch 190, training loss: 1.4526307582855225 = 0.7877888679504395 + 0.1 * 6.648419380187988
Epoch 190, val loss: 1.004026174545288
Epoch 200, training loss: 1.3910942077636719 = 0.7263817191123962 + 0.1 * 6.647124290466309
Epoch 200, val loss: 0.9600656032562256
Epoch 210, training loss: 1.3333995342254639 = 0.6697133183479309 + 0.1 * 6.636861801147461
Epoch 210, val loss: 0.9206801652908325
Epoch 220, training loss: 1.280768871307373 = 0.6179717183113098 + 0.1 * 6.627971172332764
Epoch 220, val loss: 0.8869627118110657
Epoch 230, training loss: 1.2321357727050781 = 0.5701002478599548 + 0.1 * 6.620355606079102
Epoch 230, val loss: 0.8581231832504272
Epoch 240, training loss: 1.1856532096862793 = 0.5242298245429993 + 0.1 * 6.614233016967773
Epoch 240, val loss: 0.8328737616539001
Epoch 250, training loss: 1.1393952369689941 = 0.47875797748565674 + 0.1 * 6.606371879577637
Epoch 250, val loss: 0.809785008430481
Epoch 260, training loss: 1.092850923538208 = 0.43284520506858826 + 0.1 * 6.6000566482543945
Epoch 260, val loss: 0.7885676622390747
Epoch 270, training loss: 1.046342372894287 = 0.3869032561779022 + 0.1 * 6.594390869140625
Epoch 270, val loss: 0.7697945833206177
Epoch 280, training loss: 1.0013595819473267 = 0.3423371911048889 + 0.1 * 6.590223789215088
Epoch 280, val loss: 0.7541870474815369
Epoch 290, training loss: 0.9589078426361084 = 0.3009006083011627 + 0.1 * 6.580071926116943
Epoch 290, val loss: 0.7423050403594971
Epoch 300, training loss: 0.920967698097229 = 0.26374000310897827 + 0.1 * 6.572277069091797
Epoch 300, val loss: 0.7342148423194885
Epoch 310, training loss: 0.8880344033241272 = 0.23130999505519867 + 0.1 * 6.567243576049805
Epoch 310, val loss: 0.7296668887138367
Epoch 320, training loss: 0.8595789670944214 = 0.20363494753837585 + 0.1 * 6.559440612792969
Epoch 320, val loss: 0.7283148169517517
Epoch 330, training loss: 0.8352354764938354 = 0.18010658025741577 + 0.1 * 6.551288604736328
Epoch 330, val loss: 0.7297759652137756
Epoch 340, training loss: 0.8143829107284546 = 0.16003218293190002 + 0.1 * 6.543506622314453
Epoch 340, val loss: 0.7334415316581726
Epoch 350, training loss: 0.7961374521255493 = 0.14284412562847137 + 0.1 * 6.532932758331299
Epoch 350, val loss: 0.7388037443161011
Epoch 360, training loss: 0.7805101275444031 = 0.12797559797763824 + 0.1 * 6.525345325469971
Epoch 360, val loss: 0.7455124258995056
Epoch 370, training loss: 0.7680763006210327 = 0.1150381863117218 + 0.1 * 6.530381202697754
Epoch 370, val loss: 0.7532764673233032
Epoch 380, training loss: 0.7545697093009949 = 0.10381244122982025 + 0.1 * 6.507572174072266
Epoch 380, val loss: 0.7618373036384583
Epoch 390, training loss: 0.7441259026527405 = 0.09397255629301071 + 0.1 * 6.501533031463623
Epoch 390, val loss: 0.7709568738937378
Epoch 400, training loss: 0.7358807325363159 = 0.08529441803693771 + 0.1 * 6.505863189697266
Epoch 400, val loss: 0.7804989218711853
Epoch 410, training loss: 0.7265510559082031 = 0.07764942944049835 + 0.1 * 6.489016532897949
Epoch 410, val loss: 0.7903687357902527
Epoch 420, training loss: 0.7186738848686218 = 0.07086732238531113 + 0.1 * 6.4780659675598145
Epoch 420, val loss: 0.8005212545394897
Epoch 430, training loss: 0.7128925919532776 = 0.06482886523008347 + 0.1 * 6.480637073516846
Epoch 430, val loss: 0.8109251856803894
Epoch 440, training loss: 0.7063862681388855 = 0.05944964662194252 + 0.1 * 6.46936559677124
Epoch 440, val loss: 0.8215551376342773
Epoch 450, training loss: 0.7005549073219299 = 0.05463944375514984 + 0.1 * 6.4591546058654785
Epoch 450, val loss: 0.8323962092399597
Epoch 460, training loss: 0.6967426538467407 = 0.050329893827438354 + 0.1 * 6.464127540588379
Epoch 460, val loss: 0.8433095216751099
Epoch 470, training loss: 0.6915587186813354 = 0.04647854343056679 + 0.1 * 6.450801372528076
Epoch 470, val loss: 0.8543710112571716
Epoch 480, training loss: 0.6867164969444275 = 0.04301958158612251 + 0.1 * 6.436968803405762
Epoch 480, val loss: 0.8654666543006897
Epoch 490, training loss: 0.6849084496498108 = 0.03990144282579422 + 0.1 * 6.450069904327393
Epoch 490, val loss: 0.8766207098960876
Epoch 500, training loss: 0.6804205179214478 = 0.037099745124578476 + 0.1 * 6.433207988739014
Epoch 500, val loss: 0.8875467777252197
Epoch 510, training loss: 0.6769052147865295 = 0.03457402437925339 + 0.1 * 6.423311710357666
Epoch 510, val loss: 0.898499071598053
Epoch 520, training loss: 0.6756494045257568 = 0.03228827565908432 + 0.1 * 6.4336113929748535
Epoch 520, val loss: 0.9091265201568604
Epoch 530, training loss: 0.6716613173484802 = 0.03022499941289425 + 0.1 * 6.414363384246826
Epoch 530, val loss: 0.9196760654449463
Epoch 540, training loss: 0.6681839823722839 = 0.028350545093417168 + 0.1 * 6.398334503173828
Epoch 540, val loss: 0.9300210475921631
Epoch 550, training loss: 0.6673804521560669 = 0.026641637086868286 + 0.1 * 6.407388210296631
Epoch 550, val loss: 0.9401452541351318
Epoch 560, training loss: 0.6638590097427368 = 0.025084713473916054 + 0.1 * 6.387742519378662
Epoch 560, val loss: 0.9499986171722412
Epoch 570, training loss: 0.6628319025039673 = 0.02366040274500847 + 0.1 * 6.391714572906494
Epoch 570, val loss: 0.9596135020256042
Epoch 580, training loss: 0.6600686311721802 = 0.02235705778002739 + 0.1 * 6.377115726470947
Epoch 580, val loss: 0.9690869450569153
Epoch 590, training loss: 0.6584585309028625 = 0.021159227937459946 + 0.1 * 6.372992992401123
Epoch 590, val loss: 0.9783154726028442
Epoch 600, training loss: 0.6562860608100891 = 0.020056620240211487 + 0.1 * 6.362294673919678
Epoch 600, val loss: 0.9873120188713074
Epoch 610, training loss: 0.6557717323303223 = 0.019038740545511246 + 0.1 * 6.367330074310303
Epoch 610, val loss: 0.9961355924606323
Epoch 620, training loss: 0.6541122794151306 = 0.01809842325747013 + 0.1 * 6.360138416290283
Epoch 620, val loss: 1.0046515464782715
Epoch 630, training loss: 0.6533419489860535 = 0.0172310508787632 + 0.1 * 6.361108779907227
Epoch 630, val loss: 1.013036847114563
Epoch 640, training loss: 0.6515823006629944 = 0.016427045688033104 + 0.1 * 6.351552486419678
Epoch 640, val loss: 1.0210621356964111
Epoch 650, training loss: 0.6494452357292175 = 0.015681015327572823 + 0.1 * 6.337641716003418
Epoch 650, val loss: 1.0290595293045044
Epoch 660, training loss: 0.6483948230743408 = 0.014984777197241783 + 0.1 * 6.334100246429443
Epoch 660, val loss: 1.0368117094039917
Epoch 670, training loss: 0.6484679579734802 = 0.014334606938064098 + 0.1 * 6.341333389282227
Epoch 670, val loss: 1.0443354845046997
Epoch 680, training loss: 0.6468509435653687 = 0.013728102669119835 + 0.1 * 6.331228256225586
Epoch 680, val loss: 1.0517061948776245
Epoch 690, training loss: 0.6489416360855103 = 0.013160666450858116 + 0.1 * 6.357809543609619
Epoch 690, val loss: 1.0588678121566772
Epoch 700, training loss: 0.6453443765640259 = 0.012631133198738098 + 0.1 * 6.327132701873779
Epoch 700, val loss: 1.0657379627227783
Epoch 710, training loss: 0.6433261632919312 = 0.01213662140071392 + 0.1 * 6.311895370483398
Epoch 710, val loss: 1.072625756263733
Epoch 720, training loss: 0.6428447961807251 = 0.011671026237308979 + 0.1 * 6.311737537384033
Epoch 720, val loss: 1.0793359279632568
Epoch 730, training loss: 0.6416101455688477 = 0.01123239379376173 + 0.1 * 6.303777694702148
Epoch 730, val loss: 1.085739016532898
Epoch 740, training loss: 0.6408901214599609 = 0.010820449329912663 + 0.1 * 6.30069637298584
Epoch 740, val loss: 1.0921437740325928
Epoch 750, training loss: 0.6410248875617981 = 0.010431493632495403 + 0.1 * 6.305933952331543
Epoch 750, val loss: 1.0984628200531006
Epoch 760, training loss: 0.6409457921981812 = 0.010064246132969856 + 0.1 * 6.3088154792785645
Epoch 760, val loss: 1.1044825315475464
Epoch 770, training loss: 0.6395972371101379 = 0.009719187393784523 + 0.1 * 6.29878044128418
Epoch 770, val loss: 1.1104344129562378
Epoch 780, training loss: 0.6384232640266418 = 0.009393095970153809 + 0.1 * 6.29030179977417
Epoch 780, val loss: 1.1164113283157349
Epoch 790, training loss: 0.637493371963501 = 0.009083529002964497 + 0.1 * 6.2840986251831055
Epoch 790, val loss: 1.1222057342529297
Epoch 800, training loss: 0.6386185884475708 = 0.008789479732513428 + 0.1 * 6.298290729522705
Epoch 800, val loss: 1.1276730298995972
Epoch 810, training loss: 0.6372193694114685 = 0.008511564694344997 + 0.1 * 6.287077903747559
Epoch 810, val loss: 1.1330170631408691
Epoch 820, training loss: 0.6360424757003784 = 0.008248419500887394 + 0.1 * 6.277940273284912
Epoch 820, val loss: 1.138444423675537
Epoch 830, training loss: 0.6360006332397461 = 0.007998614571988583 + 0.1 * 6.280020236968994
Epoch 830, val loss: 1.1438452005386353
Epoch 840, training loss: 0.6346673965454102 = 0.007760831154882908 + 0.1 * 6.269065856933594
Epoch 840, val loss: 1.148958683013916
Epoch 850, training loss: 0.6362109184265137 = 0.007534442935138941 + 0.1 * 6.286764621734619
Epoch 850, val loss: 1.1539781093597412
Epoch 860, training loss: 0.6347311735153198 = 0.007318171672523022 + 0.1 * 6.274129390716553
Epoch 860, val loss: 1.158719539642334
Epoch 870, training loss: 0.6337212324142456 = 0.007113431114703417 + 0.1 * 6.266077518463135
Epoch 870, val loss: 1.163634181022644
Epoch 880, training loss: 0.6340326070785522 = 0.0069178310222923756 + 0.1 * 6.271147727966309
Epoch 880, val loss: 1.1684398651123047
Epoch 890, training loss: 0.6322717070579529 = 0.006731060799211264 + 0.1 * 6.255406856536865
Epoch 890, val loss: 1.1730561256408691
Epoch 900, training loss: 0.6327448487281799 = 0.006552868988364935 + 0.1 * 6.2619194984436035
Epoch 900, val loss: 1.1776823997497559
Epoch 910, training loss: 0.6328905820846558 = 0.006382160820066929 + 0.1 * 6.265084266662598
Epoch 910, val loss: 1.182197093963623
Epoch 920, training loss: 0.6313616633415222 = 0.006219064816832542 + 0.1 * 6.251425743103027
Epoch 920, val loss: 1.1866586208343506
Epoch 930, training loss: 0.6320712566375732 = 0.006062745116651058 + 0.1 * 6.260085105895996
Epoch 930, val loss: 1.1911102533340454
Epoch 940, training loss: 0.6310028433799744 = 0.005912749096751213 + 0.1 * 6.250901222229004
Epoch 940, val loss: 1.1952526569366455
Epoch 950, training loss: 0.6302762031555176 = 0.005769485607743263 + 0.1 * 6.245067119598389
Epoch 950, val loss: 1.1994940042495728
Epoch 960, training loss: 0.630301296710968 = 0.005631850566715002 + 0.1 * 6.246694087982178
Epoch 960, val loss: 1.2036763429641724
Epoch 970, training loss: 0.6302119493484497 = 0.005499743390828371 + 0.1 * 6.247122287750244
Epoch 970, val loss: 1.2076972723007202
Epoch 980, training loss: 0.6292051076889038 = 0.005373215302824974 + 0.1 * 6.23831844329834
Epoch 980, val loss: 1.2116581201553345
Epoch 990, training loss: 0.6290457248687744 = 0.005251575261354446 + 0.1 * 6.237941265106201
Epoch 990, val loss: 1.2156062126159668
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9410
Flip ASR: 0.9333/225 nodes
The final ASR:0.70234, 0.18241, Accuracy:0.80494, 0.01772
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10576])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98278, 0.00920, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8066322803497314 = 1.96925687789917 + 0.1 * 8.373754501342773
Epoch 0, val loss: 1.9726181030273438
Epoch 10, training loss: 2.795360565185547 = 1.9580121040344238 + 0.1 * 8.373485565185547
Epoch 10, val loss: 1.9617302417755127
Epoch 20, training loss: 2.7813472747802734 = 1.9441403150558472 + 0.1 * 8.3720703125
Epoch 20, val loss: 1.9478726387023926
Epoch 30, training loss: 2.7607815265655518 = 1.9244471788406372 + 0.1 * 8.363343238830566
Epoch 30, val loss: 1.927888035774231
Epoch 40, training loss: 2.726449489593506 = 1.895277500152588 + 0.1 * 8.311720848083496
Epoch 40, val loss: 1.8985035419464111
Epoch 50, training loss: 2.6476144790649414 = 1.8565393686294556 + 0.1 * 7.910750865936279
Epoch 50, val loss: 1.8614814281463623
Epoch 60, training loss: 2.5480613708496094 = 1.8177552223205566 + 0.1 * 7.303061008453369
Epoch 60, val loss: 1.827024221420288
Epoch 70, training loss: 2.4724924564361572 = 1.7805776596069336 + 0.1 * 6.9191484451293945
Epoch 70, val loss: 1.795440912246704
Epoch 80, training loss: 2.4212634563446045 = 1.744999885559082 + 0.1 * 6.762634754180908
Epoch 80, val loss: 1.7667560577392578
Epoch 90, training loss: 2.3727824687957764 = 1.704721450805664 + 0.1 * 6.680610179901123
Epoch 90, val loss: 1.7322393655776978
Epoch 100, training loss: 2.3150408267974854 = 1.6510815620422363 + 0.1 * 6.63959264755249
Epoch 100, val loss: 1.6860579252243042
Epoch 110, training loss: 2.242766857147217 = 1.581842064857483 + 0.1 * 6.60924768447876
Epoch 110, val loss: 1.6290466785430908
Epoch 120, training loss: 2.155083417892456 = 1.4966033697128296 + 0.1 * 6.584800720214844
Epoch 120, val loss: 1.5607473850250244
Epoch 130, training loss: 2.0579583644866943 = 1.4012552499771118 + 0.1 * 6.567030906677246
Epoch 130, val loss: 1.4846705198287964
Epoch 140, training loss: 1.9589499235153198 = 1.304323673248291 + 0.1 * 6.546262264251709
Epoch 140, val loss: 1.4086772203445435
Epoch 150, training loss: 1.8634322881698608 = 1.211321473121643 + 0.1 * 6.521108150482178
Epoch 150, val loss: 1.3363441228866577
Epoch 160, training loss: 1.7761108875274658 = 1.1254788637161255 + 0.1 * 6.506320476531982
Epoch 160, val loss: 1.269442081451416
Epoch 170, training loss: 1.6968011856079102 = 1.048280954360962 + 0.1 * 6.485202312469482
Epoch 170, val loss: 1.2100422382354736
Epoch 180, training loss: 1.6241827011108398 = 0.9772481918334961 + 0.1 * 6.469345569610596
Epoch 180, val loss: 1.1553411483764648
Epoch 190, training loss: 1.556133508682251 = 0.910204291343689 + 0.1 * 6.459292411804199
Epoch 190, val loss: 1.1038557291030884
Epoch 200, training loss: 1.4895377159118652 = 0.8448963165283203 + 0.1 * 6.446413040161133
Epoch 200, val loss: 1.0531975030899048
Epoch 210, training loss: 1.42324960231781 = 0.7794592976570129 + 0.1 * 6.437902927398682
Epoch 210, val loss: 1.0019035339355469
Epoch 220, training loss: 1.3572397232055664 = 0.714188277721405 + 0.1 * 6.430513858795166
Epoch 220, val loss: 0.9508370757102966
Epoch 230, training loss: 1.2924160957336426 = 0.6499407291412354 + 0.1 * 6.424753665924072
Epoch 230, val loss: 0.9016820788383484
Epoch 240, training loss: 1.2298593521118164 = 0.5884526968002319 + 0.1 * 6.414066314697266
Epoch 240, val loss: 0.8567535877227783
Epoch 250, training loss: 1.1709022521972656 = 0.5303915739059448 + 0.1 * 6.405106067657471
Epoch 250, val loss: 0.8165467381477356
Epoch 260, training loss: 1.1167223453521729 = 0.4765077829360962 + 0.1 * 6.402146339416504
Epoch 260, val loss: 0.7816904783248901
Epoch 270, training loss: 1.0663055181503296 = 0.42734524607658386 + 0.1 * 6.3896026611328125
Epoch 270, val loss: 0.7524296045303345
Epoch 280, training loss: 1.0225552320480347 = 0.38283607363700867 + 0.1 * 6.397192001342773
Epoch 280, val loss: 0.7285176515579224
Epoch 290, training loss: 0.9808145761489868 = 0.3434014618396759 + 0.1 * 6.374131202697754
Epoch 290, val loss: 0.7101333141326904
Epoch 300, training loss: 0.9466204643249512 = 0.308379590511322 + 0.1 * 6.382408618927002
Epoch 300, val loss: 0.6963251233100891
Epoch 310, training loss: 0.9137811660766602 = 0.27738717198371887 + 0.1 * 6.3639397621154785
Epoch 310, val loss: 0.6868521571159363
Epoch 320, training loss: 0.8848637342453003 = 0.24954485893249512 + 0.1 * 6.353188514709473
Epoch 320, val loss: 0.6807142496109009
Epoch 330, training loss: 0.8606054782867432 = 0.22428995370864868 + 0.1 * 6.363154888153076
Epoch 330, val loss: 0.6772953271865845
Epoch 340, training loss: 0.8365655541419983 = 0.20117586851119995 + 0.1 * 6.353896617889404
Epoch 340, val loss: 0.6759293675422668
Epoch 350, training loss: 0.8135809898376465 = 0.17983856797218323 + 0.1 * 6.337424278259277
Epoch 350, val loss: 0.6759132742881775
Epoch 360, training loss: 0.7931139469146729 = 0.16020670533180237 + 0.1 * 6.329071998596191
Epoch 360, val loss: 0.677242636680603
Epoch 370, training loss: 0.775216817855835 = 0.14248718321323395 + 0.1 * 6.327296257019043
Epoch 370, val loss: 0.6797350645065308
Epoch 380, training loss: 0.7591921091079712 = 0.12689492106437683 + 0.1 * 6.322972297668457
Epoch 380, val loss: 0.6833496689796448
Epoch 390, training loss: 0.744469404220581 = 0.11335169523954391 + 0.1 * 6.311176776885986
Epoch 390, val loss: 0.6879346966743469
Epoch 400, training loss: 0.7349461913108826 = 0.10165376961231232 + 0.1 * 6.3329243659973145
Epoch 400, val loss: 0.6933414936065674
Epoch 410, training loss: 0.7218905687332153 = 0.09163357317447662 + 0.1 * 6.30256986618042
Epoch 410, val loss: 0.6994089484214783
Epoch 420, training loss: 0.7136658430099487 = 0.08297820389270782 + 0.1 * 6.306876182556152
Epoch 420, val loss: 0.7059113383293152
Epoch 430, training loss: 0.7054228186607361 = 0.0755043625831604 + 0.1 * 6.299184322357178
Epoch 430, val loss: 0.7127686142921448
Epoch 440, training loss: 0.6977665424346924 = 0.06898944824934006 + 0.1 * 6.287770748138428
Epoch 440, val loss: 0.7198514342308044
Epoch 450, training loss: 0.691341757774353 = 0.06326814740896225 + 0.1 * 6.280735969543457
Epoch 450, val loss: 0.7271143198013306
Epoch 460, training loss: 0.6862120032310486 = 0.058222826570272446 + 0.1 * 6.279891490936279
Epoch 460, val loss: 0.7345064878463745
Epoch 470, training loss: 0.6813251972198486 = 0.053743839263916016 + 0.1 * 6.275813579559326
Epoch 470, val loss: 0.741908073425293
Epoch 480, training loss: 0.6779292821884155 = 0.04976314678788185 + 0.1 * 6.281661033630371
Epoch 480, val loss: 0.7493863701820374
Epoch 490, training loss: 0.6728780269622803 = 0.046202246099710464 + 0.1 * 6.266757488250732
Epoch 490, val loss: 0.7567664384841919
Epoch 500, training loss: 0.6687321662902832 = 0.04299885779619217 + 0.1 * 6.257332801818848
Epoch 500, val loss: 0.7641656994819641
Epoch 510, training loss: 0.6670016646385193 = 0.040099140256643295 + 0.1 * 6.2690253257751465
Epoch 510, val loss: 0.7714926600456238
Epoch 520, training loss: 0.6626477241516113 = 0.037484075874090195 + 0.1 * 6.251636505126953
Epoch 520, val loss: 0.7787978053092957
Epoch 530, training loss: 0.6601837873458862 = 0.03511551395058632 + 0.1 * 6.250682353973389
Epoch 530, val loss: 0.7860201001167297
Epoch 540, training loss: 0.657889723777771 = 0.03295774757862091 + 0.1 * 6.249320030212402
Epoch 540, val loss: 0.7931461930274963
Epoch 550, training loss: 0.6550509333610535 = 0.030993109568953514 + 0.1 * 6.2405781745910645
Epoch 550, val loss: 0.800182044506073
Epoch 560, training loss: 0.6544269919395447 = 0.029195787385106087 + 0.1 * 6.252311706542969
Epoch 560, val loss: 0.8070874810218811
Epoch 570, training loss: 0.6513122320175171 = 0.02755635604262352 + 0.1 * 6.237558364868164
Epoch 570, val loss: 0.8138684034347534
Epoch 580, training loss: 0.6492395401000977 = 0.026050368323922157 + 0.1 * 6.231891632080078
Epoch 580, val loss: 0.8205544352531433
Epoch 590, training loss: 0.6497207283973694 = 0.02466307207942009 + 0.1 * 6.250576496124268
Epoch 590, val loss: 0.8271437883377075
Epoch 600, training loss: 0.6458646655082703 = 0.023388775065541267 + 0.1 * 6.224758625030518
Epoch 600, val loss: 0.8335879445075989
Epoch 610, training loss: 0.644427478313446 = 0.022214386612176895 + 0.1 * 6.22213077545166
Epoch 610, val loss: 0.839911937713623
Epoch 620, training loss: 0.6435932517051697 = 0.021126477047801018 + 0.1 * 6.224668025970459
Epoch 620, val loss: 0.8461465239524841
Epoch 630, training loss: 0.6415462493896484 = 0.02012099139392376 + 0.1 * 6.214252471923828
Epoch 630, val loss: 0.8522753715515137
Epoch 640, training loss: 0.6404072046279907 = 0.019187452271580696 + 0.1 * 6.212197303771973
Epoch 640, val loss: 0.8582934737205505
Epoch 650, training loss: 0.6398216485977173 = 0.018317654728889465 + 0.1 * 6.21504020690918
Epoch 650, val loss: 0.8641970753669739
Epoch 660, training loss: 0.6383381485939026 = 0.017510073259472847 + 0.1 * 6.20828104019165
Epoch 660, val loss: 0.8700023293495178
Epoch 670, training loss: 0.6372745633125305 = 0.01675843819975853 + 0.1 * 6.205161094665527
Epoch 670, val loss: 0.8756773471832275
Epoch 680, training loss: 0.6366863250732422 = 0.016054660081863403 + 0.1 * 6.206315994262695
Epoch 680, val loss: 0.8812757134437561
Epoch 690, training loss: 0.6354842782020569 = 0.015394755639135838 + 0.1 * 6.200895309448242
Epoch 690, val loss: 0.886780321598053
Epoch 700, training loss: 0.6345921754837036 = 0.014776758849620819 + 0.1 * 6.198153972625732
Epoch 700, val loss: 0.8921839594841003
Epoch 710, training loss: 0.6364632248878479 = 0.014195593073964119 + 0.1 * 6.222675800323486
Epoch 710, val loss: 0.8975388407707214
Epoch 720, training loss: 0.6327557563781738 = 0.0136528629809618 + 0.1 * 6.191028594970703
Epoch 720, val loss: 0.9027428030967712
Epoch 730, training loss: 0.6327874064445496 = 0.013145574368536472 + 0.1 * 6.196418285369873
Epoch 730, val loss: 0.9078377485275269
Epoch 740, training loss: 0.6313847303390503 = 0.01266457512974739 + 0.1 * 6.187201499938965
Epoch 740, val loss: 0.912825345993042
Epoch 750, training loss: 0.6333754658699036 = 0.012211321853101254 + 0.1 * 6.211641311645508
Epoch 750, val loss: 0.917776346206665
Epoch 760, training loss: 0.6302440166473389 = 0.011785364709794521 + 0.1 * 6.184586048126221
Epoch 760, val loss: 0.922609806060791
Epoch 770, training loss: 0.6297070384025574 = 0.011383512057363987 + 0.1 * 6.183235168457031
Epoch 770, val loss: 0.9273361563682556
Epoch 780, training loss: 0.6294843554496765 = 0.011001965031027794 + 0.1 * 6.184823989868164
Epoch 780, val loss: 0.9320192933082581
Epoch 790, training loss: 0.6281911730766296 = 0.01063910685479641 + 0.1 * 6.175520420074463
Epoch 790, val loss: 0.9366332292556763
Epoch 800, training loss: 0.6286636590957642 = 0.010296355932950974 + 0.1 * 6.183672904968262
Epoch 800, val loss: 0.9412031173706055
Epoch 810, training loss: 0.6274171471595764 = 0.009971346706151962 + 0.1 * 6.174458026885986
Epoch 810, val loss: 0.9457223415374756
Epoch 820, training loss: 0.6270569562911987 = 0.009663524106144905 + 0.1 * 6.173934459686279
Epoch 820, val loss: 0.9501156210899353
Epoch 830, training loss: 0.626761257648468 = 0.009369996376335621 + 0.1 * 6.17391300201416
Epoch 830, val loss: 0.9544587135314941
Epoch 840, training loss: 0.6258940696716309 = 0.00909047294408083 + 0.1 * 6.168035984039307
Epoch 840, val loss: 0.9587045311927795
Epoch 850, training loss: 0.6267303824424744 = 0.008824271149933338 + 0.1 * 6.179060935974121
Epoch 850, val loss: 0.9629011154174805
Epoch 860, training loss: 0.6264618635177612 = 0.008570989593863487 + 0.1 * 6.178908824920654
Epoch 860, val loss: 0.9670508503913879
Epoch 870, training loss: 0.6245962977409363 = 0.008329316973686218 + 0.1 * 6.162669658660889
Epoch 870, val loss: 0.9710911512374878
Epoch 880, training loss: 0.6252714991569519 = 0.008099065162241459 + 0.1 * 6.171724319458008
Epoch 880, val loss: 0.9751028418540955
Epoch 890, training loss: 0.6236960887908936 = 0.007879185490310192 + 0.1 * 6.158168792724609
Epoch 890, val loss: 0.9790667295455933
Epoch 900, training loss: 0.6249529123306274 = 0.007668818347156048 + 0.1 * 6.1728410720825195
Epoch 900, val loss: 0.9829468727111816
Epoch 910, training loss: 0.62376868724823 = 0.007466656621545553 + 0.1 * 6.163020133972168
Epoch 910, val loss: 0.9868228435516357
Epoch 920, training loss: 0.6230853796005249 = 0.007275266107171774 + 0.1 * 6.158100605010986
Epoch 920, val loss: 0.9905595779418945
Epoch 930, training loss: 0.6231118440628052 = 0.0070907725021243095 + 0.1 * 6.160211086273193
Epoch 930, val loss: 0.9942850470542908
Epoch 940, training loss: 0.6221991181373596 = 0.0069143520668148994 + 0.1 * 6.1528472900390625
Epoch 940, val loss: 0.9978887438774109
Epoch 950, training loss: 0.6221834421157837 = 0.006745537277311087 + 0.1 * 6.154379367828369
Epoch 950, val loss: 1.0014532804489136
Epoch 960, training loss: 0.622811496257782 = 0.006582791917026043 + 0.1 * 6.162286758422852
Epoch 960, val loss: 1.0050134658813477
Epoch 970, training loss: 0.621299147605896 = 0.006426241714507341 + 0.1 * 6.14872932434082
Epoch 970, val loss: 1.0085399150848389
Epoch 980, training loss: 0.6212844252586365 = 0.00627733301371336 + 0.1 * 6.150070667266846
Epoch 980, val loss: 1.0119693279266357
Epoch 990, training loss: 0.6201657652854919 = 0.006133084185421467 + 0.1 * 6.140326499938965
Epoch 990, val loss: 1.0153841972351074
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6052
Flip ASR: 0.5333/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.769972324371338 = 1.9325954914093018 + 0.1 * 8.373767852783203
Epoch 0, val loss: 1.937042236328125
Epoch 10, training loss: 2.7605252265930176 = 1.9231690168380737 + 0.1 * 8.373560905456543
Epoch 10, val loss: 1.9277167320251465
Epoch 20, training loss: 2.7487471103668213 = 1.911500096321106 + 0.1 * 8.37247085571289
Epoch 20, val loss: 1.9155757427215576
Epoch 30, training loss: 2.731212854385376 = 1.8946468830108643 + 0.1 * 8.3656587600708
Epoch 30, val loss: 1.8976306915283203
Epoch 40, training loss: 2.7013516426086426 = 1.869127869606018 + 0.1 * 8.322237968444824
Epoch 40, val loss: 1.8707036972045898
Epoch 50, training loss: 2.633793592453003 = 1.834438681602478 + 0.1 * 7.993549823760986
Epoch 50, val loss: 1.8355165719985962
Epoch 60, training loss: 2.550201416015625 = 1.7958698272705078 + 0.1 * 7.543315410614014
Epoch 60, val loss: 1.7984189987182617
Epoch 70, training loss: 2.4773616790771484 = 1.7578502893447876 + 0.1 * 7.1951141357421875
Epoch 70, val loss: 1.7635127305984497
Epoch 80, training loss: 2.408111333847046 = 1.7177932262420654 + 0.1 * 6.903181552886963
Epoch 80, val loss: 1.7281135320663452
Epoch 90, training loss: 2.34086275100708 = 1.662208080291748 + 0.1 * 6.78654670715332
Epoch 90, val loss: 1.6778829097747803
Epoch 100, training loss: 2.261516571044922 = 1.5870381593704224 + 0.1 * 6.7447829246521
Epoch 100, val loss: 1.6119788885116577
Epoch 110, training loss: 2.168910026550293 = 1.4970238208770752 + 0.1 * 6.718863010406494
Epoch 110, val loss: 1.5366493463516235
Epoch 120, training loss: 2.071406602859497 = 1.400803565979004 + 0.1 * 6.70603084564209
Epoch 120, val loss: 1.4570109844207764
Epoch 130, training loss: 1.9758117198944092 = 1.3064099550247192 + 0.1 * 6.69401741027832
Epoch 130, val loss: 1.3824785947799683
Epoch 140, training loss: 1.885122537612915 = 1.2172281742095947 + 0.1 * 6.678943634033203
Epoch 140, val loss: 1.3149110078811646
Epoch 150, training loss: 1.7998683452606201 = 1.1339255571365356 + 0.1 * 6.659427642822266
Epoch 150, val loss: 1.2541307210922241
Epoch 160, training loss: 1.7194218635559082 = 1.0553821325302124 + 0.1 * 6.640397548675537
Epoch 160, val loss: 1.1981781721115112
Epoch 170, training loss: 1.6420618295669556 = 0.9801065921783447 + 0.1 * 6.619552135467529
Epoch 170, val loss: 1.1445763111114502
Epoch 180, training loss: 1.5659124851226807 = 0.9058916568756104 + 0.1 * 6.600208759307861
Epoch 180, val loss: 1.0914785861968994
Epoch 190, training loss: 1.4909257888793945 = 0.8324939608573914 + 0.1 * 6.584317684173584
Epoch 190, val loss: 1.038239598274231
Epoch 200, training loss: 1.4180781841278076 = 0.7610693573951721 + 0.1 * 6.570088863372803
Epoch 200, val loss: 0.9869593977928162
Epoch 210, training loss: 1.348548412322998 = 0.6927179098129272 + 0.1 * 6.558304309844971
Epoch 210, val loss: 0.9380848407745361
Epoch 220, training loss: 1.2835276126861572 = 0.6296519041061401 + 0.1 * 6.538757801055908
Epoch 220, val loss: 0.8942664265632629
Epoch 230, training loss: 1.2259272336959839 = 0.5732938647270203 + 0.1 * 6.526333332061768
Epoch 230, val loss: 0.8568634986877441
Epoch 240, training loss: 1.1755785942077637 = 0.5247226357460022 + 0.1 * 6.5085601806640625
Epoch 240, val loss: 0.8271400332450867
Epoch 250, training loss: 1.1324130296707153 = 0.48321202397346497 + 0.1 * 6.49200963973999
Epoch 250, val loss: 0.8048101663589478
Epoch 260, training loss: 1.0952430963516235 = 0.447277694940567 + 0.1 * 6.479653835296631
Epoch 260, val loss: 0.7887588143348694
Epoch 270, training loss: 1.0638611316680908 = 0.4150577187538147 + 0.1 * 6.488033294677734
Epoch 270, val loss: 0.7769222855567932
Epoch 280, training loss: 1.0309104919433594 = 0.3852456212043762 + 0.1 * 6.456649303436279
Epoch 280, val loss: 0.7675811648368835
Epoch 290, training loss: 1.000256896018982 = 0.3563733994960785 + 0.1 * 6.4388346672058105
Epoch 290, val loss: 0.7596184611320496
Epoch 300, training loss: 0.9706666469573975 = 0.3279176354408264 + 0.1 * 6.427489757537842
Epoch 300, val loss: 0.752643883228302
Epoch 310, training loss: 0.9423753023147583 = 0.30004096031188965 + 0.1 * 6.423343181610107
Epoch 310, val loss: 0.746813952922821
Epoch 320, training loss: 0.9152908325195312 = 0.2726579010486603 + 0.1 * 6.426329612731934
Epoch 320, val loss: 0.7421003580093384
Epoch 330, training loss: 0.8863815665245056 = 0.24603931605815887 + 0.1 * 6.4034223556518555
Epoch 330, val loss: 0.7385117411613464
Epoch 340, training loss: 0.861737072467804 = 0.22030502557754517 + 0.1 * 6.414320468902588
Epoch 340, val loss: 0.7360990643501282
Epoch 350, training loss: 0.8351578712463379 = 0.19592931866645813 + 0.1 * 6.392284870147705
Epoch 350, val loss: 0.7352061867713928
Epoch 360, training loss: 0.8115009069442749 = 0.17324291169643402 + 0.1 * 6.382579803466797
Epoch 360, val loss: 0.7355840802192688
Epoch 370, training loss: 0.7921508550643921 = 0.15266135334968567 + 0.1 * 6.394895076751709
Epoch 370, val loss: 0.7374799251556396
Epoch 380, training loss: 0.771935224533081 = 0.13455983996391296 + 0.1 * 6.373753547668457
Epoch 380, val loss: 0.7406497001647949
Epoch 390, training loss: 0.7547286152839661 = 0.11882644891738892 + 0.1 * 6.3590216636657715
Epoch 390, val loss: 0.745185136795044
Epoch 400, training loss: 0.7451488971710205 = 0.10533039271831512 + 0.1 * 6.398184776306152
Epoch 400, val loss: 0.7510162591934204
Epoch 410, training loss: 0.7304315567016602 = 0.09396316856145859 + 0.1 * 6.364684104919434
Epoch 410, val loss: 0.7577323913574219
Epoch 420, training loss: 0.7188866138458252 = 0.08428642153739929 + 0.1 * 6.346002101898193
Epoch 420, val loss: 0.7653436064720154
Epoch 430, training loss: 0.7091389298439026 = 0.07597655802965164 + 0.1 * 6.331623554229736
Epoch 430, val loss: 0.7737058401107788
Epoch 440, training loss: 0.7028071880340576 = 0.06881270557641983 + 0.1 * 6.339944362640381
Epoch 440, val loss: 0.7824848890304565
Epoch 450, training loss: 0.6945571899414062 = 0.06264417618513107 + 0.1 * 6.319129943847656
Epoch 450, val loss: 0.7916768789291382
Epoch 460, training loss: 0.6898568868637085 = 0.057255420833826065 + 0.1 * 6.326014041900635
Epoch 460, val loss: 0.8011175990104675
Epoch 470, training loss: 0.684151291847229 = 0.05254724621772766 + 0.1 * 6.3160400390625
Epoch 470, val loss: 0.8107153177261353
Epoch 480, training loss: 0.6793136596679688 = 0.048392701894044876 + 0.1 * 6.30920934677124
Epoch 480, val loss: 0.8203786611557007
Epoch 490, training loss: 0.6747514009475708 = 0.044710952788591385 + 0.1 * 6.3004045486450195
Epoch 490, val loss: 0.8301459550857544
Epoch 500, training loss: 0.6709746718406677 = 0.041444189846515656 + 0.1 * 6.295304775238037
Epoch 500, val loss: 0.8397814631462097
Epoch 510, training loss: 0.6699508428573608 = 0.03852040320634842 + 0.1 * 6.314304351806641
Epoch 510, val loss: 0.849485456943512
Epoch 520, training loss: 0.6644849181175232 = 0.03590884059667587 + 0.1 * 6.285760879516602
Epoch 520, val loss: 0.858951210975647
Epoch 530, training loss: 0.6615415811538696 = 0.0335523821413517 + 0.1 * 6.279891490936279
Epoch 530, val loss: 0.8684142827987671
Epoch 540, training loss: 0.6591777801513672 = 0.031421877443790436 + 0.1 * 6.27755880355835
Epoch 540, val loss: 0.877703070640564
Epoch 550, training loss: 0.6559625864028931 = 0.029498614370822906 + 0.1 * 6.264639377593994
Epoch 550, val loss: 0.8868252038955688
Epoch 560, training loss: 0.6553131341934204 = 0.027744753286242485 + 0.1 * 6.275683403015137
Epoch 560, val loss: 0.8958559036254883
Epoch 570, training loss: 0.6526228189468384 = 0.026149962097406387 + 0.1 * 6.26472806930542
Epoch 570, val loss: 0.9046843647956848
Epoch 580, training loss: 0.652096688747406 = 0.024690840393304825 + 0.1 * 6.274057865142822
Epoch 580, val loss: 0.9134441614151001
Epoch 590, training loss: 0.6486179232597351 = 0.023356497287750244 + 0.1 * 6.2526140213012695
Epoch 590, val loss: 0.9219876527786255
Epoch 600, training loss: 0.6474387049674988 = 0.022133275866508484 + 0.1 * 6.253054141998291
Epoch 600, val loss: 0.9303981065750122
Epoch 610, training loss: 0.6454275846481323 = 0.021005364134907722 + 0.1 * 6.2442216873168945
Epoch 610, val loss: 0.9386558532714844
Epoch 620, training loss: 0.6448550224304199 = 0.019966302439570427 + 0.1 * 6.248887062072754
Epoch 620, val loss: 0.9468356370925903
Epoch 630, training loss: 0.6444355845451355 = 0.019003145396709442 + 0.1 * 6.254324436187744
Epoch 630, val loss: 0.9548628926277161
Epoch 640, training loss: 0.642202615737915 = 0.01811261847615242 + 0.1 * 6.240900039672852
Epoch 640, val loss: 0.962729811668396
Epoch 650, training loss: 0.6424381136894226 = 0.017283722758293152 + 0.1 * 6.2515435218811035
Epoch 650, val loss: 0.9705007076263428
Epoch 660, training loss: 0.6403833627700806 = 0.01651516743004322 + 0.1 * 6.238681316375732
Epoch 660, val loss: 0.9780109524726868
Epoch 670, training loss: 0.6383601427078247 = 0.015802083536982536 + 0.1 * 6.22558069229126
Epoch 670, val loss: 0.9853734970092773
Epoch 680, training loss: 0.6370923519134521 = 0.01513581071048975 + 0.1 * 6.219565391540527
Epoch 680, val loss: 0.9926632642745972
Epoch 690, training loss: 0.6372000575065613 = 0.01451123971492052 + 0.1 * 6.226888179779053
Epoch 690, val loss: 0.9997790455818176
Epoch 700, training loss: 0.635875403881073 = 0.01392889954149723 + 0.1 * 6.2194647789001465
Epoch 700, val loss: 1.0067557096481323
Epoch 710, training loss: 0.6347314119338989 = 0.013384689576923847 + 0.1 * 6.213467121124268
Epoch 710, val loss: 1.0136359930038452
Epoch 720, training loss: 0.6339031457901001 = 0.012872364372015 + 0.1 * 6.210308074951172
Epoch 720, val loss: 1.020365834236145
Epoch 730, training loss: 0.6335921287536621 = 0.012392090633511543 + 0.1 * 6.211999893188477
Epoch 730, val loss: 1.0269709825515747
Epoch 740, training loss: 0.632095217704773 = 0.011939860880374908 + 0.1 * 6.2015533447265625
Epoch 740, val loss: 1.0335040092468262
Epoch 750, training loss: 0.6312467455863953 = 0.011512872762978077 + 0.1 * 6.197338581085205
Epoch 750, val loss: 1.0399231910705566
Epoch 760, training loss: 0.6315586566925049 = 0.011108952574431896 + 0.1 * 6.20449686050415
Epoch 760, val loss: 1.0461395978927612
Epoch 770, training loss: 0.6300690174102783 = 0.010730109177529812 + 0.1 * 6.193388938903809
Epoch 770, val loss: 1.0522695779800415
Epoch 780, training loss: 0.6320984959602356 = 0.010372108779847622 + 0.1 * 6.217263698577881
Epoch 780, val loss: 1.0583409070968628
Epoch 790, training loss: 0.6292493343353271 = 0.0100339250639081 + 0.1 * 6.192154407501221
Epoch 790, val loss: 1.0642863512039185
Epoch 800, training loss: 0.6285357475280762 = 0.009713453240692616 + 0.1 * 6.188222408294678
Epoch 800, val loss: 1.070176124572754
Epoch 810, training loss: 0.6292617321014404 = 0.009407461620867252 + 0.1 * 6.198542594909668
Epoch 810, val loss: 1.07584810256958
Epoch 820, training loss: 0.6274533271789551 = 0.009118695743381977 + 0.1 * 6.183346271514893
Epoch 820, val loss: 1.081482172012329
Epoch 830, training loss: 0.6267972588539124 = 0.008844379335641861 + 0.1 * 6.179528713226318
Epoch 830, val loss: 1.0871052742004395
Epoch 840, training loss: 0.6291398406028748 = 0.00858181994408369 + 0.1 * 6.205580234527588
Epoch 840, val loss: 1.0925315618515015
Epoch 850, training loss: 0.6264873147010803 = 0.008333052508533001 + 0.1 * 6.181542873382568
Epoch 850, val loss: 1.0978589057922363
Epoch 860, training loss: 0.6258504390716553 = 0.008096907287836075 + 0.1 * 6.177535057067871
Epoch 860, val loss: 1.1032145023345947
Epoch 870, training loss: 0.6261951327323914 = 0.007870104163885117 + 0.1 * 6.183250427246094
Epoch 870, val loss: 1.1083970069885254
Epoch 880, training loss: 0.6253657341003418 = 0.007654651999473572 + 0.1 * 6.17711067199707
Epoch 880, val loss: 1.1135139465332031
Epoch 890, training loss: 0.6254058480262756 = 0.007448501419275999 + 0.1 * 6.1795735359191895
Epoch 890, val loss: 1.1185832023620605
Epoch 900, training loss: 0.6247137188911438 = 0.007251327391713858 + 0.1 * 6.174623966217041
Epoch 900, val loss: 1.1235551834106445
Epoch 910, training loss: 0.6241347789764404 = 0.007063014432787895 + 0.1 * 6.170717239379883
Epoch 910, val loss: 1.1284290552139282
Epoch 920, training loss: 0.6241037249565125 = 0.006883515510708094 + 0.1 * 6.172202110290527
Epoch 920, val loss: 1.1332777738571167
Epoch 930, training loss: 0.6232662796974182 = 0.006710651330649853 + 0.1 * 6.165555953979492
Epoch 930, val loss: 1.1380168199539185
Epoch 940, training loss: 0.6234597563743591 = 0.006545174401253462 + 0.1 * 6.169145584106445
Epoch 940, val loss: 1.142738699913025
Epoch 950, training loss: 0.6229021549224854 = 0.00638583255931735 + 0.1 * 6.165163516998291
Epoch 950, val loss: 1.1473619937896729
Epoch 960, training loss: 0.6231943368911743 = 0.006233645603060722 + 0.1 * 6.169607162475586
Epoch 960, val loss: 1.1519358158111572
Epoch 970, training loss: 0.6225361824035645 = 0.006087244022637606 + 0.1 * 6.164489269256592
Epoch 970, val loss: 1.1564254760742188
Epoch 980, training loss: 0.6213430166244507 = 0.005946559365838766 + 0.1 * 6.153964519500732
Epoch 980, val loss: 1.1608679294586182
Epoch 990, training loss: 0.6212714910507202 = 0.005811536218971014 + 0.1 * 6.154599666595459
Epoch 990, val loss: 1.1652820110321045
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7454
Flip ASR: 0.7067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7917428016662598 = 1.9543763399124146 + 0.1 * 8.373663902282715
Epoch 0, val loss: 1.962890625
Epoch 10, training loss: 2.7806832790374756 = 1.9433764219284058 + 0.1 * 8.373068809509277
Epoch 10, val loss: 1.9507980346679688
Epoch 20, training loss: 2.767212390899658 = 1.930181622505188 + 0.1 * 8.370307922363281
Epoch 20, val loss: 1.9356765747070312
Epoch 30, training loss: 2.7475743293762207 = 1.91202712059021 + 0.1 * 8.355472564697266
Epoch 30, val loss: 1.9144370555877686
Epoch 40, training loss: 2.711280345916748 = 1.8861325979232788 + 0.1 * 8.251476287841797
Epoch 40, val loss: 1.8844598531723022
Epoch 50, training loss: 2.620987892150879 = 1.8543530702590942 + 0.1 * 7.666347980499268
Epoch 50, val loss: 1.849869966506958
Epoch 60, training loss: 2.551396608352661 = 1.8208729028701782 + 0.1 * 7.30523681640625
Epoch 60, val loss: 1.8148945569992065
Epoch 70, training loss: 2.486044406890869 = 1.7851370573043823 + 0.1 * 7.0090742111206055
Epoch 70, val loss: 1.7804310321807861
Epoch 80, training loss: 2.4307312965393066 = 1.7494254112243652 + 0.1 * 6.8130598068237305
Epoch 80, val loss: 1.7487173080444336
Epoch 90, training loss: 2.382190704345703 = 1.7110540866851807 + 0.1 * 6.71136474609375
Epoch 90, val loss: 1.7153548002243042
Epoch 100, training loss: 2.3251237869262695 = 1.6600871086120605 + 0.1 * 6.65036678314209
Epoch 100, val loss: 1.6727347373962402
Epoch 110, training loss: 2.2528436183929443 = 1.59159517288208 + 0.1 * 6.612484931945801
Epoch 110, val loss: 1.6188246011734009
Epoch 120, training loss: 2.163097858428955 = 1.504603385925293 + 0.1 * 6.5849456787109375
Epoch 120, val loss: 1.5521737337112427
Epoch 130, training loss: 2.0635976791381836 = 1.4069268703460693 + 0.1 * 6.566707611083984
Epoch 130, val loss: 1.478853702545166
Epoch 140, training loss: 1.965097427368164 = 1.3099912405014038 + 0.1 * 6.551061153411865
Epoch 140, val loss: 1.4090543985366821
Epoch 150, training loss: 1.8742995262145996 = 1.2205368280410767 + 0.1 * 6.537626266479492
Epoch 150, val loss: 1.3475430011749268
Epoch 160, training loss: 1.7907238006591797 = 1.138567328453064 + 0.1 * 6.521564960479736
Epoch 160, val loss: 1.2916089296340942
Epoch 170, training loss: 1.7121624946594238 = 1.0615431070327759 + 0.1 * 6.5061936378479
Epoch 170, val loss: 1.237587571144104
Epoch 180, training loss: 1.636624813079834 = 0.987342894077301 + 0.1 * 6.492819309234619
Epoch 180, val loss: 1.184044361114502
Epoch 190, training loss: 1.5654394626617432 = 0.9171282649040222 + 0.1 * 6.483111381530762
Epoch 190, val loss: 1.1333000659942627
Epoch 200, training loss: 1.4974536895751953 = 0.8501552939414978 + 0.1 * 6.472984313964844
Epoch 200, val loss: 1.0857799053192139
Epoch 210, training loss: 1.4296698570251465 = 0.7833767533302307 + 0.1 * 6.462930202484131
Epoch 210, val loss: 1.0387083292007446
Epoch 220, training loss: 1.3605144023895264 = 0.7153258323669434 + 0.1 * 6.451884746551514
Epoch 220, val loss: 0.9910905361175537
Epoch 230, training loss: 1.291672945022583 = 0.6468297243118286 + 0.1 * 6.448431968688965
Epoch 230, val loss: 0.9436845183372498
Epoch 240, training loss: 1.2244988679885864 = 0.581194281578064 + 0.1 * 6.433045864105225
Epoch 240, val loss: 0.8990494608879089
Epoch 250, training loss: 1.1625984907150269 = 0.5201360583305359 + 0.1 * 6.424623966217041
Epoch 250, val loss: 0.8590441942214966
Epoch 260, training loss: 1.107093095779419 = 0.46569621562957764 + 0.1 * 6.413969039916992
Epoch 260, val loss: 0.8258041739463806
Epoch 270, training loss: 1.0588067770004272 = 0.4184843599796295 + 0.1 * 6.403223991394043
Epoch 270, val loss: 0.80022132396698
Epoch 280, training loss: 1.017138957977295 = 0.37807878851890564 + 0.1 * 6.390601634979248
Epoch 280, val loss: 0.7814480662345886
Epoch 290, training loss: 0.9828581809997559 = 0.3432406783103943 + 0.1 * 6.396174907684326
Epoch 290, val loss: 0.7682499289512634
Epoch 300, training loss: 0.9507614374160767 = 0.31324854493141174 + 0.1 * 6.375128746032715
Epoch 300, val loss: 0.7595205903053284
Epoch 310, training loss: 0.9234709739685059 = 0.28690773248672485 + 0.1 * 6.3656325340271
Epoch 310, val loss: 0.7540827989578247
Epoch 320, training loss: 0.9002270102500916 = 0.2635733485221863 + 0.1 * 6.366536617279053
Epoch 320, val loss: 0.751331090927124
Epoch 330, training loss: 0.877396821975708 = 0.24260695278644562 + 0.1 * 6.347898483276367
Epoch 330, val loss: 0.7508169412612915
Epoch 340, training loss: 0.8581055402755737 = 0.2234506905078888 + 0.1 * 6.346548557281494
Epoch 340, val loss: 0.7521000504493713
Epoch 350, training loss: 0.8392363786697388 = 0.2058645337820053 + 0.1 * 6.333718299865723
Epoch 350, val loss: 0.7549396753311157
Epoch 360, training loss: 0.8220992684364319 = 0.18945205211639404 + 0.1 * 6.326472282409668
Epoch 360, val loss: 0.7592243552207947
Epoch 370, training loss: 0.8062991499900818 = 0.17409028112888336 + 0.1 * 6.322088718414307
Epoch 370, val loss: 0.7647590637207031
Epoch 380, training loss: 0.7924627065658569 = 0.15979963541030884 + 0.1 * 6.326630592346191
Epoch 380, val loss: 0.7714086174964905
Epoch 390, training loss: 0.779171347618103 = 0.14657863974571228 + 0.1 * 6.325926780700684
Epoch 390, val loss: 0.7789523601531982
Epoch 400, training loss: 0.7652572393417358 = 0.13441510498523712 + 0.1 * 6.308421611785889
Epoch 400, val loss: 0.7870882749557495
Epoch 410, training loss: 0.7547577619552612 = 0.12324431538581848 + 0.1 * 6.315134525299072
Epoch 410, val loss: 0.7957264184951782
Epoch 420, training loss: 0.7428017258644104 = 0.11304988712072372 + 0.1 * 6.297518253326416
Epoch 420, val loss: 0.8046770095825195
Epoch 430, training loss: 0.7343863844871521 = 0.10373901575803757 + 0.1 * 6.306473731994629
Epoch 430, val loss: 0.8139640092849731
Epoch 440, training loss: 0.7247138619422913 = 0.09528117626905441 + 0.1 * 6.2943267822265625
Epoch 440, val loss: 0.823445737361908
Epoch 450, training loss: 0.7158293724060059 = 0.08758256584405899 + 0.1 * 6.282467842102051
Epoch 450, val loss: 0.8331621885299683
Epoch 460, training loss: 0.7098350524902344 = 0.08059117943048477 + 0.1 * 6.292438507080078
Epoch 460, val loss: 0.8430776000022888
Epoch 470, training loss: 0.7018590569496155 = 0.07426215708255768 + 0.1 * 6.2759690284729
Epoch 470, val loss: 0.853066086769104
Epoch 480, training loss: 0.6959501504898071 = 0.06852629780769348 + 0.1 * 6.274238586425781
Epoch 480, val loss: 0.863034725189209
Epoch 490, training loss: 0.6899970173835754 = 0.06333502382040024 + 0.1 * 6.26662015914917
Epoch 490, val loss: 0.8730304837226868
Epoch 500, training loss: 0.6855990290641785 = 0.058620013296604156 + 0.1 * 6.269789695739746
Epoch 500, val loss: 0.88299161195755
Epoch 510, training loss: 0.6804707050323486 = 0.05435258522629738 + 0.1 * 6.261180877685547
Epoch 510, val loss: 0.8928523659706116
Epoch 520, training loss: 0.6756575107574463 = 0.05048806220293045 + 0.1 * 6.251694679260254
Epoch 520, val loss: 0.9026714563369751
Epoch 530, training loss: 0.6735736131668091 = 0.04697425663471222 + 0.1 * 6.265993595123291
Epoch 530, val loss: 0.9123714566230774
Epoch 540, training loss: 0.6692785620689392 = 0.04379291832447052 + 0.1 * 6.254856586456299
Epoch 540, val loss: 0.9219194054603577
Epoch 550, training loss: 0.6653952598571777 = 0.04090622067451477 + 0.1 * 6.244889736175537
Epoch 550, val loss: 0.9313212633132935
Epoch 560, training loss: 0.6626611351966858 = 0.03827451169490814 + 0.1 * 6.243865966796875
Epoch 560, val loss: 0.9405808448791504
Epoch 570, training loss: 0.6604706048965454 = 0.035876188427209854 + 0.1 * 6.245944023132324
Epoch 570, val loss: 0.9496805667877197
Epoch 580, training loss: 0.6574678421020508 = 0.03368871659040451 + 0.1 * 6.237791061401367
Epoch 580, val loss: 0.9586109519004822
Epoch 590, training loss: 0.6548063158988953 = 0.0316857248544693 + 0.1 * 6.231205940246582
Epoch 590, val loss: 0.9674400091171265
Epoch 600, training loss: 0.6527430415153503 = 0.02985139936208725 + 0.1 * 6.228916645050049
Epoch 600, val loss: 0.9760052561759949
Epoch 610, training loss: 0.6506904363632202 = 0.028172550722956657 + 0.1 * 6.2251787185668945
Epoch 610, val loss: 0.9844980835914612
Epoch 620, training loss: 0.6497186422348022 = 0.0266275517642498 + 0.1 * 6.230910778045654
Epoch 620, val loss: 0.9927683472633362
Epoch 630, training loss: 0.6473975777626038 = 0.025205906480550766 + 0.1 * 6.221916675567627
Epoch 630, val loss: 1.0008896589279175
Epoch 640, training loss: 0.6462476849555969 = 0.023892663419246674 + 0.1 * 6.223550319671631
Epoch 640, val loss: 1.0088763236999512
Epoch 650, training loss: 0.6449963450431824 = 0.022679556161165237 + 0.1 * 6.223167896270752
Epoch 650, val loss: 1.0165389776229858
Epoch 660, training loss: 0.6424035429954529 = 0.021562576293945312 + 0.1 * 6.208409309387207
Epoch 660, val loss: 1.024183988571167
Epoch 670, training loss: 0.6415265798568726 = 0.020523201674222946 + 0.1 * 6.210033893585205
Epoch 670, val loss: 1.0316221714019775
Epoch 680, training loss: 0.640013575553894 = 0.0195564366877079 + 0.1 * 6.204570770263672
Epoch 680, val loss: 1.0389004945755005
Epoch 690, training loss: 0.6401654481887817 = 0.018656551837921143 + 0.1 * 6.215088844299316
Epoch 690, val loss: 1.046068549156189
Epoch 700, training loss: 0.6380672454833984 = 0.017818434163928032 + 0.1 * 6.202487945556641
Epoch 700, val loss: 1.0529958009719849
Epoch 710, training loss: 0.6372197866439819 = 0.017039231956005096 + 0.1 * 6.201805114746094
Epoch 710, val loss: 1.0598986148834229
Epoch 720, training loss: 0.6357085704803467 = 0.0163083765655756 + 0.1 * 6.1940016746521
Epoch 720, val loss: 1.0666521787643433
Epoch 730, training loss: 0.6361454129219055 = 0.015621921047568321 + 0.1 * 6.205235004425049
Epoch 730, val loss: 1.0731918811798096
Epoch 740, training loss: 0.6339399218559265 = 0.014980318024754524 + 0.1 * 6.189596176147461
Epoch 740, val loss: 1.0796537399291992
Epoch 750, training loss: 0.633176326751709 = 0.014379667118191719 + 0.1 * 6.187966346740723
Epoch 750, val loss: 1.085993766784668
Epoch 760, training loss: 0.6329625844955444 = 0.0138151366263628 + 0.1 * 6.191473960876465
Epoch 760, val loss: 1.0921739339828491
Epoch 770, training loss: 0.6325423717498779 = 0.013285505585372448 + 0.1 * 6.192568302154541
Epoch 770, val loss: 1.0981587171554565
Epoch 780, training loss: 0.6312405467033386 = 0.012789481319487095 + 0.1 * 6.184510231018066
Epoch 780, val loss: 1.1041419506072998
Epoch 790, training loss: 0.6313101053237915 = 0.012320240028202534 + 0.1 * 6.18989896774292
Epoch 790, val loss: 1.109941005706787
Epoch 800, training loss: 0.6302400231361389 = 0.011877212673425674 + 0.1 * 6.183627605438232
Epoch 800, val loss: 1.1155396699905396
Epoch 810, training loss: 0.6299845576286316 = 0.011460253968834877 + 0.1 * 6.185243129730225
Epoch 810, val loss: 1.121123194694519
Epoch 820, training loss: 0.6292463541030884 = 0.011065778322517872 + 0.1 * 6.181805610656738
Epoch 820, val loss: 1.1265228986740112
Epoch 830, training loss: 0.6282351613044739 = 0.010693909600377083 + 0.1 * 6.175412654876709
Epoch 830, val loss: 1.1318538188934326
Epoch 840, training loss: 0.6284106969833374 = 0.010341554880142212 + 0.1 * 6.180691242218018
Epoch 840, val loss: 1.1370866298675537
Epoch 850, training loss: 0.6293489336967468 = 0.010008341632783413 + 0.1 * 6.193406105041504
Epoch 850, val loss: 1.1421607732772827
Epoch 860, training loss: 0.6270114183425903 = 0.009692667052149773 + 0.1 * 6.173187255859375
Epoch 860, val loss: 1.1471753120422363
Epoch 870, training loss: 0.6260329484939575 = 0.00939316675066948 + 0.1 * 6.166397571563721
Epoch 870, val loss: 1.1521422863006592
Epoch 880, training loss: 0.6272116899490356 = 0.00910778995603323 + 0.1 * 6.181038856506348
Epoch 880, val loss: 1.156948447227478
Epoch 890, training loss: 0.6262049674987793 = 0.008836531080305576 + 0.1 * 6.173684597015381
Epoch 890, val loss: 1.1616069078445435
Epoch 900, training loss: 0.6245688796043396 = 0.008579988963901997 + 0.1 * 6.159888744354248
Epoch 900, val loss: 1.1663199663162231
Epoch 910, training loss: 0.6245299577713013 = 0.008335061371326447 + 0.1 * 6.1619486808776855
Epoch 910, val loss: 1.170952558517456
Epoch 920, training loss: 0.6243577003479004 = 0.00810066144913435 + 0.1 * 6.162569999694824
Epoch 920, val loss: 1.1753405332565308
Epoch 930, training loss: 0.6241599917411804 = 0.007877851836383343 + 0.1 * 6.162821292877197
Epoch 930, val loss: 1.1797854900360107
Epoch 940, training loss: 0.6240370273590088 = 0.007664975710213184 + 0.1 * 6.163720607757568
Epoch 940, val loss: 1.184104561805725
Epoch 950, training loss: 0.6225828528404236 = 0.007461496163159609 + 0.1 * 6.151213645935059
Epoch 950, val loss: 1.188372015953064
Epoch 960, training loss: 0.62246173620224 = 0.007266669999808073 + 0.1 * 6.151950836181641
Epoch 960, val loss: 1.1925908327102661
Epoch 970, training loss: 0.6235873699188232 = 0.007079995237290859 + 0.1 * 6.165073394775391
Epoch 970, val loss: 1.196687936782837
Epoch 980, training loss: 0.6222302913665771 = 0.006901141721755266 + 0.1 * 6.15329122543335
Epoch 980, val loss: 1.2007033824920654
Epoch 990, training loss: 0.6220484375953674 = 0.006730098742991686 + 0.1 * 6.153183460235596
Epoch 990, val loss: 1.2045931816101074
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.7159
Flip ASR: 0.6933/225 nodes
The final ASR:0.68881, 0.06036, Accuracy:0.80494, 0.02014
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7899105548858643 = 1.9525320529937744 + 0.1 * 8.373785018920898
Epoch 0, val loss: 1.9486531019210815
Epoch 10, training loss: 2.7784531116485596 = 1.9410974979400635 + 0.1 * 8.373556137084961
Epoch 10, val loss: 1.9372979402542114
Epoch 20, training loss: 2.76426362991333 = 1.9270399808883667 + 0.1 * 8.372236251831055
Epoch 20, val loss: 1.9229485988616943
Epoch 30, training loss: 2.7436327934265137 = 1.9073079824447632 + 0.1 * 8.36324691772461
Epoch 30, val loss: 1.902665615081787
Epoch 40, training loss: 2.708799362182617 = 1.878751516342163 + 0.1 * 8.300477981567383
Epoch 40, val loss: 1.874083161354065
Epoch 50, training loss: 2.6325037479400635 = 1.8434005975723267 + 0.1 * 7.8910322189331055
Epoch 50, val loss: 1.841248869895935
Epoch 60, training loss: 2.5427589416503906 = 1.8111023902893066 + 0.1 * 7.31656551361084
Epoch 60, val loss: 1.8133745193481445
Epoch 70, training loss: 2.473735809326172 = 1.7821135520935059 + 0.1 * 6.9162211418151855
Epoch 70, val loss: 1.7902500629425049
Epoch 80, training loss: 2.429941415786743 = 1.7533105611801147 + 0.1 * 6.766308784484863
Epoch 80, val loss: 1.7674731016159058
Epoch 90, training loss: 2.3846144676208496 = 1.7167190313339233 + 0.1 * 6.678953647613525
Epoch 90, val loss: 1.7367886304855347
Epoch 100, training loss: 2.3290302753448486 = 1.666961908340454 + 0.1 * 6.6206841468811035
Epoch 100, val loss: 1.6951112747192383
Epoch 110, training loss: 2.2592990398406982 = 1.601799488067627 + 0.1 * 6.5749945640563965
Epoch 110, val loss: 1.6413154602050781
Epoch 120, training loss: 2.178010940551758 = 1.5235812664031982 + 0.1 * 6.544295310974121
Epoch 120, val loss: 1.5776405334472656
Epoch 130, training loss: 2.0933754444122314 = 1.4407432079315186 + 0.1 * 6.526321887969971
Epoch 130, val loss: 1.5109312534332275
Epoch 140, training loss: 2.008622407913208 = 1.357483148574829 + 0.1 * 6.511391639709473
Epoch 140, val loss: 1.4459837675094604
Epoch 150, training loss: 1.9239139556884766 = 1.2744050025939941 + 0.1 * 6.495090007781982
Epoch 150, val loss: 1.3837543725967407
Epoch 160, training loss: 1.8394725322723389 = 1.1913771629333496 + 0.1 * 6.480953693389893
Epoch 160, val loss: 1.3235077857971191
Epoch 170, training loss: 1.7569684982299805 = 1.110042691230774 + 0.1 * 6.469257831573486
Epoch 170, val loss: 1.2657995223999023
Epoch 180, training loss: 1.6774853467941284 = 1.0322185754776 + 0.1 * 6.452667713165283
Epoch 180, val loss: 1.2115658521652222
Epoch 190, training loss: 1.6021682024002075 = 0.957491934299469 + 0.1 * 6.446762561798096
Epoch 190, val loss: 1.1597025394439697
Epoch 200, training loss: 1.5293927192687988 = 0.8868116140365601 + 0.1 * 6.425811767578125
Epoch 200, val loss: 1.1106144189834595
Epoch 210, training loss: 1.4621042013168335 = 0.8199257254600525 + 0.1 * 6.421784400939941
Epoch 210, val loss: 1.0637410879135132
Epoch 220, training loss: 1.3984832763671875 = 0.7575827240943909 + 0.1 * 6.409005165100098
Epoch 220, val loss: 1.0202131271362305
Epoch 230, training loss: 1.3395581245422363 = 0.699657142162323 + 0.1 * 6.399009704589844
Epoch 230, val loss: 0.9805133938789368
Epoch 240, training loss: 1.2858282327651978 = 0.6457862854003906 + 0.1 * 6.400419235229492
Epoch 240, val loss: 0.9447793960571289
Epoch 250, training loss: 1.234288215637207 = 0.5955873131752014 + 0.1 * 6.3870086669921875
Epoch 250, val loss: 0.9135481715202332
Epoch 260, training loss: 1.1862523555755615 = 0.5477362275123596 + 0.1 * 6.385160446166992
Epoch 260, val loss: 0.8861590027809143
Epoch 270, training loss: 1.140169382095337 = 0.501953125 + 0.1 * 6.382162570953369
Epoch 270, val loss: 0.8626102805137634
Epoch 280, training loss: 1.095138430595398 = 0.4582578241825104 + 0.1 * 6.3688063621521
Epoch 280, val loss: 0.8429943919181824
Epoch 290, training loss: 1.053077220916748 = 0.4165641963481903 + 0.1 * 6.365130424499512
Epoch 290, val loss: 0.8268869519233704
Epoch 300, training loss: 1.013324499130249 = 0.37715667486190796 + 0.1 * 6.361678600311279
Epoch 300, val loss: 0.8140907287597656
Epoch 310, training loss: 0.9792863130569458 = 0.34022852778434753 + 0.1 * 6.390578269958496
Epoch 310, val loss: 0.8040563464164734
Epoch 320, training loss: 0.9412990212440491 = 0.3061521649360657 + 0.1 * 6.351468563079834
Epoch 320, val loss: 0.7964973449707031
Epoch 330, training loss: 0.9094133377075195 = 0.2747059166431427 + 0.1 * 6.347074508666992
Epoch 330, val loss: 0.791239857673645
Epoch 340, training loss: 0.8798471093177795 = 0.24587464332580566 + 0.1 * 6.339724540710449
Epoch 340, val loss: 0.7881395220756531
Epoch 350, training loss: 0.8552572131156921 = 0.2197210192680359 + 0.1 * 6.3553619384765625
Epoch 350, val loss: 0.7869247794151306
Epoch 360, training loss: 0.829943060874939 = 0.1963481307029724 + 0.1 * 6.335948944091797
Epoch 360, val loss: 0.7875873446464539
Epoch 370, training loss: 0.808249831199646 = 0.17545828223228455 + 0.1 * 6.327915668487549
Epoch 370, val loss: 0.7897718548774719
Epoch 380, training loss: 0.7894858717918396 = 0.15687371790409088 + 0.1 * 6.3261213302612305
Epoch 380, val loss: 0.7932102680206299
Epoch 390, training loss: 0.7724671959877014 = 0.14046110212802887 + 0.1 * 6.320060729980469
Epoch 390, val loss: 0.7976703643798828
Epoch 400, training loss: 0.7570804953575134 = 0.12600888311862946 + 0.1 * 6.310716152191162
Epoch 400, val loss: 0.8028873205184937
Epoch 410, training loss: 0.7439688444137573 = 0.11329671740531921 + 0.1 * 6.306720733642578
Epoch 410, val loss: 0.8088015913963318
Epoch 420, training loss: 0.7335994839668274 = 0.10213879495859146 + 0.1 * 6.314606666564941
Epoch 420, val loss: 0.8150685429573059
Epoch 430, training loss: 0.72202068567276 = 0.09236402809619904 + 0.1 * 6.296566486358643
Epoch 430, val loss: 0.8217963576316833
Epoch 440, training loss: 0.7127645611763 = 0.08374883979558945 + 0.1 * 6.290157318115234
Epoch 440, val loss: 0.8287953734397888
Epoch 450, training loss: 0.7065695524215698 = 0.07614556699991226 + 0.1 * 6.304239273071289
Epoch 450, val loss: 0.8360551595687866
Epoch 460, training loss: 0.6975402235984802 = 0.0694555714726448 + 0.1 * 6.28084659576416
Epoch 460, val loss: 0.8435648679733276
Epoch 470, training loss: 0.6915115714073181 = 0.06353211402893066 + 0.1 * 6.279794692993164
Epoch 470, val loss: 0.8513203859329224
Epoch 480, training loss: 0.6862989068031311 = 0.05826859548687935 + 0.1 * 6.280303001403809
Epoch 480, val loss: 0.8592718243598938
Epoch 490, training loss: 0.6806589961051941 = 0.05359723046422005 + 0.1 * 6.270617485046387
Epoch 490, val loss: 0.867292046546936
Epoch 500, training loss: 0.6761413812637329 = 0.049441974610090256 + 0.1 * 6.266993999481201
Epoch 500, val loss: 0.8755298852920532
Epoch 510, training loss: 0.6732032299041748 = 0.04572629556059837 + 0.1 * 6.274769306182861
Epoch 510, val loss: 0.8837286233901978
Epoch 520, training loss: 0.6686111688613892 = 0.0424015074968338 + 0.1 * 6.262096405029297
Epoch 520, val loss: 0.8920796513557434
Epoch 530, training loss: 0.6649404764175415 = 0.039416711777448654 + 0.1 * 6.255237102508545
Epoch 530, val loss: 0.9003850221633911
Epoch 540, training loss: 0.6617457270622253 = 0.03672754019498825 + 0.1 * 6.250181674957275
Epoch 540, val loss: 0.9087245464324951
Epoch 550, training loss: 0.6589943766593933 = 0.03429948538541794 + 0.1 * 6.246948719024658
Epoch 550, val loss: 0.9168878197669983
Epoch 560, training loss: 0.6568494439125061 = 0.03210851177573204 + 0.1 * 6.247409343719482
Epoch 560, val loss: 0.9251784086227417
Epoch 570, training loss: 0.6550323367118835 = 0.030118675902485847 + 0.1 * 6.249136447906494
Epoch 570, val loss: 0.9332196712493896
Epoch 580, training loss: 0.6535954475402832 = 0.028310304507613182 + 0.1 * 6.252851486206055
Epoch 580, val loss: 0.9412389397621155
Epoch 590, training loss: 0.6505021452903748 = 0.026665475219488144 + 0.1 * 6.238366603851318
Epoch 590, val loss: 0.9491501450538635
Epoch 600, training loss: 0.6483997106552124 = 0.025160757824778557 + 0.1 * 6.2323899269104
Epoch 600, val loss: 0.9570125937461853
Epoch 610, training loss: 0.6473073959350586 = 0.02378193661570549 + 0.1 * 6.235254287719727
Epoch 610, val loss: 0.9646631479263306
Epoch 620, training loss: 0.6459453701972961 = 0.02251751534640789 + 0.1 * 6.234278678894043
Epoch 620, val loss: 0.9722644686698914
Epoch 630, training loss: 0.6439045071601868 = 0.021356193348765373 + 0.1 * 6.225483417510986
Epoch 630, val loss: 0.9798229336738586
Epoch 640, training loss: 0.6430621147155762 = 0.02028447948396206 + 0.1 * 6.227776527404785
Epoch 640, val loss: 0.9871197938919067
Epoch 650, training loss: 0.6414832472801208 = 0.019296184182167053 + 0.1 * 6.2218708992004395
Epoch 650, val loss: 0.9943832755088806
Epoch 660, training loss: 0.6405925750732422 = 0.01838087849318981 + 0.1 * 6.222116947174072
Epoch 660, val loss: 1.001386046409607
Epoch 670, training loss: 0.6387416124343872 = 0.017534377053380013 + 0.1 * 6.212072372436523
Epoch 670, val loss: 1.0084258317947388
Epoch 680, training loss: 0.6385588049888611 = 0.01674693264067173 + 0.1 * 6.218118667602539
Epoch 680, val loss: 1.0152291059494019
Epoch 690, training loss: 0.6368529200553894 = 0.01601428911089897 + 0.1 * 6.208386421203613
Epoch 690, val loss: 1.021934986114502
Epoch 700, training loss: 0.6363363265991211 = 0.015331792645156384 + 0.1 * 6.210044860839844
Epoch 700, val loss: 1.0285701751708984
Epoch 710, training loss: 0.6354724168777466 = 0.014693450182676315 + 0.1 * 6.207789421081543
Epoch 710, val loss: 1.0349977016448975
Epoch 720, training loss: 0.6349358558654785 = 0.014097978360950947 + 0.1 * 6.208378791809082
Epoch 720, val loss: 1.0413275957107544
Epoch 730, training loss: 0.6338024139404297 = 0.013540629297494888 + 0.1 * 6.202617645263672
Epoch 730, val loss: 1.047477126121521
Epoch 740, training loss: 0.6326975226402283 = 0.013019070029258728 + 0.1 * 6.196784019470215
Epoch 740, val loss: 1.0536024570465088
Epoch 750, training loss: 0.6317558288574219 = 0.0125286765396595 + 0.1 * 6.192271709442139
Epoch 750, val loss: 1.0596650838851929
Epoch 760, training loss: 0.6321600675582886 = 0.012066147290170193 + 0.1 * 6.200939178466797
Epoch 760, val loss: 1.065423607826233
Epoch 770, training loss: 0.6308645606040955 = 0.011631913483142853 + 0.1 * 6.192326545715332
Epoch 770, val loss: 1.0712143182754517
Epoch 780, training loss: 0.6304691433906555 = 0.011223255656659603 + 0.1 * 6.192458629608154
Epoch 780, val loss: 1.0769388675689697
Epoch 790, training loss: 0.6299444437026978 = 0.010837224312126637 + 0.1 * 6.191072463989258
Epoch 790, val loss: 1.0824835300445557
Epoch 800, training loss: 0.6290315985679626 = 0.010472949594259262 + 0.1 * 6.185586452484131
Epoch 800, val loss: 1.0880749225616455
Epoch 810, training loss: 0.6289710402488708 = 0.010127130895853043 + 0.1 * 6.188438892364502
Epoch 810, val loss: 1.0934144258499146
Epoch 820, training loss: 0.6286961436271667 = 0.009799766354262829 + 0.1 * 6.188963413238525
Epoch 820, val loss: 1.098662257194519
Epoch 830, training loss: 0.6275187730789185 = 0.009490539319813251 + 0.1 * 6.180282115936279
Epoch 830, val loss: 1.10394287109375
Epoch 840, training loss: 0.6268117427825928 = 0.009197029285132885 + 0.1 * 6.176146984100342
Epoch 840, val loss: 1.1090378761291504
Epoch 850, training loss: 0.6263752579689026 = 0.008918389678001404 + 0.1 * 6.1745686531066895
Epoch 850, val loss: 1.114137887954712
Epoch 860, training loss: 0.626276969909668 = 0.008652981370687485 + 0.1 * 6.176239967346191
Epoch 860, val loss: 1.1190036535263062
Epoch 870, training loss: 0.6263657212257385 = 0.008401000872254372 + 0.1 * 6.179647445678711
Epoch 870, val loss: 1.1238995790481567
Epoch 880, training loss: 0.6256917119026184 = 0.008160999976098537 + 0.1 * 6.175306797027588
Epoch 880, val loss: 1.1286375522613525
Epoch 890, training loss: 0.6252647042274475 = 0.007933002896606922 + 0.1 * 6.173316955566406
Epoch 890, val loss: 1.1334391832351685
Epoch 900, training loss: 0.6247569918632507 = 0.007715323008596897 + 0.1 * 6.170416355133057
Epoch 900, val loss: 1.1381111145019531
Epoch 910, training loss: 0.6238290071487427 = 0.0075073144398629665 + 0.1 * 6.163216590881348
Epoch 910, val loss: 1.1426576375961304
Epoch 920, training loss: 0.6242364048957825 = 0.007308847736567259 + 0.1 * 6.169275760650635
Epoch 920, val loss: 1.1472607851028442
Epoch 930, training loss: 0.6236238479614258 = 0.007118953857570887 + 0.1 * 6.165049076080322
Epoch 930, val loss: 1.151671290397644
Epoch 940, training loss: 0.6229984164237976 = 0.006937406025826931 + 0.1 * 6.160610198974609
Epoch 940, val loss: 1.1561163663864136
Epoch 950, training loss: 0.6236117482185364 = 0.006763339042663574 + 0.1 * 6.168483734130859
Epoch 950, val loss: 1.160373568534851
Epoch 960, training loss: 0.6228240728378296 = 0.006596626713871956 + 0.1 * 6.162274360656738
Epoch 960, val loss: 1.1645749807357788
Epoch 970, training loss: 0.622210681438446 = 0.0064375256188213825 + 0.1 * 6.157731533050537
Epoch 970, val loss: 1.1687843799591064
Epoch 980, training loss: 0.6217410564422607 = 0.006284606177359819 + 0.1 * 6.154564380645752
Epoch 980, val loss: 1.1729050874710083
Epoch 990, training loss: 0.6223422288894653 = 0.006137534976005554 + 0.1 * 6.162046909332275
Epoch 990, val loss: 1.176881194114685
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5572
Flip ASR: 0.4889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7906405925750732 = 1.9532748460769653 + 0.1 * 8.373658180236816
Epoch 0, val loss: 1.9595329761505127
Epoch 10, training loss: 2.7792837619781494 = 1.9420530796051025 + 0.1 * 8.372305870056152
Epoch 10, val loss: 1.9463636875152588
Epoch 20, training loss: 2.7650465965270996 = 1.9281294345855713 + 0.1 * 8.369170188903809
Epoch 20, val loss: 1.9284875392913818
Epoch 30, training loss: 2.745016574859619 = 1.9088916778564453 + 0.1 * 8.361249923706055
Epoch 30, val loss: 1.9027764797210693
Epoch 40, training loss: 2.7129526138305664 = 1.8830407857894897 + 0.1 * 8.299117088317871
Epoch 40, val loss: 1.8695423603057861
Epoch 50, training loss: 2.6482934951782227 = 1.8527015447616577 + 0.1 * 7.955920696258545
Epoch 50, val loss: 1.8341633081436157
Epoch 60, training loss: 2.5653231143951416 = 1.820303201675415 + 0.1 * 7.450199127197266
Epoch 60, val loss: 1.8002088069915771
Epoch 70, training loss: 2.4939231872558594 = 1.787118911743164 + 0.1 * 7.068043231964111
Epoch 70, val loss: 1.7686357498168945
Epoch 80, training loss: 2.435802936553955 = 1.7535022497177124 + 0.1 * 6.8230061531066895
Epoch 80, val loss: 1.738988995552063
Epoch 90, training loss: 2.3888323307037354 = 1.71706223487854 + 0.1 * 6.717700004577637
Epoch 90, val loss: 1.7084143161773682
Epoch 100, training loss: 2.3364815711975098 = 1.6701730489730835 + 0.1 * 6.663086414337158
Epoch 100, val loss: 1.6695377826690674
Epoch 110, training loss: 2.269657850265503 = 1.6076078414916992 + 0.1 * 6.620500564575195
Epoch 110, val loss: 1.6184356212615967
Epoch 120, training loss: 2.1880199909210205 = 1.5294430255889893 + 0.1 * 6.585768699645996
Epoch 120, val loss: 1.5550724267959595
Epoch 130, training loss: 2.10014271736145 = 1.4437047243118286 + 0.1 * 6.5643792152404785
Epoch 130, val loss: 1.4878069162368774
Epoch 140, training loss: 2.0153703689575195 = 1.3612490892410278 + 0.1 * 6.54121208190918
Epoch 140, val loss: 1.4270895719528198
Epoch 150, training loss: 1.9373900890350342 = 1.2850145101547241 + 0.1 * 6.52375602722168
Epoch 150, val loss: 1.3764023780822754
Epoch 160, training loss: 1.8674466609954834 = 1.2168083190917969 + 0.1 * 6.506382465362549
Epoch 160, val loss: 1.3365379571914673
Epoch 170, training loss: 1.8040895462036133 = 1.155304193496704 + 0.1 * 6.487852573394775
Epoch 170, val loss: 1.3034920692443848
Epoch 180, training loss: 1.7457537651062012 = 1.0975942611694336 + 0.1 * 6.481595039367676
Epoch 180, val loss: 1.2733161449432373
Epoch 190, training loss: 1.6891210079193115 = 1.0421358346939087 + 0.1 * 6.469851016998291
Epoch 190, val loss: 1.244214653968811
Epoch 200, training loss: 1.632096529006958 = 0.9857738614082336 + 0.1 * 6.463226795196533
Epoch 200, val loss: 1.2127968072891235
Epoch 210, training loss: 1.573641300201416 = 0.9275517463684082 + 0.1 * 6.460896015167236
Epoch 210, val loss: 1.1782087087631226
Epoch 220, training loss: 1.5134817361831665 = 0.8682140707969666 + 0.1 * 6.452676773071289
Epoch 220, val loss: 1.1403717994689941
Epoch 230, training loss: 1.453326940536499 = 0.8083871603012085 + 0.1 * 6.449397563934326
Epoch 230, val loss: 1.0998497009277344
Epoch 240, training loss: 1.3932859897613525 = 0.7490673065185547 + 0.1 * 6.44218635559082
Epoch 240, val loss: 1.058146595954895
Epoch 250, training loss: 1.3367633819580078 = 0.6923356652259827 + 0.1 * 6.444276809692383
Epoch 250, val loss: 1.0175693035125732
Epoch 260, training loss: 1.2825762033462524 = 0.6395809054374695 + 0.1 * 6.429953098297119
Epoch 260, val loss: 0.980514645576477
Epoch 270, training loss: 1.232767105102539 = 0.5903878211975098 + 0.1 * 6.423792839050293
Epoch 270, val loss: 0.9475045800209045
Epoch 280, training loss: 1.1860222816467285 = 0.5441550612449646 + 0.1 * 6.418671607971191
Epoch 280, val loss: 0.918732225894928
Epoch 290, training loss: 1.1416380405426025 = 0.5004003643989563 + 0.1 * 6.4123759269714355
Epoch 290, val loss: 0.8936985731124878
Epoch 300, training loss: 1.0997225046157837 = 0.4586128890514374 + 0.1 * 6.411096096038818
Epoch 300, val loss: 0.8717844486236572
Epoch 310, training loss: 1.0600258111953735 = 0.4194125831127167 + 0.1 * 6.406132698059082
Epoch 310, val loss: 0.8532208204269409
Epoch 320, training loss: 1.0229105949401855 = 0.3833804726600647 + 0.1 * 6.395301342010498
Epoch 320, val loss: 0.8380643129348755
Epoch 330, training loss: 0.9905085563659668 = 0.35061225295066833 + 0.1 * 6.39896297454834
Epoch 330, val loss: 0.8264668583869934
Epoch 340, training loss: 0.9600619673728943 = 0.32117629051208496 + 0.1 * 6.388856887817383
Epoch 340, val loss: 0.8179366588592529
Epoch 350, training loss: 0.932626485824585 = 0.29466941952705383 + 0.1 * 6.379570007324219
Epoch 350, val loss: 0.8120293021202087
Epoch 360, training loss: 0.9106812477111816 = 0.2705846428871155 + 0.1 * 6.400965690612793
Epoch 360, val loss: 0.8081651329994202
Epoch 370, training loss: 0.8861082792282104 = 0.2487998902797699 + 0.1 * 6.373083591461182
Epoch 370, val loss: 0.8060916662216187
Epoch 380, training loss: 0.8652105331420898 = 0.22887030243873596 + 0.1 * 6.363401889801025
Epoch 380, val loss: 0.8052830696105957
Epoch 390, training loss: 0.8469940423965454 = 0.2105095237493515 + 0.1 * 6.364844799041748
Epoch 390, val loss: 0.8053776025772095
Epoch 400, training loss: 0.8287009000778198 = 0.19357958436012268 + 0.1 * 6.351212501525879
Epoch 400, val loss: 0.8061860799789429
Epoch 410, training loss: 0.8123937249183655 = 0.17796730995178223 + 0.1 * 6.344264030456543
Epoch 410, val loss: 0.8076490759849548
Epoch 420, training loss: 0.7988277077674866 = 0.16355927288532257 + 0.1 * 6.352684497833252
Epoch 420, val loss: 0.809730589389801
Epoch 430, training loss: 0.7849778532981873 = 0.15029287338256836 + 0.1 * 6.34684944152832
Epoch 430, val loss: 0.8125342130661011
Epoch 440, training loss: 0.7712848782539368 = 0.1380530446767807 + 0.1 * 6.33231782913208
Epoch 440, val loss: 0.815906822681427
Epoch 450, training loss: 0.7594166994094849 = 0.1267113983631134 + 0.1 * 6.327052593231201
Epoch 450, val loss: 0.8197973966598511
Epoch 460, training loss: 0.7492187023162842 = 0.11629203706979752 + 0.1 * 6.32926607131958
Epoch 460, val loss: 0.8241639733314514
Epoch 470, training loss: 0.7398543357849121 = 0.10680776834487915 + 0.1 * 6.330465316772461
Epoch 470, val loss: 0.8287528157234192
Epoch 480, training loss: 0.7294590473175049 = 0.09814907610416412 + 0.1 * 6.3130998611450195
Epoch 480, val loss: 0.8342136144638062
Epoch 490, training loss: 0.7230933904647827 = 0.0902167484164238 + 0.1 * 6.328766345977783
Epoch 490, val loss: 0.84011310338974
Epoch 500, training loss: 0.7140976190567017 = 0.08300334215164185 + 0.1 * 6.310942649841309
Epoch 500, val loss: 0.8457777500152588
Epoch 510, training loss: 0.7072017788887024 = 0.07642795145511627 + 0.1 * 6.307738304138184
Epoch 510, val loss: 0.8516217470169067
Epoch 520, training loss: 0.7005568742752075 = 0.07043804973363876 + 0.1 * 6.3011884689331055
Epoch 520, val loss: 0.857606828212738
Epoch 530, training loss: 0.6935533881187439 = 0.06497873365879059 + 0.1 * 6.2857465744018555
Epoch 530, val loss: 0.8637403845787048
Epoch 540, training loss: 0.689731240272522 = 0.05999268963932991 + 0.1 * 6.2973856925964355
Epoch 540, val loss: 0.8700547814369202
Epoch 550, training loss: 0.6837241649627686 = 0.05545194447040558 + 0.1 * 6.282722473144531
Epoch 550, val loss: 0.876311719417572
Epoch 560, training loss: 0.678816556930542 = 0.05130327120423317 + 0.1 * 6.275132656097412
Epoch 560, val loss: 0.8826702833175659
Epoch 570, training loss: 0.6764281392097473 = 0.047512784600257874 + 0.1 * 6.289153575897217
Epoch 570, val loss: 0.8891089558601379
Epoch 580, training loss: 0.6711527109146118 = 0.04405757412314415 + 0.1 * 6.270951271057129
Epoch 580, val loss: 0.8954964280128479
Epoch 590, training loss: 0.6690347790718079 = 0.04090924561023712 + 0.1 * 6.28125524520874
Epoch 590, val loss: 0.9018864035606384
Epoch 600, training loss: 0.6648849844932556 = 0.038051072508096695 + 0.1 * 6.268339157104492
Epoch 600, val loss: 0.908110499382019
Epoch 610, training loss: 0.6611185073852539 = 0.03545235097408295 + 0.1 * 6.256661415100098
Epoch 610, val loss: 0.9144405722618103
Epoch 620, training loss: 0.6589695811271667 = 0.03309841826558113 + 0.1 * 6.258711814880371
Epoch 620, val loss: 0.9206576943397522
Epoch 630, training loss: 0.6554393172264099 = 0.03097129613161087 + 0.1 * 6.244679927825928
Epoch 630, val loss: 0.9268426299095154
Epoch 640, training loss: 0.6536368727684021 = 0.029020799323916435 + 0.1 * 6.246160984039307
Epoch 640, val loss: 0.9330505132675171
Epoch 650, training loss: 0.6530392169952393 = 0.027251601219177246 + 0.1 * 6.257875919342041
Epoch 650, val loss: 0.9391904473304749
Epoch 660, training loss: 0.6503956317901611 = 0.025642959401011467 + 0.1 * 6.247527122497559
Epoch 660, val loss: 0.9451249837875366
Epoch 670, training loss: 0.6476683616638184 = 0.02417215332388878 + 0.1 * 6.234961986541748
Epoch 670, val loss: 0.951047420501709
Epoch 680, training loss: 0.6460848450660706 = 0.02282177284359932 + 0.1 * 6.232630729675293
Epoch 680, val loss: 0.956917405128479
Epoch 690, training loss: 0.6458674669265747 = 0.021580874919891357 + 0.1 * 6.242866039276123
Epoch 690, val loss: 0.962651252746582
Epoch 700, training loss: 0.6433437466621399 = 0.020441532135009766 + 0.1 * 6.229022026062012
Epoch 700, val loss: 0.9683032035827637
Epoch 710, training loss: 0.642055332660675 = 0.019390452653169632 + 0.1 * 6.226648807525635
Epoch 710, val loss: 0.9738566279411316
Epoch 720, training loss: 0.6410050988197327 = 0.018419044092297554 + 0.1 * 6.225860595703125
Epoch 720, val loss: 0.9793142676353455
Epoch 730, training loss: 0.6391830444335938 = 0.01752082072198391 + 0.1 * 6.2166218757629395
Epoch 730, val loss: 0.9846538305282593
Epoch 740, training loss: 0.6390272974967957 = 0.01668977178633213 + 0.1 * 6.22337532043457
Epoch 740, val loss: 0.98991459608078
Epoch 750, training loss: 0.6371709108352661 = 0.015921946614980698 + 0.1 * 6.212489604949951
Epoch 750, val loss: 0.995117723941803
Epoch 760, training loss: 0.6365787982940674 = 0.015208980068564415 + 0.1 * 6.213698387145996
Epoch 760, val loss: 1.0002038478851318
Epoch 770, training loss: 0.6356372237205505 = 0.014541414566338062 + 0.1 * 6.210958480834961
Epoch 770, val loss: 1.0051988363265991
Epoch 780, training loss: 0.6353241801261902 = 0.013922431506216526 + 0.1 * 6.214016914367676
Epoch 780, val loss: 1.0101450681686401
Epoch 790, training loss: 0.6336755752563477 = 0.013346719555556774 + 0.1 * 6.203288555145264
Epoch 790, val loss: 1.015005111694336
Epoch 800, training loss: 0.6330581903457642 = 0.012809083797037601 + 0.1 * 6.20249080657959
Epoch 800, val loss: 1.019781470298767
Epoch 810, training loss: 0.6326722502708435 = 0.01230557356029749 + 0.1 * 6.203666687011719
Epoch 810, val loss: 1.02449369430542
Epoch 820, training loss: 0.6317782402038574 = 0.011833976954221725 + 0.1 * 6.1994428634643555
Epoch 820, val loss: 1.0291001796722412
Epoch 830, training loss: 0.6315028071403503 = 0.011390558443963528 + 0.1 * 6.201122283935547
Epoch 830, val loss: 1.0336869955062866
Epoch 840, training loss: 0.6307015419006348 = 0.010973970405757427 + 0.1 * 6.197275161743164
Epoch 840, val loss: 1.0381572246551514
Epoch 850, training loss: 0.629511833190918 = 0.010582937858998775 + 0.1 * 6.189289093017578
Epoch 850, val loss: 1.042588710784912
Epoch 860, training loss: 0.6295101642608643 = 0.010214235633611679 + 0.1 * 6.192959308624268
Epoch 860, val loss: 1.0469402074813843
Epoch 870, training loss: 0.6303454041481018 = 0.009865746833384037 + 0.1 * 6.204796314239502
Epoch 870, val loss: 1.0511847734451294
Epoch 880, training loss: 0.6278331279754639 = 0.009536306373775005 + 0.1 * 6.1829681396484375
Epoch 880, val loss: 1.0554510354995728
Epoch 890, training loss: 0.6277119517326355 = 0.009225668385624886 + 0.1 * 6.1848626136779785
Epoch 890, val loss: 1.0595405101776123
Epoch 900, training loss: 0.6286919116973877 = 0.008930888958275318 + 0.1 * 6.197610378265381
Epoch 900, val loss: 1.0635573863983154
Epoch 910, training loss: 0.6267913579940796 = 0.008651889860630035 + 0.1 * 6.181394100189209
Epoch 910, val loss: 1.0675386190414429
Epoch 920, training loss: 0.6260167360305786 = 0.008387637324631214 + 0.1 * 6.176290988922119
Epoch 920, val loss: 1.0714340209960938
Epoch 930, training loss: 0.6268979907035828 = 0.008136339485645294 + 0.1 * 6.187616348266602
Epoch 930, val loss: 1.0752944946289062
Epoch 940, training loss: 0.6248511672019958 = 0.00789716001600027 + 0.1 * 6.1695404052734375
Epoch 940, val loss: 1.0790191888809204
Epoch 950, training loss: 0.6254050135612488 = 0.007670064456760883 + 0.1 * 6.17734956741333
Epoch 950, val loss: 1.0826895236968994
Epoch 960, training loss: 0.6251006126403809 = 0.00745326979085803 + 0.1 * 6.176473140716553
Epoch 960, val loss: 1.0863463878631592
Epoch 970, training loss: 0.6247542500495911 = 0.007247062399983406 + 0.1 * 6.1750712394714355
Epoch 970, val loss: 1.0899254083633423
Epoch 980, training loss: 0.6242154240608215 = 0.007050639018416405 + 0.1 * 6.171647548675537
Epoch 980, val loss: 1.0934429168701172
Epoch 990, training loss: 0.6235721111297607 = 0.006863434799015522 + 0.1 * 6.167087078094482
Epoch 990, val loss: 1.096947193145752
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8635
Flip ASR: 0.8489/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.773477554321289 = 1.9360982179641724 + 0.1 * 8.373793601989746
Epoch 0, val loss: 1.9374704360961914
Epoch 10, training loss: 2.7634952068328857 = 1.9261420965194702 + 0.1 * 8.373530387878418
Epoch 10, val loss: 1.9270851612091064
Epoch 20, training loss: 2.75087308883667 = 1.9136455059051514 + 0.1 * 8.372276306152344
Epoch 20, val loss: 1.9138391017913818
Epoch 30, training loss: 2.7322330474853516 = 1.8957782983779907 + 0.1 * 8.364548683166504
Epoch 30, val loss: 1.8947416543960571
Epoch 40, training loss: 2.6991145610809326 = 1.8691478967666626 + 0.1 * 8.299667358398438
Epoch 40, val loss: 1.8667101860046387
Epoch 50, training loss: 2.596156120300293 = 1.8357409238815308 + 0.1 * 7.604150772094727
Epoch 50, val loss: 1.8337476253509521
Epoch 60, training loss: 2.5138673782348633 = 1.8072447776794434 + 0.1 * 7.066226005554199
Epoch 60, val loss: 1.806519865989685
Epoch 70, training loss: 2.458346366882324 = 1.7788341045379639 + 0.1 * 6.795123100280762
Epoch 70, val loss: 1.7801531553268433
Epoch 80, training loss: 2.4155726432800293 = 1.7477543354034424 + 0.1 * 6.6781840324401855
Epoch 80, val loss: 1.7526273727416992
Epoch 90, training loss: 2.3697235584259033 = 1.7083470821380615 + 0.1 * 6.613763809204102
Epoch 90, val loss: 1.7185717821121216
Epoch 100, training loss: 2.3113622665405273 = 1.654213786125183 + 0.1 * 6.571483612060547
Epoch 100, val loss: 1.6725419759750366
Epoch 110, training loss: 2.2367444038391113 = 1.582240104675293 + 0.1 * 6.545042514801025
Epoch 110, val loss: 1.612640142440796
Epoch 120, training loss: 2.1483073234558105 = 1.4953155517578125 + 0.1 * 6.5299177169799805
Epoch 120, val loss: 1.5425540208816528
Epoch 130, training loss: 2.0552549362182617 = 1.403295874595642 + 0.1 * 6.519590377807617
Epoch 130, val loss: 1.4711772203445435
Epoch 140, training loss: 1.964752435684204 = 1.313896894454956 + 0.1 * 6.508554935455322
Epoch 140, val loss: 1.4065102338790894
Epoch 150, training loss: 1.8804359436035156 = 1.2300642728805542 + 0.1 * 6.503716945648193
Epoch 150, val loss: 1.349422574043274
Epoch 160, training loss: 1.800744652748108 = 1.1526668071746826 + 0.1 * 6.480778217315674
Epoch 160, val loss: 1.2983580827713013
Epoch 170, training loss: 1.726740837097168 = 1.0802117586135864 + 0.1 * 6.465291500091553
Epoch 170, val loss: 1.2510191202163696
Epoch 180, training loss: 1.6585338115692139 = 1.0132924318313599 + 0.1 * 6.452414035797119
Epoch 180, val loss: 1.207234501838684
Epoch 190, training loss: 1.5962239503860474 = 0.9521819949150085 + 0.1 * 6.4404191970825195
Epoch 190, val loss: 1.1671981811523438
Epoch 200, training loss: 1.5397597551345825 = 0.895751416683197 + 0.1 * 6.440083026885986
Epoch 200, val loss: 1.1302117109298706
Epoch 210, training loss: 1.4845753908157349 = 0.842273473739624 + 0.1 * 6.423018932342529
Epoch 210, val loss: 1.0948123931884766
Epoch 220, training loss: 1.4303038120269775 = 0.7886966466903687 + 0.1 * 6.41607141494751
Epoch 220, val loss: 1.0587937831878662
Epoch 230, training loss: 1.3749853372573853 = 0.7337023615837097 + 0.1 * 6.412829399108887
Epoch 230, val loss: 1.0214422941207886
Epoch 240, training loss: 1.3174083232879639 = 0.6772698760032654 + 0.1 * 6.4013848304748535
Epoch 240, val loss: 0.9830790162086487
Epoch 250, training loss: 1.2593194246292114 = 0.6199201941490173 + 0.1 * 6.393991947174072
Epoch 250, val loss: 0.94436115026474
Epoch 260, training loss: 1.2035837173461914 = 0.5630915760993958 + 0.1 * 6.404922008514404
Epoch 260, val loss: 0.9073631167411804
Epoch 270, training loss: 1.146716594696045 = 0.5084140300750732 + 0.1 * 6.383025169372559
Epoch 270, val loss: 0.8741013407707214
Epoch 280, training loss: 1.0940881967544556 = 0.45638370513916016 + 0.1 * 6.377044677734375
Epoch 280, val loss: 0.8450192809104919
Epoch 290, training loss: 1.0448397397994995 = 0.40760868787765503 + 0.1 * 6.372310161590576
Epoch 290, val loss: 0.8204472661018372
Epoch 300, training loss: 1.0004794597625732 = 0.36277085542678833 + 0.1 * 6.377085208892822
Epoch 300, val loss: 0.800188422203064
Epoch 310, training loss: 0.9595562219619751 = 0.32234036922454834 + 0.1 * 6.372158527374268
Epoch 310, val loss: 0.7840688824653625
Epoch 320, training loss: 0.9222410917282104 = 0.2863742411136627 + 0.1 * 6.358667850494385
Epoch 320, val loss: 0.7717383503913879
Epoch 330, training loss: 0.8896406292915344 = 0.2545943260192871 + 0.1 * 6.350462913513184
Epoch 330, val loss: 0.7627745270729065
Epoch 340, training loss: 0.8621150255203247 = 0.22684548795223236 + 0.1 * 6.352694988250732
Epoch 340, val loss: 0.7570918798446655
Epoch 350, training loss: 0.836739182472229 = 0.20273320376873016 + 0.1 * 6.340059280395508
Epoch 350, val loss: 0.7543075680732727
Epoch 360, training loss: 0.8149030208587646 = 0.181680366396904 + 0.1 * 6.332226276397705
Epoch 360, val loss: 0.7538976669311523
Epoch 370, training loss: 0.7969009876251221 = 0.16321147978305817 + 0.1 * 6.336894989013672
Epoch 370, val loss: 0.7555986046791077
Epoch 380, training loss: 0.7812907695770264 = 0.14699935913085938 + 0.1 * 6.34291410446167
Epoch 380, val loss: 0.7589587569236755
Epoch 390, training loss: 0.764102041721344 = 0.1327575445175171 + 0.1 * 6.3134446144104
Epoch 390, val loss: 0.7635878920555115
Epoch 400, training loss: 0.7515428066253662 = 0.1201532632112503 + 0.1 * 6.3138957023620605
Epoch 400, val loss: 0.7692474722862244
Epoch 410, training loss: 0.7394457459449768 = 0.10899130254983902 + 0.1 * 6.304543972015381
Epoch 410, val loss: 0.7757085561752319
Epoch 420, training loss: 0.7291362285614014 = 0.09909168630838394 + 0.1 * 6.300445079803467
Epoch 420, val loss: 0.7828563451766968
Epoch 430, training loss: 0.7209017872810364 = 0.09029973298311234 + 0.1 * 6.306020736694336
Epoch 430, val loss: 0.790309727191925
Epoch 440, training loss: 0.7118252515792847 = 0.08251019567251205 + 0.1 * 6.293150424957275
Epoch 440, val loss: 0.7982152700424194
Epoch 450, training loss: 0.7039363384246826 = 0.07556517422199249 + 0.1 * 6.2837114334106445
Epoch 450, val loss: 0.8062540888786316
Epoch 460, training loss: 0.6977860331535339 = 0.06936126202344894 + 0.1 * 6.284247398376465
Epoch 460, val loss: 0.8144327998161316
Epoch 470, training loss: 0.6919438242912292 = 0.0638236552476883 + 0.1 * 6.2812018394470215
Epoch 470, val loss: 0.8226158618927002
Epoch 480, training loss: 0.6864676475524902 = 0.05887651443481445 + 0.1 * 6.275911331176758
Epoch 480, val loss: 0.8308361172676086
Epoch 490, training loss: 0.6814433932304382 = 0.05444575473666191 + 0.1 * 6.2699761390686035
Epoch 490, val loss: 0.8389255404472351
Epoch 500, training loss: 0.6768797636032104 = 0.05047148838639259 + 0.1 * 6.264082908630371
Epoch 500, val loss: 0.8470810651779175
Epoch 510, training loss: 0.6726997494697571 = 0.04688609763979912 + 0.1 * 6.258136749267578
Epoch 510, val loss: 0.8550240397453308
Epoch 520, training loss: 0.6695554256439209 = 0.04364951327443123 + 0.1 * 6.259058952331543
Epoch 520, val loss: 0.862830638885498
Epoch 530, training loss: 0.6665804982185364 = 0.040734127163887024 + 0.1 * 6.2584638595581055
Epoch 530, val loss: 0.8705794811248779
Epoch 540, training loss: 0.6629126667976379 = 0.03809671476483345 + 0.1 * 6.248159408569336
Epoch 540, val loss: 0.8782495260238647
Epoch 550, training loss: 0.6601468324661255 = 0.03569665923714638 + 0.1 * 6.24450159072876
Epoch 550, val loss: 0.8857078552246094
Epoch 560, training loss: 0.6576390266418457 = 0.0335078127682209 + 0.1 * 6.241312026977539
Epoch 560, val loss: 0.8929601907730103
Epoch 570, training loss: 0.6554042100906372 = 0.031511832028627396 + 0.1 * 6.2389235496521
Epoch 570, val loss: 0.9002203345298767
Epoch 580, training loss: 0.6546376347541809 = 0.029685799032449722 + 0.1 * 6.249518394470215
Epoch 580, val loss: 0.9071775078773499
Epoch 590, training loss: 0.6509595513343811 = 0.02801891416311264 + 0.1 * 6.229406356811523
Epoch 590, val loss: 0.9141022562980652
Epoch 600, training loss: 0.6495163440704346 = 0.026488643139600754 + 0.1 * 6.230276584625244
Epoch 600, val loss: 0.9209316968917847
Epoch 610, training loss: 0.647142767906189 = 0.025079011917114258 + 0.1 * 6.220637321472168
Epoch 610, val loss: 0.9273722767829895
Epoch 620, training loss: 0.646304726600647 = 0.02378217689692974 + 0.1 * 6.22522497177124
Epoch 620, val loss: 0.9338940978050232
Epoch 630, training loss: 0.643936038017273 = 0.022584619000554085 + 0.1 * 6.21351432800293
Epoch 630, val loss: 0.9402217864990234
Epoch 640, training loss: 0.6428415179252625 = 0.021476052701473236 + 0.1 * 6.213654518127441
Epoch 640, val loss: 0.9464678764343262
Epoch 650, training loss: 0.6415582895278931 = 0.020446952432394028 + 0.1 * 6.211112976074219
Epoch 650, val loss: 0.9523813724517822
Epoch 660, training loss: 0.6402751207351685 = 0.01949499547481537 + 0.1 * 6.20780086517334
Epoch 660, val loss: 0.9583783149719238
Epoch 670, training loss: 0.6393265128135681 = 0.01860835589468479 + 0.1 * 6.207181930541992
Epoch 670, val loss: 0.9642409682273865
Epoch 680, training loss: 0.6385489106178284 = 0.01778111234307289 + 0.1 * 6.207677364349365
Epoch 680, val loss: 0.9697642922401428
Epoch 690, training loss: 0.6373782753944397 = 0.017010856419801712 + 0.1 * 6.203673839569092
Epoch 690, val loss: 0.9753376245498657
Epoch 700, training loss: 0.636390745639801 = 0.016289951279759407 + 0.1 * 6.201007843017578
Epoch 700, val loss: 0.9807725548744202
Epoch 710, training loss: 0.6378695964813232 = 0.015615461394190788 + 0.1 * 6.222541332244873
Epoch 710, val loss: 0.9859577417373657
Epoch 720, training loss: 0.6341835856437683 = 0.014986453577876091 + 0.1 * 6.191971302032471
Epoch 720, val loss: 0.9910739660263062
Epoch 730, training loss: 0.6339986324310303 = 0.014399015344679356 + 0.1 * 6.195996284484863
Epoch 730, val loss: 0.9963253736495972
Epoch 740, training loss: 0.6325474381446838 = 0.013845008797943592 + 0.1 * 6.187024116516113
Epoch 740, val loss: 1.0012400150299072
Epoch 750, training loss: 0.6336747407913208 = 0.013322867453098297 + 0.1 * 6.203518390655518
Epoch 750, val loss: 1.00605046749115
Epoch 760, training loss: 0.6312721967697144 = 0.012832029722630978 + 0.1 * 6.184401512145996
Epoch 760, val loss: 1.0107717514038086
Epoch 770, training loss: 0.6303592920303345 = 0.012370743788778782 + 0.1 * 6.179885387420654
Epoch 770, val loss: 1.015588641166687
Epoch 780, training loss: 0.632523775100708 = 0.011933526955544949 + 0.1 * 6.205902576446533
Epoch 780, val loss: 1.0201092958450317
Epoch 790, training loss: 0.6304071545600891 = 0.011520220898091793 + 0.1 * 6.188869476318359
Epoch 790, val loss: 1.0243891477584839
Epoch 800, training loss: 0.6287525296211243 = 0.011132041923701763 + 0.1 * 6.176204681396484
Epoch 800, val loss: 1.0289214849472046
Epoch 810, training loss: 0.6285918951034546 = 0.010763818398118019 + 0.1 * 6.178280353546143
Epoch 810, val loss: 1.0332872867584229
Epoch 820, training loss: 0.6275891661643982 = 0.010413430631160736 + 0.1 * 6.171757698059082
Epoch 820, val loss: 1.0373389720916748
Epoch 830, training loss: 0.6276343464851379 = 0.010082606226205826 + 0.1 * 6.1755170822143555
Epoch 830, val loss: 1.0415035486221313
Epoch 840, training loss: 0.6269802451133728 = 0.009768257848918438 + 0.1 * 6.172120094299316
Epoch 840, val loss: 1.0456414222717285
Epoch 850, training loss: 0.6272173523902893 = 0.009469320997595787 + 0.1 * 6.177480220794678
Epoch 850, val loss: 1.049670934677124
Epoch 860, training loss: 0.6262925267219543 = 0.009185155853629112 + 0.1 * 6.171073913574219
Epoch 860, val loss: 1.053614616394043
Epoch 870, training loss: 0.6255176663398743 = 0.008914251811802387 + 0.1 * 6.16603422164917
Epoch 870, val loss: 1.057521939277649
Epoch 880, training loss: 0.6253904700279236 = 0.00865646731108427 + 0.1 * 6.167340278625488
Epoch 880, val loss: 1.061285138130188
Epoch 890, training loss: 0.6243129372596741 = 0.008410491980612278 + 0.1 * 6.159024238586426
Epoch 890, val loss: 1.0649046897888184
Epoch 900, training loss: 0.6242700219154358 = 0.008177121169865131 + 0.1 * 6.160929203033447
Epoch 900, val loss: 1.0686359405517578
Epoch 910, training loss: 0.6249715685844421 = 0.00795379001647234 + 0.1 * 6.170177936553955
Epoch 910, val loss: 1.072172999382019
Epoch 920, training loss: 0.6239872574806213 = 0.007741037290543318 + 0.1 * 6.16246223449707
Epoch 920, val loss: 1.0757272243499756
Epoch 930, training loss: 0.6228911280632019 = 0.0075376564636826515 + 0.1 * 6.153534412384033
Epoch 930, val loss: 1.0792760848999023
Epoch 940, training loss: 0.6228965520858765 = 0.007342122960835695 + 0.1 * 6.155544281005859
Epoch 940, val loss: 1.0826908349990845
Epoch 950, training loss: 0.6230466365814209 = 0.007154401857405901 + 0.1 * 6.15892219543457
Epoch 950, val loss: 1.0858596563339233
Epoch 960, training loss: 0.6223140358924866 = 0.006975344382226467 + 0.1 * 6.15338659286499
Epoch 960, val loss: 1.089239239692688
Epoch 970, training loss: 0.6223875880241394 = 0.006804055999964476 + 0.1 * 6.155835151672363
Epoch 970, val loss: 1.0926240682601929
Epoch 980, training loss: 0.6212649345397949 = 0.0066391476429998875 + 0.1 * 6.1462578773498535
Epoch 980, val loss: 1.0957661867141724
Epoch 990, training loss: 0.6218095421791077 = 0.006480841431766748 + 0.1 * 6.153286933898926
Epoch 990, val loss: 1.0988646745681763
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8598
Flip ASR: 0.8311/225 nodes
The final ASR:0.76015, 0.14352, Accuracy:0.81481, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9420])
updated graph: torch.Size([2, 10468])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00696, Accuracy:0.83580, 0.00349
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.774449586868286 = 1.9370607137680054 + 0.1 * 8.373888969421387
Epoch 0, val loss: 1.9396839141845703
Epoch 10, training loss: 2.7647311687469482 = 1.9273525476455688 + 0.1 * 8.373786926269531
Epoch 10, val loss: 1.930426001548767
Epoch 20, training loss: 2.7527217864990234 = 1.9153934717178345 + 0.1 * 8.373283386230469
Epoch 20, val loss: 1.9186265468597412
Epoch 30, training loss: 2.735434055328369 = 1.8984594345092773 + 0.1 * 8.369747161865234
Epoch 30, val loss: 1.901636004447937
Epoch 40, training loss: 2.7075626850128174 = 1.8734079599380493 + 0.1 * 8.341547012329102
Epoch 40, val loss: 1.8765169382095337
Epoch 50, training loss: 2.64856219291687 = 1.8390775918960571 + 0.1 * 8.09484577178955
Epoch 50, val loss: 1.843590259552002
Epoch 60, training loss: 2.561615467071533 = 1.803072214126587 + 0.1 * 7.5854315757751465
Epoch 60, val loss: 1.8116995096206665
Epoch 70, training loss: 2.478851556777954 = 1.7683149576187134 + 0.1 * 7.105365753173828
Epoch 70, val loss: 1.7814453840255737
Epoch 80, training loss: 2.4163405895233154 = 1.7276555299758911 + 0.1 * 6.8868513107299805
Epoch 80, val loss: 1.745897650718689
Epoch 90, training loss: 2.354153633117676 = 1.6737040281295776 + 0.1 * 6.804497241973877
Epoch 90, val loss: 1.6989516019821167
Epoch 100, training loss: 2.277820587158203 = 1.6019200086593628 + 0.1 * 6.759006977081299
Epoch 100, val loss: 1.637465000152588
Epoch 110, training loss: 2.1857187747955322 = 1.5132087469100952 + 0.1 * 6.725100517272949
Epoch 110, val loss: 1.5636557340621948
Epoch 120, training loss: 2.0837321281433105 = 1.4141322374343872 + 0.1 * 6.6959991455078125
Epoch 120, val loss: 1.4827240705490112
Epoch 130, training loss: 1.9796028137207031 = 1.3122193813323975 + 0.1 * 6.67383337020874
Epoch 130, val loss: 1.4016369581222534
Epoch 140, training loss: 1.87810218334198 = 1.2123692035675049 + 0.1 * 6.657329559326172
Epoch 140, val loss: 1.3256127834320068
Epoch 150, training loss: 1.7844276428222656 = 1.1199654340744019 + 0.1 * 6.644621849060059
Epoch 150, val loss: 1.257914662361145
Epoch 160, training loss: 1.7021868228912354 = 1.038695216178894 + 0.1 * 6.634915828704834
Epoch 160, val loss: 1.1997895240783691
Epoch 170, training loss: 1.6311477422714233 = 0.9690603017807007 + 0.1 * 6.620874404907227
Epoch 170, val loss: 1.1514687538146973
Epoch 180, training loss: 1.5693129301071167 = 0.9077889323234558 + 0.1 * 6.61523962020874
Epoch 180, val loss: 1.1100194454193115
Epoch 190, training loss: 1.511796236038208 = 0.8519556522369385 + 0.1 * 6.598405838012695
Epoch 190, val loss: 1.0728280544281006
Epoch 200, training loss: 1.4565755128860474 = 0.7974154353141785 + 0.1 * 6.59160041809082
Epoch 200, val loss: 1.036080002784729
Epoch 210, training loss: 1.4001249074935913 = 0.7419348359107971 + 0.1 * 6.581900596618652
Epoch 210, val loss: 0.9979422092437744
Epoch 220, training loss: 1.3426692485809326 = 0.685126543045044 + 0.1 * 6.5754265785217285
Epoch 220, val loss: 0.9577431082725525
Epoch 230, training loss: 1.2855207920074463 = 0.6285447478294373 + 0.1 * 6.569760322570801
Epoch 230, val loss: 0.9170125126838684
Epoch 240, training loss: 1.2294645309448242 = 0.5735510587692261 + 0.1 * 6.559135437011719
Epoch 240, val loss: 0.87715744972229
Epoch 250, training loss: 1.1763701438903809 = 0.5214808583259583 + 0.1 * 6.548892021179199
Epoch 250, val loss: 0.8400276303291321
Epoch 260, training loss: 1.128061294555664 = 0.4735362231731415 + 0.1 * 6.54525089263916
Epoch 260, val loss: 0.8071470856666565
Epoch 270, training loss: 1.0836275815963745 = 0.43043872714042664 + 0.1 * 6.531888961791992
Epoch 270, val loss: 0.7793509364128113
Epoch 280, training loss: 1.0433127880096436 = 0.3911019265651703 + 0.1 * 6.522108554840088
Epoch 280, val loss: 0.7555743455886841
Epoch 290, training loss: 1.007507085800171 = 0.3545071482658386 + 0.1 * 6.529999256134033
Epoch 290, val loss: 0.7351472973823547
Epoch 300, training loss: 0.9705883860588074 = 0.3201426863670349 + 0.1 * 6.504456996917725
Epoch 300, val loss: 0.7175998091697693
Epoch 310, training loss: 0.9363285303115845 = 0.2871694266796112 + 0.1 * 6.491591453552246
Epoch 310, val loss: 0.7020309567451477
Epoch 320, training loss: 0.9043718576431274 = 0.25574856996536255 + 0.1 * 6.486232757568359
Epoch 320, val loss: 0.6884435415267944
Epoch 330, training loss: 0.8739480972290039 = 0.22632506489753723 + 0.1 * 6.476229667663574
Epoch 330, val loss: 0.677078902721405
Epoch 340, training loss: 0.8452842831611633 = 0.19920922815799713 + 0.1 * 6.460750579833984
Epoch 340, val loss: 0.6678934097290039
Epoch 350, training loss: 0.8208655118942261 = 0.17490527033805847 + 0.1 * 6.459602355957031
Epoch 350, val loss: 0.6613652110099792
Epoch 360, training loss: 0.7981293797492981 = 0.15375030040740967 + 0.1 * 6.443790912628174
Epoch 360, val loss: 0.6577163338661194
Epoch 370, training loss: 0.7820214033126831 = 0.13558141887187958 + 0.1 * 6.464399337768555
Epoch 370, val loss: 0.6564043164253235
Epoch 380, training loss: 0.7626155614852905 = 0.12023994326591492 + 0.1 * 6.4237565994262695
Epoch 380, val loss: 0.657328724861145
Epoch 390, training loss: 0.7488248944282532 = 0.10720843821763992 + 0.1 * 6.416164398193359
Epoch 390, val loss: 0.6599558591842651
Epoch 400, training loss: 0.7368835210800171 = 0.0960862934589386 + 0.1 * 6.4079718589782715
Epoch 400, val loss: 0.6640520691871643
Epoch 410, training loss: 0.726479172706604 = 0.0865611806511879 + 0.1 * 6.399179458618164
Epoch 410, val loss: 0.669170618057251
Epoch 420, training loss: 0.7184286713600159 = 0.0783156231045723 + 0.1 * 6.401130199432373
Epoch 420, val loss: 0.6750695109367371
Epoch 430, training loss: 0.7097128629684448 = 0.07114165276288986 + 0.1 * 6.385711669921875
Epoch 430, val loss: 0.6816475987434387
Epoch 440, training loss: 0.702285885810852 = 0.06483110040426254 + 0.1 * 6.374547958374023
Epoch 440, val loss: 0.6886499524116516
Epoch 450, training loss: 0.6970129609107971 = 0.059259288012981415 + 0.1 * 6.377536773681641
Epoch 450, val loss: 0.6959382891654968
Epoch 460, training loss: 0.6912193298339844 = 0.05433660000562668 + 0.1 * 6.368826866149902
Epoch 460, val loss: 0.7035305500030518
Epoch 470, training loss: 0.6851176023483276 = 0.04995395615696907 + 0.1 * 6.3516364097595215
Epoch 470, val loss: 0.7112294435501099
Epoch 480, training loss: 0.6829472780227661 = 0.04602824151515961 + 0.1 * 6.369190216064453
Epoch 480, val loss: 0.7190834879875183
Epoch 490, training loss: 0.677548885345459 = 0.04252419248223305 + 0.1 * 6.350246429443359
Epoch 490, val loss: 0.7268773317337036
Epoch 500, training loss: 0.6729087233543396 = 0.03937589004635811 + 0.1 * 6.335328102111816
Epoch 500, val loss: 0.7347131371498108
Epoch 510, training loss: 0.6699808239936829 = 0.03653528541326523 + 0.1 * 6.334455490112305
Epoch 510, val loss: 0.7425257563591003
Epoch 520, training loss: 0.66794353723526 = 0.033979061990976334 + 0.1 * 6.339644432067871
Epoch 520, val loss: 0.7501246333122253
Epoch 530, training loss: 0.6639019846916199 = 0.03168129548430443 + 0.1 * 6.322206974029541
Epoch 530, val loss: 0.7577639818191528
Epoch 540, training loss: 0.6625390648841858 = 0.029596859589219093 + 0.1 * 6.3294219970703125
Epoch 540, val loss: 0.7652251720428467
Epoch 550, training loss: 0.6601986885070801 = 0.027709774672985077 + 0.1 * 6.324889183044434
Epoch 550, val loss: 0.7724491357803345
Epoch 560, training loss: 0.6568068265914917 = 0.025996405631303787 + 0.1 * 6.308104038238525
Epoch 560, val loss: 0.7797046303749084
Epoch 570, training loss: 0.6547259092330933 = 0.02443491853773594 + 0.1 * 6.302909851074219
Epoch 570, val loss: 0.7867184281349182
Epoch 580, training loss: 0.653510570526123 = 0.023011701181530952 + 0.1 * 6.304988861083984
Epoch 580, val loss: 0.7936850190162659
Epoch 590, training loss: 0.6514893174171448 = 0.021710608154535294 + 0.1 * 6.297786712646484
Epoch 590, val loss: 0.8005374670028687
Epoch 600, training loss: 0.6504490375518799 = 0.020520426332950592 + 0.1 * 6.299285888671875
Epoch 600, val loss: 0.8071489930152893
Epoch 610, training loss: 0.6482394337654114 = 0.019431518390774727 + 0.1 * 6.288078784942627
Epoch 610, val loss: 0.8136975765228271
Epoch 620, training loss: 0.6476952433586121 = 0.018429402261972427 + 0.1 * 6.29265832901001
Epoch 620, val loss: 0.8201484680175781
Epoch 630, training loss: 0.6454635858535767 = 0.017506837844848633 + 0.1 * 6.279567241668701
Epoch 630, val loss: 0.8263628482818604
Epoch 640, training loss: 0.6463669538497925 = 0.016655080020427704 + 0.1 * 6.297118663787842
Epoch 640, val loss: 0.8325424790382385
Epoch 650, training loss: 0.6432293057441711 = 0.01586897484958172 + 0.1 * 6.273603439331055
Epoch 650, val loss: 0.838505208492279
Epoch 660, training loss: 0.6422286033630371 = 0.01514273788779974 + 0.1 * 6.2708587646484375
Epoch 660, val loss: 0.8444597125053406
Epoch 670, training loss: 0.6425343155860901 = 0.014467157423496246 + 0.1 * 6.280671119689941
Epoch 670, val loss: 0.8502008318901062
Epoch 680, training loss: 0.6399827003479004 = 0.013841236010193825 + 0.1 * 6.261414527893066
Epoch 680, val loss: 0.855762779712677
Epoch 690, training loss: 0.6396102905273438 = 0.013258693739771843 + 0.1 * 6.263515949249268
Epoch 690, val loss: 0.8613751530647278
Epoch 700, training loss: 0.6381356120109558 = 0.012714413926005363 + 0.1 * 6.254211902618408
Epoch 700, val loss: 0.8667312264442444
Epoch 710, training loss: 0.638016402721405 = 0.012205427512526512 + 0.1 * 6.2581095695495605
Epoch 710, val loss: 0.8720462918281555
Epoch 720, training loss: 0.6373743414878845 = 0.011728599667549133 + 0.1 * 6.256457328796387
Epoch 720, val loss: 0.8771525025367737
Epoch 730, training loss: 0.6372884511947632 = 0.01128245610743761 + 0.1 * 6.260059833526611
Epoch 730, val loss: 0.8822836875915527
Epoch 740, training loss: 0.6351086497306824 = 0.010864336974918842 + 0.1 * 6.242443084716797
Epoch 740, val loss: 0.8872120380401611
Epoch 750, training loss: 0.6353672742843628 = 0.010471263900399208 + 0.1 * 6.248959541320801
Epoch 750, val loss: 0.8920828104019165
Epoch 760, training loss: 0.6336948871612549 = 0.010100428014993668 + 0.1 * 6.2359442710876465
Epoch 760, val loss: 0.8968433737754822
Epoch 770, training loss: 0.633946418762207 = 0.009751263074576855 + 0.1 * 6.2419514656066895
Epoch 770, val loss: 0.9015049934387207
Epoch 780, training loss: 0.6322846412658691 = 0.00942234043031931 + 0.1 * 6.2286224365234375
Epoch 780, val loss: 0.9060966372489929
Epoch 790, training loss: 0.6328362822532654 = 0.009111052379012108 + 0.1 * 6.237252235412598
Epoch 790, val loss: 0.9106939435005188
Epoch 800, training loss: 0.6317281126976013 = 0.008816393092274666 + 0.1 * 6.229116916656494
Epoch 800, val loss: 0.9150473475456238
Epoch 810, training loss: 0.6310006380081177 = 0.008538596332073212 + 0.1 * 6.224620342254639
Epoch 810, val loss: 0.9194554090499878
Epoch 820, training loss: 0.6317142248153687 = 0.008274815045297146 + 0.1 * 6.234394073486328
Epoch 820, val loss: 0.9237069487571716
Epoch 830, training loss: 0.6298064589500427 = 0.008025134913623333 + 0.1 * 6.217813491821289
Epoch 830, val loss: 0.9279261231422424
Epoch 840, training loss: 0.6307401061058044 = 0.0077874441631138325 + 0.1 * 6.229526519775391
Epoch 840, val loss: 0.9321788549423218
Epoch 850, training loss: 0.629965603351593 = 0.007560949306935072 + 0.1 * 6.224046230316162
Epoch 850, val loss: 0.9361430406570435
Epoch 860, training loss: 0.6292902827262878 = 0.0073460545390844345 + 0.1 * 6.219442367553711
Epoch 860, val loss: 0.9400879740715027
Epoch 870, training loss: 0.6284744143486023 = 0.007141819689422846 + 0.1 * 6.2133259773254395
Epoch 870, val loss: 0.9440822005271912
Epoch 880, training loss: 0.6281257271766663 = 0.0069466386921703815 + 0.1 * 6.211791038513184
Epoch 880, val loss: 0.9480138421058655
Epoch 890, training loss: 0.6280855536460876 = 0.0067599923349916935 + 0.1 * 6.213255405426025
Epoch 890, val loss: 0.9518025517463684
Epoch 900, training loss: 0.6270681619644165 = 0.006581740919500589 + 0.1 * 6.204864025115967
Epoch 900, val loss: 0.955497682094574
Epoch 910, training loss: 0.6268705725669861 = 0.006411785259842873 + 0.1 * 6.204587936401367
Epoch 910, val loss: 0.9592235684394836
Epoch 920, training loss: 0.6287684440612793 = 0.006249121855944395 + 0.1 * 6.225193023681641
Epoch 920, val loss: 0.9627877473831177
Epoch 930, training loss: 0.6264389157295227 = 0.006093963049352169 + 0.1 * 6.203449249267578
Epoch 930, val loss: 0.9663664698600769
Epoch 940, training loss: 0.6265170574188232 = 0.005945264827460051 + 0.1 * 6.205718040466309
Epoch 940, val loss: 0.9699527025222778
Epoch 950, training loss: 0.6259140968322754 = 0.00580236129462719 + 0.1 * 6.201117515563965
Epoch 950, val loss: 0.9733800888061523
Epoch 960, training loss: 0.6249661445617676 = 0.005665423348546028 + 0.1 * 6.193006992340088
Epoch 960, val loss: 0.9767165780067444
Epoch 970, training loss: 0.6255279183387756 = 0.005534037481993437 + 0.1 * 6.199938774108887
Epoch 970, val loss: 0.9801243543624878
Epoch 980, training loss: 0.6261599063873291 = 0.005407426040619612 + 0.1 * 6.207524299621582
Epoch 980, val loss: 0.9833706617355347
Epoch 990, training loss: 0.6242726445198059 = 0.00528654595836997 + 0.1 * 6.189860820770264
Epoch 990, val loss: 0.9865986108779907
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5166
Flip ASR: 0.4222/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7824268341064453 = 1.9450368881225586 + 0.1 * 8.37389850616455
Epoch 0, val loss: 1.947489619255066
Epoch 10, training loss: 2.7720913887023926 = 1.9347102642059326 + 0.1 * 8.373811721801758
Epoch 10, val loss: 1.9374945163726807
Epoch 20, training loss: 2.759488582611084 = 1.9221584796905518 + 0.1 * 8.37330150604248
Epoch 20, val loss: 1.9248799085617065
Epoch 30, training loss: 2.741588830947876 = 1.9046143293380737 + 0.1 * 8.369745254516602
Epoch 30, val loss: 1.9068992137908936
Epoch 40, training loss: 2.7138168811798096 = 1.878978967666626 + 0.1 * 8.34837818145752
Epoch 40, val loss: 1.8807600736618042
Epoch 50, training loss: 2.6678757667541504 = 1.8430033922195435 + 0.1 * 8.248723983764648
Epoch 50, val loss: 1.845327615737915
Epoch 60, training loss: 2.594327688217163 = 1.7999260425567627 + 0.1 * 7.9440155029296875
Epoch 60, val loss: 1.8044160604476929
Epoch 70, training loss: 2.5110831260681152 = 1.7531771659851074 + 0.1 * 7.579058647155762
Epoch 70, val loss: 1.7591968774795532
Epoch 80, training loss: 2.4299087524414062 = 1.7004468441009521 + 0.1 * 7.294619083404541
Epoch 80, val loss: 1.7093560695648193
Epoch 90, training loss: 2.3483939170837402 = 1.6355894804000854 + 0.1 * 7.128045082092285
Epoch 90, val loss: 1.651809573173523
Epoch 100, training loss: 2.25624418258667 = 1.5537471771240234 + 0.1 * 7.02496862411499
Epoch 100, val loss: 1.58291494846344
Epoch 110, training loss: 2.1561107635498047 = 1.4600003957748413 + 0.1 * 6.961103439331055
Epoch 110, val loss: 1.509285569190979
Epoch 120, training loss: 2.057015895843506 = 1.3660321235656738 + 0.1 * 6.9098381996154785
Epoch 120, val loss: 1.436630368232727
Epoch 130, training loss: 1.9660053253173828 = 1.2796927690505981 + 0.1 * 6.863125324249268
Epoch 130, val loss: 1.3731392621994019
Epoch 140, training loss: 1.8868067264556885 = 1.203007459640503 + 0.1 * 6.837993144989014
Epoch 140, val loss: 1.3203253746032715
Epoch 150, training loss: 1.8138494491577148 = 1.1359291076660156 + 0.1 * 6.779202461242676
Epoch 150, val loss: 1.2766295671463013
Epoch 160, training loss: 1.7458072900772095 = 1.0716636180877686 + 0.1 * 6.74143648147583
Epoch 160, val loss: 1.2357234954833984
Epoch 170, training loss: 1.67689049243927 = 1.0066263675689697 + 0.1 * 6.702641010284424
Epoch 170, val loss: 1.1941635608673096
Epoch 180, training loss: 1.6085312366485596 = 0.9400449395179749 + 0.1 * 6.684863567352295
Epoch 180, val loss: 1.1503069400787354
Epoch 190, training loss: 1.5396051406860352 = 0.8737238049507141 + 0.1 * 6.658813953399658
Epoch 190, val loss: 1.1064704656600952
Epoch 200, training loss: 1.4724725484848022 = 0.8082326650619507 + 0.1 * 6.642398834228516
Epoch 200, val loss: 1.0634506940841675
Epoch 210, training loss: 1.4072540998458862 = 0.7442008256912231 + 0.1 * 6.630532741546631
Epoch 210, val loss: 1.0213695764541626
Epoch 220, training loss: 1.3462165594100952 = 0.6833471059799194 + 0.1 * 6.628694534301758
Epoch 220, val loss: 0.982378363609314
Epoch 230, training loss: 1.2874884605407715 = 0.6262837648391724 + 0.1 * 6.61204719543457
Epoch 230, val loss: 0.946344792842865
Epoch 240, training loss: 1.2326948642730713 = 0.572760820388794 + 0.1 * 6.599340915679932
Epoch 240, val loss: 0.9137835502624512
Epoch 250, training loss: 1.184107780456543 = 0.5227692127227783 + 0.1 * 6.6133856773376465
Epoch 250, val loss: 0.8848714828491211
Epoch 260, training loss: 1.1353721618652344 = 0.4768471121788025 + 0.1 * 6.585250377655029
Epoch 260, val loss: 0.8599550127983093
Epoch 270, training loss: 1.0906622409820557 = 0.4339069724082947 + 0.1 * 6.567552089691162
Epoch 270, val loss: 0.8385354280471802
Epoch 280, training loss: 1.05072021484375 = 0.3935401439666748 + 0.1 * 6.571800708770752
Epoch 280, val loss: 0.8204558491706848
Epoch 290, training loss: 1.0110418796539307 = 0.3561978042125702 + 0.1 * 6.548440456390381
Epoch 290, val loss: 0.8060256838798523
Epoch 300, training loss: 0.9752050638198853 = 0.32196417450904846 + 0.1 * 6.532408714294434
Epoch 300, val loss: 0.7946703433990479
Epoch 310, training loss: 0.9447124600410461 = 0.29093843698501587 + 0.1 * 6.537740230560303
Epoch 310, val loss: 0.7864557504653931
Epoch 320, training loss: 0.9150943160057068 = 0.2631208300590515 + 0.1 * 6.519734859466553
Epoch 320, val loss: 0.781120240688324
Epoch 330, training loss: 0.8880271315574646 = 0.23782086372375488 + 0.1 * 6.5020623207092285
Epoch 330, val loss: 0.7778170108795166
Epoch 340, training loss: 0.864406406879425 = 0.214690700173378 + 0.1 * 6.497157096862793
Epoch 340, val loss: 0.776262640953064
Epoch 350, training loss: 0.8428406715393066 = 0.19368329644203186 + 0.1 * 6.491573810577393
Epoch 350, val loss: 0.775942862033844
Epoch 360, training loss: 0.8224354982376099 = 0.17470958828926086 + 0.1 * 6.477258682250977
Epoch 360, val loss: 0.7764921188354492
Epoch 370, training loss: 0.8051985502243042 = 0.15774312615394592 + 0.1 * 6.474554061889648
Epoch 370, val loss: 0.7781984210014343
Epoch 380, training loss: 0.789941132068634 = 0.14282147586345673 + 0.1 * 6.471196174621582
Epoch 380, val loss: 0.7810652256011963
Epoch 390, training loss: 0.7743452191352844 = 0.1296301931142807 + 0.1 * 6.447150230407715
Epoch 390, val loss: 0.7850235104560852
Epoch 400, training loss: 0.762475848197937 = 0.1179029569029808 + 0.1 * 6.445728778839111
Epoch 400, val loss: 0.7902560830116272
Epoch 410, training loss: 0.7518895268440247 = 0.10748397558927536 + 0.1 * 6.444055557250977
Epoch 410, val loss: 0.7963050007820129
Epoch 420, training loss: 0.7425041794776917 = 0.09820379316806793 + 0.1 * 6.443004131317139
Epoch 420, val loss: 0.8030798435211182
Epoch 430, training loss: 0.7322917580604553 = 0.08985017240047455 + 0.1 * 6.424415588378906
Epoch 430, val loss: 0.8105074167251587
Epoch 440, training loss: 0.7234866619110107 = 0.08223749697208405 + 0.1 * 6.412491798400879
Epoch 440, val loss: 0.8185495138168335
Epoch 450, training loss: 0.7189447283744812 = 0.07521295547485352 + 0.1 * 6.437317371368408
Epoch 450, val loss: 0.8272448778152466
Epoch 460, training loss: 0.7092623114585876 = 0.0687875971198082 + 0.1 * 6.404747009277344
Epoch 460, val loss: 0.8361031413078308
Epoch 470, training loss: 0.7021174430847168 = 0.0629359558224678 + 0.1 * 6.391814708709717
Epoch 470, val loss: 0.8454065918922424
Epoch 480, training loss: 0.6962016224861145 = 0.05765737593173981 + 0.1 * 6.385442733764648
Epoch 480, val loss: 0.8550690412521362
Epoch 490, training loss: 0.692918598651886 = 0.052991725504398346 + 0.1 * 6.39926815032959
Epoch 490, val loss: 0.8647086024284363
Epoch 500, training loss: 0.6878991723060608 = 0.048922691494226456 + 0.1 * 6.389764308929443
Epoch 500, val loss: 0.8750548362731934
Epoch 510, training loss: 0.6826887726783752 = 0.045327309519052505 + 0.1 * 6.373614311218262
Epoch 510, val loss: 0.8853475451469421
Epoch 520, training loss: 0.6799736618995667 = 0.042121436446905136 + 0.1 * 6.3785223960876465
Epoch 520, val loss: 0.8959720134735107
Epoch 530, training loss: 0.6753888726234436 = 0.03925463929772377 + 0.1 * 6.361341953277588
Epoch 530, val loss: 0.9063861966133118
Epoch 540, training loss: 0.6731390953063965 = 0.036674562841653824 + 0.1 * 6.364645481109619
Epoch 540, val loss: 0.9167768955230713
Epoch 550, training loss: 0.6699864864349365 = 0.034351978451013565 + 0.1 * 6.356344699859619
Epoch 550, val loss: 0.9269817471504211
Epoch 560, training loss: 0.6667153835296631 = 0.03224682807922363 + 0.1 * 6.3446855545043945
Epoch 560, val loss: 0.9370453357696533
Epoch 570, training loss: 0.6663647294044495 = 0.030330689623951912 + 0.1 * 6.360340118408203
Epoch 570, val loss: 0.9468544721603394
Epoch 580, training loss: 0.6631182432174683 = 0.028585873544216156 + 0.1 * 6.34532356262207
Epoch 580, val loss: 0.9565358757972717
Epoch 590, training loss: 0.6602601408958435 = 0.026987561956048012 + 0.1 * 6.332725524902344
Epoch 590, val loss: 0.9659837484359741
Epoch 600, training loss: 0.6596404314041138 = 0.025519998744130135 + 0.1 * 6.3412041664123535
Epoch 600, val loss: 0.9751652479171753
Epoch 610, training loss: 0.6579914689064026 = 0.024176830425858498 + 0.1 * 6.338146209716797
Epoch 610, val loss: 0.9841333627700806
Epoch 620, training loss: 0.6553170680999756 = 0.02293880097568035 + 0.1 * 6.323782920837402
Epoch 620, val loss: 0.9930544495582581
Epoch 630, training loss: 0.6554998755455017 = 0.02179408073425293 + 0.1 * 6.337057590484619
Epoch 630, val loss: 1.001542329788208
Epoch 640, training loss: 0.6521486043930054 = 0.020737668499350548 + 0.1 * 6.3141093254089355
Epoch 640, val loss: 1.0100449323654175
Epoch 650, training loss: 0.6516925692558289 = 0.019758766517043114 + 0.1 * 6.319337844848633
Epoch 650, val loss: 1.018253207206726
Epoch 660, training loss: 0.6494960188865662 = 0.018850507214665413 + 0.1 * 6.306455135345459
Epoch 660, val loss: 1.026349663734436
Epoch 670, training loss: 0.6490532159805298 = 0.018004141747951508 + 0.1 * 6.31049108505249
Epoch 670, val loss: 1.0340855121612549
Epoch 680, training loss: 0.6473613977432251 = 0.017218973487615585 + 0.1 * 6.301424503326416
Epoch 680, val loss: 1.0417982339859009
Epoch 690, training loss: 0.6472038626670837 = 0.016486307606101036 + 0.1 * 6.307175636291504
Epoch 690, val loss: 1.0492974519729614
Epoch 700, training loss: 0.6450687646865845 = 0.01580093801021576 + 0.1 * 6.292677879333496
Epoch 700, val loss: 1.0566771030426025
Epoch 710, training loss: 0.6441760063171387 = 0.015157872810959816 + 0.1 * 6.2901811599731445
Epoch 710, val loss: 1.0639092922210693
Epoch 720, training loss: 0.6444327235221863 = 0.014554462395608425 + 0.1 * 6.298782825469971
Epoch 720, val loss: 1.0709259510040283
Epoch 730, training loss: 0.6444230675697327 = 0.013990522362291813 + 0.1 * 6.304325103759766
Epoch 730, val loss: 1.0776088237762451
Epoch 740, training loss: 0.6422986388206482 = 0.013463660143315792 + 0.1 * 6.288349628448486
Epoch 740, val loss: 1.0844732522964478
Epoch 750, training loss: 0.6401996612548828 = 0.012967057526111603 + 0.1 * 6.2723259925842285
Epoch 750, val loss: 1.0910251140594482
Epoch 760, training loss: 0.6396998167037964 = 0.012497498653829098 + 0.1 * 6.2720232009887695
Epoch 760, val loss: 1.0975520610809326
Epoch 770, training loss: 0.6395248174667358 = 0.012053877115249634 + 0.1 * 6.274709224700928
Epoch 770, val loss: 1.1037874221801758
Epoch 780, training loss: 0.640780508518219 = 0.01163669116795063 + 0.1 * 6.291438102722168
Epoch 780, val loss: 1.1100448369979858
Epoch 790, training loss: 0.6387449502944946 = 0.011243925429880619 + 0.1 * 6.275010108947754
Epoch 790, val loss: 1.1160184144973755
Epoch 800, training loss: 0.6372643113136292 = 0.010872510261833668 + 0.1 * 6.263917922973633
Epoch 800, val loss: 1.1221014261245728
Epoch 810, training loss: 0.6362497210502625 = 0.010519559495151043 + 0.1 * 6.257301330566406
Epoch 810, val loss: 1.1279948949813843
Epoch 820, training loss: 0.6370430588722229 = 0.010184184648096561 + 0.1 * 6.268588542938232
Epoch 820, val loss: 1.1336374282836914
Epoch 830, training loss: 0.6356061100959778 = 0.009866613894701004 + 0.1 * 6.257395267486572
Epoch 830, val loss: 1.139350175857544
Epoch 840, training loss: 0.6375909447669983 = 0.009564774110913277 + 0.1 * 6.280261993408203
Epoch 840, val loss: 1.1449661254882812
Epoch 850, training loss: 0.6350310444831848 = 0.009278315119445324 + 0.1 * 6.2575273513793945
Epoch 850, val loss: 1.150324821472168
Epoch 860, training loss: 0.6343477368354797 = 0.009006001986563206 + 0.1 * 6.253417015075684
Epoch 860, val loss: 1.1555640697479248
Epoch 870, training loss: 0.6342501640319824 = 0.008746952749788761 + 0.1 * 6.255031585693359
Epoch 870, val loss: 1.160887360572815
Epoch 880, training loss: 0.633043646812439 = 0.008499315939843655 + 0.1 * 6.245442867279053
Epoch 880, val loss: 1.1660735607147217
Epoch 890, training loss: 0.6321620941162109 = 0.008262687362730503 + 0.1 * 6.238994121551514
Epoch 890, val loss: 1.171047329902649
Epoch 900, training loss: 0.6325958967208862 = 0.008036898449063301 + 0.1 * 6.245589733123779
Epoch 900, val loss: 1.1760751008987427
Epoch 910, training loss: 0.6323609352111816 = 0.00782121904194355 + 0.1 * 6.245397090911865
Epoch 910, val loss: 1.181003451347351
Epoch 920, training loss: 0.6331537961959839 = 0.00761520816013217 + 0.1 * 6.255385875701904
Epoch 920, val loss: 1.185709834098816
Epoch 930, training loss: 0.6316794753074646 = 0.007418751250952482 + 0.1 * 6.242607116699219
Epoch 930, val loss: 1.190430998802185
Epoch 940, training loss: 0.6299837231636047 = 0.007230198942124844 + 0.1 * 6.227534770965576
Epoch 940, val loss: 1.1951136589050293
Epoch 950, training loss: 0.6304946541786194 = 0.007049099542200565 + 0.1 * 6.234455585479736
Epoch 950, val loss: 1.1997519731521606
Epoch 960, training loss: 0.6301469802856445 = 0.006875357590615749 + 0.1 * 6.232716083526611
Epoch 960, val loss: 1.2042378187179565
Epoch 970, training loss: 0.6286756992340088 = 0.006709193345159292 + 0.1 * 6.219664573669434
Epoch 970, val loss: 1.2085596323013306
Epoch 980, training loss: 0.6303286552429199 = 0.00655004195868969 + 0.1 * 6.237786293029785
Epoch 980, val loss: 1.2129424810409546
Epoch 990, training loss: 0.6284740567207336 = 0.0063975355587899685 + 0.1 * 6.220765113830566
Epoch 990, val loss: 1.217152714729309
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.4613
Flip ASR: 0.4044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7777976989746094 = 1.9404077529907227 + 0.1 * 8.373900413513184
Epoch 0, val loss: 1.9367146492004395
Epoch 10, training loss: 2.7681431770324707 = 1.9307608604431152 + 0.1 * 8.373824119567871
Epoch 10, val loss: 1.9266347885131836
Epoch 20, training loss: 2.756617784500122 = 1.9192696809768677 + 0.1 * 8.373481750488281
Epoch 20, val loss: 1.9145177602767944
Epoch 30, training loss: 2.740635871887207 = 1.9034873247146606 + 0.1 * 8.371484756469727
Epoch 30, val loss: 1.8978022336959839
Epoch 40, training loss: 2.7160277366638184 = 1.8803449869155884 + 0.1 * 8.356828689575195
Epoch 40, val loss: 1.8734792470932007
Epoch 50, training loss: 2.674022912979126 = 1.8472672700881958 + 0.1 * 8.267556190490723
Epoch 50, val loss: 1.8398191928863525
Epoch 60, training loss: 2.596613645553589 = 1.8070478439331055 + 0.1 * 7.895657539367676
Epoch 60, val loss: 1.8010079860687256
Epoch 70, training loss: 2.5148611068725586 = 1.7656373977661133 + 0.1 * 7.492237091064453
Epoch 70, val loss: 1.7621554136276245
Epoch 80, training loss: 2.4317405223846436 = 1.7220637798309326 + 0.1 * 7.096767902374268
Epoch 80, val loss: 1.722755789756775
Epoch 90, training loss: 2.358941078186035 = 1.669048547744751 + 0.1 * 6.898926258087158
Epoch 90, val loss: 1.676398515701294
Epoch 100, training loss: 2.282553195953369 = 1.5992976427078247 + 0.1 * 6.832555294036865
Epoch 100, val loss: 1.6141743659973145
Epoch 110, training loss: 2.1907787322998047 = 1.5117897987365723 + 0.1 * 6.78988790512085
Epoch 110, val loss: 1.5380815267562866
Epoch 120, training loss: 2.0862817764282227 = 1.4098682403564453 + 0.1 * 6.764133930206299
Epoch 120, val loss: 1.453845500946045
Epoch 130, training loss: 1.9745396375656128 = 1.3001856803894043 + 0.1 * 6.743539333343506
Epoch 130, val loss: 1.3644627332687378
Epoch 140, training loss: 1.8604403734207153 = 1.1880474090576172 + 0.1 * 6.723929405212402
Epoch 140, val loss: 1.2758715152740479
Epoch 150, training loss: 1.7487692832946777 = 1.0776751041412354 + 0.1 * 6.710940837860107
Epoch 150, val loss: 1.191070318222046
Epoch 160, training loss: 1.6440807580947876 = 0.9752203822135925 + 0.1 * 6.68860387802124
Epoch 160, val loss: 1.1149367094039917
Epoch 170, training loss: 1.549320936203003 = 0.881758451461792 + 0.1 * 6.675624370574951
Epoch 170, val loss: 1.0473053455352783
Epoch 180, training loss: 1.4640743732452393 = 0.7971362471580505 + 0.1 * 6.669381618499756
Epoch 180, val loss: 0.987596869468689
Epoch 190, training loss: 1.3861608505249023 = 0.7213444709777832 + 0.1 * 6.648163318634033
Epoch 190, val loss: 0.9354158639907837
Epoch 200, training loss: 1.316075325012207 = 0.6526239514350891 + 0.1 * 6.634513854980469
Epoch 200, val loss: 0.8890399932861328
Epoch 210, training loss: 1.2530622482299805 = 0.5901814103126526 + 0.1 * 6.62880802154541
Epoch 210, val loss: 0.8478336334228516
Epoch 220, training loss: 1.194966435432434 = 0.5338884592056274 + 0.1 * 6.610779762268066
Epoch 220, val loss: 0.8118962645530701
Epoch 230, training loss: 1.141696572303772 = 0.4824467897415161 + 0.1 * 6.592497825622559
Epoch 230, val loss: 0.7803108096122742
Epoch 240, training loss: 1.093745470046997 = 0.43547967076301575 + 0.1 * 6.582657337188721
Epoch 240, val loss: 0.7537033557891846
Epoch 250, training loss: 1.0499948263168335 = 0.3933962881565094 + 0.1 * 6.565985202789307
Epoch 250, val loss: 0.7323505878448486
Epoch 260, training loss: 1.010688066482544 = 0.35499316453933716 + 0.1 * 6.556949615478516
Epoch 260, val loss: 0.7147907018661499
Epoch 270, training loss: 0.9756667613983154 = 0.3198927044868469 + 0.1 * 6.557740211486816
Epoch 270, val loss: 0.700402557849884
Epoch 280, training loss: 0.9414352178573608 = 0.28822624683380127 + 0.1 * 6.532089710235596
Epoch 280, val loss: 0.6888173222541809
Epoch 290, training loss: 0.9133890271186829 = 0.2594866156578064 + 0.1 * 6.5390238761901855
Epoch 290, val loss: 0.679507851600647
Epoch 300, training loss: 0.885140597820282 = 0.23372219502925873 + 0.1 * 6.514183521270752
Epoch 300, val loss: 0.672397255897522
Epoch 310, training loss: 0.8608018159866333 = 0.21061453223228455 + 0.1 * 6.501872539520264
Epoch 310, val loss: 0.6671714186668396
Epoch 320, training loss: 0.8393484354019165 = 0.18989820778369904 + 0.1 * 6.494502544403076
Epoch 320, val loss: 0.6637358665466309
Epoch 330, training loss: 0.821954607963562 = 0.1714731901884079 + 0.1 * 6.504814147949219
Epoch 330, val loss: 0.6617944836616516
Epoch 340, training loss: 0.8029316067695618 = 0.15517593920230865 + 0.1 * 6.4775567054748535
Epoch 340, val loss: 0.6613015532493591
Epoch 350, training loss: 0.7888343334197998 = 0.1406421959400177 + 0.1 * 6.481921672821045
Epoch 350, val loss: 0.6620575189590454
Epoch 360, training loss: 0.7743555903434753 = 0.12772198021411896 + 0.1 * 6.466336250305176
Epoch 360, val loss: 0.6637246012687683
Epoch 370, training loss: 0.762683093547821 = 0.116184763610363 + 0.1 * 6.464982986450195
Epoch 370, val loss: 0.6662537455558777
Epoch 380, training loss: 0.7514374256134033 = 0.10582243651151657 + 0.1 * 6.456149578094482
Epoch 380, val loss: 0.6696892976760864
Epoch 390, training loss: 0.7422159314155579 = 0.09653723239898682 + 0.1 * 6.456786632537842
Epoch 390, val loss: 0.6736396551132202
Epoch 400, training loss: 0.7326458096504211 = 0.08821994066238403 + 0.1 * 6.444258689880371
Epoch 400, val loss: 0.6780552864074707
Epoch 410, training loss: 0.7245856523513794 = 0.08072337508201599 + 0.1 * 6.43862247467041
Epoch 410, val loss: 0.6829836368560791
Epoch 420, training loss: 0.7177202701568604 = 0.07400541752576828 + 0.1 * 6.437148094177246
Epoch 420, val loss: 0.6880943179130554
Epoch 430, training loss: 0.7102420926094055 = 0.0679699182510376 + 0.1 * 6.4227213859558105
Epoch 430, val loss: 0.693483829498291
Epoch 440, training loss: 0.7034374475479126 = 0.06254442781209946 + 0.1 * 6.40893030166626
Epoch 440, val loss: 0.6990940570831299
Epoch 450, training loss: 0.6983847618103027 = 0.057669974863529205 + 0.1 * 6.407147407531738
Epoch 450, val loss: 0.7048288583755493
Epoch 460, training loss: 0.6933169364929199 = 0.05327259376645088 + 0.1 * 6.400443077087402
Epoch 460, val loss: 0.7106969952583313
Epoch 470, training loss: 0.6901714205741882 = 0.04930936172604561 + 0.1 * 6.408620357513428
Epoch 470, val loss: 0.716621994972229
Epoch 480, training loss: 0.6845130920410156 = 0.04575427621603012 + 0.1 * 6.3875885009765625
Epoch 480, val loss: 0.7225478291511536
Epoch 490, training loss: 0.6806257367134094 = 0.042546145617961884 + 0.1 * 6.380795955657959
Epoch 490, val loss: 0.7285445332527161
Epoch 500, training loss: 0.6773160696029663 = 0.03964357078075409 + 0.1 * 6.376724720001221
Epoch 500, val loss: 0.7345209717750549
Epoch 510, training loss: 0.673442006111145 = 0.037019599229097366 + 0.1 * 6.364223957061768
Epoch 510, val loss: 0.7404565215110779
Epoch 520, training loss: 0.6702897548675537 = 0.034636616706848145 + 0.1 * 6.356531143188477
Epoch 520, val loss: 0.7464300990104675
Epoch 530, training loss: 0.6684845685958862 = 0.03247203677892685 + 0.1 * 6.360125541687012
Epoch 530, val loss: 0.7523444890975952
Epoch 540, training loss: 0.6661345362663269 = 0.030507130548357964 + 0.1 * 6.356273651123047
Epoch 540, val loss: 0.7581728100776672
Epoch 550, training loss: 0.6638190150260925 = 0.028712620958685875 + 0.1 * 6.3510637283325195
Epoch 550, val loss: 0.7640187740325928
Epoch 560, training loss: 0.6606948971748352 = 0.02707180753350258 + 0.1 * 6.336230278015137
Epoch 560, val loss: 0.7696987986564636
Epoch 570, training loss: 0.659747838973999 = 0.025565536692738533 + 0.1 * 6.341822624206543
Epoch 570, val loss: 0.7753911018371582
Epoch 580, training loss: 0.6581154465675354 = 0.02418714575469494 + 0.1 * 6.339282989501953
Epoch 580, val loss: 0.780929446220398
Epoch 590, training loss: 0.6541321277618408 = 0.022920917719602585 + 0.1 * 6.312112331390381
Epoch 590, val loss: 0.7864327430725098
Epoch 600, training loss: 0.6555251479148865 = 0.02175179310142994 + 0.1 * 6.337733268737793
Epoch 600, val loss: 0.7918907999992371
Epoch 610, training loss: 0.6515350341796875 = 0.020675303414463997 + 0.1 * 6.308597087860107
Epoch 610, val loss: 0.7971863746643066
Epoch 620, training loss: 0.6523333787918091 = 0.019677821546792984 + 0.1 * 6.3265557289123535
Epoch 620, val loss: 0.8024406433105469
Epoch 630, training loss: 0.6497498750686646 = 0.01875418610870838 + 0.1 * 6.3099565505981445
Epoch 630, val loss: 0.8076022863388062
Epoch 640, training loss: 0.6476810574531555 = 0.01789655163884163 + 0.1 * 6.297844886779785
Epoch 640, val loss: 0.8126744627952576
Epoch 650, training loss: 0.6465451717376709 = 0.01709706522524357 + 0.1 * 6.294480800628662
Epoch 650, val loss: 0.8177161812782288
Epoch 660, training loss: 0.6457720398902893 = 0.016350897029042244 + 0.1 * 6.294210910797119
Epoch 660, val loss: 0.8226230144500732
Epoch 670, training loss: 0.6457475423812866 = 0.0156563688069582 + 0.1 * 6.300911903381348
Epoch 670, val loss: 0.8274303078651428
Epoch 680, training loss: 0.643622875213623 = 0.015008259564638138 + 0.1 * 6.2861456871032715
Epoch 680, val loss: 0.8321626782417297
Epoch 690, training loss: 0.6424499750137329 = 0.014402403496205807 + 0.1 * 6.280475616455078
Epoch 690, val loss: 0.8368165493011475
Epoch 700, training loss: 0.6423873901367188 = 0.013833338394761086 + 0.1 * 6.285540580749512
Epoch 700, val loss: 0.841429591178894
Epoch 710, training loss: 0.6414567232131958 = 0.013299219310283661 + 0.1 * 6.281574726104736
Epoch 710, val loss: 0.8459228277206421
Epoch 720, training loss: 0.639559268951416 = 0.012797256000339985 + 0.1 * 6.267620086669922
Epoch 720, val loss: 0.8503597974777222
Epoch 730, training loss: 0.6390435099601746 = 0.012325778603553772 + 0.1 * 6.267177581787109
Epoch 730, val loss: 0.8547227382659912
Epoch 740, training loss: 0.6374478936195374 = 0.011880727484822273 + 0.1 * 6.25567102432251
Epoch 740, val loss: 0.8590139150619507
Epoch 750, training loss: 0.6384934782981873 = 0.011460934765636921 + 0.1 * 6.270325183868408
Epoch 750, val loss: 0.8632078766822815
Epoch 760, training loss: 0.6367238163948059 = 0.011065587401390076 + 0.1 * 6.256582260131836
Epoch 760, val loss: 0.8673214912414551
Epoch 770, training loss: 0.6359820365905762 = 0.010691603645682335 + 0.1 * 6.252904415130615
Epoch 770, val loss: 0.8713818192481995
Epoch 780, training loss: 0.6375336050987244 = 0.010337376967072487 + 0.1 * 6.2719621658325195
Epoch 780, val loss: 0.8753836750984192
Epoch 790, training loss: 0.6354798674583435 = 0.010001910850405693 + 0.1 * 6.25477933883667
Epoch 790, val loss: 0.8793367743492126
Epoch 800, training loss: 0.6350918412208557 = 0.009684025309979916 + 0.1 * 6.254077911376953
Epoch 800, val loss: 0.8831958174705505
Epoch 810, training loss: 0.6331430673599243 = 0.009381805546581745 + 0.1 * 6.237612247467041
Epoch 810, val loss: 0.8870204091072083
Epoch 820, training loss: 0.6339757442474365 = 0.009094619192183018 + 0.1 * 6.2488112449646
Epoch 820, val loss: 0.890790581703186
Epoch 830, training loss: 0.6334259510040283 = 0.008821284398436546 + 0.1 * 6.246046543121338
Epoch 830, val loss: 0.8944889903068542
Epoch 840, training loss: 0.632353663444519 = 0.008561259135603905 + 0.1 * 6.237924098968506
Epoch 840, val loss: 0.8981465697288513
Epoch 850, training loss: 0.6309353709220886 = 0.008314190432429314 + 0.1 * 6.2262115478515625
Epoch 850, val loss: 0.9017149806022644
Epoch 860, training loss: 0.6308873295783997 = 0.008078742772340775 + 0.1 * 6.228085517883301
Epoch 860, val loss: 0.9052596688270569
Epoch 870, training loss: 0.632064700126648 = 0.007853299379348755 + 0.1 * 6.2421135902404785
Epoch 870, val loss: 0.9087777137756348
Epoch 880, training loss: 0.6310107707977295 = 0.007638594135642052 + 0.1 * 6.233721733093262
Epoch 880, val loss: 0.9121834635734558
Epoch 890, training loss: 0.6292579770088196 = 0.0074337199330329895 + 0.1 * 6.218242645263672
Epoch 890, val loss: 0.915534496307373
Epoch 900, training loss: 0.6296390295028687 = 0.007237452082335949 + 0.1 * 6.224015712738037
Epoch 900, val loss: 0.9189144372940063
Epoch 910, training loss: 0.6303470134735107 = 0.007049438543617725 + 0.1 * 6.232975482940674
Epoch 910, val loss: 0.9222420454025269
Epoch 920, training loss: 0.6287162899971008 = 0.0068695563822984695 + 0.1 * 6.2184672355651855
Epoch 920, val loss: 0.9254610538482666
Epoch 930, training loss: 0.6281388401985168 = 0.006697467993944883 + 0.1 * 6.214413642883301
Epoch 930, val loss: 0.9286859631538391
Epoch 940, training loss: 0.6286252737045288 = 0.006532101891934872 + 0.1 * 6.2209320068359375
Epoch 940, val loss: 0.9318867325782776
Epoch 950, training loss: 0.6286630630493164 = 0.0063735260628163815 + 0.1 * 6.22289514541626
Epoch 950, val loss: 0.9349928498268127
Epoch 960, training loss: 0.6268326640129089 = 0.00622163200750947 + 0.1 * 6.206110000610352
Epoch 960, val loss: 0.9380593299865723
Epoch 970, training loss: 0.627572238445282 = 0.006075614131987095 + 0.1 * 6.214966297149658
Epoch 970, val loss: 0.9411247372627258
Epoch 980, training loss: 0.6265280842781067 = 0.00593486987054348 + 0.1 * 6.205931663513184
Epoch 980, val loss: 0.9441351890563965
Epoch 990, training loss: 0.6275879740715027 = 0.005799287464469671 + 0.1 * 6.217886447906494
Epoch 990, val loss: 0.9471016526222229
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8672
Flip ASR: 0.8489/225 nodes
The final ASR:0.61501, 0.17972, Accuracy:0.81852, 0.00524
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9412])
updated graph: torch.Size([2, 10484])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97417, 0.00000, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7887136936187744 = 1.9513288736343384 + 0.1 * 8.373848915100098
Epoch 0, val loss: 1.9515882730484009
Epoch 10, training loss: 2.7779879570007324 = 1.9406224489212036 + 0.1 * 8.373655319213867
Epoch 10, val loss: 1.9413028955459595
Epoch 20, training loss: 2.7641475200653076 = 1.9268755912780762 + 0.1 * 8.372718811035156
Epoch 20, val loss: 1.9276460409164429
Epoch 30, training loss: 2.743712902069092 = 1.9070740938186646 + 0.1 * 8.366388320922852
Epoch 30, val loss: 1.9076621532440186
Epoch 40, training loss: 2.710597038269043 = 1.8776694536209106 + 0.1 * 8.329275131225586
Epoch 40, val loss: 1.878445029258728
Epoch 50, training loss: 2.648880958557129 = 1.838244080543518 + 0.1 * 8.106369018554688
Epoch 50, val loss: 1.8415881395339966
Epoch 60, training loss: 2.571716070175171 = 1.7959383726119995 + 0.1 * 7.757776260375977
Epoch 60, val loss: 1.8046973943710327
Epoch 70, training loss: 2.495004415512085 = 1.7557275295257568 + 0.1 * 7.392768859863281
Epoch 70, val loss: 1.7709333896636963
Epoch 80, training loss: 2.421802043914795 = 1.7139722108840942 + 0.1 * 7.078299045562744
Epoch 80, val loss: 1.735783576965332
Epoch 90, training loss: 2.3498597145080566 = 1.6597951650619507 + 0.1 * 6.900645732879639
Epoch 90, val loss: 1.6892311573028564
Epoch 100, training loss: 2.2689969539642334 = 1.5874136686325073 + 0.1 * 6.815832614898682
Epoch 100, val loss: 1.628600001335144
Epoch 110, training loss: 2.176572799682617 = 1.5007809400558472 + 0.1 * 6.757917881011963
Epoch 110, val loss: 1.557827353477478
Epoch 120, training loss: 2.082127332687378 = 1.4104735851287842 + 0.1 * 6.716536998748779
Epoch 120, val loss: 1.484606146812439
Epoch 130, training loss: 1.9912543296813965 = 1.3229851722717285 + 0.1 * 6.6826910972595215
Epoch 130, val loss: 1.4163663387298584
Epoch 140, training loss: 1.9059312343597412 = 1.2403713464736938 + 0.1 * 6.6555986404418945
Epoch 140, val loss: 1.3544490337371826
Epoch 150, training loss: 1.8278214931488037 = 1.1640554666519165 + 0.1 * 6.637660026550293
Epoch 150, val loss: 1.2994239330291748
Epoch 160, training loss: 1.7534520626068115 = 1.0912843942642212 + 0.1 * 6.621676921844482
Epoch 160, val loss: 1.2483445405960083
Epoch 170, training loss: 1.6790921688079834 = 1.0181478261947632 + 0.1 * 6.6094441413879395
Epoch 170, val loss: 1.1973565816879272
Epoch 180, training loss: 1.603480339050293 = 0.9434731602668762 + 0.1 * 6.600070953369141
Epoch 180, val loss: 1.1443792581558228
Epoch 190, training loss: 1.52676260471344 = 0.8675577044487 + 0.1 * 6.592048645019531
Epoch 190, val loss: 1.0895400047302246
Epoch 200, training loss: 1.450161099433899 = 0.7913355231285095 + 0.1 * 6.588255405426025
Epoch 200, val loss: 1.0336673259735107
Epoch 210, training loss: 1.3752834796905518 = 0.7173623442649841 + 0.1 * 6.5792107582092285
Epoch 210, val loss: 0.9792126417160034
Epoch 220, training loss: 1.304124116897583 = 0.6471630930900574 + 0.1 * 6.569610595703125
Epoch 220, val loss: 0.9273174405097961
Epoch 230, training loss: 1.2385056018829346 = 0.5823047757148743 + 0.1 * 6.562007427215576
Epoch 230, val loss: 0.8799614310264587
Epoch 240, training loss: 1.1796228885650635 = 0.5238396525382996 + 0.1 * 6.557831764221191
Epoch 240, val loss: 0.8391438722610474
Epoch 250, training loss: 1.1261343955993652 = 0.47115349769592285 + 0.1 * 6.549809455871582
Epoch 250, val loss: 0.8052518963813782
Epoch 260, training loss: 1.0782949924468994 = 0.42393410205841064 + 0.1 * 6.543609619140625
Epoch 260, val loss: 0.7785857915878296
Epoch 270, training loss: 1.0349315404891968 = 0.38142338395118713 + 0.1 * 6.53508186340332
Epoch 270, val loss: 0.7581183314323425
Epoch 280, training loss: 0.9948594570159912 = 0.3429315388202667 + 0.1 * 6.519278526306152
Epoch 280, val loss: 0.7427843809127808
Epoch 290, training loss: 0.9591706395149231 = 0.30800002813339233 + 0.1 * 6.5117058753967285
Epoch 290, val loss: 0.7314199805259705
Epoch 300, training loss: 0.9269757866859436 = 0.276191771030426 + 0.1 * 6.507840156555176
Epoch 300, val loss: 0.7233750820159912
Epoch 310, training loss: 0.897267758846283 = 0.24742771685123444 + 0.1 * 6.4984002113342285
Epoch 310, val loss: 0.7180144190788269
Epoch 320, training loss: 0.8696163892745972 = 0.22160431742668152 + 0.1 * 6.480120658874512
Epoch 320, val loss: 0.7150458693504333
Epoch 330, training loss: 0.8452860116958618 = 0.19840902090072632 + 0.1 * 6.468769550323486
Epoch 330, val loss: 0.7138254642486572
Epoch 340, training loss: 0.8244152069091797 = 0.17784106731414795 + 0.1 * 6.465741157531738
Epoch 340, val loss: 0.7143857479095459
Epoch 350, training loss: 0.8048182725906372 = 0.15976420044898987 + 0.1 * 6.450541019439697
Epoch 350, val loss: 0.7167738676071167
Epoch 360, training loss: 0.7885856628417969 = 0.14389240741729736 + 0.1 * 6.446932315826416
Epoch 360, val loss: 0.7206425070762634
Epoch 370, training loss: 0.7733056545257568 = 0.1300436407327652 + 0.1 * 6.432620048522949
Epoch 370, val loss: 0.7257753610610962
Epoch 380, training loss: 0.7604002356529236 = 0.11791444569826126 + 0.1 * 6.424858093261719
Epoch 380, val loss: 0.73190838098526
Epoch 390, training loss: 0.7489039897918701 = 0.10729308426380157 + 0.1 * 6.416109085083008
Epoch 390, val loss: 0.7387157082557678
Epoch 400, training loss: 0.738404393196106 = 0.09793830662965775 + 0.1 * 6.404660701751709
Epoch 400, val loss: 0.7460552453994751
Epoch 410, training loss: 0.7298881411552429 = 0.0896717831492424 + 0.1 * 6.402163505554199
Epoch 410, val loss: 0.7535977363586426
Epoch 420, training loss: 0.7208381295204163 = 0.08234493434429169 + 0.1 * 6.384931564331055
Epoch 420, val loss: 0.7613252997398376
Epoch 430, training loss: 0.715889573097229 = 0.07578671723604202 + 0.1 * 6.401028633117676
Epoch 430, val loss: 0.7691078186035156
Epoch 440, training loss: 0.7079469561576843 = 0.06990908831357956 + 0.1 * 6.380378723144531
Epoch 440, val loss: 0.7768273949623108
Epoch 450, training loss: 0.7012338042259216 = 0.06460358202457428 + 0.1 * 6.366302490234375
Epoch 450, val loss: 0.7845481634140015
Epoch 460, training loss: 0.6980262398719788 = 0.05981247499585152 + 0.1 * 6.382137298583984
Epoch 460, val loss: 0.7920002937316895
Epoch 470, training loss: 0.6910616159439087 = 0.05548059567809105 + 0.1 * 6.355810165405273
Epoch 470, val loss: 0.799479067325592
Epoch 480, training loss: 0.6860401034355164 = 0.051538530737161636 + 0.1 * 6.345015525817871
Epoch 480, val loss: 0.8068117499351501
Epoch 490, training loss: 0.6820693612098694 = 0.04794835299253464 + 0.1 * 6.341209888458252
Epoch 490, val loss: 0.8139219880104065
Epoch 500, training loss: 0.6781591176986694 = 0.044679637998342514 + 0.1 * 6.334794521331787
Epoch 500, val loss: 0.8210394382476807
Epoch 510, training loss: 0.675747811794281 = 0.041689682751894 + 0.1 * 6.340580940246582
Epoch 510, val loss: 0.828014612197876
Epoch 520, training loss: 0.672271192073822 = 0.038969799876213074 + 0.1 * 6.333013534545898
Epoch 520, val loss: 0.8346592783927917
Epoch 530, training loss: 0.6683833599090576 = 0.03649262338876724 + 0.1 * 6.318906784057617
Epoch 530, val loss: 0.8414087295532227
Epoch 540, training loss: 0.6656919717788696 = 0.0342189259827137 + 0.1 * 6.314730644226074
Epoch 540, val loss: 0.8478938341140747
Epoch 550, training loss: 0.6629168391227722 = 0.03213149681687355 + 0.1 * 6.3078532218933105
Epoch 550, val loss: 0.8541915416717529
Epoch 560, training loss: 0.6604382395744324 = 0.030215032398700714 + 0.1 * 6.302232265472412
Epoch 560, val loss: 0.8605354428291321
Epoch 570, training loss: 0.6607484817504883 = 0.02844936214387417 + 0.1 * 6.322991371154785
Epoch 570, val loss: 0.8667057156562805
Epoch 580, training loss: 0.6562447547912598 = 0.026826705783605576 + 0.1 * 6.294180393218994
Epoch 580, val loss: 0.8728021383285522
Epoch 590, training loss: 0.6551833748817444 = 0.02533116564154625 + 0.1 * 6.298521995544434
Epoch 590, val loss: 0.8788297772407532
Epoch 600, training loss: 0.6524330973625183 = 0.02395118586719036 + 0.1 * 6.28481912612915
Epoch 600, val loss: 0.8845511674880981
Epoch 610, training loss: 0.6509876847267151 = 0.022680625319480896 + 0.1 * 6.283070087432861
Epoch 610, val loss: 0.8903682231903076
Epoch 620, training loss: 0.6499766707420349 = 0.02150391787290573 + 0.1 * 6.284727573394775
Epoch 620, val loss: 0.8959835767745972
Epoch 630, training loss: 0.6480125188827515 = 0.02041267417371273 + 0.1 * 6.275998115539551
Epoch 630, val loss: 0.9015229344367981
Epoch 640, training loss: 0.6480156183242798 = 0.01939936727285385 + 0.1 * 6.286162376403809
Epoch 640, val loss: 0.9068742990493774
Epoch 650, training loss: 0.6475782990455627 = 0.018460821360349655 + 0.1 * 6.29117488861084
Epoch 650, val loss: 0.9121338725090027
Epoch 660, training loss: 0.6447807550430298 = 0.01759291999042034 + 0.1 * 6.271878242492676
Epoch 660, val loss: 0.9173341989517212
Epoch 670, training loss: 0.642983078956604 = 0.01678667962551117 + 0.1 * 6.261963844299316
Epoch 670, val loss: 0.9224902987480164
Epoch 680, training loss: 0.6426051259040833 = 0.01603328250348568 + 0.1 * 6.265718460083008
Epoch 680, val loss: 0.9274202585220337
Epoch 690, training loss: 0.6450494527816772 = 0.015330279245972633 + 0.1 * 6.297192096710205
Epoch 690, val loss: 0.9321137070655823
Epoch 700, training loss: 0.6402431130409241 = 0.014675725251436234 + 0.1 * 6.255673885345459
Epoch 700, val loss: 0.9367841482162476
Epoch 710, training loss: 0.6392179727554321 = 0.014065413735806942 + 0.1 * 6.25152587890625
Epoch 710, val loss: 0.94161456823349
Epoch 720, training loss: 0.6382919549942017 = 0.01349237933754921 + 0.1 * 6.247995376586914
Epoch 720, val loss: 0.9461958408355713
Epoch 730, training loss: 0.6375988721847534 = 0.012953474186360836 + 0.1 * 6.246454238891602
Epoch 730, val loss: 0.9504910111427307
Epoch 740, training loss: 0.6365963220596313 = 0.012448545545339584 + 0.1 * 6.2414774894714355
Epoch 740, val loss: 0.9548906087875366
Epoch 750, training loss: 0.6358190178871155 = 0.011973650194704533 + 0.1 * 6.238453388214111
Epoch 750, val loss: 0.9591062068939209
Epoch 760, training loss: 0.6354610323905945 = 0.01152824517339468 + 0.1 * 6.239327907562256
Epoch 760, val loss: 0.9634000658988953
Epoch 770, training loss: 0.6357961893081665 = 0.011107859201729298 + 0.1 * 6.246882915496826
Epoch 770, val loss: 0.967505693435669
Epoch 780, training loss: 0.6342015266418457 = 0.010710984468460083 + 0.1 * 6.23490571975708
Epoch 780, val loss: 0.9714589715003967
Epoch 790, training loss: 0.6337975263595581 = 0.0103370426222682 + 0.1 * 6.234604835510254
Epoch 790, val loss: 0.9754361510276794
Epoch 800, training loss: 0.6346883177757263 = 0.009982701390981674 + 0.1 * 6.247056007385254
Epoch 800, val loss: 0.9793410897254944
Epoch 810, training loss: 0.6319272518157959 = 0.009648781269788742 + 0.1 * 6.222784519195557
Epoch 810, val loss: 0.9831337332725525
Epoch 820, training loss: 0.6318996548652649 = 0.009332535788416862 + 0.1 * 6.225671291351318
Epoch 820, val loss: 0.9869095683097839
Epoch 830, training loss: 0.6307758688926697 = 0.009031970053911209 + 0.1 * 6.2174391746521
Epoch 830, val loss: 0.9906314611434937
Epoch 840, training loss: 0.6306142807006836 = 0.008746541105210781 + 0.1 * 6.218677043914795
Epoch 840, val loss: 0.994095504283905
Epoch 850, training loss: 0.630255401134491 = 0.008476458489894867 + 0.1 * 6.217789649963379
Epoch 850, val loss: 0.9976490139961243
Epoch 860, training loss: 0.6315333247184753 = 0.008220016025006771 + 0.1 * 6.233132839202881
Epoch 860, val loss: 1.0011544227600098
Epoch 870, training loss: 0.629120945930481 = 0.007975508458912373 + 0.1 * 6.211453914642334
Epoch 870, val loss: 1.0045744180679321
Epoch 880, training loss: 0.6321509480476379 = 0.007743264082819223 + 0.1 * 6.244077205657959
Epoch 880, val loss: 1.0079810619354248
Epoch 890, training loss: 0.6291349530220032 = 0.007521581370383501 + 0.1 * 6.2161335945129395
Epoch 890, val loss: 1.0111474990844727
Epoch 900, training loss: 0.6281207203865051 = 0.007311058230698109 + 0.1 * 6.208096504211426
Epoch 900, val loss: 1.014513373374939
Epoch 910, training loss: 0.6283500790596008 = 0.007109278347343206 + 0.1 * 6.212408065795898
Epoch 910, val loss: 1.0177373886108398
Epoch 920, training loss: 0.6270937919616699 = 0.006915969308465719 + 0.1 * 6.201777935028076
Epoch 920, val loss: 1.020809531211853
Epoch 930, training loss: 0.6272481083869934 = 0.006731611676514149 + 0.1 * 6.205164909362793
Epoch 930, val loss: 1.0239288806915283
Epoch 940, training loss: 0.6284120082855225 = 0.006554748397320509 + 0.1 * 6.218572616577148
Epoch 940, val loss: 1.0269205570220947
Epoch 950, training loss: 0.6265724897384644 = 0.006386243738234043 + 0.1 * 6.201862335205078
Epoch 950, val loss: 1.0297701358795166
Epoch 960, training loss: 0.6266731023788452 = 0.006225280929356813 + 0.1 * 6.204477787017822
Epoch 960, val loss: 1.0328447818756104
Epoch 970, training loss: 0.6258359551429749 = 0.006070648320019245 + 0.1 * 6.197652816772461
Epoch 970, val loss: 1.0357829332351685
Epoch 980, training loss: 0.6259527802467346 = 0.005922213196754456 + 0.1 * 6.200305938720703
Epoch 980, val loss: 1.0386333465576172
Epoch 990, training loss: 0.6246457695960999 = 0.005779395345598459 + 0.1 * 6.188663482666016
Epoch 990, val loss: 1.0414047241210938
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6753
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.8103983402252197 = 1.9730124473571777 + 0.1 * 8.373859405517578
Epoch 0, val loss: 1.9744346141815186
Epoch 10, training loss: 2.7989158630371094 = 1.9615427255630493 + 0.1 * 8.373730659484863
Epoch 10, val loss: 1.962565302848816
Epoch 20, training loss: 2.785151720046997 = 1.9478448629379272 + 0.1 * 8.373067855834961
Epoch 20, val loss: 1.948286533355713
Epoch 30, training loss: 2.7656302452087402 = 1.9287760257720947 + 0.1 * 8.36854076385498
Epoch 30, val loss: 1.928364634513855
Epoch 40, training loss: 2.734250783920288 = 1.9003814458847046 + 0.1 * 8.338693618774414
Epoch 40, val loss: 1.8989840745925903
Epoch 50, training loss: 2.674820899963379 = 1.8603156805038452 + 0.1 * 8.145052909851074
Epoch 50, val loss: 1.858981728553772
Epoch 60, training loss: 2.5911309719085693 = 1.8145006895065308 + 0.1 * 7.766302585601807
Epoch 60, val loss: 1.815940022468567
Epoch 70, training loss: 2.504667043685913 = 1.7731345891952515 + 0.1 * 7.315324783325195
Epoch 70, val loss: 1.7788925170898438
Epoch 80, training loss: 2.431406021118164 = 1.7355414628982544 + 0.1 * 6.958645820617676
Epoch 80, val loss: 1.7456774711608887
Epoch 90, training loss: 2.3730106353759766 = 1.6891392469406128 + 0.1 * 6.838715076446533
Epoch 90, val loss: 1.7034599781036377
Epoch 100, training loss: 2.303825616836548 = 1.6268404722213745 + 0.1 * 6.7698516845703125
Epoch 100, val loss: 1.6485177278518677
Epoch 110, training loss: 2.2210428714752197 = 1.5484963655471802 + 0.1 * 6.725464344024658
Epoch 110, val loss: 1.5822006464004517
Epoch 120, training loss: 2.130136489868164 = 1.4603215456008911 + 0.1 * 6.698148727416992
Epoch 120, val loss: 1.5087858438491821
Epoch 130, training loss: 2.037217140197754 = 1.3700505495071411 + 0.1 * 6.671666622161865
Epoch 130, val loss: 1.4370477199554443
Epoch 140, training loss: 1.94511079788208 = 1.280843734741211 + 0.1 * 6.642669677734375
Epoch 140, val loss: 1.368569016456604
Epoch 150, training loss: 1.8545491695404053 = 1.1929312944412231 + 0.1 * 6.6161789894104
Epoch 150, val loss: 1.302931308746338
Epoch 160, training loss: 1.7665693759918213 = 1.1072112321853638 + 0.1 * 6.5935821533203125
Epoch 160, val loss: 1.2391695976257324
Epoch 170, training loss: 1.6829383373260498 = 1.0253902673721313 + 0.1 * 6.575479984283447
Epoch 170, val loss: 1.1789926290512085
Epoch 180, training loss: 1.6035773754119873 = 0.9478036761283875 + 0.1 * 6.557736396789551
Epoch 180, val loss: 1.1226353645324707
Epoch 190, training loss: 1.5289580821990967 = 0.8739567399024963 + 0.1 * 6.550013065338135
Epoch 190, val loss: 1.0698268413543701
Epoch 200, training loss: 1.458345890045166 = 0.804591715335846 + 0.1 * 6.53754186630249
Epoch 200, val loss: 1.0208667516708374
Epoch 210, training loss: 1.3927900791168213 = 0.7392351031303406 + 0.1 * 6.535550117492676
Epoch 210, val loss: 0.9752625823020935
Epoch 220, training loss: 1.3311572074890137 = 0.6794705986976624 + 0.1 * 6.516866683959961
Epoch 220, val loss: 0.9347242116928101
Epoch 230, training loss: 1.2761633396148682 = 0.6254785656929016 + 0.1 * 6.506847858428955
Epoch 230, val loss: 0.8991985321044922
Epoch 240, training loss: 1.2282192707061768 = 0.5773914456367493 + 0.1 * 6.5082783699035645
Epoch 240, val loss: 0.8694639205932617
Epoch 250, training loss: 1.1836110353469849 = 0.534793496131897 + 0.1 * 6.488175392150879
Epoch 250, val loss: 0.8447183966636658
Epoch 260, training loss: 1.1438767910003662 = 0.49604734778404236 + 0.1 * 6.478294849395752
Epoch 260, val loss: 0.8236587047576904
Epoch 270, training loss: 1.1078885793685913 = 0.46023818850517273 + 0.1 * 6.476503849029541
Epoch 270, val loss: 0.8054978251457214
Epoch 280, training loss: 1.072697639465332 = 0.42665567994117737 + 0.1 * 6.460419178009033
Epoch 280, val loss: 0.789932906627655
Epoch 290, training loss: 1.039616346359253 = 0.3943730592727661 + 0.1 * 6.4524335861206055
Epoch 290, val loss: 0.7764837145805359
Epoch 300, training loss: 1.0070394277572632 = 0.36299583315849304 + 0.1 * 6.440436363220215
Epoch 300, val loss: 0.765204668045044
Epoch 310, training loss: 0.9754123687744141 = 0.3323603570461273 + 0.1 * 6.430520057678223
Epoch 310, val loss: 0.7558614611625671
Epoch 320, training loss: 0.951076865196228 = 0.3028011620044708 + 0.1 * 6.482756614685059
Epoch 320, val loss: 0.7486734390258789
Epoch 330, training loss: 0.918380618095398 = 0.27511391043663025 + 0.1 * 6.432667255401611
Epoch 330, val loss: 0.7439537644386292
Epoch 340, training loss: 0.8900346755981445 = 0.24902184307575226 + 0.1 * 6.410128116607666
Epoch 340, val loss: 0.7413776516914368
Epoch 350, training loss: 0.8641883134841919 = 0.22447249293327332 + 0.1 * 6.397158622741699
Epoch 350, val loss: 0.7411377429962158
Epoch 360, training loss: 0.8417984247207642 = 0.2016899287700653 + 0.1 * 6.401084899902344
Epoch 360, val loss: 0.742984414100647
Epoch 370, training loss: 0.8199324607849121 = 0.1810896247625351 + 0.1 * 6.388428211212158
Epoch 370, val loss: 0.746497392654419
Epoch 380, training loss: 0.8004225492477417 = 0.1625775843858719 + 0.1 * 6.3784499168396
Epoch 380, val loss: 0.7517170310020447
Epoch 390, training loss: 0.7826858162879944 = 0.14598582684993744 + 0.1 * 6.366999626159668
Epoch 390, val loss: 0.7581526637077332
Epoch 400, training loss: 0.7697153687477112 = 0.13138318061828613 + 0.1 * 6.383321762084961
Epoch 400, val loss: 0.7654368281364441
Epoch 410, training loss: 0.7538142800331116 = 0.11866463720798492 + 0.1 * 6.35149621963501
Epoch 410, val loss: 0.7738261222839355
Epoch 420, training loss: 0.743657648563385 = 0.10751188546419144 + 0.1 * 6.361457824707031
Epoch 420, val loss: 0.7827103137969971
Epoch 430, training loss: 0.7322096228599548 = 0.09774069488048553 + 0.1 * 6.344688892364502
Epoch 430, val loss: 0.7920510172843933
Epoch 440, training loss: 0.7232730984687805 = 0.08912402391433716 + 0.1 * 6.341490745544434
Epoch 440, val loss: 0.8017233610153198
Epoch 450, training loss: 0.7151950597763062 = 0.08151386678218842 + 0.1 * 6.336811542510986
Epoch 450, val loss: 0.811539888381958
Epoch 460, training loss: 0.7069442272186279 = 0.07476780563592911 + 0.1 * 6.32176399230957
Epoch 460, val loss: 0.8215848803520203
Epoch 470, training loss: 0.7017529010772705 = 0.06875713169574738 + 0.1 * 6.329957962036133
Epoch 470, val loss: 0.8315979838371277
Epoch 480, training loss: 0.6951072216033936 = 0.06338568031787872 + 0.1 * 6.317215442657471
Epoch 480, val loss: 0.8416141271591187
Epoch 490, training loss: 0.6890522837638855 = 0.05857057496905327 + 0.1 * 6.304817199707031
Epoch 490, val loss: 0.8515686392784119
Epoch 500, training loss: 0.686492383480072 = 0.05422995984554291 + 0.1 * 6.322624206542969
Epoch 500, val loss: 0.8613415956497192
Epoch 510, training loss: 0.6800165176391602 = 0.050324246287345886 + 0.1 * 6.296922206878662
Epoch 510, val loss: 0.8709554076194763
Epoch 520, training loss: 0.6773838996887207 = 0.04679921641945839 + 0.1 * 6.305846214294434
Epoch 520, val loss: 0.8804080486297607
Epoch 530, training loss: 0.6728612184524536 = 0.043616704642772675 + 0.1 * 6.292444705963135
Epoch 530, val loss: 0.8896455764770508
Epoch 540, training loss: 0.6690673828125 = 0.04073070362210274 + 0.1 * 6.283366680145264
Epoch 540, val loss: 0.8986666202545166
Epoch 550, training loss: 0.6691375374794006 = 0.0381004698574543 + 0.1 * 6.310370445251465
Epoch 550, val loss: 0.9075265526771545
Epoch 560, training loss: 0.6633781790733337 = 0.035710036754608154 + 0.1 * 6.276681423187256
Epoch 560, val loss: 0.9161028861999512
Epoch 570, training loss: 0.6604368090629578 = 0.03353084623813629 + 0.1 * 6.269059658050537
Epoch 570, val loss: 0.9246743321418762
Epoch 580, training loss: 0.6591377258300781 = 0.03153255209326744 + 0.1 * 6.276051998138428
Epoch 580, val loss: 0.9330046772956848
Epoch 590, training loss: 0.6572263836860657 = 0.029704229906201363 + 0.1 * 6.275221824645996
Epoch 590, val loss: 0.9409976601600647
Epoch 600, training loss: 0.6538993120193481 = 0.028030332177877426 + 0.1 * 6.258689880371094
Epoch 600, val loss: 0.9492275714874268
Epoch 610, training loss: 0.6518051028251648 = 0.026486242190003395 + 0.1 * 6.253188610076904
Epoch 610, val loss: 0.9570960402488708
Epoch 620, training loss: 0.6518317461013794 = 0.02505972981452942 + 0.1 * 6.2677202224731445
Epoch 620, val loss: 0.9647940397262573
Epoch 630, training loss: 0.6487446427345276 = 0.023743608966469765 + 0.1 * 6.250010013580322
Epoch 630, val loss: 0.9724918007850647
Epoch 640, training loss: 0.6481192708015442 = 0.02252507396042347 + 0.1 * 6.255941867828369
Epoch 640, val loss: 0.9800760746002197
Epoch 650, training loss: 0.6456881761550903 = 0.021395917981863022 + 0.1 * 6.242922306060791
Epoch 650, val loss: 0.9874537587165833
Epoch 660, training loss: 0.6446459293365479 = 0.020348697900772095 + 0.1 * 6.242972373962402
Epoch 660, val loss: 0.9948521256446838
Epoch 670, training loss: 0.6443198919296265 = 0.019373726099729538 + 0.1 * 6.249461650848389
Epoch 670, val loss: 1.002088189125061
Epoch 680, training loss: 0.6422467231750488 = 0.018467549234628677 + 0.1 * 6.237791538238525
Epoch 680, val loss: 1.0092227458953857
Epoch 690, training loss: 0.6428616046905518 = 0.01762370951473713 + 0.1 * 6.252378463745117
Epoch 690, val loss: 1.0162839889526367
Epoch 700, training loss: 0.6399489045143127 = 0.01683761179447174 + 0.1 * 6.231112957000732
Epoch 700, val loss: 1.0231465101242065
Epoch 710, training loss: 0.6402993202209473 = 0.016104381531476974 + 0.1 * 6.241949081420898
Epoch 710, val loss: 1.0299934148788452
Epoch 720, training loss: 0.6377716064453125 = 0.015419668518006802 + 0.1 * 6.223519325256348
Epoch 720, val loss: 1.0368107557296753
Epoch 730, training loss: 0.6379613280296326 = 0.014778382144868374 + 0.1 * 6.231829643249512
Epoch 730, val loss: 1.0434799194335938
Epoch 740, training loss: 0.6358080506324768 = 0.014176479540765285 + 0.1 * 6.216315269470215
Epoch 740, val loss: 1.0499430894851685
Epoch 750, training loss: 0.6358324885368347 = 0.013611714355647564 + 0.1 * 6.222207546234131
Epoch 750, val loss: 1.0564481019973755
Epoch 760, training loss: 0.6340023875236511 = 0.013080691918730736 + 0.1 * 6.209216594696045
Epoch 760, val loss: 1.0626505613327026
Epoch 770, training loss: 0.6349042057991028 = 0.012582698836922646 + 0.1 * 6.223214626312256
Epoch 770, val loss: 1.0688685178756714
Epoch 780, training loss: 0.6335007548332214 = 0.012113946489989758 + 0.1 * 6.213868141174316
Epoch 780, val loss: 1.0750154256820679
Epoch 790, training loss: 0.6318328976631165 = 0.011672357097268105 + 0.1 * 6.201605319976807
Epoch 790, val loss: 1.081096887588501
Epoch 800, training loss: 0.6317130923271179 = 0.011255355551838875 + 0.1 * 6.2045769691467285
Epoch 800, val loss: 1.0871578454971313
Epoch 810, training loss: 0.6310564279556274 = 0.010860438458621502 + 0.1 * 6.201959609985352
Epoch 810, val loss: 1.0929772853851318
Epoch 820, training loss: 0.6303389668464661 = 0.010487355291843414 + 0.1 * 6.198515892028809
Epoch 820, val loss: 1.0985023975372314
Epoch 830, training loss: 0.6299970149993896 = 0.010136246681213379 + 0.1 * 6.198607444763184
Epoch 830, val loss: 1.1043907403945923
Epoch 840, training loss: 0.6298986077308655 = 0.009803460910916328 + 0.1 * 6.200951099395752
Epoch 840, val loss: 1.1099910736083984
Epoch 850, training loss: 0.6282439827919006 = 0.009487590752542019 + 0.1 * 6.187563896179199
Epoch 850, val loss: 1.1153018474578857
Epoch 860, training loss: 0.6283776760101318 = 0.009188968688249588 + 0.1 * 6.191886901855469
Epoch 860, val loss: 1.120858907699585
Epoch 870, training loss: 0.6291009187698364 = 0.008904862217605114 + 0.1 * 6.20196008682251
Epoch 870, val loss: 1.1261541843414307
Epoch 880, training loss: 0.6274535059928894 = 0.008634905330836773 + 0.1 * 6.188185691833496
Epoch 880, val loss: 1.1313995122909546
Epoch 890, training loss: 0.626969575881958 = 0.008377967402338982 + 0.1 * 6.185915946960449
Epoch 890, val loss: 1.1366307735443115
Epoch 900, training loss: 0.6266072392463684 = 0.008133040741086006 + 0.1 * 6.184741973876953
Epoch 900, val loss: 1.1414881944656372
Epoch 910, training loss: 0.6260102391242981 = 0.007900841534137726 + 0.1 * 6.181094169616699
Epoch 910, val loss: 1.1466104984283447
Epoch 920, training loss: 0.6263853311538696 = 0.00767943263053894 + 0.1 * 6.187058925628662
Epoch 920, val loss: 1.151477336883545
Epoch 930, training loss: 0.6260657906532288 = 0.007468427997082472 + 0.1 * 6.185973644256592
Epoch 930, val loss: 1.1563433408737183
Epoch 940, training loss: 0.6255124807357788 = 0.007267033215612173 + 0.1 * 6.1824541091918945
Epoch 940, val loss: 1.161162257194519
Epoch 950, training loss: 0.6254085302352905 = 0.007074415683746338 + 0.1 * 6.183341026306152
Epoch 950, val loss: 1.1658004522323608
Epoch 960, training loss: 0.6238568425178528 = 0.006890011951327324 + 0.1 * 6.169668197631836
Epoch 960, val loss: 1.1703763008117676
Epoch 970, training loss: 0.6239745020866394 = 0.006713747512549162 + 0.1 * 6.172607421875
Epoch 970, val loss: 1.1750174760818481
Epoch 980, training loss: 0.6254469752311707 = 0.006544488947838545 + 0.1 * 6.189024925231934
Epoch 980, val loss: 1.179326057434082
Epoch 990, training loss: 0.6241249442100525 = 0.006382611580193043 + 0.1 * 6.177423477172852
Epoch 990, val loss: 1.1837122440338135
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6310
Flip ASR: 0.5822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7756235599517822 = 1.9382367134094238 + 0.1 * 8.373867988586426
Epoch 0, val loss: 1.9251267910003662
Epoch 10, training loss: 2.7660632133483887 = 1.9286919832229614 + 0.1 * 8.373713493347168
Epoch 10, val loss: 1.9161291122436523
Epoch 20, training loss: 2.753843069076538 = 1.916550636291504 + 0.1 * 8.372923851013184
Epoch 20, val loss: 1.9044153690338135
Epoch 30, training loss: 2.735848903656006 = 1.8991081714630127 + 0.1 * 8.36740779876709
Epoch 30, val loss: 1.887418508529663
Epoch 40, training loss: 2.706238269805908 = 1.8730478286743164 + 0.1 * 8.331904411315918
Epoch 40, val loss: 1.8623888492584229
Epoch 50, training loss: 2.646099090576172 = 1.8375999927520752 + 0.1 * 8.084990501403809
Epoch 50, val loss: 1.8303226232528687
Epoch 60, training loss: 2.5762147903442383 = 1.7992321252822876 + 0.1 * 7.769826889038086
Epoch 60, val loss: 1.7982838153839111
Epoch 70, training loss: 2.507370948791504 = 1.7626240253448486 + 0.1 * 7.447469711303711
Epoch 70, val loss: 1.7686655521392822
Epoch 80, training loss: 2.435227632522583 = 1.7247159481048584 + 0.1 * 7.105116367340088
Epoch 80, val loss: 1.7378315925598145
Epoch 90, training loss: 2.3630239963531494 = 1.6769301891326904 + 0.1 * 6.860937595367432
Epoch 90, val loss: 1.6962041854858398
Epoch 100, training loss: 2.2900030612945557 = 1.6124693155288696 + 0.1 * 6.775338172912598
Epoch 100, val loss: 1.6398271322250366
Epoch 110, training loss: 2.2059459686279297 = 1.5334246158599854 + 0.1 * 6.725214004516602
Epoch 110, val loss: 1.5743143558502197
Epoch 120, training loss: 2.116144895553589 = 1.4470208883285522 + 0.1 * 6.691240310668945
Epoch 120, val loss: 1.5029706954956055
Epoch 130, training loss: 2.0246055126190186 = 1.3578912019729614 + 0.1 * 6.66714334487915
Epoch 130, val loss: 1.4296166896820068
Epoch 140, training loss: 1.9322164058685303 = 1.2678771018981934 + 0.1 * 6.643393039703369
Epoch 140, val loss: 1.3566651344299316
Epoch 150, training loss: 1.8389153480529785 = 1.175532579421997 + 0.1 * 6.633826732635498
Epoch 150, val loss: 1.2836323976516724
Epoch 160, training loss: 1.7445166110992432 = 1.0835046768188477 + 0.1 * 6.610119342803955
Epoch 160, val loss: 1.2141855955123901
Epoch 170, training loss: 1.6515755653381348 = 0.9920774102210999 + 0.1 * 6.5949811935424805
Epoch 170, val loss: 1.147411584854126
Epoch 180, training loss: 1.5612739324569702 = 0.902857780456543 + 0.1 * 6.584161281585693
Epoch 180, val loss: 1.0828410387039185
Epoch 190, training loss: 1.477203369140625 = 0.8193100094795227 + 0.1 * 6.578933238983154
Epoch 190, val loss: 1.0230804681777954
Epoch 200, training loss: 1.3978278636932373 = 0.7413191199302673 + 0.1 * 6.56508731842041
Epoch 200, val loss: 0.9669947028160095
Epoch 210, training loss: 1.3251230716705322 = 0.6699208617210388 + 0.1 * 6.5520219802856445
Epoch 210, val loss: 0.9158537983894348
Epoch 220, training loss: 1.2631006240844727 = 0.6066975593566895 + 0.1 * 6.564030647277832
Epoch 220, val loss: 0.8713383078575134
Epoch 230, training loss: 1.2063744068145752 = 0.552491307258606 + 0.1 * 6.538830757141113
Epoch 230, val loss: 0.8345903754234314
Epoch 240, training loss: 1.1571935415267944 = 0.5050775408744812 + 0.1 * 6.521159648895264
Epoch 240, val loss: 0.803714394569397
Epoch 250, training loss: 1.1137064695358276 = 0.4628262221813202 + 0.1 * 6.50880241394043
Epoch 250, val loss: 0.7778206467628479
Epoch 260, training loss: 1.0764079093933105 = 0.42459428310394287 + 0.1 * 6.518136024475098
Epoch 260, val loss: 0.7558674216270447
Epoch 270, training loss: 1.0376653671264648 = 0.38937023282051086 + 0.1 * 6.482951641082764
Epoch 270, val loss: 0.737474799156189
Epoch 280, training loss: 1.0029525756835938 = 0.3558320105075836 + 0.1 * 6.471205711364746
Epoch 280, val loss: 0.7211295962333679
Epoch 290, training loss: 0.9723525047302246 = 0.3235357403755188 + 0.1 * 6.4881672859191895
Epoch 290, val loss: 0.7067936062812805
Epoch 300, training loss: 0.9385508298873901 = 0.2929810881614685 + 0.1 * 6.455697059631348
Epoch 300, val loss: 0.6945016384124756
Epoch 310, training loss: 0.9111003875732422 = 0.2642357051372528 + 0.1 * 6.46864652633667
Epoch 310, val loss: 0.6838797926902771
Epoch 320, training loss: 0.8806135654449463 = 0.2378137856721878 + 0.1 * 6.427998065948486
Epoch 320, val loss: 0.6752933263778687
Epoch 330, training loss: 0.8557261824607849 = 0.21367646753787994 + 0.1 * 6.420497417449951
Epoch 330, val loss: 0.6687045693397522
Epoch 340, training loss: 0.8342034220695496 = 0.1918431967496872 + 0.1 * 6.423602104187012
Epoch 340, val loss: 0.6642701029777527
Epoch 350, training loss: 0.8133509755134583 = 0.17240647971630096 + 0.1 * 6.409444808959961
Epoch 350, val loss: 0.6620440483093262
Epoch 360, training loss: 0.7956650257110596 = 0.15515045821666718 + 0.1 * 6.405145645141602
Epoch 360, val loss: 0.661918580532074
Epoch 370, training loss: 0.7793290615081787 = 0.1398974359035492 + 0.1 * 6.394316673278809
Epoch 370, val loss: 0.6637778282165527
Epoch 380, training loss: 0.7647857069969177 = 0.12640094757080078 + 0.1 * 6.383847713470459
Epoch 380, val loss: 0.6673396229743958
Epoch 390, training loss: 0.7515316605567932 = 0.11445368826389313 + 0.1 * 6.370779991149902
Epoch 390, val loss: 0.6723445653915405
Epoch 400, training loss: 0.7404888868331909 = 0.10388311743736267 + 0.1 * 6.366057872772217
Epoch 400, val loss: 0.67851722240448
Epoch 410, training loss: 0.729911744594574 = 0.09453512728214264 + 0.1 * 6.353765964508057
Epoch 410, val loss: 0.6855508685112
Epoch 420, training loss: 0.721980631351471 = 0.08624222129583359 + 0.1 * 6.357384204864502
Epoch 420, val loss: 0.6933166980743408
Epoch 430, training loss: 0.7128157615661621 = 0.07888659834861755 + 0.1 * 6.339291095733643
Epoch 430, val loss: 0.7015846371650696
Epoch 440, training loss: 0.7053185701370239 = 0.07232525944709778 + 0.1 * 6.329933166503906
Epoch 440, val loss: 0.7102788686752319
Epoch 450, training loss: 0.7039139270782471 = 0.06645257771015167 + 0.1 * 6.374613285064697
Epoch 450, val loss: 0.7192246317863464
Epoch 460, training loss: 0.6937457919120789 = 0.0612294040620327 + 0.1 * 6.325163841247559
Epoch 460, val loss: 0.7282879948616028
Epoch 470, training loss: 0.6886430382728577 = 0.05655372887849808 + 0.1 * 6.320892810821533
Epoch 470, val loss: 0.7374029755592346
Epoch 480, training loss: 0.6836139559745789 = 0.052341707050800323 + 0.1 * 6.312722682952881
Epoch 480, val loss: 0.7465749979019165
Epoch 490, training loss: 0.6787389516830444 = 0.0485469326376915 + 0.1 * 6.301920413970947
Epoch 490, val loss: 0.7557521462440491
Epoch 500, training loss: 0.6759679913520813 = 0.04512106254696846 + 0.1 * 6.308468818664551
Epoch 500, val loss: 0.764895498752594
Epoch 510, training loss: 0.6732557415962219 = 0.042028918862342834 + 0.1 * 6.312268257141113
Epoch 510, val loss: 0.773930549621582
Epoch 520, training loss: 0.6691038012504578 = 0.03923629969358444 + 0.1 * 6.298674583435059
Epoch 520, val loss: 0.782809853553772
Epoch 530, training loss: 0.665579617023468 = 0.03670046851038933 + 0.1 * 6.288791179656982
Epoch 530, val loss: 0.7915928363800049
Epoch 540, training loss: 0.6629431247711182 = 0.034394245594739914 + 0.1 * 6.285488605499268
Epoch 540, val loss: 0.8002260327339172
Epoch 550, training loss: 0.6621492505073547 = 0.0322965532541275 + 0.1 * 6.298526763916016
Epoch 550, val loss: 0.8086705207824707
Epoch 560, training loss: 0.6576517820358276 = 0.030382631346583366 + 0.1 * 6.272691249847412
Epoch 560, val loss: 0.816947877407074
Epoch 570, training loss: 0.6578108072280884 = 0.02863079495728016 + 0.1 * 6.291800022125244
Epoch 570, val loss: 0.8250762820243835
Epoch 580, training loss: 0.6546648740768433 = 0.02703089267015457 + 0.1 * 6.276339530944824
Epoch 580, val loss: 0.8329297304153442
Epoch 590, training loss: 0.6522164940834045 = 0.025563392788171768 + 0.1 * 6.266530513763428
Epoch 590, val loss: 0.8406375646591187
Epoch 600, training loss: 0.651365339756012 = 0.024212423712015152 + 0.1 * 6.271529197692871
Epoch 600, val loss: 0.8481427431106567
Epoch 610, training loss: 0.6481406092643738 = 0.02296614646911621 + 0.1 * 6.251744747161865
Epoch 610, val loss: 0.8555004596710205
Epoch 620, training loss: 0.6485867500305176 = 0.02181263267993927 + 0.1 * 6.2677412033081055
Epoch 620, val loss: 0.8627400398254395
Epoch 630, training loss: 0.6465253233909607 = 0.02074822224676609 + 0.1 * 6.257771015167236
Epoch 630, val loss: 0.8697553873062134
Epoch 640, training loss: 0.644504189491272 = 0.01976415514945984 + 0.1 * 6.247400283813477
Epoch 640, val loss: 0.8766050934791565
Epoch 650, training loss: 0.643182635307312 = 0.018849290907382965 + 0.1 * 6.243332862854004
Epoch 650, val loss: 0.8833310008049011
Epoch 660, training loss: 0.6433314085006714 = 0.017997540533542633 + 0.1 * 6.25333833694458
Epoch 660, val loss: 0.8898944854736328
Epoch 670, training loss: 0.6413102746009827 = 0.017207445576786995 + 0.1 * 6.241028308868408
Epoch 670, val loss: 0.896286129951477
Epoch 680, training loss: 0.6411242485046387 = 0.01646907813847065 + 0.1 * 6.246551990509033
Epoch 680, val loss: 0.9026009440422058
Epoch 690, training loss: 0.6396687626838684 = 0.01577981933951378 + 0.1 * 6.238889694213867
Epoch 690, val loss: 0.9087290167808533
Epoch 700, training loss: 0.6381540894508362 = 0.015134947374463081 + 0.1 * 6.230191707611084
Epoch 700, val loss: 0.9147391319274902
Epoch 710, training loss: 0.6390036344528198 = 0.014529617503285408 + 0.1 * 6.244740009307861
Epoch 710, val loss: 0.9206449389457703
Epoch 720, training loss: 0.6371932625770569 = 0.013963586650788784 + 0.1 * 6.232296943664551
Epoch 720, val loss: 0.9264035224914551
Epoch 730, training loss: 0.6359730958938599 = 0.013430784456431866 + 0.1 * 6.2254228591918945
Epoch 730, val loss: 0.9320834875106812
Epoch 740, training loss: 0.6375972628593445 = 0.012929205782711506 + 0.1 * 6.24668025970459
Epoch 740, val loss: 0.9376347661018372
Epoch 750, training loss: 0.6346803307533264 = 0.012456558644771576 + 0.1 * 6.222238063812256
Epoch 750, val loss: 0.9431016445159912
Epoch 760, training loss: 0.633723795413971 = 0.012012118473649025 + 0.1 * 6.217116832733154
Epoch 760, val loss: 0.9484561681747437
Epoch 770, training loss: 0.6332587599754333 = 0.011591623537242413 + 0.1 * 6.216671466827393
Epoch 770, val loss: 0.9536876678466797
Epoch 780, training loss: 0.6323069930076599 = 0.011195775121450424 + 0.1 * 6.211112022399902
Epoch 780, val loss: 0.9588075876235962
Epoch 790, training loss: 0.6320174336433411 = 0.010820516385138035 + 0.1 * 6.211968898773193
Epoch 790, val loss: 0.9638599753379822
Epoch 800, training loss: 0.6330054998397827 = 0.010465795174241066 + 0.1 * 6.225397109985352
Epoch 800, val loss: 0.96880042552948
Epoch 810, training loss: 0.630424439907074 = 0.010129122994840145 + 0.1 * 6.202953338623047
Epoch 810, val loss: 0.9736373424530029
Epoch 820, training loss: 0.6307732462882996 = 0.00980964582413435 + 0.1 * 6.2096357345581055
Epoch 820, val loss: 0.9784210324287415
Epoch 830, training loss: 0.6289523839950562 = 0.009505711495876312 + 0.1 * 6.194467067718506
Epoch 830, val loss: 0.9831210374832153
Epoch 840, training loss: 0.6308001279830933 = 0.009216071106493473 + 0.1 * 6.2158403396606445
Epoch 840, val loss: 0.9877665042877197
Epoch 850, training loss: 0.6287718415260315 = 0.008941864594817162 + 0.1 * 6.198299407958984
Epoch 850, val loss: 0.9922857284545898
Epoch 860, training loss: 0.629244327545166 = 0.008681301958858967 + 0.1 * 6.205630302429199
Epoch 860, val loss: 0.9967392683029175
Epoch 870, training loss: 0.6275960803031921 = 0.008431975729763508 + 0.1 * 6.191640853881836
Epoch 870, val loss: 1.001092791557312
Epoch 880, training loss: 0.6270846128463745 = 0.00819501280784607 + 0.1 * 6.188896179199219
Epoch 880, val loss: 1.0053677558898926
Epoch 890, training loss: 0.6287753582000732 = 0.007968089543282986 + 0.1 * 6.208072662353516
Epoch 890, val loss: 1.0096077919006348
Epoch 900, training loss: 0.6266263723373413 = 0.0077517651952803135 + 0.1 * 6.188745975494385
Epoch 900, val loss: 1.0137557983398438
Epoch 910, training loss: 0.6268124580383301 = 0.007544803898781538 + 0.1 * 6.192676067352295
Epoch 910, val loss: 1.017866849899292
Epoch 920, training loss: 0.6257452964782715 = 0.007346435450017452 + 0.1 * 6.183988571166992
Epoch 920, val loss: 1.0218908786773682
Epoch 930, training loss: 0.625756561756134 = 0.007156698498874903 + 0.1 * 6.185998916625977
Epoch 930, val loss: 1.025863766670227
Epoch 940, training loss: 0.625104546546936 = 0.006974642165005207 + 0.1 * 6.181299209594727
Epoch 940, val loss: 1.0298036336898804
Epoch 950, training loss: 0.6251189708709717 = 0.006799515802413225 + 0.1 * 6.183194160461426
Epoch 950, val loss: 1.033661961555481
Epoch 960, training loss: 0.6251470446586609 = 0.00663255387917161 + 0.1 * 6.185144424438477
Epoch 960, val loss: 1.0374504327774048
Epoch 970, training loss: 0.6244849562644958 = 0.006472817622125149 + 0.1 * 6.180120944976807
Epoch 970, val loss: 1.0411665439605713
Epoch 980, training loss: 0.6246821880340576 = 0.00631890632212162 + 0.1 * 6.1836323738098145
Epoch 980, val loss: 1.0448545217514038
Epoch 990, training loss: 0.6239584684371948 = 0.006171143148094416 + 0.1 * 6.177873134613037
Epoch 990, val loss: 1.0484583377838135
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.75769, 0.14896, Accuracy:0.81481, 0.01386
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9564])
updated graph: torch.Size([2, 10644])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00758, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7799084186553955 = 1.942518711090088 + 0.1 * 8.373896598815918
Epoch 0, val loss: 1.9424501657485962
Epoch 10, training loss: 2.7700445652008057 = 1.932664155960083 + 0.1 * 8.37380313873291
Epoch 10, val loss: 1.9333531856536865
Epoch 20, training loss: 2.7576546669006348 = 1.920316219329834 + 0.1 * 8.373384475708008
Epoch 20, val loss: 1.921563982963562
Epoch 30, training loss: 2.739812135696411 = 1.9027605056762695 + 0.1 * 8.370515823364258
Epoch 30, val loss: 1.9045785665512085
Epoch 40, training loss: 2.711601972579956 = 1.8766593933105469 + 0.1 * 8.34942626953125
Epoch 40, val loss: 1.8794188499450684
Epoch 50, training loss: 2.662740468978882 = 1.8399971723556519 + 0.1 * 8.227432250976562
Epoch 50, val loss: 1.8452730178833008
Epoch 60, training loss: 2.5867462158203125 = 1.797405481338501 + 0.1 * 7.893407821655273
Epoch 60, val loss: 1.8077479600906372
Epoch 70, training loss: 2.513416051864624 = 1.7549360990524292 + 0.1 * 7.584798812866211
Epoch 70, val loss: 1.77162766456604
Epoch 80, training loss: 2.4394469261169434 = 1.7080680131912231 + 0.1 * 7.313789367675781
Epoch 80, val loss: 1.7312290668487549
Epoch 90, training loss: 2.3617639541625977 = 1.6487330198287964 + 0.1 * 7.130308628082275
Epoch 90, val loss: 1.680527687072754
Epoch 100, training loss: 2.274467706680298 = 1.572887659072876 + 0.1 * 7.015799522399902
Epoch 100, val loss: 1.6168633699417114
Epoch 110, training loss: 2.174543857574463 = 1.4810383319854736 + 0.1 * 6.935055255889893
Epoch 110, val loss: 1.540879249572754
Epoch 120, training loss: 2.0671706199645996 = 1.3793355226516724 + 0.1 * 6.878350734710693
Epoch 120, val loss: 1.4579548835754395
Epoch 130, training loss: 1.959942102432251 = 1.2762624025344849 + 0.1 * 6.836796283721924
Epoch 130, val loss: 1.3758233785629272
Epoch 140, training loss: 1.8596800565719604 = 1.1798102855682373 + 0.1 * 6.798697471618652
Epoch 140, val loss: 1.2997817993164062
Epoch 150, training loss: 1.76875901222229 = 1.0914860963821411 + 0.1 * 6.772729873657227
Epoch 150, val loss: 1.2314660549163818
Epoch 160, training loss: 1.6862379312515259 = 1.0115526914596558 + 0.1 * 6.746852397918701
Epoch 160, val loss: 1.171271562576294
Epoch 170, training loss: 1.6121351718902588 = 0.9398860931396484 + 0.1 * 6.722490310668945
Epoch 170, val loss: 1.1185797452926636
Epoch 180, training loss: 1.5422916412353516 = 0.8728955984115601 + 0.1 * 6.693961143493652
Epoch 180, val loss: 1.0692410469055176
Epoch 190, training loss: 1.4772615432739258 = 0.8104676604270935 + 0.1 * 6.667938709259033
Epoch 190, val loss: 1.023242712020874
Epoch 200, training loss: 1.4169162511825562 = 0.7526988387107849 + 0.1 * 6.642174243927002
Epoch 200, val loss: 0.9807222485542297
Epoch 210, training loss: 1.3625235557556152 = 0.6993923187255859 + 0.1 * 6.631311893463135
Epoch 210, val loss: 0.942123532295227
Epoch 220, training loss: 1.3104710578918457 = 0.6511118412017822 + 0.1 * 6.593592166900635
Epoch 220, val loss: 0.908890962600708
Epoch 230, training loss: 1.2634153366088867 = 0.6058460474014282 + 0.1 * 6.575692653656006
Epoch 230, val loss: 0.879625141620636
Epoch 240, training loss: 1.2184092998504639 = 0.5623646974563599 + 0.1 * 6.560446739196777
Epoch 240, val loss: 0.8537331819534302
Epoch 250, training loss: 1.1738672256469727 = 0.5196572542190552 + 0.1 * 6.542100429534912
Epoch 250, val loss: 0.8302270174026489
Epoch 260, training loss: 1.130136489868164 = 0.4771943688392639 + 0.1 * 6.529420852661133
Epoch 260, val loss: 0.8083254098892212
Epoch 270, training loss: 1.0875730514526367 = 0.43542009592056274 + 0.1 * 6.521529674530029
Epoch 270, val loss: 0.788447380065918
Epoch 280, training loss: 1.0456407070159912 = 0.3947632312774658 + 0.1 * 6.5087738037109375
Epoch 280, val loss: 0.7711317539215088
Epoch 290, training loss: 1.0061225891113281 = 0.3561634421348572 + 0.1 * 6.499590873718262
Epoch 290, val loss: 0.7574572563171387
Epoch 300, training loss: 0.9714466333389282 = 0.320048063993454 + 0.1 * 6.5139851570129395
Epoch 300, val loss: 0.7474490404129028
Epoch 310, training loss: 0.9351961612701416 = 0.2870665490627289 + 0.1 * 6.481296062469482
Epoch 310, val loss: 0.7414149641990662
Epoch 320, training loss: 0.9044545888900757 = 0.25729015469551086 + 0.1 * 6.471643924713135
Epoch 320, val loss: 0.7391468286514282
Epoch 330, training loss: 0.8778045773506165 = 0.23081465065479279 + 0.1 * 6.4698991775512695
Epoch 330, val loss: 0.7405564188957214
Epoch 340, training loss: 0.853950023651123 = 0.2074674516916275 + 0.1 * 6.464825630187988
Epoch 340, val loss: 0.7452117800712585
Epoch 350, training loss: 0.8315178751945496 = 0.18687908351421356 + 0.1 * 6.446387767791748
Epoch 350, val loss: 0.752581775188446
Epoch 360, training loss: 0.8124480247497559 = 0.16863097250461578 + 0.1 * 6.438170433044434
Epoch 360, val loss: 0.7621487379074097
Epoch 370, training loss: 0.7960926294326782 = 0.15248796343803406 + 0.1 * 6.436046123504639
Epoch 370, val loss: 0.7733101844787598
Epoch 380, training loss: 0.7805809378623962 = 0.13818903267383575 + 0.1 * 6.423919200897217
Epoch 380, val loss: 0.7856095433235168
Epoch 390, training loss: 0.7692127227783203 = 0.12542057037353516 + 0.1 * 6.437921524047852
Epoch 390, val loss: 0.798804521560669
Epoch 400, training loss: 0.7548404932022095 = 0.11406055837869644 + 0.1 * 6.407799243927002
Epoch 400, val loss: 0.8125054836273193
Epoch 410, training loss: 0.7446774244308472 = 0.10389511287212372 + 0.1 * 6.407822608947754
Epoch 410, val loss: 0.826557457447052
Epoch 420, training loss: 0.7349109649658203 = 0.09476175904273987 + 0.1 * 6.401491641998291
Epoch 420, val loss: 0.8408918380737305
Epoch 430, training loss: 0.7260497212409973 = 0.08657203614711761 + 0.1 * 6.394776821136475
Epoch 430, val loss: 0.8553571105003357
Epoch 440, training loss: 0.7181835770606995 = 0.07922030985355377 + 0.1 * 6.389632701873779
Epoch 440, val loss: 0.8697594404220581
Epoch 450, training loss: 0.7103535532951355 = 0.07260753214359283 + 0.1 * 6.377460479736328
Epoch 450, val loss: 0.884255051612854
Epoch 460, training loss: 0.7050661444664001 = 0.06666313856840134 + 0.1 * 6.384029865264893
Epoch 460, val loss: 0.8986895680427551
Epoch 470, training loss: 0.6981691122055054 = 0.06132972612977028 + 0.1 * 6.368393898010254
Epoch 470, val loss: 0.9129036068916321
Epoch 480, training loss: 0.693727970123291 = 0.056532587856054306 + 0.1 * 6.371953964233398
Epoch 480, val loss: 0.9269149303436279
Epoch 490, training loss: 0.6889913082122803 = 0.05222136154770851 + 0.1 * 6.36769962310791
Epoch 490, val loss: 0.9406097531318665
Epoch 500, training loss: 0.6855682730674744 = 0.04835197702050209 + 0.1 * 6.372162342071533
Epoch 500, val loss: 0.9539410471916199
Epoch 510, training loss: 0.6798573732376099 = 0.044866472482681274 + 0.1 * 6.34990930557251
Epoch 510, val loss: 0.9669871926307678
Epoch 520, training loss: 0.6771129369735718 = 0.04171447083353996 + 0.1 * 6.353984355926514
Epoch 520, val loss: 0.9796831011772156
Epoch 530, training loss: 0.6740828156471252 = 0.03886900842189789 + 0.1 * 6.352137565612793
Epoch 530, val loss: 0.9921026229858398
Epoch 540, training loss: 0.6699991822242737 = 0.036291785538196564 + 0.1 * 6.337073802947998
Epoch 540, val loss: 1.0041447877883911
Epoch 550, training loss: 0.6677757501602173 = 0.03395155444741249 + 0.1 * 6.3382415771484375
Epoch 550, val loss: 1.0159225463867188
Epoch 560, training loss: 0.6640759706497192 = 0.03182508423924446 + 0.1 * 6.322508811950684
Epoch 560, val loss: 1.0273330211639404
Epoch 570, training loss: 0.6616608500480652 = 0.029889585450291634 + 0.1 * 6.317712306976318
Epoch 570, val loss: 1.038431167602539
Epoch 580, training loss: 0.6600662469863892 = 0.028123021125793457 + 0.1 * 6.319432258605957
Epoch 580, val loss: 1.0492440462112427
Epoch 590, training loss: 0.6603179574012756 = 0.026508573442697525 + 0.1 * 6.3380937576293945
Epoch 590, val loss: 1.0596596002578735
Epoch 600, training loss: 0.6561493873596191 = 0.025032339617609978 + 0.1 * 6.31117057800293
Epoch 600, val loss: 1.0698692798614502
Epoch 610, training loss: 0.6546400189399719 = 0.023674873635172844 + 0.1 * 6.309651851654053
Epoch 610, val loss: 1.0797739028930664
Epoch 620, training loss: 0.6526453495025635 = 0.022427231073379517 + 0.1 * 6.302180767059326
Epoch 620, val loss: 1.0894019603729248
Epoch 630, training loss: 0.6514683961868286 = 0.021276457235217094 + 0.1 * 6.301918983459473
Epoch 630, val loss: 1.098789095878601
Epoch 640, training loss: 0.6514613628387451 = 0.020213458687067032 + 0.1 * 6.312478542327881
Epoch 640, val loss: 1.1079692840576172
Epoch 650, training loss: 0.6506933569908142 = 0.019229404628276825 + 0.1 * 6.314639568328857
Epoch 650, val loss: 1.116803526878357
Epoch 660, training loss: 0.6475691795349121 = 0.0183218065649271 + 0.1 * 6.292473793029785
Epoch 660, val loss: 1.125446081161499
Epoch 670, training loss: 0.6450292468070984 = 0.01747865229845047 + 0.1 * 6.275506019592285
Epoch 670, val loss: 1.1339187622070312
Epoch 680, training loss: 0.6456599831581116 = 0.016692066565155983 + 0.1 * 6.289679050445557
Epoch 680, val loss: 1.1421518325805664
Epoch 690, training loss: 0.644024133682251 = 0.015960875898599625 + 0.1 * 6.280632495880127
Epoch 690, val loss: 1.150166630744934
Epoch 700, training loss: 0.6425632834434509 = 0.015279836021363735 + 0.1 * 6.272834300994873
Epoch 700, val loss: 1.1579842567443848
Epoch 710, training loss: 0.64337557554245 = 0.014642972499132156 + 0.1 * 6.287326335906982
Epoch 710, val loss: 1.165667176246643
Epoch 720, training loss: 0.6407132744789124 = 0.014046557247638702 + 0.1 * 6.266667366027832
Epoch 720, val loss: 1.1730927228927612
Epoch 730, training loss: 0.6404408812522888 = 0.01348849292844534 + 0.1 * 6.269524097442627
Epoch 730, val loss: 1.1804742813110352
Epoch 740, training loss: 0.6386526823043823 = 0.012963438406586647 + 0.1 * 6.256892681121826
Epoch 740, val loss: 1.1875944137573242
Epoch 750, training loss: 0.6386154294013977 = 0.012471293099224567 + 0.1 * 6.261440753936768
Epoch 750, val loss: 1.1945677995681763
Epoch 760, training loss: 0.6382644176483154 = 0.012008721008896828 + 0.1 * 6.262556552886963
Epoch 760, val loss: 1.2013682126998901
Epoch 770, training loss: 0.6374118328094482 = 0.01157339382916689 + 0.1 * 6.258384704589844
Epoch 770, val loss: 1.2081222534179688
Epoch 780, training loss: 0.6361995935440063 = 0.01116194762289524 + 0.1 * 6.2503767013549805
Epoch 780, val loss: 1.214635968208313
Epoch 790, training loss: 0.6357649564743042 = 0.010773691348731518 + 0.1 * 6.249912261962891
Epoch 790, val loss: 1.2210584878921509
Epoch 800, training loss: 0.6362049579620361 = 0.01040716003626585 + 0.1 * 6.2579779624938965
Epoch 800, val loss: 1.2273499965667725
Epoch 810, training loss: 0.6344226002693176 = 0.010060104541480541 + 0.1 * 6.243624687194824
Epoch 810, val loss: 1.233555555343628
Epoch 820, training loss: 0.6344403028488159 = 0.009731471538543701 + 0.1 * 6.2470879554748535
Epoch 820, val loss: 1.2396143674850464
Epoch 830, training loss: 0.6342319250106812 = 0.009419389069080353 + 0.1 * 6.2481255531311035
Epoch 830, val loss: 1.2455109357833862
Epoch 840, training loss: 0.6327118873596191 = 0.009124395437538624 + 0.1 * 6.235874652862549
Epoch 840, val loss: 1.2513271570205688
Epoch 850, training loss: 0.6322689056396484 = 0.008844214491546154 + 0.1 * 6.234246730804443
Epoch 850, val loss: 1.2570527791976929
Epoch 860, training loss: 0.6327971816062927 = 0.008577590808272362 + 0.1 * 6.242196083068848
Epoch 860, val loss: 1.2626101970672607
Epoch 870, training loss: 0.6316683292388916 = 0.008323648013174534 + 0.1 * 6.2334465980529785
Epoch 870, val loss: 1.2680648565292358
Epoch 880, training loss: 0.632028341293335 = 0.008082184940576553 + 0.1 * 6.2394609451293945
Epoch 880, val loss: 1.27340829372406
Epoch 890, training loss: 0.6312246322631836 = 0.007852393202483654 + 0.1 * 6.23372220993042
Epoch 890, val loss: 1.2786922454833984
Epoch 900, training loss: 0.6304659247398376 = 0.007632937747985125 + 0.1 * 6.228330135345459
Epoch 900, val loss: 1.283847689628601
Epoch 910, training loss: 0.6308407187461853 = 0.007423183415085077 + 0.1 * 6.234175682067871
Epoch 910, val loss: 1.2888497114181519
Epoch 920, training loss: 0.6298201084136963 = 0.007223357446491718 + 0.1 * 6.2259674072265625
Epoch 920, val loss: 1.2938379049301147
Epoch 930, training loss: 0.6288447380065918 = 0.007031727582216263 + 0.1 * 6.218129634857178
Epoch 930, val loss: 1.2987309694290161
Epoch 940, training loss: 0.6291451454162598 = 0.006848411168903112 + 0.1 * 6.222967624664307
Epoch 940, val loss: 1.3035012483596802
Epoch 950, training loss: 0.6279715895652771 = 0.006673247553408146 + 0.1 * 6.212983131408691
Epoch 950, val loss: 1.308167576789856
Epoch 960, training loss: 0.6275436282157898 = 0.006505331490188837 + 0.1 * 6.21038293838501
Epoch 960, val loss: 1.312807321548462
Epoch 970, training loss: 0.6305145621299744 = 0.006344563793390989 + 0.1 * 6.241700172424316
Epoch 970, val loss: 1.3173884153366089
Epoch 980, training loss: 0.6275941133499146 = 0.006189996376633644 + 0.1 * 6.214040756225586
Epoch 980, val loss: 1.3217713832855225
Epoch 990, training loss: 0.6277557611465454 = 0.0060424297116696835 + 0.1 * 6.217133045196533
Epoch 990, val loss: 1.3261432647705078
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.4945
Flip ASR: 0.3956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.768786907196045 = 1.9313982725143433 + 0.1 * 8.373885154724121
Epoch 0, val loss: 1.923780918121338
Epoch 10, training loss: 2.7594709396362305 = 1.9220956563949585 + 0.1 * 8.373753547668457
Epoch 10, val loss: 1.9145883321762085
Epoch 20, training loss: 2.747892379760742 = 1.9105814695358276 + 0.1 * 8.373108863830566
Epoch 20, val loss: 1.9027906656265259
Epoch 30, training loss: 2.7312135696411133 = 1.894353985786438 + 0.1 * 8.368595123291016
Epoch 30, val loss: 1.885925531387329
Epoch 40, training loss: 2.7042088508605957 = 1.8704173564910889 + 0.1 * 8.33791446685791
Epoch 40, val loss: 1.8612158298492432
Epoch 50, training loss: 2.651644468307495 = 1.8377861976623535 + 0.1 * 8.138582229614258
Epoch 50, val loss: 1.8294305801391602
Epoch 60, training loss: 2.5702736377716064 = 1.8022704124450684 + 0.1 * 7.680031776428223
Epoch 60, val loss: 1.7977848052978516
Epoch 70, training loss: 2.4888346195220947 = 1.7697921991348267 + 0.1 * 7.190424919128418
Epoch 70, val loss: 1.7696958780288696
Epoch 80, training loss: 2.4271299839019775 = 1.7337788343429565 + 0.1 * 6.933510780334473
Epoch 80, val loss: 1.7381656169891357
Epoch 90, training loss: 2.3690690994262695 = 1.6859159469604492 + 0.1 * 6.831532001495361
Epoch 90, val loss: 1.696605920791626
Epoch 100, training loss: 2.2985424995422363 = 1.6208415031433105 + 0.1 * 6.7770094871521
Epoch 100, val loss: 1.6416054964065552
Epoch 110, training loss: 2.211869239807129 = 1.5371227264404297 + 0.1 * 6.747465133666992
Epoch 110, val loss: 1.5716458559036255
Epoch 120, training loss: 2.1127066612243652 = 1.4400864839553833 + 0.1 * 6.726202487945557
Epoch 120, val loss: 1.4918878078460693
Epoch 130, training loss: 2.008129119873047 = 1.337397575378418 + 0.1 * 6.707315921783447
Epoch 130, val loss: 1.4092888832092285
Epoch 140, training loss: 1.9018027782440186 = 1.232917308807373 + 0.1 * 6.688855171203613
Epoch 140, val loss: 1.327636480331421
Epoch 150, training loss: 1.798046350479126 = 1.1304727792739868 + 0.1 * 6.675734996795654
Epoch 150, val loss: 1.2493983507156372
Epoch 160, training loss: 1.6986939907073975 = 1.032725214958191 + 0.1 * 6.659687519073486
Epoch 160, val loss: 1.1754815578460693
Epoch 170, training loss: 1.6059925556182861 = 0.9405335187911987 + 0.1 * 6.654590606689453
Epoch 170, val loss: 1.106353759765625
Epoch 180, training loss: 1.5189629793167114 = 0.855262279510498 + 0.1 * 6.637006759643555
Epoch 180, val loss: 1.0437417030334473
Epoch 190, training loss: 1.4385838508605957 = 0.7756437659263611 + 0.1 * 6.629400730133057
Epoch 190, val loss: 0.9866108298301697
Epoch 200, training loss: 1.3641648292541504 = 0.7020979523658752 + 0.1 * 6.620667934417725
Epoch 200, val loss: 0.9356657266616821
Epoch 210, training loss: 1.2955988645553589 = 0.6345482468605042 + 0.1 * 6.610506057739258
Epoch 210, val loss: 0.8911465406417847
Epoch 220, training loss: 1.2344257831573486 = 0.5738619565963745 + 0.1 * 6.605638027191162
Epoch 220, val loss: 0.8541483283042908
Epoch 230, training loss: 1.179337978363037 = 0.51978600025177 + 0.1 * 6.595519542694092
Epoch 230, val loss: 0.8244032263755798
Epoch 240, training loss: 1.1300843954086304 = 0.47134706377983093 + 0.1 * 6.58737325668335
Epoch 240, val loss: 0.800919234752655
Epoch 250, training loss: 1.0859484672546387 = 0.42783981561660767 + 0.1 * 6.5810866355896
Epoch 250, val loss: 0.7821948528289795
Epoch 260, training loss: 1.045405626296997 = 0.3880805969238281 + 0.1 * 6.573250770568848
Epoch 260, val loss: 0.767004668712616
Epoch 270, training loss: 1.0080665349960327 = 0.3513171374797821 + 0.1 * 6.567493915557861
Epoch 270, val loss: 0.7544589638710022
Epoch 280, training loss: 0.9743351340293884 = 0.3173690438270569 + 0.1 * 6.569660663604736
Epoch 280, val loss: 0.7442576885223389
Epoch 290, training loss: 0.9413807988166809 = 0.28616219758987427 + 0.1 * 6.552186012268066
Epoch 290, val loss: 0.7358419895172119
Epoch 300, training loss: 0.9121628999710083 = 0.2573666274547577 + 0.1 * 6.547962188720703
Epoch 300, val loss: 0.7292680144309998
Epoch 310, training loss: 0.8856643438339233 = 0.23110058903694153 + 0.1 * 6.545637607574463
Epoch 310, val loss: 0.7245500683784485
Epoch 320, training loss: 0.8603417873382568 = 0.20724041759967804 + 0.1 * 6.531013488769531
Epoch 320, val loss: 0.7217821478843689
Epoch 330, training loss: 0.83824622631073 = 0.18563047051429749 + 0.1 * 6.526157379150391
Epoch 330, val loss: 0.7209699749946594
Epoch 340, training loss: 0.8183289170265198 = 0.16630017757415771 + 0.1 * 6.520287036895752
Epoch 340, val loss: 0.7219916582107544
Epoch 350, training loss: 0.7996497750282288 = 0.149091437458992 + 0.1 * 6.5055832862854
Epoch 350, val loss: 0.7247725129127502
Epoch 360, training loss: 0.788353681564331 = 0.13379153609275818 + 0.1 * 6.545621395111084
Epoch 360, val loss: 0.729091465473175
Epoch 370, training loss: 0.7695615887641907 = 0.12039203941822052 + 0.1 * 6.491695404052734
Epoch 370, val loss: 0.7346890568733215
Epoch 380, training loss: 0.7569700479507446 = 0.10856517404317856 + 0.1 * 6.484048843383789
Epoch 380, val loss: 0.7413203120231628
Epoch 390, training loss: 0.7452127933502197 = 0.09808823466300964 + 0.1 * 6.471245765686035
Epoch 390, val loss: 0.7488411068916321
Epoch 400, training loss: 0.7396378517150879 = 0.08880103379487991 + 0.1 * 6.508368015289307
Epoch 400, val loss: 0.7569413781166077
Epoch 410, training loss: 0.7266678810119629 = 0.08065132051706314 + 0.1 * 6.460165023803711
Epoch 410, val loss: 0.7652944326400757
Epoch 420, training loss: 0.7182657718658447 = 0.07343839108943939 + 0.1 * 6.448273658752441
Epoch 420, val loss: 0.7739386558532715
Epoch 430, training loss: 0.7106338143348694 = 0.0670250728726387 + 0.1 * 6.436087131500244
Epoch 430, val loss: 0.7827839851379395
Epoch 440, training loss: 0.7074373364448547 = 0.06131623685359955 + 0.1 * 6.46121072769165
Epoch 440, val loss: 0.7917104363441467
Epoch 450, training loss: 0.699359655380249 = 0.05626785382628441 + 0.1 * 6.430917739868164
Epoch 450, val loss: 0.8005067110061646
Epoch 460, training loss: 0.6936031579971313 = 0.05176988244056702 + 0.1 * 6.418332576751709
Epoch 460, val loss: 0.8092751502990723
Epoch 470, training loss: 0.6887239217758179 = 0.047749001532793045 + 0.1 * 6.409749507904053
Epoch 470, val loss: 0.817974328994751
Epoch 480, training loss: 0.6859315633773804 = 0.044151660054922104 + 0.1 * 6.41779899597168
Epoch 480, val loss: 0.8265472054481506
Epoch 490, training loss: 0.6804451942443848 = 0.040940411388874054 + 0.1 * 6.395047664642334
Epoch 490, val loss: 0.834955632686615
Epoch 500, training loss: 0.6769587397575378 = 0.038053352385759354 + 0.1 * 6.389053821563721
Epoch 500, val loss: 0.8432810306549072
Epoch 510, training loss: 0.6746233105659485 = 0.035447847098112106 + 0.1 * 6.391754627227783
Epoch 510, val loss: 0.8514432311058044
Epoch 520, training loss: 0.671850323677063 = 0.03309822827577591 + 0.1 * 6.387520790100098
Epoch 520, val loss: 0.8594571352005005
Epoch 530, training loss: 0.6680993437767029 = 0.030977332964539528 + 0.1 * 6.371220111846924
Epoch 530, val loss: 0.8672975897789001
Epoch 540, training loss: 0.6661146879196167 = 0.029050955548882484 + 0.1 * 6.3706374168396
Epoch 540, val loss: 0.8750418424606323
Epoch 550, training loss: 0.6640713810920715 = 0.02729738876223564 + 0.1 * 6.367739677429199
Epoch 550, val loss: 0.8826351165771484
Epoch 560, training loss: 0.6620527505874634 = 0.025699689984321594 + 0.1 * 6.36353063583374
Epoch 560, val loss: 0.8900665044784546
Epoch 570, training loss: 0.6595064401626587 = 0.02424364909529686 + 0.1 * 6.352627754211426
Epoch 570, val loss: 0.8972876071929932
Epoch 580, training loss: 0.657433807849884 = 0.0229111947119236 + 0.1 * 6.345226287841797
Epoch 580, val loss: 0.9044377207756042
Epoch 590, training loss: 0.6566729545593262 = 0.0216855239123106 + 0.1 * 6.349874019622803
Epoch 590, val loss: 0.9114376306533813
Epoch 600, training loss: 0.6565744280815125 = 0.020558839663863182 + 0.1 * 6.360156059265137
Epoch 600, val loss: 0.9183008074760437
Epoch 610, training loss: 0.6533480882644653 = 0.019523561000823975 + 0.1 * 6.338245391845703
Epoch 610, val loss: 0.9249396920204163
Epoch 620, training loss: 0.6512060165405273 = 0.018568314611911774 + 0.1 * 6.326376438140869
Epoch 620, val loss: 0.9315347075462341
Epoch 630, training loss: 0.6508392691612244 = 0.01768277771770954 + 0.1 * 6.331564903259277
Epoch 630, val loss: 0.9379847049713135
Epoch 640, training loss: 0.6493529677391052 = 0.016860973089933395 + 0.1 * 6.324919700622559
Epoch 640, val loss: 0.9443196654319763
Epoch 650, training loss: 0.6475589275360107 = 0.016098380088806152 + 0.1 * 6.314605236053467
Epoch 650, val loss: 0.9504666924476624
Epoch 660, training loss: 0.646149218082428 = 0.015389805659651756 + 0.1 * 6.307593822479248
Epoch 660, val loss: 0.9565743803977966
Epoch 670, training loss: 0.6472235321998596 = 0.014728500507771969 + 0.1 * 6.324950218200684
Epoch 670, val loss: 0.9625254273414612
Epoch 680, training loss: 0.6450393199920654 = 0.014112286269664764 + 0.1 * 6.30927038192749
Epoch 680, val loss: 0.9683363437652588
Epoch 690, training loss: 0.6448333263397217 = 0.01353677548468113 + 0.1 * 6.3129658699035645
Epoch 690, val loss: 0.9740555286407471
Epoch 700, training loss: 0.6435078978538513 = 0.012998093850910664 + 0.1 * 6.305098056793213
Epoch 700, val loss: 0.9796333312988281
Epoch 710, training loss: 0.6421599388122559 = 0.012493054382503033 + 0.1 * 6.296668529510498
Epoch 710, val loss: 0.9851548671722412
Epoch 720, training loss: 0.6420642733573914 = 0.012017941102385521 + 0.1 * 6.30046272277832
Epoch 720, val loss: 0.9904979467391968
Epoch 730, training loss: 0.6412001252174377 = 0.011572153307497501 + 0.1 * 6.296279430389404
Epoch 730, val loss: 0.9957290291786194
Epoch 740, training loss: 0.6409841179847717 = 0.011152415536344051 + 0.1 * 6.298316955566406
Epoch 740, val loss: 1.0008976459503174
Epoch 750, training loss: 0.6391406655311584 = 0.01075703464448452 + 0.1 * 6.283836364746094
Epoch 750, val loss: 1.005961537361145
Epoch 760, training loss: 0.6403777599334717 = 0.010383926331996918 + 0.1 * 6.299938678741455
Epoch 760, val loss: 1.0109184980392456
Epoch 770, training loss: 0.6379314064979553 = 0.010031535290181637 + 0.1 * 6.278998851776123
Epoch 770, val loss: 1.0157469511032104
Epoch 780, training loss: 0.6380413174629211 = 0.009698815643787384 + 0.1 * 6.2834248542785645
Epoch 780, val loss: 1.020540714263916
Epoch 790, training loss: 0.6380902528762817 = 0.00938341673463583 + 0.1 * 6.287067890167236
Epoch 790, val loss: 1.0251883268356323
Epoch 800, training loss: 0.6368492841720581 = 0.009085386991500854 + 0.1 * 6.277639389038086
Epoch 800, val loss: 1.0297032594680786
Epoch 810, training loss: 0.6356353163719177 = 0.008803113363683224 + 0.1 * 6.268321514129639
Epoch 810, val loss: 1.0341906547546387
Epoch 820, training loss: 0.6368619799613953 = 0.00853441096842289 + 0.1 * 6.283275127410889
Epoch 820, val loss: 1.0386571884155273
Epoch 830, training loss: 0.6347461342811584 = 0.008278943598270416 + 0.1 * 6.264671802520752
Epoch 830, val loss: 1.0429028272628784
Epoch 840, training loss: 0.6344239115715027 = 0.008036021143198013 + 0.1 * 6.26387882232666
Epoch 840, val loss: 1.047183871269226
Epoch 850, training loss: 0.6337160468101501 = 0.0078046321868896484 + 0.1 * 6.259113788604736
Epoch 850, val loss: 1.051317572593689
Epoch 860, training loss: 0.6334081292152405 = 0.007584578823298216 + 0.1 * 6.258235454559326
Epoch 860, val loss: 1.0554486513137817
Epoch 870, training loss: 0.6338328123092651 = 0.007374412380158901 + 0.1 * 6.264584064483643
Epoch 870, val loss: 1.0594574213027954
Epoch 880, training loss: 0.6327009201049805 = 0.007174112368375063 + 0.1 * 6.25526762008667
Epoch 880, val loss: 1.0634468793869019
Epoch 890, training loss: 0.6332201957702637 = 0.0069825430400669575 + 0.1 * 6.262376308441162
Epoch 890, val loss: 1.067388653755188
Epoch 900, training loss: 0.6321719884872437 = 0.006799313239753246 + 0.1 * 6.253726482391357
Epoch 900, val loss: 1.0711201429367065
Epoch 910, training loss: 0.6316370964050293 = 0.006624744273722172 + 0.1 * 6.250123500823975
Epoch 910, val loss: 1.0749205350875854
Epoch 920, training loss: 0.6317882537841797 = 0.0064572556875646114 + 0.1 * 6.253309726715088
Epoch 920, val loss: 1.0786410570144653
Epoch 930, training loss: 0.6319833397865295 = 0.006296934559941292 + 0.1 * 6.256864070892334
Epoch 930, val loss: 1.0822465419769287
Epoch 940, training loss: 0.6305460333824158 = 0.0061433520168066025 + 0.1 * 6.2440266609191895
Epoch 940, val loss: 1.0857948064804077
Epoch 950, training loss: 0.6299882531166077 = 0.005996135529130697 + 0.1 * 6.239921569824219
Epoch 950, val loss: 1.0893242359161377
Epoch 960, training loss: 0.629494845867157 = 0.005854388698935509 + 0.1 * 6.2364044189453125
Epoch 960, val loss: 1.0927636623382568
Epoch 970, training loss: 0.6302905678749084 = 0.005718432366847992 + 0.1 * 6.245721340179443
Epoch 970, val loss: 1.0961209535598755
Epoch 980, training loss: 0.629959225654602 = 0.005587978288531303 + 0.1 * 6.243712425231934
Epoch 980, val loss: 1.0994503498077393
Epoch 990, training loss: 0.6303483247756958 = 0.0054627880454063416 + 0.1 * 6.2488555908203125
Epoch 990, val loss: 1.1027462482452393
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8044
Flip ASR: 0.7644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.778921365737915 = 1.941536784172058 + 0.1 * 8.373845100402832
Epoch 0, val loss: 1.9334630966186523
Epoch 10, training loss: 2.7691125869750977 = 1.9317461252212524 + 0.1 * 8.373665809631348
Epoch 10, val loss: 1.9241948127746582
Epoch 20, training loss: 2.756948471069336 = 1.919697642326355 + 0.1 * 8.372509002685547
Epoch 20, val loss: 1.9125633239746094
Epoch 30, training loss: 2.7388787269592285 = 1.9024722576141357 + 0.1 * 8.364063262939453
Epoch 30, val loss: 1.895766019821167
Epoch 40, training loss: 2.7076001167297363 = 1.8765122890472412 + 0.1 * 8.310876846313477
Epoch 40, val loss: 1.870809555053711
Epoch 50, training loss: 2.6478991508483887 = 1.8409868478775024 + 0.1 * 8.069122314453125
Epoch 50, val loss: 1.8383097648620605
Epoch 60, training loss: 2.577385187149048 = 1.8007959127426147 + 0.1 * 7.765893459320068
Epoch 60, val loss: 1.8033961057662964
Epoch 70, training loss: 2.489912748336792 = 1.7607969045639038 + 0.1 * 7.2911577224731445
Epoch 70, val loss: 1.7680232524871826
Epoch 80, training loss: 2.420393705368042 = 1.7165464162826538 + 0.1 * 7.038473606109619
Epoch 80, val loss: 1.727803111076355
Epoch 90, training loss: 2.352229356765747 = 1.6618642807006836 + 0.1 * 6.903651237487793
Epoch 90, val loss: 1.6790343523025513
Epoch 100, training loss: 2.2719273567199707 = 1.5904525518417358 + 0.1 * 6.814746856689453
Epoch 100, val loss: 1.6177769899368286
Epoch 110, training loss: 2.1790928840637207 = 1.5032556056976318 + 0.1 * 6.758371829986572
Epoch 110, val loss: 1.5465713739395142
Epoch 120, training loss: 2.083334445953369 = 1.4110335111618042 + 0.1 * 6.723008632659912
Epoch 120, val loss: 1.4745765924453735
Epoch 130, training loss: 1.9921419620513916 = 1.322568655014038 + 0.1 * 6.695732116699219
Epoch 130, val loss: 1.4100818634033203
Epoch 140, training loss: 1.9066579341888428 = 1.2392942905426025 + 0.1 * 6.6736369132995605
Epoch 140, val loss: 1.3531262874603271
Epoch 150, training loss: 1.8254058361053467 = 1.1604466438293457 + 0.1 * 6.649591445922852
Epoch 150, val loss: 1.300407886505127
Epoch 160, training loss: 1.746570110321045 = 1.0832337141036987 + 0.1 * 6.633363723754883
Epoch 160, val loss: 1.2494103908538818
Epoch 170, training loss: 1.6698071956634521 = 1.0086697340011597 + 0.1 * 6.611374855041504
Epoch 170, val loss: 1.2006704807281494
Epoch 180, training loss: 1.5950415134429932 = 0.9356860518455505 + 0.1 * 6.5935540199279785
Epoch 180, val loss: 1.1534870862960815
Epoch 190, training loss: 1.5215784311294556 = 0.8640300631523132 + 0.1 * 6.575483322143555
Epoch 190, val loss: 1.1065200567245483
Epoch 200, training loss: 1.4521007537841797 = 0.7950485348701477 + 0.1 * 6.570522785186768
Epoch 200, val loss: 1.060823917388916
Epoch 210, training loss: 1.384842872619629 = 0.7300459742546082 + 0.1 * 6.547968864440918
Epoch 210, val loss: 1.018250584602356
Epoch 220, training loss: 1.3238029479980469 = 0.668888509273529 + 0.1 * 6.5491437911987305
Epoch 220, val loss: 0.9785162806510925
Epoch 230, training loss: 1.2650471925735474 = 0.6125933527946472 + 0.1 * 6.524538040161133
Epoch 230, val loss: 0.9432335495948792
Epoch 240, training loss: 1.2117748260498047 = 0.5606178045272827 + 0.1 * 6.511569976806641
Epoch 240, val loss: 0.9117772579193115
Epoch 250, training loss: 1.1632813215255737 = 0.5125447511672974 + 0.1 * 6.507365703582764
Epoch 250, val loss: 0.8843372464179993
Epoch 260, training loss: 1.1176737546920776 = 0.46851179003715515 + 0.1 * 6.491619110107422
Epoch 260, val loss: 0.8607752919197083
Epoch 270, training loss: 1.0761923789978027 = 0.42801210284233093 + 0.1 * 6.481802463531494
Epoch 270, val loss: 0.8409212827682495
Epoch 280, training loss: 1.0387831926345825 = 0.3905506432056427 + 0.1 * 6.482325077056885
Epoch 280, val loss: 0.8240752220153809
Epoch 290, training loss: 1.0022865533828735 = 0.35615018010139465 + 0.1 * 6.461363315582275
Epoch 290, val loss: 0.8101739883422852
Epoch 300, training loss: 0.970674991607666 = 0.32443365454673767 + 0.1 * 6.462413787841797
Epoch 300, val loss: 0.7987506985664368
Epoch 310, training loss: 0.940064013004303 = 0.295681357383728 + 0.1 * 6.443826198577881
Epoch 310, val loss: 0.7901964783668518
Epoch 320, training loss: 0.9130600690841675 = 0.26951006054878235 + 0.1 * 6.435499668121338
Epoch 320, val loss: 0.7838807106018066
Epoch 330, training loss: 0.8888957500457764 = 0.2456139326095581 + 0.1 * 6.4328179359436035
Epoch 330, val loss: 0.7795405983924866
Epoch 340, training loss: 0.8666948080062866 = 0.2240392565727234 + 0.1 * 6.426555633544922
Epoch 340, val loss: 0.7770881056785583
Epoch 350, training loss: 0.8455855846405029 = 0.20451860129833221 + 0.1 * 6.410669803619385
Epoch 350, val loss: 0.7763607501983643
Epoch 360, training loss: 0.8273963928222656 = 0.186674565076828 + 0.1 * 6.407217979431152
Epoch 360, val loss: 0.7770128846168518
Epoch 370, training loss: 0.8099266290664673 = 0.17036017775535583 + 0.1 * 6.395664215087891
Epoch 370, val loss: 0.7788497805595398
Epoch 380, training loss: 0.7940462231636047 = 0.15546159446239471 + 0.1 * 6.385846138000488
Epoch 380, val loss: 0.7819162607192993
Epoch 390, training loss: 0.7812150120735168 = 0.14178669452667236 + 0.1 * 6.394282817840576
Epoch 390, val loss: 0.7858188152313232
Epoch 400, training loss: 0.7670154571533203 = 0.12928850948810577 + 0.1 * 6.377269268035889
Epoch 400, val loss: 0.7903907299041748
Epoch 410, training loss: 0.7546114921569824 = 0.11776839941740036 + 0.1 * 6.3684306144714355
Epoch 410, val loss: 0.7957810759544373
Epoch 420, training loss: 0.7452499270439148 = 0.10725157707929611 + 0.1 * 6.379983425140381
Epoch 420, val loss: 0.8018293976783752
Epoch 430, training loss: 0.734545111656189 = 0.09775061160326004 + 0.1 * 6.367944717407227
Epoch 430, val loss: 0.8086727857589722
Epoch 440, training loss: 0.7251377105712891 = 0.08917644619941711 + 0.1 * 6.359611988067627
Epoch 440, val loss: 0.816106379032135
Epoch 450, training loss: 0.7161306738853455 = 0.0814739391207695 + 0.1 * 6.346567153930664
Epoch 450, val loss: 0.824059247970581
Epoch 460, training loss: 0.7092409729957581 = 0.0745457336306572 + 0.1 * 6.346952438354492
Epoch 460, val loss: 0.8325121998786926
Epoch 470, training loss: 0.7035251259803772 = 0.06832306832075119 + 0.1 * 6.352020263671875
Epoch 470, val loss: 0.841302752494812
Epoch 480, training loss: 0.6961168646812439 = 0.06274870783090591 + 0.1 * 6.333681106567383
Epoch 480, val loss: 0.8502362966537476
Epoch 490, training loss: 0.6905505061149597 = 0.057747066020965576 + 0.1 * 6.328034400939941
Epoch 490, val loss: 0.859418511390686
Epoch 500, training loss: 0.6857196092605591 = 0.05325381085276604 + 0.1 * 6.324657440185547
Epoch 500, val loss: 0.86863112449646
Epoch 510, training loss: 0.6818064451217651 = 0.04921809956431389 + 0.1 * 6.325882911682129
Epoch 510, val loss: 0.8779463768005371
Epoch 520, training loss: 0.6768420934677124 = 0.045598749071359634 + 0.1 * 6.312433242797852
Epoch 520, val loss: 0.8872315883636475
Epoch 530, training loss: 0.674107551574707 = 0.04234449937939644 + 0.1 * 6.317630290985107
Epoch 530, val loss: 0.8964663147926331
Epoch 540, training loss: 0.6697906255722046 = 0.039412107318639755 + 0.1 * 6.30378532409668
Epoch 540, val loss: 0.9056785702705383
Epoch 550, training loss: 0.66718989610672 = 0.036760903894901276 + 0.1 * 6.304289817810059
Epoch 550, val loss: 0.9147758483886719
Epoch 560, training loss: 0.6634584665298462 = 0.03436437249183655 + 0.1 * 6.290940761566162
Epoch 560, val loss: 0.9237789511680603
Epoch 570, training loss: 0.6626958250999451 = 0.032189659774303436 + 0.1 * 6.305061340332031
Epoch 570, val loss: 0.932663083076477
Epoch 580, training loss: 0.6593772172927856 = 0.03021903894841671 + 0.1 * 6.291581630706787
Epoch 580, val loss: 0.941331684589386
Epoch 590, training loss: 0.6565009355545044 = 0.028427638113498688 + 0.1 * 6.28073263168335
Epoch 590, val loss: 0.9498987197875977
Epoch 600, training loss: 0.6550253629684448 = 0.0267882589250803 + 0.1 * 6.2823710441589355
Epoch 600, val loss: 0.9583638310432434
Epoch 610, training loss: 0.6531271934509277 = 0.0252882968634367 + 0.1 * 6.278388977050781
Epoch 610, val loss: 0.9665946960449219
Epoch 620, training loss: 0.652188241481781 = 0.02391575276851654 + 0.1 * 6.282724380493164
Epoch 620, val loss: 0.97470623254776
Epoch 630, training loss: 0.6493586897850037 = 0.022655315697193146 + 0.1 * 6.267033576965332
Epoch 630, val loss: 0.9826751947402954
Epoch 640, training loss: 0.6478012800216675 = 0.021491218358278275 + 0.1 * 6.263100624084473
Epoch 640, val loss: 0.9905608296394348
Epoch 650, training loss: 0.6475673317909241 = 0.020415909588336945 + 0.1 * 6.271513938903809
Epoch 650, val loss: 0.9982635378837585
Epoch 660, training loss: 0.6458647847175598 = 0.01942543312907219 + 0.1 * 6.264393329620361
Epoch 660, val loss: 1.0057857036590576
Epoch 670, training loss: 0.6449148654937744 = 0.01850934885442257 + 0.1 * 6.264055252075195
Epoch 670, val loss: 1.0132290124893188
Epoch 680, training loss: 0.6427401900291443 = 0.017658254131674767 + 0.1 * 6.250819683074951
Epoch 680, val loss: 1.0205070972442627
Epoch 690, training loss: 0.6425211429595947 = 0.016865761950612068 + 0.1 * 6.2565531730651855
Epoch 690, val loss: 1.0276565551757812
Epoch 700, training loss: 0.6420106291770935 = 0.016128765419125557 + 0.1 * 6.25881814956665
Epoch 700, val loss: 1.0345807075500488
Epoch 710, training loss: 0.6404148936271667 = 0.01544329896569252 + 0.1 * 6.249715805053711
Epoch 710, val loss: 1.0414403676986694
Epoch 720, training loss: 0.6404065489768982 = 0.014802469871938229 + 0.1 * 6.256041049957275
Epoch 720, val loss: 1.0481607913970947
Epoch 730, training loss: 0.6384275555610657 = 0.01420365646481514 + 0.1 * 6.242238521575928
Epoch 730, val loss: 1.0546987056732178
Epoch 740, training loss: 0.6368446946144104 = 0.01364295557141304 + 0.1 * 6.2320170402526855
Epoch 740, val loss: 1.061178207397461
Epoch 750, training loss: 0.6376913189888 = 0.01311550848186016 + 0.1 * 6.245757579803467
Epoch 750, val loss: 1.0675524473190308
Epoch 760, training loss: 0.6355383992195129 = 0.012619851157069206 + 0.1 * 6.229185581207275
Epoch 760, val loss: 1.073761224746704
Epoch 770, training loss: 0.6366154551506042 = 0.012152936309576035 + 0.1 * 6.244625568389893
Epoch 770, val loss: 1.079900860786438
Epoch 780, training loss: 0.6349456906318665 = 0.011714871972799301 + 0.1 * 6.2323079109191895
Epoch 780, val loss: 1.085828423500061
Epoch 790, training loss: 0.633980929851532 = 0.01130272913724184 + 0.1 * 6.226782321929932
Epoch 790, val loss: 1.091734766960144
Epoch 800, training loss: 0.6340150833129883 = 0.010912883095443249 + 0.1 * 6.231021881103516
Epoch 800, val loss: 1.0975003242492676
Epoch 810, training loss: 0.6327540278434753 = 0.010544363409280777 + 0.1 * 6.2220964431762695
Epoch 810, val loss: 1.1032299995422363
Epoch 820, training loss: 0.6328989863395691 = 0.010195043869316578 + 0.1 * 6.227039813995361
Epoch 820, val loss: 1.1088242530822754
Epoch 830, training loss: 0.6328974366188049 = 0.00986418779939413 + 0.1 * 6.230332374572754
Epoch 830, val loss: 1.1143802404403687
Epoch 840, training loss: 0.632300078868866 = 0.009550735354423523 + 0.1 * 6.2274932861328125
Epoch 840, val loss: 1.119759440422058
Epoch 850, training loss: 0.6302751302719116 = 0.009254246018826962 + 0.1 * 6.210208892822266
Epoch 850, val loss: 1.1250371932983398
Epoch 860, training loss: 0.6302245855331421 = 0.008972244337201118 + 0.1 * 6.212523460388184
Epoch 860, val loss: 1.1303002834320068
Epoch 870, training loss: 0.6309657692909241 = 0.008703566156327724 + 0.1 * 6.222621917724609
Epoch 870, val loss: 1.1354676485061646
Epoch 880, training loss: 0.6298566460609436 = 0.008448230102658272 + 0.1 * 6.214084148406982
Epoch 880, val loss: 1.1405214071273804
Epoch 890, training loss: 0.6297672986984253 = 0.008205333724617958 + 0.1 * 6.2156195640563965
Epoch 890, val loss: 1.1454797983169556
Epoch 900, training loss: 0.6287402510643005 = 0.007974009029567242 + 0.1 * 6.207662105560303
Epoch 900, val loss: 1.150410532951355
Epoch 910, training loss: 0.627632737159729 = 0.007752648089081049 + 0.1 * 6.198800563812256
Epoch 910, val loss: 1.1552748680114746
Epoch 920, training loss: 0.6280772686004639 = 0.007540891412645578 + 0.1 * 6.205363750457764
Epoch 920, val loss: 1.1600245237350464
Epoch 930, training loss: 0.6279439926147461 = 0.007339298725128174 + 0.1 * 6.2060465812683105
Epoch 930, val loss: 1.1646499633789062
Epoch 940, training loss: 0.6275851130485535 = 0.007146881427615881 + 0.1 * 6.204381942749023
Epoch 940, val loss: 1.1692582368850708
Epoch 950, training loss: 0.6268899440765381 = 0.006962490733712912 + 0.1 * 6.19927453994751
Epoch 950, val loss: 1.173741340637207
Epoch 960, training loss: 0.626771867275238 = 0.006785768084228039 + 0.1 * 6.1998610496521
Epoch 960, val loss: 1.1782073974609375
Epoch 970, training loss: 0.6263348460197449 = 0.006616472266614437 + 0.1 * 6.197183609008789
Epoch 970, val loss: 1.1826016902923584
Epoch 980, training loss: 0.6264459490776062 = 0.0064540645107626915 + 0.1 * 6.199918746948242
Epoch 980, val loss: 1.1869268417358398
Epoch 990, training loss: 0.6251713037490845 = 0.006298273801803589 + 0.1 * 6.188730716705322
Epoch 990, val loss: 1.191225290298462
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8266
Flip ASR: 0.7911/225 nodes
The final ASR:0.70849, 0.15161, Accuracy:0.81852, 0.01210
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.97294, 0.00696, Accuracy:0.83580, 0.00761
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8041958808898926 = 1.9668145179748535 + 0.1 * 8.373814582824707
Epoch 0, val loss: 1.9721299409866333
Epoch 10, training loss: 2.7932605743408203 = 1.9559009075164795 + 0.1 * 8.37359619140625
Epoch 10, val loss: 1.9611436128616333
Epoch 20, training loss: 2.779543876647949 = 1.9423161745071411 + 0.1 * 8.372278213500977
Epoch 20, val loss: 1.947172999382019
Epoch 30, training loss: 2.7588229179382324 = 1.9226036071777344 + 0.1 * 8.36219310760498
Epoch 30, val loss: 1.9269113540649414
Epoch 40, training loss: 2.7225136756896973 = 1.8926465511322021 + 0.1 * 8.298669815063477
Epoch 40, val loss: 1.8965121507644653
Epoch 50, training loss: 2.6420350074768066 = 1.8521140813827515 + 0.1 * 7.899209022521973
Epoch 50, val loss: 1.8572120666503906
Epoch 60, training loss: 2.556955337524414 = 1.8098562955856323 + 0.1 * 7.47098970413208
Epoch 60, val loss: 1.8193893432617188
Epoch 70, training loss: 2.4811129570007324 = 1.7741633653640747 + 0.1 * 7.069494724273682
Epoch 70, val loss: 1.789810061454773
Epoch 80, training loss: 2.423588275909424 = 1.742368221282959 + 0.1 * 6.812199592590332
Epoch 80, val loss: 1.762091875076294
Epoch 90, training loss: 2.371971607208252 = 1.7019293308258057 + 0.1 * 6.7004218101501465
Epoch 90, val loss: 1.7248371839523315
Epoch 100, training loss: 2.3128089904785156 = 1.6478142738342285 + 0.1 * 6.649947166442871
Epoch 100, val loss: 1.67775559425354
Epoch 110, training loss: 2.238724708557129 = 1.576904535293579 + 0.1 * 6.6182026863098145
Epoch 110, val loss: 1.6185599565505981
Epoch 120, training loss: 2.149604320526123 = 1.48970627784729 + 0.1 * 6.5989789962768555
Epoch 120, val loss: 1.5464156866073608
Epoch 130, training loss: 2.049832344055176 = 1.391412615776062 + 0.1 * 6.584198474884033
Epoch 130, val loss: 1.4649779796600342
Epoch 140, training loss: 1.9437391757965088 = 1.2867666482925415 + 0.1 * 6.569725513458252
Epoch 140, val loss: 1.3780968189239502
Epoch 150, training loss: 1.8329780101776123 = 1.1778037548065186 + 0.1 * 6.5517425537109375
Epoch 150, val loss: 1.288682222366333
Epoch 160, training loss: 1.7195978164672852 = 1.0661698579788208 + 0.1 * 6.534280300140381
Epoch 160, val loss: 1.197867751121521
Epoch 170, training loss: 1.611506462097168 = 0.9590227007865906 + 0.1 * 6.524837493896484
Epoch 170, val loss: 1.112317442893982
Epoch 180, training loss: 1.510490894317627 = 0.8595989346504211 + 0.1 * 6.5089192390441895
Epoch 180, val loss: 1.0332422256469727
Epoch 190, training loss: 1.4197721481323242 = 0.7697827219963074 + 0.1 * 6.499894618988037
Epoch 190, val loss: 0.9629583358764648
Epoch 200, training loss: 1.3406426906585693 = 0.6915845274925232 + 0.1 * 6.49058198928833
Epoch 200, val loss: 0.9035831689834595
Epoch 210, training loss: 1.2728668451309204 = 0.6247486472129822 + 0.1 * 6.481182098388672
Epoch 210, val loss: 0.8552318215370178
Epoch 220, training loss: 1.2159695625305176 = 0.5681025385856628 + 0.1 * 6.4786696434021
Epoch 220, val loss: 0.8171239495277405
Epoch 230, training loss: 1.166907548904419 = 0.5203593969345093 + 0.1 * 6.465481758117676
Epoch 230, val loss: 0.7881291508674622
Epoch 240, training loss: 1.1245242357254028 = 0.4787976145744324 + 0.1 * 6.457265853881836
Epoch 240, val loss: 0.7653278708457947
Epoch 250, training loss: 1.0879387855529785 = 0.44164445996284485 + 0.1 * 6.4629435539245605
Epoch 250, val loss: 0.7474144101142883
Epoch 260, training loss: 1.0519849061965942 = 0.4074084162712097 + 0.1 * 6.445765018463135
Epoch 260, val loss: 0.7327007055282593
Epoch 270, training loss: 1.0176215171813965 = 0.37417924404144287 + 0.1 * 6.434422016143799
Epoch 270, val loss: 0.7200093865394592
Epoch 280, training loss: 0.983940601348877 = 0.3413321077823639 + 0.1 * 6.426084995269775
Epoch 280, val loss: 0.7088162302970886
Epoch 290, training loss: 0.9506757259368896 = 0.30890992283821106 + 0.1 * 6.417657852172852
Epoch 290, val loss: 0.6993270516395569
Epoch 300, training loss: 0.9182795286178589 = 0.27723756432533264 + 0.1 * 6.410419464111328
Epoch 300, val loss: 0.6917030215263367
Epoch 310, training loss: 0.8882172107696533 = 0.2474055290222168 + 0.1 * 6.408116817474365
Epoch 310, val loss: 0.6862291693687439
Epoch 320, training loss: 0.8593708276748657 = 0.22023776173591614 + 0.1 * 6.391330242156982
Epoch 320, val loss: 0.6830376982688904
Epoch 330, training loss: 0.8354133367538452 = 0.19610346853733063 + 0.1 * 6.393098831176758
Epoch 330, val loss: 0.6820212602615356
Epoch 340, training loss: 0.8129795789718628 = 0.1752382218837738 + 0.1 * 6.377413749694824
Epoch 340, val loss: 0.6830886602401733
Epoch 350, training loss: 0.7937675714492798 = 0.15729308128356934 + 0.1 * 6.364744663238525
Epoch 350, val loss: 0.6860069632530212
Epoch 360, training loss: 0.7776490449905396 = 0.14186279475688934 + 0.1 * 6.35786247253418
Epoch 360, val loss: 0.6904187798500061
Epoch 370, training loss: 0.7637104988098145 = 0.12864932417869568 + 0.1 * 6.350612163543701
Epoch 370, val loss: 0.6961942315101624
Epoch 380, training loss: 0.7512009739875793 = 0.11711043864488602 + 0.1 * 6.34090518951416
Epoch 380, val loss: 0.7028557062149048
Epoch 390, training loss: 0.7401406764984131 = 0.10692743957042694 + 0.1 * 6.332132339477539
Epoch 390, val loss: 0.7104712128639221
Epoch 400, training loss: 0.7310401201248169 = 0.09790050238370895 + 0.1 * 6.331396102905273
Epoch 400, val loss: 0.7185307145118713
Epoch 410, training loss: 0.7224830985069275 = 0.08989834785461426 + 0.1 * 6.325847148895264
Epoch 410, val loss: 0.7271547913551331
Epoch 420, training loss: 0.7146608829498291 = 0.08271991461515427 + 0.1 * 6.319409370422363
Epoch 420, val loss: 0.7360016703605652
Epoch 430, training loss: 0.7074663043022156 = 0.07627470791339874 + 0.1 * 6.311915874481201
Epoch 430, val loss: 0.7451490759849548
Epoch 440, training loss: 0.7010087370872498 = 0.07047609984874725 + 0.1 * 6.305325984954834
Epoch 440, val loss: 0.7543869018554688
Epoch 450, training loss: 0.6954129934310913 = 0.06523825228214264 + 0.1 * 6.3017473220825195
Epoch 450, val loss: 0.7637806534767151
Epoch 460, training loss: 0.6892513036727905 = 0.060487180948257446 + 0.1 * 6.2876410484313965
Epoch 460, val loss: 0.7731534838676453
Epoch 470, training loss: 0.6853252649307251 = 0.05616535618901253 + 0.1 * 6.291599273681641
Epoch 470, val loss: 0.7825934886932373
Epoch 480, training loss: 0.681355357170105 = 0.05224335193634033 + 0.1 * 6.2911200523376465
Epoch 480, val loss: 0.7917711734771729
Epoch 490, training loss: 0.6764417290687561 = 0.048681434243917465 + 0.1 * 6.277602672576904
Epoch 490, val loss: 0.8010929226875305
Epoch 500, training loss: 0.6732662916183472 = 0.045427292585372925 + 0.1 * 6.278389930725098
Epoch 500, val loss: 0.8101175427436829
Epoch 510, training loss: 0.6689642667770386 = 0.0424594022333622 + 0.1 * 6.265048027038574
Epoch 510, val loss: 0.8191500306129456
Epoch 520, training loss: 0.6666797995567322 = 0.039739158004522324 + 0.1 * 6.269406795501709
Epoch 520, val loss: 0.8279528021812439
Epoch 530, training loss: 0.6635404825210571 = 0.03725092485547066 + 0.1 * 6.262895584106445
Epoch 530, val loss: 0.836638331413269
Epoch 540, training loss: 0.659889817237854 = 0.03497162088751793 + 0.1 * 6.249182224273682
Epoch 540, val loss: 0.8452209830284119
Epoch 550, training loss: 0.6587660908699036 = 0.03287573158740997 + 0.1 * 6.258903503417969
Epoch 550, val loss: 0.8536101579666138
Epoch 560, training loss: 0.6553953886032104 = 0.03095596842467785 + 0.1 * 6.244393825531006
Epoch 560, val loss: 0.8618107438087463
Epoch 570, training loss: 0.6535972356796265 = 0.02919405698776245 + 0.1 * 6.24403190612793
Epoch 570, val loss: 0.8699780106544495
Epoch 580, training loss: 0.6522641181945801 = 0.027570564299821854 + 0.1 * 6.2469353675842285
Epoch 580, val loss: 0.8778543472290039
Epoch 590, training loss: 0.6497622132301331 = 0.026074105873703957 + 0.1 * 6.236880779266357
Epoch 590, val loss: 0.8857064247131348
Epoch 600, training loss: 0.6480047702789307 = 0.024691864848136902 + 0.1 * 6.233128547668457
Epoch 600, val loss: 0.8933368921279907
Epoch 610, training loss: 0.6459029912948608 = 0.023414378985762596 + 0.1 * 6.224885940551758
Epoch 610, val loss: 0.9008001089096069
Epoch 620, training loss: 0.6444507241249084 = 0.02223268710076809 + 0.1 * 6.222179889678955
Epoch 620, val loss: 0.9082217812538147
Epoch 630, training loss: 0.6431822776794434 = 0.021135373041033745 + 0.1 * 6.220468521118164
Epoch 630, val loss: 0.9153721928596497
Epoch 640, training loss: 0.6439107656478882 = 0.02011897787451744 + 0.1 * 6.237917900085449
Epoch 640, val loss: 0.9223442077636719
Epoch 650, training loss: 0.6404983401298523 = 0.019177919253706932 + 0.1 * 6.2132039070129395
Epoch 650, val loss: 0.9293383359909058
Epoch 660, training loss: 0.6394129395484924 = 0.01830177754163742 + 0.1 * 6.211111545562744
Epoch 660, val loss: 0.9361665844917297
Epoch 670, training loss: 0.6390347480773926 = 0.0174834206700325 + 0.1 * 6.215513229370117
Epoch 670, val loss: 0.9427650570869446
Epoch 680, training loss: 0.6374966502189636 = 0.016721243038773537 + 0.1 * 6.207754135131836
Epoch 680, val loss: 0.9491435885429382
Epoch 690, training loss: 0.6363727450370789 = 0.016010893508791924 + 0.1 * 6.20361852645874
Epoch 690, val loss: 0.9556312561035156
Epoch 700, training loss: 0.6352695822715759 = 0.015344612300395966 + 0.1 * 6.199249744415283
Epoch 700, val loss: 0.961776852607727
Epoch 710, training loss: 0.6361932754516602 = 0.014722060412168503 + 0.1 * 6.214712142944336
Epoch 710, val loss: 0.9679036736488342
Epoch 720, training loss: 0.6337891817092896 = 0.014138629660010338 + 0.1 * 6.196505069732666
Epoch 720, val loss: 0.9738401174545288
Epoch 730, training loss: 0.6326443552970886 = 0.013592483475804329 + 0.1 * 6.190518856048584
Epoch 730, val loss: 0.9798141121864319
Epoch 740, training loss: 0.6330987215042114 = 0.013077404350042343 + 0.1 * 6.200213432312012
Epoch 740, val loss: 0.9854450225830078
Epoch 750, training loss: 0.6316729187965393 = 0.012593391351401806 + 0.1 * 6.190794944763184
Epoch 750, val loss: 0.9911035895347595
Epoch 760, training loss: 0.6321699619293213 = 0.012137407436966896 + 0.1 * 6.2003254890441895
Epoch 760, val loss: 0.9965967535972595
Epoch 770, training loss: 0.6308237314224243 = 0.011706657707691193 + 0.1 * 6.191170692443848
Epoch 770, val loss: 1.0019623041152954
Epoch 780, training loss: 0.6295059323310852 = 0.011300768703222275 + 0.1 * 6.182051658630371
Epoch 780, val loss: 1.007269263267517
Epoch 790, training loss: 0.6283860206604004 = 0.010917715728282928 + 0.1 * 6.1746826171875
Epoch 790, val loss: 1.0125808715820312
Epoch 800, training loss: 0.6284672617912292 = 0.010554138571023941 + 0.1 * 6.179131031036377
Epoch 800, val loss: 1.0177264213562012
Epoch 810, training loss: 0.629158079624176 = 0.01020847912877798 + 0.1 * 6.18949556350708
Epoch 810, val loss: 1.0224164724349976
Epoch 820, training loss: 0.627234160900116 = 0.009883361868560314 + 0.1 * 6.173508167266846
Epoch 820, val loss: 1.0274627208709717
Epoch 830, training loss: 0.6271021366119385 = 0.0095751928165555 + 0.1 * 6.17526912689209
Epoch 830, val loss: 1.0324268341064453
Epoch 840, training loss: 0.6258355975151062 = 0.009280983358621597 + 0.1 * 6.16554594039917
Epoch 840, val loss: 1.0370659828186035
Epoch 850, training loss: 0.6255584955215454 = 0.009002727456390858 + 0.1 * 6.165557384490967
Epoch 850, val loss: 1.0418319702148438
Epoch 860, training loss: 0.6267688274383545 = 0.00873640738427639 + 0.1 * 6.180323600769043
Epoch 860, val loss: 1.0462802648544312
Epoch 870, training loss: 0.6251404881477356 = 0.008484022691845894 + 0.1 * 6.16656494140625
Epoch 870, val loss: 1.0507732629776
Epoch 880, training loss: 0.6245154738426208 = 0.00824370514601469 + 0.1 * 6.162717819213867
Epoch 880, val loss: 1.0552796125411987
Epoch 890, training loss: 0.6253976821899414 = 0.008013890124857426 + 0.1 * 6.173837661743164
Epoch 890, val loss: 1.0595797300338745
Epoch 900, training loss: 0.6241845488548279 = 0.00779421953484416 + 0.1 * 6.16390323638916
Epoch 900, val loss: 1.0637507438659668
Epoch 910, training loss: 0.6231987476348877 = 0.007585596293210983 + 0.1 * 6.156131267547607
Epoch 910, val loss: 1.067995309829712
Epoch 920, training loss: 0.6235733032226562 = 0.007385205011814833 + 0.1 * 6.161880970001221
Epoch 920, val loss: 1.0720633268356323
Epoch 930, training loss: 0.6220288276672363 = 0.007193871308118105 + 0.1 * 6.148349761962891
Epoch 930, val loss: 1.0760464668273926
Epoch 940, training loss: 0.6235812306404114 = 0.007011117413640022 + 0.1 * 6.165700912475586
Epoch 940, val loss: 1.080037236213684
Epoch 950, training loss: 0.6221522092819214 = 0.006834954489022493 + 0.1 * 6.153172016143799
Epoch 950, val loss: 1.0838189125061035
Epoch 960, training loss: 0.6216639280319214 = 0.006667436100542545 + 0.1 * 6.149964809417725
Epoch 960, val loss: 1.0877470970153809
Epoch 970, training loss: 0.6215408444404602 = 0.006506352219730616 + 0.1 * 6.1503448486328125
Epoch 970, val loss: 1.0915311574935913
Epoch 980, training loss: 0.6207964420318604 = 0.006351753603667021 + 0.1 * 6.144446849822998
Epoch 980, val loss: 1.0952386856079102
Epoch 990, training loss: 0.6229825019836426 = 0.0062035853043198586 + 0.1 * 6.167788982391357
Epoch 990, val loss: 1.0989018678665161
Epoch 1000, training loss: 0.6209662556648254 = 0.006059778854250908 + 0.1 * 6.149064540863037
Epoch 1000, val loss: 1.1021673679351807
Epoch 1010, training loss: 0.6199488043785095 = 0.005923835560679436 + 0.1 * 6.140249729156494
Epoch 1010, val loss: 1.1058473587036133
Epoch 1020, training loss: 0.6196142435073853 = 0.005792793817818165 + 0.1 * 6.138214588165283
Epoch 1020, val loss: 1.1094591617584229
Epoch 1030, training loss: 0.6197847127914429 = 0.005665855016559362 + 0.1 * 6.141188621520996
Epoch 1030, val loss: 1.112810730934143
Epoch 1040, training loss: 0.62049800157547 = 0.005543331615626812 + 0.1 * 6.1495466232299805
Epoch 1040, val loss: 1.1160283088684082
Epoch 1050, training loss: 0.6192624568939209 = 0.005425423849374056 + 0.1 * 6.138370037078857
Epoch 1050, val loss: 1.1193073987960815
Epoch 1060, training loss: 0.6188645958900452 = 0.005312405060976744 + 0.1 * 6.13552188873291
Epoch 1060, val loss: 1.1226387023925781
Epoch 1070, training loss: 0.6180769205093384 = 0.00520273856818676 + 0.1 * 6.128742218017578
Epoch 1070, val loss: 1.125788688659668
Epoch 1080, training loss: 0.6188857555389404 = 0.0050972020253539085 + 0.1 * 6.137885093688965
Epoch 1080, val loss: 1.1288702487945557
Epoch 1090, training loss: 0.6175529360771179 = 0.004995402880012989 + 0.1 * 6.125575065612793
Epoch 1090, val loss: 1.1320008039474487
Epoch 1100, training loss: 0.6192956566810608 = 0.004897385369986296 + 0.1 * 6.143982887268066
Epoch 1100, val loss: 1.1350961923599243
Epoch 1110, training loss: 0.6183665990829468 = 0.004802001640200615 + 0.1 * 6.135645866394043
Epoch 1110, val loss: 1.1380202770233154
Epoch 1120, training loss: 0.6171159148216248 = 0.004709992557764053 + 0.1 * 6.124058723449707
Epoch 1120, val loss: 1.1409308910369873
Epoch 1130, training loss: 0.6182355284690857 = 0.004621533676981926 + 0.1 * 6.136139869689941
Epoch 1130, val loss: 1.1439372301101685
Epoch 1140, training loss: 0.6169913411140442 = 0.004535578191280365 + 0.1 * 6.124557971954346
Epoch 1140, val loss: 1.1467742919921875
Epoch 1150, training loss: 0.6167320609092712 = 0.0044528283178806305 + 0.1 * 6.1227922439575195
Epoch 1150, val loss: 1.1497033834457397
Epoch 1160, training loss: 0.6169881224632263 = 0.004372270777821541 + 0.1 * 6.126158237457275
Epoch 1160, val loss: 1.152486801147461
Epoch 1170, training loss: 0.6160432696342468 = 0.004294049926102161 + 0.1 * 6.117492198944092
Epoch 1170, val loss: 1.155199408531189
Epoch 1180, training loss: 0.6167594194412231 = 0.004218264948576689 + 0.1 * 6.125411510467529
Epoch 1180, val loss: 1.1578054428100586
Epoch 1190, training loss: 0.6159505844116211 = 0.004145106300711632 + 0.1 * 6.1180548667907715
Epoch 1190, val loss: 1.1605002880096436
Epoch 1200, training loss: 0.6155622601509094 = 0.004074904602020979 + 0.1 * 6.114872932434082
Epoch 1200, val loss: 1.163338541984558
Epoch 1210, training loss: 0.6169500946998596 = 0.0040062349289655685 + 0.1 * 6.129438877105713
Epoch 1210, val loss: 1.1659585237503052
Epoch 1220, training loss: 0.6155295372009277 = 0.003938964102417231 + 0.1 * 6.11590576171875
Epoch 1220, val loss: 1.1684086322784424
Epoch 1230, training loss: 0.6161072254180908 = 0.0038744329940527678 + 0.1 * 6.12232780456543
Epoch 1230, val loss: 1.1710251569747925
Epoch 1240, training loss: 0.6152478456497192 = 0.00381109700538218 + 0.1 * 6.1143670082092285
Epoch 1240, val loss: 1.1734261512756348
Epoch 1250, training loss: 0.6151711940765381 = 0.0037496942095458508 + 0.1 * 6.11421537399292
Epoch 1250, val loss: 1.1758466958999634
Epoch 1260, training loss: 0.6144468188285828 = 0.0036905130837112665 + 0.1 * 6.10756254196167
Epoch 1260, val loss: 1.1784539222717285
Epoch 1270, training loss: 0.6150758266448975 = 0.0036331769078969955 + 0.1 * 6.114426136016846
Epoch 1270, val loss: 1.1810171604156494
Epoch 1280, training loss: 0.6142375469207764 = 0.0035763191990554333 + 0.1 * 6.106612205505371
Epoch 1280, val loss: 1.183250069618225
Epoch 1290, training loss: 0.6140772700309753 = 0.003521654522046447 + 0.1 * 6.105556011199951
Epoch 1290, val loss: 1.1856237649917603
Epoch 1300, training loss: 0.6169758439064026 = 0.0034685267601162195 + 0.1 * 6.135073184967041
Epoch 1300, val loss: 1.1880046129226685
Epoch 1310, training loss: 0.6144166588783264 = 0.003416172694414854 + 0.1 * 6.110004425048828
Epoch 1310, val loss: 1.1901572942733765
Epoch 1320, training loss: 0.6140580773353577 = 0.003366281511262059 + 0.1 * 6.106917858123779
Epoch 1320, val loss: 1.1926151514053345
Epoch 1330, training loss: 0.6128720641136169 = 0.0033175749704241753 + 0.1 * 6.095544338226318
Epoch 1330, val loss: 1.195022463798523
Epoch 1340, training loss: 0.6149610280990601 = 0.003269969252869487 + 0.1 * 6.116910457611084
Epoch 1340, val loss: 1.197330117225647
Epoch 1350, training loss: 0.6134586334228516 = 0.003222700208425522 + 0.1 * 6.102359294891357
Epoch 1350, val loss: 1.199233055114746
Epoch 1360, training loss: 0.6132948398590088 = 0.003177451202645898 + 0.1 * 6.101173400878906
Epoch 1360, val loss: 1.2014780044555664
Epoch 1370, training loss: 0.6149474382400513 = 0.0031331717036664486 + 0.1 * 6.118142604827881
Epoch 1370, val loss: 1.203586220741272
Epoch 1380, training loss: 0.6132265329360962 = 0.0030896819662302732 + 0.1 * 6.101368427276611
Epoch 1380, val loss: 1.2056034803390503
Epoch 1390, training loss: 0.6131368279457092 = 0.003048078389838338 + 0.1 * 6.100887775421143
Epoch 1390, val loss: 1.207925796508789
Epoch 1400, training loss: 0.612398624420166 = 0.0030071325600147247 + 0.1 * 6.093914985656738
Epoch 1400, val loss: 1.2100163698196411
Epoch 1410, training loss: 0.6141916513442993 = 0.002967200241982937 + 0.1 * 6.112244606018066
Epoch 1410, val loss: 1.2120795249938965
Epoch 1420, training loss: 0.6123549938201904 = 0.002927745459601283 + 0.1 * 6.094272613525391
Epoch 1420, val loss: 1.2139523029327393
Epoch 1430, training loss: 0.61222904920578 = 0.0028900245670229197 + 0.1 * 6.093389987945557
Epoch 1430, val loss: 1.2161046266555786
Epoch 1440, training loss: 0.613190770149231 = 0.002853111829608679 + 0.1 * 6.103376865386963
Epoch 1440, val loss: 1.2182155847549438
Epoch 1450, training loss: 0.6116921901702881 = 0.0028164018876850605 + 0.1 * 6.0887579917907715
Epoch 1450, val loss: 1.2200841903686523
Epoch 1460, training loss: 0.6124590635299683 = 0.0027810772880911827 + 0.1 * 6.0967793464660645
Epoch 1460, val loss: 1.2221119403839111
Epoch 1470, training loss: 0.6124071478843689 = 0.00274610985070467 + 0.1 * 6.0966105461120605
Epoch 1470, val loss: 1.223979115486145
Epoch 1480, training loss: 0.6119698882102966 = 0.002712140092626214 + 0.1 * 6.0925774574279785
Epoch 1480, val loss: 1.2258368730545044
Epoch 1490, training loss: 0.6113753318786621 = 0.0026792502030730247 + 0.1 * 6.086960792541504
Epoch 1490, val loss: 1.2278605699539185
Epoch 1500, training loss: 0.6122834086418152 = 0.00264684553258121 + 0.1 * 6.096365451812744
Epoch 1500, val loss: 1.2297073602676392
Epoch 1510, training loss: 0.6114340424537659 = 0.0026149977929890156 + 0.1 * 6.08819055557251
Epoch 1510, val loss: 1.231537938117981
Epoch 1520, training loss: 0.6113826632499695 = 0.002584215486422181 + 0.1 * 6.087984085083008
Epoch 1520, val loss: 1.2334413528442383
Epoch 1530, training loss: 0.6110378503799438 = 0.0025536916218698025 + 0.1 * 6.084841251373291
Epoch 1530, val loss: 1.2351887226104736
Epoch 1540, training loss: 0.6110989451408386 = 0.00252386461943388 + 0.1 * 6.085751056671143
Epoch 1540, val loss: 1.236881971359253
Epoch 1550, training loss: 0.6104811429977417 = 0.0024950297083705664 + 0.1 * 6.079860687255859
Epoch 1550, val loss: 1.2387640476226807
Epoch 1560, training loss: 0.6115618944168091 = 0.0024669289123266935 + 0.1 * 6.090949535369873
Epoch 1560, val loss: 1.2406340837478638
Epoch 1570, training loss: 0.612698495388031 = 0.0024386299774050713 + 0.1 * 6.102598667144775
Epoch 1570, val loss: 1.2421609163284302
Epoch 1580, training loss: 0.6106895208358765 = 0.0024110835511237383 + 0.1 * 6.082784175872803
Epoch 1580, val loss: 1.2437446117401123
Epoch 1590, training loss: 0.6106348037719727 = 0.0023847841657698154 + 0.1 * 6.082499980926514
Epoch 1590, val loss: 1.2456765174865723
Epoch 1600, training loss: 0.6102738380432129 = 0.0023587604518979788 + 0.1 * 6.079150676727295
Epoch 1600, val loss: 1.2474277019500732
Epoch 1610, training loss: 0.6112841367721558 = 0.0023331111297011375 + 0.1 * 6.089509963989258
Epoch 1610, val loss: 1.249130368232727
Epoch 1620, training loss: 0.6100981831550598 = 0.002307674614712596 + 0.1 * 6.077905178070068
Epoch 1620, val loss: 1.250618815422058
Epoch 1630, training loss: 0.6112942099571228 = 0.002282977569848299 + 0.1 * 6.090112209320068
Epoch 1630, val loss: 1.2522473335266113
Epoch 1640, training loss: 0.6100398302078247 = 0.002258623717352748 + 0.1 * 6.077812194824219
Epoch 1640, val loss: 1.2538286447525024
Epoch 1650, training loss: 0.6105923652648926 = 0.002234925515949726 + 0.1 * 6.083574295043945
Epoch 1650, val loss: 1.2554917335510254
Epoch 1660, training loss: 0.6092193722724915 = 0.002211639890447259 + 0.1 * 6.070077419281006
Epoch 1660, val loss: 1.2571316957473755
Epoch 1670, training loss: 0.6109675765037537 = 0.0021890238858759403 + 0.1 * 6.087785243988037
Epoch 1670, val loss: 1.2588428258895874
Epoch 1680, training loss: 0.6096702814102173 = 0.002166271675378084 + 0.1 * 6.075039863586426
Epoch 1680, val loss: 1.2603404521942139
Epoch 1690, training loss: 0.6098601222038269 = 0.0021441832650452852 + 0.1 * 6.077159404754639
Epoch 1690, val loss: 1.2618811130523682
Epoch 1700, training loss: 0.609226644039154 = 0.002122526755556464 + 0.1 * 6.071041107177734
Epoch 1700, val loss: 1.2634360790252686
Epoch 1710, training loss: 0.6116307377815247 = 0.0021013072691857815 + 0.1 * 6.09529447555542
Epoch 1710, val loss: 1.2649949789047241
Epoch 1720, training loss: 0.609574019908905 = 0.0020800449419766665 + 0.1 * 6.074939727783203
Epoch 1720, val loss: 1.2663347721099854
Epoch 1730, training loss: 0.6092977523803711 = 0.002059826161712408 + 0.1 * 6.072379112243652
Epoch 1730, val loss: 1.2680096626281738
Epoch 1740, training loss: 0.6092733144760132 = 0.0020398064516484737 + 0.1 * 6.072335243225098
Epoch 1740, val loss: 1.2695866823196411
Epoch 1750, training loss: 0.6085984110832214 = 0.002020056126639247 + 0.1 * 6.065783500671387
Epoch 1750, val loss: 1.2711265087127686
Epoch 1760, training loss: 0.610273003578186 = 0.0020005269907414913 + 0.1 * 6.0827250480651855
Epoch 1760, val loss: 1.2725398540496826
Epoch 1770, training loss: 0.6091225147247314 = 0.0019811426755040884 + 0.1 * 6.071413516998291
Epoch 1770, val loss: 1.2738176584243774
Epoch 1780, training loss: 0.6087220311164856 = 0.0019626186694949865 + 0.1 * 6.067594051361084
Epoch 1780, val loss: 1.2753881216049194
Epoch 1790, training loss: 0.6097748875617981 = 0.001944409217685461 + 0.1 * 6.078304767608643
Epoch 1790, val loss: 1.2769193649291992
Epoch 1800, training loss: 0.6091609597206116 = 0.0019260828848928213 + 0.1 * 6.072348594665527
Epoch 1800, val loss: 1.2782384157180786
Epoch 1810, training loss: 0.6090332865715027 = 0.001908328733406961 + 0.1 * 6.071249485015869
Epoch 1810, val loss: 1.2796263694763184
Epoch 1820, training loss: 0.6080271005630493 = 0.0018908503698185086 + 0.1 * 6.0613627433776855
Epoch 1820, val loss: 1.2810405492782593
Epoch 1830, training loss: 0.6083177328109741 = 0.0018739119404926896 + 0.1 * 6.0644378662109375
Epoch 1830, val loss: 1.2825682163238525
Epoch 1840, training loss: 0.6087564826011658 = 0.0018570685060694814 + 0.1 * 6.068994522094727
Epoch 1840, val loss: 1.2839796543121338
Epoch 1850, training loss: 0.6089864373207092 = 0.0018401846755295992 + 0.1 * 6.071462631225586
Epoch 1850, val loss: 1.2851797342300415
Epoch 1860, training loss: 0.6088935136795044 = 0.0018236837349832058 + 0.1 * 6.070698261260986
Epoch 1860, val loss: 1.2864528894424438
Epoch 1870, training loss: 0.6088575720787048 = 0.0018076413543894887 + 0.1 * 6.070499420166016
Epoch 1870, val loss: 1.2877941131591797
Epoch 1880, training loss: 0.6079768538475037 = 0.001791943795979023 + 0.1 * 6.061849117279053
Epoch 1880, val loss: 1.289225459098816
Epoch 1890, training loss: 0.6083092093467712 = 0.0017764200456440449 + 0.1 * 6.065328121185303
Epoch 1890, val loss: 1.2905631065368652
Epoch 1900, training loss: 0.6083374619483948 = 0.0017610843060538173 + 0.1 * 6.065763473510742
Epoch 1900, val loss: 1.2918990850448608
Epoch 1910, training loss: 0.608245313167572 = 0.0017460085218772292 + 0.1 * 6.064992904663086
Epoch 1910, val loss: 1.2932167053222656
Epoch 1920, training loss: 0.6084434390068054 = 0.0017311781411990523 + 0.1 * 6.067122936248779
Epoch 1920, val loss: 1.2945162057876587
Epoch 1930, training loss: 0.6075076460838318 = 0.0017163382144644856 + 0.1 * 6.057913303375244
Epoch 1930, val loss: 1.295675277709961
Epoch 1940, training loss: 0.6078121066093445 = 0.0017020745435729623 + 0.1 * 6.061100482940674
Epoch 1940, val loss: 1.2970610857009888
Epoch 1950, training loss: 0.608088493347168 = 0.001688083284534514 + 0.1 * 6.064003944396973
Epoch 1950, val loss: 1.2984317541122437
Epoch 1960, training loss: 0.6083305478096008 = 0.0016741575673222542 + 0.1 * 6.066563606262207
Epoch 1960, val loss: 1.2997468709945679
Epoch 1970, training loss: 0.6076145768165588 = 0.0016602593241259456 + 0.1 * 6.059542655944824
Epoch 1970, val loss: 1.3009182214736938
Epoch 1980, training loss: 0.6073044538497925 = 0.0016468457179144025 + 0.1 * 6.056576251983643
Epoch 1980, val loss: 1.3022609949111938
Epoch 1990, training loss: 0.6076546311378479 = 0.001633655745536089 + 0.1 * 6.06020975112915
Epoch 1990, val loss: 1.3036012649536133
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.4834
Flip ASR: 0.3867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.80332350730896 = 1.965936541557312 + 0.1 * 8.373869895935059
Epoch 0, val loss: 1.9738050699234009
Epoch 10, training loss: 2.7926130294799805 = 1.955241322517395 + 0.1 * 8.373716354370117
Epoch 10, val loss: 1.9623528718948364
Epoch 20, training loss: 2.7792062759399414 = 1.9419254064559937 + 0.1 * 8.372809410095215
Epoch 20, val loss: 1.947767972946167
Epoch 30, training loss: 2.759561777114868 = 1.9229933023452759 + 0.1 * 8.36568546295166
Epoch 30, val loss: 1.926645040512085
Epoch 40, training loss: 2.7268214225769043 = 1.8947457075119019 + 0.1 * 8.320755958557129
Epoch 40, val loss: 1.8952550888061523
Epoch 50, training loss: 2.6618003845214844 = 1.8560190200805664 + 0.1 * 8.057812690734863
Epoch 50, val loss: 1.8544816970825195
Epoch 60, training loss: 2.5722413063049316 = 1.8126353025436401 + 0.1 * 7.5960588455200195
Epoch 60, val loss: 1.8130486011505127
Epoch 70, training loss: 2.4920754432678223 = 1.772402048110962 + 0.1 * 7.196732997894287
Epoch 70, val loss: 1.7777236700057983
Epoch 80, training loss: 2.4228079319000244 = 1.7343899011611938 + 0.1 * 6.884179592132568
Epoch 80, val loss: 1.7442320585250854
Epoch 90, training loss: 2.3655717372894287 = 1.6880446672439575 + 0.1 * 6.775271415710449
Epoch 90, val loss: 1.7015222311019897
Epoch 100, training loss: 2.2979683876037598 = 1.6279935836791992 + 0.1 * 6.699746608734131
Epoch 100, val loss: 1.6477667093276978
Epoch 110, training loss: 2.215059995651245 = 1.5502595901489258 + 0.1 * 6.648003101348877
Epoch 110, val loss: 1.5820605754852295
Epoch 120, training loss: 2.116487503051758 = 1.4547946453094482 + 0.1 * 6.616927146911621
Epoch 120, val loss: 1.5027530193328857
Epoch 130, training loss: 2.0078115463256836 = 1.3476660251617432 + 0.1 * 6.601456165313721
Epoch 130, val loss: 1.412947416305542
Epoch 140, training loss: 1.8952949047088623 = 1.235912799835205 + 0.1 * 6.593821048736572
Epoch 140, val loss: 1.320716142654419
Epoch 150, training loss: 1.7834343910217285 = 1.1244909763336182 + 0.1 * 6.589433670043945
Epoch 150, val loss: 1.2289451360702515
Epoch 160, training loss: 1.6742695569992065 = 1.01554274559021 + 0.1 * 6.587267875671387
Epoch 160, val loss: 1.1397340297698975
Epoch 170, training loss: 1.569584608078003 = 0.9110660552978516 + 0.1 * 6.585184574127197
Epoch 170, val loss: 1.0546284914016724
Epoch 180, training loss: 1.4719579219818115 = 0.8135989308357239 + 0.1 * 6.583590030670166
Epoch 180, val loss: 0.9754772186279297
Epoch 190, training loss: 1.384016752243042 = 0.7257946729660034 + 0.1 * 6.582220554351807
Epoch 190, val loss: 0.904870867729187
Epoch 200, training loss: 1.3068338632583618 = 0.6489368677139282 + 0.1 * 6.578969955444336
Epoch 200, val loss: 0.8442392349243164
Epoch 210, training loss: 1.2408427000045776 = 0.5832963585853577 + 0.1 * 6.57546329498291
Epoch 210, val loss: 0.7942816019058228
Epoch 220, training loss: 1.185409665107727 = 0.5279648900032043 + 0.1 * 6.5744476318359375
Epoch 220, val loss: 0.7545467019081116
Epoch 230, training loss: 1.1380256414413452 = 0.48120155930519104 + 0.1 * 6.568241119384766
Epoch 230, val loss: 0.7236670255661011
Epoch 240, training loss: 1.0968636274337769 = 0.44056805968284607 + 0.1 * 6.562955856323242
Epoch 240, val loss: 0.6992183327674866
Epoch 250, training loss: 1.0598231554031372 = 0.404059499502182 + 0.1 * 6.557636260986328
Epoch 250, val loss: 0.6792563796043396
Epoch 260, training loss: 1.0254889726638794 = 0.3701501786708832 + 0.1 * 6.5533881187438965
Epoch 260, val loss: 0.662415087223053
Epoch 270, training loss: 0.9923418760299683 = 0.33769315481185913 + 0.1 * 6.546487331390381
Epoch 270, val loss: 0.647523045539856
Epoch 280, training loss: 0.9596476554870605 = 0.3056879937648773 + 0.1 * 6.539596080780029
Epoch 280, val loss: 0.6337989568710327
Epoch 290, training loss: 0.9272398948669434 = 0.2740391492843628 + 0.1 * 6.532007217407227
Epoch 290, val loss: 0.620898425579071
Epoch 300, training loss: 0.8957775831222534 = 0.24318446218967438 + 0.1 * 6.525931358337402
Epoch 300, val loss: 0.6088657379150391
Epoch 310, training loss: 0.8660300970077515 = 0.2140074223279953 + 0.1 * 6.52022647857666
Epoch 310, val loss: 0.5979110598564148
Epoch 320, training loss: 0.8391854166984558 = 0.18741820752620697 + 0.1 * 6.517671585083008
Epoch 320, val loss: 0.5888353586196899
Epoch 330, training loss: 0.8152006268501282 = 0.16399921476840973 + 0.1 * 6.512014389038086
Epoch 330, val loss: 0.5817714333534241
Epoch 340, training loss: 0.7941588759422302 = 0.14369678497314453 + 0.1 * 6.5046210289001465
Epoch 340, val loss: 0.5770291686058044
Epoch 350, training loss: 0.776543140411377 = 0.12622889876365662 + 0.1 * 6.5031418800354
Epoch 350, val loss: 0.5743047595024109
Epoch 360, training loss: 0.7610440254211426 = 0.11125284433364868 + 0.1 * 6.49791145324707
Epoch 360, val loss: 0.5734241604804993
Epoch 370, training loss: 0.7474359273910522 = 0.0983848124742508 + 0.1 * 6.490511417388916
Epoch 370, val loss: 0.574195146560669
Epoch 380, training loss: 0.7362594604492188 = 0.08732214570045471 + 0.1 * 6.489372730255127
Epoch 380, val loss: 0.5762553215026855
Epoch 390, training loss: 0.7259928584098816 = 0.0778200626373291 + 0.1 * 6.481727600097656
Epoch 390, val loss: 0.5793202519416809
Epoch 400, training loss: 0.7178863883018494 = 0.06961847096681595 + 0.1 * 6.4826788902282715
Epoch 400, val loss: 0.5833559632301331
Epoch 410, training loss: 0.7092810869216919 = 0.0625564306974411 + 0.1 * 6.4672465324401855
Epoch 410, val loss: 0.5880542397499084
Epoch 420, training loss: 0.7029193043708801 = 0.05644451826810837 + 0.1 * 6.464747905731201
Epoch 420, val loss: 0.5934329032897949
Epoch 430, training loss: 0.696800708770752 = 0.051159437745809555 + 0.1 * 6.4564127922058105
Epoch 430, val loss: 0.5991448760032654
Epoch 440, training loss: 0.6913459300994873 = 0.046566177159547806 + 0.1 * 6.4477972984313965
Epoch 440, val loss: 0.6052728891372681
Epoch 450, training loss: 0.6865153908729553 = 0.042575638741254807 + 0.1 * 6.43939733505249
Epoch 450, val loss: 0.6116542220115662
Epoch 460, training loss: 0.6816610097885132 = 0.03908427059650421 + 0.1 * 6.425767421722412
Epoch 460, val loss: 0.6181142926216125
Epoch 470, training loss: 0.6774697303771973 = 0.03600318357348442 + 0.1 * 6.414665222167969
Epoch 470, val loss: 0.6247676014900208
Epoch 480, training loss: 0.6758665442466736 = 0.03328879177570343 + 0.1 * 6.425777435302734
Epoch 480, val loss: 0.6315351724624634
Epoch 490, training loss: 0.6706222891807556 = 0.0309029258787632 + 0.1 * 6.397193431854248
Epoch 490, val loss: 0.6379485726356506
Epoch 500, training loss: 0.6674890518188477 = 0.028773006051778793 + 0.1 * 6.387160301208496
Epoch 500, val loss: 0.6446419954299927
Epoch 510, training loss: 0.665698766708374 = 0.026870528236031532 + 0.1 * 6.388282299041748
Epoch 510, val loss: 0.651132345199585
Epoch 520, training loss: 0.6629647612571716 = 0.02517378330230713 + 0.1 * 6.3779096603393555
Epoch 520, val loss: 0.6573958396911621
Epoch 530, training loss: 0.6597008109092712 = 0.0236361026763916 + 0.1 * 6.360646724700928
Epoch 530, val loss: 0.6638231873512268
Epoch 540, training loss: 0.6574826836585999 = 0.022238964214920998 + 0.1 * 6.3524370193481445
Epoch 540, val loss: 0.6701123714447021
Epoch 550, training loss: 0.6567087769508362 = 0.02096201665699482 + 0.1 * 6.3574676513671875
Epoch 550, val loss: 0.6763573884963989
Epoch 560, training loss: 0.6537593007087708 = 0.019805217161774635 + 0.1 * 6.339540958404541
Epoch 560, val loss: 0.6824342608451843
Epoch 570, training loss: 0.6520745158195496 = 0.018748411908745766 + 0.1 * 6.333261013031006
Epoch 570, val loss: 0.6884551048278809
Epoch 580, training loss: 0.6504349112510681 = 0.017776256427168846 + 0.1 * 6.3265862464904785
Epoch 580, val loss: 0.6944258213043213
Epoch 590, training loss: 0.6496975421905518 = 0.016882458701729774 + 0.1 * 6.328150749206543
Epoch 590, val loss: 0.7002418637275696
Epoch 600, training loss: 0.6477624773979187 = 0.016061561182141304 + 0.1 * 6.317008972167969
Epoch 600, val loss: 0.7058601975440979
Epoch 610, training loss: 0.6456044316291809 = 0.015303992666304111 + 0.1 * 6.303004741668701
Epoch 610, val loss: 0.7114900350570679
Epoch 620, training loss: 0.6452533006668091 = 0.014599259942770004 + 0.1 * 6.306540489196777
Epoch 620, val loss: 0.7170442938804626
Epoch 630, training loss: 0.6440275311470032 = 0.013947786763310432 + 0.1 * 6.300797462463379
Epoch 630, val loss: 0.7223946452140808
Epoch 640, training loss: 0.6420225501060486 = 0.01334326434880495 + 0.1 * 6.286792755126953
Epoch 640, val loss: 0.7277187705039978
Epoch 650, training loss: 0.6412635445594788 = 0.012782336212694645 + 0.1 * 6.284811973571777
Epoch 650, val loss: 0.7329372763633728
Epoch 660, training loss: 0.6427115201950073 = 0.012258722446858883 + 0.1 * 6.304527759552002
Epoch 660, val loss: 0.7380716800689697
Epoch 670, training loss: 0.6391156315803528 = 0.011767547577619553 + 0.1 * 6.273480415344238
Epoch 670, val loss: 0.743027925491333
Epoch 680, training loss: 0.6377735137939453 = 0.011313648894429207 + 0.1 * 6.264598369598389
Epoch 680, val loss: 0.7479047775268555
Epoch 690, training loss: 0.639712929725647 = 0.010886130854487419 + 0.1 * 6.288267612457275
Epoch 690, val loss: 0.7527949810028076
Epoch 700, training loss: 0.6362992525100708 = 0.01048342976719141 + 0.1 * 6.258158206939697
Epoch 700, val loss: 0.7574995756149292
Epoch 710, training loss: 0.6350738406181335 = 0.010107984766364098 + 0.1 * 6.249658584594727
Epoch 710, val loss: 0.7621323466300964
Epoch 720, training loss: 0.6375662088394165 = 0.009751956909894943 + 0.1 * 6.278141975402832
Epoch 720, val loss: 0.7667776346206665
Epoch 730, training loss: 0.6343563199043274 = 0.009416489861905575 + 0.1 * 6.249398231506348
Epoch 730, val loss: 0.771238386631012
Epoch 740, training loss: 0.6336413621902466 = 0.009102835319936275 + 0.1 * 6.24538516998291
Epoch 740, val loss: 0.7756476998329163
Epoch 750, training loss: 0.632207453250885 = 0.008804873563349247 + 0.1 * 6.234025955200195
Epoch 750, val loss: 0.7800260186195374
Epoch 760, training loss: 0.6324629187583923 = 0.008523916825652122 + 0.1 * 6.239389419555664
Epoch 760, val loss: 0.784300684928894
Epoch 770, training loss: 0.6317026019096375 = 0.008255995810031891 + 0.1 * 6.234465599060059
Epoch 770, val loss: 0.7885037660598755
Epoch 780, training loss: 0.6314589977264404 = 0.008002358488738537 + 0.1 * 6.2345662117004395
Epoch 780, val loss: 0.7926484942436218
Epoch 790, training loss: 0.6302095055580139 = 0.007762589957565069 + 0.1 * 6.22446870803833
Epoch 790, val loss: 0.7967531681060791
Epoch 800, training loss: 0.6308196187019348 = 0.007533909287303686 + 0.1 * 6.2328572273254395
Epoch 800, val loss: 0.8007771968841553
Epoch 810, training loss: 0.6302688121795654 = 0.007316193543374538 + 0.1 * 6.229526042938232
Epoch 810, val loss: 0.8047100901603699
Epoch 820, training loss: 0.6283312439918518 = 0.0071098715998232365 + 0.1 * 6.212213516235352
Epoch 820, val loss: 0.8086051940917969
Epoch 830, training loss: 0.6285558938980103 = 0.006913863122463226 + 0.1 * 6.216420650482178
Epoch 830, val loss: 0.8124648332595825
Epoch 840, training loss: 0.6285622119903564 = 0.006725234445184469 + 0.1 * 6.218369483947754
Epoch 840, val loss: 0.8162462115287781
Epoch 850, training loss: 0.6279296875 = 0.006545723415911198 + 0.1 * 6.213839530944824
Epoch 850, val loss: 0.8199723362922668
Epoch 860, training loss: 0.6268914937973022 = 0.0063749924302101135 + 0.1 * 6.205164432525635
Epoch 860, val loss: 0.8236629962921143
Epoch 870, training loss: 0.6265271902084351 = 0.006212115287780762 + 0.1 * 6.203150749206543
Epoch 870, val loss: 0.8273173570632935
Epoch 880, training loss: 0.6264778971672058 = 0.006055130157619715 + 0.1 * 6.204227924346924
Epoch 880, val loss: 0.8309319019317627
Epoch 890, training loss: 0.6260704398155212 = 0.005904861260205507 + 0.1 * 6.201655864715576
Epoch 890, val loss: 0.8344868421554565
Epoch 900, training loss: 0.6251327991485596 = 0.005761174485087395 + 0.1 * 6.193716526031494
Epoch 900, val loss: 0.8379878997802734
Epoch 910, training loss: 0.6262925863265991 = 0.005624018609523773 + 0.1 * 6.206686019897461
Epoch 910, val loss: 0.8414509892463684
Epoch 920, training loss: 0.624470591545105 = 0.005491898860782385 + 0.1 * 6.189786911010742
Epoch 920, val loss: 0.8448317646980286
Epoch 930, training loss: 0.6237732172012329 = 0.005366152618080378 + 0.1 * 6.184070587158203
Epoch 930, val loss: 0.8481625318527222
Epoch 940, training loss: 0.6278218626976013 = 0.005245022475719452 + 0.1 * 6.225768089294434
Epoch 940, val loss: 0.8514792919158936
Epoch 950, training loss: 0.623883068561554 = 0.005126595497131348 + 0.1 * 6.187564849853516
Epoch 950, val loss: 0.8546661138534546
Epoch 960, training loss: 0.6232001781463623 = 0.005015638656914234 + 0.1 * 6.181845664978027
Epoch 960, val loss: 0.8578343391418457
Epoch 970, training loss: 0.6229870915412903 = 0.0049087777733802795 + 0.1 * 6.180782794952393
Epoch 970, val loss: 0.860997200012207
Epoch 980, training loss: 0.6227643489837646 = 0.004804408177733421 + 0.1 * 6.179598808288574
Epoch 980, val loss: 0.8640520572662354
Epoch 990, training loss: 0.6232292652130127 = 0.004704565741121769 + 0.1 * 6.185246467590332
Epoch 990, val loss: 0.8670754432678223
Epoch 1000, training loss: 0.6220660209655762 = 0.004608475603163242 + 0.1 * 6.174575328826904
Epoch 1000, val loss: 0.8700869083404541
Epoch 1010, training loss: 0.6215652823448181 = 0.004516270011663437 + 0.1 * 6.170490264892578
Epoch 1010, val loss: 0.8730469942092896
Epoch 1020, training loss: 0.6211435198783875 = 0.004426012746989727 + 0.1 * 6.16717529296875
Epoch 1020, val loss: 0.8759737014770508
Epoch 1030, training loss: 0.6209717988967896 = 0.0043398551642894745 + 0.1 * 6.166319370269775
Epoch 1030, val loss: 0.8788459300994873
Epoch 1040, training loss: 0.6228585243225098 = 0.0042573800310492516 + 0.1 * 6.18601131439209
Epoch 1040, val loss: 0.8816694021224976
Epoch 1050, training loss: 0.6208027601242065 = 0.004175945650786161 + 0.1 * 6.1662678718566895
Epoch 1050, val loss: 0.8844611048698425
Epoch 1060, training loss: 0.620295524597168 = 0.004098952282220125 + 0.1 * 6.161965847015381
Epoch 1060, val loss: 0.8872063159942627
Epoch 1070, training loss: 0.621938169002533 = 0.004024294205009937 + 0.1 * 6.179138660430908
Epoch 1070, val loss: 0.8899323344230652
Epoch 1080, training loss: 0.6202213168144226 = 0.0039508156478405 + 0.1 * 6.162704944610596
Epoch 1080, val loss: 0.892605185508728
Epoch 1090, training loss: 0.6192958950996399 = 0.0038811727426946163 + 0.1 * 6.154147148132324
Epoch 1090, val loss: 0.8952664136886597
Epoch 1100, training loss: 0.6198425889015198 = 0.00381315010599792 + 0.1 * 6.160294532775879
Epoch 1100, val loss: 0.8979025483131409
Epoch 1110, training loss: 0.6184181571006775 = 0.003746991278603673 + 0.1 * 6.146711826324463
Epoch 1110, val loss: 0.9004559516906738
Epoch 1120, training loss: 0.6198181509971619 = 0.0036836029030382633 + 0.1 * 6.1613450050354
Epoch 1120, val loss: 0.9030143618583679
Epoch 1130, training loss: 0.618162214756012 = 0.003621272277086973 + 0.1 * 6.145409107208252
Epoch 1130, val loss: 0.9055520296096802
Epoch 1140, training loss: 0.620417594909668 = 0.0035611032508313656 + 0.1 * 6.168565273284912
Epoch 1140, val loss: 0.9080302119255066
Epoch 1150, training loss: 0.6181507706642151 = 0.0035024622920900583 + 0.1 * 6.146482467651367
Epoch 1150, val loss: 0.9105281233787537
Epoch 1160, training loss: 0.6184468269348145 = 0.003446622285991907 + 0.1 * 6.150001525878906
Epoch 1160, val loss: 0.9129660129547119
Epoch 1170, training loss: 0.6172951459884644 = 0.003391732694581151 + 0.1 * 6.139033794403076
Epoch 1170, val loss: 0.9153676629066467
Epoch 1180, training loss: 0.6187376379966736 = 0.003338512731716037 + 0.1 * 6.153991222381592
Epoch 1180, val loss: 0.9177210330963135
Epoch 1190, training loss: 0.6172403693199158 = 0.0032861044164747 + 0.1 * 6.139542102813721
Epoch 1190, val loss: 0.920077919960022
Epoch 1200, training loss: 0.6165176630020142 = 0.003236090764403343 + 0.1 * 6.132815361022949
Epoch 1200, val loss: 0.9223747849464417
Epoch 1210, training loss: 0.6170490384101868 = 0.003187487367540598 + 0.1 * 6.138615608215332
Epoch 1210, val loss: 0.9246848821640015
Epoch 1220, training loss: 0.6163663268089294 = 0.0031394476536661386 + 0.1 * 6.132268905639648
Epoch 1220, val loss: 0.9269565343856812
Epoch 1230, training loss: 0.6172358393669128 = 0.0030931876972317696 + 0.1 * 6.1414265632629395
Epoch 1230, val loss: 0.9291951060295105
Epoch 1240, training loss: 0.6156077980995178 = 0.003048049984499812 + 0.1 * 6.1255974769592285
Epoch 1240, val loss: 0.9314335584640503
Epoch 1250, training loss: 0.6160365343093872 = 0.0030047569889575243 + 0.1 * 6.130317687988281
Epoch 1250, val loss: 0.9336352348327637
Epoch 1260, training loss: 0.6155890822410583 = 0.0029621401336044073 + 0.1 * 6.126269817352295
Epoch 1260, val loss: 0.9358161687850952
Epoch 1270, training loss: 0.6152987480163574 = 0.002920202212408185 + 0.1 * 6.123785018920898
Epoch 1270, val loss: 0.9379664659500122
Epoch 1280, training loss: 0.6158639788627625 = 0.0028794731479138136 + 0.1 * 6.129844665527344
Epoch 1280, val loss: 0.9401002526283264
Epoch 1290, training loss: 0.6148454546928406 = 0.002840268425643444 + 0.1 * 6.120051383972168
Epoch 1290, val loss: 0.9422010183334351
Epoch 1300, training loss: 0.6159221529960632 = 0.0028021223843097687 + 0.1 * 6.131199836730957
Epoch 1300, val loss: 0.9442931413650513
Epoch 1310, training loss: 0.6144143342971802 = 0.0027642608620226383 + 0.1 * 6.116500377655029
Epoch 1310, val loss: 0.9463798403739929
Epoch 1320, training loss: 0.6162983179092407 = 0.0027275558095425367 + 0.1 * 6.135707378387451
Epoch 1320, val loss: 0.9484135508537292
Epoch 1330, training loss: 0.6144187450408936 = 0.0026915280614048243 + 0.1 * 6.11727237701416
Epoch 1330, val loss: 0.9504722356796265
Epoch 1340, training loss: 0.6144307255744934 = 0.0026568935718387365 + 0.1 * 6.117738246917725
Epoch 1340, val loss: 0.9524879455566406
Epoch 1350, training loss: 0.6142095923423767 = 0.0026226132176816463 + 0.1 * 6.115869522094727
Epoch 1350, val loss: 0.9545021057128906
Epoch 1360, training loss: 0.6165583729743958 = 0.0025891398545354605 + 0.1 * 6.1396918296813965
Epoch 1360, val loss: 0.9564754962921143
Epoch 1370, training loss: 0.6139999032020569 = 0.002556001069024205 + 0.1 * 6.114438533782959
Epoch 1370, val loss: 0.9584553837776184
Epoch 1380, training loss: 0.6130196452140808 = 0.0025244022253900766 + 0.1 * 6.104952812194824
Epoch 1380, val loss: 0.9603807330131531
Epoch 1390, training loss: 0.6142834424972534 = 0.002493589650839567 + 0.1 * 6.117898941040039
Epoch 1390, val loss: 0.9622937440872192
Epoch 1400, training loss: 0.6131424307823181 = 0.0024626641534268856 + 0.1 * 6.106797218322754
Epoch 1400, val loss: 0.9642059206962585
Epoch 1410, training loss: 0.6129201054573059 = 0.0024330653250217438 + 0.1 * 6.104870319366455
Epoch 1410, val loss: 0.9660791754722595
Epoch 1420, training loss: 0.614226222038269 = 0.002404171507805586 + 0.1 * 6.118220329284668
Epoch 1420, val loss: 0.967961311340332
Epoch 1430, training loss: 0.612725019454956 = 0.0023755484726279974 + 0.1 * 6.103494644165039
Epoch 1430, val loss: 0.9697968363761902
Epoch 1440, training loss: 0.6135415434837341 = 0.0023479710798710585 + 0.1 * 6.111936092376709
Epoch 1440, val loss: 0.9716318845748901
Epoch 1450, training loss: 0.61310875415802 = 0.0023202281445264816 + 0.1 * 6.107884883880615
Epoch 1450, val loss: 0.9734660983085632
Epoch 1460, training loss: 0.6124012470245361 = 0.002293673576787114 + 0.1 * 6.101075649261475
Epoch 1460, val loss: 0.975288450717926
Epoch 1470, training loss: 0.6122992634773254 = 0.0022679753601551056 + 0.1 * 6.10031270980835
Epoch 1470, val loss: 0.9770997166633606
Epoch 1480, training loss: 0.6126110553741455 = 0.002242553047835827 + 0.1 * 6.103684902191162
Epoch 1480, val loss: 0.9788764119148254
Epoch 1490, training loss: 0.6138678193092346 = 0.002217399189248681 + 0.1 * 6.116504192352295
Epoch 1490, val loss: 0.9806446433067322
Epoch 1500, training loss: 0.6121890544891357 = 0.0021927764173597097 + 0.1 * 6.09996223449707
Epoch 1500, val loss: 0.9823975563049316
Epoch 1510, training loss: 0.612863302230835 = 0.002168922917917371 + 0.1 * 6.1069440841674805
Epoch 1510, val loss: 0.9841359257698059
Epoch 1520, training loss: 0.6118773818016052 = 0.002145398873835802 + 0.1 * 6.097319602966309
Epoch 1520, val loss: 0.9858672022819519
Epoch 1530, training loss: 0.6124381422996521 = 0.00212260358966887 + 0.1 * 6.103155612945557
Epoch 1530, val loss: 0.9875711798667908
Epoch 1540, training loss: 0.6118624806404114 = 0.002100005280226469 + 0.1 * 6.097624778747559
Epoch 1540, val loss: 0.9892614483833313
Epoch 1550, training loss: 0.611571192741394 = 0.0020778574980795383 + 0.1 * 6.094933032989502
Epoch 1550, val loss: 0.9909447431564331
Epoch 1560, training loss: 0.6133047938346863 = 0.0020561886485666037 + 0.1 * 6.112485885620117
Epoch 1560, val loss: 0.9926129579544067
Epoch 1570, training loss: 0.6116706728935242 = 0.002034467877820134 + 0.1 * 6.0963616371154785
Epoch 1570, val loss: 0.9942644834518433
Epoch 1580, training loss: 0.6113184094429016 = 0.0020138598047196865 + 0.1 * 6.093045234680176
Epoch 1580, val loss: 0.995902955532074
Epoch 1590, training loss: 0.6121600866317749 = 0.0019933783914893866 + 0.1 * 6.1016669273376465
Epoch 1590, val loss: 0.9975271821022034
Epoch 1600, training loss: 0.6109243035316467 = 0.0019732543732970953 + 0.1 * 6.089510440826416
Epoch 1600, val loss: 0.9991323947906494
Epoch 1610, training loss: 0.6115127801895142 = 0.001953655621036887 + 0.1 * 6.095591068267822
Epoch 1610, val loss: 1.000687837600708
Epoch 1620, training loss: 0.611103355884552 = 0.001933964667841792 + 0.1 * 6.09169340133667
Epoch 1620, val loss: 1.0022763013839722
Epoch 1630, training loss: 0.6107072830200195 = 0.0019150188891217113 + 0.1 * 6.087922096252441
Epoch 1630, val loss: 1.0038130283355713
Epoch 1640, training loss: 0.614316463470459 = 0.0018964190967381 + 0.1 * 6.124200344085693
Epoch 1640, val loss: 1.005365014076233
Epoch 1650, training loss: 0.6116575598716736 = 0.0018777715740725398 + 0.1 * 6.097797393798828
Epoch 1650, val loss: 1.0068947076797485
Epoch 1660, training loss: 0.6108150482177734 = 0.001860095071606338 + 0.1 * 6.089549541473389
Epoch 1660, val loss: 1.0084127187728882
Epoch 1670, training loss: 0.6108580231666565 = 0.0018426100723445415 + 0.1 * 6.090153694152832
Epoch 1670, val loss: 1.0099114179611206
Epoch 1680, training loss: 0.6102386713027954 = 0.0018252028385177255 + 0.1 * 6.084134578704834
Epoch 1680, val loss: 1.0113855600357056
Epoch 1690, training loss: 0.6118617057800293 = 0.001808225060813129 + 0.1 * 6.100534915924072
Epoch 1690, val loss: 1.0128636360168457
Epoch 1700, training loss: 0.610499382019043 = 0.0017913177143782377 + 0.1 * 6.087080955505371
Epoch 1700, val loss: 1.0143678188323975
Epoch 1710, training loss: 0.6109358072280884 = 0.001774875563569367 + 0.1 * 6.091609477996826
Epoch 1710, val loss: 1.0158278942108154
Epoch 1720, training loss: 0.6098999977111816 = 0.0017586761387065053 + 0.1 * 6.0814127922058105
Epoch 1720, val loss: 1.0172977447509766
Epoch 1730, training loss: 0.6109559535980225 = 0.0017427796265110373 + 0.1 * 6.092131614685059
Epoch 1730, val loss: 1.0187361240386963
Epoch 1740, training loss: 0.6104021668434143 = 0.0017270030220970511 + 0.1 * 6.086751461029053
Epoch 1740, val loss: 1.0201994180679321
Epoch 1750, training loss: 0.6097952723503113 = 0.0017116025555878878 + 0.1 * 6.080836772918701
Epoch 1750, val loss: 1.021623969078064
Epoch 1760, training loss: 0.6103261113166809 = 0.0016965793911367655 + 0.1 * 6.086295127868652
Epoch 1760, val loss: 1.0230352878570557
Epoch 1770, training loss: 0.6103891134262085 = 0.0016816005809232593 + 0.1 * 6.0870747566223145
Epoch 1770, val loss: 1.024451494216919
Epoch 1780, training loss: 0.6092100143432617 = 0.0016668443568050861 + 0.1 * 6.075431823730469
Epoch 1780, val loss: 1.0258508920669556
Epoch 1790, training loss: 0.6101465225219727 = 0.001652656588703394 + 0.1 * 6.0849385261535645
Epoch 1790, val loss: 1.0272290706634521
Epoch 1800, training loss: 0.6097836494445801 = 0.001638500951230526 + 0.1 * 6.081451416015625
Epoch 1800, val loss: 1.02860689163208
Epoch 1810, training loss: 0.6093355417251587 = 0.0016244524158537388 + 0.1 * 6.077110767364502
Epoch 1810, val loss: 1.0299772024154663
Epoch 1820, training loss: 0.6102971434593201 = 0.0016107639530673623 + 0.1 * 6.086863994598389
Epoch 1820, val loss: 1.0313355922698975
Epoch 1830, training loss: 0.6091969609260559 = 0.001597174326889217 + 0.1 * 6.075997829437256
Epoch 1830, val loss: 1.0326893329620361
Epoch 1840, training loss: 0.6096219420433044 = 0.0015839351108297706 + 0.1 * 6.080379962921143
Epoch 1840, val loss: 1.0340354442596436
Epoch 1850, training loss: 0.6091290712356567 = 0.0015708074206486344 + 0.1 * 6.075582504272461
Epoch 1850, val loss: 1.0353933572769165
Epoch 1860, training loss: 0.6092821955680847 = 0.0015579494647681713 + 0.1 * 6.077242374420166
Epoch 1860, val loss: 1.0367096662521362
Epoch 1870, training loss: 0.6101009249687195 = 0.0015453000087291002 + 0.1 * 6.0855560302734375
Epoch 1870, val loss: 1.0380162000656128
Epoch 1880, training loss: 0.6087095141410828 = 0.0015326360007748008 + 0.1 * 6.071768760681152
Epoch 1880, val loss: 1.0393497943878174
Epoch 1890, training loss: 0.6086567044258118 = 0.0015204945812001824 + 0.1 * 6.071361541748047
Epoch 1890, val loss: 1.0406347513198853
Epoch 1900, training loss: 0.6102741360664368 = 0.0015083628240972757 + 0.1 * 6.087657928466797
Epoch 1900, val loss: 1.041939616203308
Epoch 1910, training loss: 0.6091021299362183 = 0.0014962437562644482 + 0.1 * 6.076058864593506
Epoch 1910, val loss: 1.0432342290878296
Epoch 1920, training loss: 0.6089023947715759 = 0.0014844998950138688 + 0.1 * 6.074178695678711
Epoch 1920, val loss: 1.0445278882980347
Epoch 1930, training loss: 0.608379602432251 = 0.0014729596441611648 + 0.1 * 6.069066524505615
Epoch 1930, val loss: 1.0457994937896729
Epoch 1940, training loss: 0.6089103817939758 = 0.0014614855172112584 + 0.1 * 6.074488639831543
Epoch 1940, val loss: 1.0470664501190186
Epoch 1950, training loss: 0.6084133982658386 = 0.0014502478297799826 + 0.1 * 6.069631576538086
Epoch 1950, val loss: 1.0483226776123047
Epoch 1960, training loss: 0.6093899011611938 = 0.0014392194570973516 + 0.1 * 6.079506874084473
Epoch 1960, val loss: 1.049548625946045
Epoch 1970, training loss: 0.6081920862197876 = 0.0014282695483416319 + 0.1 * 6.067638397216797
Epoch 1970, val loss: 1.0507956743240356
Epoch 1980, training loss: 0.609765350818634 = 0.001417525578290224 + 0.1 * 6.083477973937988
Epoch 1980, val loss: 1.0520151853561401
Epoch 1990, training loss: 0.6085483431816101 = 0.0014067639131098986 + 0.1 * 6.071415424346924
Epoch 1990, val loss: 1.053247332572937
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7974328994750977 = 1.9600530862808228 + 0.1 * 8.373798370361328
Epoch 0, val loss: 1.9618828296661377
Epoch 10, training loss: 2.7865138053894043 = 1.949162244796753 + 0.1 * 8.373516082763672
Epoch 10, val loss: 1.9500359296798706
Epoch 20, training loss: 2.773104429244995 = 1.9359289407730103 + 0.1 * 8.37175464630127
Epoch 20, val loss: 1.9353759288787842
Epoch 30, training loss: 2.7536063194274902 = 1.917643666267395 + 0.1 * 8.359625816345215
Epoch 30, val loss: 1.9148417711257935
Epoch 40, training loss: 2.7197136878967285 = 1.8911447525024414 + 0.1 * 8.285689353942871
Epoch 40, val loss: 1.8852801322937012
Epoch 50, training loss: 2.640106439590454 = 1.8560222387313843 + 0.1 * 7.840841293334961
Epoch 50, val loss: 1.8479925394058228
Epoch 60, training loss: 2.555966377258301 = 1.8186655044555664 + 0.1 * 7.373008728027344
Epoch 60, val loss: 1.8108490705490112
Epoch 70, training loss: 2.4787769317626953 = 1.780591607093811 + 0.1 * 6.981852054595947
Epoch 70, val loss: 1.7746442556381226
Epoch 80, training loss: 2.4237353801727295 = 1.7413415908813477 + 0.1 * 6.823938369750977
Epoch 80, val loss: 1.739059567451477
Epoch 90, training loss: 2.3713889122009277 = 1.6967309713363647 + 0.1 * 6.746579647064209
Epoch 90, val loss: 1.700160026550293
Epoch 100, training loss: 2.3090662956237793 = 1.6395059823989868 + 0.1 * 6.695603370666504
Epoch 100, val loss: 1.6525810956954956
Epoch 110, training loss: 2.2308578491210938 = 1.565458059310913 + 0.1 * 6.653998374938965
Epoch 110, val loss: 1.592032790184021
Epoch 120, training loss: 2.1369919776916504 = 1.4745897054672241 + 0.1 * 6.624022006988525
Epoch 120, val loss: 1.5180433988571167
Epoch 130, training loss: 2.032271385192871 = 1.3718547821044922 + 0.1 * 6.604166507720947
Epoch 130, val loss: 1.4356300830841064
Epoch 140, training loss: 1.92018723487854 = 1.2613784074783325 + 0.1 * 6.588088512420654
Epoch 140, val loss: 1.3501068353652954
Epoch 150, training loss: 1.8025940656661987 = 1.1452637910842896 + 0.1 * 6.573302745819092
Epoch 150, val loss: 1.2620840072631836
Epoch 160, training loss: 1.6840369701385498 = 1.0276620388031006 + 0.1 * 6.563748359680176
Epoch 160, val loss: 1.1736838817596436
Epoch 170, training loss: 1.5721399784088135 = 0.9175410866737366 + 0.1 * 6.545989036560059
Epoch 170, val loss: 1.0912704467773438
Epoch 180, training loss: 1.47245192527771 = 0.8193671107292175 + 0.1 * 6.530848026275635
Epoch 180, val loss: 1.0176931619644165
Epoch 190, training loss: 1.3865617513656616 = 0.7346445322036743 + 0.1 * 6.519172191619873
Epoch 190, val loss: 0.954900324344635
Epoch 200, training loss: 1.3136637210845947 = 0.6627974510192871 + 0.1 * 6.508663177490234
Epoch 200, val loss: 0.9027650952339172
Epoch 210, training loss: 1.2489476203918457 = 0.59956294298172 + 0.1 * 6.493846416473389
Epoch 210, val loss: 0.8581790328025818
Epoch 220, training loss: 1.1921855211257935 = 0.5422660708427429 + 0.1 * 6.499194145202637
Epoch 220, val loss: 0.8197649121284485
Epoch 230, training loss: 1.1378196477890015 = 0.4907880425453186 + 0.1 * 6.470315933227539
Epoch 230, val loss: 0.7880591154098511
Epoch 240, training loss: 1.0889978408813477 = 0.44394147396087646 + 0.1 * 6.450563430786133
Epoch 240, val loss: 0.7622854709625244
Epoch 250, training loss: 1.044698715209961 = 0.40152183175086975 + 0.1 * 6.431768417358398
Epoch 250, val loss: 0.7424136996269226
Epoch 260, training loss: 1.0053105354309082 = 0.36326324939727783 + 0.1 * 6.420472145080566
Epoch 260, val loss: 0.7277328372001648
Epoch 270, training loss: 0.9691394567489624 = 0.3285517990589142 + 0.1 * 6.405876636505127
Epoch 270, val loss: 0.7168050408363342
Epoch 280, training loss: 0.9366068840026855 = 0.29702529311180115 + 0.1 * 6.395816326141357
Epoch 280, val loss: 0.7088848352432251
Epoch 290, training loss: 0.9055889844894409 = 0.26826462149620056 + 0.1 * 6.37324333190918
Epoch 290, val loss: 0.703432559967041
Epoch 300, training loss: 0.8822596073150635 = 0.24184992909431458 + 0.1 * 6.404096603393555
Epoch 300, val loss: 0.7000653743743896
Epoch 310, training loss: 0.8535031080245972 = 0.2180374413728714 + 0.1 * 6.35465669631958
Epoch 310, val loss: 0.6987140774726868
Epoch 320, training loss: 0.8311247229576111 = 0.19651414453983307 + 0.1 * 6.346106052398682
Epoch 320, val loss: 0.6993582248687744
Epoch 330, training loss: 0.8110060095787048 = 0.17731404304504395 + 0.1 * 6.336919784545898
Epoch 330, val loss: 0.701668381690979
Epoch 340, training loss: 0.7924097776412964 = 0.16023653745651245 + 0.1 * 6.321732044219971
Epoch 340, val loss: 0.7054786086082458
Epoch 350, training loss: 0.7773621678352356 = 0.14501802623271942 + 0.1 * 6.323441028594971
Epoch 350, val loss: 0.7107105255126953
Epoch 360, training loss: 0.762523889541626 = 0.13156017661094666 + 0.1 * 6.309637546539307
Epoch 360, val loss: 0.7171599864959717
Epoch 370, training loss: 0.7507493495941162 = 0.11963444203138351 + 0.1 * 6.311148643493652
Epoch 370, val loss: 0.7247049808502197
Epoch 380, training loss: 0.7392024993896484 = 0.1090652197599411 + 0.1 * 6.301372528076172
Epoch 380, val loss: 0.7329785823822021
Epoch 390, training loss: 0.7301557064056396 = 0.09970653057098389 + 0.1 * 6.3044915199279785
Epoch 390, val loss: 0.7419229745864868
Epoch 400, training loss: 0.7200445532798767 = 0.09138142317533493 + 0.1 * 6.286631107330322
Epoch 400, val loss: 0.7513455748558044
Epoch 410, training loss: 0.7128258347511292 = 0.08391756564378738 + 0.1 * 6.2890825271606445
Epoch 410, val loss: 0.7612188458442688
Epoch 420, training loss: 0.7051469683647156 = 0.07723349332809448 + 0.1 * 6.279134750366211
Epoch 420, val loss: 0.7713078856468201
Epoch 430, training loss: 0.6995947957038879 = 0.07121975719928741 + 0.1 * 6.283750534057617
Epoch 430, val loss: 0.7815351486206055
Epoch 440, training loss: 0.6921793818473816 = 0.06582684069871902 + 0.1 * 6.263525485992432
Epoch 440, val loss: 0.7918362021446228
Epoch 450, training loss: 0.6875582337379456 = 0.06096218153834343 + 0.1 * 6.265960216522217
Epoch 450, val loss: 0.8021976947784424
Epoch 460, training loss: 0.6831594705581665 = 0.0565769299864769 + 0.1 * 6.2658257484436035
Epoch 460, val loss: 0.8124710917472839
Epoch 470, training loss: 0.6775381565093994 = 0.052621155977249146 + 0.1 * 6.249169826507568
Epoch 470, val loss: 0.8226333260536194
Epoch 480, training loss: 0.6728987097740173 = 0.049034204334020615 + 0.1 * 6.238645076751709
Epoch 480, val loss: 0.8327024579048157
Epoch 490, training loss: 0.6708155274391174 = 0.04577461630105972 + 0.1 * 6.25040864944458
Epoch 490, val loss: 0.8426471948623657
Epoch 500, training loss: 0.6668490171432495 = 0.042820144444704056 + 0.1 * 6.240288734436035
Epoch 500, val loss: 0.8524075150489807
Epoch 510, training loss: 0.6633901596069336 = 0.040139757096767426 + 0.1 * 6.232504367828369
Epoch 510, val loss: 0.8619826436042786
Epoch 520, training loss: 0.6613581776618958 = 0.03769586235284805 + 0.1 * 6.2366228103637695
Epoch 520, val loss: 0.8713686466217041
Epoch 530, training loss: 0.6572818756103516 = 0.03546939045190811 + 0.1 * 6.2181243896484375
Epoch 530, val loss: 0.880508542060852
Epoch 540, training loss: 0.657198965549469 = 0.033427610993385315 + 0.1 * 6.23771333694458
Epoch 540, val loss: 0.8895418643951416
Epoch 550, training loss: 0.653911292552948 = 0.03155829384922981 + 0.1 * 6.223530292510986
Epoch 550, val loss: 0.8983513116836548
Epoch 560, training loss: 0.6514387130737305 = 0.029840417206287384 + 0.1 * 6.215982913970947
Epoch 560, val loss: 0.9069710373878479
Epoch 570, training loss: 0.6491979956626892 = 0.028257636353373528 + 0.1 * 6.2094035148620605
Epoch 570, val loss: 0.9154418706893921
Epoch 580, training loss: 0.6471464037895203 = 0.02680010348558426 + 0.1 * 6.203463077545166
Epoch 580, val loss: 0.9237138032913208
Epoch 590, training loss: 0.6454376578330994 = 0.025449013337492943 + 0.1 * 6.199886322021484
Epoch 590, val loss: 0.9318742156028748
Epoch 600, training loss: 0.6451425552368164 = 0.024198396131396294 + 0.1 * 6.209441184997559
Epoch 600, val loss: 0.9398757219314575
Epoch 610, training loss: 0.6424394845962524 = 0.023044437170028687 + 0.1 * 6.193950653076172
Epoch 610, val loss: 0.9476732611656189
Epoch 620, training loss: 0.6412487030029297 = 0.021971067413687706 + 0.1 * 6.192776203155518
Epoch 620, val loss: 0.9553609490394592
Epoch 630, training loss: 0.6396015286445618 = 0.020970694720745087 + 0.1 * 6.186307907104492
Epoch 630, val loss: 0.9628828167915344
Epoch 640, training loss: 0.6383263468742371 = 0.020040836185216904 + 0.1 * 6.182855129241943
Epoch 640, val loss: 0.9702473878860474
Epoch 650, training loss: 0.6375586986541748 = 0.019170187413692474 + 0.1 * 6.183885097503662
Epoch 650, val loss: 0.9774839282035828
Epoch 660, training loss: 0.6363714337348938 = 0.018355803564190865 + 0.1 * 6.180156230926514
Epoch 660, val loss: 0.9845852255821228
Epoch 670, training loss: 0.635308027267456 = 0.017595399171113968 + 0.1 * 6.177125930786133
Epoch 670, val loss: 0.9915193915367126
Epoch 680, training loss: 0.6353957653045654 = 0.01688108779489994 + 0.1 * 6.185146331787109
Epoch 680, val loss: 0.9983941912651062
Epoch 690, training loss: 0.6334558129310608 = 0.016209984198212624 + 0.1 * 6.172458648681641
Epoch 690, val loss: 1.0050755739212036
Epoch 700, training loss: 0.6352915167808533 = 0.015579569153487682 + 0.1 * 6.197119235992432
Epoch 700, val loss: 1.0116734504699707
Epoch 710, training loss: 0.6325480341911316 = 0.014988183043897152 + 0.1 * 6.17559814453125
Epoch 710, val loss: 1.0181457996368408
Epoch 720, training loss: 0.6312511563301086 = 0.01443093828856945 + 0.1 * 6.168201923370361
Epoch 720, val loss: 1.024479627609253
Epoch 730, training loss: 0.6300870180130005 = 0.013904288411140442 + 0.1 * 6.161827087402344
Epoch 730, val loss: 1.03074312210083
Epoch 740, training loss: 0.6305795907974243 = 0.013408229686319828 + 0.1 * 6.171713352203369
Epoch 740, val loss: 1.0368403196334839
Epoch 750, training loss: 0.6289218068122864 = 0.012940047308802605 + 0.1 * 6.159817218780518
Epoch 750, val loss: 1.042864203453064
Epoch 760, training loss: 0.6282641291618347 = 0.012496919371187687 + 0.1 * 6.157671928405762
Epoch 760, val loss: 1.0487499237060547
Epoch 770, training loss: 0.6278833746910095 = 0.012076325714588165 + 0.1 * 6.1580705642700195
Epoch 770, val loss: 1.0545233488082886
Epoch 780, training loss: 0.6268960237503052 = 0.0116775156930089 + 0.1 * 6.152184963226318
Epoch 780, val loss: 1.0602319240570068
Epoch 790, training loss: 0.6269354820251465 = 0.011299144476652145 + 0.1 * 6.156363487243652
Epoch 790, val loss: 1.0658570528030396
Epoch 800, training loss: 0.6271303296089172 = 0.010940488427877426 + 0.1 * 6.161898136138916
Epoch 800, val loss: 1.071351408958435
Epoch 810, training loss: 0.6252333521842957 = 0.010599851608276367 + 0.1 * 6.146334648132324
Epoch 810, val loss: 1.0767521858215332
Epoch 820, training loss: 0.625613272190094 = 0.010276621207594872 + 0.1 * 6.153366565704346
Epoch 820, val loss: 1.082047700881958
Epoch 830, training loss: 0.6247648596763611 = 0.00996825285255909 + 0.1 * 6.147965431213379
Epoch 830, val loss: 1.0872557163238525
Epoch 840, training loss: 0.6246718168258667 = 0.009674754925072193 + 0.1 * 6.149970531463623
Epoch 840, val loss: 1.0923901796340942
Epoch 850, training loss: 0.6240079998970032 = 0.009394528344273567 + 0.1 * 6.146134376525879
Epoch 850, val loss: 1.0974394083023071
Epoch 860, training loss: 0.6240437030792236 = 0.009128376841545105 + 0.1 * 6.149152755737305
Epoch 860, val loss: 1.102388620376587
Epoch 870, training loss: 0.6235379576683044 = 0.00887348409742117 + 0.1 * 6.146644592285156
Epoch 870, val loss: 1.1072474718093872
Epoch 880, training loss: 0.6216267347335815 = 0.00862998515367508 + 0.1 * 6.12996768951416
Epoch 880, val loss: 1.1120630502700806
Epoch 890, training loss: 0.6224486231803894 = 0.008397654630243778 + 0.1 * 6.140509605407715
Epoch 890, val loss: 1.1167789697647095
Epoch 900, training loss: 0.6244522333145142 = 0.008174597285687923 + 0.1 * 6.162775993347168
Epoch 900, val loss: 1.1214252710342407
Epoch 910, training loss: 0.6219606399536133 = 0.007961483672261238 + 0.1 * 6.139991283416748
Epoch 910, val loss: 1.1260285377502441
Epoch 920, training loss: 0.6218372583389282 = 0.007758193649351597 + 0.1 * 6.1407904624938965
Epoch 920, val loss: 1.130530595779419
Epoch 930, training loss: 0.6200722455978394 = 0.007562797982245684 + 0.1 * 6.125094413757324
Epoch 930, val loss: 1.1349347829818726
Epoch 940, training loss: 0.6216955780982971 = 0.007375496439635754 + 0.1 * 6.143200874328613
Epoch 940, val loss: 1.139309287071228
Epoch 950, training loss: 0.6195626258850098 = 0.007195776328444481 + 0.1 * 6.123668670654297
Epoch 950, val loss: 1.1436210870742798
Epoch 960, training loss: 0.6201983094215393 = 0.007023492828011513 + 0.1 * 6.131747722625732
Epoch 960, val loss: 1.1478520631790161
Epoch 970, training loss: 0.6188556551933289 = 0.006857762578874826 + 0.1 * 6.119978904724121
Epoch 970, val loss: 1.1520332098007202
Epoch 980, training loss: 0.6206415295600891 = 0.006698048207908869 + 0.1 * 6.139434337615967
Epoch 980, val loss: 1.1561552286148071
Epoch 990, training loss: 0.6195241212844849 = 0.0065443008206784725 + 0.1 * 6.12979793548584
Epoch 990, val loss: 1.1602431535720825
Epoch 1000, training loss: 0.6182908415794373 = 0.006397041957825422 + 0.1 * 6.1189374923706055
Epoch 1000, val loss: 1.164212942123413
Epoch 1010, training loss: 0.6185216307640076 = 0.006255097687244415 + 0.1 * 6.1226654052734375
Epoch 1010, val loss: 1.168152928352356
Epoch 1020, training loss: 0.6186760067939758 = 0.006117894314229488 + 0.1 * 6.12558126449585
Epoch 1020, val loss: 1.1720551252365112
Epoch 1030, training loss: 0.6184168457984924 = 0.005985601339489222 + 0.1 * 6.124311923980713
Epoch 1030, val loss: 1.1759274005889893
Epoch 1040, training loss: 0.6178642511367798 = 0.005858380347490311 + 0.1 * 6.120058536529541
Epoch 1040, val loss: 1.1797089576721191
Epoch 1050, training loss: 0.6182196140289307 = 0.005736170802265406 + 0.1 * 6.124834060668945
Epoch 1050, val loss: 1.1834518909454346
Epoch 1060, training loss: 0.6166663765907288 = 0.005617871880531311 + 0.1 * 6.110485076904297
Epoch 1060, val loss: 1.1871405839920044
Epoch 1070, training loss: 0.6177191734313965 = 0.005503764376044273 + 0.1 * 6.1221537590026855
Epoch 1070, val loss: 1.1907570362091064
Epoch 1080, training loss: 0.6171053051948547 = 0.005392895545810461 + 0.1 * 6.117124080657959
Epoch 1080, val loss: 1.194353461265564
Epoch 1090, training loss: 0.6164076328277588 = 0.005285813473165035 + 0.1 * 6.111217975616455
Epoch 1090, val loss: 1.1979035139083862
Epoch 1100, training loss: 0.6169293522834778 = 0.005182458087801933 + 0.1 * 6.11746883392334
Epoch 1100, val loss: 1.2014143466949463
Epoch 1110, training loss: 0.6157068610191345 = 0.005082629155367613 + 0.1 * 6.106242656707764
Epoch 1110, val loss: 1.204883337020874
Epoch 1120, training loss: 0.6163723468780518 = 0.004985880572348833 + 0.1 * 6.113864898681641
Epoch 1120, val loss: 1.2082995176315308
Epoch 1130, training loss: 0.6172873973846436 = 0.004891906399279833 + 0.1 * 6.1239542961120605
Epoch 1130, val loss: 1.211648941040039
Epoch 1140, training loss: 0.6155586838722229 = 0.004801067057996988 + 0.1 * 6.1075758934021
Epoch 1140, val loss: 1.2150540351867676
Epoch 1150, training loss: 0.6175121665000916 = 0.004713323432952166 + 0.1 * 6.127988338470459
Epoch 1150, val loss: 1.2183281183242798
Epoch 1160, training loss: 0.6151222586631775 = 0.004628096707165241 + 0.1 * 6.104941368103027
Epoch 1160, val loss: 1.2215725183486938
Epoch 1170, training loss: 0.6148066520690918 = 0.0045460062101483345 + 0.1 * 6.102606296539307
Epoch 1170, val loss: 1.2247532606124878
Epoch 1180, training loss: 0.6157733798027039 = 0.004466061480343342 + 0.1 * 6.113072872161865
Epoch 1180, val loss: 1.2278881072998047
Epoch 1190, training loss: 0.6137935519218445 = 0.004388100001960993 + 0.1 * 6.094054698944092
Epoch 1190, val loss: 1.231048583984375
Epoch 1200, training loss: 0.6147632002830505 = 0.00431283051148057 + 0.1 * 6.104503631591797
Epoch 1200, val loss: 1.2341097593307495
Epoch 1210, training loss: 0.6157779693603516 = 0.0042395140044391155 + 0.1 * 6.115384578704834
Epoch 1210, val loss: 1.237149715423584
Epoch 1220, training loss: 0.6137534379959106 = 0.0041683269664645195 + 0.1 * 6.095850944519043
Epoch 1220, val loss: 1.2401679754257202
Epoch 1230, training loss: 0.6142933368682861 = 0.0040994309820234776 + 0.1 * 6.1019392013549805
Epoch 1230, val loss: 1.2430881261825562
Epoch 1240, training loss: 0.6133968830108643 = 0.004032440017908812 + 0.1 * 6.093644618988037
Epoch 1240, val loss: 1.245987892150879
Epoch 1250, training loss: 0.6134830713272095 = 0.0039674085564911366 + 0.1 * 6.095156669616699
Epoch 1250, val loss: 1.2488454580307007
Epoch 1260, training loss: 0.6141691207885742 = 0.0039038299582898617 + 0.1 * 6.1026530265808105
Epoch 1260, val loss: 1.2517297267913818
Epoch 1270, training loss: 0.6134087443351746 = 0.003842148697003722 + 0.1 * 6.095665454864502
Epoch 1270, val loss: 1.2545089721679688
Epoch 1280, training loss: 0.6133581399917603 = 0.0037824588362127542 + 0.1 * 6.095757007598877
Epoch 1280, val loss: 1.2572671175003052
Epoch 1290, training loss: 0.6132752299308777 = 0.0037243671249598265 + 0.1 * 6.095508098602295
Epoch 1290, val loss: 1.2599897384643555
Epoch 1300, training loss: 0.6129087209701538 = 0.003667446319013834 + 0.1 * 6.09241247177124
Epoch 1300, val loss: 1.2626872062683105
Epoch 1310, training loss: 0.6130523681640625 = 0.0036121478769928217 + 0.1 * 6.094401836395264
Epoch 1310, val loss: 1.2653586864471436
Epoch 1320, training loss: 0.6123673319816589 = 0.0035586170852184296 + 0.1 * 6.08808708190918
Epoch 1320, val loss: 1.267980933189392
Epoch 1330, training loss: 0.6126127243041992 = 0.0035060096997767687 + 0.1 * 6.091066837310791
Epoch 1330, val loss: 1.2706316709518433
Epoch 1340, training loss: 0.6117081642150879 = 0.003454651217907667 + 0.1 * 6.082535266876221
Epoch 1340, val loss: 1.2732393741607666
Epoch 1350, training loss: 0.6133894920349121 = 0.0034046610817313194 + 0.1 * 6.09984827041626
Epoch 1350, val loss: 1.275843620300293
Epoch 1360, training loss: 0.6122966408729553 = 0.003355954075232148 + 0.1 * 6.089406490325928
Epoch 1360, val loss: 1.2784347534179688
Epoch 1370, training loss: 0.6126745343208313 = 0.003308286890387535 + 0.1 * 6.093662261962891
Epoch 1370, val loss: 1.280989170074463
Epoch 1380, training loss: 0.6120913624763489 = 0.003262269776314497 + 0.1 * 6.088290691375732
Epoch 1380, val loss: 1.283458948135376
Epoch 1390, training loss: 0.6118337512016296 = 0.0032173634972423315 + 0.1 * 6.0861639976501465
Epoch 1390, val loss: 1.2859015464782715
Epoch 1400, training loss: 0.6122531294822693 = 0.0031734390649944544 + 0.1 * 6.090796947479248
Epoch 1400, val loss: 1.2883327007293701
Epoch 1410, training loss: 0.6113671660423279 = 0.003130079247057438 + 0.1 * 6.082370758056641
Epoch 1410, val loss: 1.2907791137695312
Epoch 1420, training loss: 0.6108702421188354 = 0.0030881708953529596 + 0.1 * 6.077820777893066
Epoch 1420, val loss: 1.293166160583496
Epoch 1430, training loss: 0.6117218136787415 = 0.0030472129583358765 + 0.1 * 6.086745738983154
Epoch 1430, val loss: 1.2955597639083862
Epoch 1440, training loss: 0.6113916039466858 = 0.0030071630608290434 + 0.1 * 6.083844184875488
Epoch 1440, val loss: 1.2979196310043335
Epoch 1450, training loss: 0.6120591759681702 = 0.0029679564759135246 + 0.1 * 6.090911865234375
Epoch 1450, val loss: 1.3002793788909912
Epoch 1460, training loss: 0.6109154224395752 = 0.0029297340661287308 + 0.1 * 6.0798563957214355
Epoch 1460, val loss: 1.3026189804077148
Epoch 1470, training loss: 0.6111510992050171 = 0.0028925363440066576 + 0.1 * 6.082585334777832
Epoch 1470, val loss: 1.3049392700195312
Epoch 1480, training loss: 0.6112620234489441 = 0.0028561842627823353 + 0.1 * 6.0840582847595215
Epoch 1480, val loss: 1.3072043657302856
Epoch 1490, training loss: 0.6105141639709473 = 0.0028203832916915417 + 0.1 * 6.076937675476074
Epoch 1490, val loss: 1.3094748258590698
Epoch 1500, training loss: 0.610752522945404 = 0.0027855790685862303 + 0.1 * 6.07966947555542
Epoch 1500, val loss: 1.3116635084152222
Epoch 1510, training loss: 0.6112356185913086 = 0.0027514067478477955 + 0.1 * 6.084842205047607
Epoch 1510, val loss: 1.3138841390609741
Epoch 1520, training loss: 0.6104946136474609 = 0.002718056086450815 + 0.1 * 6.077765464782715
Epoch 1520, val loss: 1.3160666227340698
Epoch 1530, training loss: 0.6103082895278931 = 0.002685477724298835 + 0.1 * 6.07622766494751
Epoch 1530, val loss: 1.3182162046432495
Epoch 1540, training loss: 0.6108606457710266 = 0.002653481438755989 + 0.1 * 6.082071781158447
Epoch 1540, val loss: 1.3203626871109009
Epoch 1550, training loss: 0.6108697056770325 = 0.0026221221778541803 + 0.1 * 6.082475662231445
Epoch 1550, val loss: 1.3225120306015015
Epoch 1560, training loss: 0.61087965965271 = 0.0025915608275681734 + 0.1 * 6.082880973815918
Epoch 1560, val loss: 1.3246705532073975
Epoch 1570, training loss: 0.6092300415039062 = 0.002561640925705433 + 0.1 * 6.066684246063232
Epoch 1570, val loss: 1.3267048597335815
Epoch 1580, training loss: 0.6099068522453308 = 0.0025324877351522446 + 0.1 * 6.07374382019043
Epoch 1580, val loss: 1.3287137746810913
Epoch 1590, training loss: 0.6098294854164124 = 0.0025035589933395386 + 0.1 * 6.073258876800537
Epoch 1590, val loss: 1.3308366537094116
Epoch 1600, training loss: 0.6095696687698364 = 0.0024752430617809296 + 0.1 * 6.070943832397461
Epoch 1600, val loss: 1.3328901529312134
Epoch 1610, training loss: 0.6107580661773682 = 0.0024476307444274426 + 0.1 * 6.083104610443115
Epoch 1610, val loss: 1.334917426109314
Epoch 1620, training loss: 0.6092098951339722 = 0.0024204275105148554 + 0.1 * 6.067894458770752
Epoch 1620, val loss: 1.3369303941726685
Epoch 1630, training loss: 0.6112748384475708 = 0.00239391066133976 + 0.1 * 6.088809013366699
Epoch 1630, val loss: 1.3388952016830444
Epoch 1640, training loss: 0.6094447374343872 = 0.0023678524885326624 + 0.1 * 6.0707688331604
Epoch 1640, val loss: 1.340898871421814
Epoch 1650, training loss: 0.6091288924217224 = 0.0023423777893185616 + 0.1 * 6.067864894866943
Epoch 1650, val loss: 1.3428101539611816
Epoch 1660, training loss: 0.6095441579818726 = 0.0023173971567302942 + 0.1 * 6.072268009185791
Epoch 1660, val loss: 1.3447234630584717
Epoch 1670, training loss: 0.6098222136497498 = 0.0022928756661713123 + 0.1 * 6.075293064117432
Epoch 1670, val loss: 1.346642017364502
Epoch 1680, training loss: 0.6095899343490601 = 0.0022686957381665707 + 0.1 * 6.073212623596191
Epoch 1680, val loss: 1.3485651016235352
Epoch 1690, training loss: 0.6089496612548828 = 0.002245072042569518 + 0.1 * 6.06704568862915
Epoch 1690, val loss: 1.3504809141159058
Epoch 1700, training loss: 0.6081511378288269 = 0.002222060691565275 + 0.1 * 6.059290409088135
Epoch 1700, val loss: 1.3523139953613281
Epoch 1710, training loss: 0.6093517541885376 = 0.0021993094123899937 + 0.1 * 6.071524620056152
Epoch 1710, val loss: 1.3541724681854248
Epoch 1720, training loss: 0.6090072989463806 = 0.0021767786238342524 + 0.1 * 6.068305492401123
Epoch 1720, val loss: 1.3560609817504883
Epoch 1730, training loss: 0.6087214350700378 = 0.0021548361983150244 + 0.1 * 6.0656657218933105
Epoch 1730, val loss: 1.357910394668579
Epoch 1740, training loss: 0.609718382358551 = 0.00213325722143054 + 0.1 * 6.075850963592529
Epoch 1740, val loss: 1.359769582748413
Epoch 1750, training loss: 0.6085360646247864 = 0.0021119622979313135 + 0.1 * 6.0642409324646
Epoch 1750, val loss: 1.3616156578063965
Epoch 1760, training loss: 0.6092475056648254 = 0.002091268077492714 + 0.1 * 6.07156229019165
Epoch 1760, val loss: 1.3633450269699097
Epoch 1770, training loss: 0.60891193151474 = 0.0020709012169390917 + 0.1 * 6.068410396575928
Epoch 1770, val loss: 1.3651341199874878
Epoch 1780, training loss: 0.6079330444335938 = 0.0020508677698671818 + 0.1 * 6.058821678161621
Epoch 1780, val loss: 1.3668861389160156
Epoch 1790, training loss: 0.6093233823776245 = 0.002031289506703615 + 0.1 * 6.072920799255371
Epoch 1790, val loss: 1.3686307668685913
Epoch 1800, training loss: 0.6077522039413452 = 0.002011786215007305 + 0.1 * 6.057404041290283
Epoch 1800, val loss: 1.3703482151031494
Epoch 1810, training loss: 0.6106201410293579 = 0.0019927395042032003 + 0.1 * 6.086273670196533
Epoch 1810, val loss: 1.3721177577972412
Epoch 1820, training loss: 0.608514130115509 = 0.001974009443074465 + 0.1 * 6.065401554107666
Epoch 1820, val loss: 1.3738348484039307
Epoch 1830, training loss: 0.6075999736785889 = 0.0019557797349989414 + 0.1 * 6.056441307067871
Epoch 1830, val loss: 1.3755180835723877
Epoch 1840, training loss: 0.6088070869445801 = 0.00193792802747339 + 0.1 * 6.068691730499268
Epoch 1840, val loss: 1.3771398067474365
Epoch 1850, training loss: 0.607487142086029 = 0.0019201845861971378 + 0.1 * 6.055669784545898
Epoch 1850, val loss: 1.3787906169891357
Epoch 1860, training loss: 0.6083301305770874 = 0.0019027673406526446 + 0.1 * 6.064273834228516
Epoch 1860, val loss: 1.3804607391357422
Epoch 1870, training loss: 0.6076712608337402 = 0.001885704230517149 + 0.1 * 6.057855606079102
Epoch 1870, val loss: 1.382070779800415
Epoch 1880, training loss: 0.6090133190155029 = 0.001868911669589579 + 0.1 * 6.071444034576416
Epoch 1880, val loss: 1.3836811780929565
Epoch 1890, training loss: 0.6073978543281555 = 0.0018523529870435596 + 0.1 * 6.055455207824707
Epoch 1890, val loss: 1.3853135108947754
Epoch 1900, training loss: 0.6082475185394287 = 0.0018361315596848726 + 0.1 * 6.064114093780518
Epoch 1900, val loss: 1.386888861656189
Epoch 1910, training loss: 0.6080635786056519 = 0.0018200459890067577 + 0.1 * 6.062435150146484
Epoch 1910, val loss: 1.38851797580719
Epoch 1920, training loss: 0.6073952913284302 = 0.0018043039599433541 + 0.1 * 6.055910110473633
Epoch 1920, val loss: 1.3900854587554932
Epoch 1930, training loss: 0.6068825125694275 = 0.0017888089641928673 + 0.1 * 6.050937175750732
Epoch 1930, val loss: 1.3916586637496948
Epoch 1940, training loss: 0.6070560216903687 = 0.0017735603032633662 + 0.1 * 6.0528244972229
Epoch 1940, val loss: 1.3932085037231445
Epoch 1950, training loss: 0.6084122657775879 = 0.0017584272427484393 + 0.1 * 6.066537857055664
Epoch 1950, val loss: 1.3948071002960205
Epoch 1960, training loss: 0.6078857779502869 = 0.0017435505287721753 + 0.1 * 6.061421871185303
Epoch 1960, val loss: 1.3964203596115112
Epoch 1970, training loss: 0.6081886887550354 = 0.0017288783565163612 + 0.1 * 6.064598083496094
Epoch 1970, val loss: 1.397979736328125
Epoch 1980, training loss: 0.6071199178695679 = 0.001714535290375352 + 0.1 * 6.054053783416748
Epoch 1980, val loss: 1.3995226621627808
Epoch 1990, training loss: 0.6061404347419739 = 0.001700530294328928 + 0.1 * 6.044398784637451
Epoch 1990, val loss: 1.400970220565796
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8155
Flip ASR: 0.8000/225 nodes
The final ASR:0.76015, 0.20710, Accuracy:0.82593, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11580])
remove edge: torch.Size([2, 9436])
updated graph: torch.Size([2, 10460])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98032, 0.01218, Accuracy:0.83333, 0.01318
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.8051505088806152 = 1.9677709341049194 + 0.1 * 8.373795509338379
Epoch 0, val loss: 1.9591034650802612
Epoch 10, training loss: 2.7944774627685547 = 1.9571166038513184 + 0.1 * 8.37360954284668
Epoch 10, val loss: 1.9493274688720703
Epoch 20, training loss: 2.78145694732666 = 1.9442098140716553 + 0.1 * 8.37247085571289
Epoch 20, val loss: 1.937117099761963
Epoch 30, training loss: 2.762523889541626 = 1.9261434078216553 + 0.1 * 8.36380386352539
Epoch 30, val loss: 1.9198812246322632
Epoch 40, training loss: 2.730151891708374 = 1.8992856740951538 + 0.1 * 8.308662414550781
Epoch 40, val loss: 1.8945590257644653
Epoch 50, training loss: 2.6606192588806152 = 1.8622463941574097 + 0.1 * 7.983729362487793
Epoch 50, val loss: 1.8609739542007446
Epoch 60, training loss: 2.5644190311431885 = 1.8219929933547974 + 0.1 * 7.424259662628174
Epoch 60, val loss: 1.826611042022705
Epoch 70, training loss: 2.4858975410461426 = 1.7840800285339355 + 0.1 * 7.018176078796387
Epoch 70, val loss: 1.795759916305542
Epoch 80, training loss: 2.428562641143799 = 1.7474002838134766 + 0.1 * 6.8116230964660645
Epoch 80, val loss: 1.7661162614822388
Epoch 90, training loss: 2.376363754272461 = 1.7058075666427612 + 0.1 * 6.705562114715576
Epoch 90, val loss: 1.7302727699279785
Epoch 100, training loss: 2.314610242843628 = 1.6500811576843262 + 0.1 * 6.645290851593018
Epoch 100, val loss: 1.6820164918899536
Epoch 110, training loss: 2.238638162612915 = 1.5769187211990356 + 0.1 * 6.617193698883057
Epoch 110, val loss: 1.6206799745559692
Epoch 120, training loss: 2.144962787628174 = 1.4852150678634644 + 0.1 * 6.59747838973999
Epoch 120, val loss: 1.5445681810379028
Epoch 130, training loss: 2.0384716987609863 = 1.380223035812378 + 0.1 * 6.5824875831604
Epoch 130, val loss: 1.4577553272247314
Epoch 140, training loss: 1.927257776260376 = 1.2705562114715576 + 0.1 * 6.567015171051025
Epoch 140, val loss: 1.3677318096160889
Epoch 150, training loss: 1.8188321590423584 = 1.163779377937317 + 0.1 * 6.550528049468994
Epoch 150, val loss: 1.2812341451644897
Epoch 160, training loss: 1.7194652557373047 = 1.0657949447631836 + 0.1 * 6.5367021560668945
Epoch 160, val loss: 1.2040249109268188
Epoch 170, training loss: 1.6323966979980469 = 0.9797830581665039 + 0.1 * 6.526136875152588
Epoch 170, val loss: 1.1381422281265259
Epoch 180, training loss: 1.5529191493988037 = 0.9014796018600464 + 0.1 * 6.514395236968994
Epoch 180, val loss: 1.07892644405365
Epoch 190, training loss: 1.4768180847167969 = 0.8264226913452148 + 0.1 * 6.503952980041504
Epoch 190, val loss: 1.0225117206573486
Epoch 200, training loss: 1.4014241695404053 = 0.7520222663879395 + 0.1 * 6.494019031524658
Epoch 200, val loss: 0.9665334224700928
Epoch 210, training loss: 1.327848196029663 = 0.6790156364440918 + 0.1 * 6.488326072692871
Epoch 210, val loss: 0.9119154810905457
Epoch 220, training loss: 1.2564213275909424 = 0.6088062524795532 + 0.1 * 6.4761505126953125
Epoch 220, val loss: 0.8598898649215698
Epoch 230, training loss: 1.1888030767440796 = 0.5422928929328918 + 0.1 * 6.465101718902588
Epoch 230, val loss: 0.8119851350784302
Epoch 240, training loss: 1.126939058303833 = 0.4808734655380249 + 0.1 * 6.460655689239502
Epoch 240, val loss: 0.7700180411338806
Epoch 250, training loss: 1.0702025890350342 = 0.4257097840309143 + 0.1 * 6.4449286460876465
Epoch 250, val loss: 0.7350283265113831
Epoch 260, training loss: 1.0207103490829468 = 0.37668129801750183 + 0.1 * 6.440290451049805
Epoch 260, val loss: 0.7067984938621521
Epoch 270, training loss: 0.9768962860107422 = 0.3336630165576935 + 0.1 * 6.4323320388793945
Epoch 270, val loss: 0.6849423050880432
Epoch 280, training loss: 0.9387702941894531 = 0.29584527015686035 + 0.1 * 6.429250240325928
Epoch 280, val loss: 0.6684368848800659
Epoch 290, training loss: 0.903710663318634 = 0.262614369392395 + 0.1 * 6.4109625816345215
Epoch 290, val loss: 0.6565680503845215
Epoch 300, training loss: 0.8732008934020996 = 0.23311109840869904 + 0.1 * 6.400897979736328
Epoch 300, val loss: 0.6483103036880493
Epoch 310, training loss: 0.8464717864990234 = 0.2067595273256302 + 0.1 * 6.397122383117676
Epoch 310, val loss: 0.6431010961532593
Epoch 320, training loss: 0.8228840827941895 = 0.18355457484722137 + 0.1 * 6.393294811248779
Epoch 320, val loss: 0.6404559016227722
Epoch 330, training loss: 0.8010908961296082 = 0.16330355405807495 + 0.1 * 6.377873420715332
Epoch 330, val loss: 0.6402729153633118
Epoch 340, training loss: 0.7819808125495911 = 0.14557164907455444 + 0.1 * 6.364091396331787
Epoch 340, val loss: 0.6425051093101501
Epoch 350, training loss: 0.7671428918838501 = 0.13014551997184753 + 0.1 * 6.369973182678223
Epoch 350, val loss: 0.6467761993408203
Epoch 360, training loss: 0.7518648505210876 = 0.11688518524169922 + 0.1 * 6.349796295166016
Epoch 360, val loss: 0.6527069211006165
Epoch 370, training loss: 0.7403212785720825 = 0.10540290176868439 + 0.1 * 6.349183559417725
Epoch 370, val loss: 0.6600934267044067
Epoch 380, training loss: 0.7284323573112488 = 0.09550616890192032 + 0.1 * 6.329261302947998
Epoch 380, val loss: 0.6684203743934631
Epoch 390, training loss: 0.718656063079834 = 0.08690962195396423 + 0.1 * 6.317464828491211
Epoch 390, val loss: 0.6773930788040161
Epoch 400, training loss: 0.7114202976226807 = 0.07939985394477844 + 0.1 * 6.32020378112793
Epoch 400, val loss: 0.6867527961730957
Epoch 410, training loss: 0.7033735513687134 = 0.07284395396709442 + 0.1 * 6.305295944213867
Epoch 410, val loss: 0.696220874786377
Epoch 420, training loss: 0.6963053941726685 = 0.06704694777727127 + 0.1 * 6.292584419250488
Epoch 420, val loss: 0.7057860493659973
Epoch 430, training loss: 0.6917327046394348 = 0.06190557777881622 + 0.1 * 6.298271179199219
Epoch 430, val loss: 0.7152705788612366
Epoch 440, training loss: 0.6858247518539429 = 0.05735034868121147 + 0.1 * 6.2847442626953125
Epoch 440, val loss: 0.724561333656311
Epoch 450, training loss: 0.6805987358093262 = 0.05326027795672417 + 0.1 * 6.2733845710754395
Epoch 450, val loss: 0.7337908744812012
Epoch 460, training loss: 0.6765106916427612 = 0.04957431182265282 + 0.1 * 6.269363880157471
Epoch 460, val loss: 0.7428775429725647
Epoch 470, training loss: 0.6734664440155029 = 0.04623917490243912 + 0.1 * 6.272272109985352
Epoch 470, val loss: 0.7518187761306763
Epoch 480, training loss: 0.6706336736679077 = 0.04321948066353798 + 0.1 * 6.274141311645508
Epoch 480, val loss: 0.7605196833610535
Epoch 490, training loss: 0.6656414270401001 = 0.040488552302122116 + 0.1 * 6.251528739929199
Epoch 490, val loss: 0.7690151929855347
Epoch 500, training loss: 0.6622682213783264 = 0.03799469396471977 + 0.1 * 6.242735385894775
Epoch 500, val loss: 0.7773920893669128
Epoch 510, training loss: 0.6632388830184937 = 0.035706788301467896 + 0.1 * 6.275320529937744
Epoch 510, val loss: 0.7856343984603882
Epoch 520, training loss: 0.6571482419967651 = 0.03362102434039116 + 0.1 * 6.235272407531738
Epoch 520, val loss: 0.7936780452728271
Epoch 530, training loss: 0.6542263031005859 = 0.031709715723991394 + 0.1 * 6.225165843963623
Epoch 530, val loss: 0.8015583753585815
Epoch 540, training loss: 0.652446448802948 = 0.029944228008389473 + 0.1 * 6.225021839141846
Epoch 540, val loss: 0.8093518614768982
Epoch 550, training loss: 0.6509770750999451 = 0.028316708281636238 + 0.1 * 6.2266035079956055
Epoch 550, val loss: 0.8169659376144409
Epoch 560, training loss: 0.6484051942825317 = 0.026817981153726578 + 0.1 * 6.215871810913086
Epoch 560, val loss: 0.8244295716285706
Epoch 570, training loss: 0.647890031337738 = 0.025428779423236847 + 0.1 * 6.224612712860107
Epoch 570, val loss: 0.8317779302597046
Epoch 580, training loss: 0.6458415389060974 = 0.024143625050783157 + 0.1 * 6.216979026794434
Epoch 580, val loss: 0.8389578461647034
Epoch 590, training loss: 0.6433535814285278 = 0.02295229583978653 + 0.1 * 6.204012393951416
Epoch 590, val loss: 0.8460420370101929
Epoch 600, training loss: 0.6419378519058228 = 0.021847719326615334 + 0.1 * 6.200901031494141
Epoch 600, val loss: 0.8529471755027771
Epoch 610, training loss: 0.6415698528289795 = 0.020817268639802933 + 0.1 * 6.207525730133057
Epoch 610, val loss: 0.8597503900527954
Epoch 620, training loss: 0.6413576006889343 = 0.01985764130949974 + 0.1 * 6.214999675750732
Epoch 620, val loss: 0.8664335608482361
Epoch 630, training loss: 0.638777494430542 = 0.01896527037024498 + 0.1 * 6.198122501373291
Epoch 630, val loss: 0.8729227781295776
Epoch 640, training loss: 0.636929452419281 = 0.018131718039512634 + 0.1 * 6.187977313995361
Epoch 640, val loss: 0.8793201446533203
Epoch 650, training loss: 0.636742353439331 = 0.017351048067212105 + 0.1 * 6.193912982940674
Epoch 650, val loss: 0.8855836987495422
Epoch 660, training loss: 0.6352817416191101 = 0.016623185947537422 + 0.1 * 6.186585426330566
Epoch 660, val loss: 0.8916839361190796
Epoch 670, training loss: 0.634992778301239 = 0.01594078168272972 + 0.1 * 6.1905198097229
Epoch 670, val loss: 0.8976640701293945
Epoch 680, training loss: 0.6333509683609009 = 0.015298821963369846 + 0.1 * 6.180521011352539
Epoch 680, val loss: 0.9035508632659912
Epoch 690, training loss: 0.6335210204124451 = 0.014698086306452751 + 0.1 * 6.188229084014893
Epoch 690, val loss: 0.9092981815338135
Epoch 700, training loss: 0.6316969394683838 = 0.014132248237729073 + 0.1 * 6.175646781921387
Epoch 700, val loss: 0.9149336218833923
Epoch 710, training loss: 0.6312185525894165 = 0.01359928585588932 + 0.1 * 6.176192760467529
Epoch 710, val loss: 0.9204633235931396
Epoch 720, training loss: 0.6302129030227661 = 0.01309581845998764 + 0.1 * 6.171171188354492
Epoch 720, val loss: 0.9259243607521057
Epoch 730, training loss: 0.6291231513023376 = 0.012622727081179619 + 0.1 * 6.165004253387451
Epoch 730, val loss: 0.9312389492988586
Epoch 740, training loss: 0.6292513012886047 = 0.012174968607723713 + 0.1 * 6.17076301574707
Epoch 740, val loss: 0.9364888072013855
Epoch 750, training loss: 0.6286472082138062 = 0.011750467121601105 + 0.1 * 6.168967247009277
Epoch 750, val loss: 0.9415940642356873
Epoch 760, training loss: 0.6280145645141602 = 0.011350428685545921 + 0.1 * 6.166640758514404
Epoch 760, val loss: 0.9466471076011658
Epoch 770, training loss: 0.6268140077590942 = 0.010970551520586014 + 0.1 * 6.1584343910217285
Epoch 770, val loss: 0.9515807032585144
Epoch 780, training loss: 0.6260234713554382 = 0.010611346922814846 + 0.1 * 6.154120922088623
Epoch 780, val loss: 0.9564574360847473
Epoch 790, training loss: 0.6276735067367554 = 0.010269677266478539 + 0.1 * 6.174037933349609
Epoch 790, val loss: 0.9612497687339783
Epoch 800, training loss: 0.6254194974899292 = 0.009945320896804333 + 0.1 * 6.154741287231445
Epoch 800, val loss: 0.9659344553947449
Epoch 810, training loss: 0.6248377561569214 = 0.009637714363634586 + 0.1 * 6.152000427246094
Epoch 810, val loss: 0.9705846905708313
Epoch 820, training loss: 0.6244378089904785 = 0.009343397803604603 + 0.1 * 6.150944232940674
Epoch 820, val loss: 0.9751296639442444
Epoch 830, training loss: 0.6239096522331238 = 0.009064383804798126 + 0.1 * 6.1484527587890625
Epoch 830, val loss: 0.9796247482299805
Epoch 840, training loss: 0.6240788102149963 = 0.008798963390290737 + 0.1 * 6.152798652648926
Epoch 840, val loss: 0.9840524196624756
Epoch 850, training loss: 0.6227678656578064 = 0.0085444375872612 + 0.1 * 6.1422343254089355
Epoch 850, val loss: 0.9883989095687866
Epoch 860, training loss: 0.6223134398460388 = 0.008302662521600723 + 0.1 * 6.14010763168335
Epoch 860, val loss: 0.9926804304122925
Epoch 870, training loss: 0.6233950257301331 = 0.008070798590779305 + 0.1 * 6.153242111206055
Epoch 870, val loss: 0.9969043731689453
Epoch 880, training loss: 0.6221179366111755 = 0.00784994475543499 + 0.1 * 6.142679691314697
Epoch 880, val loss: 1.001034140586853
Epoch 890, training loss: 0.6216695308685303 = 0.007639995776116848 + 0.1 * 6.140295028686523
Epoch 890, val loss: 1.005128026008606
Epoch 900, training loss: 0.6212546825408936 = 0.007437732070684433 + 0.1 * 6.138169765472412
Epoch 900, val loss: 1.0091103315353394
Epoch 910, training loss: 0.620502233505249 = 0.007244879379868507 + 0.1 * 6.13257360458374
Epoch 910, val loss: 1.0130228996276855
Epoch 920, training loss: 0.6211346387863159 = 0.007059968076646328 + 0.1 * 6.140746593475342
Epoch 920, val loss: 1.0168882608413696
Epoch 930, training loss: 0.6205641031265259 = 0.006881770212203264 + 0.1 * 6.1368231773376465
Epoch 930, val loss: 1.0206972360610962
Epoch 940, training loss: 0.6205787658691406 = 0.006711103022098541 + 0.1 * 6.138676166534424
Epoch 940, val loss: 1.0244477987289429
Epoch 950, training loss: 0.619795024394989 = 0.006547178141772747 + 0.1 * 6.132478713989258
Epoch 950, val loss: 1.0281444787979126
Epoch 960, training loss: 0.6199262738227844 = 0.006390502210706472 + 0.1 * 6.13535737991333
Epoch 960, val loss: 1.03180992603302
Epoch 970, training loss: 0.6191418766975403 = 0.006239366717636585 + 0.1 * 6.129024982452393
Epoch 970, val loss: 1.0353642702102661
Epoch 980, training loss: 0.6183584928512573 = 0.006095021963119507 + 0.1 * 6.1226348876953125
Epoch 980, val loss: 1.038891315460205
Epoch 990, training loss: 0.6191721558570862 = 0.005956186912953854 + 0.1 * 6.13215970993042
Epoch 990, val loss: 1.042356252670288
Epoch 1000, training loss: 0.6180789470672607 = 0.005821763072162867 + 0.1 * 6.1225714683532715
Epoch 1000, val loss: 1.045742392539978
Epoch 1010, training loss: 0.6190913915634155 = 0.0056929332204163074 + 0.1 * 6.133984088897705
Epoch 1010, val loss: 1.0491154193878174
Epoch 1020, training loss: 0.618110179901123 = 0.005567874759435654 + 0.1 * 6.125423431396484
Epoch 1020, val loss: 1.0524213314056396
Epoch 1030, training loss: 0.617392361164093 = 0.005448268726468086 + 0.1 * 6.11944055557251
Epoch 1030, val loss: 1.0557128190994263
Epoch 1040, training loss: 0.6184410452842712 = 0.0053329430520534515 + 0.1 * 6.131080627441406
Epoch 1040, val loss: 1.0589393377304077
Epoch 1050, training loss: 0.6180285811424255 = 0.005220900289714336 + 0.1 * 6.128076553344727
Epoch 1050, val loss: 1.062087893486023
Epoch 1060, training loss: 0.6167598962783813 = 0.005113213788717985 + 0.1 * 6.116466999053955
Epoch 1060, val loss: 1.0652388334274292
Epoch 1070, training loss: 0.6181000471115112 = 0.0050093079917132854 + 0.1 * 6.1309075355529785
Epoch 1070, val loss: 1.0683484077453613
Epoch 1080, training loss: 0.6161031723022461 = 0.004908930044621229 + 0.1 * 6.111942291259766
Epoch 1080, val loss: 1.0714162588119507
Epoch 1090, training loss: 0.6166937947273254 = 0.004812595900148153 + 0.1 * 6.118812084197998
Epoch 1090, val loss: 1.0744272470474243
Epoch 1100, training loss: 0.6163115501403809 = 0.004718571435660124 + 0.1 * 6.115930080413818
Epoch 1100, val loss: 1.0774133205413818
Epoch 1110, training loss: 0.6156762838363647 = 0.004627863876521587 + 0.1 * 6.110483646392822
Epoch 1110, val loss: 1.0803344249725342
Epoch 1120, training loss: 0.6157152652740479 = 0.004540054127573967 + 0.1 * 6.111752510070801
Epoch 1120, val loss: 1.0832496881484985
Epoch 1130, training loss: 0.6155524849891663 = 0.0044549740850925446 + 0.1 * 6.1109747886657715
Epoch 1130, val loss: 1.086117148399353
Epoch 1140, training loss: 0.6153781414031982 = 0.004372505005449057 + 0.1 * 6.110056400299072
Epoch 1140, val loss: 1.0889772176742554
Epoch 1150, training loss: 0.6154962182044983 = 0.0042928364127874374 + 0.1 * 6.112033843994141
Epoch 1150, val loss: 1.0918103456497192
Epoch 1160, training loss: 0.6150979995727539 = 0.004215855151414871 + 0.1 * 6.108821392059326
Epoch 1160, val loss: 1.0945791006088257
Epoch 1170, training loss: 0.6148564219474792 = 0.004141057841479778 + 0.1 * 6.10715389251709
Epoch 1170, val loss: 1.0973505973815918
Epoch 1180, training loss: 0.6151541471481323 = 0.004068293608725071 + 0.1 * 6.11085844039917
Epoch 1180, val loss: 1.100052833557129
Epoch 1190, training loss: 0.6146213412284851 = 0.00399767467752099 + 0.1 * 6.106236457824707
Epoch 1190, val loss: 1.1027685403823853
Epoch 1200, training loss: 0.615946352481842 = 0.0039292494766414165 + 0.1 * 6.120170593261719
Epoch 1200, val loss: 1.1054494380950928
Epoch 1210, training loss: 0.6142213344573975 = 0.003862920682877302 + 0.1 * 6.103583812713623
Epoch 1210, val loss: 1.1080634593963623
Epoch 1220, training loss: 0.6136654019355774 = 0.003799067810177803 + 0.1 * 6.098662853240967
Epoch 1220, val loss: 1.1106795072555542
Epoch 1230, training loss: 0.6141894459724426 = 0.003736393293365836 + 0.1 * 6.104530334472656
Epoch 1230, val loss: 1.1132498979568481
Epoch 1240, training loss: 0.6134582161903381 = 0.003675587475299835 + 0.1 * 6.0978264808654785
Epoch 1240, val loss: 1.1157888174057007
Epoch 1250, training loss: 0.6130049228668213 = 0.003616787027567625 + 0.1 * 6.093881130218506
Epoch 1250, val loss: 1.1183446645736694
Epoch 1260, training loss: 0.6144232749938965 = 0.0035593232605606318 + 0.1 * 6.108639240264893
Epoch 1260, val loss: 1.1208558082580566
Epoch 1270, training loss: 0.6133700609207153 = 0.0035028639249503613 + 0.1 * 6.098671913146973
Epoch 1270, val loss: 1.1233583688735962
Epoch 1280, training loss: 0.6134611964225769 = 0.0034484972711652517 + 0.1 * 6.10012674331665
Epoch 1280, val loss: 1.1258529424667358
Epoch 1290, training loss: 0.6128600835800171 = 0.003395566949620843 + 0.1 * 6.094645023345947
Epoch 1290, val loss: 1.1282906532287598
Epoch 1300, training loss: 0.6127232313156128 = 0.0033444464206695557 + 0.1 * 6.09378719329834
Epoch 1300, val loss: 1.1307041645050049
Epoch 1310, training loss: 0.6127683520317078 = 0.0032944323029369116 + 0.1 * 6.0947394371032715
Epoch 1310, val loss: 1.1331079006195068
Epoch 1320, training loss: 0.6122043132781982 = 0.0032453262247145176 + 0.1 * 6.089589595794678
Epoch 1320, val loss: 1.1354827880859375
Epoch 1330, training loss: 0.6129720211029053 = 0.0031978373881429434 + 0.1 * 6.097741603851318
Epoch 1330, val loss: 1.1378310918807983
Epoch 1340, training loss: 0.6120503544807434 = 0.0031514011789113283 + 0.1 * 6.0889892578125
Epoch 1340, val loss: 1.1401753425598145
Epoch 1350, training loss: 0.611971914768219 = 0.003106383141130209 + 0.1 * 6.088655471801758
Epoch 1350, val loss: 1.1424757242202759
Epoch 1360, training loss: 0.6122534871101379 = 0.0030625262297689915 + 0.1 * 6.091909408569336
Epoch 1360, val loss: 1.1447356939315796
Epoch 1370, training loss: 0.611812949180603 = 0.003019497497007251 + 0.1 * 6.087934494018555
Epoch 1370, val loss: 1.1469860076904297
Epoch 1380, training loss: 0.6111429333686829 = 0.0029774510767310858 + 0.1 * 6.081655025482178
Epoch 1380, val loss: 1.149218201637268
Epoch 1390, training loss: 0.6122080087661743 = 0.002936655655503273 + 0.1 * 6.092713356018066
Epoch 1390, val loss: 1.151430368423462
Epoch 1400, training loss: 0.6115967035293579 = 0.0028966290410608053 + 0.1 * 6.087000846862793
Epoch 1400, val loss: 1.1536341905593872
Epoch 1410, training loss: 0.6108183264732361 = 0.002857844578102231 + 0.1 * 6.079604625701904
Epoch 1410, val loss: 1.1557947397232056
Epoch 1420, training loss: 0.6117310523986816 = 0.002820088528096676 + 0.1 * 6.089109897613525
Epoch 1420, val loss: 1.1579402685165405
Epoch 1430, training loss: 0.6106855273246765 = 0.002782585099339485 + 0.1 * 6.079029083251953
Epoch 1430, val loss: 1.1600648164749146
Epoch 1440, training loss: 0.61165851354599 = 0.002746362704783678 + 0.1 * 6.0891218185424805
Epoch 1440, val loss: 1.1621675491333008
Epoch 1450, training loss: 0.6104505658149719 = 0.002710816217586398 + 0.1 * 6.077397346496582
Epoch 1450, val loss: 1.1642917394638062
Epoch 1460, training loss: 0.6107267737388611 = 0.002676479984074831 + 0.1 * 6.080502510070801
Epoch 1460, val loss: 1.1663532257080078
Epoch 1470, training loss: 0.6128930449485779 = 0.0026427768170833588 + 0.1 * 6.102502822875977
Epoch 1470, val loss: 1.1683778762817383
Epoch 1480, training loss: 0.610596776008606 = 0.002609399612993002 + 0.1 * 6.079874038696289
Epoch 1480, val loss: 1.1703916788101196
Epoch 1490, training loss: 0.6102548837661743 = 0.002577281789854169 + 0.1 * 6.076775550842285
Epoch 1490, val loss: 1.1724051237106323
Epoch 1500, training loss: 0.6105162501335144 = 0.002545769792050123 + 0.1 * 6.079704761505127
Epoch 1500, val loss: 1.174396276473999
Epoch 1510, training loss: 0.6111454367637634 = 0.0025148505810648203 + 0.1 * 6.086305618286133
Epoch 1510, val loss: 1.1763473749160767
Epoch 1520, training loss: 0.6103019714355469 = 0.0024844110012054443 + 0.1 * 6.078176021575928
Epoch 1520, val loss: 1.1783195734024048
Epoch 1530, training loss: 0.6097601652145386 = 0.002454909263178706 + 0.1 * 6.073052406311035
Epoch 1530, val loss: 1.18026864528656
Epoch 1540, training loss: 0.6106405854225159 = 0.002426146762445569 + 0.1 * 6.082144260406494
Epoch 1540, val loss: 1.1822013854980469
Epoch 1550, training loss: 0.6101804971694946 = 0.0023978680837899446 + 0.1 * 6.077826023101807
Epoch 1550, val loss: 1.18414306640625
Epoch 1560, training loss: 0.6100619435310364 = 0.0023702967446297407 + 0.1 * 6.076916217803955
Epoch 1560, val loss: 1.1860268115997314
Epoch 1570, training loss: 0.6091653108596802 = 0.0023431098088622093 + 0.1 * 6.0682220458984375
Epoch 1570, val loss: 1.1879459619522095
Epoch 1580, training loss: 0.6095970273017883 = 0.002316744765266776 + 0.1 * 6.072802543640137
Epoch 1580, val loss: 1.189805269241333
Epoch 1590, training loss: 0.6099128127098083 = 0.00229082559235394 + 0.1 * 6.07621955871582
Epoch 1590, val loss: 1.191672682762146
Epoch 1600, training loss: 0.6088712811470032 = 0.0022652980405837297 + 0.1 * 6.0660600662231445
Epoch 1600, val loss: 1.1935232877731323
Epoch 1610, training loss: 0.609837532043457 = 0.0022403874900192022 + 0.1 * 6.075971603393555
Epoch 1610, val loss: 1.1953601837158203
Epoch 1620, training loss: 0.6095225214958191 = 0.002215808257460594 + 0.1 * 6.0730671882629395
Epoch 1620, val loss: 1.1971856355667114
Epoch 1630, training loss: 0.6101818680763245 = 0.002191732404753566 + 0.1 * 6.079901218414307
Epoch 1630, val loss: 1.1990293264389038
Epoch 1640, training loss: 0.6088098883628845 = 0.0021683601662516594 + 0.1 * 6.066415309906006
Epoch 1640, val loss: 1.2008256912231445
Epoch 1650, training loss: 0.6091227531433105 = 0.002145472215488553 + 0.1 * 6.069772720336914
Epoch 1650, val loss: 1.2026419639587402
Epoch 1660, training loss: 0.6092407703399658 = 0.0021227907855063677 + 0.1 * 6.071179389953613
Epoch 1660, val loss: 1.2044092416763306
Epoch 1670, training loss: 0.609809160232544 = 0.002100559649989009 + 0.1 * 6.077085494995117
Epoch 1670, val loss: 1.2061880826950073
Epoch 1680, training loss: 0.6089705228805542 = 0.002078642137348652 + 0.1 * 6.068918704986572
Epoch 1680, val loss: 1.2079262733459473
Epoch 1690, training loss: 0.6085605025291443 = 0.0020572685170918703 + 0.1 * 6.065032482147217
Epoch 1690, val loss: 1.2096775770187378
Epoch 1700, training loss: 0.6096168756484985 = 0.002036329824477434 + 0.1 * 6.075805187225342
Epoch 1700, val loss: 1.2113863229751587
Epoch 1710, training loss: 0.6081710457801819 = 0.002015732927247882 + 0.1 * 6.061553478240967
Epoch 1710, val loss: 1.21307373046875
Epoch 1720, training loss: 0.6087907552719116 = 0.0019956061150878668 + 0.1 * 6.067951202392578
Epoch 1720, val loss: 1.2147403955459595
Epoch 1730, training loss: 0.6077865362167358 = 0.0019756099209189415 + 0.1 * 6.058109283447266
Epoch 1730, val loss: 1.2164313793182373
Epoch 1740, training loss: 0.6091210246086121 = 0.001956182299181819 + 0.1 * 6.071648597717285
Epoch 1740, val loss: 1.2180906534194946
Epoch 1750, training loss: 0.6081785559654236 = 0.0019370070658624172 + 0.1 * 6.06241512298584
Epoch 1750, val loss: 1.2197554111480713
Epoch 1760, training loss: 0.6095190048217773 = 0.0019182944670319557 + 0.1 * 6.076006889343262
Epoch 1760, val loss: 1.2214159965515137
Epoch 1770, training loss: 0.6077683568000793 = 0.0018998973537236452 + 0.1 * 6.058684349060059
Epoch 1770, val loss: 1.2230310440063477
Epoch 1780, training loss: 0.6095435619354248 = 0.0018818723037838936 + 0.1 * 6.076616287231445
Epoch 1780, val loss: 1.2246615886688232
Epoch 1790, training loss: 0.6077010035514832 = 0.001863962970674038 + 0.1 * 6.058370113372803
Epoch 1790, val loss: 1.2262848615646362
Epoch 1800, training loss: 0.6075230836868286 = 0.0018466701731085777 + 0.1 * 6.056764125823975
Epoch 1800, val loss: 1.2278778553009033
Epoch 1810, training loss: 0.6084714531898499 = 0.001829463173635304 + 0.1 * 6.066420078277588
Epoch 1810, val loss: 1.2294812202453613
Epoch 1820, training loss: 0.6075853109359741 = 0.001812407048419118 + 0.1 * 6.0577287673950195
Epoch 1820, val loss: 1.2310901880264282
Epoch 1830, training loss: 0.6089338064193726 = 0.001795845222659409 + 0.1 * 6.071379661560059
Epoch 1830, val loss: 1.2326713800430298
Epoch 1840, training loss: 0.6077721118927002 = 0.001779586891643703 + 0.1 * 6.059925079345703
Epoch 1840, val loss: 1.2342368364334106
Epoch 1850, training loss: 0.6070396900177002 = 0.0017637222772464156 + 0.1 * 6.052759647369385
Epoch 1850, val loss: 1.235777497291565
Epoch 1860, training loss: 0.6080565452575684 = 0.0017478688387200236 + 0.1 * 6.063086986541748
Epoch 1860, val loss: 1.2373254299163818
Epoch 1870, training loss: 0.6076658368110657 = 0.0017322627827525139 + 0.1 * 6.059335231781006
Epoch 1870, val loss: 1.2388468980789185
Epoch 1880, training loss: 0.6077716946601868 = 0.0017169383354485035 + 0.1 * 6.060547828674316
Epoch 1880, val loss: 1.240397334098816
Epoch 1890, training loss: 0.6075299382209778 = 0.001701978500932455 + 0.1 * 6.058279991149902
Epoch 1890, val loss: 1.2419222593307495
Epoch 1900, training loss: 0.6075084805488586 = 0.001687236363068223 + 0.1 * 6.0582122802734375
Epoch 1900, val loss: 1.2434492111206055
Epoch 1910, training loss: 0.6073137521743774 = 0.0016727301990613341 + 0.1 * 6.05640983581543
Epoch 1910, val loss: 1.244928002357483
Epoch 1920, training loss: 0.6068815588951111 = 0.0016584335826337337 + 0.1 * 6.052231311798096
Epoch 1920, val loss: 1.2464267015457153
Epoch 1930, training loss: 0.6067506670951843 = 0.001644433825276792 + 0.1 * 6.051062107086182
Epoch 1930, val loss: 1.2478934526443481
Epoch 1940, training loss: 0.607859194278717 = 0.001630536513403058 + 0.1 * 6.062286376953125
Epoch 1940, val loss: 1.2493972778320312
Epoch 1950, training loss: 0.6070377826690674 = 0.001616858527995646 + 0.1 * 6.054209232330322
Epoch 1950, val loss: 1.2508794069290161
Epoch 1960, training loss: 0.6075732707977295 = 0.0016034649452194571 + 0.1 * 6.05969762802124
Epoch 1960, val loss: 1.2523549795150757
Epoch 1970, training loss: 0.6063476204872131 = 0.0015901584411039948 + 0.1 * 6.047574996948242
Epoch 1970, val loss: 1.2537977695465088
Epoch 1980, training loss: 0.6073852777481079 = 0.0015771895414218307 + 0.1 * 6.058080673217773
Epoch 1980, val loss: 1.2552449703216553
Epoch 1990, training loss: 0.6066901087760925 = 0.0015642779180780053 + 0.1 * 6.051258563995361
Epoch 1990, val loss: 1.2567048072814941
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6236
Flip ASR: 0.5556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7789037227630615 = 1.941519856452942 + 0.1 * 8.373838424682617
Epoch 0, val loss: 1.9357075691223145
Epoch 10, training loss: 2.7685275077819824 = 1.931166648864746 + 0.1 * 8.37360954284668
Epoch 10, val loss: 1.9245829582214355
Epoch 20, training loss: 2.7552623748779297 = 1.918018102645874 + 0.1 * 8.372442245483398
Epoch 20, val loss: 1.9103949069976807
Epoch 30, training loss: 2.735515832901001 = 1.8991734981536865 + 0.1 * 8.363423347473145
Epoch 30, val loss: 1.8900911808013916
Epoch 40, training loss: 2.7013297080993652 = 1.8714460134506226 + 0.1 * 8.298835754394531
Epoch 40, val loss: 1.8607845306396484
Epoch 50, training loss: 2.6196296215057373 = 1.8356961011886597 + 0.1 * 7.8393354415893555
Epoch 50, val loss: 1.8255692720413208
Epoch 60, training loss: 2.5306363105773926 = 1.8012123107910156 + 0.1 * 7.294238567352295
Epoch 60, val loss: 1.7940670251846313
Epoch 70, training loss: 2.4679677486419678 = 1.7678625583648682 + 0.1 * 7.001052379608154
Epoch 70, val loss: 1.7651426792144775
Epoch 80, training loss: 2.4122538566589355 = 1.7320592403411865 + 0.1 * 6.801945209503174
Epoch 80, val loss: 1.7351577281951904
Epoch 90, training loss: 2.354400157928467 = 1.6854454278945923 + 0.1 * 6.689547061920166
Epoch 90, val loss: 1.6952860355377197
Epoch 100, training loss: 2.285256862640381 = 1.621802806854248 + 0.1 * 6.634541034698486
Epoch 100, val loss: 1.6405892372131348
Epoch 110, training loss: 2.2003841400146484 = 1.540475845336914 + 0.1 * 6.599083423614502
Epoch 110, val loss: 1.5715512037277222
Epoch 120, training loss: 2.106191396713257 = 1.4483193159103394 + 0.1 * 6.578721523284912
Epoch 120, val loss: 1.495498538017273
Epoch 130, training loss: 2.0115842819213867 = 1.3550982475280762 + 0.1 * 6.564861297607422
Epoch 130, val loss: 1.4233207702636719
Epoch 140, training loss: 1.9197614192962646 = 1.264736533164978 + 0.1 * 6.550249099731445
Epoch 140, val loss: 1.3584930896759033
Epoch 150, training loss: 1.830580472946167 = 1.177114725112915 + 0.1 * 6.534657955169678
Epoch 150, val loss: 1.2995530366897583
Epoch 160, training loss: 1.7451252937316895 = 1.0930768251419067 + 0.1 * 6.520484924316406
Epoch 160, val loss: 1.2446959018707275
Epoch 170, training loss: 1.6650476455688477 = 1.0137382745742798 + 0.1 * 6.513092994689941
Epoch 170, val loss: 1.193047285079956
Epoch 180, training loss: 1.5895479917526245 = 0.9392406344413757 + 0.1 * 6.503073215484619
Epoch 180, val loss: 1.1437679529190063
Epoch 190, training loss: 1.5177977085113525 = 0.8679900169372559 + 0.1 * 6.498076438903809
Epoch 190, val loss: 1.095194935798645
Epoch 200, training loss: 1.4471033811569214 = 0.7982344031333923 + 0.1 * 6.488689422607422
Epoch 200, val loss: 1.0456780195236206
Epoch 210, training loss: 1.3769513368606567 = 0.7289831042289734 + 0.1 * 6.479682445526123
Epoch 210, val loss: 0.9950512647628784
Epoch 220, training loss: 1.3087254762649536 = 0.6614023447036743 + 0.1 * 6.473231315612793
Epoch 220, val loss: 0.9448378086090088
Epoch 230, training loss: 1.2437057495117188 = 0.597978413105011 + 0.1 * 6.457272529602051
Epoch 230, val loss: 0.8975576758384705
Epoch 240, training loss: 1.1846152544021606 = 0.5398620963096619 + 0.1 * 6.447531223297119
Epoch 240, val loss: 0.8555512428283691
Epoch 250, training loss: 1.1311206817626953 = 0.4879723787307739 + 0.1 * 6.431482791900635
Epoch 250, val loss: 0.8201123476028442
Epoch 260, training loss: 1.0838934183120728 = 0.44181129336357117 + 0.1 * 6.420820713043213
Epoch 260, val loss: 0.791064441204071
Epoch 270, training loss: 1.0415067672729492 = 0.400550901889801 + 0.1 * 6.409558296203613
Epoch 270, val loss: 0.7679815292358398
Epoch 280, training loss: 1.0031887292861938 = 0.36335819959640503 + 0.1 * 6.3983049392700195
Epoch 280, val loss: 0.7496374249458313
Epoch 290, training loss: 0.9684649705886841 = 0.32967039942741394 + 0.1 * 6.387946128845215
Epoch 290, val loss: 0.7351037859916687
Epoch 300, training loss: 0.936384916305542 = 0.29889848828315735 + 0.1 * 6.37486457824707
Epoch 300, val loss: 0.7235845923423767
Epoch 310, training loss: 0.9094640016555786 = 0.27106043696403503 + 0.1 * 6.384035587310791
Epoch 310, val loss: 0.7142283320426941
Epoch 320, training loss: 0.8824905753135681 = 0.24601764976978302 + 0.1 * 6.364729404449463
Epoch 320, val loss: 0.706930935382843
Epoch 330, training loss: 0.8583089113235474 = 0.22314320504665375 + 0.1 * 6.351656913757324
Epoch 330, val loss: 0.7009450197219849
Epoch 340, training loss: 0.8377669453620911 = 0.20220385491847992 + 0.1 * 6.355630874633789
Epoch 340, val loss: 0.6965486407279968
Epoch 350, training loss: 0.8178904056549072 = 0.18331359326839447 + 0.1 * 6.345767974853516
Epoch 350, val loss: 0.6933376789093018
Epoch 360, training loss: 0.7990410923957825 = 0.16622336208820343 + 0.1 * 6.328177452087402
Epoch 360, val loss: 0.6913654208183289
Epoch 370, training loss: 0.7826049327850342 = 0.15070167183876038 + 0.1 * 6.319032669067383
Epoch 370, val loss: 0.690189778804779
Epoch 380, training loss: 0.7682392597198486 = 0.13665416836738586 + 0.1 * 6.315850257873535
Epoch 380, val loss: 0.6898636221885681
Epoch 390, training loss: 0.7554991245269775 = 0.12408968806266785 + 0.1 * 6.314094066619873
Epoch 390, val loss: 0.6904569864273071
Epoch 400, training loss: 0.7437199354171753 = 0.11279693990945816 + 0.1 * 6.309229373931885
Epoch 400, val loss: 0.6918143630027771
Epoch 410, training loss: 0.7328833341598511 = 0.10270829498767853 + 0.1 * 6.301750659942627
Epoch 410, val loss: 0.6937882304191589
Epoch 420, training loss: 0.722231388092041 = 0.0936741903424263 + 0.1 * 6.285572052001953
Epoch 420, val loss: 0.6964774131774902
Epoch 430, training loss: 0.7148100137710571 = 0.08559387922286987 + 0.1 * 6.292160987854004
Epoch 430, val loss: 0.6996573209762573
Epoch 440, training loss: 0.7058272361755371 = 0.07839574664831161 + 0.1 * 6.274314880371094
Epoch 440, val loss: 0.7035938501358032
Epoch 450, training loss: 0.6988092064857483 = 0.07194007933139801 + 0.1 * 6.268691062927246
Epoch 450, val loss: 0.7079628109931946
Epoch 460, training loss: 0.6935256123542786 = 0.06614971160888672 + 0.1 * 6.273758888244629
Epoch 460, val loss: 0.7128127217292786
Epoch 470, training loss: 0.6870385408401489 = 0.06097785755991936 + 0.1 * 6.260606288909912
Epoch 470, val loss: 0.7180758714675903
Epoch 480, training loss: 0.6815110445022583 = 0.05633822828531265 + 0.1 * 6.251728057861328
Epoch 480, val loss: 0.7238115072250366
Epoch 490, training loss: 0.677172064781189 = 0.052157558500766754 + 0.1 * 6.250144958496094
Epoch 490, val loss: 0.7297610640525818
Epoch 500, training loss: 0.673244833946228 = 0.048395536839962006 + 0.1 * 6.248493194580078
Epoch 500, val loss: 0.7359325289726257
Epoch 510, training loss: 0.6690467596054077 = 0.04500701650977135 + 0.1 * 6.2403974533081055
Epoch 510, val loss: 0.7423863410949707
Epoch 520, training loss: 0.6656359434127808 = 0.04195587337017059 + 0.1 * 6.236800193786621
Epoch 520, val loss: 0.7488994002342224
Epoch 530, training loss: 0.6618661880493164 = 0.03919833153486252 + 0.1 * 6.226678371429443
Epoch 530, val loss: 0.7555732727050781
Epoch 540, training loss: 0.6590567827224731 = 0.03669007867574692 + 0.1 * 6.223667144775391
Epoch 540, val loss: 0.7623077630996704
Epoch 550, training loss: 0.6569896936416626 = 0.03440701588988304 + 0.1 * 6.225826740264893
Epoch 550, val loss: 0.7690160870552063
Epoch 560, training loss: 0.6537473201751709 = 0.032335251569747925 + 0.1 * 6.214120388031006
Epoch 560, val loss: 0.7757775187492371
Epoch 570, training loss: 0.6524995565414429 = 0.030441710725426674 + 0.1 * 6.220578670501709
Epoch 570, val loss: 0.7826052308082581
Epoch 580, training loss: 0.6497697234153748 = 0.028709447011351585 + 0.1 * 6.210602283477783
Epoch 580, val loss: 0.7893127799034119
Epoch 590, training loss: 0.648658037185669 = 0.027124864980578423 + 0.1 * 6.215331554412842
Epoch 590, val loss: 0.7960531711578369
Epoch 600, training loss: 0.6455658674240112 = 0.02566959522664547 + 0.1 * 6.198962688446045
Epoch 600, val loss: 0.8026573657989502
Epoch 610, training loss: 0.6438280940055847 = 0.02433123253285885 + 0.1 * 6.1949687004089355
Epoch 610, val loss: 0.8093091249465942
Epoch 620, training loss: 0.6437054872512817 = 0.02309279702603817 + 0.1 * 6.206126689910889
Epoch 620, val loss: 0.8158655762672424
Epoch 630, training loss: 0.6409338116645813 = 0.021948950365185738 + 0.1 * 6.18984842300415
Epoch 630, val loss: 0.8222695589065552
Epoch 640, training loss: 0.6398537755012512 = 0.02089054323732853 + 0.1 * 6.189632415771484
Epoch 640, val loss: 0.8286997675895691
Epoch 650, training loss: 0.6381468176841736 = 0.019909527152776718 + 0.1 * 6.182373046875
Epoch 650, val loss: 0.834990918636322
Epoch 660, training loss: 0.6365132927894592 = 0.019000694155693054 + 0.1 * 6.175125598907471
Epoch 660, val loss: 0.8412606120109558
Epoch 670, training loss: 0.6366447806358337 = 0.018153676763176918 + 0.1 * 6.184910774230957
Epoch 670, val loss: 0.8474881052970886
Epoch 680, training loss: 0.6354213356971741 = 0.01736391708254814 + 0.1 * 6.1805739402771
Epoch 680, val loss: 0.8535695672035217
Epoch 690, training loss: 0.6334887146949768 = 0.01662757433950901 + 0.1 * 6.168611526489258
Epoch 690, val loss: 0.8595695495605469
Epoch 700, training loss: 0.6324933767318726 = 0.015941407531499863 + 0.1 * 6.165519714355469
Epoch 700, val loss: 0.865570604801178
Epoch 710, training loss: 0.6339486241340637 = 0.015296869911253452 + 0.1 * 6.186517238616943
Epoch 710, val loss: 0.8714537024497986
Epoch 720, training loss: 0.6315415501594543 = 0.014694062061607838 + 0.1 * 6.168475151062012
Epoch 720, val loss: 0.8771867752075195
Epoch 730, training loss: 0.6301281452178955 = 0.01412882562726736 + 0.1 * 6.1599931716918945
Epoch 730, val loss: 0.8829428553581238
Epoch 740, training loss: 0.6300457715988159 = 0.013595986180007458 + 0.1 * 6.1644978523254395
Epoch 740, val loss: 0.888580858707428
Epoch 750, training loss: 0.6293827295303345 = 0.013094290159642696 + 0.1 * 6.16288423538208
Epoch 750, val loss: 0.894130289554596
Epoch 760, training loss: 0.6284428834915161 = 0.012621782720088959 + 0.1 * 6.1582112312316895
Epoch 760, val loss: 0.8995333313941956
Epoch 770, training loss: 0.6272445917129517 = 0.012178624048829079 + 0.1 * 6.150660037994385
Epoch 770, val loss: 0.9049685001373291
Epoch 780, training loss: 0.6273096203804016 = 0.011758605018258095 + 0.1 * 6.155509948730469
Epoch 780, val loss: 0.9103327989578247
Epoch 790, training loss: 0.6261302828788757 = 0.011361617594957352 + 0.1 * 6.147686958312988
Epoch 790, val loss: 0.915561854839325
Epoch 800, training loss: 0.6263116598129272 = 0.010985106229782104 + 0.1 * 6.153264999389648
Epoch 800, val loss: 0.9207172393798828
Epoch 810, training loss: 0.6259126663208008 = 0.010628760792315006 + 0.1 * 6.152839183807373
Epoch 810, val loss: 0.925785481929779
Epoch 820, training loss: 0.6248783469200134 = 0.01029135286808014 + 0.1 * 6.145870208740234
Epoch 820, val loss: 0.9307833313941956
Epoch 830, training loss: 0.624305009841919 = 0.00997070036828518 + 0.1 * 6.1433424949646
Epoch 830, val loss: 0.9357147216796875
Epoch 840, training loss: 0.6241834163665771 = 0.009666434489190578 + 0.1 * 6.145169734954834
Epoch 840, val loss: 0.9405749440193176
Epoch 850, training loss: 0.6236209273338318 = 0.009376262314617634 + 0.1 * 6.142446517944336
Epoch 850, val loss: 0.9453601837158203
Epoch 860, training loss: 0.6227816343307495 = 0.009100982919335365 + 0.1 * 6.136806488037109
Epoch 860, val loss: 0.9500643014907837
Epoch 870, training loss: 0.6242682337760925 = 0.00883801095187664 + 0.1 * 6.15430212020874
Epoch 870, val loss: 0.9547085165977478
Epoch 880, training loss: 0.6225587129592896 = 0.008587570860981941 + 0.1 * 6.139710903167725
Epoch 880, val loss: 0.959261417388916
Epoch 890, training loss: 0.621563732624054 = 0.008348864503204823 + 0.1 * 6.132148265838623
Epoch 890, val loss: 0.9638186693191528
Epoch 900, training loss: 0.6221508979797363 = 0.008120124228298664 + 0.1 * 6.140307903289795
Epoch 900, val loss: 0.9682902097702026
Epoch 910, training loss: 0.6214316487312317 = 0.007901011034846306 + 0.1 * 6.135306358337402
Epoch 910, val loss: 0.9726791977882385
Epoch 920, training loss: 0.6205362677574158 = 0.007692360784858465 + 0.1 * 6.128439426422119
Epoch 920, val loss: 0.9769905209541321
Epoch 930, training loss: 0.6205913424491882 = 0.007492548320442438 + 0.1 * 6.130988121032715
Epoch 930, val loss: 0.981289803981781
Epoch 940, training loss: 0.6207190752029419 = 0.007301105186343193 + 0.1 * 6.134179592132568
Epoch 940, val loss: 0.9855055212974548
Epoch 950, training loss: 0.61985844373703 = 0.0071172513999044895 + 0.1 * 6.127411842346191
Epoch 950, val loss: 0.9896451830863953
Epoch 960, training loss: 0.6190279126167297 = 0.006941503379493952 + 0.1 * 6.120863914489746
Epoch 960, val loss: 0.9937472939491272
Epoch 970, training loss: 0.6202893853187561 = 0.006772666238248348 + 0.1 * 6.135167598724365
Epoch 970, val loss: 0.9978057742118835
Epoch 980, training loss: 0.618704617023468 = 0.006609896197915077 + 0.1 * 6.120946884155273
Epoch 980, val loss: 1.0017507076263428
Epoch 990, training loss: 0.6184619665145874 = 0.006454539950937033 + 0.1 * 6.120074272155762
Epoch 990, val loss: 1.0056856870651245
Epoch 1000, training loss: 0.6185157299041748 = 0.0063046119175851345 + 0.1 * 6.122110843658447
Epoch 1000, val loss: 1.0095422267913818
Epoch 1010, training loss: 0.6178221106529236 = 0.0061612315475940704 + 0.1 * 6.116608619689941
Epoch 1010, val loss: 1.0133492946624756
Epoch 1020, training loss: 0.6178519129753113 = 0.006023634225130081 + 0.1 * 6.118282318115234
Epoch 1020, val loss: 1.017183780670166
Epoch 1030, training loss: 0.6175182461738586 = 0.005890083499252796 + 0.1 * 6.116281509399414
Epoch 1030, val loss: 1.020912528038025
Epoch 1040, training loss: 0.6167499423027039 = 0.005762110929936171 + 0.1 * 6.1098785400390625
Epoch 1040, val loss: 1.0246013402938843
Epoch 1050, training loss: 0.6190435886383057 = 0.005638509523123503 + 0.1 * 6.1340508460998535
Epoch 1050, val loss: 1.0282807350158691
Epoch 1060, training loss: 0.616822361946106 = 0.0055191973224282265 + 0.1 * 6.113031387329102
Epoch 1060, val loss: 1.031814694404602
Epoch 1070, training loss: 0.6172693371772766 = 0.005404794588685036 + 0.1 * 6.118645668029785
Epoch 1070, val loss: 1.035401701927185
Epoch 1080, training loss: 0.6158522367477417 = 0.00529368594288826 + 0.1 * 6.10558557510376
Epoch 1080, val loss: 1.0388901233673096
Epoch 1090, training loss: 0.6168718934059143 = 0.005187178496271372 + 0.1 * 6.116847038269043
Epoch 1090, val loss: 1.0423834323883057
Epoch 1100, training loss: 0.6155860424041748 = 0.0050836666487157345 + 0.1 * 6.105023384094238
Epoch 1100, val loss: 1.0457839965820312
Epoch 1110, training loss: 0.6151597499847412 = 0.004984545987099409 + 0.1 * 6.101751804351807
Epoch 1110, val loss: 1.0492033958435059
Epoch 1120, training loss: 0.6150253415107727 = 0.004888422787189484 + 0.1 * 6.101369380950928
Epoch 1120, val loss: 1.052596926689148
Epoch 1130, training loss: 0.615673303604126 = 0.004794899839907885 + 0.1 * 6.108784198760986
Epoch 1130, val loss: 1.0559114217758179
Epoch 1140, training loss: 0.6150815486907959 = 0.004704761318862438 + 0.1 * 6.1037678718566895
Epoch 1140, val loss: 1.0591939687728882
Epoch 1150, training loss: 0.617134153842926 = 0.0046171704307198524 + 0.1 * 6.12516975402832
Epoch 1150, val loss: 1.0624215602874756
Epoch 1160, training loss: 0.6147521734237671 = 0.004532527178525925 + 0.1 * 6.10219669342041
Epoch 1160, val loss: 1.0655912160873413
Epoch 1170, training loss: 0.6144635677337646 = 0.0044510820880532265 + 0.1 * 6.100124835968018
Epoch 1170, val loss: 1.0687870979309082
Epoch 1180, training loss: 0.615851104259491 = 0.004371688235551119 + 0.1 * 6.11479377746582
Epoch 1180, val loss: 1.0719070434570312
Epoch 1190, training loss: 0.6139312982559204 = 0.004294775426387787 + 0.1 * 6.096365451812744
Epoch 1190, val loss: 1.0749937295913696
Epoch 1200, training loss: 0.6140913963317871 = 0.0042208107188344 + 0.1 * 6.098706245422363
Epoch 1200, val loss: 1.0780916213989258
Epoch 1210, training loss: 0.6137139201164246 = 0.004148510284721851 + 0.1 * 6.095654487609863
Epoch 1210, val loss: 1.0811283588409424
Epoch 1220, training loss: 0.6137140393257141 = 0.004078519996255636 + 0.1 * 6.096355438232422
Epoch 1220, val loss: 1.0841314792633057
Epoch 1230, training loss: 0.6135541200637817 = 0.004010292701423168 + 0.1 * 6.095438003540039
Epoch 1230, val loss: 1.0870996713638306
Epoch 1240, training loss: 0.612909197807312 = 0.003944241441786289 + 0.1 * 6.089649677276611
Epoch 1240, val loss: 1.0900394916534424
Epoch 1250, training loss: 0.6129292249679565 = 0.003880326636135578 + 0.1 * 6.090488910675049
Epoch 1250, val loss: 1.0929685831069946
Epoch 1260, training loss: 0.6130107045173645 = 0.003817605087533593 + 0.1 * 6.091930866241455
Epoch 1260, val loss: 1.0958455801010132
Epoch 1270, training loss: 0.6143888235092163 = 0.0037569222040474415 + 0.1 * 6.106318473815918
Epoch 1270, val loss: 1.0986841917037964
Epoch 1280, training loss: 0.6123864054679871 = 0.003697632346302271 + 0.1 * 6.086887359619141
Epoch 1280, val loss: 1.1014926433563232
Epoch 1290, training loss: 0.6128928065299988 = 0.0036406279541552067 + 0.1 * 6.092521667480469
Epoch 1290, val loss: 1.1043466329574585
Epoch 1300, training loss: 0.6126241087913513 = 0.0035849378909915686 + 0.1 * 6.090391635894775
Epoch 1300, val loss: 1.1071221828460693
Epoch 1310, training loss: 0.6125054955482483 = 0.0035302022006362677 + 0.1 * 6.089752674102783
Epoch 1310, val loss: 1.1098672151565552
Epoch 1320, training loss: 0.6120274662971497 = 0.003477363381534815 + 0.1 * 6.085501194000244
Epoch 1320, val loss: 1.1125519275665283
Epoch 1330, training loss: 0.613886833190918 = 0.003425865201279521 + 0.1 * 6.104609966278076
Epoch 1330, val loss: 1.1152563095092773
Epoch 1340, training loss: 0.6117145419120789 = 0.0033756576012820005 + 0.1 * 6.083388805389404
Epoch 1340, val loss: 1.1179299354553223
Epoch 1350, training loss: 0.6117904782295227 = 0.0033271461725234985 + 0.1 * 6.0846333503723145
Epoch 1350, val loss: 1.1206215620040894
Epoch 1360, training loss: 0.6115657091140747 = 0.0032793362624943256 + 0.1 * 6.082863807678223
Epoch 1360, val loss: 1.123236060142517
Epoch 1370, training loss: 0.6119742393493652 = 0.0032328511588275433 + 0.1 * 6.087413787841797
Epoch 1370, val loss: 1.1258182525634766
Epoch 1380, training loss: 0.6111962199211121 = 0.003187614493072033 + 0.1 * 6.0800862312316895
Epoch 1380, val loss: 1.1284135580062866
Epoch 1390, training loss: 0.6121706962585449 = 0.003143716137856245 + 0.1 * 6.090269565582275
Epoch 1390, val loss: 1.1309962272644043
Epoch 1400, training loss: 0.611001193523407 = 0.003100381698459387 + 0.1 * 6.079008102416992
Epoch 1400, val loss: 1.1335434913635254
Epoch 1410, training loss: 0.6126822829246521 = 0.0030583436600863934 + 0.1 * 6.0962395668029785
Epoch 1410, val loss: 1.136034607887268
Epoch 1420, training loss: 0.6111431121826172 = 0.003017266048118472 + 0.1 * 6.081258773803711
Epoch 1420, val loss: 1.1385257244110107
Epoch 1430, training loss: 0.6109273433685303 = 0.0029774298891425133 + 0.1 * 6.079499244689941
Epoch 1430, val loss: 1.1409934759140015
Epoch 1440, training loss: 0.6106395125389099 = 0.0029382449574768543 + 0.1 * 6.077012538909912
Epoch 1440, val loss: 1.1434369087219238
Epoch 1450, training loss: 0.610494077205658 = 0.002900202525779605 + 0.1 * 6.075938701629639
Epoch 1450, val loss: 1.1458537578582764
Epoch 1460, training loss: 0.6103769540786743 = 0.002862812951207161 + 0.1 * 6.075141429901123
Epoch 1460, val loss: 1.1482610702514648
Epoch 1470, training loss: 0.6102816462516785 = 0.0028262571431696415 + 0.1 * 6.074553489685059
Epoch 1470, val loss: 1.150625467300415
Epoch 1480, training loss: 0.610233724117279 = 0.0027905809693038464 + 0.1 * 6.074431419372559
Epoch 1480, val loss: 1.1529653072357178
Epoch 1490, training loss: 0.6095994114875793 = 0.0027559297159314156 + 0.1 * 6.068434715270996
Epoch 1490, val loss: 1.1552988290786743
Epoch 1500, training loss: 0.6111743450164795 = 0.002721749246120453 + 0.1 * 6.084526062011719
Epoch 1500, val loss: 1.1575937271118164
Epoch 1510, training loss: 0.6099269986152649 = 0.002688267268240452 + 0.1 * 6.072386741638184
Epoch 1510, val loss: 1.1598485708236694
Epoch 1520, training loss: 0.6128408908843994 = 0.0026557869277894497 + 0.1 * 6.101850986480713
Epoch 1520, val loss: 1.1621298789978027
Epoch 1530, training loss: 0.6101891398429871 = 0.0026238802820444107 + 0.1 * 6.075652599334717
Epoch 1530, val loss: 1.1643342971801758
Epoch 1540, training loss: 0.609661877155304 = 0.0025930339470505714 + 0.1 * 6.070688247680664
Epoch 1540, val loss: 1.166614055633545
Epoch 1550, training loss: 0.6105552315711975 = 0.002562748733907938 + 0.1 * 6.079924583435059
Epoch 1550, val loss: 1.168814778327942
Epoch 1560, training loss: 0.6099711656570435 = 0.002532568760216236 + 0.1 * 6.074386119842529
Epoch 1560, val loss: 1.1709572076797485
Epoch 1570, training loss: 0.6097419261932373 = 0.00250356737524271 + 0.1 * 6.072383403778076
Epoch 1570, val loss: 1.1731349229812622
Epoch 1580, training loss: 0.6097131371498108 = 0.002475135726854205 + 0.1 * 6.072380065917969
Epoch 1580, val loss: 1.175264835357666
Epoch 1590, training loss: 0.6090962886810303 = 0.00244733365252614 + 0.1 * 6.0664896965026855
Epoch 1590, val loss: 1.1773802042007446
Epoch 1600, training loss: 0.6090721487998962 = 0.0024198582395911217 + 0.1 * 6.06652307510376
Epoch 1600, val loss: 1.179451823234558
Epoch 1610, training loss: 0.6087795495986938 = 0.002393126953393221 + 0.1 * 6.063864231109619
Epoch 1610, val loss: 1.1815481185913086
Epoch 1620, training loss: 0.6104452013969421 = 0.002367016626521945 + 0.1 * 6.080781936645508
Epoch 1620, val loss: 1.1836546659469604
Epoch 1630, training loss: 0.6085615754127502 = 0.002341031562536955 + 0.1 * 6.0622053146362305
Epoch 1630, val loss: 1.1856764554977417
Epoch 1640, training loss: 0.6090053915977478 = 0.002315924037247896 + 0.1 * 6.06689453125
Epoch 1640, val loss: 1.1877058744430542
Epoch 1650, training loss: 0.609613299369812 = 0.0022912120912224054 + 0.1 * 6.073220729827881
Epoch 1650, val loss: 1.1897369623184204
Epoch 1660, training loss: 0.6089770197868347 = 0.002266873838379979 + 0.1 * 6.067101001739502
Epoch 1660, val loss: 1.1917014122009277
Epoch 1670, training loss: 0.6086696982383728 = 0.0022429681848734617 + 0.1 * 6.064267158508301
Epoch 1670, val loss: 1.1936887502670288
Epoch 1680, training loss: 0.6083542108535767 = 0.0022197768557816744 + 0.1 * 6.061344146728516
Epoch 1680, val loss: 1.1956576108932495
Epoch 1690, training loss: 0.6086891889572144 = 0.0021969848312437534 + 0.1 * 6.064921855926514
Epoch 1690, val loss: 1.1976338624954224
Epoch 1700, training loss: 0.6081053614616394 = 0.0021743816323578358 + 0.1 * 6.059309482574463
Epoch 1700, val loss: 1.1995633840560913
Epoch 1710, training loss: 0.608866810798645 = 0.0021524245385080576 + 0.1 * 6.067143440246582
Epoch 1710, val loss: 1.2015010118484497
Epoch 1720, training loss: 0.6083388328552246 = 0.0021306038834154606 + 0.1 * 6.062081813812256
Epoch 1720, val loss: 1.2033931016921997
Epoch 1730, training loss: 0.6078187227249146 = 0.0021094772964715958 + 0.1 * 6.057092189788818
Epoch 1730, val loss: 1.2052781581878662
Epoch 1740, training loss: 0.6086528301239014 = 0.0020888596773147583 + 0.1 * 6.065639972686768
Epoch 1740, val loss: 1.2071928977966309
Epoch 1750, training loss: 0.6077786684036255 = 0.002068226458504796 + 0.1 * 6.057104587554932
Epoch 1750, val loss: 1.2090462446212769
Epoch 1760, training loss: 0.6080819368362427 = 0.0020482020918279886 + 0.1 * 6.060337543487549
Epoch 1760, val loss: 1.2109050750732422
Epoch 1770, training loss: 0.6083321571350098 = 0.002028444316238165 + 0.1 * 6.063036918640137
Epoch 1770, val loss: 1.2127193212509155
Epoch 1780, training loss: 0.6075959801673889 = 0.0020090595353394747 + 0.1 * 6.0558695793151855
Epoch 1780, val loss: 1.214550495147705
Epoch 1790, training loss: 0.6079497933387756 = 0.001990212593227625 + 0.1 * 6.059596061706543
Epoch 1790, val loss: 1.2163711786270142
Epoch 1800, training loss: 0.6077791452407837 = 0.0019716352690011263 + 0.1 * 6.058074474334717
Epoch 1800, val loss: 1.2181813716888428
Epoch 1810, training loss: 0.6077179908752441 = 0.0019532355945557356 + 0.1 * 6.057647228240967
Epoch 1810, val loss: 1.2199615240097046
Epoch 1820, training loss: 0.6076371669769287 = 0.0019352486124262214 + 0.1 * 6.057019233703613
Epoch 1820, val loss: 1.2217414379119873
Epoch 1830, training loss: 0.6077409982681274 = 0.001917588640935719 + 0.1 * 6.058233737945557
Epoch 1830, val loss: 1.2235068082809448
Epoch 1840, training loss: 0.6076191663742065 = 0.0019002240151166916 + 0.1 * 6.057189464569092
Epoch 1840, val loss: 1.2252552509307861
Epoch 1850, training loss: 0.6069402098655701 = 0.0018829155014827847 + 0.1 * 6.050572872161865
Epoch 1850, val loss: 1.2269827127456665
Epoch 1860, training loss: 0.6070760488510132 = 0.0018661892972886562 + 0.1 * 6.052098751068115
Epoch 1860, val loss: 1.2287081480026245
Epoch 1870, training loss: 0.60804682970047 = 0.0018498041899874806 + 0.1 * 6.061969757080078
Epoch 1870, val loss: 1.230449914932251
Epoch 1880, training loss: 0.6075054407119751 = 0.0018333810148760676 + 0.1 * 6.056720733642578
Epoch 1880, val loss: 1.2321490049362183
Epoch 1890, training loss: 0.6073601841926575 = 0.0018173446878790855 + 0.1 * 6.055428504943848
Epoch 1890, val loss: 1.233817458152771
Epoch 1900, training loss: 0.6073974967002869 = 0.001801621401682496 + 0.1 * 6.0559587478637695
Epoch 1900, val loss: 1.2354878187179565
Epoch 1910, training loss: 0.6068099737167358 = 0.0017861881060525775 + 0.1 * 6.050237655639648
Epoch 1910, val loss: 1.2371642589569092
Epoch 1920, training loss: 0.6074303984642029 = 0.001771018374711275 + 0.1 * 6.056593418121338
Epoch 1920, val loss: 1.2388368844985962
Epoch 1930, training loss: 0.6066642999649048 = 0.001756037469021976 + 0.1 * 6.0490827560424805
Epoch 1930, val loss: 1.2405043840408325
Epoch 1940, training loss: 0.6079438328742981 = 0.0017413169844076037 + 0.1 * 6.06202507019043
Epoch 1940, val loss: 1.2421373128890991
Epoch 1950, training loss: 0.6064345836639404 = 0.0017266246723011136 + 0.1 * 6.047079563140869
Epoch 1950, val loss: 1.2437852621078491
Epoch 1960, training loss: 0.6071752309799194 = 0.0017125237500295043 + 0.1 * 6.054626941680908
Epoch 1960, val loss: 1.245452880859375
Epoch 1970, training loss: 0.6068328022956848 = 0.001698436914011836 + 0.1 * 6.0513434410095215
Epoch 1970, val loss: 1.2470674514770508
Epoch 1980, training loss: 0.6077423691749573 = 0.001684527494944632 + 0.1 * 6.060577869415283
Epoch 1980, val loss: 1.248677372932434
Epoch 1990, training loss: 0.6066277027130127 = 0.0016708873445168138 + 0.1 * 6.049568176269531
Epoch 1990, val loss: 1.2502423524856567
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.5904
Flip ASR: 0.5333/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.8012900352478027 = 1.963919997215271 + 0.1 * 8.373701095581055
Epoch 0, val loss: 1.9581298828125
Epoch 10, training loss: 2.789997100830078 = 1.952670931816101 + 0.1 * 8.373262405395508
Epoch 10, val loss: 1.9475339651107788
Epoch 20, training loss: 2.7763278484344482 = 1.9392921924591064 + 0.1 * 8.370355606079102
Epoch 20, val loss: 1.9346860647201538
Epoch 30, training loss: 2.755733013153076 = 1.9209458827972412 + 0.1 * 8.347870826721191
Epoch 30, val loss: 1.916853427886963
Epoch 40, training loss: 2.7130892276763916 = 1.894315481185913 + 0.1 * 8.187737464904785
Epoch 40, val loss: 1.8910540342330933
Epoch 50, training loss: 2.595006227493286 = 1.8610291481018066 + 0.1 * 7.339770317077637
Epoch 50, val loss: 1.8601754903793335
Epoch 60, training loss: 2.52467679977417 = 1.825142502784729 + 0.1 * 6.995344161987305
Epoch 60, val loss: 1.8268274068832397
Epoch 70, training loss: 2.46691632270813 = 1.7839808464050293 + 0.1 * 6.829355239868164
Epoch 70, val loss: 1.790608525276184
Epoch 80, training loss: 2.415004253387451 = 1.7417690753936768 + 0.1 * 6.732350826263428
Epoch 80, val loss: 1.755102515220642
Epoch 90, training loss: 2.3611109256744385 = 1.6946488618850708 + 0.1 * 6.6646199226379395
Epoch 90, val loss: 1.7137155532836914
Epoch 100, training loss: 2.2943813800811768 = 1.6313704252243042 + 0.1 * 6.6301093101501465
Epoch 100, val loss: 1.6589739322662354
Epoch 110, training loss: 2.2088658809661865 = 1.5479824542999268 + 0.1 * 6.608833312988281
Epoch 110, val loss: 1.5904725790023804
Epoch 120, training loss: 2.1039340496063232 = 1.4446780681610107 + 0.1 * 6.592560291290283
Epoch 120, val loss: 1.507796049118042
Epoch 130, training loss: 1.988947868347168 = 1.3310836553573608 + 0.1 * 6.578641891479492
Epoch 130, val loss: 1.4180244207382202
Epoch 140, training loss: 1.8743112087249756 = 1.2180957794189453 + 0.1 * 6.562154769897461
Epoch 140, val loss: 1.3314133882522583
Epoch 150, training loss: 1.7655882835388184 = 1.1115611791610718 + 0.1 * 6.540270805358887
Epoch 150, val loss: 1.2518473863601685
Epoch 160, training loss: 1.6656813621520996 = 1.0134854316711426 + 0.1 * 6.521958827972412
Epoch 160, val loss: 1.1790651082992554
Epoch 170, training loss: 1.574751377105713 = 0.9254494309425354 + 0.1 * 6.493020057678223
Epoch 170, val loss: 1.1148192882537842
Epoch 180, training loss: 1.493013858795166 = 0.8459351658821106 + 0.1 * 6.470787048339844
Epoch 180, val loss: 1.0580562353134155
Epoch 190, training loss: 1.4209612607955933 = 0.7749384641647339 + 0.1 * 6.460227966308594
Epoch 190, val loss: 1.009116768836975
Epoch 200, training loss: 1.3577431440353394 = 0.7130786776542664 + 0.1 * 6.446644306182861
Epoch 200, val loss: 0.9682618379592896
Epoch 210, training loss: 1.302752137184143 = 0.6591099500656128 + 0.1 * 6.436421871185303
Epoch 210, val loss: 0.9348509907722473
Epoch 220, training loss: 1.2542753219604492 = 0.6113079786300659 + 0.1 * 6.429673671722412
Epoch 220, val loss: 0.9072964191436768
Epoch 230, training loss: 1.2099502086639404 = 0.5681614875793457 + 0.1 * 6.417886734008789
Epoch 230, val loss: 0.884655773639679
Epoch 240, training loss: 1.170050859451294 = 0.528215765953064 + 0.1 * 6.418351650238037
Epoch 240, val loss: 0.8655951619148254
Epoch 250, training loss: 1.1314666271209717 = 0.4907906651496887 + 0.1 * 6.406759262084961
Epoch 250, val loss: 0.8493355512619019
Epoch 260, training loss: 1.0945870876312256 = 0.4549393653869629 + 0.1 * 6.396477222442627
Epoch 260, val loss: 0.835114061832428
Epoch 270, training loss: 1.061439037322998 = 0.42006629705429077 + 0.1 * 6.413727760314941
Epoch 270, val loss: 0.8225399255752563
Epoch 280, training loss: 1.0256575345993042 = 0.3864644467830658 + 0.1 * 6.391931056976318
Epoch 280, val loss: 0.8118176460266113
Epoch 290, training loss: 0.9911580085754395 = 0.35370200872421265 + 0.1 * 6.3745598793029785
Epoch 290, val loss: 0.8026689291000366
Epoch 300, training loss: 0.9589413404464722 = 0.32167932391166687 + 0.1 * 6.37261962890625
Epoch 300, val loss: 0.7955209612846375
Epoch 310, training loss: 0.9281826019287109 = 0.29089197516441345 + 0.1 * 6.372906684875488
Epoch 310, val loss: 0.790318489074707
Epoch 320, training loss: 0.8966526985168457 = 0.26144126057624817 + 0.1 * 6.352114677429199
Epoch 320, val loss: 0.7869646549224854
Epoch 330, training loss: 0.8700865507125854 = 0.2334800660610199 + 0.1 * 6.366064548492432
Epoch 330, val loss: 0.7856142520904541
Epoch 340, training loss: 0.8420737981796265 = 0.20765389502048492 + 0.1 * 6.344199180603027
Epoch 340, val loss: 0.7862700819969177
Epoch 350, training loss: 0.8173933625221252 = 0.18411557376384735 + 0.1 * 6.332777500152588
Epoch 350, val loss: 0.7890222668647766
Epoch 360, training loss: 0.7963478565216064 = 0.16320253908634186 + 0.1 * 6.331453323364258
Epoch 360, val loss: 0.7939628958702087
Epoch 370, training loss: 0.7770639657974243 = 0.1450025737285614 + 0.1 * 6.320614337921143
Epoch 370, val loss: 0.8007011413574219
Epoch 380, training loss: 0.7601507306098938 = 0.12921440601348877 + 0.1 * 6.30936336517334
Epoch 380, val loss: 0.809103786945343
Epoch 390, training loss: 0.7465869784355164 = 0.11562757194042206 + 0.1 * 6.309593677520752
Epoch 390, val loss: 0.8187518119812012
Epoch 400, training loss: 0.7341343760490417 = 0.10396618396043777 + 0.1 * 6.301681995391846
Epoch 400, val loss: 0.8293365836143494
Epoch 410, training loss: 0.7247600555419922 = 0.09387938678264618 + 0.1 * 6.308806419372559
Epoch 410, val loss: 0.8407437801361084
Epoch 420, training loss: 0.7130263447761536 = 0.08515436947345734 + 0.1 * 6.278719902038574
Epoch 420, val loss: 0.852379322052002
Epoch 430, training loss: 0.7055603265762329 = 0.07753519713878632 + 0.1 * 6.2802510261535645
Epoch 430, val loss: 0.8643108606338501
Epoch 440, training loss: 0.6980101466178894 = 0.0708504319190979 + 0.1 * 6.271596908569336
Epoch 440, val loss: 0.876390814781189
Epoch 450, training loss: 0.6912561655044556 = 0.06494054943323135 + 0.1 * 6.263155937194824
Epoch 450, val loss: 0.8885132670402527
Epoch 460, training loss: 0.6857519149780273 = 0.05968121066689491 + 0.1 * 6.260706424713135
Epoch 460, val loss: 0.9006722569465637
Epoch 470, training loss: 0.6827654838562012 = 0.055042069405317307 + 0.1 * 6.277234077453613
Epoch 470, val loss: 0.9127252697944641
Epoch 480, training loss: 0.6754434704780579 = 0.05093155428767204 + 0.1 * 6.245119094848633
Epoch 480, val loss: 0.9245461225509644
Epoch 490, training loss: 0.6720063090324402 = 0.047234583646059036 + 0.1 * 6.247716903686523
Epoch 490, val loss: 0.9362909197807312
Epoch 500, training loss: 0.6678017377853394 = 0.04390835762023926 + 0.1 * 6.238933563232422
Epoch 500, val loss: 0.9478893876075745
Epoch 510, training loss: 0.6667717099189758 = 0.04090096428990364 + 0.1 * 6.258707523345947
Epoch 510, val loss: 0.9593403935432434
Epoch 520, training loss: 0.6610915064811707 = 0.038187719881534576 + 0.1 * 6.229037284851074
Epoch 520, val loss: 0.9705308675765991
Epoch 530, training loss: 0.6586874723434448 = 0.03572262451052666 + 0.1 * 6.229648590087891
Epoch 530, val loss: 0.9815698862075806
Epoch 540, training loss: 0.656037449836731 = 0.03348464146256447 + 0.1 * 6.225527763366699
Epoch 540, val loss: 0.9924402236938477
Epoch 550, training loss: 0.6529300212860107 = 0.03144754469394684 + 0.1 * 6.214824676513672
Epoch 550, val loss: 1.002984881401062
Epoch 560, training loss: 0.6508805751800537 = 0.029582809656858444 + 0.1 * 6.212977886199951
Epoch 560, val loss: 1.0133801698684692
Epoch 570, training loss: 0.6484981775283813 = 0.027879171073436737 + 0.1 * 6.20619010925293
Epoch 570, val loss: 1.023496150970459
Epoch 580, training loss: 0.6475356817245483 = 0.02631881833076477 + 0.1 * 6.212168216705322
Epoch 580, val loss: 1.033360481262207
Epoch 590, training loss: 0.6446306109428406 = 0.024887792766094208 + 0.1 * 6.197428226470947
Epoch 590, val loss: 1.0429807901382446
Epoch 600, training loss: 0.6447666883468628 = 0.02356979250907898 + 0.1 * 6.211968421936035
Epoch 600, val loss: 1.0524063110351562
Epoch 610, training loss: 0.64136803150177 = 0.022355761379003525 + 0.1 * 6.190123081207275
Epoch 610, val loss: 1.0615993738174438
Epoch 620, training loss: 0.6405507326126099 = 0.021234426647424698 + 0.1 * 6.19316291809082
Epoch 620, val loss: 1.0705394744873047
Epoch 630, training loss: 0.6394286751747131 = 0.02019774541258812 + 0.1 * 6.192309379577637
Epoch 630, val loss: 1.0792518854141235
Epoch 640, training loss: 0.6370999813079834 = 0.019240453839302063 + 0.1 * 6.178595066070557
Epoch 640, val loss: 1.0877879858016968
Epoch 650, training loss: 0.6373065710067749 = 0.018348930403590202 + 0.1 * 6.189576148986816
Epoch 650, val loss: 1.096131682395935
Epoch 660, training loss: 0.635095477104187 = 0.017519162967801094 + 0.1 * 6.175763130187988
Epoch 660, val loss: 1.104254961013794
Epoch 670, training loss: 0.6360585689544678 = 0.01674586348235607 + 0.1 * 6.193126678466797
Epoch 670, val loss: 1.1122130155563354
Epoch 680, training loss: 0.633245587348938 = 0.016026947647333145 + 0.1 * 6.172185897827148
Epoch 680, val loss: 1.119955062866211
Epoch 690, training loss: 0.6320636868476868 = 0.015356404706835747 + 0.1 * 6.167072772979736
Epoch 690, val loss: 1.1275581121444702
Epoch 700, training loss: 0.6312288641929626 = 0.014726869761943817 + 0.1 * 6.165019512176514
Epoch 700, val loss: 1.1350185871124268
Epoch 710, training loss: 0.6298866271972656 = 0.01413948368281126 + 0.1 * 6.157471179962158
Epoch 710, val loss: 1.1423003673553467
Epoch 720, training loss: 0.6299726963043213 = 0.013586781919002533 + 0.1 * 6.1638593673706055
Epoch 720, val loss: 1.149442195892334
Epoch 730, training loss: 0.6286911964416504 = 0.013068248517811298 + 0.1 * 6.156229496002197
Epoch 730, val loss: 1.15639328956604
Epoch 740, training loss: 0.6276801824569702 = 0.01258136611431837 + 0.1 * 6.1509881019592285
Epoch 740, val loss: 1.1632330417633057
Epoch 750, training loss: 0.627373993396759 = 0.012122104875743389 + 0.1 * 6.1525187492370605
Epoch 750, val loss: 1.169932246208191
Epoch 760, training loss: 0.6273148655891418 = 0.011688114143908024 + 0.1 * 6.156267166137695
Epoch 760, val loss: 1.176477313041687
Epoch 770, training loss: 0.6268671751022339 = 0.011279811151325703 + 0.1 * 6.1558732986450195
Epoch 770, val loss: 1.1829214096069336
Epoch 780, training loss: 0.6249793171882629 = 0.010894184932112694 + 0.1 * 6.140851020812988
Epoch 780, val loss: 1.1892062425613403
Epoch 790, training loss: 0.6247462034225464 = 0.010530086234211922 + 0.1 * 6.142160892486572
Epoch 790, val loss: 1.1954041719436646
Epoch 800, training loss: 0.6246464848518372 = 0.010184353217482567 + 0.1 * 6.1446213722229
Epoch 800, val loss: 1.2014962434768677
Epoch 810, training loss: 0.6232678294181824 = 0.009858150966465473 + 0.1 * 6.134096622467041
Epoch 810, val loss: 1.207423210144043
Epoch 820, training loss: 0.6236786246299744 = 0.009548555128276348 + 0.1 * 6.141300678253174
Epoch 820, val loss: 1.2132883071899414
Epoch 830, training loss: 0.6223546266555786 = 0.009253457188606262 + 0.1 * 6.131011962890625
Epoch 830, val loss: 1.2190513610839844
Epoch 840, training loss: 0.6221097111701965 = 0.008974220603704453 + 0.1 * 6.131354808807373
Epoch 840, val loss: 1.2247076034545898
Epoch 850, training loss: 0.6216559410095215 = 0.00870789960026741 + 0.1 * 6.129480361938477
Epoch 850, val loss: 1.2302560806274414
Epoch 860, training loss: 0.6227919459342957 = 0.008453725837171078 + 0.1 * 6.1433820724487305
Epoch 860, val loss: 1.2357209920883179
Epoch 870, training loss: 0.6209719777107239 = 0.008212203159928322 + 0.1 * 6.127597332000732
Epoch 870, val loss: 1.2410311698913574
Epoch 880, training loss: 0.622338056564331 = 0.007981588132679462 + 0.1 * 6.143564701080322
Epoch 880, val loss: 1.2463054656982422
Epoch 890, training loss: 0.6204487681388855 = 0.007761513348668814 + 0.1 * 6.1268720626831055
Epoch 890, val loss: 1.2515116930007935
Epoch 900, training loss: 0.6213282942771912 = 0.007551472634077072 + 0.1 * 6.137767791748047
Epoch 900, val loss: 1.2565861940383911
Epoch 910, training loss: 0.6196112632751465 = 0.007351072039455175 + 0.1 * 6.122601509094238
Epoch 910, val loss: 1.2616121768951416
Epoch 920, training loss: 0.618444561958313 = 0.007159417495131493 + 0.1 * 6.112851142883301
Epoch 920, val loss: 1.2665348052978516
Epoch 930, training loss: 0.6205182075500488 = 0.00697573134675622 + 0.1 * 6.135424613952637
Epoch 930, val loss: 1.2713721990585327
Epoch 940, training loss: 0.6189199686050415 = 0.006799027323722839 + 0.1 * 6.121209144592285
Epoch 940, val loss: 1.276072382926941
Epoch 950, training loss: 0.6181174516677856 = 0.00663081044331193 + 0.1 * 6.114865779876709
Epoch 950, val loss: 1.2806963920593262
Epoch 960, training loss: 0.6177185773849487 = 0.00646989094093442 + 0.1 * 6.112486839294434
Epoch 960, val loss: 1.2852338552474976
Epoch 970, training loss: 0.6190863251686096 = 0.006314862985163927 + 0.1 * 6.12771463394165
Epoch 970, val loss: 1.2897142171859741
Epoch 980, training loss: 0.6180850267410278 = 0.006165699567645788 + 0.1 * 6.1191935539245605
Epoch 980, val loss: 1.2941560745239258
Epoch 990, training loss: 0.6171208024024963 = 0.006022599991410971 + 0.1 * 6.1109819412231445
Epoch 990, val loss: 1.2984308004379272
Epoch 1000, training loss: 0.6167048811912537 = 0.00588555121794343 + 0.1 * 6.108193397521973
Epoch 1000, val loss: 1.3027247190475464
Epoch 1010, training loss: 0.6164719462394714 = 0.005753083620220423 + 0.1 * 6.1071882247924805
Epoch 1010, val loss: 1.3069524765014648
Epoch 1020, training loss: 0.617491602897644 = 0.00562559999525547 + 0.1 * 6.118659973144531
Epoch 1020, val loss: 1.3111110925674438
Epoch 1030, training loss: 0.6157417893409729 = 0.005502911750227213 + 0.1 * 6.102388381958008
Epoch 1030, val loss: 1.315180778503418
Epoch 1040, training loss: 0.6165662407875061 = 0.005385072436183691 + 0.1 * 6.11181116104126
Epoch 1040, val loss: 1.3192138671875
Epoch 1050, training loss: 0.6153526306152344 = 0.0052709574811160564 + 0.1 * 6.10081672668457
Epoch 1050, val loss: 1.3232421875
Epoch 1060, training loss: 0.6171625852584839 = 0.005160575266927481 + 0.1 * 6.120019912719727
Epoch 1060, val loss: 1.3271487951278687
Epoch 1070, training loss: 0.6153782606124878 = 0.005055087618529797 + 0.1 * 6.103231906890869
Epoch 1070, val loss: 1.3310301303863525
Epoch 1080, training loss: 0.6151130199432373 = 0.004953356459736824 + 0.1 * 6.101596355438232
Epoch 1080, val loss: 1.3348275423049927
Epoch 1090, training loss: 0.6145802140235901 = 0.004854843486100435 + 0.1 * 6.097253322601318
Epoch 1090, val loss: 1.3386104106903076
Epoch 1100, training loss: 0.6146723628044128 = 0.004759094212204218 + 0.1 * 6.099133014678955
Epoch 1100, val loss: 1.342306137084961
Epoch 1110, training loss: 0.614603579044342 = 0.004666629713028669 + 0.1 * 6.099369525909424
Epoch 1110, val loss: 1.3459527492523193
Epoch 1120, training loss: 0.6147987246513367 = 0.00457743089646101 + 0.1 * 6.102212429046631
Epoch 1120, val loss: 1.349575161933899
Epoch 1130, training loss: 0.613345742225647 = 0.004490815568715334 + 0.1 * 6.0885491371154785
Epoch 1130, val loss: 1.35316801071167
Epoch 1140, training loss: 0.614119827747345 = 0.0044074649922549725 + 0.1 * 6.097123622894287
Epoch 1140, val loss: 1.3567276000976562
Epoch 1150, training loss: 0.6139025688171387 = 0.0043264031410217285 + 0.1 * 6.095761775970459
Epoch 1150, val loss: 1.3602627515792847
Epoch 1160, training loss: 0.6138134598731995 = 0.004247755743563175 + 0.1 * 6.095656394958496
Epoch 1160, val loss: 1.3637439012527466
Epoch 1170, training loss: 0.6137710809707642 = 0.004171556327491999 + 0.1 * 6.09599494934082
Epoch 1170, val loss: 1.367190957069397
Epoch 1180, training loss: 0.6126324534416199 = 0.004097912926226854 + 0.1 * 6.085345268249512
Epoch 1180, val loss: 1.3706084489822388
Epoch 1190, training loss: 0.6138043403625488 = 0.004026724025607109 + 0.1 * 6.097776412963867
Epoch 1190, val loss: 1.3739752769470215
Epoch 1200, training loss: 0.6123262047767639 = 0.003957467619329691 + 0.1 * 6.0836873054504395
Epoch 1200, val loss: 1.3772962093353271
Epoch 1210, training loss: 0.6125721335411072 = 0.003890684340149164 + 0.1 * 6.0868144035339355
Epoch 1210, val loss: 1.3805979490280151
Epoch 1220, training loss: 0.6131531000137329 = 0.0038254440296441317 + 0.1 * 6.093276500701904
Epoch 1220, val loss: 1.3838564157485962
Epoch 1230, training loss: 0.6123033165931702 = 0.003762156469747424 + 0.1 * 6.085411071777344
Epoch 1230, val loss: 1.3870537281036377
Epoch 1240, training loss: 0.6142640113830566 = 0.003700468223541975 + 0.1 * 6.105635166168213
Epoch 1240, val loss: 1.3902088403701782
Epoch 1250, training loss: 0.6119504570960999 = 0.0036407504230737686 + 0.1 * 6.083096504211426
Epoch 1250, val loss: 1.3933569192886353
Epoch 1260, training loss: 0.611621081829071 = 0.0035829455591738224 + 0.1 * 6.080380916595459
Epoch 1260, val loss: 1.3964221477508545
Epoch 1270, training loss: 0.6119710206985474 = 0.003526668529957533 + 0.1 * 6.084443092346191
Epoch 1270, val loss: 1.3995013236999512
Epoch 1280, training loss: 0.6113721132278442 = 0.0034716553054749966 + 0.1 * 6.079004287719727
Epoch 1280, val loss: 1.4025144577026367
Epoch 1290, training loss: 0.6117292046546936 = 0.003418194828554988 + 0.1 * 6.0831098556518555
Epoch 1290, val loss: 1.4054902791976929
Epoch 1300, training loss: 0.6109261512756348 = 0.003366463351994753 + 0.1 * 6.075596809387207
Epoch 1300, val loss: 1.408422589302063
Epoch 1310, training loss: 0.6118555068969727 = 0.0033161684405058622 + 0.1 * 6.08539342880249
Epoch 1310, val loss: 1.4113478660583496
Epoch 1320, training loss: 0.611572265625 = 0.003267081920057535 + 0.1 * 6.0830512046813965
Epoch 1320, val loss: 1.41425621509552
Epoch 1330, training loss: 0.6110547780990601 = 0.003219200298190117 + 0.1 * 6.07835578918457
Epoch 1330, val loss: 1.4171116352081299
Epoch 1340, training loss: 0.6111552715301514 = 0.003172706812620163 + 0.1 * 6.079825401306152
Epoch 1340, val loss: 1.4199187755584717
Epoch 1350, training loss: 0.6109116673469543 = 0.0031272773630917072 + 0.1 * 6.07784366607666
Epoch 1350, val loss: 1.4227046966552734
Epoch 1360, training loss: 0.6105920672416687 = 0.00308301136828959 + 0.1 * 6.0750908851623535
Epoch 1360, val loss: 1.4254568815231323
Epoch 1370, training loss: 0.6117650866508484 = 0.0030399898532778025 + 0.1 * 6.087250709533691
Epoch 1370, val loss: 1.4281750917434692
Epoch 1380, training loss: 0.6102097034454346 = 0.002998160431161523 + 0.1 * 6.072114944458008
Epoch 1380, val loss: 1.4309027194976807
Epoch 1390, training loss: 0.6100652813911438 = 0.0029573289211839437 + 0.1 * 6.071079254150391
Epoch 1390, val loss: 1.4335747957229614
Epoch 1400, training loss: 0.6105905175209045 = 0.0029172678478062153 + 0.1 * 6.076732158660889
Epoch 1400, val loss: 1.4362094402313232
Epoch 1410, training loss: 0.6103042960166931 = 0.002878151834011078 + 0.1 * 6.074261665344238
Epoch 1410, val loss: 1.4388245344161987
Epoch 1420, training loss: 0.6107609272003174 = 0.002840148750692606 + 0.1 * 6.079207420349121
Epoch 1420, val loss: 1.4413857460021973
Epoch 1430, training loss: 0.6094234585762024 = 0.0028028676752001047 + 0.1 * 6.066205978393555
Epoch 1430, val loss: 1.4439131021499634
Epoch 1440, training loss: 0.6103550791740417 = 0.0027667514514178038 + 0.1 * 6.075883388519287
Epoch 1440, val loss: 1.4464292526245117
Epoch 1450, training loss: 0.6104475259780884 = 0.0027312259189784527 + 0.1 * 6.077163219451904
Epoch 1450, val loss: 1.448943018913269
Epoch 1460, training loss: 0.6092022061347961 = 0.0026965795550495386 + 0.1 * 6.065056324005127
Epoch 1460, val loss: 1.451416254043579
Epoch 1470, training loss: 0.6091563701629639 = 0.0026628400664776564 + 0.1 * 6.064935207366943
Epoch 1470, val loss: 1.4538722038269043
Epoch 1480, training loss: 0.608889102935791 = 0.0026298000011593103 + 0.1 * 6.06259298324585
Epoch 1480, val loss: 1.4563242197036743
Epoch 1490, training loss: 0.6113117933273315 = 0.0025973482988774776 + 0.1 * 6.087144374847412
Epoch 1490, val loss: 1.458755612373352
Epoch 1500, training loss: 0.6102632284164429 = 0.002565201139077544 + 0.1 * 6.076980113983154
Epoch 1500, val loss: 1.4611278772354126
Epoch 1510, training loss: 0.6090496778488159 = 0.0025342032313346863 + 0.1 * 6.065154552459717
Epoch 1510, val loss: 1.4634649753570557
Epoch 1520, training loss: 0.6089730262756348 = 0.002504198346287012 + 0.1 * 6.064688205718994
Epoch 1520, val loss: 1.4657989740371704
Epoch 1530, training loss: 0.6092983484268188 = 0.0024746654089540243 + 0.1 * 6.068236827850342
Epoch 1530, val loss: 1.4681602716445923
Epoch 1540, training loss: 0.6089951992034912 = 0.0024457157123833895 + 0.1 * 6.065495014190674
Epoch 1540, val loss: 1.4704760313034058
Epoch 1550, training loss: 0.6091511249542236 = 0.002417387440800667 + 0.1 * 6.0673370361328125
Epoch 1550, val loss: 1.4727412462234497
Epoch 1560, training loss: 0.6084977984428406 = 0.002389725297689438 + 0.1 * 6.061080455780029
Epoch 1560, val loss: 1.474985957145691
Epoch 1570, training loss: 0.6094084978103638 = 0.0023627090267837048 + 0.1 * 6.070457458496094
Epoch 1570, val loss: 1.4772182703018188
Epoch 1580, training loss: 0.6083176732063293 = 0.002335991943255067 + 0.1 * 6.059816837310791
Epoch 1580, val loss: 1.479419469833374
Epoch 1590, training loss: 0.6089945435523987 = 0.0023100501857697964 + 0.1 * 6.066844940185547
Epoch 1590, val loss: 1.481602430343628
Epoch 1600, training loss: 0.6079099774360657 = 0.002284633694216609 + 0.1 * 6.056252956390381
Epoch 1600, val loss: 1.4837902784347534
Epoch 1610, training loss: 0.6085026860237122 = 0.002259791363030672 + 0.1 * 6.062428951263428
Epoch 1610, val loss: 1.485949158668518
Epoch 1620, training loss: 0.6080793738365173 = 0.002235210267826915 + 0.1 * 6.058441162109375
Epoch 1620, val loss: 1.488072395324707
Epoch 1630, training loss: 0.6087502837181091 = 0.002211097627878189 + 0.1 * 6.065392017364502
Epoch 1630, val loss: 1.4902094602584839
Epoch 1640, training loss: 0.6075242161750793 = 0.0021874692756682634 + 0.1 * 6.0533671379089355
Epoch 1640, val loss: 1.4923348426818848
Epoch 1650, training loss: 0.6091840267181396 = 0.002164430683478713 + 0.1 * 6.070196151733398
Epoch 1650, val loss: 1.494428277015686
Epoch 1660, training loss: 0.6083140969276428 = 0.002141536446288228 + 0.1 * 6.061725616455078
Epoch 1660, val loss: 1.4964947700500488
Epoch 1670, training loss: 0.6086978316307068 = 0.0021194142755120993 + 0.1 * 6.065783977508545
Epoch 1670, val loss: 1.498548984527588
Epoch 1680, training loss: 0.6071791648864746 = 0.002097703516483307 + 0.1 * 6.050814151763916
Epoch 1680, val loss: 1.5005851984024048
Epoch 1690, training loss: 0.6080867648124695 = 0.0020764905493706465 + 0.1 * 6.060102462768555
Epoch 1690, val loss: 1.5026206970214844
Epoch 1700, training loss: 0.6078802347183228 = 0.002055373042821884 + 0.1 * 6.058248519897461
Epoch 1700, val loss: 1.5046511888504028
Epoch 1710, training loss: 0.6073179244995117 = 0.0020347326062619686 + 0.1 * 6.052831649780273
Epoch 1710, val loss: 1.5066537857055664
Epoch 1720, training loss: 0.6070303916931152 = 0.0020143233705312014 + 0.1 * 6.0501604080200195
Epoch 1720, val loss: 1.508655309677124
Epoch 1730, training loss: 0.6075655817985535 = 0.001994494115933776 + 0.1 * 6.055710792541504
Epoch 1730, val loss: 1.510623574256897
Epoch 1740, training loss: 0.6073665618896484 = 0.001974891172721982 + 0.1 * 6.0539164543151855
Epoch 1740, val loss: 1.512591004371643
Epoch 1750, training loss: 0.6069208383560181 = 0.0019558477215468884 + 0.1 * 6.049649715423584
Epoch 1750, val loss: 1.514512062072754
Epoch 1760, training loss: 0.6079419851303101 = 0.001937048975378275 + 0.1 * 6.060049533843994
Epoch 1760, val loss: 1.5164244174957275
Epoch 1770, training loss: 0.607286810874939 = 0.0019184884149581194 + 0.1 * 6.053683280944824
Epoch 1770, val loss: 1.5183457136154175
Epoch 1780, training loss: 0.6077074408531189 = 0.0019003654597327113 + 0.1 * 6.058070659637451
Epoch 1780, val loss: 1.5202184915542603
Epoch 1790, training loss: 0.6068769693374634 = 0.0018824493745341897 + 0.1 * 6.04994535446167
Epoch 1790, val loss: 1.522080421447754
Epoch 1800, training loss: 0.607360303401947 = 0.0018650352722033858 + 0.1 * 6.054952621459961
Epoch 1800, val loss: 1.5239200592041016
Epoch 1810, training loss: 0.6065195202827454 = 0.001847677631303668 + 0.1 * 6.046718120574951
Epoch 1810, val loss: 1.525787115097046
Epoch 1820, training loss: 0.6068779230117798 = 0.0018309701699763536 + 0.1 * 6.050469398498535
Epoch 1820, val loss: 1.5276085138320923
Epoch 1830, training loss: 0.6065559387207031 = 0.0018143614288419485 + 0.1 * 6.047415733337402
Epoch 1830, val loss: 1.5294392108917236
Epoch 1840, training loss: 0.607108473777771 = 0.001797994482330978 + 0.1 * 6.053104877471924
Epoch 1840, val loss: 1.5312515497207642
Epoch 1850, training loss: 0.6061791777610779 = 0.0017816772451624274 + 0.1 * 6.04397439956665
Epoch 1850, val loss: 1.5330628156661987
Epoch 1860, training loss: 0.6065207719802856 = 0.0017659444129094481 + 0.1 * 6.047547817230225
Epoch 1860, val loss: 1.5348384380340576
Epoch 1870, training loss: 0.6074119806289673 = 0.001750418683513999 + 0.1 * 6.056615829467773
Epoch 1870, val loss: 1.5366175174713135
Epoch 1880, training loss: 0.6062315702438354 = 0.0017350779380649328 + 0.1 * 6.044964790344238
Epoch 1880, val loss: 1.5383981466293335
Epoch 1890, training loss: 0.6066129803657532 = 0.0017200818983837962 + 0.1 * 6.048928737640381
Epoch 1890, val loss: 1.5401729345321655
Epoch 1900, training loss: 0.6065816879272461 = 0.0017052945913746953 + 0.1 * 6.048763751983643
Epoch 1900, val loss: 1.5419234037399292
Epoch 1910, training loss: 0.6061882376670837 = 0.0016907418612390757 + 0.1 * 6.0449748039245605
Epoch 1910, val loss: 1.5436521768569946
Epoch 1920, training loss: 0.6061610579490662 = 0.001676300074905157 + 0.1 * 6.04484748840332
Epoch 1920, val loss: 1.545403242111206
Epoch 1930, training loss: 0.606380045413971 = 0.0016620930982753634 + 0.1 * 6.047179698944092
Epoch 1930, val loss: 1.5471216440200806
Epoch 1940, training loss: 0.6061372756958008 = 0.001648199395276606 + 0.1 * 6.044890880584717
Epoch 1940, val loss: 1.5488078594207764
Epoch 1950, training loss: 0.6063377261161804 = 0.001634513959288597 + 0.1 * 6.047031879425049
Epoch 1950, val loss: 1.5505053997039795
Epoch 1960, training loss: 0.6069155931472778 = 0.001621043193154037 + 0.1 * 6.052945137023926
Epoch 1960, val loss: 1.5522139072418213
Epoch 1970, training loss: 0.6057835221290588 = 0.001607762766070664 + 0.1 * 6.041757583618164
Epoch 1970, val loss: 1.5538883209228516
Epoch 1980, training loss: 0.6078903675079346 = 0.0015947747742757201 + 0.1 * 6.062955856323242
Epoch 1980, val loss: 1.5555657148361206
Epoch 1990, training loss: 0.6056861281394958 = 0.0015818928368389606 + 0.1 * 6.041042327880859
Epoch 1990, val loss: 1.5572088956832886
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8339
Flip ASR: 0.8089/225 nodes
The final ASR:0.68266, 0.10783, Accuracy:0.82469, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9562])
updated graph: torch.Size([2, 10644])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.83580, 0.00349
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7622265815734863 = 1.9248467683792114 + 0.1 * 8.373799324035645
Epoch 0, val loss: 1.9244298934936523
Epoch 10, training loss: 2.752988815307617 = 1.915629267692566 + 0.1 * 8.373595237731934
Epoch 10, val loss: 1.9158214330673218
Epoch 20, training loss: 2.7417123317718506 = 1.9044896364212036 + 0.1 * 8.37222671508789
Epoch 20, val loss: 1.9049912691116333
Epoch 30, training loss: 2.725005626678467 = 1.888923168182373 + 0.1 * 8.360823631286621
Epoch 30, val loss: 1.8894727230072021
Epoch 40, training loss: 2.693481922149658 = 1.8661669492721558 + 0.1 * 8.273150444030762
Epoch 40, val loss: 1.8667593002319336
Epoch 50, training loss: 2.6060259342193604 = 1.8367928266525269 + 0.1 * 7.692330837249756
Epoch 50, val loss: 1.8388373851776123
Epoch 60, training loss: 2.5294058322906494 = 1.8086739778518677 + 0.1 * 7.20731782913208
Epoch 60, val loss: 1.8137753009796143
Epoch 70, training loss: 2.4684653282165527 = 1.7808897495269775 + 0.1 * 6.875755310058594
Epoch 70, val loss: 1.7899646759033203
Epoch 80, training loss: 2.423706293106079 = 1.7493997812271118 + 0.1 * 6.743065357208252
Epoch 80, val loss: 1.7633213996887207
Epoch 90, training loss: 2.3741555213928223 = 1.707918405532837 + 0.1 * 6.662371635437012
Epoch 90, val loss: 1.7273658514022827
Epoch 100, training loss: 2.3125932216644287 = 1.651223063468933 + 0.1 * 6.613700866699219
Epoch 100, val loss: 1.6780898571014404
Epoch 110, training loss: 2.2342312335968018 = 1.5763784646987915 + 0.1 * 6.578527450561523
Epoch 110, val loss: 1.6143758296966553
Epoch 120, training loss: 2.1403579711914062 = 1.4850642681121826 + 0.1 * 6.552937030792236
Epoch 120, val loss: 1.5380659103393555
Epoch 130, training loss: 2.037102699279785 = 1.3833526372909546 + 0.1 * 6.53749942779541
Epoch 130, val loss: 1.4548372030258179
Epoch 140, training loss: 1.9290636777877808 = 1.276776671409607 + 0.1 * 6.522870063781738
Epoch 140, val loss: 1.3702560663223267
Epoch 150, training loss: 1.8217408657073975 = 1.1700814962387085 + 0.1 * 6.516592979431152
Epoch 150, val loss: 1.2894221544265747
Epoch 160, training loss: 1.7213103771209717 = 1.0712660551071167 + 0.1 * 6.500443458557129
Epoch 160, val loss: 1.2172973155975342
Epoch 170, training loss: 1.6313984394073486 = 0.982363760471344 + 0.1 * 6.490347385406494
Epoch 170, val loss: 1.1531625986099243
Epoch 180, training loss: 1.5515251159667969 = 0.9033060073852539 + 0.1 * 6.482190132141113
Epoch 180, val loss: 1.0967971086502075
Epoch 190, training loss: 1.4786696434020996 = 0.8311896324157715 + 0.1 * 6.474799633026123
Epoch 190, val loss: 1.0454134941101074
Epoch 200, training loss: 1.4106781482696533 = 0.7632200717926025 + 0.1 * 6.474581241607666
Epoch 200, val loss: 0.9967926144599915
Epoch 210, training loss: 1.3454341888427734 = 0.6991166472434998 + 0.1 * 6.4631757736206055
Epoch 210, val loss: 0.9511188864707947
Epoch 220, training loss: 1.2837491035461426 = 0.6382318139076233 + 0.1 * 6.455172538757324
Epoch 220, val loss: 0.9079616665840149
Epoch 230, training loss: 1.2260953187942505 = 0.5811843872070312 + 0.1 * 6.449109077453613
Epoch 230, val loss: 0.8684926629066467
Epoch 240, training loss: 1.172858715057373 = 0.5285146236419678 + 0.1 * 6.4434404373168945
Epoch 240, val loss: 0.8335718512535095
Epoch 250, training loss: 1.1233729124069214 = 0.4796556234359741 + 0.1 * 6.437172889709473
Epoch 250, val loss: 0.8036565780639648
Epoch 260, training loss: 1.0775355100631714 = 0.43428120017051697 + 0.1 * 6.43254280090332
Epoch 260, val loss: 0.7785399556159973
Epoch 270, training loss: 1.0339330434799194 = 0.3917194902896881 + 0.1 * 6.422135829925537
Epoch 270, val loss: 0.757714033126831
Epoch 280, training loss: 0.9930515885353088 = 0.3514885902404785 + 0.1 * 6.415629863739014
Epoch 280, val loss: 0.7403414249420166
Epoch 290, training loss: 0.954773485660553 = 0.31374406814575195 + 0.1 * 6.410294055938721
Epoch 290, val loss: 0.726025402545929
Epoch 300, training loss: 0.9202231764793396 = 0.27869731187820435 + 0.1 * 6.415258407592773
Epoch 300, val loss: 0.71441650390625
Epoch 310, training loss: 0.8866509795188904 = 0.24697448313236237 + 0.1 * 6.396765232086182
Epoch 310, val loss: 0.7054902911186218
Epoch 320, training loss: 0.857851505279541 = 0.21866858005523682 + 0.1 * 6.391829013824463
Epoch 320, val loss: 0.6989943981170654
Epoch 330, training loss: 0.8327361345291138 = 0.1939057856798172 + 0.1 * 6.388303756713867
Epoch 330, val loss: 0.6950089335441589
Epoch 340, training loss: 0.8100569248199463 = 0.1724359542131424 + 0.1 * 6.376209735870361
Epoch 340, val loss: 0.6930723190307617
Epoch 350, training loss: 0.790452241897583 = 0.15369708836078644 + 0.1 * 6.367551803588867
Epoch 350, val loss: 0.69306480884552
Epoch 360, training loss: 0.7748836278915405 = 0.1373164802789688 + 0.1 * 6.37567138671875
Epoch 360, val loss: 0.6948347091674805
Epoch 370, training loss: 0.759098470211029 = 0.12307129800319672 + 0.1 * 6.360271453857422
Epoch 370, val loss: 0.6978890299797058
Epoch 380, training loss: 0.7462315559387207 = 0.11061730235815048 + 0.1 * 6.356142520904541
Epoch 380, val loss: 0.7022005319595337
Epoch 390, training loss: 0.7337837815284729 = 0.09972494840621948 + 0.1 * 6.340588092803955
Epoch 390, val loss: 0.7073343992233276
Epoch 400, training loss: 0.724709689617157 = 0.09013641625642776 + 0.1 * 6.345732688903809
Epoch 400, val loss: 0.7131699323654175
Epoch 410, training loss: 0.7147578001022339 = 0.08173159509897232 + 0.1 * 6.330261707305908
Epoch 410, val loss: 0.719476044178009
Epoch 420, training loss: 0.705908477306366 = 0.07432787120342255 + 0.1 * 6.315805912017822
Epoch 420, val loss: 0.7260770797729492
Epoch 430, training loss: 0.7006804943084717 = 0.0677839070558548 + 0.1 * 6.32896614074707
Epoch 430, val loss: 0.7328555583953857
Epoch 440, training loss: 0.6928214430809021 = 0.06201458349823952 + 0.1 * 6.308068752288818
Epoch 440, val loss: 0.7397285103797913
Epoch 450, training loss: 0.6865187287330627 = 0.056883927434682846 + 0.1 * 6.296347618103027
Epoch 450, val loss: 0.7466832995414734
Epoch 460, training loss: 0.6817782521247864 = 0.05231926590204239 + 0.1 * 6.294589519500732
Epoch 460, val loss: 0.7536170482635498
Epoch 470, training loss: 0.6766310334205627 = 0.048253752291202545 + 0.1 * 6.2837724685668945
Epoch 470, val loss: 0.760550856590271
Epoch 480, training loss: 0.6734338402748108 = 0.044610243290662766 + 0.1 * 6.288235664367676
Epoch 480, val loss: 0.7674622535705566
Epoch 490, training loss: 0.6696485280990601 = 0.041341934353113174 + 0.1 * 6.283065319061279
Epoch 490, val loss: 0.7742727398872375
Epoch 500, training loss: 0.6649892926216125 = 0.038410112261772156 + 0.1 * 6.265791416168213
Epoch 500, val loss: 0.7809905409812927
Epoch 510, training loss: 0.6618047952651978 = 0.03576710447669029 + 0.1 * 6.260376930236816
Epoch 510, val loss: 0.7876843810081482
Epoch 520, training loss: 0.6598361730575562 = 0.033373426645994186 + 0.1 * 6.264627456665039
Epoch 520, val loss: 0.7942965626716614
Epoch 530, training loss: 0.6565445065498352 = 0.03120466321706772 + 0.1 * 6.253398418426514
Epoch 530, val loss: 0.8008741736412048
Epoch 540, training loss: 0.6549028158187866 = 0.0292318444699049 + 0.1 * 6.256709575653076
Epoch 540, val loss: 0.8073299527168274
Epoch 550, training loss: 0.6522517800331116 = 0.02744043432176113 + 0.1 * 6.248113632202148
Epoch 550, val loss: 0.8136406540870667
Epoch 560, training loss: 0.6497429013252258 = 0.025809114798903465 + 0.1 * 6.239337921142578
Epoch 560, val loss: 0.8199011087417603
Epoch 570, training loss: 0.6478332877159119 = 0.02431561052799225 + 0.1 * 6.235177040100098
Epoch 570, val loss: 0.826062798500061
Epoch 580, training loss: 0.6471193432807922 = 0.02294745109975338 + 0.1 * 6.2417192459106445
Epoch 580, val loss: 0.8320711255073547
Epoch 590, training loss: 0.6445705890655518 = 0.021695131435990334 + 0.1 * 6.22875452041626
Epoch 590, val loss: 0.8380306959152222
Epoch 600, training loss: 0.6426196694374084 = 0.02054394781589508 + 0.1 * 6.220757007598877
Epoch 600, val loss: 0.8438654541969299
Epoch 610, training loss: 0.6409270763397217 = 0.019480552524328232 + 0.1 * 6.214465141296387
Epoch 610, val loss: 0.8496202826499939
Epoch 620, training loss: 0.6413363218307495 = 0.01849748007953167 + 0.1 * 6.22838830947876
Epoch 620, val loss: 0.855255663394928
Epoch 630, training loss: 0.6392236351966858 = 0.017591828480362892 + 0.1 * 6.216318130493164
Epoch 630, val loss: 0.8607746958732605
Epoch 640, training loss: 0.6373571753501892 = 0.0167540255934 + 0.1 * 6.206031322479248
Epoch 640, val loss: 0.8662253022193909
Epoch 650, training loss: 0.6371824741363525 = 0.01597517542541027 + 0.1 * 6.212072849273682
Epoch 650, val loss: 0.871547520160675
Epoch 660, training loss: 0.6354060173034668 = 0.015253336168825626 + 0.1 * 6.201526641845703
Epoch 660, val loss: 0.8768194913864136
Epoch 670, training loss: 0.6368687748908997 = 0.014580375514924526 + 0.1 * 6.222884178161621
Epoch 670, val loss: 0.8820276856422424
Epoch 680, training loss: 0.6341124773025513 = 0.013953402638435364 + 0.1 * 6.2015910148620605
Epoch 680, val loss: 0.8870749473571777
Epoch 690, training loss: 0.6329967975616455 = 0.013369414955377579 + 0.1 * 6.196273326873779
Epoch 690, val loss: 0.8920843601226807
Epoch 700, training loss: 0.6329787373542786 = 0.012821182608604431 + 0.1 * 6.20157527923584
Epoch 700, val loss: 0.8969571590423584
Epoch 710, training loss: 0.6320651173591614 = 0.012308724224567413 + 0.1 * 6.197563648223877
Epoch 710, val loss: 0.9017840623855591
Epoch 720, training loss: 0.6306211948394775 = 0.011827566660940647 + 0.1 * 6.187935829162598
Epoch 720, val loss: 0.906451404094696
Epoch 730, training loss: 0.6298810839653015 = 0.011377534829080105 + 0.1 * 6.185035705566406
Epoch 730, val loss: 0.9111194610595703
Epoch 740, training loss: 0.6297447085380554 = 0.01095392182469368 + 0.1 * 6.187907695770264
Epoch 740, val loss: 0.9156811237335205
Epoch 750, training loss: 0.629325270652771 = 0.010554262436926365 + 0.1 * 6.187709808349609
Epoch 750, val loss: 0.9201502203941345
Epoch 760, training loss: 0.6287216544151306 = 0.010179315693676472 + 0.1 * 6.185422897338867
Epoch 760, val loss: 0.9245875477790833
Epoch 770, training loss: 0.6285979747772217 = 0.00982501357793808 + 0.1 * 6.187729835510254
Epoch 770, val loss: 0.9289314150810242
Epoch 780, training loss: 0.628173828125 = 0.009490584023296833 + 0.1 * 6.186831951141357
Epoch 780, val loss: 0.9331660866737366
Epoch 790, training loss: 0.6269904971122742 = 0.009174840524792671 + 0.1 * 6.178156852722168
Epoch 790, val loss: 0.9373698830604553
Epoch 800, training loss: 0.6268070936203003 = 0.00887557864189148 + 0.1 * 6.179314613342285
Epoch 800, val loss: 0.9415205121040344
Epoch 810, training loss: 0.6264010667800903 = 0.008592158555984497 + 0.1 * 6.178088665008545
Epoch 810, val loss: 0.9455953240394592
Epoch 820, training loss: 0.625178873538971 = 0.00832324754446745 + 0.1 * 6.168556213378906
Epoch 820, val loss: 0.9496006965637207
Epoch 830, training loss: 0.6258144974708557 = 0.008068272843956947 + 0.1 * 6.177462100982666
Epoch 830, val loss: 0.9535543918609619
Epoch 840, training loss: 0.624640703201294 = 0.00782517995685339 + 0.1 * 6.168155193328857
Epoch 840, val loss: 0.9573904275894165
Epoch 850, training loss: 0.624289870262146 = 0.007595261558890343 + 0.1 * 6.166945934295654
Epoch 850, val loss: 0.9612400531768799
Epoch 860, training loss: 0.6236072182655334 = 0.007376379333436489 + 0.1 * 6.162308216094971
Epoch 860, val loss: 0.9650291204452515
Epoch 870, training loss: 0.62418133020401 = 0.0071675279177725315 + 0.1 * 6.170137882232666
Epoch 870, val loss: 0.9687889814376831
Epoch 880, training loss: 0.6228131651878357 = 0.00696807075291872 + 0.1 * 6.158450603485107
Epoch 880, val loss: 0.972368597984314
Epoch 890, training loss: 0.6222045421600342 = 0.006778310053050518 + 0.1 * 6.154262542724609
Epoch 890, val loss: 0.9759965538978577
Epoch 900, training loss: 0.6239725947380066 = 0.006597024388611317 + 0.1 * 6.173755168914795
Epoch 900, val loss: 0.9795687198638916
Epoch 910, training loss: 0.6222806572914124 = 0.006423278711736202 + 0.1 * 6.158573627471924
Epoch 910, val loss: 0.9830198884010315
Epoch 920, training loss: 0.6220759749412537 = 0.006257958710193634 + 0.1 * 6.158180236816406
Epoch 920, val loss: 0.9865484237670898
Epoch 930, training loss: 0.6228442192077637 = 0.006099651101976633 + 0.1 * 6.167445659637451
Epoch 930, val loss: 0.9899480938911438
Epoch 940, training loss: 0.6209487318992615 = 0.005947602912783623 + 0.1 * 6.15001106262207
Epoch 940, val loss: 0.993293821811676
Epoch 950, training loss: 0.6215177774429321 = 0.00580254290252924 + 0.1 * 6.15715217590332
Epoch 950, val loss: 0.9966806173324585
Epoch 960, training loss: 0.6207171678543091 = 0.005662968382239342 + 0.1 * 6.15054178237915
Epoch 960, val loss: 0.9999436140060425
Epoch 970, training loss: 0.6197010278701782 = 0.005529334768652916 + 0.1 * 6.141716480255127
Epoch 970, val loss: 1.0032373666763306
Epoch 980, training loss: 0.6207094192504883 = 0.005401119124144316 + 0.1 * 6.153082847595215
Epoch 980, val loss: 1.006485104560852
Epoch 990, training loss: 0.6202533841133118 = 0.005277144722640514 + 0.1 * 6.149762153625488
Epoch 990, val loss: 1.0096089839935303
Epoch 1000, training loss: 0.6193538904190063 = 0.005158375948667526 + 0.1 * 6.141955375671387
Epoch 1000, val loss: 1.0127363204956055
Epoch 1010, training loss: 0.6199332475662231 = 0.005044413264840841 + 0.1 * 6.148888111114502
Epoch 1010, val loss: 1.0158329010009766
Epoch 1020, training loss: 0.6187834143638611 = 0.004935022443532944 + 0.1 * 6.13848352432251
Epoch 1020, val loss: 1.0188953876495361
Epoch 1030, training loss: 0.6186332702636719 = 0.004829697776585817 + 0.1 * 6.138035297393799
Epoch 1030, val loss: 1.0219511985778809
Epoch 1040, training loss: 0.6185433864593506 = 0.0047276681289076805 + 0.1 * 6.138156890869141
Epoch 1040, val loss: 1.024901032447815
Epoch 1050, training loss: 0.6189891695976257 = 0.004629132803529501 + 0.1 * 6.143599987030029
Epoch 1050, val loss: 1.0277979373931885
Epoch 1060, training loss: 0.6183631420135498 = 0.004534638486802578 + 0.1 * 6.138284683227539
Epoch 1060, val loss: 1.0306743383407593
Epoch 1070, training loss: 0.6175437569618225 = 0.004443732090294361 + 0.1 * 6.131000518798828
Epoch 1070, val loss: 1.0335876941680908
Epoch 1080, training loss: 0.618424654006958 = 0.004356091842055321 + 0.1 * 6.140685558319092
Epoch 1080, val loss: 1.0364444255828857
Epoch 1090, training loss: 0.6184225678443909 = 0.004270776174962521 + 0.1 * 6.141517639160156
Epoch 1090, val loss: 1.0391464233398438
Epoch 1100, training loss: 0.6173046827316284 = 0.004188485909253359 + 0.1 * 6.131161689758301
Epoch 1100, val loss: 1.0418709516525269
Epoch 1110, training loss: 0.6170515418052673 = 0.004109218716621399 + 0.1 * 6.129423141479492
Epoch 1110, val loss: 1.0446281433105469
Epoch 1120, training loss: 0.6176388263702393 = 0.004032533150166273 + 0.1 * 6.1360626220703125
Epoch 1120, val loss: 1.0473116636276245
Epoch 1130, training loss: 0.6173129081726074 = 0.003958128392696381 + 0.1 * 6.133547782897949
Epoch 1130, val loss: 1.049953818321228
Epoch 1140, training loss: 0.6177836060523987 = 0.0038864940870553255 + 0.1 * 6.138970851898193
Epoch 1140, val loss: 1.0525563955307007
Epoch 1150, training loss: 0.616467297077179 = 0.003816678887233138 + 0.1 * 6.126506328582764
Epoch 1150, val loss: 1.0551056861877441
Epoch 1160, training loss: 0.6169923543930054 = 0.003749716328456998 + 0.1 * 6.1324262619018555
Epoch 1160, val loss: 1.0576707124710083
Epoch 1170, training loss: 0.6157166957855225 = 0.0036845551803708076 + 0.1 * 6.120321273803711
Epoch 1170, val loss: 1.060194492340088
Epoch 1180, training loss: 0.6164061427116394 = 0.0036215679720044136 + 0.1 * 6.127845764160156
Epoch 1180, val loss: 1.062696099281311
Epoch 1190, training loss: 0.6156043410301208 = 0.0035602503921836615 + 0.1 * 6.12044095993042
Epoch 1190, val loss: 1.0651495456695557
Epoch 1200, training loss: 0.6168192625045776 = 0.003500712802633643 + 0.1 * 6.133185386657715
Epoch 1200, val loss: 1.0675736665725708
Epoch 1210, training loss: 0.6152520179748535 = 0.00344292214140296 + 0.1 * 6.118090629577637
Epoch 1210, val loss: 1.0699448585510254
Epoch 1220, training loss: 0.6153091788291931 = 0.003387212520465255 + 0.1 * 6.119219779968262
Epoch 1220, val loss: 1.0723828077316284
Epoch 1230, training loss: 0.6149868965148926 = 0.003332702210173011 + 0.1 * 6.116541862487793
Epoch 1230, val loss: 1.074758768081665
Epoch 1240, training loss: 0.615513801574707 = 0.0032795616425573826 + 0.1 * 6.122342109680176
Epoch 1240, val loss: 1.0770454406738281
Epoch 1250, training loss: 0.6147745251655579 = 0.0032280844170600176 + 0.1 * 6.115464210510254
Epoch 1250, val loss: 1.0793352127075195
Epoch 1260, training loss: 0.6170809268951416 = 0.003178392071276903 + 0.1 * 6.1390252113342285
Epoch 1260, val loss: 1.0816677808761597
Epoch 1270, training loss: 0.615001916885376 = 0.0031295171938836575 + 0.1 * 6.118723392486572
Epoch 1270, val loss: 1.08390474319458
Epoch 1280, training loss: 0.6144242286682129 = 0.0030825685244053602 + 0.1 * 6.11341667175293
Epoch 1280, val loss: 1.0861365795135498
Epoch 1290, training loss: 0.6147224307060242 = 0.003036897163838148 + 0.1 * 6.116855144500732
Epoch 1290, val loss: 1.0883939266204834
Epoch 1300, training loss: 0.6147953867912292 = 0.002992121037095785 + 0.1 * 6.118032932281494
Epoch 1300, val loss: 1.0905152559280396
Epoch 1310, training loss: 0.613909900188446 = 0.0029485393315553665 + 0.1 * 6.10961389541626
Epoch 1310, val loss: 1.092612624168396
Epoch 1320, training loss: 0.6137135028839111 = 0.0029064661357551813 + 0.1 * 6.108070373535156
Epoch 1320, val loss: 1.0948185920715332
Epoch 1330, training loss: 0.6135945320129395 = 0.002865381073206663 + 0.1 * 6.107291221618652
Epoch 1330, val loss: 1.096942663192749
Epoch 1340, training loss: 0.6135351657867432 = 0.002825153525918722 + 0.1 * 6.107100009918213
Epoch 1340, val loss: 1.0989998579025269
Epoch 1350, training loss: 0.614167332649231 = 0.0027861183043569326 + 0.1 * 6.113812446594238
Epoch 1350, val loss: 1.1011207103729248
Epoch 1360, training loss: 0.6133371591567993 = 0.0027479175478219986 + 0.1 * 6.105892658233643
Epoch 1360, val loss: 1.1031352281570435
Epoch 1370, training loss: 0.6126856803894043 = 0.002710857195779681 + 0.1 * 6.099747657775879
Epoch 1370, val loss: 1.1052125692367554
Epoch 1380, training loss: 0.6138281226158142 = 0.0026745658833533525 + 0.1 * 6.11153507232666
Epoch 1380, val loss: 1.1072306632995605
Epoch 1390, training loss: 0.6127234101295471 = 0.0026391951832920313 + 0.1 * 6.100841999053955
Epoch 1390, val loss: 1.109183430671692
Epoch 1400, training loss: 0.6136119365692139 = 0.0026048298459500074 + 0.1 * 6.110070705413818
Epoch 1400, val loss: 1.1111962795257568
Epoch 1410, training loss: 0.6121580004692078 = 0.002571092452853918 + 0.1 * 6.0958685874938965
Epoch 1410, val loss: 1.1130763292312622
Epoch 1420, training loss: 0.6121763586997986 = 0.0025385443586856127 + 0.1 * 6.096377849578857
Epoch 1420, val loss: 1.1150484085083008
Epoch 1430, training loss: 0.613041877746582 = 0.0025066749658435583 + 0.1 * 6.10535192489624
Epoch 1430, val loss: 1.1169860363006592
Epoch 1440, training loss: 0.6118571758270264 = 0.002475235378369689 + 0.1 * 6.0938191413879395
Epoch 1440, val loss: 1.1188547611236572
Epoch 1450, training loss: 0.6131560802459717 = 0.0024447517935186625 + 0.1 * 6.107113361358643
Epoch 1450, val loss: 1.120725154876709
Epoch 1460, training loss: 0.6122739911079407 = 0.002414803020656109 + 0.1 * 6.0985918045043945
Epoch 1460, val loss: 1.1225539445877075
Epoch 1470, training loss: 0.6118066906929016 = 0.0023859734646975994 + 0.1 * 6.094206809997559
Epoch 1470, val loss: 1.1244474649429321
Epoch 1480, training loss: 0.6115391850471497 = 0.002357861725613475 + 0.1 * 6.091813564300537
Epoch 1480, val loss: 1.1263246536254883
Epoch 1490, training loss: 0.6127568483352661 = 0.0023301306646317244 + 0.1 * 6.104267120361328
Epoch 1490, val loss: 1.1281194686889648
Epoch 1500, training loss: 0.6119247078895569 = 0.0023029036819934845 + 0.1 * 6.096218109130859
Epoch 1500, val loss: 1.12986159324646
Epoch 1510, training loss: 0.6119312047958374 = 0.0022763588931411505 + 0.1 * 6.096548557281494
Epoch 1510, val loss: 1.1316378116607666
Epoch 1520, training loss: 0.6113491654396057 = 0.0022505363449454308 + 0.1 * 6.0909857749938965
Epoch 1520, val loss: 1.1334245204925537
Epoch 1530, training loss: 0.6112936735153198 = 0.0022253158967942 + 0.1 * 6.0906829833984375
Epoch 1530, val loss: 1.1352320909500122
Epoch 1540, training loss: 0.6114363074302673 = 0.0022002803161740303 + 0.1 * 6.092360019683838
Epoch 1540, val loss: 1.136942744255066
Epoch 1550, training loss: 0.6107603907585144 = 0.0021759169176220894 + 0.1 * 6.08584451675415
Epoch 1550, val loss: 1.1386232376098633
Epoch 1560, training loss: 0.6111422777175903 = 0.0021522166207432747 + 0.1 * 6.089900493621826
Epoch 1560, val loss: 1.140365481376648
Epoch 1570, training loss: 0.6106271147727966 = 0.0021290197037160397 + 0.1 * 6.0849809646606445
Epoch 1570, val loss: 1.1420782804489136
Epoch 1580, training loss: 0.611393392086029 = 0.00210616085678339 + 0.1 * 6.092872142791748
Epoch 1580, val loss: 1.143749713897705
Epoch 1590, training loss: 0.6105316281318665 = 0.0020836477633565664 + 0.1 * 6.084479331970215
Epoch 1590, val loss: 1.1453279256820679
Epoch 1600, training loss: 0.6108643412590027 = 0.0020618517883121967 + 0.1 * 6.088024616241455
Epoch 1600, val loss: 1.1470050811767578
Epoch 1610, training loss: 0.6104255318641663 = 0.0020403380040079355 + 0.1 * 6.0838518142700195
Epoch 1610, val loss: 1.1486529111862183
Epoch 1620, training loss: 0.6102713942527771 = 0.002019431907683611 + 0.1 * 6.08251953125
Epoch 1620, val loss: 1.1502869129180908
Epoch 1630, training loss: 0.609708845615387 = 0.001998927677050233 + 0.1 * 6.077099323272705
Epoch 1630, val loss: 1.15193772315979
Epoch 1640, training loss: 0.6117782592773438 = 0.001978616463020444 + 0.1 * 6.097996234893799
Epoch 1640, val loss: 1.1535065174102783
Epoch 1650, training loss: 0.609989583492279 = 0.00195869617164135 + 0.1 * 6.08030891418457
Epoch 1650, val loss: 1.1550205945968628
Epoch 1660, training loss: 0.6116953492164612 = 0.0019393248949199915 + 0.1 * 6.097560405731201
Epoch 1660, val loss: 1.1566182374954224
Epoch 1670, training loss: 0.6095126867294312 = 0.0019201177638024092 + 0.1 * 6.075925827026367
Epoch 1670, val loss: 1.158172607421875
Epoch 1680, training loss: 0.6096756458282471 = 0.0019015991128981113 + 0.1 * 6.07774019241333
Epoch 1680, val loss: 1.159732460975647
Epoch 1690, training loss: 0.6116725206375122 = 0.001883281278423965 + 0.1 * 6.0978922843933105
Epoch 1690, val loss: 1.1613028049468994
Epoch 1700, training loss: 0.6097354292869568 = 0.0018651577411219478 + 0.1 * 6.078702926635742
Epoch 1700, val loss: 1.1627150774002075
Epoch 1710, training loss: 0.6091309189796448 = 0.001847569947130978 + 0.1 * 6.07283353805542
Epoch 1710, val loss: 1.164273977279663
Epoch 1720, training loss: 0.6114691495895386 = 0.0018301698146387935 + 0.1 * 6.0963897705078125
Epoch 1720, val loss: 1.1657862663269043
Epoch 1730, training loss: 0.6092118620872498 = 0.001812924281693995 + 0.1 * 6.073989391326904
Epoch 1730, val loss: 1.1671968698501587
Epoch 1740, training loss: 0.6094988584518433 = 0.0017961564008146524 + 0.1 * 6.0770263671875
Epoch 1740, val loss: 1.168703556060791
Epoch 1750, training loss: 0.6102680563926697 = 0.001779515529051423 + 0.1 * 6.084885597229004
Epoch 1750, val loss: 1.1701545715332031
Epoch 1760, training loss: 0.609549880027771 = 0.0017634014366194606 + 0.1 * 6.077864646911621
Epoch 1760, val loss: 1.1716119050979614
Epoch 1770, training loss: 0.6099997758865356 = 0.0017475323984399438 + 0.1 * 6.082522392272949
Epoch 1770, val loss: 1.1730997562408447
Epoch 1780, training loss: 0.6093934178352356 = 0.0017317826859652996 + 0.1 * 6.0766167640686035
Epoch 1780, val loss: 1.1744575500488281
Epoch 1790, training loss: 0.6104799509048462 = 0.0017163337906822562 + 0.1 * 6.0876359939575195
Epoch 1790, val loss: 1.1758408546447754
Epoch 1800, training loss: 0.6088923811912537 = 0.001701219123788178 + 0.1 * 6.071911334991455
Epoch 1800, val loss: 1.1772704124450684
Epoch 1810, training loss: 0.6089524030685425 = 0.0016864727949723601 + 0.1 * 6.072659492492676
Epoch 1810, val loss: 1.1787021160125732
Epoch 1820, training loss: 0.6088826656341553 = 0.0016718326369300485 + 0.1 * 6.072107791900635
Epoch 1820, val loss: 1.1801055669784546
Epoch 1830, training loss: 0.6099786758422852 = 0.0016573058674111962 + 0.1 * 6.083213806152344
Epoch 1830, val loss: 1.1814448833465576
Epoch 1840, training loss: 0.6100783944129944 = 0.0016430849209427834 + 0.1 * 6.084353446960449
Epoch 1840, val loss: 1.182770013809204
Epoch 1850, training loss: 0.6084847450256348 = 0.0016291700303554535 + 0.1 * 6.06855583190918
Epoch 1850, val loss: 1.184163212776184
Epoch 1860, training loss: 0.6097967028617859 = 0.0016155523480847478 + 0.1 * 6.081811428070068
Epoch 1860, val loss: 1.1855427026748657
Epoch 1870, training loss: 0.60880047082901 = 0.0016019052127376199 + 0.1 * 6.071985244750977
Epoch 1870, val loss: 1.1867979764938354
Epoch 1880, training loss: 0.6081048250198364 = 0.001588699291460216 + 0.1 * 6.065161228179932
Epoch 1880, val loss: 1.1881263256072998
Epoch 1890, training loss: 0.6088623404502869 = 0.0015757252695038915 + 0.1 * 6.072865962982178
Epoch 1890, val loss: 1.1895079612731934
Epoch 1900, training loss: 0.6087384223937988 = 0.0015627797693014145 + 0.1 * 6.071756362915039
Epoch 1900, val loss: 1.190792202949524
Epoch 1910, training loss: 0.6081355810165405 = 0.0015500311274081469 + 0.1 * 6.065855503082275
Epoch 1910, val loss: 1.192084789276123
Epoch 1920, training loss: 0.609279990196228 = 0.0015376113587990403 + 0.1 * 6.077423572540283
Epoch 1920, val loss: 1.1933515071868896
Epoch 1930, training loss: 0.6079100966453552 = 0.001525281579233706 + 0.1 * 6.06384801864624
Epoch 1930, val loss: 1.194647192955017
Epoch 1940, training loss: 0.6083241105079651 = 0.0015133488923311234 + 0.1 * 6.068107604980469
Epoch 1940, val loss: 1.1959683895111084
Epoch 1950, training loss: 0.60882169008255 = 0.00150141934864223 + 0.1 * 6.073202133178711
Epoch 1950, val loss: 1.197213888168335
Epoch 1960, training loss: 0.6083971858024597 = 0.001489624846726656 + 0.1 * 6.069075584411621
Epoch 1960, val loss: 1.1984107494354248
Epoch 1970, training loss: 0.6076667904853821 = 0.0014781764475628734 + 0.1 * 6.061885833740234
Epoch 1970, val loss: 1.1997127532958984
Epoch 1980, training loss: 0.6086950898170471 = 0.0014668438816443086 + 0.1 * 6.072281837463379
Epoch 1980, val loss: 1.2009562253952026
Epoch 1990, training loss: 0.607829213142395 = 0.0014555752277374268 + 0.1 * 6.063736438751221
Epoch 1990, val loss: 1.2021405696868896
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5572
Flip ASR: 0.4844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.803934097290039 = 1.966546893119812 + 0.1 * 8.373871803283691
Epoch 0, val loss: 1.9670627117156982
Epoch 10, training loss: 2.7933156490325928 = 1.9559390544891357 + 0.1 * 8.373764991760254
Epoch 10, val loss: 1.9566363096237183
Epoch 20, training loss: 2.780378818511963 = 1.9430642127990723 + 0.1 * 8.37314510345459
Epoch 20, val loss: 1.9433903694152832
Epoch 30, training loss: 2.7616586685180664 = 1.9248396158218384 + 0.1 * 8.368191719055176
Epoch 30, val loss: 1.9243254661560059
Epoch 40, training loss: 2.7310428619384766 = 1.8975740671157837 + 0.1 * 8.334688186645508
Epoch 40, val loss: 1.8960742950439453
Epoch 50, training loss: 2.6712093353271484 = 1.8594951629638672 + 0.1 * 8.117142677307129
Epoch 50, val loss: 1.8585786819458008
Epoch 60, training loss: 2.570113182067871 = 1.817278265953064 + 0.1 * 7.52834939956665
Epoch 60, val loss: 1.8192768096923828
Epoch 70, training loss: 2.489351749420166 = 1.7788405418395996 + 0.1 * 7.105111122131348
Epoch 70, val loss: 1.7844507694244385
Epoch 80, training loss: 2.4294919967651367 = 1.7409628629684448 + 0.1 * 6.8852925300598145
Epoch 80, val loss: 1.749919056892395
Epoch 90, training loss: 2.3736374378204346 = 1.6942499876022339 + 0.1 * 6.7938737869262695
Epoch 90, val loss: 1.7071374654769897
Epoch 100, training loss: 2.305603504180908 = 1.633191704750061 + 0.1 * 6.724118232727051
Epoch 100, val loss: 1.65336012840271
Epoch 110, training loss: 2.2210912704467773 = 1.553312063217163 + 0.1 * 6.677793025970459
Epoch 110, val loss: 1.5857053995132446
Epoch 120, training loss: 2.120713710784912 = 1.455214262008667 + 0.1 * 6.654994010925293
Epoch 120, val loss: 1.5052249431610107
Epoch 130, training loss: 2.012077569961548 = 1.3476431369781494 + 0.1 * 6.644343376159668
Epoch 130, val loss: 1.4199943542480469
Epoch 140, training loss: 1.904132604598999 = 1.2405201196670532 + 0.1 * 6.636124134063721
Epoch 140, val loss: 1.3386249542236328
Epoch 150, training loss: 1.8004696369171143 = 1.1377599239349365 + 0.1 * 6.627097129821777
Epoch 150, val loss: 1.2633556127548218
Epoch 160, training loss: 1.7018170356750488 = 1.0402023792266846 + 0.1 * 6.616146087646484
Epoch 160, val loss: 1.1922755241394043
Epoch 170, training loss: 1.610123872756958 = 0.9495953321456909 + 0.1 * 6.605285167694092
Epoch 170, val loss: 1.1274735927581787
Epoch 180, training loss: 1.5261608362197876 = 0.8672488331794739 + 0.1 * 6.589119911193848
Epoch 180, val loss: 1.0698418617248535
Epoch 190, training loss: 1.4515851736068726 = 0.7941007018089294 + 0.1 * 6.574844837188721
Epoch 190, val loss: 1.0200552940368652
Epoch 200, training loss: 1.386612892150879 = 0.7308585047721863 + 0.1 * 6.557543754577637
Epoch 200, val loss: 0.979421079158783
Epoch 210, training loss: 1.330709457397461 = 0.6762579083442688 + 0.1 * 6.544516086578369
Epoch 210, val loss: 0.9461876153945923
Epoch 220, training loss: 1.2814733982086182 = 0.6282950043678284 + 0.1 * 6.531784534454346
Epoch 220, val loss: 0.919677734375
Epoch 230, training loss: 1.2368993759155273 = 0.5848527550697327 + 0.1 * 6.520465850830078
Epoch 230, val loss: 0.8976180553436279
Epoch 240, training loss: 1.1948765516281128 = 0.5442630648612976 + 0.1 * 6.506134986877441
Epoch 240, val loss: 0.8786458373069763
Epoch 250, training loss: 1.1558313369750977 = 0.5053707957267761 + 0.1 * 6.504604816436768
Epoch 250, val loss: 0.8615850210189819
Epoch 260, training loss: 1.1165573596954346 = 0.4679647386074066 + 0.1 * 6.485926151275635
Epoch 260, val loss: 0.8464323878288269
Epoch 270, training loss: 1.0793160200119019 = 0.43180686235427856 + 0.1 * 6.475091457366943
Epoch 270, val loss: 0.833165168762207
Epoch 280, training loss: 1.0438039302825928 = 0.39723408222198486 + 0.1 * 6.465699195861816
Epoch 280, val loss: 0.8221048712730408
Epoch 290, training loss: 1.0114761590957642 = 0.3645291328430176 + 0.1 * 6.469470024108887
Epoch 290, val loss: 0.8133209943771362
Epoch 300, training loss: 0.9791683554649353 = 0.334312379360199 + 0.1 * 6.448559761047363
Epoch 300, val loss: 0.8066167831420898
Epoch 310, training loss: 0.9497257471084595 = 0.30634766817092896 + 0.1 * 6.433780670166016
Epoch 310, val loss: 0.8022980690002441
Epoch 320, training loss: 0.9221012592315674 = 0.280351459980011 + 0.1 * 6.4174981117248535
Epoch 320, val loss: 0.8002529740333557
Epoch 330, training loss: 0.8973578214645386 = 0.2562240958213806 + 0.1 * 6.411336898803711
Epoch 330, val loss: 0.8002237677574158
Epoch 340, training loss: 0.8742639422416687 = 0.23393535614013672 + 0.1 * 6.403285503387451
Epoch 340, val loss: 0.8022672533988953
Epoch 350, training loss: 0.8529627323150635 = 0.213172048330307 + 0.1 * 6.397907257080078
Epoch 350, val loss: 0.806103527545929
Epoch 360, training loss: 0.8321086764335632 = 0.19397521018981934 + 0.1 * 6.38133430480957
Epoch 360, val loss: 0.8117002248764038
Epoch 370, training loss: 0.8157534599304199 = 0.17620185017585754 + 0.1 * 6.3955159187316895
Epoch 370, val loss: 0.8190135955810547
Epoch 380, training loss: 0.7968975305557251 = 0.15996140241622925 + 0.1 * 6.369361400604248
Epoch 380, val loss: 0.8278264999389648
Epoch 390, training loss: 0.7812283635139465 = 0.14493931829929352 + 0.1 * 6.362890720367432
Epoch 390, val loss: 0.8379567861557007
Epoch 400, training loss: 0.7658874988555908 = 0.13112467527389526 + 0.1 * 6.347628116607666
Epoch 400, val loss: 0.8493130803108215
Epoch 410, training loss: 0.7549682259559631 = 0.11853424459695816 + 0.1 * 6.364339351654053
Epoch 410, val loss: 0.8618437647819519
Epoch 420, training loss: 0.7406890988349915 = 0.10714638233184814 + 0.1 * 6.335427284240723
Epoch 420, val loss: 0.8752509951591492
Epoch 430, training loss: 0.7319945096969604 = 0.09686229377985 + 0.1 * 6.351321697235107
Epoch 430, val loss: 0.8895480632781982
Epoch 440, training loss: 0.7203474044799805 = 0.08770719915628433 + 0.1 * 6.326401710510254
Epoch 440, val loss: 0.904355525970459
Epoch 450, training loss: 0.7126369476318359 = 0.0795835554599762 + 0.1 * 6.330533981323242
Epoch 450, val loss: 0.919628918170929
Epoch 460, training loss: 0.7036581635475159 = 0.0724252313375473 + 0.1 * 6.312329292297363
Epoch 460, val loss: 0.9350622296333313
Epoch 470, training loss: 0.6970862150192261 = 0.0660964697599411 + 0.1 * 6.309897422790527
Epoch 470, val loss: 0.9505729675292969
Epoch 480, training loss: 0.6910529732704163 = 0.060513485223054886 + 0.1 * 6.305394649505615
Epoch 480, val loss: 0.9659962058067322
Epoch 490, training loss: 0.6847310066223145 = 0.055577799677848816 + 0.1 * 6.291532039642334
Epoch 490, val loss: 0.9811908006668091
Epoch 500, training loss: 0.6809134483337402 = 0.051182717084884644 + 0.1 * 6.29730749130249
Epoch 500, val loss: 0.996174693107605
Epoch 510, training loss: 0.6765856146812439 = 0.04726918041706085 + 0.1 * 6.293164253234863
Epoch 510, val loss: 1.010750412940979
Epoch 520, training loss: 0.671390950679779 = 0.04377958923578262 + 0.1 * 6.276113510131836
Epoch 520, val loss: 1.0250071287155151
Epoch 530, training loss: 0.6683400869369507 = 0.040642935782670975 + 0.1 * 6.276971817016602
Epoch 530, val loss: 1.038963794708252
Epoch 540, training loss: 0.6654801964759827 = 0.03781881555914879 + 0.1 * 6.276613712310791
Epoch 540, val loss: 1.0525907278060913
Epoch 550, training loss: 0.6629763245582581 = 0.03527262434363365 + 0.1 * 6.277037143707275
Epoch 550, val loss: 1.065885066986084
Epoch 560, training loss: 0.6595902442932129 = 0.03297020494937897 + 0.1 * 6.266200542449951
Epoch 560, val loss: 1.0787633657455444
Epoch 570, training loss: 0.6593355536460876 = 0.030880970880389214 + 0.1 * 6.284545421600342
Epoch 570, val loss: 1.0913878679275513
Epoch 580, training loss: 0.6550401449203491 = 0.028983118012547493 + 0.1 * 6.260570049285889
Epoch 580, val loss: 1.1036136150360107
Epoch 590, training loss: 0.6522534489631653 = 0.02725483477115631 + 0.1 * 6.249986171722412
Epoch 590, val loss: 1.1155259609222412
Epoch 600, training loss: 0.6530556082725525 = 0.025671767070889473 + 0.1 * 6.273838043212891
Epoch 600, val loss: 1.1270992755889893
Epoch 610, training loss: 0.6487084627151489 = 0.02422732673585415 + 0.1 * 6.244811058044434
Epoch 610, val loss: 1.1383060216903687
Epoch 620, training loss: 0.6468920707702637 = 0.022901901975274086 + 0.1 * 6.239901542663574
Epoch 620, val loss: 1.1492735147476196
Epoch 630, training loss: 0.6468138694763184 = 0.021681711077690125 + 0.1 * 6.251321315765381
Epoch 630, val loss: 1.159978985786438
Epoch 640, training loss: 0.6442673206329346 = 0.020560618489980698 + 0.1 * 6.237066745758057
Epoch 640, val loss: 1.170390009880066
Epoch 650, training loss: 0.6423729658126831 = 0.019525453448295593 + 0.1 * 6.2284746170043945
Epoch 650, val loss: 1.1805195808410645
Epoch 660, training loss: 0.6419512629508972 = 0.018568959087133408 + 0.1 * 6.233823299407959
Epoch 660, val loss: 1.190420389175415
Epoch 670, training loss: 0.6399976015090942 = 0.017682868987321854 + 0.1 * 6.223147392272949
Epoch 670, val loss: 1.2000818252563477
Epoch 680, training loss: 0.639785647392273 = 0.016860105097293854 + 0.1 * 6.229255199432373
Epoch 680, val loss: 1.2095322608947754
Epoch 690, training loss: 0.638618528842926 = 0.016094567254185677 + 0.1 * 6.225239276885986
Epoch 690, val loss: 1.2187304496765137
Epoch 700, training loss: 0.6387603282928467 = 0.015383802354335785 + 0.1 * 6.233765602111816
Epoch 700, val loss: 1.2276653051376343
Epoch 710, training loss: 0.6359730958938599 = 0.014722692780196667 + 0.1 * 6.212503910064697
Epoch 710, val loss: 1.2363909482955933
Epoch 720, training loss: 0.6348046660423279 = 0.014104765839874744 + 0.1 * 6.206998825073242
Epoch 720, val loss: 1.2449426651000977
Epoch 730, training loss: 0.63619464635849 = 0.013524718582630157 + 0.1 * 6.226699352264404
Epoch 730, val loss: 1.2532985210418701
Epoch 740, training loss: 0.6333833932876587 = 0.012983093038201332 + 0.1 * 6.204002857208252
Epoch 740, val loss: 1.261427879333496
Epoch 750, training loss: 0.6332495808601379 = 0.012476821430027485 + 0.1 * 6.207727909088135
Epoch 750, val loss: 1.2694121599197388
Epoch 760, training loss: 0.6317785978317261 = 0.012000233866274357 + 0.1 * 6.197783470153809
Epoch 760, val loss: 1.2772070169448853
Epoch 770, training loss: 0.6326475739479065 = 0.011552992276847363 + 0.1 * 6.2109456062316895
Epoch 770, val loss: 1.284837245941162
Epoch 780, training loss: 0.6301244497299194 = 0.011131180450320244 + 0.1 * 6.189932823181152
Epoch 780, val loss: 1.2922765016555786
Epoch 790, training loss: 0.6301118731498718 = 0.010735020972788334 + 0.1 * 6.193768501281738
Epoch 790, val loss: 1.299607515335083
Epoch 800, training loss: 0.6294482350349426 = 0.010360822081565857 + 0.1 * 6.190874099731445
Epoch 800, val loss: 1.3067177534103394
Epoch 810, training loss: 0.6301183700561523 = 0.010006958618760109 + 0.1 * 6.201113700866699
Epoch 810, val loss: 1.3136957883834839
Epoch 820, training loss: 0.6288321018218994 = 0.009673504158854485 + 0.1 * 6.191586017608643
Epoch 820, val loss: 1.3204940557479858
Epoch 830, training loss: 0.6283445358276367 = 0.00935666449368 + 0.1 * 6.189878463745117
Epoch 830, val loss: 1.3271639347076416
Epoch 840, training loss: 0.6276029944419861 = 0.009057016111910343 + 0.1 * 6.185459136962891
Epoch 840, val loss: 1.333726406097412
Epoch 850, training loss: 0.6266570091247559 = 0.008772127330303192 + 0.1 * 6.178848743438721
Epoch 850, val loss: 1.3401552438735962
Epoch 860, training loss: 0.6275731921195984 = 0.008502106182277203 + 0.1 * 6.190710544586182
Epoch 860, val loss: 1.3464733362197876
Epoch 870, training loss: 0.6259076595306396 = 0.00824575312435627 + 0.1 * 6.176619052886963
Epoch 870, val loss: 1.3526360988616943
Epoch 880, training loss: 0.6263946890830994 = 0.008001813665032387 + 0.1 * 6.183928966522217
Epoch 880, val loss: 1.3587117195129395
Epoch 890, training loss: 0.625389039516449 = 0.007770020514726639 + 0.1 * 6.176190376281738
Epoch 890, val loss: 1.3646305799484253
Epoch 900, training loss: 0.6259998083114624 = 0.007549052592366934 + 0.1 * 6.184507369995117
Epoch 900, val loss: 1.3704191446304321
Epoch 910, training loss: 0.6239189505577087 = 0.00733931502327323 + 0.1 * 6.165796279907227
Epoch 910, val loss: 1.3760734796524048
Epoch 920, training loss: 0.6235498785972595 = 0.007140185218304396 + 0.1 * 6.164097309112549
Epoch 920, val loss: 1.3816988468170166
Epoch 930, training loss: 0.6235760450363159 = 0.006948642898350954 + 0.1 * 6.166274070739746
Epoch 930, val loss: 1.3871777057647705
Epoch 940, training loss: 0.6234270930290222 = 0.006765841972082853 + 0.1 * 6.166612148284912
Epoch 940, val loss: 1.3925598859786987
Epoch 950, training loss: 0.622800886631012 = 0.006590461358428001 + 0.1 * 6.16210412979126
Epoch 950, val loss: 1.397888422012329
Epoch 960, training loss: 0.6244605183601379 = 0.00642360420897603 + 0.1 * 6.180368900299072
Epoch 960, val loss: 1.4031240940093994
Epoch 970, training loss: 0.6219045519828796 = 0.0062623051926493645 + 0.1 * 6.1564226150512695
Epoch 970, val loss: 1.408190131187439
Epoch 980, training loss: 0.6218976378440857 = 0.006108995992690325 + 0.1 * 6.157886505126953
Epoch 980, val loss: 1.4132475852966309
Epoch 990, training loss: 0.6211832165718079 = 0.005962270312011242 + 0.1 * 6.152209281921387
Epoch 990, val loss: 1.4182265996932983
Epoch 1000, training loss: 0.6243922114372253 = 0.005819822195917368 + 0.1 * 6.185723781585693
Epoch 1000, val loss: 1.423088788986206
Epoch 1010, training loss: 0.6214937567710876 = 0.00568413408473134 + 0.1 * 6.1580963134765625
Epoch 1010, val loss: 1.4278253316879272
Epoch 1020, training loss: 0.6203678250312805 = 0.005553684663027525 + 0.1 * 6.148141384124756
Epoch 1020, val loss: 1.4325525760650635
Epoch 1030, training loss: 0.6205593943595886 = 0.005429265554994345 + 0.1 * 6.15130090713501
Epoch 1030, val loss: 1.4372296333312988
Epoch 1040, training loss: 0.6208043694496155 = 0.005308202002197504 + 0.1 * 6.154961585998535
Epoch 1040, val loss: 1.4417390823364258
Epoch 1050, training loss: 0.6196156144142151 = 0.005192128010094166 + 0.1 * 6.144235134124756
Epoch 1050, val loss: 1.446189522743225
Epoch 1060, training loss: 0.619536280632019 = 0.005081082694232464 + 0.1 * 6.144552230834961
Epoch 1060, val loss: 1.450603723526001
Epoch 1070, training loss: 0.6197643280029297 = 0.0049729798920452595 + 0.1 * 6.147913455963135
Epoch 1070, val loss: 1.4549146890640259
Epoch 1080, training loss: 0.6195294857025146 = 0.004869448486715555 + 0.1 * 6.146600246429443
Epoch 1080, val loss: 1.4591432809829712
Epoch 1090, training loss: 0.6189722418785095 = 0.004769539460539818 + 0.1 * 6.142026424407959
Epoch 1090, val loss: 1.4633287191390991
Epoch 1100, training loss: 0.6184051632881165 = 0.004673238843679428 + 0.1 * 6.137319087982178
Epoch 1100, val loss: 1.4674214124679565
Epoch 1110, training loss: 0.6179482340812683 = 0.004580668173730373 + 0.1 * 6.133675575256348
Epoch 1110, val loss: 1.4714585542678833
Epoch 1120, training loss: 0.6185125708580017 = 0.0044914716854691505 + 0.1 * 6.14021110534668
Epoch 1120, val loss: 1.4754664897918701
Epoch 1130, training loss: 0.6181342601776123 = 0.00440450944006443 + 0.1 * 6.137297630310059
Epoch 1130, val loss: 1.4793845415115356
Epoch 1140, training loss: 0.6177401542663574 = 0.004320880863815546 + 0.1 * 6.134192943572998
Epoch 1140, val loss: 1.4832476377487183
Epoch 1150, training loss: 0.6179230213165283 = 0.004239439032971859 + 0.1 * 6.13683557510376
Epoch 1150, val loss: 1.4870240688323975
Epoch 1160, training loss: 0.6165815591812134 = 0.004160840529948473 + 0.1 * 6.124207496643066
Epoch 1160, val loss: 1.4907556772232056
Epoch 1170, training loss: 0.6170371174812317 = 0.004084862768650055 + 0.1 * 6.129522323608398
Epoch 1170, val loss: 1.494444489479065
Epoch 1180, training loss: 0.6193416118621826 = 0.0040109725669026375 + 0.1 * 6.153306007385254
Epoch 1180, val loss: 1.498081088066101
Epoch 1190, training loss: 0.6171644330024719 = 0.003939528949558735 + 0.1 * 6.132249355316162
Epoch 1190, val loss: 1.5015796422958374
Epoch 1200, training loss: 0.6170160174369812 = 0.003870620857924223 + 0.1 * 6.131453990936279
Epoch 1200, val loss: 1.5050452947616577
Epoch 1210, training loss: 0.6165186762809753 = 0.003804375883191824 + 0.1 * 6.127142906188965
Epoch 1210, val loss: 1.5085058212280273
Epoch 1220, training loss: 0.6163976192474365 = 0.003739549545571208 + 0.1 * 6.126580715179443
Epoch 1220, val loss: 1.511870265007019
Epoch 1230, training loss: 0.6162840127944946 = 0.0036764463875442743 + 0.1 * 6.126075267791748
Epoch 1230, val loss: 1.5151818990707397
Epoch 1240, training loss: 0.6153914332389832 = 0.003615498775616288 + 0.1 * 6.1177592277526855
Epoch 1240, val loss: 1.5184656381607056
Epoch 1250, training loss: 0.6161118745803833 = 0.0035567055456340313 + 0.1 * 6.125551223754883
Epoch 1250, val loss: 1.521714448928833
Epoch 1260, training loss: 0.6160750985145569 = 0.0034988776315003633 + 0.1 * 6.125762462615967
Epoch 1260, val loss: 1.5249067544937134
Epoch 1270, training loss: 0.6148452758789062 = 0.003443049732595682 + 0.1 * 6.1140217781066895
Epoch 1270, val loss: 1.528048038482666
Epoch 1280, training loss: 0.6149911284446716 = 0.0033890267368406057 + 0.1 * 6.116021156311035
Epoch 1280, val loss: 1.531178593635559
Epoch 1290, training loss: 0.6163187623023987 = 0.003336321795359254 + 0.1 * 6.129824638366699
Epoch 1290, val loss: 1.5342563390731812
Epoch 1300, training loss: 0.6148882508277893 = 0.0032852236181497574 + 0.1 * 6.116030216217041
Epoch 1300, val loss: 1.5372716188430786
Epoch 1310, training loss: 0.6158786416053772 = 0.003235709620639682 + 0.1 * 6.126429557800293
Epoch 1310, val loss: 1.5402766466140747
Epoch 1320, training loss: 0.6145238876342773 = 0.003187034511938691 + 0.1 * 6.113368034362793
Epoch 1320, val loss: 1.5432102680206299
Epoch 1330, training loss: 0.6154400706291199 = 0.003140267450362444 + 0.1 * 6.122997760772705
Epoch 1330, val loss: 1.5461424589157104
Epoch 1340, training loss: 0.6141436696052551 = 0.00309434300288558 + 0.1 * 6.110493183135986
Epoch 1340, val loss: 1.5490214824676514
Epoch 1350, training loss: 0.6140170097351074 = 0.003050263039767742 + 0.1 * 6.1096673011779785
Epoch 1350, val loss: 1.5519002676010132
Epoch 1360, training loss: 0.6136937737464905 = 0.003006548620760441 + 0.1 * 6.106872081756592
Epoch 1360, val loss: 1.5546964406967163
Epoch 1370, training loss: 0.6139236092567444 = 0.002964187413454056 + 0.1 * 6.109593868255615
Epoch 1370, val loss: 1.557451605796814
Epoch 1380, training loss: 0.6138789653778076 = 0.002923298394307494 + 0.1 * 6.109556198120117
Epoch 1380, val loss: 1.5602219104766846
Epoch 1390, training loss: 0.6141296029090881 = 0.002882956760004163 + 0.1 * 6.112466335296631
Epoch 1390, val loss: 1.5629295110702515
Epoch 1400, training loss: 0.6148530840873718 = 0.0028438435401767492 + 0.1 * 6.120092391967773
Epoch 1400, val loss: 1.5655977725982666
Epoch 1410, training loss: 0.6131080985069275 = 0.002805710071697831 + 0.1 * 6.103023529052734
Epoch 1410, val loss: 1.5682390928268433
Epoch 1420, training loss: 0.6144205331802368 = 0.0027686930261552334 + 0.1 * 6.116518497467041
Epoch 1420, val loss: 1.5708444118499756
Epoch 1430, training loss: 0.6127203702926636 = 0.0027326710987836123 + 0.1 * 6.099876880645752
Epoch 1430, val loss: 1.5734037160873413
Epoch 1440, training loss: 0.6135547757148743 = 0.0026972703635692596 + 0.1 * 6.108574867248535
Epoch 1440, val loss: 1.5759484767913818
Epoch 1450, training loss: 0.612815260887146 = 0.002662565326318145 + 0.1 * 6.101527214050293
Epoch 1450, val loss: 1.5784488916397095
Epoch 1460, training loss: 0.612743079662323 = 0.002628958085551858 + 0.1 * 6.101140975952148
Epoch 1460, val loss: 1.5809375047683716
Epoch 1470, training loss: 0.6124768257141113 = 0.0025959948543459177 + 0.1 * 6.0988078117370605
Epoch 1470, val loss: 1.5833817720413208
Epoch 1480, training loss: 0.6145926713943481 = 0.00256374035961926 + 0.1 * 6.120289325714111
Epoch 1480, val loss: 1.5858091115951538
Epoch 1490, training loss: 0.6127870678901672 = 0.002532376442104578 + 0.1 * 6.102546691894531
Epoch 1490, val loss: 1.5881903171539307
Epoch 1500, training loss: 0.6121372580528259 = 0.0025017629377543926 + 0.1 * 6.0963544845581055
Epoch 1500, val loss: 1.5905425548553467
Epoch 1510, training loss: 0.6120871901512146 = 0.0024719065986573696 + 0.1 * 6.0961527824401855
Epoch 1510, val loss: 1.5928871631622314
Epoch 1520, training loss: 0.6128661632537842 = 0.002442575292661786 + 0.1 * 6.104235649108887
Epoch 1520, val loss: 1.595198154449463
Epoch 1530, training loss: 0.6123949289321899 = 0.0024139131419360638 + 0.1 * 6.0998101234436035
Epoch 1530, val loss: 1.5974584817886353
Epoch 1540, training loss: 0.6125725507736206 = 0.0023858225904405117 + 0.1 * 6.101867198944092
Epoch 1540, val loss: 1.5997071266174316
Epoch 1550, training loss: 0.611224353313446 = 0.002358346013352275 + 0.1 * 6.08866024017334
Epoch 1550, val loss: 1.601904034614563
Epoch 1560, training loss: 0.6117495894432068 = 0.002331745345145464 + 0.1 * 6.094178199768066
Epoch 1560, val loss: 1.6041361093521118
Epoch 1570, training loss: 0.6117067337036133 = 0.0023053663317114115 + 0.1 * 6.094013214111328
Epoch 1570, val loss: 1.6063274145126343
Epoch 1580, training loss: 0.6120174527168274 = 0.002279615728184581 + 0.1 * 6.097377777099609
Epoch 1580, val loss: 1.6084848642349243
Epoch 1590, training loss: 0.6111306548118591 = 0.002254445804283023 + 0.1 * 6.088762283325195
Epoch 1590, val loss: 1.6105878353118896
Epoch 1600, training loss: 0.6111396551132202 = 0.0022298533003777266 + 0.1 * 6.089097499847412
Epoch 1600, val loss: 1.6127055883407593
Epoch 1610, training loss: 0.6120470762252808 = 0.002205673372372985 + 0.1 * 6.098413944244385
Epoch 1610, val loss: 1.6147807836532593
Epoch 1620, training loss: 0.6116980910301208 = 0.002181767951697111 + 0.1 * 6.095163345336914
Epoch 1620, val loss: 1.6168389320373535
Epoch 1630, training loss: 0.6111860275268555 = 0.002158784307539463 + 0.1 * 6.090272426605225
Epoch 1630, val loss: 1.6188687086105347
Epoch 1640, training loss: 0.610817015171051 = 0.00213608518242836 + 0.1 * 6.086808681488037
Epoch 1640, val loss: 1.620888113975525
Epoch 1650, training loss: 0.6116238832473755 = 0.0021138694137334824 + 0.1 * 6.095099925994873
Epoch 1650, val loss: 1.62289297580719
Epoch 1660, training loss: 0.6098236441612244 = 0.002092009410262108 + 0.1 * 6.0773162841796875
Epoch 1660, val loss: 1.6248811483383179
Epoch 1670, training loss: 0.6105139851570129 = 0.0020707338117063046 + 0.1 * 6.084432601928711
Epoch 1670, val loss: 1.6268651485443115
Epoch 1680, training loss: 0.6125938296318054 = 0.002049456350505352 + 0.1 * 6.105443477630615
Epoch 1680, val loss: 1.6288238763809204
Epoch 1690, training loss: 0.6105486154556274 = 0.002028804738074541 + 0.1 * 6.085197925567627
Epoch 1690, val loss: 1.630759596824646
Epoch 1700, training loss: 0.610274612903595 = 0.0020086499862372875 + 0.1 * 6.0826592445373535
Epoch 1700, val loss: 1.6327025890350342
Epoch 1710, training loss: 0.6110580563545227 = 0.001988879172131419 + 0.1 * 6.090691566467285
Epoch 1710, val loss: 1.6346168518066406
Epoch 1720, training loss: 0.6108759641647339 = 0.0019692094065248966 + 0.1 * 6.089067459106445
Epoch 1720, val loss: 1.636499047279358
Epoch 1730, training loss: 0.6105241775512695 = 0.0019501190399751067 + 0.1 * 6.085740089416504
Epoch 1730, val loss: 1.6383694410324097
Epoch 1740, training loss: 0.609943687915802 = 0.0019315610406920314 + 0.1 * 6.080121040344238
Epoch 1740, val loss: 1.6402252912521362
Epoch 1750, training loss: 0.6097286343574524 = 0.0019132719608023763 + 0.1 * 6.078153610229492
Epoch 1750, val loss: 1.642092227935791
Epoch 1760, training loss: 0.6099081039428711 = 0.00189517205581069 + 0.1 * 6.080129146575928
Epoch 1760, val loss: 1.6439467668533325
Epoch 1770, training loss: 0.6115297079086304 = 0.0018772929906845093 + 0.1 * 6.096523761749268
Epoch 1770, val loss: 1.6457792520523071
Epoch 1780, training loss: 0.6100926399230957 = 0.001859809854067862 + 0.1 * 6.0823283195495605
Epoch 1780, val loss: 1.6475825309753418
Epoch 1790, training loss: 0.6100766062736511 = 0.001842615194618702 + 0.1 * 6.082339763641357
Epoch 1790, val loss: 1.649382472038269
Epoch 1800, training loss: 0.6090705394744873 = 0.001825947081670165 + 0.1 * 6.072445869445801
Epoch 1800, val loss: 1.651167631149292
Epoch 1810, training loss: 0.610099196434021 = 0.0018094489350914955 + 0.1 * 6.082897186279297
Epoch 1810, val loss: 1.6529656648635864
Epoch 1820, training loss: 0.6092848181724548 = 0.0017932443879544735 + 0.1 * 6.074915409088135
Epoch 1820, val loss: 1.6547578573226929
Epoch 1830, training loss: 0.6095012426376343 = 0.0017771368147805333 + 0.1 * 6.07724142074585
Epoch 1830, val loss: 1.6565148830413818
Epoch 1840, training loss: 0.6087340116500854 = 0.001761501538567245 + 0.1 * 6.069725036621094
Epoch 1840, val loss: 1.6582496166229248
Epoch 1850, training loss: 0.6093978881835938 = 0.0017462242394685745 + 0.1 * 6.076516151428223
Epoch 1850, val loss: 1.6599717140197754
Epoch 1860, training loss: 0.6090415716171265 = 0.0017309198155999184 + 0.1 * 6.073106288909912
Epoch 1860, val loss: 1.661669135093689
Epoch 1870, training loss: 0.6097042560577393 = 0.0017159327398985624 + 0.1 * 6.079883098602295
Epoch 1870, val loss: 1.6633625030517578
Epoch 1880, training loss: 0.6084332466125488 = 0.0017011899035423994 + 0.1 * 6.067320823669434
Epoch 1880, val loss: 1.6649998426437378
Epoch 1890, training loss: 0.6099274754524231 = 0.0016869079554453492 + 0.1 * 6.082405090332031
Epoch 1890, val loss: 1.6666536331176758
Epoch 1900, training loss: 0.6086328625679016 = 0.0016728430055081844 + 0.1 * 6.069599628448486
Epoch 1900, val loss: 1.6683062314987183
Epoch 1910, training loss: 0.6084064841270447 = 0.0016588879516348243 + 0.1 * 6.067475318908691
Epoch 1910, val loss: 1.669914960861206
Epoch 1920, training loss: 0.6092057228088379 = 0.0016451755072921515 + 0.1 * 6.075605392456055
Epoch 1920, val loss: 1.6715434789657593
Epoch 1930, training loss: 0.6093472838401794 = 0.0016315397806465626 + 0.1 * 6.077157020568848
Epoch 1930, val loss: 1.673135757446289
Epoch 1940, training loss: 0.6081535816192627 = 0.0016182282706722617 + 0.1 * 6.0653533935546875
Epoch 1940, val loss: 1.6747056245803833
Epoch 1950, training loss: 0.6090523600578308 = 0.0016052754363045096 + 0.1 * 6.0744709968566895
Epoch 1950, val loss: 1.6762864589691162
Epoch 1960, training loss: 0.6086748838424683 = 0.0015922558959573507 + 0.1 * 6.070826053619385
Epoch 1960, val loss: 1.6778225898742676
Epoch 1970, training loss: 0.6080267429351807 = 0.001579643227159977 + 0.1 * 6.0644707679748535
Epoch 1970, val loss: 1.6793678998947144
Epoch 1980, training loss: 0.6085509657859802 = 0.0015671716537326574 + 0.1 * 6.06983757019043
Epoch 1980, val loss: 1.6809070110321045
Epoch 1990, training loss: 0.6083675026893616 = 0.001554948976263404 + 0.1 * 6.068125247955322
Epoch 1990, val loss: 1.6824402809143066
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7565
Flip ASR: 0.7156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.795961618423462 = 1.9585723876953125 + 0.1 * 8.373892784118652
Epoch 0, val loss: 1.9619622230529785
Epoch 10, training loss: 2.7859668731689453 = 1.948588490486145 + 0.1 * 8.373783111572266
Epoch 10, val loss: 1.9522216320037842
Epoch 20, training loss: 2.7737483978271484 = 1.9364386796951294 + 0.1 * 8.373098373413086
Epoch 20, val loss: 1.940108060836792
Epoch 30, training loss: 2.756218433380127 = 1.9194190502166748 + 0.1 * 8.367992401123047
Epoch 30, val loss: 1.9229100942611694
Epoch 40, training loss: 2.7270679473876953 = 1.8940105438232422 + 0.1 * 8.330574989318848
Epoch 40, val loss: 1.8971792459487915
Epoch 50, training loss: 2.6612086296081543 = 1.8584927320480347 + 0.1 * 8.0271577835083
Epoch 50, val loss: 1.862310528755188
Epoch 60, training loss: 2.5695276260375977 = 1.818373203277588 + 0.1 * 7.511544227600098
Epoch 60, val loss: 1.8252769708633423
Epoch 70, training loss: 2.496310234069824 = 1.7806826829910278 + 0.1 * 7.156275272369385
Epoch 70, val loss: 1.7931890487670898
Epoch 80, training loss: 2.4353833198547363 = 1.7462149858474731 + 0.1 * 6.891684532165527
Epoch 80, val loss: 1.7631807327270508
Epoch 90, training loss: 2.3798582553863525 = 1.7046995162963867 + 0.1 * 6.751587867736816
Epoch 90, val loss: 1.7264975309371948
Epoch 100, training loss: 2.3175506591796875 = 1.648342490196228 + 0.1 * 6.692082405090332
Epoch 100, val loss: 1.6784526109695435
Epoch 110, training loss: 2.2382023334503174 = 1.5724070072174072 + 0.1 * 6.657952308654785
Epoch 110, val loss: 1.6151913404464722
Epoch 120, training loss: 2.1404929161071777 = 1.4768590927124023 + 0.1 * 6.636338710784912
Epoch 120, val loss: 1.535801649093628
Epoch 130, training loss: 2.0295250415802 = 1.3672388792037964 + 0.1 * 6.622862339019775
Epoch 130, val loss: 1.444901943206787
Epoch 140, training loss: 1.9127483367919922 = 1.251595139503479 + 0.1 * 6.611531734466553
Epoch 140, val loss: 1.349310040473938
Epoch 150, training loss: 1.7978124618530273 = 1.1380574703216553 + 0.1 * 6.597549915313721
Epoch 150, val loss: 1.2568405866622925
Epoch 160, training loss: 1.6923284530639648 = 1.033569574356079 + 0.1 * 6.587587833404541
Epoch 160, val loss: 1.1731903553009033
Epoch 170, training loss: 1.5968785285949707 = 0.9396932125091553 + 0.1 * 6.571852207183838
Epoch 170, val loss: 1.099328875541687
Epoch 180, training loss: 1.510944128036499 = 0.854909360408783 + 0.1 * 6.560347080230713
Epoch 180, val loss: 1.0335018634796143
Epoch 190, training loss: 1.4342434406280518 = 0.7798791527748108 + 0.1 * 6.543642520904541
Epoch 190, val loss: 0.9760017395019531
Epoch 200, training loss: 1.3668367862701416 = 0.7138316035270691 + 0.1 * 6.530052185058594
Epoch 200, val loss: 0.9268452525138855
Epoch 210, training loss: 1.3081345558166504 = 0.6556878685951233 + 0.1 * 6.524466514587402
Epoch 210, val loss: 0.885780930519104
Epoch 220, training loss: 1.2553937435150146 = 0.6045830845832825 + 0.1 * 6.5081071853637695
Epoch 220, val loss: 0.8525163531303406
Epoch 230, training loss: 1.2075852155685425 = 0.5582276582717896 + 0.1 * 6.493575572967529
Epoch 230, val loss: 0.8250606060028076
Epoch 240, training loss: 1.1631982326507568 = 0.5148410201072693 + 0.1 * 6.483572483062744
Epoch 240, val loss: 0.8022154569625854
Epoch 250, training loss: 1.1217572689056396 = 0.4737907648086548 + 0.1 * 6.4796648025512695
Epoch 250, val loss: 0.7826513051986694
Epoch 260, training loss: 1.080745816230774 = 0.43454471230506897 + 0.1 * 6.462010860443115
Epoch 260, val loss: 0.7657966613769531
Epoch 270, training loss: 1.0419456958770752 = 0.39641380310058594 + 0.1 * 6.455317974090576
Epoch 270, val loss: 0.7510793209075928
Epoch 280, training loss: 1.003366231918335 = 0.35923242568969727 + 0.1 * 6.441338062286377
Epoch 280, val loss: 0.7384189963340759
Epoch 290, training loss: 0.9663456678390503 = 0.32302260398864746 + 0.1 * 6.433230400085449
Epoch 290, val loss: 0.7277930378913879
Epoch 300, training loss: 0.9306492805480957 = 0.2881637513637543 + 0.1 * 6.424854755401611
Epoch 300, val loss: 0.7191382646560669
Epoch 310, training loss: 0.8974961042404175 = 0.25520288944244385 + 0.1 * 6.422932147979736
Epoch 310, val loss: 0.712155818939209
Epoch 320, training loss: 0.8660937547683716 = 0.22489400207996368 + 0.1 * 6.411997318267822
Epoch 320, val loss: 0.7071513533592224
Epoch 330, training loss: 0.8392130136489868 = 0.19775095582008362 + 0.1 * 6.414620399475098
Epoch 330, val loss: 0.7044902443885803
Epoch 340, training loss: 0.8140925168991089 = 0.17422392964363098 + 0.1 * 6.398685455322266
Epoch 340, val loss: 0.7043190598487854
Epoch 350, training loss: 0.7935346364974976 = 0.15409131348133087 + 0.1 * 6.394433498382568
Epoch 350, val loss: 0.7067296504974365
Epoch 360, training loss: 0.7762643098831177 = 0.1369759440422058 + 0.1 * 6.39288330078125
Epoch 360, val loss: 0.7112590074539185
Epoch 370, training loss: 0.760854959487915 = 0.12247414141893387 + 0.1 * 6.383808135986328
Epoch 370, val loss: 0.7176506519317627
Epoch 380, training loss: 0.7477321028709412 = 0.11005377024412155 + 0.1 * 6.37678337097168
Epoch 380, val loss: 0.7253379225730896
Epoch 390, training loss: 0.7371076345443726 = 0.09934777021408081 + 0.1 * 6.377598285675049
Epoch 390, val loss: 0.7340067625045776
Epoch 400, training loss: 0.7265456914901733 = 0.09009461104869843 + 0.1 * 6.364510536193848
Epoch 400, val loss: 0.7432582974433899
Epoch 410, training loss: 0.7183864116668701 = 0.08198626339435577 + 0.1 * 6.364001750946045
Epoch 410, val loss: 0.7529359459877014
Epoch 420, training loss: 0.7101453542709351 = 0.07484100013971329 + 0.1 * 6.353043079376221
Epoch 420, val loss: 0.7629855275154114
Epoch 430, training loss: 0.7030851244926453 = 0.06849141418933868 + 0.1 * 6.345937252044678
Epoch 430, val loss: 0.7732139825820923
Epoch 440, training loss: 0.6982975602149963 = 0.06281667947769165 + 0.1 * 6.354808807373047
Epoch 440, val loss: 0.7836617231369019
Epoch 450, training loss: 0.6913267374038696 = 0.057753272354602814 + 0.1 * 6.335734844207764
Epoch 450, val loss: 0.7941805720329285
Epoch 460, training loss: 0.6864403486251831 = 0.05319327488541603 + 0.1 * 6.332470893859863
Epoch 460, val loss: 0.804771363735199
Epoch 470, training loss: 0.6823097467422485 = 0.049080051481723785 + 0.1 * 6.332296371459961
Epoch 470, val loss: 0.8154285550117493
Epoch 480, training loss: 0.6781539916992188 = 0.04537340998649597 + 0.1 * 6.327805995941162
Epoch 480, val loss: 0.8262203931808472
Epoch 490, training loss: 0.674636960029602 = 0.04202195629477501 + 0.1 * 6.326150417327881
Epoch 490, val loss: 0.837012529373169
Epoch 500, training loss: 0.6703152060508728 = 0.03898811340332031 + 0.1 * 6.3132710456848145
Epoch 500, val loss: 0.8479005694389343
Epoch 510, training loss: 0.6675221920013428 = 0.03624255210161209 + 0.1 * 6.312796592712402
Epoch 510, val loss: 0.8586889505386353
Epoch 520, training loss: 0.6644896864891052 = 0.033759839832782745 + 0.1 * 6.30729866027832
Epoch 520, val loss: 0.8694842457771301
Epoch 530, training loss: 0.6622723937034607 = 0.031507231295108795 + 0.1 * 6.307651042938232
Epoch 530, val loss: 0.8801539540290833
Epoch 540, training loss: 0.6602127552032471 = 0.029467834159731865 + 0.1 * 6.3074493408203125
Epoch 540, val loss: 0.8906978368759155
Epoch 550, training loss: 0.6574678421020508 = 0.02761610597372055 + 0.1 * 6.298517227172852
Epoch 550, val loss: 0.9010187387466431
Epoch 560, training loss: 0.6553613543510437 = 0.025931041687726974 + 0.1 * 6.294302940368652
Epoch 560, val loss: 0.9111778736114502
Epoch 570, training loss: 0.6531803607940674 = 0.024393895640969276 + 0.1 * 6.2878642082214355
Epoch 570, val loss: 0.9210328459739685
Epoch 580, training loss: 0.6510851383209229 = 0.022992493584752083 + 0.1 * 6.28092622756958
Epoch 580, val loss: 0.9308179616928101
Epoch 590, training loss: 0.6491349935531616 = 0.02170804888010025 + 0.1 * 6.274269104003906
Epoch 590, val loss: 0.9403141140937805
Epoch 600, training loss: 0.6476707458496094 = 0.02052886225283146 + 0.1 * 6.271418571472168
Epoch 600, val loss: 0.9495002031326294
Epoch 610, training loss: 0.6464056968688965 = 0.019450485706329346 + 0.1 * 6.269551753997803
Epoch 610, val loss: 0.9586521983146667
Epoch 620, training loss: 0.6467270255088806 = 0.018455808982253075 + 0.1 * 6.282711982727051
Epoch 620, val loss: 0.9675226211547852
Epoch 630, training loss: 0.6433805227279663 = 0.017539622262120247 + 0.1 * 6.258409023284912
Epoch 630, val loss: 0.9760408997535706
Epoch 640, training loss: 0.6417616009712219 = 0.016693327575922012 + 0.1 * 6.250682353973389
Epoch 640, val loss: 0.9845203757286072
Epoch 650, training loss: 0.6405721306800842 = 0.0159074105322361 + 0.1 * 6.246647357940674
Epoch 650, val loss: 0.9927517175674438
Epoch 660, training loss: 0.6396409273147583 = 0.015179584734141827 + 0.1 * 6.2446136474609375
Epoch 660, val loss: 1.0008562803268433
Epoch 670, training loss: 0.6401876211166382 = 0.014501512050628662 + 0.1 * 6.256860733032227
Epoch 670, val loss: 1.008786916732788
Epoch 680, training loss: 0.6377032995223999 = 0.013870671391487122 + 0.1 * 6.238326549530029
Epoch 680, val loss: 1.016433596611023
Epoch 690, training loss: 0.636352002620697 = 0.013283689506351948 + 0.1 * 6.230683326721191
Epoch 690, val loss: 1.0240778923034668
Epoch 700, training loss: 0.6356671452522278 = 0.012733452022075653 + 0.1 * 6.229336738586426
Epoch 700, val loss: 1.0313900709152222
Epoch 710, training loss: 0.6358335614204407 = 0.012220166623592377 + 0.1 * 6.236134052276611
Epoch 710, val loss: 1.0385881662368774
Epoch 720, training loss: 0.6339057683944702 = 0.01174155529588461 + 0.1 * 6.221641540527344
Epoch 720, val loss: 1.04560387134552
Epoch 730, training loss: 0.633822500705719 = 0.011292356066405773 + 0.1 * 6.225301265716553
Epoch 730, val loss: 1.0525550842285156
Epoch 740, training loss: 0.6333796977996826 = 0.010869355872273445 + 0.1 * 6.22510290145874
Epoch 740, val loss: 1.0592153072357178
Epoch 750, training loss: 0.6312830448150635 = 0.010471846908330917 + 0.1 * 6.2081122398376465
Epoch 750, val loss: 1.0658754110336304
Epoch 760, training loss: 0.6314672231674194 = 0.010097017511725426 + 0.1 * 6.2137017250061035
Epoch 760, val loss: 1.072402000427246
Epoch 770, training loss: 0.6324065923690796 = 0.009743164293467999 + 0.1 * 6.2266340255737305
Epoch 770, val loss: 1.0787067413330078
Epoch 780, training loss: 0.6300424933433533 = 0.00941064115613699 + 0.1 * 6.206318378448486
Epoch 780, val loss: 1.0848991870880127
Epoch 790, training loss: 0.6294959187507629 = 0.009096062742173672 + 0.1 * 6.20399808883667
Epoch 790, val loss: 1.0910855531692505
Epoch 800, training loss: 0.6295055150985718 = 0.008797900751233101 + 0.1 * 6.207076072692871
Epoch 800, val loss: 1.0970641374588013
Epoch 810, training loss: 0.6281730532646179 = 0.008516336791217327 + 0.1 * 6.196567535400391
Epoch 810, val loss: 1.1028947830200195
Epoch 820, training loss: 0.6282910108566284 = 0.008250375278294086 + 0.1 * 6.200406551361084
Epoch 820, val loss: 1.1087108850479126
Epoch 830, training loss: 0.6281931400299072 = 0.007996372878551483 + 0.1 * 6.201967716217041
Epoch 830, val loss: 1.1143426895141602
Epoch 840, training loss: 0.6263514161109924 = 0.007755804341286421 + 0.1 * 6.185956001281738
Epoch 840, val loss: 1.119956374168396
Epoch 850, training loss: 0.6264930963516235 = 0.007527027279138565 + 0.1 * 6.18966007232666
Epoch 850, val loss: 1.1254725456237793
Epoch 860, training loss: 0.627170979976654 = 0.007309450302273035 + 0.1 * 6.198615074157715
Epoch 860, val loss: 1.1306887865066528
Epoch 870, training loss: 0.626227617263794 = 0.007102878298610449 + 0.1 * 6.19124698638916
Epoch 870, val loss: 1.135939121246338
Epoch 880, training loss: 0.6254860162734985 = 0.006906524300575256 + 0.1 * 6.185794830322266
Epoch 880, val loss: 1.1411973237991333
Epoch 890, training loss: 0.6240751147270203 = 0.006718414835631847 + 0.1 * 6.173566818237305
Epoch 890, val loss: 1.1462682485580444
Epoch 900, training loss: 0.625568687915802 = 0.006539124995470047 + 0.1 * 6.190295696258545
Epoch 900, val loss: 1.151258111000061
Epoch 910, training loss: 0.6249651312828064 = 0.006366973742842674 + 0.1 * 6.185981273651123
Epoch 910, val loss: 1.1560980081558228
Epoch 920, training loss: 0.6234200596809387 = 0.006202978547662497 + 0.1 * 6.172170639038086
Epoch 920, val loss: 1.1610121726989746
Epoch 930, training loss: 0.6241098046302795 = 0.006045498885214329 + 0.1 * 6.180642604827881
Epoch 930, val loss: 1.1656936407089233
Epoch 940, training loss: 0.6231717467308044 = 0.005895619746297598 + 0.1 * 6.172760963439941
Epoch 940, val loss: 1.1703126430511475
Epoch 950, training loss: 0.6230369806289673 = 0.005752112716436386 + 0.1 * 6.172848701477051
Epoch 950, val loss: 1.1749407052993774
Epoch 960, training loss: 0.6220749616622925 = 0.005613489076495171 + 0.1 * 6.164614677429199
Epoch 960, val loss: 1.1793287992477417
Epoch 970, training loss: 0.6229796409606934 = 0.005481299012899399 + 0.1 * 6.174983501434326
Epoch 970, val loss: 1.183726191520691
Epoch 980, training loss: 0.6214677691459656 = 0.0053541953675448895 + 0.1 * 6.161135673522949
Epoch 980, val loss: 1.188119649887085
Epoch 990, training loss: 0.6208798289299011 = 0.0052328770980238914 + 0.1 * 6.156469345092773
Epoch 990, val loss: 1.192436933517456
Epoch 1000, training loss: 0.6212724447250366 = 0.005114739295095205 + 0.1 * 6.161577224731445
Epoch 1000, val loss: 1.1965304613113403
Epoch 1010, training loss: 0.6205063462257385 = 0.0050018420442938805 + 0.1 * 6.155045032501221
Epoch 1010, val loss: 1.2006646394729614
Epoch 1020, training loss: 0.6201832294464111 = 0.004893758334219456 + 0.1 * 6.152894496917725
Epoch 1020, val loss: 1.204852819442749
Epoch 1030, training loss: 0.6215906143188477 = 0.004788878373801708 + 0.1 * 6.168017387390137
Epoch 1030, val loss: 1.2088292837142944
Epoch 1040, training loss: 0.6200202107429504 = 0.004688001703470945 + 0.1 * 6.153321743011475
Epoch 1040, val loss: 1.2127866744995117
Epoch 1050, training loss: 0.6200460195541382 = 0.00459115719422698 + 0.1 * 6.154548645019531
Epoch 1050, val loss: 1.2167736291885376
Epoch 1060, training loss: 0.6193892955780029 = 0.004497414920479059 + 0.1 * 6.148918628692627
Epoch 1060, val loss: 1.220482349395752
Epoch 1070, training loss: 0.6186068654060364 = 0.0044072410091757774 + 0.1 * 6.141996383666992
Epoch 1070, val loss: 1.2243092060089111
Epoch 1080, training loss: 0.619442343711853 = 0.004320302978157997 + 0.1 * 6.151219844818115
Epoch 1080, val loss: 1.228114366531372
Epoch 1090, training loss: 0.6184460520744324 = 0.004236119799315929 + 0.1 * 6.142099380493164
Epoch 1090, val loss: 1.2317055463790894
Epoch 1100, training loss: 0.6187835335731506 = 0.004155208356678486 + 0.1 * 6.146283149719238
Epoch 1100, val loss: 1.2353569269180298
Epoch 1110, training loss: 0.620093047618866 = 0.004076739307492971 + 0.1 * 6.160163402557373
Epoch 1110, val loss: 1.238892912864685
Epoch 1120, training loss: 0.6182137727737427 = 0.004000860266387463 + 0.1 * 6.142128944396973
Epoch 1120, val loss: 1.242380976676941
Epoch 1130, training loss: 0.61784428358078 = 0.003928174264729023 + 0.1 * 6.139161109924316
Epoch 1130, val loss: 1.2458926439285278
Epoch 1140, training loss: 0.6185239553451538 = 0.0038571571931242943 + 0.1 * 6.146667957305908
Epoch 1140, val loss: 1.2493776082992554
Epoch 1150, training loss: 0.6167402267456055 = 0.003788153873756528 + 0.1 * 6.129520416259766
Epoch 1150, val loss: 1.2525594234466553
Epoch 1160, training loss: 0.617037296295166 = 0.0037219899240881205 + 0.1 * 6.133152484893799
Epoch 1160, val loss: 1.2559176683425903
Epoch 1170, training loss: 0.6166105270385742 = 0.0036576692946255207 + 0.1 * 6.129528045654297
Epoch 1170, val loss: 1.259277105331421
Epoch 1180, training loss: 0.6179453730583191 = 0.0035949896555393934 + 0.1 * 6.143503665924072
Epoch 1180, val loss: 1.2624424695968628
Epoch 1190, training loss: 0.6165981888771057 = 0.003534360323101282 + 0.1 * 6.130638122558594
Epoch 1190, val loss: 1.2655832767486572
Epoch 1200, training loss: 0.6171287298202515 = 0.003475778503343463 + 0.1 * 6.136529445648193
Epoch 1200, val loss: 1.2688602209091187
Epoch 1210, training loss: 0.615591287612915 = 0.0034190434962511063 + 0.1 * 6.121722221374512
Epoch 1210, val loss: 1.2718768119812012
Epoch 1220, training loss: 0.6160304546356201 = 0.003364172298461199 + 0.1 * 6.126662731170654
Epoch 1220, val loss: 1.2749769687652588
Epoch 1230, training loss: 0.6156537532806396 = 0.0033102398738265038 + 0.1 * 6.123435020446777
Epoch 1230, val loss: 1.2779897451400757
Epoch 1240, training loss: 0.6154125928878784 = 0.0032584848813712597 + 0.1 * 6.1215410232543945
Epoch 1240, val loss: 1.2810125350952148
Epoch 1250, training loss: 0.6158549785614014 = 0.003208157606422901 + 0.1 * 6.126468181610107
Epoch 1250, val loss: 1.283980131149292
Epoch 1260, training loss: 0.6154322028160095 = 0.0031587821431457996 + 0.1 * 6.1227335929870605
Epoch 1260, val loss: 1.2869635820388794
Epoch 1270, training loss: 0.6160165667533875 = 0.0031107128597795963 + 0.1 * 6.129058361053467
Epoch 1270, val loss: 1.2897510528564453
Epoch 1280, training loss: 0.6154047846794128 = 0.0030644298531115055 + 0.1 * 6.123403072357178
Epoch 1280, val loss: 1.2925666570663452
Epoch 1290, training loss: 0.6151778101921082 = 0.0030191882979124784 + 0.1 * 6.121585845947266
Epoch 1290, val loss: 1.295460820198059
Epoch 1300, training loss: 0.613932728767395 = 0.0029751118272542953 + 0.1 * 6.109576225280762
Epoch 1300, val loss: 1.2983033657073975
Epoch 1310, training loss: 0.6161947846412659 = 0.0029322251211851835 + 0.1 * 6.132625102996826
Epoch 1310, val loss: 1.3010635375976562
Epoch 1320, training loss: 0.6143136024475098 = 0.0028900494799017906 + 0.1 * 6.1142354011535645
Epoch 1320, val loss: 1.3036803007125854
Epoch 1330, training loss: 0.6148744821548462 = 0.0028493935242295265 + 0.1 * 6.120250701904297
Epoch 1330, val loss: 1.3064689636230469
Epoch 1340, training loss: 0.6143044829368591 = 0.0028096456080675125 + 0.1 * 6.114948272705078
Epoch 1340, val loss: 1.3091366291046143
Epoch 1350, training loss: 0.6143194437026978 = 0.0027706297114491463 + 0.1 * 6.115488529205322
Epoch 1350, val loss: 1.3116461038589478
Epoch 1360, training loss: 0.6143726706504822 = 0.002732702065259218 + 0.1 * 6.116399765014648
Epoch 1360, val loss: 1.314170002937317
Epoch 1370, training loss: 0.6132766008377075 = 0.002696097129955888 + 0.1 * 6.105804920196533
Epoch 1370, val loss: 1.316769003868103
Epoch 1380, training loss: 0.6142016649246216 = 0.002660477301105857 + 0.1 * 6.115411758422852
Epoch 1380, val loss: 1.3193894624710083
Epoch 1390, training loss: 0.6127674579620361 = 0.002625040477141738 + 0.1 * 6.101423740386963
Epoch 1390, val loss: 1.3218415975570679
Epoch 1400, training loss: 0.6144498586654663 = 0.0025907994713634253 + 0.1 * 6.118590831756592
Epoch 1400, val loss: 1.3243484497070312
Epoch 1410, training loss: 0.6136000156402588 = 0.0025573906023055315 + 0.1 * 6.11042594909668
Epoch 1410, val loss: 1.3266984224319458
Epoch 1420, training loss: 0.6128013134002686 = 0.0025246667210012674 + 0.1 * 6.102766513824463
Epoch 1420, val loss: 1.3291658163070679
Epoch 1430, training loss: 0.6135956048965454 = 0.002492808038368821 + 0.1 * 6.11102819442749
Epoch 1430, val loss: 1.3316165208816528
Epoch 1440, training loss: 0.6127873659133911 = 0.0024613861460238695 + 0.1 * 6.103259563446045
Epoch 1440, val loss: 1.3338801860809326
Epoch 1450, training loss: 0.6134403944015503 = 0.0024310853332281113 + 0.1 * 6.110092639923096
Epoch 1450, val loss: 1.3362568616867065
Epoch 1460, training loss: 0.6126395463943481 = 0.002401404781267047 + 0.1 * 6.102381229400635
Epoch 1460, val loss: 1.3385268449783325
Epoch 1470, training loss: 0.6128103137016296 = 0.002372455783188343 + 0.1 * 6.104378700256348
Epoch 1470, val loss: 1.3409919738769531
Epoch 1480, training loss: 0.6126653552055359 = 0.0023441475350409746 + 0.1 * 6.103211879730225
Epoch 1480, val loss: 1.343204379081726
Epoch 1490, training loss: 0.6122501492500305 = 0.002316097030416131 + 0.1 * 6.099340438842773
Epoch 1490, val loss: 1.3454070091247559
Epoch 1500, training loss: 0.6128506660461426 = 0.002288901712745428 + 0.1 * 6.105617523193359
Epoch 1500, val loss: 1.347527265548706
Epoch 1510, training loss: 0.6118214726448059 = 0.0022624649573117495 + 0.1 * 6.095590114593506
Epoch 1510, val loss: 1.349791407585144
Epoch 1520, training loss: 0.6124951839447021 = 0.002236567670479417 + 0.1 * 6.102586269378662
Epoch 1520, val loss: 1.352029800415039
Epoch 1530, training loss: 0.611388623714447 = 0.0022111006546765566 + 0.1 * 6.091775417327881
Epoch 1530, val loss: 1.3540912866592407
Epoch 1540, training loss: 0.6128883361816406 = 0.002186237135902047 + 0.1 * 6.107020854949951
Epoch 1540, val loss: 1.3563097715377808
Epoch 1550, training loss: 0.6124016642570496 = 0.0021617391612380743 + 0.1 * 6.1023993492126465
Epoch 1550, val loss: 1.3582103252410889
Epoch 1560, training loss: 0.6119459867477417 = 0.002137891948223114 + 0.1 * 6.098080635070801
Epoch 1560, val loss: 1.3602842092514038
Epoch 1570, training loss: 0.611322283744812 = 0.002114628441631794 + 0.1 * 6.092076301574707
Epoch 1570, val loss: 1.3624573945999146
Epoch 1580, training loss: 0.6119132041931152 = 0.00209177378565073 + 0.1 * 6.098214149475098
Epoch 1580, val loss: 1.3645150661468506
Epoch 1590, training loss: 0.6111855506896973 = 0.0020693279802799225 + 0.1 * 6.091161727905273
Epoch 1590, val loss: 1.3665168285369873
Epoch 1600, training loss: 0.6130474805831909 = 0.0020474535413086414 + 0.1 * 6.109999656677246
Epoch 1600, val loss: 1.3685423135757446
Epoch 1610, training loss: 0.6112286448478699 = 0.002025825437158346 + 0.1 * 6.0920281410217285
Epoch 1610, val loss: 1.3704851865768433
Epoch 1620, training loss: 0.6106909513473511 = 0.0020049423910677433 + 0.1 * 6.086860179901123
Epoch 1620, val loss: 1.3725967407226562
Epoch 1630, training loss: 0.6118771433830261 = 0.001984385307878256 + 0.1 * 6.098927974700928
Epoch 1630, val loss: 1.374562382698059
Epoch 1640, training loss: 0.6115205883979797 = 0.0019640352111309767 + 0.1 * 6.095565319061279
Epoch 1640, val loss: 1.3765021562576294
Epoch 1650, training loss: 0.6114004850387573 = 0.0019443234195932746 + 0.1 * 6.094561576843262
Epoch 1650, val loss: 1.3783916234970093
Epoch 1660, training loss: 0.6105627417564392 = 0.0019250112818554044 + 0.1 * 6.0863776206970215
Epoch 1660, val loss: 1.38037109375
Epoch 1670, training loss: 0.6110808253288269 = 0.0019061106722801924 + 0.1 * 6.091747283935547
Epoch 1670, val loss: 1.3823286294937134
Epoch 1680, training loss: 0.611018180847168 = 0.0018874835222959518 + 0.1 * 6.091306686401367
Epoch 1680, val loss: 1.3842179775238037
Epoch 1690, training loss: 0.610680341720581 = 0.0018690123688429594 + 0.1 * 6.088113307952881
Epoch 1690, val loss: 1.386060118675232
Epoch 1700, training loss: 0.610586941242218 = 0.0018510863883420825 + 0.1 * 6.087358474731445
Epoch 1700, val loss: 1.387883186340332
Epoch 1710, training loss: 0.6104544401168823 = 0.0018333852058276534 + 0.1 * 6.086210250854492
Epoch 1710, val loss: 1.3898290395736694
Epoch 1720, training loss: 0.6101780533790588 = 0.0018161272164434195 + 0.1 * 6.083619117736816
Epoch 1720, val loss: 1.3916598558425903
Epoch 1730, training loss: 0.6100952625274658 = 0.0017990865744650364 + 0.1 * 6.082961559295654
Epoch 1730, val loss: 1.3935621976852417
Epoch 1740, training loss: 0.6102566123008728 = 0.0017823203234001994 + 0.1 * 6.084743022918701
Epoch 1740, val loss: 1.3953570127487183
Epoch 1750, training loss: 0.6107683777809143 = 0.0017658320721238852 + 0.1 * 6.090025424957275
Epoch 1750, val loss: 1.3971420526504517
Epoch 1760, training loss: 0.6114251017570496 = 0.0017495770007371902 + 0.1 * 6.096755027770996
Epoch 1760, val loss: 1.3988066911697388
Epoch 1770, training loss: 0.6097371578216553 = 0.0017336102901026607 + 0.1 * 6.08003568649292
Epoch 1770, val loss: 1.4005992412567139
Epoch 1780, training loss: 0.6097970008850098 = 0.0017181450966745615 + 0.1 * 6.0807881355285645
Epoch 1780, val loss: 1.4024808406829834
Epoch 1790, training loss: 0.6091029047966003 = 0.0017029999289661646 + 0.1 * 6.07399845123291
Epoch 1790, val loss: 1.4042775630950928
Epoch 1800, training loss: 0.610852062702179 = 0.001687821582891047 + 0.1 * 6.091642379760742
Epoch 1800, val loss: 1.4060295820236206
Epoch 1810, training loss: 0.6097860932350159 = 0.0016729200724512339 + 0.1 * 6.081131458282471
Epoch 1810, val loss: 1.407588005065918
Epoch 1820, training loss: 0.6095878481864929 = 0.0016583333490416408 + 0.1 * 6.0792951583862305
Epoch 1820, val loss: 1.4093353748321533
Epoch 1830, training loss: 0.6097914576530457 = 0.0016441073967143893 + 0.1 * 6.081472873687744
Epoch 1830, val loss: 1.41096830368042
Epoch 1840, training loss: 0.6095525026321411 = 0.0016300949500873685 + 0.1 * 6.079224109649658
Epoch 1840, val loss: 1.4126642942428589
Epoch 1850, training loss: 0.6092460751533508 = 0.001616303576156497 + 0.1 * 6.076297760009766
Epoch 1850, val loss: 1.4143790006637573
Epoch 1860, training loss: 0.6088438034057617 = 0.0016025571385398507 + 0.1 * 6.072412490844727
Epoch 1860, val loss: 1.4160152673721313
Epoch 1870, training loss: 0.6090437173843384 = 0.0015892189694568515 + 0.1 * 6.074544906616211
Epoch 1870, val loss: 1.4176384210586548
Epoch 1880, training loss: 0.609085738658905 = 0.0015759756788611412 + 0.1 * 6.075097560882568
Epoch 1880, val loss: 1.4193446636199951
Epoch 1890, training loss: 0.6093621253967285 = 0.0015628772089257836 + 0.1 * 6.0779924392700195
Epoch 1890, val loss: 1.4209051132202148
Epoch 1900, training loss: 0.6085639595985413 = 0.0015500237932428718 + 0.1 * 6.070139408111572
Epoch 1900, val loss: 1.4224401712417603
Epoch 1910, training loss: 0.6098783612251282 = 0.0015374586218968034 + 0.1 * 6.083409309387207
Epoch 1910, val loss: 1.4239736795425415
Epoch 1920, training loss: 0.6088962554931641 = 0.00152500718832016 + 0.1 * 6.0737128257751465
Epoch 1920, val loss: 1.425639033317566
Epoch 1930, training loss: 0.6099414229393005 = 0.0015128962695598602 + 0.1 * 6.084285259246826
Epoch 1930, val loss: 1.4271408319473267
Epoch 1940, training loss: 0.6085340976715088 = 0.0015008138725534081 + 0.1 * 6.070333003997803
Epoch 1940, val loss: 1.428800344467163
Epoch 1950, training loss: 0.6085699796676636 = 0.0014891375321894884 + 0.1 * 6.070807933807373
Epoch 1950, val loss: 1.4303858280181885
Epoch 1960, training loss: 0.6084082126617432 = 0.001477480516768992 + 0.1 * 6.069307327270508
Epoch 1960, val loss: 1.4319876432418823
Epoch 1970, training loss: 0.609330415725708 = 0.0014660254819318652 + 0.1 * 6.078643798828125
Epoch 1970, val loss: 1.4334416389465332
Epoch 1980, training loss: 0.6084128618240356 = 0.0014547293540090322 + 0.1 * 6.069581508636475
Epoch 1980, val loss: 1.434956431388855
Epoch 1990, training loss: 0.6084918975830078 = 0.0014436396304517984 + 0.1 * 6.0704827308654785
Epoch 1990, val loss: 1.4364975690841675
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8561
Flip ASR: 0.8267/225 nodes
The final ASR:0.72325, 0.12426, Accuracy:0.82593, 0.00302
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11652])
remove edge: torch.Size([2, 9442])
updated graph: torch.Size([2, 10538])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.783346176147461 = 1.9459577798843384 + 0.1 * 8.373884201049805
Epoch 0, val loss: 1.9385274648666382
Epoch 10, training loss: 2.7734673023223877 = 1.936090111732483 + 0.1 * 8.373772621154785
Epoch 10, val loss: 1.9285228252410889
Epoch 20, training loss: 2.7615227699279785 = 1.9242326021194458 + 0.1 * 8.37290096282959
Epoch 20, val loss: 1.9162728786468506
Epoch 30, training loss: 2.7441442012786865 = 1.9076104164123535 + 0.1 * 8.365338325500488
Epoch 30, val loss: 1.8990761041641235
Epoch 40, training loss: 2.7146503925323486 = 1.882771611213684 + 0.1 * 8.318788528442383
Epoch 40, val loss: 1.8740415573120117
Epoch 50, training loss: 2.657241106033325 = 1.8484638929367065 + 0.1 * 8.087772369384766
Epoch 50, val loss: 1.841784119606018
Epoch 60, training loss: 2.5806941986083984 = 1.807730793952942 + 0.1 * 7.729633808135986
Epoch 60, val loss: 1.8069770336151123
Epoch 70, training loss: 2.5067105293273926 = 1.7670552730560303 + 0.1 * 7.396551609039307
Epoch 70, val loss: 1.7748700380325317
Epoch 80, training loss: 2.4295873641967773 = 1.7234885692596436 + 0.1 * 7.0609869956970215
Epoch 80, val loss: 1.7400798797607422
Epoch 90, training loss: 2.3572046756744385 = 1.6679354906082153 + 0.1 * 6.892692565917969
Epoch 90, val loss: 1.692180871963501
Epoch 100, training loss: 2.277986764907837 = 1.5950006246566772 + 0.1 * 6.829860687255859
Epoch 100, val loss: 1.629172444343567
Epoch 110, training loss: 2.184462547302246 = 1.504254937171936 + 0.1 * 6.802075386047363
Epoch 110, val loss: 1.5540193319320679
Epoch 120, training loss: 2.080078125 = 1.4016238451004028 + 0.1 * 6.784541606903076
Epoch 120, val loss: 1.471316933631897
Epoch 130, training loss: 1.971245288848877 = 1.2940593957901 + 0.1 * 6.771858215332031
Epoch 130, val loss: 1.386167287826538
Epoch 140, training loss: 1.861281394958496 = 1.185125708580017 + 0.1 * 6.761557579040527
Epoch 140, val loss: 1.3013261556625366
Epoch 150, training loss: 1.7532083988189697 = 1.0781242847442627 + 0.1 * 6.7508416175842285
Epoch 150, val loss: 1.2192578315734863
Epoch 160, training loss: 1.6498308181762695 = 0.9762006402015686 + 0.1 * 6.736302375793457
Epoch 160, val loss: 1.1423004865646362
Epoch 170, training loss: 1.5531435012817383 = 0.8811336755752563 + 0.1 * 6.720097541809082
Epoch 170, val loss: 1.0718754529953003
Epoch 180, training loss: 1.4640529155731201 = 0.7942346334457397 + 0.1 * 6.698183059692383
Epoch 180, val loss: 1.0092917680740356
Epoch 190, training loss: 1.3829312324523926 = 0.7153286933898926 + 0.1 * 6.676024913787842
Epoch 190, val loss: 0.9535029530525208
Epoch 200, training loss: 1.311019778251648 = 0.6451213359832764 + 0.1 * 6.658984184265137
Epoch 200, val loss: 0.9050569534301758
Epoch 210, training loss: 1.248030424118042 = 0.5843914747238159 + 0.1 * 6.63638973236084
Epoch 210, val loss: 0.8653005957603455
Epoch 220, training loss: 1.1928226947784424 = 0.5311757326126099 + 0.1 * 6.616469860076904
Epoch 220, val loss: 0.8323656916618347
Epoch 230, training loss: 1.1440467834472656 = 0.4840138256549835 + 0.1 * 6.600329875946045
Epoch 230, val loss: 0.805855929851532
Epoch 240, training loss: 1.0996575355529785 = 0.44188177585601807 + 0.1 * 6.577757358551025
Epoch 240, val loss: 0.7849088907241821
Epoch 250, training loss: 1.057774543762207 = 0.40245410799980164 + 0.1 * 6.553204536437988
Epoch 250, val loss: 0.7674227952957153
Epoch 260, training loss: 1.0201008319854736 = 0.36493080854415894 + 0.1 * 6.551700115203857
Epoch 260, val loss: 0.7531076073646545
Epoch 270, training loss: 0.9819173812866211 = 0.3292674422264099 + 0.1 * 6.526499271392822
Epoch 270, val loss: 0.7419923543930054
Epoch 280, training loss: 0.947668731212616 = 0.2954567074775696 + 0.1 * 6.522119998931885
Epoch 280, val loss: 0.7335954904556274
Epoch 290, training loss: 0.9140346050262451 = 0.2642499804496765 + 0.1 * 6.4978461265563965
Epoch 290, val loss: 0.7281861305236816
Epoch 300, training loss: 0.884650468826294 = 0.23581472039222717 + 0.1 * 6.488357067108154
Epoch 300, val loss: 0.725041925907135
Epoch 310, training loss: 0.8589639067649841 = 0.21056820452213287 + 0.1 * 6.483956813812256
Epoch 310, val loss: 0.7245016098022461
Epoch 320, training loss: 0.835116982460022 = 0.18839024007320404 + 0.1 * 6.467267036437988
Epoch 320, val loss: 0.7262099385261536
Epoch 330, training loss: 0.8146377801895142 = 0.1688922941684723 + 0.1 * 6.457454204559326
Epoch 330, val loss: 0.73007732629776
Epoch 340, training loss: 0.7999551296234131 = 0.1519032120704651 + 0.1 * 6.480518817901611
Epoch 340, val loss: 0.735501766204834
Epoch 350, training loss: 0.7823341488838196 = 0.13716955482959747 + 0.1 * 6.451645851135254
Epoch 350, val loss: 0.7422599792480469
Epoch 360, training loss: 0.7674885988235474 = 0.1241956278681755 + 0.1 * 6.432929515838623
Epoch 360, val loss: 0.7502914071083069
Epoch 370, training loss: 0.7590845823287964 = 0.11272197961807251 + 0.1 * 6.463625907897949
Epoch 370, val loss: 0.7593393921852112
Epoch 380, training loss: 0.7454588413238525 = 0.10264478623867035 + 0.1 * 6.428140163421631
Epoch 380, val loss: 0.7691776752471924
Epoch 390, training loss: 0.7350268363952637 = 0.093698650598526 + 0.1 * 6.4132819175720215
Epoch 390, val loss: 0.7795575261116028
Epoch 400, training loss: 0.7272733449935913 = 0.08573314547538757 + 0.1 * 6.415402412414551
Epoch 400, val loss: 0.7905371189117432
Epoch 410, training loss: 0.7183141112327576 = 0.07866822183132172 + 0.1 * 6.396458625793457
Epoch 410, val loss: 0.8018027544021606
Epoch 420, training loss: 0.7111458778381348 = 0.07234150171279907 + 0.1 * 6.3880438804626465
Epoch 420, val loss: 0.8131715655326843
Epoch 430, training loss: 0.7048428058624268 = 0.06666472554206848 + 0.1 * 6.381781101226807
Epoch 430, val loss: 0.8246716260910034
Epoch 440, training loss: 0.6998141407966614 = 0.061589356511831284 + 0.1 * 6.3822479248046875
Epoch 440, val loss: 0.8361987471580505
Epoch 450, training loss: 0.6942540407180786 = 0.05702697113156319 + 0.1 * 6.372270584106445
Epoch 450, val loss: 0.8473197221755981
Epoch 460, training loss: 0.6894311308860779 = 0.05292860046029091 + 0.1 * 6.365025043487549
Epoch 460, val loss: 0.8586581945419312
Epoch 470, training loss: 0.6846581101417542 = 0.04920909181237221 + 0.1 * 6.354489803314209
Epoch 470, val loss: 0.8695993423461914
Epoch 480, training loss: 0.6821634769439697 = 0.045824192464351654 + 0.1 * 6.3633928298950195
Epoch 480, val loss: 0.8805720210075378
Epoch 490, training loss: 0.6773529052734375 = 0.04275711625814438 + 0.1 * 6.345958232879639
Epoch 490, val loss: 0.8911551833152771
Epoch 500, training loss: 0.6744170188903809 = 0.03997594118118286 + 0.1 * 6.3444108963012695
Epoch 500, val loss: 0.9017091989517212
Epoch 510, training loss: 0.6718167662620544 = 0.03743536397814751 + 0.1 * 6.343813896179199
Epoch 510, val loss: 0.911902666091919
Epoch 520, training loss: 0.6678314208984375 = 0.03511437401175499 + 0.1 * 6.327170372009277
Epoch 520, val loss: 0.9220514297485352
Epoch 530, training loss: 0.6657368540763855 = 0.03298823535442352 + 0.1 * 6.327486038208008
Epoch 530, val loss: 0.9318571090698242
Epoch 540, training loss: 0.663067638874054 = 0.03104364685714245 + 0.1 * 6.320239543914795
Epoch 540, val loss: 0.9416947364807129
Epoch 550, training loss: 0.6610748171806335 = 0.02925989218056202 + 0.1 * 6.318149089813232
Epoch 550, val loss: 0.9511474370956421
Epoch 560, training loss: 0.6588388681411743 = 0.027620377019047737 + 0.1 * 6.312184810638428
Epoch 560, val loss: 0.9605860710144043
Epoch 570, training loss: 0.6569378972053528 = 0.026110345497727394 + 0.1 * 6.30827522277832
Epoch 570, val loss: 0.9696385264396667
Epoch 580, training loss: 0.6541332006454468 = 0.02472132444381714 + 0.1 * 6.294118881225586
Epoch 580, val loss: 0.978699266910553
Epoch 590, training loss: 0.6526952981948853 = 0.02343623712658882 + 0.1 * 6.292590141296387
Epoch 590, val loss: 0.9874550104141235
Epoch 600, training loss: 0.6534342765808105 = 0.022244969382882118 + 0.1 * 6.311892509460449
Epoch 600, val loss: 0.9960879683494568
Epoch 610, training loss: 0.6501695513725281 = 0.021146943792700768 + 0.1 * 6.290225982666016
Epoch 610, val loss: 1.0046133995056152
Epoch 620, training loss: 0.6505640745162964 = 0.020126570016145706 + 0.1 * 6.304374694824219
Epoch 620, val loss: 1.0129562616348267
Epoch 630, training loss: 0.6476926207542419 = 0.01918097771704197 + 0.1 * 6.285116672515869
Epoch 630, val loss: 1.0210133790969849
Epoch 640, training loss: 0.6481611728668213 = 0.01829940639436245 + 0.1 * 6.298617839813232
Epoch 640, val loss: 1.0290985107421875
Epoch 650, training loss: 0.6448682546615601 = 0.017479214817285538 + 0.1 * 6.273890018463135
Epoch 650, val loss: 1.0367640256881714
Epoch 660, training loss: 0.6449271440505981 = 0.016712592914700508 + 0.1 * 6.2821455001831055
Epoch 660, val loss: 1.0445746183395386
Epoch 670, training loss: 0.6413893103599548 = 0.01599705033004284 + 0.1 * 6.253922462463379
Epoch 670, val loss: 1.052075743675232
Epoch 680, training loss: 0.6432508230209351 = 0.015325970947742462 + 0.1 * 6.2792487144470215
Epoch 680, val loss: 1.059390664100647
Epoch 690, training loss: 0.6402276754379272 = 0.014697960577905178 + 0.1 * 6.25529670715332
Epoch 690, val loss: 1.06666898727417
Epoch 700, training loss: 0.6388923525810242 = 0.014109422452747822 + 0.1 * 6.247828960418701
Epoch 700, val loss: 1.073765516281128
Epoch 710, training loss: 0.6386862993240356 = 0.01355705875903368 + 0.1 * 6.2512922286987305
Epoch 710, val loss: 1.0807133913040161
Epoch 720, training loss: 0.6372842788696289 = 0.013037730939686298 + 0.1 * 6.242465019226074
Epoch 720, val loss: 1.0873937606811523
Epoch 730, training loss: 0.6358711123466492 = 0.012550684623420238 + 0.1 * 6.233203887939453
Epoch 730, val loss: 1.0941810607910156
Epoch 740, training loss: 0.6367756724357605 = 0.012089835479855537 + 0.1 * 6.2468581199646
Epoch 740, val loss: 1.1006470918655396
Epoch 750, training loss: 0.634182870388031 = 0.01165616512298584 + 0.1 * 6.225266933441162
Epoch 750, val loss: 1.1069856882095337
Epoch 760, training loss: 0.6347701549530029 = 0.0112460283562541 + 0.1 * 6.235240936279297
Epoch 760, val loss: 1.1133884191513062
Epoch 770, training loss: 0.6343303322792053 = 0.010859300382435322 + 0.1 * 6.234710216522217
Epoch 770, val loss: 1.1194032430648804
Epoch 780, training loss: 0.6332908272743225 = 0.010493204928934574 + 0.1 * 6.227975845336914
Epoch 780, val loss: 1.12564218044281
Epoch 790, training loss: 0.6326210498809814 = 0.010145562700927258 + 0.1 * 6.224754810333252
Epoch 790, val loss: 1.1314817667007446
Epoch 800, training loss: 0.6326265335083008 = 0.00981634110212326 + 0.1 * 6.22810173034668
Epoch 800, val loss: 1.1372846364974976
Epoch 810, training loss: 0.6310871839523315 = 0.009504531510174274 + 0.1 * 6.215826034545898
Epoch 810, val loss: 1.1429682970046997
Epoch 820, training loss: 0.6310516595840454 = 0.009208319708704948 + 0.1 * 6.218433380126953
Epoch 820, val loss: 1.1486566066741943
Epoch 830, training loss: 0.6299843788146973 = 0.00892603024840355 + 0.1 * 6.210583209991455
Epoch 830, val loss: 1.1541064977645874
Epoch 840, training loss: 0.6298396587371826 = 0.008658172562718391 + 0.1 * 6.211814880371094
Epoch 840, val loss: 1.15946364402771
Epoch 850, training loss: 0.6290911436080933 = 0.008402884937822819 + 0.1 * 6.206882476806641
Epoch 850, val loss: 1.1648688316345215
Epoch 860, training loss: 0.628167450428009 = 0.00815939623862505 + 0.1 * 6.200080394744873
Epoch 860, val loss: 1.1700260639190674
Epoch 870, training loss: 0.6295322775840759 = 0.007927524857223034 + 0.1 * 6.216047286987305
Epoch 870, val loss: 1.175110936164856
Epoch 880, training loss: 0.6270552277565002 = 0.007706172298640013 + 0.1 * 6.193490505218506
Epoch 880, val loss: 1.1801700592041016
Epoch 890, training loss: 0.6268240213394165 = 0.007494546473026276 + 0.1 * 6.193294525146484
Epoch 890, val loss: 1.1851847171783447
Epoch 900, training loss: 0.6279119253158569 = 0.007291757967323065 + 0.1 * 6.206201553344727
Epoch 900, val loss: 1.1899843215942383
Epoch 910, training loss: 0.625726580619812 = 0.007098611909896135 + 0.1 * 6.186279773712158
Epoch 910, val loss: 1.1947691440582275
Epoch 920, training loss: 0.6263290643692017 = 0.006913356017321348 + 0.1 * 6.194157123565674
Epoch 920, val loss: 1.199528694152832
Epoch 930, training loss: 0.6261787414550781 = 0.006735851522535086 + 0.1 * 6.19442892074585
Epoch 930, val loss: 1.2040902376174927
Epoch 940, training loss: 0.6239870190620422 = 0.006566246040165424 + 0.1 * 6.17420768737793
Epoch 940, val loss: 1.2086948156356812
Epoch 950, training loss: 0.6242210865020752 = 0.006403001025319099 + 0.1 * 6.178180694580078
Epoch 950, val loss: 1.2132230997085571
Epoch 960, training loss: 0.6265720725059509 = 0.006246099714189768 + 0.1 * 6.2032599449157715
Epoch 960, val loss: 1.2175379991531372
Epoch 970, training loss: 0.6237245202064514 = 0.006096149329096079 + 0.1 * 6.176283836364746
Epoch 970, val loss: 1.2217822074890137
Epoch 980, training loss: 0.6225625872612 = 0.0059520225040614605 + 0.1 * 6.1661057472229
Epoch 980, val loss: 1.226227879524231
Epoch 990, training loss: 0.6259236335754395 = 0.005812883377075195 + 0.1 * 6.201107501983643
Epoch 990, val loss: 1.2303953170776367
Epoch 1000, training loss: 0.6230396032333374 = 0.0056793163530528545 + 0.1 * 6.17360258102417
Epoch 1000, val loss: 1.234346866607666
Epoch 1010, training loss: 0.6223879456520081 = 0.0055513568222522736 + 0.1 * 6.168365955352783
Epoch 1010, val loss: 1.2385965585708618
Epoch 1020, training loss: 0.6239710450172424 = 0.0054274084977805614 + 0.1 * 6.185436248779297
Epoch 1020, val loss: 1.2426055669784546
Epoch 1030, training loss: 0.6230268478393555 = 0.005308541469275951 + 0.1 * 6.177182674407959
Epoch 1030, val loss: 1.246429681777954
Epoch 1040, training loss: 0.6210145950317383 = 0.005193953402340412 + 0.1 * 6.158206462860107
Epoch 1040, val loss: 1.2502912282943726
Epoch 1050, training loss: 0.6207820177078247 = 0.005083685275167227 + 0.1 * 6.156983375549316
Epoch 1050, val loss: 1.2542743682861328
Epoch 1060, training loss: 0.6212171912193298 = 0.004976796451956034 + 0.1 * 6.162403583526611
Epoch 1060, val loss: 1.2579971551895142
Epoch 1070, training loss: 0.6206684708595276 = 0.004873898345977068 + 0.1 * 6.15794563293457
Epoch 1070, val loss: 1.2617292404174805
Epoch 1080, training loss: 0.6198757886886597 = 0.004774297587573528 + 0.1 * 6.151014804840088
Epoch 1080, val loss: 1.2654235363006592
Epoch 1090, training loss: 0.621250569820404 = 0.004678208846598864 + 0.1 * 6.16572380065918
Epoch 1090, val loss: 1.269026279449463
Epoch 1100, training loss: 0.6201263070106506 = 0.004585405811667442 + 0.1 * 6.15540885925293
Epoch 1100, val loss: 1.2725939750671387
Epoch 1110, training loss: 0.6199926733970642 = 0.004495879169553518 + 0.1 * 6.15496826171875
Epoch 1110, val loss: 1.2762105464935303
Epoch 1120, training loss: 0.6185929179191589 = 0.004409098532050848 + 0.1 * 6.141838073730469
Epoch 1120, val loss: 1.2796974182128906
Epoch 1130, training loss: 0.6199455261230469 = 0.004324986599385738 + 0.1 * 6.156205654144287
Epoch 1130, val loss: 1.2831528186798096
Epoch 1140, training loss: 0.6194185018539429 = 0.004243627190589905 + 0.1 * 6.1517486572265625
Epoch 1140, val loss: 1.2865612506866455
Epoch 1150, training loss: 0.6202230453491211 = 0.004164938349276781 + 0.1 * 6.160581111907959
Epoch 1150, val loss: 1.289817452430725
Epoch 1160, training loss: 0.6182937026023865 = 0.004089027177542448 + 0.1 * 6.142046928405762
Epoch 1160, val loss: 1.2931376695632935
Epoch 1170, training loss: 0.6194099187850952 = 0.004015385173261166 + 0.1 * 6.153944969177246
Epoch 1170, val loss: 1.2965004444122314
Epoch 1180, training loss: 0.6190184354782104 = 0.003943960648030043 + 0.1 * 6.150744438171387
Epoch 1180, val loss: 1.299729347229004
Epoch 1190, training loss: 0.6182968616485596 = 0.003874627174809575 + 0.1 * 6.144222736358643
Epoch 1190, val loss: 1.3028804063796997
Epoch 1200, training loss: 0.6182435154914856 = 0.0038073700852692127 + 0.1 * 6.14436149597168
Epoch 1200, val loss: 1.3060572147369385
Epoch 1210, training loss: 0.6173233389854431 = 0.003742038970813155 + 0.1 * 6.135812759399414
Epoch 1210, val loss: 1.3090569972991943
Epoch 1220, training loss: 0.6172033548355103 = 0.0036788322031497955 + 0.1 * 6.135244846343994
Epoch 1220, val loss: 1.3121637105941772
Epoch 1230, training loss: 0.6170976758003235 = 0.003617256646975875 + 0.1 * 6.1348042488098145
Epoch 1230, val loss: 1.3151957988739014
Epoch 1240, training loss: 0.6185736656188965 = 0.003557490883395076 + 0.1 * 6.1501617431640625
Epoch 1240, val loss: 1.3182100057601929
Epoch 1250, training loss: 0.6174261569976807 = 0.0034995528403669596 + 0.1 * 6.139265537261963
Epoch 1250, val loss: 1.3211455345153809
Epoch 1260, training loss: 0.6161810755729675 = 0.0034432269167155027 + 0.1 * 6.127378463745117
Epoch 1260, val loss: 1.3240859508514404
Epoch 1270, training loss: 0.6169236302375793 = 0.0033884302247315645 + 0.1 * 6.13535213470459
Epoch 1270, val loss: 1.3270233869552612
Epoch 1280, training loss: 0.6157240867614746 = 0.0033351408783346415 + 0.1 * 6.123889446258545
Epoch 1280, val loss: 1.329848289489746
Epoch 1290, training loss: 0.6164022088050842 = 0.0032833577133715153 + 0.1 * 6.131187915802002
Epoch 1290, val loss: 1.3326135873794556
Epoch 1300, training loss: 0.6159225106239319 = 0.003233105642721057 + 0.1 * 6.126893520355225
Epoch 1300, val loss: 1.335383415222168
Epoch 1310, training loss: 0.6171576380729675 = 0.0031841264571994543 + 0.1 * 6.139734745025635
Epoch 1310, val loss: 1.3382818698883057
Epoch 1320, training loss: 0.6157441735267639 = 0.0031363775487989187 + 0.1 * 6.126077651977539
Epoch 1320, val loss: 1.3409570455551147
Epoch 1330, training loss: 0.6167765259742737 = 0.0030898256227374077 + 0.1 * 6.136866569519043
Epoch 1330, val loss: 1.3436542749404907
Epoch 1340, training loss: 0.6145268082618713 = 0.0030446762684732676 + 0.1 * 6.114821434020996
Epoch 1340, val loss: 1.3462389707565308
Epoch 1350, training loss: 0.6156201958656311 = 0.003000600729137659 + 0.1 * 6.126195907592773
Epoch 1350, val loss: 1.3489711284637451
Epoch 1360, training loss: 0.6161625981330872 = 0.0029575603548437357 + 0.1 * 6.132050037384033
Epoch 1360, val loss: 1.3515573740005493
Epoch 1370, training loss: 0.6149185299873352 = 0.0029157674871385098 + 0.1 * 6.120027542114258
Epoch 1370, val loss: 1.3540818691253662
Epoch 1380, training loss: 0.6150829195976257 = 0.0028750125784426928 + 0.1 * 6.122078895568848
Epoch 1380, val loss: 1.3566120862960815
Epoch 1390, training loss: 0.6146576404571533 = 0.002835357328876853 + 0.1 * 6.118223190307617
Epoch 1390, val loss: 1.359248161315918
Epoch 1400, training loss: 0.6141061782836914 = 0.002796499291434884 + 0.1 * 6.113096714019775
Epoch 1400, val loss: 1.3617674112319946
Epoch 1410, training loss: 0.615536093711853 = 0.0027586452197283506 + 0.1 * 6.127774715423584
Epoch 1410, val loss: 1.3642634153366089
Epoch 1420, training loss: 0.6145694851875305 = 0.00272172293625772 + 0.1 * 6.1184773445129395
Epoch 1420, val loss: 1.3666255474090576
Epoch 1430, training loss: 0.6138526201248169 = 0.0026856837794184685 + 0.1 * 6.111669540405273
Epoch 1430, val loss: 1.3690762519836426
Epoch 1440, training loss: 0.6130717992782593 = 0.00265033938921988 + 0.1 * 6.104214668273926
Epoch 1440, val loss: 1.3715417385101318
Epoch 1450, training loss: 0.6144790053367615 = 0.0026158990804105997 + 0.1 * 6.118630886077881
Epoch 1450, val loss: 1.3739351034164429
Epoch 1460, training loss: 0.6133037805557251 = 0.0025823062751442194 + 0.1 * 6.107214450836182
Epoch 1460, val loss: 1.376175045967102
Epoch 1470, training loss: 0.6156027317047119 = 0.0025495674926787615 + 0.1 * 6.130531311035156
Epoch 1470, val loss: 1.378525972366333
Epoch 1480, training loss: 0.6134690642356873 = 0.002517556771636009 + 0.1 * 6.1095147132873535
Epoch 1480, val loss: 1.3808788061141968
Epoch 1490, training loss: 0.6145247220993042 = 0.0024862007703632116 + 0.1 * 6.120384693145752
Epoch 1490, val loss: 1.3832969665527344
Epoch 1500, training loss: 0.6131645441055298 = 0.002455532317981124 + 0.1 * 6.107089996337891
Epoch 1500, val loss: 1.3855143785476685
Epoch 1510, training loss: 0.6144095659255981 = 0.0024255032185465097 + 0.1 * 6.119840621948242
Epoch 1510, val loss: 1.3877485990524292
Epoch 1520, training loss: 0.6125720143318176 = 0.002396309282630682 + 0.1 * 6.101757049560547
Epoch 1520, val loss: 1.389922022819519
Epoch 1530, training loss: 0.6127279996871948 = 0.002367633394896984 + 0.1 * 6.103603839874268
Epoch 1530, val loss: 1.3922208547592163
Epoch 1540, training loss: 0.6128536462783813 = 0.002339473459869623 + 0.1 * 6.105141639709473
Epoch 1540, val loss: 1.3944584131240845
Epoch 1550, training loss: 0.6139265894889832 = 0.0023119677789509296 + 0.1 * 6.116146564483643
Epoch 1550, val loss: 1.3966445922851562
Epoch 1560, training loss: 0.6133822798728943 = 0.0022850490640848875 + 0.1 * 6.110971927642822
Epoch 1560, val loss: 1.3987147808074951
Epoch 1570, training loss: 0.6135867238044739 = 0.0022587997373193502 + 0.1 * 6.113279342651367
Epoch 1570, val loss: 1.4007779359817505
Epoch 1580, training loss: 0.6120668053627014 = 0.002233159728348255 + 0.1 * 6.098336696624756
Epoch 1580, val loss: 1.4030706882476807
Epoch 1590, training loss: 0.6126120686531067 = 0.0022078652400523424 + 0.1 * 6.104042053222656
Epoch 1590, val loss: 1.4052181243896484
Epoch 1600, training loss: 0.6117053627967834 = 0.002183143747970462 + 0.1 * 6.095221996307373
Epoch 1600, val loss: 1.407318353652954
Epoch 1610, training loss: 0.613124668598175 = 0.002158907474949956 + 0.1 * 6.1096577644348145
Epoch 1610, val loss: 1.4093629121780396
Epoch 1620, training loss: 0.6121085286140442 = 0.002135239774361253 + 0.1 * 6.099732875823975
Epoch 1620, val loss: 1.411324143409729
Epoch 1630, training loss: 0.6124153733253479 = 0.002112042158842087 + 0.1 * 6.103033542633057
Epoch 1630, val loss: 1.413417935371399
Epoch 1640, training loss: 0.6125527024269104 = 0.002089299261569977 + 0.1 * 6.104633808135986
Epoch 1640, val loss: 1.4154672622680664
Epoch 1650, training loss: 0.6113510727882385 = 0.002067053224891424 + 0.1 * 6.092840194702148
Epoch 1650, val loss: 1.4174602031707764
Epoch 1660, training loss: 0.6135461330413818 = 0.0020451974123716354 + 0.1 * 6.11500883102417
Epoch 1660, val loss: 1.4194616079330444
Epoch 1670, training loss: 0.6112879514694214 = 0.0020238468423485756 + 0.1 * 6.0926408767700195
Epoch 1670, val loss: 1.4214165210723877
Epoch 1680, training loss: 0.6110954284667969 = 0.00200279732234776 + 0.1 * 6.090926170349121
Epoch 1680, val loss: 1.4234447479248047
Epoch 1690, training loss: 0.6115679740905762 = 0.0019821464084088802 + 0.1 * 6.095858097076416
Epoch 1690, val loss: 1.4254696369171143
Epoch 1700, training loss: 0.612144947052002 = 0.001961824717000127 + 0.1 * 6.101830959320068
Epoch 1700, val loss: 1.4272493124008179
Epoch 1710, training loss: 0.6115460991859436 = 0.001942032016813755 + 0.1 * 6.09604024887085
Epoch 1710, val loss: 1.4291560649871826
Epoch 1720, training loss: 0.6109898686408997 = 0.0019225595751777291 + 0.1 * 6.090672969818115
Epoch 1720, val loss: 1.4310401678085327
Epoch 1730, training loss: 0.6110383868217468 = 0.0019035311415791512 + 0.1 * 6.091348648071289
Epoch 1730, val loss: 1.433037519454956
Epoch 1740, training loss: 0.611179769039154 = 0.001884726108983159 + 0.1 * 6.092950344085693
Epoch 1740, val loss: 1.4349085092544556
Epoch 1750, training loss: 0.6114626526832581 = 0.0018662860384210944 + 0.1 * 6.095963478088379
Epoch 1750, val loss: 1.4367761611938477
Epoch 1760, training loss: 0.6111526489257812 = 0.0018482627347111702 + 0.1 * 6.093043804168701
Epoch 1760, val loss: 1.438576340675354
Epoch 1770, training loss: 0.6122363805770874 = 0.0018305170815438032 + 0.1 * 6.104058265686035
Epoch 1770, val loss: 1.4403791427612305
Epoch 1780, training loss: 0.6106587648391724 = 0.0018131331307813525 + 0.1 * 6.088456630706787
Epoch 1780, val loss: 1.4423401355743408
Epoch 1790, training loss: 0.6114863753318787 = 0.0017959484830498695 + 0.1 * 6.096904277801514
Epoch 1790, val loss: 1.444166898727417
Epoch 1800, training loss: 0.6104544997215271 = 0.0017791899153962731 + 0.1 * 6.086752891540527
Epoch 1800, val loss: 1.44593346118927
Epoch 1810, training loss: 0.6111540198326111 = 0.0017626653425395489 + 0.1 * 6.0939130783081055
Epoch 1810, val loss: 1.4476592540740967
Epoch 1820, training loss: 0.6109563708305359 = 0.0017464787233620882 + 0.1 * 6.092098712921143
Epoch 1820, val loss: 1.4494681358337402
Epoch 1830, training loss: 0.6109408736228943 = 0.001730556134134531 + 0.1 * 6.092103004455566
Epoch 1830, val loss: 1.4512269496917725
Epoch 1840, training loss: 0.6104593873023987 = 0.0017149659106507897 + 0.1 * 6.087444305419922
Epoch 1840, val loss: 1.4530284404754639
Epoch 1850, training loss: 0.6099668145179749 = 0.0016995624173432589 + 0.1 * 6.082672595977783
Epoch 1850, val loss: 1.454777717590332
Epoch 1860, training loss: 0.610709011554718 = 0.0016844500787556171 + 0.1 * 6.090245723724365
Epoch 1860, val loss: 1.456501841545105
Epoch 1870, training loss: 0.6100173592567444 = 0.001669599092565477 + 0.1 * 6.08347749710083
Epoch 1870, val loss: 1.4582347869873047
Epoch 1880, training loss: 0.6111035943031311 = 0.0016549589345231652 + 0.1 * 6.094485759735107
Epoch 1880, val loss: 1.4599744081497192
Epoch 1890, training loss: 0.6103832125663757 = 0.0016405724454671144 + 0.1 * 6.08742618560791
Epoch 1890, val loss: 1.461645483970642
Epoch 1900, training loss: 0.6095150709152222 = 0.001626439276151359 + 0.1 * 6.07888650894165
Epoch 1900, val loss: 1.4633550643920898
Epoch 1910, training loss: 0.611298143863678 = 0.0016124988906085491 + 0.1 * 6.096856594085693
Epoch 1910, val loss: 1.4650975465774536
Epoch 1920, training loss: 0.610029399394989 = 0.001598842442035675 + 0.1 * 6.084305763244629
Epoch 1920, val loss: 1.4666568040847778
Epoch 1930, training loss: 0.6099187135696411 = 0.0015854021767154336 + 0.1 * 6.0833330154418945
Epoch 1930, val loss: 1.4683754444122314
Epoch 1940, training loss: 0.6098761558532715 = 0.001572166453115642 + 0.1 * 6.083039283752441
Epoch 1940, val loss: 1.4700607061386108
Epoch 1950, training loss: 0.6098654866218567 = 0.0015591338742524385 + 0.1 * 6.08306360244751
Epoch 1950, val loss: 1.4716262817382812
Epoch 1960, training loss: 0.6107198596000671 = 0.0015463994350284338 + 0.1 * 6.091734886169434
Epoch 1960, val loss: 1.4732664823532104
Epoch 1970, training loss: 0.6092720627784729 = 0.0015338038792833686 + 0.1 * 6.077382564544678
Epoch 1970, val loss: 1.4748741388320923
Epoch 1980, training loss: 0.6095476150512695 = 0.0015213524457067251 + 0.1 * 6.080262660980225
Epoch 1980, val loss: 1.4765807390213013
Epoch 1990, training loss: 0.6091483235359192 = 0.001509075635112822 + 0.1 * 6.076392650604248
Epoch 1990, val loss: 1.4780833721160889
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.5535
Flip ASR: 0.4667/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7997779846191406 = 1.9623900651931763 + 0.1 * 8.373880386352539
Epoch 0, val loss: 1.9773401021957397
Epoch 10, training loss: 2.788926601409912 = 1.9515488147735596 + 0.1 * 8.373778343200684
Epoch 10, val loss: 1.9653304815292358
Epoch 20, training loss: 2.776270866394043 = 1.9389691352844238 + 0.1 * 8.373016357421875
Epoch 20, val loss: 1.950860619544983
Epoch 30, training loss: 2.7583675384521484 = 1.9217113256454468 + 0.1 * 8.366560935974121
Epoch 30, val loss: 1.9305987358093262
Epoch 40, training loss: 2.7281246185302734 = 1.8965047597885132 + 0.1 * 8.316197395324707
Epoch 40, val loss: 1.900732159614563
Epoch 50, training loss: 2.651444911956787 = 1.861589789390564 + 0.1 * 7.898551940917969
Epoch 50, val loss: 1.8605085611343384
Epoch 60, training loss: 2.5581960678100586 = 1.8225659132003784 + 0.1 * 7.3563008308410645
Epoch 60, val loss: 1.8181445598602295
Epoch 70, training loss: 2.486585855484009 = 1.7829540967941284 + 0.1 * 7.036317825317383
Epoch 70, val loss: 1.7768994569778442
Epoch 80, training loss: 2.4282524585723877 = 1.74169921875 + 0.1 * 6.865532398223877
Epoch 80, val loss: 1.737938404083252
Epoch 90, training loss: 2.3715012073516846 = 1.6948928833007812 + 0.1 * 6.766082763671875
Epoch 90, val loss: 1.696848750114441
Epoch 100, training loss: 2.305062770843506 = 1.6333569288253784 + 0.1 * 6.717059135437012
Epoch 100, val loss: 1.644836187362671
Epoch 110, training loss: 2.2227814197540283 = 1.5538146495819092 + 0.1 * 6.689666748046875
Epoch 110, val loss: 1.5790784358978271
Epoch 120, training loss: 2.1257991790771484 = 1.4586774110794067 + 0.1 * 6.67121696472168
Epoch 120, val loss: 1.5016552209854126
Epoch 130, training loss: 2.0223610401153564 = 1.356291651725769 + 0.1 * 6.660693168640137
Epoch 130, val loss: 1.42095947265625
Epoch 140, training loss: 1.9191310405731201 = 1.253923773765564 + 0.1 * 6.652071952819824
Epoch 140, val loss: 1.3444896936416626
Epoch 150, training loss: 1.8193522691726685 = 1.1550753116607666 + 0.1 * 6.6427693367004395
Epoch 150, val loss: 1.2734150886535645
Epoch 160, training loss: 1.7241787910461426 = 1.06095290184021 + 0.1 * 6.63225793838501
Epoch 160, val loss: 1.2057276964187622
Epoch 170, training loss: 1.635408878326416 = 0.9727331399917603 + 0.1 * 6.62675666809082
Epoch 170, val loss: 1.1414610147476196
Epoch 180, training loss: 1.5533865690231323 = 0.892253577709198 + 0.1 * 6.611330032348633
Epoch 180, val loss: 1.0816305875778198
Epoch 190, training loss: 1.4787344932556152 = 0.8189837336540222 + 0.1 * 6.597507476806641
Epoch 190, val loss: 1.0260589122772217
Epoch 200, training loss: 1.4113543033599854 = 0.7530103921890259 + 0.1 * 6.583439826965332
Epoch 200, val loss: 0.9752899408340454
Epoch 210, training loss: 1.3508102893829346 = 0.6941469311714172 + 0.1 * 6.566633224487305
Epoch 210, val loss: 0.9300220608711243
Epoch 220, training loss: 1.2970777750015259 = 0.6414005160331726 + 0.1 * 6.556772232055664
Epoch 220, val loss: 0.8904300332069397
Epoch 230, training loss: 1.24760103225708 = 0.5935494303703308 + 0.1 * 6.540516376495361
Epoch 230, val loss: 0.8560779690742493
Epoch 240, training loss: 1.2014825344085693 = 0.5482775568962097 + 0.1 * 6.532050132751465
Epoch 240, val loss: 0.8258820176124573
Epoch 250, training loss: 1.1562159061431885 = 0.5041143298149109 + 0.1 * 6.521015644073486
Epoch 250, val loss: 0.7985665798187256
Epoch 260, training loss: 1.1119186878204346 = 0.4603448212146759 + 0.1 * 6.5157389640808105
Epoch 260, val loss: 0.7737667560577393
Epoch 270, training loss: 1.0672829151153564 = 0.4167431890964508 + 0.1 * 6.505396842956543
Epoch 270, val loss: 0.750984787940979
Epoch 280, training loss: 1.024346947669983 = 0.37369129061698914 + 0.1 * 6.506556987762451
Epoch 280, val loss: 0.7300398945808411
Epoch 290, training loss: 0.9817171096801758 = 0.3326573669910431 + 0.1 * 6.490597248077393
Epoch 290, val loss: 0.7116850018501282
Epoch 300, training loss: 0.9444584250450134 = 0.29453063011169434 + 0.1 * 6.4992780685424805
Epoch 300, val loss: 0.6961308121681213
Epoch 310, training loss: 0.9075539112091064 = 0.2602199614048004 + 0.1 * 6.473339080810547
Epoch 310, val loss: 0.683691680431366
Epoch 320, training loss: 0.8771250247955322 = 0.22976547479629517 + 0.1 * 6.473595142364502
Epoch 320, val loss: 0.6743487119674683
Epoch 330, training loss: 0.8485938310623169 = 0.20319011807441711 + 0.1 * 6.454036712646484
Epoch 330, val loss: 0.667779266834259
Epoch 340, training loss: 0.8242530226707458 = 0.18012338876724243 + 0.1 * 6.441296100616455
Epoch 340, val loss: 0.6636549234390259
Epoch 350, training loss: 0.8040423393249512 = 0.1602797955274582 + 0.1 * 6.437625408172607
Epoch 350, val loss: 0.661733090877533
Epoch 360, training loss: 0.7859316468238831 = 0.1432284712791443 + 0.1 * 6.427031517028809
Epoch 360, val loss: 0.6616836190223694
Epoch 370, training loss: 0.7709836363792419 = 0.12850986421108246 + 0.1 * 6.424737453460693
Epoch 370, val loss: 0.663216233253479
Epoch 380, training loss: 0.7570268511772156 = 0.11580746620893478 + 0.1 * 6.412193775177002
Epoch 380, val loss: 0.6660597920417786
Epoch 390, training loss: 0.7458955645561218 = 0.10473238676786423 + 0.1 * 6.4116315841674805
Epoch 390, val loss: 0.6698675155639648
Epoch 400, training loss: 0.734311580657959 = 0.09505929052829742 + 0.1 * 6.392522811889648
Epoch 400, val loss: 0.6745737791061401
Epoch 410, training loss: 0.7252863049507141 = 0.08655203133821487 + 0.1 * 6.387342929840088
Epoch 410, val loss: 0.6800129413604736
Epoch 420, training loss: 0.7170211672782898 = 0.07903233915567398 + 0.1 * 6.37988805770874
Epoch 420, val loss: 0.6860489249229431
Epoch 430, training loss: 0.710515022277832 = 0.07236649096012115 + 0.1 * 6.381485462188721
Epoch 430, val loss: 0.6924630999565125
Epoch 440, training loss: 0.7034270167350769 = 0.06645876169204712 + 0.1 * 6.369682312011719
Epoch 440, val loss: 0.6992576718330383
Epoch 450, training loss: 0.6980822086334229 = 0.061197079718112946 + 0.1 * 6.3688507080078125
Epoch 450, val loss: 0.7062978148460388
Epoch 460, training loss: 0.6919866800308228 = 0.05651353672146797 + 0.1 * 6.35473108291626
Epoch 460, val loss: 0.7134466767311096
Epoch 470, training loss: 0.686762809753418 = 0.05231885612010956 + 0.1 * 6.344439506530762
Epoch 470, val loss: 0.7208266258239746
Epoch 480, training loss: 0.6839234232902527 = 0.04854092746973038 + 0.1 * 6.353824615478516
Epoch 480, val loss: 0.7282580137252808
Epoch 490, training loss: 0.6797671914100647 = 0.04514514282345772 + 0.1 * 6.346220016479492
Epoch 490, val loss: 0.7357006072998047
Epoch 500, training loss: 0.6761011481285095 = 0.042089834809303284 + 0.1 * 6.340113162994385
Epoch 500, val loss: 0.7431014776229858
Epoch 510, training loss: 0.6715496182441711 = 0.039326298981904984 + 0.1 * 6.322233200073242
Epoch 510, val loss: 0.7505329251289368
Epoch 520, training loss: 0.6691769361495972 = 0.0368083193898201 + 0.1 * 6.323686122894287
Epoch 520, val loss: 0.7578753232955933
Epoch 530, training loss: 0.6666668057441711 = 0.034514591097831726 + 0.1 * 6.321521759033203
Epoch 530, val loss: 0.7651482224464417
Epoch 540, training loss: 0.663790762424469 = 0.03242051973938942 + 0.1 * 6.313702583312988
Epoch 540, val loss: 0.772369921207428
Epoch 550, training loss: 0.6611550450325012 = 0.030508579686284065 + 0.1 * 6.306464672088623
Epoch 550, val loss: 0.7793923616409302
Epoch 560, training loss: 0.6588414311408997 = 0.028760991990566254 + 0.1 * 6.300804615020752
Epoch 560, val loss: 0.786454439163208
Epoch 570, training loss: 0.6581729054450989 = 0.027154460549354553 + 0.1 * 6.310184478759766
Epoch 570, val loss: 0.7933467626571655
Epoch 580, training loss: 0.655709981918335 = 0.025685446336865425 + 0.1 * 6.30024528503418
Epoch 580, val loss: 0.8000190854072571
Epoch 590, training loss: 0.6530408263206482 = 0.024335511028766632 + 0.1 * 6.287053108215332
Epoch 590, val loss: 0.8067745566368103
Epoch 600, training loss: 0.6543728709220886 = 0.02308824472129345 + 0.1 * 6.312846660614014
Epoch 600, val loss: 0.8132203221321106
Epoch 610, training loss: 0.6507529020309448 = 0.0219433456659317 + 0.1 * 6.288095474243164
Epoch 610, val loss: 0.8196028470993042
Epoch 620, training loss: 0.6484483480453491 = 0.02088269591331482 + 0.1 * 6.275656223297119
Epoch 620, val loss: 0.8259605169296265
Epoch 630, training loss: 0.6472374796867371 = 0.019897950813174248 + 0.1 * 6.273395538330078
Epoch 630, val loss: 0.8320208787918091
Epoch 640, training loss: 0.6456210017204285 = 0.018983803689479828 + 0.1 * 6.266372203826904
Epoch 640, val loss: 0.8381208777427673
Epoch 650, training loss: 0.6446866393089294 = 0.018131457269191742 + 0.1 * 6.265552043914795
Epoch 650, val loss: 0.8440176248550415
Epoch 660, training loss: 0.6440441608428955 = 0.017337139695882797 + 0.1 * 6.267070293426514
Epoch 660, val loss: 0.8497730493545532
Epoch 670, training loss: 0.642899215221405 = 0.016597608104348183 + 0.1 * 6.2630157470703125
Epoch 670, val loss: 0.8553723096847534
Epoch 680, training loss: 0.6413973569869995 = 0.015907004475593567 + 0.1 * 6.254903793334961
Epoch 680, val loss: 0.8610281944274902
Epoch 690, training loss: 0.6414220333099365 = 0.015258649364113808 + 0.1 * 6.261633396148682
Epoch 690, val loss: 0.8664588332176208
Epoch 700, training loss: 0.6396664977073669 = 0.014651876874268055 + 0.1 * 6.25014591217041
Epoch 700, val loss: 0.8716757297515869
Epoch 710, training loss: 0.6409144401550293 = 0.014083079993724823 + 0.1 * 6.268313407897949
Epoch 710, val loss: 0.8768671751022339
Epoch 720, training loss: 0.6376094818115234 = 0.013549434021115303 + 0.1 * 6.240600109100342
Epoch 720, val loss: 0.8819659352302551
Epoch 730, training loss: 0.6372986435890198 = 0.013046758249402046 + 0.1 * 6.242518901824951
Epoch 730, val loss: 0.8870146870613098
Epoch 740, training loss: 0.6360546350479126 = 0.01257284265011549 + 0.1 * 6.234817981719971
Epoch 740, val loss: 0.8919302225112915
Epoch 750, training loss: 0.636324405670166 = 0.012126042507588863 + 0.1 * 6.241983413696289
Epoch 750, val loss: 0.8966615796089172
Epoch 760, training loss: 0.6352134346961975 = 0.011704548262059689 + 0.1 * 6.235088348388672
Epoch 760, val loss: 0.9013534188270569
Epoch 770, training loss: 0.6341902613639832 = 0.011306991800665855 + 0.1 * 6.228832721710205
Epoch 770, val loss: 0.9060351252555847
Epoch 780, training loss: 0.6359133720397949 = 0.010930345393717289 + 0.1 * 6.24983024597168
Epoch 780, val loss: 0.9105628728866577
Epoch 790, training loss: 0.6332475543022156 = 0.01057392731308937 + 0.1 * 6.226736068725586
Epoch 790, val loss: 0.9148942232131958
Epoch 800, training loss: 0.6324962973594666 = 0.010235745459794998 + 0.1 * 6.2226057052612305
Epoch 800, val loss: 0.9193568229675293
Epoch 810, training loss: 0.6314693689346313 = 0.009914892725646496 + 0.1 * 6.215544700622559
Epoch 810, val loss: 0.9234997630119324
Epoch 820, training loss: 0.6310818791389465 = 0.00961106177419424 + 0.1 * 6.214707851409912
Epoch 820, val loss: 0.927722692489624
Epoch 830, training loss: 0.6315187811851501 = 0.009321697987616062 + 0.1 * 6.221970558166504
Epoch 830, val loss: 0.9318910241127014
Epoch 840, training loss: 0.6301546096801758 = 0.009045833721756935 + 0.1 * 6.211087703704834
Epoch 840, val loss: 0.9359151124954224
Epoch 850, training loss: 0.6302953362464905 = 0.008783092722296715 + 0.1 * 6.215122222900391
Epoch 850, val loss: 0.9399147033691406
Epoch 860, training loss: 0.6290698647499084 = 0.00853237695991993 + 0.1 * 6.205374717712402
Epoch 860, val loss: 0.943793535232544
Epoch 870, training loss: 0.6297127604484558 = 0.008293314836919308 + 0.1 * 6.214194297790527
Epoch 870, val loss: 0.947577714920044
Epoch 880, training loss: 0.6284751892089844 = 0.008065856993198395 + 0.1 * 6.204092979431152
Epoch 880, val loss: 0.9513094425201416
Epoch 890, training loss: 0.6276342868804932 = 0.007848602719604969 + 0.1 * 6.197856426239014
Epoch 890, val loss: 0.9551118016242981
Epoch 900, training loss: 0.6269232034683228 = 0.0076406290754675865 + 0.1 * 6.192825794219971
Epoch 900, val loss: 0.9587323069572449
Epoch 910, training loss: 0.6271633505821228 = 0.0074416534043848515 + 0.1 * 6.197216987609863
Epoch 910, val loss: 0.9623347520828247
Epoch 920, training loss: 0.6282109022140503 = 0.0072509413585066795 + 0.1 * 6.209599018096924
Epoch 920, val loss: 0.9657909274101257
Epoch 930, training loss: 0.6265096068382263 = 0.007068171165883541 + 0.1 * 6.1944146156311035
Epoch 930, val loss: 0.9692016839981079
Epoch 940, training loss: 0.6273157596588135 = 0.00689322454854846 + 0.1 * 6.204225540161133
Epoch 940, val loss: 0.9727057814598083
Epoch 950, training loss: 0.6260445713996887 = 0.006725456099957228 + 0.1 * 6.193191051483154
Epoch 950, val loss: 0.9758999347686768
Epoch 960, training loss: 0.6250958442687988 = 0.006564550567418337 + 0.1 * 6.185312747955322
Epoch 960, val loss: 0.9792823791503906
Epoch 970, training loss: 0.6261546015739441 = 0.006409772671759129 + 0.1 * 6.197448253631592
Epoch 970, val loss: 0.9825889468193054
Epoch 980, training loss: 0.6245072484016418 = 0.006261155474931002 + 0.1 * 6.182460784912109
Epoch 980, val loss: 0.9856915473937988
Epoch 990, training loss: 0.6237319707870483 = 0.006117859855294228 + 0.1 * 6.176140785217285
Epoch 990, val loss: 0.9888758659362793
Epoch 1000, training loss: 0.6242316365242004 = 0.005980059038847685 + 0.1 * 6.182515621185303
Epoch 1000, val loss: 0.9919119477272034
Epoch 1010, training loss: 0.6229785084724426 = 0.005847862921655178 + 0.1 * 6.171306133270264
Epoch 1010, val loss: 0.9949774742126465
Epoch 1020, training loss: 0.6239972710609436 = 0.005720179993659258 + 0.1 * 6.182770729064941
Epoch 1020, val loss: 0.9980682134628296
Epoch 1030, training loss: 0.6230112314224243 = 0.005597623065114021 + 0.1 * 6.174135684967041
Epoch 1030, val loss: 1.0008869171142578
Epoch 1040, training loss: 0.6219392418861389 = 0.005479393061250448 + 0.1 * 6.16459846496582
Epoch 1040, val loss: 1.003862738609314
Epoch 1050, training loss: 0.62226402759552 = 0.0053654820658266544 + 0.1 * 6.168985366821289
Epoch 1050, val loss: 1.006837010383606
Epoch 1060, training loss: 0.6217050552368164 = 0.005254964344203472 + 0.1 * 6.164500713348389
Epoch 1060, val loss: 1.0096246004104614
Epoch 1070, training loss: 0.6218186020851135 = 0.005148528143763542 + 0.1 * 6.16670036315918
Epoch 1070, val loss: 1.012391209602356
Epoch 1080, training loss: 0.6231553554534912 = 0.005045698024332523 + 0.1 * 6.181096076965332
Epoch 1080, val loss: 1.015056848526001
Epoch 1090, training loss: 0.6212956309318542 = 0.004946875385940075 + 0.1 * 6.163487434387207
Epoch 1090, val loss: 1.0178074836730957
Epoch 1100, training loss: 0.620812177658081 = 0.0048511214554309845 + 0.1 * 6.159610748291016
Epoch 1100, val loss: 1.020559310913086
Epoch 1110, training loss: 0.6209203600883484 = 0.00475847814232111 + 0.1 * 6.161618709564209
Epoch 1110, val loss: 1.023188829421997
Epoch 1120, training loss: 0.6202375292778015 = 0.004668783862143755 + 0.1 * 6.15568733215332
Epoch 1120, val loss: 1.0258147716522217
Epoch 1130, training loss: 0.6200534701347351 = 0.004581798333674669 + 0.1 * 6.154716491699219
Epoch 1130, val loss: 1.0283914804458618
Epoch 1140, training loss: 0.6198594570159912 = 0.004497802350670099 + 0.1 * 6.153616428375244
Epoch 1140, val loss: 1.030880093574524
Epoch 1150, training loss: 0.6197673082351685 = 0.0044163125567138195 + 0.1 * 6.153510093688965
Epoch 1150, val loss: 1.0333831310272217
Epoch 1160, training loss: 0.6202348470687866 = 0.004337668418884277 + 0.1 * 6.158971786499023
Epoch 1160, val loss: 1.0359069108963013
Epoch 1170, training loss: 0.6188787817955017 = 0.004261226858943701 + 0.1 * 6.146175384521484
Epoch 1170, val loss: 1.038318157196045
Epoch 1180, training loss: 0.6189903616905212 = 0.004187318496406078 + 0.1 * 6.148029804229736
Epoch 1180, val loss: 1.040806770324707
Epoch 1190, training loss: 0.6196091175079346 = 0.004115389194339514 + 0.1 * 6.154937267303467
Epoch 1190, val loss: 1.0431865453720093
Epoch 1200, training loss: 0.6187843084335327 = 0.004045711364597082 + 0.1 * 6.147385597229004
Epoch 1200, val loss: 1.045492172241211
Epoch 1210, training loss: 0.6178206205368042 = 0.003978126682341099 + 0.1 * 6.138424873352051
Epoch 1210, val loss: 1.0478661060333252
Epoch 1220, training loss: 0.6191375255584717 = 0.003912478219717741 + 0.1 * 6.15225076675415
Epoch 1220, val loss: 1.0502088069915771
Epoch 1230, training loss: 0.6177272200584412 = 0.0038485685363411903 + 0.1 * 6.138786315917969
Epoch 1230, val loss: 1.052471399307251
Epoch 1240, training loss: 0.6176954507827759 = 0.0037864844780415297 + 0.1 * 6.139089584350586
Epoch 1240, val loss: 1.054736614227295
Epoch 1250, training loss: 0.6180544495582581 = 0.003726237453520298 + 0.1 * 6.143281936645508
Epoch 1250, val loss: 1.0569535493850708
Epoch 1260, training loss: 0.6189460158348083 = 0.003667664248496294 + 0.1 * 6.152783393859863
Epoch 1260, val loss: 1.0591493844985962
Epoch 1270, training loss: 0.6173105835914612 = 0.0036106815095990896 + 0.1 * 6.136999130249023
Epoch 1270, val loss: 1.0612919330596924
Epoch 1280, training loss: 0.6176466345787048 = 0.003555422881618142 + 0.1 * 6.140912055969238
Epoch 1280, val loss: 1.0634886026382446
Epoch 1290, training loss: 0.6168003082275391 = 0.003501580096781254 + 0.1 * 6.132987022399902
Epoch 1290, val loss: 1.0656198263168335
Epoch 1300, training loss: 0.6167556643486023 = 0.0034491107799112797 + 0.1 * 6.133065700531006
Epoch 1300, val loss: 1.0677647590637207
Epoch 1310, training loss: 0.6163914203643799 = 0.0033980354201048613 + 0.1 * 6.1299333572387695
Epoch 1310, val loss: 1.069800853729248
Epoch 1320, training loss: 0.6164778470993042 = 0.003348281607031822 + 0.1 * 6.131295680999756
Epoch 1320, val loss: 1.0718224048614502
Epoch 1330, training loss: 0.6160059571266174 = 0.003299886593595147 + 0.1 * 6.127060413360596
Epoch 1330, val loss: 1.0738872289657593
Epoch 1340, training loss: 0.6160225868225098 = 0.0032526759896427393 + 0.1 * 6.12769889831543
Epoch 1340, val loss: 1.0759340524673462
Epoch 1350, training loss: 0.6162911653518677 = 0.0032066761050373316 + 0.1 * 6.130844593048096
Epoch 1350, val loss: 1.077955961227417
Epoch 1360, training loss: 0.6161180734634399 = 0.003161623142659664 + 0.1 * 6.1295647621154785
Epoch 1360, val loss: 1.0798410177230835
Epoch 1370, training loss: 0.6156856417655945 = 0.003117932239547372 + 0.1 * 6.125677108764648
Epoch 1370, val loss: 1.0817625522613525
Epoch 1380, training loss: 0.6151162981987 = 0.0030752515885978937 + 0.1 * 6.120410442352295
Epoch 1380, val loss: 1.0837254524230957
Epoch 1390, training loss: 0.6149670481681824 = 0.0030336203053593636 + 0.1 * 6.1193342208862305
Epoch 1390, val loss: 1.0856728553771973
Epoch 1400, training loss: 0.6159278750419617 = 0.002992972731590271 + 0.1 * 6.1293487548828125
Epoch 1400, val loss: 1.0875451564788818
Epoch 1410, training loss: 0.6150829195976257 = 0.0029531987383961678 + 0.1 * 6.121297359466553
Epoch 1410, val loss: 1.089382529258728
Epoch 1420, training loss: 0.6152080297470093 = 0.00291452556848526 + 0.1 * 6.122934818267822
Epoch 1420, val loss: 1.0912452936172485
Epoch 1430, training loss: 0.6140621304512024 = 0.0028766142204403877 + 0.1 * 6.1118550300598145
Epoch 1430, val loss: 1.093091607093811
Epoch 1440, training loss: 0.6170125603675842 = 0.002839671680703759 + 0.1 * 6.14172887802124
Epoch 1440, val loss: 1.0949691534042358
Epoch 1450, training loss: 0.614791750907898 = 0.002803511917591095 + 0.1 * 6.119882106781006
Epoch 1450, val loss: 1.0966342687606812
Epoch 1460, training loss: 0.6145433783531189 = 0.002768254606053233 + 0.1 * 6.117751121520996
Epoch 1460, val loss: 1.0984364748001099
Epoch 1470, training loss: 0.6135619878768921 = 0.002733815461397171 + 0.1 * 6.10828161239624
Epoch 1470, val loss: 1.100162148475647
Epoch 1480, training loss: 0.6140298247337341 = 0.002700082492083311 + 0.1 * 6.113296985626221
Epoch 1480, val loss: 1.101949691772461
Epoch 1490, training loss: 0.6149606108665466 = 0.002667204709723592 + 0.1 * 6.122934341430664
Epoch 1490, val loss: 1.1036726236343384
Epoch 1500, training loss: 0.6141648292541504 = 0.0026349094696342945 + 0.1 * 6.115299224853516
Epoch 1500, val loss: 1.105347990989685
Epoch 1510, training loss: 0.6139854192733765 = 0.0026034486945718527 + 0.1 * 6.113819599151611
Epoch 1510, val loss: 1.1069945096969604
Epoch 1520, training loss: 0.6134003400802612 = 0.002572630299255252 + 0.1 * 6.108276844024658
Epoch 1520, val loss: 1.1087088584899902
Epoch 1530, training loss: 0.6139580607414246 = 0.002542523667216301 + 0.1 * 6.1141557693481445
Epoch 1530, val loss: 1.1103662252426147
Epoch 1540, training loss: 0.6136887073516846 = 0.002512978855520487 + 0.1 * 6.111757278442383
Epoch 1540, val loss: 1.1120134592056274
Epoch 1550, training loss: 0.6131704449653625 = 0.0024841392878443003 + 0.1 * 6.106863021850586
Epoch 1550, val loss: 1.1135834455490112
Epoch 1560, training loss: 0.6137506365776062 = 0.0024558575823903084 + 0.1 * 6.112947940826416
Epoch 1560, val loss: 1.1151937246322632
Epoch 1570, training loss: 0.6131410598754883 = 0.002428176114335656 + 0.1 * 6.107128620147705
Epoch 1570, val loss: 1.1167290210723877
Epoch 1580, training loss: 0.6134806871414185 = 0.0024010781198740005 + 0.1 * 6.110795974731445
Epoch 1580, val loss: 1.1183242797851562
Epoch 1590, training loss: 0.6133463382720947 = 0.0023744867648929358 + 0.1 * 6.1097187995910645
Epoch 1590, val loss: 1.119848608970642
Epoch 1600, training loss: 0.6125699877738953 = 0.0023484458215534687 + 0.1 * 6.102215766906738
Epoch 1600, val loss: 1.1213696002960205
Epoch 1610, training loss: 0.6129610538482666 = 0.0023229694925248623 + 0.1 * 6.106380939483643
Epoch 1610, val loss: 1.1229445934295654
Epoch 1620, training loss: 0.6124469637870789 = 0.0022979110945016146 + 0.1 * 6.101490020751953
Epoch 1620, val loss: 1.1244289875030518
Epoch 1630, training loss: 0.6120805740356445 = 0.0022733910009264946 + 0.1 * 6.098071575164795
Epoch 1630, val loss: 1.1259106397628784
Epoch 1640, training loss: 0.6133320331573486 = 0.0022492934949696064 + 0.1 * 6.110827445983887
Epoch 1640, val loss: 1.127334713935852
Epoch 1650, training loss: 0.6120055317878723 = 0.00222580274567008 + 0.1 * 6.09779691696167
Epoch 1650, val loss: 1.128813624382019
Epoch 1660, training loss: 0.6125625371932983 = 0.0022027534432709217 + 0.1 * 6.103597640991211
Epoch 1660, val loss: 1.1303437948226929
Epoch 1670, training loss: 0.6123803853988647 = 0.002180109964683652 + 0.1 * 6.1020026206970215
Epoch 1670, val loss: 1.1318038702011108
Epoch 1680, training loss: 0.6123678088188171 = 0.002157901180908084 + 0.1 * 6.10209846496582
Epoch 1680, val loss: 1.13322913646698
Epoch 1690, training loss: 0.6115306615829468 = 0.002136084483936429 + 0.1 * 6.093945503234863
Epoch 1690, val loss: 1.134645938873291
Epoch 1700, training loss: 0.6123495697975159 = 0.0021146335639059544 + 0.1 * 6.102349281311035
Epoch 1700, val loss: 1.136063575744629
Epoch 1710, training loss: 0.6120239496231079 = 0.00209366949275136 + 0.1 * 6.099302768707275
Epoch 1710, val loss: 1.1374719142913818
Epoch 1720, training loss: 0.6121397018432617 = 0.002073021372780204 + 0.1 * 6.100666522979736
Epoch 1720, val loss: 1.1388397216796875
Epoch 1730, training loss: 0.6118853688240051 = 0.0020527730230242014 + 0.1 * 6.098325729370117
Epoch 1730, val loss: 1.1402614116668701
Epoch 1740, training loss: 0.6114435195922852 = 0.0020328445825725794 + 0.1 * 6.094106674194336
Epoch 1740, val loss: 1.141513705253601
Epoch 1750, training loss: 0.6112014055252075 = 0.0020134123042225838 + 0.1 * 6.091879844665527
Epoch 1750, val loss: 1.1428718566894531
Epoch 1760, training loss: 0.6114107370376587 = 0.001994305057451129 + 0.1 * 6.0941643714904785
Epoch 1760, val loss: 1.1442632675170898
Epoch 1770, training loss: 0.6113041043281555 = 0.0019755209796130657 + 0.1 * 6.09328556060791
Epoch 1770, val loss: 1.1456218957901
Epoch 1780, training loss: 0.6111586689949036 = 0.001957043306902051 + 0.1 * 6.092015743255615
Epoch 1780, val loss: 1.1469590663909912
Epoch 1790, training loss: 0.6104568243026733 = 0.0019388236105442047 + 0.1 * 6.085179805755615
Epoch 1790, val loss: 1.1482995748519897
Epoch 1800, training loss: 0.6110802292823792 = 0.00192092580255121 + 0.1 * 6.091592788696289
Epoch 1800, val loss: 1.14958918094635
Epoch 1810, training loss: 0.6114607453346252 = 0.0019033466232940555 + 0.1 * 6.095573902130127
Epoch 1810, val loss: 1.15082585811615
Epoch 1820, training loss: 0.6108287572860718 = 0.001886095735244453 + 0.1 * 6.089426517486572
Epoch 1820, val loss: 1.1520309448242188
Epoch 1830, training loss: 0.6101992130279541 = 0.0018691355362534523 + 0.1 * 6.083300590515137
Epoch 1830, val loss: 1.153326392173767
Epoch 1840, training loss: 0.6109020113945007 = 0.001852494664490223 + 0.1 * 6.0904951095581055
Epoch 1840, val loss: 1.154683232307434
Epoch 1850, training loss: 0.610998809337616 = 0.0018361201509833336 + 0.1 * 6.0916266441345215
Epoch 1850, val loss: 1.155922293663025
Epoch 1860, training loss: 0.6104162335395813 = 0.001819980563595891 + 0.1 * 6.085962772369385
Epoch 1860, val loss: 1.157080888748169
Epoch 1870, training loss: 0.6101411581039429 = 0.0018041428411379457 + 0.1 * 6.083370208740234
Epoch 1870, val loss: 1.1582838296890259
Epoch 1880, training loss: 0.6113292574882507 = 0.0017886575078591704 + 0.1 * 6.095405578613281
Epoch 1880, val loss: 1.1595466136932373
Epoch 1890, training loss: 0.6101340651512146 = 0.0017732928972691298 + 0.1 * 6.0836076736450195
Epoch 1890, val loss: 1.1607319116592407
Epoch 1900, training loss: 0.6102774739265442 = 0.0017582694999873638 + 0.1 * 6.08519172668457
Epoch 1900, val loss: 1.161954641342163
Epoch 1910, training loss: 0.6101427674293518 = 0.001743418863043189 + 0.1 * 6.083993434906006
Epoch 1910, val loss: 1.1631605625152588
Epoch 1920, training loss: 0.6102590560913086 = 0.0017288683447986841 + 0.1 * 6.085301399230957
Epoch 1920, val loss: 1.1643726825714111
Epoch 1930, training loss: 0.6094159483909607 = 0.0017144812736660242 + 0.1 * 6.077014446258545
Epoch 1930, val loss: 1.1655153036117554
Epoch 1940, training loss: 0.6115195155143738 = 0.0017003826797008514 + 0.1 * 6.098190784454346
Epoch 1940, val loss: 1.166666030883789
Epoch 1950, training loss: 0.6094651818275452 = 0.001686417730525136 + 0.1 * 6.07778787612915
Epoch 1950, val loss: 1.167738676071167
Epoch 1960, training loss: 0.6096357703208923 = 0.0016727956244722009 + 0.1 * 6.079629421234131
Epoch 1960, val loss: 1.1689269542694092
Epoch 1970, training loss: 0.6101030111312866 = 0.0016593339387327433 + 0.1 * 6.084436893463135
Epoch 1970, val loss: 1.170084834098816
Epoch 1980, training loss: 0.6099905967712402 = 0.0016460915794596076 + 0.1 * 6.083445072174072
Epoch 1980, val loss: 1.1712392568588257
Epoch 1990, training loss: 0.6087378859519958 = 0.0016330054495483637 + 0.1 * 6.071048736572266
Epoch 1990, val loss: 1.1723161935806274
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8593
Overall ASR: 0.3100
Flip ASR: 0.2311/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.787559747695923 = 1.9502793550491333 + 0.1 * 8.372804641723633
Epoch 0, val loss: 1.9453009366989136
Epoch 10, training loss: 2.773746967315674 = 1.936964750289917 + 0.1 * 8.367822647094727
Epoch 10, val loss: 1.9283913373947144
Epoch 20, training loss: 2.757134437561035 = 1.92097806930542 + 0.1 * 8.361564636230469
Epoch 20, val loss: 1.9068058729171753
Epoch 30, training loss: 2.7384538650512695 = 1.903002381324768 + 0.1 * 8.354515075683594
Epoch 30, val loss: 1.8846579790115356
Epoch 40, training loss: 2.713898181915283 = 1.882559895515442 + 0.1 * 8.313384056091309
Epoch 40, val loss: 1.8632657527923584
Epoch 50, training loss: 2.6680400371551514 = 1.855522871017456 + 0.1 * 8.125170707702637
Epoch 50, val loss: 1.8374292850494385
Epoch 60, training loss: 2.5986766815185547 = 1.824155330657959 + 0.1 * 7.745213985443115
Epoch 60, val loss: 1.8090531826019287
Epoch 70, training loss: 2.5339293479919434 = 1.7888190746307373 + 0.1 * 7.451103210449219
Epoch 70, val loss: 1.7779916524887085
Epoch 80, training loss: 2.4643404483795166 = 1.7505117654800415 + 0.1 * 7.13828706741333
Epoch 80, val loss: 1.7445951700210571
Epoch 90, training loss: 2.4039344787597656 = 1.707228422164917 + 0.1 * 6.967059135437012
Epoch 90, val loss: 1.7077255249023438
Epoch 100, training loss: 2.341034173965454 = 1.653109073638916 + 0.1 * 6.879251480102539
Epoch 100, val loss: 1.6614055633544922
Epoch 110, training loss: 2.264256477355957 = 1.5816009044647217 + 0.1 * 6.826554775238037
Epoch 110, val loss: 1.6013309955596924
Epoch 120, training loss: 2.1697592735290527 = 1.491142749786377 + 0.1 * 6.786165237426758
Epoch 120, val loss: 1.5279661417007446
Epoch 130, training loss: 2.062154769897461 = 1.387162685394287 + 0.1 * 6.749919414520264
Epoch 130, val loss: 1.4463688135147095
Epoch 140, training loss: 1.950150966644287 = 1.2784688472747803 + 0.1 * 6.716821193695068
Epoch 140, val loss: 1.363797903060913
Epoch 150, training loss: 1.8418761491775513 = 1.173386573791504 + 0.1 * 6.6848955154418945
Epoch 150, val loss: 1.2864984273910522
Epoch 160, training loss: 1.7430102825164795 = 1.0775120258331299 + 0.1 * 6.65498161315918
Epoch 160, val loss: 1.2178484201431274
Epoch 170, training loss: 1.656858205795288 = 0.9936571717262268 + 0.1 * 6.632009506225586
Epoch 170, val loss: 1.1590269804000854
Epoch 180, training loss: 1.582533359527588 = 0.9217945337295532 + 0.1 * 6.607387542724609
Epoch 180, val loss: 1.109588384628296
Epoch 190, training loss: 1.515994668006897 = 0.8572975397109985 + 0.1 * 6.586971282958984
Epoch 190, val loss: 1.065417766571045
Epoch 200, training loss: 1.4535455703735352 = 0.7967802882194519 + 0.1 * 6.567653179168701
Epoch 200, val loss: 1.0239102840423584
Epoch 210, training loss: 1.3939814567565918 = 0.7387775182723999 + 0.1 * 6.552040100097656
Epoch 210, val loss: 0.9841029644012451
Epoch 220, training loss: 1.3377764225006104 = 0.6835618615150452 + 0.1 * 6.542144775390625
Epoch 220, val loss: 0.9473342299461365
Epoch 230, training loss: 1.2858072519302368 = 0.6324330568313599 + 0.1 * 6.5337419509887695
Epoch 230, val loss: 0.915131688117981
Epoch 240, training loss: 1.2379112243652344 = 0.5852833986282349 + 0.1 * 6.526278972625732
Epoch 240, val loss: 0.8878078460693359
Epoch 250, training loss: 1.1934611797332764 = 0.5416107773780823 + 0.1 * 6.518503665924072
Epoch 250, val loss: 0.8652248978614807
Epoch 260, training loss: 1.1522860527038574 = 0.5008251070976257 + 0.1 * 6.514608860015869
Epoch 260, val loss: 0.846705973148346
Epoch 270, training loss: 1.1135281324386597 = 0.46247056126594543 + 0.1 * 6.510575771331787
Epoch 270, val loss: 0.8312012553215027
Epoch 280, training loss: 1.0763859748840332 = 0.4261494278907776 + 0.1 * 6.502365589141846
Epoch 280, val loss: 0.8181400299072266
Epoch 290, training loss: 1.040958285331726 = 0.3914576768875122 + 0.1 * 6.495006084442139
Epoch 290, val loss: 0.8064201474189758
Epoch 300, training loss: 1.0073585510253906 = 0.35834041237831116 + 0.1 * 6.490180969238281
Epoch 300, val loss: 0.7954924702644348
Epoch 310, training loss: 0.974845290184021 = 0.32679232954978943 + 0.1 * 6.480529308319092
Epoch 310, val loss: 0.7856443524360657
Epoch 320, training loss: 0.9442192316055298 = 0.2966900169849396 + 0.1 * 6.475291728973389
Epoch 320, val loss: 0.7763041853904724
Epoch 330, training loss: 0.9149307608604431 = 0.26811498403549194 + 0.1 * 6.468157768249512
Epoch 330, val loss: 0.7677996754646301
Epoch 340, training loss: 0.8886827230453491 = 0.24122007191181183 + 0.1 * 6.474626541137695
Epoch 340, val loss: 0.7602830529212952
Epoch 350, training loss: 0.8622502088546753 = 0.21633538603782654 + 0.1 * 6.459147930145264
Epoch 350, val loss: 0.7540886402130127
Epoch 360, training loss: 0.8382453918457031 = 0.19348514080047607 + 0.1 * 6.447602272033691
Epoch 360, val loss: 0.7493170499801636
Epoch 370, training loss: 0.8183202743530273 = 0.1727781891822815 + 0.1 * 6.455420970916748
Epoch 370, val loss: 0.746084988117218
Epoch 380, training loss: 0.797929584980011 = 0.1543605923652649 + 0.1 * 6.435689926147461
Epoch 380, val loss: 0.7443470358848572
Epoch 390, training loss: 0.7806466817855835 = 0.1380646973848343 + 0.1 * 6.4258198738098145
Epoch 390, val loss: 0.7440773844718933
Epoch 400, training loss: 0.7664644718170166 = 0.1237543374300003 + 0.1 * 6.4271016120910645
Epoch 400, val loss: 0.7450884580612183
Epoch 410, training loss: 0.7526212930679321 = 0.11130765080451965 + 0.1 * 6.413136005401611
Epoch 410, val loss: 0.7472814321517944
Epoch 420, training loss: 0.7402676343917847 = 0.10039088129997253 + 0.1 * 6.398767948150635
Epoch 420, val loss: 0.7503383755683899
Epoch 430, training loss: 0.7318854928016663 = 0.09082090109586716 + 0.1 * 6.410645961761475
Epoch 430, val loss: 0.7542665004730225
Epoch 440, training loss: 0.7219808101654053 = 0.08247893303632736 + 0.1 * 6.395018577575684
Epoch 440, val loss: 0.7587101459503174
Epoch 450, training loss: 0.7126802206039429 = 0.07514405250549316 + 0.1 * 6.375361442565918
Epoch 450, val loss: 0.7636322379112244
Epoch 460, training loss: 0.708724319934845 = 0.06866301596164703 + 0.1 * 6.400613307952881
Epoch 460, val loss: 0.7689039707183838
Epoch 470, training loss: 0.6991267204284668 = 0.06295827031135559 + 0.1 * 6.361684799194336
Epoch 470, val loss: 0.7745149731636047
Epoch 480, training loss: 0.6934152245521545 = 0.05789683386683464 + 0.1 * 6.3551836013793945
Epoch 480, val loss: 0.7803725004196167
Epoch 490, training loss: 0.6877522468566895 = 0.053377121686935425 + 0.1 * 6.343750953674316
Epoch 490, val loss: 0.7864418029785156
Epoch 500, training loss: 0.6856542229652405 = 0.04933463782072067 + 0.1 * 6.363195896148682
Epoch 500, val loss: 0.7926090359687805
Epoch 510, training loss: 0.6790006756782532 = 0.04572929069399834 + 0.1 * 6.332713603973389
Epoch 510, val loss: 0.7988311648368835
Epoch 520, training loss: 0.676555335521698 = 0.042496927082538605 + 0.1 * 6.3405842781066895
Epoch 520, val loss: 0.8052204847335815
Epoch 530, training loss: 0.6718661189079285 = 0.0395880788564682 + 0.1 * 6.322780132293701
Epoch 530, val loss: 0.8114915490150452
Epoch 540, training loss: 0.6678068041801453 = 0.036954037845134735 + 0.1 * 6.30852746963501
Epoch 540, val loss: 0.8179061412811279
Epoch 550, training loss: 0.666206955909729 = 0.0345514640212059 + 0.1 * 6.316555023193359
Epoch 550, val loss: 0.824188232421875
Epoch 560, training loss: 0.6644122004508972 = 0.032353147864341736 + 0.1 * 6.320590496063232
Epoch 560, val loss: 0.8304311037063599
Epoch 570, training loss: 0.6600123047828674 = 0.030382271856069565 + 0.1 * 6.296299934387207
Epoch 570, val loss: 0.8367617726325989
Epoch 580, training loss: 0.6583713293075562 = 0.028593186289072037 + 0.1 * 6.297781467437744
Epoch 580, val loss: 0.8430889844894409
Epoch 590, training loss: 0.6555101871490479 = 0.026956431567668915 + 0.1 * 6.285537242889404
Epoch 590, val loss: 0.849259614944458
Epoch 600, training loss: 0.656308650970459 = 0.025453122332692146 + 0.1 * 6.308554649353027
Epoch 600, val loss: 0.8554809093475342
Epoch 610, training loss: 0.6522214412689209 = 0.024075640365481377 + 0.1 * 6.281457901000977
Epoch 610, val loss: 0.8615614771842957
Epoch 620, training loss: 0.6516408324241638 = 0.022804662585258484 + 0.1 * 6.288361549377441
Epoch 620, val loss: 0.8676029443740845
Epoch 630, training loss: 0.6497231125831604 = 0.021630963310599327 + 0.1 * 6.280921459197998
Epoch 630, val loss: 0.8734002709388733
Epoch 640, training loss: 0.6473721861839294 = 0.0205510426312685 + 0.1 * 6.268211364746094
Epoch 640, val loss: 0.8792936205863953
Epoch 650, training loss: 0.6484918594360352 = 0.019549019634723663 + 0.1 * 6.289428234100342
Epoch 650, val loss: 0.8850561380386353
Epoch 660, training loss: 0.6446806788444519 = 0.018627755343914032 + 0.1 * 6.260529041290283
Epoch 660, val loss: 0.8906294703483582
Epoch 670, training loss: 0.6447060704231262 = 0.017770851030945778 + 0.1 * 6.269352436065674
Epoch 670, val loss: 0.8961588144302368
Epoch 680, training loss: 0.6425151228904724 = 0.016972891986370087 + 0.1 * 6.255422115325928
Epoch 680, val loss: 0.9015841484069824
Epoch 690, training loss: 0.6419016122817993 = 0.016230508685112 + 0.1 * 6.256710529327393
Epoch 690, val loss: 0.9069714546203613
Epoch 700, training loss: 0.6417249441146851 = 0.015536858700215816 + 0.1 * 6.261880874633789
Epoch 700, val loss: 0.9122694134712219
Epoch 710, training loss: 0.6395041346549988 = 0.014889680780470371 + 0.1 * 6.246144771575928
Epoch 710, val loss: 0.9174489378929138
Epoch 720, training loss: 0.6401408314704895 = 0.014283424243330956 + 0.1 * 6.25857400894165
Epoch 720, val loss: 0.9225789904594421
Epoch 730, training loss: 0.6394012570381165 = 0.013717505149543285 + 0.1 * 6.256837368011475
Epoch 730, val loss: 0.9275225400924683
Epoch 740, training loss: 0.6378949284553528 = 0.013186358846724033 + 0.1 * 6.247086048126221
Epoch 740, val loss: 0.9323843121528625
Epoch 750, training loss: 0.6367286443710327 = 0.012688124552369118 + 0.1 * 6.240405559539795
Epoch 750, val loss: 0.9372060894966125
Epoch 760, training loss: 0.636855959892273 = 0.012218734249472618 + 0.1 * 6.246372222900391
Epoch 760, val loss: 0.9418771862983704
Epoch 770, training loss: 0.6356106400489807 = 0.011778144165873528 + 0.1 * 6.238325119018555
Epoch 770, val loss: 0.9465692639350891
Epoch 780, training loss: 0.6346141695976257 = 0.011362012475728989 + 0.1 * 6.2325215339660645
Epoch 780, val loss: 0.9511274695396423
Epoch 790, training loss: 0.6344958543777466 = 0.01096836756914854 + 0.1 * 6.235274791717529
Epoch 790, val loss: 0.9556729793548584
Epoch 800, training loss: 0.6353157758712769 = 0.01059629675000906 + 0.1 * 6.247194766998291
Epoch 800, val loss: 0.9600425362586975
Epoch 810, training loss: 0.6332864761352539 = 0.010247896425426006 + 0.1 * 6.230385780334473
Epoch 810, val loss: 0.9643626809120178
Epoch 820, training loss: 0.6317538619041443 = 0.009916568174958229 + 0.1 * 6.2183732986450195
Epoch 820, val loss: 0.9686592221260071
Epoch 830, training loss: 0.6318187117576599 = 0.009601888246834278 + 0.1 * 6.22216796875
Epoch 830, val loss: 0.9728595018386841
Epoch 840, training loss: 0.6322819590568542 = 0.009303098544478416 + 0.1 * 6.229788780212402
Epoch 840, val loss: 0.9769708514213562
Epoch 850, training loss: 0.6313349604606628 = 0.009020054712891579 + 0.1 * 6.223149299621582
Epoch 850, val loss: 0.9809634685516357
Epoch 860, training loss: 0.6314380168914795 = 0.008750430308282375 + 0.1 * 6.2268757820129395
Epoch 860, val loss: 0.985005795955658
Epoch 870, training loss: 0.6292412281036377 = 0.008494393900036812 + 0.1 * 6.207468509674072
Epoch 870, val loss: 0.9888561964035034
Epoch 880, training loss: 0.6293064951896667 = 0.008250073529779911 + 0.1 * 6.210564136505127
Epoch 880, val loss: 0.9927029013633728
Epoch 890, training loss: 0.6298400163650513 = 0.00801740400493145 + 0.1 * 6.218225955963135
Epoch 890, val loss: 0.996475100517273
Epoch 900, training loss: 0.6276653409004211 = 0.007796366233378649 + 0.1 * 6.198689937591553
Epoch 900, val loss: 1.0001802444458008
Epoch 910, training loss: 0.6277610063552856 = 0.00758537370711565 + 0.1 * 6.201756477355957
Epoch 910, val loss: 1.003818154335022
Epoch 920, training loss: 0.6284130811691284 = 0.007381289731711149 + 0.1 * 6.210318088531494
Epoch 920, val loss: 1.007398009300232
Epoch 930, training loss: 0.6276540756225586 = 0.0071881674230098724 + 0.1 * 6.2046589851379395
Epoch 930, val loss: 1.0108888149261475
Epoch 940, training loss: 0.6268002390861511 = 0.007003791630268097 + 0.1 * 6.197964668273926
Epoch 940, val loss: 1.0143858194351196
Epoch 950, training loss: 0.6261427998542786 = 0.006827049423009157 + 0.1 * 6.193157196044922
Epoch 950, val loss: 1.0177888870239258
Epoch 960, training loss: 0.6272130012512207 = 0.006657225079834461 + 0.1 * 6.205557346343994
Epoch 960, val loss: 1.0211760997772217
Epoch 970, training loss: 0.6261319518089294 = 0.0064949640072882175 + 0.1 * 6.1963701248168945
Epoch 970, val loss: 1.0244269371032715
Epoch 980, training loss: 0.6254057884216309 = 0.006338880397379398 + 0.1 * 6.190669059753418
Epoch 980, val loss: 1.0276910066604614
Epoch 990, training loss: 0.6259862184524536 = 0.0061889891512691975 + 0.1 * 6.197972297668457
Epoch 990, val loss: 1.0308935642242432
Epoch 1000, training loss: 0.6243875026702881 = 0.00604524789378047 + 0.1 * 6.183422565460205
Epoch 1000, val loss: 1.0340211391448975
Epoch 1010, training loss: 0.6253951787948608 = 0.00590697443112731 + 0.1 * 6.194881916046143
Epoch 1010, val loss: 1.037107229232788
Epoch 1020, training loss: 0.623370885848999 = 0.005774448625743389 + 0.1 * 6.17596435546875
Epoch 1020, val loss: 1.0402185916900635
Epoch 1030, training loss: 0.6247323751449585 = 0.005646646022796631 + 0.1 * 6.19085693359375
Epoch 1030, val loss: 1.043270468711853
Epoch 1040, training loss: 0.6231788396835327 = 0.005523971747606993 + 0.1 * 6.176548480987549
Epoch 1040, val loss: 1.0462065935134888
Epoch 1050, training loss: 0.6231401562690735 = 0.005405558738857508 + 0.1 * 6.1773457527160645
Epoch 1050, val loss: 1.0492198467254639
Epoch 1060, training loss: 0.6218874454498291 = 0.005290314555168152 + 0.1 * 6.165971279144287
Epoch 1060, val loss: 1.052101731300354
Epoch 1070, training loss: 0.6226463317871094 = 0.005179026164114475 + 0.1 * 6.174672603607178
Epoch 1070, val loss: 1.0549520254135132
Epoch 1080, training loss: 0.6220853328704834 = 0.005070255137979984 + 0.1 * 6.170150279998779
Epoch 1080, val loss: 1.0577366352081299
Epoch 1090, training loss: 0.6223125457763672 = 0.004964698571711779 + 0.1 * 6.173478126525879
Epoch 1090, val loss: 1.0605158805847168
Epoch 1100, training loss: 0.6210362911224365 = 0.004865894094109535 + 0.1 * 6.161704063415527
Epoch 1100, val loss: 1.0632928609848022
Epoch 1110, training loss: 0.6213755011558533 = 0.004770747385919094 + 0.1 * 6.166047096252441
Epoch 1110, val loss: 1.066066861152649
Epoch 1120, training loss: 0.6208235025405884 = 0.004678737837821245 + 0.1 * 6.161447525024414
Epoch 1120, val loss: 1.068764090538025
Epoch 1130, training loss: 0.6208892464637756 = 0.0045899925753474236 + 0.1 * 6.162992477416992
Epoch 1130, val loss: 1.0714298486709595
Epoch 1140, training loss: 0.6214120388031006 = 0.004503682721406221 + 0.1 * 6.169083595275879
Epoch 1140, val loss: 1.0740638971328735
Epoch 1150, training loss: 0.6204689145088196 = 0.00441998103633523 + 0.1 * 6.160489082336426
Epoch 1150, val loss: 1.0767408609390259
Epoch 1160, training loss: 0.6198374032974243 = 0.004338869825005531 + 0.1 * 6.154985427856445
Epoch 1160, val loss: 1.0792521238327026
Epoch 1170, training loss: 0.6187976598739624 = 0.004259802866727114 + 0.1 * 6.145378112792969
Epoch 1170, val loss: 1.0818060636520386
Epoch 1180, training loss: 0.6209927797317505 = 0.004183007869869471 + 0.1 * 6.168097496032715
Epoch 1180, val loss: 1.0843358039855957
Epoch 1190, training loss: 0.6193379759788513 = 0.004108664579689503 + 0.1 * 6.152292728424072
Epoch 1190, val loss: 1.0868072509765625
Epoch 1200, training loss: 0.6210902333259583 = 0.004037243314087391 + 0.1 * 6.170529365539551
Epoch 1200, val loss: 1.0892994403839111
Epoch 1210, training loss: 0.6185585856437683 = 0.003968454897403717 + 0.1 * 6.145900726318359
Epoch 1210, val loss: 1.091708779335022
Epoch 1220, training loss: 0.618204653263092 = 0.0039018034003674984 + 0.1 * 6.143028736114502
Epoch 1220, val loss: 1.0941694974899292
Epoch 1230, training loss: 0.6184262633323669 = 0.003836957970634103 + 0.1 * 6.145893096923828
Epoch 1230, val loss: 1.0966202020645142
Epoch 1240, training loss: 0.6179376840591431 = 0.0037740773987025023 + 0.1 * 6.141635894775391
Epoch 1240, val loss: 1.0990115404129028
Epoch 1250, training loss: 0.6181122064590454 = 0.0037127358373254538 + 0.1 * 6.1439948081970215
Epoch 1250, val loss: 1.1013586521148682
Epoch 1260, training loss: 0.6201120018959045 = 0.0036530836950987577 + 0.1 * 6.164588928222656
Epoch 1260, val loss: 1.1037027835845947
Epoch 1270, training loss: 0.6184217929840088 = 0.003595385467633605 + 0.1 * 6.148263931274414
Epoch 1270, val loss: 1.105919361114502
Epoch 1280, training loss: 0.617633581161499 = 0.0035393827129155397 + 0.1 * 6.140942096710205
Epoch 1280, val loss: 1.1081929206848145
Epoch 1290, training loss: 0.6170293688774109 = 0.00348484655842185 + 0.1 * 6.1354451179504395
Epoch 1290, val loss: 1.1104485988616943
Epoch 1300, training loss: 0.617032527923584 = 0.003431669669225812 + 0.1 * 6.136008262634277
Epoch 1300, val loss: 1.1126683950424194
Epoch 1310, training loss: 0.6176836490631104 = 0.0033799621742218733 + 0.1 * 6.143036842346191
Epoch 1310, val loss: 1.1147993803024292
Epoch 1320, training loss: 0.6166813969612122 = 0.003329739673063159 + 0.1 * 6.133516311645508
Epoch 1320, val loss: 1.1169836521148682
Epoch 1330, training loss: 0.6167895793914795 = 0.0032808585092425346 + 0.1 * 6.135087013244629
Epoch 1330, val loss: 1.1191412210464478
Epoch 1340, training loss: 0.6168071627616882 = 0.003233046503737569 + 0.1 * 6.135740756988525
Epoch 1340, val loss: 1.1212046146392822
Epoch 1350, training loss: 0.6169635057449341 = 0.003186645917594433 + 0.1 * 6.137768268585205
Epoch 1350, val loss: 1.1232801675796509
Epoch 1360, training loss: 0.6170750856399536 = 0.003141299821436405 + 0.1 * 6.13933801651001
Epoch 1360, val loss: 1.1253756284713745
Epoch 1370, training loss: 0.616199791431427 = 0.003097205888479948 + 0.1 * 6.131025791168213
Epoch 1370, val loss: 1.1274546384811401
Epoch 1380, training loss: 0.6160171627998352 = 0.0030541792511940002 + 0.1 * 6.129629611968994
Epoch 1380, val loss: 1.1294548511505127
Epoch 1390, training loss: 0.6156267523765564 = 0.003012270200997591 + 0.1 * 6.1261444091796875
Epoch 1390, val loss: 1.131462574005127
Epoch 1400, training loss: 0.6162369251251221 = 0.0029713509138673544 + 0.1 * 6.132655620574951
Epoch 1400, val loss: 1.1334865093231201
Epoch 1410, training loss: 0.6152531504631042 = 0.0029313552659004927 + 0.1 * 6.123217582702637
Epoch 1410, val loss: 1.135453701019287
Epoch 1420, training loss: 0.6168670654296875 = 0.002892208518460393 + 0.1 * 6.139748573303223
Epoch 1420, val loss: 1.1373628377914429
Epoch 1430, training loss: 0.6161139011383057 = 0.0028543167281895876 + 0.1 * 6.132596015930176
Epoch 1430, val loss: 1.1392990350723267
Epoch 1440, training loss: 0.6150736212730408 = 0.0028171990998089314 + 0.1 * 6.12256383895874
Epoch 1440, val loss: 1.1411705017089844
Epoch 1450, training loss: 0.614287793636322 = 0.0027809510938823223 + 0.1 * 6.115067958831787
Epoch 1450, val loss: 1.1430938243865967
Epoch 1460, training loss: 0.6157169938087463 = 0.002745478181168437 + 0.1 * 6.129715442657471
Epoch 1460, val loss: 1.145027995109558
Epoch 1470, training loss: 0.6152119636535645 = 0.0027108201757073402 + 0.1 * 6.125011444091797
Epoch 1470, val loss: 1.146824598312378
Epoch 1480, training loss: 0.6148329377174377 = 0.0026772834826260805 + 0.1 * 6.121556758880615
Epoch 1480, val loss: 1.1486696004867554
Epoch 1490, training loss: 0.6155693531036377 = 0.0026443758979439735 + 0.1 * 6.129249572753906
Epoch 1490, val loss: 1.150415062904358
Epoch 1500, training loss: 0.6141857504844666 = 0.0026120739057660103 + 0.1 * 6.115736961364746
Epoch 1500, val loss: 1.1522009372711182
Epoch 1510, training loss: 0.6149311065673828 = 0.0025805390905588865 + 0.1 * 6.123505592346191
Epoch 1510, val loss: 1.1539744138717651
Epoch 1520, training loss: 0.6156036853790283 = 0.0025497216265648603 + 0.1 * 6.130539417266846
Epoch 1520, val loss: 1.155756950378418
Epoch 1530, training loss: 0.6136541366577148 = 0.0025194084737449884 + 0.1 * 6.11134672164917
Epoch 1530, val loss: 1.157468557357788
Epoch 1540, training loss: 0.61378014087677 = 0.002489890204742551 + 0.1 * 6.112902641296387
Epoch 1540, val loss: 1.1592034101486206
Epoch 1550, training loss: 0.6147369742393494 = 0.0024609642568975687 + 0.1 * 6.12276029586792
Epoch 1550, val loss: 1.16091787815094
Epoch 1560, training loss: 0.6154734492301941 = 0.0024327472783625126 + 0.1 * 6.130406856536865
Epoch 1560, val loss: 1.1625339984893799
Epoch 1570, training loss: 0.6130370497703552 = 0.0024050860665738583 + 0.1 * 6.106319427490234
Epoch 1570, val loss: 1.164262056350708
Epoch 1580, training loss: 0.6144737005233765 = 0.0023779261391609907 + 0.1 * 6.120957374572754
Epoch 1580, val loss: 1.1659168004989624
Epoch 1590, training loss: 0.6138430833816528 = 0.002351424191147089 + 0.1 * 6.114916801452637
Epoch 1590, val loss: 1.1675207614898682
Epoch 1600, training loss: 0.6136347651481628 = 0.0023255441337823868 + 0.1 * 6.113091945648193
Epoch 1600, val loss: 1.1691757440567017
Epoch 1610, training loss: 0.6131865382194519 = 0.0023001711815595627 + 0.1 * 6.108863830566406
Epoch 1610, val loss: 1.1707645654678345
Epoch 1620, training loss: 0.6139988899230957 = 0.0022752019576728344 + 0.1 * 6.117236614227295
Epoch 1620, val loss: 1.1723347902297974
Epoch 1630, training loss: 0.6121939420700073 = 0.0022508397232741117 + 0.1 * 6.099431037902832
Epoch 1630, val loss: 1.173928141593933
Epoch 1640, training loss: 0.6147752404212952 = 0.002226935001090169 + 0.1 * 6.12548303604126
Epoch 1640, val loss: 1.175513505935669
Epoch 1650, training loss: 0.6130507588386536 = 0.0022036507725715637 + 0.1 * 6.108470916748047
Epoch 1650, val loss: 1.177013874053955
Epoch 1660, training loss: 0.6122681498527527 = 0.002180709969252348 + 0.1 * 6.100874423980713
Epoch 1660, val loss: 1.1785844564437866
Epoch 1670, training loss: 0.6136403679847717 = 0.0021582641638815403 + 0.1 * 6.11482048034668
Epoch 1670, val loss: 1.1801363229751587
Epoch 1680, training loss: 0.612125813961029 = 0.0021360772661864758 + 0.1 * 6.099897384643555
Epoch 1680, val loss: 1.181579351425171
Epoch 1690, training loss: 0.6143151521682739 = 0.0021144922357052565 + 0.1 * 6.122006893157959
Epoch 1690, val loss: 1.183062195777893
Epoch 1700, training loss: 0.612027645111084 = 0.002093320479616523 + 0.1 * 6.099343299865723
Epoch 1700, val loss: 1.1845539808273315
Epoch 1710, training loss: 0.612653911113739 = 0.0020726921502500772 + 0.1 * 6.105812072753906
Epoch 1710, val loss: 1.186011552810669
Epoch 1720, training loss: 0.611958384513855 = 0.002052189316600561 + 0.1 * 6.099061965942383
Epoch 1720, val loss: 1.1874818801879883
Epoch 1730, training loss: 0.6116728186607361 = 0.002032063202932477 + 0.1 * 6.096407890319824
Epoch 1730, val loss: 1.1889288425445557
Epoch 1740, training loss: 0.6140527725219727 = 0.0020123098511248827 + 0.1 * 6.120404243469238
Epoch 1740, val loss: 1.1903725862503052
Epoch 1750, training loss: 0.6122663021087646 = 0.0019930368289351463 + 0.1 * 6.102732181549072
Epoch 1750, val loss: 1.1917887926101685
Epoch 1760, training loss: 0.6118280291557312 = 0.001974158687517047 + 0.1 * 6.098538398742676
Epoch 1760, val loss: 1.1931922435760498
Epoch 1770, training loss: 0.612098753452301 = 0.0019556055776774883 + 0.1 * 6.101430892944336
Epoch 1770, val loss: 1.1945866346359253
Epoch 1780, training loss: 0.6114783883094788 = 0.001937368419021368 + 0.1 * 6.095409870147705
Epoch 1780, val loss: 1.195996880531311
Epoch 1790, training loss: 0.6118948459625244 = 0.0019194097258150578 + 0.1 * 6.099754333496094
Epoch 1790, val loss: 1.1973844766616821
Epoch 1800, training loss: 0.6137250065803528 = 0.0019017924787476659 + 0.1 * 6.118231773376465
Epoch 1800, val loss: 1.198689579963684
Epoch 1810, training loss: 0.6116343140602112 = 0.0018840940902009606 + 0.1 * 6.097501754760742
Epoch 1810, val loss: 1.200069785118103
Epoch 1820, training loss: 0.6110897660255432 = 0.0018663923256099224 + 0.1 * 6.092233657836914
Epoch 1820, val loss: 1.2014378309249878
Epoch 1830, training loss: 0.6115866899490356 = 0.0018491672817617655 + 0.1 * 6.09737491607666
Epoch 1830, val loss: 1.202776312828064
Epoch 1840, training loss: 0.6121336817741394 = 0.00183273502625525 + 0.1 * 6.1030097007751465
Epoch 1840, val loss: 1.2041178941726685
Epoch 1850, training loss: 0.6108723282814026 = 0.0018166139489039779 + 0.1 * 6.090556621551514
Epoch 1850, val loss: 1.2054227590560913
Epoch 1860, training loss: 0.6106875538825989 = 0.0018007445614784956 + 0.1 * 6.088868141174316
Epoch 1860, val loss: 1.2067114114761353
Epoch 1870, training loss: 0.6118496060371399 = 0.0017851407174021006 + 0.1 * 6.100644588470459
Epoch 1870, val loss: 1.208025336265564
Epoch 1880, training loss: 0.6106692552566528 = 0.0017697442090138793 + 0.1 * 6.088995456695557
Epoch 1880, val loss: 1.2092394828796387
Epoch 1890, training loss: 0.61106276512146 = 0.0017546468880027533 + 0.1 * 6.093080997467041
Epoch 1890, val loss: 1.2104781866073608
Epoch 1900, training loss: 0.611691415309906 = 0.001739842933602631 + 0.1 * 6.099515438079834
Epoch 1900, val loss: 1.2117919921875
Epoch 1910, training loss: 0.6115217208862305 = 0.0017252594698220491 + 0.1 * 6.097964763641357
Epoch 1910, val loss: 1.2130221128463745
Epoch 1920, training loss: 0.6116942763328552 = 0.0017109442269429564 + 0.1 * 6.099833011627197
Epoch 1920, val loss: 1.2142449617385864
Epoch 1930, training loss: 0.6103918552398682 = 0.001696778810583055 + 0.1 * 6.086950778961182
Epoch 1930, val loss: 1.2154380083084106
Epoch 1940, training loss: 0.6105567216873169 = 0.0016829344676807523 + 0.1 * 6.088737964630127
Epoch 1940, val loss: 1.2167024612426758
Epoch 1950, training loss: 0.6107036471366882 = 0.001669282908551395 + 0.1 * 6.090343475341797
Epoch 1950, val loss: 1.2179336547851562
Epoch 1960, training loss: 0.6107372641563416 = 0.0016557940980419517 + 0.1 * 6.090814113616943
Epoch 1960, val loss: 1.2191410064697266
Epoch 1970, training loss: 0.6102842092514038 = 0.0016425015637651086 + 0.1 * 6.086416721343994
Epoch 1970, val loss: 1.2203222513198853
Epoch 1980, training loss: 0.6107094883918762 = 0.001629459671676159 + 0.1 * 6.090799808502197
Epoch 1980, val loss: 1.2215263843536377
Epoch 1990, training loss: 0.6113170981407166 = 0.0016165393171831965 + 0.1 * 6.097005367279053
Epoch 1990, val loss: 1.2227067947387695
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8598
Flip ASR: 0.8400/225 nodes
The final ASR:0.57442, 0.22495, Accuracy:0.82099, 0.02744
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11576])
remove edge: torch.Size([2, 9412])
updated graph: torch.Size([2, 10432])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97294, 0.00174, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7798547744750977 = 1.942466378211975 + 0.1 * 8.373885154724121
Epoch 0, val loss: 1.9418147802352905
Epoch 10, training loss: 2.7691493034362793 = 1.9317741394042969 + 0.1 * 8.373750686645508
Epoch 10, val loss: 1.9311470985412598
Epoch 20, training loss: 2.755988121032715 = 1.918692708015442 + 0.1 * 8.372954368591309
Epoch 20, val loss: 1.9178862571716309
Epoch 30, training loss: 2.7370476722717285 = 1.9004098176956177 + 0.1 * 8.366377830505371
Epoch 30, val loss: 1.8994940519332886
Epoch 40, training loss: 2.7055673599243164 = 1.873591423034668 + 0.1 * 8.319758415222168
Epoch 40, val loss: 1.8732043504714966
Epoch 50, training loss: 2.638991117477417 = 1.8378114700317383 + 0.1 * 8.011795997619629
Epoch 50, val loss: 1.8405739068984985
Epoch 60, training loss: 2.562014102935791 = 1.7997922897338867 + 0.1 * 7.622217655181885
Epoch 60, val loss: 1.8089637756347656
Epoch 70, training loss: 2.488635540008545 = 1.7647297382354736 + 0.1 * 7.2390570640563965
Epoch 70, val loss: 1.7811965942382812
Epoch 80, training loss: 2.4220430850982666 = 1.7268706560134888 + 0.1 * 6.951723575592041
Epoch 80, val loss: 1.7491815090179443
Epoch 90, training loss: 2.357717990875244 = 1.677022933959961 + 0.1 * 6.806950569152832
Epoch 90, val loss: 1.7060351371765137
Epoch 100, training loss: 2.2841103076934814 = 1.611295223236084 + 0.1 * 6.728149890899658
Epoch 100, val loss: 1.6508289575576782
Epoch 110, training loss: 2.196260929107666 = 1.528477430343628 + 0.1 * 6.677834987640381
Epoch 110, val loss: 1.583294153213501
Epoch 120, training loss: 2.1000983715057373 = 1.4349325895309448 + 0.1 * 6.651657581329346
Epoch 120, val loss: 1.5088074207305908
Epoch 130, training loss: 2.0022623538970947 = 1.3383644819259644 + 0.1 * 6.638978004455566
Epoch 130, val loss: 1.4346367120742798
Epoch 140, training loss: 1.9042285680770874 = 1.2411760091781616 + 0.1 * 6.630525588989258
Epoch 140, val loss: 1.3622322082519531
Epoch 150, training loss: 1.8059428930282593 = 1.1437339782714844 + 0.1 * 6.62208890914917
Epoch 150, val loss: 1.2903592586517334
Epoch 160, training loss: 1.7091107368469238 = 1.0478568077087402 + 0.1 * 6.612539291381836
Epoch 160, val loss: 1.2197145223617554
Epoch 170, training loss: 1.6157028675079346 = 0.9557294845581055 + 0.1 * 6.599732875823975
Epoch 170, val loss: 1.1519067287445068
Epoch 180, training loss: 1.5275161266326904 = 0.8688852190971375 + 0.1 * 6.58630895614624
Epoch 180, val loss: 1.0877779722213745
Epoch 190, training loss: 1.4459028244018555 = 0.7889073491096497 + 0.1 * 6.569955348968506
Epoch 190, val loss: 1.0298750400543213
Epoch 200, training loss: 1.3713393211364746 = 0.7161842584609985 + 0.1 * 6.551549911499023
Epoch 200, val loss: 0.979912519454956
Epoch 210, training loss: 1.3032655715942383 = 0.6499263644218445 + 0.1 * 6.53339147567749
Epoch 210, val loss: 0.9374110698699951
Epoch 220, training loss: 1.2416456937789917 = 0.5895414352416992 + 0.1 * 6.521042346954346
Epoch 220, val loss: 0.9020184278488159
Epoch 230, training loss: 1.1844836473464966 = 0.5338374972343445 + 0.1 * 6.506461143493652
Epoch 230, val loss: 0.8721393346786499
Epoch 240, training loss: 1.13103187084198 = 0.4813741147518158 + 0.1 * 6.496577739715576
Epoch 240, val loss: 0.8462633490562439
Epoch 250, training loss: 1.0808494091033936 = 0.4320429563522339 + 0.1 * 6.488063812255859
Epoch 250, val loss: 0.824048638343811
Epoch 260, training loss: 1.0334402322769165 = 0.38557592034339905 + 0.1 * 6.47864294052124
Epoch 260, val loss: 0.8056215047836304
Epoch 270, training loss: 0.9899877905845642 = 0.34174901247024536 + 0.1 * 6.482387542724609
Epoch 270, val loss: 0.7909984588623047
Epoch 280, training loss: 0.9477006196975708 = 0.3011961877346039 + 0.1 * 6.4650444984436035
Epoch 280, val loss: 0.7807024717330933
Epoch 290, training loss: 0.9095321893692017 = 0.2640192210674286 + 0.1 * 6.455129146575928
Epoch 290, val loss: 0.7748892307281494
Epoch 300, training loss: 0.8778787851333618 = 0.23059716820716858 + 0.1 * 6.472816467285156
Epoch 300, val loss: 0.7735979557037354
Epoch 310, training loss: 0.8459997177124023 = 0.20154359936714172 + 0.1 * 6.444560527801514
Epoch 310, val loss: 0.7766435146331787
Epoch 320, training loss: 0.8197997808456421 = 0.17654778063297272 + 0.1 * 6.432519912719727
Epoch 320, val loss: 0.7834411263465881
Epoch 330, training loss: 0.7989294528961182 = 0.15537674725055695 + 0.1 * 6.435527324676514
Epoch 330, val loss: 0.7934327721595764
Epoch 340, training loss: 0.7795642614364624 = 0.13757619261741638 + 0.1 * 6.419880390167236
Epoch 340, val loss: 0.8057317137718201
Epoch 350, training loss: 0.7637240290641785 = 0.12260454148054123 + 0.1 * 6.411194801330566
Epoch 350, val loss: 0.8198434710502625
Epoch 360, training loss: 0.749965488910675 = 0.10997973382472992 + 0.1 * 6.399857044219971
Epoch 360, val loss: 0.8351686000823975
Epoch 370, training loss: 0.7393826842308044 = 0.0992414727807045 + 0.1 * 6.401411533355713
Epoch 370, val loss: 0.851134717464447
Epoch 380, training loss: 0.728556215763092 = 0.09009229391813278 + 0.1 * 6.384639263153076
Epoch 380, val loss: 0.8674017190933228
Epoch 390, training loss: 0.7198761105537415 = 0.08217301219701767 + 0.1 * 6.377031326293945
Epoch 390, val loss: 0.8838337063789368
Epoch 400, training loss: 0.7128832340240479 = 0.0752749890089035 + 0.1 * 6.376082420349121
Epoch 400, val loss: 0.9002093076705933
Epoch 410, training loss: 0.7061780095100403 = 0.06924380362033844 + 0.1 * 6.369341850280762
Epoch 410, val loss: 0.9163159132003784
Epoch 420, training loss: 0.699761688709259 = 0.06391585618257523 + 0.1 * 6.358458518981934
Epoch 420, val loss: 0.932234525680542
Epoch 430, training loss: 0.6954905390739441 = 0.05918026342988014 + 0.1 * 6.363102436065674
Epoch 430, val loss: 0.9478452801704407
Epoch 440, training loss: 0.689976692199707 = 0.0549599789083004 + 0.1 * 6.350167274475098
Epoch 440, val loss: 0.9631567001342773
Epoch 450, training loss: 0.6852699518203735 = 0.05118200182914734 + 0.1 * 6.340878963470459
Epoch 450, val loss: 0.9781259894371033
Epoch 460, training loss: 0.6817705631256104 = 0.04777085781097412 + 0.1 * 6.339996814727783
Epoch 460, val loss: 0.9927491545677185
Epoch 470, training loss: 0.6775123476982117 = 0.04468703642487526 + 0.1 * 6.328252792358398
Epoch 470, val loss: 1.006998896598816
Epoch 480, training loss: 0.6748232245445251 = 0.041883841156959534 + 0.1 * 6.3293938636779785
Epoch 480, val loss: 1.0209007263183594
Epoch 490, training loss: 0.6722784638404846 = 0.03932727500796318 + 0.1 * 6.329512119293213
Epoch 490, val loss: 1.0345357656478882
Epoch 500, training loss: 0.6683151721954346 = 0.03699363023042679 + 0.1 * 6.313215255737305
Epoch 500, val loss: 1.0478434562683105
Epoch 510, training loss: 0.6661017537117004 = 0.0348539762198925 + 0.1 * 6.3124775886535645
Epoch 510, val loss: 1.0609006881713867
Epoch 520, training loss: 0.66278475522995 = 0.032889604568481445 + 0.1 * 6.298951625823975
Epoch 520, val loss: 1.0736405849456787
Epoch 530, training loss: 0.6630906462669373 = 0.031081095337867737 + 0.1 * 6.320095539093018
Epoch 530, val loss: 1.086121916770935
Epoch 540, training loss: 0.659914493560791 = 0.029418546706438065 + 0.1 * 6.304959774017334
Epoch 540, val loss: 1.0981056690216064
Epoch 550, training loss: 0.6571159362792969 = 0.02788666822016239 + 0.1 * 6.29229211807251
Epoch 550, val loss: 1.1100629568099976
Epoch 560, training loss: 0.6564139723777771 = 0.02646446041762829 + 0.1 * 6.299494743347168
Epoch 560, val loss: 1.121634602546692
Epoch 570, training loss: 0.6534938216209412 = 0.025151779875159264 + 0.1 * 6.283420085906982
Epoch 570, val loss: 1.1327928304672241
Epoch 580, training loss: 0.6519584059715271 = 0.023936834186315536 + 0.1 * 6.280215740203857
Epoch 580, val loss: 1.1439344882965088
Epoch 590, training loss: 0.6502583622932434 = 0.022806305438280106 + 0.1 * 6.2745208740234375
Epoch 590, val loss: 1.1546074151992798
Epoch 600, training loss: 0.6494455337524414 = 0.021752992644906044 + 0.1 * 6.276925086975098
Epoch 600, val loss: 1.16513991355896
Epoch 610, training loss: 0.648201048374176 = 0.020770898088812828 + 0.1 * 6.274301528930664
Epoch 610, val loss: 1.1754217147827148
Epoch 620, training loss: 0.6463118195533752 = 0.019853413105010986 + 0.1 * 6.264584064483643
Epoch 620, val loss: 1.185610294342041
Epoch 630, training loss: 0.6463256478309631 = 0.01899571903049946 + 0.1 * 6.273299217224121
Epoch 630, val loss: 1.1953719854354858
Epoch 640, training loss: 0.6429036855697632 = 0.01819552667438984 + 0.1 * 6.247081756591797
Epoch 640, val loss: 1.2050966024398804
Epoch 650, training loss: 0.6413909792900085 = 0.017445141449570656 + 0.1 * 6.239458084106445
Epoch 650, val loss: 1.2145723104476929
Epoch 660, training loss: 0.6429548859596252 = 0.016739722341299057 + 0.1 * 6.262151718139648
Epoch 660, val loss: 1.2237738370895386
Epoch 670, training loss: 0.6404755115509033 = 0.01607772707939148 + 0.1 * 6.2439775466918945
Epoch 670, val loss: 1.232875108718872
Epoch 680, training loss: 0.6399837732315063 = 0.015455812215805054 + 0.1 * 6.245279312133789
Epoch 680, val loss: 1.2416949272155762
Epoch 690, training loss: 0.6378461718559265 = 0.014870484359562397 + 0.1 * 6.229756832122803
Epoch 690, val loss: 1.250368595123291
Epoch 700, training loss: 0.6368816494941711 = 0.014319084584712982 + 0.1 * 6.225625991821289
Epoch 700, val loss: 1.2589614391326904
Epoch 710, training loss: 0.6371057629585266 = 0.01379772461950779 + 0.1 * 6.2330803871154785
Epoch 710, val loss: 1.2672666311264038
Epoch 720, training loss: 0.6355064511299133 = 0.01330531481653452 + 0.1 * 6.222011089324951
Epoch 720, val loss: 1.2754108905792236
Epoch 730, training loss: 0.6360923647880554 = 0.01283980906009674 + 0.1 * 6.232525825500488
Epoch 730, val loss: 1.283441185951233
Epoch 740, training loss: 0.635964035987854 = 0.012399279512465 + 0.1 * 6.235647678375244
Epoch 740, val loss: 1.2912216186523438
Epoch 750, training loss: 0.6323527693748474 = 0.011983240954577923 + 0.1 * 6.203695297241211
Epoch 750, val loss: 1.298967957496643
Epoch 760, training loss: 0.6336966156959534 = 0.01158845890313387 + 0.1 * 6.221081733703613
Epoch 760, val loss: 1.306565761566162
Epoch 770, training loss: 0.6328380703926086 = 0.011212487705051899 + 0.1 * 6.2162556648254395
Epoch 770, val loss: 1.3139568567276
Epoch 780, training loss: 0.6322662830352783 = 0.010856528766453266 + 0.1 * 6.214097499847412
Epoch 780, val loss: 1.3212517499923706
Epoch 790, training loss: 0.6309822201728821 = 0.010517820715904236 + 0.1 * 6.204643726348877
Epoch 790, val loss: 1.3284571170806885
Epoch 800, training loss: 0.6305533051490784 = 0.010197491385042667 + 0.1 * 6.203557968139648
Epoch 800, val loss: 1.3354462385177612
Epoch 810, training loss: 0.6297086477279663 = 0.009891781024634838 + 0.1 * 6.1981682777404785
Epoch 810, val loss: 1.3423744440078735
Epoch 820, training loss: 0.6286993622779846 = 0.009600206278264523 + 0.1 * 6.190991401672363
Epoch 820, val loss: 1.3490848541259766
Epoch 830, training loss: 0.6298028230667114 = 0.009321323595941067 + 0.1 * 6.204814910888672
Epoch 830, val loss: 1.3558318614959717
Epoch 840, training loss: 0.627360463142395 = 0.009055230766534805 + 0.1 * 6.183052062988281
Epoch 840, val loss: 1.3623223304748535
Epoch 850, training loss: 0.6302882432937622 = 0.008801810443401337 + 0.1 * 6.214864253997803
Epoch 850, val loss: 1.3688232898712158
Epoch 860, training loss: 0.6261321306228638 = 0.008560508489608765 + 0.1 * 6.175716400146484
Epoch 860, val loss: 1.3751578330993652
Epoch 870, training loss: 0.6262287497520447 = 0.00832973513752222 + 0.1 * 6.178989887237549
Epoch 870, val loss: 1.3814212083816528
Epoch 880, training loss: 0.6272466778755188 = 0.008108357898890972 + 0.1 * 6.191382884979248
Epoch 880, val loss: 1.387616753578186
Epoch 890, training loss: 0.6250124573707581 = 0.007895873859524727 + 0.1 * 6.171165466308594
Epoch 890, val loss: 1.3936299085617065
Epoch 900, training loss: 0.6246426105499268 = 0.007692795712500811 + 0.1 * 6.169497966766357
Epoch 900, val loss: 1.399656057357788
Epoch 910, training loss: 0.6262245178222656 = 0.007498092949390411 + 0.1 * 6.187264442443848
Epoch 910, val loss: 1.4055941104888916
Epoch 920, training loss: 0.6242961287498474 = 0.0073111578822135925 + 0.1 * 6.169849395751953
Epoch 920, val loss: 1.4113142490386963
Epoch 930, training loss: 0.6261677145957947 = 0.0071319700218737125 + 0.1 * 6.190357208251953
Epoch 930, val loss: 1.4170807600021362
Epoch 940, training loss: 0.6238228678703308 = 0.006960137281566858 + 0.1 * 6.16862678527832
Epoch 940, val loss: 1.4227290153503418
Epoch 950, training loss: 0.6227824091911316 = 0.006794604007154703 + 0.1 * 6.159877777099609
Epoch 950, val loss: 1.4282917976379395
Epoch 960, training loss: 0.6241353750228882 = 0.00663522444665432 + 0.1 * 6.17500114440918
Epoch 960, val loss: 1.4337114095687866
Epoch 970, training loss: 0.6220705509185791 = 0.006481869611889124 + 0.1 * 6.155886650085449
Epoch 970, val loss: 1.4391605854034424
Epoch 980, training loss: 0.6228211522102356 = 0.006334756966680288 + 0.1 * 6.164863586425781
Epoch 980, val loss: 1.444611668586731
Epoch 990, training loss: 0.6217027306556702 = 0.006192650180310011 + 0.1 * 6.1551008224487305
Epoch 990, val loss: 1.4498577117919922
Epoch 1000, training loss: 0.6209009885787964 = 0.006055629812180996 + 0.1 * 6.148453235626221
Epoch 1000, val loss: 1.4550405740737915
Epoch 1010, training loss: 0.6209882497787476 = 0.005923928692936897 + 0.1 * 6.1506428718566895
Epoch 1010, val loss: 1.4601572751998901
Epoch 1020, training loss: 0.6202455163002014 = 0.00579662574455142 + 0.1 * 6.14448881149292
Epoch 1020, val loss: 1.4652502536773682
Epoch 1030, training loss: 0.6208456754684448 = 0.005673834588378668 + 0.1 * 6.151718616485596
Epoch 1030, val loss: 1.4702481031417847
Epoch 1040, training loss: 0.6194729208946228 = 0.005555149633437395 + 0.1 * 6.139177322387695
Epoch 1040, val loss: 1.4752435684204102
Epoch 1050, training loss: 0.6198784112930298 = 0.005440899170935154 + 0.1 * 6.144374847412109
Epoch 1050, val loss: 1.4801942110061646
Epoch 1060, training loss: 0.6194555759429932 = 0.005330410320311785 + 0.1 * 6.141252040863037
Epoch 1060, val loss: 1.4850289821624756
Epoch 1070, training loss: 0.6193094849586487 = 0.005223281215876341 + 0.1 * 6.140861988067627
Epoch 1070, val loss: 1.4897884130477905
Epoch 1080, training loss: 0.619461715221405 = 0.00512006226927042 + 0.1 * 6.143416404724121
Epoch 1080, val loss: 1.4944344758987427
Epoch 1090, training loss: 0.6191356182098389 = 0.00502034230157733 + 0.1 * 6.141152858734131
Epoch 1090, val loss: 1.4991466999053955
Epoch 1100, training loss: 0.6204107999801636 = 0.004923389293253422 + 0.1 * 6.154873847961426
Epoch 1100, val loss: 1.5038223266601562
Epoch 1110, training loss: 0.6182982921600342 = 0.004829843528568745 + 0.1 * 6.1346845626831055
Epoch 1110, val loss: 1.508301854133606
Epoch 1120, training loss: 0.6185032725334167 = 0.004739295691251755 + 0.1 * 6.13763952255249
Epoch 1120, val loss: 1.5128427743911743
Epoch 1130, training loss: 0.6178845763206482 = 0.004651343449950218 + 0.1 * 6.132331848144531
Epoch 1130, val loss: 1.517359972000122
Epoch 1140, training loss: 0.6192508935928345 = 0.004566383082419634 + 0.1 * 6.146844863891602
Epoch 1140, val loss: 1.5217629671096802
Epoch 1150, training loss: 0.6168583035469055 = 0.004483694676309824 + 0.1 * 6.123745918273926
Epoch 1150, val loss: 1.5260627269744873
Epoch 1160, training loss: 0.6167358756065369 = 0.004403818864375353 + 0.1 * 6.123320579528809
Epoch 1160, val loss: 1.5303642749786377
Epoch 1170, training loss: 0.6173948645591736 = 0.004326104186475277 + 0.1 * 6.130687236785889
Epoch 1170, val loss: 1.5346029996871948
Epoch 1180, training loss: 0.6180356740951538 = 0.004250794183462858 + 0.1 * 6.137848854064941
Epoch 1180, val loss: 1.5387994050979614
Epoch 1190, training loss: 0.616349995136261 = 0.004177484195679426 + 0.1 * 6.121724605560303
Epoch 1190, val loss: 1.5429275035858154
Epoch 1200, training loss: 0.616917610168457 = 0.004106574691832066 + 0.1 * 6.128109931945801
Epoch 1200, val loss: 1.5470527410507202
Epoch 1210, training loss: 0.6163290143013 = 0.004037685226649046 + 0.1 * 6.122913360595703
Epoch 1210, val loss: 1.5510871410369873
Epoch 1220, training loss: 0.617068886756897 = 0.003970498684793711 + 0.1 * 6.130983829498291
Epoch 1220, val loss: 1.5550709962844849
Epoch 1230, training loss: 0.615353524684906 = 0.0039052937645465136 + 0.1 * 6.114482402801514
Epoch 1230, val loss: 1.5590890645980835
Epoch 1240, training loss: 0.6173064112663269 = 0.0038421149365603924 + 0.1 * 6.134642601013184
Epoch 1240, val loss: 1.563014030456543
Epoch 1250, training loss: 0.6154890060424805 = 0.003780784085392952 + 0.1 * 6.117082118988037
Epoch 1250, val loss: 1.5668913125991821
Epoch 1260, training loss: 0.6156927347183228 = 0.0037211270537227392 + 0.1 * 6.119716167449951
Epoch 1260, val loss: 1.5707893371582031
Epoch 1270, training loss: 0.615824818611145 = 0.003662771312519908 + 0.1 * 6.121620178222656
Epoch 1270, val loss: 1.574549913406372
Epoch 1280, training loss: 0.6155390739440918 = 0.003606110345572233 + 0.1 * 6.119329929351807
Epoch 1280, val loss: 1.5782324075698853
Epoch 1290, training loss: 0.6151739358901978 = 0.003551050089299679 + 0.1 * 6.1162285804748535
Epoch 1290, val loss: 1.582016944885254
Epoch 1300, training loss: 0.6141720414161682 = 0.0034974010195583105 + 0.1 * 6.106746196746826
Epoch 1300, val loss: 1.585678219795227
Epoch 1310, training loss: 0.6146810054779053 = 0.0034452250692993402 + 0.1 * 6.112358093261719
Epoch 1310, val loss: 1.5893428325653076
Epoch 1320, training loss: 0.6149547696113586 = 0.003394247032701969 + 0.1 * 6.115604877471924
Epoch 1320, val loss: 1.5929369926452637
Epoch 1330, training loss: 0.6137836575508118 = 0.0033444350119680166 + 0.1 * 6.104392051696777
Epoch 1330, val loss: 1.5965031385421753
Epoch 1340, training loss: 0.6160013675689697 = 0.0032960991375148296 + 0.1 * 6.1270527839660645
Epoch 1340, val loss: 1.600027084350586
Epoch 1350, training loss: 0.6144593954086304 = 0.0032488731667399406 + 0.1 * 6.112104892730713
Epoch 1350, val loss: 1.6035687923431396
Epoch 1360, training loss: 0.6136160492897034 = 0.003202892607077956 + 0.1 * 6.10413122177124
Epoch 1360, val loss: 1.6070693731307983
Epoch 1370, training loss: 0.6156992316246033 = 0.003158119972795248 + 0.1 * 6.125411033630371
Epoch 1370, val loss: 1.6105451583862305
Epoch 1380, training loss: 0.6136436462402344 = 0.0031144493259489536 + 0.1 * 6.105291843414307
Epoch 1380, val loss: 1.6139612197875977
Epoch 1390, training loss: 0.6138684749603271 = 0.003071829676628113 + 0.1 * 6.10796594619751
Epoch 1390, val loss: 1.6173651218414307
Epoch 1400, training loss: 0.6136715412139893 = 0.0030301318038254976 + 0.1 * 6.106413841247559
Epoch 1400, val loss: 1.6207324266433716
Epoch 1410, training loss: 0.6126805543899536 = 0.002989503787830472 + 0.1 * 6.096909999847412
Epoch 1410, val loss: 1.624001145362854
Epoch 1420, training loss: 0.6137778759002686 = 0.0029498455114662647 + 0.1 * 6.108280181884766
Epoch 1420, val loss: 1.6273021697998047
Epoch 1430, training loss: 0.6138010621070862 = 0.002910949755460024 + 0.1 * 6.108900547027588
Epoch 1430, val loss: 1.6306281089782715
Epoch 1440, training loss: 0.6126444935798645 = 0.002873091958463192 + 0.1 * 6.097714424133301
Epoch 1440, val loss: 1.633838415145874
Epoch 1450, training loss: 0.6135295033454895 = 0.002836082596331835 + 0.1 * 6.10693359375
Epoch 1450, val loss: 1.6370576620101929
Epoch 1460, training loss: 0.6124531030654907 = 0.002799923764541745 + 0.1 * 6.096531391143799
Epoch 1460, val loss: 1.6401623487472534
Epoch 1470, training loss: 0.6124961972236633 = 0.0027647456154227257 + 0.1 * 6.097314357757568
Epoch 1470, val loss: 1.643286108970642
Epoch 1480, training loss: 0.6130504012107849 = 0.0027301085647195578 + 0.1 * 6.103202819824219
Epoch 1480, val loss: 1.646406888961792
Epoch 1490, training loss: 0.6119547486305237 = 0.0026961336843669415 + 0.1 * 6.092585563659668
Epoch 1490, val loss: 1.649498701095581
Epoch 1500, training loss: 0.6123770475387573 = 0.0026629073545336723 + 0.1 * 6.097141265869141
Epoch 1500, val loss: 1.6525577306747437
Epoch 1510, training loss: 0.6126564741134644 = 0.002630474278703332 + 0.1 * 6.100259780883789
Epoch 1510, val loss: 1.6555815935134888
Epoch 1520, training loss: 0.6122309565544128 = 0.0025989743880927563 + 0.1 * 6.096319198608398
Epoch 1520, val loss: 1.6585952043533325
Epoch 1530, training loss: 0.612130880355835 = 0.0025680549442768097 + 0.1 * 6.095628261566162
Epoch 1530, val loss: 1.6616190671920776
Epoch 1540, training loss: 0.6125006079673767 = 0.0025377508718520403 + 0.1 * 6.099628925323486
Epoch 1540, val loss: 1.664602518081665
Epoch 1550, training loss: 0.6116253137588501 = 0.0025079732295125723 + 0.1 * 6.09117317199707
Epoch 1550, val loss: 1.667497158050537
Epoch 1560, training loss: 0.611870288848877 = 0.002478941809386015 + 0.1 * 6.0939130783081055
Epoch 1560, val loss: 1.670363187789917
Epoch 1570, training loss: 0.6122344136238098 = 0.0024503376334905624 + 0.1 * 6.097840785980225
Epoch 1570, val loss: 1.6732858419418335
Epoch 1580, training loss: 0.6115370392799377 = 0.0024224319495260715 + 0.1 * 6.091145992279053
Epoch 1580, val loss: 1.6761683225631714
Epoch 1590, training loss: 0.6109403371810913 = 0.002395156305283308 + 0.1 * 6.085451602935791
Epoch 1590, val loss: 1.6789917945861816
Epoch 1600, training loss: 0.6123598217964172 = 0.002368390094488859 + 0.1 * 6.099914073944092
Epoch 1600, val loss: 1.6818174123764038
Epoch 1610, training loss: 0.6121797561645508 = 0.002342061372473836 + 0.1 * 6.098376750946045
Epoch 1610, val loss: 1.6846182346343994
Epoch 1620, training loss: 0.6112607717514038 = 0.0023163941223174334 + 0.1 * 6.089443206787109
Epoch 1620, val loss: 1.687348484992981
Epoch 1630, training loss: 0.6115670800209045 = 0.002291234442964196 + 0.1 * 6.0927581787109375
Epoch 1630, val loss: 1.69010329246521
Epoch 1640, training loss: 0.6103875637054443 = 0.002266485942527652 + 0.1 * 6.081211090087891
Epoch 1640, val loss: 1.692758560180664
Epoch 1650, training loss: 0.611838161945343 = 0.002242305548861623 + 0.1 * 6.095958232879639
Epoch 1650, val loss: 1.695410966873169
Epoch 1660, training loss: 0.6104725003242493 = 0.002218466717749834 + 0.1 * 6.082540035247803
Epoch 1660, val loss: 1.6981282234191895
Epoch 1670, training loss: 0.6107466816902161 = 0.0021952984388917685 + 0.1 * 6.085513591766357
Epoch 1670, val loss: 1.7007653713226318
Epoch 1680, training loss: 0.6119483113288879 = 0.002172376262024045 + 0.1 * 6.097759246826172
Epoch 1680, val loss: 1.7033673524856567
Epoch 1690, training loss: 0.6113635301589966 = 0.0021498720161616802 + 0.1 * 6.092136383056641
Epoch 1690, val loss: 1.7059485912322998
Epoch 1700, training loss: 0.610807478427887 = 0.002127863233909011 + 0.1 * 6.086795806884766
Epoch 1700, val loss: 1.7085461616516113
Epoch 1710, training loss: 0.6116899847984314 = 0.002106426516547799 + 0.1 * 6.0958356857299805
Epoch 1710, val loss: 1.7111320495605469
Epoch 1720, training loss: 0.6104176044464111 = 0.0020851739682257175 + 0.1 * 6.083324432373047
Epoch 1720, val loss: 1.7136720418930054
Epoch 1730, training loss: 0.6100011467933655 = 0.0020645682234317064 + 0.1 * 6.079365253448486
Epoch 1730, val loss: 1.7161657810211182
Epoch 1740, training loss: 0.6105875372886658 = 0.002044225577265024 + 0.1 * 6.085433483123779
Epoch 1740, val loss: 1.7186756134033203
Epoch 1750, training loss: 0.6110391020774841 = 0.0020241232123225927 + 0.1 * 6.090149879455566
Epoch 1750, val loss: 1.7211838960647583
Epoch 1760, training loss: 0.6102979779243469 = 0.002004381036385894 + 0.1 * 6.082935810089111
Epoch 1760, val loss: 1.7236484289169312
Epoch 1770, training loss: 0.6099840402603149 = 0.0019850819371640682 + 0.1 * 6.079989433288574
Epoch 1770, val loss: 1.726085901260376
Epoch 1780, training loss: 0.610031247138977 = 0.0019661695696413517 + 0.1 * 6.080650806427002
Epoch 1780, val loss: 1.728550672531128
Epoch 1790, training loss: 0.6095988750457764 = 0.0019476123852655292 + 0.1 * 6.076512336730957
Epoch 1790, val loss: 1.7310518026351929
Epoch 1800, training loss: 0.6097592115402222 = 0.00192934344522655 + 0.1 * 6.078298568725586
Epoch 1800, val loss: 1.7334781885147095
Epoch 1810, training loss: 0.6108084917068481 = 0.0019113438902422786 + 0.1 * 6.0889716148376465
Epoch 1810, val loss: 1.7358540296554565
Epoch 1820, training loss: 0.6097302436828613 = 0.001893644337542355 + 0.1 * 6.078365802764893
Epoch 1820, val loss: 1.7382289171218872
Epoch 1830, training loss: 0.6104525327682495 = 0.0018762568943202496 + 0.1 * 6.085762977600098
Epoch 1830, val loss: 1.740556240081787
Epoch 1840, training loss: 0.6102504730224609 = 0.001859126496128738 + 0.1 * 6.083913326263428
Epoch 1840, val loss: 1.7429121732711792
Epoch 1850, training loss: 0.6097220182418823 = 0.0018423930741846561 + 0.1 * 6.07879638671875
Epoch 1850, val loss: 1.7452634572982788
Epoch 1860, training loss: 0.609189510345459 = 0.0018258857307955623 + 0.1 * 6.073636054992676
Epoch 1860, val loss: 1.7475980520248413
Epoch 1870, training loss: 0.6100037693977356 = 0.0018096849089488387 + 0.1 * 6.0819411277771
Epoch 1870, val loss: 1.7498739957809448
Epoch 1880, training loss: 0.6096553802490234 = 0.0017937251832336187 + 0.1 * 6.078616142272949
Epoch 1880, val loss: 1.7520924806594849
Epoch 1890, training loss: 0.608853816986084 = 0.001777993398718536 + 0.1 * 6.070757865905762
Epoch 1890, val loss: 1.7543672323226929
Epoch 1900, training loss: 0.6092159748077393 = 0.001762577798217535 + 0.1 * 6.074533939361572
Epoch 1900, val loss: 1.756638765335083
Epoch 1910, training loss: 0.6095241904258728 = 0.0017473861807957292 + 0.1 * 6.077768325805664
Epoch 1910, val loss: 1.7588638067245483
Epoch 1920, training loss: 0.6090254783630371 = 0.0017323876963928342 + 0.1 * 6.072930812835693
Epoch 1920, val loss: 1.761090636253357
Epoch 1930, training loss: 0.6094852089881897 = 0.0017177024856209755 + 0.1 * 6.0776753425598145
Epoch 1930, val loss: 1.7632975578308105
Epoch 1940, training loss: 0.6091583967208862 = 0.0017032258911058307 + 0.1 * 6.074552059173584
Epoch 1940, val loss: 1.7654539346694946
Epoch 1950, training loss: 0.6093022227287292 = 0.001689002150669694 + 0.1 * 6.076131820678711
Epoch 1950, val loss: 1.7676008939743042
Epoch 1960, training loss: 0.6080476641654968 = 0.0016749044880270958 + 0.1 * 6.063727855682373
Epoch 1960, val loss: 1.7697824239730835
Epoch 1970, training loss: 0.6103960275650024 = 0.0016611347673460841 + 0.1 * 6.087348937988281
Epoch 1970, val loss: 1.7719396352767944
Epoch 1980, training loss: 0.6081832051277161 = 0.0016474586445838213 + 0.1 * 6.065357208251953
Epoch 1980, val loss: 1.7740159034729004
Epoch 1990, training loss: 0.6086515784263611 = 0.0016341055743396282 + 0.1 * 6.070174217224121
Epoch 1990, val loss: 1.7761445045471191
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6052
Flip ASR: 0.5289/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7759552001953125 = 1.9385666847229004 + 0.1 * 8.373886108398438
Epoch 0, val loss: 1.9388127326965332
Epoch 10, training loss: 2.7664051055908203 = 1.9290279150009155 + 0.1 * 8.373770713806152
Epoch 10, val loss: 1.928582787513733
Epoch 20, training loss: 2.7549943923950195 = 1.9176859855651855 + 0.1 * 8.37308406829834
Epoch 20, val loss: 1.9163926839828491
Epoch 30, training loss: 2.7389109134674072 = 1.9021391868591309 + 0.1 * 8.367716789245605
Epoch 30, val loss: 1.8998503684997559
Epoch 40, training loss: 2.7120065689086914 = 1.8795260190963745 + 0.1 * 8.324806213378906
Epoch 40, val loss: 1.8759719133377075
Epoch 50, training loss: 2.645425796508789 = 1.847779393196106 + 0.1 * 7.97646427154541
Epoch 50, val loss: 1.8438198566436768
Epoch 60, training loss: 2.5439271926879883 = 1.8103619813919067 + 0.1 * 7.335651397705078
Epoch 60, val loss: 1.8076369762420654
Epoch 70, training loss: 2.4802472591400146 = 1.7706880569458008 + 0.1 * 7.0955915451049805
Epoch 70, val loss: 1.7704246044158936
Epoch 80, training loss: 2.4193503856658936 = 1.7269845008850098 + 0.1 * 6.9236578941345215
Epoch 80, val loss: 1.7314788103103638
Epoch 90, training loss: 2.355290174484253 = 1.6727718114852905 + 0.1 * 6.825182914733887
Epoch 90, val loss: 1.6832391023635864
Epoch 100, training loss: 2.2758710384368896 = 1.5994127988815308 + 0.1 * 6.764583110809326
Epoch 100, val loss: 1.6182438135147095
Epoch 110, training loss: 2.1781482696533203 = 1.505752682685852 + 0.1 * 6.72395658493042
Epoch 110, val loss: 1.5387319326400757
Epoch 120, training loss: 2.0666580200195312 = 1.39663827419281 + 0.1 * 6.700198173522949
Epoch 120, val loss: 1.4498450756072998
Epoch 130, training loss: 1.9524893760681152 = 1.283737301826477 + 0.1 * 6.687521457672119
Epoch 130, val loss: 1.361196517944336
Epoch 140, training loss: 1.8451491594314575 = 1.1772304773330688 + 0.1 * 6.679186820983887
Epoch 140, val loss: 1.2807646989822388
Epoch 150, training loss: 1.748351812362671 = 1.0810108184814453 + 0.1 * 6.673409938812256
Epoch 150, val loss: 1.209464430809021
Epoch 160, training loss: 1.66177237033844 = 0.9948399662971497 + 0.1 * 6.669323921203613
Epoch 160, val loss: 1.146192193031311
Epoch 170, training loss: 1.5835890769958496 = 0.9169660806655884 + 0.1 * 6.666229248046875
Epoch 170, val loss: 1.089327096939087
Epoch 180, training loss: 1.5111782550811768 = 0.8447920680046082 + 0.1 * 6.6638617515563965
Epoch 180, val loss: 1.03682541847229
Epoch 190, training loss: 1.4419026374816895 = 0.775987982749939 + 0.1 * 6.659145832061768
Epoch 190, val loss: 0.9869866371154785
Epoch 200, training loss: 1.3744697570800781 = 0.7091406583786011 + 0.1 * 6.653290748596191
Epoch 200, val loss: 0.938895046710968
Epoch 210, training loss: 1.3090862035751343 = 0.6445434093475342 + 0.1 * 6.645427703857422
Epoch 210, val loss: 0.8935590982437134
Epoch 220, training loss: 1.2471795082092285 = 0.5835512280464172 + 0.1 * 6.636282444000244
Epoch 220, val loss: 0.8529923558235168
Epoch 230, training loss: 1.1892811059951782 = 0.5269923806190491 + 0.1 * 6.622887134552002
Epoch 230, val loss: 0.8185502290725708
Epoch 240, training loss: 1.1356265544891357 = 0.4748203158378601 + 0.1 * 6.608062744140625
Epoch 240, val loss: 0.7903378009796143
Epoch 250, training loss: 1.08600652217865 = 0.4267677962779999 + 0.1 * 6.592386722564697
Epoch 250, val loss: 0.7679345607757568
Epoch 260, training loss: 1.0397244691848755 = 0.38175076246261597 + 0.1 * 6.579737186431885
Epoch 260, val loss: 0.7500628232955933
Epoch 270, training loss: 0.9966108798980713 = 0.3396400511264801 + 0.1 * 6.569708347320557
Epoch 270, val loss: 0.7363104820251465
Epoch 280, training loss: 0.9564772844314575 = 0.300583153963089 + 0.1 * 6.558940887451172
Epoch 280, val loss: 0.7263784408569336
Epoch 290, training loss: 0.9202755689620972 = 0.2650306522846222 + 0.1 * 6.5524492263793945
Epoch 290, val loss: 0.720217227935791
Epoch 300, training loss: 0.887807309627533 = 0.23339325189590454 + 0.1 * 6.544140338897705
Epoch 300, val loss: 0.7178090810775757
Epoch 310, training loss: 0.8592730164527893 = 0.20555537939071655 + 0.1 * 6.537176132202148
Epoch 310, val loss: 0.7189876437187195
Epoch 320, training loss: 0.8374027013778687 = 0.18145601451396942 + 0.1 * 6.559466361999512
Epoch 320, val loss: 0.7233985662460327
Epoch 330, training loss: 0.8141974210739136 = 0.16112317144870758 + 0.1 * 6.530742645263672
Epoch 330, val loss: 0.7301784753799438
Epoch 340, training loss: 0.7958409190177917 = 0.14382308721542358 + 0.1 * 6.520178318023682
Epoch 340, val loss: 0.7389160394668579
Epoch 350, training loss: 0.780174195766449 = 0.1290224939584732 + 0.1 * 6.51151704788208
Epoch 350, val loss: 0.7492191195487976
Epoch 360, training loss: 0.7667726874351501 = 0.11630057543516159 + 0.1 * 6.504720687866211
Epoch 360, val loss: 0.7607854604721069
Epoch 370, training loss: 0.7551590800285339 = 0.10540514439344406 + 0.1 * 6.497539043426514
Epoch 370, val loss: 0.7728666067123413
Epoch 380, training loss: 0.745152473449707 = 0.09601013362407684 + 0.1 * 6.4914231300354
Epoch 380, val loss: 0.7852792739868164
Epoch 390, training loss: 0.7356840372085571 = 0.08776716887950897 + 0.1 * 6.47916841506958
Epoch 390, val loss: 0.7982162833213806
Epoch 400, training loss: 0.7291726469993591 = 0.08047986775636673 + 0.1 * 6.4869279861450195
Epoch 400, val loss: 0.8109756112098694
Epoch 410, training loss: 0.7206881046295166 = 0.07405031472444534 + 0.1 * 6.4663777351379395
Epoch 410, val loss: 0.824098527431488
Epoch 420, training loss: 0.713773787021637 = 0.0682634562253952 + 0.1 * 6.455102920532227
Epoch 420, val loss: 0.8367756605148315
Epoch 430, training loss: 0.7076709866523743 = 0.06300624459981918 + 0.1 * 6.446647644042969
Epoch 430, val loss: 0.8496333360671997
Epoch 440, training loss: 0.7021675705909729 = 0.05821714922785759 + 0.1 * 6.439504146575928
Epoch 440, val loss: 0.8619827628135681
Epoch 450, training loss: 0.6973006129264832 = 0.05386405810713768 + 0.1 * 6.434365272521973
Epoch 450, val loss: 0.8744974732398987
Epoch 460, training loss: 0.6923491954803467 = 0.04985925555229187 + 0.1 * 6.424899101257324
Epoch 460, val loss: 0.8864777088165283
Epoch 470, training loss: 0.6880504488945007 = 0.04617142677307129 + 0.1 * 6.418790340423584
Epoch 470, val loss: 0.8985320925712585
Epoch 480, training loss: 0.6854650974273682 = 0.04280409961938858 + 0.1 * 6.426609516143799
Epoch 480, val loss: 0.9101719260215759
Epoch 490, training loss: 0.6796541810035706 = 0.03975838050246239 + 0.1 * 6.3989577293396
Epoch 490, val loss: 0.9218102693557739
Epoch 500, training loss: 0.6787378787994385 = 0.03699147701263428 + 0.1 * 6.417463779449463
Epoch 500, val loss: 0.9330424070358276
Epoch 510, training loss: 0.6737027764320374 = 0.03447844088077545 + 0.1 * 6.392243385314941
Epoch 510, val loss: 0.9446651935577393
Epoch 520, training loss: 0.6697421669960022 = 0.03219243139028549 + 0.1 * 6.375497341156006
Epoch 520, val loss: 0.9559187293052673
Epoch 530, training loss: 0.6672589182853699 = 0.03011474944651127 + 0.1 * 6.37144136428833
Epoch 530, val loss: 0.9667624235153198
Epoch 540, training loss: 0.664550244808197 = 0.02824523113667965 + 0.1 * 6.36305046081543
Epoch 540, val loss: 0.9779284596443176
Epoch 550, training loss: 0.6642003059387207 = 0.026546575129032135 + 0.1 * 6.376536846160889
Epoch 550, val loss: 0.9886261820793152
Epoch 560, training loss: 0.6601735353469849 = 0.025001870468258858 + 0.1 * 6.3517165184021
Epoch 560, val loss: 0.9991238117218018
Epoch 570, training loss: 0.6584703922271729 = 0.023588933050632477 + 0.1 * 6.348814010620117
Epoch 570, val loss: 1.0094527006149292
Epoch 580, training loss: 0.6560451984405518 = 0.022292103618383408 + 0.1 * 6.337531089782715
Epoch 580, val loss: 1.0195105075836182
Epoch 590, training loss: 0.6540999412536621 = 0.021100372076034546 + 0.1 * 6.329996109008789
Epoch 590, val loss: 1.0292412042617798
Epoch 600, training loss: 0.6526139974594116 = 0.0200056079775095 + 0.1 * 6.326084136962891
Epoch 600, val loss: 1.0388987064361572
Epoch 610, training loss: 0.6503474712371826 = 0.018992841243743896 + 0.1 * 6.313546180725098
Epoch 610, val loss: 1.0481873750686646
Epoch 620, training loss: 0.64955735206604 = 0.01805892586708069 + 0.1 * 6.31498384475708
Epoch 620, val loss: 1.0568493604660034
Epoch 630, training loss: 0.6472600698471069 = 0.017201388254761696 + 0.1 * 6.300586700439453
Epoch 630, val loss: 1.0657753944396973
Epoch 640, training loss: 0.6459614634513855 = 0.016406016424298286 + 0.1 * 6.2955546379089355
Epoch 640, val loss: 1.0740704536437988
Epoch 650, training loss: 0.6456279158592224 = 0.01567096821963787 + 0.1 * 6.299569606781006
Epoch 650, val loss: 1.0824636220932007
Epoch 660, training loss: 0.6439213156700134 = 0.014986279420554638 + 0.1 * 6.2893500328063965
Epoch 660, val loss: 1.0906583070755005
Epoch 670, training loss: 0.6429281234741211 = 0.01434761006385088 + 0.1 * 6.2858052253723145
Epoch 670, val loss: 1.0984727144241333
Epoch 680, training loss: 0.6426911354064941 = 0.01375254150480032 + 0.1 * 6.289385795593262
Epoch 680, val loss: 1.1060683727264404
Epoch 690, training loss: 0.6415234208106995 = 0.013198887929320335 + 0.1 * 6.283245086669922
Epoch 690, val loss: 1.1137012243270874
Epoch 700, training loss: 0.6394966244697571 = 0.012680447660386562 + 0.1 * 6.268161296844482
Epoch 700, val loss: 1.1210981607437134
Epoch 710, training loss: 0.6406568884849548 = 0.012194054201245308 + 0.1 * 6.284628391265869
Epoch 710, val loss: 1.1281037330627441
Epoch 720, training loss: 0.6388009786605835 = 0.011740290559828281 + 0.1 * 6.270606994628906
Epoch 720, val loss: 1.135093092918396
Epoch 730, training loss: 0.6376076936721802 = 0.011312768794596195 + 0.1 * 6.262948989868164
Epoch 730, val loss: 1.1420338153839111
Epoch 740, training loss: 0.6371594071388245 = 0.010908910073339939 + 0.1 * 6.262505054473877
Epoch 740, val loss: 1.148598074913025
Epoch 750, training loss: 0.6364777684211731 = 0.010527988895773888 + 0.1 * 6.25949764251709
Epoch 750, val loss: 1.1551388502120972
Epoch 760, training loss: 0.6350018978118896 = 0.010169102810323238 + 0.1 * 6.248327732086182
Epoch 760, val loss: 1.1617094278335571
Epoch 770, training loss: 0.6358176469802856 = 0.00982828252017498 + 0.1 * 6.259893417358398
Epoch 770, val loss: 1.168067216873169
Epoch 780, training loss: 0.6354383826255798 = 0.00950647983700037 + 0.1 * 6.259318828582764
Epoch 780, val loss: 1.1739317178726196
Epoch 790, training loss: 0.6335205435752869 = 0.009203191846609116 + 0.1 * 6.243173599243164
Epoch 790, val loss: 1.1798712015151978
Epoch 800, training loss: 0.6338881254196167 = 0.008915629237890244 + 0.1 * 6.249725341796875
Epoch 800, val loss: 1.1859421730041504
Epoch 810, training loss: 0.6316319704055786 = 0.008642482571303844 + 0.1 * 6.229894638061523
Epoch 810, val loss: 1.1917164325714111
Epoch 820, training loss: 0.6311938762664795 = 0.0083821015432477 + 0.1 * 6.228117942810059
Epoch 820, val loss: 1.1972373723983765
Epoch 830, training loss: 0.6323027014732361 = 0.008135353215038776 + 0.1 * 6.241672992706299
Epoch 830, val loss: 1.2026917934417725
Epoch 840, training loss: 0.6303017139434814 = 0.007900035008788109 + 0.1 * 6.224016189575195
Epoch 840, val loss: 1.2081820964813232
Epoch 850, training loss: 0.6306669116020203 = 0.007675874046981335 + 0.1 * 6.229910373687744
Epoch 850, val loss: 1.213621735572815
Epoch 860, training loss: 0.6287519931793213 = 0.007461738307029009 + 0.1 * 6.212902545928955
Epoch 860, val loss: 1.2187447547912598
Epoch 870, training loss: 0.6320307850837708 = 0.007257821969687939 + 0.1 * 6.247729778289795
Epoch 870, val loss: 1.2237985134124756
Epoch 880, training loss: 0.629658043384552 = 0.007063413504511118 + 0.1 * 6.225946426391602
Epoch 880, val loss: 1.2288424968719482
Epoch 890, training loss: 0.6283659338951111 = 0.006878000218421221 + 0.1 * 6.214879035949707
Epoch 890, val loss: 1.2339506149291992
Epoch 900, training loss: 0.628400981426239 = 0.006699754390865564 + 0.1 * 6.217012405395508
Epoch 900, val loss: 1.2387068271636963
Epoch 910, training loss: 0.6271874308586121 = 0.006529472302645445 + 0.1 * 6.206579685211182
Epoch 910, val loss: 1.2434684038162231
Epoch 920, training loss: 0.628383994102478 = 0.006366207264363766 + 0.1 * 6.220178127288818
Epoch 920, val loss: 1.2482552528381348
Epoch 930, training loss: 0.6263769865036011 = 0.006209125276654959 + 0.1 * 6.201678276062012
Epoch 930, val loss: 1.2526861429214478
Epoch 940, training loss: 0.6266711354255676 = 0.006058895494788885 + 0.1 * 6.206122398376465
Epoch 940, val loss: 1.257300853729248
Epoch 950, training loss: 0.6266831755638123 = 0.005914937239140272 + 0.1 * 6.2076826095581055
Epoch 950, val loss: 1.2616349458694458
Epoch 960, training loss: 0.6252360343933105 = 0.005776204168796539 + 0.1 * 6.194598197937012
Epoch 960, val loss: 1.2661019563674927
Epoch 970, training loss: 0.6254974007606506 = 0.005643032491207123 + 0.1 * 6.198544025421143
Epoch 970, val loss: 1.2703617811203003
Epoch 980, training loss: 0.625176727771759 = 0.005514496471732855 + 0.1 * 6.196621894836426
Epoch 980, val loss: 1.2747224569320679
Epoch 990, training loss: 0.6267527937889099 = 0.005391109734773636 + 0.1 * 6.213616371154785
Epoch 990, val loss: 1.2788128852844238
Epoch 1000, training loss: 0.6243889927864075 = 0.005272993817925453 + 0.1 * 6.191160202026367
Epoch 1000, val loss: 1.2828375101089478
Epoch 1010, training loss: 0.6256869435310364 = 0.005158987361937761 + 0.1 * 6.20527982711792
Epoch 1010, val loss: 1.2870831489562988
Epoch 1020, training loss: 0.6233120560646057 = 0.005048924591392279 + 0.1 * 6.182631015777588
Epoch 1020, val loss: 1.2909326553344727
Epoch 1030, training loss: 0.6239899396896362 = 0.0049433233216404915 + 0.1 * 6.190466403961182
Epoch 1030, val loss: 1.294891595840454
Epoch 1040, training loss: 0.623041570186615 = 0.004840855486690998 + 0.1 * 6.182007312774658
Epoch 1040, val loss: 1.2989567518234253
Epoch 1050, training loss: 0.6229546070098877 = 0.004741749260574579 + 0.1 * 6.18212890625
Epoch 1050, val loss: 1.3026286363601685
Epoch 1060, training loss: 0.6231189966201782 = 0.004646326880902052 + 0.1 * 6.184726715087891
Epoch 1060, val loss: 1.3064639568328857
Epoch 1070, training loss: 0.6227912306785583 = 0.004553735256195068 + 0.1 * 6.182374954223633
Epoch 1070, val loss: 1.3101691007614136
Epoch 1080, training loss: 0.6223231554031372 = 0.0044645387679338455 + 0.1 * 6.178586006164551
Epoch 1080, val loss: 1.3138396739959717
Epoch 1090, training loss: 0.6218438744544983 = 0.004378415644168854 + 0.1 * 6.174654483795166
Epoch 1090, val loss: 1.317475438117981
Epoch 1100, training loss: 0.6223819851875305 = 0.004294791724532843 + 0.1 * 6.180871486663818
Epoch 1100, val loss: 1.3211311101913452
Epoch 1110, training loss: 0.6209152340888977 = 0.004214318003505468 + 0.1 * 6.167008876800537
Epoch 1110, val loss: 1.3246111869812012
Epoch 1120, training loss: 0.621123194694519 = 0.004136021714657545 + 0.1 * 6.1698713302612305
Epoch 1120, val loss: 1.3280421495437622
Epoch 1130, training loss: 0.6219651699066162 = 0.004060695879161358 + 0.1 * 6.179044723510742
Epoch 1130, val loss: 1.3313162326812744
Epoch 1140, training loss: 0.6199899911880493 = 0.003987252712249756 + 0.1 * 6.160027027130127
Epoch 1140, val loss: 1.3346277475357056
Epoch 1150, training loss: 0.6225032210350037 = 0.003916471730917692 + 0.1 * 6.185867786407471
Epoch 1150, val loss: 1.3380073308944702
Epoch 1160, training loss: 0.6205579042434692 = 0.00384733360260725 + 0.1 * 6.167105197906494
Epoch 1160, val loss: 1.341127872467041
Epoch 1170, training loss: 0.6207172870635986 = 0.0037806646432727575 + 0.1 * 6.169366359710693
Epoch 1170, val loss: 1.344399094581604
Epoch 1180, training loss: 0.6205484867095947 = 0.003715770086273551 + 0.1 * 6.168327331542969
Epoch 1180, val loss: 1.347631812095642
Epoch 1190, training loss: 0.6193270087242126 = 0.003652840154245496 + 0.1 * 6.156742095947266
Epoch 1190, val loss: 1.3507965803146362
Epoch 1200, training loss: 0.6210689544677734 = 0.0035914848558604717 + 0.1 * 6.174774646759033
Epoch 1200, val loss: 1.3539711236953735
Epoch 1210, training loss: 0.619215726852417 = 0.0035322559997439384 + 0.1 * 6.156834602355957
Epoch 1210, val loss: 1.3568812608718872
Epoch 1220, training loss: 0.6207512617111206 = 0.003474367083981633 + 0.1 * 6.172769069671631
Epoch 1220, val loss: 1.3599961996078491
Epoch 1230, training loss: 0.6190118789672852 = 0.003418430220335722 + 0.1 * 6.1559343338012695
Epoch 1230, val loss: 1.3627649545669556
Epoch 1240, training loss: 0.6199964284896851 = 0.0033640393521636724 + 0.1 * 6.166323661804199
Epoch 1240, val loss: 1.3658114671707153
Epoch 1250, training loss: 0.6174845099449158 = 0.003311161184683442 + 0.1 * 6.141733646392822
Epoch 1250, val loss: 1.368680715560913
Epoch 1260, training loss: 0.6187983751296997 = 0.003259856952354312 + 0.1 * 6.155385494232178
Epoch 1260, val loss: 1.3716202974319458
Epoch 1270, training loss: 0.6180709004402161 = 0.003209830494597554 + 0.1 * 6.148610591888428
Epoch 1270, val loss: 1.3744769096374512
Epoch 1280, training loss: 0.6188231706619263 = 0.003160960040986538 + 0.1 * 6.156621932983398
Epoch 1280, val loss: 1.3771154880523682
Epoch 1290, training loss: 0.6174382567405701 = 0.003113523358479142 + 0.1 * 6.143247604370117
Epoch 1290, val loss: 1.3799289464950562
Epoch 1300, training loss: 0.6173187494277954 = 0.0030673795845359564 + 0.1 * 6.142513751983643
Epoch 1300, val loss: 1.3826978206634521
Epoch 1310, training loss: 0.6200268864631653 = 0.0030223485082387924 + 0.1 * 6.170044898986816
Epoch 1310, val loss: 1.3854542970657349
Epoch 1320, training loss: 0.6172422170639038 = 0.002978425705805421 + 0.1 * 6.142637729644775
Epoch 1320, val loss: 1.3880696296691895
Epoch 1330, training loss: 0.6168846487998962 = 0.002935634693130851 + 0.1 * 6.139490127563477
Epoch 1330, val loss: 1.3906328678131104
Epoch 1340, training loss: 0.6164782643318176 = 0.002893999917432666 + 0.1 * 6.135842800140381
Epoch 1340, val loss: 1.3933465480804443
Epoch 1350, training loss: 0.6173690557479858 = 0.002853326965123415 + 0.1 * 6.1451568603515625
Epoch 1350, val loss: 1.3958758115768433
Epoch 1360, training loss: 0.6169302463531494 = 0.0028134763706475496 + 0.1 * 6.141167640686035
Epoch 1360, val loss: 1.398500919342041
Epoch 1370, training loss: 0.6193411350250244 = 0.002774761524051428 + 0.1 * 6.165663719177246
Epoch 1370, val loss: 1.4007480144500732
Epoch 1380, training loss: 0.6165151000022888 = 0.0027371698524802923 + 0.1 * 6.137779235839844
Epoch 1380, val loss: 1.4031777381896973
Epoch 1390, training loss: 0.6152828931808472 = 0.002700402634218335 + 0.1 * 6.125824928283691
Epoch 1390, val loss: 1.4056799411773682
Epoch 1400, training loss: 0.6163174510002136 = 0.0026646885089576244 + 0.1 * 6.1365275382995605
Epoch 1400, val loss: 1.4083093404769897
Epoch 1410, training loss: 0.6158019304275513 = 0.002629399998113513 + 0.1 * 6.131725311279297
Epoch 1410, val loss: 1.410767674446106
Epoch 1420, training loss: 0.6169830560684204 = 0.00259515387006104 + 0.1 * 6.143879413604736
Epoch 1420, val loss: 1.413092017173767
Epoch 1430, training loss: 0.6156647801399231 = 0.0025615403428673744 + 0.1 * 6.1310319900512695
Epoch 1430, val loss: 1.4153186082839966
Epoch 1440, training loss: 0.61514812707901 = 0.002528737299144268 + 0.1 * 6.126194000244141
Epoch 1440, val loss: 1.4177985191345215
Epoch 1450, training loss: 0.6155504584312439 = 0.0024967193603515625 + 0.1 * 6.130537033081055
Epoch 1450, val loss: 1.4201282262802124
Epoch 1460, training loss: 0.6172835230827332 = 0.0024654814042150974 + 0.1 * 6.14818000793457
Epoch 1460, val loss: 1.4223092794418335
Epoch 1470, training loss: 0.6145284175872803 = 0.0024348266888409853 + 0.1 * 6.120935440063477
Epoch 1470, val loss: 1.4244351387023926
Epoch 1480, training loss: 0.6159098148345947 = 0.002405116567388177 + 0.1 * 6.13504695892334
Epoch 1480, val loss: 1.4267522096633911
Epoch 1490, training loss: 0.6143782734870911 = 0.002375942189246416 + 0.1 * 6.120023250579834
Epoch 1490, val loss: 1.4290698766708374
Epoch 1500, training loss: 0.6162796020507812 = 0.002347504021599889 + 0.1 * 6.1393208503723145
Epoch 1500, val loss: 1.4313238859176636
Epoch 1510, training loss: 0.6151080131530762 = 0.0023194006644189358 + 0.1 * 6.127885818481445
Epoch 1510, val loss: 1.4333571195602417
Epoch 1520, training loss: 0.6164218187332153 = 0.0022921101190149784 + 0.1 * 6.141296863555908
Epoch 1520, val loss: 1.435589075088501
Epoch 1530, training loss: 0.6150133609771729 = 0.00226527894847095 + 0.1 * 6.127480506896973
Epoch 1530, val loss: 1.4375536441802979
Epoch 1540, training loss: 0.6137351393699646 = 0.0022391071543097496 + 0.1 * 6.114960193634033
Epoch 1540, val loss: 1.4397035837173462
Epoch 1550, training loss: 0.6151578426361084 = 0.002213546307757497 + 0.1 * 6.129443168640137
Epoch 1550, val loss: 1.4419320821762085
Epoch 1560, training loss: 0.6142653226852417 = 0.002188364276662469 + 0.1 * 6.120769023895264
Epoch 1560, val loss: 1.4439841508865356
Epoch 1570, training loss: 0.6143333911895752 = 0.0021637133322656155 + 0.1 * 6.121696472167969
Epoch 1570, val loss: 1.4459761381149292
Epoch 1580, training loss: 0.6148196458816528 = 0.0021396938245743513 + 0.1 * 6.126799583435059
Epoch 1580, val loss: 1.447943091392517
Epoch 1590, training loss: 0.613376796245575 = 0.002115942072123289 + 0.1 * 6.112607955932617
Epoch 1590, val loss: 1.4501405954360962
Epoch 1600, training loss: 0.6139833331108093 = 0.0020928792655467987 + 0.1 * 6.118904113769531
Epoch 1600, val loss: 1.4523088932037354
Epoch 1610, training loss: 0.6137866377830505 = 0.0020699896849691868 + 0.1 * 6.117166042327881
Epoch 1610, val loss: 1.4542171955108643
Epoch 1620, training loss: 0.6133045554161072 = 0.0020477117504924536 + 0.1 * 6.112568378448486
Epoch 1620, val loss: 1.4560673236846924
Epoch 1630, training loss: 0.6143724918365479 = 0.0020259995944797993 + 0.1 * 6.123464584350586
Epoch 1630, val loss: 1.458133339881897
Epoch 1640, training loss: 0.6138517260551453 = 0.0020045912824571133 + 0.1 * 6.118471622467041
Epoch 1640, val loss: 1.4601147174835205
Epoch 1650, training loss: 0.6130499839782715 = 0.001983685651794076 + 0.1 * 6.110662937164307
Epoch 1650, val loss: 1.4620682001113892
Epoch 1660, training loss: 0.6142878532409668 = 0.001963099930435419 + 0.1 * 6.1232476234436035
Epoch 1660, val loss: 1.4639534950256348
Epoch 1670, training loss: 0.6136070489883423 = 0.0019429773092269897 + 0.1 * 6.116640567779541
Epoch 1670, val loss: 1.4658074378967285
Epoch 1680, training loss: 0.6128782629966736 = 0.0019231674959883094 + 0.1 * 6.109550952911377
Epoch 1680, val loss: 1.4677948951721191
Epoch 1690, training loss: 0.6126721501350403 = 0.0019037381280213594 + 0.1 * 6.107684135437012
Epoch 1690, val loss: 1.4697049856185913
Epoch 1700, training loss: 0.613304615020752 = 0.0018847879255190492 + 0.1 * 6.114198207855225
Epoch 1700, val loss: 1.4715732336044312
Epoch 1710, training loss: 0.6135331988334656 = 0.0018660943023860455 + 0.1 * 6.116670608520508
Epoch 1710, val loss: 1.47341787815094
Epoch 1720, training loss: 0.6123054623603821 = 0.0018476429395377636 + 0.1 * 6.104578018188477
Epoch 1720, val loss: 1.4752206802368164
Epoch 1730, training loss: 0.6143770813941956 = 0.0018296850612387061 + 0.1 * 6.125473976135254
Epoch 1730, val loss: 1.477089524269104
Epoch 1740, training loss: 0.6125728487968445 = 0.0018119665328413248 + 0.1 * 6.107608795166016
Epoch 1740, val loss: 1.4788655042648315
Epoch 1750, training loss: 0.6125218868255615 = 0.0017945721046999097 + 0.1 * 6.107273101806641
Epoch 1750, val loss: 1.4806979894638062
Epoch 1760, training loss: 0.6128340363502502 = 0.001777654280886054 + 0.1 * 6.1105637550354
Epoch 1760, val loss: 1.482483983039856
Epoch 1770, training loss: 0.6124487519264221 = 0.0017608640482649207 + 0.1 * 6.106879234313965
Epoch 1770, val loss: 1.4842536449432373
Epoch 1780, training loss: 0.6128983497619629 = 0.0017443652031943202 + 0.1 * 6.111539363861084
Epoch 1780, val loss: 1.48605215549469
Epoch 1790, training loss: 0.6116114258766174 = 0.0017281887121498585 + 0.1 * 6.098832130432129
Epoch 1790, val loss: 1.487784504890442
Epoch 1800, training loss: 0.6120665073394775 = 0.0017122292192652822 + 0.1 * 6.103542804718018
Epoch 1800, val loss: 1.4895905256271362
Epoch 1810, training loss: 0.6118823885917664 = 0.00169661408290267 + 0.1 * 6.101858139038086
Epoch 1810, val loss: 1.491289734840393
Epoch 1820, training loss: 0.6121644973754883 = 0.0016812614630907774 + 0.1 * 6.104832172393799
Epoch 1820, val loss: 1.4928642511367798
Epoch 1830, training loss: 0.6122864484786987 = 0.001666117226704955 + 0.1 * 6.106203079223633
Epoch 1830, val loss: 1.4946227073669434
Epoch 1840, training loss: 0.6121480464935303 = 0.0016513398149982095 + 0.1 * 6.104966640472412
Epoch 1840, val loss: 1.4963449239730835
Epoch 1850, training loss: 0.6113519668579102 = 0.0016367411008104682 + 0.1 * 6.097152233123779
Epoch 1850, val loss: 1.4980077743530273
Epoch 1860, training loss: 0.6128145456314087 = 0.001622467185370624 + 0.1 * 6.1119208335876465
Epoch 1860, val loss: 1.4997212886810303
Epoch 1870, training loss: 0.610722005367279 = 0.0016083430964499712 + 0.1 * 6.0911359786987305
Epoch 1870, val loss: 1.5013799667358398
Epoch 1880, training loss: 0.6123160123825073 = 0.0015945328632369637 + 0.1 * 6.107214450836182
Epoch 1880, val loss: 1.503143310546875
Epoch 1890, training loss: 0.6117811799049377 = 0.001580876880325377 + 0.1 * 6.1020026206970215
Epoch 1890, val loss: 1.5046714544296265
Epoch 1900, training loss: 0.6125449538230896 = 0.0015674687456339598 + 0.1 * 6.109774589538574
Epoch 1900, val loss: 1.506232500076294
Epoch 1910, training loss: 0.6106349229812622 = 0.0015542098553851247 + 0.1 * 6.09080696105957
Epoch 1910, val loss: 1.5078437328338623
Epoch 1920, training loss: 0.6118564605712891 = 0.0015411963686347008 + 0.1 * 6.103152275085449
Epoch 1920, val loss: 1.5096138715744019
Epoch 1930, training loss: 0.6125630140304565 = 0.0015284685650840402 + 0.1 * 6.110345363616943
Epoch 1930, val loss: 1.5111128091812134
Epoch 1940, training loss: 0.6104428172111511 = 0.0015158826718106866 + 0.1 * 6.089269161224365
Epoch 1940, val loss: 1.5127530097961426
Epoch 1950, training loss: 0.6119192242622375 = 0.0015035768738016486 + 0.1 * 6.104156494140625
Epoch 1950, val loss: 1.5143625736236572
Epoch 1960, training loss: 0.6106816530227661 = 0.0014913695631548762 + 0.1 * 6.091902732849121
Epoch 1960, val loss: 1.5159132480621338
Epoch 1970, training loss: 0.6112344264984131 = 0.0014794383896514773 + 0.1 * 6.097549915313721
Epoch 1970, val loss: 1.5174977779388428
Epoch 1980, training loss: 0.6104269623756409 = 0.0014676518039777875 + 0.1 * 6.089592933654785
Epoch 1980, val loss: 1.5190293788909912
Epoch 1990, training loss: 0.6116273403167725 = 0.0014560564886778593 + 0.1 * 6.101712703704834
Epoch 1990, val loss: 1.5206916332244873
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8266
Flip ASR: 0.7956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7757925987243652 = 1.9384047985076904 + 0.1 * 8.373878479003906
Epoch 0, val loss: 1.9343338012695312
Epoch 10, training loss: 2.765509605407715 = 1.928135871887207 + 0.1 * 8.373738288879395
Epoch 10, val loss: 1.924383521080017
Epoch 20, training loss: 2.7527904510498047 = 1.9155179262161255 + 0.1 * 8.372724533081055
Epoch 20, val loss: 1.911631464958191
Epoch 30, training loss: 2.733962059020996 = 1.8976777791976929 + 0.1 * 8.362841606140137
Epoch 30, val loss: 1.893257975578308
Epoch 40, training loss: 2.698958396911621 = 1.8715276718139648 + 0.1 * 8.274307250976562
Epoch 40, val loss: 1.8664976358413696
Epoch 50, training loss: 2.6102793216705322 = 1.8377797603607178 + 0.1 * 7.724995136260986
Epoch 50, val loss: 1.8337230682373047
Epoch 60, training loss: 2.546318769454956 = 1.8036856651306152 + 0.1 * 7.426330089569092
Epoch 60, val loss: 1.8034753799438477
Epoch 70, training loss: 2.4751672744750977 = 1.7717689275741577 + 0.1 * 7.0339837074279785
Epoch 70, val loss: 1.776911973953247
Epoch 80, training loss: 2.4146485328674316 = 1.7373110055923462 + 0.1 * 6.773375034332275
Epoch 80, val loss: 1.7482731342315674
Epoch 90, training loss: 2.35787296295166 = 1.6906980276107788 + 0.1 * 6.671750545501709
Epoch 90, val loss: 1.708358645439148
Epoch 100, training loss: 2.290210485458374 = 1.627322793006897 + 0.1 * 6.628876209259033
Epoch 100, val loss: 1.6538312435150146
Epoch 110, training loss: 2.206738233566284 = 1.546738624572754 + 0.1 * 6.599996566772461
Epoch 110, val loss: 1.5856616497039795
Epoch 120, training loss: 2.1116604804992676 = 1.4538990259170532 + 0.1 * 6.5776143074035645
Epoch 120, val loss: 1.5094953775405884
Epoch 130, training loss: 2.0107593536376953 = 1.3551336526870728 + 0.1 * 6.55625581741333
Epoch 130, val loss: 1.4311846494674683
Epoch 140, training loss: 1.9082415103912354 = 1.2544134855270386 + 0.1 * 6.538280010223389
Epoch 140, val loss: 1.3530305624008179
Epoch 150, training loss: 1.8066802024841309 = 1.1545997858047485 + 0.1 * 6.520804405212402
Epoch 150, val loss: 1.2768406867980957
Epoch 160, training loss: 1.7091012001037598 = 1.0585204362869263 + 0.1 * 6.505807876586914
Epoch 160, val loss: 1.2035447359085083
Epoch 170, training loss: 1.6176899671554565 = 0.9685534834861755 + 0.1 * 6.491364479064941
Epoch 170, val loss: 1.1356838941574097
Epoch 180, training loss: 1.5319228172302246 = 0.884233295917511 + 0.1 * 6.476894855499268
Epoch 180, val loss: 1.0733134746551514
Epoch 190, training loss: 1.4509156942367554 = 0.8043625354766846 + 0.1 * 6.465531349182129
Epoch 190, val loss: 1.015557885169983
Epoch 200, training loss: 1.374467372894287 = 0.7293435335159302 + 0.1 * 6.451239109039307
Epoch 200, val loss: 0.9630861878395081
Epoch 210, training loss: 1.305182695388794 = 0.6604594588279724 + 0.1 * 6.447232246398926
Epoch 210, val loss: 0.916544497013092
Epoch 220, training loss: 1.242741346359253 = 0.6000152826309204 + 0.1 * 6.427260875701904
Epoch 220, val loss: 0.878211498260498
Epoch 230, training loss: 1.1895689964294434 = 0.5477322936058044 + 0.1 * 6.418367385864258
Epoch 230, val loss: 0.8483202457427979
Epoch 240, training loss: 1.1442795991897583 = 0.5033237338066101 + 0.1 * 6.409558296203613
Epoch 240, val loss: 0.8263956904411316
Epoch 250, training loss: 1.1033800840377808 = 0.464753657579422 + 0.1 * 6.386264324188232
Epoch 250, val loss: 0.8103033900260925
Epoch 260, training loss: 1.0680214166641235 = 0.4298866093158722 + 0.1 * 6.38134765625
Epoch 260, val loss: 0.7979522943496704
Epoch 270, training loss: 1.0355968475341797 = 0.39768844842910767 + 0.1 * 6.379083633422852
Epoch 270, val loss: 0.7881196141242981
Epoch 280, training loss: 1.002626657485962 = 0.36707794666290283 + 0.1 * 6.3554863929748535
Epoch 280, val loss: 0.7800632119178772
Epoch 290, training loss: 0.9726430177688599 = 0.3374328315258026 + 0.1 * 6.352102279663086
Epoch 290, val loss: 0.7734364867210388
Epoch 300, training loss: 0.943234920501709 = 0.3090624511241913 + 0.1 * 6.341724395751953
Epoch 300, val loss: 0.7682311534881592
Epoch 310, training loss: 0.9155540466308594 = 0.28218308091163635 + 0.1 * 6.333709239959717
Epoch 310, val loss: 0.7643164396286011
Epoch 320, training loss: 0.8905476927757263 = 0.25681912899017334 + 0.1 * 6.33728551864624
Epoch 320, val loss: 0.7616900205612183
Epoch 330, training loss: 0.8652513027191162 = 0.23315952718257904 + 0.1 * 6.32091760635376
Epoch 330, val loss: 0.7603241205215454
Epoch 340, training loss: 0.8433226943016052 = 0.2112141102552414 + 0.1 * 6.321085453033447
Epoch 340, val loss: 0.7599870562553406
Epoch 350, training loss: 0.8219410181045532 = 0.19107016921043396 + 0.1 * 6.308708190917969
Epoch 350, val loss: 0.7606737017631531
Epoch 360, training loss: 0.8029499650001526 = 0.17272235453128815 + 0.1 * 6.302276134490967
Epoch 360, val loss: 0.7624276876449585
Epoch 370, training loss: 0.7875992059707642 = 0.15610243380069733 + 0.1 * 6.314967632293701
Epoch 370, val loss: 0.7652992010116577
Epoch 380, training loss: 0.7704451084136963 = 0.14125558733940125 + 0.1 * 6.291894912719727
Epoch 380, val loss: 0.7691912055015564
Epoch 390, training loss: 0.7567139863967896 = 0.12802667915821075 + 0.1 * 6.2868733406066895
Epoch 390, val loss: 0.7740728259086609
Epoch 400, training loss: 0.7451068162918091 = 0.11623433232307434 + 0.1 * 6.288724899291992
Epoch 400, val loss: 0.7799943685531616
Epoch 410, training loss: 0.733711838722229 = 0.10576972365379333 + 0.1 * 6.279420852661133
Epoch 410, val loss: 0.78675776720047
Epoch 420, training loss: 0.7266073226928711 = 0.09647887945175171 + 0.1 * 6.301284313201904
Epoch 420, val loss: 0.7943289875984192
Epoch 430, training loss: 0.7148808240890503 = 0.08828052878379822 + 0.1 * 6.266002655029297
Epoch 430, val loss: 0.802399218082428
Epoch 440, training loss: 0.7069779634475708 = 0.08096430450677872 + 0.1 * 6.260136127471924
Epoch 440, val loss: 0.8110905289649963
Epoch 450, training loss: 0.6998370289802551 = 0.07441451400518417 + 0.1 * 6.254225254058838
Epoch 450, val loss: 0.8202557563781738
Epoch 460, training loss: 0.6937305331230164 = 0.06853639334440231 + 0.1 * 6.251941204071045
Epoch 460, val loss: 0.8296880125999451
Epoch 470, training loss: 0.6896483302116394 = 0.06323268264532089 + 0.1 * 6.264156818389893
Epoch 470, val loss: 0.8394786715507507
Epoch 480, training loss: 0.6839552521705627 = 0.058461062610149384 + 0.1 * 6.254941463470459
Epoch 480, val loss: 0.8493933081626892
Epoch 490, training loss: 0.6790446043014526 = 0.05414940416812897 + 0.1 * 6.2489519119262695
Epoch 490, val loss: 0.8594157099723816
Epoch 500, training loss: 0.673383891582489 = 0.050250764936208725 + 0.1 * 6.2313313484191895
Epoch 500, val loss: 0.8694606423377991
Epoch 510, training loss: 0.6713274121284485 = 0.04671264439821243 + 0.1 * 6.246147632598877
Epoch 510, val loss: 0.8795427680015564
Epoch 520, training loss: 0.6674241423606873 = 0.04351609945297241 + 0.1 * 6.239080429077148
Epoch 520, val loss: 0.8895676136016846
Epoch 530, training loss: 0.6638672947883606 = 0.04062037914991379 + 0.1 * 6.232469081878662
Epoch 530, val loss: 0.8994200229644775
Epoch 540, training loss: 0.6607223153114319 = 0.03799064829945564 + 0.1 * 6.227316856384277
Epoch 540, val loss: 0.9091823697090149
Epoch 550, training loss: 0.6572890281677246 = 0.03559936583042145 + 0.1 * 6.2168965339660645
Epoch 550, val loss: 0.9187189340591431
Epoch 560, training loss: 0.6555809378623962 = 0.033414192497730255 + 0.1 * 6.221667289733887
Epoch 560, val loss: 0.9281560182571411
Epoch 570, training loss: 0.653415322303772 = 0.03141697496175766 + 0.1 * 6.2199835777282715
Epoch 570, val loss: 0.9374775886535645
Epoch 580, training loss: 0.6507443189620972 = 0.029595308005809784 + 0.1 * 6.211489677429199
Epoch 580, val loss: 0.9465431571006775
Epoch 590, training loss: 0.6488942503929138 = 0.027925029397010803 + 0.1 * 6.209692478179932
Epoch 590, val loss: 0.9554391503334045
Epoch 600, training loss: 0.6484081149101257 = 0.026389410719275475 + 0.1 * 6.220186710357666
Epoch 600, val loss: 0.9641996026039124
Epoch 610, training loss: 0.6454406976699829 = 0.024980813264846802 + 0.1 * 6.204598426818848
Epoch 610, val loss: 0.9726882576942444
Epoch 620, training loss: 0.6451135277748108 = 0.02368190698325634 + 0.1 * 6.214316368103027
Epoch 620, val loss: 0.9809653759002686
Epoch 630, training loss: 0.6424453258514404 = 0.022481141611933708 + 0.1 * 6.199641704559326
Epoch 630, val loss: 0.9891037344932556
Epoch 640, training loss: 0.6413347125053406 = 0.02137048728764057 + 0.1 * 6.199642181396484
Epoch 640, val loss: 0.9970331192016602
Epoch 650, training loss: 0.6401064395904541 = 0.020339448004961014 + 0.1 * 6.197669982910156
Epoch 650, val loss: 1.0048481225967407
Epoch 660, training loss: 0.6389383673667908 = 0.019382938742637634 + 0.1 * 6.195553779602051
Epoch 660, val loss: 1.0124540328979492
Epoch 670, training loss: 0.6384180188179016 = 0.018490927293896675 + 0.1 * 6.199270725250244
Epoch 670, val loss: 1.0199401378631592
Epoch 680, training loss: 0.637090265750885 = 0.017662357538938522 + 0.1 * 6.194279193878174
Epoch 680, val loss: 1.0272815227508545
Epoch 690, training loss: 0.6360647678375244 = 0.016890259459614754 + 0.1 * 6.191744804382324
Epoch 690, val loss: 1.03440260887146
Epoch 700, training loss: 0.6337978839874268 = 0.01616913452744484 + 0.1 * 6.1762871742248535
Epoch 700, val loss: 1.041468858718872
Epoch 710, training loss: 0.6328635215759277 = 0.015494591556489468 + 0.1 * 6.173689365386963
Epoch 710, val loss: 1.0483616590499878
Epoch 720, training loss: 0.6355578899383545 = 0.014861068688333035 + 0.1 * 6.206968307495117
Epoch 720, val loss: 1.055179476737976
Epoch 730, training loss: 0.6325193047523499 = 0.014268237166106701 + 0.1 * 6.182510852813721
Epoch 730, val loss: 1.0618603229522705
Epoch 740, training loss: 0.6320334672927856 = 0.01371333934366703 + 0.1 * 6.183200836181641
Epoch 740, val loss: 1.0683671236038208
Epoch 750, training loss: 0.6295990347862244 = 0.01319094467908144 + 0.1 * 6.16408109664917
Epoch 750, val loss: 1.0747718811035156
Epoch 760, training loss: 0.6297976970672607 = 0.012698711827397346 + 0.1 * 6.170989990234375
Epoch 760, val loss: 1.0810585021972656
Epoch 770, training loss: 0.6295571327209473 = 0.012233894318342209 + 0.1 * 6.173232078552246
Epoch 770, val loss: 1.087267518043518
Epoch 780, training loss: 0.6301641464233398 = 0.011795778758823872 + 0.1 * 6.1836838722229
Epoch 780, val loss: 1.0933583974838257
Epoch 790, training loss: 0.6281753182411194 = 0.011382351629436016 + 0.1 * 6.167929649353027
Epoch 790, val loss: 1.099338173866272
Epoch 800, training loss: 0.6264615058898926 = 0.01099258940666914 + 0.1 * 6.154688835144043
Epoch 800, val loss: 1.10518217086792
Epoch 810, training loss: 0.6278945207595825 = 0.010622262954711914 + 0.1 * 6.172722339630127
Epoch 810, val loss: 1.1109716892242432
Epoch 820, training loss: 0.6267990469932556 = 0.010271862149238586 + 0.1 * 6.165271759033203
Epoch 820, val loss: 1.1166741847991943
Epoch 830, training loss: 0.6270313858985901 = 0.009940452873706818 + 0.1 * 6.170909404754639
Epoch 830, val loss: 1.1222317218780518
Epoch 840, training loss: 0.6253098249435425 = 0.009625762701034546 + 0.1 * 6.1568403244018555
Epoch 840, val loss: 1.1277319192886353
Epoch 850, training loss: 0.6248877048492432 = 0.009327699430286884 + 0.1 * 6.155600070953369
Epoch 850, val loss: 1.133086085319519
Epoch 860, training loss: 0.6240569949150085 = 0.009043321013450623 + 0.1 * 6.150136470794678
Epoch 860, val loss: 1.1384022235870361
Epoch 870, training loss: 0.6236358284950256 = 0.008772452361881733 + 0.1 * 6.14863395690918
Epoch 870, val loss: 1.1436151266098022
Epoch 880, training loss: 0.6227468252182007 = 0.008514710702002048 + 0.1 * 6.1423211097717285
Epoch 880, val loss: 1.1487659215927124
Epoch 890, training loss: 0.6227645874023438 = 0.008269567973911762 + 0.1 * 6.1449503898620605
Epoch 890, val loss: 1.153803825378418
Epoch 900, training loss: 0.6252179145812988 = 0.008035647682845592 + 0.1 * 6.171822547912598
Epoch 900, val loss: 1.1587896347045898
Epoch 910, training loss: 0.6227137446403503 = 0.007812128867954016 + 0.1 * 6.1490159034729
Epoch 910, val loss: 1.1637004613876343
Epoch 920, training loss: 0.6224405169487 = 0.007599348668009043 + 0.1 * 6.148411273956299
Epoch 920, val loss: 1.1684924364089966
Epoch 930, training loss: 0.6220158934593201 = 0.007395368069410324 + 0.1 * 6.146204948425293
Epoch 930, val loss: 1.1732515096664429
Epoch 940, training loss: 0.6208049654960632 = 0.007200288586318493 + 0.1 * 6.136046409606934
Epoch 940, val loss: 1.177907943725586
Epoch 950, training loss: 0.6205112934112549 = 0.0070132543332874775 + 0.1 * 6.134980201721191
Epoch 950, val loss: 1.1825300455093384
Epoch 960, training loss: 0.6208813786506653 = 0.006834566593170166 + 0.1 * 6.140468120574951
Epoch 960, val loss: 1.1870540380477905
Epoch 970, training loss: 0.619358241558075 = 0.006663088221102953 + 0.1 * 6.126951217651367
Epoch 970, val loss: 1.191552996635437
Epoch 980, training loss: 0.6214340329170227 = 0.006499209441244602 + 0.1 * 6.14934778213501
Epoch 980, val loss: 1.1959383487701416
Epoch 990, training loss: 0.6191043257713318 = 0.006341859698295593 + 0.1 * 6.12762451171875
Epoch 990, val loss: 1.200289011001587
Epoch 1000, training loss: 0.619613528251648 = 0.006190864834934473 + 0.1 * 6.1342267990112305
Epoch 1000, val loss: 1.2045310735702515
Epoch 1010, training loss: 0.6196591854095459 = 0.0060455938801169395 + 0.1 * 6.136136054992676
Epoch 1010, val loss: 1.2087514400482178
Epoch 1020, training loss: 0.6185935139656067 = 0.0059056393802165985 + 0.1 * 6.12687873840332
Epoch 1020, val loss: 1.2129344940185547
Epoch 1030, training loss: 0.6191189289093018 = 0.0057714879512786865 + 0.1 * 6.133474826812744
Epoch 1030, val loss: 1.2170192003250122
Epoch 1040, training loss: 0.6184197068214417 = 0.005642344709485769 + 0.1 * 6.127773284912109
Epoch 1040, val loss: 1.2210843563079834
Epoch 1050, training loss: 0.6182947158813477 = 0.005517906509339809 + 0.1 * 6.127768039703369
Epoch 1050, val loss: 1.2250553369522095
Epoch 1060, training loss: 0.6185159683227539 = 0.005397843196988106 + 0.1 * 6.131181240081787
Epoch 1060, val loss: 1.2290228605270386
Epoch 1070, training loss: 0.6172218322753906 = 0.005282544065266848 + 0.1 * 6.1193928718566895
Epoch 1070, val loss: 1.232910394668579
Epoch 1080, training loss: 0.6177632212638855 = 0.005171413067728281 + 0.1 * 6.125917911529541
Epoch 1080, val loss: 1.2367585897445679
Epoch 1090, training loss: 0.6163942813873291 = 0.00506397383287549 + 0.1 * 6.113303184509277
Epoch 1090, val loss: 1.2405521869659424
Epoch 1100, training loss: 0.617510199546814 = 0.004960344173014164 + 0.1 * 6.125498294830322
Epoch 1100, val loss: 1.244279384613037
Epoch 1110, training loss: 0.6174185276031494 = 0.004859946668148041 + 0.1 * 6.125586032867432
Epoch 1110, val loss: 1.248015284538269
Epoch 1120, training loss: 0.6171689033508301 = 0.004763154312968254 + 0.1 * 6.124057769775391
Epoch 1120, val loss: 1.2516577243804932
Epoch 1130, training loss: 0.6181408166885376 = 0.004669812507927418 + 0.1 * 6.134710311889648
Epoch 1130, val loss: 1.2552776336669922
Epoch 1140, training loss: 0.6156811714172363 = 0.004579673521220684 + 0.1 * 6.1110148429870605
Epoch 1140, val loss: 1.2588186264038086
Epoch 1150, training loss: 0.6165206432342529 = 0.004492577165365219 + 0.1 * 6.120280742645264
Epoch 1150, val loss: 1.2622950077056885
Epoch 1160, training loss: 0.615159809589386 = 0.004408103413879871 + 0.1 * 6.107516765594482
Epoch 1160, val loss: 1.2657634019851685
Epoch 1170, training loss: 0.6160417199134827 = 0.004326250869780779 + 0.1 * 6.117155075073242
Epoch 1170, val loss: 1.2691766023635864
Epoch 1180, training loss: 0.615509033203125 = 0.004246898461133242 + 0.1 * 6.112621307373047
Epoch 1180, val loss: 1.2725623846054077
Epoch 1190, training loss: 0.615906298160553 = 0.004170216154307127 + 0.1 * 6.117360591888428
Epoch 1190, val loss: 1.2759068012237549
Epoch 1200, training loss: 0.6163997650146484 = 0.004095776937901974 + 0.1 * 6.123039722442627
Epoch 1200, val loss: 1.2792198657989502
Epoch 1210, training loss: 0.6152958273887634 = 0.004023643210530281 + 0.1 * 6.1127214431762695
Epoch 1210, val loss: 1.282461166381836
Epoch 1220, training loss: 0.6147383451461792 = 0.0039537991397082806 + 0.1 * 6.107845306396484
Epoch 1220, val loss: 1.2856897115707397
Epoch 1230, training loss: 0.6142314672470093 = 0.0038860223721712828 + 0.1 * 6.103454113006592
Epoch 1230, val loss: 1.2888587713241577
Epoch 1240, training loss: 0.615111768245697 = 0.003820146434009075 + 0.1 * 6.112916469573975
Epoch 1240, val loss: 1.2919825315475464
Epoch 1250, training loss: 0.614264190196991 = 0.003756192047148943 + 0.1 * 6.1050801277160645
Epoch 1250, val loss: 1.2950916290283203
Epoch 1260, training loss: 0.6134418249130249 = 0.0036942316219210625 + 0.1 * 6.097476005554199
Epoch 1260, val loss: 1.2981500625610352
Epoch 1270, training loss: 0.6139710545539856 = 0.0036339114885777235 + 0.1 * 6.1033711433410645
Epoch 1270, val loss: 1.301186442375183
Epoch 1280, training loss: 0.6138546466827393 = 0.0035752877593040466 + 0.1 * 6.1027936935424805
Epoch 1280, val loss: 1.3041784763336182
Epoch 1290, training loss: 0.6140890717506409 = 0.00351824052631855 + 0.1 * 6.105708122253418
Epoch 1290, val loss: 1.307143211364746
Epoch 1300, training loss: 0.6132688522338867 = 0.0034627860877662897 + 0.1 * 6.098060607910156
Epoch 1300, val loss: 1.3100731372833252
Epoch 1310, training loss: 0.6145889163017273 = 0.003408983815461397 + 0.1 * 6.111799240112305
Epoch 1310, val loss: 1.3129630088806152
Epoch 1320, training loss: 0.6138368844985962 = 0.003356784349307418 + 0.1 * 6.104801177978516
Epoch 1320, val loss: 1.3158183097839355
Epoch 1330, training loss: 0.6133111715316772 = 0.0033060768619179726 + 0.1 * 6.100050926208496
Epoch 1330, val loss: 1.3186345100402832
Epoch 1340, training loss: 0.6122928857803345 = 0.003256607335060835 + 0.1 * 6.090362548828125
Epoch 1340, val loss: 1.3214304447174072
Epoch 1350, training loss: 0.615425169467926 = 0.0032082628458738327 + 0.1 * 6.12216854095459
Epoch 1350, val loss: 1.3242146968841553
Epoch 1360, training loss: 0.613341212272644 = 0.003161070868372917 + 0.1 * 6.10180139541626
Epoch 1360, val loss: 1.3269059658050537
Epoch 1370, training loss: 0.6134997606277466 = 0.003115318715572357 + 0.1 * 6.10384464263916
Epoch 1370, val loss: 1.3296196460723877
Epoch 1380, training loss: 0.6118660569190979 = 0.0030706136021763086 + 0.1 * 6.087954521179199
Epoch 1380, val loss: 1.3322852849960327
Epoch 1390, training loss: 0.6139044165611267 = 0.0030270495917648077 + 0.1 * 6.108773708343506
Epoch 1390, val loss: 1.3349436521530151
Epoch 1400, training loss: 0.6125070452690125 = 0.002984600607305765 + 0.1 * 6.095224380493164
Epoch 1400, val loss: 1.337571144104004
Epoch 1410, training loss: 0.6120706796646118 = 0.002943226136267185 + 0.1 * 6.091274261474609
Epoch 1410, val loss: 1.3401565551757812
Epoch 1420, training loss: 0.6133261322975159 = 0.0029028900898993015 + 0.1 * 6.1042327880859375
Epoch 1420, val loss: 1.342743992805481
Epoch 1430, training loss: 0.612087607383728 = 0.002863618079572916 + 0.1 * 6.0922393798828125
Epoch 1430, val loss: 1.3452515602111816
Epoch 1440, training loss: 0.6116538047790527 = 0.0028252622578293085 + 0.1 * 6.088285446166992
Epoch 1440, val loss: 1.3477842807769775
Epoch 1450, training loss: 0.612227737903595 = 0.002787809120491147 + 0.1 * 6.0943989753723145
Epoch 1450, val loss: 1.3502764701843262
Epoch 1460, training loss: 0.6112362146377563 = 0.0027511678636074066 + 0.1 * 6.084850311279297
Epoch 1460, val loss: 1.3527402877807617
Epoch 1470, training loss: 0.6126056909561157 = 0.002715359441936016 + 0.1 * 6.098903179168701
Epoch 1470, val loss: 1.3552193641662598
Epoch 1480, training loss: 0.6118968725204468 = 0.002680355217307806 + 0.1 * 6.092164993286133
Epoch 1480, val loss: 1.3576478958129883
Epoch 1490, training loss: 0.6117755770683289 = 0.002646178239956498 + 0.1 * 6.091293811798096
Epoch 1490, val loss: 1.360051155090332
Epoch 1500, training loss: 0.6116592884063721 = 0.0026128627359867096 + 0.1 * 6.090464115142822
Epoch 1500, val loss: 1.3624368906021118
Epoch 1510, training loss: 0.6112889051437378 = 0.0025804154574871063 + 0.1 * 6.087084770202637
Epoch 1510, val loss: 1.3647985458374023
Epoch 1520, training loss: 0.6115017533302307 = 0.0025485875084996223 + 0.1 * 6.089531898498535
Epoch 1520, val loss: 1.3671311140060425
Epoch 1530, training loss: 0.6103867292404175 = 0.00251744594424963 + 0.1 * 6.078692436218262
Epoch 1530, val loss: 1.369438886642456
Epoch 1540, training loss: 0.6120202541351318 = 0.002486945828422904 + 0.1 * 6.095333099365234
Epoch 1540, val loss: 1.3717235326766968
Epoch 1550, training loss: 0.6120281219482422 = 0.0024571053218096495 + 0.1 * 6.095710277557373
Epoch 1550, val loss: 1.3740068674087524
Epoch 1560, training loss: 0.6106938719749451 = 0.0024279651697725058 + 0.1 * 6.0826592445373535
Epoch 1560, val loss: 1.3762413263320923
Epoch 1570, training loss: 0.6123266220092773 = 0.0023995861411094666 + 0.1 * 6.099270343780518
Epoch 1570, val loss: 1.378463625907898
Epoch 1580, training loss: 0.6103752851486206 = 0.002371611073613167 + 0.1 * 6.080036640167236
Epoch 1580, val loss: 1.380679726600647
Epoch 1590, training loss: 0.6103177070617676 = 0.0023444031830877066 + 0.1 * 6.079732894897461
Epoch 1590, val loss: 1.382853388786316
Epoch 1600, training loss: 0.6109462976455688 = 0.0023177319671958685 + 0.1 * 6.086285591125488
Epoch 1600, val loss: 1.3850111961364746
Epoch 1610, training loss: 0.6108351349830627 = 0.002291504293680191 + 0.1 * 6.0854363441467285
Epoch 1610, val loss: 1.387161135673523
Epoch 1620, training loss: 0.6103688478469849 = 0.0022658761590719223 + 0.1 * 6.081029891967773
Epoch 1620, val loss: 1.389276146888733
Epoch 1630, training loss: 0.6112695336341858 = 0.0022407076321542263 + 0.1 * 6.090288162231445
Epoch 1630, val loss: 1.3913700580596924
Epoch 1640, training loss: 0.6100367307662964 = 0.002216228749603033 + 0.1 * 6.078205108642578
Epoch 1640, val loss: 1.3934284448623657
Epoch 1650, training loss: 0.6100564002990723 = 0.002192189684137702 + 0.1 * 6.078641891479492
Epoch 1650, val loss: 1.3954817056655884
Epoch 1660, training loss: 0.6098054647445679 = 0.0021685659885406494 + 0.1 * 6.076369285583496
Epoch 1660, val loss: 1.3975293636322021
Epoch 1670, training loss: 0.6108269691467285 = 0.0021454240195453167 + 0.1 * 6.086815357208252
Epoch 1670, val loss: 1.3995469808578491
Epoch 1680, training loss: 0.6105381846427917 = 0.0021226981189101934 + 0.1 * 6.0841546058654785
Epoch 1680, val loss: 1.401545763015747
Epoch 1690, training loss: 0.6098124980926514 = 0.0021005196031183004 + 0.1 * 6.077119827270508
Epoch 1690, val loss: 1.403542399406433
Epoch 1700, training loss: 0.610095739364624 = 0.002078779973089695 + 0.1 * 6.080169677734375
Epoch 1700, val loss: 1.4054961204528809
Epoch 1710, training loss: 0.6096675992012024 = 0.002057366305962205 + 0.1 * 6.076102256774902
Epoch 1710, val loss: 1.4074761867523193
Epoch 1720, training loss: 0.6101100444793701 = 0.00203641876578331 + 0.1 * 6.0807366371154785
Epoch 1720, val loss: 1.4094041585922241
Epoch 1730, training loss: 0.6093527674674988 = 0.002015885431319475 + 0.1 * 6.073368549346924
Epoch 1730, val loss: 1.4112998247146606
Epoch 1740, training loss: 0.6097491979598999 = 0.0019957220647484064 + 0.1 * 6.0775346755981445
Epoch 1740, val loss: 1.4131970405578613
Epoch 1750, training loss: 0.6101179718971252 = 0.001975890016183257 + 0.1 * 6.0814208984375
Epoch 1750, val loss: 1.4150811433792114
Epoch 1760, training loss: 0.6088717579841614 = 0.0019565094262361526 + 0.1 * 6.069152355194092
Epoch 1760, val loss: 1.4169267416000366
Epoch 1770, training loss: 0.6100279092788696 = 0.0019374677212908864 + 0.1 * 6.080904483795166
Epoch 1770, val loss: 1.4187818765640259
Epoch 1780, training loss: 0.6094556450843811 = 0.0019186827121302485 + 0.1 * 6.075369358062744
Epoch 1780, val loss: 1.42063570022583
Epoch 1790, training loss: 0.6096476316452026 = 0.0019003534689545631 + 0.1 * 6.07747220993042
Epoch 1790, val loss: 1.422450065612793
Epoch 1800, training loss: 0.6098275780677795 = 0.0018822961719706655 + 0.1 * 6.0794525146484375
Epoch 1800, val loss: 1.4242604970932007
Epoch 1810, training loss: 0.6086811423301697 = 0.0018645896343514323 + 0.1 * 6.068165302276611
Epoch 1810, val loss: 1.4260340929031372
Epoch 1820, training loss: 0.6090829372406006 = 0.00184717308729887 + 0.1 * 6.072357654571533
Epoch 1820, val loss: 1.4278119802474976
Epoch 1830, training loss: 0.609144926071167 = 0.0018301407108083367 + 0.1 * 6.073147773742676
Epoch 1830, val loss: 1.429569959640503
Epoch 1840, training loss: 0.6089218854904175 = 0.0018133268458768725 + 0.1 * 6.071085453033447
Epoch 1840, val loss: 1.4313311576843262
Epoch 1850, training loss: 0.6084014773368835 = 0.001796784927137196 + 0.1 * 6.066046714782715
Epoch 1850, val loss: 1.4330722093582153
Epoch 1860, training loss: 0.6101119518280029 = 0.0017805625684559345 + 0.1 * 6.083313465118408
Epoch 1860, val loss: 1.4347885847091675
Epoch 1870, training loss: 0.609466552734375 = 0.0017645623302087188 + 0.1 * 6.077020168304443
Epoch 1870, val loss: 1.4364932775497437
Epoch 1880, training loss: 0.6094785332679749 = 0.0017488972516730428 + 0.1 * 6.077295780181885
Epoch 1880, val loss: 1.4381920099258423
Epoch 1890, training loss: 0.6088920831680298 = 0.0017335460288450122 + 0.1 * 6.071585178375244
Epoch 1890, val loss: 1.4398490190505981
Epoch 1900, training loss: 0.6083542704582214 = 0.0017184700118377805 + 0.1 * 6.0663580894470215
Epoch 1900, val loss: 1.4415189027786255
Epoch 1910, training loss: 0.6091716885566711 = 0.0017036142526194453 + 0.1 * 6.074680328369141
Epoch 1910, val loss: 1.443173885345459
Epoch 1920, training loss: 0.608192503452301 = 0.0016889080870896578 + 0.1 * 6.065035820007324
Epoch 1920, val loss: 1.4448268413543701
Epoch 1930, training loss: 0.6087525486946106 = 0.001674502738751471 + 0.1 * 6.0707807540893555
Epoch 1930, val loss: 1.446458339691162
Epoch 1940, training loss: 0.6078414916992188 = 0.001660316251218319 + 0.1 * 6.061811923980713
Epoch 1940, val loss: 1.448061227798462
Epoch 1950, training loss: 0.6086466312408447 = 0.001646408112719655 + 0.1 * 6.07000207901001
Epoch 1950, val loss: 1.4496753215789795
Epoch 1960, training loss: 0.6086888313293457 = 0.001632731524296105 + 0.1 * 6.070560932159424
Epoch 1960, val loss: 1.451277494430542
Epoch 1970, training loss: 0.607830822467804 = 0.0016192407347261906 + 0.1 * 6.062115669250488
Epoch 1970, val loss: 1.452850103378296
Epoch 1980, training loss: 0.6093509793281555 = 0.0016060002380982041 + 0.1 * 6.077449798583984
Epoch 1980, val loss: 1.4544166326522827
Epoch 1990, training loss: 0.6080597639083862 = 0.0015929818619042635 + 0.1 * 6.064667701721191
Epoch 1990, val loss: 1.455987572669983
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8155
Flip ASR: 0.7867/225 nodes
The final ASR:0.74908, 0.10186, Accuracy:0.81358, 0.01522
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9518])
updated graph: torch.Size([2, 10556])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98278, 0.00460, Accuracy:0.82840, 0.00175
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.784104585647583 = 1.946710228919983 + 0.1 * 8.373943328857422
Epoch 0, val loss: 1.9442975521087646
Epoch 10, training loss: 2.773907423019409 = 1.936517596244812 + 0.1 * 8.37389850616455
Epoch 10, val loss: 1.9349982738494873
Epoch 20, training loss: 2.7614586353302 = 1.9240955114364624 + 0.1 * 8.373631477355957
Epoch 20, val loss: 1.923279047012329
Epoch 30, training loss: 2.7438364028930664 = 1.9066721200942993 + 0.1 * 8.37164306640625
Epoch 30, val loss: 1.906495451927185
Epoch 40, training loss: 2.716292381286621 = 1.8807053565979004 + 0.1 * 8.355870246887207
Epoch 40, val loss: 1.8816293478012085
Epoch 50, training loss: 2.6711483001708984 = 1.8435956239700317 + 0.1 * 8.27552604675293
Epoch 50, val loss: 1.847581148147583
Epoch 60, training loss: 2.5919673442840576 = 1.7998371124267578 + 0.1 * 7.921302318572998
Epoch 60, val loss: 1.8099031448364258
Epoch 70, training loss: 2.5298173427581787 = 1.7564020156860352 + 0.1 * 7.7341532707214355
Epoch 70, val loss: 1.7732535600662231
Epoch 80, training loss: 2.4534716606140137 = 1.7076787948608398 + 0.1 * 7.457929611206055
Epoch 80, val loss: 1.7293925285339355
Epoch 90, training loss: 2.360260009765625 = 1.6448040008544922 + 0.1 * 7.154560565948486
Epoch 90, val loss: 1.6730430126190186
Epoch 100, training loss: 2.261807918548584 = 1.563725471496582 + 0.1 * 6.980825424194336
Epoch 100, val loss: 1.6032196283340454
Epoch 110, training loss: 2.156597137451172 = 1.4671226739883423 + 0.1 * 6.894745349884033
Epoch 110, val loss: 1.5217820405960083
Epoch 120, training loss: 2.0491597652435303 = 1.3644686937332153 + 0.1 * 6.84691047668457
Epoch 120, val loss: 1.4369317293167114
Epoch 130, training loss: 1.9460086822509766 = 1.2638647556304932 + 0.1 * 6.821439743041992
Epoch 130, val loss: 1.3568321466445923
Epoch 140, training loss: 1.8494255542755127 = 1.16850745677948 + 0.1 * 6.8091816902160645
Epoch 140, val loss: 1.282339334487915
Epoch 150, training loss: 1.7587590217590332 = 1.078663945198059 + 0.1 * 6.800950050354004
Epoch 150, val loss: 1.2131034135818481
Epoch 160, training loss: 1.6722849607467651 = 0.9930866956710815 + 0.1 * 6.791982650756836
Epoch 160, val loss: 1.1477402448654175
Epoch 170, training loss: 1.5880205631256104 = 0.9100204110145569 + 0.1 * 6.780000686645508
Epoch 170, val loss: 1.0848091840744019
Epoch 180, training loss: 1.5055168867111206 = 0.8289877772331238 + 0.1 * 6.765291213989258
Epoch 180, val loss: 1.0237274169921875
Epoch 190, training loss: 1.4250829219818115 = 0.7503211498260498 + 0.1 * 6.747616767883301
Epoch 190, val loss: 0.964569091796875
Epoch 200, training loss: 1.3479976654052734 = 0.6747802495956421 + 0.1 * 6.732173919677734
Epoch 200, val loss: 0.9082862138748169
Epoch 210, training loss: 1.2757213115692139 = 0.6036565899848938 + 0.1 * 6.720646381378174
Epoch 210, val loss: 0.8567010164260864
Epoch 220, training loss: 1.2095715999603271 = 0.5381298065185547 + 0.1 * 6.714416980743408
Epoch 220, val loss: 0.8117763996124268
Epoch 230, training loss: 1.1496460437774658 = 0.4791492223739624 + 0.1 * 6.704967975616455
Epoch 230, val loss: 0.7749384641647339
Epoch 240, training loss: 1.0961952209472656 = 0.4263495206832886 + 0.1 * 6.698457717895508
Epoch 240, val loss: 0.7456350922584534
Epoch 250, training loss: 1.0483607053756714 = 0.3792380690574646 + 0.1 * 6.691226482391357
Epoch 250, val loss: 0.7230891585350037
Epoch 260, training loss: 1.0059258937835693 = 0.33738741278648376 + 0.1 * 6.685384750366211
Epoch 260, val loss: 0.7062875032424927
Epoch 270, training loss: 0.9678828120231628 = 0.300152063369751 + 0.1 * 6.677307605743408
Epoch 270, val loss: 0.6944019198417664
Epoch 280, training loss: 0.934407114982605 = 0.2668633759021759 + 0.1 * 6.6754374504089355
Epoch 280, val loss: 0.6870300769805908
Epoch 290, training loss: 0.903441846370697 = 0.2371930629014969 + 0.1 * 6.662487983703613
Epoch 290, val loss: 0.6835117936134338
Epoch 300, training loss: 0.8761516213417053 = 0.21074162423610687 + 0.1 * 6.654099464416504
Epoch 300, val loss: 0.6831507682800293
Epoch 310, training loss: 0.8517464995384216 = 0.18726865947246552 + 0.1 * 6.644778251647949
Epoch 310, val loss: 0.6855029463768005
Epoch 320, training loss: 0.8303974270820618 = 0.16658413410186768 + 0.1 * 6.638132572174072
Epoch 320, val loss: 0.689847469329834
Epoch 330, training loss: 0.8109898567199707 = 0.1483401507139206 + 0.1 * 6.6264967918396
Epoch 330, val loss: 0.6958122253417969
Epoch 340, training loss: 0.7937979698181152 = 0.13226017355918884 + 0.1 * 6.615377426147461
Epoch 340, val loss: 0.7028916478157043
Epoch 350, training loss: 0.7791134119033813 = 0.11816916614770889 + 0.1 * 6.609442234039307
Epoch 350, val loss: 0.7105753421783447
Epoch 360, training loss: 0.7651264667510986 = 0.10581883788108826 + 0.1 * 6.593076229095459
Epoch 360, val loss: 0.7186471223831177
Epoch 370, training loss: 0.7528865337371826 = 0.09491687268018723 + 0.1 * 6.5796966552734375
Epoch 370, val loss: 0.7270585894584656
Epoch 380, training loss: 0.7439137697219849 = 0.08527775853872299 + 0.1 * 6.586359977722168
Epoch 380, val loss: 0.7357490658760071
Epoch 390, training loss: 0.732732892036438 = 0.0768074169754982 + 0.1 * 6.559254169464111
Epoch 390, val loss: 0.7444233298301697
Epoch 400, training loss: 0.724383533000946 = 0.06933540850877762 + 0.1 * 6.55048131942749
Epoch 400, val loss: 0.7530927062034607
Epoch 410, training loss: 0.7178346514701843 = 0.06274306774139404 + 0.1 * 6.550915718078613
Epoch 410, val loss: 0.7617499828338623
Epoch 420, training loss: 0.7111479043960571 = 0.056954655796289444 + 0.1 * 6.541932106018066
Epoch 420, val loss: 0.7705382704734802
Epoch 430, training loss: 0.7046952247619629 = 0.0518660731613636 + 0.1 * 6.528291702270508
Epoch 430, val loss: 0.7790587544441223
Epoch 440, training loss: 0.7001779675483704 = 0.04737701267004013 + 0.1 * 6.528009414672852
Epoch 440, val loss: 0.7875978946685791
Epoch 450, training loss: 0.6946911215782166 = 0.043422531336545944 + 0.1 * 6.512685775756836
Epoch 450, val loss: 0.7959720492362976
Epoch 460, training loss: 0.6908372044563293 = 0.03992021828889847 + 0.1 * 6.509169578552246
Epoch 460, val loss: 0.8043667674064636
Epoch 470, training loss: 0.689816951751709 = 0.036819033324718475 + 0.1 * 6.529979228973389
Epoch 470, val loss: 0.81266188621521
Epoch 480, training loss: 0.6828866004943848 = 0.03407497704029083 + 0.1 * 6.488116264343262
Epoch 480, val loss: 0.8205638527870178
Epoch 490, training loss: 0.6799073219299316 = 0.031629107892513275 + 0.1 * 6.482782363891602
Epoch 490, val loss: 0.8285315632820129
Epoch 500, training loss: 0.6773200035095215 = 0.029435664415359497 + 0.1 * 6.478842735290527
Epoch 500, val loss: 0.8364418148994446
Epoch 510, training loss: 0.6749091148376465 = 0.027464019134640694 + 0.1 * 6.474450588226318
Epoch 510, val loss: 0.8441551327705383
Epoch 520, training loss: 0.6725079417228699 = 0.025687571614980698 + 0.1 * 6.468203544616699
Epoch 520, val loss: 0.8517293930053711
Epoch 530, training loss: 0.6707932353019714 = 0.024079550057649612 + 0.1 * 6.467136859893799
Epoch 530, val loss: 0.8591819405555725
Epoch 540, training loss: 0.6680936217308044 = 0.022621382027864456 + 0.1 * 6.454721927642822
Epoch 540, val loss: 0.8664907217025757
Epoch 550, training loss: 0.6679489016532898 = 0.021293265745043755 + 0.1 * 6.466556072235107
Epoch 550, val loss: 0.873676061630249
Epoch 560, training loss: 0.665008008480072 = 0.02008564956486225 + 0.1 * 6.449223518371582
Epoch 560, val loss: 0.880696713924408
Epoch 570, training loss: 0.6630167961120605 = 0.018983367830514908 + 0.1 * 6.440334320068359
Epoch 570, val loss: 0.8875741362571716
Epoch 580, training loss: 0.6615709066390991 = 0.017971670255064964 + 0.1 * 6.435992240905762
Epoch 580, val loss: 0.8943177461624146
Epoch 590, training loss: 0.660620391368866 = 0.01704106293618679 + 0.1 * 6.435792922973633
Epoch 590, val loss: 0.9008001089096069
Epoch 600, training loss: 0.6593720316886902 = 0.016184521839022636 + 0.1 * 6.431875228881836
Epoch 600, val loss: 0.9074226021766663
Epoch 610, training loss: 0.6582650542259216 = 0.015393405221402645 + 0.1 * 6.428716659545898
Epoch 610, val loss: 0.9137985110282898
Epoch 620, training loss: 0.6572229862213135 = 0.014661164954304695 + 0.1 * 6.425617694854736
Epoch 620, val loss: 0.9199430346488953
Epoch 630, training loss: 0.6558095216751099 = 0.013983727432787418 + 0.1 * 6.418257713317871
Epoch 630, val loss: 0.9259935021400452
Epoch 640, training loss: 0.6547204256057739 = 0.013357000425457954 + 0.1 * 6.413634300231934
Epoch 640, val loss: 0.931951105594635
Epoch 650, training loss: 0.6537554264068604 = 0.01277365442365408 + 0.1 * 6.409817695617676
Epoch 650, val loss: 0.9378002882003784
Epoch 660, training loss: 0.6530213952064514 = 0.01222892664372921 + 0.1 * 6.407924652099609
Epoch 660, val loss: 0.9434611797332764
Epoch 670, training loss: 0.6525358557701111 = 0.011722621507942677 + 0.1 * 6.408132076263428
Epoch 670, val loss: 0.94903564453125
Epoch 680, training loss: 0.6517699360847473 = 0.011249695904552937 + 0.1 * 6.4052019119262695
Epoch 680, val loss: 0.9547266364097595
Epoch 690, training loss: 0.6505920886993408 = 0.010807068087160587 + 0.1 * 6.3978495597839355
Epoch 690, val loss: 0.9599108695983887
Epoch 700, training loss: 0.6492997407913208 = 0.010392461903393269 + 0.1 * 6.389072418212891
Epoch 700, val loss: 0.96529620885849
Epoch 710, training loss: 0.6498652100563049 = 0.010002481751143932 + 0.1 * 6.398627281188965
Epoch 710, val loss: 0.9704629182815552
Epoch 720, training loss: 0.6477916240692139 = 0.00963654089719057 + 0.1 * 6.3815507888793945
Epoch 720, val loss: 0.975479781627655
Epoch 730, training loss: 0.6481069326400757 = 0.009292546659708023 + 0.1 * 6.388143539428711
Epoch 730, val loss: 0.9806922078132629
Epoch 740, training loss: 0.647380530834198 = 0.008968371897935867 + 0.1 * 6.384121894836426
Epoch 740, val loss: 0.9853248000144958
Epoch 750, training loss: 0.6455802917480469 = 0.008663676679134369 + 0.1 * 6.369166374206543
Epoch 750, val loss: 0.9903414249420166
Epoch 760, training loss: 0.6448776721954346 = 0.008375217206776142 + 0.1 * 6.365024089813232
Epoch 760, val loss: 0.995252251625061
Epoch 770, training loss: 0.6454112529754639 = 0.008101437240839005 + 0.1 * 6.373097896575928
Epoch 770, val loss: 0.999625563621521
Epoch 780, training loss: 0.6434876322746277 = 0.007843331433832645 + 0.1 * 6.356442928314209
Epoch 780, val loss: 1.0041530132293701
Epoch 790, training loss: 0.6438049077987671 = 0.007600159849971533 + 0.1 * 6.36204719543457
Epoch 790, val loss: 1.008705496788025
Epoch 800, training loss: 0.6418324112892151 = 0.007368916645646095 + 0.1 * 6.344635009765625
Epoch 800, val loss: 1.0131499767303467
Epoch 810, training loss: 0.6418647766113281 = 0.007148756179958582 + 0.1 * 6.347160339355469
Epoch 810, val loss: 1.0174853801727295
Epoch 820, training loss: 0.6407070159912109 = 0.0069387126713991165 + 0.1 * 6.337683200836182
Epoch 820, val loss: 1.0213453769683838
Epoch 830, training loss: 0.6406599879264832 = 0.0067403255961835384 + 0.1 * 6.339196681976318
Epoch 830, val loss: 1.0256010293960571
Epoch 840, training loss: 0.6401426196098328 = 0.006551092956215143 + 0.1 * 6.3359150886535645
Epoch 840, val loss: 1.0298163890838623
Epoch 850, training loss: 0.6398698687553406 = 0.0063699474558234215 + 0.1 * 6.334999084472656
Epoch 850, val loss: 1.033587098121643
Epoch 860, training loss: 0.6383860111236572 = 0.006197392009198666 + 0.1 * 6.32188606262207
Epoch 860, val loss: 1.037487506866455
Epoch 870, training loss: 0.6408036351203918 = 0.006032824981957674 + 0.1 * 6.347708225250244
Epoch 870, val loss: 1.041232943534851
Epoch 880, training loss: 0.6376149654388428 = 0.005875781178474426 + 0.1 * 6.317391872406006
Epoch 880, val loss: 1.044936180114746
Epoch 890, training loss: 0.639668345451355 = 0.005725975148379803 + 0.1 * 6.339423656463623
Epoch 890, val loss: 1.04875910282135
Epoch 900, training loss: 0.6366392374038696 = 0.005582765676081181 + 0.1 * 6.3105645179748535
Epoch 900, val loss: 1.0521140098571777
Epoch 910, training loss: 0.6372660398483276 = 0.0054458072409033775 + 0.1 * 6.318202495574951
Epoch 910, val loss: 1.0557490587234497
Epoch 920, training loss: 0.6350244283676147 = 0.005314298905432224 + 0.1 * 6.297101020812988
Epoch 920, val loss: 1.0592490434646606
Epoch 930, training loss: 0.6365281343460083 = 0.005187561269849539 + 0.1 * 6.313405513763428
Epoch 930, val loss: 1.0626643896102905
Epoch 940, training loss: 0.6349307298660278 = 0.005066059064120054 + 0.1 * 6.298646926879883
Epoch 940, val loss: 1.0657612085342407
Epoch 950, training loss: 0.6365938186645508 = 0.0049506002105772495 + 0.1 * 6.316431999206543
Epoch 950, val loss: 1.0692377090454102
Epoch 960, training loss: 0.6351063847541809 = 0.004839724861085415 + 0.1 * 6.302666664123535
Epoch 960, val loss: 1.0725598335266113
Epoch 970, training loss: 0.6346081495285034 = 0.004732514265924692 + 0.1 * 6.2987565994262695
Epoch 970, val loss: 1.075564980506897
Epoch 980, training loss: 0.6330087184906006 = 0.004629937466233969 + 0.1 * 6.283788204193115
Epoch 980, val loss: 1.0786980390548706
Epoch 990, training loss: 0.6352968215942383 = 0.004530789330601692 + 0.1 * 6.307660102844238
Epoch 990, val loss: 1.0817097425460815
Epoch 1000, training loss: 0.6329421997070312 = 0.004435440991073847 + 0.1 * 6.285067558288574
Epoch 1000, val loss: 1.0845136642456055
Epoch 1010, training loss: 0.6319559812545776 = 0.0043443054892122746 + 0.1 * 6.276116847991943
Epoch 1010, val loss: 1.087432861328125
Epoch 1020, training loss: 0.6311958432197571 = 0.004256385378539562 + 0.1 * 6.269394874572754
Epoch 1020, val loss: 1.09032142162323
Epoch 1030, training loss: 0.6303179264068604 = 0.004171147011220455 + 0.1 * 6.261467933654785
Epoch 1030, val loss: 1.093003749847412
Epoch 1040, training loss: 0.634611189365387 = 0.004089163616299629 + 0.1 * 6.305220127105713
Epoch 1040, val loss: 1.0956507921218872
Epoch 1050, training loss: 0.6294379830360413 = 0.004009812138974667 + 0.1 * 6.254281997680664
Epoch 1050, val loss: 1.0981210470199585
Epoch 1060, training loss: 0.6285520792007446 = 0.003933832980692387 + 0.1 * 6.246181964874268
Epoch 1060, val loss: 1.100849986076355
Epoch 1070, training loss: 0.6316574811935425 = 0.003859817748889327 + 0.1 * 6.2779765129089355
Epoch 1070, val loss: 1.103425145149231
Epoch 1080, training loss: 0.6290476322174072 = 0.0037885045167058706 + 0.1 * 6.252590656280518
Epoch 1080, val loss: 1.1057140827178955
Epoch 1090, training loss: 0.6293700933456421 = 0.0037200350780040026 + 0.1 * 6.256500244140625
Epoch 1090, val loss: 1.1083464622497559
Epoch 1100, training loss: 0.6277986764907837 = 0.003653096966445446 + 0.1 * 6.241455554962158
Epoch 1100, val loss: 1.1107378005981445
Epoch 1110, training loss: 0.6273660659790039 = 0.0035882038064301014 + 0.1 * 6.237778663635254
Epoch 1110, val loss: 1.113005518913269
Epoch 1120, training loss: 0.6273285746574402 = 0.003525882726535201 + 0.1 * 6.238027095794678
Epoch 1120, val loss: 1.1154060363769531
Epoch 1130, training loss: 0.6266545057296753 = 0.0034650585148483515 + 0.1 * 6.231894493103027
Epoch 1130, val loss: 1.1177805662155151
Epoch 1140, training loss: 0.6262215971946716 = 0.0034060636535286903 + 0.1 * 6.228155612945557
Epoch 1140, val loss: 1.1200852394104004
Epoch 1150, training loss: 0.6271023154258728 = 0.003348903264850378 + 0.1 * 6.237534046173096
Epoch 1150, val loss: 1.122266411781311
Epoch 1160, training loss: 0.6265988945960999 = 0.003293823916465044 + 0.1 * 6.23305082321167
Epoch 1160, val loss: 1.1244289875030518
Epoch 1170, training loss: 0.6256086826324463 = 0.003240386489778757 + 0.1 * 6.223682880401611
Epoch 1170, val loss: 1.126807689666748
Epoch 1180, training loss: 0.6250370740890503 = 0.0031881590839475393 + 0.1 * 6.218489170074463
Epoch 1180, val loss: 1.128865122795105
Epoch 1190, training loss: 0.6258050799369812 = 0.003137571271508932 + 0.1 * 6.226675033569336
Epoch 1190, val loss: 1.1310012340545654
Epoch 1200, training loss: 0.6253257393836975 = 0.003088506404310465 + 0.1 * 6.222372055053711
Epoch 1200, val loss: 1.1331350803375244
Epoch 1210, training loss: 0.6248502731323242 = 0.003040656913071871 + 0.1 * 6.218095779418945
Epoch 1210, val loss: 1.135245442390442
Epoch 1220, training loss: 0.6246494650840759 = 0.0029942416585981846 + 0.1 * 6.216552257537842
Epoch 1220, val loss: 1.1373236179351807
Epoch 1230, training loss: 0.624122679233551 = 0.002949173329398036 + 0.1 * 6.211734771728516
Epoch 1230, val loss: 1.1393622159957886
Epoch 1240, training loss: 0.6253092885017395 = 0.0029054787009954453 + 0.1 * 6.224038124084473
Epoch 1240, val loss: 1.1414847373962402
Epoch 1250, training loss: 0.6232606768608093 = 0.0028626665007323027 + 0.1 * 6.203979969024658
Epoch 1250, val loss: 1.1435154676437378
Epoch 1260, training loss: 0.6269493103027344 = 0.0028210615273565054 + 0.1 * 6.241281986236572
Epoch 1260, val loss: 1.1456018686294556
Epoch 1270, training loss: 0.6239001154899597 = 0.002780602779239416 + 0.1 * 6.21119499206543
Epoch 1270, val loss: 1.147375464439392
Epoch 1280, training loss: 0.6227303147315979 = 0.002741405274719 + 0.1 * 6.199889183044434
Epoch 1280, val loss: 1.1494507789611816
Epoch 1290, training loss: 0.6225534677505493 = 0.0027030056808143854 + 0.1 * 6.198504447937012
Epoch 1290, val loss: 1.1514140367507935
Epoch 1300, training loss: 0.6224114298820496 = 0.00266556185670197 + 0.1 * 6.197458267211914
Epoch 1300, val loss: 1.1533284187316895
Epoch 1310, training loss: 0.6223523616790771 = 0.0026291217654943466 + 0.1 * 6.197232246398926
Epoch 1310, val loss: 1.1552209854125977
Epoch 1320, training loss: 0.6219979524612427 = 0.002593779470771551 + 0.1 * 6.194041728973389
Epoch 1320, val loss: 1.157060146331787
Epoch 1330, training loss: 0.6216028928756714 = 0.002559395506978035 + 0.1 * 6.190434455871582
Epoch 1330, val loss: 1.1590349674224854
Epoch 1340, training loss: 0.6210852265357971 = 0.0025257309898734093 + 0.1 * 6.18559455871582
Epoch 1340, val loss: 1.16110098361969
Epoch 1350, training loss: 0.6209401488304138 = 0.0024924399331212044 + 0.1 * 6.184476852416992
Epoch 1350, val loss: 1.1628667116165161
Epoch 1360, training loss: 0.6238356828689575 = 0.002460426650941372 + 0.1 * 6.213752269744873
Epoch 1360, val loss: 1.1646137237548828
Epoch 1370, training loss: 0.6214593648910522 = 0.0024292063899338245 + 0.1 * 6.190301418304443
Epoch 1370, val loss: 1.1663442850112915
Epoch 1380, training loss: 0.6218781471252441 = 0.0023987970780581236 + 0.1 * 6.194793224334717
Epoch 1380, val loss: 1.1682437658309937
Epoch 1390, training loss: 0.6198410391807556 = 0.0023689796216785908 + 0.1 * 6.174720287322998
Epoch 1390, val loss: 1.1700540781021118
Epoch 1400, training loss: 0.6222093105316162 = 0.0023398592602461576 + 0.1 * 6.198694705963135
Epoch 1400, val loss: 1.171921730041504
Epoch 1410, training loss: 0.6205279231071472 = 0.0023112958297133446 + 0.1 * 6.18216609954834
Epoch 1410, val loss: 1.1735920906066895
Epoch 1420, training loss: 0.6221235990524292 = 0.0022835920099169016 + 0.1 * 6.198400020599365
Epoch 1420, val loss: 1.1753709316253662
Epoch 1430, training loss: 0.6203848719596863 = 0.0022561855148524046 + 0.1 * 6.181286334991455
Epoch 1430, val loss: 1.1769627332687378
Epoch 1440, training loss: 0.6201421022415161 = 0.002229885198175907 + 0.1 * 6.179122447967529
Epoch 1440, val loss: 1.1787469387054443
Epoch 1450, training loss: 0.6195816993713379 = 0.0022037182934582233 + 0.1 * 6.1737799644470215
Epoch 1450, val loss: 1.1803512573242188
Epoch 1460, training loss: 0.6205387711524963 = 0.002178447088226676 + 0.1 * 6.183603286743164
Epoch 1460, val loss: 1.182011365890503
Epoch 1470, training loss: 0.6185998916625977 = 0.002153633162379265 + 0.1 * 6.164462566375732
Epoch 1470, val loss: 1.1836408376693726
Epoch 1480, training loss: 0.6191417574882507 = 0.0021293836180120707 + 0.1 * 6.17012357711792
Epoch 1480, val loss: 1.1854389905929565
Epoch 1490, training loss: 0.6185544729232788 = 0.0021053904201835394 + 0.1 * 6.164490222930908
Epoch 1490, val loss: 1.186983346939087
Epoch 1500, training loss: 0.6196879744529724 = 0.002082128543406725 + 0.1 * 6.176058292388916
Epoch 1500, val loss: 1.1886059045791626
Epoch 1510, training loss: 0.6179307103157043 = 0.0020594072993844748 + 0.1 * 6.158712863922119
Epoch 1510, val loss: 1.1901777982711792
Epoch 1520, training loss: 0.61893230676651 = 0.0020371167920529842 + 0.1 * 6.168951988220215
Epoch 1520, val loss: 1.1918493509292603
Epoch 1530, training loss: 0.618575394153595 = 0.0020150435157120228 + 0.1 * 6.165603160858154
Epoch 1530, val loss: 1.1934127807617188
Epoch 1540, training loss: 0.6199604272842407 = 0.001993590733036399 + 0.1 * 6.179668426513672
Epoch 1540, val loss: 1.1949479579925537
Epoch 1550, training loss: 0.6179224252700806 = 0.001972679514437914 + 0.1 * 6.159497261047363
Epoch 1550, val loss: 1.1964740753173828
Epoch 1560, training loss: 0.6193429231643677 = 0.0019521780777722597 + 0.1 * 6.173907279968262
Epoch 1560, val loss: 1.1981513500213623
Epoch 1570, training loss: 0.6183698773384094 = 0.001931982347741723 + 0.1 * 6.164379119873047
Epoch 1570, val loss: 1.1996231079101562
Epoch 1580, training loss: 0.6173751950263977 = 0.0019122377270832658 + 0.1 * 6.154629230499268
Epoch 1580, val loss: 1.2011659145355225
Epoch 1590, training loss: 0.619359016418457 = 0.0018929243087768555 + 0.1 * 6.174660682678223
Epoch 1590, val loss: 1.202722430229187
Epoch 1600, training loss: 0.6170738339424133 = 0.001873860484920442 + 0.1 * 6.151999473571777
Epoch 1600, val loss: 1.2041488885879517
Epoch 1610, training loss: 0.6193909049034119 = 0.0018552729161456227 + 0.1 * 6.175356388092041
Epoch 1610, val loss: 1.2056289911270142
Epoch 1620, training loss: 0.6176828145980835 = 0.0018369959434494376 + 0.1 * 6.1584577560424805
Epoch 1620, val loss: 1.2071113586425781
Epoch 1630, training loss: 0.6180819869041443 = 0.001819135621190071 + 0.1 * 6.162628173828125
Epoch 1630, val loss: 1.2085943222045898
Epoch 1640, training loss: 0.616112470626831 = 0.0018015062669292092 + 0.1 * 6.1431097984313965
Epoch 1640, val loss: 1.2100023031234741
Epoch 1650, training loss: 0.6167716383934021 = 0.0017842251108959317 + 0.1 * 6.149874210357666
Epoch 1650, val loss: 1.211496114730835
Epoch 1660, training loss: 0.6182799339294434 = 0.001767176785506308 + 0.1 * 6.165127754211426
Epoch 1660, val loss: 1.2129430770874023
Epoch 1670, training loss: 0.6172856688499451 = 0.0017505615251138806 + 0.1 * 6.155350685119629
Epoch 1670, val loss: 1.214264154434204
Epoch 1680, training loss: 0.6166742444038391 = 0.0017342882929369807 + 0.1 * 6.149399280548096
Epoch 1680, val loss: 1.2156703472137451
Epoch 1690, training loss: 0.61735999584198 = 0.0017183333402499557 + 0.1 * 6.156416416168213
Epoch 1690, val loss: 1.2170504331588745
Epoch 1700, training loss: 0.6167131662368774 = 0.0017027279827743769 + 0.1 * 6.150104522705078
Epoch 1700, val loss: 1.2185232639312744
Epoch 1710, training loss: 0.6159347295761108 = 0.0016872403211891651 + 0.1 * 6.142475128173828
Epoch 1710, val loss: 1.2198858261108398
Epoch 1720, training loss: 0.6163725256919861 = 0.0016721057472750545 + 0.1 * 6.147004127502441
Epoch 1720, val loss: 1.2212975025177002
Epoch 1730, training loss: 0.6163230538368225 = 0.001657195040024817 + 0.1 * 6.146658420562744
Epoch 1730, val loss: 1.222671627998352
Epoch 1740, training loss: 0.6160866022109985 = 0.001642585382796824 + 0.1 * 6.144440174102783
Epoch 1740, val loss: 1.2239948511123657
Epoch 1750, training loss: 0.6155503988265991 = 0.0016282445285469294 + 0.1 * 6.139221668243408
Epoch 1750, val loss: 1.225313663482666
Epoch 1760, training loss: 0.6167399287223816 = 0.0016140735242515802 + 0.1 * 6.15125846862793
Epoch 1760, val loss: 1.2266433238983154
Epoch 1770, training loss: 0.6161516308784485 = 0.0016001766780391335 + 0.1 * 6.145514011383057
Epoch 1770, val loss: 1.2279847860336304
Epoch 1780, training loss: 0.6167389154434204 = 0.0015864858869463205 + 0.1 * 6.151524066925049
Epoch 1780, val loss: 1.2292267084121704
Epoch 1790, training loss: 0.6157828569412231 = 0.0015732735628262162 + 0.1 * 6.142095565795898
Epoch 1790, val loss: 1.2306149005889893
Epoch 1800, training loss: 0.6152627468109131 = 0.0015600873157382011 + 0.1 * 6.137026309967041
Epoch 1800, val loss: 1.2319642305374146
Epoch 1810, training loss: 0.6163980960845947 = 0.0015470709186047316 + 0.1 * 6.148509979248047
Epoch 1810, val loss: 1.2332388162612915
Epoch 1820, training loss: 0.6151058077812195 = 0.0015343433478847146 + 0.1 * 6.135714530944824
Epoch 1820, val loss: 1.2344495058059692
Epoch 1830, training loss: 0.6162636280059814 = 0.001521910191513598 + 0.1 * 6.147417068481445
Epoch 1830, val loss: 1.2357467412948608
Epoch 1840, training loss: 0.6152679920196533 = 0.0015095146372914314 + 0.1 * 6.137585163116455
Epoch 1840, val loss: 1.2370094060897827
Epoch 1850, training loss: 0.6149269342422485 = 0.0014974693767726421 + 0.1 * 6.1342949867248535
Epoch 1850, val loss: 1.238324522972107
Epoch 1860, training loss: 0.6159606575965881 = 0.0014855093322694302 + 0.1 * 6.144751071929932
Epoch 1860, val loss: 1.2395341396331787
Epoch 1870, training loss: 0.6141836047172546 = 0.0014738254249095917 + 0.1 * 6.1270976066589355
Epoch 1870, val loss: 1.2407538890838623
Epoch 1880, training loss: 0.6156074404716492 = 0.0014623060123994946 + 0.1 * 6.141451358795166
Epoch 1880, val loss: 1.2420077323913574
Epoch 1890, training loss: 0.6149075627326965 = 0.0014510126784443855 + 0.1 * 6.134565353393555
Epoch 1890, val loss: 1.2432633638381958
Epoch 1900, training loss: 0.6145517826080322 = 0.0014398194616660476 + 0.1 * 6.131119728088379
Epoch 1900, val loss: 1.2444229125976562
Epoch 1910, training loss: 0.6142728328704834 = 0.0014289180980995297 + 0.1 * 6.128439426422119
Epoch 1910, val loss: 1.2457247972488403
Epoch 1920, training loss: 0.6139098405838013 = 0.0014180642319843173 + 0.1 * 6.124917984008789
Epoch 1920, val loss: 1.2468619346618652
Epoch 1930, training loss: 0.6139274835586548 = 0.001407493487931788 + 0.1 * 6.125199794769287
Epoch 1930, val loss: 1.248063564300537
Epoch 1940, training loss: 0.6145056486129761 = 0.0013970030704513192 + 0.1 * 6.131086349487305
Epoch 1940, val loss: 1.2493005990982056
Epoch 1950, training loss: 0.6138986945152283 = 0.0013866379158571362 + 0.1 * 6.125120639801025
Epoch 1950, val loss: 1.2504589557647705
Epoch 1960, training loss: 0.6136718988418579 = 0.001376564847305417 + 0.1 * 6.122953414916992
Epoch 1960, val loss: 1.2517600059509277
Epoch 1970, training loss: 0.6136691570281982 = 0.0013664130819961429 + 0.1 * 6.1230268478393555
Epoch 1970, val loss: 1.2529417276382446
Epoch 1980, training loss: 0.6142476201057434 = 0.0013565437402576208 + 0.1 * 6.128910541534424
Epoch 1980, val loss: 1.2540664672851562
Epoch 1990, training loss: 0.6136648058891296 = 0.0013468185206875205 + 0.1 * 6.123179912567139
Epoch 1990, val loss: 1.2551957368850708
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.6568
Flip ASR: 0.6044/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7987875938415527 = 1.961399793624878 + 0.1 * 8.373876571655273
Epoch 0, val loss: 1.95706307888031
Epoch 10, training loss: 2.788174867630005 = 1.9508029222488403 + 0.1 * 8.373719215393066
Epoch 10, val loss: 1.946259617805481
Epoch 20, training loss: 2.7752177715301514 = 1.937954306602478 + 0.1 * 8.372634887695312
Epoch 20, val loss: 1.9324675798416138
Epoch 30, training loss: 2.7563161849975586 = 1.9198945760726929 + 0.1 * 8.364216804504395
Epoch 30, val loss: 1.9124945402145386
Epoch 40, training loss: 2.724177598953247 = 1.893203854560852 + 0.1 * 8.309738159179688
Epoch 40, val loss: 1.8829400539398193
Epoch 50, training loss: 2.6541826725006104 = 1.85651695728302 + 0.1 * 7.976657867431641
Epoch 50, val loss: 1.8442047834396362
Epoch 60, training loss: 2.5791220664978027 = 1.81458580493927 + 0.1 * 7.64536190032959
Epoch 60, val loss: 1.8029792308807373
Epoch 70, training loss: 2.502547264099121 = 1.7734904289245605 + 0.1 * 7.2905683517456055
Epoch 70, val loss: 1.764870047569275
Epoch 80, training loss: 2.4351894855499268 = 1.7301537990570068 + 0.1 * 7.050356864929199
Epoch 80, val loss: 1.7266660928726196
Epoch 90, training loss: 2.36590313911438 = 1.6785606145858765 + 0.1 * 6.8734259605407715
Epoch 90, val loss: 1.6820451021194458
Epoch 100, training loss: 2.288713216781616 = 1.612392544746399 + 0.1 * 6.763206481933594
Epoch 100, val loss: 1.624866247177124
Epoch 110, training loss: 2.2000837326049805 = 1.5285649299621582 + 0.1 * 6.715188026428223
Epoch 110, val loss: 1.5522702932357788
Epoch 120, training loss: 2.100407123565674 = 1.431538462638855 + 0.1 * 6.688685894012451
Epoch 120, val loss: 1.4710557460784912
Epoch 130, training loss: 1.997220754623413 = 1.3297101259231567 + 0.1 * 6.675106048583984
Epoch 130, val loss: 1.39053475856781
Epoch 140, training loss: 1.8953176736831665 = 1.228556513786316 + 0.1 * 6.667611598968506
Epoch 140, val loss: 1.3140891790390015
Epoch 150, training loss: 1.7958239316940308 = 1.1296441555023193 + 0.1 * 6.661797523498535
Epoch 150, val loss: 1.242063283920288
Epoch 160, training loss: 1.7009172439575195 = 1.035160779953003 + 0.1 * 6.657564163208008
Epoch 160, val loss: 1.1744165420532227
Epoch 170, training loss: 1.6126019954681396 = 0.9471522569656372 + 0.1 * 6.6544976234436035
Epoch 170, val loss: 1.1111313104629517
Epoch 180, training loss: 1.5290892124176025 = 0.8641065359115601 + 0.1 * 6.6498260498046875
Epoch 180, val loss: 1.0507639646530151
Epoch 190, training loss: 1.4494961500167847 = 0.7846836447715759 + 0.1 * 6.648124694824219
Epoch 190, val loss: 0.9922292232513428
Epoch 200, training loss: 1.3729851245880127 = 0.7090442180633545 + 0.1 * 6.63940954208374
Epoch 200, val loss: 0.9363935589790344
Epoch 210, training loss: 1.301006555557251 = 0.6380630731582642 + 0.1 * 6.629434108734131
Epoch 210, val loss: 0.8855991363525391
Epoch 220, training loss: 1.2367255687713623 = 0.574284017086029 + 0.1 * 6.624415874481201
Epoch 220, val loss: 0.8428556323051453
Epoch 230, training loss: 1.1797337532043457 = 0.5190161466598511 + 0.1 * 6.607176780700684
Epoch 230, val loss: 0.8090968132019043
Epoch 240, training loss: 1.1305705308914185 = 0.4715368151664734 + 0.1 * 6.59033727645874
Epoch 240, val loss: 0.7833889722824097
Epoch 250, training loss: 1.0879225730895996 = 0.4306560754776001 + 0.1 * 6.572664260864258
Epoch 250, val loss: 0.7648015022277832
Epoch 260, training loss: 1.0525158643722534 = 0.395503431558609 + 0.1 * 6.57012414932251
Epoch 260, val loss: 0.7519489526748657
Epoch 270, training loss: 1.019186019897461 = 0.36465469002723694 + 0.1 * 6.545312881469727
Epoch 270, val loss: 0.7431802153587341
Epoch 280, training loss: 0.9894826412200928 = 0.3364797830581665 + 0.1 * 6.530028343200684
Epoch 280, val loss: 0.7367424964904785
Epoch 290, training loss: 0.9634477496147156 = 0.3100389242172241 + 0.1 * 6.534088134765625
Epoch 290, val loss: 0.7318634390830994
Epoch 300, training loss: 0.9357527494430542 = 0.2849917709827423 + 0.1 * 6.5076093673706055
Epoch 300, val loss: 0.7275406718254089
Epoch 310, training loss: 0.9113481640815735 = 0.2606891393661499 + 0.1 * 6.506589889526367
Epoch 310, val loss: 0.7234175205230713
Epoch 320, training loss: 0.8862326145172119 = 0.23708491027355194 + 0.1 * 6.491476535797119
Epoch 320, val loss: 0.7190718054771423
Epoch 330, training loss: 0.8608847856521606 = 0.21407629549503326 + 0.1 * 6.468084812164307
Epoch 330, val loss: 0.714968740940094
Epoch 340, training loss: 0.8377310037612915 = 0.1921323835849762 + 0.1 * 6.4559855461120605
Epoch 340, val loss: 0.7117087244987488
Epoch 350, training loss: 0.8180748224258423 = 0.1720333993434906 + 0.1 * 6.460414409637451
Epoch 350, val loss: 0.7098582983016968
Epoch 360, training loss: 0.7991031408309937 = 0.15443864464759827 + 0.1 * 6.446645259857178
Epoch 360, val loss: 0.7097023725509644
Epoch 370, training loss: 0.7845604419708252 = 0.13926610350608826 + 0.1 * 6.452943801879883
Epoch 370, val loss: 0.71149080991745
Epoch 380, training loss: 0.7693115472793579 = 0.1263732612133026 + 0.1 * 6.42938232421875
Epoch 380, val loss: 0.7150245904922485
Epoch 390, training loss: 0.756847083568573 = 0.1153012365102768 + 0.1 * 6.4154582023620605
Epoch 390, val loss: 0.720035970211029
Epoch 400, training loss: 0.7488818764686584 = 0.10566403716802597 + 0.1 * 6.432178497314453
Epoch 400, val loss: 0.7262425422668457
Epoch 410, training loss: 0.7382125854492188 = 0.09727124869823456 + 0.1 * 6.4094133377075195
Epoch 410, val loss: 0.7330822944641113
Epoch 420, training loss: 0.7290406227111816 = 0.0898226797580719 + 0.1 * 6.392179489135742
Epoch 420, val loss: 0.740804135799408
Epoch 430, training loss: 0.7214126586914062 = 0.08311965316534042 + 0.1 * 6.382930278778076
Epoch 430, val loss: 0.749023973941803
Epoch 440, training loss: 0.717528760433197 = 0.07705634832382202 + 0.1 * 6.40472412109375
Epoch 440, val loss: 0.7577374577522278
Epoch 450, training loss: 0.7090011835098267 = 0.07157891243696213 + 0.1 * 6.374222278594971
Epoch 450, val loss: 0.7664464712142944
Epoch 460, training loss: 0.7031926512718201 = 0.06657479703426361 + 0.1 * 6.366178512573242
Epoch 460, val loss: 0.7754722237586975
Epoch 470, training loss: 0.6976903080940247 = 0.061975110322237015 + 0.1 * 6.357151985168457
Epoch 470, val loss: 0.7846049666404724
Epoch 480, training loss: 0.6923972368240356 = 0.05773182213306427 + 0.1 * 6.346653938293457
Epoch 480, val loss: 0.7937783002853394
Epoch 490, training loss: 0.6899146437644958 = 0.05380295589566231 + 0.1 * 6.361116409301758
Epoch 490, val loss: 0.8030992150306702
Epoch 500, training loss: 0.6841105222702026 = 0.05017384514212608 + 0.1 * 6.339366912841797
Epoch 500, val loss: 0.8122031092643738
Epoch 510, training loss: 0.6794376969337463 = 0.046798836439847946 + 0.1 * 6.326388359069824
Epoch 510, val loss: 0.8213411569595337
Epoch 520, training loss: 0.6760758757591248 = 0.04367027431726456 + 0.1 * 6.3240556716918945
Epoch 520, val loss: 0.8303967714309692
Epoch 530, training loss: 0.675748884677887 = 0.04077684134244919 + 0.1 * 6.349720001220703
Epoch 530, val loss: 0.8393750786781311
Epoch 540, training loss: 0.6701174378395081 = 0.03810326009988785 + 0.1 * 6.320141315460205
Epoch 540, val loss: 0.8481460809707642
Epoch 550, training loss: 0.6665032505989075 = 0.03562759980559349 + 0.1 * 6.308756351470947
Epoch 550, val loss: 0.856895387172699
Epoch 560, training loss: 0.6636301279067993 = 0.033339329063892365 + 0.1 * 6.302907943725586
Epoch 560, val loss: 0.8654612898826599
Epoch 570, training loss: 0.6608348488807678 = 0.03124888241291046 + 0.1 * 6.2958598136901855
Epoch 570, val loss: 0.873885989189148
Epoch 580, training loss: 0.6584311127662659 = 0.029336629435420036 + 0.1 * 6.290944576263428
Epoch 580, val loss: 0.8823038935661316
Epoch 590, training loss: 0.6568223237991333 = 0.0275859534740448 + 0.1 * 6.292363166809082
Epoch 590, val loss: 0.8906387686729431
Epoch 600, training loss: 0.6545233726501465 = 0.02598360739648342 + 0.1 * 6.285397529602051
Epoch 600, val loss: 0.8987530469894409
Epoch 610, training loss: 0.6516688466072083 = 0.024505916982889175 + 0.1 * 6.271629333496094
Epoch 610, val loss: 0.9068472981452942
Epoch 620, training loss: 0.6527155637741089 = 0.023145947605371475 + 0.1 * 6.295695781707764
Epoch 620, val loss: 0.914753258228302
Epoch 630, training loss: 0.6494688391685486 = 0.021903423592448235 + 0.1 * 6.275654315948486
Epoch 630, val loss: 0.9224647879600525
Epoch 640, training loss: 0.6488670110702515 = 0.02075866237282753 + 0.1 * 6.281083106994629
Epoch 640, val loss: 0.930141031742096
Epoch 650, training loss: 0.6461211442947388 = 0.019704513251781464 + 0.1 * 6.264166355133057
Epoch 650, val loss: 0.9375932216644287
Epoch 660, training loss: 0.6450541615486145 = 0.018731985241174698 + 0.1 * 6.263221263885498
Epoch 660, val loss: 0.9448696970939636
Epoch 670, training loss: 0.643173336982727 = 0.017832934856414795 + 0.1 * 6.253404140472412
Epoch 670, val loss: 0.9521347880363464
Epoch 680, training loss: 0.6424127817153931 = 0.016998736187815666 + 0.1 * 6.254140853881836
Epoch 680, val loss: 0.9591204524040222
Epoch 690, training loss: 0.641158401966095 = 0.016226278617978096 + 0.1 * 6.249320983886719
Epoch 690, val loss: 0.9660381078720093
Epoch 700, training loss: 0.6397210955619812 = 0.01551020797342062 + 0.1 * 6.2421088218688965
Epoch 700, val loss: 0.9727442860603333
Epoch 710, training loss: 0.6388921737670898 = 0.01484215259552002 + 0.1 * 6.240499973297119
Epoch 710, val loss: 0.9794827103614807
Epoch 720, training loss: 0.6381451487541199 = 0.014219168573617935 + 0.1 * 6.239259719848633
Epoch 720, val loss: 0.9859896898269653
Epoch 730, training loss: 0.6366485953330994 = 0.013637218624353409 + 0.1 * 6.230113506317139
Epoch 730, val loss: 0.9922986030578613
Epoch 740, training loss: 0.6354942321777344 = 0.013092252425849438 + 0.1 * 6.224019527435303
Epoch 740, val loss: 0.9985744953155518
Epoch 750, training loss: 0.636177122592926 = 0.012581785209476948 + 0.1 * 6.235953330993652
Epoch 750, val loss: 1.0047043561935425
Epoch 760, training loss: 0.6336618065834045 = 0.012103804387152195 + 0.1 * 6.215579986572266
Epoch 760, val loss: 1.0106723308563232
Epoch 770, training loss: 0.63515704870224 = 0.011654489673674107 + 0.1 * 6.235025405883789
Epoch 770, val loss: 1.0165655612945557
Epoch 780, training loss: 0.6322169899940491 = 0.011232005432248116 + 0.1 * 6.2098493576049805
Epoch 780, val loss: 1.0223244428634644
Epoch 790, training loss: 0.6337851881980896 = 0.010833478532731533 + 0.1 * 6.229516983032227
Epoch 790, val loss: 1.0279587507247925
Epoch 800, training loss: 0.6314080357551575 = 0.010457882657647133 + 0.1 * 6.20950174331665
Epoch 800, val loss: 1.0334327220916748
Epoch 810, training loss: 0.630587100982666 = 0.010102604515850544 + 0.1 * 6.2048444747924805
Epoch 810, val loss: 1.038847804069519
Epoch 820, training loss: 0.6301490664482117 = 0.009765401482582092 + 0.1 * 6.203836917877197
Epoch 820, val loss: 1.0441725254058838
Epoch 830, training loss: 0.6293972730636597 = 0.009446199052035809 + 0.1 * 6.19951057434082
Epoch 830, val loss: 1.0494511127471924
Epoch 840, training loss: 0.6305618286132812 = 0.009144351817667484 + 0.1 * 6.214174270629883
Epoch 840, val loss: 1.0545355081558228
Epoch 850, training loss: 0.6282483339309692 = 0.008858337998390198 + 0.1 * 6.193900108337402
Epoch 850, val loss: 1.0596014261245728
Epoch 860, training loss: 0.6286689639091492 = 0.008586360141634941 + 0.1 * 6.200826168060303
Epoch 860, val loss: 1.064589500427246
Epoch 870, training loss: 0.6282062530517578 = 0.008327933959662914 + 0.1 * 6.198782920837402
Epoch 870, val loss: 1.069513201713562
Epoch 880, training loss: 0.6282976865768433 = 0.008082181215286255 + 0.1 * 6.202154636383057
Epoch 880, val loss: 1.074326992034912
Epoch 890, training loss: 0.6264318823814392 = 0.00784803181886673 + 0.1 * 6.18583869934082
Epoch 890, val loss: 1.0790460109710693
Epoch 900, training loss: 0.6265841722488403 = 0.007624716963618994 + 0.1 * 6.189594268798828
Epoch 900, val loss: 1.0837328433990479
Epoch 910, training loss: 0.6259472966194153 = 0.00741167226806283 + 0.1 * 6.185356140136719
Epoch 910, val loss: 1.0883939266204834
Epoch 920, training loss: 0.6259222626686096 = 0.007208687718957663 + 0.1 * 6.187135696411133
Epoch 920, val loss: 1.092866063117981
Epoch 930, training loss: 0.6247683167457581 = 0.007014753762632608 + 0.1 * 6.177535533905029
Epoch 930, val loss: 1.0973707437515259
Epoch 940, training loss: 0.6269972324371338 = 0.006829173304140568 + 0.1 * 6.2016801834106445
Epoch 940, val loss: 1.101784348487854
Epoch 950, training loss: 0.6236335635185242 = 0.006651695817708969 + 0.1 * 6.16981840133667
Epoch 950, val loss: 1.106075406074524
Epoch 960, training loss: 0.6240379810333252 = 0.0064821490086615086 + 0.1 * 6.175558567047119
Epoch 960, val loss: 1.1103423833847046
Epoch 970, training loss: 0.6256400942802429 = 0.006319412030279636 + 0.1 * 6.193206787109375
Epoch 970, val loss: 1.1145528554916382
Epoch 980, training loss: 0.6231866478919983 = 0.006163565907627344 + 0.1 * 6.170230865478516
Epoch 980, val loss: 1.1186904907226562
Epoch 990, training loss: 0.6246966123580933 = 0.0060142287984490395 + 0.1 * 6.186823844909668
Epoch 990, val loss: 1.1227275133132935
Epoch 1000, training loss: 0.6227895617485046 = 0.005870569963008165 + 0.1 * 6.169189453125
Epoch 1000, val loss: 1.1268237829208374
Epoch 1010, training loss: 0.6228277683258057 = 0.005732858553528786 + 0.1 * 6.1709489822387695
Epoch 1010, val loss: 1.1307708024978638
Epoch 1020, training loss: 0.6211442947387695 = 0.005600382573902607 + 0.1 * 6.1554388999938965
Epoch 1020, val loss: 1.134704351425171
Epoch 1030, training loss: 0.6229432821273804 = 0.005472990218549967 + 0.1 * 6.1747026443481445
Epoch 1030, val loss: 1.138593316078186
Epoch 1040, training loss: 0.6213206052780151 = 0.005350290797650814 + 0.1 * 6.159702777862549
Epoch 1040, val loss: 1.1423869132995605
Epoch 1050, training loss: 0.6222651600837708 = 0.005232437048107386 + 0.1 * 6.1703267097473145
Epoch 1050, val loss: 1.1461752653121948
Epoch 1060, training loss: 0.6205198764801025 = 0.00511891208589077 + 0.1 * 6.1540093421936035
Epoch 1060, val loss: 1.1498637199401855
Epoch 1070, training loss: 0.6211938261985779 = 0.005009470507502556 + 0.1 * 6.161843299865723
Epoch 1070, val loss: 1.1535676717758179
Epoch 1080, training loss: 0.6205493211746216 = 0.004903657827526331 + 0.1 * 6.156456470489502
Epoch 1080, val loss: 1.1572201251983643
Epoch 1090, training loss: 0.6198115944862366 = 0.004802000243216753 + 0.1 * 6.1500959396362305
Epoch 1090, val loss: 1.1608015298843384
Epoch 1100, training loss: 0.6196441054344177 = 0.00470378901809454 + 0.1 * 6.1494035720825195
Epoch 1100, val loss: 1.1643147468566895
Epoch 1110, training loss: 0.6196890473365784 = 0.004608893301337957 + 0.1 * 6.150801658630371
Epoch 1110, val loss: 1.1678218841552734
Epoch 1120, training loss: 0.6191213726997375 = 0.0045171743258833885 + 0.1 * 6.1460418701171875
Epoch 1120, val loss: 1.1713027954101562
Epoch 1130, training loss: 0.6188482046127319 = 0.004428553860634565 + 0.1 * 6.144196510314941
Epoch 1130, val loss: 1.174699068069458
Epoch 1140, training loss: 0.6193073987960815 = 0.004342962522059679 + 0.1 * 6.149644374847412
Epoch 1140, val loss: 1.1780815124511719
Epoch 1150, training loss: 0.6184940338134766 = 0.004260494373738766 + 0.1 * 6.142335414886475
Epoch 1150, val loss: 1.1813377141952515
Epoch 1160, training loss: 0.6189208030700684 = 0.004180533345788717 + 0.1 * 6.147402286529541
Epoch 1160, val loss: 1.1846356391906738
Epoch 1170, training loss: 0.6183300614356995 = 0.004103331360965967 + 0.1 * 6.142267227172852
Epoch 1170, val loss: 1.187874674797058
Epoch 1180, training loss: 0.620297908782959 = 0.004028313793241978 + 0.1 * 6.16269588470459
Epoch 1180, val loss: 1.1910662651062012
Epoch 1190, training loss: 0.6178817749023438 = 0.0039558447897434235 + 0.1 * 6.139259338378906
Epoch 1190, val loss: 1.19419264793396
Epoch 1200, training loss: 0.6176931858062744 = 0.0038854158483445644 + 0.1 * 6.138077735900879
Epoch 1200, val loss: 1.1972979307174683
Epoch 1210, training loss: 0.6175097227096558 = 0.0038172435015439987 + 0.1 * 6.1369242668151855
Epoch 1210, val loss: 1.2003920078277588
Epoch 1220, training loss: 0.6172030568122864 = 0.003751019947230816 + 0.1 * 6.134520053863525
Epoch 1220, val loss: 1.2034519910812378
Epoch 1230, training loss: 0.6168723702430725 = 0.003686805022880435 + 0.1 * 6.131855487823486
Epoch 1230, val loss: 1.2064552307128906
Epoch 1240, training loss: 0.6184660196304321 = 0.003624363336712122 + 0.1 * 6.148416519165039
Epoch 1240, val loss: 1.2094656229019165
Epoch 1250, training loss: 0.6170827746391296 = 0.0035642327275127172 + 0.1 * 6.135185241699219
Epoch 1250, val loss: 1.2123924493789673
Epoch 1260, training loss: 0.617851734161377 = 0.003505644854158163 + 0.1 * 6.143460750579834
Epoch 1260, val loss: 1.2152605056762695
Epoch 1270, training loss: 0.6174473762512207 = 0.003448809962719679 + 0.1 * 6.139986038208008
Epoch 1270, val loss: 1.2181687355041504
Epoch 1280, training loss: 0.6156097650527954 = 0.0033935841638594866 + 0.1 * 6.122161388397217
Epoch 1280, val loss: 1.2209830284118652
Epoch 1290, training loss: 0.6159467101097107 = 0.0033397723454982042 + 0.1 * 6.12606954574585
Epoch 1290, val loss: 1.2237616777420044
Epoch 1300, training loss: 0.6173055171966553 = 0.003287208965048194 + 0.1 * 6.140182971954346
Epoch 1300, val loss: 1.226577877998352
Epoch 1310, training loss: 0.6160500645637512 = 0.003236239543184638 + 0.1 * 6.128138065338135
Epoch 1310, val loss: 1.2293651103973389
Epoch 1320, training loss: 0.6156453490257263 = 0.003186695510521531 + 0.1 * 6.12458610534668
Epoch 1320, val loss: 1.2320855855941772
Epoch 1330, training loss: 0.6151494383811951 = 0.0031385435722768307 + 0.1 * 6.120108604431152
Epoch 1330, val loss: 1.234804391860962
Epoch 1340, training loss: 0.6158744096755981 = 0.003091606078669429 + 0.1 * 6.1278276443481445
Epoch 1340, val loss: 1.2374906539916992
Epoch 1350, training loss: 0.6159358024597168 = 0.003045926336199045 + 0.1 * 6.128898620605469
Epoch 1350, val loss: 1.2401149272918701
Epoch 1360, training loss: 0.6147324442863464 = 0.00300140050239861 + 0.1 * 6.117310523986816
Epoch 1360, val loss: 1.2427890300750732
Epoch 1370, training loss: 0.6153536438941956 = 0.0029583070427179337 + 0.1 * 6.123953342437744
Epoch 1370, val loss: 1.2453657388687134
Epoch 1380, training loss: 0.6152365803718567 = 0.002916075522080064 + 0.1 * 6.123204708099365
Epoch 1380, val loss: 1.247925877571106
Epoch 1390, training loss: 0.6146532893180847 = 0.002875089645385742 + 0.1 * 6.117782115936279
Epoch 1390, val loss: 1.2504825592041016
Epoch 1400, training loss: 0.6148765087127686 = 0.002835106337442994 + 0.1 * 6.120413780212402
Epoch 1400, val loss: 1.2529902458190918
Epoch 1410, training loss: 0.6153813004493713 = 0.0027960282750427723 + 0.1 * 6.125852584838867
Epoch 1410, val loss: 1.2554837465286255
Epoch 1420, training loss: 0.6142078638076782 = 0.002757934620603919 + 0.1 * 6.114499568939209
Epoch 1420, val loss: 1.258002519607544
Epoch 1430, training loss: 0.614522397518158 = 0.002720795338973403 + 0.1 * 6.118016242980957
Epoch 1430, val loss: 1.2604660987854004
Epoch 1440, training loss: 0.614154577255249 = 0.0026846372056752443 + 0.1 * 6.114699363708496
Epoch 1440, val loss: 1.2628803253173828
Epoch 1450, training loss: 0.6138724088668823 = 0.002649355214089155 + 0.1 * 6.11223030090332
Epoch 1450, val loss: 1.2652863264083862
Epoch 1460, training loss: 0.614273190498352 = 0.002614791737869382 + 0.1 * 6.116583824157715
Epoch 1460, val loss: 1.2676897048950195
Epoch 1470, training loss: 0.6155450344085693 = 0.002581218024715781 + 0.1 * 6.129637718200684
Epoch 1470, val loss: 1.2700682878494263
Epoch 1480, training loss: 0.6142157912254333 = 0.0025484508369117975 + 0.1 * 6.116672992706299
Epoch 1480, val loss: 1.272415280342102
Epoch 1490, training loss: 0.6143292188644409 = 0.002516454551368952 + 0.1 * 6.118127346038818
Epoch 1490, val loss: 1.2746986150741577
Epoch 1500, training loss: 0.6128214001655579 = 0.002485116245225072 + 0.1 * 6.103363037109375
Epoch 1500, val loss: 1.2769986391067505
Epoch 1510, training loss: 0.614676296710968 = 0.0024545472115278244 + 0.1 * 6.122217655181885
Epoch 1510, val loss: 1.2792742252349854
Epoch 1520, training loss: 0.6134107708930969 = 0.002424689009785652 + 0.1 * 6.109860420227051
Epoch 1520, val loss: 1.2815625667572021
Epoch 1530, training loss: 0.6137778162956238 = 0.0023954156786203384 + 0.1 * 6.113823890686035
Epoch 1530, val loss: 1.2837870121002197
Epoch 1540, training loss: 0.6133578419685364 = 0.0023668913636356592 + 0.1 * 6.109909534454346
Epoch 1540, val loss: 1.286047339439392
Epoch 1550, training loss: 0.6129926443099976 = 0.002338956343010068 + 0.1 * 6.106536865234375
Epoch 1550, val loss: 1.288248896598816
Epoch 1560, training loss: 0.6134559512138367 = 0.0023117444943636656 + 0.1 * 6.111441612243652
Epoch 1560, val loss: 1.2904369831085205
Epoch 1570, training loss: 0.6124695539474487 = 0.002284953836351633 + 0.1 * 6.101846218109131
Epoch 1570, val loss: 1.292594075202942
Epoch 1580, training loss: 0.6126843690872192 = 0.002258874010294676 + 0.1 * 6.104254722595215
Epoch 1580, val loss: 1.2947357892990112
Epoch 1590, training loss: 0.612035870552063 = 0.0022332174703478813 + 0.1 * 6.098026752471924
Epoch 1590, val loss: 1.2968693971633911
Epoch 1600, training loss: 0.6130170822143555 = 0.002208138583227992 + 0.1 * 6.108089447021484
Epoch 1600, val loss: 1.2989933490753174
Epoch 1610, training loss: 0.6128148436546326 = 0.002183598466217518 + 0.1 * 6.106312274932861
Epoch 1610, val loss: 1.3011205196380615
Epoch 1620, training loss: 0.6125317215919495 = 0.0021596390288323164 + 0.1 * 6.103720664978027
Epoch 1620, val loss: 1.3032146692276
Epoch 1630, training loss: 0.6121854186058044 = 0.0021361319813877344 + 0.1 * 6.10049295425415
Epoch 1630, val loss: 1.3052523136138916
Epoch 1640, training loss: 0.6117980480194092 = 0.0021131266839802265 + 0.1 * 6.09684944152832
Epoch 1640, val loss: 1.307296872138977
Epoch 1650, training loss: 0.6121614575386047 = 0.002090564463287592 + 0.1 * 6.100708961486816
Epoch 1650, val loss: 1.3093410730361938
Epoch 1660, training loss: 0.6122798919677734 = 0.002068498870357871 + 0.1 * 6.102113723754883
Epoch 1660, val loss: 1.3114296197891235
Epoch 1670, training loss: 0.6117806434631348 = 0.0020469024311751127 + 0.1 * 6.09733772277832
Epoch 1670, val loss: 1.3134090900421143
Epoch 1680, training loss: 0.6119428277015686 = 0.0020257506985217333 + 0.1 * 6.099170684814453
Epoch 1680, val loss: 1.3153742551803589
Epoch 1690, training loss: 0.6117441058158875 = 0.0020049121230840683 + 0.1 * 6.097391605377197
Epoch 1690, val loss: 1.317358136177063
Epoch 1700, training loss: 0.6119464635848999 = 0.00198447541333735 + 0.1 * 6.0996198654174805
Epoch 1700, val loss: 1.3193360567092896
Epoch 1710, training loss: 0.6124504804611206 = 0.0019644650164991617 + 0.1 * 6.104860305786133
Epoch 1710, val loss: 1.3212766647338867
Epoch 1720, training loss: 0.6112299561500549 = 0.0019449167884886265 + 0.1 * 6.092850208282471
Epoch 1720, val loss: 1.3232333660125732
Epoch 1730, training loss: 0.6115542054176331 = 0.001925638527609408 + 0.1 * 6.096285343170166
Epoch 1730, val loss: 1.3251025676727295
Epoch 1740, training loss: 0.611300528049469 = 0.0019067093962803483 + 0.1 * 6.09393835067749
Epoch 1740, val loss: 1.3270190954208374
Epoch 1750, training loss: 0.6111811995506287 = 0.0018881546566262841 + 0.1 * 6.092930316925049
Epoch 1750, val loss: 1.3289328813552856
Epoch 1760, training loss: 0.6122397184371948 = 0.0018699872307479382 + 0.1 * 6.103697299957275
Epoch 1760, val loss: 1.3308404684066772
Epoch 1770, training loss: 0.6109141111373901 = 0.001852150191552937 + 0.1 * 6.0906195640563965
Epoch 1770, val loss: 1.3326860666275024
Epoch 1780, training loss: 0.611099123954773 = 0.0018346289871260524 + 0.1 * 6.092644691467285
Epoch 1780, val loss: 1.3345513343811035
Epoch 1790, training loss: 0.6107823252677917 = 0.00181750173214823 + 0.1 * 6.0896477699279785
Epoch 1790, val loss: 1.33634352684021
Epoch 1800, training loss: 0.6107366681098938 = 0.0018006518948823214 + 0.1 * 6.089359760284424
Epoch 1800, val loss: 1.3381881713867188
Epoch 1810, training loss: 0.6111399531364441 = 0.0017840348882600665 + 0.1 * 6.093559265136719
Epoch 1810, val loss: 1.340004801750183
Epoch 1820, training loss: 0.6116478443145752 = 0.0017677139258012176 + 0.1 * 6.098801612854004
Epoch 1820, val loss: 1.3417892456054688
Epoch 1830, training loss: 0.6115498542785645 = 0.0017518170643597841 + 0.1 * 6.09798002243042
Epoch 1830, val loss: 1.3435801267623901
Epoch 1840, training loss: 0.610177218914032 = 0.0017361508216708899 + 0.1 * 6.084410190582275
Epoch 1840, val loss: 1.345351219177246
Epoch 1850, training loss: 0.6108925938606262 = 0.001720817293971777 + 0.1 * 6.09171724319458
Epoch 1850, val loss: 1.347105622291565
Epoch 1860, training loss: 0.6115177869796753 = 0.0017057379009202123 + 0.1 * 6.098120212554932
Epoch 1860, val loss: 1.3488736152648926
Epoch 1870, training loss: 0.6103770732879639 = 0.0016908780671656132 + 0.1 * 6.086862087249756
Epoch 1870, val loss: 1.3505761623382568
Epoch 1880, training loss: 0.6109615564346313 = 0.0016762679442763329 + 0.1 * 6.09285306930542
Epoch 1880, val loss: 1.3523073196411133
Epoch 1890, training loss: 0.6105297803878784 = 0.001661932677961886 + 0.1 * 6.088677883148193
Epoch 1890, val loss: 1.3540167808532715
Epoch 1900, training loss: 0.609589159488678 = 0.0016477687750011683 + 0.1 * 6.079413890838623
Epoch 1900, val loss: 1.3557242155075073
Epoch 1910, training loss: 0.6105159521102905 = 0.001633856212720275 + 0.1 * 6.088820457458496
Epoch 1910, val loss: 1.3574085235595703
Epoch 1920, training loss: 0.6118650436401367 = 0.0016201965045183897 + 0.1 * 6.102448463439941
Epoch 1920, val loss: 1.3591382503509521
Epoch 1930, training loss: 0.6101310849189758 = 0.001606797450222075 + 0.1 * 6.08524227142334
Epoch 1930, val loss: 1.3608312606811523
Epoch 1940, training loss: 0.6105787754058838 = 0.0015936444979161024 + 0.1 * 6.089850902557373
Epoch 1940, val loss: 1.362459421157837
Epoch 1950, training loss: 0.6100955605506897 = 0.001580718089826405 + 0.1 * 6.085148334503174
Epoch 1950, val loss: 1.3641046285629272
Epoch 1960, training loss: 0.6098861694335938 = 0.0015679725911468267 + 0.1 * 6.083181858062744
Epoch 1960, val loss: 1.3657491207122803
Epoch 1970, training loss: 0.609850287437439 = 0.0015554234851151705 + 0.1 * 6.082948207855225
Epoch 1970, val loss: 1.3674061298370361
Epoch 1980, training loss: 0.6098840832710266 = 0.0015430540079250932 + 0.1 * 6.083410263061523
Epoch 1980, val loss: 1.3690112829208374
Epoch 1990, training loss: 0.6090356707572937 = 0.0015308752190321684 + 0.1 * 6.075047969818115
Epoch 1990, val loss: 1.3706241846084595
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6568
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7632365226745605 = 1.9258465766906738 + 0.1 * 8.373900413513184
Epoch 0, val loss: 1.9201585054397583
Epoch 10, training loss: 2.7536001205444336 = 1.9162242412567139 + 0.1 * 8.373758316040039
Epoch 10, val loss: 1.9108879566192627
Epoch 20, training loss: 2.740934371948242 = 1.9036461114883423 + 0.1 * 8.372881889343262
Epoch 20, val loss: 1.8984451293945312
Epoch 30, training loss: 2.7220053672790527 = 1.8855299949645996 + 0.1 * 8.364753723144531
Epoch 30, val loss: 1.8802233934402466
Epoch 40, training loss: 2.6897497177124023 = 1.8589460849761963 + 0.1 * 8.308035850524902
Epoch 40, val loss: 1.8539108037948608
Epoch 50, training loss: 2.6234352588653564 = 1.824843406677246 + 0.1 * 7.985918045043945
Epoch 50, val loss: 1.822588562965393
Epoch 60, training loss: 2.563178300857544 = 1.7898677587509155 + 0.1 * 7.733105659484863
Epoch 60, val loss: 1.7944247722625732
Epoch 70, training loss: 2.492788791656494 = 1.755720615386963 + 0.1 * 7.370682239532471
Epoch 70, val loss: 1.7682826519012451
Epoch 80, training loss: 2.4216341972351074 = 1.7125358581542969 + 0.1 * 7.090984344482422
Epoch 80, val loss: 1.733104944229126
Epoch 90, training loss: 2.3453259468078613 = 1.6550363302230835 + 0.1 * 6.902894973754883
Epoch 90, val loss: 1.6848043203353882
Epoch 100, training loss: 2.2555816173553467 = 1.581337571144104 + 0.1 * 6.742440223693848
Epoch 100, val loss: 1.6249288320541382
Epoch 110, training loss: 2.1575043201446533 = 1.49240243434906 + 0.1 * 6.65101957321167
Epoch 110, val loss: 1.555753469467163
Epoch 120, training loss: 2.056067705154419 = 1.395226001739502 + 0.1 * 6.608417510986328
Epoch 120, val loss: 1.4818131923675537
Epoch 130, training loss: 1.9555590152740479 = 1.2974622249603271 + 0.1 * 6.580968379974365
Epoch 130, val loss: 1.4092570543289185
Epoch 140, training loss: 1.8561952114105225 = 1.1999962329864502 + 0.1 * 6.561988830566406
Epoch 140, val loss: 1.3378487825393677
Epoch 150, training loss: 1.7589764595031738 = 1.1041080951690674 + 0.1 * 6.548684120178223
Epoch 150, val loss: 1.266537070274353
Epoch 160, training loss: 1.6670830249786377 = 1.0131824016571045 + 0.1 * 6.53900671005249
Epoch 160, val loss: 1.1988650560379028
Epoch 170, training loss: 1.580181360244751 = 0.9281946420669556 + 0.1 * 6.519866466522217
Epoch 170, val loss: 1.1363130807876587
Epoch 180, training loss: 1.5030512809753418 = 0.8499825596809387 + 0.1 * 6.53068733215332
Epoch 180, val loss: 1.0800890922546387
Epoch 190, training loss: 1.4320322275161743 = 0.7823200225830078 + 0.1 * 6.497121810913086
Epoch 190, val loss: 1.0338655710220337
Epoch 200, training loss: 1.3717236518859863 = 0.7239411473274231 + 0.1 * 6.477824687957764
Epoch 200, val loss: 0.9960801601409912
Epoch 210, training loss: 1.3229703903198242 = 0.6733797788619995 + 0.1 * 6.495906829833984
Epoch 210, val loss: 0.9661204218864441
Epoch 220, training loss: 1.2752212285995483 = 0.6295465230941772 + 0.1 * 6.456747055053711
Epoch 220, val loss: 0.9430109858512878
Epoch 230, training loss: 1.2338248491287231 = 0.5895302891731262 + 0.1 * 6.44294548034668
Epoch 230, val loss: 0.9244644641876221
Epoch 240, training loss: 1.1948893070220947 = 0.5517078638076782 + 0.1 * 6.431813716888428
Epoch 240, val loss: 0.9089863896369934
Epoch 250, training loss: 1.1576414108276367 = 0.5151967406272888 + 0.1 * 6.424446105957031
Epoch 250, val loss: 0.8959338068962097
Epoch 260, training loss: 1.1210715770721436 = 0.479473739862442 + 0.1 * 6.415977954864502
Epoch 260, val loss: 0.8845739960670471
Epoch 270, training loss: 1.0859763622283936 = 0.4445240795612335 + 0.1 * 6.414523124694824
Epoch 270, val loss: 0.8752856850624084
Epoch 280, training loss: 1.0505106449127197 = 0.41070476174354553 + 0.1 * 6.398058891296387
Epoch 280, val loss: 0.8680557608604431
Epoch 290, training loss: 1.0176706314086914 = 0.3781612813472748 + 0.1 * 6.39509391784668
Epoch 290, val loss: 0.8631812930107117
Epoch 300, training loss: 0.9858036041259766 = 0.34717273712158203 + 0.1 * 6.386308670043945
Epoch 300, val loss: 0.8608758449554443
Epoch 310, training loss: 0.9560991525650024 = 0.3177697956562042 + 0.1 * 6.383293628692627
Epoch 310, val loss: 0.8610125184059143
Epoch 320, training loss: 0.9274424314498901 = 0.2901357412338257 + 0.1 * 6.3730669021606445
Epoch 320, val loss: 0.86341392993927
Epoch 330, training loss: 0.9007183909416199 = 0.26439225673675537 + 0.1 * 6.3632612228393555
Epoch 330, val loss: 0.8681119084358215
Epoch 340, training loss: 0.8779538869857788 = 0.2405683398246765 + 0.1 * 6.373855113983154
Epoch 340, val loss: 0.8749013543128967
Epoch 350, training loss: 0.854958176612854 = 0.21887531876564026 + 0.1 * 6.360827922821045
Epoch 350, val loss: 0.8836153149604797
Epoch 360, training loss: 0.8343943357467651 = 0.19925126433372498 + 0.1 * 6.351430892944336
Epoch 360, val loss: 0.8942369222640991
Epoch 370, training loss: 0.8172703385353088 = 0.18153469264507294 + 0.1 * 6.357356071472168
Epoch 370, val loss: 0.9065822958946228
Epoch 380, training loss: 0.8004463911056519 = 0.1656789928674698 + 0.1 * 6.3476738929748535
Epoch 380, val loss: 0.9202461242675781
Epoch 390, training loss: 0.7851191163063049 = 0.15149559080600739 + 0.1 * 6.336235523223877
Epoch 390, val loss: 0.9351606369018555
Epoch 400, training loss: 0.7721298933029175 = 0.13875970244407654 + 0.1 * 6.333702087402344
Epoch 400, val loss: 0.9511237144470215
Epoch 410, training loss: 0.7593567371368408 = 0.12731695175170898 + 0.1 * 6.320397853851318
Epoch 410, val loss: 0.9678425192832947
Epoch 420, training loss: 0.7505363821983337 = 0.1170034185051918 + 0.1 * 6.335329055786133
Epoch 420, val loss: 0.98521888256073
Epoch 430, training loss: 0.7409976720809937 = 0.1077214777469635 + 0.1 * 6.332761287689209
Epoch 430, val loss: 1.0028349161148071
Epoch 440, training loss: 0.7303246259689331 = 0.09936351329088211 + 0.1 * 6.3096113204956055
Epoch 440, val loss: 1.0205706357955933
Epoch 450, training loss: 0.7229882478713989 = 0.0917825773358345 + 0.1 * 6.312056541442871
Epoch 450, val loss: 1.0384114980697632
Epoch 460, training loss: 0.7155736088752747 = 0.08491240441799164 + 0.1 * 6.306612014770508
Epoch 460, val loss: 1.0561902523040771
Epoch 470, training loss: 0.7082870006561279 = 0.07866787165403366 + 0.1 * 6.296191215515137
Epoch 470, val loss: 1.073864459991455
Epoch 480, training loss: 0.7023563981056213 = 0.07298216968774796 + 0.1 * 6.293741703033447
Epoch 480, val loss: 1.0912760496139526
Epoch 490, training loss: 0.6965299248695374 = 0.06780948489904404 + 0.1 * 6.287204742431641
Epoch 490, val loss: 1.108453631401062
Epoch 500, training loss: 0.6931988000869751 = 0.06308753043413162 + 0.1 * 6.301112651824951
Epoch 500, val loss: 1.1252880096435547
Epoch 510, training loss: 0.6871441602706909 = 0.058775193989276886 + 0.1 * 6.283689498901367
Epoch 510, val loss: 1.1418403387069702
Epoch 520, training loss: 0.6832252740859985 = 0.054832641035318375 + 0.1 * 6.283926486968994
Epoch 520, val loss: 1.15802001953125
Epoch 530, training loss: 0.6780173778533936 = 0.051225773990154266 + 0.1 * 6.267915725708008
Epoch 530, val loss: 1.1739317178726196
Epoch 540, training loss: 0.6749827861785889 = 0.04791684076189995 + 0.1 * 6.270659446716309
Epoch 540, val loss: 1.1894357204437256
Epoch 550, training loss: 0.6718552708625793 = 0.044881172478199005 + 0.1 * 6.269741058349609
Epoch 550, val loss: 1.2046650648117065
Epoch 560, training loss: 0.6693547964096069 = 0.042091283947229385 + 0.1 * 6.272634506225586
Epoch 560, val loss: 1.2193922996520996
Epoch 570, training loss: 0.6654843091964722 = 0.0395301878452301 + 0.1 * 6.2595415115356445
Epoch 570, val loss: 1.2337384223937988
Epoch 580, training loss: 0.6650065183639526 = 0.03717311844229698 + 0.1 * 6.278334140777588
Epoch 580, val loss: 1.2478349208831787
Epoch 590, training loss: 0.6607246994972229 = 0.035004690289497375 + 0.1 * 6.257200241088867
Epoch 590, val loss: 1.2614489793777466
Epoch 600, training loss: 0.6579993367195129 = 0.03300566226243973 + 0.1 * 6.249936580657959
Epoch 600, val loss: 1.2748140096664429
Epoch 610, training loss: 0.6560760736465454 = 0.03115614503622055 + 0.1 * 6.249199390411377
Epoch 610, val loss: 1.2878880500793457
Epoch 620, training loss: 0.6535568833351135 = 0.029447408393025398 + 0.1 * 6.24109411239624
Epoch 620, val loss: 1.3007092475891113
Epoch 630, training loss: 0.6509749889373779 = 0.0278624277561903 + 0.1 * 6.231125354766846
Epoch 630, val loss: 1.3131874799728394
Epoch 640, training loss: 0.6507510542869568 = 0.026392173022031784 + 0.1 * 6.243588924407959
Epoch 640, val loss: 1.3255352973937988
Epoch 650, training loss: 0.6511716842651367 = 0.025028808042407036 + 0.1 * 6.261428356170654
Epoch 650, val loss: 1.337365984916687
Epoch 660, training loss: 0.6466152667999268 = 0.02376541495323181 + 0.1 * 6.228498458862305
Epoch 660, val loss: 1.3490132093429565
Epoch 670, training loss: 0.6446409225463867 = 0.02259099669754505 + 0.1 * 6.220499038696289
Epoch 670, val loss: 1.3603696823120117
Epoch 680, training loss: 0.645145833492279 = 0.02149653062224388 + 0.1 * 6.236493110656738
Epoch 680, val loss: 1.3715112209320068
Epoch 690, training loss: 0.6436349153518677 = 0.020478954538702965 + 0.1 * 6.2315592765808105
Epoch 690, val loss: 1.382277250289917
Epoch 700, training loss: 0.6421591639518738 = 0.01953207328915596 + 0.1 * 6.22627067565918
Epoch 700, val loss: 1.39286208152771
Epoch 710, training loss: 0.6399614810943604 = 0.01864747144281864 + 0.1 * 6.213139533996582
Epoch 710, val loss: 1.4031141996383667
Epoch 720, training loss: 0.6399916410446167 = 0.017821038141846657 + 0.1 * 6.221706390380859
Epoch 720, val loss: 1.4132493734359741
Epoch 730, training loss: 0.6400020122528076 = 0.01704769767820835 + 0.1 * 6.229543209075928
Epoch 730, val loss: 1.4230878353118896
Epoch 740, training loss: 0.6372093558311462 = 0.016323555260896683 + 0.1 * 6.208858013153076
Epoch 740, val loss: 1.432639241218567
Epoch 750, training loss: 0.6364791393280029 = 0.015644751489162445 + 0.1 * 6.208343982696533
Epoch 750, val loss: 1.4420713186264038
Epoch 760, training loss: 0.6351421475410461 = 0.015007413923740387 + 0.1 * 6.2013468742370605
Epoch 760, val loss: 1.4512252807617188
Epoch 770, training loss: 0.6361393928527832 = 0.014408034272491932 + 0.1 * 6.217313289642334
Epoch 770, val loss: 1.4601346254348755
Epoch 780, training loss: 0.6347329616546631 = 0.013844589702785015 + 0.1 * 6.208883285522461
Epoch 780, val loss: 1.4689124822616577
Epoch 790, training loss: 0.6342017650604248 = 0.013314773328602314 + 0.1 * 6.208869934082031
Epoch 790, val loss: 1.4773324728012085
Epoch 800, training loss: 0.6327385306358337 = 0.012816615402698517 + 0.1 * 6.199219226837158
Epoch 800, val loss: 1.4856679439544678
Epoch 810, training loss: 0.6320178508758545 = 0.012346274219453335 + 0.1 * 6.196715831756592
Epoch 810, val loss: 1.4938011169433594
Epoch 820, training loss: 0.631754457950592 = 0.011902576312422752 + 0.1 * 6.198518753051758
Epoch 820, val loss: 1.5017995834350586
Epoch 830, training loss: 0.6307579874992371 = 0.011482691392302513 + 0.1 * 6.192752838134766
Epoch 830, val loss: 1.509660243988037
Epoch 840, training loss: 0.6299377679824829 = 0.011085671372711658 + 0.1 * 6.188520908355713
Epoch 840, val loss: 1.5172560214996338
Epoch 850, training loss: 0.6305564641952515 = 0.01070998702198267 + 0.1 * 6.198464393615723
Epoch 850, val loss: 1.5248010158538818
Epoch 860, training loss: 0.6290172934532166 = 0.010354152880609035 + 0.1 * 6.186631202697754
Epoch 860, val loss: 1.532080054283142
Epoch 870, training loss: 0.6283693909645081 = 0.010016607120633125 + 0.1 * 6.18352746963501
Epoch 870, val loss: 1.5393799543380737
Epoch 880, training loss: 0.6287767291069031 = 0.009695542976260185 + 0.1 * 6.190812110900879
Epoch 880, val loss: 1.5463840961456299
Epoch 890, training loss: 0.6285200715065002 = 0.0093914233148098 + 0.1 * 6.191286087036133
Epoch 890, val loss: 1.553285837173462
Epoch 900, training loss: 0.6264321208000183 = 0.00910194218158722 + 0.1 * 6.173301696777344
Epoch 900, val loss: 1.559967279434204
Epoch 910, training loss: 0.6260460615158081 = 0.00882723368704319 + 0.1 * 6.1721882820129395
Epoch 910, val loss: 1.5666557550430298
Epoch 920, training loss: 0.6261988282203674 = 0.008565020747482777 + 0.1 * 6.176338195800781
Epoch 920, val loss: 1.5732427835464478
Epoch 930, training loss: 0.6263127326965332 = 0.008315004408359528 + 0.1 * 6.179976940155029
Epoch 930, val loss: 1.5795291662216187
Epoch 940, training loss: 0.6266219615936279 = 0.008077317848801613 + 0.1 * 6.185446262359619
Epoch 940, val loss: 1.5858186483383179
Epoch 950, training loss: 0.6248373985290527 = 0.007850096561014652 + 0.1 * 6.169872760772705
Epoch 950, val loss: 1.5919102430343628
Epoch 960, training loss: 0.6237544417381287 = 0.007633701898157597 + 0.1 * 6.161207675933838
Epoch 960, val loss: 1.5979901552200317
Epoch 970, training loss: 0.6266734004020691 = 0.0074261948466300964 + 0.1 * 6.192471981048584
Epoch 970, val loss: 1.6039341688156128
Epoch 980, training loss: 0.6239514946937561 = 0.007228578440845013 + 0.1 * 6.167229175567627
Epoch 980, val loss: 1.6095941066741943
Epoch 990, training loss: 0.6231706142425537 = 0.007039185613393784 + 0.1 * 6.161314010620117
Epoch 990, val loss: 1.6153515577316284
Epoch 1000, training loss: 0.6235841512680054 = 0.006857538130134344 + 0.1 * 6.167266368865967
Epoch 1000, val loss: 1.6210798025131226
Epoch 1010, training loss: 0.6256101131439209 = 0.006683658808469772 + 0.1 * 6.18926477432251
Epoch 1010, val loss: 1.6264591217041016
Epoch 1020, training loss: 0.6233091354370117 = 0.006517230533063412 + 0.1 * 6.167918682098389
Epoch 1020, val loss: 1.631794810295105
Epoch 1030, training loss: 0.6230455636978149 = 0.006357867270708084 + 0.1 * 6.166876792907715
Epoch 1030, val loss: 1.6371792554855347
Epoch 1040, training loss: 0.6223450303077698 = 0.006204461213201284 + 0.1 * 6.161405563354492
Epoch 1040, val loss: 1.6424368619918823
Epoch 1050, training loss: 0.6219437718391418 = 0.006057131569832563 + 0.1 * 6.158865928649902
Epoch 1050, val loss: 1.6475259065628052
Epoch 1060, training loss: 0.6217472553253174 = 0.005915756803005934 + 0.1 * 6.1583147048950195
Epoch 1060, val loss: 1.652564525604248
Epoch 1070, training loss: 0.6208528280258179 = 0.005779698491096497 + 0.1 * 6.150731086730957
Epoch 1070, val loss: 1.6575121879577637
Epoch 1080, training loss: 0.6210024952888489 = 0.005648924503475428 + 0.1 * 6.153535842895508
Epoch 1080, val loss: 1.6624752283096313
Epoch 1090, training loss: 0.6220021843910217 = 0.005522841587662697 + 0.1 * 6.164793491363525
Epoch 1090, val loss: 1.667277455329895
Epoch 1100, training loss: 0.621912956237793 = 0.005401615519076586 + 0.1 * 6.16511344909668
Epoch 1100, val loss: 1.671858549118042
Epoch 1110, training loss: 0.620721697807312 = 0.005285023245960474 + 0.1 * 6.154366493225098
Epoch 1110, val loss: 1.6765350103378296
Epoch 1120, training loss: 0.6201645731925964 = 0.005172668024897575 + 0.1 * 6.149919033050537
Epoch 1120, val loss: 1.6810985803604126
Epoch 1130, training loss: 0.6196813583374023 = 0.0050643025897443295 + 0.1 * 6.146170616149902
Epoch 1130, val loss: 1.6856032609939575
Epoch 1140, training loss: 0.620212733745575 = 0.004959790036082268 + 0.1 * 6.152529716491699
Epoch 1140, val loss: 1.6899542808532715
Epoch 1150, training loss: 0.6185126304626465 = 0.00485927565023303 + 0.1 * 6.136533737182617
Epoch 1150, val loss: 1.694145679473877
Epoch 1160, training loss: 0.6199666857719421 = 0.0047620791010558605 + 0.1 * 6.152046203613281
Epoch 1160, val loss: 1.6985070705413818
Epoch 1170, training loss: 0.6191743016242981 = 0.004668098874390125 + 0.1 * 6.14506196975708
Epoch 1170, val loss: 1.7027007341384888
Epoch 1180, training loss: 0.6196476221084595 = 0.004577252082526684 + 0.1 * 6.150703430175781
Epoch 1180, val loss: 1.7067859172821045
Epoch 1190, training loss: 0.6183556914329529 = 0.004489734768867493 + 0.1 * 6.138659477233887
Epoch 1190, val loss: 1.7108489274978638
Epoch 1200, training loss: 0.6179177165031433 = 0.004404732026159763 + 0.1 * 6.135129928588867
Epoch 1200, val loss: 1.714974284172058
Epoch 1210, training loss: 0.6194420456886292 = 0.004322378430515528 + 0.1 * 6.1511969566345215
Epoch 1210, val loss: 1.7188774347305298
Epoch 1220, training loss: 0.6180678009986877 = 0.004242789000272751 + 0.1 * 6.13824987411499
Epoch 1220, val loss: 1.7227251529693604
Epoch 1230, training loss: 0.6188809871673584 = 0.0041657621040940285 + 0.1 * 6.147151947021484
Epoch 1230, val loss: 1.7266263961791992
Epoch 1240, training loss: 0.6178030371665955 = 0.004091356880962849 + 0.1 * 6.137116432189941
Epoch 1240, val loss: 1.7302719354629517
Epoch 1250, training loss: 0.6175371408462524 = 0.004019335377961397 + 0.1 * 6.135178089141846
Epoch 1250, val loss: 1.7340549230575562
Epoch 1260, training loss: 0.6173991560935974 = 0.0039494894444942474 + 0.1 * 6.134496688842773
Epoch 1260, val loss: 1.737720251083374
Epoch 1270, training loss: 0.6167179942131042 = 0.0038816710002720356 + 0.1 * 6.128362655639648
Epoch 1270, val loss: 1.741249680519104
Epoch 1280, training loss: 0.6175462603569031 = 0.0038160134572535753 + 0.1 * 6.137302398681641
Epoch 1280, val loss: 1.744788408279419
Epoch 1290, training loss: 0.6162312626838684 = 0.0037522814236581326 + 0.1 * 6.124789714813232
Epoch 1290, val loss: 1.7483501434326172
Epoch 1300, training loss: 0.6166166663169861 = 0.0036902690771967173 + 0.1 * 6.129263877868652
Epoch 1300, val loss: 1.7518914937973022
Epoch 1310, training loss: 0.6177793145179749 = 0.0036300208885222673 + 0.1 * 6.14149284362793
Epoch 1310, val loss: 1.7552013397216797
Epoch 1320, training loss: 0.6168660521507263 = 0.0035715680569410324 + 0.1 * 6.132944583892822
Epoch 1320, val loss: 1.7585675716400146
Epoch 1330, training loss: 0.6158783435821533 = 0.0035147182643413544 + 0.1 * 6.123636245727539
Epoch 1330, val loss: 1.7619297504425049
Epoch 1340, training loss: 0.6171054840087891 = 0.0034595709294080734 + 0.1 * 6.136458873748779
Epoch 1340, val loss: 1.7652137279510498
Epoch 1350, training loss: 0.6157998442649841 = 0.003405921859666705 + 0.1 * 6.123939037322998
Epoch 1350, val loss: 1.7682745456695557
Epoch 1360, training loss: 0.6152079701423645 = 0.0033538523130118847 + 0.1 * 6.118541240692139
Epoch 1360, val loss: 1.7715750932693481
Epoch 1370, training loss: 0.615277886390686 = 0.0033030668273568153 + 0.1 * 6.119748115539551
Epoch 1370, val loss: 1.7747292518615723
Epoch 1380, training loss: 0.6157212257385254 = 0.003253757953643799 + 0.1 * 6.1246747970581055
Epoch 1380, val loss: 1.7778414487838745
Epoch 1390, training loss: 0.6154008507728577 = 0.003205610904842615 + 0.1 * 6.121952533721924
Epoch 1390, val loss: 1.7808500528335571
Epoch 1400, training loss: 0.6155895590782166 = 0.003158852458000183 + 0.1 * 6.124306678771973
Epoch 1400, val loss: 1.7839148044586182
Epoch 1410, training loss: 0.6154470443725586 = 0.003113252343609929 + 0.1 * 6.123337745666504
Epoch 1410, val loss: 1.7868890762329102
Epoch 1420, training loss: 0.614348828792572 = 0.003068957244977355 + 0.1 * 6.11279821395874
Epoch 1420, val loss: 1.7898203134536743
Epoch 1430, training loss: 0.6162160038948059 = 0.0030256735626608133 + 0.1 * 6.131903648376465
Epoch 1430, val loss: 1.7927377223968506
Epoch 1440, training loss: 0.6150239109992981 = 0.0029836902394890785 + 0.1 * 6.1204023361206055
Epoch 1440, val loss: 1.7955666780471802
Epoch 1450, training loss: 0.6157891154289246 = 0.0029426163528114557 + 0.1 * 6.128464698791504
Epoch 1450, val loss: 1.7984468936920166
Epoch 1460, training loss: 0.6155309081077576 = 0.0029026467818766832 + 0.1 * 6.126282691955566
Epoch 1460, val loss: 1.8011420965194702
Epoch 1470, training loss: 0.6139999628067017 = 0.002863811096176505 + 0.1 * 6.111361503601074
Epoch 1470, val loss: 1.8038570880889893
Epoch 1480, training loss: 0.614102303981781 = 0.002825899049639702 + 0.1 * 6.11276388168335
Epoch 1480, val loss: 1.8066543340682983
Epoch 1490, training loss: 0.6142933964729309 = 0.0027886880561709404 + 0.1 * 6.115047454833984
Epoch 1490, val loss: 1.8093407154083252
Epoch 1500, training loss: 0.6142387986183167 = 0.00275243166834116 + 0.1 * 6.114863872528076
Epoch 1500, val loss: 1.8120274543762207
Epoch 1510, training loss: 0.6150261759757996 = 0.0027170206885784864 + 0.1 * 6.123091220855713
Epoch 1510, val loss: 1.8145811557769775
Epoch 1520, training loss: 0.6142429113388062 = 0.002682542661204934 + 0.1 * 6.115603446960449
Epoch 1520, val loss: 1.817134141921997
Epoch 1530, training loss: 0.6129586696624756 = 0.0026488478761166334 + 0.1 * 6.103098392486572
Epoch 1530, val loss: 1.8197101354599
Epoch 1540, training loss: 0.6139354705810547 = 0.002615981502458453 + 0.1 * 6.113194465637207
Epoch 1540, val loss: 1.8222920894622803
Epoch 1550, training loss: 0.6133182048797607 = 0.0025837800931185484 + 0.1 * 6.107344150543213
Epoch 1550, val loss: 1.8247755765914917
Epoch 1560, training loss: 0.613471508026123 = 0.0025523293297737837 + 0.1 * 6.10919189453125
Epoch 1560, val loss: 1.827261209487915
Epoch 1570, training loss: 0.6132610440254211 = 0.002521594287827611 + 0.1 * 6.107394695281982
Epoch 1570, val loss: 1.829686164855957
Epoch 1580, training loss: 0.6139117479324341 = 0.002491574501618743 + 0.1 * 6.114201545715332
Epoch 1580, val loss: 1.8320934772491455
Epoch 1590, training loss: 0.6129107475280762 = 0.002462139120325446 + 0.1 * 6.104485988616943
Epoch 1590, val loss: 1.83449387550354
Epoch 1600, training loss: 0.6132693886756897 = 0.0024333603214472532 + 0.1 * 6.108360290527344
Epoch 1600, val loss: 1.8368580341339111
Epoch 1610, training loss: 0.6146260499954224 = 0.002405199920758605 + 0.1 * 6.122208118438721
Epoch 1610, val loss: 1.8391404151916504
Epoch 1620, training loss: 0.6121770143508911 = 0.0023778406903147697 + 0.1 * 6.097991466522217
Epoch 1620, val loss: 1.841370701789856
Epoch 1630, training loss: 0.6130476593971252 = 0.0023510002065449953 + 0.1 * 6.106966495513916
Epoch 1630, val loss: 1.8437570333480835
Epoch 1640, training loss: 0.6124993562698364 = 0.0023246316704899073 + 0.1 * 6.101747035980225
Epoch 1640, val loss: 1.8459815979003906
Epoch 1650, training loss: 0.6131560206413269 = 0.002298822393640876 + 0.1 * 6.108572006225586
Epoch 1650, val loss: 1.8482569456100464
Epoch 1660, training loss: 0.6126241087913513 = 0.0022735416423529387 + 0.1 * 6.103505611419678
Epoch 1660, val loss: 1.850393533706665
Epoch 1670, training loss: 0.6121340990066528 = 0.002248892094939947 + 0.1 * 6.098852157592773
Epoch 1670, val loss: 1.8525553941726685
Epoch 1680, training loss: 0.6131691336631775 = 0.0022247829474508762 + 0.1 * 6.109443187713623
Epoch 1680, val loss: 1.8546655178070068
Epoch 1690, training loss: 0.611515998840332 = 0.0022012051194906235 + 0.1 * 6.0931477546691895
Epoch 1690, val loss: 1.8567432165145874
Epoch 1700, training loss: 0.613369345664978 = 0.002178123453631997 + 0.1 * 6.111912250518799
Epoch 1700, val loss: 1.8588508367538452
Epoch 1710, training loss: 0.6113689541816711 = 0.0021554192062467337 + 0.1 * 6.092135429382324
Epoch 1710, val loss: 1.8608896732330322
Epoch 1720, training loss: 0.6123006343841553 = 0.002133175963535905 + 0.1 * 6.101674556732178
Epoch 1720, val loss: 1.863032341003418
Epoch 1730, training loss: 0.6120954155921936 = 0.0021113159600645304 + 0.1 * 6.099841117858887
Epoch 1730, val loss: 1.8650639057159424
Epoch 1740, training loss: 0.611488938331604 = 0.002089965157210827 + 0.1 * 6.093989372253418
Epoch 1740, val loss: 1.8670644760131836
Epoch 1750, training loss: 0.6141420006752014 = 0.002068980596959591 + 0.1 * 6.120729923248291
Epoch 1750, val loss: 1.8691024780273438
Epoch 1760, training loss: 0.6121466159820557 = 0.0020484605338424444 + 0.1 * 6.100981712341309
Epoch 1760, val loss: 1.870905876159668
Epoch 1770, training loss: 0.6110997796058655 = 0.0020283262711018324 + 0.1 * 6.090714454650879
Epoch 1770, val loss: 1.872973084449768
Epoch 1780, training loss: 0.6116087436676025 = 0.00200852332636714 + 0.1 * 6.096002101898193
Epoch 1780, val loss: 1.8749380111694336
Epoch 1790, training loss: 0.6120091676712036 = 0.0019891266711056232 + 0.1 * 6.100200176239014
Epoch 1790, val loss: 1.876866340637207
Epoch 1800, training loss: 0.6112745404243469 = 0.001970072044059634 + 0.1 * 6.093044757843018
Epoch 1800, val loss: 1.8786237239837646
Epoch 1810, training loss: 0.611245334148407 = 0.001951478305272758 + 0.1 * 6.0929388999938965
Epoch 1810, val loss: 1.8804576396942139
Epoch 1820, training loss: 0.6108515858650208 = 0.0019332247320562601 + 0.1 * 6.089183807373047
Epoch 1820, val loss: 1.882336974143982
Epoch 1830, training loss: 0.6123740077018738 = 0.001915269298478961 + 0.1 * 6.104587078094482
Epoch 1830, val loss: 1.884150505065918
Epoch 1840, training loss: 0.6111154556274414 = 0.0018976442515850067 + 0.1 * 6.092177867889404
Epoch 1840, val loss: 1.8858983516693115
Epoch 1850, training loss: 0.6105566620826721 = 0.00188035040628165 + 0.1 * 6.08676290512085
Epoch 1850, val loss: 1.8877010345458984
Epoch 1860, training loss: 0.61073237657547 = 0.0018633307190611959 + 0.1 * 6.088690280914307
Epoch 1860, val loss: 1.8895987272262573
Epoch 1870, training loss: 0.6103779673576355 = 0.0018465108005329967 + 0.1 * 6.085314750671387
Epoch 1870, val loss: 1.891310691833496
Epoch 1880, training loss: 0.6118822693824768 = 0.0018300748197361827 + 0.1 * 6.100521564483643
Epoch 1880, val loss: 1.8930132389068604
Epoch 1890, training loss: 0.6106047630310059 = 0.0018139593303203583 + 0.1 * 6.087907791137695
Epoch 1890, val loss: 1.8946536779403687
Epoch 1900, training loss: 0.6113156676292419 = 0.0017981661949306726 + 0.1 * 6.095174789428711
Epoch 1900, val loss: 1.8964097499847412
Epoch 1910, training loss: 0.6110714673995972 = 0.0017826122930273414 + 0.1 * 6.092888832092285
Epoch 1910, val loss: 1.8981002569198608
Epoch 1920, training loss: 0.6104464530944824 = 0.0017673289403319359 + 0.1 * 6.086791038513184
Epoch 1920, val loss: 1.899653673171997
Epoch 1930, training loss: 0.6103179454803467 = 0.0017523678252473474 + 0.1 * 6.085655689239502
Epoch 1930, val loss: 1.9013525247573853
Epoch 1940, training loss: 0.6105166077613831 = 0.0017375964671373367 + 0.1 * 6.087790012359619
Epoch 1940, val loss: 1.9030574560165405
Epoch 1950, training loss: 0.6109552383422852 = 0.001723084133118391 + 0.1 * 6.092321395874023
Epoch 1950, val loss: 1.9046359062194824
Epoch 1960, training loss: 0.6108208298683167 = 0.001708816853351891 + 0.1 * 6.09112024307251
Epoch 1960, val loss: 1.9061697721481323
Epoch 1970, training loss: 0.6097995638847351 = 0.0016948326956480742 + 0.1 * 6.081047058105469
Epoch 1970, val loss: 1.9077309370040894
Epoch 1980, training loss: 0.6104093790054321 = 0.0016810663510113955 + 0.1 * 6.087282657623291
Epoch 1980, val loss: 1.9093804359436035
Epoch 1990, training loss: 0.6095119714736938 = 0.0016674554208293557 + 0.1 * 6.078444957733154
Epoch 1990, val loss: 1.9109454154968262
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6716
Flip ASR: 0.6178/225 nodes
The final ASR:0.66175, 0.00696, Accuracy:0.80617, 0.01944
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10590])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.98524, 0.01044, Accuracy:0.83704, 0.00800
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.7792677879333496 = 1.9418878555297852 + 0.1 * 8.373799324035645
Epoch 0, val loss: 1.9363892078399658
Epoch 10, training loss: 2.7690253257751465 = 1.9316720962524414 + 0.1 * 8.373531341552734
Epoch 10, val loss: 1.9268287420272827
Epoch 20, training loss: 2.7561709880828857 = 1.9189766645431519 + 0.1 * 8.371942520141602
Epoch 20, val loss: 1.9145926237106323
Epoch 30, training loss: 2.737143039703369 = 1.9011636972427368 + 0.1 * 8.359794616699219
Epoch 30, val loss: 1.8972066640853882
Epoch 40, training loss: 2.702361822128296 = 1.8752864599227905 + 0.1 * 8.270752906799316
Epoch 40, val loss: 1.8723607063293457
Epoch 50, training loss: 2.615485668182373 = 1.8425816297531128 + 0.1 * 7.729039192199707
Epoch 50, val loss: 1.842908263206482
Epoch 60, training loss: 2.5415308475494385 = 1.8114492893218994 + 0.1 * 7.300815105438232
Epoch 60, val loss: 1.81667160987854
Epoch 70, training loss: 2.476224899291992 = 1.7831578254699707 + 0.1 * 6.930669784545898
Epoch 70, val loss: 1.7938807010650635
Epoch 80, training loss: 2.4296350479125977 = 1.7542970180511475 + 0.1 * 6.753379821777344
Epoch 80, val loss: 1.77028489112854
Epoch 90, training loss: 2.3830971717834473 = 1.7176769971847534 + 0.1 * 6.654202938079834
Epoch 90, val loss: 1.7394869327545166
Epoch 100, training loss: 2.3274245262145996 = 1.6680681705474854 + 0.1 * 6.593563556671143
Epoch 100, val loss: 1.697870135307312
Epoch 110, training loss: 2.2572243213653564 = 1.6014187335968018 + 0.1 * 6.558055400848389
Epoch 110, val loss: 1.6430834531784058
Epoch 120, training loss: 2.170910120010376 = 1.516994595527649 + 0.1 * 6.539154529571533
Epoch 120, val loss: 1.5747711658477783
Epoch 130, training loss: 2.0732593536376953 = 1.4205220937728882 + 0.1 * 6.527371406555176
Epoch 130, val loss: 1.4972504377365112
Epoch 140, training loss: 1.9710670709609985 = 1.319581389427185 + 0.1 * 6.514856815338135
Epoch 140, val loss: 1.4172484874725342
Epoch 150, training loss: 1.8686566352844238 = 1.217837929725647 + 0.1 * 6.508186340332031
Epoch 150, val loss: 1.3373860120773315
Epoch 160, training loss: 1.7703313827514648 = 1.1211748123168945 + 0.1 * 6.491564750671387
Epoch 160, val loss: 1.262398362159729
Epoch 170, training loss: 1.6801331043243408 = 1.032312035560608 + 0.1 * 6.478210926055908
Epoch 170, val loss: 1.1940548419952393
Epoch 180, training loss: 1.6000676155090332 = 0.9528526067733765 + 0.1 * 6.4721503257751465
Epoch 180, val loss: 1.1337095499038696
Epoch 190, training loss: 1.5269196033477783 = 0.8814037442207336 + 0.1 * 6.455158710479736
Epoch 190, val loss: 1.0804351568222046
Epoch 200, training loss: 1.4577804803848267 = 0.8132691383361816 + 0.1 * 6.445113182067871
Epoch 200, val loss: 1.0296510457992554
Epoch 210, training loss: 1.3900256156921387 = 0.7463818192481995 + 0.1 * 6.43643856048584
Epoch 210, val loss: 0.9799559712409973
Epoch 220, training loss: 1.3227646350860596 = 0.680020809173584 + 0.1 * 6.427438735961914
Epoch 220, val loss: 0.9311532378196716
Epoch 230, training loss: 1.2582027912139893 = 0.6163302659988403 + 0.1 * 6.41872501373291
Epoch 230, val loss: 0.8857763409614563
Epoch 240, training loss: 1.1982440948486328 = 0.557220995426178 + 0.1 * 6.410231113433838
Epoch 240, val loss: 0.8464374542236328
Epoch 250, training loss: 1.1442837715148926 = 0.5041804909706116 + 0.1 * 6.401031970977783
Epoch 250, val loss: 0.8145037889480591
Epoch 260, training loss: 1.0962889194488525 = 0.4570831060409546 + 0.1 * 6.3920578956604
Epoch 260, val loss: 0.7896820902824402
Epoch 270, training loss: 1.053991436958313 = 0.4147971570491791 + 0.1 * 6.391942501068115
Epoch 270, val loss: 0.7702229619026184
Epoch 280, training loss: 1.0146231651306152 = 0.3765135705471039 + 0.1 * 6.3810954093933105
Epoch 280, val loss: 0.7548649907112122
Epoch 290, training loss: 0.9781219959259033 = 0.3411461412906647 + 0.1 * 6.369758605957031
Epoch 290, val loss: 0.7425188422203064
Epoch 300, training loss: 0.9444215297698975 = 0.3080354630947113 + 0.1 * 6.363860130310059
Epoch 300, val loss: 0.7324420213699341
Epoch 310, training loss: 0.9133782386779785 = 0.2769820988178253 + 0.1 * 6.363961696624756
Epoch 310, val loss: 0.7247204780578613
Epoch 320, training loss: 0.8832435607910156 = 0.24810892343521118 + 0.1 * 6.351346015930176
Epoch 320, val loss: 0.7190495729446411
Epoch 330, training loss: 0.8557150959968567 = 0.221296489238739 + 0.1 * 6.344185829162598
Epoch 330, val loss: 0.7156210541725159
Epoch 340, training loss: 0.8320156335830688 = 0.19659054279327393 + 0.1 * 6.354250907897949
Epoch 340, val loss: 0.7142928242683411
Epoch 350, training loss: 0.8075649738311768 = 0.1743866503238678 + 0.1 * 6.331782817840576
Epoch 350, val loss: 0.7151867747306824
Epoch 360, training loss: 0.7870181202888489 = 0.1547912359237671 + 0.1 * 6.322268962860107
Epoch 360, val loss: 0.7181061506271362
Epoch 370, training loss: 0.770165205001831 = 0.13786491751670837 + 0.1 * 6.323002815246582
Epoch 370, val loss: 0.7229230999946594
Epoch 380, training loss: 0.754626989364624 = 0.12342778593301773 + 0.1 * 6.3119916915893555
Epoch 380, val loss: 0.7295090556144714
Epoch 390, training loss: 0.7422003149986267 = 0.11108364909887314 + 0.1 * 6.311166763305664
Epoch 390, val loss: 0.7373129725456238
Epoch 400, training loss: 0.7302746176719666 = 0.10051079839468002 + 0.1 * 6.297638416290283
Epoch 400, val loss: 0.7461140751838684
Epoch 410, training loss: 0.7209246158599854 = 0.091341532766819 + 0.1 * 6.295830249786377
Epoch 410, val loss: 0.7555387020111084
Epoch 420, training loss: 0.71221923828125 = 0.08335502445697784 + 0.1 * 6.288642406463623
Epoch 420, val loss: 0.7654215097427368
Epoch 430, training loss: 0.7050225734710693 = 0.07633612304925919 + 0.1 * 6.286864280700684
Epoch 430, val loss: 0.775604248046875
Epoch 440, training loss: 0.6975030899047852 = 0.07012968510389328 + 0.1 * 6.273734092712402
Epoch 440, val loss: 0.7858390808105469
Epoch 450, training loss: 0.6912086009979248 = 0.06459791958332062 + 0.1 * 6.266106605529785
Epoch 450, val loss: 0.7962391376495361
Epoch 460, training loss: 0.6867902278900146 = 0.059643495827913284 + 0.1 * 6.271467208862305
Epoch 460, val loss: 0.8065218329429626
Epoch 470, training loss: 0.6809749603271484 = 0.05520113930106163 + 0.1 * 6.2577385902404785
Epoch 470, val loss: 0.8168882131576538
Epoch 480, training loss: 0.6773998141288757 = 0.05118606984615326 + 0.1 * 6.262136936187744
Epoch 480, val loss: 0.8271999955177307
Epoch 490, training loss: 0.6728318333625793 = 0.0475543811917305 + 0.1 * 6.252774238586426
Epoch 490, val loss: 0.8374104499816895
Epoch 500, training loss: 0.6686205863952637 = 0.044267941266298294 + 0.1 * 6.243526458740234
Epoch 500, val loss: 0.8475945591926575
Epoch 510, training loss: 0.6650067567825317 = 0.041276343166828156 + 0.1 * 6.237304210662842
Epoch 510, val loss: 0.8576852679252625
Epoch 520, training loss: 0.6629214286804199 = 0.03855154290795326 + 0.1 * 6.243698596954346
Epoch 520, val loss: 0.8676864504814148
Epoch 530, training loss: 0.6595483422279358 = 0.036068327724933624 + 0.1 * 6.234800338745117
Epoch 530, val loss: 0.8775373697280884
Epoch 540, training loss: 0.6582198143005371 = 0.033799201250076294 + 0.1 * 6.244206428527832
Epoch 540, val loss: 0.8873714208602905
Epoch 550, training loss: 0.6543159484863281 = 0.031729210168123245 + 0.1 * 6.22586727142334
Epoch 550, val loss: 0.8968631625175476
Epoch 560, training loss: 0.6526590585708618 = 0.029830167070031166 + 0.1 * 6.2282891273498535
Epoch 560, val loss: 0.9063961505889893
Epoch 570, training loss: 0.6505357027053833 = 0.028096608817577362 + 0.1 * 6.224390506744385
Epoch 570, val loss: 0.9156183004379272
Epoch 580, training loss: 0.6481284499168396 = 0.02650606818497181 + 0.1 * 6.21622371673584
Epoch 580, val loss: 0.9248117804527283
Epoch 590, training loss: 0.6468496322631836 = 0.02504121884703636 + 0.1 * 6.21808385848999
Epoch 590, val loss: 0.93386310338974
Epoch 600, training loss: 0.6451963186264038 = 0.02369375340640545 + 0.1 * 6.215025424957275
Epoch 600, val loss: 0.9427598714828491
Epoch 610, training loss: 0.6437095403671265 = 0.022450385615229607 + 0.1 * 6.212591171264648
Epoch 610, val loss: 0.9514265060424805
Epoch 620, training loss: 0.6418828368186951 = 0.021304138004779816 + 0.1 * 6.205787181854248
Epoch 620, val loss: 0.9599992632865906
Epoch 630, training loss: 0.6402892470359802 = 0.02024242654442787 + 0.1 * 6.200467586517334
Epoch 630, val loss: 0.9683980345726013
Epoch 640, training loss: 0.6386620998382568 = 0.019259966909885406 + 0.1 * 6.194020748138428
Epoch 640, val loss: 0.9766390323638916
Epoch 650, training loss: 0.637690007686615 = 0.018345998600125313 + 0.1 * 6.193439960479736
Epoch 650, val loss: 0.9848207831382751
Epoch 660, training loss: 0.6380804181098938 = 0.01749476045370102 + 0.1 * 6.205856800079346
Epoch 660, val loss: 0.9928441643714905
Epoch 670, training loss: 0.6379154324531555 = 0.01670517772436142 + 0.1 * 6.212102890014648
Epoch 670, val loss: 1.0006376504898071
Epoch 680, training loss: 0.6352809071540833 = 0.01597263291478157 + 0.1 * 6.193082809448242
Epoch 680, val loss: 1.008273959159851
Epoch 690, training loss: 0.6336097717285156 = 0.015288476832211018 + 0.1 * 6.183212757110596
Epoch 690, val loss: 1.015913963317871
Epoch 700, training loss: 0.6335379481315613 = 0.014645823277533054 + 0.1 * 6.188920974731445
Epoch 700, val loss: 1.0233993530273438
Epoch 710, training loss: 0.6319572925567627 = 0.014043169096112251 + 0.1 * 6.179141521453857
Epoch 710, val loss: 1.0306979417800903
Epoch 720, training loss: 0.6325644850730896 = 0.013479113578796387 + 0.1 * 6.190853595733643
Epoch 720, val loss: 1.0379550457000732
Epoch 730, training loss: 0.6310346722602844 = 0.012950056232511997 + 0.1 * 6.180846214294434
Epoch 730, val loss: 1.0449949502944946
Epoch 740, training loss: 0.6300119757652283 = 0.012453941628336906 + 0.1 * 6.1755805015563965
Epoch 740, val loss: 1.051914930343628
Epoch 750, training loss: 0.6290371417999268 = 0.011986400932073593 + 0.1 * 6.170507431030273
Epoch 750, val loss: 1.0587931871414185
Epoch 760, training loss: 0.6300381422042847 = 0.011544114910066128 + 0.1 * 6.184940338134766
Epoch 760, val loss: 1.065476655960083
Epoch 770, training loss: 0.6291871070861816 = 0.011129133403301239 + 0.1 * 6.180579662322998
Epoch 770, val loss: 1.0719760656356812
Epoch 780, training loss: 0.6276419162750244 = 0.010736997239291668 + 0.1 * 6.169049263000488
Epoch 780, val loss: 1.0783365964889526
Epoch 790, training loss: 0.6275494694709778 = 0.010368446819484234 + 0.1 * 6.171810150146484
Epoch 790, val loss: 1.0846118927001953
Epoch 800, training loss: 0.6260807514190674 = 0.010020064190030098 + 0.1 * 6.160606384277344
Epoch 800, val loss: 1.0907341241836548
Epoch 810, training loss: 0.6258304119110107 = 0.009689141996204853 + 0.1 * 6.161412715911865
Epoch 810, val loss: 1.09683096408844
Epoch 820, training loss: 0.6255850791931152 = 0.009374224580824375 + 0.1 * 6.162108421325684
Epoch 820, val loss: 1.102794885635376
Epoch 830, training loss: 0.6265167593955994 = 0.00907661858946085 + 0.1 * 6.17440128326416
Epoch 830, val loss: 1.1086267232894897
Epoch 840, training loss: 0.6244488954544067 = 0.008794423192739487 + 0.1 * 6.1565446853637695
Epoch 840, val loss: 1.1142218112945557
Epoch 850, training loss: 0.6237610578536987 = 0.008527512662112713 + 0.1 * 6.152335166931152
Epoch 850, val loss: 1.119947910308838
Epoch 860, training loss: 0.6233567595481873 = 0.008272252976894379 + 0.1 * 6.150845050811768
Epoch 860, val loss: 1.1255502700805664
Epoch 870, training loss: 0.6233276128768921 = 0.008028813637793064 + 0.1 * 6.152987957000732
Epoch 870, val loss: 1.1309884786605835
Epoch 880, training loss: 0.6228057742118835 = 0.007798239588737488 + 0.1 * 6.1500749588012695
Epoch 880, val loss: 1.1362898349761963
Epoch 890, training loss: 0.6235786080360413 = 0.007577778305858374 + 0.1 * 6.160008430480957
Epoch 890, val loss: 1.1415581703186035
Epoch 900, training loss: 0.621947169303894 = 0.0073675732128322124 + 0.1 * 6.145795822143555
Epoch 900, val loss: 1.1467280387878418
Epoch 910, training loss: 0.6223726868629456 = 0.007167156785726547 + 0.1 * 6.152055263519287
Epoch 910, val loss: 1.1518737077713013
Epoch 920, training loss: 0.6210505962371826 = 0.0069756824523210526 + 0.1 * 6.140748500823975
Epoch 920, val loss: 1.1568562984466553
Epoch 930, training loss: 0.6233508586883545 = 0.006792388390749693 + 0.1 * 6.165584564208984
Epoch 930, val loss: 1.161857008934021
Epoch 940, training loss: 0.6209724545478821 = 0.0066168955527246 + 0.1 * 6.143555641174316
Epoch 940, val loss: 1.1665838956832886
Epoch 950, training loss: 0.6201276779174805 = 0.006449707318097353 + 0.1 * 6.13677978515625
Epoch 950, val loss: 1.171427845954895
Epoch 960, training loss: 0.6201686263084412 = 0.00628947326913476 + 0.1 * 6.138791084289551
Epoch 960, val loss: 1.1761630773544312
Epoch 970, training loss: 0.6202014684677124 = 0.006134936586022377 + 0.1 * 6.140665054321289
Epoch 970, val loss: 1.1808167695999146
Epoch 980, training loss: 0.6193857789039612 = 0.005987276788800955 + 0.1 * 6.1339850425720215
Epoch 980, val loss: 1.1853735446929932
Epoch 990, training loss: 0.6196446418762207 = 0.005845984909683466 + 0.1 * 6.137986660003662
Epoch 990, val loss: 1.1898951530456543
Epoch 1000, training loss: 0.6198319792747498 = 0.005710163619369268 + 0.1 * 6.1412177085876465
Epoch 1000, val loss: 1.1943655014038086
Epoch 1010, training loss: 0.6186715960502625 = 0.005579684395343065 + 0.1 * 6.130918502807617
Epoch 1010, val loss: 1.1987031698226929
Epoch 1020, training loss: 0.6187498569488525 = 0.0054541644640266895 + 0.1 * 6.132956504821777
Epoch 1020, val loss: 1.2030622959136963
Epoch 1030, training loss: 0.6191538572311401 = 0.005333446431905031 + 0.1 * 6.138204097747803
Epoch 1030, val loss: 1.2073338031768799
Epoch 1040, training loss: 0.618398129940033 = 0.005216910503804684 + 0.1 * 6.13181209564209
Epoch 1040, val loss: 1.2114582061767578
Epoch 1050, training loss: 0.6187991499900818 = 0.005105402320623398 + 0.1 * 6.136937618255615
Epoch 1050, val loss: 1.2155872583389282
Epoch 1060, training loss: 0.6175007224082947 = 0.004997850861400366 + 0.1 * 6.125028610229492
Epoch 1060, val loss: 1.219660758972168
Epoch 1070, training loss: 0.6174803376197815 = 0.004894461017102003 + 0.1 * 6.125858783721924
Epoch 1070, val loss: 1.2237452268600464
Epoch 1080, training loss: 0.6174294948577881 = 0.004794417880475521 + 0.1 * 6.126350402832031
Epoch 1080, val loss: 1.2276995182037354
Epoch 1090, training loss: 0.6169209480285645 = 0.004697894211858511 + 0.1 * 6.122230052947998
Epoch 1090, val loss: 1.2315822839736938
Epoch 1100, training loss: 0.6168661713600159 = 0.0046048853546381 + 0.1 * 6.122612953186035
Epoch 1100, val loss: 1.2354310750961304
Epoch 1110, training loss: 0.6167386770248413 = 0.004514774773269892 + 0.1 * 6.122239112854004
Epoch 1110, val loss: 1.2392523288726807
Epoch 1120, training loss: 0.6162121295928955 = 0.004428240470588207 + 0.1 * 6.117838382720947
Epoch 1120, val loss: 1.2429604530334473
Epoch 1130, training loss: 0.616051197052002 = 0.004344503860920668 + 0.1 * 6.117066383361816
Epoch 1130, val loss: 1.2467268705368042
Epoch 1140, training loss: 0.6164496541023254 = 0.004262823611497879 + 0.1 * 6.12186861038208
Epoch 1140, val loss: 1.2504037618637085
Epoch 1150, training loss: 0.6158120632171631 = 0.004184330347925425 + 0.1 * 6.116277694702148
Epoch 1150, val loss: 1.2539969682693481
Epoch 1160, training loss: 0.6160862445831299 = 0.004108386114239693 + 0.1 * 6.119778633117676
Epoch 1160, val loss: 1.2576043605804443
Epoch 1170, training loss: 0.615576982498169 = 0.004034683108329773 + 0.1 * 6.11542272567749
Epoch 1170, val loss: 1.2611658573150635
Epoch 1180, training loss: 0.6151462197303772 = 0.003963232506066561 + 0.1 * 6.11182975769043
Epoch 1180, val loss: 1.2646592855453491
Epoch 1190, training loss: 0.6152343153953552 = 0.0038945903070271015 + 0.1 * 6.113397121429443
Epoch 1190, val loss: 1.2681689262390137
Epoch 1200, training loss: 0.6156023740768433 = 0.0038272610399872065 + 0.1 * 6.117750644683838
Epoch 1200, val loss: 1.2716912031173706
Epoch 1210, training loss: 0.6151852011680603 = 0.0037621797528117895 + 0.1 * 6.114230155944824
Epoch 1210, val loss: 1.275061011314392
Epoch 1220, training loss: 0.615151047706604 = 0.003699048887938261 + 0.1 * 6.1145195960998535
Epoch 1220, val loss: 1.2783913612365723
Epoch 1230, training loss: 0.6145979166030884 = 0.003637999063357711 + 0.1 * 6.1095991134643555
Epoch 1230, val loss: 1.281680941581726
Epoch 1240, training loss: 0.614680290222168 = 0.0035784796345978975 + 0.1 * 6.11101770401001
Epoch 1240, val loss: 1.2850128412246704
Epoch 1250, training loss: 0.613843560218811 = 0.003520994447171688 + 0.1 * 6.1032257080078125
Epoch 1250, val loss: 1.2882791757583618
Epoch 1260, training loss: 0.6141260266304016 = 0.003465041983872652 + 0.1 * 6.10660982131958
Epoch 1260, val loss: 1.2915812730789185
Epoch 1270, training loss: 0.6142802238464355 = 0.0034104178193956614 + 0.1 * 6.108697891235352
Epoch 1270, val loss: 1.2947744131088257
Epoch 1280, training loss: 0.6133368611335754 = 0.0033574120607227087 + 0.1 * 6.099794387817383
Epoch 1280, val loss: 1.2978968620300293
Epoch 1290, training loss: 0.6153988242149353 = 0.003306193742901087 + 0.1 * 6.1209259033203125
Epoch 1290, val loss: 1.3010451793670654
Epoch 1300, training loss: 0.6137190461158752 = 0.003256218507885933 + 0.1 * 6.104628086090088
Epoch 1300, val loss: 1.3040804862976074
Epoch 1310, training loss: 0.6130253672599792 = 0.003207874484360218 + 0.1 * 6.098175048828125
Epoch 1310, val loss: 1.307106375694275
Epoch 1320, training loss: 0.6136192679405212 = 0.003161077620461583 + 0.1 * 6.104581832885742
Epoch 1320, val loss: 1.310176968574524
Epoch 1330, training loss: 0.6125074625015259 = 0.003115106839686632 + 0.1 * 6.093923091888428
Epoch 1330, val loss: 1.3131555318832397
Epoch 1340, training loss: 0.6144033670425415 = 0.003070523263886571 + 0.1 * 6.113327980041504
Epoch 1340, val loss: 1.3161228895187378
Epoch 1350, training loss: 0.613503634929657 = 0.003026969963684678 + 0.1 * 6.104766368865967
Epoch 1350, val loss: 1.3190287351608276
Epoch 1360, training loss: 0.6131436228752136 = 0.002984297927469015 + 0.1 * 6.101593494415283
Epoch 1360, val loss: 1.3219189643859863
Epoch 1370, training loss: 0.6124380826950073 = 0.0029432550072669983 + 0.1 * 6.094947814941406
Epoch 1370, val loss: 1.3248270750045776
Epoch 1380, training loss: 0.6123685240745544 = 0.0029031161684542894 + 0.1 * 6.094654083251953
Epoch 1380, val loss: 1.3277448415756226
Epoch 1390, training loss: 0.6122426390647888 = 0.0028636599890887737 + 0.1 * 6.093790054321289
Epoch 1390, val loss: 1.3305860757827759
Epoch 1400, training loss: 0.6139374375343323 = 0.0028253658674657345 + 0.1 * 6.111120700836182
Epoch 1400, val loss: 1.3333936929702759
Epoch 1410, training loss: 0.6121156811714172 = 0.0027877064421772957 + 0.1 * 6.0932793617248535
Epoch 1410, val loss: 1.3360600471496582
Epoch 1420, training loss: 0.6115215420722961 = 0.002751571126282215 + 0.1 * 6.0876994132995605
Epoch 1420, val loss: 1.3388203382492065
Epoch 1430, training loss: 0.6133596301078796 = 0.0027160486206412315 + 0.1 * 6.106435298919678
Epoch 1430, val loss: 1.341623306274414
Epoch 1440, training loss: 0.6119852066040039 = 0.002681031823158264 + 0.1 * 6.093041896820068
Epoch 1440, val loss: 1.34431791305542
Epoch 1450, training loss: 0.6115176677703857 = 0.0026472867466509342 + 0.1 * 6.088703632354736
Epoch 1450, val loss: 1.3469619750976562
Epoch 1460, training loss: 0.6116703748703003 = 0.002614374505355954 + 0.1 * 6.090559959411621
Epoch 1460, val loss: 1.349699854850769
Epoch 1470, training loss: 0.6115422248840332 = 0.002581836422905326 + 0.1 * 6.089603900909424
Epoch 1470, val loss: 1.3524017333984375
Epoch 1480, training loss: 0.6120138168334961 = 0.0025499416515231133 + 0.1 * 6.094638824462891
Epoch 1480, val loss: 1.3549977540969849
Epoch 1490, training loss: 0.6110230088233948 = 0.002519013825803995 + 0.1 * 6.0850396156311035
Epoch 1490, val loss: 1.3575248718261719
Epoch 1500, training loss: 0.6110086441040039 = 0.002488936297595501 + 0.1 * 6.085197448730469
Epoch 1500, val loss: 1.3601561784744263
Epoch 1510, training loss: 0.6116839051246643 = 0.0024593437556177378 + 0.1 * 6.092245578765869
Epoch 1510, val loss: 1.3627939224243164
Epoch 1520, training loss: 0.6114292740821838 = 0.002430194756016135 + 0.1 * 6.089990615844727
Epoch 1520, val loss: 1.3652408123016357
Epoch 1530, training loss: 0.6110129952430725 = 0.002401776611804962 + 0.1 * 6.086112022399902
Epoch 1530, val loss: 1.3677000999450684
Epoch 1540, training loss: 0.6106133460998535 = 0.0023742017801851034 + 0.1 * 6.082391738891602
Epoch 1540, val loss: 1.370132565498352
Epoch 1550, training loss: 0.6118320226669312 = 0.002347205299884081 + 0.1 * 6.094847679138184
Epoch 1550, val loss: 1.3726381063461304
Epoch 1560, training loss: 0.6107698082923889 = 0.0023205357138067484 + 0.1 * 6.0844926834106445
Epoch 1560, val loss: 1.3750693798065186
Epoch 1570, training loss: 0.610720157623291 = 0.0022946232929825783 + 0.1 * 6.084255695343018
Epoch 1570, val loss: 1.3774281740188599
Epoch 1580, training loss: 0.6100607514381409 = 0.0022694359067827463 + 0.1 * 6.0779128074646
Epoch 1580, val loss: 1.3798259496688843
Epoch 1590, training loss: 0.6111741662025452 = 0.002244609175249934 + 0.1 * 6.089295387268066
Epoch 1590, val loss: 1.3822239637374878
Epoch 1600, training loss: 0.6107849478721619 = 0.0022203417029231787 + 0.1 * 6.085646152496338
Epoch 1600, val loss: 1.384507179260254
Epoch 1610, training loss: 0.6105705499649048 = 0.002196602988988161 + 0.1 * 6.083739280700684
Epoch 1610, val loss: 1.3867895603179932
Epoch 1620, training loss: 0.6101396679878235 = 0.002173513872548938 + 0.1 * 6.0796613693237305
Epoch 1620, val loss: 1.389045000076294
Epoch 1630, training loss: 0.6113870739936829 = 0.0021508666686713696 + 0.1 * 6.0923614501953125
Epoch 1630, val loss: 1.3913613557815552
Epoch 1640, training loss: 0.6105805039405823 = 0.0021282550878822803 + 0.1 * 6.084522247314453
Epoch 1640, val loss: 1.393587350845337
Epoch 1650, training loss: 0.6097112894058228 = 0.0021065527107566595 + 0.1 * 6.076047420501709
Epoch 1650, val loss: 1.3957483768463135
Epoch 1660, training loss: 0.6100333333015442 = 0.002085315063595772 + 0.1 * 6.079480171203613
Epoch 1660, val loss: 1.3980165719985962
Epoch 1670, training loss: 0.6097652912139893 = 0.0020642688032239676 + 0.1 * 6.077010154724121
Epoch 1670, val loss: 1.400319218635559
Epoch 1680, training loss: 0.609991729259491 = 0.002043743385002017 + 0.1 * 6.079479694366455
Epoch 1680, val loss: 1.4024988412857056
Epoch 1690, training loss: 0.6093024611473083 = 0.002023502252995968 + 0.1 * 6.072789192199707
Epoch 1690, val loss: 1.4046342372894287
Epoch 1700, training loss: 0.6107542514801025 = 0.002003729110583663 + 0.1 * 6.087504863739014
Epoch 1700, val loss: 1.4067851305007935
Epoch 1710, training loss: 0.6102451682090759 = 0.0019841387402266264 + 0.1 * 6.082610607147217
Epoch 1710, val loss: 1.4089466333389282
Epoch 1720, training loss: 0.6096629500389099 = 0.001965057570487261 + 0.1 * 6.076979160308838
Epoch 1720, val loss: 1.4110002517700195
Epoch 1730, training loss: 0.6096718311309814 = 0.0019464802462607622 + 0.1 * 6.077253341674805
Epoch 1730, val loss: 1.4131286144256592
Epoch 1740, training loss: 0.6093735694885254 = 0.00192816904745996 + 0.1 * 6.074453830718994
Epoch 1740, val loss: 1.4152257442474365
Epoch 1750, training loss: 0.6094905138015747 = 0.0019101109355688095 + 0.1 * 6.075804233551025
Epoch 1750, val loss: 1.4173210859298706
Epoch 1760, training loss: 0.6094200015068054 = 0.001892322557978332 + 0.1 * 6.0752763748168945
Epoch 1760, val loss: 1.4193698167800903
Epoch 1770, training loss: 0.6101543307304382 = 0.0018749316222965717 + 0.1 * 6.082793712615967
Epoch 1770, val loss: 1.4214366674423218
Epoch 1780, training loss: 0.6089876294136047 = 0.0018578084418550134 + 0.1 * 6.071298122406006
Epoch 1780, val loss: 1.4234634637832642
Epoch 1790, training loss: 0.6091047525405884 = 0.0018411558121442795 + 0.1 * 6.072636127471924
Epoch 1790, val loss: 1.4254933595657349
Epoch 1800, training loss: 0.6088635921478271 = 0.0018247634870931506 + 0.1 * 6.070388317108154
Epoch 1800, val loss: 1.4275180101394653
Epoch 1810, training loss: 0.6090908646583557 = 0.001808555331081152 + 0.1 * 6.072822570800781
Epoch 1810, val loss: 1.429520845413208
Epoch 1820, training loss: 0.6093416213989258 = 0.0017927270382642746 + 0.1 * 6.075488567352295
Epoch 1820, val loss: 1.431500792503357
Epoch 1830, training loss: 0.6088294386863708 = 0.0017771351849660277 + 0.1 * 6.070523262023926
Epoch 1830, val loss: 1.4334125518798828
Epoch 1840, training loss: 0.6095079183578491 = 0.0017618233105167747 + 0.1 * 6.077461242675781
Epoch 1840, val loss: 1.4353840351104736
Epoch 1850, training loss: 0.6087785363197327 = 0.001746675232425332 + 0.1 * 6.070318698883057
Epoch 1850, val loss: 1.4373316764831543
Epoch 1860, training loss: 0.608607292175293 = 0.0017319215694442391 + 0.1 * 6.068753242492676
Epoch 1860, val loss: 1.4392441511154175
Epoch 1870, training loss: 0.6083852648735046 = 0.0017173700034618378 + 0.1 * 6.066678524017334
Epoch 1870, val loss: 1.4412100315093994
Epoch 1880, training loss: 0.6084119081497192 = 0.0017031586030498147 + 0.1 * 6.067087650299072
Epoch 1880, val loss: 1.4431730508804321
Epoch 1890, training loss: 0.6097243428230286 = 0.0016890945844352245 + 0.1 * 6.080352783203125
Epoch 1890, val loss: 1.4450613260269165
Epoch 1900, training loss: 0.6084215641021729 = 0.0016751618823036551 + 0.1 * 6.0674638748168945
Epoch 1900, val loss: 1.4469082355499268
Epoch 1910, training loss: 0.6085045337677002 = 0.001661609043367207 + 0.1 * 6.068429470062256
Epoch 1910, val loss: 1.4487642049789429
Epoch 1920, training loss: 0.6085559129714966 = 0.001648260047659278 + 0.1 * 6.0690765380859375
Epoch 1920, val loss: 1.4506582021713257
Epoch 1930, training loss: 0.6078386306762695 = 0.0016349854413419962 + 0.1 * 6.062036037445068
Epoch 1930, val loss: 1.4526077508926392
Epoch 1940, training loss: 0.6090774536132812 = 0.0016219839453697205 + 0.1 * 6.074554920196533
Epoch 1940, val loss: 1.4544785022735596
Epoch 1950, training loss: 0.6079688668251038 = 0.001609219703823328 + 0.1 * 6.063596725463867
Epoch 1950, val loss: 1.4562294483184814
Epoch 1960, training loss: 0.6097826957702637 = 0.0015966568607836962 + 0.1 * 6.081860542297363
Epoch 1960, val loss: 1.458109736442566
Epoch 1970, training loss: 0.6085872650146484 = 0.0015842383727431297 + 0.1 * 6.070030212402344
Epoch 1970, val loss: 1.459875226020813
Epoch 1980, training loss: 0.6083986759185791 = 0.0015720168594270945 + 0.1 * 6.06826639175415
Epoch 1980, val loss: 1.4616655111312866
Epoch 1990, training loss: 0.6075941324234009 = 0.001560129807330668 + 0.1 * 6.06033992767334
Epoch 1990, val loss: 1.4634203910827637
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5904
Flip ASR: 0.5111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7677292823791504 = 1.9303470849990845 + 0.1 * 8.373821258544922
Epoch 0, val loss: 1.9260351657867432
Epoch 10, training loss: 2.757999897003174 = 1.9206390380859375 + 0.1 * 8.373608589172363
Epoch 10, val loss: 1.9162977933883667
Epoch 20, training loss: 2.745783805847168 = 1.9085330963134766 + 0.1 * 8.372507095336914
Epoch 20, val loss: 1.9036921262741089
Epoch 30, training loss: 2.727968692779541 = 1.8914819955825806 + 0.1 * 8.364867210388184
Epoch 30, val loss: 1.8857181072235107
Epoch 40, training loss: 2.697288990020752 = 1.8664735555648804 + 0.1 * 8.308154106140137
Epoch 40, val loss: 1.8595064878463745
Epoch 50, training loss: 2.6143295764923096 = 1.8334828615188599 + 0.1 * 7.808467388153076
Epoch 50, val loss: 1.8265607357025146
Epoch 60, training loss: 2.5225038528442383 = 1.8006184101104736 + 0.1 * 7.218855381011963
Epoch 60, val loss: 1.7959380149841309
Epoch 70, training loss: 2.4591598510742188 = 1.7670512199401855 + 0.1 * 6.921087265014648
Epoch 70, val loss: 1.7650164365768433
Epoch 80, training loss: 2.407522201538086 = 1.7296607494354248 + 0.1 * 6.778613090515137
Epoch 80, val loss: 1.7315870523452759
Epoch 90, training loss: 2.351773738861084 = 1.6820435523986816 + 0.1 * 6.697301864624023
Epoch 90, val loss: 1.6899453401565552
Epoch 100, training loss: 2.2824928760528564 = 1.6173957586288452 + 0.1 * 6.650970458984375
Epoch 100, val loss: 1.6347460746765137
Epoch 110, training loss: 2.1952924728393555 = 1.532962679862976 + 0.1 * 6.623297214508057
Epoch 110, val loss: 1.5640400648117065
Epoch 120, training loss: 2.091738224029541 = 1.4311898946762085 + 0.1 * 6.6054840087890625
Epoch 120, val loss: 1.4803941249847412
Epoch 130, training loss: 1.9791843891143799 = 1.319931983947754 + 0.1 * 6.59252405166626
Epoch 130, val loss: 1.3908120393753052
Epoch 140, training loss: 1.8633666038513184 = 1.205308437347412 + 0.1 * 6.580580711364746
Epoch 140, val loss: 1.3012709617614746
Epoch 150, training loss: 1.747503399848938 = 1.0906906127929688 + 0.1 * 6.568127632141113
Epoch 150, val loss: 1.2134016752243042
Epoch 160, training loss: 1.6364620923995972 = 0.9807776212692261 + 0.1 * 6.556844711303711
Epoch 160, val loss: 1.1304996013641357
Epoch 170, training loss: 1.5378289222717285 = 0.8830089569091797 + 0.1 * 6.548198699951172
Epoch 170, val loss: 1.0590181350708008
Epoch 180, training loss: 1.454738974571228 = 0.8009764552116394 + 0.1 * 6.537624835968018
Epoch 180, val loss: 1.0019142627716064
Epoch 190, training loss: 1.3863685131072998 = 0.7336338758468628 + 0.1 * 6.527347087860107
Epoch 190, val loss: 0.958066463470459
Epoch 200, training loss: 1.3304415941238403 = 0.6781116724014282 + 0.1 * 6.523299217224121
Epoch 200, val loss: 0.9256815314292908
Epoch 210, training loss: 1.281428575515747 = 0.630769670009613 + 0.1 * 6.506588459014893
Epoch 210, val loss: 0.9014777541160583
Epoch 220, training loss: 1.2371344566345215 = 0.5878450274467468 + 0.1 * 6.492894649505615
Epoch 220, val loss: 0.8818264603614807
Epoch 230, training loss: 1.195375919342041 = 0.5473721027374268 + 0.1 * 6.480038166046143
Epoch 230, val loss: 0.8647004961967468
Epoch 240, training loss: 1.1556026935577393 = 0.5084004402160645 + 0.1 * 6.472022533416748
Epoch 240, val loss: 0.8488733172416687
Epoch 250, training loss: 1.1166832447052002 = 0.4708765745162964 + 0.1 * 6.458067417144775
Epoch 250, val loss: 0.8346124887466431
Epoch 260, training loss: 1.079809546470642 = 0.4350897967815399 + 0.1 * 6.447196960449219
Epoch 260, val loss: 0.8222333192825317
Epoch 270, training loss: 1.0448609590530396 = 0.40118470788002014 + 0.1 * 6.436762809753418
Epoch 270, val loss: 0.8119285106658936
Epoch 280, training loss: 1.0125200748443604 = 0.36945411562919617 + 0.1 * 6.430659770965576
Epoch 280, val loss: 0.8037773966789246
Epoch 290, training loss: 0.9817167520523071 = 0.33988282084465027 + 0.1 * 6.418338775634766
Epoch 290, val loss: 0.7979313135147095
Epoch 300, training loss: 0.9532268643379211 = 0.31230878829956055 + 0.1 * 6.409180641174316
Epoch 300, val loss: 0.7942296862602234
Epoch 310, training loss: 0.9268827438354492 = 0.2867468595504761 + 0.1 * 6.401358604431152
Epoch 310, val loss: 0.7926192283630371
Epoch 320, training loss: 0.9015294909477234 = 0.26279765367507935 + 0.1 * 6.387318134307861
Epoch 320, val loss: 0.7929046750068665
Epoch 330, training loss: 0.8811390399932861 = 0.24026095867156982 + 0.1 * 6.408780574798584
Epoch 330, val loss: 0.7951005101203918
Epoch 340, training loss: 0.8579326868057251 = 0.21948276460170746 + 0.1 * 6.3844990730285645
Epoch 340, val loss: 0.7991216778755188
Epoch 350, training loss: 0.8367547988891602 = 0.2003479152917862 + 0.1 * 6.364068984985352
Epoch 350, val loss: 0.8049048781394958
Epoch 360, training loss: 0.8190193176269531 = 0.18277482688426971 + 0.1 * 6.3624444007873535
Epoch 360, val loss: 0.8123021125793457
Epoch 370, training loss: 0.8023836612701416 = 0.16674675047397614 + 0.1 * 6.3563690185546875
Epoch 370, val loss: 0.8210482001304626
Epoch 380, training loss: 0.7865726947784424 = 0.15224803984165192 + 0.1 * 6.3432464599609375
Epoch 380, val loss: 0.8307769298553467
Epoch 390, training loss: 0.7726560831069946 = 0.13907618820667267 + 0.1 * 6.335798740386963
Epoch 390, val loss: 0.841552734375
Epoch 400, training loss: 0.7597475051879883 = 0.12706241011619568 + 0.1 * 6.326850414276123
Epoch 400, val loss: 0.8532039523124695
Epoch 410, training loss: 0.7480448484420776 = 0.11603213101625443 + 0.1 * 6.320127010345459
Epoch 410, val loss: 0.8652998805046082
Epoch 420, training loss: 0.7372625470161438 = 0.1059393435716629 + 0.1 * 6.313231945037842
Epoch 420, val loss: 0.8778868913650513
Epoch 430, training loss: 0.728975236415863 = 0.09661505371332169 + 0.1 * 6.323601245880127
Epoch 430, val loss: 0.89080810546875
Epoch 440, training loss: 0.7185733914375305 = 0.08812499046325684 + 0.1 * 6.304483890533447
Epoch 440, val loss: 0.9038102030754089
Epoch 450, training loss: 0.7096135020256042 = 0.08028693497180939 + 0.1 * 6.2932658195495605
Epoch 450, val loss: 0.9170212149620056
Epoch 460, training loss: 0.7037349343299866 = 0.07322169840335846 + 0.1 * 6.3051323890686035
Epoch 460, val loss: 0.9301499724388123
Epoch 470, training loss: 0.6954724192619324 = 0.06697580963373184 + 0.1 * 6.284965991973877
Epoch 470, val loss: 0.9436435103416443
Epoch 480, training loss: 0.6899441480636597 = 0.061485499143600464 + 0.1 * 6.284585952758789
Epoch 480, val loss: 0.9573849439620972
Epoch 490, training loss: 0.6848971843719482 = 0.05664615333080292 + 0.1 * 6.282510280609131
Epoch 490, val loss: 0.971031129360199
Epoch 500, training loss: 0.678980827331543 = 0.05234844237565994 + 0.1 * 6.266323566436768
Epoch 500, val loss: 0.9846625328063965
Epoch 510, training loss: 0.6773595213890076 = 0.048504821956157684 + 0.1 * 6.288546562194824
Epoch 510, val loss: 0.9980328679084778
Epoch 520, training loss: 0.6710498929023743 = 0.045074593275785446 + 0.1 * 6.259753227233887
Epoch 520, val loss: 1.0109878778457642
Epoch 530, training loss: 0.6675979495048523 = 0.04199143126606941 + 0.1 * 6.256065368652344
Epoch 530, val loss: 1.023790717124939
Epoch 540, training loss: 0.6653504967689514 = 0.03920184448361397 + 0.1 * 6.261486053466797
Epoch 540, val loss: 1.0363030433654785
Epoch 550, training loss: 0.661318838596344 = 0.036678120493888855 + 0.1 * 6.2464070320129395
Epoch 550, val loss: 1.0485670566558838
Epoch 560, training loss: 0.6597504019737244 = 0.034381527453660965 + 0.1 * 6.253688812255859
Epoch 560, val loss: 1.0605660676956177
Epoch 570, training loss: 0.6572253704071045 = 0.03229503333568573 + 0.1 * 6.249303340911865
Epoch 570, val loss: 1.072199821472168
Epoch 580, training loss: 0.6539940237998962 = 0.03039384074509144 + 0.1 * 6.236001968383789
Epoch 580, val loss: 1.0836124420166016
Epoch 590, training loss: 0.6527074575424194 = 0.02865189127624035 + 0.1 * 6.240555763244629
Epoch 590, val loss: 1.0947434902191162
Epoch 600, training loss: 0.6505051851272583 = 0.02705496922135353 + 0.1 * 6.23450231552124
Epoch 600, val loss: 1.1055701971054077
Epoch 610, training loss: 0.6496970057487488 = 0.025591352954506874 + 0.1 * 6.241055965423584
Epoch 610, val loss: 1.1160080432891846
Epoch 620, training loss: 0.6463466882705688 = 0.024248836562037468 + 0.1 * 6.220978260040283
Epoch 620, val loss: 1.1263623237609863
Epoch 630, training loss: 0.644576370716095 = 0.023006904870271683 + 0.1 * 6.215694427490234
Epoch 630, val loss: 1.1364600658416748
Epoch 640, training loss: 0.6454899907112122 = 0.021856751292943954 + 0.1 * 6.236332416534424
Epoch 640, val loss: 1.1463335752487183
Epoch 650, training loss: 0.6430952548980713 = 0.020792672410607338 + 0.1 * 6.223025798797607
Epoch 650, val loss: 1.155761480331421
Epoch 660, training loss: 0.6407836675643921 = 0.01980961114168167 + 0.1 * 6.20974063873291
Epoch 660, val loss: 1.1652015447616577
Epoch 670, training loss: 0.6399877071380615 = 0.018894294276833534 + 0.1 * 6.210933685302734
Epoch 670, val loss: 1.1743088960647583
Epoch 680, training loss: 0.6402965188026428 = 0.01804317533969879 + 0.1 * 6.222533702850342
Epoch 680, val loss: 1.183245062828064
Epoch 690, training loss: 0.6379807591438293 = 0.017249541357159615 + 0.1 * 6.207312107086182
Epoch 690, val loss: 1.1918667554855347
Epoch 700, training loss: 0.6363052725791931 = 0.016509851440787315 + 0.1 * 6.197954177856445
Epoch 700, val loss: 1.2004623413085938
Epoch 710, training loss: 0.6365630030632019 = 0.015815945342183113 + 0.1 * 6.207469940185547
Epoch 710, val loss: 1.208771824836731
Epoch 720, training loss: 0.6345824003219604 = 0.015166809782385826 + 0.1 * 6.194155693054199
Epoch 720, val loss: 1.2168134450912476
Epoch 730, training loss: 0.6354273557662964 = 0.014559618197381496 + 0.1 * 6.208677291870117
Epoch 730, val loss: 1.2247529029846191
Epoch 740, training loss: 0.6331502199172974 = 0.01398886926472187 + 0.1 * 6.191613674163818
Epoch 740, val loss: 1.2325292825698853
Epoch 750, training loss: 0.6327836513519287 = 0.013453105464577675 + 0.1 * 6.193305015563965
Epoch 750, val loss: 1.2402230501174927
Epoch 760, training loss: 0.6317010521888733 = 0.012947996146976948 + 0.1 * 6.187530517578125
Epoch 760, val loss: 1.247588038444519
Epoch 770, training loss: 0.6317111849784851 = 0.012471490539610386 + 0.1 * 6.192396640777588
Epoch 770, val loss: 1.2549303770065308
Epoch 780, training loss: 0.6312535405158997 = 0.012022600509226322 + 0.1 * 6.192309379577637
Epoch 780, val loss: 1.262054681777954
Epoch 790, training loss: 0.6309306025505066 = 0.011598341166973114 + 0.1 * 6.19332218170166
Epoch 790, val loss: 1.2690613269805908
Epoch 800, training loss: 0.6294823288917542 = 0.011199167929589748 + 0.1 * 6.182831287384033
Epoch 800, val loss: 1.2758114337921143
Epoch 810, training loss: 0.6281739473342896 = 0.01082104817032814 + 0.1 * 6.173528671264648
Epoch 810, val loss: 1.2825840711593628
Epoch 820, training loss: 0.6282614469528198 = 0.010463457554578781 + 0.1 * 6.177979469299316
Epoch 820, val loss: 1.2891356945037842
Epoch 830, training loss: 0.6281050443649292 = 0.010123789310455322 + 0.1 * 6.179812431335449
Epoch 830, val loss: 1.2954227924346924
Epoch 840, training loss: 0.6277257800102234 = 0.009801944717764854 + 0.1 * 6.179238319396973
Epoch 840, val loss: 1.3017168045043945
Epoch 850, training loss: 0.626240611076355 = 0.009496377781033516 + 0.1 * 6.167442321777344
Epoch 850, val loss: 1.3079968690872192
Epoch 860, training loss: 0.626507043838501 = 0.00920528918504715 + 0.1 * 6.173017501831055
Epoch 860, val loss: 1.3140515089035034
Epoch 870, training loss: 0.6257573366165161 = 0.008928269147872925 + 0.1 * 6.168290138244629
Epoch 870, val loss: 1.3198140859603882
Epoch 880, training loss: 0.6259474158287048 = 0.008665335364639759 + 0.1 * 6.172820568084717
Epoch 880, val loss: 1.3256181478500366
Epoch 890, training loss: 0.624514639377594 = 0.008415170945227146 + 0.1 * 6.160994529724121
Epoch 890, val loss: 1.3314261436462402
Epoch 900, training loss: 0.6246783137321472 = 0.008176341652870178 + 0.1 * 6.165019512176514
Epoch 900, val loss: 1.3370643854141235
Epoch 910, training loss: 0.6251381635665894 = 0.007948352955281734 + 0.1 * 6.171898365020752
Epoch 910, val loss: 1.3424009084701538
Epoch 920, training loss: 0.6235408782958984 = 0.007730418350547552 + 0.1 * 6.158104419708252
Epoch 920, val loss: 1.3478575944900513
Epoch 930, training loss: 0.62516188621521 = 0.0075228712521493435 + 0.1 * 6.176389694213867
Epoch 930, val loss: 1.3532085418701172
Epoch 940, training loss: 0.6231145858764648 = 0.007323785685002804 + 0.1 * 6.157907962799072
Epoch 940, val loss: 1.3583232164382935
Epoch 950, training loss: 0.6226853132247925 = 0.007134395185858011 + 0.1 * 6.155508995056152
Epoch 950, val loss: 1.3635936975479126
Epoch 960, training loss: 0.6225004196166992 = 0.006952442694455385 + 0.1 * 6.155479431152344
Epoch 960, val loss: 1.3686692714691162
Epoch 970, training loss: 0.6220849752426147 = 0.006778227165341377 + 0.1 * 6.153067111968994
Epoch 970, val loss: 1.3734711408615112
Epoch 980, training loss: 0.6215824484825134 = 0.0066113583743572235 + 0.1 * 6.149710655212402
Epoch 980, val loss: 1.3783624172210693
Epoch 990, training loss: 0.6221638321876526 = 0.006451603025197983 + 0.1 * 6.1571221351623535
Epoch 990, val loss: 1.3831899166107178
Epoch 1000, training loss: 0.6226111650466919 = 0.006297523155808449 + 0.1 * 6.163136005401611
Epoch 1000, val loss: 1.3878304958343506
Epoch 1010, training loss: 0.6211829781532288 = 0.006149656604975462 + 0.1 * 6.150332927703857
Epoch 1010, val loss: 1.3924250602722168
Epoch 1020, training loss: 0.620366096496582 = 0.0060078916139900684 + 0.1 * 6.143581867218018
Epoch 1020, val loss: 1.3970141410827637
Epoch 1030, training loss: 0.619676947593689 = 0.005871420260518789 + 0.1 * 6.138055324554443
Epoch 1030, val loss: 1.4015858173370361
Epoch 1040, training loss: 0.6221823692321777 = 0.005739560350775719 + 0.1 * 6.164427757263184
Epoch 1040, val loss: 1.4059969186782837
Epoch 1050, training loss: 0.6215940713882446 = 0.005612460896372795 + 0.1 * 6.159816265106201
Epoch 1050, val loss: 1.4101439714431763
Epoch 1060, training loss: 0.6202547550201416 = 0.005490672308951616 + 0.1 * 6.147640705108643
Epoch 1060, val loss: 1.414343237876892
Epoch 1070, training loss: 0.6185579299926758 = 0.005374029278755188 + 0.1 * 6.131838798522949
Epoch 1070, val loss: 1.418639063835144
Epoch 1080, training loss: 0.6196510195732117 = 0.005261620972305536 + 0.1 * 6.143894195556641
Epoch 1080, val loss: 1.4228743314743042
Epoch 1090, training loss: 0.6189702153205872 = 0.0051522450521588326 + 0.1 * 6.138179779052734
Epoch 1090, val loss: 1.4267196655273438
Epoch 1100, training loss: 0.6183677911758423 = 0.005047041457146406 + 0.1 * 6.133207321166992
Epoch 1100, val loss: 1.4307411909103394
Epoch 1110, training loss: 0.6183137893676758 = 0.004945479333400726 + 0.1 * 6.133683204650879
Epoch 1110, val loss: 1.4346216917037964
Epoch 1120, training loss: 0.6183540225028992 = 0.004847681615501642 + 0.1 * 6.135063171386719
Epoch 1120, val loss: 1.4385355710983276
Epoch 1130, training loss: 0.618100106716156 = 0.004752674605697393 + 0.1 * 6.133473873138428
Epoch 1130, val loss: 1.4422544240951538
Epoch 1140, training loss: 0.6179022789001465 = 0.004661329556256533 + 0.1 * 6.132409572601318
Epoch 1140, val loss: 1.4460564851760864
Epoch 1150, training loss: 0.6178627610206604 = 0.004572459962219 + 0.1 * 6.132903099060059
Epoch 1150, val loss: 1.4498155117034912
Epoch 1160, training loss: 0.6182321906089783 = 0.004486153367906809 + 0.1 * 6.137460231781006
Epoch 1160, val loss: 1.4533239603042603
Epoch 1170, training loss: 0.6170264482498169 = 0.004402797669172287 + 0.1 * 6.1262359619140625
Epoch 1170, val loss: 1.4569250345230103
Epoch 1180, training loss: 0.6175550222396851 = 0.004322693683207035 + 0.1 * 6.132323265075684
Epoch 1180, val loss: 1.4605661630630493
Epoch 1190, training loss: 0.6165841817855835 = 0.004244478885084391 + 0.1 * 6.123396873474121
Epoch 1190, val loss: 1.4639978408813477
Epoch 1200, training loss: 0.617293119430542 = 0.004168963525444269 + 0.1 * 6.131241321563721
Epoch 1200, val loss: 1.4674561023712158
Epoch 1210, training loss: 0.6163769960403442 = 0.004095743875950575 + 0.1 * 6.122812271118164
Epoch 1210, val loss: 1.4707478284835815
Epoch 1220, training loss: 0.6161634922027588 = 0.004024943802505732 + 0.1 * 6.12138557434082
Epoch 1220, val loss: 1.4740957021713257
Epoch 1230, training loss: 0.6164049506187439 = 0.0039565288461744785 + 0.1 * 6.124484062194824
Epoch 1230, val loss: 1.4774911403656006
Epoch 1240, training loss: 0.6158764362335205 = 0.0038894598837941885 + 0.1 * 6.119869709014893
Epoch 1240, val loss: 1.4806077480316162
Epoch 1250, training loss: 0.6165390014648438 = 0.003824504790827632 + 0.1 * 6.127144813537598
Epoch 1250, val loss: 1.4837311506271362
Epoch 1260, training loss: 0.6154733896255493 = 0.0037616132758557796 + 0.1 * 6.117117881774902
Epoch 1260, val loss: 1.486804723739624
Epoch 1270, training loss: 0.6153597235679626 = 0.0037007660139352083 + 0.1 * 6.116589069366455
Epoch 1270, val loss: 1.4900025129318237
Epoch 1280, training loss: 0.6163299679756165 = 0.0036414291244000196 + 0.1 * 6.126885414123535
Epoch 1280, val loss: 1.4930731058120728
Epoch 1290, training loss: 0.6151201128959656 = 0.0035836556926369667 + 0.1 * 6.115364074707031
Epoch 1290, val loss: 1.4960873126983643
Epoch 1300, training loss: 0.6159349679946899 = 0.0035271081142127514 + 0.1 * 6.124078750610352
Epoch 1300, val loss: 1.498910665512085
Epoch 1310, training loss: 0.6150364875793457 = 0.003472656011581421 + 0.1 * 6.11563777923584
Epoch 1310, val loss: 1.5018548965454102
Epoch 1320, training loss: 0.6143724918365479 = 0.003420042572543025 + 0.1 * 6.109524250030518
Epoch 1320, val loss: 1.5048960447311401
Epoch 1330, training loss: 0.6154198050498962 = 0.0033685537055134773 + 0.1 * 6.12051248550415
Epoch 1330, val loss: 1.5077979564666748
Epoch 1340, training loss: 0.615227222442627 = 0.0033175977878272533 + 0.1 * 6.119096279144287
Epoch 1340, val loss: 1.510372519493103
Epoch 1350, training loss: 0.6148646473884583 = 0.003268674947321415 + 0.1 * 6.115959644317627
Epoch 1350, val loss: 1.5130248069763184
Epoch 1360, training loss: 0.6136249899864197 = 0.0032212906517088413 + 0.1 * 6.104036808013916
Epoch 1360, val loss: 1.515863060951233
Epoch 1370, training loss: 0.6157878637313843 = 0.0031752584036439657 + 0.1 * 6.126125812530518
Epoch 1370, val loss: 1.5185916423797607
Epoch 1380, training loss: 0.6157576441764832 = 0.0031295649241656065 + 0.1 * 6.126280784606934
Epoch 1380, val loss: 1.521073579788208
Epoch 1390, training loss: 0.6139912605285645 = 0.003085461212322116 + 0.1 * 6.109057903289795
Epoch 1390, val loss: 1.5236153602600098
Epoch 1400, training loss: 0.6132349967956543 = 0.003043052740395069 + 0.1 * 6.101919174194336
Epoch 1400, val loss: 1.5263986587524414
Epoch 1410, training loss: 0.614120602607727 = 0.003001331351697445 + 0.1 * 6.11119270324707
Epoch 1410, val loss: 1.5289720296859741
Epoch 1420, training loss: 0.6149742007255554 = 0.002960311248898506 + 0.1 * 6.120138645172119
Epoch 1420, val loss: 1.531362533569336
Epoch 1430, training loss: 0.613120973110199 = 0.0029199598357081413 + 0.1 * 6.1020097732543945
Epoch 1430, val loss: 1.533692479133606
Epoch 1440, training loss: 0.613403856754303 = 0.002881455235183239 + 0.1 * 6.105223655700684
Epoch 1440, val loss: 1.5362995862960815
Epoch 1450, training loss: 0.6140933632850647 = 0.0028436598367989063 + 0.1 * 6.112496852874756
Epoch 1450, val loss: 1.5388035774230957
Epoch 1460, training loss: 0.6130983829498291 = 0.002806606702506542 + 0.1 * 6.102917671203613
Epoch 1460, val loss: 1.5411392450332642
Epoch 1470, training loss: 0.612671434879303 = 0.002770450431853533 + 0.1 * 6.099009990692139
Epoch 1470, val loss: 1.5435519218444824
Epoch 1480, training loss: 0.613399088382721 = 0.0027352492325007915 + 0.1 * 6.106638431549072
Epoch 1480, val loss: 1.5459624528884888
Epoch 1490, training loss: 0.6129159927368164 = 0.0027004671283066273 + 0.1 * 6.1021552085876465
Epoch 1490, val loss: 1.5482070446014404
Epoch 1500, training loss: 0.61272794008255 = 0.0026666205376386642 + 0.1 * 6.100613117218018
Epoch 1500, val loss: 1.5504652261734009
Epoch 1510, training loss: 0.6124465465545654 = 0.0026336561422795057 + 0.1 * 6.098128795623779
Epoch 1510, val loss: 1.5527170896530151
Epoch 1520, training loss: 0.6125273704528809 = 0.002601646352559328 + 0.1 * 6.099257469177246
Epoch 1520, val loss: 1.5550111532211304
Epoch 1530, training loss: 0.6120159029960632 = 0.0025700924452394247 + 0.1 * 6.094458103179932
Epoch 1530, val loss: 1.557266354560852
Epoch 1540, training loss: 0.6124524474143982 = 0.0025394142139703035 + 0.1 * 6.099130153656006
Epoch 1540, val loss: 1.5594803094863892
Epoch 1550, training loss: 0.6124733686447144 = 0.0025090291164815426 + 0.1 * 6.099643230438232
Epoch 1550, val loss: 1.5614712238311768
Epoch 1560, training loss: 0.6116262078285217 = 0.0024793727789074183 + 0.1 * 6.091468334197998
Epoch 1560, val loss: 1.563597559928894
Epoch 1570, training loss: 0.612227737903595 = 0.0024508493952453136 + 0.1 * 6.097768783569336
Epoch 1570, val loss: 1.5657917261123657
Epoch 1580, training loss: 0.6119954586029053 = 0.0024225017987191677 + 0.1 * 6.095729351043701
Epoch 1580, val loss: 1.5677810907363892
Epoch 1590, training loss: 0.6110715270042419 = 0.002395007526502013 + 0.1 * 6.086764812469482
Epoch 1590, val loss: 1.5698652267456055
Epoch 1600, training loss: 0.6143854260444641 = 0.0023682094179093838 + 0.1 * 6.120172023773193
Epoch 1600, val loss: 1.57193922996521
Epoch 1610, training loss: 0.6120814681053162 = 0.0023413803428411484 + 0.1 * 6.097400665283203
Epoch 1610, val loss: 1.573764443397522
Epoch 1620, training loss: 0.6115909218788147 = 0.0023155389353632927 + 0.1 * 6.092753887176514
Epoch 1620, val loss: 1.5758048295974731
Epoch 1630, training loss: 0.6123371720314026 = 0.0022902670316398144 + 0.1 * 6.10046911239624
Epoch 1630, val loss: 1.5777922868728638
Epoch 1640, training loss: 0.6112435460090637 = 0.0022652021143585443 + 0.1 * 6.089783191680908
Epoch 1640, val loss: 1.5796787738800049
Epoch 1650, training loss: 0.6124185919761658 = 0.002240838948637247 + 0.1 * 6.10177755355835
Epoch 1650, val loss: 1.5815571546554565
Epoch 1660, training loss: 0.6104419231414795 = 0.0022167812567204237 + 0.1 * 6.08225154876709
Epoch 1660, val loss: 1.5834484100341797
Epoch 1670, training loss: 0.6113139390945435 = 0.002193494699895382 + 0.1 * 6.0912041664123535
Epoch 1670, val loss: 1.5854477882385254
Epoch 1680, training loss: 0.6119134426116943 = 0.0021704479586333036 + 0.1 * 6.0974297523498535
Epoch 1680, val loss: 1.5872822999954224
Epoch 1690, training loss: 0.6103993654251099 = 0.002147612627595663 + 0.1 * 6.082517147064209
Epoch 1690, val loss: 1.5889602899551392
Epoch 1700, training loss: 0.6111476421356201 = 0.0021257365588098764 + 0.1 * 6.090219020843506
Epoch 1700, val loss: 1.590867519378662
Epoch 1710, training loss: 0.6111026406288147 = 0.002104172483086586 + 0.1 * 6.08998441696167
Epoch 1710, val loss: 1.5926871299743652
Epoch 1720, training loss: 0.6105981469154358 = 0.0020827255211770535 + 0.1 * 6.085154056549072
Epoch 1720, val loss: 1.5943487882614136
Epoch 1730, training loss: 0.6104281544685364 = 0.0020620087161660194 + 0.1 * 6.083661079406738
Epoch 1730, val loss: 1.5961203575134277
Epoch 1740, training loss: 0.6106566786766052 = 0.002041642786934972 + 0.1 * 6.086150169372559
Epoch 1740, val loss: 1.5977849960327148
Epoch 1750, training loss: 0.6102742552757263 = 0.0020215455442667007 + 0.1 * 6.082527160644531
Epoch 1750, val loss: 1.599492073059082
Epoch 1760, training loss: 0.6106187701225281 = 0.002002113964408636 + 0.1 * 6.0861663818359375
Epoch 1760, val loss: 1.6012245416641235
Epoch 1770, training loss: 0.6103487014770508 = 0.0019826367497444153 + 0.1 * 6.08366060256958
Epoch 1770, val loss: 1.602738380432129
Epoch 1780, training loss: 0.6098495125770569 = 0.0019636170472949743 + 0.1 * 6.078858852386475
Epoch 1780, val loss: 1.6043256521224976
Epoch 1790, training loss: 0.6110464930534363 = 0.0019451159751042724 + 0.1 * 6.091013431549072
Epoch 1790, val loss: 1.6059668064117432
Epoch 1800, training loss: 0.6110055446624756 = 0.0019266522722318769 + 0.1 * 6.090788841247559
Epoch 1800, val loss: 1.6074633598327637
Epoch 1810, training loss: 0.6093955039978027 = 0.0019085719250142574 + 0.1 * 6.074869155883789
Epoch 1810, val loss: 1.6089744567871094
Epoch 1820, training loss: 0.6097311973571777 = 0.0018909903010353446 + 0.1 * 6.078401565551758
Epoch 1820, val loss: 1.6105796098709106
Epoch 1830, training loss: 0.6098388433456421 = 0.0018736104248091578 + 0.1 * 6.079652309417725
Epoch 1830, val loss: 1.6121203899383545
Epoch 1840, training loss: 0.6104745268821716 = 0.0018565774662420154 + 0.1 * 6.086179733276367
Epoch 1840, val loss: 1.6136587858200073
Epoch 1850, training loss: 0.6103147268295288 = 0.0018395676743239164 + 0.1 * 6.084751129150391
Epoch 1850, val loss: 1.6150593757629395
Epoch 1860, training loss: 0.609365701675415 = 0.0018230435671284795 + 0.1 * 6.07542610168457
Epoch 1860, val loss: 1.6165893077850342
Epoch 1870, training loss: 0.609757125377655 = 0.0018068939680233598 + 0.1 * 6.079502105712891
Epoch 1870, val loss: 1.6180989742279053
Epoch 1880, training loss: 0.6107621788978577 = 0.001790814334526658 + 0.1 * 6.0897135734558105
Epoch 1880, val loss: 1.6194401979446411
Epoch 1890, training loss: 0.6089075803756714 = 0.0017749398248270154 + 0.1 * 6.07132625579834
Epoch 1890, val loss: 1.6208007335662842
Epoch 1900, training loss: 0.6097366213798523 = 0.001759820501320064 + 0.1 * 6.07976770401001
Epoch 1900, val loss: 1.622363805770874
Epoch 1910, training loss: 0.6094607710838318 = 0.0017446466954424977 + 0.1 * 6.0771613121032715
Epoch 1910, val loss: 1.6237722635269165
Epoch 1920, training loss: 0.609341025352478 = 0.0017297276062890887 + 0.1 * 6.076112747192383
Epoch 1920, val loss: 1.6251691579818726
Epoch 1930, training loss: 0.6089723110198975 = 0.0017150006024166942 + 0.1 * 6.072572708129883
Epoch 1930, val loss: 1.6265820264816284
Epoch 1940, training loss: 0.609882116317749 = 0.0017004896653816104 + 0.1 * 6.081816673278809
Epoch 1940, val loss: 1.627967357635498
Epoch 1950, training loss: 0.6092966198921204 = 0.0016862127231433988 + 0.1 * 6.076104164123535
Epoch 1950, val loss: 1.6292877197265625
Epoch 1960, training loss: 0.608765184879303 = 0.0016721786232665181 + 0.1 * 6.070929527282715
Epoch 1960, val loss: 1.6305891275405884
Epoch 1970, training loss: 0.6087300777435303 = 0.0016584102995693684 + 0.1 * 6.070716381072998
Epoch 1970, val loss: 1.6319345235824585
Epoch 1980, training loss: 0.6099851727485657 = 0.0016448205569759011 + 0.1 * 6.08340311050415
Epoch 1980, val loss: 1.6331475973129272
Epoch 1990, training loss: 0.6088610887527466 = 0.0016314283711835742 + 0.1 * 6.072296142578125
Epoch 1990, val loss: 1.634441614151001
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7306
Flip ASR: 0.6844/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7736356258392334 = 1.9362519979476929 + 0.1 * 8.373835563659668
Epoch 0, val loss: 1.9299960136413574
Epoch 10, training loss: 2.764057159423828 = 1.9266952276229858 + 0.1 * 8.373618125915527
Epoch 10, val loss: 1.9206293821334839
Epoch 20, training loss: 2.7515368461608887 = 1.9142860174179077 + 0.1 * 8.37250804901123
Epoch 20, val loss: 1.9082921743392944
Epoch 30, training loss: 2.7327404022216797 = 1.8962726593017578 + 0.1 * 8.364677429199219
Epoch 30, val loss: 1.890250563621521
Epoch 40, training loss: 2.701401472091675 = 1.8693788051605225 + 0.1 * 8.320225715637207
Epoch 40, val loss: 1.863612413406372
Epoch 50, training loss: 2.640678644180298 = 1.8338534832000732 + 0.1 * 8.06825065612793
Epoch 50, val loss: 1.8304485082626343
Epoch 60, training loss: 2.5609030723571777 = 1.7964324951171875 + 0.1 * 7.644705772399902
Epoch 60, val loss: 1.7983500957489014
Epoch 70, training loss: 2.4880259037017822 = 1.7622580528259277 + 0.1 * 7.257678985595703
Epoch 70, val loss: 1.7705626487731934
Epoch 80, training loss: 2.4231491088867188 = 1.7246038913726807 + 0.1 * 6.9854512214660645
Epoch 80, val loss: 1.7392394542694092
Epoch 90, training loss: 2.3590445518493652 = 1.6738574504852295 + 0.1 * 6.851870059967041
Epoch 90, val loss: 1.6956793069839478
Epoch 100, training loss: 2.2848658561706543 = 1.6061502695083618 + 0.1 * 6.787156105041504
Epoch 100, val loss: 1.6381440162658691
Epoch 110, training loss: 2.195435047149658 = 1.5223575830459595 + 0.1 * 6.730774402618408
Epoch 110, val loss: 1.568546175956726
Epoch 120, training loss: 2.096860885620117 = 1.4283932447433472 + 0.1 * 6.6846771240234375
Epoch 120, val loss: 1.4924447536468506
Epoch 130, training loss: 1.9941775798797607 = 1.3291041851043701 + 0.1 * 6.650733470916748
Epoch 130, val loss: 1.414243459701538
Epoch 140, training loss: 1.8893084526062012 = 1.2272132635116577 + 0.1 * 6.620952606201172
Epoch 140, val loss: 1.3353807926177979
Epoch 150, training loss: 1.7859691381454468 = 1.1258165836334229 + 0.1 * 6.60152530670166
Epoch 150, val loss: 1.2584302425384521
Epoch 160, training loss: 1.688767910003662 = 1.0313712358474731 + 0.1 * 6.5739665031433105
Epoch 160, val loss: 1.187986135482788
Epoch 170, training loss: 1.5997259616851807 = 0.9442508816719055 + 0.1 * 6.554751396179199
Epoch 170, val loss: 1.1237965822219849
Epoch 180, training loss: 1.5189478397369385 = 0.8639960289001465 + 0.1 * 6.549518585205078
Epoch 180, val loss: 1.0653411149978638
Epoch 190, training loss: 1.4442250728607178 = 0.7918803691864014 + 0.1 * 6.523446559906006
Epoch 190, val loss: 1.0135974884033203
Epoch 200, training loss: 1.3772931098937988 = 0.7264700531959534 + 0.1 * 6.508229732513428
Epoch 200, val loss: 0.9676452875137329
Epoch 210, training loss: 1.3173643350601196 = 0.6671375632286072 + 0.1 * 6.502267837524414
Epoch 210, val loss: 0.9277522563934326
Epoch 220, training loss: 1.262402057647705 = 0.6136160492897034 + 0.1 * 6.487860202789307
Epoch 220, val loss: 0.8941535353660583
Epoch 230, training loss: 1.2112891674041748 = 0.5638478994369507 + 0.1 * 6.474412441253662
Epoch 230, val loss: 0.8652946352958679
Epoch 240, training loss: 1.1626098155975342 = 0.5160126090049744 + 0.1 * 6.465972423553467
Epoch 240, val loss: 0.8396965861320496
Epoch 250, training loss: 1.1156816482543945 = 0.46990999579429626 + 0.1 * 6.457716464996338
Epoch 250, val loss: 0.8165589570999146
Epoch 260, training loss: 1.0710639953613281 = 0.4259513318538666 + 0.1 * 6.451126575469971
Epoch 260, val loss: 0.7958036065101624
Epoch 270, training loss: 1.0294023752212524 = 0.3847384750843048 + 0.1 * 6.446639060974121
Epoch 270, val loss: 0.7777900099754333
Epoch 280, training loss: 0.9899734258651733 = 0.34681275486946106 + 0.1 * 6.431606769561768
Epoch 280, val loss: 0.7631803154945374
Epoch 290, training loss: 0.9556617736816406 = 0.3124587833881378 + 0.1 * 6.432029724121094
Epoch 290, val loss: 0.7522339224815369
Epoch 300, training loss: 0.9225005507469177 = 0.2816673517227173 + 0.1 * 6.408331871032715
Epoch 300, val loss: 0.7450229525566101
Epoch 310, training loss: 0.894589900970459 = 0.2538814842700958 + 0.1 * 6.407083511352539
Epoch 310, val loss: 0.7410966753959656
Epoch 320, training loss: 0.8683515191078186 = 0.22879303991794586 + 0.1 * 6.395585060119629
Epoch 320, val loss: 0.7401301860809326
Epoch 330, training loss: 0.8443549275398254 = 0.2060638815164566 + 0.1 * 6.38291072845459
Epoch 330, val loss: 0.7416620850563049
Epoch 340, training loss: 0.8238074779510498 = 0.18543845415115356 + 0.1 * 6.383690357208252
Epoch 340, val loss: 0.7454639077186584
Epoch 350, training loss: 0.8042864799499512 = 0.16685988008975983 + 0.1 * 6.374266147613525
Epoch 350, val loss: 0.7511465549468994
Epoch 360, training loss: 0.7883247137069702 = 0.15015670657157898 + 0.1 * 6.381680488586426
Epoch 360, val loss: 0.7584213018417358
Epoch 370, training loss: 0.7705201506614685 = 0.1353192925453186 + 0.1 * 6.35200834274292
Epoch 370, val loss: 0.7671241164207458
Epoch 380, training loss: 0.7573479413986206 = 0.12205640971660614 + 0.1 * 6.352915287017822
Epoch 380, val loss: 0.7769534587860107
Epoch 390, training loss: 0.74482661485672 = 0.11029695719480515 + 0.1 * 6.345296382904053
Epoch 390, val loss: 0.7877403497695923
Epoch 400, training loss: 0.7324559688568115 = 0.0998409166932106 + 0.1 * 6.326149940490723
Epoch 400, val loss: 0.7993687391281128
Epoch 410, training loss: 0.7240368127822876 = 0.09054190665483475 + 0.1 * 6.334949016571045
Epoch 410, val loss: 0.8116034865379333
Epoch 420, training loss: 0.7138121128082275 = 0.08231069892644882 + 0.1 * 6.315014362335205
Epoch 420, val loss: 0.8242488503456116
Epoch 430, training loss: 0.7065867185592651 = 0.0749787762761116 + 0.1 * 6.316079616546631
Epoch 430, val loss: 0.8372817039489746
Epoch 440, training loss: 0.6995182037353516 = 0.06849245727062225 + 0.1 * 6.3102569580078125
Epoch 440, val loss: 0.8503693342208862
Epoch 450, training loss: 0.6926113367080688 = 0.0627565085887909 + 0.1 * 6.298548698425293
Epoch 450, val loss: 0.8634882569313049
Epoch 460, training loss: 0.6863993406295776 = 0.0576346218585968 + 0.1 * 6.287647247314453
Epoch 460, val loss: 0.8765529990196228
Epoch 470, training loss: 0.6838746070861816 = 0.053048647940158844 + 0.1 * 6.308259010314941
Epoch 470, val loss: 0.8895858526229858
Epoch 480, training loss: 0.6768807768821716 = 0.04896550625562668 + 0.1 * 6.2791523933410645
Epoch 480, val loss: 0.9023587107658386
Epoch 490, training loss: 0.6728944182395935 = 0.04531169310212135 + 0.1 * 6.275827407836914
Epoch 490, val loss: 0.9150426387786865
Epoch 500, training loss: 0.6698152422904968 = 0.04202541336417198 + 0.1 * 6.27789831161499
Epoch 500, val loss: 0.9273160696029663
Epoch 510, training loss: 0.6657463312149048 = 0.03907697647809982 + 0.1 * 6.266693115234375
Epoch 510, val loss: 0.9394212961196899
Epoch 520, training loss: 0.6636726260185242 = 0.036413803696632385 + 0.1 * 6.27258825302124
Epoch 520, val loss: 0.9511464238166809
Epoch 530, training loss: 0.6600644588470459 = 0.034011758863925934 + 0.1 * 6.260526657104492
Epoch 530, val loss: 0.962508499622345
Epoch 540, training loss: 0.656654417514801 = 0.03183424472808838 + 0.1 * 6.248201847076416
Epoch 540, val loss: 0.9736438989639282
Epoch 550, training loss: 0.6563156247138977 = 0.02985231578350067 + 0.1 * 6.264632701873779
Epoch 550, val loss: 0.9844130873680115
Epoch 560, training loss: 0.65355384349823 = 0.028056878596544266 + 0.1 * 6.254969596862793
Epoch 560, val loss: 0.9947145581245422
Epoch 570, training loss: 0.6506258249282837 = 0.026422234252095222 + 0.1 * 6.242035388946533
Epoch 570, val loss: 1.0049209594726562
Epoch 580, training loss: 0.6494122743606567 = 0.024922525510191917 + 0.1 * 6.244897365570068
Epoch 580, val loss: 1.014648675918579
Epoch 590, training loss: 0.6467235088348389 = 0.023550042882561684 + 0.1 * 6.231734752655029
Epoch 590, val loss: 1.0241841077804565
Epoch 600, training loss: 0.6465493440628052 = 0.02228705957531929 + 0.1 * 6.2426228523254395
Epoch 600, val loss: 1.033447265625
Epoch 610, training loss: 0.6445121169090271 = 0.021127227693796158 + 0.1 * 6.233848571777344
Epoch 610, val loss: 1.042354702949524
Epoch 620, training loss: 0.6431899070739746 = 0.020058730617165565 + 0.1 * 6.231311321258545
Epoch 620, val loss: 1.0512446165084839
Epoch 630, training loss: 0.6420108079910278 = 0.019072165712714195 + 0.1 * 6.229386329650879
Epoch 630, val loss: 1.0595645904541016
Epoch 640, training loss: 0.639606237411499 = 0.0181628055870533 + 0.1 * 6.214434623718262
Epoch 640, val loss: 1.0679844617843628
Epoch 650, training loss: 0.6390174627304077 = 0.01731625758111477 + 0.1 * 6.217011451721191
Epoch 650, val loss: 1.0760818719863892
Epoch 660, training loss: 0.6374677419662476 = 0.01652938313782215 + 0.1 * 6.209383487701416
Epoch 660, val loss: 1.083804965019226
Epoch 670, training loss: 0.6363672018051147 = 0.01579900272190571 + 0.1 * 6.205681800842285
Epoch 670, val loss: 1.0915721654891968
Epoch 680, training loss: 0.6358636617660522 = 0.015117200091481209 + 0.1 * 6.207464218139648
Epoch 680, val loss: 1.0991218090057373
Epoch 690, training loss: 0.6350501179695129 = 0.014480280689895153 + 0.1 * 6.205698013305664
Epoch 690, val loss: 1.1063321828842163
Epoch 700, training loss: 0.6332287192344666 = 0.013886739499866962 + 0.1 * 6.193419456481934
Epoch 700, val loss: 1.113549828529358
Epoch 710, training loss: 0.6347507834434509 = 0.013330446556210518 + 0.1 * 6.214202880859375
Epoch 710, val loss: 1.1205520629882812
Epoch 720, training loss: 0.6324321627616882 = 0.012809030711650848 + 0.1 * 6.196230888366699
Epoch 720, val loss: 1.1272149085998535
Epoch 730, training loss: 0.6309694051742554 = 0.01232034619897604 + 0.1 * 6.186490535736084
Epoch 730, val loss: 1.133988618850708
Epoch 740, training loss: 0.631877601146698 = 0.011859425343573093 + 0.1 * 6.200181484222412
Epoch 740, val loss: 1.1404757499694824
Epoch 750, training loss: 0.6293390393257141 = 0.011426041834056377 + 0.1 * 6.1791300773620605
Epoch 750, val loss: 1.1467863321304321
Epoch 760, training loss: 0.629065215587616 = 0.01101853046566248 + 0.1 * 6.180466651916504
Epoch 760, val loss: 1.1531351804733276
Epoch 770, training loss: 0.630802571773529 = 0.010632507503032684 + 0.1 * 6.201700687408447
Epoch 770, val loss: 1.1592150926589966
Epoch 780, training loss: 0.628561794757843 = 0.010268586687743664 + 0.1 * 6.182932376861572
Epoch 780, val loss: 1.1650551557540894
Epoch 790, training loss: 0.6272566318511963 = 0.009924741461873055 + 0.1 * 6.173318386077881
Epoch 790, val loss: 1.1709870100021362
Epoch 800, training loss: 0.6268012523651123 = 0.009598451666533947 + 0.1 * 6.172028064727783
Epoch 800, val loss: 1.176594614982605
Epoch 810, training loss: 0.6262848973274231 = 0.009290122427046299 + 0.1 * 6.169947624206543
Epoch 810, val loss: 1.1822302341461182
Epoch 820, training loss: 0.6268669366836548 = 0.008996624499559402 + 0.1 * 6.178703308105469
Epoch 820, val loss: 1.187670350074768
Epoch 830, training loss: 0.6251370310783386 = 0.008718740195035934 + 0.1 * 6.164183139801025
Epoch 830, val loss: 1.193007230758667
Epoch 840, training loss: 0.6258395910263062 = 0.00845499336719513 + 0.1 * 6.1738457679748535
Epoch 840, val loss: 1.1983540058135986
Epoch 850, training loss: 0.6241956353187561 = 0.00820265430957079 + 0.1 * 6.159929275512695
Epoch 850, val loss: 1.2033973932266235
Epoch 860, training loss: 0.6248034238815308 = 0.007963849231600761 + 0.1 * 6.16839599609375
Epoch 860, val loss: 1.2085058689117432
Epoch 870, training loss: 0.6246939301490784 = 0.007735624443739653 + 0.1 * 6.169582843780518
Epoch 870, val loss: 1.2133756875991821
Epoch 880, training loss: 0.6232383847236633 = 0.0075183045119047165 + 0.1 * 6.157200813293457
Epoch 880, val loss: 1.2181307077407837
Epoch 890, training loss: 0.6232815384864807 = 0.0073119341395795345 + 0.1 * 6.159696102142334
Epoch 890, val loss: 1.2230665683746338
Epoch 900, training loss: 0.6224491000175476 = 0.007114123553037643 + 0.1 * 6.153349876403809
Epoch 900, val loss: 1.2277758121490479
Epoch 910, training loss: 0.6227614283561707 = 0.00692483875900507 + 0.1 * 6.158365249633789
Epoch 910, val loss: 1.2323769330978394
Epoch 920, training loss: 0.6218512058258057 = 0.006743273232132196 + 0.1 * 6.151079177856445
Epoch 920, val loss: 1.2368143796920776
Epoch 930, training loss: 0.6221747994422913 = 0.006570232100784779 + 0.1 * 6.156045913696289
Epoch 930, val loss: 1.2412368059158325
Epoch 940, training loss: 0.6218118667602539 = 0.006404480431228876 + 0.1 * 6.154073715209961
Epoch 940, val loss: 1.2456269264221191
Epoch 950, training loss: 0.6216830015182495 = 0.006245882250368595 + 0.1 * 6.1543707847595215
Epoch 950, val loss: 1.2499653100967407
Epoch 960, training loss: 0.6220239400863647 = 0.00609337305650115 + 0.1 * 6.159305572509766
Epoch 960, val loss: 1.2542054653167725
Epoch 970, training loss: 0.620042622089386 = 0.005947145167738199 + 0.1 * 6.140954494476318
Epoch 970, val loss: 1.2583227157592773
Epoch 980, training loss: 0.6209375262260437 = 0.005806830246001482 + 0.1 * 6.151307106018066
Epoch 980, val loss: 1.2624973058700562
Epoch 990, training loss: 0.6194787621498108 = 0.005671365652233362 + 0.1 * 6.138073444366455
Epoch 990, val loss: 1.2663079500198364
Epoch 1000, training loss: 0.6194602251052856 = 0.00554244639351964 + 0.1 * 6.1391777992248535
Epoch 1000, val loss: 1.2703481912612915
Epoch 1010, training loss: 0.6202077269554138 = 0.0054180859588086605 + 0.1 * 6.1478962898254395
Epoch 1010, val loss: 1.2743850946426392
Epoch 1020, training loss: 0.6200272440910339 = 0.005297772586345673 + 0.1 * 6.147294521331787
Epoch 1020, val loss: 1.2780903577804565
Epoch 1030, training loss: 0.6192529797554016 = 0.005182135850191116 + 0.1 * 6.1407084465026855
Epoch 1030, val loss: 1.2818241119384766
Epoch 1040, training loss: 0.6186066269874573 = 0.005071515683084726 + 0.1 * 6.135351181030273
Epoch 1040, val loss: 1.2856440544128418
Epoch 1050, training loss: 0.6189996600151062 = 0.004964379593729973 + 0.1 * 6.140353202819824
Epoch 1050, val loss: 1.2894480228424072
Epoch 1060, training loss: 0.6186666488647461 = 0.004860249813646078 + 0.1 * 6.138063907623291
Epoch 1060, val loss: 1.2929402589797974
Epoch 1070, training loss: 0.6180846095085144 = 0.004760367330163717 + 0.1 * 6.133242130279541
Epoch 1070, val loss: 1.2965161800384521
Epoch 1080, training loss: 0.6187039613723755 = 0.004664093255996704 + 0.1 * 6.1403985023498535
Epoch 1080, val loss: 1.3000383377075195
Epoch 1090, training loss: 0.6179417371749878 = 0.004571152850985527 + 0.1 * 6.1337056159973145
Epoch 1090, val loss: 1.3036125898361206
Epoch 1100, training loss: 0.6173791885375977 = 0.004481572192162275 + 0.1 * 6.128976345062256
Epoch 1100, val loss: 1.3071225881576538
Epoch 1110, training loss: 0.6174399852752686 = 0.0043946923688054085 + 0.1 * 6.130452632904053
Epoch 1110, val loss: 1.310494303703308
Epoch 1120, training loss: 0.6172904968261719 = 0.004310703370720148 + 0.1 * 6.129797458648682
Epoch 1120, val loss: 1.3139384984970093
Epoch 1130, training loss: 0.6187024712562561 = 0.0042293923906981945 + 0.1 * 6.144731044769287
Epoch 1130, val loss: 1.317204236984253
Epoch 1140, training loss: 0.6169825792312622 = 0.004150837659835815 + 0.1 * 6.128317832946777
Epoch 1140, val loss: 1.3203811645507812
Epoch 1150, training loss: 0.6158959865570068 = 0.0040750219486653805 + 0.1 * 6.118209362030029
Epoch 1150, val loss: 1.3237277269363403
Epoch 1160, training loss: 0.6179698705673218 = 0.004001538269221783 + 0.1 * 6.139682769775391
Epoch 1160, val loss: 1.3269497156143188
Epoch 1170, training loss: 0.6165423393249512 = 0.0039297230541706085 + 0.1 * 6.126125812530518
Epoch 1170, val loss: 1.3299384117126465
Epoch 1180, training loss: 0.6164592504501343 = 0.0038605662994086742 + 0.1 * 6.125986576080322
Epoch 1180, val loss: 1.332961082458496
Epoch 1190, training loss: 0.6161085367202759 = 0.0037938710302114487 + 0.1 * 6.1231465339660645
Epoch 1190, val loss: 1.3361048698425293
Epoch 1200, training loss: 0.6160793900489807 = 0.00372895784676075 + 0.1 * 6.123504161834717
Epoch 1200, val loss: 1.3391270637512207
Epoch 1210, training loss: 0.6152999997138977 = 0.003665620693936944 + 0.1 * 6.1163434982299805
Epoch 1210, val loss: 1.3420257568359375
Epoch 1220, training loss: 0.6165871024131775 = 0.0036043222062289715 + 0.1 * 6.129827976226807
Epoch 1220, val loss: 1.3449140787124634
Epoch 1230, training loss: 0.6152375340461731 = 0.003544917795807123 + 0.1 * 6.116926193237305
Epoch 1230, val loss: 1.3478615283966064
Epoch 1240, training loss: 0.6144310235977173 = 0.0034874274861067533 + 0.1 * 6.10943603515625
Epoch 1240, val loss: 1.350835919380188
Epoch 1250, training loss: 0.6166519522666931 = 0.0034313565120100975 + 0.1 * 6.132205486297607
Epoch 1250, val loss: 1.3536312580108643
Epoch 1260, training loss: 0.6151072978973389 = 0.00337632792070508 + 0.1 * 6.117310047149658
Epoch 1260, val loss: 1.3562734127044678
Epoch 1270, training loss: 0.6157577633857727 = 0.003323541022837162 + 0.1 * 6.12434196472168
Epoch 1270, val loss: 1.3589812517166138
Epoch 1280, training loss: 0.6147875189781189 = 0.0032720337621867657 + 0.1 * 6.11515474319458
Epoch 1280, val loss: 1.361775279045105
Epoch 1290, training loss: 0.6142464876174927 = 0.0032222559675574303 + 0.1 * 6.1102423667907715
Epoch 1290, val loss: 1.3645707368850708
Epoch 1300, training loss: 0.6145928502082825 = 0.003173350589349866 + 0.1 * 6.114194869995117
Epoch 1300, val loss: 1.3672244548797607
Epoch 1310, training loss: 0.6159810423851013 = 0.0031258303206413984 + 0.1 * 6.128551959991455
Epoch 1310, val loss: 1.3698259592056274
Epoch 1320, training loss: 0.6134847402572632 = 0.0030793631449341774 + 0.1 * 6.104053974151611
Epoch 1320, val loss: 1.3723100423812866
Epoch 1330, training loss: 0.6149616837501526 = 0.0030346300918608904 + 0.1 * 6.119270324707031
Epoch 1330, val loss: 1.374948501586914
Epoch 1340, training loss: 0.615304708480835 = 0.002990513341501355 + 0.1 * 6.123142242431641
Epoch 1340, val loss: 1.3774241209030151
Epoch 1350, training loss: 0.6138849258422852 = 0.002947825938463211 + 0.1 * 6.109371185302734
Epoch 1350, val loss: 1.3799152374267578
Epoch 1360, training loss: 0.6134754419326782 = 0.0029063851106911898 + 0.1 * 6.1056904792785645
Epoch 1360, val loss: 1.3824504613876343
Epoch 1370, training loss: 0.6137409806251526 = 0.0028658900409936905 + 0.1 * 6.108750343322754
Epoch 1370, val loss: 1.3849146366119385
Epoch 1380, training loss: 0.6131318807601929 = 0.0028262801934033632 + 0.1 * 6.103055953979492
Epoch 1380, val loss: 1.3872864246368408
Epoch 1390, training loss: 0.6135949492454529 = 0.002787747187539935 + 0.1 * 6.108071804046631
Epoch 1390, val loss: 1.3896821737289429
Epoch 1400, training loss: 0.6132830381393433 = 0.0027498449198901653 + 0.1 * 6.1053314208984375
Epoch 1400, val loss: 1.3920167684555054
Epoch 1410, training loss: 0.613351583480835 = 0.0027129591908305883 + 0.1 * 6.106386184692383
Epoch 1410, val loss: 1.39427649974823
Epoch 1420, training loss: 0.6120498180389404 = 0.002677302807569504 + 0.1 * 6.093724727630615
Epoch 1420, val loss: 1.3966388702392578
Epoch 1430, training loss: 0.6129549741744995 = 0.00264257681556046 + 0.1 * 6.103123664855957
Epoch 1430, val loss: 1.3990195989608765
Epoch 1440, training loss: 0.6126559376716614 = 0.0026082450058311224 + 0.1 * 6.1004767417907715
Epoch 1440, val loss: 1.4012099504470825
Epoch 1450, training loss: 0.6128373146057129 = 0.002574605867266655 + 0.1 * 6.102626800537109
Epoch 1450, val loss: 1.403366208076477
Epoch 1460, training loss: 0.6126100420951843 = 0.0025420018937438726 + 0.1 * 6.100679874420166
Epoch 1460, val loss: 1.4055792093276978
Epoch 1470, training loss: 0.6124196648597717 = 0.0025101983919739723 + 0.1 * 6.099094390869141
Epoch 1470, val loss: 1.4077389240264893
Epoch 1480, training loss: 0.6122370958328247 = 0.002479156479239464 + 0.1 * 6.097579479217529
Epoch 1480, val loss: 1.4099327325820923
Epoch 1490, training loss: 0.6118389964103699 = 0.0024487972259521484 + 0.1 * 6.093901634216309
Epoch 1490, val loss: 1.4121475219726562
Epoch 1500, training loss: 0.6122003197669983 = 0.0024190221447497606 + 0.1 * 6.097813129425049
Epoch 1500, val loss: 1.414283275604248
Epoch 1510, training loss: 0.612159788608551 = 0.002389924367889762 + 0.1 * 6.09769868850708
Epoch 1510, val loss: 1.4163562059402466
Epoch 1520, training loss: 0.6113886833190918 = 0.002361498773097992 + 0.1 * 6.090271949768066
Epoch 1520, val loss: 1.4184167385101318
Epoch 1530, training loss: 0.6123000383377075 = 0.002333858050405979 + 0.1 * 6.099661350250244
Epoch 1530, val loss: 1.4205553531646729
Epoch 1540, training loss: 0.6119295954704285 = 0.0023064168635755777 + 0.1 * 6.096231937408447
Epoch 1540, val loss: 1.4225752353668213
Epoch 1550, training loss: 0.611532986164093 = 0.0022798688150942326 + 0.1 * 6.092530727386475
Epoch 1550, val loss: 1.4246976375579834
Epoch 1560, training loss: 0.6117739081382751 = 0.002253778511658311 + 0.1 * 6.09520149230957
Epoch 1560, val loss: 1.4267327785491943
Epoch 1570, training loss: 0.6113560199737549 = 0.002228241181001067 + 0.1 * 6.091277599334717
Epoch 1570, val loss: 1.4286829233169556
Epoch 1580, training loss: 0.6111045479774475 = 0.0022033718414604664 + 0.1 * 6.089012145996094
Epoch 1580, val loss: 1.4307515621185303
Epoch 1590, training loss: 0.6120248436927795 = 0.0021789204329252243 + 0.1 * 6.098459243774414
Epoch 1590, val loss: 1.4327430725097656
Epoch 1600, training loss: 0.6107633113861084 = 0.0021548911463469267 + 0.1 * 6.086084365844727
Epoch 1600, val loss: 1.4346758127212524
Epoch 1610, training loss: 0.6106516122817993 = 0.002131393179297447 + 0.1 * 6.085202217102051
Epoch 1610, val loss: 1.4366390705108643
Epoch 1620, training loss: 0.6119069457054138 = 0.002108368556946516 + 0.1 * 6.09798526763916
Epoch 1620, val loss: 1.4385802745819092
Epoch 1630, training loss: 0.6116828918457031 = 0.002085647080093622 + 0.1 * 6.095972537994385
Epoch 1630, val loss: 1.4402934312820435
Epoch 1640, training loss: 0.6101500391960144 = 0.0020636541303247213 + 0.1 * 6.080863952636719
Epoch 1640, val loss: 1.442242980003357
Epoch 1650, training loss: 0.6116732954978943 = 0.0020424104295670986 + 0.1 * 6.096308708190918
Epoch 1650, val loss: 1.4442880153656006
Epoch 1660, training loss: 0.6103726625442505 = 0.002021035412326455 + 0.1 * 6.083516597747803
Epoch 1660, val loss: 1.4460536241531372
Epoch 1670, training loss: 0.6117040514945984 = 0.002000086475163698 + 0.1 * 6.097039699554443
Epoch 1670, val loss: 1.4478764533996582
Epoch 1680, training loss: 0.6103346943855286 = 0.001979591092094779 + 0.1 * 6.083550930023193
Epoch 1680, val loss: 1.4495826959609985
Epoch 1690, training loss: 0.6105369925498962 = 0.001959617715328932 + 0.1 * 6.085773468017578
Epoch 1690, val loss: 1.451446533203125
Epoch 1700, training loss: 0.6103644371032715 = 0.0019400519086048007 + 0.1 * 6.084243297576904
Epoch 1700, val loss: 1.4533456563949585
Epoch 1710, training loss: 0.6103648543357849 = 0.0019207100849598646 + 0.1 * 6.084441661834717
Epoch 1710, val loss: 1.4551215171813965
Epoch 1720, training loss: 0.6105159521102905 = 0.001901694922707975 + 0.1 * 6.086142539978027
Epoch 1720, val loss: 1.4568634033203125
Epoch 1730, training loss: 0.6096903085708618 = 0.001883012941107154 + 0.1 * 6.078072547912598
Epoch 1730, val loss: 1.4585473537445068
Epoch 1740, training loss: 0.6099827885627747 = 0.0018648378318175673 + 0.1 * 6.081179141998291
Epoch 1740, val loss: 1.460354208946228
Epoch 1750, training loss: 0.6094638705253601 = 0.0018468591151759028 + 0.1 * 6.076169967651367
Epoch 1750, val loss: 1.462126612663269
Epoch 1760, training loss: 0.6106843948364258 = 0.0018290806328877807 + 0.1 * 6.088552951812744
Epoch 1760, val loss: 1.4638185501098633
Epoch 1770, training loss: 0.6107847690582275 = 0.0018115048296749592 + 0.1 * 6.089732646942139
Epoch 1770, val loss: 1.4653818607330322
Epoch 1780, training loss: 0.6100598573684692 = 0.001794410403817892 + 0.1 * 6.0826544761657715
Epoch 1780, val loss: 1.4670604467391968
Epoch 1790, training loss: 0.6105350852012634 = 0.0017778212204575539 + 0.1 * 6.0875725746154785
Epoch 1790, val loss: 1.46883225440979
Epoch 1800, training loss: 0.6097232699394226 = 0.0017613352974876761 + 0.1 * 6.079619407653809
Epoch 1800, val loss: 1.4705103635787964
Epoch 1810, training loss: 0.6091570258140564 = 0.001745188725180924 + 0.1 * 6.074118614196777
Epoch 1810, val loss: 1.4721759557724
Epoch 1820, training loss: 0.6098259687423706 = 0.001729399897158146 + 0.1 * 6.080965518951416
Epoch 1820, val loss: 1.4739195108413696
Epoch 1830, training loss: 0.6094544529914856 = 0.0017136596143245697 + 0.1 * 6.077408313751221
Epoch 1830, val loss: 1.4754420518875122
Epoch 1840, training loss: 0.6096901297569275 = 0.0016981946537271142 + 0.1 * 6.079919338226318
Epoch 1840, val loss: 1.4769902229309082
Epoch 1850, training loss: 0.6089779138565063 = 0.0016831244574859738 + 0.1 * 6.072947978973389
Epoch 1850, val loss: 1.4786783456802368
Epoch 1860, training loss: 0.6100635528564453 = 0.0016682882560417056 + 0.1 * 6.0839524269104
Epoch 1860, val loss: 1.48026704788208
Epoch 1870, training loss: 0.6086264252662659 = 0.0016535636968910694 + 0.1 * 6.069728374481201
Epoch 1870, val loss: 1.481791377067566
Epoch 1880, training loss: 0.6097190976142883 = 0.0016392841935157776 + 0.1 * 6.0807976722717285
Epoch 1880, val loss: 1.4833810329437256
Epoch 1890, training loss: 0.6093770265579224 = 0.0016249926993623376 + 0.1 * 6.07751989364624
Epoch 1890, val loss: 1.4849129915237427
Epoch 1900, training loss: 0.609029233455658 = 0.0016110813012346625 + 0.1 * 6.07418155670166
Epoch 1900, val loss: 1.4864529371261597
Epoch 1910, training loss: 0.6091864705085754 = 0.0015974401030689478 + 0.1 * 6.075890064239502
Epoch 1910, val loss: 1.4880491495132446
Epoch 1920, training loss: 0.6086030006408691 = 0.001583912642672658 + 0.1 * 6.0701904296875
Epoch 1920, val loss: 1.4895703792572021
Epoch 1930, training loss: 0.6089816689491272 = 0.0015706041594967246 + 0.1 * 6.074110507965088
Epoch 1930, val loss: 1.4910328388214111
Epoch 1940, training loss: 0.6092231869697571 = 0.0015575708821415901 + 0.1 * 6.076655864715576
Epoch 1940, val loss: 1.4924875497817993
Epoch 1950, training loss: 0.6082727909088135 = 0.0015446627512574196 + 0.1 * 6.067281246185303
Epoch 1950, val loss: 1.4939253330230713
Epoch 1960, training loss: 0.6086170673370361 = 0.0015321389073505998 + 0.1 * 6.070849418640137
Epoch 1960, val loss: 1.4954943656921387
Epoch 1970, training loss: 0.6083558797836304 = 0.0015197728062048554 + 0.1 * 6.068360805511475
Epoch 1970, val loss: 1.4969910383224487
Epoch 1980, training loss: 0.6090870499610901 = 0.0015074111288413405 + 0.1 * 6.075796604156494
Epoch 1980, val loss: 1.4983781576156616
Epoch 1990, training loss: 0.6090705394744873 = 0.0014951779739931226 + 0.1 * 6.075753211975098
Epoch 1990, val loss: 1.4997307062149048
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8229
Flip ASR: 0.7911/225 nodes
The final ASR:0.71464, 0.09558, Accuracy:0.81975, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9420])
updated graph: torch.Size([2, 10508])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83086, 0.00349
Begin epxeriment: cont_weight: 0.1 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.1, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.783817768096924 = 1.9464412927627563 + 0.1 * 8.37376594543457
Epoch 0, val loss: 1.9507418870925903
Epoch 10, training loss: 2.773056745529175 = 1.935710072517395 + 0.1 * 8.373467445373535
Epoch 10, val loss: 1.9394804239273071
Epoch 20, training loss: 2.759697914123535 = 1.9225177764892578 + 0.1 * 8.371801376342773
Epoch 20, val loss: 1.9254586696624756
Epoch 30, training loss: 2.740262031555176 = 1.9042787551879883 + 0.1 * 8.359832763671875
Epoch 30, val loss: 1.9060338735580444
Epoch 40, training loss: 2.705256462097168 = 1.8777023553848267 + 0.1 * 8.275541305541992
Epoch 40, val loss: 1.878272533416748
Epoch 50, training loss: 2.614220380783081 = 1.8443725109100342 + 0.1 * 7.698477745056152
Epoch 50, val loss: 1.8460099697113037
Epoch 60, training loss: 2.5342636108398438 = 1.8130450248718262 + 0.1 * 7.212186336517334
Epoch 60, val loss: 1.8178761005401611
Epoch 70, training loss: 2.466916084289551 = 1.7838720083236694 + 0.1 * 6.830440998077393
Epoch 70, val loss: 1.7936426401138306
Epoch 80, training loss: 2.4189553260803223 = 1.7535269260406494 + 0.1 * 6.65428352355957
Epoch 80, val loss: 1.7687804698944092
Epoch 90, training loss: 2.3733935356140137 = 1.714388132095337 + 0.1 * 6.590054512023926
Epoch 90, val loss: 1.7360421419143677
Epoch 100, training loss: 2.3170626163482666 = 1.6608697175979614 + 0.1 * 6.5619282722473145
Epoch 100, val loss: 1.691693902015686
Epoch 110, training loss: 2.2441699504852295 = 1.5897722244262695 + 0.1 * 6.5439772605896
Epoch 110, val loss: 1.6337167024612427
Epoch 120, training loss: 2.155060052871704 = 1.5020490884780884 + 0.1 * 6.53010892868042
Epoch 120, val loss: 1.5632193088531494
Epoch 130, training loss: 2.056265354156494 = 1.4045193195343018 + 0.1 * 6.517459392547607
Epoch 130, val loss: 1.4864863157272339
Epoch 140, training loss: 1.9529489278793335 = 1.3025469779968262 + 0.1 * 6.504019260406494
Epoch 140, val loss: 1.4079943895339966
Epoch 150, training loss: 1.8482913970947266 = 1.1995657682418823 + 0.1 * 6.4872565269470215
Epoch 150, val loss: 1.3297467231750488
Epoch 160, training loss: 1.7472307682037354 = 1.099724531173706 + 0.1 * 6.475061893463135
Epoch 160, val loss: 1.2546659708023071
Epoch 170, training loss: 1.6538358926773071 = 1.0079238414764404 + 0.1 * 6.459120273590088
Epoch 170, val loss: 1.1849833726882935
Epoch 180, training loss: 1.5698710680007935 = 0.9248874187469482 + 0.1 * 6.449836254119873
Epoch 180, val loss: 1.1221047639846802
Epoch 190, training loss: 1.4922711849212646 = 0.8482728600502014 + 0.1 * 6.43998384475708
Epoch 190, val loss: 1.0646910667419434
Epoch 200, training loss: 1.4179704189300537 = 0.7747639417648315 + 0.1 * 6.432064533233643
Epoch 200, val loss: 1.0106297731399536
Epoch 210, training loss: 1.345609426498413 = 0.7032166719436646 + 0.1 * 6.4239277839660645
Epoch 210, val loss: 0.9591385722160339
Epoch 220, training loss: 1.2760430574417114 = 0.6336962580680847 + 0.1 * 6.423468112945557
Epoch 220, val loss: 0.9096075892448425
Epoch 230, training loss: 1.2097101211547852 = 0.5684171319007874 + 0.1 * 6.412930488586426
Epoch 230, val loss: 0.8641384243965149
Epoch 240, training loss: 1.1489582061767578 = 0.5082257986068726 + 0.1 * 6.407324314117432
Epoch 240, val loss: 0.8231717348098755
Epoch 250, training loss: 1.094245433807373 = 0.45391538739204407 + 0.1 * 6.403299808502197
Epoch 250, val loss: 0.7877660393714905
Epoch 260, training loss: 1.0449610948562622 = 0.40522152185440063 + 0.1 * 6.397395610809326
Epoch 260, val loss: 0.7579203248023987
Epoch 270, training loss: 1.0006794929504395 = 0.3618542551994324 + 0.1 * 6.388251781463623
Epoch 270, val loss: 0.7334015369415283
Epoch 280, training loss: 0.9617341756820679 = 0.32308247685432434 + 0.1 * 6.386516571044922
Epoch 280, val loss: 0.7138773202896118
Epoch 290, training loss: 0.9259735345840454 = 0.2886514961719513 + 0.1 * 6.373220443725586
Epoch 290, val loss: 0.6991344690322876
Epoch 300, training loss: 0.8950165510177612 = 0.25802096724510193 + 0.1 * 6.369955539703369
Epoch 300, val loss: 0.6885758638381958
Epoch 310, training loss: 0.866789698600769 = 0.23092758655548096 + 0.1 * 6.358621120452881
Epoch 310, val loss: 0.6816599369049072
Epoch 320, training loss: 0.8422881364822388 = 0.20700517296791077 + 0.1 * 6.3528289794921875
Epoch 320, val loss: 0.6779190301895142
Epoch 330, training loss: 0.8204721808433533 = 0.18598012626171112 + 0.1 * 6.3449201583862305
Epoch 330, val loss: 0.6768377423286438
Epoch 340, training loss: 0.8008530735969543 = 0.16757012903690338 + 0.1 * 6.332829475402832
Epoch 340, val loss: 0.6778274178504944
Epoch 350, training loss: 0.7842565774917603 = 0.1514822542667389 + 0.1 * 6.3277435302734375
Epoch 350, val loss: 0.6805630326271057
Epoch 360, training loss: 0.769188404083252 = 0.1372366100549698 + 0.1 * 6.319518089294434
Epoch 360, val loss: 0.6847158670425415
Epoch 370, training loss: 0.7573487758636475 = 0.12457096576690674 + 0.1 * 6.327777862548828
Epoch 370, val loss: 0.6899953484535217
Epoch 380, training loss: 0.7444419264793396 = 0.11335740983486176 + 0.1 * 6.310844898223877
Epoch 380, val loss: 0.6961641311645508
Epoch 390, training loss: 0.733712911605835 = 0.10333077609539032 + 0.1 * 6.303821086883545
Epoch 390, val loss: 0.7031010389328003
Epoch 400, training loss: 0.7240505218505859 = 0.09437078982591629 + 0.1 * 6.296797275543213
Epoch 400, val loss: 0.7105669975280762
Epoch 410, training loss: 0.7157432436943054 = 0.08637033402919769 + 0.1 * 6.293728828430176
Epoch 410, val loss: 0.7184997797012329
Epoch 420, training loss: 0.7073503732681274 = 0.07918520271778107 + 0.1 * 6.281651973724365
Epoch 420, val loss: 0.7269027233123779
Epoch 430, training loss: 0.7009909749031067 = 0.07270926237106323 + 0.1 * 6.2828168869018555
Epoch 430, val loss: 0.7356094717979431
Epoch 440, training loss: 0.6947962045669556 = 0.06689223647117615 + 0.1 * 6.2790398597717285
Epoch 440, val loss: 0.7444264888763428
Epoch 450, training loss: 0.6884880661964417 = 0.06166569143533707 + 0.1 * 6.268223285675049
Epoch 450, val loss: 0.7534465193748474
Epoch 460, training loss: 0.6844134330749512 = 0.056949444115161896 + 0.1 * 6.27463960647583
Epoch 460, val loss: 0.7625003457069397
Epoch 470, training loss: 0.6783751249313354 = 0.052697308361530304 + 0.1 * 6.256778240203857
Epoch 470, val loss: 0.7716068029403687
Epoch 480, training loss: 0.6761108636856079 = 0.04885596036911011 + 0.1 * 6.272548675537109
Epoch 480, val loss: 0.7806693911552429
Epoch 490, training loss: 0.6709944009780884 = 0.04540398344397545 + 0.1 * 6.255904197692871
Epoch 490, val loss: 0.789645791053772
Epoch 500, training loss: 0.6666358113288879 = 0.04227607697248459 + 0.1 * 6.243597030639648
Epoch 500, val loss: 0.7985506057739258
Epoch 510, training loss: 0.6638224720954895 = 0.03943407163023949 + 0.1 * 6.2438836097717285
Epoch 510, val loss: 0.807333767414093
Epoch 520, training loss: 0.6603919267654419 = 0.03685158118605614 + 0.1 * 6.235403060913086
Epoch 520, val loss: 0.8159984946250916
Epoch 530, training loss: 0.6575484275817871 = 0.03449877351522446 + 0.1 * 6.230496883392334
Epoch 530, val loss: 0.824605405330658
Epoch 540, training loss: 0.6560933589935303 = 0.03235294669866562 + 0.1 * 6.2374043464660645
Epoch 540, val loss: 0.8329440951347351
Epoch 550, training loss: 0.6525850892066956 = 0.030402980744838715 + 0.1 * 6.221821308135986
Epoch 550, val loss: 0.841204047203064
Epoch 560, training loss: 0.651675283908844 = 0.028618095442652702 + 0.1 * 6.230571746826172
Epoch 560, val loss: 0.8492884039878845
Epoch 570, training loss: 0.6489032506942749 = 0.026983393356204033 + 0.1 * 6.219198703765869
Epoch 570, val loss: 0.8571245670318604
Epoch 580, training loss: 0.6469501256942749 = 0.025483369827270508 + 0.1 * 6.214667320251465
Epoch 580, val loss: 0.8648402690887451
Epoch 590, training loss: 0.6456966400146484 = 0.024101845920085907 + 0.1 * 6.21594762802124
Epoch 590, val loss: 0.8723470568656921
Epoch 600, training loss: 0.6433982253074646 = 0.022831358015537262 + 0.1 * 6.2056684494018555
Epoch 600, val loss: 0.8797292709350586
Epoch 610, training loss: 0.6425126791000366 = 0.021656930446624756 + 0.1 * 6.20855712890625
Epoch 610, val loss: 0.8869181275367737
Epoch 620, training loss: 0.6415918469429016 = 0.02057204395532608 + 0.1 * 6.210197925567627
Epoch 620, val loss: 0.8938388824462891
Epoch 630, training loss: 0.639046847820282 = 0.019571540877223015 + 0.1 * 6.1947526931762695
Epoch 630, val loss: 0.9007417559623718
Epoch 640, training loss: 0.6390621662139893 = 0.018641745671629906 + 0.1 * 6.204204082489014
Epoch 640, val loss: 0.907455563545227
Epoch 650, training loss: 0.6363663673400879 = 0.017777901142835617 + 0.1 * 6.185884475708008
Epoch 650, val loss: 0.9139523506164551
Epoch 660, training loss: 0.6384432911872864 = 0.016973186284303665 + 0.1 * 6.214700698852539
Epoch 660, val loss: 0.9203439950942993
Epoch 670, training loss: 0.6347609758377075 = 0.01622338779270649 + 0.1 * 6.185375690460205
Epoch 670, val loss: 0.9265710711479187
Epoch 680, training loss: 0.6346801519393921 = 0.015525243245065212 + 0.1 * 6.191549301147461
Epoch 680, val loss: 0.9327029585838318
Epoch 690, training loss: 0.632994532585144 = 0.01487303338944912 + 0.1 * 6.181214809417725
Epoch 690, val loss: 0.9386923909187317
Epoch 700, training loss: 0.6325807571411133 = 0.014261533506214619 + 0.1 * 6.183191776275635
Epoch 700, val loss: 0.9444831013679504
Epoch 710, training loss: 0.6316776275634766 = 0.013688117265701294 + 0.1 * 6.17989444732666
Epoch 710, val loss: 0.9501401782035828
Epoch 720, training loss: 0.6307555437088013 = 0.013151155784726143 + 0.1 * 6.176043510437012
Epoch 720, val loss: 0.9557228088378906
Epoch 730, training loss: 0.6296648979187012 = 0.012647034600377083 + 0.1 * 6.170178413391113
Epoch 730, val loss: 0.9612780809402466
Epoch 740, training loss: 0.6307108402252197 = 0.012171812355518341 + 0.1 * 6.185390472412109
Epoch 740, val loss: 0.9665946960449219
Epoch 750, training loss: 0.6287621259689331 = 0.011724155396223068 + 0.1 * 6.170379638671875
Epoch 750, val loss: 0.9717235565185547
Epoch 760, training loss: 0.6278309226036072 = 0.011304120533168316 + 0.1 * 6.1652679443359375
Epoch 760, val loss: 0.9769636392593384
Epoch 770, training loss: 0.6280767917633057 = 0.01090666837990284 + 0.1 * 6.171700954437256
Epoch 770, val loss: 0.982007622718811
Epoch 780, training loss: 0.6266981959342957 = 0.01053078193217516 + 0.1 * 6.1616740226745605
Epoch 780, val loss: 0.9867668151855469
Epoch 790, training loss: 0.6259593963623047 = 0.01017686165869236 + 0.1 * 6.157825469970703
Epoch 790, val loss: 0.9916466474533081
Epoch 800, training loss: 0.6270350217819214 = 0.009840848855674267 + 0.1 * 6.171941757202148
Epoch 800, val loss: 0.9963338375091553
Epoch 810, training loss: 0.6254734396934509 = 0.009522737003862858 + 0.1 * 6.159506797790527
Epoch 810, val loss: 1.0009580850601196
Epoch 820, training loss: 0.6248945593833923 = 0.009221774525940418 + 0.1 * 6.156727313995361
Epoch 820, val loss: 1.0055313110351562
Epoch 830, training loss: 0.6243664026260376 = 0.008934704586863518 + 0.1 * 6.1543169021606445
Epoch 830, val loss: 1.009934425354004
Epoch 840, training loss: 0.6253491640090942 = 0.008662006817758083 + 0.1 * 6.166871547698975
Epoch 840, val loss: 1.0142087936401367
Epoch 850, training loss: 0.6233809590339661 = 0.00840317364782095 + 0.1 * 6.149777412414551
Epoch 850, val loss: 1.0185463428497314
Epoch 860, training loss: 0.6245976686477661 = 0.00815686397254467 + 0.1 * 6.164408206939697
Epoch 860, val loss: 1.0227470397949219
Epoch 870, training loss: 0.6226262450218201 = 0.007921931333839893 + 0.1 * 6.147043228149414
Epoch 870, val loss: 1.0268597602844238
Epoch 880, training loss: 0.6228059530258179 = 0.007698674686253071 + 0.1 * 6.151072978973389
Epoch 880, val loss: 1.0309189558029175
Epoch 890, training loss: 0.6215584874153137 = 0.0074845426715910435 + 0.1 * 6.140739440917969
Epoch 890, val loss: 1.034796118736267
Epoch 900, training loss: 0.6213184595108032 = 0.00728123914450407 + 0.1 * 6.140371799468994
Epoch 900, val loss: 1.0387511253356934
Epoch 910, training loss: 0.6213266849517822 = 0.0070868367329239845 + 0.1 * 6.142398834228516
Epoch 910, val loss: 1.042641520500183
Epoch 920, training loss: 0.6209965348243713 = 0.006900230422616005 + 0.1 * 6.140962600708008
Epoch 920, val loss: 1.0463736057281494
Epoch 930, training loss: 0.6205289959907532 = 0.006721906363964081 + 0.1 * 6.138070583343506
Epoch 930, val loss: 1.0501574277877808
Epoch 940, training loss: 0.6207435131072998 = 0.006550387945026159 + 0.1 * 6.141931533813477
Epoch 940, val loss: 1.0536656379699707
Epoch 950, training loss: 0.6200289726257324 = 0.006386817432940006 + 0.1 * 6.1364216804504395
Epoch 950, val loss: 1.0573147535324097
Epoch 960, training loss: 0.619691789150238 = 0.006230400875210762 + 0.1 * 6.134613513946533
Epoch 960, val loss: 1.0609017610549927
Epoch 970, training loss: 0.6193945407867432 = 0.006079948041588068 + 0.1 * 6.133145332336426
Epoch 970, val loss: 1.0644294023513794
Epoch 980, training loss: 0.6188872456550598 = 0.005935534834861755 + 0.1 * 6.129517078399658
Epoch 980, val loss: 1.0678342580795288
Epoch 990, training loss: 0.6206188797950745 = 0.005796771962195635 + 0.1 * 6.148220539093018
Epoch 990, val loss: 1.0712101459503174
Epoch 1000, training loss: 0.6189318895339966 = 0.005662888288497925 + 0.1 * 6.132689476013184
Epoch 1000, val loss: 1.0744460821151733
Epoch 1010, training loss: 0.6181345582008362 = 0.005535151809453964 + 0.1 * 6.1259942054748535
Epoch 1010, val loss: 1.0778166055679321
Epoch 1020, training loss: 0.6180572509765625 = 0.00541170546784997 + 0.1 * 6.126455307006836
Epoch 1020, val loss: 1.0810768604278564
Epoch 1030, training loss: 0.618181049823761 = 0.005292767658829689 + 0.1 * 6.128882884979248
Epoch 1030, val loss: 1.084248661994934
Epoch 1040, training loss: 0.6172348260879517 = 0.005178214516490698 + 0.1 * 6.120565891265869
Epoch 1040, val loss: 1.0872987508773804
Epoch 1050, training loss: 0.6171960234642029 = 0.005068578757345676 + 0.1 * 6.121273994445801
Epoch 1050, val loss: 1.0904395580291748
Epoch 1060, training loss: 0.6177086234092712 = 0.004962426610291004 + 0.1 * 6.1274614334106445
Epoch 1060, val loss: 1.0935006141662598
Epoch 1070, training loss: 0.6170700192451477 = 0.004859631881117821 + 0.1 * 6.122103691101074
Epoch 1070, val loss: 1.0964394807815552
Epoch 1080, training loss: 0.6164451241493225 = 0.004760632291436195 + 0.1 * 6.116844654083252
Epoch 1080, val loss: 1.0993846654891968
Epoch 1090, training loss: 0.6170122027397156 = 0.004665364045649767 + 0.1 * 6.123467922210693
Epoch 1090, val loss: 1.1023130416870117
Epoch 1100, training loss: 0.6159972548484802 = 0.0045732720755040646 + 0.1 * 6.114239692687988
Epoch 1100, val loss: 1.105258584022522
Epoch 1110, training loss: 0.616885781288147 = 0.004484470933675766 + 0.1 * 6.1240129470825195
Epoch 1110, val loss: 1.1081554889678955
Epoch 1120, training loss: 0.6161904335021973 = 0.00439818948507309 + 0.1 * 6.117922306060791
Epoch 1120, val loss: 1.1108633279800415
Epoch 1130, training loss: 0.6155199408531189 = 0.0043150875717401505 + 0.1 * 6.112048625946045
Epoch 1130, val loss: 1.1136982440948486
Epoch 1140, training loss: 0.6160432696342468 = 0.0042344979010522366 + 0.1 * 6.1180877685546875
Epoch 1140, val loss: 1.1164199113845825
Epoch 1150, training loss: 0.6169765591621399 = 0.004156158771365881 + 0.1 * 6.128203868865967
Epoch 1150, val loss: 1.1190330982208252
Epoch 1160, training loss: 0.6153424978256226 = 0.004080635495483875 + 0.1 * 6.112618446350098
Epoch 1160, val loss: 1.1216647624969482
Epoch 1170, training loss: 0.6143931746482849 = 0.004008057527244091 + 0.1 * 6.103850841522217
Epoch 1170, val loss: 1.1244016885757446
Epoch 1180, training loss: 0.6165443658828735 = 0.003937524277716875 + 0.1 * 6.126068115234375
Epoch 1180, val loss: 1.1270383596420288
Epoch 1190, training loss: 0.6150334477424622 = 0.003867973806336522 + 0.1 * 6.111654758453369
Epoch 1190, val loss: 1.1294230222702026
Epoch 1200, training loss: 0.6148576736450195 = 0.003801285522058606 + 0.1 * 6.1105637550354
Epoch 1200, val loss: 1.1319433450698853
Epoch 1210, training loss: 0.6144235730171204 = 0.003736516460776329 + 0.1 * 6.106870651245117
Epoch 1210, val loss: 1.1344200372695923
Epoch 1220, training loss: 0.6150981187820435 = 0.0036739548668265343 + 0.1 * 6.114241123199463
Epoch 1220, val loss: 1.1368805170059204
Epoch 1230, training loss: 0.6147330403327942 = 0.003612753702327609 + 0.1 * 6.111202716827393
Epoch 1230, val loss: 1.1393078565597534
Epoch 1240, training loss: 0.6135005950927734 = 0.003553612157702446 + 0.1 * 6.0994696617126465
Epoch 1240, val loss: 1.1416618824005127
Epoch 1250, training loss: 0.6159035563468933 = 0.0034961500205099583 + 0.1 * 6.1240739822387695
Epoch 1250, val loss: 1.1439886093139648
Epoch 1260, training loss: 0.6138512492179871 = 0.0034400331787765026 + 0.1 * 6.104112148284912
Epoch 1260, val loss: 1.1462846994400024
Epoch 1270, training loss: 0.6134620904922485 = 0.003385969903320074 + 0.1 * 6.100761413574219
Epoch 1270, val loss: 1.1486399173736572
Epoch 1280, training loss: 0.6146655082702637 = 0.0033332358580082655 + 0.1 * 6.113322734832764
Epoch 1280, val loss: 1.150891900062561
Epoch 1290, training loss: 0.6134322881698608 = 0.0032814538571983576 + 0.1 * 6.101508140563965
Epoch 1290, val loss: 1.1529816389083862
Epoch 1300, training loss: 0.6139058470726013 = 0.003231537528336048 + 0.1 * 6.106742858886719
Epoch 1300, val loss: 1.1552177667617798
Epoch 1310, training loss: 0.6126577258110046 = 0.003182729473337531 + 0.1 * 6.094749927520752
Epoch 1310, val loss: 1.1573518514633179
Epoch 1320, training loss: 0.6129500865936279 = 0.0031356443651020527 + 0.1 * 6.09814453125
Epoch 1320, val loss: 1.1595526933670044
Epoch 1330, training loss: 0.6135574579238892 = 0.003089730627834797 + 0.1 * 6.104677200317383
Epoch 1330, val loss: 1.161718487739563
Epoch 1340, training loss: 0.6132969856262207 = 0.0030448008328676224 + 0.1 * 6.102521896362305
Epoch 1340, val loss: 1.1637423038482666
Epoch 1350, training loss: 0.6124106049537659 = 0.0030009413603693247 + 0.1 * 6.094096660614014
Epoch 1350, val loss: 1.1658035516738892
Epoch 1360, training loss: 0.6137584447860718 = 0.0029584572184830904 + 0.1 * 6.107999801635742
Epoch 1360, val loss: 1.1679102182388306
Epoch 1370, training loss: 0.6121355891227722 = 0.002916684839874506 + 0.1 * 6.092188835144043
Epoch 1370, val loss: 1.1698544025421143
Epoch 1380, training loss: 0.6125885844230652 = 0.002876286394894123 + 0.1 * 6.097123146057129
Epoch 1380, val loss: 1.1719084978103638
Epoch 1390, training loss: 0.6116961240768433 = 0.0028368376661092043 + 0.1 * 6.088593006134033
Epoch 1390, val loss: 1.1739031076431274
Epoch 1400, training loss: 0.6130942702293396 = 0.0027985034976154566 + 0.1 * 6.102957725524902
Epoch 1400, val loss: 1.175869345664978
Epoch 1410, training loss: 0.6121386885643005 = 0.002760640811175108 + 0.1 * 6.093780517578125
Epoch 1410, val loss: 1.1778117418289185
Epoch 1420, training loss: 0.6113236546516418 = 0.0027240642812103033 + 0.1 * 6.085996150970459
Epoch 1420, val loss: 1.1797412633895874
Epoch 1430, training loss: 0.6125744581222534 = 0.0026882190722972155 + 0.1 * 6.098862171173096
Epoch 1430, val loss: 1.181677222251892
Epoch 1440, training loss: 0.6120880246162415 = 0.0026529869064688683 + 0.1 * 6.094350337982178
Epoch 1440, val loss: 1.1835167407989502
Epoch 1450, training loss: 0.611291229724884 = 0.0026188958436250687 + 0.1 * 6.0867228507995605
Epoch 1450, val loss: 1.1854019165039062
Epoch 1460, training loss: 0.6118186712265015 = 0.002585746580734849 + 0.1 * 6.092329025268555
Epoch 1460, val loss: 1.1872578859329224
Epoch 1470, training loss: 0.6113348007202148 = 0.0025532315485179424 + 0.1 * 6.087815761566162
Epoch 1470, val loss: 1.1890873908996582
Epoch 1480, training loss: 0.6130304932594299 = 0.0025214122142642736 + 0.1 * 6.105091094970703
Epoch 1480, val loss: 1.1908609867095947
Epoch 1490, training loss: 0.6110828518867493 = 0.002490217098966241 + 0.1 * 6.085926055908203
Epoch 1490, val loss: 1.1926339864730835
Epoch 1500, training loss: 0.6105339527130127 = 0.002460036426782608 + 0.1 * 6.080738544464111
Epoch 1500, val loss: 1.1944539546966553
Epoch 1510, training loss: 0.6105541586875916 = 0.002430474618449807 + 0.1 * 6.081236362457275
Epoch 1510, val loss: 1.1962499618530273
Epoch 1520, training loss: 0.6121368408203125 = 0.0024012019857764244 + 0.1 * 6.097356796264648
Epoch 1520, val loss: 1.1979446411132812
Epoch 1530, training loss: 0.6107773780822754 = 0.0023723477497696877 + 0.1 * 6.08405065536499
Epoch 1530, val loss: 1.199587106704712
Epoch 1540, training loss: 0.6109942197799683 = 0.0023445363622158766 + 0.1 * 6.086496829986572
Epoch 1540, val loss: 1.2013299465179443
Epoch 1550, training loss: 0.609825611114502 = 0.0023172693327069283 + 0.1 * 6.0750837326049805
Epoch 1550, val loss: 1.2030649185180664
Epoch 1560, training loss: 0.6124786734580994 = 0.002290631178766489 + 0.1 * 6.1018805503845215
Epoch 1560, val loss: 1.204769253730774
Epoch 1570, training loss: 0.6101183891296387 = 0.002264320617541671 + 0.1 * 6.078540325164795
Epoch 1570, val loss: 1.2063779830932617
Epoch 1580, training loss: 0.6106967329978943 = 0.0022389220539480448 + 0.1 * 6.084578037261963
Epoch 1580, val loss: 1.2080527544021606
Epoch 1590, training loss: 0.6103389263153076 = 0.0022138701751828194 + 0.1 * 6.081250190734863
Epoch 1590, val loss: 1.209717869758606
Epoch 1600, training loss: 0.6100461483001709 = 0.002189377788454294 + 0.1 * 6.0785675048828125
Epoch 1600, val loss: 1.2113549709320068
Epoch 1610, training loss: 0.609886109828949 = 0.0021654171869158745 + 0.1 * 6.077206611633301
Epoch 1610, val loss: 1.2129535675048828
Epoch 1620, training loss: 0.6099332571029663 = 0.0021419362165033817 + 0.1 * 6.0779128074646
Epoch 1620, val loss: 1.2145506143569946
Epoch 1630, training loss: 0.6100797653198242 = 0.002118639647960663 + 0.1 * 6.079611301422119
Epoch 1630, val loss: 1.2160900831222534
Epoch 1640, training loss: 0.609209418296814 = 0.002095886506140232 + 0.1 * 6.0711350440979
Epoch 1640, val loss: 1.217628002166748
Epoch 1650, training loss: 0.609637975692749 = 0.002073870738968253 + 0.1 * 6.075640678405762
Epoch 1650, val loss: 1.219262719154358
Epoch 1660, training loss: 0.6114490032196045 = 0.002052002353593707 + 0.1 * 6.093969821929932
Epoch 1660, val loss: 1.220846176147461
Epoch 1670, training loss: 0.6099710464477539 = 0.002030419884249568 + 0.1 * 6.079406261444092
Epoch 1670, val loss: 1.222199559211731
Epoch 1680, training loss: 0.6101307272911072 = 0.002009592717513442 + 0.1 * 6.081211566925049
Epoch 1680, val loss: 1.2238222360610962
Epoch 1690, training loss: 0.6094262599945068 = 0.001989145763218403 + 0.1 * 6.074371337890625
Epoch 1690, val loss: 1.2252851724624634
Epoch 1700, training loss: 0.6094419956207275 = 0.0019691064953804016 + 0.1 * 6.074728965759277
Epoch 1700, val loss: 1.226814866065979
Epoch 1710, training loss: 0.6091086268424988 = 0.001949312980286777 + 0.1 * 6.071593284606934
Epoch 1710, val loss: 1.228257417678833
Epoch 1720, training loss: 0.6088184118270874 = 0.0019299665000289679 + 0.1 * 6.068884372711182
Epoch 1720, val loss: 1.2296770811080933
Epoch 1730, training loss: 0.6099448204040527 = 0.0019109720597043633 + 0.1 * 6.080338478088379
Epoch 1730, val loss: 1.2311325073242188
Epoch 1740, training loss: 0.6091354489326477 = 0.0018919723806902766 + 0.1 * 6.072434902191162
Epoch 1740, val loss: 1.232485055923462
Epoch 1750, training loss: 0.6087929010391235 = 0.0018736552447080612 + 0.1 * 6.069191932678223
Epoch 1750, val loss: 1.233887791633606
Epoch 1760, training loss: 0.6093683242797852 = 0.0018555299611762166 + 0.1 * 6.075127601623535
Epoch 1760, val loss: 1.2353266477584839
Epoch 1770, training loss: 0.6086121201515198 = 0.001837850664742291 + 0.1 * 6.067742824554443
Epoch 1770, val loss: 1.2366880178451538
Epoch 1780, training loss: 0.6089876294136047 = 0.0018206893000751734 + 0.1 * 6.071669578552246
Epoch 1780, val loss: 1.2381404638290405
Epoch 1790, training loss: 0.6087430715560913 = 0.0018036009278148413 + 0.1 * 6.069394588470459
Epoch 1790, val loss: 1.239508867263794
Epoch 1800, training loss: 0.6086788177490234 = 0.0017868385184556246 + 0.1 * 6.068919658660889
Epoch 1800, val loss: 1.240840196609497
Epoch 1810, training loss: 0.6086398959159851 = 0.0017701848410069942 + 0.1 * 6.068696975708008
Epoch 1810, val loss: 1.242110013961792
Epoch 1820, training loss: 0.6087437272071838 = 0.0017539083492010832 + 0.1 * 6.0698981285095215
Epoch 1820, val loss: 1.2433847188949585
Epoch 1830, training loss: 0.6078320741653442 = 0.001738018705509603 + 0.1 * 6.060940742492676
Epoch 1830, val loss: 1.244782567024231
Epoch 1840, training loss: 0.6090470552444458 = 0.001722516375593841 + 0.1 * 6.073245048522949
Epoch 1840, val loss: 1.2461743354797363
Epoch 1850, training loss: 0.6076708436012268 = 0.0017069565365090966 + 0.1 * 6.059638977050781
Epoch 1850, val loss: 1.2473423480987549
Epoch 1860, training loss: 0.6083393096923828 = 0.0016918489709496498 + 0.1 * 6.066473960876465
Epoch 1860, val loss: 1.2486158609390259
Epoch 1870, training loss: 0.6090563535690308 = 0.0016769382636994123 + 0.1 * 6.073794364929199
Epoch 1870, val loss: 1.249922752380371
Epoch 1880, training loss: 0.6083153486251831 = 0.0016621677204966545 + 0.1 * 6.066531658172607
Epoch 1880, val loss: 1.2511892318725586
Epoch 1890, training loss: 0.6086863875389099 = 0.001647690893150866 + 0.1 * 6.0703864097595215
Epoch 1890, val loss: 1.2524296045303345
Epoch 1900, training loss: 0.607815146446228 = 0.0016334675019606948 + 0.1 * 6.061816692352295
Epoch 1900, val loss: 1.2536805868148804
Epoch 1910, training loss: 0.6079576015472412 = 0.001619682414457202 + 0.1 * 6.063378810882568
Epoch 1910, val loss: 1.254998803138733
Epoch 1920, training loss: 0.6082823276519775 = 0.0016060550697147846 + 0.1 * 6.066762447357178
Epoch 1920, val loss: 1.2562583684921265
Epoch 1930, training loss: 0.6072270274162292 = 0.0015925334300845861 + 0.1 * 6.056344985961914
Epoch 1930, val loss: 1.2574208974838257
Epoch 1940, training loss: 0.6105911731719971 = 0.0015791994519531727 + 0.1 * 6.090119361877441
Epoch 1940, val loss: 1.2585645914077759
Epoch 1950, training loss: 0.607805609703064 = 0.0015659109922125936 + 0.1 * 6.06239652633667
Epoch 1950, val loss: 1.2596756219863892
Epoch 1960, training loss: 0.6076061129570007 = 0.0015531327808275819 + 0.1 * 6.060529708862305
Epoch 1960, val loss: 1.2609195709228516
Epoch 1970, training loss: 0.6083077788352966 = 0.001540485885925591 + 0.1 * 6.0676727294921875
Epoch 1970, val loss: 1.2621530294418335
Epoch 1980, training loss: 0.6073570251464844 = 0.0015279343351721764 + 0.1 * 6.058290958404541
Epoch 1980, val loss: 1.2633106708526611
Epoch 1990, training loss: 0.6078833937644958 = 0.001515712821856141 + 0.1 * 6.063676834106445
Epoch 1990, val loss: 1.2645021677017212
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7638
Flip ASR: 0.7200/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.788735866546631 = 1.9513659477233887 + 0.1 * 8.373698234558105
Epoch 0, val loss: 1.951162338256836
Epoch 10, training loss: 2.778463840484619 = 1.9411325454711914 + 0.1 * 8.373311996459961
Epoch 10, val loss: 1.9403128623962402
Epoch 20, training loss: 2.7656569480895996 = 1.9285553693771362 + 0.1 * 8.371015548706055
Epoch 20, val loss: 1.9268417358398438
Epoch 30, training loss: 2.7460689544677734 = 1.9106422662734985 + 0.1 * 8.354267120361328
Epoch 30, val loss: 1.9075350761413574
Epoch 40, training loss: 2.7061209678649902 = 1.8841110467910767 + 0.1 * 8.220098495483398
Epoch 40, val loss: 1.8792330026626587
Epoch 50, training loss: 2.5872697830200195 = 1.8516170978546143 + 0.1 * 7.356527805328369
Epoch 50, val loss: 1.8464069366455078
Epoch 60, training loss: 2.5159244537353516 = 1.821212887763977 + 0.1 * 6.947116851806641
Epoch 60, val loss: 1.8165539503097534
Epoch 70, training loss: 2.464745283126831 = 1.7872244119644165 + 0.1 * 6.775209426879883
Epoch 70, val loss: 1.784305453300476
Epoch 80, training loss: 2.4182491302490234 = 1.7500990629196167 + 0.1 * 6.681501865386963
Epoch 80, val loss: 1.751265048980713
Epoch 90, training loss: 2.369619846343994 = 1.7076166868209839 + 0.1 * 6.620030403137207
Epoch 90, val loss: 1.7143476009368896
Epoch 100, training loss: 2.310352325439453 = 1.6513386964797974 + 0.1 * 6.59013557434082
Epoch 100, val loss: 1.665622353553772
Epoch 110, training loss: 2.234097719192505 = 1.5766856670379639 + 0.1 * 6.574120044708252
Epoch 110, val loss: 1.6020606756210327
Epoch 120, training loss: 2.138364315032959 = 1.4823431968688965 + 0.1 * 6.560211181640625
Epoch 120, val loss: 1.5230820178985596
Epoch 130, training loss: 2.0282065868377686 = 1.3736275434494019 + 0.1 * 6.54578971862793
Epoch 130, val loss: 1.434751033782959
Epoch 140, training loss: 1.9117989540100098 = 1.2590969800949097 + 0.1 * 6.527019023895264
Epoch 140, val loss: 1.345585823059082
Epoch 150, training loss: 1.7964423894882202 = 1.1462054252624512 + 0.1 * 6.502369403839111
Epoch 150, val loss: 1.259182095527649
Epoch 160, training loss: 1.6887465715408325 = 1.040651559829712 + 0.1 * 6.480949878692627
Epoch 160, val loss: 1.1792235374450684
Epoch 170, training loss: 1.5926129817962646 = 0.946831464767456 + 0.1 * 6.4578142166137695
Epoch 170, val loss: 1.108399748802185
Epoch 180, training loss: 1.5066289901733398 = 0.8626149296760559 + 0.1 * 6.4401397705078125
Epoch 180, val loss: 1.0453567504882812
Epoch 190, training loss: 1.4301719665527344 = 0.786346971988678 + 0.1 * 6.4382500648498535
Epoch 190, val loss: 0.9895820617675781
Epoch 200, training loss: 1.3598791360855103 = 0.7184377312660217 + 0.1 * 6.414413928985596
Epoch 200, val loss: 0.941826581954956
Epoch 210, training loss: 1.2971086502075195 = 0.6570545434951782 + 0.1 * 6.400540828704834
Epoch 210, val loss: 0.9012114405632019
Epoch 220, training loss: 1.2406035661697388 = 0.6017132997512817 + 0.1 * 6.38890266418457
Epoch 220, val loss: 0.8674363493919373
Epoch 230, training loss: 1.189380168914795 = 0.5513041615486145 + 0.1 * 6.380760192871094
Epoch 230, val loss: 0.8393763899803162
Epoch 240, training loss: 1.1414813995361328 = 0.5043963193893433 + 0.1 * 6.370851039886475
Epoch 240, val loss: 0.8154417872428894
Epoch 250, training loss: 1.096980094909668 = 0.4604695737361908 + 0.1 * 6.365105152130127
Epoch 250, val loss: 0.7945361733436584
Epoch 260, training loss: 1.0544099807739258 = 0.41898301243782043 + 0.1 * 6.354269027709961
Epoch 260, val loss: 0.7764419317245483
Epoch 270, training loss: 1.015958309173584 = 0.3794635534286499 + 0.1 * 6.364947319030762
Epoch 270, val loss: 0.7610411643981934
Epoch 280, training loss: 0.9764258861541748 = 0.34226447343826294 + 0.1 * 6.34161376953125
Epoch 280, val loss: 0.7488517761230469
Epoch 290, training loss: 0.9444024562835693 = 0.30739888548851013 + 0.1 * 6.3700361251831055
Epoch 290, val loss: 0.7396105527877808
Epoch 300, training loss: 0.9081968069076538 = 0.27564680576324463 + 0.1 * 6.325500011444092
Epoch 300, val loss: 0.7333990931510925
Epoch 310, training loss: 0.8789024949073792 = 0.2469439059495926 + 0.1 * 6.319585800170898
Epoch 310, val loss: 0.7299491167068481
Epoch 320, training loss: 0.8548122048377991 = 0.2213648110628128 + 0.1 * 6.334474086761475
Epoch 320, val loss: 0.7292020320892334
Epoch 330, training loss: 0.8307997584342957 = 0.1991887092590332 + 0.1 * 6.316110134124756
Epoch 330, val loss: 0.7308729290962219
Epoch 340, training loss: 0.8102797865867615 = 0.18002723157405853 + 0.1 * 6.302525043487549
Epoch 340, val loss: 0.7347687482833862
Epoch 350, training loss: 0.7946301102638245 = 0.1634722799062729 + 0.1 * 6.311577796936035
Epoch 350, val loss: 0.7405387759208679
Epoch 360, training loss: 0.7778843641281128 = 0.14922143518924713 + 0.1 * 6.2866291999816895
Epoch 360, val loss: 0.7478302121162415
Epoch 370, training loss: 0.76725834608078 = 0.13682328164577484 + 0.1 * 6.30435037612915
Epoch 370, val loss: 0.7562824487686157
Epoch 380, training loss: 0.7537505626678467 = 0.12605038285255432 + 0.1 * 6.27700138092041
Epoch 380, val loss: 0.7655572891235352
Epoch 390, training loss: 0.7436676025390625 = 0.11656595021486282 + 0.1 * 6.271016597747803
Epoch 390, val loss: 0.7755362391471863
Epoch 400, training loss: 0.7350572347640991 = 0.10814807564020157 + 0.1 * 6.2690911293029785
Epoch 400, val loss: 0.7859521508216858
Epoch 410, training loss: 0.7272610068321228 = 0.10067213326692581 + 0.1 * 6.265888690948486
Epoch 410, val loss: 0.796615719795227
Epoch 420, training loss: 0.7195047736167908 = 0.09400651603937149 + 0.1 * 6.2549824714660645
Epoch 420, val loss: 0.8074612617492676
Epoch 430, training loss: 0.7156693935394287 = 0.08799495548009872 + 0.1 * 6.276744365692139
Epoch 430, val loss: 0.8185160160064697
Epoch 440, training loss: 0.7074373960494995 = 0.08257761597633362 + 0.1 * 6.248598098754883
Epoch 440, val loss: 0.829566240310669
Epoch 450, training loss: 0.7017167210578918 = 0.07765869796276093 + 0.1 * 6.240580081939697
Epoch 450, val loss: 0.8407029509544373
Epoch 460, training loss: 0.6972827911376953 = 0.07316406816244125 + 0.1 * 6.241187572479248
Epoch 460, val loss: 0.8518006205558777
Epoch 470, training loss: 0.6937037110328674 = 0.06905920058488846 + 0.1 * 6.246445178985596
Epoch 470, val loss: 0.8628278970718384
Epoch 480, training loss: 0.688115119934082 = 0.0652969479560852 + 0.1 * 6.228181838989258
Epoch 480, val loss: 0.8737700581550598
Epoch 490, training loss: 0.6851412057876587 = 0.06183004006743431 + 0.1 * 6.233111381530762
Epoch 490, val loss: 0.8845104575157166
Epoch 500, training loss: 0.6812708973884583 = 0.05862392485141754 + 0.1 * 6.22646951675415
Epoch 500, val loss: 0.8950669169425964
Epoch 510, training loss: 0.6775152683258057 = 0.055622298270463943 + 0.1 * 6.218929767608643
Epoch 510, val loss: 0.9054791331291199
Epoch 520, training loss: 0.6751803159713745 = 0.05280344560742378 + 0.1 * 6.22376823425293
Epoch 520, val loss: 0.9156485795974731
Epoch 530, training loss: 0.6714251637458801 = 0.05015188455581665 + 0.1 * 6.212732791900635
Epoch 530, val loss: 0.9256633520126343
Epoch 540, training loss: 0.6697635650634766 = 0.047646693885326385 + 0.1 * 6.221168518066406
Epoch 540, val loss: 0.935575008392334
Epoch 550, training loss: 0.6666370034217834 = 0.04526889696717262 + 0.1 * 6.213680744171143
Epoch 550, val loss: 0.9452632665634155
Epoch 560, training loss: 0.6628704071044922 = 0.04299825429916382 + 0.1 * 6.198721408843994
Epoch 560, val loss: 0.9548375010490417
Epoch 570, training loss: 0.6621465086936951 = 0.04082229360938072 + 0.1 * 6.213242053985596
Epoch 570, val loss: 0.9642629027366638
Epoch 580, training loss: 0.6595825552940369 = 0.03872990980744362 + 0.1 * 6.208526134490967
Epoch 580, val loss: 0.9734845161437988
Epoch 590, training loss: 0.6556743383407593 = 0.036705780774354935 + 0.1 * 6.189685344696045
Epoch 590, val loss: 0.9826467037200928
Epoch 600, training loss: 0.6538196206092834 = 0.03473528474569321 + 0.1 * 6.19084358215332
Epoch 600, val loss: 0.9916549324989319
Epoch 610, training loss: 0.6514163017272949 = 0.032820284366607666 + 0.1 * 6.185959815979004
Epoch 610, val loss: 1.0004832744598389
Epoch 620, training loss: 0.6495031118392944 = 0.03096388280391693 + 0.1 * 6.185391902923584
Epoch 620, val loss: 1.0093551874160767
Epoch 630, training loss: 0.6487006545066833 = 0.02915285713970661 + 0.1 * 6.1954779624938965
Epoch 630, val loss: 1.0182323455810547
Epoch 640, training loss: 0.6456034779548645 = 0.027379991486668587 + 0.1 * 6.182234764099121
Epoch 640, val loss: 1.0269665718078613
Epoch 650, training loss: 0.6435035467147827 = 0.02565518394112587 + 0.1 * 6.178483009338379
Epoch 650, val loss: 1.0357943773269653
Epoch 660, training loss: 0.6430487632751465 = 0.02399422787129879 + 0.1 * 6.190545558929443
Epoch 660, val loss: 1.0445358753204346
Epoch 670, training loss: 0.6398026347160339 = 0.02242870256304741 + 0.1 * 6.173739433288574
Epoch 670, val loss: 1.0531057119369507
Epoch 680, training loss: 0.6404979228973389 = 0.02097385562956333 + 0.1 * 6.195240497589111
Epoch 680, val loss: 1.061776041984558
Epoch 690, training loss: 0.6367555260658264 = 0.01964418962597847 + 0.1 * 6.171113014221191
Epoch 690, val loss: 1.0702910423278809
Epoch 700, training loss: 0.6352086067199707 = 0.01842574030160904 + 0.1 * 6.16782808303833
Epoch 700, val loss: 1.0788291692733765
Epoch 710, training loss: 0.6344625353813171 = 0.017318887636065483 + 0.1 * 6.171436309814453
Epoch 710, val loss: 1.0870453119277954
Epoch 720, training loss: 0.632673442363739 = 0.016320617869496346 + 0.1 * 6.163527965545654
Epoch 720, val loss: 1.0951151847839355
Epoch 730, training loss: 0.6319406628608704 = 0.015418758615851402 + 0.1 * 6.165218830108643
Epoch 730, val loss: 1.1030925512313843
Epoch 740, training loss: 0.630343496799469 = 0.014607745222747326 + 0.1 * 6.157357692718506
Epoch 740, val loss: 1.110823392868042
Epoch 750, training loss: 0.631145715713501 = 0.013878163881599903 + 0.1 * 6.172675132751465
Epoch 750, val loss: 1.1183245182037354
Epoch 760, training loss: 0.6287748217582703 = 0.013215282000601292 + 0.1 * 6.155595302581787
Epoch 760, val loss: 1.1256606578826904
Epoch 770, training loss: 0.6299213767051697 = 0.012608353979885578 + 0.1 * 6.173130035400391
Epoch 770, val loss: 1.1329350471496582
Epoch 780, training loss: 0.6278438568115234 = 0.012052900157868862 + 0.1 * 6.157909393310547
Epoch 780, val loss: 1.1398744583129883
Epoch 790, training loss: 0.6262290477752686 = 0.011543558910489082 + 0.1 * 6.146854877471924
Epoch 790, val loss: 1.146827220916748
Epoch 800, training loss: 0.6268060207366943 = 0.011073199100792408 + 0.1 * 6.157328128814697
Epoch 800, val loss: 1.1535307168960571
Epoch 810, training loss: 0.625207245349884 = 0.010637613944709301 + 0.1 * 6.14569616317749
Epoch 810, val loss: 1.1599621772766113
Epoch 820, training loss: 0.6262948513031006 = 0.010232621803879738 + 0.1 * 6.1606221199035645
Epoch 820, val loss: 1.1663731336593628
Epoch 830, training loss: 0.624634861946106 = 0.009854758158326149 + 0.1 * 6.147800922393799
Epoch 830, val loss: 1.1725304126739502
Epoch 840, training loss: 0.6238453388214111 = 0.009502456523478031 + 0.1 * 6.143428802490234
Epoch 840, val loss: 1.1786398887634277
Epoch 850, training loss: 0.622832715511322 = 0.009172150865197182 + 0.1 * 6.136605739593506
Epoch 850, val loss: 1.1845322847366333
Epoch 860, training loss: 0.6234108805656433 = 0.008862136863172054 + 0.1 * 6.145487308502197
Epoch 860, val loss: 1.1903187036514282
Epoch 870, training loss: 0.621924638748169 = 0.008570265024900436 + 0.1 * 6.133543968200684
Epoch 870, val loss: 1.1958966255187988
Epoch 880, training loss: 0.624813973903656 = 0.008295624516904354 + 0.1 * 6.1651835441589355
Epoch 880, val loss: 1.201427936553955
Epoch 890, training loss: 0.6219038963317871 = 0.008036660961806774 + 0.1 * 6.138672351837158
Epoch 890, val loss: 1.206786036491394
Epoch 900, training loss: 0.620552122592926 = 0.007793181110173464 + 0.1 * 6.127589225769043
Epoch 900, val loss: 1.2122083902359009
Epoch 910, training loss: 0.620405375957489 = 0.007561667822301388 + 0.1 * 6.12843656539917
Epoch 910, val loss: 1.2175382375717163
Epoch 920, training loss: 0.6211680769920349 = 0.007340847048908472 + 0.1 * 6.138272285461426
Epoch 920, val loss: 1.2226066589355469
Epoch 930, training loss: 0.6202920079231262 = 0.00713133392855525 + 0.1 * 6.131606578826904
Epoch 930, val loss: 1.2276090383529663
Epoch 940, training loss: 0.6195723414421082 = 0.006932475138455629 + 0.1 * 6.126398086547852
Epoch 940, val loss: 1.2325881719589233
Epoch 950, training loss: 0.6187230944633484 = 0.006743841804563999 + 0.1 * 6.119792461395264
Epoch 950, val loss: 1.2375746965408325
Epoch 960, training loss: 0.6192728877067566 = 0.006563741713762283 + 0.1 * 6.127091407775879
Epoch 960, val loss: 1.2423744201660156
Epoch 970, training loss: 0.617882251739502 = 0.006391689646989107 + 0.1 * 6.11490535736084
Epoch 970, val loss: 1.2470215559005737
Epoch 980, training loss: 0.6200414299964905 = 0.006227343343198299 + 0.1 * 6.13814115524292
Epoch 980, val loss: 1.2515825033187866
Epoch 990, training loss: 0.6177722215652466 = 0.006070057395845652 + 0.1 * 6.117021560668945
Epoch 990, val loss: 1.2560524940490723
Epoch 1000, training loss: 0.6177593469619751 = 0.005920689087361097 + 0.1 * 6.118386268615723
Epoch 1000, val loss: 1.2605761289596558
Epoch 1010, training loss: 0.6172972917556763 = 0.005777040030807257 + 0.1 * 6.115201950073242
Epoch 1010, val loss: 1.2648652791976929
Epoch 1020, training loss: 0.6172099113464355 = 0.005640112329274416 + 0.1 * 6.115698337554932
Epoch 1020, val loss: 1.2691597938537598
Epoch 1030, training loss: 0.6177040934562683 = 0.005508876405656338 + 0.1 * 6.121951580047607
Epoch 1030, val loss: 1.2734098434448242
Epoch 1040, training loss: 0.6164316534996033 = 0.005381949245929718 + 0.1 * 6.110496997833252
Epoch 1040, val loss: 1.277540922164917
Epoch 1050, training loss: 0.6162976026535034 = 0.00526067242026329 + 0.1 * 6.110369682312012
Epoch 1050, val loss: 1.2816458940505981
Epoch 1060, training loss: 0.6167912483215332 = 0.005143750924617052 + 0.1 * 6.116474628448486
Epoch 1060, val loss: 1.2857205867767334
Epoch 1070, training loss: 0.6155732870101929 = 0.005031233187764883 + 0.1 * 6.1054205894470215
Epoch 1070, val loss: 1.2897001504898071
Epoch 1080, training loss: 0.6164376139640808 = 0.004923118278384209 + 0.1 * 6.115144729614258
Epoch 1080, val loss: 1.293653130531311
Epoch 1090, training loss: 0.6156805753707886 = 0.00481877475976944 + 0.1 * 6.108617782592773
Epoch 1090, val loss: 1.2975640296936035
Epoch 1100, training loss: 0.6160527467727661 = 0.004718258045613766 + 0.1 * 6.113345146179199
Epoch 1100, val loss: 1.3013124465942383
Epoch 1110, training loss: 0.6152815222740173 = 0.0046217180788517 + 0.1 * 6.106597900390625
Epoch 1110, val loss: 1.3050916194915771
Epoch 1120, training loss: 0.614574670791626 = 0.004529074765741825 + 0.1 * 6.1004557609558105
Epoch 1120, val loss: 1.3089076280593872
Epoch 1130, training loss: 0.6159217953681946 = 0.004439511802047491 + 0.1 * 6.1148223876953125
Epoch 1130, val loss: 1.3126811981201172
Epoch 1140, training loss: 0.6147323250770569 = 0.004352398682385683 + 0.1 * 6.103799343109131
Epoch 1140, val loss: 1.3161890506744385
Epoch 1150, training loss: 0.6144875288009644 = 0.00426898617297411 + 0.1 * 6.102185249328613
Epoch 1150, val loss: 1.3197929859161377
Epoch 1160, training loss: 0.6143766641616821 = 0.004188183695077896 + 0.1 * 6.101884841918945
Epoch 1160, val loss: 1.3232816457748413
Epoch 1170, training loss: 0.6138663291931152 = 0.004110374953597784 + 0.1 * 6.097558975219727
Epoch 1170, val loss: 1.3268029689788818
Epoch 1180, training loss: 0.6153912544250488 = 0.004035192541778088 + 0.1 * 6.113560676574707
Epoch 1180, val loss: 1.3303074836730957
Epoch 1190, training loss: 0.6134573221206665 = 0.0039618052542209625 + 0.1 * 6.094954967498779
Epoch 1190, val loss: 1.333606243133545
Epoch 1200, training loss: 0.6128954291343689 = 0.0038914091419428587 + 0.1 * 6.09004020690918
Epoch 1200, val loss: 1.3369556665420532
Epoch 1210, training loss: 0.614398181438446 = 0.0038230475038290024 + 0.1 * 6.1057515144348145
Epoch 1210, val loss: 1.3402976989746094
Epoch 1220, training loss: 0.6131830215454102 = 0.0037562865763902664 + 0.1 * 6.09426736831665
Epoch 1220, val loss: 1.3434544801712036
Epoch 1230, training loss: 0.6127985715866089 = 0.0036923480220139027 + 0.1 * 6.091062545776367
Epoch 1230, val loss: 1.346643090248108
Epoch 1240, training loss: 0.6136115193367004 = 0.0036303880624473095 + 0.1 * 6.09981107711792
Epoch 1240, val loss: 1.3499313592910767
Epoch 1250, training loss: 0.6137523651123047 = 0.003569918917492032 + 0.1 * 6.101824760437012
Epoch 1250, val loss: 1.3530583381652832
Epoch 1260, training loss: 0.612647533416748 = 0.0035112518817186356 + 0.1 * 6.091362953186035
Epoch 1260, val loss: 1.3560112714767456
Epoch 1270, training loss: 0.6132855415344238 = 0.003454705234616995 + 0.1 * 6.098308563232422
Epoch 1270, val loss: 1.3591275215148926
Epoch 1280, training loss: 0.6123930811882019 = 0.003399499459192157 + 0.1 * 6.089935779571533
Epoch 1280, val loss: 1.362149953842163
Epoch 1290, training loss: 0.6121703386306763 = 0.0033459998667240143 + 0.1 * 6.088243007659912
Epoch 1290, val loss: 1.3651518821716309
Epoch 1300, training loss: 0.6121752262115479 = 0.0032939875964075327 + 0.1 * 6.088812351226807
Epoch 1300, val loss: 1.3680987358093262
Epoch 1310, training loss: 0.6121160984039307 = 0.0032431830186396837 + 0.1 * 6.088728904724121
Epoch 1310, val loss: 1.3709282875061035
Epoch 1320, training loss: 0.6131832003593445 = 0.0031939796172082424 + 0.1 * 6.0998921394348145
Epoch 1320, val loss: 1.3737807273864746
Epoch 1330, training loss: 0.6114469170570374 = 0.00314592313952744 + 0.1 * 6.083009719848633
Epoch 1330, val loss: 1.376538634300232
Epoch 1340, training loss: 0.6118090152740479 = 0.0030997174326330423 + 0.1 * 6.087092876434326
Epoch 1340, val loss: 1.379414677619934
Epoch 1350, training loss: 0.6123163104057312 = 0.003054617205634713 + 0.1 * 6.092616558074951
Epoch 1350, val loss: 1.3822448253631592
Epoch 1360, training loss: 0.6117476224899292 = 0.0030104643665254116 + 0.1 * 6.087371349334717
Epoch 1360, val loss: 1.3848768472671509
Epoch 1370, training loss: 0.6115030646324158 = 0.002967686392366886 + 0.1 * 6.085353374481201
Epoch 1370, val loss: 1.3875839710235596
Epoch 1380, training loss: 0.6111645698547363 = 0.0029259324073791504 + 0.1 * 6.082386016845703
Epoch 1380, val loss: 1.3902654647827148
Epoch 1390, training loss: 0.6117266416549683 = 0.0028851255774497986 + 0.1 * 6.088415145874023
Epoch 1390, val loss: 1.3928947448730469
Epoch 1400, training loss: 0.6109258532524109 = 0.0028452679980546236 + 0.1 * 6.08080530166626
Epoch 1400, val loss: 1.3954553604125977
Epoch 1410, training loss: 0.6127528548240662 = 0.00280655175447464 + 0.1 * 6.099462985992432
Epoch 1410, val loss: 1.3979675769805908
Epoch 1420, training loss: 0.6113766431808472 = 0.002768747042864561 + 0.1 * 6.086078643798828
Epoch 1420, val loss: 1.4005002975463867
Epoch 1430, training loss: 0.6107134819030762 = 0.0027322983369231224 + 0.1 * 6.079812049865723
Epoch 1430, val loss: 1.4030466079711914
Epoch 1440, training loss: 0.6113114953041077 = 0.002696647308766842 + 0.1 * 6.086148262023926
Epoch 1440, val loss: 1.405612587928772
Epoch 1450, training loss: 0.6102519631385803 = 0.002661638194695115 + 0.1 * 6.075902938842773
Epoch 1450, val loss: 1.408045768737793
Epoch 1460, training loss: 0.6108962297439575 = 0.0026276714634150267 + 0.1 * 6.082685470581055
Epoch 1460, val loss: 1.4105321168899536
Epoch 1470, training loss: 0.6101024150848389 = 0.002594191115349531 + 0.1 * 6.075082302093506
Epoch 1470, val loss: 1.4128868579864502
Epoch 1480, training loss: 0.6102174520492554 = 0.002561613917350769 + 0.1 * 6.0765581130981445
Epoch 1480, val loss: 1.4153127670288086
Epoch 1490, training loss: 0.6108905673027039 = 0.002529715420678258 + 0.1 * 6.083608150482178
Epoch 1490, val loss: 1.4176597595214844
Epoch 1500, training loss: 0.6112634539604187 = 0.0024986208882182837 + 0.1 * 6.087647914886475
Epoch 1500, val loss: 1.419950008392334
Epoch 1510, training loss: 0.6100333333015442 = 0.0024682609364390373 + 0.1 * 6.075650215148926
Epoch 1510, val loss: 1.4222650527954102
Epoch 1520, training loss: 0.6095603704452515 = 0.002438893308863044 + 0.1 * 6.07121467590332
Epoch 1520, val loss: 1.4246419668197632
Epoch 1530, training loss: 0.6104684472084045 = 0.002410005545243621 + 0.1 * 6.080584526062012
Epoch 1530, val loss: 1.4269585609436035
Epoch 1540, training loss: 0.6099573373794556 = 0.002381520112976432 + 0.1 * 6.07575798034668
Epoch 1540, val loss: 1.4291534423828125
Epoch 1550, training loss: 0.609681248664856 = 0.0023538193199783564 + 0.1 * 6.073273658752441
Epoch 1550, val loss: 1.4313205480575562
Epoch 1560, training loss: 0.6101972460746765 = 0.0023267294745892286 + 0.1 * 6.078704833984375
Epoch 1560, val loss: 1.4335582256317139
Epoch 1570, training loss: 0.609321653842926 = 0.0023001248482614756 + 0.1 * 6.070215225219727
Epoch 1570, val loss: 1.4356848001480103
Epoch 1580, training loss: 0.6107446551322937 = 0.00227423501200974 + 0.1 * 6.084704399108887
Epoch 1580, val loss: 1.4378700256347656
Epoch 1590, training loss: 0.609321653842926 = 0.0022486720699816942 + 0.1 * 6.070730209350586
Epoch 1590, val loss: 1.439914345741272
Epoch 1600, training loss: 0.6098797917366028 = 0.002224036492407322 + 0.1 * 6.076557636260986
Epoch 1600, val loss: 1.442050814628601
Epoch 1610, training loss: 0.6089653968811035 = 0.0021996083669364452 + 0.1 * 6.067657947540283
Epoch 1610, val loss: 1.444135308265686
Epoch 1620, training loss: 0.6090793013572693 = 0.0021759208757430315 + 0.1 * 6.069033622741699
Epoch 1620, val loss: 1.446250319480896
Epoch 1630, training loss: 0.6089731454849243 = 0.002152608009055257 + 0.1 * 6.068204879760742
Epoch 1630, val loss: 1.448311448097229
Epoch 1640, training loss: 0.6086432337760925 = 0.002129703527316451 + 0.1 * 6.065135478973389
Epoch 1640, val loss: 1.4503629207611084
Epoch 1650, training loss: 0.610000491142273 = 0.0021071797236800194 + 0.1 * 6.078932762145996
Epoch 1650, val loss: 1.4523625373840332
Epoch 1660, training loss: 0.6090131998062134 = 0.002084906678646803 + 0.1 * 6.069282531738281
Epoch 1660, val loss: 1.4542028903961182
Epoch 1670, training loss: 0.6103020310401917 = 0.002063411520794034 + 0.1 * 6.082386016845703
Epoch 1670, val loss: 1.4562314748764038
Epoch 1680, training loss: 0.608466386795044 = 0.0020421689841896296 + 0.1 * 6.064241886138916
Epoch 1680, val loss: 1.4581496715545654
Epoch 1690, training loss: 0.6083950400352478 = 0.0020216857083141804 + 0.1 * 6.0637335777282715
Epoch 1690, val loss: 1.4601622819900513
Epoch 1700, training loss: 0.6093146800994873 = 0.002001553773880005 + 0.1 * 6.073131084442139
Epoch 1700, val loss: 1.4621527194976807
Epoch 1710, training loss: 0.6078100204467773 = 0.001981554552912712 + 0.1 * 6.058284282684326
Epoch 1710, val loss: 1.4640460014343262
Epoch 1720, training loss: 0.6089767217636108 = 0.0019620838575065136 + 0.1 * 6.070146560668945
Epoch 1720, val loss: 1.4659749269485474
Epoch 1730, training loss: 0.6083889603614807 = 0.0019426961662247777 + 0.1 * 6.064462661743164
Epoch 1730, val loss: 1.4677484035491943
Epoch 1740, training loss: 0.6081491708755493 = 0.0019237276865169406 + 0.1 * 6.062254428863525
Epoch 1740, val loss: 1.4695439338684082
Epoch 1750, training loss: 0.6089537143707275 = 0.0019052500138059258 + 0.1 * 6.070484161376953
Epoch 1750, val loss: 1.4714607000350952
Epoch 1760, training loss: 0.6082493662834167 = 0.0018869531340897083 + 0.1 * 6.063623905181885
Epoch 1760, val loss: 1.4732592105865479
Epoch 1770, training loss: 0.6087958216667175 = 0.0018691046861931682 + 0.1 * 6.069267272949219
Epoch 1770, val loss: 1.475077748298645
Epoch 1780, training loss: 0.6077330708503723 = 0.0018516192212700844 + 0.1 * 6.058814525604248
Epoch 1780, val loss: 1.4768644571304321
Epoch 1790, training loss: 0.6077003479003906 = 0.0018345903372392058 + 0.1 * 6.058657646179199
Epoch 1790, val loss: 1.478747010231018
Epoch 1800, training loss: 0.6084461212158203 = 0.0018176871817559004 + 0.1 * 6.0662841796875
Epoch 1800, val loss: 1.4805437326431274
Epoch 1810, training loss: 0.6079318523406982 = 0.0018010647036135197 + 0.1 * 6.061307907104492
Epoch 1810, val loss: 1.4823211431503296
Epoch 1820, training loss: 0.6087232828140259 = 0.0017845731927081943 + 0.1 * 6.069386959075928
Epoch 1820, val loss: 1.4839352369308472
Epoch 1830, training loss: 0.6077197790145874 = 0.0017684610793367028 + 0.1 * 6.059513092041016
Epoch 1830, val loss: 1.4856197834014893
Epoch 1840, training loss: 0.6075038909912109 = 0.0017528978642076254 + 0.1 * 6.057509422302246
Epoch 1840, val loss: 1.48745596408844
Epoch 1850, training loss: 0.6082279086112976 = 0.0017373213777318597 + 0.1 * 6.064906120300293
Epoch 1850, val loss: 1.4891599416732788
Epoch 1860, training loss: 0.6077237725257874 = 0.001722082495689392 + 0.1 * 6.060016632080078
Epoch 1860, val loss: 1.4908024072647095
Epoch 1870, training loss: 0.6091587543487549 = 0.001707213232293725 + 0.1 * 6.074515342712402
Epoch 1870, val loss: 1.4925541877746582
Epoch 1880, training loss: 0.6072245836257935 = 0.0016923951916396618 + 0.1 * 6.05532169342041
Epoch 1880, val loss: 1.494137167930603
Epoch 1890, training loss: 0.6076395511627197 = 0.0016780358273535967 + 0.1 * 6.059615135192871
Epoch 1890, val loss: 1.4958598613739014
Epoch 1900, training loss: 0.607319712638855 = 0.0016638124361634254 + 0.1 * 6.056559085845947
Epoch 1900, val loss: 1.497534155845642
Epoch 1910, training loss: 0.6081328988075256 = 0.0016497722826898098 + 0.1 * 6.064830780029297
Epoch 1910, val loss: 1.499131441116333
Epoch 1920, training loss: 0.6078463196754456 = 0.0016358979046344757 + 0.1 * 6.062104225158691
Epoch 1920, val loss: 1.5006980895996094
Epoch 1930, training loss: 0.6075471043586731 = 0.0016222989652305841 + 0.1 * 6.059247970581055
Epoch 1930, val loss: 1.5022236108779907
Epoch 1940, training loss: 0.6069207191467285 = 0.00160898023750633 + 0.1 * 6.053117275238037
Epoch 1940, val loss: 1.5038727521896362
Epoch 1950, training loss: 0.6076819896697998 = 0.0015959525480866432 + 0.1 * 6.0608601570129395
Epoch 1950, val loss: 1.5055049657821655
Epoch 1960, training loss: 0.6068645715713501 = 0.001582933822646737 + 0.1 * 6.052816390991211
Epoch 1960, val loss: 1.507027268409729
Epoch 1970, training loss: 0.6068848967552185 = 0.0015702852979302406 + 0.1 * 6.053145885467529
Epoch 1970, val loss: 1.508594036102295
Epoch 1980, training loss: 0.6073544025421143 = 0.0015577924204990268 + 0.1 * 6.057966232299805
Epoch 1980, val loss: 1.5101878643035889
Epoch 1990, training loss: 0.6074668169021606 = 0.0015453825471922755 + 0.1 * 6.0592145919799805
Epoch 1990, val loss: 1.5117661952972412
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6753
Flip ASR: 0.6533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.7835981845855713 = 1.9462188482284546 + 0.1 * 8.373793601989746
Epoch 0, val loss: 1.9423412084579468
Epoch 10, training loss: 2.7730958461761475 = 1.9357529878616333 + 0.1 * 8.373429298400879
Epoch 10, val loss: 1.9322693347930908
Epoch 20, training loss: 2.7600529193878174 = 1.9228965044021606 + 0.1 * 8.371563911437988
Epoch 20, val loss: 1.919373869895935
Epoch 30, training loss: 2.741295576095581 = 1.9051342010498047 + 0.1 * 8.361613273620605
Epoch 30, val loss: 1.90103280544281
Epoch 40, training loss: 2.709533214569092 = 1.8793636560440063 + 0.1 * 8.301695823669434
Epoch 40, val loss: 1.874480128288269
Epoch 50, training loss: 2.6353611946105957 = 1.845229148864746 + 0.1 * 7.9013214111328125
Epoch 50, val loss: 1.8410166501998901
Epoch 60, training loss: 2.546929359436035 = 1.8080841302871704 + 0.1 * 7.38845157623291
Epoch 60, val loss: 1.8067233562469482
Epoch 70, training loss: 2.4817988872528076 = 1.7724312543869019 + 0.1 * 7.093677043914795
Epoch 70, val loss: 1.7751643657684326
Epoch 80, training loss: 2.424987316131592 = 1.7354726791381836 + 0.1 * 6.895147323608398
Epoch 80, val loss: 1.7438526153564453
Epoch 90, training loss: 2.368321418762207 = 1.6906929016113281 + 0.1 * 6.7762861251831055
Epoch 90, val loss: 1.7048600912094116
Epoch 100, training loss: 2.3019206523895264 = 1.630393385887146 + 0.1 * 6.715272426605225
Epoch 100, val loss: 1.6537995338439941
Epoch 110, training loss: 2.221569538116455 = 1.5540796518325806 + 0.1 * 6.67489767074585
Epoch 110, val loss: 1.5925565958023071
Epoch 120, training loss: 2.1310760974884033 = 1.4668095111846924 + 0.1 * 6.642666339874268
Epoch 120, val loss: 1.524261474609375
Epoch 130, training loss: 2.0386459827423096 = 1.3766120672225952 + 0.1 * 6.620339393615723
Epoch 130, val loss: 1.4539119005203247
Epoch 140, training loss: 1.9486947059631348 = 1.288446068763733 + 0.1 * 6.602485656738281
Epoch 140, val loss: 1.3891757726669312
Epoch 150, training loss: 1.8609733581542969 = 1.2022507190704346 + 0.1 * 6.587226390838623
Epoch 150, val loss: 1.3274039030075073
Epoch 160, training loss: 1.7743666172027588 = 1.1167947053909302 + 0.1 * 6.575719356536865
Epoch 160, val loss: 1.265289068222046
Epoch 170, training loss: 1.688236951828003 = 1.031058669090271 + 0.1 * 6.57178258895874
Epoch 170, val loss: 1.2021292448043823
Epoch 180, training loss: 1.602149248123169 = 0.9455839991569519 + 0.1 * 6.565652370452881
Epoch 180, val loss: 1.1382136344909668
Epoch 190, training loss: 1.517196536064148 = 0.8610851168632507 + 0.1 * 6.561114311218262
Epoch 190, val loss: 1.0747987031936646
Epoch 200, training loss: 1.4350736141204834 = 0.7793092727661133 + 0.1 * 6.557643890380859
Epoch 200, val loss: 1.0138527154922485
Epoch 210, training loss: 1.357492208480835 = 0.7020394802093506 + 0.1 * 6.554527282714844
Epoch 210, val loss: 0.9573355317115784
Epoch 220, training loss: 1.285780906677246 = 0.6307355165481567 + 0.1 * 6.550454616546631
Epoch 220, val loss: 0.9065842628479004
Epoch 230, training loss: 1.2217737436294556 = 0.5669966340065002 + 0.1 * 6.547770977020264
Epoch 230, val loss: 0.8623789548873901
Epoch 240, training loss: 1.1653082370758057 = 0.5116661787033081 + 0.1 * 6.536421298980713
Epoch 240, val loss: 0.8257965445518494
Epoch 250, training loss: 1.116647720336914 = 0.46382731199264526 + 0.1 * 6.528203964233398
Epoch 250, val loss: 0.796101450920105
Epoch 260, training loss: 1.0735384225845337 = 0.4217977821826935 + 0.1 * 6.517406463623047
Epoch 260, val loss: 0.7723345756530762
Epoch 270, training loss: 1.0366151332855225 = 0.38429903984069824 + 0.1 * 6.523159980773926
Epoch 270, val loss: 0.7538256645202637
Epoch 280, training loss: 1.0000693798065186 = 0.35016512870788574 + 0.1 * 6.499042510986328
Epoch 280, val loss: 0.7395801544189453
Epoch 290, training loss: 0.966304361820221 = 0.3179756999015808 + 0.1 * 6.483286380767822
Epoch 290, val loss: 0.7282130718231201
Epoch 300, training loss: 0.9342809915542603 = 0.28737276792526245 + 0.1 * 6.469082355499268
Epoch 300, val loss: 0.7191455364227295
Epoch 310, training loss: 0.9053999781608582 = 0.25871747732162476 + 0.1 * 6.466825008392334
Epoch 310, val loss: 0.7125100493431091
Epoch 320, training loss: 0.8776918053627014 = 0.23242805898189545 + 0.1 * 6.452637195587158
Epoch 320, val loss: 0.7081674933433533
Epoch 330, training loss: 0.8520838618278503 = 0.20869112014770508 + 0.1 * 6.433927059173584
Epoch 330, val loss: 0.7060450911521912
Epoch 340, training loss: 0.8295361995697021 = 0.18742118775844574 + 0.1 * 6.421149730682373
Epoch 340, val loss: 0.7061036229133606
Epoch 350, training loss: 0.8109093308448792 = 0.16850179433822632 + 0.1 * 6.424075126647949
Epoch 350, val loss: 0.7081758975982666
Epoch 360, training loss: 0.7927625179290771 = 0.1518745720386505 + 0.1 * 6.408878803253174
Epoch 360, val loss: 0.7118756175041199
Epoch 370, training loss: 0.7766835689544678 = 0.13717970252037048 + 0.1 * 6.395038604736328
Epoch 370, val loss: 0.7171306610107422
Epoch 380, training loss: 0.7653610110282898 = 0.12416057288646698 + 0.1 * 6.412003993988037
Epoch 380, val loss: 0.7236266136169434
Epoch 390, training loss: 0.75104159116745 = 0.11273490637540817 + 0.1 * 6.383066654205322
Epoch 390, val loss: 0.7309582829475403
Epoch 400, training loss: 0.7398398518562317 = 0.1026209369301796 + 0.1 * 6.372189044952393
Epoch 400, val loss: 0.7390677332878113
Epoch 410, training loss: 0.7314436435699463 = 0.09362340718507767 + 0.1 * 6.378202438354492
Epoch 410, val loss: 0.7477554082870483
Epoch 420, training loss: 0.721218466758728 = 0.08565157651901245 + 0.1 * 6.355669021606445
Epoch 420, val loss: 0.7567266821861267
Epoch 430, training loss: 0.7137736678123474 = 0.07853412628173828 + 0.1 * 6.352395057678223
Epoch 430, val loss: 0.7659236192703247
Epoch 440, training loss: 0.7062506079673767 = 0.07217419147491455 + 0.1 * 6.340764045715332
Epoch 440, val loss: 0.7752913236618042
Epoch 450, training loss: 0.7004961371421814 = 0.06647507846355438 + 0.1 * 6.340210437774658
Epoch 450, val loss: 0.7847601771354675
Epoch 460, training loss: 0.6951344013214111 = 0.061376843601465225 + 0.1 * 6.3375749588012695
Epoch 460, val loss: 0.7940062284469604
Epoch 470, training loss: 0.6887848377227783 = 0.0568135641515255 + 0.1 * 6.3197126388549805
Epoch 470, val loss: 0.8033073544502258
Epoch 480, training loss: 0.6840646266937256 = 0.0527103915810585 + 0.1 * 6.313541889190674
Epoch 480, val loss: 0.8122876286506653
Epoch 490, training loss: 0.6801488399505615 = 0.04902268573641777 + 0.1 * 6.3112616539001465
Epoch 490, val loss: 0.8212482333183289
Epoch 500, training loss: 0.6758087873458862 = 0.045678406953811646 + 0.1 * 6.301303863525391
Epoch 500, val loss: 0.8299662470817566
Epoch 510, training loss: 0.6749104261398315 = 0.042640820145606995 + 0.1 * 6.322696208953857
Epoch 510, val loss: 0.8384302258491516
Epoch 520, training loss: 0.6696454286575317 = 0.03989564627408981 + 0.1 * 6.297497749328613
Epoch 520, val loss: 0.8467724919319153
Epoch 530, training loss: 0.6666532754898071 = 0.03739359974861145 + 0.1 * 6.292596817016602
Epoch 530, val loss: 0.8549057841300964
Epoch 540, training loss: 0.6631836891174316 = 0.03510725498199463 + 0.1 * 6.280764102935791
Epoch 540, val loss: 0.8627970218658447
Epoch 550, training loss: 0.6609033346176147 = 0.03301490843296051 + 0.1 * 6.278884410858154
Epoch 550, val loss: 0.8705720901489258
Epoch 560, training loss: 0.6580261588096619 = 0.03109588287770748 + 0.1 * 6.269302845001221
Epoch 560, val loss: 0.8780560493469238
Epoch 570, training loss: 0.6560521125793457 = 0.029337216168642044 + 0.1 * 6.267148494720459
Epoch 570, val loss: 0.8855503797531128
Epoch 580, training loss: 0.6551783680915833 = 0.02772003598511219 + 0.1 * 6.274583339691162
Epoch 580, val loss: 0.8925328254699707
Epoch 590, training loss: 0.6524884700775146 = 0.026238076388835907 + 0.1 * 6.262503623962402
Epoch 590, val loss: 0.8996442556381226
Epoch 600, training loss: 0.6513881683349609 = 0.024867888540029526 + 0.1 * 6.265202522277832
Epoch 600, val loss: 0.9064752459526062
Epoch 610, training loss: 0.6489159464836121 = 0.023603038862347603 + 0.1 * 6.253129005432129
Epoch 610, val loss: 0.9131653904914856
Epoch 620, training loss: 0.6468058228492737 = 0.0224311500787735 + 0.1 * 6.243746757507324
Epoch 620, val loss: 0.9197001457214355
Epoch 630, training loss: 0.6459449529647827 = 0.021343300119042397 + 0.1 * 6.246016502380371
Epoch 630, val loss: 0.9259686470031738
Epoch 640, training loss: 0.6447714567184448 = 0.02033737488090992 + 0.1 * 6.244340419769287
Epoch 640, val loss: 0.9322301745414734
Epoch 650, training loss: 0.6434593796730042 = 0.01940048672258854 + 0.1 * 6.240589141845703
Epoch 650, val loss: 0.9383171200752258
Epoch 660, training loss: 0.642564058303833 = 0.018528267741203308 + 0.1 * 6.240357875823975
Epoch 660, val loss: 0.9441762566566467
Epoch 670, training loss: 0.6405161619186401 = 0.01771731860935688 + 0.1 * 6.227988243103027
Epoch 670, val loss: 0.9500694274902344
Epoch 680, training loss: 0.6399129629135132 = 0.016957895830273628 + 0.1 * 6.229550838470459
Epoch 680, val loss: 0.9557011127471924
Epoch 690, training loss: 0.6391220688819885 = 0.016249408945441246 + 0.1 * 6.228726387023926
Epoch 690, val loss: 0.9613059759140015
Epoch 700, training loss: 0.6372279524803162 = 0.01558421179652214 + 0.1 * 6.216436862945557
Epoch 700, val loss: 0.9666362404823303
Epoch 710, training loss: 0.6366914510726929 = 0.014963301829993725 + 0.1 * 6.217280864715576
Epoch 710, val loss: 0.9720718860626221
Epoch 720, training loss: 0.6357568502426147 = 0.014378507621586323 + 0.1 * 6.213783264160156
Epoch 720, val loss: 0.9773015975952148
Epoch 730, training loss: 0.6348525285720825 = 0.013829062692821026 + 0.1 * 6.210234642028809
Epoch 730, val loss: 0.9824321866035461
Epoch 740, training loss: 0.6351596117019653 = 0.013311605900526047 + 0.1 * 6.218479633331299
Epoch 740, val loss: 0.9873877763748169
Epoch 750, training loss: 0.6331287622451782 = 0.012824805453419685 + 0.1 * 6.203039169311523
Epoch 750, val loss: 0.9922173619270325
Epoch 760, training loss: 0.6323896050453186 = 0.012368548661470413 + 0.1 * 6.200210094451904
Epoch 760, val loss: 0.9971444606781006
Epoch 770, training loss: 0.6322368383407593 = 0.011936645023524761 + 0.1 * 6.203001499176025
Epoch 770, val loss: 1.0019042491912842
Epoch 780, training loss: 0.6313753724098206 = 0.011527192778885365 + 0.1 * 6.198482036590576
Epoch 780, val loss: 1.006449818611145
Epoch 790, training loss: 0.630165159702301 = 0.011141037568449974 + 0.1 * 6.190240859985352
Epoch 790, val loss: 1.0109819173812866
Epoch 800, training loss: 0.6297434568405151 = 0.010774731636047363 + 0.1 * 6.189687252044678
Epoch 800, val loss: 1.015348196029663
Epoch 810, training loss: 0.6289275884628296 = 0.010428492911159992 + 0.1 * 6.184990882873535
Epoch 810, val loss: 1.0196824073791504
Epoch 820, training loss: 0.6285156011581421 = 0.010099992156028748 + 0.1 * 6.1841559410095215
Epoch 820, val loss: 1.0239821672439575
Epoch 830, training loss: 0.6286224126815796 = 0.009786094538867474 + 0.1 * 6.188363552093506
Epoch 830, val loss: 1.0280637741088867
Epoch 840, training loss: 0.6278699040412903 = 0.009489912539720535 + 0.1 * 6.183800220489502
Epoch 840, val loss: 1.0322325229644775
Epoch 850, training loss: 0.6269092559814453 = 0.009207677096128464 + 0.1 * 6.177015781402588
Epoch 850, val loss: 1.0362492799758911
Epoch 860, training loss: 0.6265585422515869 = 0.008940167725086212 + 0.1 * 6.176183700561523
Epoch 860, val loss: 1.0403238534927368
Epoch 870, training loss: 0.626976490020752 = 0.008684452623128891 + 0.1 * 6.182920455932617
Epoch 870, val loss: 1.0442650318145752
Epoch 880, training loss: 0.626783013343811 = 0.008440333418548107 + 0.1 * 6.183426856994629
Epoch 880, val loss: 1.0480562448501587
Epoch 890, training loss: 0.624698281288147 = 0.008207735605537891 + 0.1 * 6.164905548095703
Epoch 890, val loss: 1.0517743825912476
Epoch 900, training loss: 0.624741792678833 = 0.007986830547451973 + 0.1 * 6.1675496101379395
Epoch 900, val loss: 1.0556355714797974
Epoch 910, training loss: 0.6239235997200012 = 0.007774238474667072 + 0.1 * 6.16149377822876
Epoch 910, val loss: 1.0592759847640991
Epoch 920, training loss: 0.6249407529830933 = 0.007571429945528507 + 0.1 * 6.17369270324707
Epoch 920, val loss: 1.0628502368927002
Epoch 930, training loss: 0.6244611740112305 = 0.007376338820904493 + 0.1 * 6.170848369598389
Epoch 930, val loss: 1.0662373304367065
Epoch 940, training loss: 0.6228656768798828 = 0.0071907625533640385 + 0.1 * 6.156749248504639
Epoch 940, val loss: 1.0697144269943237
Epoch 950, training loss: 0.6226600408554077 = 0.007012946996837854 + 0.1 * 6.156470775604248
Epoch 950, val loss: 1.0731866359710693
Epoch 960, training loss: 0.6222810745239258 = 0.006841802038252354 + 0.1 * 6.154392719268799
Epoch 960, val loss: 1.076493263244629
Epoch 970, training loss: 0.6219271421432495 = 0.00667799636721611 + 0.1 * 6.152491569519043
Epoch 970, val loss: 1.079780101776123
Epoch 980, training loss: 0.6211315393447876 = 0.0065199751406908035 + 0.1 * 6.146115779876709
Epoch 980, val loss: 1.082920789718628
Epoch 990, training loss: 0.6226444840431213 = 0.006368291098624468 + 0.1 * 6.162761688232422
Epoch 990, val loss: 1.086044192314148
Epoch 1000, training loss: 0.6213061809539795 = 0.006222318392246962 + 0.1 * 6.150838375091553
Epoch 1000, val loss: 1.0891457796096802
Epoch 1010, training loss: 0.6219338178634644 = 0.006081787869334221 + 0.1 * 6.158520221710205
Epoch 1010, val loss: 1.09213125705719
Epoch 1020, training loss: 0.6198440790176392 = 0.005946884397417307 + 0.1 * 6.13897180557251
Epoch 1020, val loss: 1.0951282978057861
Epoch 1030, training loss: 0.6203793883323669 = 0.0058178952895104885 + 0.1 * 6.1456146240234375
Epoch 1030, val loss: 1.098230004310608
Epoch 1040, training loss: 0.6199102401733398 = 0.005692333448678255 + 0.1 * 6.142178535461426
Epoch 1040, val loss: 1.1011314392089844
Epoch 1050, training loss: 0.6200437545776367 = 0.005571161862462759 + 0.1 * 6.144725799560547
Epoch 1050, val loss: 1.1039780378341675
Epoch 1060, training loss: 0.6191736459732056 = 0.005454822909086943 + 0.1 * 6.137187957763672
Epoch 1060, val loss: 1.1068276166915894
Epoch 1070, training loss: 0.618864893913269 = 0.005342898424714804 + 0.1 * 6.135220050811768
Epoch 1070, val loss: 1.1097370386123657
Epoch 1080, training loss: 0.6196869611740112 = 0.005234665237367153 + 0.1 * 6.144522666931152
Epoch 1080, val loss: 1.1126219034194946
Epoch 1090, training loss: 0.6183492541313171 = 0.0051294718869030476 + 0.1 * 6.132197856903076
Epoch 1090, val loss: 1.115315556526184
Epoch 1100, training loss: 0.6190999150276184 = 0.005028863437473774 + 0.1 * 6.140710353851318
Epoch 1100, val loss: 1.1181327104568481
Epoch 1110, training loss: 0.6186320185661316 = 0.004930540919303894 + 0.1 * 6.137014389038086
Epoch 1110, val loss: 1.120729684829712
Epoch 1120, training loss: 0.6182978749275208 = 0.004836575128138065 + 0.1 * 6.134613037109375
Epoch 1120, val loss: 1.1234310865402222
Epoch 1130, training loss: 0.6180646419525146 = 0.004745559301227331 + 0.1 * 6.133191108703613
Epoch 1130, val loss: 1.1261072158813477
Epoch 1140, training loss: 0.6174760460853577 = 0.004656687844544649 + 0.1 * 6.128193378448486
Epoch 1140, val loss: 1.128633737564087
Epoch 1150, training loss: 0.61677485704422 = 0.004571054596453905 + 0.1 * 6.122037887573242
Epoch 1150, val loss: 1.1311286687850952
Epoch 1160, training loss: 0.6175023913383484 = 0.004488279111683369 + 0.1 * 6.130140781402588
Epoch 1160, val loss: 1.1336771249771118
Epoch 1170, training loss: 0.6171773672103882 = 0.004407227039337158 + 0.1 * 6.127701282501221
Epoch 1170, val loss: 1.135986089706421
Epoch 1180, training loss: 0.616604208946228 = 0.004329446703195572 + 0.1 * 6.122747421264648
Epoch 1180, val loss: 1.138418436050415
Epoch 1190, training loss: 0.617423415184021 = 0.0042542265728116035 + 0.1 * 6.131691932678223
Epoch 1190, val loss: 1.140847086906433
Epoch 1200, training loss: 0.6158251762390137 = 0.004180984105914831 + 0.1 * 6.11644172668457
Epoch 1200, val loss: 1.1432158946990967
Epoch 1210, training loss: 0.6163997054100037 = 0.004110080190002918 + 0.1 * 6.12289571762085
Epoch 1210, val loss: 1.145591139793396
Epoch 1220, training loss: 0.6175265312194824 = 0.004040864761918783 + 0.1 * 6.134856700897217
Epoch 1220, val loss: 1.1478826999664307
Epoch 1230, training loss: 0.6158590316772461 = 0.003973523620516062 + 0.1 * 6.118854999542236
Epoch 1230, val loss: 1.150072693824768
Epoch 1240, training loss: 0.6156229972839355 = 0.003908707294613123 + 0.1 * 6.117143154144287
Epoch 1240, val loss: 1.1523737907409668
Epoch 1250, training loss: 0.6153958439826965 = 0.0038453314919024706 + 0.1 * 6.115505218505859
Epoch 1250, val loss: 1.1545817852020264
Epoch 1260, training loss: 0.6155080199241638 = 0.003784035099670291 + 0.1 * 6.117239475250244
Epoch 1260, val loss: 1.1568100452423096
Epoch 1270, training loss: 0.615883469581604 = 0.0037242677062749863 + 0.1 * 6.121592044830322
Epoch 1270, val loss: 1.1589637994766235
Epoch 1280, training loss: 0.6162770390510559 = 0.0036662607453763485 + 0.1 * 6.126107692718506
Epoch 1280, val loss: 1.1611577272415161
Epoch 1290, training loss: 0.6145713925361633 = 0.003609756473451853 + 0.1 * 6.109615802764893
Epoch 1290, val loss: 1.163267731666565
Epoch 1300, training loss: 0.6144587993621826 = 0.003555301111191511 + 0.1 * 6.109035015106201
Epoch 1300, val loss: 1.1655070781707764
Epoch 1310, training loss: 0.614703357219696 = 0.003501676023006439 + 0.1 * 6.1120171546936035
Epoch 1310, val loss: 1.1675922870635986
Epoch 1320, training loss: 0.6146936416625977 = 0.003449411829933524 + 0.1 * 6.1124420166015625
Epoch 1320, val loss: 1.1696234941482544
Epoch 1330, training loss: 0.6144524216651917 = 0.003398795612156391 + 0.1 * 6.110536098480225
Epoch 1330, val loss: 1.1716545820236206
Epoch 1340, training loss: 0.6146853566169739 = 0.0033497135154902935 + 0.1 * 6.113356113433838
Epoch 1340, val loss: 1.1737533807754517
Epoch 1350, training loss: 0.613778293132782 = 0.003301632357761264 + 0.1 * 6.104766368865967
Epoch 1350, val loss: 1.175776481628418
Epoch 1360, training loss: 0.6145164966583252 = 0.0032547106966376305 + 0.1 * 6.1126179695129395
Epoch 1360, val loss: 1.1777572631835938
Epoch 1370, training loss: 0.6136557459831238 = 0.0032089976593852043 + 0.1 * 6.104467391967773
Epoch 1370, val loss: 1.1796940565109253
Epoch 1380, training loss: 0.616241991519928 = 0.0031645200215280056 + 0.1 * 6.13077449798584
Epoch 1380, val loss: 1.1816195249557495
Epoch 1390, training loss: 0.6140344142913818 = 0.0031207669526338577 + 0.1 * 6.10913610458374
Epoch 1390, val loss: 1.1835505962371826
Epoch 1400, training loss: 0.6134932041168213 = 0.0030787771102041006 + 0.1 * 6.104144096374512
Epoch 1400, val loss: 1.1855098009109497
Epoch 1410, training loss: 0.6145114302635193 = 0.0030373039189726114 + 0.1 * 6.11474084854126
Epoch 1410, val loss: 1.187409520149231
Epoch 1420, training loss: 0.6128768920898438 = 0.002996783936396241 + 0.1 * 6.0988006591796875
Epoch 1420, val loss: 1.1892218589782715
Epoch 1430, training loss: 0.6145215034484863 = 0.0029577738605439663 + 0.1 * 6.115637302398682
Epoch 1430, val loss: 1.1911565065383911
Epoch 1440, training loss: 0.6129990816116333 = 0.0029190971981734037 + 0.1 * 6.100800037384033
Epoch 1440, val loss: 1.1929007768630981
Epoch 1450, training loss: 0.6125413775444031 = 0.00288183125667274 + 0.1 * 6.096595287322998
Epoch 1450, val loss: 1.1947929859161377
Epoch 1460, training loss: 0.6134791374206543 = 0.0028453099075704813 + 0.1 * 6.1063385009765625
Epoch 1460, val loss: 1.196649193763733
Epoch 1470, training loss: 0.6122977137565613 = 0.002808997640386224 + 0.1 * 6.0948872566223145
Epoch 1470, val loss: 1.1983202695846558
Epoch 1480, training loss: 0.613287091255188 = 0.0027740602381527424 + 0.1 * 6.105130195617676
Epoch 1480, val loss: 1.200076937675476
Epoch 1490, training loss: 0.6124684810638428 = 0.0027397230733186007 + 0.1 * 6.097287654876709
Epoch 1490, val loss: 1.201865553855896
Epoch 1500, training loss: 0.6121346354484558 = 0.002706316765397787 + 0.1 * 6.094283580780029
Epoch 1500, val loss: 1.203670620918274
Epoch 1510, training loss: 0.6131076216697693 = 0.002673582173883915 + 0.1 * 6.104340553283691
Epoch 1510, val loss: 1.2054656744003296
Epoch 1520, training loss: 0.6124352812767029 = 0.0026412014849483967 + 0.1 * 6.097940921783447
Epoch 1520, val loss: 1.2071399688720703
Epoch 1530, training loss: 0.6130086779594421 = 0.002609764924272895 + 0.1 * 6.1039886474609375
Epoch 1530, val loss: 1.2088123559951782
Epoch 1540, training loss: 0.6116301417350769 = 0.002579099964350462 + 0.1 * 6.090510368347168
Epoch 1540, val loss: 1.210525393486023
Epoch 1550, training loss: 0.6111095547676086 = 0.0025492734275758266 + 0.1 * 6.085602760314941
Epoch 1550, val loss: 1.2122989892959595
Epoch 1560, training loss: 0.612914502620697 = 0.00251984060741961 + 0.1 * 6.103946208953857
Epoch 1560, val loss: 1.213996171951294
Epoch 1570, training loss: 0.6118188500404358 = 0.0024907435290515423 + 0.1 * 6.093280792236328
Epoch 1570, val loss: 1.215606927871704
Epoch 1580, training loss: 0.611562967300415 = 0.0024623156059533358 + 0.1 * 6.091006278991699
Epoch 1580, val loss: 1.2171367406845093
Epoch 1590, training loss: 0.6117992401123047 = 0.0024347365833818913 + 0.1 * 6.093645095825195
Epoch 1590, val loss: 1.2187808752059937
Epoch 1600, training loss: 0.6112350821495056 = 0.002407558262348175 + 0.1 * 6.08827543258667
Epoch 1600, val loss: 1.2203739881515503
Epoch 1610, training loss: 0.610963761806488 = 0.002381090074777603 + 0.1 * 6.085826396942139
Epoch 1610, val loss: 1.2219938039779663
Epoch 1620, training loss: 0.6111882328987122 = 0.0023552337661385536 + 0.1 * 6.088330268859863
Epoch 1620, val loss: 1.2236363887786865
Epoch 1630, training loss: 0.6114619970321655 = 0.0023296598810702562 + 0.1 * 6.091323375701904
Epoch 1630, val loss: 1.2252053022384644
Epoch 1640, training loss: 0.6109485626220703 = 0.0023045563139021397 + 0.1 * 6.086440086364746
Epoch 1640, val loss: 1.2267708778381348
Epoch 1650, training loss: 0.6106829643249512 = 0.0022800324950367212 + 0.1 * 6.084029197692871
Epoch 1650, val loss: 1.2283391952514648
Epoch 1660, training loss: 0.6124085187911987 = 0.002255915431305766 + 0.1 * 6.101525783538818
Epoch 1660, val loss: 1.2298524379730225
Epoch 1670, training loss: 0.6106276512145996 = 0.0022321606520563364 + 0.1 * 6.08395528793335
Epoch 1670, val loss: 1.2312253713607788
Epoch 1680, training loss: 0.6106666922569275 = 0.0022093006409704685 + 0.1 * 6.084573745727539
Epoch 1680, val loss: 1.2328336238861084
Epoch 1690, training loss: 0.6103218793869019 = 0.0021867111790925264 + 0.1 * 6.081351280212402
Epoch 1690, val loss: 1.2343947887420654
Epoch 1700, training loss: 0.6108978390693665 = 0.0021642728243023157 + 0.1 * 6.08733606338501
Epoch 1700, val loss: 1.2358850240707397
Epoch 1710, training loss: 0.6112704277038574 = 0.002142318058758974 + 0.1 * 6.091280937194824
Epoch 1710, val loss: 1.2372822761535645
Epoch 1720, training loss: 0.6100999116897583 = 0.002120879478752613 + 0.1 * 6.0797905921936035
Epoch 1720, val loss: 1.2387157678604126
Epoch 1730, training loss: 0.6110369563102722 = 0.0021000176202505827 + 0.1 * 6.089369297027588
Epoch 1730, val loss: 1.2402058839797974
Epoch 1740, training loss: 0.6102654933929443 = 0.0020792209543287754 + 0.1 * 6.081862449645996
Epoch 1740, val loss: 1.2416458129882812
Epoch 1750, training loss: 0.6097880601882935 = 0.002059072256088257 + 0.1 * 6.077290058135986
Epoch 1750, val loss: 1.243072509765625
Epoch 1760, training loss: 0.6106062531471252 = 0.00203939457423985 + 0.1 * 6.085668563842773
Epoch 1760, val loss: 1.2445802688598633
Epoch 1770, training loss: 0.6103442907333374 = 0.002019766950979829 + 0.1 * 6.083245277404785
Epoch 1770, val loss: 1.2459850311279297
Epoch 1780, training loss: 0.610090434551239 = 0.002000578911975026 + 0.1 * 6.080898284912109
Epoch 1780, val loss: 1.2473827600479126
Epoch 1790, training loss: 0.609415590763092 = 0.0019817615393549204 + 0.1 * 6.074338436126709
Epoch 1790, val loss: 1.2488032579421997
Epoch 1800, training loss: 0.6100713610649109 = 0.001963316695764661 + 0.1 * 6.081080436706543
Epoch 1800, val loss: 1.2502354383468628
Epoch 1810, training loss: 0.609856128692627 = 0.0019450881518423557 + 0.1 * 6.079110622406006
Epoch 1810, val loss: 1.2516480684280396
Epoch 1820, training loss: 0.6095664501190186 = 0.0019271827768534422 + 0.1 * 6.07639217376709
Epoch 1820, val loss: 1.2530243396759033
Epoch 1830, training loss: 0.610409140586853 = 0.001909601385705173 + 0.1 * 6.084995269775391
Epoch 1830, val loss: 1.254423975944519
Epoch 1840, training loss: 0.6100441217422485 = 0.0018921983428299427 + 0.1 * 6.08151912689209
Epoch 1840, val loss: 1.2557508945465088
Epoch 1850, training loss: 0.609635055065155 = 0.0018751895986497402 + 0.1 * 6.077598571777344
Epoch 1850, val loss: 1.2571302652359009
Epoch 1860, training loss: 0.6090065240859985 = 0.001858478062786162 + 0.1 * 6.0714802742004395
Epoch 1860, val loss: 1.25846266746521
Epoch 1870, training loss: 0.6093799471855164 = 0.0018423101864755154 + 0.1 * 6.075376033782959
Epoch 1870, val loss: 1.259905457496643
Epoch 1880, training loss: 0.6097858548164368 = 0.0018263220554217696 + 0.1 * 6.07959508895874
Epoch 1880, val loss: 1.2613773345947266
Epoch 1890, training loss: 0.6088072061538696 = 0.0018102478934451938 + 0.1 * 6.069969177246094
Epoch 1890, val loss: 1.2626285552978516
Epoch 1900, training loss: 0.6096671223640442 = 0.0017946711741387844 + 0.1 * 6.078724384307861
Epoch 1900, val loss: 1.263975739479065
Epoch 1910, training loss: 0.6092299222946167 = 0.0017793008591979742 + 0.1 * 6.074505805969238
Epoch 1910, val loss: 1.2653136253356934
Epoch 1920, training loss: 0.6094273924827576 = 0.0017642115708440542 + 0.1 * 6.076631546020508
Epoch 1920, val loss: 1.2666172981262207
Epoch 1930, training loss: 0.6082638502120972 = 0.001749393530189991 + 0.1 * 6.0651445388793945
Epoch 1930, val loss: 1.2679814100265503
Epoch 1940, training loss: 0.609809160232544 = 0.0017348532564938068 + 0.1 * 6.080742835998535
Epoch 1940, val loss: 1.2693252563476562
Epoch 1950, training loss: 0.6089717745780945 = 0.0017202383605763316 + 0.1 * 6.07251501083374
Epoch 1950, val loss: 1.2705556154251099
Epoch 1960, training loss: 0.6095735430717468 = 0.0017061032122001052 + 0.1 * 6.078673839569092
Epoch 1960, val loss: 1.2718535661697388
Epoch 1970, training loss: 0.6081293821334839 = 0.001692159567028284 + 0.1 * 6.0643720626831055
Epoch 1970, val loss: 1.2730863094329834
Epoch 1980, training loss: 0.6100625395774841 = 0.0016784925246611238 + 0.1 * 6.083840847015381
Epoch 1980, val loss: 1.2743762731552124
Epoch 1990, training loss: 0.6084896326065063 = 0.0016649002209305763 + 0.1 * 6.068247318267822
Epoch 1990, val loss: 1.2756048440933228
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
