Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.00537109375 = 1.102010726928711 + 100.0 * 10.35903263092041
Epoch 0, val loss: 1.1017861366271973
Epoch 10, training loss: 1036.5455322265625 = 1.0915964841842651 + 100.0 * 10.35453987121582
Epoch 10, val loss: 1.0909758806228638
Epoch 20, training loss: 1029.4130859375 = 1.0776262283325195 + 100.0 * 10.283354759216309
Epoch 20, val loss: 1.0767227411270142
Epoch 30, training loss: 977.3160400390625 = 1.0631599426269531 + 100.0 * 9.762528419494629
Epoch 30, val loss: 1.0619425773620605
Epoch 40, training loss: 948.6112060546875 = 1.0486892461776733 + 100.0 * 9.475625038146973
Epoch 40, val loss: 1.047440528869629
Epoch 50, training loss: 939.490966796875 = 1.0325512886047363 + 100.0 * 9.384584426879883
Epoch 50, val loss: 1.0313605070114136
Epoch 60, training loss: 937.33544921875 = 1.0181889533996582 + 100.0 * 9.36317253112793
Epoch 60, val loss: 1.0175280570983887
Epoch 70, training loss: 935.4737548828125 = 1.0075875520706177 + 100.0 * 9.344661712646484
Epoch 70, val loss: 1.007262945175171
Epoch 80, training loss: 933.0829467773438 = 1.000091791152954 + 100.0 * 9.320828437805176
Epoch 80, val loss: 0.9997274875640869
Epoch 90, training loss: 929.5472412109375 = 0.9932595491409302 + 100.0 * 9.285539627075195
Epoch 90, val loss: 0.9927870035171509
Epoch 100, training loss: 924.5302124023438 = 0.9855823516845703 + 100.0 * 9.235445976257324
Epoch 100, val loss: 0.985262930393219
Epoch 110, training loss: 921.24560546875 = 0.9763320684432983 + 100.0 * 9.202692985534668
Epoch 110, val loss: 0.9762052893638611
Epoch 120, training loss: 919.372802734375 = 0.9650728702545166 + 100.0 * 9.184077262878418
Epoch 120, val loss: 0.9652076363563538
Epoch 130, training loss: 918.1065063476562 = 0.9544855952262878 + 100.0 * 9.171520233154297
Epoch 130, val loss: 0.9549163579940796
Epoch 140, training loss: 916.783203125 = 0.9457894563674927 + 100.0 * 9.158373832702637
Epoch 140, val loss: 0.9467725157737732
Epoch 150, training loss: 915.4655151367188 = 0.9381964802742004 + 100.0 * 9.145273208618164
Epoch 150, val loss: 0.9391041994094849
Epoch 160, training loss: 914.3737182617188 = 0.9304431080818176 + 100.0 * 9.134432792663574
Epoch 160, val loss: 0.9320403337478638
Epoch 170, training loss: 913.4030151367188 = 0.9233438968658447 + 100.0 * 9.124796867370605
Epoch 170, val loss: 0.9252526164054871
Epoch 180, training loss: 912.2963256835938 = 0.9155784249305725 + 100.0 * 9.113807678222656
Epoch 180, val loss: 0.9177574515342712
Epoch 190, training loss: 911.4842529296875 = 0.9072379469871521 + 100.0 * 9.105770111083984
Epoch 190, val loss: 0.9096079468727112
Epoch 200, training loss: 911.0114135742188 = 0.8977625966072083 + 100.0 * 9.101136207580566
Epoch 200, val loss: 0.9005096554756165
Epoch 210, training loss: 910.3567504882812 = 0.8872882127761841 + 100.0 * 9.094695091247559
Epoch 210, val loss: 0.8904463052749634
Epoch 220, training loss: 909.823486328125 = 0.87660813331604 + 100.0 * 9.089468955993652
Epoch 220, val loss: 0.880402684211731
Epoch 230, training loss: 909.3484497070312 = 0.8653877973556519 + 100.0 * 9.084830284118652
Epoch 230, val loss: 0.8696177005767822
Epoch 240, training loss: 908.929443359375 = 0.853667676448822 + 100.0 * 9.080758094787598
Epoch 240, val loss: 0.8584916591644287
Epoch 250, training loss: 908.49267578125 = 0.8416412472724915 + 100.0 * 9.076510429382324
Epoch 250, val loss: 0.8472112417221069
Epoch 260, training loss: 908.2164916992188 = 0.8289042115211487 + 100.0 * 9.073875427246094
Epoch 260, val loss: 0.8351340889930725
Epoch 270, training loss: 907.8974609375 = 0.8158624172210693 + 100.0 * 9.070816040039062
Epoch 270, val loss: 0.8228881359100342
Epoch 280, training loss: 907.4813842773438 = 0.8031796813011169 + 100.0 * 9.066781997680664
Epoch 280, val loss: 0.8110604286193848
Epoch 290, training loss: 907.619140625 = 0.7902542352676392 + 100.0 * 9.068288803100586
Epoch 290, val loss: 0.7991608381271362
Epoch 300, training loss: 907.0250244140625 = 0.776231050491333 + 100.0 * 9.062487602233887
Epoch 300, val loss: 0.7859033346176147
Epoch 310, training loss: 906.6986694335938 = 0.7626062631607056 + 100.0 * 9.05936050415039
Epoch 310, val loss: 0.773061215877533
Epoch 320, training loss: 906.457763671875 = 0.7490772008895874 + 100.0 * 9.057086944580078
Epoch 320, val loss: 0.7603998780250549
Epoch 330, training loss: 906.3328857421875 = 0.7348284125328064 + 100.0 * 9.055980682373047
Epoch 330, val loss: 0.7471125721931458
Epoch 340, training loss: 906.1466064453125 = 0.7201935052871704 + 100.0 * 9.054264068603516
Epoch 340, val loss: 0.7336078882217407
Epoch 350, training loss: 905.992431640625 = 0.7062561511993408 + 100.0 * 9.052862167358398
Epoch 350, val loss: 0.7206835150718689
Epoch 360, training loss: 905.95068359375 = 0.6921738386154175 + 100.0 * 9.052584648132324
Epoch 360, val loss: 0.7075067758560181
Epoch 370, training loss: 905.7587280273438 = 0.6779255867004395 + 100.0 * 9.05080795288086
Epoch 370, val loss: 0.6944888830184937
Epoch 380, training loss: 905.6029052734375 = 0.6646392941474915 + 100.0 * 9.049383163452148
Epoch 380, val loss: 0.6823810338973999
Epoch 390, training loss: 905.5079956054688 = 0.6516876220703125 + 100.0 * 9.048563003540039
Epoch 390, val loss: 0.6706666350364685
Epoch 400, training loss: 905.4703979492188 = 0.6384849548339844 + 100.0 * 9.048318862915039
Epoch 400, val loss: 0.6582512855529785
Epoch 410, training loss: 905.2643432617188 = 0.625770092010498 + 100.0 * 9.046385765075684
Epoch 410, val loss: 0.6467901468276978
Epoch 420, training loss: 905.1151123046875 = 0.6140987873077393 + 100.0 * 9.045010566711426
Epoch 420, val loss: 0.6362705230712891
Epoch 430, training loss: 904.9805908203125 = 0.6028145551681519 + 100.0 * 9.043777465820312
Epoch 430, val loss: 0.6260135173797607
Epoch 440, training loss: 905.1212768554688 = 0.591268002986908 + 100.0 * 9.045300483703613
Epoch 440, val loss: 0.6154676079750061
Epoch 450, training loss: 904.82861328125 = 0.5793538093566895 + 100.0 * 9.042492866516113
Epoch 450, val loss: 0.6050476431846619
Epoch 460, training loss: 904.7216796875 = 0.5692464113235474 + 100.0 * 9.041523933410645
Epoch 460, val loss: 0.5958937406539917
Epoch 470, training loss: 904.5633544921875 = 0.5597869157791138 + 100.0 * 9.040035247802734
Epoch 470, val loss: 0.5873451828956604
Epoch 480, training loss: 904.5241088867188 = 0.5504229664802551 + 100.0 * 9.0397367477417
Epoch 480, val loss: 0.5789666771888733
Epoch 490, training loss: 904.6006469726562 = 0.5404354333877563 + 100.0 * 9.040602684020996
Epoch 490, val loss: 0.5700060129165649
Epoch 500, training loss: 904.3952026367188 = 0.5312007069587708 + 100.0 * 9.038640022277832
Epoch 500, val loss: 0.5619509220123291
Epoch 510, training loss: 904.2551879882812 = 0.5230349898338318 + 100.0 * 9.037322044372559
Epoch 510, val loss: 0.5547208189964294
Epoch 520, training loss: 904.1622924804688 = 0.5151413679122925 + 100.0 * 9.036471366882324
Epoch 520, val loss: 0.547753095626831
Epoch 530, training loss: 904.6968383789062 = 0.5073009729385376 + 100.0 * 9.041894912719727
Epoch 530, val loss: 0.5406315922737122
Epoch 540, training loss: 904.178466796875 = 0.4990910589694977 + 100.0 * 9.03679370880127
Epoch 540, val loss: 0.5336050391197205
Epoch 550, training loss: 903.9874877929688 = 0.4921617805957794 + 100.0 * 9.034953117370605
Epoch 550, val loss: 0.5276753902435303
Epoch 560, training loss: 903.87255859375 = 0.4856661260128021 + 100.0 * 9.033868789672852
Epoch 560, val loss: 0.522091269493103
Epoch 570, training loss: 903.8577880859375 = 0.4793098270893097 + 100.0 * 9.033784866333008
Epoch 570, val loss: 0.5166881084442139
Epoch 580, training loss: 903.76416015625 = 0.4727366268634796 + 100.0 * 9.032914161682129
Epoch 580, val loss: 0.510850727558136
Epoch 590, training loss: 903.72265625 = 0.4667474627494812 + 100.0 * 9.032559394836426
Epoch 590, val loss: 0.5058597326278687
Epoch 600, training loss: 903.6590576171875 = 0.4613313674926758 + 100.0 * 9.031977653503418
Epoch 600, val loss: 0.5013052821159363
Epoch 610, training loss: 903.7802734375 = 0.45591503381729126 + 100.0 * 9.033243179321289
Epoch 610, val loss: 0.49668794870376587
Epoch 620, training loss: 903.5357666015625 = 0.4505768418312073 + 100.0 * 9.030852317810059
Epoch 620, val loss: 0.49213656783103943
Epoch 630, training loss: 903.4423217773438 = 0.4458814859390259 + 100.0 * 9.029964447021484
Epoch 630, val loss: 0.4883504807949066
Epoch 640, training loss: 904.0670776367188 = 0.44125163555145264 + 100.0 * 9.036258697509766
Epoch 640, val loss: 0.4842507839202881
Epoch 650, training loss: 903.335205078125 = 0.4361375868320465 + 100.0 * 9.028990745544434
Epoch 650, val loss: 0.4802311956882477
Epoch 660, training loss: 903.2584838867188 = 0.43201109766960144 + 100.0 * 9.028264999389648
Epoch 660, val loss: 0.47693872451782227
Epoch 670, training loss: 903.2064819335938 = 0.42813217639923096 + 100.0 * 9.027783393859863
Epoch 670, val loss: 0.47380927205085754
Epoch 680, training loss: 903.6131591796875 = 0.4241844117641449 + 100.0 * 9.031889915466309
Epoch 680, val loss: 0.4705393612384796
Epoch 690, training loss: 903.2853393554688 = 0.4200221300125122 + 100.0 * 9.028653144836426
Epoch 690, val loss: 0.4673387408256531
Epoch 700, training loss: 903.12939453125 = 0.4163828492164612 + 100.0 * 9.027130126953125
Epoch 700, val loss: 0.46457424759864807
Epoch 710, training loss: 902.9949951171875 = 0.4130708575248718 + 100.0 * 9.025818824768066
Epoch 710, val loss: 0.46203356981277466
Epoch 720, training loss: 903.1270751953125 = 0.40979182720184326 + 100.0 * 9.027173042297363
Epoch 720, val loss: 0.45953378081321716
Epoch 730, training loss: 903.165283203125 = 0.4061916172504425 + 100.0 * 9.02759075164795
Epoch 730, val loss: 0.456767737865448
Epoch 740, training loss: 902.8782348632812 = 0.4029538333415985 + 100.0 * 9.024752616882324
Epoch 740, val loss: 0.45422109961509705
Epoch 750, training loss: 902.8408203125 = 0.400136798620224 + 100.0 * 9.024406433105469
Epoch 750, val loss: 0.4521959722042084
Epoch 760, training loss: 902.9041748046875 = 0.39728012681007385 + 100.0 * 9.025069236755371
Epoch 760, val loss: 0.4500373601913452
Epoch 770, training loss: 902.7752685546875 = 0.3943314850330353 + 100.0 * 9.023809432983398
Epoch 770, val loss: 0.44791629910469055
Epoch 780, training loss: 902.7554321289062 = 0.3915514349937439 + 100.0 * 9.023638725280762
Epoch 780, val loss: 0.44586724042892456
Epoch 790, training loss: 902.92919921875 = 0.38894572854042053 + 100.0 * 9.025403022766113
Epoch 790, val loss: 0.44391682744026184
Epoch 800, training loss: 902.7348022460938 = 0.3861629366874695 + 100.0 * 9.023486137390137
Epoch 800, val loss: 0.44244185090065
Epoch 810, training loss: 902.618896484375 = 0.38377058506011963 + 100.0 * 9.022351264953613
Epoch 810, val loss: 0.4404292702674866
Epoch 820, training loss: 902.5481567382812 = 0.38137999176979065 + 100.0 * 9.02166748046875
Epoch 820, val loss: 0.4389722943305969
Epoch 830, training loss: 902.6456298828125 = 0.37901705503463745 + 100.0 * 9.022665977478027
Epoch 830, val loss: 0.43712717294692993
Epoch 840, training loss: 902.50390625 = 0.3765214681625366 + 100.0 * 9.021273612976074
Epoch 840, val loss: 0.43578746914863586
Epoch 850, training loss: 902.4486083984375 = 0.3742961883544922 + 100.0 * 9.020743370056152
Epoch 850, val loss: 0.4341391623020172
Epoch 860, training loss: 902.4426879882812 = 0.37217769026756287 + 100.0 * 9.020705223083496
Epoch 860, val loss: 0.4329696297645569
Epoch 870, training loss: 902.6317749023438 = 0.3699456751346588 + 100.0 * 9.022618293762207
Epoch 870, val loss: 0.43122598528862
Epoch 880, training loss: 902.3853149414062 = 0.3677339553833008 + 100.0 * 9.02017593383789
Epoch 880, val loss: 0.43023136258125305
Epoch 890, training loss: 902.4550170898438 = 0.36570581793785095 + 100.0 * 9.020893096923828
Epoch 890, val loss: 0.4290235638618469
Epoch 900, training loss: 902.3192749023438 = 0.3636566400527954 + 100.0 * 9.019556045532227
Epoch 900, val loss: 0.4277234673500061
Epoch 910, training loss: 902.2439575195312 = 0.36174458265304565 + 100.0 * 9.018821716308594
Epoch 910, val loss: 0.4265555441379547
Epoch 920, training loss: 902.3917846679688 = 0.359846830368042 + 100.0 * 9.020318984985352
Epoch 920, val loss: 0.4254886209964752
Epoch 930, training loss: 902.2306518554688 = 0.3578967750072479 + 100.0 * 9.01872730255127
Epoch 930, val loss: 0.42423295974731445
Epoch 940, training loss: 902.525634765625 = 0.3560467064380646 + 100.0 * 9.021696090698242
Epoch 940, val loss: 0.4231938421726227
Epoch 950, training loss: 902.274658203125 = 0.3541530966758728 + 100.0 * 9.019205093383789
Epoch 950, val loss: 0.4222046434879303
Epoch 960, training loss: 902.119384765625 = 0.3524208962917328 + 100.0 * 9.017669677734375
Epoch 960, val loss: 0.4212018847465515
Epoch 970, training loss: 902.08349609375 = 0.3507647216320038 + 100.0 * 9.017327308654785
Epoch 970, val loss: 0.42044326663017273
Epoch 980, training loss: 902.2838745117188 = 0.34909000992774963 + 100.0 * 9.01934814453125
Epoch 980, val loss: 0.4196127653121948
Epoch 990, training loss: 902.0825805664062 = 0.3473362922668457 + 100.0 * 9.017352104187012
Epoch 990, val loss: 0.41824114322662354
Epoch 1000, training loss: 902.0103149414062 = 0.3457176387310028 + 100.0 * 9.016646385192871
Epoch 1000, val loss: 0.41761314868927
Epoch 1010, training loss: 902.071533203125 = 0.34413301944732666 + 100.0 * 9.017273902893066
Epoch 1010, val loss: 0.41653913259506226
Epoch 1020, training loss: 902.025634765625 = 0.3424457609653473 + 100.0 * 9.016831398010254
Epoch 1020, val loss: 0.4159318804740906
Epoch 1030, training loss: 901.9620971679688 = 0.3408617675304413 + 100.0 * 9.016212463378906
Epoch 1030, val loss: 0.41511040925979614
Epoch 1040, training loss: 901.9537353515625 = 0.33936807513237 + 100.0 * 9.016143798828125
Epoch 1040, val loss: 0.41460829973220825
Epoch 1050, training loss: 901.9287719726562 = 0.337858110666275 + 100.0 * 9.015909194946289
Epoch 1050, val loss: 0.41372987627983093
Epoch 1060, training loss: 901.97802734375 = 0.33637186884880066 + 100.0 * 9.016416549682617
Epoch 1060, val loss: 0.4131109416484833
Epoch 1070, training loss: 901.8369140625 = 0.3348771631717682 + 100.0 * 9.015020370483398
Epoch 1070, val loss: 0.4122014045715332
Epoch 1080, training loss: 901.7669067382812 = 0.33345869183540344 + 100.0 * 9.014334678649902
Epoch 1080, val loss: 0.4116612374782562
Epoch 1090, training loss: 901.8820190429688 = 0.3320833444595337 + 100.0 * 9.015499114990234
Epoch 1090, val loss: 0.4110235869884491
Epoch 1100, training loss: 901.89794921875 = 0.33053329586982727 + 100.0 * 9.015674591064453
Epoch 1100, val loss: 0.4100300073623657
Epoch 1110, training loss: 901.8165893554688 = 0.32914572954177856 + 100.0 * 9.014874458312988
Epoch 1110, val loss: 0.40976279973983765
Epoch 1120, training loss: 901.6737060546875 = 0.32785123586654663 + 100.0 * 9.013458251953125
Epoch 1120, val loss: 0.4092491567134857
Epoch 1130, training loss: 901.671142578125 = 0.3265438675880432 + 100.0 * 9.013445854187012
Epoch 1130, val loss: 0.4086242616176605
Epoch 1140, training loss: 901.802978515625 = 0.32525962591171265 + 100.0 * 9.014777183532715
Epoch 1140, val loss: 0.4083489179611206
Epoch 1150, training loss: 901.7282104492188 = 0.3238650858402252 + 100.0 * 9.014043807983398
Epoch 1150, val loss: 0.40770232677459717
Epoch 1160, training loss: 901.6039428710938 = 0.32256019115448 + 100.0 * 9.012813568115234
Epoch 1160, val loss: 0.40709802508354187
Epoch 1170, training loss: 901.57763671875 = 0.3213399350643158 + 100.0 * 9.01256275177002
Epoch 1170, val loss: 0.40650367736816406
Epoch 1180, training loss: 901.6027221679688 = 0.32010236382484436 + 100.0 * 9.012825965881348
Epoch 1180, val loss: 0.4061078727245331
Epoch 1190, training loss: 901.7146606445312 = 0.31877923011779785 + 100.0 * 9.013958930969238
Epoch 1190, val loss: 0.4057800769805908
Epoch 1200, training loss: 901.6085815429688 = 0.3175351023674011 + 100.0 * 9.012910842895508
Epoch 1200, val loss: 0.40530309081077576
Epoch 1210, training loss: 901.5331420898438 = 0.3163645267486572 + 100.0 * 9.012167930603027
Epoch 1210, val loss: 0.40477195382118225
Epoch 1220, training loss: 901.524658203125 = 0.3152068555355072 + 100.0 * 9.012094497680664
Epoch 1220, val loss: 0.4043101966381073
Epoch 1230, training loss: 901.7238159179688 = 0.31401535868644714 + 100.0 * 9.014098167419434
Epoch 1230, val loss: 0.40380874276161194
Epoch 1240, training loss: 901.5760498046875 = 0.3127925992012024 + 100.0 * 9.012632369995117
Epoch 1240, val loss: 0.40373000502586365
Epoch 1250, training loss: 901.42529296875 = 0.31168133020401 + 100.0 * 9.011136054992676
Epoch 1250, val loss: 0.4033696949481964
Epoch 1260, training loss: 901.3798828125 = 0.31057557463645935 + 100.0 * 9.010693550109863
Epoch 1260, val loss: 0.4030846059322357
Epoch 1270, training loss: 901.4276123046875 = 0.30947762727737427 + 100.0 * 9.011181831359863
Epoch 1270, val loss: 0.40294134616851807
Epoch 1280, training loss: 901.5135498046875 = 0.30831387639045715 + 100.0 * 9.012052536010742
Epoch 1280, val loss: 0.4025368094444275
Epoch 1290, training loss: 901.365966796875 = 0.307177871465683 + 100.0 * 9.010587692260742
Epoch 1290, val loss: 0.4022676348686218
Epoch 1300, training loss: 901.3422241210938 = 0.30614447593688965 + 100.0 * 9.010360717773438
Epoch 1300, val loss: 0.40169286727905273
Epoch 1310, training loss: 901.3545532226562 = 0.30509892106056213 + 100.0 * 9.010494232177734
Epoch 1310, val loss: 0.4016411602497101
Epoch 1320, training loss: 901.3619384765625 = 0.30400240421295166 + 100.0 * 9.010579109191895
Epoch 1320, val loss: 0.4013631045818329
Epoch 1330, training loss: 901.3237915039062 = 0.30298250913619995 + 100.0 * 9.010208129882812
Epoch 1330, val loss: 0.4008409380912781
Epoch 1340, training loss: 901.3399047851562 = 0.30194786190986633 + 100.0 * 9.010379791259766
Epoch 1340, val loss: 0.40050771832466125
Epoch 1350, training loss: 901.25146484375 = 0.30093684792518616 + 100.0 * 9.009505271911621
Epoch 1350, val loss: 0.400655061006546
Epoch 1360, training loss: 901.2296752929688 = 0.29994723200798035 + 100.0 * 9.009297370910645
Epoch 1360, val loss: 0.40026557445526123
Epoch 1370, training loss: 901.5155639648438 = 0.29894644021987915 + 100.0 * 9.012166023254395
Epoch 1370, val loss: 0.40011265873908997
Epoch 1380, training loss: 901.3798828125 = 0.2978871762752533 + 100.0 * 9.010819435119629
Epoch 1380, val loss: 0.39991676807403564
Epoch 1390, training loss: 901.2124633789062 = 0.2969469130039215 + 100.0 * 9.0091552734375
Epoch 1390, val loss: 0.39981892704963684
Epoch 1400, training loss: 901.1417236328125 = 0.29599225521087646 + 100.0 * 9.00845718383789
Epoch 1400, val loss: 0.39958083629608154
Epoch 1410, training loss: 901.224609375 = 0.2950478494167328 + 100.0 * 9.009295463562012
Epoch 1410, val loss: 0.3993561565876007
Epoch 1420, training loss: 901.1768798828125 = 0.2940514385700226 + 100.0 * 9.008828163146973
Epoch 1420, val loss: 0.39943113923072815
Epoch 1430, training loss: 901.1207885742188 = 0.2931106686592102 + 100.0 * 9.00827693939209
Epoch 1430, val loss: 0.39918017387390137
Epoch 1440, training loss: 901.1519165039062 = 0.29219719767570496 + 100.0 * 9.008597373962402
Epoch 1440, val loss: 0.39922499656677246
Epoch 1450, training loss: 901.3035888671875 = 0.2912566363811493 + 100.0 * 9.010123252868652
Epoch 1450, val loss: 0.3988954424858093
Epoch 1460, training loss: 901.1148071289062 = 0.29030677676200867 + 100.0 * 9.008245468139648
Epoch 1460, val loss: 0.3986658751964569
Epoch 1470, training loss: 901.07421875 = 0.2894260287284851 + 100.0 * 9.007847785949707
Epoch 1470, val loss: 0.39870327711105347
Epoch 1480, training loss: 901.0955810546875 = 0.28853562474250793 + 100.0 * 9.008070945739746
Epoch 1480, val loss: 0.3983091115951538
Epoch 1490, training loss: 901.1693725585938 = 0.2876223623752594 + 100.0 * 9.008817672729492
Epoch 1490, val loss: 0.3983929455280304
Epoch 1500, training loss: 901.0464477539062 = 0.2867182791233063 + 100.0 * 9.007596969604492
Epoch 1500, val loss: 0.3985077440738678
Epoch 1510, training loss: 901.0201416015625 = 0.285858154296875 + 100.0 * 9.007342338562012
Epoch 1510, val loss: 0.3983296751976013
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.31 GiB already allocated; 781.69 MiB free; 7.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.9920654296875 = 1.092875599861145 + 100.0 * 10.358991622924805
Epoch 0, val loss: 1.0918070077896118
Epoch 10, training loss: 1036.5079345703125 = 1.0834294557571411 + 100.0 * 10.354246139526367
Epoch 10, val loss: 1.0820069313049316
Epoch 20, training loss: 1029.962890625 = 1.0700105428695679 + 100.0 * 10.288928031921387
Epoch 20, val loss: 1.0684810876846313
Epoch 30, training loss: 988.0134887695312 = 1.0577170848846436 + 100.0 * 9.86955738067627
Epoch 30, val loss: 1.0563085079193115
Epoch 40, training loss: 955.8396606445312 = 1.0467466115951538 + 100.0 * 9.547928810119629
Epoch 40, val loss: 1.0456899404525757
Epoch 50, training loss: 942.0250244140625 = 1.0341089963912964 + 100.0 * 9.40990924835205
Epoch 50, val loss: 1.0331798791885376
Epoch 60, training loss: 934.39990234375 = 1.0209072828292847 + 100.0 * 9.333789825439453
Epoch 60, val loss: 1.0204706192016602
Epoch 70, training loss: 932.142333984375 = 1.0086002349853516 + 100.0 * 9.3113374710083
Epoch 70, val loss: 1.0085608959197998
Epoch 80, training loss: 927.7257080078125 = 0.9988053441047668 + 100.0 * 9.267269134521484
Epoch 80, val loss: 0.9991441369056702
Epoch 90, training loss: 923.161865234375 = 0.9904544949531555 + 100.0 * 9.22171401977539
Epoch 90, val loss: 0.9909383058547974
Epoch 100, training loss: 919.569580078125 = 0.9796910285949707 + 100.0 * 9.185898780822754
Epoch 100, val loss: 0.9802864789962769
Epoch 110, training loss: 917.3389282226562 = 0.9666159152984619 + 100.0 * 9.16372299194336
Epoch 110, val loss: 0.9677761197090149
Epoch 120, training loss: 915.4095458984375 = 0.9545398354530334 + 100.0 * 9.144550323486328
Epoch 120, val loss: 0.9564995169639587
Epoch 130, training loss: 913.7659301757812 = 0.9447468519210815 + 100.0 * 9.128211975097656
Epoch 130, val loss: 0.9473161697387695
Epoch 140, training loss: 912.5830078125 = 0.9342818856239319 + 100.0 * 9.116487503051758
Epoch 140, val loss: 0.9369989037513733
Epoch 150, training loss: 911.8717041015625 = 0.921682596206665 + 100.0 * 9.10949993133545
Epoch 150, val loss: 0.92472243309021
Epoch 160, training loss: 911.625244140625 = 0.9077820181846619 + 100.0 * 9.10717487335205
Epoch 160, val loss: 0.9111720323562622
Epoch 170, training loss: 911.0454711914062 = 0.893273115158081 + 100.0 * 9.101522445678711
Epoch 170, val loss: 0.8973331451416016
Epoch 180, training loss: 910.5557861328125 = 0.8794230222702026 + 100.0 * 9.096763610839844
Epoch 180, val loss: 0.8840689659118652
Epoch 190, training loss: 910.0443115234375 = 0.866464376449585 + 100.0 * 9.091778755187988
Epoch 190, val loss: 0.8716422915458679
Epoch 200, training loss: 909.782470703125 = 0.8536949753761292 + 100.0 * 9.089287757873535
Epoch 200, val loss: 0.8594275712966919
Epoch 210, training loss: 909.0713500976562 = 0.8410599231719971 + 100.0 * 9.082303047180176
Epoch 210, val loss: 0.8473233580589294
Epoch 220, training loss: 908.8464965820312 = 0.8285073041915894 + 100.0 * 9.080180168151855
Epoch 220, val loss: 0.8350816965103149
Epoch 230, training loss: 908.2344360351562 = 0.8156282901763916 + 100.0 * 9.074188232421875
Epoch 230, val loss: 0.822738766670227
Epoch 240, training loss: 907.745849609375 = 0.8033108711242676 + 100.0 * 9.069425582885742
Epoch 240, val loss: 0.8110689520835876
Epoch 250, training loss: 907.4284057617188 = 0.7911111116409302 + 100.0 * 9.066372871398926
Epoch 250, val loss: 0.7993887662887573
Epoch 260, training loss: 907.0611572265625 = 0.778258740901947 + 100.0 * 9.06282901763916
Epoch 260, val loss: 0.7868604063987732
Epoch 270, training loss: 906.6650390625 = 0.7655055522918701 + 100.0 * 9.058995246887207
Epoch 270, val loss: 0.7747988700866699
Epoch 280, training loss: 906.350341796875 = 0.7523782849311829 + 100.0 * 9.05597972869873
Epoch 280, val loss: 0.7623414397239685
Epoch 290, training loss: 906.1394653320312 = 0.7390149831771851 + 100.0 * 9.054004669189453
Epoch 290, val loss: 0.7494189143180847
Epoch 300, training loss: 906.243408203125 = 0.7251445055007935 + 100.0 * 9.055182456970215
Epoch 300, val loss: 0.7362810969352722
Epoch 310, training loss: 905.8419189453125 = 0.7107653617858887 + 100.0 * 9.051311492919922
Epoch 310, val loss: 0.722615122795105
Epoch 320, training loss: 905.5707397460938 = 0.6969446539878845 + 100.0 * 9.048737525939941
Epoch 320, val loss: 0.7096042633056641
Epoch 330, training loss: 905.7200927734375 = 0.6835520267486572 + 100.0 * 9.050365447998047
Epoch 330, val loss: 0.696779191493988
Epoch 340, training loss: 905.2459716796875 = 0.6697003245353699 + 100.0 * 9.04576301574707
Epoch 340, val loss: 0.6837666630744934
Epoch 350, training loss: 905.0471801757812 = 0.6563541889190674 + 100.0 * 9.04390811920166
Epoch 350, val loss: 0.6714306473731995
Epoch 360, training loss: 904.9414672851562 = 0.6435903310775757 + 100.0 * 9.042978286743164
Epoch 360, val loss: 0.6596923470497131
Epoch 370, training loss: 904.8341064453125 = 0.6308553218841553 + 100.0 * 9.042032241821289
Epoch 370, val loss: 0.6475400924682617
Epoch 380, training loss: 904.6980590820312 = 0.6180001497268677 + 100.0 * 9.040801048278809
Epoch 380, val loss: 0.6357860565185547
Epoch 390, training loss: 904.4511108398438 = 0.605915367603302 + 100.0 * 9.0384521484375
Epoch 390, val loss: 0.6245797872543335
Epoch 400, training loss: 904.5029296875 = 0.5942906737327576 + 100.0 * 9.03908634185791
Epoch 400, val loss: 0.6138703227043152
Epoch 410, training loss: 904.4325561523438 = 0.582249641418457 + 100.0 * 9.03850269317627
Epoch 410, val loss: 0.6025859117507935
Epoch 420, training loss: 904.2013549804688 = 0.5707902908325195 + 100.0 * 9.03630542755127
Epoch 420, val loss: 0.5922678112983704
Epoch 430, training loss: 904.1431274414062 = 0.5600813031196594 + 100.0 * 9.0358304977417
Epoch 430, val loss: 0.5826008915901184
Epoch 440, training loss: 903.9990844726562 = 0.5495647192001343 + 100.0 * 9.03449535369873
Epoch 440, val loss: 0.5726802945137024
Epoch 450, training loss: 904.022705078125 = 0.5393180251121521 + 100.0 * 9.034833908081055
Epoch 450, val loss: 0.5636003613471985
Epoch 460, training loss: 903.9208374023438 = 0.5294981002807617 + 100.0 * 9.033913612365723
Epoch 460, val loss: 0.5547924637794495
Epoch 470, training loss: 903.746337890625 = 0.5202469825744629 + 100.0 * 9.03226089477539
Epoch 470, val loss: 0.5463054776191711
Epoch 480, training loss: 903.6497192382812 = 0.5114786624908447 + 100.0 * 9.03138256072998
Epoch 480, val loss: 0.5385013222694397
Epoch 490, training loss: 903.6644287109375 = 0.5031418800354004 + 100.0 * 9.03161334991455
Epoch 490, val loss: 0.5312309265136719
Epoch 500, training loss: 903.9767456054688 = 0.49458780884742737 + 100.0 * 9.034821510314941
Epoch 500, val loss: 0.5236137509346008
Epoch 510, training loss: 903.5490112304688 = 0.4864148199558258 + 100.0 * 9.03062629699707
Epoch 510, val loss: 0.5163388252258301
Epoch 520, training loss: 903.4301147460938 = 0.479215532541275 + 100.0 * 9.029509544372559
Epoch 520, val loss: 0.5100433230400085
Epoch 530, training loss: 903.3931884765625 = 0.47255340218544006 + 100.0 * 9.029206275939941
Epoch 530, val loss: 0.5042287111282349
Epoch 540, training loss: 903.3814086914062 = 0.46597108244895935 + 100.0 * 9.029154777526855
Epoch 540, val loss: 0.4985596835613251
Epoch 550, training loss: 903.3134155273438 = 0.45964112877845764 + 100.0 * 9.02853775024414
Epoch 550, val loss: 0.49329280853271484
Epoch 560, training loss: 903.3427124023438 = 0.45373591780662537 + 100.0 * 9.028889656066895
Epoch 560, val loss: 0.4881511926651001
Epoch 570, training loss: 903.2035522460938 = 0.44796836376190186 + 100.0 * 9.027555465698242
Epoch 570, val loss: 0.4835737943649292
Epoch 580, training loss: 903.33349609375 = 0.44252142310142517 + 100.0 * 9.028909683227539
Epoch 580, val loss: 0.47899794578552246
Epoch 590, training loss: 903.025390625 = 0.43730390071868896 + 100.0 * 9.025880813598633
Epoch 590, val loss: 0.4746004641056061
Epoch 600, training loss: 902.9987182617188 = 0.4325745701789856 + 100.0 * 9.02566146850586
Epoch 600, val loss: 0.4706408679485321
Epoch 610, training loss: 903.0033569335938 = 0.4280487895011902 + 100.0 * 9.025753021240234
Epoch 610, val loss: 0.46692582964897156
Epoch 620, training loss: 902.8863525390625 = 0.42344585061073303 + 100.0 * 9.024628639221191
Epoch 620, val loss: 0.4634098708629608
Epoch 630, training loss: 902.9407348632812 = 0.4191402792930603 + 100.0 * 9.025216102600098
Epoch 630, val loss: 0.45985814929008484
Epoch 640, training loss: 902.8512573242188 = 0.41524606943130493 + 100.0 * 9.024360656738281
Epoch 640, val loss: 0.45691362023353577
Epoch 650, training loss: 902.775634765625 = 0.4113691747188568 + 100.0 * 9.023642539978027
Epoch 650, val loss: 0.4538685083389282
Epoch 660, training loss: 902.82470703125 = 0.40774041414260864 + 100.0 * 9.024169921875
Epoch 660, val loss: 0.4511454403400421
Epoch 670, training loss: 902.7069702148438 = 0.40411433577537537 + 100.0 * 9.023028373718262
Epoch 670, val loss: 0.44835060834884644
Epoch 680, training loss: 902.6443481445312 = 0.40081551671028137 + 100.0 * 9.022435188293457
Epoch 680, val loss: 0.44586271047592163
Epoch 690, training loss: 902.6008911132812 = 0.3976994454860687 + 100.0 * 9.022031784057617
Epoch 690, val loss: 0.44362714886665344
Epoch 700, training loss: 903.0083618164062 = 0.3946613371372223 + 100.0 * 9.026137351989746
Epoch 700, val loss: 0.44158637523651123
Epoch 710, training loss: 902.5975341796875 = 0.3915773332118988 + 100.0 * 9.022059440612793
Epoch 710, val loss: 0.4391842186450958
Epoch 720, training loss: 902.5217895507812 = 0.3887908458709717 + 100.0 * 9.021329879760742
Epoch 720, val loss: 0.4371291399002075
Epoch 730, training loss: 902.7003173828125 = 0.38610348105430603 + 100.0 * 9.023141860961914
Epoch 730, val loss: 0.4353359639644623
Epoch 740, training loss: 902.4901733398438 = 0.3834395408630371 + 100.0 * 9.02106761932373
Epoch 740, val loss: 0.43357038497924805
Epoch 750, training loss: 902.4144287109375 = 0.38096511363983154 + 100.0 * 9.020334243774414
Epoch 750, val loss: 0.43191713094711304
Epoch 760, training loss: 902.6453857421875 = 0.3784342408180237 + 100.0 * 9.022669792175293
Epoch 760, val loss: 0.43016907572746277
Epoch 770, training loss: 902.3860473632812 = 0.3759719133377075 + 100.0 * 9.020100593566895
Epoch 770, val loss: 0.428609162569046
Epoch 780, training loss: 902.25927734375 = 0.3738183081150055 + 100.0 * 9.018854141235352
Epoch 780, val loss: 0.4272727370262146
Epoch 790, training loss: 902.2656860351562 = 0.3717207908630371 + 100.0 * 9.018939971923828
Epoch 790, val loss: 0.4257351756095886
Epoch 800, training loss: 902.6494140625 = 0.3695864677429199 + 100.0 * 9.022798538208008
Epoch 800, val loss: 0.4242713153362274
Epoch 810, training loss: 902.2003173828125 = 0.36736077070236206 + 100.0 * 9.018329620361328
Epoch 810, val loss: 0.4231932759284973
Epoch 820, training loss: 902.1878051757812 = 0.36541035771369934 + 100.0 * 9.018223762512207
Epoch 820, val loss: 0.4222123622894287
Epoch 830, training loss: 902.3291625976562 = 0.36348477005958557 + 100.0 * 9.019657135009766
Epoch 830, val loss: 0.4208868443965912
Epoch 840, training loss: 902.2915649414062 = 0.36153480410575867 + 100.0 * 9.01930046081543
Epoch 840, val loss: 0.41971027851104736
Epoch 850, training loss: 902.1332397460938 = 0.35966983437538147 + 100.0 * 9.017735481262207
Epoch 850, val loss: 0.4188023507595062
Epoch 860, training loss: 902.031005859375 = 0.3578990697860718 + 100.0 * 9.016731262207031
Epoch 860, val loss: 0.41774678230285645
Epoch 870, training loss: 901.9973754882812 = 0.3562142252922058 + 100.0 * 9.016411781311035
Epoch 870, val loss: 0.4168458580970764
Epoch 880, training loss: 902.0113525390625 = 0.35454636812210083 + 100.0 * 9.016568183898926
Epoch 880, val loss: 0.4159747064113617
Epoch 890, training loss: 902.1769409179688 = 0.35282987356185913 + 100.0 * 9.018240928649902
Epoch 890, val loss: 0.41504406929016113
Epoch 900, training loss: 902.021728515625 = 0.35109013319015503 + 100.0 * 9.016706466674805
Epoch 900, val loss: 0.4141957461833954
Epoch 910, training loss: 901.9120483398438 = 0.349521666765213 + 100.0 * 9.015625
Epoch 910, val loss: 0.41346344351768494
Epoch 920, training loss: 901.895263671875 = 0.3480180501937866 + 100.0 * 9.015472412109375
Epoch 920, val loss: 0.41261065006256104
Epoch 930, training loss: 901.884521484375 = 0.34652820229530334 + 100.0 * 9.015379905700684
Epoch 930, val loss: 0.41202467679977417
Epoch 940, training loss: 902.0628051757812 = 0.3449971377849579 + 100.0 * 9.017178535461426
Epoch 940, val loss: 0.41126298904418945
Epoch 950, training loss: 901.899658203125 = 0.34345608949661255 + 100.0 * 9.015562057495117
Epoch 950, val loss: 0.4105162024497986
Epoch 960, training loss: 901.8045043945312 = 0.3420409560203552 + 100.0 * 9.01462459564209
Epoch 960, val loss: 0.40999332070350647
Epoch 970, training loss: 901.7655029296875 = 0.3406706750392914 + 100.0 * 9.01424789428711
Epoch 970, val loss: 0.4093722105026245
Epoch 980, training loss: 901.9378662109375 = 0.3392934501171112 + 100.0 * 9.015985488891602
Epoch 980, val loss: 0.40871673822402954
Epoch 990, training loss: 901.8257446289062 = 0.33788052201271057 + 100.0 * 9.014878273010254
Epoch 990, val loss: 0.4083254039287567
Epoch 1000, training loss: 901.7734375 = 0.33654317259788513 + 100.0 * 9.014369010925293
Epoch 1000, val loss: 0.40750670433044434
Epoch 1010, training loss: 901.775390625 = 0.33525553345680237 + 100.0 * 9.01440143585205
Epoch 1010, val loss: 0.40716928243637085
Epoch 1020, training loss: 901.732666015625 = 0.33396193385124207 + 100.0 * 9.013986587524414
Epoch 1020, val loss: 0.40648260712623596
Epoch 1030, training loss: 901.8287963867188 = 0.3326791822910309 + 100.0 * 9.014961242675781
Epoch 1030, val loss: 0.40583014488220215
Epoch 1040, training loss: 901.6281127929688 = 0.3314359188079834 + 100.0 * 9.012967109680176
Epoch 1040, val loss: 0.40571048855781555
Epoch 1050, training loss: 901.5759887695312 = 0.3302285969257355 + 100.0 * 9.012457847595215
Epoch 1050, val loss: 0.40524208545684814
Epoch 1060, training loss: 901.5587768554688 = 0.3290393650531769 + 100.0 * 9.012297630310059
Epoch 1060, val loss: 0.40477269887924194
Epoch 1070, training loss: 901.9292602539062 = 0.32782238721847534 + 100.0 * 9.016014099121094
Epoch 1070, val loss: 0.4044334590435028
Epoch 1080, training loss: 901.763427734375 = 0.3266172409057617 + 100.0 * 9.014368057250977
Epoch 1080, val loss: 0.4041365087032318
Epoch 1090, training loss: 901.6202392578125 = 0.3254323899745941 + 100.0 * 9.012948036193848
Epoch 1090, val loss: 0.40360915660858154
Epoch 1100, training loss: 901.5298461914062 = 0.32428860664367676 + 100.0 * 9.012055397033691
Epoch 1100, val loss: 0.40331506729125977
Epoch 1110, training loss: 901.4859008789062 = 0.32320380210876465 + 100.0 * 9.011627197265625
Epoch 1110, val loss: 0.40293607115745544
Epoch 1120, training loss: 901.5958251953125 = 0.32210424542427063 + 100.0 * 9.012737274169922
Epoch 1120, val loss: 0.4026118516921997
Epoch 1130, training loss: 901.562744140625 = 0.3209581673145294 + 100.0 * 9.012417793273926
Epoch 1130, val loss: 0.4025768041610718
Epoch 1140, training loss: 901.4410400390625 = 0.3198752701282501 + 100.0 * 9.011211395263672
Epoch 1140, val loss: 0.40203624963760376
Epoch 1150, training loss: 901.4786376953125 = 0.31881165504455566 + 100.0 * 9.011598587036133
Epoch 1150, val loss: 0.4019697606563568
Epoch 1160, training loss: 901.4282836914062 = 0.3177431523799896 + 100.0 * 9.01110553741455
Epoch 1160, val loss: 0.4016893208026886
Epoch 1170, training loss: 901.3963012695312 = 0.3166944980621338 + 100.0 * 9.010795593261719
Epoch 1170, val loss: 0.40130680799484253
Epoch 1180, training loss: 901.5787963867188 = 0.31565430760383606 + 100.0 * 9.0126314163208
Epoch 1180, val loss: 0.4011441171169281
Epoch 1190, training loss: 901.4613647460938 = 0.3146095275878906 + 100.0 * 9.011466979980469
Epoch 1190, val loss: 0.40071043372154236
Epoch 1200, training loss: 901.3871459960938 = 0.313578337430954 + 100.0 * 9.010735511779785
Epoch 1200, val loss: 0.40077534317970276
Epoch 1210, training loss: 901.4620361328125 = 0.31257733702659607 + 100.0 * 9.011494636535645
Epoch 1210, val loss: 0.4005579948425293
Epoch 1220, training loss: 901.2766723632812 = 0.31158673763275146 + 100.0 * 9.009651184082031
Epoch 1220, val loss: 0.40039968490600586
Epoch 1230, training loss: 901.2920532226562 = 0.3106251358985901 + 100.0 * 9.009814262390137
Epoch 1230, val loss: 0.4002825617790222
Epoch 1240, training loss: 901.4774169921875 = 0.3096504211425781 + 100.0 * 9.011677742004395
Epoch 1240, val loss: 0.4004112780094147
Epoch 1250, training loss: 901.3662109375 = 0.3086423873901367 + 100.0 * 9.010575294494629
Epoch 1250, val loss: 0.3999597430229187
Epoch 1260, training loss: 901.4454345703125 = 0.3077014684677124 + 100.0 * 9.011377334594727
Epoch 1260, val loss: 0.39992740750312805
Epoch 1270, training loss: 901.261474609375 = 0.3067374527454376 + 100.0 * 9.009547233581543
Epoch 1270, val loss: 0.3995703160762787
Epoch 1280, training loss: 901.2072143554688 = 0.3058112561702728 + 100.0 * 9.009014129638672
Epoch 1280, val loss: 0.39951983094215393
Epoch 1290, training loss: 901.433349609375 = 0.3048771321773529 + 100.0 * 9.011284828186035
Epoch 1290, val loss: 0.39947250485420227
Epoch 1300, training loss: 901.2178344726562 = 0.3039601445198059 + 100.0 * 9.009139060974121
Epoch 1300, val loss: 0.3993457555770874
Epoch 1310, training loss: 901.1718139648438 = 0.30304068326950073 + 100.0 * 9.008687973022461
Epoch 1310, val loss: 0.3992392420768738
Epoch 1320, training loss: 901.2098999023438 = 0.3021467924118042 + 100.0 * 9.009078025817871
Epoch 1320, val loss: 0.3990921676158905
Epoch 1330, training loss: 901.2307739257812 = 0.301237553358078 + 100.0 * 9.009295463562012
Epoch 1330, val loss: 0.3990485966205597
Epoch 1340, training loss: 901.2291259765625 = 0.30033794045448303 + 100.0 * 9.00928783416748
Epoch 1340, val loss: 0.3992108702659607
Epoch 1350, training loss: 901.2235717773438 = 0.29944491386413574 + 100.0 * 9.009241104125977
Epoch 1350, val loss: 0.3989686667919159
Epoch 1360, training loss: 901.2548828125 = 0.2985456883907318 + 100.0 * 9.009563446044922
Epoch 1360, val loss: 0.39927586913108826
Epoch 1370, training loss: 901.166259765625 = 0.2976781129837036 + 100.0 * 9.008686065673828
Epoch 1370, val loss: 0.3988945186138153
Epoch 1380, training loss: 901.09130859375 = 0.29681164026260376 + 100.0 * 9.00794506072998
Epoch 1380, val loss: 0.39889490604400635
Epoch 1390, training loss: 901.0482177734375 = 0.29596951603889465 + 100.0 * 9.007522583007812
Epoch 1390, val loss: 0.398870587348938
Epoch 1400, training loss: 901.1874389648438 = 0.29511716961860657 + 100.0 * 9.008923530578613
Epoch 1400, val loss: 0.3987751603126526
Epoch 1410, training loss: 901.0435791015625 = 0.2942598760128021 + 100.0 * 9.007493019104004
Epoch 1410, val loss: 0.3990355432033539
Epoch 1420, training loss: 901.0184936523438 = 0.2934255301952362 + 100.0 * 9.007250785827637
Epoch 1420, val loss: 0.39902836084365845
Epoch 1430, training loss: 901.00439453125 = 0.29260027408599854 + 100.0 * 9.007118225097656
Epoch 1430, val loss: 0.39896389842033386
Epoch 1440, training loss: 901.1396484375 = 0.291778028011322 + 100.0 * 9.008479118347168
Epoch 1440, val loss: 0.3987889587879181
Epoch 1450, training loss: 901.1572875976562 = 0.290927529335022 + 100.0 * 9.008663177490234
Epoch 1450, val loss: 0.3989313840866089
Epoch 1460, training loss: 901.0304565429688 = 0.2901083827018738 + 100.0 * 9.007403373718262
Epoch 1460, val loss: 0.3991074860095978
Epoch 1470, training loss: 900.9627075195312 = 0.2892949879169464 + 100.0 * 9.006733894348145
Epoch 1470, val loss: 0.3990127444267273
Epoch 1480, training loss: 900.9877319335938 = 0.2884889245033264 + 100.0 * 9.00699234008789
Epoch 1480, val loss: 0.3991212546825409
Epoch 1490, training loss: 900.959228515625 = 0.2876771092414856 + 100.0 * 9.006715774536133
Epoch 1490, val loss: 0.39914530515670776
Epoch 1500, training loss: 900.9419555664062 = 0.28688105940818787 + 100.0 * 9.006550788879395
Epoch 1500, val loss: 0.39934492111206055
Epoch 1510, training loss: 900.9771118164062 = 0.2860793173313141 + 100.0 * 9.00691032409668
Epoch 1510, val loss: 0.3995528817176819
Epoch 1520, training loss: 901.0034790039062 = 0.2852930426597595 + 100.0 * 9.007182121276855
Epoch 1520, val loss: 0.3995017111301422
Epoch 1530, training loss: 900.8798828125 = 0.28449854254722595 + 100.0 * 9.005953788757324
Epoch 1530, val loss: 0.3994702100753784
Epoch 1540, training loss: 900.980712890625 = 0.28373169898986816 + 100.0 * 9.006969451904297
Epoch 1540, val loss: 0.3995215594768524
Epoch 1550, training loss: 900.9024658203125 = 0.2829551100730896 + 100.0 * 9.006195068359375
Epoch 1550, val loss: 0.3998773992061615
Epoch 1560, training loss: 900.8235473632812 = 0.2821754217147827 + 100.0 * 9.005414009094238
Epoch 1560, val loss: 0.3998740017414093
Epoch 1570, training loss: 900.8060302734375 = 0.28141605854034424 + 100.0 * 9.00524616241455
Epoch 1570, val loss: 0.4000914394855499
Epoch 1580, training loss: 900.8701782226562 = 0.2806582450866699 + 100.0 * 9.005895614624023
Epoch 1580, val loss: 0.40024492144584656
Epoch 1590, training loss: 900.8748168945312 = 0.27988430857658386 + 100.0 * 9.005949020385742
Epoch 1590, val loss: 0.400204598903656
Epoch 1600, training loss: 900.9081420898438 = 0.27913329005241394 + 100.0 * 9.006290435791016
Epoch 1600, val loss: 0.4000050723552704
Epoch 1610, training loss: 900.8258056640625 = 0.2783859372138977 + 100.0 * 9.005474090576172
Epoch 1610, val loss: 0.40070441365242004
Epoch 1620, training loss: 900.767578125 = 0.2776331305503845 + 100.0 * 9.004899024963379
Epoch 1620, val loss: 0.40060707926750183
Epoch 1630, training loss: 900.8904418945312 = 0.2768869400024414 + 100.0 * 9.006134986877441
Epoch 1630, val loss: 0.40102431178092957
Epoch 1640, training loss: 900.8330688476562 = 0.27611735463142395 + 100.0 * 9.005569458007812
Epoch 1640, val loss: 0.40070444345474243
Epoch 1650, training loss: 900.7901000976562 = 0.27539771795272827 + 100.0 * 9.005146980285645
Epoch 1650, val loss: 0.4012741446495056
Epoch 1660, training loss: 900.7988891601562 = 0.2746518552303314 + 100.0 * 9.005242347717285
Epoch 1660, val loss: 0.4012525677680969
Epoch 1670, training loss: 900.7130126953125 = 0.2739224135875702 + 100.0 * 9.004390716552734
Epoch 1670, val loss: 0.4013187289237976
Epoch 1680, training loss: 900.6926879882812 = 0.2731921672821045 + 100.0 * 9.004195213317871
Epoch 1680, val loss: 0.40133726596832275
Epoch 1690, training loss: 900.8112182617188 = 0.27244991064071655 + 100.0 * 9.005387306213379
Epoch 1690, val loss: 0.4014359712600708
Epoch 1700, training loss: 900.681884765625 = 0.2717205584049225 + 100.0 * 9.004101753234863
Epoch 1700, val loss: 0.40192267298698425
Epoch 1710, training loss: 900.687744140625 = 0.27099692821502686 + 100.0 * 9.004167556762695
Epoch 1710, val loss: 0.40200141072273254
Epoch 1720, training loss: 900.836669921875 = 0.27027246356010437 + 100.0 * 9.005663871765137
Epoch 1720, val loss: 0.4021475315093994
Epoch 1730, training loss: 900.747314453125 = 0.2695574164390564 + 100.0 * 9.004777908325195
Epoch 1730, val loss: 0.40221840143203735
Epoch 1740, training loss: 900.6600341796875 = 0.26884227991104126 + 100.0 * 9.003911972045898
Epoch 1740, val loss: 0.4026466906070709
Epoch 1750, training loss: 900.5902099609375 = 0.26813340187072754 + 100.0 * 9.003220558166504
Epoch 1750, val loss: 0.40279144048690796
Epoch 1760, training loss: 900.576171875 = 0.26741698384284973 + 100.0 * 9.003087997436523
Epoch 1760, val loss: 0.40291211009025574
Epoch 1770, training loss: 900.9528198242188 = 0.26669612526893616 + 100.0 * 9.006860733032227
Epoch 1770, val loss: 0.402974009513855
Epoch 1780, training loss: 900.7273559570312 = 0.26600518822669983 + 100.0 * 9.004613876342773
Epoch 1780, val loss: 0.4035751223564148
Epoch 1790, training loss: 900.6163940429688 = 0.26529791951179504 + 100.0 * 9.003510475158691
Epoch 1790, val loss: 0.40372994542121887
Epoch 1800, training loss: 900.6048583984375 = 0.2646038234233856 + 100.0 * 9.003402709960938
Epoch 1800, val loss: 0.40395739674568176
Epoch 1810, training loss: 900.5638427734375 = 0.2639126479625702 + 100.0 * 9.002999305725098
Epoch 1810, val loss: 0.40415284037590027
Epoch 1820, training loss: 900.6378784179688 = 0.26322028040885925 + 100.0 * 9.003746032714844
Epoch 1820, val loss: 0.4042836129665375
Epoch 1830, training loss: 900.7160034179688 = 0.26251500844955444 + 100.0 * 9.004534721374512
Epoch 1830, val loss: 0.4044591188430786
Epoch 1840, training loss: 900.569580078125 = 0.26184460520744324 + 100.0 * 9.003077507019043
Epoch 1840, val loss: 0.40489432215690613
Epoch 1850, training loss: 900.62158203125 = 0.2611526548862457 + 100.0 * 9.0036039352417
Epoch 1850, val loss: 0.4050382077693939
Epoch 1860, training loss: 900.6161499023438 = 0.2604658007621765 + 100.0 * 9.003557205200195
Epoch 1860, val loss: 0.4050549268722534
Epoch 1870, training loss: 900.5133056640625 = 0.2597971558570862 + 100.0 * 9.002534866333008
Epoch 1870, val loss: 0.40520337224006653
Epoch 1880, training loss: 900.4580688476562 = 0.25912582874298096 + 100.0 * 9.001989364624023
Epoch 1880, val loss: 0.4055844843387604
Epoch 1890, training loss: 900.447265625 = 0.25844606757164 + 100.0 * 9.001888275146484
Epoch 1890, val loss: 0.40556666254997253
Epoch 1900, training loss: 900.469970703125 = 0.25776705145835876 + 100.0 * 9.002121925354004
Epoch 1900, val loss: 0.40577855706214905
Epoch 1910, training loss: 900.7831420898438 = 0.2570964992046356 + 100.0 * 9.005260467529297
Epoch 1910, val loss: 0.40591850876808167
Epoch 1920, training loss: 900.5502319335938 = 0.25641319155693054 + 100.0 * 9.002938270568848
Epoch 1920, val loss: 0.40636715292930603
Epoch 1930, training loss: 900.4443969726562 = 0.25575125217437744 + 100.0 * 9.001886367797852
Epoch 1930, val loss: 0.40676149725914
Epoch 1940, training loss: 900.4476318359375 = 0.25508052110671997 + 100.0 * 9.001925468444824
Epoch 1940, val loss: 0.4068831205368042
Epoch 1950, training loss: 900.5068359375 = 0.2544173002243042 + 100.0 * 9.002524375915527
Epoch 1950, val loss: 0.40737852454185486
Epoch 1960, training loss: 900.5625 = 0.25375401973724365 + 100.0 * 9.003087043762207
Epoch 1960, val loss: 0.4078861176967621
Epoch 1970, training loss: 900.3990478515625 = 0.25307798385620117 + 100.0 * 9.001460075378418
Epoch 1970, val loss: 0.40774592757225037
Epoch 1980, training loss: 900.4349975585938 = 0.25242045521736145 + 100.0 * 9.001825332641602
Epoch 1980, val loss: 0.4082815945148468
Epoch 1990, training loss: 900.3994140625 = 0.25176021456718445 + 100.0 * 9.001476287841797
Epoch 1990, val loss: 0.4084681272506714
Epoch 2000, training loss: 900.4684448242188 = 0.25111475586891174 + 100.0 * 9.00217342376709
Epoch 2000, val loss: 0.4089071750640869
Epoch 2010, training loss: 900.343994140625 = 0.250451922416687 + 100.0 * 9.000935554504395
Epoch 2010, val loss: 0.40893131494522095
Epoch 2020, training loss: 900.3827514648438 = 0.24979905784130096 + 100.0 * 9.00132942199707
Epoch 2020, val loss: 0.40919867157936096
Epoch 2030, training loss: 900.3411865234375 = 0.2491440325975418 + 100.0 * 9.000920295715332
Epoch 2030, val loss: 0.40950751304626465
Epoch 2040, training loss: 900.353271484375 = 0.2484908103942871 + 100.0 * 9.00104808807373
Epoch 2040, val loss: 0.40980207920074463
Epoch 2050, training loss: 900.48095703125 = 0.2478439211845398 + 100.0 * 9.002330780029297
Epoch 2050, val loss: 0.40972551703453064
Epoch 2060, training loss: 900.4552001953125 = 0.24721626937389374 + 100.0 * 9.002079963684082
Epoch 2060, val loss: 0.41015422344207764
Epoch 2070, training loss: 900.2852172851562 = 0.24655359983444214 + 100.0 * 9.000386238098145
Epoch 2070, val loss: 0.4107823371887207
Epoch 2080, training loss: 900.3152465820312 = 0.2459188848733902 + 100.0 * 9.000693321228027
Epoch 2080, val loss: 0.4113368093967438
Epoch 2090, training loss: 900.4382934570312 = 0.24527530372142792 + 100.0 * 9.001930236816406
Epoch 2090, val loss: 0.4114013910293579
Epoch 2100, training loss: 900.359375 = 0.24465201795101166 + 100.0 * 9.001147270202637
Epoch 2100, val loss: 0.4116309583187103
Epoch 2110, training loss: 900.2996215820312 = 0.24400517344474792 + 100.0 * 9.000555992126465
Epoch 2110, val loss: 0.41211554408073425
Epoch 2120, training loss: 900.2216186523438 = 0.24335810542106628 + 100.0 * 8.99978256225586
Epoch 2120, val loss: 0.4122197926044464
Epoch 2130, training loss: 900.2937622070312 = 0.24272124469280243 + 100.0 * 9.000510215759277
Epoch 2130, val loss: 0.41259491443634033
Epoch 2140, training loss: 900.4217529296875 = 0.24209533631801605 + 100.0 * 9.00179672241211
Epoch 2140, val loss: 0.4129526615142822
Epoch 2150, training loss: 900.2726440429688 = 0.24145501852035522 + 100.0 * 9.000311851501465
Epoch 2150, val loss: 0.41325563192367554
Epoch 2160, training loss: 900.3079833984375 = 0.24082335829734802 + 100.0 * 9.00067138671875
Epoch 2160, val loss: 0.4134994447231293
Epoch 2170, training loss: 900.3071899414062 = 0.24020473659038544 + 100.0 * 9.000669479370117
Epoch 2170, val loss: 0.4138348400592804
Epoch 2180, training loss: 900.22314453125 = 0.23955881595611572 + 100.0 * 8.999835968017578
Epoch 2180, val loss: 0.4145185947418213
Epoch 2190, training loss: 900.23486328125 = 0.2389255315065384 + 100.0 * 8.999958992004395
Epoch 2190, val loss: 0.41475537419319153
Epoch 2200, training loss: 900.204833984375 = 0.23830389976501465 + 100.0 * 8.999665260314941
Epoch 2200, val loss: 0.41513681411743164
Epoch 2210, training loss: 900.1930541992188 = 0.23767328262329102 + 100.0 * 8.999553680419922
Epoch 2210, val loss: 0.41545867919921875
Epoch 2220, training loss: 900.3909912109375 = 0.23704245686531067 + 100.0 * 9.00153923034668
Epoch 2220, val loss: 0.41583162546157837
Epoch 2230, training loss: 900.2007446289062 = 0.2364186942577362 + 100.0 * 8.999643325805664
Epoch 2230, val loss: 0.4162578284740448
Epoch 2240, training loss: 900.1686401367188 = 0.2357945591211319 + 100.0 * 8.99932861328125
Epoch 2240, val loss: 0.4165101945400238
Epoch 2250, training loss: 900.2930297851562 = 0.23518334329128265 + 100.0 * 9.000578880310059
Epoch 2250, val loss: 0.4166407287120819
Epoch 2260, training loss: 900.1788940429688 = 0.23454751074314117 + 100.0 * 8.999443054199219
Epoch 2260, val loss: 0.41714340448379517
Epoch 2270, training loss: 900.1717529296875 = 0.2339315116405487 + 100.0 * 8.999378204345703
Epoch 2270, val loss: 0.41758257150650024
Epoch 2280, training loss: 900.1856079101562 = 0.23330187797546387 + 100.0 * 8.999523162841797
Epoch 2280, val loss: 0.4181719720363617
Epoch 2290, training loss: 900.1224365234375 = 0.23268164694309235 + 100.0 * 8.998897552490234
Epoch 2290, val loss: 0.41863662004470825
Epoch 2300, training loss: 900.2284545898438 = 0.2320544570684433 + 100.0 * 8.999963760375977
Epoch 2300, val loss: 0.4190491735935211
Epoch 2310, training loss: 900.1983642578125 = 0.2314574420452118 + 100.0 * 8.999669075012207
Epoch 2310, val loss: 0.41951531171798706
Epoch 2320, training loss: 900.1128540039062 = 0.23084895312786102 + 100.0 * 8.998820304870605
Epoch 2320, val loss: 0.41989004611968994
Epoch 2330, training loss: 900.0552978515625 = 0.23022645711898804 + 100.0 * 8.998250961303711
Epoch 2330, val loss: 0.4200529456138611
Epoch 2340, training loss: 900.0436401367188 = 0.22960978746414185 + 100.0 * 8.998140335083008
Epoch 2340, val loss: 0.4204674959182739
Epoch 2350, training loss: 900.0484008789062 = 0.2289888709783554 + 100.0 * 8.998193740844727
Epoch 2350, val loss: 0.4207514822483063
Epoch 2360, training loss: 900.4613647460938 = 0.22840920090675354 + 100.0 * 9.00232982635498
Epoch 2360, val loss: 0.4207218289375305
Epoch 2370, training loss: 900.1810302734375 = 0.22778008878231049 + 100.0 * 8.999532699584961
Epoch 2370, val loss: 0.42201805114746094
Epoch 2380, training loss: 900.1571044921875 = 0.2271750569343567 + 100.0 * 8.999299049377441
Epoch 2380, val loss: 0.42220035195350647
Epoch 2390, training loss: 900.0916137695312 = 0.22656041383743286 + 100.0 * 8.998650550842285
Epoch 2390, val loss: 0.42253783345222473
Epoch 2400, training loss: 900.0250244140625 = 0.2259424477815628 + 100.0 * 8.997990608215332
Epoch 2400, val loss: 0.42312321066856384
Epoch 2410, training loss: 900.0708618164062 = 0.2253493368625641 + 100.0 * 8.998455047607422
Epoch 2410, val loss: 0.42342403531074524
Epoch 2420, training loss: 900.0703125 = 0.22474029660224915 + 100.0 * 8.998456001281738
Epoch 2420, val loss: 0.42371049523353577
Epoch 2430, training loss: 900.0758056640625 = 0.22414787113666534 + 100.0 * 8.998517036437988
Epoch 2430, val loss: 0.4243791699409485
Epoch 2440, training loss: 900.1618041992188 = 0.22354178130626678 + 100.0 * 8.999382019042969
Epoch 2440, val loss: 0.4249195456504822
Epoch 2450, training loss: 899.982177734375 = 0.22293220460414886 + 100.0 * 8.997591972351074
Epoch 2450, val loss: 0.4254589080810547
Epoch 2460, training loss: 899.9365234375 = 0.22233884036540985 + 100.0 * 8.99714183807373
Epoch 2460, val loss: 0.4258772134780884
Epoch 2470, training loss: 899.946533203125 = 0.22173266112804413 + 100.0 * 8.997247695922852
Epoch 2470, val loss: 0.426407128572464
Epoch 2480, training loss: 900.3201904296875 = 0.22114183008670807 + 100.0 * 9.000990867614746
Epoch 2480, val loss: 0.4272966682910919
Epoch 2490, training loss: 900.048583984375 = 0.2205299437046051 + 100.0 * 8.99828052520752
Epoch 2490, val loss: 0.42708176374435425
Epoch 2500, training loss: 899.991943359375 = 0.21994201838970184 + 100.0 * 8.997719764709473
Epoch 2500, val loss: 0.42789340019226074
Epoch 2510, training loss: 900.119384765625 = 0.21933811902999878 + 100.0 * 8.999000549316406
Epoch 2510, val loss: 0.42849481105804443
Epoch 2520, training loss: 899.9456176757812 = 0.21875686943531036 + 100.0 * 8.997268676757812
Epoch 2520, val loss: 0.4287260174751282
Epoch 2530, training loss: 900.0042114257812 = 0.21816881000995636 + 100.0 * 8.9978609085083
Epoch 2530, val loss: 0.42947322130203247
Epoch 2540, training loss: 899.9654541015625 = 0.21757131814956665 + 100.0 * 8.997478485107422
Epoch 2540, val loss: 0.42966747283935547
Epoch 2550, training loss: 899.9324951171875 = 0.21698123216629028 + 100.0 * 8.99715518951416
Epoch 2550, val loss: 0.43011194467544556
Epoch 2560, training loss: 899.98779296875 = 0.2163904756307602 + 100.0 * 8.997714042663574
Epoch 2560, val loss: 0.43081289529800415
Epoch 2570, training loss: 899.9998779296875 = 0.21580637991428375 + 100.0 * 8.997840881347656
Epoch 2570, val loss: 0.4309684932231903
Epoch 2580, training loss: 900.1376342773438 = 0.2152475118637085 + 100.0 * 8.999223709106445
Epoch 2580, val loss: 0.4316026270389557
Epoch 2590, training loss: 899.90625 = 0.21463800966739655 + 100.0 * 8.996915817260742
Epoch 2590, val loss: 0.4323181211948395
Epoch 2600, training loss: 899.8709106445312 = 0.21406888961791992 + 100.0 * 8.99656867980957
Epoch 2600, val loss: 0.4329774081707001
Epoch 2610, training loss: 899.8395385742188 = 0.21348163485527039 + 100.0 * 8.996260643005371
Epoch 2610, val loss: 0.4333955645561218
Epoch 2620, training loss: 899.8186645507812 = 0.21289579570293427 + 100.0 * 8.996057510375977
Epoch 2620, val loss: 0.4338766932487488
Epoch 2630, training loss: 899.8141479492188 = 0.21230624616146088 + 100.0 * 8.996018409729004
Epoch 2630, val loss: 0.4343583881855011
Epoch 2640, training loss: 900.059814453125 = 0.2117176055908203 + 100.0 * 8.998480796813965
Epoch 2640, val loss: 0.4347947835922241
Epoch 2650, training loss: 899.9544067382812 = 0.21113300323486328 + 100.0 * 8.997432708740234
Epoch 2650, val loss: 0.4357247054576874
Epoch 2660, training loss: 899.911376953125 = 0.21057073771953583 + 100.0 * 8.997008323669434
Epoch 2660, val loss: 0.4359716475009918
Epoch 2670, training loss: 899.8809814453125 = 0.20999906957149506 + 100.0 * 8.996709823608398
Epoch 2670, val loss: 0.4371072053909302
Epoch 2680, training loss: 899.9242553710938 = 0.2094389647245407 + 100.0 * 8.997148513793945
Epoch 2680, val loss: 0.4377099871635437
Epoch 2690, training loss: 899.835205078125 = 0.20884650945663452 + 100.0 * 8.99626350402832
Epoch 2690, val loss: 0.43743863701820374
Epoch 2700, training loss: 899.9114990234375 = 0.2082742303609848 + 100.0 * 8.997032165527344
Epoch 2700, val loss: 0.4382864832878113
Epoch 2710, training loss: 899.8635864257812 = 0.2076994776725769 + 100.0 * 8.996559143066406
Epoch 2710, val loss: 0.43888387084007263
Epoch 2720, training loss: 899.8099365234375 = 0.2071244865655899 + 100.0 * 8.996027946472168
Epoch 2720, val loss: 0.43928414583206177
Epoch 2730, training loss: 899.782470703125 = 0.20653948187828064 + 100.0 * 8.995759010314941
Epoch 2730, val loss: 0.44017666578292847
Epoch 2740, training loss: 900.0131225585938 = 0.20596271753311157 + 100.0 * 8.998071670532227
Epoch 2740, val loss: 0.4408539831638336
Epoch 2750, training loss: 899.8214111328125 = 0.20540018379688263 + 100.0 * 8.996160507202148
Epoch 2750, val loss: 0.4410596787929535
Epoch 2760, training loss: 899.8817138671875 = 0.2048235684633255 + 100.0 * 8.996768951416016
Epoch 2760, val loss: 0.441960871219635
Epoch 2770, training loss: 899.8718872070312 = 0.20424409210681915 + 100.0 * 8.996676445007324
Epoch 2770, val loss: 0.44221392273902893
Epoch 2780, training loss: 899.8057861328125 = 0.203674778342247 + 100.0 * 8.996021270751953
Epoch 2780, val loss: 0.4429095685482025
Epoch 2790, training loss: 899.7570190429688 = 0.2031141221523285 + 100.0 * 8.995538711547852
Epoch 2790, val loss: 0.4436448812484741
Epoch 2800, training loss: 899.7547607421875 = 0.20253676176071167 + 100.0 * 8.995522499084473
Epoch 2800, val loss: 0.44411152601242065
Epoch 2810, training loss: 899.8639526367188 = 0.20196771621704102 + 100.0 * 8.996620178222656
Epoch 2810, val loss: 0.44456014037132263
Epoch 2820, training loss: 899.8623657226562 = 0.20141898095607758 + 100.0 * 8.996609687805176
Epoch 2820, val loss: 0.4450197219848633
Epoch 2830, training loss: 899.8064575195312 = 0.20085200667381287 + 100.0 * 8.996055603027344
Epoch 2830, val loss: 0.4462888836860657
Epoch 2840, training loss: 899.7056884765625 = 0.20028525590896606 + 100.0 * 8.995054244995117
Epoch 2840, val loss: 0.4465589225292206
Epoch 2850, training loss: 899.6824340820312 = 0.19971948862075806 + 100.0 * 8.994827270507812
Epoch 2850, val loss: 0.44722509384155273
Epoch 2860, training loss: 899.7266845703125 = 0.19916844367980957 + 100.0 * 8.995275497436523
Epoch 2860, val loss: 0.44745340943336487
Epoch 2870, training loss: 899.90576171875 = 0.19862137734889984 + 100.0 * 8.997071266174316
Epoch 2870, val loss: 0.44802743196487427
Epoch 2880, training loss: 899.8621826171875 = 0.19805146753787994 + 100.0 * 8.996641159057617
Epoch 2880, val loss: 0.44984057545661926
Epoch 2890, training loss: 899.7570190429688 = 0.19747671484947205 + 100.0 * 8.995595932006836
Epoch 2890, val loss: 0.44969597458839417
Epoch 2900, training loss: 899.6726684570312 = 0.196918323636055 + 100.0 * 8.994757652282715
Epoch 2900, val loss: 0.45054832100868225
Epoch 2910, training loss: 899.6951904296875 = 0.19637362658977509 + 100.0 * 8.994988441467285
Epoch 2910, val loss: 0.45102837681770325
Epoch 2920, training loss: 899.808349609375 = 0.19582191109657288 + 100.0 * 8.996125221252441
Epoch 2920, val loss: 0.45163968205451965
Epoch 2930, training loss: 899.7982177734375 = 0.19526147842407227 + 100.0 * 8.9960298538208
Epoch 2930, val loss: 0.45287632942199707
Epoch 2940, training loss: 899.7752685546875 = 0.19471082091331482 + 100.0 * 8.995805740356445
Epoch 2940, val loss: 0.4533338248729706
Epoch 2950, training loss: 899.7369384765625 = 0.1941537708044052 + 100.0 * 8.995428085327148
Epoch 2950, val loss: 0.45393699407577515
Epoch 2960, training loss: 899.6531372070312 = 0.19360068440437317 + 100.0 * 8.994595527648926
Epoch 2960, val loss: 0.45474207401275635
Epoch 2970, training loss: 899.624755859375 = 0.1930515021085739 + 100.0 * 8.994317054748535
Epoch 2970, val loss: 0.4553300142288208
Epoch 2980, training loss: 899.7535400390625 = 0.19249889254570007 + 100.0 * 8.995610237121582
Epoch 2980, val loss: 0.45616576075553894
Epoch 2990, training loss: 899.6119384765625 = 0.19194912910461426 + 100.0 * 8.994199752807617
Epoch 2990, val loss: 0.4569937586784363
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8483
Overall ASR: 0.7054
Flip ASR: 0.6332/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.30 GiB already allocated; 547.69 MiB free; 4.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 855.69 MiB free; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 851.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 577.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 573.69 MiB free; 5.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 573.69 MiB free; 5.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 577.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0084228515625 = 1.0988227128982544 + 100.0 * 10.359095573425293
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 297.69 MiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 577.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 573.69 MiB free; 5.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 577.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 531.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 535.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.999755859375 = 1.095029592514038 + 100.0 * 10.359047889709473
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 255.69 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0098876953125 = 1.099192500114441 + 100.0 * 10.359106063842773
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 251.69 MiB free; 6.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0040283203125 = 1.0984503030776978 + 100.0 * 10.359054565429688
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 847.69 MiB free; 6.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0048828125 = 1.0937975645065308 + 100.0 * 10.359111785888672
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 255.69 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.983154296875 = 1.0918047428131104 + 100.0 * 10.35891342163086
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 255.69 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 535.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 809.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 809.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0106201171875 = 1.1036721467971802 + 100.0 * 10.35906982421875
Epoch 0, val loss: 1.1022316217422485
Epoch 10, training loss: 1036.665283203125 = 1.0923488140106201 + 100.0 * 10.355729103088379
Epoch 10, val loss: 1.0905733108520508
Epoch 20, training loss: 1030.9339599609375 = 1.0770244598388672 + 100.0 * 10.298568725585938
Epoch 20, val loss: 1.0753326416015625
Epoch 30, training loss: 977.0936889648438 = 1.0628474950790405 + 100.0 * 9.760308265686035
Epoch 30, val loss: 1.0615369081497192
Epoch 40, training loss: 962.3369750976562 = 1.0489869117736816 + 100.0 * 9.612879753112793
Epoch 40, val loss: 1.0477757453918457
Epoch 50, training loss: 952.3353881835938 = 1.0353230237960815 + 100.0 * 9.51300048828125
Epoch 50, val loss: 1.0344949960708618
Epoch 60, training loss: 944.659423828125 = 1.0230655670166016 + 100.0 * 9.436363220214844
Epoch 60, val loss: 1.022437334060669
Epoch 70, training loss: 939.9969482421875 = 1.005637526512146 + 100.0 * 9.389913558959961
Epoch 70, val loss: 1.0052294731140137
Epoch 80, training loss: 935.0240478515625 = 0.9894517064094543 + 100.0 * 9.340346336364746
Epoch 80, val loss: 0.9896970987319946
Epoch 90, training loss: 928.2360229492188 = 0.9770313501358032 + 100.0 * 9.272589683532715
Epoch 90, val loss: 0.9773054718971252
Epoch 100, training loss: 923.5347900390625 = 0.9641364216804504 + 100.0 * 9.225707054138184
Epoch 100, val loss: 0.9640556573867798
Epoch 110, training loss: 921.259765625 = 0.9452584385871887 + 100.0 * 9.203145027160645
Epoch 110, val loss: 0.9453654885292053
Epoch 120, training loss: 919.4675903320312 = 0.9220799803733826 + 100.0 * 9.185455322265625
Epoch 120, val loss: 0.9227330684661865
Epoch 130, training loss: 918.3672485351562 = 0.8974548578262329 + 100.0 * 9.174697875976562
Epoch 130, val loss: 0.8984338045120239
Epoch 140, training loss: 917.3546142578125 = 0.8717660307884216 + 100.0 * 9.164828300476074
Epoch 140, val loss: 0.873109757900238
Epoch 150, training loss: 916.5725708007812 = 0.844819188117981 + 100.0 * 9.15727710723877
Epoch 150, val loss: 0.846708357334137
Epoch 160, training loss: 916.0282592773438 = 0.8167967796325684 + 100.0 * 9.152114868164062
Epoch 160, val loss: 0.8193270564079285
Epoch 170, training loss: 915.2647705078125 = 0.7880855798721313 + 100.0 * 9.144766807556152
Epoch 170, val loss: 0.7916739583015442
Epoch 180, training loss: 914.6776123046875 = 0.7595207691192627 + 100.0 * 9.139181137084961
Epoch 180, val loss: 0.7643444538116455
Epoch 190, training loss: 914.3301391601562 = 0.7311131954193115 + 100.0 * 9.135990142822266
Epoch 190, val loss: 0.7373829483985901
Epoch 200, training loss: 913.7740478515625 = 0.7032164335250854 + 100.0 * 9.130708694458008
Epoch 200, val loss: 0.7113160490989685
Epoch 210, training loss: 913.3582763671875 = 0.6768648028373718 + 100.0 * 9.126813888549805
Epoch 210, val loss: 0.6866819858551025
Epoch 220, training loss: 913.3226318359375 = 0.6518839001655579 + 100.0 * 9.126708030700684
Epoch 220, val loss: 0.6636980772018433
Epoch 230, training loss: 912.86767578125 = 0.6281290054321289 + 100.0 * 9.122395515441895
Epoch 230, val loss: 0.6420754194259644
Epoch 240, training loss: 912.5817260742188 = 0.6063044667243958 + 100.0 * 9.11975383758545
Epoch 240, val loss: 0.6225115060806274
Epoch 250, training loss: 912.5297241210938 = 0.5863714814186096 + 100.0 * 9.119433403015137
Epoch 250, val loss: 0.6048935055732727
Epoch 260, training loss: 912.1611328125 = 0.5682095885276794 + 100.0 * 9.115928649902344
Epoch 260, val loss: 0.589239776134491
Epoch 270, training loss: 911.9752807617188 = 0.5519925951957703 + 100.0 * 9.114233016967773
Epoch 270, val loss: 0.5756112337112427
Epoch 280, training loss: 911.7534790039062 = 0.5375151634216309 + 100.0 * 9.112159729003906
Epoch 280, val loss: 0.563585638999939
Epoch 290, training loss: 911.7218017578125 = 0.5245359539985657 + 100.0 * 9.11197280883789
Epoch 290, val loss: 0.5531060695648193
Epoch 300, training loss: 911.378173828125 = 0.5125681757926941 + 100.0 * 9.10865592956543
Epoch 300, val loss: 0.5437335968017578
Epoch 310, training loss: 911.1397705078125 = 0.5020060539245605 + 100.0 * 9.106377601623535
Epoch 310, val loss: 0.5355658531188965
Epoch 320, training loss: 911.0014038085938 = 0.4925273358821869 + 100.0 * 9.105088233947754
Epoch 320, val loss: 0.5281034111976624
Epoch 330, training loss: 910.8118896484375 = 0.483421266078949 + 100.0 * 9.10328483581543
Epoch 330, val loss: 0.5216953158378601
Epoch 340, training loss: 910.5302124023438 = 0.47521349787712097 + 100.0 * 9.100549697875977
Epoch 340, val loss: 0.5154765844345093
Epoch 350, training loss: 910.355712890625 = 0.4678482115268707 + 100.0 * 9.098878860473633
Epoch 350, val loss: 0.5102946162223816
Epoch 360, training loss: 910.5233154296875 = 0.46098560094833374 + 100.0 * 9.10062313079834
Epoch 360, val loss: 0.5055630207061768
Epoch 370, training loss: 910.1095581054688 = 0.45432910323143005 + 100.0 * 9.096551895141602
Epoch 370, val loss: 0.5008878707885742
Epoch 380, training loss: 909.8936157226562 = 0.4482688307762146 + 100.0 * 9.094453811645508
Epoch 380, val loss: 0.4966898560523987
Epoch 390, training loss: 909.7481689453125 = 0.4427056610584259 + 100.0 * 9.09305477142334
Epoch 390, val loss: 0.49293455481529236
Epoch 400, training loss: 909.7945556640625 = 0.43734392523765564 + 100.0 * 9.093572616577148
Epoch 400, val loss: 0.4894300401210785
Epoch 410, training loss: 909.5640258789062 = 0.43214455246925354 + 100.0 * 9.09131908416748
Epoch 410, val loss: 0.4855548143386841
Epoch 420, training loss: 909.3627319335938 = 0.42747464776039124 + 100.0 * 9.08935260772705
Epoch 420, val loss: 0.48229801654815674
Epoch 430, training loss: 909.215087890625 = 0.42308229207992554 + 100.0 * 9.087920188903809
Epoch 430, val loss: 0.4793137013912201
Epoch 440, training loss: 909.22900390625 = 0.41878557205200195 + 100.0 * 9.088102340698242
Epoch 440, val loss: 0.4762311577796936
Epoch 450, training loss: 909.0548706054688 = 0.41461342573165894 + 100.0 * 9.086402893066406
Epoch 450, val loss: 0.47325390577316284
Epoch 460, training loss: 908.8919677734375 = 0.4107789099216461 + 100.0 * 9.08481216430664
Epoch 460, val loss: 0.4705439805984497
Epoch 470, training loss: 909.0057373046875 = 0.4071060121059418 + 100.0 * 9.085986137390137
Epoch 470, val loss: 0.4677639305591583
Epoch 480, training loss: 908.7455444335938 = 0.4034140706062317 + 100.0 * 9.083420753479004
Epoch 480, val loss: 0.465334415435791
Epoch 490, training loss: 908.6319580078125 = 0.3999941647052765 + 100.0 * 9.082319259643555
Epoch 490, val loss: 0.46282514929771423
Epoch 500, training loss: 908.6663818359375 = 0.3966597020626068 + 100.0 * 9.082696914672852
Epoch 500, val loss: 0.4603678584098816
Epoch 510, training loss: 908.4336547851562 = 0.39340850710868835 + 100.0 * 9.080402374267578
Epoch 510, val loss: 0.4581100046634674
Epoch 520, training loss: 908.374267578125 = 0.3903597593307495 + 100.0 * 9.079838752746582
Epoch 520, val loss: 0.45617222785949707
Epoch 530, training loss: 908.3301391601562 = 0.3872873783111572 + 100.0 * 9.079428672790527
Epoch 530, val loss: 0.4538666009902954
Epoch 540, training loss: 908.1854858398438 = 0.3843368887901306 + 100.0 * 9.078011512756348
Epoch 540, val loss: 0.4518399238586426
Epoch 550, training loss: 908.112548828125 = 0.3815803527832031 + 100.0 * 9.077309608459473
Epoch 550, val loss: 0.4500800371170044
Epoch 560, training loss: 908.12158203125 = 0.37891432642936707 + 100.0 * 9.07742691040039
Epoch 560, val loss: 0.4482232928276062
Epoch 570, training loss: 908.11572265625 = 0.3761247992515564 + 100.0 * 9.077396392822266
Epoch 570, val loss: 0.4464908838272095
Epoch 580, training loss: 907.8988647460938 = 0.3735183775424957 + 100.0 * 9.0752534866333
Epoch 580, val loss: 0.4447832703590393
Epoch 590, training loss: 907.8099365234375 = 0.37106937170028687 + 100.0 * 9.07438850402832
Epoch 590, val loss: 0.44337186217308044
Epoch 600, training loss: 907.758544921875 = 0.3686859607696533 + 100.0 * 9.073898315429688
Epoch 600, val loss: 0.4418579638004303
Epoch 610, training loss: 908.139892578125 = 0.36632394790649414 + 100.0 * 9.077735900878906
Epoch 610, val loss: 0.44034022092819214
Epoch 620, training loss: 907.6787109375 = 0.36382946372032166 + 100.0 * 9.073148727416992
Epoch 620, val loss: 0.43890219926834106
Epoch 630, training loss: 907.6016845703125 = 0.36153730750083923 + 100.0 * 9.072402000427246
Epoch 630, val loss: 0.4378586411476135
Epoch 640, training loss: 907.5162963867188 = 0.359354704618454 + 100.0 * 9.071569442749023
Epoch 640, val loss: 0.4366612136363983
Epoch 650, training loss: 907.6002197265625 = 0.35721084475517273 + 100.0 * 9.072429656982422
Epoch 650, val loss: 0.4355008602142334
Epoch 660, training loss: 907.4521484375 = 0.35500839352607727 + 100.0 * 9.070971488952637
Epoch 660, val loss: 0.4344857335090637
Epoch 670, training loss: 907.540283203125 = 0.35289642214775085 + 100.0 * 9.071873664855957
Epoch 670, val loss: 0.43356961011886597
Epoch 680, training loss: 907.3124389648438 = 0.3508325517177582 + 100.0 * 9.069616317749023
Epoch 680, val loss: 0.43198809027671814
Epoch 690, training loss: 907.228271484375 = 0.34885191917419434 + 100.0 * 9.068794250488281
Epoch 690, val loss: 0.43154796957969666
Epoch 700, training loss: 907.1906127929688 = 0.3469122350215912 + 100.0 * 9.068436622619629
Epoch 700, val loss: 0.43073415756225586
Epoch 710, training loss: 907.3034057617188 = 0.3449248671531677 + 100.0 * 9.069584846496582
Epoch 710, val loss: 0.42970699071884155
Epoch 720, training loss: 907.134521484375 = 0.34294888377189636 + 100.0 * 9.067915916442871
Epoch 720, val loss: 0.42889222502708435
Epoch 730, training loss: 907.1323852539062 = 0.34105148911476135 + 100.0 * 9.067913055419922
Epoch 730, val loss: 0.4282175898551941
Epoch 740, training loss: 907.0126953125 = 0.3392088711261749 + 100.0 * 9.066734313964844
Epoch 740, val loss: 0.42763960361480713
Epoch 750, training loss: 906.9166870117188 = 0.33740830421447754 + 100.0 * 9.06579303741455
Epoch 750, val loss: 0.4269097149372101
Epoch 760, training loss: 906.9229736328125 = 0.33563926815986633 + 100.0 * 9.065873146057129
Epoch 760, val loss: 0.42628568410873413
Epoch 770, training loss: 907.0530395507812 = 0.3338091969490051 + 100.0 * 9.067192077636719
Epoch 770, val loss: 0.4260897636413574
Epoch 780, training loss: 906.891357421875 = 0.33195507526397705 + 100.0 * 9.065593719482422
Epoch 780, val loss: 0.4251523017883301
Epoch 790, training loss: 906.7745361328125 = 0.33022382855415344 + 100.0 * 9.064443588256836
Epoch 790, val loss: 0.4250772297382355
Epoch 800, training loss: 906.7781372070312 = 0.32853880524635315 + 100.0 * 9.064496040344238
Epoch 800, val loss: 0.42492520809173584
Epoch 810, training loss: 906.7002563476562 = 0.3268524408340454 + 100.0 * 9.06373405456543
Epoch 810, val loss: 0.42428648471832275
Epoch 820, training loss: 906.7338256835938 = 0.325151264667511 + 100.0 * 9.0640869140625
Epoch 820, val loss: 0.42361682653427124
Epoch 830, training loss: 906.5602416992188 = 0.32347097992897034 + 100.0 * 9.06236743927002
Epoch 830, val loss: 0.42328721284866333
Epoch 840, training loss: 906.540283203125 = 0.3218417465686798 + 100.0 * 9.06218433380127
Epoch 840, val loss: 0.4231604337692261
Epoch 850, training loss: 907.0772094726562 = 0.32022908329963684 + 100.0 * 9.067569732666016
Epoch 850, val loss: 0.42243000864982605
Epoch 860, training loss: 906.4931640625 = 0.31849223375320435 + 100.0 * 9.061746597290039
Epoch 860, val loss: 0.42228394746780396
Epoch 870, training loss: 906.4432983398438 = 0.316913366317749 + 100.0 * 9.061264038085938
Epoch 870, val loss: 0.42245611548423767
Epoch 880, training loss: 906.3788452148438 = 0.31535300612449646 + 100.0 * 9.06063461303711
Epoch 880, val loss: 0.4220640957355499
Epoch 890, training loss: 906.3138427734375 = 0.3138079047203064 + 100.0 * 9.0600004196167
Epoch 890, val loss: 0.4218541979789734
Epoch 900, training loss: 906.36376953125 = 0.31226181983947754 + 100.0 * 9.060515403747559
Epoch 900, val loss: 0.42159131169319153
Epoch 910, training loss: 906.4244384765625 = 0.3105999231338501 + 100.0 * 9.061138153076172
Epoch 910, val loss: 0.42160046100616455
Epoch 920, training loss: 906.2333374023438 = 0.30901041626930237 + 100.0 * 9.059243202209473
Epoch 920, val loss: 0.42135047912597656
Epoch 930, training loss: 906.1715698242188 = 0.30749309062957764 + 100.0 * 9.058640480041504
Epoch 930, val loss: 0.42120394110679626
Epoch 940, training loss: 906.129150390625 = 0.30600088834762573 + 100.0 * 9.058231353759766
Epoch 940, val loss: 0.421028196811676
Epoch 950, training loss: 906.37646484375 = 0.30452805757522583 + 100.0 * 9.06071949005127
Epoch 950, val loss: 0.42153918743133545
Epoch 960, training loss: 906.3445434570312 = 0.30290931463241577 + 100.0 * 9.060416221618652
Epoch 960, val loss: 0.42023009061813354
Epoch 970, training loss: 906.0908813476562 = 0.3013714551925659 + 100.0 * 9.057894706726074
Epoch 970, val loss: 0.42044883966445923
Epoch 980, training loss: 905.99560546875 = 0.2998863458633423 + 100.0 * 9.056957244873047
Epoch 980, val loss: 0.4202648103237152
Epoch 990, training loss: 905.9560546875 = 0.29842832684516907 + 100.0 * 9.0565767288208
Epoch 990, val loss: 0.4204113781452179
Epoch 1000, training loss: 905.921875 = 0.29696550965309143 + 100.0 * 9.056249618530273
Epoch 1000, val loss: 0.42027291655540466
Epoch 1010, training loss: 905.9573364257812 = 0.2954922020435333 + 100.0 * 9.056618690490723
Epoch 1010, val loss: 0.4200511574745178
Epoch 1020, training loss: 905.8834228515625 = 0.2939348816871643 + 100.0 * 9.05589485168457
Epoch 1020, val loss: 0.42025187611579895
Epoch 1030, training loss: 905.870849609375 = 0.2924235463142395 + 100.0 * 9.055784225463867
Epoch 1030, val loss: 0.4201328754425049
Epoch 1040, training loss: 905.84375 = 0.2909446656703949 + 100.0 * 9.055527687072754
Epoch 1040, val loss: 0.41976478695869446
Epoch 1050, training loss: 906.087646484375 = 0.2894715368747711 + 100.0 * 9.057981491088867
Epoch 1050, val loss: 0.41957977414131165
Epoch 1060, training loss: 905.7827758789062 = 0.2879433035850525 + 100.0 * 9.054947853088379
Epoch 1060, val loss: 0.4198325276374817
Epoch 1070, training loss: 905.6970825195312 = 0.28646668791770935 + 100.0 * 9.054106712341309
Epoch 1070, val loss: 0.4199292063713074
Epoch 1080, training loss: 905.6644897460938 = 0.284996896982193 + 100.0 * 9.053794860839844
Epoch 1080, val loss: 0.41964396834373474
Epoch 1090, training loss: 905.644287109375 = 0.28352996706962585 + 100.0 * 9.053607940673828
Epoch 1090, val loss: 0.4197222590446472
Epoch 1100, training loss: 906.1427612304688 = 0.28202930092811584 + 100.0 * 9.05860710144043
Epoch 1100, val loss: 0.4196341633796692
Epoch 1110, training loss: 905.6908569335938 = 0.28046491742134094 + 100.0 * 9.05410385131836
Epoch 1110, val loss: 0.4195573329925537
Epoch 1120, training loss: 905.5503540039062 = 0.27896398305892944 + 100.0 * 9.052713394165039
Epoch 1120, val loss: 0.41951820254325867
Epoch 1130, training loss: 905.5185546875 = 0.27749109268188477 + 100.0 * 9.052411079406738
Epoch 1130, val loss: 0.41941124200820923
Epoch 1140, training loss: 905.776123046875 = 0.2760311961174011 + 100.0 * 9.055001258850098
Epoch 1140, val loss: 0.4195413589477539
Epoch 1150, training loss: 905.6151733398438 = 0.27446138858795166 + 100.0 * 9.053406715393066
Epoch 1150, val loss: 0.4189992845058441
Epoch 1160, training loss: 905.4528198242188 = 0.27297264337539673 + 100.0 * 9.051798820495605
Epoch 1160, val loss: 0.4192665219306946
Epoch 1170, training loss: 905.4014892578125 = 0.27148333191871643 + 100.0 * 9.051300048828125
Epoch 1170, val loss: 0.41931068897247314
Epoch 1180, training loss: 905.3773193359375 = 0.26999783515930176 + 100.0 * 9.05107307434082
Epoch 1180, val loss: 0.41902753710746765
Epoch 1190, training loss: 905.4321899414062 = 0.2685084939002991 + 100.0 * 9.051636695861816
Epoch 1190, val loss: 0.41899070143699646
Epoch 1200, training loss: 905.53173828125 = 0.2669869363307953 + 100.0 * 9.052647590637207
Epoch 1200, val loss: 0.4193331301212311
Epoch 1210, training loss: 905.4105834960938 = 0.26543352007865906 + 100.0 * 9.051451683044434
Epoch 1210, val loss: 0.41868776082992554
Epoch 1220, training loss: 905.2967529296875 = 0.2639062702655792 + 100.0 * 9.050328254699707
Epoch 1220, val loss: 0.4188169836997986
Epoch 1230, training loss: 905.2341918945312 = 0.2624075710773468 + 100.0 * 9.049717903137207
Epoch 1230, val loss: 0.41866323351860046
Epoch 1240, training loss: 905.2291870117188 = 0.26091256737709045 + 100.0 * 9.0496826171875
Epoch 1240, val loss: 0.4184713363647461
Epoch 1250, training loss: 905.4960327148438 = 0.2593972086906433 + 100.0 * 9.052366256713867
Epoch 1250, val loss: 0.41846510767936707
Epoch 1260, training loss: 905.2405395507812 = 0.25784832239151 + 100.0 * 9.049826622009277
Epoch 1260, val loss: 0.41822052001953125
Epoch 1270, training loss: 905.1412963867188 = 0.2563060224056244 + 100.0 * 9.048850059509277
Epoch 1270, val loss: 0.41837435960769653
Epoch 1280, training loss: 905.1142578125 = 0.25479891896247864 + 100.0 * 9.04859447479248
Epoch 1280, val loss: 0.41851523518562317
Epoch 1290, training loss: 905.1630859375 = 0.2532827854156494 + 100.0 * 9.049098014831543
Epoch 1290, val loss: 0.41820311546325684
Epoch 1300, training loss: 905.1007690429688 = 0.25171151757240295 + 100.0 * 9.048490524291992
Epoch 1300, val loss: 0.4184672236442566
Epoch 1310, training loss: 905.0938720703125 = 0.25017237663269043 + 100.0 * 9.048437118530273
Epoch 1310, val loss: 0.41838085651397705
Epoch 1320, training loss: 905.0562133789062 = 0.24863338470458984 + 100.0 * 9.048075675964355
Epoch 1320, val loss: 0.4185788631439209
Epoch 1330, training loss: 905.076416015625 = 0.24710239470005035 + 100.0 * 9.048293113708496
Epoch 1330, val loss: 0.4185834228992462
Epoch 1340, training loss: 904.9854125976562 = 0.24553801119327545 + 100.0 * 9.047398567199707
Epoch 1340, val loss: 0.4182601571083069
Epoch 1350, training loss: 905.1654052734375 = 0.2439827173948288 + 100.0 * 9.049214363098145
Epoch 1350, val loss: 0.41841235756874084
Epoch 1360, training loss: 904.9696044921875 = 0.24241387844085693 + 100.0 * 9.047271728515625
Epoch 1360, val loss: 0.4183335304260254
Epoch 1370, training loss: 904.91845703125 = 0.24084962904453278 + 100.0 * 9.046775817871094
Epoch 1370, val loss: 0.41808244585990906
Epoch 1380, training loss: 904.8978271484375 = 0.239297017455101 + 100.0 * 9.046585083007812
Epoch 1380, val loss: 0.41856610774993896
Epoch 1390, training loss: 905.095458984375 = 0.2377287596464157 + 100.0 * 9.048577308654785
Epoch 1390, val loss: 0.4184707999229431
Epoch 1400, training loss: 904.908935546875 = 0.23613391816616058 + 100.0 * 9.046728134155273
Epoch 1400, val loss: 0.4179946184158325
Epoch 1410, training loss: 904.818115234375 = 0.23454859852790833 + 100.0 * 9.045835494995117
Epoch 1410, val loss: 0.4184209108352661
Epoch 1420, training loss: 904.8440551757812 = 0.23298227787017822 + 100.0 * 9.046111106872559
Epoch 1420, val loss: 0.4182030260562897
Epoch 1430, training loss: 904.9042358398438 = 0.231448695063591 + 100.0 * 9.046728134155273
Epoch 1430, val loss: 0.41915544867515564
Epoch 1440, training loss: 904.819091796875 = 0.2297898381948471 + 100.0 * 9.045892715454102
Epoch 1440, val loss: 0.41852867603302
Epoch 1450, training loss: 904.688720703125 = 0.2281942069530487 + 100.0 * 9.044605255126953
Epoch 1450, val loss: 0.4187110364437103
Epoch 1460, training loss: 904.6734008789062 = 0.2266116738319397 + 100.0 * 9.04446792602539
Epoch 1460, val loss: 0.4187870919704437
Epoch 1470, training loss: 904.683837890625 = 0.22502657771110535 + 100.0 * 9.044588088989258
Epoch 1470, val loss: 0.4185384213924408
Epoch 1480, training loss: 904.869384765625 = 0.22342011332511902 + 100.0 * 9.046459197998047
Epoch 1480, val loss: 0.4188005030155182
Epoch 1490, training loss: 904.6618041992188 = 0.2217959612607956 + 100.0 * 9.044400215148926
Epoch 1490, val loss: 0.4188242554664612
Epoch 1500, training loss: 904.5888671875 = 0.22017699480056763 + 100.0 * 9.043686866760254
Epoch 1500, val loss: 0.4193361699581146
Epoch 1510, training loss: 904.56005859375 = 0.21856114268302917 + 100.0 * 9.043415069580078
Epoch 1510, val loss: 0.4194587469100952
Epoch 1520, training loss: 904.550537109375 = 0.2169455736875534 + 100.0 * 9.043335914611816
Epoch 1520, val loss: 0.41945043206214905
Epoch 1530, training loss: 904.958740234375 = 0.2153405100107193 + 100.0 * 9.047433853149414
Epoch 1530, val loss: 0.41948604583740234
Epoch 1540, training loss: 904.658203125 = 0.21366603672504425 + 100.0 * 9.044445037841797
Epoch 1540, val loss: 0.4202142059803009
Epoch 1550, training loss: 904.4916381835938 = 0.21201525628566742 + 100.0 * 9.04279613494873
Epoch 1550, val loss: 0.42061948776245117
Epoch 1560, training loss: 904.4857788085938 = 0.2103739231824875 + 100.0 * 9.042754173278809
Epoch 1560, val loss: 0.42069968581199646
Epoch 1570, training loss: 904.5372924804688 = 0.20874375104904175 + 100.0 * 9.043285369873047
Epoch 1570, val loss: 0.4210265576839447
Epoch 1580, training loss: 904.4366455078125 = 0.20707465708255768 + 100.0 * 9.042295455932617
Epoch 1580, val loss: 0.4213551878929138
Epoch 1590, training loss: 904.44140625 = 0.20542939007282257 + 100.0 * 9.042359352111816
Epoch 1590, val loss: 0.42171433568000793
Epoch 1600, training loss: 904.3850708007812 = 0.20379121601581573 + 100.0 * 9.041812896728516
Epoch 1600, val loss: 0.421936959028244
Epoch 1610, training loss: 904.5844116210938 = 0.2021913230419159 + 100.0 * 9.043822288513184
Epoch 1610, val loss: 0.42158278822898865
Epoch 1620, training loss: 904.5042114257812 = 0.20051756501197815 + 100.0 * 9.043037414550781
Epoch 1620, val loss: 0.4233739674091339
Epoch 1630, training loss: 904.4337158203125 = 0.19890300929546356 + 100.0 * 9.04234790802002
Epoch 1630, val loss: 0.42387768626213074
Epoch 1640, training loss: 904.3174438476562 = 0.19724629819393158 + 100.0 * 9.0412015914917
Epoch 1640, val loss: 0.42376890778541565
Epoch 1650, training loss: 904.2783813476562 = 0.19561555981636047 + 100.0 * 9.040827751159668
Epoch 1650, val loss: 0.4248279333114624
Epoch 1660, training loss: 904.2766723632812 = 0.19399294257164001 + 100.0 * 9.040826797485352
Epoch 1660, val loss: 0.4250466525554657
Epoch 1670, training loss: 904.4232788085938 = 0.19237768650054932 + 100.0 * 9.042308807373047
Epoch 1670, val loss: 0.4260104298591614
Epoch 1680, training loss: 904.2609252929688 = 0.1907375603914261 + 100.0 * 9.040701866149902
Epoch 1680, val loss: 0.4265916347503662
Epoch 1690, training loss: 904.2407836914062 = 0.18911559879779816 + 100.0 * 9.04051685333252
Epoch 1690, val loss: 0.4273475408554077
Epoch 1700, training loss: 904.24853515625 = 0.18747977912425995 + 100.0 * 9.040610313415527
Epoch 1700, val loss: 0.42775243520736694
Epoch 1710, training loss: 904.1763305664062 = 0.1858559399843216 + 100.0 * 9.039904594421387
Epoch 1710, val loss: 0.42843863368034363
Epoch 1720, training loss: 904.265625 = 0.1842452585697174 + 100.0 * 9.040813446044922
Epoch 1720, val loss: 0.4286203682422638
Epoch 1730, training loss: 904.2184448242188 = 0.1826513260602951 + 100.0 * 9.04035758972168
Epoch 1730, val loss: 0.43136894702911377
Epoch 1740, training loss: 904.2410278320312 = 0.18103335797786713 + 100.0 * 9.040599822998047
Epoch 1740, val loss: 0.43022289872169495
Epoch 1750, training loss: 904.0574340820312 = 0.1793845295906067 + 100.0 * 9.038780212402344
Epoch 1750, val loss: 0.43183526396751404
Epoch 1760, training loss: 904.0419921875 = 0.17780332267284393 + 100.0 * 9.038641929626465
Epoch 1760, val loss: 0.4330013394355774
Epoch 1770, training loss: 904.0365600585938 = 0.17621228098869324 + 100.0 * 9.038603782653809
Epoch 1770, val loss: 0.433533638715744
Epoch 1780, training loss: 904.40869140625 = 0.17465394735336304 + 100.0 * 9.042340278625488
Epoch 1780, val loss: 0.4345300495624542
Epoch 1790, training loss: 904.0682983398438 = 0.17304286360740662 + 100.0 * 9.038952827453613
Epoch 1790, val loss: 0.43571937084198
Epoch 1800, training loss: 903.968505859375 = 0.1714523434638977 + 100.0 * 9.037970542907715
Epoch 1800, val loss: 0.43662357330322266
Epoch 1810, training loss: 904.0756225585938 = 0.16988696157932281 + 100.0 * 9.039057731628418
Epoch 1810, val loss: 0.4373489320278168
Epoch 1820, training loss: 903.9628295898438 = 0.16832667589187622 + 100.0 * 9.037944793701172
Epoch 1820, val loss: 0.4391293525695801
Epoch 1830, training loss: 904.037109375 = 0.16679489612579346 + 100.0 * 9.038702964782715
Epoch 1830, val loss: 0.44042736291885376
Epoch 1840, training loss: 903.9301147460938 = 0.1651936024427414 + 100.0 * 9.037649154663086
Epoch 1840, val loss: 0.4407009184360504
Epoch 1850, training loss: 903.8980102539062 = 0.163641557097435 + 100.0 * 9.037343978881836
Epoch 1850, val loss: 0.4419580399990082
Epoch 1860, training loss: 903.9049682617188 = 0.1621483862400055 + 100.0 * 9.03742790222168
Epoch 1860, val loss: 0.4440053403377533
Epoch 1870, training loss: 904.0401611328125 = 0.16068913042545319 + 100.0 * 9.03879451751709
Epoch 1870, val loss: 0.4450855553150177
Epoch 1880, training loss: 903.8540649414062 = 0.1590898483991623 + 100.0 * 9.036949157714844
Epoch 1880, val loss: 0.44534096121788025
Epoch 1890, training loss: 903.8055419921875 = 0.1575579047203064 + 100.0 * 9.036479949951172
Epoch 1890, val loss: 0.4469242990016937
Epoch 1900, training loss: 903.955810546875 = 0.15606380999088287 + 100.0 * 9.037997245788574
Epoch 1900, val loss: 0.448059618473053
Epoch 1910, training loss: 903.8013916015625 = 0.15456750988960266 + 100.0 * 9.036468505859375
Epoch 1910, val loss: 0.4487580358982086
Epoch 1920, training loss: 903.9374389648438 = 0.15311448276042938 + 100.0 * 9.037842750549316
Epoch 1920, val loss: 0.4509021043777466
Epoch 1930, training loss: 903.8352661132812 = 0.1516096442937851 + 100.0 * 9.036836624145508
Epoch 1930, val loss: 0.45132532715797424
Epoch 1940, training loss: 903.7252807617188 = 0.15013322234153748 + 100.0 * 9.035751342773438
Epoch 1940, val loss: 0.45296287536621094
Epoch 1950, training loss: 903.6907348632812 = 0.14866942167282104 + 100.0 * 9.035420417785645
Epoch 1950, val loss: 0.4543873369693756
Epoch 1960, training loss: 903.673828125 = 0.14719629287719727 + 100.0 * 9.035265922546387
Epoch 1960, val loss: 0.4552421271800995
Epoch 1970, training loss: 903.69091796875 = 0.1457664966583252 + 100.0 * 9.035451889038086
Epoch 1970, val loss: 0.4570496082305908
Epoch 1980, training loss: 903.856689453125 = 0.14433181285858154 + 100.0 * 9.037123680114746
Epoch 1980, val loss: 0.45805177092552185
Epoch 1990, training loss: 903.8660888671875 = 0.14293718338012695 + 100.0 * 9.0372314453125
Epoch 1990, val loss: 0.4591695964336395
Epoch 2000, training loss: 903.6785888671875 = 0.1414901316165924 + 100.0 * 9.035370826721191
Epoch 2000, val loss: 0.4606066644191742
Epoch 2010, training loss: 903.6357421875 = 0.14008642733097076 + 100.0 * 9.034956932067871
Epoch 2010, val loss: 0.4628034234046936
Epoch 2020, training loss: 903.5874633789062 = 0.13866876065731049 + 100.0 * 9.0344877243042
Epoch 2020, val loss: 0.46385878324508667
Epoch 2030, training loss: 903.7726440429688 = 0.13728941977024078 + 100.0 * 9.036354064941406
Epoch 2030, val loss: 0.4648745357990265
Epoch 2040, training loss: 903.6890258789062 = 0.13595007359981537 + 100.0 * 9.035531044006348
Epoch 2040, val loss: 0.46766600012779236
Epoch 2050, training loss: 903.5623779296875 = 0.13451606035232544 + 100.0 * 9.034278869628906
Epoch 2050, val loss: 0.46723371744155884
Epoch 2060, training loss: 903.5191650390625 = 0.13313990831375122 + 100.0 * 9.033860206604004
Epoch 2060, val loss: 0.4695781469345093
Epoch 2070, training loss: 903.4901123046875 = 0.13177117705345154 + 100.0 * 9.033583641052246
Epoch 2070, val loss: 0.47088319063186646
Epoch 2080, training loss: 903.7353515625 = 0.13048884272575378 + 100.0 * 9.036048889160156
Epoch 2080, val loss: 0.47318288683891296
Epoch 2090, training loss: 903.6136474609375 = 0.1291193813085556 + 100.0 * 9.034845352172852
Epoch 2090, val loss: 0.472883939743042
Epoch 2100, training loss: 903.5569458007812 = 0.12780334055423737 + 100.0 * 9.03429126739502
Epoch 2100, val loss: 0.4764329195022583
Epoch 2110, training loss: 903.4467163085938 = 0.12642718851566315 + 100.0 * 9.033203125
Epoch 2110, val loss: 0.47704681754112244
Epoch 2120, training loss: 903.448486328125 = 0.12510794401168823 + 100.0 * 9.033233642578125
Epoch 2120, val loss: 0.478098064661026
Epoch 2130, training loss: 903.4651489257812 = 0.12379874289035797 + 100.0 * 9.033413887023926
Epoch 2130, val loss: 0.4803772270679474
Epoch 2140, training loss: 903.57666015625 = 0.1225474402308464 + 100.0 * 9.034541130065918
Epoch 2140, val loss: 0.4832034707069397
Epoch 2150, training loss: 903.400146484375 = 0.12121099978685379 + 100.0 * 9.03278923034668
Epoch 2150, val loss: 0.48295676708221436
Epoch 2160, training loss: 903.342041015625 = 0.11990709602832794 + 100.0 * 9.032220840454102
Epoch 2160, val loss: 0.4853154718875885
Epoch 2170, training loss: 903.3533935546875 = 0.11863493174314499 + 100.0 * 9.032347679138184
Epoch 2170, val loss: 0.4860781729221344
Epoch 2180, training loss: 903.749755859375 = 0.11745214462280273 + 100.0 * 9.036323547363281
Epoch 2180, val loss: 0.488237202167511
Epoch 2190, training loss: 903.3974609375 = 0.11613434553146362 + 100.0 * 9.03281307220459
Epoch 2190, val loss: 0.4904094338417053
Epoch 2200, training loss: 903.2832641601562 = 0.11487652361392975 + 100.0 * 9.031683921813965
Epoch 2200, val loss: 0.4925690293312073
Epoch 2210, training loss: 903.27099609375 = 0.11362887918949127 + 100.0 * 9.031573295593262
Epoch 2210, val loss: 0.49313148856163025
Epoch 2220, training loss: 903.2841186523438 = 0.11238770186901093 + 100.0 * 9.031717300415039
Epoch 2220, val loss: 0.4954206347465515
Epoch 2230, training loss: 903.5333862304688 = 0.11119617521762848 + 100.0 * 9.034221649169922
Epoch 2230, val loss: 0.4967546761035919
Epoch 2240, training loss: 903.396240234375 = 0.10999426990747452 + 100.0 * 9.032862663269043
Epoch 2240, val loss: 0.4995945692062378
Epoch 2250, training loss: 903.2537841796875 = 0.10874711722135544 + 100.0 * 9.031450271606445
Epoch 2250, val loss: 0.5011040568351746
Epoch 2260, training loss: 903.2474365234375 = 0.10753440111875534 + 100.0 * 9.03139877319336
Epoch 2260, val loss: 0.5032370686531067
Epoch 2270, training loss: 903.2479858398438 = 0.10634970664978027 + 100.0 * 9.031416893005371
Epoch 2270, val loss: 0.5050219297409058
Epoch 2280, training loss: 903.1602783203125 = 0.10514919459819794 + 100.0 * 9.030550956726074
Epoch 2280, val loss: 0.5056451559066772
Epoch 2290, training loss: 903.2470092773438 = 0.103998564183712 + 100.0 * 9.0314302444458
Epoch 2290, val loss: 0.5070431232452393
Epoch 2300, training loss: 903.261962890625 = 0.10286518931388855 + 100.0 * 9.031591415405273
Epoch 2300, val loss: 0.510813295841217
Epoch 2310, training loss: 903.1461181640625 = 0.10168050229549408 + 100.0 * 9.030444145202637
Epoch 2310, val loss: 0.5128629207611084
Epoch 2320, training loss: 903.1318969726562 = 0.10051599889993668 + 100.0 * 9.030313491821289
Epoch 2320, val loss: 0.5139808058738708
Epoch 2330, training loss: 903.142333984375 = 0.09942067414522171 + 100.0 * 9.030428886413574
Epoch 2330, val loss: 0.5171887874603271
Epoch 2340, training loss: 903.2735595703125 = 0.09835419058799744 + 100.0 * 9.031752586364746
Epoch 2340, val loss: 0.519180178642273
Epoch 2350, training loss: 903.283935546875 = 0.09718673676252365 + 100.0 * 9.031867980957031
Epoch 2350, val loss: 0.5195263028144836
Epoch 2360, training loss: 903.0950927734375 = 0.09607446938753128 + 100.0 * 9.029990196228027
Epoch 2360, val loss: 0.521484911441803
Epoch 2370, training loss: 903.0194091796875 = 0.09496776014566422 + 100.0 * 9.029244422912598
Epoch 2370, val loss: 0.5239610075950623
Epoch 2380, training loss: 903.0169677734375 = 0.0938837081193924 + 100.0 * 9.029231071472168
Epoch 2380, val loss: 0.5253946185112
Epoch 2390, training loss: 903.42431640625 = 0.09286309033632278 + 100.0 * 9.03331470489502
Epoch 2390, val loss: 0.5268392562866211
Epoch 2400, training loss: 903.1040649414062 = 0.0917678102850914 + 100.0 * 9.030122756958008
Epoch 2400, val loss: 0.5295462012290955
Epoch 2410, training loss: 902.9856567382812 = 0.09067639708518982 + 100.0 * 9.028949737548828
Epoch 2410, val loss: 0.531700849533081
Epoch 2420, training loss: 903.0596923828125 = 0.0896286591887474 + 100.0 * 9.029701232910156
Epoch 2420, val loss: 0.5327540636062622
Epoch 2430, training loss: 903.0053100585938 = 0.08858473598957062 + 100.0 * 9.029167175292969
Epoch 2430, val loss: 0.535396933555603
Epoch 2440, training loss: 902.9799194335938 = 0.0875486508011818 + 100.0 * 9.028923988342285
Epoch 2440, val loss: 0.5375115275382996
Epoch 2450, training loss: 902.9277954101562 = 0.08650850504636765 + 100.0 * 9.028412818908691
Epoch 2450, val loss: 0.5397937893867493
Epoch 2460, training loss: 903.0811157226562 = 0.08550800383090973 + 100.0 * 9.029955863952637
Epoch 2460, val loss: 0.541979193687439
Epoch 2470, training loss: 902.9420776367188 = 0.08449096232652664 + 100.0 * 9.028575897216797
Epoch 2470, val loss: 0.5442322492599487
Epoch 2480, training loss: 902.8919677734375 = 0.08348788321018219 + 100.0 * 9.028084754943848
Epoch 2480, val loss: 0.5465019941329956
Epoch 2490, training loss: 902.9299926757812 = 0.08257006853818893 + 100.0 * 9.028473854064941
Epoch 2490, val loss: 0.5498510003089905
Epoch 2500, training loss: 902.9913330078125 = 0.08163616061210632 + 100.0 * 9.029096603393555
Epoch 2500, val loss: 0.5516989827156067
Epoch 2510, training loss: 902.996337890625 = 0.08057861030101776 + 100.0 * 9.029157638549805
Epoch 2510, val loss: 0.551024854183197
Epoch 2520, training loss: 902.8869018554688 = 0.07964819669723511 + 100.0 * 9.028072357177734
Epoch 2520, val loss: 0.5560007095336914
Epoch 2530, training loss: 902.9326171875 = 0.07869286090135574 + 100.0 * 9.028539657592773
Epoch 2530, val loss: 0.5568408966064453
Epoch 2540, training loss: 902.8160400390625 = 0.07776018977165222 + 100.0 * 9.027382850646973
Epoch 2540, val loss: 0.5596551895141602
Epoch 2550, training loss: 902.84814453125 = 0.07682126015424728 + 100.0 * 9.02771282196045
Epoch 2550, val loss: 0.5612439513206482
Epoch 2560, training loss: 902.9014282226562 = 0.07596859335899353 + 100.0 * 9.028254508972168
Epoch 2560, val loss: 0.5647648572921753
Epoch 2570, training loss: 902.7822265625 = 0.07500012964010239 + 100.0 * 9.027071952819824
Epoch 2570, val loss: 0.564569890499115
Epoch 2580, training loss: 902.7410278320312 = 0.07409787178039551 + 100.0 * 9.0266695022583
Epoch 2580, val loss: 0.5677582621574402
Epoch 2590, training loss: 902.8499755859375 = 0.07325544953346252 + 100.0 * 9.027767181396484
Epoch 2590, val loss: 0.5705781579017639
Epoch 2600, training loss: 902.8207397460938 = 0.07235683500766754 + 100.0 * 9.027483940124512
Epoch 2600, val loss: 0.5709272027015686
Epoch 2610, training loss: 902.7390747070312 = 0.07145846635103226 + 100.0 * 9.026676177978516
Epoch 2610, val loss: 0.574988603591919
Epoch 2620, training loss: 902.8190307617188 = 0.070576511323452 + 100.0 * 9.027484893798828
Epoch 2620, val loss: 0.5761551856994629
Epoch 2630, training loss: 902.8567504882812 = 0.06978613883256912 + 100.0 * 9.027870178222656
Epoch 2630, val loss: 0.579868733882904
Epoch 2640, training loss: 902.68212890625 = 0.06886361539363861 + 100.0 * 9.026132583618164
Epoch 2640, val loss: 0.580475926399231
Epoch 2650, training loss: 902.657958984375 = 0.06803140044212341 + 100.0 * 9.025898933410645
Epoch 2650, val loss: 0.5838240385055542
Epoch 2660, training loss: 902.6349487304688 = 0.06718325614929199 + 100.0 * 9.025677680969238
Epoch 2660, val loss: 0.5844372510910034
Epoch 2670, training loss: 902.6951904296875 = 0.06644092500209808 + 100.0 * 9.026287078857422
Epoch 2670, val loss: 0.5892504453659058
Epoch 2680, training loss: 902.7151489257812 = 0.06565894931554794 + 100.0 * 9.026494979858398
Epoch 2680, val loss: 0.5907996892929077
Epoch 2690, training loss: 902.5965576171875 = 0.06481796503067017 + 100.0 * 9.025317192077637
Epoch 2690, val loss: 0.5928817391395569
Epoch 2700, training loss: 902.6285400390625 = 0.06402388215065002 + 100.0 * 9.02564525604248
Epoch 2700, val loss: 0.5932308435440063
Epoch 2710, training loss: 902.8246459960938 = 0.06331189721822739 + 100.0 * 9.027613639831543
Epoch 2710, val loss: 0.5968253016471863
Epoch 2720, training loss: 902.6425170898438 = 0.06253122538328171 + 100.0 * 9.025799751281738
Epoch 2720, val loss: 0.5985203981399536
Epoch 2730, training loss: 902.5986328125 = 0.06175727769732475 + 100.0 * 9.025368690490723
Epoch 2730, val loss: 0.6005853414535522
Epoch 2740, training loss: 902.6087646484375 = 0.06102675944566727 + 100.0 * 9.025477409362793
Epoch 2740, val loss: 0.6028404235839844
Epoch 2750, training loss: 902.6395263671875 = 0.06029202416539192 + 100.0 * 9.025792121887207
Epoch 2750, val loss: 0.6052509546279907
Epoch 2760, training loss: 902.5708618164062 = 0.059592604637145996 + 100.0 * 9.025113105773926
Epoch 2760, val loss: 0.6087934374809265
Epoch 2770, training loss: 902.553955078125 = 0.05885667726397514 + 100.0 * 9.024950981140137
Epoch 2770, val loss: 0.6106700897216797
Epoch 2780, training loss: 902.7243041992188 = 0.05826777219772339 + 100.0 * 9.026659965515137
Epoch 2780, val loss: 0.6148467063903809
Epoch 2790, training loss: 902.553466796875 = 0.05747169256210327 + 100.0 * 9.024959564208984
Epoch 2790, val loss: 0.6148453950881958
Epoch 2800, training loss: 902.5228881835938 = 0.05678602308034897 + 100.0 * 9.02466106414795
Epoch 2800, val loss: 0.6172773838043213
Epoch 2810, training loss: 902.5139770507812 = 0.05610926076769829 + 100.0 * 9.024579048156738
Epoch 2810, val loss: 0.619711697101593
Epoch 2820, training loss: 902.5116577148438 = 0.055464889854192734 + 100.0 * 9.024561882019043
Epoch 2820, val loss: 0.6226510405540466
Epoch 2830, training loss: 902.5987548828125 = 0.05481291189789772 + 100.0 * 9.025439262390137
Epoch 2830, val loss: 0.6230134963989258
Epoch 2840, training loss: 902.4463500976562 = 0.05413486063480377 + 100.0 * 9.023921966552734
Epoch 2840, val loss: 0.6258666515350342
Epoch 2850, training loss: 902.4318237304688 = 0.053493987768888474 + 100.0 * 9.023783683776855
Epoch 2850, val loss: 0.6283773183822632
Epoch 2860, training loss: 902.4684448242188 = 0.05286670848727226 + 100.0 * 9.024155616760254
Epoch 2860, val loss: 0.6312125325202942
Epoch 2870, training loss: 902.4878540039062 = 0.0522380992770195 + 100.0 * 9.0243558883667
Epoch 2870, val loss: 0.6327369213104248
Epoch 2880, training loss: 902.4434814453125 = 0.05161426588892937 + 100.0 * 9.023918151855469
Epoch 2880, val loss: 0.6336946487426758
Epoch 2890, training loss: 902.6309814453125 = 0.05105028674006462 + 100.0 * 9.025799751281738
Epoch 2890, val loss: 0.6347015500068665
Epoch 2900, training loss: 902.4201049804688 = 0.05041470378637314 + 100.0 * 9.023696899414062
Epoch 2900, val loss: 0.6401209235191345
Epoch 2910, training loss: 902.3701782226562 = 0.04980219528079033 + 100.0 * 9.02320384979248
Epoch 2910, val loss: 0.6411673426628113
Epoch 2920, training loss: 902.4205322265625 = 0.04921696335077286 + 100.0 * 9.023713111877441
Epoch 2920, val loss: 0.6445486545562744
Epoch 2930, training loss: 902.3834228515625 = 0.048633672297000885 + 100.0 * 9.023347854614258
Epoch 2930, val loss: 0.6453666687011719
Epoch 2940, training loss: 902.3381958007812 = 0.04806248098611832 + 100.0 * 9.02290153503418
Epoch 2940, val loss: 0.6486758589744568
Epoch 2950, training loss: 902.457763671875 = 0.047525808215141296 + 100.0 * 9.024102210998535
Epoch 2950, val loss: 0.652056097984314
Epoch 2960, training loss: 902.3162841796875 = 0.04695006087422371 + 100.0 * 9.022693634033203
Epoch 2960, val loss: 0.6521019339561462
Epoch 2970, training loss: 902.3067016601562 = 0.04639096558094025 + 100.0 * 9.022603034973145
Epoch 2970, val loss: 0.6546248197555542
Epoch 2980, training loss: 902.2813720703125 = 0.04585059732198715 + 100.0 * 9.022355079650879
Epoch 2980, val loss: 0.6570011377334595
Epoch 2990, training loss: 902.2744140625 = 0.04531135410070419 + 100.0 * 9.02229118347168
Epoch 2990, val loss: 0.6588594913482666
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8447
Overall ASR: 0.7317
Flip ASR: 0.6647/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.26 GiB already allocated; 289.69 MiB free; 5.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 401.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 467.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 467.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 745.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0015869140625 = 1.093468189239502 + 100.0 * 10.35908031463623
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 187.69 MiB free; 6.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.00341796875 = 1.0951136350631714 + 100.0 * 10.35908317565918
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 707.69 MiB free; 6.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0062255859375 = 1.0930330753326416 + 100.0 * 10.359131813049316
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 707.69 MiB free; 6.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0072021484375 = 1.103893518447876 + 100.0 * 10.35903263092041
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 729.69 MiB free; 6.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.9893798828125 = 1.096742033958435 + 100.0 * 10.358925819396973
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 729.69 MiB free; 6.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 745.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 745.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 745.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 467.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 465.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 351.69 MiB free; 5.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 465.69 MiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 743.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 743.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 743.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 425.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.996826171875 = 1.1078017950057983 + 100.0 * 10.358890533447266
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 147.69 MiB free; 6.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0079345703125 = 1.1063755750656128 + 100.0 * 10.359016418457031
Epoch 0, val loss: 1.1049226522445679
Epoch 10, training loss: 1036.579345703125 = 1.0950953960418701 + 100.0 * 10.354842185974121
Epoch 10, val loss: 1.093320608139038
Epoch 20, training loss: 1029.420166015625 = 1.0800718069076538 + 100.0 * 10.283400535583496
Epoch 20, val loss: 1.078230619430542
Epoch 30, training loss: 969.33837890625 = 1.0643022060394287 + 100.0 * 9.682740211486816
Epoch 30, val loss: 1.0626580715179443
Epoch 40, training loss: 953.1436767578125 = 1.0513627529144287 + 100.0 * 9.520922660827637
Epoch 40, val loss: 1.0503156185150146
Epoch 50, training loss: 943.2288818359375 = 1.0416761636734009 + 100.0 * 9.42187213897705
Epoch 50, val loss: 1.041027545928955
Epoch 60, training loss: 938.5017700195312 = 1.0326579809188843 + 100.0 * 9.374691009521484
Epoch 60, val loss: 1.0322672128677368
Epoch 70, training loss: 933.1566162109375 = 1.0263346433639526 + 100.0 * 9.321303367614746
Epoch 70, val loss: 1.0262081623077393
Epoch 80, training loss: 927.0023803710938 = 1.0219316482543945 + 100.0 * 9.259804725646973
Epoch 80, val loss: 1.0217763185501099
Epoch 90, training loss: 923.6743774414062 = 1.016063928604126 + 100.0 * 9.226583480834961
Epoch 90, val loss: 1.0157030820846558
Epoch 100, training loss: 921.388916015625 = 1.0084272623062134 + 100.0 * 9.203804969787598
Epoch 100, val loss: 1.00827157497406
Epoch 110, training loss: 919.683349609375 = 1.0011345148086548 + 100.0 * 9.186821937561035
Epoch 110, val loss: 1.00119948387146
Epoch 120, training loss: 918.6754760742188 = 0.9937398433685303 + 100.0 * 9.176817893981934
Epoch 120, val loss: 0.9938362836837769
Epoch 130, training loss: 917.7206420898438 = 0.9849794507026672 + 100.0 * 9.167356491088867
Epoch 130, val loss: 0.9852967858314514
Epoch 140, training loss: 916.822021484375 = 0.9756218791007996 + 100.0 * 9.158463478088379
Epoch 140, val loss: 0.976108193397522
Epoch 150, training loss: 916.0475463867188 = 0.9658612608909607 + 100.0 * 9.150816917419434
Epoch 150, val loss: 0.9665822386741638
Epoch 160, training loss: 915.4674682617188 = 0.9554132223129272 + 100.0 * 9.145120620727539
Epoch 160, val loss: 0.9563410878181458
Epoch 170, training loss: 914.9796752929688 = 0.9443504810333252 + 100.0 * 9.140353202819824
Epoch 170, val loss: 0.9456201791763306
Epoch 180, training loss: 914.489990234375 = 0.933114230632782 + 100.0 * 9.135568618774414
Epoch 180, val loss: 0.9346878528594971
Epoch 190, training loss: 914.1195678710938 = 0.9216719269752502 + 100.0 * 9.131978988647461
Epoch 190, val loss: 0.9236418008804321
Epoch 200, training loss: 913.673583984375 = 0.9096066355705261 + 100.0 * 9.127639770507812
Epoch 200, val loss: 0.9118778705596924
Epoch 210, training loss: 913.28369140625 = 0.8971511125564575 + 100.0 * 9.123865127563477
Epoch 210, val loss: 0.899810254573822
Epoch 220, training loss: 913.0305786132812 = 0.8844172954559326 + 100.0 * 9.121461868286133
Epoch 220, val loss: 0.8874582052230835
Epoch 230, training loss: 912.69921875 = 0.8709721565246582 + 100.0 * 9.118282318115234
Epoch 230, val loss: 0.874288022518158
Epoch 240, training loss: 912.3682250976562 = 0.8573001027107239 + 100.0 * 9.11510944366455
Epoch 240, val loss: 0.8610876798629761
Epoch 250, training loss: 912.2291870117188 = 0.8433658480644226 + 100.0 * 9.113858222961426
Epoch 250, val loss: 0.8476542234420776
Epoch 260, training loss: 911.8901977539062 = 0.8284596800804138 + 100.0 * 9.110617637634277
Epoch 260, val loss: 0.8332957625389099
Epoch 270, training loss: 911.6947021484375 = 0.8137970566749573 + 100.0 * 9.108809471130371
Epoch 270, val loss: 0.8192644715309143
Epoch 280, training loss: 911.46533203125 = 0.7992660403251648 + 100.0 * 9.106660842895508
Epoch 280, val loss: 0.8053571581840515
Epoch 290, training loss: 911.3534545898438 = 0.78401780128479 + 100.0 * 9.105694770812988
Epoch 290, val loss: 0.7907555103302002
Epoch 300, training loss: 911.1124877929688 = 0.7689476609230042 + 100.0 * 9.103435516357422
Epoch 300, val loss: 0.7765711545944214
Epoch 310, training loss: 910.9249877929688 = 0.7541400194168091 + 100.0 * 9.10170841217041
Epoch 310, val loss: 0.7626095414161682
Epoch 320, training loss: 910.71826171875 = 0.7394084334373474 + 100.0 * 9.099788665771484
Epoch 320, val loss: 0.7487557530403137
Epoch 330, training loss: 910.7850952148438 = 0.724583625793457 + 100.0 * 9.100605010986328
Epoch 330, val loss: 0.7348824143409729
Epoch 340, training loss: 910.4435424804688 = 0.709478497505188 + 100.0 * 9.09734058380127
Epoch 340, val loss: 0.7207829356193542
Epoch 350, training loss: 910.2557983398438 = 0.6951035857200623 + 100.0 * 9.095606803894043
Epoch 350, val loss: 0.7074816823005676
Epoch 360, training loss: 910.1246948242188 = 0.6811659336090088 + 100.0 * 9.094435691833496
Epoch 360, val loss: 0.6945399045944214
Epoch 370, training loss: 910.180419921875 = 0.6673309803009033 + 100.0 * 9.095130920410156
Epoch 370, val loss: 0.6817315220832825
Epoch 380, training loss: 909.9618530273438 = 0.6534172296524048 + 100.0 * 9.093084335327148
Epoch 380, val loss: 0.6689494252204895
Epoch 390, training loss: 909.7849731445312 = 0.640363872051239 + 100.0 * 9.091445922851562
Epoch 390, val loss: 0.6570871472358704
Epoch 400, training loss: 909.650634765625 = 0.6277273893356323 + 100.0 * 9.090229034423828
Epoch 400, val loss: 0.6455743312835693
Epoch 410, training loss: 909.697265625 = 0.6150226593017578 + 100.0 * 9.090822219848633
Epoch 410, val loss: 0.6339114904403687
Epoch 420, training loss: 909.50439453125 = 0.6027640700340271 + 100.0 * 9.08901596069336
Epoch 420, val loss: 0.6229761242866516
Epoch 430, training loss: 909.3507080078125 = 0.5916393995285034 + 100.0 * 9.087591171264648
Epoch 430, val loss: 0.6130000352859497
Epoch 440, training loss: 909.239501953125 = 0.5808196663856506 + 100.0 * 9.086586952209473
Epoch 440, val loss: 0.6033040881156921
Epoch 450, training loss: 909.197998046875 = 0.5703927278518677 + 100.0 * 9.086276054382324
Epoch 450, val loss: 0.593916654586792
Epoch 460, training loss: 909.22314453125 = 0.5595801472663879 + 100.0 * 9.08663558959961
Epoch 460, val loss: 0.5845102071762085
Epoch 470, training loss: 909.0084228515625 = 0.5499469041824341 + 100.0 * 9.084585189819336
Epoch 470, val loss: 0.5760380029678345
Epoch 480, training loss: 908.8943481445312 = 0.5409888029098511 + 100.0 * 9.08353328704834
Epoch 480, val loss: 0.5680379867553711
Epoch 490, training loss: 908.8896484375 = 0.5322673916816711 + 100.0 * 9.083573341369629
Epoch 490, val loss: 0.5602439641952515
Epoch 500, training loss: 908.75048828125 = 0.5235958695411682 + 100.0 * 9.082268714904785
Epoch 500, val loss: 0.5528061389923096
Epoch 510, training loss: 908.7781372070312 = 0.5155960321426392 + 100.0 * 9.082625389099121
Epoch 510, val loss: 0.5456175804138184
Epoch 520, training loss: 908.5906372070312 = 0.5078560709953308 + 100.0 * 9.080827713012695
Epoch 520, val loss: 0.5392128229141235
Epoch 530, training loss: 908.49365234375 = 0.5006322264671326 + 100.0 * 9.079930305480957
Epoch 530, val loss: 0.5329222679138184
Epoch 540, training loss: 908.4730224609375 = 0.4936888813972473 + 100.0 * 9.079792976379395
Epoch 540, val loss: 0.5268334746360779
Epoch 550, training loss: 908.4951171875 = 0.48661789298057556 + 100.0 * 9.080084800720215
Epoch 550, val loss: 0.5210584402084351
Epoch 560, training loss: 908.385498046875 = 0.48009413480758667 + 100.0 * 9.07905387878418
Epoch 560, val loss: 0.5154257416725159
Epoch 570, training loss: 908.2528076171875 = 0.4742092788219452 + 100.0 * 9.077786445617676
Epoch 570, val loss: 0.5105286836624146
Epoch 580, training loss: 908.1904296875 = 0.46847033500671387 + 100.0 * 9.07721996307373
Epoch 580, val loss: 0.5057740807533264
Epoch 590, training loss: 908.4832763671875 = 0.462744802236557 + 100.0 * 9.080204963684082
Epoch 590, val loss: 0.5009441375732422
Epoch 600, training loss: 908.1016845703125 = 0.45718303322792053 + 100.0 * 9.076445579528809
Epoch 600, val loss: 0.4964355528354645
Epoch 610, training loss: 908.002197265625 = 0.4523090720176697 + 100.0 * 9.075498580932617
Epoch 610, val loss: 0.4926223158836365
Epoch 620, training loss: 907.9559326171875 = 0.4475516974925995 + 100.0 * 9.07508373260498
Epoch 620, val loss: 0.48880061507225037
Epoch 630, training loss: 907.9699096679688 = 0.44295161962509155 + 100.0 * 9.07526969909668
Epoch 630, val loss: 0.48518726229667664
Epoch 640, training loss: 907.8673095703125 = 0.43816694617271423 + 100.0 * 9.074291229248047
Epoch 640, val loss: 0.481179416179657
Epoch 650, training loss: 907.8798828125 = 0.4339534342288971 + 100.0 * 9.074459075927734
Epoch 650, val loss: 0.478076308965683
Epoch 660, training loss: 907.7942504882812 = 0.4298236072063446 + 100.0 * 9.073644638061523
Epoch 660, val loss: 0.4748044013977051
Epoch 670, training loss: 907.7787475585938 = 0.42587459087371826 + 100.0 * 9.073528289794922
Epoch 670, val loss: 0.47166261076927185
Epoch 680, training loss: 907.6741943359375 = 0.4220719635486603 + 100.0 * 9.072521209716797
Epoch 680, val loss: 0.46895045042037964
Epoch 690, training loss: 907.7183227539062 = 0.41840144991874695 + 100.0 * 9.072999000549316
Epoch 690, val loss: 0.4661552309989929
Epoch 700, training loss: 907.5796508789062 = 0.41483402252197266 + 100.0 * 9.071647644042969
Epoch 700, val loss: 0.46355506777763367
Epoch 710, training loss: 907.529296875 = 0.411543071269989 + 100.0 * 9.07117748260498
Epoch 710, val loss: 0.46118566393852234
Epoch 720, training loss: 907.6701049804688 = 0.4082666337490082 + 100.0 * 9.07261848449707
Epoch 720, val loss: 0.4587022066116333
Epoch 730, training loss: 907.495361328125 = 0.40496304631233215 + 100.0 * 9.070903778076172
Epoch 730, val loss: 0.4564189612865448
Epoch 740, training loss: 907.4067993164062 = 0.4019988477230072 + 100.0 * 9.070048332214355
Epoch 740, val loss: 0.45426809787750244
Epoch 750, training loss: 907.4337768554688 = 0.3990909159183502 + 100.0 * 9.07034683227539
Epoch 750, val loss: 0.4521954357624054
Epoch 760, training loss: 907.3934936523438 = 0.39599108695983887 + 100.0 * 9.069974899291992
Epoch 760, val loss: 0.45020192861557007
Epoch 770, training loss: 907.3179321289062 = 0.3933025598526001 + 100.0 * 9.069246292114258
Epoch 770, val loss: 0.44833701848983765
Epoch 780, training loss: 907.2073364257812 = 0.39075276255607605 + 100.0 * 9.06816577911377
Epoch 780, val loss: 0.446664959192276
Epoch 790, training loss: 907.1599731445312 = 0.3882247805595398 + 100.0 * 9.067717552185059
Epoch 790, val loss: 0.44504258036613464
Epoch 800, training loss: 907.1217041015625 = 0.3857332468032837 + 100.0 * 9.067359924316406
Epoch 800, val loss: 0.44341233372688293
Epoch 810, training loss: 907.5054321289062 = 0.38316211104393005 + 100.0 * 9.071222305297852
Epoch 810, val loss: 0.44160521030426025
Epoch 820, training loss: 907.0398559570312 = 0.3806305527687073 + 100.0 * 9.0665922164917
Epoch 820, val loss: 0.44019171595573425
Epoch 830, training loss: 907.0255126953125 = 0.3784615695476532 + 100.0 * 9.0664701461792
Epoch 830, val loss: 0.43887314200401306
Epoch 840, training loss: 906.974365234375 = 0.37629276514053345 + 100.0 * 9.065980911254883
Epoch 840, val loss: 0.43752816319465637
Epoch 850, training loss: 907.053466796875 = 0.37412816286087036 + 100.0 * 9.066793441772461
Epoch 850, val loss: 0.4364314675331116
Epoch 860, training loss: 906.9437255859375 = 0.3717556893825531 + 100.0 * 9.065719604492188
Epoch 860, val loss: 0.4346540868282318
Epoch 870, training loss: 906.9259643554688 = 0.3697608411312103 + 100.0 * 9.06556224822998
Epoch 870, val loss: 0.43378859758377075
Epoch 880, training loss: 906.8726806640625 = 0.3677472770214081 + 100.0 * 9.065049171447754
Epoch 880, val loss: 0.43267786502838135
Epoch 890, training loss: 906.801025390625 = 0.36572694778442383 + 100.0 * 9.064352989196777
Epoch 890, val loss: 0.431370347738266
Epoch 900, training loss: 906.8256225585938 = 0.36381638050079346 + 100.0 * 9.064618110656738
Epoch 900, val loss: 0.43036603927612305
Epoch 910, training loss: 906.79736328125 = 0.3618515133857727 + 100.0 * 9.06435489654541
Epoch 910, val loss: 0.42930805683135986
Epoch 920, training loss: 906.760498046875 = 0.35999470949172974 + 100.0 * 9.064004898071289
Epoch 920, val loss: 0.4280490279197693
Epoch 930, training loss: 906.6371459960938 = 0.35816526412963867 + 100.0 * 9.062789916992188
Epoch 930, val loss: 0.42726364731788635
Epoch 940, training loss: 906.6326293945312 = 0.3564085066318512 + 100.0 * 9.062762260437012
Epoch 940, val loss: 0.4263935387134552
Epoch 950, training loss: 906.7848510742188 = 0.35461440682411194 + 100.0 * 9.064302444458008
Epoch 950, val loss: 0.42550379037857056
Epoch 960, training loss: 906.586669921875 = 0.35280343890190125 + 100.0 * 9.062338829040527
Epoch 960, val loss: 0.42451736330986023
Epoch 970, training loss: 906.5144653320312 = 0.35114070773124695 + 100.0 * 9.061633110046387
Epoch 970, val loss: 0.42377930879592896
Epoch 980, training loss: 906.5457153320312 = 0.3495095670223236 + 100.0 * 9.061962127685547
Epoch 980, val loss: 0.4231685698032379
Epoch 990, training loss: 906.5262451171875 = 0.3477751910686493 + 100.0 * 9.061784744262695
Epoch 990, val loss: 0.42217886447906494
Epoch 1000, training loss: 906.4067993164062 = 0.3461873531341553 + 100.0 * 9.060606002807617
Epoch 1000, val loss: 0.4215107858181
Epoch 1010, training loss: 906.379638671875 = 0.34462544322013855 + 100.0 * 9.06035041809082
Epoch 1010, val loss: 0.4207591712474823
Epoch 1020, training loss: 906.6915283203125 = 0.3430209457874298 + 100.0 * 9.063485145568848
Epoch 1020, val loss: 0.4200756251811981
Epoch 1030, training loss: 906.439697265625 = 0.34142476320266724 + 100.0 * 9.060982704162598
Epoch 1030, val loss: 0.41928958892822266
Epoch 1040, training loss: 906.2940063476562 = 0.3399321436882019 + 100.0 * 9.059540748596191
Epoch 1040, val loss: 0.4187680780887604
Epoch 1050, training loss: 906.3444213867188 = 0.33845481276512146 + 100.0 * 9.060059547424316
Epoch 1050, val loss: 0.41802817583084106
Epoch 1060, training loss: 906.2843017578125 = 0.33696427941322327 + 100.0 * 9.059473037719727
Epoch 1060, val loss: 0.4175485372543335
Epoch 1070, training loss: 906.3256225585938 = 0.3354995548725128 + 100.0 * 9.059901237487793
Epoch 1070, val loss: 0.4171498417854309
Epoch 1080, training loss: 906.1796875 = 0.33406156301498413 + 100.0 * 9.058456420898438
Epoch 1080, val loss: 0.4164387881755829
Epoch 1090, training loss: 906.1351928710938 = 0.332663893699646 + 100.0 * 9.058025360107422
Epoch 1090, val loss: 0.4157348573207855
Epoch 1100, training loss: 906.1329956054688 = 0.33127978444099426 + 100.0 * 9.058016777038574
Epoch 1100, val loss: 0.4151587188243866
Epoch 1110, training loss: 906.3758544921875 = 0.32985249161720276 + 100.0 * 9.060460090637207
Epoch 1110, val loss: 0.4146384000778198
Epoch 1120, training loss: 906.12255859375 = 0.3284235894680023 + 100.0 * 9.057941436767578
Epoch 1120, val loss: 0.4143121838569641
Epoch 1130, training loss: 906.0653686523438 = 0.3270905911922455 + 100.0 * 9.057382583618164
Epoch 1130, val loss: 0.41382890939712524
Epoch 1140, training loss: 906.0989990234375 = 0.3257879912853241 + 100.0 * 9.057731628417969
Epoch 1140, val loss: 0.41327643394470215
Epoch 1150, training loss: 905.96142578125 = 0.3244551122188568 + 100.0 * 9.05636978149414
Epoch 1150, val loss: 0.41261976957321167
Epoch 1160, training loss: 906.0331420898438 = 0.3231632113456726 + 100.0 * 9.057099342346191
Epoch 1160, val loss: 0.41194507479667664
Epoch 1170, training loss: 905.9842529296875 = 0.3218129873275757 + 100.0 * 9.056624412536621
Epoch 1170, val loss: 0.41174906492233276
Epoch 1180, training loss: 905.9969482421875 = 0.320524662733078 + 100.0 * 9.056764602661133
Epoch 1180, val loss: 0.41114744544029236
Epoch 1190, training loss: 905.9080810546875 = 0.319261759519577 + 100.0 * 9.055888175964355
Epoch 1190, val loss: 0.41104230284690857
Epoch 1200, training loss: 905.8500366210938 = 0.3180372416973114 + 100.0 * 9.055319786071777
Epoch 1200, val loss: 0.4104379713535309
Epoch 1210, training loss: 905.8046875 = 0.31679368019104004 + 100.0 * 9.054879188537598
Epoch 1210, val loss: 0.4100489318370819
Epoch 1220, training loss: 905.90625 = 0.3155575394630432 + 100.0 * 9.055907249450684
Epoch 1220, val loss: 0.40944376587867737
Epoch 1230, training loss: 905.8822021484375 = 0.31429383158683777 + 100.0 * 9.055679321289062
Epoch 1230, val loss: 0.40934881567955017
Epoch 1240, training loss: 905.8179321289062 = 0.3130570352077484 + 100.0 * 9.055048942565918
Epoch 1240, val loss: 0.40884101390838623
Epoch 1250, training loss: 905.7266235351562 = 0.31192922592163086 + 100.0 * 9.054146766662598
Epoch 1250, val loss: 0.4085156321525574
Epoch 1260, training loss: 905.743896484375 = 0.3107497990131378 + 100.0 * 9.05433177947998
Epoch 1260, val loss: 0.40833213925361633
Epoch 1270, training loss: 905.73828125 = 0.3095579445362091 + 100.0 * 9.05428695678711
Epoch 1270, val loss: 0.4077940285205841
Epoch 1280, training loss: 905.681884765625 = 0.3083880841732025 + 100.0 * 9.05373477935791
Epoch 1280, val loss: 0.40733426809310913
Epoch 1290, training loss: 905.6045532226562 = 0.3072378635406494 + 100.0 * 9.052972793579102
Epoch 1290, val loss: 0.40722814202308655
Epoch 1300, training loss: 905.6148071289062 = 0.30608153343200684 + 100.0 * 9.05308723449707
Epoch 1300, val loss: 0.40654706954956055
Epoch 1310, training loss: 905.619384765625 = 0.3048909902572632 + 100.0 * 9.053145408630371
Epoch 1310, val loss: 0.406450092792511
Epoch 1320, training loss: 905.5436401367188 = 0.3037670850753784 + 100.0 * 9.052398681640625
Epoch 1320, val loss: 0.4059642255306244
Epoch 1330, training loss: 905.5167236328125 = 0.3026939928531647 + 100.0 * 9.052140235900879
Epoch 1330, val loss: 0.40577730536460876
Epoch 1340, training loss: 905.525634765625 = 0.3015795648097992 + 100.0 * 9.052240371704102
Epoch 1340, val loss: 0.40543287992477417
Epoch 1350, training loss: 905.5479736328125 = 0.3004516363143921 + 100.0 * 9.052474975585938
Epoch 1350, val loss: 0.4051077663898468
Epoch 1360, training loss: 905.4939575195312 = 0.2993389666080475 + 100.0 * 9.051946640014648
Epoch 1360, val loss: 0.4049311876296997
Epoch 1370, training loss: 905.4298095703125 = 0.2982556223869324 + 100.0 * 9.051315307617188
Epoch 1370, val loss: 0.40462422370910645
Epoch 1380, training loss: 905.4697265625 = 0.2972069978713989 + 100.0 * 9.051725387573242
Epoch 1380, val loss: 0.4041804075241089
Epoch 1390, training loss: 905.4046020507812 = 0.2961221933364868 + 100.0 * 9.051084518432617
Epoch 1390, val loss: 0.4040294885635376
Epoch 1400, training loss: 905.4072265625 = 0.29505202174186707 + 100.0 * 9.051121711730957
Epoch 1400, val loss: 0.4036567807197571
Epoch 1410, training loss: 905.3848876953125 = 0.2939891219139099 + 100.0 * 9.050909042358398
Epoch 1410, val loss: 0.4033918082714081
Epoch 1420, training loss: 905.3329467773438 = 0.29293888807296753 + 100.0 * 9.050399780273438
Epoch 1420, val loss: 0.4033450782299042
Epoch 1430, training loss: 905.349609375 = 0.29189708828926086 + 100.0 * 9.050577163696289
Epoch 1430, val loss: 0.4029671251773834
Epoch 1440, training loss: 905.2699584960938 = 0.29087692499160767 + 100.0 * 9.049790382385254
Epoch 1440, val loss: 0.40286144614219666
Epoch 1450, training loss: 905.2501831054688 = 0.2898547947406769 + 100.0 * 9.049603462219238
Epoch 1450, val loss: 0.40270885825157166
Epoch 1460, training loss: 905.3430786132812 = 0.28882214426994324 + 100.0 * 9.050542831420898
Epoch 1460, val loss: 0.4023948013782501
Epoch 1470, training loss: 905.2352294921875 = 0.28780597448349 + 100.0 * 9.049474716186523
Epoch 1470, val loss: 0.4024638831615448
Epoch 1480, training loss: 905.337646484375 = 0.28681519627571106 + 100.0 * 9.050508499145508
Epoch 1480, val loss: 0.40234375
Epoch 1490, training loss: 905.150146484375 = 0.2857912480831146 + 100.0 * 9.048644065856934
Epoch 1490, val loss: 0.40194761753082275
Epoch 1500, training loss: 905.1219482421875 = 0.28481975197792053 + 100.0 * 9.048371315002441
Epoch 1500, val loss: 0.4018325209617615
Epoch 1510, training loss: 905.1422119140625 = 0.2838309407234192 + 100.0 * 9.048583984375
Epoch 1510, val loss: 0.40148791670799255
Epoch 1520, training loss: 905.0737915039062 = 0.28286105394363403 + 100.0 * 9.0479097366333
Epoch 1520, val loss: 0.40137600898742676
Epoch 1530, training loss: 905.2799682617188 = 0.2819041907787323 + 100.0 * 9.049980163574219
Epoch 1530, val loss: 0.40117648243904114
Epoch 1540, training loss: 905.0789184570312 = 0.2809125483036041 + 100.0 * 9.047980308532715
Epoch 1540, val loss: 0.4013623297214508
Epoch 1550, training loss: 905.0841674804688 = 0.27999576926231384 + 100.0 * 9.048042297363281
Epoch 1550, val loss: 0.40135136246681213
Epoch 1560, training loss: 905.037841796875 = 0.27900606393814087 + 100.0 * 9.047588348388672
Epoch 1560, val loss: 0.4008297920227051
Epoch 1570, training loss: 904.9876708984375 = 0.27806374430656433 + 100.0 * 9.047096252441406
Epoch 1570, val loss: 0.40083250403404236
Epoch 1580, training loss: 904.9411010742188 = 0.27714046835899353 + 100.0 * 9.046639442443848
Epoch 1580, val loss: 0.4007854759693146
Epoch 1590, training loss: 904.9763793945312 = 0.27619779109954834 + 100.0 * 9.047001838684082
Epoch 1590, val loss: 0.40069136023521423
Epoch 1600, training loss: 905.0128173828125 = 0.2752470076084137 + 100.0 * 9.047375679016113
Epoch 1600, val loss: 0.4005733132362366
Epoch 1610, training loss: 904.9174194335938 = 0.2743157744407654 + 100.0 * 9.046431541442871
Epoch 1610, val loss: 0.4002671539783478
Epoch 1620, training loss: 904.878662109375 = 0.27342280745506287 + 100.0 * 9.046051979064941
Epoch 1620, val loss: 0.4003446102142334
Epoch 1630, training loss: 904.8399658203125 = 0.2725130617618561 + 100.0 * 9.045674324035645
Epoch 1630, val loss: 0.4001583456993103
Epoch 1640, training loss: 904.9644775390625 = 0.2716028094291687 + 100.0 * 9.046928405761719
Epoch 1640, val loss: 0.4000551402568817
Epoch 1650, training loss: 904.8028564453125 = 0.2706987261772156 + 100.0 * 9.045321464538574
Epoch 1650, val loss: 0.4002417325973511
Epoch 1660, training loss: 904.7708129882812 = 0.2698046863079071 + 100.0 * 9.045010566711426
Epoch 1660, val loss: 0.3999745845794678
Epoch 1670, training loss: 904.9013671875 = 0.2689383924007416 + 100.0 * 9.046324729919434
Epoch 1670, val loss: 0.4002710282802582
Epoch 1680, training loss: 904.8080444335938 = 0.26801684498786926 + 100.0 * 9.045400619506836
Epoch 1680, val loss: 0.399722695350647
Epoch 1690, training loss: 904.817138671875 = 0.2671445906162262 + 100.0 * 9.045499801635742
Epoch 1690, val loss: 0.399798721075058
Epoch 1700, training loss: 904.8654174804688 = 0.2662650942802429 + 100.0 * 9.045991897583008
Epoch 1700, val loss: 0.3996507227420807
Epoch 1710, training loss: 904.6943969726562 = 0.26542213559150696 + 100.0 * 9.044289588928223
Epoch 1710, val loss: 0.39985814690589905
Epoch 1720, training loss: 904.6485595703125 = 0.26454418897628784 + 100.0 * 9.043840408325195
Epoch 1720, val loss: 0.3995618224143982
Epoch 1730, training loss: 904.6190185546875 = 0.2636886239051819 + 100.0 * 9.043553352355957
Epoch 1730, val loss: 0.3994656801223755
Epoch 1740, training loss: 904.8390502929688 = 0.2628222405910492 + 100.0 * 9.045762062072754
Epoch 1740, val loss: 0.39948567748069763
Epoch 1750, training loss: 904.6744995117188 = 0.2619471251964569 + 100.0 * 9.0441255569458
Epoch 1750, val loss: 0.39958107471466064
Epoch 1760, training loss: 904.6365966796875 = 0.26111066341400146 + 100.0 * 9.043754577636719
Epoch 1760, val loss: 0.3996414542198181
Epoch 1770, training loss: 904.6666259765625 = 0.26027384400367737 + 100.0 * 9.044063568115234
Epoch 1770, val loss: 0.39978504180908203
Epoch 1780, training loss: 904.6441650390625 = 0.2594490349292755 + 100.0 * 9.04384708404541
Epoch 1780, val loss: 0.39945071935653687
Epoch 1790, training loss: 904.5892333984375 = 0.2586054503917694 + 100.0 * 9.043306350708008
Epoch 1790, val loss: 0.3996388912200928
Epoch 1800, training loss: 904.493408203125 = 0.25777187943458557 + 100.0 * 9.042356491088867
Epoch 1800, val loss: 0.39946624636650085
Epoch 1810, training loss: 904.4872436523438 = 0.25695908069610596 + 100.0 * 9.042303085327148
Epoch 1810, val loss: 0.3993893563747406
Epoch 1820, training loss: 904.564453125 = 0.25612637400627136 + 100.0 * 9.043083190917969
Epoch 1820, val loss: 0.3993748724460602
Epoch 1830, training loss: 904.4742431640625 = 0.25530433654785156 + 100.0 * 9.042189598083496
Epoch 1830, val loss: 0.39956697821617126
Epoch 1840, training loss: 904.522216796875 = 0.25449368357658386 + 100.0 * 9.04267692565918
Epoch 1840, val loss: 0.39962127804756165
Epoch 1850, training loss: 904.443359375 = 0.2536856532096863 + 100.0 * 9.04189682006836
Epoch 1850, val loss: 0.3998572528362274
Epoch 1860, training loss: 904.4837646484375 = 0.2528798282146454 + 100.0 * 9.042308807373047
Epoch 1860, val loss: 0.3999786376953125
Epoch 1870, training loss: 904.372802734375 = 0.25207218527793884 + 100.0 * 9.041207313537598
Epoch 1870, val loss: 0.399787575006485
Epoch 1880, training loss: 904.6679077148438 = 0.25127917528152466 + 100.0 * 9.044166564941406
Epoch 1880, val loss: 0.39971494674682617
Epoch 1890, training loss: 904.4600219726562 = 0.2504837214946747 + 100.0 * 9.042095184326172
Epoch 1890, val loss: 0.4001953601837158
Epoch 1900, training loss: 904.3314208984375 = 0.2497083693742752 + 100.0 * 9.040817260742188
Epoch 1900, val loss: 0.4000483751296997
Epoch 1910, training loss: 904.2789306640625 = 0.24892733991146088 + 100.0 * 9.040300369262695
Epoch 1910, val loss: 0.40034034848213196
Epoch 1920, training loss: 904.3192138671875 = 0.24814750254154205 + 100.0 * 9.04071044921875
Epoch 1920, val loss: 0.4004453122615814
Epoch 1930, training loss: 904.3845825195312 = 0.2473745495080948 + 100.0 * 9.041372299194336
Epoch 1930, val loss: 0.40063348412513733
Epoch 1940, training loss: 904.2874755859375 = 0.24658749997615814 + 100.0 * 9.040409088134766
Epoch 1940, val loss: 0.40058961510658264
Epoch 1950, training loss: 904.3715209960938 = 0.24583709239959717 + 100.0 * 9.04125690460205
Epoch 1950, val loss: 0.4006171226501465
Epoch 1960, training loss: 904.3008422851562 = 0.24504083395004272 + 100.0 * 9.040557861328125
Epoch 1960, val loss: 0.40110450983047485
Epoch 1970, training loss: 904.2335205078125 = 0.2442973256111145 + 100.0 * 9.039892196655273
Epoch 1970, val loss: 0.4010603725910187
Epoch 1980, training loss: 904.2413330078125 = 0.24354465305805206 + 100.0 * 9.03997802734375
Epoch 1980, val loss: 0.4014033377170563
Epoch 1990, training loss: 904.1881713867188 = 0.24277742207050323 + 100.0 * 9.039453506469727
Epoch 1990, val loss: 0.4014153778553009
Epoch 2000, training loss: 904.1594848632812 = 0.24202308058738708 + 100.0 * 9.039175033569336
Epoch 2000, val loss: 0.40123528242111206
Epoch 2010, training loss: 904.2764282226562 = 0.24127312004566193 + 100.0 * 9.040351867675781
Epoch 2010, val loss: 0.4013817608356476
Epoch 2020, training loss: 904.1669921875 = 0.2405223846435547 + 100.0 * 9.039264678955078
Epoch 2020, val loss: 0.4018005132675171
Epoch 2030, training loss: 904.098876953125 = 0.2397882491350174 + 100.0 * 9.038590431213379
Epoch 2030, val loss: 0.4019921123981476
Epoch 2040, training loss: 904.111083984375 = 0.23904509842395782 + 100.0 * 9.03872013092041
Epoch 2040, val loss: 0.4021463394165039
Epoch 2050, training loss: 904.1781005859375 = 0.23830363154411316 + 100.0 * 9.039398193359375
Epoch 2050, val loss: 0.4023205041885376
Epoch 2060, training loss: 904.235107421875 = 0.2375897318124771 + 100.0 * 9.0399751663208
Epoch 2060, val loss: 0.40211087465286255
Epoch 2070, training loss: 904.0936889648438 = 0.23684008419513702 + 100.0 * 9.038568496704102
Epoch 2070, val loss: 0.402664452791214
Epoch 2080, training loss: 904.0667724609375 = 0.23614750802516937 + 100.0 * 9.03830623626709
Epoch 2080, val loss: 0.4029931128025055
Epoch 2090, training loss: 904.057373046875 = 0.23541297018527985 + 100.0 * 9.038219451904297
Epoch 2090, val loss: 0.4027535021305084
Epoch 2100, training loss: 904.0780639648438 = 0.23469114303588867 + 100.0 * 9.038434028625488
Epoch 2100, val loss: 0.4032427668571472
Epoch 2110, training loss: 904.0079345703125 = 0.23395881056785583 + 100.0 * 9.037739753723145
Epoch 2110, val loss: 0.4036547839641571
Epoch 2120, training loss: 904.0125732421875 = 0.23325906693935394 + 100.0 * 9.037793159484863
Epoch 2120, val loss: 0.4039655923843384
Epoch 2130, training loss: 904.0034790039062 = 0.23253627121448517 + 100.0 * 9.03770923614502
Epoch 2130, val loss: 0.40415629744529724
Epoch 2140, training loss: 903.9354858398438 = 0.23182469606399536 + 100.0 * 9.037036895751953
Epoch 2140, val loss: 0.4044107496738434
Epoch 2150, training loss: 903.958740234375 = 0.23111382126808167 + 100.0 * 9.037276268005371
Epoch 2150, val loss: 0.40454089641571045
Epoch 2160, training loss: 904.0244140625 = 0.2304006814956665 + 100.0 * 9.03794002532959
Epoch 2160, val loss: 0.4045238792896271
Epoch 2170, training loss: 903.9613037109375 = 0.22969114780426025 + 100.0 * 9.03731632232666
Epoch 2170, val loss: 0.4050195515155792
Epoch 2180, training loss: 903.9935302734375 = 0.22898857295513153 + 100.0 * 9.03764533996582
Epoch 2180, val loss: 0.4052969813346863
Epoch 2190, training loss: 903.8533325195312 = 0.228318989276886 + 100.0 * 9.036250114440918
Epoch 2190, val loss: 0.40585339069366455
Epoch 2200, training loss: 903.8230590820312 = 0.22762291133403778 + 100.0 * 9.035954475402832
Epoch 2200, val loss: 0.40597695112228394
Epoch 2210, training loss: 903.7981567382812 = 0.22692276537418365 + 100.0 * 9.035712242126465
Epoch 2210, val loss: 0.40623989701271057
Epoch 2220, training loss: 903.9926147460938 = 0.22623324394226074 + 100.0 * 9.037663459777832
Epoch 2220, val loss: 0.4062330722808838
Epoch 2230, training loss: 903.8981323242188 = 0.2255469113588333 + 100.0 * 9.036725997924805
Epoch 2230, val loss: 0.4070545732975006
Epoch 2240, training loss: 903.857177734375 = 0.22488032281398773 + 100.0 * 9.036323547363281
Epoch 2240, val loss: 0.4070323407649994
Epoch 2250, training loss: 903.7676391601562 = 0.2242124229669571 + 100.0 * 9.03543472290039
Epoch 2250, val loss: 0.40760934352874756
Epoch 2260, training loss: 903.7606201171875 = 0.2235347479581833 + 100.0 * 9.035370826721191
Epoch 2260, val loss: 0.4078005850315094
Epoch 2270, training loss: 903.9373779296875 = 0.2228657305240631 + 100.0 * 9.037145614624023
Epoch 2270, val loss: 0.40828385949134827
Epoch 2280, training loss: 903.8431396484375 = 0.22220416367053986 + 100.0 * 9.036209106445312
Epoch 2280, val loss: 0.4089218080043793
Epoch 2290, training loss: 903.759765625 = 0.22155652940273285 + 100.0 * 9.035382270812988
Epoch 2290, val loss: 0.40878400206565857
Epoch 2300, training loss: 903.7525024414062 = 0.22087594866752625 + 100.0 * 9.035316467285156
Epoch 2300, val loss: 0.40932199358940125
Epoch 2310, training loss: 903.7229614257812 = 0.2202187329530716 + 100.0 * 9.035027503967285
Epoch 2310, val loss: 0.4096689522266388
Epoch 2320, training loss: 903.676025390625 = 0.21956604719161987 + 100.0 * 9.034564971923828
Epoch 2320, val loss: 0.4104498326778412
Epoch 2330, training loss: 903.7363891601562 = 0.21889711916446686 + 100.0 * 9.035175323486328
Epoch 2330, val loss: 0.4106827974319458
Epoch 2340, training loss: 903.7042236328125 = 0.21824292838573456 + 100.0 * 9.034859657287598
Epoch 2340, val loss: 0.41086405515670776
Epoch 2350, training loss: 903.714599609375 = 0.2175985872745514 + 100.0 * 9.0349702835083
Epoch 2350, val loss: 0.41150540113449097
Epoch 2360, training loss: 903.6946411132812 = 0.21694983541965485 + 100.0 * 9.03477668762207
Epoch 2360, val loss: 0.4118323028087616
Epoch 2370, training loss: 903.6345825195312 = 0.2163109928369522 + 100.0 * 9.03418254852295
Epoch 2370, val loss: 0.41219785809516907
Epoch 2380, training loss: 903.5701293945312 = 0.2156827300786972 + 100.0 * 9.033544540405273
Epoch 2380, val loss: 0.41301771998405457
Epoch 2390, training loss: 903.7965698242188 = 0.2150557041168213 + 100.0 * 9.035815238952637
Epoch 2390, val loss: 0.41362109780311584
Epoch 2400, training loss: 903.5950317382812 = 0.21439427137374878 + 100.0 * 9.033805847167969
Epoch 2400, val loss: 0.41340696811676025
Epoch 2410, training loss: 903.5352172851562 = 0.2137642800807953 + 100.0 * 9.033214569091797
Epoch 2410, val loss: 0.4139920771121979
Epoch 2420, training loss: 903.5267944335938 = 0.21312811970710754 + 100.0 * 9.033136367797852
Epoch 2420, val loss: 0.4143829941749573
Epoch 2430, training loss: 903.819091796875 = 0.21249352395534515 + 100.0 * 9.036066055297852
Epoch 2430, val loss: 0.41484183073043823
Epoch 2440, training loss: 903.604248046875 = 0.21188026666641235 + 100.0 * 9.033924102783203
Epoch 2440, val loss: 0.41543716192245483
Epoch 2450, training loss: 903.5968017578125 = 0.21126562356948853 + 100.0 * 9.033855438232422
Epoch 2450, val loss: 0.4160492718219757
Epoch 2460, training loss: 903.49853515625 = 0.21063171327114105 + 100.0 * 9.032878875732422
Epoch 2460, val loss: 0.4163593649864197
Epoch 2470, training loss: 903.4927368164062 = 0.21000030636787415 + 100.0 * 9.032827377319336
Epoch 2470, val loss: 0.41655611991882324
Epoch 2480, training loss: 903.4915771484375 = 0.20939069986343384 + 100.0 * 9.032821655273438
Epoch 2480, val loss: 0.41705626249313354
Epoch 2490, training loss: 903.513916015625 = 0.2087676078081131 + 100.0 * 9.033051490783691
Epoch 2490, val loss: 0.41781654953956604
Epoch 2500, training loss: 903.5274658203125 = 0.20815682411193848 + 100.0 * 9.033193588256836
Epoch 2500, val loss: 0.4184611737728119
Epoch 2510, training loss: 903.4386596679688 = 0.20755010843276978 + 100.0 * 9.032310485839844
Epoch 2510, val loss: 0.41883087158203125
Epoch 2520, training loss: 903.4552001953125 = 0.20692820847034454 + 100.0 * 9.032483100891113
Epoch 2520, val loss: 0.4193701148033142
Epoch 2530, training loss: 903.430419921875 = 0.20633086562156677 + 100.0 * 9.032240867614746
Epoch 2530, val loss: 0.4196847081184387
Epoch 2540, training loss: 903.4072265625 = 0.2057289183139801 + 100.0 * 9.032014846801758
Epoch 2540, val loss: 0.41987061500549316
Epoch 2550, training loss: 903.5263061523438 = 0.20513387024402618 + 100.0 * 9.033211708068848
Epoch 2550, val loss: 0.4207230508327484
Epoch 2560, training loss: 903.4234619140625 = 0.20455783605575562 + 100.0 * 9.03218936920166
Epoch 2560, val loss: 0.4219492971897125
Epoch 2570, training loss: 903.3331298828125 = 0.203931987285614 + 100.0 * 9.031291961669922
Epoch 2570, val loss: 0.42213547229766846
Epoch 2580, training loss: 903.2933349609375 = 0.20333066582679749 + 100.0 * 9.030900001525879
Epoch 2580, val loss: 0.4224931001663208
Epoch 2590, training loss: 903.3810424804688 = 0.20273542404174805 + 100.0 * 9.031783103942871
Epoch 2590, val loss: 0.42301344871520996
Epoch 2600, training loss: 903.3460083007812 = 0.20213930308818817 + 100.0 * 9.031438827514648
Epoch 2600, val loss: 0.4237034022808075
Epoch 2610, training loss: 903.28076171875 = 0.2015565186738968 + 100.0 * 9.030792236328125
Epoch 2610, val loss: 0.42432525753974915
Epoch 2620, training loss: 903.2929077148438 = 0.2009691298007965 + 100.0 * 9.030919075012207
Epoch 2620, val loss: 0.4248497784137726
Epoch 2630, training loss: 903.3380737304688 = 0.20038747787475586 + 100.0 * 9.031376838684082
Epoch 2630, val loss: 0.4257250428199768
Epoch 2640, training loss: 903.3575439453125 = 0.1998153179883957 + 100.0 * 9.031577110290527
Epoch 2640, val loss: 0.4262985289096832
Epoch 2650, training loss: 903.2618408203125 = 0.1992027759552002 + 100.0 * 9.03062629699707
Epoch 2650, val loss: 0.426155686378479
Epoch 2660, training loss: 903.22021484375 = 0.1986338496208191 + 100.0 * 9.030216217041016
Epoch 2660, val loss: 0.4268072843551636
Epoch 2670, training loss: 903.3200073242188 = 0.1980588436126709 + 100.0 * 9.031219482421875
Epoch 2670, val loss: 0.4275771379470825
Epoch 2680, training loss: 903.2616577148438 = 0.19748255610466003 + 100.0 * 9.030641555786133
Epoch 2680, val loss: 0.42800673842430115
Epoch 2690, training loss: 903.2514038085938 = 0.19692757725715637 + 100.0 * 9.030545234680176
Epoch 2690, val loss: 0.42899808287620544
Epoch 2700, training loss: 903.280517578125 = 0.19634830951690674 + 100.0 * 9.030841827392578
Epoch 2700, val loss: 0.4293418824672699
Epoch 2710, training loss: 903.191650390625 = 0.19577042758464813 + 100.0 * 9.029958724975586
Epoch 2710, val loss: 0.42978665232658386
Epoch 2720, training loss: 903.2023315429688 = 0.19519701600074768 + 100.0 * 9.030071258544922
Epoch 2720, val loss: 0.43026426434516907
Epoch 2730, training loss: 903.2177734375 = 0.19463960826396942 + 100.0 * 9.030231475830078
Epoch 2730, val loss: 0.4309499263763428
Epoch 2740, training loss: 903.2578735351562 = 0.19408616423606873 + 100.0 * 9.030637741088867
Epoch 2740, val loss: 0.4318937063217163
Epoch 2750, training loss: 903.1275634765625 = 0.19350628554821014 + 100.0 * 9.029340744018555
Epoch 2750, val loss: 0.43248236179351807
Epoch 2760, training loss: 903.1384887695312 = 0.19296897947788239 + 100.0 * 9.029455184936523
Epoch 2760, val loss: 0.4335865378379822
Epoch 2770, training loss: 903.1484375 = 0.1924150437116623 + 100.0 * 9.029560089111328
Epoch 2770, val loss: 0.434034138917923
Epoch 2780, training loss: 903.1350708007812 = 0.19183428585529327 + 100.0 * 9.02943229675293
Epoch 2780, val loss: 0.43412113189697266
Epoch 2790, training loss: 903.1433715820312 = 0.1912994533777237 + 100.0 * 9.029520988464355
Epoch 2790, val loss: 0.43516212701797485
Epoch 2800, training loss: 903.1485595703125 = 0.19073407351970673 + 100.0 * 9.02957820892334
Epoch 2800, val loss: 0.43524298071861267
Epoch 2810, training loss: 903.0761108398438 = 0.19019022583961487 + 100.0 * 9.02885913848877
Epoch 2810, val loss: 0.43625664710998535
Epoch 2820, training loss: 903.0844116210938 = 0.18964765965938568 + 100.0 * 9.028947830200195
Epoch 2820, val loss: 0.43633973598480225
Epoch 2830, training loss: 903.0194702148438 = 0.18908725678920746 + 100.0 * 9.028304100036621
Epoch 2830, val loss: 0.437318354845047
Epoch 2840, training loss: 903.167236328125 = 0.18856215476989746 + 100.0 * 9.029787063598633
Epoch 2840, val loss: 0.43727388978004456
Epoch 2850, training loss: 903.0595703125 = 0.18798483908176422 + 100.0 * 9.028716087341309
Epoch 2850, val loss: 0.439071923494339
Epoch 2860, training loss: 902.9741821289062 = 0.18744447827339172 + 100.0 * 9.027867317199707
Epoch 2860, val loss: 0.4390891492366791
Epoch 2870, training loss: 903.00146484375 = 0.18690600991249084 + 100.0 * 9.028145790100098
Epoch 2870, val loss: 0.4398101270198822
Epoch 2880, training loss: 903.1443481445312 = 0.18636754155158997 + 100.0 * 9.029580116271973
Epoch 2880, val loss: 0.44042709469795227
Epoch 2890, training loss: 902.9906005859375 = 0.18582160770893097 + 100.0 * 9.028047561645508
Epoch 2890, val loss: 0.4415777623653412
Epoch 2900, training loss: 902.9454345703125 = 0.18527783453464508 + 100.0 * 9.02760124206543
Epoch 2900, val loss: 0.4419158697128296
Epoch 2910, training loss: 903.1029052734375 = 0.18477146327495575 + 100.0 * 9.029181480407715
Epoch 2910, val loss: 0.4424491226673126
Epoch 2920, training loss: 903.0189819335938 = 0.18420656025409698 + 100.0 * 9.028347969055176
Epoch 2920, val loss: 0.4432627558708191
Epoch 2930, training loss: 902.9244995117188 = 0.18367645144462585 + 100.0 * 9.027408599853516
Epoch 2930, val loss: 0.4439839720726013
Epoch 2940, training loss: 902.935546875 = 0.1831413358449936 + 100.0 * 9.0275239944458
Epoch 2940, val loss: 0.4448782503604889
Epoch 2950, training loss: 903.0093994140625 = 0.18261852860450745 + 100.0 * 9.028267860412598
Epoch 2950, val loss: 0.4453607201576233
Epoch 2960, training loss: 902.8782958984375 = 0.18206806480884552 + 100.0 * 9.026962280273438
Epoch 2960, val loss: 0.44589635729789734
Epoch 2970, training loss: 902.861328125 = 0.181540384888649 + 100.0 * 9.026798248291016
Epoch 2970, val loss: 0.44682788848876953
Epoch 2980, training loss: 903.0510864257812 = 0.1810210943222046 + 100.0 * 9.028700828552246
Epoch 2980, val loss: 0.447476327419281
Epoch 2990, training loss: 902.92431640625 = 0.18050651252269745 + 100.0 * 9.027438163757324
Epoch 2990, val loss: 0.4475575089454651
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8488
Overall ASR: 0.7647
Flip ASR: 0.7066/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.26 GiB already allocated; 779.69 MiB free; 5.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 915.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 915.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 915.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.021240234375 = 1.1172086000442505 + 100.0 * 10.359040260314941
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 399.69 MiB free; 6.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0018310546875 = 1.102136254310608 + 100.0 * 10.358996391296387
Epoch 0, val loss: 1.1012955904006958
Epoch 10, training loss: 1036.5946044921875 = 1.0920158624649048 + 100.0 * 10.355025291442871
Epoch 10, val loss: 1.090791940689087
Epoch 20, training loss: 1029.3724365234375 = 1.0776792764663696 + 100.0 * 10.28294849395752
Epoch 20, val loss: 1.0761168003082275
Epoch 30, training loss: 960.1920166015625 = 1.0599980354309082 + 100.0 * 9.591320037841797
Epoch 30, val loss: 1.0581783056259155
Epoch 40, training loss: 947.900634765625 = 1.0446218252182007 + 100.0 * 9.468560218811035
Epoch 40, val loss: 1.0433754920959473
Epoch 50, training loss: 941.6690673828125 = 1.0312108993530273 + 100.0 * 9.406378746032715
Epoch 50, val loss: 1.0300363302230835
Epoch 60, training loss: 938.1507568359375 = 1.0166698694229126 + 100.0 * 9.37134075164795
Epoch 60, val loss: 1.0156619548797607
Epoch 70, training loss: 932.1785888671875 = 1.0062037706375122 + 100.0 * 9.311723709106445
Epoch 70, val loss: 1.005899429321289
Epoch 80, training loss: 925.7944946289062 = 1.0006725788116455 + 100.0 * 9.24793815612793
Epoch 80, val loss: 1.0004909038543701
Epoch 90, training loss: 921.6400146484375 = 0.993889570236206 + 100.0 * 9.206460952758789
Epoch 90, val loss: 0.9935441017150879
Epoch 100, training loss: 919.2933349609375 = 0.9848604798316956 + 100.0 * 9.183084487915039
Epoch 100, val loss: 0.9844144582748413
Epoch 110, training loss: 917.8811645507812 = 0.9729165434837341 + 100.0 * 9.169082641601562
Epoch 110, val loss: 0.9725099205970764
Epoch 120, training loss: 916.535888671875 = 0.960940957069397 + 100.0 * 9.155749320983887
Epoch 120, val loss: 0.9609168767929077
Epoch 130, training loss: 915.6320190429688 = 0.9511505961418152 + 100.0 * 9.146808624267578
Epoch 130, val loss: 0.951281726360321
Epoch 140, training loss: 914.9317016601562 = 0.9409341812133789 + 100.0 * 9.139907836914062
Epoch 140, val loss: 0.9410645365715027
Epoch 150, training loss: 914.379150390625 = 0.9285179972648621 + 100.0 * 9.134506225585938
Epoch 150, val loss: 0.9287526607513428
Epoch 160, training loss: 913.8995971679688 = 0.9146878123283386 + 100.0 * 9.129849433898926
Epoch 160, val loss: 0.9152743816375732
Epoch 170, training loss: 913.3771362304688 = 0.9010906219482422 + 100.0 * 9.124760627746582
Epoch 170, val loss: 0.9021598100662231
Epoch 180, training loss: 912.969970703125 = 0.8885789513587952 + 100.0 * 9.120814323425293
Epoch 180, val loss: 0.8900351524353027
Epoch 190, training loss: 912.2509765625 = 0.876529335975647 + 100.0 * 9.113744735717773
Epoch 190, val loss: 0.8785732984542847
Epoch 200, training loss: 911.5911865234375 = 0.8650162816047668 + 100.0 * 9.107261657714844
Epoch 200, val loss: 0.867637038230896
Epoch 210, training loss: 911.1710205078125 = 0.8530439138412476 + 100.0 * 9.103179931640625
Epoch 210, val loss: 0.856208860874176
Epoch 220, training loss: 910.6824951171875 = 0.8402317762374878 + 100.0 * 9.09842300415039
Epoch 220, val loss: 0.8437896370887756
Epoch 230, training loss: 910.378173828125 = 0.8260304927825928 + 100.0 * 9.095520973205566
Epoch 230, val loss: 0.8301348686218262
Epoch 240, training loss: 910.0263061523438 = 0.8111312985420227 + 100.0 * 9.092151641845703
Epoch 240, val loss: 0.8158289194107056
Epoch 250, training loss: 909.755859375 = 0.7960950136184692 + 100.0 * 9.089597702026367
Epoch 250, val loss: 0.8014919757843018
Epoch 260, training loss: 909.5480346679688 = 0.7804766297340393 + 100.0 * 9.087676048278809
Epoch 260, val loss: 0.7865625023841858
Epoch 270, training loss: 909.348876953125 = 0.7644597291946411 + 100.0 * 9.085844039916992
Epoch 270, val loss: 0.7713626027107239
Epoch 280, training loss: 909.1417236328125 = 0.7486882209777832 + 100.0 * 9.083930015563965
Epoch 280, val loss: 0.7564308047294617
Epoch 290, training loss: 908.9380493164062 = 0.733073353767395 + 100.0 * 9.082049369812012
Epoch 290, val loss: 0.7416483759880066
Epoch 300, training loss: 909.0203857421875 = 0.717258870601654 + 100.0 * 9.083030700683594
Epoch 300, val loss: 0.7266110777854919
Epoch 310, training loss: 908.6005249023438 = 0.7009550929069519 + 100.0 * 9.078995704650879
Epoch 310, val loss: 0.7113644480705261
Epoch 320, training loss: 908.454345703125 = 0.6856265664100647 + 100.0 * 9.07768726348877
Epoch 320, val loss: 0.6970111131668091
Epoch 330, training loss: 908.2460327148438 = 0.6709105372428894 + 100.0 * 9.075751304626465
Epoch 330, val loss: 0.6832784414291382
Epoch 340, training loss: 908.0977172851562 = 0.6564756035804749 + 100.0 * 9.07441234588623
Epoch 340, val loss: 0.6698268055915833
Epoch 350, training loss: 907.956298828125 = 0.6418853402137756 + 100.0 * 9.07314395904541
Epoch 350, val loss: 0.6561485528945923
Epoch 360, training loss: 907.8058471679688 = 0.6274245381355286 + 100.0 * 9.071784019470215
Epoch 360, val loss: 0.6428239345550537
Epoch 370, training loss: 907.67822265625 = 0.6139454245567322 + 100.0 * 9.070642471313477
Epoch 370, val loss: 0.6304448843002319
Epoch 380, training loss: 907.5827026367188 = 0.6010059714317322 + 100.0 * 9.069816589355469
Epoch 380, val loss: 0.6186184287071228
Epoch 390, training loss: 907.5693969726562 = 0.5881618857383728 + 100.0 * 9.069812774658203
Epoch 390, val loss: 0.6067512035369873
Epoch 400, training loss: 907.3087768554688 = 0.5757221579551697 + 100.0 * 9.067330360412598
Epoch 400, val loss: 0.5954791903495789
Epoch 410, training loss: 907.1962890625 = 0.5642554759979248 + 100.0 * 9.066320419311523
Epoch 410, val loss: 0.585107147693634
Epoch 420, training loss: 907.1139526367188 = 0.5533480644226074 + 100.0 * 9.065606117248535
Epoch 420, val loss: 0.57530277967453
Epoch 430, training loss: 907.0021362304688 = 0.5425640940666199 + 100.0 * 9.064596176147461
Epoch 430, val loss: 0.5654282569885254
Epoch 440, training loss: 906.91455078125 = 0.5323230028152466 + 100.0 * 9.063821792602539
Epoch 440, val loss: 0.556342363357544
Epoch 450, training loss: 906.776123046875 = 0.5228790640830994 + 100.0 * 9.062532424926758
Epoch 450, val loss: 0.5479863286018372
Epoch 460, training loss: 906.7802734375 = 0.5139893293380737 + 100.0 * 9.062663078308105
Epoch 460, val loss: 0.5401446223258972
Epoch 470, training loss: 906.726318359375 = 0.5051398873329163 + 100.0 * 9.062211990356445
Epoch 470, val loss: 0.5323141813278198
Epoch 480, training loss: 906.5213623046875 = 0.4969044029712677 + 100.0 * 9.0602445602417
Epoch 480, val loss: 0.5251407027244568
Epoch 490, training loss: 906.4323120117188 = 0.4893171191215515 + 100.0 * 9.059430122375488
Epoch 490, val loss: 0.5185264945030212
Epoch 500, training loss: 906.3651123046875 = 0.48215386271476746 + 100.0 * 9.058829307556152
Epoch 500, val loss: 0.5123066902160645
Epoch 510, training loss: 906.3043212890625 = 0.4751087725162506 + 100.0 * 9.058292388916016
Epoch 510, val loss: 0.5061097145080566
Epoch 520, training loss: 906.3344116210938 = 0.46836185455322266 + 100.0 * 9.058660507202148
Epoch 520, val loss: 0.5002660751342773
Epoch 530, training loss: 906.1840209960938 = 0.4619595408439636 + 100.0 * 9.057220458984375
Epoch 530, val loss: 0.49501413106918335
Epoch 540, training loss: 906.0490112304688 = 0.45610207319259644 + 100.0 * 9.055929183959961
Epoch 540, val loss: 0.4900311231613159
Epoch 550, training loss: 905.990966796875 = 0.45066624879837036 + 100.0 * 9.055402755737305
Epoch 550, val loss: 0.48545199632644653
Epoch 560, training loss: 906.0310668945312 = 0.445493221282959 + 100.0 * 9.055855751037598
Epoch 560, val loss: 0.4810631275177002
Epoch 570, training loss: 905.9522705078125 = 0.4402526617050171 + 100.0 * 9.055120468139648
Epoch 570, val loss: 0.47701016068458557
Epoch 580, training loss: 905.8445434570312 = 0.4354708790779114 + 100.0 * 9.05409049987793
Epoch 580, val loss: 0.47297197580337524
Epoch 590, training loss: 905.7876586914062 = 0.4309682846069336 + 100.0 * 9.053566932678223
Epoch 590, val loss: 0.4692833125591278
Epoch 600, training loss: 905.6957397460938 = 0.42670902609825134 + 100.0 * 9.052690505981445
Epoch 600, val loss: 0.46588248014450073
Epoch 610, training loss: 905.7677001953125 = 0.42266666889190674 + 100.0 * 9.053450584411621
Epoch 610, val loss: 0.4626007378101349
Epoch 620, training loss: 905.8924560546875 = 0.41856318712234497 + 100.0 * 9.054738998413086
Epoch 620, val loss: 0.4592110514640808
Epoch 630, training loss: 905.5631713867188 = 0.4146655797958374 + 100.0 * 9.051485061645508
Epoch 630, val loss: 0.45634546875953674
Epoch 640, training loss: 905.4724731445312 = 0.4112356901168823 + 100.0 * 9.050612449645996
Epoch 640, val loss: 0.4537077844142914
Epoch 650, training loss: 905.421875 = 0.4080145061016083 + 100.0 * 9.050138473510742
Epoch 650, val loss: 0.4512907862663269
Epoch 660, training loss: 905.458740234375 = 0.40492013096809387 + 100.0 * 9.050538063049316
Epoch 660, val loss: 0.44894856214523315
Epoch 670, training loss: 905.359130859375 = 0.40164217352867126 + 100.0 * 9.049574851989746
Epoch 670, val loss: 0.4464588463306427
Epoch 680, training loss: 905.303955078125 = 0.3987181484699249 + 100.0 * 9.049052238464355
Epoch 680, val loss: 0.4443114697933197
Epoch 690, training loss: 905.3754272460938 = 0.39591917395591736 + 100.0 * 9.049795150756836
Epoch 690, val loss: 0.4422933757305145
Epoch 700, training loss: 905.2320556640625 = 0.3931196928024292 + 100.0 * 9.048389434814453
Epoch 700, val loss: 0.44035670161247253
Epoch 710, training loss: 905.18603515625 = 0.3905659019947052 + 100.0 * 9.047954559326172
Epoch 710, val loss: 0.43850722908973694
Epoch 720, training loss: 905.1068115234375 = 0.3880910277366638 + 100.0 * 9.047187805175781
Epoch 720, val loss: 0.43683046102523804
Epoch 730, training loss: 905.076171875 = 0.38571181893348694 + 100.0 * 9.046904563903809
Epoch 730, val loss: 0.43517595529556274
Epoch 740, training loss: 905.1552124023438 = 0.3833232820034027 + 100.0 * 9.04771900177002
Epoch 740, val loss: 0.43334388732910156
Epoch 750, training loss: 905.0286865234375 = 0.3810098171234131 + 100.0 * 9.046477317810059
Epoch 750, val loss: 0.4318836033344269
Epoch 760, training loss: 904.9165649414062 = 0.37879326939582825 + 100.0 * 9.045377731323242
Epoch 760, val loss: 0.43022263050079346
Epoch 770, training loss: 904.8566284179688 = 0.37675580382347107 + 100.0 * 9.044798851013184
Epoch 770, val loss: 0.42901188135147095
Epoch 780, training loss: 904.8084716796875 = 0.37478312849998474 + 100.0 * 9.044337272644043
Epoch 780, val loss: 0.4276798367500305
Epoch 790, training loss: 904.9595336914062 = 0.37283948063850403 + 100.0 * 9.045866966247559
Epoch 790, val loss: 0.4263911545276642
Epoch 800, training loss: 904.9600219726562 = 0.37075740098953247 + 100.0 * 9.045892715454102
Epoch 800, val loss: 0.42507871985435486
Epoch 810, training loss: 904.6953735351562 = 0.368843674659729 + 100.0 * 9.043265342712402
Epoch 810, val loss: 0.42382559180259705
Epoch 820, training loss: 904.657470703125 = 0.3670913875102997 + 100.0 * 9.042903900146484
Epoch 820, val loss: 0.4227341413497925
Epoch 830, training loss: 904.594482421875 = 0.3653932213783264 + 100.0 * 9.042290687561035
Epoch 830, val loss: 0.4216884970664978
Epoch 840, training loss: 904.82080078125 = 0.36371758580207825 + 100.0 * 9.044570922851562
Epoch 840, val loss: 0.42083486914634705
Epoch 850, training loss: 904.790771484375 = 0.36179953813552856 + 100.0 * 9.044289588928223
Epoch 850, val loss: 0.4195222854614258
Epoch 860, training loss: 904.5153198242188 = 0.3601739704608917 + 100.0 * 9.04155158996582
Epoch 860, val loss: 0.41847747564315796
Epoch 870, training loss: 904.44482421875 = 0.35863909125328064 + 100.0 * 9.040862083435059
Epoch 870, val loss: 0.4176662862300873
Epoch 880, training loss: 904.4146728515625 = 0.3571668565273285 + 100.0 * 9.04057502746582
Epoch 880, val loss: 0.41683149337768555
Epoch 890, training loss: 904.4999389648438 = 0.3556808531284332 + 100.0 * 9.04144287109375
Epoch 890, val loss: 0.4158325791358948
Epoch 900, training loss: 904.4551391601562 = 0.3540797233581543 + 100.0 * 9.041010856628418
Epoch 900, val loss: 0.4152921736240387
Epoch 910, training loss: 904.4461669921875 = 0.3525809049606323 + 100.0 * 9.040935516357422
Epoch 910, val loss: 0.4141920208930969
Epoch 920, training loss: 904.2972412109375 = 0.3511171340942383 + 100.0 * 9.039461135864258
Epoch 920, val loss: 0.41343533992767334
Epoch 930, training loss: 904.2360229492188 = 0.3497582972049713 + 100.0 * 9.038863182067871
Epoch 930, val loss: 0.41282930970191956
Epoch 940, training loss: 904.1979370117188 = 0.34842514991760254 + 100.0 * 9.038495063781738
Epoch 940, val loss: 0.4121812880039215
Epoch 950, training loss: 904.2146606445312 = 0.3471088111400604 + 100.0 * 9.038675308227539
Epoch 950, val loss: 0.4114592969417572
Epoch 960, training loss: 904.18798828125 = 0.34567001461982727 + 100.0 * 9.038423538208008
Epoch 960, val loss: 0.41066402196884155
Epoch 970, training loss: 904.1482543945312 = 0.3443305492401123 + 100.0 * 9.038039207458496
Epoch 970, val loss: 0.40997248888015747
Epoch 980, training loss: 904.2491455078125 = 0.3430209755897522 + 100.0 * 9.039061546325684
Epoch 980, val loss: 0.4092053174972534
Epoch 990, training loss: 904.14404296875 = 0.341674268245697 + 100.0 * 9.038023948669434
Epoch 990, val loss: 0.40884146094322205
Epoch 1000, training loss: 904.0460205078125 = 0.3404546082019806 + 100.0 * 9.037055969238281
Epoch 1000, val loss: 0.4080129861831665
Epoch 1010, training loss: 903.9932250976562 = 0.3392503261566162 + 100.0 * 9.036540031433105
Epoch 1010, val loss: 0.407687783241272
Epoch 1020, training loss: 904.1573486328125 = 0.3380717635154724 + 100.0 * 9.038192749023438
Epoch 1020, val loss: 0.4069315493106842
Epoch 1030, training loss: 903.988525390625 = 0.336779922246933 + 100.0 * 9.036517143249512
Epoch 1030, val loss: 0.40645676851272583
Epoch 1040, training loss: 903.9072265625 = 0.3356182277202606 + 100.0 * 9.03571605682373
Epoch 1040, val loss: 0.4058557450771332
Epoch 1050, training loss: 903.83740234375 = 0.3344520926475525 + 100.0 * 9.035029411315918
Epoch 1050, val loss: 0.40532103180885315
Epoch 1060, training loss: 903.8589477539062 = 0.33330947160720825 + 100.0 * 9.035256385803223
Epoch 1060, val loss: 0.4047619104385376
Epoch 1070, training loss: 903.8761596679688 = 0.33206769824028015 + 100.0 * 9.035440444946289
Epoch 1070, val loss: 0.4042235314846039
Epoch 1080, training loss: 903.8517456054688 = 0.3308960795402527 + 100.0 * 9.035208702087402
Epoch 1080, val loss: 0.40360018610954285
Epoch 1090, training loss: 903.7353515625 = 0.32978853583335876 + 100.0 * 9.034055709838867
Epoch 1090, val loss: 0.4032634496688843
Epoch 1100, training loss: 903.7468872070312 = 0.32871198654174805 + 100.0 * 9.034181594848633
Epoch 1100, val loss: 0.4026584029197693
Epoch 1110, training loss: 903.8423461914062 = 0.32759636640548706 + 100.0 * 9.035147666931152
Epoch 1110, val loss: 0.4023164212703705
Epoch 1120, training loss: 903.8429565429688 = 0.3264849781990051 + 100.0 * 9.035164833068848
Epoch 1120, val loss: 0.40198448300361633
Epoch 1130, training loss: 903.6463623046875 = 0.3253800570964813 + 100.0 * 9.033209800720215
Epoch 1130, val loss: 0.4014050364494324
Epoch 1140, training loss: 903.5931396484375 = 0.3243439793586731 + 100.0 * 9.03268814086914
Epoch 1140, val loss: 0.4009823203086853
Epoch 1150, training loss: 903.69091796875 = 0.3233071565628052 + 100.0 * 9.033676147460938
Epoch 1150, val loss: 0.40064510703086853
Epoch 1160, training loss: 903.7276000976562 = 0.32219967246055603 + 100.0 * 9.034053802490234
Epoch 1160, val loss: 0.39988836646080017
Epoch 1170, training loss: 903.6178588867188 = 0.3211316764354706 + 100.0 * 9.032967567443848
Epoch 1170, val loss: 0.3998155891895294
Epoch 1180, training loss: 903.4851684570312 = 0.3201214373111725 + 100.0 * 9.03165054321289
Epoch 1180, val loss: 0.39939507842063904
Epoch 1190, training loss: 903.455810546875 = 0.3191334307193756 + 100.0 * 9.031366348266602
Epoch 1190, val loss: 0.39893168210983276
Epoch 1200, training loss: 903.4630126953125 = 0.3181464970111847 + 100.0 * 9.031448364257812
Epoch 1200, val loss: 0.39869359135627747
Epoch 1210, training loss: 903.5849609375 = 0.31707677245140076 + 100.0 * 9.032678604125977
Epoch 1210, val loss: 0.39842361211776733
Epoch 1220, training loss: 903.4193115234375 = 0.3160589039325714 + 100.0 * 9.03103256225586
Epoch 1220, val loss: 0.3978974223136902
Epoch 1230, training loss: 903.4346923828125 = 0.3150891363620758 + 100.0 * 9.031196594238281
Epoch 1230, val loss: 0.39745593070983887
Epoch 1240, training loss: 903.4605712890625 = 0.3140967786312103 + 100.0 * 9.031464576721191
Epoch 1240, val loss: 0.3972388505935669
Epoch 1250, training loss: 903.3294067382812 = 0.3131083846092224 + 100.0 * 9.030162811279297
Epoch 1250, val loss: 0.39688748121261597
Epoch 1260, training loss: 903.3179321289062 = 0.31215864419937134 + 100.0 * 9.030057907104492
Epoch 1260, val loss: 0.39655572175979614
Epoch 1270, training loss: 903.38671875 = 0.31119853258132935 + 100.0 * 9.030755043029785
Epoch 1270, val loss: 0.39639708399772644
Epoch 1280, training loss: 903.2454833984375 = 0.3102436661720276 + 100.0 * 9.029352188110352
Epoch 1280, val loss: 0.39600419998168945
Epoch 1290, training loss: 903.2517700195312 = 0.30930131673812866 + 100.0 * 9.029424667358398
Epoch 1290, val loss: 0.3958389461040497
Epoch 1300, training loss: 903.340087890625 = 0.3083609938621521 + 100.0 * 9.030317306518555
Epoch 1300, val loss: 0.3957071006298065
Epoch 1310, training loss: 903.3751220703125 = 0.30738410353660583 + 100.0 * 9.030677795410156
Epoch 1310, val loss: 0.3948478102684021
Epoch 1320, training loss: 903.21240234375 = 0.306439608335495 + 100.0 * 9.029059410095215
Epoch 1320, val loss: 0.3950481712818146
Epoch 1330, training loss: 903.163330078125 = 0.305528849363327 + 100.0 * 9.02857780456543
Epoch 1330, val loss: 0.3945280611515045
Epoch 1340, training loss: 903.3448486328125 = 0.3046356439590454 + 100.0 * 9.030402183532715
Epoch 1340, val loss: 0.3945549428462982
Epoch 1350, training loss: 903.1251831054688 = 0.3036797344684601 + 100.0 * 9.028215408325195
Epoch 1350, val loss: 0.3939533531665802
Epoch 1360, training loss: 903.06103515625 = 0.30279895663261414 + 100.0 * 9.027582168579102
Epoch 1360, val loss: 0.3939407765865326
Epoch 1370, training loss: 903.0559692382812 = 0.30190417170524597 + 100.0 * 9.027541160583496
Epoch 1370, val loss: 0.3934881389141083
Epoch 1380, training loss: 903.30419921875 = 0.3009820580482483 + 100.0 * 9.03003215789795
Epoch 1380, val loss: 0.3934146463871002
Epoch 1390, training loss: 903.0690307617188 = 0.3000665009021759 + 100.0 * 9.027689933776855
Epoch 1390, val loss: 0.39317652583122253
Epoch 1400, training loss: 902.9830322265625 = 0.29917478561401367 + 100.0 * 9.026838302612305
Epoch 1400, val loss: 0.3930851221084595
Epoch 1410, training loss: 903.0696411132812 = 0.29830387234687805 + 100.0 * 9.027713775634766
Epoch 1410, val loss: 0.39299827814102173
Epoch 1420, training loss: 902.954833984375 = 0.297419935464859 + 100.0 * 9.02657413482666
Epoch 1420, val loss: 0.3925594389438629
Epoch 1430, training loss: 902.9339599609375 = 0.2965518534183502 + 100.0 * 9.026373863220215
Epoch 1430, val loss: 0.39240461587905884
Epoch 1440, training loss: 903.3065795898438 = 0.2956693470478058 + 100.0 * 9.030109405517578
Epoch 1440, val loss: 0.39222532510757446
Epoch 1450, training loss: 902.9618530273438 = 0.29476481676101685 + 100.0 * 9.026671409606934
Epoch 1450, val loss: 0.3920682668685913
Epoch 1460, training loss: 902.8505859375 = 0.29391661286354065 + 100.0 * 9.025566101074219
Epoch 1460, val loss: 0.3919975459575653
Epoch 1470, training loss: 902.827392578125 = 0.29307690262794495 + 100.0 * 9.02534294128418
Epoch 1470, val loss: 0.3918132185935974
Epoch 1480, training loss: 902.8550415039062 = 0.29223039746284485 + 100.0 * 9.025628089904785
Epoch 1480, val loss: 0.3916977047920227
Epoch 1490, training loss: 902.9366455078125 = 0.2913462817668915 + 100.0 * 9.026453018188477
Epoch 1490, val loss: 0.39154744148254395
Epoch 1500, training loss: 902.799072265625 = 0.29051274061203003 + 100.0 * 9.02508544921875
Epoch 1500, val loss: 0.39138123393058777
Epoch 1510, training loss: 902.7354736328125 = 0.2896656095981598 + 100.0 * 9.024457931518555
Epoch 1510, val loss: 0.39117300510406494
Epoch 1520, training loss: 902.7771606445312 = 0.28884607553482056 + 100.0 * 9.024883270263672
Epoch 1520, val loss: 0.3910992443561554
Epoch 1530, training loss: 902.8068237304688 = 0.28797999024391174 + 100.0 * 9.025188446044922
Epoch 1530, val loss: 0.39112767577171326
Epoch 1540, training loss: 903.0433959960938 = 0.28713521361351013 + 100.0 * 9.027563095092773
Epoch 1540, val loss: 0.390768826007843
Epoch 1550, training loss: 902.8008422851562 = 0.2863217294216156 + 100.0 * 9.025145530700684
Epoch 1550, val loss: 0.3910652995109558
Epoch 1560, training loss: 902.643310546875 = 0.2855052053928375 + 100.0 * 9.023577690124512
Epoch 1560, val loss: 0.39093342423439026
Epoch 1570, training loss: 902.6265869140625 = 0.28471821546554565 + 100.0 * 9.023418426513672
Epoch 1570, val loss: 0.3907160460948944
Epoch 1580, training loss: 902.626953125 = 0.28392407298088074 + 100.0 * 9.023429870605469
Epoch 1580, val loss: 0.3906479477882385
Epoch 1590, training loss: 902.912841796875 = 0.28310427069664 + 100.0 * 9.026297569274902
Epoch 1590, val loss: 0.39064908027648926
Epoch 1600, training loss: 902.6880493164062 = 0.2822885811328888 + 100.0 * 9.024057388305664
Epoch 1600, val loss: 0.3906559646129608
Epoch 1610, training loss: 902.5787963867188 = 0.2814806401729584 + 100.0 * 9.02297306060791
Epoch 1610, val loss: 0.3906385600566864
Epoch 1620, training loss: 902.554443359375 = 0.2806963324546814 + 100.0 * 9.022737503051758
Epoch 1620, val loss: 0.3906744122505188
Epoch 1630, training loss: 902.6190795898438 = 0.27989789843559265 + 100.0 * 9.023391723632812
Epoch 1630, val loss: 0.39069634675979614
Epoch 1640, training loss: 902.5272827148438 = 0.2790971100330353 + 100.0 * 9.022481918334961
Epoch 1640, val loss: 0.39025193452835083
Epoch 1650, training loss: 902.6493530273438 = 0.2783007025718689 + 100.0 * 9.023710250854492
Epoch 1650, val loss: 0.39029788970947266
Epoch 1660, training loss: 902.6200561523438 = 0.2775074243545532 + 100.0 * 9.023425102233887
Epoch 1660, val loss: 0.3904389441013336
Epoch 1670, training loss: 902.5153198242188 = 0.27668917179107666 + 100.0 * 9.02238655090332
Epoch 1670, val loss: 0.3902866840362549
Epoch 1680, training loss: 902.4342041015625 = 0.27593448758125305 + 100.0 * 9.02158260345459
Epoch 1680, val loss: 0.39030009508132935
Epoch 1690, training loss: 902.4061279296875 = 0.2751653790473938 + 100.0 * 9.021309852600098
Epoch 1690, val loss: 0.3903975188732147
Epoch 1700, training loss: 902.4155883789062 = 0.27440494298934937 + 100.0 * 9.021411895751953
Epoch 1700, val loss: 0.3902503550052643
Epoch 1710, training loss: 902.7222900390625 = 0.27361369132995605 + 100.0 * 9.024486541748047
Epoch 1710, val loss: 0.3903290927410126
Epoch 1720, training loss: 902.3704223632812 = 0.2728317081928253 + 100.0 * 9.020976066589355
Epoch 1720, val loss: 0.39030829071998596
Epoch 1730, training loss: 902.4072875976562 = 0.27206239104270935 + 100.0 * 9.021352767944336
Epoch 1730, val loss: 0.39035120606422424
Epoch 1740, training loss: 902.46435546875 = 0.2713107168674469 + 100.0 * 9.021930694580078
Epoch 1740, val loss: 0.39030149579048157
Epoch 1750, training loss: 902.3096923828125 = 0.27056190371513367 + 100.0 * 9.020391464233398
Epoch 1750, val loss: 0.39049598574638367
Epoch 1760, training loss: 902.3270263671875 = 0.2698099613189697 + 100.0 * 9.0205717086792
Epoch 1760, val loss: 0.39056047797203064
Epoch 1770, training loss: 902.5370483398438 = 0.26904717087745667 + 100.0 * 9.022680282592773
Epoch 1770, val loss: 0.39048266410827637
Epoch 1780, training loss: 902.3894653320312 = 0.2682856321334839 + 100.0 * 9.021211624145508
Epoch 1780, val loss: 0.39087721705436707
Epoch 1790, training loss: 902.34912109375 = 0.26751676201820374 + 100.0 * 9.0208158493042
Epoch 1790, val loss: 0.3906540274620056
Epoch 1800, training loss: 902.2608032226562 = 0.26678895950317383 + 100.0 * 9.019940376281738
Epoch 1800, val loss: 0.3908320665359497
Epoch 1810, training loss: 902.3349609375 = 0.26604291796684265 + 100.0 * 9.020689010620117
Epoch 1810, val loss: 0.39081281423568726
Epoch 1820, training loss: 902.2611694335938 = 0.265292227268219 + 100.0 * 9.01995849609375
Epoch 1820, val loss: 0.39093002676963806
Epoch 1830, training loss: 902.2269897460938 = 0.26454514265060425 + 100.0 * 9.019624710083008
Epoch 1830, val loss: 0.3910767734050751
Epoch 1840, training loss: 902.357666015625 = 0.2638189196586609 + 100.0 * 9.020938873291016
Epoch 1840, val loss: 0.3912271559238434
Epoch 1850, training loss: 902.1861572265625 = 0.2630549967288971 + 100.0 * 9.019230842590332
Epoch 1850, val loss: 0.3910437226295471
Epoch 1860, training loss: 902.2926025390625 = 0.2623162269592285 + 100.0 * 9.020302772521973
Epoch 1860, val loss: 0.3913615345954895
Epoch 1870, training loss: 902.1503295898438 = 0.2615838050842285 + 100.0 * 9.018887519836426
Epoch 1870, val loss: 0.3915935754776001
Epoch 1880, training loss: 902.1561889648438 = 0.26085829734802246 + 100.0 * 9.018953323364258
Epoch 1880, val loss: 0.39181146025657654
Epoch 1890, training loss: 902.1812133789062 = 0.2601236402988434 + 100.0 * 9.019210815429688
Epoch 1890, val loss: 0.391745001077652
Epoch 1900, training loss: 902.1594848632812 = 0.2593880295753479 + 100.0 * 9.019001007080078
Epoch 1900, val loss: 0.39182960987091064
Epoch 1910, training loss: 902.2825317382812 = 0.2586447596549988 + 100.0 * 9.020238876342773
Epoch 1910, val loss: 0.3918556272983551
Epoch 1920, training loss: 902.1162719726562 = 0.25792139768600464 + 100.0 * 9.018583297729492
Epoch 1920, val loss: 0.3922397792339325
Epoch 1930, training loss: 902.0621337890625 = 0.25719964504241943 + 100.0 * 9.018049240112305
Epoch 1930, val loss: 0.3921235501766205
Epoch 1940, training loss: 902.0477294921875 = 0.25647759437561035 + 100.0 * 9.017912864685059
Epoch 1940, val loss: 0.39235374331474304
Epoch 1950, training loss: 902.1693725585938 = 0.2557472884654999 + 100.0 * 9.019136428833008
Epoch 1950, val loss: 0.3923681974411011
Epoch 1960, training loss: 902.020263671875 = 0.2550193667411804 + 100.0 * 9.01765251159668
Epoch 1960, val loss: 0.3925958573818207
Epoch 1970, training loss: 902.11376953125 = 0.2542911171913147 + 100.0 * 9.018594741821289
Epoch 1970, val loss: 0.3926863968372345
Epoch 1980, training loss: 902.0665283203125 = 0.2535742521286011 + 100.0 * 9.018129348754883
Epoch 1980, val loss: 0.3933607041835785
Epoch 1990, training loss: 902.0345458984375 = 0.2528509497642517 + 100.0 * 9.017816543579102
Epoch 1990, val loss: 0.3933110535144806
Epoch 2000, training loss: 902.038818359375 = 0.2521258592605591 + 100.0 * 9.017867088317871
Epoch 2000, val loss: 0.3936293125152588
Epoch 2010, training loss: 901.9940795898438 = 0.2514282464981079 + 100.0 * 9.017426490783691
Epoch 2010, val loss: 0.39375460147857666
Epoch 2020, training loss: 901.93603515625 = 0.25071853399276733 + 100.0 * 9.016853332519531
Epoch 2020, val loss: 0.3935507535934448
Epoch 2030, training loss: 901.9583129882812 = 0.25001421570777893 + 100.0 * 9.017083168029785
Epoch 2030, val loss: 0.3937327563762665
Epoch 2040, training loss: 901.9937744140625 = 0.24929827451705933 + 100.0 * 9.017444610595703
Epoch 2040, val loss: 0.39414912462234497
Epoch 2050, training loss: 901.9324340820312 = 0.24857616424560547 + 100.0 * 9.016838073730469
Epoch 2050, val loss: 0.3945188820362091
Epoch 2060, training loss: 901.894287109375 = 0.24787397682666779 + 100.0 * 9.016464233398438
Epoch 2060, val loss: 0.39438772201538086
Epoch 2070, training loss: 902.0582275390625 = 0.24716924130916595 + 100.0 * 9.018110275268555
Epoch 2070, val loss: 0.3947385549545288
Epoch 2080, training loss: 901.8953857421875 = 0.2464437335729599 + 100.0 * 9.016489028930664
Epoch 2080, val loss: 0.3950517177581787
Epoch 2090, training loss: 901.833984375 = 0.24575592577457428 + 100.0 * 9.01588249206543
Epoch 2090, val loss: 0.39515528082847595
Epoch 2100, training loss: 901.8084106445312 = 0.24505145847797394 + 100.0 * 9.015633583068848
Epoch 2100, val loss: 0.3956465423107147
Epoch 2110, training loss: 902.0852661132812 = 0.24437595903873444 + 100.0 * 9.01840877532959
Epoch 2110, val loss: 0.3963625431060791
Epoch 2120, training loss: 901.9175415039062 = 0.24363866448402405 + 100.0 * 9.016738891601562
Epoch 2120, val loss: 0.3954097032546997
Epoch 2130, training loss: 901.92041015625 = 0.24295516312122345 + 100.0 * 9.01677417755127
Epoch 2130, val loss: 0.39663392305374146
Epoch 2140, training loss: 901.7852172851562 = 0.24224853515625 + 100.0 * 9.015429496765137
Epoch 2140, val loss: 0.3965000808238983
Epoch 2150, training loss: 901.8539428710938 = 0.24155236780643463 + 100.0 * 9.01612377166748
Epoch 2150, val loss: 0.39643368124961853
Epoch 2160, training loss: 901.8252563476562 = 0.24087248742580414 + 100.0 * 9.015844345092773
Epoch 2160, val loss: 0.396994948387146
Epoch 2170, training loss: 901.7592163085938 = 0.24016985297203064 + 100.0 * 9.015190124511719
Epoch 2170, val loss: 0.3971238434314728
Epoch 2180, training loss: 901.7639770507812 = 0.2394792139530182 + 100.0 * 9.015244483947754
Epoch 2180, val loss: 0.39742839336395264
Epoch 2190, training loss: 901.7788696289062 = 0.23879598081111908 + 100.0 * 9.015400886535645
Epoch 2190, val loss: 0.39784687757492065
Epoch 2200, training loss: 901.7537841796875 = 0.23810632526874542 + 100.0 * 9.015156745910645
Epoch 2200, val loss: 0.3979453444480896
Epoch 2210, training loss: 901.7993774414062 = 0.2374114841222763 + 100.0 * 9.015619277954102
Epoch 2210, val loss: 0.39817944169044495
Epoch 2220, training loss: 901.7877807617188 = 0.2367268055677414 + 100.0 * 9.015510559082031
Epoch 2220, val loss: 0.39849215745925903
Epoch 2230, training loss: 901.6495971679688 = 0.23606035113334656 + 100.0 * 9.014135360717773
Epoch 2230, val loss: 0.3988919258117676
Epoch 2240, training loss: 901.6131591796875 = 0.23538605868816376 + 100.0 * 9.013777732849121
Epoch 2240, val loss: 0.39909178018569946
Epoch 2250, training loss: 901.6087036132812 = 0.23471470177173615 + 100.0 * 9.013739585876465
Epoch 2250, val loss: 0.3992173671722412
Epoch 2260, training loss: 901.875244140625 = 0.23404279351234436 + 100.0 * 9.016411781311035
Epoch 2260, val loss: 0.3993300497531891
Epoch 2270, training loss: 901.6943359375 = 0.23337039351463318 + 100.0 * 9.014609336853027
Epoch 2270, val loss: 0.4003135859966278
Epoch 2280, training loss: 901.64697265625 = 0.23269057273864746 + 100.0 * 9.014142990112305
Epoch 2280, val loss: 0.40013930201530457
Epoch 2290, training loss: 901.5641479492188 = 0.23202849924564362 + 100.0 * 9.013320922851562
Epoch 2290, val loss: 0.4006277918815613
Epoch 2300, training loss: 901.708740234375 = 0.2313680201768875 + 100.0 * 9.01477336883545
Epoch 2300, val loss: 0.4010052978992462
Epoch 2310, training loss: 901.5510864257812 = 0.2307000309228897 + 100.0 * 9.013203620910645
Epoch 2310, val loss: 0.4011944532394409
Epoch 2320, training loss: 901.5543823242188 = 0.23003482818603516 + 100.0 * 9.013243675231934
Epoch 2320, val loss: 0.40148016810417175
Epoch 2330, training loss: 901.6264038085938 = 0.22938062250614166 + 100.0 * 9.013970375061035
Epoch 2330, val loss: 0.40198957920074463
Epoch 2340, training loss: 901.5189208984375 = 0.2287016212940216 + 100.0 * 9.01290225982666
Epoch 2340, val loss: 0.4022104740142822
Epoch 2350, training loss: 901.5879516601562 = 0.2280350774526596 + 100.0 * 9.013599395751953
Epoch 2350, val loss: 0.4026740491390228
Epoch 2360, training loss: 901.6978149414062 = 0.2273891717195511 + 100.0 * 9.014703750610352
Epoch 2360, val loss: 0.4029720425605774
Epoch 2370, training loss: 901.6370849609375 = 0.22671259939670563 + 100.0 * 9.014103889465332
Epoch 2370, val loss: 0.40315625071525574
Epoch 2380, training loss: 901.5054931640625 = 0.22605392336845398 + 100.0 * 9.012794494628906
Epoch 2380, val loss: 0.40357401967048645
Epoch 2390, training loss: 901.4673461914062 = 0.22539322078227997 + 100.0 * 9.012419700622559
Epoch 2390, val loss: 0.40413331985473633
Epoch 2400, training loss: 901.4454345703125 = 0.22474656999111176 + 100.0 * 9.01220703125
Epoch 2400, val loss: 0.4044293761253357
Epoch 2410, training loss: 901.4925537109375 = 0.22408127784729004 + 100.0 * 9.01268482208252
Epoch 2410, val loss: 0.40485167503356934
Epoch 2420, training loss: 901.6000366210938 = 0.22341355681419373 + 100.0 * 9.013766288757324
Epoch 2420, val loss: 0.4049975574016571
Epoch 2430, training loss: 901.4916381835938 = 0.22278104722499847 + 100.0 * 9.012688636779785
Epoch 2430, val loss: 0.4056571424007416
Epoch 2440, training loss: 901.4013061523438 = 0.22213485836982727 + 100.0 * 9.011792182922363
Epoch 2440, val loss: 0.4058162271976471
Epoch 2450, training loss: 901.411865234375 = 0.22147761285305023 + 100.0 * 9.011903762817383
Epoch 2450, val loss: 0.406068354845047
Epoch 2460, training loss: 901.608642578125 = 0.2208498865365982 + 100.0 * 9.013877868652344
Epoch 2460, val loss: 0.4063788652420044
Epoch 2470, training loss: 901.4075927734375 = 0.22017838060855865 + 100.0 * 9.011874198913574
Epoch 2470, val loss: 0.4073178172111511
Epoch 2480, training loss: 901.3531494140625 = 0.2195330411195755 + 100.0 * 9.011336326599121
Epoch 2480, val loss: 0.4071899652481079
Epoch 2490, training loss: 901.3912963867188 = 0.21888889372348785 + 100.0 * 9.011724472045898
Epoch 2490, val loss: 0.40760770440101624
Epoch 2500, training loss: 901.4966430664062 = 0.21823902428150177 + 100.0 * 9.012784004211426
Epoch 2500, val loss: 0.40797004103660583
Epoch 2510, training loss: 901.3922119140625 = 0.2176005095243454 + 100.0 * 9.011746406555176
Epoch 2510, val loss: 0.4091270864009857
Epoch 2520, training loss: 901.3384399414062 = 0.2169482707977295 + 100.0 * 9.011215209960938
Epoch 2520, val loss: 0.4089062511920929
Epoch 2530, training loss: 901.8953247070312 = 0.21631604433059692 + 100.0 * 9.016790390014648
Epoch 2530, val loss: 0.4099070727825165
Epoch 2540, training loss: 901.423828125 = 0.21568818390369415 + 100.0 * 9.012081146240234
Epoch 2540, val loss: 0.4098947048187256
Epoch 2550, training loss: 901.2972412109375 = 0.2150316685438156 + 100.0 * 9.010822296142578
Epoch 2550, val loss: 0.4101313054561615
Epoch 2560, training loss: 901.2840576171875 = 0.21440136432647705 + 100.0 * 9.010696411132812
Epoch 2560, val loss: 0.4107704758644104
Epoch 2570, training loss: 901.2684936523438 = 0.21376734972000122 + 100.0 * 9.010547637939453
Epoch 2570, val loss: 0.4110209345817566
Epoch 2580, training loss: 901.5828247070312 = 0.21314473450183868 + 100.0 * 9.013696670532227
Epoch 2580, val loss: 0.41123032569885254
Epoch 2590, training loss: 901.3734130859375 = 0.21252386271953583 + 100.0 * 9.011609077453613
Epoch 2590, val loss: 0.412393718957901
Epoch 2600, training loss: 901.3644409179688 = 0.21186678111553192 + 100.0 * 9.011526107788086
Epoch 2600, val loss: 0.41225576400756836
Epoch 2610, training loss: 901.288818359375 = 0.21124915778636932 + 100.0 * 9.010775566101074
Epoch 2610, val loss: 0.4128951132297516
Epoch 2620, training loss: 901.262451171875 = 0.21061566472053528 + 100.0 * 9.010518074035645
Epoch 2620, val loss: 0.41329672932624817
Epoch 2630, training loss: 901.2025146484375 = 0.20999231934547424 + 100.0 * 9.00992488861084
Epoch 2630, val loss: 0.41386693716049194
Epoch 2640, training loss: 901.2924194335938 = 0.2093721330165863 + 100.0 * 9.010830879211426
Epoch 2640, val loss: 0.4144784212112427
Epoch 2650, training loss: 901.303955078125 = 0.2087465524673462 + 100.0 * 9.01095199584961
Epoch 2650, val loss: 0.41477417945861816
Epoch 2660, training loss: 901.2423095703125 = 0.20810654759407043 + 100.0 * 9.01034164428711
Epoch 2660, val loss: 0.4151790738105774
Epoch 2670, training loss: 901.2325439453125 = 0.20749187469482422 + 100.0 * 9.010250091552734
Epoch 2670, val loss: 0.41565826535224915
Epoch 2680, training loss: 901.2633056640625 = 0.20687921345233917 + 100.0 * 9.010563850402832
Epoch 2680, val loss: 0.41599246859550476
Epoch 2690, training loss: 901.19482421875 = 0.20626667141914368 + 100.0 * 9.009885787963867
Epoch 2690, val loss: 0.4165680706501007
Epoch 2700, training loss: 901.1854248046875 = 0.20564836263656616 + 100.0 * 9.009798049926758
Epoch 2700, val loss: 0.4170382618904114
Epoch 2710, training loss: 901.1759643554688 = 0.20502859354019165 + 100.0 * 9.009709358215332
Epoch 2710, val loss: 0.4175034463405609
Epoch 2720, training loss: 901.1903076171875 = 0.2044043242931366 + 100.0 * 9.009859085083008
Epoch 2720, val loss: 0.41776058077812195
Epoch 2730, training loss: 901.3308715820312 = 0.20379310846328735 + 100.0 * 9.011270523071289
Epoch 2730, val loss: 0.41812393069267273
Epoch 2740, training loss: 901.2677001953125 = 0.20319190621376038 + 100.0 * 9.010644912719727
Epoch 2740, val loss: 0.41882577538490295
Epoch 2750, training loss: 901.1557006835938 = 0.2025567591190338 + 100.0 * 9.009531021118164
Epoch 2750, val loss: 0.41932207345962524
Epoch 2760, training loss: 901.1051025390625 = 0.20195768773555756 + 100.0 * 9.009031295776367
Epoch 2760, val loss: 0.42006567120552063
Epoch 2770, training loss: 901.1053466796875 = 0.20133890211582184 + 100.0 * 9.009039878845215
Epoch 2770, val loss: 0.4202379584312439
Epoch 2780, training loss: 901.2869873046875 = 0.20072688162326813 + 100.0 * 9.010862350463867
Epoch 2780, val loss: 0.4207093119621277
Epoch 2790, training loss: 901.1676635742188 = 0.2001156061887741 + 100.0 * 9.009675025939941
Epoch 2790, val loss: 0.42199403047561646
Epoch 2800, training loss: 901.0843505859375 = 0.199494868516922 + 100.0 * 9.008848190307617
Epoch 2800, val loss: 0.4218508005142212
Epoch 2810, training loss: 901.1279907226562 = 0.19889171421527863 + 100.0 * 9.00929069519043
Epoch 2810, val loss: 0.4227714240550995
Epoch 2820, training loss: 901.0507202148438 = 0.19827958941459656 + 100.0 * 9.008523941040039
Epoch 2820, val loss: 0.42314890027046204
Epoch 2830, training loss: 901.1259765625 = 0.19766277074813843 + 100.0 * 9.009283065795898
Epoch 2830, val loss: 0.4234257638454437
Epoch 2840, training loss: 901.2116088867188 = 0.19707798957824707 + 100.0 * 9.01014518737793
Epoch 2840, val loss: 0.4237794578075409
Epoch 2850, training loss: 901.0504150390625 = 0.19647037982940674 + 100.0 * 9.008539199829102
Epoch 2850, val loss: 0.4247320890426636
Epoch 2860, training loss: 900.99560546875 = 0.19585414230823517 + 100.0 * 9.007997512817383
Epoch 2860, val loss: 0.4252740144729614
Epoch 2870, training loss: 901.0087280273438 = 0.19526046514511108 + 100.0 * 9.008134841918945
Epoch 2870, val loss: 0.42560261487960815
Epoch 2880, training loss: 901.2613525390625 = 0.19466538727283478 + 100.0 * 9.010666847229004
Epoch 2880, val loss: 0.42621731758117676
Epoch 2890, training loss: 901.0703125 = 0.19406156241893768 + 100.0 * 9.00876235961914
Epoch 2890, val loss: 0.42668718099594116
Epoch 2900, training loss: 901.2132568359375 = 0.1934480369091034 + 100.0 * 9.010198593139648
Epoch 2900, val loss: 0.4271000623703003
Epoch 2910, training loss: 900.9878540039062 = 0.19285398721694946 + 100.0 * 9.007949829101562
Epoch 2910, val loss: 0.428017795085907
Epoch 2920, training loss: 900.937255859375 = 0.19225859642028809 + 100.0 * 9.007450103759766
Epoch 2920, val loss: 0.4282263517379761
Epoch 2930, training loss: 900.9522705078125 = 0.19166366755962372 + 100.0 * 9.007606506347656
Epoch 2930, val loss: 0.4289579391479492
Epoch 2940, training loss: 901.2158203125 = 0.1910725235939026 + 100.0 * 9.010247230529785
Epoch 2940, val loss: 0.4293956756591797
Epoch 2950, training loss: 901.0148315429688 = 0.19047565758228302 + 100.0 * 9.008243560791016
Epoch 2950, val loss: 0.42995625734329224
Epoch 2960, training loss: 900.974853515625 = 0.18987895548343658 + 100.0 * 9.00784969329834
Epoch 2960, val loss: 0.4307078421115875
Epoch 2970, training loss: 900.945068359375 = 0.18928037583827972 + 100.0 * 9.00755786895752
Epoch 2970, val loss: 0.4311482906341553
Epoch 2980, training loss: 901.2153930664062 = 0.1886831521987915 + 100.0 * 9.01026725769043
Epoch 2980, val loss: 0.4319502115249634
Epoch 2990, training loss: 900.9761962890625 = 0.18813778460025787 + 100.0 * 9.007881164550781
Epoch 2990, val loss: 0.4321954548358917
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7363
Flip ASR: 0.6712/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.26 GiB already allocated; 687.69 MiB free; 5.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 269.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 941.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 943.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 941.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 293.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.9949951171875 = 1.1030008792877197 + 100.0 * 10.358919143676758
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 553.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.00390625 = 1.0972155332565308 + 100.0 * 10.359067916870117
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 553.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.007080078125 = 1.0993927717208862 + 100.0 * 10.359077453613281
Epoch 0, val loss: 1.0985888242721558
Epoch 10, training loss: 1036.650146484375 = 1.088917851448059 + 100.0 * 10.355612754821777
Epoch 10, val loss: 1.0878938436508179
Epoch 20, training loss: 1030.9417724609375 = 1.0748651027679443 + 100.0 * 10.29866886138916
Epoch 20, val loss: 1.0739452838897705
Epoch 30, training loss: 976.9850463867188 = 1.0614310503005981 + 100.0 * 9.759236335754395
Epoch 30, val loss: 1.0606446266174316
Epoch 40, training loss: 958.4369506835938 = 1.0477838516235352 + 100.0 * 9.573891639709473
Epoch 40, val loss: 1.0466923713684082
Epoch 50, training loss: 947.5108032226562 = 1.033107876777649 + 100.0 * 9.464776992797852
Epoch 50, val loss: 1.0320196151733398
Epoch 60, training loss: 942.2052612304688 = 1.0173895359039307 + 100.0 * 9.41187858581543
Epoch 60, val loss: 1.0164035558700562
Epoch 70, training loss: 937.7071533203125 = 1.0001180171966553 + 100.0 * 9.367070198059082
Epoch 70, val loss: 0.999392569065094
Epoch 80, training loss: 932.2650756835938 = 0.9844058752059937 + 100.0 * 9.312806129455566
Epoch 80, val loss: 0.98418128490448
Epoch 90, training loss: 927.3502197265625 = 0.9689828753471375 + 100.0 * 9.263812065124512
Epoch 90, val loss: 0.969165027141571
Epoch 100, training loss: 924.9344482421875 = 0.9512216448783875 + 100.0 * 9.239831924438477
Epoch 100, val loss: 0.9520610570907593
Epoch 110, training loss: 923.1556396484375 = 0.9306700825691223 + 100.0 * 9.222249984741211
Epoch 110, val loss: 0.9321304559707642
Epoch 120, training loss: 921.6160278320312 = 0.9069427251815796 + 100.0 * 9.207091331481934
Epoch 120, val loss: 0.9093703031539917
Epoch 130, training loss: 920.5068969726562 = 0.8814211487770081 + 100.0 * 9.19625473022461
Epoch 130, val loss: 0.8849880695343018
Epoch 140, training loss: 919.3314208984375 = 0.8548438549041748 + 100.0 * 9.184765815734863
Epoch 140, val loss: 0.8598600625991821
Epoch 150, training loss: 918.4617309570312 = 0.8274473547935486 + 100.0 * 9.176342964172363
Epoch 150, val loss: 0.8342084288597107
Epoch 160, training loss: 917.7279052734375 = 0.7988351583480835 + 100.0 * 9.169290542602539
Epoch 160, val loss: 0.8074164390563965
Epoch 170, training loss: 916.921630859375 = 0.7696658372879028 + 100.0 * 9.161520004272461
Epoch 170, val loss: 0.7804697155952454
Epoch 180, training loss: 916.1473388671875 = 0.7410906553268433 + 100.0 * 9.154062271118164
Epoch 180, val loss: 0.7544003129005432
Epoch 190, training loss: 915.5924682617188 = 0.7130656242370605 + 100.0 * 9.148794174194336
Epoch 190, val loss: 0.7290521860122681
Epoch 200, training loss: 915.0792236328125 = 0.6856328248977661 + 100.0 * 9.143936157226562
Epoch 200, val loss: 0.7044336199760437
Epoch 210, training loss: 914.664306640625 = 0.6592450737953186 + 100.0 * 9.140050888061523
Epoch 210, val loss: 0.6811034679412842
Epoch 220, training loss: 914.4654541015625 = 0.634418785572052 + 100.0 * 9.138310432434082
Epoch 220, val loss: 0.6595376133918762
Epoch 230, training loss: 913.99365234375 = 0.6116974949836731 + 100.0 * 9.133819580078125
Epoch 230, val loss: 0.6401205658912659
Epoch 240, training loss: 913.6414794921875 = 0.5914854407310486 + 100.0 * 9.130499839782715
Epoch 240, val loss: 0.6231646537780762
Epoch 250, training loss: 913.8804931640625 = 0.573397696018219 + 100.0 * 9.133070945739746
Epoch 250, val loss: 0.6082819700241089
Epoch 260, training loss: 913.1453857421875 = 0.5568867325782776 + 100.0 * 9.125885009765625
Epoch 260, val loss: 0.5949364304542542
Epoch 270, training loss: 912.9398803710938 = 0.5425792932510376 + 100.0 * 9.12397289276123
Epoch 270, val loss: 0.5837324857711792
Epoch 280, training loss: 912.6908569335938 = 0.5299787521362305 + 100.0 * 9.12160873413086
Epoch 280, val loss: 0.574126124382019
Epoch 290, training loss: 912.4974365234375 = 0.5188189744949341 + 100.0 * 9.119786262512207
Epoch 290, val loss: 0.5659011006355286
Epoch 300, training loss: 912.3748168945312 = 0.5089260339736938 + 100.0 * 9.118659019470215
Epoch 300, val loss: 0.5589345693588257
Epoch 310, training loss: 912.4323120117188 = 0.4998047351837158 + 100.0 * 9.119324684143066
Epoch 310, val loss: 0.5522649884223938
Epoch 320, training loss: 912.0341186523438 = 0.49165254831314087 + 100.0 * 9.115425109863281
Epoch 320, val loss: 0.5468804836273193
Epoch 330, training loss: 911.8086547851562 = 0.48456871509552 + 100.0 * 9.113241195678711
Epoch 330, val loss: 0.5420633554458618
Epoch 340, training loss: 911.6497192382812 = 0.4782277047634125 + 100.0 * 9.111715316772461
Epoch 340, val loss: 0.5381931662559509
Epoch 350, training loss: 911.4790649414062 = 0.47242864966392517 + 100.0 * 9.110066413879395
Epoch 350, val loss: 0.5347561836242676
Epoch 360, training loss: 911.3360595703125 = 0.46706029772758484 + 100.0 * 9.10869026184082
Epoch 360, val loss: 0.5316033959388733
Epoch 370, training loss: 911.36962890625 = 0.46198925375938416 + 100.0 * 9.109076499938965
Epoch 370, val loss: 0.5286300778388977
Epoch 380, training loss: 911.1179809570312 = 0.4571291506290436 + 100.0 * 9.106608390808105
Epoch 380, val loss: 0.525890052318573
Epoch 390, training loss: 910.9325561523438 = 0.45265430212020874 + 100.0 * 9.104799270629883
Epoch 390, val loss: 0.5234237313270569
Epoch 400, training loss: 910.820068359375 = 0.44845613837242126 + 100.0 * 9.103715896606445
Epoch 400, val loss: 0.5211710929870605
Epoch 410, training loss: 910.7311401367188 = 0.4444260001182556 + 100.0 * 9.102867126464844
Epoch 410, val loss: 0.5190000534057617
Epoch 420, training loss: 910.6543579101562 = 0.4404760003089905 + 100.0 * 9.10213851928711
Epoch 420, val loss: 0.5168386101722717
Epoch 430, training loss: 910.55078125 = 0.4365783929824829 + 100.0 * 9.101141929626465
Epoch 430, val loss: 0.5149562358856201
Epoch 440, training loss: 910.3628540039062 = 0.4329189956188202 + 100.0 * 9.099299430847168
Epoch 440, val loss: 0.5129266381263733
Epoch 450, training loss: 910.3777465820312 = 0.42940428853034973 + 100.0 * 9.099483489990234
Epoch 450, val loss: 0.510979413986206
Epoch 460, training loss: 910.3274536132812 = 0.42582419514656067 + 100.0 * 9.099016189575195
Epoch 460, val loss: 0.5093597173690796
Epoch 470, training loss: 910.1134643554688 = 0.42239758372306824 + 100.0 * 9.09691047668457
Epoch 470, val loss: 0.5075151324272156
Epoch 480, training loss: 909.9879760742188 = 0.41916149854660034 + 100.0 * 9.095687866210938
Epoch 480, val loss: 0.505778431892395
Epoch 490, training loss: 909.9744873046875 = 0.4160158336162567 + 100.0 * 9.095584869384766
Epoch 490, val loss: 0.5039915442466736
Epoch 500, training loss: 909.8880004882812 = 0.4127894937992096 + 100.0 * 9.094752311706543
Epoch 500, val loss: 0.5026364326477051
Epoch 510, training loss: 909.698486328125 = 0.4096693694591522 + 100.0 * 9.092887878417969
Epoch 510, val loss: 0.5007707476615906
Epoch 520, training loss: 909.6475219726562 = 0.40663689374923706 + 100.0 * 9.092409133911133
Epoch 520, val loss: 0.4993510842323303
Epoch 530, training loss: 909.6259765625 = 0.403543084859848 + 100.0 * 9.09222412109375
Epoch 530, val loss: 0.4977332055568695
Epoch 540, training loss: 909.451904296875 = 0.40048444271087646 + 100.0 * 9.090514183044434
Epoch 540, val loss: 0.49583864212036133
Epoch 550, training loss: 909.402099609375 = 0.3975411355495453 + 100.0 * 9.090045928955078
Epoch 550, val loss: 0.4943079948425293
Epoch 560, training loss: 909.2791137695312 = 0.39456841349601746 + 100.0 * 9.088845252990723
Epoch 560, val loss: 0.4927056133747101
Epoch 570, training loss: 909.198486328125 = 0.39163434505462646 + 100.0 * 9.088068008422852
Epoch 570, val loss: 0.4911345839500427
Epoch 580, training loss: 909.22607421875 = 0.3887181878089905 + 100.0 * 9.088373184204102
Epoch 580, val loss: 0.4898991286754608
Epoch 590, training loss: 909.054931640625 = 0.3857215344905853 + 100.0 * 9.086691856384277
Epoch 590, val loss: 0.4875973165035248
Epoch 600, training loss: 908.9657592773438 = 0.3827863931655884 + 100.0 * 9.085829734802246
Epoch 600, val loss: 0.48630866408348083
Epoch 610, training loss: 909.3585205078125 = 0.37993529438972473 + 100.0 * 9.0897855758667
Epoch 610, val loss: 0.4840192496776581
Epoch 620, training loss: 908.8226318359375 = 0.37675487995147705 + 100.0 * 9.084458351135254
Epoch 620, val loss: 0.4826256036758423
Epoch 630, training loss: 908.7379760742188 = 0.37383759021759033 + 100.0 * 9.083641052246094
Epoch 630, val loss: 0.4812518358230591
Epoch 640, training loss: 908.6710815429688 = 0.3709995746612549 + 100.0 * 9.083001136779785
Epoch 640, val loss: 0.47982341051101685
Epoch 650, training loss: 908.7332153320312 = 0.36817997694015503 + 100.0 * 9.083650588989258
Epoch 650, val loss: 0.4780954122543335
Epoch 660, training loss: 908.81640625 = 0.36513957381248474 + 100.0 * 9.084512710571289
Epoch 660, val loss: 0.4762917459011078
Epoch 670, training loss: 908.5631713867188 = 0.3621510863304138 + 100.0 * 9.082010269165039
Epoch 670, val loss: 0.4746171534061432
Epoch 680, training loss: 908.4627075195312 = 0.35930928587913513 + 100.0 * 9.081033706665039
Epoch 680, val loss: 0.47308772802352905
Epoch 690, training loss: 908.3787231445312 = 0.35652777552604675 + 100.0 * 9.080222129821777
Epoch 690, val loss: 0.4715123772621155
Epoch 700, training loss: 908.379638671875 = 0.3537285625934601 + 100.0 * 9.080259323120117
Epoch 700, val loss: 0.4700441360473633
Epoch 710, training loss: 908.2572021484375 = 0.35080936551094055 + 100.0 * 9.079063415527344
Epoch 710, val loss: 0.46841275691986084
Epoch 720, training loss: 908.285400390625 = 0.3479282855987549 + 100.0 * 9.079375267028809
Epoch 720, val loss: 0.46696728467941284
Epoch 730, training loss: 908.24072265625 = 0.3450784683227539 + 100.0 * 9.078956604003906
Epoch 730, val loss: 0.4654061198234558
Epoch 740, training loss: 908.178955078125 = 0.3422357141971588 + 100.0 * 9.078367233276367
Epoch 740, val loss: 0.46419742703437805
Epoch 750, training loss: 908.09228515625 = 0.3393922448158264 + 100.0 * 9.077528953552246
Epoch 750, val loss: 0.46254417300224304
Epoch 760, training loss: 908.0270385742188 = 0.3365955948829651 + 100.0 * 9.076904296875
Epoch 760, val loss: 0.46085214614868164
Epoch 770, training loss: 908.0550537109375 = 0.3338299095630646 + 100.0 * 9.0772123336792
Epoch 770, val loss: 0.45975834131240845
Epoch 780, training loss: 907.9081420898438 = 0.33100828528404236 + 100.0 * 9.07577133178711
Epoch 780, val loss: 0.4584488868713379
Epoch 790, training loss: 907.8696899414062 = 0.3282558023929596 + 100.0 * 9.075414657592773
Epoch 790, val loss: 0.4571438431739807
Epoch 800, training loss: 907.9235229492188 = 0.3254848122596741 + 100.0 * 9.075980186462402
Epoch 800, val loss: 0.4562259614467621
Epoch 810, training loss: 907.72998046875 = 0.32270222902297974 + 100.0 * 9.07407283782959
Epoch 810, val loss: 0.45480117201805115
Epoch 820, training loss: 907.7345581054688 = 0.32000815868377686 + 100.0 * 9.074145317077637
Epoch 820, val loss: 0.453849196434021
Epoch 830, training loss: 907.7318725585938 = 0.3172779977321625 + 100.0 * 9.074146270751953
Epoch 830, val loss: 0.4524577260017395
Epoch 840, training loss: 907.6492919921875 = 0.3145590126514435 + 100.0 * 9.073347091674805
Epoch 840, val loss: 0.45165514945983887
Epoch 850, training loss: 907.5990600585938 = 0.31192871928215027 + 100.0 * 9.072871208190918
Epoch 850, val loss: 0.4495818614959717
Epoch 860, training loss: 907.56494140625 = 0.30925342440605164 + 100.0 * 9.072556495666504
Epoch 860, val loss: 0.4490680694580078
Epoch 870, training loss: 907.4794311523438 = 0.3066498935222626 + 100.0 * 9.071727752685547
Epoch 870, val loss: 0.44855305552482605
Epoch 880, training loss: 907.4310913085938 = 0.3040793836116791 + 100.0 * 9.071269989013672
Epoch 880, val loss: 0.44758516550064087
Epoch 890, training loss: 907.6151733398438 = 0.3014938235282898 + 100.0 * 9.073136329650879
Epoch 890, val loss: 0.44712579250335693
Epoch 900, training loss: 907.3375244140625 = 0.2988046705722809 + 100.0 * 9.07038688659668
Epoch 900, val loss: 0.44602862000465393
Epoch 910, training loss: 907.282470703125 = 0.2962866723537445 + 100.0 * 9.069862365722656
Epoch 910, val loss: 0.4449099600315094
Epoch 920, training loss: 907.22509765625 = 0.29381483793258667 + 100.0 * 9.069313049316406
Epoch 920, val loss: 0.444402813911438
Epoch 930, training loss: 907.5062255859375 = 0.29135024547576904 + 100.0 * 9.072149276733398
Epoch 930, val loss: 0.4435316026210785
Epoch 940, training loss: 907.3895263671875 = 0.28878289461135864 + 100.0 * 9.07100772857666
Epoch 940, val loss: 0.4436413645744324
Epoch 950, training loss: 907.1019897460938 = 0.28625011444091797 + 100.0 * 9.068157196044922
Epoch 950, val loss: 0.442596971988678
Epoch 960, training loss: 907.070068359375 = 0.28381913900375366 + 100.0 * 9.067862510681152
Epoch 960, val loss: 0.44222426414489746
Epoch 970, training loss: 907.016357421875 = 0.28142714500427246 + 100.0 * 9.067349433898926
Epoch 970, val loss: 0.4422321319580078
Epoch 980, training loss: 907.3587036132812 = 0.2790023982524872 + 100.0 * 9.070796966552734
Epoch 980, val loss: 0.4415926933288574
Epoch 990, training loss: 906.9873046875 = 0.2765127122402191 + 100.0 * 9.067108154296875
Epoch 990, val loss: 0.44196969270706177
Epoch 1000, training loss: 906.965087890625 = 0.2741011083126068 + 100.0 * 9.066909790039062
Epoch 1000, val loss: 0.4412517249584198
Epoch 1010, training loss: 906.8795166015625 = 0.2717561721801758 + 100.0 * 9.066078186035156
Epoch 1010, val loss: 0.4408179521560669
Epoch 1020, training loss: 906.9476928710938 = 0.2694028317928314 + 100.0 * 9.06678295135498
Epoch 1020, val loss: 0.4406682252883911
Epoch 1030, training loss: 906.829345703125 = 0.26703062653541565 + 100.0 * 9.06562328338623
Epoch 1030, val loss: 0.4405979514122009
Epoch 1040, training loss: 906.8053588867188 = 0.26470860838890076 + 100.0 * 9.065406799316406
Epoch 1040, val loss: 0.4406526982784271
Epoch 1050, training loss: 906.8013916015625 = 0.26234954595565796 + 100.0 * 9.065390586853027
Epoch 1050, val loss: 0.4402821362018585
Epoch 1060, training loss: 906.8092041015625 = 0.26001471281051636 + 100.0 * 9.065491676330566
Epoch 1060, val loss: 0.4400208294391632
Epoch 1070, training loss: 906.7214965820312 = 0.2576805353164673 + 100.0 * 9.064638137817383
Epoch 1070, val loss: 0.4397438168525696
Epoch 1080, training loss: 906.6339111328125 = 0.2553655505180359 + 100.0 * 9.063785552978516
Epoch 1080, val loss: 0.43993180990219116
Epoch 1090, training loss: 906.5560302734375 = 0.2531004846096039 + 100.0 * 9.063029289245605
Epoch 1090, val loss: 0.4396245777606964
Epoch 1100, training loss: 906.6039428710938 = 0.2509070336818695 + 100.0 * 9.063529968261719
Epoch 1100, val loss: 0.4393007755279541
Epoch 1110, training loss: 906.482177734375 = 0.24860335886478424 + 100.0 * 9.062335968017578
Epoch 1110, val loss: 0.440083384513855
Epoch 1120, training loss: 906.4597778320312 = 0.24639329314231873 + 100.0 * 9.0621337890625
Epoch 1120, val loss: 0.44123345613479614
Epoch 1130, training loss: 906.40625 = 0.24417392909526825 + 100.0 * 9.061620712280273
Epoch 1130, val loss: 0.44057875871658325
Epoch 1140, training loss: 906.8643188476562 = 0.2419770210981369 + 100.0 * 9.06622314453125
Epoch 1140, val loss: 0.44097232818603516
Epoch 1150, training loss: 906.3867797851562 = 0.239685520529747 + 100.0 * 9.061470985412598
Epoch 1150, val loss: 0.4407837986946106
Epoch 1160, training loss: 906.3790283203125 = 0.23750050365924835 + 100.0 * 9.061415672302246
Epoch 1160, val loss: 0.4420163333415985
Epoch 1170, training loss: 906.271728515625 = 0.23535127937793732 + 100.0 * 9.06036376953125
Epoch 1170, val loss: 0.4420958161354065
Epoch 1180, training loss: 906.3176879882812 = 0.23322626948356628 + 100.0 * 9.060844421386719
Epoch 1180, val loss: 0.44245821237564087
Epoch 1190, training loss: 906.2623291015625 = 0.2310388684272766 + 100.0 * 9.06031322479248
Epoch 1190, val loss: 0.4426251947879791
Epoch 1200, training loss: 906.2412109375 = 0.2288815975189209 + 100.0 * 9.060123443603516
Epoch 1200, val loss: 0.4424414038658142
Epoch 1210, training loss: 906.2041015625 = 0.2267078012228012 + 100.0 * 9.059774398803711
Epoch 1210, val loss: 0.4435908794403076
Epoch 1220, training loss: 906.281494140625 = 0.22458341717720032 + 100.0 * 9.060568809509277
Epoch 1220, val loss: 0.444027841091156
Epoch 1230, training loss: 906.1580200195312 = 0.2224549949169159 + 100.0 * 9.059355735778809
Epoch 1230, val loss: 0.44411030411720276
Epoch 1240, training loss: 906.080078125 = 0.220337375998497 + 100.0 * 9.058597564697266
Epoch 1240, val loss: 0.4456351101398468
Epoch 1250, training loss: 906.1383666992188 = 0.21824127435684204 + 100.0 * 9.05920124053955
Epoch 1250, val loss: 0.44580182433128357
Epoch 1260, training loss: 906.195556640625 = 0.21620263159275055 + 100.0 * 9.059793472290039
Epoch 1260, val loss: 0.44430217146873474
Epoch 1270, training loss: 906.1287841796875 = 0.21409201622009277 + 100.0 * 9.059146881103516
Epoch 1270, val loss: 0.44799208641052246
Epoch 1280, training loss: 905.9691162109375 = 0.2119559794664383 + 100.0 * 9.057571411132812
Epoch 1280, val loss: 0.44642916321754456
Epoch 1290, training loss: 905.8778686523438 = 0.20991230010986328 + 100.0 * 9.056679725646973
Epoch 1290, val loss: 0.44739389419555664
Epoch 1300, training loss: 905.8775024414062 = 0.20790429413318634 + 100.0 * 9.056695938110352
Epoch 1300, val loss: 0.4487152099609375
Epoch 1310, training loss: 906.2894897460938 = 0.20590530335903168 + 100.0 * 9.060835838317871
Epoch 1310, val loss: 0.44842350482940674
Epoch 1320, training loss: 905.8197021484375 = 0.2037983536720276 + 100.0 * 9.056159019470215
Epoch 1320, val loss: 0.4492989480495453
Epoch 1330, training loss: 905.8215942382812 = 0.20178517699241638 + 100.0 * 9.056198120117188
Epoch 1330, val loss: 0.45079025626182556
Epoch 1340, training loss: 905.7586669921875 = 0.1997859627008438 + 100.0 * 9.055588722229004
Epoch 1340, val loss: 0.45124930143356323
Epoch 1350, training loss: 906.1130981445312 = 0.19781576097011566 + 100.0 * 9.059152603149414
Epoch 1350, val loss: 0.4511452317237854
Epoch 1360, training loss: 905.8190307617188 = 0.19577232003211975 + 100.0 * 9.056232452392578
Epoch 1360, val loss: 0.45279136300086975
Epoch 1370, training loss: 905.6719970703125 = 0.19376803934574127 + 100.0 * 9.054781913757324
Epoch 1370, val loss: 0.4536367654800415
Epoch 1380, training loss: 905.636474609375 = 0.1918017864227295 + 100.0 * 9.054447174072266
Epoch 1380, val loss: 0.45444777607917786
Epoch 1390, training loss: 905.9597778320312 = 0.18993473052978516 + 100.0 * 9.057698249816895
Epoch 1390, val loss: 0.4568846821784973
Epoch 1400, training loss: 905.8690795898438 = 0.18787387013435364 + 100.0 * 9.056812286376953
Epoch 1400, val loss: 0.4551646113395691
Epoch 1410, training loss: 905.5872802734375 = 0.1858517825603485 + 100.0 * 9.054014205932617
Epoch 1410, val loss: 0.4564676880836487
Epoch 1420, training loss: 905.5390625 = 0.18390880525112152 + 100.0 * 9.05355167388916
Epoch 1420, val loss: 0.45838043093681335
Epoch 1430, training loss: 905.5794067382812 = 0.1819852590560913 + 100.0 * 9.053974151611328
Epoch 1430, val loss: 0.45899489521980286
Epoch 1440, training loss: 905.5870971679688 = 0.1800469160079956 + 100.0 * 9.054070472717285
Epoch 1440, val loss: 0.4600297510623932
Epoch 1450, training loss: 905.4945068359375 = 0.1781742423772812 + 100.0 * 9.053163528442383
Epoch 1450, val loss: 0.46248310804367065
Epoch 1460, training loss: 905.4548950195312 = 0.17618776857852936 + 100.0 * 9.052786827087402
Epoch 1460, val loss: 0.4621115028858185
Epoch 1470, training loss: 905.41015625 = 0.17430207133293152 + 100.0 * 9.052358627319336
Epoch 1470, val loss: 0.46299657225608826
Epoch 1480, training loss: 905.6810302734375 = 0.17245104908943176 + 100.0 * 9.055086135864258
Epoch 1480, val loss: 0.4639114439487457
Epoch 1490, training loss: 905.4415893554688 = 0.17051178216934204 + 100.0 * 9.05271053314209
Epoch 1490, val loss: 0.46573227643966675
Epoch 1500, training loss: 905.3742065429688 = 0.16862429678440094 + 100.0 * 9.052055358886719
Epoch 1500, val loss: 0.46639180183410645
Epoch 1510, training loss: 905.5639038085938 = 0.16677826642990112 + 100.0 * 9.053971290588379
Epoch 1510, val loss: 0.46691423654556274
Epoch 1520, training loss: 905.3216552734375 = 0.16487574577331543 + 100.0 * 9.051568031311035
Epoch 1520, val loss: 0.4693329632282257
Epoch 1530, training loss: 905.3792114257812 = 0.16305199265480042 + 100.0 * 9.052162170410156
Epoch 1530, val loss: 0.4712868332862854
Epoch 1540, training loss: 905.28955078125 = 0.16123174130916595 + 100.0 * 9.05128288269043
Epoch 1540, val loss: 0.4725237488746643
Epoch 1550, training loss: 905.2274780273438 = 0.15938051044940948 + 100.0 * 9.050681114196777
Epoch 1550, val loss: 0.4727136492729187
Epoch 1560, training loss: 905.189453125 = 0.1575966328382492 + 100.0 * 9.050318717956543
Epoch 1560, val loss: 0.47498634457588196
Epoch 1570, training loss: 905.2382202148438 = 0.15580661594867706 + 100.0 * 9.050824165344238
Epoch 1570, val loss: 0.47539252042770386
Epoch 1580, training loss: 905.2359008789062 = 0.15407873690128326 + 100.0 * 9.05081844329834
Epoch 1580, val loss: 0.47796785831451416
Epoch 1590, training loss: 905.1406860351562 = 0.15223312377929688 + 100.0 * 9.049884796142578
Epoch 1590, val loss: 0.47804245352745056
Epoch 1600, training loss: 905.1148681640625 = 0.15050165355205536 + 100.0 * 9.049643516540527
Epoch 1600, val loss: 0.4801338315010071
Epoch 1610, training loss: 905.1387939453125 = 0.1488368660211563 + 100.0 * 9.049899101257324
Epoch 1610, val loss: 0.4827022850513458
Epoch 1620, training loss: 905.1512451171875 = 0.1470649093389511 + 100.0 * 9.050041198730469
Epoch 1620, val loss: 0.48211556673049927
Epoch 1630, training loss: 905.062255859375 = 0.14539973437786102 + 100.0 * 9.049168586730957
Epoch 1630, val loss: 0.4857960045337677
Epoch 1640, training loss: 905.0105590820312 = 0.14371928572654724 + 100.0 * 9.048667907714844
Epoch 1640, val loss: 0.48456111550331116
Epoch 1650, training loss: 904.9882202148438 = 0.14204323291778564 + 100.0 * 9.0484619140625
Epoch 1650, val loss: 0.4866321384906769
Epoch 1660, training loss: 905.4109497070312 = 0.140577495098114 + 100.0 * 9.052703857421875
Epoch 1660, val loss: 0.4869081676006317
Epoch 1670, training loss: 905.037353515625 = 0.13878537714481354 + 100.0 * 9.048985481262207
Epoch 1670, val loss: 0.4907163381576538
Epoch 1680, training loss: 904.9265747070312 = 0.1371428668498993 + 100.0 * 9.047894477844238
Epoch 1680, val loss: 0.492534339427948
Epoch 1690, training loss: 905.0322265625 = 0.13558487594127655 + 100.0 * 9.048966407775879
Epoch 1690, val loss: 0.49460890889167786
Epoch 1700, training loss: 904.9174194335938 = 0.13399578630924225 + 100.0 * 9.047834396362305
Epoch 1700, val loss: 0.4955662488937378
Epoch 1710, training loss: 904.86767578125 = 0.13240274786949158 + 100.0 * 9.04735279083252
Epoch 1710, val loss: 0.4968814551830292
Epoch 1720, training loss: 904.8836669921875 = 0.1308777630329132 + 100.0 * 9.047528266906738
Epoch 1720, val loss: 0.49836477637290955
Epoch 1730, training loss: 905.0847778320312 = 0.12938165664672852 + 100.0 * 9.049553871154785
Epoch 1730, val loss: 0.49970659613609314
Epoch 1740, training loss: 905.0516967773438 = 0.12785977125167847 + 100.0 * 9.049238204956055
Epoch 1740, val loss: 0.502265214920044
Epoch 1750, training loss: 904.774658203125 = 0.1263326108455658 + 100.0 * 9.046483039855957
Epoch 1750, val loss: 0.5034609436988831
Epoch 1760, training loss: 904.7476196289062 = 0.12485985457897186 + 100.0 * 9.04622745513916
Epoch 1760, val loss: 0.5050905346870422
Epoch 1770, training loss: 904.716796875 = 0.12340082228183746 + 100.0 * 9.045933723449707
Epoch 1770, val loss: 0.5067620277404785
Epoch 1780, training loss: 904.9301147460938 = 0.12206512689590454 + 100.0 * 9.048080444335938
Epoch 1780, val loss: 0.5070066452026367
Epoch 1790, training loss: 904.7894287109375 = 0.12053477019071579 + 100.0 * 9.0466890335083
Epoch 1790, val loss: 0.5112845301628113
Epoch 1800, training loss: 904.7297973632812 = 0.11911854147911072 + 100.0 * 9.046106338500977
Epoch 1800, val loss: 0.5117965936660767
Epoch 1810, training loss: 904.640625 = 0.11770789325237274 + 100.0 * 9.045228958129883
Epoch 1810, val loss: 0.514244556427002
Epoch 1820, training loss: 904.6729736328125 = 0.11633296310901642 + 100.0 * 9.04556655883789
Epoch 1820, val loss: 0.5164250135421753
Epoch 1830, training loss: 904.9208374023438 = 0.1149783581495285 + 100.0 * 9.04805850982666
Epoch 1830, val loss: 0.5181351900100708
Epoch 1840, training loss: 904.588134765625 = 0.1135590448975563 + 100.0 * 9.044745445251465
Epoch 1840, val loss: 0.519920289516449
Epoch 1850, training loss: 904.6173095703125 = 0.11220614612102509 + 100.0 * 9.045051574707031
Epoch 1850, val loss: 0.5216317772865295
Epoch 1860, training loss: 904.5526733398438 = 0.11086287349462509 + 100.0 * 9.044418334960938
Epoch 1860, val loss: 0.523303747177124
Epoch 1870, training loss: 904.7250366210938 = 0.1095646545290947 + 100.0 * 9.046154975891113
Epoch 1870, val loss: 0.5260013937950134
Epoch 1880, training loss: 904.63818359375 = 0.1082267090678215 + 100.0 * 9.045299530029297
Epoch 1880, val loss: 0.5266193747520447
Epoch 1890, training loss: 904.5843505859375 = 0.1068907082080841 + 100.0 * 9.044775009155273
Epoch 1890, val loss: 0.5292984247207642
Epoch 1900, training loss: 904.4760131835938 = 0.10558982938528061 + 100.0 * 9.04370403289795
Epoch 1900, val loss: 0.531412661075592
Epoch 1910, training loss: 904.4593505859375 = 0.10430899262428284 + 100.0 * 9.043550491333008
Epoch 1910, val loss: 0.5328816175460815
Epoch 1920, training loss: 904.5101318359375 = 0.10304921120405197 + 100.0 * 9.044071197509766
Epoch 1920, val loss: 0.5352495908737183
Epoch 1930, training loss: 904.5404663085938 = 0.10179546475410461 + 100.0 * 9.044386863708496
Epoch 1930, val loss: 0.5367023944854736
Epoch 1940, training loss: 904.4522094726562 = 0.10052915662527084 + 100.0 * 9.043517112731934
Epoch 1940, val loss: 0.5393387079238892
Epoch 1950, training loss: 904.44921875 = 0.09931095689535141 + 100.0 * 9.043498992919922
Epoch 1950, val loss: 0.5402077436447144
Epoch 1960, training loss: 904.4280395507812 = 0.0980762168765068 + 100.0 * 9.043299674987793
Epoch 1960, val loss: 0.5427430272102356
Epoch 1970, training loss: 904.4603271484375 = 0.0968698039650917 + 100.0 * 9.043634414672852
Epoch 1970, val loss: 0.5450855493545532
Epoch 1980, training loss: 904.3704833984375 = 0.09566880762577057 + 100.0 * 9.04274845123291
Epoch 1980, val loss: 0.5464792847633362
Epoch 1990, training loss: 904.4705810546875 = 0.0944989025592804 + 100.0 * 9.043761253356934
Epoch 1990, val loss: 0.5483965873718262
Epoch 2000, training loss: 904.5353393554688 = 0.09333594888448715 + 100.0 * 9.04442024230957
Epoch 2000, val loss: 0.5512793660163879
Epoch 2010, training loss: 904.385009765625 = 0.0921683982014656 + 100.0 * 9.042928695678711
Epoch 2010, val loss: 0.5537046790122986
Epoch 2020, training loss: 904.3402709960938 = 0.09100829064846039 + 100.0 * 9.042492866516113
Epoch 2020, val loss: 0.555396556854248
Epoch 2030, training loss: 904.5018920898438 = 0.08990988880395889 + 100.0 * 9.044119834899902
Epoch 2030, val loss: 0.5562334060668945
Epoch 2040, training loss: 904.3822631835938 = 0.08879465609788895 + 100.0 * 9.04293441772461
Epoch 2040, val loss: 0.5577515363693237
Epoch 2050, training loss: 904.261474609375 = 0.08763939887285233 + 100.0 * 9.041738510131836
Epoch 2050, val loss: 0.5609462857246399
Epoch 2060, training loss: 904.2191772460938 = 0.08654019981622696 + 100.0 * 9.041326522827148
Epoch 2060, val loss: 0.5642259120941162
Epoch 2070, training loss: 904.2564697265625 = 0.08554556220769882 + 100.0 * 9.041708946228027
Epoch 2070, val loss: 0.5681244134902954
Epoch 2080, training loss: 904.3494873046875 = 0.08450101315975189 + 100.0 * 9.04265022277832
Epoch 2080, val loss: 0.5693137049674988
Epoch 2090, training loss: 904.3014526367188 = 0.08344804495573044 + 100.0 * 9.042180061340332
Epoch 2090, val loss: 0.5685248970985413
Epoch 2100, training loss: 904.2630615234375 = 0.08235275000333786 + 100.0 * 9.041807174682617
Epoch 2100, val loss: 0.5728082060813904
Epoch 2110, training loss: 904.2157592773438 = 0.08133363723754883 + 100.0 * 9.041343688964844
Epoch 2110, val loss: 0.5759777426719666
Epoch 2120, training loss: 904.1510620117188 = 0.08030688762664795 + 100.0 * 9.0407075881958
Epoch 2120, val loss: 0.5767524242401123
Epoch 2130, training loss: 904.1806030273438 = 0.07932760566473007 + 100.0 * 9.04101276397705
Epoch 2130, val loss: 0.5792837738990784
Epoch 2140, training loss: 904.2285766601562 = 0.07836409658193588 + 100.0 * 9.041501998901367
Epoch 2140, val loss: 0.5812755823135376
Epoch 2150, training loss: 904.1016235351562 = 0.07737047225236893 + 100.0 * 9.040242195129395
Epoch 2150, val loss: 0.5830841064453125
Epoch 2160, training loss: 904.1327514648438 = 0.07646578550338745 + 100.0 * 9.040562629699707
Epoch 2160, val loss: 0.5839678049087524
Epoch 2170, training loss: 904.1747436523438 = 0.07549820095300674 + 100.0 * 9.040992736816406
Epoch 2170, val loss: 0.5881342887878418
Epoch 2180, training loss: 904.1217041015625 = 0.07455972582101822 + 100.0 * 9.040471076965332
Epoch 2180, val loss: 0.5897981524467468
Epoch 2190, training loss: 904.088623046875 = 0.07367019355297089 + 100.0 * 9.040149688720703
Epoch 2190, val loss: 0.5907184481620789
Epoch 2200, training loss: 904.1229248046875 = 0.0727456584572792 + 100.0 * 9.040501594543457
Epoch 2200, val loss: 0.5949694514274597
Epoch 2210, training loss: 904.0759887695312 = 0.07187207043170929 + 100.0 * 9.040040969848633
Epoch 2210, val loss: 0.5967192053794861
Epoch 2220, training loss: 903.9918212890625 = 0.0709744542837143 + 100.0 * 9.03920841217041
Epoch 2220, val loss: 0.5982376337051392
Epoch 2230, training loss: 904.0045776367188 = 0.07014378905296326 + 100.0 * 9.039344787597656
Epoch 2230, val loss: 0.602333664894104
Epoch 2240, training loss: 904.1377563476562 = 0.06928327679634094 + 100.0 * 9.040684700012207
Epoch 2240, val loss: 0.601824939250946
Epoch 2250, training loss: 904.069580078125 = 0.06840864568948746 + 100.0 * 9.040011405944824
Epoch 2250, val loss: 0.6059229969978333
Epoch 2260, training loss: 904.0081176757812 = 0.06760843098163605 + 100.0 * 9.03940486907959
Epoch 2260, val loss: 0.6063306927680969
Epoch 2270, training loss: 904.1189575195312 = 0.06680279970169067 + 100.0 * 9.040521621704102
Epoch 2270, val loss: 0.6114944815635681
Epoch 2280, training loss: 903.937255859375 = 0.06592284888029099 + 100.0 * 9.038713455200195
Epoch 2280, val loss: 0.6112675666809082
Epoch 2290, training loss: 903.8728637695312 = 0.06510455906391144 + 100.0 * 9.038077354431152
Epoch 2290, val loss: 0.6148146986961365
Epoch 2300, training loss: 903.876708984375 = 0.0643080398440361 + 100.0 * 9.038124084472656
Epoch 2300, val loss: 0.6161587834358215
Epoch 2310, training loss: 904.0861206054688 = 0.06354634463787079 + 100.0 * 9.040225982666016
Epoch 2310, val loss: 0.6183436512947083
Epoch 2320, training loss: 904.0807495117188 = 0.0627548024058342 + 100.0 * 9.040180206298828
Epoch 2320, val loss: 0.6212653517723083
Epoch 2330, training loss: 903.9285888671875 = 0.0619945228099823 + 100.0 * 9.038665771484375
Epoch 2330, val loss: 0.6224797964096069
Epoch 2340, training loss: 903.7962036132812 = 0.06121428683400154 + 100.0 * 9.037349700927734
Epoch 2340, val loss: 0.6264938116073608
Epoch 2350, training loss: 903.8038330078125 = 0.06045742705464363 + 100.0 * 9.037433624267578
Epoch 2350, val loss: 0.6281419396400452
Epoch 2360, training loss: 904.1890258789062 = 0.0597965307533741 + 100.0 * 9.041292190551758
Epoch 2360, val loss: 0.6323117017745972
Epoch 2370, training loss: 903.8873291015625 = 0.059009794145822525 + 100.0 * 9.038283348083496
Epoch 2370, val loss: 0.6309562921524048
Epoch 2380, training loss: 903.8005981445312 = 0.0582716204226017 + 100.0 * 9.037423133850098
Epoch 2380, val loss: 0.6351836323738098
Epoch 2390, training loss: 903.8502197265625 = 0.05754560977220535 + 100.0 * 9.03792667388916
Epoch 2390, val loss: 0.6363593339920044
Epoch 2400, training loss: 903.8128051757812 = 0.05683927983045578 + 100.0 * 9.037559509277344
Epoch 2400, val loss: 0.6389783024787903
Epoch 2410, training loss: 903.7258911132812 = 0.056137800216674805 + 100.0 * 9.036697387695312
Epoch 2410, val loss: 0.6408491730690002
Epoch 2420, training loss: 903.72509765625 = 0.055452875792980194 + 100.0 * 9.036696434020996
Epoch 2420, val loss: 0.6429917812347412
Epoch 2430, training loss: 903.8330078125 = 0.05478956922888756 + 100.0 * 9.037781715393066
Epoch 2430, val loss: 0.6452303528785706
Epoch 2440, training loss: 903.7158813476562 = 0.054100070148706436 + 100.0 * 9.03661823272705
Epoch 2440, val loss: 0.6475023031234741
Epoch 2450, training loss: 903.7808837890625 = 0.053468361496925354 + 100.0 * 9.037274360656738
Epoch 2450, val loss: 0.6511572003364563
Epoch 2460, training loss: 903.7664184570312 = 0.05279885232448578 + 100.0 * 9.03713607788086
Epoch 2460, val loss: 0.652708113193512
Epoch 2470, training loss: 903.7763671875 = 0.052192509174346924 + 100.0 * 9.03724193572998
Epoch 2470, val loss: 0.6558604836463928
Epoch 2480, training loss: 903.6744384765625 = 0.05153351277112961 + 100.0 * 9.036229133605957
Epoch 2480, val loss: 0.6556612253189087
Epoch 2490, training loss: 903.6448364257812 = 0.05087309330701828 + 100.0 * 9.03593921661377
Epoch 2490, val loss: 0.6587650179862976
Epoch 2500, training loss: 903.8037719726562 = 0.050271518528461456 + 100.0 * 9.037534713745117
Epoch 2500, val loss: 0.6605813503265381
Epoch 2510, training loss: 903.7134399414062 = 0.049676526337862015 + 100.0 * 9.036637306213379
Epoch 2510, val loss: 0.6652505993843079
Epoch 2520, training loss: 903.7633056640625 = 0.04903579503297806 + 100.0 * 9.037142753601074
Epoch 2520, val loss: 0.6648829579353333
Epoch 2530, training loss: 903.5924682617188 = 0.04841665178537369 + 100.0 * 9.035440444946289
Epoch 2530, val loss: 0.6687156558036804
Epoch 2540, training loss: 903.54052734375 = 0.04781420901417732 + 100.0 * 9.034927368164062
Epoch 2540, val loss: 0.6698811054229736
Epoch 2550, training loss: 903.5440673828125 = 0.04724331945180893 + 100.0 * 9.034968376159668
Epoch 2550, val loss: 0.6732794046401978
Epoch 2560, training loss: 903.9421997070312 = 0.0467521958053112 + 100.0 * 9.038954734802246
Epoch 2560, val loss: 0.6758584976196289
Epoch 2570, training loss: 903.7572021484375 = 0.04615803435444832 + 100.0 * 9.037110328674316
Epoch 2570, val loss: 0.6749067306518555
Epoch 2580, training loss: 903.557373046875 = 0.0455377958714962 + 100.0 * 9.035118103027344
Epoch 2580, val loss: 0.6791852712631226
Epoch 2590, training loss: 903.5146484375 = 0.04497348144650459 + 100.0 * 9.034696578979492
Epoch 2590, val loss: 0.6813568472862244
Epoch 2600, training loss: 903.6602783203125 = 0.04444248601794243 + 100.0 * 9.036158561706543
Epoch 2600, val loss: 0.6824656128883362
Epoch 2610, training loss: 903.5787963867188 = 0.04389208182692528 + 100.0 * 9.035348892211914
Epoch 2610, val loss: 0.6849798560142517
Epoch 2620, training loss: 903.488037109375 = 0.04335084185004234 + 100.0 * 9.034446716308594
Epoch 2620, val loss: 0.6876574158668518
Epoch 2630, training loss: 903.60986328125 = 0.04283996298909187 + 100.0 * 9.035670280456543
Epoch 2630, val loss: 0.6898766160011292
Epoch 2640, training loss: 903.4412841796875 = 0.042303651571273804 + 100.0 * 9.033989906311035
Epoch 2640, val loss: 0.6914918422698975
Epoch 2650, training loss: 903.4356689453125 = 0.041800204664468765 + 100.0 * 9.03393840789795
Epoch 2650, val loss: 0.6943466067314148
Epoch 2660, training loss: 903.5484619140625 = 0.041339028626680374 + 100.0 * 9.03507137298584
Epoch 2660, val loss: 0.6978259682655334
Epoch 2670, training loss: 903.5013427734375 = 0.04081391543149948 + 100.0 * 9.034605026245117
Epoch 2670, val loss: 0.6967991590499878
Epoch 2680, training loss: 903.4566040039062 = 0.04032174125313759 + 100.0 * 9.034162521362305
Epoch 2680, val loss: 0.7011134028434753
Epoch 2690, training loss: 903.4299926757812 = 0.0398436076939106 + 100.0 * 9.03390121459961
Epoch 2690, val loss: 0.7021470665931702
Epoch 2700, training loss: 903.3868408203125 = 0.03938782587647438 + 100.0 * 9.033474922180176
Epoch 2700, val loss: 0.7057218551635742
Epoch 2710, training loss: 903.542724609375 = 0.038937509059906006 + 100.0 * 9.035037994384766
Epoch 2710, val loss: 0.708304226398468
Epoch 2720, training loss: 903.3934936523438 = 0.03845924884080887 + 100.0 * 9.033550262451172
Epoch 2720, val loss: 0.7072261571884155
Epoch 2730, training loss: 903.3692626953125 = 0.03797227889299393 + 100.0 * 9.033312797546387
Epoch 2730, val loss: 0.7110236287117004
Epoch 2740, training loss: 903.37158203125 = 0.03752840310335159 + 100.0 * 9.033340454101562
Epoch 2740, val loss: 0.7119248509407043
Epoch 2750, training loss: 903.34814453125 = 0.037079039961099625 + 100.0 * 9.033110618591309
Epoch 2750, val loss: 0.7145881652832031
Epoch 2760, training loss: 903.7662963867188 = 0.03667973354458809 + 100.0 * 9.037296295166016
Epoch 2760, val loss: 0.7160986065864563
Epoch 2770, training loss: 903.3761596679688 = 0.0362238734960556 + 100.0 * 9.03339958190918
Epoch 2770, val loss: 0.7200843095779419
Epoch 2780, training loss: 903.2800903320312 = 0.03577315807342529 + 100.0 * 9.032443046569824
Epoch 2780, val loss: 0.7211501002311707
Epoch 2790, training loss: 903.2734375 = 0.035354409366846085 + 100.0 * 9.032381057739258
Epoch 2790, val loss: 0.7235074639320374
Epoch 2800, training loss: 903.3008422851562 = 0.03494654595851898 + 100.0 * 9.032658576965332
Epoch 2800, val loss: 0.7258520126342773
Epoch 2810, training loss: 903.5678100585938 = 0.034551508724689484 + 100.0 * 9.035332679748535
Epoch 2810, val loss: 0.7272824048995972
Epoch 2820, training loss: 903.3939819335938 = 0.03418677672743797 + 100.0 * 9.033597946166992
Epoch 2820, val loss: 0.7273415923118591
Epoch 2830, training loss: 903.232666015625 = 0.033723264932632446 + 100.0 * 9.031989097595215
Epoch 2830, val loss: 0.7315388917922974
Epoch 2840, training loss: 903.4218139648438 = 0.03334642946720123 + 100.0 * 9.03388500213623
Epoch 2840, val loss: 0.7328664064407349
Epoch 2850, training loss: 903.2481079101562 = 0.03294755890965462 + 100.0 * 9.032151222229004
Epoch 2850, val loss: 0.7352608442306519
Epoch 2860, training loss: 903.2027587890625 = 0.03255332261323929 + 100.0 * 9.031702041625977
Epoch 2860, val loss: 0.7377232313156128
Epoch 2870, training loss: 903.1847534179688 = 0.032174065709114075 + 100.0 * 9.031525611877441
Epoch 2870, val loss: 0.7395204305648804
Epoch 2880, training loss: 903.1705322265625 = 0.031797509640455246 + 100.0 * 9.031387329101562
Epoch 2880, val loss: 0.7409057021141052
Epoch 2890, training loss: 903.3555297851562 = 0.03147648647427559 + 100.0 * 9.03324031829834
Epoch 2890, val loss: 0.7421268820762634
Epoch 2900, training loss: 903.26123046875 = 0.031087670475244522 + 100.0 * 9.032301902770996
Epoch 2900, val loss: 0.7467151284217834
Epoch 2910, training loss: 903.2620239257812 = 0.030749911442399025 + 100.0 * 9.032312393188477
Epoch 2910, val loss: 0.7460960149765015
Epoch 2920, training loss: 903.1727294921875 = 0.03036509081721306 + 100.0 * 9.031423568725586
Epoch 2920, val loss: 0.7490246295928955
Epoch 2930, training loss: 903.2145385742188 = 0.030014505609869957 + 100.0 * 9.031845092773438
Epoch 2930, val loss: 0.7509645223617554
Epoch 2940, training loss: 903.2781372070312 = 0.029694415628910065 + 100.0 * 9.03248405456543
Epoch 2940, val loss: 0.7525150179862976
Epoch 2950, training loss: 903.1482543945312 = 0.029327483847737312 + 100.0 * 9.03118896484375
Epoch 2950, val loss: 0.7550067901611328
Epoch 2960, training loss: 903.1219482421875 = 0.028989439830183983 + 100.0 * 9.030929565429688
Epoch 2960, val loss: 0.7565047144889832
Epoch 2970, training loss: 903.316162109375 = 0.028676657006144524 + 100.0 * 9.032875061035156
Epoch 2970, val loss: 0.7586045265197754
Epoch 2980, training loss: 903.102294921875 = 0.0283353328704834 + 100.0 * 9.030739784240723
Epoch 2980, val loss: 0.7606015801429749
Epoch 2990, training loss: 903.0712890625 = 0.02801121026277542 + 100.0 * 9.03043270111084
Epoch 2990, val loss: 0.7623288035392761
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8447
Overall ASR: 0.7094
Flip ASR: 0.6377/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.34 GiB already allocated; 767.69 MiB free; 3.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 691.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 283.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 289.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 289.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 291.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 291.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 291.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 289.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.15 GiB already allocated; 303.69 MiB free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.15 GiB already allocated; 303.69 MiB free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 17.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 689.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 689.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 689.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 645.69 MiB free; 4.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 641.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 115.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 919.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 923.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 919.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 923.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 923.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.9921875 = 1.0882201194763184 + 100.0 * 10.359039306640625
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 87.69 MiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0006103515625 = 1.0922032594680786 + 100.0 * 10.359084129333496
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 87.69 MiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.01123046875 = 1.1024799346923828 + 100.0 * 10.359086990356445
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 87.69 MiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0164794921875 = 1.1032969951629639 + 100.0 * 10.359131813049316
Epoch 0, val loss: 1.102829933166504
Epoch 10, training loss: 1036.7349853515625 = 1.0932179689407349 + 100.0 * 10.356417655944824
Epoch 10, val loss: 1.0924872159957886
Epoch 20, training loss: 1031.8216552734375 = 1.079645037651062 + 100.0 * 10.30742073059082
Epoch 20, val loss: 1.0788071155548096
Epoch 30, training loss: 987.0298461914062 = 1.065529465675354 + 100.0 * 9.85964298248291
Epoch 30, val loss: 1.0648514032363892
Epoch 40, training loss: 961.1732177734375 = 1.051107406616211 + 100.0 * 9.601221084594727
Epoch 40, val loss: 1.050554633140564
Epoch 50, training loss: 951.814697265625 = 1.0367026329040527 + 100.0 * 9.507780075073242
Epoch 50, val loss: 1.0365163087844849
Epoch 60, training loss: 943.387939453125 = 1.0210886001586914 + 100.0 * 9.423667907714844
Epoch 60, val loss: 1.0213202238082886
Epoch 70, training loss: 940.8246459960938 = 1.0023999214172363 + 100.0 * 9.398222923278809
Epoch 70, val loss: 1.0030059814453125
Epoch 80, training loss: 938.6893310546875 = 0.9821122288703918 + 100.0 * 9.37707233428955
Epoch 80, val loss: 0.9834681153297424
Epoch 90, training loss: 936.1725463867188 = 0.9612597227096558 + 100.0 * 9.352112770080566
Epoch 90, val loss: 0.9633282423019409
Epoch 100, training loss: 931.8861694335938 = 0.9384590983390808 + 100.0 * 9.309476852416992
Epoch 100, val loss: 0.9413052201271057
Epoch 110, training loss: 927.9236450195312 = 0.9143929481506348 + 100.0 * 9.270092964172363
Epoch 110, val loss: 0.9182709455490112
Epoch 120, training loss: 925.23681640625 = 0.8895760774612427 + 100.0 * 9.2434720993042
Epoch 120, val loss: 0.8947780132293701
Epoch 130, training loss: 922.625244140625 = 0.8635505437850952 + 100.0 * 9.21761703491211
Epoch 130, val loss: 0.8706653118133545
Epoch 140, training loss: 921.2147216796875 = 0.8355633020401001 + 100.0 * 9.203791618347168
Epoch 140, val loss: 0.8445063233375549
Epoch 150, training loss: 920.4075927734375 = 0.8029098510742188 + 100.0 * 9.196046829223633
Epoch 150, val loss: 0.8140866160392761
Epoch 160, training loss: 919.6636352539062 = 0.7679872512817383 + 100.0 * 9.188956260681152
Epoch 160, val loss: 0.7820993065834045
Epoch 170, training loss: 918.9915161132812 = 0.7339929342269897 + 100.0 * 9.182575225830078
Epoch 170, val loss: 0.751737117767334
Epoch 180, training loss: 918.3421630859375 = 0.7025285959243774 + 100.0 * 9.176396369934082
Epoch 180, val loss: 0.7240319848060608
Epoch 190, training loss: 917.6624145507812 = 0.6737663149833679 + 100.0 * 9.169886589050293
Epoch 190, val loss: 0.6992639899253845
Epoch 200, training loss: 916.9423217773438 = 0.6473528146743774 + 100.0 * 9.162949562072754
Epoch 200, val loss: 0.6770197153091431
Epoch 210, training loss: 916.22412109375 = 0.6241060495376587 + 100.0 * 9.156000137329102
Epoch 210, val loss: 0.657980740070343
Epoch 220, training loss: 916.1087036132812 = 0.6037437915802002 + 100.0 * 9.155049324035645
Epoch 220, val loss: 0.6417272686958313
Epoch 230, training loss: 915.1174926757812 = 0.5851895213127136 + 100.0 * 9.145322799682617
Epoch 230, val loss: 0.6272584795951843
Epoch 240, training loss: 914.4767456054688 = 0.5691571235656738 + 100.0 * 9.139076232910156
Epoch 240, val loss: 0.6153178811073303
Epoch 250, training loss: 913.8698120117188 = 0.5555588603019714 + 100.0 * 9.133142471313477
Epoch 250, val loss: 0.6057419776916504
Epoch 260, training loss: 913.3389892578125 = 0.5435133576393127 + 100.0 * 9.127954483032227
Epoch 260, val loss: 0.5974719524383545
Epoch 270, training loss: 912.96142578125 = 0.532588005065918 + 100.0 * 9.124288558959961
Epoch 270, val loss: 0.5903977155685425
Epoch 280, training loss: 912.4510498046875 = 0.5231242775917053 + 100.0 * 9.119278907775879
Epoch 280, val loss: 0.5844175219535828
Epoch 290, training loss: 912.222412109375 = 0.5144451260566711 + 100.0 * 9.117079734802246
Epoch 290, val loss: 0.579201877117157
Epoch 300, training loss: 911.9417114257812 = 0.5062737464904785 + 100.0 * 9.114354133605957
Epoch 300, val loss: 0.5745051503181458
Epoch 310, training loss: 911.5512084960938 = 0.4990595877170563 + 100.0 * 9.11052131652832
Epoch 310, val loss: 0.570396900177002
Epoch 320, training loss: 911.258056640625 = 0.49250584840774536 + 100.0 * 9.10765552520752
Epoch 320, val loss: 0.5670174956321716
Epoch 330, training loss: 911.3782348632812 = 0.48637691140174866 + 100.0 * 9.108918190002441
Epoch 330, val loss: 0.5639905333518982
Epoch 340, training loss: 910.8760375976562 = 0.4805183708667755 + 100.0 * 9.103955268859863
Epoch 340, val loss: 0.560816764831543
Epoch 350, training loss: 910.6416625976562 = 0.4752257764339447 + 100.0 * 9.101664543151855
Epoch 350, val loss: 0.5580218434333801
Epoch 360, training loss: 910.4260864257812 = 0.47039371728897095 + 100.0 * 9.099556922912598
Epoch 360, val loss: 0.5556456446647644
Epoch 370, training loss: 910.7100830078125 = 0.46588629484176636 + 100.0 * 9.102441787719727
Epoch 370, val loss: 0.5533210039138794
Epoch 380, training loss: 910.2593994140625 = 0.46126872301101685 + 100.0 * 9.097981452941895
Epoch 380, val loss: 0.5511950254440308
Epoch 390, training loss: 909.848388671875 = 0.4571501910686493 + 100.0 * 9.093912124633789
Epoch 390, val loss: 0.5494706034660339
Epoch 400, training loss: 909.6946411132812 = 0.4533846080303192 + 100.0 * 9.092412948608398
Epoch 400, val loss: 0.5477989315986633
Epoch 410, training loss: 909.4834594726562 = 0.44974440336227417 + 100.0 * 9.090336799621582
Epoch 410, val loss: 0.5462128520011902
Epoch 420, training loss: 909.8937377929688 = 0.44617587327957153 + 100.0 * 9.094475746154785
Epoch 420, val loss: 0.5445204377174377
Epoch 430, training loss: 909.3064575195312 = 0.442523717880249 + 100.0 * 9.088639259338379
Epoch 430, val loss: 0.5428420901298523
Epoch 440, training loss: 909.0979614257812 = 0.43914860486984253 + 100.0 * 9.086587905883789
Epoch 440, val loss: 0.5414101481437683
Epoch 450, training loss: 908.9758911132812 = 0.43592512607574463 + 100.0 * 9.085399627685547
Epoch 450, val loss: 0.5401092767715454
Epoch 460, training loss: 908.7977294921875 = 0.4326721727848053 + 100.0 * 9.083650588989258
Epoch 460, val loss: 0.5384158492088318
Epoch 470, training loss: 908.7438354492188 = 0.4295040965080261 + 100.0 * 9.08314323425293
Epoch 470, val loss: 0.5370945334434509
Epoch 480, training loss: 908.7428588867188 = 0.42651447653770447 + 100.0 * 9.083163261413574
Epoch 480, val loss: 0.5360589623451233
Epoch 490, training loss: 908.6495971679688 = 0.4234378933906555 + 100.0 * 9.082261085510254
Epoch 490, val loss: 0.5340616106987
Epoch 500, training loss: 908.3949584960938 = 0.4205125868320465 + 100.0 * 9.079744338989258
Epoch 500, val loss: 0.532814621925354
Epoch 510, training loss: 908.2915649414062 = 0.41771334409713745 + 100.0 * 9.07873821258545
Epoch 510, val loss: 0.531719446182251
Epoch 520, training loss: 908.2330932617188 = 0.41495582461357117 + 100.0 * 9.078181266784668
Epoch 520, val loss: 0.5305423140525818
Epoch 530, training loss: 908.1061401367188 = 0.41215112805366516 + 100.0 * 9.076939582824707
Epoch 530, val loss: 0.5292018055915833
Epoch 540, training loss: 908.0975952148438 = 0.4093833863735199 + 100.0 * 9.076882362365723
Epoch 540, val loss: 0.5279020667076111
Epoch 550, training loss: 908.0516967773438 = 0.40674489736557007 + 100.0 * 9.076449394226074
Epoch 550, val loss: 0.526654064655304
Epoch 560, training loss: 907.9208374023438 = 0.40404149889945984 + 100.0 * 9.075167655944824
Epoch 560, val loss: 0.5254958868026733
Epoch 570, training loss: 907.8131713867188 = 0.4014209508895874 + 100.0 * 9.074117660522461
Epoch 570, val loss: 0.5242578387260437
Epoch 580, training loss: 907.7246704101562 = 0.39890506863594055 + 100.0 * 9.073257446289062
Epoch 580, val loss: 0.5231943726539612
Epoch 590, training loss: 907.633544921875 = 0.3964226245880127 + 100.0 * 9.072371482849121
Epoch 590, val loss: 0.5222077369689941
Epoch 600, training loss: 907.694091796875 = 0.39392805099487305 + 100.0 * 9.073001861572266
Epoch 600, val loss: 0.5213439464569092
Epoch 610, training loss: 907.90673828125 = 0.3913182020187378 + 100.0 * 9.075154304504395
Epoch 610, val loss: 0.5197486281394958
Epoch 620, training loss: 907.53662109375 = 0.3887055218219757 + 100.0 * 9.071478843688965
Epoch 620, val loss: 0.5184481739997864
Epoch 630, training loss: 907.3733520507812 = 0.38625243306159973 + 100.0 * 9.069870948791504
Epoch 630, val loss: 0.517664909362793
Epoch 640, training loss: 907.278076171875 = 0.3838804066181183 + 100.0 * 9.068942070007324
Epoch 640, val loss: 0.5166236162185669
Epoch 650, training loss: 907.309814453125 = 0.38149216771125793 + 100.0 * 9.069283485412598
Epoch 650, val loss: 0.5157487988471985
Epoch 660, training loss: 907.3578491210938 = 0.3789835572242737 + 100.0 * 9.069788932800293
Epoch 660, val loss: 0.5145432949066162
Epoch 670, training loss: 907.1569213867188 = 0.3764039874076843 + 100.0 * 9.067805290222168
Epoch 670, val loss: 0.5131393671035767
Epoch 680, training loss: 907.075439453125 = 0.37398436665534973 + 100.0 * 9.067014694213867
Epoch 680, val loss: 0.5123562216758728
Epoch 690, training loss: 906.9851684570312 = 0.3716388940811157 + 100.0 * 9.06613540649414
Epoch 690, val loss: 0.5112335085868835
Epoch 700, training loss: 906.9321899414062 = 0.36928248405456543 + 100.0 * 9.065629005432129
Epoch 700, val loss: 0.5103036165237427
Epoch 710, training loss: 907.1275024414062 = 0.3668561279773712 + 100.0 * 9.067605972290039
Epoch 710, val loss: 0.5093554258346558
Epoch 720, training loss: 906.8817138671875 = 0.36432185769081116 + 100.0 * 9.065174102783203
Epoch 720, val loss: 0.5082378387451172
Epoch 730, training loss: 906.77197265625 = 0.36186742782592773 + 100.0 * 9.064101219177246
Epoch 730, val loss: 0.5070545673370361
Epoch 740, training loss: 906.695068359375 = 0.3594580292701721 + 100.0 * 9.063356399536133
Epoch 740, val loss: 0.5062066912651062
Epoch 750, training loss: 906.67626953125 = 0.357052743434906 + 100.0 * 9.063192367553711
Epoch 750, val loss: 0.5050014853477478
Epoch 760, training loss: 906.6104736328125 = 0.3545142114162445 + 100.0 * 9.062560081481934
Epoch 760, val loss: 0.5040456652641296
Epoch 770, training loss: 906.658935546875 = 0.35194242000579834 + 100.0 * 9.063070297241211
Epoch 770, val loss: 0.5028301477432251
Epoch 780, training loss: 906.600830078125 = 0.3494254946708679 + 100.0 * 9.062514305114746
Epoch 780, val loss: 0.5018967390060425
Epoch 790, training loss: 906.4602661132812 = 0.3469409644603729 + 100.0 * 9.06113338470459
Epoch 790, val loss: 0.500925600528717
Epoch 800, training loss: 906.585205078125 = 0.34446054697036743 + 100.0 * 9.062407493591309
Epoch 800, val loss: 0.5001676678657532
Epoch 810, training loss: 906.3800659179688 = 0.3418678045272827 + 100.0 * 9.060381889343262
Epoch 810, val loss: 0.4986717700958252
Epoch 820, training loss: 906.3102416992188 = 0.3393149971961975 + 100.0 * 9.059709548950195
Epoch 820, val loss: 0.4979797899723053
Epoch 830, training loss: 906.24365234375 = 0.3367885649204254 + 100.0 * 9.05906867980957
Epoch 830, val loss: 0.49684280157089233
Epoch 840, training loss: 906.5606689453125 = 0.3342388868331909 + 100.0 * 9.062264442443848
Epoch 840, val loss: 0.495826780796051
Epoch 850, training loss: 906.3253784179688 = 0.33152469992637634 + 100.0 * 9.059938430786133
Epoch 850, val loss: 0.4949549734592438
Epoch 860, training loss: 906.1770629882812 = 0.32888177037239075 + 100.0 * 9.05848217010498
Epoch 860, val loss: 0.4940195083618164
Epoch 870, training loss: 906.0757446289062 = 0.32626044750213623 + 100.0 * 9.0574951171875
Epoch 870, val loss: 0.4929799735546112
Epoch 880, training loss: 906.0289306640625 = 0.32365521788597107 + 100.0 * 9.057052612304688
Epoch 880, val loss: 0.4919443428516388
Epoch 890, training loss: 906.0674438476562 = 0.3210139572620392 + 100.0 * 9.057464599609375
Epoch 890, val loss: 0.4909954071044922
Epoch 900, training loss: 906.2343139648438 = 0.3183032274246216 + 100.0 * 9.059160232543945
Epoch 900, val loss: 0.49033597111701965
Epoch 910, training loss: 906.0151977539062 = 0.31551477313041687 + 100.0 * 9.056997299194336
Epoch 910, val loss: 0.48890620470046997
Epoch 920, training loss: 905.8761596679688 = 0.31278687715530396 + 100.0 * 9.055633544921875
Epoch 920, val loss: 0.4882199466228485
Epoch 930, training loss: 905.860595703125 = 0.31009867787361145 + 100.0 * 9.05550479888916
Epoch 930, val loss: 0.48730379343032837
Epoch 940, training loss: 905.896484375 = 0.30734720826148987 + 100.0 * 9.055891036987305
Epoch 940, val loss: 0.4864225685596466
Epoch 950, training loss: 905.7703857421875 = 0.30456414818763733 + 100.0 * 9.054657936096191
Epoch 950, val loss: 0.48565685749053955
Epoch 960, training loss: 905.7899169921875 = 0.30180877447128296 + 100.0 * 9.05488109588623
Epoch 960, val loss: 0.48494166135787964
Epoch 970, training loss: 905.7103271484375 = 0.29899656772613525 + 100.0 * 9.054113388061523
Epoch 970, val loss: 0.48350656032562256
Epoch 980, training loss: 905.6738891601562 = 0.29617545008659363 + 100.0 * 9.053776741027832
Epoch 980, val loss: 0.4829134941101074
Epoch 990, training loss: 905.6166381835938 = 0.2934117019176483 + 100.0 * 9.053232192993164
Epoch 990, val loss: 0.4818524718284607
Epoch 1000, training loss: 905.6201782226562 = 0.290643572807312 + 100.0 * 9.053295135498047
Epoch 1000, val loss: 0.4808374047279358
Epoch 1010, training loss: 905.75439453125 = 0.28781798481941223 + 100.0 * 9.054665565490723
Epoch 1010, val loss: 0.47995519638061523
Epoch 1020, training loss: 905.5223999023438 = 0.2849130630493164 + 100.0 * 9.052374839782715
Epoch 1020, val loss: 0.47921106219291687
Epoch 1030, training loss: 905.5120849609375 = 0.28212130069732666 + 100.0 * 9.052299499511719
Epoch 1030, val loss: 0.47875693440437317
Epoch 1040, training loss: 905.4600830078125 = 0.27935561537742615 + 100.0 * 9.051807403564453
Epoch 1040, val loss: 0.4779525101184845
Epoch 1050, training loss: 905.6671142578125 = 0.2765703499317169 + 100.0 * 9.053905487060547
Epoch 1050, val loss: 0.4772666394710541
Epoch 1060, training loss: 905.5443725585938 = 0.27370405197143555 + 100.0 * 9.052706718444824
Epoch 1060, val loss: 0.4766256511211395
Epoch 1070, training loss: 905.4155883789062 = 0.2708688974380493 + 100.0 * 9.051446914672852
Epoch 1070, val loss: 0.4759100675582886
Epoch 1080, training loss: 905.35791015625 = 0.26809120178222656 + 100.0 * 9.050898551940918
Epoch 1080, val loss: 0.47498029470443726
Epoch 1090, training loss: 905.3673706054688 = 0.2653181850910187 + 100.0 * 9.051020622253418
Epoch 1090, val loss: 0.47467392683029175
Epoch 1100, training loss: 905.3799438476562 = 0.2625266909599304 + 100.0 * 9.05117416381836
Epoch 1100, val loss: 0.4739421606063843
Epoch 1110, training loss: 905.3031005859375 = 0.2597166895866394 + 100.0 * 9.050434112548828
Epoch 1110, val loss: 0.47381606698036194
Epoch 1120, training loss: 905.4354248046875 = 0.25694045424461365 + 100.0 * 9.05178451538086
Epoch 1120, val loss: 0.4734119772911072
Epoch 1130, training loss: 905.2257690429688 = 0.254146009683609 + 100.0 * 9.049715995788574
Epoch 1130, val loss: 0.4728506803512573
Epoch 1140, training loss: 905.1712036132812 = 0.25138187408447266 + 100.0 * 9.049198150634766
Epoch 1140, val loss: 0.4726582467556
Epoch 1150, training loss: 905.3477783203125 = 0.24863843619823456 + 100.0 * 9.05099105834961
Epoch 1150, val loss: 0.47283467650413513
Epoch 1160, training loss: 905.186767578125 = 0.24582576751708984 + 100.0 * 9.049408912658691
Epoch 1160, val loss: 0.4720064103603363
Epoch 1170, training loss: 905.1666870117188 = 0.24306032061576843 + 100.0 * 9.049236297607422
Epoch 1170, val loss: 0.47237274050712585
Epoch 1180, training loss: 905.102294921875 = 0.24034567177295685 + 100.0 * 9.048619270324707
Epoch 1180, val loss: 0.47181275486946106
Epoch 1190, training loss: 905.0899047851562 = 0.23763507604599 + 100.0 * 9.04852294921875
Epoch 1190, val loss: 0.4716469943523407
Epoch 1200, training loss: 905.147705078125 = 0.2349526286125183 + 100.0 * 9.049127578735352
Epoch 1200, val loss: 0.4712713062763214
Epoch 1210, training loss: 905.0355834960938 = 0.23221516609191895 + 100.0 * 9.048033714294434
Epoch 1210, val loss: 0.47181236743927
Epoch 1220, training loss: 905.0165405273438 = 0.22952152788639069 + 100.0 * 9.047870635986328
Epoch 1220, val loss: 0.4719291031360626
Epoch 1230, training loss: 905.10693359375 = 0.2268546223640442 + 100.0 * 9.048800468444824
Epoch 1230, val loss: 0.47186487913131714
Epoch 1240, training loss: 904.9490966796875 = 0.22414705157279968 + 100.0 * 9.047249794006348
Epoch 1240, val loss: 0.472713828086853
Epoch 1250, training loss: 904.9133911132812 = 0.2214958369731903 + 100.0 * 9.046918869018555
Epoch 1250, val loss: 0.47280317544937134
Epoch 1260, training loss: 904.91455078125 = 0.2188551276922226 + 100.0 * 9.046957015991211
Epoch 1260, val loss: 0.4732726812362671
Epoch 1270, training loss: 904.82958984375 = 0.21620556712150574 + 100.0 * 9.046133995056152
Epoch 1270, val loss: 0.47325751185417175
Epoch 1280, training loss: 904.983154296875 = 0.21361102163791656 + 100.0 * 9.04769515991211
Epoch 1280, val loss: 0.47319018840789795
Epoch 1290, training loss: 904.8470458984375 = 0.21093016862869263 + 100.0 * 9.046360969543457
Epoch 1290, val loss: 0.47414636611938477
Epoch 1300, training loss: 904.9039306640625 = 0.2083178460597992 + 100.0 * 9.046956062316895
Epoch 1300, val loss: 0.4744151532649994
Epoch 1310, training loss: 904.8111572265625 = 0.20571841299533844 + 100.0 * 9.04605484008789
Epoch 1310, val loss: 0.4752408564090729
Epoch 1320, training loss: 904.7670288085938 = 0.20314766466617584 + 100.0 * 9.045639038085938
Epoch 1320, val loss: 0.4758369028568268
Epoch 1330, training loss: 904.6975708007812 = 0.200603649020195 + 100.0 * 9.04496955871582
Epoch 1330, val loss: 0.4766211211681366
Epoch 1340, training loss: 904.675537109375 = 0.19808503985404968 + 100.0 * 9.044775009155273
Epoch 1340, val loss: 0.47721973061561584
Epoch 1350, training loss: 904.84716796875 = 0.1955866813659668 + 100.0 * 9.046516418457031
Epoch 1350, val loss: 0.4786934554576874
Epoch 1360, training loss: 904.8128662109375 = 0.1930513232946396 + 100.0 * 9.046197891235352
Epoch 1360, val loss: 0.4792785942554474
Epoch 1370, training loss: 904.6671142578125 = 0.19055955111980438 + 100.0 * 9.04476547241211
Epoch 1370, val loss: 0.47966909408569336
Epoch 1380, training loss: 904.6699829101562 = 0.18812446296215057 + 100.0 * 9.044818878173828
Epoch 1380, val loss: 0.48118045926094055
Epoch 1390, training loss: 904.6534423828125 = 0.1856914609670639 + 100.0 * 9.044677734375
Epoch 1390, val loss: 0.4818597137928009
Epoch 1400, training loss: 904.5750732421875 = 0.18326643109321594 + 100.0 * 9.043917655944824
Epoch 1400, val loss: 0.48291778564453125
Epoch 1410, training loss: 904.7051391601562 = 0.1809077262878418 + 100.0 * 9.045242309570312
Epoch 1410, val loss: 0.48434922099113464
Epoch 1420, training loss: 904.5021362304688 = 0.17849814891815186 + 100.0 * 9.043235778808594
Epoch 1420, val loss: 0.4846542179584503
Epoch 1430, training loss: 904.4691162109375 = 0.1761523187160492 + 100.0 * 9.042929649353027
Epoch 1430, val loss: 0.48627835512161255
Epoch 1440, training loss: 904.4974975585938 = 0.1738417148590088 + 100.0 * 9.04323673248291
Epoch 1440, val loss: 0.48758724331855774
Epoch 1450, training loss: 904.4942016601562 = 0.17151318490505219 + 100.0 * 9.043227195739746
Epoch 1450, val loss: 0.4886530935764313
Epoch 1460, training loss: 904.4298095703125 = 0.16921406984329224 + 100.0 * 9.042606353759766
Epoch 1460, val loss: 0.49011585116386414
Epoch 1470, training loss: 904.458251953125 = 0.16695630550384521 + 100.0 * 9.042913436889648
Epoch 1470, val loss: 0.4911864995956421
Epoch 1480, training loss: 904.5331420898438 = 0.16476087272167206 + 100.0 * 9.043684005737305
Epoch 1480, val loss: 0.4917066693305969
Epoch 1490, training loss: 904.4530639648438 = 0.16246512532234192 + 100.0 * 9.042905807495117
Epoch 1490, val loss: 0.49452054500579834
Epoch 1500, training loss: 904.331298828125 = 0.16024897992610931 + 100.0 * 9.041709899902344
Epoch 1500, val loss: 0.49573808908462524
Epoch 1510, training loss: 904.3069458007812 = 0.15806619822978973 + 100.0 * 9.041488647460938
Epoch 1510, val loss: 0.49770742654800415
Epoch 1520, training loss: 904.2723999023438 = 0.1559065729379654 + 100.0 * 9.041165351867676
Epoch 1520, val loss: 0.499138742685318
Epoch 1530, training loss: 904.4161987304688 = 0.15376925468444824 + 100.0 * 9.042624473571777
Epoch 1530, val loss: 0.5012651681900024
Epoch 1540, training loss: 904.2574462890625 = 0.15160194039344788 + 100.0 * 9.041058540344238
Epoch 1540, val loss: 0.5022535920143127
Epoch 1550, training loss: 904.256591796875 = 0.14947417378425598 + 100.0 * 9.041070938110352
Epoch 1550, val loss: 0.5046128630638123
Epoch 1560, training loss: 904.2891235351562 = 0.14738839864730835 + 100.0 * 9.041417121887207
Epoch 1560, val loss: 0.5060597658157349
Epoch 1570, training loss: 904.2545166015625 = 0.14530564844608307 + 100.0 * 9.041091918945312
Epoch 1570, val loss: 0.5078031420707703
Epoch 1580, training loss: 904.2933349609375 = 0.14328227937221527 + 100.0 * 9.041500091552734
Epoch 1580, val loss: 0.5083989500999451
Epoch 1590, training loss: 904.23681640625 = 0.141214981675148 + 100.0 * 9.040955543518066
Epoch 1590, val loss: 0.5105854868888855
Epoch 1600, training loss: 904.2626953125 = 0.1391911804676056 + 100.0 * 9.041234970092773
Epoch 1600, val loss: 0.512620747089386
Epoch 1610, training loss: 904.1570434570312 = 0.13719123601913452 + 100.0 * 9.04019832611084
Epoch 1610, val loss: 0.5148687958717346
Epoch 1620, training loss: 904.146240234375 = 0.1352287083864212 + 100.0 * 9.040109634399414
Epoch 1620, val loss: 0.5166717171669006
Epoch 1630, training loss: 904.1058349609375 = 0.13328808546066284 + 100.0 * 9.039725303649902
Epoch 1630, val loss: 0.5178741812705994
Epoch 1640, training loss: 904.0618896484375 = 0.13136498630046844 + 100.0 * 9.039305686950684
Epoch 1640, val loss: 0.519783079624176
Epoch 1650, training loss: 904.1966552734375 = 0.12949320673942566 + 100.0 * 9.040671348571777
Epoch 1650, val loss: 0.5213885307312012
Epoch 1660, training loss: 904.1304321289062 = 0.1275879293680191 + 100.0 * 9.04002857208252
Epoch 1660, val loss: 0.5249437093734741
Epoch 1670, training loss: 904.0838623046875 = 0.1257086843252182 + 100.0 * 9.039581298828125
Epoch 1670, val loss: 0.5264252424240112
Epoch 1680, training loss: 904.021728515625 = 0.12387952953577042 + 100.0 * 9.038978576660156
Epoch 1680, val loss: 0.5290749073028564
Epoch 1690, training loss: 904.0703125 = 0.12207002192735672 + 100.0 * 9.039482116699219
Epoch 1690, val loss: 0.5311412811279297
Epoch 1700, training loss: 904.030029296875 = 0.12026321887969971 + 100.0 * 9.039097785949707
Epoch 1700, val loss: 0.5327908992767334
Epoch 1710, training loss: 903.9934692382812 = 0.11847170442342758 + 100.0 * 9.038749694824219
Epoch 1710, val loss: 0.5349205136299133
Epoch 1720, training loss: 903.9609375 = 0.11672496795654297 + 100.0 * 9.038442611694336
Epoch 1720, val loss: 0.5378149747848511
Epoch 1730, training loss: 904.0529174804688 = 0.11499381810426712 + 100.0 * 9.039379119873047
Epoch 1730, val loss: 0.5404258966445923
Epoch 1740, training loss: 903.9052734375 = 0.11325371265411377 + 100.0 * 9.037919998168945
Epoch 1740, val loss: 0.5417097210884094
Epoch 1750, training loss: 903.8753662109375 = 0.1115599200129509 + 100.0 * 9.037637710571289
Epoch 1750, val loss: 0.5434200167655945
Epoch 1760, training loss: 903.9086303710938 = 0.1098821610212326 + 100.0 * 9.03798770904541
Epoch 1760, val loss: 0.5464624166488647
Epoch 1770, training loss: 903.8569946289062 = 0.10821723192930222 + 100.0 * 9.037487983703613
Epoch 1770, val loss: 0.5482298135757446
Epoch 1780, training loss: 903.9768676757812 = 0.10659371316432953 + 100.0 * 9.038702964782715
Epoch 1780, val loss: 0.5505787134170532
Epoch 1790, training loss: 903.7918090820312 = 0.10493934899568558 + 100.0 * 9.036869049072266
Epoch 1790, val loss: 0.5532627105712891
Epoch 1800, training loss: 903.9141845703125 = 0.10336394608020782 + 100.0 * 9.038107872009277
Epoch 1800, val loss: 0.554676353931427
Epoch 1810, training loss: 903.7772827148438 = 0.10175222158432007 + 100.0 * 9.036755561828613
Epoch 1810, val loss: 0.5576815605163574
Epoch 1820, training loss: 903.7864379882812 = 0.10019262880086899 + 100.0 * 9.03686237335205
Epoch 1820, val loss: 0.56017005443573
Epoch 1830, training loss: 903.7046508789062 = 0.09863541275262833 + 100.0 * 9.036060333251953
Epoch 1830, val loss: 0.5632438063621521
Epoch 1840, training loss: 903.6895751953125 = 0.09711547195911407 + 100.0 * 9.035924911499023
Epoch 1840, val loss: 0.5660305023193359
Epoch 1850, training loss: 903.870361328125 = 0.09563357383012772 + 100.0 * 9.037747383117676
Epoch 1850, val loss: 0.5690253376960754
Epoch 1860, training loss: 903.7455444335938 = 0.09412829577922821 + 100.0 * 9.036514282226562
Epoch 1860, val loss: 0.5703405141830444
Epoch 1870, training loss: 903.7561645507812 = 0.0926777720451355 + 100.0 * 9.036635398864746
Epoch 1870, val loss: 0.5747561454772949
Epoch 1880, training loss: 903.7974243164062 = 0.09121498465538025 + 100.0 * 9.037062644958496
Epoch 1880, val loss: 0.5765688419342041
Epoch 1890, training loss: 903.6463012695312 = 0.08976523578166962 + 100.0 * 9.035565376281738
Epoch 1890, val loss: 0.5791215300559998
Epoch 1900, training loss: 903.6585693359375 = 0.08838081359863281 + 100.0 * 9.035701751708984
Epoch 1900, val loss: 0.5823535323143005
Epoch 1910, training loss: 903.7546997070312 = 0.08701444417238235 + 100.0 * 9.036676406860352
Epoch 1910, val loss: 0.585506021976471
Epoch 1920, training loss: 903.6305541992188 = 0.08561001718044281 + 100.0 * 9.035449028015137
Epoch 1920, val loss: 0.585558295249939
Epoch 1930, training loss: 903.5479736328125 = 0.08424504846334457 + 100.0 * 9.034637451171875
Epoch 1930, val loss: 0.5893316268920898
Epoch 1940, training loss: 903.5076904296875 = 0.08292169868946075 + 100.0 * 9.034247398376465
Epoch 1940, val loss: 0.591543436050415
Epoch 1950, training loss: 903.5189208984375 = 0.08161840587854385 + 100.0 * 9.03437328338623
Epoch 1950, val loss: 0.5944111943244934
Epoch 1960, training loss: 903.7692260742188 = 0.08035464584827423 + 100.0 * 9.036888122558594
Epoch 1960, val loss: 0.5963870882987976
Epoch 1970, training loss: 903.707763671875 = 0.07911484688520432 + 100.0 * 9.036286354064941
Epoch 1970, val loss: 0.5982325673103333
Epoch 1980, training loss: 903.587158203125 = 0.07783100008964539 + 100.0 * 9.035093307495117
Epoch 1980, val loss: 0.6032484173774719
Epoch 1990, training loss: 903.4453735351562 = 0.0765661746263504 + 100.0 * 9.03368854522705
Epoch 1990, val loss: 0.6048687696456909
Epoch 2000, training loss: 903.4321899414062 = 0.07536060363054276 + 100.0 * 9.033568382263184
Epoch 2000, val loss: 0.6074784994125366
Epoch 2010, training loss: 903.5137939453125 = 0.07417301833629608 + 100.0 * 9.034396171569824
Epoch 2010, val loss: 0.6100825071334839
Epoch 2020, training loss: 903.6903686523438 = 0.07306825369596481 + 100.0 * 9.036172866821289
Epoch 2020, val loss: 0.6113480925559998
Epoch 2030, training loss: 903.4535522460938 = 0.07181882858276367 + 100.0 * 9.033817291259766
Epoch 2030, val loss: 0.6161317825317383
Epoch 2040, training loss: 903.3628540039062 = 0.07067771255970001 + 100.0 * 9.03292179107666
Epoch 2040, val loss: 0.6181844472885132
Epoch 2050, training loss: 903.3878784179688 = 0.06955765187740326 + 100.0 * 9.033183097839355
Epoch 2050, val loss: 0.6213424205780029
Epoch 2060, training loss: 903.5346069335938 = 0.06848367303609848 + 100.0 * 9.034661293029785
Epoch 2060, val loss: 0.6228200793266296
Epoch 2070, training loss: 903.3982543945312 = 0.06736321747303009 + 100.0 * 9.033308982849121
Epoch 2070, val loss: 0.6266515851020813
Epoch 2080, training loss: 903.3252563476562 = 0.06626897305250168 + 100.0 * 9.03258991241455
Epoch 2080, val loss: 0.6287256479263306
Epoch 2090, training loss: 903.529296875 = 0.06526453793048859 + 100.0 * 9.034640312194824
Epoch 2090, val loss: 0.6308923363685608
Epoch 2100, training loss: 903.343017578125 = 0.06420909613370895 + 100.0 * 9.032788276672363
Epoch 2100, val loss: 0.6371359825134277
Epoch 2110, training loss: 903.2954711914062 = 0.06313789635896683 + 100.0 * 9.032323837280273
Epoch 2110, val loss: 0.6378925442695618
Epoch 2120, training loss: 903.2919311523438 = 0.06213437393307686 + 100.0 * 9.03229808807373
Epoch 2120, val loss: 0.6413533091545105
Epoch 2130, training loss: 903.3699951171875 = 0.06114339083433151 + 100.0 * 9.033088684082031
Epoch 2130, val loss: 0.6440608501434326
Epoch 2140, training loss: 903.36962890625 = 0.06015973910689354 + 100.0 * 9.03309440612793
Epoch 2140, val loss: 0.6468467116355896
Epoch 2150, training loss: 903.2076416015625 = 0.059199631214141846 + 100.0 * 9.031484603881836
Epoch 2150, val loss: 0.6487185955047607
Epoch 2160, training loss: 903.2555541992188 = 0.05827748030424118 + 100.0 * 9.031972885131836
Epoch 2160, val loss: 0.6508713960647583
Epoch 2170, training loss: 903.208740234375 = 0.05733434110879898 + 100.0 * 9.031514167785645
Epoch 2170, val loss: 0.654356837272644
Epoch 2180, training loss: 903.2582397460938 = 0.05641487240791321 + 100.0 * 9.032018661499023
Epoch 2180, val loss: 0.6586580276489258
Epoch 2190, training loss: 903.2239379882812 = 0.055514153093099594 + 100.0 * 9.031683921813965
Epoch 2190, val loss: 0.6602712273597717
Epoch 2200, training loss: 903.2118530273438 = 0.05463103577494621 + 100.0 * 9.031572341918945
Epoch 2200, val loss: 0.6636945009231567
Epoch 2210, training loss: 903.2745361328125 = 0.053784165531396866 + 100.0 * 9.032207489013672
Epoch 2210, val loss: 0.6662724018096924
Epoch 2220, training loss: 903.2152709960938 = 0.052924901247024536 + 100.0 * 9.031623840332031
Epoch 2220, val loss: 0.6686321496963501
Epoch 2230, training loss: 903.1893920898438 = 0.05208319425582886 + 100.0 * 9.031373023986816
Epoch 2230, val loss: 0.6716793775558472
Epoch 2240, training loss: 903.1036987304688 = 0.051237188279628754 + 100.0 * 9.030525207519531
Epoch 2240, val loss: 0.6754494905471802
Epoch 2250, training loss: 903.149169921875 = 0.05044407397508621 + 100.0 * 9.030987739562988
Epoch 2250, val loss: 0.6778584718704224
Epoch 2260, training loss: 903.2172241210938 = 0.049672700464725494 + 100.0 * 9.031675338745117
Epoch 2260, val loss: 0.6800263524055481
Epoch 2270, training loss: 903.1170654296875 = 0.048844482749700546 + 100.0 * 9.030682563781738
Epoch 2270, val loss: 0.6845583319664001
Epoch 2280, training loss: 903.0601806640625 = 0.04807974398136139 + 100.0 * 9.030120849609375
Epoch 2280, val loss: 0.6869564652442932
Epoch 2290, training loss: 903.1248168945312 = 0.04732301086187363 + 100.0 * 9.03077507019043
Epoch 2290, val loss: 0.6910490393638611
Epoch 2300, training loss: 903.0529174804688 = 0.04656873643398285 + 100.0 * 9.03006362915039
Epoch 2300, val loss: 0.6932279467582703
Epoch 2310, training loss: 903.0313110351562 = 0.04583403840661049 + 100.0 * 9.029854774475098
Epoch 2310, val loss: 0.6962707042694092
Epoch 2320, training loss: 903.1961059570312 = 0.0451463907957077 + 100.0 * 9.031509399414062
Epoch 2320, val loss: 0.7003635168075562
Epoch 2330, training loss: 903.0701293945312 = 0.044414445757865906 + 100.0 * 9.030257225036621
Epoch 2330, val loss: 0.7024588584899902
Epoch 2340, training loss: 902.97412109375 = 0.043715670704841614 + 100.0 * 9.029304504394531
Epoch 2340, val loss: 0.705409586429596
Epoch 2350, training loss: 902.949951171875 = 0.04302448034286499 + 100.0 * 9.029068946838379
Epoch 2350, val loss: 0.707249641418457
Epoch 2360, training loss: 902.95166015625 = 0.04236219450831413 + 100.0 * 9.029092788696289
Epoch 2360, val loss: 0.7109391689300537
Epoch 2370, training loss: 903.1831665039062 = 0.0417647510766983 + 100.0 * 9.031414031982422
Epoch 2370, val loss: 0.7150047421455383
Epoch 2380, training loss: 903.0339965820312 = 0.04106154292821884 + 100.0 * 9.029929161071777
Epoch 2380, val loss: 0.7160190939903259
Epoch 2390, training loss: 902.9781494140625 = 0.04044688493013382 + 100.0 * 9.029376983642578
Epoch 2390, val loss: 0.720192551612854
Epoch 2400, training loss: 903.037353515625 = 0.03980182856321335 + 100.0 * 9.029975891113281
Epoch 2400, val loss: 0.7217414379119873
Epoch 2410, training loss: 902.8958129882812 = 0.03918807953596115 + 100.0 * 9.028566360473633
Epoch 2410, val loss: 0.7239500880241394
Epoch 2420, training loss: 902.8868408203125 = 0.038582973182201385 + 100.0 * 9.028482437133789
Epoch 2420, val loss: 0.7273281216621399
Epoch 2430, training loss: 903.0858764648438 = 0.038008783012628555 + 100.0 * 9.030478477478027
Epoch 2430, val loss: 0.7298969030380249
Epoch 2440, training loss: 902.916748046875 = 0.03742063418030739 + 100.0 * 9.028793334960938
Epoch 2440, val loss: 0.7322733998298645
Epoch 2450, training loss: 902.8546142578125 = 0.03684566542506218 + 100.0 * 9.028177261352539
Epoch 2450, val loss: 0.7358384728431702
Epoch 2460, training loss: 902.8334350585938 = 0.0362991988658905 + 100.0 * 9.027971267700195
Epoch 2460, val loss: 0.7380820512771606
Epoch 2470, training loss: 903.0148315429688 = 0.035800233483314514 + 100.0 * 9.029789924621582
Epoch 2470, val loss: 0.7386839985847473
Epoch 2480, training loss: 902.8832397460938 = 0.03522881865501404 + 100.0 * 9.028480529785156
Epoch 2480, val loss: 0.7429261803627014
Epoch 2490, training loss: 902.8047485351562 = 0.034699078649282455 + 100.0 * 9.027700424194336
Epoch 2490, val loss: 0.7454431056976318
Epoch 2500, training loss: 902.8223266601562 = 0.034186746925115585 + 100.0 * 9.027881622314453
Epoch 2500, val loss: 0.7481447458267212
Epoch 2510, training loss: 902.8030395507812 = 0.03368190675973892 + 100.0 * 9.027693748474121
Epoch 2510, val loss: 0.7504931092262268
Epoch 2520, training loss: 902.9238891601562 = 0.03320065513253212 + 100.0 * 9.02890682220459
Epoch 2520, val loss: 0.7544203400611877
Epoch 2530, training loss: 902.78857421875 = 0.03269021958112717 + 100.0 * 9.027558326721191
Epoch 2530, val loss: 0.7575947046279907
Epoch 2540, training loss: 902.7257690429688 = 0.03221188113093376 + 100.0 * 9.026935577392578
Epoch 2540, val loss: 0.758745551109314
Epoch 2550, training loss: 902.7720947265625 = 0.03178972378373146 + 100.0 * 9.027402877807617
Epoch 2550, val loss: 0.7607486844062805
Epoch 2560, training loss: 902.8516845703125 = 0.031288400292396545 + 100.0 * 9.028203964233398
Epoch 2560, val loss: 0.7648086547851562
Epoch 2570, training loss: 902.8037109375 = 0.03084244765341282 + 100.0 * 9.027729034423828
Epoch 2570, val loss: 0.7684157490730286
Epoch 2580, training loss: 902.7142333984375 = 0.030388182029128075 + 100.0 * 9.026838302612305
Epoch 2580, val loss: 0.7702473998069763
Epoch 2590, training loss: 902.6793823242188 = 0.02995182015001774 + 100.0 * 9.026494026184082
Epoch 2590, val loss: 0.7734536528587341
Epoch 2600, training loss: 902.7605590820312 = 0.029551437124609947 + 100.0 * 9.027310371398926
Epoch 2600, val loss: 0.7744751572608948
Epoch 2610, training loss: 902.6812744140625 = 0.029118195176124573 + 100.0 * 9.026521682739258
Epoch 2610, val loss: 0.7778102159500122
Epoch 2620, training loss: 902.7428588867188 = 0.028710976243019104 + 100.0 * 9.027141571044922
Epoch 2620, val loss: 0.7803675532341003
Epoch 2630, training loss: 902.6386108398438 = 0.02829621732234955 + 100.0 * 9.026103019714355
Epoch 2630, val loss: 0.7839414477348328
Epoch 2640, training loss: 902.706787109375 = 0.027915742248296738 + 100.0 * 9.026788711547852
Epoch 2640, val loss: 0.7876154184341431
Epoch 2650, training loss: 902.6964111328125 = 0.02752440795302391 + 100.0 * 9.026688575744629
Epoch 2650, val loss: 0.7872589230537415
Epoch 2660, training loss: 902.8442993164062 = 0.027180371806025505 + 100.0 * 9.02817153930664
Epoch 2660, val loss: 0.7904027700424194
Epoch 2670, training loss: 902.6161499023438 = 0.026757054030895233 + 100.0 * 9.025894165039062
Epoch 2670, val loss: 0.7929677367210388
Epoch 2680, training loss: 902.5810546875 = 0.02638791874051094 + 100.0 * 9.02554702758789
Epoch 2680, val loss: 0.7977834343910217
Epoch 2690, training loss: 902.5684204101562 = 0.026024499908089638 + 100.0 * 9.025424003601074
Epoch 2690, val loss: 0.7988253235816956
Epoch 2700, training loss: 902.95654296875 = 0.02569371648132801 + 100.0 * 9.029308319091797
Epoch 2700, val loss: 0.8022574186325073
Epoch 2710, training loss: 902.7174682617188 = 0.025344014167785645 + 100.0 * 9.026921272277832
Epoch 2710, val loss: 0.8034306764602661
Epoch 2720, training loss: 902.5294189453125 = 0.024971049278974533 + 100.0 * 9.025044441223145
Epoch 2720, val loss: 0.806387722492218
Epoch 2730, training loss: 902.4915161132812 = 0.024637099355459213 + 100.0 * 9.02466869354248
Epoch 2730, val loss: 0.8094682097434998
Epoch 2740, training loss: 902.5678100585938 = 0.024316225200891495 + 100.0 * 9.025435447692871
Epoch 2740, val loss: 0.8116274476051331
Epoch 2750, training loss: 902.6810913085938 = 0.02400502935051918 + 100.0 * 9.026571273803711
Epoch 2750, val loss: 0.8146615624427795
Epoch 2760, training loss: 902.541748046875 = 0.02367573417723179 + 100.0 * 9.02518081665039
Epoch 2760, val loss: 0.815510630607605
Epoch 2770, training loss: 902.5078125 = 0.023355040699243546 + 100.0 * 9.0248441696167
Epoch 2770, val loss: 0.819123387336731
Epoch 2780, training loss: 902.72509765625 = 0.023059599101543427 + 100.0 * 9.027020454406738
Epoch 2780, val loss: 0.821951687335968
Epoch 2790, training loss: 902.5936889648438 = 0.022753626108169556 + 100.0 * 9.02570915222168
Epoch 2790, val loss: 0.8238976001739502
Epoch 2800, training loss: 902.4597778320312 = 0.02245071716606617 + 100.0 * 9.024373054504395
Epoch 2800, val loss: 0.8254611492156982
Epoch 2810, training loss: 902.4320068359375 = 0.0221560038626194 + 100.0 * 9.02409839630127
Epoch 2810, val loss: 0.8282140493392944
Epoch 2820, training loss: 902.443603515625 = 0.021878233179450035 + 100.0 * 9.02421760559082
Epoch 2820, val loss: 0.8296501040458679
Epoch 2830, training loss: 902.6488037109375 = 0.021617893129587173 + 100.0 * 9.02627182006836
Epoch 2830, val loss: 0.8314155340194702
Epoch 2840, training loss: 902.5924072265625 = 0.021323570981621742 + 100.0 * 9.025711059570312
Epoch 2840, val loss: 0.8359435200691223
Epoch 2850, training loss: 902.4638671875 = 0.02105041779577732 + 100.0 * 9.024428367614746
Epoch 2850, val loss: 0.8370760083198547
Epoch 2860, training loss: 902.413330078125 = 0.020778043195605278 + 100.0 * 9.02392578125
Epoch 2860, val loss: 0.8398637175559998
Epoch 2870, training loss: 902.5261840820312 = 0.020548995584249496 + 100.0 * 9.025055885314941
Epoch 2870, val loss: 0.8400049805641174
Epoch 2880, training loss: 902.3969116210938 = 0.020263124257326126 + 100.0 * 9.02376651763916
Epoch 2880, val loss: 0.8438503742218018
Epoch 2890, training loss: 902.7274780273438 = 0.020072955638170242 + 100.0 * 9.027073860168457
Epoch 2890, val loss: 0.8444312810897827
Epoch 2900, training loss: 902.4706420898438 = 0.019767016172409058 + 100.0 * 9.024508476257324
Epoch 2900, val loss: 0.8495174646377563
Epoch 2910, training loss: 902.3517456054688 = 0.019520742818713188 + 100.0 * 9.023322105407715
Epoch 2910, val loss: 0.8504636287689209
Epoch 2920, training loss: 902.3158569335938 = 0.01927381567656994 + 100.0 * 9.022965431213379
Epoch 2920, val loss: 0.8537918329238892
Epoch 2930, training loss: 902.328369140625 = 0.01904132589697838 + 100.0 * 9.023093223571777
Epoch 2930, val loss: 0.8554654717445374
Epoch 2940, training loss: 902.608154296875 = 0.018852094188332558 + 100.0 * 9.025893211364746
Epoch 2940, val loss: 0.8587740659713745
Epoch 2950, training loss: 902.6253051757812 = 0.018614424392580986 + 100.0 * 9.026066780090332
Epoch 2950, val loss: 0.8612636923789978
Epoch 2960, training loss: 902.3370361328125 = 0.018369847908616066 + 100.0 * 9.023186683654785
Epoch 2960, val loss: 0.8622960448265076
Epoch 2970, training loss: 902.2659301757812 = 0.018139975145459175 + 100.0 * 9.022478103637695
Epoch 2970, val loss: 0.8640224933624268
Epoch 2980, training loss: 902.26708984375 = 0.01793147623538971 + 100.0 * 9.022491455078125
Epoch 2980, val loss: 0.8661965131759644
Epoch 2990, training loss: 902.3564453125 = 0.017741734161973 + 100.0 * 9.02338695526123
Epoch 2990, val loss: 0.8670165538787842
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8351
Overall ASR: 0.6968
Flip ASR: 0.6236/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.30 GiB already allocated; 323.69 MiB free; 4.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 565.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 609.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 565.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 569.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 565.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 287.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.995361328125 = 1.0987359285354614 + 100.0 * 10.358965873718262
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 11.69 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.00244140625 = 1.109650731086731 + 100.0 * 10.358927726745605
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 7.69 MiB free; 6.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.010498046875 = 1.1027066707611084 + 100.0 * 10.359078407287598
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 679.69 MiB free; 6.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0120849609375 = 1.1053889989852905 + 100.0 * 10.359067916870117
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 683.69 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0076904296875 = 1.102618932723999 + 100.0 * 10.359049797058105
Epoch 0, val loss: 1.100579857826233
Epoch 10, training loss: 1036.6407470703125 = 1.092157244682312 + 100.0 * 10.355485916137695
Epoch 10, val loss: 1.089869737625122
Epoch 20, training loss: 1031.1593017578125 = 1.0777862071990967 + 100.0 * 10.30081558227539
Epoch 20, val loss: 1.075626254081726
Epoch 30, training loss: 981.5995483398438 = 1.0641270875930786 + 100.0 * 9.805354118347168
Epoch 30, val loss: 1.0623124837875366
Epoch 40, training loss: 959.0311279296875 = 1.0502570867538452 + 100.0 * 9.579809188842773
Epoch 40, val loss: 1.048680067062378
Epoch 50, training loss: 948.5745239257812 = 1.0357892513275146 + 100.0 * 9.475387573242188
Epoch 50, val loss: 1.03444242477417
Epoch 60, training loss: 942.15185546875 = 1.0213274955749512 + 100.0 * 9.41130542755127
Epoch 60, val loss: 1.020187497138977
Epoch 70, training loss: 937.7516479492188 = 1.0048551559448242 + 100.0 * 9.367467880249023
Epoch 70, val loss: 1.0041011571884155
Epoch 80, training loss: 933.6365356445312 = 0.9900292158126831 + 100.0 * 9.326464653015137
Epoch 80, val loss: 0.9898291230201721
Epoch 90, training loss: 930.5594482421875 = 0.9753829836845398 + 100.0 * 9.2958402633667
Epoch 90, val loss: 0.9753838777542114
Epoch 100, training loss: 927.4802856445312 = 0.9581692814826965 + 100.0 * 9.265220642089844
Epoch 100, val loss: 0.9587341547012329
Epoch 110, training loss: 924.5475463867188 = 0.94123375415802 + 100.0 * 9.236063003540039
Epoch 110, val loss: 0.9425275921821594
Epoch 120, training loss: 922.8442993164062 = 0.9221204519271851 + 100.0 * 9.219222068786621
Epoch 120, val loss: 0.9241454005241394
Epoch 130, training loss: 920.940673828125 = 0.9004988074302673 + 100.0 * 9.200401306152344
Epoch 130, val loss: 0.903350293636322
Epoch 140, training loss: 919.6671752929688 = 0.87893146276474 + 100.0 * 9.187882423400879
Epoch 140, val loss: 0.8827394247055054
Epoch 150, training loss: 918.8778076171875 = 0.856444776058197 + 100.0 * 9.180213928222656
Epoch 150, val loss: 0.8615376353263855
Epoch 160, training loss: 917.5906372070312 = 0.83363938331604 + 100.0 * 9.167570114135742
Epoch 160, val loss: 0.8401918411254883
Epoch 170, training loss: 916.6602783203125 = 0.8110983967781067 + 100.0 * 9.158492088317871
Epoch 170, val loss: 0.819665789604187
Epoch 180, training loss: 915.8621826171875 = 0.788719654083252 + 100.0 * 9.150734901428223
Epoch 180, val loss: 0.7991650700569153
Epoch 190, training loss: 915.0745239257812 = 0.7663647532463074 + 100.0 * 9.143081665039062
Epoch 190, val loss: 0.7789564728736877
Epoch 200, training loss: 914.88671875 = 0.7442095279693604 + 100.0 * 9.141425132751465
Epoch 200, val loss: 0.7591485381126404
Epoch 210, training loss: 913.8507080078125 = 0.7218758463859558 + 100.0 * 9.131288528442383
Epoch 210, val loss: 0.7393138408660889
Epoch 220, training loss: 913.3003540039062 = 0.7006161212921143 + 100.0 * 9.125997543334961
Epoch 220, val loss: 0.7209295034408569
Epoch 230, training loss: 912.833984375 = 0.6804861426353455 + 100.0 * 9.121535301208496
Epoch 230, val loss: 0.7037136554718018
Epoch 240, training loss: 913.4453125 = 0.6613983511924744 + 100.0 * 9.127839088439941
Epoch 240, val loss: 0.6875718832015991
Epoch 250, training loss: 912.322265625 = 0.6422295570373535 + 100.0 * 9.116800308227539
Epoch 250, val loss: 0.6715646386146545
Epoch 260, training loss: 911.8368530273438 = 0.6250277161598206 + 100.0 * 9.1121187210083
Epoch 260, val loss: 0.6577542424201965
Epoch 270, training loss: 911.513427734375 = 0.609639048576355 + 100.0 * 9.109038352966309
Epoch 270, val loss: 0.6454539895057678
Epoch 280, training loss: 911.277099609375 = 0.5954387784004211 + 100.0 * 9.106816291809082
Epoch 280, val loss: 0.6344954967498779
Epoch 290, training loss: 911.1939086914062 = 0.582245945930481 + 100.0 * 9.10611629486084
Epoch 290, val loss: 0.6245139241218567
Epoch 300, training loss: 910.9080810546875 = 0.5697659850120544 + 100.0 * 9.10338306427002
Epoch 300, val loss: 0.615023672580719
Epoch 310, training loss: 910.7022094726562 = 0.5585457682609558 + 100.0 * 9.101436614990234
Epoch 310, val loss: 0.6069428324699402
Epoch 320, training loss: 910.5952758789062 = 0.5485633015632629 + 100.0 * 9.10046672821045
Epoch 320, val loss: 0.5999689698219299
Epoch 330, training loss: 910.3753051757812 = 0.5392797589302063 + 100.0 * 9.098360061645508
Epoch 330, val loss: 0.5935282111167908
Epoch 340, training loss: 910.2398681640625 = 0.5308034420013428 + 100.0 * 9.097090721130371
Epoch 340, val loss: 0.5878676772117615
Epoch 350, training loss: 910.1392211914062 = 0.5231151580810547 + 100.0 * 9.096160888671875
Epoch 350, val loss: 0.5827397108078003
Epoch 360, training loss: 910.2304077148438 = 0.5156545042991638 + 100.0 * 9.097147941589355
Epoch 360, val loss: 0.578311026096344
Epoch 370, training loss: 909.910400390625 = 0.5087842345237732 + 100.0 * 9.094016075134277
Epoch 370, val loss: 0.573900580406189
Epoch 380, training loss: 909.7277221679688 = 0.5026454329490662 + 100.0 * 9.09225082397461
Epoch 380, val loss: 0.5702239871025085
Epoch 390, training loss: 909.6005859375 = 0.4968856871128082 + 100.0 * 9.091036796569824
Epoch 390, val loss: 0.5668331980705261
Epoch 400, training loss: 909.6405639648438 = 0.49137523770332336 + 100.0 * 9.09149169921875
Epoch 400, val loss: 0.563540518283844
Epoch 410, training loss: 909.4014282226562 = 0.48605117201805115 + 100.0 * 9.089154243469238
Epoch 410, val loss: 0.5604502558708191
Epoch 420, training loss: 909.3046875 = 0.4811088442802429 + 100.0 * 9.088235855102539
Epoch 420, val loss: 0.5575484037399292
Epoch 430, training loss: 909.2127685546875 = 0.4760732650756836 + 100.0 * 9.087367057800293
Epoch 430, val loss: 0.5544477105140686
Epoch 440, training loss: 909.0445556640625 = 0.4713372588157654 + 100.0 * 9.085732460021973
Epoch 440, val loss: 0.5515986680984497
Epoch 450, training loss: 908.9400024414062 = 0.46698468923568726 + 100.0 * 9.08473014831543
Epoch 450, val loss: 0.5490162968635559
Epoch 460, training loss: 908.8757934570312 = 0.4627712666988373 + 100.0 * 9.08413028717041
Epoch 460, val loss: 0.5463632941246033
Epoch 470, training loss: 908.8033447265625 = 0.45845142006874084 + 100.0 * 9.083449363708496
Epoch 470, val loss: 0.5438848733901978
Epoch 480, training loss: 908.724853515625 = 0.45421308279037476 + 100.0 * 9.082706451416016
Epoch 480, val loss: 0.5408515930175781
Epoch 490, training loss: 908.6060791015625 = 0.45023053884506226 + 100.0 * 9.081558227539062
Epoch 490, val loss: 0.5381960272789001
Epoch 500, training loss: 908.6353759765625 = 0.44632628560066223 + 100.0 * 9.081890106201172
Epoch 500, val loss: 0.535502016544342
Epoch 510, training loss: 908.5165405273438 = 0.44230785965919495 + 100.0 * 9.080741882324219
Epoch 510, val loss: 0.5327703356742859
Epoch 520, training loss: 908.4381713867188 = 0.43837109208106995 + 100.0 * 9.079998016357422
Epoch 520, val loss: 0.5299310684204102
Epoch 530, training loss: 908.3018188476562 = 0.4345375895500183 + 100.0 * 9.078673362731934
Epoch 530, val loss: 0.5270169377326965
Epoch 540, training loss: 908.3076782226562 = 0.4307959973812103 + 100.0 * 9.078768730163574
Epoch 540, val loss: 0.5244917273521423
Epoch 550, training loss: 908.142333984375 = 0.4269677996635437 + 100.0 * 9.077154159545898
Epoch 550, val loss: 0.5214036703109741
Epoch 560, training loss: 908.09423828125 = 0.4232618510723114 + 100.0 * 9.076709747314453
Epoch 560, val loss: 0.5185645818710327
Epoch 570, training loss: 908.2305297851562 = 0.41956835985183716 + 100.0 * 9.078109741210938
Epoch 570, val loss: 0.5158276557922363
Epoch 580, training loss: 907.9552001953125 = 0.4158386290073395 + 100.0 * 9.075393676757812
Epoch 580, val loss: 0.5126610398292542
Epoch 590, training loss: 907.847900390625 = 0.4122729003429413 + 100.0 * 9.074356079101562
Epoch 590, val loss: 0.5097803473472595
Epoch 600, training loss: 907.8748779296875 = 0.4087400734424591 + 100.0 * 9.074661254882812
Epoch 600, val loss: 0.5070533752441406
Epoch 610, training loss: 907.7711181640625 = 0.4050162434577942 + 100.0 * 9.073660850524902
Epoch 610, val loss: 0.5040071606636047
Epoch 620, training loss: 907.7036743164062 = 0.4014698565006256 + 100.0 * 9.07302188873291
Epoch 620, val loss: 0.5011410713195801
Epoch 630, training loss: 907.658935546875 = 0.39804744720458984 + 100.0 * 9.072608947753906
Epoch 630, val loss: 0.4983859062194824
Epoch 640, training loss: 907.6013793945312 = 0.39465394616127014 + 100.0 * 9.072067260742188
Epoch 640, val loss: 0.4955461323261261
Epoch 650, training loss: 907.4915771484375 = 0.3913169205188751 + 100.0 * 9.071002960205078
Epoch 650, val loss: 0.4927772283554077
Epoch 660, training loss: 907.7569580078125 = 0.3879856765270233 + 100.0 * 9.073689460754395
Epoch 660, val loss: 0.4898570477962494
Epoch 670, training loss: 907.4144287109375 = 0.3845210373401642 + 100.0 * 9.07029914855957
Epoch 670, val loss: 0.48725050687789917
Epoch 680, training loss: 907.3361206054688 = 0.38133206963539124 + 100.0 * 9.069547653198242
Epoch 680, val loss: 0.4850930869579315
Epoch 690, training loss: 907.5321655273438 = 0.3781664967536926 + 100.0 * 9.071539878845215
Epoch 690, val loss: 0.4828779995441437
Epoch 700, training loss: 907.2711181640625 = 0.3750200867652893 + 100.0 * 9.068961143493652
Epoch 700, val loss: 0.4798291325569153
Epoch 710, training loss: 907.1743774414062 = 0.3720017671585083 + 100.0 * 9.068023681640625
Epoch 710, val loss: 0.4777962267398834
Epoch 720, training loss: 907.302978515625 = 0.3690299391746521 + 100.0 * 9.069339752197266
Epoch 720, val loss: 0.47551971673965454
Epoch 730, training loss: 907.1917114257812 = 0.3659384548664093 + 100.0 * 9.068257331848145
Epoch 730, val loss: 0.47304007411003113
Epoch 740, training loss: 907.0963745117188 = 0.36299818754196167 + 100.0 * 9.067334175109863
Epoch 740, val loss: 0.47122618556022644
Epoch 750, training loss: 907.0025024414062 = 0.3601633608341217 + 100.0 * 9.066423416137695
Epoch 750, val loss: 0.46927008032798767
Epoch 760, training loss: 907.0465087890625 = 0.3573949933052063 + 100.0 * 9.066890716552734
Epoch 760, val loss: 0.467174768447876
Epoch 770, training loss: 906.909912109375 = 0.3546023666858673 + 100.0 * 9.065552711486816
Epoch 770, val loss: 0.465375691652298
Epoch 780, training loss: 907.1643676757812 = 0.3518696129322052 + 100.0 * 9.068124771118164
Epoch 780, val loss: 0.46381568908691406
Epoch 790, training loss: 906.9114990234375 = 0.34904688596725464 + 100.0 * 9.065624237060547
Epoch 790, val loss: 0.46125924587249756
Epoch 800, training loss: 906.7783813476562 = 0.3464309871196747 + 100.0 * 9.064319610595703
Epoch 800, val loss: 0.45996129512786865
Epoch 810, training loss: 906.713134765625 = 0.34392544627189636 + 100.0 * 9.063692092895508
Epoch 810, val loss: 0.4584336280822754
Epoch 820, training loss: 906.6593627929688 = 0.341474711894989 + 100.0 * 9.063179016113281
Epoch 820, val loss: 0.4567963182926178
Epoch 830, training loss: 906.6530151367188 = 0.3390214741230011 + 100.0 * 9.063139915466309
Epoch 830, val loss: 0.45533373951911926
Epoch 840, training loss: 906.694091796875 = 0.3364526927471161 + 100.0 * 9.063576698303223
Epoch 840, val loss: 0.45398420095443726
Epoch 850, training loss: 906.6747436523438 = 0.33390122652053833 + 100.0 * 9.063407897949219
Epoch 850, val loss: 0.4522342085838318
Epoch 860, training loss: 906.53955078125 = 0.33151328563690186 + 100.0 * 9.062080383300781
Epoch 860, val loss: 0.4508424401283264
Epoch 870, training loss: 906.4554443359375 = 0.32920461893081665 + 100.0 * 9.061262130737305
Epoch 870, val loss: 0.44958803057670593
Epoch 880, training loss: 906.6165161132812 = 0.32695192098617554 + 100.0 * 9.062895774841309
Epoch 880, val loss: 0.4479091167449951
Epoch 890, training loss: 906.5296020507812 = 0.3245466351509094 + 100.0 * 9.062050819396973
Epoch 890, val loss: 0.4472883343696594
Epoch 900, training loss: 906.5186767578125 = 0.32222893834114075 + 100.0 * 9.061964988708496
Epoch 900, val loss: 0.4460224211215973
Epoch 910, training loss: 906.341552734375 = 0.31993958353996277 + 100.0 * 9.060215950012207
Epoch 910, val loss: 0.44480833411216736
Epoch 920, training loss: 906.2771606445312 = 0.3177734613418579 + 100.0 * 9.05959415435791
Epoch 920, val loss: 0.44377103447914124
Epoch 930, training loss: 906.299560546875 = 0.3156425654888153 + 100.0 * 9.059839248657227
Epoch 930, val loss: 0.4426962435245514
Epoch 940, training loss: 906.5675048828125 = 0.3134465515613556 + 100.0 * 9.062540054321289
Epoch 940, val loss: 0.44215601682662964
Epoch 950, training loss: 906.209716796875 = 0.3112190365791321 + 100.0 * 9.058984756469727
Epoch 950, val loss: 0.4404264986515045
Epoch 960, training loss: 906.1575317382812 = 0.30911675095558167 + 100.0 * 9.058484077453613
Epoch 960, val loss: 0.4400516152381897
Epoch 970, training loss: 906.0828857421875 = 0.3070763647556305 + 100.0 * 9.057758331298828
Epoch 970, val loss: 0.439034640789032
Epoch 980, training loss: 906.3936157226562 = 0.30505889654159546 + 100.0 * 9.060885429382324
Epoch 980, val loss: 0.43785759806632996
Epoch 990, training loss: 906.22705078125 = 0.30289652943611145 + 100.0 * 9.05924129486084
Epoch 990, val loss: 0.43735647201538086
Epoch 1000, training loss: 905.9820556640625 = 0.30084285140037537 + 100.0 * 9.056812286376953
Epoch 1000, val loss: 0.4366190433502197
Epoch 1010, training loss: 905.9755249023438 = 0.29886409640312195 + 100.0 * 9.056766510009766
Epoch 1010, val loss: 0.43549007177352905
Epoch 1020, training loss: 906.2734985351562 = 0.29692453145980835 + 100.0 * 9.059765815734863
Epoch 1020, val loss: 0.43449661135673523
Epoch 1030, training loss: 906.04638671875 = 0.2948264181613922 + 100.0 * 9.057516098022461
Epoch 1030, val loss: 0.4346015453338623
Epoch 1040, training loss: 905.8563842773438 = 0.29285410046577454 + 100.0 * 9.055635452270508
Epoch 1040, val loss: 0.4336811900138855
Epoch 1050, training loss: 905.8342895507812 = 0.2909254729747772 + 100.0 * 9.05543327331543
Epoch 1050, val loss: 0.43293336033821106
Epoch 1060, training loss: 906.1945190429688 = 0.2890135645866394 + 100.0 * 9.05905532836914
Epoch 1060, val loss: 0.43300673365592957
Epoch 1070, training loss: 905.8382568359375 = 0.2869863212108612 + 100.0 * 9.055512428283691
Epoch 1070, val loss: 0.43158647418022156
Epoch 1080, training loss: 905.744873046875 = 0.285072386264801 + 100.0 * 9.054597854614258
Epoch 1080, val loss: 0.4307733178138733
Epoch 1090, training loss: 905.7116088867188 = 0.2831974923610687 + 100.0 * 9.05428409576416
Epoch 1090, val loss: 0.4304029047489166
Epoch 1100, training loss: 905.881591796875 = 0.2813345789909363 + 100.0 * 9.056002616882324
Epoch 1100, val loss: 0.429929256439209
Epoch 1110, training loss: 905.694580078125 = 0.2794054448604584 + 100.0 * 9.05415153503418
Epoch 1110, val loss: 0.4287598133087158
Epoch 1120, training loss: 905.7861328125 = 0.27752283215522766 + 100.0 * 9.055086135864258
Epoch 1120, val loss: 0.428226500749588
Epoch 1130, training loss: 905.5936279296875 = 0.2756347954273224 + 100.0 * 9.053179740905762
Epoch 1130, val loss: 0.42823708057403564
Epoch 1140, training loss: 905.5545043945312 = 0.27380067110061646 + 100.0 * 9.052806854248047
Epoch 1140, val loss: 0.4275789260864258
Epoch 1150, training loss: 905.8068237304688 = 0.2719779312610626 + 100.0 * 9.05534839630127
Epoch 1150, val loss: 0.42756521701812744
Epoch 1160, training loss: 905.6406860351562 = 0.27008578181266785 + 100.0 * 9.053706169128418
Epoch 1160, val loss: 0.42637038230895996
Epoch 1170, training loss: 905.4689331054688 = 0.26824066042900085 + 100.0 * 9.052006721496582
Epoch 1170, val loss: 0.4260959327220917
Epoch 1180, training loss: 905.424072265625 = 0.2664557099342346 + 100.0 * 9.051575660705566
Epoch 1180, val loss: 0.4258051812648773
Epoch 1190, training loss: 905.5582885742188 = 0.2646898627281189 + 100.0 * 9.052935600280762
Epoch 1190, val loss: 0.4258837401866913
Epoch 1200, training loss: 905.4111938476562 = 0.26285549998283386 + 100.0 * 9.051483154296875
Epoch 1200, val loss: 0.4248702824115753
Epoch 1210, training loss: 905.5169067382812 = 0.2610722482204437 + 100.0 * 9.052558898925781
Epoch 1210, val loss: 0.4251551032066345
Epoch 1220, training loss: 905.4352416992188 = 0.2592538893222809 + 100.0 * 9.051759719848633
Epoch 1220, val loss: 0.4239351749420166
Epoch 1230, training loss: 905.3264770507812 = 0.2574777901172638 + 100.0 * 9.050689697265625
Epoch 1230, val loss: 0.424015074968338
Epoch 1240, training loss: 905.2598876953125 = 0.2557332217693329 + 100.0 * 9.050041198730469
Epoch 1240, val loss: 0.4232267439365387
Epoch 1250, training loss: 905.4490966796875 = 0.25403478741645813 + 100.0 * 9.051950454711914
Epoch 1250, val loss: 0.4223755896091461
Epoch 1260, training loss: 905.3228759765625 = 0.2522391676902771 + 100.0 * 9.050705909729004
Epoch 1260, val loss: 0.42374441027641296
Epoch 1270, training loss: 905.3011474609375 = 0.25051191449165344 + 100.0 * 9.050506591796875
Epoch 1270, val loss: 0.421875923871994
Epoch 1280, training loss: 905.2451782226562 = 0.24873921275138855 + 100.0 * 9.049964904785156
Epoch 1280, val loss: 0.42264339327812195
Epoch 1290, training loss: 905.2110595703125 = 0.24703249335289001 + 100.0 * 9.049640655517578
Epoch 1290, val loss: 0.4221157133579254
Epoch 1300, training loss: 905.1160278320312 = 0.2452988177537918 + 100.0 * 9.048707008361816
Epoch 1300, val loss: 0.42224058508872986
Epoch 1310, training loss: 905.0914306640625 = 0.24360549449920654 + 100.0 * 9.048478126525879
Epoch 1310, val loss: 0.4221215546131134
Epoch 1320, training loss: 905.2759399414062 = 0.24193723499774933 + 100.0 * 9.050339698791504
Epoch 1320, val loss: 0.42267724871635437
Epoch 1330, training loss: 905.1110229492188 = 0.24021019041538239 + 100.0 * 9.048707962036133
Epoch 1330, val loss: 0.42089784145355225
Epoch 1340, training loss: 905.04248046875 = 0.23849357664585114 + 100.0 * 9.048040390014648
Epoch 1340, val loss: 0.4217837154865265
Epoch 1350, training loss: 905.0775146484375 = 0.23687078058719635 + 100.0 * 9.048406600952148
Epoch 1350, val loss: 0.4209926426410675
Epoch 1360, training loss: 904.9572143554688 = 0.23515066504478455 + 100.0 * 9.047220230102539
Epoch 1360, val loss: 0.4214780330657959
Epoch 1370, training loss: 904.933349609375 = 0.23350685834884644 + 100.0 * 9.046998023986816
Epoch 1370, val loss: 0.42165887355804443
Epoch 1380, training loss: 904.9425659179688 = 0.23186084628105164 + 100.0 * 9.047106742858887
Epoch 1380, val loss: 0.42150866985321045
Epoch 1390, training loss: 905.2081298828125 = 0.23022007942199707 + 100.0 * 9.049778938293457
Epoch 1390, val loss: 0.42084914445877075
Epoch 1400, training loss: 904.8912353515625 = 0.22852419316768646 + 100.0 * 9.046627044677734
Epoch 1400, val loss: 0.4219070374965668
Epoch 1410, training loss: 904.8445434570312 = 0.22690452635288239 + 100.0 * 9.046175956726074
Epoch 1410, val loss: 0.42085880041122437
Epoch 1420, training loss: 904.81494140625 = 0.22529159486293793 + 100.0 * 9.045896530151367
Epoch 1420, val loss: 0.4215407073497772
Epoch 1430, training loss: 904.9768676757812 = 0.22373265027999878 + 100.0 * 9.047531127929688
Epoch 1430, val loss: 0.4208548069000244
Epoch 1440, training loss: 904.791259765625 = 0.22205883264541626 + 100.0 * 9.045692443847656
Epoch 1440, val loss: 0.4222923815250397
Epoch 1450, training loss: 904.838623046875 = 0.2204483598470688 + 100.0 * 9.046181678771973
Epoch 1450, val loss: 0.42214953899383545
Epoch 1460, training loss: 904.7243041992188 = 0.2188495695590973 + 100.0 * 9.04505443572998
Epoch 1460, val loss: 0.4225621223449707
Epoch 1470, training loss: 904.7510375976562 = 0.2172757089138031 + 100.0 * 9.045337677001953
Epoch 1470, val loss: 0.42284026741981506
Epoch 1480, training loss: 904.8739013671875 = 0.21568563580513 + 100.0 * 9.046582221984863
Epoch 1480, val loss: 0.4232400953769684
Epoch 1490, training loss: 904.7173461914062 = 0.21408595144748688 + 100.0 * 9.045032501220703
Epoch 1490, val loss: 0.4224465787410736
Epoch 1500, training loss: 904.6309204101562 = 0.21250851452350616 + 100.0 * 9.044183731079102
Epoch 1500, val loss: 0.42310240864753723
Epoch 1510, training loss: 904.6372680664062 = 0.21096812188625336 + 100.0 * 9.044262886047363
Epoch 1510, val loss: 0.42292335629463196
Epoch 1520, training loss: 904.7041015625 = 0.20939607918262482 + 100.0 * 9.044946670532227
Epoch 1520, val loss: 0.4236060678958893
Epoch 1530, training loss: 904.7050170898438 = 0.20785535871982574 + 100.0 * 9.044971466064453
Epoch 1530, val loss: 0.4249502420425415
Epoch 1540, training loss: 904.6985473632812 = 0.20633283257484436 + 100.0 * 9.044921875
Epoch 1540, val loss: 0.4256506562232971
Epoch 1550, training loss: 904.51904296875 = 0.20473037660121918 + 100.0 * 9.043143272399902
Epoch 1550, val loss: 0.4241630733013153
Epoch 1560, training loss: 904.4783935546875 = 0.20320411026477814 + 100.0 * 9.042752265930176
Epoch 1560, val loss: 0.42504847049713135
Epoch 1570, training loss: 904.5599975585938 = 0.20170359313488007 + 100.0 * 9.043582916259766
Epoch 1570, val loss: 0.4257374107837677
Epoch 1580, training loss: 904.5023193359375 = 0.20018720626831055 + 100.0 * 9.043021202087402
Epoch 1580, val loss: 0.4248825013637543
Epoch 1590, training loss: 904.5475463867188 = 0.19864891469478607 + 100.0 * 9.043488502502441
Epoch 1590, val loss: 0.42597270011901855
Epoch 1600, training loss: 904.3984375 = 0.19713455438613892 + 100.0 * 9.042013168334961
Epoch 1600, val loss: 0.42643895745277405
Epoch 1610, training loss: 904.401123046875 = 0.1956290304660797 + 100.0 * 9.042055130004883
Epoch 1610, val loss: 0.42728519439697266
Epoch 1620, training loss: 904.5690307617188 = 0.19413433969020844 + 100.0 * 9.04374885559082
Epoch 1620, val loss: 0.4277317523956299
Epoch 1630, training loss: 904.3908081054688 = 0.1926463544368744 + 100.0 * 9.04198169708252
Epoch 1630, val loss: 0.42850467562675476
Epoch 1640, training loss: 904.3466796875 = 0.19114342331886292 + 100.0 * 9.041555404663086
Epoch 1640, val loss: 0.42907145619392395
Epoch 1650, training loss: 904.3643188476562 = 0.18967916071414948 + 100.0 * 9.041746139526367
Epoch 1650, val loss: 0.42999204993247986
Epoch 1660, training loss: 904.3187255859375 = 0.18819130957126617 + 100.0 * 9.041305541992188
Epoch 1660, val loss: 0.4306584298610687
Epoch 1670, training loss: 904.4413452148438 = 0.18669599294662476 + 100.0 * 9.042546272277832
Epoch 1670, val loss: 0.4309071898460388
Epoch 1680, training loss: 904.343994140625 = 0.18520587682724 + 100.0 * 9.041587829589844
Epoch 1680, val loss: 0.4301304817199707
Epoch 1690, training loss: 904.294921875 = 0.18373394012451172 + 100.0 * 9.041111946105957
Epoch 1690, val loss: 0.4322890043258667
Epoch 1700, training loss: 904.2125854492188 = 0.18223977088928223 + 100.0 * 9.040303230285645
Epoch 1700, val loss: 0.4319922924041748
Epoch 1710, training loss: 904.2787475585938 = 0.18078415095806122 + 100.0 * 9.040979385375977
Epoch 1710, val loss: 0.43256959319114685
Epoch 1720, training loss: 904.2918701171875 = 0.17931176722049713 + 100.0 * 9.041125297546387
Epoch 1720, val loss: 0.4327782392501831
Epoch 1730, training loss: 904.1517333984375 = 0.17785026133060455 + 100.0 * 9.039738655090332
Epoch 1730, val loss: 0.4337046444416046
Epoch 1740, training loss: 904.1007690429688 = 0.17640604078769684 + 100.0 * 9.039243698120117
Epoch 1740, val loss: 0.43431204557418823
Epoch 1750, training loss: 904.1287231445312 = 0.1749936044216156 + 100.0 * 9.03953742980957
Epoch 1750, val loss: 0.43418624997138977
Epoch 1760, training loss: 904.3075561523438 = 0.173605814576149 + 100.0 * 9.041339874267578
Epoch 1760, val loss: 0.43461310863494873
Epoch 1770, training loss: 904.1070556640625 = 0.17210587859153748 + 100.0 * 9.039349555969238
Epoch 1770, val loss: 0.43667078018188477
Epoch 1780, training loss: 904.0313110351562 = 0.17068244516849518 + 100.0 * 9.038606643676758
Epoch 1780, val loss: 0.4369603991508484
Epoch 1790, training loss: 904.1957397460938 = 0.16934171319007874 + 100.0 * 9.040264129638672
Epoch 1790, val loss: 0.4366157650947571
Epoch 1800, training loss: 904.0036010742188 = 0.16786494851112366 + 100.0 * 9.038357734680176
Epoch 1800, val loss: 0.4391155540943146
Epoch 1810, training loss: 903.9749755859375 = 0.1664799004793167 + 100.0 * 9.038084983825684
Epoch 1810, val loss: 0.4391946792602539
Epoch 1820, training loss: 904.16650390625 = 0.1651199907064438 + 100.0 * 9.040014266967773
Epoch 1820, val loss: 0.4398241937160492
Epoch 1830, training loss: 903.98193359375 = 0.16370706260204315 + 100.0 * 9.038182258605957
Epoch 1830, val loss: 0.4414171874523163
Epoch 1840, training loss: 903.95263671875 = 0.16233520209789276 + 100.0 * 9.03790283203125
Epoch 1840, val loss: 0.44178447127342224
Epoch 1850, training loss: 903.8900146484375 = 0.16096767783164978 + 100.0 * 9.037290573120117
Epoch 1850, val loss: 0.4434284269809723
Epoch 1860, training loss: 903.876953125 = 0.1596023440361023 + 100.0 * 9.0371732711792
Epoch 1860, val loss: 0.4439072012901306
Epoch 1870, training loss: 904.2022705078125 = 0.15829908847808838 + 100.0 * 9.04043960571289
Epoch 1870, val loss: 0.44612884521484375
Epoch 1880, training loss: 904.131591796875 = 0.15687955915927887 + 100.0 * 9.03974723815918
Epoch 1880, val loss: 0.44610217213630676
Epoch 1890, training loss: 903.8467407226562 = 0.15550391376018524 + 100.0 * 9.036911964416504
Epoch 1890, val loss: 0.44613075256347656
Epoch 1900, training loss: 903.8037719726562 = 0.15414634346961975 + 100.0 * 9.03649616241455
Epoch 1900, val loss: 0.44780242443084717
Epoch 1910, training loss: 903.8263549804688 = 0.15279529988765717 + 100.0 * 9.036735534667969
Epoch 1910, val loss: 0.44837233424186707
Epoch 1920, training loss: 904.1368408203125 = 0.15153492987155914 + 100.0 * 9.0398530960083
Epoch 1920, val loss: 0.4477386474609375
Epoch 1930, training loss: 903.7579345703125 = 0.15010149776935577 + 100.0 * 9.036078453063965
Epoch 1930, val loss: 0.45086216926574707
Epoch 1940, training loss: 903.736083984375 = 0.14875055849552155 + 100.0 * 9.035873413085938
Epoch 1940, val loss: 0.4515312612056732
Epoch 1950, training loss: 903.7106323242188 = 0.14741772413253784 + 100.0 * 9.035632133483887
Epoch 1950, val loss: 0.45248058438301086
Epoch 1960, training loss: 903.7998046875 = 0.14612555503845215 + 100.0 * 9.036537170410156
Epoch 1960, val loss: 0.45463448762893677
Epoch 1970, training loss: 903.7078247070312 = 0.14476335048675537 + 100.0 * 9.035630226135254
Epoch 1970, val loss: 0.4543653130531311
Epoch 1980, training loss: 903.6527709960938 = 0.14342767000198364 + 100.0 * 9.035093307495117
Epoch 1980, val loss: 0.4557742178440094
Epoch 1990, training loss: 903.6455078125 = 0.14211906492710114 + 100.0 * 9.0350341796875
Epoch 1990, val loss: 0.4561116695404053
Epoch 2000, training loss: 903.746826171875 = 0.1408286690711975 + 100.0 * 9.036060333251953
Epoch 2000, val loss: 0.4569143056869507
Epoch 2010, training loss: 903.683349609375 = 0.13951881229877472 + 100.0 * 9.035438537597656
Epoch 2010, val loss: 0.4600522816181183
Epoch 2020, training loss: 903.6580200195312 = 0.13821139931678772 + 100.0 * 9.035198211669922
Epoch 2020, val loss: 0.45981696248054504
Epoch 2030, training loss: 903.6203002929688 = 0.13692407310009003 + 100.0 * 9.034833908081055
Epoch 2030, val loss: 0.4612392485141754
Epoch 2040, training loss: 903.5877075195312 = 0.13565927743911743 + 100.0 * 9.034520149230957
Epoch 2040, val loss: 0.46177488565444946
Epoch 2050, training loss: 903.7582397460938 = 0.13437466323375702 + 100.0 * 9.036238670349121
Epoch 2050, val loss: 0.4635423719882965
Epoch 2060, training loss: 903.6012573242188 = 0.1331057995557785 + 100.0 * 9.03468132019043
Epoch 2060, val loss: 0.4663979113101959
Epoch 2070, training loss: 903.5569458007812 = 0.1318364292383194 + 100.0 * 9.03425121307373
Epoch 2070, val loss: 0.4669319987297058
Epoch 2080, training loss: 903.6341552734375 = 0.13057346642017365 + 100.0 * 9.035036087036133
Epoch 2080, val loss: 0.46797582507133484
Epoch 2090, training loss: 903.5635986328125 = 0.1293574571609497 + 100.0 * 9.034342765808105
Epoch 2090, val loss: 0.46748197078704834
Epoch 2100, training loss: 903.4469604492188 = 0.1280328929424286 + 100.0 * 9.033188819885254
Epoch 2100, val loss: 0.4705651104450226
Epoch 2110, training loss: 903.480224609375 = 0.1268022358417511 + 100.0 * 9.033534049987793
Epoch 2110, val loss: 0.47181427478790283
Epoch 2120, training loss: 903.7623901367188 = 0.1256820410490036 + 100.0 * 9.036367416381836
Epoch 2120, val loss: 0.47127076983451843
Epoch 2130, training loss: 903.5145874023438 = 0.12436604499816895 + 100.0 * 9.033902168273926
Epoch 2130, val loss: 0.47488510608673096
Epoch 2140, training loss: 903.4036254882812 = 0.12313642352819443 + 100.0 * 9.032805442810059
Epoch 2140, val loss: 0.47638431191444397
Epoch 2150, training loss: 903.4039916992188 = 0.12193053215742111 + 100.0 * 9.032820701599121
Epoch 2150, val loss: 0.4774686396121979
Epoch 2160, training loss: 903.693359375 = 0.12076959758996964 + 100.0 * 9.035725593566895
Epoch 2160, val loss: 0.4780671298503876
Epoch 2170, training loss: 903.4805297851562 = 0.1195746585726738 + 100.0 * 9.033609390258789
Epoch 2170, val loss: 0.48030975461006165
Epoch 2180, training loss: 903.3359375 = 0.11835913360118866 + 100.0 * 9.03217601776123
Epoch 2180, val loss: 0.48192453384399414
Epoch 2190, training loss: 903.32421875 = 0.11717505007982254 + 100.0 * 9.03207015991211
Epoch 2190, val loss: 0.4832229018211365
Epoch 2200, training loss: 903.9339599609375 = 0.1160588413476944 + 100.0 * 9.038178443908691
Epoch 2200, val loss: 0.4844353199005127
Epoch 2210, training loss: 903.371826171875 = 0.1148681640625 + 100.0 * 9.032569885253906
Epoch 2210, val loss: 0.486823707818985
Epoch 2220, training loss: 903.27978515625 = 0.11368195712566376 + 100.0 * 9.031661033630371
Epoch 2220, val loss: 0.4877455532550812
Epoch 2230, training loss: 903.258056640625 = 0.11252643913030624 + 100.0 * 9.031455039978027
Epoch 2230, val loss: 0.4895072877407074
Epoch 2240, training loss: 903.224365234375 = 0.11137320846319199 + 100.0 * 9.031129837036133
Epoch 2240, val loss: 0.4910563826560974
Epoch 2250, training loss: 903.427490234375 = 0.11036849021911621 + 100.0 * 9.033171653747559
Epoch 2250, val loss: 0.49521562457084656
Epoch 2260, training loss: 903.3084106445312 = 0.1091431975364685 + 100.0 * 9.03199291229248
Epoch 2260, val loss: 0.4932708740234375
Epoch 2270, training loss: 903.2901000976562 = 0.10808786749839783 + 100.0 * 9.031820297241211
Epoch 2270, val loss: 0.49684661626815796
Epoch 2280, training loss: 903.18701171875 = 0.10695014894008636 + 100.0 * 9.030800819396973
Epoch 2280, val loss: 0.4968571364879608
Epoch 2290, training loss: 903.2651977539062 = 0.10587051510810852 + 100.0 * 9.031593322753906
Epoch 2290, val loss: 0.49800392985343933
Epoch 2300, training loss: 903.2164306640625 = 0.10479100793600082 + 100.0 * 9.031116485595703
Epoch 2300, val loss: 0.5010307431221008
Epoch 2310, training loss: 903.2904052734375 = 0.1037924587726593 + 100.0 * 9.031866073608398
Epoch 2310, val loss: 0.5032153725624084
Epoch 2320, training loss: 903.16064453125 = 0.1026536300778389 + 100.0 * 9.030579566955566
Epoch 2320, val loss: 0.5037238597869873
Epoch 2330, training loss: 903.2183227539062 = 0.10163667798042297 + 100.0 * 9.031167030334473
Epoch 2330, val loss: 0.5060181021690369
Epoch 2340, training loss: 903.182861328125 = 0.10056671500205994 + 100.0 * 9.03082275390625
Epoch 2340, val loss: 0.5074583888053894
Epoch 2350, training loss: 903.1826782226562 = 0.09952311217784882 + 100.0 * 9.030831336975098
Epoch 2350, val loss: 0.5085046887397766
Epoch 2360, training loss: 903.1253051757812 = 0.09849052876234055 + 100.0 * 9.030267715454102
Epoch 2360, val loss: 0.5089898705482483
Epoch 2370, training loss: 903.0838012695312 = 0.09745797514915466 + 100.0 * 9.029863357543945
Epoch 2370, val loss: 0.5113892555236816
Epoch 2380, training loss: 903.1609497070312 = 0.09647803753614426 + 100.0 * 9.030644416809082
Epoch 2380, val loss: 0.5139382481575012
Epoch 2390, training loss: 903.123779296875 = 0.09545545279979706 + 100.0 * 9.030282974243164
Epoch 2390, val loss: 0.5152480006217957
Epoch 2400, training loss: 903.0760498046875 = 0.09450677782297134 + 100.0 * 9.029815673828125
Epoch 2400, val loss: 0.5184313654899597
Epoch 2410, training loss: 903.1068725585938 = 0.09356590360403061 + 100.0 * 9.030133247375488
Epoch 2410, val loss: 0.5197770595550537
Epoch 2420, training loss: 903.039306640625 = 0.09250378608703613 + 100.0 * 9.029467582702637
Epoch 2420, val loss: 0.5190894603729248
Epoch 2430, training loss: 903.135498046875 = 0.09152967482805252 + 100.0 * 9.030439376831055
Epoch 2430, val loss: 0.5224356055259705
Epoch 2440, training loss: 903.0228271484375 = 0.09058614820241928 + 100.0 * 9.029322624206543
Epoch 2440, val loss: 0.5244970321655273
Epoch 2450, training loss: 902.977294921875 = 0.08960910141468048 + 100.0 * 9.028877258300781
Epoch 2450, val loss: 0.5252081751823425
Epoch 2460, training loss: 903.1305541992188 = 0.08868968486785889 + 100.0 * 9.030418395996094
Epoch 2460, val loss: 0.5260947942733765
Epoch 2470, training loss: 902.9881591796875 = 0.0877436101436615 + 100.0 * 9.029004096984863
Epoch 2470, val loss: 0.5285682678222656
Epoch 2480, training loss: 902.916748046875 = 0.08680678904056549 + 100.0 * 9.028299331665039
Epoch 2480, val loss: 0.5300111174583435
Epoch 2490, training loss: 902.92431640625 = 0.08588087558746338 + 100.0 * 9.0283842086792
Epoch 2490, val loss: 0.5322045087814331
Epoch 2500, training loss: 903.1222534179688 = 0.08500630408525467 + 100.0 * 9.030372619628906
Epoch 2500, val loss: 0.5342390537261963
Epoch 2510, training loss: 902.95166015625 = 0.08407089859247208 + 100.0 * 9.02867603302002
Epoch 2510, val loss: 0.5365158319473267
Epoch 2520, training loss: 902.92236328125 = 0.08319877088069916 + 100.0 * 9.02839183807373
Epoch 2520, val loss: 0.539415180683136
Epoch 2530, training loss: 902.990966796875 = 0.08243182301521301 + 100.0 * 9.029085159301758
Epoch 2530, val loss: 0.5426841378211975
Epoch 2540, training loss: 902.8591918945312 = 0.08140137046575546 + 100.0 * 9.027777671813965
Epoch 2540, val loss: 0.5414770841598511
Epoch 2550, training loss: 902.9835205078125 = 0.08055740594863892 + 100.0 * 9.029029846191406
Epoch 2550, val loss: 0.5425631403923035
Epoch 2560, training loss: 902.8692626953125 = 0.07967198640108109 + 100.0 * 9.0278959274292
Epoch 2560, val loss: 0.5455068349838257
Epoch 2570, training loss: 902.8097534179688 = 0.07881439477205276 + 100.0 * 9.02730941772461
Epoch 2570, val loss: 0.5460363626480103
Epoch 2580, training loss: 902.8162231445312 = 0.07797016203403473 + 100.0 * 9.027382850646973
Epoch 2580, val loss: 0.5485544204711914
Epoch 2590, training loss: 903.001708984375 = 0.0771494060754776 + 100.0 * 9.029245376586914
Epoch 2590, val loss: 0.5506354570388794
Epoch 2600, training loss: 902.8829345703125 = 0.07631786167621613 + 100.0 * 9.028066635131836
Epoch 2600, val loss: 0.5526069402694702
Epoch 2610, training loss: 902.788330078125 = 0.07550127804279327 + 100.0 * 9.027128219604492
Epoch 2610, val loss: 0.5552892088890076
Epoch 2620, training loss: 902.7420654296875 = 0.07467029243707657 + 100.0 * 9.026674270629883
Epoch 2620, val loss: 0.5566456317901611
Epoch 2630, training loss: 902.77490234375 = 0.07392711192369461 + 100.0 * 9.027009963989258
Epoch 2630, val loss: 0.5599291324615479
Epoch 2640, training loss: 902.9227294921875 = 0.07323252409696579 + 100.0 * 9.028494834899902
Epoch 2640, val loss: 0.5621510744094849
Epoch 2650, training loss: 902.798583984375 = 0.07229918241500854 + 100.0 * 9.027262687683105
Epoch 2650, val loss: 0.5614019632339478
Epoch 2660, training loss: 902.8071899414062 = 0.07164418697357178 + 100.0 * 9.027355194091797
Epoch 2660, val loss: 0.5655694603919983
Epoch 2670, training loss: 902.67236328125 = 0.070736825466156 + 100.0 * 9.026016235351562
Epoch 2670, val loss: 0.5653117299079895
Epoch 2680, training loss: 902.6878662109375 = 0.06998142600059509 + 100.0 * 9.026178359985352
Epoch 2680, val loss: 0.5677621960639954
Epoch 2690, training loss: 902.770751953125 = 0.06926761567592621 + 100.0 * 9.02701473236084
Epoch 2690, val loss: 0.5702917575836182
Epoch 2700, training loss: 902.7420654296875 = 0.0685291513800621 + 100.0 * 9.026735305786133
Epoch 2700, val loss: 0.5719286799430847
Epoch 2710, training loss: 902.6902465820312 = 0.06776601076126099 + 100.0 * 9.026225090026855
Epoch 2710, val loss: 0.5733632445335388
Epoch 2720, training loss: 902.7706298828125 = 0.06708019971847534 + 100.0 * 9.0270357131958
Epoch 2720, val loss: 0.5759581327438354
Epoch 2730, training loss: 902.658935546875 = 0.06631676107645035 + 100.0 * 9.025925636291504
Epoch 2730, val loss: 0.5766143202781677
Epoch 2740, training loss: 902.60107421875 = 0.0656004399061203 + 100.0 * 9.025354385375977
Epoch 2740, val loss: 0.5782465934753418
Epoch 2750, training loss: 902.609375 = 0.06491759419441223 + 100.0 * 9.025444030761719
Epoch 2750, val loss: 0.5821716785430908
Epoch 2760, training loss: 902.6893920898438 = 0.06420744210481644 + 100.0 * 9.026251792907715
Epoch 2760, val loss: 0.5837388038635254
Epoch 2770, training loss: 902.5884399414062 = 0.06349217891693115 + 100.0 * 9.025249481201172
Epoch 2770, val loss: 0.5842912197113037
Epoch 2780, training loss: 902.6174926757812 = 0.06281127035617828 + 100.0 * 9.02554702758789
Epoch 2780, val loss: 0.585767924785614
Epoch 2790, training loss: 902.5740966796875 = 0.06213551014661789 + 100.0 * 9.02511978149414
Epoch 2790, val loss: 0.5896292328834534
Epoch 2800, training loss: 902.7560424804688 = 0.06155015900731087 + 100.0 * 9.026945114135742
Epoch 2800, val loss: 0.5924949049949646
Epoch 2810, training loss: 902.5656127929688 = 0.06083124130964279 + 100.0 * 9.025047302246094
Epoch 2810, val loss: 0.590468168258667
Epoch 2820, training loss: 902.5077514648438 = 0.06011085957288742 + 100.0 * 9.024476051330566
Epoch 2820, val loss: 0.5945455431938171
Epoch 2830, training loss: 902.6543579101562 = 0.05950389802455902 + 100.0 * 9.025948524475098
Epoch 2830, val loss: 0.594049334526062
Epoch 2840, training loss: 902.4574584960938 = 0.05880143865942955 + 100.0 * 9.02398681640625
Epoch 2840, val loss: 0.5986372232437134
Epoch 2850, training loss: 902.4672241210938 = 0.058166202157735825 + 100.0 * 9.024090766906738
Epoch 2850, val loss: 0.5999045968055725
Epoch 2860, training loss: 902.5800170898438 = 0.05753616988658905 + 100.0 * 9.025224685668945
Epoch 2860, val loss: 0.6018553376197815
Epoch 2870, training loss: 902.5062866210938 = 0.056901779025793076 + 100.0 * 9.024494171142578
Epoch 2870, val loss: 0.6033281683921814
Epoch 2880, training loss: 902.4906005859375 = 0.05628766864538193 + 100.0 * 9.024343490600586
Epoch 2880, val loss: 0.6047026515007019
Epoch 2890, training loss: 902.4843139648438 = 0.05566846579313278 + 100.0 * 9.024286270141602
Epoch 2890, val loss: 0.607103705406189
Epoch 2900, training loss: 902.407470703125 = 0.055043745785951614 + 100.0 * 9.023524284362793
Epoch 2900, val loss: 0.6098534464836121
Epoch 2910, training loss: 902.6559448242188 = 0.05450395867228508 + 100.0 * 9.02601432800293
Epoch 2910, val loss: 0.6131318211555481
Epoch 2920, training loss: 902.4224853515625 = 0.05386021360754967 + 100.0 * 9.023686408996582
Epoch 2920, val loss: 0.6131892204284668
Epoch 2930, training loss: 902.3779907226562 = 0.053260110318660736 + 100.0 * 9.023246765136719
Epoch 2930, val loss: 0.6158204078674316
Epoch 2940, training loss: 902.3687744140625 = 0.05266966298222542 + 100.0 * 9.023160934448242
Epoch 2940, val loss: 0.6172330975532532
Epoch 2950, training loss: 902.6098022460938 = 0.0521196573972702 + 100.0 * 9.0255765914917
Epoch 2950, val loss: 0.6203962564468384
Epoch 2960, training loss: 902.3484497070312 = 0.051522716879844666 + 100.0 * 9.022969245910645
Epoch 2960, val loss: 0.6214661598205566
Epoch 2970, training loss: 902.332275390625 = 0.05096765607595444 + 100.0 * 9.022812843322754
Epoch 2970, val loss: 0.6244136095046997
Epoch 2980, training loss: 902.44384765625 = 0.05044059455394745 + 100.0 * 9.023934364318848
Epoch 2980, val loss: 0.6260026693344116
Epoch 2990, training loss: 902.4044189453125 = 0.04984475299715996 + 100.0 * 9.023545265197754
Epoch 2990, val loss: 0.6265878677368164
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8437
Overall ASR: 0.6917
Flip ASR: 0.6158/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1037.00927734375 = 1.1047862768173218 + 100.0 * 10.35904598236084
Epoch 0, val loss: 1.103559970855713
Epoch 10, training loss: 1036.6414794921875 = 1.0938892364501953 + 100.0 * 10.355476379394531
Epoch 10, val loss: 1.0923947095870972
Epoch 20, training loss: 1031.256591796875 = 1.0786439180374146 + 100.0 * 10.301779747009277
Epoch 20, val loss: 1.0772536993026733
Epoch 30, training loss: 979.6845703125 = 1.0644649267196655 + 100.0 * 9.786201477050781
Epoch 30, val loss: 1.0633307695388794
Epoch 40, training loss: 954.7173461914062 = 1.049976110458374 + 100.0 * 9.536673545837402
Epoch 40, val loss: 1.0489188432693481
Epoch 50, training loss: 945.8003540039062 = 1.032984972000122 + 100.0 * 9.447673797607422
Epoch 50, val loss: 1.0319007635116577
Epoch 60, training loss: 942.4929809570312 = 1.0133775472640991 + 100.0 * 9.414795875549316
Epoch 60, val loss: 1.0124491453170776
Epoch 70, training loss: 938.559814453125 = 0.9953587651252747 + 100.0 * 9.37564468383789
Epoch 70, val loss: 0.9948854446411133
Epoch 80, training loss: 933.4888916015625 = 0.9817665815353394 + 100.0 * 9.325071334838867
Epoch 80, val loss: 0.981554388999939
Epoch 90, training loss: 930.2980346679688 = 0.9675819277763367 + 100.0 * 9.293304443359375
Epoch 90, val loss: 0.9670645594596863
Epoch 100, training loss: 928.085693359375 = 0.9476601481437683 + 100.0 * 9.271380424499512
Epoch 100, val loss: 0.9472769498825073
Epoch 110, training loss: 924.9559326171875 = 0.9256442785263062 + 100.0 * 9.240303039550781
Epoch 110, val loss: 0.926277220249176
Epoch 120, training loss: 922.183837890625 = 0.9060124158859253 + 100.0 * 9.212778091430664
Epoch 120, val loss: 0.9078055620193481
Epoch 130, training loss: 920.4786376953125 = 0.8858442902565002 + 100.0 * 9.195927619934082
Epoch 130, val loss: 0.8882840871810913
Epoch 140, training loss: 918.6609497070312 = 0.8632463216781616 + 100.0 * 9.177977561950684
Epoch 140, val loss: 0.8668287992477417
Epoch 150, training loss: 917.86181640625 = 0.839264988899231 + 100.0 * 9.170225143432617
Epoch 150, val loss: 0.8440728187561035
Epoch 160, training loss: 916.9345703125 = 0.8127975463867188 + 100.0 * 9.16121768951416
Epoch 160, val loss: 0.8186778426170349
Epoch 170, training loss: 916.2645263671875 = 0.7852668166160583 + 100.0 * 9.154792785644531
Epoch 170, val loss: 0.7929093837738037
Epoch 180, training loss: 915.5862426757812 = 0.7587870359420776 + 100.0 * 9.148274421691895
Epoch 180, val loss: 0.7684153914451599
Epoch 190, training loss: 915.4518432617188 = 0.7338888645172119 + 100.0 * 9.14717960357666
Epoch 190, val loss: 0.745796263217926
Epoch 200, training loss: 914.6788940429688 = 0.7092633247375488 + 100.0 * 9.13969612121582
Epoch 200, val loss: 0.7231808304786682
Epoch 210, training loss: 914.0595092773438 = 0.6857593655586243 + 100.0 * 9.133737564086914
Epoch 210, val loss: 0.7021364569664001
Epoch 220, training loss: 913.638671875 = 0.6639670133590698 + 100.0 * 9.12974739074707
Epoch 220, val loss: 0.6829413175582886
Epoch 230, training loss: 913.1914672851562 = 0.6440222263336182 + 100.0 * 9.125473976135254
Epoch 230, val loss: 0.6656379699707031
Epoch 240, training loss: 912.79150390625 = 0.6256968379020691 + 100.0 * 9.121658325195312
Epoch 240, val loss: 0.6500573754310608
Epoch 250, training loss: 912.491455078125 = 0.6087433695793152 + 100.0 * 9.118826866149902
Epoch 250, val loss: 0.6359473466873169
Epoch 260, training loss: 912.1552734375 = 0.5936936140060425 + 100.0 * 9.115615844726562
Epoch 260, val loss: 0.6237335205078125
Epoch 270, training loss: 911.8079223632812 = 0.5801579356193542 + 100.0 * 9.11227798461914
Epoch 270, val loss: 0.6130780577659607
Epoch 280, training loss: 911.8646240234375 = 0.5677047371864319 + 100.0 * 9.112969398498535
Epoch 280, val loss: 0.6035168170928955
Epoch 290, training loss: 911.3778686523438 = 0.556192934513092 + 100.0 * 9.108216285705566
Epoch 290, val loss: 0.5948134064674377
Epoch 300, training loss: 911.0637817382812 = 0.5460565686225891 + 100.0 * 9.10517692565918
Epoch 300, val loss: 0.5874358415603638
Epoch 310, training loss: 910.8024291992188 = 0.5370192527770996 + 100.0 * 9.102653503417969
Epoch 310, val loss: 0.5811043381690979
Epoch 320, training loss: 911.1464233398438 = 0.5286914706230164 + 100.0 * 9.10617733001709
Epoch 320, val loss: 0.5753164887428284
Epoch 330, training loss: 910.5936889648438 = 0.5205367207527161 + 100.0 * 9.10073184967041
Epoch 330, val loss: 0.5698225498199463
Epoch 340, training loss: 910.2449951171875 = 0.5136937499046326 + 100.0 * 9.097312927246094
Epoch 340, val loss: 0.5653257369995117
Epoch 350, training loss: 910.0081787109375 = 0.5075421929359436 + 100.0 * 9.095005989074707
Epoch 350, val loss: 0.5614271759986877
Epoch 360, training loss: 909.8143310546875 = 0.50176602602005 + 100.0 * 9.093125343322754
Epoch 360, val loss: 0.5578281879425049
Epoch 370, training loss: 909.9794311523438 = 0.49628010392189026 + 100.0 * 9.094831466674805
Epoch 370, val loss: 0.5543510317802429
Epoch 380, training loss: 909.5200805664062 = 0.4907941222190857 + 100.0 * 9.090292930603027
Epoch 380, val loss: 0.550890326499939
Epoch 390, training loss: 909.3971557617188 = 0.48582518100738525 + 100.0 * 9.089113235473633
Epoch 390, val loss: 0.5477216839790344
Epoch 400, training loss: 909.2180786132812 = 0.48122453689575195 + 100.0 * 9.087368965148926
Epoch 400, val loss: 0.5448848605155945
Epoch 410, training loss: 909.2745361328125 = 0.47678160667419434 + 100.0 * 9.087977409362793
Epoch 410, val loss: 0.5421537756919861
Epoch 420, training loss: 909.0109252929688 = 0.4722350835800171 + 100.0 * 9.085387229919434
Epoch 420, val loss: 0.5391502380371094
Epoch 430, training loss: 908.8515625 = 0.46801239252090454 + 100.0 * 9.08383560180664
Epoch 430, val loss: 0.5364541411399841
Epoch 440, training loss: 908.728759765625 = 0.46404704451560974 + 100.0 * 9.082647323608398
Epoch 440, val loss: 0.5340024828910828
Epoch 450, training loss: 908.6365356445312 = 0.46018874645233154 + 100.0 * 9.08176326751709
Epoch 450, val loss: 0.5316208004951477
Epoch 460, training loss: 908.5443725585938 = 0.45625659823417664 + 100.0 * 9.080881118774414
Epoch 460, val loss: 0.529074490070343
Epoch 470, training loss: 908.452880859375 = 0.45239007472991943 + 100.0 * 9.080004692077637
Epoch 470, val loss: 0.526557445526123
Epoch 480, training loss: 908.3671264648438 = 0.4488202929496765 + 100.0 * 9.079183578491211
Epoch 480, val loss: 0.5243210196495056
Epoch 490, training loss: 908.2646484375 = 0.44535622000694275 + 100.0 * 9.078192710876465
Epoch 490, val loss: 0.5221041440963745
Epoch 500, training loss: 908.5712280273438 = 0.4418782889842987 + 100.0 * 9.081293106079102
Epoch 500, val loss: 0.5197448134422302
Epoch 510, training loss: 908.1524047851562 = 0.43829089403152466 + 100.0 * 9.077140808105469
Epoch 510, val loss: 0.5174709558486938
Epoch 520, training loss: 908.0089721679688 = 0.4349801540374756 + 100.0 * 9.075739860534668
Epoch 520, val loss: 0.5154352784156799
Epoch 530, training loss: 907.9166870117188 = 0.4317755103111267 + 100.0 * 9.074849128723145
Epoch 530, val loss: 0.5134711861610413
Epoch 540, training loss: 908.1759643554688 = 0.42858991026878357 + 100.0 * 9.077473640441895
Epoch 540, val loss: 0.5113538503646851
Epoch 550, training loss: 907.8173217773438 = 0.42522963881492615 + 100.0 * 9.073921203613281
Epoch 550, val loss: 0.5093117952346802
Epoch 560, training loss: 907.73681640625 = 0.4221058189868927 + 100.0 * 9.07314682006836
Epoch 560, val loss: 0.5073660016059875
Epoch 570, training loss: 907.628662109375 = 0.4190788269042969 + 100.0 * 9.07209587097168
Epoch 570, val loss: 0.5055018067359924
Epoch 580, training loss: 907.6344604492188 = 0.4160623252391815 + 100.0 * 9.072183609008789
Epoch 580, val loss: 0.5035394430160522
Epoch 590, training loss: 907.6102905273438 = 0.41287973523139954 + 100.0 * 9.07197380065918
Epoch 590, val loss: 0.5014948844909668
Epoch 600, training loss: 907.423583984375 = 0.4098120927810669 + 100.0 * 9.070137977600098
Epoch 600, val loss: 0.4994758367538452
Epoch 610, training loss: 907.353759765625 = 0.40689384937286377 + 100.0 * 9.06946849822998
Epoch 610, val loss: 0.4976951479911804
Epoch 620, training loss: 907.3309326171875 = 0.40400296449661255 + 100.0 * 9.069269180297852
Epoch 620, val loss: 0.4959274232387543
Epoch 630, training loss: 907.2863159179688 = 0.40097329020500183 + 100.0 * 9.068853378295898
Epoch 630, val loss: 0.49405571818351746
Epoch 640, training loss: 907.1915283203125 = 0.3979783356189728 + 100.0 * 9.067935943603516
Epoch 640, val loss: 0.49213752150535583
Epoch 650, training loss: 907.1210327148438 = 0.3951481282711029 + 100.0 * 9.067258834838867
Epoch 650, val loss: 0.4903668761253357
Epoch 660, training loss: 907.0660400390625 = 0.39235126972198486 + 100.0 * 9.066737174987793
Epoch 660, val loss: 0.48857948184013367
Epoch 670, training loss: 907.091064453125 = 0.3895595073699951 + 100.0 * 9.067014694213867
Epoch 670, val loss: 0.4868154525756836
Epoch 680, training loss: 906.97607421875 = 0.38668033480644226 + 100.0 * 9.06589412689209
Epoch 680, val loss: 0.4850614368915558
Epoch 690, training loss: 907.0306396484375 = 0.3838181793689728 + 100.0 * 9.066468238830566
Epoch 690, val loss: 0.48321112990379333
Epoch 700, training loss: 906.8565063476562 = 0.38104021549224854 + 100.0 * 9.064754486083984
Epoch 700, val loss: 0.48150378465652466
Epoch 710, training loss: 906.8065185546875 = 0.3783588707447052 + 100.0 * 9.064281463623047
Epoch 710, val loss: 0.47983625531196594
Epoch 720, training loss: 906.74560546875 = 0.3757137060165405 + 100.0 * 9.063698768615723
Epoch 720, val loss: 0.4782678186893463
Epoch 730, training loss: 906.6964111328125 = 0.37306976318359375 + 100.0 * 9.063233375549316
Epoch 730, val loss: 0.47670724987983704
Epoch 740, training loss: 907.0386352539062 = 0.37040963768959045 + 100.0 * 9.066681861877441
Epoch 740, val loss: 0.4752357304096222
Epoch 750, training loss: 906.6492919921875 = 0.36757633090019226 + 100.0 * 9.062817573547363
Epoch 750, val loss: 0.47322648763656616
Epoch 760, training loss: 906.57275390625 = 0.3649459183216095 + 100.0 * 9.062078475952148
Epoch 760, val loss: 0.47170695662498474
Epoch 770, training loss: 906.5166625976562 = 0.3624063730239868 + 100.0 * 9.061542510986328
Epoch 770, val loss: 0.4702368378639221
Epoch 780, training loss: 906.4960327148438 = 0.35990193486213684 + 100.0 * 9.061361312866211
Epoch 780, val loss: 0.4687153100967407
Epoch 790, training loss: 906.5364379882812 = 0.35730820894241333 + 100.0 * 9.06179141998291
Epoch 790, val loss: 0.4673309326171875
Epoch 800, training loss: 906.4480590820312 = 0.3547377288341522 + 100.0 * 9.060933113098145
Epoch 800, val loss: 0.46566715836524963
Epoch 810, training loss: 906.3274536132812 = 0.3522832989692688 + 100.0 * 9.059751510620117
Epoch 810, val loss: 0.4644674062728882
Epoch 820, training loss: 906.3248901367188 = 0.34987765550613403 + 100.0 * 9.0597505569458
Epoch 820, val loss: 0.463219553232193
Epoch 830, training loss: 906.4462280273438 = 0.3474281430244446 + 100.0 * 9.060988426208496
Epoch 830, val loss: 0.461836576461792
Epoch 840, training loss: 906.2482299804688 = 0.3449815511703491 + 100.0 * 9.059032440185547
Epoch 840, val loss: 0.4604552984237671
Epoch 850, training loss: 906.1950073242188 = 0.34262004494667053 + 100.0 * 9.058524131774902
Epoch 850, val loss: 0.4592841863632202
Epoch 860, training loss: 906.2678833007812 = 0.34022629261016846 + 100.0 * 9.059276580810547
Epoch 860, val loss: 0.4580085873603821
Epoch 870, training loss: 906.10693359375 = 0.3378606140613556 + 100.0 * 9.057690620422363
Epoch 870, val loss: 0.4567730724811554
Epoch 880, training loss: 906.0603637695312 = 0.3355715870857239 + 100.0 * 9.05724811553955
Epoch 880, val loss: 0.4555276334285736
Epoch 890, training loss: 906.0053100585938 = 0.3333241939544678 + 100.0 * 9.056719779968262
Epoch 890, val loss: 0.4544973075389862
Epoch 900, training loss: 906.1441650390625 = 0.3310984671115875 + 100.0 * 9.058130264282227
Epoch 900, val loss: 0.453127384185791
Epoch 910, training loss: 906.0908203125 = 0.32876160740852356 + 100.0 * 9.057621002197266
Epoch 910, val loss: 0.4523972272872925
Epoch 920, training loss: 905.90380859375 = 0.3265201449394226 + 100.0 * 9.05577278137207
Epoch 920, val loss: 0.45135965943336487
Epoch 930, training loss: 905.89599609375 = 0.32435014843940735 + 100.0 * 9.055716514587402
Epoch 930, val loss: 0.4501439034938812
Epoch 940, training loss: 905.9334106445312 = 0.32219117879867554 + 100.0 * 9.056112289428711
Epoch 940, val loss: 0.4491969048976898
Epoch 950, training loss: 905.8121948242188 = 0.32002708315849304 + 100.0 * 9.054922103881836
Epoch 950, val loss: 0.4485124349594116
Epoch 960, training loss: 905.8361206054688 = 0.3178897500038147 + 100.0 * 9.055182456970215
Epoch 960, val loss: 0.4474868178367615
Epoch 970, training loss: 905.7164916992188 = 0.31573426723480225 + 100.0 * 9.054007530212402
Epoch 970, val loss: 0.4467288851737976
Epoch 980, training loss: 905.713134765625 = 0.3136289715766907 + 100.0 * 9.053995132446289
Epoch 980, val loss: 0.4456748366355896
Epoch 990, training loss: 905.8294067382812 = 0.3115256726741791 + 100.0 * 9.05517864227295
Epoch 990, val loss: 0.44490671157836914
Epoch 1000, training loss: 905.7015380859375 = 0.3093987703323364 + 100.0 * 9.053921699523926
Epoch 1000, val loss: 0.4440758228302002
Epoch 1010, training loss: 905.5947875976562 = 0.3073405623435974 + 100.0 * 9.052874565124512
Epoch 1010, val loss: 0.4434618055820465
Epoch 1020, training loss: 905.5401000976562 = 0.3053225874900818 + 100.0 * 9.052348136901855
Epoch 1020, val loss: 0.4425848424434662
Epoch 1030, training loss: 905.5169067382812 = 0.303321897983551 + 100.0 * 9.052135467529297
Epoch 1030, val loss: 0.44203469157218933
Epoch 1040, training loss: 905.93896484375 = 0.30131176114082336 + 100.0 * 9.056376457214355
Epoch 1040, val loss: 0.4417361617088318
Epoch 1050, training loss: 905.4556884765625 = 0.2991940677165985 + 100.0 * 9.051565170288086
Epoch 1050, val loss: 0.44050654768943787
Epoch 1060, training loss: 905.4314575195312 = 0.29720374941825867 + 100.0 * 9.051342964172363
Epoch 1060, val loss: 0.43976888060569763
Epoch 1070, training loss: 905.38671875 = 0.29524803161621094 + 100.0 * 9.050914764404297
Epoch 1070, val loss: 0.43912193179130554
Epoch 1080, training loss: 905.3564453125 = 0.29330167174339294 + 100.0 * 9.050631523132324
Epoch 1080, val loss: 0.43871843814849854
Epoch 1090, training loss: 905.7356567382812 = 0.29132676124572754 + 100.0 * 9.054443359375
Epoch 1090, val loss: 0.4379270076751709
Epoch 1100, training loss: 905.2871704101562 = 0.28927597403526306 + 100.0 * 9.049979209899902
Epoch 1100, val loss: 0.4374755024909973
Epoch 1110, training loss: 905.2868041992188 = 0.28734108805656433 + 100.0 * 9.049994468688965
Epoch 1110, val loss: 0.43716081976890564
Epoch 1120, training loss: 905.2276611328125 = 0.2854368984699249 + 100.0 * 9.049422264099121
Epoch 1120, val loss: 0.4364747405052185
Epoch 1130, training loss: 905.19873046875 = 0.283543199300766 + 100.0 * 9.049151420593262
Epoch 1130, val loss: 0.4359516501426697
Epoch 1140, training loss: 905.42919921875 = 0.2816370725631714 + 100.0 * 9.051475524902344
Epoch 1140, val loss: 0.4356386661529541
Epoch 1150, training loss: 905.3448486328125 = 0.279680460691452 + 100.0 * 9.050651550292969
Epoch 1150, val loss: 0.4355635344982147
Epoch 1160, training loss: 905.1384887695312 = 0.27774685621261597 + 100.0 * 9.048606872558594
Epoch 1160, val loss: 0.43486708402633667
Epoch 1170, training loss: 905.0827026367188 = 0.2758747637271881 + 100.0 * 9.048068046569824
Epoch 1170, val loss: 0.4345424473285675
Epoch 1180, training loss: 905.0447998046875 = 0.27402788400650024 + 100.0 * 9.047707557678223
Epoch 1180, val loss: 0.43418213725090027
Epoch 1190, training loss: 905.2138061523438 = 0.2721816599369049 + 100.0 * 9.049416542053223
Epoch 1190, val loss: 0.4340365529060364
Epoch 1200, training loss: 905.04638671875 = 0.27026769518852234 + 100.0 * 9.047760963439941
Epoch 1200, val loss: 0.43383708596229553
Epoch 1210, training loss: 905.0491943359375 = 0.26840561628341675 + 100.0 * 9.047807693481445
Epoch 1210, val loss: 0.43382835388183594
Epoch 1220, training loss: 904.9849243164062 = 0.2665363848209381 + 100.0 * 9.047183990478516
Epoch 1220, val loss: 0.4333958923816681
Epoch 1230, training loss: 904.9203491210938 = 0.2646903395652771 + 100.0 * 9.04655647277832
Epoch 1230, val loss: 0.4329804480075836
Epoch 1240, training loss: 904.8812255859375 = 0.2628611922264099 + 100.0 * 9.046183586120605
Epoch 1240, val loss: 0.4329955577850342
Epoch 1250, training loss: 904.9392700195312 = 0.2610328495502472 + 100.0 * 9.046782493591309
Epoch 1250, val loss: 0.43270742893218994
Epoch 1260, training loss: 904.8593139648438 = 0.25913432240486145 + 100.0 * 9.046001434326172
Epoch 1260, val loss: 0.4328080415725708
Epoch 1270, training loss: 904.8797607421875 = 0.25729528069496155 + 100.0 * 9.046224594116211
Epoch 1270, val loss: 0.4324468970298767
Epoch 1280, training loss: 904.7763061523438 = 0.25546714663505554 + 100.0 * 9.045207977294922
Epoch 1280, val loss: 0.4326076805591583
Epoch 1290, training loss: 905.1300659179688 = 0.25369033217430115 + 100.0 * 9.0487642288208
Epoch 1290, val loss: 0.4332975745201111
Epoch 1300, training loss: 904.8129272460938 = 0.25179213285446167 + 100.0 * 9.045611381530762
Epoch 1300, val loss: 0.4322243928909302
Epoch 1310, training loss: 904.6714477539062 = 0.24997277557849884 + 100.0 * 9.044214248657227
Epoch 1310, val loss: 0.4322068393230438
Epoch 1320, training loss: 904.650634765625 = 0.24818585813045502 + 100.0 * 9.044024467468262
Epoch 1320, val loss: 0.43240052461624146
Epoch 1330, training loss: 904.7233276367188 = 0.24639099836349487 + 100.0 * 9.044769287109375
Epoch 1330, val loss: 0.4322919547557831
Epoch 1340, training loss: 904.615478515625 = 0.244533970952034 + 100.0 * 9.043709754943848
Epoch 1340, val loss: 0.4323224425315857
Epoch 1350, training loss: 904.6390991210938 = 0.24273569881916046 + 100.0 * 9.043963432312012
Epoch 1350, val loss: 0.4323403537273407
Epoch 1360, training loss: 904.5595703125 = 0.24093757569789886 + 100.0 * 9.04318618774414
Epoch 1360, val loss: 0.4324582815170288
Epoch 1370, training loss: 904.8091430664062 = 0.23915784060955048 + 100.0 * 9.045700073242188
Epoch 1370, val loss: 0.43295156955718994
Epoch 1380, training loss: 904.6040649414062 = 0.23733079433441162 + 100.0 * 9.043667793273926
Epoch 1380, val loss: 0.4321892261505127
Epoch 1390, training loss: 904.501708984375 = 0.2355329543352127 + 100.0 * 9.042661666870117
Epoch 1390, val loss: 0.4327445328235626
Epoch 1400, training loss: 904.6611938476562 = 0.23375944793224335 + 100.0 * 9.04427433013916
Epoch 1400, val loss: 0.43304091691970825
Epoch 1410, training loss: 904.5269775390625 = 0.23196005821228027 + 100.0 * 9.042950630187988
Epoch 1410, val loss: 0.4323006868362427
Epoch 1420, training loss: 904.43701171875 = 0.23016776144504547 + 100.0 * 9.042068481445312
Epoch 1420, val loss: 0.4330463409423828
Epoch 1430, training loss: 904.3884887695312 = 0.22839830815792084 + 100.0 * 9.041601181030273
Epoch 1430, val loss: 0.4330161213874817
Epoch 1440, training loss: 904.4570922851562 = 0.22663389146327972 + 100.0 * 9.042304992675781
Epoch 1440, val loss: 0.4332245886325836
Epoch 1450, training loss: 904.3909912109375 = 0.22485528886318207 + 100.0 * 9.041661262512207
Epoch 1450, val loss: 0.4335378110408783
Epoch 1460, training loss: 904.3855590820312 = 0.22306229174137115 + 100.0 * 9.041625022888184
Epoch 1460, val loss: 0.43374788761138916
Epoch 1470, training loss: 904.3216552734375 = 0.22131448984146118 + 100.0 * 9.041003227233887
Epoch 1470, val loss: 0.4341124892234802
Epoch 1480, training loss: 904.2711181640625 = 0.21954965591430664 + 100.0 * 9.040515899658203
Epoch 1480, val loss: 0.4344194233417511
Epoch 1490, training loss: 904.360595703125 = 0.21782322227954865 + 100.0 * 9.041427612304688
Epoch 1490, val loss: 0.43511533737182617
Epoch 1500, training loss: 904.2184448242188 = 0.21603898704051971 + 100.0 * 9.040023803710938
Epoch 1500, val loss: 0.4348934590816498
Epoch 1510, training loss: 904.2427978515625 = 0.21430066227912903 + 100.0 * 9.040285110473633
Epoch 1510, val loss: 0.4356284737586975
Epoch 1520, training loss: 904.2653198242188 = 0.2125752866268158 + 100.0 * 9.04052734375
Epoch 1520, val loss: 0.4360402524471283
Epoch 1530, training loss: 904.2667846679688 = 0.21082863211631775 + 100.0 * 9.040559768676758
Epoch 1530, val loss: 0.4361659288406372
Epoch 1540, training loss: 904.1503295898438 = 0.20910677313804626 + 100.0 * 9.039412498474121
Epoch 1540, val loss: 0.4361680746078491
Epoch 1550, training loss: 904.1082153320312 = 0.2074020653963089 + 100.0 * 9.039008140563965
Epoch 1550, val loss: 0.43717896938323975
Epoch 1560, training loss: 904.2490234375 = 0.20571938157081604 + 100.0 * 9.040432929992676
Epoch 1560, val loss: 0.43707171082496643
Epoch 1570, training loss: 904.1283569335938 = 0.20399482548236847 + 100.0 * 9.039243698120117
Epoch 1570, val loss: 0.4378562271595001
Epoch 1580, training loss: 904.0668334960938 = 0.2022852599620819 + 100.0 * 9.03864574432373
Epoch 1580, val loss: 0.43791282176971436
Epoch 1590, training loss: 904.0223388671875 = 0.2005884051322937 + 100.0 * 9.038217544555664
Epoch 1590, val loss: 0.43868064880371094
Epoch 1600, training loss: 904.2337036132812 = 0.19891765713691711 + 100.0 * 9.040348052978516
Epoch 1600, val loss: 0.4387567937374115
Epoch 1610, training loss: 904.1007080078125 = 0.1972133070230484 + 100.0 * 9.039034843444824
Epoch 1610, val loss: 0.43991217017173767
Epoch 1620, training loss: 904.02880859375 = 0.19550350308418274 + 100.0 * 9.03833293914795
Epoch 1620, val loss: 0.4403795599937439
Epoch 1630, training loss: 904.017578125 = 0.19381919503211975 + 100.0 * 9.038237571716309
Epoch 1630, val loss: 0.4408571422100067
Epoch 1640, training loss: 903.9276123046875 = 0.19214129447937012 + 100.0 * 9.037354469299316
Epoch 1640, val loss: 0.4412752389907837
Epoch 1650, training loss: 903.9244995117188 = 0.19046658277511597 + 100.0 * 9.03734016418457
Epoch 1650, val loss: 0.44186776876449585
Epoch 1660, training loss: 904.055908203125 = 0.18883337080478668 + 100.0 * 9.038670539855957
Epoch 1660, val loss: 0.4418678879737854
Epoch 1670, training loss: 904.0059204101562 = 0.18715202808380127 + 100.0 * 9.038187980651855
Epoch 1670, val loss: 0.4435518980026245
Epoch 1680, training loss: 903.831787109375 = 0.18550853431224823 + 100.0 * 9.036462783813477
Epoch 1680, val loss: 0.44407737255096436
Epoch 1690, training loss: 903.7817993164062 = 0.18386904895305634 + 100.0 * 9.035979270935059
Epoch 1690, val loss: 0.44484207034111023
Epoch 1700, training loss: 903.7561645507812 = 0.1822386533021927 + 100.0 * 9.035738945007324
Epoch 1700, val loss: 0.44554662704467773
Epoch 1710, training loss: 903.8220825195312 = 0.18061546981334686 + 100.0 * 9.036415100097656
Epoch 1710, val loss: 0.4461917281150818
Epoch 1720, training loss: 903.848388671875 = 0.17896772921085358 + 100.0 * 9.036694526672363
Epoch 1720, val loss: 0.4463447630405426
Epoch 1730, training loss: 903.8245849609375 = 0.1773802787065506 + 100.0 * 9.03647232055664
Epoch 1730, val loss: 0.4479712247848511
Epoch 1740, training loss: 903.7015380859375 = 0.17573827505111694 + 100.0 * 9.035258293151855
Epoch 1740, val loss: 0.4483896493911743
Epoch 1750, training loss: 903.6565551757812 = 0.17413543164730072 + 100.0 * 9.03482437133789
Epoch 1750, val loss: 0.4494570195674896
Epoch 1760, training loss: 903.6978149414062 = 0.17255596816539764 + 100.0 * 9.035252571105957
Epoch 1760, val loss: 0.45046624541282654
Epoch 1770, training loss: 903.7325439453125 = 0.17096780240535736 + 100.0 * 9.035615921020508
Epoch 1770, val loss: 0.4511183202266693
Epoch 1780, training loss: 903.6165161132812 = 0.16938145458698273 + 100.0 * 9.03447151184082
Epoch 1780, val loss: 0.45235317945480347
Epoch 1790, training loss: 903.61474609375 = 0.16779674589633942 + 100.0 * 9.034469604492188
Epoch 1790, val loss: 0.4526808559894562
Epoch 1800, training loss: 903.710693359375 = 0.16626471281051636 + 100.0 * 9.035444259643555
Epoch 1800, val loss: 0.4543304741382599
Epoch 1810, training loss: 903.5545043945312 = 0.1646931916475296 + 100.0 * 9.03389835357666
Epoch 1810, val loss: 0.4546740651130676
Epoch 1820, training loss: 903.5458984375 = 0.16313497722148895 + 100.0 * 9.033827781677246
Epoch 1820, val loss: 0.4556998312473297
Epoch 1830, training loss: 903.5303955078125 = 0.1616060733795166 + 100.0 * 9.033687591552734
Epoch 1830, val loss: 0.45645105838775635
Epoch 1840, training loss: 903.691162109375 = 0.160128653049469 + 100.0 * 9.035309791564941
Epoch 1840, val loss: 0.45864126086235046
Epoch 1850, training loss: 903.75927734375 = 0.15858474373817444 + 100.0 * 9.036006927490234
Epoch 1850, val loss: 0.45765018463134766
Epoch 1860, training loss: 903.5638427734375 = 0.15707513689994812 + 100.0 * 9.034067153930664
Epoch 1860, val loss: 0.4595622420310974
Epoch 1870, training loss: 903.44287109375 = 0.15555422008037567 + 100.0 * 9.032873153686523
Epoch 1870, val loss: 0.4603404700756073
Epoch 1880, training loss: 903.4088745117188 = 0.15406356751918793 + 100.0 * 9.032547950744629
Epoch 1880, val loss: 0.4610570967197418
Epoch 1890, training loss: 903.4000244140625 = 0.15258173644542694 + 100.0 * 9.032474517822266
Epoch 1890, val loss: 0.46239039301872253
Epoch 1900, training loss: 903.8146362304688 = 0.1511603146791458 + 100.0 * 9.03663444519043
Epoch 1900, val loss: 0.4628172516822815
Epoch 1910, training loss: 903.5247802734375 = 0.14968381822109222 + 100.0 * 9.033751487731934
Epoch 1910, val loss: 0.46443212032318115
Epoch 1920, training loss: 903.3480834960938 = 0.1481752246618271 + 100.0 * 9.031998634338379
Epoch 1920, val loss: 0.4658340811729431
Epoch 1930, training loss: 903.3982543945312 = 0.14672023057937622 + 100.0 * 9.032515525817871
Epoch 1930, val loss: 0.4661504030227661
Epoch 1940, training loss: 903.4596557617188 = 0.1452660858631134 + 100.0 * 9.033143997192383
Epoch 1940, val loss: 0.4673302173614502
Epoch 1950, training loss: 903.3030395507812 = 0.1438187062740326 + 100.0 * 9.03159236907959
Epoch 1950, val loss: 0.46890103816986084
Epoch 1960, training loss: 903.2766723632812 = 0.14237752556800842 + 100.0 * 9.031342506408691
Epoch 1960, val loss: 0.47058776021003723
Epoch 1970, training loss: 903.3738403320312 = 0.1409710943698883 + 100.0 * 9.032328605651855
Epoch 1970, val loss: 0.4711565673351288
Epoch 1980, training loss: 903.2877197265625 = 0.13953642547130585 + 100.0 * 9.031481742858887
Epoch 1980, val loss: 0.4719180166721344
Epoch 1990, training loss: 903.2299194335938 = 0.13811112940311432 + 100.0 * 9.03091812133789
Epoch 1990, val loss: 0.4735877811908722
Epoch 2000, training loss: 903.231201171875 = 0.1367182582616806 + 100.0 * 9.03094482421875
Epoch 2000, val loss: 0.47483256459236145
Epoch 2010, training loss: 903.4478759765625 = 0.13534042239189148 + 100.0 * 9.033125877380371
Epoch 2010, val loss: 0.4758855402469635
Epoch 2020, training loss: 903.2591552734375 = 0.13394832611083984 + 100.0 * 9.031251907348633
Epoch 2020, val loss: 0.4779117703437805
Epoch 2030, training loss: 903.2100830078125 = 0.13255269825458527 + 100.0 * 9.03077507019043
Epoch 2030, val loss: 0.47919315099716187
Epoch 2040, training loss: 903.3055419921875 = 0.13120734691619873 + 100.0 * 9.031743049621582
Epoch 2040, val loss: 0.4813448190689087
Epoch 2050, training loss: 903.2182006835938 = 0.12980803847312927 + 100.0 * 9.0308837890625
Epoch 2050, val loss: 0.4823418855667114
Epoch 2060, training loss: 903.1441040039062 = 0.12844142317771912 + 100.0 * 9.030157089233398
Epoch 2060, val loss: 0.48373472690582275
Epoch 2070, training loss: 903.1316528320312 = 0.12706489861011505 + 100.0 * 9.030045509338379
Epoch 2070, val loss: 0.48418763279914856
Epoch 2080, training loss: 903.2252807617188 = 0.12574002146720886 + 100.0 * 9.03099536895752
Epoch 2080, val loss: 0.4865678548812866
Epoch 2090, training loss: 903.1079711914062 = 0.1243891492486 + 100.0 * 9.02983570098877
Epoch 2090, val loss: 0.4873841106891632
Epoch 2100, training loss: 903.1239624023438 = 0.12305741012096405 + 100.0 * 9.030009269714355
Epoch 2100, val loss: 0.48893246054649353
Epoch 2110, training loss: 903.0914306640625 = 0.12173165380954742 + 100.0 * 9.02969741821289
Epoch 2110, val loss: 0.489572137594223
Epoch 2120, training loss: 903.1832885742188 = 0.12047522515058517 + 100.0 * 9.030628204345703
Epoch 2120, val loss: 0.49013176560401917
Epoch 2130, training loss: 903.0983276367188 = 0.11916239559650421 + 100.0 * 9.029791831970215
Epoch 2130, val loss: 0.49400702118873596
Epoch 2140, training loss: 903.0560302734375 = 0.11787022650241852 + 100.0 * 9.02938175201416
Epoch 2140, val loss: 0.493649959564209
Epoch 2150, training loss: 903.1122436523438 = 0.11657042056322098 + 100.0 * 9.029956817626953
Epoch 2150, val loss: 0.4965732991695404
Epoch 2160, training loss: 903.0313720703125 = 0.11530141532421112 + 100.0 * 9.029160499572754
Epoch 2160, val loss: 0.49786868691444397
Epoch 2170, training loss: 903.0148315429688 = 0.11403615027666092 + 100.0 * 9.029007911682129
Epoch 2170, val loss: 0.49920517206192017
Epoch 2180, training loss: 902.9857177734375 = 0.11278316378593445 + 100.0 * 9.028729438781738
Epoch 2180, val loss: 0.5011497139930725
Epoch 2190, training loss: 903.0215454101562 = 0.1115598976612091 + 100.0 * 9.029099464416504
Epoch 2190, val loss: 0.503242015838623
Epoch 2200, training loss: 902.9581909179688 = 0.11032301932573318 + 100.0 * 9.028478622436523
Epoch 2200, val loss: 0.5036855936050415
Epoch 2210, training loss: 902.9256591796875 = 0.10907865315675735 + 100.0 * 9.028165817260742
Epoch 2210, val loss: 0.5059453845024109
Epoch 2220, training loss: 902.8927612304688 = 0.10786392539739609 + 100.0 * 9.027849197387695
Epoch 2220, val loss: 0.5075689554214478
Epoch 2230, training loss: 903.0040283203125 = 0.10667911171913147 + 100.0 * 9.028973579406738
Epoch 2230, val loss: 0.5083772540092468
Epoch 2240, training loss: 902.8588256835938 = 0.10545364022254944 + 100.0 * 9.027533531188965
Epoch 2240, val loss: 0.5114623308181763
Epoch 2250, training loss: 902.9590454101562 = 0.10427152365446091 + 100.0 * 9.028548240661621
Epoch 2250, val loss: 0.5128219127655029
Epoch 2260, training loss: 902.8009643554688 = 0.10308229178190231 + 100.0 * 9.026978492736816
Epoch 2260, val loss: 0.5144047141075134
Epoch 2270, training loss: 902.7930297851562 = 0.10191336274147034 + 100.0 * 9.026910781860352
Epoch 2270, val loss: 0.5159950256347656
Epoch 2280, training loss: 902.820068359375 = 0.10075368732213974 + 100.0 * 9.027193069458008
Epoch 2280, val loss: 0.5179948210716248
Epoch 2290, training loss: 902.8517456054688 = 0.09960612654685974 + 100.0 * 9.027521133422852
Epoch 2290, val loss: 0.5189731121063232
Epoch 2300, training loss: 902.9683837890625 = 0.09848390519618988 + 100.0 * 9.028698921203613
Epoch 2300, val loss: 0.520656168460846
Epoch 2310, training loss: 902.7869262695312 = 0.09733792394399643 + 100.0 * 9.026895523071289
Epoch 2310, val loss: 0.5232869386672974
Epoch 2320, training loss: 902.803955078125 = 0.09621649235486984 + 100.0 * 9.027077674865723
Epoch 2320, val loss: 0.5256277918815613
Epoch 2330, training loss: 902.804931640625 = 0.09512404352426529 + 100.0 * 9.027097702026367
Epoch 2330, val loss: 0.5277026891708374
Epoch 2340, training loss: 902.766845703125 = 0.09400630742311478 + 100.0 * 9.026728630065918
Epoch 2340, val loss: 0.5286701321601868
Epoch 2350, training loss: 902.6802368164062 = 0.09289742261171341 + 100.0 * 9.025873184204102
Epoch 2350, val loss: 0.5295664668083191
Epoch 2360, training loss: 902.9887084960938 = 0.09187973290681839 + 100.0 * 9.028968811035156
Epoch 2360, val loss: 0.5314053893089294
Epoch 2370, training loss: 902.7324829101562 = 0.09080665558576584 + 100.0 * 9.026416778564453
Epoch 2370, val loss: 0.5348432660102844
Epoch 2380, training loss: 902.64794921875 = 0.08971038460731506 + 100.0 * 9.025582313537598
Epoch 2380, val loss: 0.5352380275726318
Epoch 2390, training loss: 902.6290893554688 = 0.08865358680486679 + 100.0 * 9.02540397644043
Epoch 2390, val loss: 0.5376074314117432
Epoch 2400, training loss: 902.9887084960938 = 0.08768515288829803 + 100.0 * 9.029009819030762
Epoch 2400, val loss: 0.5400881171226501
Epoch 2410, training loss: 902.7265625 = 0.08665603399276733 + 100.0 * 9.026398658752441
Epoch 2410, val loss: 0.5414921045303345
Epoch 2420, training loss: 902.6296997070312 = 0.0855988934636116 + 100.0 * 9.02544116973877
Epoch 2420, val loss: 0.5437533259391785
Epoch 2430, training loss: 902.6516723632812 = 0.08459481596946716 + 100.0 * 9.025671005249023
Epoch 2430, val loss: 0.5453775525093079
Epoch 2440, training loss: 902.614013671875 = 0.08359465003013611 + 100.0 * 9.025303840637207
Epoch 2440, val loss: 0.5459141135215759
Epoch 2450, training loss: 902.6864624023438 = 0.08265909552574158 + 100.0 * 9.02603816986084
Epoch 2450, val loss: 0.5474602580070496
Epoch 2460, training loss: 902.54052734375 = 0.08164285868406296 + 100.0 * 9.024588584899902
Epoch 2460, val loss: 0.5519638657569885
Epoch 2470, training loss: 902.529541015625 = 0.08067720383405685 + 100.0 * 9.02448844909668
Epoch 2470, val loss: 0.5535792708396912
Epoch 2480, training loss: 902.8912963867188 = 0.0797666683793068 + 100.0 * 9.028115272521973
Epoch 2480, val loss: 0.5560888051986694
Epoch 2490, training loss: 902.612060546875 = 0.07879989594221115 + 100.0 * 9.0253324508667
Epoch 2490, val loss: 0.5569479465484619
Epoch 2500, training loss: 902.4962768554688 = 0.07785133272409439 + 100.0 * 9.024184226989746
Epoch 2500, val loss: 0.5597243309020996
Epoch 2510, training loss: 902.5454711914062 = 0.07692162692546844 + 100.0 * 9.024685859680176
Epoch 2510, val loss: 0.5615517497062683
Epoch 2520, training loss: 902.4935302734375 = 0.07599424570798874 + 100.0 * 9.024175643920898
Epoch 2520, val loss: 0.5633659958839417
Epoch 2530, training loss: 902.447265625 = 0.07508223503828049 + 100.0 * 9.023721694946289
Epoch 2530, val loss: 0.5651832222938538
Epoch 2540, training loss: 902.60400390625 = 0.07421012967824936 + 100.0 * 9.025298118591309
Epoch 2540, val loss: 0.5666783452033997
Epoch 2550, training loss: 902.4735107421875 = 0.07332025468349457 + 100.0 * 9.024002075195312
Epoch 2550, val loss: 0.5691603422164917
Epoch 2560, training loss: 902.4675903320312 = 0.07243220508098602 + 100.0 * 9.023951530456543
Epoch 2560, val loss: 0.5701342821121216
Epoch 2570, training loss: 902.4341430664062 = 0.0715634822845459 + 100.0 * 9.023626327514648
Epoch 2570, val loss: 0.5725852847099304
Epoch 2580, training loss: 902.4052734375 = 0.07069000601768494 + 100.0 * 9.023345947265625
Epoch 2580, val loss: 0.5750375390052795
Epoch 2590, training loss: 902.4652099609375 = 0.06985718756914139 + 100.0 * 9.023953437805176
Epoch 2590, val loss: 0.5771536231040955
Epoch 2600, training loss: 902.5504760742188 = 0.06906363368034363 + 100.0 * 9.024813652038574
Epoch 2600, val loss: 0.5811163783073425
Epoch 2610, training loss: 902.3634033203125 = 0.06818853318691254 + 100.0 * 9.02295207977295
Epoch 2610, val loss: 0.5815088152885437
Epoch 2620, training loss: 902.331787109375 = 0.06736183166503906 + 100.0 * 9.02264404296875
Epoch 2620, val loss: 0.5834349989891052
Epoch 2630, training loss: 902.387451171875 = 0.0665641650557518 + 100.0 * 9.023208618164062
Epoch 2630, val loss: 0.5859310030937195
Epoch 2640, training loss: 902.378173828125 = 0.06577058136463165 + 100.0 * 9.023123741149902
Epoch 2640, val loss: 0.5882439017295837
Epoch 2650, training loss: 902.4168701171875 = 0.06498469412326813 + 100.0 * 9.023518562316895
Epoch 2650, val loss: 0.5899013876914978
Epoch 2660, training loss: 902.3235473632812 = 0.06419922411441803 + 100.0 * 9.02259349822998
Epoch 2660, val loss: 0.5919433832168579
Epoch 2670, training loss: 902.4268798828125 = 0.06344020366668701 + 100.0 * 9.023634910583496
Epoch 2670, val loss: 0.5948596000671387
Epoch 2680, training loss: 902.3092651367188 = 0.06267500668764114 + 100.0 * 9.022465705871582
Epoch 2680, val loss: 0.5978392362594604
Epoch 2690, training loss: 902.2762451171875 = 0.06190819293260574 + 100.0 * 9.022143363952637
Epoch 2690, val loss: 0.5990148186683655
Epoch 2700, training loss: 902.3971557617188 = 0.0611693300306797 + 100.0 * 9.023360252380371
Epoch 2700, val loss: 0.6004801392555237
Epoch 2710, training loss: 902.2344360351562 = 0.060437556356191635 + 100.0 * 9.021739959716797
Epoch 2710, val loss: 0.6023228168487549
Epoch 2720, training loss: 902.2732543945312 = 0.05973483994603157 + 100.0 * 9.022134780883789
Epoch 2720, val loss: 0.603755533695221
Epoch 2730, training loss: 902.2692260742188 = 0.05900069698691368 + 100.0 * 9.022102355957031
Epoch 2730, val loss: 0.607126772403717
Epoch 2740, training loss: 902.2584838867188 = 0.058322012424468994 + 100.0 * 9.022001266479492
Epoch 2740, val loss: 0.6109246015548706
Epoch 2750, training loss: 902.2089233398438 = 0.05760147422552109 + 100.0 * 9.021512985229492
Epoch 2750, val loss: 0.6123192310333252
Epoch 2760, training loss: 902.2050170898438 = 0.05690518766641617 + 100.0 * 9.02148151397705
Epoch 2760, val loss: 0.6133415699005127
Epoch 2770, training loss: 902.3336791992188 = 0.05624183639883995 + 100.0 * 9.022774696350098
Epoch 2770, val loss: 0.6166452765464783
Epoch 2780, training loss: 902.21923828125 = 0.05556321516633034 + 100.0 * 9.021636962890625
Epoch 2780, val loss: 0.6174341440200806
Epoch 2790, training loss: 902.1751098632812 = 0.05488166958093643 + 100.0 * 9.021202087402344
Epoch 2790, val loss: 0.6199718117713928
Epoch 2800, training loss: 902.2178344726562 = 0.05423003062605858 + 100.0 * 9.021636009216309
Epoch 2800, val loss: 0.6224913001060486
Epoch 2810, training loss: 902.1395874023438 = 0.053577687591314316 + 100.0 * 9.020859718322754
Epoch 2810, val loss: 0.6252760887145996
Epoch 2820, training loss: 902.173583984375 = 0.05293992534279823 + 100.0 * 9.021206855773926
Epoch 2820, val loss: 0.6269546151161194
Epoch 2830, training loss: 902.2039184570312 = 0.05231945589184761 + 100.0 * 9.021515846252441
Epoch 2830, val loss: 0.629272997379303
Epoch 2840, training loss: 902.135009765625 = 0.05167349800467491 + 100.0 * 9.020833015441895
Epoch 2840, val loss: 0.6297402381896973
Epoch 2850, training loss: 902.1776733398438 = 0.05106629431247711 + 100.0 * 9.021265983581543
Epoch 2850, val loss: 0.6318366527557373
Epoch 2860, training loss: 902.1407470703125 = 0.05044688656926155 + 100.0 * 9.020902633666992
Epoch 2860, val loss: 0.634554922580719
Epoch 2870, training loss: 902.100341796875 = 0.049857523292303085 + 100.0 * 9.02050495147705
Epoch 2870, val loss: 0.6377165913581848
Epoch 2880, training loss: 902.1358032226562 = 0.04927165061235428 + 100.0 * 9.020865440368652
Epoch 2880, val loss: 0.6395349502563477
Epoch 2890, training loss: 902.1076049804688 = 0.04866747930645943 + 100.0 * 9.020589828491211
Epoch 2890, val loss: 0.6409035325050354
Epoch 2900, training loss: 902.148193359375 = 0.04810221865773201 + 100.0 * 9.021000862121582
Epoch 2900, val loss: 0.6433789730072021
Epoch 2910, training loss: 902.112060546875 = 0.04753422737121582 + 100.0 * 9.020645141601562
Epoch 2910, val loss: 0.6458627581596375
Epoch 2920, training loss: 902.0130004882812 = 0.04694853350520134 + 100.0 * 9.019660949707031
Epoch 2920, val loss: 0.646651566028595
Epoch 2930, training loss: 902.0103149414062 = 0.04638992249965668 + 100.0 * 9.019639015197754
Epoch 2930, val loss: 0.6487341523170471
Epoch 2940, training loss: 902.2078247070312 = 0.04586494341492653 + 100.0 * 9.02161979675293
Epoch 2940, val loss: 0.6500139236450195
Epoch 2950, training loss: 902.041748046875 = 0.0453316830098629 + 100.0 * 9.019964218139648
Epoch 2950, val loss: 0.6544025540351868
Epoch 2960, training loss: 901.9663696289062 = 0.044764917343854904 + 100.0 * 9.019216537475586
Epoch 2960, val loss: 0.6554557681083679
Epoch 2970, training loss: 902.0966796875 = 0.04427528753876686 + 100.0 * 9.020524024963379
Epoch 2970, val loss: 0.659176230430603
Epoch 2980, training loss: 901.9625854492188 = 0.043733324855566025 + 100.0 * 9.019187927246094
Epoch 2980, val loss: 0.6597874164581299
Epoch 2990, training loss: 901.9303588867188 = 0.04320107772946358 + 100.0 * 9.018871307373047
Epoch 2990, val loss: 0.6617811918258667
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.6709
Flip ASR: 0.5907/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1037.0076904296875 = 1.1012728214263916 + 100.0 * 10.359063148498535
Epoch 0, val loss: 1.1008901596069336
Epoch 10, training loss: 1036.6566162109375 = 1.0911403894424438 + 100.0 * 10.355653762817383
Epoch 10, val loss: 1.0905588865280151
Epoch 20, training loss: 1030.344970703125 = 1.078282356262207 + 100.0 * 10.292667388916016
Epoch 20, val loss: 1.0778363943099976
Epoch 30, training loss: 969.1880493164062 = 1.0658810138702393 + 100.0 * 9.681221961975098
Epoch 30, val loss: 1.0656726360321045
Epoch 40, training loss: 954.155029296875 = 1.0537949800491333 + 100.0 * 9.531012535095215
Epoch 40, val loss: 1.0535006523132324
Epoch 50, training loss: 945.7858276367188 = 1.0397109985351562 + 100.0 * 9.447461128234863
Epoch 50, val loss: 1.0394102334976196
Epoch 60, training loss: 943.0224609375 = 1.0234489440917969 + 100.0 * 9.419990539550781
Epoch 60, val loss: 1.023352861404419
Epoch 70, training loss: 940.0681762695312 = 1.0080503225326538 + 100.0 * 9.39060115814209
Epoch 70, val loss: 1.0085668563842773
Epoch 80, training loss: 935.5951538085938 = 0.9963786602020264 + 100.0 * 9.345987319946289
Epoch 80, val loss: 0.9976019859313965
Epoch 90, training loss: 929.3162841796875 = 0.9873946309089661 + 100.0 * 9.283288955688477
Epoch 90, val loss: 0.9891415238380432
Epoch 100, training loss: 925.7365112304688 = 0.9769580364227295 + 100.0 * 9.24759578704834
Epoch 100, val loss: 0.9785007834434509
Epoch 110, training loss: 923.205810546875 = 0.9597012400627136 + 100.0 * 9.222460746765137
Epoch 110, val loss: 0.9613884091377258
Epoch 120, training loss: 921.2822875976562 = 0.9388905763626099 + 100.0 * 9.203433990478516
Epoch 120, val loss: 0.9409735798835754
Epoch 130, training loss: 919.828369140625 = 0.9167270660400391 + 100.0 * 9.189116477966309
Epoch 130, val loss: 0.919636607170105
Epoch 140, training loss: 918.701904296875 = 0.8933876156806946 + 100.0 * 9.178085327148438
Epoch 140, val loss: 0.8969561457633972
Epoch 150, training loss: 917.4431762695312 = 0.8693667054176331 + 100.0 * 9.165738105773926
Epoch 150, val loss: 0.8740554451942444
Epoch 160, training loss: 916.8137817382812 = 0.8449229598045349 + 100.0 * 9.159688949584961
Epoch 160, val loss: 0.8506082892417908
Epoch 170, training loss: 915.6421508789062 = 0.8187713027000427 + 100.0 * 9.148233413696289
Epoch 170, val loss: 0.8253689408302307
Epoch 180, training loss: 914.8173217773438 = 0.791815459728241 + 100.0 * 9.140254974365234
Epoch 180, val loss: 0.8000691533088684
Epoch 190, training loss: 914.5879516601562 = 0.7653270959854126 + 100.0 * 9.138226509094238
Epoch 190, val loss: 0.774809718132019
Epoch 200, training loss: 913.682861328125 = 0.7384583353996277 + 100.0 * 9.129444122314453
Epoch 200, val loss: 0.7496261596679688
Epoch 210, training loss: 913.1901245117188 = 0.7127341628074646 + 100.0 * 9.124773979187012
Epoch 210, val loss: 0.7254230976104736
Epoch 220, training loss: 912.9693603515625 = 0.6882131099700928 + 100.0 * 9.122811317443848
Epoch 220, val loss: 0.7027714848518372
Epoch 230, training loss: 912.44677734375 = 0.6649201512336731 + 100.0 * 9.117818832397461
Epoch 230, val loss: 0.6814046502113342
Epoch 240, training loss: 912.14599609375 = 0.6435891389846802 + 100.0 * 9.115023612976074
Epoch 240, val loss: 0.662143349647522
Epoch 250, training loss: 911.9530639648438 = 0.6240810751914978 + 100.0 * 9.113289833068848
Epoch 250, val loss: 0.6448851227760315
Epoch 260, training loss: 911.6481323242188 = 0.6059025526046753 + 100.0 * 9.110422134399414
Epoch 260, val loss: 0.6287885904312134
Epoch 270, training loss: 911.4135131835938 = 0.589604914188385 + 100.0 * 9.10823917388916
Epoch 270, val loss: 0.6148669719696045
Epoch 280, training loss: 911.1268310546875 = 0.5750666260719299 + 100.0 * 9.105517387390137
Epoch 280, val loss: 0.6025413274765015
Epoch 290, training loss: 910.884765625 = 0.5618233680725098 + 100.0 * 9.103229522705078
Epoch 290, val loss: 0.5916330814361572
Epoch 300, training loss: 911.072265625 = 0.549615204334259 + 100.0 * 9.105226516723633
Epoch 300, val loss: 0.581763505935669
Epoch 310, training loss: 910.4918212890625 = 0.5381344556808472 + 100.0 * 9.099536895751953
Epoch 310, val loss: 0.5726243853569031
Epoch 320, training loss: 910.3324584960938 = 0.5280050039291382 + 100.0 * 9.098044395446777
Epoch 320, val loss: 0.5647712349891663
Epoch 330, training loss: 910.115478515625 = 0.5187162756919861 + 100.0 * 9.095967292785645
Epoch 330, val loss: 0.5578744411468506
Epoch 340, training loss: 909.9774169921875 = 0.5098118782043457 + 100.0 * 9.09467601776123
Epoch 340, val loss: 0.5511285066604614
Epoch 350, training loss: 909.8191528320312 = 0.5015602111816406 + 100.0 * 9.093175888061523
Epoch 350, val loss: 0.5449784398078918
Epoch 360, training loss: 909.6397705078125 = 0.49402448534965515 + 100.0 * 9.09145736694336
Epoch 360, val loss: 0.539623498916626
Epoch 370, training loss: 909.5033569335938 = 0.4869456887245178 + 100.0 * 9.090164184570312
Epoch 370, val loss: 0.5346398949623108
Epoch 380, training loss: 909.3867797851562 = 0.48006826639175415 + 100.0 * 9.089067459106445
Epoch 380, val loss: 0.5298900604248047
Epoch 390, training loss: 909.3056640625 = 0.47355830669403076 + 100.0 * 9.0883207321167
Epoch 390, val loss: 0.5253987908363342
Epoch 400, training loss: 909.1343994140625 = 0.46750903129577637 + 100.0 * 9.086668968200684
Epoch 400, val loss: 0.5214431881904602
Epoch 410, training loss: 909.0692749023438 = 0.46165210008621216 + 100.0 * 9.086075782775879
Epoch 410, val loss: 0.5176870822906494
Epoch 420, training loss: 909.064208984375 = 0.45607349276542664 + 100.0 * 9.086081504821777
Epoch 420, val loss: 0.5140914916992188
Epoch 430, training loss: 908.8735961914062 = 0.45061370730400085 + 100.0 * 9.084229469299316
Epoch 430, val loss: 0.5104625821113586
Epoch 440, training loss: 908.7099609375 = 0.44549986720085144 + 100.0 * 9.08264446258545
Epoch 440, val loss: 0.5072450637817383
Epoch 450, training loss: 908.6057739257812 = 0.44060689210891724 + 100.0 * 9.08165168762207
Epoch 450, val loss: 0.5041373372077942
Epoch 460, training loss: 908.8780517578125 = 0.43591541051864624 + 100.0 * 9.084421157836914
Epoch 460, val loss: 0.5007084608078003
Epoch 470, training loss: 908.4299926757812 = 0.43103352189064026 + 100.0 * 9.079989433288574
Epoch 470, val loss: 0.49816182255744934
Epoch 480, training loss: 908.36962890625 = 0.42657628655433655 + 100.0 * 9.07943058013916
Epoch 480, val loss: 0.4958218038082123
Epoch 490, training loss: 908.3895874023438 = 0.4222819209098816 + 100.0 * 9.079672813415527
Epoch 490, val loss: 0.49343350529670715
Epoch 500, training loss: 908.2317504882812 = 0.4180498719215393 + 100.0 * 9.078137397766113
Epoch 500, val loss: 0.4906134009361267
Epoch 510, training loss: 908.2302856445312 = 0.41397956013679504 + 100.0 * 9.078163146972656
Epoch 510, val loss: 0.48823386430740356
Epoch 520, training loss: 908.0711059570312 = 0.40998008847236633 + 100.0 * 9.076611518859863
Epoch 520, val loss: 0.48627203702926636
Epoch 530, training loss: 908.0025634765625 = 0.4061742424964905 + 100.0 * 9.075963973999023
Epoch 530, val loss: 0.48429346084594727
Epoch 540, training loss: 908.1084594726562 = 0.4024611711502075 + 100.0 * 9.077059745788574
Epoch 540, val loss: 0.48204830288887024
Epoch 550, training loss: 908.0314331054688 = 0.39863136410713196 + 100.0 * 9.07632827758789
Epoch 550, val loss: 0.4804017245769501
Epoch 560, training loss: 907.828125 = 0.39504268765449524 + 100.0 * 9.074331283569336
Epoch 560, val loss: 0.4783165752887726
Epoch 570, training loss: 907.7429809570312 = 0.39162304997444153 + 100.0 * 9.073513984680176
Epoch 570, val loss: 0.47651809453964233
Epoch 580, training loss: 907.6669311523438 = 0.38830453157424927 + 100.0 * 9.072786331176758
Epoch 580, val loss: 0.47492948174476624
Epoch 590, training loss: 907.7398681640625 = 0.3850328326225281 + 100.0 * 9.073548316955566
Epoch 590, val loss: 0.47361305356025696
Epoch 600, training loss: 907.6632080078125 = 0.3816923201084137 + 100.0 * 9.07281494140625
Epoch 600, val loss: 0.4710066020488739
Epoch 610, training loss: 907.568359375 = 0.37841489911079407 + 100.0 * 9.0718994140625
Epoch 610, val loss: 0.4698997437953949
Epoch 620, training loss: 907.483154296875 = 0.3753276467323303 + 100.0 * 9.071078300476074
Epoch 620, val loss: 0.4683752655982971
Epoch 630, training loss: 907.3834228515625 = 0.372336745262146 + 100.0 * 9.070111274719238
Epoch 630, val loss: 0.46677953004837036
Epoch 640, training loss: 907.4593505859375 = 0.3693945109844208 + 100.0 * 9.070899963378906
Epoch 640, val loss: 0.4650306701660156
Epoch 650, training loss: 907.4449462890625 = 0.3662852942943573 + 100.0 * 9.070786476135254
Epoch 650, val loss: 0.4638482928276062
Epoch 660, training loss: 907.3040771484375 = 0.3633306920528412 + 100.0 * 9.06940746307373
Epoch 660, val loss: 0.4624447822570801
Epoch 670, training loss: 907.1974487304688 = 0.3604907989501953 + 100.0 * 9.06836986541748
Epoch 670, val loss: 0.4607658088207245
Epoch 680, training loss: 907.1737060546875 = 0.35769814252853394 + 100.0 * 9.068160057067871
Epoch 680, val loss: 0.45954787731170654
Epoch 690, training loss: 907.2272338867188 = 0.3548169732093811 + 100.0 * 9.068724632263184
Epoch 690, val loss: 0.4580572545528412
Epoch 700, training loss: 907.076416015625 = 0.3519895076751709 + 100.0 * 9.067244529724121
Epoch 700, val loss: 0.4565892517566681
Epoch 710, training loss: 907.0167236328125 = 0.3492826223373413 + 100.0 * 9.06667423248291
Epoch 710, val loss: 0.45536008477211
Epoch 720, training loss: 906.962158203125 = 0.34662020206451416 + 100.0 * 9.066155433654785
Epoch 720, val loss: 0.4540547728538513
Epoch 730, training loss: 907.0503540039062 = 0.3439665734767914 + 100.0 * 9.067063331604004
Epoch 730, val loss: 0.45294398069381714
Epoch 740, training loss: 906.878662109375 = 0.3412647843360901 + 100.0 * 9.065374374389648
Epoch 740, val loss: 0.4511784613132477
Epoch 750, training loss: 907.0804443359375 = 0.3386130928993225 + 100.0 * 9.067418098449707
Epoch 750, val loss: 0.45038843154907227
Epoch 760, training loss: 906.866455078125 = 0.3359726667404175 + 100.0 * 9.06530475616455
Epoch 760, val loss: 0.44830840826034546
Epoch 770, training loss: 906.7313232421875 = 0.33341410756111145 + 100.0 * 9.063979148864746
Epoch 770, val loss: 0.44744426012039185
Epoch 780, training loss: 906.6984252929688 = 0.33091944456100464 + 100.0 * 9.063674926757812
Epoch 780, val loss: 0.4461863934993744
Epoch 790, training loss: 906.8290405273438 = 0.32843008637428284 + 100.0 * 9.065006256103516
Epoch 790, val loss: 0.44499844312667847
Epoch 800, training loss: 906.6664428710938 = 0.32589036226272583 + 100.0 * 9.063405990600586
Epoch 800, val loss: 0.4436984658241272
Epoch 810, training loss: 906.7860107421875 = 0.3233949840068817 + 100.0 * 9.06462574005127
Epoch 810, val loss: 0.442851722240448
Epoch 820, training loss: 906.5615234375 = 0.3209682106971741 + 100.0 * 9.062405586242676
Epoch 820, val loss: 0.44119369983673096
Epoch 830, training loss: 906.4921264648438 = 0.3185855448246002 + 100.0 * 9.061735153198242
Epoch 830, val loss: 0.44021838903427124
Epoch 840, training loss: 906.4569702148438 = 0.3162652552127838 + 100.0 * 9.061407089233398
Epoch 840, val loss: 0.43919456005096436
Epoch 850, training loss: 906.885498046875 = 0.3139292597770691 + 100.0 * 9.065715789794922
Epoch 850, val loss: 0.4382491111755371
Epoch 860, training loss: 906.5170288085938 = 0.3114607334136963 + 100.0 * 9.062055587768555
Epoch 860, val loss: 0.4368770122528076
Epoch 870, training loss: 906.3574829101562 = 0.3091815114021301 + 100.0 * 9.0604829788208
Epoch 870, val loss: 0.4362557530403137
Epoch 880, training loss: 906.3241577148438 = 0.3069520890712738 + 100.0 * 9.060172080993652
Epoch 880, val loss: 0.4351610243320465
Epoch 890, training loss: 906.5357666015625 = 0.3047209680080414 + 100.0 * 9.062310218811035
Epoch 890, val loss: 0.4343838393688202
Epoch 900, training loss: 906.3114013671875 = 0.3024756908416748 + 100.0 * 9.060089111328125
Epoch 900, val loss: 0.43341097235679626
Epoch 910, training loss: 906.2220458984375 = 0.3003007471561432 + 100.0 * 9.05921745300293
Epoch 910, val loss: 0.4329119622707367
Epoch 920, training loss: 906.3716430664062 = 0.29816722869873047 + 100.0 * 9.060734748840332
Epoch 920, val loss: 0.43251171708106995
Epoch 930, training loss: 906.2380981445312 = 0.29598867893218994 + 100.0 * 9.05942153930664
Epoch 930, val loss: 0.4310876429080963
Epoch 940, training loss: 906.1915283203125 = 0.2938602864742279 + 100.0 * 9.058976173400879
Epoch 940, val loss: 0.4309884011745453
Epoch 950, training loss: 906.0745239257812 = 0.2918025851249695 + 100.0 * 9.05782699584961
Epoch 950, val loss: 0.4296330213546753
Epoch 960, training loss: 906.0233154296875 = 0.2897701859474182 + 100.0 * 9.057334899902344
Epoch 960, val loss: 0.42926210165023804
Epoch 970, training loss: 906.10986328125 = 0.2877630591392517 + 100.0 * 9.058220863342285
Epoch 970, val loss: 0.42867225408554077
Epoch 980, training loss: 906.0286865234375 = 0.28567269444465637 + 100.0 * 9.057430267333984
Epoch 980, val loss: 0.42796099185943604
Epoch 990, training loss: 905.9896240234375 = 0.28366875648498535 + 100.0 * 9.057059288024902
Epoch 990, val loss: 0.4276072382926941
Epoch 1000, training loss: 905.895751953125 = 0.28169047832489014 + 100.0 * 9.056140899658203
Epoch 1000, val loss: 0.4270128905773163
Epoch 1010, training loss: 906.0501098632812 = 0.27973347902297974 + 100.0 * 9.057703971862793
Epoch 1010, val loss: 0.4268079698085785
Epoch 1020, training loss: 905.9602661132812 = 0.2777523398399353 + 100.0 * 9.056824684143066
Epoch 1020, val loss: 0.42711228132247925
Epoch 1030, training loss: 905.8457641601562 = 0.2757963538169861 + 100.0 * 9.055699348449707
Epoch 1030, val loss: 0.42592790722846985
Epoch 1040, training loss: 905.81005859375 = 0.27387553453445435 + 100.0 * 9.0553617477417
Epoch 1040, val loss: 0.4258682131767273
Epoch 1050, training loss: 905.86865234375 = 0.2720114290714264 + 100.0 * 9.0559663772583
Epoch 1050, val loss: 0.4251556992530823
Epoch 1060, training loss: 905.7506103515625 = 0.2700308859348297 + 100.0 * 9.054805755615234
Epoch 1060, val loss: 0.4257044792175293
Epoch 1070, training loss: 905.6996459960938 = 0.2681930363178253 + 100.0 * 9.054314613342285
Epoch 1070, val loss: 0.42496880888938904
Epoch 1080, training loss: 905.6505126953125 = 0.26634228229522705 + 100.0 * 9.053841590881348
Epoch 1080, val loss: 0.42528036236763
Epoch 1090, training loss: 905.7326049804688 = 0.26458463072776794 + 100.0 * 9.054679870605469
Epoch 1090, val loss: 0.42468780279159546
Epoch 1100, training loss: 905.5686645507812 = 0.2626636326313019 + 100.0 * 9.053060531616211
Epoch 1100, val loss: 0.42512020468711853
Epoch 1110, training loss: 905.6002197265625 = 0.2608710825443268 + 100.0 * 9.053393363952637
Epoch 1110, val loss: 0.4248654544353485
Epoch 1120, training loss: 905.5728149414062 = 0.25905317068099976 + 100.0 * 9.05313777923584
Epoch 1120, val loss: 0.42516791820526123
Epoch 1130, training loss: 905.5914306640625 = 0.2573074400424957 + 100.0 * 9.053340911865234
Epoch 1130, val loss: 0.42524605989456177
Epoch 1140, training loss: 905.5597534179688 = 0.2555018663406372 + 100.0 * 9.0530424118042
Epoch 1140, val loss: 0.42527270317077637
Epoch 1150, training loss: 905.4599609375 = 0.25377747416496277 + 100.0 * 9.052062034606934
Epoch 1150, val loss: 0.4253182113170624
Epoch 1160, training loss: 905.4027099609375 = 0.2520456612110138 + 100.0 * 9.051506042480469
Epoch 1160, val loss: 0.4253588914871216
Epoch 1170, training loss: 905.4705200195312 = 0.2503366768360138 + 100.0 * 9.052201271057129
Epoch 1170, val loss: 0.4255017638206482
Epoch 1180, training loss: 905.3369140625 = 0.24857217073440552 + 100.0 * 9.050883293151855
Epoch 1180, val loss: 0.42547139525413513
Epoch 1190, training loss: 905.3082275390625 = 0.2468750923871994 + 100.0 * 9.050613403320312
Epoch 1190, val loss: 0.4255462884902954
Epoch 1200, training loss: 905.4681396484375 = 0.24520206451416016 + 100.0 * 9.052229881286621
Epoch 1200, val loss: 0.42561638355255127
Epoch 1210, training loss: 905.35791015625 = 0.24347977340221405 + 100.0 * 9.05114459991455
Epoch 1210, val loss: 0.42640766501426697
Epoch 1220, training loss: 905.3130493164062 = 0.2417878657579422 + 100.0 * 9.050712585449219
Epoch 1220, val loss: 0.42607057094573975
Epoch 1230, training loss: 905.245361328125 = 0.240137979388237 + 100.0 * 9.050052642822266
Epoch 1230, val loss: 0.4262100160121918
Epoch 1240, training loss: 905.2778930664062 = 0.23848815262317657 + 100.0 * 9.050394058227539
Epoch 1240, val loss: 0.42679527401924133
Epoch 1250, training loss: 905.2716674804688 = 0.2368302345275879 + 100.0 * 9.050348281860352
Epoch 1250, val loss: 0.42713600397109985
Epoch 1260, training loss: 905.1549072265625 = 0.23519425094127655 + 100.0 * 9.04919719696045
Epoch 1260, val loss: 0.4266923666000366
Epoch 1270, training loss: 905.1315307617188 = 0.23356962203979492 + 100.0 * 9.048979759216309
Epoch 1270, val loss: 0.4274672865867615
Epoch 1280, training loss: 905.221923828125 = 0.23194767534732819 + 100.0 * 9.04990005493164
Epoch 1280, val loss: 0.42732489109039307
Epoch 1290, training loss: 905.1126708984375 = 0.23036862909793854 + 100.0 * 9.048823356628418
Epoch 1290, val loss: 0.42726001143455505
Epoch 1300, training loss: 905.078125 = 0.22872941195964813 + 100.0 * 9.048493385314941
Epoch 1300, val loss: 0.42814093828201294
Epoch 1310, training loss: 905.0222778320312 = 0.22714096307754517 + 100.0 * 9.047951698303223
Epoch 1310, val loss: 0.428093820810318
Epoch 1320, training loss: 905.0490112304688 = 0.22555939853191376 + 100.0 * 9.048233985900879
Epoch 1320, val loss: 0.4285965859889984
Epoch 1330, training loss: 904.9979248046875 = 0.22397135198116302 + 100.0 * 9.047739028930664
Epoch 1330, val loss: 0.42878392338752747
Epoch 1340, training loss: 905.0818481445312 = 0.22242110967636108 + 100.0 * 9.04859447479248
Epoch 1340, val loss: 0.42877107858657837
Epoch 1350, training loss: 904.9290161132812 = 0.22080889344215393 + 100.0 * 9.04708194732666
Epoch 1350, val loss: 0.4295646846294403
Epoch 1360, training loss: 904.8755493164062 = 0.21924744546413422 + 100.0 * 9.046563148498535
Epoch 1360, val loss: 0.42964041233062744
Epoch 1370, training loss: 904.948974609375 = 0.21771372854709625 + 100.0 * 9.04731273651123
Epoch 1370, val loss: 0.4304075241088867
Epoch 1380, training loss: 904.857666015625 = 0.21612012386322021 + 100.0 * 9.046415328979492
Epoch 1380, val loss: 0.4307000935077667
Epoch 1390, training loss: 904.9119873046875 = 0.21457333862781525 + 100.0 * 9.046974182128906
Epoch 1390, val loss: 0.43154770135879517
Epoch 1400, training loss: 904.7984619140625 = 0.21301543712615967 + 100.0 * 9.045854568481445
Epoch 1400, val loss: 0.43144622445106506
Epoch 1410, training loss: 904.75830078125 = 0.2114812582731247 + 100.0 * 9.0454683303833
Epoch 1410, val loss: 0.43201273679733276
Epoch 1420, training loss: 904.8321533203125 = 0.209967702627182 + 100.0 * 9.046221733093262
Epoch 1420, val loss: 0.4322047531604767
Epoch 1430, training loss: 904.8062133789062 = 0.20844735205173492 + 100.0 * 9.045977592468262
Epoch 1430, val loss: 0.4321975111961365
Epoch 1440, training loss: 904.7159423828125 = 0.2068670690059662 + 100.0 * 9.045090675354004
Epoch 1440, val loss: 0.43342408537864685
Epoch 1450, training loss: 904.64306640625 = 0.2053517997264862 + 100.0 * 9.044377326965332
Epoch 1450, val loss: 0.43381425738334656
Epoch 1460, training loss: 904.660400390625 = 0.20384447276592255 + 100.0 * 9.044565200805664
Epoch 1460, val loss: 0.43457910418510437
Epoch 1470, training loss: 904.8155517578125 = 0.20234759151935577 + 100.0 * 9.04613208770752
Epoch 1470, val loss: 0.4352129101753235
Epoch 1480, training loss: 904.6386108398438 = 0.20080269873142242 + 100.0 * 9.044378280639648
Epoch 1480, val loss: 0.4353054463863373
Epoch 1490, training loss: 904.5494384765625 = 0.19928649067878723 + 100.0 * 9.043501853942871
Epoch 1490, val loss: 0.43590402603149414
Epoch 1500, training loss: 904.7745971679688 = 0.1978163719177246 + 100.0 * 9.045767784118652
Epoch 1500, val loss: 0.43597397208213806
Epoch 1510, training loss: 904.5880737304688 = 0.19629156589508057 + 100.0 * 9.043917655944824
Epoch 1510, val loss: 0.4371493458747864
Epoch 1520, training loss: 904.5233154296875 = 0.19477161765098572 + 100.0 * 9.043285369873047
Epoch 1520, val loss: 0.4374799430370331
Epoch 1530, training loss: 904.4487915039062 = 0.19327110052108765 + 100.0 * 9.04255485534668
Epoch 1530, val loss: 0.43848520517349243
Epoch 1540, training loss: 904.5096435546875 = 0.1917942762374878 + 100.0 * 9.04317855834961
Epoch 1540, val loss: 0.43910354375839233
Epoch 1550, training loss: 904.5103759765625 = 0.19029100239276886 + 100.0 * 9.043200492858887
Epoch 1550, val loss: 0.4394298791885376
Epoch 1560, training loss: 904.6655883789062 = 0.18883392214775085 + 100.0 * 9.044767379760742
Epoch 1560, val loss: 0.44096192717552185
Epoch 1570, training loss: 904.412109375 = 0.18734493851661682 + 100.0 * 9.042247772216797
Epoch 1570, val loss: 0.44066017866134644
Epoch 1580, training loss: 904.3589477539062 = 0.18587885797023773 + 100.0 * 9.041730880737305
Epoch 1580, val loss: 0.44147950410842896
Epoch 1590, training loss: 904.3102416992188 = 0.18441767990589142 + 100.0 * 9.041258811950684
Epoch 1590, val loss: 0.44208618998527527
Epoch 1600, training loss: 904.297607421875 = 0.18296822905540466 + 100.0 * 9.041146278381348
Epoch 1600, val loss: 0.4427209496498108
Epoch 1610, training loss: 904.6005249023438 = 0.18156158924102783 + 100.0 * 9.044189453125
Epoch 1610, val loss: 0.44381240010261536
Epoch 1620, training loss: 904.4297485351562 = 0.1800742745399475 + 100.0 * 9.042496681213379
Epoch 1620, val loss: 0.4441806972026825
Epoch 1630, training loss: 904.2838745117188 = 0.17861436307430267 + 100.0 * 9.04105281829834
Epoch 1630, val loss: 0.4448954463005066
Epoch 1640, training loss: 904.2177124023438 = 0.1771731972694397 + 100.0 * 9.0404052734375
Epoch 1640, val loss: 0.4458189308643341
Epoch 1650, training loss: 904.3739624023438 = 0.17579467594623566 + 100.0 * 9.04198169708252
Epoch 1650, val loss: 0.4459158778190613
Epoch 1660, training loss: 904.1864013671875 = 0.17430227994918823 + 100.0 * 9.040121078491211
Epoch 1660, val loss: 0.4474599063396454
Epoch 1670, training loss: 904.1591186523438 = 0.17287850379943848 + 100.0 * 9.039862632751465
Epoch 1670, val loss: 0.4478611946105957
Epoch 1680, training loss: 904.2495727539062 = 0.17147283256053925 + 100.0 * 9.040781021118164
Epoch 1680, val loss: 0.4491420388221741
Epoch 1690, training loss: 904.1654663085938 = 0.1700393408536911 + 100.0 * 9.03995418548584
Epoch 1690, val loss: 0.45003724098205566
Epoch 1700, training loss: 904.1436767578125 = 0.1686280518770218 + 100.0 * 9.039750099182129
Epoch 1700, val loss: 0.4510526955127716
Epoch 1710, training loss: 904.23095703125 = 0.16723105311393738 + 100.0 * 9.040637016296387
Epoch 1710, val loss: 0.4515208601951599
Epoch 1720, training loss: 904.0912475585938 = 0.1658151000738144 + 100.0 * 9.039254188537598
Epoch 1720, val loss: 0.4522291421890259
Epoch 1730, training loss: 904.0475463867188 = 0.16439348459243774 + 100.0 * 9.03883171081543
Epoch 1730, val loss: 0.45374664664268494
Epoch 1740, training loss: 904.0292358398438 = 0.16300241649150848 + 100.0 * 9.03866195678711
Epoch 1740, val loss: 0.4546826481819153
Epoch 1750, training loss: 904.1884155273438 = 0.1616380512714386 + 100.0 * 9.040267944335938
Epoch 1750, val loss: 0.45605000853538513
Epoch 1760, training loss: 904.0607299804688 = 0.16021135449409485 + 100.0 * 9.039005279541016
Epoch 1760, val loss: 0.4559164345264435
Epoch 1770, training loss: 904.0189819335938 = 0.1588222086429596 + 100.0 * 9.038601875305176
Epoch 1770, val loss: 0.4568895399570465
Epoch 1780, training loss: 904.0348510742188 = 0.1574840098619461 + 100.0 * 9.038773536682129
Epoch 1780, val loss: 0.4574342370033264
Epoch 1790, training loss: 903.990478515625 = 0.15607407689094543 + 100.0 * 9.038344383239746
Epoch 1790, val loss: 0.45921140909194946
Epoch 1800, training loss: 904.0606689453125 = 0.15471883118152618 + 100.0 * 9.03905963897705
Epoch 1800, val loss: 0.4604872763156891
Epoch 1810, training loss: 903.9381103515625 = 0.15335842967033386 + 100.0 * 9.037847518920898
Epoch 1810, val loss: 0.46098482608795166
Epoch 1820, training loss: 903.9002685546875 = 0.1520010381937027 + 100.0 * 9.037483215332031
Epoch 1820, val loss: 0.4626074433326721
Epoch 1830, training loss: 904.0895385742188 = 0.150673046708107 + 100.0 * 9.039388656616211
Epoch 1830, val loss: 0.4627561867237091
Epoch 1840, training loss: 904.088623046875 = 0.1493365466594696 + 100.0 * 9.039392471313477
Epoch 1840, val loss: 0.46397385001182556
Epoch 1850, training loss: 903.8690795898438 = 0.14796578884124756 + 100.0 * 9.037211418151855
Epoch 1850, val loss: 0.4660341739654541
Epoch 1860, training loss: 903.7951049804688 = 0.14661289751529694 + 100.0 * 9.036484718322754
Epoch 1860, val loss: 0.4665444791316986
Epoch 1870, training loss: 903.7940063476562 = 0.1452818363904953 + 100.0 * 9.036487579345703
Epoch 1870, val loss: 0.4671180844306946
Epoch 1880, training loss: 904.0888671875 = 0.14399591088294983 + 100.0 * 9.039448738098145
Epoch 1880, val loss: 0.46829983592033386
Epoch 1890, training loss: 903.8650512695312 = 0.14263911545276642 + 100.0 * 9.037223815917969
Epoch 1890, val loss: 0.4703691005706787
Epoch 1900, training loss: 903.8002319335938 = 0.14131000638008118 + 100.0 * 9.036589622497559
Epoch 1900, val loss: 0.47053053975105286
Epoch 1910, training loss: 903.9297485351562 = 0.139985129237175 + 100.0 * 9.037897109985352
Epoch 1910, val loss: 0.4722530245780945
Epoch 1920, training loss: 903.7930297851562 = 0.1387537270784378 + 100.0 * 9.036542892456055
Epoch 1920, val loss: 0.47445034980773926
Epoch 1930, training loss: 903.6879272460938 = 0.13738001883029938 + 100.0 * 9.035505294799805
Epoch 1930, val loss: 0.4741792678833008
Epoch 1940, training loss: 903.705322265625 = 0.13610434532165527 + 100.0 * 9.03569221496582
Epoch 1940, val loss: 0.4757881462574005
Epoch 1950, training loss: 903.7657470703125 = 0.13484111428260803 + 100.0 * 9.036309242248535
Epoch 1950, val loss: 0.47732752561569214
Epoch 1960, training loss: 903.6988525390625 = 0.133571058511734 + 100.0 * 9.035653114318848
Epoch 1960, val loss: 0.4783036410808563
Epoch 1970, training loss: 903.720703125 = 0.13229775428771973 + 100.0 * 9.035883903503418
Epoch 1970, val loss: 0.4790176451206207
Epoch 1980, training loss: 903.681640625 = 0.131063774228096 + 100.0 * 9.035506248474121
Epoch 1980, val loss: 0.4800908863544464
Epoch 1990, training loss: 903.636474609375 = 0.12979210913181305 + 100.0 * 9.035066604614258
Epoch 1990, val loss: 0.48166826367378235
Epoch 2000, training loss: 903.6152954101562 = 0.12855152785778046 + 100.0 * 9.034867286682129
Epoch 2000, val loss: 0.4831247329711914
Epoch 2010, training loss: 903.5733032226562 = 0.12730835378170013 + 100.0 * 9.034460067749023
Epoch 2010, val loss: 0.48433274030685425
Epoch 2020, training loss: 903.5469360351562 = 0.12608113884925842 + 100.0 * 9.034208297729492
Epoch 2020, val loss: 0.48523852229118347
Epoch 2030, training loss: 903.7147827148438 = 0.1248830035328865 + 100.0 * 9.03589916229248
Epoch 2030, val loss: 0.48641860485076904
Epoch 2040, training loss: 903.6624145507812 = 0.12372149527072906 + 100.0 * 9.03538703918457
Epoch 2040, val loss: 0.48915302753448486
Epoch 2050, training loss: 903.6516723632812 = 0.1224704310297966 + 100.0 * 9.03529167175293
Epoch 2050, val loss: 0.49017345905303955
Epoch 2060, training loss: 903.5770263671875 = 0.1212516501545906 + 100.0 * 9.034557342529297
Epoch 2060, val loss: 0.4906189441680908
Epoch 2070, training loss: 903.4666137695312 = 0.12004225701093674 + 100.0 * 9.033465385437012
Epoch 2070, val loss: 0.4919402599334717
Epoch 2080, training loss: 903.4617919921875 = 0.11886627227067947 + 100.0 * 9.033429145812988
Epoch 2080, val loss: 0.4931585490703583
Epoch 2090, training loss: 903.6657104492188 = 0.11771447211503983 + 100.0 * 9.035479545593262
Epoch 2090, val loss: 0.4941268265247345
Epoch 2100, training loss: 903.5833740234375 = 0.1165509894490242 + 100.0 * 9.03466796875
Epoch 2100, val loss: 0.49737828969955444
Epoch 2110, training loss: 903.4285278320312 = 0.11534548550844193 + 100.0 * 9.03313159942627
Epoch 2110, val loss: 0.49729469418525696
Epoch 2120, training loss: 903.3907470703125 = 0.11418571323156357 + 100.0 * 9.03276538848877
Epoch 2120, val loss: 0.49861064553260803
Epoch 2130, training loss: 903.4165649414062 = 0.11306079477071762 + 100.0 * 9.033035278320312
Epoch 2130, val loss: 0.4990989863872528
Epoch 2140, training loss: 903.4992065429688 = 0.11190155148506165 + 100.0 * 9.033873558044434
Epoch 2140, val loss: 0.5013747811317444
Epoch 2150, training loss: 903.5143432617188 = 0.110805444419384 + 100.0 * 9.034035682678223
Epoch 2150, val loss: 0.5045011043548584
Epoch 2160, training loss: 903.4423217773438 = 0.10964108258485794 + 100.0 * 9.033327102661133
Epoch 2160, val loss: 0.5042078495025635
Epoch 2170, training loss: 903.3652954101562 = 0.1085030734539032 + 100.0 * 9.032567977905273
Epoch 2170, val loss: 0.5072669982910156
Epoch 2180, training loss: 903.40869140625 = 0.10738319903612137 + 100.0 * 9.033013343811035
Epoch 2180, val loss: 0.5084369778633118
Epoch 2190, training loss: 903.275390625 = 0.10627163201570511 + 100.0 * 9.031691551208496
Epoch 2190, val loss: 0.5099828839302063
Epoch 2200, training loss: 903.2745971679688 = 0.1051517128944397 + 100.0 * 9.031694412231445
Epoch 2200, val loss: 0.5111006498336792
Epoch 2210, training loss: 903.317626953125 = 0.10411792993545532 + 100.0 * 9.032135009765625
Epoch 2210, val loss: 0.5135473608970642
Epoch 2220, training loss: 903.312255859375 = 0.10304282605648041 + 100.0 * 9.032092094421387
Epoch 2220, val loss: 0.5152003765106201
Epoch 2230, training loss: 903.37353515625 = 0.10190502554178238 + 100.0 * 9.032715797424316
Epoch 2230, val loss: 0.5154609084129333
Epoch 2240, training loss: 903.2359619140625 = 0.10083173215389252 + 100.0 * 9.031351089477539
Epoch 2240, val loss: 0.5176859498023987
Epoch 2250, training loss: 903.241455078125 = 0.09977076202630997 + 100.0 * 9.031416893005371
Epoch 2250, val loss: 0.5185021758079529
Epoch 2260, training loss: 903.3612060546875 = 0.09873080253601074 + 100.0 * 9.032624244689941
Epoch 2260, val loss: 0.5209009051322937
Epoch 2270, training loss: 903.3646850585938 = 0.09768103808164597 + 100.0 * 9.032670021057129
Epoch 2270, val loss: 0.5229451656341553
Epoch 2280, training loss: 903.1878662109375 = 0.09662166982889175 + 100.0 * 9.030912399291992
Epoch 2280, val loss: 0.5241110920906067
Epoch 2290, training loss: 903.1345825195312 = 0.09557046741247177 + 100.0 * 9.030389785766602
Epoch 2290, val loss: 0.5259425640106201
Epoch 2300, training loss: 903.2425537109375 = 0.09454632550477982 + 100.0 * 9.031479835510254
Epoch 2300, val loss: 0.5276418924331665
Epoch 2310, training loss: 903.1115112304688 = 0.09351547807455063 + 100.0 * 9.030179977416992
Epoch 2310, val loss: 0.5290257334709167
Epoch 2320, training loss: 903.0940551757812 = 0.09248542040586472 + 100.0 * 9.03001594543457
Epoch 2320, val loss: 0.5303876399993896
Epoch 2330, training loss: 903.1362915039062 = 0.09147508442401886 + 100.0 * 9.030447959899902
Epoch 2330, val loss: 0.5325347781181335
Epoch 2340, training loss: 903.1847534179688 = 0.09047579020261765 + 100.0 * 9.030942916870117
Epoch 2340, val loss: 0.5336909890174866
Epoch 2350, training loss: 903.1495361328125 = 0.08948100358247757 + 100.0 * 9.030600547790527
Epoch 2350, val loss: 0.5352067351341248
Epoch 2360, training loss: 903.036376953125 = 0.08848085254430771 + 100.0 * 9.029479026794434
Epoch 2360, val loss: 0.5386258363723755
Epoch 2370, training loss: 903.0325927734375 = 0.08748670667409897 + 100.0 * 9.029451370239258
Epoch 2370, val loss: 0.5400386452674866
Epoch 2380, training loss: 903.225830078125 = 0.0865587443113327 + 100.0 * 9.031393051147461
Epoch 2380, val loss: 0.5422843098640442
Epoch 2390, training loss: 903.2749633789062 = 0.08558854460716248 + 100.0 * 9.031893730163574
Epoch 2390, val loss: 0.5418717265129089
Epoch 2400, training loss: 903.0715942382812 = 0.08463633060455322 + 100.0 * 9.029869079589844
Epoch 2400, val loss: 0.5468094348907471
Epoch 2410, training loss: 902.964599609375 = 0.08361890912055969 + 100.0 * 9.028809547424316
Epoch 2410, val loss: 0.5465919375419617
Epoch 2420, training loss: 902.9361572265625 = 0.08266156911849976 + 100.0 * 9.028534889221191
Epoch 2420, val loss: 0.5486002564430237
Epoch 2430, training loss: 903.0548706054688 = 0.08174744248390198 + 100.0 * 9.029731750488281
Epoch 2430, val loss: 0.5508206486701965
Epoch 2440, training loss: 902.989013671875 = 0.08086106926202774 + 100.0 * 9.029081344604492
Epoch 2440, val loss: 0.5530498027801514
Epoch 2450, training loss: 902.9240112304688 = 0.07989777624607086 + 100.0 * 9.028441429138184
Epoch 2450, val loss: 0.5544395446777344
Epoch 2460, training loss: 902.904541015625 = 0.07895871251821518 + 100.0 * 9.028255462646484
Epoch 2460, val loss: 0.5557824969291687
Epoch 2470, training loss: 902.9591064453125 = 0.07805374264717102 + 100.0 * 9.028810501098633
Epoch 2470, val loss: 0.5575481653213501
Epoch 2480, training loss: 903.0022583007812 = 0.07715535163879395 + 100.0 * 9.029251098632812
Epoch 2480, val loss: 0.5592250823974609
Epoch 2490, training loss: 903.015625 = 0.07630623131990433 + 100.0 * 9.029393196105957
Epoch 2490, val loss: 0.5598329901695251
Epoch 2500, training loss: 902.8699340820312 = 0.07536488771438599 + 100.0 * 9.027945518493652
Epoch 2500, val loss: 0.5637680888175964
Epoch 2510, training loss: 902.8302001953125 = 0.07448544353246689 + 100.0 * 9.027557373046875
Epoch 2510, val loss: 0.5659723281860352
Epoch 2520, training loss: 902.9384155273438 = 0.07365456968545914 + 100.0 * 9.028647422790527
Epoch 2520, val loss: 0.5685714483261108
Epoch 2530, training loss: 902.9236450195312 = 0.07282595336437225 + 100.0 * 9.028508186340332
Epoch 2530, val loss: 0.5680868029594421
Epoch 2540, training loss: 902.7808227539062 = 0.07190924137830734 + 100.0 * 9.02708911895752
Epoch 2540, val loss: 0.5716153383255005
Epoch 2550, training loss: 902.7540893554688 = 0.07105454057455063 + 100.0 * 9.026830673217773
Epoch 2550, val loss: 0.5730872750282288
Epoch 2560, training loss: 902.7484130859375 = 0.07021505385637283 + 100.0 * 9.026782035827637
Epoch 2560, val loss: 0.5747762322425842
Epoch 2570, training loss: 903.0562744140625 = 0.06942840665578842 + 100.0 * 9.029868125915527
Epoch 2570, val loss: 0.5770846605300903
Epoch 2580, training loss: 902.8619995117188 = 0.06863100826740265 + 100.0 * 9.027934074401855
Epoch 2580, val loss: 0.5795972943305969
Epoch 2590, training loss: 902.9063110351562 = 0.06782548874616623 + 100.0 * 9.028385162353516
Epoch 2590, val loss: 0.5807563066482544
Epoch 2600, training loss: 902.7245483398438 = 0.06697651743888855 + 100.0 * 9.026576042175293
Epoch 2600, val loss: 0.5841431617736816
Epoch 2610, training loss: 902.7208862304688 = 0.06619026511907578 + 100.0 * 9.0265474319458
Epoch 2610, val loss: 0.5856359601020813
Epoch 2620, training loss: 902.73974609375 = 0.06541363149881363 + 100.0 * 9.026742935180664
Epoch 2620, val loss: 0.5861347317695618
Epoch 2630, training loss: 902.8987426757812 = 0.06463399529457092 + 100.0 * 9.028341293334961
Epoch 2630, val loss: 0.5883795619010925
Epoch 2640, training loss: 902.7275390625 = 0.06389033794403076 + 100.0 * 9.026636123657227
Epoch 2640, val loss: 0.5914385318756104
Epoch 2650, training loss: 902.698974609375 = 0.063094362616539 + 100.0 * 9.026358604431152
Epoch 2650, val loss: 0.5930038094520569
Epoch 2660, training loss: 902.7838745117188 = 0.06235425919294357 + 100.0 * 9.027215003967285
Epoch 2660, val loss: 0.5954951047897339
Epoch 2670, training loss: 902.7139282226562 = 0.061613064259290695 + 100.0 * 9.02652359008789
Epoch 2670, val loss: 0.5981535911560059
Epoch 2680, training loss: 902.6139526367188 = 0.060855284333229065 + 100.0 * 9.025530815124512
Epoch 2680, val loss: 0.5995162129402161
Epoch 2690, training loss: 902.6814575195312 = 0.0601985827088356 + 100.0 * 9.026212692260742
Epoch 2690, val loss: 0.60166996717453
Epoch 2700, training loss: 902.73974609375 = 0.05944889411330223 + 100.0 * 9.026803016662598
Epoch 2700, val loss: 0.6038426160812378
Epoch 2710, training loss: 902.782958984375 = 0.05872047320008278 + 100.0 * 9.027242660522461
Epoch 2710, val loss: 0.6046902537345886
Epoch 2720, training loss: 902.6021728515625 = 0.05801558122038841 + 100.0 * 9.02544116973877
Epoch 2720, val loss: 0.6076897978782654
Epoch 2730, training loss: 902.5791015625 = 0.05732329562306404 + 100.0 * 9.02521800994873
Epoch 2730, val loss: 0.6102479100227356
Epoch 2740, training loss: 902.593994140625 = 0.05665544420480728 + 100.0 * 9.025373458862305
Epoch 2740, val loss: 0.6123453974723816
Epoch 2750, training loss: 902.74658203125 = 0.055970583111047745 + 100.0 * 9.02690601348877
Epoch 2750, val loss: 0.614139974117279
Epoch 2760, training loss: 902.5933227539062 = 0.05531890690326691 + 100.0 * 9.02538013458252
Epoch 2760, val loss: 0.6148115992546082
Epoch 2770, training loss: 902.5763549804688 = 0.0546305812895298 + 100.0 * 9.025217056274414
Epoch 2770, val loss: 0.6173477172851562
Epoch 2780, training loss: 902.7276611328125 = 0.05401517450809479 + 100.0 * 9.02673625946045
Epoch 2780, val loss: 0.6184093952178955
Epoch 2790, training loss: 902.6939697265625 = 0.05336334928870201 + 100.0 * 9.026406288146973
Epoch 2790, val loss: 0.6222518682479858
Epoch 2800, training loss: 902.594970703125 = 0.05272126570343971 + 100.0 * 9.025422096252441
Epoch 2800, val loss: 0.6236618161201477
Epoch 2810, training loss: 902.5213012695312 = 0.05207138508558273 + 100.0 * 9.02469253540039
Epoch 2810, val loss: 0.6263588666915894
Epoch 2820, training loss: 902.4857788085938 = 0.051428914070129395 + 100.0 * 9.024343490600586
Epoch 2820, val loss: 0.6280466318130493
Epoch 2830, training loss: 902.5636596679688 = 0.05083678662776947 + 100.0 * 9.025128364562988
Epoch 2830, val loss: 0.6299414038658142
Epoch 2840, training loss: 902.4702758789062 = 0.05024072900414467 + 100.0 * 9.024200439453125
Epoch 2840, val loss: 0.6314855217933655
Epoch 2850, training loss: 902.4703369140625 = 0.049607135355472565 + 100.0 * 9.02420711517334
Epoch 2850, val loss: 0.633737325668335
Epoch 2860, training loss: 902.5910034179688 = 0.049032457172870636 + 100.0 * 9.025420188903809
Epoch 2860, val loss: 0.6352993845939636
Epoch 2870, training loss: 902.4008178710938 = 0.048426635563373566 + 100.0 * 9.023524284362793
Epoch 2870, val loss: 0.6381544470787048
Epoch 2880, training loss: 902.4650268554688 = 0.04787847772240639 + 100.0 * 9.024171829223633
Epoch 2880, val loss: 0.6385543346405029
Epoch 2890, training loss: 902.5553588867188 = 0.047363243997097015 + 100.0 * 9.025079727172852
Epoch 2890, val loss: 0.6405442953109741
Epoch 2900, training loss: 902.4366455078125 = 0.04674286022782326 + 100.0 * 9.02389907836914
Epoch 2900, val loss: 0.6437211036682129
Epoch 2910, training loss: 902.377197265625 = 0.046176403760910034 + 100.0 * 9.023309707641602
Epoch 2910, val loss: 0.6472107768058777
Epoch 2920, training loss: 902.3475952148438 = 0.045622311532497406 + 100.0 * 9.023019790649414
Epoch 2920, val loss: 0.6481019854545593
Epoch 2930, training loss: 902.3666381835938 = 0.04509308189153671 + 100.0 * 9.023215293884277
Epoch 2930, val loss: 0.6511821150779724
Epoch 2940, training loss: 902.605712890625 = 0.04459045082330704 + 100.0 * 9.02561092376709
Epoch 2940, val loss: 0.6531206965446472
Epoch 2950, training loss: 902.4242553710938 = 0.044059932231903076 + 100.0 * 9.023801803588867
Epoch 2950, val loss: 0.6555903553962708
Epoch 2960, training loss: 902.3644409179688 = 0.043553318828344345 + 100.0 * 9.023208618164062
Epoch 2960, val loss: 0.6576547622680664
Epoch 2970, training loss: 902.5145263671875 = 0.043067507445812225 + 100.0 * 9.024714469909668
Epoch 2970, val loss: 0.6586410403251648
Epoch 2980, training loss: 902.4276123046875 = 0.04255268722772598 + 100.0 * 9.023850440979004
Epoch 2980, val loss: 0.6612868309020996
Epoch 2990, training loss: 902.3840942382812 = 0.04206844046711922 + 100.0 * 9.023420333862305
Epoch 2990, val loss: 0.6635017991065979
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8468
Overall ASR: 0.6846
Flip ASR: 0.6049/1554 nodes
The final ASR:0.68239, 0.00863, Accuracy:0.84373, 0.00249
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97510])
remove edge: torch.Size([2, 79930])
updated graph: torch.Size([2, 88792])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.7003
Flip ASR: 0.6293/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8508
Overall ASR: 0.7622
Flip ASR: 0.7040/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72921, 0.02542, Accuracy:0.85033, 0.00041
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.0118408203125 = 1.1002635955810547 + 100.0 * 10.359115600585938
Epoch 0, val loss: 1.0998790264129639
Epoch 10, training loss: 1036.7437744140625 = 1.0891026258468628 + 100.0 * 10.356546401977539
Epoch 10, val loss: 1.08855402469635
Epoch 20, training loss: 1032.772216796875 = 1.0747569799423218 + 100.0 * 10.316975593566895
Epoch 20, val loss: 1.0744142532348633
Epoch 30, training loss: 1002.1528930664062 = 1.0641108751296997 + 100.0 * 10.01088809967041
Epoch 30, val loss: 1.0643110275268555
Epoch 40, training loss: 968.753173828125 = 1.053546667098999 + 100.0 * 9.676996231079102
Epoch 40, val loss: 1.053156852722168
Epoch 50, training loss: 949.1416625976562 = 1.0348843336105347 + 100.0 * 9.481067657470703
Epoch 50, val loss: 1.0338937044143677
Epoch 60, training loss: 945.39404296875 = 1.0135486125946045 + 100.0 * 9.443804740905762
Epoch 60, val loss: 1.0129053592681885
Epoch 70, training loss: 942.9688110351562 = 0.9920347332954407 + 100.0 * 9.419768333435059
Epoch 70, val loss: 0.991678774356842
Epoch 80, training loss: 939.9749755859375 = 0.9701030254364014 + 100.0 * 9.39004898071289
Epoch 80, val loss: 0.9701497554779053
Epoch 90, training loss: 936.3753051757812 = 0.9479653239250183 + 100.0 * 9.354273796081543
Epoch 90, val loss: 0.9483842849731445
Epoch 100, training loss: 932.7578125 = 0.9227323532104492 + 100.0 * 9.318350791931152
Epoch 100, val loss: 0.9234442114830017
Epoch 110, training loss: 930.634521484375 = 0.8924103379249573 + 100.0 * 9.2974214553833
Epoch 110, val loss: 0.8939810991287231
Epoch 120, training loss: 927.4673461914062 = 0.8572629690170288 + 100.0 * 9.266100883483887
Epoch 120, val loss: 0.8597463965415955
Epoch 130, training loss: 924.5933227539062 = 0.820880115032196 + 100.0 * 9.237724304199219
Epoch 130, val loss: 0.8249033689498901
Epoch 140, training loss: 922.9170532226562 = 0.7832319140434265 + 100.0 * 9.221338272094727
Epoch 140, val loss: 0.7887470722198486
Epoch 150, training loss: 921.4373168945312 = 0.7430104613304138 + 100.0 * 9.20694351196289
Epoch 150, val loss: 0.7504338026046753
Epoch 160, training loss: 920.4508056640625 = 0.7025598883628845 + 100.0 * 9.197482109069824
Epoch 160, val loss: 0.7126680016517639
Epoch 170, training loss: 919.4307250976562 = 0.6638173460960388 + 100.0 * 9.187668800354004
Epoch 170, val loss: 0.6772751808166504
Epoch 180, training loss: 918.6151733398438 = 0.6281022429466248 + 100.0 * 9.17987060546875
Epoch 180, val loss: 0.6449575424194336
Epoch 190, training loss: 917.7817993164062 = 0.5952739715576172 + 100.0 * 9.171865463256836
Epoch 190, val loss: 0.6158428192138672
Epoch 200, training loss: 917.2122192382812 = 0.5656408071517944 + 100.0 * 9.166465759277344
Epoch 200, val loss: 0.5901534557342529
Epoch 210, training loss: 916.6881103515625 = 0.5393603444099426 + 100.0 * 9.161487579345703
Epoch 210, val loss: 0.5678308606147766
Epoch 220, training loss: 916.2483520507812 = 0.5167022347450256 + 100.0 * 9.157316207885742
Epoch 220, val loss: 0.5492621064186096
Epoch 230, training loss: 916.0452880859375 = 0.4972739517688751 + 100.0 * 9.15548038482666
Epoch 230, val loss: 0.533971905708313
Epoch 240, training loss: 915.4829711914062 = 0.4805353581905365 + 100.0 * 9.1500244140625
Epoch 240, val loss: 0.5211677551269531
Epoch 250, training loss: 915.1182250976562 = 0.4663490056991577 + 100.0 * 9.14651870727539
Epoch 250, val loss: 0.5106929540634155
Epoch 260, training loss: 914.9368286132812 = 0.4541577994823456 + 100.0 * 9.144826889038086
Epoch 260, val loss: 0.5021367073059082
Epoch 270, training loss: 914.572998046875 = 0.4433547556400299 + 100.0 * 9.14129638671875
Epoch 270, val loss: 0.4949192404747009
Epoch 280, training loss: 914.2987060546875 = 0.4339284896850586 + 100.0 * 9.13864803314209
Epoch 280, val loss: 0.4888719320297241
Epoch 290, training loss: 914.0498657226562 = 0.42562806606292725 + 100.0 * 9.136242866516113
Epoch 290, val loss: 0.48380905389785767
Epoch 300, training loss: 913.82373046875 = 0.418227881193161 + 100.0 * 9.134055137634277
Epoch 300, val loss: 0.47943079471588135
Epoch 310, training loss: 913.9490356445312 = 0.4115441143512726 + 100.0 * 9.135375022888184
Epoch 310, val loss: 0.47573330998420715
Epoch 320, training loss: 913.4862670898438 = 0.405396044254303 + 100.0 * 9.13080883026123
Epoch 320, val loss: 0.47223830223083496
Epoch 330, training loss: 913.2095336914062 = 0.39988794922828674 + 100.0 * 9.128096580505371
Epoch 330, val loss: 0.46932944655418396
Epoch 340, training loss: 913.021484375 = 0.39483556151390076 + 100.0 * 9.126266479492188
Epoch 340, val loss: 0.46662163734436035
Epoch 350, training loss: 912.80322265625 = 0.39017361402511597 + 100.0 * 9.124130249023438
Epoch 350, val loss: 0.46427610516548157
Epoch 360, training loss: 913.3072509765625 = 0.385829895734787 + 100.0 * 9.1292142868042
Epoch 360, val loss: 0.4621342122554779
Epoch 370, training loss: 912.7012939453125 = 0.3815403878688812 + 100.0 * 9.123197555541992
Epoch 370, val loss: 0.45989763736724854
Epoch 380, training loss: 912.3333129882812 = 0.3776730000972748 + 100.0 * 9.119556427001953
Epoch 380, val loss: 0.4579547345638275
Epoch 390, training loss: 912.1256713867188 = 0.3741037845611572 + 100.0 * 9.117515563964844
Epoch 390, val loss: 0.456414133310318
Epoch 400, training loss: 911.9431762695312 = 0.37071752548217773 + 100.0 * 9.115724563598633
Epoch 400, val loss: 0.4548977017402649
Epoch 410, training loss: 912.0723266601562 = 0.3674921989440918 + 100.0 * 9.117048263549805
Epoch 410, val loss: 0.4537099301815033
Epoch 420, training loss: 911.7888793945312 = 0.3642490804195404 + 100.0 * 9.114246368408203
Epoch 420, val loss: 0.45219358801841736
Epoch 430, training loss: 911.5026245117188 = 0.36121106147766113 + 100.0 * 9.111413955688477
Epoch 430, val loss: 0.45062658190727234
Epoch 440, training loss: 911.3397827148438 = 0.3583574593067169 + 100.0 * 9.109814643859863
Epoch 440, val loss: 0.44942256808280945
Epoch 450, training loss: 911.2027587890625 = 0.35560205578804016 + 100.0 * 9.108471870422363
Epoch 450, val loss: 0.44816792011260986
Epoch 460, training loss: 911.135498046875 = 0.3528834581375122 + 100.0 * 9.107826232910156
Epoch 460, val loss: 0.44707196950912476
Epoch 470, training loss: 911.0952758789062 = 0.35022059082984924 + 100.0 * 9.107450485229492
Epoch 470, val loss: 0.4459053575992584
Epoch 480, training loss: 910.8547973632812 = 0.347601056098938 + 100.0 * 9.105072021484375
Epoch 480, val loss: 0.44484421610832214
Epoch 490, training loss: 910.7322387695312 = 0.3451019823551178 + 100.0 * 9.10387134552002
Epoch 490, val loss: 0.4437415301799774
Epoch 500, training loss: 910.6017456054688 = 0.34268730878829956 + 100.0 * 9.102590560913086
Epoch 500, val loss: 0.4429054260253906
Epoch 510, training loss: 910.5343627929688 = 0.3403435945510864 + 100.0 * 9.101940155029297
Epoch 510, val loss: 0.44199666380882263
Epoch 520, training loss: 910.55859375 = 0.3379734754562378 + 100.0 * 9.102206230163574
Epoch 520, val loss: 0.4412284791469574
Epoch 530, training loss: 910.3175048828125 = 0.3356681764125824 + 100.0 * 9.099818229675293
Epoch 530, val loss: 0.4401094317436218
Epoch 540, training loss: 910.2203369140625 = 0.33347392082214355 + 100.0 * 9.098868370056152
Epoch 540, val loss: 0.43929585814476013
Epoch 550, training loss: 910.1792602539062 = 0.33134037256240845 + 100.0 * 9.098479270935059
Epoch 550, val loss: 0.4384107291698456
Epoch 560, training loss: 910.0286865234375 = 0.32919445633888245 + 100.0 * 9.096994400024414
Epoch 560, val loss: 0.43768510222435
Epoch 570, training loss: 910.0143432617188 = 0.3271101117134094 + 100.0 * 9.096872329711914
Epoch 570, val loss: 0.4367063641548157
Epoch 580, training loss: 909.8593139648438 = 0.325088769197464 + 100.0 * 9.095342636108398
Epoch 580, val loss: 0.43599680066108704
Epoch 590, training loss: 909.8201904296875 = 0.32311728596687317 + 100.0 * 9.094970703125
Epoch 590, val loss: 0.4354265332221985
Epoch 600, training loss: 909.7349243164062 = 0.3211196959018707 + 100.0 * 9.094138145446777
Epoch 600, val loss: 0.4343339204788208
Epoch 610, training loss: 909.6519165039062 = 0.3191620111465454 + 100.0 * 9.093327522277832
Epoch 610, val loss: 0.4337102472782135
Epoch 620, training loss: 909.61279296875 = 0.3172673285007477 + 100.0 * 9.092955589294434
Epoch 620, val loss: 0.4332335293292999
Epoch 630, training loss: 909.5589599609375 = 0.3153783082962036 + 100.0 * 9.092435836791992
Epoch 630, val loss: 0.43247368931770325
Epoch 640, training loss: 909.39794921875 = 0.3135257661342621 + 100.0 * 9.09084415435791
Epoch 640, val loss: 0.43178293108940125
Epoch 650, training loss: 909.3374633789062 = 0.3117346465587616 + 100.0 * 9.09025764465332
Epoch 650, val loss: 0.4311563968658447
Epoch 660, training loss: 909.2682495117188 = 0.3099668323993683 + 100.0 * 9.089583396911621
Epoch 660, val loss: 0.43058469891548157
Epoch 670, training loss: 909.456298828125 = 0.30819204449653625 + 100.0 * 9.09148120880127
Epoch 670, val loss: 0.4301273822784424
Epoch 680, training loss: 909.2518310546875 = 0.30640625953674316 + 100.0 * 9.089454650878906
Epoch 680, val loss: 0.42923372983932495
Epoch 690, training loss: 909.0617065429688 = 0.3046847879886627 + 100.0 * 9.087570190429688
Epoch 690, val loss: 0.4290350079536438
Epoch 700, training loss: 908.976318359375 = 0.303011417388916 + 100.0 * 9.086732864379883
Epoch 700, val loss: 0.42867910861968994
Epoch 710, training loss: 909.3209228515625 = 0.3013537526130676 + 100.0 * 9.090195655822754
Epoch 710, val loss: 0.42851483821868896
Epoch 720, training loss: 909.0335693359375 = 0.2996262013912201 + 100.0 * 9.087339401245117
Epoch 720, val loss: 0.42785704135894775
Epoch 730, training loss: 908.8732299804688 = 0.2979714870452881 + 100.0 * 9.085752487182617
Epoch 730, val loss: 0.4275909960269928
Epoch 740, training loss: 908.7509765625 = 0.2963641583919525 + 100.0 * 9.084546089172363
Epoch 740, val loss: 0.42723551392555237
Epoch 750, training loss: 908.7032470703125 = 0.29478615522384644 + 100.0 * 9.084084510803223
Epoch 750, val loss: 0.426922082901001
Epoch 760, training loss: 908.8232421875 = 0.293194979429245 + 100.0 * 9.08530044555664
Epoch 760, val loss: 0.42679858207702637
Epoch 770, training loss: 908.6342163085938 = 0.291581928730011 + 100.0 * 9.083426475524902
Epoch 770, val loss: 0.4263657033443451
Epoch 780, training loss: 908.5269165039062 = 0.2900193929672241 + 100.0 * 9.082368850708008
Epoch 780, val loss: 0.4263957440853119
Epoch 790, training loss: 908.487548828125 = 0.2884902060031891 + 100.0 * 9.081990242004395
Epoch 790, val loss: 0.42605069279670715
Epoch 800, training loss: 908.832275390625 = 0.28697848320007324 + 100.0 * 9.085453033447266
Epoch 800, val loss: 0.4257877767086029
Epoch 810, training loss: 908.5054321289062 = 0.28540363907814026 + 100.0 * 9.082200050354004
Epoch 810, val loss: 0.42587608098983765
Epoch 820, training loss: 908.3571166992188 = 0.28388282656669617 + 100.0 * 9.080732345581055
Epoch 820, val loss: 0.42590808868408203
Epoch 830, training loss: 908.29443359375 = 0.28238824009895325 + 100.0 * 9.080120086669922
Epoch 830, val loss: 0.4256529211997986
Epoch 840, training loss: 908.34619140625 = 0.280908465385437 + 100.0 * 9.080653190612793
Epoch 840, val loss: 0.42551493644714355
Epoch 850, training loss: 908.2363891601562 = 0.27939170598983765 + 100.0 * 9.079569816589355
Epoch 850, val loss: 0.42564329504966736
Epoch 860, training loss: 908.23486328125 = 0.27789199352264404 + 100.0 * 9.079569816589355
Epoch 860, val loss: 0.42562299966812134
Epoch 870, training loss: 908.1317749023438 = 0.27640482783317566 + 100.0 * 9.078553199768066
Epoch 870, val loss: 0.4255622625350952
Epoch 880, training loss: 908.0850219726562 = 0.27493157982826233 + 100.0 * 9.07810115814209
Epoch 880, val loss: 0.42558225989341736
Epoch 890, training loss: 908.0718994140625 = 0.2734529674053192 + 100.0 * 9.077984809875488
Epoch 890, val loss: 0.4256450831890106
Epoch 900, training loss: 908.1322631835938 = 0.2719593644142151 + 100.0 * 9.07860279083252
Epoch 900, val loss: 0.42568838596343994
Epoch 910, training loss: 908.0604248046875 = 0.2704399824142456 + 100.0 * 9.077899932861328
Epoch 910, val loss: 0.42583224177360535
Epoch 920, training loss: 907.9776611328125 = 0.2689362168312073 + 100.0 * 9.07708740234375
Epoch 920, val loss: 0.42574894428253174
Epoch 930, training loss: 907.8812866210938 = 0.26743650436401367 + 100.0 * 9.076138496398926
Epoch 930, val loss: 0.4258607029914856
Epoch 940, training loss: 907.9642333984375 = 0.2659609615802765 + 100.0 * 9.076982498168945
Epoch 940, val loss: 0.4259040057659149
Epoch 950, training loss: 907.8226318359375 = 0.2644394636154175 + 100.0 * 9.075581550598145
Epoch 950, val loss: 0.42633894085884094
Epoch 960, training loss: 907.7594604492188 = 0.2629391551017761 + 100.0 * 9.074965476989746
Epoch 960, val loss: 0.42633306980133057
Epoch 970, training loss: 907.758056640625 = 0.2614416778087616 + 100.0 * 9.074966430664062
Epoch 970, val loss: 0.4262774586677551
Epoch 980, training loss: 907.8319091796875 = 0.25992175936698914 + 100.0 * 9.075719833374023
Epoch 980, val loss: 0.42654117941856384
Epoch 990, training loss: 907.7200317382812 = 0.25838279724121094 + 100.0 * 9.074616432189941
Epoch 990, val loss: 0.4269343316555023
Epoch 1000, training loss: 907.6283569335938 = 0.2568516731262207 + 100.0 * 9.073715209960938
Epoch 1000, val loss: 0.4270131289958954
Epoch 1010, training loss: 907.6236572265625 = 0.2553258240222931 + 100.0 * 9.073683738708496
Epoch 1010, val loss: 0.4272606670856476
Epoch 1020, training loss: 907.53564453125 = 0.2537924647331238 + 100.0 * 9.072818756103516
Epoch 1020, val loss: 0.4272453784942627
Epoch 1030, training loss: 907.8466796875 = 0.2522646188735962 + 100.0 * 9.075943946838379
Epoch 1030, val loss: 0.4274093210697174
Epoch 1040, training loss: 907.5582275390625 = 0.2506607472896576 + 100.0 * 9.073075294494629
Epoch 1040, val loss: 0.427924245595932
Epoch 1050, training loss: 907.3966674804688 = 0.2490788698196411 + 100.0 * 9.071475982666016
Epoch 1050, val loss: 0.42809629440307617
Epoch 1060, training loss: 907.380126953125 = 0.24751734733581543 + 100.0 * 9.07132625579834
Epoch 1060, val loss: 0.4282945990562439
Epoch 1070, training loss: 907.470703125 = 0.24594636261463165 + 100.0 * 9.072247505187988
Epoch 1070, val loss: 0.4285110533237457
Epoch 1080, training loss: 907.672607421875 = 0.24434512853622437 + 100.0 * 9.0742826461792
Epoch 1080, val loss: 0.4290068447589874
Epoch 1090, training loss: 907.3954467773438 = 0.24270203709602356 + 100.0 * 9.071527481079102
Epoch 1090, val loss: 0.4291093051433563
Epoch 1100, training loss: 907.2392578125 = 0.24108457565307617 + 100.0 * 9.069981575012207
Epoch 1100, val loss: 0.42938432097435
Epoch 1110, training loss: 907.1845703125 = 0.23948058485984802 + 100.0 * 9.069450378417969
Epoch 1110, val loss: 0.42974725365638733
Epoch 1120, training loss: 907.1461791992188 = 0.23786260187625885 + 100.0 * 9.069083213806152
Epoch 1120, val loss: 0.430051326751709
Epoch 1130, training loss: 907.8635864257812 = 0.23625244200229645 + 100.0 * 9.076272964477539
Epoch 1130, val loss: 0.4303985834121704
Epoch 1140, training loss: 907.110107421875 = 0.2345247119665146 + 100.0 * 9.068756103515625
Epoch 1140, val loss: 0.4304558038711548
Epoch 1150, training loss: 907.0780639648438 = 0.23284442722797394 + 100.0 * 9.068451881408691
Epoch 1150, val loss: 0.43111100792884827
Epoch 1160, training loss: 907.0277709960938 = 0.23118452727794647 + 100.0 * 9.067965507507324
Epoch 1160, val loss: 0.43147751688957214
Epoch 1170, training loss: 906.99267578125 = 0.22952152788639069 + 100.0 * 9.067631721496582
Epoch 1170, val loss: 0.43172329664230347
Epoch 1180, training loss: 907.1044921875 = 0.22784827649593353 + 100.0 * 9.068766593933105
Epoch 1180, val loss: 0.43239885568618774
Epoch 1190, training loss: 906.9370727539062 = 0.22612468898296356 + 100.0 * 9.067109107971191
Epoch 1190, val loss: 0.43234384059906006
Epoch 1200, training loss: 906.953369140625 = 0.2244141846895218 + 100.0 * 9.067289352416992
Epoch 1200, val loss: 0.4330120086669922
Epoch 1210, training loss: 907.0369262695312 = 0.22273193299770355 + 100.0 * 9.06814193725586
Epoch 1210, val loss: 0.43312689661979675
Epoch 1220, training loss: 906.9567260742188 = 0.22098857164382935 + 100.0 * 9.067357063293457
Epoch 1220, val loss: 0.4337049722671509
Epoch 1230, training loss: 906.8649291992188 = 0.21924260258674622 + 100.0 * 9.06645679473877
Epoch 1230, val loss: 0.43449002504348755
Epoch 1240, training loss: 906.8008422851562 = 0.21750345826148987 + 100.0 * 9.06583309173584
Epoch 1240, val loss: 0.4353008270263672
Epoch 1250, training loss: 906.7548217773438 = 0.21576717495918274 + 100.0 * 9.065390586853027
Epoch 1250, val loss: 0.43588852882385254
Epoch 1260, training loss: 906.7360229492188 = 0.21402430534362793 + 100.0 * 9.06521987915039
Epoch 1260, val loss: 0.4367113709449768
Epoch 1270, training loss: 906.836669921875 = 0.21226927638053894 + 100.0 * 9.066244125366211
Epoch 1270, val loss: 0.4374708831310272
Epoch 1280, training loss: 906.7260131835938 = 0.21048298478126526 + 100.0 * 9.065155029296875
Epoch 1280, val loss: 0.4375133216381073
Epoch 1290, training loss: 906.779052734375 = 0.208721324801445 + 100.0 * 9.065703392028809
Epoch 1290, val loss: 0.4388757050037384
Epoch 1300, training loss: 906.723388671875 = 0.2069174200296402 + 100.0 * 9.065164566040039
Epoch 1300, val loss: 0.438990980386734
Epoch 1310, training loss: 906.7489624023438 = 0.20513369143009186 + 100.0 * 9.065438270568848
Epoch 1310, val loss: 0.4395925998687744
Epoch 1320, training loss: 906.5801391601562 = 0.2033105045557022 + 100.0 * 9.06376838684082
Epoch 1320, val loss: 0.440669447183609
Epoch 1330, training loss: 906.50732421875 = 0.2015145868062973 + 100.0 * 9.063057899475098
Epoch 1330, val loss: 0.44160187244415283
Epoch 1340, training loss: 906.5131225585938 = 0.19971002638339996 + 100.0 * 9.06313419342041
Epoch 1340, val loss: 0.4425394535064697
Epoch 1350, training loss: 906.7035522460938 = 0.1979115754365921 + 100.0 * 9.065055847167969
Epoch 1350, val loss: 0.4437007009983063
Epoch 1360, training loss: 906.655517578125 = 0.19608159363269806 + 100.0 * 9.064594268798828
Epoch 1360, val loss: 0.44354352355003357
Epoch 1370, training loss: 906.404052734375 = 0.1942128688097 + 100.0 * 9.062098503112793
Epoch 1370, val loss: 0.44518429040908813
Epoch 1380, training loss: 906.3579711914062 = 0.19237880408763885 + 100.0 * 9.06165599822998
Epoch 1380, val loss: 0.4460093379020691
Epoch 1390, training loss: 906.402099609375 = 0.19055236876010895 + 100.0 * 9.062115669250488
Epoch 1390, val loss: 0.44694840908050537
Epoch 1400, training loss: 906.369873046875 = 0.18870042264461517 + 100.0 * 9.061811447143555
Epoch 1400, val loss: 0.44790616631507874
Epoch 1410, training loss: 906.2985229492188 = 0.18685291707515717 + 100.0 * 9.061117172241211
Epoch 1410, val loss: 0.44885390996932983
Epoch 1420, training loss: 906.4041137695312 = 0.18502697348594666 + 100.0 * 9.062191009521484
Epoch 1420, val loss: 0.44961220026016235
Epoch 1430, training loss: 906.2230834960938 = 0.18313045799732208 + 100.0 * 9.060400009155273
Epoch 1430, val loss: 0.45108938217163086
Epoch 1440, training loss: 906.227783203125 = 0.1812903881072998 + 100.0 * 9.060464859008789
Epoch 1440, val loss: 0.45234808325767517
Epoch 1450, training loss: 906.519775390625 = 0.17946472764015198 + 100.0 * 9.063403129577637
Epoch 1450, val loss: 0.4533345401287079
Epoch 1460, training loss: 906.3011474609375 = 0.1775878369808197 + 100.0 * 9.061235427856445
Epoch 1460, val loss: 0.4539804458618164
Epoch 1470, training loss: 906.1668090820312 = 0.17570661008358002 + 100.0 * 9.059910774230957
Epoch 1470, val loss: 0.45550867915153503
Epoch 1480, training loss: 906.0856323242188 = 0.17384861409664154 + 100.0 * 9.059118270874023
Epoch 1480, val loss: 0.4567410945892334
Epoch 1490, training loss: 906.247802734375 = 0.1720293164253235 + 100.0 * 9.060757637023926
Epoch 1490, val loss: 0.45817267894744873
Epoch 1500, training loss: 906.1130981445312 = 0.17014411091804504 + 100.0 * 9.059429168701172
Epoch 1500, val loss: 0.45859426259994507
Epoch 1510, training loss: 906.0902709960938 = 0.16831064224243164 + 100.0 * 9.059219360351562
Epoch 1510, val loss: 0.46010705828666687
Epoch 1520, training loss: 905.9995727539062 = 0.16642768681049347 + 100.0 * 9.058331489562988
Epoch 1520, val loss: 0.4614500105381012
Epoch 1530, training loss: 906.0699462890625 = 0.16458846628665924 + 100.0 * 9.059053421020508
Epoch 1530, val loss: 0.4626537561416626
Epoch 1540, training loss: 906.01806640625 = 0.16271808743476868 + 100.0 * 9.058553695678711
Epoch 1540, val loss: 0.46430841088294983
Epoch 1550, training loss: 906.0369262695312 = 0.16090478003025055 + 100.0 * 9.058760643005371
Epoch 1550, val loss: 0.4661739766597748
Epoch 1560, training loss: 905.9371337890625 = 0.15900999307632446 + 100.0 * 9.057781219482422
Epoch 1560, val loss: 0.4669099748134613
Epoch 1570, training loss: 905.911376953125 = 0.15716886520385742 + 100.0 * 9.057541847229004
Epoch 1570, val loss: 0.4681991636753082
Epoch 1580, training loss: 905.96728515625 = 0.1553245186805725 + 100.0 * 9.058119773864746
Epoch 1580, val loss: 0.47009506821632385
Epoch 1590, training loss: 905.855224609375 = 0.1534755527973175 + 100.0 * 9.05701732635498
Epoch 1590, val loss: 0.4715738892555237
Epoch 1600, training loss: 905.9008178710938 = 0.15165723860263824 + 100.0 * 9.057491302490234
Epoch 1600, val loss: 0.4729616641998291
Epoch 1610, training loss: 906.0165405273438 = 0.14981824159622192 + 100.0 * 9.058667182922363
Epoch 1610, val loss: 0.4738366901874542
Epoch 1620, training loss: 905.8296508789062 = 0.14797912538051605 + 100.0 * 9.056817054748535
Epoch 1620, val loss: 0.4756697416305542
Epoch 1630, training loss: 905.7525634765625 = 0.1461331695318222 + 100.0 * 9.05606460571289
Epoch 1630, val loss: 0.4774192273616791
Epoch 1640, training loss: 905.8602905273438 = 0.1443280726671219 + 100.0 * 9.057159423828125
Epoch 1640, val loss: 0.4784492552280426
Epoch 1650, training loss: 905.7132568359375 = 0.1424843817949295 + 100.0 * 9.055707931518555
Epoch 1650, val loss: 0.4801974594593048
Epoch 1660, training loss: 905.7359619140625 = 0.1406659334897995 + 100.0 * 9.055953025817871
Epoch 1660, val loss: 0.4815026819705963
Epoch 1670, training loss: 905.7484741210938 = 0.13883718848228455 + 100.0 * 9.056096076965332
Epoch 1670, val loss: 0.48386019468307495
Epoch 1680, training loss: 905.71142578125 = 0.1370381861925125 + 100.0 * 9.055744171142578
Epoch 1680, val loss: 0.4860946536064148
Epoch 1690, training loss: 905.7346801757812 = 0.13523192703723907 + 100.0 * 9.055994033813477
Epoch 1690, val loss: 0.48754268884658813
Epoch 1700, training loss: 905.6129760742188 = 0.1334470510482788 + 100.0 * 9.054795265197754
Epoch 1700, val loss: 0.48862791061401367
Epoch 1710, training loss: 905.6051635742188 = 0.13165338337421417 + 100.0 * 9.05473518371582
Epoch 1710, val loss: 0.4911384582519531
Epoch 1720, training loss: 905.7042236328125 = 0.12988914549350739 + 100.0 * 9.055743217468262
Epoch 1720, val loss: 0.4923655092716217
Epoch 1730, training loss: 905.6638793945312 = 0.12812693417072296 + 100.0 * 9.055357933044434
Epoch 1730, val loss: 0.49538540840148926
Epoch 1740, training loss: 905.575927734375 = 0.1263452023267746 + 100.0 * 9.054495811462402
Epoch 1740, val loss: 0.4960693120956421
Epoch 1750, training loss: 905.5055541992188 = 0.1245967298746109 + 100.0 * 9.053810119628906
Epoch 1750, val loss: 0.49833813309669495
Epoch 1760, training loss: 905.4517211914062 = 0.12284782528877258 + 100.0 * 9.053288459777832
Epoch 1760, val loss: 0.5004891157150269
Epoch 1770, training loss: 905.5244140625 = 0.1211572140455246 + 100.0 * 9.054032325744629
Epoch 1770, val loss: 0.5024116635322571
Epoch 1780, training loss: 905.5059204101562 = 0.11945755034685135 + 100.0 * 9.053864479064941
Epoch 1780, val loss: 0.5041003227233887
Epoch 1790, training loss: 905.4352416992188 = 0.11772226542234421 + 100.0 * 9.05317497253418
Epoch 1790, val loss: 0.5065121054649353
Epoch 1800, training loss: 905.3906860351562 = 0.11602882295846939 + 100.0 * 9.052746772766113
Epoch 1800, val loss: 0.5086756944656372
Epoch 1810, training loss: 905.47509765625 = 0.11437491327524185 + 100.0 * 9.053606986999512
Epoch 1810, val loss: 0.5103922486305237
Epoch 1820, training loss: 905.4442749023438 = 0.11269980669021606 + 100.0 * 9.053316116333008
Epoch 1820, val loss: 0.513117253780365
Epoch 1830, training loss: 905.3590087890625 = 0.11104937642812729 + 100.0 * 9.05247974395752
Epoch 1830, val loss: 0.5156459212303162
Epoch 1840, training loss: 905.2913818359375 = 0.10940322279930115 + 100.0 * 9.051819801330566
Epoch 1840, val loss: 0.5174264907836914
Epoch 1850, training loss: 905.2568359375 = 0.10778620094060898 + 100.0 * 9.051490783691406
Epoch 1850, val loss: 0.5198267698287964
Epoch 1860, training loss: 905.5442504882812 = 0.10621843487024307 + 100.0 * 9.054380416870117
Epoch 1860, val loss: 0.5228895545005798
Epoch 1870, training loss: 905.3220825195312 = 0.10461482405662537 + 100.0 * 9.05217456817627
Epoch 1870, val loss: 0.5240630507469177
Epoch 1880, training loss: 905.2998657226562 = 0.1030222699046135 + 100.0 * 9.051968574523926
Epoch 1880, val loss: 0.5266740322113037
Epoch 1890, training loss: 905.220947265625 = 0.10145136713981628 + 100.0 * 9.05119514465332
Epoch 1890, val loss: 0.5288233160972595
Epoch 1900, training loss: 905.1532592773438 = 0.099892757833004 + 100.0 * 9.050533294677734
Epoch 1900, val loss: 0.5313021540641785
Epoch 1910, training loss: 905.1454467773438 = 0.09836668521165848 + 100.0 * 9.050470352172852
Epoch 1910, val loss: 0.5338400602340698
Epoch 1920, training loss: 905.3821411132812 = 0.09688600152730942 + 100.0 * 9.052852630615234
Epoch 1920, val loss: 0.5357413291931152
Epoch 1930, training loss: 905.3673706054688 = 0.09547973424196243 + 100.0 * 9.052719116210938
Epoch 1930, val loss: 0.5380799174308777
Epoch 1940, training loss: 905.164794921875 = 0.09392189979553223 + 100.0 * 9.050708770751953
Epoch 1940, val loss: 0.5416708588600159
Epoch 1950, training loss: 905.1419677734375 = 0.09244261682033539 + 100.0 * 9.050495147705078
Epoch 1950, val loss: 0.5439476370811462
Epoch 1960, training loss: 905.0967407226562 = 0.09101469814777374 + 100.0 * 9.050057411193848
Epoch 1960, val loss: 0.546646237373352
Epoch 1970, training loss: 905.2299194335938 = 0.08961596339941025 + 100.0 * 9.051403045654297
Epoch 1970, val loss: 0.549250066280365
Epoch 1980, training loss: 905.0571899414062 = 0.08818039298057556 + 100.0 * 9.049690246582031
Epoch 1980, val loss: 0.5521380305290222
Epoch 1990, training loss: 905.0140380859375 = 0.08679146319627762 + 100.0 * 9.049272537231445
Epoch 1990, val loss: 0.5545464754104614
Epoch 2000, training loss: 904.9546508789062 = 0.08541268855333328 + 100.0 * 9.04869270324707
Epoch 2000, val loss: 0.5569928884506226
Epoch 2010, training loss: 904.934326171875 = 0.08406331390142441 + 100.0 * 9.048502922058105
Epoch 2010, val loss: 0.5595635771751404
Epoch 2020, training loss: 905.4781494140625 = 0.08283934742212296 + 100.0 * 9.053953170776367
Epoch 2020, val loss: 0.5618582367897034
Epoch 2030, training loss: 905.01513671875 = 0.08144988864660263 + 100.0 * 9.049337387084961
Epoch 2030, val loss: 0.5650671720504761
Epoch 2040, training loss: 904.9275512695312 = 0.0801420584321022 + 100.0 * 9.048474311828613
Epoch 2040, val loss: 0.5675100088119507
Epoch 2050, training loss: 905.0647583007812 = 0.07890690863132477 + 100.0 * 9.049858093261719
Epoch 2050, val loss: 0.5697781443595886
Epoch 2060, training loss: 904.9581298828125 = 0.07762613892555237 + 100.0 * 9.048805236816406
Epoch 2060, val loss: 0.5740609765052795
Epoch 2070, training loss: 904.8406372070312 = 0.07637646049261093 + 100.0 * 9.047642707824707
Epoch 2070, val loss: 0.5758658647537231
Epoch 2080, training loss: 904.8082885742188 = 0.07515910267829895 + 100.0 * 9.047331809997559
Epoch 2080, val loss: 0.5783647894859314
Epoch 2090, training loss: 904.9573364257812 = 0.07401518523693085 + 100.0 * 9.048832893371582
Epoch 2090, val loss: 0.5811066031455994
Epoch 2100, training loss: 904.804931640625 = 0.07281138002872467 + 100.0 * 9.047321319580078
Epoch 2100, val loss: 0.584296703338623
Epoch 2110, training loss: 904.8185424804688 = 0.07166098058223724 + 100.0 * 9.047469139099121
Epoch 2110, val loss: 0.5873205065727234
Epoch 2120, training loss: 904.9173583984375 = 0.07059883326292038 + 100.0 * 9.048467636108398
Epoch 2120, val loss: 0.5901703238487244
Epoch 2130, training loss: 904.897216796875 = 0.06946581602096558 + 100.0 * 9.048277854919434
Epoch 2130, val loss: 0.5925403833389282
Epoch 2140, training loss: 904.7646484375 = 0.06832253932952881 + 100.0 * 9.046963691711426
Epoch 2140, val loss: 0.5954532027244568
Epoch 2150, training loss: 904.7060546875 = 0.06722898036241531 + 100.0 * 9.046388626098633
Epoch 2150, val loss: 0.5982732176780701
Epoch 2160, training loss: 904.672607421875 = 0.06616964191198349 + 100.0 * 9.046064376831055
Epoch 2160, val loss: 0.6012865304946899
Epoch 2170, training loss: 904.7138061523438 = 0.06513050943613052 + 100.0 * 9.046486854553223
Epoch 2170, val loss: 0.6042216420173645
Epoch 2180, training loss: 904.822021484375 = 0.0641392171382904 + 100.0 * 9.047578811645508
Epoch 2180, val loss: 0.6072291135787964
Epoch 2190, training loss: 904.7908325195312 = 0.0631205216050148 + 100.0 * 9.047277450561523
Epoch 2190, val loss: 0.6093887090682983
Epoch 2200, training loss: 904.6770629882812 = 0.06209636107087135 + 100.0 * 9.046150207519531
Epoch 2200, val loss: 0.6124540567398071
Epoch 2210, training loss: 904.6376953125 = 0.06110887601971626 + 100.0 * 9.04576587677002
Epoch 2210, val loss: 0.615290105342865
Epoch 2220, training loss: 904.9614868164062 = 0.06018289178609848 + 100.0 * 9.049013137817383
Epoch 2220, val loss: 0.6178755164146423
Epoch 2230, training loss: 904.7275390625 = 0.05921230837702751 + 100.0 * 9.046683311462402
Epoch 2230, val loss: 0.6199692487716675
Epoch 2240, training loss: 904.5828247070312 = 0.0582495778799057 + 100.0 * 9.045246124267578
Epoch 2240, val loss: 0.6234896779060364
Epoch 2250, training loss: 904.5233764648438 = 0.057319577783346176 + 100.0 * 9.044660568237305
Epoch 2250, val loss: 0.6261799931526184
Epoch 2260, training loss: 904.52734375 = 0.056418709456920624 + 100.0 * 9.044709205627441
Epoch 2260, val loss: 0.6296629309654236
Epoch 2270, training loss: 904.9302368164062 = 0.05559086427092552 + 100.0 * 9.048746109008789
Epoch 2270, val loss: 0.6332497596740723
Epoch 2280, training loss: 904.6080322265625 = 0.05466613918542862 + 100.0 * 9.045533180236816
Epoch 2280, val loss: 0.6338638663291931
Epoch 2290, training loss: 904.4841918945312 = 0.05377258360385895 + 100.0 * 9.044303894042969
Epoch 2290, val loss: 0.6376680135726929
Epoch 2300, training loss: 904.4796752929688 = 0.0529230460524559 + 100.0 * 9.044267654418945
Epoch 2300, val loss: 0.6398757100105286
Epoch 2310, training loss: 904.8375854492188 = 0.05212263762950897 + 100.0 * 9.04785442352295
Epoch 2310, val loss: 0.6429889798164368
Epoch 2320, training loss: 904.466796875 = 0.051258061081171036 + 100.0 * 9.04415512084961
Epoch 2320, val loss: 0.6460781693458557
Epoch 2330, training loss: 904.453125 = 0.050437718629837036 + 100.0 * 9.044027328491211
Epoch 2330, val loss: 0.6484841704368591
Epoch 2340, training loss: 904.3797607421875 = 0.04963850975036621 + 100.0 * 9.043301582336426
Epoch 2340, val loss: 0.6512722969055176
Epoch 2350, training loss: 904.4635620117188 = 0.04889474809169769 + 100.0 * 9.044146537780762
Epoch 2350, val loss: 0.653768002986908
Epoch 2360, training loss: 904.5383911132812 = 0.04810801148414612 + 100.0 * 9.044902801513672
Epoch 2360, val loss: 0.6569817662239075
Epoch 2370, training loss: 904.40234375 = 0.04734796658158302 + 100.0 * 9.043549537658691
Epoch 2370, val loss: 0.6598779559135437
Epoch 2380, training loss: 904.3614501953125 = 0.046593740582466125 + 100.0 * 9.0431489944458
Epoch 2380, val loss: 0.6629489064216614
Epoch 2390, training loss: 904.4002075195312 = 0.04588302597403526 + 100.0 * 9.043542861938477
Epoch 2390, val loss: 0.6654917597770691
Epoch 2400, training loss: 904.4468383789062 = 0.04515895992517471 + 100.0 * 9.04401683807373
Epoch 2400, val loss: 0.6684914231300354
Epoch 2410, training loss: 904.3492431640625 = 0.04444430023431778 + 100.0 * 9.043047904968262
Epoch 2410, val loss: 0.6712145805358887
Epoch 2420, training loss: 904.3474731445312 = 0.043754857033491135 + 100.0 * 9.043037414550781
Epoch 2420, val loss: 0.6742887496948242
Epoch 2430, training loss: 904.517822265625 = 0.043091174215078354 + 100.0 * 9.044747352600098
Epoch 2430, val loss: 0.6776392459869385
Epoch 2440, training loss: 904.3267822265625 = 0.04239894077181816 + 100.0 * 9.04284381866455
Epoch 2440, val loss: 0.679720401763916
Epoch 2450, training loss: 904.30712890625 = 0.04174208641052246 + 100.0 * 9.042654037475586
Epoch 2450, val loss: 0.6829028129577637
Epoch 2460, training loss: 904.4520874023438 = 0.04110885411500931 + 100.0 * 9.044109344482422
Epoch 2460, val loss: 0.6858418583869934
Epoch 2470, training loss: 904.3092041015625 = 0.040475066751241684 + 100.0 * 9.04268741607666
Epoch 2470, val loss: 0.6882287263870239
Epoch 2480, training loss: 904.2030639648438 = 0.03980725631117821 + 100.0 * 9.041632652282715
Epoch 2480, val loss: 0.6909431219100952
Epoch 2490, training loss: 904.1868896484375 = 0.03919557109475136 + 100.0 * 9.04147720336914
Epoch 2490, val loss: 0.6934577822685242
Epoch 2500, training loss: 904.4939575195312 = 0.038695745170116425 + 100.0 * 9.04455280303955
Epoch 2500, val loss: 0.6958428621292114
Epoch 2510, training loss: 904.3165283203125 = 0.03807630389928818 + 100.0 * 9.042784690856934
Epoch 2510, val loss: 0.6993186473846436
Epoch 2520, training loss: 904.3364868164062 = 0.037434130907058716 + 100.0 * 9.042990684509277
Epoch 2520, val loss: 0.7012421488761902
Epoch 2530, training loss: 904.1638793945312 = 0.03683539107441902 + 100.0 * 9.04127025604248
Epoch 2530, val loss: 0.7048478126525879
Epoch 2540, training loss: 904.1320190429688 = 0.03627421706914902 + 100.0 * 9.0409574508667
Epoch 2540, val loss: 0.7074054479598999
Epoch 2550, training loss: 904.2265625 = 0.03573717176914215 + 100.0 * 9.041908264160156
Epoch 2550, val loss: 0.7095456123352051
Epoch 2560, training loss: 904.2462768554688 = 0.03519531711935997 + 100.0 * 9.042110443115234
Epoch 2560, val loss: 0.7128154635429382
Epoch 2570, training loss: 904.2637329101562 = 0.03464900702238083 + 100.0 * 9.042290687561035
Epoch 2570, val loss: 0.71567302942276
Epoch 2580, training loss: 904.1077270507812 = 0.0341319814324379 + 100.0 * 9.040736198425293
Epoch 2580, val loss: 0.7194508910179138
Epoch 2590, training loss: 904.0399780273438 = 0.03358229994773865 + 100.0 * 9.040063858032227
Epoch 2590, val loss: 0.7212668061256409
Epoch 2600, training loss: 904.076171875 = 0.033087100833654404 + 100.0 * 9.040431022644043
Epoch 2600, val loss: 0.7241204977035522
Epoch 2610, training loss: 904.297607421875 = 0.03266002610325813 + 100.0 * 9.042649269104004
Epoch 2610, val loss: 0.7273046374320984
Epoch 2620, training loss: 904.1321411132812 = 0.032100848853588104 + 100.0 * 9.041000366210938
Epoch 2620, val loss: 0.7301289439201355
Epoch 2630, training loss: 904.0917358398438 = 0.03168224170804024 + 100.0 * 9.040600776672363
Epoch 2630, val loss: 0.7326158881187439
Epoch 2640, training loss: 904.3809204101562 = 0.031170664355158806 + 100.0 * 9.043497085571289
Epoch 2640, val loss: 0.7358988523483276
Epoch 2650, training loss: 904.0521850585938 = 0.03068736381828785 + 100.0 * 9.040214538574219
Epoch 2650, val loss: 0.7373949885368347
Epoch 2660, training loss: 903.9739379882812 = 0.030216731131076813 + 100.0 * 9.039437294006348
Epoch 2660, val loss: 0.7403676509857178
Epoch 2670, training loss: 903.9392700195312 = 0.02976195700466633 + 100.0 * 9.039094924926758
Epoch 2670, val loss: 0.7429412603378296
Epoch 2680, training loss: 903.9822387695312 = 0.029335282742977142 + 100.0 * 9.039528846740723
Epoch 2680, val loss: 0.7462968230247498
Epoch 2690, training loss: 904.2640991210938 = 0.02898305281996727 + 100.0 * 9.042350769042969
Epoch 2690, val loss: 0.7494790554046631
Epoch 2700, training loss: 903.9682006835938 = 0.028477802872657776 + 100.0 * 9.039397239685059
Epoch 2700, val loss: 0.7500059008598328
Epoch 2710, training loss: 903.9075317382812 = 0.028046496212482452 + 100.0 * 9.03879451751709
Epoch 2710, val loss: 0.7529504299163818
Epoch 2720, training loss: 904.0054931640625 = 0.027655726298689842 + 100.0 * 9.039778709411621
Epoch 2720, val loss: 0.7565818428993225
Epoch 2730, training loss: 903.955322265625 = 0.027244675904512405 + 100.0 * 9.039280891418457
Epoch 2730, val loss: 0.7579287886619568
Epoch 2740, training loss: 903.9137573242188 = 0.026837950572371483 + 100.0 * 9.03886890411377
Epoch 2740, val loss: 0.7606520056724548
Epoch 2750, training loss: 904.04296875 = 0.02646518312394619 + 100.0 * 9.040164947509766
Epoch 2750, val loss: 0.7628560066223145
Epoch 2760, training loss: 903.880126953125 = 0.02607046067714691 + 100.0 * 9.038540840148926
Epoch 2760, val loss: 0.7656126618385315
Epoch 2770, training loss: 903.8447875976562 = 0.02568555437028408 + 100.0 * 9.038190841674805
Epoch 2770, val loss: 0.7684158086776733
Epoch 2780, training loss: 903.9051513671875 = 0.025328442454338074 + 100.0 * 9.038798332214355
Epoch 2780, val loss: 0.7704872488975525
Epoch 2790, training loss: 903.9047241210938 = 0.024967724457383156 + 100.0 * 9.038797378540039
Epoch 2790, val loss: 0.772930920124054
Epoch 2800, training loss: 904.1029052734375 = 0.02461874671280384 + 100.0 * 9.040782928466797
Epoch 2800, val loss: 0.7765302062034607
Epoch 2810, training loss: 903.8709716796875 = 0.024253137409687042 + 100.0 * 9.038467407226562
Epoch 2810, val loss: 0.7787961363792419
Epoch 2820, training loss: 903.7952270507812 = 0.023904217407107353 + 100.0 * 9.037713050842285
Epoch 2820, val loss: 0.7807004451751709
Epoch 2830, training loss: 903.7494506835938 = 0.02355518937110901 + 100.0 * 9.037259101867676
Epoch 2830, val loss: 0.7832813262939453
Epoch 2840, training loss: 903.7469482421875 = 0.023229198530316353 + 100.0 * 9.037237167358398
Epoch 2840, val loss: 0.7861805558204651
Epoch 2850, training loss: 904.0008544921875 = 0.02298996038734913 + 100.0 * 9.039778709411621
Epoch 2850, val loss: 0.7901507019996643
Epoch 2860, training loss: 903.943115234375 = 0.0226137712597847 + 100.0 * 9.039204597473145
Epoch 2860, val loss: 0.7888432741165161
Epoch 2870, training loss: 903.8609619140625 = 0.022310830652713776 + 100.0 * 9.038386344909668
Epoch 2870, val loss: 0.7934727668762207
Epoch 2880, training loss: 903.7381591796875 = 0.02195872738957405 + 100.0 * 9.037161827087402
Epoch 2880, val loss: 0.7958672046661377
Epoch 2890, training loss: 903.8040161132812 = 0.021673034876585007 + 100.0 * 9.037823677062988
Epoch 2890, val loss: 0.7976570129394531
Epoch 2900, training loss: 903.761474609375 = 0.021360211074352264 + 100.0 * 9.03740119934082
Epoch 2900, val loss: 0.7997092604637146
Epoch 2910, training loss: 903.807373046875 = 0.02108960784971714 + 100.0 * 9.037862777709961
Epoch 2910, val loss: 0.8016644716262817
Epoch 2920, training loss: 903.7621459960938 = 0.02078593336045742 + 100.0 * 9.037413597106934
Epoch 2920, val loss: 0.8050690293312073
Epoch 2930, training loss: 903.7133178710938 = 0.0205027237534523 + 100.0 * 9.036928176879883
Epoch 2930, val loss: 0.8069413304328918
Epoch 2940, training loss: 903.7364501953125 = 0.020264100283384323 + 100.0 * 9.037161827087402
Epoch 2940, val loss: 0.8098574280738831
Epoch 2950, training loss: 903.6712036132812 = 0.019957810640335083 + 100.0 * 9.03651237487793
Epoch 2950, val loss: 0.8123984336853027
Epoch 2960, training loss: 903.6860961914062 = 0.019713113084435463 + 100.0 * 9.036664009094238
Epoch 2960, val loss: 0.8139938712120056
Epoch 2970, training loss: 903.8251953125 = 0.019441155716776848 + 100.0 * 9.038057327270508
Epoch 2970, val loss: 0.8158506751060486
Epoch 2980, training loss: 903.7288818359375 = 0.019180964678525925 + 100.0 * 9.037096977233887
Epoch 2980, val loss: 0.8191253542900085
Epoch 2990, training loss: 903.6965942382812 = 0.018922895193099976 + 100.0 * 9.036776542663574
Epoch 2990, val loss: 0.8207281827926636
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8524
Overall ASR: 0.7196
Flip ASR: 0.6499/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1037.011962890625 = 1.0992021560668945 + 100.0 * 10.359127044677734
Epoch 0, val loss: 1.0975950956344604
Epoch 10, training loss: 1036.728759765625 = 1.088929533958435 + 100.0 * 10.35639762878418
Epoch 10, val loss: 1.087058186531067
Epoch 20, training loss: 1032.108642578125 = 1.0746705532073975 + 100.0 * 10.31033992767334
Epoch 20, val loss: 1.0727771520614624
Epoch 30, training loss: 1001.2617797851562 = 1.0615651607513428 + 100.0 * 10.002001762390137
Epoch 30, val loss: 1.0601061582565308
Epoch 40, training loss: 967.4097900390625 = 1.0496654510498047 + 100.0 * 9.66360092163086
Epoch 40, val loss: 1.0480207204818726
Epoch 50, training loss: 949.6237182617188 = 1.0345007181167603 + 100.0 * 9.485892295837402
Epoch 50, val loss: 1.032684564590454
Epoch 60, training loss: 944.1615600585938 = 1.016822099685669 + 100.0 * 9.43144702911377
Epoch 60, val loss: 1.0153714418411255
Epoch 70, training loss: 941.0623168945312 = 0.9986274838447571 + 100.0 * 9.400636672973633
Epoch 70, val loss: 0.9976935386657715
Epoch 80, training loss: 936.398681640625 = 0.9815011024475098 + 100.0 * 9.354171752929688
Epoch 80, val loss: 0.9810699224472046
Epoch 90, training loss: 931.5607299804688 = 0.9644860029220581 + 100.0 * 9.305962562561035
Epoch 90, val loss: 0.9642603993415833
Epoch 100, training loss: 928.04736328125 = 0.9426078796386719 + 100.0 * 9.271047592163086
Epoch 100, val loss: 0.9424680471420288
Epoch 110, training loss: 925.5057983398438 = 0.9150131344795227 + 100.0 * 9.2459077835083
Epoch 110, val loss: 0.915397584438324
Epoch 120, training loss: 923.5648803710938 = 0.8830046057701111 + 100.0 * 9.226819038391113
Epoch 120, val loss: 0.8841481804847717
Epoch 130, training loss: 922.13623046875 = 0.8474962115287781 + 100.0 * 9.21288776397705
Epoch 130, val loss: 0.8497772216796875
Epoch 140, training loss: 921.1453857421875 = 0.8101118803024292 + 100.0 * 9.203352928161621
Epoch 140, val loss: 0.8139320611953735
Epoch 150, training loss: 920.0581665039062 = 0.7724652886390686 + 100.0 * 9.192856788635254
Epoch 150, val loss: 0.7782774567604065
Epoch 160, training loss: 919.1256713867188 = 0.7357833385467529 + 100.0 * 9.18389892578125
Epoch 160, val loss: 0.7439256310462952
Epoch 170, training loss: 918.5504760742188 = 0.7000511884689331 + 100.0 * 9.17850399017334
Epoch 170, val loss: 0.7107300162315369
Epoch 180, training loss: 917.8109130859375 = 0.6657485961914062 + 100.0 * 9.171451568603516
Epoch 180, val loss: 0.6791704297065735
Epoch 190, training loss: 917.3281860351562 = 0.6339061856269836 + 100.0 * 9.166942596435547
Epoch 190, val loss: 0.6501532793045044
Epoch 200, training loss: 916.9014892578125 = 0.6049923300743103 + 100.0 * 9.162964820861816
Epoch 200, val loss: 0.6242896914482117
Epoch 210, training loss: 916.53076171875 = 0.5791983604431152 + 100.0 * 9.159515380859375
Epoch 210, val loss: 0.6015005707740784
Epoch 220, training loss: 916.3524169921875 = 0.5565407276153564 + 100.0 * 9.157958984375
Epoch 220, val loss: 0.5820914506912231
Epoch 230, training loss: 915.9442138671875 = 0.5369319915771484 + 100.0 * 9.154072761535645
Epoch 230, val loss: 0.5655972957611084
Epoch 240, training loss: 915.5408935546875 = 0.5203132033348083 + 100.0 * 9.150205612182617
Epoch 240, val loss: 0.5520280599594116
Epoch 250, training loss: 915.3251342773438 = 0.5061192512512207 + 100.0 * 9.14819049835205
Epoch 250, val loss: 0.5408797264099121
Epoch 260, training loss: 914.9649047851562 = 0.4938299357891083 + 100.0 * 9.144710540771484
Epoch 260, val loss: 0.5315118432044983
Epoch 270, training loss: 914.6342163085938 = 0.4833458662033081 + 100.0 * 9.141509056091309
Epoch 270, val loss: 0.5240824222564697
Epoch 280, training loss: 914.4028930664062 = 0.4741356670856476 + 100.0 * 9.139287948608398
Epoch 280, val loss: 0.5174458622932434
Epoch 290, training loss: 914.1751098632812 = 0.46605226397514343 + 100.0 * 9.137090682983398
Epoch 290, val loss: 0.5121858716011047
Epoch 300, training loss: 913.864501953125 = 0.45909494161605835 + 100.0 * 9.134054183959961
Epoch 300, val loss: 0.507860541343689
Epoch 310, training loss: 913.6375122070312 = 0.4529425799846649 + 100.0 * 9.131845474243164
Epoch 310, val loss: 0.5041597485542297
Epoch 320, training loss: 913.779541015625 = 0.44724124670028687 + 100.0 * 9.133322715759277
Epoch 320, val loss: 0.5010143518447876
Epoch 330, training loss: 913.261474609375 = 0.4420209527015686 + 100.0 * 9.128194808959961
Epoch 330, val loss: 0.4981464445590973
Epoch 340, training loss: 913.052978515625 = 0.4374772608280182 + 100.0 * 9.126154899597168
Epoch 340, val loss: 0.4957098662853241
Epoch 350, training loss: 912.8445434570312 = 0.43334758281707764 + 100.0 * 9.124112129211426
Epoch 350, val loss: 0.4936642646789551
Epoch 360, training loss: 912.6791381835938 = 0.42950597405433655 + 100.0 * 9.122496604919434
Epoch 360, val loss: 0.49175262451171875
Epoch 370, training loss: 912.53271484375 = 0.4258311688899994 + 100.0 * 9.121068954467773
Epoch 370, val loss: 0.4899800717830658
Epoch 380, training loss: 912.4138793945312 = 0.42238444089889526 + 100.0 * 9.119915008544922
Epoch 380, val loss: 0.4885140657424927
Epoch 390, training loss: 912.2747802734375 = 0.4191664159297943 + 100.0 * 9.118556022644043
Epoch 390, val loss: 0.4871824085712433
Epoch 400, training loss: 912.1729736328125 = 0.4160861670970917 + 100.0 * 9.117568969726562
Epoch 400, val loss: 0.48575079441070557
Epoch 410, training loss: 911.8884887695312 = 0.413175106048584 + 100.0 * 9.114753723144531
Epoch 410, val loss: 0.4845472574234009
Epoch 420, training loss: 911.74755859375 = 0.41044673323631287 + 100.0 * 9.113370895385742
Epoch 420, val loss: 0.48352086544036865
Epoch 430, training loss: 912.0780029296875 = 0.40780460834503174 + 100.0 * 9.11670207977295
Epoch 430, val loss: 0.48209214210510254
Epoch 440, training loss: 911.5 = 0.4051215946674347 + 100.0 * 9.11094856262207
Epoch 440, val loss: 0.48117610812187195
Epoch 450, training loss: 911.38037109375 = 0.4026554822921753 + 100.0 * 9.109777450561523
Epoch 450, val loss: 0.48035505414009094
Epoch 460, training loss: 911.4281005859375 = 0.40024858713150024 + 100.0 * 9.110278129577637
Epoch 460, val loss: 0.47959357500076294
Epoch 470, training loss: 911.1835327148438 = 0.3978594243526459 + 100.0 * 9.107856750488281
Epoch 470, val loss: 0.47866111993789673
Epoch 480, training loss: 911.0245361328125 = 0.3956405222415924 + 100.0 * 9.10628890991211
Epoch 480, val loss: 0.4779335558414459
Epoch 490, training loss: 910.9197387695312 = 0.3934924304485321 + 100.0 * 9.105262756347656
Epoch 490, val loss: 0.47744816541671753
Epoch 500, training loss: 910.8938598632812 = 0.39126285910606384 + 100.0 * 9.105026245117188
Epoch 500, val loss: 0.4762520492076874
Epoch 510, training loss: 910.7203979492188 = 0.38901403546333313 + 100.0 * 9.103313446044922
Epoch 510, val loss: 0.47551071643829346
Epoch 520, training loss: 910.6578369140625 = 0.38698047399520874 + 100.0 * 9.10270881652832
Epoch 520, val loss: 0.47490814328193665
Epoch 530, training loss: 910.5552978515625 = 0.3850173056125641 + 100.0 * 9.101702690124512
Epoch 530, val loss: 0.4744172692298889
Epoch 540, training loss: 910.4735107421875 = 0.38306477665901184 + 100.0 * 9.10090446472168
Epoch 540, val loss: 0.4738515615463257
Epoch 550, training loss: 910.5430297851562 = 0.38103818893432617 + 100.0 * 9.101619720458984
Epoch 550, val loss: 0.4733330011367798
Epoch 560, training loss: 910.455322265625 = 0.37898948788642883 + 100.0 * 9.100763320922852
Epoch 560, val loss: 0.4724311828613281
Epoch 570, training loss: 910.2713623046875 = 0.3770798146724701 + 100.0 * 9.098942756652832
Epoch 570, val loss: 0.47195497155189514
Epoch 580, training loss: 910.1593627929688 = 0.37522944808006287 + 100.0 * 9.097841262817383
Epoch 580, val loss: 0.47149768471717834
Epoch 590, training loss: 910.4044799804688 = 0.373362272977829 + 100.0 * 9.100311279296875
Epoch 590, val loss: 0.47107142210006714
Epoch 600, training loss: 910.2882080078125 = 0.37136223912239075 + 100.0 * 9.09916877746582
Epoch 600, val loss: 0.47032594680786133
Epoch 610, training loss: 910.0204467773438 = 0.369452565908432 + 100.0 * 9.09650993347168
Epoch 610, val loss: 0.46963268518447876
Epoch 620, training loss: 909.8939819335938 = 0.36762845516204834 + 100.0 * 9.095263481140137
Epoch 620, val loss: 0.4693741202354431
Epoch 630, training loss: 909.8777465820312 = 0.36581334471702576 + 100.0 * 9.09511947631836
Epoch 630, val loss: 0.46896108984947205
Epoch 640, training loss: 909.8518676757812 = 0.36393260955810547 + 100.0 * 9.094879150390625
Epoch 640, val loss: 0.4682139754295349
Epoch 650, training loss: 909.7276000976562 = 0.3620448708534241 + 100.0 * 9.093655586242676
Epoch 650, val loss: 0.4676063656806946
Epoch 660, training loss: 909.6651611328125 = 0.36018502712249756 + 100.0 * 9.093050003051758
Epoch 660, val loss: 0.4672453701496124
Epoch 670, training loss: 909.74560546875 = 0.35828477144241333 + 100.0 * 9.093873023986816
Epoch 670, val loss: 0.46664443612098694
Epoch 680, training loss: 909.5697021484375 = 0.35635262727737427 + 100.0 * 9.092133522033691
Epoch 680, val loss: 0.4660194218158722
Epoch 690, training loss: 909.4666748046875 = 0.35451072454452515 + 100.0 * 9.091121673583984
Epoch 690, val loss: 0.4653113782405853
Epoch 700, training loss: 909.437255859375 = 0.352682888507843 + 100.0 * 9.090846061706543
Epoch 700, val loss: 0.46501845121383667
Epoch 710, training loss: 909.516357421875 = 0.3507798910140991 + 100.0 * 9.091655731201172
Epoch 710, val loss: 0.46455129981040955
Epoch 720, training loss: 909.3519897460938 = 0.348832368850708 + 100.0 * 9.090031623840332
Epoch 720, val loss: 0.4642297029495239
Epoch 730, training loss: 909.33251953125 = 0.34692347049713135 + 100.0 * 9.089856147766113
Epoch 730, val loss: 0.4633333683013916
Epoch 740, training loss: 909.217041015625 = 0.3450227975845337 + 100.0 * 9.088720321655273
Epoch 740, val loss: 0.4627548158168793
Epoch 750, training loss: 909.1372680664062 = 0.34312576055526733 + 100.0 * 9.08794116973877
Epoch 750, val loss: 0.46242234110832214
Epoch 760, training loss: 909.2193603515625 = 0.34122198820114136 + 100.0 * 9.088781356811523
Epoch 760, val loss: 0.46215564012527466
Epoch 770, training loss: 909.1033325195312 = 0.33924898505210876 + 100.0 * 9.087640762329102
Epoch 770, val loss: 0.46109122037887573
Epoch 780, training loss: 909.0210571289062 = 0.33725985884666443 + 100.0 * 9.086837768554688
Epoch 780, val loss: 0.4612036943435669
Epoch 790, training loss: 909.1505126953125 = 0.33530712127685547 + 100.0 * 9.088151931762695
Epoch 790, val loss: 0.4607566297054291
Epoch 800, training loss: 909.0131225585938 = 0.33325421810150146 + 100.0 * 9.086798667907715
Epoch 800, val loss: 0.45958858728408813
Epoch 810, training loss: 908.9107055664062 = 0.3312433063983917 + 100.0 * 9.085794448852539
Epoch 810, val loss: 0.45971348881721497
Epoch 820, training loss: 908.7882690429688 = 0.32927989959716797 + 100.0 * 9.084589958190918
Epoch 820, val loss: 0.4591677486896515
Epoch 830, training loss: 909.0266723632812 = 0.3273046910762787 + 100.0 * 9.086993217468262
Epoch 830, val loss: 0.4591420590877533
Epoch 840, training loss: 908.7974243164062 = 0.32520392537117004 + 100.0 * 9.084722518920898
Epoch 840, val loss: 0.4575751721858978
Epoch 850, training loss: 908.6790771484375 = 0.3231515884399414 + 100.0 * 9.083559036254883
Epoch 850, val loss: 0.4577987790107727
Epoch 860, training loss: 908.6254272460938 = 0.32113316655158997 + 100.0 * 9.083043098449707
Epoch 860, val loss: 0.4569671154022217
Epoch 870, training loss: 908.8695068359375 = 0.3190566301345825 + 100.0 * 9.085504531860352
Epoch 870, val loss: 0.456638902425766
Epoch 880, training loss: 908.6259765625 = 0.31692445278167725 + 100.0 * 9.083090782165527
Epoch 880, val loss: 0.4563736319541931
Epoch 890, training loss: 908.6600341796875 = 0.31482911109924316 + 100.0 * 9.083452224731445
Epoch 890, val loss: 0.45553576946258545
Epoch 900, training loss: 908.4978637695312 = 0.3126978874206543 + 100.0 * 9.081851959228516
Epoch 900, val loss: 0.45545247197151184
Epoch 910, training loss: 908.5371704101562 = 0.3106076717376709 + 100.0 * 9.082265853881836
Epoch 910, val loss: 0.4545742869377136
Epoch 920, training loss: 908.43115234375 = 0.30842721462249756 + 100.0 * 9.08122730255127
Epoch 920, val loss: 0.45496925711631775
Epoch 930, training loss: 908.324462890625 = 0.30628833174705505 + 100.0 * 9.080182075500488
Epoch 930, val loss: 0.45414867997169495
Epoch 940, training loss: 908.437744140625 = 0.3041330873966217 + 100.0 * 9.08133602142334
Epoch 940, val loss: 0.4542236328125
Epoch 950, training loss: 908.2890625 = 0.3019357919692993 + 100.0 * 9.07987117767334
Epoch 950, val loss: 0.4534158408641815
Epoch 960, training loss: 908.2374877929688 = 0.29974111914634705 + 100.0 * 9.079377174377441
Epoch 960, val loss: 0.45318523049354553
Epoch 970, training loss: 908.326416015625 = 0.29755914211273193 + 100.0 * 9.080288887023926
Epoch 970, val loss: 0.4524538218975067
Epoch 980, training loss: 908.2500610351562 = 0.2952785789966583 + 100.0 * 9.079547882080078
Epoch 980, val loss: 0.4529831111431122
Epoch 990, training loss: 908.131103515625 = 0.2930556833744049 + 100.0 * 9.078380584716797
Epoch 990, val loss: 0.45199641585350037
Epoch 1000, training loss: 908.2308349609375 = 0.29080283641815186 + 100.0 * 9.079400062561035
Epoch 1000, val loss: 0.4520385265350342
Epoch 1010, training loss: 908.0618286132812 = 0.28853142261505127 + 100.0 * 9.077733039855957
Epoch 1010, val loss: 0.45171549916267395
Epoch 1020, training loss: 908.0383911132812 = 0.286251038312912 + 100.0 * 9.077521324157715
Epoch 1020, val loss: 0.45185235142707825
Epoch 1030, training loss: 907.9579467773438 = 0.2839774489402771 + 100.0 * 9.076739311218262
Epoch 1030, val loss: 0.45133939385414124
Epoch 1040, training loss: 908.1085815429688 = 0.2816915512084961 + 100.0 * 9.078269004821777
Epoch 1040, val loss: 0.45159414410591125
Epoch 1050, training loss: 907.9288940429688 = 0.279371052980423 + 100.0 * 9.076495170593262
Epoch 1050, val loss: 0.45071616768836975
Epoch 1060, training loss: 907.8871459960938 = 0.27706459164619446 + 100.0 * 9.076101303100586
Epoch 1060, val loss: 0.4510177969932556
Epoch 1070, training loss: 908.0128173828125 = 0.27474308013916016 + 100.0 * 9.077381134033203
Epoch 1070, val loss: 0.45069703459739685
Epoch 1080, training loss: 907.8417358398438 = 0.2723848819732666 + 100.0 * 9.075693130493164
Epoch 1080, val loss: 0.4510815143585205
Epoch 1090, training loss: 907.842041015625 = 0.27008557319641113 + 100.0 * 9.075719833374023
Epoch 1090, val loss: 0.45164424180984497
Epoch 1100, training loss: 907.7282104492188 = 0.2676960527896881 + 100.0 * 9.074604988098145
Epoch 1100, val loss: 0.450796514749527
Epoch 1110, training loss: 907.7227172851562 = 0.2653791904449463 + 100.0 * 9.074573516845703
Epoch 1110, val loss: 0.45036816596984863
Epoch 1120, training loss: 907.8153076171875 = 0.2630394995212555 + 100.0 * 9.075522422790527
Epoch 1120, val loss: 0.450989693403244
Epoch 1130, training loss: 907.6588745117188 = 0.2606850564479828 + 100.0 * 9.073982238769531
Epoch 1130, val loss: 0.45090392231941223
Epoch 1140, training loss: 907.7560424804688 = 0.25836053490638733 + 100.0 * 9.074976921081543
Epoch 1140, val loss: 0.4506877064704895
Epoch 1150, training loss: 907.5807495117188 = 0.2560019791126251 + 100.0 * 9.073247909545898
Epoch 1150, val loss: 0.45123860239982605
Epoch 1160, training loss: 907.5344848632812 = 0.2536731958389282 + 100.0 * 9.072808265686035
Epoch 1160, val loss: 0.4509980380535126
Epoch 1170, training loss: 907.5274658203125 = 0.25135716795921326 + 100.0 * 9.072761535644531
Epoch 1170, val loss: 0.4522473216056824
Epoch 1180, training loss: 907.5111083984375 = 0.2489907145500183 + 100.0 * 9.07262134552002
Epoch 1180, val loss: 0.45174965262413025
Epoch 1190, training loss: 907.5518188476562 = 0.24664346873760223 + 100.0 * 9.073051452636719
Epoch 1190, val loss: 0.45173847675323486
Epoch 1200, training loss: 907.4740600585938 = 0.2442793846130371 + 100.0 * 9.072298049926758
Epoch 1200, val loss: 0.4523116648197174
Epoch 1210, training loss: 907.4464111328125 = 0.24194137752056122 + 100.0 * 9.072044372558594
Epoch 1210, val loss: 0.45251551270484924
Epoch 1220, training loss: 907.3504638671875 = 0.2395869940519333 + 100.0 * 9.0711088180542
Epoch 1220, val loss: 0.45361706614494324
Epoch 1230, training loss: 907.4283447265625 = 0.23724275827407837 + 100.0 * 9.071910858154297
Epoch 1230, val loss: 0.45326438546180725
Epoch 1240, training loss: 907.3529663085938 = 0.23489271104335785 + 100.0 * 9.071181297302246
Epoch 1240, val loss: 0.4535980820655823
Epoch 1250, training loss: 907.2993774414062 = 0.2325318455696106 + 100.0 * 9.07066822052002
Epoch 1250, val loss: 0.4540862739086151
Epoch 1260, training loss: 907.2822265625 = 0.2301846146583557 + 100.0 * 9.070520401000977
Epoch 1260, val loss: 0.4546627700328827
Epoch 1270, training loss: 907.2003784179688 = 0.22783152759075165 + 100.0 * 9.069725036621094
Epoch 1270, val loss: 0.4555199444293976
Epoch 1280, training loss: 907.2374877929688 = 0.2254967987537384 + 100.0 * 9.070119857788086
Epoch 1280, val loss: 0.45629337430000305
Epoch 1290, training loss: 907.1304931640625 = 0.22314050793647766 + 100.0 * 9.069073677062988
Epoch 1290, val loss: 0.45648056268692017
Epoch 1300, training loss: 907.2168579101562 = 0.22079740464687347 + 100.0 * 9.069960594177246
Epoch 1300, val loss: 0.45779746770858765
Epoch 1310, training loss: 907.1859130859375 = 0.2184523493051529 + 100.0 * 9.069674491882324
Epoch 1310, val loss: 0.4590950608253479
Epoch 1320, training loss: 907.0716552734375 = 0.2160588949918747 + 100.0 * 9.06855583190918
Epoch 1320, val loss: 0.4587726891040802
Epoch 1330, training loss: 907.0691528320312 = 0.21372780203819275 + 100.0 * 9.068553924560547
Epoch 1330, val loss: 0.4600532650947571
Epoch 1340, training loss: 907.0877075195312 = 0.211390420794487 + 100.0 * 9.068763732910156
Epoch 1340, val loss: 0.4608430564403534
Epoch 1350, training loss: 907.0239868164062 = 0.2090495526790619 + 100.0 * 9.06814956665039
Epoch 1350, val loss: 0.46132785081863403
Epoch 1360, training loss: 906.9180908203125 = 0.20671938359737396 + 100.0 * 9.067113876342773
Epoch 1360, val loss: 0.4617759883403778
Epoch 1370, training loss: 906.9096069335938 = 0.20439894497394562 + 100.0 * 9.067051887512207
Epoch 1370, val loss: 0.4624786674976349
Epoch 1380, training loss: 907.1032104492188 = 0.20214414596557617 + 100.0 * 9.069010734558105
Epoch 1380, val loss: 0.46262019872665405
Epoch 1390, training loss: 906.9801025390625 = 0.19976338744163513 + 100.0 * 9.067803382873535
Epoch 1390, val loss: 0.46529996395111084
Epoch 1400, training loss: 906.9324951171875 = 0.19746722280979156 + 100.0 * 9.067350387573242
Epoch 1400, val loss: 0.46531006693840027
Epoch 1410, training loss: 906.8078002929688 = 0.19517366588115692 + 100.0 * 9.066125869750977
Epoch 1410, val loss: 0.467290997505188
Epoch 1420, training loss: 906.8178100585938 = 0.19292093813419342 + 100.0 * 9.066248893737793
Epoch 1420, val loss: 0.4689641296863556
Epoch 1430, training loss: 906.802734375 = 0.19072145223617554 + 100.0 * 9.066120147705078
Epoch 1430, val loss: 0.47072741389274597
Epoch 1440, training loss: 906.8289794921875 = 0.18844638764858246 + 100.0 * 9.066405296325684
Epoch 1440, val loss: 0.47108811140060425
Epoch 1450, training loss: 906.72509765625 = 0.18621647357940674 + 100.0 * 9.065388679504395
Epoch 1450, val loss: 0.4711977243423462
Epoch 1460, training loss: 906.7176513671875 = 0.18403303623199463 + 100.0 * 9.065336227416992
Epoch 1460, val loss: 0.47387245297431946
Epoch 1470, training loss: 906.8942260742188 = 0.18187256157398224 + 100.0 * 9.067123413085938
Epoch 1470, val loss: 0.4733331501483917
Epoch 1480, training loss: 906.6690673828125 = 0.17963124811649323 + 100.0 * 9.064894676208496
Epoch 1480, val loss: 0.47606876492500305
Epoch 1490, training loss: 906.65673828125 = 0.17747431993484497 + 100.0 * 9.06479263305664
Epoch 1490, val loss: 0.47769173979759216
Epoch 1500, training loss: 906.6452026367188 = 0.17530806362628937 + 100.0 * 9.064699172973633
Epoch 1500, val loss: 0.47883933782577515
Epoch 1510, training loss: 906.544921875 = 0.17315690219402313 + 100.0 * 9.06371784210205
Epoch 1510, val loss: 0.47916147112846375
Epoch 1520, training loss: 906.4837036132812 = 0.1710033416748047 + 100.0 * 9.063126564025879
Epoch 1520, val loss: 0.4812496304512024
Epoch 1530, training loss: 906.6177368164062 = 0.16889134049415588 + 100.0 * 9.064488410949707
Epoch 1530, val loss: 0.48288199305534363
Epoch 1540, training loss: 906.4927368164062 = 0.16675734519958496 + 100.0 * 9.063260078430176
Epoch 1540, val loss: 0.4835638105869293
Epoch 1550, training loss: 906.5115356445312 = 0.1646389216184616 + 100.0 * 9.063468933105469
Epoch 1550, val loss: 0.4861651360988617
Epoch 1560, training loss: 906.4340209960938 = 0.16253912448883057 + 100.0 * 9.062714576721191
Epoch 1560, val loss: 0.48703503608703613
Epoch 1570, training loss: 906.5527954101562 = 0.16050034761428833 + 100.0 * 9.063922882080078
Epoch 1570, val loss: 0.4895196557044983
Epoch 1580, training loss: 906.3832397460938 = 0.15841099619865417 + 100.0 * 9.062248229980469
Epoch 1580, val loss: 0.48914363980293274
Epoch 1590, training loss: 906.4210205078125 = 0.15646107494831085 + 100.0 * 9.06264591217041
Epoch 1590, val loss: 0.4902190566062927
Epoch 1600, training loss: 906.3411254882812 = 0.15434227883815765 + 100.0 * 9.061867713928223
Epoch 1600, val loss: 0.49347710609436035
Epoch 1610, training loss: 906.302490234375 = 0.1523572951555252 + 100.0 * 9.061501502990723
Epoch 1610, val loss: 0.49473893642425537
Epoch 1620, training loss: 906.2571411132812 = 0.15038515627384186 + 100.0 * 9.061067581176758
Epoch 1620, val loss: 0.49646759033203125
Epoch 1630, training loss: 906.5930786132812 = 0.1484801173210144 + 100.0 * 9.064445495605469
Epoch 1630, val loss: 0.497371107339859
Epoch 1640, training loss: 906.5226440429688 = 0.14650455117225647 + 100.0 * 9.063761711120605
Epoch 1640, val loss: 0.4989486038684845
Epoch 1650, training loss: 906.1715698242188 = 0.14453105628490448 + 100.0 * 9.060270309448242
Epoch 1650, val loss: 0.5017613172531128
Epoch 1660, training loss: 906.1754760742188 = 0.14263151586055756 + 100.0 * 9.060328483581543
Epoch 1660, val loss: 0.5035619735717773
Epoch 1670, training loss: 906.1226806640625 = 0.14074358344078064 + 100.0 * 9.059819221496582
Epoch 1670, val loss: 0.5049107074737549
Epoch 1680, training loss: 906.1627197265625 = 0.13892439007759094 + 100.0 * 9.060237884521484
Epoch 1680, val loss: 0.5055509805679321
Epoch 1690, training loss: 906.37841796875 = 0.1370813101530075 + 100.0 * 9.062413215637207
Epoch 1690, val loss: 0.5083516240119934
Epoch 1700, training loss: 906.0718994140625 = 0.1352282613515854 + 100.0 * 9.059366226196289
Epoch 1700, val loss: 0.5089500546455383
Epoch 1710, training loss: 906.074462890625 = 0.1334114670753479 + 100.0 * 9.059410095214844
Epoch 1710, val loss: 0.5131533741950989
Epoch 1720, training loss: 906.0108032226562 = 0.1316094845533371 + 100.0 * 9.058792114257812
Epoch 1720, val loss: 0.5142070651054382
Epoch 1730, training loss: 906.2763061523438 = 0.12986965477466583 + 100.0 * 9.061464309692383
Epoch 1730, val loss: 0.5153789520263672
Epoch 1740, training loss: 906.1356811523438 = 0.1281147599220276 + 100.0 * 9.060075759887695
Epoch 1740, val loss: 0.5166765451431274
Epoch 1750, training loss: 906.0379638671875 = 0.12636007368564606 + 100.0 * 9.05911636352539
Epoch 1750, val loss: 0.5196846127510071
Epoch 1760, training loss: 905.9188842773438 = 0.12464349716901779 + 100.0 * 9.057942390441895
Epoch 1760, val loss: 0.5207664370536804
Epoch 1770, training loss: 905.9447021484375 = 0.12295667827129364 + 100.0 * 9.05821704864502
Epoch 1770, val loss: 0.5231622457504272
Epoch 1780, training loss: 906.322998046875 = 0.12131340056657791 + 100.0 * 9.062016487121582
Epoch 1780, val loss: 0.5246953368186951
Epoch 1790, training loss: 905.979736328125 = 0.11962207406759262 + 100.0 * 9.058601379394531
Epoch 1790, val loss: 0.5263350009918213
Epoch 1800, training loss: 905.8633422851562 = 0.11794595420360565 + 100.0 * 9.057454109191895
Epoch 1800, val loss: 0.5279032588005066
Epoch 1810, training loss: 905.9644165039062 = 0.11648623645305634 + 100.0 * 9.058479309082031
Epoch 1810, val loss: 0.5280330777168274
Epoch 1820, training loss: 905.9163208007812 = 0.11472368240356445 + 100.0 * 9.058015823364258
Epoch 1820, val loss: 0.5328027606010437
Epoch 1830, training loss: 905.79296875 = 0.1131356731057167 + 100.0 * 9.056797981262207
Epoch 1830, val loss: 0.5342767834663391
Epoch 1840, training loss: 905.80078125 = 0.11160393059253693 + 100.0 * 9.056891441345215
Epoch 1840, val loss: 0.5359121561050415
Epoch 1850, training loss: 905.8936767578125 = 0.11006037145853043 + 100.0 * 9.057836532592773
Epoch 1850, val loss: 0.5384045839309692
Epoch 1860, training loss: 905.7741088867188 = 0.10854446142911911 + 100.0 * 9.056655883789062
Epoch 1860, val loss: 0.5396208763122559
Epoch 1870, training loss: 905.7341918945312 = 0.10703485459089279 + 100.0 * 9.05627155303955
Epoch 1870, val loss: 0.5433496832847595
Epoch 1880, training loss: 905.7462158203125 = 0.10554619878530502 + 100.0 * 9.05640697479248
Epoch 1880, val loss: 0.5453197360038757
Epoch 1890, training loss: 905.8530883789062 = 0.10407213121652603 + 100.0 * 9.057490348815918
Epoch 1890, val loss: 0.5465136766433716
Epoch 1900, training loss: 905.6450805664062 = 0.10260724276304245 + 100.0 * 9.055424690246582
Epoch 1900, val loss: 0.5487083792686462
Epoch 1910, training loss: 905.625732421875 = 0.10116642713546753 + 100.0 * 9.055245399475098
Epoch 1910, val loss: 0.5504488945007324
Epoch 1920, training loss: 905.6134033203125 = 0.0997549369931221 + 100.0 * 9.055136680603027
Epoch 1920, val loss: 0.5529717206954956
Epoch 1930, training loss: 906.2015380859375 = 0.09841694682836533 + 100.0 * 9.061031341552734
Epoch 1930, val loss: 0.5554366707801819
Epoch 1940, training loss: 905.8109130859375 = 0.096981942653656 + 100.0 * 9.05713939666748
Epoch 1940, val loss: 0.5559215545654297
Epoch 1950, training loss: 905.56201171875 = 0.09559234231710434 + 100.0 * 9.054664611816406
Epoch 1950, val loss: 0.5584140419960022
Epoch 1960, training loss: 905.5467529296875 = 0.09423051029443741 + 100.0 * 9.054525375366211
Epoch 1960, val loss: 0.5609418153762817
Epoch 1970, training loss: 905.588134765625 = 0.09289874136447906 + 100.0 * 9.054952621459961
Epoch 1970, val loss: 0.563264012336731
Epoch 1980, training loss: 905.6032104492188 = 0.09157001227140427 + 100.0 * 9.055116653442383
Epoch 1980, val loss: 0.564358115196228
Epoch 1990, training loss: 905.5228881835938 = 0.0902705043554306 + 100.0 * 9.054326057434082
Epoch 1990, val loss: 0.5659724473953247
Epoch 2000, training loss: 905.60986328125 = 0.08905411511659622 + 100.0 * 9.055208206176758
Epoch 2000, val loss: 0.5667088627815247
Epoch 2010, training loss: 905.555419921875 = 0.08772240579128265 + 100.0 * 9.05467700958252
Epoch 2010, val loss: 0.5706126093864441
Epoch 2020, training loss: 905.4456176757812 = 0.08643452078104019 + 100.0 * 9.05359172821045
Epoch 2020, val loss: 0.5728487968444824
Epoch 2030, training loss: 905.4532470703125 = 0.08520644158124924 + 100.0 * 9.053680419921875
Epoch 2030, val loss: 0.574161946773529
Epoch 2040, training loss: 905.5518188476562 = 0.08411550521850586 + 100.0 * 9.05467700958252
Epoch 2040, val loss: 0.5749417543411255
Epoch 2050, training loss: 905.4910278320312 = 0.08281590044498444 + 100.0 * 9.054081916809082
Epoch 2050, val loss: 0.5785873532295227
Epoch 2060, training loss: 905.4526977539062 = 0.0816301554441452 + 100.0 * 9.0537109375
Epoch 2060, val loss: 0.5800960063934326
Epoch 2070, training loss: 905.3353881835938 = 0.08043462783098221 + 100.0 * 9.052549362182617
Epoch 2070, val loss: 0.5844212770462036
Epoch 2080, training loss: 905.326904296875 = 0.07930410653352737 + 100.0 * 9.052475929260254
Epoch 2080, val loss: 0.5868378281593323
Epoch 2090, training loss: 905.6421508789062 = 0.07833590358495712 + 100.0 * 9.055638313293457
Epoch 2090, val loss: 0.5908093452453613
Epoch 2100, training loss: 905.424560546875 = 0.07712257653474808 + 100.0 * 9.053474426269531
Epoch 2100, val loss: 0.5882874727249146
Epoch 2110, training loss: 905.3369140625 = 0.07597879320383072 + 100.0 * 9.05260944366455
Epoch 2110, val loss: 0.5935039520263672
Epoch 2120, training loss: 905.2642211914062 = 0.07486210018396378 + 100.0 * 9.05189323425293
Epoch 2120, val loss: 0.5950654149055481
Epoch 2130, training loss: 905.4518432617188 = 0.07392170280218124 + 100.0 * 9.053779602050781
Epoch 2130, val loss: 0.5991519689559937
Epoch 2140, training loss: 905.2238159179688 = 0.0727711021900177 + 100.0 * 9.05151081085205
Epoch 2140, val loss: 0.5972577929496765
Epoch 2150, training loss: 905.2220458984375 = 0.07171197980642319 + 100.0 * 9.05150318145752
Epoch 2150, val loss: 0.6013067364692688
Epoch 2160, training loss: 905.5159301757812 = 0.07074688374996185 + 100.0 * 9.054451942443848
Epoch 2160, val loss: 0.6025403141975403
Epoch 2170, training loss: 905.2645874023438 = 0.06974268704652786 + 100.0 * 9.051948547363281
Epoch 2170, val loss: 0.6072770953178406
Epoch 2180, training loss: 905.1526489257812 = 0.06870471686124802 + 100.0 * 9.0508394241333
Epoch 2180, val loss: 0.6066385507583618
Epoch 2190, training loss: 905.1229248046875 = 0.06770961731672287 + 100.0 * 9.050552368164062
Epoch 2190, val loss: 0.6098175644874573
Epoch 2200, training loss: 905.2030029296875 = 0.06676296144723892 + 100.0 * 9.051362037658691
Epoch 2200, val loss: 0.6114581823348999
Epoch 2210, training loss: 905.1900634765625 = 0.06580237299203873 + 100.0 * 9.05124282836914
Epoch 2210, val loss: 0.6145195364952087
Epoch 2220, training loss: 905.07470703125 = 0.06486385315656662 + 100.0 * 9.050098419189453
Epoch 2220, val loss: 0.6173625588417053
Epoch 2230, training loss: 905.0674438476562 = 0.06391909718513489 + 100.0 * 9.05003547668457
Epoch 2230, val loss: 0.6189779043197632
Epoch 2240, training loss: 905.2549438476562 = 0.06303004920482635 + 100.0 * 9.051918983459473
Epoch 2240, val loss: 0.6209519505500793
Epoch 2250, training loss: 905.0897827148438 = 0.06213501840829849 + 100.0 * 9.050276756286621
Epoch 2250, val loss: 0.6250694990158081
Epoch 2260, training loss: 905.046142578125 = 0.06121562793850899 + 100.0 * 9.049849510192871
Epoch 2260, val loss: 0.6246346831321716
Epoch 2270, training loss: 905.0239868164062 = 0.06032935902476311 + 100.0 * 9.049636840820312
Epoch 2270, val loss: 0.6282621026039124
Epoch 2280, training loss: 905.001953125 = 0.059475064277648926 + 100.0 * 9.04942512512207
Epoch 2280, val loss: 0.6292943358421326
Epoch 2290, training loss: 905.4100952148438 = 0.05872412770986557 + 100.0 * 9.053513526916504
Epoch 2290, val loss: 0.6304038166999817
Epoch 2300, training loss: 905.2330932617188 = 0.05789162963628769 + 100.0 * 9.051752090454102
Epoch 2300, val loss: 0.6380053758621216
Epoch 2310, training loss: 905.0761108398438 = 0.056970298290252686 + 100.0 * 9.050191879272461
Epoch 2310, val loss: 0.6386989951133728
Epoch 2320, training loss: 904.928955078125 = 0.056146059185266495 + 100.0 * 9.048727989196777
Epoch 2320, val loss: 0.6383512020111084
Epoch 2330, training loss: 904.9485473632812 = 0.05535074695944786 + 100.0 * 9.048932075500488
Epoch 2330, val loss: 0.6411281824111938
Epoch 2340, training loss: 905.0610961914062 = 0.05459267646074295 + 100.0 * 9.050065040588379
Epoch 2340, val loss: 0.6430684328079224
Epoch 2350, training loss: 904.9990234375 = 0.05379049479961395 + 100.0 * 9.049452781677246
Epoch 2350, val loss: 0.6467972993850708
Epoch 2360, training loss: 904.9339599609375 = 0.05302603170275688 + 100.0 * 9.048809051513672
Epoch 2360, val loss: 0.6485073566436768
Epoch 2370, training loss: 904.9935302734375 = 0.052459437400102615 + 100.0 * 9.049410820007324
Epoch 2370, val loss: 0.6480066180229187
Epoch 2380, training loss: 904.8648071289062 = 0.051532063633203506 + 100.0 * 9.04813289642334
Epoch 2380, val loss: 0.6531209945678711
Epoch 2390, training loss: 904.8763427734375 = 0.050802722573280334 + 100.0 * 9.048255920410156
Epoch 2390, val loss: 0.6561376452445984
Epoch 2400, training loss: 904.9302368164062 = 0.050093550235033035 + 100.0 * 9.04880142211914
Epoch 2400, val loss: 0.6584659814834595
Epoch 2410, training loss: 904.8435668945312 = 0.04939012601971626 + 100.0 * 9.047942161560059
Epoch 2410, val loss: 0.660554051399231
Epoch 2420, training loss: 904.9138793945312 = 0.048730771988630295 + 100.0 * 9.048651695251465
Epoch 2420, val loss: 0.6621844172477722
Epoch 2430, training loss: 904.99169921875 = 0.048024117946624756 + 100.0 * 9.049436569213867
Epoch 2430, val loss: 0.6657388806343079
Epoch 2440, training loss: 904.8206787109375 = 0.04735253378748894 + 100.0 * 9.047733306884766
Epoch 2440, val loss: 0.6693534851074219
Epoch 2450, training loss: 904.7960815429688 = 0.046687472611665726 + 100.0 * 9.047493934631348
Epoch 2450, val loss: 0.671449601650238
Epoch 2460, training loss: 904.811279296875 = 0.04606795683503151 + 100.0 * 9.047652244567871
Epoch 2460, val loss: 0.6747024655342102
Epoch 2470, training loss: 904.7510986328125 = 0.04540297016501427 + 100.0 * 9.047057151794434
Epoch 2470, val loss: 0.6762177348136902
Epoch 2480, training loss: 904.7474975585938 = 0.044739287346601486 + 100.0 * 9.047027587890625
Epoch 2480, val loss: 0.6765013933181763
Epoch 2490, training loss: 904.8238525390625 = 0.04413212835788727 + 100.0 * 9.047797203063965
Epoch 2490, val loss: 0.6784558892250061
Epoch 2500, training loss: 904.927001953125 = 0.043543048202991486 + 100.0 * 9.048834800720215
Epoch 2500, val loss: 0.6802041530609131
Epoch 2510, training loss: 904.7400512695312 = 0.042905230075120926 + 100.0 * 9.046971321105957
Epoch 2510, val loss: 0.6840769648551941
Epoch 2520, training loss: 904.7218017578125 = 0.04230909422039986 + 100.0 * 9.046794891357422
Epoch 2520, val loss: 0.6867221593856812
Epoch 2530, training loss: 904.7378540039062 = 0.04172225296497345 + 100.0 * 9.046960830688477
Epoch 2530, val loss: 0.6888816952705383
Epoch 2540, training loss: 904.6083984375 = 0.04114106670022011 + 100.0 * 9.045672416687012
Epoch 2540, val loss: 0.6900976300239563
Epoch 2550, training loss: 904.7662963867188 = 0.04074494168162346 + 100.0 * 9.047255516052246
Epoch 2550, val loss: 0.6901818513870239
Epoch 2560, training loss: 904.700439453125 = 0.04004782810807228 + 100.0 * 9.04660415649414
Epoch 2560, val loss: 0.6957755088806152
Epoch 2570, training loss: 904.5723266601562 = 0.03947749361395836 + 100.0 * 9.045328140258789
Epoch 2570, val loss: 0.6965143084526062
Epoch 2580, training loss: 904.6276245117188 = 0.038987983018159866 + 100.0 * 9.045886039733887
Epoch 2580, val loss: 0.6982075572013855
Epoch 2590, training loss: 904.7714233398438 = 0.038677409291267395 + 100.0 * 9.047327041625977
Epoch 2590, val loss: 0.6989903450012207
Epoch 2600, training loss: 904.7738037109375 = 0.03797396272420883 + 100.0 * 9.047358512878418
Epoch 2600, val loss: 0.7063542604446411
Epoch 2610, training loss: 904.5791015625 = 0.03742941841483116 + 100.0 * 9.045416831970215
Epoch 2610, val loss: 0.706541121006012
Epoch 2620, training loss: 904.5026245117188 = 0.036933623254299164 + 100.0 * 9.044656753540039
Epoch 2620, val loss: 0.7100169062614441
Epoch 2630, training loss: 904.7132568359375 = 0.03645730018615723 + 100.0 * 9.046768188476562
Epoch 2630, val loss: 0.7104129195213318
Epoch 2640, training loss: 904.4927368164062 = 0.03596287593245506 + 100.0 * 9.044568061828613
Epoch 2640, val loss: 0.7143566012382507
Epoch 2650, training loss: 904.4575805664062 = 0.035475172102451324 + 100.0 * 9.044220924377441
Epoch 2650, val loss: 0.7156296968460083
Epoch 2660, training loss: 904.5303955078125 = 0.03503384068608284 + 100.0 * 9.044953346252441
Epoch 2660, val loss: 0.7173609733581543
Epoch 2670, training loss: 904.6304931640625 = 0.03457344323396683 + 100.0 * 9.04595947265625
Epoch 2670, val loss: 0.7200430035591125
Epoch 2680, training loss: 904.5183715820312 = 0.0341075174510479 + 100.0 * 9.044842720031738
Epoch 2680, val loss: 0.7224214673042297
Epoch 2690, training loss: 904.5748291015625 = 0.03369225189089775 + 100.0 * 9.045411109924316
Epoch 2690, val loss: 0.7237474918365479
Epoch 2700, training loss: 904.425537109375 = 0.03322524204850197 + 100.0 * 9.043923377990723
Epoch 2700, val loss: 0.7263458967208862
Epoch 2710, training loss: 904.4720458984375 = 0.032799798995256424 + 100.0 * 9.044392585754395
Epoch 2710, val loss: 0.7272871732711792
Epoch 2720, training loss: 904.4847412109375 = 0.03239443898200989 + 100.0 * 9.044523239135742
Epoch 2720, val loss: 0.72947758436203
Epoch 2730, training loss: 904.4385375976562 = 0.031946226954460144 + 100.0 * 9.044066429138184
Epoch 2730, val loss: 0.7337592244148254
Epoch 2740, training loss: 904.5014038085938 = 0.031553804874420166 + 100.0 * 9.044698715209961
Epoch 2740, val loss: 0.7364670038223267
Epoch 2750, training loss: 904.4371948242188 = 0.031130535528063774 + 100.0 * 9.044060707092285
Epoch 2750, val loss: 0.736772894859314
Epoch 2760, training loss: 904.3787841796875 = 0.03075874224305153 + 100.0 * 9.043479919433594
Epoch 2760, val loss: 0.7383309006690979
Epoch 2770, training loss: 904.4118041992188 = 0.030336711555719376 + 100.0 * 9.043814659118652
Epoch 2770, val loss: 0.7426736354827881
Epoch 2780, training loss: 904.3941040039062 = 0.02995252050459385 + 100.0 * 9.043641090393066
Epoch 2780, val loss: 0.7438947558403015
Epoch 2790, training loss: 904.3945922851562 = 0.029591066762804985 + 100.0 * 9.043649673461914
Epoch 2790, val loss: 0.7477936744689941
Epoch 2800, training loss: 904.4035034179688 = 0.02927294932305813 + 100.0 * 9.043742179870605
Epoch 2800, val loss: 0.7511313557624817
Epoch 2810, training loss: 904.3953857421875 = 0.028826214373111725 + 100.0 * 9.043665885925293
Epoch 2810, val loss: 0.7511687278747559
Epoch 2820, training loss: 904.3560180664062 = 0.028464345261454582 + 100.0 * 9.043275833129883
Epoch 2820, val loss: 0.7529954314231873
Epoch 2830, training loss: 904.3541259765625 = 0.02811899036169052 + 100.0 * 9.043259620666504
Epoch 2830, val loss: 0.7534167766571045
Epoch 2840, training loss: 904.3883056640625 = 0.027801718562841415 + 100.0 * 9.043604850769043
Epoch 2840, val loss: 0.7552595138549805
Epoch 2850, training loss: 904.270751953125 = 0.027411891147494316 + 100.0 * 9.042433738708496
Epoch 2850, val loss: 0.7603670954704285
Epoch 2860, training loss: 904.4017944335938 = 0.027074545621871948 + 100.0 * 9.043746948242188
Epoch 2860, val loss: 0.7603424787521362
Epoch 2870, training loss: 904.2471923828125 = 0.026772530749440193 + 100.0 * 9.042203903198242
Epoch 2870, val loss: 0.7658717632293701
Epoch 2880, training loss: 904.1993408203125 = 0.026396170258522034 + 100.0 * 9.041729927062988
Epoch 2880, val loss: 0.7644890546798706
Epoch 2890, training loss: 904.195068359375 = 0.0260765440762043 + 100.0 * 9.0416898727417
Epoch 2890, val loss: 0.7669660449028015
Epoch 2900, training loss: 904.5413818359375 = 0.02580610103905201 + 100.0 * 9.04515552520752
Epoch 2900, val loss: 0.7682265043258667
Epoch 2910, training loss: 904.3285522460938 = 0.02545832470059395 + 100.0 * 9.043030738830566
Epoch 2910, val loss: 0.7711623907089233
Epoch 2920, training loss: 904.2406005859375 = 0.02513311058282852 + 100.0 * 9.042154312133789
Epoch 2920, val loss: 0.7729203701019287
Epoch 2930, training loss: 904.1372680664062 = 0.024818366393446922 + 100.0 * 9.04112434387207
Epoch 2930, val loss: 0.7753411531448364
Epoch 2940, training loss: 904.296630859375 = 0.02455171011388302 + 100.0 * 9.042720794677734
Epoch 2940, val loss: 0.7759884595870972
Epoch 2950, training loss: 904.2007446289062 = 0.0242938045412302 + 100.0 * 9.041764259338379
Epoch 2950, val loss: 0.777599573135376
Epoch 2960, training loss: 904.0886840820312 = 0.023925747722387314 + 100.0 * 9.040647506713867
Epoch 2960, val loss: 0.7812949419021606
Epoch 2970, training loss: 904.0836181640625 = 0.023646563291549683 + 100.0 * 9.040599822998047
Epoch 2970, val loss: 0.7842611074447632
Epoch 2980, training loss: 904.2443237304688 = 0.023406170308589935 + 100.0 * 9.04220962524414
Epoch 2980, val loss: 0.7871639728546143
Epoch 2990, training loss: 904.130859375 = 0.023093542084097862 + 100.0 * 9.041077613830566
Epoch 2990, val loss: 0.7867746949195862
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8326
Overall ASR: 0.7196
Flip ASR: 0.6519/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1037.012939453125 = 1.1037122011184692 + 100.0 * 10.359091758728027
Epoch 0, val loss: 1.1037652492523193
Epoch 10, training loss: 1036.6629638671875 = 1.0927832126617432 + 100.0 * 10.35570240020752
Epoch 10, val loss: 1.092488408088684
Epoch 20, training loss: 1030.991943359375 = 1.078041672706604 + 100.0 * 10.299139976501465
Epoch 20, val loss: 1.077638030052185
Epoch 30, training loss: 981.231689453125 = 1.0638738870620728 + 100.0 * 9.801677703857422
Epoch 30, val loss: 1.0635758638381958
Epoch 40, training loss: 959.7603149414062 = 1.048600196838379 + 100.0 * 9.587117195129395
Epoch 40, val loss: 1.0480146408081055
Epoch 50, training loss: 946.805419921875 = 1.0298957824707031 + 100.0 * 9.457755088806152
Epoch 50, val loss: 1.0292332172393799
Epoch 60, training loss: 944.6753540039062 = 1.0097827911376953 + 100.0 * 9.43665599822998
Epoch 60, val loss: 1.0091363191604614
Epoch 70, training loss: 942.2562866210938 = 0.9883189797401428 + 100.0 * 9.412679672241211
Epoch 70, val loss: 0.988007128238678
Epoch 80, training loss: 940.2430419921875 = 0.9686473608016968 + 100.0 * 9.392744064331055
Epoch 80, val loss: 0.9687820672988892
Epoch 90, training loss: 937.6978149414062 = 0.9488407373428345 + 100.0 * 9.3674898147583
Epoch 90, val loss: 0.9490935802459717
Epoch 100, training loss: 934.5560913085938 = 0.9258690476417542 + 100.0 * 9.336302757263184
Epoch 100, val loss: 0.9265970587730408
Epoch 110, training loss: 931.5114135742188 = 0.9017506837844849 + 100.0 * 9.306097030639648
Epoch 110, val loss: 0.903308093547821
Epoch 120, training loss: 928.8245239257812 = 0.8758177757263184 + 100.0 * 9.279487609863281
Epoch 120, val loss: 0.8778460621833801
Epoch 130, training loss: 926.4760131835938 = 0.846642792224884 + 100.0 * 9.256294250488281
Epoch 130, val loss: 0.8498023152351379
Epoch 140, training loss: 925.0519409179688 = 0.8143876194953918 + 100.0 * 9.242375373840332
Epoch 140, val loss: 0.8188397288322449
Epoch 150, training loss: 923.890625 = 0.7795474529266357 + 100.0 * 9.231110572814941
Epoch 150, val loss: 0.7858349680900574
Epoch 160, training loss: 922.4644165039062 = 0.7455602884292603 + 100.0 * 9.217188835144043
Epoch 160, val loss: 0.7542551755905151
Epoch 170, training loss: 921.1397705078125 = 0.7130109071731567 + 100.0 * 9.204267501831055
Epoch 170, val loss: 0.7242069840431213
Epoch 180, training loss: 920.2105712890625 = 0.6811953186988831 + 100.0 * 9.195293426513672
Epoch 180, val loss: 0.6952832937240601
Epoch 190, training loss: 919.3445434570312 = 0.6511951088905334 + 100.0 * 9.186933517456055
Epoch 190, val loss: 0.6682369709014893
Epoch 200, training loss: 918.4833984375 = 0.6243075728416443 + 100.0 * 9.178590774536133
Epoch 200, val loss: 0.6445162296295166
Epoch 210, training loss: 917.50048828125 = 0.601055383682251 + 100.0 * 9.168993949890137
Epoch 210, val loss: 0.6246656179428101
Epoch 220, training loss: 917.0036010742188 = 0.5799021124839783 + 100.0 * 9.164237022399902
Epoch 220, val loss: 0.6068583726882935
Epoch 230, training loss: 916.1180419921875 = 0.5598224997520447 + 100.0 * 9.155582427978516
Epoch 230, val loss: 0.5903446078300476
Epoch 240, training loss: 915.4093017578125 = 0.5421136021614075 + 100.0 * 9.148672103881836
Epoch 240, val loss: 0.5760851502418518
Epoch 250, training loss: 914.9408569335938 = 0.5262190103530884 + 100.0 * 9.144145965576172
Epoch 250, val loss: 0.5637118220329285
Epoch 260, training loss: 914.5993041992188 = 0.5118283629417419 + 100.0 * 9.140874862670898
Epoch 260, val loss: 0.5528759956359863
Epoch 270, training loss: 914.31396484375 = 0.49897509813308716 + 100.0 * 9.138150215148926
Epoch 270, val loss: 0.5433506369590759
Epoch 280, training loss: 914.2300415039062 = 0.48748183250427246 + 100.0 * 9.137425422668457
Epoch 280, val loss: 0.5351476073265076
Epoch 290, training loss: 913.8565673828125 = 0.47714877128601074 + 100.0 * 9.133793830871582
Epoch 290, val loss: 0.528015673160553
Epoch 300, training loss: 913.6246948242188 = 0.46809321641921997 + 100.0 * 9.131566047668457
Epoch 300, val loss: 0.5220374464988708
Epoch 310, training loss: 913.8601684570312 = 0.4598882496356964 + 100.0 * 9.134002685546875
Epoch 310, val loss: 0.5167526602745056
Epoch 320, training loss: 913.3172607421875 = 0.4522719383239746 + 100.0 * 9.128649711608887
Epoch 320, val loss: 0.5120581984519958
Epoch 330, training loss: 912.93505859375 = 0.4455721080303192 + 100.0 * 9.124895095825195
Epoch 330, val loss: 0.5081643462181091
Epoch 340, training loss: 912.6598510742188 = 0.4394208788871765 + 100.0 * 9.122204780578613
Epoch 340, val loss: 0.5047850608825684
Epoch 350, training loss: 912.3931274414062 = 0.43368491530418396 + 100.0 * 9.11959457397461
Epoch 350, val loss: 0.5015175938606262
Epoch 360, training loss: 912.6509399414062 = 0.42832183837890625 + 100.0 * 9.122225761413574
Epoch 360, val loss: 0.4983879327774048
Epoch 370, training loss: 912.0729370117188 = 0.4229854941368103 + 100.0 * 9.116499900817871
Epoch 370, val loss: 0.49592673778533936
Epoch 380, training loss: 911.73046875 = 0.4181749224662781 + 100.0 * 9.113122940063477
Epoch 380, val loss: 0.49372678995132446
Epoch 390, training loss: 911.5000610351562 = 0.41358983516693115 + 100.0 * 9.110864639282227
Epoch 390, val loss: 0.49160802364349365
Epoch 400, training loss: 911.5941772460938 = 0.4091009497642517 + 100.0 * 9.11185073852539
Epoch 400, val loss: 0.48969322443008423
Epoch 410, training loss: 911.2903442382812 = 0.4047481119632721 + 100.0 * 9.108856201171875
Epoch 410, val loss: 0.48786661028862
Epoch 420, training loss: 911.0052490234375 = 0.40068838000297546 + 100.0 * 9.106045722961426
Epoch 420, val loss: 0.4861435294151306
Epoch 430, training loss: 910.8530883789062 = 0.39681151509284973 + 100.0 * 9.104562759399414
Epoch 430, val loss: 0.484537810087204
Epoch 440, training loss: 910.804931640625 = 0.3929802477359772 + 100.0 * 9.104119300842285
Epoch 440, val loss: 0.4831666648387909
Epoch 450, training loss: 910.619140625 = 0.38928115367889404 + 100.0 * 9.102298736572266
Epoch 450, val loss: 0.48141568899154663
Epoch 460, training loss: 910.4589233398438 = 0.3857811987400055 + 100.0 * 9.100730895996094
Epoch 460, val loss: 0.48043158650398254
Epoch 470, training loss: 910.3190307617188 = 0.38240620493888855 + 100.0 * 9.099366188049316
Epoch 470, val loss: 0.47903570532798767
Epoch 480, training loss: 910.5633544921875 = 0.3791026473045349 + 100.0 * 9.101842880249023
Epoch 480, val loss: 0.47807618975639343
Epoch 490, training loss: 910.1048583984375 = 0.3758365511894226 + 100.0 * 9.0972900390625
Epoch 490, val loss: 0.4766717255115509
Epoch 500, training loss: 909.9864501953125 = 0.37273937463760376 + 100.0 * 9.096137046813965
Epoch 500, val loss: 0.47566890716552734
Epoch 510, training loss: 909.9131469726562 = 0.36973458528518677 + 100.0 * 9.095434188842773
Epoch 510, val loss: 0.4748324155807495
Epoch 520, training loss: 909.81640625 = 0.3667502999305725 + 100.0 * 9.094496726989746
Epoch 520, val loss: 0.47372114658355713
Epoch 530, training loss: 909.746337890625 = 0.3638472855091095 + 100.0 * 9.093825340270996
Epoch 530, val loss: 0.4729945659637451
Epoch 540, training loss: 909.58984375 = 0.36104488372802734 + 100.0 * 9.09228801727295
Epoch 540, val loss: 0.472108393907547
Epoch 550, training loss: 909.5535888671875 = 0.35830360651016235 + 100.0 * 9.09195327758789
Epoch 550, val loss: 0.4714969992637634
Epoch 560, training loss: 909.5021362304688 = 0.3555464744567871 + 100.0 * 9.091465950012207
Epoch 560, val loss: 0.47065815329551697
Epoch 570, training loss: 909.4102783203125 = 0.3528561294078827 + 100.0 * 9.090574264526367
Epoch 570, val loss: 0.4698082208633423
Epoch 580, training loss: 909.2933349609375 = 0.3502749502658844 + 100.0 * 9.089430809020996
Epoch 580, val loss: 0.46915706992149353
Epoch 590, training loss: 909.1972045898438 = 0.3477685749530792 + 100.0 * 9.088494300842285
Epoch 590, val loss: 0.4685359597206116
Epoch 600, training loss: 909.4577026367188 = 0.3452671468257904 + 100.0 * 9.091124534606934
Epoch 600, val loss: 0.4681171774864197
Epoch 610, training loss: 909.15625 = 0.3427612781524658 + 100.0 * 9.088134765625
Epoch 610, val loss: 0.46684759855270386
Epoch 620, training loss: 908.9756469726562 = 0.3403238356113434 + 100.0 * 9.086353302001953
Epoch 620, val loss: 0.4665941596031189
Epoch 630, training loss: 908.9104614257812 = 0.3379659950733185 + 100.0 * 9.085724830627441
Epoch 630, val loss: 0.46602174639701843
Epoch 640, training loss: 909.5287475585938 = 0.33562591671943665 + 100.0 * 9.091931343078613
Epoch 640, val loss: 0.4658466577529907
Epoch 650, training loss: 908.86083984375 = 0.3331924378871918 + 100.0 * 9.08527660369873
Epoch 650, val loss: 0.46496957540512085
Epoch 660, training loss: 908.7301635742188 = 0.3308928310871124 + 100.0 * 9.083992958068848
Epoch 660, val loss: 0.4644274413585663
Epoch 670, training loss: 908.6488037109375 = 0.32866615056991577 + 100.0 * 9.08320140838623
Epoch 670, val loss: 0.4636894166469574
Epoch 680, training loss: 908.5848999023438 = 0.3264710009098053 + 100.0 * 9.082584381103516
Epoch 680, val loss: 0.4632469713687897
Epoch 690, training loss: 908.7564697265625 = 0.32428058981895447 + 100.0 * 9.084321975708008
Epoch 690, val loss: 0.46259739995002747
Epoch 700, training loss: 908.6807861328125 = 0.32204312086105347 + 100.0 * 9.083587646484375
Epoch 700, val loss: 0.46193182468414307
Epoch 710, training loss: 908.4159545898438 = 0.31983768939971924 + 100.0 * 9.080961227416992
Epoch 710, val loss: 0.4616570472717285
Epoch 720, training loss: 908.372314453125 = 0.31770673394203186 + 100.0 * 9.080546379089355
Epoch 720, val loss: 0.4613752067089081
Epoch 730, training loss: 908.3851318359375 = 0.31561291217803955 + 100.0 * 9.080695152282715
Epoch 730, val loss: 0.46110838651657104
Epoch 740, training loss: 908.2383422851562 = 0.3134726285934448 + 100.0 * 9.079248428344727
Epoch 740, val loss: 0.46027976274490356
Epoch 750, training loss: 908.1815795898438 = 0.31137898564338684 + 100.0 * 9.078701972961426
Epoch 750, val loss: 0.4600170850753784
Epoch 760, training loss: 908.3704223632812 = 0.3092981278896332 + 100.0 * 9.080611228942871
Epoch 760, val loss: 0.4595600962638855
Epoch 770, training loss: 908.2542724609375 = 0.3072037994861603 + 100.0 * 9.07947063446045
Epoch 770, val loss: 0.4591062366962433
Epoch 780, training loss: 908.1781005859375 = 0.30512312054634094 + 100.0 * 9.078729629516602
Epoch 780, val loss: 0.45863863825798035
Epoch 790, training loss: 908.01611328125 = 0.3030787706375122 + 100.0 * 9.077130317687988
Epoch 790, val loss: 0.4581592381000519
Epoch 800, training loss: 907.9063110351562 = 0.3010452389717102 + 100.0 * 9.07605266571045
Epoch 800, val loss: 0.4578477740287781
Epoch 810, training loss: 907.902099609375 = 0.2990344762802124 + 100.0 * 9.076030731201172
Epoch 810, val loss: 0.457697331905365
Epoch 820, training loss: 908.1393432617188 = 0.2970341145992279 + 100.0 * 9.078422546386719
Epoch 820, val loss: 0.45789575576782227
Epoch 830, training loss: 907.89208984375 = 0.2949032783508301 + 100.0 * 9.075971603393555
Epoch 830, val loss: 0.4569602906703949
Epoch 840, training loss: 907.7268676757812 = 0.29289770126342773 + 100.0 * 9.074339866638184
Epoch 840, val loss: 0.4566918909549713
Epoch 850, training loss: 907.6983032226562 = 0.2909456193447113 + 100.0 * 9.074073791503906
Epoch 850, val loss: 0.4560496211051941
Epoch 860, training loss: 907.6619873046875 = 0.2889695167541504 + 100.0 * 9.07373046875
Epoch 860, val loss: 0.45589858293533325
Epoch 870, training loss: 907.8167114257812 = 0.28697487711906433 + 100.0 * 9.075297355651855
Epoch 870, val loss: 0.45594996213912964
Epoch 880, training loss: 907.5562744140625 = 0.2849653661251068 + 100.0 * 9.072712898254395
Epoch 880, val loss: 0.45569634437561035
Epoch 890, training loss: 907.488525390625 = 0.2830091714859009 + 100.0 * 9.072054862976074
Epoch 890, val loss: 0.4553651511669159
Epoch 900, training loss: 907.466796875 = 0.2810620665550232 + 100.0 * 9.071857452392578
Epoch 900, val loss: 0.455217570066452
Epoch 910, training loss: 907.6150512695312 = 0.2790895998477936 + 100.0 * 9.073359489440918
Epoch 910, val loss: 0.45514124631881714
Epoch 920, training loss: 907.3941040039062 = 0.2770991325378418 + 100.0 * 9.07116985321045
Epoch 920, val loss: 0.45464980602264404
Epoch 930, training loss: 907.3984375 = 0.27516576647758484 + 100.0 * 9.071232795715332
Epoch 930, val loss: 0.4542815685272217
Epoch 940, training loss: 907.4013671875 = 0.2732186019420624 + 100.0 * 9.071281433105469
Epoch 940, val loss: 0.4541343152523041
Epoch 950, training loss: 907.3184204101562 = 0.2712198793888092 + 100.0 * 9.07047176361084
Epoch 950, val loss: 0.4545285105705261
Epoch 960, training loss: 907.2156372070312 = 0.269280344247818 + 100.0 * 9.069463729858398
Epoch 960, val loss: 0.45414429903030396
Epoch 970, training loss: 907.1397705078125 = 0.26734524965286255 + 100.0 * 9.068724632263184
Epoch 970, val loss: 0.4542527496814728
Epoch 980, training loss: 907.1981811523438 = 0.26541316509246826 + 100.0 * 9.069327354431152
Epoch 980, val loss: 0.4540966749191284
Epoch 990, training loss: 907.179443359375 = 0.26344045996665955 + 100.0 * 9.069160461425781
Epoch 990, val loss: 0.4541884958744049
Epoch 1000, training loss: 907.0909423828125 = 0.26147589087486267 + 100.0 * 9.068294525146484
Epoch 1000, val loss: 0.4544702470302582
Epoch 1010, training loss: 907.005615234375 = 0.25953012704849243 + 100.0 * 9.067461013793945
Epoch 1010, val loss: 0.4540379047393799
Epoch 1020, training loss: 906.9502563476562 = 0.2576128840446472 + 100.0 * 9.066926002502441
Epoch 1020, val loss: 0.45403823256492615
Epoch 1030, training loss: 907.1688842773438 = 0.25568127632141113 + 100.0 * 9.069131851196289
Epoch 1030, val loss: 0.4541670083999634
Epoch 1040, training loss: 907.0009765625 = 0.25373297929763794 + 100.0 * 9.067472457885742
Epoch 1040, val loss: 0.45488813519477844
Epoch 1050, training loss: 906.9423828125 = 0.2517615258693695 + 100.0 * 9.066905975341797
Epoch 1050, val loss: 0.4548150300979614
Epoch 1060, training loss: 906.86669921875 = 0.2498365193605423 + 100.0 * 9.066168785095215
Epoch 1060, val loss: 0.4544444978237152
Epoch 1070, training loss: 906.9788208007812 = 0.24788421392440796 + 100.0 * 9.067309379577637
Epoch 1070, val loss: 0.4551483988761902
Epoch 1080, training loss: 906.718017578125 = 0.24593740701675415 + 100.0 * 9.06472110748291
Epoch 1080, val loss: 0.4548540711402893
Epoch 1090, training loss: 906.661376953125 = 0.24400587379932404 + 100.0 * 9.064173698425293
Epoch 1090, val loss: 0.4550134241580963
Epoch 1100, training loss: 906.6314086914062 = 0.24209067225456238 + 100.0 * 9.06389331817627
Epoch 1100, val loss: 0.45542532205581665
Epoch 1110, training loss: 906.9613647460938 = 0.24022948741912842 + 100.0 * 9.067211151123047
Epoch 1110, val loss: 0.4549430310726166
Epoch 1120, training loss: 906.8030395507812 = 0.23820286989212036 + 100.0 * 9.065648078918457
Epoch 1120, val loss: 0.4557795524597168
Epoch 1130, training loss: 906.5325927734375 = 0.23625236749649048 + 100.0 * 9.062963485717773
Epoch 1130, val loss: 0.4564135670661926
Epoch 1140, training loss: 906.5289306640625 = 0.234335795044899 + 100.0 * 9.062946319580078
Epoch 1140, val loss: 0.4564862549304962
Epoch 1150, training loss: 906.5565185546875 = 0.23244769871234894 + 100.0 * 9.063241004943848
Epoch 1150, val loss: 0.4574550986289978
Epoch 1160, training loss: 906.4815063476562 = 0.2304934412240982 + 100.0 * 9.06251049041748
Epoch 1160, val loss: 0.4572733938694
Epoch 1170, training loss: 906.462646484375 = 0.22859147191047668 + 100.0 * 9.06234073638916
Epoch 1170, val loss: 0.4572165310382843
Epoch 1180, training loss: 906.5101928710938 = 0.22665244340896606 + 100.0 * 9.062835693359375
Epoch 1180, val loss: 0.45792287588119507
Epoch 1190, training loss: 906.5829467773438 = 0.22474393248558044 + 100.0 * 9.063582420349121
Epoch 1190, val loss: 0.4580785930156708
Epoch 1200, training loss: 906.37744140625 = 0.22279225289821625 + 100.0 * 9.061546325683594
Epoch 1200, val loss: 0.45858776569366455
Epoch 1210, training loss: 906.2998657226562 = 0.22087615728378296 + 100.0 * 9.060790061950684
Epoch 1210, val loss: 0.45921531319618225
Epoch 1220, training loss: 906.26318359375 = 0.21897004544734955 + 100.0 * 9.060441970825195
Epoch 1220, val loss: 0.45998576283454895
Epoch 1230, training loss: 906.4205932617188 = 0.21707171201705933 + 100.0 * 9.062034606933594
Epoch 1230, val loss: 0.4607507586479187
Epoch 1240, training loss: 906.3515014648438 = 0.21512022614479065 + 100.0 * 9.061363220214844
Epoch 1240, val loss: 0.4606900215148926
Epoch 1250, training loss: 906.2570190429688 = 0.21317140758037567 + 100.0 * 9.06043815612793
Epoch 1250, val loss: 0.4614889323711395
Epoch 1260, training loss: 906.290283203125 = 0.2112826406955719 + 100.0 * 9.060790061950684
Epoch 1260, val loss: 0.4618479907512665
Epoch 1270, training loss: 906.1558227539062 = 0.20933401584625244 + 100.0 * 9.059464454650879
Epoch 1270, val loss: 0.46302834153175354
Epoch 1280, training loss: 906.1348876953125 = 0.2074110507965088 + 100.0 * 9.059274673461914
Epoch 1280, val loss: 0.4635004699230194
Epoch 1290, training loss: 906.283203125 = 0.2055017054080963 + 100.0 * 9.060776710510254
Epoch 1290, val loss: 0.46466028690338135
Epoch 1300, training loss: 906.099365234375 = 0.20356710255146027 + 100.0 * 9.058958053588867
Epoch 1300, val loss: 0.4650062918663025
Epoch 1310, training loss: 906.1995239257812 = 0.20163525640964508 + 100.0 * 9.059978485107422
Epoch 1310, val loss: 0.466112345457077
Epoch 1320, training loss: 906.0040283203125 = 0.19971203804016113 + 100.0 * 9.058043479919434
Epoch 1320, val loss: 0.46611547470092773
Epoch 1330, training loss: 906.107666015625 = 0.19780796766281128 + 100.0 * 9.059098243713379
Epoch 1330, val loss: 0.46685534715652466
Epoch 1340, training loss: 905.8994750976562 = 0.19584128260612488 + 100.0 * 9.057036399841309
Epoch 1340, val loss: 0.46864554286003113
Epoch 1350, training loss: 905.8782958984375 = 0.19393369555473328 + 100.0 * 9.056843757629395
Epoch 1350, val loss: 0.4694892168045044
Epoch 1360, training loss: 905.9155883789062 = 0.19202920794487 + 100.0 * 9.057235717773438
Epoch 1360, val loss: 0.47020164132118225
Epoch 1370, training loss: 905.9408569335938 = 0.19011062383651733 + 100.0 * 9.057507514953613
Epoch 1370, val loss: 0.47142115235328674
Epoch 1380, training loss: 905.8359375 = 0.18819519877433777 + 100.0 * 9.056477546691895
Epoch 1380, val loss: 0.4721806049346924
Epoch 1390, training loss: 905.9193725585938 = 0.18629606068134308 + 100.0 * 9.057331085205078
Epoch 1390, val loss: 0.47275322675704956
Epoch 1400, training loss: 905.746337890625 = 0.1843605935573578 + 100.0 * 9.055619239807129
Epoch 1400, val loss: 0.4743068814277649
Epoch 1410, training loss: 905.745849609375 = 0.1824759691953659 + 100.0 * 9.055633544921875
Epoch 1410, val loss: 0.4757874310016632
Epoch 1420, training loss: 905.8321533203125 = 0.18061873316764832 + 100.0 * 9.05651569366455
Epoch 1420, val loss: 0.47746044397354126
Epoch 1430, training loss: 905.9063110351562 = 0.1786833554506302 + 100.0 * 9.057275772094727
Epoch 1430, val loss: 0.4776296317577362
Epoch 1440, training loss: 905.7393798828125 = 0.17677263915538788 + 100.0 * 9.055625915527344
Epoch 1440, val loss: 0.47877171635627747
Epoch 1450, training loss: 905.688720703125 = 0.1748858392238617 + 100.0 * 9.05513858795166
Epoch 1450, val loss: 0.48027166724205017
Epoch 1460, training loss: 905.7821044921875 = 0.1730036437511444 + 100.0 * 9.05609130859375
Epoch 1460, val loss: 0.4813607633113861
Epoch 1470, training loss: 905.6293334960938 = 0.17111875116825104 + 100.0 * 9.054581642150879
Epoch 1470, val loss: 0.48172152042388916
Epoch 1480, training loss: 905.5679321289062 = 0.1692078858613968 + 100.0 * 9.053987503051758
Epoch 1480, val loss: 0.48320260643959045
Epoch 1490, training loss: 905.701416015625 = 0.16732893884181976 + 100.0 * 9.055340766906738
Epoch 1490, val loss: 0.4845787584781647
Epoch 1500, training loss: 905.5651245117188 = 0.16543075442314148 + 100.0 * 9.053997039794922
Epoch 1500, val loss: 0.4863038957118988
Epoch 1510, training loss: 905.679931640625 = 0.1635640412569046 + 100.0 * 9.055163383483887
Epoch 1510, val loss: 0.4874242842197418
Epoch 1520, training loss: 905.497314453125 = 0.16167129576206207 + 100.0 * 9.053356170654297
Epoch 1520, val loss: 0.488292396068573
Epoch 1530, training loss: 905.44970703125 = 0.15978679060935974 + 100.0 * 9.052899360656738
Epoch 1530, val loss: 0.4901808202266693
Epoch 1540, training loss: 905.4286499023438 = 0.15792685747146606 + 100.0 * 9.05270767211914
Epoch 1540, val loss: 0.4914640784263611
Epoch 1550, training loss: 905.5880737304688 = 0.15608584880828857 + 100.0 * 9.054320335388184
Epoch 1550, val loss: 0.49326246976852417
Epoch 1560, training loss: 905.4916381835938 = 0.15423578023910522 + 100.0 * 9.053374290466309
Epoch 1560, val loss: 0.4951143264770508
Epoch 1570, training loss: 905.622802734375 = 0.1525556445121765 + 100.0 * 9.054702758789062
Epoch 1570, val loss: 0.4975048899650574
Epoch 1580, training loss: 905.3580932617188 = 0.15058475732803345 + 100.0 * 9.052075386047363
Epoch 1580, val loss: 0.496651828289032
Epoch 1590, training loss: 905.2830200195312 = 0.14874377846717834 + 100.0 * 9.051342964172363
Epoch 1590, val loss: 0.4988134801387787
Epoch 1600, training loss: 905.2592163085938 = 0.14694806933403015 + 100.0 * 9.051122665405273
Epoch 1600, val loss: 0.5001044869422913
Epoch 1610, training loss: 905.6898803710938 = 0.1451912820339203 + 100.0 * 9.05544662475586
Epoch 1610, val loss: 0.501656174659729
Epoch 1620, training loss: 905.4465942382812 = 0.14339357614517212 + 100.0 * 9.053031921386719
Epoch 1620, val loss: 0.5035432577133179
Epoch 1630, training loss: 905.1939086914062 = 0.14159299433231354 + 100.0 * 9.050522804260254
Epoch 1630, val loss: 0.504638135433197
Epoch 1640, training loss: 905.1809692382812 = 0.13981138169765472 + 100.0 * 9.050411224365234
Epoch 1640, val loss: 0.5064191818237305
Epoch 1650, training loss: 905.1536254882812 = 0.13805171847343445 + 100.0 * 9.050155639648438
Epoch 1650, val loss: 0.5080555081367493
Epoch 1660, training loss: 905.5562744140625 = 0.13634568452835083 + 100.0 * 9.05419921875
Epoch 1660, val loss: 0.5092555284500122
Epoch 1670, training loss: 905.2875366210938 = 0.13461922109127045 + 100.0 * 9.051528930664062
Epoch 1670, val loss: 0.5108320116996765
Epoch 1680, training loss: 905.1878662109375 = 0.13282370567321777 + 100.0 * 9.05055046081543
Epoch 1680, val loss: 0.5127846598625183
Epoch 1690, training loss: 905.0880126953125 = 0.13109253346920013 + 100.0 * 9.049569129943848
Epoch 1690, val loss: 0.5150333046913147
Epoch 1700, training loss: 905.0625610351562 = 0.1293942928314209 + 100.0 * 9.049331665039062
Epoch 1700, val loss: 0.5172544121742249
Epoch 1710, training loss: 905.2664184570312 = 0.12777796387672424 + 100.0 * 9.051385879516602
Epoch 1710, val loss: 0.5200284719467163
Epoch 1720, training loss: 905.0692138671875 = 0.1260528862476349 + 100.0 * 9.049431800842285
Epoch 1720, val loss: 0.5193289518356323
Epoch 1730, training loss: 905.068603515625 = 0.12437523156404495 + 100.0 * 9.049442291259766
Epoch 1730, val loss: 0.5232719779014587
Epoch 1740, training loss: 905.0538940429688 = 0.12275777012109756 + 100.0 * 9.049311637878418
Epoch 1740, val loss: 0.5253159999847412
Epoch 1750, training loss: 905.1371459960938 = 0.121200330555439 + 100.0 * 9.050159454345703
Epoch 1750, val loss: 0.5268504023551941
Epoch 1760, training loss: 904.9331665039062 = 0.11948410421609879 + 100.0 * 9.048136711120605
Epoch 1760, val loss: 0.5276302695274353
Epoch 1770, training loss: 904.9038696289062 = 0.11788403242826462 + 100.0 * 9.047860145568848
Epoch 1770, val loss: 0.529724657535553
Epoch 1780, training loss: 904.8773803710938 = 0.11631511896848679 + 100.0 * 9.04761028289795
Epoch 1780, val loss: 0.5316027998924255
Epoch 1790, training loss: 904.92724609375 = 0.11476387083530426 + 100.0 * 9.048125267028809
Epoch 1790, val loss: 0.5334579944610596
Epoch 1800, training loss: 905.1016845703125 = 0.11322973668575287 + 100.0 * 9.049884796142578
Epoch 1800, val loss: 0.535606861114502
Epoch 1810, training loss: 904.8961791992188 = 0.11169066280126572 + 100.0 * 9.047844886779785
Epoch 1810, val loss: 0.5376359820365906
Epoch 1820, training loss: 904.8042602539062 = 0.11014269292354584 + 100.0 * 9.046940803527832
Epoch 1820, val loss: 0.5396263599395752
Epoch 1830, training loss: 904.7725830078125 = 0.1086200550198555 + 100.0 * 9.046639442443848
Epoch 1830, val loss: 0.541781485080719
Epoch 1840, training loss: 904.91943359375 = 0.10713814944028854 + 100.0 * 9.048123359680176
Epoch 1840, val loss: 0.5443987846374512
Epoch 1850, training loss: 904.7787475585938 = 0.10563047230243683 + 100.0 * 9.046730995178223
Epoch 1850, val loss: 0.5457202792167664
Epoch 1860, training loss: 904.7691040039062 = 0.10418076813220978 + 100.0 * 9.046648979187012
Epoch 1860, val loss: 0.5492941737174988
Epoch 1870, training loss: 904.7821044921875 = 0.10270135849714279 + 100.0 * 9.046793937683105
Epoch 1870, val loss: 0.5510159730911255
Epoch 1880, training loss: 904.6842651367188 = 0.10122169554233551 + 100.0 * 9.045830726623535
Epoch 1880, val loss: 0.5526853799819946
Epoch 1890, training loss: 904.6901245117188 = 0.09979497641324997 + 100.0 * 9.045903205871582
Epoch 1890, val loss: 0.553916335105896
Epoch 1900, training loss: 904.8441772460938 = 0.09838885068893433 + 100.0 * 9.047457695007324
Epoch 1900, val loss: 0.5563197731971741
Epoch 1910, training loss: 904.6917724609375 = 0.09701263159513474 + 100.0 * 9.045948028564453
Epoch 1910, val loss: 0.5608038306236267
Epoch 1920, training loss: 904.816162109375 = 0.09562135487794876 + 100.0 * 9.047204971313477
Epoch 1920, val loss: 0.5623602271080017
Epoch 1930, training loss: 904.8461303710938 = 0.09426170587539673 + 100.0 * 9.047518730163574
Epoch 1930, val loss: 0.5626734495162964
Epoch 1940, training loss: 904.647216796875 = 0.09285296499729156 + 100.0 * 9.045543670654297
Epoch 1940, val loss: 0.5667105913162231
Epoch 1950, training loss: 904.5709838867188 = 0.09149443358182907 + 100.0 * 9.044795036315918
Epoch 1950, val loss: 0.5679694414138794
Epoch 1960, training loss: 904.5428466796875 = 0.0901549756526947 + 100.0 * 9.044527053833008
Epoch 1960, val loss: 0.5708550810813904
Epoch 1970, training loss: 904.5768432617188 = 0.08884839713573456 + 100.0 * 9.044879913330078
Epoch 1970, val loss: 0.5736818909645081
Epoch 1980, training loss: 904.7060546875 = 0.08756855130195618 + 100.0 * 9.046184539794922
Epoch 1980, val loss: 0.5762544870376587
Epoch 1990, training loss: 904.5472412109375 = 0.08626724034547806 + 100.0 * 9.044610023498535
Epoch 1990, val loss: 0.577653706073761
Epoch 2000, training loss: 904.65625 = 0.08510423451662064 + 100.0 * 9.045711517333984
Epoch 2000, val loss: 0.5826199650764465
Epoch 2010, training loss: 904.4442138671875 = 0.0837264284491539 + 100.0 * 9.043604850769043
Epoch 2010, val loss: 0.5821325778961182
Epoch 2020, training loss: 904.423095703125 = 0.08248192816972733 + 100.0 * 9.04340648651123
Epoch 2020, val loss: 0.5857675671577454
Epoch 2030, training loss: 904.45166015625 = 0.0812624841928482 + 100.0 * 9.04370403289795
Epoch 2030, val loss: 0.5880257487297058
Epoch 2040, training loss: 904.7282104492188 = 0.08006693422794342 + 100.0 * 9.046481132507324
Epoch 2040, val loss: 0.5904895067214966
Epoch 2050, training loss: 904.482666015625 = 0.07888732105493546 + 100.0 * 9.044037818908691
Epoch 2050, val loss: 0.5923761129379272
Epoch 2060, training loss: 904.3771362304688 = 0.07767985016107559 + 100.0 * 9.042994499206543
Epoch 2060, val loss: 0.5952587127685547
Epoch 2070, training loss: 904.4163208007812 = 0.07655128091573715 + 100.0 * 9.043397903442383
Epoch 2070, val loss: 0.5973148941993713
Epoch 2080, training loss: 904.4810180664062 = 0.07542462646961212 + 100.0 * 9.044055938720703
Epoch 2080, val loss: 0.5998031497001648
Epoch 2090, training loss: 904.5438842773438 = 0.07427634298801422 + 100.0 * 9.044695854187012
Epoch 2090, val loss: 0.6019589304924011
Epoch 2100, training loss: 904.3367309570312 = 0.07314268499612808 + 100.0 * 9.042635917663574
Epoch 2100, val loss: 0.6052871346473694
Epoch 2110, training loss: 904.29296875 = 0.07205026596784592 + 100.0 * 9.04220962524414
Epoch 2110, val loss: 0.6091351509094238
Epoch 2120, training loss: 904.2767944335938 = 0.0709557831287384 + 100.0 * 9.042057991027832
Epoch 2120, val loss: 0.6114833950996399
Epoch 2130, training loss: 904.4922485351562 = 0.0699554830789566 + 100.0 * 9.044222831726074
Epoch 2130, val loss: 0.6144080758094788
Epoch 2140, training loss: 904.3107299804688 = 0.0688931941986084 + 100.0 * 9.042418479919434
Epoch 2140, val loss: 0.6140879988670349
Epoch 2150, training loss: 904.226318359375 = 0.06780082732439041 + 100.0 * 9.041584968566895
Epoch 2150, val loss: 0.618796169757843
Epoch 2160, training loss: 904.2013549804688 = 0.06678105890750885 + 100.0 * 9.041345596313477
Epoch 2160, val loss: 0.6199840903282166
Epoch 2170, training loss: 904.3465576171875 = 0.0657937228679657 + 100.0 * 9.042807579040527
Epoch 2170, val loss: 0.6220991611480713
Epoch 2180, training loss: 904.171875 = 0.06478244811296463 + 100.0 * 9.041070938110352
Epoch 2180, val loss: 0.6254220008850098
Epoch 2190, training loss: 904.1858520507812 = 0.0637960210442543 + 100.0 * 9.041220664978027
Epoch 2190, val loss: 0.6278617978096008
Epoch 2200, training loss: 904.2877197265625 = 0.06291211396455765 + 100.0 * 9.042247772216797
Epoch 2200, val loss: 0.6291843056678772
Epoch 2210, training loss: 904.2260131835938 = 0.061899714171886444 + 100.0 * 9.041641235351562
Epoch 2210, val loss: 0.6345921754837036
Epoch 2220, training loss: 904.1696166992188 = 0.06095237657427788 + 100.0 * 9.041086196899414
Epoch 2220, val loss: 0.6364080309867859
Epoch 2230, training loss: 904.1639404296875 = 0.0600360743701458 + 100.0 * 9.041038513183594
Epoch 2230, val loss: 0.6387054324150085
Epoch 2240, training loss: 904.1783447265625 = 0.05913303792476654 + 100.0 * 9.041192054748535
Epoch 2240, val loss: 0.6416850090026855
Epoch 2250, training loss: 904.0846557617188 = 0.05822719633579254 + 100.0 * 9.040264129638672
Epoch 2250, val loss: 0.643883228302002
Epoch 2260, training loss: 904.188232421875 = 0.057354886084795 + 100.0 * 9.041308403015137
Epoch 2260, val loss: 0.6477601528167725
Epoch 2270, training loss: 904.0789794921875 = 0.056482478976249695 + 100.0 * 9.0402250289917
Epoch 2270, val loss: 0.6500356197357178
Epoch 2280, training loss: 904.1270141601562 = 0.055632736533880234 + 100.0 * 9.040714263916016
Epoch 2280, val loss: 0.6528635025024414
Epoch 2290, training loss: 904.0771484375 = 0.054790645837783813 + 100.0 * 9.040223121643066
Epoch 2290, val loss: 0.6556885242462158
Epoch 2300, training loss: 904.0357666015625 = 0.053958505392074585 + 100.0 * 9.039817810058594
Epoch 2300, val loss: 0.656579852104187
Epoch 2310, training loss: 904.1497802734375 = 0.05317613109946251 + 100.0 * 9.040966033935547
Epoch 2310, val loss: 0.6592909693717957
Epoch 2320, training loss: 904.0765380859375 = 0.052481137216091156 + 100.0 * 9.040240287780762
Epoch 2320, val loss: 0.6656674742698669
Epoch 2330, training loss: 904.0518188476562 = 0.051543064415454865 + 100.0 * 9.040002822875977
Epoch 2330, val loss: 0.6655353307723999
Epoch 2340, training loss: 904.060302734375 = 0.05078067258000374 + 100.0 * 9.040095329284668
Epoch 2340, val loss: 0.6688739657402039
Epoch 2350, training loss: 903.9761962890625 = 0.04999386519193649 + 100.0 * 9.039261817932129
Epoch 2350, val loss: 0.6696320176124573
Epoch 2360, training loss: 903.9275512695312 = 0.04924030974507332 + 100.0 * 9.038783073425293
Epoch 2360, val loss: 0.6737728714942932
Epoch 2370, training loss: 903.8914794921875 = 0.04848196357488632 + 100.0 * 9.038430213928223
Epoch 2370, val loss: 0.6754616498947144
Epoch 2380, training loss: 903.929443359375 = 0.04777137190103531 + 100.0 * 9.038816452026367
Epoch 2380, val loss: 0.6802016496658325
Epoch 2390, training loss: 904.0511474609375 = 0.047089483588933945 + 100.0 * 9.040040016174316
Epoch 2390, val loss: 0.6818133592605591
Epoch 2400, training loss: 903.853759765625 = 0.04633970186114311 + 100.0 * 9.038074493408203
Epoch 2400, val loss: 0.684251070022583
Epoch 2410, training loss: 903.857666015625 = 0.045638181269168854 + 100.0 * 9.03812026977539
Epoch 2410, val loss: 0.6850705742835999
Epoch 2420, training loss: 903.8382568359375 = 0.04493167623877525 + 100.0 * 9.037933349609375
Epoch 2420, val loss: 0.6887930035591125
Epoch 2430, training loss: 904.3043823242188 = 0.044425688683986664 + 100.0 * 9.04259967803955
Epoch 2430, val loss: 0.6893889904022217
Epoch 2440, training loss: 903.9365234375 = 0.043626025319099426 + 100.0 * 9.038928985595703
Epoch 2440, val loss: 0.6958979964256287
Epoch 2450, training loss: 903.7940673828125 = 0.04294946789741516 + 100.0 * 9.037510871887207
Epoch 2450, val loss: 0.6972286701202393
Epoch 2460, training loss: 903.8477783203125 = 0.042338695377111435 + 100.0 * 9.038054466247559
Epoch 2460, val loss: 0.6988236904144287
Epoch 2470, training loss: 903.8015747070312 = 0.04167817905545235 + 100.0 * 9.037598609924316
Epoch 2470, val loss: 0.7026259303092957
Epoch 2480, training loss: 903.80712890625 = 0.041052158921957016 + 100.0 * 9.037660598754883
Epoch 2480, val loss: 0.7053772211074829
Epoch 2490, training loss: 904.0664672851562 = 0.04047861322760582 + 100.0 * 9.040260314941406
Epoch 2490, val loss: 0.7078092098236084
Epoch 2500, training loss: 903.8408203125 = 0.0398421511054039 + 100.0 * 9.038009643554688
Epoch 2500, val loss: 0.7102759480476379
Epoch 2510, training loss: 903.7333984375 = 0.039250947535037994 + 100.0 * 9.036941528320312
Epoch 2510, val loss: 0.7126696109771729
Epoch 2520, training loss: 903.7061767578125 = 0.038640737533569336 + 100.0 * 9.036675453186035
Epoch 2520, val loss: 0.7158191204071045
Epoch 2530, training loss: 903.8690795898438 = 0.03811901807785034 + 100.0 * 9.038309097290039
Epoch 2530, val loss: 0.7173802852630615
Epoch 2540, training loss: 903.6557006835938 = 0.03750559315085411 + 100.0 * 9.036182403564453
Epoch 2540, val loss: 0.7218242287635803
Epoch 2550, training loss: 903.6973876953125 = 0.036953285336494446 + 100.0 * 9.036604881286621
Epoch 2550, val loss: 0.7245296239852905
Epoch 2560, training loss: 903.7982177734375 = 0.03641749545931816 + 100.0 * 9.037617683410645
Epoch 2560, val loss: 0.7273246645927429
Epoch 2570, training loss: 903.7886962890625 = 0.03587651997804642 + 100.0 * 9.037528038024902
Epoch 2570, val loss: 0.7283958196640015
Epoch 2580, training loss: 903.6548461914062 = 0.03533322736620903 + 100.0 * 9.036194801330566
Epoch 2580, val loss: 0.7314613461494446
Epoch 2590, training loss: 903.6128540039062 = 0.03480486944317818 + 100.0 * 9.035780906677246
Epoch 2590, val loss: 0.7338871955871582
Epoch 2600, training loss: 903.6226806640625 = 0.03429631143808365 + 100.0 * 9.035883903503418
Epoch 2600, val loss: 0.7366994619369507
Epoch 2610, training loss: 903.7846069335938 = 0.03379195183515549 + 100.0 * 9.037508010864258
Epoch 2610, val loss: 0.7395206093788147
Epoch 2620, training loss: 903.627197265625 = 0.033291179686784744 + 100.0 * 9.03593921661377
Epoch 2620, val loss: 0.7417241334915161
Epoch 2630, training loss: 903.6180419921875 = 0.03279354050755501 + 100.0 * 9.035852432250977
Epoch 2630, val loss: 0.7441238164901733
Epoch 2640, training loss: 903.5789184570312 = 0.0323038212954998 + 100.0 * 9.035466194152832
Epoch 2640, val loss: 0.7473737597465515
Epoch 2650, training loss: 903.5582885742188 = 0.031857434660196304 + 100.0 * 9.035264015197754
Epoch 2650, val loss: 0.7508703470230103
Epoch 2660, training loss: 903.6273193359375 = 0.03139762207865715 + 100.0 * 9.035959243774414
Epoch 2660, val loss: 0.7530803084373474
Epoch 2670, training loss: 903.547607421875 = 0.03092433698475361 + 100.0 * 9.03516674041748
Epoch 2670, val loss: 0.7556893229484558
Epoch 2680, training loss: 903.5193481445312 = 0.030467716977000237 + 100.0 * 9.034889221191406
Epoch 2680, val loss: 0.7568559646606445
Epoch 2690, training loss: 903.4982299804688 = 0.030012115836143494 + 100.0 * 9.034682273864746
Epoch 2690, val loss: 0.7596577405929565
Epoch 2700, training loss: 903.6715087890625 = 0.029597537592053413 + 100.0 * 9.036418914794922
Epoch 2700, val loss: 0.7611298561096191
Epoch 2710, training loss: 903.7000732421875 = 0.0293072909116745 + 100.0 * 9.036707878112793
Epoch 2710, val loss: 0.7614865303039551
Epoch 2720, training loss: 903.5067749023438 = 0.02876037359237671 + 100.0 * 9.034780502319336
Epoch 2720, val loss: 0.7684019207954407
Epoch 2730, training loss: 903.4220581054688 = 0.028339790180325508 + 100.0 * 9.033937454223633
Epoch 2730, val loss: 0.7684241533279419
Epoch 2740, training loss: 903.4078369140625 = 0.027922844514250755 + 100.0 * 9.033799171447754
Epoch 2740, val loss: 0.7719720602035522
Epoch 2750, training loss: 903.60205078125 = 0.027579428628087044 + 100.0 * 9.035744667053223
Epoch 2750, val loss: 0.7730359435081482
Epoch 2760, training loss: 903.4219360351562 = 0.027156904339790344 + 100.0 * 9.033947944641113
Epoch 2760, val loss: 0.776477575302124
Epoch 2770, training loss: 903.461669921875 = 0.026766400784254074 + 100.0 * 9.034348487854004
Epoch 2770, val loss: 0.7787615656852722
Epoch 2780, training loss: 903.5506591796875 = 0.026401599869132042 + 100.0 * 9.035242080688477
Epoch 2780, val loss: 0.7816766500473022
Epoch 2790, training loss: 903.3729248046875 = 0.026021551340818405 + 100.0 * 9.033469200134277
Epoch 2790, val loss: 0.7847481369972229
Epoch 2800, training loss: 903.3635864257812 = 0.025676095858216286 + 100.0 * 9.033378601074219
Epoch 2800, val loss: 0.7863485217094421
Epoch 2810, training loss: 903.4520874023438 = 0.025325587019324303 + 100.0 * 9.03426742553711
Epoch 2810, val loss: 0.7887909412384033
Epoch 2820, training loss: 903.3348999023438 = 0.02495647594332695 + 100.0 * 9.033099174499512
Epoch 2820, val loss: 0.7926440834999084
Epoch 2830, training loss: 903.341552734375 = 0.024624548852443695 + 100.0 * 9.033169746398926
Epoch 2830, val loss: 0.7958988547325134
Epoch 2840, training loss: 903.4443359375 = 0.02429775521159172 + 100.0 * 9.034200668334961
Epoch 2840, val loss: 0.7985031008720398
Epoch 2850, training loss: 903.3269653320312 = 0.023960605263710022 + 100.0 * 9.033029556274414
Epoch 2850, val loss: 0.7998149394989014
Epoch 2860, training loss: 903.39306640625 = 0.023648440837860107 + 100.0 * 9.03369426727295
Epoch 2860, val loss: 0.8019497394561768
Epoch 2870, training loss: 903.3358154296875 = 0.02334509789943695 + 100.0 * 9.033124923706055
Epoch 2870, val loss: 0.8041452169418335
Epoch 2880, training loss: 903.2950439453125 = 0.023003777489066124 + 100.0 * 9.032720565795898
Epoch 2880, val loss: 0.806972086429596
Epoch 2890, training loss: 903.3350830078125 = 0.02271384932100773 + 100.0 * 9.033123970031738
Epoch 2890, val loss: 0.8107914328575134
Epoch 2900, training loss: 903.2847290039062 = 0.02240942418575287 + 100.0 * 9.032623291015625
Epoch 2900, val loss: 0.8132253885269165
Epoch 2910, training loss: 903.2644653320312 = 0.02213573083281517 + 100.0 * 9.03242301940918
Epoch 2910, val loss: 0.8159266114234924
Epoch 2920, training loss: 903.23583984375 = 0.021820439025759697 + 100.0 * 9.032139778137207
Epoch 2920, val loss: 0.8177201747894287
Epoch 2930, training loss: 903.2778930664062 = 0.02154693566262722 + 100.0 * 9.032563209533691
Epoch 2930, val loss: 0.8200105428695679
Epoch 2940, training loss: 903.3657836914062 = 0.021267041563987732 + 100.0 * 9.033445358276367
Epoch 2940, val loss: 0.8203360438346863
Epoch 2950, training loss: 903.234619140625 = 0.02100476622581482 + 100.0 * 9.032135963439941
Epoch 2950, val loss: 0.8229441046714783
Epoch 2960, training loss: 903.1879272460938 = 0.02071256749331951 + 100.0 * 9.031672477722168
Epoch 2960, val loss: 0.8259091377258301
Epoch 2970, training loss: 903.2818603515625 = 0.0204820204526186 + 100.0 * 9.032613754272461
Epoch 2970, val loss: 0.8262399435043335
Epoch 2980, training loss: 903.2835693359375 = 0.02020427957177162 + 100.0 * 9.032633781433105
Epoch 2980, val loss: 0.8301779627799988
Epoch 2990, training loss: 903.1290893554688 = 0.019925221800804138 + 100.0 * 9.031091690063477
Epoch 2990, val loss: 0.8327808976173401
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8437
Overall ASR: 0.7312
Flip ASR: 0.6654/1554 nodes
The final ASR:0.72346, 0.00550, Accuracy:0.84289, 0.00810
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97536])
remove edge: torch.Size([2, 79708])
updated graph: torch.Size([2, 88596])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7388
Flip ASR: 0.6763/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72126, 0.01618, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1037.009033203125 = 1.1044210195541382 + 100.0 * 10.359046936035156
Epoch 0, val loss: 1.1028050184249878
Epoch 10, training loss: 1036.6527099609375 = 1.0934650897979736 + 100.0 * 10.355591773986816
Epoch 10, val loss: 1.0915926694869995
Epoch 20, training loss: 1031.1348876953125 = 1.0782039165496826 + 100.0 * 10.300566673278809
Epoch 20, val loss: 1.0763658285140991
Epoch 30, training loss: 974.2254638671875 = 1.0612633228302002 + 100.0 * 9.73164176940918
Epoch 30, val loss: 1.059512734413147
Epoch 40, training loss: 945.5196533203125 = 1.0436573028564453 + 100.0 * 9.4447603225708
Epoch 40, val loss: 1.0423932075500488
Epoch 50, training loss: 940.55224609375 = 1.0255924463272095 + 100.0 * 9.39526653289795
Epoch 50, val loss: 1.0244059562683105
Epoch 60, training loss: 939.03369140625 = 1.0089539289474487 + 100.0 * 9.380247116088867
Epoch 60, val loss: 1.0083104372024536
Epoch 70, training loss: 936.4400024414062 = 0.9959107637405396 + 100.0 * 9.354440689086914
Epoch 70, val loss: 0.9956570267677307
Epoch 80, training loss: 933.2808837890625 = 0.985866129398346 + 100.0 * 9.32295036315918
Epoch 80, val loss: 0.9856923222541809
Epoch 90, training loss: 928.6572875976562 = 0.9756761789321899 + 100.0 * 9.276816368103027
Epoch 90, val loss: 0.9753928184509277
Epoch 100, training loss: 925.2636108398438 = 0.9646798968315125 + 100.0 * 9.242989540100098
Epoch 100, val loss: 0.9643760323524475
Epoch 110, training loss: 923.0205688476562 = 0.9523583650588989 + 100.0 * 9.220682144165039
Epoch 110, val loss: 0.9522718787193298
Epoch 120, training loss: 920.2012329101562 = 0.9419649243354797 + 100.0 * 9.19259262084961
Epoch 120, val loss: 0.9426747560501099
Epoch 130, training loss: 918.5372314453125 = 0.9344888925552368 + 100.0 * 9.176027297973633
Epoch 130, val loss: 0.9352335929870605
Epoch 140, training loss: 917.9652099609375 = 0.9216556549072266 + 100.0 * 9.170435905456543
Epoch 140, val loss: 0.9221060872077942
Epoch 150, training loss: 917.3319091796875 = 0.9063734412193298 + 100.0 * 9.164255142211914
Epoch 150, val loss: 0.907497227191925
Epoch 160, training loss: 916.4517211914062 = 0.8945820331573486 + 100.0 * 9.155570983886719
Epoch 160, val loss: 0.8964864611625671
Epoch 170, training loss: 915.3511962890625 = 0.8861634731292725 + 100.0 * 9.14465045928955
Epoch 170, val loss: 0.8885908126831055
Epoch 180, training loss: 914.2562866210938 = 0.8787313103675842 + 100.0 * 9.13377571105957
Epoch 180, val loss: 0.8812592029571533
Epoch 190, training loss: 913.3998413085938 = 0.8682789206504822 + 100.0 * 9.12531566619873
Epoch 190, val loss: 0.8708160519599915
Epoch 200, training loss: 912.8034057617188 = 0.8559209108352661 + 100.0 * 9.119475364685059
Epoch 200, val loss: 0.8588756918907166
Epoch 210, training loss: 912.217529296875 = 0.843157947063446 + 100.0 * 9.113743782043457
Epoch 210, val loss: 0.8467534780502319
Epoch 220, training loss: 911.7269897460938 = 0.830152690410614 + 100.0 * 9.108968734741211
Epoch 220, val loss: 0.8341507911682129
Epoch 230, training loss: 911.4293823242188 = 0.8166170120239258 + 100.0 * 9.106127738952637
Epoch 230, val loss: 0.8213126063346863
Epoch 240, training loss: 910.9635009765625 = 0.8027916550636292 + 100.0 * 9.101607322692871
Epoch 240, val loss: 0.8081223368644714
Epoch 250, training loss: 910.7195434570312 = 0.7892152070999146 + 100.0 * 9.099303245544434
Epoch 250, val loss: 0.7950429320335388
Epoch 260, training loss: 910.323486328125 = 0.7752890586853027 + 100.0 * 9.095481872558594
Epoch 260, val loss: 0.7822969555854797
Epoch 270, training loss: 909.962158203125 = 0.7620695233345032 + 100.0 * 9.092000961303711
Epoch 270, val loss: 0.7697468996047974
Epoch 280, training loss: 909.691162109375 = 0.748636782169342 + 100.0 * 9.089425086975098
Epoch 280, val loss: 0.7571493983268738
Epoch 290, training loss: 909.5095825195312 = 0.7342517375946045 + 100.0 * 9.087753295898438
Epoch 290, val loss: 0.7433220744132996
Epoch 300, training loss: 909.2935180664062 = 0.7195786833763123 + 100.0 * 9.085739135742188
Epoch 300, val loss: 0.7297369241714478
Epoch 310, training loss: 909.0425415039062 = 0.7061601877212524 + 100.0 * 9.08336353302002
Epoch 310, val loss: 0.7172046899795532
Epoch 320, training loss: 908.7791748046875 = 0.693251371383667 + 100.0 * 9.080859184265137
Epoch 320, val loss: 0.7050631046295166
Epoch 330, training loss: 908.8385620117188 = 0.679819643497467 + 100.0 * 9.081587791442871
Epoch 330, val loss: 0.692093551158905
Epoch 340, training loss: 908.4740600585938 = 0.6657821536064148 + 100.0 * 9.078083038330078
Epoch 340, val loss: 0.6796149611473083
Epoch 350, training loss: 908.197998046875 = 0.6539531946182251 + 100.0 * 9.075440406799316
Epoch 350, val loss: 0.6684026122093201
Epoch 360, training loss: 907.9397583007812 = 0.6423386931419373 + 100.0 * 9.07297420501709
Epoch 360, val loss: 0.6578976511955261
Epoch 370, training loss: 907.7079467773438 = 0.6308215856552124 + 100.0 * 9.070771217346191
Epoch 370, val loss: 0.6471810340881348
Epoch 380, training loss: 907.5313110351562 = 0.6189786195755005 + 100.0 * 9.069123268127441
Epoch 380, val loss: 0.6361531615257263
Epoch 390, training loss: 907.4930419921875 = 0.6069018840789795 + 100.0 * 9.068861961364746
Epoch 390, val loss: 0.6251449584960938
Epoch 400, training loss: 907.203125 = 0.5957239866256714 + 100.0 * 9.06607437133789
Epoch 400, val loss: 0.6151750087738037
Epoch 410, training loss: 907.01171875 = 0.585204005241394 + 100.0 * 9.064265251159668
Epoch 410, val loss: 0.6055957674980164
Epoch 420, training loss: 907.0029296875 = 0.5749803185462952 + 100.0 * 9.064279556274414
Epoch 420, val loss: 0.596159040927887
Epoch 430, training loss: 906.692626953125 = 0.5640427470207214 + 100.0 * 9.061285972595215
Epoch 430, val loss: 0.5864429473876953
Epoch 440, training loss: 906.531982421875 = 0.5543137192726135 + 100.0 * 9.059776306152344
Epoch 440, val loss: 0.5778324604034424
Epoch 450, training loss: 906.4037475585938 = 0.5452359318733215 + 100.0 * 9.058585166931152
Epoch 450, val loss: 0.5698111057281494
Epoch 460, training loss: 906.6444702148438 = 0.5360845327377319 + 100.0 * 9.061083793640137
Epoch 460, val loss: 0.5614650845527649
Epoch 470, training loss: 906.1962890625 = 0.5266230702400208 + 100.0 * 9.056696891784668
Epoch 470, val loss: 0.5534631609916687
Epoch 480, training loss: 906.05908203125 = 0.5185990929603577 + 100.0 * 9.055404663085938
Epoch 480, val loss: 0.5465097427368164
Epoch 490, training loss: 905.9359130859375 = 0.5110352039337158 + 100.0 * 9.054248809814453
Epoch 490, val loss: 0.5400017499923706
Epoch 500, training loss: 906.1539306640625 = 0.5037038922309875 + 100.0 * 9.056502342224121
Epoch 500, val loss: 0.5333391427993774
Epoch 510, training loss: 905.7589111328125 = 0.49558404088020325 + 100.0 * 9.052633285522461
Epoch 510, val loss: 0.5265332460403442
Epoch 520, training loss: 905.68701171875 = 0.4888194799423218 + 100.0 * 9.051981925964355
Epoch 520, val loss: 0.5209237337112427
Epoch 530, training loss: 905.510498046875 = 0.48259487748146057 + 100.0 * 9.050278663635254
Epoch 530, val loss: 0.5158740878105164
Epoch 540, training loss: 905.4244995117188 = 0.4765346050262451 + 100.0 * 9.049479484558105
Epoch 540, val loss: 0.5108242630958557
Epoch 550, training loss: 905.4421997070312 = 0.4703854024410248 + 100.0 * 9.049717903137207
Epoch 550, val loss: 0.5057424306869507
Epoch 560, training loss: 905.341552734375 = 0.4644830524921417 + 100.0 * 9.048770904541016
Epoch 560, val loss: 0.5009190440177917
Epoch 570, training loss: 905.167724609375 = 0.4592258930206299 + 100.0 * 9.04708480834961
Epoch 570, val loss: 0.4965555965900421
Epoch 580, training loss: 905.065185546875 = 0.4541791081428528 + 100.0 * 9.046110153198242
Epoch 580, val loss: 0.4925069212913513
Epoch 590, training loss: 905.4382934570312 = 0.44903913140296936 + 100.0 * 9.04989242553711
Epoch 590, val loss: 0.48834753036499023
Epoch 600, training loss: 905.0648803710938 = 0.44375696778297424 + 100.0 * 9.046211242675781
Epoch 600, val loss: 0.484126478433609
Epoch 610, training loss: 904.8819580078125 = 0.4393405020236969 + 100.0 * 9.044425964355469
Epoch 610, val loss: 0.4805961549282074
Epoch 620, training loss: 904.78759765625 = 0.43507498502731323 + 100.0 * 9.043525695800781
Epoch 620, val loss: 0.477364718914032
Epoch 630, training loss: 904.923583984375 = 0.430850625038147 + 100.0 * 9.044927597045898
Epoch 630, val loss: 0.4741014838218689
Epoch 640, training loss: 904.8268432617188 = 0.42657536268234253 + 100.0 * 9.044002532958984
Epoch 640, val loss: 0.4710036516189575
Epoch 650, training loss: 904.6375732421875 = 0.4225560426712036 + 100.0 * 9.042150497436523
Epoch 650, val loss: 0.46799248456954956
Epoch 660, training loss: 904.5931396484375 = 0.4189026355743408 + 100.0 * 9.041742324829102
Epoch 660, val loss: 0.4652007222175598
Epoch 670, training loss: 904.59521484375 = 0.4153439998626709 + 100.0 * 9.04179859161377
Epoch 670, val loss: 0.4627431035041809
Epoch 680, training loss: 904.4528198242188 = 0.4119375944137573 + 100.0 * 9.040409088134766
Epoch 680, val loss: 0.46002283692359924
Epoch 690, training loss: 904.3674926757812 = 0.40865442156791687 + 100.0 * 9.039588928222656
Epoch 690, val loss: 0.45780134201049805
Epoch 700, training loss: 904.4007568359375 = 0.40547704696655273 + 100.0 * 9.039953231811523
Epoch 700, val loss: 0.45553165674209595
Epoch 710, training loss: 904.48876953125 = 0.40225541591644287 + 100.0 * 9.040864944458008
Epoch 710, val loss: 0.4532306492328644
Epoch 720, training loss: 904.3369140625 = 0.39899519085884094 + 100.0 * 9.039379119873047
Epoch 720, val loss: 0.4511818289756775
Epoch 730, training loss: 904.2041015625 = 0.3962275981903076 + 100.0 * 9.038078308105469
Epoch 730, val loss: 0.44922900199890137
Epoch 740, training loss: 904.1270751953125 = 0.39359262585639954 + 100.0 * 9.037334442138672
Epoch 740, val loss: 0.44754162430763245
Epoch 750, training loss: 904.1351318359375 = 0.39096322655677795 + 100.0 * 9.03744125366211
Epoch 750, val loss: 0.4458332061767578
Epoch 760, training loss: 904.0934448242188 = 0.3881293833255768 + 100.0 * 9.037053108215332
Epoch 760, val loss: 0.4439198672771454
Epoch 770, training loss: 903.9957885742188 = 0.3855763077735901 + 100.0 * 9.036102294921875
Epoch 770, val loss: 0.442350834608078
Epoch 780, training loss: 903.9468994140625 = 0.3832869529724121 + 100.0 * 9.035635948181152
Epoch 780, val loss: 0.4409767687320709
Epoch 790, training loss: 903.9048461914062 = 0.3810392916202545 + 100.0 * 9.035238265991211
Epoch 790, val loss: 0.4395975172519684
Epoch 800, training loss: 903.8858032226562 = 0.37880927324295044 + 100.0 * 9.035070419311523
Epoch 800, val loss: 0.43824678659439087
Epoch 810, training loss: 904.0386962890625 = 0.3764427602291107 + 100.0 * 9.036622047424316
Epoch 810, val loss: 0.4368290901184082
Epoch 820, training loss: 903.9252319335938 = 0.37420132756233215 + 100.0 * 9.035510063171387
Epoch 820, val loss: 0.435473769903183
Epoch 830, training loss: 903.765869140625 = 0.3722151517868042 + 100.0 * 9.033936500549316
Epoch 830, val loss: 0.43435072898864746
Epoch 840, training loss: 903.766845703125 = 0.37025684118270874 + 100.0 * 9.033966064453125
Epoch 840, val loss: 0.4332141876220703
Epoch 850, training loss: 903.8068237304688 = 0.3682313561439514 + 100.0 * 9.034385681152344
Epoch 850, val loss: 0.4321412742137909
Epoch 860, training loss: 903.6602783203125 = 0.36625027656555176 + 100.0 * 9.032939910888672
Epoch 860, val loss: 0.4312148094177246
Epoch 870, training loss: 903.6192626953125 = 0.3644382357597351 + 100.0 * 9.032547950744629
Epoch 870, val loss: 0.4301755428314209
Epoch 880, training loss: 903.6949462890625 = 0.3625856339931488 + 100.0 * 9.033323287963867
Epoch 880, val loss: 0.4294081926345825
Epoch 890, training loss: 903.5557250976562 = 0.3607295751571655 + 100.0 * 9.031949996948242
Epoch 890, val loss: 0.4281354546546936
Epoch 900, training loss: 903.5303344726562 = 0.3589801490306854 + 100.0 * 9.031713485717773
Epoch 900, val loss: 0.42740684747695923
Epoch 910, training loss: 903.6210327148438 = 0.3572757840156555 + 100.0 * 9.032637596130371
Epoch 910, val loss: 0.4263884127140045
Epoch 920, training loss: 903.6677856445312 = 0.3555668592453003 + 100.0 * 9.033122062683105
Epoch 920, val loss: 0.4259008765220642
Epoch 930, training loss: 903.4319458007812 = 0.35388413071632385 + 100.0 * 9.030780792236328
Epoch 930, val loss: 0.42498812079429626
Epoch 940, training loss: 903.3701782226562 = 0.3523372709751129 + 100.0 * 9.03017807006836
Epoch 940, val loss: 0.42415109276771545
Epoch 950, training loss: 903.4774780273438 = 0.35079917311668396 + 100.0 * 9.031267166137695
Epoch 950, val loss: 0.42360058426856995
Epoch 960, training loss: 903.3174438476562 = 0.34914305806159973 + 100.0 * 9.029683113098145
Epoch 960, val loss: 0.4228314459323883
Epoch 970, training loss: 903.3143310546875 = 0.34767407178878784 + 100.0 * 9.029666900634766
Epoch 970, val loss: 0.42221274971961975
Epoch 980, training loss: 903.4794921875 = 0.34616610407829285 + 100.0 * 9.031332969665527
Epoch 980, val loss: 0.4217482805252075
Epoch 990, training loss: 903.2352905273438 = 0.34471961855888367 + 100.0 * 9.028905868530273
Epoch 990, val loss: 0.42079004645347595
Epoch 1000, training loss: 903.182861328125 = 0.34331440925598145 + 100.0 * 9.028395652770996
Epoch 1000, val loss: 0.420367568731308
Epoch 1010, training loss: 903.343994140625 = 0.34194210171699524 + 100.0 * 9.030020713806152
Epoch 1010, val loss: 0.4196692109107971
Epoch 1020, training loss: 903.1342163085938 = 0.34046292304992676 + 100.0 * 9.027937889099121
Epoch 1020, val loss: 0.41928404569625854
Epoch 1030, training loss: 903.095703125 = 0.3391294479370117 + 100.0 * 9.027565956115723
Epoch 1030, val loss: 0.4186764657497406
Epoch 1040, training loss: 903.27099609375 = 0.33774468302726746 + 100.0 * 9.029332160949707
Epoch 1040, val loss: 0.41826510429382324
Epoch 1050, training loss: 903.2644653320312 = 0.33644214272499084 + 100.0 * 9.029280662536621
Epoch 1050, val loss: 0.4176701605319977
Epoch 1060, training loss: 903.0081176757812 = 0.3350704610347748 + 100.0 * 9.02673053741455
Epoch 1060, val loss: 0.41706281900405884
Epoch 1070, training loss: 902.964111328125 = 0.3338233530521393 + 100.0 * 9.0263032913208
Epoch 1070, val loss: 0.4167482554912567
Epoch 1080, training loss: 903.1986694335938 = 0.3325594365596771 + 100.0 * 9.028660774230957
Epoch 1080, val loss: 0.4162420332431793
Epoch 1090, training loss: 903.021728515625 = 0.3312445282936096 + 100.0 * 9.026905059814453
Epoch 1090, val loss: 0.41574564576148987
Epoch 1100, training loss: 902.8924560546875 = 0.33003076910972595 + 100.0 * 9.02562427520752
Epoch 1100, val loss: 0.41537201404571533
Epoch 1110, training loss: 903.0474853515625 = 0.32883507013320923 + 100.0 * 9.027186393737793
Epoch 1110, val loss: 0.4149194061756134
Epoch 1120, training loss: 902.9020385742188 = 0.3275734782218933 + 100.0 * 9.025744438171387
Epoch 1120, val loss: 0.41447144746780396
Epoch 1130, training loss: 902.81982421875 = 0.3264046609401703 + 100.0 * 9.024933815002441
Epoch 1130, val loss: 0.414101243019104
Epoch 1140, training loss: 902.7653198242188 = 0.32524406909942627 + 100.0 * 9.02440071105957
Epoch 1140, val loss: 0.413687139749527
Epoch 1150, training loss: 902.8731689453125 = 0.3240850269794464 + 100.0 * 9.025490760803223
Epoch 1150, val loss: 0.4133147895336151
Epoch 1160, training loss: 902.75830078125 = 0.3228522837162018 + 100.0 * 9.024353981018066
Epoch 1160, val loss: 0.4130706787109375
Epoch 1170, training loss: 902.8292236328125 = 0.3217458128929138 + 100.0 * 9.02507495880127
Epoch 1170, val loss: 0.4125064015388489
Epoch 1180, training loss: 902.7064208984375 = 0.32060706615448 + 100.0 * 9.023858070373535
Epoch 1180, val loss: 0.4122419059276581
Epoch 1190, training loss: 902.6671752929688 = 0.31952425837516785 + 100.0 * 9.023476600646973
Epoch 1190, val loss: 0.4119061529636383
Epoch 1200, training loss: 902.628662109375 = 0.3184575140476227 + 100.0 * 9.023101806640625
Epoch 1200, val loss: 0.41162893176078796
Epoch 1210, training loss: 903.021240234375 = 0.3173615336418152 + 100.0 * 9.02703857421875
Epoch 1210, val loss: 0.41113337874412537
Epoch 1220, training loss: 902.6995239257812 = 0.31624749302864075 + 100.0 * 9.023833274841309
Epoch 1220, val loss: 0.4109957218170166
Epoch 1230, training loss: 902.5343017578125 = 0.3151971995830536 + 100.0 * 9.022191047668457
Epoch 1230, val loss: 0.4107268154621124
Epoch 1240, training loss: 902.5487060546875 = 0.3141666054725647 + 100.0 * 9.022345542907715
Epoch 1240, val loss: 0.41029107570648193
Epoch 1250, training loss: 902.80517578125 = 0.31313031911849976 + 100.0 * 9.024920463562012
Epoch 1250, val loss: 0.40984046459198
Epoch 1260, training loss: 902.5501708984375 = 0.3120529055595398 + 100.0 * 9.022380828857422
Epoch 1260, val loss: 0.4100647270679474
Epoch 1270, training loss: 902.5099487304688 = 0.31104686856269836 + 100.0 * 9.021988868713379
Epoch 1270, val loss: 0.4095572531223297
Epoch 1280, training loss: 902.539306640625 = 0.31000930070877075 + 100.0 * 9.022293090820312
Epoch 1280, val loss: 0.40929847955703735
Epoch 1290, training loss: 902.4277954101562 = 0.30899932980537415 + 100.0 * 9.021187782287598
Epoch 1290, val loss: 0.40920549631118774
Epoch 1300, training loss: 902.3764038085938 = 0.30801451206207275 + 100.0 * 9.020683288574219
Epoch 1300, val loss: 0.4089371860027313
Epoch 1310, training loss: 902.3634033203125 = 0.3070301413536072 + 100.0 * 9.020564079284668
Epoch 1310, val loss: 0.4087280035018921
Epoch 1320, training loss: 902.6580810546875 = 0.3060304820537567 + 100.0 * 9.023520469665527
Epoch 1320, val loss: 0.40848276019096375
Epoch 1330, training loss: 902.5587768554688 = 0.3050457537174225 + 100.0 * 9.022537231445312
Epoch 1330, val loss: 0.40836966037750244
Epoch 1340, training loss: 902.3137817382812 = 0.30404505133628845 + 100.0 * 9.020097732543945
Epoch 1340, val loss: 0.40811607241630554
Epoch 1350, training loss: 902.3204956054688 = 0.30309656262397766 + 100.0 * 9.020174026489258
Epoch 1350, val loss: 0.40781766176223755
Epoch 1360, training loss: 902.2693481445312 = 0.3021523058414459 + 100.0 * 9.019672393798828
Epoch 1360, val loss: 0.4077507555484772
Epoch 1370, training loss: 902.265380859375 = 0.3012096881866455 + 100.0 * 9.019641876220703
Epoch 1370, val loss: 0.4075040817260742
Epoch 1380, training loss: 902.3566284179688 = 0.30025753378868103 + 100.0 * 9.020564079284668
Epoch 1380, val loss: 0.4073275327682495
Epoch 1390, training loss: 902.2098999023438 = 0.2993062436580658 + 100.0 * 9.019105911254883
Epoch 1390, val loss: 0.4073043167591095
Epoch 1400, training loss: 902.1853637695312 = 0.2983703017234802 + 100.0 * 9.01887035369873
Epoch 1400, val loss: 0.4071941375732422
Epoch 1410, training loss: 902.4304809570312 = 0.297427773475647 + 100.0 * 9.021330833435059
Epoch 1410, val loss: 0.40729793906211853
Epoch 1420, training loss: 902.3869018554688 = 0.2964754104614258 + 100.0 * 9.020904541015625
Epoch 1420, val loss: 0.4064983129501343
Epoch 1430, training loss: 902.1358642578125 = 0.2955567538738251 + 100.0 * 9.018403053283691
Epoch 1430, val loss: 0.40667489171028137
Epoch 1440, training loss: 902.1250610351562 = 0.2946651577949524 + 100.0 * 9.018303871154785
Epoch 1440, val loss: 0.40659791231155396
Epoch 1450, training loss: 902.0908813476562 = 0.29376596212387085 + 100.0 * 9.01797103881836
Epoch 1450, val loss: 0.4064137041568756
Epoch 1460, training loss: 902.2310180664062 = 0.2928621172904968 + 100.0 * 9.019381523132324
Epoch 1460, val loss: 0.4063158631324768
Epoch 1470, training loss: 902.0931396484375 = 0.29195350408554077 + 100.0 * 9.018012046813965
Epoch 1470, val loss: 0.4059397280216217
Epoch 1480, training loss: 902.2950439453125 = 0.2910473644733429 + 100.0 * 9.020039558410645
Epoch 1480, val loss: 0.406021386384964
Epoch 1490, training loss: 902.0690307617188 = 0.2901570498943329 + 100.0 * 9.017788887023926
Epoch 1490, val loss: 0.4059126079082489
Epoch 1500, training loss: 901.9867553710938 = 0.28929027915000916 + 100.0 * 9.016974449157715
Epoch 1500, val loss: 0.40589937567710876
Epoch 1510, training loss: 902.0140380859375 = 0.28841447830200195 + 100.0 * 9.017256736755371
Epoch 1510, val loss: 0.4057391881942749
Epoch 1520, training loss: 902.0582885742188 = 0.28753817081451416 + 100.0 * 9.017707824707031
Epoch 1520, val loss: 0.405694842338562
Epoch 1530, training loss: 902.000244140625 = 0.2866840958595276 + 100.0 * 9.017135620117188
Epoch 1530, val loss: 0.40543997287750244
Epoch 1540, training loss: 902.1603393554688 = 0.28581005334854126 + 100.0 * 9.018745422363281
Epoch 1540, val loss: 0.4054679274559021
Epoch 1550, training loss: 902.0316772460938 = 0.28494006395339966 + 100.0 * 9.017467498779297
Epoch 1550, val loss: 0.4056188464164734
Epoch 1560, training loss: 901.8947143554688 = 0.2840932309627533 + 100.0 * 9.016105651855469
Epoch 1560, val loss: 0.40537986159324646
Epoch 1570, training loss: 902.01513671875 = 0.2832536995410919 + 100.0 * 9.017318725585938
Epoch 1570, val loss: 0.40544000267982483
Epoch 1580, training loss: 901.8670043945312 = 0.28240519762039185 + 100.0 * 9.015846252441406
Epoch 1580, val loss: 0.40527772903442383
Epoch 1590, training loss: 901.8408813476562 = 0.2815622091293335 + 100.0 * 9.015593528747559
Epoch 1590, val loss: 0.4053135812282562
Epoch 1600, training loss: 901.8820190429688 = 0.280729204416275 + 100.0 * 9.016013145446777
Epoch 1600, val loss: 0.40521731972694397
Epoch 1610, training loss: 901.82421875 = 0.2798998951911926 + 100.0 * 9.015442848205566
Epoch 1610, val loss: 0.4050433337688446
Epoch 1620, training loss: 901.87109375 = 0.2790673077106476 + 100.0 * 9.015920639038086
Epoch 1620, val loss: 0.40472137928009033
Epoch 1630, training loss: 901.8703002929688 = 0.2782302498817444 + 100.0 * 9.015920639038086
Epoch 1630, val loss: 0.4049668312072754
Epoch 1640, training loss: 901.8428344726562 = 0.2773890793323517 + 100.0 * 9.015654563903809
Epoch 1640, val loss: 0.405049592256546
Epoch 1650, training loss: 901.8502807617188 = 0.2765842080116272 + 100.0 * 9.01573657989502
Epoch 1650, val loss: 0.40497004985809326
Epoch 1660, training loss: 901.734130859375 = 0.2757498621940613 + 100.0 * 9.014583587646484
Epoch 1660, val loss: 0.4049893021583557
Epoch 1670, training loss: 901.6884765625 = 0.274945467710495 + 100.0 * 9.014135360717773
Epoch 1670, val loss: 0.40501293540000916
Epoch 1680, training loss: 901.7181396484375 = 0.2741365432739258 + 100.0 * 9.014440536499023
Epoch 1680, val loss: 0.40513259172439575
Epoch 1690, training loss: 901.8156127929688 = 0.2733338475227356 + 100.0 * 9.015422821044922
Epoch 1690, val loss: 0.40506935119628906
Epoch 1700, training loss: 901.7460327148438 = 0.2725354731082916 + 100.0 * 9.014735221862793
Epoch 1700, val loss: 0.4048268795013428
Epoch 1710, training loss: 901.7039794921875 = 0.2717358469963074 + 100.0 * 9.014322280883789
Epoch 1710, val loss: 0.40512514114379883
Epoch 1720, training loss: 901.698974609375 = 0.27094095945358276 + 100.0 * 9.014280319213867
Epoch 1720, val loss: 0.4049939513206482
Epoch 1730, training loss: 901.8306274414062 = 0.2701498568058014 + 100.0 * 9.015604972839355
Epoch 1730, val loss: 0.4049634635448456
Epoch 1740, training loss: 901.6253051757812 = 0.26935696601867676 + 100.0 * 9.013559341430664
Epoch 1740, val loss: 0.4051666557788849
Epoch 1750, training loss: 901.5772094726562 = 0.26858052611351013 + 100.0 * 9.013086318969727
Epoch 1750, val loss: 0.40513527393341064
Epoch 1760, training loss: 901.5458374023438 = 0.2678017318248749 + 100.0 * 9.01278018951416
Epoch 1760, val loss: 0.4052296280860901
Epoch 1770, training loss: 901.58984375 = 0.2670261263847351 + 100.0 * 9.013228416442871
Epoch 1770, val loss: 0.4053710401058197
Epoch 1780, training loss: 901.6000366210938 = 0.2662356495857239 + 100.0 * 9.013338088989258
Epoch 1780, val loss: 0.40538206696510315
Epoch 1790, training loss: 901.6932983398438 = 0.26545774936676025 + 100.0 * 9.014278411865234
Epoch 1790, val loss: 0.40510401129722595
Epoch 1800, training loss: 901.56591796875 = 0.26469069719314575 + 100.0 * 9.013011932373047
Epoch 1800, val loss: 0.4055638015270233
Epoch 1810, training loss: 901.5331420898438 = 0.2639349102973938 + 100.0 * 9.01269245147705
Epoch 1810, val loss: 0.40550827980041504
Epoch 1820, training loss: 901.4526977539062 = 0.26317158341407776 + 100.0 * 9.011895179748535
Epoch 1820, val loss: 0.405595600605011
Epoch 1830, training loss: 901.447509765625 = 0.2624172866344452 + 100.0 * 9.01185131072998
Epoch 1830, val loss: 0.40561026334762573
Epoch 1840, training loss: 901.6805419921875 = 0.26167160272598267 + 100.0 * 9.014188766479492
Epoch 1840, val loss: 0.40572014451026917
Epoch 1850, training loss: 901.5128173828125 = 0.26088860630989075 + 100.0 * 9.012519836425781
Epoch 1850, val loss: 0.40575405955314636
Epoch 1860, training loss: 901.4368896484375 = 0.2601400315761566 + 100.0 * 9.011767387390137
Epoch 1860, val loss: 0.40611401200294495
Epoch 1870, training loss: 901.4752197265625 = 0.2593848705291748 + 100.0 * 9.012158393859863
Epoch 1870, val loss: 0.40615221858024597
Epoch 1880, training loss: 901.5559692382812 = 0.2586452066898346 + 100.0 * 9.012972831726074
Epoch 1880, val loss: 0.40617913007736206
Epoch 1890, training loss: 901.365966796875 = 0.2579036355018616 + 100.0 * 9.011080741882324
Epoch 1890, val loss: 0.4061715602874756
Epoch 1900, training loss: 901.34326171875 = 0.257154256105423 + 100.0 * 9.01086139678955
Epoch 1900, val loss: 0.40618547797203064
Epoch 1910, training loss: 901.3277587890625 = 0.2564212381839752 + 100.0 * 9.010713577270508
Epoch 1910, val loss: 0.40645790100097656
Epoch 1920, training loss: 901.429931640625 = 0.25567737221717834 + 100.0 * 9.01174259185791
Epoch 1920, val loss: 0.40665701031684875
Epoch 1930, training loss: 901.3081665039062 = 0.254938542842865 + 100.0 * 9.01053237915039
Epoch 1930, val loss: 0.4065675139427185
Epoch 1940, training loss: 901.3616333007812 = 0.25420132279396057 + 100.0 * 9.01107406616211
Epoch 1940, val loss: 0.40677353739738464
Epoch 1950, training loss: 901.4254760742188 = 0.2534836232662201 + 100.0 * 9.011719703674316
Epoch 1950, val loss: 0.4067201316356659
Epoch 1960, training loss: 901.37939453125 = 0.25274473428726196 + 100.0 * 9.011266708374023
Epoch 1960, val loss: 0.4071074426174164
Epoch 1970, training loss: 901.2791137695312 = 0.25202706456184387 + 100.0 * 9.010271072387695
Epoch 1970, val loss: 0.40711602568626404
Epoch 1980, training loss: 901.2509155273438 = 0.25129929184913635 + 100.0 * 9.00999641418457
Epoch 1980, val loss: 0.40730956196784973
Epoch 1990, training loss: 901.3124389648438 = 0.2505873441696167 + 100.0 * 9.010618209838867
Epoch 1990, val loss: 0.4072849750518799
Epoch 2000, training loss: 901.2769165039062 = 0.24986642599105835 + 100.0 * 9.010270118713379
Epoch 2000, val loss: 0.4074491262435913
Epoch 2010, training loss: 901.306396484375 = 0.24913068115711212 + 100.0 * 9.01057243347168
Epoch 2010, val loss: 0.4079492688179016
Epoch 2020, training loss: 901.2281494140625 = 0.24841845035552979 + 100.0 * 9.009797096252441
Epoch 2020, val loss: 0.40794694423675537
Epoch 2030, training loss: 901.265625 = 0.247710183262825 + 100.0 * 9.01017951965332
Epoch 2030, val loss: 0.40793532133102417
Epoch 2040, training loss: 901.2481079101562 = 0.24699600040912628 + 100.0 * 9.010010719299316
Epoch 2040, val loss: 0.40832841396331787
Epoch 2050, training loss: 901.2095947265625 = 0.2462834268808365 + 100.0 * 9.00963306427002
Epoch 2050, val loss: 0.40872257947921753
Epoch 2060, training loss: 901.2928466796875 = 0.2455633133649826 + 100.0 * 9.010473251342773
Epoch 2060, val loss: 0.4090040922164917
Epoch 2070, training loss: 901.1548461914062 = 0.2448827177286148 + 100.0 * 9.009099960327148
Epoch 2070, val loss: 0.4087291955947876
Epoch 2080, training loss: 901.1040649414062 = 0.24417386949062347 + 100.0 * 9.008598327636719
Epoch 2080, val loss: 0.4092671871185303
Epoch 2090, training loss: 901.0919799804688 = 0.24347183108329773 + 100.0 * 9.008484840393066
Epoch 2090, val loss: 0.409248948097229
Epoch 2100, training loss: 901.1820068359375 = 0.24278458952903748 + 100.0 * 9.009391784667969
Epoch 2100, val loss: 0.4092320501804352
Epoch 2110, training loss: 901.29150390625 = 0.24207688868045807 + 100.0 * 9.010494232177734
Epoch 2110, val loss: 0.40978941321372986
Epoch 2120, training loss: 901.1085815429688 = 0.24138057231903076 + 100.0 * 9.008671760559082
Epoch 2120, val loss: 0.41003891825675964
Epoch 2130, training loss: 901.0733032226562 = 0.24066394567489624 + 100.0 * 9.008326530456543
Epoch 2130, val loss: 0.4103359878063202
Epoch 2140, training loss: 901.026123046875 = 0.23998764157295227 + 100.0 * 9.007861137390137
Epoch 2140, val loss: 0.41044095158576965
Epoch 2150, training loss: 901.2130126953125 = 0.239294171333313 + 100.0 * 9.009737014770508
Epoch 2150, val loss: 0.41054052114486694
Epoch 2160, training loss: 901.02783203125 = 0.2385968267917633 + 100.0 * 9.007892608642578
Epoch 2160, val loss: 0.4111535847187042
Epoch 2170, training loss: 901.0476684570312 = 0.23790867626667023 + 100.0 * 9.008097648620605
Epoch 2170, val loss: 0.41134902834892273
Epoch 2180, training loss: 901.0123901367188 = 0.2372230589389801 + 100.0 * 9.00775146484375
Epoch 2180, val loss: 0.41151395440101624
Epoch 2190, training loss: 901.11181640625 = 0.2365412563085556 + 100.0 * 9.008752822875977
Epoch 2190, val loss: 0.41193339228630066
Epoch 2200, training loss: 900.9802856445312 = 0.23584698140621185 + 100.0 * 9.007444381713867
Epoch 2200, val loss: 0.4120437204837799
Epoch 2210, training loss: 901.1459350585938 = 0.23516319692134857 + 100.0 * 9.00910758972168
Epoch 2210, val loss: 0.41232866048812866
Epoch 2220, training loss: 900.9574584960938 = 0.23449060320854187 + 100.0 * 9.007229804992676
Epoch 2220, val loss: 0.4125453531742096
Epoch 2230, training loss: 900.9331665039062 = 0.23379886150360107 + 100.0 * 9.006993293762207
Epoch 2230, val loss: 0.41289061307907104
Epoch 2240, training loss: 901.0924072265625 = 0.2331237941980362 + 100.0 * 9.00859260559082
Epoch 2240, val loss: 0.41312938928604126
Epoch 2250, training loss: 900.9090576171875 = 0.2324514240026474 + 100.0 * 9.006766319274902
Epoch 2250, val loss: 0.41329675912857056
Epoch 2260, training loss: 900.9075317382812 = 0.231767475605011 + 100.0 * 9.006757736206055
Epoch 2260, val loss: 0.41349390149116516
Epoch 2270, training loss: 900.99658203125 = 0.2311035394668579 + 100.0 * 9.007655143737793
Epoch 2270, val loss: 0.4136626422405243
Epoch 2280, training loss: 900.9657592773438 = 0.23041757941246033 + 100.0 * 9.007353782653809
Epoch 2280, val loss: 0.4144741892814636
Epoch 2290, training loss: 900.9053344726562 = 0.22973574697971344 + 100.0 * 9.006755828857422
Epoch 2290, val loss: 0.41476550698280334
Epoch 2300, training loss: 900.8770751953125 = 0.22906950116157532 + 100.0 * 9.00648021697998
Epoch 2300, val loss: 0.4147632122039795
Epoch 2310, training loss: 900.9420166015625 = 0.2283962070941925 + 100.0 * 9.007136344909668
Epoch 2310, val loss: 0.4149807393550873
Epoch 2320, training loss: 900.939453125 = 0.227750763297081 + 100.0 * 9.00711727142334
Epoch 2320, val loss: 0.41532257199287415
Epoch 2330, training loss: 900.8707885742188 = 0.22705583274364471 + 100.0 * 9.006437301635742
Epoch 2330, val loss: 0.41579028964042664
Epoch 2340, training loss: 900.9124755859375 = 0.22640414535999298 + 100.0 * 9.006860733032227
Epoch 2340, val loss: 0.4163883626461029
Epoch 2350, training loss: 900.834716796875 = 0.2257310152053833 + 100.0 * 9.00609016418457
Epoch 2350, val loss: 0.4165889024734497
Epoch 2360, training loss: 900.8103637695312 = 0.22507242858409882 + 100.0 * 9.005852699279785
Epoch 2360, val loss: 0.4168395698070526
Epoch 2370, training loss: 900.7977905273438 = 0.22441139817237854 + 100.0 * 9.005733489990234
Epoch 2370, val loss: 0.417219877243042
Epoch 2380, training loss: 900.8967895507812 = 0.22374337911605835 + 100.0 * 9.006730079650879
Epoch 2380, val loss: 0.41747793555259705
Epoch 2390, training loss: 900.7535400390625 = 0.22308437526226044 + 100.0 * 9.005304336547852
Epoch 2390, val loss: 0.41776126623153687
Epoch 2400, training loss: 900.8866577148438 = 0.22242996096611023 + 100.0 * 9.00664234161377
Epoch 2400, val loss: 0.4180219769477844
Epoch 2410, training loss: 900.7656860351562 = 0.22177670896053314 + 100.0 * 9.005438804626465
Epoch 2410, val loss: 0.4188278615474701
Epoch 2420, training loss: 900.7374267578125 = 0.22111845016479492 + 100.0 * 9.005163192749023
Epoch 2420, val loss: 0.41871580481529236
Epoch 2430, training loss: 900.864990234375 = 0.22048987448215485 + 100.0 * 9.006444931030273
Epoch 2430, val loss: 0.41887620091438293
Epoch 2440, training loss: 900.723876953125 = 0.21983499825000763 + 100.0 * 9.005040168762207
Epoch 2440, val loss: 0.41968485713005066
Epoch 2450, training loss: 900.772705078125 = 0.21918079257011414 + 100.0 * 9.005535125732422
Epoch 2450, val loss: 0.41996943950653076
Epoch 2460, training loss: 900.7276611328125 = 0.21853165328502655 + 100.0 * 9.005091667175293
Epoch 2460, val loss: 0.42014095187187195
Epoch 2470, training loss: 900.8016357421875 = 0.21787162125110626 + 100.0 * 9.005837440490723
Epoch 2470, val loss: 0.4205898344516754
Epoch 2480, training loss: 900.7742919921875 = 0.21724675595760345 + 100.0 * 9.005570411682129
Epoch 2480, val loss: 0.4209252893924713
Epoch 2490, training loss: 900.6513061523438 = 0.21659187972545624 + 100.0 * 9.00434684753418
Epoch 2490, val loss: 0.42154017090797424
Epoch 2500, training loss: 900.6363525390625 = 0.21595245599746704 + 100.0 * 9.004203796386719
Epoch 2500, val loss: 0.4218299388885498
Epoch 2510, training loss: 900.6232299804688 = 0.21530427038669586 + 100.0 * 9.00407886505127
Epoch 2510, val loss: 0.42201805114746094
Epoch 2520, training loss: 900.7999267578125 = 0.21468110382556915 + 100.0 * 9.005852699279785
Epoch 2520, val loss: 0.42202064394950867
Epoch 2530, training loss: 900.6561889648438 = 0.21402400732040405 + 100.0 * 9.00442123413086
Epoch 2530, val loss: 0.42339253425598145
Epoch 2540, training loss: 900.667236328125 = 0.21338897943496704 + 100.0 * 9.004538536071777
Epoch 2540, val loss: 0.42321574687957764
Epoch 2550, training loss: 900.6220703125 = 0.21274539828300476 + 100.0 * 9.004093170166016
Epoch 2550, val loss: 0.4239749014377594
Epoch 2560, training loss: 900.6129150390625 = 0.21210911870002747 + 100.0 * 9.004008293151855
Epoch 2560, val loss: 0.4241480529308319
Epoch 2570, training loss: 900.740478515625 = 0.21146953105926514 + 100.0 * 9.005290031433105
Epoch 2570, val loss: 0.4245821237564087
Epoch 2580, training loss: 900.7089233398438 = 0.2108408808708191 + 100.0 * 9.00498104095459
Epoch 2580, val loss: 0.4252871572971344
Epoch 2590, training loss: 900.6343994140625 = 0.21021702885627747 + 100.0 * 9.004241943359375
Epoch 2590, val loss: 0.42584240436553955
Epoch 2600, training loss: 900.5808715820312 = 0.20957469940185547 + 100.0 * 9.00371265411377
Epoch 2600, val loss: 0.4260406494140625
Epoch 2610, training loss: 900.5758056640625 = 0.20894615352153778 + 100.0 * 9.003668785095215
Epoch 2610, val loss: 0.42634764313697815
Epoch 2620, training loss: 900.6550903320312 = 0.20831851661205292 + 100.0 * 9.004467964172363
Epoch 2620, val loss: 0.4264512062072754
Epoch 2630, training loss: 900.69580078125 = 0.20769579708576202 + 100.0 * 9.004880905151367
Epoch 2630, val loss: 0.4271524250507355
Epoch 2640, training loss: 900.573974609375 = 0.207060769200325 + 100.0 * 9.003669738769531
Epoch 2640, val loss: 0.4278918504714966
Epoch 2650, training loss: 900.5258178710938 = 0.2064429074525833 + 100.0 * 9.003193855285645
Epoch 2650, val loss: 0.428235799074173
Epoch 2660, training loss: 900.6620483398438 = 0.2058137208223343 + 100.0 * 9.004562377929688
Epoch 2660, val loss: 0.42865535616874695
Epoch 2670, training loss: 900.5061645507812 = 0.2051965296268463 + 100.0 * 9.003009796142578
Epoch 2670, val loss: 0.42893746495246887
Epoch 2680, training loss: 900.4918212890625 = 0.20457203686237335 + 100.0 * 9.002872467041016
Epoch 2680, val loss: 0.4292716383934021
Epoch 2690, training loss: 900.5469360351562 = 0.20395807921886444 + 100.0 * 9.003429412841797
Epoch 2690, val loss: 0.4295910894870758
Epoch 2700, training loss: 900.4900512695312 = 0.20334887504577637 + 100.0 * 9.002866744995117
Epoch 2700, val loss: 0.429934024810791
Epoch 2710, training loss: 900.550048828125 = 0.2027265429496765 + 100.0 * 9.003473281860352
Epoch 2710, val loss: 0.4305519163608551
Epoch 2720, training loss: 900.4579467773438 = 0.20209278166294098 + 100.0 * 9.002558708190918
Epoch 2720, val loss: 0.43098872900009155
Epoch 2730, training loss: 900.5913696289062 = 0.20146609842777252 + 100.0 * 9.003898620605469
Epoch 2730, val loss: 0.43146705627441406
Epoch 2740, training loss: 900.4351806640625 = 0.20087338984012604 + 100.0 * 9.00234317779541
Epoch 2740, val loss: 0.43237119913101196
Epoch 2750, training loss: 900.48388671875 = 0.2002491056919098 + 100.0 * 9.002836227416992
Epoch 2750, val loss: 0.4325622320175171
Epoch 2760, training loss: 900.4938354492188 = 0.19963881373405457 + 100.0 * 9.002942085266113
Epoch 2760, val loss: 0.43303409218788147
Epoch 2770, training loss: 900.4391479492188 = 0.19902309775352478 + 100.0 * 9.002401351928711
Epoch 2770, val loss: 0.4333987832069397
Epoch 2780, training loss: 900.3751220703125 = 0.19839902222156525 + 100.0 * 9.0017671585083
Epoch 2780, val loss: 0.43418848514556885
Epoch 2790, training loss: 900.4567260742188 = 0.1977965086698532 + 100.0 * 9.002589225769043
Epoch 2790, val loss: 0.4348450005054474
Epoch 2800, training loss: 900.4998779296875 = 0.1971801221370697 + 100.0 * 9.003026962280273
Epoch 2800, val loss: 0.43502911925315857
Epoch 2810, training loss: 900.4703979492188 = 0.19657664000988007 + 100.0 * 9.002737998962402
Epoch 2810, val loss: 0.435767263174057
Epoch 2820, training loss: 900.3824462890625 = 0.1959560215473175 + 100.0 * 9.001864433288574
Epoch 2820, val loss: 0.4362610876560211
Epoch 2830, training loss: 900.3873291015625 = 0.1953698992729187 + 100.0 * 9.001919746398926
Epoch 2830, val loss: 0.436915785074234
Epoch 2840, training loss: 900.400634765625 = 0.19475702941417694 + 100.0 * 9.002058982849121
Epoch 2840, val loss: 0.4371432960033417
Epoch 2850, training loss: 900.35009765625 = 0.19415085017681122 + 100.0 * 9.001559257507324
Epoch 2850, val loss: 0.43783432245254517
Epoch 2860, training loss: 900.4154663085938 = 0.19354388117790222 + 100.0 * 9.002219200134277
Epoch 2860, val loss: 0.4383433163166046
Epoch 2870, training loss: 900.44873046875 = 0.19294250011444092 + 100.0 * 9.002557754516602
Epoch 2870, val loss: 0.4390110671520233
Epoch 2880, training loss: 900.3422241210938 = 0.1923530548810959 + 100.0 * 9.001498222351074
Epoch 2880, val loss: 0.43975400924682617
Epoch 2890, training loss: 900.3136596679688 = 0.19174014031887054 + 100.0 * 9.001219749450684
Epoch 2890, val loss: 0.4398607611656189
Epoch 2900, training loss: 900.2979736328125 = 0.19114449620246887 + 100.0 * 9.001068115234375
Epoch 2900, val loss: 0.4405304193496704
Epoch 2910, training loss: 900.2811279296875 = 0.19054526090621948 + 100.0 * 9.000905990600586
Epoch 2910, val loss: 0.4408358335494995
Epoch 2920, training loss: 900.3089599609375 = 0.1899525374174118 + 100.0 * 9.001190185546875
Epoch 2920, val loss: 0.4412118196487427
Epoch 2930, training loss: 900.4828491210938 = 0.18937534093856812 + 100.0 * 9.002934455871582
Epoch 2930, val loss: 0.44171425700187683
Epoch 2940, training loss: 900.3312377929688 = 0.1887568086385727 + 100.0 * 9.001424789428711
Epoch 2940, val loss: 0.4425206780433655
Epoch 2950, training loss: 900.3080444335938 = 0.18816164135932922 + 100.0 * 9.001198768615723
Epoch 2950, val loss: 0.44308823347091675
Epoch 2960, training loss: 900.2565307617188 = 0.1875767856836319 + 100.0 * 9.000689506530762
Epoch 2960, val loss: 0.4439416229724884
Epoch 2970, training loss: 900.4506225585938 = 0.1870058923959732 + 100.0 * 9.002635955810547
Epoch 2970, val loss: 0.44485771656036377
Epoch 2980, training loss: 900.3283081054688 = 0.18642771244049072 + 100.0 * 9.001419067382812
Epoch 2980, val loss: 0.44426316022872925
Epoch 2990, training loss: 900.2444458007812 = 0.1858023703098297 + 100.0 * 9.00058650970459
Epoch 2990, val loss: 0.445439875125885
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8463
Overall ASR: 0.7328
Flip ASR: 0.6680/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1037.0028076171875 = 1.0994206666946411 + 100.0 * 10.359034538269043
Epoch 0, val loss: 1.0994592905044556
Epoch 10, training loss: 1036.5997314453125 = 1.09010648727417 + 100.0 * 10.355095863342285
Epoch 10, val loss: 1.089930534362793
Epoch 20, training loss: 1029.5986328125 = 1.0780025720596313 + 100.0 * 10.28520679473877
Epoch 20, val loss: 1.0777498483657837
Epoch 30, training loss: 973.408447265625 = 1.065099835395813 + 100.0 * 9.723433494567871
Epoch 30, val loss: 1.0647573471069336
Epoch 40, training loss: 960.6430053710938 = 1.0524635314941406 + 100.0 * 9.595905303955078
Epoch 40, val loss: 1.051924467086792
Epoch 50, training loss: 947.64697265625 = 1.0413787364959717 + 100.0 * 9.466055870056152
Epoch 50, val loss: 1.0408544540405273
Epoch 60, training loss: 935.9141235351562 = 1.033275842666626 + 100.0 * 9.348808288574219
Epoch 60, val loss: 1.0327534675598145
Epoch 70, training loss: 930.3118286132812 = 1.0226811170578003 + 100.0 * 9.292891502380371
Epoch 70, val loss: 1.0224066972732544
Epoch 80, training loss: 925.1897583007812 = 1.0120793581008911 + 100.0 * 9.241776466369629
Epoch 80, val loss: 1.012482762336731
Epoch 90, training loss: 922.4579467773438 = 1.0031378269195557 + 100.0 * 9.214548110961914
Epoch 90, val loss: 1.0040909051895142
Epoch 100, training loss: 920.6829223632812 = 0.9943448305130005 + 100.0 * 9.19688606262207
Epoch 100, val loss: 0.9955399036407471
Epoch 110, training loss: 919.1939697265625 = 0.9842369556427002 + 100.0 * 9.182097434997559
Epoch 110, val loss: 0.9857616424560547
Epoch 120, training loss: 917.9824829101562 = 0.9749095439910889 + 100.0 * 9.170075416564941
Epoch 120, val loss: 0.9770060181617737
Epoch 130, training loss: 917.2657470703125 = 0.9649290442466736 + 100.0 * 9.163008689880371
Epoch 130, val loss: 0.9674939513206482
Epoch 140, training loss: 916.1915283203125 = 0.9540092349052429 + 100.0 * 9.152375221252441
Epoch 140, val loss: 0.957203209400177
Epoch 150, training loss: 915.0486450195312 = 0.943896472454071 + 100.0 * 9.141047477722168
Epoch 150, val loss: 0.9477217197418213
Epoch 160, training loss: 914.3154907226562 = 0.9344879388809204 + 100.0 * 9.133810043334961
Epoch 160, val loss: 0.9384713768959045
Epoch 170, training loss: 913.2752075195312 = 0.9231052398681641 + 100.0 * 9.123520851135254
Epoch 170, val loss: 0.927586555480957
Epoch 180, training loss: 912.5612182617188 = 0.9113190770149231 + 100.0 * 9.116498947143555
Epoch 180, val loss: 0.9163514971733093
Epoch 190, training loss: 911.948974609375 = 0.9002312421798706 + 100.0 * 9.110487937927246
Epoch 190, val loss: 0.9056680202484131
Epoch 200, training loss: 911.3225708007812 = 0.8891040682792664 + 100.0 * 9.104334831237793
Epoch 200, val loss: 0.8951108455657959
Epoch 210, training loss: 910.9072875976562 = 0.8769105076789856 + 100.0 * 9.100303649902344
Epoch 210, val loss: 0.8831740021705627
Epoch 220, training loss: 910.4472045898438 = 0.8631299138069153 + 100.0 * 9.095840454101562
Epoch 220, val loss: 0.870052695274353
Epoch 230, training loss: 910.0088500976562 = 0.8497911095619202 + 100.0 * 9.091590881347656
Epoch 230, val loss: 0.8572343587875366
Epoch 240, training loss: 909.6079711914062 = 0.836549699306488 + 100.0 * 9.087714195251465
Epoch 240, val loss: 0.8446494340896606
Epoch 250, training loss: 909.3953857421875 = 0.8231111168861389 + 100.0 * 9.085722923278809
Epoch 250, val loss: 0.8316779732704163
Epoch 260, training loss: 908.9046020507812 = 0.8084054589271545 + 100.0 * 9.080962181091309
Epoch 260, val loss: 0.8175010085105896
Epoch 270, training loss: 908.6906127929688 = 0.7938544750213623 + 100.0 * 9.078967094421387
Epoch 270, val loss: 0.8035907745361328
Epoch 280, training loss: 908.3901977539062 = 0.779241144657135 + 100.0 * 9.076109886169434
Epoch 280, val loss: 0.7896031141281128
Epoch 290, training loss: 908.203125 = 0.7644351124763489 + 100.0 * 9.074386596679688
Epoch 290, val loss: 0.7754136919975281
Epoch 300, training loss: 907.9285278320312 = 0.749730110168457 + 100.0 * 9.07178783416748
Epoch 300, val loss: 0.7614069581031799
Epoch 310, training loss: 907.8931884765625 = 0.7349480986595154 + 100.0 * 9.071582794189453
Epoch 310, val loss: 0.7472631931304932
Epoch 320, training loss: 907.584716796875 = 0.7199426889419556 + 100.0 * 9.068647384643555
Epoch 320, val loss: 0.7329953908920288
Epoch 330, training loss: 907.5205688476562 = 0.7053513526916504 + 100.0 * 9.06815242767334
Epoch 330, val loss: 0.7190492153167725
Epoch 340, training loss: 907.2724609375 = 0.6907765865325928 + 100.0 * 9.065816879272461
Epoch 340, val loss: 0.7054348587989807
Epoch 350, training loss: 907.0845947265625 = 0.676468014717102 + 100.0 * 9.064081192016602
Epoch 350, val loss: 0.6918723583221436
Epoch 360, training loss: 906.9441528320312 = 0.6625787615776062 + 100.0 * 9.06281566619873
Epoch 360, val loss: 0.678715705871582
Epoch 370, training loss: 906.7942504882812 = 0.649157702922821 + 100.0 * 9.061450958251953
Epoch 370, val loss: 0.6661660075187683
Epoch 380, training loss: 906.8133544921875 = 0.6360551118850708 + 100.0 * 9.061773300170898
Epoch 380, val loss: 0.6538109183311462
Epoch 390, training loss: 906.692138671875 = 0.6226819157600403 + 100.0 * 9.060694694519043
Epoch 390, val loss: 0.6414241194725037
Epoch 400, training loss: 906.4844360351562 = 0.6101938486099243 + 100.0 * 9.05874252319336
Epoch 400, val loss: 0.629891037940979
Epoch 410, training loss: 906.453369140625 = 0.5983848571777344 + 100.0 * 9.058549880981445
Epoch 410, val loss: 0.6189899444580078
Epoch 420, training loss: 906.2425537109375 = 0.5867915749549866 + 100.0 * 9.056557655334473
Epoch 420, val loss: 0.6081132888793945
Epoch 430, training loss: 906.1220703125 = 0.5757870674133301 + 100.0 * 9.055462837219238
Epoch 430, val loss: 0.5981244444847107
Epoch 440, training loss: 906.0548706054688 = 0.5653366446495056 + 100.0 * 9.054895401000977
Epoch 440, val loss: 0.5884965658187866
Epoch 450, training loss: 905.9877319335938 = 0.5549298524856567 + 100.0 * 9.054327964782715
Epoch 450, val loss: 0.5791046023368835
Epoch 460, training loss: 905.8335571289062 = 0.5450663566589355 + 100.0 * 9.052885055541992
Epoch 460, val loss: 0.5700972080230713
Epoch 470, training loss: 905.8131103515625 = 0.5359203219413757 + 100.0 * 9.05277156829834
Epoch 470, val loss: 0.5618494153022766
Epoch 480, training loss: 905.6549682617188 = 0.5270631313323975 + 100.0 * 9.051279067993164
Epoch 480, val loss: 0.5539878606796265
Epoch 490, training loss: 905.7415161132812 = 0.5185567736625671 + 100.0 * 9.052229881286621
Epoch 490, val loss: 0.5464274287223816
Epoch 500, training loss: 905.523681640625 = 0.5103680491447449 + 100.0 * 9.050132751464844
Epoch 500, val loss: 0.5391265749931335
Epoch 510, training loss: 905.4276733398438 = 0.5027863383293152 + 100.0 * 9.049248695373535
Epoch 510, val loss: 0.5324723720550537
Epoch 520, training loss: 905.33056640625 = 0.4956103265285492 + 100.0 * 9.048349380493164
Epoch 520, val loss: 0.5261765122413635
Epoch 530, training loss: 905.4755859375 = 0.4888073801994324 + 100.0 * 9.049867630004883
Epoch 530, val loss: 0.5203583240509033
Epoch 540, training loss: 905.4469604492188 = 0.48187366127967834 + 100.0 * 9.049651145935059
Epoch 540, val loss: 0.514018177986145
Epoch 550, training loss: 905.1761474609375 = 0.4754059314727783 + 100.0 * 9.04700756072998
Epoch 550, val loss: 0.5085265636444092
Epoch 560, training loss: 905.0557861328125 = 0.4696691930294037 + 100.0 * 9.04586124420166
Epoch 560, val loss: 0.5036612749099731
Epoch 570, training loss: 904.98095703125 = 0.46416404843330383 + 100.0 * 9.045167922973633
Epoch 570, val loss: 0.4990265369415283
Epoch 580, training loss: 905.1692504882812 = 0.45877009630203247 + 100.0 * 9.047104835510254
Epoch 580, val loss: 0.49451157450675964
Epoch 590, training loss: 905.0665283203125 = 0.4534531235694885 + 100.0 * 9.046131134033203
Epoch 590, val loss: 0.4900943636894226
Epoch 600, training loss: 904.7930297851562 = 0.4484780430793762 + 100.0 * 9.043445587158203
Epoch 600, val loss: 0.48577895760536194
Epoch 610, training loss: 904.7474365234375 = 0.44392597675323486 + 100.0 * 9.043035507202148
Epoch 610, val loss: 0.4820684790611267
Epoch 620, training loss: 904.687744140625 = 0.4395926296710968 + 100.0 * 9.042481422424316
Epoch 620, val loss: 0.47857585549354553
Epoch 630, training loss: 904.7093505859375 = 0.4353557825088501 + 100.0 * 9.042739868164062
Epoch 630, val loss: 0.47521746158599854
Epoch 640, training loss: 904.7564697265625 = 0.4310968220233917 + 100.0 * 9.043253898620605
Epoch 640, val loss: 0.47137999534606934
Epoch 650, training loss: 904.7963256835938 = 0.426939994096756 + 100.0 * 9.043693542480469
Epoch 650, val loss: 0.4683747887611389
Epoch 660, training loss: 904.5045776367188 = 0.4232083559036255 + 100.0 * 9.040813446044922
Epoch 660, val loss: 0.4654580354690552
Epoch 670, training loss: 904.4131469726562 = 0.41977784037590027 + 100.0 * 9.039933204650879
Epoch 670, val loss: 0.4626385569572449
Epoch 680, training loss: 904.3947143554688 = 0.4164506793022156 + 100.0 * 9.039782524108887
Epoch 680, val loss: 0.4599088430404663
Epoch 690, training loss: 904.6072387695312 = 0.41314497590065 + 100.0 * 9.041940689086914
Epoch 690, val loss: 0.4573686122894287
Epoch 700, training loss: 904.3292236328125 = 0.40977197885513306 + 100.0 * 9.039194107055664
Epoch 700, val loss: 0.45488452911376953
Epoch 710, training loss: 904.2526245117188 = 0.40676185488700867 + 100.0 * 9.038458824157715
Epoch 710, val loss: 0.45261454582214355
Epoch 720, training loss: 904.2026977539062 = 0.40394333004951477 + 100.0 * 9.03798770904541
Epoch 720, val loss: 0.4503943920135498
Epoch 730, training loss: 904.38818359375 = 0.4011702239513397 + 100.0 * 9.039870262145996
Epoch 730, val loss: 0.4482126533985138
Epoch 740, training loss: 904.3096923828125 = 0.3981858789920807 + 100.0 * 9.039114952087402
Epoch 740, val loss: 0.44622310996055603
Epoch 750, training loss: 904.0811157226562 = 0.3956109285354614 + 100.0 * 9.03685474395752
Epoch 750, val loss: 0.44435766339302063
Epoch 760, training loss: 904.0403442382812 = 0.39314666390419006 + 100.0 * 9.03647232055664
Epoch 760, val loss: 0.4425005614757538
Epoch 770, training loss: 903.9824829101562 = 0.39075735211372375 + 100.0 * 9.035917282104492
Epoch 770, val loss: 0.4408450126647949
Epoch 780, training loss: 904.4114379882812 = 0.3883571922779083 + 100.0 * 9.040230751037598
Epoch 780, val loss: 0.43917950987815857
Epoch 790, training loss: 903.9122924804688 = 0.38586172461509705 + 100.0 * 9.035264015197754
Epoch 790, val loss: 0.43735814094543457
Epoch 800, training loss: 903.9027099609375 = 0.3836873173713684 + 100.0 * 9.03519058227539
Epoch 800, val loss: 0.43589940667152405
Epoch 810, training loss: 903.9971923828125 = 0.3815833032131195 + 100.0 * 9.036155700683594
Epoch 810, val loss: 0.4344949722290039
Epoch 820, training loss: 903.9212646484375 = 0.379412442445755 + 100.0 * 9.035418510437012
Epoch 820, val loss: 0.43286994099617004
Epoch 830, training loss: 903.8241577148438 = 0.3773379623889923 + 100.0 * 9.034468650817871
Epoch 830, val loss: 0.4316008687019348
Epoch 840, training loss: 903.7379150390625 = 0.37540751695632935 + 100.0 * 9.033624649047852
Epoch 840, val loss: 0.4303370714187622
Epoch 850, training loss: 903.7402954101562 = 0.3735424280166626 + 100.0 * 9.03366756439209
Epoch 850, val loss: 0.42914190888404846
Epoch 860, training loss: 903.7465209960938 = 0.37159451842308044 + 100.0 * 9.0337495803833
Epoch 860, val loss: 0.4278189241886139
Epoch 870, training loss: 903.6600341796875 = 0.36973661184310913 + 100.0 * 9.032902717590332
Epoch 870, val loss: 0.4265724718570709
Epoch 880, training loss: 903.7623291015625 = 0.36794474720954895 + 100.0 * 9.033944129943848
Epoch 880, val loss: 0.42534196376800537
Epoch 890, training loss: 903.6036987304688 = 0.36617311835289 + 100.0 * 9.03237533569336
Epoch 890, val loss: 0.4246440827846527
Epoch 900, training loss: 903.5297241210938 = 0.36451026797294617 + 100.0 * 9.031652450561523
Epoch 900, val loss: 0.42345869541168213
Epoch 910, training loss: 903.565185546875 = 0.36285582184791565 + 100.0 * 9.032023429870605
Epoch 910, val loss: 0.4225604832172394
Epoch 920, training loss: 903.5650634765625 = 0.3610672950744629 + 100.0 * 9.032039642333984
Epoch 920, val loss: 0.4214603304862976
Epoch 930, training loss: 903.5518798828125 = 0.359413743019104 + 100.0 * 9.0319242477417
Epoch 930, val loss: 0.42044857144355774
Epoch 940, training loss: 903.4235229492188 = 0.35791754722595215 + 100.0 * 9.030655860900879
Epoch 940, val loss: 0.41978421807289124
Epoch 950, training loss: 903.38427734375 = 0.35643327236175537 + 100.0 * 9.030278205871582
Epoch 950, val loss: 0.41885942220687866
Epoch 960, training loss: 903.567138671875 = 0.3549548387527466 + 100.0 * 9.032121658325195
Epoch 960, val loss: 0.4182920753955841
Epoch 970, training loss: 903.3603515625 = 0.35337671637535095 + 100.0 * 9.030069351196289
Epoch 970, val loss: 0.4168800413608551
Epoch 980, training loss: 903.3120727539062 = 0.35196006298065186 + 100.0 * 9.029601097106934
Epoch 980, val loss: 0.4164494574069977
Epoch 990, training loss: 903.6455688476562 = 0.35052645206451416 + 100.0 * 9.032950401306152
Epoch 990, val loss: 0.4151289463043213
Epoch 1000, training loss: 903.3846435546875 = 0.3490038514137268 + 100.0 * 9.030356407165527
Epoch 1000, val loss: 0.4148918390274048
Epoch 1010, training loss: 903.368896484375 = 0.34762728214263916 + 100.0 * 9.03021240234375
Epoch 1010, val loss: 0.4142022728919983
Epoch 1020, training loss: 903.2025756835938 = 0.3462510108947754 + 100.0 * 9.028563499450684
Epoch 1020, val loss: 0.4134332537651062
Epoch 1030, training loss: 903.169677734375 = 0.34495604038238525 + 100.0 * 9.028246879577637
Epoch 1030, val loss: 0.41282179951667786
Epoch 1040, training loss: 903.145263671875 = 0.3436567783355713 + 100.0 * 9.028016090393066
Epoch 1040, val loss: 0.4121401607990265
Epoch 1050, training loss: 903.4070434570312 = 0.3423684239387512 + 100.0 * 9.030647277832031
Epoch 1050, val loss: 0.4115205407142639
Epoch 1060, training loss: 903.2828369140625 = 0.34093177318573 + 100.0 * 9.0294189453125
Epoch 1060, val loss: 0.4109472334384918
Epoch 1070, training loss: 903.1061401367188 = 0.33969610929489136 + 100.0 * 9.027664184570312
Epoch 1070, val loss: 0.41028133034706116
Epoch 1080, training loss: 903.0845336914062 = 0.3384644687175751 + 100.0 * 9.027461051940918
Epoch 1080, val loss: 0.4097808301448822
Epoch 1090, training loss: 903.1017456054688 = 0.3372372090816498 + 100.0 * 9.027645111083984
Epoch 1090, val loss: 0.40934062004089355
Epoch 1100, training loss: 903.1177368164062 = 0.33603397011756897 + 100.0 * 9.027816772460938
Epoch 1100, val loss: 0.4090063273906708
Epoch 1110, training loss: 902.9967651367188 = 0.3347679376602173 + 100.0 * 9.026619911193848
Epoch 1110, val loss: 0.40816304087638855
Epoch 1120, training loss: 903.0237426757812 = 0.3335927128791809 + 100.0 * 9.026901245117188
Epoch 1120, val loss: 0.40782424807548523
Epoch 1130, training loss: 902.9944458007812 = 0.33242812752723694 + 100.0 * 9.026619911193848
Epoch 1130, val loss: 0.4070690870285034
Epoch 1140, training loss: 902.8934326171875 = 0.3312870264053345 + 100.0 * 9.02562141418457
Epoch 1140, val loss: 0.4067445397377014
Epoch 1150, training loss: 902.9368896484375 = 0.33015382289886475 + 100.0 * 9.026067733764648
Epoch 1150, val loss: 0.40621429681777954
Epoch 1160, training loss: 902.9788818359375 = 0.3289618194103241 + 100.0 * 9.026498794555664
Epoch 1160, val loss: 0.40566232800483704
Epoch 1170, training loss: 902.9004516601562 = 0.327828586101532 + 100.0 * 9.025726318359375
Epoch 1170, val loss: 0.40529054403305054
Epoch 1180, training loss: 902.8635864257812 = 0.3267382085323334 + 100.0 * 9.025368690490723
Epoch 1180, val loss: 0.4050990045070648
Epoch 1190, training loss: 902.8529663085938 = 0.32565200328826904 + 100.0 * 9.025273323059082
Epoch 1190, val loss: 0.4045616686344147
Epoch 1200, training loss: 902.8775024414062 = 0.32456374168395996 + 100.0 * 9.025528907775879
Epoch 1200, val loss: 0.40421316027641296
Epoch 1210, training loss: 902.8653564453125 = 0.3234475553035736 + 100.0 * 9.025419235229492
Epoch 1210, val loss: 0.4040897786617279
Epoch 1220, training loss: 902.75390625 = 0.32239285111427307 + 100.0 * 9.024314880371094
Epoch 1220, val loss: 0.4034803509712219
Epoch 1230, training loss: 902.8417358398438 = 0.32135990262031555 + 100.0 * 9.025203704833984
Epoch 1230, val loss: 0.4034299850463867
Epoch 1240, training loss: 902.6880493164062 = 0.3202737271785736 + 100.0 * 9.023677825927734
Epoch 1240, val loss: 0.4027388095855713
Epoch 1250, training loss: 902.7403564453125 = 0.31925109028816223 + 100.0 * 9.024210929870605
Epoch 1250, val loss: 0.40248289704322815
Epoch 1260, training loss: 902.7431030273438 = 0.31818413734436035 + 100.0 * 9.024249076843262
Epoch 1260, val loss: 0.40199926495552063
Epoch 1270, training loss: 902.6647338867188 = 0.3171805441379547 + 100.0 * 9.023475646972656
Epoch 1270, val loss: 0.40188854932785034
Epoch 1280, training loss: 902.6580200195312 = 0.3161696791648865 + 100.0 * 9.023418426513672
Epoch 1280, val loss: 0.40143486857414246
Epoch 1290, training loss: 902.7001953125 = 0.3151375353336334 + 100.0 * 9.023850440979004
Epoch 1290, val loss: 0.40123167634010315
Epoch 1300, training loss: 902.6011352539062 = 0.3141552805900574 + 100.0 * 9.022870063781738
Epoch 1300, val loss: 0.4012164771556854
Epoch 1310, training loss: 902.7440795898438 = 0.3131445050239563 + 100.0 * 9.024309158325195
Epoch 1310, val loss: 0.4010090231895447
Epoch 1320, training loss: 902.6065063476562 = 0.3121100664138794 + 100.0 * 9.022943496704102
Epoch 1320, val loss: 0.4004189372062683
Epoch 1330, training loss: 902.55712890625 = 0.3111535310745239 + 100.0 * 9.022459983825684
Epoch 1330, val loss: 0.4005199074745178
Epoch 1340, training loss: 902.6688842773438 = 0.31015872955322266 + 100.0 * 9.023587226867676
Epoch 1340, val loss: 0.4002453088760376
Epoch 1350, training loss: 902.6260375976562 = 0.309200644493103 + 100.0 * 9.023168563842773
Epoch 1350, val loss: 0.399730384349823
Epoch 1360, training loss: 902.4659423828125 = 0.30821695923805237 + 100.0 * 9.021576881408691
Epoch 1360, val loss: 0.39964237809181213
Epoch 1370, training loss: 902.4396362304688 = 0.3072863817214966 + 100.0 * 9.021323204040527
Epoch 1370, val loss: 0.3993755280971527
Epoch 1380, training loss: 902.5144653320312 = 0.30634474754333496 + 100.0 * 9.02208137512207
Epoch 1380, val loss: 0.39934951066970825
Epoch 1390, training loss: 902.434326171875 = 0.3053591847419739 + 100.0 * 9.021289825439453
Epoch 1390, val loss: 0.398911714553833
Epoch 1400, training loss: 902.4596557617188 = 0.3044213354587555 + 100.0 * 9.021552085876465
Epoch 1400, val loss: 0.3987128734588623
Epoch 1410, training loss: 902.5411376953125 = 0.303483247756958 + 100.0 * 9.022377014160156
Epoch 1410, val loss: 0.3984905183315277
Epoch 1420, training loss: 902.3702392578125 = 0.30253496766090393 + 100.0 * 9.020676612854004
Epoch 1420, val loss: 0.39843589067459106
Epoch 1430, training loss: 902.4994506835938 = 0.30161789059638977 + 100.0 * 9.021978378295898
Epoch 1430, val loss: 0.3979857861995697
Epoch 1440, training loss: 902.3006591796875 = 0.3006977438926697 + 100.0 * 9.019999504089355
Epoch 1440, val loss: 0.39832428097724915
Epoch 1450, training loss: 902.3221435546875 = 0.2998036742210388 + 100.0 * 9.020223617553711
Epoch 1450, val loss: 0.3980928957462311
Epoch 1460, training loss: 902.4766845703125 = 0.29888981580734253 + 100.0 * 9.021778106689453
Epoch 1460, val loss: 0.39805319905281067
Epoch 1470, training loss: 902.2971801757812 = 0.29799482226371765 + 100.0 * 9.019991874694824
Epoch 1470, val loss: 0.3976367712020874
Epoch 1480, training loss: 902.2344360351562 = 0.29711025953292847 + 100.0 * 9.019372940063477
Epoch 1480, val loss: 0.39756935834884644
Epoch 1490, training loss: 902.2249145507812 = 0.29623177647590637 + 100.0 * 9.019287109375
Epoch 1490, val loss: 0.39731863141059875
Epoch 1500, training loss: 902.6535034179688 = 0.295330673456192 + 100.0 * 9.023581504821777
Epoch 1500, val loss: 0.3967997431755066
Epoch 1510, training loss: 902.2918090820312 = 0.29442140460014343 + 100.0 * 9.019973754882812
Epoch 1510, val loss: 0.3972508907318115
Epoch 1520, training loss: 902.213134765625 = 0.2935682237148285 + 100.0 * 9.019195556640625
Epoch 1520, val loss: 0.3973946273326874
Epoch 1530, training loss: 902.2072143554688 = 0.29269933700561523 + 100.0 * 9.019145011901855
Epoch 1530, val loss: 0.39714065194129944
Epoch 1540, training loss: 902.1704711914062 = 0.2918373942375183 + 100.0 * 9.018786430358887
Epoch 1540, val loss: 0.3968644142150879
Epoch 1550, training loss: 902.26953125 = 0.2909812033176422 + 100.0 * 9.01978588104248
Epoch 1550, val loss: 0.39664316177368164
Epoch 1560, training loss: 902.227783203125 = 0.2901100516319275 + 100.0 * 9.019376754760742
Epoch 1560, val loss: 0.3967640995979309
Epoch 1570, training loss: 902.134033203125 = 0.28925222158432007 + 100.0 * 9.018447875976562
Epoch 1570, val loss: 0.3966562747955322
Epoch 1580, training loss: 902.08837890625 = 0.28841444849967957 + 100.0 * 9.017999649047852
Epoch 1580, val loss: 0.3965952694416046
Epoch 1590, training loss: 902.1181640625 = 0.28758543729782104 + 100.0 * 9.018305778503418
Epoch 1590, val loss: 0.39652857184410095
Epoch 1600, training loss: 902.2545776367188 = 0.2867320775985718 + 100.0 * 9.019678115844727
Epoch 1600, val loss: 0.3968774676322937
Epoch 1610, training loss: 902.0413818359375 = 0.28588229417800903 + 100.0 * 9.017555236816406
Epoch 1610, val loss: 0.396359384059906
Epoch 1620, training loss: 902.0293579101562 = 0.2850826382637024 + 100.0 * 9.01744270324707
Epoch 1620, val loss: 0.39653098583221436
Epoch 1630, training loss: 902.1967163085938 = 0.28426748514175415 + 100.0 * 9.019124984741211
Epoch 1630, val loss: 0.39653176069259644
Epoch 1640, training loss: 901.9752197265625 = 0.28341108560562134 + 100.0 * 9.016918182373047
Epoch 1640, val loss: 0.39635035395622253
Epoch 1650, training loss: 901.9473266601562 = 0.28262069821357727 + 100.0 * 9.016647338867188
Epoch 1650, val loss: 0.3964175879955292
Epoch 1660, training loss: 901.9332885742188 = 0.2818056643009186 + 100.0 * 9.016514778137207
Epoch 1660, val loss: 0.39633429050445557
Epoch 1670, training loss: 902.280029296875 = 0.280985563993454 + 100.0 * 9.019989967346191
Epoch 1670, val loss: 0.39619675278663635
Epoch 1680, training loss: 902.0511474609375 = 0.280154287815094 + 100.0 * 9.017709732055664
Epoch 1680, val loss: 0.3962538242340088
Epoch 1690, training loss: 901.9354858398438 = 0.27937600016593933 + 100.0 * 9.016561508178711
Epoch 1690, val loss: 0.3964199721813202
Epoch 1700, training loss: 901.9161987304688 = 0.2785722315311432 + 100.0 * 9.016376495361328
Epoch 1700, val loss: 0.3964274525642395
Epoch 1710, training loss: 902.0101318359375 = 0.2777629792690277 + 100.0 * 9.01732349395752
Epoch 1710, val loss: 0.3964340090751648
Epoch 1720, training loss: 902.127197265625 = 0.2769656181335449 + 100.0 * 9.018502235412598
Epoch 1720, val loss: 0.3959091305732727
Epoch 1730, training loss: 902.03955078125 = 0.27617543935775757 + 100.0 * 9.017633438110352
Epoch 1730, val loss: 0.3964882493019104
Epoch 1740, training loss: 901.8512573242188 = 0.275381475687027 + 100.0 * 9.015758514404297
Epoch 1740, val loss: 0.39607658982276917
Epoch 1750, training loss: 901.8072509765625 = 0.27461695671081543 + 100.0 * 9.015326499938965
Epoch 1750, val loss: 0.39603516459465027
Epoch 1760, training loss: 901.7749633789062 = 0.2738390266895294 + 100.0 * 9.015010833740234
Epoch 1760, val loss: 0.39611485600471497
Epoch 1770, training loss: 901.8065795898438 = 0.27305638790130615 + 100.0 * 9.015335083007812
Epoch 1770, val loss: 0.396075576543808
Epoch 1780, training loss: 902.057861328125 = 0.2722477614879608 + 100.0 * 9.017855644226074
Epoch 1780, val loss: 0.3959507644176483
Epoch 1790, training loss: 901.776123046875 = 0.2714773118495941 + 100.0 * 9.015046119689941
Epoch 1790, val loss: 0.395962119102478
Epoch 1800, training loss: 901.7843627929688 = 0.27071335911750793 + 100.0 * 9.01513671875
Epoch 1800, val loss: 0.3961600363254547
Epoch 1810, training loss: 901.96923828125 = 0.2699345052242279 + 100.0 * 9.016992568969727
Epoch 1810, val loss: 0.3959198594093323
Epoch 1820, training loss: 901.7998046875 = 0.26919087767601013 + 100.0 * 9.01530647277832
Epoch 1820, val loss: 0.396161824464798
Epoch 1830, training loss: 901.6919555664062 = 0.26842862367630005 + 100.0 * 9.014235496520996
Epoch 1830, val loss: 0.39631691575050354
Epoch 1840, training loss: 901.6697998046875 = 0.2676810026168823 + 100.0 * 9.014020919799805
Epoch 1840, val loss: 0.39645320177078247
Epoch 1850, training loss: 901.7365112304688 = 0.2669319808483124 + 100.0 * 9.01469612121582
Epoch 1850, val loss: 0.3966653347015381
Epoch 1860, training loss: 901.8234252929688 = 0.2661623954772949 + 100.0 * 9.015572547912598
Epoch 1860, val loss: 0.39678284525871277
Epoch 1870, training loss: 901.8525390625 = 0.26541730761528015 + 100.0 * 9.015871047973633
Epoch 1870, val loss: 0.39660096168518066
Epoch 1880, training loss: 901.655029296875 = 0.2646598815917969 + 100.0 * 9.013903617858887
Epoch 1880, val loss: 0.3964601755142212
Epoch 1890, training loss: 901.6781005859375 = 0.263923704624176 + 100.0 * 9.014142036437988
Epoch 1890, val loss: 0.39671042561531067
Epoch 1900, training loss: 901.678466796875 = 0.26318156719207764 + 100.0 * 9.014152526855469
Epoch 1900, val loss: 0.39655473828315735
Epoch 1910, training loss: 901.6009521484375 = 0.2624484598636627 + 100.0 * 9.013384819030762
Epoch 1910, val loss: 0.396899938583374
Epoch 1920, training loss: 901.6017456054688 = 0.26170653104782104 + 100.0 * 9.013400077819824
Epoch 1920, val loss: 0.39687326550483704
Epoch 1930, training loss: 901.8756713867188 = 0.26095691323280334 + 100.0 * 9.016146659851074
Epoch 1930, val loss: 0.3967302739620209
Epoch 1940, training loss: 901.6809692382812 = 0.26023489236831665 + 100.0 * 9.014206886291504
Epoch 1940, val loss: 0.3972559869289398
Epoch 1950, training loss: 901.5926513671875 = 0.2595027685165405 + 100.0 * 9.013331413269043
Epoch 1950, val loss: 0.3974139392375946
Epoch 1960, training loss: 901.6141357421875 = 0.258783221244812 + 100.0 * 9.013553619384766
Epoch 1960, val loss: 0.39772719144821167
Epoch 1970, training loss: 901.6077880859375 = 0.25803831219673157 + 100.0 * 9.013497352600098
Epoch 1970, val loss: 0.3976305425167084
Epoch 1980, training loss: 901.5361328125 = 0.25732600688934326 + 100.0 * 9.012787818908691
Epoch 1980, val loss: 0.39766061305999756
Epoch 1990, training loss: 901.5697021484375 = 0.25661730766296387 + 100.0 * 9.013131141662598
Epoch 1990, val loss: 0.39818060398101807
Epoch 2000, training loss: 901.5888061523438 = 0.25587397813796997 + 100.0 * 9.01332950592041
Epoch 2000, val loss: 0.39792919158935547
Epoch 2010, training loss: 901.5272827148438 = 0.2551690638065338 + 100.0 * 9.012721061706543
Epoch 2010, val loss: 0.39762118458747864
Epoch 2020, training loss: 901.6759033203125 = 0.2544357180595398 + 100.0 * 9.014214515686035
Epoch 2020, val loss: 0.39791277050971985
Epoch 2030, training loss: 901.5166625976562 = 0.2537429630756378 + 100.0 * 9.012629508972168
Epoch 2030, val loss: 0.3983267843723297
Epoch 2040, training loss: 901.5256958007812 = 0.2530176639556885 + 100.0 * 9.012726783752441
Epoch 2040, val loss: 0.39839375019073486
Epoch 2050, training loss: 901.4979858398438 = 0.2523254454135895 + 100.0 * 9.012456893920898
Epoch 2050, val loss: 0.3985635042190552
Epoch 2060, training loss: 901.4534301757812 = 0.2516169548034668 + 100.0 * 9.012018203735352
Epoch 2060, val loss: 0.39882388710975647
Epoch 2070, training loss: 901.4159545898438 = 0.2509247064590454 + 100.0 * 9.011650085449219
Epoch 2070, val loss: 0.3990030288696289
Epoch 2080, training loss: 901.5989379882812 = 0.25023153424263 + 100.0 * 9.013486862182617
Epoch 2080, val loss: 0.399469256401062
Epoch 2090, training loss: 901.4739379882812 = 0.24948719143867493 + 100.0 * 9.01224422454834
Epoch 2090, val loss: 0.3990998864173889
Epoch 2100, training loss: 901.3937377929688 = 0.2488117665052414 + 100.0 * 9.011448860168457
Epoch 2100, val loss: 0.3995446562767029
Epoch 2110, training loss: 901.370849609375 = 0.24811115860939026 + 100.0 * 9.01122760772705
Epoch 2110, val loss: 0.39961713552474976
Epoch 2120, training loss: 901.3801879882812 = 0.24742703139781952 + 100.0 * 9.011327743530273
Epoch 2120, val loss: 0.399962842464447
Epoch 2130, training loss: 901.5155029296875 = 0.24672316014766693 + 100.0 * 9.012687683105469
Epoch 2130, val loss: 0.4001855254173279
Epoch 2140, training loss: 901.4216918945312 = 0.24603548645973206 + 100.0 * 9.011756896972656
Epoch 2140, val loss: 0.4003460705280304
Epoch 2150, training loss: 901.4199829101562 = 0.2453450858592987 + 100.0 * 9.011746406555176
Epoch 2150, val loss: 0.40055224299430847
Epoch 2160, training loss: 901.3407592773438 = 0.2446420043706894 + 100.0 * 9.010961532592773
Epoch 2160, val loss: 0.4003206491470337
Epoch 2170, training loss: 901.378173828125 = 0.24395106732845306 + 100.0 * 9.01134204864502
Epoch 2170, val loss: 0.4007876515388489
Epoch 2180, training loss: 901.4257202148438 = 0.24327033758163452 + 100.0 * 9.011824607849121
Epoch 2180, val loss: 0.4012551009654999
Epoch 2190, training loss: 901.300048828125 = 0.2425912618637085 + 100.0 * 9.010574340820312
Epoch 2190, val loss: 0.4011574387550354
Epoch 2200, training loss: 901.250732421875 = 0.24191351234912872 + 100.0 * 9.010087966918945
Epoch 2200, val loss: 0.4013507068157196
Epoch 2210, training loss: 901.2347412109375 = 0.24123573303222656 + 100.0 * 9.00993537902832
Epoch 2210, val loss: 0.40155908465385437
Epoch 2220, training loss: 901.3078002929688 = 0.24056130647659302 + 100.0 * 9.010672569274902
Epoch 2220, val loss: 0.40191182494163513
Epoch 2230, training loss: 901.3118896484375 = 0.23988555371761322 + 100.0 * 9.010720252990723
Epoch 2230, val loss: 0.40208375453948975
Epoch 2240, training loss: 901.3993530273438 = 0.23922200500965118 + 100.0 * 9.011601448059082
Epoch 2240, val loss: 0.4027533531188965
Epoch 2250, training loss: 901.2677001953125 = 0.23853759467601776 + 100.0 * 9.010292053222656
Epoch 2250, val loss: 0.40263429284095764
Epoch 2260, training loss: 901.220703125 = 0.23785676062107086 + 100.0 * 9.009828567504883
Epoch 2260, val loss: 0.4026634991168976
Epoch 2270, training loss: 901.368896484375 = 0.23718899488449097 + 100.0 * 9.011317253112793
Epoch 2270, val loss: 0.4028889834880829
Epoch 2280, training loss: 901.2374877929688 = 0.23652862012386322 + 100.0 * 9.010009765625
Epoch 2280, val loss: 0.40313300490379333
Epoch 2290, training loss: 901.2037353515625 = 0.2358601987361908 + 100.0 * 9.009678840637207
Epoch 2290, val loss: 0.4032740592956543
Epoch 2300, training loss: 901.1293334960938 = 0.23519378900527954 + 100.0 * 9.008941650390625
Epoch 2300, val loss: 0.4037017822265625
Epoch 2310, training loss: 901.1817626953125 = 0.2345302402973175 + 100.0 * 9.009471893310547
Epoch 2310, val loss: 0.4040350019931793
Epoch 2320, training loss: 901.3010864257812 = 0.23385478556156158 + 100.0 * 9.010672569274902
Epoch 2320, val loss: 0.4039507210254669
Epoch 2330, training loss: 901.1923217773438 = 0.23320649564266205 + 100.0 * 9.009591102600098
Epoch 2330, val loss: 0.4040856957435608
Epoch 2340, training loss: 901.1751708984375 = 0.23253774642944336 + 100.0 * 9.00942611694336
Epoch 2340, val loss: 0.4045332968235016
Epoch 2350, training loss: 901.1570434570312 = 0.23187316954135895 + 100.0 * 9.009251594543457
Epoch 2350, val loss: 0.4047442376613617
Epoch 2360, training loss: 901.2150268554688 = 0.2312290221452713 + 100.0 * 9.009838104248047
Epoch 2360, val loss: 0.4053765833377838
Epoch 2370, training loss: 901.1600341796875 = 0.2305791974067688 + 100.0 * 9.009294509887695
Epoch 2370, val loss: 0.4051675796508789
Epoch 2380, training loss: 901.2403564453125 = 0.22990988194942474 + 100.0 * 9.010104179382324
Epoch 2380, val loss: 0.4053027927875519
Epoch 2390, training loss: 901.1041259765625 = 0.22925688326358795 + 100.0 * 9.008749008178711
Epoch 2390, val loss: 0.4058101773262024
Epoch 2400, training loss: 901.0933837890625 = 0.2286023199558258 + 100.0 * 9.008647918701172
Epoch 2400, val loss: 0.40602022409439087
Epoch 2410, training loss: 901.089599609375 = 0.22795842587947845 + 100.0 * 9.00861644744873
Epoch 2410, val loss: 0.406421035528183
Epoch 2420, training loss: 901.0719604492188 = 0.2273072749376297 + 100.0 * 9.00844669342041
Epoch 2420, val loss: 0.4070574939250946
Epoch 2430, training loss: 901.0455932617188 = 0.22665798664093018 + 100.0 * 9.00818920135498
Epoch 2430, val loss: 0.4072621464729309
Epoch 2440, training loss: 901.099853515625 = 0.2260032445192337 + 100.0 * 9.00873851776123
Epoch 2440, val loss: 0.4071424603462219
Epoch 2450, training loss: 901.16845703125 = 0.2253507673740387 + 100.0 * 9.009430885314941
Epoch 2450, val loss: 0.4071648418903351
Epoch 2460, training loss: 901.0390014648438 = 0.224702849984169 + 100.0 * 9.008142471313477
Epoch 2460, val loss: 0.40811994671821594
Epoch 2470, training loss: 900.9583129882812 = 0.22407013177871704 + 100.0 * 9.007342338562012
Epoch 2470, val loss: 0.4083966612815857
Epoch 2480, training loss: 900.9766845703125 = 0.22343528270721436 + 100.0 * 9.007532119750977
Epoch 2480, val loss: 0.4088112413883209
Epoch 2490, training loss: 901.3014526367188 = 0.2227945774793625 + 100.0 * 9.010787010192871
Epoch 2490, val loss: 0.40915989875793457
Epoch 2500, training loss: 901.0829467773438 = 0.22214604914188385 + 100.0 * 9.008607864379883
Epoch 2500, val loss: 0.4091181457042694
Epoch 2510, training loss: 900.9933471679688 = 0.22153550386428833 + 100.0 * 9.007718086242676
Epoch 2510, val loss: 0.40987154841423035
Epoch 2520, training loss: 900.98046875 = 0.2208958864212036 + 100.0 * 9.007596015930176
Epoch 2520, val loss: 0.4101427495479584
Epoch 2530, training loss: 900.919677734375 = 0.2202630490064621 + 100.0 * 9.006994247436523
Epoch 2530, val loss: 0.41038843989372253
Epoch 2540, training loss: 901.0006103515625 = 0.21964304149150848 + 100.0 * 9.00780963897705
Epoch 2540, val loss: 0.41090822219848633
Epoch 2550, training loss: 900.9869995117188 = 0.21900327503681183 + 100.0 * 9.00767993927002
Epoch 2550, val loss: 0.4111906588077545
Epoch 2560, training loss: 900.892333984375 = 0.21838606894016266 + 100.0 * 9.006739616394043
Epoch 2560, val loss: 0.4114621877670288
Epoch 2570, training loss: 900.9317016601562 = 0.21775005757808685 + 100.0 * 9.007139205932617
Epoch 2570, val loss: 0.4113987982273102
Epoch 2580, training loss: 901.0337524414062 = 0.21712684631347656 + 100.0 * 9.008166313171387
Epoch 2580, val loss: 0.4118229150772095
Epoch 2590, training loss: 901.0300903320312 = 0.21651636064052582 + 100.0 * 9.008135795593262
Epoch 2590, val loss: 0.41279032826423645
Epoch 2600, training loss: 900.8832397460938 = 0.21590375900268555 + 100.0 * 9.006673812866211
Epoch 2600, val loss: 0.4131634831428528
Epoch 2610, training loss: 900.8452758789062 = 0.21528303623199463 + 100.0 * 9.00629997253418
Epoch 2610, val loss: 0.41358670592308044
Epoch 2620, training loss: 900.926025390625 = 0.21465618908405304 + 100.0 * 9.007113456726074
Epoch 2620, val loss: 0.41409170627593994
Epoch 2630, training loss: 900.8872680664062 = 0.2140350043773651 + 100.0 * 9.006731986999512
Epoch 2630, val loss: 0.41442301869392395
Epoch 2640, training loss: 900.8585815429688 = 0.21342609822750092 + 100.0 * 9.006451606750488
Epoch 2640, val loss: 0.4146594703197479
Epoch 2650, training loss: 900.8480834960938 = 0.212809756398201 + 100.0 * 9.006352424621582
Epoch 2650, val loss: 0.41496601700782776
Epoch 2660, training loss: 900.919189453125 = 0.21220433712005615 + 100.0 * 9.00706958770752
Epoch 2660, val loss: 0.4155151844024658
Epoch 2670, training loss: 900.8412475585938 = 0.2115999013185501 + 100.0 * 9.006296157836914
Epoch 2670, val loss: 0.4161357581615448
Epoch 2680, training loss: 900.7989501953125 = 0.2109849750995636 + 100.0 * 9.005879402160645
Epoch 2680, val loss: 0.41645359992980957
Epoch 2690, training loss: 900.7857666015625 = 0.21038059890270233 + 100.0 * 9.005753517150879
Epoch 2690, val loss: 0.41673147678375244
Epoch 2700, training loss: 900.9098510742188 = 0.20977547764778137 + 100.0 * 9.007000923156738
Epoch 2700, val loss: 0.41690096259117126
Epoch 2710, training loss: 900.9244384765625 = 0.20917998254299164 + 100.0 * 9.007152557373047
Epoch 2710, val loss: 0.4172431230545044
Epoch 2720, training loss: 900.7911376953125 = 0.20858201384544373 + 100.0 * 9.005825996398926
Epoch 2720, val loss: 0.4184325039386749
Epoch 2730, training loss: 900.767333984375 = 0.20798033475875854 + 100.0 * 9.005593299865723
Epoch 2730, val loss: 0.41876325011253357
Epoch 2740, training loss: 900.7542114257812 = 0.20738768577575684 + 100.0 * 9.005468368530273
Epoch 2740, val loss: 0.4192511737346649
Epoch 2750, training loss: 900.8680419921875 = 0.20680081844329834 + 100.0 * 9.006612777709961
Epoch 2750, val loss: 0.41976985335350037
Epoch 2760, training loss: 900.7122802734375 = 0.206197127699852 + 100.0 * 9.005061149597168
Epoch 2760, val loss: 0.4202464520931244
Epoch 2770, training loss: 900.7365112304688 = 0.20560681819915771 + 100.0 * 9.005309104919434
Epoch 2770, val loss: 0.420612633228302
Epoch 2780, training loss: 901.0656127929688 = 0.20500248670578003 + 100.0 * 9.00860595703125
Epoch 2780, val loss: 0.4211687445640564
Epoch 2790, training loss: 900.8009033203125 = 0.20442412793636322 + 100.0 * 9.005965232849121
Epoch 2790, val loss: 0.4214921295642853
Epoch 2800, training loss: 900.7205810546875 = 0.2038322538137436 + 100.0 * 9.005167007446289
Epoch 2800, val loss: 0.42199528217315674
Epoch 2810, training loss: 900.748291015625 = 0.20324043929576874 + 100.0 * 9.005450248718262
Epoch 2810, val loss: 0.42254337668418884
Epoch 2820, training loss: 900.6841430664062 = 0.20265251398086548 + 100.0 * 9.004815101623535
Epoch 2820, val loss: 0.4230388104915619
Epoch 2830, training loss: 900.6917114257812 = 0.20207418501377106 + 100.0 * 9.00489616394043
Epoch 2830, val loss: 0.42356815934181213
Epoch 2840, training loss: 900.7918701171875 = 0.2014838457107544 + 100.0 * 9.005904197692871
Epoch 2840, val loss: 0.4237671196460724
Epoch 2850, training loss: 900.85888671875 = 0.20093201100826263 + 100.0 * 9.006579399108887
Epoch 2850, val loss: 0.42395034432411194
Epoch 2860, training loss: 900.7137451171875 = 0.20034176111221313 + 100.0 * 9.005134582519531
Epoch 2860, val loss: 0.42539677023887634
Epoch 2870, training loss: 900.6563110351562 = 0.19975243508815765 + 100.0 * 9.004565238952637
Epoch 2870, val loss: 0.4256645739078522
Epoch 2880, training loss: 900.7025756835938 = 0.19918178021907806 + 100.0 * 9.005034446716309
Epoch 2880, val loss: 0.4258418679237366
Epoch 2890, training loss: 900.7202758789062 = 0.19859528541564941 + 100.0 * 9.005216598510742
Epoch 2890, val loss: 0.42669203877449036
Epoch 2900, training loss: 900.665283203125 = 0.19803394377231598 + 100.0 * 9.004672050476074
Epoch 2900, val loss: 0.42761868238449097
Epoch 2910, training loss: 900.66455078125 = 0.1974502056837082 + 100.0 * 9.004671096801758
Epoch 2910, val loss: 0.4275679588317871
Epoch 2920, training loss: 900.7361450195312 = 0.19686327874660492 + 100.0 * 9.005393028259277
Epoch 2920, val loss: 0.42842787504196167
Epoch 2930, training loss: 900.6107788085938 = 0.19631651043891907 + 100.0 * 9.004144668579102
Epoch 2930, val loss: 0.42907974123954773
Epoch 2940, training loss: 900.5919189453125 = 0.19574151933193207 + 100.0 * 9.003961563110352
Epoch 2940, val loss: 0.42965415120124817
Epoch 2950, training loss: 900.79248046875 = 0.19517715275287628 + 100.0 * 9.005972862243652
Epoch 2950, val loss: 0.43013957142829895
Epoch 2960, training loss: 900.6402587890625 = 0.194600448012352 + 100.0 * 9.004456520080566
Epoch 2960, val loss: 0.4309311509132385
Epoch 2970, training loss: 900.5492553710938 = 0.1940227448940277 + 100.0 * 9.003552436828613
Epoch 2970, val loss: 0.4312496781349182
Epoch 2980, training loss: 900.5956420898438 = 0.19346150755882263 + 100.0 * 9.004021644592285
Epoch 2980, val loss: 0.43218547105789185
Epoch 2990, training loss: 900.66455078125 = 0.19289511442184448 + 100.0 * 9.004716873168945
Epoch 2990, val loss: 0.4326885938644409
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8564
Overall ASR: 0.7657
Flip ASR: 0.7085/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1037.0048828125 = 1.1006428003311157 + 100.0 * 10.35904312133789
Epoch 0, val loss: 1.0991039276123047
Epoch 10, training loss: 1036.6451416015625 = 1.0905606746673584 + 100.0 * 10.355545997619629
Epoch 10, val loss: 1.0886586904525757
Epoch 20, training loss: 1030.940673828125 = 1.0764106512069702 + 100.0 * 10.2986421585083
Epoch 20, val loss: 1.0743087530136108
Epoch 30, training loss: 973.4874267578125 = 1.060814380645752 + 100.0 * 9.724266052246094
Epoch 30, val loss: 1.058720350265503
Epoch 40, training loss: 953.1014404296875 = 1.0483652353286743 + 100.0 * 9.520530700683594
Epoch 40, val loss: 1.0467664003372192
Epoch 50, training loss: 941.7888793945312 = 1.0375151634216309 + 100.0 * 9.407513618469238
Epoch 50, val loss: 1.036105990409851
Epoch 60, training loss: 937.787841796875 = 1.0260906219482422 + 100.0 * 9.3676176071167
Epoch 60, val loss: 1.02479887008667
Epoch 70, training loss: 935.1596069335938 = 1.0150232315063477 + 100.0 * 9.341445922851562
Epoch 70, val loss: 1.0140825510025024
Epoch 80, training loss: 931.7586669921875 = 1.007285237312317 + 100.0 * 9.307514190673828
Epoch 80, val loss: 1.0066300630569458
Epoch 90, training loss: 927.775146484375 = 1.0003578662872314 + 100.0 * 9.26774787902832
Epoch 90, val loss: 0.9998173117637634
Epoch 100, training loss: 923.2359008789062 = 0.9922000765800476 + 100.0 * 9.222436904907227
Epoch 100, val loss: 0.992000162601471
Epoch 110, training loss: 920.1008911132812 = 0.983463704586029 + 100.0 * 9.191174507141113
Epoch 110, val loss: 0.9838072657585144
Epoch 120, training loss: 918.1986083984375 = 0.9751527905464172 + 100.0 * 9.172234535217285
Epoch 120, val loss: 0.9759691953659058
Epoch 130, training loss: 916.5523071289062 = 0.9665089845657349 + 100.0 * 9.155858039855957
Epoch 130, val loss: 0.9678306579589844
Epoch 140, training loss: 915.3399658203125 = 0.957028329372406 + 100.0 * 9.143829345703125
Epoch 140, val loss: 0.9588852524757385
Epoch 150, training loss: 914.26025390625 = 0.947117805480957 + 100.0 * 9.13313102722168
Epoch 150, val loss: 0.9495100975036621
Epoch 160, training loss: 913.3580932617188 = 0.9372640252113342 + 100.0 * 9.124208450317383
Epoch 160, val loss: 0.9401941895484924
Epoch 170, training loss: 912.6138305664062 = 0.9273272156715393 + 100.0 * 9.116865158081055
Epoch 170, val loss: 0.9309439063072205
Epoch 180, training loss: 912.2177734375 = 0.916113018989563 + 100.0 * 9.113016128540039
Epoch 180, val loss: 0.9201741814613342
Epoch 190, training loss: 911.4542236328125 = 0.9041793346405029 + 100.0 * 9.105500221252441
Epoch 190, val loss: 0.909043550491333
Epoch 200, training loss: 910.9061279296875 = 0.8924558758735657 + 100.0 * 9.100136756896973
Epoch 200, val loss: 0.8979606628417969
Epoch 210, training loss: 910.761474609375 = 0.8800550103187561 + 100.0 * 9.098814010620117
Epoch 210, val loss: 0.8861408233642578
Epoch 220, training loss: 910.0750732421875 = 0.8668939471244812 + 100.0 * 9.092082023620605
Epoch 220, val loss: 0.8737384080886841
Epoch 230, training loss: 909.67236328125 = 0.8540143966674805 + 100.0 * 9.088183403015137
Epoch 230, val loss: 0.8616209030151367
Epoch 240, training loss: 909.645751953125 = 0.841038703918457 + 100.0 * 9.08804702758789
Epoch 240, val loss: 0.8493590950965881
Epoch 250, training loss: 909.081298828125 = 0.8271379470825195 + 100.0 * 9.082541465759277
Epoch 250, val loss: 0.8361867666244507
Epoch 260, training loss: 908.7775268554688 = 0.8133119344711304 + 100.0 * 9.079642295837402
Epoch 260, val loss: 0.8231991529464722
Epoch 270, training loss: 908.4767456054688 = 0.7996712923049927 + 100.0 * 9.076770782470703
Epoch 270, val loss: 0.8103104829788208
Epoch 280, training loss: 908.4422607421875 = 0.7857761979103088 + 100.0 * 9.07656478881836
Epoch 280, val loss: 0.7971588969230652
Epoch 290, training loss: 907.9896850585938 = 0.7712603211402893 + 100.0 * 9.072184562683105
Epoch 290, val loss: 0.7835900783538818
Epoch 300, training loss: 907.7608642578125 = 0.7572487592697144 + 100.0 * 9.070035934448242
Epoch 300, val loss: 0.7704330682754517
Epoch 310, training loss: 907.7290649414062 = 0.7430945634841919 + 100.0 * 9.069859504699707
Epoch 310, val loss: 0.757114589214325
Epoch 320, training loss: 907.3735961914062 = 0.728670597076416 + 100.0 * 9.066449165344238
Epoch 320, val loss: 0.743674635887146
Epoch 330, training loss: 907.1686401367188 = 0.7149221897125244 + 100.0 * 9.064537048339844
Epoch 330, val loss: 0.7308821082115173
Epoch 340, training loss: 907.1105346679688 = 0.7010841965675354 + 100.0 * 9.064094543457031
Epoch 340, val loss: 0.7178459167480469
Epoch 350, training loss: 906.8682861328125 = 0.6870482563972473 + 100.0 * 9.061812400817871
Epoch 350, val loss: 0.7048873901367188
Epoch 360, training loss: 906.6509399414062 = 0.6736930012702942 + 100.0 * 9.059772491455078
Epoch 360, val loss: 0.6924905776977539
Epoch 370, training loss: 907.15576171875 = 0.6602566838264465 + 100.0 * 9.06495475769043
Epoch 370, val loss: 0.6800161600112915
Epoch 380, training loss: 906.5757446289062 = 0.6463388800621033 + 100.0 * 9.059293746948242
Epoch 380, val loss: 0.6670999526977539
Epoch 390, training loss: 906.2784423828125 = 0.6336514353752136 + 100.0 * 9.056447982788086
Epoch 390, val loss: 0.6554229855537415
Epoch 400, training loss: 906.1898803710938 = 0.6215660572052002 + 100.0 * 9.055683135986328
Epoch 400, val loss: 0.6443552374839783
Epoch 410, training loss: 906.0911254882812 = 0.6091746687889099 + 100.0 * 9.054819107055664
Epoch 410, val loss: 0.6326597929000854
Epoch 420, training loss: 905.9414672851562 = 0.5973067283630371 + 100.0 * 9.053442001342773
Epoch 420, val loss: 0.6219709515571594
Epoch 430, training loss: 905.8318481445312 = 0.5862572193145752 + 100.0 * 9.05245590209961
Epoch 430, val loss: 0.6119056940078735
Epoch 440, training loss: 905.7650756835938 = 0.5755428671836853 + 100.0 * 9.051895141601562
Epoch 440, val loss: 0.6019906401634216
Epoch 450, training loss: 905.9481811523438 = 0.5643579959869385 + 100.0 * 9.053838729858398
Epoch 450, val loss: 0.5919077396392822
Epoch 460, training loss: 905.669921875 = 0.5540386438369751 + 100.0 * 9.051158905029297
Epoch 460, val loss: 0.582642138004303
Epoch 470, training loss: 905.4749145507812 = 0.5448264479637146 + 100.0 * 9.049301147460938
Epoch 470, val loss: 0.5742824673652649
Epoch 480, training loss: 905.435302734375 = 0.535868227481842 + 100.0 * 9.048994064331055
Epoch 480, val loss: 0.566248893737793
Epoch 490, training loss: 905.3193359375 = 0.5268696546554565 + 100.0 * 9.047924995422363
Epoch 490, val loss: 0.5580998063087463
Epoch 500, training loss: 905.299560546875 = 0.5184981226921082 + 100.0 * 9.047810554504395
Epoch 500, val loss: 0.5507045984268188
Epoch 510, training loss: 905.1715087890625 = 0.5107281804084778 + 100.0 * 9.046607971191406
Epoch 510, val loss: 0.5438179969787598
Epoch 520, training loss: 905.1063842773438 = 0.5032870769500732 + 100.0 * 9.04603099822998
Epoch 520, val loss: 0.5372498631477356
Epoch 530, training loss: 905.2257080078125 = 0.49580928683280945 + 100.0 * 9.0472993850708
Epoch 530, val loss: 0.530582845211029
Epoch 540, training loss: 905.0972900390625 = 0.4886268079280853 + 100.0 * 9.046086311340332
Epoch 540, val loss: 0.5244498252868652
Epoch 550, training loss: 904.9735107421875 = 0.4823511838912964 + 100.0 * 9.04491138458252
Epoch 550, val loss: 0.5190231800079346
Epoch 560, training loss: 905.1048583984375 = 0.4761849343776703 + 100.0 * 9.046286582946777
Epoch 560, val loss: 0.5136617422103882
Epoch 570, training loss: 904.822509765625 = 0.4703066647052765 + 100.0 * 9.043521881103516
Epoch 570, val loss: 0.5086291432380676
Epoch 580, training loss: 904.748046875 = 0.46487855911254883 + 100.0 * 9.042831420898438
Epoch 580, val loss: 0.5040345191955566
Epoch 590, training loss: 904.9916381835938 = 0.45961689949035645 + 100.0 * 9.045320510864258
Epoch 590, val loss: 0.4994610846042633
Epoch 600, training loss: 904.7034301757812 = 0.4543921649456024 + 100.0 * 9.042490005493164
Epoch 600, val loss: 0.4952513873577118
Epoch 610, training loss: 904.7183837890625 = 0.44970259070396423 + 100.0 * 9.042686462402344
Epoch 610, val loss: 0.491296648979187
Epoch 620, training loss: 904.5900268554688 = 0.44498685002326965 + 100.0 * 9.041450500488281
Epoch 620, val loss: 0.48747366666793823
Epoch 630, training loss: 904.517333984375 = 0.4407542645931244 + 100.0 * 9.040765762329102
Epoch 630, val loss: 0.4839739203453064
Epoch 640, training loss: 904.4568481445312 = 0.43672701716423035 + 100.0 * 9.040201187133789
Epoch 640, val loss: 0.48078393936157227
Epoch 650, training loss: 904.6708374023438 = 0.43279770016670227 + 100.0 * 9.042380332946777
Epoch 650, val loss: 0.47768449783325195
Epoch 660, training loss: 904.4346313476562 = 0.4288516938686371 + 100.0 * 9.040058135986328
Epoch 660, val loss: 0.4744841158390045
Epoch 670, training loss: 904.3114013671875 = 0.4253202974796295 + 100.0 * 9.038861274719238
Epoch 670, val loss: 0.47174185514450073
Epoch 680, training loss: 904.3828125 = 0.4219072759151459 + 100.0 * 9.0396089553833
Epoch 680, val loss: 0.46924644708633423
Epoch 690, training loss: 904.237060546875 = 0.41846901178359985 + 100.0 * 9.038186073303223
Epoch 690, val loss: 0.4662853181362152
Epoch 700, training loss: 904.358642578125 = 0.4152797758579254 + 100.0 * 9.039433479309082
Epoch 700, val loss: 0.4640129506587982
Epoch 710, training loss: 904.18212890625 = 0.4122176468372345 + 100.0 * 9.037698745727539
Epoch 710, val loss: 0.46163907647132874
Epoch 720, training loss: 904.0867309570312 = 0.40934106707572937 + 100.0 * 9.036773681640625
Epoch 720, val loss: 0.45949262380599976
Epoch 730, training loss: 904.0646362304688 = 0.4065525531768799 + 100.0 * 9.036581039428711
Epoch 730, val loss: 0.45746707916259766
Epoch 740, training loss: 904.2366333007812 = 0.40377041697502136 + 100.0 * 9.038329124450684
Epoch 740, val loss: 0.45525112748146057
Epoch 750, training loss: 904.2481689453125 = 0.40094655752182007 + 100.0 * 9.038472175598145
Epoch 750, val loss: 0.4532166123390198
Epoch 760, training loss: 904.0205688476562 = 0.39831504225730896 + 100.0 * 9.036222457885742
Epoch 760, val loss: 0.45165109634399414
Epoch 770, training loss: 903.918212890625 = 0.39597612619400024 + 100.0 * 9.035222053527832
Epoch 770, val loss: 0.4498156011104584
Epoch 780, training loss: 903.8585815429688 = 0.3936734199523926 + 100.0 * 9.034648895263672
Epoch 780, val loss: 0.4482681453227997
Epoch 790, training loss: 904.0379028320312 = 0.3913949131965637 + 100.0 * 9.03646469116211
Epoch 790, val loss: 0.4467102885246277
Epoch 800, training loss: 903.866455078125 = 0.38899359107017517 + 100.0 * 9.034774780273438
Epoch 800, val loss: 0.44514694809913635
Epoch 810, training loss: 903.9292602539062 = 0.38683775067329407 + 100.0 * 9.03542423248291
Epoch 810, val loss: 0.4435643255710602
Epoch 820, training loss: 903.7199096679688 = 0.3846934139728546 + 100.0 * 9.03335189819336
Epoch 820, val loss: 0.4422297179698944
Epoch 830, training loss: 903.7324829101562 = 0.3826809823513031 + 100.0 * 9.03349781036377
Epoch 830, val loss: 0.44084903597831726
Epoch 840, training loss: 903.8006591796875 = 0.3806132674217224 + 100.0 * 9.034200668334961
Epoch 840, val loss: 0.4395418167114258
Epoch 850, training loss: 903.6401977539062 = 0.37861767411231995 + 100.0 * 9.032615661621094
Epoch 850, val loss: 0.43827176094055176
Epoch 860, training loss: 903.6080322265625 = 0.3767637610435486 + 100.0 * 9.032312393188477
Epoch 860, val loss: 0.4373311996459961
Epoch 870, training loss: 903.6830444335938 = 0.37492311000823975 + 100.0 * 9.0330810546875
Epoch 870, val loss: 0.4361364245414734
Epoch 880, training loss: 903.6293334960938 = 0.37299150228500366 + 100.0 * 9.032563209533691
Epoch 880, val loss: 0.43483319878578186
Epoch 890, training loss: 903.5725708007812 = 0.37117400765419006 + 100.0 * 9.032013893127441
Epoch 890, val loss: 0.43383339047431946
Epoch 900, training loss: 903.47265625 = 0.3695102035999298 + 100.0 * 9.031031608581543
Epoch 900, val loss: 0.433024525642395
Epoch 910, training loss: 903.4779663085938 = 0.3678523898124695 + 100.0 * 9.03110122680664
Epoch 910, val loss: 0.43203437328338623
Epoch 920, training loss: 903.587646484375 = 0.366146445274353 + 100.0 * 9.032215118408203
Epoch 920, val loss: 0.4311533570289612
Epoch 930, training loss: 903.5582275390625 = 0.3644621670246124 + 100.0 * 9.031937599182129
Epoch 930, val loss: 0.430166095495224
Epoch 940, training loss: 903.4465942382812 = 0.36277806758880615 + 100.0 * 9.030838012695312
Epoch 940, val loss: 0.4291374981403351
Epoch 950, training loss: 903.4004516601562 = 0.36121001839637756 + 100.0 * 9.03039264678955
Epoch 950, val loss: 0.4281674921512604
Epoch 960, training loss: 903.3867797851562 = 0.3596416413784027 + 100.0 * 9.030271530151367
Epoch 960, val loss: 0.4274486005306244
Epoch 970, training loss: 903.2979736328125 = 0.35812440514564514 + 100.0 * 9.029397964477539
Epoch 970, val loss: 0.42672210931777954
Epoch 980, training loss: 903.3429565429688 = 0.3566014766693115 + 100.0 * 9.029863357543945
Epoch 980, val loss: 0.4260515868663788
Epoch 990, training loss: 903.2340698242188 = 0.35508933663368225 + 100.0 * 9.028789520263672
Epoch 990, val loss: 0.42506808042526245
Epoch 1000, training loss: 903.3563842773438 = 0.3536113202571869 + 100.0 * 9.030027389526367
Epoch 1000, val loss: 0.4241751730442047
Epoch 1010, training loss: 903.27099609375 = 0.3521125316619873 + 100.0 * 9.029189109802246
Epoch 1010, val loss: 0.4237575829029083
Epoch 1020, training loss: 903.2091674804688 = 0.3506636321544647 + 100.0 * 9.028585433959961
Epoch 1020, val loss: 0.422918438911438
Epoch 1030, training loss: 903.190185546875 = 0.34927475452423096 + 100.0 * 9.028409004211426
Epoch 1030, val loss: 0.4225120544433594
Epoch 1040, training loss: 903.2135620117188 = 0.3478282392024994 + 100.0 * 9.028656959533691
Epoch 1040, val loss: 0.42180415987968445
Epoch 1050, training loss: 903.1082763671875 = 0.34645241498947144 + 100.0 * 9.027618408203125
Epoch 1050, val loss: 0.4210522770881653
Epoch 1060, training loss: 903.044189453125 = 0.34510165452957153 + 100.0 * 9.02699089050293
Epoch 1060, val loss: 0.4204977750778198
Epoch 1070, training loss: 903.133544921875 = 0.34376347064971924 + 100.0 * 9.027897834777832
Epoch 1070, val loss: 0.4197808504104614
Epoch 1080, training loss: 903.0546264648438 = 0.3423469364643097 + 100.0 * 9.027122497558594
Epoch 1080, val loss: 0.4193272590637207
Epoch 1090, training loss: 903.062744140625 = 0.3410360515117645 + 100.0 * 9.027216911315918
Epoch 1090, val loss: 0.4184722900390625
Epoch 1100, training loss: 903.031494140625 = 0.3397367000579834 + 100.0 * 9.026917457580566
Epoch 1100, val loss: 0.4179900884628296
Epoch 1110, training loss: 902.9848022460938 = 0.33845213055610657 + 100.0 * 9.026463508605957
Epoch 1110, val loss: 0.41762611269950867
Epoch 1120, training loss: 902.9833984375 = 0.3371652662754059 + 100.0 * 9.02646255493164
Epoch 1120, val loss: 0.4169842600822449
Epoch 1130, training loss: 902.9012451171875 = 0.3359077572822571 + 100.0 * 9.025652885437012
Epoch 1130, val loss: 0.41663309931755066
Epoch 1140, training loss: 902.9271240234375 = 0.33467188477516174 + 100.0 * 9.025924682617188
Epoch 1140, val loss: 0.41620495915412903
Epoch 1150, training loss: 902.9483032226562 = 0.33340486884117126 + 100.0 * 9.026148796081543
Epoch 1150, val loss: 0.4155619740486145
Epoch 1160, training loss: 902.9877319335938 = 0.3321516811847687 + 100.0 * 9.026556015014648
Epoch 1160, val loss: 0.4151657223701477
Epoch 1170, training loss: 902.8244018554688 = 0.33093753457069397 + 100.0 * 9.024934768676758
Epoch 1170, val loss: 0.4146742522716522
Epoch 1180, training loss: 902.7582397460938 = 0.32977280020713806 + 100.0 * 9.024284362792969
Epoch 1180, val loss: 0.4141157865524292
Epoch 1190, training loss: 902.7425537109375 = 0.3286026418209076 + 100.0 * 9.024139404296875
Epoch 1190, val loss: 0.41379520297050476
Epoch 1200, training loss: 902.9622802734375 = 0.327412486076355 + 100.0 * 9.026349067687988
Epoch 1200, val loss: 0.41329535841941833
Epoch 1210, training loss: 902.849365234375 = 0.32620951533317566 + 100.0 * 9.02523136138916
Epoch 1210, val loss: 0.4131079316139221
Epoch 1220, training loss: 902.8505249023438 = 0.3250058889389038 + 100.0 * 9.02525520324707
Epoch 1220, val loss: 0.4125511646270752
Epoch 1230, training loss: 902.6927490234375 = 0.32390037178993225 + 100.0 * 9.023688316345215
Epoch 1230, val loss: 0.41199129819869995
Epoch 1240, training loss: 902.6805419921875 = 0.3227802813053131 + 100.0 * 9.023577690124512
Epoch 1240, val loss: 0.41190335154533386
Epoch 1250, training loss: 902.76953125 = 0.32165589928627014 + 100.0 * 9.024478912353516
Epoch 1250, val loss: 0.41145244240760803
Epoch 1260, training loss: 902.6473388671875 = 0.3204958438873291 + 100.0 * 9.023268699645996
Epoch 1260, val loss: 0.4107758700847626
Epoch 1270, training loss: 902.5637817382812 = 0.3194099962711334 + 100.0 * 9.022443771362305
Epoch 1270, val loss: 0.41069892048835754
Epoch 1280, training loss: 902.5379028320312 = 0.3183120787143707 + 100.0 * 9.022195816040039
Epoch 1280, val loss: 0.4101053774356842
Epoch 1290, training loss: 902.6548461914062 = 0.3172072470188141 + 100.0 * 9.02337646484375
Epoch 1290, val loss: 0.4097835421562195
Epoch 1300, training loss: 902.548095703125 = 0.3160761892795563 + 100.0 * 9.022319793701172
Epoch 1300, val loss: 0.40963125228881836
Epoch 1310, training loss: 902.509521484375 = 0.31498146057128906 + 100.0 * 9.021944999694824
Epoch 1310, val loss: 0.40903565287590027
Epoch 1320, training loss: 902.4850463867188 = 0.3139323890209198 + 100.0 * 9.021711349487305
Epoch 1320, val loss: 0.40893644094467163
Epoch 1330, training loss: 902.5331420898438 = 0.31286293268203735 + 100.0 * 9.022202491760254
Epoch 1330, val loss: 0.4086366891860962
Epoch 1340, training loss: 902.6317749023438 = 0.3117782771587372 + 100.0 * 9.023200035095215
Epoch 1340, val loss: 0.4086243510246277
Epoch 1350, training loss: 902.427490234375 = 0.3107154071331024 + 100.0 * 9.021167755126953
Epoch 1350, val loss: 0.4078090786933899
Epoch 1360, training loss: 902.3905029296875 = 0.30969932675361633 + 100.0 * 9.020808219909668
Epoch 1360, val loss: 0.40766042470932007
Epoch 1370, training loss: 902.5690307617188 = 0.3086734116077423 + 100.0 * 9.022603988647461
Epoch 1370, val loss: 0.4071134328842163
Epoch 1380, training loss: 902.3689575195312 = 0.3076244592666626 + 100.0 * 9.020613670349121
Epoch 1380, val loss: 0.40714386105537415
Epoch 1390, training loss: 902.3526611328125 = 0.30660825967788696 + 100.0 * 9.020461082458496
Epoch 1390, val loss: 0.40668007731437683
Epoch 1400, training loss: 902.4204711914062 = 0.3056061267852783 + 100.0 * 9.021148681640625
Epoch 1400, val loss: 0.40647757053375244
Epoch 1410, training loss: 902.3912963867188 = 0.30459243059158325 + 100.0 * 9.020867347717285
Epoch 1410, val loss: 0.4064888656139374
Epoch 1420, training loss: 902.3032836914062 = 0.3035573661327362 + 100.0 * 9.019997596740723
Epoch 1420, val loss: 0.4061601459980011
Epoch 1430, training loss: 902.2882080078125 = 0.30256882309913635 + 100.0 * 9.019856452941895
Epoch 1430, val loss: 0.4058051109313965
Epoch 1440, training loss: 902.5569458007812 = 0.30157437920570374 + 100.0 * 9.022553443908691
Epoch 1440, val loss: 0.4056333005428314
Epoch 1450, training loss: 902.29052734375 = 0.30058932304382324 + 100.0 * 9.019899368286133
Epoch 1450, val loss: 0.4052734375
Epoch 1460, training loss: 902.2022705078125 = 0.2996278703212738 + 100.0 * 9.019026756286621
Epoch 1460, val loss: 0.405038446187973
Epoch 1470, training loss: 902.2090454101562 = 0.29866719245910645 + 100.0 * 9.01910400390625
Epoch 1470, val loss: 0.4049571752548218
Epoch 1480, training loss: 902.4249267578125 = 0.2976763844490051 + 100.0 * 9.021272659301758
Epoch 1480, val loss: 0.4044678807258606
Epoch 1490, training loss: 902.1786499023438 = 0.29670676589012146 + 100.0 * 9.018819808959961
Epoch 1490, val loss: 0.40456825494766235
Epoch 1500, training loss: 902.1259765625 = 0.29576560854911804 + 100.0 * 9.018301963806152
Epoch 1500, val loss: 0.4042154848575592
Epoch 1510, training loss: 902.1792602539062 = 0.29482409358024597 + 100.0 * 9.018844604492188
Epoch 1510, val loss: 0.40444937348365784
Epoch 1520, training loss: 902.3096313476562 = 0.29382848739624023 + 100.0 * 9.020157814025879
Epoch 1520, val loss: 0.4040355682373047
Epoch 1530, training loss: 902.0693969726562 = 0.2928983271121979 + 100.0 * 9.017765045166016
Epoch 1530, val loss: 0.4037569463253021
Epoch 1540, training loss: 902.0811767578125 = 0.29199060797691345 + 100.0 * 9.017891883850098
Epoch 1540, val loss: 0.4035414755344391
Epoch 1550, training loss: 902.0863647460938 = 0.2910662889480591 + 100.0 * 9.017952919006348
Epoch 1550, val loss: 0.4035632908344269
Epoch 1560, training loss: 902.2749633789062 = 0.29012149572372437 + 100.0 * 9.019848823547363
Epoch 1560, val loss: 0.40338006615638733
Epoch 1570, training loss: 902.0914306640625 = 0.2891981601715088 + 100.0 * 9.018022537231445
Epoch 1570, val loss: 0.40298932790756226
Epoch 1580, training loss: 902.0137329101562 = 0.2882860600948334 + 100.0 * 9.017254829406738
Epoch 1580, val loss: 0.402990460395813
Epoch 1590, training loss: 902.0177001953125 = 0.2873721420764923 + 100.0 * 9.017303466796875
Epoch 1590, val loss: 0.40283215045928955
Epoch 1600, training loss: 902.205810546875 = 0.28643783926963806 + 100.0 * 9.019193649291992
Epoch 1600, val loss: 0.4026834964752197
Epoch 1610, training loss: 902.136962890625 = 0.28554701805114746 + 100.0 * 9.018514633178711
Epoch 1610, val loss: 0.402536541223526
Epoch 1620, training loss: 902.0881958007812 = 0.2846345901489258 + 100.0 * 9.018035888671875
Epoch 1620, val loss: 0.40243226289749146
Epoch 1630, training loss: 901.9476928710938 = 0.28372663259506226 + 100.0 * 9.016639709472656
Epoch 1630, val loss: 0.4022125005722046
Epoch 1640, training loss: 901.9378662109375 = 0.28285810351371765 + 100.0 * 9.016550064086914
Epoch 1640, val loss: 0.4021318554878235
Epoch 1650, training loss: 901.9205932617188 = 0.2819735109806061 + 100.0 * 9.016386032104492
Epoch 1650, val loss: 0.402037650346756
Epoch 1660, training loss: 901.896484375 = 0.2810823619365692 + 100.0 * 9.016154289245605
Epoch 1660, val loss: 0.40181270241737366
Epoch 1670, training loss: 901.8795166015625 = 0.2801968455314636 + 100.0 * 9.015993118286133
Epoch 1670, val loss: 0.40168628096580505
Epoch 1680, training loss: 901.8883056640625 = 0.27930739521980286 + 100.0 * 9.016090393066406
Epoch 1680, val loss: 0.40165799856185913
Epoch 1690, training loss: 902.009765625 = 0.27842485904693604 + 100.0 * 9.017313003540039
Epoch 1690, val loss: 0.4016169011592865
Epoch 1700, training loss: 901.8666381835938 = 0.2775537073612213 + 100.0 * 9.015891075134277
Epoch 1700, val loss: 0.4017125070095062
Epoch 1710, training loss: 901.9102172851562 = 0.2766842246055603 + 100.0 * 9.016335487365723
Epoch 1710, val loss: 0.40169888734817505
Epoch 1720, training loss: 902.0037841796875 = 0.27578726410865784 + 100.0 * 9.017280578613281
Epoch 1720, val loss: 0.4011790156364441
Epoch 1730, training loss: 901.7860717773438 = 0.2749444544315338 + 100.0 * 9.015110969543457
Epoch 1730, val loss: 0.40153607726097107
Epoch 1740, training loss: 901.787109375 = 0.2741071283817291 + 100.0 * 9.015130043029785
Epoch 1740, val loss: 0.40138471126556396
Epoch 1750, training loss: 901.7630615234375 = 0.2732512354850769 + 100.0 * 9.014898300170898
Epoch 1750, val loss: 0.40138161182403564
Epoch 1760, training loss: 901.9315795898438 = 0.27238431572914124 + 100.0 * 9.016592025756836
Epoch 1760, val loss: 0.401462197303772
Epoch 1770, training loss: 901.7659301757812 = 0.2715436518192291 + 100.0 * 9.014944076538086
Epoch 1770, val loss: 0.401330828666687
Epoch 1780, training loss: 901.939697265625 = 0.2706666588783264 + 100.0 * 9.016690254211426
Epoch 1780, val loss: 0.4012204706668854
Epoch 1790, training loss: 901.7392578125 = 0.26985451579093933 + 100.0 * 9.014694213867188
Epoch 1790, val loss: 0.4011765122413635
Epoch 1800, training loss: 901.680908203125 = 0.26900967955589294 + 100.0 * 9.014119148254395
Epoch 1800, val loss: 0.40126579999923706
Epoch 1810, training loss: 901.6935424804688 = 0.26817965507507324 + 100.0 * 9.014253616333008
Epoch 1810, val loss: 0.4011725187301636
Epoch 1820, training loss: 901.869873046875 = 0.26732486486434937 + 100.0 * 9.01602554321289
Epoch 1820, val loss: 0.40104806423187256
Epoch 1830, training loss: 901.6849975585938 = 0.26648858189582825 + 100.0 * 9.014184951782227
Epoch 1830, val loss: 0.40133777260780334
Epoch 1840, training loss: 901.6179809570312 = 0.265668123960495 + 100.0 * 9.01352310180664
Epoch 1840, val loss: 0.4011911153793335
Epoch 1850, training loss: 901.63037109375 = 0.2648333013057709 + 100.0 * 9.013655662536621
Epoch 1850, val loss: 0.40125444531440735
Epoch 1860, training loss: 901.8804321289062 = 0.2640060782432556 + 100.0 * 9.016164779663086
Epoch 1860, val loss: 0.40114620327949524
Epoch 1870, training loss: 901.62744140625 = 0.2631787657737732 + 100.0 * 9.013642311096191
Epoch 1870, val loss: 0.4016334116458893
Epoch 1880, training loss: 901.6074829101562 = 0.2623758018016815 + 100.0 * 9.013450622558594
Epoch 1880, val loss: 0.40157610177993774
Epoch 1890, training loss: 901.5560302734375 = 0.26156309247016907 + 100.0 * 9.012945175170898
Epoch 1890, val loss: 0.4016374349594116
Epoch 1900, training loss: 901.5526733398438 = 0.2607525885105133 + 100.0 * 9.012919425964355
Epoch 1900, val loss: 0.40161576867103577
Epoch 1910, training loss: 901.6532592773438 = 0.25992974638938904 + 100.0 * 9.013933181762695
Epoch 1910, val loss: 0.40163880586624146
Epoch 1920, training loss: 901.669677734375 = 0.2591046988964081 + 100.0 * 9.014105796813965
Epoch 1920, val loss: 0.40103262662887573
Epoch 1930, training loss: 901.50341796875 = 0.25830137729644775 + 100.0 * 9.012451171875
Epoch 1930, val loss: 0.40176090598106384
Epoch 1940, training loss: 901.49853515625 = 0.2575102746486664 + 100.0 * 9.012410163879395
Epoch 1940, val loss: 0.4016055166721344
Epoch 1950, training loss: 901.593017578125 = 0.2567157745361328 + 100.0 * 9.013362884521484
Epoch 1950, val loss: 0.4017443358898163
Epoch 1960, training loss: 901.5640869140625 = 0.2559317946434021 + 100.0 * 9.013081550598145
Epoch 1960, val loss: 0.4018818736076355
Epoch 1970, training loss: 901.4634399414062 = 0.25512760877609253 + 100.0 * 9.012083053588867
Epoch 1970, val loss: 0.4019908010959625
Epoch 1980, training loss: 901.4302978515625 = 0.25433382391929626 + 100.0 * 9.011759757995605
Epoch 1980, val loss: 0.4018639922142029
Epoch 1990, training loss: 901.4471435546875 = 0.2535557746887207 + 100.0 * 9.01193618774414
Epoch 1990, val loss: 0.4020160436630249
Epoch 2000, training loss: 901.704345703125 = 0.25276458263397217 + 100.0 * 9.01451587677002
Epoch 2000, val loss: 0.40186718106269836
Epoch 2010, training loss: 901.48291015625 = 0.25198686122894287 + 100.0 * 9.012309074401855
Epoch 2010, val loss: 0.40255680680274963
Epoch 2020, training loss: 901.4100952148438 = 0.2511870265007019 + 100.0 * 9.011589050292969
Epoch 2020, val loss: 0.4022948741912842
Epoch 2030, training loss: 901.477783203125 = 0.2504142224788666 + 100.0 * 9.012273788452148
Epoch 2030, val loss: 0.4024564027786255
Epoch 2040, training loss: 901.4143676757812 = 0.24963046610355377 + 100.0 * 9.01164722442627
Epoch 2040, val loss: 0.40282899141311646
Epoch 2050, training loss: 901.4639282226562 = 0.24885286390781403 + 100.0 * 9.012150764465332
Epoch 2050, val loss: 0.40293896198272705
Epoch 2060, training loss: 901.3878784179688 = 0.24805957078933716 + 100.0 * 9.011398315429688
Epoch 2060, val loss: 0.40280935168266296
Epoch 2070, training loss: 901.34765625 = 0.24730542302131653 + 100.0 * 9.011003494262695
Epoch 2070, val loss: 0.4032396078109741
Epoch 2080, training loss: 901.4932861328125 = 0.246540829539299 + 100.0 * 9.012467384338379
Epoch 2080, val loss: 0.40346142649650574
Epoch 2090, training loss: 901.4766235351562 = 0.24576595425605774 + 100.0 * 9.012308120727539
Epoch 2090, val loss: 0.4031807780265808
Epoch 2100, training loss: 901.3870849609375 = 0.24499784409999847 + 100.0 * 9.011421203613281
Epoch 2100, val loss: 0.4035462737083435
Epoch 2110, training loss: 901.29150390625 = 0.24425368010997772 + 100.0 * 9.010472297668457
Epoch 2110, val loss: 0.4038921892642975
Epoch 2120, training loss: 901.25634765625 = 0.24351590871810913 + 100.0 * 9.010128021240234
Epoch 2120, val loss: 0.40401071310043335
Epoch 2130, training loss: 901.233154296875 = 0.2427477091550827 + 100.0 * 9.009903907775879
Epoch 2130, val loss: 0.40411946177482605
Epoch 2140, training loss: 901.3601684570312 = 0.24198663234710693 + 100.0 * 9.011181831359863
Epoch 2140, val loss: 0.40426018834114075
Epoch 2150, training loss: 901.2571411132812 = 0.2412266880273819 + 100.0 * 9.010159492492676
Epoch 2150, val loss: 0.40441739559173584
Epoch 2160, training loss: 901.229248046875 = 0.24047867953777313 + 100.0 * 9.0098876953125
Epoch 2160, val loss: 0.40459707379341125
Epoch 2170, training loss: 901.5203247070312 = 0.23973852396011353 + 100.0 * 9.012805938720703
Epoch 2170, val loss: 0.40509849786758423
Epoch 2180, training loss: 901.3017578125 = 0.23900599777698517 + 100.0 * 9.010627746582031
Epoch 2180, val loss: 0.40504974126815796
Epoch 2190, training loss: 901.2020874023438 = 0.23826396465301514 + 100.0 * 9.009637832641602
Epoch 2190, val loss: 0.40529635548591614
Epoch 2200, training loss: 901.157958984375 = 0.2375379502773285 + 100.0 * 9.009203910827637
Epoch 2200, val loss: 0.4055599272251129
Epoch 2210, training loss: 901.220703125 = 0.23680758476257324 + 100.0 * 9.009839057922363
Epoch 2210, val loss: 0.4059355556964874
Epoch 2220, training loss: 901.2993774414062 = 0.23604176938533783 + 100.0 * 9.01063346862793
Epoch 2220, val loss: 0.40596139430999756
Epoch 2230, training loss: 901.203857421875 = 0.23531611263751984 + 100.0 * 9.009685516357422
Epoch 2230, val loss: 0.4062534272670746
Epoch 2240, training loss: 901.2217407226562 = 0.23462554812431335 + 100.0 * 9.009871482849121
Epoch 2240, val loss: 0.40672191977500916
Epoch 2250, training loss: 901.2283935546875 = 0.23387445509433746 + 100.0 * 9.009944915771484
Epoch 2250, val loss: 0.4068794250488281
Epoch 2260, training loss: 901.1163330078125 = 0.23316846787929535 + 100.0 * 9.008831977844238
Epoch 2260, val loss: 0.40667086839675903
Epoch 2270, training loss: 901.0794677734375 = 0.2324565351009369 + 100.0 * 9.008469581604004
Epoch 2270, val loss: 0.4071297347545624
Epoch 2280, training loss: 901.063720703125 = 0.23172400891780853 + 100.0 * 9.008319854736328
Epoch 2280, val loss: 0.4071498215198517
Epoch 2290, training loss: 901.1170043945312 = 0.23100340366363525 + 100.0 * 9.008859634399414
Epoch 2290, val loss: 0.40766826272010803
Epoch 2300, training loss: 901.2559814453125 = 0.23027212917804718 + 100.0 * 9.01025676727295
Epoch 2300, val loss: 0.4076714813709259
Epoch 2310, training loss: 901.0656127929688 = 0.22957053780555725 + 100.0 * 9.008360862731934
Epoch 2310, val loss: 0.4079049527645111
Epoch 2320, training loss: 901.169189453125 = 0.22886896133422852 + 100.0 * 9.009403228759766
Epoch 2320, val loss: 0.4076603353023529
Epoch 2330, training loss: 901.1111450195312 = 0.22815589606761932 + 100.0 * 9.008830070495605
Epoch 2330, val loss: 0.40867915749549866
Epoch 2340, training loss: 901.0277099609375 = 0.2274499237537384 + 100.0 * 9.008002281188965
Epoch 2340, val loss: 0.40874868631362915
Epoch 2350, training loss: 900.9825439453125 = 0.2267535924911499 + 100.0 * 9.00755786895752
Epoch 2350, val loss: 0.40906691551208496
Epoch 2360, training loss: 900.9888305664062 = 0.2260589301586151 + 100.0 * 9.007627487182617
Epoch 2360, val loss: 0.4095022678375244
Epoch 2370, training loss: 901.0716552734375 = 0.2253502905368805 + 100.0 * 9.008462905883789
Epoch 2370, val loss: 0.409746915102005
Epoch 2380, training loss: 901.129638671875 = 0.2246338576078415 + 100.0 * 9.009050369262695
Epoch 2380, val loss: 0.41022226214408875
Epoch 2390, training loss: 901.0722045898438 = 0.2239236980676651 + 100.0 * 9.008482933044434
Epoch 2390, val loss: 0.410011887550354
Epoch 2400, training loss: 900.96923828125 = 0.22323064506053925 + 100.0 * 9.007460594177246
Epoch 2400, val loss: 0.41056859493255615
Epoch 2410, training loss: 900.94140625 = 0.22255420684814453 + 100.0 * 9.00718879699707
Epoch 2410, val loss: 0.4108325242996216
Epoch 2420, training loss: 900.96240234375 = 0.22186832129955292 + 100.0 * 9.007405281066895
Epoch 2420, val loss: 0.41120824217796326
Epoch 2430, training loss: 901.1487426757812 = 0.221181258559227 + 100.0 * 9.009275436401367
Epoch 2430, val loss: 0.4113351106643677
Epoch 2440, training loss: 901.0778198242188 = 0.22047598659992218 + 100.0 * 9.008573532104492
Epoch 2440, val loss: 0.41206103563308716
Epoch 2450, training loss: 901.0125122070312 = 0.2198093980550766 + 100.0 * 9.007926940917969
Epoch 2450, val loss: 0.4125131666660309
Epoch 2460, training loss: 900.8855590820312 = 0.21911035478115082 + 100.0 * 9.006664276123047
Epoch 2460, val loss: 0.4123704731464386
Epoch 2470, training loss: 900.8799438476562 = 0.21843750774860382 + 100.0 * 9.006614685058594
Epoch 2470, val loss: 0.4129614233970642
Epoch 2480, training loss: 901.0283203125 = 0.2177797555923462 + 100.0 * 9.008105278015137
Epoch 2480, val loss: 0.41354408860206604
Epoch 2490, training loss: 900.9284057617188 = 0.21705786883831024 + 100.0 * 9.007113456726074
Epoch 2490, val loss: 0.41342997550964355
Epoch 2500, training loss: 900.8452758789062 = 0.21640363335609436 + 100.0 * 9.006288528442383
Epoch 2500, val loss: 0.4143599271774292
Epoch 2510, training loss: 900.8179321289062 = 0.21572420001029968 + 100.0 * 9.006022453308105
Epoch 2510, val loss: 0.4145016372203827
Epoch 2520, training loss: 900.8291625976562 = 0.21504129469394684 + 100.0 * 9.006141662597656
Epoch 2520, val loss: 0.41474008560180664
Epoch 2530, training loss: 901.2576904296875 = 0.21434888243675232 + 100.0 * 9.010433197021484
Epoch 2530, val loss: 0.41517874598503113
Epoch 2540, training loss: 900.9559936523438 = 0.2137056291103363 + 100.0 * 9.007423400878906
Epoch 2540, val loss: 0.4156111478805542
Epoch 2550, training loss: 900.82861328125 = 0.21302103996276855 + 100.0 * 9.006155967712402
Epoch 2550, val loss: 0.415720671415329
Epoch 2560, training loss: 900.8961181640625 = 0.21236497163772583 + 100.0 * 9.006837844848633
Epoch 2560, val loss: 0.4161704480648041
Epoch 2570, training loss: 900.8439331054688 = 0.21169719099998474 + 100.0 * 9.006322860717773
Epoch 2570, val loss: 0.41697633266448975
Epoch 2580, training loss: 900.8333129882812 = 0.21105362474918365 + 100.0 * 9.00622272491455
Epoch 2580, val loss: 0.417583703994751
Epoch 2590, training loss: 900.9102783203125 = 0.21039162576198578 + 100.0 * 9.006999015808105
Epoch 2590, val loss: 0.4179401397705078
Epoch 2600, training loss: 900.7957763671875 = 0.20973782241344452 + 100.0 * 9.005860328674316
Epoch 2600, val loss: 0.4185795485973358
Epoch 2610, training loss: 900.7855224609375 = 0.20906050503253937 + 100.0 * 9.005764961242676
Epoch 2610, val loss: 0.41863346099853516
Epoch 2620, training loss: 900.8728637695312 = 0.20844247937202454 + 100.0 * 9.006644248962402
Epoch 2620, val loss: 0.4195452928543091
Epoch 2630, training loss: 900.751708984375 = 0.20775790512561798 + 100.0 * 9.005439758300781
Epoch 2630, val loss: 0.41912001371383667
Epoch 2640, training loss: 900.7142333984375 = 0.20712389051914215 + 100.0 * 9.005070686340332
Epoch 2640, val loss: 0.4200059771537781
Epoch 2650, training loss: 900.7025756835938 = 0.20646679401397705 + 100.0 * 9.004961013793945
Epoch 2650, val loss: 0.4204196333885193
Epoch 2660, training loss: 900.8736572265625 = 0.20581135153770447 + 100.0 * 9.006678581237793
Epoch 2660, val loss: 0.4209696650505066
Epoch 2670, training loss: 900.7722778320312 = 0.20518973469734192 + 100.0 * 9.005670547485352
Epoch 2670, val loss: 0.4215224087238312
Epoch 2680, training loss: 900.7230224609375 = 0.20453666150569916 + 100.0 * 9.0051851272583
Epoch 2680, val loss: 0.4219569265842438
Epoch 2690, training loss: 900.7045288085938 = 0.20391210913658142 + 100.0 * 9.005005836486816
Epoch 2690, val loss: 0.4224383234977722
Epoch 2700, training loss: 900.6874389648438 = 0.20326527953147888 + 100.0 * 9.004841804504395
Epoch 2700, val loss: 0.42273879051208496
Epoch 2710, training loss: 900.7808227539062 = 0.2026374638080597 + 100.0 * 9.005782127380371
Epoch 2710, val loss: 0.4227863550186157
Epoch 2720, training loss: 900.7881469726562 = 0.20199963450431824 + 100.0 * 9.005861282348633
Epoch 2720, val loss: 0.4235509932041168
Epoch 2730, training loss: 900.671630859375 = 0.20136065781116486 + 100.0 * 9.0047025680542
Epoch 2730, val loss: 0.42444175481796265
Epoch 2740, training loss: 900.640869140625 = 0.2007347047328949 + 100.0 * 9.004401206970215
Epoch 2740, val loss: 0.42487940192222595
Epoch 2750, training loss: 900.7528686523438 = 0.20011809468269348 + 100.0 * 9.00552749633789
Epoch 2750, val loss: 0.425253301858902
Epoch 2760, training loss: 900.7185668945312 = 0.199464812874794 + 100.0 * 9.0051908493042
Epoch 2760, val loss: 0.4258211851119995
Epoch 2770, training loss: 900.6142578125 = 0.19884438812732697 + 100.0 * 9.004154205322266
Epoch 2770, val loss: 0.42623046040534973
Epoch 2780, training loss: 900.6331176757812 = 0.19822442531585693 + 100.0 * 9.004348754882812
Epoch 2780, val loss: 0.42704498767852783
Epoch 2790, training loss: 900.6240234375 = 0.19760455191135406 + 100.0 * 9.004263877868652
Epoch 2790, val loss: 0.4271955192089081
Epoch 2800, training loss: 900.5757446289062 = 0.19698525965213776 + 100.0 * 9.003787994384766
Epoch 2800, val loss: 0.4279349744319916
Epoch 2810, training loss: 900.5852661132812 = 0.1963643580675125 + 100.0 * 9.003889083862305
Epoch 2810, val loss: 0.4287662208080292
Epoch 2820, training loss: 900.7371215820312 = 0.19575156271457672 + 100.0 * 9.005414009094238
Epoch 2820, val loss: 0.42902982234954834
Epoch 2830, training loss: 900.7735595703125 = 0.19510714709758759 + 100.0 * 9.005784034729004
Epoch 2830, val loss: 0.4295758903026581
Epoch 2840, training loss: 900.6810913085938 = 0.19452767074108124 + 100.0 * 9.004865646362305
Epoch 2840, val loss: 0.4308823049068451
Epoch 2850, training loss: 900.5809936523438 = 0.19389180839061737 + 100.0 * 9.003870964050293
Epoch 2850, val loss: 0.43071961402893066
Epoch 2860, training loss: 900.5413818359375 = 0.19328390061855316 + 100.0 * 9.003480911254883
Epoch 2860, val loss: 0.43166279792785645
Epoch 2870, training loss: 900.6369018554688 = 0.19265779852867126 + 100.0 * 9.00444221496582
Epoch 2870, val loss: 0.431986927986145
Epoch 2880, training loss: 900.5402221679688 = 0.19204169511795044 + 100.0 * 9.0034818649292
Epoch 2880, val loss: 0.4328814446926117
Epoch 2890, training loss: 900.6177978515625 = 0.1914350539445877 + 100.0 * 9.004263877868652
Epoch 2890, val loss: 0.43346428871154785
Epoch 2900, training loss: 900.5292358398438 = 0.19083213806152344 + 100.0 * 9.00338363647461
Epoch 2900, val loss: 0.4342343509197235
Epoch 2910, training loss: 900.604736328125 = 0.19023014605045319 + 100.0 * 9.004144668579102
Epoch 2910, val loss: 0.4347326159477234
Epoch 2920, training loss: 900.533447265625 = 0.1896226853132248 + 100.0 * 9.003437995910645
Epoch 2920, val loss: 0.4350171685218811
Epoch 2930, training loss: 900.4852294921875 = 0.18902473151683807 + 100.0 * 9.002962112426758
Epoch 2930, val loss: 0.43550023436546326
Epoch 2940, training loss: 900.5337524414062 = 0.1884087473154068 + 100.0 * 9.003453254699707
Epoch 2940, val loss: 0.43608731031417847
Epoch 2950, training loss: 900.5092163085938 = 0.18780319392681122 + 100.0 * 9.003213882446289
Epoch 2950, val loss: 0.43694940209388733
Epoch 2960, training loss: 900.5269775390625 = 0.18720509111881256 + 100.0 * 9.003397941589355
Epoch 2960, val loss: 0.4377844035625458
Epoch 2970, training loss: 900.474853515625 = 0.186608225107193 + 100.0 * 9.002882957458496
Epoch 2970, val loss: 0.43795958161354065
Epoch 2980, training loss: 900.563720703125 = 0.18603621423244476 + 100.0 * 9.003776550292969
Epoch 2980, val loss: 0.4387580156326294
Epoch 2990, training loss: 900.4606323242188 = 0.1854291558265686 + 100.0 * 9.002752304077148
Epoch 2990, val loss: 0.4400191307067871
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8508
Overall ASR: 0.7779
Flip ASR: 0.7233/1554 nodes
The final ASR:0.75879, 0.01907, Accuracy:0.85118, 0.00415
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97552])
remove edge: torch.Size([2, 79822])
updated graph: torch.Size([2, 88726])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7424
Flip ASR: 0.6808/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72245, 0.01749, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1036.992919921875 = 1.093984603881836 + 100.0 * 10.358988761901855
Epoch 0, val loss: 1.0930465459823608
Epoch 10, training loss: 1036.578125 = 1.0843846797943115 + 100.0 * 10.354937553405762
Epoch 10, val loss: 1.083077311515808
Epoch 20, training loss: 1030.4068603515625 = 1.0714415311813354 + 100.0 * 10.293354988098145
Epoch 20, val loss: 1.0701324939727783
Epoch 30, training loss: 972.8453369140625 = 1.0596654415130615 + 100.0 * 9.717856407165527
Epoch 30, val loss: 1.0583419799804688
Epoch 40, training loss: 954.714111328125 = 1.0471299886703491 + 100.0 * 9.536669731140137
Epoch 40, val loss: 1.0454773902893066
Epoch 50, training loss: 941.6119384765625 = 1.0330573320388794 + 100.0 * 9.40578842163086
Epoch 50, val loss: 1.0313963890075684
Epoch 60, training loss: 936.2225952148438 = 1.0207439661026 + 100.0 * 9.352018356323242
Epoch 60, val loss: 1.0192034244537354
Epoch 70, training loss: 931.1455078125 = 1.0098743438720703 + 100.0 * 9.301356315612793
Epoch 70, val loss: 1.0084141492843628
Epoch 80, training loss: 926.6445922851562 = 1.0022937059402466 + 100.0 * 9.256422996520996
Epoch 80, val loss: 1.00095534324646
Epoch 90, training loss: 923.4949951171875 = 0.9947779178619385 + 100.0 * 9.22500228881836
Epoch 90, val loss: 0.9931731820106506
Epoch 100, training loss: 919.8433227539062 = 0.9848513007164001 + 100.0 * 9.188584327697754
Epoch 100, val loss: 0.9833424687385559
Epoch 110, training loss: 917.994873046875 = 0.9747470021247864 + 100.0 * 9.170201301574707
Epoch 110, val loss: 0.973320722579956
Epoch 120, training loss: 917.0205688476562 = 0.963463544845581 + 100.0 * 9.160571098327637
Epoch 120, val loss: 0.9620280265808105
Epoch 130, training loss: 915.9185180664062 = 0.9513055086135864 + 100.0 * 9.149672508239746
Epoch 130, val loss: 0.9500998258590698
Epoch 140, training loss: 914.836181640625 = 0.9403146505355835 + 100.0 * 9.138958930969238
Epoch 140, val loss: 0.9394779205322266
Epoch 150, training loss: 913.85009765625 = 0.9306386709213257 + 100.0 * 9.129194259643555
Epoch 150, val loss: 0.9301981329917908
Epoch 160, training loss: 913.2792358398438 = 0.9207610487937927 + 100.0 * 9.123584747314453
Epoch 160, val loss: 0.9204475283622742
Epoch 170, training loss: 912.5020141601562 = 0.9093366861343384 + 100.0 * 9.115926742553711
Epoch 170, val loss: 0.9091745615005493
Epoch 180, training loss: 911.8721923828125 = 0.8970448970794678 + 100.0 * 9.10975170135498
Epoch 180, val loss: 0.8973991274833679
Epoch 190, training loss: 911.2431640625 = 0.8851383924484253 + 100.0 * 9.103580474853516
Epoch 190, val loss: 0.8860110640525818
Epoch 200, training loss: 910.5830688476562 = 0.8739094734191895 + 100.0 * 9.097091674804688
Epoch 200, val loss: 0.8754158020019531
Epoch 210, training loss: 909.9638671875 = 0.8626890778541565 + 100.0 * 9.091012001037598
Epoch 210, val loss: 0.8646655082702637
Epoch 220, training loss: 909.3936767578125 = 0.8512450456619263 + 100.0 * 9.085424423217773
Epoch 220, val loss: 0.853771448135376
Epoch 230, training loss: 908.9135131835938 = 0.8395208716392517 + 100.0 * 9.080739974975586
Epoch 230, val loss: 0.8425790667533875
Epoch 240, training loss: 908.5126953125 = 0.8265666961669922 + 100.0 * 9.076861381530762
Epoch 240, val loss: 0.8301759362220764
Epoch 250, training loss: 908.2390747070312 = 0.8131331205368042 + 100.0 * 9.074259757995605
Epoch 250, val loss: 0.8173704743385315
Epoch 260, training loss: 907.753173828125 = 0.7996043562889099 + 100.0 * 9.069535255432129
Epoch 260, val loss: 0.8044925332069397
Epoch 270, training loss: 907.4708251953125 = 0.7857568860054016 + 100.0 * 9.066850662231445
Epoch 270, val loss: 0.7914457321166992
Epoch 280, training loss: 907.3911743164062 = 0.7712034583091736 + 100.0 * 9.066200256347656
Epoch 280, val loss: 0.7774532437324524
Epoch 290, training loss: 907.0245361328125 = 0.7561103105545044 + 100.0 * 9.062684059143066
Epoch 290, val loss: 0.7630910873413086
Epoch 300, training loss: 906.8165893554688 = 0.7413724660873413 + 100.0 * 9.060751914978027
Epoch 300, val loss: 0.7491596341133118
Epoch 310, training loss: 906.6532592773438 = 0.7267241477966309 + 100.0 * 9.05926513671875
Epoch 310, val loss: 0.735314667224884
Epoch 320, training loss: 906.609619140625 = 0.7115833759307861 + 100.0 * 9.058979988098145
Epoch 320, val loss: 0.7209377884864807
Epoch 330, training loss: 906.4139404296875 = 0.6963769197463989 + 100.0 * 9.057175636291504
Epoch 330, val loss: 0.7065834403038025
Epoch 340, training loss: 906.2014770507812 = 0.6818873286247253 + 100.0 * 9.055195808410645
Epoch 340, val loss: 0.6930054426193237
Epoch 350, training loss: 906.0551147460938 = 0.6677125692367554 + 100.0 * 9.053874015808105
Epoch 350, val loss: 0.6796972155570984
Epoch 360, training loss: 906.2520141601562 = 0.6534687280654907 + 100.0 * 9.055985450744629
Epoch 360, val loss: 0.6661659479141235
Epoch 370, training loss: 905.9326171875 = 0.6390269994735718 + 100.0 * 9.052935600280762
Epoch 370, val loss: 0.652851402759552
Epoch 380, training loss: 905.7432250976562 = 0.6255814433097839 + 100.0 * 9.051176071166992
Epoch 380, val loss: 0.6404146552085876
Epoch 390, training loss: 905.62353515625 = 0.6126867532730103 + 100.0 * 9.050108909606934
Epoch 390, val loss: 0.6284668445587158
Epoch 400, training loss: 906.2000732421875 = 0.5999490022659302 + 100.0 * 9.056000709533691
Epoch 400, val loss: 0.616570234298706
Epoch 410, training loss: 905.6171875 = 0.5869173407554626 + 100.0 * 9.050302505493164
Epoch 410, val loss: 0.6046812534332275
Epoch 420, training loss: 905.3442993164062 = 0.5751646757125854 + 100.0 * 9.047691345214844
Epoch 420, val loss: 0.5940676927566528
Epoch 430, training loss: 905.2698974609375 = 0.5642390251159668 + 100.0 * 9.047057151794434
Epoch 430, val loss: 0.5840636491775513
Epoch 440, training loss: 905.1747436523438 = 0.5536789298057556 + 100.0 * 9.046211242675781
Epoch 440, val loss: 0.5744941234588623
Epoch 450, training loss: 905.0935668945312 = 0.543516218662262 + 100.0 * 9.045500755310059
Epoch 450, val loss: 0.5653162002563477
Epoch 460, training loss: 905.3607177734375 = 0.5337139368057251 + 100.0 * 9.048270225524902
Epoch 460, val loss: 0.5567091703414917
Epoch 470, training loss: 905.1497802734375 = 0.5237923264503479 + 100.0 * 9.046259880065918
Epoch 470, val loss: 0.5474929213523865
Epoch 480, training loss: 904.886962890625 = 0.5146449208259583 + 100.0 * 9.043723106384277
Epoch 480, val loss: 0.5393930077552795
Epoch 490, training loss: 904.8185424804688 = 0.5064269304275513 + 100.0 * 9.043121337890625
Epoch 490, val loss: 0.5321053266525269
Epoch 500, training loss: 904.7457275390625 = 0.4986647665500641 + 100.0 * 9.042470932006836
Epoch 500, val loss: 0.5252543687820435
Epoch 510, training loss: 905.2417602539062 = 0.4911455810070038 + 100.0 * 9.047506332397461
Epoch 510, val loss: 0.5185627341270447
Epoch 520, training loss: 904.6399536132812 = 0.48347872495651245 + 100.0 * 9.04156494140625
Epoch 520, val loss: 0.5118682980537415
Epoch 530, training loss: 904.5665893554688 = 0.47673457860946655 + 100.0 * 9.040898323059082
Epoch 530, val loss: 0.5060574412345886
Epoch 540, training loss: 904.4979248046875 = 0.4704860746860504 + 100.0 * 9.040274620056152
Epoch 540, val loss: 0.5006842613220215
Epoch 550, training loss: 904.4373168945312 = 0.46449676156044006 + 100.0 * 9.039728164672852
Epoch 550, val loss: 0.49556928873062134
Epoch 560, training loss: 904.6554565429688 = 0.458737313747406 + 100.0 * 9.041967391967773
Epoch 560, val loss: 0.49060338735580444
Epoch 570, training loss: 904.5155639648438 = 0.4528070092201233 + 100.0 * 9.040627479553223
Epoch 570, val loss: 0.485709011554718
Epoch 580, training loss: 904.364990234375 = 0.44753551483154297 + 100.0 * 9.039175033569336
Epoch 580, val loss: 0.48131823539733887
Epoch 590, training loss: 904.3294677734375 = 0.44268494844436646 + 100.0 * 9.038867950439453
Epoch 590, val loss: 0.47733423113822937
Epoch 600, training loss: 904.1915893554688 = 0.43794894218444824 + 100.0 * 9.03753662109375
Epoch 600, val loss: 0.4733640253543854
Epoch 610, training loss: 904.1702880859375 = 0.4335196912288666 + 100.0 * 9.037367820739746
Epoch 610, val loss: 0.4697645604610443
Epoch 620, training loss: 904.4140625 = 0.4291754364967346 + 100.0 * 9.039848327636719
Epoch 620, val loss: 0.46637415885925293
Epoch 630, training loss: 904.1002807617188 = 0.424903005361557 + 100.0 * 9.03675365447998
Epoch 630, val loss: 0.46279412508010864
Epoch 640, training loss: 903.9952392578125 = 0.42107343673706055 + 100.0 * 9.035741806030273
Epoch 640, val loss: 0.4598059058189392
Epoch 650, training loss: 903.9647827148438 = 0.41746172308921814 + 100.0 * 9.035472869873047
Epoch 650, val loss: 0.4569973647594452
Epoch 660, training loss: 903.9385986328125 = 0.41400063037872314 + 100.0 * 9.035245895385742
Epoch 660, val loss: 0.45438864827156067
Epoch 670, training loss: 903.9082641601562 = 0.4104272425174713 + 100.0 * 9.034978866577148
Epoch 670, val loss: 0.4515663981437683
Epoch 680, training loss: 903.8953857421875 = 0.4070349335670471 + 100.0 * 9.034883499145508
Epoch 680, val loss: 0.44900012016296387
Epoch 690, training loss: 903.8704833984375 = 0.403991162776947 + 100.0 * 9.03466510772705
Epoch 690, val loss: 0.4466877579689026
Epoch 700, training loss: 903.8142700195312 = 0.40102216601371765 + 100.0 * 9.034132957458496
Epoch 700, val loss: 0.44443634152412415
Epoch 710, training loss: 903.7417602539062 = 0.3982085585594177 + 100.0 * 9.033435821533203
Epoch 710, val loss: 0.4424512982368469
Epoch 720, training loss: 903.734130859375 = 0.3955107629299164 + 100.0 * 9.03338623046875
Epoch 720, val loss: 0.44052648544311523
Epoch 730, training loss: 903.7037353515625 = 0.39270636439323425 + 100.0 * 9.033110618591309
Epoch 730, val loss: 0.4384593367576599
Epoch 740, training loss: 903.7080688476562 = 0.3900785446166992 + 100.0 * 9.033180236816406
Epoch 740, val loss: 0.4365573227405548
Epoch 750, training loss: 903.5792846679688 = 0.3876902163028717 + 100.0 * 9.031915664672852
Epoch 750, val loss: 0.4348873496055603
Epoch 760, training loss: 903.5534057617188 = 0.3853796720504761 + 100.0 * 9.0316801071167
Epoch 760, val loss: 0.4331928789615631
Epoch 770, training loss: 903.7282104492188 = 0.38309040665626526 + 100.0 * 9.033451080322266
Epoch 770, val loss: 0.43141505122184753
Epoch 780, training loss: 903.6668090820312 = 0.3807477056980133 + 100.0 * 9.03286075592041
Epoch 780, val loss: 0.430136114358902
Epoch 790, training loss: 903.4691162109375 = 0.378539115190506 + 100.0 * 9.030905723571777
Epoch 790, val loss: 0.4287920892238617
Epoch 800, training loss: 903.4424438476562 = 0.3764703869819641 + 100.0 * 9.030659675598145
Epoch 800, val loss: 0.427169531583786
Epoch 810, training loss: 903.4374389648438 = 0.3744893968105316 + 100.0 * 9.03062915802002
Epoch 810, val loss: 0.42613714933395386
Epoch 820, training loss: 903.3861694335938 = 0.37244608998298645 + 100.0 * 9.030137062072754
Epoch 820, val loss: 0.4247834086418152
Epoch 830, training loss: 903.3277587890625 = 0.3705068528652191 + 100.0 * 9.029572486877441
Epoch 830, val loss: 0.4235188066959381
Epoch 840, training loss: 903.28759765625 = 0.36868420243263245 + 100.0 * 9.029189109802246
Epoch 840, val loss: 0.4223378598690033
Epoch 850, training loss: 903.264892578125 = 0.3668883740901947 + 100.0 * 9.028980255126953
Epoch 850, val loss: 0.42118480801582336
Epoch 860, training loss: 903.4873046875 = 0.36503341794013977 + 100.0 * 9.031222343444824
Epoch 860, val loss: 0.42001062631607056
Epoch 870, training loss: 903.2296752929688 = 0.36320844292640686 + 100.0 * 9.028664588928223
Epoch 870, val loss: 0.4190168082714081
Epoch 880, training loss: 903.2142944335938 = 0.36154836416244507 + 100.0 * 9.02852725982666
Epoch 880, val loss: 0.41811758279800415
Epoch 890, training loss: 903.2315063476562 = 0.3598986268043518 + 100.0 * 9.028716087341309
Epoch 890, val loss: 0.41712483763694763
Epoch 900, training loss: 903.2109375 = 0.35826465487480164 + 100.0 * 9.028526306152344
Epoch 900, val loss: 0.4161050319671631
Epoch 910, training loss: 903.0996704101562 = 0.3566773533821106 + 100.0 * 9.027429580688477
Epoch 910, val loss: 0.41536617279052734
Epoch 920, training loss: 903.1129150390625 = 0.35513395071029663 + 100.0 * 9.02757740020752
Epoch 920, val loss: 0.4146292209625244
Epoch 930, training loss: 903.1322021484375 = 0.3535784184932709 + 100.0 * 9.027786254882812
Epoch 930, val loss: 0.4137517511844635
Epoch 940, training loss: 903.0883178710938 = 0.35206711292266846 + 100.0 * 9.027362823486328
Epoch 940, val loss: 0.41281476616859436
Epoch 950, training loss: 903.0242919921875 = 0.3505813181400299 + 100.0 * 9.026737213134766
Epoch 950, val loss: 0.41208869218826294
Epoch 960, training loss: 903.0755615234375 = 0.3491422235965729 + 100.0 * 9.027264595031738
Epoch 960, val loss: 0.41146862506866455
Epoch 970, training loss: 902.9393310546875 = 0.34764814376831055 + 100.0 * 9.025917053222656
Epoch 970, val loss: 0.41048291325569153
Epoch 980, training loss: 902.9307861328125 = 0.3462563455104828 + 100.0 * 9.025845527648926
Epoch 980, val loss: 0.40986689925193787
Epoch 990, training loss: 903.1004638671875 = 0.3448743224143982 + 100.0 * 9.027556419372559
Epoch 990, val loss: 0.40905410051345825
Epoch 1000, training loss: 902.92822265625 = 0.34348198771476746 + 100.0 * 9.025847434997559
Epoch 1000, val loss: 0.4084651470184326
Epoch 1010, training loss: 902.8496704101562 = 0.3421628177165985 + 100.0 * 9.02507495880127
Epoch 1010, val loss: 0.40786537528038025
Epoch 1020, training loss: 902.9473876953125 = 0.3408505320549011 + 100.0 * 9.026065826416016
Epoch 1020, val loss: 0.4070398211479187
Epoch 1030, training loss: 902.9666748046875 = 0.33949145674705505 + 100.0 * 9.02627182006836
Epoch 1030, val loss: 0.40636903047561646
Epoch 1040, training loss: 902.8274536132812 = 0.33818143606185913 + 100.0 * 9.024892807006836
Epoch 1040, val loss: 0.4061155915260315
Epoch 1050, training loss: 902.7362670898438 = 0.3369255065917969 + 100.0 * 9.023993492126465
Epoch 1050, val loss: 0.4054396450519562
Epoch 1060, training loss: 902.7708740234375 = 0.33569520711898804 + 100.0 * 9.024352073669434
Epoch 1060, val loss: 0.40480703115463257
Epoch 1070, training loss: 902.7646484375 = 0.3344097137451172 + 100.0 * 9.02430248260498
Epoch 1070, val loss: 0.40442541241645813
Epoch 1080, training loss: 902.7088012695312 = 0.333193302154541 + 100.0 * 9.02375602722168
Epoch 1080, val loss: 0.40394261479377747
Epoch 1090, training loss: 902.8107299804688 = 0.3319883346557617 + 100.0 * 9.024787902832031
Epoch 1090, val loss: 0.40333348512649536
Epoch 1100, training loss: 902.6787719726562 = 0.3307596743106842 + 100.0 * 9.023480415344238
Epoch 1100, val loss: 0.40311604738235474
Epoch 1110, training loss: 902.5956420898438 = 0.32959234714508057 + 100.0 * 9.022660255432129
Epoch 1110, val loss: 0.40262165665626526
Epoch 1120, training loss: 902.6923217773438 = 0.32843640446662903 + 100.0 * 9.023638725280762
Epoch 1120, val loss: 0.40235161781311035
Epoch 1130, training loss: 902.552001953125 = 0.3272712230682373 + 100.0 * 9.022247314453125
Epoch 1130, val loss: 0.40167245268821716
Epoch 1140, training loss: 902.5618286132812 = 0.3261362612247467 + 100.0 * 9.022356986999512
Epoch 1140, val loss: 0.4012884497642517
Epoch 1150, training loss: 902.6279296875 = 0.3249909579753876 + 100.0 * 9.023029327392578
Epoch 1150, val loss: 0.4010050296783447
Epoch 1160, training loss: 902.589599609375 = 0.32383355498313904 + 100.0 * 9.02265739440918
Epoch 1160, val loss: 0.40055564045906067
Epoch 1170, training loss: 902.5696411132812 = 0.3226989209651947 + 100.0 * 9.022469520568848
Epoch 1170, val loss: 0.4000086784362793
Epoch 1180, training loss: 902.577880859375 = 0.3215975761413574 + 100.0 * 9.022562980651855
Epoch 1180, val loss: 0.39952415227890015
Epoch 1190, training loss: 902.457275390625 = 0.32049503922462463 + 100.0 * 9.021368026733398
Epoch 1190, val loss: 0.3993265926837921
Epoch 1200, training loss: 902.4156494140625 = 0.3194272816181183 + 100.0 * 9.020962715148926
Epoch 1200, val loss: 0.39903947710990906
Epoch 1210, training loss: 902.3825073242188 = 0.31837302446365356 + 100.0 * 9.020641326904297
Epoch 1210, val loss: 0.39867573976516724
Epoch 1220, training loss: 902.5338745117188 = 0.31730031967163086 + 100.0 * 9.022165298461914
Epoch 1220, val loss: 0.39814457297325134
Epoch 1230, training loss: 902.414306640625 = 0.316195011138916 + 100.0 * 9.020980834960938
Epoch 1230, val loss: 0.3980252146720886
Epoch 1240, training loss: 902.4064331054688 = 0.3151364326477051 + 100.0 * 9.020913124084473
Epoch 1240, val loss: 0.3975856602191925
Epoch 1250, training loss: 902.35791015625 = 0.31409624218940735 + 100.0 * 9.020438194274902
Epoch 1250, val loss: 0.397564560174942
Epoch 1260, training loss: 902.3850708007812 = 0.3130621910095215 + 100.0 * 9.020720481872559
Epoch 1260, val loss: 0.3973509967327118
Epoch 1270, training loss: 902.4215698242188 = 0.3120427429676056 + 100.0 * 9.021095275878906
Epoch 1270, val loss: 0.3971104621887207
Epoch 1280, training loss: 902.312744140625 = 0.3109853267669678 + 100.0 * 9.020017623901367
Epoch 1280, val loss: 0.3966887593269348
Epoch 1290, training loss: 902.2501831054688 = 0.3099900782108307 + 100.0 * 9.019401550292969
Epoch 1290, val loss: 0.39636969566345215
Epoch 1300, training loss: 902.3021850585938 = 0.3089958131313324 + 100.0 * 9.01993179321289
Epoch 1300, val loss: 0.3958994746208191
Epoch 1310, training loss: 902.2994384765625 = 0.3079908788204193 + 100.0 * 9.019914627075195
Epoch 1310, val loss: 0.39597707986831665
Epoch 1320, training loss: 902.2811889648438 = 0.3069884777069092 + 100.0 * 9.019742012023926
Epoch 1320, val loss: 0.39598673582077026
Epoch 1330, training loss: 902.208251953125 = 0.3059964179992676 + 100.0 * 9.019022941589355
Epoch 1330, val loss: 0.3955412209033966
Epoch 1340, training loss: 902.2033081054688 = 0.30502310395240784 + 100.0 * 9.018982887268066
Epoch 1340, val loss: 0.3955630362033844
Epoch 1350, training loss: 902.2006225585938 = 0.3040578067302704 + 100.0 * 9.018965721130371
Epoch 1350, val loss: 0.3951641619205475
Epoch 1360, training loss: 902.1687622070312 = 0.30309465527534485 + 100.0 * 9.018656730651855
Epoch 1360, val loss: 0.3948754370212555
Epoch 1370, training loss: 902.2289428710938 = 0.30211830139160156 + 100.0 * 9.019268035888672
Epoch 1370, val loss: 0.3947436213493347
Epoch 1380, training loss: 902.2198486328125 = 0.3011627793312073 + 100.0 * 9.019186973571777
Epoch 1380, val loss: 0.3946915566921234
Epoch 1390, training loss: 902.122314453125 = 0.3002162277698517 + 100.0 * 9.018220901489258
Epoch 1390, val loss: 0.39437976479530334
Epoch 1400, training loss: 902.0774536132812 = 0.29927220940589905 + 100.0 * 9.017782211303711
Epoch 1400, val loss: 0.39413517713546753
Epoch 1410, training loss: 902.1165161132812 = 0.298334538936615 + 100.0 * 9.018181800842285
Epoch 1410, val loss: 0.3939267694950104
Epoch 1420, training loss: 902.081787109375 = 0.29738783836364746 + 100.0 * 9.017844200134277
Epoch 1420, val loss: 0.3938976526260376
Epoch 1430, training loss: 902.0516967773438 = 0.2964476943016052 + 100.0 * 9.017552375793457
Epoch 1430, val loss: 0.3936927616596222
Epoch 1440, training loss: 902.0011596679688 = 0.29553478956222534 + 100.0 * 9.017056465148926
Epoch 1440, val loss: 0.3935801684856415
Epoch 1450, training loss: 902.1874389648438 = 0.2946167588233948 + 100.0 * 9.018928527832031
Epoch 1450, val loss: 0.39329835772514343
Epoch 1460, training loss: 901.9816284179688 = 0.2936708331108093 + 100.0 * 9.016879081726074
Epoch 1460, val loss: 0.3935801684856415
Epoch 1470, training loss: 902.0278930664062 = 0.29277288913726807 + 100.0 * 9.017351150512695
Epoch 1470, val loss: 0.3935074508190155
Epoch 1480, training loss: 901.942138671875 = 0.291860967874527 + 100.0 * 9.016502380371094
Epoch 1480, val loss: 0.39335769414901733
Epoch 1490, training loss: 901.934326171875 = 0.2909841537475586 + 100.0 * 9.016433715820312
Epoch 1490, val loss: 0.3930453360080719
Epoch 1500, training loss: 902.0776977539062 = 0.29008010029792786 + 100.0 * 9.017875671386719
Epoch 1500, val loss: 0.3932259976863861
Epoch 1510, training loss: 901.9457397460938 = 0.2891850471496582 + 100.0 * 9.016565322875977
Epoch 1510, val loss: 0.39268121123313904
Epoch 1520, training loss: 902.0159301757812 = 0.28830820322036743 + 100.0 * 9.0172758102417
Epoch 1520, val loss: 0.3925229609012604
Epoch 1530, training loss: 901.8633422851562 = 0.28740909695625305 + 100.0 * 9.015759468078613
Epoch 1530, val loss: 0.3930361568927765
Epoch 1540, training loss: 901.827392578125 = 0.28654801845550537 + 100.0 * 9.015408515930176
Epoch 1540, val loss: 0.3925042748451233
Epoch 1550, training loss: 901.95751953125 = 0.28566819429397583 + 100.0 * 9.016718864440918
Epoch 1550, val loss: 0.3925953209400177
Epoch 1560, training loss: 901.8052978515625 = 0.28478872776031494 + 100.0 * 9.015205383300781
Epoch 1560, val loss: 0.3926681876182556
Epoch 1570, training loss: 901.87255859375 = 0.283918559551239 + 100.0 * 9.015886306762695
Epoch 1570, val loss: 0.3925078511238098
Epoch 1580, training loss: 901.8673706054688 = 0.2830493450164795 + 100.0 * 9.015843391418457
Epoch 1580, val loss: 0.39269688725471497
Epoch 1590, training loss: 901.7373046875 = 0.2821955978870392 + 100.0 * 9.014551162719727
Epoch 1590, val loss: 0.3925250470638275
Epoch 1600, training loss: 901.7428588867188 = 0.28135451674461365 + 100.0 * 9.014615058898926
Epoch 1600, val loss: 0.3924630582332611
Epoch 1610, training loss: 902.06201171875 = 0.28050675988197327 + 100.0 * 9.017814636230469
Epoch 1610, val loss: 0.3925391137599945
Epoch 1620, training loss: 901.7675170898438 = 0.2796326279640198 + 100.0 * 9.014878273010254
Epoch 1620, val loss: 0.3925955891609192
Epoch 1630, training loss: 901.6598510742188 = 0.2788064181804657 + 100.0 * 9.013810157775879
Epoch 1630, val loss: 0.39247381687164307
Epoch 1640, training loss: 901.6510620117188 = 0.2779783606529236 + 100.0 * 9.013731002807617
Epoch 1640, val loss: 0.3926412761211395
Epoch 1650, training loss: 901.921875 = 0.27715471386909485 + 100.0 * 9.016447067260742
Epoch 1650, val loss: 0.3928031623363495
Epoch 1660, training loss: 901.7286376953125 = 0.27627578377723694 + 100.0 * 9.01452350616455
Epoch 1660, val loss: 0.39235901832580566
Epoch 1670, training loss: 901.71484375 = 0.2754688262939453 + 100.0 * 9.01439380645752
Epoch 1670, val loss: 0.3926657736301422
Epoch 1680, training loss: 901.7278442382812 = 0.2746318280696869 + 100.0 * 9.014532089233398
Epoch 1680, val loss: 0.39251160621643066
Epoch 1690, training loss: 901.6314697265625 = 0.2738265097141266 + 100.0 * 9.01357650756836
Epoch 1690, val loss: 0.3924364745616913
Epoch 1700, training loss: 901.5630493164062 = 0.2730134129524231 + 100.0 * 9.012900352478027
Epoch 1700, val loss: 0.39263010025024414
Epoch 1710, training loss: 901.7044677734375 = 0.2722109258174896 + 100.0 * 9.014322280883789
Epoch 1710, val loss: 0.3923691511154175
Epoch 1720, training loss: 901.5991821289062 = 0.27136898040771484 + 100.0 * 9.013278007507324
Epoch 1720, val loss: 0.392428457736969
Epoch 1730, training loss: 901.6004638671875 = 0.27056875824928284 + 100.0 * 9.013298988342285
Epoch 1730, val loss: 0.3925471603870392
Epoch 1740, training loss: 901.552734375 = 0.2697601616382599 + 100.0 * 9.012829780578613
Epoch 1740, val loss: 0.39287278056144714
Epoch 1750, training loss: 901.5634155273438 = 0.2689680755138397 + 100.0 * 9.012944221496582
Epoch 1750, val loss: 0.3928242623806
Epoch 1760, training loss: 901.6054077148438 = 0.26817208528518677 + 100.0 * 9.013372421264648
Epoch 1760, val loss: 0.3926681578159332
Epoch 1770, training loss: 901.5968627929688 = 0.2673562169075012 + 100.0 * 9.01329517364502
Epoch 1770, val loss: 0.3928826153278351
Epoch 1780, training loss: 901.5440673828125 = 0.26656001806259155 + 100.0 * 9.012775421142578
Epoch 1780, val loss: 0.39336803555488586
Epoch 1790, training loss: 901.4662475585938 = 0.26576849818229675 + 100.0 * 9.012004852294922
Epoch 1790, val loss: 0.39311620593070984
Epoch 1800, training loss: 901.4741821289062 = 0.2649833858013153 + 100.0 * 9.012092590332031
Epoch 1800, val loss: 0.39311328530311584
Epoch 1810, training loss: 901.6691284179688 = 0.264180988073349 + 100.0 * 9.014049530029297
Epoch 1810, val loss: 0.3931155502796173
Epoch 1820, training loss: 901.4309692382812 = 0.26339173316955566 + 100.0 * 9.011675834655762
Epoch 1820, val loss: 0.393387109041214
Epoch 1830, training loss: 901.4319458007812 = 0.2626146972179413 + 100.0 * 9.011693000793457
Epoch 1830, val loss: 0.39340803027153015
Epoch 1840, training loss: 901.451904296875 = 0.26182791590690613 + 100.0 * 9.011900901794434
Epoch 1840, val loss: 0.3935594856739044
Epoch 1850, training loss: 901.3732299804688 = 0.2610487937927246 + 100.0 * 9.01112174987793
Epoch 1850, val loss: 0.3937227427959442
Epoch 1860, training loss: 901.5950927734375 = 0.2602737545967102 + 100.0 * 9.013348579406738
Epoch 1860, val loss: 0.39399048686027527
Epoch 1870, training loss: 901.47509765625 = 0.25948017835617065 + 100.0 * 9.01215648651123
Epoch 1870, val loss: 0.393601655960083
Epoch 1880, training loss: 901.4374389648438 = 0.2587204575538635 + 100.0 * 9.011787414550781
Epoch 1880, val loss: 0.39393264055252075
Epoch 1890, training loss: 901.3375244140625 = 0.2579479515552521 + 100.0 * 9.010795593261719
Epoch 1890, val loss: 0.39423540234565735
Epoch 1900, training loss: 901.297119140625 = 0.2571830153465271 + 100.0 * 9.010398864746094
Epoch 1900, val loss: 0.3942487835884094
Epoch 1910, training loss: 901.40283203125 = 0.25642046332359314 + 100.0 * 9.01146411895752
Epoch 1910, val loss: 0.3941909968852997
Epoch 1920, training loss: 901.3561401367188 = 0.25563445687294006 + 100.0 * 9.011005401611328
Epoch 1920, val loss: 0.39483824372291565
Epoch 1930, training loss: 901.4174194335938 = 0.2548667788505554 + 100.0 * 9.011625289916992
Epoch 1930, val loss: 0.39480897784233093
Epoch 1940, training loss: 901.311767578125 = 0.2541294991970062 + 100.0 * 9.010576248168945
Epoch 1940, val loss: 0.3950614035129547
Epoch 1950, training loss: 901.2708129882812 = 0.25337648391723633 + 100.0 * 9.010174751281738
Epoch 1950, val loss: 0.39515992999076843
Epoch 1960, training loss: 901.3826293945312 = 0.2526269555091858 + 100.0 * 9.011300086975098
Epoch 1960, val loss: 0.395645409822464
Epoch 1970, training loss: 901.2277221679688 = 0.2518599033355713 + 100.0 * 9.009758949279785
Epoch 1970, val loss: 0.3954364061355591
Epoch 1980, training loss: 901.209716796875 = 0.25111815333366394 + 100.0 * 9.009586334228516
Epoch 1980, val loss: 0.3954737186431885
Epoch 1990, training loss: 901.2172241210938 = 0.25036972761154175 + 100.0 * 9.009668350219727
Epoch 1990, val loss: 0.395943284034729
Epoch 2000, training loss: 901.48388671875 = 0.24963168799877167 + 100.0 * 9.01234245300293
Epoch 2000, val loss: 0.39635327458381653
Epoch 2010, training loss: 901.3258056640625 = 0.2488795667886734 + 100.0 * 9.01076889038086
Epoch 2010, val loss: 0.39603742957115173
Epoch 2020, training loss: 901.2304077148438 = 0.248137429356575 + 100.0 * 9.009822845458984
Epoch 2020, val loss: 0.39657318592071533
Epoch 2030, training loss: 901.1748657226562 = 0.2474171370267868 + 100.0 * 9.00927448272705
Epoch 2030, val loss: 0.3964169919490814
Epoch 2040, training loss: 901.1966552734375 = 0.24668379127979279 + 100.0 * 9.009499549865723
Epoch 2040, val loss: 0.39672473073005676
Epoch 2050, training loss: 901.1709594726562 = 0.24594883620738983 + 100.0 * 9.009249687194824
Epoch 2050, val loss: 0.3970643877983093
Epoch 2060, training loss: 901.1983642578125 = 0.24521814286708832 + 100.0 * 9.009531021118164
Epoch 2060, val loss: 0.3971598446369171
Epoch 2070, training loss: 901.2194213867188 = 0.2445051670074463 + 100.0 * 9.009749412536621
Epoch 2070, val loss: 0.3970743715763092
Epoch 2080, training loss: 901.2389526367188 = 0.2437734603881836 + 100.0 * 9.0099515914917
Epoch 2080, val loss: 0.39734235405921936
Epoch 2090, training loss: 901.0997314453125 = 0.2430386245250702 + 100.0 * 9.008566856384277
Epoch 2090, val loss: 0.39801445603370667
Epoch 2100, training loss: 901.0779418945312 = 0.24232661724090576 + 100.0 * 9.008356094360352
Epoch 2100, val loss: 0.39795491099357605
Epoch 2110, training loss: 901.2787475585938 = 0.24161434173583984 + 100.0 * 9.010371208190918
Epoch 2110, val loss: 0.39800575375556946
Epoch 2120, training loss: 901.081787109375 = 0.24088236689567566 + 100.0 * 9.008408546447754
Epoch 2120, val loss: 0.3988037109375
Epoch 2130, training loss: 901.045166015625 = 0.2401716709136963 + 100.0 * 9.008049964904785
Epoch 2130, val loss: 0.3987587094306946
Epoch 2140, training loss: 901.0277709960938 = 0.23946011066436768 + 100.0 * 9.007883071899414
Epoch 2140, val loss: 0.3991512060165405
Epoch 2150, training loss: 901.2740478515625 = 0.23874959349632263 + 100.0 * 9.010353088378906
Epoch 2150, val loss: 0.3991083800792694
Epoch 2160, training loss: 901.2142944335938 = 0.23804046213626862 + 100.0 * 9.00976276397705
Epoch 2160, val loss: 0.3998800814151764
Epoch 2170, training loss: 901.0798950195312 = 0.23732684552669525 + 100.0 * 9.00842571258545
Epoch 2170, val loss: 0.40014588832855225
Epoch 2180, training loss: 900.9660034179688 = 0.23663067817687988 + 100.0 * 9.007293701171875
Epoch 2180, val loss: 0.40030884742736816
Epoch 2190, training loss: 900.969970703125 = 0.23592892289161682 + 100.0 * 9.007340431213379
Epoch 2190, val loss: 0.40056338906288147
Epoch 2200, training loss: 901.1279907226562 = 0.23523569107055664 + 100.0 * 9.008927345275879
Epoch 2200, val loss: 0.4005841314792633
Epoch 2210, training loss: 901.0557250976562 = 0.2345147281885147 + 100.0 * 9.008212089538574
Epoch 2210, val loss: 0.4015232026576996
Epoch 2220, training loss: 900.9592895507812 = 0.23382391035556793 + 100.0 * 9.007254600524902
Epoch 2220, val loss: 0.4015295207500458
Epoch 2230, training loss: 901.0038452148438 = 0.2331351786851883 + 100.0 * 9.007706642150879
Epoch 2230, val loss: 0.402413547039032
Epoch 2240, training loss: 900.9824829101562 = 0.2324458211660385 + 100.0 * 9.007500648498535
Epoch 2240, val loss: 0.4025850296020508
Epoch 2250, training loss: 900.9301147460938 = 0.23175038397312164 + 100.0 * 9.006983757019043
Epoch 2250, val loss: 0.40284717082977295
Epoch 2260, training loss: 901.0940551757812 = 0.23105674982070923 + 100.0 * 9.00862979888916
Epoch 2260, val loss: 0.4032452702522278
Epoch 2270, training loss: 900.9295654296875 = 0.23037004470825195 + 100.0 * 9.00699234008789
Epoch 2270, val loss: 0.4031572639942169
Epoch 2280, training loss: 901.158447265625 = 0.2296982705593109 + 100.0 * 9.00928783416748
Epoch 2280, val loss: 0.4035887122154236
Epoch 2290, training loss: 900.9336547851562 = 0.22901594638824463 + 100.0 * 9.007046699523926
Epoch 2290, val loss: 0.4041427671909332
Epoch 2300, training loss: 900.8629150390625 = 0.2283274084329605 + 100.0 * 9.006345748901367
Epoch 2300, val loss: 0.4044335186481476
Epoch 2310, training loss: 900.8342895507812 = 0.2276487648487091 + 100.0 * 9.00606632232666
Epoch 2310, val loss: 0.404788613319397
Epoch 2320, training loss: 900.8682250976562 = 0.22696471214294434 + 100.0 * 9.006412506103516
Epoch 2320, val loss: 0.40521448850631714
Epoch 2330, training loss: 901.1046752929688 = 0.2262853980064392 + 100.0 * 9.008784294128418
Epoch 2330, val loss: 0.40564948320388794
Epoch 2340, training loss: 900.9987182617188 = 0.22561663389205933 + 100.0 * 9.007730484008789
Epoch 2340, val loss: 0.40613502264022827
Epoch 2350, training loss: 900.8656616210938 = 0.22494842112064362 + 100.0 * 9.006406784057617
Epoch 2350, val loss: 0.40668419003486633
Epoch 2360, training loss: 900.802978515625 = 0.22428005933761597 + 100.0 * 9.005786895751953
Epoch 2360, val loss: 0.4069006145000458
Epoch 2370, training loss: 900.7850341796875 = 0.22362419962882996 + 100.0 * 9.005614280700684
Epoch 2370, val loss: 0.4069398045539856
Epoch 2380, training loss: 900.7897338867188 = 0.22295981645584106 + 100.0 * 9.005667686462402
Epoch 2380, val loss: 0.4071780741214752
Epoch 2390, training loss: 900.981689453125 = 0.22231675684452057 + 100.0 * 9.007594108581543
Epoch 2390, val loss: 0.40736135840415955
Epoch 2400, training loss: 900.9847412109375 = 0.22163140773773193 + 100.0 * 9.007631301879883
Epoch 2400, val loss: 0.4089958667755127
Epoch 2410, training loss: 900.8436889648438 = 0.22095398604869843 + 100.0 * 9.006227493286133
Epoch 2410, val loss: 0.4085412919521332
Epoch 2420, training loss: 900.7566528320312 = 0.22030292451381683 + 100.0 * 9.005363464355469
Epoch 2420, val loss: 0.40942177176475525
Epoch 2430, training loss: 900.8419189453125 = 0.2196466326713562 + 100.0 * 9.00622272491455
Epoch 2430, val loss: 0.40965038537979126
Epoch 2440, training loss: 900.76171875 = 0.21897481381893158 + 100.0 * 9.005427360534668
Epoch 2440, val loss: 0.410302996635437
Epoch 2450, training loss: 900.7171630859375 = 0.21833635866641998 + 100.0 * 9.004988670349121
Epoch 2450, val loss: 0.41052916646003723
Epoch 2460, training loss: 900.6934204101562 = 0.21767942607402802 + 100.0 * 9.00475788116455
Epoch 2460, val loss: 0.41103971004486084
Epoch 2470, training loss: 900.7418212890625 = 0.21702539920806885 + 100.0 * 9.005248069763184
Epoch 2470, val loss: 0.4115650951862335
Epoch 2480, training loss: 900.9129028320312 = 0.21635568141937256 + 100.0 * 9.006965637207031
Epoch 2480, val loss: 0.4120149612426758
Epoch 2490, training loss: 900.7910766601562 = 0.21572628617286682 + 100.0 * 9.005753517150879
Epoch 2490, val loss: 0.41223037242889404
Epoch 2500, training loss: 900.691650390625 = 0.21509110927581787 + 100.0 * 9.004765510559082
Epoch 2500, val loss: 0.4127914309501648
Epoch 2510, training loss: 900.65966796875 = 0.21443943679332733 + 100.0 * 9.0044527053833
Epoch 2510, val loss: 0.4135141372680664
Epoch 2520, training loss: 900.6271362304688 = 0.21379409730434418 + 100.0 * 9.004133224487305
Epoch 2520, val loss: 0.4139481484889984
Epoch 2530, training loss: 900.7672119140625 = 0.2131492644548416 + 100.0 * 9.00554084777832
Epoch 2530, val loss: 0.41474631428718567
Epoch 2540, training loss: 900.6509399414062 = 0.212498277425766 + 100.0 * 9.00438404083252
Epoch 2540, val loss: 0.415044367313385
Epoch 2550, training loss: 900.7656860351562 = 0.21188463270664215 + 100.0 * 9.005537986755371
Epoch 2550, val loss: 0.41565847396850586
Epoch 2560, training loss: 900.6810302734375 = 0.2112298458814621 + 100.0 * 9.004697799682617
Epoch 2560, val loss: 0.41560688614845276
Epoch 2570, training loss: 900.59033203125 = 0.2105979472398758 + 100.0 * 9.00379753112793
Epoch 2570, val loss: 0.416463702917099
Epoch 2580, training loss: 900.5809326171875 = 0.2099679559469223 + 100.0 * 9.00370979309082
Epoch 2580, val loss: 0.41696953773498535
Epoch 2590, training loss: 900.5762329101562 = 0.2093336135149002 + 100.0 * 9.003668785095215
Epoch 2590, val loss: 0.417368084192276
Epoch 2600, training loss: 900.7267456054688 = 0.20870263874530792 + 100.0 * 9.005180358886719
Epoch 2600, val loss: 0.4180768132209778
Epoch 2610, training loss: 900.716064453125 = 0.20805935561656952 + 100.0 * 9.005080223083496
Epoch 2610, val loss: 0.41812267899513245
Epoch 2620, training loss: 900.5919799804688 = 0.20744729042053223 + 100.0 * 9.00384521484375
Epoch 2620, val loss: 0.4189165532588959
Epoch 2630, training loss: 900.5535278320312 = 0.2068169265985489 + 100.0 * 9.003467559814453
Epoch 2630, val loss: 0.41922277212142944
Epoch 2640, training loss: 900.6708984375 = 0.20620478689670563 + 100.0 * 9.004647254943848
Epoch 2640, val loss: 0.41969358921051025
Epoch 2650, training loss: 900.6008911132812 = 0.2055770307779312 + 100.0 * 9.003952980041504
Epoch 2650, val loss: 0.4206725060939789
Epoch 2660, training loss: 900.5220336914062 = 0.20494967699050903 + 100.0 * 9.00317096710205
Epoch 2660, val loss: 0.4208160638809204
Epoch 2670, training loss: 900.5149536132812 = 0.2043200135231018 + 100.0 * 9.003106117248535
Epoch 2670, val loss: 0.4216753840446472
Epoch 2680, training loss: 900.4864501953125 = 0.2037040889263153 + 100.0 * 9.002827644348145
Epoch 2680, val loss: 0.4219658672809601
Epoch 2690, training loss: 900.5601196289062 = 0.20307907462120056 + 100.0 * 9.003570556640625
Epoch 2690, val loss: 0.42261481285095215
Epoch 2700, training loss: 900.5973510742188 = 0.20245163142681122 + 100.0 * 9.003949165344238
Epoch 2700, val loss: 0.4235199987888336
Epoch 2710, training loss: 900.63671875 = 0.2018534243106842 + 100.0 * 9.004348754882812
Epoch 2710, val loss: 0.4241732954978943
Epoch 2720, training loss: 900.5 = 0.20124784111976624 + 100.0 * 9.0029878616333
Epoch 2720, val loss: 0.42395979166030884
Epoch 2730, training loss: 900.472412109375 = 0.20062899589538574 + 100.0 * 9.002717971801758
Epoch 2730, val loss: 0.4247358441352844
Epoch 2740, training loss: 900.6145629882812 = 0.2000284641981125 + 100.0 * 9.004145622253418
Epoch 2740, val loss: 0.4249614179134369
Epoch 2750, training loss: 900.4464721679688 = 0.19940651953220367 + 100.0 * 9.002470970153809
Epoch 2750, val loss: 0.42594459652900696
Epoch 2760, training loss: 900.4456787109375 = 0.1988002359867096 + 100.0 * 9.002469062805176
Epoch 2760, val loss: 0.42676442861557007
Epoch 2770, training loss: 900.6004638671875 = 0.19820259511470795 + 100.0 * 9.004022598266602
Epoch 2770, val loss: 0.42703092098236084
Epoch 2780, training loss: 900.4478149414062 = 0.19759222865104675 + 100.0 * 9.00250244140625
Epoch 2780, val loss: 0.42741334438323975
Epoch 2790, training loss: 900.484375 = 0.1969919204711914 + 100.0 * 9.002873420715332
Epoch 2790, val loss: 0.4281139075756073
Epoch 2800, training loss: 900.4552612304688 = 0.19638818502426147 + 100.0 * 9.002588272094727
Epoch 2800, val loss: 0.42890432476997375
Epoch 2810, training loss: 900.509765625 = 0.19577904045581818 + 100.0 * 9.00313949584961
Epoch 2810, val loss: 0.42945149540901184
Epoch 2820, training loss: 900.3738403320312 = 0.19518297910690308 + 100.0 * 9.001786231994629
Epoch 2820, val loss: 0.4301505386829376
Epoch 2830, training loss: 900.3822631835938 = 0.19458794593811035 + 100.0 * 9.001876831054688
Epoch 2830, val loss: 0.4308767318725586
Epoch 2840, training loss: 900.5413818359375 = 0.1940004676580429 + 100.0 * 9.003473281860352
Epoch 2840, val loss: 0.43194153904914856
Epoch 2850, training loss: 900.3949584960938 = 0.1933945119380951 + 100.0 * 9.002015113830566
Epoch 2850, val loss: 0.4318205416202545
Epoch 2860, training loss: 900.371337890625 = 0.19279758632183075 + 100.0 * 9.001785278320312
Epoch 2860, val loss: 0.4326893091201782
Epoch 2870, training loss: 900.4923095703125 = 0.19221794605255127 + 100.0 * 9.00300121307373
Epoch 2870, val loss: 0.43301427364349365
Epoch 2880, training loss: 900.3707275390625 = 0.19163917005062103 + 100.0 * 9.001791000366211
Epoch 2880, val loss: 0.43338704109191895
Epoch 2890, training loss: 900.3748779296875 = 0.19105574488639832 + 100.0 * 9.001838684082031
Epoch 2890, val loss: 0.4339243173599243
Epoch 2900, training loss: 900.3466186523438 = 0.19044558703899384 + 100.0 * 9.001562118530273
Epoch 2900, val loss: 0.43489763140678406
Epoch 2910, training loss: 900.3436279296875 = 0.1898607611656189 + 100.0 * 9.001537322998047
Epoch 2910, val loss: 0.4353804290294647
Epoch 2920, training loss: 900.4274291992188 = 0.18927453458309174 + 100.0 * 9.002381324768066
Epoch 2920, val loss: 0.43609222769737244
Epoch 2930, training loss: 900.4390869140625 = 0.18869806826114655 + 100.0 * 9.002503395080566
Epoch 2930, val loss: 0.4368492662906647
Epoch 2940, training loss: 900.3491821289062 = 0.18811409175395966 + 100.0 * 9.00161075592041
Epoch 2940, val loss: 0.4372124969959259
Epoch 2950, training loss: 900.3764038085938 = 0.18753549456596375 + 100.0 * 9.001888275146484
Epoch 2950, val loss: 0.4381408393383026
Epoch 2960, training loss: 900.283935546875 = 0.18695290386676788 + 100.0 * 9.000969886779785
Epoch 2960, val loss: 0.43904659152030945
Epoch 2970, training loss: 900.3145751953125 = 0.18638332188129425 + 100.0 * 9.00128173828125
Epoch 2970, val loss: 0.4401164650917053
Epoch 2980, training loss: 900.395751953125 = 0.18582387268543243 + 100.0 * 9.00209903717041
Epoch 2980, val loss: 0.4409755766391754
Epoch 2990, training loss: 900.33984375 = 0.18522992730140686 + 100.0 * 9.001545906066895
Epoch 2990, val loss: 0.4405937194824219
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8473
Overall ASR: 0.7404
Flip ASR: 0.6770/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1037.0015869140625 = 1.1041889190673828 + 100.0 * 10.358973503112793
Epoch 0, val loss: 1.104020595550537
Epoch 10, training loss: 1036.49951171875 = 1.093125581741333 + 100.0 * 10.354063987731934
Epoch 10, val loss: 1.09254789352417
Epoch 20, training loss: 1028.49853515625 = 1.0784040689468384 + 100.0 * 10.274201393127441
Epoch 20, val loss: 1.077783226966858
Epoch 30, training loss: 966.615478515625 = 1.0651230812072754 + 100.0 * 9.655503273010254
Epoch 30, val loss: 1.0645273923873901
Epoch 40, training loss: 949.682373046875 = 1.0529133081436157 + 100.0 * 9.486294746398926
Epoch 40, val loss: 1.0524218082427979
Epoch 50, training loss: 938.3848876953125 = 1.040144681930542 + 100.0 * 9.37344741821289
Epoch 50, val loss: 1.0396109819412231
Epoch 60, training loss: 933.7257690429688 = 1.027915358543396 + 100.0 * 9.32697868347168
Epoch 60, val loss: 1.0276588201522827
Epoch 70, training loss: 929.9987182617188 = 1.0168447494506836 + 100.0 * 9.28981876373291
Epoch 70, val loss: 1.016795039176941
Epoch 80, training loss: 927.6380004882812 = 1.0065120458602905 + 100.0 * 9.266314506530762
Epoch 80, val loss: 1.006505012512207
Epoch 90, training loss: 925.21435546875 = 0.9952789545059204 + 100.0 * 9.24219036102295
Epoch 90, val loss: 0.9954318404197693
Epoch 100, training loss: 921.859619140625 = 0.9854620099067688 + 100.0 * 9.208741188049316
Epoch 100, val loss: 0.9860982298851013
Epoch 110, training loss: 918.499755859375 = 0.9775407910346985 + 100.0 * 9.175222396850586
Epoch 110, val loss: 0.978493332862854
Epoch 120, training loss: 916.8323974609375 = 0.9687085747718811 + 100.0 * 9.158637046813965
Epoch 120, val loss: 0.9697766304016113
Epoch 130, training loss: 915.5402221679688 = 0.9578708410263062 + 100.0 * 9.14582347869873
Epoch 130, val loss: 0.9588758945465088
Epoch 140, training loss: 914.52783203125 = 0.946547269821167 + 100.0 * 9.135812759399414
Epoch 140, val loss: 0.9478051066398621
Epoch 150, training loss: 913.5797729492188 = 0.9354663491249084 + 100.0 * 9.126442909240723
Epoch 150, val loss: 0.9371490478515625
Epoch 160, training loss: 912.7489013671875 = 0.9246609210968018 + 100.0 * 9.118242263793945
Epoch 160, val loss: 0.9267117381095886
Epoch 170, training loss: 912.0682373046875 = 0.9134965538978577 + 100.0 * 9.111547470092773
Epoch 170, val loss: 0.9160438179969788
Epoch 180, training loss: 911.3988037109375 = 0.9023104310035706 + 100.0 * 9.104965209960938
Epoch 180, val loss: 0.9051529169082642
Epoch 190, training loss: 910.8499755859375 = 0.891048789024353 + 100.0 * 9.099589347839355
Epoch 190, val loss: 0.8942490816116333
Epoch 200, training loss: 910.617919921875 = 0.8792346119880676 + 100.0 * 9.097387313842773
Epoch 200, val loss: 0.8828491568565369
Epoch 210, training loss: 910.0625 = 0.8665395975112915 + 100.0 * 9.091959953308105
Epoch 210, val loss: 0.8705792427062988
Epoch 220, training loss: 909.5289916992188 = 0.8544744849205017 + 100.0 * 9.086745262145996
Epoch 220, val loss: 0.8590944409370422
Epoch 230, training loss: 909.116455078125 = 0.8429744839668274 + 100.0 * 9.082735061645508
Epoch 230, val loss: 0.8480463027954102
Epoch 240, training loss: 908.830322265625 = 0.8306784629821777 + 100.0 * 9.079996109008789
Epoch 240, val loss: 0.8362412452697754
Epoch 250, training loss: 908.3727416992188 = 0.8180099129676819 + 100.0 * 9.075547218322754
Epoch 250, val loss: 0.8241375088691711
Epoch 260, training loss: 908.188232421875 = 0.805199921131134 + 100.0 * 9.073830604553223
Epoch 260, val loss: 0.8117644786834717
Epoch 270, training loss: 907.84375 = 0.7918969392776489 + 100.0 * 9.070518493652344
Epoch 270, val loss: 0.7991212606430054
Epoch 280, training loss: 907.5593872070312 = 0.7786946892738342 + 100.0 * 9.0678071975708
Epoch 280, val loss: 0.7866505980491638
Epoch 290, training loss: 907.6487426757812 = 0.764819860458374 + 100.0 * 9.068839073181152
Epoch 290, val loss: 0.7734978795051575
Epoch 300, training loss: 907.13525390625 = 0.7506533265113831 + 100.0 * 9.06384563446045
Epoch 300, val loss: 0.7600346207618713
Epoch 310, training loss: 906.9257202148438 = 0.7371293306350708 + 100.0 * 9.061885833740234
Epoch 310, val loss: 0.747169554233551
Epoch 320, training loss: 906.832275390625 = 0.7236865758895874 + 100.0 * 9.06108570098877
Epoch 320, val loss: 0.7343465089797974
Epoch 330, training loss: 906.7797241210938 = 0.7092673182487488 + 100.0 * 9.060704231262207
Epoch 330, val loss: 0.7207980751991272
Epoch 340, training loss: 906.4814453125 = 0.6951538920402527 + 100.0 * 9.057863235473633
Epoch 340, val loss: 0.7074872255325317
Epoch 350, training loss: 906.3263549804688 = 0.6815956234931946 + 100.0 * 9.056447982788086
Epoch 350, val loss: 0.6946876049041748
Epoch 360, training loss: 906.1670532226562 = 0.6676658987998962 + 100.0 * 9.054993629455566
Epoch 360, val loss: 0.6815400123596191
Epoch 370, training loss: 906.0639038085938 = 0.6538419127464294 + 100.0 * 9.054100036621094
Epoch 370, val loss: 0.668565571308136
Epoch 380, training loss: 905.943359375 = 0.6408268809318542 + 100.0 * 9.053025245666504
Epoch 380, val loss: 0.6563887596130371
Epoch 390, training loss: 905.8910522460938 = 0.6281636953353882 + 100.0 * 9.052628517150879
Epoch 390, val loss: 0.6445450782775879
Epoch 400, training loss: 906.0218505859375 = 0.6149671673774719 + 100.0 * 9.054068565368652
Epoch 400, val loss: 0.6320175528526306
Epoch 410, training loss: 905.7207641601562 = 0.6023147106170654 + 100.0 * 9.05118465423584
Epoch 410, val loss: 0.6203113198280334
Epoch 420, training loss: 905.55517578125 = 0.5906375050544739 + 100.0 * 9.04964542388916
Epoch 420, val loss: 0.6094489097595215
Epoch 430, training loss: 905.4719848632812 = 0.5793338418006897 + 100.0 * 9.04892635345459
Epoch 430, val loss: 0.5989381074905396
Epoch 440, training loss: 905.5316772460938 = 0.5680191516876221 + 100.0 * 9.049636840820312
Epoch 440, val loss: 0.5884928703308105
Epoch 450, training loss: 905.4221801757812 = 0.5569882988929749 + 100.0 * 9.048651695251465
Epoch 450, val loss: 0.5782517790794373
Epoch 460, training loss: 905.2245483398438 = 0.5466716289520264 + 100.0 * 9.046778678894043
Epoch 460, val loss: 0.5687019228935242
Epoch 470, training loss: 905.1853637695312 = 0.5369728803634644 + 100.0 * 9.046483993530273
Epoch 470, val loss: 0.5597626566886902
Epoch 480, training loss: 905.2444458007812 = 0.5276100039482117 + 100.0 * 9.047168731689453
Epoch 480, val loss: 0.5510139465332031
Epoch 490, training loss: 905.1397094726562 = 0.5181977152824402 + 100.0 * 9.046215057373047
Epoch 490, val loss: 0.5425455570220947
Epoch 500, training loss: 905.010009765625 = 0.509525716304779 + 100.0 * 9.045004844665527
Epoch 500, val loss: 0.5345868468284607
Epoch 510, training loss: 904.9215698242188 = 0.5014415979385376 + 100.0 * 9.044200897216797
Epoch 510, val loss: 0.527302622795105
Epoch 520, training loss: 904.8604125976562 = 0.4938103258609772 + 100.0 * 9.043665885925293
Epoch 520, val loss: 0.5203821659088135
Epoch 530, training loss: 904.8460693359375 = 0.48648685216903687 + 100.0 * 9.043595314025879
Epoch 530, val loss: 0.5137836933135986
Epoch 540, training loss: 904.8388671875 = 0.4790748357772827 + 100.0 * 9.043598175048828
Epoch 540, val loss: 0.5070876479148865
Epoch 550, training loss: 904.7448120117188 = 0.4722336232662201 + 100.0 * 9.042725563049316
Epoch 550, val loss: 0.5009857416152954
Epoch 560, training loss: 904.667236328125 = 0.4659658372402191 + 100.0 * 9.042013168334961
Epoch 560, val loss: 0.49542373418807983
Epoch 570, training loss: 904.6190185546875 = 0.4599953293800354 + 100.0 * 9.041589736938477
Epoch 570, val loss: 0.4901709258556366
Epoch 580, training loss: 904.8289794921875 = 0.45410487055778503 + 100.0 * 9.04374885559082
Epoch 580, val loss: 0.48508140444755554
Epoch 590, training loss: 904.5714721679688 = 0.4483358860015869 + 100.0 * 9.041231155395508
Epoch 590, val loss: 0.4799627363681793
Epoch 600, training loss: 904.5001831054688 = 0.44320714473724365 + 100.0 * 9.040569305419922
Epoch 600, val loss: 0.4753863215446472
Epoch 610, training loss: 904.4024658203125 = 0.4384342432022095 + 100.0 * 9.039640426635742
Epoch 610, val loss: 0.4713357388973236
Epoch 620, training loss: 904.38525390625 = 0.43388813734054565 + 100.0 * 9.03951358795166
Epoch 620, val loss: 0.4674873948097229
Epoch 630, training loss: 904.4017944335938 = 0.4292706847190857 + 100.0 * 9.039725303649902
Epoch 630, val loss: 0.46349626779556274
Epoch 640, training loss: 904.3189086914062 = 0.42489078640937805 + 100.0 * 9.0389404296875
Epoch 640, val loss: 0.45982909202575684
Epoch 650, training loss: 904.3259887695312 = 0.42091459035873413 + 100.0 * 9.039051055908203
Epoch 650, val loss: 0.4564450979232788
Epoch 660, training loss: 904.2140502929688 = 0.4168204069137573 + 100.0 * 9.037972450256348
Epoch 660, val loss: 0.4532128572463989
Epoch 670, training loss: 904.1876220703125 = 0.4131338596343994 + 100.0 * 9.037744522094727
Epoch 670, val loss: 0.45009860396385193
Epoch 680, training loss: 904.11181640625 = 0.40968602895736694 + 100.0 * 9.03702163696289
Epoch 680, val loss: 0.44739893078804016
Epoch 690, training loss: 904.0991821289062 = 0.4063919484615326 + 100.0 * 9.036928176879883
Epoch 690, val loss: 0.4447072148323059
Epoch 700, training loss: 904.090087890625 = 0.40298643708229065 + 100.0 * 9.036870956420898
Epoch 700, val loss: 0.44201216101646423
Epoch 710, training loss: 904.0213623046875 = 0.39979198575019836 + 100.0 * 9.036215782165527
Epoch 710, val loss: 0.43953216075897217
Epoch 720, training loss: 903.9494018554688 = 0.396880179643631 + 100.0 * 9.03552532196045
Epoch 720, val loss: 0.4373103976249695
Epoch 730, training loss: 904.172607421875 = 0.39404135942459106 + 100.0 * 9.037785530090332
Epoch 730, val loss: 0.4350314736366272
Epoch 740, training loss: 904.073974609375 = 0.3910009562969208 + 100.0 * 9.036829948425293
Epoch 740, val loss: 0.43292075395584106
Epoch 750, training loss: 903.8438110351562 = 0.3883665204048157 + 100.0 * 9.034554481506348
Epoch 750, val loss: 0.4309646487236023
Epoch 760, training loss: 903.8248291015625 = 0.38585901260375977 + 100.0 * 9.03438949584961
Epoch 760, val loss: 0.4292071461677551
Epoch 770, training loss: 903.91064453125 = 0.3833165168762207 + 100.0 * 9.035273551940918
Epoch 770, val loss: 0.4273740351200104
Epoch 780, training loss: 903.7674560546875 = 0.3808594048023224 + 100.0 * 9.033865928649902
Epoch 780, val loss: 0.4254440367221832
Epoch 790, training loss: 903.7646484375 = 0.37851712107658386 + 100.0 * 9.03386116027832
Epoch 790, val loss: 0.4237208068370819
Epoch 800, training loss: 903.7493286132812 = 0.37625667452812195 + 100.0 * 9.033730506896973
Epoch 800, val loss: 0.4224466383457184
Epoch 810, training loss: 903.7304077148438 = 0.3739623427391052 + 100.0 * 9.033564567565918
Epoch 810, val loss: 0.42072737216949463
Epoch 820, training loss: 903.6223754882812 = 0.37178394198417664 + 100.0 * 9.032505989074707
Epoch 820, val loss: 0.4194013774394989
Epoch 830, training loss: 903.6494750976562 = 0.36971238255500793 + 100.0 * 9.032797813415527
Epoch 830, val loss: 0.41795364022254944
Epoch 840, training loss: 903.6124267578125 = 0.36764663457870483 + 100.0 * 9.032447814941406
Epoch 840, val loss: 0.4165981709957123
Epoch 850, training loss: 903.5238037109375 = 0.36569836735725403 + 100.0 * 9.031580924987793
Epoch 850, val loss: 0.41551801562309265
Epoch 860, training loss: 903.4811401367188 = 0.3637964725494385 + 100.0 * 9.031173706054688
Epoch 860, val loss: 0.4141300916671753
Epoch 870, training loss: 903.7172241210938 = 0.36194083094596863 + 100.0 * 9.033553123474121
Epoch 870, val loss: 0.41308894753456116
Epoch 880, training loss: 903.6026000976562 = 0.35990723967552185 + 100.0 * 9.032426834106445
Epoch 880, val loss: 0.41183173656463623
Epoch 890, training loss: 903.3858032226562 = 0.35815557837486267 + 100.0 * 9.03027629852295
Epoch 890, val loss: 0.4107789695262909
Epoch 900, training loss: 903.5523681640625 = 0.3564203977584839 + 100.0 * 9.031959533691406
Epoch 900, val loss: 0.40991926193237305
Epoch 910, training loss: 903.34912109375 = 0.3546382784843445 + 100.0 * 9.029945373535156
Epoch 910, val loss: 0.40890413522720337
Epoch 920, training loss: 903.3754272460938 = 0.3529725968837738 + 100.0 * 9.030224800109863
Epoch 920, val loss: 0.4079412519931793
Epoch 930, training loss: 903.3025512695312 = 0.3513354957103729 + 100.0 * 9.029512405395508
Epoch 930, val loss: 0.4071516990661621
Epoch 940, training loss: 903.2804565429688 = 0.34972789883613586 + 100.0 * 9.02930736541748
Epoch 940, val loss: 0.40620002150535583
Epoch 950, training loss: 903.2705078125 = 0.34812602400779724 + 100.0 * 9.029223442077637
Epoch 950, val loss: 0.4052744209766388
Epoch 960, training loss: 903.2872314453125 = 0.3465454578399658 + 100.0 * 9.029406547546387
Epoch 960, val loss: 0.404365211725235
Epoch 970, training loss: 903.24853515625 = 0.3449721932411194 + 100.0 * 9.029035568237305
Epoch 970, val loss: 0.4037080705165863
Epoch 980, training loss: 903.2489624023438 = 0.3434109687805176 + 100.0 * 9.02905559539795
Epoch 980, val loss: 0.40293803811073303
Epoch 990, training loss: 903.133056640625 = 0.341921329498291 + 100.0 * 9.027911186218262
Epoch 990, val loss: 0.4021688401699066
Epoch 1000, training loss: 903.0950927734375 = 0.34050503373146057 + 100.0 * 9.027545928955078
Epoch 1000, val loss: 0.4014533758163452
Epoch 1010, training loss: 903.2246704101562 = 0.3390876352787018 + 100.0 * 9.028855323791504
Epoch 1010, val loss: 0.40053316950798035
Epoch 1020, training loss: 903.1506958007812 = 0.3376237154006958 + 100.0 * 9.028130531311035
Epoch 1020, val loss: 0.4002995789051056
Epoch 1030, training loss: 903.0986938476562 = 0.3361951410770416 + 100.0 * 9.02762508392334
Epoch 1030, val loss: 0.39948952198028564
Epoch 1040, training loss: 903.0490112304688 = 0.3348330557346344 + 100.0 * 9.027141571044922
Epoch 1040, val loss: 0.3990526795387268
Epoch 1050, training loss: 902.9627685546875 = 0.33348920941352844 + 100.0 * 9.02629280090332
Epoch 1050, val loss: 0.3982677757740021
Epoch 1060, training loss: 903.0076904296875 = 0.33216995000839233 + 100.0 * 9.026755332946777
Epoch 1060, val loss: 0.3978182375431061
Epoch 1070, training loss: 902.9944458007812 = 0.33080750703811646 + 100.0 * 9.026636123657227
Epoch 1070, val loss: 0.3972395658493042
Epoch 1080, training loss: 902.9984130859375 = 0.3294762372970581 + 100.0 * 9.026689529418945
Epoch 1080, val loss: 0.39672183990478516
Epoch 1090, training loss: 902.9401245117188 = 0.32816970348358154 + 100.0 * 9.026119232177734
Epoch 1090, val loss: 0.39630988240242004
Epoch 1100, training loss: 902.866943359375 = 0.32688531279563904 + 100.0 * 9.025400161743164
Epoch 1100, val loss: 0.39571425318717957
Epoch 1110, training loss: 902.8349609375 = 0.3256707787513733 + 100.0 * 9.025093078613281
Epoch 1110, val loss: 0.3953745365142822
Epoch 1120, training loss: 902.8807983398438 = 0.3244471251964569 + 100.0 * 9.02556324005127
Epoch 1120, val loss: 0.39489084482192993
Epoch 1130, training loss: 902.897216796875 = 0.3231772482395172 + 100.0 * 9.025740623474121
Epoch 1130, val loss: 0.3942986726760864
Epoch 1140, training loss: 902.7577514648438 = 0.3219476342201233 + 100.0 * 9.024357795715332
Epoch 1140, val loss: 0.39414215087890625
Epoch 1150, training loss: 902.7355346679688 = 0.32075217366218567 + 100.0 * 9.024147987365723
Epoch 1150, val loss: 0.3935641348361969
Epoch 1160, training loss: 902.804443359375 = 0.31957685947418213 + 100.0 * 9.024848937988281
Epoch 1160, val loss: 0.3933836817741394
Epoch 1170, training loss: 902.8182983398438 = 0.31838178634643555 + 100.0 * 9.024999618530273
Epoch 1170, val loss: 0.39295172691345215
Epoch 1180, training loss: 902.700439453125 = 0.31720611453056335 + 100.0 * 9.023832321166992
Epoch 1180, val loss: 0.392442911863327
Epoch 1190, training loss: 902.6627807617188 = 0.3160768449306488 + 100.0 * 9.023467063903809
Epoch 1190, val loss: 0.3922649919986725
Epoch 1200, training loss: 902.7322998046875 = 0.3149458169937134 + 100.0 * 9.024173736572266
Epoch 1200, val loss: 0.39193281531333923
Epoch 1210, training loss: 902.6529541015625 = 0.31380677223205566 + 100.0 * 9.023391723632812
Epoch 1210, val loss: 0.39129629731178284
Epoch 1220, training loss: 902.6686401367188 = 0.3126955032348633 + 100.0 * 9.0235595703125
Epoch 1220, val loss: 0.3912104070186615
Epoch 1230, training loss: 902.6161499023438 = 0.31158071756362915 + 100.0 * 9.023045539855957
Epoch 1230, val loss: 0.39080435037612915
Epoch 1240, training loss: 902.5823364257812 = 0.31049633026123047 + 100.0 * 9.02271842956543
Epoch 1240, val loss: 0.3906171917915344
Epoch 1250, training loss: 902.738037109375 = 0.3093734085559845 + 100.0 * 9.024286270141602
Epoch 1250, val loss: 0.39016062021255493
Epoch 1260, training loss: 902.5045776367188 = 0.3083053529262543 + 100.0 * 9.021963119506836
Epoch 1260, val loss: 0.39019274711608887
Epoch 1270, training loss: 902.4896240234375 = 0.30724817514419556 + 100.0 * 9.02182388305664
Epoch 1270, val loss: 0.38969749212265015
Epoch 1280, training loss: 902.6004028320312 = 0.3062109053134918 + 100.0 * 9.022941589355469
Epoch 1280, val loss: 0.3896549940109253
Epoch 1290, training loss: 902.4401245117188 = 0.30513694882392883 + 100.0 * 9.021349906921387
Epoch 1290, val loss: 0.3892471492290497
Epoch 1300, training loss: 902.4883422851562 = 0.30411186814308167 + 100.0 * 9.021842002868652
Epoch 1300, val loss: 0.3891216218471527
Epoch 1310, training loss: 902.4896240234375 = 0.30305224657058716 + 100.0 * 9.021865844726562
Epoch 1310, val loss: 0.38889607787132263
Epoch 1320, training loss: 902.4557495117188 = 0.30203574895858765 + 100.0 * 9.021536827087402
Epoch 1320, val loss: 0.3887859880924225
Epoch 1330, training loss: 902.3651733398438 = 0.30101263523101807 + 100.0 * 9.020641326904297
Epoch 1330, val loss: 0.3885129690170288
Epoch 1340, training loss: 902.400146484375 = 0.30000269412994385 + 100.0 * 9.021001815795898
Epoch 1340, val loss: 0.38831424713134766
Epoch 1350, training loss: 902.4579467773438 = 0.2989916205406189 + 100.0 * 9.021589279174805
Epoch 1350, val loss: 0.3882879316806793
Epoch 1360, training loss: 902.7343139648438 = 0.2979685366153717 + 100.0 * 9.02436351776123
Epoch 1360, val loss: 0.387709379196167
Epoch 1370, training loss: 902.3446655273438 = 0.29695627093315125 + 100.0 * 9.020477294921875
Epoch 1370, val loss: 0.38795292377471924
Epoch 1380, training loss: 902.2900390625 = 0.2960051894187927 + 100.0 * 9.019940376281738
Epoch 1380, val loss: 0.38768845796585083
Epoch 1390, training loss: 902.24072265625 = 0.29506364464759827 + 100.0 * 9.01945686340332
Epoch 1390, val loss: 0.38758254051208496
Epoch 1400, training loss: 902.2041625976562 = 0.294119268655777 + 100.0 * 9.019100189208984
Epoch 1400, val loss: 0.38741207122802734
Epoch 1410, training loss: 902.2679443359375 = 0.29317596554756165 + 100.0 * 9.019747734069824
Epoch 1410, val loss: 0.3871511220932007
Epoch 1420, training loss: 902.3386840820312 = 0.2921609878540039 + 100.0 * 9.020464897155762
Epoch 1420, val loss: 0.3872871696949005
Epoch 1430, training loss: 902.2947998046875 = 0.2912152111530304 + 100.0 * 9.020035743713379
Epoch 1430, val loss: 0.3869575560092926
Epoch 1440, training loss: 902.138671875 = 0.2903074324131012 + 100.0 * 9.018484115600586
Epoch 1440, val loss: 0.3869296610355377
Epoch 1450, training loss: 902.1344604492188 = 0.2893831133842468 + 100.0 * 9.018450736999512
Epoch 1450, val loss: 0.3869687020778656
Epoch 1460, training loss: 902.113525390625 = 0.28847745060920715 + 100.0 * 9.018250465393066
Epoch 1460, val loss: 0.38689160346984863
Epoch 1470, training loss: 902.4637451171875 = 0.28755220770835876 + 100.0 * 9.021761894226074
Epoch 1470, val loss: 0.3867947459220886
Epoch 1480, training loss: 902.3279418945312 = 0.2865961492061615 + 100.0 * 9.020413398742676
Epoch 1480, val loss: 0.38641661405563354
Epoch 1490, training loss: 902.07568359375 = 0.2856982946395874 + 100.0 * 9.017899513244629
Epoch 1490, val loss: 0.3867153823375702
Epoch 1500, training loss: 902.119873046875 = 0.28480637073516846 + 100.0 * 9.018350601196289
Epoch 1500, val loss: 0.386588990688324
Epoch 1510, training loss: 902.1567993164062 = 0.2838784158229828 + 100.0 * 9.018729209899902
Epoch 1510, val loss: 0.3864675462245941
Epoch 1520, training loss: 902.0028076171875 = 0.28300154209136963 + 100.0 * 9.017197608947754
Epoch 1520, val loss: 0.38645195960998535
Epoch 1530, training loss: 902.0001220703125 = 0.28214019536972046 + 100.0 * 9.017179489135742
Epoch 1530, val loss: 0.386466920375824
Epoch 1540, training loss: 902.013427734375 = 0.28126487135887146 + 100.0 * 9.017321586608887
Epoch 1540, val loss: 0.3864569365978241
Epoch 1550, training loss: 902.2332153320312 = 0.2803802788257599 + 100.0 * 9.01952838897705
Epoch 1550, val loss: 0.386497437953949
Epoch 1560, training loss: 902.0276489257812 = 0.2794940769672394 + 100.0 * 9.017481803894043
Epoch 1560, val loss: 0.386379599571228
Epoch 1570, training loss: 901.9615478515625 = 0.27863723039627075 + 100.0 * 9.016829490661621
Epoch 1570, val loss: 0.386342853307724
Epoch 1580, training loss: 901.9718627929688 = 0.2777833938598633 + 100.0 * 9.01694107055664
Epoch 1580, val loss: 0.3864502012729645
Epoch 1590, training loss: 902.2117309570312 = 0.2769167125225067 + 100.0 * 9.01934814453125
Epoch 1590, val loss: 0.38654544949531555
Epoch 1600, training loss: 901.9179077148438 = 0.2760452628135681 + 100.0 * 9.01641845703125
Epoch 1600, val loss: 0.38642463088035583
Epoch 1610, training loss: 901.896728515625 = 0.2752177119255066 + 100.0 * 9.016215324401855
Epoch 1610, val loss: 0.3865477442741394
Epoch 1620, training loss: 901.8564453125 = 0.2743898928165436 + 100.0 * 9.015820503234863
Epoch 1620, val loss: 0.38651981949806213
Epoch 1630, training loss: 901.8965454101562 = 0.2735559046268463 + 100.0 * 9.016229629516602
Epoch 1630, val loss: 0.3864544928073883
Epoch 1640, training loss: 901.9903564453125 = 0.27270442247390747 + 100.0 * 9.017176628112793
Epoch 1640, val loss: 0.3865993618965149
Epoch 1650, training loss: 901.910400390625 = 0.2719000577926636 + 100.0 * 9.016385078430176
Epoch 1650, val loss: 0.3867819309234619
Epoch 1660, training loss: 902.0255126953125 = 0.27104759216308594 + 100.0 * 9.017544746398926
Epoch 1660, val loss: 0.3866232633590698
Epoch 1670, training loss: 901.821533203125 = 0.2702517509460449 + 100.0 * 9.015512466430664
Epoch 1670, val loss: 0.38677337765693665
Epoch 1680, training loss: 901.80126953125 = 0.2694372534751892 + 100.0 * 9.015318870544434
Epoch 1680, val loss: 0.38671377301216125
Epoch 1690, training loss: 902.018310546875 = 0.26862815022468567 + 100.0 * 9.017497062683105
Epoch 1690, val loss: 0.3869284391403198
Epoch 1700, training loss: 901.8795166015625 = 0.2678048014640808 + 100.0 * 9.016117095947266
Epoch 1700, val loss: 0.38735246658325195
Epoch 1710, training loss: 901.756591796875 = 0.2670089602470398 + 100.0 * 9.01489543914795
Epoch 1710, val loss: 0.3870341181755066
Epoch 1720, training loss: 901.7213134765625 = 0.2662033140659332 + 100.0 * 9.014551162719727
Epoch 1720, val loss: 0.38721776008605957
Epoch 1730, training loss: 901.7711181640625 = 0.2654135525226593 + 100.0 * 9.015056610107422
Epoch 1730, val loss: 0.38746392726898193
Epoch 1740, training loss: 901.790283203125 = 0.26459285616874695 + 100.0 * 9.015256881713867
Epoch 1740, val loss: 0.3875197172164917
Epoch 1750, training loss: 901.7474975585938 = 0.263802170753479 + 100.0 * 9.014837265014648
Epoch 1750, val loss: 0.3875589668750763
Epoch 1760, training loss: 901.7000122070312 = 0.263015478849411 + 100.0 * 9.01436996459961
Epoch 1760, val loss: 0.38786858320236206
Epoch 1770, training loss: 901.8764038085938 = 0.2622266113758087 + 100.0 * 9.016141891479492
Epoch 1770, val loss: 0.3879319727420807
Epoch 1780, training loss: 901.711181640625 = 0.26143163442611694 + 100.0 * 9.014497756958008
Epoch 1780, val loss: 0.38788142800331116
Epoch 1790, training loss: 901.7112426757812 = 0.26064905524253845 + 100.0 * 9.014506340026855
Epoch 1790, val loss: 0.3882042169570923
Epoch 1800, training loss: 901.6403198242188 = 0.25987598299980164 + 100.0 * 9.01380443572998
Epoch 1800, val loss: 0.3881917893886566
Epoch 1810, training loss: 901.7384643554688 = 0.25911882519721985 + 100.0 * 9.014793395996094
Epoch 1810, val loss: 0.38850411772727966
Epoch 1820, training loss: 901.641357421875 = 0.2583349645137787 + 100.0 * 9.013830184936523
Epoch 1820, val loss: 0.3884504437446594
Epoch 1830, training loss: 901.5886840820312 = 0.25756552815437317 + 100.0 * 9.013311386108398
Epoch 1830, val loss: 0.388628214597702
Epoch 1840, training loss: 901.5596313476562 = 0.2568076550960541 + 100.0 * 9.013028144836426
Epoch 1840, val loss: 0.3888300657272339
Epoch 1850, training loss: 901.669189453125 = 0.2560516595840454 + 100.0 * 9.014131546020508
Epoch 1850, val loss: 0.3890058994293213
Epoch 1860, training loss: 901.5924682617188 = 0.25528228282928467 + 100.0 * 9.013371467590332
Epoch 1860, val loss: 0.38915595412254333
Epoch 1870, training loss: 901.5294799804688 = 0.2545255124568939 + 100.0 * 9.012749671936035
Epoch 1870, val loss: 0.38919103145599365
Epoch 1880, training loss: 901.62841796875 = 0.2537728250026703 + 100.0 * 9.01374626159668
Epoch 1880, val loss: 0.38939177989959717
Epoch 1890, training loss: 901.5015258789062 = 0.2530198395252228 + 100.0 * 9.01248550415039
Epoch 1890, val loss: 0.3897339999675751
Epoch 1900, training loss: 901.499755859375 = 0.25228351354599 + 100.0 * 9.01247501373291
Epoch 1900, val loss: 0.38990622758865356
Epoch 1910, training loss: 901.647216796875 = 0.251539409160614 + 100.0 * 9.013957023620605
Epoch 1910, val loss: 0.3900253176689148
Epoch 1920, training loss: 901.54150390625 = 0.25078073143959045 + 100.0 * 9.012907028198242
Epoch 1920, val loss: 0.3900521695613861
Epoch 1930, training loss: 901.5011596679688 = 0.25005894899368286 + 100.0 * 9.012511253356934
Epoch 1930, val loss: 0.39035773277282715
Epoch 1940, training loss: 901.4600219726562 = 0.24932458996772766 + 100.0 * 9.012106895446777
Epoch 1940, val loss: 0.3907119333744049
Epoch 1950, training loss: 901.6954956054688 = 0.24859057366847992 + 100.0 * 9.014469146728516
Epoch 1950, val loss: 0.39105430245399475
Epoch 1960, training loss: 901.5245361328125 = 0.247843936085701 + 100.0 * 9.01276683807373
Epoch 1960, val loss: 0.3911278247833252
Epoch 1970, training loss: 901.4337768554688 = 0.24713613092899323 + 100.0 * 9.011866569519043
Epoch 1970, val loss: 0.3915637731552124
Epoch 1980, training loss: 901.357421875 = 0.24641911685466766 + 100.0 * 9.011110305786133
Epoch 1980, val loss: 0.3916226029396057
Epoch 1990, training loss: 901.3389282226562 = 0.24568870663642883 + 100.0 * 9.010932922363281
Epoch 1990, val loss: 0.39182573556900024
Epoch 2000, training loss: 901.63818359375 = 0.24495281279087067 + 100.0 * 9.013932228088379
Epoch 2000, val loss: 0.3922268748283386
Epoch 2010, training loss: 901.4381713867188 = 0.24422630667686462 + 100.0 * 9.011940002441406
Epoch 2010, val loss: 0.39231985807418823
Epoch 2020, training loss: 901.453369140625 = 0.24352049827575684 + 100.0 * 9.01209831237793
Epoch 2020, val loss: 0.39282456040382385
Epoch 2030, training loss: 901.3177490234375 = 0.24281315505504608 + 100.0 * 9.010749816894531
Epoch 2030, val loss: 0.3929747939109802
Epoch 2040, training loss: 901.3314208984375 = 0.2421078085899353 + 100.0 * 9.010892868041992
Epoch 2040, val loss: 0.39307671785354614
Epoch 2050, training loss: 901.3948364257812 = 0.24138914048671722 + 100.0 * 9.011534690856934
Epoch 2050, val loss: 0.39344775676727295
Epoch 2060, training loss: 901.3339233398438 = 0.2406940758228302 + 100.0 * 9.010931968688965
Epoch 2060, val loss: 0.39356404542922974
Epoch 2070, training loss: 901.4234008789062 = 0.23997022211551666 + 100.0 * 9.011834144592285
Epoch 2070, val loss: 0.3938571512699127
Epoch 2080, training loss: 901.3134765625 = 0.23928581178188324 + 100.0 * 9.0107421875
Epoch 2080, val loss: 0.3942897915840149
Epoch 2090, training loss: 901.3541870117188 = 0.2385895848274231 + 100.0 * 9.01115608215332
Epoch 2090, val loss: 0.3947587311267853
Epoch 2100, training loss: 901.2573852539062 = 0.23789088428020477 + 100.0 * 9.010194778442383
Epoch 2100, val loss: 0.3948814570903778
Epoch 2110, training loss: 901.2944946289062 = 0.23719656467437744 + 100.0 * 9.010573387145996
Epoch 2110, val loss: 0.3951071500778198
Epoch 2120, training loss: 901.2767944335938 = 0.2365114986896515 + 100.0 * 9.01040267944336
Epoch 2120, val loss: 0.39536523818969727
Epoch 2130, training loss: 901.2727661132812 = 0.23582138121128082 + 100.0 * 9.010369300842285
Epoch 2130, val loss: 0.3956117630004883
Epoch 2140, training loss: 901.3204956054688 = 0.23512138426303864 + 100.0 * 9.01085376739502
Epoch 2140, val loss: 0.3959963619709015
Epoch 2150, training loss: 901.2330932617188 = 0.2344481498003006 + 100.0 * 9.009986877441406
Epoch 2150, val loss: 0.39651787281036377
Epoch 2160, training loss: 901.195556640625 = 0.2337542027235031 + 100.0 * 9.009617805480957
Epoch 2160, val loss: 0.39665648341178894
Epoch 2170, training loss: 901.2625122070312 = 0.2330792248249054 + 100.0 * 9.010293960571289
Epoch 2170, val loss: 0.39650747179985046
Epoch 2180, training loss: 901.183349609375 = 0.23239360749721527 + 100.0 * 9.009509086608887
Epoch 2180, val loss: 0.3972381353378296
Epoch 2190, training loss: 901.1510009765625 = 0.23172540962696075 + 100.0 * 9.00919246673584
Epoch 2190, val loss: 0.397553950548172
Epoch 2200, training loss: 901.1515502929688 = 0.23105156421661377 + 100.0 * 9.009204864501953
Epoch 2200, val loss: 0.39787930250167847
Epoch 2210, training loss: 901.17626953125 = 0.23038220405578613 + 100.0 * 9.009458541870117
Epoch 2210, val loss: 0.3978744149208069
Epoch 2220, training loss: 901.2318725585938 = 0.22970087826251984 + 100.0 * 9.010022163391113
Epoch 2220, val loss: 0.3984307050704956
Epoch 2230, training loss: 901.1612548828125 = 0.2290399819612503 + 100.0 * 9.009322166442871
Epoch 2230, val loss: 0.39890575408935547
Epoch 2240, training loss: 901.1356201171875 = 0.2283705770969391 + 100.0 * 9.009072303771973
Epoch 2240, val loss: 0.3991369307041168
Epoch 2250, training loss: 901.1677856445312 = 0.22771607339382172 + 100.0 * 9.009400367736816
Epoch 2250, val loss: 0.39978349208831787
Epoch 2260, training loss: 901.2355346679688 = 0.2270474135875702 + 100.0 * 9.010085105895996
Epoch 2260, val loss: 0.39993420243263245
Epoch 2270, training loss: 901.0776977539062 = 0.22638452053070068 + 100.0 * 9.008513450622559
Epoch 2270, val loss: 0.4002215564250946
Epoch 2280, training loss: 901.0478515625 = 0.22572381794452667 + 100.0 * 9.008221626281738
Epoch 2280, val loss: 0.40051135420799255
Epoch 2290, training loss: 901.042236328125 = 0.2250659316778183 + 100.0 * 9.008172035217285
Epoch 2290, val loss: 0.40085071325302124
Epoch 2300, training loss: 901.2269287109375 = 0.22440747916698456 + 100.0 * 9.010025024414062
Epoch 2300, val loss: 0.40118324756622314
Epoch 2310, training loss: 901.0532836914062 = 0.22373753786087036 + 100.0 * 9.008295059204102
Epoch 2310, val loss: 0.4016767740249634
Epoch 2320, training loss: 901.1842651367188 = 0.22309079766273499 + 100.0 * 9.009612083435059
Epoch 2320, val loss: 0.40221962332725525
Epoch 2330, training loss: 901.07958984375 = 0.22243185341358185 + 100.0 * 9.00857162475586
Epoch 2330, val loss: 0.40256237983703613
Epoch 2340, training loss: 901.0103759765625 = 0.2217819094657898 + 100.0 * 9.007885932922363
Epoch 2340, val loss: 0.4031347334384918
Epoch 2350, training loss: 901.029541015625 = 0.22113251686096191 + 100.0 * 9.008084297180176
Epoch 2350, val loss: 0.4032348394393921
Epoch 2360, training loss: 901.0643310546875 = 0.22049331665039062 + 100.0 * 9.008438110351562
Epoch 2360, val loss: 0.40379804372787476
Epoch 2370, training loss: 901.0036010742188 = 0.219828799366951 + 100.0 * 9.007837295532227
Epoch 2370, val loss: 0.4039287269115448
Epoch 2380, training loss: 901.057861328125 = 0.21917618811130524 + 100.0 * 9.008386611938477
Epoch 2380, val loss: 0.4044018089771271
Epoch 2390, training loss: 901.0030517578125 = 0.21853666007518768 + 100.0 * 9.007844924926758
Epoch 2390, val loss: 0.4051137864589691
Epoch 2400, training loss: 900.9324951171875 = 0.21789944171905518 + 100.0 * 9.007145881652832
Epoch 2400, val loss: 0.40545663237571716
Epoch 2410, training loss: 900.9510498046875 = 0.21726107597351074 + 100.0 * 9.00733757019043
Epoch 2410, val loss: 0.405810683965683
Epoch 2420, training loss: 900.9578857421875 = 0.21661970019340515 + 100.0 * 9.007412910461426
Epoch 2420, val loss: 0.40640389919281006
Epoch 2430, training loss: 900.9691162109375 = 0.21598151326179504 + 100.0 * 9.00753116607666
Epoch 2430, val loss: 0.4068792164325714
Epoch 2440, training loss: 900.9838256835938 = 0.21533794701099396 + 100.0 * 9.007684707641602
Epoch 2440, val loss: 0.40700170397758484
Epoch 2450, training loss: 900.8878173828125 = 0.21470604836940765 + 100.0 * 9.006731033325195
Epoch 2450, val loss: 0.4075423777103424
Epoch 2460, training loss: 900.8619995117188 = 0.21405762434005737 + 100.0 * 9.006479263305664
Epoch 2460, val loss: 0.4080060124397278
Epoch 2470, training loss: 901.0458984375 = 0.2134218066930771 + 100.0 * 9.00832462310791
Epoch 2470, val loss: 0.40829744935035706
Epoch 2480, training loss: 901.0552368164062 = 0.21279968321323395 + 100.0 * 9.008423805236816
Epoch 2480, val loss: 0.4093683958053589
Epoch 2490, training loss: 900.9149169921875 = 0.212162584066391 + 100.0 * 9.007027626037598
Epoch 2490, val loss: 0.40935930609703064
Epoch 2500, training loss: 900.8222045898438 = 0.21152882277965546 + 100.0 * 9.00610637664795
Epoch 2500, val loss: 0.41005435585975647
Epoch 2510, training loss: 900.7860107421875 = 0.21090266108512878 + 100.0 * 9.005751609802246
Epoch 2510, val loss: 0.41035598516464233
Epoch 2520, training loss: 900.8250122070312 = 0.21026663482189178 + 100.0 * 9.006147384643555
Epoch 2520, val loss: 0.41064193844795227
Epoch 2530, training loss: 900.9937744140625 = 0.20963801443576813 + 100.0 * 9.007841110229492
Epoch 2530, val loss: 0.4111096262931824
Epoch 2540, training loss: 900.863525390625 = 0.20901842415332794 + 100.0 * 9.006545066833496
Epoch 2540, val loss: 0.4120979309082031
Epoch 2550, training loss: 900.8492431640625 = 0.2083892673254013 + 100.0 * 9.00640869140625
Epoch 2550, val loss: 0.412373423576355
Epoch 2560, training loss: 900.9376831054688 = 0.20776966214179993 + 100.0 * 9.007299423217773
Epoch 2560, val loss: 0.41315215826034546
Epoch 2570, training loss: 900.8002319335938 = 0.20714206993579865 + 100.0 * 9.00593090057373
Epoch 2570, val loss: 0.41340935230255127
Epoch 2580, training loss: 900.811767578125 = 0.2065231204032898 + 100.0 * 9.006052017211914
Epoch 2580, val loss: 0.41371098160743713
Epoch 2590, training loss: 900.9861450195312 = 0.20589542388916016 + 100.0 * 9.007802963256836
Epoch 2590, val loss: 0.4142470359802246
Epoch 2600, training loss: 900.8063354492188 = 0.20530901849269867 + 100.0 * 9.006010055541992
Epoch 2600, val loss: 0.41514623165130615
Epoch 2610, training loss: 900.7288208007812 = 0.20467609167099 + 100.0 * 9.005241394042969
Epoch 2610, val loss: 0.41560348868370056
Epoch 2620, training loss: 900.74755859375 = 0.2040664255619049 + 100.0 * 9.0054349899292
Epoch 2620, val loss: 0.41623759269714355
Epoch 2630, training loss: 900.8156127929688 = 0.2034435123205185 + 100.0 * 9.006121635437012
Epoch 2630, val loss: 0.416648268699646
Epoch 2640, training loss: 900.725830078125 = 0.2028464376926422 + 100.0 * 9.005229949951172
Epoch 2640, val loss: 0.41697293519973755
Epoch 2650, training loss: 900.7437744140625 = 0.2022249549627304 + 100.0 * 9.005415916442871
Epoch 2650, val loss: 0.41761407256126404
Epoch 2660, training loss: 900.7634887695312 = 0.20161931216716766 + 100.0 * 9.005619049072266
Epoch 2660, val loss: 0.4184785485267639
Epoch 2670, training loss: 900.732177734375 = 0.20101816952228546 + 100.0 * 9.005311965942383
Epoch 2670, val loss: 0.4187174439430237
Epoch 2680, training loss: 900.6942749023438 = 0.2004082053899765 + 100.0 * 9.004939079284668
Epoch 2680, val loss: 0.41923901438713074
Epoch 2690, training loss: 900.6648559570312 = 0.19980590045452118 + 100.0 * 9.004650115966797
Epoch 2690, val loss: 0.4199928641319275
Epoch 2700, training loss: 900.9359130859375 = 0.19920915365219116 + 100.0 * 9.007367134094238
Epoch 2700, val loss: 0.42049989104270935
Epoch 2710, training loss: 900.7061767578125 = 0.19861054420471191 + 100.0 * 9.005075454711914
Epoch 2710, val loss: 0.4210328459739685
Epoch 2720, training loss: 900.5899658203125 = 0.198008731007576 + 100.0 * 9.00391960144043
Epoch 2720, val loss: 0.42166590690612793
Epoch 2730, training loss: 900.5697021484375 = 0.1974075883626938 + 100.0 * 9.00372314453125
Epoch 2730, val loss: 0.4222642481327057
Epoch 2740, training loss: 900.5941772460938 = 0.19680534303188324 + 100.0 * 9.003973960876465
Epoch 2740, val loss: 0.422932505607605
Epoch 2750, training loss: 900.8690185546875 = 0.1962110847234726 + 100.0 * 9.006728172302246
Epoch 2750, val loss: 0.4236897826194763
Epoch 2760, training loss: 900.7966918945312 = 0.19562505185604095 + 100.0 * 9.006011009216309
Epoch 2760, val loss: 0.42377984523773193
Epoch 2770, training loss: 900.5999145507812 = 0.19501911103725433 + 100.0 * 9.004049301147461
Epoch 2770, val loss: 0.4247967004776001
Epoch 2780, training loss: 900.5905151367188 = 0.19443942606449127 + 100.0 * 9.003960609436035
Epoch 2780, val loss: 0.42559704184532166
Epoch 2790, training loss: 900.602783203125 = 0.19384123384952545 + 100.0 * 9.00408935546875
Epoch 2790, val loss: 0.42587822675704956
Epoch 2800, training loss: 900.5885620117188 = 0.19324347376823425 + 100.0 * 9.003952980041504
Epoch 2800, val loss: 0.42650437355041504
Epoch 2810, training loss: 900.7103881835938 = 0.1926567554473877 + 100.0 * 9.00517749786377
Epoch 2810, val loss: 0.42740383744239807
Epoch 2820, training loss: 900.644775390625 = 0.19207921624183655 + 100.0 * 9.00452709197998
Epoch 2820, val loss: 0.42790958285331726
Epoch 2830, training loss: 900.546142578125 = 0.19148406386375427 + 100.0 * 9.003546714782715
Epoch 2830, val loss: 0.4284619390964508
Epoch 2840, training loss: 900.551513671875 = 0.19089846312999725 + 100.0 * 9.003605842590332
Epoch 2840, val loss: 0.4288114607334137
Epoch 2850, training loss: 900.5208740234375 = 0.19030886888504028 + 100.0 * 9.003305435180664
Epoch 2850, val loss: 0.4298114478588104
Epoch 2860, training loss: 900.61865234375 = 0.18971753120422363 + 100.0 * 9.004289627075195
Epoch 2860, val loss: 0.4302941858768463
Epoch 2870, training loss: 900.56201171875 = 0.18914015591144562 + 100.0 * 9.003728866577148
Epoch 2870, val loss: 0.43110191822052
Epoch 2880, training loss: 900.5491943359375 = 0.1885712742805481 + 100.0 * 9.003605842590332
Epoch 2880, val loss: 0.4317587912082672
Epoch 2890, training loss: 900.5151977539062 = 0.18799325823783875 + 100.0 * 9.00327205657959
Epoch 2890, val loss: 0.43231573700904846
Epoch 2900, training loss: 900.49462890625 = 0.18740589916706085 + 100.0 * 9.003072738647461
Epoch 2900, val loss: 0.43305420875549316
Epoch 2910, training loss: 900.57861328125 = 0.18683364987373352 + 100.0 * 9.003917694091797
Epoch 2910, val loss: 0.43419361114501953
Epoch 2920, training loss: 900.4389038085938 = 0.18625521659851074 + 100.0 * 9.00252628326416
Epoch 2920, val loss: 0.43459123373031616
Epoch 2930, training loss: 900.4381103515625 = 0.18568140268325806 + 100.0 * 9.002524375915527
Epoch 2930, val loss: 0.43539538979530334
Epoch 2940, training loss: 900.5811767578125 = 0.18510369956493378 + 100.0 * 9.003960609436035
Epoch 2940, val loss: 0.43620744347572327
Epoch 2950, training loss: 900.5127563476562 = 0.1845245510339737 + 100.0 * 9.00328254699707
Epoch 2950, val loss: 0.43635058403015137
Epoch 2960, training loss: 900.456298828125 = 0.18396355211734772 + 100.0 * 9.002723693847656
Epoch 2960, val loss: 0.4374856650829315
Epoch 2970, training loss: 900.48974609375 = 0.18339283764362335 + 100.0 * 9.003063201904297
Epoch 2970, val loss: 0.4380042254924774
Epoch 2980, training loss: 900.437255859375 = 0.18281856179237366 + 100.0 * 9.002544403076172
Epoch 2980, val loss: 0.4387594163417816
Epoch 2990, training loss: 900.3912963867188 = 0.18225452303886414 + 100.0 * 9.002090454101562
Epoch 2990, val loss: 0.4395873248577118
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7414
Flip ASR: 0.6776/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1037.003662109375 = 1.1022145748138428 + 100.0 * 10.359015464782715
Epoch 0, val loss: 1.1015608310699463
Epoch 10, training loss: 1036.6102294921875 = 1.0924092531204224 + 100.0 * 10.355178833007812
Epoch 10, val loss: 1.0914798974990845
Epoch 20, training loss: 1029.956298828125 = 1.07887601852417 + 100.0 * 10.288774490356445
Epoch 20, val loss: 1.0779387950897217
Epoch 30, training loss: 969.6124267578125 = 1.0654973983764648 + 100.0 * 9.685469627380371
Epoch 30, val loss: 1.0646486282348633
Epoch 40, training loss: 957.9424438476562 = 1.0525330305099487 + 100.0 * 9.568899154663086
Epoch 40, val loss: 1.0518437623977661
Epoch 50, training loss: 947.5634155273438 = 1.0405820608139038 + 100.0 * 9.465228080749512
Epoch 50, val loss: 1.0401344299316406
Epoch 60, training loss: 937.8705444335938 = 1.031236171722412 + 100.0 * 9.368392944335938
Epoch 60, val loss: 1.0310783386230469
Epoch 70, training loss: 931.6195678710938 = 1.019600749015808 + 100.0 * 9.305999755859375
Epoch 70, val loss: 1.0196293592453003
Epoch 80, training loss: 925.2311401367188 = 1.0095453262329102 + 100.0 * 9.242216110229492
Epoch 80, val loss: 1.0102146863937378
Epoch 90, training loss: 921.0613403320312 = 1.0030126571655273 + 100.0 * 9.200583457946777
Epoch 90, val loss: 1.0037113428115845
Epoch 100, training loss: 917.9180297851562 = 0.995885968208313 + 100.0 * 9.169220924377441
Epoch 100, val loss: 0.996565043926239
Epoch 110, training loss: 915.8956298828125 = 0.9873033165931702 + 100.0 * 9.149083137512207
Epoch 110, val loss: 0.9883430004119873
Epoch 120, training loss: 914.5718383789062 = 0.9780591130256653 + 100.0 * 9.135937690734863
Epoch 120, val loss: 0.9794586896896362
Epoch 130, training loss: 913.730224609375 = 0.9683057069778442 + 100.0 * 9.127618789672852
Epoch 130, val loss: 0.9698277115821838
Epoch 140, training loss: 912.7083740234375 = 0.9586097598075867 + 100.0 * 9.117497444152832
Epoch 140, val loss: 0.960426390171051
Epoch 150, training loss: 911.8368530273438 = 0.9493826627731323 + 100.0 * 9.108874320983887
Epoch 150, val loss: 0.9516787528991699
Epoch 160, training loss: 911.0797119140625 = 0.9401345252990723 + 100.0 * 9.101395606994629
Epoch 160, val loss: 0.9428616762161255
Epoch 170, training loss: 910.5858764648438 = 0.9300094842910767 + 100.0 * 9.096558570861816
Epoch 170, val loss: 0.9330356121063232
Epoch 180, training loss: 909.9534912109375 = 0.9186216592788696 + 100.0 * 9.090348243713379
Epoch 180, val loss: 0.922106146812439
Epoch 190, training loss: 909.6746826171875 = 0.9065984487533569 + 100.0 * 9.08768081665039
Epoch 190, val loss: 0.9105958342552185
Epoch 200, training loss: 909.1284790039062 = 0.8941751718521118 + 100.0 * 9.082343101501465
Epoch 200, val loss: 0.8987306356430054
Epoch 210, training loss: 908.7572021484375 = 0.8816024661064148 + 100.0 * 9.078756332397461
Epoch 210, val loss: 0.8867980241775513
Epoch 220, training loss: 908.4708251953125 = 0.8685005903244019 + 100.0 * 9.07602310180664
Epoch 220, val loss: 0.8742769956588745
Epoch 230, training loss: 908.2330322265625 = 0.8544538617134094 + 100.0 * 9.073785781860352
Epoch 230, val loss: 0.8608635663986206
Epoch 240, training loss: 907.889404296875 = 0.8405652046203613 + 100.0 * 9.070487976074219
Epoch 240, val loss: 0.8477849960327148
Epoch 250, training loss: 907.5950927734375 = 0.8267557621002197 + 100.0 * 9.067683219909668
Epoch 250, val loss: 0.8347187638282776
Epoch 260, training loss: 907.5534057617188 = 0.8124322295188904 + 100.0 * 9.06740951538086
Epoch 260, val loss: 0.8210579752922058
Epoch 270, training loss: 907.17578125 = 0.7972869873046875 + 100.0 * 9.0637845993042
Epoch 270, val loss: 0.8066670298576355
Epoch 280, training loss: 906.9910888671875 = 0.7823097705841064 + 100.0 * 9.062088012695312
Epoch 280, val loss: 0.7925370335578918
Epoch 290, training loss: 906.8270263671875 = 0.7673695683479309 + 100.0 * 9.060596466064453
Epoch 290, val loss: 0.7784292697906494
Epoch 300, training loss: 906.7740478515625 = 0.7519013285636902 + 100.0 * 9.060221672058105
Epoch 300, val loss: 0.7637985348701477
Epoch 310, training loss: 906.5108032226562 = 0.7360032796859741 + 100.0 * 9.057747840881348
Epoch 310, val loss: 0.7486703991889954
Epoch 320, training loss: 906.347412109375 = 0.7204753756523132 + 100.0 * 9.056269645690918
Epoch 320, val loss: 0.7341406345367432
Epoch 330, training loss: 906.2102661132812 = 0.7052124738693237 + 100.0 * 9.05505084991455
Epoch 330, val loss: 0.7197752594947815
Epoch 340, training loss: 906.131103515625 = 0.6899973750114441 + 100.0 * 9.054410934448242
Epoch 340, val loss: 0.7054652571678162
Epoch 350, training loss: 906.163330078125 = 0.6743626594543457 + 100.0 * 9.054889678955078
Epoch 350, val loss: 0.6908801198005676
Epoch 360, training loss: 905.9256591796875 = 0.659075140953064 + 100.0 * 9.052665710449219
Epoch 360, val loss: 0.6766087412834167
Epoch 370, training loss: 905.769287109375 = 0.6445078253746033 + 100.0 * 9.051247596740723
Epoch 370, val loss: 0.6630642414093018
Epoch 380, training loss: 906.010986328125 = 0.630199670791626 + 100.0 * 9.053808212280273
Epoch 380, val loss: 0.6497946381568909
Epoch 390, training loss: 905.5574340820312 = 0.6159589290618896 + 100.0 * 9.04941463470459
Epoch 390, val loss: 0.636577308177948
Epoch 400, training loss: 905.4403686523438 = 0.6024952530860901 + 100.0 * 9.048378944396973
Epoch 400, val loss: 0.6241487264633179
Epoch 410, training loss: 905.3700561523438 = 0.5896532535552979 + 100.0 * 9.04780387878418
Epoch 410, val loss: 0.612282395362854
Epoch 420, training loss: 905.3916015625 = 0.5769134163856506 + 100.0 * 9.048147201538086
Epoch 420, val loss: 0.6005336046218872
Epoch 430, training loss: 905.2714233398438 = 0.5644291639328003 + 100.0 * 9.047069549560547
Epoch 430, val loss: 0.5891633033752441
Epoch 440, training loss: 905.1197509765625 = 0.5529082417488098 + 100.0 * 9.045668601989746
Epoch 440, val loss: 0.5787072777748108
Epoch 450, training loss: 905.0724487304688 = 0.5419925451278687 + 100.0 * 9.045304298400879
Epoch 450, val loss: 0.5688518285751343
Epoch 460, training loss: 904.931884765625 = 0.5312639474868774 + 100.0 * 9.04400634765625
Epoch 460, val loss: 0.5590111017227173
Epoch 470, training loss: 905.1641235351562 = 0.521016001701355 + 100.0 * 9.046431541442871
Epoch 470, val loss: 0.5496706366539001
Epoch 480, training loss: 904.8341064453125 = 0.5112730264663696 + 100.0 * 9.043228149414062
Epoch 480, val loss: 0.5411460995674133
Epoch 490, training loss: 904.7407836914062 = 0.5022480487823486 + 100.0 * 9.04238510131836
Epoch 490, val loss: 0.5330039262771606
Epoch 500, training loss: 904.6685791015625 = 0.49377021193504333 + 100.0 * 9.041748046875
Epoch 500, val loss: 0.5255299806594849
Epoch 510, training loss: 904.6658935546875 = 0.4853760600090027 + 100.0 * 9.041805267333984
Epoch 510, val loss: 0.517957866191864
Epoch 520, training loss: 904.607666015625 = 0.4773923456668854 + 100.0 * 9.041302680969238
Epoch 520, val loss: 0.511025071144104
Epoch 530, training loss: 904.4667358398438 = 0.47014227509498596 + 100.0 * 9.039965629577637
Epoch 530, val loss: 0.504648745059967
Epoch 540, training loss: 904.578125 = 0.4633021950721741 + 100.0 * 9.04114818572998
Epoch 540, val loss: 0.49861204624176025
Epoch 550, training loss: 904.4778442382812 = 0.4564805030822754 + 100.0 * 9.040213584899902
Epoch 550, val loss: 0.49267107248306274
Epoch 560, training loss: 904.2804565429688 = 0.4502077102661133 + 100.0 * 9.038302421569824
Epoch 560, val loss: 0.48727622628211975
Epoch 570, training loss: 904.2365112304688 = 0.4444027543067932 + 100.0 * 9.037920951843262
Epoch 570, val loss: 0.48231756687164307
Epoch 580, training loss: 904.6605224609375 = 0.4388068616390228 + 100.0 * 9.042217254638672
Epoch 580, val loss: 0.4774070680141449
Epoch 590, training loss: 904.19189453125 = 0.43318697810173035 + 100.0 * 9.03758716583252
Epoch 590, val loss: 0.47287461161613464
Epoch 600, training loss: 904.111572265625 = 0.42808976769447327 + 100.0 * 9.036834716796875
Epoch 600, val loss: 0.4686090052127838
Epoch 610, training loss: 904.0286865234375 = 0.4234130084514618 + 100.0 * 9.036052703857422
Epoch 610, val loss: 0.46470633149147034
Epoch 620, training loss: 903.9844360351562 = 0.41898229718208313 + 100.0 * 9.035654067993164
Epoch 620, val loss: 0.4609789252281189
Epoch 630, training loss: 904.496826171875 = 0.4145588278770447 + 100.0 * 9.040822982788086
Epoch 630, val loss: 0.45715242624282837
Epoch 640, training loss: 903.9500122070312 = 0.4101344645023346 + 100.0 * 9.035398483276367
Epoch 640, val loss: 0.45369917154312134
Epoch 650, training loss: 903.8319702148438 = 0.40625518560409546 + 100.0 * 9.034256935119629
Epoch 650, val loss: 0.45069020986557007
Epoch 660, training loss: 903.7913208007812 = 0.40262189507484436 + 100.0 * 9.033886909484863
Epoch 660, val loss: 0.44784781336784363
Epoch 670, training loss: 903.8567504882812 = 0.39912599325180054 + 100.0 * 9.034576416015625
Epoch 670, val loss: 0.4452192485332489
Epoch 680, training loss: 903.89111328125 = 0.3956297039985657 + 100.0 * 9.034955024719238
Epoch 680, val loss: 0.44240573048591614
Epoch 690, training loss: 903.7389526367188 = 0.3922221064567566 + 100.0 * 9.033467292785645
Epoch 690, val loss: 0.43967777490615845
Epoch 700, training loss: 903.6356811523438 = 0.38917285203933716 + 100.0 * 9.032464981079102
Epoch 700, val loss: 0.43742889165878296
Epoch 710, training loss: 903.7328491210938 = 0.3862137198448181 + 100.0 * 9.033466339111328
Epoch 710, val loss: 0.4349597990512848
Epoch 720, training loss: 903.6131591796875 = 0.3832017779350281 + 100.0 * 9.032299995422363
Epoch 720, val loss: 0.4331496059894562
Epoch 730, training loss: 903.5455322265625 = 0.38046345114707947 + 100.0 * 9.03165054321289
Epoch 730, val loss: 0.43091708421707153
Epoch 740, training loss: 903.5194702148438 = 0.37784457206726074 + 100.0 * 9.031415939331055
Epoch 740, val loss: 0.4291934072971344
Epoch 750, training loss: 903.5901489257812 = 0.375222384929657 + 100.0 * 9.032149314880371
Epoch 750, val loss: 0.4272235631942749
Epoch 760, training loss: 903.4200439453125 = 0.3727053999900818 + 100.0 * 9.030473709106445
Epoch 760, val loss: 0.42559224367141724
Epoch 770, training loss: 903.3775634765625 = 0.3703498840332031 + 100.0 * 9.030072212219238
Epoch 770, val loss: 0.4239457845687866
Epoch 780, training loss: 903.37353515625 = 0.36807137727737427 + 100.0 * 9.030055046081543
Epoch 780, val loss: 0.42242103815078735
Epoch 790, training loss: 903.4462890625 = 0.3658246397972107 + 100.0 * 9.030804634094238
Epoch 790, val loss: 0.42104682326316833
Epoch 800, training loss: 903.583740234375 = 0.36358100175857544 + 100.0 * 9.032201766967773
Epoch 800, val loss: 0.41950610280036926
Epoch 810, training loss: 903.2387084960938 = 0.36136046051979065 + 100.0 * 9.028773307800293
Epoch 810, val loss: 0.41810718178749084
Epoch 820, training loss: 903.2355346679688 = 0.35934606194496155 + 100.0 * 9.028761863708496
Epoch 820, val loss: 0.41680124402046204
Epoch 830, training loss: 903.34814453125 = 0.357409805059433 + 100.0 * 9.0299072265625
Epoch 830, val loss: 0.41568559408187866
Epoch 840, training loss: 903.220703125 = 0.355409175157547 + 100.0 * 9.028653144836426
Epoch 840, val loss: 0.4144006669521332
Epoch 850, training loss: 903.1458740234375 = 0.3535363972187042 + 100.0 * 9.027923583984375
Epoch 850, val loss: 0.41336533427238464
Epoch 860, training loss: 903.0778198242188 = 0.3517531156539917 + 100.0 * 9.027260780334473
Epoch 860, val loss: 0.4122960865497589
Epoch 870, training loss: 903.1814575195312 = 0.34999993443489075 + 100.0 * 9.028314590454102
Epoch 870, val loss: 0.4111727774143219
Epoch 880, training loss: 903.0254516601562 = 0.3481946885585785 + 100.0 * 9.026772499084473
Epoch 880, val loss: 0.41047677397727966
Epoch 890, training loss: 903.0114135742188 = 0.3464938700199127 + 100.0 * 9.026649475097656
Epoch 890, val loss: 0.40936920046806335
Epoch 900, training loss: 903.0341186523438 = 0.34486809372901917 + 100.0 * 9.02689266204834
Epoch 900, val loss: 0.40879368782043457
Epoch 910, training loss: 903.0201416015625 = 0.34321215748786926 + 100.0 * 9.026769638061523
Epoch 910, val loss: 0.40769681334495544
Epoch 920, training loss: 903.1580200195312 = 0.34164372086524963 + 100.0 * 9.02816390991211
Epoch 920, val loss: 0.40661174058914185
Epoch 930, training loss: 902.9608154296875 = 0.3400043845176697 + 100.0 * 9.02620792388916
Epoch 930, val loss: 0.40631628036499023
Epoch 940, training loss: 902.8809814453125 = 0.3385087847709656 + 100.0 * 9.02542495727539
Epoch 940, val loss: 0.4052213132381439
Epoch 950, training loss: 902.885986328125 = 0.3370262682437897 + 100.0 * 9.025489807128906
Epoch 950, val loss: 0.4046156704425812
Epoch 960, training loss: 902.8470458984375 = 0.3355724513530731 + 100.0 * 9.025115013122559
Epoch 960, val loss: 0.4040335416793823
Epoch 970, training loss: 902.781494140625 = 0.3341711461544037 + 100.0 * 9.024473190307617
Epoch 970, val loss: 0.4034808576107025
Epoch 980, training loss: 902.9403686523438 = 0.33277443051338196 + 100.0 * 9.026076316833496
Epoch 980, val loss: 0.4029111862182617
Epoch 990, training loss: 902.9046020507812 = 0.33131104707717896 + 100.0 * 9.02573299407959
Epoch 990, val loss: 0.40203964710235596
Epoch 1000, training loss: 902.7854614257812 = 0.32996904850006104 + 100.0 * 9.024555206298828
Epoch 1000, val loss: 0.4016641080379486
Epoch 1010, training loss: 902.7714233398438 = 0.32860979437828064 + 100.0 * 9.024428367614746
Epoch 1010, val loss: 0.40092241764068604
Epoch 1020, training loss: 902.779541015625 = 0.32728299498558044 + 100.0 * 9.02452278137207
Epoch 1020, val loss: 0.4003607928752899
Epoch 1030, training loss: 902.6258544921875 = 0.3259924650192261 + 100.0 * 9.022998809814453
Epoch 1030, val loss: 0.40002012252807617
Epoch 1040, training loss: 902.5986328125 = 0.3247164785861969 + 100.0 * 9.02273941040039
Epoch 1040, val loss: 0.3994273841381073
Epoch 1050, training loss: 902.6068725585938 = 0.3234749138355255 + 100.0 * 9.022833824157715
Epoch 1050, val loss: 0.39892810583114624
Epoch 1060, training loss: 902.8110961914062 = 0.32219618558883667 + 100.0 * 9.02488899230957
Epoch 1060, val loss: 0.398468554019928
Epoch 1070, training loss: 902.54833984375 = 0.3209294378757477 + 100.0 * 9.022274017333984
Epoch 1070, val loss: 0.398080974817276
Epoch 1080, training loss: 902.5350341796875 = 0.3197456896305084 + 100.0 * 9.0221529006958
Epoch 1080, val loss: 0.39766350388526917
Epoch 1090, training loss: 902.5001220703125 = 0.31857460737228394 + 100.0 * 9.021815299987793
Epoch 1090, val loss: 0.397314190864563
Epoch 1100, training loss: 902.711181640625 = 0.31738024950027466 + 100.0 * 9.023938179016113
Epoch 1100, val loss: 0.39684000611305237
Epoch 1110, training loss: 902.5314331054688 = 0.316193163394928 + 100.0 * 9.022151947021484
Epoch 1110, val loss: 0.3965076804161072
Epoch 1120, training loss: 902.4506225585938 = 0.31505078077316284 + 100.0 * 9.021355628967285
Epoch 1120, val loss: 0.3961099684238434
Epoch 1130, training loss: 902.5442504882812 = 0.31391817331314087 + 100.0 * 9.022303581237793
Epoch 1130, val loss: 0.39566144347190857
Epoch 1140, training loss: 902.4042358398438 = 0.31275832653045654 + 100.0 * 9.020915031433105
Epoch 1140, val loss: 0.3954978585243225
Epoch 1150, training loss: 902.5550537109375 = 0.3116599917411804 + 100.0 * 9.02243423461914
Epoch 1150, val loss: 0.3954053819179535
Epoch 1160, training loss: 902.441162109375 = 0.310509592294693 + 100.0 * 9.021306991577148
Epoch 1160, val loss: 0.394915908575058
Epoch 1170, training loss: 902.3460083007812 = 0.3094261884689331 + 100.0 * 9.020365715026855
Epoch 1170, val loss: 0.3944553732872009
Epoch 1180, training loss: 902.2948608398438 = 0.3083588778972626 + 100.0 * 9.019865036010742
Epoch 1180, val loss: 0.3942405581474304
Epoch 1190, training loss: 902.273681640625 = 0.3073108196258545 + 100.0 * 9.01966381072998
Epoch 1190, val loss: 0.393907755613327
Epoch 1200, training loss: 902.56640625 = 0.30624786019325256 + 100.0 * 9.022601127624512
Epoch 1200, val loss: 0.39348381757736206
Epoch 1210, training loss: 902.4036254882812 = 0.30516478419303894 + 100.0 * 9.020984649658203
Epoch 1210, val loss: 0.3933623433113098
Epoch 1220, training loss: 902.31640625 = 0.3041307330131531 + 100.0 * 9.020122528076172
Epoch 1220, val loss: 0.3934228718280792
Epoch 1230, training loss: 902.2391357421875 = 0.3030770421028137 + 100.0 * 9.019360542297363
Epoch 1230, val loss: 0.39296916127204895
Epoch 1240, training loss: 902.2740478515625 = 0.3020597994327545 + 100.0 * 9.019720077514648
Epoch 1240, val loss: 0.39271754026412964
Epoch 1250, training loss: 902.158203125 = 0.30106472969055176 + 100.0 * 9.018570899963379
Epoch 1250, val loss: 0.3926912248134613
Epoch 1260, training loss: 902.1234741210938 = 0.30006998777389526 + 100.0 * 9.018234252929688
Epoch 1260, val loss: 0.3922225534915924
Epoch 1270, training loss: 902.273681640625 = 0.29907870292663574 + 100.0 * 9.019745826721191
Epoch 1270, val loss: 0.39204341173171997
Epoch 1280, training loss: 902.2484741210938 = 0.2980618476867676 + 100.0 * 9.01950454711914
Epoch 1280, val loss: 0.39233848452568054
Epoch 1290, training loss: 902.14697265625 = 0.2970704138278961 + 100.0 * 9.018499374389648
Epoch 1290, val loss: 0.3915240466594696
Epoch 1300, training loss: 902.0668334960938 = 0.29610002040863037 + 100.0 * 9.017707824707031
Epoch 1300, val loss: 0.3915921151638031
Epoch 1310, training loss: 902.1199951171875 = 0.2951365113258362 + 100.0 * 9.018248558044434
Epoch 1310, val loss: 0.39121267199516296
Epoch 1320, training loss: 902.0941772460938 = 0.29415163397789 + 100.0 * 9.018000602722168
Epoch 1320, val loss: 0.39113372564315796
Epoch 1330, training loss: 902.013916015625 = 0.29320788383483887 + 100.0 * 9.017207145690918
Epoch 1330, val loss: 0.39097297191619873
Epoch 1340, training loss: 902.0219116210938 = 0.2922608256340027 + 100.0 * 9.01729679107666
Epoch 1340, val loss: 0.3909662663936615
Epoch 1350, training loss: 902.1286010742188 = 0.2913188636302948 + 100.0 * 9.018372535705566
Epoch 1350, val loss: 0.3909071981906891
Epoch 1360, training loss: 902.049560546875 = 0.29036176204681396 + 100.0 * 9.017592430114746
Epoch 1360, val loss: 0.39066800475120544
Epoch 1370, training loss: 901.9868774414062 = 0.2894277274608612 + 100.0 * 9.016974449157715
Epoch 1370, val loss: 0.39038288593292236
Epoch 1380, training loss: 902.05224609375 = 0.2885061502456665 + 100.0 * 9.017637252807617
Epoch 1380, val loss: 0.3903530240058899
Epoch 1390, training loss: 901.8978881835938 = 0.28759098052978516 + 100.0 * 9.01610279083252
Epoch 1390, val loss: 0.3900589048862457
Epoch 1400, training loss: 901.8605346679688 = 0.2866862714290619 + 100.0 * 9.015738487243652
Epoch 1400, val loss: 0.3900740146636963
Epoch 1410, training loss: 901.8670654296875 = 0.2857888340950012 + 100.0 * 9.015812873840332
Epoch 1410, val loss: 0.3900505304336548
Epoch 1420, training loss: 902.1178588867188 = 0.28489550948143005 + 100.0 * 9.018329620361328
Epoch 1420, val loss: 0.39010345935821533
Epoch 1430, training loss: 901.9688720703125 = 0.28397512435913086 + 100.0 * 9.01684856414795
Epoch 1430, val loss: 0.3893924355506897
Epoch 1440, training loss: 901.8604736328125 = 0.2830793559551239 + 100.0 * 9.01577377319336
Epoch 1440, val loss: 0.3898463845252991
Epoch 1450, training loss: 901.8179321289062 = 0.28218546509742737 + 100.0 * 9.015357971191406
Epoch 1450, val loss: 0.3894306719303131
Epoch 1460, training loss: 902.0009765625 = 0.2813100516796112 + 100.0 * 9.017196655273438
Epoch 1460, val loss: 0.38947251439094543
Epoch 1470, training loss: 901.9449462890625 = 0.2804253101348877 + 100.0 * 9.016645431518555
Epoch 1470, val loss: 0.3892498016357422
Epoch 1480, training loss: 901.786376953125 = 0.279544860124588 + 100.0 * 9.015068054199219
Epoch 1480, val loss: 0.38922885060310364
Epoch 1490, training loss: 901.74169921875 = 0.2786974608898163 + 100.0 * 9.014630317687988
Epoch 1490, val loss: 0.38921236991882324
Epoch 1500, training loss: 901.7366943359375 = 0.27783775329589844 + 100.0 * 9.014588356018066
Epoch 1500, val loss: 0.38893070816993713
Epoch 1510, training loss: 901.9908447265625 = 0.2769681215286255 + 100.0 * 9.017138481140137
Epoch 1510, val loss: 0.3888327181339264
Epoch 1520, training loss: 901.7433471679688 = 0.27613672614097595 + 100.0 * 9.01467227935791
Epoch 1520, val loss: 0.3892586827278137
Epoch 1530, training loss: 901.6807250976562 = 0.2752757966518402 + 100.0 * 9.014054298400879
Epoch 1530, val loss: 0.3889724016189575
Epoch 1540, training loss: 901.8199462890625 = 0.2744350731372833 + 100.0 * 9.01545524597168
Epoch 1540, val loss: 0.3890383839607239
Epoch 1550, training loss: 901.6635131835938 = 0.2735883891582489 + 100.0 * 9.013899803161621
Epoch 1550, val loss: 0.3890177607536316
Epoch 1560, training loss: 901.7701416015625 = 0.27276062965393066 + 100.0 * 9.014973640441895
Epoch 1560, val loss: 0.38883665204048157
Epoch 1570, training loss: 901.6514282226562 = 0.2719349265098572 + 100.0 * 9.013794898986816
Epoch 1570, val loss: 0.3889496326446533
