Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.300893783569336 = 1.9271517992019653 + 1.0 * 8.37374210357666
Epoch 0, val loss: 1.926690936088562
Epoch 10, training loss: 10.290312767028809 = 1.9172868728637695 + 1.0 * 8.373025894165039
Epoch 10, val loss: 1.9176452159881592
Epoch 20, training loss: 10.273138046264648 = 1.904914379119873 + 1.0 * 8.368224143981934
Epoch 20, val loss: 1.9058105945587158
Epoch 30, training loss: 10.222384452819824 = 1.8876333236694336 + 1.0 * 8.33475112915039
Epoch 30, val loss: 1.8889743089675903
Epoch 40, training loss: 9.985101699829102 = 1.8662352561950684 + 1.0 * 8.118865966796875
Epoch 40, val loss: 1.8686535358428955
Epoch 50, training loss: 9.401519775390625 = 1.843599796295166 + 1.0 * 7.557920455932617
Epoch 50, val loss: 1.8474197387695312
Epoch 60, training loss: 9.031747817993164 = 1.8265928030014038 + 1.0 * 7.205155372619629
Epoch 60, val loss: 1.8319512605667114
Epoch 70, training loss: 8.63465690612793 = 1.8162753582000732 + 1.0 * 6.8183817863464355
Epoch 70, val loss: 1.8234784603118896
Epoch 80, training loss: 8.432795524597168 = 1.8091715574264526 + 1.0 * 6.623623847961426
Epoch 80, val loss: 1.8170289993286133
Epoch 90, training loss: 8.318926811218262 = 1.799748182296753 + 1.0 * 6.519178867340088
Epoch 90, val loss: 1.8078685998916626
Epoch 100, training loss: 8.221827507019043 = 1.788644790649414 + 1.0 * 6.433182716369629
Epoch 100, val loss: 1.7974345684051514
Epoch 110, training loss: 8.147425651550293 = 1.7779663801193237 + 1.0 * 6.369459629058838
Epoch 110, val loss: 1.7876304388046265
Epoch 120, training loss: 8.088858604431152 = 1.7672337293624878 + 1.0 * 6.321625232696533
Epoch 120, val loss: 1.777923345565796
Epoch 130, training loss: 8.043390274047852 = 1.755037784576416 + 1.0 * 6.288352966308594
Epoch 130, val loss: 1.7672936916351318
Epoch 140, training loss: 8.001646995544434 = 1.7406480312347412 + 1.0 * 6.2609992027282715
Epoch 140, val loss: 1.7551963329315186
Epoch 150, training loss: 7.962985992431641 = 1.7234801054000854 + 1.0 * 6.239505767822266
Epoch 150, val loss: 1.7411755323410034
Epoch 160, training loss: 7.923623085021973 = 1.7029101848602295 + 1.0 * 6.220712661743164
Epoch 160, val loss: 1.7246503829956055
Epoch 170, training loss: 7.88173770904541 = 1.678145408630371 + 1.0 * 6.203592300415039
Epoch 170, val loss: 1.7048747539520264
Epoch 180, training loss: 7.837646961212158 = 1.648037075996399 + 1.0 * 6.189610004425049
Epoch 180, val loss: 1.680922269821167
Epoch 190, training loss: 7.789874076843262 = 1.6117689609527588 + 1.0 * 6.178105354309082
Epoch 190, val loss: 1.6520158052444458
Epoch 200, training loss: 7.736141681671143 = 1.5686821937561035 + 1.0 * 6.167459487915039
Epoch 200, val loss: 1.6175686120986938
Epoch 210, training loss: 7.677091598510742 = 1.5182716846466064 + 1.0 * 6.158819675445557
Epoch 210, val loss: 1.577477216720581
Epoch 220, training loss: 7.615031719207764 = 1.4615215063095093 + 1.0 * 6.153510093688965
Epoch 220, val loss: 1.5326652526855469
Epoch 230, training loss: 7.547508239746094 = 1.400822401046753 + 1.0 * 6.14668607711792
Epoch 230, val loss: 1.4855992794036865
Epoch 240, training loss: 7.4787068367004395 = 1.3376940488815308 + 1.0 * 6.141012668609619
Epoch 240, val loss: 1.4374516010284424
Epoch 250, training loss: 7.417885780334473 = 1.2736631631851196 + 1.0 * 6.144222736358643
Epoch 250, val loss: 1.389968991279602
Epoch 260, training loss: 7.344766616821289 = 1.2120685577392578 + 1.0 * 6.132698059082031
Epoch 260, val loss: 1.345246434211731
Epoch 270, training loss: 7.280265808105469 = 1.1521519422531128 + 1.0 * 6.128113746643066
Epoch 270, val loss: 1.3024765253067017
Epoch 280, training loss: 7.22090482711792 = 1.0936976671218872 + 1.0 * 6.127207279205322
Epoch 280, val loss: 1.2610236406326294
Epoch 290, training loss: 7.160030841827393 = 1.0378762483596802 + 1.0 * 6.122154712677002
Epoch 290, val loss: 1.2216464281082153
Epoch 300, training loss: 7.103151798248291 = 0.9845763444900513 + 1.0 * 6.118575572967529
Epoch 300, val loss: 1.18410325050354
Epoch 310, training loss: 7.047881126403809 = 0.9328183531761169 + 1.0 * 6.115062713623047
Epoch 310, val loss: 1.1475600004196167
Epoch 320, training loss: 7.001193046569824 = 0.8825094699859619 + 1.0 * 6.118683338165283
Epoch 320, val loss: 1.1119881868362427
Epoch 330, training loss: 6.945262432098389 = 0.8341656923294067 + 1.0 * 6.1110968589782715
Epoch 330, val loss: 1.0780119895935059
Epoch 340, training loss: 6.893744468688965 = 0.7872577905654907 + 1.0 * 6.106486797332764
Epoch 340, val loss: 1.0452922582626343
Epoch 350, training loss: 6.850811958312988 = 0.7416565418243408 + 1.0 * 6.109155654907227
Epoch 350, val loss: 1.013822317123413
Epoch 360, training loss: 6.802832126617432 = 0.6984752416610718 + 1.0 * 6.10435676574707
Epoch 360, val loss: 0.9845411777496338
Epoch 370, training loss: 6.7565836906433105 = 0.6576128602027893 + 1.0 * 6.098970890045166
Epoch 370, val loss: 0.9577852487564087
Epoch 380, training loss: 6.719278335571289 = 0.6188124418258667 + 1.0 * 6.100465774536133
Epoch 380, val loss: 0.9332503080368042
Epoch 390, training loss: 6.676999092102051 = 0.5829440355300903 + 1.0 * 6.09405517578125
Epoch 390, val loss: 0.9116482138633728
Epoch 400, training loss: 6.643274784088135 = 0.5497751235961914 + 1.0 * 6.093499660491943
Epoch 400, val loss: 0.8932961225509644
Epoch 410, training loss: 6.608389854431152 = 0.5187884569168091 + 1.0 * 6.089601516723633
Epoch 410, val loss: 0.8773592710494995
Epoch 420, training loss: 6.585444450378418 = 0.4897884726524353 + 1.0 * 6.095655918121338
Epoch 420, val loss: 0.8639271259307861
Epoch 430, training loss: 6.548941135406494 = 0.4629850387573242 + 1.0 * 6.08595609664917
Epoch 430, val loss: 0.8528826236724854
Epoch 440, training loss: 6.52271032333374 = 0.43798506259918213 + 1.0 * 6.084725379943848
Epoch 440, val loss: 0.8441765308380127
Epoch 450, training loss: 6.498871326446533 = 0.41444453597068787 + 1.0 * 6.0844268798828125
Epoch 450, val loss: 0.8370648622512817
Epoch 460, training loss: 6.474309921264648 = 0.3922610580921173 + 1.0 * 6.0820488929748535
Epoch 460, val loss: 0.8316570520401001
Epoch 470, training loss: 6.449710369110107 = 0.371291846036911 + 1.0 * 6.078418731689453
Epoch 470, val loss: 0.8276116251945496
Epoch 480, training loss: 6.429128646850586 = 0.3513566553592682 + 1.0 * 6.07777214050293
Epoch 480, val loss: 0.8248162865638733
Epoch 490, training loss: 6.410028457641602 = 0.33247998356819153 + 1.0 * 6.077548503875732
Epoch 490, val loss: 0.8229604959487915
Epoch 500, training loss: 6.396127223968506 = 0.3146023154258728 + 1.0 * 6.081524848937988
Epoch 500, val loss: 0.822085440158844
Epoch 510, training loss: 6.371026515960693 = 0.2977355122566223 + 1.0 * 6.073290824890137
Epoch 510, val loss: 0.821976900100708
Epoch 520, training loss: 6.351670265197754 = 0.2816219627857208 + 1.0 * 6.0700483322143555
Epoch 520, val loss: 0.8226318359375
Epoch 530, training loss: 6.336841106414795 = 0.2662072479724884 + 1.0 * 6.070633888244629
Epoch 530, val loss: 0.8238804340362549
Epoch 540, training loss: 6.325798034667969 = 0.25152716040611267 + 1.0 * 6.074270725250244
Epoch 540, val loss: 0.825711727142334
Epoch 550, training loss: 6.304437160491943 = 0.2376570999622345 + 1.0 * 6.066780090332031
Epoch 550, val loss: 0.8282106518745422
Epoch 560, training loss: 6.290958404541016 = 0.22441799938678741 + 1.0 * 6.066540241241455
Epoch 560, val loss: 0.8311837315559387
Epoch 570, training loss: 6.276653289794922 = 0.21186964213848114 + 1.0 * 6.064783573150635
Epoch 570, val loss: 0.8345410227775574
Epoch 580, training loss: 6.263058662414551 = 0.2000146359205246 + 1.0 * 6.06304407119751
Epoch 580, val loss: 0.8385676741600037
Epoch 590, training loss: 6.257941246032715 = 0.18878352642059326 + 1.0 * 6.069157600402832
Epoch 590, val loss: 0.8428479433059692
Epoch 600, training loss: 6.240822792053223 = 0.1782311350107193 + 1.0 * 6.062591552734375
Epoch 600, val loss: 0.8474780321121216
Epoch 610, training loss: 6.22597599029541 = 0.16831862926483154 + 1.0 * 6.057657241821289
Epoch 610, val loss: 0.852618396282196
Epoch 620, training loss: 6.2160420417785645 = 0.1589650809764862 + 1.0 * 6.057076930999756
Epoch 620, val loss: 0.8579676747322083
Epoch 630, training loss: 6.2085723876953125 = 0.1501782387495041 + 1.0 * 6.058393955230713
Epoch 630, val loss: 0.8635215163230896
Epoch 640, training loss: 6.200825214385986 = 0.14197973906993866 + 1.0 * 6.058845520019531
Epoch 640, val loss: 0.8693994879722595
Epoch 650, training loss: 6.189063549041748 = 0.13430902361869812 + 1.0 * 6.054754734039307
Epoch 650, val loss: 0.875319242477417
Epoch 660, training loss: 6.17975378036499 = 0.12711380422115326 + 1.0 * 6.052639961242676
Epoch 660, val loss: 0.8814810514450073
Epoch 670, training loss: 6.174960136413574 = 0.12035050988197327 + 1.0 * 6.054609775543213
Epoch 670, val loss: 0.8878011107444763
Epoch 680, training loss: 6.1644439697265625 = 0.11400193721055984 + 1.0 * 6.050442218780518
Epoch 680, val loss: 0.8941500186920166
Epoch 690, training loss: 6.162220478057861 = 0.10806506872177124 + 1.0 * 6.054155349731445
Epoch 690, val loss: 0.9004250764846802
Epoch 700, training loss: 6.15151309967041 = 0.10254819691181183 + 1.0 * 6.048964977264404
Epoch 700, val loss: 0.9068165421485901
Epoch 710, training loss: 6.143892288208008 = 0.09737363457679749 + 1.0 * 6.046518802642822
Epoch 710, val loss: 0.9133507609367371
Epoch 720, training loss: 6.141146183013916 = 0.09249420464038849 + 1.0 * 6.048652172088623
Epoch 720, val loss: 0.9198399782180786
Epoch 730, training loss: 6.136098384857178 = 0.08791113644838333 + 1.0 * 6.048187255859375
Epoch 730, val loss: 0.9263355135917664
Epoch 740, training loss: 6.128814697265625 = 0.08362351357936859 + 1.0 * 6.045191287994385
Epoch 740, val loss: 0.9328854084014893
Epoch 750, training loss: 6.12235689163208 = 0.07958335429430008 + 1.0 * 6.042773723602295
Epoch 750, val loss: 0.9394180774688721
Epoch 760, training loss: 6.130634784698486 = 0.07577621936798096 + 1.0 * 6.054858684539795
Epoch 760, val loss: 0.9459153413772583
Epoch 770, training loss: 6.11708927154541 = 0.07219994813203812 + 1.0 * 6.044889450073242
Epoch 770, val loss: 0.9523899555206299
Epoch 780, training loss: 6.108951568603516 = 0.06885407865047455 + 1.0 * 6.040097713470459
Epoch 780, val loss: 0.9590181708335876
Epoch 790, training loss: 6.1058125495910645 = 0.06568882614374161 + 1.0 * 6.04012393951416
Epoch 790, val loss: 0.9655399918556213
Epoch 800, training loss: 6.107199192047119 = 0.06270209699869156 + 1.0 * 6.044497013092041
Epoch 800, val loss: 0.9719939231872559
Epoch 810, training loss: 6.098251819610596 = 0.05990070477128029 + 1.0 * 6.038351058959961
Epoch 810, val loss: 0.9784805774688721
Epoch 820, training loss: 6.095897674560547 = 0.057264696806669235 + 1.0 * 6.038632869720459
Epoch 820, val loss: 0.9849661588668823
Epoch 830, training loss: 6.094780445098877 = 0.0547824464738369 + 1.0 * 6.0399980545043945
Epoch 830, val loss: 0.9913042783737183
Epoch 840, training loss: 6.089369297027588 = 0.05245039612054825 + 1.0 * 6.036919116973877
Epoch 840, val loss: 0.9975531697273254
Epoch 850, training loss: 6.08512020111084 = 0.05025540292263031 + 1.0 * 6.034864902496338
Epoch 850, val loss: 1.0038813352584839
Epoch 860, training loss: 6.081815719604492 = 0.04818045720458031 + 1.0 * 6.033635139465332
Epoch 860, val loss: 1.0101393461227417
Epoch 870, training loss: 6.08225154876709 = 0.04621141403913498 + 1.0 * 6.036040306091309
Epoch 870, val loss: 1.0163767337799072
Epoch 880, training loss: 6.088132858276367 = 0.04435489699244499 + 1.0 * 6.043777942657471
Epoch 880, val loss: 1.0222835540771484
Epoch 890, training loss: 6.075585842132568 = 0.04262165352702141 + 1.0 * 6.03296422958374
Epoch 890, val loss: 1.0282516479492188
Epoch 900, training loss: 6.072876930236816 = 0.04098911210894585 + 1.0 * 6.031888008117676
Epoch 900, val loss: 1.0343354940414429
Epoch 910, training loss: 6.069336414337158 = 0.03943675011396408 + 1.0 * 6.029899597167969
Epoch 910, val loss: 1.0402058362960815
Epoch 920, training loss: 6.067623615264893 = 0.03796001151204109 + 1.0 * 6.029663562774658
Epoch 920, val loss: 1.0460807085037231
Epoch 930, training loss: 6.072418212890625 = 0.03655760735273361 + 1.0 * 6.035860538482666
Epoch 930, val loss: 1.0517234802246094
Epoch 940, training loss: 6.065393447875977 = 0.03523920848965645 + 1.0 * 6.030154228210449
Epoch 940, val loss: 1.0573700666427612
Epoch 950, training loss: 6.064696788787842 = 0.033993951976299286 + 1.0 * 6.030703067779541
Epoch 950, val loss: 1.0630875825881958
Epoch 960, training loss: 6.0610857009887695 = 0.03281119465827942 + 1.0 * 6.0282745361328125
Epoch 960, val loss: 1.0684268474578857
Epoch 970, training loss: 6.057946681976318 = 0.031691793352365494 + 1.0 * 6.026254653930664
Epoch 970, val loss: 1.0739519596099854
Epoch 980, training loss: 6.055802345275879 = 0.030627159401774406 + 1.0 * 6.025175094604492
Epoch 980, val loss: 1.079477310180664
Epoch 990, training loss: 6.057398796081543 = 0.029608450829982758 + 1.0 * 6.027790546417236
Epoch 990, val loss: 1.0848584175109863
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6790
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.306832313537598 = 1.9330403804779053 + 1.0 * 8.373791694641113
Epoch 0, val loss: 1.9357428550720215
Epoch 10, training loss: 10.296418190002441 = 1.923331618309021 + 1.0 * 8.373086929321289
Epoch 10, val loss: 1.925761103630066
Epoch 20, training loss: 10.279484748840332 = 1.9110132455825806 + 1.0 * 8.368471145629883
Epoch 20, val loss: 1.9123777151107788
Epoch 30, training loss: 10.232915878295898 = 1.894054651260376 + 1.0 * 8.338861465454102
Epoch 30, val loss: 1.8935855627059937
Epoch 40, training loss: 10.029234886169434 = 1.8732824325561523 + 1.0 * 8.155952453613281
Epoch 40, val loss: 1.871252179145813
Epoch 50, training loss: 9.44854736328125 = 1.8497310876846313 + 1.0 * 7.59881591796875
Epoch 50, val loss: 1.8462753295898438
Epoch 60, training loss: 9.011493682861328 = 1.831284999847412 + 1.0 * 7.180209159851074
Epoch 60, val loss: 1.8283368349075317
Epoch 70, training loss: 8.634298324584961 = 1.8187553882598877 + 1.0 * 6.815542697906494
Epoch 70, val loss: 1.815672755241394
Epoch 80, training loss: 8.466623306274414 = 1.8062406778335571 + 1.0 * 6.660382270812988
Epoch 80, val loss: 1.8026432991027832
Epoch 90, training loss: 8.36717700958252 = 1.792937994003296 + 1.0 * 6.574239253997803
Epoch 90, val loss: 1.7891966104507446
Epoch 100, training loss: 8.284463882446289 = 1.7800097465515137 + 1.0 * 6.504454612731934
Epoch 100, val loss: 1.7764688730239868
Epoch 110, training loss: 8.215702056884766 = 1.7679225206375122 + 1.0 * 6.447779178619385
Epoch 110, val loss: 1.7647370100021362
Epoch 120, training loss: 8.146748542785645 = 1.7553588151931763 + 1.0 * 6.3913893699646
Epoch 120, val loss: 1.7529046535491943
Epoch 130, training loss: 8.08318042755127 = 1.741315245628357 + 1.0 * 6.341865062713623
Epoch 130, val loss: 1.7403727769851685
Epoch 140, training loss: 8.031926155090332 = 1.7245393991470337 + 1.0 * 6.307386875152588
Epoch 140, val loss: 1.7255803346633911
Epoch 150, training loss: 7.982884407043457 = 1.7037804126739502 + 1.0 * 6.279103755950928
Epoch 150, val loss: 1.7076565027236938
Epoch 160, training loss: 7.933995246887207 = 1.678572654724121 + 1.0 * 6.255422592163086
Epoch 160, val loss: 1.6861408948898315
Epoch 170, training loss: 7.8873701095581055 = 1.6477967500686646 + 1.0 * 6.2395734786987305
Epoch 170, val loss: 1.6602015495300293
Epoch 180, training loss: 7.834290504455566 = 1.6111575365066528 + 1.0 * 6.223133087158203
Epoch 180, val loss: 1.6297416687011719
Epoch 190, training loss: 7.778570175170898 = 1.567653775215149 + 1.0 * 6.210916519165039
Epoch 190, val loss: 1.5939583778381348
Epoch 200, training loss: 7.717726230621338 = 1.5170655250549316 + 1.0 * 6.200660705566406
Epoch 200, val loss: 1.5529118776321411
Epoch 210, training loss: 7.655223846435547 = 1.4611321687698364 + 1.0 * 6.194091796875
Epoch 210, val loss: 1.5082476139068604
Epoch 220, training loss: 7.5866217613220215 = 1.4024511575698853 + 1.0 * 6.184170722961426
Epoch 220, val loss: 1.4623675346374512
Epoch 230, training loss: 7.520083427429199 = 1.343398094177246 + 1.0 * 6.176685333251953
Epoch 230, val loss: 1.4172972440719604
Epoch 240, training loss: 7.454643249511719 = 1.284929871559143 + 1.0 * 6.169713497161865
Epoch 240, val loss: 1.3741426467895508
Epoch 250, training loss: 7.393596649169922 = 1.2282606363296509 + 1.0 * 6.1653361320495605
Epoch 250, val loss: 1.3339194059371948
Epoch 260, training loss: 7.332083225250244 = 1.1745339632034302 + 1.0 * 6.1575493812561035
Epoch 260, val loss: 1.2971506118774414
Epoch 270, training loss: 7.274056434631348 = 1.1231987476348877 + 1.0 * 6.150857448577881
Epoch 270, val loss: 1.2628315687179565
Epoch 280, training loss: 7.222384452819824 = 1.0742075443267822 + 1.0 * 6.148177146911621
Epoch 280, val loss: 1.230576992034912
Epoch 290, training loss: 7.169482231140137 = 1.0281832218170166 + 1.0 * 6.141299247741699
Epoch 290, val loss: 1.200298547744751
Epoch 300, training loss: 7.119463920593262 = 0.9845991134643555 + 1.0 * 6.134864807128906
Epoch 300, val loss: 1.1714556217193604
Epoch 310, training loss: 7.073415756225586 = 0.9426339864730835 + 1.0 * 6.130781650543213
Epoch 310, val loss: 1.1433868408203125
Epoch 320, training loss: 7.031215190887451 = 0.9022213220596313 + 1.0 * 6.128993988037109
Epoch 320, val loss: 1.116133689880371
Epoch 330, training loss: 6.986437797546387 = 0.8631799221038818 + 1.0 * 6.123257637023926
Epoch 330, val loss: 1.0895487070083618
Epoch 340, training loss: 6.942839622497559 = 0.8248183727264404 + 1.0 * 6.118021488189697
Epoch 340, val loss: 1.0633492469787598
Epoch 350, training loss: 6.901322364807129 = 0.7866272926330566 + 1.0 * 6.114695072174072
Epoch 350, val loss: 1.0372759103775024
Epoch 360, training loss: 6.861913681030273 = 0.7486526966094971 + 1.0 * 6.113260746002197
Epoch 360, val loss: 1.0114128589630127
Epoch 370, training loss: 6.821052551269531 = 0.7114336490631104 + 1.0 * 6.109619140625
Epoch 370, val loss: 0.9861695170402527
Epoch 380, training loss: 6.780571460723877 = 0.6749334335327148 + 1.0 * 6.105638027191162
Epoch 380, val loss: 0.9616285562515259
Epoch 390, training loss: 6.745097637176514 = 0.6393830180168152 + 1.0 * 6.105714797973633
Epoch 390, val loss: 0.9379326701164246
Epoch 400, training loss: 6.706704139709473 = 0.6053107380867004 + 1.0 * 6.101393222808838
Epoch 400, val loss: 0.9156286716461182
Epoch 410, training loss: 6.670337200164795 = 0.5727820992469788 + 1.0 * 6.097555160522461
Epoch 410, val loss: 0.8948507905006409
Epoch 420, training loss: 6.637763023376465 = 0.5416407585144043 + 1.0 * 6.0961222648620605
Epoch 420, val loss: 0.8755009770393372
Epoch 430, training loss: 6.608383655548096 = 0.5121310353279114 + 1.0 * 6.09625244140625
Epoch 430, val loss: 0.8577427864074707
Epoch 440, training loss: 6.575133800506592 = 0.48424258828163147 + 1.0 * 6.090891361236572
Epoch 440, val loss: 0.8416602611541748
Epoch 450, training loss: 6.545398712158203 = 0.45765140652656555 + 1.0 * 6.087747097015381
Epoch 450, val loss: 0.8269798755645752
Epoch 460, training loss: 6.53175687789917 = 0.43231120705604553 + 1.0 * 6.099445819854736
Epoch 460, val loss: 0.8136568069458008
Epoch 470, training loss: 6.4951958656311035 = 0.408481627702713 + 1.0 * 6.086714267730713
Epoch 470, val loss: 0.8017531633377075
Epoch 480, training loss: 6.468893051147461 = 0.38578009605407715 + 1.0 * 6.083112716674805
Epoch 480, val loss: 0.7910166382789612
Epoch 490, training loss: 6.4452290534973145 = 0.36409205198287964 + 1.0 * 6.081137180328369
Epoch 490, val loss: 0.7811411023139954
Epoch 500, training loss: 6.4222612380981445 = 0.3433648645877838 + 1.0 * 6.078896522521973
Epoch 500, val loss: 0.7721987962722778
Epoch 510, training loss: 6.406131744384766 = 0.3234734535217285 + 1.0 * 6.082658290863037
Epoch 510, val loss: 0.7639275193214417
Epoch 520, training loss: 6.38153076171875 = 0.30456453561782837 + 1.0 * 6.076966285705566
Epoch 520, val loss: 0.7562806010246277
Epoch 530, training loss: 6.362303733825684 = 0.28659215569496155 + 1.0 * 6.075711727142334
Epoch 530, val loss: 0.749471127986908
Epoch 540, training loss: 6.347038269042969 = 0.26948630809783936 + 1.0 * 6.07755184173584
Epoch 540, val loss: 0.7433292269706726
Epoch 550, training loss: 6.326128005981445 = 0.2533029317855835 + 1.0 * 6.072824954986572
Epoch 550, val loss: 0.7378455400466919
Epoch 560, training loss: 6.307722091674805 = 0.23801282048225403 + 1.0 * 6.069709300994873
Epoch 560, val loss: 0.7331820130348206
Epoch 570, training loss: 6.294830322265625 = 0.22357283532619476 + 1.0 * 6.071257591247559
Epoch 570, val loss: 0.7293093800544739
Epoch 580, training loss: 6.279092311859131 = 0.21010297536849976 + 1.0 * 6.068989276885986
Epoch 580, val loss: 0.7261821031570435
Epoch 590, training loss: 6.265688896179199 = 0.19752541184425354 + 1.0 * 6.0681633949279785
Epoch 590, val loss: 0.7238589525222778
Epoch 600, training loss: 6.252227306365967 = 0.1858128011226654 + 1.0 * 6.0664143562316895
Epoch 600, val loss: 0.7222225666046143
Epoch 610, training loss: 6.23838472366333 = 0.17493849992752075 + 1.0 * 6.063446044921875
Epoch 610, val loss: 0.7214336395263672
Epoch 620, training loss: 6.230962753295898 = 0.1647854596376419 + 1.0 * 6.0661773681640625
Epoch 620, val loss: 0.7213906645774841
Epoch 630, training loss: 6.221097946166992 = 0.1554042249917984 + 1.0 * 6.0656938552856445
Epoch 630, val loss: 0.7218314409255981
Epoch 640, training loss: 6.206335544586182 = 0.146719828248024 + 1.0 * 6.059615612030029
Epoch 640, val loss: 0.7230445146560669
Epoch 650, training loss: 6.198835372924805 = 0.1386142075061798 + 1.0 * 6.060221195220947
Epoch 650, val loss: 0.7248473167419434
Epoch 660, training loss: 6.1952009201049805 = 0.13112370669841766 + 1.0 * 6.064077377319336
Epoch 660, val loss: 0.727016806602478
Epoch 670, training loss: 6.179738521575928 = 0.12419602274894714 + 1.0 * 6.055542469024658
Epoch 670, val loss: 0.7298175096511841
Epoch 680, training loss: 6.171565055847168 = 0.11772149801254272 + 1.0 * 6.0538434982299805
Epoch 680, val loss: 0.733087956905365
Epoch 690, training loss: 6.166826248168945 = 0.11165013164281845 + 1.0 * 6.055176258087158
Epoch 690, val loss: 0.7366912961006165
Epoch 700, training loss: 6.162972450256348 = 0.1060066893696785 + 1.0 * 6.0569658279418945
Epoch 700, val loss: 0.7405750155448914
Epoch 710, training loss: 6.151005268096924 = 0.10076842457056046 + 1.0 * 6.050236701965332
Epoch 710, val loss: 0.7448033094406128
Epoch 720, training loss: 6.146196365356445 = 0.09584657847881317 + 1.0 * 6.050349712371826
Epoch 720, val loss: 0.749325156211853
Epoch 730, training loss: 6.1443915367126465 = 0.09122923016548157 + 1.0 * 6.053162097930908
Epoch 730, val loss: 0.7539591193199158
Epoch 740, training loss: 6.135262966156006 = 0.08691241592168808 + 1.0 * 6.0483503341674805
Epoch 740, val loss: 0.7588706016540527
Epoch 750, training loss: 6.129786491394043 = 0.08285873383283615 + 1.0 * 6.0469279289245605
Epoch 750, val loss: 0.764035165309906
Epoch 760, training loss: 6.132660388946533 = 0.07903462648391724 + 1.0 * 6.053625583648682
Epoch 760, val loss: 0.7693673372268677
Epoch 770, training loss: 6.125312328338623 = 0.0754668191075325 + 1.0 * 6.0498456954956055
Epoch 770, val loss: 0.7747217416763306
Epoch 780, training loss: 6.118452548980713 = 0.07211321592330933 + 1.0 * 6.046339511871338
Epoch 780, val loss: 0.7803279757499695
Epoch 790, training loss: 6.115541934967041 = 0.06895323097705841 + 1.0 * 6.046588897705078
Epoch 790, val loss: 0.7859907150268555
Epoch 800, training loss: 6.109713077545166 = 0.06597716361284256 + 1.0 * 6.043735980987549
Epoch 800, val loss: 0.7917643785476685
Epoch 810, training loss: 6.1055803298950195 = 0.06316124647855759 + 1.0 * 6.042418956756592
Epoch 810, val loss: 0.7976645827293396
Epoch 820, training loss: 6.1065545082092285 = 0.0605025589466095 + 1.0 * 6.046051979064941
Epoch 820, val loss: 0.8035884499549866
Epoch 830, training loss: 6.099762439727783 = 0.05800352618098259 + 1.0 * 6.041759014129639
Epoch 830, val loss: 0.8095558881759644
Epoch 840, training loss: 6.096890449523926 = 0.05564890056848526 + 1.0 * 6.041241645812988
Epoch 840, val loss: 0.8155817985534668
Epoch 850, training loss: 6.092472553253174 = 0.05341833457350731 + 1.0 * 6.0390543937683105
Epoch 850, val loss: 0.8215849995613098
Epoch 860, training loss: 6.094216346740723 = 0.05130312964320183 + 1.0 * 6.042913436889648
Epoch 860, val loss: 0.8276024460792542
Epoch 870, training loss: 6.091071605682373 = 0.04931334778666496 + 1.0 * 6.041758060455322
Epoch 870, val loss: 0.83353590965271
Epoch 880, training loss: 6.084277629852295 = 0.04744698852300644 + 1.0 * 6.036830425262451
Epoch 880, val loss: 0.8395283818244934
Epoch 890, training loss: 6.084288597106934 = 0.045676473528146744 + 1.0 * 6.038611888885498
Epoch 890, val loss: 0.8454829454421997
Epoch 900, training loss: 6.079656600952148 = 0.043991927057504654 + 1.0 * 6.0356645584106445
Epoch 900, val loss: 0.8513399958610535
Epoch 910, training loss: 6.07908821105957 = 0.04239622876048088 + 1.0 * 6.036692142486572
Epoch 910, val loss: 0.8572818636894226
Epoch 920, training loss: 6.07560920715332 = 0.040878891944885254 + 1.0 * 6.034730434417725
Epoch 920, val loss: 0.8631438612937927
Epoch 930, training loss: 6.072793483734131 = 0.039437226951122284 + 1.0 * 6.033356189727783
Epoch 930, val loss: 0.8689864277839661
Epoch 940, training loss: 6.070623397827148 = 0.03806024789810181 + 1.0 * 6.032563209533691
Epoch 940, val loss: 0.874857485294342
Epoch 950, training loss: 6.0761942863464355 = 0.03674452751874924 + 1.0 * 6.039449691772461
Epoch 950, val loss: 0.8806365728378296
Epoch 960, training loss: 6.071193695068359 = 0.03550146520137787 + 1.0 * 6.03569221496582
Epoch 960, val loss: 0.8863697052001953
Epoch 970, training loss: 6.067080020904541 = 0.034324001520872116 + 1.0 * 6.0327558517456055
Epoch 970, val loss: 0.8920267820358276
Epoch 980, training loss: 6.0650153160095215 = 0.03320622071623802 + 1.0 * 6.031809329986572
Epoch 980, val loss: 0.8975794315338135
Epoch 990, training loss: 6.063559055328369 = 0.03214839845895767 + 1.0 * 6.0314106941223145
Epoch 990, val loss: 0.9030830264091492
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7232
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.33399772644043 = 1.9601938724517822 + 1.0 * 8.373804092407227
Epoch 0, val loss: 1.9594426155090332
Epoch 10, training loss: 10.323282241821289 = 1.9500497579574585 + 1.0 * 8.3732328414917
Epoch 10, val loss: 1.949937343597412
Epoch 20, training loss: 10.306252479553223 = 1.9374024868011475 + 1.0 * 8.368849754333496
Epoch 20, val loss: 1.9376636743545532
Epoch 30, training loss: 10.256237983703613 = 1.9195760488510132 + 1.0 * 8.336662292480469
Epoch 30, val loss: 1.920282006263733
Epoch 40, training loss: 10.032464027404785 = 1.897847294807434 + 1.0 * 8.13461685180664
Epoch 40, val loss: 1.9000998735427856
Epoch 50, training loss: 9.411616325378418 = 1.8744992017745972 + 1.0 * 7.5371174812316895
Epoch 50, val loss: 1.8781428337097168
Epoch 60, training loss: 8.998010635375977 = 1.8549920320510864 + 1.0 * 7.14301872253418
Epoch 60, val loss: 1.8602160215377808
Epoch 70, training loss: 8.698896408081055 = 1.8398268222808838 + 1.0 * 6.859069347381592
Epoch 70, val loss: 1.8456156253814697
Epoch 80, training loss: 8.497846603393555 = 1.8243508338928223 + 1.0 * 6.673496246337891
Epoch 80, val loss: 1.8307840824127197
Epoch 90, training loss: 8.39470100402832 = 1.8093481063842773 + 1.0 * 6.585353374481201
Epoch 90, val loss: 1.816328525543213
Epoch 100, training loss: 8.302168846130371 = 1.7945412397384644 + 1.0 * 6.507627487182617
Epoch 100, val loss: 1.8024406433105469
Epoch 110, training loss: 8.203264236450195 = 1.7820624113082886 + 1.0 * 6.421201705932617
Epoch 110, val loss: 1.7906991243362427
Epoch 120, training loss: 8.129916191101074 = 1.7709420919418335 + 1.0 * 6.358973979949951
Epoch 120, val loss: 1.7801117897033691
Epoch 130, training loss: 8.075471878051758 = 1.7586185932159424 + 1.0 * 6.316853046417236
Epoch 130, val loss: 1.7683314085006714
Epoch 140, training loss: 8.02835464477539 = 1.744165062904358 + 1.0 * 6.284189224243164
Epoch 140, val loss: 1.7550404071807861
Epoch 150, training loss: 7.9823689460754395 = 1.7273621559143066 + 1.0 * 6.255006790161133
Epoch 150, val loss: 1.7403171062469482
Epoch 160, training loss: 7.942120552062988 = 1.7075355052947998 + 1.0 * 6.234585285186768
Epoch 160, val loss: 1.7234597206115723
Epoch 170, training loss: 7.901119232177734 = 1.6837184429168701 + 1.0 * 6.217400550842285
Epoch 170, val loss: 1.703640103340149
Epoch 180, training loss: 7.858253002166748 = 1.6546682119369507 + 1.0 * 6.203584671020508
Epoch 180, val loss: 1.6796090602874756
Epoch 190, training loss: 7.811469554901123 = 1.6191821098327637 + 1.0 * 6.192287445068359
Epoch 190, val loss: 1.650136113166809
Epoch 200, training loss: 7.762542724609375 = 1.5764684677124023 + 1.0 * 6.186074256896973
Epoch 200, val loss: 1.6147828102111816
Epoch 210, training loss: 7.703420162200928 = 1.5277433395385742 + 1.0 * 6.1756768226623535
Epoch 210, val loss: 1.574459433555603
Epoch 220, training loss: 7.641696453094482 = 1.4735499620437622 + 1.0 * 6.16814661026001
Epoch 220, val loss: 1.5297576189041138
Epoch 230, training loss: 7.57918643951416 = 1.4158296585083008 + 1.0 * 6.163356781005859
Epoch 230, val loss: 1.4822967052459717
Epoch 240, training loss: 7.513102054595947 = 1.3567395210266113 + 1.0 * 6.156362533569336
Epoch 240, val loss: 1.4343104362487793
Epoch 250, training loss: 7.448056221008301 = 1.2972261905670166 + 1.0 * 6.150829792022705
Epoch 250, val loss: 1.3860276937484741
Epoch 260, training loss: 7.385075569152832 = 1.2383506298065186 + 1.0 * 6.146724700927734
Epoch 260, val loss: 1.338999629020691
Epoch 270, training loss: 7.326822280883789 = 1.1811152696609497 + 1.0 * 6.145707130432129
Epoch 270, val loss: 1.2940367460250854
Epoch 280, training loss: 7.264187812805176 = 1.1261109113693237 + 1.0 * 6.1380767822265625
Epoch 280, val loss: 1.2515476942062378
Epoch 290, training loss: 7.206891059875488 = 1.0727076530456543 + 1.0 * 6.134183406829834
Epoch 290, val loss: 1.211091160774231
Epoch 300, training loss: 7.152534484863281 = 1.0217065811157227 + 1.0 * 6.130827903747559
Epoch 300, val loss: 1.1733908653259277
Epoch 310, training loss: 7.100872039794922 = 0.9737383127212524 + 1.0 * 6.127133846282959
Epoch 310, val loss: 1.1385430097579956
Epoch 320, training loss: 7.051702499389648 = 0.9285084009170532 + 1.0 * 6.123194217681885
Epoch 320, val loss: 1.1063504219055176
Epoch 330, training loss: 7.0085768699646 = 0.8866711854934692 + 1.0 * 6.12190580368042
Epoch 330, val loss: 1.077113151550293
Epoch 340, training loss: 6.965092658996582 = 0.8484606742858887 + 1.0 * 6.116631984710693
Epoch 340, val loss: 1.0510724782943726
Epoch 350, training loss: 6.929797649383545 = 0.8131977319717407 + 1.0 * 6.116600036621094
Epoch 350, val loss: 1.0275893211364746
Epoch 360, training loss: 6.891152381896973 = 0.7808124423027039 + 1.0 * 6.110340118408203
Epoch 360, val loss: 1.006570816040039
Epoch 370, training loss: 6.858565330505371 = 0.750784695148468 + 1.0 * 6.107780456542969
Epoch 370, val loss: 0.987808108329773
Epoch 380, training loss: 6.826862812042236 = 0.722647488117218 + 1.0 * 6.104215145111084
Epoch 380, val loss: 0.9706907868385315
Epoch 390, training loss: 6.798344135284424 = 0.6960043907165527 + 1.0 * 6.102339744567871
Epoch 390, val loss: 0.955121636390686
Epoch 400, training loss: 6.771395206451416 = 0.6703514456748962 + 1.0 * 6.101043701171875
Epoch 400, val loss: 0.9406226873397827
Epoch 410, training loss: 6.742828369140625 = 0.645502507686615 + 1.0 * 6.097325801849365
Epoch 410, val loss: 0.9269779920578003
Epoch 420, training loss: 6.7137651443481445 = 0.6210983395576477 + 1.0 * 6.0926666259765625
Epoch 420, val loss: 0.9140139222145081
Epoch 430, training loss: 6.68681526184082 = 0.5966370701789856 + 1.0 * 6.0901780128479
Epoch 430, val loss: 0.9011887311935425
Epoch 440, training loss: 6.662697792053223 = 0.5720023512840271 + 1.0 * 6.090695381164551
Epoch 440, val loss: 0.8881434202194214
Epoch 450, training loss: 6.632628917694092 = 0.5473251938819885 + 1.0 * 6.085303783416748
Epoch 450, val loss: 0.8752254247665405
Epoch 460, training loss: 6.605648517608643 = 0.5222437977790833 + 1.0 * 6.083404541015625
Epoch 460, val loss: 0.8619178533554077
Epoch 470, training loss: 6.578396320343018 = 0.49671104550361633 + 1.0 * 6.0816850662231445
Epoch 470, val loss: 0.8481879234313965
Epoch 480, training loss: 6.563682556152344 = 0.47101473808288574 + 1.0 * 6.092668056488037
Epoch 480, val loss: 0.8341907262802124
Epoch 490, training loss: 6.525198459625244 = 0.44580042362213135 + 1.0 * 6.079398155212402
Epoch 490, val loss: 0.8205932378768921
Epoch 500, training loss: 6.49860954284668 = 0.42080217599868774 + 1.0 * 6.077807426452637
Epoch 500, val loss: 0.8072760701179504
Epoch 510, training loss: 6.481696605682373 = 0.396169513463974 + 1.0 * 6.085526943206787
Epoch 510, val loss: 0.7944157123565674
Epoch 520, training loss: 6.445850372314453 = 0.372446745634079 + 1.0 * 6.073403835296631
Epoch 520, val loss: 0.7826104164123535
Epoch 530, training loss: 6.42279577255249 = 0.349593847990036 + 1.0 * 6.073202133178711
Epoch 530, val loss: 0.7719857096672058
Epoch 540, training loss: 6.407271862030029 = 0.3277527391910553 + 1.0 * 6.079519271850586
Epoch 540, val loss: 0.76248699426651
Epoch 550, training loss: 6.377225875854492 = 0.30712923407554626 + 1.0 * 6.070096492767334
Epoch 550, val loss: 0.7543238997459412
Epoch 560, training loss: 6.357224941253662 = 0.2876458466053009 + 1.0 * 6.069579124450684
Epoch 560, val loss: 0.7473529577255249
Epoch 570, training loss: 6.34372615814209 = 0.2692239284515381 + 1.0 * 6.074502468109131
Epoch 570, val loss: 0.7413232922554016
Epoch 580, training loss: 6.320333957672119 = 0.25198274850845337 + 1.0 * 6.0683512687683105
Epoch 580, val loss: 0.7361807823181152
Epoch 590, training loss: 6.300049304962158 = 0.2356918454170227 + 1.0 * 6.064357280731201
Epoch 590, val loss: 0.7318586111068726
Epoch 600, training loss: 6.2961225509643555 = 0.22033946216106415 + 1.0 * 6.0757832527160645
Epoch 600, val loss: 0.7281330227851868
Epoch 610, training loss: 6.270683765411377 = 0.2059740126132965 + 1.0 * 6.064709663391113
Epoch 610, val loss: 0.7249402403831482
Epoch 620, training loss: 6.25327730178833 = 0.19250665605068207 + 1.0 * 6.060770511627197
Epoch 620, val loss: 0.7222692370414734
Epoch 630, training loss: 6.23900842666626 = 0.17983068525791168 + 1.0 * 6.059177875518799
Epoch 630, val loss: 0.7200455069541931
Epoch 640, training loss: 6.229329586029053 = 0.16789041459560394 + 1.0 * 6.061439037322998
Epoch 640, val loss: 0.7182028293609619
Epoch 650, training loss: 6.218225002288818 = 0.1567155122756958 + 1.0 * 6.061509609222412
Epoch 650, val loss: 0.7166322469711304
Epoch 660, training loss: 6.210888862609863 = 0.14630675315856934 + 1.0 * 6.064581871032715
Epoch 660, val loss: 0.7154961824417114
Epoch 670, training loss: 6.194243907928467 = 0.13663892447948456 + 1.0 * 6.057604789733887
Epoch 670, val loss: 0.7146362066268921
Epoch 680, training loss: 6.181344509124756 = 0.12762509286403656 + 1.0 * 6.053719520568848
Epoch 680, val loss: 0.7142002582550049
Epoch 690, training loss: 6.172603130340576 = 0.11922530084848404 + 1.0 * 6.053377628326416
Epoch 690, val loss: 0.7140882611274719
Epoch 700, training loss: 6.168601036071777 = 0.11144303530454636 + 1.0 * 6.05715799331665
Epoch 700, val loss: 0.7141795754432678
Epoch 710, training loss: 6.1569743156433105 = 0.10428465157747269 + 1.0 * 6.052689552307129
Epoch 710, val loss: 0.71457439661026
Epoch 720, training loss: 6.146426200866699 = 0.09768223017454147 + 1.0 * 6.048744201660156
Epoch 720, val loss: 0.7154062986373901
Epoch 730, training loss: 6.141052722930908 = 0.09157183021306992 + 1.0 * 6.04948091506958
Epoch 730, val loss: 0.7165240049362183
Epoch 740, training loss: 6.135167598724365 = 0.08594409376382828 + 1.0 * 6.04922342300415
Epoch 740, val loss: 0.7177879810333252
Epoch 750, training loss: 6.129026412963867 = 0.08079420030117035 + 1.0 * 6.048232078552246
Epoch 750, val loss: 0.7193638682365417
Epoch 760, training loss: 6.121761322021484 = 0.07606116682291031 + 1.0 * 6.0457000732421875
Epoch 760, val loss: 0.7212391495704651
Epoch 770, training loss: 6.116467475891113 = 0.07169627398252487 + 1.0 * 6.044771194458008
Epoch 770, val loss: 0.7233654260635376
Epoch 780, training loss: 6.111423015594482 = 0.06766313314437866 + 1.0 * 6.043759822845459
Epoch 780, val loss: 0.7257229089736938
Epoch 790, training loss: 6.109160900115967 = 0.06394989788532257 + 1.0 * 6.045210838317871
Epoch 790, val loss: 0.7281639575958252
Epoch 800, training loss: 6.104883193969727 = 0.06055813655257225 + 1.0 * 6.04432487487793
Epoch 800, val loss: 0.730714738368988
Epoch 810, training loss: 6.098282337188721 = 0.05743014067411423 + 1.0 * 6.040852069854736
Epoch 810, val loss: 0.7335121035575867
Epoch 820, training loss: 6.10408353805542 = 0.05452457442879677 + 1.0 * 6.049559116363525
Epoch 820, val loss: 0.7364460825920105
Epoch 830, training loss: 6.093929290771484 = 0.05185150355100632 + 1.0 * 6.042078018188477
Epoch 830, val loss: 0.739342451095581
Epoch 840, training loss: 6.086451530456543 = 0.049374159425497055 + 1.0 * 6.0370774269104
Epoch 840, val loss: 0.7424046993255615
Epoch 850, training loss: 6.083672046661377 = 0.04706278815865517 + 1.0 * 6.036609172821045
Epoch 850, val loss: 0.7455823421478271
Epoch 860, training loss: 6.093160629272461 = 0.04490484297275543 + 1.0 * 6.048255920410156
Epoch 860, val loss: 0.7487524747848511
Epoch 870, training loss: 6.084014415740967 = 0.042914554476737976 + 1.0 * 6.041100025177002
Epoch 870, val loss: 0.7518189549446106
Epoch 880, training loss: 6.07724142074585 = 0.04106893762946129 + 1.0 * 6.036172389984131
Epoch 880, val loss: 0.7549943923950195
Epoch 890, training loss: 6.07371711730957 = 0.03934631124138832 + 1.0 * 6.0343708992004395
Epoch 890, val loss: 0.758311927318573
Epoch 900, training loss: 6.070257186889648 = 0.03772106021642685 + 1.0 * 6.032536029815674
Epoch 900, val loss: 0.7616658806800842
Epoch 910, training loss: 6.070143699645996 = 0.0361899733543396 + 1.0 * 6.033953666687012
Epoch 910, val loss: 0.7650321125984192
Epoch 920, training loss: 6.0683979988098145 = 0.034754227846860886 + 1.0 * 6.03364372253418
Epoch 920, val loss: 0.7683242559432983
Epoch 930, training loss: 6.067427635192871 = 0.03341345489025116 + 1.0 * 6.0340142250061035
Epoch 930, val loss: 0.77158522605896
Epoch 940, training loss: 6.063507556915283 = 0.032159578055143356 + 1.0 * 6.031347751617432
Epoch 940, val loss: 0.7749216556549072
Epoch 950, training loss: 6.061092376708984 = 0.03097524307668209 + 1.0 * 6.030117034912109
Epoch 950, val loss: 0.7783268094062805
Epoch 960, training loss: 6.062454700469971 = 0.029851099476218224 + 1.0 * 6.032603740692139
Epoch 960, val loss: 0.7817110419273376
Epoch 970, training loss: 6.05900239944458 = 0.028789253905415535 + 1.0 * 6.030213356018066
Epoch 970, val loss: 0.7849854826927185
Epoch 980, training loss: 6.0576090812683105 = 0.02779281884431839 + 1.0 * 6.029816150665283
Epoch 980, val loss: 0.7882536053657532
Epoch 990, training loss: 6.054049968719482 = 0.026851005852222443 + 1.0 * 6.027198791503906
Epoch 990, val loss: 0.7915824055671692
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.8856
Flip ASR: 0.8622/225 nodes
The final ASR:0.76261, 0.08883, Accuracy:0.82099, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11568])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10502])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.313475608825684 = 1.9397697448730469 + 1.0 * 8.373705863952637
Epoch 0, val loss: 1.934869408607483
Epoch 10, training loss: 10.301322937011719 = 1.9285839796066284 + 1.0 * 8.3727388381958
Epoch 10, val loss: 1.9243265390396118
Epoch 20, training loss: 10.280893325805664 = 1.9140150547027588 + 1.0 * 8.366878509521484
Epoch 20, val loss: 1.9101139307022095
Epoch 30, training loss: 10.221624374389648 = 1.8935340642929077 + 1.0 * 8.32809066772461
Epoch 30, val loss: 1.8901017904281616
Epoch 40, training loss: 9.899909973144531 = 1.8693629503250122 + 1.0 * 8.030547142028809
Epoch 40, val loss: 1.8676581382751465
Epoch 50, training loss: 8.937692642211914 = 1.845181941986084 + 1.0 * 7.09251070022583
Epoch 50, val loss: 1.8460954427719116
Epoch 60, training loss: 8.597132682800293 = 1.8315403461456299 + 1.0 * 6.765592098236084
Epoch 60, val loss: 1.8345086574554443
Epoch 70, training loss: 8.42288875579834 = 1.8206874132156372 + 1.0 * 6.602201461791992
Epoch 70, val loss: 1.8253475427627563
Epoch 80, training loss: 8.32943058013916 = 1.8100792169570923 + 1.0 * 6.519351482391357
Epoch 80, val loss: 1.8163228034973145
Epoch 90, training loss: 8.238218307495117 = 1.798719882965088 + 1.0 * 6.4394989013671875
Epoch 90, val loss: 1.8067811727523804
Epoch 100, training loss: 8.167723655700684 = 1.7883118391036987 + 1.0 * 6.379411697387695
Epoch 100, val loss: 1.798139214515686
Epoch 110, training loss: 8.113506317138672 = 1.7782745361328125 + 1.0 * 6.335232257843018
Epoch 110, val loss: 1.7897030115127563
Epoch 120, training loss: 8.05788516998291 = 1.7676728963851929 + 1.0 * 6.290212631225586
Epoch 120, val loss: 1.780759334564209
Epoch 130, training loss: 8.00927734375 = 1.755780816078186 + 1.0 * 6.253496170043945
Epoch 130, val loss: 1.7709872722625732
Epoch 140, training loss: 7.969925880432129 = 1.7415086030960083 + 1.0 * 6.22841739654541
Epoch 140, val loss: 1.7595388889312744
Epoch 150, training loss: 7.933825969696045 = 1.7239011526107788 + 1.0 * 6.209924697875977
Epoch 150, val loss: 1.7456352710723877
Epoch 160, training loss: 7.896992206573486 = 1.7023512125015259 + 1.0 * 6.19464111328125
Epoch 160, val loss: 1.728670597076416
Epoch 170, training loss: 7.8576555252075195 = 1.6757923364639282 + 1.0 * 6.181863307952881
Epoch 170, val loss: 1.7077312469482422
Epoch 180, training loss: 7.814200401306152 = 1.6431020498275757 + 1.0 * 6.171098232269287
Epoch 180, val loss: 1.6819279193878174
Epoch 190, training loss: 7.7648701667785645 = 1.6034432649612427 + 1.0 * 6.161427021026611
Epoch 190, val loss: 1.6505398750305176
Epoch 200, training loss: 7.711144924163818 = 1.5559682846069336 + 1.0 * 6.155176639556885
Epoch 200, val loss: 1.6128106117248535
Epoch 210, training loss: 7.647928237915039 = 1.5015708208084106 + 1.0 * 6.146357536315918
Epoch 210, val loss: 1.5695736408233643
Epoch 220, training loss: 7.580705642700195 = 1.4410873651504517 + 1.0 * 6.139618396759033
Epoch 220, val loss: 1.5216468572616577
Epoch 230, training loss: 7.511434555053711 = 1.376560091972351 + 1.0 * 6.13487434387207
Epoch 230, val loss: 1.4710400104522705
Epoch 240, training loss: 7.44136905670166 = 1.3116662502288818 + 1.0 * 6.129703044891357
Epoch 240, val loss: 1.4210269451141357
Epoch 250, training loss: 7.374166011810303 = 1.2482386827468872 + 1.0 * 6.125927448272705
Epoch 250, val loss: 1.3729135990142822
Epoch 260, training loss: 7.307881832122803 = 1.1869152784347534 + 1.0 * 6.12096643447876
Epoch 260, val loss: 1.3268499374389648
Epoch 270, training loss: 7.2445831298828125 = 1.1274938583374023 + 1.0 * 6.11708927154541
Epoch 270, val loss: 1.2825393676757812
Epoch 280, training loss: 7.185946941375732 = 1.0708540678024292 + 1.0 * 6.115092754364014
Epoch 280, val loss: 1.240889310836792
Epoch 290, training loss: 7.127199172973633 = 1.0179729461669922 + 1.0 * 6.109226226806641
Epoch 290, val loss: 1.2020922899246216
Epoch 300, training loss: 7.072972297668457 = 0.9677011370658875 + 1.0 * 6.105271339416504
Epoch 300, val loss: 1.1651010513305664
Epoch 310, training loss: 7.0207695960998535 = 0.919310986995697 + 1.0 * 6.101458549499512
Epoch 310, val loss: 1.1294653415679932
Epoch 320, training loss: 6.972133159637451 = 0.8727733492851257 + 1.0 * 6.09935998916626
Epoch 320, val loss: 1.0951471328735352
Epoch 330, training loss: 6.923201560974121 = 0.8280946016311646 + 1.0 * 6.095107078552246
Epoch 330, val loss: 1.0622931718826294
Epoch 340, training loss: 6.878054618835449 = 0.7846647500991821 + 1.0 * 6.093389987945557
Epoch 340, val loss: 1.0303727388381958
Epoch 350, training loss: 6.834348678588867 = 0.7426365613937378 + 1.0 * 6.09171199798584
Epoch 350, val loss: 0.9995450377464294
Epoch 360, training loss: 6.789279937744141 = 0.7023209929466248 + 1.0 * 6.086958885192871
Epoch 360, val loss: 0.9704654216766357
Epoch 370, training loss: 6.748541355133057 = 0.6637014150619507 + 1.0 * 6.084839820861816
Epoch 370, val loss: 0.9431147575378418
Epoch 380, training loss: 6.708390235900879 = 0.626413106918335 + 1.0 * 6.081976890563965
Epoch 380, val loss: 0.9174034595489502
Epoch 390, training loss: 6.6719160079956055 = 0.5907026529312134 + 1.0 * 6.081213474273682
Epoch 390, val loss: 0.8936419486999512
Epoch 400, training loss: 6.635335922241211 = 0.5567335486412048 + 1.0 * 6.078602313995361
Epoch 400, val loss: 0.8721752762794495
Epoch 410, training loss: 6.599095821380615 = 0.5239459872245789 + 1.0 * 6.075150012969971
Epoch 410, val loss: 0.8524776697158813
Epoch 420, training loss: 6.573874473571777 = 0.4922470152378082 + 1.0 * 6.081627368927002
Epoch 420, val loss: 0.8344655632972717
Epoch 430, training loss: 6.538352012634277 = 0.46203330159187317 + 1.0 * 6.076318740844727
Epoch 430, val loss: 0.818244218826294
Epoch 440, training loss: 6.502235412597656 = 0.43314653635025024 + 1.0 * 6.069088935852051
Epoch 440, val loss: 0.8036609292030334
Epoch 450, training loss: 6.472851753234863 = 0.40530720353126526 + 1.0 * 6.067544460296631
Epoch 450, val loss: 0.790527880191803
Epoch 460, training loss: 6.4490814208984375 = 0.37863484025001526 + 1.0 * 6.070446491241455
Epoch 460, val loss: 0.7787624597549438
Epoch 470, training loss: 6.421217918395996 = 0.35337740182876587 + 1.0 * 6.067840576171875
Epoch 470, val loss: 0.7685054540634155
Epoch 480, training loss: 6.392329692840576 = 0.32961195707321167 + 1.0 * 6.062717914581299
Epoch 480, val loss: 0.7597336173057556
Epoch 490, training loss: 6.36839485168457 = 0.3071899712085724 + 1.0 * 6.06120491027832
Epoch 490, val loss: 0.7524088621139526
Epoch 500, training loss: 6.348143577575684 = 0.2860756516456604 + 1.0 * 6.062067985534668
Epoch 500, val loss: 0.7464078664779663
Epoch 510, training loss: 6.326728820800781 = 0.26644328236579895 + 1.0 * 6.060285568237305
Epoch 510, val loss: 0.7417752742767334
Epoch 520, training loss: 6.312936782836914 = 0.2482810914516449 + 1.0 * 6.064655780792236
Epoch 520, val loss: 0.7384723424911499
Epoch 530, training loss: 6.288543701171875 = 0.23156431317329407 + 1.0 * 6.056979179382324
Epoch 530, val loss: 0.736375629901886
Epoch 540, training loss: 6.2699809074401855 = 0.21600593626499176 + 1.0 * 6.0539751052856445
Epoch 540, val loss: 0.7353530526161194
Epoch 550, training loss: 6.254605293273926 = 0.2015368491411209 + 1.0 * 6.0530686378479
Epoch 550, val loss: 0.7353213429450989
Epoch 560, training loss: 6.2416605949401855 = 0.18814536929130554 + 1.0 * 6.053515434265137
Epoch 560, val loss: 0.7361201643943787
Epoch 570, training loss: 6.226048469543457 = 0.17591117322444916 + 1.0 * 6.050137519836426
Epoch 570, val loss: 0.7377832531929016
Epoch 580, training loss: 6.214079856872559 = 0.1646127998828888 + 1.0 * 6.049467086791992
Epoch 580, val loss: 0.7401974201202393
Epoch 590, training loss: 6.2044596672058105 = 0.15416523814201355 + 1.0 * 6.050294399261475
Epoch 590, val loss: 0.7431669235229492
Epoch 600, training loss: 6.196895122528076 = 0.14452868700027466 + 1.0 * 6.052366256713867
Epoch 600, val loss: 0.7466351985931396
Epoch 610, training loss: 6.184226989746094 = 0.13572433590888977 + 1.0 * 6.048502445220947
Epoch 610, val loss: 0.75048828125
Epoch 620, training loss: 6.172961235046387 = 0.12761318683624268 + 1.0 * 6.045348167419434
Epoch 620, val loss: 0.7546813488006592
Epoch 630, training loss: 6.163186550140381 = 0.1200796589255333 + 1.0 * 6.043107032775879
Epoch 630, val loss: 0.7591456770896912
Epoch 640, training loss: 6.1591315269470215 = 0.11306529492139816 + 1.0 * 6.0460662841796875
Epoch 640, val loss: 0.7638444304466248
Epoch 650, training loss: 6.155778408050537 = 0.10657846182584763 + 1.0 * 6.049200057983398
Epoch 650, val loss: 0.7685915231704712
Epoch 660, training loss: 6.140159606933594 = 0.10061687976121902 + 1.0 * 6.0395426750183105
Epoch 660, val loss: 0.7736173272132874
Epoch 670, training loss: 6.134340286254883 = 0.09505906701087952 + 1.0 * 6.039281368255615
Epoch 670, val loss: 0.7788329124450684
Epoch 680, training loss: 6.134645462036133 = 0.08987002074718475 + 1.0 * 6.044775485992432
Epoch 680, val loss: 0.784138023853302
Epoch 690, training loss: 6.126102447509766 = 0.08506115525960922 + 1.0 * 6.041041374206543
Epoch 690, val loss: 0.7895505428314209
Epoch 700, training loss: 6.1162872314453125 = 0.08061117678880692 + 1.0 * 6.035676002502441
Epoch 700, val loss: 0.7950863838195801
Epoch 710, training loss: 6.117453098297119 = 0.07645617425441742 + 1.0 * 6.04099702835083
Epoch 710, val loss: 0.8007345795631409
Epoch 720, training loss: 6.111428260803223 = 0.0726066455245018 + 1.0 * 6.038821697235107
Epoch 720, val loss: 0.8063459992408752
Epoch 730, training loss: 6.1027069091796875 = 0.0690423995256424 + 1.0 * 6.033664703369141
Epoch 730, val loss: 0.8120408058166504
Epoch 740, training loss: 6.0990095138549805 = 0.06571169197559357 + 1.0 * 6.033298015594482
Epoch 740, val loss: 0.8178055286407471
Epoch 750, training loss: 6.096586227416992 = 0.06260182708501816 + 1.0 * 6.033984184265137
Epoch 750, val loss: 0.8236309289932251
Epoch 760, training loss: 6.090762138366699 = 0.059714123606681824 + 1.0 * 6.031047821044922
Epoch 760, val loss: 0.8295118808746338
Epoch 770, training loss: 6.087672233581543 = 0.05702036991715431 + 1.0 * 6.030652046203613
Epoch 770, val loss: 0.8354615569114685
Epoch 780, training loss: 6.088770389556885 = 0.05449475720524788 + 1.0 * 6.034275531768799
Epoch 780, val loss: 0.8414403796195984
Epoch 790, training loss: 6.083825588226318 = 0.052149899303913116 + 1.0 * 6.031675815582275
Epoch 790, val loss: 0.8474224805831909
Epoch 800, training loss: 6.080010890960693 = 0.0499555803835392 + 1.0 * 6.030055522918701
Epoch 800, val loss: 0.8533670902252197
Epoch 810, training loss: 6.074808120727539 = 0.04789629206061363 + 1.0 * 6.026911735534668
Epoch 810, val loss: 0.859295129776001
Epoch 820, training loss: 6.073060035705566 = 0.045963089913129807 + 1.0 * 6.027096748352051
Epoch 820, val loss: 0.8652357459068298
Epoch 830, training loss: 6.075246810913086 = 0.04414328932762146 + 1.0 * 6.031103610992432
Epoch 830, val loss: 0.871198832988739
Epoch 840, training loss: 6.069031715393066 = 0.042438581585884094 + 1.0 * 6.026593208312988
Epoch 840, val loss: 0.8770825862884521
Epoch 850, training loss: 6.077230453491211 = 0.04083661735057831 + 1.0 * 6.036393642425537
Epoch 850, val loss: 0.8828877806663513
Epoch 860, training loss: 6.0649824142456055 = 0.03933648392558098 + 1.0 * 6.025645732879639
Epoch 860, val loss: 0.8886417150497437
Epoch 870, training loss: 6.060297966003418 = 0.037918172776699066 + 1.0 * 6.0223798751831055
Epoch 870, val loss: 0.8942884206771851
Epoch 880, training loss: 6.057742595672607 = 0.03656769171357155 + 1.0 * 6.021174907684326
Epoch 880, val loss: 0.8999815583229065
Epoch 890, training loss: 6.055846691131592 = 0.035279322415590286 + 1.0 * 6.020567417144775
Epoch 890, val loss: 0.9056887626647949
Epoch 900, training loss: 6.063098430633545 = 0.034055955708026886 + 1.0 * 6.0290422439575195
Epoch 900, val loss: 0.9113855957984924
Epoch 910, training loss: 6.053373336791992 = 0.03290504217147827 + 1.0 * 6.020468235015869
Epoch 910, val loss: 0.9170119166374207
Epoch 920, training loss: 6.0559401512146 = 0.0318150520324707 + 1.0 * 6.024125099182129
Epoch 920, val loss: 0.9224899411201477
Epoch 930, training loss: 6.050047874450684 = 0.03078419528901577 + 1.0 * 6.019263744354248
Epoch 930, val loss: 0.9279685020446777
Epoch 940, training loss: 6.04828405380249 = 0.029802758246660233 + 1.0 * 6.018481254577637
Epoch 940, val loss: 0.933319091796875
Epoch 950, training loss: 6.045660495758057 = 0.028866266831755638 + 1.0 * 6.016794204711914
Epoch 950, val loss: 0.9386961460113525
Epoch 960, training loss: 6.046877384185791 = 0.027965953573584557 + 1.0 * 6.018911361694336
Epoch 960, val loss: 0.9440798759460449
Epoch 970, training loss: 6.043380260467529 = 0.02710898593068123 + 1.0 * 6.016271114349365
Epoch 970, val loss: 0.9494186043739319
Epoch 980, training loss: 6.056088924407959 = 0.02629826031625271 + 1.0 * 6.029790878295898
Epoch 980, val loss: 0.9546576738357544
Epoch 990, training loss: 6.042107582092285 = 0.025530902668833733 + 1.0 * 6.016576766967773
Epoch 990, val loss: 0.9597566723823547
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5830
Flip ASR: 0.5067/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32961654663086 = 1.9557850360870361 + 1.0 * 8.373831748962402
Epoch 0, val loss: 1.9501317739486694
Epoch 10, training loss: 10.317991256713867 = 1.9447225332260132 + 1.0 * 8.373269081115723
Epoch 10, val loss: 1.938287615776062
Epoch 20, training loss: 10.301431655883789 = 1.931553602218628 + 1.0 * 8.369877815246582
Epoch 20, val loss: 1.923844575881958
Epoch 30, training loss: 10.265398979187012 = 1.914005994796753 + 1.0 * 8.35139274597168
Epoch 30, val loss: 1.9043415784835815
Epoch 40, training loss: 10.139548301696777 = 1.8921376466751099 + 1.0 * 8.247410774230957
Epoch 40, val loss: 1.8804736137390137
Epoch 50, training loss: 9.624048233032227 = 1.8714473247528076 + 1.0 * 7.752601146697998
Epoch 50, val loss: 1.8581207990646362
Epoch 60, training loss: 9.109190940856934 = 1.8507041931152344 + 1.0 * 7.258486747741699
Epoch 60, val loss: 1.8366249799728394
Epoch 70, training loss: 8.831696510314941 = 1.8293508291244507 + 1.0 * 7.002346038818359
Epoch 70, val loss: 1.8158334493637085
Epoch 80, training loss: 8.626667022705078 = 1.8115650415420532 + 1.0 * 6.8151021003723145
Epoch 80, val loss: 1.799173355102539
Epoch 90, training loss: 8.482970237731934 = 1.7958860397338867 + 1.0 * 6.687084197998047
Epoch 90, val loss: 1.7835533618927002
Epoch 100, training loss: 8.371414184570312 = 1.7792601585388184 + 1.0 * 6.592154026031494
Epoch 100, val loss: 1.7671270370483398
Epoch 110, training loss: 8.278519630432129 = 1.7632546424865723 + 1.0 * 6.515264987945557
Epoch 110, val loss: 1.752144455909729
Epoch 120, training loss: 8.180604934692383 = 1.7490921020507812 + 1.0 * 6.43151330947876
Epoch 120, val loss: 1.7398169040679932
Epoch 130, training loss: 8.108078956604004 = 1.7346524000167847 + 1.0 * 6.373426914215088
Epoch 130, val loss: 1.7272801399230957
Epoch 140, training loss: 8.054261207580566 = 1.7171251773834229 + 1.0 * 6.3371357917785645
Epoch 140, val loss: 1.711782693862915
Epoch 150, training loss: 8.002963066101074 = 1.695198655128479 + 1.0 * 6.307764530181885
Epoch 150, val loss: 1.693018913269043
Epoch 160, training loss: 7.9515156745910645 = 1.6689294576644897 + 1.0 * 6.282586097717285
Epoch 160, val loss: 1.6711775064468384
Epoch 170, training loss: 7.8965959548950195 = 1.6377817392349243 + 1.0 * 6.258814334869385
Epoch 170, val loss: 1.6457805633544922
Epoch 180, training loss: 7.8407087326049805 = 1.5997687578201294 + 1.0 * 6.240940093994141
Epoch 180, val loss: 1.6148184537887573
Epoch 190, training loss: 7.779268264770508 = 1.5531331300735474 + 1.0 * 6.22613525390625
Epoch 190, val loss: 1.5767219066619873
Epoch 200, training loss: 7.712893962860107 = 1.4978772401809692 + 1.0 * 6.215016841888428
Epoch 200, val loss: 1.5318292379379272
Epoch 210, training loss: 7.640393257141113 = 1.4370726346969604 + 1.0 * 6.203320503234863
Epoch 210, val loss: 1.4827888011932373
Epoch 220, training loss: 7.568739891052246 = 1.3724607229232788 + 1.0 * 6.196279048919678
Epoch 220, val loss: 1.431087851524353
Epoch 230, training loss: 7.495912551879883 = 1.3076131343841553 + 1.0 * 6.188299655914307
Epoch 230, val loss: 1.3804110288619995
Epoch 240, training loss: 7.4255805015563965 = 1.2447255849838257 + 1.0 * 6.180854797363281
Epoch 240, val loss: 1.3317252397537231
Epoch 250, training loss: 7.35775899887085 = 1.1842141151428223 + 1.0 * 6.173544883728027
Epoch 250, val loss: 1.2855677604675293
Epoch 260, training loss: 7.294114112854004 = 1.1263095140457153 + 1.0 * 6.167804718017578
Epoch 260, val loss: 1.2419939041137695
Epoch 270, training loss: 7.233252048492432 = 1.071475625038147 + 1.0 * 6.161776542663574
Epoch 270, val loss: 1.2012312412261963
Epoch 280, training loss: 7.176159858703613 = 1.0187742710113525 + 1.0 * 6.15738582611084
Epoch 280, val loss: 1.1621966361999512
Epoch 290, training loss: 7.116855144500732 = 0.9673103094100952 + 1.0 * 6.149544715881348
Epoch 290, val loss: 1.1243857145309448
Epoch 300, training loss: 7.060924053192139 = 0.9164299368858337 + 1.0 * 6.14449405670166
Epoch 300, val loss: 1.0869468450546265
Epoch 310, training loss: 7.016878604888916 = 0.8662076592445374 + 1.0 * 6.150671005249023
Epoch 310, val loss: 1.0500099658966064
Epoch 320, training loss: 6.95519495010376 = 0.8177834749221802 + 1.0 * 6.137411594390869
Epoch 320, val loss: 1.0143250226974487
Epoch 330, training loss: 6.902862548828125 = 0.7711606025695801 + 1.0 * 6.131701946258545
Epoch 330, val loss: 0.9798170328140259
Epoch 340, training loss: 6.853339195251465 = 0.7263312935829163 + 1.0 * 6.127007961273193
Epoch 340, val loss: 0.9466365575790405
Epoch 350, training loss: 6.810245513916016 = 0.6834644675254822 + 1.0 * 6.126780986785889
Epoch 350, val loss: 0.9151210188865662
Epoch 360, training loss: 6.767073154449463 = 0.6438566446304321 + 1.0 * 6.12321662902832
Epoch 360, val loss: 0.886125922203064
Epoch 370, training loss: 6.724156379699707 = 0.6074263453483582 + 1.0 * 6.116730213165283
Epoch 370, val loss: 0.8600810766220093
Epoch 380, training loss: 6.68862771987915 = 0.5736218690872192 + 1.0 * 6.115005970001221
Epoch 380, val loss: 0.8364296555519104
Epoch 390, training loss: 6.655869483947754 = 0.5425195097923279 + 1.0 * 6.113349914550781
Epoch 390, val loss: 0.8152933120727539
Epoch 400, training loss: 6.6215901374816895 = 0.5139514207839966 + 1.0 * 6.107638835906982
Epoch 400, val loss: 0.7966302037239075
Epoch 410, training loss: 6.592417240142822 = 0.48735660314559937 + 1.0 * 6.105060577392578
Epoch 410, val loss: 0.7799761295318604
Epoch 420, training loss: 6.566685676574707 = 0.4626089930534363 + 1.0 * 6.104076862335205
Epoch 420, val loss: 0.7651852369308472
Epoch 430, training loss: 6.5388336181640625 = 0.4397650957107544 + 1.0 * 6.099068641662598
Epoch 430, val loss: 0.7522796988487244
Epoch 440, training loss: 6.514191150665283 = 0.4183726906776428 + 1.0 * 6.095818519592285
Epoch 440, val loss: 0.7409483790397644
Epoch 450, training loss: 6.491247177124023 = 0.39821842312812805 + 1.0 * 6.093028545379639
Epoch 450, val loss: 0.7309104204177856
Epoch 460, training loss: 6.471932411193848 = 0.37928369641304016 + 1.0 * 6.092648506164551
Epoch 460, val loss: 0.7220361828804016
Epoch 470, training loss: 6.452267169952393 = 0.36172136664390564 + 1.0 * 6.090545654296875
Epoch 470, val loss: 0.7144293785095215
Epoch 480, training loss: 6.432696342468262 = 0.34520643949508667 + 1.0 * 6.087490081787109
Epoch 480, val loss: 0.7078598141670227
Epoch 490, training loss: 6.413136959075928 = 0.32949551939964294 + 1.0 * 6.083641529083252
Epoch 490, val loss: 0.7020311951637268
Epoch 500, training loss: 6.397989273071289 = 0.3144836127758026 + 1.0 * 6.083505630493164
Epoch 500, val loss: 0.6967983841896057
Epoch 510, training loss: 6.391777992248535 = 0.3002226650714874 + 1.0 * 6.091555118560791
Epoch 510, val loss: 0.6922379732131958
Epoch 520, training loss: 6.3658223152160645 = 0.2866691052913666 + 1.0 * 6.079153060913086
Epoch 520, val loss: 0.6882505416870117
Epoch 530, training loss: 6.349339008331299 = 0.2736365497112274 + 1.0 * 6.075702667236328
Epoch 530, val loss: 0.6847193241119385
Epoch 540, training loss: 6.344048023223877 = 0.2609884440898895 + 1.0 * 6.083059787750244
Epoch 540, val loss: 0.6815639734268188
Epoch 550, training loss: 6.325714111328125 = 0.24871912598609924 + 1.0 * 6.076994895935059
Epoch 550, val loss: 0.6786723136901855
Epoch 560, training loss: 6.307861328125 = 0.23684574663639069 + 1.0 * 6.071015357971191
Epoch 560, val loss: 0.6761379837989807
Epoch 570, training loss: 6.294099807739258 = 0.2252376228570938 + 1.0 * 6.068861961364746
Epoch 570, val loss: 0.6739659905433655
Epoch 580, training loss: 6.288249492645264 = 0.21393220126628876 + 1.0 * 6.074317455291748
Epoch 580, val loss: 0.672061026096344
Epoch 590, training loss: 6.269454002380371 = 0.2030407339334488 + 1.0 * 6.066413402557373
Epoch 590, val loss: 0.6704391241073608
Epoch 600, training loss: 6.271181106567383 = 0.1926012933254242 + 1.0 * 6.078579902648926
Epoch 600, val loss: 0.669158935546875
Epoch 610, training loss: 6.248197555541992 = 0.18264050781726837 + 1.0 * 6.06555700302124
Epoch 610, val loss: 0.6681490540504456
Epoch 620, training loss: 6.236340522766113 = 0.17315666377544403 + 1.0 * 6.063183784484863
Epoch 620, val loss: 0.667428731918335
Epoch 630, training loss: 6.22481632232666 = 0.16410838067531586 + 1.0 * 6.060708045959473
Epoch 630, val loss: 0.6670410633087158
Epoch 640, training loss: 6.227179527282715 = 0.15549686551094055 + 1.0 * 6.071682453155518
Epoch 640, val loss: 0.6669905185699463
Epoch 650, training loss: 6.209936618804932 = 0.14742812514305115 + 1.0 * 6.062508583068848
Epoch 650, val loss: 0.6671198606491089
Epoch 660, training loss: 6.198489189147949 = 0.13982640206813812 + 1.0 * 6.0586628913879395
Epoch 660, val loss: 0.6675114631652832
Epoch 670, training loss: 6.193131446838379 = 0.13263198733329773 + 1.0 * 6.060499668121338
Epoch 670, val loss: 0.6682519316673279
Epoch 680, training loss: 6.183565139770508 = 0.12584112584590912 + 1.0 * 6.0577239990234375
Epoch 680, val loss: 0.66914963722229
Epoch 690, training loss: 6.174527168273926 = 0.1194283589720726 + 1.0 * 6.055099010467529
Epoch 690, val loss: 0.6703243851661682
Epoch 700, training loss: 6.173747539520264 = 0.11333621293306351 + 1.0 * 6.06041145324707
Epoch 700, val loss: 0.671761155128479
Epoch 710, training loss: 6.162993907928467 = 0.10756689310073853 + 1.0 * 6.055427074432373
Epoch 710, val loss: 0.673323392868042
Epoch 720, training loss: 6.1541924476623535 = 0.10206975787878036 + 1.0 * 6.052122592926025
Epoch 720, val loss: 0.6750351190567017
Epoch 730, training loss: 6.147047996520996 = 0.0968133732676506 + 1.0 * 6.050234794616699
Epoch 730, val loss: 0.6769603490829468
Epoch 740, training loss: 6.153987884521484 = 0.09179045259952545 + 1.0 * 6.062197208404541
Epoch 740, val loss: 0.6790069341659546
Epoch 750, training loss: 6.135430335998535 = 0.08702440559864044 + 1.0 * 6.04840612411499
Epoch 750, val loss: 0.6811927556991577
Epoch 760, training loss: 6.131236553192139 = 0.0825294628739357 + 1.0 * 6.048707008361816
Epoch 760, val loss: 0.6835787296295166
Epoch 770, training loss: 6.127889633178711 = 0.07834602892398834 + 1.0 * 6.049543380737305
Epoch 770, val loss: 0.6862692832946777
Epoch 780, training loss: 6.1220903396606445 = 0.07452880591154099 + 1.0 * 6.0475616455078125
Epoch 780, val loss: 0.6892695426940918
Epoch 790, training loss: 6.117690086364746 = 0.07104489952325821 + 1.0 * 6.046645164489746
Epoch 790, val loss: 0.6926044225692749
Epoch 800, training loss: 6.1124444007873535 = 0.06781096756458282 + 1.0 * 6.044633388519287
Epoch 800, val loss: 0.696098268032074
Epoch 810, training loss: 6.115108013153076 = 0.06478586792945862 + 1.0 * 6.05032205581665
Epoch 810, val loss: 0.6994214653968811
Epoch 820, training loss: 6.106825351715088 = 0.061950452625751495 + 1.0 * 6.044874668121338
Epoch 820, val loss: 0.702694296836853
Epoch 830, training loss: 6.1013994216918945 = 0.05930107459425926 + 1.0 * 6.042098522186279
Epoch 830, val loss: 0.7059940695762634
Epoch 840, training loss: 6.096768379211426 = 0.05679694190621376 + 1.0 * 6.039971351623535
Epoch 840, val loss: 0.7094108462333679
Epoch 850, training loss: 6.112670421600342 = 0.054433293640613556 + 1.0 * 6.058237075805664
Epoch 850, val loss: 0.7129018306732178
Epoch 860, training loss: 6.093348979949951 = 0.05223533883690834 + 1.0 * 6.04111385345459
Epoch 860, val loss: 0.7163219451904297
Epoch 870, training loss: 6.088911533355713 = 0.05018112435936928 + 1.0 * 6.038730621337891
Epoch 870, val loss: 0.719763457775116
Epoch 880, training loss: 6.085294723510742 = 0.048234131187200546 + 1.0 * 6.037060737609863
Epoch 880, val loss: 0.7232338190078735
Epoch 890, training loss: 6.081943988800049 = 0.04637621343135834 + 1.0 * 6.035567760467529
Epoch 890, val loss: 0.7267576456069946
Epoch 900, training loss: 6.085662841796875 = 0.0446036159992218 + 1.0 * 6.0410590171813965
Epoch 900, val loss: 0.7302811145782471
Epoch 910, training loss: 6.085338115692139 = 0.0429200753569603 + 1.0 * 6.042418003082275
Epoch 910, val loss: 0.7339174151420593
Epoch 920, training loss: 6.074532508850098 = 0.041342806071043015 + 1.0 * 6.03318977355957
Epoch 920, val loss: 0.7373150587081909
Epoch 930, training loss: 6.0738677978515625 = 0.03984445706009865 + 1.0 * 6.034023284912109
Epoch 930, val loss: 0.7407585978507996
Epoch 940, training loss: 6.0846686363220215 = 0.038413356989622116 + 1.0 * 6.046255111694336
Epoch 940, val loss: 0.7442573308944702
Epoch 950, training loss: 6.070098876953125 = 0.03706274926662445 + 1.0 * 6.033036231994629
Epoch 950, val loss: 0.7476746439933777
Epoch 960, training loss: 6.066651344299316 = 0.03578110411763191 + 1.0 * 6.03087043762207
Epoch 960, val loss: 0.7510307431221008
Epoch 970, training loss: 6.064548969268799 = 0.03455933555960655 + 1.0 * 6.029989719390869
Epoch 970, val loss: 0.7544375658035278
Epoch 980, training loss: 6.07072114944458 = 0.03339194506406784 + 1.0 * 6.037329196929932
Epoch 980, val loss: 0.7578328847885132
Epoch 990, training loss: 6.067056179046631 = 0.032278694212436676 + 1.0 * 6.034777641296387
Epoch 990, val loss: 0.7612113952636719
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6089
Flip ASR: 0.5778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.312009811401367 = 1.9382140636444092 + 1.0 * 8.373795509338379
Epoch 0, val loss: 1.9337122440338135
Epoch 10, training loss: 10.301436424255371 = 1.9283008575439453 + 1.0 * 8.373135566711426
Epoch 10, val loss: 1.9240801334381104
Epoch 20, training loss: 10.284674644470215 = 1.9157623052597046 + 1.0 * 8.368912696838379
Epoch 20, val loss: 1.9113245010375977
Epoch 30, training loss: 10.237317085266113 = 1.8985460996627808 + 1.0 * 8.338770866394043
Epoch 30, val loss: 1.8934816122055054
Epoch 40, training loss: 9.99864387512207 = 1.877169132232666 + 1.0 * 8.121475219726562
Epoch 40, val loss: 1.8720464706420898
Epoch 50, training loss: 9.174320220947266 = 1.8529342412948608 + 1.0 * 7.321386337280273
Epoch 50, val loss: 1.8476439714431763
Epoch 60, training loss: 8.854701042175293 = 1.8333739042282104 + 1.0 * 7.021327495574951
Epoch 60, val loss: 1.829369306564331
Epoch 70, training loss: 8.5738525390625 = 1.8197150230407715 + 1.0 * 6.754137992858887
Epoch 70, val loss: 1.8165819644927979
Epoch 80, training loss: 8.412666320800781 = 1.8053044080734253 + 1.0 * 6.607362270355225
Epoch 80, val loss: 1.8028616905212402
Epoch 90, training loss: 8.305585861206055 = 1.7910624742507935 + 1.0 * 6.514523506164551
Epoch 90, val loss: 1.789138674736023
Epoch 100, training loss: 8.239152908325195 = 1.7766492366790771 + 1.0 * 6.462503910064697
Epoch 100, val loss: 1.7750879526138306
Epoch 110, training loss: 8.167160034179688 = 1.7626633644104004 + 1.0 * 6.404496669769287
Epoch 110, val loss: 1.7618598937988281
Epoch 120, training loss: 8.103792190551758 = 1.748486876487732 + 1.0 * 6.3553056716918945
Epoch 120, val loss: 1.7486873865127563
Epoch 130, training loss: 8.053643226623535 = 1.7324731349945068 + 1.0 * 6.321170330047607
Epoch 130, val loss: 1.7339626550674438
Epoch 140, training loss: 8.005520820617676 = 1.7136340141296387 + 1.0 * 6.291886806488037
Epoch 140, val loss: 1.7171056270599365
Epoch 150, training loss: 7.958588123321533 = 1.6914864778518677 + 1.0 * 6.267101764678955
Epoch 150, val loss: 1.697766900062561
Epoch 160, training loss: 7.910888195037842 = 1.6647344827651978 + 1.0 * 6.246153831481934
Epoch 160, val loss: 1.674979329109192
Epoch 170, training loss: 7.862135410308838 = 1.6318049430847168 + 1.0 * 6.230330467224121
Epoch 170, val loss: 1.6474064588546753
Epoch 180, training loss: 7.807159900665283 = 1.5917762517929077 + 1.0 * 6.215383529663086
Epoch 180, val loss: 1.614243745803833
Epoch 190, training loss: 7.747337341308594 = 1.5431615114212036 + 1.0 * 6.20417594909668
Epoch 190, val loss: 1.574000597000122
Epoch 200, training loss: 7.681604385375977 = 1.4853899478912354 + 1.0 * 6.196214199066162
Epoch 200, val loss: 1.5264681577682495
Epoch 210, training loss: 7.607445240020752 = 1.4199714660644531 + 1.0 * 6.187473773956299
Epoch 210, val loss: 1.4731005430221558
Epoch 220, training loss: 7.533381462097168 = 1.3490427732467651 + 1.0 * 6.184338569641113
Epoch 220, val loss: 1.4159845113754272
Epoch 230, training loss: 7.452746868133545 = 1.2771042585372925 + 1.0 * 6.175642490386963
Epoch 230, val loss: 1.3585172891616821
Epoch 240, training loss: 7.373637676239014 = 1.2051461935043335 + 1.0 * 6.168491363525391
Epoch 240, val loss: 1.301378607749939
Epoch 250, training loss: 7.297392845153809 = 1.1340116262435913 + 1.0 * 6.163381099700928
Epoch 250, val loss: 1.2452319860458374
Epoch 260, training loss: 7.226761341094971 = 1.0662546157836914 + 1.0 * 6.160506725311279
Epoch 260, val loss: 1.1925216913223267
Epoch 270, training loss: 7.154524803161621 = 1.002101182937622 + 1.0 * 6.152423858642578
Epoch 270, val loss: 1.143249750137329
Epoch 280, training loss: 7.090899467468262 = 0.940864622592926 + 1.0 * 6.1500349044799805
Epoch 280, val loss: 1.0964583158493042
Epoch 290, training loss: 7.025454044342041 = 0.8834519386291504 + 1.0 * 6.142002105712891
Epoch 290, val loss: 1.0529084205627441
Epoch 300, training loss: 6.965606212615967 = 0.829250693321228 + 1.0 * 6.136355400085449
Epoch 300, val loss: 1.0121148824691772
Epoch 310, training loss: 6.913857460021973 = 0.778792142868042 + 1.0 * 6.135065078735352
Epoch 310, val loss: 0.9741775989532471
Epoch 320, training loss: 6.8600006103515625 = 0.7323683500289917 + 1.0 * 6.127632141113281
Epoch 320, val loss: 0.9396622776985168
Epoch 330, training loss: 6.812716484069824 = 0.6890993118286133 + 1.0 * 6.123617172241211
Epoch 330, val loss: 0.9078707695007324
Epoch 340, training loss: 6.769344329833984 = 0.6491274833679199 + 1.0 * 6.1202168464660645
Epoch 340, val loss: 0.8790163397789001
Epoch 350, training loss: 6.726878643035889 = 0.6121540665626526 + 1.0 * 6.114724636077881
Epoch 350, val loss: 0.8531094789505005
Epoch 360, training loss: 6.693326473236084 = 0.5776264071464539 + 1.0 * 6.1157002449035645
Epoch 360, val loss: 0.8296281099319458
Epoch 370, training loss: 6.656002044677734 = 0.5457050800323486 + 1.0 * 6.110297203063965
Epoch 370, val loss: 0.8087510466575623
Epoch 380, training loss: 6.623360633850098 = 0.5160123109817505 + 1.0 * 6.107348442077637
Epoch 380, val loss: 0.7902432084083557
Epoch 390, training loss: 6.592411994934082 = 0.48813581466674805 + 1.0 * 6.104276180267334
Epoch 390, val loss: 0.7736806273460388
Epoch 400, training loss: 6.561439037322998 = 0.46178919076919556 + 1.0 * 6.099649906158447
Epoch 400, val loss: 0.7587431073188782
Epoch 410, training loss: 6.5391387939453125 = 0.4368274211883545 + 1.0 * 6.102311611175537
Epoch 410, val loss: 0.7452039122581482
Epoch 420, training loss: 6.511335849761963 = 0.4134868383407593 + 1.0 * 6.097848892211914
Epoch 420, val loss: 0.7331014275550842
Epoch 430, training loss: 6.484723091125488 = 0.3913410007953644 + 1.0 * 6.093381881713867
Epoch 430, val loss: 0.7222685217857361
Epoch 440, training loss: 6.462167263031006 = 0.3701508343219757 + 1.0 * 6.092016220092773
Epoch 440, val loss: 0.7125229239463806
Epoch 450, training loss: 6.438472747802734 = 0.34998762607574463 + 1.0 * 6.088485240936279
Epoch 450, val loss: 0.7038663029670715
Epoch 460, training loss: 6.4185638427734375 = 0.3307833969593048 + 1.0 * 6.087780475616455
Epoch 460, val loss: 0.6962528228759766
Epoch 470, training loss: 6.404027938842773 = 0.3124399185180664 + 1.0 * 6.091588020324707
Epoch 470, val loss: 0.6895623803138733
Epoch 480, training loss: 6.3794121742248535 = 0.295026570558548 + 1.0 * 6.084385395050049
Epoch 480, val loss: 0.6838353872299194
Epoch 490, training loss: 6.35910177230835 = 0.27839407324790955 + 1.0 * 6.080707550048828
Epoch 490, val loss: 0.6789476275444031
Epoch 500, training loss: 6.34304141998291 = 0.26256805658340454 + 1.0 * 6.08047342300415
Epoch 500, val loss: 0.6748924255371094
Epoch 510, training loss: 6.325981140136719 = 0.24770604074001312 + 1.0 * 6.078275203704834
Epoch 510, val loss: 0.6716076135635376
Epoch 520, training loss: 6.316111087799072 = 0.2335924208164215 + 1.0 * 6.082518577575684
Epoch 520, val loss: 0.6690354943275452
Epoch 530, training loss: 6.296485424041748 = 0.22023066878318787 + 1.0 * 6.076254844665527
Epoch 530, val loss: 0.6672782897949219
Epoch 540, training loss: 6.279570579528809 = 0.20754948258399963 + 1.0 * 6.072021007537842
Epoch 540, val loss: 0.6660959720611572
Epoch 550, training loss: 6.274631500244141 = 0.19552870094776154 + 1.0 * 6.079102993011475
Epoch 550, val loss: 0.6655595302581787
Epoch 560, training loss: 6.255629062652588 = 0.1842460036277771 + 1.0 * 6.071382999420166
Epoch 560, val loss: 0.6656452417373657
Epoch 570, training loss: 6.241037368774414 = 0.17361131310462952 + 1.0 * 6.0674262046813965
Epoch 570, val loss: 0.666352391242981
Epoch 580, training loss: 6.235861301422119 = 0.16356083750724792 + 1.0 * 6.072300434112549
Epoch 580, val loss: 0.6676673293113708
Epoch 590, training loss: 6.222132682800293 = 0.15417741239070892 + 1.0 * 6.067955493927002
Epoch 590, val loss: 0.6695008277893066
Epoch 600, training loss: 6.209905624389648 = 0.14540158212184906 + 1.0 * 6.064504146575928
Epoch 600, val loss: 0.6717017889022827
Epoch 610, training loss: 6.2097907066345215 = 0.13717930018901825 + 1.0 * 6.072611331939697
Epoch 610, val loss: 0.6744834184646606
Epoch 620, training loss: 6.191686153411865 = 0.12952393293380737 + 1.0 * 6.062162399291992
Epoch 620, val loss: 0.6776902675628662
Epoch 630, training loss: 6.182332515716553 = 0.1223917230963707 + 1.0 * 6.059940814971924
Epoch 630, val loss: 0.6811087727546692
Epoch 640, training loss: 6.177194118499756 = 0.11570220440626144 + 1.0 * 6.061491966247559
Epoch 640, val loss: 0.6850922107696533
Epoch 650, training loss: 6.167342662811279 = 0.10945389419794083 + 1.0 * 6.057888984680176
Epoch 650, val loss: 0.6893634796142578
Epoch 660, training loss: 6.159882545471191 = 0.1036304384469986 + 1.0 * 6.0562520027160645
Epoch 660, val loss: 0.6937999129295349
Epoch 670, training loss: 6.160364151000977 = 0.09816791117191315 + 1.0 * 6.062196254730225
Epoch 670, val loss: 0.6986855268478394
Epoch 680, training loss: 6.149324417114258 = 0.09309961646795273 + 1.0 * 6.056224822998047
Epoch 680, val loss: 0.703736424446106
Epoch 690, training loss: 6.141324996948242 = 0.08835773169994354 + 1.0 * 6.052967071533203
Epoch 690, val loss: 0.7089311480522156
Epoch 700, training loss: 6.135349273681641 = 0.08392376452684402 + 1.0 * 6.051425457000732
Epoch 700, val loss: 0.7144328951835632
Epoch 710, training loss: 6.131082534790039 = 0.07977243512868881 + 1.0 * 6.051310062408447
Epoch 710, val loss: 0.7199969291687012
Epoch 720, training loss: 6.128847122192383 = 0.07588167488574982 + 1.0 * 6.0529656410217285
Epoch 720, val loss: 0.7257372736930847
Epoch 730, training loss: 6.1190409660339355 = 0.07225008308887482 + 1.0 * 6.046791076660156
Epoch 730, val loss: 0.7315227389335632
Epoch 740, training loss: 6.1158013343811035 = 0.0688432976603508 + 1.0 * 6.046957969665527
Epoch 740, val loss: 0.7373725771903992
Epoch 750, training loss: 6.11736536026001 = 0.0656527578830719 + 1.0 * 6.051712512969971
Epoch 750, val loss: 0.7434248924255371
Epoch 760, training loss: 6.110200881958008 = 0.06270067393779755 + 1.0 * 6.047500133514404
Epoch 760, val loss: 0.749295711517334
Epoch 770, training loss: 6.103860378265381 = 0.059934888035058975 + 1.0 * 6.0439252853393555
Epoch 770, val loss: 0.7551029920578003
Epoch 780, training loss: 6.099862575531006 = 0.057319045066833496 + 1.0 * 6.042543411254883
Epoch 780, val loss: 0.7611028552055359
Epoch 790, training loss: 6.105031490325928 = 0.05484228581190109 + 1.0 * 6.050189018249512
Epoch 790, val loss: 0.7670648694038391
Epoch 800, training loss: 6.096388339996338 = 0.05252043530344963 + 1.0 * 6.043868064880371
Epoch 800, val loss: 0.7730247378349304
Epoch 810, training loss: 6.090561866760254 = 0.05033331736922264 + 1.0 * 6.040228366851807
Epoch 810, val loss: 0.7789646983146667
Epoch 820, training loss: 6.09175443649292 = 0.04826965928077698 + 1.0 * 6.043484687805176
Epoch 820, val loss: 0.7849217057228088
Epoch 830, training loss: 6.08677864074707 = 0.04632537066936493 + 1.0 * 6.0404534339904785
Epoch 830, val loss: 0.7908331751823425
Epoch 840, training loss: 6.084552764892578 = 0.04449612274765968 + 1.0 * 6.0400567054748535
Epoch 840, val loss: 0.7967129349708557
Epoch 850, training loss: 6.080018043518066 = 0.04278069734573364 + 1.0 * 6.037237167358398
Epoch 850, val loss: 0.8024190068244934
Epoch 860, training loss: 6.076410293579102 = 0.0411560982465744 + 1.0 * 6.035254001617432
Epoch 860, val loss: 0.8080897331237793
Epoch 870, training loss: 6.073851585388184 = 0.03961116075515747 + 1.0 * 6.034240245819092
Epoch 870, val loss: 0.8138182163238525
Epoch 880, training loss: 6.079380512237549 = 0.03814229741692543 + 1.0 * 6.041238307952881
Epoch 880, val loss: 0.8195182681083679
Epoch 890, training loss: 6.074213981628418 = 0.03675604239106178 + 1.0 * 6.0374579429626465
Epoch 890, val loss: 0.8250714540481567
Epoch 900, training loss: 6.068429470062256 = 0.03545331954956055 + 1.0 * 6.032976150512695
Epoch 900, val loss: 0.8305086493492126
Epoch 910, training loss: 6.070659160614014 = 0.034214869141578674 + 1.0 * 6.036444187164307
Epoch 910, val loss: 0.8359984755516052
Epoch 920, training loss: 6.064983367919922 = 0.03304047882556915 + 1.0 * 6.031942844390869
Epoch 920, val loss: 0.8414453864097595
Epoch 930, training loss: 6.06547212600708 = 0.03192565590143204 + 1.0 * 6.033546447753906
Epoch 930, val loss: 0.8467056155204773
Epoch 940, training loss: 6.063509464263916 = 0.03086729720234871 + 1.0 * 6.032642364501953
Epoch 940, val loss: 0.8519798517227173
Epoch 950, training loss: 6.06168270111084 = 0.029861006885766983 + 1.0 * 6.0318217277526855
Epoch 950, val loss: 0.8571574091911316
Epoch 960, training loss: 6.057521343231201 = 0.02890044078230858 + 1.0 * 6.028620719909668
Epoch 960, val loss: 0.8623346090316772
Epoch 970, training loss: 6.05710506439209 = 0.02798144891858101 + 1.0 * 6.029123783111572
Epoch 970, val loss: 0.8674437403678894
Epoch 980, training loss: 6.054253578186035 = 0.027104852721095085 + 1.0 * 6.027148723602295
Epoch 980, val loss: 0.8725809454917908
Epoch 990, training loss: 6.053372383117676 = 0.026272859424352646 + 1.0 * 6.027099609375
Epoch 990, val loss: 0.8775212168693542
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7823
Flip ASR: 0.7467/225 nodes
The final ASR:0.65806, 0.08848, Accuracy:0.82469, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.316855430603027 = 1.9429748058319092 + 1.0 * 8.373880386352539
Epoch 0, val loss: 1.9469728469848633
Epoch 10, training loss: 10.307065963745117 = 1.933558702468872 + 1.0 * 8.373507499694824
Epoch 10, val loss: 1.937788724899292
Epoch 20, training loss: 10.292852401733398 = 1.9218839406967163 + 1.0 * 8.37096881866455
Epoch 20, val loss: 1.9257192611694336
Epoch 30, training loss: 10.25767993927002 = 1.9057466983795166 + 1.0 * 8.351933479309082
Epoch 30, val loss: 1.9085880517959595
Epoch 40, training loss: 10.103864669799805 = 1.8849873542785645 + 1.0 * 8.218877792358398
Epoch 40, val loss: 1.887107491493225
Epoch 50, training loss: 9.395276069641113 = 1.8619894981384277 + 1.0 * 7.5332865715026855
Epoch 50, val loss: 1.8644826412200928
Epoch 60, training loss: 8.905497550964355 = 1.8443299531936646 + 1.0 * 7.061167240142822
Epoch 60, val loss: 1.848434329032898
Epoch 70, training loss: 8.61340045928955 = 1.8312736749649048 + 1.0 * 6.782126426696777
Epoch 70, val loss: 1.8348314762115479
Epoch 80, training loss: 8.46187973022461 = 1.8175718784332275 + 1.0 * 6.644307613372803
Epoch 80, val loss: 1.8208847045898438
Epoch 90, training loss: 8.361654281616211 = 1.803246259689331 + 1.0 * 6.558407783508301
Epoch 90, val loss: 1.8064122200012207
Epoch 100, training loss: 8.297769546508789 = 1.789249062538147 + 1.0 * 6.508520603179932
Epoch 100, val loss: 1.7926472425460815
Epoch 110, training loss: 8.24215030670166 = 1.7757798433303833 + 1.0 * 6.466370582580566
Epoch 110, val loss: 1.7795895338058472
Epoch 120, training loss: 8.186739921569824 = 1.762660264968872 + 1.0 * 6.424079418182373
Epoch 120, val loss: 1.7672853469848633
Epoch 130, training loss: 8.136858940124512 = 1.7487281560897827 + 1.0 * 6.388131141662598
Epoch 130, val loss: 1.7546391487121582
Epoch 140, training loss: 8.09178638458252 = 1.732438325881958 + 1.0 * 6.359347820281982
Epoch 140, val loss: 1.7404028177261353
Epoch 150, training loss: 8.049148559570312 = 1.7129380702972412 + 1.0 * 6.33621072769165
Epoch 150, val loss: 1.7238622903823853
Epoch 160, training loss: 8.006868362426758 = 1.6895976066589355 + 1.0 * 6.3172712326049805
Epoch 160, val loss: 1.70457124710083
Epoch 170, training loss: 7.957752227783203 = 1.661738634109497 + 1.0 * 6.296013355255127
Epoch 170, val loss: 1.6819781064987183
Epoch 180, training loss: 7.905409812927246 = 1.6279829740524292 + 1.0 * 6.277426719665527
Epoch 180, val loss: 1.655055046081543
Epoch 190, training loss: 7.847218990325928 = 1.5870329141616821 + 1.0 * 6.260186195373535
Epoch 190, val loss: 1.6224099397659302
Epoch 200, training loss: 7.783156394958496 = 1.536950945854187 + 1.0 * 6.2462053298950195
Epoch 200, val loss: 1.5825880765914917
Epoch 210, training loss: 7.713980674743652 = 1.4764668941497803 + 1.0 * 6.237513542175293
Epoch 210, val loss: 1.5345122814178467
Epoch 220, training loss: 7.634276390075684 = 1.407259225845337 + 1.0 * 6.227017402648926
Epoch 220, val loss: 1.479187250137329
Epoch 230, training loss: 7.549192428588867 = 1.3304073810577393 + 1.0 * 6.218784809112549
Epoch 230, val loss: 1.4178557395935059
Epoch 240, training loss: 7.462780952453613 = 1.2488290071487427 + 1.0 * 6.21395206451416
Epoch 240, val loss: 1.3532499074935913
Epoch 250, training loss: 7.374109268188477 = 1.1680601835250854 + 1.0 * 6.206048965454102
Epoch 250, val loss: 1.2894642353057861
Epoch 260, training loss: 7.289360523223877 = 1.0891436338424683 + 1.0 * 6.200216770172119
Epoch 260, val loss: 1.227291226387024
Epoch 270, training loss: 7.21234130859375 = 1.0134999752044678 + 1.0 * 6.198841094970703
Epoch 270, val loss: 1.1681514978408813
Epoch 280, training loss: 7.136983871459961 = 0.9444468021392822 + 1.0 * 6.1925368309021
Epoch 280, val loss: 1.1142305135726929
Epoch 290, training loss: 7.065849304199219 = 0.8813433051109314 + 1.0 * 6.184505939483643
Epoch 290, val loss: 1.0653218030929565
Epoch 300, training loss: 7.000445365905762 = 0.8222993016242981 + 1.0 * 6.178145885467529
Epoch 300, val loss: 1.0198315382003784
Epoch 310, training loss: 6.945507526397705 = 0.7669784426689148 + 1.0 * 6.178529262542725
Epoch 310, val loss: 0.9775373935699463
Epoch 320, training loss: 6.887042045593262 = 0.7162731289863586 + 1.0 * 6.170768737792969
Epoch 320, val loss: 0.9390674233436584
Epoch 330, training loss: 6.833610534667969 = 0.6694818735122681 + 1.0 * 6.16412878036499
Epoch 330, val loss: 0.9043166637420654
Epoch 340, training loss: 6.787799835205078 = 0.6263653635978699 + 1.0 * 6.161434650421143
Epoch 340, val loss: 0.8731517791748047
Epoch 350, training loss: 6.742224216461182 = 0.5868204236030579 + 1.0 * 6.1554036140441895
Epoch 350, val loss: 0.8455753326416016
Epoch 360, training loss: 6.709314823150635 = 0.5500554442405701 + 1.0 * 6.15925931930542
Epoch 360, val loss: 0.8210930228233337
Epoch 370, training loss: 6.663463115692139 = 0.5160489082336426 + 1.0 * 6.147414207458496
Epoch 370, val loss: 0.7997743487358093
Epoch 380, training loss: 6.626706123352051 = 0.48422011733055115 + 1.0 * 6.142486095428467
Epoch 380, val loss: 0.7811393141746521
Epoch 390, training loss: 6.5921711921691895 = 0.4540042281150818 + 1.0 * 6.138166904449463
Epoch 390, val loss: 0.7646023035049438
Epoch 400, training loss: 6.566948890686035 = 0.4254175126552582 + 1.0 * 6.141531467437744
Epoch 400, val loss: 0.7501577138900757
Epoch 410, training loss: 6.531275749206543 = 0.398672491312027 + 1.0 * 6.132603168487549
Epoch 410, val loss: 0.7377588152885437
Epoch 420, training loss: 6.504461765289307 = 0.373260498046875 + 1.0 * 6.131201267242432
Epoch 420, val loss: 0.7270997166633606
Epoch 430, training loss: 6.475916385650635 = 0.34911102056503296 + 1.0 * 6.126805305480957
Epoch 430, val loss: 0.7180857062339783
Epoch 440, training loss: 6.455938816070557 = 0.32624348998069763 + 1.0 * 6.129695415496826
Epoch 440, val loss: 0.7106829285621643
Epoch 450, training loss: 6.427004337310791 = 0.30481651425361633 + 1.0 * 6.122187614440918
Epoch 450, val loss: 0.7047340273857117
Epoch 460, training loss: 6.402700901031494 = 0.28461912274360657 + 1.0 * 6.118081569671631
Epoch 460, val loss: 0.7002531290054321
Epoch 470, training loss: 6.3801069259643555 = 0.2655177712440491 + 1.0 * 6.114589214324951
Epoch 470, val loss: 0.6970239281654358
Epoch 480, training loss: 6.361030101776123 = 0.24760663509368896 + 1.0 * 6.1134233474731445
Epoch 480, val loss: 0.695009708404541
Epoch 490, training loss: 6.340682506561279 = 0.2310301810503006 + 1.0 * 6.109652519226074
Epoch 490, val loss: 0.6941337585449219
Epoch 500, training loss: 6.325438022613525 = 0.21554845571517944 + 1.0 * 6.109889507293701
Epoch 500, val loss: 0.6943482160568237
Epoch 510, training loss: 6.312687873840332 = 0.20119044184684753 + 1.0 * 6.111497402191162
Epoch 510, val loss: 0.6954504251480103
Epoch 520, training loss: 6.291285514831543 = 0.18794527649879456 + 1.0 * 6.103340148925781
Epoch 520, val loss: 0.6973966360092163
Epoch 530, training loss: 6.2780256271362305 = 0.17565308511257172 + 1.0 * 6.102372646331787
Epoch 530, val loss: 0.7001171112060547
Epoch 540, training loss: 6.264344215393066 = 0.16429729759693146 + 1.0 * 6.1000471115112305
Epoch 540, val loss: 0.7034769654273987
Epoch 550, training loss: 6.2561445236206055 = 0.15385398268699646 + 1.0 * 6.102290630340576
Epoch 550, val loss: 0.7074374556541443
Epoch 560, training loss: 6.243908405303955 = 0.1442979872226715 + 1.0 * 6.099610328674316
Epoch 560, val loss: 0.7119024991989136
Epoch 570, training loss: 6.227964401245117 = 0.13552387058734894 + 1.0 * 6.092440605163574
Epoch 570, val loss: 0.7167935967445374
Epoch 580, training loss: 6.219196319580078 = 0.12740741670131683 + 1.0 * 6.0917887687683105
Epoch 580, val loss: 0.7220927476882935
Epoch 590, training loss: 6.209550857543945 = 0.11995037645101547 + 1.0 * 6.089600563049316
Epoch 590, val loss: 0.7276518940925598
Epoch 600, training loss: 6.203664779663086 = 0.11310194432735443 + 1.0 * 6.09056282043457
Epoch 600, val loss: 0.7335129380226135
Epoch 610, training loss: 6.192593574523926 = 0.10680138319730759 + 1.0 * 6.085792064666748
Epoch 610, val loss: 0.7394922375679016
Epoch 620, training loss: 6.18314266204834 = 0.10095835477113724 + 1.0 * 6.082184314727783
Epoch 620, val loss: 0.7457373738288879
Epoch 630, training loss: 6.181783676147461 = 0.09552419930696487 + 1.0 * 6.086259365081787
Epoch 630, val loss: 0.7521583437919617
Epoch 640, training loss: 6.174054145812988 = 0.09051158279180527 + 1.0 * 6.083542346954346
Epoch 640, val loss: 0.7585721015930176
Epoch 650, training loss: 6.164253234863281 = 0.0858759731054306 + 1.0 * 6.0783772468566895
Epoch 650, val loss: 0.765169084072113
Epoch 660, training loss: 6.163572788238525 = 0.0815412774682045 + 1.0 * 6.082031726837158
Epoch 660, val loss: 0.7718936800956726
Epoch 670, training loss: 6.156290531158447 = 0.07750365138053894 + 1.0 * 6.078786849975586
Epoch 670, val loss: 0.7785788774490356
Epoch 680, training loss: 6.146663188934326 = 0.07373917102813721 + 1.0 * 6.0729241371154785
Epoch 680, val loss: 0.7853909730911255
Epoch 690, training loss: 6.142336845397949 = 0.0702189952135086 + 1.0 * 6.072117805480957
Epoch 690, val loss: 0.7922817468643188
Epoch 700, training loss: 6.141324520111084 = 0.06691183894872665 + 1.0 * 6.074412822723389
Epoch 700, val loss: 0.7992300391197205
Epoch 710, training loss: 6.13320779800415 = 0.0638154000043869 + 1.0 * 6.069392204284668
Epoch 710, val loss: 0.8061836361885071
Epoch 720, training loss: 6.127779483795166 = 0.06091418117284775 + 1.0 * 6.06686544418335
Epoch 720, val loss: 0.8132051825523376
Epoch 730, training loss: 6.131237030029297 = 0.058175574988126755 + 1.0 * 6.073061466217041
Epoch 730, val loss: 0.820227861404419
Epoch 740, training loss: 6.125115394592285 = 0.055607423186302185 + 1.0 * 6.069508075714111
Epoch 740, val loss: 0.8272441625595093
Epoch 750, training loss: 6.114646911621094 = 0.05320441350340843 + 1.0 * 6.0614423751831055
Epoch 750, val loss: 0.8342115879058838
Epoch 760, training loss: 6.112770080566406 = 0.050935227423906326 + 1.0 * 6.061834812164307
Epoch 760, val loss: 0.8412280082702637
Epoch 770, training loss: 6.112547397613525 = 0.04878859594464302 + 1.0 * 6.063758850097656
Epoch 770, val loss: 0.8482170104980469
Epoch 780, training loss: 6.1087164878845215 = 0.046765025705099106 + 1.0 * 6.061951637268066
Epoch 780, val loss: 0.8551624417304993
Epoch 790, training loss: 6.107095241546631 = 0.044861238449811935 + 1.0 * 6.062233924865723
Epoch 790, val loss: 0.8620280623435974
Epoch 800, training loss: 6.1027936935424805 = 0.04307461157441139 + 1.0 * 6.059719085693359
Epoch 800, val loss: 0.8688474297523499
Epoch 810, training loss: 6.096686840057373 = 0.041387513279914856 + 1.0 * 6.055299282073975
Epoch 810, val loss: 0.8756213188171387
Epoch 820, training loss: 6.094661712646484 = 0.039781998842954636 + 1.0 * 6.054879665374756
Epoch 820, val loss: 0.8823506236076355
Epoch 830, training loss: 6.101462364196777 = 0.03825830668210983 + 1.0 * 6.063204288482666
Epoch 830, val loss: 0.8889598250389099
Epoch 840, training loss: 6.092512130737305 = 0.03683225437998772 + 1.0 * 6.055679798126221
Epoch 840, val loss: 0.8954889178276062
Epoch 850, training loss: 6.086884021759033 = 0.03548706695437431 + 1.0 * 6.05139684677124
Epoch 850, val loss: 0.9019890427589417
Epoch 860, training loss: 6.089011192321777 = 0.034204285591840744 + 1.0 * 6.054806709289551
Epoch 860, val loss: 0.9083940386772156
Epoch 870, training loss: 6.083117485046387 = 0.03298523277044296 + 1.0 * 6.0501322746276855
Epoch 870, val loss: 0.9146476984024048
Epoch 880, training loss: 6.082526206970215 = 0.03183364495635033 + 1.0 * 6.050692558288574
Epoch 880, val loss: 0.920859158039093
Epoch 890, training loss: 6.08219051361084 = 0.03074011579155922 + 1.0 * 6.051450252532959
Epoch 890, val loss: 0.9270117878913879
Epoch 900, training loss: 6.078427791595459 = 0.02970343455672264 + 1.0 * 6.048724174499512
Epoch 900, val loss: 0.9330152273178101
Epoch 910, training loss: 6.077394962310791 = 0.028714530169963837 + 1.0 * 6.048680305480957
Epoch 910, val loss: 0.9389792084693909
Epoch 920, training loss: 6.080058574676514 = 0.0277713593095541 + 1.0 * 6.0522871017456055
Epoch 920, val loss: 0.9448391795158386
Epoch 930, training loss: 6.074547290802002 = 0.026876911520957947 + 1.0 * 6.047670364379883
Epoch 930, val loss: 0.9505633115768433
Epoch 940, training loss: 6.07188081741333 = 0.02602776512503624 + 1.0 * 6.045853137969971
Epoch 940, val loss: 0.9562661647796631
Epoch 950, training loss: 6.0691633224487305 = 0.025216536596417427 + 1.0 * 6.043946743011475
Epoch 950, val loss: 0.9619051218032837
Epoch 960, training loss: 6.071131229400635 = 0.02443784475326538 + 1.0 * 6.046693325042725
Epoch 960, val loss: 0.9675122499465942
Epoch 970, training loss: 6.066072463989258 = 0.023693708702921867 + 1.0 * 6.042378902435303
Epoch 970, val loss: 0.9729519486427307
Epoch 980, training loss: 6.068410396575928 = 0.0229850672185421 + 1.0 * 6.0454254150390625
Epoch 980, val loss: 0.9783957004547119
Epoch 990, training loss: 6.065592288970947 = 0.02230914682149887 + 1.0 * 6.043282985687256
Epoch 990, val loss: 0.9837283492088318
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.325859069824219 = 1.9520314931869507 + 1.0 * 8.373827934265137
Epoch 0, val loss: 1.955134391784668
Epoch 10, training loss: 10.314699172973633 = 1.9413061141967773 + 1.0 * 8.373393058776855
Epoch 10, val loss: 1.9438748359680176
Epoch 20, training loss: 10.298616409301758 = 1.928370475769043 + 1.0 * 8.370245933532715
Epoch 20, val loss: 1.9299266338348389
Epoch 30, training loss: 10.256834983825684 = 1.910778284072876 + 1.0 * 8.346056938171387
Epoch 30, val loss: 1.910974144935608
Epoch 40, training loss: 10.060527801513672 = 1.888691782951355 + 1.0 * 8.171835899353027
Epoch 40, val loss: 1.8879224061965942
Epoch 50, training loss: 9.474663734436035 = 1.8638001680374146 + 1.0 * 7.61086368560791
Epoch 50, val loss: 1.8621655702590942
Epoch 60, training loss: 9.059894561767578 = 1.845025897026062 + 1.0 * 7.214869022369385
Epoch 60, val loss: 1.8441288471221924
Epoch 70, training loss: 8.744697570800781 = 1.8321157693862915 + 1.0 * 6.912581443786621
Epoch 70, val loss: 1.831532597541809
Epoch 80, training loss: 8.538229942321777 = 1.8179428577423096 + 1.0 * 6.720287322998047
Epoch 80, val loss: 1.8175040483474731
Epoch 90, training loss: 8.399381637573242 = 1.8044052124023438 + 1.0 * 6.594976425170898
Epoch 90, val loss: 1.8043485879898071
Epoch 100, training loss: 8.297905921936035 = 1.7923718690872192 + 1.0 * 6.505533695220947
Epoch 100, val loss: 1.792721152305603
Epoch 110, training loss: 8.218332290649414 = 1.781131386756897 + 1.0 * 6.437200546264648
Epoch 110, val loss: 1.781663179397583
Epoch 120, training loss: 8.158768653869629 = 1.769382357597351 + 1.0 * 6.3893866539001465
Epoch 120, val loss: 1.7701947689056396
Epoch 130, training loss: 8.106447219848633 = 1.7566765546798706 + 1.0 * 6.349771022796631
Epoch 130, val loss: 1.758568525314331
Epoch 140, training loss: 8.055024147033691 = 1.7430232763290405 + 1.0 * 6.3120012283325195
Epoch 140, val loss: 1.746791958808899
Epoch 150, training loss: 8.00739860534668 = 1.7272069454193115 + 1.0 * 6.280191421508789
Epoch 150, val loss: 1.7336829900741577
Epoch 160, training loss: 7.962831974029541 = 1.7079033851623535 + 1.0 * 6.2549285888671875
Epoch 160, val loss: 1.717833399772644
Epoch 170, training loss: 7.9214372634887695 = 1.6844183206558228 + 1.0 * 6.237019062042236
Epoch 170, val loss: 1.698657512664795
Epoch 180, training loss: 7.8766608238220215 = 1.6561903953552246 + 1.0 * 6.220470428466797
Epoch 180, val loss: 1.6757054328918457
Epoch 190, training loss: 7.829350471496582 = 1.622175693511963 + 1.0 * 6.207174777984619
Epoch 190, val loss: 1.6479332447052002
Epoch 200, training loss: 7.77632999420166 = 1.581565499305725 + 1.0 * 6.194764614105225
Epoch 200, val loss: 1.6146247386932373
Epoch 210, training loss: 7.720134735107422 = 1.5345189571380615 + 1.0 * 6.1856160163879395
Epoch 210, val loss: 1.5761585235595703
Epoch 220, training loss: 7.6596245765686035 = 1.4833130836486816 + 1.0 * 6.176311492919922
Epoch 220, val loss: 1.5343339443206787
Epoch 230, training loss: 7.596199035644531 = 1.4286035299301147 + 1.0 * 6.167595386505127
Epoch 230, val loss: 1.4899016618728638
Epoch 240, training loss: 7.5346269607543945 = 1.3722115755081177 + 1.0 * 6.162415504455566
Epoch 240, val loss: 1.4444591999053955
Epoch 250, training loss: 7.471341133117676 = 1.3166675567626953 + 1.0 * 6.1546735763549805
Epoch 250, val loss: 1.4000262022018433
Epoch 260, training loss: 7.410628795623779 = 1.2623497247695923 + 1.0 * 6.148279190063477
Epoch 260, val loss: 1.3569689989089966
Epoch 270, training loss: 7.357451438903809 = 1.2098557949066162 + 1.0 * 6.1475958824157715
Epoch 270, val loss: 1.3158973455429077
Epoch 280, training loss: 7.298486709594727 = 1.1599925756454468 + 1.0 * 6.13849401473999
Epoch 280, val loss: 1.2772802114486694
Epoch 290, training loss: 7.244597434997559 = 1.1117879152297974 + 1.0 * 6.132809638977051
Epoch 290, val loss: 1.2402079105377197
Epoch 300, training loss: 7.1972150802612305 = 1.0645573139190674 + 1.0 * 6.132658004760742
Epoch 300, val loss: 1.20415461063385
Epoch 310, training loss: 7.1443939208984375 = 1.0187947750091553 + 1.0 * 6.125599384307861
Epoch 310, val loss: 1.169702172279358
Epoch 320, training loss: 7.096880912780762 = 0.9743065237998962 + 1.0 * 6.122574329376221
Epoch 320, val loss: 1.1363290548324585
Epoch 330, training loss: 7.0497894287109375 = 0.9306356310844421 + 1.0 * 6.11915397644043
Epoch 330, val loss: 1.1036067008972168
Epoch 340, training loss: 7.004246711730957 = 0.8879661560058594 + 1.0 * 6.116280555725098
Epoch 340, val loss: 1.0718275308609009
Epoch 350, training loss: 6.9596943855285645 = 0.8465894460678101 + 1.0 * 6.113104820251465
Epoch 350, val loss: 1.0412440299987793
Epoch 360, training loss: 6.91807222366333 = 0.806178092956543 + 1.0 * 6.111894130706787
Epoch 360, val loss: 1.0114293098449707
Epoch 370, training loss: 6.879630088806152 = 0.767328143119812 + 1.0 * 6.112301826477051
Epoch 370, val loss: 0.982752799987793
Epoch 380, training loss: 6.836113929748535 = 0.7304306030273438 + 1.0 * 6.105683326721191
Epoch 380, val loss: 0.9559476375579834
Epoch 390, training loss: 6.797702312469482 = 0.6953327059745789 + 1.0 * 6.102369785308838
Epoch 390, val loss: 0.9306796193122864
Epoch 400, training loss: 6.76726770401001 = 0.6622642874717712 + 1.0 * 6.105003356933594
Epoch 400, val loss: 0.9073132276535034
Epoch 410, training loss: 6.734287261962891 = 0.6319021582603455 + 1.0 * 6.1023850440979
Epoch 410, val loss: 0.886242687702179
Epoch 420, training loss: 6.69973611831665 = 0.6042191386222839 + 1.0 * 6.095517158508301
Epoch 420, val loss: 0.8677493929862976
Epoch 430, training loss: 6.671328544616699 = 0.5784513354301453 + 1.0 * 6.092877388000488
Epoch 430, val loss: 0.851116418838501
Epoch 440, training loss: 6.644850254058838 = 0.554240882396698 + 1.0 * 6.090609550476074
Epoch 440, val loss: 0.8361585140228271
Epoch 450, training loss: 6.620236873626709 = 0.5313363671302795 + 1.0 * 6.088900566101074
Epoch 450, val loss: 0.8225820660591125
Epoch 460, training loss: 6.599760055541992 = 0.5096558332443237 + 1.0 * 6.090104103088379
Epoch 460, val loss: 0.8101864457130432
Epoch 470, training loss: 6.5754075050354 = 0.4890938103199005 + 1.0 * 6.086313724517822
Epoch 470, val loss: 0.7991716861724854
Epoch 480, training loss: 6.552430152893066 = 0.4691482484340668 + 1.0 * 6.083281993865967
Epoch 480, val loss: 0.7889634966850281
Epoch 490, training loss: 6.532803058624268 = 0.4495605528354645 + 1.0 * 6.083242416381836
Epoch 490, val loss: 0.7792550921440125
Epoch 500, training loss: 6.514491081237793 = 0.43034300208091736 + 1.0 * 6.084147930145264
Epoch 500, val loss: 0.7701672911643982
Epoch 510, training loss: 6.490907669067383 = 0.41130509972572327 + 1.0 * 6.0796027183532715
Epoch 510, val loss: 0.7615783214569092
Epoch 520, training loss: 6.469823360443115 = 0.3922729194164276 + 1.0 * 6.077550411224365
Epoch 520, val loss: 0.7534189820289612
Epoch 530, training loss: 6.4480485916137695 = 0.37311649322509766 + 1.0 * 6.074932098388672
Epoch 530, val loss: 0.7454407811164856
Epoch 540, training loss: 6.428351402282715 = 0.3539672791957855 + 1.0 * 6.0743842124938965
Epoch 540, val loss: 0.7376387119293213
Epoch 550, training loss: 6.406765937805176 = 0.3350561857223511 + 1.0 * 6.071709632873535
Epoch 550, val loss: 0.7304006814956665
Epoch 560, training loss: 6.386503219604492 = 0.3163415193557739 + 1.0 * 6.070161819458008
Epoch 560, val loss: 0.7236616015434265
Epoch 570, training loss: 6.380255699157715 = 0.2979314923286438 + 1.0 * 6.082324028015137
Epoch 570, val loss: 0.7173674702644348
Epoch 580, training loss: 6.350340366363525 = 0.28022924065589905 + 1.0 * 6.070111274719238
Epoch 580, val loss: 0.7117400765419006
Epoch 590, training loss: 6.331074237823486 = 0.26330286264419556 + 1.0 * 6.0677714347839355
Epoch 590, val loss: 0.7071812152862549
Epoch 600, training loss: 6.315438747406006 = 0.24717748165130615 + 1.0 * 6.06826114654541
Epoch 600, val loss: 0.7033705115318298
Epoch 610, training loss: 6.296631813049316 = 0.23200653493404388 + 1.0 * 6.064625263214111
Epoch 610, val loss: 0.7003462910652161
Epoch 620, training loss: 6.280344486236572 = 0.21781513094902039 + 1.0 * 6.062529563903809
Epoch 620, val loss: 0.6983155608177185
Epoch 630, training loss: 6.271649360656738 = 0.2045634537935257 + 1.0 * 6.0670857429504395
Epoch 630, val loss: 0.6969624161720276
Epoch 640, training loss: 6.256809234619141 = 0.1923491209745407 + 1.0 * 6.064460277557373
Epoch 640, val loss: 0.6963882446289062
Epoch 650, training loss: 6.241242408752441 = 0.18104644119739532 + 1.0 * 6.0601959228515625
Epoch 650, val loss: 0.6966673731803894
Epoch 660, training loss: 6.230388164520264 = 0.1705133318901062 + 1.0 * 6.059875011444092
Epoch 660, val loss: 0.6974460482597351
Epoch 670, training loss: 6.218630790710449 = 0.16071434319019318 + 1.0 * 6.057916641235352
Epoch 670, val loss: 0.6985825300216675
Epoch 680, training loss: 6.208859443664551 = 0.15163105726242065 + 1.0 * 6.0572285652160645
Epoch 680, val loss: 0.7004205584526062
Epoch 690, training loss: 6.205602169036865 = 0.14314836263656616 + 1.0 * 6.062453746795654
Epoch 690, val loss: 0.7026271224021912
Epoch 700, training loss: 6.192652702331543 = 0.13525456190109253 + 1.0 * 6.057398319244385
Epoch 700, val loss: 0.7050463557243347
Epoch 710, training loss: 6.180746555328369 = 0.1278965026140213 + 1.0 * 6.052850246429443
Epoch 710, val loss: 0.7080649137496948
Epoch 720, training loss: 6.175191402435303 = 0.12099943310022354 + 1.0 * 6.054192066192627
Epoch 720, val loss: 0.711231529712677
Epoch 730, training loss: 6.169390678405762 = 0.11456026136875153 + 1.0 * 6.054830551147461
Epoch 730, val loss: 0.7145480513572693
Epoch 740, training loss: 6.160547256469727 = 0.1085844412446022 + 1.0 * 6.051962852478027
Epoch 740, val loss: 0.7182551622390747
Epoch 750, training loss: 6.152421951293945 = 0.10299694538116455 + 1.0 * 6.04942512512207
Epoch 750, val loss: 0.7221638560295105
Epoch 760, training loss: 6.15084171295166 = 0.09776055067777634 + 1.0 * 6.053081035614014
Epoch 760, val loss: 0.726090133190155
Epoch 770, training loss: 6.14164400100708 = 0.09286900609731674 + 1.0 * 6.0487751960754395
Epoch 770, val loss: 0.7302464842796326
Epoch 780, training loss: 6.138463020324707 = 0.0882965475320816 + 1.0 * 6.050166606903076
Epoch 780, val loss: 0.7345418930053711
Epoch 790, training loss: 6.135528564453125 = 0.08402372896671295 + 1.0 * 6.051504611968994
Epoch 790, val loss: 0.7387333512306213
Epoch 800, training loss: 6.124717712402344 = 0.08004188537597656 + 1.0 * 6.044675827026367
Epoch 800, val loss: 0.7431812882423401
Epoch 810, training loss: 6.119417190551758 = 0.07631243020296097 + 1.0 * 6.043104648590088
Epoch 810, val loss: 0.7477691173553467
Epoch 820, training loss: 6.114516735076904 = 0.07279717177152634 + 1.0 * 6.041719436645508
Epoch 820, val loss: 0.7523573040962219
Epoch 830, training loss: 6.122946739196777 = 0.06948915868997574 + 1.0 * 6.053457736968994
Epoch 830, val loss: 0.7569071054458618
Epoch 840, training loss: 6.110806465148926 = 0.06638672947883606 + 1.0 * 6.044419765472412
Epoch 840, val loss: 0.761397659778595
Epoch 850, training loss: 6.104923248291016 = 0.06348609179258347 + 1.0 * 6.041437149047852
Epoch 850, val loss: 0.766204297542572
Epoch 860, training loss: 6.0996198654174805 = 0.06075644493103027 + 1.0 * 6.038863658905029
Epoch 860, val loss: 0.7709704041481018
Epoch 870, training loss: 6.100695610046387 = 0.05817181244492531 + 1.0 * 6.0425238609313965
Epoch 870, val loss: 0.7756165266036987
Epoch 880, training loss: 6.096821308135986 = 0.05573876574635506 + 1.0 * 6.041082382202148
Epoch 880, val loss: 0.7802022099494934
Epoch 890, training loss: 6.091170310974121 = 0.053453244268894196 + 1.0 * 6.037716865539551
Epoch 890, val loss: 0.785001277923584
Epoch 900, training loss: 6.092202663421631 = 0.051293689757585526 + 1.0 * 6.0409088134765625
Epoch 900, val loss: 0.7898038625717163
Epoch 910, training loss: 6.0911173820495605 = 0.049255192279815674 + 1.0 * 6.0418620109558105
Epoch 910, val loss: 0.7941976189613342
Epoch 920, training loss: 6.0817999839782715 = 0.04733980819582939 + 1.0 * 6.034460067749023
Epoch 920, val loss: 0.7990136742591858
Epoch 930, training loss: 6.079084396362305 = 0.0455256886780262 + 1.0 * 6.0335588455200195
Epoch 930, val loss: 0.8037942051887512
Epoch 940, training loss: 6.079402923583984 = 0.0437956340610981 + 1.0 * 6.03560733795166
Epoch 940, val loss: 0.8083565831184387
Epoch 950, training loss: 6.075136661529541 = 0.0421571359038353 + 1.0 * 6.032979488372803
Epoch 950, val loss: 0.8127784132957458
Epoch 960, training loss: 6.072916030883789 = 0.04061288759112358 + 1.0 * 6.032303333282471
Epoch 960, val loss: 0.8174321055412292
Epoch 970, training loss: 6.070980072021484 = 0.039146631956100464 + 1.0 * 6.031833648681641
Epoch 970, val loss: 0.8221204876899719
Epoch 980, training loss: 6.070410251617432 = 0.03774886205792427 + 1.0 * 6.032661437988281
Epoch 980, val loss: 0.8265641331672668
Epoch 990, training loss: 6.068898677825928 = 0.036422211676836014 + 1.0 * 6.032476425170898
Epoch 990, val loss: 0.8310666084289551
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6089
Flip ASR: 0.5600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.31503677368164 = 1.9412215948104858 + 1.0 * 8.373815536499023
Epoch 0, val loss: 1.941671371459961
Epoch 10, training loss: 10.302836418151855 = 1.9303385019302368 + 1.0 * 8.37249755859375
Epoch 10, val loss: 1.9300930500030518
Epoch 20, training loss: 10.283534049987793 = 1.9168672561645508 + 1.0 * 8.366666793823242
Epoch 20, val loss: 1.9151641130447388
Epoch 30, training loss: 10.24250602722168 = 1.8983744382858276 + 1.0 * 8.344131469726562
Epoch 30, val loss: 1.8943703174591064
Epoch 40, training loss: 10.082497596740723 = 1.8752145767211914 + 1.0 * 8.207283020019531
Epoch 40, val loss: 1.8692065477371216
Epoch 50, training loss: 9.486964225769043 = 1.8527367115020752 + 1.0 * 7.634227275848389
Epoch 50, val loss: 1.845476508140564
Epoch 60, training loss: 9.090864181518555 = 1.8312437534332275 + 1.0 * 7.259620189666748
Epoch 60, val loss: 1.824604868888855
Epoch 70, training loss: 8.862290382385254 = 1.8130077123641968 + 1.0 * 7.049283027648926
Epoch 70, val loss: 1.8072224855422974
Epoch 80, training loss: 8.628495216369629 = 1.7990748882293701 + 1.0 * 6.82942008972168
Epoch 80, val loss: 1.7940999269485474
Epoch 90, training loss: 8.416986465454102 = 1.7900620698928833 + 1.0 * 6.62692403793335
Epoch 90, val loss: 1.7844969034194946
Epoch 100, training loss: 8.293937683105469 = 1.780597448348999 + 1.0 * 6.513339996337891
Epoch 100, val loss: 1.7740976810455322
Epoch 110, training loss: 8.202844619750977 = 1.7692621946334839 + 1.0 * 6.433582782745361
Epoch 110, val loss: 1.762833595275879
Epoch 120, training loss: 8.148031234741211 = 1.7567130327224731 + 1.0 * 6.391318321228027
Epoch 120, val loss: 1.7513008117675781
Epoch 130, training loss: 8.094850540161133 = 1.7433918714523315 + 1.0 * 6.351458549499512
Epoch 130, val loss: 1.739501953125
Epoch 140, training loss: 8.047589302062988 = 1.729009985923767 + 1.0 * 6.31857967376709
Epoch 140, val loss: 1.7272628545761108
Epoch 150, training loss: 8.002476692199707 = 1.7123734951019287 + 1.0 * 6.290103435516357
Epoch 150, val loss: 1.7135404348373413
Epoch 160, training loss: 7.959380626678467 = 1.692216396331787 + 1.0 * 6.26716423034668
Epoch 160, val loss: 1.6974847316741943
Epoch 170, training loss: 7.913214206695557 = 1.6675900220870972 + 1.0 * 6.24562406539917
Epoch 170, val loss: 1.6782571077346802
Epoch 180, training loss: 7.869001388549805 = 1.6370813846588135 + 1.0 * 6.23192024230957
Epoch 180, val loss: 1.654409646987915
Epoch 190, training loss: 7.816287040710449 = 1.5997685194015503 + 1.0 * 6.216518402099609
Epoch 190, val loss: 1.6253827810287476
Epoch 200, training loss: 7.760319709777832 = 1.5550016164779663 + 1.0 * 6.205317974090576
Epoch 200, val loss: 1.5903302431106567
Epoch 210, training loss: 7.697925567626953 = 1.5027698278427124 + 1.0 * 6.195155620574951
Epoch 210, val loss: 1.5494272708892822
Epoch 220, training loss: 7.633371353149414 = 1.4453825950622559 + 1.0 * 6.187988758087158
Epoch 220, val loss: 1.5050920248031616
Epoch 230, training loss: 7.567895889282227 = 1.387127161026001 + 1.0 * 6.1807684898376465
Epoch 230, val loss: 1.4607664346694946
Epoch 240, training loss: 7.5036091804504395 = 1.3295025825500488 + 1.0 * 6.174106597900391
Epoch 240, val loss: 1.4171243906021118
Epoch 250, training loss: 7.440807342529297 = 1.2736395597457886 + 1.0 * 6.167167663574219
Epoch 250, val loss: 1.3759667873382568
Epoch 260, training loss: 7.39362907409668 = 1.2200777530670166 + 1.0 * 6.173551082611084
Epoch 260, val loss: 1.3373934030532837
Epoch 270, training loss: 7.327709197998047 = 1.1704760789871216 + 1.0 * 6.157233238220215
Epoch 270, val loss: 1.3019917011260986
Epoch 280, training loss: 7.273690700531006 = 1.1223372220993042 + 1.0 * 6.151353359222412
Epoch 280, val loss: 1.2684061527252197
Epoch 290, training loss: 7.220352649688721 = 1.073649287223816 + 1.0 * 6.146703243255615
Epoch 290, val loss: 1.2347874641418457
Epoch 300, training loss: 7.166443824768066 = 1.0233209133148193 + 1.0 * 6.143123149871826
Epoch 300, val loss: 1.199968695640564
Epoch 310, training loss: 7.11191463470459 = 0.9715942740440369 + 1.0 * 6.140320301055908
Epoch 310, val loss: 1.16390860080719
Epoch 320, training loss: 7.05608606338501 = 0.9192566871643066 + 1.0 * 6.136829376220703
Epoch 320, val loss: 1.1273527145385742
Epoch 330, training loss: 6.998623371124268 = 0.8664790391921997 + 1.0 * 6.132144451141357
Epoch 330, val loss: 1.0906057357788086
Epoch 340, training loss: 6.951349258422852 = 0.8152968883514404 + 1.0 * 6.136052131652832
Epoch 340, val loss: 1.054709553718567
Epoch 350, training loss: 6.89478063583374 = 0.7673870921134949 + 1.0 * 6.12739372253418
Epoch 350, val loss: 1.021726131439209
Epoch 360, training loss: 6.845296382904053 = 0.722999632358551 + 1.0 * 6.1222968101501465
Epoch 360, val loss: 0.9918484091758728
Epoch 370, training loss: 6.80084753036499 = 0.6823589205741882 + 1.0 * 6.118488788604736
Epoch 370, val loss: 0.9651193022727966
Epoch 380, training loss: 6.772182464599609 = 0.6454460620880127 + 1.0 * 6.126736640930176
Epoch 380, val loss: 0.9417732357978821
Epoch 390, training loss: 6.729404449462891 = 0.6128069758415222 + 1.0 * 6.116597652435303
Epoch 390, val loss: 0.9222040176391602
Epoch 400, training loss: 6.693936824798584 = 0.5835219025611877 + 1.0 * 6.110414981842041
Epoch 400, val loss: 0.9060632586479187
Epoch 410, training loss: 6.667820930480957 = 0.5567290186882019 + 1.0 * 6.1110920906066895
Epoch 410, val loss: 0.892406702041626
Epoch 420, training loss: 6.640377521514893 = 0.5323328375816345 + 1.0 * 6.108044624328613
Epoch 420, val loss: 0.880940318107605
Epoch 430, training loss: 6.611831188201904 = 0.5097066164016724 + 1.0 * 6.1021246910095215
Epoch 430, val loss: 0.8715325593948364
Epoch 440, training loss: 6.58695650100708 = 0.4884343445301056 + 1.0 * 6.098522186279297
Epoch 440, val loss: 0.8637036085128784
Epoch 450, training loss: 6.574094295501709 = 0.4682762324810028 + 1.0 * 6.105818271636963
Epoch 450, val loss: 0.8571348190307617
Epoch 460, training loss: 6.545145511627197 = 0.4491884112358093 + 1.0 * 6.095957279205322
Epoch 460, val loss: 0.8518162965774536
Epoch 470, training loss: 6.528470039367676 = 0.43102338910102844 + 1.0 * 6.097446441650391
Epoch 470, val loss: 0.8475787043571472
Epoch 480, training loss: 6.504727840423584 = 0.4137202501296997 + 1.0 * 6.091007709503174
Epoch 480, val loss: 0.8442658185958862
Epoch 490, training loss: 6.485267639160156 = 0.39693817496299744 + 1.0 * 6.088329315185547
Epoch 490, val loss: 0.8417647480964661
Epoch 500, training loss: 6.466704368591309 = 0.3806127607822418 + 1.0 * 6.0860915184021
Epoch 500, val loss: 0.8398980498313904
Epoch 510, training loss: 6.461953639984131 = 0.3647761642932892 + 1.0 * 6.097177505493164
Epoch 510, val loss: 0.838765561580658
Epoch 520, training loss: 6.432786464691162 = 0.3495447337627411 + 1.0 * 6.083241939544678
Epoch 520, val loss: 0.8382593989372253
Epoch 530, training loss: 6.416864395141602 = 0.3348791003227234 + 1.0 * 6.0819854736328125
Epoch 530, val loss: 0.8383941054344177
Epoch 540, training loss: 6.400040626525879 = 0.32064080238342285 + 1.0 * 6.079400062561035
Epoch 540, val loss: 0.8389988541603088
Epoch 550, training loss: 6.40882682800293 = 0.30686309933662415 + 1.0 * 6.101963520050049
Epoch 550, val loss: 0.8400942087173462
Epoch 560, training loss: 6.376062393188477 = 0.293804794549942 + 1.0 * 6.0822577476501465
Epoch 560, val loss: 0.8417482376098633
Epoch 570, training loss: 6.357635498046875 = 0.28131842613220215 + 1.0 * 6.076316833496094
Epoch 570, val loss: 0.8438935875892639
Epoch 580, training loss: 6.3424506187438965 = 0.26926711201667786 + 1.0 * 6.073183536529541
Epoch 580, val loss: 0.8463303446769714
Epoch 590, training loss: 6.329288959503174 = 0.2576461434364319 + 1.0 * 6.071642875671387
Epoch 590, val loss: 0.849104106426239
Epoch 600, training loss: 6.326805114746094 = 0.24644766747951508 + 1.0 * 6.080357551574707
Epoch 600, val loss: 0.8523467183113098
Epoch 610, training loss: 6.306594371795654 = 0.23583148419857025 + 1.0 * 6.070763111114502
Epoch 610, val loss: 0.8556882739067078
Epoch 620, training loss: 6.299659252166748 = 0.22577227652072906 + 1.0 * 6.073886871337891
Epoch 620, val loss: 0.8594086766242981
Epoch 630, training loss: 6.283342361450195 = 0.21611876785755157 + 1.0 * 6.06722354888916
Epoch 630, val loss: 0.863214373588562
Epoch 640, training loss: 6.273861408233643 = 0.20684665441513062 + 1.0 * 6.067014694213867
Epoch 640, val loss: 0.867228090763092
Epoch 650, training loss: 6.267017364501953 = 0.19791747629642487 + 1.0 * 6.0690999031066895
Epoch 650, val loss: 0.8714200854301453
Epoch 660, training loss: 6.252779960632324 = 0.1893562227487564 + 1.0 * 6.0634236335754395
Epoch 660, val loss: 0.8757646083831787
Epoch 670, training loss: 6.243546962738037 = 0.1810709536075592 + 1.0 * 6.06247615814209
Epoch 670, val loss: 0.8802720904350281
Epoch 680, training loss: 6.237622261047363 = 0.1730029135942459 + 1.0 * 6.064619541168213
Epoch 680, val loss: 0.8848199248313904
Epoch 690, training loss: 6.230700492858887 = 0.16522260010242462 + 1.0 * 6.0654778480529785
Epoch 690, val loss: 0.8893193602561951
Epoch 700, training loss: 6.219051361083984 = 0.15772950649261475 + 1.0 * 6.06132173538208
Epoch 700, val loss: 0.8940325975418091
Epoch 710, training loss: 6.208791255950928 = 0.15050703287124634 + 1.0 * 6.058284282684326
Epoch 710, val loss: 0.8989191055297852
Epoch 720, training loss: 6.211094379425049 = 0.14354002475738525 + 1.0 * 6.067554473876953
Epoch 720, val loss: 0.9037822484970093
Epoch 730, training loss: 6.199477195739746 = 0.13691705465316772 + 1.0 * 6.062560081481934
Epoch 730, val loss: 0.9088810086250305
Epoch 740, training loss: 6.187603950500488 = 0.13058051466941833 + 1.0 * 6.057023525238037
Epoch 740, val loss: 0.9142494201660156
Epoch 750, training loss: 6.179293155670166 = 0.12452943623065948 + 1.0 * 6.0547637939453125
Epoch 750, val loss: 0.9198421239852905
Epoch 760, training loss: 6.184041500091553 = 0.11873549968004227 + 1.0 * 6.065306186676025
Epoch 760, val loss: 0.9255073666572571
Epoch 770, training loss: 6.167166233062744 = 0.11321929842233658 + 1.0 * 6.0539469718933105
Epoch 770, val loss: 0.9313194751739502
Epoch 780, training loss: 6.160433292388916 = 0.10796782374382019 + 1.0 * 6.052465438842773
Epoch 780, val loss: 0.9374926686286926
Epoch 790, training loss: 6.156858921051025 = 0.10294084995985031 + 1.0 * 6.05391788482666
Epoch 790, val loss: 0.9438461065292358
Epoch 800, training loss: 6.152202606201172 = 0.0981593206524849 + 1.0 * 6.054043292999268
Epoch 800, val loss: 0.9503318667411804
Epoch 810, training loss: 6.144796848297119 = 0.09361787885427475 + 1.0 * 6.051178932189941
Epoch 810, val loss: 0.9570302367210388
Epoch 820, training loss: 6.137584686279297 = 0.08926957100629807 + 1.0 * 6.048315048217773
Epoch 820, val loss: 0.9639598727226257
Epoch 830, training loss: 6.1446380615234375 = 0.08513890206813812 + 1.0 * 6.059499263763428
Epoch 830, val loss: 0.9711076617240906
Epoch 840, training loss: 6.133308410644531 = 0.08125551044940948 + 1.0 * 6.052052974700928
Epoch 840, val loss: 0.9782193899154663
Epoch 850, training loss: 6.126061916351318 = 0.07763071358203888 + 1.0 * 6.048431396484375
Epoch 850, val loss: 0.9857960939407349
Epoch 860, training loss: 6.124096870422363 = 0.07420947402715683 + 1.0 * 6.049887180328369
Epoch 860, val loss: 0.9935205578804016
Epoch 870, training loss: 6.116284370422363 = 0.07101290673017502 + 1.0 * 6.045271396636963
Epoch 870, val loss: 1.0014302730560303
Epoch 880, training loss: 6.112258434295654 = 0.06800658255815506 + 1.0 * 6.044251918792725
Epoch 880, val loss: 1.009469985961914
Epoch 890, training loss: 6.110087871551514 = 0.065172478556633 + 1.0 * 6.044915199279785
Epoch 890, val loss: 1.0176849365234375
Epoch 900, training loss: 6.106403827667236 = 0.06250664591789246 + 1.0 * 6.0438971519470215
Epoch 900, val loss: 1.0258510112762451
Epoch 910, training loss: 6.104766845703125 = 0.06000852212309837 + 1.0 * 6.044758319854736
Epoch 910, val loss: 1.0340056419372559
Epoch 920, training loss: 6.099572658538818 = 0.05764453113079071 + 1.0 * 6.041928291320801
Epoch 920, val loss: 1.0422283411026
Epoch 930, training loss: 6.10392427444458 = 0.05540575832128525 + 1.0 * 6.048518657684326
Epoch 930, val loss: 1.050363302230835
Epoch 940, training loss: 6.0941081047058105 = 0.05329817533493042 + 1.0 * 6.0408101081848145
Epoch 940, val loss: 1.0585026741027832
Epoch 950, training loss: 6.090346813201904 = 0.05129683017730713 + 1.0 * 6.039050102233887
Epoch 950, val loss: 1.0666030645370483
Epoch 960, training loss: 6.088083267211914 = 0.04939332976937294 + 1.0 * 6.038690090179443
Epoch 960, val loss: 1.0746254920959473
Epoch 970, training loss: 6.086721420288086 = 0.047580938786268234 + 1.0 * 6.039140701293945
Epoch 970, val loss: 1.0825799703598022
Epoch 980, training loss: 6.089724063873291 = 0.04585668444633484 + 1.0 * 6.043867588043213
Epoch 980, val loss: 1.090416431427002
Epoch 990, training loss: 6.093413829803467 = 0.04422684386372566 + 1.0 * 6.049187183380127
Epoch 990, val loss: 1.0980536937713623
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.9225
Flip ASR: 0.9067/225 nodes
The final ASR:0.83395, 0.16036, Accuracy:0.80864, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10550])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00522, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.317157745361328 = 1.9432406425476074 + 1.0 * 8.373916625976562
Epoch 0, val loss: 1.9394956827163696
Epoch 10, training loss: 10.306991577148438 = 1.9334481954574585 + 1.0 * 8.373543739318848
Epoch 10, val loss: 1.9295566082000732
Epoch 20, training loss: 10.291878700256348 = 1.9211269617080688 + 1.0 * 8.37075138092041
Epoch 20, val loss: 1.9167733192443848
Epoch 30, training loss: 10.25235366821289 = 1.9039028882980347 + 1.0 * 8.348450660705566
Epoch 30, val loss: 1.8988423347473145
Epoch 40, training loss: 10.056289672851562 = 1.8816895484924316 + 1.0 * 8.174600601196289
Epoch 40, val loss: 1.876447319984436
Epoch 50, training loss: 9.469406127929688 = 1.8578392267227173 + 1.0 * 7.611566543579102
Epoch 50, val loss: 1.8536838293075562
Epoch 60, training loss: 9.048904418945312 = 1.8397103548049927 + 1.0 * 7.209194183349609
Epoch 60, val loss: 1.8374557495117188
Epoch 70, training loss: 8.637420654296875 = 1.8246222734451294 + 1.0 * 6.812798023223877
Epoch 70, val loss: 1.8231812715530396
Epoch 80, training loss: 8.43238639831543 = 1.8112261295318604 + 1.0 * 6.621160507202148
Epoch 80, val loss: 1.8105032444000244
Epoch 90, training loss: 8.293726921081543 = 1.7971221208572388 + 1.0 * 6.4966044425964355
Epoch 90, val loss: 1.7977145910263062
Epoch 100, training loss: 8.215707778930664 = 1.781626582145691 + 1.0 * 6.434081554412842
Epoch 100, val loss: 1.7839317321777344
Epoch 110, training loss: 8.156173706054688 = 1.7648078203201294 + 1.0 * 6.391366004943848
Epoch 110, val loss: 1.7693041563034058
Epoch 120, training loss: 8.106019020080566 = 1.7471401691436768 + 1.0 * 6.358879089355469
Epoch 120, val loss: 1.7542316913604736
Epoch 130, training loss: 8.054615020751953 = 1.7282942533493042 + 1.0 * 6.326321125030518
Epoch 130, val loss: 1.7381983995437622
Epoch 140, training loss: 8.00733470916748 = 1.7070858478546143 + 1.0 * 6.300249099731445
Epoch 140, val loss: 1.7202839851379395
Epoch 150, training loss: 7.96504545211792 = 1.6823736429214478 + 1.0 * 6.282671928405762
Epoch 150, val loss: 1.699392318725586
Epoch 160, training loss: 7.914134979248047 = 1.6535968780517578 + 1.0 * 6.260538101196289
Epoch 160, val loss: 1.6750801801681519
Epoch 170, training loss: 7.8634562492370605 = 1.6198062896728516 + 1.0 * 6.243649959564209
Epoch 170, val loss: 1.6465648412704468
Epoch 180, training loss: 7.812136650085449 = 1.5802972316741943 + 1.0 * 6.231839656829834
Epoch 180, val loss: 1.6133153438568115
Epoch 190, training loss: 7.754217147827148 = 1.5350072383880615 + 1.0 * 6.219210147857666
Epoch 190, val loss: 1.5751891136169434
Epoch 200, training loss: 7.691547393798828 = 1.4833042621612549 + 1.0 * 6.208243370056152
Epoch 200, val loss: 1.5318125486373901
Epoch 210, training loss: 7.624205589294434 = 1.425196886062622 + 1.0 * 6.199008464813232
Epoch 210, val loss: 1.483466386795044
Epoch 220, training loss: 7.554758548736572 = 1.3619303703308105 + 1.0 * 6.192828178405762
Epoch 220, val loss: 1.4316036701202393
Epoch 230, training loss: 7.481013298034668 = 1.2962925434112549 + 1.0 * 6.184720993041992
Epoch 230, val loss: 1.3783079385757446
Epoch 240, training loss: 7.405736923217773 = 1.2286757230758667 + 1.0 * 6.177061080932617
Epoch 240, val loss: 1.3240801095962524
Epoch 250, training loss: 7.3335089683532715 = 1.1609187126159668 + 1.0 * 6.172590255737305
Epoch 250, val loss: 1.270678997039795
Epoch 260, training loss: 7.2606892585754395 = 1.094884991645813 + 1.0 * 6.165804386138916
Epoch 260, val loss: 1.2193964719772339
Epoch 270, training loss: 7.190738677978516 = 1.030700922012329 + 1.0 * 6.160037994384766
Epoch 270, val loss: 1.1701664924621582
Epoch 280, training loss: 7.125707626342773 = 0.969017744064331 + 1.0 * 6.1566901206970215
Epoch 280, val loss: 1.1235061883926392
Epoch 290, training loss: 7.062966823577881 = 0.9109048843383789 + 1.0 * 6.152061939239502
Epoch 290, val loss: 1.0800758600234985
Epoch 300, training loss: 7.003519058227539 = 0.8558985590934753 + 1.0 * 6.147620677947998
Epoch 300, val loss: 1.0392670631408691
Epoch 310, training loss: 6.948432445526123 = 0.8038203120231628 + 1.0 * 6.1446123123168945
Epoch 310, val loss: 1.0008838176727295
Epoch 320, training loss: 6.8937883377075195 = 0.7549102902412415 + 1.0 * 6.138877868652344
Epoch 320, val loss: 0.9651963114738464
Epoch 330, training loss: 6.844108581542969 = 0.7093406319618225 + 1.0 * 6.134768009185791
Epoch 330, val loss: 0.9322759509086609
Epoch 340, training loss: 6.802177906036377 = 0.6673839688301086 + 1.0 * 6.134793758392334
Epoch 340, val loss: 0.9024300575256348
Epoch 350, training loss: 6.760528564453125 = 0.6296042799949646 + 1.0 * 6.130924224853516
Epoch 350, val loss: 0.8760809898376465
Epoch 360, training loss: 6.720662593841553 = 0.5956671833992004 + 1.0 * 6.124995231628418
Epoch 360, val loss: 0.8531628847122192
Epoch 370, training loss: 6.686838626861572 = 0.565010130405426 + 1.0 * 6.121828556060791
Epoch 370, val loss: 0.8332686424255371
Epoch 380, training loss: 6.657355308532715 = 0.5374341607093811 + 1.0 * 6.1199212074279785
Epoch 380, val loss: 0.8161686062812805
Epoch 390, training loss: 6.629408836364746 = 0.5126979351043701 + 1.0 * 6.116710662841797
Epoch 390, val loss: 0.8017979264259338
Epoch 400, training loss: 6.60265588760376 = 0.49024713039398193 + 1.0 * 6.112408638000488
Epoch 400, val loss: 0.7897660136222839
Epoch 410, training loss: 6.58401346206665 = 0.46960705518722534 + 1.0 * 6.114406585693359
Epoch 410, val loss: 0.7796006798744202
Epoch 420, training loss: 6.564239978790283 = 0.45067861676216125 + 1.0 * 6.113561153411865
Epoch 420, val loss: 0.7710440158843994
Epoch 430, training loss: 6.539642810821533 = 0.433138906955719 + 1.0 * 6.106503963470459
Epoch 430, val loss: 0.7639296650886536
Epoch 440, training loss: 6.51861572265625 = 0.4164522588253021 + 1.0 * 6.102163314819336
Epoch 440, val loss: 0.757905900478363
Epoch 450, training loss: 6.499613285064697 = 0.4002719819545746 + 1.0 * 6.09934139251709
Epoch 450, val loss: 0.7526638507843018
Epoch 460, training loss: 6.492840766906738 = 0.3844000995159149 + 1.0 * 6.10844087600708
Epoch 460, val loss: 0.7480201125144958
Epoch 470, training loss: 6.468023300170898 = 0.36882516741752625 + 1.0 * 6.099198341369629
Epoch 470, val loss: 0.7439723610877991
Epoch 480, training loss: 6.448067665100098 = 0.3532675802707672 + 1.0 * 6.094799995422363
Epoch 480, val loss: 0.7403891086578369
Epoch 490, training loss: 6.432949066162109 = 0.3375205099582672 + 1.0 * 6.095428466796875
Epoch 490, val loss: 0.7372202277183533
Epoch 500, training loss: 6.411183834075928 = 0.32160109281539917 + 1.0 * 6.089582920074463
Epoch 500, val loss: 0.7344376444816589
Epoch 510, training loss: 6.393929958343506 = 0.3054700195789337 + 1.0 * 6.0884599685668945
Epoch 510, val loss: 0.732117235660553
Epoch 520, training loss: 6.381685256958008 = 0.2891879379749298 + 1.0 * 6.0924973487854
Epoch 520, val loss: 0.7303723096847534
Epoch 530, training loss: 6.359858989715576 = 0.27305540442466736 + 1.0 * 6.086803436279297
Epoch 530, val loss: 0.7290872931480408
Epoch 540, training loss: 6.340747356414795 = 0.2571488320827484 + 1.0 * 6.083598613739014
Epoch 540, val loss: 0.7285550236701965
Epoch 550, training loss: 6.326128959655762 = 0.24157701432704926 + 1.0 * 6.084551811218262
Epoch 550, val loss: 0.7287870645523071
Epoch 560, training loss: 6.314239978790283 = 0.2265656292438507 + 1.0 * 6.087674140930176
Epoch 560, val loss: 0.7299191355705261
Epoch 570, training loss: 6.291329383850098 = 0.2122807800769806 + 1.0 * 6.0790486335754395
Epoch 570, val loss: 0.7317803502082825
Epoch 580, training loss: 6.276442527770996 = 0.1986856907606125 + 1.0 * 6.077756881713867
Epoch 580, val loss: 0.7345716953277588
Epoch 590, training loss: 6.262567043304443 = 0.1858094483613968 + 1.0 * 6.076757431030273
Epoch 590, val loss: 0.7382754683494568
Epoch 600, training loss: 6.248945713043213 = 0.17376260459423065 + 1.0 * 6.075182914733887
Epoch 600, val loss: 0.7427514791488647
Epoch 610, training loss: 6.238551616668701 = 0.1626184582710266 + 1.0 * 6.07593297958374
Epoch 610, val loss: 0.7479360103607178
Epoch 620, training loss: 6.226363658905029 = 0.1523376703262329 + 1.0 * 6.074026107788086
Epoch 620, val loss: 0.7537779211997986
Epoch 630, training loss: 6.213954925537109 = 0.14280195534229279 + 1.0 * 6.071153163909912
Epoch 630, val loss: 0.7602670192718506
Epoch 640, training loss: 6.202976703643799 = 0.13395287096500397 + 1.0 * 6.069023609161377
Epoch 640, val loss: 0.7673930525779724
Epoch 650, training loss: 6.199775218963623 = 0.1257924884557724 + 1.0 * 6.0739827156066895
Epoch 650, val loss: 0.7750113606452942
Epoch 660, training loss: 6.188345432281494 = 0.11828029155731201 + 1.0 * 6.070065021514893
Epoch 660, val loss: 0.7828121781349182
Epoch 670, training loss: 6.189089298248291 = 0.1113494262099266 + 1.0 * 6.077739715576172
Epoch 670, val loss: 0.7910550832748413
Epoch 680, training loss: 6.172094821929932 = 0.10499142855405807 + 1.0 * 6.067103385925293
Epoch 680, val loss: 0.7994723916053772
Epoch 690, training loss: 6.162015914916992 = 0.09908906370401382 + 1.0 * 6.062926769256592
Epoch 690, val loss: 0.8080891370773315
Epoch 700, training loss: 6.1554718017578125 = 0.09360814094543457 + 1.0 * 6.061863899230957
Epoch 700, val loss: 0.8169795870780945
Epoch 710, training loss: 6.164631366729736 = 0.0885164812207222 + 1.0 * 6.076114654541016
Epoch 710, val loss: 0.8260668516159058
Epoch 720, training loss: 6.146921157836914 = 0.08381951600313187 + 1.0 * 6.063101768493652
Epoch 720, val loss: 0.8351629972457886
Epoch 730, training loss: 6.140594959259033 = 0.0794588029384613 + 1.0 * 6.061136245727539
Epoch 730, val loss: 0.8444111347198486
Epoch 740, training loss: 6.136465549468994 = 0.07538822293281555 + 1.0 * 6.061077117919922
Epoch 740, val loss: 0.8537821769714355
Epoch 750, training loss: 6.1317572593688965 = 0.07159824669361115 + 1.0 * 6.060159206390381
Epoch 750, val loss: 0.8631771802902222
Epoch 760, training loss: 6.124705791473389 = 0.06805992126464844 + 1.0 * 6.05664587020874
Epoch 760, val loss: 0.8725942969322205
Epoch 770, training loss: 6.120821475982666 = 0.06475892663002014 + 1.0 * 6.056062698364258
Epoch 770, val loss: 0.8820809721946716
Epoch 780, training loss: 6.117021560668945 = 0.06165461242198944 + 1.0 * 6.0553669929504395
Epoch 780, val loss: 0.8916711807250977
Epoch 790, training loss: 6.119784355163574 = 0.05874847620725632 + 1.0 * 6.061036109924316
Epoch 790, val loss: 0.901197075843811
Epoch 800, training loss: 6.110528469085693 = 0.05603437125682831 + 1.0 * 6.0544939041137695
Epoch 800, val loss: 0.9105880856513977
Epoch 810, training loss: 6.105447769165039 = 0.05348043516278267 + 1.0 * 6.051967144012451
Epoch 810, val loss: 0.9200082421302795
Epoch 820, training loss: 6.1013031005859375 = 0.05107712745666504 + 1.0 * 6.050225734710693
Epoch 820, val loss: 0.9295474886894226
Epoch 830, training loss: 6.104085445404053 = 0.04880833253264427 + 1.0 * 6.055277347564697
Epoch 830, val loss: 0.9390891790390015
Epoch 840, training loss: 6.103006839752197 = 0.046681102365255356 + 1.0 * 6.056325912475586
Epoch 840, val loss: 0.9483855962753296
Epoch 850, training loss: 6.094491481781006 = 0.04468904808163643 + 1.0 * 6.049802303314209
Epoch 850, val loss: 0.9576213955879211
Epoch 860, training loss: 6.090554714202881 = 0.042808543890714645 + 1.0 * 6.047746181488037
Epoch 860, val loss: 0.9668712615966797
Epoch 870, training loss: 6.087884426116943 = 0.04103066772222519 + 1.0 * 6.046853542327881
Epoch 870, val loss: 0.9761004447937012
Epoch 880, training loss: 6.092878341674805 = 0.039349909871816635 + 1.0 * 6.053528308868408
Epoch 880, val loss: 0.9851674437522888
Epoch 890, training loss: 6.082866668701172 = 0.03777119517326355 + 1.0 * 6.045095443725586
Epoch 890, val loss: 0.9940856695175171
Epoch 900, training loss: 6.0810651779174805 = 0.03627591207623482 + 1.0 * 6.0447893142700195
Epoch 900, val loss: 1.0029889345169067
Epoch 910, training loss: 6.088050365447998 = 0.0348619781434536 + 1.0 * 6.053188323974609
Epoch 910, val loss: 1.0118484497070312
Epoch 920, training loss: 6.078306674957275 = 0.03352786600589752 + 1.0 * 6.044778823852539
Epoch 920, val loss: 1.0204455852508545
Epoch 930, training loss: 6.074357986450195 = 0.03226295858621597 + 1.0 * 6.042095184326172
Epoch 930, val loss: 1.0289306640625
Epoch 940, training loss: 6.073329448699951 = 0.03106401301920414 + 1.0 * 6.04226541519165
Epoch 940, val loss: 1.0374162197113037
Epoch 950, training loss: 6.074975967407227 = 0.02992892637848854 + 1.0 * 6.045046806335449
Epoch 950, val loss: 1.0456959009170532
Epoch 960, training loss: 6.07211971282959 = 0.02885698899626732 + 1.0 * 6.043262958526611
Epoch 960, val loss: 1.0538009405136108
Epoch 970, training loss: 6.068455696105957 = 0.02784050442278385 + 1.0 * 6.040615081787109
Epoch 970, val loss: 1.0618420839309692
Epoch 980, training loss: 6.065235614776611 = 0.026875313371419907 + 1.0 * 6.038360118865967
Epoch 980, val loss: 1.0698199272155762
Epoch 990, training loss: 6.066775798797607 = 0.02595583163201809 + 1.0 * 6.040820121765137
Epoch 990, val loss: 1.077659249305725
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6421
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.334490776062012 = 1.9605858325958252 + 1.0 * 8.373905181884766
Epoch 0, val loss: 1.9530675411224365
Epoch 10, training loss: 10.324031829833984 = 1.9505521059036255 + 1.0 * 8.373479843139648
Epoch 10, val loss: 1.943472146987915
Epoch 20, training loss: 10.30843448638916 = 1.9383708238601685 + 1.0 * 8.370063781738281
Epoch 20, val loss: 1.9312739372253418
Epoch 30, training loss: 10.271679878234863 = 1.9217582941055298 + 1.0 * 8.349921226501465
Epoch 30, val loss: 1.9140048027038574
Epoch 40, training loss: 10.141318321228027 = 1.8995991945266724 + 1.0 * 8.241719245910645
Epoch 40, val loss: 1.891098976135254
Epoch 50, training loss: 9.70355224609375 = 1.8760957717895508 + 1.0 * 7.827455997467041
Epoch 50, val loss: 1.8674328327178955
Epoch 60, training loss: 9.304156303405762 = 1.8512545824050903 + 1.0 * 7.452901363372803
Epoch 60, val loss: 1.8428658246994019
Epoch 70, training loss: 8.940945625305176 = 1.826605200767517 + 1.0 * 7.114340782165527
Epoch 70, val loss: 1.8184287548065186
Epoch 80, training loss: 8.697656631469727 = 1.8020944595336914 + 1.0 * 6.895561695098877
Epoch 80, val loss: 1.7949548959732056
Epoch 90, training loss: 8.50391960144043 = 1.7819600105285645 + 1.0 * 6.721959590911865
Epoch 90, val loss: 1.7753249406814575
Epoch 100, training loss: 8.343700408935547 = 1.7614011764526367 + 1.0 * 6.582298755645752
Epoch 100, val loss: 1.754974603652954
Epoch 110, training loss: 8.221803665161133 = 1.7396844625473022 + 1.0 * 6.482119083404541
Epoch 110, val loss: 1.7344080209732056
Epoch 120, training loss: 8.138057708740234 = 1.7166200876235962 + 1.0 * 6.4214372634887695
Epoch 120, val loss: 1.7130165100097656
Epoch 130, training loss: 8.074630737304688 = 1.691300630569458 + 1.0 * 6.383330345153809
Epoch 130, val loss: 1.6895008087158203
Epoch 140, training loss: 8.018982887268066 = 1.662692666053772 + 1.0 * 6.356289863586426
Epoch 140, val loss: 1.663102626800537
Epoch 150, training loss: 7.962535858154297 = 1.6298946142196655 + 1.0 * 6.332641124725342
Epoch 150, val loss: 1.6334543228149414
Epoch 160, training loss: 7.902341842651367 = 1.5920560359954834 + 1.0 * 6.310285568237305
Epoch 160, val loss: 1.59993314743042
Epoch 170, training loss: 7.843363285064697 = 1.549202561378479 + 1.0 * 6.294160842895508
Epoch 170, val loss: 1.5627096891403198
Epoch 180, training loss: 7.776652812957764 = 1.5020251274108887 + 1.0 * 6.274627685546875
Epoch 180, val loss: 1.5224921703338623
Epoch 190, training loss: 7.70890998840332 = 1.4504460096359253 + 1.0 * 6.2584638595581055
Epoch 190, val loss: 1.4788085222244263
Epoch 200, training loss: 7.639894485473633 = 1.3952505588531494 + 1.0 * 6.244643688201904
Epoch 200, val loss: 1.4327813386917114
Epoch 210, training loss: 7.575084209442139 = 1.3391427993774414 + 1.0 * 6.235941410064697
Epoch 210, val loss: 1.3867014646530151
Epoch 220, training loss: 7.503693580627441 = 1.2837135791778564 + 1.0 * 6.219980239868164
Epoch 220, val loss: 1.342528223991394
Epoch 230, training loss: 7.438025951385498 = 1.229286551475525 + 1.0 * 6.208739280700684
Epoch 230, val loss: 1.2997039556503296
Epoch 240, training loss: 7.375539779663086 = 1.1758787631988525 + 1.0 * 6.1996612548828125
Epoch 240, val loss: 1.25815749168396
Epoch 250, training loss: 7.315515518188477 = 1.1239888668060303 + 1.0 * 6.191526412963867
Epoch 250, val loss: 1.2183681726455688
Epoch 260, training loss: 7.258708477020264 = 1.0734015703201294 + 1.0 * 6.185307025909424
Epoch 260, val loss: 1.1797847747802734
Epoch 270, training loss: 7.202040195465088 = 1.0231126546859741 + 1.0 * 6.178927421569824
Epoch 270, val loss: 1.141631007194519
Epoch 280, training loss: 7.158397197723389 = 0.9730542302131653 + 1.0 * 6.185342788696289
Epoch 280, val loss: 1.1033728122711182
Epoch 290, training loss: 7.095709800720215 = 0.9240956902503967 + 1.0 * 6.171614170074463
Epoch 290, val loss: 1.0662022829055786
Epoch 300, training loss: 7.040678024291992 = 0.876228928565979 + 1.0 * 6.164449214935303
Epoch 300, val loss: 1.0298880338668823
Epoch 310, training loss: 6.989334583282471 = 0.8295426368713379 + 1.0 * 6.159791946411133
Epoch 310, val loss: 0.9943532347679138
Epoch 320, training loss: 6.939617156982422 = 0.7843139171600342 + 1.0 * 6.155303001403809
Epoch 320, val loss: 0.9601244926452637
Epoch 330, training loss: 6.892226696014404 = 0.7409101128578186 + 1.0 * 6.1513166427612305
Epoch 330, val loss: 0.9275956749916077
Epoch 340, training loss: 6.853582859039307 = 0.700107991695404 + 1.0 * 6.153474807739258
Epoch 340, val loss: 0.8974441289901733
Epoch 350, training loss: 6.808209419250488 = 0.6622833013534546 + 1.0 * 6.145925998687744
Epoch 350, val loss: 0.8700911402702332
Epoch 360, training loss: 6.775983810424805 = 0.6269900798797607 + 1.0 * 6.148993492126465
Epoch 360, val loss: 0.8452121615409851
Epoch 370, training loss: 6.732874870300293 = 0.5945097208023071 + 1.0 * 6.138365268707275
Epoch 370, val loss: 0.8228655457496643
Epoch 380, training loss: 6.698070526123047 = 0.5642080903053284 + 1.0 * 6.133862495422363
Epoch 380, val loss: 0.8027772307395935
Epoch 390, training loss: 6.666035175323486 = 0.5356341004371643 + 1.0 * 6.130401134490967
Epoch 390, val loss: 0.7845022082328796
Epoch 400, training loss: 6.636153221130371 = 0.5085591077804565 + 1.0 * 6.127593994140625
Epoch 400, val loss: 0.767811119556427
Epoch 410, training loss: 6.607672214508057 = 0.4828706383705139 + 1.0 * 6.1248016357421875
Epoch 410, val loss: 0.7525200247764587
Epoch 420, training loss: 6.5796589851379395 = 0.4584599435329437 + 1.0 * 6.121199131011963
Epoch 420, val loss: 0.73847895860672
Epoch 430, training loss: 6.553213119506836 = 0.4349273145198822 + 1.0 * 6.118285655975342
Epoch 430, val loss: 0.7255021929740906
Epoch 440, training loss: 6.528270244598389 = 0.4119804799556732 + 1.0 * 6.1162896156311035
Epoch 440, val loss: 0.713409960269928
Epoch 450, training loss: 6.504865646362305 = 0.38962313532829285 + 1.0 * 6.1152424812316895
Epoch 450, val loss: 0.7019320726394653
Epoch 460, training loss: 6.481569766998291 = 0.36801421642303467 + 1.0 * 6.113555431365967
Epoch 460, val loss: 0.6912181377410889
Epoch 470, training loss: 6.455811023712158 = 0.34679073095321655 + 1.0 * 6.109020233154297
Epoch 470, val loss: 0.6812734007835388
Epoch 480, training loss: 6.432100772857666 = 0.3258480727672577 + 1.0 * 6.106252670288086
Epoch 480, val loss: 0.671903133392334
Epoch 490, training loss: 6.408973693847656 = 0.30521926283836365 + 1.0 * 6.10375452041626
Epoch 490, val loss: 0.6630599498748779
Epoch 500, training loss: 6.400479316711426 = 0.2850302755832672 + 1.0 * 6.115448951721191
Epoch 500, val loss: 0.6549108624458313
Epoch 510, training loss: 6.366250991821289 = 0.26569482684135437 + 1.0 * 6.100556373596191
Epoch 510, val loss: 0.6476150751113892
Epoch 520, training loss: 6.346997261047363 = 0.2471923530101776 + 1.0 * 6.099804878234863
Epoch 520, val loss: 0.641265332698822
Epoch 530, training loss: 6.326196670532227 = 0.2296883761882782 + 1.0 * 6.096508502960205
Epoch 530, val loss: 0.6358632445335388
Epoch 540, training loss: 6.310599327087402 = 0.21332727372646332 + 1.0 * 6.097271919250488
Epoch 540, val loss: 0.6314210295677185
Epoch 550, training loss: 6.290409088134766 = 0.1980636715888977 + 1.0 * 6.092345237731934
Epoch 550, val loss: 0.6280307173728943
Epoch 560, training loss: 6.274684906005859 = 0.18384915590286255 + 1.0 * 6.0908355712890625
Epoch 560, val loss: 0.625640332698822
Epoch 570, training loss: 6.26094388961792 = 0.1707683801651001 + 1.0 * 6.090175628662109
Epoch 570, val loss: 0.6241507530212402
Epoch 580, training loss: 6.248133182525635 = 0.15888775885105133 + 1.0 * 6.089245319366455
Epoch 580, val loss: 0.6235080361366272
Epoch 590, training loss: 6.234320163726807 = 0.14798417687416077 + 1.0 * 6.086336135864258
Epoch 590, val loss: 0.6237959265708923
Epoch 600, training loss: 6.224794387817383 = 0.13797888159751892 + 1.0 * 6.086815357208252
Epoch 600, val loss: 0.6248756647109985
Epoch 610, training loss: 6.212965488433838 = 0.12887875735759735 + 1.0 * 6.084086894989014
Epoch 610, val loss: 0.6265089511871338
Epoch 620, training loss: 6.203145980834961 = 0.12053972482681274 + 1.0 * 6.082606315612793
Epoch 620, val loss: 0.6288344860076904
Epoch 630, training loss: 6.19196891784668 = 0.11290945112705231 + 1.0 * 6.079059600830078
Epoch 630, val loss: 0.631716787815094
Epoch 640, training loss: 6.183626651763916 = 0.10586868226528168 + 1.0 * 6.077757835388184
Epoch 640, val loss: 0.6350816488265991
Epoch 650, training loss: 6.176279544830322 = 0.09939555823802948 + 1.0 * 6.076883792877197
Epoch 650, val loss: 0.6388618350028992
Epoch 660, training loss: 6.179916858673096 = 0.09345172345638275 + 1.0 * 6.086465358734131
Epoch 660, val loss: 0.6429784297943115
Epoch 670, training loss: 6.1655168533325195 = 0.08802767097949982 + 1.0 * 6.077489376068115
Epoch 670, val loss: 0.6473784446716309
Epoch 680, training loss: 6.158510684967041 = 0.08308621495962143 + 1.0 * 6.075424671173096
Epoch 680, val loss: 0.652050793170929
Epoch 690, training loss: 6.1500701904296875 = 0.07851258665323257 + 1.0 * 6.071557521820068
Epoch 690, val loss: 0.6570231318473816
Epoch 700, training loss: 6.143587112426758 = 0.07428500056266785 + 1.0 * 6.069302082061768
Epoch 700, val loss: 0.6621785163879395
Epoch 710, training loss: 6.138169288635254 = 0.07035857439041138 + 1.0 * 6.067810535430908
Epoch 710, val loss: 0.6675650477409363
Epoch 720, training loss: 6.150503158569336 = 0.06673672795295715 + 1.0 * 6.083766460418701
Epoch 720, val loss: 0.6730949282646179
Epoch 730, training loss: 6.134189605712891 = 0.06340165436267853 + 1.0 * 6.0707879066467285
Epoch 730, val loss: 0.6786450147628784
Epoch 740, training loss: 6.124931335449219 = 0.06031276658177376 + 1.0 * 6.0646185874938965
Epoch 740, val loss: 0.6843740940093994
Epoch 750, training loss: 6.120671272277832 = 0.057430069893598557 + 1.0 * 6.063241004943848
Epoch 750, val loss: 0.6901736259460449
Epoch 760, training loss: 6.134690284729004 = 0.05474516376852989 + 1.0 * 6.079945087432861
Epoch 760, val loss: 0.6959947347640991
Epoch 770, training loss: 6.116541385650635 = 0.052240680903196335 + 1.0 * 6.064300537109375
Epoch 770, val loss: 0.7018468976020813
Epoch 780, training loss: 6.110158443450928 = 0.049913838505744934 + 1.0 * 6.060244560241699
Epoch 780, val loss: 0.707818865776062
Epoch 790, training loss: 6.106781482696533 = 0.047732219099998474 + 1.0 * 6.059049129486084
Epoch 790, val loss: 0.7138317823410034
Epoch 800, training loss: 6.1035847663879395 = 0.04567330330610275 + 1.0 * 6.057911396026611
Epoch 800, val loss: 0.7198302745819092
Epoch 810, training loss: 6.120467185974121 = 0.043742917478084564 + 1.0 * 6.076724052429199
Epoch 810, val loss: 0.7258498668670654
Epoch 820, training loss: 6.100165367126465 = 0.04194241762161255 + 1.0 * 6.058222770690918
Epoch 820, val loss: 0.7317824363708496
Epoch 830, training loss: 6.096405982971191 = 0.04025726392865181 + 1.0 * 6.056148529052734
Epoch 830, val loss: 0.7377706170082092
Epoch 840, training loss: 6.093358039855957 = 0.03866753354668617 + 1.0 * 6.054690361022949
Epoch 840, val loss: 0.7437765002250671
Epoch 850, training loss: 6.100530624389648 = 0.037159230560064316 + 1.0 * 6.063371181488037
Epoch 850, val loss: 0.7497273087501526
Epoch 860, training loss: 6.091124057769775 = 0.03575155884027481 + 1.0 * 6.055372714996338
Epoch 860, val loss: 0.7555959224700928
Epoch 870, training loss: 6.086956977844238 = 0.034410007297992706 + 1.0 * 6.052546977996826
Epoch 870, val loss: 0.761510968208313
Epoch 880, training loss: 6.083910942077637 = 0.033143702894449234 + 1.0 * 6.050767421722412
Epoch 880, val loss: 0.7674194574356079
Epoch 890, training loss: 6.09052038192749 = 0.03194050490856171 + 1.0 * 6.058579921722412
Epoch 890, val loss: 0.77326899766922
Epoch 900, training loss: 6.083700180053711 = 0.030808791518211365 + 1.0 * 6.052891254425049
Epoch 900, val loss: 0.7789686918258667
Epoch 910, training loss: 6.078404903411865 = 0.02973734401166439 + 1.0 * 6.0486674308776855
Epoch 910, val loss: 0.7847281098365784
Epoch 920, training loss: 6.079275131225586 = 0.028721444308757782 + 1.0 * 6.050553798675537
Epoch 920, val loss: 0.7904276251792908
Epoch 930, training loss: 6.0742411613464355 = 0.027755556628108025 + 1.0 * 6.046485424041748
Epoch 930, val loss: 0.795974850654602
Epoch 940, training loss: 6.073012828826904 = 0.026843776926398277 + 1.0 * 6.046169281005859
Epoch 940, val loss: 0.8014907240867615
Epoch 950, training loss: 6.070647239685059 = 0.025971917435526848 + 1.0 * 6.044675350189209
Epoch 950, val loss: 0.807043194770813
Epoch 960, training loss: 6.068918228149414 = 0.02513868547976017 + 1.0 * 6.043779373168945
Epoch 960, val loss: 0.8125307559967041
Epoch 970, training loss: 6.075285911560059 = 0.024342233315110207 + 1.0 * 6.050943851470947
Epoch 970, val loss: 0.8179514408111572
Epoch 980, training loss: 6.069402694702148 = 0.02358766831457615 + 1.0 * 6.0458149909973145
Epoch 980, val loss: 0.8232759833335876
Epoch 990, training loss: 6.0657196044921875 = 0.02286796271800995 + 1.0 * 6.042851448059082
Epoch 990, val loss: 0.8286042809486389
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6605
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.329792022705078 = 1.955898642539978 + 1.0 * 8.373893737792969
Epoch 0, val loss: 1.9642629623413086
Epoch 10, training loss: 10.318276405334473 = 1.9447529315948486 + 1.0 * 8.373523712158203
Epoch 10, val loss: 1.952360987663269
Epoch 20, training loss: 10.301420211791992 = 1.9306846857070923 + 1.0 * 8.370735168457031
Epoch 20, val loss: 1.9370417594909668
Epoch 30, training loss: 10.260749816894531 = 1.910793662071228 + 1.0 * 8.349956512451172
Epoch 30, val loss: 1.915460467338562
Epoch 40, training loss: 10.113261222839355 = 1.8845853805541992 + 1.0 * 8.228675842285156
Epoch 40, val loss: 1.888412594795227
Epoch 50, training loss: 9.574310302734375 = 1.8564348220825195 + 1.0 * 7.7178754806518555
Epoch 50, val loss: 1.8609427213668823
Epoch 60, training loss: 9.14242935180664 = 1.8330683708190918 + 1.0 * 7.309360504150391
Epoch 60, val loss: 1.840128779411316
Epoch 70, training loss: 8.686188697814941 = 1.8163782358169556 + 1.0 * 6.869810581207275
Epoch 70, val loss: 1.8246713876724243
Epoch 80, training loss: 8.469682693481445 = 1.799514651298523 + 1.0 * 6.670167922973633
Epoch 80, val loss: 1.808159589767456
Epoch 90, training loss: 8.36486530303955 = 1.781505823135376 + 1.0 * 6.583359241485596
Epoch 90, val loss: 1.7901302576065063
Epoch 100, training loss: 8.27983570098877 = 1.762640357017517 + 1.0 * 6.517195701599121
Epoch 100, val loss: 1.7718408107757568
Epoch 110, training loss: 8.212750434875488 = 1.742775559425354 + 1.0 * 6.469974517822266
Epoch 110, val loss: 1.7531756162643433
Epoch 120, training loss: 8.15298843383789 = 1.7204194068908691 + 1.0 * 6.4325690269470215
Epoch 120, val loss: 1.7329635620117188
Epoch 130, training loss: 8.095130920410156 = 1.6946760416030884 + 1.0 * 6.400454521179199
Epoch 130, val loss: 1.710332989692688
Epoch 140, training loss: 8.032346725463867 = 1.665165662765503 + 1.0 * 6.367180824279785
Epoch 140, val loss: 1.6851658821105957
Epoch 150, training loss: 7.968811511993408 = 1.6307382583618164 + 1.0 * 6.338073253631592
Epoch 150, val loss: 1.6561129093170166
Epoch 160, training loss: 7.903848648071289 = 1.5904513597488403 + 1.0 * 6.313397407531738
Epoch 160, val loss: 1.6223008632659912
Epoch 170, training loss: 7.836154460906982 = 1.5441522598266602 + 1.0 * 6.292002201080322
Epoch 170, val loss: 1.5833508968353271
Epoch 180, training loss: 7.764684200286865 = 1.4907935857772827 + 1.0 * 6.273890495300293
Epoch 180, val loss: 1.538433313369751
Epoch 190, training loss: 7.6913275718688965 = 1.4305391311645508 + 1.0 * 6.260788440704346
Epoch 190, val loss: 1.488091230392456
Epoch 200, training loss: 7.612133979797363 = 1.3653018474578857 + 1.0 * 6.246831893920898
Epoch 200, val loss: 1.4339542388916016
Epoch 210, training loss: 7.530398368835449 = 1.2956640720367432 + 1.0 * 6.234734535217285
Epoch 210, val loss: 1.3767579793930054
Epoch 220, training loss: 7.446966171264648 = 1.223367691040039 + 1.0 * 6.223598480224609
Epoch 220, val loss: 1.3184454441070557
Epoch 230, training loss: 7.3658599853515625 = 1.151244878768921 + 1.0 * 6.2146148681640625
Epoch 230, val loss: 1.261068344116211
Epoch 240, training loss: 7.287250518798828 = 1.0798126459121704 + 1.0 * 6.207437992095947
Epoch 240, val loss: 1.2049721479415894
Epoch 250, training loss: 7.207893371582031 = 1.0116658210754395 + 1.0 * 6.196227550506592
Epoch 250, val loss: 1.151963233947754
Epoch 260, training loss: 7.13607120513916 = 0.9472936987876892 + 1.0 * 6.188777446746826
Epoch 260, val loss: 1.102782130241394
Epoch 270, training loss: 7.077284812927246 = 0.8872328996658325 + 1.0 * 6.190052032470703
Epoch 270, val loss: 1.0576552152633667
Epoch 280, training loss: 7.012932777404785 = 0.8331258296966553 + 1.0 * 6.179807186126709
Epoch 280, val loss: 1.01822829246521
Epoch 290, training loss: 6.954033374786377 = 0.7841196656227112 + 1.0 * 6.1699137687683105
Epoch 290, val loss: 0.9836163520812988
Epoch 300, training loss: 6.903131008148193 = 0.7394181489944458 + 1.0 * 6.163712978363037
Epoch 300, val loss: 0.95304936170578
Epoch 310, training loss: 6.859083652496338 = 0.6984665989875793 + 1.0 * 6.160616874694824
Epoch 310, val loss: 0.9262094497680664
Epoch 320, training loss: 6.817480087280273 = 0.6611325740814209 + 1.0 * 6.156347274780273
Epoch 320, val loss: 0.9028968214988708
Epoch 330, training loss: 6.776176452636719 = 0.6264892816543579 + 1.0 * 6.14968729019165
Epoch 330, val loss: 0.8821961879730225
Epoch 340, training loss: 6.745837211608887 = 0.5939434170722961 + 1.0 * 6.151893615722656
Epoch 340, val loss: 0.8636239171028137
Epoch 350, training loss: 6.715272903442383 = 0.5640034079551697 + 1.0 * 6.151269435882568
Epoch 350, val loss: 0.8470444083213806
Epoch 360, training loss: 6.675340175628662 = 0.53612220287323 + 1.0 * 6.139217853546143
Epoch 360, val loss: 0.8324276804924011
Epoch 370, training loss: 6.644336700439453 = 0.5098551511764526 + 1.0 * 6.134481430053711
Epoch 370, val loss: 0.8191112875938416
Epoch 380, training loss: 6.616219520568848 = 0.4849640130996704 + 1.0 * 6.131255626678467
Epoch 380, val loss: 0.8070343732833862
Epoch 390, training loss: 6.590262413024902 = 0.461389422416687 + 1.0 * 6.128872871398926
Epoch 390, val loss: 0.7961944937705994
Epoch 400, training loss: 6.567503452301025 = 0.4393133223056793 + 1.0 * 6.128190040588379
Epoch 400, val loss: 0.7864997386932373
Epoch 410, training loss: 6.540234088897705 = 0.418560653924942 + 1.0 * 6.121673583984375
Epoch 410, val loss: 0.7782022953033447
Epoch 420, training loss: 6.5179524421691895 = 0.3988756537437439 + 1.0 * 6.119076728820801
Epoch 420, val loss: 0.7709521055221558
Epoch 430, training loss: 6.511972904205322 = 0.38012611865997314 + 1.0 * 6.131846904754639
Epoch 430, val loss: 0.764617383480072
Epoch 440, training loss: 6.48084831237793 = 0.36252373456954956 + 1.0 * 6.1183247566223145
Epoch 440, val loss: 0.7592341899871826
Epoch 450, training loss: 6.458226680755615 = 0.345740407705307 + 1.0 * 6.112486362457275
Epoch 450, val loss: 0.7547188997268677
Epoch 460, training loss: 6.438026428222656 = 0.32957684993743896 + 1.0 * 6.108449459075928
Epoch 460, val loss: 0.7507957220077515
Epoch 470, training loss: 6.421903133392334 = 0.3139210343360901 + 1.0 * 6.107982158660889
Epoch 470, val loss: 0.7474294900894165
Epoch 480, training loss: 6.413365840911865 = 0.29888173937797546 + 1.0 * 6.1144843101501465
Epoch 480, val loss: 0.7445423603057861
Epoch 490, training loss: 6.3890533447265625 = 0.2844068706035614 + 1.0 * 6.104646682739258
Epoch 490, val loss: 0.7422531843185425
Epoch 500, training loss: 6.37095308303833 = 0.2703615128993988 + 1.0 * 6.100591659545898
Epoch 500, val loss: 0.7404631972312927
Epoch 510, training loss: 6.354732513427734 = 0.2566491365432739 + 1.0 * 6.09808349609375
Epoch 510, val loss: 0.7391023635864258
Epoch 520, training loss: 6.356149673461914 = 0.2432834804058075 + 1.0 * 6.112866401672363
Epoch 520, val loss: 0.7380996346473694
Epoch 530, training loss: 6.326120853424072 = 0.23035576939582825 + 1.0 * 6.095765113830566
Epoch 530, val loss: 0.7375578880310059
Epoch 540, training loss: 6.310825347900391 = 0.21783854067325592 + 1.0 * 6.092986583709717
Epoch 540, val loss: 0.7374441623687744
Epoch 550, training loss: 6.296923637390137 = 0.2056601345539093 + 1.0 * 6.091263294219971
Epoch 550, val loss: 0.7377423048019409
Epoch 560, training loss: 6.294546604156494 = 0.1939137876033783 + 1.0 * 6.100632667541504
Epoch 560, val loss: 0.7384389638900757
Epoch 570, training loss: 6.272502422332764 = 0.18265356123447418 + 1.0 * 6.08984899520874
Epoch 570, val loss: 0.7395773530006409
Epoch 580, training loss: 6.258078575134277 = 0.17193061113357544 + 1.0 * 6.086147785186768
Epoch 580, val loss: 0.7411068081855774
Epoch 590, training loss: 6.246693134307861 = 0.16170868277549744 + 1.0 * 6.084984302520752
Epoch 590, val loss: 0.7430105805397034
Epoch 600, training loss: 6.2356343269348145 = 0.15203972160816193 + 1.0 * 6.083594799041748
Epoch 600, val loss: 0.7452886700630188
Epoch 610, training loss: 6.226972579956055 = 0.14296598732471466 + 1.0 * 6.0840067863464355
Epoch 610, val loss: 0.7478699684143066
Epoch 620, training loss: 6.215185642242432 = 0.1344248354434967 + 1.0 * 6.080760955810547
Epoch 620, val loss: 0.7508294582366943
Epoch 630, training loss: 6.214218616485596 = 0.1264083981513977 + 1.0 * 6.087810039520264
Epoch 630, val loss: 0.7540320754051208
Epoch 640, training loss: 6.201508522033691 = 0.11899448931217194 + 1.0 * 6.082513809204102
Epoch 640, val loss: 0.7573789954185486
Epoch 650, training loss: 6.189650535583496 = 0.11204887926578522 + 1.0 * 6.077601432800293
Epoch 650, val loss: 0.7610695362091064
Epoch 660, training loss: 6.181263446807861 = 0.1055467426776886 + 1.0 * 6.075716495513916
Epoch 660, val loss: 0.7649990916252136
Epoch 670, training loss: 6.178600311279297 = 0.09946207702159882 + 1.0 * 6.079138278961182
Epoch 670, val loss: 0.7691529989242554
Epoch 680, training loss: 6.17662239074707 = 0.09379997849464417 + 1.0 * 6.082822322845459
Epoch 680, val loss: 0.7734318971633911
Epoch 690, training loss: 6.1626787185668945 = 0.08854793757200241 + 1.0 * 6.074131011962891
Epoch 690, val loss: 0.777870237827301
Epoch 700, training loss: 6.154359340667725 = 0.08363404870033264 + 1.0 * 6.070725440979004
Epoch 700, val loss: 0.7825424075126648
Epoch 710, training loss: 6.1547722816467285 = 0.07905112951993942 + 1.0 * 6.075721263885498
Epoch 710, val loss: 0.7873550653457642
Epoch 720, training loss: 6.145913124084473 = 0.07477178424596786 + 1.0 * 6.071141242980957
Epoch 720, val loss: 0.7921965718269348
Epoch 730, training loss: 6.1405029296875 = 0.07080608606338501 + 1.0 * 6.06969690322876
Epoch 730, val loss: 0.7972300052642822
Epoch 740, training loss: 6.136484146118164 = 0.0671011433005333 + 1.0 * 6.069383144378662
Epoch 740, val loss: 0.8023085594177246
Epoch 750, training loss: 6.131592273712158 = 0.06364290416240692 + 1.0 * 6.067949295043945
Epoch 750, val loss: 0.8075136542320251
Epoch 760, training loss: 6.12465238571167 = 0.060429275035858154 + 1.0 * 6.064223289489746
Epoch 760, val loss: 0.8127766251564026
Epoch 770, training loss: 6.127277851104736 = 0.057431403547525406 + 1.0 * 6.0698466300964355
Epoch 770, val loss: 0.8181074857711792
Epoch 780, training loss: 6.120542049407959 = 0.054629094898700714 + 1.0 * 6.06591272354126
Epoch 780, val loss: 0.8233586549758911
Epoch 790, training loss: 6.114457130432129 = 0.05203463137149811 + 1.0 * 6.062422275543213
Epoch 790, val loss: 0.8287379145622253
Epoch 800, training loss: 6.1102614402771 = 0.049592938274145126 + 1.0 * 6.060668468475342
Epoch 800, val loss: 0.8341803550720215
Epoch 810, training loss: 6.113188743591309 = 0.04730909317731857 + 1.0 * 6.065879821777344
Epoch 810, val loss: 0.8396316170692444
Epoch 820, training loss: 6.106748580932617 = 0.04517681151628494 + 1.0 * 6.0615715980529785
Epoch 820, val loss: 0.84496009349823
Epoch 830, training loss: 6.104255199432373 = 0.04318194463849068 + 1.0 * 6.061073303222656
Epoch 830, val loss: 0.8503952622413635
Epoch 840, training loss: 6.099815368652344 = 0.04131360352039337 + 1.0 * 6.058501720428467
Epoch 840, val loss: 0.8557386994361877
Epoch 850, training loss: 6.096074104309082 = 0.039564721286296844 + 1.0 * 6.056509494781494
Epoch 850, val loss: 0.8611086010932922
Epoch 860, training loss: 6.09737491607666 = 0.037917062640190125 + 1.0 * 6.059457778930664
Epoch 860, val loss: 0.8664571642875671
Epoch 870, training loss: 6.093379020690918 = 0.036371879279613495 + 1.0 * 6.057007312774658
Epoch 870, val loss: 0.8717638850212097
Epoch 880, training loss: 6.090160369873047 = 0.034921836107969284 + 1.0 * 6.055238723754883
Epoch 880, val loss: 0.8770277500152588
Epoch 890, training loss: 6.0863237380981445 = 0.033552464097738266 + 1.0 * 6.052771091461182
Epoch 890, val loss: 0.8822934627532959
Epoch 900, training loss: 6.088265419006348 = 0.03225551173090935 + 1.0 * 6.056009769439697
Epoch 900, val loss: 0.887572705745697
Epoch 910, training loss: 6.084726333618164 = 0.031045371666550636 + 1.0 * 6.053680896759033
Epoch 910, val loss: 0.8925743103027344
Epoch 920, training loss: 6.082711696624756 = 0.029899824410676956 + 1.0 * 6.052812099456787
Epoch 920, val loss: 0.8976589441299438
Epoch 930, training loss: 6.076964378356934 = 0.028818747028708458 + 1.0 * 6.048145771026611
Epoch 930, val loss: 0.9027681350708008
Epoch 940, training loss: 6.075600624084473 = 0.027787864208221436 + 1.0 * 6.0478129386901855
Epoch 940, val loss: 0.9078890681266785
Epoch 950, training loss: 6.088448524475098 = 0.02681724727153778 + 1.0 * 6.061631202697754
Epoch 950, val loss: 0.9127891063690186
Epoch 960, training loss: 6.07712459564209 = 0.025901785120368004 + 1.0 * 6.051222801208496
Epoch 960, val loss: 0.9175078868865967
Epoch 970, training loss: 6.07125997543335 = 0.02503586746752262 + 1.0 * 6.046224117279053
Epoch 970, val loss: 0.9224019646644592
Epoch 980, training loss: 6.068822860717773 = 0.02420787699520588 + 1.0 * 6.044614791870117
Epoch 980, val loss: 0.9272979497909546
Epoch 990, training loss: 6.068332672119141 = 0.023419033735990524 + 1.0 * 6.0449137687683105
Epoch 990, val loss: 0.9321331977844238
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.8376
Flip ASR: 0.8089/225 nodes
The final ASR:0.71341, 0.08817, Accuracy:0.79877, 0.01552
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11542])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10410])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00460, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.324051856994629 = 1.9501638412475586 + 1.0 * 8.37388801574707
Epoch 0, val loss: 1.9509310722351074
Epoch 10, training loss: 10.313576698303223 = 1.9400442838668823 + 1.0 * 8.37353229522705
Epoch 10, val loss: 1.9412027597427368
Epoch 20, training loss: 10.298042297363281 = 1.9273607730865479 + 1.0 * 8.370681762695312
Epoch 20, val loss: 1.928517460823059
Epoch 30, training loss: 10.257669448852539 = 1.9096746444702148 + 1.0 * 8.347994804382324
Epoch 30, val loss: 1.9103971719741821
Epoch 40, training loss: 10.092617988586426 = 1.8864699602127075 + 1.0 * 8.206148147583008
Epoch 40, val loss: 1.8874067068099976
Epoch 50, training loss: 9.563286781311035 = 1.8617007732391357 + 1.0 * 7.7015862464904785
Epoch 50, val loss: 1.8641237020492554
Epoch 60, training loss: 9.253012657165527 = 1.839308261871338 + 1.0 * 7.4137043952941895
Epoch 60, val loss: 1.8446855545043945
Epoch 70, training loss: 8.858773231506348 = 1.822589635848999 + 1.0 * 7.0361833572387695
Epoch 70, val loss: 1.8304182291030884
Epoch 80, training loss: 8.592639923095703 = 1.8093390464782715 + 1.0 * 6.78330135345459
Epoch 80, val loss: 1.8186737298965454
Epoch 90, training loss: 8.439135551452637 = 1.7944787740707397 + 1.0 * 6.644657135009766
Epoch 90, val loss: 1.805114507675171
Epoch 100, training loss: 8.33559513092041 = 1.7778816223144531 + 1.0 * 6.557713508605957
Epoch 100, val loss: 1.7907614707946777
Epoch 110, training loss: 8.254772186279297 = 1.7615909576416016 + 1.0 * 6.493181228637695
Epoch 110, val loss: 1.7769449949264526
Epoch 120, training loss: 8.182994842529297 = 1.744958519935608 + 1.0 * 6.43803596496582
Epoch 120, val loss: 1.7627538442611694
Epoch 130, training loss: 8.109234809875488 = 1.7270740270614624 + 1.0 * 6.382160663604736
Epoch 130, val loss: 1.7474929094314575
Epoch 140, training loss: 8.044742584228516 = 1.7067513465881348 + 1.0 * 6.337991237640381
Epoch 140, val loss: 1.7305316925048828
Epoch 150, training loss: 7.985596179962158 = 1.6833038330078125 + 1.0 * 6.302292346954346
Epoch 150, val loss: 1.711313247680664
Epoch 160, training loss: 7.930792808532715 = 1.6556575298309326 + 1.0 * 6.275135517120361
Epoch 160, val loss: 1.6888140439987183
Epoch 170, training loss: 7.877232074737549 = 1.6229771375656128 + 1.0 * 6.2542548179626465
Epoch 170, val loss: 1.6621192693710327
Epoch 180, training loss: 7.823336601257324 = 1.5848709344863892 + 1.0 * 6.238465785980225
Epoch 180, val loss: 1.6309243440628052
Epoch 190, training loss: 7.76585578918457 = 1.5414406061172485 + 1.0 * 6.224415302276611
Epoch 190, val loss: 1.595428228378296
Epoch 200, training loss: 7.706009864807129 = 1.4932012557983398 + 1.0 * 6.212808609008789
Epoch 200, val loss: 1.556122899055481
Epoch 210, training loss: 7.642488479614258 = 1.4406534433364868 + 1.0 * 6.2018351554870605
Epoch 210, val loss: 1.5134674310684204
Epoch 220, training loss: 7.5786333084106445 = 1.3842052221298218 + 1.0 * 6.194427967071533
Epoch 220, val loss: 1.4679911136627197
Epoch 230, training loss: 7.51176643371582 = 1.3255231380462646 + 1.0 * 6.186243534088135
Epoch 230, val loss: 1.4217736721038818
Epoch 240, training loss: 7.444047451019287 = 1.2656718492507935 + 1.0 * 6.178375720977783
Epoch 240, val loss: 1.3752820491790771
Epoch 250, training loss: 7.381924629211426 = 1.2046219110488892 + 1.0 * 6.177302837371826
Epoch 250, val loss: 1.3286000490188599
Epoch 260, training loss: 7.311920166015625 = 1.1444133520126343 + 1.0 * 6.167506694793701
Epoch 260, val loss: 1.2829043865203857
Epoch 270, training loss: 7.243781089782715 = 1.0849645137786865 + 1.0 * 6.158816814422607
Epoch 270, val loss: 1.2382348775863647
Epoch 280, training loss: 7.189258575439453 = 1.0264819860458374 + 1.0 * 6.162776470184326
Epoch 280, val loss: 1.194632887840271
Epoch 290, training loss: 7.120584487915039 = 0.9708535671234131 + 1.0 * 6.149730682373047
Epoch 290, val loss: 1.153118371963501
Epoch 300, training loss: 7.06102180480957 = 0.9174008369445801 + 1.0 * 6.14362096786499
Epoch 300, val loss: 1.113610029220581
Epoch 310, training loss: 7.005183696746826 = 0.8660195469856262 + 1.0 * 6.139163970947266
Epoch 310, val loss: 1.0756582021713257
Epoch 320, training loss: 6.959145545959473 = 0.8171274662017822 + 1.0 * 6.1420183181762695
Epoch 320, val loss: 1.0396535396575928
Epoch 330, training loss: 6.9045305252075195 = 0.7718198299407959 + 1.0 * 6.132710933685303
Epoch 330, val loss: 1.0066356658935547
Epoch 340, training loss: 6.860281944274902 = 0.7294012308120728 + 1.0 * 6.130880832672119
Epoch 340, val loss: 0.9763482213020325
Epoch 350, training loss: 6.814657211303711 = 0.6897582411766052 + 1.0 * 6.124898910522461
Epoch 350, val loss: 0.9485686421394348
Epoch 360, training loss: 6.7739105224609375 = 0.6524357795715332 + 1.0 * 6.121474742889404
Epoch 360, val loss: 0.9232723712921143
Epoch 370, training loss: 6.7364325523376465 = 0.6173656582832336 + 1.0 * 6.1190667152404785
Epoch 370, val loss: 0.9002771973609924
Epoch 380, training loss: 6.69881010055542 = 0.5845860242843628 + 1.0 * 6.114223957061768
Epoch 380, val loss: 0.8797956109046936
Epoch 390, training loss: 6.665212631225586 = 0.5535925030708313 + 1.0 * 6.11161994934082
Epoch 390, val loss: 0.8614736199378967
Epoch 400, training loss: 6.637575149536133 = 0.5242969393730164 + 1.0 * 6.113278388977051
Epoch 400, val loss: 0.8449811339378357
Epoch 410, training loss: 6.604440689086914 = 0.4966990649700165 + 1.0 * 6.107741832733154
Epoch 410, val loss: 0.8306045532226562
Epoch 420, training loss: 6.574349403381348 = 0.47047749161720276 + 1.0 * 6.103871822357178
Epoch 420, val loss: 0.8179246783256531
Epoch 430, training loss: 6.554388999938965 = 0.4453916549682617 + 1.0 * 6.108997344970703
Epoch 430, val loss: 0.8064855933189392
Epoch 440, training loss: 6.523225784301758 = 0.4214935600757599 + 1.0 * 6.10173225402832
Epoch 440, val loss: 0.7963891625404358
Epoch 450, training loss: 6.495115756988525 = 0.3986701965332031 + 1.0 * 6.096445560455322
Epoch 450, val loss: 0.787745475769043
Epoch 460, training loss: 6.473215579986572 = 0.3766844570636749 + 1.0 * 6.096530914306641
Epoch 460, val loss: 0.7801730036735535
Epoch 470, training loss: 6.45368766784668 = 0.355774849653244 + 1.0 * 6.097912788391113
Epoch 470, val loss: 0.7736078500747681
Epoch 480, training loss: 6.427196025848389 = 0.3359147310256958 + 1.0 * 6.091281414031982
Epoch 480, val loss: 0.7683957815170288
Epoch 490, training loss: 6.406930923461914 = 0.3168712258338928 + 1.0 * 6.090059757232666
Epoch 490, val loss: 0.7642080783843994
Epoch 500, training loss: 6.389517784118652 = 0.2985595166683197 + 1.0 * 6.090958118438721
Epoch 500, val loss: 0.7608543634414673
Epoch 510, training loss: 6.37434196472168 = 0.28120848536491394 + 1.0 * 6.093133449554443
Epoch 510, val loss: 0.7584561109542847
Epoch 520, training loss: 6.356209754943848 = 0.2647102177143097 + 1.0 * 6.091499328613281
Epoch 520, val loss: 0.7571319341659546
Epoch 530, training loss: 6.333960056304932 = 0.24909794330596924 + 1.0 * 6.084862232208252
Epoch 530, val loss: 0.7566473484039307
Epoch 540, training loss: 6.315819263458252 = 0.23421576619148254 + 1.0 * 6.081603527069092
Epoch 540, val loss: 0.7568854093551636
Epoch 550, training loss: 6.299802780151367 = 0.22006477415561676 + 1.0 * 6.079738140106201
Epoch 550, val loss: 0.7578766942024231
Epoch 560, training loss: 6.291310787200928 = 0.2067108303308487 + 1.0 * 6.08459997177124
Epoch 560, val loss: 0.7594220042228699
Epoch 570, training loss: 6.273194313049316 = 0.19430018961429596 + 1.0 * 6.078894138336182
Epoch 570, val loss: 0.7617039680480957
Epoch 580, training loss: 6.2582292556762695 = 0.18261797726154327 + 1.0 * 6.075611114501953
Epoch 580, val loss: 0.7647194862365723
Epoch 590, training loss: 6.250110626220703 = 0.17161796987056732 + 1.0 * 6.078492641448975
Epoch 590, val loss: 0.7681611776351929
Epoch 600, training loss: 6.241828918457031 = 0.16140444576740265 + 1.0 * 6.0804243087768555
Epoch 600, val loss: 0.7720797061920166
Epoch 610, training loss: 6.2245097160339355 = 0.15188173949718475 + 1.0 * 6.072628021240234
Epoch 610, val loss: 0.7765607833862305
Epoch 620, training loss: 6.215511322021484 = 0.1429891437292099 + 1.0 * 6.072522163391113
Epoch 620, val loss: 0.7815313339233398
Epoch 630, training loss: 6.206590175628662 = 0.13470901548862457 + 1.0 * 6.071881294250488
Epoch 630, val loss: 0.7867651581764221
Epoch 640, training loss: 6.196859836578369 = 0.12702111899852753 + 1.0 * 6.069838523864746
Epoch 640, val loss: 0.7924026846885681
Epoch 650, training loss: 6.193224906921387 = 0.11986053735017776 + 1.0 * 6.0733642578125
Epoch 650, val loss: 0.79831862449646
Epoch 660, training loss: 6.181353569030762 = 0.11324310302734375 + 1.0 * 6.068110466003418
Epoch 660, val loss: 0.8045303821563721
Epoch 670, training loss: 6.172530651092529 = 0.10706692188978195 + 1.0 * 6.065463542938232
Epoch 670, val loss: 0.8109774589538574
Epoch 680, training loss: 6.1771063804626465 = 0.10131657123565674 + 1.0 * 6.075789928436279
Epoch 680, val loss: 0.8175898194313049
Epoch 690, training loss: 6.163872718811035 = 0.09602895379066467 + 1.0 * 6.067843914031982
Epoch 690, val loss: 0.8243275284767151
Epoch 700, training loss: 6.153400421142578 = 0.09110113233327866 + 1.0 * 6.0622992515563965
Epoch 700, val loss: 0.8313905596733093
Epoch 710, training loss: 6.150356292724609 = 0.08649487048387527 + 1.0 * 6.06386137008667
Epoch 710, val loss: 0.8384702801704407
Epoch 720, training loss: 6.145175933837891 = 0.08220558613538742 + 1.0 * 6.062970161437988
Epoch 720, val loss: 0.84556645154953
Epoch 730, training loss: 6.140174388885498 = 0.07821556180715561 + 1.0 * 6.0619587898254395
Epoch 730, val loss: 0.8528781533241272
Epoch 740, training loss: 6.135719299316406 = 0.07446841150522232 + 1.0 * 6.061250686645508
Epoch 740, val loss: 0.8601571917533875
Epoch 750, training loss: 6.129932880401611 = 0.07096493989229202 + 1.0 * 6.0589680671691895
Epoch 750, val loss: 0.867582380771637
Epoch 760, training loss: 6.128690242767334 = 0.06767463684082031 + 1.0 * 6.061015605926514
Epoch 760, val loss: 0.8749318718910217
Epoch 770, training loss: 6.122649192810059 = 0.06460200995206833 + 1.0 * 6.058047294616699
Epoch 770, val loss: 0.88232421875
Epoch 780, training loss: 6.118043422698975 = 0.061719078570604324 + 1.0 * 6.056324481964111
Epoch 780, val loss: 0.889693558216095
Epoch 790, training loss: 6.119010925292969 = 0.05901378020644188 + 1.0 * 6.059997081756592
Epoch 790, val loss: 0.8970522284507751
Epoch 800, training loss: 6.109983921051025 = 0.056470926851034164 + 1.0 * 6.053513050079346
Epoch 800, val loss: 0.9042832851409912
Epoch 810, training loss: 6.106201648712158 = 0.0540747307240963 + 1.0 * 6.052126884460449
Epoch 810, val loss: 0.9116318821907043
Epoch 820, training loss: 6.103985786437988 = 0.051804784685373306 + 1.0 * 6.052180767059326
Epoch 820, val loss: 0.9189148545265198
Epoch 830, training loss: 6.10984468460083 = 0.0496610552072525 + 1.0 * 6.060183525085449
Epoch 830, val loss: 0.9260388016700745
Epoch 840, training loss: 6.105309963226318 = 0.04765744134783745 + 1.0 * 6.057652473449707
Epoch 840, val loss: 0.9331067800521851
Epoch 850, training loss: 6.095890998840332 = 0.04577269405126572 + 1.0 * 6.050118446350098
Epoch 850, val loss: 0.9402824640274048
Epoch 860, training loss: 6.093110084533691 = 0.043983094394207 + 1.0 * 6.049127101898193
Epoch 860, val loss: 0.9474242329597473
Epoch 870, training loss: 6.0928544998168945 = 0.042285386472940445 + 1.0 * 6.0505690574646
Epoch 870, val loss: 0.9544550180435181
Epoch 880, training loss: 6.090156078338623 = 0.040676943957805634 + 1.0 * 6.049479007720947
Epoch 880, val loss: 0.9613247513771057
Epoch 890, training loss: 6.0851030349731445 = 0.03915302827954292 + 1.0 * 6.045949935913086
Epoch 890, val loss: 0.968193769454956
Epoch 900, training loss: 6.084597587585449 = 0.03770648315548897 + 1.0 * 6.046891212463379
Epoch 900, val loss: 0.9751114249229431
Epoch 910, training loss: 6.090234756469727 = 0.03632958605885506 + 1.0 * 6.053905010223389
Epoch 910, val loss: 0.9817977547645569
Epoch 920, training loss: 6.081783771514893 = 0.035033728927373886 + 1.0 * 6.046750068664551
Epoch 920, val loss: 0.9885348677635193
Epoch 930, training loss: 6.0867228507995605 = 0.03379886597394943 + 1.0 * 6.052924156188965
Epoch 930, val loss: 0.995119571685791
Epoch 940, training loss: 6.07659912109375 = 0.032629963010549545 + 1.0 * 6.04396915435791
Epoch 940, val loss: 1.0016611814498901
Epoch 950, training loss: 6.073310375213623 = 0.03151782974600792 + 1.0 * 6.041792392730713
Epoch 950, val loss: 1.0082175731658936
Epoch 960, training loss: 6.075607776641846 = 0.030454734340310097 + 1.0 * 6.0451531410217285
Epoch 960, val loss: 1.0146390199661255
Epoch 970, training loss: 6.072049140930176 = 0.02944108657538891 + 1.0 * 6.042608261108398
Epoch 970, val loss: 1.0209274291992188
Epoch 980, training loss: 6.07785701751709 = 0.028476713225245476 + 1.0 * 6.049380302429199
Epoch 980, val loss: 1.0271714925765991
Epoch 990, training loss: 6.0697808265686035 = 0.027562079951167107 + 1.0 * 6.0422186851501465
Epoch 990, val loss: 1.0334429740905762
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6125
Flip ASR: 0.5378/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.316697120666504 = 1.9428071975708008 + 1.0 * 8.373889923095703
Epoch 0, val loss: 1.9315859079360962
Epoch 10, training loss: 10.305866241455078 = 1.9323030710220337 + 1.0 * 8.373562812805176
Epoch 10, val loss: 1.9208778142929077
Epoch 20, training loss: 10.290675163269043 = 1.919351577758789 + 1.0 * 8.371323585510254
Epoch 20, val loss: 1.9076194763183594
Epoch 30, training loss: 10.256269454956055 = 1.9012359380722046 + 1.0 * 8.355033874511719
Epoch 30, val loss: 1.8892269134521484
Epoch 40, training loss: 10.137460708618164 = 1.8767575025558472 + 1.0 * 8.260703086853027
Epoch 40, val loss: 1.8656129837036133
Epoch 50, training loss: 9.624032020568848 = 1.850701928138733 + 1.0 * 7.773330211639404
Epoch 50, val loss: 1.8414756059646606
Epoch 60, training loss: 9.087413787841797 = 1.8269662857055664 + 1.0 * 7.260447025299072
Epoch 60, val loss: 1.8197284936904907
Epoch 70, training loss: 8.718722343444824 = 1.807991862297058 + 1.0 * 6.910730838775635
Epoch 70, val loss: 1.8018094301223755
Epoch 80, training loss: 8.490824699401855 = 1.7904986143112183 + 1.0 * 6.700326442718506
Epoch 80, val loss: 1.785165786743164
Epoch 90, training loss: 8.358555793762207 = 1.7725425958633423 + 1.0 * 6.586012840270996
Epoch 90, val loss: 1.7681769132614136
Epoch 100, training loss: 8.278261184692383 = 1.752514123916626 + 1.0 * 6.525747299194336
Epoch 100, val loss: 1.7499046325683594
Epoch 110, training loss: 8.206525802612305 = 1.7304741144180298 + 1.0 * 6.476051330566406
Epoch 110, val loss: 1.7299729585647583
Epoch 120, training loss: 8.131176948547363 = 1.7064672708511353 + 1.0 * 6.424709320068359
Epoch 120, val loss: 1.708356499671936
Epoch 130, training loss: 8.058831214904785 = 1.6797385215759277 + 1.0 * 6.379092693328857
Epoch 130, val loss: 1.6847577095031738
Epoch 140, training loss: 7.995670318603516 = 1.648819923400879 + 1.0 * 6.346850395202637
Epoch 140, val loss: 1.6579108238220215
Epoch 150, training loss: 7.935568332672119 = 1.6116634607315063 + 1.0 * 6.323904991149902
Epoch 150, val loss: 1.626020073890686
Epoch 160, training loss: 7.871718406677246 = 1.5669379234313965 + 1.0 * 6.30478048324585
Epoch 160, val loss: 1.5882782936096191
Epoch 170, training loss: 7.8027143478393555 = 1.5152912139892578 + 1.0 * 6.287423133850098
Epoch 170, val loss: 1.5453077554702759
Epoch 180, training loss: 7.729121208190918 = 1.4575144052505493 + 1.0 * 6.271606922149658
Epoch 180, val loss: 1.498199224472046
Epoch 190, training loss: 7.653430938720703 = 1.3945229053497314 + 1.0 * 6.258908271789551
Epoch 190, val loss: 1.447919487953186
Epoch 200, training loss: 7.57403039932251 = 1.3289517164230347 + 1.0 * 6.2450785636901855
Epoch 200, val loss: 1.396735668182373
Epoch 210, training loss: 7.496527671813965 = 1.2630943059921265 + 1.0 * 6.233433246612549
Epoch 210, val loss: 1.3465216159820557
Epoch 220, training loss: 7.419290065765381 = 1.197752594947815 + 1.0 * 6.2215375900268555
Epoch 220, val loss: 1.2978366613388062
Epoch 230, training loss: 7.349871635437012 = 1.1327365636825562 + 1.0 * 6.217134952545166
Epoch 230, val loss: 1.249843955039978
Epoch 240, training loss: 7.272758960723877 = 1.0695689916610718 + 1.0 * 6.203189849853516
Epoch 240, val loss: 1.203432559967041
Epoch 250, training loss: 7.203388214111328 = 1.00736665725708 + 1.0 * 6.196021556854248
Epoch 250, val loss: 1.158061146736145
Epoch 260, training loss: 7.139403820037842 = 0.9470182657241821 + 1.0 * 6.192385673522949
Epoch 260, val loss: 1.1145378351211548
Epoch 270, training loss: 7.072666645050049 = 0.8900681138038635 + 1.0 * 6.18259859085083
Epoch 270, val loss: 1.0734829902648926
Epoch 280, training loss: 7.012180328369141 = 0.8364579081535339 + 1.0 * 6.175722599029541
Epoch 280, val loss: 1.0353529453277588
Epoch 290, training loss: 6.9669084548950195 = 0.7869033217430115 + 1.0 * 6.180005073547363
Epoch 290, val loss: 1.0005559921264648
Epoch 300, training loss: 6.9080705642700195 = 0.7427402138710022 + 1.0 * 6.165330410003662
Epoch 300, val loss: 0.9702504277229309
Epoch 310, training loss: 6.863346099853516 = 0.7028151154518127 + 1.0 * 6.160531044006348
Epoch 310, val loss: 0.9441003203392029
Epoch 320, training loss: 6.822336673736572 = 0.6663679480552673 + 1.0 * 6.15596866607666
Epoch 320, val loss: 0.920915961265564
Epoch 330, training loss: 6.786624908447266 = 0.6327822804450989 + 1.0 * 6.153842449188232
Epoch 330, val loss: 0.9004461765289307
Epoch 340, training loss: 6.7590765953063965 = 0.6024156808853149 + 1.0 * 6.156661033630371
Epoch 340, val loss: 0.8825095891952515
Epoch 350, training loss: 6.71964693069458 = 0.5746976137161255 + 1.0 * 6.144949436187744
Epoch 350, val loss: 0.8669508099555969
Epoch 360, training loss: 6.689987659454346 = 0.5486529469490051 + 1.0 * 6.141334533691406
Epoch 360, val loss: 0.8528973460197449
Epoch 370, training loss: 6.660594940185547 = 0.5239217281341553 + 1.0 * 6.136673450469971
Epoch 370, val loss: 0.8398187160491943
Epoch 380, training loss: 6.6332197189331055 = 0.5003229975700378 + 1.0 * 6.132896900177002
Epoch 380, val loss: 0.8279544115066528
Epoch 390, training loss: 6.612278461456299 = 0.47786542773246765 + 1.0 * 6.134413242340088
Epoch 390, val loss: 0.817145586013794
Epoch 400, training loss: 6.585146427154541 = 0.45681333541870117 + 1.0 * 6.12833309173584
Epoch 400, val loss: 0.8074997067451477
Epoch 410, training loss: 6.560777187347412 = 0.43674346804618835 + 1.0 * 6.1240339279174805
Epoch 410, val loss: 0.7990773916244507
Epoch 420, training loss: 6.537617206573486 = 0.4175030589103699 + 1.0 * 6.120114326477051
Epoch 420, val loss: 0.7915304899215698
Epoch 430, training loss: 6.528791904449463 = 0.3990189731121063 + 1.0 * 6.129773139953613
Epoch 430, val loss: 0.7848700284957886
Epoch 440, training loss: 6.501038074493408 = 0.38152700662612915 + 1.0 * 6.119511127471924
Epoch 440, val loss: 0.7789376378059387
Epoch 450, training loss: 6.477538108825684 = 0.36476385593414307 + 1.0 * 6.11277437210083
Epoch 450, val loss: 0.7739572525024414
Epoch 460, training loss: 6.458165168762207 = 0.3485545516014099 + 1.0 * 6.109610557556152
Epoch 460, val loss: 0.7696985602378845
Epoch 470, training loss: 6.446795463562012 = 0.3328140676021576 + 1.0 * 6.113981246948242
Epoch 470, val loss: 0.7661290168762207
Epoch 480, training loss: 6.424487113952637 = 0.31773531436920166 + 1.0 * 6.106751918792725
Epoch 480, val loss: 0.7630422115325928
Epoch 490, training loss: 6.40606689453125 = 0.30315858125686646 + 1.0 * 6.102908134460449
Epoch 490, val loss: 0.760641872882843
Epoch 500, training loss: 6.3899078369140625 = 0.2889920175075531 + 1.0 * 6.100915908813477
Epoch 500, val loss: 0.7589132785797119
Epoch 510, training loss: 6.3852128982543945 = 0.275235652923584 + 1.0 * 6.1099772453308105
Epoch 510, val loss: 0.7577111124992371
Epoch 520, training loss: 6.360692501068115 = 0.2620285153388977 + 1.0 * 6.098663806915283
Epoch 520, val loss: 0.7569881081581116
Epoch 530, training loss: 6.346036434173584 = 0.24922920763492584 + 1.0 * 6.09680700302124
Epoch 530, val loss: 0.7568954825401306
Epoch 540, training loss: 6.335076332092285 = 0.23680704832077026 + 1.0 * 6.098269462585449
Epoch 540, val loss: 0.7574257850646973
Epoch 550, training loss: 6.321792125701904 = 0.22491152584552765 + 1.0 * 6.0968804359436035
Epoch 550, val loss: 0.7582171559333801
Epoch 560, training loss: 6.304869174957275 = 0.21346305310726166 + 1.0 * 6.091406345367432
Epoch 560, val loss: 0.7596659064292908
Epoch 570, training loss: 6.291457176208496 = 0.20243829488754272 + 1.0 * 6.089018821716309
Epoch 570, val loss: 0.7616501450538635
Epoch 580, training loss: 6.280038833618164 = 0.19183902442455292 + 1.0 * 6.088199615478516
Epoch 580, val loss: 0.7641844749450684
Epoch 590, training loss: 6.270939350128174 = 0.18173818290233612 + 1.0 * 6.089200973510742
Epoch 590, val loss: 0.7670645117759705
Epoch 600, training loss: 6.262444019317627 = 0.1721869558095932 + 1.0 * 6.090257167816162
Epoch 600, val loss: 0.7704344391822815
Epoch 610, training loss: 6.248219966888428 = 0.16313454508781433 + 1.0 * 6.085085391998291
Epoch 610, val loss: 0.7743893265724182
Epoch 620, training loss: 6.235924243927002 = 0.1545371413230896 + 1.0 * 6.081387042999268
Epoch 620, val loss: 0.7788193225860596
Epoch 630, training loss: 6.231111526489258 = 0.14636151492595673 + 1.0 * 6.084750175476074
Epoch 630, val loss: 0.783650815486908
Epoch 640, training loss: 6.221398830413818 = 0.1386687010526657 + 1.0 * 6.082730293273926
Epoch 640, val loss: 0.7889072299003601
Epoch 650, training loss: 6.211119651794434 = 0.13145434856414795 + 1.0 * 6.079665184020996
Epoch 650, val loss: 0.7944873571395874
Epoch 660, training loss: 6.2048187255859375 = 0.12468285113573074 + 1.0 * 6.080135822296143
Epoch 660, val loss: 0.8004443645477295
Epoch 670, training loss: 6.196893692016602 = 0.11836998909711838 + 1.0 * 6.078523635864258
Epoch 670, val loss: 0.8065744638442993
Epoch 680, training loss: 6.187716007232666 = 0.11243685334920883 + 1.0 * 6.075279235839844
Epoch 680, val loss: 0.8130552172660828
Epoch 690, training loss: 6.1799540519714355 = 0.10684122145175934 + 1.0 * 6.073112964630127
Epoch 690, val loss: 0.8198714256286621
Epoch 700, training loss: 6.173687934875488 = 0.10155986994504929 + 1.0 * 6.0721282958984375
Epoch 700, val loss: 0.8268672227859497
Epoch 710, training loss: 6.168931484222412 = 0.09659158438444138 + 1.0 * 6.07234001159668
Epoch 710, val loss: 0.8339790105819702
Epoch 720, training loss: 6.1629204750061035 = 0.09193848073482513 + 1.0 * 6.070981979370117
Epoch 720, val loss: 0.841172456741333
Epoch 730, training loss: 6.156742095947266 = 0.08754759281873703 + 1.0 * 6.069194316864014
Epoch 730, val loss: 0.848504900932312
Epoch 740, training loss: 6.158377647399902 = 0.08339275419712067 + 1.0 * 6.074985027313232
Epoch 740, val loss: 0.8559859991073608
Epoch 750, training loss: 6.150326728820801 = 0.07945901155471802 + 1.0 * 6.070867538452148
Epoch 750, val loss: 0.8635531663894653
Epoch 760, training loss: 6.155359745025635 = 0.07574578374624252 + 1.0 * 6.079614162445068
Epoch 760, val loss: 0.8710378408432007
Epoch 770, training loss: 6.13933801651001 = 0.07225348800420761 + 1.0 * 6.067084312438965
Epoch 770, val loss: 0.8786149621009827
Epoch 780, training loss: 6.131713390350342 = 0.06892972439527512 + 1.0 * 6.062783718109131
Epoch 780, val loss: 0.8861923813819885
Epoch 790, training loss: 6.127437114715576 = 0.06576192378997803 + 1.0 * 6.061675071716309
Epoch 790, val loss: 0.8937757015228271
Epoch 800, training loss: 6.12351655960083 = 0.06273885816335678 + 1.0 * 6.06077766418457
Epoch 800, val loss: 0.9013877511024475
Epoch 810, training loss: 6.132103443145752 = 0.059858839958906174 + 1.0 * 6.072244644165039
Epoch 810, val loss: 0.909062385559082
Epoch 820, training loss: 6.124640941619873 = 0.05716029554605484 + 1.0 * 6.067480564117432
Epoch 820, val loss: 0.9164510369300842
Epoch 830, training loss: 6.1149725914001465 = 0.05461311340332031 + 1.0 * 6.060359477996826
Epoch 830, val loss: 0.9238285422325134
Epoch 840, training loss: 6.109820365905762 = 0.05219529569149017 + 1.0 * 6.0576252937316895
Epoch 840, val loss: 0.931268572807312
Epoch 850, training loss: 6.111055850982666 = 0.0498991422355175 + 1.0 * 6.061156749725342
Epoch 850, val loss: 0.9386930465698242
Epoch 860, training loss: 6.105106353759766 = 0.04772960767149925 + 1.0 * 6.057376861572266
Epoch 860, val loss: 0.9459805488586426
Epoch 870, training loss: 6.100414276123047 = 0.04567831754684448 + 1.0 * 6.054736137390137
Epoch 870, val loss: 0.9532896876335144
Epoch 880, training loss: 6.097323894500732 = 0.04372638463973999 + 1.0 * 6.053597450256348
Epoch 880, val loss: 0.9605914354324341
Epoch 890, training loss: 6.1045074462890625 = 0.04187145084142685 + 1.0 * 6.062635898590088
Epoch 890, val loss: 0.9679375290870667
Epoch 900, training loss: 6.094013214111328 = 0.0401153601706028 + 1.0 * 6.053897857666016
Epoch 900, val loss: 0.9751363396644592
Epoch 910, training loss: 6.093667507171631 = 0.03844459354877472 + 1.0 * 6.055222988128662
Epoch 910, val loss: 0.9823415279388428
Epoch 920, training loss: 6.088618755340576 = 0.036863356828689575 + 1.0 * 6.051755428314209
Epoch 920, val loss: 0.9895076155662537
Epoch 930, training loss: 6.0856099128723145 = 0.035358671098947525 + 1.0 * 6.050251007080078
Epoch 930, val loss: 0.9965856671333313
Epoch 940, training loss: 6.083550930023193 = 0.033925727009773254 + 1.0 * 6.049625396728516
Epoch 940, val loss: 1.0036871433258057
Epoch 950, training loss: 6.087871074676514 = 0.03256573900580406 + 1.0 * 6.055305480957031
Epoch 950, val loss: 1.0107468366622925
Epoch 960, training loss: 6.08099889755249 = 0.031288933008909225 + 1.0 * 6.049709796905518
Epoch 960, val loss: 1.0176317691802979
Epoch 970, training loss: 6.075984001159668 = 0.030079973861575127 + 1.0 * 6.045904159545898
Epoch 970, val loss: 1.0245885848999023
Epoch 980, training loss: 6.078378677368164 = 0.028932619839906693 + 1.0 * 6.049446105957031
Epoch 980, val loss: 1.0314997434616089
Epoch 990, training loss: 6.072980880737305 = 0.0278524961322546 + 1.0 * 6.045128345489502
Epoch 990, val loss: 1.0382261276245117
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6089
Flip ASR: 0.5467/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.318384170532227 = 1.9445070028305054 + 1.0 * 8.37387752532959
Epoch 0, val loss: 1.9445548057556152
Epoch 10, training loss: 10.307394981384277 = 1.933986783027649 + 1.0 * 8.373408317565918
Epoch 10, val loss: 1.933344841003418
Epoch 20, training loss: 10.290742874145508 = 1.9206198453903198 + 1.0 * 8.370122909545898
Epoch 20, val loss: 1.918959379196167
Epoch 30, training loss: 10.24866771697998 = 1.901962161064148 + 1.0 * 8.346705436706543
Epoch 30, val loss: 1.899049162864685
Epoch 40, training loss: 10.079925537109375 = 1.8779053688049316 + 1.0 * 8.202019691467285
Epoch 40, val loss: 1.8747978210449219
Epoch 50, training loss: 9.428529739379883 = 1.8521701097488403 + 1.0 * 7.576359272003174
Epoch 50, val loss: 1.849433183670044
Epoch 60, training loss: 9.061186790466309 = 1.8299697637557983 + 1.0 * 7.231216907501221
Epoch 60, val loss: 1.8294891119003296
Epoch 70, training loss: 8.793106079101562 = 1.8118633031845093 + 1.0 * 6.981242656707764
Epoch 70, val loss: 1.812952995300293
Epoch 80, training loss: 8.554985046386719 = 1.794357419013977 + 1.0 * 6.760627269744873
Epoch 80, val loss: 1.7972620725631714
Epoch 90, training loss: 8.405970573425293 = 1.7790825366973877 + 1.0 * 6.626887798309326
Epoch 90, val loss: 1.7834546566009521
Epoch 100, training loss: 8.314583778381348 = 1.7634925842285156 + 1.0 * 6.551091194152832
Epoch 100, val loss: 1.7697633504867554
Epoch 110, training loss: 8.231881141662598 = 1.7467561960220337 + 1.0 * 6.485124588012695
Epoch 110, val loss: 1.75557541847229
Epoch 120, training loss: 8.172536849975586 = 1.7284786701202393 + 1.0 * 6.444058418273926
Epoch 120, val loss: 1.7401660680770874
Epoch 130, training loss: 8.115954399108887 = 1.7073243856430054 + 1.0 * 6.40863037109375
Epoch 130, val loss: 1.7223005294799805
Epoch 140, training loss: 8.055442810058594 = 1.6824005842208862 + 1.0 * 6.373042583465576
Epoch 140, val loss: 1.7015695571899414
Epoch 150, training loss: 7.99332857131958 = 1.6534442901611328 + 1.0 * 6.339884281158447
Epoch 150, val loss: 1.6781690120697021
Epoch 160, training loss: 7.931222915649414 = 1.6200776100158691 + 1.0 * 6.311145305633545
Epoch 160, val loss: 1.6517393589019775
Epoch 170, training loss: 7.870028018951416 = 1.5810532569885254 + 1.0 * 6.288974761962891
Epoch 170, val loss: 1.6209379434585571
Epoch 180, training loss: 7.803553581237793 = 1.5366686582565308 + 1.0 * 6.266884803771973
Epoch 180, val loss: 1.5858393907546997
Epoch 190, training loss: 7.737188339233398 = 1.4866453409194946 + 1.0 * 6.250543117523193
Epoch 190, val loss: 1.5462948083877563
Epoch 200, training loss: 7.668881416320801 = 1.4320847988128662 + 1.0 * 6.2367963790893555
Epoch 200, val loss: 1.5033642053604126
Epoch 210, training loss: 7.598127365112305 = 1.374211311340332 + 1.0 * 6.223916053771973
Epoch 210, val loss: 1.4581456184387207
Epoch 220, training loss: 7.529038429260254 = 1.3140928745269775 + 1.0 * 6.214945316314697
Epoch 220, val loss: 1.4117732048034668
Epoch 230, training loss: 7.457780361175537 = 1.2530527114868164 + 1.0 * 6.204727649688721
Epoch 230, val loss: 1.3652360439300537
Epoch 240, training loss: 7.391328811645508 = 1.1913788318634033 + 1.0 * 6.199950218200684
Epoch 240, val loss: 1.3186224699020386
Epoch 250, training loss: 7.321138381958008 = 1.130192518234253 + 1.0 * 6.190945625305176
Epoch 250, val loss: 1.2725526094436646
Epoch 260, training loss: 7.254491806030273 = 1.069896936416626 + 1.0 * 6.184594631195068
Epoch 260, val loss: 1.227182388305664
Epoch 270, training loss: 7.189371585845947 = 1.0107522010803223 + 1.0 * 6.178619384765625
Epoch 270, val loss: 1.1826890707015991
Epoch 280, training loss: 7.130270957946777 = 0.9530364871025085 + 1.0 * 6.177234649658203
Epoch 280, val loss: 1.1391758918762207
Epoch 290, training loss: 7.068662643432617 = 0.8979489207267761 + 1.0 * 6.170713901519775
Epoch 290, val loss: 1.0976903438568115
Epoch 300, training loss: 7.009089946746826 = 0.8454858064651489 + 1.0 * 6.163604259490967
Epoch 300, val loss: 1.0583021640777588
Epoch 310, training loss: 6.967844009399414 = 0.7956096529960632 + 1.0 * 6.172234535217285
Epoch 310, val loss: 1.0212281942367554
Epoch 320, training loss: 6.9061689376831055 = 0.7499023079872131 + 1.0 * 6.156266689300537
Epoch 320, val loss: 0.9877130389213562
Epoch 330, training loss: 6.858373165130615 = 0.7074481248855591 + 1.0 * 6.150925159454346
Epoch 330, val loss: 0.9576038718223572
Epoch 340, training loss: 6.8133015632629395 = 0.6677053570747375 + 1.0 * 6.145596027374268
Epoch 340, val loss: 0.9302507042884827
Epoch 350, training loss: 6.774418830871582 = 0.6306107044219971 + 1.0 * 6.143808364868164
Epoch 350, val loss: 0.905738890171051
Epoch 360, training loss: 6.736819267272949 = 0.5963855981826782 + 1.0 * 6.1404337882995605
Epoch 360, val loss: 0.8841594457626343
Epoch 370, training loss: 6.69973611831665 = 0.5644190907478333 + 1.0 * 6.135316848754883
Epoch 370, val loss: 0.8651542067527771
Epoch 380, training loss: 6.667263031005859 = 0.534572422504425 + 1.0 * 6.1326904296875
Epoch 380, val loss: 0.8483415842056274
Epoch 390, training loss: 6.6366190910339355 = 0.5068341493606567 + 1.0 * 6.129785060882568
Epoch 390, val loss: 0.8337188363075256
Epoch 400, training loss: 6.605864524841309 = 0.48075157403945923 + 1.0 * 6.125113010406494
Epoch 400, val loss: 0.8208892941474915
Epoch 410, training loss: 6.577950954437256 = 0.45606958866119385 + 1.0 * 6.121881484985352
Epoch 410, val loss: 0.8095922470092773
Epoch 420, training loss: 6.5580291748046875 = 0.4326702654361725 + 1.0 * 6.125359058380127
Epoch 420, val loss: 0.7996699213981628
Epoch 430, training loss: 6.52781343460083 = 0.4106661081314087 + 1.0 * 6.117147445678711
Epoch 430, val loss: 0.7910312414169312
Epoch 440, training loss: 6.503829002380371 = 0.38965821266174316 + 1.0 * 6.114171028137207
Epoch 440, val loss: 0.7835196852684021
Epoch 450, training loss: 6.482763290405273 = 0.3694118559360504 + 1.0 * 6.113351345062256
Epoch 450, val loss: 0.7768434286117554
Epoch 460, training loss: 6.462364673614502 = 0.35002806782722473 + 1.0 * 6.1123366355896
Epoch 460, val loss: 0.7708535194396973
Epoch 470, training loss: 6.4390082359313965 = 0.33148670196533203 + 1.0 * 6.1075215339660645
Epoch 470, val loss: 0.7656135559082031
Epoch 480, training loss: 6.416519641876221 = 0.3135204613208771 + 1.0 * 6.102999210357666
Epoch 480, val loss: 0.7609823942184448
Epoch 490, training loss: 6.396340847015381 = 0.296078622341156 + 1.0 * 6.10026216506958
Epoch 490, val loss: 0.7568746209144592
Epoch 500, training loss: 6.380244255065918 = 0.2792757749557495 + 1.0 * 6.100968360900879
Epoch 500, val loss: 0.7532512545585632
Epoch 510, training loss: 6.359655380249023 = 0.2632698118686676 + 1.0 * 6.096385478973389
Epoch 510, val loss: 0.7501209378242493
Epoch 520, training loss: 6.341345310211182 = 0.24790792167186737 + 1.0 * 6.093437194824219
Epoch 520, val loss: 0.7475327253341675
Epoch 530, training loss: 6.347465515136719 = 0.2332165241241455 + 1.0 * 6.114249229431152
Epoch 530, val loss: 0.745410680770874
Epoch 540, training loss: 6.314899921417236 = 0.21941211819648743 + 1.0 * 6.095487594604492
Epoch 540, val loss: 0.7438220381736755
Epoch 550, training loss: 6.295271873474121 = 0.20636224746704102 + 1.0 * 6.08890962600708
Epoch 550, val loss: 0.7427522540092468
Epoch 560, training loss: 6.279519557952881 = 0.19400204718112946 + 1.0 * 6.085517406463623
Epoch 560, val loss: 0.7422342300415039
Epoch 570, training loss: 6.285282135009766 = 0.1823553889989853 + 1.0 * 6.102926731109619
Epoch 570, val loss: 0.7421668767929077
Epoch 580, training loss: 6.256585597991943 = 0.17151384055614471 + 1.0 * 6.085071563720703
Epoch 580, val loss: 0.7425142526626587
Epoch 590, training loss: 6.241314888000488 = 0.16135871410369873 + 1.0 * 6.0799560546875
Epoch 590, val loss: 0.7433298826217651
Epoch 600, training loss: 6.23196268081665 = 0.15182985365390778 + 1.0 * 6.080132961273193
Epoch 600, val loss: 0.7446659207344055
Epoch 610, training loss: 6.2274489402771 = 0.14296290278434753 + 1.0 * 6.08448600769043
Epoch 610, val loss: 0.7463367581367493
Epoch 620, training loss: 6.212176322937012 = 0.134732186794281 + 1.0 * 6.077444076538086
Epoch 620, val loss: 0.7484578490257263
Epoch 630, training loss: 6.201272964477539 = 0.1270304024219513 + 1.0 * 6.07424259185791
Epoch 630, val loss: 0.7510021328926086
Epoch 640, training loss: 6.206893444061279 = 0.11982811242341995 + 1.0 * 6.08706521987915
Epoch 640, val loss: 0.7539827227592468
Epoch 650, training loss: 6.184469699859619 = 0.11315492540597916 + 1.0 * 6.071314811706543
Epoch 650, val loss: 0.7571806311607361
Epoch 660, training loss: 6.1793975830078125 = 0.10692456364631653 + 1.0 * 6.072473049163818
Epoch 660, val loss: 0.7606728672981262
Epoch 670, training loss: 6.171448230743408 = 0.10112375020980835 + 1.0 * 6.070324420928955
Epoch 670, val loss: 0.7644915580749512
Epoch 680, training loss: 6.16485071182251 = 0.09573151171207428 + 1.0 * 6.069118976593018
Epoch 680, val loss: 0.7685155272483826
Epoch 690, training loss: 6.156832218170166 = 0.09068217873573303 + 1.0 * 6.066150188446045
Epoch 690, val loss: 0.7728080749511719
Epoch 700, training loss: 6.150673866271973 = 0.0859403908252716 + 1.0 * 6.064733505249023
Epoch 700, val loss: 0.7773472666740417
Epoch 710, training loss: 6.15416955947876 = 0.08150430768728256 + 1.0 * 6.072665214538574
Epoch 710, val loss: 0.7820730805397034
Epoch 720, training loss: 6.143162250518799 = 0.07739493250846863 + 1.0 * 6.065767288208008
Epoch 720, val loss: 0.78682941198349
Epoch 730, training loss: 6.134648323059082 = 0.0735483169555664 + 1.0 * 6.061100006103516
Epoch 730, val loss: 0.7917446494102478
Epoch 740, training loss: 6.130599021911621 = 0.06993146985769272 + 1.0 * 6.060667514801025
Epoch 740, val loss: 0.7968589663505554
Epoch 750, training loss: 6.133105754852295 = 0.06653933972120285 + 1.0 * 6.066566467285156
Epoch 750, val loss: 0.8020543456077576
Epoch 760, training loss: 6.12189245223999 = 0.06337187439203262 + 1.0 * 6.058520793914795
Epoch 760, val loss: 0.8072884678840637
Epoch 770, training loss: 6.122635841369629 = 0.06039605289697647 + 1.0 * 6.062239646911621
Epoch 770, val loss: 0.8126024007797241
Epoch 780, training loss: 6.115542411804199 = 0.057611849159002304 + 1.0 * 6.0579304695129395
Epoch 780, val loss: 0.8179802894592285
Epoch 790, training loss: 6.110837936401367 = 0.054992757737636566 + 1.0 * 6.055845260620117
Epoch 790, val loss: 0.8234199285507202
Epoch 800, training loss: 6.109731197357178 = 0.05252758786082268 + 1.0 * 6.057203769683838
Epoch 800, val loss: 0.8288885951042175
Epoch 810, training loss: 6.105411529541016 = 0.05021362006664276 + 1.0 * 6.055197715759277
Epoch 810, val loss: 0.8343151807785034
Epoch 820, training loss: 6.104776382446289 = 0.04804758355021477 + 1.0 * 6.056728839874268
Epoch 820, val loss: 0.8397527933120728
Epoch 830, training loss: 6.100131034851074 = 0.04601514711976051 + 1.0 * 6.0541157722473145
Epoch 830, val loss: 0.8451216220855713
Epoch 840, training loss: 6.095107078552246 = 0.0440916083753109 + 1.0 * 6.051015377044678
Epoch 840, val loss: 0.8505980372428894
Epoch 850, training loss: 6.094526290893555 = 0.0422729030251503 + 1.0 * 6.052253246307373
Epoch 850, val loss: 0.856035053730011
Epoch 860, training loss: 6.091312408447266 = 0.04055940359830856 + 1.0 * 6.050753116607666
Epoch 860, val loss: 0.8614867925643921
Epoch 870, training loss: 6.08937406539917 = 0.03894885256886482 + 1.0 * 6.050425052642822
Epoch 870, val loss: 0.8667475581169128
Epoch 880, training loss: 6.085901737213135 = 0.03742443025112152 + 1.0 * 6.0484771728515625
Epoch 880, val loss: 0.8721535205841064
Epoch 890, training loss: 6.091273784637451 = 0.03598148375749588 + 1.0 * 6.055292129516602
Epoch 890, val loss: 0.8774674534797668
Epoch 900, training loss: 6.084009647369385 = 0.03461949899792671 + 1.0 * 6.0493903160095215
Epoch 900, val loss: 0.8827137351036072
Epoch 910, training loss: 6.079081058502197 = 0.03332952782511711 + 1.0 * 6.045751571655273
Epoch 910, val loss: 0.8879474401473999
Epoch 920, training loss: 6.082224369049072 = 0.03210560604929924 + 1.0 * 6.050118923187256
Epoch 920, val loss: 0.8931648135185242
Epoch 930, training loss: 6.0777812004089355 = 0.03094450943171978 + 1.0 * 6.046836853027344
Epoch 930, val loss: 0.8983561992645264
Epoch 940, training loss: 6.073770523071289 = 0.029849112033843994 + 1.0 * 6.04392147064209
Epoch 940, val loss: 0.9033911824226379
Epoch 950, training loss: 6.072526454925537 = 0.02880953997373581 + 1.0 * 6.043716907501221
Epoch 950, val loss: 0.9084051251411438
Epoch 960, training loss: 6.071491241455078 = 0.02782035991549492 + 1.0 * 6.043670654296875
Epoch 960, val loss: 0.9134089946746826
Epoch 970, training loss: 6.071491241455078 = 0.026880398392677307 + 1.0 * 6.044610977172852
Epoch 970, val loss: 0.9183053374290466
Epoch 980, training loss: 6.068343639373779 = 0.02598509192466736 + 1.0 * 6.0423583984375
Epoch 980, val loss: 0.9232549667358398
Epoch 990, training loss: 6.064741611480713 = 0.0251363143324852 + 1.0 * 6.039605140686035
Epoch 990, val loss: 0.928034782409668
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8192
Flip ASR: 0.7822/225 nodes
The final ASR:0.68020, 0.09829, Accuracy:0.81235, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10528])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.344003677368164 = 1.9700756072998047 + 1.0 * 8.37392807006836
Epoch 0, val loss: 1.974971055984497
Epoch 10, training loss: 10.332598686218262 = 1.9589506387710571 + 1.0 * 8.373647689819336
Epoch 10, val loss: 1.9636636972427368
Epoch 20, training loss: 10.316497802734375 = 1.9452617168426514 + 1.0 * 8.371235847473145
Epoch 20, val loss: 1.949615716934204
Epoch 30, training loss: 10.278059005737305 = 1.926221251487732 + 1.0 * 8.351838111877441
Epoch 30, val loss: 1.9300007820129395
Epoch 40, training loss: 10.124805450439453 = 1.9008753299713135 + 1.0 * 8.223930358886719
Epoch 40, val loss: 1.9047493934631348
Epoch 50, training loss: 9.683343887329102 = 1.871798038482666 + 1.0 * 7.811546325683594
Epoch 50, val loss: 1.8767298460006714
Epoch 60, training loss: 9.464756965637207 = 1.8465298414230347 + 1.0 * 7.618227481842041
Epoch 60, val loss: 1.8538146018981934
Epoch 70, training loss: 9.025585174560547 = 1.8269295692443848 + 1.0 * 7.19865608215332
Epoch 70, val loss: 1.8361717462539673
Epoch 80, training loss: 8.637632369995117 = 1.809985637664795 + 1.0 * 6.827646255493164
Epoch 80, val loss: 1.8208763599395752
Epoch 90, training loss: 8.450109481811523 = 1.7924829721450806 + 1.0 * 6.657626152038574
Epoch 90, val loss: 1.8050305843353271
Epoch 100, training loss: 8.338035583496094 = 1.7727497816085815 + 1.0 * 6.565285682678223
Epoch 100, val loss: 1.78813636302948
Epoch 110, training loss: 8.252283096313477 = 1.7530874013900757 + 1.0 * 6.4991960525512695
Epoch 110, val loss: 1.7718275785446167
Epoch 120, training loss: 8.17890739440918 = 1.7333368062973022 + 1.0 * 6.445570945739746
Epoch 120, val loss: 1.7555761337280273
Epoch 130, training loss: 8.112425804138184 = 1.7118737697601318 + 1.0 * 6.400551795959473
Epoch 130, val loss: 1.7377774715423584
Epoch 140, training loss: 8.050361633300781 = 1.687300205230713 + 1.0 * 6.363061428070068
Epoch 140, val loss: 1.717644453048706
Epoch 150, training loss: 7.991432189941406 = 1.6589936017990112 + 1.0 * 6.3324384689331055
Epoch 150, val loss: 1.6949042081832886
Epoch 160, training loss: 7.928455352783203 = 1.626971960067749 + 1.0 * 6.301483631134033
Epoch 160, val loss: 1.6695263385772705
Epoch 170, training loss: 7.868440628051758 = 1.590429663658142 + 1.0 * 6.278010845184326
Epoch 170, val loss: 1.640197992324829
Epoch 180, training loss: 7.808054447174072 = 1.5487486124038696 + 1.0 * 6.259305953979492
Epoch 180, val loss: 1.6065653562545776
Epoch 190, training loss: 7.7471513748168945 = 1.5018587112426758 + 1.0 * 6.245292663574219
Epoch 190, val loss: 1.56858229637146
Epoch 200, training loss: 7.683354377746582 = 1.4511924982070923 + 1.0 * 6.232161998748779
Epoch 200, val loss: 1.5277414321899414
Epoch 210, training loss: 7.619449615478516 = 1.39817214012146 + 1.0 * 6.221277236938477
Epoch 210, val loss: 1.4852583408355713
Epoch 220, training loss: 7.553647041320801 = 1.3428764343261719 + 1.0 * 6.210770606994629
Epoch 220, val loss: 1.4410978555679321
Epoch 230, training loss: 7.4876203536987305 = 1.2856765985488892 + 1.0 * 6.201943874359131
Epoch 230, val loss: 1.3954726457595825
Epoch 240, training loss: 7.424888610839844 = 1.2277252674102783 + 1.0 * 6.197163105010986
Epoch 240, val loss: 1.3497706651687622
Epoch 250, training loss: 7.361482620239258 = 1.1709208488464355 + 1.0 * 6.190561771392822
Epoch 250, val loss: 1.3057109117507935
Epoch 260, training loss: 7.297844886779785 = 1.1150259971618652 + 1.0 * 6.18281888961792
Epoch 260, val loss: 1.2628991603851318
Epoch 270, training loss: 7.237067699432373 = 1.0600138902664185 + 1.0 * 6.177053928375244
Epoch 270, val loss: 1.2214525938034058
Epoch 280, training loss: 7.1830244064331055 = 1.0060508251190186 + 1.0 * 6.176973819732666
Epoch 280, val loss: 1.1815545558929443
Epoch 290, training loss: 7.124270915985107 = 0.9543671607971191 + 1.0 * 6.169903755187988
Epoch 290, val loss: 1.1438188552856445
Epoch 300, training loss: 7.068659782409668 = 0.9045730829238892 + 1.0 * 6.164086818695068
Epoch 300, val loss: 1.108005166053772
Epoch 310, training loss: 7.019922733306885 = 0.8566535115242004 + 1.0 * 6.16326904296875
Epoch 310, val loss: 1.0739177465438843
Epoch 320, training loss: 6.968979835510254 = 0.811144232749939 + 1.0 * 6.157835483551025
Epoch 320, val loss: 1.0420174598693848
Epoch 330, training loss: 6.920517444610596 = 0.7677444815635681 + 1.0 * 6.152772903442383
Epoch 330, val loss: 1.0120689868927002
Epoch 340, training loss: 6.875666618347168 = 0.7264330387115479 + 1.0 * 6.149233341217041
Epoch 340, val loss: 0.9839901328086853
Epoch 350, training loss: 6.846495628356934 = 0.6872665882110596 + 1.0 * 6.159229278564453
Epoch 350, val loss: 0.9579014778137207
Epoch 360, training loss: 6.799653053283691 = 0.6510833501815796 + 1.0 * 6.148569583892822
Epoch 360, val loss: 0.9344069361686707
Epoch 370, training loss: 6.758580207824707 = 0.6167363524436951 + 1.0 * 6.141843795776367
Epoch 370, val loss: 0.9133909940719604
Epoch 380, training loss: 6.72128963470459 = 0.5838178992271423 + 1.0 * 6.137471675872803
Epoch 380, val loss: 0.8939763307571411
Epoch 390, training loss: 6.698757171630859 = 0.552115797996521 + 1.0 * 6.146641254425049
Epoch 390, val loss: 0.8761798143386841
Epoch 400, training loss: 6.655492305755615 = 0.5218040347099304 + 1.0 * 6.133688449859619
Epoch 400, val loss: 0.8602783679962158
Epoch 410, training loss: 6.622500419616699 = 0.4926910400390625 + 1.0 * 6.129809379577637
Epoch 410, val loss: 0.8462343811988831
Epoch 420, training loss: 6.591236591339111 = 0.4645514190196991 + 1.0 * 6.12668514251709
Epoch 420, val loss: 0.8336247205734253
Epoch 430, training loss: 6.571502208709717 = 0.43737465143203735 + 1.0 * 6.134127616882324
Epoch 430, val loss: 0.8225576877593994
Epoch 440, training loss: 6.536136627197266 = 0.41133400797843933 + 1.0 * 6.124802589416504
Epoch 440, val loss: 0.8130612373352051
Epoch 450, training loss: 6.507801055908203 = 0.38647210597991943 + 1.0 * 6.121328830718994
Epoch 450, val loss: 0.8051618933677673
Epoch 460, training loss: 6.48002815246582 = 0.3626178503036499 + 1.0 * 6.117410182952881
Epoch 460, val loss: 0.7983343005180359
Epoch 470, training loss: 6.463198661804199 = 0.33986082673072815 + 1.0 * 6.123337745666504
Epoch 470, val loss: 0.7925797700881958
Epoch 480, training loss: 6.431525707244873 = 0.31836557388305664 + 1.0 * 6.113160133361816
Epoch 480, val loss: 0.7880851626396179
Epoch 490, training loss: 6.407273769378662 = 0.29791420698165894 + 1.0 * 6.1093597412109375
Epoch 490, val loss: 0.7845301628112793
Epoch 500, training loss: 6.4056925773620605 = 0.2785458564758301 + 1.0 * 6.1271467208862305
Epoch 500, val loss: 0.7816734313964844
Epoch 510, training loss: 6.3672966957092285 = 0.2604108452796936 + 1.0 * 6.10688591003418
Epoch 510, val loss: 0.7794914841651917
Epoch 520, training loss: 6.347439289093018 = 0.24351680278778076 + 1.0 * 6.103922367095947
Epoch 520, val loss: 0.7783300876617432
Epoch 530, training loss: 6.329151153564453 = 0.22768539190292358 + 1.0 * 6.101465702056885
Epoch 530, val loss: 0.7777570486068726
Epoch 540, training loss: 6.314964294433594 = 0.21287301182746887 + 1.0 * 6.102091312408447
Epoch 540, val loss: 0.7778627872467041
Epoch 550, training loss: 6.299543857574463 = 0.19917844235897064 + 1.0 * 6.10036563873291
Epoch 550, val loss: 0.7785451412200928
Epoch 560, training loss: 6.28340482711792 = 0.18650579452514648 + 1.0 * 6.096899032592773
Epoch 560, val loss: 0.780037522315979
Epoch 570, training loss: 6.27046537399292 = 0.17478658258914948 + 1.0 * 6.095678806304932
Epoch 570, val loss: 0.7820444703102112
Epoch 580, training loss: 6.259955406188965 = 0.16395387053489685 + 1.0 * 6.096001625061035
Epoch 580, val loss: 0.7844400405883789
Epoch 590, training loss: 6.246582508087158 = 0.1540171056985855 + 1.0 * 6.092565536499023
Epoch 590, val loss: 0.7874422669410706
Epoch 600, training loss: 6.234706878662109 = 0.14482612907886505 + 1.0 * 6.08988094329834
Epoch 600, val loss: 0.7909280061721802
Epoch 610, training loss: 6.2371931076049805 = 0.13630929589271545 + 1.0 * 6.100883960723877
Epoch 610, val loss: 0.7946498394012451
Epoch 620, training loss: 6.215022087097168 = 0.12850233912467957 + 1.0 * 6.086519718170166
Epoch 620, val loss: 0.798774003982544
Epoch 630, training loss: 6.205039978027344 = 0.12123147398233414 + 1.0 * 6.083808422088623
Epoch 630, val loss: 0.8032902479171753
Epoch 640, training loss: 6.198094367980957 = 0.1144685298204422 + 1.0 * 6.083625793457031
Epoch 640, val loss: 0.8080140948295593
Epoch 650, training loss: 6.194789886474609 = 0.10820330679416656 + 1.0 * 6.0865864753723145
Epoch 650, val loss: 0.8128318786621094
Epoch 660, training loss: 6.183650970458984 = 0.10239194333553314 + 1.0 * 6.081259250640869
Epoch 660, val loss: 0.8179689049720764
Epoch 670, training loss: 6.1768083572387695 = 0.09698480367660522 + 1.0 * 6.0798234939575195
Epoch 670, val loss: 0.8232974410057068
Epoch 680, training loss: 6.175676345825195 = 0.09194319695234299 + 1.0 * 6.083733081817627
Epoch 680, val loss: 0.828601062297821
Epoch 690, training loss: 6.164219856262207 = 0.08727072924375534 + 1.0 * 6.076949119567871
Epoch 690, val loss: 0.8340979218482971
Epoch 700, training loss: 6.1676225662231445 = 0.0828930214047432 + 1.0 * 6.0847296714782715
Epoch 700, val loss: 0.8396750092506409
Epoch 710, training loss: 6.1548309326171875 = 0.07883219420909882 + 1.0 * 6.075998783111572
Epoch 710, val loss: 0.8453127145767212
Epoch 720, training loss: 6.146974563598633 = 0.07501641660928726 + 1.0 * 6.071958065032959
Epoch 720, val loss: 0.8510307669639587
Epoch 730, training loss: 6.142981052398682 = 0.07144037634134293 + 1.0 * 6.071540832519531
Epoch 730, val loss: 0.856784999370575
Epoch 740, training loss: 6.144782066345215 = 0.06809934973716736 + 1.0 * 6.0766825675964355
Epoch 740, val loss: 0.8624711036682129
Epoch 750, training loss: 6.137188911437988 = 0.06497620791196823 + 1.0 * 6.0722126960754395
Epoch 750, val loss: 0.8681250214576721
Epoch 760, training loss: 6.131688594818115 = 0.062057629227638245 + 1.0 * 6.069631099700928
Epoch 760, val loss: 0.8739255666732788
Epoch 770, training loss: 6.125521183013916 = 0.05930892750620842 + 1.0 * 6.066212177276611
Epoch 770, val loss: 0.8797082901000977
Epoch 780, training loss: 6.124808311462402 = 0.05671321973204613 + 1.0 * 6.0680952072143555
Epoch 780, val loss: 0.8854678273200989
Epoch 790, training loss: 6.1282148361206055 = 0.054276999086141586 + 1.0 * 6.073937892913818
Epoch 790, val loss: 0.8910543918609619
Epoch 800, training loss: 6.116121292114258 = 0.05200888216495514 + 1.0 * 6.064112186431885
Epoch 800, val loss: 0.8966904878616333
Epoch 810, training loss: 6.112391948699951 = 0.049866415560245514 + 1.0 * 6.062525749206543
Epoch 810, val loss: 0.9023944139480591
Epoch 820, training loss: 6.110235691070557 = 0.04783467948436737 + 1.0 * 6.062400817871094
Epoch 820, val loss: 0.9080156683921814
Epoch 830, training loss: 6.114420413970947 = 0.04591105505824089 + 1.0 * 6.068509578704834
Epoch 830, val loss: 0.9135216474533081
Epoch 840, training loss: 6.104513645172119 = 0.04410839453339577 + 1.0 * 6.060405254364014
Epoch 840, val loss: 0.9190545082092285
Epoch 850, training loss: 6.103238105773926 = 0.0423942469060421 + 1.0 * 6.0608439445495605
Epoch 850, val loss: 0.9245622158050537
Epoch 860, training loss: 6.101995944976807 = 0.04077065363526344 + 1.0 * 6.061225414276123
Epoch 860, val loss: 0.9299253821372986
Epoch 870, training loss: 6.096428871154785 = 0.03924328088760376 + 1.0 * 6.057185649871826
Epoch 870, val loss: 0.9353488087654114
Epoch 880, training loss: 6.096501350402832 = 0.03779234364628792 + 1.0 * 6.058709144592285
Epoch 880, val loss: 0.9407320022583008
Epoch 890, training loss: 6.097558498382568 = 0.036415476351976395 + 1.0 * 6.061142921447754
Epoch 890, val loss: 0.9459473490715027
Epoch 900, training loss: 6.092247486114502 = 0.03510346636176109 + 1.0 * 6.0571441650390625
Epoch 900, val loss: 0.9511679410934448
Epoch 910, training loss: 6.0959906578063965 = 0.03386607766151428 + 1.0 * 6.062124729156494
Epoch 910, val loss: 0.9563127160072327
Epoch 920, training loss: 6.0889105796813965 = 0.03269173577427864 + 1.0 * 6.05621862411499
Epoch 920, val loss: 0.9614246487617493
Epoch 930, training loss: 6.08319616317749 = 0.031575243920087814 + 1.0 * 6.051620960235596
Epoch 930, val loss: 0.9665309190750122
Epoch 940, training loss: 6.081677436828613 = 0.03051064722239971 + 1.0 * 6.051167011260986
Epoch 940, val loss: 0.9715963006019592
Epoch 950, training loss: 6.099846363067627 = 0.029497943818569183 + 1.0 * 6.070348262786865
Epoch 950, val loss: 0.9765467047691345
Epoch 960, training loss: 6.085085868835449 = 0.02852874994277954 + 1.0 * 6.0565571784973145
Epoch 960, val loss: 0.9811937212944031
Epoch 970, training loss: 6.0766282081604 = 0.027616633102297783 + 1.0 * 6.049011707305908
Epoch 970, val loss: 0.9861335158348083
Epoch 980, training loss: 6.074471950531006 = 0.026740094646811485 + 1.0 * 6.047731876373291
Epoch 980, val loss: 0.9909758567810059
Epoch 990, training loss: 6.078258991241455 = 0.02589908242225647 + 1.0 * 6.0523600578308105
Epoch 990, val loss: 0.9956841468811035
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.4465
Flip ASR: 0.3511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.330268859863281 = 1.9563641548156738 + 1.0 * 8.37390422821045
Epoch 0, val loss: 1.9598106145858765
Epoch 10, training loss: 10.318388938903809 = 1.9449342489242554 + 1.0 * 8.373455047607422
Epoch 10, val loss: 1.9471254348754883
Epoch 20, training loss: 10.301560401916504 = 1.9311058521270752 + 1.0 * 8.370454788208008
Epoch 20, val loss: 1.9312242269515991
Epoch 30, training loss: 10.264166831970215 = 1.9125734567642212 + 1.0 * 8.351593017578125
Epoch 30, val loss: 1.9095042943954468
Epoch 40, training loss: 10.123390197753906 = 1.8887711763381958 + 1.0 * 8.234619140625
Epoch 40, val loss: 1.882185935974121
Epoch 50, training loss: 9.582535743713379 = 1.8640369176864624 + 1.0 * 7.718499183654785
Epoch 50, val loss: 1.854488492012024
Epoch 60, training loss: 9.089805603027344 = 1.8390218019485474 + 1.0 * 7.250783920288086
Epoch 60, val loss: 1.8277980089187622
Epoch 70, training loss: 8.764354705810547 = 1.8173272609710693 + 1.0 * 6.947027683258057
Epoch 70, val loss: 1.806535243988037
Epoch 80, training loss: 8.555840492248535 = 1.7983925342559814 + 1.0 * 6.757448196411133
Epoch 80, val loss: 1.7880996465682983
Epoch 90, training loss: 8.450494766235352 = 1.7813891172409058 + 1.0 * 6.6691060066223145
Epoch 90, val loss: 1.7711952924728394
Epoch 100, training loss: 8.368507385253906 = 1.7624789476394653 + 1.0 * 6.606028079986572
Epoch 100, val loss: 1.752431869506836
Epoch 110, training loss: 8.278190612792969 = 1.7427548170089722 + 1.0 * 6.535435676574707
Epoch 110, val loss: 1.7342309951782227
Epoch 120, training loss: 8.198614120483398 = 1.7226039171218872 + 1.0 * 6.476010322570801
Epoch 120, val loss: 1.7165278196334839
Epoch 130, training loss: 8.128750801086426 = 1.7003750801086426 + 1.0 * 6.428375720977783
Epoch 130, val loss: 1.6973586082458496
Epoch 140, training loss: 8.063239097595215 = 1.6736795902252197 + 1.0 * 6.389559745788574
Epoch 140, val loss: 1.6749569177627563
Epoch 150, training loss: 7.99662446975708 = 1.6414390802383423 + 1.0 * 6.355185508728027
Epoch 150, val loss: 1.6486190557479858
Epoch 160, training loss: 7.928967475891113 = 1.6042284965515137 + 1.0 * 6.3247389793396
Epoch 160, val loss: 1.6187782287597656
Epoch 170, training loss: 7.862468242645264 = 1.5619269609451294 + 1.0 * 6.300541400909424
Epoch 170, val loss: 1.5852115154266357
Epoch 180, training loss: 7.794936656951904 = 1.5142477750778198 + 1.0 * 6.280688762664795
Epoch 180, val loss: 1.5475330352783203
Epoch 190, training loss: 7.727067947387695 = 1.4625866413116455 + 1.0 * 6.264481067657471
Epoch 190, val loss: 1.5071923732757568
Epoch 200, training loss: 7.65977668762207 = 1.4082691669464111 + 1.0 * 6.25150728225708
Epoch 200, val loss: 1.46565580368042
Epoch 210, training loss: 7.598031997680664 = 1.353940725326538 + 1.0 * 6.244091033935547
Epoch 210, val loss: 1.424748182296753
Epoch 220, training loss: 7.53300142288208 = 1.3016366958618164 + 1.0 * 6.231364727020264
Epoch 220, val loss: 1.3867191076278687
Epoch 230, training loss: 7.4739532470703125 = 1.2518101930618286 + 1.0 * 6.222143173217773
Epoch 230, val loss: 1.351139783859253
Epoch 240, training loss: 7.420427322387695 = 1.2044390439987183 + 1.0 * 6.2159881591796875
Epoch 240, val loss: 1.317829966545105
Epoch 250, training loss: 7.366342544555664 = 1.1595072746276855 + 1.0 * 6.2068352699279785
Epoch 250, val loss: 1.2868021726608276
Epoch 260, training loss: 7.315854549407959 = 1.1156949996948242 + 1.0 * 6.200159549713135
Epoch 260, val loss: 1.2568811178207397
Epoch 270, training loss: 7.265966415405273 = 1.07285737991333 + 1.0 * 6.193109035491943
Epoch 270, val loss: 1.2277122735977173
Epoch 280, training loss: 7.217625617980957 = 1.030132532119751 + 1.0 * 6.187492847442627
Epoch 280, val loss: 1.1985619068145752
Epoch 290, training loss: 7.175877094268799 = 0.9871753454208374 + 1.0 * 6.188701629638672
Epoch 290, val loss: 1.1689233779907227
Epoch 300, training loss: 7.122021198272705 = 0.9442638158798218 + 1.0 * 6.177757263183594
Epoch 300, val loss: 1.139036774635315
Epoch 310, training loss: 7.072470188140869 = 0.9015172123908997 + 1.0 * 6.170952796936035
Epoch 310, val loss: 1.1088597774505615
Epoch 320, training loss: 7.025466442108154 = 0.8592248558998108 + 1.0 * 6.166241645812988
Epoch 320, val loss: 1.0786361694335938
Epoch 330, training loss: 6.980891227722168 = 0.8177690505981445 + 1.0 * 6.163122177124023
Epoch 330, val loss: 1.0487189292907715
Epoch 340, training loss: 6.94186544418335 = 0.7780550718307495 + 1.0 * 6.1638102531433105
Epoch 340, val loss: 1.0196644067764282
Epoch 350, training loss: 6.895914554595947 = 0.7407768368721008 + 1.0 * 6.155137538909912
Epoch 350, val loss: 0.9925524592399597
Epoch 360, training loss: 6.855967998504639 = 0.7057482600212097 + 1.0 * 6.150219917297363
Epoch 360, val loss: 0.9673669934272766
Epoch 370, training loss: 6.819071292877197 = 0.6726222038269043 + 1.0 * 6.146449089050293
Epoch 370, val loss: 0.9439533352851868
Epoch 380, training loss: 6.785113334655762 = 0.6412842869758606 + 1.0 * 6.143828868865967
Epoch 380, val loss: 0.9221501350402832
Epoch 390, training loss: 6.753046035766602 = 0.6117749810218811 + 1.0 * 6.141271114349365
Epoch 390, val loss: 0.902016282081604
Epoch 400, training loss: 6.720536231994629 = 0.583381175994873 + 1.0 * 6.137155055999756
Epoch 400, val loss: 0.8832198977470398
Epoch 410, training loss: 6.7124505043029785 = 0.555669903755188 + 1.0 * 6.15678071975708
Epoch 410, val loss: 0.8653293251991272
Epoch 420, training loss: 6.664017200469971 = 0.5291247963905334 + 1.0 * 6.134892463684082
Epoch 420, val loss: 0.8482795357704163
Epoch 430, training loss: 6.633225917816162 = 0.5030907392501831 + 1.0 * 6.1301350593566895
Epoch 430, val loss: 0.8319551944732666
Epoch 440, training loss: 6.603603839874268 = 0.47732147574424744 + 1.0 * 6.126282215118408
Epoch 440, val loss: 0.8163111209869385
Epoch 450, training loss: 6.5755462646484375 = 0.4518527090549469 + 1.0 * 6.123693466186523
Epoch 450, val loss: 0.801250159740448
Epoch 460, training loss: 6.549346446990967 = 0.42691507935523987 + 1.0 * 6.12243127822876
Epoch 460, val loss: 0.7869288921356201
Epoch 470, training loss: 6.525581359863281 = 0.40287455916404724 + 1.0 * 6.122706890106201
Epoch 470, val loss: 0.7736071944236755
Epoch 480, training loss: 6.4967193603515625 = 0.3795851171016693 + 1.0 * 6.117134094238281
Epoch 480, val loss: 0.7612077593803406
Epoch 490, training loss: 6.471590995788574 = 0.3570461869239807 + 1.0 * 6.114544868469238
Epoch 490, val loss: 0.7498229146003723
Epoch 500, training loss: 6.448096752166748 = 0.3352759778499603 + 1.0 * 6.112820625305176
Epoch 500, val loss: 0.7393861413002014
Epoch 510, training loss: 6.443483829498291 = 0.31451207399368286 + 1.0 * 6.128971576690674
Epoch 510, val loss: 0.7299684286117554
Epoch 520, training loss: 6.404168128967285 = 0.2950078845024109 + 1.0 * 6.109160423278809
Epoch 520, val loss: 0.721611499786377
Epoch 530, training loss: 6.3848876953125 = 0.27649855613708496 + 1.0 * 6.108388900756836
Epoch 530, val loss: 0.7143402099609375
Epoch 540, training loss: 6.363243579864502 = 0.25888535380363464 + 1.0 * 6.104358196258545
Epoch 540, val loss: 0.7081748843193054
Epoch 550, training loss: 6.344637393951416 = 0.24219085276126862 + 1.0 * 6.102446556091309
Epoch 550, val loss: 0.7029527425765991
Epoch 560, training loss: 6.351171016693115 = 0.2265027016401291 + 1.0 * 6.124668121337891
Epoch 560, val loss: 0.6987183094024658
Epoch 570, training loss: 6.315061569213867 = 0.21192917227745056 + 1.0 * 6.103132247924805
Epoch 570, val loss: 0.695463240146637
Epoch 580, training loss: 6.296395301818848 = 0.1984238177537918 + 1.0 * 6.097971439361572
Epoch 580, val loss: 0.6930915117263794
Epoch 590, training loss: 6.280457496643066 = 0.18585990369319916 + 1.0 * 6.094597816467285
Epoch 590, val loss: 0.6917094588279724
Epoch 600, training loss: 6.283481597900391 = 0.17422214150428772 + 1.0 * 6.109259605407715
Epoch 600, val loss: 0.6912060976028442
Epoch 610, training loss: 6.259397983551025 = 0.1635560393333435 + 1.0 * 6.095841884613037
Epoch 610, val loss: 0.691352128982544
Epoch 620, training loss: 6.244856357574463 = 0.15376506745815277 + 1.0 * 6.091091156005859
Epoch 620, val loss: 0.6922494173049927
Epoch 630, training loss: 6.234294891357422 = 0.1447543203830719 + 1.0 * 6.089540481567383
Epoch 630, val loss: 0.6939138770103455
Epoch 640, training loss: 6.226318359375 = 0.136519193649292 + 1.0 * 6.089799404144287
Epoch 640, val loss: 0.6961966156959534
Epoch 650, training loss: 6.216756343841553 = 0.1289786696434021 + 1.0 * 6.087777614593506
Epoch 650, val loss: 0.6989038586616516
Epoch 660, training loss: 6.2071990966796875 = 0.12205009162425995 + 1.0 * 6.085148811340332
Epoch 660, val loss: 0.7021581530570984
Epoch 670, training loss: 6.201927661895752 = 0.11566653102636337 + 1.0 * 6.08626127243042
Epoch 670, val loss: 0.70587557554245
Epoch 680, training loss: 6.197539329528809 = 0.10979615896940231 + 1.0 * 6.087743282318115
Epoch 680, val loss: 0.7098994851112366
Epoch 690, training loss: 6.186792373657227 = 0.10433352738618851 + 1.0 * 6.082458972930908
Epoch 690, val loss: 0.7142379879951477
Epoch 700, training loss: 6.177657604217529 = 0.0992722138762474 + 1.0 * 6.078385353088379
Epoch 700, val loss: 0.7188469767570496
Epoch 710, training loss: 6.174548149108887 = 0.09455461800098419 + 1.0 * 6.079993724822998
Epoch 710, val loss: 0.7236588001251221
Epoch 720, training loss: 6.168504238128662 = 0.09015253931283951 + 1.0 * 6.0783514976501465
Epoch 720, val loss: 0.7287068963050842
Epoch 730, training loss: 6.160638809204102 = 0.08606084436178207 + 1.0 * 6.074577808380127
Epoch 730, val loss: 0.7337285280227661
Epoch 740, training loss: 6.155608654022217 = 0.08221489936113358 + 1.0 * 6.073393821716309
Epoch 740, val loss: 0.7389656901359558
Epoch 750, training loss: 6.165892601013184 = 0.07860028743743896 + 1.0 * 6.087292194366455
Epoch 750, val loss: 0.7443650960922241
Epoch 760, training loss: 6.151010513305664 = 0.07519504427909851 + 1.0 * 6.075815677642822
Epoch 760, val loss: 0.749726414680481
Epoch 770, training loss: 6.140558242797852 = 0.0719996765255928 + 1.0 * 6.068558692932129
Epoch 770, val loss: 0.7551113963127136
Epoch 780, training loss: 6.137146472930908 = 0.06896587461233139 + 1.0 * 6.068180561065674
Epoch 780, val loss: 0.7606260180473328
Epoch 790, training loss: 6.147672653198242 = 0.06607776135206223 + 1.0 * 6.081594944000244
Epoch 790, val loss: 0.7662377953529358
Epoch 800, training loss: 6.131228923797607 = 0.0633278489112854 + 1.0 * 6.067901134490967
Epoch 800, val loss: 0.7717180848121643
Epoch 810, training loss: 6.1272358894348145 = 0.06072084233164787 + 1.0 * 6.06651496887207
Epoch 810, val loss: 0.7771755456924438
Epoch 820, training loss: 6.1248884201049805 = 0.058212947100400925 + 1.0 * 6.066675662994385
Epoch 820, val loss: 0.782685399055481
Epoch 830, training loss: 6.119865417480469 = 0.05581868067383766 + 1.0 * 6.064046859741211
Epoch 830, val loss: 0.7881456017494202
Epoch 840, training loss: 6.115748405456543 = 0.05351800099015236 + 1.0 * 6.062230587005615
Epoch 840, val loss: 0.793556809425354
Epoch 850, training loss: 6.1136274337768555 = 0.0513019934296608 + 1.0 * 6.062325477600098
Epoch 850, val loss: 0.7989348769187927
Epoch 860, training loss: 6.1135993003845215 = 0.04918057844042778 + 1.0 * 6.064418792724609
Epoch 860, val loss: 0.8043400645256042
Epoch 870, training loss: 6.107778549194336 = 0.04719674214720726 + 1.0 * 6.060581684112549
Epoch 870, val loss: 0.8096837997436523
Epoch 880, training loss: 6.1115193367004395 = 0.04531734064221382 + 1.0 * 6.066202163696289
Epoch 880, val loss: 0.8150859475135803
Epoch 890, training loss: 6.103418350219727 = 0.043558329343795776 + 1.0 * 6.0598602294921875
Epoch 890, val loss: 0.8204935789108276
Epoch 900, training loss: 6.0989227294921875 = 0.04191246256232262 + 1.0 * 6.057010173797607
Epoch 900, val loss: 0.8259069323539734
Epoch 910, training loss: 6.095994472503662 = 0.04033558815717697 + 1.0 * 6.05565881729126
Epoch 910, val loss: 0.8313531279563904
Epoch 920, training loss: 6.097409248352051 = 0.038838330656290054 + 1.0 * 6.058570861816406
Epoch 920, val loss: 0.8367108106613159
Epoch 930, training loss: 6.0970282554626465 = 0.037445373833179474 + 1.0 * 6.059582710266113
Epoch 930, val loss: 0.8420331478118896
Epoch 940, training loss: 6.0903425216674805 = 0.036141060292720795 + 1.0 * 6.054201602935791
Epoch 940, val loss: 0.8473049402236938
Epoch 950, training loss: 6.092362880706787 = 0.03490811958909035 + 1.0 * 6.057454586029053
Epoch 950, val loss: 0.8525777459144592
Epoch 960, training loss: 6.086366653442383 = 0.03373735025525093 + 1.0 * 6.052629470825195
Epoch 960, val loss: 0.8578093647956848
Epoch 970, training loss: 6.084486961364746 = 0.032629724591970444 + 1.0 * 6.0518574714660645
Epoch 970, val loss: 0.862951397895813
Epoch 980, training loss: 6.083273410797119 = 0.03157287463545799 + 1.0 * 6.051700592041016
Epoch 980, val loss: 0.8681163191795349
Epoch 990, training loss: 6.083481311798096 = 0.03055977076292038 + 1.0 * 6.052921772003174
Epoch 990, val loss: 0.8732501268386841
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.8450
Flip ASR: 0.8133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.327608108520508 = 1.9536776542663574 + 1.0 * 8.373929977416992
Epoch 0, val loss: 1.9573246240615845
Epoch 10, training loss: 10.317866325378418 = 1.9441622495651245 + 1.0 * 8.373703956604004
Epoch 10, val loss: 1.9478901624679565
Epoch 20, training loss: 10.304261207580566 = 1.9325402975082397 + 1.0 * 8.371721267700195
Epoch 20, val loss: 1.9358195066452026
Epoch 30, training loss: 10.271489143371582 = 1.916359305381775 + 1.0 * 8.355130195617676
Epoch 30, val loss: 1.9187899827957153
Epoch 40, training loss: 10.150894165039062 = 1.8948228359222412 + 1.0 * 8.256071090698242
Epoch 40, val loss: 1.8967161178588867
Epoch 50, training loss: 9.722606658935547 = 1.8706319332122803 + 1.0 * 7.851974964141846
Epoch 50, val loss: 1.872827410697937
Epoch 60, training loss: 9.452545166015625 = 1.8480205535888672 + 1.0 * 7.604525089263916
Epoch 60, val loss: 1.8521932363510132
Epoch 70, training loss: 9.08437728881836 = 1.8282694816589355 + 1.0 * 7.256108283996582
Epoch 70, val loss: 1.8337984085083008
Epoch 80, training loss: 8.813606262207031 = 1.808159351348877 + 1.0 * 7.005446434020996
Epoch 80, val loss: 1.8141708374023438
Epoch 90, training loss: 8.583168029785156 = 1.7884844541549683 + 1.0 * 6.794683933258057
Epoch 90, val loss: 1.7951303720474243
Epoch 100, training loss: 8.419471740722656 = 1.7693678140640259 + 1.0 * 6.65010404586792
Epoch 100, val loss: 1.777396559715271
Epoch 110, training loss: 8.31244945526123 = 1.7489800453186035 + 1.0 * 6.563469409942627
Epoch 110, val loss: 1.759903907775879
Epoch 120, training loss: 8.226165771484375 = 1.7274962663650513 + 1.0 * 6.498669624328613
Epoch 120, val loss: 1.7419105768203735
Epoch 130, training loss: 8.154651641845703 = 1.7041822671890259 + 1.0 * 6.450469493865967
Epoch 130, val loss: 1.7220789194107056
Epoch 140, training loss: 8.086366653442383 = 1.6777595281600952 + 1.0 * 6.408607482910156
Epoch 140, val loss: 1.6998143196105957
Epoch 150, training loss: 8.021466255187988 = 1.6471474170684814 + 1.0 * 6.374318599700928
Epoch 150, val loss: 1.6748048067092896
Epoch 160, training loss: 7.960459232330322 = 1.6114062070846558 + 1.0 * 6.349052906036377
Epoch 160, val loss: 1.646263599395752
Epoch 170, training loss: 7.898879528045654 = 1.5698589086532593 + 1.0 * 6.3290205001831055
Epoch 170, val loss: 1.6133456230163574
Epoch 180, training loss: 7.836038112640381 = 1.5220681428909302 + 1.0 * 6.31397008895874
Epoch 180, val loss: 1.5754841566085815
Epoch 190, training loss: 7.765071868896484 = 1.4686200618743896 + 1.0 * 6.296452045440674
Epoch 190, val loss: 1.5333802700042725
Epoch 200, training loss: 7.6926093101501465 = 1.4096976518630981 + 1.0 * 6.282911777496338
Epoch 200, val loss: 1.4872897863388062
Epoch 210, training loss: 7.618592739105225 = 1.3469600677490234 + 1.0 * 6.271632671356201
Epoch 210, val loss: 1.438401222229004
Epoch 220, training loss: 7.541297912597656 = 1.2815144062042236 + 1.0 * 6.2597832679748535
Epoch 220, val loss: 1.3877838850021362
Epoch 230, training loss: 7.4651899337768555 = 1.2143274545669556 + 1.0 * 6.2508625984191895
Epoch 230, val loss: 1.3362500667572021
Epoch 240, training loss: 7.39316463470459 = 1.1479040384292603 + 1.0 * 6.245260715484619
Epoch 240, val loss: 1.2856591939926147
Epoch 250, training loss: 7.3188886642456055 = 1.0833780765533447 + 1.0 * 6.235510349273682
Epoch 250, val loss: 1.2371623516082764
Epoch 260, training loss: 7.253876686096191 = 1.0211538076400757 + 1.0 * 6.232722759246826
Epoch 260, val loss: 1.190890908241272
Epoch 270, training loss: 7.184628963470459 = 0.9628453254699707 + 1.0 * 6.221783638000488
Epoch 270, val loss: 1.147444486618042
Epoch 280, training loss: 7.122305870056152 = 0.9077869057655334 + 1.0 * 6.214519023895264
Epoch 280, val loss: 1.106689453125
Epoch 290, training loss: 7.071971416473389 = 0.8564059138298035 + 1.0 * 6.2155656814575195
Epoch 290, val loss: 1.0687323808670044
Epoch 300, training loss: 7.013698577880859 = 0.8092443346977234 + 1.0 * 6.20445442199707
Epoch 300, val loss: 1.0342297554016113
Epoch 310, training loss: 6.963346004486084 = 0.7656530141830444 + 1.0 * 6.19769287109375
Epoch 310, val loss: 1.002800464630127
Epoch 320, training loss: 6.923264980316162 = 0.7252777814865112 + 1.0 * 6.197987079620361
Epoch 320, val loss: 0.9740649461746216
Epoch 330, training loss: 6.877514362335205 = 0.6883648037910461 + 1.0 * 6.189149379730225
Epoch 330, val loss: 0.9482790231704712
Epoch 340, training loss: 6.838059902191162 = 0.6537660956382751 + 1.0 * 6.184293746948242
Epoch 340, val loss: 0.9250003099441528
Epoch 350, training loss: 6.805669784545898 = 0.621137261390686 + 1.0 * 6.184532642364502
Epoch 350, val loss: 0.9036920666694641
Epoch 360, training loss: 6.765748023986816 = 0.5900334119796753 + 1.0 * 6.175714492797852
Epoch 360, val loss: 0.8843433260917664
Epoch 370, training loss: 6.730946063995361 = 0.5601431131362915 + 1.0 * 6.170803070068359
Epoch 370, val loss: 0.8666646480560303
Epoch 380, training loss: 6.705050468444824 = 0.5310409069061279 + 1.0 * 6.174009799957275
Epoch 380, val loss: 0.8501425385475159
Epoch 390, training loss: 6.665876388549805 = 0.5027268528938293 + 1.0 * 6.163149356842041
Epoch 390, val loss: 0.8348227739334106
Epoch 400, training loss: 6.634371280670166 = 0.47525420784950256 + 1.0 * 6.159117221832275
Epoch 400, val loss: 0.8208843469619751
Epoch 410, training loss: 6.603979587554932 = 0.4483509063720703 + 1.0 * 6.155628681182861
Epoch 410, val loss: 0.8080313801765442
Epoch 420, training loss: 6.586244583129883 = 0.42220887541770935 + 1.0 * 6.164035797119141
Epoch 420, val loss: 0.7962525486946106
Epoch 430, training loss: 6.549334526062012 = 0.3971315324306488 + 1.0 * 6.15220308303833
Epoch 430, val loss: 0.7858613133430481
Epoch 440, training loss: 6.521571159362793 = 0.37302064895629883 + 1.0 * 6.148550510406494
Epoch 440, val loss: 0.7769335508346558
Epoch 450, training loss: 6.493898868560791 = 0.34980344772338867 + 1.0 * 6.144095420837402
Epoch 450, val loss: 0.7690786719322205
Epoch 460, training loss: 6.489708423614502 = 0.3276074528694153 + 1.0 * 6.162100791931152
Epoch 460, val loss: 0.7622528672218323
Epoch 470, training loss: 6.448239326477051 = 0.3066520690917969 + 1.0 * 6.141587257385254
Epoch 470, val loss: 0.7566197514533997
Epoch 480, training loss: 6.42437219619751 = 0.2869063913822174 + 1.0 * 6.137465953826904
Epoch 480, val loss: 0.7523109912872314
Epoch 490, training loss: 6.402961730957031 = 0.2682430148124695 + 1.0 * 6.134718894958496
Epoch 490, val loss: 0.7489679455757141
Epoch 500, training loss: 6.392298698425293 = 0.2507285475730896 + 1.0 * 6.141570091247559
Epoch 500, val loss: 0.7464777231216431
Epoch 510, training loss: 6.367884159088135 = 0.23440511524677277 + 1.0 * 6.133479118347168
Epoch 510, val loss: 0.7450189590454102
Epoch 520, training loss: 6.347045421600342 = 0.21916453540325165 + 1.0 * 6.127881050109863
Epoch 520, val loss: 0.7445892095565796
Epoch 530, training loss: 6.337313175201416 = 0.20495085418224335 + 1.0 * 6.132362365722656
Epoch 530, val loss: 0.7448421716690063
Epoch 540, training loss: 6.314878940582275 = 0.19184960424900055 + 1.0 * 6.1230292320251465
Epoch 540, val loss: 0.7459747195243835
Epoch 550, training loss: 6.299604415893555 = 0.17963366210460663 + 1.0 * 6.119970798492432
Epoch 550, val loss: 0.7479602694511414
Epoch 560, training loss: 6.2886152267456055 = 0.16827619075775146 + 1.0 * 6.1203389167785645
Epoch 560, val loss: 0.7505893707275391
Epoch 570, training loss: 6.27963924407959 = 0.15774476528167725 + 1.0 * 6.121894359588623
Epoch 570, val loss: 0.7537029385566711
Epoch 580, training loss: 6.266905784606934 = 0.14815878868103027 + 1.0 * 6.118746757507324
Epoch 580, val loss: 0.7575507164001465
Epoch 590, training loss: 6.252440929412842 = 0.13925375044345856 + 1.0 * 6.113187313079834
Epoch 590, val loss: 0.7620307803153992
Epoch 600, training loss: 6.243900775909424 = 0.13097672164440155 + 1.0 * 6.112924098968506
Epoch 600, val loss: 0.7669454216957092
Epoch 610, training loss: 6.2338337898254395 = 0.12332571297883987 + 1.0 * 6.110507965087891
Epoch 610, val loss: 0.7722465395927429
Epoch 620, training loss: 6.223966598510742 = 0.11626363545656204 + 1.0 * 6.107702732086182
Epoch 620, val loss: 0.7780947685241699
Epoch 630, training loss: 6.222925186157227 = 0.10968414694070816 + 1.0 * 6.113241195678711
Epoch 630, val loss: 0.784233570098877
Epoch 640, training loss: 6.214171886444092 = 0.10364080965518951 + 1.0 * 6.110530853271484
Epoch 640, val loss: 0.7906151413917542
Epoch 650, training loss: 6.2014970779418945 = 0.09803205728530884 + 1.0 * 6.1034650802612305
Epoch 650, val loss: 0.7973572611808777
Epoch 660, training loss: 6.192798137664795 = 0.09279751032590866 + 1.0 * 6.100000858306885
Epoch 660, val loss: 0.8042923212051392
Epoch 670, training loss: 6.210082530975342 = 0.08789613842964172 + 1.0 * 6.122186183929443
Epoch 670, val loss: 0.811303436756134
Epoch 680, training loss: 6.182377815246582 = 0.08343565464019775 + 1.0 * 6.098942279815674
Epoch 680, val loss: 0.8184454441070557
Epoch 690, training loss: 6.176392078399658 = 0.07925990223884583 + 1.0 * 6.097132205963135
Epoch 690, val loss: 0.8259757161140442
Epoch 700, training loss: 6.169960021972656 = 0.07535221427679062 + 1.0 * 6.094607830047607
Epoch 700, val loss: 0.8335264921188354
Epoch 710, training loss: 6.1646599769592285 = 0.07167856395244598 + 1.0 * 6.092981338500977
Epoch 710, val loss: 0.8410941362380981
Epoch 720, training loss: 6.161728858947754 = 0.06822717934846878 + 1.0 * 6.093501567840576
Epoch 720, val loss: 0.8487500548362732
Epoch 730, training loss: 6.1680378913879395 = 0.06502708047628403 + 1.0 * 6.103010654449463
Epoch 730, val loss: 0.8562611937522888
Epoch 740, training loss: 6.155900955200195 = 0.0620516873896122 + 1.0 * 6.093849182128906
Epoch 740, val loss: 0.8638855218887329
Epoch 750, training loss: 6.148502826690674 = 0.05926458537578583 + 1.0 * 6.089238166809082
Epoch 750, val loss: 0.8716863989830017
Epoch 760, training loss: 6.14325475692749 = 0.0566365011036396 + 1.0 * 6.086618423461914
Epoch 760, val loss: 0.8793372511863708
Epoch 770, training loss: 6.144560813903809 = 0.05415203049778938 + 1.0 * 6.090408802032471
Epoch 770, val loss: 0.8869184851646423
Epoch 780, training loss: 6.1419596672058105 = 0.051847267895936966 + 1.0 * 6.090112209320068
Epoch 780, val loss: 0.8944181203842163
Epoch 790, training loss: 6.132987976074219 = 0.049665309488773346 + 1.0 * 6.083322525024414
Epoch 790, val loss: 0.9019333124160767
Epoch 800, training loss: 6.131155014038086 = 0.047612257301807404 + 1.0 * 6.083542823791504
Epoch 800, val loss: 0.9094372391700745
Epoch 810, training loss: 6.13214111328125 = 0.04567990079522133 + 1.0 * 6.086461067199707
Epoch 810, val loss: 0.9167019128799438
Epoch 820, training loss: 6.123531818389893 = 0.04385840520262718 + 1.0 * 6.0796732902526855
Epoch 820, val loss: 0.9239835143089294
Epoch 830, training loss: 6.120428562164307 = 0.042140331119298935 + 1.0 * 6.0782880783081055
Epoch 830, val loss: 0.9312869906425476
Epoch 840, training loss: 6.121298313140869 = 0.040504612028598785 + 1.0 * 6.080793857574463
Epoch 840, val loss: 0.9383968114852905
Epoch 850, training loss: 6.1151957511901855 = 0.038971539586782455 + 1.0 * 6.076224327087402
Epoch 850, val loss: 0.945368230342865
Epoch 860, training loss: 6.114504814147949 = 0.037519291043281555 + 1.0 * 6.0769853591918945
Epoch 860, val loss: 0.9524037837982178
Epoch 870, training loss: 6.1113433837890625 = 0.036142461001873016 + 1.0 * 6.075201034545898
Epoch 870, val loss: 0.9593845009803772
Epoch 880, training loss: 6.116277694702148 = 0.03483467549085617 + 1.0 * 6.081442832946777
Epoch 880, val loss: 0.9662057161331177
Epoch 890, training loss: 6.109816074371338 = 0.033593691885471344 + 1.0 * 6.0762224197387695
Epoch 890, val loss: 0.9727346897125244
Epoch 900, training loss: 6.105268478393555 = 0.03241845592856407 + 1.0 * 6.072850227355957
Epoch 900, val loss: 0.9794692397117615
Epoch 910, training loss: 6.103093147277832 = 0.031302157789468765 + 1.0 * 6.071791172027588
Epoch 910, val loss: 0.9860337972640991
Epoch 920, training loss: 6.106301784515381 = 0.03024173527956009 + 1.0 * 6.076059818267822
Epoch 920, val loss: 0.9924771785736084
Epoch 930, training loss: 6.104216575622559 = 0.029241004958748817 + 1.0 * 6.074975490570068
Epoch 930, val loss: 0.9988108277320862
Epoch 940, training loss: 6.096611499786377 = 0.028279758989810944 + 1.0 * 6.068331718444824
Epoch 940, val loss: 1.0050468444824219
Epoch 950, training loss: 6.094186305999756 = 0.027368750423192978 + 1.0 * 6.066817760467529
Epoch 950, val loss: 1.0113199949264526
Epoch 960, training loss: 6.094202518463135 = 0.026495909318327904 + 1.0 * 6.06770658493042
Epoch 960, val loss: 1.017433762550354
Epoch 970, training loss: 6.093647003173828 = 0.025663698092103004 + 1.0 * 6.067983150482178
Epoch 970, val loss: 1.0233986377716064
Epoch 980, training loss: 6.101203918457031 = 0.024870799854397774 + 1.0 * 6.076333045959473
Epoch 980, val loss: 1.0293152332305908
Epoch 990, training loss: 6.091739654541016 = 0.024124380201101303 + 1.0 * 6.067615509033203
Epoch 990, val loss: 1.0350873470306396
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7897
Flip ASR: 0.7467/225 nodes
The final ASR:0.69373, 0.17627, Accuracy:0.80741, 0.01571
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10546])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00696, Accuracy:0.83457, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.31408977508545 = 1.9403315782546997 + 1.0 * 8.373758316040039
Epoch 0, val loss: 1.932459831237793
Epoch 10, training loss: 10.302900314331055 = 1.929905652999878 + 1.0 * 8.372994422912598
Epoch 10, val loss: 1.9227123260498047
Epoch 20, training loss: 10.284786224365234 = 1.9170016050338745 + 1.0 * 8.36778450012207
Epoch 20, val loss: 1.910343885421753
Epoch 30, training loss: 10.234394073486328 = 1.8994990587234497 + 1.0 * 8.334895133972168
Epoch 30, val loss: 1.8934197425842285
Epoch 40, training loss: 10.007451057434082 = 1.8788299560546875 + 1.0 * 8.128621101379395
Epoch 40, val loss: 1.8739120960235596
Epoch 50, training loss: 9.481761932373047 = 1.855919599533081 + 1.0 * 7.625842571258545
Epoch 50, val loss: 1.852430820465088
Epoch 60, training loss: 9.034933090209961 = 1.8401769399642944 + 1.0 * 7.194756031036377
Epoch 60, val loss: 1.8390625715255737
Epoch 70, training loss: 8.621837615966797 = 1.8290557861328125 + 1.0 * 6.792781829833984
Epoch 70, val loss: 1.8296786546707153
Epoch 80, training loss: 8.418575286865234 = 1.8189406394958496 + 1.0 * 6.599634170532227
Epoch 80, val loss: 1.8208366632461548
Epoch 90, training loss: 8.318036079406738 = 1.8062092065811157 + 1.0 * 6.511826515197754
Epoch 90, val loss: 1.8094274997711182
Epoch 100, training loss: 8.239033699035645 = 1.7929590940475464 + 1.0 * 6.446074485778809
Epoch 100, val loss: 1.798307180404663
Epoch 110, training loss: 8.179701805114746 = 1.781171441078186 + 1.0 * 6.39853048324585
Epoch 110, val loss: 1.7892532348632812
Epoch 120, training loss: 8.128135681152344 = 1.7702451944351196 + 1.0 * 6.357890605926514
Epoch 120, val loss: 1.7805415391921997
Epoch 130, training loss: 8.082931518554688 = 1.758505940437317 + 1.0 * 6.324425220489502
Epoch 130, val loss: 1.770647644996643
Epoch 140, training loss: 8.043187141418457 = 1.7447110414505005 + 1.0 * 6.298476219177246
Epoch 140, val loss: 1.7589739561080933
Epoch 150, training loss: 8.006440162658691 = 1.7282058000564575 + 1.0 * 6.278234004974365
Epoch 150, val loss: 1.7453218698501587
Epoch 160, training loss: 7.966983795166016 = 1.7083981037139893 + 1.0 * 6.258585453033447
Epoch 160, val loss: 1.7292569875717163
Epoch 170, training loss: 7.9277777671813965 = 1.6844549179077148 + 1.0 * 6.243322849273682
Epoch 170, val loss: 1.7099769115447998
Epoch 180, training loss: 7.885500907897949 = 1.6557338237762451 + 1.0 * 6.229767322540283
Epoch 180, val loss: 1.6869487762451172
Epoch 190, training loss: 7.837510108947754 = 1.6218035221099854 + 1.0 * 6.2157063484191895
Epoch 190, val loss: 1.6597051620483398
Epoch 200, training loss: 7.786696910858154 = 1.582254409790039 + 1.0 * 6.204442501068115
Epoch 200, val loss: 1.6281236410140991
Epoch 210, training loss: 7.730800628662109 = 1.5377891063690186 + 1.0 * 6.193011283874512
Epoch 210, val loss: 1.592534065246582
Epoch 220, training loss: 7.6743340492248535 = 1.4891585111618042 + 1.0 * 6.18517541885376
Epoch 220, val loss: 1.553654670715332
Epoch 230, training loss: 7.616824150085449 = 1.438816785812378 + 1.0 * 6.17800760269165
Epoch 230, val loss: 1.5143171548843384
Epoch 240, training loss: 7.556673049926758 = 1.387971043586731 + 1.0 * 6.168702125549316
Epoch 240, val loss: 1.475102186203003
Epoch 250, training loss: 7.500338554382324 = 1.3368158340454102 + 1.0 * 6.163522720336914
Epoch 250, val loss: 1.4362767934799194
Epoch 260, training loss: 7.443149089813232 = 1.2865628004074097 + 1.0 * 6.156586170196533
Epoch 260, val loss: 1.3992542028427124
Epoch 270, training loss: 7.387465476989746 = 1.2369513511657715 + 1.0 * 6.150514125823975
Epoch 270, val loss: 1.363275170326233
Epoch 280, training loss: 7.333615779876709 = 1.1872214078903198 + 1.0 * 6.1463942527771
Epoch 280, val loss: 1.327641487121582
Epoch 290, training loss: 7.278326988220215 = 1.1374599933624268 + 1.0 * 6.140866756439209
Epoch 290, val loss: 1.2923369407653809
Epoch 300, training loss: 7.225086688995361 = 1.087166666984558 + 1.0 * 6.137919902801514
Epoch 300, val loss: 1.2569894790649414
Epoch 310, training loss: 7.1688232421875 = 1.0370533466339111 + 1.0 * 6.131770133972168
Epoch 310, val loss: 1.2217340469360352
Epoch 320, training loss: 7.115265846252441 = 0.9868652820587158 + 1.0 * 6.1284003257751465
Epoch 320, val loss: 1.1865487098693848
Epoch 330, training loss: 7.061716556549072 = 0.9375272393226624 + 1.0 * 6.124189376831055
Epoch 330, val loss: 1.1516450643539429
Epoch 340, training loss: 7.0123491287231445 = 0.8892929553985596 + 1.0 * 6.123056411743164
Epoch 340, val loss: 1.117674469947815
Epoch 350, training loss: 6.962359428405762 = 0.8432502746582031 + 1.0 * 6.119109153747559
Epoch 350, val loss: 1.085195541381836
Epoch 360, training loss: 6.913485527038574 = 0.7993066310882568 + 1.0 * 6.114178657531738
Epoch 360, val loss: 1.0548198223114014
Epoch 370, training loss: 6.8702569007873535 = 0.757045328617096 + 1.0 * 6.113211631774902
Epoch 370, val loss: 1.0261921882629395
Epoch 380, training loss: 6.826335906982422 = 0.7165607810020447 + 1.0 * 6.109775066375732
Epoch 380, val loss: 0.9990398287773132
Epoch 390, training loss: 6.784882545471191 = 0.6774829626083374 + 1.0 * 6.1073994636535645
Epoch 390, val loss: 0.973530650138855
Epoch 400, training loss: 6.74550199508667 = 0.6397067308425903 + 1.0 * 6.105795383453369
Epoch 400, val loss: 0.9491276741027832
Epoch 410, training loss: 6.708676338195801 = 0.6033753156661987 + 1.0 * 6.1053009033203125
Epoch 410, val loss: 0.9260954856872559
Epoch 420, training loss: 6.669957160949707 = 0.5686056613922119 + 1.0 * 6.101351261138916
Epoch 420, val loss: 0.904286801815033
Epoch 430, training loss: 6.63361120223999 = 0.5351793169975281 + 1.0 * 6.0984320640563965
Epoch 430, val loss: 0.8839304447174072
Epoch 440, training loss: 6.602902412414551 = 0.5031673312187195 + 1.0 * 6.099735260009766
Epoch 440, val loss: 0.864896833896637
Epoch 450, training loss: 6.566648960113525 = 0.47285333275794983 + 1.0 * 6.0937957763671875
Epoch 450, val loss: 0.8475754261016846
Epoch 460, training loss: 6.535409927368164 = 0.4440438151359558 + 1.0 * 6.091366291046143
Epoch 460, val loss: 0.8320258259773254
Epoch 470, training loss: 6.511721611022949 = 0.416744202375412 + 1.0 * 6.094977378845215
Epoch 470, val loss: 0.8180014491081238
Epoch 480, training loss: 6.4788079261779785 = 0.3911581039428711 + 1.0 * 6.087649822235107
Epoch 480, val loss: 0.8059026598930359
Epoch 490, training loss: 6.4519829750061035 = 0.3672259747982025 + 1.0 * 6.084756851196289
Epoch 490, val loss: 0.7958372831344604
Epoch 500, training loss: 6.430972099304199 = 0.34471943974494934 + 1.0 * 6.086252689361572
Epoch 500, val loss: 0.7873630523681641
Epoch 510, training loss: 6.411233901977539 = 0.3237082362174988 + 1.0 * 6.087525844573975
Epoch 510, val loss: 0.7805314660072327
Epoch 520, training loss: 6.384814262390137 = 0.3041227459907532 + 1.0 * 6.080691337585449
Epoch 520, val loss: 0.7756218314170837
Epoch 530, training loss: 6.378846645355225 = 0.2855987846851349 + 1.0 * 6.093247890472412
Epoch 530, val loss: 0.7718620300292969
Epoch 540, training loss: 6.34515380859375 = 0.26830193400382996 + 1.0 * 6.076851844787598
Epoch 540, val loss: 0.7692680954933167
Epoch 550, training loss: 6.32724666595459 = 0.2518419027328491 + 1.0 * 6.075404644012451
Epoch 550, val loss: 0.7681210041046143
Epoch 560, training loss: 6.310020446777344 = 0.2360537201166153 + 1.0 * 6.0739665031433105
Epoch 560, val loss: 0.7675995826721191
Epoch 570, training loss: 6.308079242706299 = 0.22090137004852295 + 1.0 * 6.087177753448486
Epoch 570, val loss: 0.7679039835929871
Epoch 580, training loss: 6.281407833099365 = 0.20668165385723114 + 1.0 * 6.074726104736328
Epoch 580, val loss: 0.7688845992088318
Epoch 590, training loss: 6.265219688415527 = 0.19326947629451752 + 1.0 * 6.071950435638428
Epoch 590, val loss: 0.7709790468215942
Epoch 600, training loss: 6.24970006942749 = 0.18060483038425446 + 1.0 * 6.069095134735107
Epoch 600, val loss: 0.7732948064804077
Epoch 610, training loss: 6.239117622375488 = 0.16874010860919952 + 1.0 * 6.070377349853516
Epoch 610, val loss: 0.7761102318763733
Epoch 620, training loss: 6.229522705078125 = 0.1577816605567932 + 1.0 * 6.071741104125977
Epoch 620, val loss: 0.7795490026473999
Epoch 630, training loss: 6.214865207672119 = 0.14776374399662018 + 1.0 * 6.06710147857666
Epoch 630, val loss: 0.7835556864738464
Epoch 640, training loss: 6.201903820037842 = 0.13853251934051514 + 1.0 * 6.063371181488037
Epoch 640, val loss: 0.7879175543785095
Epoch 650, training loss: 6.192887783050537 = 0.13004466891288757 + 1.0 * 6.062843322753906
Epoch 650, val loss: 0.7925207018852234
Epoch 660, training loss: 6.193405628204346 = 0.12227127701044083 + 1.0 * 6.071134567260742
Epoch 660, val loss: 0.797235369682312
Epoch 670, training loss: 6.1772308349609375 = 0.1152946949005127 + 1.0 * 6.061936378479004
Epoch 670, val loss: 0.8024782538414001
Epoch 680, training loss: 6.167095184326172 = 0.10889361053705215 + 1.0 * 6.058201789855957
Epoch 680, val loss: 0.8081082105636597
Epoch 690, training loss: 6.159954071044922 = 0.10299055278301239 + 1.0 * 6.0569634437561035
Epoch 690, val loss: 0.8134607672691345
Epoch 700, training loss: 6.154016494750977 = 0.09753167629241943 + 1.0 * 6.056484699249268
Epoch 700, val loss: 0.8191877603530884
Epoch 710, training loss: 6.149125576019287 = 0.09250825643539429 + 1.0 * 6.056617259979248
Epoch 710, val loss: 0.8248298168182373
Epoch 720, training loss: 6.144522666931152 = 0.08791717141866684 + 1.0 * 6.056605339050293
Epoch 720, val loss: 0.8307985067367554
Epoch 730, training loss: 6.13916540145874 = 0.08366761356592178 + 1.0 * 6.055497646331787
Epoch 730, val loss: 0.8369024395942688
Epoch 740, training loss: 6.133570194244385 = 0.07971780002117157 + 1.0 * 6.053852558135986
Epoch 740, val loss: 0.8425488471984863
Epoch 750, training loss: 6.126956462860107 = 0.07605424523353577 + 1.0 * 6.050902366638184
Epoch 750, val loss: 0.8486666083335876
Epoch 760, training loss: 6.123790264129639 = 0.0726364478468895 + 1.0 * 6.051153659820557
Epoch 760, val loss: 0.854682981967926
Epoch 770, training loss: 6.118993282318115 = 0.06944022327661514 + 1.0 * 6.049552917480469
Epoch 770, val loss: 0.8603448271751404
Epoch 780, training loss: 6.1188063621521 = 0.06645390391349792 + 1.0 * 6.052352428436279
Epoch 780, val loss: 0.866362988948822
Epoch 790, training loss: 6.112702369689941 = 0.06366612017154694 + 1.0 * 6.049036026000977
Epoch 790, val loss: 0.8722155094146729
Epoch 800, training loss: 6.1103315353393555 = 0.061040010303258896 + 1.0 * 6.049291610717773
Epoch 800, val loss: 0.8779217600822449
Epoch 810, training loss: 6.107769966125488 = 0.058578114956617355 + 1.0 * 6.049191951751709
Epoch 810, val loss: 0.883560836315155
Epoch 820, training loss: 6.102441310882568 = 0.056273918598890305 + 1.0 * 6.046167373657227
Epoch 820, val loss: 0.8893159627914429
Epoch 830, training loss: 6.098872661590576 = 0.05408897623419762 + 1.0 * 6.044783592224121
Epoch 830, val loss: 0.8949066996574402
Epoch 840, training loss: 6.0991926193237305 = 0.05202851817011833 + 1.0 * 6.047163963317871
Epoch 840, val loss: 0.9002711772918701
Epoch 850, training loss: 6.0943217277526855 = 0.05008978769183159 + 1.0 * 6.04423189163208
Epoch 850, val loss: 0.9058292508125305
Epoch 860, training loss: 6.094425201416016 = 0.04825008660554886 + 1.0 * 6.046175003051758
Epoch 860, val loss: 0.9112619161605835
Epoch 870, training loss: 6.089183330535889 = 0.046516165137290955 + 1.0 * 6.042667388916016
Epoch 870, val loss: 0.9164007306098938
Epoch 880, training loss: 6.084562301635742 = 0.04487495869398117 + 1.0 * 6.039687156677246
Epoch 880, val loss: 0.921787679195404
Epoch 890, training loss: 6.083000659942627 = 0.043312449008226395 + 1.0 * 6.0396881103515625
Epoch 890, val loss: 0.92704838514328
Epoch 900, training loss: 6.085859775543213 = 0.0418223962187767 + 1.0 * 6.044037342071533
Epoch 900, val loss: 0.9320814609527588
Epoch 910, training loss: 6.0823750495910645 = 0.04040724039077759 + 1.0 * 6.041967868804932
Epoch 910, val loss: 0.9371362924575806
Epoch 920, training loss: 6.077193737030029 = 0.0390695184469223 + 1.0 * 6.038124084472656
Epoch 920, val loss: 0.9423031210899353
Epoch 930, training loss: 6.074084281921387 = 0.037791408598423004 + 1.0 * 6.036293029785156
Epoch 930, val loss: 0.9473446607589722
Epoch 940, training loss: 6.077880382537842 = 0.03656696155667305 + 1.0 * 6.041313648223877
Epoch 940, val loss: 0.952150821685791
Epoch 950, training loss: 6.074921131134033 = 0.035401321947574615 + 1.0 * 6.039519786834717
Epoch 950, val loss: 0.9569239020347595
Epoch 960, training loss: 6.072747707366943 = 0.03429984301328659 + 1.0 * 6.038447856903076
Epoch 960, val loss: 0.9617812633514404
Epoch 970, training loss: 6.066871166229248 = 0.033247072249650955 + 1.0 * 6.033624172210693
Epoch 970, val loss: 0.966674268245697
Epoch 980, training loss: 6.065711975097656 = 0.03223511204123497 + 1.0 * 6.033476829528809
Epoch 980, val loss: 0.9713340401649475
Epoch 990, training loss: 6.0664191246032715 = 0.03126450255513191 + 1.0 * 6.035154819488525
Epoch 990, val loss: 0.9759021997451782
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7970
Flip ASR: 0.7556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.324172019958496 = 1.950372576713562 + 1.0 * 8.373799324035645
Epoch 0, val loss: 1.9454549551010132
Epoch 10, training loss: 10.313019752502441 = 1.9397896528244019 + 1.0 * 8.37322998046875
Epoch 10, val loss: 1.9345036745071411
Epoch 20, training loss: 10.296427726745605 = 1.927328109741211 + 1.0 * 8.369099617004395
Epoch 20, val loss: 1.9214832782745361
Epoch 30, training loss: 10.249414443969727 = 1.9112755060195923 + 1.0 * 8.338138580322266
Epoch 30, val loss: 1.9048194885253906
Epoch 40, training loss: 9.995024681091309 = 1.8929542303085327 + 1.0 * 8.102070808410645
Epoch 40, val loss: 1.8863950967788696
Epoch 50, training loss: 9.208821296691895 = 1.8733785152435303 + 1.0 * 7.335442543029785
Epoch 50, val loss: 1.8665622472763062
Epoch 60, training loss: 8.888671875 = 1.8573384284973145 + 1.0 * 7.031332969665527
Epoch 60, val loss: 1.850545883178711
Epoch 70, training loss: 8.636299133300781 = 1.8423184156417847 + 1.0 * 6.793980598449707
Epoch 70, val loss: 1.8356138467788696
Epoch 80, training loss: 8.451592445373535 = 1.828872561454773 + 1.0 * 6.622719764709473
Epoch 80, val loss: 1.8224542140960693
Epoch 90, training loss: 8.33169174194336 = 1.8167953491210938 + 1.0 * 6.514896392822266
Epoch 90, val loss: 1.8105244636535645
Epoch 100, training loss: 8.225887298583984 = 1.8053313493728638 + 1.0 * 6.42055606842041
Epoch 100, val loss: 1.7994846105575562
Epoch 110, training loss: 8.151971817016602 = 1.7942858934402466 + 1.0 * 6.357685565948486
Epoch 110, val loss: 1.7889668941497803
Epoch 120, training loss: 8.099299430847168 = 1.7828670740127563 + 1.0 * 6.316431999206543
Epoch 120, val loss: 1.7780336141586304
Epoch 130, training loss: 8.053812980651855 = 1.7706496715545654 + 1.0 * 6.283163070678711
Epoch 130, val loss: 1.7663342952728271
Epoch 140, training loss: 8.014811515808105 = 1.7572602033615112 + 1.0 * 6.257551193237305
Epoch 140, val loss: 1.753659963607788
Epoch 150, training loss: 7.9827423095703125 = 1.7419427633285522 + 1.0 * 6.240799427032471
Epoch 150, val loss: 1.7396727800369263
Epoch 160, training loss: 7.946422576904297 = 1.7243046760559082 + 1.0 * 6.222117900848389
Epoch 160, val loss: 1.7239258289337158
Epoch 170, training loss: 7.912530422210693 = 1.7032300233840942 + 1.0 * 6.209300518035889
Epoch 170, val loss: 1.7058693170547485
Epoch 180, training loss: 7.876305103302002 = 1.6776622533798218 + 1.0 * 6.198642730712891
Epoch 180, val loss: 1.6845940351486206
Epoch 190, training loss: 7.83664608001709 = 1.6467030048370361 + 1.0 * 6.189943313598633
Epoch 190, val loss: 1.6593186855316162
Epoch 200, training loss: 7.790831089019775 = 1.6092430353164673 + 1.0 * 6.181588172912598
Epoch 200, val loss: 1.6291621923446655
Epoch 210, training loss: 7.740241050720215 = 1.5641764402389526 + 1.0 * 6.176064491271973
Epoch 210, val loss: 1.5930758714675903
Epoch 220, training loss: 7.679365158081055 = 1.5112972259521484 + 1.0 * 6.168067932128906
Epoch 220, val loss: 1.5510227680206299
Epoch 230, training loss: 7.6133503913879395 = 1.450853943824768 + 1.0 * 6.162496566772461
Epoch 230, val loss: 1.5028505325317383
Epoch 240, training loss: 7.543834209442139 = 1.3850058317184448 + 1.0 * 6.158828258514404
Epoch 240, val loss: 1.4510862827301025
Epoch 250, training loss: 7.471480369567871 = 1.3176915645599365 + 1.0 * 6.1537885665893555
Epoch 250, val loss: 1.3980530500411987
Epoch 260, training loss: 7.398900032043457 = 1.249968409538269 + 1.0 * 6.148931503295898
Epoch 260, val loss: 1.3450995683670044
Epoch 270, training loss: 7.331852912902832 = 1.1844656467437744 + 1.0 * 6.147387504577637
Epoch 270, val loss: 1.2946043014526367
Epoch 280, training loss: 7.262307167053223 = 1.122724175453186 + 1.0 * 6.139583110809326
Epoch 280, val loss: 1.2473881244659424
Epoch 290, training loss: 7.1992340087890625 = 1.0641430616378784 + 1.0 * 6.1350908279418945
Epoch 290, val loss: 1.2031162977218628
Epoch 300, training loss: 7.14096212387085 = 1.0090179443359375 + 1.0 * 6.131944179534912
Epoch 300, val loss: 1.1620988845825195
Epoch 310, training loss: 7.085483074188232 = 0.9572765231132507 + 1.0 * 6.128206729888916
Epoch 310, val loss: 1.1242287158966064
Epoch 320, training loss: 7.032395362854004 = 0.908858060836792 + 1.0 * 6.123537063598633
Epoch 320, val loss: 1.089134931564331
Epoch 330, training loss: 6.981189727783203 = 0.8629693388938904 + 1.0 * 6.118220329284668
Epoch 330, val loss: 1.0565824508666992
Epoch 340, training loss: 6.938362121582031 = 0.819442093372345 + 1.0 * 6.118919849395752
Epoch 340, val loss: 1.025984525680542
Epoch 350, training loss: 6.889864921569824 = 0.7785486578941345 + 1.0 * 6.111316204071045
Epoch 350, val loss: 0.997603714466095
Epoch 360, training loss: 6.851456642150879 = 0.7401854991912842 + 1.0 * 6.111271381378174
Epoch 360, val loss: 0.9713775515556335
Epoch 370, training loss: 6.811564922332764 = 0.7045175433158875 + 1.0 * 6.1070475578308105
Epoch 370, val loss: 0.9474589824676514
Epoch 380, training loss: 6.7754364013671875 = 0.6713857650756836 + 1.0 * 6.104050636291504
Epoch 380, val loss: 0.925658643245697
Epoch 390, training loss: 6.741030693054199 = 0.640667200088501 + 1.0 * 6.100363254547119
Epoch 390, val loss: 0.9057984948158264
Epoch 400, training loss: 6.707833290100098 = 0.6120442748069763 + 1.0 * 6.095788955688477
Epoch 400, val loss: 0.8879152536392212
Epoch 410, training loss: 6.6818976402282715 = 0.5850119590759277 + 1.0 * 6.096885681152344
Epoch 410, val loss: 0.871673583984375
Epoch 420, training loss: 6.652781963348389 = 0.5595241785049438 + 1.0 * 6.093257904052734
Epoch 420, val loss: 0.8567475080490112
Epoch 430, training loss: 6.625268936157227 = 0.5352246761322021 + 1.0 * 6.090044021606445
Epoch 430, val loss: 0.8433693051338196
Epoch 440, training loss: 6.601178169250488 = 0.5117847919464111 + 1.0 * 6.089393615722656
Epoch 440, val loss: 0.8311638236045837
Epoch 450, training loss: 6.579075813293457 = 0.48923566937446594 + 1.0 * 6.089839935302734
Epoch 450, val loss: 0.8201544880867004
Epoch 460, training loss: 6.551450252532959 = 0.46747061610221863 + 1.0 * 6.083979606628418
Epoch 460, val loss: 0.8103928565979004
Epoch 470, training loss: 6.528497219085693 = 0.44625425338745117 + 1.0 * 6.082242965698242
Epoch 470, val loss: 0.8015773296356201
Epoch 480, training loss: 6.520665645599365 = 0.42577722668647766 + 1.0 * 6.094888210296631
Epoch 480, val loss: 0.7936614751815796
Epoch 490, training loss: 6.485031604766846 = 0.4063107669353485 + 1.0 * 6.078721046447754
Epoch 490, val loss: 0.7873339653015137
Epoch 500, training loss: 6.465174198150635 = 0.38752681016921997 + 1.0 * 6.0776472091674805
Epoch 500, val loss: 0.7822011113166809
Epoch 510, training loss: 6.444368362426758 = 0.3693455457687378 + 1.0 * 6.0750226974487305
Epoch 510, val loss: 0.778063178062439
Epoch 520, training loss: 6.428934097290039 = 0.35183247923851013 + 1.0 * 6.077101707458496
Epoch 520, val loss: 0.7751078009605408
Epoch 530, training loss: 6.410889625549316 = 0.3352065980434418 + 1.0 * 6.075683116912842
Epoch 530, val loss: 0.7731181383132935
Epoch 540, training loss: 6.39027214050293 = 0.3194827437400818 + 1.0 * 6.070789337158203
Epoch 540, val loss: 0.7723012566566467
Epoch 550, training loss: 6.376222133636475 = 0.30450546741485596 + 1.0 * 6.071716785430908
Epoch 550, val loss: 0.7721855044364929
Epoch 560, training loss: 6.362943649291992 = 0.2903069257736206 + 1.0 * 6.072636604309082
Epoch 560, val loss: 0.7725632190704346
Epoch 570, training loss: 6.343808650970459 = 0.276922345161438 + 1.0 * 6.0668864250183105
Epoch 570, val loss: 0.7737400531768799
Epoch 580, training loss: 6.330843448638916 = 0.264138400554657 + 1.0 * 6.066705226898193
Epoch 580, val loss: 0.7753484845161438
Epoch 590, training loss: 6.32773494720459 = 0.25188326835632324 + 1.0 * 6.075851917266846
Epoch 590, val loss: 0.7772604823112488
Epoch 600, training loss: 6.305461406707764 = 0.24026432633399963 + 1.0 * 6.065196990966797
Epoch 600, val loss: 0.7793945074081421
Epoch 610, training loss: 6.293577671051025 = 0.22919052839279175 + 1.0 * 6.064387321472168
Epoch 610, val loss: 0.7820654511451721
Epoch 620, training loss: 6.280467987060547 = 0.21855756640434265 + 1.0 * 6.061910629272461
Epoch 620, val loss: 0.7849419116973877
Epoch 630, training loss: 6.271225929260254 = 0.2083718627691269 + 1.0 * 6.062854290008545
Epoch 630, val loss: 0.7880217432975769
Epoch 640, training loss: 6.262474536895752 = 0.19866235554218292 + 1.0 * 6.063812255859375
Epoch 640, val loss: 0.7913199067115784
Epoch 650, training loss: 6.24822473526001 = 0.18936821818351746 + 1.0 * 6.05885648727417
Epoch 650, val loss: 0.7948991060256958
Epoch 660, training loss: 6.238317489624023 = 0.18043121695518494 + 1.0 * 6.057886123657227
Epoch 660, val loss: 0.7986595034599304
Epoch 670, training loss: 6.240749359130859 = 0.17184674739837646 + 1.0 * 6.068902492523193
Epoch 670, val loss: 0.8024755120277405
Epoch 680, training loss: 6.2202911376953125 = 0.16363990306854248 + 1.0 * 6.0566511154174805
Epoch 680, val loss: 0.8064031600952148
Epoch 690, training loss: 6.212431907653809 = 0.1558092087507248 + 1.0 * 6.056622505187988
Epoch 690, val loss: 0.8106849789619446
Epoch 700, training loss: 6.201700210571289 = 0.14827291667461395 + 1.0 * 6.053427219390869
Epoch 700, val loss: 0.815048098564148
Epoch 710, training loss: 6.210118293762207 = 0.14104288816452026 + 1.0 * 6.069075584411621
Epoch 710, val loss: 0.8195509314537048
Epoch 720, training loss: 6.188687801361084 = 0.13418008387088776 + 1.0 * 6.054507732391357
Epoch 720, val loss: 0.8239902257919312
Epoch 730, training loss: 6.178806781768799 = 0.12766167521476746 + 1.0 * 6.051145076751709
Epoch 730, val loss: 0.8287470936775208
Epoch 740, training loss: 6.171548843383789 = 0.1214236468076706 + 1.0 * 6.0501251220703125
Epoch 740, val loss: 0.8335912227630615
Epoch 750, training loss: 6.175677299499512 = 0.11547822505235672 + 1.0 * 6.06019926071167
Epoch 750, val loss: 0.8385365605354309
Epoch 760, training loss: 6.159228801727295 = 0.10988239198923111 + 1.0 * 6.049346446990967
Epoch 760, val loss: 0.8435408473014832
Epoch 770, training loss: 6.153491497039795 = 0.10462077707052231 + 1.0 * 6.04887056350708
Epoch 770, val loss: 0.8487706780433655
Epoch 780, training loss: 6.148806571960449 = 0.09966456890106201 + 1.0 * 6.049141883850098
Epoch 780, val loss: 0.8541077375411987
Epoch 790, training loss: 6.143387317657471 = 0.09500132501125336 + 1.0 * 6.048386096954346
Epoch 790, val loss: 0.8595479726791382
Epoch 800, training loss: 6.137186527252197 = 0.09063145518302917 + 1.0 * 6.046555042266846
Epoch 800, val loss: 0.8650703430175781
Epoch 810, training loss: 6.13210391998291 = 0.08654171973466873 + 1.0 * 6.045562267303467
Epoch 810, val loss: 0.8708181381225586
Epoch 820, training loss: 6.1274261474609375 = 0.0826854482293129 + 1.0 * 6.044740676879883
Epoch 820, val loss: 0.8766471743583679
Epoch 830, training loss: 6.127551555633545 = 0.07904508709907532 + 1.0 * 6.048506259918213
Epoch 830, val loss: 0.8825858235359192
Epoch 840, training loss: 6.122319221496582 = 0.07562737166881561 + 1.0 * 6.04669189453125
Epoch 840, val loss: 0.8884851932525635
Epoch 850, training loss: 6.1151227951049805 = 0.0724174827337265 + 1.0 * 6.042705535888672
Epoch 850, val loss: 0.8945139646530151
Epoch 860, training loss: 6.112218379974365 = 0.06938573718070984 + 1.0 * 6.042832851409912
Epoch 860, val loss: 0.9006814956665039
Epoch 870, training loss: 6.115047454833984 = 0.06652089208364487 + 1.0 * 6.048526763916016
Epoch 870, val loss: 0.9067476987838745
Epoch 880, training loss: 6.103918552398682 = 0.06382431834936142 + 1.0 * 6.040094375610352
Epoch 880, val loss: 0.912804365158081
Epoch 890, training loss: 6.1008477210998535 = 0.061281345784664154 + 1.0 * 6.039566516876221
Epoch 890, val loss: 0.9190451502799988
Epoch 900, training loss: 6.097193241119385 = 0.058869101107120514 + 1.0 * 6.038324356079102
Epoch 900, val loss: 0.925250232219696
Epoch 910, training loss: 6.096027851104736 = 0.0565774030983448 + 1.0 * 6.039450645446777
Epoch 910, val loss: 0.931442141532898
Epoch 920, training loss: 6.096272945404053 = 0.054403454065322876 + 1.0 * 6.041869640350342
Epoch 920, val loss: 0.937518835067749
Epoch 930, training loss: 6.090832710266113 = 0.0523453988134861 + 1.0 * 6.038487434387207
Epoch 930, val loss: 0.9435681104660034
Epoch 940, training loss: 6.087111949920654 = 0.050402022898197174 + 1.0 * 6.036709785461426
Epoch 940, val loss: 0.9497643113136292
Epoch 950, training loss: 6.08435583114624 = 0.04855092242360115 + 1.0 * 6.035804748535156
Epoch 950, val loss: 0.9559029936790466
Epoch 960, training loss: 6.086462020874023 = 0.046787962317466736 + 1.0 * 6.039674282073975
Epoch 960, val loss: 0.9619159698486328
Epoch 970, training loss: 6.0823259353637695 = 0.04511652886867523 + 1.0 * 6.037209510803223
Epoch 970, val loss: 0.96791011095047
Epoch 980, training loss: 6.0844645500183105 = 0.04353197291493416 + 1.0 * 6.040932655334473
Epoch 980, val loss: 0.9737604260444641
Epoch 990, training loss: 6.0757155418396 = 0.042032379657030106 + 1.0 * 6.0336833000183105
Epoch 990, val loss: 0.9796687364578247
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.4022
Flip ASR: 0.3156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.341684341430664 = 1.9682854413986206 + 1.0 * 8.373398780822754
Epoch 0, val loss: 1.9694828987121582
Epoch 10, training loss: 10.325178146362305 = 1.9563016891479492 + 1.0 * 8.368876457214355
Epoch 10, val loss: 1.9557702541351318
Epoch 20, training loss: 10.302850723266602 = 1.941527009010315 + 1.0 * 8.361323356628418
Epoch 20, val loss: 1.9372525215148926
Epoch 30, training loss: 10.240662574768066 = 1.922450065612793 + 1.0 * 8.318212509155273
Epoch 30, val loss: 1.9131381511688232
Epoch 40, training loss: 9.937578201293945 = 1.902263879776001 + 1.0 * 8.035314559936523
Epoch 40, val loss: 1.888594388961792
Epoch 50, training loss: 9.360642433166504 = 1.8832050561904907 + 1.0 * 7.477437496185303
Epoch 50, val loss: 1.8672775030136108
Epoch 60, training loss: 8.907355308532715 = 1.8654049634933472 + 1.0 * 7.041950225830078
Epoch 60, val loss: 1.8504050970077515
Epoch 70, training loss: 8.595285415649414 = 1.848625898361206 + 1.0 * 6.746659755706787
Epoch 70, val loss: 1.8347012996673584
Epoch 80, training loss: 8.42314624786377 = 1.8352587223052979 + 1.0 * 6.587887763977051
Epoch 80, val loss: 1.822279691696167
Epoch 90, training loss: 8.306726455688477 = 1.822669506072998 + 1.0 * 6.4840569496154785
Epoch 90, val loss: 1.8099596500396729
Epoch 100, training loss: 8.235980987548828 = 1.8100186586380005 + 1.0 * 6.425962448120117
Epoch 100, val loss: 1.79752516746521
Epoch 110, training loss: 8.177080154418945 = 1.7973828315734863 + 1.0 * 6.379696846008301
Epoch 110, val loss: 1.7854758501052856
Epoch 120, training loss: 8.122562408447266 = 1.7856554985046387 + 1.0 * 6.336906909942627
Epoch 120, val loss: 1.7744804620742798
Epoch 130, training loss: 8.07468032836914 = 1.7745797634124756 + 1.0 * 6.300100326538086
Epoch 130, val loss: 1.7643547058105469
Epoch 140, training loss: 8.032464027404785 = 1.7634450197219849 + 1.0 * 6.26901912689209
Epoch 140, val loss: 1.7544108629226685
Epoch 150, training loss: 7.995567798614502 = 1.751313328742981 + 1.0 * 6.2442545890808105
Epoch 150, val loss: 1.7439807653427124
Epoch 160, training loss: 7.9640960693359375 = 1.7373661994934082 + 1.0 * 6.226729869842529
Epoch 160, val loss: 1.7324612140655518
Epoch 170, training loss: 7.930612564086914 = 1.7209928035736084 + 1.0 * 6.209619522094727
Epoch 170, val loss: 1.7193102836608887
Epoch 180, training loss: 7.896981716156006 = 1.7013932466506958 + 1.0 * 6.1955885887146
Epoch 180, val loss: 1.7037990093231201
Epoch 190, training loss: 7.863080024719238 = 1.6776982545852661 + 1.0 * 6.185381889343262
Epoch 190, val loss: 1.6851623058319092
Epoch 200, training loss: 7.823614120483398 = 1.6488405466079712 + 1.0 * 6.174773693084717
Epoch 200, val loss: 1.6625508069992065
Epoch 210, training loss: 7.779818058013916 = 1.6133540868759155 + 1.0 * 6.166463851928711
Epoch 210, val loss: 1.6347492933273315
Epoch 220, training loss: 7.729553699493408 = 1.5702365636825562 + 1.0 * 6.1593170166015625
Epoch 220, val loss: 1.6010926961898804
Epoch 230, training loss: 7.677155017852783 = 1.5196622610092163 + 1.0 * 6.157492637634277
Epoch 230, val loss: 1.5617502927780151
Epoch 240, training loss: 7.614439010620117 = 1.4643458127975464 + 1.0 * 6.150093078613281
Epoch 240, val loss: 1.5191301107406616
Epoch 250, training loss: 7.553328037261963 = 1.407356858253479 + 1.0 * 6.145971298217773
Epoch 250, val loss: 1.4753086566925049
Epoch 260, training loss: 7.493236541748047 = 1.3513963222503662 + 1.0 * 6.141839981079102
Epoch 260, val loss: 1.432586908340454
Epoch 270, training loss: 7.440670013427734 = 1.2998664379119873 + 1.0 * 6.140803813934326
Epoch 270, val loss: 1.3940579891204834
Epoch 280, training loss: 7.388335227966309 = 1.2544794082641602 + 1.0 * 6.133855819702148
Epoch 280, val loss: 1.360490083694458
Epoch 290, training loss: 7.3414812088012695 = 1.2135294675827026 + 1.0 * 6.127951622009277
Epoch 290, val loss: 1.3308172225952148
Epoch 300, training loss: 7.299630165100098 = 1.1758503913879395 + 1.0 * 6.123779773712158
Epoch 300, val loss: 1.3040844202041626
Epoch 310, training loss: 7.263014793395996 = 1.1404660940170288 + 1.0 * 6.122548580169678
Epoch 310, val loss: 1.279348373413086
Epoch 320, training loss: 7.223783493041992 = 1.1064366102218628 + 1.0 * 6.11734676361084
Epoch 320, val loss: 1.2557151317596436
Epoch 330, training loss: 7.1854119300842285 = 1.0721956491470337 + 1.0 * 6.113216400146484
Epoch 330, val loss: 1.2319681644439697
Epoch 340, training loss: 7.151659965515137 = 1.0372711420059204 + 1.0 * 6.114388942718506
Epoch 340, val loss: 1.2074795961380005
Epoch 350, training loss: 7.108944416046143 = 1.0012011528015137 + 1.0 * 6.107743263244629
Epoch 350, val loss: 1.18222975730896
Epoch 360, training loss: 7.067795753479004 = 0.963537871837616 + 1.0 * 6.104258060455322
Epoch 360, val loss: 1.1557284593582153
Epoch 370, training loss: 7.0254034996032715 = 0.924113392829895 + 1.0 * 6.101290225982666
Epoch 370, val loss: 1.127888560295105
Epoch 380, training loss: 6.9822001457214355 = 0.8831297159194946 + 1.0 * 6.0990705490112305
Epoch 380, val loss: 1.0988010168075562
Epoch 390, training loss: 6.946330547332764 = 0.8414521813392639 + 1.0 * 6.1048784255981445
Epoch 390, val loss: 1.0689067840576172
Epoch 400, training loss: 6.897775650024414 = 0.8002564907073975 + 1.0 * 6.0975189208984375
Epoch 400, val loss: 1.0393620729446411
Epoch 410, training loss: 6.852969646453857 = 0.7594764828681946 + 1.0 * 6.0934929847717285
Epoch 410, val loss: 1.010193109512329
Epoch 420, training loss: 6.810431480407715 = 0.7193384766578674 + 1.0 * 6.091093063354492
Epoch 420, val loss: 0.9813924431800842
Epoch 430, training loss: 6.779302597045898 = 0.6801502704620361 + 1.0 * 6.099152565002441
Epoch 430, val loss: 0.9534003138542175
Epoch 440, training loss: 6.72964334487915 = 0.6424551010131836 + 1.0 * 6.087188243865967
Epoch 440, val loss: 0.9268627166748047
Epoch 450, training loss: 6.693104267120361 = 0.6062531471252441 + 1.0 * 6.086851119995117
Epoch 450, val loss: 0.9019529223442078
Epoch 460, training loss: 6.657473564147949 = 0.5714704394340515 + 1.0 * 6.086003303527832
Epoch 460, val loss: 0.8787133097648621
Epoch 470, training loss: 6.621166706085205 = 0.5382606387138367 + 1.0 * 6.082906246185303
Epoch 470, val loss: 0.8574945330619812
Epoch 480, training loss: 6.592497825622559 = 0.5064054727554321 + 1.0 * 6.086092472076416
Epoch 480, val loss: 0.8383148908615112
Epoch 490, training loss: 6.5574421882629395 = 0.4760289490222931 + 1.0 * 6.081413269042969
Epoch 490, val loss: 0.8213980793952942
Epoch 500, training loss: 6.525704383850098 = 0.447377473115921 + 1.0 * 6.07832670211792
Epoch 500, val loss: 0.806828498840332
Epoch 510, training loss: 6.495599746704102 = 0.42010799050331116 + 1.0 * 6.075491905212402
Epoch 510, val loss: 0.7942747473716736
Epoch 520, training loss: 6.481457710266113 = 0.3943873643875122 + 1.0 * 6.087070465087891
Epoch 520, val loss: 0.7837187647819519
Epoch 530, training loss: 6.441911220550537 = 0.37025296688079834 + 1.0 * 6.071658134460449
Epoch 530, val loss: 0.7750640511512756
Epoch 540, training loss: 6.419070720672607 = 0.3476576805114746 + 1.0 * 6.071413040161133
Epoch 540, val loss: 0.7681080102920532
Epoch 550, training loss: 6.401642799377441 = 0.3263991177082062 + 1.0 * 6.0752434730529785
Epoch 550, val loss: 0.7624430656433105
Epoch 560, training loss: 6.378771781921387 = 0.30661171674728394 + 1.0 * 6.072160243988037
Epoch 560, val loss: 0.7580326199531555
Epoch 570, training loss: 6.35433292388916 = 0.28807348012924194 + 1.0 * 6.066259384155273
Epoch 570, val loss: 0.754707396030426
Epoch 580, training loss: 6.335012435913086 = 0.27063629031181335 + 1.0 * 6.064376354217529
Epoch 580, val loss: 0.7522488236427307
Epoch 590, training loss: 6.330209255218506 = 0.2542206048965454 + 1.0 * 6.07598876953125
Epoch 590, val loss: 0.7506229877471924
Epoch 600, training loss: 6.31206750869751 = 0.2391771376132965 + 1.0 * 6.072890281677246
Epoch 600, val loss: 0.7496083974838257
Epoch 610, training loss: 6.288097858428955 = 0.2252848744392395 + 1.0 * 6.062812805175781
Epoch 610, val loss: 0.7495447993278503
Epoch 620, training loss: 6.272857666015625 = 0.21230442821979523 + 1.0 * 6.060553073883057
Epoch 620, val loss: 0.7500350475311279
Epoch 630, training loss: 6.258316993713379 = 0.20016935467720032 + 1.0 * 6.058147430419922
Epoch 630, val loss: 0.7511278390884399
Epoch 640, training loss: 6.245340347290039 = 0.18880215287208557 + 1.0 * 6.056538105010986
Epoch 640, val loss: 0.7528315782546997
Epoch 650, training loss: 6.236850738525391 = 0.17816682159900665 + 1.0 * 6.0586838722229
Epoch 650, val loss: 0.7550947666168213
Epoch 660, training loss: 6.2330803871154785 = 0.1683201640844345 + 1.0 * 6.064760208129883
Epoch 660, val loss: 0.7576931715011597
Epoch 670, training loss: 6.215950965881348 = 0.15926173329353333 + 1.0 * 6.056689262390137
Epoch 670, val loss: 0.7608532309532166
Epoch 680, training loss: 6.204187393188477 = 0.15081886947155 + 1.0 * 6.05336856842041
Epoch 680, val loss: 0.7643958330154419
Epoch 690, training loss: 6.195178985595703 = 0.1429252177476883 + 1.0 * 6.052253723144531
Epoch 690, val loss: 0.7683085799217224
Epoch 700, training loss: 6.191659450531006 = 0.13554911315441132 + 1.0 * 6.056110382080078
Epoch 700, val loss: 0.7726160883903503
Epoch 710, training loss: 6.17882776260376 = 0.1286521553993225 + 1.0 * 6.050175666809082
Epoch 710, val loss: 0.7771775126457214
Epoch 720, training loss: 6.182869911193848 = 0.122256338596344 + 1.0 * 6.060613632202148
Epoch 720, val loss: 0.7820287346839905
Epoch 730, training loss: 6.168617248535156 = 0.11627577990293503 + 1.0 * 6.052341461181641
Epoch 730, val loss: 0.786872386932373
Epoch 740, training loss: 6.15958833694458 = 0.11072900891304016 + 1.0 * 6.048859119415283
Epoch 740, val loss: 0.792071521282196
Epoch 750, training loss: 6.151003360748291 = 0.10550947487354279 + 1.0 * 6.045494079589844
Epoch 750, val loss: 0.7974001169204712
Epoch 760, training loss: 6.14762020111084 = 0.10058654099702835 + 1.0 * 6.047033786773682
Epoch 760, val loss: 0.8028987050056458
Epoch 770, training loss: 6.139848232269287 = 0.09595783054828644 + 1.0 * 6.043890476226807
Epoch 770, val loss: 0.8084039092063904
Epoch 780, training loss: 6.136707305908203 = 0.0916282907128334 + 1.0 * 6.045079231262207
Epoch 780, val loss: 0.8140726685523987
Epoch 790, training loss: 6.130237579345703 = 0.08754204213619232 + 1.0 * 6.04269552230835
Epoch 790, val loss: 0.8198091983795166
Epoch 800, training loss: 6.136516571044922 = 0.08367409557104111 + 1.0 * 6.052842617034912
Epoch 800, val loss: 0.8255624771118164
Epoch 810, training loss: 6.123978614807129 = 0.08006276190280914 + 1.0 * 6.043915748596191
Epoch 810, val loss: 0.8312469124794006
Epoch 820, training loss: 6.116535186767578 = 0.07664206624031067 + 1.0 * 6.03989315032959
Epoch 820, val loss: 0.8370705842971802
Epoch 830, training loss: 6.113428592681885 = 0.07340437173843384 + 1.0 * 6.040024280548096
Epoch 830, val loss: 0.842892050743103
Epoch 840, training loss: 6.115969181060791 = 0.0703454464673996 + 1.0 * 6.045623779296875
Epoch 840, val loss: 0.8486539125442505
Epoch 850, training loss: 6.105863571166992 = 0.06745055317878723 + 1.0 * 6.038413047790527
Epoch 850, val loss: 0.854396641254425
Epoch 860, training loss: 6.101169586181641 = 0.06472823768854141 + 1.0 * 6.036441326141357
Epoch 860, val loss: 0.86018967628479
Epoch 870, training loss: 6.0978288650512695 = 0.06214046850800514 + 1.0 * 6.035688400268555
Epoch 870, val loss: 0.8659664392471313
Epoch 880, training loss: 6.104339599609375 = 0.05968456342816353 + 1.0 * 6.044654846191406
Epoch 880, val loss: 0.8716687560081482
Epoch 890, training loss: 6.099493503570557 = 0.057349711656570435 + 1.0 * 6.042143821716309
Epoch 890, val loss: 0.8772670030593872
Epoch 900, training loss: 6.090862274169922 = 0.0551731213927269 + 1.0 * 6.035689353942871
Epoch 900, val loss: 0.8829120993614197
Epoch 910, training loss: 6.0854668617248535 = 0.05309132859110832 + 1.0 * 6.032375335693359
Epoch 910, val loss: 0.8884875178337097
Epoch 920, training loss: 6.082861423492432 = 0.051107633858919144 + 1.0 * 6.031754016876221
Epoch 920, val loss: 0.8940437436103821
Epoch 930, training loss: 6.092317581176758 = 0.04921717196702957 + 1.0 * 6.043100357055664
Epoch 930, val loss: 0.8995606303215027
Epoch 940, training loss: 6.08159065246582 = 0.047423265874385834 + 1.0 * 6.034167289733887
Epoch 940, val loss: 0.9050168991088867
Epoch 950, training loss: 6.079995155334473 = 0.04572411626577377 + 1.0 * 6.034271240234375
Epoch 950, val loss: 0.9104802012443542
Epoch 960, training loss: 6.073842525482178 = 0.04410742223262787 + 1.0 * 6.029735088348389
Epoch 960, val loss: 0.9157937169075012
Epoch 970, training loss: 6.072822570800781 = 0.04256901517510414 + 1.0 * 6.0302534103393555
Epoch 970, val loss: 0.9211422204971313
Epoch 980, training loss: 6.0751166343688965 = 0.04110250249505043 + 1.0 * 6.0340142250061035
Epoch 980, val loss: 0.9263810515403748
Epoch 990, training loss: 6.069134712219238 = 0.03971457481384277 + 1.0 * 6.029420375823975
Epoch 990, val loss: 0.931553840637207
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.1771
Flip ASR: 0.2000/225 nodes
The final ASR:0.45879, 0.25623, Accuracy:0.80247, 0.02874
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9550])
updated graph: torch.Size([2, 10622])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00920, Accuracy:0.82840, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.334716796875 = 1.9609405994415283 + 1.0 * 8.37377643585205
Epoch 0, val loss: 1.9710842370986938
Epoch 10, training loss: 10.322909355163574 = 1.9498957395553589 + 1.0 * 8.373013496398926
Epoch 10, val loss: 1.959269404411316
Epoch 20, training loss: 10.303400039672852 = 1.9356352090835571 + 1.0 * 8.367764472961426
Epoch 20, val loss: 1.9436918497085571
Epoch 30, training loss: 10.248661041259766 = 1.9155865907669067 + 1.0 * 8.333074569702148
Epoch 30, val loss: 1.9218984842300415
Epoch 40, training loss: 9.994308471679688 = 1.8930188417434692 + 1.0 * 8.101289749145508
Epoch 40, val loss: 1.8992667198181152
Epoch 50, training loss: 9.365813255310059 = 1.8688099384307861 + 1.0 * 7.497003555297852
Epoch 50, val loss: 1.8745094537734985
Epoch 60, training loss: 8.946666717529297 = 1.8491867780685425 + 1.0 * 7.097480297088623
Epoch 60, val loss: 1.8570274114608765
Epoch 70, training loss: 8.569975852966309 = 1.835142731666565 + 1.0 * 6.734832763671875
Epoch 70, val loss: 1.844669222831726
Epoch 80, training loss: 8.396055221557617 = 1.8233305215835571 + 1.0 * 6.572724342346191
Epoch 80, val loss: 1.833836555480957
Epoch 90, training loss: 8.299172401428223 = 1.8096762895584106 + 1.0 * 6.489496231079102
Epoch 90, val loss: 1.8206515312194824
Epoch 100, training loss: 8.218475341796875 = 1.795668125152588 + 1.0 * 6.422807216644287
Epoch 100, val loss: 1.807267427444458
Epoch 110, training loss: 8.156570434570312 = 1.7828407287597656 + 1.0 * 6.373729228973389
Epoch 110, val loss: 1.795316219329834
Epoch 120, training loss: 8.103851318359375 = 1.7709949016571045 + 1.0 * 6.33285665512085
Epoch 120, val loss: 1.7846474647521973
Epoch 130, training loss: 8.057106971740723 = 1.7590736150741577 + 1.0 * 6.298033237457275
Epoch 130, val loss: 1.774472713470459
Epoch 140, training loss: 8.015953063964844 = 1.7460846900939941 + 1.0 * 6.26986837387085
Epoch 140, val loss: 1.7639293670654297
Epoch 150, training loss: 7.976743221282959 = 1.7311244010925293 + 1.0 * 6.24561882019043
Epoch 150, val loss: 1.7522011995315552
Epoch 160, training loss: 7.939160346984863 = 1.7134151458740234 + 1.0 * 6.22574520111084
Epoch 160, val loss: 1.7385553121566772
Epoch 170, training loss: 7.900379657745361 = 1.6922398805618286 + 1.0 * 6.208139896392822
Epoch 170, val loss: 1.7224149703979492
Epoch 180, training loss: 7.860644817352295 = 1.6667728424072266 + 1.0 * 6.193871974945068
Epoch 180, val loss: 1.7030019760131836
Epoch 190, training loss: 7.817526340484619 = 1.6360112428665161 + 1.0 * 6.181515216827393
Epoch 190, val loss: 1.6794123649597168
Epoch 200, training loss: 7.770839691162109 = 1.5989421606063843 + 1.0 * 6.1718974113464355
Epoch 200, val loss: 1.6507477760314941
Epoch 210, training loss: 7.719112396240234 = 1.5546900033950806 + 1.0 * 6.164422512054443
Epoch 210, val loss: 1.6161813735961914
Epoch 220, training loss: 7.660346031188965 = 1.5034332275390625 + 1.0 * 6.156912803649902
Epoch 220, val loss: 1.575901746749878
Epoch 230, training loss: 7.596229076385498 = 1.445557713508606 + 1.0 * 6.150671482086182
Epoch 230, val loss: 1.5301297903060913
Epoch 240, training loss: 7.531516075134277 = 1.3824403285980225 + 1.0 * 6.149075508117676
Epoch 240, val loss: 1.4799708127975464
Epoch 250, training loss: 7.458305358886719 = 1.3175199031829834 + 1.0 * 6.1407856941223145
Epoch 250, val loss: 1.4280413389205933
Epoch 260, training loss: 7.389360427856445 = 1.2520878314971924 + 1.0 * 6.137272834777832
Epoch 260, val loss: 1.3751195669174194
Epoch 270, training loss: 7.321444988250732 = 1.1882396936416626 + 1.0 * 6.133205413818359
Epoch 270, val loss: 1.323180913925171
Epoch 280, training loss: 7.257072448730469 = 1.1275286674499512 + 1.0 * 6.129543781280518
Epoch 280, val loss: 1.2737514972686768
Epoch 290, training loss: 7.198705196380615 = 1.0706418752670288 + 1.0 * 6.128063201904297
Epoch 290, val loss: 1.2274478673934937
Epoch 300, training loss: 7.139640808105469 = 1.017822027206421 + 1.0 * 6.121819019317627
Epoch 300, val loss: 1.184546947479248
Epoch 310, training loss: 7.08583402633667 = 0.9675110578536987 + 1.0 * 6.118322849273682
Epoch 310, val loss: 1.143936276435852
Epoch 320, training loss: 7.0406813621521 = 0.9193888306617737 + 1.0 * 6.121292591094971
Epoch 320, val loss: 1.1055614948272705
Epoch 330, training loss: 6.986116886138916 = 0.8740549087524414 + 1.0 * 6.112061977386475
Epoch 330, val loss: 1.0696264505386353
Epoch 340, training loss: 6.938055515289307 = 0.8300369381904602 + 1.0 * 6.108018398284912
Epoch 340, val loss: 1.035085678100586
Epoch 350, training loss: 6.893695831298828 = 0.7870442867279053 + 1.0 * 6.106651782989502
Epoch 350, val loss: 1.0014570951461792
Epoch 360, training loss: 6.850833415985107 = 0.7457908391952515 + 1.0 * 6.105042457580566
Epoch 360, val loss: 0.9694435596466064
Epoch 370, training loss: 6.805746078491211 = 0.7064534425735474 + 1.0 * 6.099292755126953
Epoch 370, val loss: 0.9394547343254089
Epoch 380, training loss: 6.764887809753418 = 0.668497622013092 + 1.0 * 6.096390247344971
Epoch 380, val loss: 0.9108554124832153
Epoch 390, training loss: 6.734243392944336 = 0.6319301128387451 + 1.0 * 6.10231351852417
Epoch 390, val loss: 0.8837776184082031
Epoch 400, training loss: 6.6901469230651855 = 0.5974289774894714 + 1.0 * 6.092718124389648
Epoch 400, val loss: 0.8588998913764954
Epoch 410, training loss: 6.653323173522949 = 0.5643064975738525 + 1.0 * 6.089016914367676
Epoch 410, val loss: 0.8357460498809814
Epoch 420, training loss: 6.619265079498291 = 0.5322247743606567 + 1.0 * 6.087040424346924
Epoch 420, val loss: 0.8138391375541687
Epoch 430, training loss: 6.595322608947754 = 0.5015279650688171 + 1.0 * 6.093794822692871
Epoch 430, val loss: 0.7934524416923523
Epoch 440, training loss: 6.5548858642578125 = 0.4724256992340088 + 1.0 * 6.082459926605225
Epoch 440, val loss: 0.774816632270813
Epoch 450, training loss: 6.525238037109375 = 0.44443634152412415 + 1.0 * 6.080801486968994
Epoch 450, val loss: 0.7573919892311096
Epoch 460, training loss: 6.497709274291992 = 0.41749727725982666 + 1.0 * 6.080212116241455
Epoch 460, val loss: 0.7410889863967896
Epoch 470, training loss: 6.476104736328125 = 0.3919064700603485 + 1.0 * 6.084198474884033
Epoch 470, val loss: 0.7263071537017822
Epoch 480, training loss: 6.443397045135498 = 0.36781948804855347 + 1.0 * 6.075577735900879
Epoch 480, val loss: 0.712863564491272
Epoch 490, training loss: 6.418435096740723 = 0.3449321389198303 + 1.0 * 6.073503017425537
Epoch 490, val loss: 0.7005808353424072
Epoch 500, training loss: 6.393878936767578 = 0.3233225643634796 + 1.0 * 6.070556163787842
Epoch 500, val loss: 0.6894351243972778
Epoch 510, training loss: 6.373575210571289 = 0.3029727339744568 + 1.0 * 6.0706024169921875
Epoch 510, val loss: 0.6794790029525757
Epoch 520, training loss: 6.359170913696289 = 0.2837805449962616 + 1.0 * 6.075390338897705
Epoch 520, val loss: 0.6707184314727783
Epoch 530, training loss: 6.33317232131958 = 0.2658487856388092 + 1.0 * 6.067323684692383
Epoch 530, val loss: 0.6632096171379089
Epoch 540, training loss: 6.313490867614746 = 0.24900603294372559 + 1.0 * 6.064484596252441
Epoch 540, val loss: 0.6567439436912537
Epoch 550, training loss: 6.313192844390869 = 0.23311558365821838 + 1.0 * 6.080077171325684
Epoch 550, val loss: 0.6513997316360474
Epoch 560, training loss: 6.283392906188965 = 0.21839097142219543 + 1.0 * 6.065001964569092
Epoch 560, val loss: 0.6470625996589661
Epoch 570, training loss: 6.266912460327148 = 0.20459948480129242 + 1.0 * 6.062313079833984
Epoch 570, val loss: 0.6435077786445618
Epoch 580, training loss: 6.250539302825928 = 0.19159381091594696 + 1.0 * 6.058945655822754
Epoch 580, val loss: 0.6408549547195435
Epoch 590, training loss: 6.238996505737305 = 0.17938372492790222 + 1.0 * 6.05961275100708
Epoch 590, val loss: 0.6390709280967712
Epoch 600, training loss: 6.225223064422607 = 0.16800840198993683 + 1.0 * 6.057214736938477
Epoch 600, val loss: 0.637900173664093
Epoch 610, training loss: 6.212234973907471 = 0.15734675526618958 + 1.0 * 6.0548882484436035
Epoch 610, val loss: 0.6374887824058533
Epoch 620, training loss: 6.207971572875977 = 0.14741438627243042 + 1.0 * 6.0605573654174805
Epoch 620, val loss: 0.637714147567749
Epoch 630, training loss: 6.194613933563232 = 0.13825388252735138 + 1.0 * 6.056360244750977
Epoch 630, val loss: 0.6383326649665833
Epoch 640, training loss: 6.180564880371094 = 0.12975628674030304 + 1.0 * 6.050808429718018
Epoch 640, val loss: 0.6393941044807434
Epoch 650, training loss: 6.1738080978393555 = 0.12185203284025192 + 1.0 * 6.0519561767578125
Epoch 650, val loss: 0.6410202383995056
Epoch 660, training loss: 6.168135643005371 = 0.11452911794185638 + 1.0 * 6.0536065101623535
Epoch 660, val loss: 0.6431068778038025
Epoch 670, training loss: 6.158683776855469 = 0.10779812932014465 + 1.0 * 6.0508856773376465
Epoch 670, val loss: 0.6454746723175049
Epoch 680, training loss: 6.148478031158447 = 0.10158023983240128 + 1.0 * 6.046897888183594
Epoch 680, val loss: 0.6480154395103455
Epoch 690, training loss: 6.141491889953613 = 0.09578944742679596 + 1.0 * 6.0457024574279785
Epoch 690, val loss: 0.6509311199188232
Epoch 700, training loss: 6.146639347076416 = 0.09042275696992874 + 1.0 * 6.056216716766357
Epoch 700, val loss: 0.6541470289230347
Epoch 710, training loss: 6.1335930824279785 = 0.08550311625003815 + 1.0 * 6.048089981079102
Epoch 710, val loss: 0.657519519329071
Epoch 720, training loss: 6.128402233123779 = 0.0809711292386055 + 1.0 * 6.047430992126465
Epoch 720, val loss: 0.6609376072883606
Epoch 730, training loss: 6.118139743804932 = 0.07676230370998383 + 1.0 * 6.041377544403076
Epoch 730, val loss: 0.6645469665527344
Epoch 740, training loss: 6.115863800048828 = 0.07283607870340347 + 1.0 * 6.043027877807617
Epoch 740, val loss: 0.668367326259613
Epoch 750, training loss: 6.113003253936768 = 0.06918812543153763 + 1.0 * 6.0438151359558105
Epoch 750, val loss: 0.6724269390106201
Epoch 760, training loss: 6.105193614959717 = 0.06580854207277298 + 1.0 * 6.039384841918945
Epoch 760, val loss: 0.6763012409210205
Epoch 770, training loss: 6.10172700881958 = 0.06265487521886826 + 1.0 * 6.039072036743164
Epoch 770, val loss: 0.6802495121955872
Epoch 780, training loss: 6.105431079864502 = 0.05971074476838112 + 1.0 * 6.045720100402832
Epoch 780, val loss: 0.6845229864120483
Epoch 790, training loss: 6.097012996673584 = 0.05698023736476898 + 1.0 * 6.040032863616943
Epoch 790, val loss: 0.6889084577560425
Epoch 800, training loss: 6.091331958770752 = 0.054442744702100754 + 1.0 * 6.03688907623291
Epoch 800, val loss: 0.6928386092185974
Epoch 810, training loss: 6.087062358856201 = 0.05205848067998886 + 1.0 * 6.035003662109375
Epoch 810, val loss: 0.6970016956329346
Epoch 820, training loss: 6.091010093688965 = 0.04981409013271332 + 1.0 * 6.041195869445801
Epoch 820, val loss: 0.7013772130012512
Epoch 830, training loss: 6.085991859436035 = 0.04771946743130684 + 1.0 * 6.038272380828857
Epoch 830, val loss: 0.7058830261230469
Epoch 840, training loss: 6.084164619445801 = 0.04576141759753227 + 1.0 * 6.038403034210205
Epoch 840, val loss: 0.7099775075912476
Epoch 850, training loss: 6.0759429931640625 = 0.04392305016517639 + 1.0 * 6.032020092010498
Epoch 850, val loss: 0.7141758799552917
Epoch 860, training loss: 6.074814319610596 = 0.04218536987900734 + 1.0 * 6.032629013061523
Epoch 860, val loss: 0.7183780670166016
Epoch 870, training loss: 6.076236248016357 = 0.04054541140794754 + 1.0 * 6.035690784454346
Epoch 870, val loss: 0.722781777381897
Epoch 880, training loss: 6.07048225402832 = 0.03900910168886185 + 1.0 * 6.031473159790039
Epoch 880, val loss: 0.7270938158035278
Epoch 890, training loss: 6.067553997039795 = 0.037559181451797485 + 1.0 * 6.029994964599609
Epoch 890, val loss: 0.7310687303543091
Epoch 900, training loss: 6.073383331298828 = 0.036184243857860565 + 1.0 * 6.037199020385742
Epoch 900, val loss: 0.7353910803794861
Epoch 910, training loss: 6.06725549697876 = 0.034885603934526443 + 1.0 * 6.032370090484619
Epoch 910, val loss: 0.7397685647010803
Epoch 920, training loss: 6.06130313873291 = 0.03366469964385033 + 1.0 * 6.0276384353637695
Epoch 920, val loss: 0.7436196804046631
Epoch 930, training loss: 6.059648513793945 = 0.03249756246805191 + 1.0 * 6.027151107788086
Epoch 930, val loss: 0.7477129697799683
Epoch 940, training loss: 6.062353610992432 = 0.03139016777276993 + 1.0 * 6.03096342086792
Epoch 940, val loss: 0.752143383026123
Epoch 950, training loss: 6.056817054748535 = 0.030348047614097595 + 1.0 * 6.0264692306518555
Epoch 950, val loss: 0.7561251521110535
Epoch 960, training loss: 6.066009998321533 = 0.02935807779431343 + 1.0 * 6.036652088165283
Epoch 960, val loss: 0.7599982023239136
Epoch 970, training loss: 6.056934833526611 = 0.028423331677913666 + 1.0 * 6.0285115242004395
Epoch 970, val loss: 0.7641192078590393
Epoch 980, training loss: 6.051906108856201 = 0.027534769847989082 + 1.0 * 6.024371147155762
Epoch 980, val loss: 0.7677519917488098
Epoch 990, training loss: 6.049632549285889 = 0.026682049036026 + 1.0 * 6.022950649261475
Epoch 990, val loss: 0.7715699672698975
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.5018
Flip ASR: 0.4044/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.324206352233887 = 1.9504940509796143 + 1.0 * 8.373712539672852
Epoch 0, val loss: 1.9569473266601562
Epoch 10, training loss: 10.31186580657959 = 1.9401973485946655 + 1.0 * 8.371668815612793
Epoch 10, val loss: 1.944824457168579
Epoch 20, training loss: 10.290542602539062 = 1.927699089050293 + 1.0 * 8.36284351348877
Epoch 20, val loss: 1.9293195009231567
Epoch 30, training loss: 10.243327140808105 = 1.9110136032104492 + 1.0 * 8.332313537597656
Epoch 30, val loss: 1.908219575881958
Epoch 40, training loss: 10.042033195495605 = 1.8915928602218628 + 1.0 * 8.150440216064453
Epoch 40, val loss: 1.884440302848816
Epoch 50, training loss: 9.405319213867188 = 1.8726876974105835 + 1.0 * 7.532631874084473
Epoch 50, val loss: 1.861950159072876
Epoch 60, training loss: 9.031997680664062 = 1.851905345916748 + 1.0 * 7.180091857910156
Epoch 60, val loss: 1.8402173519134521
Epoch 70, training loss: 8.783504486083984 = 1.8330086469650269 + 1.0 * 6.950495719909668
Epoch 70, val loss: 1.8214179277420044
Epoch 80, training loss: 8.569801330566406 = 1.8167724609375 + 1.0 * 6.753028392791748
Epoch 80, val loss: 1.8054609298706055
Epoch 90, training loss: 8.453516006469727 = 1.8039149045944214 + 1.0 * 6.649600982666016
Epoch 90, val loss: 1.7921210527420044
Epoch 100, training loss: 8.35690975189209 = 1.789954423904419 + 1.0 * 6.566955089569092
Epoch 100, val loss: 1.777626872062683
Epoch 110, training loss: 8.25958251953125 = 1.7764506340026855 + 1.0 * 6.4831318855285645
Epoch 110, val loss: 1.7643020153045654
Epoch 120, training loss: 8.18201732635498 = 1.7639663219451904 + 1.0 * 6.418051242828369
Epoch 120, val loss: 1.7520400285720825
Epoch 130, training loss: 8.122133255004883 = 1.750432014465332 + 1.0 * 6.371701240539551
Epoch 130, val loss: 1.7389945983886719
Epoch 140, training loss: 8.06925106048584 = 1.7347482442855835 + 1.0 * 6.334503173828125
Epoch 140, val loss: 1.7249534130096436
Epoch 150, training loss: 8.018938064575195 = 1.71631920337677 + 1.0 * 6.302618980407715
Epoch 150, val loss: 1.7092937231063843
Epoch 160, training loss: 7.972232818603516 = 1.6943652629852295 + 1.0 * 6.277867794036865
Epoch 160, val loss: 1.6914907693862915
Epoch 170, training loss: 7.924891471862793 = 1.667914628982544 + 1.0 * 6.25697660446167
Epoch 170, val loss: 1.6703940629959106
Epoch 180, training loss: 7.876155853271484 = 1.6351004838943481 + 1.0 * 6.241055488586426
Epoch 180, val loss: 1.6442781686782837
Epoch 190, training loss: 7.821773529052734 = 1.5950886011123657 + 1.0 * 6.226685047149658
Epoch 190, val loss: 1.612302303314209
Epoch 200, training loss: 7.762388706207275 = 1.5473095178604126 + 1.0 * 6.215079307556152
Epoch 200, val loss: 1.5741900205612183
Epoch 210, training loss: 7.698321342468262 = 1.4928419589996338 + 1.0 * 6.205479145050049
Epoch 210, val loss: 1.5311951637268066
Epoch 220, training loss: 7.630794525146484 = 1.4336965084075928 + 1.0 * 6.1970977783203125
Epoch 220, val loss: 1.4843220710754395
Epoch 230, training loss: 7.561272144317627 = 1.3715291023254395 + 1.0 * 6.1897430419921875
Epoch 230, val loss: 1.4356589317321777
Epoch 240, training loss: 7.493260860443115 = 1.3089765310287476 + 1.0 * 6.184284210205078
Epoch 240, val loss: 1.3870140314102173
Epoch 250, training loss: 7.4236226081848145 = 1.2475295066833496 + 1.0 * 6.176093101501465
Epoch 250, val loss: 1.339684247970581
Epoch 260, training loss: 7.359221935272217 = 1.1876411437988281 + 1.0 * 6.171580791473389
Epoch 260, val loss: 1.2938995361328125
Epoch 270, training loss: 7.293551445007324 = 1.1299865245819092 + 1.0 * 6.163564682006836
Epoch 270, val loss: 1.250084400177002
Epoch 280, training loss: 7.231222629547119 = 1.0739985704421997 + 1.0 * 6.157224178314209
Epoch 280, val loss: 1.2076233625411987
Epoch 290, training loss: 7.172614574432373 = 1.020177960395813 + 1.0 * 6.15243673324585
Epoch 290, val loss: 1.1669431924819946
Epoch 300, training loss: 7.117076396942139 = 0.9694955945014954 + 1.0 * 6.147580623626709
Epoch 300, val loss: 1.1286991834640503
Epoch 310, training loss: 7.0635857582092285 = 0.921469509601593 + 1.0 * 6.142116069793701
Epoch 310, val loss: 1.0927612781524658
Epoch 320, training loss: 7.01636266708374 = 0.876064658164978 + 1.0 * 6.140297889709473
Epoch 320, val loss: 1.0590282678604126
Epoch 330, training loss: 6.969676971435547 = 0.8336767554283142 + 1.0 * 6.136000156402588
Epoch 330, val loss: 1.0280786752700806
Epoch 340, training loss: 6.92482328414917 = 0.7941679954528809 + 1.0 * 6.130655288696289
Epoch 340, val loss: 0.9996675848960876
Epoch 350, training loss: 6.882078170776367 = 0.756912112236023 + 1.0 * 6.125165939331055
Epoch 350, val loss: 0.9734276533126831
Epoch 360, training loss: 6.846861362457275 = 0.7217546105384827 + 1.0 * 6.1251068115234375
Epoch 360, val loss: 0.9492745995521545
Epoch 370, training loss: 6.8104023933410645 = 0.6890825629234314 + 1.0 * 6.121319770812988
Epoch 370, val loss: 0.9276261329650879
Epoch 380, training loss: 6.77337646484375 = 0.658808171749115 + 1.0 * 6.11456823348999
Epoch 380, val loss: 0.9084686040878296
Epoch 390, training loss: 6.742718696594238 = 0.6303696632385254 + 1.0 * 6.112349033355713
Epoch 390, val loss: 0.891193151473999
Epoch 400, training loss: 6.713895797729492 = 0.6037407517433167 + 1.0 * 6.11015510559082
Epoch 400, val loss: 0.8756802678108215
Epoch 410, training loss: 6.6832733154296875 = 0.5786876678466797 + 1.0 * 6.104585647583008
Epoch 410, val loss: 0.8619725704193115
Epoch 420, training loss: 6.667455673217773 = 0.55477374792099 + 1.0 * 6.112681865692139
Epoch 420, val loss: 0.849484384059906
Epoch 430, training loss: 6.63494873046875 = 0.5320940613746643 + 1.0 * 6.1028547286987305
Epoch 430, val loss: 0.838377058506012
Epoch 440, training loss: 6.608126163482666 = 0.5103893876075745 + 1.0 * 6.097736835479736
Epoch 440, val loss: 0.8284329175949097
Epoch 450, training loss: 6.5849103927612305 = 0.48930320143699646 + 1.0 * 6.095607280731201
Epoch 450, val loss: 0.8192221522331238
Epoch 460, training loss: 6.570294380187988 = 0.468936562538147 + 1.0 * 6.101357936859131
Epoch 460, val loss: 0.8108270168304443
Epoch 470, training loss: 6.542510032653809 = 0.44947099685668945 + 1.0 * 6.093039035797119
Epoch 470, val loss: 0.8033981919288635
Epoch 480, training loss: 6.519233226776123 = 0.43056219816207886 + 1.0 * 6.0886712074279785
Epoch 480, val loss: 0.796493649482727
Epoch 490, training loss: 6.498348236083984 = 0.41215887665748596 + 1.0 * 6.086189270019531
Epoch 490, val loss: 0.7902761101722717
Epoch 500, training loss: 6.4893598556518555 = 0.394316166639328 + 1.0 * 6.095043659210205
Epoch 500, val loss: 0.7845798134803772
Epoch 510, training loss: 6.463442325592041 = 0.37723308801651 + 1.0 * 6.086209297180176
Epoch 510, val loss: 0.7796329855918884
Epoch 520, training loss: 6.442985534667969 = 0.3607025742530823 + 1.0 * 6.082283020019531
Epoch 520, val loss: 0.7751561403274536
Epoch 530, training loss: 6.435006141662598 = 0.34466928243637085 + 1.0 * 6.090336799621582
Epoch 530, val loss: 0.7710031867027283
Epoch 540, training loss: 6.410121917724609 = 0.3292272686958313 + 1.0 * 6.080894470214844
Epoch 540, val loss: 0.7672821879386902
Epoch 550, training loss: 6.391915798187256 = 0.3141908049583435 + 1.0 * 6.077724933624268
Epoch 550, val loss: 0.7639160752296448
Epoch 560, training loss: 6.3829264640808105 = 0.2995034158229828 + 1.0 * 6.083423137664795
Epoch 560, val loss: 0.7608020901679993
Epoch 570, training loss: 6.362019062042236 = 0.28522616624832153 + 1.0 * 6.0767927169799805
Epoch 570, val loss: 0.7579773664474487
Epoch 580, training loss: 6.344120502471924 = 0.27133145928382874 + 1.0 * 6.072789192199707
Epoch 580, val loss: 0.7554146647453308
Epoch 590, training loss: 6.330672264099121 = 0.25767311453819275 + 1.0 * 6.072999000549316
Epoch 590, val loss: 0.7530785202980042
Epoch 600, training loss: 6.321288585662842 = 0.24444541335105896 + 1.0 * 6.07684326171875
Epoch 600, val loss: 0.7510648369789124
Epoch 610, training loss: 6.303581714630127 = 0.2318088412284851 + 1.0 * 6.071773052215576
Epoch 610, val loss: 0.7493273615837097
Epoch 620, training loss: 6.287967681884766 = 0.21968068182468414 + 1.0 * 6.068286895751953
Epoch 620, val loss: 0.7478663325309753
Epoch 630, training loss: 6.282620429992676 = 0.20806628465652466 + 1.0 * 6.074553966522217
Epoch 630, val loss: 0.7467449903488159
Epoch 640, training loss: 6.2661542892456055 = 0.19705672562122345 + 1.0 * 6.069097518920898
Epoch 640, val loss: 0.7459379434585571
Epoch 650, training loss: 6.255394458770752 = 0.18659256398677826 + 1.0 * 6.0688018798828125
Epoch 650, val loss: 0.7455042600631714
Epoch 660, training loss: 6.240771293640137 = 0.17671409249305725 + 1.0 * 6.064057350158691
Epoch 660, val loss: 0.7454175353050232
Epoch 670, training loss: 6.231741428375244 = 0.1673678755760193 + 1.0 * 6.06437349319458
Epoch 670, val loss: 0.7456814050674438
Epoch 680, training loss: 6.225001335144043 = 0.1585269570350647 + 1.0 * 6.066474437713623
Epoch 680, val loss: 0.746250569820404
Epoch 690, training loss: 6.211830139160156 = 0.15024793148040771 + 1.0 * 6.061582088470459
Epoch 690, val loss: 0.7470549941062927
Epoch 700, training loss: 6.2018046379089355 = 0.1424548178911209 + 1.0 * 6.05935001373291
Epoch 700, val loss: 0.7482149600982666
Epoch 710, training loss: 6.194295406341553 = 0.1351061463356018 + 1.0 * 6.059189319610596
Epoch 710, val loss: 0.7495893239974976
Epoch 720, training loss: 6.2001824378967285 = 0.12818893790245056 + 1.0 * 6.071993350982666
Epoch 720, val loss: 0.7511619925498962
Epoch 730, training loss: 6.178928852081299 = 0.12176485359668732 + 1.0 * 6.057164192199707
Epoch 730, val loss: 0.7529042363166809
Epoch 740, training loss: 6.171087265014648 = 0.11570407450199127 + 1.0 * 6.055383205413818
Epoch 740, val loss: 0.7548365592956543
Epoch 750, training loss: 6.163550853729248 = 0.10996497422456741 + 1.0 * 6.053586006164551
Epoch 750, val loss: 0.7569730877876282
Epoch 760, training loss: 6.159745216369629 = 0.10452900826931 + 1.0 * 6.055216312408447
Epoch 760, val loss: 0.7592765688896179
Epoch 770, training loss: 6.153680324554443 = 0.09942188858985901 + 1.0 * 6.054258346557617
Epoch 770, val loss: 0.7616193294525146
Epoch 780, training loss: 6.147231578826904 = 0.09461794048547745 + 1.0 * 6.052613735198975
Epoch 780, val loss: 0.7641786336898804
Epoch 790, training loss: 6.14074182510376 = 0.09007292985916138 + 1.0 * 6.050668716430664
Epoch 790, val loss: 0.766869306564331
Epoch 800, training loss: 6.135153293609619 = 0.08576283603906631 + 1.0 * 6.0493903160095215
Epoch 800, val loss: 0.7696927785873413
Epoch 810, training loss: 6.131316661834717 = 0.08165434002876282 + 1.0 * 6.049662113189697
Epoch 810, val loss: 0.7726444005966187
Epoch 820, training loss: 6.1288933753967285 = 0.07778300344944 + 1.0 * 6.05111026763916
Epoch 820, val loss: 0.7755902409553528
Epoch 830, training loss: 6.121072292327881 = 0.0741676613688469 + 1.0 * 6.046904563903809
Epoch 830, val loss: 0.778624415397644
Epoch 840, training loss: 6.116345405578613 = 0.07075730711221695 + 1.0 * 6.04558801651001
Epoch 840, val loss: 0.7817478775978088
Epoch 850, training loss: 6.112536907196045 = 0.06752360612154007 + 1.0 * 6.045013427734375
Epoch 850, val loss: 0.7849628329277039
Epoch 860, training loss: 6.120144367218018 = 0.0644620954990387 + 1.0 * 6.055682182312012
Epoch 860, val loss: 0.7882355451583862
Epoch 870, training loss: 6.109318733215332 = 0.06157887727022171 + 1.0 * 6.0477399826049805
Epoch 870, val loss: 0.7914837598800659
Epoch 880, training loss: 6.102643966674805 = 0.05886336416006088 + 1.0 * 6.04378080368042
Epoch 880, val loss: 0.7948557734489441
Epoch 890, training loss: 6.1030402183532715 = 0.05629589036107063 + 1.0 * 6.046744346618652
Epoch 890, val loss: 0.7982666492462158
Epoch 900, training loss: 6.0994157791137695 = 0.053870949894189835 + 1.0 * 6.045544624328613
Epoch 900, val loss: 0.8016694188117981
Epoch 910, training loss: 6.098170280456543 = 0.051573410630226135 + 1.0 * 6.046597003936768
Epoch 910, val loss: 0.8051212430000305
Epoch 920, training loss: 6.090621471405029 = 0.049411170184612274 + 1.0 * 6.041210174560547
Epoch 920, val loss: 0.8085601925849915
Epoch 930, training loss: 6.087635517120361 = 0.04737038537859917 + 1.0 * 6.040265083312988
Epoch 930, val loss: 0.8120673298835754
Epoch 940, training loss: 6.084304332733154 = 0.04543483629822731 + 1.0 * 6.038869380950928
Epoch 940, val loss: 0.8155907988548279
Epoch 950, training loss: 6.090975761413574 = 0.04359928518533707 + 1.0 * 6.04737663269043
Epoch 950, val loss: 0.8191390037536621
Epoch 960, training loss: 6.0874342918396 = 0.04187842085957527 + 1.0 * 6.04555606842041
Epoch 960, val loss: 0.8225571513175964
Epoch 970, training loss: 6.076511383056641 = 0.04025498405098915 + 1.0 * 6.036256313323975
Epoch 970, val loss: 0.8260339498519897
Epoch 980, training loss: 6.075300216674805 = 0.03871527686715126 + 1.0 * 6.036584854125977
Epoch 980, val loss: 0.829560399055481
Epoch 990, training loss: 6.072149276733398 = 0.03725024685263634 + 1.0 * 6.0348992347717285
Epoch 990, val loss: 0.8330757021903992
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.1771
Flip ASR: 0.2000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.322213172912598 = 1.9485417604446411 + 1.0 * 8.373671531677246
Epoch 0, val loss: 1.9434411525726318
Epoch 10, training loss: 10.310641288757324 = 1.9380587339401245 + 1.0 * 8.37258243560791
Epoch 10, val loss: 1.9325902462005615
Epoch 20, training loss: 10.290376663208008 = 1.9249627590179443 + 1.0 * 8.365413665771484
Epoch 20, val loss: 1.9189265966415405
Epoch 30, training loss: 10.225102424621582 = 1.907166600227356 + 1.0 * 8.317935943603516
Epoch 30, val loss: 1.9006226062774658
Epoch 40, training loss: 9.876853942871094 = 1.887553334236145 + 1.0 * 7.98930025100708
Epoch 40, val loss: 1.881790280342102
Epoch 50, training loss: 9.195510864257812 = 1.8646513223648071 + 1.0 * 7.330859661102295
Epoch 50, val loss: 1.8595709800720215
Epoch 60, training loss: 8.790929794311523 = 1.8483686447143555 + 1.0 * 6.942561626434326
Epoch 60, val loss: 1.8446288108825684
Epoch 70, training loss: 8.530207633972168 = 1.8366410732269287 + 1.0 * 6.693566799163818
Epoch 70, val loss: 1.8332490921020508
Epoch 80, training loss: 8.393346786499023 = 1.8247164487838745 + 1.0 * 6.568630218505859
Epoch 80, val loss: 1.8221149444580078
Epoch 90, training loss: 8.278046607971191 = 1.8125404119491577 + 1.0 * 6.465506553649902
Epoch 90, val loss: 1.8110958337783813
Epoch 100, training loss: 8.199742317199707 = 1.8007780313491821 + 1.0 * 6.398963928222656
Epoch 100, val loss: 1.8009361028671265
Epoch 110, training loss: 8.144954681396484 = 1.790043592453003 + 1.0 * 6.354910850524902
Epoch 110, val loss: 1.7918663024902344
Epoch 120, training loss: 8.098443984985352 = 1.7800374031066895 + 1.0 * 6.31840705871582
Epoch 120, val loss: 1.7835203409194946
Epoch 130, training loss: 8.05675983428955 = 1.7697151899337769 + 1.0 * 6.287044525146484
Epoch 130, val loss: 1.774923324584961
Epoch 140, training loss: 8.01878833770752 = 1.7581788301467896 + 1.0 * 6.260609149932861
Epoch 140, val loss: 1.765388011932373
Epoch 150, training loss: 7.9823784828186035 = 1.7449331283569336 + 1.0 * 6.23744535446167
Epoch 150, val loss: 1.7546032667160034
Epoch 160, training loss: 7.948312759399414 = 1.7291810512542725 + 1.0 * 6.219131946563721
Epoch 160, val loss: 1.7420642375946045
Epoch 170, training loss: 7.914185523986816 = 1.7101439237594604 + 1.0 * 6.204041481018066
Epoch 170, val loss: 1.7271499633789062
Epoch 180, training loss: 7.878566265106201 = 1.6869045495986938 + 1.0 * 6.191661834716797
Epoch 180, val loss: 1.7091070413589478
Epoch 190, training loss: 7.840010166168213 = 1.6587557792663574 + 1.0 * 6.1812543869018555
Epoch 190, val loss: 1.6872328519821167
Epoch 200, training loss: 7.796255111694336 = 1.6249010562896729 + 1.0 * 6.171353816986084
Epoch 200, val loss: 1.6607871055603027
Epoch 210, training loss: 7.747132778167725 = 1.5843106508255005 + 1.0 * 6.162822246551514
Epoch 210, val loss: 1.6287915706634521
Epoch 220, training loss: 7.693148612976074 = 1.536466360092163 + 1.0 * 6.156682014465332
Epoch 220, val loss: 1.590946078300476
Epoch 230, training loss: 7.6323747634887695 = 1.4829943180084229 + 1.0 * 6.149380207061768
Epoch 230, val loss: 1.5488132238388062
Epoch 240, training loss: 7.5692620277404785 = 1.4249612092971802 + 1.0 * 6.144300937652588
Epoch 240, val loss: 1.502982497215271
Epoch 250, training loss: 7.5073933601379395 = 1.363576054573059 + 1.0 * 6.14381742477417
Epoch 250, val loss: 1.4546308517456055
Epoch 260, training loss: 7.438013553619385 = 1.3017621040344238 + 1.0 * 6.136251449584961
Epoch 260, val loss: 1.4058647155761719
Epoch 270, training loss: 7.3706560134887695 = 1.2396557331085205 + 1.0 * 6.13100004196167
Epoch 270, val loss: 1.3565090894699097
Epoch 280, training loss: 7.308379173278809 = 1.1776063442230225 + 1.0 * 6.130773067474365
Epoch 280, val loss: 1.307184100151062
Epoch 290, training loss: 7.241938591003418 = 1.1177340745925903 + 1.0 * 6.124204635620117
Epoch 290, val loss: 1.2596502304077148
Epoch 300, training loss: 7.180503845214844 = 1.0598599910736084 + 1.0 * 6.120643615722656
Epoch 300, val loss: 1.2137939929962158
Epoch 310, training loss: 7.119607925415039 = 1.004084825515747 + 1.0 * 6.115522861480713
Epoch 310, val loss: 1.1698449850082397
Epoch 320, training loss: 7.062510967254639 = 0.9504103064537048 + 1.0 * 6.112100601196289
Epoch 320, val loss: 1.127907395362854
Epoch 330, training loss: 7.010831356048584 = 0.8995925784111023 + 1.0 * 6.111238956451416
Epoch 330, val loss: 1.0884069204330444
Epoch 340, training loss: 6.957551956176758 = 0.8519240617752075 + 1.0 * 6.10562801361084
Epoch 340, val loss: 1.051880955696106
Epoch 350, training loss: 6.912354946136475 = 0.8073254227638245 + 1.0 * 6.105029582977295
Epoch 350, val loss: 1.0178625583648682
Epoch 360, training loss: 6.866369247436523 = 0.7660832405090332 + 1.0 * 6.10028600692749
Epoch 360, val loss: 0.9870752692222595
Epoch 370, training loss: 6.823816776275635 = 0.727590024471283 + 1.0 * 6.096226692199707
Epoch 370, val loss: 0.9586865305900574
Epoch 380, training loss: 6.786957740783691 = 0.6919540166854858 + 1.0 * 6.095003604888916
Epoch 380, val loss: 0.9329752326011658
Epoch 390, training loss: 6.752182960510254 = 0.6596004962921143 + 1.0 * 6.092582702636719
Epoch 390, val loss: 0.9104505181312561
Epoch 400, training loss: 6.718340873718262 = 0.629491925239563 + 1.0 * 6.088849067687988
Epoch 400, val loss: 0.8902420997619629
Epoch 410, training loss: 6.686583042144775 = 0.6011872291564941 + 1.0 * 6.085395812988281
Epoch 410, val loss: 0.8719155192375183
Epoch 420, training loss: 6.670593738555908 = 0.574499249458313 + 1.0 * 6.096094608306885
Epoch 420, val loss: 0.855512261390686
Epoch 430, training loss: 6.631161689758301 = 0.5497156381607056 + 1.0 * 6.081446170806885
Epoch 430, val loss: 0.8409461379051208
Epoch 440, training loss: 6.605959415435791 = 0.5263123512268066 + 1.0 * 6.079647064208984
Epoch 440, val loss: 0.8281003832817078
Epoch 450, training loss: 6.581844806671143 = 0.5039049386978149 + 1.0 * 6.077939987182617
Epoch 450, val loss: 0.8163625001907349
Epoch 460, training loss: 6.557857036590576 = 0.48244601488113403 + 1.0 * 6.075410842895508
Epoch 460, val loss: 0.8059282302856445
Epoch 470, training loss: 6.539373874664307 = 0.46189072728157043 + 1.0 * 6.077483177185059
Epoch 470, val loss: 0.7965996861457825
Epoch 480, training loss: 6.514652252197266 = 0.44208186864852905 + 1.0 * 6.072570323944092
Epoch 480, val loss: 0.7882246971130371
Epoch 490, training loss: 6.491884708404541 = 0.42281150817871094 + 1.0 * 6.06907320022583
Epoch 490, val loss: 0.7806968688964844
Epoch 500, training loss: 6.475968360900879 = 0.4039452075958252 + 1.0 * 6.072022914886475
Epoch 500, val loss: 0.7738062143325806
Epoch 510, training loss: 6.458208084106445 = 0.38567689061164856 + 1.0 * 6.072531223297119
Epoch 510, val loss: 0.7674879431724548
Epoch 520, training loss: 6.433961391448975 = 0.3679549992084503 + 1.0 * 6.066006183624268
Epoch 520, val loss: 0.7619476318359375
Epoch 530, training loss: 6.414677619934082 = 0.3506203889846802 + 1.0 * 6.064057350158691
Epoch 530, val loss: 0.7567827105522156
Epoch 540, training loss: 6.402638912200928 = 0.33370858430862427 + 1.0 * 6.068930149078369
Epoch 540, val loss: 0.7521457672119141
Epoch 550, training loss: 6.379555702209473 = 0.3173777163028717 + 1.0 * 6.062178134918213
Epoch 550, val loss: 0.7481547594070435
Epoch 560, training loss: 6.361903190612793 = 0.30150002241134644 + 1.0 * 6.060403347015381
Epoch 560, val loss: 0.7446702122688293
Epoch 570, training loss: 6.347509384155273 = 0.2861612141132355 + 1.0 * 6.061347961425781
Epoch 570, val loss: 0.7417651414871216
Epoch 580, training loss: 6.336449146270752 = 0.2714192271232605 + 1.0 * 6.065030097961426
Epoch 580, val loss: 0.7395407557487488
Epoch 590, training loss: 6.316274166107178 = 0.25732919573783875 + 1.0 * 6.058945178985596
Epoch 590, val loss: 0.7379910349845886
Epoch 600, training loss: 6.299065589904785 = 0.24374811351299286 + 1.0 * 6.055317401885986
Epoch 600, val loss: 0.7371112704277039
Epoch 610, training loss: 6.293829917907715 = 0.23068861663341522 + 1.0 * 6.063141345977783
Epoch 610, val loss: 0.7369139790534973
Epoch 620, training loss: 6.276291370391846 = 0.21823929250240326 + 1.0 * 6.058052062988281
Epoch 620, val loss: 0.7373765110969543
Epoch 630, training loss: 6.259790420532227 = 0.20638218522071838 + 1.0 * 6.053408145904541
Epoch 630, val loss: 0.7386922240257263
Epoch 640, training loss: 6.246480941772461 = 0.19505371153354645 + 1.0 * 6.051427364349365
Epoch 640, val loss: 0.7406051158905029
Epoch 650, training loss: 6.253271579742432 = 0.18426266312599182 + 1.0 * 6.069008827209473
Epoch 650, val loss: 0.7430846691131592
Epoch 660, training loss: 6.226765155792236 = 0.1742527335882187 + 1.0 * 6.0525126457214355
Epoch 660, val loss: 0.7460437417030334
Epoch 670, training loss: 6.2151970863342285 = 0.16485930979251862 + 1.0 * 6.050337791442871
Epoch 670, val loss: 0.7497245669364929
Epoch 680, training loss: 6.204444885253906 = 0.15602873265743256 + 1.0 * 6.0484161376953125
Epoch 680, val loss: 0.7536090016365051
Epoch 690, training loss: 6.197809219360352 = 0.14780046045780182 + 1.0 * 6.050008773803711
Epoch 690, val loss: 0.7577610015869141
Epoch 700, training loss: 6.187206745147705 = 0.14020130038261414 + 1.0 * 6.047005653381348
Epoch 700, val loss: 0.7624604105949402
Epoch 710, training loss: 6.178123474121094 = 0.13311012089252472 + 1.0 * 6.045013427734375
Epoch 710, val loss: 0.7674668431282043
Epoch 720, training loss: 6.170555114746094 = 0.12646952271461487 + 1.0 * 6.044085502624512
Epoch 720, val loss: 0.7726351022720337
Epoch 730, training loss: 6.1653733253479 = 0.12024278193712234 + 1.0 * 6.045130729675293
Epoch 730, val loss: 0.77802973985672
Epoch 740, training loss: 6.159570217132568 = 0.1144510954618454 + 1.0 * 6.045119285583496
Epoch 740, val loss: 0.7834349274635315
Epoch 750, training loss: 6.152409553527832 = 0.10906177014112473 + 1.0 * 6.0433478355407715
Epoch 750, val loss: 0.7890816926956177
Epoch 760, training loss: 6.146511554718018 = 0.10400733351707458 + 1.0 * 6.04250431060791
Epoch 760, val loss: 0.7947956919670105
Epoch 770, training loss: 6.1398539543151855 = 0.09924931824207306 + 1.0 * 6.040604591369629
Epoch 770, val loss: 0.8005297183990479
Epoch 780, training loss: 6.141872882843018 = 0.09478078037500381 + 1.0 * 6.047091960906982
Epoch 780, val loss: 0.8063381314277649
Epoch 790, training loss: 6.131162166595459 = 0.09058172255754471 + 1.0 * 6.0405802726745605
Epoch 790, val loss: 0.8121948838233948
Epoch 800, training loss: 6.126365661621094 = 0.08663572371006012 + 1.0 * 6.039730072021484
Epoch 800, val loss: 0.8181928992271423
Epoch 810, training loss: 6.120682239532471 = 0.08291691541671753 + 1.0 * 6.0377655029296875
Epoch 810, val loss: 0.8240954875946045
Epoch 820, training loss: 6.119553565979004 = 0.0794074758887291 + 1.0 * 6.0401458740234375
Epoch 820, val loss: 0.8301165699958801
Epoch 830, training loss: 6.113105297088623 = 0.07610198110342026 + 1.0 * 6.037003517150879
Epoch 830, val loss: 0.8359742760658264
Epoch 840, training loss: 6.107964992523193 = 0.07298023998737335 + 1.0 * 6.034984588623047
Epoch 840, val loss: 0.8420931100845337
Epoch 850, training loss: 6.104668140411377 = 0.07001965492963791 + 1.0 * 6.034648418426514
Epoch 850, val loss: 0.8481523990631104
Epoch 860, training loss: 6.109724044799805 = 0.06721214205026627 + 1.0 * 6.042511940002441
Epoch 860, val loss: 0.8539566397666931
Epoch 870, training loss: 6.098177433013916 = 0.06457266211509705 + 1.0 * 6.033604621887207
Epoch 870, val loss: 0.8599048256874084
Epoch 880, training loss: 6.098352909088135 = 0.06207176297903061 + 1.0 * 6.036281108856201
Epoch 880, val loss: 0.8659651279449463
Epoch 890, training loss: 6.097147464752197 = 0.05969931185245514 + 1.0 * 6.037447929382324
Epoch 890, val loss: 0.8716846704483032
Epoch 900, training loss: 6.089882850646973 = 0.0574529692530632 + 1.0 * 6.0324296951293945
Epoch 900, val loss: 0.8775001168251038
Epoch 910, training loss: 6.086629390716553 = 0.05531960725784302 + 1.0 * 6.031309604644775
Epoch 910, val loss: 0.8834286332130432
Epoch 920, training loss: 6.082820415496826 = 0.05328044295310974 + 1.0 * 6.029540061950684
Epoch 920, val loss: 0.8891404271125793
Epoch 930, training loss: 6.087944507598877 = 0.051333069801330566 + 1.0 * 6.036611557006836
Epoch 930, val loss: 0.8947770595550537
Epoch 940, training loss: 6.083344459533691 = 0.04948779195547104 + 1.0 * 6.0338568687438965
Epoch 940, val loss: 0.9003106951713562
Epoch 950, training loss: 6.077601432800293 = 0.04773227497935295 + 1.0 * 6.029869079589844
Epoch 950, val loss: 0.9060540795326233
Epoch 960, training loss: 6.073681831359863 = 0.04605533927679062 + 1.0 * 6.0276265144348145
Epoch 960, val loss: 0.9116630554199219
Epoch 970, training loss: 6.076045513153076 = 0.04444952309131622 + 1.0 * 6.0315961837768555
Epoch 970, val loss: 0.9171071648597717
Epoch 980, training loss: 6.0760016441345215 = 0.042919810861349106 + 1.0 * 6.033082008361816
Epoch 980, val loss: 0.9225107431411743
Epoch 990, training loss: 6.071176528930664 = 0.04146263003349304 + 1.0 * 6.029714107513428
Epoch 990, val loss: 0.9277285933494568
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7675
Flip ASR: 0.7200/225 nodes
The final ASR:0.48216, 0.24143, Accuracy:0.82346, 0.01429
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10576])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.83333, 0.00907
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.3233642578125 = 1.9495158195495605 + 1.0 * 8.373847961425781
Epoch 0, val loss: 1.9397196769714355
Epoch 10, training loss: 10.312392234802246 = 1.9389686584472656 + 1.0 * 8.37342357635498
Epoch 10, val loss: 1.9302774667739868
Epoch 20, training loss: 10.296649932861328 = 1.9259871244430542 + 1.0 * 8.370662689208984
Epoch 20, val loss: 1.9183077812194824
Epoch 30, training loss: 10.260004043579102 = 1.908215880393982 + 1.0 * 8.351788520812988
Epoch 30, val loss: 1.9016867876052856
Epoch 40, training loss: 10.121460914611816 = 1.8858346939086914 + 1.0 * 8.235626220703125
Epoch 40, val loss: 1.881368637084961
Epoch 50, training loss: 9.602938652038574 = 1.8615572452545166 + 1.0 * 7.741381645202637
Epoch 50, val loss: 1.859829306602478
Epoch 60, training loss: 9.122758865356445 = 1.8416707515716553 + 1.0 * 7.281087875366211
Epoch 60, val loss: 1.8431514501571655
Epoch 70, training loss: 8.759408950805664 = 1.8290687799453735 + 1.0 * 6.930339813232422
Epoch 70, val loss: 1.8318830728530884
Epoch 80, training loss: 8.539546966552734 = 1.816066861152649 + 1.0 * 6.723479747772217
Epoch 80, val loss: 1.8202719688415527
Epoch 90, training loss: 8.42659854888916 = 1.8029754161834717 + 1.0 * 6.623623371124268
Epoch 90, val loss: 1.808692455291748
Epoch 100, training loss: 8.336822509765625 = 1.7902073860168457 + 1.0 * 6.546614646911621
Epoch 100, val loss: 1.7980674505233765
Epoch 110, training loss: 8.266400337219238 = 1.77946937084198 + 1.0 * 6.486930847167969
Epoch 110, val loss: 1.7894428968429565
Epoch 120, training loss: 8.208935737609863 = 1.7690885066986084 + 1.0 * 6.439847469329834
Epoch 120, val loss: 1.7806040048599243
Epoch 130, training loss: 8.15716552734375 = 1.7572903633117676 + 1.0 * 6.399875640869141
Epoch 130, val loss: 1.7703478336334229
Epoch 140, training loss: 8.10373592376709 = 1.7439141273498535 + 1.0 * 6.359821796417236
Epoch 140, val loss: 1.7591406106948853
Epoch 150, training loss: 8.048333168029785 = 1.7291404008865356 + 1.0 * 6.319192886352539
Epoch 150, val loss: 1.7470355033874512
Epoch 160, training loss: 7.99634313583374 = 1.7119888067245483 + 1.0 * 6.284354209899902
Epoch 160, val loss: 1.7332626581192017
Epoch 170, training loss: 7.949127197265625 = 1.6910789012908936 + 1.0 * 6.2580485343933105
Epoch 170, val loss: 1.7163844108581543
Epoch 180, training loss: 7.905716896057129 = 1.6653995513916016 + 1.0 * 6.240317344665527
Epoch 180, val loss: 1.695626974105835
Epoch 190, training loss: 7.859028339385986 = 1.6346148252487183 + 1.0 * 6.2244133949279785
Epoch 190, val loss: 1.6706349849700928
Epoch 200, training loss: 7.8105950355529785 = 1.597861886024475 + 1.0 * 6.212733268737793
Epoch 200, val loss: 1.6407496929168701
Epoch 210, training loss: 7.758413314819336 = 1.554713487625122 + 1.0 * 6.203699588775635
Epoch 210, val loss: 1.6057329177856445
Epoch 220, training loss: 7.702955722808838 = 1.5059820413589478 + 1.0 * 6.19697380065918
Epoch 220, val loss: 1.5663560628890991
Epoch 230, training loss: 7.643557548522949 = 1.4529391527175903 + 1.0 * 6.190618515014648
Epoch 230, val loss: 1.5235122442245483
Epoch 240, training loss: 7.581223964691162 = 1.3968437910079956 + 1.0 * 6.184380054473877
Epoch 240, val loss: 1.4780100584030151
Epoch 250, training loss: 7.516811847686768 = 1.3385556936264038 + 1.0 * 6.178256034851074
Epoch 250, val loss: 1.4309625625610352
Epoch 260, training loss: 7.452010631561279 = 1.278782844543457 + 1.0 * 6.173227787017822
Epoch 260, val loss: 1.3832859992980957
Epoch 270, training loss: 7.394027233123779 = 1.2190719842910767 + 1.0 * 6.174955368041992
Epoch 270, val loss: 1.3361817598342896
Epoch 280, training loss: 7.326460361480713 = 1.1605323553085327 + 1.0 * 6.165927886962891
Epoch 280, val loss: 1.290449619293213
Epoch 290, training loss: 7.262379169464111 = 1.1018885374069214 + 1.0 * 6.1604905128479
Epoch 290, val loss: 1.2447577714920044
Epoch 300, training loss: 7.199333190917969 = 1.0426698923110962 + 1.0 * 6.156663417816162
Epoch 300, val loss: 1.1987131834030151
Epoch 310, training loss: 7.140113830566406 = 0.983811616897583 + 1.0 * 6.156301975250244
Epoch 310, val loss: 1.1530606746673584
Epoch 320, training loss: 7.077042102813721 = 0.9268932938575745 + 1.0 * 6.150148868560791
Epoch 320, val loss: 1.1091307401657104
Epoch 330, training loss: 7.018624305725098 = 0.8717318177223206 + 1.0 * 6.146892547607422
Epoch 330, val loss: 1.0666004419326782
Epoch 340, training loss: 6.977487564086914 = 0.8191077709197998 + 1.0 * 6.158379554748535
Epoch 340, val loss: 1.026185393333435
Epoch 350, training loss: 6.912264823913574 = 0.7708055377006531 + 1.0 * 6.1414594650268555
Epoch 350, val loss: 0.9895851612091064
Epoch 360, training loss: 6.8641180992126465 = 0.7263062596321106 + 1.0 * 6.137811660766602
Epoch 360, val loss: 0.9563500285148621
Epoch 370, training loss: 6.819369316101074 = 0.6854285597801208 + 1.0 * 6.133940696716309
Epoch 370, val loss: 0.9265047907829285
Epoch 380, training loss: 6.779815196990967 = 0.6479242444038391 + 1.0 * 6.131890773773193
Epoch 380, val loss: 0.9000862836837769
Epoch 390, training loss: 6.744578838348389 = 0.6140612363815308 + 1.0 * 6.130517482757568
Epoch 390, val loss: 0.8773892521858215
Epoch 400, training loss: 6.709417343139648 = 0.5836050510406494 + 1.0 * 6.12581205368042
Epoch 400, val loss: 0.8583499789237976
Epoch 410, training loss: 6.677556991577148 = 0.5555886030197144 + 1.0 * 6.1219682693481445
Epoch 410, val loss: 0.8419445753097534
Epoch 420, training loss: 6.649760723114014 = 0.5295190215110779 + 1.0 * 6.120241641998291
Epoch 420, val loss: 0.8278493881225586
Epoch 430, training loss: 6.625182628631592 = 0.5054128766059875 + 1.0 * 6.11976957321167
Epoch 430, val loss: 0.8158067464828491
Epoch 440, training loss: 6.598538875579834 = 0.4829406142234802 + 1.0 * 6.115598201751709
Epoch 440, val loss: 0.8058459758758545
Epoch 450, training loss: 6.572868347167969 = 0.461424320936203 + 1.0 * 6.111443996429443
Epoch 450, val loss: 0.797130823135376
Epoch 460, training loss: 6.550560474395752 = 0.44050660729408264 + 1.0 * 6.110054016113281
Epoch 460, val loss: 0.7894397377967834
Epoch 470, training loss: 6.536378383636475 = 0.4201832413673401 + 1.0 * 6.116195201873779
Epoch 470, val loss: 0.7826620936393738
Epoch 480, training loss: 6.508396148681641 = 0.4002523422241211 + 1.0 * 6.1081438064575195
Epoch 480, val loss: 0.776974081993103
Epoch 490, training loss: 6.484169960021973 = 0.3804101347923279 + 1.0 * 6.103759765625
Epoch 490, val loss: 0.7718656659126282
Epoch 500, training loss: 6.463240146636963 = 0.36045366525650024 + 1.0 * 6.102786540985107
Epoch 500, val loss: 0.7673168778419495
Epoch 510, training loss: 6.442197322845459 = 0.34046927094459534 + 1.0 * 6.1017279624938965
Epoch 510, val loss: 0.7632890939712524
Epoch 520, training loss: 6.4230828285217285 = 0.32062822580337524 + 1.0 * 6.102454662322998
Epoch 520, val loss: 0.7599530220031738
Epoch 530, training loss: 6.399348258972168 = 0.3011624813079834 + 1.0 * 6.098186016082764
Epoch 530, val loss: 0.7573233246803284
Epoch 540, training loss: 6.3780412673950195 = 0.2820191979408264 + 1.0 * 6.096022129058838
Epoch 540, val loss: 0.755477249622345
Epoch 550, training loss: 6.360009670257568 = 0.2634114623069763 + 1.0 * 6.096598148345947
Epoch 550, val loss: 0.7543489933013916
Epoch 560, training loss: 6.340807914733887 = 0.24567770957946777 + 1.0 * 6.09512996673584
Epoch 560, val loss: 0.7540569305419922
Epoch 570, training loss: 6.321918487548828 = 0.2289639264345169 + 1.0 * 6.092954635620117
Epoch 570, val loss: 0.7545332312583923
Epoch 580, training loss: 6.306195259094238 = 0.21330894529819489 + 1.0 * 6.092886447906494
Epoch 580, val loss: 0.755740761756897
Epoch 590, training loss: 6.2911458015441895 = 0.19887366890907288 + 1.0 * 6.0922722816467285
Epoch 590, val loss: 0.7575242519378662
Epoch 600, training loss: 6.273736953735352 = 0.18560682237148285 + 1.0 * 6.088129997253418
Epoch 600, val loss: 0.7601402401924133
Epoch 610, training loss: 6.263171195983887 = 0.1734578013420105 + 1.0 * 6.0897135734558105
Epoch 610, val loss: 0.763147234916687
Epoch 620, training loss: 6.250371932983398 = 0.16242150962352753 + 1.0 * 6.087950229644775
Epoch 620, val loss: 0.7667545676231384
Epoch 630, training loss: 6.23507022857666 = 0.15239766240119934 + 1.0 * 6.082672595977783
Epoch 630, val loss: 0.7708688974380493
Epoch 640, training loss: 6.226970672607422 = 0.1432209014892578 + 1.0 * 6.083749771118164
Epoch 640, val loss: 0.7753848433494568
Epoch 650, training loss: 6.215915679931641 = 0.13483543694019318 + 1.0 * 6.081080436706543
Epoch 650, val loss: 0.7801699042320251
Epoch 660, training loss: 6.207696914672852 = 0.12716463208198547 + 1.0 * 6.080532073974609
Epoch 660, val loss: 0.7852354645729065
Epoch 670, training loss: 6.198299407958984 = 0.1201639175415039 + 1.0 * 6.0781354904174805
Epoch 670, val loss: 0.7906324863433838
Epoch 680, training loss: 6.195438861846924 = 0.11371827870607376 + 1.0 * 6.081720352172852
Epoch 680, val loss: 0.796257734298706
Epoch 690, training loss: 6.185908794403076 = 0.1077868789434433 + 1.0 * 6.078122138977051
Epoch 690, val loss: 0.8020250201225281
Epoch 700, training loss: 6.175961971282959 = 0.10232441127300262 + 1.0 * 6.07363748550415
Epoch 700, val loss: 0.8081068992614746
Epoch 710, training loss: 6.178375244140625 = 0.09726079553365707 + 1.0 * 6.081114292144775
Epoch 710, val loss: 0.8142062425613403
Epoch 720, training loss: 6.163639545440674 = 0.09258747845888138 + 1.0 * 6.071052074432373
Epoch 720, val loss: 0.8204806447029114
Epoch 730, training loss: 6.158173561096191 = 0.08823235332965851 + 1.0 * 6.06994104385376
Epoch 730, val loss: 0.8269495368003845
Epoch 740, training loss: 6.15175199508667 = 0.08415182679891586 + 1.0 * 6.067600250244141
Epoch 740, val loss: 0.8335392475128174
Epoch 750, training loss: 6.159052848815918 = 0.08030932396650314 + 1.0 * 6.0787434577941895
Epoch 750, val loss: 0.840246856212616
Epoch 760, training loss: 6.142753601074219 = 0.07674074172973633 + 1.0 * 6.066012859344482
Epoch 760, val loss: 0.8469088077545166
Epoch 770, training loss: 6.1398844718933105 = 0.07339484244585037 + 1.0 * 6.0664896965026855
Epoch 770, val loss: 0.8538000583648682
Epoch 780, training loss: 6.136693000793457 = 0.0702442154288292 + 1.0 * 6.06644868850708
Epoch 780, val loss: 0.8605950474739075
Epoch 790, training loss: 6.13459587097168 = 0.06728906184434891 + 1.0 * 6.067306995391846
Epoch 790, val loss: 0.8673964738845825
Epoch 800, training loss: 6.1274518966674805 = 0.06452342867851257 + 1.0 * 6.062928676605225
Epoch 800, val loss: 0.8741623759269714
Epoch 810, training loss: 6.121994495391846 = 0.06191490590572357 + 1.0 * 6.060079574584961
Epoch 810, val loss: 0.8810970783233643
Epoch 820, training loss: 6.118912220001221 = 0.05943980813026428 + 1.0 * 6.059472560882568
Epoch 820, val loss: 0.8880191445350647
Epoch 830, training loss: 6.12546968460083 = 0.05709674954414368 + 1.0 * 6.06837272644043
Epoch 830, val loss: 0.8949704170227051
Epoch 840, training loss: 6.115696430206299 = 0.054875802248716354 + 1.0 * 6.060820579528809
Epoch 840, val loss: 0.9017143845558167
Epoch 850, training loss: 6.110069751739502 = 0.052789609879255295 + 1.0 * 6.05728006362915
Epoch 850, val loss: 0.9087996482849121
Epoch 860, training loss: 6.1076202392578125 = 0.05079633742570877 + 1.0 * 6.05682373046875
Epoch 860, val loss: 0.9156449437141418
Epoch 870, training loss: 6.10421085357666 = 0.04891056939959526 + 1.0 * 6.055300235748291
Epoch 870, val loss: 0.9224671125411987
Epoch 880, training loss: 6.1091108322143555 = 0.04712660610675812 + 1.0 * 6.061984062194824
Epoch 880, val loss: 0.9292201995849609
Epoch 890, training loss: 6.09930419921875 = 0.045446936041116714 + 1.0 * 6.053857326507568
Epoch 890, val loss: 0.9359322786331177
Epoch 900, training loss: 6.096005916595459 = 0.04384354129433632 + 1.0 * 6.052162170410156
Epoch 900, val loss: 0.9425987601280212
Epoch 910, training loss: 6.103003025054932 = 0.04231666401028633 + 1.0 * 6.0606865882873535
Epoch 910, val loss: 0.949114203453064
Epoch 920, training loss: 6.090919494628906 = 0.04087165743112564 + 1.0 * 6.050047874450684
Epoch 920, val loss: 0.9556148052215576
Epoch 930, training loss: 6.088954925537109 = 0.03949679434299469 + 1.0 * 6.049458026885986
Epoch 930, val loss: 0.9621368646621704
Epoch 940, training loss: 6.088093280792236 = 0.038179825991392136 + 1.0 * 6.04991340637207
Epoch 940, val loss: 0.9685961604118347
Epoch 950, training loss: 6.086123466491699 = 0.036924269050359726 + 1.0 * 6.049199104309082
Epoch 950, val loss: 0.9747167229652405
Epoch 960, training loss: 6.086424827575684 = 0.03573941811919212 + 1.0 * 6.050685405731201
Epoch 960, val loss: 0.9811065793037415
Epoch 970, training loss: 6.081713676452637 = 0.03460802510380745 + 1.0 * 6.04710578918457
Epoch 970, val loss: 0.9873597025871277
Epoch 980, training loss: 6.084296226501465 = 0.03352321684360504 + 1.0 * 6.0507731437683105
Epoch 980, val loss: 0.9933990240097046
Epoch 990, training loss: 6.079085350036621 = 0.032484736293554306 + 1.0 * 6.046600818634033
Epoch 990, val loss: 0.9995149970054626
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.313711166381836 = 1.9397969245910645 + 1.0 * 8.37391471862793
Epoch 0, val loss: 1.9370828866958618
Epoch 10, training loss: 10.304243087768555 = 1.9305427074432373 + 1.0 * 8.373700141906738
Epoch 10, val loss: 1.928330898284912
Epoch 20, training loss: 10.291790962219238 = 1.9196075201034546 + 1.0 * 8.372183799743652
Epoch 20, val loss: 1.9176323413848877
Epoch 30, training loss: 10.26435661315918 = 1.9049296379089355 + 1.0 * 8.359426498413086
Epoch 30, val loss: 1.9030766487121582
Epoch 40, training loss: 10.146592140197754 = 1.8853780031204224 + 1.0 * 8.261214256286621
Epoch 40, val loss: 1.8840429782867432
Epoch 50, training loss: 9.58691692352295 = 1.8641207218170166 + 1.0 * 7.722796440124512
Epoch 50, val loss: 1.8638681173324585
Epoch 60, training loss: 9.149463653564453 = 1.8431919813156128 + 1.0 * 7.306272029876709
Epoch 60, val loss: 1.8443342447280884
Epoch 70, training loss: 8.841958045959473 = 1.8259986639022827 + 1.0 * 7.0159592628479
Epoch 70, val loss: 1.828416109085083
Epoch 80, training loss: 8.63261604309082 = 1.8082493543624878 + 1.0 * 6.824366569519043
Epoch 80, val loss: 1.8118611574172974
Epoch 90, training loss: 8.470357894897461 = 1.7910346984863281 + 1.0 * 6.679323673248291
Epoch 90, val loss: 1.7966034412384033
Epoch 100, training loss: 8.360583305358887 = 1.7742044925689697 + 1.0 * 6.586378574371338
Epoch 100, val loss: 1.781877040863037
Epoch 110, training loss: 8.27371597290039 = 1.7571579217910767 + 1.0 * 6.516557693481445
Epoch 110, val loss: 1.7667328119277954
Epoch 120, training loss: 8.1946382522583 = 1.7388674020767212 + 1.0 * 6.455770969390869
Epoch 120, val loss: 1.7504414319992065
Epoch 130, training loss: 8.127840995788574 = 1.7181907892227173 + 1.0 * 6.4096503257751465
Epoch 130, val loss: 1.7322465181350708
Epoch 140, training loss: 8.066646575927734 = 1.6943655014038086 + 1.0 * 6.372281074523926
Epoch 140, val loss: 1.7114107608795166
Epoch 150, training loss: 8.00719165802002 = 1.6662569046020508 + 1.0 * 6.340934753417969
Epoch 150, val loss: 1.6873209476470947
Epoch 160, training loss: 7.949315071105957 = 1.6328585147857666 + 1.0 * 6.316456317901611
Epoch 160, val loss: 1.65879487991333
Epoch 170, training loss: 7.888128280639648 = 1.5935759544372559 + 1.0 * 6.294552326202393
Epoch 170, val loss: 1.6254304647445679
Epoch 180, training loss: 7.824769020080566 = 1.547354817390442 + 1.0 * 6.277414321899414
Epoch 180, val loss: 1.5862139463424683
Epoch 190, training loss: 7.7563276290893555 = 1.4932341575622559 + 1.0 * 6.2630934715271
Epoch 190, val loss: 1.5404020547866821
Epoch 200, training loss: 7.694896697998047 = 1.4311063289642334 + 1.0 * 6.263790130615234
Epoch 200, val loss: 1.4884461164474487
Epoch 210, training loss: 7.609843730926514 = 1.3653162717819214 + 1.0 * 6.244527339935303
Epoch 210, val loss: 1.4331544637680054
Epoch 220, training loss: 7.527664661407471 = 1.2957000732421875 + 1.0 * 6.231964588165283
Epoch 220, val loss: 1.3755483627319336
Epoch 230, training loss: 7.447476387023926 = 1.224357008934021 + 1.0 * 6.223119258880615
Epoch 230, val loss: 1.3170576095581055
Epoch 240, training loss: 7.368147373199463 = 1.1533178091049194 + 1.0 * 6.214829444885254
Epoch 240, val loss: 1.259188175201416
Epoch 250, training loss: 7.295852184295654 = 1.0853157043457031 + 1.0 * 6.210536479949951
Epoch 250, val loss: 1.2045670747756958
Epoch 260, training loss: 7.223489761352539 = 1.0227270126342773 + 1.0 * 6.200762748718262
Epoch 260, val loss: 1.1546430587768555
Epoch 270, training loss: 7.158588409423828 = 0.964691698551178 + 1.0 * 6.193896770477295
Epoch 270, val loss: 1.1086267232894897
Epoch 280, training loss: 7.107837200164795 = 0.9106484651565552 + 1.0 * 6.197188854217529
Epoch 280, val loss: 1.0660346746444702
Epoch 290, training loss: 7.044467926025391 = 0.8609479665756226 + 1.0 * 6.1835198402404785
Epoch 290, val loss: 1.0269227027893066
Epoch 300, training loss: 6.991069793701172 = 0.8140872120857239 + 1.0 * 6.176982402801514
Epoch 300, val loss: 0.9903714656829834
Epoch 310, training loss: 6.943511009216309 = 0.7695086002349854 + 1.0 * 6.174002647399902
Epoch 310, val loss: 0.9556811451911926
Epoch 320, training loss: 6.895979404449463 = 0.7274959683418274 + 1.0 * 6.168483257293701
Epoch 320, val loss: 0.9234069585800171
Epoch 330, training loss: 6.852108955383301 = 0.6880289912223816 + 1.0 * 6.1640801429748535
Epoch 330, val loss: 0.8936392068862915
Epoch 340, training loss: 6.810749530792236 = 0.6509116291999817 + 1.0 * 6.15983772277832
Epoch 340, val loss: 0.8662620186805725
Epoch 350, training loss: 6.772243022918701 = 0.6163550019264221 + 1.0 * 6.155888080596924
Epoch 350, val loss: 0.8415058255195618
Epoch 360, training loss: 6.737335205078125 = 0.5846124291419983 + 1.0 * 6.1527228355407715
Epoch 360, val loss: 0.8196653127670288
Epoch 370, training loss: 6.705300807952881 = 0.5555134415626526 + 1.0 * 6.149787425994873
Epoch 370, val loss: 0.8005093336105347
Epoch 380, training loss: 6.674723148345947 = 0.5289038419723511 + 1.0 * 6.145819187164307
Epoch 380, val loss: 0.7838391661643982
Epoch 390, training loss: 6.647359371185303 = 0.5043822526931763 + 1.0 * 6.142977237701416
Epoch 390, val loss: 0.7694402933120728
Epoch 400, training loss: 6.626155376434326 = 0.48167553544044495 + 1.0 * 6.144479751586914
Epoch 400, val loss: 0.7569032907485962
Epoch 410, training loss: 6.598359107971191 = 0.4606795907020569 + 1.0 * 6.137679576873779
Epoch 410, val loss: 0.7461170554161072
Epoch 420, training loss: 6.575937747955322 = 0.44103485345840454 + 1.0 * 6.1349029541015625
Epoch 420, val loss: 0.7367534041404724
Epoch 430, training loss: 6.556993007659912 = 0.42241039872169495 + 1.0 * 6.13458251953125
Epoch 430, val loss: 0.7284719347953796
Epoch 440, training loss: 6.534747123718262 = 0.4046314060688019 + 1.0 * 6.130115509033203
Epoch 440, val loss: 0.7210895419120789
Epoch 450, training loss: 6.514704704284668 = 0.3874605596065521 + 1.0 * 6.127243995666504
Epoch 450, val loss: 0.7144607305526733
Epoch 460, training loss: 6.504382610321045 = 0.3707078695297241 + 1.0 * 6.133674621582031
Epoch 460, val loss: 0.70842045545578
Epoch 470, training loss: 6.477440357208252 = 0.3544520139694214 + 1.0 * 6.122988224029541
Epoch 470, val loss: 0.7030202150344849
Epoch 480, training loss: 6.460740089416504 = 0.3385460376739502 + 1.0 * 6.122194290161133
Epoch 480, val loss: 0.6981611847877502
Epoch 490, training loss: 6.44224739074707 = 0.3229967951774597 + 1.0 * 6.119250774383545
Epoch 490, val loss: 0.693787157535553
Epoch 500, training loss: 6.424838542938232 = 0.3077763617038727 + 1.0 * 6.117062091827393
Epoch 500, val loss: 0.6899658441543579
Epoch 510, training loss: 6.407996654510498 = 0.2928832173347473 + 1.0 * 6.115113258361816
Epoch 510, val loss: 0.686625599861145
Epoch 520, training loss: 6.3936991691589355 = 0.2783425450325012 + 1.0 * 6.1153564453125
Epoch 520, val loss: 0.6838477849960327
Epoch 530, training loss: 6.378665447235107 = 0.2642732560634613 + 1.0 * 6.114392280578613
Epoch 530, val loss: 0.6815425157546997
Epoch 540, training loss: 6.360894203186035 = 0.2505912482738495 + 1.0 * 6.110302925109863
Epoch 540, val loss: 0.6797968149185181
Epoch 550, training loss: 6.348882675170898 = 0.2373245358467102 + 1.0 * 6.111557960510254
Epoch 550, val loss: 0.6785318851470947
Epoch 560, training loss: 6.334730625152588 = 0.2245122492313385 + 1.0 * 6.110218524932861
Epoch 560, val loss: 0.6777220368385315
Epoch 570, training loss: 6.318896293640137 = 0.2122090756893158 + 1.0 * 6.106687068939209
Epoch 570, val loss: 0.6774622201919556
Epoch 580, training loss: 6.303965091705322 = 0.2004041224718094 + 1.0 * 6.103560924530029
Epoch 580, val loss: 0.6776914000511169
Epoch 590, training loss: 6.2899394035339355 = 0.18907921016216278 + 1.0 * 6.100860118865967
Epoch 590, val loss: 0.6785312294960022
Epoch 600, training loss: 6.277896404266357 = 0.17825543880462646 + 1.0 * 6.099640846252441
Epoch 600, val loss: 0.6799079775810242
Epoch 610, training loss: 6.277395248413086 = 0.1680169701576233 + 1.0 * 6.109378337860107
Epoch 610, val loss: 0.6818075776100159
Epoch 620, training loss: 6.261270523071289 = 0.15844517946243286 + 1.0 * 6.102825164794922
Epoch 620, val loss: 0.6840150356292725
Epoch 630, training loss: 6.245038032531738 = 0.14943906664848328 + 1.0 * 6.095599174499512
Epoch 630, val loss: 0.6867417097091675
Epoch 640, training loss: 6.234861850738525 = 0.1409338116645813 + 1.0 * 6.09392786026001
Epoch 640, val loss: 0.6899980306625366
Epoch 650, training loss: 6.225013256072998 = 0.13290685415267944 + 1.0 * 6.092106342315674
Epoch 650, val loss: 0.6936819553375244
Epoch 660, training loss: 6.216508865356445 = 0.12533897161483765 + 1.0 * 6.091169834136963
Epoch 660, val loss: 0.6977617144584656
Epoch 670, training loss: 6.208897113800049 = 0.11825019121170044 + 1.0 * 6.090646743774414
Epoch 670, val loss: 0.7020516991615295
Epoch 680, training loss: 6.201358795166016 = 0.1116737648844719 + 1.0 * 6.089684963226318
Epoch 680, val loss: 0.7065539360046387
Epoch 690, training loss: 6.193817138671875 = 0.10552213340997696 + 1.0 * 6.088294982910156
Epoch 690, val loss: 0.7113988995552063
Epoch 700, training loss: 6.185800075531006 = 0.0997520387172699 + 1.0 * 6.086048126220703
Epoch 700, val loss: 0.7165465354919434
Epoch 710, training loss: 6.1786885261535645 = 0.09433560073375702 + 1.0 * 6.084352970123291
Epoch 710, val loss: 0.7219122648239136
Epoch 720, training loss: 6.18375301361084 = 0.08925724029541016 + 1.0 * 6.09449577331543
Epoch 720, val loss: 0.7274037003517151
Epoch 730, training loss: 6.16874361038208 = 0.0845797210931778 + 1.0 * 6.084163665771484
Epoch 730, val loss: 0.7328216433525085
Epoch 740, training loss: 6.163039207458496 = 0.08020798861980438 + 1.0 * 6.082831382751465
Epoch 740, val loss: 0.7383657097816467
Epoch 750, training loss: 6.155908584594727 = 0.07610159367322922 + 1.0 * 6.079806804656982
Epoch 750, val loss: 0.7440896034240723
Epoch 760, training loss: 6.150928497314453 = 0.07224228978157043 + 1.0 * 6.078686237335205
Epoch 760, val loss: 0.74992436170578
Epoch 770, training loss: 6.145991802215576 = 0.06861165165901184 + 1.0 * 6.077380180358887
Epoch 770, val loss: 0.7558001279830933
Epoch 780, training loss: 6.166960716247559 = 0.06519695371389389 + 1.0 * 6.101763725280762
Epoch 780, val loss: 0.7616704702377319
Epoch 790, training loss: 6.147630214691162 = 0.06202477589249611 + 1.0 * 6.085605621337891
Epoch 790, val loss: 0.7674275636672974
Epoch 800, training loss: 6.134146690368652 = 0.059063173830509186 + 1.0 * 6.0750837326049805
Epoch 800, val loss: 0.7731128931045532
Epoch 810, training loss: 6.130589962005615 = 0.05627954378724098 + 1.0 * 6.074310302734375
Epoch 810, val loss: 0.7788843512535095
Epoch 820, training loss: 6.126031398773193 = 0.05365230515599251 + 1.0 * 6.072379112243652
Epoch 820, val loss: 0.7846705913543701
Epoch 830, training loss: 6.122381210327148 = 0.05117110535502434 + 1.0 * 6.071209907531738
Epoch 830, val loss: 0.7904253602027893
Epoch 840, training loss: 6.120965957641602 = 0.04882863163948059 + 1.0 * 6.072137355804443
Epoch 840, val loss: 0.796173632144928
Epoch 850, training loss: 6.1199116706848145 = 0.04661911353468895 + 1.0 * 6.0732927322387695
Epoch 850, val loss: 0.8018324971199036
Epoch 860, training loss: 6.113064765930176 = 0.04455765709280968 + 1.0 * 6.068507194519043
Epoch 860, val loss: 0.8073733448982239
Epoch 870, training loss: 6.110647678375244 = 0.042616065591573715 + 1.0 * 6.0680317878723145
Epoch 870, val loss: 0.812943160533905
Epoch 880, training loss: 6.108345031738281 = 0.040778998285532 + 1.0 * 6.06756591796875
Epoch 880, val loss: 0.8185080289840698
Epoch 890, training loss: 6.113810062408447 = 0.03904259204864502 + 1.0 * 6.074767589569092
Epoch 890, val loss: 0.8239827752113342
Epoch 900, training loss: 6.106036186218262 = 0.037412215024232864 + 1.0 * 6.068624019622803
Epoch 900, val loss: 0.8293384313583374
Epoch 910, training loss: 6.10399866104126 = 0.0358792245388031 + 1.0 * 6.068119525909424
Epoch 910, val loss: 0.834615170955658
Epoch 920, training loss: 6.099755764007568 = 0.03443146497011185 + 1.0 * 6.065324306488037
Epoch 920, val loss: 0.8398249745368958
Epoch 930, training loss: 6.096388816833496 = 0.033063556998968124 + 1.0 * 6.06332540512085
Epoch 930, val loss: 0.8450457453727722
Epoch 940, training loss: 6.0944719314575195 = 0.031765762716531754 + 1.0 * 6.062705993652344
Epoch 940, val loss: 0.8501951098442078
Epoch 950, training loss: 6.0978593826293945 = 0.030534828081727028 + 1.0 * 6.067324638366699
Epoch 950, val loss: 0.8552755117416382
Epoch 960, training loss: 6.093268871307373 = 0.029369927942752838 + 1.0 * 6.063899040222168
Epoch 960, val loss: 0.8602339625358582
Epoch 970, training loss: 6.090384006500244 = 0.028268199414014816 + 1.0 * 6.062115669250488
Epoch 970, val loss: 0.8651727437973022
Epoch 980, training loss: 6.086674213409424 = 0.027226068079471588 + 1.0 * 6.0594482421875
Epoch 980, val loss: 0.8700717091560364
Epoch 990, training loss: 6.086065292358398 = 0.026235418394207954 + 1.0 * 6.0598297119140625
Epoch 990, val loss: 0.8749239444732666
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32111930847168 = 1.9472099542617798 + 1.0 * 8.373908996582031
Epoch 0, val loss: 1.9449564218521118
Epoch 10, training loss: 10.311023712158203 = 1.937384843826294 + 1.0 * 8.373639106750488
Epoch 10, val loss: 1.9352538585662842
Epoch 20, training loss: 10.297043800354004 = 1.9257680177688599 + 1.0 * 8.371275901794434
Epoch 20, val loss: 1.9231784343719482
Epoch 30, training loss: 10.262334823608398 = 1.9100630283355713 + 1.0 * 8.352272033691406
Epoch 30, val loss: 1.9063513278961182
Epoch 40, training loss: 10.093986511230469 = 1.8900984525680542 + 1.0 * 8.203887939453125
Epoch 40, val loss: 1.8855165243148804
Epoch 50, training loss: 9.457562446594238 = 1.8696011304855347 + 1.0 * 7.587961673736572
Epoch 50, val loss: 1.8647165298461914
Epoch 60, training loss: 9.0176362991333 = 1.8511770963668823 + 1.0 * 7.166459560394287
Epoch 60, val loss: 1.8464046716690063
Epoch 70, training loss: 8.712920188903809 = 1.833505630493164 + 1.0 * 6.8794145584106445
Epoch 70, val loss: 1.8290293216705322
Epoch 80, training loss: 8.49186897277832 = 1.818143367767334 + 1.0 * 6.673725128173828
Epoch 80, val loss: 1.8142505884170532
Epoch 90, training loss: 8.349900245666504 = 1.8025120496749878 + 1.0 * 6.547388076782227
Epoch 90, val loss: 1.7994678020477295
Epoch 100, training loss: 8.252569198608398 = 1.7856190204620361 + 1.0 * 6.466949939727783
Epoch 100, val loss: 1.7831345796585083
Epoch 110, training loss: 8.177124977111816 = 1.7672778367996216 + 1.0 * 6.409846782684326
Epoch 110, val loss: 1.7655174732208252
Epoch 120, training loss: 8.111770629882812 = 1.7479676008224487 + 1.0 * 6.363803386688232
Epoch 120, val loss: 1.747172474861145
Epoch 130, training loss: 8.052968978881836 = 1.7276116609573364 + 1.0 * 6.325357437133789
Epoch 130, val loss: 1.7279804944992065
Epoch 140, training loss: 8.001547813415527 = 1.705439805984497 + 1.0 * 6.296107769012451
Epoch 140, val loss: 1.7073211669921875
Epoch 150, training loss: 7.954358100891113 = 1.6802908182144165 + 1.0 * 6.274067401885986
Epoch 150, val loss: 1.6844041347503662
Epoch 160, training loss: 7.9066162109375 = 1.651201605796814 + 1.0 * 6.2554144859313965
Epoch 160, val loss: 1.6584128141403198
Epoch 170, training loss: 7.857231140136719 = 1.6171977519989014 + 1.0 * 6.2400336265563965
Epoch 170, val loss: 1.6283951997756958
Epoch 180, training loss: 7.80632209777832 = 1.577358603477478 + 1.0 * 6.228963375091553
Epoch 180, val loss: 1.5936346054077148
Epoch 190, training loss: 7.749187469482422 = 1.531844139099121 + 1.0 * 6.217343330383301
Epoch 190, val loss: 1.5544594526290894
Epoch 200, training loss: 7.687184810638428 = 1.4805388450622559 + 1.0 * 6.206645965576172
Epoch 200, val loss: 1.5105879306793213
Epoch 210, training loss: 7.621006488800049 = 1.4231349229812622 + 1.0 * 6.197871685028076
Epoch 210, val loss: 1.4620810747146606
Epoch 220, training loss: 7.554187297821045 = 1.3605934381484985 + 1.0 * 6.193593978881836
Epoch 220, val loss: 1.4102048873901367
Epoch 230, training loss: 7.479523658752441 = 1.2956655025482178 + 1.0 * 6.183858394622803
Epoch 230, val loss: 1.3570393323898315
Epoch 240, training loss: 7.4059648513793945 = 1.228848934173584 + 1.0 * 6.1771159172058105
Epoch 240, val loss: 1.3033865690231323
Epoch 250, training loss: 7.335057735443115 = 1.1613496541976929 + 1.0 * 6.173707962036133
Epoch 250, val loss: 1.2504550218582153
Epoch 260, training loss: 7.264597415924072 = 1.0952982902526855 + 1.0 * 6.169299125671387
Epoch 260, val loss: 1.2001396417617798
Epoch 270, training loss: 7.195043563842773 = 1.0318019390106201 + 1.0 * 6.163241386413574
Epoch 270, val loss: 1.152743935585022
Epoch 280, training loss: 7.1277241706848145 = 0.9702996611595154 + 1.0 * 6.157424449920654
Epoch 280, val loss: 1.1078954935073853
Epoch 290, training loss: 7.066738128662109 = 0.9110236167907715 + 1.0 * 6.155714511871338
Epoch 290, val loss: 1.06557297706604
Epoch 300, training loss: 7.006661891937256 = 0.8548992276191711 + 1.0 * 6.15176248550415
Epoch 300, val loss: 1.0265506505966187
Epoch 310, training loss: 6.950216770172119 = 0.8025435209274292 + 1.0 * 6.1476731300354
Epoch 310, val loss: 0.9907313585281372
Epoch 320, training loss: 6.896276473999023 = 0.7539491653442383 + 1.0 * 6.142327308654785
Epoch 320, val loss: 0.9582791924476624
Epoch 330, training loss: 6.845978260040283 = 0.7087876200675964 + 1.0 * 6.137190818786621
Epoch 330, val loss: 0.9288312792778015
Epoch 340, training loss: 6.8077616691589355 = 0.6668328642845154 + 1.0 * 6.140928745269775
Epoch 340, val loss: 0.902210533618927
Epoch 350, training loss: 6.761224746704102 = 0.6284468770027161 + 1.0 * 6.132777690887451
Epoch 350, val loss: 0.8785982728004456
Epoch 360, training loss: 6.722696781158447 = 0.5928800702095032 + 1.0 * 6.12981653213501
Epoch 360, val loss: 0.8575885891914368
Epoch 370, training loss: 6.683502197265625 = 0.5598137378692627 + 1.0 * 6.123688220977783
Epoch 370, val loss: 0.8387327194213867
Epoch 380, training loss: 6.6515374183654785 = 0.5287007093429565 + 1.0 * 6.122836589813232
Epoch 380, val loss: 0.8217920660972595
Epoch 390, training loss: 6.624558448791504 = 0.4993646740913391 + 1.0 * 6.1251935958862305
Epoch 390, val loss: 0.8066295981407166
Epoch 400, training loss: 6.589452266693115 = 0.47183412313461304 + 1.0 * 6.117618083953857
Epoch 400, val loss: 0.7931938171386719
Epoch 410, training loss: 6.559290885925293 = 0.4457491934299469 + 1.0 * 6.113541603088379
Epoch 410, val loss: 0.7813435196876526
Epoch 420, training loss: 6.536994457244873 = 0.42099660634994507 + 1.0 * 6.115997791290283
Epoch 420, val loss: 0.7710642218589783
Epoch 430, training loss: 6.509273052215576 = 0.3974587023258209 + 1.0 * 6.111814498901367
Epoch 430, val loss: 0.762242317199707
Epoch 440, training loss: 6.481199264526367 = 0.3750319480895996 + 1.0 * 6.106167316436768
Epoch 440, val loss: 0.7546898126602173
Epoch 450, training loss: 6.459528923034668 = 0.3534865975379944 + 1.0 * 6.106042385101318
Epoch 450, val loss: 0.7481297254562378
Epoch 460, training loss: 6.438600540161133 = 0.3329560458660126 + 1.0 * 6.105644702911377
Epoch 460, val loss: 0.742436408996582
Epoch 470, training loss: 6.415040493011475 = 0.3134685754776001 + 1.0 * 6.101572036743164
Epoch 470, val loss: 0.737851619720459
Epoch 480, training loss: 6.3928327560424805 = 0.2949393689632416 + 1.0 * 6.097893238067627
Epoch 480, val loss: 0.7340655326843262
Epoch 490, training loss: 6.374283790588379 = 0.2773147225379944 + 1.0 * 6.096969127655029
Epoch 490, val loss: 0.7310060262680054
Epoch 500, training loss: 6.356579303741455 = 0.2606380879878998 + 1.0 * 6.095941066741943
Epoch 500, val loss: 0.7286749482154846
Epoch 510, training loss: 6.342710018157959 = 0.2449692338705063 + 1.0 * 6.097740650177002
Epoch 510, val loss: 0.726963460445404
Epoch 520, training loss: 6.321703910827637 = 0.2302190214395523 + 1.0 * 6.091485023498535
Epoch 520, val loss: 0.7259237766265869
Epoch 530, training loss: 6.3097357749938965 = 0.21632207930088043 + 1.0 * 6.093413829803467
Epoch 530, val loss: 0.7253784537315369
Epoch 540, training loss: 6.29197883605957 = 0.20335650444030762 + 1.0 * 6.088622093200684
Epoch 540, val loss: 0.7252720594406128
Epoch 550, training loss: 6.276773929595947 = 0.19119402766227722 + 1.0 * 6.085579872131348
Epoch 550, val loss: 0.7257154583930969
Epoch 560, training loss: 6.2708635330200195 = 0.17980070412158966 + 1.0 * 6.091063022613525
Epoch 560, val loss: 0.7266057133674622
Epoch 570, training loss: 6.255496501922607 = 0.16922441124916077 + 1.0 * 6.086272239685059
Epoch 570, val loss: 0.7278487086296082
Epoch 580, training loss: 6.241102695465088 = 0.15933050215244293 + 1.0 * 6.081772327423096
Epoch 580, val loss: 0.7296121716499329
Epoch 590, training loss: 6.236337184906006 = 0.1501031368970871 + 1.0 * 6.086234092712402
Epoch 590, val loss: 0.7317014336585999
Epoch 600, training loss: 6.226417064666748 = 0.1415385752916336 + 1.0 * 6.084878444671631
Epoch 600, val loss: 0.7341368794441223
Epoch 610, training loss: 6.211117744445801 = 0.13353809714317322 + 1.0 * 6.077579498291016
Epoch 610, val loss: 0.7370100617408752
Epoch 620, training loss: 6.204876899719238 = 0.12606193125247955 + 1.0 * 6.07881498336792
Epoch 620, val loss: 0.7401829957962036
Epoch 630, training loss: 6.194256782531738 = 0.11911136656999588 + 1.0 * 6.075145244598389
Epoch 630, val loss: 0.7434794902801514
Epoch 640, training loss: 6.186438083648682 = 0.11263646930456161 + 1.0 * 6.073801517486572
Epoch 640, val loss: 0.7471745610237122
Epoch 650, training loss: 6.179727077484131 = 0.10658740252256393 + 1.0 * 6.073139667510986
Epoch 650, val loss: 0.7510631084442139
Epoch 660, training loss: 6.173763751983643 = 0.10093510150909424 + 1.0 * 6.072828769683838
Epoch 660, val loss: 0.7550457119941711
Epoch 670, training loss: 6.167926788330078 = 0.09569009393453598 + 1.0 * 6.07223653793335
Epoch 670, val loss: 0.7592552304267883
Epoch 680, training loss: 6.163312911987305 = 0.09077717363834381 + 1.0 * 6.072535514831543
Epoch 680, val loss: 0.7637274265289307
Epoch 690, training loss: 6.1545515060424805 = 0.08619046211242676 + 1.0 * 6.068360805511475
Epoch 690, val loss: 0.7682110071182251
Epoch 700, training loss: 6.150070667266846 = 0.0819035992026329 + 1.0 * 6.068167209625244
Epoch 700, val loss: 0.7728590965270996
Epoch 710, training loss: 6.148736953735352 = 0.077896349132061 + 1.0 * 6.070840835571289
Epoch 710, val loss: 0.7776039242744446
Epoch 720, training loss: 6.140990257263184 = 0.07417158037424088 + 1.0 * 6.066818714141846
Epoch 720, val loss: 0.7823920845985413
Epoch 730, training loss: 6.1387763023376465 = 0.07066179066896439 + 1.0 * 6.068114280700684
Epoch 730, val loss: 0.7873438000679016
Epoch 740, training loss: 6.132688045501709 = 0.06738905608654022 + 1.0 * 6.065299034118652
Epoch 740, val loss: 0.7922167181968689
Epoch 750, training loss: 6.129077434539795 = 0.06431106477975845 + 1.0 * 6.0647664070129395
Epoch 750, val loss: 0.7972177863121033
Epoch 760, training loss: 6.128417015075684 = 0.06143409386277199 + 1.0 * 6.066982746124268
Epoch 760, val loss: 0.8022434711456299
Epoch 770, training loss: 6.12080192565918 = 0.05872184410691261 + 1.0 * 6.062079906463623
Epoch 770, val loss: 0.8073091506958008
Epoch 780, training loss: 6.115523338317871 = 0.05618245154619217 + 1.0 * 6.059340953826904
Epoch 780, val loss: 0.8124666213989258
Epoch 790, training loss: 6.115554332733154 = 0.05378620699048042 + 1.0 * 6.061768054962158
Epoch 790, val loss: 0.8175986409187317
Epoch 800, training loss: 6.116314888000488 = 0.05152323469519615 + 1.0 * 6.064791679382324
Epoch 800, val loss: 0.8226476907730103
Epoch 810, training loss: 6.109931468963623 = 0.04941337928175926 + 1.0 * 6.060518264770508
Epoch 810, val loss: 0.8278094530105591
Epoch 820, training loss: 6.104678630828857 = 0.047416698187589645 + 1.0 * 6.057261943817139
Epoch 820, val loss: 0.8330286741256714
Epoch 830, training loss: 6.100912570953369 = 0.0455322340130806 + 1.0 * 6.055380344390869
Epoch 830, val loss: 0.8382163643836975
Epoch 840, training loss: 6.115561008453369 = 0.043752651661634445 + 1.0 * 6.071808338165283
Epoch 840, val loss: 0.8433265686035156
Epoch 850, training loss: 6.096640110015869 = 0.04206569120287895 + 1.0 * 6.054574489593506
Epoch 850, val loss: 0.8484269976615906
Epoch 860, training loss: 6.0952839851379395 = 0.04047992452979088 + 1.0 * 6.054803848266602
Epoch 860, val loss: 0.8536069393157959
Epoch 870, training loss: 6.091363430023193 = 0.038974516093730927 + 1.0 * 6.052389144897461
Epoch 870, val loss: 0.8588106036186218
Epoch 880, training loss: 6.107318878173828 = 0.03754303604364395 + 1.0 * 6.0697760581970215
Epoch 880, val loss: 0.863888680934906
Epoch 890, training loss: 6.088011264801025 = 0.03619745001196861 + 1.0 * 6.05181360244751
Epoch 890, val loss: 0.8688805103302002
Epoch 900, training loss: 6.086820125579834 = 0.03492243215441704 + 1.0 * 6.0518975257873535
Epoch 900, val loss: 0.8740381598472595
Epoch 910, training loss: 6.084513187408447 = 0.03370636701583862 + 1.0 * 6.050806999206543
Epoch 910, val loss: 0.879123866558075
Epoch 920, training loss: 6.0883402824401855 = 0.032550591975450516 + 1.0 * 6.055789470672607
Epoch 920, val loss: 0.8841103911399841
Epoch 930, training loss: 6.081796169281006 = 0.03144468367099762 + 1.0 * 6.050351619720459
Epoch 930, val loss: 0.8891803026199341
Epoch 940, training loss: 6.080402374267578 = 0.030400628224015236 + 1.0 * 6.050001621246338
Epoch 940, val loss: 0.8941851854324341
Epoch 950, training loss: 6.076485633850098 = 0.029404696077108383 + 1.0 * 6.047080993652344
Epoch 950, val loss: 0.8991426229476929
Epoch 960, training loss: 6.078374862670898 = 0.02846132405102253 + 1.0 * 6.04991340637207
Epoch 960, val loss: 0.9039812088012695
Epoch 970, training loss: 6.072873592376709 = 0.027559028938412666 + 1.0 * 6.045314788818359
Epoch 970, val loss: 0.9089877605438232
Epoch 980, training loss: 6.072644233703613 = 0.026695383712649345 + 1.0 * 6.0459489822387695
Epoch 980, val loss: 0.913925290107727
Epoch 990, training loss: 6.073098182678223 = 0.025870859622955322 + 1.0 * 6.047227382659912
Epoch 990, val loss: 0.9186573028564453
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7122
Flip ASR: 0.6533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.33029556274414 = 1.9563677310943604 + 1.0 * 8.37392807006836
Epoch 0, val loss: 1.9606969356536865
Epoch 10, training loss: 10.319598197937012 = 1.9458554983139038 + 1.0 * 8.373743057250977
Epoch 10, val loss: 1.9505380392074585
Epoch 20, training loss: 10.305248260498047 = 1.9329266548156738 + 1.0 * 8.372321128845215
Epoch 20, val loss: 1.9375734329223633
Epoch 30, training loss: 10.275848388671875 = 1.914762258529663 + 1.0 * 8.361085891723633
Epoch 30, val loss: 1.9189988374710083
Epoch 40, training loss: 10.189892768859863 = 1.8892055749893188 + 1.0 * 8.300686836242676
Epoch 40, val loss: 1.8933255672454834
Epoch 50, training loss: 9.858220100402832 = 1.860935926437378 + 1.0 * 7.997283935546875
Epoch 50, val loss: 1.865972638130188
Epoch 60, training loss: 9.445072174072266 = 1.8314322233200073 + 1.0 * 7.613639831542969
Epoch 60, val loss: 1.8394962549209595
Epoch 70, training loss: 9.009209632873535 = 1.8071942329406738 + 1.0 * 7.202015399932861
Epoch 70, val loss: 1.8173891305923462
Epoch 80, training loss: 8.692094802856445 = 1.7870875597000122 + 1.0 * 6.9050068855285645
Epoch 80, val loss: 1.798878788948059
Epoch 90, training loss: 8.536334037780762 = 1.767121434211731 + 1.0 * 6.76921272277832
Epoch 90, val loss: 1.7799440622329712
Epoch 100, training loss: 8.43815803527832 = 1.7443063259124756 + 1.0 * 6.693851470947266
Epoch 100, val loss: 1.7585947513580322
Epoch 110, training loss: 8.345063209533691 = 1.7205629348754883 + 1.0 * 6.624500274658203
Epoch 110, val loss: 1.7368073463439941
Epoch 120, training loss: 8.256409645080566 = 1.6958706378936768 + 1.0 * 6.560539245605469
Epoch 120, val loss: 1.7141780853271484
Epoch 130, training loss: 8.179244995117188 = 1.668900728225708 + 1.0 * 6.510344505310059
Epoch 130, val loss: 1.6896958351135254
Epoch 140, training loss: 8.097390174865723 = 1.6380325555801392 + 1.0 * 6.459357261657715
Epoch 140, val loss: 1.6622734069824219
Epoch 150, training loss: 8.013086318969727 = 1.6030349731445312 + 1.0 * 6.410051345825195
Epoch 150, val loss: 1.6323636770248413
Epoch 160, training loss: 7.940369606018066 = 1.5633662939071655 + 1.0 * 6.377003192901611
Epoch 160, val loss: 1.5988950729370117
Epoch 170, training loss: 7.865593433380127 = 1.519083023071289 + 1.0 * 6.346510410308838
Epoch 170, val loss: 1.5621144771575928
Epoch 180, training loss: 7.796091079711914 = 1.4701670408248901 + 1.0 * 6.325923919677734
Epoch 180, val loss: 1.521780252456665
Epoch 190, training loss: 7.727423191070557 = 1.4174643754959106 + 1.0 * 6.3099589347839355
Epoch 190, val loss: 1.4790982007980347
Epoch 200, training loss: 7.655892372131348 = 1.3623900413513184 + 1.0 * 6.293502330780029
Epoch 200, val loss: 1.435273289680481
Epoch 210, training loss: 7.585422515869141 = 1.3057560920715332 + 1.0 * 6.279666423797607
Epoch 210, val loss: 1.3911875486373901
Epoch 220, training loss: 7.516429901123047 = 1.24810791015625 + 1.0 * 6.268321990966797
Epoch 220, val loss: 1.3470346927642822
Epoch 230, training loss: 7.446815490722656 = 1.18890380859375 + 1.0 * 6.257911682128906
Epoch 230, val loss: 1.3023810386657715
Epoch 240, training loss: 7.378631114959717 = 1.1279157400131226 + 1.0 * 6.250715255737305
Epoch 240, val loss: 1.2568774223327637
Epoch 250, training loss: 7.310122966766357 = 1.0667074918746948 + 1.0 * 6.243415355682373
Epoch 250, val loss: 1.2114485502243042
Epoch 260, training loss: 7.242517948150635 = 1.0056339502334595 + 1.0 * 6.236884117126465
Epoch 260, val loss: 1.1665246486663818
Epoch 270, training loss: 7.174232482910156 = 0.9450016021728516 + 1.0 * 6.229230880737305
Epoch 270, val loss: 1.122258186340332
Epoch 280, training loss: 7.1161723136901855 = 0.8858738541603088 + 1.0 * 6.2302985191345215
Epoch 280, val loss: 1.0796152353286743
Epoch 290, training loss: 7.049321174621582 = 0.8303857445716858 + 1.0 * 6.218935489654541
Epoch 290, val loss: 1.040408730506897
Epoch 300, training loss: 6.992170333862305 = 0.7786473035812378 + 1.0 * 6.213522911071777
Epoch 300, val loss: 1.0048454999923706
Epoch 310, training loss: 6.949147701263428 = 0.7309484481811523 + 1.0 * 6.218199253082275
Epoch 310, val loss: 0.972961962223053
Epoch 320, training loss: 6.8931193351745605 = 0.6881548166275024 + 1.0 * 6.204964637756348
Epoch 320, val loss: 0.9453650712966919
Epoch 330, training loss: 6.848601818084717 = 0.649459183216095 + 1.0 * 6.1991424560546875
Epoch 330, val loss: 0.9217617511749268
Epoch 340, training loss: 6.809239387512207 = 0.6142557859420776 + 1.0 * 6.19498348236084
Epoch 340, val loss: 0.9012482166290283
Epoch 350, training loss: 6.774050235748291 = 0.5821231603622437 + 1.0 * 6.191926956176758
Epoch 350, val loss: 0.8836722373962402
Epoch 360, training loss: 6.7393975257873535 = 0.5524149537086487 + 1.0 * 6.18698263168335
Epoch 360, val loss: 0.8684296607971191
Epoch 370, training loss: 6.718328475952148 = 0.5246847867965698 + 1.0 * 6.193643569946289
Epoch 370, val loss: 0.8551158308982849
Epoch 380, training loss: 6.681108474731445 = 0.4991114139556885 + 1.0 * 6.181997299194336
Epoch 380, val loss: 0.8437360525131226
Epoch 390, training loss: 6.65089225769043 = 0.47512441873550415 + 1.0 * 6.17576789855957
Epoch 390, val loss: 0.834086000919342
Epoch 400, training loss: 6.632883071899414 = 0.4524720311164856 + 1.0 * 6.180410861968994
Epoch 400, val loss: 0.8256248235702515
Epoch 410, training loss: 6.602709770202637 = 0.4312831163406372 + 1.0 * 6.171426773071289
Epoch 410, val loss: 0.818461537361145
Epoch 420, training loss: 6.574961185455322 = 0.41121721267700195 + 1.0 * 6.16374397277832
Epoch 420, val loss: 0.8125236630439758
Epoch 430, training loss: 6.5525803565979 = 0.3920052945613861 + 1.0 * 6.160574913024902
Epoch 430, val loss: 0.8075069189071655
Epoch 440, training loss: 6.534646511077881 = 0.3736863434314728 + 1.0 * 6.1609601974487305
Epoch 440, val loss: 0.8032776713371277
Epoch 450, training loss: 6.514103412628174 = 0.3563583791255951 + 1.0 * 6.157744884490967
Epoch 450, val loss: 0.7999894618988037
Epoch 460, training loss: 6.490890026092529 = 0.3397022485733032 + 1.0 * 6.151187896728516
Epoch 460, val loss: 0.7975136637687683
Epoch 470, training loss: 6.488560676574707 = 0.3236594498157501 + 1.0 * 6.164901256561279
Epoch 470, val loss: 0.7955578565597534
Epoch 480, training loss: 6.454458236694336 = 0.30829066038131714 + 1.0 * 6.146167755126953
Epoch 480, val loss: 0.7941428422927856
Epoch 490, training loss: 6.435513019561768 = 0.29344838857650757 + 1.0 * 6.142064571380615
Epoch 490, val loss: 0.7934845089912415
Epoch 500, training loss: 6.418004512786865 = 0.279053270816803 + 1.0 * 6.138951301574707
Epoch 500, val loss: 0.7932831645011902
Epoch 510, training loss: 6.401160717010498 = 0.26507624983787537 + 1.0 * 6.13608455657959
Epoch 510, val loss: 0.7936092615127563
Epoch 520, training loss: 6.3989763259887695 = 0.2515379786491394 + 1.0 * 6.1474385261535645
Epoch 520, val loss: 0.7943627238273621
Epoch 530, training loss: 6.376832008361816 = 0.23864324390888214 + 1.0 * 6.13818883895874
Epoch 530, val loss: 0.7955105900764465
Epoch 540, training loss: 6.3564229011535645 = 0.22631466388702393 + 1.0 * 6.13010835647583
Epoch 540, val loss: 0.7971833944320679
Epoch 550, training loss: 6.3420729637146 = 0.214457169175148 + 1.0 * 6.127615928649902
Epoch 550, val loss: 0.7993069887161255
Epoch 560, training loss: 6.328727722167969 = 0.2030811607837677 + 1.0 * 6.125646591186523
Epoch 560, val loss: 0.8018635511398315
Epoch 570, training loss: 6.324522972106934 = 0.19222840666770935 + 1.0 * 6.132294654846191
Epoch 570, val loss: 0.8047915697097778
Epoch 580, training loss: 6.304677486419678 = 0.18190798163414001 + 1.0 * 6.122769355773926
Epoch 580, val loss: 0.8079838752746582
Epoch 590, training loss: 6.295186519622803 = 0.17213551700115204 + 1.0 * 6.123051166534424
Epoch 590, val loss: 0.8115878701210022
Epoch 600, training loss: 6.283003807067871 = 0.16288556158542633 + 1.0 * 6.120118141174316
Epoch 600, val loss: 0.8155350089073181
Epoch 610, training loss: 6.273413181304932 = 0.15413038432598114 + 1.0 * 6.1192827224731445
Epoch 610, val loss: 0.8196982741355896
Epoch 620, training loss: 6.260318279266357 = 0.14586210250854492 + 1.0 * 6.1144561767578125
Epoch 620, val loss: 0.8241020441055298
Epoch 630, training loss: 6.258829593658447 = 0.13806255161762238 + 1.0 * 6.120767116546631
Epoch 630, val loss: 0.8287925720214844
Epoch 640, training loss: 6.2446370124816895 = 0.13070227205753326 + 1.0 * 6.113934516906738
Epoch 640, val loss: 0.8335327506065369
Epoch 650, training loss: 6.2336249351501465 = 0.1237761527299881 + 1.0 * 6.109848976135254
Epoch 650, val loss: 0.8385578989982605
Epoch 660, training loss: 6.228092670440674 = 0.1172158420085907 + 1.0 * 6.11087703704834
Epoch 660, val loss: 0.8438157439231873
Epoch 670, training loss: 6.22227144241333 = 0.11104713380336761 + 1.0 * 6.111224174499512
Epoch 670, val loss: 0.8490298390388489
Epoch 680, training loss: 6.209214687347412 = 0.10524656623601913 + 1.0 * 6.103968143463135
Epoch 680, val loss: 0.8543415665626526
Epoch 690, training loss: 6.20433235168457 = 0.09978037327528 + 1.0 * 6.104551792144775
Epoch 690, val loss: 0.8598619103431702
Epoch 700, training loss: 6.200926780700684 = 0.09463689476251602 + 1.0 * 6.106289863586426
Epoch 700, val loss: 0.8653545379638672
Epoch 710, training loss: 6.198098182678223 = 0.08981728553771973 + 1.0 * 6.108280658721924
Epoch 710, val loss: 0.8707475066184998
Epoch 720, training loss: 6.185406684875488 = 0.0852728858590126 + 1.0 * 6.100133895874023
Epoch 720, val loss: 0.8762190341949463
Epoch 730, training loss: 6.17971134185791 = 0.08100195974111557 + 1.0 * 6.098709583282471
Epoch 730, val loss: 0.8819203972816467
Epoch 740, training loss: 6.177730083465576 = 0.07697518169879913 + 1.0 * 6.100754737854004
Epoch 740, val loss: 0.8876739144325256
Epoch 750, training loss: 6.171280384063721 = 0.07320962101221085 + 1.0 * 6.0980706214904785
Epoch 750, val loss: 0.8932847380638123
Epoch 760, training loss: 6.165481090545654 = 0.0696725994348526 + 1.0 * 6.095808506011963
Epoch 760, val loss: 0.8988445401191711
Epoch 770, training loss: 6.160898208618164 = 0.0663534626364708 + 1.0 * 6.094544887542725
Epoch 770, val loss: 0.9045315384864807
Epoch 780, training loss: 6.15445613861084 = 0.06323180347681046 + 1.0 * 6.091224193572998
Epoch 780, val loss: 0.9103569984436035
Epoch 790, training loss: 6.150225639343262 = 0.06028579920530319 + 1.0 * 6.089940071105957
Epoch 790, val loss: 0.916257381439209
Epoch 800, training loss: 6.150317668914795 = 0.05750637128949165 + 1.0 * 6.092811107635498
Epoch 800, val loss: 0.9221512675285339
Epoch 810, training loss: 6.146298408508301 = 0.054900139570236206 + 1.0 * 6.091398239135742
Epoch 810, val loss: 0.9280139207839966
Epoch 820, training loss: 6.14310359954834 = 0.05245613679289818 + 1.0 * 6.0906476974487305
Epoch 820, val loss: 0.9338712692260742
Epoch 830, training loss: 6.139272212982178 = 0.05016471818089485 + 1.0 * 6.089107513427734
Epoch 830, val loss: 0.9396639466285706
Epoch 840, training loss: 6.13461971282959 = 0.0480157844722271 + 1.0 * 6.086604118347168
Epoch 840, val loss: 0.9454671144485474
Epoch 850, training loss: 6.129419803619385 = 0.045984283089637756 + 1.0 * 6.083435535430908
Epoch 850, val loss: 0.9514040350914001
Epoch 860, training loss: 6.1264119148254395 = 0.04406184330582619 + 1.0 * 6.082350254058838
Epoch 860, val loss: 0.9573937058448792
Epoch 870, training loss: 6.127159118652344 = 0.042245443910360336 + 1.0 * 6.084913730621338
Epoch 870, val loss: 0.9633569717407227
Epoch 880, training loss: 6.1216888427734375 = 0.04053014516830444 + 1.0 * 6.081158638000488
Epoch 880, val loss: 0.9691537618637085
Epoch 890, training loss: 6.121764659881592 = 0.038918234407901764 + 1.0 * 6.082846641540527
Epoch 890, val loss: 0.9748926162719727
Epoch 900, training loss: 6.118407249450684 = 0.037386491894721985 + 1.0 * 6.081020832061768
Epoch 900, val loss: 0.9808086156845093
Epoch 910, training loss: 6.115114212036133 = 0.03594183921813965 + 1.0 * 6.079172134399414
Epoch 910, val loss: 0.9866713881492615
Epoch 920, training loss: 6.112479209899902 = 0.034574978053569794 + 1.0 * 6.077904224395752
Epoch 920, val loss: 0.9924168586730957
Epoch 930, training loss: 6.116686820983887 = 0.03327687084674835 + 1.0 * 6.083409786224365
Epoch 930, val loss: 0.9981462359428406
Epoch 940, training loss: 6.109585762023926 = 0.03205178305506706 + 1.0 * 6.077534198760986
Epoch 940, val loss: 1.0038267374038696
Epoch 950, training loss: 6.106375217437744 = 0.030887959524989128 + 1.0 * 6.07548713684082
Epoch 950, val loss: 1.0094447135925293
Epoch 960, training loss: 6.110604763031006 = 0.029784047976136208 + 1.0 * 6.080820560455322
Epoch 960, val loss: 1.0150868892669678
Epoch 970, training loss: 6.105762481689453 = 0.028733819723129272 + 1.0 * 6.077028751373291
Epoch 970, val loss: 1.0205833911895752
Epoch 980, training loss: 6.103152275085449 = 0.0277397558093071 + 1.0 * 6.075412750244141
Epoch 980, val loss: 1.026029348373413
Epoch 990, training loss: 6.0990705490112305 = 0.02679119072854519 + 1.0 * 6.072279453277588
Epoch 990, val loss: 1.0314706563949585
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
The final ASR:0.79336, 0.05740, Accuracy:0.78765, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10532])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98155, 0.00301, Accuracy:0.82963, 0.00302
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.325715065002441 = 1.951810598373413 + 1.0 * 8.37390422821045
Epoch 0, val loss: 1.9486088752746582
Epoch 10, training loss: 10.314404487609863 = 1.9408949613571167 + 1.0 * 8.373509407043457
Epoch 10, val loss: 1.9378777742385864
Epoch 20, training loss: 10.297525405883789 = 1.9270998239517212 + 1.0 * 8.3704252243042
Epoch 20, val loss: 1.9243892431259155
Epoch 30, training loss: 10.255396842956543 = 1.9075424671173096 + 1.0 * 8.347854614257812
Epoch 30, val loss: 1.905455470085144
Epoch 40, training loss: 10.079832077026367 = 1.8822815418243408 + 1.0 * 8.197550773620605
Epoch 40, val loss: 1.8818495273590088
Epoch 50, training loss: 9.572625160217285 = 1.8537914752960205 + 1.0 * 7.7188334465026855
Epoch 50, val loss: 1.8556691408157349
Epoch 60, training loss: 9.171429634094238 = 1.8313099145889282 + 1.0 * 7.3401198387146
Epoch 60, val loss: 1.8365060091018677
Epoch 70, training loss: 8.734728813171387 = 1.8169316053390503 + 1.0 * 6.917797088623047
Epoch 70, val loss: 1.8241055011749268
Epoch 80, training loss: 8.523896217346191 = 1.8031028509140015 + 1.0 * 6.720793724060059
Epoch 80, val loss: 1.8126195669174194
Epoch 90, training loss: 8.414579391479492 = 1.7851463556289673 + 1.0 * 6.6294331550598145
Epoch 90, val loss: 1.7982420921325684
Epoch 100, training loss: 8.315269470214844 = 1.7663836479187012 + 1.0 * 6.548885345458984
Epoch 100, val loss: 1.783949851989746
Epoch 110, training loss: 8.236747741699219 = 1.748856544494629 + 1.0 * 6.487890720367432
Epoch 110, val loss: 1.7701820135116577
Epoch 120, training loss: 8.168310165405273 = 1.7302485704421997 + 1.0 * 6.438061237335205
Epoch 120, val loss: 1.7548490762710571
Epoch 130, training loss: 8.109246253967285 = 1.708962082862854 + 1.0 * 6.400284290313721
Epoch 130, val loss: 1.7372219562530518
Epoch 140, training loss: 8.053194046020508 = 1.6836556196212769 + 1.0 * 6.3695387840271
Epoch 140, val loss: 1.7165467739105225
Epoch 150, training loss: 7.99813175201416 = 1.6532546281814575 + 1.0 * 6.344877243041992
Epoch 150, val loss: 1.6919299364089966
Epoch 160, training loss: 7.942262649536133 = 1.617527961730957 + 1.0 * 6.324734687805176
Epoch 160, val loss: 1.6633695363998413
Epoch 170, training loss: 7.880051612854004 = 1.576606035232544 + 1.0 * 6.303445339202881
Epoch 170, val loss: 1.6304876804351807
Epoch 180, training loss: 7.816677093505859 = 1.530394196510315 + 1.0 * 6.286283016204834
Epoch 180, val loss: 1.5933003425598145
Epoch 190, training loss: 7.748909950256348 = 1.4807747602462769 + 1.0 * 6.268135070800781
Epoch 190, val loss: 1.5532896518707275
Epoch 200, training loss: 7.680182456970215 = 1.428910732269287 + 1.0 * 6.251271724700928
Epoch 200, val loss: 1.511406421661377
Epoch 210, training loss: 7.619693756103516 = 1.3769423961639404 + 1.0 * 6.242751121520996
Epoch 210, val loss: 1.4699006080627441
Epoch 220, training loss: 7.557048797607422 = 1.328212022781372 + 1.0 * 6.228837013244629
Epoch 220, val loss: 1.4314496517181396
Epoch 230, training loss: 7.49916410446167 = 1.2818608283996582 + 1.0 * 6.217303276062012
Epoch 230, val loss: 1.395429253578186
Epoch 240, training loss: 7.444979190826416 = 1.237517237663269 + 1.0 * 6.207461833953857
Epoch 240, val loss: 1.3615827560424805
Epoch 250, training loss: 7.399756908416748 = 1.195186734199524 + 1.0 * 6.204570293426514
Epoch 250, val loss: 1.3300718069076538
Epoch 260, training loss: 7.350988388061523 = 1.155665636062622 + 1.0 * 6.1953229904174805
Epoch 260, val loss: 1.3015285730361938
Epoch 270, training loss: 7.304512977600098 = 1.118349313735962 + 1.0 * 6.186163902282715
Epoch 270, val loss: 1.2754440307617188
Epoch 280, training loss: 7.262491703033447 = 1.0826187133789062 + 1.0 * 6.179872989654541
Epoch 280, val loss: 1.2510403394699097
Epoch 290, training loss: 7.231298923492432 = 1.0483425855636597 + 1.0 * 6.182956218719482
Epoch 290, val loss: 1.2279391288757324
Epoch 300, training loss: 7.187306880950928 = 1.0160536766052246 + 1.0 * 6.171253204345703
Epoch 300, val loss: 1.2064588069915771
Epoch 310, training loss: 7.149658679962158 = 0.9852973222732544 + 1.0 * 6.164361476898193
Epoch 310, val loss: 1.1865370273590088
Epoch 320, training loss: 7.122838973999023 = 0.9557914137840271 + 1.0 * 6.167047500610352
Epoch 320, val loss: 1.1674745082855225
Epoch 330, training loss: 7.08527946472168 = 0.9275014400482178 + 1.0 * 6.157778263092041
Epoch 330, val loss: 1.1493381261825562
Epoch 340, training loss: 7.051669120788574 = 0.8997304439544678 + 1.0 * 6.151938438415527
Epoch 340, val loss: 1.131866455078125
Epoch 350, training loss: 7.022428512573242 = 0.8720667958259583 + 1.0 * 6.15036153793335
Epoch 350, val loss: 1.114116907119751
Epoch 360, training loss: 6.990023136138916 = 0.8444570899009705 + 1.0 * 6.145565986633301
Epoch 360, val loss: 1.0964152812957764
Epoch 370, training loss: 6.957422256469727 = 0.8163149356842041 + 1.0 * 6.141107082366943
Epoch 370, val loss: 1.0784775018692017
Epoch 380, training loss: 6.925543785095215 = 0.7873972058296204 + 1.0 * 6.13814640045166
Epoch 380, val loss: 1.0597918033599854
Epoch 390, training loss: 6.89682674407959 = 0.7579638957977295 + 1.0 * 6.138862609863281
Epoch 390, val loss: 1.040396809577942
Epoch 400, training loss: 6.862051963806152 = 0.7282923460006714 + 1.0 * 6.133759498596191
Epoch 400, val loss: 1.0211105346679688
Epoch 410, training loss: 6.8291473388671875 = 0.698460042476654 + 1.0 * 6.130687236785889
Epoch 410, val loss: 1.0017156600952148
Epoch 420, training loss: 6.80214262008667 = 0.6689077019691467 + 1.0 * 6.133234977722168
Epoch 420, val loss: 0.9824131727218628
Epoch 430, training loss: 6.767132759094238 = 0.6400166749954224 + 1.0 * 6.1271162033081055
Epoch 430, val loss: 0.9639655351638794
Epoch 440, training loss: 6.733731269836426 = 0.611548662185669 + 1.0 * 6.122182369232178
Epoch 440, val loss: 0.9460561871528625
Epoch 450, training loss: 6.703545570373535 = 0.5835217833518982 + 1.0 * 6.120023727416992
Epoch 450, val loss: 0.9286586046218872
Epoch 460, training loss: 6.674415588378906 = 0.5560827851295471 + 1.0 * 6.118332862854004
Epoch 460, val loss: 0.9119625091552734
Epoch 470, training loss: 6.64608097076416 = 0.5293508768081665 + 1.0 * 6.116730213165283
Epoch 470, val loss: 0.8962913751602173
Epoch 480, training loss: 6.620165824890137 = 0.503200113773346 + 1.0 * 6.1169657707214355
Epoch 480, val loss: 0.8814112544059753
Epoch 490, training loss: 6.595082759857178 = 0.4778781235218048 + 1.0 * 6.117204666137695
Epoch 490, val loss: 0.8674564361572266
Epoch 500, training loss: 6.564750671386719 = 0.45341894030570984 + 1.0 * 6.111331939697266
Epoch 500, val loss: 0.8549237251281738
Epoch 510, training loss: 6.541801929473877 = 0.4296938180923462 + 1.0 * 6.11210823059082
Epoch 510, val loss: 0.8435996174812317
Epoch 520, training loss: 6.517406463623047 = 0.40685325860977173 + 1.0 * 6.11055326461792
Epoch 520, val loss: 0.8333727121353149
Epoch 530, training loss: 6.490168571472168 = 0.3848394751548767 + 1.0 * 6.1053290367126465
Epoch 530, val loss: 0.8245981931686401
Epoch 540, training loss: 6.4680070877075195 = 0.36360669136047363 + 1.0 * 6.104400634765625
Epoch 540, val loss: 0.8170821070671082
Epoch 550, training loss: 6.4442901611328125 = 0.3430529832839966 + 1.0 * 6.1012372970581055
Epoch 550, val loss: 0.8107181191444397
Epoch 560, training loss: 6.429044246673584 = 0.3231062889099121 + 1.0 * 6.105937957763672
Epoch 560, val loss: 0.805338442325592
Epoch 570, training loss: 6.405988693237305 = 0.3038802146911621 + 1.0 * 6.102108478546143
Epoch 570, val loss: 0.8007723093032837
Epoch 580, training loss: 6.390195846557617 = 0.2853459417819977 + 1.0 * 6.104849815368652
Epoch 580, val loss: 0.7971780300140381
Epoch 590, training loss: 6.365302562713623 = 0.2675362229347229 + 1.0 * 6.097766399383545
Epoch 590, val loss: 0.7942003607749939
Epoch 600, training loss: 6.3449625968933105 = 0.25046640634536743 + 1.0 * 6.094496250152588
Epoch 600, val loss: 0.791924774646759
Epoch 610, training loss: 6.333517074584961 = 0.2341129034757614 + 1.0 * 6.099404335021973
Epoch 610, val loss: 0.7900533080101013
Epoch 620, training loss: 6.313966751098633 = 0.21859769523143768 + 1.0 * 6.0953688621521
Epoch 620, val loss: 0.7885937094688416
Epoch 630, training loss: 6.294445514678955 = 0.20396707952022552 + 1.0 * 6.090478420257568
Epoch 630, val loss: 0.787874698638916
Epoch 640, training loss: 6.2809553146362305 = 0.19019241631031036 + 1.0 * 6.090763092041016
Epoch 640, val loss: 0.7875529527664185
Epoch 650, training loss: 6.268866062164307 = 0.17731793224811554 + 1.0 * 6.091547966003418
Epoch 650, val loss: 0.7876721024513245
Epoch 660, training loss: 6.254190444946289 = 0.16533894836902618 + 1.0 * 6.088851451873779
Epoch 660, val loss: 0.7883819937705994
Epoch 670, training loss: 6.241129398345947 = 0.15426324307918549 + 1.0 * 6.08686637878418
Epoch 670, val loss: 0.7896623611450195
Epoch 680, training loss: 6.232337951660156 = 0.14403486251831055 + 1.0 * 6.088303089141846
Epoch 680, val loss: 0.7911144495010376
Epoch 690, training loss: 6.217640399932861 = 0.13468042016029358 + 1.0 * 6.08296012878418
Epoch 690, val loss: 0.7933346033096313
Epoch 700, training loss: 6.207328796386719 = 0.12609359622001648 + 1.0 * 6.081235408782959
Epoch 700, val loss: 0.7959669828414917
Epoch 710, training loss: 6.213602542877197 = 0.11819899827241898 + 1.0 * 6.095403671264648
Epoch 710, val loss: 0.7989386320114136
Epoch 720, training loss: 6.190340518951416 = 0.11102179437875748 + 1.0 * 6.079318523406982
Epoch 720, val loss: 0.8018807768821716
Epoch 730, training loss: 6.181895732879639 = 0.1044669970870018 + 1.0 * 6.077428817749023
Epoch 730, val loss: 0.8057136535644531
Epoch 740, training loss: 6.174380779266357 = 0.09845256805419922 + 1.0 * 6.075928211212158
Epoch 740, val loss: 0.8096033930778503
Epoch 750, training loss: 6.16817045211792 = 0.09290546178817749 + 1.0 * 6.075264930725098
Epoch 750, val loss: 0.8137533068656921
Epoch 760, training loss: 6.1770734786987305 = 0.08779319375753403 + 1.0 * 6.089280128479004
Epoch 760, val loss: 0.8179675340652466
Epoch 770, training loss: 6.158192157745361 = 0.08310636132955551 + 1.0 * 6.075085639953613
Epoch 770, val loss: 0.8221879601478577
Epoch 780, training loss: 6.151174545288086 = 0.07879551500082016 + 1.0 * 6.072379112243652
Epoch 780, val loss: 0.8269853591918945
Epoch 790, training loss: 6.1450347900390625 = 0.0748060941696167 + 1.0 * 6.070228576660156
Epoch 790, val loss: 0.831853449344635
Epoch 800, training loss: 6.140884876251221 = 0.07109414041042328 + 1.0 * 6.069790840148926
Epoch 800, val loss: 0.8367301821708679
Epoch 810, training loss: 6.139959812164307 = 0.0676373690366745 + 1.0 * 6.072322368621826
Epoch 810, val loss: 0.8417375087738037
Epoch 820, training loss: 6.1386518478393555 = 0.06442159414291382 + 1.0 * 6.074230194091797
Epoch 820, val loss: 0.8465884923934937
Epoch 830, training loss: 6.128914833068848 = 0.06144627183675766 + 1.0 * 6.067468643188477
Epoch 830, val loss: 0.8516978621482849
Epoch 840, training loss: 6.124521732330322 = 0.05867024138569832 + 1.0 * 6.06585168838501
Epoch 840, val loss: 0.8569638729095459
Epoch 850, training loss: 6.121628284454346 = 0.05606485903263092 + 1.0 * 6.065563201904297
Epoch 850, val loss: 0.8621984124183655
Epoch 860, training loss: 6.1326584815979 = 0.05361834540963173 + 1.0 * 6.079040050506592
Epoch 860, val loss: 0.8670879602432251
Epoch 870, training loss: 6.119247913360596 = 0.05134805664420128 + 1.0 * 6.067899703979492
Epoch 870, val loss: 0.8721852898597717
Epoch 880, training loss: 6.111671447753906 = 0.049217481166124344 + 1.0 * 6.062453746795654
Epoch 880, val loss: 0.8775235414505005
Epoch 890, training loss: 6.108433723449707 = 0.04721233993768692 + 1.0 * 6.061221599578857
Epoch 890, val loss: 0.8828368186950684
Epoch 900, training loss: 6.10632848739624 = 0.04531578719615936 + 1.0 * 6.0610127449035645
Epoch 900, val loss: 0.8880864977836609
Epoch 910, training loss: 6.109333515167236 = 0.04352504014968872 + 1.0 * 6.065808296203613
Epoch 910, val loss: 0.893071174621582
Epoch 920, training loss: 6.102600574493408 = 0.041841890662908554 + 1.0 * 6.060758590698242
Epoch 920, val loss: 0.8981971740722656
Epoch 930, training loss: 6.098384380340576 = 0.04025713726878166 + 1.0 * 6.058127403259277
Epoch 930, val loss: 0.9036211371421814
Epoch 940, training loss: 6.097940444946289 = 0.03875540569424629 + 1.0 * 6.059185028076172
Epoch 940, val loss: 0.9089380502700806
Epoch 950, training loss: 6.097785949707031 = 0.03733110427856445 + 1.0 * 6.060454845428467
Epoch 950, val loss: 0.9137693643569946
Epoch 960, training loss: 6.092094898223877 = 0.03598402068018913 + 1.0 * 6.056110858917236
Epoch 960, val loss: 0.9189345240592957
Epoch 970, training loss: 6.090071201324463 = 0.0347096286714077 + 1.0 * 6.055361747741699
Epoch 970, val loss: 0.9242393970489502
Epoch 980, training loss: 6.089117050170898 = 0.03349548578262329 + 1.0 * 6.05562162399292
Epoch 980, val loss: 0.9294556975364685
Epoch 990, training loss: 6.0892558097839355 = 0.03234042972326279 + 1.0 * 6.056915283203125
Epoch 990, val loss: 0.9343610405921936
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.323343276977539 = 1.9494056701660156 + 1.0 * 8.373937606811523
Epoch 0, val loss: 1.9445111751556396
Epoch 10, training loss: 10.312566757202148 = 1.9388364553451538 + 1.0 * 8.373730659484863
Epoch 10, val loss: 1.934553861618042
Epoch 20, training loss: 10.297953605651855 = 1.9258754253387451 + 1.0 * 8.372077941894531
Epoch 20, val loss: 1.9218999147415161
Epoch 30, training loss: 10.26550006866455 = 1.907889723777771 + 1.0 * 8.357610702514648
Epoch 30, val loss: 1.9039795398712158
Epoch 40, training loss: 10.145493507385254 = 1.883831262588501 + 1.0 * 8.261662483215332
Epoch 40, val loss: 1.8805959224700928
Epoch 50, training loss: 9.729671478271484 = 1.8581269979476929 + 1.0 * 7.871544361114502
Epoch 50, val loss: 1.85642409324646
Epoch 60, training loss: 9.355278015136719 = 1.8356471061706543 + 1.0 * 7.519631385803223
Epoch 60, val loss: 1.8362573385238647
Epoch 70, training loss: 8.868937492370605 = 1.8182982206344604 + 1.0 * 7.050639629364014
Epoch 70, val loss: 1.8210293054580688
Epoch 80, training loss: 8.562084197998047 = 1.8041369915008545 + 1.0 * 6.757946968078613
Epoch 80, val loss: 1.808485507965088
Epoch 90, training loss: 8.421164512634277 = 1.7872861623764038 + 1.0 * 6.633878707885742
Epoch 90, val loss: 1.7938501834869385
Epoch 100, training loss: 8.311681747436523 = 1.7682685852050781 + 1.0 * 6.5434136390686035
Epoch 100, val loss: 1.7775458097457886
Epoch 110, training loss: 8.231547355651855 = 1.7492598295211792 + 1.0 * 6.482287406921387
Epoch 110, val loss: 1.7612892389297485
Epoch 120, training loss: 8.171919822692871 = 1.7294518947601318 + 1.0 * 6.44246768951416
Epoch 120, val loss: 1.7443132400512695
Epoch 130, training loss: 8.116601943969727 = 1.707067608833313 + 1.0 * 6.409533977508545
Epoch 130, val loss: 1.7251132726669312
Epoch 140, training loss: 8.064156532287598 = 1.6813933849334717 + 1.0 * 6.382762908935547
Epoch 140, val loss: 1.7033214569091797
Epoch 150, training loss: 8.016261100769043 = 1.6516422033309937 + 1.0 * 6.36461877822876
Epoch 150, val loss: 1.6782978773117065
Epoch 160, training loss: 7.957315921783447 = 1.6176928281784058 + 1.0 * 6.339622974395752
Epoch 160, val loss: 1.6501052379608154
Epoch 170, training loss: 7.900106906890869 = 1.578513741493225 + 1.0 * 6.321593284606934
Epoch 170, val loss: 1.6175389289855957
Epoch 180, training loss: 7.844733238220215 = 1.5335137844085693 + 1.0 * 6.311219692230225
Epoch 180, val loss: 1.5801997184753418
Epoch 190, training loss: 7.77492094039917 = 1.483779788017273 + 1.0 * 6.291141033172607
Epoch 190, val loss: 1.5391554832458496
Epoch 200, training loss: 7.708006381988525 = 1.429909110069275 + 1.0 * 6.278097152709961
Epoch 200, val loss: 1.4949003458023071
Epoch 210, training loss: 7.638403415679932 = 1.3723565340042114 + 1.0 * 6.26604700088501
Epoch 210, val loss: 1.448077917098999
Epoch 220, training loss: 7.5734429359436035 = 1.3121867179870605 + 1.0 * 6.261256217956543
Epoch 220, val loss: 1.3999689817428589
Epoch 230, training loss: 7.500110626220703 = 1.2527765035629272 + 1.0 * 6.247334003448486
Epoch 230, val loss: 1.3531063795089722
Epoch 240, training loss: 7.431766033172607 = 1.1939516067504883 + 1.0 * 6.237814426422119
Epoch 240, val loss: 1.3073029518127441
Epoch 250, training loss: 7.364991188049316 = 1.135380506515503 + 1.0 * 6.229610443115234
Epoch 250, val loss: 1.2621757984161377
Epoch 260, training loss: 7.3054327964782715 = 1.0770797729492188 + 1.0 * 6.228353023529053
Epoch 260, val loss: 1.2177051305770874
Epoch 270, training loss: 7.238365173339844 = 1.0197622776031494 + 1.0 * 6.218603134155273
Epoch 270, val loss: 1.1741831302642822
Epoch 280, training loss: 7.172304153442383 = 0.963035523891449 + 1.0 * 6.209268569946289
Epoch 280, val loss: 1.1312695741653442
Epoch 290, training loss: 7.11310338973999 = 0.9067460298538208 + 1.0 * 6.206357479095459
Epoch 290, val loss: 1.088866114616394
Epoch 300, training loss: 7.0528998374938965 = 0.8521102070808411 + 1.0 * 6.200789451599121
Epoch 300, val loss: 1.0479387044906616
Epoch 310, training loss: 6.994417190551758 = 0.800155758857727 + 1.0 * 6.19426155090332
Epoch 310, val loss: 1.0091564655303955
Epoch 320, training loss: 6.939222812652588 = 0.7510751485824585 + 1.0 * 6.18814754486084
Epoch 320, val loss: 0.9729472994804382
Epoch 330, training loss: 6.888308048248291 = 0.70525062084198 + 1.0 * 6.1830573081970215
Epoch 330, val loss: 0.9394688606262207
Epoch 340, training loss: 6.846766471862793 = 0.6627876162528992 + 1.0 * 6.183979034423828
Epoch 340, val loss: 0.9091123938560486
Epoch 350, training loss: 6.797081470489502 = 0.6239975094795227 + 1.0 * 6.173083782196045
Epoch 350, val loss: 0.882127583026886
Epoch 360, training loss: 6.763535499572754 = 0.5884292125701904 + 1.0 * 6.175106048583984
Epoch 360, val loss: 0.8581834435462952
Epoch 370, training loss: 6.721224784851074 = 0.5560352802276611 + 1.0 * 6.165189743041992
Epoch 370, val loss: 0.8370254635810852
Epoch 380, training loss: 6.687253952026367 = 0.526013195514679 + 1.0 * 6.161240577697754
Epoch 380, val loss: 0.8181852698326111
Epoch 390, training loss: 6.654701232910156 = 0.497870534658432 + 1.0 * 6.156830787658691
Epoch 390, val loss: 0.801223874092102
Epoch 400, training loss: 6.626244068145752 = 0.471402108669281 + 1.0 * 6.154841899871826
Epoch 400, val loss: 0.786014974117279
Epoch 410, training loss: 6.5969157218933105 = 0.4464365243911743 + 1.0 * 6.150479316711426
Epoch 410, val loss: 0.7723786234855652
Epoch 420, training loss: 6.570396900177002 = 0.42263180017471313 + 1.0 * 6.147765159606934
Epoch 420, val loss: 0.7601085305213928
Epoch 430, training loss: 6.543123722076416 = 0.3998847007751465 + 1.0 * 6.1432390213012695
Epoch 430, val loss: 0.7489386796951294
Epoch 440, training loss: 6.517387390136719 = 0.37810567021369934 + 1.0 * 6.139281749725342
Epoch 440, val loss: 0.7391638159751892
Epoch 450, training loss: 6.502206325531006 = 0.3571743667125702 + 1.0 * 6.145031929016113
Epoch 450, val loss: 0.7304413318634033
Epoch 460, training loss: 6.4711689949035645 = 0.33734577894210815 + 1.0 * 6.133823394775391
Epoch 460, val loss: 0.7228881120681763
Epoch 470, training loss: 6.448669910430908 = 0.31840163469314575 + 1.0 * 6.130268096923828
Epoch 470, val loss: 0.7165233492851257
Epoch 480, training loss: 6.427375793457031 = 0.3002545237541199 + 1.0 * 6.127121448516846
Epoch 480, val loss: 0.7111936211585999
Epoch 490, training loss: 6.410057067871094 = 0.2828618288040161 + 1.0 * 6.127195358276367
Epoch 490, val loss: 0.7068120837211609
Epoch 500, training loss: 6.390555381774902 = 0.26626056432724 + 1.0 * 6.124294757843018
Epoch 500, val loss: 0.7033101916313171
Epoch 510, training loss: 6.369785785675049 = 0.25036710500717163 + 1.0 * 6.119418621063232
Epoch 510, val loss: 0.7006398439407349
Epoch 520, training loss: 6.35922908782959 = 0.2351263463497162 + 1.0 * 6.124102592468262
Epoch 520, val loss: 0.6988390684127808
Epoch 530, training loss: 6.340716361999512 = 0.22056622803211212 + 1.0 * 6.120150089263916
Epoch 530, val loss: 0.6977730989456177
Epoch 540, training loss: 6.31980037689209 = 0.20675471425056458 + 1.0 * 6.113045692443848
Epoch 540, val loss: 0.6974785923957825
Epoch 550, training loss: 6.304813861846924 = 0.19360116124153137 + 1.0 * 6.111212730407715
Epoch 550, val loss: 0.6980137825012207
Epoch 560, training loss: 6.297065258026123 = 0.1810884177684784 + 1.0 * 6.115976810455322
Epoch 560, val loss: 0.6992658972740173
Epoch 570, training loss: 6.279919147491455 = 0.16932404041290283 + 1.0 * 6.110595226287842
Epoch 570, val loss: 0.7011640071868896
Epoch 580, training loss: 6.263791561126709 = 0.1583266258239746 + 1.0 * 6.105464935302734
Epoch 580, val loss: 0.7037661671638489
Epoch 590, training loss: 6.251192092895508 = 0.14804935455322266 + 1.0 * 6.103142738342285
Epoch 590, val loss: 0.7068964242935181
Epoch 600, training loss: 6.239406585693359 = 0.13846156001091003 + 1.0 * 6.100944995880127
Epoch 600, val loss: 0.7107094526290894
Epoch 610, training loss: 6.232525825500488 = 0.12953807413578033 + 1.0 * 6.102987766265869
Epoch 610, val loss: 0.7150529026985168
Epoch 620, training loss: 6.228852272033691 = 0.12129725515842438 + 1.0 * 6.107554912567139
Epoch 620, val loss: 0.7197901010513306
Epoch 630, training loss: 6.212654113769531 = 0.11375318467617035 + 1.0 * 6.09890079498291
Epoch 630, val loss: 0.7247668504714966
Epoch 640, training loss: 6.202057838439941 = 0.10677477717399597 + 1.0 * 6.095283031463623
Epoch 640, val loss: 0.7301357388496399
Epoch 650, training loss: 6.196683406829834 = 0.1003134697675705 + 1.0 * 6.096369743347168
Epoch 650, val loss: 0.73589026927948
Epoch 660, training loss: 6.185676574707031 = 0.09436670690774918 + 1.0 * 6.091310024261475
Epoch 660, val loss: 0.7418466210365295
Epoch 670, training loss: 6.186242580413818 = 0.08886494487524033 + 1.0 * 6.097377777099609
Epoch 670, val loss: 0.7480224370956421
Epoch 680, training loss: 6.176915168762207 = 0.08380323648452759 + 1.0 * 6.093111991882324
Epoch 680, val loss: 0.754172146320343
Epoch 690, training loss: 6.165799617767334 = 0.0791328027844429 + 1.0 * 6.086666584014893
Epoch 690, val loss: 0.7604773640632629
Epoch 700, training loss: 6.159891128540039 = 0.07479984313249588 + 1.0 * 6.0850911140441895
Epoch 700, val loss: 0.7669398188591003
Epoch 710, training loss: 6.157122611999512 = 0.07078097015619278 + 1.0 * 6.086341857910156
Epoch 710, val loss: 0.7734718322753906
Epoch 720, training loss: 6.153503894805908 = 0.06705550104379654 + 1.0 * 6.0864481925964355
Epoch 720, val loss: 0.7798686027526855
Epoch 730, training loss: 6.145157337188721 = 0.06361284106969833 + 1.0 * 6.081544399261475
Epoch 730, val loss: 0.7862430810928345
Epoch 740, training loss: 6.142783164978027 = 0.060409680008888245 + 1.0 * 6.08237361907959
Epoch 740, val loss: 0.792717456817627
Epoch 750, training loss: 6.13632869720459 = 0.05742701515555382 + 1.0 * 6.078901767730713
Epoch 750, val loss: 0.7991507649421692
Epoch 760, training loss: 6.132790565490723 = 0.054653000086545944 + 1.0 * 6.078137397766113
Epoch 760, val loss: 0.8054887652397156
Epoch 770, training loss: 6.130980491638184 = 0.05206582322716713 + 1.0 * 6.078914642333984
Epoch 770, val loss: 0.8118364214897156
Epoch 780, training loss: 6.125176429748535 = 0.04964550584554672 + 1.0 * 6.075531005859375
Epoch 780, val loss: 0.818030834197998
Epoch 790, training loss: 6.120791912078857 = 0.04738577827811241 + 1.0 * 6.073406219482422
Epoch 790, val loss: 0.8243298530578613
Epoch 800, training loss: 6.134891510009766 = 0.04527253285050392 + 1.0 * 6.089619159698486
Epoch 800, val loss: 0.830569326877594
Epoch 810, training loss: 6.1190714836120605 = 0.04329725727438927 + 1.0 * 6.075774192810059
Epoch 810, val loss: 0.836499810218811
Epoch 820, training loss: 6.112887859344482 = 0.04145127907395363 + 1.0 * 6.071436405181885
Epoch 820, val loss: 0.8423498868942261
Epoch 830, training loss: 6.10892915725708 = 0.039712272584438324 + 1.0 * 6.069216728210449
Epoch 830, val loss: 0.8483137488365173
Epoch 840, training loss: 6.114861965179443 = 0.03807228058576584 + 1.0 * 6.076789855957031
Epoch 840, val loss: 0.8542357087135315
Epoch 850, training loss: 6.106942176818848 = 0.03654051199555397 + 1.0 * 6.070401668548584
Epoch 850, val loss: 0.8599900603294373
Epoch 860, training loss: 6.103461265563965 = 0.035090371966362 + 1.0 * 6.068370819091797
Epoch 860, val loss: 0.8656724691390991
Epoch 870, training loss: 6.1089630126953125 = 0.033726830035448074 + 1.0 * 6.0752363204956055
Epoch 870, val loss: 0.8713776469230652
Epoch 880, training loss: 6.099242210388184 = 0.03243760019540787 + 1.0 * 6.0668044090271
Epoch 880, val loss: 0.8768705725669861
Epoch 890, training loss: 6.094702243804932 = 0.031222714111208916 + 1.0 * 6.063479423522949
Epoch 890, val loss: 0.88238126039505
Epoch 900, training loss: 6.093164920806885 = 0.030069252476096153 + 1.0 * 6.063095569610596
Epoch 900, val loss: 0.8879004716873169
Epoch 910, training loss: 6.105127811431885 = 0.028982924297451973 + 1.0 * 6.076144695281982
Epoch 910, val loss: 0.8933329582214355
Epoch 920, training loss: 6.090814590454102 = 0.02794882468879223 + 1.0 * 6.062865734100342
Epoch 920, val loss: 0.8984805345535278
Epoch 930, training loss: 6.087373733520508 = 0.026974553242325783 + 1.0 * 6.060399055480957
Epoch 930, val loss: 0.9036197662353516
Epoch 940, training loss: 6.089297294616699 = 0.02604985423386097 + 1.0 * 6.063247203826904
Epoch 940, val loss: 0.9088518619537354
Epoch 950, training loss: 6.083288669586182 = 0.025169702246785164 + 1.0 * 6.05811882019043
Epoch 950, val loss: 0.9139784574508667
Epoch 960, training loss: 6.082974910736084 = 0.02433343231678009 + 1.0 * 6.05864143371582
Epoch 960, val loss: 0.9190500974655151
Epoch 970, training loss: 6.091111183166504 = 0.023536324501037598 + 1.0 * 6.067574977874756
Epoch 970, val loss: 0.9241093993186951
Epoch 980, training loss: 6.088799953460693 = 0.022785387933254242 + 1.0 * 6.066014766693115
Epoch 980, val loss: 0.928963303565979
Epoch 990, training loss: 6.077868461608887 = 0.0220663920044899 + 1.0 * 6.055801868438721
Epoch 990, val loss: 0.9336768984794617
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.6716
Flip ASR: 0.6133/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.319957733154297 = 1.946051001548767 + 1.0 * 8.373907089233398
Epoch 0, val loss: 1.9451239109039307
Epoch 10, training loss: 10.309398651123047 = 1.9358855485916138 + 1.0 * 8.373513221740723
Epoch 10, val loss: 1.9345533847808838
Epoch 20, training loss: 10.294408798217773 = 1.9234317541122437 + 1.0 * 8.370977401733398
Epoch 20, val loss: 1.920904517173767
Epoch 30, training loss: 10.261516571044922 = 1.9065102338790894 + 1.0 * 8.355006217956543
Epoch 30, val loss: 1.9017720222473145
Epoch 40, training loss: 10.151139259338379 = 1.8844287395477295 + 1.0 * 8.26671028137207
Epoch 40, val loss: 1.8774139881134033
Epoch 50, training loss: 9.742949485778809 = 1.8618910312652588 + 1.0 * 7.881058216094971
Epoch 50, val loss: 1.8537898063659668
Epoch 60, training loss: 9.469470977783203 = 1.8402915000915527 + 1.0 * 7.629179954528809
Epoch 60, val loss: 1.832243800163269
Epoch 70, training loss: 8.996950149536133 = 1.8200817108154297 + 1.0 * 7.176868438720703
Epoch 70, val loss: 1.8115519285202026
Epoch 80, training loss: 8.625848770141602 = 1.8012583255767822 + 1.0 * 6.824590682983398
Epoch 80, val loss: 1.7922683954238892
Epoch 90, training loss: 8.472821235656738 = 1.7827199697494507 + 1.0 * 6.690101623535156
Epoch 90, val loss: 1.7734214067459106
Epoch 100, training loss: 8.362038612365723 = 1.7626070976257324 + 1.0 * 6.59943151473999
Epoch 100, val loss: 1.7539730072021484
Epoch 110, training loss: 8.278609275817871 = 1.7418570518493652 + 1.0 * 6.536752223968506
Epoch 110, val loss: 1.734788417816162
Epoch 120, training loss: 8.208898544311523 = 1.7198679447174072 + 1.0 * 6.489030838012695
Epoch 120, val loss: 1.7151826620101929
Epoch 130, training loss: 8.141780853271484 = 1.6956950426101685 + 1.0 * 6.446085453033447
Epoch 130, val loss: 1.6945171356201172
Epoch 140, training loss: 8.073892593383789 = 1.66879403591156 + 1.0 * 6.4050984382629395
Epoch 140, val loss: 1.6718372106552124
Epoch 150, training loss: 8.008892059326172 = 1.6373425722122192 + 1.0 * 6.371549606323242
Epoch 150, val loss: 1.6458429098129272
Epoch 160, training loss: 7.947054862976074 = 1.6001523733139038 + 1.0 * 6.346902370452881
Epoch 160, val loss: 1.6157152652740479
Epoch 170, training loss: 7.8802947998046875 = 1.5568525791168213 + 1.0 * 6.323441982269287
Epoch 170, val loss: 1.5806442499160767
Epoch 180, training loss: 7.811664581298828 = 1.5068166255950928 + 1.0 * 6.304847717285156
Epoch 180, val loss: 1.540299654006958
Epoch 190, training loss: 7.738929271697998 = 1.4509297609329224 + 1.0 * 6.287999629974365
Epoch 190, val loss: 1.495674967765808
Epoch 200, training loss: 7.66212797164917 = 1.3904935121536255 + 1.0 * 6.271634578704834
Epoch 200, val loss: 1.4476031064987183
Epoch 210, training loss: 7.590771198272705 = 1.3264431953430176 + 1.0 * 6.2643280029296875
Epoch 210, val loss: 1.3969459533691406
Epoch 220, training loss: 7.509373664855957 = 1.2618329524993896 + 1.0 * 6.2475409507751465
Epoch 220, val loss: 1.34684157371521
Epoch 230, training loss: 7.434100151062012 = 1.1976028680801392 + 1.0 * 6.236497402191162
Epoch 230, val loss: 1.2973333597183228
Epoch 240, training loss: 7.3614983558654785 = 1.13401460647583 + 1.0 * 6.227483749389648
Epoch 240, val loss: 1.2487156391143799
Epoch 250, training loss: 7.294763565063477 = 1.0718209743499756 + 1.0 * 6.22294282913208
Epoch 250, val loss: 1.2015661001205444
Epoch 260, training loss: 7.225554943084717 = 1.0127878189086914 + 1.0 * 6.212767124176025
Epoch 260, val loss: 1.1573342084884644
Epoch 270, training loss: 7.16231632232666 = 0.9571303129196167 + 1.0 * 6.205185890197754
Epoch 270, val loss: 1.116404414176941
Epoch 280, training loss: 7.1053056716918945 = 0.9047707319259644 + 1.0 * 6.200534820556641
Epoch 280, val loss: 1.0787194967269897
Epoch 290, training loss: 7.055015563964844 = 0.8565853834152222 + 1.0 * 6.198430061340332
Epoch 290, val loss: 1.044742226600647
Epoch 300, training loss: 7.001131057739258 = 0.812595009803772 + 1.0 * 6.188536167144775
Epoch 300, val loss: 1.0149778127670288
Epoch 310, training loss: 6.954315662384033 = 0.7723259925842285 + 1.0 * 6.181989669799805
Epoch 310, val loss: 0.9890685081481934
Epoch 320, training loss: 6.914610385894775 = 0.7355636358261108 + 1.0 * 6.179046630859375
Epoch 320, val loss: 0.9665073156356812
Epoch 330, training loss: 6.875250816345215 = 0.7022170424461365 + 1.0 * 6.173033714294434
Epoch 330, val loss: 0.9472376704216003
Epoch 340, training loss: 6.840508460998535 = 0.6717560291290283 + 1.0 * 6.168752193450928
Epoch 340, val loss: 0.930918276309967
Epoch 350, training loss: 6.807783126831055 = 0.6438514590263367 + 1.0 * 6.163931846618652
Epoch 350, val loss: 0.9171366095542908
Epoch 360, training loss: 6.775838375091553 = 0.617781937122345 + 1.0 * 6.158056259155273
Epoch 360, val loss: 0.9052521586418152
Epoch 370, training loss: 6.747326374053955 = 0.593077540397644 + 1.0 * 6.1542487144470215
Epoch 370, val loss: 0.8947837352752686
Epoch 380, training loss: 6.724309921264648 = 0.569511353969574 + 1.0 * 6.15479850769043
Epoch 380, val loss: 0.8856602311134338
Epoch 390, training loss: 6.694978713989258 = 0.547082781791687 + 1.0 * 6.147895812988281
Epoch 390, val loss: 0.8775714635848999
Epoch 400, training loss: 6.669686317443848 = 0.5254696011543274 + 1.0 * 6.144216537475586
Epoch 400, val loss: 0.8703115582466125
Epoch 410, training loss: 6.643728256225586 = 0.5045481324195862 + 1.0 * 6.1391801834106445
Epoch 410, val loss: 0.8637587428092957
Epoch 420, training loss: 6.620339393615723 = 0.4840507209300995 + 1.0 * 6.136288642883301
Epoch 420, val loss: 0.8580380082130432
Epoch 430, training loss: 6.599428653717041 = 0.4639994204044342 + 1.0 * 6.135429382324219
Epoch 430, val loss: 0.85282963514328
Epoch 440, training loss: 6.574361801147461 = 0.4443243443965912 + 1.0 * 6.130037307739258
Epoch 440, val loss: 0.8483328223228455
Epoch 450, training loss: 6.551440715789795 = 0.42499881982803345 + 1.0 * 6.126441955566406
Epoch 450, val loss: 0.8445947170257568
Epoch 460, training loss: 6.5300188064575195 = 0.4059876799583435 + 1.0 * 6.124031066894531
Epoch 460, val loss: 0.8416330218315125
Epoch 470, training loss: 6.509650707244873 = 0.38753461837768555 + 1.0 * 6.1221160888671875
Epoch 470, val loss: 0.8395232558250427
Epoch 480, training loss: 6.489530563354492 = 0.36992743611335754 + 1.0 * 6.119603157043457
Epoch 480, val loss: 0.8387265801429749
Epoch 490, training loss: 6.473480224609375 = 0.3531295359134674 + 1.0 * 6.1203508377075195
Epoch 490, val loss: 0.8390756845474243
Epoch 500, training loss: 6.452820777893066 = 0.33726194500923157 + 1.0 * 6.115558624267578
Epoch 500, val loss: 0.8402422070503235
Epoch 510, training loss: 6.434811592102051 = 0.3222154974937439 + 1.0 * 6.112596035003662
Epoch 510, val loss: 0.8423610925674438
Epoch 520, training loss: 6.4215803146362305 = 0.3079476058483124 + 1.0 * 6.113632678985596
Epoch 520, val loss: 0.8452371954917908
Epoch 530, training loss: 6.403801918029785 = 0.2945179045200348 + 1.0 * 6.109283924102783
Epoch 530, val loss: 0.8486535549163818
Epoch 540, training loss: 6.388591289520264 = 0.28179070353507996 + 1.0 * 6.106800556182861
Epoch 540, val loss: 0.8528285026550293
Epoch 550, training loss: 6.380617141723633 = 0.2697092890739441 + 1.0 * 6.110908031463623
Epoch 550, val loss: 0.8574224710464478
Epoch 560, training loss: 6.361423492431641 = 0.25815802812576294 + 1.0 * 6.103265285491943
Epoch 560, val loss: 0.862488329410553
Epoch 570, training loss: 6.34913444519043 = 0.2470453530550003 + 1.0 * 6.102088928222656
Epoch 570, val loss: 0.867812991142273
Epoch 580, training loss: 6.3427042961120605 = 0.23629048466682434 + 1.0 * 6.106413841247559
Epoch 580, val loss: 0.8733762502670288
Epoch 590, training loss: 6.327418804168701 = 0.22579248249530792 + 1.0 * 6.101626396179199
Epoch 590, val loss: 0.8790692090988159
Epoch 600, training loss: 6.312554359436035 = 0.21556060016155243 + 1.0 * 6.096993923187256
Epoch 600, val loss: 0.8850081562995911
Epoch 610, training loss: 6.307907581329346 = 0.20545339584350586 + 1.0 * 6.10245418548584
Epoch 610, val loss: 0.8909853100776672
Epoch 620, training loss: 6.291791915893555 = 0.1955185830593109 + 1.0 * 6.096273422241211
Epoch 620, val loss: 0.8969588875770569
Epoch 630, training loss: 6.277563095092773 = 0.18573066592216492 + 1.0 * 6.091832637786865
Epoch 630, val loss: 0.9030534625053406
Epoch 640, training loss: 6.265316009521484 = 0.1760895699262619 + 1.0 * 6.089226245880127
Epoch 640, val loss: 0.9092705845832825
Epoch 650, training loss: 6.270064830780029 = 0.16665740311145782 + 1.0 * 6.103407382965088
Epoch 650, val loss: 0.9154750108718872
Epoch 660, training loss: 6.24664306640625 = 0.15753012895584106 + 1.0 * 6.089112758636475
Epoch 660, val loss: 0.9217988848686218
Epoch 670, training loss: 6.234034061431885 = 0.14874376356601715 + 1.0 * 6.085290431976318
Epoch 670, val loss: 0.9283645153045654
Epoch 680, training loss: 6.23897123336792 = 0.1403113305568695 + 1.0 * 6.098659992218018
Epoch 680, val loss: 0.9350700378417969
Epoch 690, training loss: 6.219091415405273 = 0.1323697715997696 + 1.0 * 6.086721420288086
Epoch 690, val loss: 0.9418784379959106
Epoch 700, training loss: 6.2069411277771 = 0.12484302371740341 + 1.0 * 6.082098007202148
Epoch 700, val loss: 0.9490496516227722
Epoch 710, training loss: 6.198176860809326 = 0.11776556074619293 + 1.0 * 6.080411434173584
Epoch 710, val loss: 0.9565054178237915
Epoch 720, training loss: 6.191563129425049 = 0.11112011969089508 + 1.0 * 6.080442905426025
Epoch 720, val loss: 0.9641534686088562
Epoch 730, training loss: 6.1846089363098145 = 0.1049153059720993 + 1.0 * 6.079693794250488
Epoch 730, val loss: 0.9717922806739807
Epoch 740, training loss: 6.177593231201172 = 0.09917454421520233 + 1.0 * 6.078418731689453
Epoch 740, val loss: 0.9796930551528931
Epoch 750, training loss: 6.170411586761475 = 0.09382259100675583 + 1.0 * 6.076589107513428
Epoch 750, val loss: 0.9878097176551819
Epoch 760, training loss: 6.16450309753418 = 0.08883020281791687 + 1.0 * 6.0756731033325195
Epoch 760, val loss: 0.9959924817085266
Epoch 770, training loss: 6.158779144287109 = 0.08419628441333771 + 1.0 * 6.074583053588867
Epoch 770, val loss: 1.0040769577026367
Epoch 780, training loss: 6.153972148895264 = 0.07989594340324402 + 1.0 * 6.074076175689697
Epoch 780, val loss: 1.0123363733291626
Epoch 790, training loss: 6.147107124328613 = 0.07588724046945572 + 1.0 * 6.0712199211120605
Epoch 790, val loss: 1.0206466913223267
Epoch 800, training loss: 6.160683631896973 = 0.07215579599142075 + 1.0 * 6.088527679443359
Epoch 800, val loss: 1.0288569927215576
Epoch 810, training loss: 6.140617370605469 = 0.06866874545812607 + 1.0 * 6.071948528289795
Epoch 810, val loss: 1.0368988513946533
Epoch 820, training loss: 6.134362697601318 = 0.0654304251074791 + 1.0 * 6.068932056427002
Epoch 820, val loss: 1.0451147556304932
Epoch 830, training loss: 6.130186557769775 = 0.062391482293605804 + 1.0 * 6.067795276641846
Epoch 830, val loss: 1.0532978773117065
Epoch 840, training loss: 6.125853538513184 = 0.059541430324316025 + 1.0 * 6.066312313079834
Epoch 840, val loss: 1.0614150762557983
Epoch 850, training loss: 6.131816387176514 = 0.056867942214012146 + 1.0 * 6.074948310852051
Epoch 850, val loss: 1.0693563222885132
Epoch 860, training loss: 6.119666576385498 = 0.054361648857593536 + 1.0 * 6.065304756164551
Epoch 860, val loss: 1.077241063117981
Epoch 870, training loss: 6.115384101867676 = 0.052010100334882736 + 1.0 * 6.063374042510986
Epoch 870, val loss: 1.085132122039795
Epoch 880, training loss: 6.12408447265625 = 0.04979739710688591 + 1.0 * 6.074286937713623
Epoch 880, val loss: 1.092909812927246
Epoch 890, training loss: 6.113466739654541 = 0.04771021753549576 + 1.0 * 6.065756320953369
Epoch 890, val loss: 1.1004104614257812
Epoch 900, training loss: 6.10660457611084 = 0.0457550585269928 + 1.0 * 6.060849666595459
Epoch 900, val loss: 1.108012318611145
Epoch 910, training loss: 6.1050920486450195 = 0.04390927031636238 + 1.0 * 6.061182975769043
Epoch 910, val loss: 1.1155437231063843
Epoch 920, training loss: 6.110873699188232 = 0.042160648852586746 + 1.0 * 6.068713188171387
Epoch 920, val loss: 1.1228296756744385
Epoch 930, training loss: 6.102267265319824 = 0.04051889106631279 + 1.0 * 6.061748504638672
Epoch 930, val loss: 1.1299660205841064
Epoch 940, training loss: 6.097446918487549 = 0.03895622119307518 + 1.0 * 6.058490753173828
Epoch 940, val loss: 1.1371862888336182
Epoch 950, training loss: 6.095515727996826 = 0.0374673455953598 + 1.0 * 6.058048248291016
Epoch 950, val loss: 1.1443113088607788
Epoch 960, training loss: 6.104348659515381 = 0.036048758774995804 + 1.0 * 6.068299770355225
Epoch 960, val loss: 1.1511646509170532
Epoch 970, training loss: 6.092878818511963 = 0.03469707444310188 + 1.0 * 6.0581817626953125
Epoch 970, val loss: 1.1577366590499878
Epoch 980, training loss: 6.088590145111084 = 0.03341959789395332 + 1.0 * 6.05517053604126
Epoch 980, val loss: 1.1644283533096313
Epoch 990, training loss: 6.085925579071045 = 0.03220491111278534 + 1.0 * 6.053720474243164
Epoch 990, val loss: 1.171083688735962
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7481
Overall ASR: 0.6937
Flip ASR: 0.6356/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.329737663269043 = 1.9558380842208862 + 1.0 * 8.373899459838867
Epoch 0, val loss: 1.9603039026260376
Epoch 10, training loss: 10.31812572479248 = 1.9445134401321411 + 1.0 * 8.373612403869629
Epoch 10, val loss: 1.9487700462341309
Epoch 20, training loss: 10.302071571350098 = 1.9303703308105469 + 1.0 * 8.37170124053955
Epoch 20, val loss: 1.9341531991958618
Epoch 30, training loss: 10.267868041992188 = 1.9101625680923462 + 1.0 * 8.357705116271973
Epoch 30, val loss: 1.9133353233337402
Epoch 40, training loss: 10.161794662475586 = 1.8818119764328003 + 1.0 * 8.279982566833496
Epoch 40, val loss: 1.8854141235351562
Epoch 50, training loss: 9.741999626159668 = 1.8507640361785889 + 1.0 * 7.891235828399658
Epoch 50, val loss: 1.8563095331192017
Epoch 60, training loss: 9.459860801696777 = 1.8208446502685547 + 1.0 * 7.639016151428223
Epoch 60, val loss: 1.8304498195648193
Epoch 70, training loss: 9.08810043334961 = 1.7980369329452515 + 1.0 * 7.290063381195068
Epoch 70, val loss: 1.8111194372177124
Epoch 80, training loss: 8.702138900756836 = 1.7834429740905762 + 1.0 * 6.91869592666626
Epoch 80, val loss: 1.7990667819976807
Epoch 90, training loss: 8.473873138427734 = 1.7696785926818848 + 1.0 * 6.704194068908691
Epoch 90, val loss: 1.7856826782226562
Epoch 100, training loss: 8.359098434448242 = 1.7500942945480347 + 1.0 * 6.609004497528076
Epoch 100, val loss: 1.767177939414978
Epoch 110, training loss: 8.26790714263916 = 1.727778434753418 + 1.0 * 6.540128707885742
Epoch 110, val loss: 1.7474842071533203
Epoch 120, training loss: 8.190765380859375 = 1.7037135362625122 + 1.0 * 6.487051486968994
Epoch 120, val loss: 1.7267987728118896
Epoch 130, training loss: 8.121040344238281 = 1.6772630214691162 + 1.0 * 6.443777084350586
Epoch 130, val loss: 1.7043050527572632
Epoch 140, training loss: 8.052635192871094 = 1.6470578908920288 + 1.0 * 6.405577659606934
Epoch 140, val loss: 1.6789467334747314
Epoch 150, training loss: 7.985255718231201 = 1.612675666809082 + 1.0 * 6.372580051422119
Epoch 150, val loss: 1.6505212783813477
Epoch 160, training loss: 7.919325351715088 = 1.5739439725875854 + 1.0 * 6.345381259918213
Epoch 160, val loss: 1.618913173675537
Epoch 170, training loss: 7.854641914367676 = 1.530773401260376 + 1.0 * 6.323868274688721
Epoch 170, val loss: 1.5841400623321533
Epoch 180, training loss: 7.792572975158691 = 1.4843883514404297 + 1.0 * 6.308184623718262
Epoch 180, val loss: 1.5474456548690796
Epoch 190, training loss: 7.724462985992432 = 1.4366254806518555 + 1.0 * 6.287837505340576
Epoch 190, val loss: 1.5103174448013306
Epoch 200, training loss: 7.659482002258301 = 1.3876291513442993 + 1.0 * 6.271852970123291
Epoch 200, val loss: 1.472999095916748
Epoch 210, training loss: 7.598104476928711 = 1.3376224040985107 + 1.0 * 6.260481834411621
Epoch 210, val loss: 1.4359408617019653
Epoch 220, training loss: 7.535820484161377 = 1.2886461019515991 + 1.0 * 6.247174263000488
Epoch 220, val loss: 1.400420904159546
Epoch 230, training loss: 7.475607872009277 = 1.240567684173584 + 1.0 * 6.235040187835693
Epoch 230, val loss: 1.3661597967147827
Epoch 240, training loss: 7.417506694793701 = 1.19254732131958 + 1.0 * 6.224959373474121
Epoch 240, val loss: 1.332145094871521
Epoch 250, training loss: 7.360607624053955 = 1.1443349123001099 + 1.0 * 6.216272830963135
Epoch 250, val loss: 1.2980637550354004
Epoch 260, training loss: 7.305105209350586 = 1.096078634262085 + 1.0 * 6.209026336669922
Epoch 260, val loss: 1.264082431793213
Epoch 270, training loss: 7.2579665184021 = 1.0485800504684448 + 1.0 * 6.209386348724365
Epoch 270, val loss: 1.2306102514266968
Epoch 280, training loss: 7.200098037719727 = 1.0027180910110474 + 1.0 * 6.197380065917969
Epoch 280, val loss: 1.197908639907837
Epoch 290, training loss: 7.147959232330322 = 0.9577019810676575 + 1.0 * 6.1902570724487305
Epoch 290, val loss: 1.1655499935150146
Epoch 300, training loss: 7.098293781280518 = 0.9132870435714722 + 1.0 * 6.185006618499756
Epoch 300, val loss: 1.1333235502243042
Epoch 310, training loss: 7.0579447746276855 = 0.8694233894348145 + 1.0 * 6.188521385192871
Epoch 310, val loss: 1.1011474132537842
Epoch 320, training loss: 7.003337860107422 = 0.8267786502838135 + 1.0 * 6.1765594482421875
Epoch 320, val loss: 1.0694423913955688
Epoch 330, training loss: 6.9558868408203125 = 0.784882664680481 + 1.0 * 6.171004295349121
Epoch 330, val loss: 1.03825044631958
Epoch 340, training loss: 6.919373989105225 = 0.7437124848365784 + 1.0 * 6.175661563873291
Epoch 340, val loss: 1.007527232170105
Epoch 350, training loss: 6.870183944702148 = 0.7039617896080017 + 1.0 * 6.166222095489502
Epoch 350, val loss: 0.9778539538383484
Epoch 360, training loss: 6.825906753540039 = 0.6654317378997803 + 1.0 * 6.160475254058838
Epoch 360, val loss: 0.9495760202407837
Epoch 370, training loss: 6.786856174468994 = 0.6281072497367859 + 1.0 * 6.158749103546143
Epoch 370, val loss: 0.9226223826408386
Epoch 380, training loss: 6.7468719482421875 = 0.5923787355422974 + 1.0 * 6.15449333190918
Epoch 380, val loss: 0.8973321914672852
Epoch 390, training loss: 6.709534645080566 = 0.5580951571464539 + 1.0 * 6.151439666748047
Epoch 390, val loss: 0.8740274310112
Epoch 400, training loss: 6.672625541687012 = 0.5254544615745544 + 1.0 * 6.1471710205078125
Epoch 400, val loss: 0.8527472615242004
Epoch 410, training loss: 6.641092300415039 = 0.49426013231277466 + 1.0 * 6.14683198928833
Epoch 410, val loss: 0.8336936235427856
Epoch 420, training loss: 6.606369495391846 = 0.46452730894088745 + 1.0 * 6.141842365264893
Epoch 420, val loss: 0.8167725801467896
Epoch 430, training loss: 6.576173782348633 = 0.4360286593437195 + 1.0 * 6.140145301818848
Epoch 430, val loss: 0.8018484115600586
Epoch 440, training loss: 6.5449604988098145 = 0.4088187515735626 + 1.0 * 6.136141777038574
Epoch 440, val loss: 0.7888978719711304
Epoch 450, training loss: 6.52635383605957 = 0.38273167610168457 + 1.0 * 6.143622398376465
Epoch 450, val loss: 0.7777926325798035
Epoch 460, training loss: 6.4906158447265625 = 0.35804322361946106 + 1.0 * 6.132572650909424
Epoch 460, val loss: 0.7684754729270935
Epoch 470, training loss: 6.467174530029297 = 0.33458638191223145 + 1.0 * 6.1325883865356445
Epoch 470, val loss: 0.7609513998031616
Epoch 480, training loss: 6.441584587097168 = 0.3123398721218109 + 1.0 * 6.129244804382324
Epoch 480, val loss: 0.7550382614135742
Epoch 490, training loss: 6.414127826690674 = 0.2913503646850586 + 1.0 * 6.122777462005615
Epoch 490, val loss: 0.7505796551704407
Epoch 500, training loss: 6.392448425292969 = 0.2714953124523163 + 1.0 * 6.12095308303833
Epoch 500, val loss: 0.7475175261497498
Epoch 510, training loss: 6.370420932769775 = 0.2528150677680969 + 1.0 * 6.117605686187744
Epoch 510, val loss: 0.7456455230712891
Epoch 520, training loss: 6.352762699127197 = 0.23532937467098236 + 1.0 * 6.117433547973633
Epoch 520, val loss: 0.7450719475746155
Epoch 530, training loss: 6.331968784332275 = 0.2188759595155716 + 1.0 * 6.11309289932251
Epoch 530, val loss: 0.7456197738647461
Epoch 540, training loss: 6.325934886932373 = 0.20349127054214478 + 1.0 * 6.122443675994873
Epoch 540, val loss: 0.7471609711647034
Epoch 550, training loss: 6.312010288238525 = 0.18927739560604095 + 1.0 * 6.122733116149902
Epoch 550, val loss: 0.7497046589851379
Epoch 560, training loss: 6.286894798278809 = 0.17621926963329315 + 1.0 * 6.11067533493042
Epoch 560, val loss: 0.753173828125
Epoch 570, training loss: 6.270226955413818 = 0.16409730911254883 + 1.0 * 6.1061296463012695
Epoch 570, val loss: 0.7575090527534485
Epoch 580, training loss: 6.256749153137207 = 0.15285511314868927 + 1.0 * 6.103894233703613
Epoch 580, val loss: 0.7625084519386292
Epoch 590, training loss: 6.247408390045166 = 0.14246350526809692 + 1.0 * 6.104944705963135
Epoch 590, val loss: 0.768114447593689
Epoch 600, training loss: 6.244187355041504 = 0.13296204805374146 + 1.0 * 6.111225128173828
Epoch 600, val loss: 0.7742505669593811
Epoch 610, training loss: 6.22498083114624 = 0.12429298460483551 + 1.0 * 6.1006879806518555
Epoch 610, val loss: 0.7807608842849731
Epoch 620, training loss: 6.2135539054870605 = 0.11632031947374344 + 1.0 * 6.097233772277832
Epoch 620, val loss: 0.7876374125480652
Epoch 630, training loss: 6.20695161819458 = 0.10896574705839157 + 1.0 * 6.097985744476318
Epoch 630, val loss: 0.7947850823402405
Epoch 640, training loss: 6.205777645111084 = 0.10224955528974533 + 1.0 * 6.103528022766113
Epoch 640, val loss: 0.8020680546760559
Epoch 650, training loss: 6.189979553222656 = 0.09612264484167099 + 1.0 * 6.0938568115234375
Epoch 650, val loss: 0.8095208406448364
Epoch 660, training loss: 6.182005882263184 = 0.09049125015735626 + 1.0 * 6.091514587402344
Epoch 660, val loss: 0.8171247839927673
Epoch 670, training loss: 6.182214260101318 = 0.0852927416563034 + 1.0 * 6.096921443939209
Epoch 670, val loss: 0.8248807787895203
Epoch 680, training loss: 6.173620700836182 = 0.08050145953893661 + 1.0 * 6.093119144439697
Epoch 680, val loss: 0.8326077461242676
Epoch 690, training loss: 6.164454936981201 = 0.0760972648859024 + 1.0 * 6.088357448577881
Epoch 690, val loss: 0.8404257297515869
Epoch 700, training loss: 6.160881996154785 = 0.07200471311807632 + 1.0 * 6.088877201080322
Epoch 700, val loss: 0.84830242395401
Epoch 710, training loss: 6.1543707847595215 = 0.06823481619358063 + 1.0 * 6.0861358642578125
Epoch 710, val loss: 0.8561459183692932
Epoch 720, training loss: 6.1549973487854 = 0.06474367529153824 + 1.0 * 6.090253829956055
Epoch 720, val loss: 0.863959789276123
Epoch 730, training loss: 6.14447546005249 = 0.06151212751865387 + 1.0 * 6.082963466644287
Epoch 730, val loss: 0.8717347979545593
Epoch 740, training loss: 6.141310214996338 = 0.05850415676832199 + 1.0 * 6.08280611038208
Epoch 740, val loss: 0.8794683218002319
Epoch 750, training loss: 6.136992454528809 = 0.055694326758384705 + 1.0 * 6.081298351287842
Epoch 750, val loss: 0.8872137665748596
Epoch 760, training loss: 6.134985446929932 = 0.05308258906006813 + 1.0 * 6.081902980804443
Epoch 760, val loss: 0.894913911819458
Epoch 770, training loss: 6.132190227508545 = 0.05065808817744255 + 1.0 * 6.081532001495361
Epoch 770, val loss: 0.9024079442024231
Epoch 780, training loss: 6.126790523529053 = 0.04838868975639343 + 1.0 * 6.078402042388916
Epoch 780, val loss: 0.9098713397979736
Epoch 790, training loss: 6.1400275230407715 = 0.04626162350177765 + 1.0 * 6.093765735626221
Epoch 790, val loss: 0.9173786640167236
Epoch 800, training loss: 6.1229424476623535 = 0.044271320104599 + 1.0 * 6.078670978546143
Epoch 800, val loss: 0.9247013330459595
Epoch 810, training loss: 6.118437767028809 = 0.042415034025907516 + 1.0 * 6.076022624969482
Epoch 810, val loss: 0.9318665266036987
Epoch 820, training loss: 6.114686965942383 = 0.04066077619791031 + 1.0 * 6.074026107788086
Epoch 820, val loss: 0.9390848875045776
Epoch 830, training loss: 6.134851455688477 = 0.03900635987520218 + 1.0 * 6.0958452224731445
Epoch 830, val loss: 0.9462525248527527
Epoch 840, training loss: 6.109787940979004 = 0.03746352344751358 + 1.0 * 6.072324275970459
Epoch 840, val loss: 0.9532586932182312
Epoch 850, training loss: 6.10798978805542 = 0.036010295152664185 + 1.0 * 6.071979522705078
Epoch 850, val loss: 0.960096001625061
Epoch 860, training loss: 6.105356693267822 = 0.03462948277592659 + 1.0 * 6.070727348327637
Epoch 860, val loss: 0.9669520854949951
Epoch 870, training loss: 6.107183456420898 = 0.033316437155008316 + 1.0 * 6.073866844177246
Epoch 870, val loss: 0.9737367033958435
Epoch 880, training loss: 6.1071085929870605 = 0.032088879495859146 + 1.0 * 6.075019836425781
Epoch 880, val loss: 0.9804607033729553
Epoch 890, training loss: 6.098361968994141 = 0.03092501312494278 + 1.0 * 6.067437171936035
Epoch 890, val loss: 0.9870131611824036
Epoch 900, training loss: 6.097476482391357 = 0.029820641502738 + 1.0 * 6.06765604019165
Epoch 900, val loss: 0.9934980869293213
Epoch 910, training loss: 6.103577136993408 = 0.02876977249979973 + 1.0 * 6.074807167053223
Epoch 910, val loss: 0.9999834895133972
Epoch 920, training loss: 6.1007537841796875 = 0.02777690440416336 + 1.0 * 6.072977066040039
Epoch 920, val loss: 1.006326675415039
Epoch 930, training loss: 6.091817378997803 = 0.02683841995894909 + 1.0 * 6.064979076385498
Epoch 930, val loss: 1.0125004053115845
Epoch 940, training loss: 6.089919090270996 = 0.025941042229533195 + 1.0 * 6.06397819519043
Epoch 940, val loss: 1.0186614990234375
Epoch 950, training loss: 6.0885419845581055 = 0.02508321963250637 + 1.0 * 6.0634589195251465
Epoch 950, val loss: 1.0247796773910522
Epoch 960, training loss: 6.093200206756592 = 0.024273550137877464 + 1.0 * 6.068926811218262
Epoch 960, val loss: 1.0309566259384155
Epoch 970, training loss: 6.088083744049072 = 0.023508450016379356 + 1.0 * 6.0645751953125
Epoch 970, val loss: 1.0367904901504517
Epoch 980, training loss: 6.0847296714782715 = 0.022782115265727043 + 1.0 * 6.061947345733643
Epoch 980, val loss: 1.0424882173538208
Epoch 990, training loss: 6.082728385925293 = 0.022082269191741943 + 1.0 * 6.060646057128906
Epoch 990, val loss: 1.0482158660888672
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8561
Flip ASR: 0.8267/225 nodes
The final ASR:0.74047, 0.08225, Accuracy:0.79630, 0.03666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11564])
remove edge: torch.Size([2, 9452])
updated graph: torch.Size([2, 10460])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.32590103149414 = 1.9521088600158691 + 1.0 * 8.373791694641113
Epoch 0, val loss: 1.9524775743484497
Epoch 10, training loss: 10.314227104187012 = 1.941065788269043 + 1.0 * 8.373161315917969
Epoch 10, val loss: 1.9419676065444946
Epoch 20, training loss: 10.296110153198242 = 1.9272921085357666 + 1.0 * 8.368818283081055
Epoch 20, val loss: 1.9284526109695435
Epoch 30, training loss: 10.247039794921875 = 1.9083459377288818 + 1.0 * 8.338693618774414
Epoch 30, val loss: 1.9098069667816162
Epoch 40, training loss: 10.02421760559082 = 1.8855968713760376 + 1.0 * 8.138620376586914
Epoch 40, val loss: 1.8885226249694824
Epoch 50, training loss: 9.47520637512207 = 1.8604413270950317 + 1.0 * 7.61476469039917
Epoch 50, val loss: 1.865161657333374
Epoch 60, training loss: 9.050734519958496 = 1.8402787446975708 + 1.0 * 7.210455894470215
Epoch 60, val loss: 1.8471403121948242
Epoch 70, training loss: 8.737787246704102 = 1.8279974460601807 + 1.0 * 6.909789562225342
Epoch 70, val loss: 1.8357622623443604
Epoch 80, training loss: 8.57288932800293 = 1.8144521713256836 + 1.0 * 6.758437156677246
Epoch 80, val loss: 1.8232476711273193
Epoch 90, training loss: 8.42753791809082 = 1.8007466793060303 + 1.0 * 6.626791477203369
Epoch 90, val loss: 1.811065673828125
Epoch 100, training loss: 8.320317268371582 = 1.788532018661499 + 1.0 * 6.531785011291504
Epoch 100, val loss: 1.800601840019226
Epoch 110, training loss: 8.24437141418457 = 1.7776778936386108 + 1.0 * 6.466693878173828
Epoch 110, val loss: 1.7913683652877808
Epoch 120, training loss: 8.183832168579102 = 1.7668882608413696 + 1.0 * 6.4169440269470215
Epoch 120, val loss: 1.7819818258285522
Epoch 130, training loss: 8.127437591552734 = 1.7550369501113892 + 1.0 * 6.372400283813477
Epoch 130, val loss: 1.771923303604126
Epoch 140, training loss: 8.075193405151367 = 1.7417701482772827 + 1.0 * 6.333423614501953
Epoch 140, val loss: 1.7610993385314941
Epoch 150, training loss: 8.027413368225098 = 1.7265675067901611 + 1.0 * 6.300846099853516
Epoch 150, val loss: 1.7491201162338257
Epoch 160, training loss: 7.981832027435303 = 1.7088613510131836 + 1.0 * 6.272970676422119
Epoch 160, val loss: 1.7354816198349
Epoch 170, training loss: 7.938544750213623 = 1.6877174377441406 + 1.0 * 6.250827312469482
Epoch 170, val loss: 1.719269037246704
Epoch 180, training loss: 7.89506196975708 = 1.6622346639633179 + 1.0 * 6.232827186584473
Epoch 180, val loss: 1.6996161937713623
Epoch 190, training loss: 7.852107048034668 = 1.6318089962005615 + 1.0 * 6.220297813415527
Epoch 190, val loss: 1.6760824918746948
Epoch 200, training loss: 7.806156635284424 = 1.5966566801071167 + 1.0 * 6.209499835968018
Epoch 200, val loss: 1.6488628387451172
Epoch 210, training loss: 7.754701137542725 = 1.5568875074386597 + 1.0 * 6.197813510894775
Epoch 210, val loss: 1.6179299354553223
Epoch 220, training loss: 7.700412750244141 = 1.5121307373046875 + 1.0 * 6.188282012939453
Epoch 220, val loss: 1.5833200216293335
Epoch 230, training loss: 7.643125534057617 = 1.4627845287322998 + 1.0 * 6.180340766906738
Epoch 230, val loss: 1.5450451374053955
Epoch 240, training loss: 7.584349632263184 = 1.409658670425415 + 1.0 * 6.174691200256348
Epoch 240, val loss: 1.5039379596710205
Epoch 250, training loss: 7.5251288414001465 = 1.3553504943847656 + 1.0 * 6.169778347015381
Epoch 250, val loss: 1.4620481729507446
Epoch 260, training loss: 7.463227272033691 = 1.3001596927642822 + 1.0 * 6.16306734085083
Epoch 260, val loss: 1.419344425201416
Epoch 270, training loss: 7.401462554931641 = 1.2443218231201172 + 1.0 * 6.157140731811523
Epoch 270, val loss: 1.3761773109436035
Epoch 280, training loss: 7.341809272766113 = 1.1882824897766113 + 1.0 * 6.153526782989502
Epoch 280, val loss: 1.332977294921875
Epoch 290, training loss: 7.284346103668213 = 1.1337872743606567 + 1.0 * 6.150558948516846
Epoch 290, val loss: 1.2907965183258057
Epoch 300, training loss: 7.2247514724731445 = 1.0810648202896118 + 1.0 * 6.143686771392822
Epoch 300, val loss: 1.2501142024993896
Epoch 310, training loss: 7.16948127746582 = 1.0298784971237183 + 1.0 * 6.1396026611328125
Epoch 310, val loss: 1.2107163667678833
Epoch 320, training loss: 7.119574069976807 = 0.9809346795082092 + 1.0 * 6.138639450073242
Epoch 320, val loss: 1.1732150316238403
Epoch 330, training loss: 7.064905166625977 = 0.9347051382064819 + 1.0 * 6.130199909210205
Epoch 330, val loss: 1.1379692554473877
Epoch 340, training loss: 7.021114349365234 = 0.8904297947883606 + 1.0 * 6.1306843757629395
Epoch 340, val loss: 1.1043806076049805
Epoch 350, training loss: 6.972968578338623 = 0.8486984968185425 + 1.0 * 6.124269962310791
Epoch 350, val loss: 1.072882056236267
Epoch 360, training loss: 6.927739143371582 = 0.8092703223228455 + 1.0 * 6.118468761444092
Epoch 360, val loss: 1.0435960292816162
Epoch 370, training loss: 6.894465923309326 = 0.7720813155174255 + 1.0 * 6.122384548187256
Epoch 370, val loss: 1.0164650678634644
Epoch 380, training loss: 6.852980613708496 = 0.7377771735191345 + 1.0 * 6.115203380584717
Epoch 380, val loss: 0.992263913154602
Epoch 390, training loss: 6.814253807067871 = 0.70603346824646 + 1.0 * 6.108220100402832
Epoch 390, val loss: 0.9708466529846191
Epoch 400, training loss: 6.78255558013916 = 0.676116943359375 + 1.0 * 6.106438636779785
Epoch 400, val loss: 0.9514268636703491
Epoch 410, training loss: 6.752732276916504 = 0.6480043530464172 + 1.0 * 6.104727745056152
Epoch 410, val loss: 0.9340851306915283
Epoch 420, training loss: 6.720492839813232 = 0.6214532256126404 + 1.0 * 6.099039554595947
Epoch 420, val loss: 0.9187960028648376
Epoch 430, training loss: 6.692331314086914 = 0.5959790945053101 + 1.0 * 6.0963521003723145
Epoch 430, val loss: 0.9050889611244202
Epoch 440, training loss: 6.669448375701904 = 0.571490466594696 + 1.0 * 6.097958087921143
Epoch 440, val loss: 0.8928191065788269
Epoch 450, training loss: 6.6405205726623535 = 0.5481182336807251 + 1.0 * 6.092402458190918
Epoch 450, val loss: 0.8820962309837341
Epoch 460, training loss: 6.61435604095459 = 0.5253675580024719 + 1.0 * 6.088988304138184
Epoch 460, val loss: 0.872448742389679
Epoch 470, training loss: 6.592336177825928 = 0.5029367804527283 + 1.0 * 6.089399337768555
Epoch 470, val loss: 0.8635665774345398
Epoch 480, training loss: 6.566537857055664 = 0.48089006543159485 + 1.0 * 6.0856475830078125
Epoch 480, val loss: 0.8555546402931213
Epoch 490, training loss: 6.544945240020752 = 0.45916643738746643 + 1.0 * 6.085778713226318
Epoch 490, val loss: 0.848444402217865
Epoch 500, training loss: 6.522167682647705 = 0.43772125244140625 + 1.0 * 6.084446430206299
Epoch 500, val loss: 0.8419856429100037
Epoch 510, training loss: 6.497220516204834 = 0.4166927933692932 + 1.0 * 6.0805277824401855
Epoch 510, val loss: 0.836298942565918
Epoch 520, training loss: 6.478992462158203 = 0.3961213529109955 + 1.0 * 6.082870960235596
Epoch 520, val loss: 0.8314138650894165
Epoch 530, training loss: 6.457756996154785 = 0.376321017742157 + 1.0 * 6.0814361572265625
Epoch 530, val loss: 0.8274949789047241
Epoch 540, training loss: 6.4356207847595215 = 0.3572531044483185 + 1.0 * 6.078367710113525
Epoch 540, val loss: 0.8246448636054993
Epoch 550, training loss: 6.414996147155762 = 0.3388725519180298 + 1.0 * 6.0761237144470215
Epoch 550, val loss: 0.8226687908172607
Epoch 560, training loss: 6.396122455596924 = 0.3212389051914215 + 1.0 * 6.074883460998535
Epoch 560, val loss: 0.8217028975486755
Epoch 570, training loss: 6.377633094787598 = 0.3043185770511627 + 1.0 * 6.073314666748047
Epoch 570, val loss: 0.821675717830658
Epoch 580, training loss: 6.371474742889404 = 0.2881586253643036 + 1.0 * 6.083316326141357
Epoch 580, val loss: 0.8223869204521179
Epoch 590, training loss: 6.346410274505615 = 0.2728041112422943 + 1.0 * 6.073606014251709
Epoch 590, val loss: 0.8238981366157532
Epoch 600, training loss: 6.327744483947754 = 0.2580544650554657 + 1.0 * 6.069690227508545
Epoch 600, val loss: 0.8261804580688477
Epoch 610, training loss: 6.311220645904541 = 0.24372819066047668 + 1.0 * 6.067492485046387
Epoch 610, val loss: 0.8289085626602173
Epoch 620, training loss: 6.298366546630859 = 0.22980959713459015 + 1.0 * 6.068556785583496
Epoch 620, val loss: 0.8322198987007141
Epoch 630, training loss: 6.289822578430176 = 0.21640446782112122 + 1.0 * 6.073418140411377
Epoch 630, val loss: 0.8360178470611572
Epoch 640, training loss: 6.269202709197998 = 0.20355460047721863 + 1.0 * 6.065648078918457
Epoch 640, val loss: 0.840427041053772
Epoch 650, training loss: 6.255667209625244 = 0.19117113947868347 + 1.0 * 6.064496040344238
Epoch 650, val loss: 0.8452501893043518
Epoch 660, training loss: 6.242424488067627 = 0.17932412028312683 + 1.0 * 6.063100337982178
Epoch 660, val loss: 0.850581169128418
Epoch 670, training loss: 6.238242149353027 = 0.16809841990470886 + 1.0 * 6.070143699645996
Epoch 670, val loss: 0.856357753276825
Epoch 680, training loss: 6.222400188446045 = 0.1576610952615738 + 1.0 * 6.064739227294922
Epoch 680, val loss: 0.8624000549316406
Epoch 690, training loss: 6.20845365524292 = 0.1479383111000061 + 1.0 * 6.060515403747559
Epoch 690, val loss: 0.8689008951187134
Epoch 700, training loss: 6.197137355804443 = 0.13884028792381287 + 1.0 * 6.058297157287598
Epoch 700, val loss: 0.8755525350570679
Epoch 710, training loss: 6.187690734863281 = 0.13033469021320343 + 1.0 * 6.057355880737305
Epoch 710, val loss: 0.8825374841690063
Epoch 720, training loss: 6.184791088104248 = 0.12241853773593903 + 1.0 * 6.06237268447876
Epoch 720, val loss: 0.889853298664093
Epoch 730, training loss: 6.182656288146973 = 0.11515996605157852 + 1.0 * 6.067496299743652
Epoch 730, val loss: 0.8971301317214966
Epoch 740, training loss: 6.166247844696045 = 0.10853783041238785 + 1.0 * 6.05771017074585
Epoch 740, val loss: 0.9046935439109802
Epoch 750, training loss: 6.156808376312256 = 0.10242366790771484 + 1.0 * 6.054384708404541
Epoch 750, val loss: 0.912345290184021
Epoch 760, training loss: 6.149519443511963 = 0.09674407541751862 + 1.0 * 6.0527753829956055
Epoch 760, val loss: 0.9201298356056213
Epoch 770, training loss: 6.143911838531494 = 0.09147711843252182 + 1.0 * 6.052434921264648
Epoch 770, val loss: 0.9280704259872437
Epoch 780, training loss: 6.142490386962891 = 0.08661898970603943 + 1.0 * 6.055871486663818
Epoch 780, val loss: 0.9358918070793152
Epoch 790, training loss: 6.138460636138916 = 0.08217722922563553 + 1.0 * 6.056283473968506
Epoch 790, val loss: 0.9438259601593018
Epoch 800, training loss: 6.12935733795166 = 0.07805220782756805 + 1.0 * 6.051305294036865
Epoch 800, val loss: 0.9517320394515991
Epoch 810, training loss: 6.124507427215576 = 0.07419461011886597 + 1.0 * 6.0503129959106445
Epoch 810, val loss: 0.9596865177154541
Epoch 820, training loss: 6.1204705238342285 = 0.07061056047677994 + 1.0 * 6.049860000610352
Epoch 820, val loss: 0.9677016735076904
Epoch 830, training loss: 6.115703105926514 = 0.06728009879589081 + 1.0 * 6.048422813415527
Epoch 830, val loss: 0.9756710529327393
Epoch 840, training loss: 6.128567218780518 = 0.06416796147823334 + 1.0 * 6.064399242401123
Epoch 840, val loss: 0.9836270809173584
Epoch 850, training loss: 6.108767032623291 = 0.06129157170653343 + 1.0 * 6.047475337982178
Epoch 850, val loss: 0.9914208054542542
Epoch 860, training loss: 6.1045732498168945 = 0.058601781725883484 + 1.0 * 6.045971393585205
Epoch 860, val loss: 0.99923175573349
Epoch 870, training loss: 6.100521564483643 = 0.05605776235461235 + 1.0 * 6.044463634490967
Epoch 870, val loss: 1.0069583654403687
Epoch 880, training loss: 6.096889495849609 = 0.05366208404302597 + 1.0 * 6.043227195739746
Epoch 880, val loss: 1.0147336721420288
Epoch 890, training loss: 6.102768898010254 = 0.05139987915754318 + 1.0 * 6.0513691902160645
Epoch 890, val loss: 1.0224534273147583
Epoch 900, training loss: 6.09945011138916 = 0.04929347336292267 + 1.0 * 6.050156593322754
Epoch 900, val loss: 1.0299568176269531
Epoch 910, training loss: 6.090277671813965 = 0.04732198268175125 + 1.0 * 6.0429558753967285
Epoch 910, val loss: 1.0374693870544434
Epoch 920, training loss: 6.086831092834473 = 0.0454532653093338 + 1.0 * 6.041378021240234
Epoch 920, val loss: 1.0449177026748657
Epoch 930, training loss: 6.090908527374268 = 0.04368508979678154 + 1.0 * 6.0472235679626465
Epoch 930, val loss: 1.0523443222045898
Epoch 940, training loss: 6.081830024719238 = 0.0420161709189415 + 1.0 * 6.039813995361328
Epoch 940, val loss: 1.0595817565917969
Epoch 950, training loss: 6.080105781555176 = 0.040439534932374954 + 1.0 * 6.039666175842285
Epoch 950, val loss: 1.0667705535888672
Epoch 960, training loss: 6.084725379943848 = 0.038945842534303665 + 1.0 * 6.045779705047607
Epoch 960, val loss: 1.0739105939865112
Epoch 970, training loss: 6.0800275802612305 = 0.037537507712841034 + 1.0 * 6.042490005493164
Epoch 970, val loss: 1.0809370279312134
Epoch 980, training loss: 6.073723316192627 = 0.036200739443302155 + 1.0 * 6.037522792816162
Epoch 980, val loss: 1.0878163576126099
Epoch 990, training loss: 6.073331832885742 = 0.03492647036910057 + 1.0 * 6.038405418395996
Epoch 990, val loss: 1.0946635007858276
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.6236
Flip ASR: 0.5556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.33314323425293 = 1.9593353271484375 + 1.0 * 8.373807907104492
Epoch 0, val loss: 1.9593344926834106
Epoch 10, training loss: 10.321435928344727 = 1.9482033252716064 + 1.0 * 8.3732328414917
Epoch 10, val loss: 1.947774052619934
Epoch 20, training loss: 10.303766250610352 = 1.9340087175369263 + 1.0 * 8.369757652282715
Epoch 20, val loss: 1.9328817129135132
Epoch 30, training loss: 10.262301445007324 = 1.913995385169983 + 1.0 * 8.348305702209473
Epoch 30, val loss: 1.9117573499679565
Epoch 40, training loss: 10.105520248413086 = 1.8883421421051025 + 1.0 * 8.217178344726562
Epoch 40, val loss: 1.885883092880249
Epoch 50, training loss: 9.376134872436523 = 1.8598358631134033 + 1.0 * 7.516298770904541
Epoch 50, val loss: 1.8574724197387695
Epoch 60, training loss: 9.032186508178711 = 1.835694670677185 + 1.0 * 7.1964921951293945
Epoch 60, val loss: 1.8350117206573486
Epoch 70, training loss: 8.821613311767578 = 1.816744327545166 + 1.0 * 7.004868507385254
Epoch 70, val loss: 1.816254734992981
Epoch 80, training loss: 8.644813537597656 = 1.7986137866973877 + 1.0 * 6.8461995124816895
Epoch 80, val loss: 1.799168348312378
Epoch 90, training loss: 8.482865333557129 = 1.7854158878326416 + 1.0 * 6.697449684143066
Epoch 90, val loss: 1.786868691444397
Epoch 100, training loss: 8.349604606628418 = 1.7722889184951782 + 1.0 * 6.577315807342529
Epoch 100, val loss: 1.7749122381210327
Epoch 110, training loss: 8.233927726745605 = 1.7586263418197632 + 1.0 * 6.475301265716553
Epoch 110, val loss: 1.7632282972335815
Epoch 120, training loss: 8.152202606201172 = 1.7451658248901367 + 1.0 * 6.407036304473877
Epoch 120, val loss: 1.751606822013855
Epoch 130, training loss: 8.091036796569824 = 1.7301431894302368 + 1.0 * 6.360893726348877
Epoch 130, val loss: 1.738513708114624
Epoch 140, training loss: 8.039347648620605 = 1.7122408151626587 + 1.0 * 6.327106475830078
Epoch 140, val loss: 1.7230087518692017
Epoch 150, training loss: 7.9929118156433105 = 1.69062077999115 + 1.0 * 6.302290916442871
Epoch 150, val loss: 1.704689383506775
Epoch 160, training loss: 7.948548793792725 = 1.6639727354049683 + 1.0 * 6.284575939178467
Epoch 160, val loss: 1.682600975036621
Epoch 170, training loss: 7.90060567855835 = 1.631637454032898 + 1.0 * 6.268968105316162
Epoch 170, val loss: 1.6559333801269531
Epoch 180, training loss: 7.8486433029174805 = 1.592679500579834 + 1.0 * 6.2559638023376465
Epoch 180, val loss: 1.6236759424209595
Epoch 190, training loss: 7.790802001953125 = 1.5463417768478394 + 1.0 * 6.244460105895996
Epoch 190, val loss: 1.5852433443069458
Epoch 200, training loss: 7.726682186126709 = 1.4932547807693481 + 1.0 * 6.23342752456665
Epoch 200, val loss: 1.5413336753845215
Epoch 210, training loss: 7.657060146331787 = 1.433970332145691 + 1.0 * 6.223089694976807
Epoch 210, val loss: 1.4924720525741577
Epoch 220, training loss: 7.584149360656738 = 1.3699002265930176 + 1.0 * 6.214249134063721
Epoch 220, val loss: 1.4401299953460693
Epoch 230, training loss: 7.513819694519043 = 1.3039582967758179 + 1.0 * 6.2098612785339355
Epoch 230, val loss: 1.3872188329696655
Epoch 240, training loss: 7.437302589416504 = 1.2392568588256836 + 1.0 * 6.19804573059082
Epoch 240, val loss: 1.3354356288909912
Epoch 250, training loss: 7.364616870880127 = 1.1754306554794312 + 1.0 * 6.189186096191406
Epoch 250, val loss: 1.284440040588379
Epoch 260, training loss: 7.297204971313477 = 1.1129330396652222 + 1.0 * 6.184271812438965
Epoch 260, val loss: 1.2345638275146484
Epoch 270, training loss: 7.231076240539551 = 1.0537036657333374 + 1.0 * 6.177372455596924
Epoch 270, val loss: 1.1875320672988892
Epoch 280, training loss: 7.170321464538574 = 0.9990691542625427 + 1.0 * 6.171252250671387
Epoch 280, val loss: 1.144542932510376
Epoch 290, training loss: 7.111860275268555 = 0.9484514594078064 + 1.0 * 6.1634087562561035
Epoch 290, val loss: 1.1050260066986084
Epoch 300, training loss: 7.059296131134033 = 0.9012464880943298 + 1.0 * 6.158049583435059
Epoch 300, val loss: 1.0685640573501587
Epoch 310, training loss: 7.011075973510742 = 0.8581526279449463 + 1.0 * 6.152923107147217
Epoch 310, val loss: 1.036030888557434
Epoch 320, training loss: 6.966551780700684 = 0.8190032243728638 + 1.0 * 6.147548675537109
Epoch 320, val loss: 1.0070838928222656
Epoch 330, training loss: 6.9253668785095215 = 0.7829458117485046 + 1.0 * 6.142421245574951
Epoch 330, val loss: 0.9810448288917542
Epoch 340, training loss: 6.8881635665893555 = 0.7498371601104736 + 1.0 * 6.138326168060303
Epoch 340, val loss: 0.9580462574958801
Epoch 350, training loss: 6.85487699508667 = 0.7195931077003479 + 1.0 * 6.135283946990967
Epoch 350, val loss: 0.9378092288970947
Epoch 360, training loss: 6.821257591247559 = 0.691677451133728 + 1.0 * 6.129580020904541
Epoch 360, val loss: 0.9198919534683228
Epoch 370, training loss: 6.7906951904296875 = 0.6655423641204834 + 1.0 * 6.125153064727783
Epoch 370, val loss: 0.9040992856025696
Epoch 380, training loss: 6.765027046203613 = 0.6407555937767029 + 1.0 * 6.124271392822266
Epoch 380, val loss: 0.8898103833198547
Epoch 390, training loss: 6.738103866577148 = 0.6173003315925598 + 1.0 * 6.120803356170654
Epoch 390, val loss: 0.8771116733551025
Epoch 400, training loss: 6.709944725036621 = 0.5947483777999878 + 1.0 * 6.115196228027344
Epoch 400, val loss: 0.8655117750167847
Epoch 410, training loss: 6.6880598068237305 = 0.5727596879005432 + 1.0 * 6.115300178527832
Epoch 410, val loss: 0.8546223044395447
Epoch 420, training loss: 6.663171768188477 = 0.5514293313026428 + 1.0 * 6.1117424964904785
Epoch 420, val loss: 0.8445101976394653
Epoch 430, training loss: 6.638336658477783 = 0.5307445526123047 + 1.0 * 6.1075921058654785
Epoch 430, val loss: 0.8351909518241882
Epoch 440, training loss: 6.6171183586120605 = 0.5104227066040039 + 1.0 * 6.106695652008057
Epoch 440, val loss: 0.8263899087905884
Epoch 450, training loss: 6.598366737365723 = 0.4906674325466156 + 1.0 * 6.107699394226074
Epoch 450, val loss: 0.8182092905044556
Epoch 460, training loss: 6.573525905609131 = 0.47152799367904663 + 1.0 * 6.1019978523254395
Epoch 460, val loss: 0.8107612133026123
Epoch 470, training loss: 6.550918102264404 = 0.45278480648994446 + 1.0 * 6.098133087158203
Epoch 470, val loss: 0.8038080930709839
Epoch 480, training loss: 6.534989356994629 = 0.4344311058521271 + 1.0 * 6.100558280944824
Epoch 480, val loss: 0.7975621819496155
Epoch 490, training loss: 6.512964248657227 = 0.4168016314506531 + 1.0 * 6.096162796020508
Epoch 490, val loss: 0.7920736074447632
Epoch 500, training loss: 6.495711803436279 = 0.3998536169528961 + 1.0 * 6.095858097076416
Epoch 500, val loss: 0.7875664830207825
Epoch 510, training loss: 6.474645614624023 = 0.3832992911338806 + 1.0 * 6.091346263885498
Epoch 510, val loss: 0.7835837602615356
Epoch 520, training loss: 6.456778526306152 = 0.36707618832588196 + 1.0 * 6.089702129364014
Epoch 520, val loss: 0.780229389667511
Epoch 530, training loss: 6.456552505493164 = 0.3512479066848755 + 1.0 * 6.105304718017578
Epoch 530, val loss: 0.7775646448135376
Epoch 540, training loss: 6.425998687744141 = 0.3361307382583618 + 1.0 * 6.089868068695068
Epoch 540, val loss: 0.7756388187408447
Epoch 550, training loss: 6.406480312347412 = 0.3214479684829712 + 1.0 * 6.0850324630737305
Epoch 550, val loss: 0.7744296193122864
Epoch 560, training loss: 6.392709255218506 = 0.30709150433540344 + 1.0 * 6.085617542266846
Epoch 560, val loss: 0.7737059593200684
Epoch 570, training loss: 6.387450695037842 = 0.2932666540145874 + 1.0 * 6.094183921813965
Epoch 570, val loss: 0.7736411690711975
Epoch 580, training loss: 6.3652262687683105 = 0.2799989581108093 + 1.0 * 6.0852274894714355
Epoch 580, val loss: 0.7744287848472595
Epoch 590, training loss: 6.34830379486084 = 0.26714253425598145 + 1.0 * 6.081161022186279
Epoch 590, val loss: 0.775667130947113
Epoch 600, training loss: 6.334555625915527 = 0.2546711564064026 + 1.0 * 6.0798845291137695
Epoch 600, val loss: 0.777312695980072
Epoch 610, training loss: 6.320799350738525 = 0.24267041683197021 + 1.0 * 6.078128814697266
Epoch 610, val loss: 0.7793393135070801
Epoch 620, training loss: 6.308789253234863 = 0.23120051622390747 + 1.0 * 6.0775885581970215
Epoch 620, val loss: 0.7819826006889343
Epoch 630, training loss: 6.3018059730529785 = 0.22014470398426056 + 1.0 * 6.081661224365234
Epoch 630, val loss: 0.7849368453025818
Epoch 640, training loss: 6.287944793701172 = 0.20958304405212402 + 1.0 * 6.078361988067627
Epoch 640, val loss: 0.7881191968917847
Epoch 650, training loss: 6.273613929748535 = 0.19949252903461456 + 1.0 * 6.074121475219727
Epoch 650, val loss: 0.791888415813446
Epoch 660, training loss: 6.26240873336792 = 0.18984752893447876 + 1.0 * 6.072561264038086
Epoch 660, val loss: 0.7957483530044556
Epoch 670, training loss: 6.268586158752441 = 0.18065309524536133 + 1.0 * 6.08793306350708
Epoch 670, val loss: 0.7999535799026489
Epoch 680, training loss: 6.242590427398682 = 0.17198708653450012 + 1.0 * 6.070603370666504
Epoch 680, val loss: 0.8044856190681458
Epoch 690, training loss: 6.234690189361572 = 0.16381435096263885 + 1.0 * 6.070875644683838
Epoch 690, val loss: 0.8096352219581604
Epoch 700, training loss: 6.225986480712891 = 0.15607383847236633 + 1.0 * 6.069912433624268
Epoch 700, val loss: 0.814887285232544
Epoch 710, training loss: 6.220787048339844 = 0.1487678438425064 + 1.0 * 6.072019100189209
Epoch 710, val loss: 0.8203306198120117
Epoch 720, training loss: 6.213108539581299 = 0.1419178694486618 + 1.0 * 6.07119083404541
Epoch 720, val loss: 0.8260871767997742
Epoch 730, training loss: 6.2022223472595215 = 0.1354464739561081 + 1.0 * 6.066775798797607
Epoch 730, val loss: 0.8321126699447632
Epoch 740, training loss: 6.196104526519775 = 0.12929584085941315 + 1.0 * 6.066808700561523
Epoch 740, val loss: 0.8382014632225037
Epoch 750, training loss: 6.1890549659729 = 0.1234765276312828 + 1.0 * 6.065578460693359
Epoch 750, val loss: 0.8443604111671448
Epoch 760, training loss: 6.184869766235352 = 0.11797404289245605 + 1.0 * 6.066895484924316
Epoch 760, val loss: 0.8507603406906128
Epoch 770, training loss: 6.175682544708252 = 0.11277776956558228 + 1.0 * 6.0629048347473145
Epoch 770, val loss: 0.8573060631752014
Epoch 780, training loss: 6.170166492462158 = 0.10784069448709488 + 1.0 * 6.062325954437256
Epoch 780, val loss: 0.8640317320823669
Epoch 790, training loss: 6.16873025894165 = 0.10311907529830933 + 1.0 * 6.065611362457275
Epoch 790, val loss: 0.8707926273345947
Epoch 800, training loss: 6.161630630493164 = 0.09865071624517441 + 1.0 * 6.062979698181152
Epoch 800, val loss: 0.8775961995124817
Epoch 810, training loss: 6.159790992736816 = 0.09443977475166321 + 1.0 * 6.0653510093688965
Epoch 810, val loss: 0.8846091032028198
Epoch 820, training loss: 6.150961399078369 = 0.0904618352651596 + 1.0 * 6.060499668121338
Epoch 820, val loss: 0.8915019035339355
Epoch 830, training loss: 6.14622688293457 = 0.08665720373392105 + 1.0 * 6.059569835662842
Epoch 830, val loss: 0.898677408695221
Epoch 840, training loss: 6.140350341796875 = 0.08304235339164734 + 1.0 * 6.057308197021484
Epoch 840, val loss: 0.9056878685951233
Epoch 850, training loss: 6.140050411224365 = 0.07959376275539398 + 1.0 * 6.0604567527771
Epoch 850, val loss: 0.9127414226531982
Epoch 860, training loss: 6.138260364532471 = 0.07632970064878464 + 1.0 * 6.0619306564331055
Epoch 860, val loss: 0.9196420907974243
Epoch 870, training loss: 6.134459972381592 = 0.07324621826410294 + 1.0 * 6.061213970184326
Epoch 870, val loss: 0.9265670776367188
Epoch 880, training loss: 6.12451171875 = 0.07032731920480728 + 1.0 * 6.054184436798096
Epoch 880, val loss: 0.9336771368980408
Epoch 890, training loss: 6.121477127075195 = 0.06754454225301743 + 1.0 * 6.0539326667785645
Epoch 890, val loss: 0.9406387805938721
Epoch 900, training loss: 6.11997652053833 = 0.06489073485136032 + 1.0 * 6.0550856590271
Epoch 900, val loss: 0.9475081562995911
Epoch 910, training loss: 6.118988513946533 = 0.062371786683797836 + 1.0 * 6.05661678314209
Epoch 910, val loss: 0.9542613625526428
Epoch 920, training loss: 6.112707138061523 = 0.05998751148581505 + 1.0 * 6.052719593048096
Epoch 920, val loss: 0.961191713809967
Epoch 930, training loss: 6.108832359313965 = 0.05769919231534004 + 1.0 * 6.051133155822754
Epoch 930, val loss: 0.9681283235549927
Epoch 940, training loss: 6.106608867645264 = 0.05552227050065994 + 1.0 * 6.05108642578125
Epoch 940, val loss: 0.9748141765594482
Epoch 950, training loss: 6.104201316833496 = 0.053456198424100876 + 1.0 * 6.050745010375977
Epoch 950, val loss: 0.9815178513526917
Epoch 960, training loss: 6.108645439147949 = 0.051503825932741165 + 1.0 * 6.0571417808532715
Epoch 960, val loss: 0.9883265495300293
Epoch 970, training loss: 6.1027936935424805 = 0.04967349395155907 + 1.0 * 6.053120136260986
Epoch 970, val loss: 0.9946132898330688
Epoch 980, training loss: 6.0976057052612305 = 0.047955505549907684 + 1.0 * 6.049650192260742
Epoch 980, val loss: 1.0016052722930908
Epoch 990, training loss: 6.092710018157959 = 0.046318184584379196 + 1.0 * 6.04639196395874
Epoch 990, val loss: 1.0082651376724243
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6494
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32939338684082 = 1.955653190612793 + 1.0 * 8.373740196228027
Epoch 0, val loss: 1.9604098796844482
Epoch 10, training loss: 10.317214965820312 = 1.9447600841522217 + 1.0 * 8.372454643249512
Epoch 10, val loss: 1.948256015777588
Epoch 20, training loss: 10.297968864440918 = 1.9315091371536255 + 1.0 * 8.366459846496582
Epoch 20, val loss: 1.9331210851669312
Epoch 30, training loss: 10.247960090637207 = 1.913853645324707 + 1.0 * 8.3341064453125
Epoch 30, val loss: 1.9131391048431396
Epoch 40, training loss: 10.003976821899414 = 1.8928465843200684 + 1.0 * 8.111130714416504
Epoch 40, val loss: 1.8903837203979492
Epoch 50, training loss: 9.371054649353027 = 1.870658278465271 + 1.0 * 7.500396251678467
Epoch 50, val loss: 1.8667562007904053
Epoch 60, training loss: 8.986921310424805 = 1.8526917695999146 + 1.0 * 7.1342291831970215
Epoch 60, val loss: 1.8486658334732056
Epoch 70, training loss: 8.712688446044922 = 1.8373768329620361 + 1.0 * 6.875311374664307
Epoch 70, val loss: 1.832927942276001
Epoch 80, training loss: 8.47626781463623 = 1.8250267505645752 + 1.0 * 6.651241302490234
Epoch 80, val loss: 1.8205368518829346
Epoch 90, training loss: 8.327683448791504 = 1.8143106698989868 + 1.0 * 6.513372421264648
Epoch 90, val loss: 1.8097479343414307
Epoch 100, training loss: 8.226171493530273 = 1.8026784658432007 + 1.0 * 6.423492908477783
Epoch 100, val loss: 1.797969102859497
Epoch 110, training loss: 8.157245635986328 = 1.790441632270813 + 1.0 * 6.3668036460876465
Epoch 110, val loss: 1.7854604721069336
Epoch 120, training loss: 8.100563049316406 = 1.7786225080490112 + 1.0 * 6.321940898895264
Epoch 120, val loss: 1.7731225490570068
Epoch 130, training loss: 8.055047988891602 = 1.7673121690750122 + 1.0 * 6.287735462188721
Epoch 130, val loss: 1.761262059211731
Epoch 140, training loss: 8.017237663269043 = 1.7556565999984741 + 1.0 * 6.2615814208984375
Epoch 140, val loss: 1.7494964599609375
Epoch 150, training loss: 7.982604503631592 = 1.7427219152450562 + 1.0 * 6.239882469177246
Epoch 150, val loss: 1.7373261451721191
Epoch 160, training loss: 7.95049524307251 = 1.7280607223510742 + 1.0 * 6.2224345207214355
Epoch 160, val loss: 1.7243560552597046
Epoch 170, training loss: 7.919290542602539 = 1.7111473083496094 + 1.0 * 6.20814323425293
Epoch 170, val loss: 1.7101290225982666
Epoch 180, training loss: 7.8898820877075195 = 1.6912626028060913 + 1.0 * 6.198619365692139
Epoch 180, val loss: 1.6940075159072876
Epoch 190, training loss: 7.854266166687012 = 1.6680432558059692 + 1.0 * 6.186223030090332
Epoch 190, val loss: 1.6755791902542114
Epoch 200, training loss: 7.817258834838867 = 1.640492558479309 + 1.0 * 6.176766395568848
Epoch 200, val loss: 1.6540111303329468
Epoch 210, training loss: 7.776213645935059 = 1.6076050996780396 + 1.0 * 6.168608665466309
Epoch 210, val loss: 1.6283949613571167
Epoch 220, training loss: 7.73219108581543 = 1.5684605836868286 + 1.0 * 6.163730621337891
Epoch 220, val loss: 1.5980443954467773
Epoch 230, training loss: 7.6796441078186035 = 1.5233570337295532 + 1.0 * 6.15628719329834
Epoch 230, val loss: 1.5632503032684326
Epoch 240, training loss: 7.624661445617676 = 1.4726849794387817 + 1.0 * 6.151976585388184
Epoch 240, val loss: 1.5244303941726685
Epoch 250, training loss: 7.568267822265625 = 1.417402744293213 + 1.0 * 6.150865077972412
Epoch 250, val loss: 1.4825462102890015
Epoch 260, training loss: 7.504074573516846 = 1.360056757926941 + 1.0 * 6.144017696380615
Epoch 260, val loss: 1.4395956993103027
Epoch 270, training loss: 7.441281795501709 = 1.3012770414352417 + 1.0 * 6.140004634857178
Epoch 270, val loss: 1.39603590965271
Epoch 280, training loss: 7.381153583526611 = 1.2414427995681763 + 1.0 * 6.139710903167725
Epoch 280, val loss: 1.3520652055740356
Epoch 290, training loss: 7.313961982727051 = 1.1818805932998657 + 1.0 * 6.132081508636475
Epoch 290, val loss: 1.3087518215179443
Epoch 300, training loss: 7.251229286193848 = 1.1226073503494263 + 1.0 * 6.128622055053711
Epoch 300, val loss: 1.265470027923584
Epoch 310, training loss: 7.194174289703369 = 1.0644296407699585 + 1.0 * 6.129744529724121
Epoch 310, val loss: 1.2230007648468018
Epoch 320, training loss: 7.1299147605896 = 1.0084999799728394 + 1.0 * 6.121414661407471
Epoch 320, val loss: 1.1818554401397705
Epoch 330, training loss: 7.071719646453857 = 0.9540392756462097 + 1.0 * 6.117680549621582
Epoch 330, val loss: 1.141629695892334
Epoch 340, training loss: 7.017143726348877 = 0.9017602205276489 + 1.0 * 6.115383625030518
Epoch 340, val loss: 1.1028356552124023
Epoch 350, training loss: 6.9660234451293945 = 0.8531306385993958 + 1.0 * 6.1128926277160645
Epoch 350, val loss: 1.066787838935852
Epoch 360, training loss: 6.916247367858887 = 0.807640790939331 + 1.0 * 6.108606338500977
Epoch 360, val loss: 1.0333261489868164
Epoch 370, training loss: 6.872784614562988 = 0.765661895275116 + 1.0 * 6.107122898101807
Epoch 370, val loss: 1.0026086568832397
Epoch 380, training loss: 6.830170631408691 = 0.7276540398597717 + 1.0 * 6.1025166511535645
Epoch 380, val loss: 0.9751002788543701
Epoch 390, training loss: 6.792592525482178 = 0.6928706169128418 + 1.0 * 6.099721908569336
Epoch 390, val loss: 0.9504392147064209
Epoch 400, training loss: 6.7641215324401855 = 0.6608738303184509 + 1.0 * 6.10324764251709
Epoch 400, val loss: 0.9282464981079102
Epoch 410, training loss: 6.729230880737305 = 0.6314826011657715 + 1.0 * 6.097748279571533
Epoch 410, val loss: 0.9085632562637329
Epoch 420, training loss: 6.696571350097656 = 0.6039693355560303 + 1.0 * 6.092602252960205
Epoch 420, val loss: 0.8908010125160217
Epoch 430, training loss: 6.675127983093262 = 0.577613115310669 + 1.0 * 6.097514629364014
Epoch 430, val loss: 0.8743738532066345
Epoch 440, training loss: 6.644915580749512 = 0.5522849559783936 + 1.0 * 6.092630386352539
Epoch 440, val loss: 0.8591938018798828
Epoch 450, training loss: 6.615339756011963 = 0.5277351140975952 + 1.0 * 6.087604522705078
Epoch 450, val loss: 0.844935953617096
Epoch 460, training loss: 6.587399005889893 = 0.5034236907958984 + 1.0 * 6.083975315093994
Epoch 460, val loss: 0.8313549757003784
Epoch 470, training loss: 6.56702995300293 = 0.4792507290840149 + 1.0 * 6.0877790451049805
Epoch 470, val loss: 0.8183749914169312
Epoch 480, training loss: 6.53981351852417 = 0.45549866557121277 + 1.0 * 6.084314823150635
Epoch 480, val loss: 0.8060471415519714
Epoch 490, training loss: 6.5114617347717285 = 0.43193456530570984 + 1.0 * 6.079527378082275
Epoch 490, val loss: 0.7944210171699524
Epoch 500, training loss: 6.488170146942139 = 0.4086248576641083 + 1.0 * 6.079545497894287
Epoch 500, val loss: 0.7833826541900635
Epoch 510, training loss: 6.4615397453308105 = 0.38577112555503845 + 1.0 * 6.07576847076416
Epoch 510, val loss: 0.773223340511322
Epoch 520, training loss: 6.439873218536377 = 0.3633609116077423 + 1.0 * 6.076512336730957
Epoch 520, val loss: 0.763892412185669
Epoch 530, training loss: 6.4144792556762695 = 0.34165313839912415 + 1.0 * 6.072825908660889
Epoch 530, val loss: 0.7553829550743103
Epoch 540, training loss: 6.394759178161621 = 0.32074522972106934 + 1.0 * 6.074013710021973
Epoch 540, val loss: 0.7479940056800842
Epoch 550, training loss: 6.373361110687256 = 0.3008100688457489 + 1.0 * 6.072551250457764
Epoch 550, val loss: 0.7414839863777161
Epoch 560, training loss: 6.354832172393799 = 0.2819405794143677 + 1.0 * 6.072891712188721
Epoch 560, val loss: 0.7359828352928162
Epoch 570, training loss: 6.333075523376465 = 0.2642061412334442 + 1.0 * 6.068869590759277
Epoch 570, val loss: 0.7314498424530029
Epoch 580, training loss: 6.316039085388184 = 0.24758000671863556 + 1.0 * 6.0684590339660645
Epoch 580, val loss: 0.7279030680656433
Epoch 590, training loss: 6.301208019256592 = 0.23207858204841614 + 1.0 * 6.069129467010498
Epoch 590, val loss: 0.7253010272979736
Epoch 600, training loss: 6.2844743728637695 = 0.21778172254562378 + 1.0 * 6.06669282913208
Epoch 600, val loss: 0.7236750721931458
Epoch 610, training loss: 6.270415306091309 = 0.20460690557956696 + 1.0 * 6.065808296203613
Epoch 610, val loss: 0.7229704856872559
Epoch 620, training loss: 6.255129814147949 = 0.1924162358045578 + 1.0 * 6.062713623046875
Epoch 620, val loss: 0.7231121063232422
Epoch 630, training loss: 6.244350433349609 = 0.1811261773109436 + 1.0 * 6.0632243156433105
Epoch 630, val loss: 0.7239956259727478
Epoch 640, training loss: 6.230184555053711 = 0.17070381343364716 + 1.0 * 6.059480667114258
Epoch 640, val loss: 0.7255317568778992
Epoch 650, training loss: 6.2218427658081055 = 0.16108062863349915 + 1.0 * 6.06076192855835
Epoch 650, val loss: 0.7276257276535034
Epoch 660, training loss: 6.211987018585205 = 0.1522151231765747 + 1.0 * 6.05977201461792
Epoch 660, val loss: 0.730250895023346
Epoch 670, training loss: 6.203420162200928 = 0.14400623738765717 + 1.0 * 6.059413909912109
Epoch 670, val loss: 0.733307421207428
Epoch 680, training loss: 6.191652774810791 = 0.13638970255851746 + 1.0 * 6.055263042449951
Epoch 680, val loss: 0.736760139465332
Epoch 690, training loss: 6.185111045837402 = 0.1292978674173355 + 1.0 * 6.055813312530518
Epoch 690, val loss: 0.7405515909194946
Epoch 700, training loss: 6.183230876922607 = 0.12269195169210434 + 1.0 * 6.0605387687683105
Epoch 700, val loss: 0.7446040511131287
Epoch 710, training loss: 6.173243999481201 = 0.11658696085214615 + 1.0 * 6.056656837463379
Epoch 710, val loss: 0.7488512992858887
Epoch 720, training loss: 6.163942337036133 = 0.11090172827243805 + 1.0 * 6.053040504455566
Epoch 720, val loss: 0.7533683180809021
Epoch 730, training loss: 6.155025005340576 = 0.10557197034358978 + 1.0 * 6.049453258514404
Epoch 730, val loss: 0.7580656409263611
Epoch 740, training loss: 6.157108783721924 = 0.10055810958147049 + 1.0 * 6.0565505027771
Epoch 740, val loss: 0.7629325985908508
Epoch 750, training loss: 6.149629592895508 = 0.09585991501808167 + 1.0 * 6.053769588470459
Epoch 750, val loss: 0.7679480314254761
Epoch 760, training loss: 6.140349864959717 = 0.09147418290376663 + 1.0 * 6.04887580871582
Epoch 760, val loss: 0.7730530500411987
Epoch 770, training loss: 6.13380765914917 = 0.08734309673309326 + 1.0 * 6.046464443206787
Epoch 770, val loss: 0.7783185243606567
Epoch 780, training loss: 6.130730628967285 = 0.08344433456659317 + 1.0 * 6.047286510467529
Epoch 780, val loss: 0.7836766839027405
Epoch 790, training loss: 6.124475955963135 = 0.07976621389389038 + 1.0 * 6.0447096824646
Epoch 790, val loss: 0.7891116738319397
Epoch 800, training loss: 6.131946563720703 = 0.07629825174808502 + 1.0 * 6.055648326873779
Epoch 800, val loss: 0.7946076393127441
Epoch 810, training loss: 6.116683006286621 = 0.07305613905191422 + 1.0 * 6.04362678527832
Epoch 810, val loss: 0.8000592589378357
Epoch 820, training loss: 6.1127028465271 = 0.06999260187149048 + 1.0 * 6.042710304260254
Epoch 820, val loss: 0.8056942820549011
Epoch 830, training loss: 6.108177185058594 = 0.06708691269159317 + 1.0 * 6.041090488433838
Epoch 830, val loss: 0.8113604187965393
Epoch 840, training loss: 6.115593910217285 = 0.06432989239692688 + 1.0 * 6.051263809204102
Epoch 840, val loss: 0.8170156478881836
Epoch 850, training loss: 6.104275703430176 = 0.061733029782772064 + 1.0 * 6.042542457580566
Epoch 850, val loss: 0.8226912617683411
Epoch 860, training loss: 6.098932266235352 = 0.05927581340074539 + 1.0 * 6.039656639099121
Epoch 860, val loss: 0.8284665942192078
Epoch 870, training loss: 6.1023359298706055 = 0.05694884434342384 + 1.0 * 6.045387268066406
Epoch 870, val loss: 0.8341571092605591
Epoch 880, training loss: 6.092238903045654 = 0.05476079508662224 + 1.0 * 6.037477970123291
Epoch 880, val loss: 0.8399049639701843
Epoch 890, training loss: 6.090054988861084 = 0.05268441513180733 + 1.0 * 6.037370681762695
Epoch 890, val loss: 0.8457018733024597
Epoch 900, training loss: 6.086573123931885 = 0.05070182681083679 + 1.0 * 6.035871505737305
Epoch 900, val loss: 0.851576566696167
Epoch 910, training loss: 6.088564395904541 = 0.048811428248882294 + 1.0 * 6.039752960205078
Epoch 910, val loss: 0.8574355840682983
Epoch 920, training loss: 6.083839416503906 = 0.04701751843094826 + 1.0 * 6.0368218421936035
Epoch 920, val loss: 0.863222599029541
Epoch 930, training loss: 6.083709716796875 = 0.04531833156943321 + 1.0 * 6.038391590118408
Epoch 930, val loss: 0.8690378665924072
Epoch 940, training loss: 6.078811168670654 = 0.04370774328708649 + 1.0 * 6.0351033210754395
Epoch 940, val loss: 0.8748664855957031
Epoch 950, training loss: 6.078751564025879 = 0.04217423126101494 + 1.0 * 6.036577224731445
Epoch 950, val loss: 0.8806800246238708
Epoch 960, training loss: 6.074141979217529 = 0.040713366121053696 + 1.0 * 6.03342866897583
Epoch 960, val loss: 0.8864610195159912
Epoch 970, training loss: 6.074411869049072 = 0.03932223841547966 + 1.0 * 6.035089492797852
Epoch 970, val loss: 0.8921721577644348
Epoch 980, training loss: 6.075292587280273 = 0.03799763694405556 + 1.0 * 6.037294864654541
Epoch 980, val loss: 0.8978261351585388
Epoch 990, training loss: 6.069610118865967 = 0.03673769533634186 + 1.0 * 6.032872200012207
Epoch 990, val loss: 0.9034936428070068
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8967
Flip ASR: 0.8756/225 nodes
The final ASR:0.72325, 0.12309, Accuracy:0.79630, 0.01386
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9466])
updated graph: torch.Size([2, 10518])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.82840, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.335805892944336 = 1.9620146751403809 + 1.0 * 8.373791694641113
Epoch 0, val loss: 1.9719104766845703
Epoch 10, training loss: 10.32486343383789 = 1.9515148401260376 + 1.0 * 8.373348236083984
Epoch 10, val loss: 1.9613145589828491
Epoch 20, training loss: 10.309661865234375 = 1.9391924142837524 + 1.0 * 8.370469093322754
Epoch 20, val loss: 1.9486581087112427
Epoch 30, training loss: 10.271991729736328 = 1.9225910902023315 + 1.0 * 8.349400520324707
Epoch 30, val loss: 1.9316492080688477
Epoch 40, training loss: 10.074677467346191 = 1.901829481124878 + 1.0 * 8.172847747802734
Epoch 40, val loss: 1.9111343622207642
Epoch 50, training loss: 9.131919860839844 = 1.8806545734405518 + 1.0 * 7.251265048980713
Epoch 50, val loss: 1.8904422521591187
Epoch 60, training loss: 8.798627853393555 = 1.862562894821167 + 1.0 * 6.936064720153809
Epoch 60, val loss: 1.873066782951355
Epoch 70, training loss: 8.546689987182617 = 1.8474706411361694 + 1.0 * 6.699219703674316
Epoch 70, val loss: 1.8580561876296997
Epoch 80, training loss: 8.407737731933594 = 1.8329274654388428 + 1.0 * 6.574810028076172
Epoch 80, val loss: 1.8441654443740845
Epoch 90, training loss: 8.306678771972656 = 1.8194894790649414 + 1.0 * 6.487189769744873
Epoch 90, val loss: 1.8313521146774292
Epoch 100, training loss: 8.219986915588379 = 1.80696702003479 + 1.0 * 6.41301965713501
Epoch 100, val loss: 1.8197897672653198
Epoch 110, training loss: 8.154836654663086 = 1.7951805591583252 + 1.0 * 6.35965633392334
Epoch 110, val loss: 1.8086133003234863
Epoch 120, training loss: 8.104331016540527 = 1.7832210063934326 + 1.0 * 6.321109771728516
Epoch 120, val loss: 1.797002911567688
Epoch 130, training loss: 8.061119079589844 = 1.7709537744522095 + 1.0 * 6.290164947509766
Epoch 130, val loss: 1.7851678133010864
Epoch 140, training loss: 8.022273063659668 = 1.7578002214431763 + 1.0 * 6.264472961425781
Epoch 140, val loss: 1.772982120513916
Epoch 150, training loss: 7.987443923950195 = 1.7428672313690186 + 1.0 * 6.244576454162598
Epoch 150, val loss: 1.7597724199295044
Epoch 160, training loss: 7.952441215515137 = 1.7258038520812988 + 1.0 * 6.226637363433838
Epoch 160, val loss: 1.7452926635742188
Epoch 170, training loss: 7.916858673095703 = 1.7055774927139282 + 1.0 * 6.2112812995910645
Epoch 170, val loss: 1.7285208702087402
Epoch 180, training loss: 7.879435062408447 = 1.6809908151626587 + 1.0 * 6.198444366455078
Epoch 180, val loss: 1.7084413766860962
Epoch 190, training loss: 7.841131210327148 = 1.6507236957550049 + 1.0 * 6.1904072761535645
Epoch 190, val loss: 1.683785319328308
Epoch 200, training loss: 7.793645858764648 = 1.6141998767852783 + 1.0 * 6.179445743560791
Epoch 200, val loss: 1.6539576053619385
Epoch 210, training loss: 7.741118431091309 = 1.569986343383789 + 1.0 * 6.1711320877075195
Epoch 210, val loss: 1.617647409439087
Epoch 220, training loss: 7.681354522705078 = 1.5171560049057007 + 1.0 * 6.164198398590088
Epoch 220, val loss: 1.5742343664169312
Epoch 230, training loss: 7.614706039428711 = 1.4559478759765625 + 1.0 * 6.158758163452148
Epoch 230, val loss: 1.5241806507110596
Epoch 240, training loss: 7.54351806640625 = 1.389605164527893 + 1.0 * 6.1539130210876465
Epoch 240, val loss: 1.4704842567443848
Epoch 250, training loss: 7.470361709594727 = 1.3218721151351929 + 1.0 * 6.148489475250244
Epoch 250, val loss: 1.4155160188674927
Epoch 260, training loss: 7.399324417114258 = 1.2543065547943115 + 1.0 * 6.145018100738525
Epoch 260, val loss: 1.3613756895065308
Epoch 270, training loss: 7.330918312072754 = 1.1894323825836182 + 1.0 * 6.141486167907715
Epoch 270, val loss: 1.3103649616241455
Epoch 280, training loss: 7.264394283294678 = 1.1287564039230347 + 1.0 * 6.1356377601623535
Epoch 280, val loss: 1.2630287408828735
Epoch 290, training loss: 7.205639839172363 = 1.0721770524978638 + 1.0 * 6.133462905883789
Epoch 290, val loss: 1.2195227146148682
Epoch 300, training loss: 7.1488566398620605 = 1.019981861114502 + 1.0 * 6.128874778747559
Epoch 300, val loss: 1.1799571514129639
Epoch 310, training loss: 7.095211029052734 = 0.9712612628936768 + 1.0 * 6.123950004577637
Epoch 310, val loss: 1.1435047388076782
Epoch 320, training loss: 7.046671390533447 = 0.9252218008041382 + 1.0 * 6.1214494705200195
Epoch 320, val loss: 1.1093578338623047
Epoch 330, training loss: 6.99992036819458 = 0.8815065026283264 + 1.0 * 6.118413925170898
Epoch 330, val loss: 1.0771594047546387
Epoch 340, training loss: 6.95900821685791 = 0.8391114473342896 + 1.0 * 6.11989688873291
Epoch 340, val loss: 1.0458667278289795
Epoch 350, training loss: 6.9103546142578125 = 0.7978153824806213 + 1.0 * 6.112539291381836
Epoch 350, val loss: 1.0155689716339111
Epoch 360, training loss: 6.873172760009766 = 0.7570695281028748 + 1.0 * 6.116103172302246
Epoch 360, val loss: 0.9857370853424072
Epoch 370, training loss: 6.826023101806641 = 0.7172355055809021 + 1.0 * 6.108787536621094
Epoch 370, val loss: 0.9565780162811279
Epoch 380, training loss: 6.783462047576904 = 0.678285539150238 + 1.0 * 6.1051764488220215
Epoch 380, val loss: 0.9282721877098083
Epoch 390, training loss: 6.74439811706543 = 0.6404072046279907 + 1.0 * 6.1039910316467285
Epoch 390, val loss: 0.9008893966674805
Epoch 400, training loss: 6.710938453674316 = 0.6045492887496948 + 1.0 * 6.106389045715332
Epoch 400, val loss: 0.8752383589744568
Epoch 410, training loss: 6.6727399826049805 = 0.5712409615516663 + 1.0 * 6.101499080657959
Epoch 410, val loss: 0.8521180748939514
Epoch 420, training loss: 6.637673854827881 = 0.5399528741836548 + 1.0 * 6.097721099853516
Epoch 420, val loss: 0.8309372663497925
Epoch 430, training loss: 6.605342864990234 = 0.5104592442512512 + 1.0 * 6.094883441925049
Epoch 430, val loss: 0.8116558790206909
Epoch 440, training loss: 6.577036380767822 = 0.48280853033065796 + 1.0 * 6.0942277908325195
Epoch 440, val loss: 0.7943003177642822
Epoch 450, training loss: 6.548461437225342 = 0.4570247530937195 + 1.0 * 6.091436862945557
Epoch 450, val loss: 0.7788156867027283
Epoch 460, training loss: 6.522037506103516 = 0.4324842095375061 + 1.0 * 6.089553356170654
Epoch 460, val loss: 0.7647818922996521
Epoch 470, training loss: 6.500051498413086 = 0.4089930057525635 + 1.0 * 6.091058731079102
Epoch 470, val loss: 0.7518219947814941
Epoch 480, training loss: 6.474155426025391 = 0.38648542761802673 + 1.0 * 6.087669849395752
Epoch 480, val loss: 0.7400526404380798
Epoch 490, training loss: 6.449370384216309 = 0.3645232319831848 + 1.0 * 6.0848469734191895
Epoch 490, val loss: 0.7289619445800781
Epoch 500, training loss: 6.426716327667236 = 0.3430284261703491 + 1.0 * 6.083687782287598
Epoch 500, val loss: 0.7184230089187622
Epoch 510, training loss: 6.404220104217529 = 0.3220643997192383 + 1.0 * 6.082155704498291
Epoch 510, val loss: 0.7086115479469299
Epoch 520, training loss: 6.380903720855713 = 0.3015316128730774 + 1.0 * 6.079371929168701
Epoch 520, val loss: 0.6994100213050842
Epoch 530, training loss: 6.361252784729004 = 0.28147411346435547 + 1.0 * 6.079778671264648
Epoch 530, val loss: 0.6908532381057739
Epoch 540, training loss: 6.3409247398376465 = 0.2621564269065857 + 1.0 * 6.078768253326416
Epoch 540, val loss: 0.6829795837402344
Epoch 550, training loss: 6.3192243576049805 = 0.24374109506607056 + 1.0 * 6.075483322143555
Epoch 550, val loss: 0.6759452819824219
Epoch 560, training loss: 6.306763648986816 = 0.22627973556518555 + 1.0 * 6.080483913421631
Epoch 560, val loss: 0.6696303486824036
Epoch 570, training loss: 6.28632926940918 = 0.20996862649917603 + 1.0 * 6.076360702514648
Epoch 570, val loss: 0.6642400622367859
Epoch 580, training loss: 6.266446590423584 = 0.19482657313346863 + 1.0 * 6.071619987487793
Epoch 580, val loss: 0.6598184108734131
Epoch 590, training loss: 6.251890659332275 = 0.18081754446029663 + 1.0 * 6.071073055267334
Epoch 590, val loss: 0.6563153266906738
Epoch 600, training loss: 6.237020492553711 = 0.16799265146255493 + 1.0 * 6.069027900695801
Epoch 600, val loss: 0.6535784602165222
Epoch 610, training loss: 6.227628231048584 = 0.1563616394996643 + 1.0 * 6.0712666511535645
Epoch 610, val loss: 0.6518866419792175
Epoch 620, training loss: 6.213054180145264 = 0.14581863582134247 + 1.0 * 6.067235469818115
Epoch 620, val loss: 0.6508961915969849
Epoch 630, training loss: 6.206659317016602 = 0.13624148070812225 + 1.0 * 6.070417881011963
Epoch 630, val loss: 0.6505049467086792
Epoch 640, training loss: 6.192866802215576 = 0.1275671273469925 + 1.0 * 6.0652995109558105
Epoch 640, val loss: 0.6508206725120544
Epoch 650, training loss: 6.181434154510498 = 0.11965616792440414 + 1.0 * 6.0617780685424805
Epoch 650, val loss: 0.651757538318634
Epoch 660, training loss: 6.184042453765869 = 0.11241853982210159 + 1.0 * 6.071623802185059
Epoch 660, val loss: 0.6531049013137817
Epoch 670, training loss: 6.165923118591309 = 0.10584989935159683 + 1.0 * 6.060073375701904
Epoch 670, val loss: 0.6549128890037537
Epoch 680, training loss: 6.164962291717529 = 0.09982527792453766 + 1.0 * 6.065136909484863
Epoch 680, val loss: 0.6571506261825562
Epoch 690, training loss: 6.1537394523620605 = 0.09430769830942154 + 1.0 * 6.059431552886963
Epoch 690, val loss: 0.6595880389213562
Epoch 700, training loss: 6.145586013793945 = 0.08920739591121674 + 1.0 * 6.0563788414001465
Epoch 700, val loss: 0.6623806357383728
Epoch 710, training loss: 6.140166282653809 = 0.08447402715682983 + 1.0 * 6.055692195892334
Epoch 710, val loss: 0.6653190851211548
Epoch 720, training loss: 6.134092807769775 = 0.08009857684373856 + 1.0 * 6.053994178771973
Epoch 720, val loss: 0.6683884263038635
Epoch 730, training loss: 6.13107442855835 = 0.0760561004281044 + 1.0 * 6.055018424987793
Epoch 730, val loss: 0.6717244386672974
Epoch 740, training loss: 6.123955726623535 = 0.07228995859622955 + 1.0 * 6.051665782928467
Epoch 740, val loss: 0.6752755045890808
Epoch 750, training loss: 6.1285271644592285 = 0.06877603381872177 + 1.0 * 6.059751033782959
Epoch 750, val loss: 0.6787697672843933
Epoch 760, training loss: 6.115520477294922 = 0.06551574170589447 + 1.0 * 6.050004959106445
Epoch 760, val loss: 0.6823570132255554
Epoch 770, training loss: 6.113606929779053 = 0.062469761818647385 + 1.0 * 6.0511369705200195
Epoch 770, val loss: 0.6861469745635986
Epoch 780, training loss: 6.108013153076172 = 0.059616975486278534 + 1.0 * 6.048396110534668
Epoch 780, val loss: 0.6899416446685791
Epoch 790, training loss: 6.108810901641846 = 0.056939538568258286 + 1.0 * 6.051871299743652
Epoch 790, val loss: 0.6937408447265625
Epoch 800, training loss: 6.1029767990112305 = 0.0544392354786396 + 1.0 * 6.048537731170654
Epoch 800, val loss: 0.6976543664932251
Epoch 810, training loss: 6.098260879516602 = 0.052091084420681 + 1.0 * 6.046169757843018
Epoch 810, val loss: 0.7015618681907654
Epoch 820, training loss: 6.100106716156006 = 0.04988325759768486 + 1.0 * 6.050223350524902
Epoch 820, val loss: 0.7054797410964966
Epoch 830, training loss: 6.094403266906738 = 0.04781269282102585 + 1.0 * 6.046590805053711
Epoch 830, val loss: 0.7093991041183472
Epoch 840, training loss: 6.089346885681152 = 0.04586443677544594 + 1.0 * 6.043482303619385
Epoch 840, val loss: 0.713344156742096
Epoch 850, training loss: 6.085672378540039 = 0.04402581602334976 + 1.0 * 6.041646480560303
Epoch 850, val loss: 0.7172769904136658
Epoch 860, training loss: 6.086893081665039 = 0.04228493943810463 + 1.0 * 6.044608116149902
Epoch 860, val loss: 0.7211907505989075
Epoch 870, training loss: 6.082906723022461 = 0.04065006598830223 + 1.0 * 6.042256832122803
Epoch 870, val loss: 0.7250630259513855
Epoch 880, training loss: 6.085757732391357 = 0.0391114205121994 + 1.0 * 6.0466461181640625
Epoch 880, val loss: 0.728947103023529
Epoch 890, training loss: 6.077437877655029 = 0.03766011446714401 + 1.0 * 6.039777755737305
Epoch 890, val loss: 0.7327359914779663
Epoch 900, training loss: 6.074390888214111 = 0.03628607094287872 + 1.0 * 6.038105010986328
Epoch 900, val loss: 0.7365873456001282
Epoch 910, training loss: 6.073611736297607 = 0.034978196024894714 + 1.0 * 6.038633346557617
Epoch 910, val loss: 0.7404096126556396
Epoch 920, training loss: 6.072862148284912 = 0.03373580798506737 + 1.0 * 6.039126396179199
Epoch 920, val loss: 0.7441730499267578
Epoch 930, training loss: 6.07136869430542 = 0.032559413462877274 + 1.0 * 6.038809299468994
Epoch 930, val loss: 0.7479504942893982
Epoch 940, training loss: 6.077946662902832 = 0.03144795820116997 + 1.0 * 6.046498775482178
Epoch 940, val loss: 0.7517458200454712
Epoch 950, training loss: 6.068936347961426 = 0.03039526380598545 + 1.0 * 6.038541316986084
Epoch 950, val loss: 0.7552637457847595
Epoch 960, training loss: 6.062516689300537 = 0.02939867414534092 + 1.0 * 6.03311824798584
Epoch 960, val loss: 0.7590770721435547
Epoch 970, training loss: 6.061484336853027 = 0.028444373980164528 + 1.0 * 6.0330400466918945
Epoch 970, val loss: 0.7627607583999634
Epoch 980, training loss: 6.07105827331543 = 0.02752990461885929 + 1.0 * 6.0435285568237305
Epoch 980, val loss: 0.7661977410316467
Epoch 990, training loss: 6.063091278076172 = 0.026668362319469452 + 1.0 * 6.0364227294921875
Epoch 990, val loss: 0.7697688937187195
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7417
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32866096496582 = 1.9549068212509155 + 1.0 * 8.373754501342773
Epoch 0, val loss: 1.9481271505355835
Epoch 10, training loss: 10.317441940307617 = 1.9445070028305054 + 1.0 * 8.37293529510498
Epoch 10, val loss: 1.9380929470062256
Epoch 20, training loss: 10.298702239990234 = 1.9314751625061035 + 1.0 * 8.367226600646973
Epoch 20, val loss: 1.925212025642395
Epoch 30, training loss: 10.240710258483887 = 1.9138119220733643 + 1.0 * 8.326898574829102
Epoch 30, val loss: 1.9075771570205688
Epoch 40, training loss: 9.92772388458252 = 1.8930416107177734 + 1.0 * 8.034682273864746
Epoch 40, val loss: 1.887157917022705
Epoch 50, training loss: 9.147043228149414 = 1.8710978031158447 + 1.0 * 7.275945663452148
Epoch 50, val loss: 1.8660287857055664
Epoch 60, training loss: 8.735078811645508 = 1.8543647527694702 + 1.0 * 6.880713939666748
Epoch 60, val loss: 1.849243402481079
Epoch 70, training loss: 8.509991645812988 = 1.8403314352035522 + 1.0 * 6.6696600914001465
Epoch 70, val loss: 1.8350337743759155
Epoch 80, training loss: 8.393674850463867 = 1.826534628868103 + 1.0 * 6.567140579223633
Epoch 80, val loss: 1.821824073791504
Epoch 90, training loss: 8.293800354003906 = 1.812103271484375 + 1.0 * 6.481697082519531
Epoch 90, val loss: 1.8078601360321045
Epoch 100, training loss: 8.213482856750488 = 1.798595905303955 + 1.0 * 6.414886951446533
Epoch 100, val loss: 1.7948848009109497
Epoch 110, training loss: 8.149800300598145 = 1.7855802774429321 + 1.0 * 6.364220142364502
Epoch 110, val loss: 1.7825430631637573
Epoch 120, training loss: 8.097986221313477 = 1.772153615951538 + 1.0 * 6.325832843780518
Epoch 120, val loss: 1.7700508832931519
Epoch 130, training loss: 8.054020881652832 = 1.7577990293502808 + 1.0 * 6.29622220993042
Epoch 130, val loss: 1.7568271160125732
Epoch 140, training loss: 8.01514720916748 = 1.741683840751648 + 1.0 * 6.273463249206543
Epoch 140, val loss: 1.7423288822174072
Epoch 150, training loss: 7.976408958435059 = 1.7229102849960327 + 1.0 * 6.253498554229736
Epoch 150, val loss: 1.7260353565216064
Epoch 160, training loss: 7.936905860900879 = 1.7006853818893433 + 1.0 * 6.236220359802246
Epoch 160, val loss: 1.7074052095413208
Epoch 170, training loss: 7.895129680633545 = 1.6737734079360962 + 1.0 * 6.221356391906738
Epoch 170, val loss: 1.6852961778640747
Epoch 180, training loss: 7.848647117614746 = 1.640978455543518 + 1.0 * 6.207668781280518
Epoch 180, val loss: 1.6587415933609009
Epoch 190, training loss: 7.799298286437988 = 1.6010878086090088 + 1.0 * 6.1982102394104
Epoch 190, val loss: 1.6268259286880493
Epoch 200, training loss: 7.741841793060303 = 1.5546003580093384 + 1.0 * 6.187241554260254
Epoch 200, val loss: 1.5900176763534546
Epoch 210, training loss: 7.681257247924805 = 1.5021575689315796 + 1.0 * 6.1790995597839355
Epoch 210, val loss: 1.5489273071289062
Epoch 220, training loss: 7.624191761016846 = 1.4458898305892944 + 1.0 * 6.178301811218262
Epoch 220, val loss: 1.5054882764816284
Epoch 230, training loss: 7.556504249572754 = 1.3899414539337158 + 1.0 * 6.166562557220459
Epoch 230, val loss: 1.463335633277893
Epoch 240, training loss: 7.495413780212402 = 1.3357409238815308 + 1.0 * 6.159672737121582
Epoch 240, val loss: 1.4232909679412842
Epoch 250, training loss: 7.438251972198486 = 1.2841315269470215 + 1.0 * 6.154120445251465
Epoch 250, val loss: 1.385873794555664
Epoch 260, training loss: 7.385483741760254 = 1.236199140548706 + 1.0 * 6.149284839630127
Epoch 260, val loss: 1.3517183065414429
Epoch 270, training loss: 7.334833145141602 = 1.191457748413086 + 1.0 * 6.143375396728516
Epoch 270, val loss: 1.3200230598449707
Epoch 280, training loss: 7.285962104797363 = 1.1482487916946411 + 1.0 * 6.137713432312012
Epoch 280, val loss: 1.2895950078964233
Epoch 290, training loss: 7.238088130950928 = 1.1058751344680786 + 1.0 * 6.132213115692139
Epoch 290, val loss: 1.2600412368774414
Epoch 300, training loss: 7.194103240966797 = 1.0638917684555054 + 1.0 * 6.130211353302002
Epoch 300, val loss: 1.2308372259140015
Epoch 310, training loss: 7.150346755981445 = 1.0231709480285645 + 1.0 * 6.127175807952881
Epoch 310, val loss: 1.2023985385894775
Epoch 320, training loss: 7.104293346405029 = 0.9838190674781799 + 1.0 * 6.120474338531494
Epoch 320, val loss: 1.1747967004776
Epoch 330, training loss: 7.062275409698486 = 0.9454745054244995 + 1.0 * 6.116800785064697
Epoch 330, val loss: 1.1477634906768799
Epoch 340, training loss: 7.024571895599365 = 0.9084129333496094 + 1.0 * 6.116158962249756
Epoch 340, val loss: 1.1214969158172607
Epoch 350, training loss: 6.983736991882324 = 0.872368335723877 + 1.0 * 6.111368656158447
Epoch 350, val loss: 1.0960887670516968
Epoch 360, training loss: 6.946744441986084 = 0.8367547392845154 + 1.0 * 6.109989643096924
Epoch 360, val loss: 1.0709893703460693
Epoch 370, training loss: 6.909619331359863 = 0.8015748858451843 + 1.0 * 6.108044624328613
Epoch 370, val loss: 1.0462883710861206
Epoch 380, training loss: 6.870041847229004 = 0.7668298482894897 + 1.0 * 6.103211879730225
Epoch 380, val loss: 1.0220203399658203
Epoch 390, training loss: 6.834298133850098 = 0.7323435544967651 + 1.0 * 6.101954460144043
Epoch 390, val loss: 0.9982115626335144
Epoch 400, training loss: 6.799114227294922 = 0.6985714435577393 + 1.0 * 6.1005425453186035
Epoch 400, val loss: 0.9751483201980591
Epoch 410, training loss: 6.76219367980957 = 0.6659810543060303 + 1.0 * 6.096212387084961
Epoch 410, val loss: 0.953177809715271
Epoch 420, training loss: 6.732919692993164 = 0.634568452835083 + 1.0 * 6.09835147857666
Epoch 420, val loss: 0.9324594736099243
Epoch 430, training loss: 6.6982879638671875 = 0.604934573173523 + 1.0 * 6.093353271484375
Epoch 430, val loss: 0.9132036566734314
Epoch 440, training loss: 6.666721820831299 = 0.5768122673034668 + 1.0 * 6.089909553527832
Epoch 440, val loss: 0.8956365585327148
Epoch 450, training loss: 6.6441802978515625 = 0.5499969124794006 + 1.0 * 6.094183444976807
Epoch 450, val loss: 0.8794384598731995
Epoch 460, training loss: 6.612217426300049 = 0.5244938135147095 + 1.0 * 6.087723731994629
Epoch 460, val loss: 0.8647540807723999
Epoch 470, training loss: 6.584108352661133 = 0.50003582239151 + 1.0 * 6.084072589874268
Epoch 470, val loss: 0.8513475656509399
Epoch 480, training loss: 6.567116737365723 = 0.4763626158237457 + 1.0 * 6.09075403213501
Epoch 480, val loss: 0.8390222787857056
Epoch 490, training loss: 6.53734827041626 = 0.453570157289505 + 1.0 * 6.083777904510498
Epoch 490, val loss: 0.8280205726623535
Epoch 500, training loss: 6.511664390563965 = 0.43153315782546997 + 1.0 * 6.0801310539245605
Epoch 500, val loss: 0.8180116415023804
Epoch 510, training loss: 6.489087104797363 = 0.4098902642726898 + 1.0 * 6.079196929931641
Epoch 510, val loss: 0.8087725639343262
Epoch 520, training loss: 6.465504169464111 = 0.38870537281036377 + 1.0 * 6.076798915863037
Epoch 520, val loss: 0.800305962562561
Epoch 530, training loss: 6.445940971374512 = 0.36805635690689087 + 1.0 * 6.077884674072266
Epoch 530, val loss: 0.7927059531211853
Epoch 540, training loss: 6.42386531829834 = 0.34791162610054016 + 1.0 * 6.075953483581543
Epoch 540, val loss: 0.7858988046646118
Epoch 550, training loss: 6.401055812835693 = 0.32837867736816406 + 1.0 * 6.072677135467529
Epoch 550, val loss: 0.7799234390258789
Epoch 560, training loss: 6.398192882537842 = 0.3096427619457245 + 1.0 * 6.088550090789795
Epoch 560, val loss: 0.7748621106147766
Epoch 570, training loss: 6.366644859313965 = 0.29195141792297363 + 1.0 * 6.07469367980957
Epoch 570, val loss: 0.7708686590194702
Epoch 580, training loss: 6.3451738357543945 = 0.275276243686676 + 1.0 * 6.069897651672363
Epoch 580, val loss: 0.7679128646850586
Epoch 590, training loss: 6.326440811157227 = 0.25953245162963867 + 1.0 * 6.066908359527588
Epoch 590, val loss: 0.7658727765083313
Epoch 600, training loss: 6.311822891235352 = 0.24472272396087646 + 1.0 * 6.0671000480651855
Epoch 600, val loss: 0.7647826075553894
Epoch 610, training loss: 6.306264400482178 = 0.23095309734344482 + 1.0 * 6.075311183929443
Epoch 610, val loss: 0.7644703984260559
Epoch 620, training loss: 6.285464286804199 = 0.21822147071361542 + 1.0 * 6.067242622375488
Epoch 620, val loss: 0.7651100754737854
Epoch 630, training loss: 6.268464088439941 = 0.2063797414302826 + 1.0 * 6.062084197998047
Epoch 630, val loss: 0.766433596611023
Epoch 640, training loss: 6.26248025894165 = 0.19528314471244812 + 1.0 * 6.067197322845459
Epoch 640, val loss: 0.768312931060791
Epoch 650, training loss: 6.248764991760254 = 0.18498758971691132 + 1.0 * 6.063777446746826
Epoch 650, val loss: 0.770630419254303
Epoch 660, training loss: 6.235954761505127 = 0.17542490363121033 + 1.0 * 6.060529708862305
Epoch 660, val loss: 0.7736079692840576
Epoch 670, training loss: 6.224108695983887 = 0.16642074286937714 + 1.0 * 6.057687759399414
Epoch 670, val loss: 0.7769647240638733
Epoch 680, training loss: 6.213907241821289 = 0.1579311639070511 + 1.0 * 6.055975914001465
Epoch 680, val loss: 0.7806309461593628
Epoch 690, training loss: 6.213058948516846 = 0.14995110034942627 + 1.0 * 6.063107967376709
Epoch 690, val loss: 0.7846824526786804
Epoch 700, training loss: 6.20063591003418 = 0.1425342559814453 + 1.0 * 6.058101654052734
Epoch 700, val loss: 0.7890549302101135
Epoch 710, training loss: 6.189929962158203 = 0.1355517953634262 + 1.0 * 6.054378032684326
Epoch 710, val loss: 0.7937996983528137
Epoch 720, training loss: 6.181108474731445 = 0.12893106043338776 + 1.0 * 6.052177429199219
Epoch 720, val loss: 0.7985906600952148
Epoch 730, training loss: 6.184594631195068 = 0.12264920771121979 + 1.0 * 6.06194543838501
Epoch 730, val loss: 0.8035256266593933
Epoch 740, training loss: 6.167287349700928 = 0.11674769222736359 + 1.0 * 6.050539493560791
Epoch 740, val loss: 0.8086268901824951
Epoch 750, training loss: 6.15972375869751 = 0.1111498549580574 + 1.0 * 6.048573970794678
Epoch 750, val loss: 0.813989520072937
Epoch 760, training loss: 6.156030654907227 = 0.10583968460559845 + 1.0 * 6.0501909255981445
Epoch 760, val loss: 0.8194213509559631
Epoch 770, training loss: 6.1500654220581055 = 0.10082823038101196 + 1.0 * 6.049237251281738
Epoch 770, val loss: 0.8249463438987732
Epoch 780, training loss: 6.145404815673828 = 0.09613186866044998 + 1.0 * 6.0492730140686035
Epoch 780, val loss: 0.8305595517158508
Epoch 790, training loss: 6.138189315795898 = 0.09171202033758163 + 1.0 * 6.046477317810059
Epoch 790, val loss: 0.83634352684021
Epoch 800, training loss: 6.131775856018066 = 0.08753692358732224 + 1.0 * 6.044239044189453
Epoch 800, val loss: 0.842100203037262
Epoch 810, training loss: 6.131211757659912 = 0.08358560502529144 + 1.0 * 6.04762601852417
Epoch 810, val loss: 0.8479210138320923
Epoch 820, training loss: 6.1275315284729 = 0.0798623338341713 + 1.0 * 6.047669410705566
Epoch 820, val loss: 0.8536235690116882
Epoch 830, training loss: 6.1179399490356445 = 0.07638883590698242 + 1.0 * 6.041551113128662
Epoch 830, val loss: 0.8595835566520691
Epoch 840, training loss: 6.114307403564453 = 0.07308768481016159 + 1.0 * 6.041219711303711
Epoch 840, val loss: 0.8654162883758545
Epoch 850, training loss: 6.110027313232422 = 0.06994123011827469 + 1.0 * 6.040086269378662
Epoch 850, val loss: 0.8712244033813477
Epoch 860, training loss: 6.108827114105225 = 0.06696006655693054 + 1.0 * 6.041867256164551
Epoch 860, val loss: 0.8769127726554871
Epoch 870, training loss: 6.104098796844482 = 0.06415683776140213 + 1.0 * 6.039941787719727
Epoch 870, val loss: 0.8828656077384949
Epoch 880, training loss: 6.099216461181641 = 0.06149425357580185 + 1.0 * 6.037722110748291
Epoch 880, val loss: 0.8887370228767395
Epoch 890, training loss: 6.098267555236816 = 0.058948587626218796 + 1.0 * 6.039319038391113
Epoch 890, val loss: 0.8945321440696716
Epoch 900, training loss: 6.092617034912109 = 0.05653630569577217 + 1.0 * 6.036080837249756
Epoch 900, val loss: 0.9002118110656738
Epoch 910, training loss: 6.090458393096924 = 0.054252978414297104 + 1.0 * 6.036205291748047
Epoch 910, val loss: 0.906086266040802
Epoch 920, training loss: 6.086411476135254 = 0.052078306674957275 + 1.0 * 6.034333229064941
Epoch 920, val loss: 0.911859393119812
Epoch 930, training loss: 6.085294246673584 = 0.05000389739871025 + 1.0 * 6.035290241241455
Epoch 930, val loss: 0.9175387024879456
Epoch 940, training loss: 6.082667827606201 = 0.04803808033466339 + 1.0 * 6.034629821777344
Epoch 940, val loss: 0.9230707883834839
Epoch 950, training loss: 6.083775043487549 = 0.04619636386632919 + 1.0 * 6.037578582763672
Epoch 950, val loss: 0.9287497401237488
Epoch 960, training loss: 6.077369689941406 = 0.04445403441786766 + 1.0 * 6.0329155921936035
Epoch 960, val loss: 0.934325635433197
Epoch 970, training loss: 6.073617458343506 = 0.042798835784196854 + 1.0 * 6.030818462371826
Epoch 970, val loss: 0.9397865533828735
Epoch 980, training loss: 6.076137065887451 = 0.04122455045580864 + 1.0 * 6.034912586212158
Epoch 980, val loss: 0.9451547265052795
Epoch 990, training loss: 6.068697929382324 = 0.03972744941711426 + 1.0 * 6.028970718383789
Epoch 990, val loss: 0.9504680633544922
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.7380
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.328266143798828 = 1.9544434547424316 + 1.0 * 8.373822212219238
Epoch 0, val loss: 1.9545342922210693
Epoch 10, training loss: 10.317170143127441 = 1.9437276124954224 + 1.0 * 8.373442649841309
Epoch 10, val loss: 1.943527102470398
Epoch 20, training loss: 10.302040100097656 = 1.930907130241394 + 1.0 * 8.371132850646973
Epoch 20, val loss: 1.930160641670227
Epoch 30, training loss: 10.270196914672852 = 1.9137530326843262 + 1.0 * 8.356444358825684
Epoch 30, val loss: 1.9122639894485474
Epoch 40, training loss: 10.167561531066895 = 1.8914200067520142 + 1.0 * 8.276141166687012
Epoch 40, val loss: 1.889448642730713
Epoch 50, training loss: 9.748895645141602 = 1.8667758703231812 + 1.0 * 7.882120132446289
Epoch 50, val loss: 1.864585518836975
Epoch 60, training loss: 9.076882362365723 = 1.841227650642395 + 1.0 * 7.235654830932617
Epoch 60, val loss: 1.839681625366211
Epoch 70, training loss: 8.791097640991211 = 1.8236541748046875 + 1.0 * 6.967443466186523
Epoch 70, val loss: 1.8230626583099365
Epoch 80, training loss: 8.611144065856934 = 1.8083118200302124 + 1.0 * 6.802832126617432
Epoch 80, val loss: 1.8083269596099854
Epoch 90, training loss: 8.489774703979492 = 1.791669249534607 + 1.0 * 6.698105335235596
Epoch 90, val loss: 1.792696475982666
Epoch 100, training loss: 8.382399559020996 = 1.7755286693572998 + 1.0 * 6.606871128082275
Epoch 100, val loss: 1.778181791305542
Epoch 110, training loss: 8.297845840454102 = 1.7609553337097168 + 1.0 * 6.536890506744385
Epoch 110, val loss: 1.7658185958862305
Epoch 120, training loss: 8.215158462524414 = 1.7482211589813232 + 1.0 * 6.46693754196167
Epoch 120, val loss: 1.7550034523010254
Epoch 130, training loss: 8.147491455078125 = 1.7337369918823242 + 1.0 * 6.413753986358643
Epoch 130, val loss: 1.7420774698257446
Epoch 140, training loss: 8.08626937866211 = 1.7155351638793945 + 1.0 * 6.370734691619873
Epoch 140, val loss: 1.7260534763336182
Epoch 150, training loss: 8.030468940734863 = 1.6938987970352173 + 1.0 * 6.336569786071777
Epoch 150, val loss: 1.7076154947280884
Epoch 160, training loss: 7.977863311767578 = 1.667771577835083 + 1.0 * 6.310091972351074
Epoch 160, val loss: 1.6858340501785278
Epoch 170, training loss: 7.924485683441162 = 1.6347407102584839 + 1.0 * 6.289744853973389
Epoch 170, val loss: 1.658597469329834
Epoch 180, training loss: 7.869255542755127 = 1.5940004587173462 + 1.0 * 6.27525520324707
Epoch 180, val loss: 1.624935507774353
Epoch 190, training loss: 7.807643890380859 = 1.5451408624649048 + 1.0 * 6.262503147125244
Epoch 190, val loss: 1.5846678018569946
Epoch 200, training loss: 7.740439414978027 = 1.4878383874893188 + 1.0 * 6.252601146697998
Epoch 200, val loss: 1.537442922592163
Epoch 210, training loss: 7.667041301727295 = 1.4230784177780151 + 1.0 * 6.24396276473999
Epoch 210, val loss: 1.484359860420227
Epoch 220, training loss: 7.591120719909668 = 1.35355806350708 + 1.0 * 6.237562656402588
Epoch 220, val loss: 1.427885890007019
Epoch 230, training loss: 7.5132951736450195 = 1.2840416431427002 + 1.0 * 6.229253768920898
Epoch 230, val loss: 1.3716977834701538
Epoch 240, training loss: 7.434991359710693 = 1.2145318984985352 + 1.0 * 6.220459461212158
Epoch 240, val loss: 1.3161951303482056
Epoch 250, training loss: 7.359713077545166 = 1.1458951234817505 + 1.0 * 6.213818073272705
Epoch 250, val loss: 1.2618569135665894
Epoch 260, training loss: 7.288110256195068 = 1.080328345298767 + 1.0 * 6.207781791687012
Epoch 260, val loss: 1.2102811336517334
Epoch 270, training loss: 7.216167449951172 = 1.0174912214279175 + 1.0 * 6.198676109313965
Epoch 270, val loss: 1.1613366603851318
Epoch 280, training loss: 7.147995948791504 = 0.9561233520507812 + 1.0 * 6.191872596740723
Epoch 280, val loss: 1.1136242151260376
Epoch 290, training loss: 7.088613510131836 = 0.8965391516685486 + 1.0 * 6.192074298858643
Epoch 290, val loss: 1.0673539638519287
Epoch 300, training loss: 7.022711753845215 = 0.8412018418312073 + 1.0 * 6.181509971618652
Epoch 300, val loss: 1.0244128704071045
Epoch 310, training loss: 6.9649505615234375 = 0.789534330368042 + 1.0 * 6.175416469573975
Epoch 310, val loss: 0.9849069118499756
Epoch 320, training loss: 6.910008430480957 = 0.7411726117134094 + 1.0 * 6.168835639953613
Epoch 320, val loss: 0.9480113983154297
Epoch 330, training loss: 6.859297275543213 = 0.6960568428039551 + 1.0 * 6.163240432739258
Epoch 330, val loss: 0.9140870571136475
Epoch 340, training loss: 6.815609931945801 = 0.6544780731201172 + 1.0 * 6.161131858825684
Epoch 340, val loss: 0.8835570216178894
Epoch 350, training loss: 6.773348808288574 = 0.6169998049736023 + 1.0 * 6.156349182128906
Epoch 350, val loss: 0.8568251729011536
Epoch 360, training loss: 6.731660842895508 = 0.582762598991394 + 1.0 * 6.148898124694824
Epoch 360, val loss: 0.8331958055496216
Epoch 370, training loss: 6.704246997833252 = 0.5513170957565308 + 1.0 * 6.152929782867432
Epoch 370, val loss: 0.8123065233230591
Epoch 380, training loss: 6.664529800415039 = 0.5227545499801636 + 1.0 * 6.141775131225586
Epoch 380, val loss: 0.7943511605262756
Epoch 390, training loss: 6.634637832641602 = 0.4963952302932739 + 1.0 * 6.138242721557617
Epoch 390, val loss: 0.7786049842834473
Epoch 400, training loss: 6.606247901916504 = 0.47171780467033386 + 1.0 * 6.134530067443848
Epoch 400, val loss: 0.7645905017852783
Epoch 410, training loss: 6.587348937988281 = 0.44861137866973877 + 1.0 * 6.138737678527832
Epoch 410, val loss: 0.7522233128547668
Epoch 420, training loss: 6.5566840171813965 = 0.4269018769264221 + 1.0 * 6.129782199859619
Epoch 420, val loss: 0.7412534356117249
Epoch 430, training loss: 6.5303874015808105 = 0.40603601932525635 + 1.0 * 6.124351501464844
Epoch 430, val loss: 0.7311499714851379
Epoch 440, training loss: 6.509274959564209 = 0.38565322756767273 + 1.0 * 6.123621940612793
Epoch 440, val loss: 0.7219021320343018
Epoch 450, training loss: 6.4880523681640625 = 0.36575374007225037 + 1.0 * 6.122298717498779
Epoch 450, val loss: 0.7134189009666443
Epoch 460, training loss: 6.464181900024414 = 0.34628158807754517 + 1.0 * 6.117900371551514
Epoch 460, val loss: 0.7056366801261902
Epoch 470, training loss: 6.4412522315979 = 0.3269483745098114 + 1.0 * 6.114304065704346
Epoch 470, val loss: 0.6983086466789246
Epoch 480, training loss: 6.42172384262085 = 0.3076658844947815 + 1.0 * 6.114058017730713
Epoch 480, val loss: 0.6916009187698364
Epoch 490, training loss: 6.404977321624756 = 0.2887362241744995 + 1.0 * 6.116240978240967
Epoch 490, val loss: 0.6855137348175049
Epoch 500, training loss: 6.3799848556518555 = 0.2701984941959381 + 1.0 * 6.109786510467529
Epoch 500, val loss: 0.67997145652771
Epoch 510, training loss: 6.3670196533203125 = 0.25217607617378235 + 1.0 * 6.114843368530273
Epoch 510, val loss: 0.6750840544700623
Epoch 520, training loss: 6.34083366394043 = 0.23490791022777557 + 1.0 * 6.105925559997559
Epoch 520, val loss: 0.6710399985313416
Epoch 530, training loss: 6.320584774017334 = 0.2185221016407013 + 1.0 * 6.102062702178955
Epoch 530, val loss: 0.6676047444343567
Epoch 540, training loss: 6.30839204788208 = 0.20311151444911957 + 1.0 * 6.10528039932251
Epoch 540, val loss: 0.6650310754776001
Epoch 550, training loss: 6.288147449493408 = 0.18886083364486694 + 1.0 * 6.0992865562438965
Epoch 550, val loss: 0.6630880236625671
Epoch 560, training loss: 6.272737979888916 = 0.1756478250026703 + 1.0 * 6.097090244293213
Epoch 560, val loss: 0.6618866324424744
Epoch 570, training loss: 6.281657695770264 = 0.16344721615314484 + 1.0 * 6.118210315704346
Epoch 570, val loss: 0.6614182591438293
Epoch 580, training loss: 6.2491254806518555 = 0.15236948430538177 + 1.0 * 6.0967559814453125
Epoch 580, val loss: 0.6615001559257507
Epoch 590, training loss: 6.235883712768555 = 0.1422438770532608 + 1.0 * 6.093639850616455
Epoch 590, val loss: 0.6621584892272949
Epoch 600, training loss: 6.222980976104736 = 0.13290750980377197 + 1.0 * 6.090073585510254
Epoch 600, val loss: 0.6635630130767822
Epoch 610, training loss: 6.211550235748291 = 0.12428704649209976 + 1.0 * 6.087263107299805
Epoch 610, val loss: 0.6654816269874573
Epoch 620, training loss: 6.218817710876465 = 0.11633729934692383 + 1.0 * 6.102480411529541
Epoch 620, val loss: 0.6679191589355469
Epoch 630, training loss: 6.207747459411621 = 0.10914042592048645 + 1.0 * 6.098607063293457
Epoch 630, val loss: 0.6705905795097351
Epoch 640, training loss: 6.186710357666016 = 0.10257402062416077 + 1.0 * 6.084136486053467
Epoch 640, val loss: 0.6735011339187622
Epoch 650, training loss: 6.179043292999268 = 0.09653458744287491 + 1.0 * 6.082508563995361
Epoch 650, val loss: 0.6768903732299805
Epoch 660, training loss: 6.170943737030029 = 0.09093951433897018 + 1.0 * 6.0800042152404785
Epoch 660, val loss: 0.6806311011314392
Epoch 670, training loss: 6.164524078369141 = 0.08574756234884262 + 1.0 * 6.0787763595581055
Epoch 670, val loss: 0.684609055519104
Epoch 680, training loss: 6.16880464553833 = 0.08094238489866257 + 1.0 * 6.087862491607666
Epoch 680, val loss: 0.6887524724006653
Epoch 690, training loss: 6.1569905281066895 = 0.07653017342090607 + 1.0 * 6.080460548400879
Epoch 690, val loss: 0.6929759979248047
Epoch 700, training loss: 6.147379398345947 = 0.07245084643363953 + 1.0 * 6.0749287605285645
Epoch 700, val loss: 0.6974314451217651
Epoch 710, training loss: 6.153781890869141 = 0.06865642219781876 + 1.0 * 6.08512544631958
Epoch 710, val loss: 0.7020703554153442
Epoch 720, training loss: 6.137592792510986 = 0.06516698002815247 + 1.0 * 6.072425842285156
Epoch 720, val loss: 0.706689715385437
Epoch 730, training loss: 6.133425712585449 = 0.06192295253276825 + 1.0 * 6.071502685546875
Epoch 730, val loss: 0.7113771438598633
Epoch 740, training loss: 6.129202365875244 = 0.05888655409216881 + 1.0 * 6.070315837860107
Epoch 740, val loss: 0.7162753343582153
Epoch 750, training loss: 6.131070137023926 = 0.05605267360806465 + 1.0 * 6.07501745223999
Epoch 750, val loss: 0.7212105989456177
Epoch 760, training loss: 6.120808124542236 = 0.053412653505802155 + 1.0 * 6.0673956871032715
Epoch 760, val loss: 0.7261276245117188
Epoch 770, training loss: 6.121230602264404 = 0.05095155909657478 + 1.0 * 6.070279121398926
Epoch 770, val loss: 0.7311059832572937
Epoch 780, training loss: 6.116541385650635 = 0.04865254461765289 + 1.0 * 6.0678887367248535
Epoch 780, val loss: 0.7360334992408752
Epoch 790, training loss: 6.117334365844727 = 0.04650626704096794 + 1.0 * 6.070827960968018
Epoch 790, val loss: 0.7409139275550842
Epoch 800, training loss: 6.108612537384033 = 0.04449482262134552 + 1.0 * 6.064117908477783
Epoch 800, val loss: 0.7457923889160156
Epoch 810, training loss: 6.105804443359375 = 0.04261159896850586 + 1.0 * 6.063192844390869
Epoch 810, val loss: 0.7507449388504028
Epoch 820, training loss: 6.103871822357178 = 0.04083731025457382 + 1.0 * 6.063034534454346
Epoch 820, val loss: 0.7556620836257935
Epoch 830, training loss: 6.103303909301758 = 0.03916923329234123 + 1.0 * 6.06413459777832
Epoch 830, val loss: 0.7604774236679077
Epoch 840, training loss: 6.104602336883545 = 0.03760848939418793 + 1.0 * 6.066993713378906
Epoch 840, val loss: 0.7652921080589294
Epoch 850, training loss: 6.0960845947265625 = 0.03613461181521416 + 1.0 * 6.05994987487793
Epoch 850, val loss: 0.7700019478797913
Epoch 860, training loss: 6.0919718742370605 = 0.034758180379867554 + 1.0 * 6.05721378326416
Epoch 860, val loss: 0.7747273445129395
Epoch 870, training loss: 6.090267658233643 = 0.0334530808031559 + 1.0 * 6.056814670562744
Epoch 870, val loss: 0.7795382142066956
Epoch 880, training loss: 6.093774795532227 = 0.032214123755693436 + 1.0 * 6.06156063079834
Epoch 880, val loss: 0.7842245101928711
Epoch 890, training loss: 6.090795516967773 = 0.031044278293848038 + 1.0 * 6.059751033782959
Epoch 890, val loss: 0.7888293862342834
Epoch 900, training loss: 6.084260940551758 = 0.029947711154818535 + 1.0 * 6.0543131828308105
Epoch 900, val loss: 0.7933362722396851
Epoch 910, training loss: 6.089169025421143 = 0.02890835516154766 + 1.0 * 6.060260772705078
Epoch 910, val loss: 0.7978557348251343
Epoch 920, training loss: 6.081596374511719 = 0.027920851483941078 + 1.0 * 6.053675651550293
Epoch 920, val loss: 0.8023480176925659
Epoch 930, training loss: 6.078442573547363 = 0.026989499107003212 + 1.0 * 6.051453113555908
Epoch 930, val loss: 0.806753396987915
Epoch 940, training loss: 6.076713562011719 = 0.026100900024175644 + 1.0 * 6.050612449645996
Epoch 940, val loss: 0.8111921548843384
Epoch 950, training loss: 6.085577964782715 = 0.02525205723941326 + 1.0 * 6.060326099395752
Epoch 950, val loss: 0.815551221370697
Epoch 960, training loss: 6.072787284851074 = 0.024453245103359222 + 1.0 * 6.048334121704102
Epoch 960, val loss: 0.8197299242019653
Epoch 970, training loss: 6.072909832000732 = 0.023697877302765846 + 1.0 * 6.0492119789123535
Epoch 970, val loss: 0.8239519000053406
Epoch 980, training loss: 6.070093631744385 = 0.02297249436378479 + 1.0 * 6.047121047973633
Epoch 980, val loss: 0.8282347917556763
Epoch 990, training loss: 6.086696624755859 = 0.02227924019098282 + 1.0 * 6.064417362213135
Epoch 990, val loss: 0.8323351740837097
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8561
Flip ASR: 0.8267/225 nodes
The final ASR:0.77860, 0.05481, Accuracy:0.81728, 0.01364
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10516])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98401, 0.00460, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.311092376708984 = 1.9372607469558716 + 1.0 * 8.373831748962402
Epoch 0, val loss: 1.9339789152145386
Epoch 10, training loss: 10.30117416381836 = 1.9278939962387085 + 1.0 * 8.37328052520752
Epoch 10, val loss: 1.925450325012207
Epoch 20, training loss: 10.286048889160156 = 1.9163241386413574 + 1.0 * 8.369725227355957
Epoch 20, val loss: 1.9144753217697144
Epoch 30, training loss: 10.244290351867676 = 1.90049147605896 + 1.0 * 8.343798637390137
Epoch 30, val loss: 1.8991620540618896
Epoch 40, training loss: 10.037365913391113 = 1.8807777166366577 + 1.0 * 8.156588554382324
Epoch 40, val loss: 1.8802565336227417
Epoch 50, training loss: 9.480056762695312 = 1.8576023578643799 + 1.0 * 7.622454643249512
Epoch 50, val loss: 1.8578513860702515
Epoch 60, training loss: 9.07204532623291 = 1.8407907485961914 + 1.0 * 7.231254577636719
Epoch 60, val loss: 1.8435871601104736
Epoch 70, training loss: 8.748125076293945 = 1.8291420936584473 + 1.0 * 6.918983459472656
Epoch 70, val loss: 1.8329960107803345
Epoch 80, training loss: 8.559996604919434 = 1.8163994550704956 + 1.0 * 6.743597030639648
Epoch 80, val loss: 1.8210489749908447
Epoch 90, training loss: 8.435724258422852 = 1.803173542022705 + 1.0 * 6.632550239562988
Epoch 90, val loss: 1.8086888790130615
Epoch 100, training loss: 8.336334228515625 = 1.7909836769104004 + 1.0 * 6.545350551605225
Epoch 100, val loss: 1.7979556322097778
Epoch 110, training loss: 8.256467819213867 = 1.7797207832336426 + 1.0 * 6.476747035980225
Epoch 110, val loss: 1.7880619764328003
Epoch 120, training loss: 8.19167709350586 = 1.7684221267700195 + 1.0 * 6.423255443572998
Epoch 120, val loss: 1.7779878377914429
Epoch 130, training loss: 8.135835647583008 = 1.7563443183898926 + 1.0 * 6.379491806030273
Epoch 130, val loss: 1.7673382759094238
Epoch 140, training loss: 8.08780288696289 = 1.7426910400390625 + 1.0 * 6.34511137008667
Epoch 140, val loss: 1.7557930946350098
Epoch 150, training loss: 8.0443696975708 = 1.726697325706482 + 1.0 * 6.317672252655029
Epoch 150, val loss: 1.7427175045013428
Epoch 160, training loss: 8.003551483154297 = 1.707642912864685 + 1.0 * 6.295908451080322
Epoch 160, val loss: 1.7274397611618042
Epoch 170, training loss: 7.962823867797852 = 1.6847712993621826 + 1.0 * 6.27805233001709
Epoch 170, val loss: 1.7092316150665283
Epoch 180, training loss: 7.922145843505859 = 1.657444715499878 + 1.0 * 6.2647013664245605
Epoch 180, val loss: 1.687577486038208
Epoch 190, training loss: 7.874143123626709 = 1.6245359182357788 + 1.0 * 6.249607086181641
Epoch 190, val loss: 1.6616758108139038
Epoch 200, training loss: 7.8236613273620605 = 1.5853625535964966 + 1.0 * 6.2382988929748535
Epoch 200, val loss: 1.6305681467056274
Epoch 210, training loss: 7.7679009437561035 = 1.5393357276916504 + 1.0 * 6.228565216064453
Epoch 210, val loss: 1.5939955711364746
Epoch 220, training loss: 7.707035064697266 = 1.4871947765350342 + 1.0 * 6.2198405265808105
Epoch 220, val loss: 1.5525240898132324
Epoch 230, training loss: 7.641271591186523 = 1.4293608665466309 + 1.0 * 6.211910724639893
Epoch 230, val loss: 1.5066008567810059
Epoch 240, training loss: 7.5739545822143555 = 1.3680212497711182 + 1.0 * 6.205933094024658
Epoch 240, val loss: 1.4581482410430908
Epoch 250, training loss: 7.505487442016602 = 1.306289792060852 + 1.0 * 6.199197769165039
Epoch 250, val loss: 1.4096853733062744
Epoch 260, training loss: 7.437229633331299 = 1.245315670967102 + 1.0 * 6.191914081573486
Epoch 260, val loss: 1.3619945049285889
Epoch 270, training loss: 7.3730010986328125 = 1.1868934631347656 + 1.0 * 6.186107635498047
Epoch 270, val loss: 1.3165912628173828
Epoch 280, training loss: 7.313475131988525 = 1.1324902772903442 + 1.0 * 6.180984973907471
Epoch 280, val loss: 1.2745691537857056
Epoch 290, training loss: 7.258001804351807 = 1.0825227499008179 + 1.0 * 6.175478935241699
Epoch 290, val loss: 1.236411690711975
Epoch 300, training loss: 7.204750061035156 = 1.036320447921753 + 1.0 * 6.168429851531982
Epoch 300, val loss: 1.2017287015914917
Epoch 310, training loss: 7.15568733215332 = 0.9927312135696411 + 1.0 * 6.162956237792969
Epoch 310, val loss: 1.16933012008667
Epoch 320, training loss: 7.1149773597717285 = 0.9514793157577515 + 1.0 * 6.1634979248046875
Epoch 320, val loss: 1.1389049291610718
Epoch 330, training loss: 7.065471649169922 = 0.9122082591056824 + 1.0 * 6.153263568878174
Epoch 330, val loss: 1.1108450889587402
Epoch 340, training loss: 7.023301124572754 = 0.873938798904419 + 1.0 * 6.149362087249756
Epoch 340, val loss: 1.083534836769104
Epoch 350, training loss: 6.986560821533203 = 0.8361490368843079 + 1.0 * 6.150411605834961
Epoch 350, val loss: 1.0566682815551758
Epoch 360, training loss: 6.942958354949951 = 0.7991234064102173 + 1.0 * 6.143835067749023
Epoch 360, val loss: 1.0304234027862549
Epoch 370, training loss: 6.902484893798828 = 0.7629156112670898 + 1.0 * 6.139569282531738
Epoch 370, val loss: 1.0049383640289307
Epoch 380, training loss: 6.86188268661499 = 0.7269757390022278 + 1.0 * 6.134906768798828
Epoch 380, val loss: 0.9793938398361206
Epoch 390, training loss: 6.823277950286865 = 0.6915324330329895 + 1.0 * 6.131745338439941
Epoch 390, val loss: 0.9540091156959534
Epoch 400, training loss: 6.788164138793945 = 0.6570981740951538 + 1.0 * 6.131065845489502
Epoch 400, val loss: 0.9293383359909058
Epoch 410, training loss: 6.750783443450928 = 0.6238611936569214 + 1.0 * 6.126922130584717
Epoch 410, val loss: 0.9053651094436646
Epoch 420, training loss: 6.7192864418029785 = 0.5921621322631836 + 1.0 * 6.127124309539795
Epoch 420, val loss: 0.8827462792396545
Epoch 430, training loss: 6.683352470397949 = 0.5624735355377197 + 1.0 * 6.120879173278809
Epoch 430, val loss: 0.8619191646575928
Epoch 440, training loss: 6.652390480041504 = 0.5345638394355774 + 1.0 * 6.117826461791992
Epoch 440, val loss: 0.8428374528884888
Epoch 450, training loss: 6.623753547668457 = 0.508421778678894 + 1.0 * 6.115331649780273
Epoch 450, val loss: 0.8256857991218567
Epoch 460, training loss: 6.604796886444092 = 0.48409026861190796 + 1.0 * 6.120706558227539
Epoch 460, val loss: 0.8104714155197144
Epoch 470, training loss: 6.5742926597595215 = 0.46154624223709106 + 1.0 * 6.112746238708496
Epoch 470, val loss: 0.797487735748291
Epoch 480, training loss: 6.54777193069458 = 0.44028425216674805 + 1.0 * 6.107487678527832
Epoch 480, val loss: 0.7862560749053955
Epoch 490, training loss: 6.527619361877441 = 0.41982340812683105 + 1.0 * 6.107795715332031
Epoch 490, val loss: 0.7762391567230225
Epoch 500, training loss: 6.507220268249512 = 0.4000746011734009 + 1.0 * 6.1071457862854
Epoch 500, val loss: 0.7673017382621765
Epoch 510, training loss: 6.483255863189697 = 0.3807738721370697 + 1.0 * 6.102481842041016
Epoch 510, val loss: 0.75946044921875
Epoch 520, training loss: 6.46491813659668 = 0.3616146743297577 + 1.0 * 6.1033034324646
Epoch 520, val loss: 0.7520520091056824
Epoch 530, training loss: 6.442664623260498 = 0.34243834018707275 + 1.0 * 6.100226402282715
Epoch 530, val loss: 0.7451923489570618
Epoch 540, training loss: 6.420746326446533 = 0.32330840826034546 + 1.0 * 6.097437858581543
Epoch 540, val loss: 0.7387626767158508
Epoch 550, training loss: 6.397404670715332 = 0.3041110336780548 + 1.0 * 6.0932936668396
Epoch 550, val loss: 0.7327979207038879
Epoch 560, training loss: 6.383386135101318 = 0.2849639356136322 + 1.0 * 6.098422050476074
Epoch 560, val loss: 0.7272564172744751
Epoch 570, training loss: 6.358782768249512 = 0.2662387490272522 + 1.0 * 6.092544078826904
Epoch 570, val loss: 0.7223758697509766
Epoch 580, training loss: 6.335832595825195 = 0.24795493483543396 + 1.0 * 6.0878777503967285
Epoch 580, val loss: 0.7183090448379517
Epoch 590, training loss: 6.326645374298096 = 0.23030105233192444 + 1.0 * 6.096344470977783
Epoch 590, val loss: 0.7149984836578369
Epoch 600, training loss: 6.300312519073486 = 0.2136995494365692 + 1.0 * 6.086613178253174
Epoch 600, val loss: 0.7125263810157776
Epoch 610, training loss: 6.283032417297363 = 0.1982380896806717 + 1.0 * 6.084794521331787
Epoch 610, val loss: 0.7111652493476868
Epoch 620, training loss: 6.269539833068848 = 0.18390175700187683 + 1.0 * 6.085638046264648
Epoch 620, val loss: 0.7105519771575928
Epoch 630, training loss: 6.260234832763672 = 0.17077535390853882 + 1.0 * 6.089459419250488
Epoch 630, val loss: 0.7107081413269043
Epoch 640, training loss: 6.2402753829956055 = 0.15887188911437988 + 1.0 * 6.081403732299805
Epoch 640, val loss: 0.7116268873214722
Epoch 650, training loss: 6.2270307540893555 = 0.14805002510547638 + 1.0 * 6.078980922698975
Epoch 650, val loss: 0.7132487893104553
Epoch 660, training loss: 6.216703414916992 = 0.13822193443775177 + 1.0 * 6.078481674194336
Epoch 660, val loss: 0.715270459651947
Epoch 670, training loss: 6.20648193359375 = 0.12934915721416473 + 1.0 * 6.077132701873779
Epoch 670, val loss: 0.7179026007652283
Epoch 680, training loss: 6.194615364074707 = 0.12124284356832504 + 1.0 * 6.0733723640441895
Epoch 680, val loss: 0.7209427356719971
Epoch 690, training loss: 6.1900529861450195 = 0.11380218714475632 + 1.0 * 6.076251029968262
Epoch 690, val loss: 0.7242573499679565
Epoch 700, training loss: 6.188020706176758 = 0.10701711475849152 + 1.0 * 6.081003665924072
Epoch 700, val loss: 0.7276698350906372
Epoch 710, training loss: 6.1725287437438965 = 0.10087637603282928 + 1.0 * 6.071652412414551
Epoch 710, val loss: 0.7316418290138245
Epoch 720, training loss: 6.164027690887451 = 0.09520531445741653 + 1.0 * 6.068822383880615
Epoch 720, val loss: 0.7358087301254272
Epoch 730, training loss: 6.164308547973633 = 0.08994948118925095 + 1.0 * 6.074358940124512
Epoch 730, val loss: 0.7400707602500916
Epoch 740, training loss: 6.153936862945557 = 0.08510232716798782 + 1.0 * 6.06883430480957
Epoch 740, val loss: 0.7445916533470154
Epoch 750, training loss: 6.146783828735352 = 0.08060140907764435 + 1.0 * 6.066182613372803
Epoch 750, val loss: 0.7493334412574768
Epoch 760, training loss: 6.158148765563965 = 0.07641192525625229 + 1.0 * 6.081737041473389
Epoch 760, val loss: 0.754116952419281
Epoch 770, training loss: 6.136293888092041 = 0.0725538581609726 + 1.0 * 6.063740253448486
Epoch 770, val loss: 0.7590059041976929
Epoch 780, training loss: 6.131173610687256 = 0.06896836310625076 + 1.0 * 6.0622053146362305
Epoch 780, val loss: 0.764220654964447
Epoch 790, training loss: 6.130431175231934 = 0.06560271233320236 + 1.0 * 6.064828395843506
Epoch 790, val loss: 0.7694925665855408
Epoch 800, training loss: 6.1317138671875 = 0.06247333064675331 + 1.0 * 6.069240570068359
Epoch 800, val loss: 0.774643063545227
Epoch 810, training loss: 6.121078968048096 = 0.05957077443599701 + 1.0 * 6.0615081787109375
Epoch 810, val loss: 0.7801041007041931
Epoch 820, training loss: 6.115469455718994 = 0.056846845895051956 + 1.0 * 6.05862283706665
Epoch 820, val loss: 0.7856477499008179
Epoch 830, training loss: 6.114864826202393 = 0.054272398352622986 + 1.0 * 6.0605926513671875
Epoch 830, val loss: 0.7911637425422668
Epoch 840, training loss: 6.1131181716918945 = 0.05185860022902489 + 1.0 * 6.061259746551514
Epoch 840, val loss: 0.7967071533203125
Epoch 850, training loss: 6.107431888580322 = 0.04961315169930458 + 1.0 * 6.05781888961792
Epoch 850, val loss: 0.8023726940155029
Epoch 860, training loss: 6.102339267730713 = 0.04749484360218048 + 1.0 * 6.054844379425049
Epoch 860, val loss: 0.8081507682800293
Epoch 870, training loss: 6.103623390197754 = 0.045489851385354996 + 1.0 * 6.058133602142334
Epoch 870, val loss: 0.8138896226882935
Epoch 880, training loss: 6.099883556365967 = 0.043603163212537766 + 1.0 * 6.056280612945557
Epoch 880, val loss: 0.8194599151611328
Epoch 890, training loss: 6.096669673919678 = 0.04184607043862343 + 1.0 * 6.054823398590088
Epoch 890, val loss: 0.8252496123313904
Epoch 900, training loss: 6.091825485229492 = 0.040182147175073624 + 1.0 * 6.051643371582031
Epoch 900, val loss: 0.8310009837150574
Epoch 910, training loss: 6.091274738311768 = 0.038599513471126556 + 1.0 * 6.052675247192383
Epoch 910, val loss: 0.8366976380348206
Epoch 920, training loss: 6.089921951293945 = 0.03710746392607689 + 1.0 * 6.052814483642578
Epoch 920, val loss: 0.8423361778259277
Epoch 930, training loss: 6.088136672973633 = 0.03571686893701553 + 1.0 * 6.052419662475586
Epoch 930, val loss: 0.8479886651039124
Epoch 940, training loss: 6.089018821716309 = 0.03440563380718231 + 1.0 * 6.05461311340332
Epoch 940, val loss: 0.8537428379058838
Epoch 950, training loss: 6.081908226013184 = 0.03315822407603264 + 1.0 * 6.048749923706055
Epoch 950, val loss: 0.8592774271965027
Epoch 960, training loss: 6.078917026519775 = 0.031978972256183624 + 1.0 * 6.046937942504883
Epoch 960, val loss: 0.864894688129425
Epoch 970, training loss: 6.079083442687988 = 0.03085382655262947 + 1.0 * 6.048229694366455
Epoch 970, val loss: 0.8704942464828491
Epoch 980, training loss: 6.081040859222412 = 0.029784876853227615 + 1.0 * 6.05125617980957
Epoch 980, val loss: 0.8759618997573853
Epoch 990, training loss: 6.075879096984863 = 0.028773358091711998 + 1.0 * 6.04710578918457
Epoch 990, val loss: 0.8814451694488525
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.317658424377441 = 1.9437835216522217 + 1.0 * 8.37387466430664
Epoch 0, val loss: 1.9395873546600342
Epoch 10, training loss: 10.30673599243164 = 1.9332165718078613 + 1.0 * 8.373519897460938
Epoch 10, val loss: 1.9298756122589111
Epoch 20, training loss: 10.290904998779297 = 1.9199587106704712 + 1.0 * 8.370945930480957
Epoch 20, val loss: 1.9173855781555176
Epoch 30, training loss: 10.253199577331543 = 1.9012095928192139 + 1.0 * 8.35198974609375
Epoch 30, val loss: 1.89957594871521
Epoch 40, training loss: 10.107864379882812 = 1.8767961263656616 + 1.0 * 8.23106861114502
Epoch 40, val loss: 1.8775602579116821
Epoch 50, training loss: 9.659384727478027 = 1.8511475324630737 + 1.0 * 7.808237552642822
Epoch 50, val loss: 1.8547817468643188
Epoch 60, training loss: 9.362679481506348 = 1.8281012773513794 + 1.0 * 7.5345778465271
Epoch 60, val loss: 1.8350510597229004
Epoch 70, training loss: 8.926570892333984 = 1.8114993572235107 + 1.0 * 7.1150712966918945
Epoch 70, val loss: 1.8216214179992676
Epoch 80, training loss: 8.577642440795898 = 1.8012406826019287 + 1.0 * 6.776401519775391
Epoch 80, val loss: 1.813233494758606
Epoch 90, training loss: 8.416157722473145 = 1.7879835367202759 + 1.0 * 6.628174304962158
Epoch 90, val loss: 1.8010426759719849
Epoch 100, training loss: 8.297587394714355 = 1.7708700895309448 + 1.0 * 6.526717662811279
Epoch 100, val loss: 1.7864888906478882
Epoch 110, training loss: 8.221029281616211 = 1.754080057144165 + 1.0 * 6.466948986053467
Epoch 110, val loss: 1.7724251747131348
Epoch 120, training loss: 8.152892112731934 = 1.7367221117019653 + 1.0 * 6.4161696434021
Epoch 120, val loss: 1.757434606552124
Epoch 130, training loss: 8.093433380126953 = 1.7174144983291626 + 1.0 * 6.37601900100708
Epoch 130, val loss: 1.7408188581466675
Epoch 140, training loss: 8.038490295410156 = 1.695268988609314 + 1.0 * 6.343221664428711
Epoch 140, val loss: 1.7224831581115723
Epoch 150, training loss: 7.985939979553223 = 1.6693731546401978 + 1.0 * 6.3165669441223145
Epoch 150, val loss: 1.7015401124954224
Epoch 160, training loss: 7.941250324249268 = 1.6387368440628052 + 1.0 * 6.302513599395752
Epoch 160, val loss: 1.6770344972610474
Epoch 170, training loss: 7.883597373962402 = 1.6040029525756836 + 1.0 * 6.279594421386719
Epoch 170, val loss: 1.6488910913467407
Epoch 180, training loss: 7.828930377960205 = 1.5645803213119507 + 1.0 * 6.264349937438965
Epoch 180, val loss: 1.6168112754821777
Epoch 190, training loss: 7.771115779876709 = 1.520534873008728 + 1.0 * 6.250580787658691
Epoch 190, val loss: 1.5808697938919067
Epoch 200, training loss: 7.713314056396484 = 1.4722400903701782 + 1.0 * 6.241074085235596
Epoch 200, val loss: 1.5415219068527222
Epoch 210, training loss: 7.6521315574646 = 1.4215329885482788 + 1.0 * 6.230598449707031
Epoch 210, val loss: 1.5006103515625
Epoch 220, training loss: 7.588992118835449 = 1.3695173263549805 + 1.0 * 6.219474792480469
Epoch 220, val loss: 1.4588850736618042
Epoch 230, training loss: 7.527182579040527 = 1.3166592121124268 + 1.0 * 6.21052360534668
Epoch 230, val loss: 1.4168343544006348
Epoch 240, training loss: 7.467324256896973 = 1.263644814491272 + 1.0 * 6.20367956161499
Epoch 240, val loss: 1.3753547668457031
Epoch 250, training loss: 7.407858848571777 = 1.2113810777664185 + 1.0 * 6.196477890014648
Epoch 250, val loss: 1.335097074508667
Epoch 260, training loss: 7.349403381347656 = 1.1598132848739624 + 1.0 * 6.189589977264404
Epoch 260, val loss: 1.2961188554763794
Epoch 270, training loss: 7.299426078796387 = 1.1097862720489502 + 1.0 * 6.189639568328857
Epoch 270, val loss: 1.2589473724365234
Epoch 280, training loss: 7.24262809753418 = 1.0619843006134033 + 1.0 * 6.180643558502197
Epoch 280, val loss: 1.22372567653656
Epoch 290, training loss: 7.189377307891846 = 1.0155850648880005 + 1.0 * 6.173792362213135
Epoch 290, val loss: 1.190114140510559
Epoch 300, training loss: 7.138750076293945 = 0.9704208374023438 + 1.0 * 6.168329238891602
Epoch 300, val loss: 1.157480239868164
Epoch 310, training loss: 7.094083309173584 = 0.926671028137207 + 1.0 * 6.167412281036377
Epoch 310, val loss: 1.1261041164398193
Epoch 320, training loss: 7.046639442443848 = 0.8848710060119629 + 1.0 * 6.161768436431885
Epoch 320, val loss: 1.0961848497390747
Epoch 330, training loss: 7.000215530395508 = 0.8444566130638123 + 1.0 * 6.155758857727051
Epoch 330, val loss: 1.0673397779464722
Epoch 340, training loss: 6.9574737548828125 = 0.8051013350486755 + 1.0 * 6.152372360229492
Epoch 340, val loss: 1.0394442081451416
Epoch 350, training loss: 6.917937755584717 = 0.766932487487793 + 1.0 * 6.151005268096924
Epoch 350, val loss: 1.0126159191131592
Epoch 360, training loss: 6.877351760864258 = 0.7301703691482544 + 1.0 * 6.147181510925293
Epoch 360, val loss: 0.9870569109916687
Epoch 370, training loss: 6.837214946746826 = 0.6946295499801636 + 1.0 * 6.142585277557373
Epoch 370, val loss: 0.9627349972724915
Epoch 380, training loss: 6.8024373054504395 = 0.660093367099762 + 1.0 * 6.142343997955322
Epoch 380, val loss: 0.9394783973693848
Epoch 390, training loss: 6.766887187957764 = 0.6268400549888611 + 1.0 * 6.140047073364258
Epoch 390, val loss: 0.9174665212631226
Epoch 400, training loss: 6.729161739349365 = 0.5948860049247742 + 1.0 * 6.134275913238525
Epoch 400, val loss: 0.8969199657440186
Epoch 410, training loss: 6.700302600860596 = 0.5641213059425354 + 1.0 * 6.136181354522705
Epoch 410, val loss: 0.8775689005851746
Epoch 420, training loss: 6.665703773498535 = 0.5346578359603882 + 1.0 * 6.131045818328857
Epoch 420, val loss: 0.8596866726875305
Epoch 430, training loss: 6.632950305938721 = 0.5064882040023804 + 1.0 * 6.126461982727051
Epoch 430, val loss: 0.8431602716445923
Epoch 440, training loss: 6.606984615325928 = 0.4794524610042572 + 1.0 * 6.127532005310059
Epoch 440, val loss: 0.8279386162757874
Epoch 450, training loss: 6.576620578765869 = 0.45353713631629944 + 1.0 * 6.123083591461182
Epoch 450, val loss: 0.8140339255332947
Epoch 460, training loss: 6.551593780517578 = 0.42863497138023376 + 1.0 * 6.122958660125732
Epoch 460, val loss: 0.8013631701469421
Epoch 470, training loss: 6.521638870239258 = 0.40468043088912964 + 1.0 * 6.1169586181640625
Epoch 470, val loss: 0.7898202538490295
Epoch 480, training loss: 6.499281883239746 = 0.38147106766700745 + 1.0 * 6.1178107261657715
Epoch 480, val loss: 0.7793663144111633
Epoch 490, training loss: 6.47612190246582 = 0.3592347800731659 + 1.0 * 6.116887092590332
Epoch 490, val loss: 0.7698177099227905
Epoch 500, training loss: 6.4480791091918945 = 0.3379189968109131 + 1.0 * 6.1101603507995605
Epoch 500, val loss: 0.7614413499832153
Epoch 510, training loss: 6.425296783447266 = 0.3174503445625305 + 1.0 * 6.107846260070801
Epoch 510, val loss: 0.7540236711502075
Epoch 520, training loss: 6.408784866333008 = 0.29787421226501465 + 1.0 * 6.110910415649414
Epoch 520, val loss: 0.7475448250770569
Epoch 530, training loss: 6.387308597564697 = 0.27934154868125916 + 1.0 * 6.107966899871826
Epoch 530, val loss: 0.7419971227645874
Epoch 540, training loss: 6.371864318847656 = 0.26180514693260193 + 1.0 * 6.1100592613220215
Epoch 540, val loss: 0.7374131679534912
Epoch 550, training loss: 6.345818519592285 = 0.2453959584236145 + 1.0 * 6.100422382354736
Epoch 550, val loss: 0.7337719798088074
Epoch 560, training loss: 6.328547477722168 = 0.2299261838197708 + 1.0 * 6.098621368408203
Epoch 560, val loss: 0.731059193611145
Epoch 570, training loss: 6.312384605407715 = 0.21533483266830444 + 1.0 * 6.097049713134766
Epoch 570, val loss: 0.7291282415390015
Epoch 580, training loss: 6.302750587463379 = 0.20157980918884277 + 1.0 * 6.101170539855957
Epoch 580, val loss: 0.7279750108718872
Epoch 590, training loss: 6.291182994842529 = 0.1887512058019638 + 1.0 * 6.102431774139404
Epoch 590, val loss: 0.7274807691574097
Epoch 600, training loss: 6.272633075714111 = 0.1768013834953308 + 1.0 * 6.095831871032715
Epoch 600, val loss: 0.7276781797409058
Epoch 610, training loss: 6.257387161254883 = 0.16562819480895996 + 1.0 * 6.091758728027344
Epoch 610, val loss: 0.7286010384559631
Epoch 620, training loss: 6.243865013122559 = 0.15517745912075043 + 1.0 * 6.088687419891357
Epoch 620, val loss: 0.7301549315452576
Epoch 630, training loss: 6.235551834106445 = 0.14540624618530273 + 1.0 * 6.090145587921143
Epoch 630, val loss: 0.7323095798492432
Epoch 640, training loss: 6.22712516784668 = 0.136336088180542 + 1.0 * 6.090788841247559
Epoch 640, val loss: 0.7349619269371033
Epoch 650, training loss: 6.212527275085449 = 0.12795162200927734 + 1.0 * 6.084575653076172
Epoch 650, val loss: 0.7381067276000977
Epoch 660, training loss: 6.217939376831055 = 0.12017747014760971 + 1.0 * 6.097762107849121
Epoch 660, val loss: 0.7417199611663818
Epoch 670, training loss: 6.195413589477539 = 0.11298630386590958 + 1.0 * 6.082427501678467
Epoch 670, val loss: 0.7457816004753113
Epoch 680, training loss: 6.1876397132873535 = 0.10632674396038055 + 1.0 * 6.081313133239746
Epoch 680, val loss: 0.7502491474151611
Epoch 690, training loss: 6.1817474365234375 = 0.10014083981513977 + 1.0 * 6.081606388092041
Epoch 690, val loss: 0.7551097869873047
Epoch 700, training loss: 6.172584533691406 = 0.0944252461194992 + 1.0 * 6.078159332275391
Epoch 700, val loss: 0.7602710127830505
Epoch 710, training loss: 6.167487621307373 = 0.08913750946521759 + 1.0 * 6.078350067138672
Epoch 710, val loss: 0.7657339572906494
Epoch 720, training loss: 6.161332130432129 = 0.08422810584306717 + 1.0 * 6.077104091644287
Epoch 720, val loss: 0.7714797258377075
Epoch 730, training loss: 6.154644012451172 = 0.07967216521501541 + 1.0 * 6.074971675872803
Epoch 730, val loss: 0.7774578332901001
Epoch 740, training loss: 6.150731086730957 = 0.07545901834964752 + 1.0 * 6.075272083282471
Epoch 740, val loss: 0.7835694551467896
Epoch 750, training loss: 6.146767616271973 = 0.07154200971126556 + 1.0 * 6.075225830078125
Epoch 750, val loss: 0.7898552417755127
Epoch 760, training loss: 6.139272212982178 = 0.06789561361074448 + 1.0 * 6.071376800537109
Epoch 760, val loss: 0.7962973117828369
Epoch 770, training loss: 6.135344505310059 = 0.06449431926012039 + 1.0 * 6.070850372314453
Epoch 770, val loss: 0.802918553352356
Epoch 780, training loss: 6.134807109832764 = 0.061322249472141266 + 1.0 * 6.073484897613525
Epoch 780, val loss: 0.8096367716789246
Epoch 790, training loss: 6.132216453552246 = 0.058378759771585464 + 1.0 * 6.073837757110596
Epoch 790, val loss: 0.816416323184967
Epoch 800, training loss: 6.124719619750977 = 0.05563078448176384 + 1.0 * 6.069088935852051
Epoch 800, val loss: 0.8231790661811829
Epoch 810, training loss: 6.120210647583008 = 0.05306270718574524 + 1.0 * 6.067147731781006
Epoch 810, val loss: 0.8300396800041199
Epoch 820, training loss: 6.116792678833008 = 0.05065705254673958 + 1.0 * 6.066135406494141
Epoch 820, val loss: 0.8369359970092773
Epoch 830, training loss: 6.1117472648620605 = 0.04840022325515747 + 1.0 * 6.063346862792969
Epoch 830, val loss: 0.8438490033149719
Epoch 840, training loss: 6.122980117797852 = 0.046279702335596085 + 1.0 * 6.076700210571289
Epoch 840, val loss: 0.8507987260818481
Epoch 850, training loss: 6.105754852294922 = 0.04430225118994713 + 1.0 * 6.061452388763428
Epoch 850, val loss: 0.8576570153236389
Epoch 860, training loss: 6.104086875915527 = 0.042443085461854935 + 1.0 * 6.061643600463867
Epoch 860, val loss: 0.8645192384719849
Epoch 870, training loss: 6.100146293640137 = 0.040687814354896545 + 1.0 * 6.059458255767822
Epoch 870, val loss: 0.8714151978492737
Epoch 880, training loss: 6.110877513885498 = 0.03903492912650108 + 1.0 * 6.071842670440674
Epoch 880, val loss: 0.8782778978347778
Epoch 890, training loss: 6.095385551452637 = 0.03748464584350586 + 1.0 * 6.057900905609131
Epoch 890, val loss: 0.8850082159042358
Epoch 900, training loss: 6.093657970428467 = 0.03602088242769241 + 1.0 * 6.0576372146606445
Epoch 900, val loss: 0.8917023539543152
Epoch 910, training loss: 6.092748641967773 = 0.03463355079293251 + 1.0 * 6.058115005493164
Epoch 910, val loss: 0.8984204530715942
Epoch 920, training loss: 6.08984899520874 = 0.03332563862204552 + 1.0 * 6.056523323059082
Epoch 920, val loss: 0.9050343632698059
Epoch 930, training loss: 6.0904717445373535 = 0.032093506306409836 + 1.0 * 6.058378219604492
Epoch 930, val loss: 0.9115020632743835
Epoch 940, training loss: 6.084855556488037 = 0.03092925064265728 + 1.0 * 6.053926467895508
Epoch 940, val loss: 0.9179638624191284
Epoch 950, training loss: 6.082853317260742 = 0.029820697382092476 + 1.0 * 6.053032398223877
Epoch 950, val loss: 0.9244174361228943
Epoch 960, training loss: 6.082576751708984 = 0.028766172006726265 + 1.0 * 6.0538105964660645
Epoch 960, val loss: 0.9308313131332397
Epoch 970, training loss: 6.081093788146973 = 0.027766261249780655 + 1.0 * 6.053327560424805
Epoch 970, val loss: 0.937146782875061
Epoch 980, training loss: 6.077536106109619 = 0.026819458231329918 + 1.0 * 6.050716876983643
Epoch 980, val loss: 0.9433391690254211
Epoch 990, training loss: 6.076313495635986 = 0.025917958468198776 + 1.0 * 6.050395488739014
Epoch 990, val loss: 0.9495135545730591
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.314667701721191 = 1.9408352375030518 + 1.0 * 8.373832702636719
Epoch 0, val loss: 1.9392788410186768
Epoch 10, training loss: 10.304367065429688 = 1.930995225906372 + 1.0 * 8.373372077941895
Epoch 10, val loss: 1.9296034574508667
Epoch 20, training loss: 10.289011001586914 = 1.9189348220825195 + 1.0 * 8.370076179504395
Epoch 20, val loss: 1.9175631999969482
Epoch 30, training loss: 10.248438835144043 = 1.9021848440170288 + 1.0 * 8.346254348754883
Epoch 30, val loss: 1.9007446765899658
Epoch 40, training loss: 10.06143569946289 = 1.8810549974441528 + 1.0 * 8.180380821228027
Epoch 40, val loss: 1.8802791833877563
Epoch 50, training loss: 9.445365905761719 = 1.8580255508422852 + 1.0 * 7.587339878082275
Epoch 50, val loss: 1.8581181764602661
Epoch 60, training loss: 8.944940567016602 = 1.8397464752197266 + 1.0 * 7.105194568634033
Epoch 60, val loss: 1.8413830995559692
Epoch 70, training loss: 8.581207275390625 = 1.8276071548461914 + 1.0 * 6.753600597381592
Epoch 70, val loss: 1.8301600217819214
Epoch 80, training loss: 8.451872825622559 = 1.814876675605774 + 1.0 * 6.636996269226074
Epoch 80, val loss: 1.8183953762054443
Epoch 90, training loss: 8.368378639221191 = 1.7973662614822388 + 1.0 * 6.571012496948242
Epoch 90, val loss: 1.8029499053955078
Epoch 100, training loss: 8.291389465332031 = 1.7795779705047607 + 1.0 * 6.511811256408691
Epoch 100, val loss: 1.7875362634658813
Epoch 110, training loss: 8.225604057312012 = 1.7630207538604736 + 1.0 * 6.462583065032959
Epoch 110, val loss: 1.7731772661209106
Epoch 120, training loss: 8.166297912597656 = 1.7464988231658936 + 1.0 * 6.419798851013184
Epoch 120, val loss: 1.7585633993148804
Epoch 130, training loss: 8.107965469360352 = 1.7282886505126953 + 1.0 * 6.379676818847656
Epoch 130, val loss: 1.7426172494888306
Epoch 140, training loss: 8.054394721984863 = 1.7072737216949463 + 1.0 * 6.347120761871338
Epoch 140, val loss: 1.7247391939163208
Epoch 150, training loss: 8.002942085266113 = 1.6826070547103882 + 1.0 * 6.3203349113464355
Epoch 150, val loss: 1.7041749954223633
Epoch 160, training loss: 7.951074600219727 = 1.6531760692596436 + 1.0 * 6.297898292541504
Epoch 160, val loss: 1.6798259019851685
Epoch 170, training loss: 7.896800518035889 = 1.6179341077804565 + 1.0 * 6.278866291046143
Epoch 170, val loss: 1.650729775428772
Epoch 180, training loss: 7.84117317199707 = 1.5759246349334717 + 1.0 * 6.265248775482178
Epoch 180, val loss: 1.6161291599273682
Epoch 190, training loss: 7.777425765991211 = 1.5277293920516968 + 1.0 * 6.249696254730225
Epoch 190, val loss: 1.5768135786056519
Epoch 200, training loss: 7.710137367248535 = 1.4736961126327515 + 1.0 * 6.236441135406494
Epoch 200, val loss: 1.5327082872390747
Epoch 210, training loss: 7.641368389129639 = 1.414687156677246 + 1.0 * 6.226681232452393
Epoch 210, val loss: 1.4849820137023926
Epoch 220, training loss: 7.570630073547363 = 1.353887915611267 + 1.0 * 6.216742038726807
Epoch 220, val loss: 1.4361070394515991
Epoch 230, training loss: 7.500406742095947 = 1.2932466268539429 + 1.0 * 6.207159996032715
Epoch 230, val loss: 1.3878517150878906
Epoch 240, training loss: 7.436578273773193 = 1.2335925102233887 + 1.0 * 6.202985763549805
Epoch 240, val loss: 1.3408124446868896
Epoch 250, training loss: 7.370211601257324 = 1.1765131950378418 + 1.0 * 6.193698406219482
Epoch 250, val loss: 1.296488642692566
Epoch 260, training loss: 7.3082427978515625 = 1.121838927268982 + 1.0 * 6.186403751373291
Epoch 260, val loss: 1.254328966140747
Epoch 270, training loss: 7.251031875610352 = 1.0689324140548706 + 1.0 * 6.182099342346191
Epoch 270, val loss: 1.2137857675552368
Epoch 280, training loss: 7.194953918457031 = 1.0177781581878662 + 1.0 * 6.177175998687744
Epoch 280, val loss: 1.1748000383377075
Epoch 290, training loss: 7.140516757965088 = 0.9683898091316223 + 1.0 * 6.172126770019531
Epoch 290, val loss: 1.1372355222702026
Epoch 300, training loss: 7.085556507110596 = 0.9203709363937378 + 1.0 * 6.165185451507568
Epoch 300, val loss: 1.1009596586227417
Epoch 310, training loss: 7.037100791931152 = 0.8734720945358276 + 1.0 * 6.163628578186035
Epoch 310, val loss: 1.065644383430481
Epoch 320, training loss: 6.984422206878662 = 0.8282067179679871 + 1.0 * 6.156215667724609
Epoch 320, val loss: 1.031673789024353
Epoch 330, training loss: 6.93855619430542 = 0.7845039963722229 + 1.0 * 6.154052257537842
Epoch 330, val loss: 0.9990851283073425
Epoch 340, training loss: 6.891810417175293 = 0.7424500584602356 + 1.0 * 6.149360179901123
Epoch 340, val loss: 0.9680445790290833
Epoch 350, training loss: 6.849977970123291 = 0.7023286819458008 + 1.0 * 6.14764928817749
Epoch 350, val loss: 0.9387157559394836
Epoch 360, training loss: 6.804788112640381 = 0.6641495823860168 + 1.0 * 6.14063835144043
Epoch 360, val loss: 0.9113206267356873
Epoch 370, training loss: 6.768773078918457 = 0.6280919313430786 + 1.0 * 6.140681266784668
Epoch 370, val loss: 0.8860079646110535
Epoch 380, training loss: 6.728863716125488 = 0.5943885445594788 + 1.0 * 6.134475231170654
Epoch 380, val loss: 0.8630298376083374
Epoch 390, training loss: 6.692667007446289 = 0.5626764297485352 + 1.0 * 6.129990577697754
Epoch 390, val loss: 0.8422856330871582
Epoch 400, training loss: 6.661149024963379 = 0.5329430103302002 + 1.0 * 6.1282057762146
Epoch 400, val loss: 0.8236587643623352
Epoch 410, training loss: 6.63011360168457 = 0.5051920413970947 + 1.0 * 6.1249213218688965
Epoch 410, val loss: 0.8073986172676086
Epoch 420, training loss: 6.600376129150391 = 0.4790644347667694 + 1.0 * 6.121311664581299
Epoch 420, val loss: 0.7931106686592102
Epoch 430, training loss: 6.571920394897461 = 0.45435410737991333 + 1.0 * 6.117566108703613
Epoch 430, val loss: 0.780595064163208
Epoch 440, training loss: 6.5524492263793945 = 0.4308260977268219 + 1.0 * 6.1216230392456055
Epoch 440, val loss: 0.769615113735199
Epoch 450, training loss: 6.520570278167725 = 0.4084988236427307 + 1.0 * 6.112071514129639
Epoch 450, val loss: 0.7600335478782654
Epoch 460, training loss: 6.497620105743408 = 0.38718318939208984 + 1.0 * 6.110436916351318
Epoch 460, val loss: 0.7517833113670349
Epoch 470, training loss: 6.4803948402404785 = 0.3667844831943512 + 1.0 * 6.11361026763916
Epoch 470, val loss: 0.7446576356887817
Epoch 480, training loss: 6.457664966583252 = 0.34736746549606323 + 1.0 * 6.110297679901123
Epoch 480, val loss: 0.7386577725410461
Epoch 490, training loss: 6.433679103851318 = 0.3289981782436371 + 1.0 * 6.104681015014648
Epoch 490, val loss: 0.7336556911468506
Epoch 500, training loss: 6.412177085876465 = 0.31137973070144653 + 1.0 * 6.100797176361084
Epoch 500, val loss: 0.7297043204307556
Epoch 510, training loss: 6.393024921417236 = 0.29447364807128906 + 1.0 * 6.098551273345947
Epoch 510, val loss: 0.7265629768371582
Epoch 520, training loss: 6.383333683013916 = 0.2781778872013092 + 1.0 * 6.105155944824219
Epoch 520, val loss: 0.7242176532745361
Epoch 530, training loss: 6.3620123863220215 = 0.26266366243362427 + 1.0 * 6.099348545074463
Epoch 530, val loss: 0.7225168347358704
Epoch 540, training loss: 6.341538429260254 = 0.24774591624736786 + 1.0 * 6.09379243850708
Epoch 540, val loss: 0.7215611934661865
Epoch 550, training loss: 6.325425624847412 = 0.23333978652954102 + 1.0 * 6.092085838317871
Epoch 550, val loss: 0.7211509943008423
Epoch 560, training loss: 6.312368869781494 = 0.2194574475288391 + 1.0 * 6.092911243438721
Epoch 560, val loss: 0.7212095260620117
Epoch 570, training loss: 6.295075416564941 = 0.2060748189687729 + 1.0 * 6.089000701904297
Epoch 570, val loss: 0.7218057513237
Epoch 580, training loss: 6.2842254638671875 = 0.193211168050766 + 1.0 * 6.091014385223389
Epoch 580, val loss: 0.7227972745895386
Epoch 590, training loss: 6.2720537185668945 = 0.1809755265712738 + 1.0 * 6.091078281402588
Epoch 590, val loss: 0.7241573929786682
Epoch 600, training loss: 6.2538018226623535 = 0.16939722001552582 + 1.0 * 6.084404468536377
Epoch 600, val loss: 0.7260622382164001
Epoch 610, training loss: 6.241666316986084 = 0.15844614803791046 + 1.0 * 6.0832200050354
Epoch 610, val loss: 0.7283432483673096
Epoch 620, training loss: 6.243110656738281 = 0.1481233537197113 + 1.0 * 6.094987392425537
Epoch 620, val loss: 0.7310436964035034
Epoch 630, training loss: 6.21860408782959 = 0.13860201835632324 + 1.0 * 6.0800018310546875
Epoch 630, val loss: 0.7340605854988098
Epoch 640, training loss: 6.209322929382324 = 0.1297377347946167 + 1.0 * 6.079585075378418
Epoch 640, val loss: 0.7376251220703125
Epoch 650, training loss: 6.205092906951904 = 0.12150868773460388 + 1.0 * 6.083584308624268
Epoch 650, val loss: 0.7414920330047607
Epoch 660, training loss: 6.19409704208374 = 0.1139555275440216 + 1.0 * 6.080141544342041
Epoch 660, val loss: 0.7457303404808044
Epoch 670, training loss: 6.185256004333496 = 0.106998510658741 + 1.0 * 6.0782575607299805
Epoch 670, val loss: 0.7502732276916504
Epoch 680, training loss: 6.174356460571289 = 0.10060404986143112 + 1.0 * 6.073752403259277
Epoch 680, val loss: 0.7550625205039978
Epoch 690, training loss: 6.169423580169678 = 0.09471577405929565 + 1.0 * 6.074707984924316
Epoch 690, val loss: 0.7601566910743713
Epoch 700, training loss: 6.161737442016602 = 0.08930862694978714 + 1.0 * 6.0724287033081055
Epoch 700, val loss: 0.7653604745864868
Epoch 710, training loss: 6.156514644622803 = 0.084341861307621 + 1.0 * 6.07217264175415
Epoch 710, val loss: 0.7708600759506226
Epoch 720, training loss: 6.149100303649902 = 0.07976245880126953 + 1.0 * 6.069337844848633
Epoch 720, val loss: 0.7765267491340637
Epoch 730, training loss: 6.150914669036865 = 0.07552352547645569 + 1.0 * 6.0753912925720215
Epoch 730, val loss: 0.7822476625442505
Epoch 740, training loss: 6.14268684387207 = 0.0716027021408081 + 1.0 * 6.071084022521973
Epoch 740, val loss: 0.7880104184150696
Epoch 750, training loss: 6.1379828453063965 = 0.06799402832984924 + 1.0 * 6.06998872756958
Epoch 750, val loss: 0.7939696907997131
Epoch 760, training loss: 6.132173538208008 = 0.06463830918073654 + 1.0 * 6.067535400390625
Epoch 760, val loss: 0.7999309301376343
Epoch 770, training loss: 6.1258039474487305 = 0.06151140108704567 + 1.0 * 6.0642924308776855
Epoch 770, val loss: 0.8059422969818115
Epoch 780, training loss: 6.125398635864258 = 0.058594461530447006 + 1.0 * 6.0668044090271
Epoch 780, val loss: 0.8120440244674683
Epoch 790, training loss: 6.120907783508301 = 0.055885616689920425 + 1.0 * 6.065021991729736
Epoch 790, val loss: 0.8179363012313843
Epoch 800, training loss: 6.115207672119141 = 0.05335105583071709 + 1.0 * 6.061856746673584
Epoch 800, val loss: 0.8238728642463684
Epoch 810, training loss: 6.111745834350586 = 0.0509827546775341 + 1.0 * 6.060762882232666
Epoch 810, val loss: 0.829859733581543
Epoch 820, training loss: 6.116613388061523 = 0.04875499755144119 + 1.0 * 6.0678582191467285
Epoch 820, val loss: 0.8357165455818176
Epoch 830, training loss: 6.112910270690918 = 0.04668397828936577 + 1.0 * 6.066226482391357
Epoch 830, val loss: 0.8414202928543091
Epoch 840, training loss: 6.104764938354492 = 0.044742386788129807 + 1.0 * 6.060022354125977
Epoch 840, val loss: 0.8472818732261658
Epoch 850, training loss: 6.099469184875488 = 0.0429096519947052 + 1.0 * 6.0565595626831055
Epoch 850, val loss: 0.8530861139297485
Epoch 860, training loss: 6.104010581970215 = 0.041181452572345734 + 1.0 * 6.06282901763916
Epoch 860, val loss: 0.858704149723053
Epoch 870, training loss: 6.0987138748168945 = 0.03954672813415527 + 1.0 * 6.05916690826416
Epoch 870, val loss: 0.8642547726631165
Epoch 880, training loss: 6.093395709991455 = 0.03801463916897774 + 1.0 * 6.0553812980651855
Epoch 880, val loss: 0.8698195815086365
Epoch 890, training loss: 6.0928263664245605 = 0.03655937314033508 + 1.0 * 6.056266784667969
Epoch 890, val loss: 0.8753502368927002
Epoch 900, training loss: 6.089825630187988 = 0.0351887010037899 + 1.0 * 6.0546369552612305
Epoch 900, val loss: 0.8807129859924316
Epoch 910, training loss: 6.087634563446045 = 0.03389287367463112 + 1.0 * 6.053741455078125
Epoch 910, val loss: 0.8861003518104553
Epoch 920, training loss: 6.085106372833252 = 0.032666370272636414 + 1.0 * 6.052440166473389
Epoch 920, val loss: 0.891441822052002
Epoch 930, training loss: 6.089150428771973 = 0.03149950131773949 + 1.0 * 6.057651042938232
Epoch 930, val loss: 0.8966690897941589
Epoch 940, training loss: 6.085240840911865 = 0.03038889169692993 + 1.0 * 6.05485200881958
Epoch 940, val loss: 0.9018649458885193
Epoch 950, training loss: 6.080718994140625 = 0.029342403635382652 + 1.0 * 6.051376819610596
Epoch 950, val loss: 0.9069600105285645
Epoch 960, training loss: 6.077384948730469 = 0.028342492878437042 + 1.0 * 6.049042224884033
Epoch 960, val loss: 0.9120852947235107
Epoch 970, training loss: 6.081063270568848 = 0.027389882132411003 + 1.0 * 6.053673267364502
Epoch 970, val loss: 0.9170688986778259
Epoch 980, training loss: 6.075523853302002 = 0.026482142508029938 + 1.0 * 6.049041748046875
Epoch 980, val loss: 0.9219942092895508
Epoch 990, training loss: 6.078073501586914 = 0.025620821863412857 + 1.0 * 6.052452564239502
Epoch 990, val loss: 0.9268051981925964
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7048
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.337847709655762 = 1.9639521837234497 + 1.0 * 8.373895645141602
Epoch 0, val loss: 1.9636071920394897
Epoch 10, training loss: 10.327356338500977 = 1.9538109302520752 + 1.0 * 8.37354564666748
Epoch 10, val loss: 1.9539892673492432
Epoch 20, training loss: 10.311768531799316 = 1.940593957901001 + 1.0 * 8.371174812316895
Epoch 20, val loss: 1.9411799907684326
Epoch 30, training loss: 10.27508544921875 = 1.921330213546753 + 1.0 * 8.353754997253418
Epoch 30, val loss: 1.9222464561462402
Epoch 40, training loss: 10.134613037109375 = 1.8950504064559937 + 1.0 * 8.23956298828125
Epoch 40, val loss: 1.8971489667892456
Epoch 50, training loss: 9.650178909301758 = 1.8649688959121704 + 1.0 * 7.785210132598877
Epoch 50, val loss: 1.8681979179382324
Epoch 60, training loss: 9.264890670776367 = 1.8385173082351685 + 1.0 * 7.42637300491333
Epoch 60, val loss: 1.8443063497543335
Epoch 70, training loss: 8.803115844726562 = 1.8222103118896484 + 1.0 * 6.980905532836914
Epoch 70, val loss: 1.8300751447677612
Epoch 80, training loss: 8.562241554260254 = 1.8082727193832397 + 1.0 * 6.753969192504883
Epoch 80, val loss: 1.8173364400863647
Epoch 90, training loss: 8.418638229370117 = 1.7911362648010254 + 1.0 * 6.62750244140625
Epoch 90, val loss: 1.8016899824142456
Epoch 100, training loss: 8.320920944213867 = 1.773761510848999 + 1.0 * 6.547159671783447
Epoch 100, val loss: 1.786085605621338
Epoch 110, training loss: 8.236820220947266 = 1.7569327354431152 + 1.0 * 6.479887008666992
Epoch 110, val loss: 1.7705873250961304
Epoch 120, training loss: 8.167510032653809 = 1.7399221658706665 + 1.0 * 6.427587985992432
Epoch 120, val loss: 1.7547019720077515
Epoch 130, training loss: 8.10604190826416 = 1.7216808795928955 + 1.0 * 6.384361267089844
Epoch 130, val loss: 1.7379096746444702
Epoch 140, training loss: 8.054015159606934 = 1.7010929584503174 + 1.0 * 6.352921962738037
Epoch 140, val loss: 1.7196766138076782
Epoch 150, training loss: 7.999997138977051 = 1.6778199672698975 + 1.0 * 6.322177410125732
Epoch 150, val loss: 1.6997668743133545
Epoch 160, training loss: 7.951597213745117 = 1.651216745376587 + 1.0 * 6.300380229949951
Epoch 160, val loss: 1.6773180961608887
Epoch 170, training loss: 7.903320789337158 = 1.6204134225845337 + 1.0 * 6.282907485961914
Epoch 170, val loss: 1.6515649557113647
Epoch 180, training loss: 7.852713108062744 = 1.5850200653076172 + 1.0 * 6.267693042755127
Epoch 180, val loss: 1.6221153736114502
Epoch 190, training loss: 7.799482345581055 = 1.5449591875076294 + 1.0 * 6.254523277282715
Epoch 190, val loss: 1.5890142917633057
Epoch 200, training loss: 7.7447285652160645 = 1.5011144876480103 + 1.0 * 6.243614196777344
Epoch 200, val loss: 1.5532913208007812
Epoch 210, training loss: 7.687811374664307 = 1.454690933227539 + 1.0 * 6.233120441436768
Epoch 210, val loss: 1.5156936645507812
Epoch 220, training loss: 7.6289215087890625 = 1.4058969020843506 + 1.0 * 6.223024845123291
Epoch 220, val loss: 1.4765584468841553
Epoch 230, training loss: 7.56968879699707 = 1.3551698923110962 + 1.0 * 6.214519023895264
Epoch 230, val loss: 1.436356544494629
Epoch 240, training loss: 7.512739181518555 = 1.3041348457336426 + 1.0 * 6.208604335784912
Epoch 240, val loss: 1.396315336227417
Epoch 250, training loss: 7.453516483306885 = 1.2534642219543457 + 1.0 * 6.200052261352539
Epoch 250, val loss: 1.3567938804626465
Epoch 260, training loss: 7.395561695098877 = 1.2029948234558105 + 1.0 * 6.192566871643066
Epoch 260, val loss: 1.3174316883087158
Epoch 270, training loss: 7.340946197509766 = 1.152872920036316 + 1.0 * 6.18807315826416
Epoch 270, val loss: 1.278247594833374
Epoch 280, training loss: 7.287583827972412 = 1.1044483184814453 + 1.0 * 6.183135509490967
Epoch 280, val loss: 1.2403074502944946
Epoch 290, training loss: 7.233554840087891 = 1.0575945377349854 + 1.0 * 6.175960540771484
Epoch 290, val loss: 1.2034870386123657
Epoch 300, training loss: 7.182117938995361 = 1.0116299390792847 + 1.0 * 6.170487880706787
Epoch 300, val loss: 1.1673896312713623
Epoch 310, training loss: 7.136270523071289 = 0.9663770198822021 + 1.0 * 6.169893264770508
Epoch 310, val loss: 1.131874680519104
Epoch 320, training loss: 7.087273120880127 = 0.9221779108047485 + 1.0 * 6.165095329284668
Epoch 320, val loss: 1.0973455905914307
Epoch 330, training loss: 7.039830207824707 = 0.8788558840751648 + 1.0 * 6.160974502563477
Epoch 330, val loss: 1.0638293027877808
Epoch 340, training loss: 6.991760730743408 = 0.8362974524497986 + 1.0 * 6.155463218688965
Epoch 340, val loss: 1.0313974618911743
Epoch 350, training loss: 6.951759338378906 = 0.7944050431251526 + 1.0 * 6.157354354858398
Epoch 350, val loss: 0.9998897910118103
Epoch 360, training loss: 6.9027299880981445 = 0.7537077069282532 + 1.0 * 6.149022102355957
Epoch 360, val loss: 0.9697990417480469
Epoch 370, training loss: 6.860265731811523 = 0.7141700983047485 + 1.0 * 6.1460957527160645
Epoch 370, val loss: 0.9413402080535889
Epoch 380, training loss: 6.822739124298096 = 0.6760208010673523 + 1.0 * 6.146718502044678
Epoch 380, val loss: 0.9143099784851074
Epoch 390, training loss: 6.781583786010742 = 0.6397890448570251 + 1.0 * 6.141794681549072
Epoch 390, val loss: 0.8894929885864258
Epoch 400, training loss: 6.742948532104492 = 0.6054332852363586 + 1.0 * 6.137515068054199
Epoch 400, val loss: 0.8668532967567444
Epoch 410, training loss: 6.712706565856934 = 0.5728975534439087 + 1.0 * 6.1398091316223145
Epoch 410, val loss: 0.8463007211685181
Epoch 420, training loss: 6.676028728485107 = 0.5423708558082581 + 1.0 * 6.133657932281494
Epoch 420, val loss: 0.828052282333374
Epoch 430, training loss: 6.644923210144043 = 0.5136566162109375 + 1.0 * 6.1312665939331055
Epoch 430, val loss: 0.8122566342353821
Epoch 440, training loss: 6.613224983215332 = 0.486465722322464 + 1.0 * 6.126759052276611
Epoch 440, val loss: 0.7983778119087219
Epoch 450, training loss: 6.586464881896973 = 0.46041154861450195 + 1.0 * 6.126053333282471
Epoch 450, val loss: 0.7863467335700989
Epoch 460, training loss: 6.5612077713012695 = 0.4352876543998718 + 1.0 * 6.125920295715332
Epoch 460, val loss: 0.7757431268692017
Epoch 470, training loss: 6.535148620605469 = 0.4109669029712677 + 1.0 * 6.124181747436523
Epoch 470, val loss: 0.7663720846176147
Epoch 480, training loss: 6.506486415863037 = 0.3874918222427368 + 1.0 * 6.11899471282959
Epoch 480, val loss: 0.7583749294281006
Epoch 490, training loss: 6.481191635131836 = 0.3644949793815613 + 1.0 * 6.116696834564209
Epoch 490, val loss: 0.7514417171478271
Epoch 500, training loss: 6.455153942108154 = 0.34188807010650635 + 1.0 * 6.1132659912109375
Epoch 500, val loss: 0.7452794909477234
Epoch 510, training loss: 6.437708377838135 = 0.31971117854118347 + 1.0 * 6.117997169494629
Epoch 510, val loss: 0.7399623394012451
Epoch 520, training loss: 6.417600154876709 = 0.2982821762561798 + 1.0 * 6.119318008422852
Epoch 520, val loss: 0.7353980541229248
Epoch 530, training loss: 6.387254238128662 = 0.2778001129627228 + 1.0 * 6.109454154968262
Epoch 530, val loss: 0.731882631778717
Epoch 540, training loss: 6.363899230957031 = 0.2580985724925995 + 1.0 * 6.105800628662109
Epoch 540, val loss: 0.7294053435325623
Epoch 550, training loss: 6.343137264251709 = 0.23922158777713776 + 1.0 * 6.103915691375732
Epoch 550, val loss: 0.7276113629341125
Epoch 560, training loss: 6.333272933959961 = 0.2212793380022049 + 1.0 * 6.111993789672852
Epoch 560, val loss: 0.7266970276832581
Epoch 570, training loss: 6.30945348739624 = 0.20455965399742126 + 1.0 * 6.104893684387207
Epoch 570, val loss: 0.7267254590988159
Epoch 580, training loss: 6.290846347808838 = 0.18900102376937866 + 1.0 * 6.1018452644348145
Epoch 580, val loss: 0.7277640700340271
Epoch 590, training loss: 6.27596378326416 = 0.1745767593383789 + 1.0 * 6.101387023925781
Epoch 590, val loss: 0.7294905185699463
Epoch 600, training loss: 6.260012149810791 = 0.1613578498363495 + 1.0 * 6.098654270172119
Epoch 600, val loss: 0.7320379614830017
Epoch 610, training loss: 6.246567726135254 = 0.14930562674999237 + 1.0 * 6.097261905670166
Epoch 610, val loss: 0.7353722453117371
Epoch 620, training loss: 6.233754634857178 = 0.13840825855731964 + 1.0 * 6.095346450805664
Epoch 620, val loss: 0.7392804026603699
Epoch 630, training loss: 6.22185754776001 = 0.12858636677265167 + 1.0 * 6.093271255493164
Epoch 630, val loss: 0.7439204454421997
Epoch 640, training loss: 6.210607051849365 = 0.11969377100467682 + 1.0 * 6.09091329574585
Epoch 640, val loss: 0.7489815950393677
Epoch 650, training loss: 6.213451862335205 = 0.11167977005243301 + 1.0 * 6.101772308349609
Epoch 650, val loss: 0.7542911767959595
Epoch 660, training loss: 6.19382905960083 = 0.10454189032316208 + 1.0 * 6.089287281036377
Epoch 660, val loss: 0.760050356388092
Epoch 670, training loss: 6.18432092666626 = 0.09808074682950974 + 1.0 * 6.086240291595459
Epoch 670, val loss: 0.7662650942802429
Epoch 680, training loss: 6.176650047302246 = 0.09219014644622803 + 1.0 * 6.0844597816467285
Epoch 680, val loss: 0.7725409865379333
Epoch 690, training loss: 6.171494007110596 = 0.0868038460612297 + 1.0 * 6.084690093994141
Epoch 690, val loss: 0.7790159583091736
Epoch 700, training loss: 6.1694746017456055 = 0.08190643042325974 + 1.0 * 6.087568283081055
Epoch 700, val loss: 0.7855397462844849
Epoch 710, training loss: 6.15853214263916 = 0.07745619863271713 + 1.0 * 6.081076145172119
Epoch 710, val loss: 0.7923175692558289
Epoch 720, training loss: 6.1521759033203125 = 0.07336026430130005 + 1.0 * 6.078815460205078
Epoch 720, val loss: 0.7990932464599609
Epoch 730, training loss: 6.149580955505371 = 0.0695652961730957 + 1.0 * 6.080015659332275
Epoch 730, val loss: 0.805847704410553
Epoch 740, training loss: 6.146042346954346 = 0.06606456637382507 + 1.0 * 6.079977989196777
Epoch 740, val loss: 0.8125429749488831
Epoch 750, training loss: 6.140995979309082 = 0.0628451257944107 + 1.0 * 6.078150749206543
Epoch 750, val loss: 0.8193961381912231
Epoch 760, training loss: 6.1357831954956055 = 0.05985037237405777 + 1.0 * 6.07593297958374
Epoch 760, val loss: 0.8263236880302429
Epoch 770, training loss: 6.133963584899902 = 0.05705203488469124 + 1.0 * 6.076911449432373
Epoch 770, val loss: 0.8331001400947571
Epoch 780, training loss: 6.126515865325928 = 0.054437242448329926 + 1.0 * 6.072078704833984
Epoch 780, val loss: 0.8398846387863159
Epoch 790, training loss: 6.13052225112915 = 0.051989078521728516 + 1.0 * 6.078533172607422
Epoch 790, val loss: 0.8467386960983276
Epoch 800, training loss: 6.126601219177246 = 0.04971464350819588 + 1.0 * 6.0768866539001465
Epoch 800, val loss: 0.8533216714859009
Epoch 810, training loss: 6.116819858551025 = 0.04758531600236893 + 1.0 * 6.069234371185303
Epoch 810, val loss: 0.8600835800170898
Epoch 820, training loss: 6.119416236877441 = 0.04558597877621651 + 1.0 * 6.0738301277160645
Epoch 820, val loss: 0.8667200207710266
Epoch 830, training loss: 6.114284992218018 = 0.04370396211743355 + 1.0 * 6.070580959320068
Epoch 830, val loss: 0.873246967792511
Epoch 840, training loss: 6.109731674194336 = 0.04193749651312828 + 1.0 * 6.067794322967529
Epoch 840, val loss: 0.8797858357429504
Epoch 850, training loss: 6.10907506942749 = 0.040271125733852386 + 1.0 * 6.068803787231445
Epoch 850, val loss: 0.8863233923912048
Epoch 860, training loss: 6.103996753692627 = 0.03869856521487236 + 1.0 * 6.065298080444336
Epoch 860, val loss: 0.8927781581878662
Epoch 870, training loss: 6.112574100494385 = 0.03721887245774269 + 1.0 * 6.075355052947998
Epoch 870, val loss: 0.8990902900695801
Epoch 880, training loss: 6.100188255310059 = 0.03582422435283661 + 1.0 * 6.064363956451416
Epoch 880, val loss: 0.9053832292556763
Epoch 890, training loss: 6.096820831298828 = 0.034510426223278046 + 1.0 * 6.062310218811035
Epoch 890, val loss: 0.9117403626441956
Epoch 900, training loss: 6.095162391662598 = 0.033259522169828415 + 1.0 * 6.06190299987793
Epoch 900, val loss: 0.9179525971412659
Epoch 910, training loss: 6.095871448516846 = 0.03207201138138771 + 1.0 * 6.0637993812561035
Epoch 910, val loss: 0.9239888787269592
Epoch 920, training loss: 6.091254234313965 = 0.030949627980589867 + 1.0 * 6.060304641723633
Epoch 920, val loss: 0.9301053285598755
Epoch 930, training loss: 6.091745376586914 = 0.0298840943723917 + 1.0 * 6.061861515045166
Epoch 930, val loss: 0.9362432956695557
Epoch 940, training loss: 6.088480472564697 = 0.028874985873699188 + 1.0 * 6.059605598449707
Epoch 940, val loss: 0.9421213269233704
Epoch 950, training loss: 6.089149475097656 = 0.027918973937630653 + 1.0 * 6.061230659484863
Epoch 950, val loss: 0.9480745792388916
Epoch 960, training loss: 6.083089828491211 = 0.027006281539797783 + 1.0 * 6.056083679199219
Epoch 960, val loss: 0.9539446830749512
Epoch 970, training loss: 6.0812764167785645 = 0.026135722175240517 + 1.0 * 6.055140495300293
Epoch 970, val loss: 0.9597992897033691
Epoch 980, training loss: 6.088461875915527 = 0.02530226670205593 + 1.0 * 6.063159465789795
Epoch 980, val loss: 0.965433657169342
Epoch 990, training loss: 6.083343982696533 = 0.024517830461263657 + 1.0 * 6.058825969696045
Epoch 990, val loss: 0.970886766910553
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320862770080566 = 1.9469859600067139 + 1.0 * 8.373876571655273
Epoch 0, val loss: 1.943232536315918
Epoch 10, training loss: 10.310552597045898 = 1.9370763301849365 + 1.0 * 8.373476028442383
Epoch 10, val loss: 1.9333170652389526
Epoch 20, training loss: 10.295453071594238 = 1.9246835708618164 + 1.0 * 8.370769500732422
Epoch 20, val loss: 1.9203647375106812
Epoch 30, training loss: 10.259321212768555 = 1.9074060916900635 + 1.0 * 8.35191535949707
Epoch 30, val loss: 1.9020135402679443
Epoch 40, training loss: 10.11357307434082 = 1.8855687379837036 + 1.0 * 8.228004455566406
Epoch 40, val loss: 1.8801085948944092
Epoch 50, training loss: 9.55234146118164 = 1.8638865947723389 + 1.0 * 7.688455104827881
Epoch 50, val loss: 1.859218955039978
Epoch 60, training loss: 9.079504013061523 = 1.8454160690307617 + 1.0 * 7.23408842086792
Epoch 60, val loss: 1.8425588607788086
Epoch 70, training loss: 8.714723587036133 = 1.8322923183441162 + 1.0 * 6.8824310302734375
Epoch 70, val loss: 1.83082115650177
Epoch 80, training loss: 8.496837615966797 = 1.8189464807510376 + 1.0 * 6.677891254425049
Epoch 80, val loss: 1.8184046745300293
Epoch 90, training loss: 8.36479663848877 = 1.8034536838531494 + 1.0 * 6.561342716217041
Epoch 90, val loss: 1.803968071937561
Epoch 100, training loss: 8.28537654876709 = 1.7854787111282349 + 1.0 * 6.4998979568481445
Epoch 100, val loss: 1.7874248027801514
Epoch 110, training loss: 8.221391677856445 = 1.7675199508666992 + 1.0 * 6.453871250152588
Epoch 110, val loss: 1.771368145942688
Epoch 120, training loss: 8.165616989135742 = 1.7508361339569092 + 1.0 * 6.414780616760254
Epoch 120, val loss: 1.756578803062439
Epoch 130, training loss: 8.113181114196777 = 1.733596682548523 + 1.0 * 6.379584789276123
Epoch 130, val loss: 1.7415626049041748
Epoch 140, training loss: 8.065215110778809 = 1.7143594026565552 + 1.0 * 6.350855827331543
Epoch 140, val loss: 1.7250261306762695
Epoch 150, training loss: 8.019786834716797 = 1.6925920248031616 + 1.0 * 6.327195167541504
Epoch 150, val loss: 1.7066118717193604
Epoch 160, training loss: 7.970486164093018 = 1.6674710512161255 + 1.0 * 6.303015232086182
Epoch 160, val loss: 1.6858725547790527
Epoch 170, training loss: 7.919801712036133 = 1.6380507946014404 + 1.0 * 6.2817511558532715
Epoch 170, val loss: 1.661725401878357
Epoch 180, training loss: 7.867812156677246 = 1.6031501293182373 + 1.0 * 6.264662265777588
Epoch 180, val loss: 1.633248209953308
Epoch 190, training loss: 7.81275749206543 = 1.5633976459503174 + 1.0 * 6.249360084533691
Epoch 190, val loss: 1.6007332801818848
Epoch 200, training loss: 7.752779006958008 = 1.5185558795928955 + 1.0 * 6.234223365783691
Epoch 200, val loss: 1.5640013217926025
Epoch 210, training loss: 7.690621852874756 = 1.468450665473938 + 1.0 * 6.222171306610107
Epoch 210, val loss: 1.5231404304504395
Epoch 220, training loss: 7.630301475524902 = 1.414894461631775 + 1.0 * 6.215406894683838
Epoch 220, val loss: 1.4800219535827637
Epoch 230, training loss: 7.563521862030029 = 1.3596724271774292 + 1.0 * 6.2038493156433105
Epoch 230, val loss: 1.4354954957962036
Epoch 240, training loss: 7.49699592590332 = 1.3021092414855957 + 1.0 * 6.194886684417725
Epoch 240, val loss: 1.3896019458770752
Epoch 250, training loss: 7.429687023162842 = 1.24267578125 + 1.0 * 6.187011241912842
Epoch 250, val loss: 1.342553973197937
Epoch 260, training loss: 7.361656188964844 = 1.1817750930786133 + 1.0 * 6.1798810958862305
Epoch 260, val loss: 1.2946527004241943
Epoch 270, training loss: 7.300283432006836 = 1.1207773685455322 + 1.0 * 6.179506301879883
Epoch 270, val loss: 1.2470431327819824
Epoch 280, training loss: 7.231191158294678 = 1.0616792440414429 + 1.0 * 6.169511795043945
Epoch 280, val loss: 1.2007665634155273
Epoch 290, training loss: 7.167103290557861 = 1.0038987398147583 + 1.0 * 6.163204669952393
Epoch 290, val loss: 1.1560382843017578
Epoch 300, training loss: 7.1108222007751465 = 0.9476324915885925 + 1.0 * 6.163189888000488
Epoch 300, val loss: 1.1127679347991943
Epoch 310, training loss: 7.049051284790039 = 0.8944103717803955 + 1.0 * 6.154641151428223
Epoch 310, val loss: 1.0720670223236084
Epoch 320, training loss: 6.99691915512085 = 0.8440520167350769 + 1.0 * 6.152867317199707
Epoch 320, val loss: 1.034383773803711
Epoch 330, training loss: 6.942714691162109 = 0.7965224385261536 + 1.0 * 6.1461920738220215
Epoch 330, val loss: 0.9991786479949951
Epoch 340, training loss: 6.892938137054443 = 0.7513251304626465 + 1.0 * 6.141613006591797
Epoch 340, val loss: 0.9661861658096313
Epoch 350, training loss: 6.848289966583252 = 0.7086414098739624 + 1.0 * 6.1396484375
Epoch 350, val loss: 0.935438334941864
Epoch 360, training loss: 6.803206920623779 = 0.6687249541282654 + 1.0 * 6.134481906890869
Epoch 360, val loss: 0.9072083234786987
Epoch 370, training loss: 6.761641979217529 = 0.6309531927108765 + 1.0 * 6.130688667297363
Epoch 370, val loss: 0.8810991048812866
Epoch 380, training loss: 6.722433567047119 = 0.5951442122459412 + 1.0 * 6.127289295196533
Epoch 380, val loss: 0.856803297996521
Epoch 390, training loss: 6.689671516418457 = 0.5615110993385315 + 1.0 * 6.12816047668457
Epoch 390, val loss: 0.8345823287963867
Epoch 400, training loss: 6.653834342956543 = 0.5302829742431641 + 1.0 * 6.123551368713379
Epoch 400, val loss: 0.8148792386054993
Epoch 410, training loss: 6.620203018188477 = 0.5009214878082275 + 1.0 * 6.11928129196167
Epoch 410, val loss: 0.7972347140312195
Epoch 420, training loss: 6.5983781814575195 = 0.4732514023780823 + 1.0 * 6.125126838684082
Epoch 420, val loss: 0.7814226150512695
Epoch 430, training loss: 6.562331676483154 = 0.4475392997264862 + 1.0 * 6.114792346954346
Epoch 430, val loss: 0.7676115036010742
Epoch 440, training loss: 6.5339674949646 = 0.4232454001903534 + 1.0 * 6.110722064971924
Epoch 440, val loss: 0.7557482719421387
Epoch 450, training loss: 6.509586334228516 = 0.40018630027770996 + 1.0 * 6.109400272369385
Epoch 450, val loss: 0.7453738451004028
Epoch 460, training loss: 6.489826202392578 = 0.37830185890197754 + 1.0 * 6.1115241050720215
Epoch 460, val loss: 0.7362291812896729
Epoch 470, training loss: 6.462052822113037 = 0.3576527237892151 + 1.0 * 6.104400157928467
Epoch 470, val loss: 0.728560745716095
Epoch 480, training loss: 6.439565658569336 = 0.3378247320652008 + 1.0 * 6.101740837097168
Epoch 480, val loss: 0.7220396399497986
Epoch 490, training loss: 6.419469833374023 = 0.31871870160102844 + 1.0 * 6.100750923156738
Epoch 490, val loss: 0.71630859375
Epoch 500, training loss: 6.405569076538086 = 0.3003199100494385 + 1.0 * 6.105248928070068
Epoch 500, val loss: 0.7113776803016663
Epoch 510, training loss: 6.3831305503845215 = 0.2828519642353058 + 1.0 * 6.100278377532959
Epoch 510, val loss: 0.7072780132293701
Epoch 520, training loss: 6.361810207366943 = 0.266048401594162 + 1.0 * 6.095761775970459
Epoch 520, val loss: 0.7040281891822815
Epoch 530, training loss: 6.342623233795166 = 0.2499794363975525 + 1.0 * 6.092643737792969
Epoch 530, val loss: 0.7013542056083679
Epoch 540, training loss: 6.332403182983398 = 0.23465658724308014 + 1.0 * 6.0977463722229
Epoch 540, val loss: 0.6993539333343506
Epoch 550, training loss: 6.311363220214844 = 0.2203160673379898 + 1.0 * 6.091047286987305
Epoch 550, val loss: 0.6978137493133545
Epoch 560, training loss: 6.297184467315674 = 0.20690302550792694 + 1.0 * 6.0902814865112305
Epoch 560, val loss: 0.697210431098938
Epoch 570, training loss: 6.281149864196777 = 0.1943054050207138 + 1.0 * 6.086844444274902
Epoch 570, val loss: 0.6971150636672974
Epoch 580, training loss: 6.2855119705200195 = 0.18249309062957764 + 1.0 * 6.103018760681152
Epoch 580, val loss: 0.6974160671234131
Epoch 590, training loss: 6.257692337036133 = 0.17162054777145386 + 1.0 * 6.086071968078613
Epoch 590, val loss: 0.6982548236846924
Epoch 600, training loss: 6.243430137634277 = 0.1614866405725479 + 1.0 * 6.081943511962891
Epoch 600, val loss: 0.6997261643409729
Epoch 610, training loss: 6.232980728149414 = 0.1520257592201233 + 1.0 * 6.0809550285339355
Epoch 610, val loss: 0.7015287280082703
Epoch 620, training loss: 6.222108364105225 = 0.14317958056926727 + 1.0 * 6.0789289474487305
Epoch 620, val loss: 0.7037461400032043
Epoch 630, training loss: 6.214278221130371 = 0.1349230408668518 + 1.0 * 6.079355239868164
Epoch 630, val loss: 0.7064090967178345
Epoch 640, training loss: 6.21987771987915 = 0.1273060292005539 + 1.0 * 6.09257173538208
Epoch 640, val loss: 0.7090963125228882
Epoch 650, training loss: 6.1965765953063965 = 0.12030303478240967 + 1.0 * 6.076273441314697
Epoch 650, val loss: 0.7123425006866455
Epoch 660, training loss: 6.189377784729004 = 0.11378470063209534 + 1.0 * 6.075592994689941
Epoch 660, val loss: 0.7159280776977539
Epoch 670, training loss: 6.181169033050537 = 0.10768526047468185 + 1.0 * 6.073483943939209
Epoch 670, val loss: 0.719636082649231
Epoch 680, training loss: 6.184732913970947 = 0.1019822508096695 + 1.0 * 6.0827507972717285
Epoch 680, val loss: 0.7234944701194763
Epoch 690, training loss: 6.17230224609375 = 0.09669411182403564 + 1.0 * 6.075608253479004
Epoch 690, val loss: 0.7275030016899109
Epoch 700, training loss: 6.163751125335693 = 0.09179076552391052 + 1.0 * 6.07196044921875
Epoch 700, val loss: 0.7317280769348145
Epoch 710, training loss: 6.156074047088623 = 0.08719377964735031 + 1.0 * 6.068880081176758
Epoch 710, val loss: 0.7360430955886841
Epoch 720, training loss: 6.156327247619629 = 0.08288884162902832 + 1.0 * 6.07343864440918
Epoch 720, val loss: 0.7404101490974426
Epoch 730, training loss: 6.152725696563721 = 0.07887942343950272 + 1.0 * 6.073846340179443
Epoch 730, val loss: 0.7447835206985474
Epoch 740, training loss: 6.141402244567871 = 0.07512480020523071 + 1.0 * 6.066277503967285
Epoch 740, val loss: 0.749245285987854
Epoch 750, training loss: 6.137409687042236 = 0.07160956412553787 + 1.0 * 6.065800189971924
Epoch 750, val loss: 0.7537941932678223
Epoch 760, training loss: 6.134548187255859 = 0.06831280142068863 + 1.0 * 6.066235542297363
Epoch 760, val loss: 0.7582942843437195
Epoch 770, training loss: 6.1293134689331055 = 0.06522446870803833 + 1.0 * 6.064088821411133
Epoch 770, val loss: 0.7628194093704224
Epoch 780, training loss: 6.127683162689209 = 0.06232653558254242 + 1.0 * 6.065356731414795
Epoch 780, val loss: 0.7673386931419373
Epoch 790, training loss: 6.121796607971191 = 0.05960850045084953 + 1.0 * 6.062188148498535
Epoch 790, val loss: 0.7719723582267761
Epoch 800, training loss: 6.131525993347168 = 0.05705980211496353 + 1.0 * 6.074466228485107
Epoch 800, val loss: 0.7764464616775513
Epoch 810, training loss: 6.117392539978027 = 0.05466151982545853 + 1.0 * 6.06273078918457
Epoch 810, val loss: 0.7807570695877075
Epoch 820, training loss: 6.111697673797607 = 0.05242135375738144 + 1.0 * 6.059276103973389
Epoch 820, val loss: 0.7853485941886902
Epoch 830, training loss: 6.110105991363525 = 0.05030471831560135 + 1.0 * 6.05980110168457
Epoch 830, val loss: 0.7897955179214478
Epoch 840, training loss: 6.1047282218933105 = 0.04831131547689438 + 1.0 * 6.056416988372803
Epoch 840, val loss: 0.7941356897354126
Epoch 850, training loss: 6.102565288543701 = 0.046431396156549454 + 1.0 * 6.05613374710083
Epoch 850, val loss: 0.7986047863960266
Epoch 860, training loss: 6.109407424926758 = 0.04464917257428169 + 1.0 * 6.06475830078125
Epoch 860, val loss: 0.8029338121414185
Epoch 870, training loss: 6.102597713470459 = 0.042985107749700546 + 1.0 * 6.05961275100708
Epoch 870, val loss: 0.8070836663246155
Epoch 880, training loss: 6.095677852630615 = 0.041416093707084656 + 1.0 * 6.054261684417725
Epoch 880, val loss: 0.8113287687301636
Epoch 890, training loss: 6.092432498931885 = 0.039927829056978226 + 1.0 * 6.052504539489746
Epoch 890, val loss: 0.8155984282493591
Epoch 900, training loss: 6.090286731719971 = 0.03851259499788284 + 1.0 * 6.051774024963379
Epoch 900, val loss: 0.819841742515564
Epoch 910, training loss: 6.094823837280273 = 0.037171028554439545 + 1.0 * 6.057652950286865
Epoch 910, val loss: 0.8239783644676208
Epoch 920, training loss: 6.091007232666016 = 0.035898685455322266 + 1.0 * 6.055108547210693
Epoch 920, val loss: 0.8280691504478455
Epoch 930, training loss: 6.083993911743164 = 0.034693192690610886 + 1.0 * 6.049300670623779
Epoch 930, val loss: 0.8320924043655396
Epoch 940, training loss: 6.086938858032227 = 0.03355111926794052 + 1.0 * 6.053387641906738
Epoch 940, val loss: 0.8361618518829346
Epoch 950, training loss: 6.082334995269775 = 0.03246681019663811 + 1.0 * 6.049868106842041
Epoch 950, val loss: 0.8401098251342773
Epoch 960, training loss: 6.078972339630127 = 0.03142967447638512 + 1.0 * 6.047542572021484
Epoch 960, val loss: 0.8440723419189453
Epoch 970, training loss: 6.078664302825928 = 0.03044268488883972 + 1.0 * 6.048221588134766
Epoch 970, val loss: 0.8480531573295593
Epoch 980, training loss: 6.078494548797607 = 0.029501093551516533 + 1.0 * 6.0489935874938965
Epoch 980, val loss: 0.851818859577179
Epoch 990, training loss: 6.074772357940674 = 0.02860860899090767 + 1.0 * 6.046163558959961
Epoch 990, val loss: 0.8557153940200806
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8487
Flip ASR: 0.8178/225 nodes
The final ASR:0.84133, 0.10859, Accuracy:0.81975, 0.00761
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11582])
remove edge: torch.Size([2, 9458])
updated graph: torch.Size([2, 10484])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98032, 0.00460, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.314871788024902 = 1.9409407377243042 + 1.0 * 8.373930931091309
Epoch 0, val loss: 1.9373360872268677
Epoch 10, training loss: 10.305201530456543 = 1.931530237197876 + 1.0 * 8.373671531677246
Epoch 10, val loss: 1.9281127452850342
Epoch 20, training loss: 10.291653633117676 = 1.9198577404022217 + 1.0 * 8.371795654296875
Epoch 20, val loss: 1.9165961742401123
Epoch 30, training loss: 10.26169204711914 = 1.9035966396331787 + 1.0 * 8.358095169067383
Epoch 30, val loss: 1.9002610445022583
Epoch 40, training loss: 10.160978317260742 = 1.8815761804580688 + 1.0 * 8.279401779174805
Epoch 40, val loss: 1.8784887790679932
Epoch 50, training loss: 9.814271926879883 = 1.8565223217010498 + 1.0 * 7.957749366760254
Epoch 50, val loss: 1.8541287183761597
Epoch 60, training loss: 9.549556732177734 = 1.8285030126571655 + 1.0 * 7.721053600311279
Epoch 60, val loss: 1.8278579711914062
Epoch 70, training loss: 9.3037748336792 = 1.8024965524673462 + 1.0 * 7.501278400421143
Epoch 70, val loss: 1.804492712020874
Epoch 80, training loss: 8.988765716552734 = 1.7825196981430054 + 1.0 * 7.2062458992004395
Epoch 80, val loss: 1.7879825830459595
Epoch 90, training loss: 8.692159652709961 = 1.7677676677703857 + 1.0 * 6.924392223358154
Epoch 90, val loss: 1.7761294841766357
Epoch 100, training loss: 8.505535125732422 = 1.7493458986282349 + 1.0 * 6.756189346313477
Epoch 100, val loss: 1.7613517045974731
Epoch 110, training loss: 8.385385513305664 = 1.7264289855957031 + 1.0 * 6.658956527709961
Epoch 110, val loss: 1.7431360483169556
Epoch 120, training loss: 8.295934677124023 = 1.7012889385223389 + 1.0 * 6.594645977020264
Epoch 120, val loss: 1.7231613397598267
Epoch 130, training loss: 8.21485710144043 = 1.6731055974960327 + 1.0 * 6.541751384735107
Epoch 130, val loss: 1.7004767656326294
Epoch 140, training loss: 8.132549285888672 = 1.6404507160186768 + 1.0 * 6.492098808288574
Epoch 140, val loss: 1.6742932796478271
Epoch 150, training loss: 8.051491737365723 = 1.6036360263824463 + 1.0 * 6.4478559494018555
Epoch 150, val loss: 1.6449850797653198
Epoch 160, training loss: 7.97197151184082 = 1.561370849609375 + 1.0 * 6.410600662231445
Epoch 160, val loss: 1.6111828088760376
Epoch 170, training loss: 7.892883777618408 = 1.513492465019226 + 1.0 * 6.379391193389893
Epoch 170, val loss: 1.5727936029434204
Epoch 180, training loss: 7.814950466156006 = 1.4612020254135132 + 1.0 * 6.353748321533203
Epoch 180, val loss: 1.5310767889022827
Epoch 190, training loss: 7.737537860870361 = 1.4043563604354858 + 1.0 * 6.333181381225586
Epoch 190, val loss: 1.4858524799346924
Epoch 200, training loss: 7.660516738891602 = 1.3436076641082764 + 1.0 * 6.316908836364746
Epoch 200, val loss: 1.4376194477081299
Epoch 210, training loss: 7.5871734619140625 = 1.2812016010284424 + 1.0 * 6.305971622467041
Epoch 210, val loss: 1.3887797594070435
Epoch 220, training loss: 7.513167381286621 = 1.219115972518921 + 1.0 * 6.294051170349121
Epoch 220, val loss: 1.340609073638916
Epoch 230, training loss: 7.441476821899414 = 1.1571959257125854 + 1.0 * 6.284280776977539
Epoch 230, val loss: 1.2933294773101807
Epoch 240, training loss: 7.372093677520752 = 1.096441626548767 + 1.0 * 6.275651931762695
Epoch 240, val loss: 1.2476595640182495
Epoch 250, training loss: 7.306741714477539 = 1.0382397174835205 + 1.0 * 6.2685017585754395
Epoch 250, val loss: 1.2047854661941528
Epoch 260, training loss: 7.243772983551025 = 0.9828845858573914 + 1.0 * 6.260888576507568
Epoch 260, val loss: 1.1648424863815308
Epoch 270, training loss: 7.183512210845947 = 0.9297805428504944 + 1.0 * 6.253731727600098
Epoch 270, val loss: 1.1274279356002808
Epoch 280, training loss: 7.1273274421691895 = 0.8790551424026489 + 1.0 * 6.24827241897583
Epoch 280, val loss: 1.0923000574111938
Epoch 290, training loss: 7.074645519256592 = 0.8314405679702759 + 1.0 * 6.2432050704956055
Epoch 290, val loss: 1.0598657131195068
Epoch 300, training loss: 7.021029472351074 = 0.7867043018341064 + 1.0 * 6.234324932098389
Epoch 300, val loss: 1.0300015211105347
Epoch 310, training loss: 6.973156929016113 = 0.7444729804992676 + 1.0 * 6.228683948516846
Epoch 310, val loss: 1.0023318529129028
Epoch 320, training loss: 6.9298858642578125 = 0.7048770785331726 + 1.0 * 6.225008964538574
Epoch 320, val loss: 0.9769870042800903
Epoch 330, training loss: 6.887128829956055 = 0.6680386662483215 + 1.0 * 6.219089984893799
Epoch 330, val loss: 0.9539598226547241
Epoch 340, training loss: 6.8441572189331055 = 0.6334809064865112 + 1.0 * 6.210676193237305
Epoch 340, val loss: 0.9328809976577759
Epoch 350, training loss: 6.806831359863281 = 0.600597620010376 + 1.0 * 6.206233501434326
Epoch 350, val loss: 0.9135205745697021
Epoch 360, training loss: 6.771111488342285 = 0.5692012906074524 + 1.0 * 6.201910018920898
Epoch 360, val loss: 0.8956558108329773
Epoch 370, training loss: 6.734598159790039 = 0.5391433238983154 + 1.0 * 6.1954545974731445
Epoch 370, val loss: 0.8794006705284119
Epoch 380, training loss: 6.704622268676758 = 0.5100740790367126 + 1.0 * 6.1945481300354
Epoch 380, val loss: 0.8645076155662537
Epoch 390, training loss: 6.671389102935791 = 0.48233479261398315 + 1.0 * 6.189054489135742
Epoch 390, val loss: 0.8510758280754089
Epoch 400, training loss: 6.638066291809082 = 0.4556993246078491 + 1.0 * 6.182366847991943
Epoch 400, val loss: 0.8392139077186584
Epoch 410, training loss: 6.608707904815674 = 0.42996808886528015 + 1.0 * 6.17874002456665
Epoch 410, val loss: 0.8287075161933899
Epoch 420, training loss: 6.579864978790283 = 0.4050770103931427 + 1.0 * 6.174787998199463
Epoch 420, val loss: 0.8194993734359741
Epoch 430, training loss: 6.5522780418396 = 0.3811630606651306 + 1.0 * 6.171114921569824
Epoch 430, val loss: 0.8117444515228271
Epoch 440, training loss: 6.526628494262695 = 0.35834765434265137 + 1.0 * 6.168281078338623
Epoch 440, val loss: 0.8052962422370911
Epoch 450, training loss: 6.50221586227417 = 0.33649396896362305 + 1.0 * 6.165721893310547
Epoch 450, val loss: 0.7999794483184814
Epoch 460, training loss: 6.4786295890808105 = 0.31571120023727417 + 1.0 * 6.162918567657471
Epoch 460, val loss: 0.7957510352134705
Epoch 470, training loss: 6.455965042114258 = 0.29610371589660645 + 1.0 * 6.1598615646362305
Epoch 470, val loss: 0.7927520275115967
Epoch 480, training loss: 6.4349045753479 = 0.27751657366752625 + 1.0 * 6.157388210296631
Epoch 480, val loss: 0.7908540964126587
Epoch 490, training loss: 6.418087959289551 = 0.2599788010120392 + 1.0 * 6.158109188079834
Epoch 490, val loss: 0.7898630499839783
Epoch 500, training loss: 6.394214153289795 = 0.24356196820735931 + 1.0 * 6.1506524085998535
Epoch 500, val loss: 0.7899585962295532
Epoch 510, training loss: 6.380339622497559 = 0.22816091775894165 + 1.0 * 6.152178764343262
Epoch 510, val loss: 0.7909091711044312
Epoch 520, training loss: 6.364247798919678 = 0.213852658867836 + 1.0 * 6.150394916534424
Epoch 520, val loss: 0.7927289009094238
Epoch 530, training loss: 6.345369815826416 = 0.20053400099277496 + 1.0 * 6.144835948944092
Epoch 530, val loss: 0.7954722046852112
Epoch 540, training loss: 6.330574989318848 = 0.18812833726406097 + 1.0 * 6.142446517944336
Epoch 540, val loss: 0.7989474534988403
Epoch 550, training loss: 6.317921161651611 = 0.17662149667739868 + 1.0 * 6.141299724578857
Epoch 550, val loss: 0.8030847907066345
Epoch 560, training loss: 6.3066020011901855 = 0.16598616540431976 + 1.0 * 6.140615940093994
Epoch 560, val loss: 0.8080134391784668
Epoch 570, training loss: 6.2920708656311035 = 0.15616333484649658 + 1.0 * 6.1359076499938965
Epoch 570, val loss: 0.8135746717453003
Epoch 580, training loss: 6.281927108764648 = 0.14705805480480194 + 1.0 * 6.13486909866333
Epoch 580, val loss: 0.8197092413902283
Epoch 590, training loss: 6.27091646194458 = 0.13858292996883392 + 1.0 * 6.132333755493164
Epoch 590, val loss: 0.8264580368995667
Epoch 600, training loss: 6.267096519470215 = 0.13068225979804993 + 1.0 * 6.136414051055908
Epoch 600, val loss: 0.833618700504303
Epoch 610, training loss: 6.257508277893066 = 0.12338143587112427 + 1.0 * 6.134126663208008
Epoch 610, val loss: 0.8413395881652832
Epoch 620, training loss: 6.243537425994873 = 0.11658027023077011 + 1.0 * 6.126956939697266
Epoch 620, val loss: 0.8493532538414001
Epoch 630, training loss: 6.243277072906494 = 0.1102452203631401 + 1.0 * 6.133031845092773
Epoch 630, val loss: 0.8575692176818848
Epoch 640, training loss: 6.2287211418151855 = 0.10437184572219849 + 1.0 * 6.124349117279053
Epoch 640, val loss: 0.866096019744873
Epoch 650, training loss: 6.2220940589904785 = 0.09889237582683563 + 1.0 * 6.123201847076416
Epoch 650, val loss: 0.8749281167984009
Epoch 660, training loss: 6.217984199523926 = 0.093773752450943 + 1.0 * 6.124210357666016
Epoch 660, val loss: 0.8837803602218628
Epoch 670, training loss: 6.207709789276123 = 0.08898936957120895 + 1.0 * 6.118720531463623
Epoch 670, val loss: 0.8928431272506714
Epoch 680, training loss: 6.2050347328186035 = 0.08450964838266373 + 1.0 * 6.120524883270264
Epoch 680, val loss: 0.9019927382469177
Epoch 690, training loss: 6.196462631225586 = 0.08031623810529709 + 1.0 * 6.116146564483643
Epoch 690, val loss: 0.9112035632133484
Epoch 700, training loss: 6.197965621948242 = 0.07640295475721359 + 1.0 * 6.121562480926514
Epoch 700, val loss: 0.9203062653541565
Epoch 710, training loss: 6.185825824737549 = 0.07276304811239243 + 1.0 * 6.113062858581543
Epoch 710, val loss: 0.9294902086257935
Epoch 720, training loss: 6.18061637878418 = 0.06935353577136993 + 1.0 * 6.111262798309326
Epoch 720, val loss: 0.9386516809463501
Epoch 730, training loss: 6.17588472366333 = 0.06614471971988678 + 1.0 * 6.109739780426025
Epoch 730, val loss: 0.9477327466011047
Epoch 740, training loss: 6.171634197235107 = 0.06311943382024765 + 1.0 * 6.108514785766602
Epoch 740, val loss: 0.9569001197814941
Epoch 750, training loss: 6.172567844390869 = 0.06026933714747429 + 1.0 * 6.112298488616943
Epoch 750, val loss: 0.9661034345626831
Epoch 760, training loss: 6.172489166259766 = 0.057590167969465256 + 1.0 * 6.114899158477783
Epoch 760, val loss: 0.9749014377593994
Epoch 770, training loss: 6.1621551513671875 = 0.05508405715227127 + 1.0 * 6.1070709228515625
Epoch 770, val loss: 0.9837732315063477
Epoch 780, training loss: 6.157314777374268 = 0.05272693559527397 + 1.0 * 6.104588031768799
Epoch 780, val loss: 0.9925880432128906
Epoch 790, training loss: 6.153056621551514 = 0.05049861967563629 + 1.0 * 6.102558135986328
Epoch 790, val loss: 1.0012073516845703
Epoch 800, training loss: 6.153582572937012 = 0.048390112817287445 + 1.0 * 6.1051926612854
Epoch 800, val loss: 1.0097224712371826
Epoch 810, training loss: 6.148825168609619 = 0.04640216380357742 + 1.0 * 6.102423191070557
Epoch 810, val loss: 1.0182275772094727
Epoch 820, training loss: 6.144599914550781 = 0.04452638328075409 + 1.0 * 6.100073337554932
Epoch 820, val loss: 1.0266389846801758
Epoch 830, training loss: 6.143095970153809 = 0.04274986684322357 + 1.0 * 6.100346088409424
Epoch 830, val loss: 1.0349161624908447
Epoch 840, training loss: 6.140440940856934 = 0.04106820002198219 + 1.0 * 6.099372863769531
Epoch 840, val loss: 1.0429208278656006
Epoch 850, training loss: 6.136767387390137 = 0.03947452828288078 + 1.0 * 6.097292900085449
Epoch 850, val loss: 1.0509876012802124
Epoch 860, training loss: 6.140221118927002 = 0.03796476870775223 + 1.0 * 6.1022562980651855
Epoch 860, val loss: 1.0588607788085938
Epoch 870, training loss: 6.131048679351807 = 0.03653689846396446 + 1.0 * 6.094511985778809
Epoch 870, val loss: 1.066683292388916
Epoch 880, training loss: 6.129242897033691 = 0.03518011420965195 + 1.0 * 6.094062805175781
Epoch 880, val loss: 1.0744231939315796
Epoch 890, training loss: 6.134274482727051 = 0.03388822078704834 + 1.0 * 6.100386142730713
Epoch 890, val loss: 1.0818920135498047
Epoch 900, training loss: 6.126571178436279 = 0.032665006816387177 + 1.0 * 6.093906402587891
Epoch 900, val loss: 1.0893691778182983
Epoch 910, training loss: 6.127060890197754 = 0.031501319259405136 + 1.0 * 6.095559597015381
Epoch 910, val loss: 1.0967934131622314
Epoch 920, training loss: 6.120941162109375 = 0.0303951408714056 + 1.0 * 6.090546131134033
Epoch 920, val loss: 1.1038728952407837
Epoch 930, training loss: 6.118428707122803 = 0.029342221096158028 + 1.0 * 6.089086532592773
Epoch 930, val loss: 1.111063003540039
Epoch 940, training loss: 6.119874477386475 = 0.028337694704532623 + 1.0 * 6.091536998748779
Epoch 940, val loss: 1.1181063652038574
Epoch 950, training loss: 6.114223480224609 = 0.02738146297633648 + 1.0 * 6.086842060089111
Epoch 950, val loss: 1.1250832080841064
Epoch 960, training loss: 6.113121509552002 = 0.026468893513083458 + 1.0 * 6.086652755737305
Epoch 960, val loss: 1.1320854425430298
Epoch 970, training loss: 6.117034435272217 = 0.02559676021337509 + 1.0 * 6.091437816619873
Epoch 970, val loss: 1.1387070417404175
Epoch 980, training loss: 6.110927104949951 = 0.024765141308307648 + 1.0 * 6.086162090301514
Epoch 980, val loss: 1.145392656326294
Epoch 990, training loss: 6.1086955070495605 = 0.023973917588591576 + 1.0 * 6.084721565246582
Epoch 990, val loss: 1.1520252227783203
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5535
Flip ASR: 0.4711/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.321307182312012 = 1.947374701499939 + 1.0 * 8.373932838439941
Epoch 0, val loss: 1.9515832662582397
Epoch 10, training loss: 10.310235977172852 = 1.9365813732147217 + 1.0 * 8.37365436553955
Epoch 10, val loss: 1.9400601387023926
Epoch 20, training loss: 10.294608116149902 = 1.9230318069458008 + 1.0 * 8.371576309204102
Epoch 20, val loss: 1.9254026412963867
Epoch 30, training loss: 10.259761810302734 = 1.9042481184005737 + 1.0 * 8.355513572692871
Epoch 30, val loss: 1.9050605297088623
Epoch 40, training loss: 10.147270202636719 = 1.8795148134231567 + 1.0 * 8.267755508422852
Epoch 40, val loss: 1.879154920578003
Epoch 50, training loss: 9.745899200439453 = 1.8533549308776855 + 1.0 * 7.892544746398926
Epoch 50, val loss: 1.8532072305679321
Epoch 60, training loss: 9.352073669433594 = 1.8303231000900269 + 1.0 * 7.5217509269714355
Epoch 60, val loss: 1.8310191631317139
Epoch 70, training loss: 9.011470794677734 = 1.810601830482483 + 1.0 * 7.200869083404541
Epoch 70, val loss: 1.811826229095459
Epoch 80, training loss: 8.762434005737305 = 1.7905489206314087 + 1.0 * 6.971884727478027
Epoch 80, val loss: 1.792770266532898
Epoch 90, training loss: 8.546212196350098 = 1.7707974910736084 + 1.0 * 6.775414943695068
Epoch 90, val loss: 1.7745941877365112
Epoch 100, training loss: 8.414128303527832 = 1.7506639957427979 + 1.0 * 6.663464069366455
Epoch 100, val loss: 1.7559536695480347
Epoch 110, training loss: 8.302445411682129 = 1.728713035583496 + 1.0 * 6.573732376098633
Epoch 110, val loss: 1.736333966255188
Epoch 120, training loss: 8.207334518432617 = 1.705602765083313 + 1.0 * 6.5017313957214355
Epoch 120, val loss: 1.7159976959228516
Epoch 130, training loss: 8.12885570526123 = 1.6797081232070923 + 1.0 * 6.4491472244262695
Epoch 130, val loss: 1.6936882734298706
Epoch 140, training loss: 8.05637264251709 = 1.6498879194259644 + 1.0 * 6.406485080718994
Epoch 140, val loss: 1.6684417724609375
Epoch 150, training loss: 7.993327617645264 = 1.6150761842727661 + 1.0 * 6.378251552581787
Epoch 150, val loss: 1.639768362045288
Epoch 160, training loss: 7.925441741943359 = 1.5756416320800781 + 1.0 * 6.349800109863281
Epoch 160, val loss: 1.607677698135376
Epoch 170, training loss: 7.859608173370361 = 1.5308923721313477 + 1.0 * 6.328715801239014
Epoch 170, val loss: 1.5716674327850342
Epoch 180, training loss: 7.793588638305664 = 1.4809926748275757 + 1.0 * 6.312595844268799
Epoch 180, val loss: 1.5315614938735962
Epoch 190, training loss: 7.727466583251953 = 1.4269715547561646 + 1.0 * 6.300495147705078
Epoch 190, val loss: 1.488457202911377
Epoch 200, training loss: 7.65921688079834 = 1.3706481456756592 + 1.0 * 6.288568496704102
Epoch 200, val loss: 1.4436538219451904
Epoch 210, training loss: 7.591963768005371 = 1.3128976821899414 + 1.0 * 6.27906608581543
Epoch 210, val loss: 1.3979097604751587
Epoch 220, training loss: 7.523882865905762 = 1.2555707693099976 + 1.0 * 6.268311977386475
Epoch 220, val loss: 1.3526288270950317
Epoch 230, training loss: 7.457892417907715 = 1.1994593143463135 + 1.0 * 6.2584333419799805
Epoch 230, val loss: 1.3085042238235474
Epoch 240, training loss: 7.396336555480957 = 1.145013451576233 + 1.0 * 6.251323223114014
Epoch 240, val loss: 1.266220211982727
Epoch 250, training loss: 7.336235523223877 = 1.0936757326126099 + 1.0 * 6.242559909820557
Epoch 250, val loss: 1.2266515493392944
Epoch 260, training loss: 7.278802871704102 = 1.0451902151107788 + 1.0 * 6.233612537384033
Epoch 260, val loss: 1.189858078956604
Epoch 270, training loss: 7.2308573722839355 = 0.9993063807487488 + 1.0 * 6.231551170349121
Epoch 270, val loss: 1.1553609371185303
Epoch 280, training loss: 7.177268028259277 = 0.9562499523162842 + 1.0 * 6.221018314361572
Epoch 280, val loss: 1.1235682964324951
Epoch 290, training loss: 7.128119945526123 = 0.9152857661247253 + 1.0 * 6.212834358215332
Epoch 290, val loss: 1.0937952995300293
Epoch 300, training loss: 7.095020771026611 = 0.8757314085960388 + 1.0 * 6.219289302825928
Epoch 300, val loss: 1.0654832124710083
Epoch 310, training loss: 7.041547775268555 = 0.8381421566009521 + 1.0 * 6.203405857086182
Epoch 310, val loss: 1.0385684967041016
Epoch 320, training loss: 6.997901916503906 = 0.801782488822937 + 1.0 * 6.19611930847168
Epoch 320, val loss: 1.0129998922348022
Epoch 330, training loss: 6.957208156585693 = 0.7664485573768616 + 1.0 * 6.190759658813477
Epoch 330, val loss: 0.9881946444511414
Epoch 340, training loss: 6.925477504730225 = 0.7322412133216858 + 1.0 * 6.193236351013184
Epoch 340, val loss: 0.9641827344894409
Epoch 350, training loss: 6.883813858032227 = 0.6996935606002808 + 1.0 * 6.184120178222656
Epoch 350, val loss: 0.9415169954299927
Epoch 360, training loss: 6.847736358642578 = 0.6685378551483154 + 1.0 * 6.179198265075684
Epoch 360, val loss: 0.9202597141265869
Epoch 370, training loss: 6.8134331703186035 = 0.6385497450828552 + 1.0 * 6.1748833656311035
Epoch 370, val loss: 0.8999778628349304
Epoch 380, training loss: 6.780794143676758 = 0.6095268726348877 + 1.0 * 6.171267032623291
Epoch 380, val loss: 0.8806746602058411
Epoch 390, training loss: 6.751173496246338 = 0.5816295742988586 + 1.0 * 6.169543743133545
Epoch 390, val loss: 0.8625643849372864
Epoch 400, training loss: 6.721736907958984 = 0.5550663471221924 + 1.0 * 6.166670799255371
Epoch 400, val loss: 0.8459042906761169
Epoch 410, training loss: 6.695871829986572 = 0.5296181440353394 + 1.0 * 6.166253566741943
Epoch 410, val loss: 0.8305267095565796
Epoch 420, training loss: 6.665226936340332 = 0.5052873492240906 + 1.0 * 6.159939765930176
Epoch 420, val loss: 0.8162997961044312
Epoch 430, training loss: 6.637990474700928 = 0.4819740056991577 + 1.0 * 6.1560163497924805
Epoch 430, val loss: 0.803416907787323
Epoch 440, training loss: 6.6207804679870605 = 0.4596896171569824 + 1.0 * 6.161090850830078
Epoch 440, val loss: 0.7918314933776855
Epoch 450, training loss: 6.589659214019775 = 0.4385206997394562 + 1.0 * 6.1511383056640625
Epoch 450, val loss: 0.7816797494888306
Epoch 460, training loss: 6.566285133361816 = 0.4182717800140381 + 1.0 * 6.148013591766357
Epoch 460, val loss: 0.7730474472045898
Epoch 470, training loss: 6.544189929962158 = 0.39880266785621643 + 1.0 * 6.145387172698975
Epoch 470, val loss: 0.7656146287918091
Epoch 480, training loss: 6.52921724319458 = 0.3800242245197296 + 1.0 * 6.149192810058594
Epoch 480, val loss: 0.7593371272087097
Epoch 490, training loss: 6.509404182434082 = 0.36208322644233704 + 1.0 * 6.147320747375488
Epoch 490, val loss: 0.754237949848175
Epoch 500, training loss: 6.483684062957764 = 0.3448285460472107 + 1.0 * 6.138855457305908
Epoch 500, val loss: 0.7502761483192444
Epoch 510, training loss: 6.466580867767334 = 0.32811781764030457 + 1.0 * 6.138463020324707
Epoch 510, val loss: 0.7471602559089661
Epoch 520, training loss: 6.451226234436035 = 0.3119540512561798 + 1.0 * 6.139272212982178
Epoch 520, val loss: 0.7447799444198608
Epoch 530, training loss: 6.43213415145874 = 0.2963864505290985 + 1.0 * 6.135747909545898
Epoch 530, val loss: 0.743141770362854
Epoch 540, training loss: 6.41193962097168 = 0.2813389301300049 + 1.0 * 6.130600452423096
Epoch 540, val loss: 0.7421851754188538
Epoch 550, training loss: 6.403353691101074 = 0.2667919099330902 + 1.0 * 6.136561870574951
Epoch 550, val loss: 0.7417815327644348
Epoch 560, training loss: 6.379900932312012 = 0.2528441250324249 + 1.0 * 6.12705659866333
Epoch 560, val loss: 0.7418676614761353
Epoch 570, training loss: 6.36660623550415 = 0.2394159585237503 + 1.0 * 6.127190113067627
Epoch 570, val loss: 0.7425368428230286
Epoch 580, training loss: 6.3538289070129395 = 0.2265290915966034 + 1.0 * 6.127299785614014
Epoch 580, val loss: 0.7436923980712891
Epoch 590, training loss: 6.337320804595947 = 0.21427854895591736 + 1.0 * 6.123042106628418
Epoch 590, val loss: 0.7452914118766785
Epoch 600, training loss: 6.324199199676514 = 0.20261873304843903 + 1.0 * 6.121580600738525
Epoch 600, val loss: 0.7474288940429688
Epoch 610, training loss: 6.313035488128662 = 0.19154056906700134 + 1.0 * 6.121494770050049
Epoch 610, val loss: 0.750000536441803
Epoch 620, training loss: 6.299170970916748 = 0.18105994164943695 + 1.0 * 6.1181111335754395
Epoch 620, val loss: 0.7530756592750549
Epoch 630, training loss: 6.288766860961914 = 0.17120835185050964 + 1.0 * 6.117558479309082
Epoch 630, val loss: 0.7566784024238586
Epoch 640, training loss: 6.280388355255127 = 0.1619729846715927 + 1.0 * 6.118415355682373
Epoch 640, val loss: 0.7606959342956543
Epoch 650, training loss: 6.267577171325684 = 0.15330177545547485 + 1.0 * 6.1142754554748535
Epoch 650, val loss: 0.7652909159660339
Epoch 660, training loss: 6.263636112213135 = 0.14518435299396515 + 1.0 * 6.1184515953063965
Epoch 660, val loss: 0.7702592015266418
Epoch 670, training loss: 6.251734256744385 = 0.13764424622058868 + 1.0 * 6.1140899658203125
Epoch 670, val loss: 0.7756043076515198
Epoch 680, training loss: 6.24099063873291 = 0.13058345019817352 + 1.0 * 6.11040735244751
Epoch 680, val loss: 0.7814505100250244
Epoch 690, training loss: 6.231991291046143 = 0.12398897856473923 + 1.0 * 6.108002185821533
Epoch 690, val loss: 0.7876952886581421
Epoch 700, training loss: 6.230637073516846 = 0.11782626062631607 + 1.0 * 6.1128106117248535
Epoch 700, val loss: 0.7942366003990173
Epoch 710, training loss: 6.218771934509277 = 0.11207406967878342 + 1.0 * 6.106698036193848
Epoch 710, val loss: 0.8010200262069702
Epoch 720, training loss: 6.212235927581787 = 0.10668502748012543 + 1.0 * 6.105550765991211
Epoch 720, val loss: 0.8081421852111816
Epoch 730, training loss: 6.204593181610107 = 0.10164258629083633 + 1.0 * 6.102950572967529
Epoch 730, val loss: 0.815466582775116
Epoch 740, training loss: 6.201760768890381 = 0.09691256284713745 + 1.0 * 6.104848384857178
Epoch 740, val loss: 0.8229988217353821
Epoch 750, training loss: 6.1924052238464355 = 0.09249502420425415 + 1.0 * 6.099910259246826
Epoch 750, val loss: 0.8306887745857239
Epoch 760, training loss: 6.186004161834717 = 0.08833565562963486 + 1.0 * 6.097668647766113
Epoch 760, val loss: 0.8386022448539734
Epoch 770, training loss: 6.181313514709473 = 0.08440683782100677 + 1.0 * 6.096906661987305
Epoch 770, val loss: 0.8467117547988892
Epoch 780, training loss: 6.181538105010986 = 0.08070562779903412 + 1.0 * 6.100832462310791
Epoch 780, val loss: 0.8549630641937256
Epoch 790, training loss: 6.179721832275391 = 0.0772528350353241 + 1.0 * 6.102468967437744
Epoch 790, val loss: 0.8632156848907471
Epoch 800, training loss: 6.171158790588379 = 0.07399632036685944 + 1.0 * 6.097162246704102
Epoch 800, val loss: 0.8715412020683289
Epoch 810, training loss: 6.162752628326416 = 0.07092233747243881 + 1.0 * 6.091830253601074
Epoch 810, val loss: 0.8799933791160583
Epoch 820, training loss: 6.162581920623779 = 0.06800590455532074 + 1.0 * 6.094575881958008
Epoch 820, val loss: 0.8885250091552734
Epoch 830, training loss: 6.1548638343811035 = 0.06526322662830353 + 1.0 * 6.089600563049316
Epoch 830, val loss: 0.8970663547515869
Epoch 840, training loss: 6.151088237762451 = 0.0626649558544159 + 1.0 * 6.088423252105713
Epoch 840, val loss: 0.9056224226951599
Epoch 850, training loss: 6.148013114929199 = 0.0601922906935215 + 1.0 * 6.087821006774902
Epoch 850, val loss: 0.9142711758613586
Epoch 860, training loss: 6.1544575691223145 = 0.05785243958234787 + 1.0 * 6.09660530090332
Epoch 860, val loss: 0.9229356646537781
Epoch 870, training loss: 6.142360687255859 = 0.05564335361123085 + 1.0 * 6.086717128753662
Epoch 870, val loss: 0.931487500667572
Epoch 880, training loss: 6.140878200531006 = 0.05354468151926994 + 1.0 * 6.087333679199219
Epoch 880, val loss: 0.9400584697723389
Epoch 890, training loss: 6.145016670227051 = 0.05155741423368454 + 1.0 * 6.093459129333496
Epoch 890, val loss: 0.9486097693443298
Epoch 900, training loss: 6.134756088256836 = 0.049660567194223404 + 1.0 * 6.085095405578613
Epoch 900, val loss: 0.9570842385292053
Epoch 910, training loss: 6.12997579574585 = 0.04786329343914986 + 1.0 * 6.0821123123168945
Epoch 910, val loss: 0.9655556082725525
Epoch 920, training loss: 6.128737449645996 = 0.04614592343568802 + 1.0 * 6.082591533660889
Epoch 920, val loss: 0.9740399122238159
Epoch 930, training loss: 6.129829406738281 = 0.04451445862650871 + 1.0 * 6.085314750671387
Epoch 930, val loss: 0.9824068546295166
Epoch 940, training loss: 6.123115062713623 = 0.042968522757291794 + 1.0 * 6.080146312713623
Epoch 940, val loss: 0.9906860589981079
Epoch 950, training loss: 6.121616363525391 = 0.041495855897665024 + 1.0 * 6.08012056350708
Epoch 950, val loss: 0.9989107251167297
Epoch 960, training loss: 6.117917537689209 = 0.04008693993091583 + 1.0 * 6.077830791473389
Epoch 960, val loss: 1.0071289539337158
Epoch 970, training loss: 6.1367692947387695 = 0.03873920440673828 + 1.0 * 6.098030090332031
Epoch 970, val loss: 1.015270709991455
Epoch 980, training loss: 6.116453170776367 = 0.03747523948550224 + 1.0 * 6.078978061676025
Epoch 980, val loss: 1.023188829421997
Epoch 990, training loss: 6.112843990325928 = 0.03626219183206558 + 1.0 * 6.076581954956055
Epoch 990, val loss: 1.0311000347137451
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7085
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.316739082336426 = 1.9428693056106567 + 1.0 * 8.373869895935059
Epoch 0, val loss: 1.937153697013855
Epoch 10, training loss: 10.305561065673828 = 1.9321823120117188 + 1.0 * 8.37337875366211
Epoch 10, val loss: 1.9260932207107544
Epoch 20, training loss: 10.287914276123047 = 1.9189667701721191 + 1.0 * 8.368947982788086
Epoch 20, val loss: 1.9121946096420288
Epoch 30, training loss: 10.236043930053711 = 1.9005098342895508 + 1.0 * 8.33553409576416
Epoch 30, val loss: 1.8930425643920898
Epoch 40, training loss: 9.996826171875 = 1.878440499305725 + 1.0 * 8.118385314941406
Epoch 40, val loss: 1.8714845180511475
Epoch 50, training loss: 9.67863941192627 = 1.855147361755371 + 1.0 * 7.823492050170898
Epoch 50, val loss: 1.8495757579803467
Epoch 60, training loss: 9.322711944580078 = 1.836669921875 + 1.0 * 7.486042499542236
Epoch 60, val loss: 1.8326528072357178
Epoch 70, training loss: 8.846315383911133 = 1.8236637115478516 + 1.0 * 7.022651195526123
Epoch 70, val loss: 1.8205989599227905
Epoch 80, training loss: 8.57979965209961 = 1.8123514652252197 + 1.0 * 6.7674479484558105
Epoch 80, val loss: 1.8103866577148438
Epoch 90, training loss: 8.447214126586914 = 1.7959731817245483 + 1.0 * 6.651240825653076
Epoch 90, val loss: 1.7956453561782837
Epoch 100, training loss: 8.328157424926758 = 1.7777987718582153 + 1.0 * 6.550358772277832
Epoch 100, val loss: 1.7800159454345703
Epoch 110, training loss: 8.236955642700195 = 1.7602691650390625 + 1.0 * 6.476686477661133
Epoch 110, val loss: 1.7648123502731323
Epoch 120, training loss: 8.16073226928711 = 1.74148690700531 + 1.0 * 6.41924524307251
Epoch 120, val loss: 1.748322606086731
Epoch 130, training loss: 8.091068267822266 = 1.720069408416748 + 1.0 * 6.370998859405518
Epoch 130, val loss: 1.72971510887146
Epoch 140, training loss: 8.034830093383789 = 1.6953328847885132 + 1.0 * 6.3394975662231445
Epoch 140, val loss: 1.7084906101226807
Epoch 150, training loss: 7.977948188781738 = 1.6669301986694336 + 1.0 * 6.311017990112305
Epoch 150, val loss: 1.6842190027236938
Epoch 160, training loss: 7.927348613739014 = 1.6341923475265503 + 1.0 * 6.293156147003174
Epoch 160, val loss: 1.6563315391540527
Epoch 170, training loss: 7.870194911956787 = 1.5971540212631226 + 1.0 * 6.273040771484375
Epoch 170, val loss: 1.625117540359497
Epoch 180, training loss: 7.814226150512695 = 1.5560189485549927 + 1.0 * 6.258207321166992
Epoch 180, val loss: 1.5906462669372559
Epoch 190, training loss: 7.758609771728516 = 1.5114326477050781 + 1.0 * 6.2471771240234375
Epoch 190, val loss: 1.553798794746399
Epoch 200, training loss: 7.701139450073242 = 1.4650492668151855 + 1.0 * 6.236090183258057
Epoch 200, val loss: 1.516279935836792
Epoch 210, training loss: 7.643734931945801 = 1.4177515506744385 + 1.0 * 6.225983142852783
Epoch 210, val loss: 1.478962779045105
Epoch 220, training loss: 7.591686725616455 = 1.3701472282409668 + 1.0 * 6.221539497375488
Epoch 220, val loss: 1.4424841403961182
Epoch 230, training loss: 7.536386489868164 = 1.3244704008102417 + 1.0 * 6.211915969848633
Epoch 230, val loss: 1.4083532094955444
Epoch 240, training loss: 7.482034683227539 = 1.2802796363830566 + 1.0 * 6.201755046844482
Epoch 240, val loss: 1.3761409521102905
Epoch 250, training loss: 7.432209014892578 = 1.2370765209197998 + 1.0 * 6.195132255554199
Epoch 250, val loss: 1.3451707363128662
Epoch 260, training loss: 7.382907390594482 = 1.1944470405578613 + 1.0 * 6.188460350036621
Epoch 260, val loss: 1.314961314201355
Epoch 270, training loss: 7.3520097732543945 = 1.152324914932251 + 1.0 * 6.1996846199035645
Epoch 270, val loss: 1.2852270603179932
Epoch 280, training loss: 7.2889227867126465 = 1.1110364198684692 + 1.0 * 6.177886486053467
Epoch 280, val loss: 1.256224274635315
Epoch 290, training loss: 7.243913650512695 = 1.070288896560669 + 1.0 * 6.1736249923706055
Epoch 290, val loss: 1.2273528575897217
Epoch 300, training loss: 7.197885990142822 = 1.0299493074417114 + 1.0 * 6.1679368019104
Epoch 300, val loss: 1.1986162662506104
Epoch 310, training loss: 7.164567470550537 = 0.9900177121162415 + 1.0 * 6.174549579620361
Epoch 310, val loss: 1.1702800989151
Epoch 320, training loss: 7.11400032043457 = 0.9515131711959839 + 1.0 * 6.162487030029297
Epoch 320, val loss: 1.1428723335266113
Epoch 330, training loss: 7.0705485343933105 = 0.9142407774925232 + 1.0 * 6.156307697296143
Epoch 330, val loss: 1.116486668586731
Epoch 340, training loss: 7.0301513671875 = 0.8782266974449158 + 1.0 * 6.1519246101379395
Epoch 340, val loss: 1.0912673473358154
Epoch 350, training loss: 6.991691589355469 = 0.8437392115592957 + 1.0 * 6.147952556610107
Epoch 350, val loss: 1.067516565322876
Epoch 360, training loss: 6.95580530166626 = 0.8109508156776428 + 1.0 * 6.144854545593262
Epoch 360, val loss: 1.0454200506210327
Epoch 370, training loss: 6.92686128616333 = 0.7796455025672913 + 1.0 * 6.147215843200684
Epoch 370, val loss: 1.0249820947647095
Epoch 380, training loss: 6.888762474060059 = 0.7498828172683716 + 1.0 * 6.138879776000977
Epoch 380, val loss: 1.0061140060424805
Epoch 390, training loss: 6.856938362121582 = 0.7212435603141785 + 1.0 * 6.135694980621338
Epoch 390, val loss: 0.9886085987091064
Epoch 400, training loss: 6.845386028289795 = 0.6935078501701355 + 1.0 * 6.151878356933594
Epoch 400, val loss: 0.9722248315811157
Epoch 410, training loss: 6.801023960113525 = 0.6666785478591919 + 1.0 * 6.134345531463623
Epoch 410, val loss: 0.9567954540252686
Epoch 420, training loss: 6.768027305603027 = 0.6404075026512146 + 1.0 * 6.127619743347168
Epoch 420, val loss: 0.9420995712280273
Epoch 430, training loss: 6.738749027252197 = 0.6142268180847168 + 1.0 * 6.1245222091674805
Epoch 430, val loss: 0.9278081059455872
Epoch 440, training loss: 6.709574222564697 = 0.5878028869628906 + 1.0 * 6.121771335601807
Epoch 440, val loss: 0.9136424660682678
Epoch 450, training loss: 6.684723377227783 = 0.5609156489372253 + 1.0 * 6.123807907104492
Epoch 450, val loss: 0.8993772864341736
Epoch 460, training loss: 6.658957481384277 = 0.5338384509086609 + 1.0 * 6.125119209289551
Epoch 460, val loss: 0.8847560882568359
Epoch 470, training loss: 6.623086929321289 = 0.5063908100128174 + 1.0 * 6.116695880889893
Epoch 470, val loss: 0.8701360821723938
Epoch 480, training loss: 6.592120170593262 = 0.47845885157585144 + 1.0 * 6.113661289215088
Epoch 480, val loss: 0.8554616570472717
Epoch 490, training loss: 6.565162658691406 = 0.4501672685146332 + 1.0 * 6.11499547958374
Epoch 490, val loss: 0.8407196402549744
Epoch 500, training loss: 6.545342445373535 = 0.4220390319824219 + 1.0 * 6.123303413391113
Epoch 500, val loss: 0.8262393474578857
Epoch 510, training loss: 6.506139278411865 = 0.3941803574562073 + 1.0 * 6.111958980560303
Epoch 510, val loss: 0.8124851584434509
Epoch 520, training loss: 6.474587917327881 = 0.3670662045478821 + 1.0 * 6.1075215339660645
Epoch 520, val loss: 0.7998027205467224
Epoch 530, training loss: 6.445650577545166 = 0.3408385217189789 + 1.0 * 6.104812145233154
Epoch 530, val loss: 0.7885099053382874
Epoch 540, training loss: 6.43045711517334 = 0.31584227085113525 + 1.0 * 6.114614963531494
Epoch 540, val loss: 0.7787405252456665
Epoch 550, training loss: 6.397140979766846 = 0.29262247681617737 + 1.0 * 6.104518413543701
Epoch 550, val loss: 0.7708208560943604
Epoch 560, training loss: 6.3813982009887695 = 0.2710520029067993 + 1.0 * 6.11034631729126
Epoch 560, val loss: 0.7646947503089905
Epoch 570, training loss: 6.3531622886657715 = 0.2511969804763794 + 1.0 * 6.101965427398682
Epoch 570, val loss: 0.76025390625
Epoch 580, training loss: 6.3295440673828125 = 0.23302564024925232 + 1.0 * 6.096518516540527
Epoch 580, val loss: 0.7573766112327576
Epoch 590, training loss: 6.312396049499512 = 0.21629935503005981 + 1.0 * 6.096096515655518
Epoch 590, val loss: 0.7558408379554749
Epoch 600, training loss: 6.296901702880859 = 0.20101581513881683 + 1.0 * 6.095885753631592
Epoch 600, val loss: 0.7553342580795288
Epoch 610, training loss: 6.280951023101807 = 0.18709713220596313 + 1.0 * 6.093853950500488
Epoch 610, val loss: 0.7558242678642273
Epoch 620, training loss: 6.265656471252441 = 0.1743147373199463 + 1.0 * 6.091341495513916
Epoch 620, val loss: 0.7572826147079468
Epoch 630, training loss: 6.257604598999023 = 0.16256456077098846 + 1.0 * 6.0950398445129395
Epoch 630, val loss: 0.7595164179801941
Epoch 640, training loss: 6.24355411529541 = 0.15178945660591125 + 1.0 * 6.091764450073242
Epoch 640, val loss: 0.7623456120491028
Epoch 650, training loss: 6.227961540222168 = 0.1418909728527069 + 1.0 * 6.086070537567139
Epoch 650, val loss: 0.7657791972160339
Epoch 660, training loss: 6.226582050323486 = 0.13278979063034058 + 1.0 * 6.09379243850708
Epoch 660, val loss: 0.7697530388832092
Epoch 670, training loss: 6.215097904205322 = 0.12449008971452713 + 1.0 * 6.090607643127441
Epoch 670, val loss: 0.7740316987037659
Epoch 680, training loss: 6.1994147300720215 = 0.11686711758375168 + 1.0 * 6.082547664642334
Epoch 680, val loss: 0.7788286209106445
Epoch 690, training loss: 6.190161228179932 = 0.10984710603952408 + 1.0 * 6.0803141593933105
Epoch 690, val loss: 0.7840298414230347
Epoch 700, training loss: 6.193171501159668 = 0.10334485024213791 + 1.0 * 6.089826583862305
Epoch 700, val loss: 0.7894487380981445
Epoch 710, training loss: 6.18276309967041 = 0.09744984656572342 + 1.0 * 6.085313320159912
Epoch 710, val loss: 0.7950088977813721
Epoch 720, training loss: 6.169489860534668 = 0.0919613316655159 + 1.0 * 6.077528476715088
Epoch 720, val loss: 0.800820529460907
Epoch 730, training loss: 6.16225528717041 = 0.08688343316316605 + 1.0 * 6.075371742248535
Epoch 730, val loss: 0.806823194026947
Epoch 740, training loss: 6.159093856811523 = 0.08217547833919525 + 1.0 * 6.076918601989746
Epoch 740, val loss: 0.8129510283470154
Epoch 750, training loss: 6.15437650680542 = 0.0778350830078125 + 1.0 * 6.076541423797607
Epoch 750, val loss: 0.8190593719482422
Epoch 760, training loss: 6.147959232330322 = 0.07381031662225723 + 1.0 * 6.074149131774902
Epoch 760, val loss: 0.8252663016319275
Epoch 770, training loss: 6.143128395080566 = 0.07007986307144165 + 1.0 * 6.0730485916137695
Epoch 770, val loss: 0.8315732479095459
Epoch 780, training loss: 6.136329650878906 = 0.06660746783018112 + 1.0 * 6.0697221755981445
Epoch 780, val loss: 0.8379321098327637
Epoch 790, training loss: 6.142123222351074 = 0.06337817758321762 + 1.0 * 6.078744888305664
Epoch 790, val loss: 0.8442938327789307
Epoch 800, training loss: 6.131001949310303 = 0.060360442847013474 + 1.0 * 6.07064151763916
Epoch 800, val loss: 0.8504645228385925
Epoch 810, training loss: 6.125342845916748 = 0.057573799043893814 + 1.0 * 6.0677690505981445
Epoch 810, val loss: 0.8568093180656433
Epoch 820, training loss: 6.120946884155273 = 0.054948411881923676 + 1.0 * 6.065998554229736
Epoch 820, val loss: 0.8631573915481567
Epoch 830, training loss: 6.123712539672852 = 0.052501898258924484 + 1.0 * 6.071210861206055
Epoch 830, val loss: 0.8695246577262878
Epoch 840, training loss: 6.118534564971924 = 0.05020665004849434 + 1.0 * 6.068327903747559
Epoch 840, val loss: 0.8757328391075134
Epoch 850, training loss: 6.112484455108643 = 0.048057131469249725 + 1.0 * 6.064427375793457
Epoch 850, val loss: 0.8819571137428284
Epoch 860, training loss: 6.108715057373047 = 0.04603952541947365 + 1.0 * 6.062675476074219
Epoch 860, val loss: 0.8882150650024414
Epoch 870, training loss: 6.109898090362549 = 0.044143687933683395 + 1.0 * 6.065754413604736
Epoch 870, val loss: 0.8944215774536133
Epoch 880, training loss: 6.106539249420166 = 0.04236552119255066 + 1.0 * 6.064173698425293
Epoch 880, val loss: 0.9005439281463623
Epoch 890, training loss: 6.101226806640625 = 0.040699053555727005 + 1.0 * 6.060527801513672
Epoch 890, val loss: 0.9065790176391602
Epoch 900, training loss: 6.097956657409668 = 0.03912343457341194 + 1.0 * 6.058833122253418
Epoch 900, val loss: 0.9126173257827759
Epoch 910, training loss: 6.095273017883301 = 0.03763318061828613 + 1.0 * 6.0576395988464355
Epoch 910, val loss: 0.9186257719993591
Epoch 920, training loss: 6.09904146194458 = 0.03622918948531151 + 1.0 * 6.062812328338623
Epoch 920, val loss: 0.9245689511299133
Epoch 930, training loss: 6.098441123962402 = 0.034898050129413605 + 1.0 * 6.06354284286499
Epoch 930, val loss: 0.9303531646728516
Epoch 940, training loss: 6.091283798217773 = 0.03364286944270134 + 1.0 * 6.05764102935791
Epoch 940, val loss: 0.9360253810882568
Epoch 950, training loss: 6.086945056915283 = 0.03245604783296585 + 1.0 * 6.0544891357421875
Epoch 950, val loss: 0.9417572617530823
Epoch 960, training loss: 6.084715366363525 = 0.03132783621549606 + 1.0 * 6.053387641906738
Epoch 960, val loss: 0.9474347829818726
Epoch 970, training loss: 6.0926899909973145 = 0.030253641307353973 + 1.0 * 6.062436580657959
Epoch 970, val loss: 0.9529827833175659
Epoch 980, training loss: 6.085598945617676 = 0.029242027550935745 + 1.0 * 6.056356906890869
Epoch 980, val loss: 0.9584794640541077
Epoch 990, training loss: 6.080254077911377 = 0.028276830911636353 + 1.0 * 6.051977157592773
Epoch 990, val loss: 0.9638946652412415
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7417
Flip ASR: 0.7067/225 nodes
The final ASR:0.66790, 0.08202, Accuracy:0.80494, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10596])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.83086, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.327709197998047 = 1.9538929462432861 + 1.0 * 8.37381649017334
Epoch 0, val loss: 1.9524730443954468
Epoch 10, training loss: 10.31631851196289 = 1.9430633783340454 + 1.0 * 8.373254776000977
Epoch 10, val loss: 1.9414663314819336
Epoch 20, training loss: 10.299549102783203 = 1.9294520616531372 + 1.0 * 8.370097160339355
Epoch 20, val loss: 1.9275001287460327
Epoch 30, training loss: 10.26116943359375 = 1.9104084968566895 + 1.0 * 8.350761413574219
Epoch 30, val loss: 1.9081413745880127
Epoch 40, training loss: 10.109652519226074 = 1.886440396308899 + 1.0 * 8.223212242126465
Epoch 40, val loss: 1.885115385055542
Epoch 50, training loss: 9.321803092956543 = 1.8616626262664795 + 1.0 * 7.460140228271484
Epoch 50, val loss: 1.8612931966781616
Epoch 60, training loss: 8.863551139831543 = 1.8421603441238403 + 1.0 * 7.021390438079834
Epoch 60, val loss: 1.8437167406082153
Epoch 70, training loss: 8.59868049621582 = 1.829729676246643 + 1.0 * 6.768950939178467
Epoch 70, val loss: 1.8319435119628906
Epoch 80, training loss: 8.461397171020508 = 1.8159109354019165 + 1.0 * 6.645486354827881
Epoch 80, val loss: 1.8191097974777222
Epoch 90, training loss: 8.374969482421875 = 1.8026806116104126 + 1.0 * 6.572288990020752
Epoch 90, val loss: 1.8067833185195923
Epoch 100, training loss: 8.29338264465332 = 1.7899514436721802 + 1.0 * 6.50343132019043
Epoch 100, val loss: 1.7955728769302368
Epoch 110, training loss: 8.220061302185059 = 1.7793331146240234 + 1.0 * 6.440728187561035
Epoch 110, val loss: 1.7864924669265747
Epoch 120, training loss: 8.15866470336914 = 1.76911199092865 + 1.0 * 6.389552593231201
Epoch 120, val loss: 1.7776257991790771
Epoch 130, training loss: 8.1051025390625 = 1.7574938535690308 + 1.0 * 6.34760856628418
Epoch 130, val loss: 1.767526388168335
Epoch 140, training loss: 8.057409286499023 = 1.743652582168579 + 1.0 * 6.313756942749023
Epoch 140, val loss: 1.755839467048645
Epoch 150, training loss: 8.015055656433105 = 1.7268574237823486 + 1.0 * 6.288197994232178
Epoch 150, val loss: 1.7420129776000977
Epoch 160, training loss: 7.971736431121826 = 1.7063535451889038 + 1.0 * 6.265382766723633
Epoch 160, val loss: 1.725317358970642
Epoch 170, training loss: 7.927592754364014 = 1.6807299852371216 + 1.0 * 6.246862888336182
Epoch 170, val loss: 1.7044011354446411
Epoch 180, training loss: 7.882289409637451 = 1.6488474607467651 + 1.0 * 6.2334418296813965
Epoch 180, val loss: 1.678222417831421
Epoch 190, training loss: 7.831588268280029 = 1.6099730730056763 + 1.0 * 6.221615314483643
Epoch 190, val loss: 1.6460087299346924
Epoch 200, training loss: 7.775440216064453 = 1.5627548694610596 + 1.0 * 6.2126851081848145
Epoch 200, val loss: 1.6066325902938843
Epoch 210, training loss: 7.713536262512207 = 1.5072414875030518 + 1.0 * 6.206294536590576
Epoch 210, val loss: 1.5602288246154785
Epoch 220, training loss: 7.645108222961426 = 1.4462082386016846 + 1.0 * 6.198899745941162
Epoch 220, val loss: 1.5095901489257812
Epoch 230, training loss: 7.5753278732299805 = 1.382098913192749 + 1.0 * 6.193228721618652
Epoch 230, val loss: 1.4574270248413086
Epoch 240, training loss: 7.507663726806641 = 1.3178143501281738 + 1.0 * 6.189849376678467
Epoch 240, val loss: 1.4058761596679688
Epoch 250, training loss: 7.438650131225586 = 1.2558842897415161 + 1.0 * 6.182765960693359
Epoch 250, val loss: 1.3575094938278198
Epoch 260, training loss: 7.373193740844727 = 1.1957980394363403 + 1.0 * 6.177395820617676
Epoch 260, val loss: 1.3111867904663086
Epoch 270, training loss: 7.310052871704102 = 1.1379077434539795 + 1.0 * 6.172145366668701
Epoch 270, val loss: 1.2673943042755127
Epoch 280, training loss: 7.249496936798096 = 1.0822019577026367 + 1.0 * 6.167294979095459
Epoch 280, val loss: 1.2256877422332764
Epoch 290, training loss: 7.192509651184082 = 1.028045892715454 + 1.0 * 6.164463996887207
Epoch 290, val loss: 1.185637354850769
Epoch 300, training loss: 7.135174751281738 = 0.9759504795074463 + 1.0 * 6.159224033355713
Epoch 300, val loss: 1.1477351188659668
Epoch 310, training loss: 7.080525875091553 = 0.9258129000663757 + 1.0 * 6.154713153839111
Epoch 310, val loss: 1.1118255853652954
Epoch 320, training loss: 7.031731128692627 = 0.8779956698417664 + 1.0 * 6.153735637664795
Epoch 320, val loss: 1.078270673751831
Epoch 330, training loss: 6.981856822967529 = 0.8333727717399597 + 1.0 * 6.148484230041504
Epoch 330, val loss: 1.0479029417037964
Epoch 340, training loss: 6.93520975112915 = 0.7914851307868958 + 1.0 * 6.14372444152832
Epoch 340, val loss: 1.0198864936828613
Epoch 350, training loss: 6.891159534454346 = 0.751506507396698 + 1.0 * 6.139653205871582
Epoch 350, val loss: 0.9937093257904053
Epoch 360, training loss: 6.85261344909668 = 0.7131548523902893 + 1.0 * 6.139458656311035
Epoch 360, val loss: 0.9689502716064453
Epoch 370, training loss: 6.812078952789307 = 0.6767475008964539 + 1.0 * 6.135331630706787
Epoch 370, val loss: 0.9458016157150269
Epoch 380, training loss: 6.772008895874023 = 0.6420125365257263 + 1.0 * 6.129996299743652
Epoch 380, val loss: 0.9238826036453247
Epoch 390, training loss: 6.736185073852539 = 0.6084861755371094 + 1.0 * 6.12769889831543
Epoch 390, val loss: 0.9029677510261536
Epoch 400, training loss: 6.701165676116943 = 0.5764128565788269 + 1.0 * 6.124752998352051
Epoch 400, val loss: 0.883264422416687
Epoch 410, training loss: 6.672616481781006 = 0.5459719300270081 + 1.0 * 6.126644611358643
Epoch 410, val loss: 0.8650263547897339
Epoch 420, training loss: 6.636970520019531 = 0.5173782706260681 + 1.0 * 6.119592189788818
Epoch 420, val loss: 0.8483922481536865
Epoch 430, training loss: 6.605633735656738 = 0.49023017287254333 + 1.0 * 6.115403652191162
Epoch 430, val loss: 0.8333208560943604
Epoch 440, training loss: 6.5843186378479 = 0.4643481969833374 + 1.0 * 6.119970321655273
Epoch 440, val loss: 0.8196259140968323
Epoch 450, training loss: 6.5513997077941895 = 0.44002825021743774 + 1.0 * 6.1113715171813965
Epoch 450, val loss: 0.8074043393135071
Epoch 460, training loss: 6.524526119232178 = 0.41681134700775146 + 1.0 * 6.107714653015137
Epoch 460, val loss: 0.7965596318244934
Epoch 470, training loss: 6.500368118286133 = 0.39443841576576233 + 1.0 * 6.105929851531982
Epoch 470, val loss: 0.7865703105926514
Epoch 480, training loss: 6.4891204833984375 = 0.3728216588497162 + 1.0 * 6.116298675537109
Epoch 480, val loss: 0.7776488661766052
Epoch 490, training loss: 6.457064628601074 = 0.3522719144821167 + 1.0 * 6.104792594909668
Epoch 490, val loss: 0.769681990146637
Epoch 500, training loss: 6.43189811706543 = 0.332419216632843 + 1.0 * 6.099478721618652
Epoch 500, val loss: 0.7625970244407654
Epoch 510, training loss: 6.421454429626465 = 0.3131891191005707 + 1.0 * 6.108265399932861
Epoch 510, val loss: 0.7562422752380371
Epoch 520, training loss: 6.391130447387695 = 0.2947353422641754 + 1.0 * 6.096395015716553
Epoch 520, val loss: 0.7508267164230347
Epoch 530, training loss: 6.37091588973999 = 0.2769722640514374 + 1.0 * 6.0939435958862305
Epoch 530, val loss: 0.7462241053581238
Epoch 540, training loss: 6.352957248687744 = 0.2598685026168823 + 1.0 * 6.093088626861572
Epoch 540, val loss: 0.7423518300056458
Epoch 550, training loss: 6.343458652496338 = 0.24360983073711395 + 1.0 * 6.099848747253418
Epoch 550, val loss: 0.7391589283943176
Epoch 560, training loss: 6.320160865783691 = 0.22840172052383423 + 1.0 * 6.091759204864502
Epoch 560, val loss: 0.7367941737174988
Epoch 570, training loss: 6.302181243896484 = 0.21400336921215057 + 1.0 * 6.088177680969238
Epoch 570, val loss: 0.7351396679878235
Epoch 580, training loss: 6.28622579574585 = 0.20035797357559204 + 1.0 * 6.085867881774902
Epoch 580, val loss: 0.7341882586479187
Epoch 590, training loss: 6.272233009338379 = 0.1874767243862152 + 1.0 * 6.084756374359131
Epoch 590, val loss: 0.7340921759605408
Epoch 600, training loss: 6.258347034454346 = 0.17542871832847595 + 1.0 * 6.082918167114258
Epoch 600, val loss: 0.734718382358551
Epoch 610, training loss: 6.245594024658203 = 0.16425319015979767 + 1.0 * 6.081340789794922
Epoch 610, val loss: 0.7360893487930298
Epoch 620, training loss: 6.234435081481934 = 0.15382325649261475 + 1.0 * 6.080611705780029
Epoch 620, val loss: 0.7380731701850891
Epoch 630, training loss: 6.2264509201049805 = 0.14414525032043457 + 1.0 * 6.082305431365967
Epoch 630, val loss: 0.7406355738639832
Epoch 640, training loss: 6.21740198135376 = 0.13524673879146576 + 1.0 * 6.082155227661133
Epoch 640, val loss: 0.7438433170318604
Epoch 650, training loss: 6.204792499542236 = 0.12700730562210083 + 1.0 * 6.077785015106201
Epoch 650, val loss: 0.7475273609161377
Epoch 660, training loss: 6.196577072143555 = 0.11936350166797638 + 1.0 * 6.077213764190674
Epoch 660, val loss: 0.7516031861305237
Epoch 670, training loss: 6.187808513641357 = 0.11228522658348083 + 1.0 * 6.075523376464844
Epoch 670, val loss: 0.7561269402503967
Epoch 680, training loss: 6.181540489196777 = 0.10574983805418015 + 1.0 * 6.075790882110596
Epoch 680, val loss: 0.7609629034996033
Epoch 690, training loss: 6.171743392944336 = 0.09972862154245377 + 1.0 * 6.072014808654785
Epoch 690, val loss: 0.7659952640533447
Epoch 700, training loss: 6.170655250549316 = 0.09415888786315918 + 1.0 * 6.076496124267578
Epoch 700, val loss: 0.7712516784667969
Epoch 710, training loss: 6.158960342407227 = 0.0890321135520935 + 1.0 * 6.069928169250488
Epoch 710, val loss: 0.7766344547271729
Epoch 720, training loss: 6.1538987159729 = 0.08427568525075912 + 1.0 * 6.069622993469238
Epoch 720, val loss: 0.7821407914161682
Epoch 730, training loss: 6.153327941894531 = 0.07986345887184143 + 1.0 * 6.073464393615723
Epoch 730, val loss: 0.7877470254898071
Epoch 740, training loss: 6.146581172943115 = 0.07580779492855072 + 1.0 * 6.070773601531982
Epoch 740, val loss: 0.7932703495025635
Epoch 750, training loss: 6.137868404388428 = 0.07206990569829941 + 1.0 * 6.065798282623291
Epoch 750, val loss: 0.7989039421081543
Epoch 760, training loss: 6.132569789886475 = 0.0685846135020256 + 1.0 * 6.063985347747803
Epoch 760, val loss: 0.8044694662094116
Epoch 770, training loss: 6.128301620483398 = 0.06531892716884613 + 1.0 * 6.062982559204102
Epoch 770, val loss: 0.8101071715354919
Epoch 780, training loss: 6.135854244232178 = 0.062264781445264816 + 1.0 * 6.073589324951172
Epoch 780, val loss: 0.8157142400741577
Epoch 790, training loss: 6.122679233551025 = 0.059446174651384354 + 1.0 * 6.063232898712158
Epoch 790, val loss: 0.821288526058197
Epoch 800, training loss: 6.119528770446777 = 0.05681991949677467 + 1.0 * 6.062708854675293
Epoch 800, val loss: 0.8268645405769348
Epoch 810, training loss: 6.122110366821289 = 0.05436382815241814 + 1.0 * 6.067746639251709
Epoch 810, val loss: 0.8322902321815491
Epoch 820, training loss: 6.11199426651001 = 0.052061960101127625 + 1.0 * 6.059932231903076
Epoch 820, val loss: 0.8377515077590942
Epoch 830, training loss: 6.107694625854492 = 0.04990558326244354 + 1.0 * 6.057788848876953
Epoch 830, val loss: 0.8431921005249023
Epoch 840, training loss: 6.10903787612915 = 0.04786623269319534 + 1.0 * 6.061171531677246
Epoch 840, val loss: 0.8486105799674988
Epoch 850, training loss: 6.104615688323975 = 0.045963604003190994 + 1.0 * 6.058651924133301
Epoch 850, val loss: 0.8539066314697266
Epoch 860, training loss: 6.1007490158081055 = 0.04417263716459274 + 1.0 * 6.056576251983643
Epoch 860, val loss: 0.8592869639396667
Epoch 870, training loss: 6.099188804626465 = 0.042484160512685776 + 1.0 * 6.056704521179199
Epoch 870, val loss: 0.8645801544189453
Epoch 880, training loss: 6.097412109375 = 0.04089280590415001 + 1.0 * 6.056519508361816
Epoch 880, val loss: 0.869676947593689
Epoch 890, training loss: 6.092621803283691 = 0.03939373418688774 + 1.0 * 6.05322790145874
Epoch 890, val loss: 0.8748864531517029
Epoch 900, training loss: 6.090297222137451 = 0.037972696125507355 + 1.0 * 6.052324295043945
Epoch 900, val loss: 0.8799143433570862
Epoch 910, training loss: 6.092369079589844 = 0.03662600368261337 + 1.0 * 6.055743217468262
Epoch 910, val loss: 0.8849056959152222
Epoch 920, training loss: 6.086716175079346 = 0.03535635769367218 + 1.0 * 6.0513596534729
Epoch 920, val loss: 0.8898782134056091
Epoch 930, training loss: 6.095461368560791 = 0.03415011987090111 + 1.0 * 6.0613112449646
Epoch 930, val loss: 0.8948149085044861
Epoch 940, training loss: 6.083177089691162 = 0.033010177314281464 + 1.0 * 6.050167083740234
Epoch 940, val loss: 0.8996113538742065
Epoch 950, training loss: 6.080105781555176 = 0.03193127363920212 + 1.0 * 6.0481743812561035
Epoch 950, val loss: 0.9044190049171448
Epoch 960, training loss: 6.079464435577393 = 0.030897654592990875 + 1.0 * 6.048566818237305
Epoch 960, val loss: 0.9090768694877625
Epoch 970, training loss: 6.078110218048096 = 0.029913678765296936 + 1.0 * 6.048196315765381
Epoch 970, val loss: 0.9136561751365662
Epoch 980, training loss: 6.0756330490112305 = 0.02898290753364563 + 1.0 * 6.046649932861328
Epoch 980, val loss: 0.9183289408683777
Epoch 990, training loss: 6.073702812194824 = 0.028091687709093094 + 1.0 * 6.0456109046936035
Epoch 990, val loss: 0.9228722453117371
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6863
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.330492973327637 = 1.9567641019821167 + 1.0 * 8.37372875213623
Epoch 0, val loss: 1.9548728466033936
Epoch 10, training loss: 10.318327903747559 = 1.945691466331482 + 1.0 * 8.372636795043945
Epoch 10, val loss: 1.9441686868667603
Epoch 20, training loss: 10.297330856323242 = 1.932242751121521 + 1.0 * 8.36508846282959
Epoch 20, val loss: 1.9307280778884888
Epoch 30, training loss: 10.231657981872559 = 1.9144943952560425 + 1.0 * 8.317163467407227
Epoch 30, val loss: 1.9127497673034668
Epoch 40, training loss: 9.787432670593262 = 1.8943984508514404 + 1.0 * 7.893033981323242
Epoch 40, val loss: 1.8928462266921997
Epoch 50, training loss: 9.080582618713379 = 1.874773621559143 + 1.0 * 7.205809116363525
Epoch 50, val loss: 1.8731917142868042
Epoch 60, training loss: 8.70868968963623 = 1.858910083770752 + 1.0 * 6.8497796058654785
Epoch 60, val loss: 1.8576608896255493
Epoch 70, training loss: 8.501117706298828 = 1.8452085256576538 + 1.0 * 6.655909061431885
Epoch 70, val loss: 1.8437778949737549
Epoch 80, training loss: 8.377983093261719 = 1.8335340023040771 + 1.0 * 6.544449329376221
Epoch 80, val loss: 1.8315510749816895
Epoch 90, training loss: 8.286319732666016 = 1.820710301399231 + 1.0 * 6.465609550476074
Epoch 90, val loss: 1.8184149265289307
Epoch 100, training loss: 8.208821296691895 = 1.8085590600967407 + 1.0 * 6.400262355804443
Epoch 100, val loss: 1.8059875965118408
Epoch 110, training loss: 8.1512451171875 = 1.7967414855957031 + 1.0 * 6.354503631591797
Epoch 110, val loss: 1.7938508987426758
Epoch 120, training loss: 8.104397773742676 = 1.784253478050232 + 1.0 * 6.3201446533203125
Epoch 120, val loss: 1.7810745239257812
Epoch 130, training loss: 8.065278053283691 = 1.7711718082427979 + 1.0 * 6.2941060066223145
Epoch 130, val loss: 1.7679417133331299
Epoch 140, training loss: 8.030618667602539 = 1.7572693824768066 + 1.0 * 6.273348808288574
Epoch 140, val loss: 1.7545385360717773
Epoch 150, training loss: 7.996833801269531 = 1.7418137788772583 + 1.0 * 6.2550201416015625
Epoch 150, val loss: 1.7402291297912598
Epoch 160, training loss: 7.9641947746276855 = 1.7240166664123535 + 1.0 * 6.240178108215332
Epoch 160, val loss: 1.7245428562164307
Epoch 170, training loss: 7.9277262687683105 = 1.7034730911254883 + 1.0 * 6.224253177642822
Epoch 170, val loss: 1.707037329673767
Epoch 180, training loss: 7.890523910522461 = 1.6790251731872559 + 1.0 * 6.211498737335205
Epoch 180, val loss: 1.6868741512298584
Epoch 190, training loss: 7.851239204406738 = 1.649310827255249 + 1.0 * 6.20192813873291
Epoch 190, val loss: 1.6627825498580933
Epoch 200, training loss: 7.806404113769531 = 1.613395094871521 + 1.0 * 6.193008899688721
Epoch 200, val loss: 1.6340587139129639
Epoch 210, training loss: 7.75396728515625 = 1.5702897310256958 + 1.0 * 6.183677673339844
Epoch 210, val loss: 1.599974274635315
Epoch 220, training loss: 7.696059703826904 = 1.5192241668701172 + 1.0 * 6.176835536956787
Epoch 220, val loss: 1.5595136880874634
Epoch 230, training loss: 7.632659435272217 = 1.460134506225586 + 1.0 * 6.172524929046631
Epoch 230, val loss: 1.5130234956741333
Epoch 240, training loss: 7.562504768371582 = 1.3958977460861206 + 1.0 * 6.166606903076172
Epoch 240, val loss: 1.463183045387268
Epoch 250, training loss: 7.492425441741943 = 1.329206943511963 + 1.0 * 6.1632184982299805
Epoch 250, val loss: 1.4120686054229736
Epoch 260, training loss: 7.4207563400268555 = 1.2633225917816162 + 1.0 * 6.157433986663818
Epoch 260, val loss: 1.3619407415390015
Epoch 270, training loss: 7.351463794708252 = 1.199454665184021 + 1.0 * 6.152009010314941
Epoch 270, val loss: 1.3138846158981323
Epoch 280, training loss: 7.292217254638672 = 1.139120101928711 + 1.0 * 6.153097152709961
Epoch 280, val loss: 1.268856406211853
Epoch 290, training loss: 7.228486061096191 = 1.0842868089675903 + 1.0 * 6.144199371337891
Epoch 290, val loss: 1.2286200523376465
Epoch 300, training loss: 7.172629356384277 = 1.0344518423080444 + 1.0 * 6.138177394866943
Epoch 300, val loss: 1.1924101114273071
Epoch 310, training loss: 7.124238014221191 = 0.9887951016426086 + 1.0 * 6.135442733764648
Epoch 310, val loss: 1.1598010063171387
Epoch 320, training loss: 7.080743789672852 = 0.9472382664680481 + 1.0 * 6.133505344390869
Epoch 320, val loss: 1.1306709051132202
Epoch 330, training loss: 7.035549640655518 = 0.9089153409004211 + 1.0 * 6.126634120941162
Epoch 330, val loss: 1.1046580076217651
Epoch 340, training loss: 6.996472358703613 = 0.8729288578033447 + 1.0 * 6.1235432624816895
Epoch 340, val loss: 1.0807197093963623
Epoch 350, training loss: 6.960723876953125 = 0.839040994644165 + 1.0 * 6.121683120727539
Epoch 350, val loss: 1.0585730075836182
Epoch 360, training loss: 6.922860145568848 = 0.8068761825561523 + 1.0 * 6.115983963012695
Epoch 360, val loss: 1.0380891561508179
Epoch 370, training loss: 6.8888373374938965 = 0.7759033441543579 + 1.0 * 6.112934112548828
Epoch 370, val loss: 1.0188796520233154
Epoch 380, training loss: 6.862122535705566 = 0.7463418841362 + 1.0 * 6.115780830383301
Epoch 380, val loss: 1.0008071660995483
Epoch 390, training loss: 6.825835704803467 = 0.7181182503700256 + 1.0 * 6.107717514038086
Epoch 390, val loss: 0.9843441247940063
Epoch 400, training loss: 6.799080848693848 = 0.6906893849372864 + 1.0 * 6.108391284942627
Epoch 400, val loss: 0.9687724709510803
Epoch 410, training loss: 6.768627166748047 = 0.6640021800994873 + 1.0 * 6.1046247482299805
Epoch 410, val loss: 0.9538843035697937
Epoch 420, training loss: 6.73781156539917 = 0.637739360332489 + 1.0 * 6.100072383880615
Epoch 420, val loss: 0.9398329854011536
Epoch 430, training loss: 6.714240074157715 = 0.6117347478866577 + 1.0 * 6.102505207061768
Epoch 430, val loss: 0.9261943697929382
Epoch 440, training loss: 6.681687355041504 = 0.586035430431366 + 1.0 * 6.095652103424072
Epoch 440, val loss: 0.9132269620895386
Epoch 450, training loss: 6.654055118560791 = 0.5605276226997375 + 1.0 * 6.093527317047119
Epoch 450, val loss: 0.9008547067642212
Epoch 460, training loss: 6.630388259887695 = 0.5353894233703613 + 1.0 * 6.094998836517334
Epoch 460, val loss: 0.8890542984008789
Epoch 470, training loss: 6.600512504577637 = 0.510744571685791 + 1.0 * 6.089767932891846
Epoch 470, val loss: 0.8783806562423706
Epoch 480, training loss: 6.574760437011719 = 0.4864838421344757 + 1.0 * 6.088276386260986
Epoch 480, val loss: 0.8686109185218811
Epoch 490, training loss: 6.5543928146362305 = 0.4628062844276428 + 1.0 * 6.091586589813232
Epoch 490, val loss: 0.8599419593811035
Epoch 500, training loss: 6.525306701660156 = 0.43986934423446655 + 1.0 * 6.085437297821045
Epoch 500, val loss: 0.8527104258537292
Epoch 510, training loss: 6.499508857727051 = 0.41741669178009033 + 1.0 * 6.08209228515625
Epoch 510, val loss: 0.8464357852935791
Epoch 520, training loss: 6.487082481384277 = 0.39545753598213196 + 1.0 * 6.091624736785889
Epoch 520, val loss: 0.8412164449691772
Epoch 530, training loss: 6.453545570373535 = 0.3743140399456024 + 1.0 * 6.0792317390441895
Epoch 530, val loss: 0.8370005488395691
Epoch 540, training loss: 6.433995246887207 = 0.35394296050071716 + 1.0 * 6.080052375793457
Epoch 540, val loss: 0.8338867425918579
Epoch 550, training loss: 6.411670207977295 = 0.334512323141098 + 1.0 * 6.077157974243164
Epoch 550, val loss: 0.8316963315010071
Epoch 560, training loss: 6.3908491134643555 = 0.31596502661705017 + 1.0 * 6.074883937835693
Epoch 560, val loss: 0.8304395079612732
Epoch 570, training loss: 6.382441997528076 = 0.2983768582344055 + 1.0 * 6.084064960479736
Epoch 570, val loss: 0.8299745321273804
Epoch 580, training loss: 6.353882312774658 = 0.2818602919578552 + 1.0 * 6.072021961212158
Epoch 580, val loss: 0.8304335474967957
Epoch 590, training loss: 6.340265274047852 = 0.2664426565170288 + 1.0 * 6.073822498321533
Epoch 590, val loss: 0.8318399786949158
Epoch 600, training loss: 6.32155704498291 = 0.25201788544654846 + 1.0 * 6.0695390701293945
Epoch 600, val loss: 0.8340339660644531
Epoch 610, training loss: 6.308338642120361 = 0.23850218951702118 + 1.0 * 6.069836616516113
Epoch 610, val loss: 0.8370857238769531
Epoch 620, training loss: 6.297335147857666 = 0.22582778334617615 + 1.0 * 6.071507453918457
Epoch 620, val loss: 0.84062659740448
Epoch 630, training loss: 6.283934116363525 = 0.21396853029727936 + 1.0 * 6.069965362548828
Epoch 630, val loss: 0.8448665142059326
Epoch 640, training loss: 6.267561912536621 = 0.20286288857460022 + 1.0 * 6.064699172973633
Epoch 640, val loss: 0.8494923114776611
Epoch 650, training loss: 6.256588459014893 = 0.19241856038570404 + 1.0 * 6.064169883728027
Epoch 650, val loss: 0.8544210195541382
Epoch 660, training loss: 6.246628284454346 = 0.1825704127550125 + 1.0 * 6.06405782699585
Epoch 660, val loss: 0.859772264957428
Epoch 670, training loss: 6.236302375793457 = 0.17332912981510162 + 1.0 * 6.0629730224609375
Epoch 670, val loss: 0.8652637600898743
Epoch 680, training loss: 6.22651481628418 = 0.16466422379016876 + 1.0 * 6.061850547790527
Epoch 680, val loss: 0.8710755705833435
Epoch 690, training loss: 6.218312740325928 = 0.15652094781398773 + 1.0 * 6.061791896820068
Epoch 690, val loss: 0.8771277070045471
Epoch 700, training loss: 6.208369731903076 = 0.14887475967407227 + 1.0 * 6.059494972229004
Epoch 700, val loss: 0.8830954432487488
Epoch 710, training loss: 6.198910713195801 = 0.14167261123657227 + 1.0 * 6.0572381019592285
Epoch 710, val loss: 0.8893324136734009
Epoch 720, training loss: 6.194645404815674 = 0.13486874103546143 + 1.0 * 6.059776782989502
Epoch 720, val loss: 0.8956900238990784
Epoch 730, training loss: 6.18342924118042 = 0.12848065793514252 + 1.0 * 6.054948806762695
Epoch 730, val loss: 0.9020806550979614
Epoch 740, training loss: 6.175372123718262 = 0.12242279946804047 + 1.0 * 6.05294942855835
Epoch 740, val loss: 0.9085983633995056
Epoch 750, training loss: 6.17069149017334 = 0.11669005453586578 + 1.0 * 6.054001331329346
Epoch 750, val loss: 0.915166437625885
Epoch 760, training loss: 6.16464900970459 = 0.11128055304288864 + 1.0 * 6.05336856842041
Epoch 760, val loss: 0.9216998219490051
Epoch 770, training loss: 6.159548282623291 = 0.10619675368070602 + 1.0 * 6.053351402282715
Epoch 770, val loss: 0.9284966588020325
Epoch 780, training loss: 6.154304027557373 = 0.10139308869838715 + 1.0 * 6.052910804748535
Epoch 780, val loss: 0.9351698756217957
Epoch 790, training loss: 6.1448187828063965 = 0.0968581885099411 + 1.0 * 6.0479607582092285
Epoch 790, val loss: 0.9419150948524475
Epoch 800, training loss: 6.150007724761963 = 0.0925569236278534 + 1.0 * 6.057450771331787
Epoch 800, val loss: 0.9486615061759949
Epoch 810, training loss: 6.140908241271973 = 0.08847913891077042 + 1.0 * 6.05242919921875
Epoch 810, val loss: 0.9551528096199036
Epoch 820, training loss: 6.130563735961914 = 0.0846526175737381 + 1.0 * 6.0459113121032715
Epoch 820, val loss: 0.962018609046936
Epoch 830, training loss: 6.1267313957214355 = 0.08101101964712143 + 1.0 * 6.04572057723999
Epoch 830, val loss: 0.9688568115234375
Epoch 840, training loss: 6.129833698272705 = 0.07754513621330261 + 1.0 * 6.05228853225708
Epoch 840, val loss: 0.9755564332008362
Epoch 850, training loss: 6.1222100257873535 = 0.07426764816045761 + 1.0 * 6.047942161560059
Epoch 850, val loss: 0.982170045375824
Epoch 860, training loss: 6.116835594177246 = 0.07116803526878357 + 1.0 * 6.04566764831543
Epoch 860, val loss: 0.989062488079071
Epoch 870, training loss: 6.110899925231934 = 0.06821832805871964 + 1.0 * 6.042681694030762
Epoch 870, val loss: 0.9957134127616882
Epoch 880, training loss: 6.107632637023926 = 0.06541090458631516 + 1.0 * 6.042221546173096
Epoch 880, val loss: 1.0024203062057495
Epoch 890, training loss: 6.111338138580322 = 0.06272821128368378 + 1.0 * 6.048609733581543
Epoch 890, val loss: 1.008984923362732
Epoch 900, training loss: 6.104530334472656 = 0.060186225920915604 + 1.0 * 6.044343948364258
Epoch 900, val loss: 1.0155538320541382
Epoch 910, training loss: 6.103464126586914 = 0.057767435908317566 + 1.0 * 6.04569673538208
Epoch 910, val loss: 1.0222281217575073
Epoch 920, training loss: 6.094130039215088 = 0.05546462535858154 + 1.0 * 6.038665294647217
Epoch 920, val loss: 1.028774619102478
Epoch 930, training loss: 6.091383457183838 = 0.05327290669083595 + 1.0 * 6.038110733032227
Epoch 930, val loss: 1.035454273223877
Epoch 940, training loss: 6.088666915893555 = 0.05117492750287056 + 1.0 * 6.037491798400879
Epoch 940, val loss: 1.042018175125122
Epoch 950, training loss: 6.088017463684082 = 0.049167316406965256 + 1.0 * 6.0388503074646
Epoch 950, val loss: 1.0486172437667847
Epoch 960, training loss: 6.0895514488220215 = 0.04725443571805954 + 1.0 * 6.042296886444092
Epoch 960, val loss: 1.0550280809402466
Epoch 970, training loss: 6.083278656005859 = 0.04544449970126152 + 1.0 * 6.037834167480469
Epoch 970, val loss: 1.061517357826233
Epoch 980, training loss: 6.084251880645752 = 0.04371945559978485 + 1.0 * 6.04053258895874
Epoch 980, val loss: 1.0680464506149292
Epoch 990, training loss: 6.0760884284973145 = 0.04207734391093254 + 1.0 * 6.034010887145996
Epoch 990, val loss: 1.0744045972824097
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.7823
Flip ASR: 0.7467/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.326790809631348 = 1.9529602527618408 + 1.0 * 8.373830795288086
Epoch 0, val loss: 1.9529086351394653
Epoch 10, training loss: 10.316120147705078 = 1.942715048789978 + 1.0 * 8.373405456542969
Epoch 10, val loss: 1.9426063299179077
Epoch 20, training loss: 10.301237106323242 = 1.9304064512252808 + 1.0 * 8.370830535888672
Epoch 20, val loss: 1.93001389503479
Epoch 30, training loss: 10.266600608825684 = 1.9136041402816772 + 1.0 * 8.352996826171875
Epoch 30, val loss: 1.9129306077957153
Epoch 40, training loss: 10.098479270935059 = 1.8922483921051025 + 1.0 * 8.206231117248535
Epoch 40, val loss: 1.8919696807861328
Epoch 50, training loss: 9.282065391540527 = 1.8701519966125488 + 1.0 * 7.4119133949279785
Epoch 50, val loss: 1.8708138465881348
Epoch 60, training loss: 8.831087112426758 = 1.854090929031372 + 1.0 * 6.976995944976807
Epoch 60, val loss: 1.8558435440063477
Epoch 70, training loss: 8.559429168701172 = 1.841529369354248 + 1.0 * 6.717900276184082
Epoch 70, val loss: 1.843668818473816
Epoch 80, training loss: 8.382484436035156 = 1.8295443058013916 + 1.0 * 6.552940368652344
Epoch 80, val loss: 1.8326060771942139
Epoch 90, training loss: 8.281139373779297 = 1.8174681663513184 + 1.0 * 6.4636712074279785
Epoch 90, val loss: 1.821318507194519
Epoch 100, training loss: 8.206992149353027 = 1.8052486181259155 + 1.0 * 6.4017438888549805
Epoch 100, val loss: 1.8096561431884766
Epoch 110, training loss: 8.150543212890625 = 1.7928791046142578 + 1.0 * 6.357664108276367
Epoch 110, val loss: 1.798140048980713
Epoch 120, training loss: 8.104719161987305 = 1.7807358503341675 + 1.0 * 6.323983192443848
Epoch 120, val loss: 1.7870646715164185
Epoch 130, training loss: 8.065834045410156 = 1.7682595252990723 + 1.0 * 6.297574520111084
Epoch 130, val loss: 1.7759381532669067
Epoch 140, training loss: 8.030102729797363 = 1.7547348737716675 + 1.0 * 6.275367736816406
Epoch 140, val loss: 1.764098048210144
Epoch 150, training loss: 7.995891571044922 = 1.7394345998764038 + 1.0 * 6.2564568519592285
Epoch 150, val loss: 1.7510617971420288
Epoch 160, training loss: 7.962192535400391 = 1.7216060161590576 + 1.0 * 6.240586757659912
Epoch 160, val loss: 1.7363330125808716
Epoch 170, training loss: 7.926641464233398 = 1.700208067893982 + 1.0 * 6.226433277130127
Epoch 170, val loss: 1.7190409898757935
Epoch 180, training loss: 7.890005111694336 = 1.6741055250167847 + 1.0 * 6.215899467468262
Epoch 180, val loss: 1.6981958150863647
Epoch 190, training loss: 7.847933769226074 = 1.6422003507614136 + 1.0 * 6.205733299255371
Epoch 190, val loss: 1.6726816892623901
Epoch 200, training loss: 7.802031517028809 = 1.6033122539520264 + 1.0 * 6.198719501495361
Epoch 200, val loss: 1.6416176557540894
Epoch 210, training loss: 7.74738073348999 = 1.5571309328079224 + 1.0 * 6.190249919891357
Epoch 210, val loss: 1.6049151420593262
Epoch 220, training loss: 7.686469078063965 = 1.5031639337539673 + 1.0 * 6.183305263519287
Epoch 220, val loss: 1.5619572401046753
Epoch 230, training loss: 7.62107515335083 = 1.442024827003479 + 1.0 * 6.179050445556641
Epoch 230, val loss: 1.5136747360229492
Epoch 240, training loss: 7.549896240234375 = 1.377039909362793 + 1.0 * 6.172856330871582
Epoch 240, val loss: 1.4626758098602295
Epoch 250, training loss: 7.477723598480225 = 1.3101719617843628 + 1.0 * 6.167551517486572
Epoch 250, val loss: 1.4107164144515991
Epoch 260, training loss: 7.409980297088623 = 1.2436747550964355 + 1.0 * 6.1663055419921875
Epoch 260, val loss: 1.3596335649490356
Epoch 270, training loss: 7.338031768798828 = 1.180225133895874 + 1.0 * 6.157806396484375
Epoch 270, val loss: 1.3111696243286133
Epoch 280, training loss: 7.274911880493164 = 1.1203633546829224 + 1.0 * 6.154548645019531
Epoch 280, val loss: 1.2658631801605225
Epoch 290, training loss: 7.213091850280762 = 1.0646944046020508 + 1.0 * 6.148397445678711
Epoch 290, val loss: 1.2242615222930908
Epoch 300, training loss: 7.158699989318848 = 1.0129969120025635 + 1.0 * 6.145702838897705
Epoch 300, val loss: 1.1858620643615723
Epoch 310, training loss: 7.106195449829102 = 0.965458333492279 + 1.0 * 6.140737056732178
Epoch 310, val loss: 1.1508314609527588
Epoch 320, training loss: 7.0563249588012695 = 0.9213139414787292 + 1.0 * 6.135011196136475
Epoch 320, val loss: 1.1186785697937012
Epoch 330, training loss: 7.012563705444336 = 0.8794938921928406 + 1.0 * 6.13306999206543
Epoch 330, val loss: 1.0884681940078735
Epoch 340, training loss: 6.972341537475586 = 0.8396726846694946 + 1.0 * 6.132668972015381
Epoch 340, val loss: 1.0598829984664917
Epoch 350, training loss: 6.928535461425781 = 0.8018483519554138 + 1.0 * 6.126687049865723
Epoch 350, val loss: 1.0328465700149536
Epoch 360, training loss: 6.886326789855957 = 0.765559732913971 + 1.0 * 6.120767116546631
Epoch 360, val loss: 1.007399082183838
Epoch 370, training loss: 6.849534034729004 = 0.7303227186203003 + 1.0 * 6.119211196899414
Epoch 370, val loss: 0.9829971790313721
Epoch 380, training loss: 6.814661979675293 = 0.6963689923286438 + 1.0 * 6.118292808532715
Epoch 380, val loss: 0.959752082824707
Epoch 390, training loss: 6.776926517486572 = 0.663787305355072 + 1.0 * 6.1131391525268555
Epoch 390, val loss: 0.9382187724113464
Epoch 400, training loss: 6.743035793304443 = 0.6324333548545837 + 1.0 * 6.110602378845215
Epoch 400, val loss: 0.9180113077163696
Epoch 410, training loss: 6.709743022918701 = 0.6022151708602905 + 1.0 * 6.107527732849121
Epoch 410, val loss: 0.8992586731910706
Epoch 420, training loss: 6.677060127258301 = 0.572780430316925 + 1.0 * 6.104279518127441
Epoch 420, val loss: 0.8816375136375427
Epoch 430, training loss: 6.650676250457764 = 0.5440792441368103 + 1.0 * 6.106596946716309
Epoch 430, val loss: 0.865134596824646
Epoch 440, training loss: 6.616987228393555 = 0.5165470242500305 + 1.0 * 6.10044002532959
Epoch 440, val loss: 0.8498567342758179
Epoch 450, training loss: 6.587313652038574 = 0.4898448586463928 + 1.0 * 6.097468852996826
Epoch 450, val loss: 0.8356878757476807
Epoch 460, training loss: 6.56919527053833 = 0.46389251947402954 + 1.0 * 6.105302810668945
Epoch 460, val loss: 0.8223250508308411
Epoch 470, training loss: 6.533484935760498 = 0.4390122592449188 + 1.0 * 6.094472885131836
Epoch 470, val loss: 0.8100282549858093
Epoch 480, training loss: 6.5074238777160645 = 0.4151145815849304 + 1.0 * 6.092309474945068
Epoch 480, val loss: 0.7987224459648132
Epoch 490, training loss: 6.487145900726318 = 0.39224058389663696 + 1.0 * 6.094905376434326
Epoch 490, val loss: 0.7883265018463135
Epoch 500, training loss: 6.459895610809326 = 0.3706244230270386 + 1.0 * 6.089271068572998
Epoch 500, val loss: 0.7789563536643982
Epoch 510, training loss: 6.438998222351074 = 0.3501894474029541 + 1.0 * 6.088808536529541
Epoch 510, val loss: 0.770565390586853
Epoch 520, training loss: 6.420650005340576 = 0.3309439718723297 + 1.0 * 6.089705944061279
Epoch 520, val loss: 0.7632594704627991
Epoch 530, training loss: 6.397381782531738 = 0.312931627035141 + 1.0 * 6.0844502449035645
Epoch 530, val loss: 0.756974458694458
Epoch 540, training loss: 6.377983570098877 = 0.29596418142318726 + 1.0 * 6.082019329071045
Epoch 540, val loss: 0.7517868876457214
Epoch 550, training loss: 6.365012168884277 = 0.27995944023132324 + 1.0 * 6.085052490234375
Epoch 550, val loss: 0.7475183606147766
Epoch 560, training loss: 6.347609043121338 = 0.26491907238960266 + 1.0 * 6.0826897621154785
Epoch 560, val loss: 0.7441670894622803
Epoch 570, training loss: 6.329617500305176 = 0.2508142590522766 + 1.0 * 6.078803062438965
Epoch 570, val loss: 0.7416619062423706
Epoch 580, training loss: 6.329683780670166 = 0.23752392828464508 + 1.0 * 6.092159748077393
Epoch 580, val loss: 0.7399293184280396
Epoch 590, training loss: 6.302065849304199 = 0.22508762776851654 + 1.0 * 6.0769782066345215
Epoch 590, val loss: 0.7389262914657593
Epoch 600, training loss: 6.287833213806152 = 0.21336965262889862 + 1.0 * 6.074463367462158
Epoch 600, val loss: 0.7387232780456543
Epoch 610, training loss: 6.273550987243652 = 0.20221972465515137 + 1.0 * 6.07133150100708
Epoch 610, val loss: 0.7391571402549744
Epoch 620, training loss: 6.269972324371338 = 0.19160571694374084 + 1.0 * 6.078366756439209
Epoch 620, val loss: 0.740123450756073
Epoch 630, training loss: 6.2551045417785645 = 0.1816500425338745 + 1.0 * 6.0734543800354
Epoch 630, val loss: 0.7415721416473389
Epoch 640, training loss: 6.241084098815918 = 0.17230063676834106 + 1.0 * 6.068783283233643
Epoch 640, val loss: 0.7436829209327698
Epoch 650, training loss: 6.231598854064941 = 0.16342921555042267 + 1.0 * 6.068169593811035
Epoch 650, val loss: 0.746335506439209
Epoch 660, training loss: 6.221625804901123 = 0.15505510568618774 + 1.0 * 6.06657075881958
Epoch 660, val loss: 0.7493287324905396
Epoch 670, training loss: 6.213659286499023 = 0.14718733727931976 + 1.0 * 6.066472053527832
Epoch 670, val loss: 0.7526655197143555
Epoch 680, training loss: 6.203145503997803 = 0.13975246250629425 + 1.0 * 6.0633931159973145
Epoch 680, val loss: 0.7565051317214966
Epoch 690, training loss: 6.205964088439941 = 0.13271702826023102 + 1.0 * 6.073246955871582
Epoch 690, val loss: 0.7606616020202637
Epoch 700, training loss: 6.191004276275635 = 0.1261545717716217 + 1.0 * 6.064849853515625
Epoch 700, val loss: 0.7650052905082703
Epoch 710, training loss: 6.1824140548706055 = 0.11999746412038803 + 1.0 * 6.0624165534973145
Epoch 710, val loss: 0.7696787714958191
Epoch 720, training loss: 6.174614906311035 = 0.11417921632528305 + 1.0 * 6.060435771942139
Epoch 720, val loss: 0.7746477127075195
Epoch 730, training loss: 6.172476291656494 = 0.10869741439819336 + 1.0 * 6.063778877258301
Epoch 730, val loss: 0.7797082662582397
Epoch 740, training loss: 6.160615921020508 = 0.10355924069881439 + 1.0 * 6.057056903839111
Epoch 740, val loss: 0.7849107384681702
Epoch 750, training loss: 6.156040668487549 = 0.09871522337198257 + 1.0 * 6.05732536315918
Epoch 750, val loss: 0.7903156280517578
Epoch 760, training loss: 6.156282901763916 = 0.09414628148078918 + 1.0 * 6.062136650085449
Epoch 760, val loss: 0.7957732081413269
Epoch 770, training loss: 6.150180816650391 = 0.08987190574407578 + 1.0 * 6.060308933258057
Epoch 770, val loss: 0.8011800646781921
Epoch 780, training loss: 6.141279697418213 = 0.08585913479328156 + 1.0 * 6.055420398712158
Epoch 780, val loss: 0.8067008852958679
Epoch 790, training loss: 6.135269641876221 = 0.0820675939321518 + 1.0 * 6.053202152252197
Epoch 790, val loss: 0.8123375773429871
Epoch 800, training loss: 6.1384806632995605 = 0.07848016917705536 + 1.0 * 6.060000419616699
Epoch 800, val loss: 0.8179911971092224
Epoch 810, training loss: 6.131099224090576 = 0.07510565221309662 + 1.0 * 6.055993556976318
Epoch 810, val loss: 0.8234763741493225
Epoch 820, training loss: 6.125148296356201 = 0.07193975150585175 + 1.0 * 6.053208351135254
Epoch 820, val loss: 0.8290073871612549
Epoch 830, training loss: 6.119213581085205 = 0.06895553320646286 + 1.0 * 6.050258159637451
Epoch 830, val loss: 0.8345890641212463
Epoch 840, training loss: 6.115830898284912 = 0.06612659990787506 + 1.0 * 6.049704074859619
Epoch 840, val loss: 0.840188205242157
Epoch 850, training loss: 6.121838569641113 = 0.06344019621610641 + 1.0 * 6.058398246765137
Epoch 850, val loss: 0.8457018733024597
Epoch 860, training loss: 6.114714622497559 = 0.06092089042067528 + 1.0 * 6.053793907165527
Epoch 860, val loss: 0.8510807752609253
Epoch 870, training loss: 6.1051225662231445 = 0.05854092910885811 + 1.0 * 6.046581745147705
Epoch 870, val loss: 0.8564730286598206
Epoch 880, training loss: 6.104236125946045 = 0.05627850070595741 + 1.0 * 6.047957420349121
Epoch 880, val loss: 0.8618925213813782
Epoch 890, training loss: 6.1024017333984375 = 0.054129090160131454 + 1.0 * 6.048272609710693
Epoch 890, val loss: 0.8672152757644653
Epoch 900, training loss: 6.101580619812012 = 0.05209038779139519 + 1.0 * 6.049490451812744
Epoch 900, val loss: 0.8724405765533447
Epoch 910, training loss: 6.096070766448975 = 0.05016176402568817 + 1.0 * 6.0459089279174805
Epoch 910, val loss: 0.8776136636734009
Epoch 920, training loss: 6.092250347137451 = 0.04833455756306648 + 1.0 * 6.043915748596191
Epoch 920, val loss: 0.8828004598617554
Epoch 930, training loss: 6.091720104217529 = 0.04658939316868782 + 1.0 * 6.045130729675293
Epoch 930, val loss: 0.8879569172859192
Epoch 940, training loss: 6.096285820007324 = 0.04492712765932083 + 1.0 * 6.051358699798584
Epoch 940, val loss: 0.8929598331451416
Epoch 950, training loss: 6.088615894317627 = 0.04336865246295929 + 1.0 * 6.0452470779418945
Epoch 950, val loss: 0.8978471755981445
Epoch 960, training loss: 6.082679271697998 = 0.0418844074010849 + 1.0 * 6.040794849395752
Epoch 960, val loss: 0.9028180241584778
Epoch 970, training loss: 6.080512046813965 = 0.040459878742694855 + 1.0 * 6.0400519371032715
Epoch 970, val loss: 0.9077568054199219
Epoch 980, training loss: 6.079930305480957 = 0.03909371793270111 + 1.0 * 6.040836811065674
Epoch 980, val loss: 0.9126278758049011
Epoch 990, training loss: 6.07906436920166 = 0.03779017552733421 + 1.0 * 6.041274070739746
Epoch 990, val loss: 0.9173447489738464
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
The final ASR:0.76753, 0.06116, Accuracy:0.80494, 0.02229
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10552])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97294, 0.00460, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.32101821899414 = 1.9473800659179688 + 1.0 * 8.373638153076172
Epoch 0, val loss: 1.9370646476745605
Epoch 10, training loss: 10.308581352233887 = 1.9360315799713135 + 1.0 * 8.372550010681152
Epoch 10, val loss: 1.9257930517196655
Epoch 20, training loss: 10.28714370727539 = 1.92144775390625 + 1.0 * 8.36569595336914
Epoch 20, val loss: 1.9111244678497314
Epoch 30, training loss: 10.217239379882812 = 1.9014067649841309 + 1.0 * 8.31583309173584
Epoch 30, val loss: 1.8914042711257935
Epoch 40, training loss: 9.696975708007812 = 1.880088448524475 + 1.0 * 7.816887378692627
Epoch 40, val loss: 1.8715553283691406
Epoch 50, training loss: 8.928581237792969 = 1.8606780767440796 + 1.0 * 7.0679030418396
Epoch 50, val loss: 1.854224681854248
Epoch 60, training loss: 8.55407428741455 = 1.8478933572769165 + 1.0 * 6.706180572509766
Epoch 60, val loss: 1.842637538909912
Epoch 70, training loss: 8.386909484863281 = 1.8374797105789185 + 1.0 * 6.549429893493652
Epoch 70, val loss: 1.8333122730255127
Epoch 80, training loss: 8.281051635742188 = 1.8270610570907593 + 1.0 * 6.453990936279297
Epoch 80, val loss: 1.824079155921936
Epoch 90, training loss: 8.1995210647583 = 1.8165262937545776 + 1.0 * 6.382995128631592
Epoch 90, val loss: 1.8151419162750244
Epoch 100, training loss: 8.136240005493164 = 1.8067164421081543 + 1.0 * 6.329523086547852
Epoch 100, val loss: 1.807239055633545
Epoch 110, training loss: 8.0865478515625 = 1.7974457740783691 + 1.0 * 6.289102077484131
Epoch 110, val loss: 1.7999091148376465
Epoch 120, training loss: 8.045967102050781 = 1.7880232334136963 + 1.0 * 6.257943630218506
Epoch 120, val loss: 1.7923904657363892
Epoch 130, training loss: 8.011106491088867 = 1.777909278869629 + 1.0 * 6.2331976890563965
Epoch 130, val loss: 1.7842923402786255
Epoch 140, training loss: 7.97960090637207 = 1.7665965557098389 + 1.0 * 6.213004112243652
Epoch 140, val loss: 1.775278091430664
Epoch 150, training loss: 7.950040817260742 = 1.7535487413406372 + 1.0 * 6.1964921951293945
Epoch 150, val loss: 1.7649672031402588
Epoch 160, training loss: 7.9212751388549805 = 1.7382808923721313 + 1.0 * 6.182994365692139
Epoch 160, val loss: 1.7529712915420532
Epoch 170, training loss: 7.8904852867126465 = 1.7201895713806152 + 1.0 * 6.170295715332031
Epoch 170, val loss: 1.738826036453247
Epoch 180, training loss: 7.858232021331787 = 1.6982465982437134 + 1.0 * 6.159985542297363
Epoch 180, val loss: 1.721625804901123
Epoch 190, training loss: 7.822229862213135 = 1.671223759651184 + 1.0 * 6.15100622177124
Epoch 190, val loss: 1.7003142833709717
Epoch 200, training loss: 7.782898426055908 = 1.6382495164871216 + 1.0 * 6.144649028778076
Epoch 200, val loss: 1.6743158102035522
Epoch 210, training loss: 7.735681533813477 = 1.5987482070922852 + 1.0 * 6.136933326721191
Epoch 210, val loss: 1.6430246829986572
Epoch 220, training loss: 7.683160781860352 = 1.5519402027130127 + 1.0 * 6.131220817565918
Epoch 220, val loss: 1.6058810949325562
Epoch 230, training loss: 7.624703407287598 = 1.4984534978866577 + 1.0 * 6.12624979019165
Epoch 230, val loss: 1.5636030435562134
Epoch 240, training loss: 7.564116477966309 = 1.439992904663086 + 1.0 * 6.124123573303223
Epoch 240, val loss: 1.517661690711975
Epoch 250, training loss: 7.499317169189453 = 1.3793423175811768 + 1.0 * 6.1199750900268555
Epoch 250, val loss: 1.4704300165176392
Epoch 260, training loss: 7.434440612792969 = 1.3182096481323242 + 1.0 * 6.1162309646606445
Epoch 260, val loss: 1.4236657619476318
Epoch 270, training loss: 7.3700385093688965 = 1.2582379579544067 + 1.0 * 6.111800670623779
Epoch 270, val loss: 1.3788198232650757
Epoch 280, training loss: 7.307709693908691 = 1.1996338367462158 + 1.0 * 6.108076095581055
Epoch 280, val loss: 1.3355859518051147
Epoch 290, training loss: 7.248661994934082 = 1.1429699659347534 + 1.0 * 6.105691909790039
Epoch 290, val loss: 1.2943668365478516
Epoch 300, training loss: 7.190326690673828 = 1.0890134572982788 + 1.0 * 6.10131311416626
Epoch 300, val loss: 1.2554017305374146
Epoch 310, training loss: 7.135501384735107 = 1.0375828742980957 + 1.0 * 6.097918510437012
Epoch 310, val loss: 1.2184045314788818
Epoch 320, training loss: 7.083096981048584 = 0.988233745098114 + 1.0 * 6.094863414764404
Epoch 320, val loss: 1.182990312576294
Epoch 330, training loss: 7.036365985870361 = 0.941358208656311 + 1.0 * 6.09500789642334
Epoch 330, val loss: 1.1495708227157593
Epoch 340, training loss: 6.9887003898620605 = 0.8973066210746765 + 1.0 * 6.091393947601318
Epoch 340, val loss: 1.1182819604873657
Epoch 350, training loss: 6.9425530433654785 = 0.8550384640693665 + 1.0 * 6.087514400482178
Epoch 350, val loss: 1.0881308317184448
Epoch 360, training loss: 6.903477191925049 = 0.8141270875930786 + 1.0 * 6.08935022354126
Epoch 360, val loss: 1.0589613914489746
Epoch 370, training loss: 6.858584403991699 = 0.7750240564346313 + 1.0 * 6.083560466766357
Epoch 370, val loss: 1.0311169624328613
Epoch 380, training loss: 6.819650173187256 = 0.737400233745575 + 1.0 * 6.082250118255615
Epoch 380, val loss: 1.0044275522232056
Epoch 390, training loss: 6.787242889404297 = 0.7009750604629517 + 1.0 * 6.086267948150635
Epoch 390, val loss: 0.9786489009857178
Epoch 400, training loss: 6.746279239654541 = 0.6664185523986816 + 1.0 * 6.079860687255859
Epoch 400, val loss: 0.9541592001914978
Epoch 410, training loss: 6.710844993591309 = 0.6335154175758362 + 1.0 * 6.077329635620117
Epoch 410, val loss: 0.9313870668411255
Epoch 420, training loss: 6.676585674285889 = 0.6019657850265503 + 1.0 * 6.074619770050049
Epoch 420, val loss: 0.9096934199333191
Epoch 430, training loss: 6.648429870605469 = 0.5718663334846497 + 1.0 * 6.076563358306885
Epoch 430, val loss: 0.8894740343093872
Epoch 440, training loss: 6.61652946472168 = 0.5434404611587524 + 1.0 * 6.073089122772217
Epoch 440, val loss: 0.8709854483604431
Epoch 450, training loss: 6.587862968444824 = 0.5164011716842651 + 1.0 * 6.0714616775512695
Epoch 450, val loss: 0.8541148900985718
Epoch 460, training loss: 6.5625410079956055 = 0.49082595109939575 + 1.0 * 6.071714878082275
Epoch 460, val loss: 0.8387464284896851
Epoch 470, training loss: 6.536795139312744 = 0.46663859486579895 + 1.0 * 6.070156574249268
Epoch 470, val loss: 0.8253016471862793
Epoch 480, training loss: 6.510310649871826 = 0.44367823004722595 + 1.0 * 6.066632270812988
Epoch 480, val loss: 0.8133378624916077
Epoch 490, training loss: 6.490256309509277 = 0.4218008816242218 + 1.0 * 6.068455219268799
Epoch 490, val loss: 0.8028172254562378
Epoch 500, training loss: 6.465786457061768 = 0.40100738406181335 + 1.0 * 6.064779281616211
Epoch 500, val loss: 0.7936141490936279
Epoch 510, training loss: 6.44366979598999 = 0.38106924295425415 + 1.0 * 6.062600612640381
Epoch 510, val loss: 0.785797119140625
Epoch 520, training loss: 6.423473358154297 = 0.3619707226753235 + 1.0 * 6.061502456665039
Epoch 520, val loss: 0.7790152430534363
Epoch 530, training loss: 6.405192852020264 = 0.3437466621398926 + 1.0 * 6.061446189880371
Epoch 530, val loss: 0.7734512686729431
Epoch 540, training loss: 6.385068893432617 = 0.3262213468551636 + 1.0 * 6.058847427368164
Epoch 540, val loss: 0.7690026760101318
Epoch 550, training loss: 6.368239879608154 = 0.3093377649784088 + 1.0 * 6.058902263641357
Epoch 550, val loss: 0.7653893828392029
Epoch 560, training loss: 6.349734783172607 = 0.293112188577652 + 1.0 * 6.056622505187988
Epoch 560, val loss: 0.7626315355300903
Epoch 570, training loss: 6.3365020751953125 = 0.277529776096344 + 1.0 * 6.058972358703613
Epoch 570, val loss: 0.7608329057693481
Epoch 580, training loss: 6.3156962394714355 = 0.26252850890159607 + 1.0 * 6.053167819976807
Epoch 580, val loss: 0.759829044342041
Epoch 590, training loss: 6.3037309646606445 = 0.2480568289756775 + 1.0 * 6.055674076080322
Epoch 590, val loss: 0.7595567107200623
Epoch 600, training loss: 6.288330078125 = 0.23423683643341064 + 1.0 * 6.054093360900879
Epoch 600, val loss: 0.7600582838058472
Epoch 610, training loss: 6.276547908782959 = 0.22111529111862183 + 1.0 * 6.0554327964782715
Epoch 610, val loss: 0.7612889409065247
Epoch 620, training loss: 6.259507179260254 = 0.20869939029216766 + 1.0 * 6.050807952880859
Epoch 620, val loss: 0.7633232474327087
Epoch 630, training loss: 6.24539852142334 = 0.19692127406597137 + 1.0 * 6.0484771728515625
Epoch 630, val loss: 0.7660500407218933
Epoch 640, training loss: 6.23928689956665 = 0.18577471375465393 + 1.0 * 6.053512096405029
Epoch 640, val loss: 0.7693220973014832
Epoch 650, training loss: 6.224777698516846 = 0.1753835529088974 + 1.0 * 6.049394130706787
Epoch 650, val loss: 0.7732009291648865
Epoch 660, training loss: 6.211755752563477 = 0.16565226018428802 + 1.0 * 6.046103477478027
Epoch 660, val loss: 0.7777862548828125
Epoch 670, training loss: 6.205516338348389 = 0.15653328597545624 + 1.0 * 6.048983097076416
Epoch 670, val loss: 0.7828414440155029
Epoch 680, training loss: 6.196732997894287 = 0.1480664610862732 + 1.0 * 6.048666477203369
Epoch 680, val loss: 0.7882553935050964
Epoch 690, training loss: 6.183698654174805 = 0.14019805192947388 + 1.0 * 6.0435004234313965
Epoch 690, val loss: 0.7943083643913269
Epoch 700, training loss: 6.175387859344482 = 0.1328209638595581 + 1.0 * 6.042566776275635
Epoch 700, val loss: 0.8006289601325989
Epoch 710, training loss: 6.177703857421875 = 0.125931054353714 + 1.0 * 6.051772594451904
Epoch 710, val loss: 0.807300329208374
Epoch 720, training loss: 6.163914680480957 = 0.11955026537179947 + 1.0 * 6.0443644523620605
Epoch 720, val loss: 0.8140537142753601
Epoch 730, training loss: 6.156771659851074 = 0.11360476911067963 + 1.0 * 6.0431671142578125
Epoch 730, val loss: 0.821172297000885
Epoch 740, training loss: 6.147397041320801 = 0.10805349797010422 + 1.0 * 6.039343357086182
Epoch 740, val loss: 0.8284107446670532
Epoch 750, training loss: 6.141633987426758 = 0.10284552723169327 + 1.0 * 6.038788318634033
Epoch 750, val loss: 0.835868239402771
Epoch 760, training loss: 6.140644550323486 = 0.09795881062746048 + 1.0 * 6.042685508728027
Epoch 760, val loss: 0.8433663845062256
Epoch 770, training loss: 6.132198333740234 = 0.09338092058897018 + 1.0 * 6.038817405700684
Epoch 770, val loss: 0.8509871363639832
Epoch 780, training loss: 6.1284003257751465 = 0.08907576650381088 + 1.0 * 6.039324760437012
Epoch 780, val loss: 0.8587056994438171
Epoch 790, training loss: 6.122847080230713 = 0.08504410833120346 + 1.0 * 6.0378031730651855
Epoch 790, val loss: 0.8663069009780884
Epoch 800, training loss: 6.115168571472168 = 0.08126730471849442 + 1.0 * 6.033901214599609
Epoch 800, val loss: 0.8741087317466736
Epoch 810, training loss: 6.111672878265381 = 0.07770532369613647 + 1.0 * 6.0339674949646
Epoch 810, val loss: 0.8819302320480347
Epoch 820, training loss: 6.110518455505371 = 0.07433906197547913 + 1.0 * 6.036179542541504
Epoch 820, val loss: 0.8896816968917847
Epoch 830, training loss: 6.113451957702637 = 0.07116997987031937 + 1.0 * 6.0422821044921875
Epoch 830, val loss: 0.8973222374916077
Epoch 840, training loss: 6.102311611175537 = 0.06820370256900787 + 1.0 * 6.034107685089111
Epoch 840, val loss: 0.9050241112709045
Epoch 850, training loss: 6.096316814422607 = 0.06540589034557343 + 1.0 * 6.030910968780518
Epoch 850, val loss: 0.9128040671348572
Epoch 860, training loss: 6.093002796173096 = 0.06274721771478653 + 1.0 * 6.0302557945251465
Epoch 860, val loss: 0.9204496741294861
Epoch 870, training loss: 6.096432209014893 = 0.060228243470191956 + 1.0 * 6.036203861236572
Epoch 870, val loss: 0.9280820488929749
Epoch 880, training loss: 6.093069553375244 = 0.05785426124930382 + 1.0 * 6.035215377807617
Epoch 880, val loss: 0.9355370998382568
Epoch 890, training loss: 6.083787441253662 = 0.05561630055308342 + 1.0 * 6.028171062469482
Epoch 890, val loss: 0.9430185556411743
Epoch 900, training loss: 6.081202983856201 = 0.05349745601415634 + 1.0 * 6.027705669403076
Epoch 900, val loss: 0.950499951839447
Epoch 910, training loss: 6.079073429107666 = 0.05148143321275711 + 1.0 * 6.027592182159424
Epoch 910, val loss: 0.9578399658203125
Epoch 920, training loss: 6.079945087432861 = 0.049567580223083496 + 1.0 * 6.030377388000488
Epoch 920, val loss: 0.9650176167488098
Epoch 930, training loss: 6.075342178344727 = 0.047758087515830994 + 1.0 * 6.027584075927734
Epoch 930, val loss: 0.9721729159355164
Epoch 940, training loss: 6.075179100036621 = 0.04604584723711014 + 1.0 * 6.029133319854736
Epoch 940, val loss: 0.9791457653045654
Epoch 950, training loss: 6.069995403289795 = 0.04442537948489189 + 1.0 * 6.025569915771484
Epoch 950, val loss: 0.9861096143722534
Epoch 960, training loss: 6.066858768463135 = 0.04288063570857048 + 1.0 * 6.023978233337402
Epoch 960, val loss: 0.9930773973464966
Epoch 970, training loss: 6.0668206214904785 = 0.04140211641788483 + 1.0 * 6.025418281555176
Epoch 970, val loss: 0.9998537302017212
Epoch 980, training loss: 6.063460350036621 = 0.03999781236052513 + 1.0 * 6.023462772369385
Epoch 980, val loss: 1.0064698457717896
Epoch 990, training loss: 6.062102317810059 = 0.038665663450956345 + 1.0 * 6.023436546325684
Epoch 990, val loss: 1.01321280002594
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6679
Flip ASR: 0.6044/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.325023651123047 = 1.9512689113616943 + 1.0 * 8.373754501342773
Epoch 0, val loss: 1.9568047523498535
Epoch 10, training loss: 10.313061714172363 = 1.9400269985198975 + 1.0 * 8.373034477233887
Epoch 10, val loss: 1.9444105625152588
Epoch 20, training loss: 10.294560432434082 = 1.9256938695907593 + 1.0 * 8.368866920471191
Epoch 20, val loss: 1.9284024238586426
Epoch 30, training loss: 10.248164176940918 = 1.905714988708496 + 1.0 * 8.342449188232422
Epoch 30, val loss: 1.9062401056289673
Epoch 40, training loss: 10.05141830444336 = 1.8820226192474365 + 1.0 * 8.169395446777344
Epoch 40, val loss: 1.8816837072372437
Epoch 50, training loss: 9.491771697998047 = 1.8567548990249634 + 1.0 * 7.635016918182373
Epoch 50, val loss: 1.855897068977356
Epoch 60, training loss: 9.096409797668457 = 1.8370769023895264 + 1.0 * 7.259332656860352
Epoch 60, val loss: 1.8378355503082275
Epoch 70, training loss: 8.697224617004395 = 1.8244359493255615 + 1.0 * 6.872788429260254
Epoch 70, val loss: 1.8252599239349365
Epoch 80, training loss: 8.475142478942871 = 1.8125051259994507 + 1.0 * 6.662637233734131
Epoch 80, val loss: 1.8126236200332642
Epoch 90, training loss: 8.332267761230469 = 1.7999345064163208 + 1.0 * 6.532332897186279
Epoch 90, val loss: 1.7989305257797241
Epoch 100, training loss: 8.227009773254395 = 1.7880265712738037 + 1.0 * 6.43898344039917
Epoch 100, val loss: 1.7857427597045898
Epoch 110, training loss: 8.158985137939453 = 1.776691198348999 + 1.0 * 6.382294178009033
Epoch 110, val loss: 1.7729812860488892
Epoch 120, training loss: 8.108461380004883 = 1.7646218538284302 + 1.0 * 6.343839168548584
Epoch 120, val loss: 1.7598673105239868
Epoch 130, training loss: 8.063681602478027 = 1.751103401184082 + 1.0 * 6.312578201293945
Epoch 130, val loss: 1.746132493019104
Epoch 140, training loss: 8.023539543151855 = 1.7360011339187622 + 1.0 * 6.287538051605225
Epoch 140, val loss: 1.7317761182785034
Epoch 150, training loss: 7.98717737197876 = 1.7188912630081177 + 1.0 * 6.268286228179932
Epoch 150, val loss: 1.716422200202942
Epoch 160, training loss: 7.949678421020508 = 1.6990878582000732 + 1.0 * 6.2505903244018555
Epoch 160, val loss: 1.6992509365081787
Epoch 170, training loss: 7.910946846008301 = 1.6753532886505127 + 1.0 * 6.235593318939209
Epoch 170, val loss: 1.6792831420898438
Epoch 180, training loss: 7.86883544921875 = 1.646558403968811 + 1.0 * 6.2222771644592285
Epoch 180, val loss: 1.6555231809616089
Epoch 190, training loss: 7.823536396026611 = 1.6115937232971191 + 1.0 * 6.211942672729492
Epoch 190, val loss: 1.6270033121109009
Epoch 200, training loss: 7.77128267288208 = 1.5702919960021973 + 1.0 * 6.200990676879883
Epoch 200, val loss: 1.5936944484710693
Epoch 210, training loss: 7.713648796081543 = 1.5222402811050415 + 1.0 * 6.191408634185791
Epoch 210, val loss: 1.5552642345428467
Epoch 220, training loss: 7.651902198791504 = 1.4685149192810059 + 1.0 * 6.183387279510498
Epoch 220, val loss: 1.5128633975982666
Epoch 230, training loss: 7.589237689971924 = 1.4117761850357056 + 1.0 * 6.177461624145508
Epoch 230, val loss: 1.4686453342437744
Epoch 240, training loss: 7.524928092956543 = 1.354225516319275 + 1.0 * 6.1707024574279785
Epoch 240, val loss: 1.424415946006775
Epoch 250, training loss: 7.4605021476745605 = 1.2973040342330933 + 1.0 * 6.163197994232178
Epoch 250, val loss: 1.3813118934631348
Epoch 260, training loss: 7.398799896240234 = 1.2413456439971924 + 1.0 * 6.157454490661621
Epoch 260, val loss: 1.3394880294799805
Epoch 270, training loss: 7.340766429901123 = 1.1872196197509766 + 1.0 * 6.1535468101501465
Epoch 270, val loss: 1.2994505167007446
Epoch 280, training loss: 7.2837300300598145 = 1.1366958618164062 + 1.0 * 6.147034168243408
Epoch 280, val loss: 1.2620041370391846
Epoch 290, training loss: 7.230326175689697 = 1.0888673067092896 + 1.0 * 6.141458988189697
Epoch 290, val loss: 1.2266520261764526
Epoch 300, training loss: 7.183008193969727 = 1.0435869693756104 + 1.0 * 6.139421463012695
Epoch 300, val loss: 1.1930351257324219
Epoch 310, training loss: 7.135082244873047 = 1.0014821290969849 + 1.0 * 6.133600234985352
Epoch 310, val loss: 1.162204384803772
Epoch 320, training loss: 7.0917181968688965 = 0.9622793793678284 + 1.0 * 6.129438877105713
Epoch 320, val loss: 1.1336230039596558
Epoch 330, training loss: 7.050496578216553 = 0.924873411655426 + 1.0 * 6.1256232261657715
Epoch 330, val loss: 1.1064625978469849
Epoch 340, training loss: 7.014039516448975 = 0.8888400197029114 + 1.0 * 6.125199317932129
Epoch 340, val loss: 1.0805004835128784
Epoch 350, training loss: 6.973804473876953 = 0.853521466255188 + 1.0 * 6.120283126831055
Epoch 350, val loss: 1.0553574562072754
Epoch 360, training loss: 6.9336652755737305 = 0.8181583285331726 + 1.0 * 6.115507125854492
Epoch 360, val loss: 1.0301257371902466
Epoch 370, training loss: 6.89478874206543 = 0.782227098941803 + 1.0 * 6.1125617027282715
Epoch 370, val loss: 1.0047523975372314
Epoch 380, training loss: 6.85597562789917 = 0.7458092570304871 + 1.0 * 6.110166549682617
Epoch 380, val loss: 0.9790740013122559
Epoch 390, training loss: 6.81779670715332 = 0.7095110416412354 + 1.0 * 6.108285427093506
Epoch 390, val loss: 0.9536334872245789
Epoch 400, training loss: 6.7788615226745605 = 0.6737862825393677 + 1.0 * 6.105075359344482
Epoch 400, val loss: 0.9288344383239746
Epoch 410, training loss: 6.742191314697266 = 0.639150083065033 + 1.0 * 6.103041172027588
Epoch 410, val loss: 0.9050846099853516
Epoch 420, training loss: 6.708652496337891 = 0.6060535907745361 + 1.0 * 6.102598667144775
Epoch 420, val loss: 0.8827701807022095
Epoch 430, training loss: 6.6733598709106445 = 0.5748054385185242 + 1.0 * 6.098554611206055
Epoch 430, val loss: 0.8621976971626282
Epoch 440, training loss: 6.641176700592041 = 0.5451895594596863 + 1.0 * 6.095987319946289
Epoch 440, val loss: 0.8433037996292114
Epoch 450, training loss: 6.616168022155762 = 0.517121434211731 + 1.0 * 6.09904670715332
Epoch 450, val loss: 0.8262507915496826
Epoch 460, training loss: 6.585545063018799 = 0.49102407693862915 + 1.0 * 6.0945210456848145
Epoch 460, val loss: 0.8111462593078613
Epoch 470, training loss: 6.558633804321289 = 0.46657219529151917 + 1.0 * 6.092061519622803
Epoch 470, val loss: 0.7980591654777527
Epoch 480, training loss: 6.5310869216918945 = 0.44330844283103943 + 1.0 * 6.087778568267822
Epoch 480, val loss: 0.7865520119667053
Epoch 490, training loss: 6.5120849609375 = 0.42111119627952576 + 1.0 * 6.090973854064941
Epoch 490, val loss: 0.7763683795928955
Epoch 500, training loss: 6.487832546234131 = 0.40005064010620117 + 1.0 * 6.08778190612793
Epoch 500, val loss: 0.7675217390060425
Epoch 510, training loss: 6.463554382324219 = 0.38009950518608093 + 1.0 * 6.0834550857543945
Epoch 510, val loss: 0.7600690722465515
Epoch 520, training loss: 6.444119930267334 = 0.36102819442749023 + 1.0 * 6.083091735839844
Epoch 520, val loss: 0.7536739706993103
Epoch 530, training loss: 6.429238796234131 = 0.34265315532684326 + 1.0 * 6.086585521697998
Epoch 530, val loss: 0.7481678128242493
Epoch 540, training loss: 6.406324863433838 = 0.32497966289520264 + 1.0 * 6.081345081329346
Epoch 540, val loss: 0.7435151934623718
Epoch 550, training loss: 6.389362812042236 = 0.30804479122161865 + 1.0 * 6.081317901611328
Epoch 550, val loss: 0.7399088144302368
Epoch 560, training loss: 6.369693279266357 = 0.29181385040283203 + 1.0 * 6.077879428863525
Epoch 560, val loss: 0.7371484041213989
Epoch 570, training loss: 6.353844165802002 = 0.2762351632118225 + 1.0 * 6.077609062194824
Epoch 570, val loss: 0.7351621389389038
Epoch 580, training loss: 6.337307929992676 = 0.2614055275917053 + 1.0 * 6.075902462005615
Epoch 580, val loss: 0.7339638471603394
Epoch 590, training loss: 6.320801258087158 = 0.24722976982593536 + 1.0 * 6.073571681976318
Epoch 590, val loss: 0.7335326671600342
Epoch 600, training loss: 6.317761421203613 = 0.23373425006866455 + 1.0 * 6.084027290344238
Epoch 600, val loss: 0.7338663935661316
Epoch 610, training loss: 6.292124271392822 = 0.22102457284927368 + 1.0 * 6.071099758148193
Epoch 610, val loss: 0.7348735928535461
Epoch 620, training loss: 6.27886962890625 = 0.20902341604232788 + 1.0 * 6.069846153259277
Epoch 620, val loss: 0.7366729974746704
Epoch 630, training loss: 6.267817497253418 = 0.19766056537628174 + 1.0 * 6.070157051086426
Epoch 630, val loss: 0.7391533851623535
Epoch 640, training loss: 6.254977703094482 = 0.18694491684436798 + 1.0 * 6.068032741546631
Epoch 640, val loss: 0.7421996593475342
Epoch 650, training loss: 6.2428412437438965 = 0.17687828838825226 + 1.0 * 6.065962791442871
Epoch 650, val loss: 0.745765745639801
Epoch 660, training loss: 6.23426628112793 = 0.16741317510604858 + 1.0 * 6.066853046417236
Epoch 660, val loss: 0.7498859167098999
Epoch 670, training loss: 6.222187519073486 = 0.15841999650001526 + 1.0 * 6.063767433166504
Epoch 670, val loss: 0.7545149922370911
Epoch 680, training loss: 6.223608493804932 = 0.1499253660440445 + 1.0 * 6.073683261871338
Epoch 680, val loss: 0.7596336007118225
Epoch 690, training loss: 6.203585624694824 = 0.1419600248336792 + 1.0 * 6.0616254806518555
Epoch 690, val loss: 0.7650798559188843
Epoch 700, training loss: 6.196110725402832 = 0.13444393873214722 + 1.0 * 6.061666965484619
Epoch 700, val loss: 0.7709781527519226
Epoch 710, training loss: 6.194097995758057 = 0.127370223402977 + 1.0 * 6.066727638244629
Epoch 710, val loss: 0.7772257328033447
Epoch 720, training loss: 6.183787822723389 = 0.12075740098953247 + 1.0 * 6.063030242919922
Epoch 720, val loss: 0.7836963534355164
Epoch 730, training loss: 6.1722612380981445 = 0.11451135575771332 + 1.0 * 6.0577497482299805
Epoch 730, val loss: 0.7904314994812012
Epoch 740, training loss: 6.168083190917969 = 0.10862720012664795 + 1.0 * 6.059455871582031
Epoch 740, val loss: 0.7974529266357422
Epoch 750, training loss: 6.160967826843262 = 0.10309098660945892 + 1.0 * 6.057877063751221
Epoch 750, val loss: 0.8047089576721191
Epoch 760, training loss: 6.154250144958496 = 0.09789256006479263 + 1.0 * 6.056357383728027
Epoch 760, val loss: 0.8121871948242188
Epoch 770, training loss: 6.153046607971191 = 0.093035027384758 + 1.0 * 6.060011386871338
Epoch 770, val loss: 0.8198726177215576
Epoch 780, training loss: 6.143265724182129 = 0.08848689496517181 + 1.0 * 6.054779052734375
Epoch 780, val loss: 0.8276125192642212
Epoch 790, training loss: 6.136081695556641 = 0.08422272652387619 + 1.0 * 6.051858901977539
Epoch 790, val loss: 0.8355003595352173
Epoch 800, training loss: 6.132741928100586 = 0.08019889146089554 + 1.0 * 6.0525431632995605
Epoch 800, val loss: 0.843472957611084
Epoch 810, training loss: 6.130232334136963 = 0.0764218419790268 + 1.0 * 6.0538105964660645
Epoch 810, val loss: 0.8514736294746399
Epoch 820, training loss: 6.127321720123291 = 0.07289399206638336 + 1.0 * 6.054427623748779
Epoch 820, val loss: 0.8594827055931091
Epoch 830, training loss: 6.119868278503418 = 0.0695807933807373 + 1.0 * 6.050287246704102
Epoch 830, val loss: 0.8675228953361511
Epoch 840, training loss: 6.11611795425415 = 0.06645725667476654 + 1.0 * 6.049660682678223
Epoch 840, val loss: 0.8756384253501892
Epoch 850, training loss: 6.111389636993408 = 0.06351420283317566 + 1.0 * 6.04787540435791
Epoch 850, val loss: 0.8836559653282166
Epoch 860, training loss: 6.108255386352539 = 0.06075228005647659 + 1.0 * 6.0475029945373535
Epoch 860, val loss: 0.8917015194892883
Epoch 870, training loss: 6.113706588745117 = 0.05813809111714363 + 1.0 * 6.055568695068359
Epoch 870, val loss: 0.8997263312339783
Epoch 880, training loss: 6.10323429107666 = 0.05569000914692879 + 1.0 * 6.047544479370117
Epoch 880, val loss: 0.9076927900314331
Epoch 890, training loss: 6.097136974334717 = 0.05336897820234299 + 1.0 * 6.043767929077148
Epoch 890, val loss: 0.9157074689865112
Epoch 900, training loss: 6.094869613647461 = 0.05116651952266693 + 1.0 * 6.043703079223633
Epoch 900, val loss: 0.9237327575683594
Epoch 910, training loss: 6.09735631942749 = 0.04908426105976105 + 1.0 * 6.048272132873535
Epoch 910, val loss: 0.931688666343689
Epoch 920, training loss: 6.092505931854248 = 0.047137048095464706 + 1.0 * 6.045368671417236
Epoch 920, val loss: 0.9394568800926208
Epoch 930, training loss: 6.086104393005371 = 0.045304328203201294 + 1.0 * 6.040800094604492
Epoch 930, val loss: 0.9472545981407166
Epoch 940, training loss: 6.08378267288208 = 0.04356173798441887 + 1.0 * 6.040220737457275
Epoch 940, val loss: 0.9549872279167175
Epoch 950, training loss: 6.090157985687256 = 0.04190405085682869 + 1.0 * 6.048254013061523
Epoch 950, val loss: 0.9625989198684692
Epoch 960, training loss: 6.084440231323242 = 0.04034089297056198 + 1.0 * 6.0440993309021
Epoch 960, val loss: 0.9700548052787781
Epoch 970, training loss: 6.0803141593933105 = 0.038862135261297226 + 1.0 * 6.041451930999756
Epoch 970, val loss: 0.9773786664009094
Epoch 980, training loss: 6.076047420501709 = 0.037460774183273315 + 1.0 * 6.038586616516113
Epoch 980, val loss: 0.9846510291099548
Epoch 990, training loss: 6.073570251464844 = 0.03612672910094261 + 1.0 * 6.0374436378479
Epoch 990, val loss: 0.99189293384552
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8081
Flip ASR: 0.7822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.318217277526855 = 1.944493055343628 + 1.0 * 8.373723983764648
Epoch 0, val loss: 1.9508531093597412
Epoch 10, training loss: 10.307126998901367 = 1.9341741800308228 + 1.0 * 8.372952461242676
Epoch 10, val loss: 1.9402332305908203
Epoch 20, training loss: 10.289667129516602 = 1.9210842847824097 + 1.0 * 8.368582725524902
Epoch 20, val loss: 1.92660391330719
Epoch 30, training loss: 10.24510383605957 = 1.9027506113052368 + 1.0 * 8.342352867126465
Epoch 30, val loss: 1.9075098037719727
Epoch 40, training loss: 10.04836654663086 = 1.8799753189086914 + 1.0 * 8.168391227722168
Epoch 40, val loss: 1.8846334218978882
Epoch 50, training loss: 9.341167449951172 = 1.8550760746002197 + 1.0 * 7.486091613769531
Epoch 50, val loss: 1.859537124633789
Epoch 60, training loss: 8.959779739379883 = 1.835567593574524 + 1.0 * 7.12421178817749
Epoch 60, val loss: 1.8413711786270142
Epoch 70, training loss: 8.629096984863281 = 1.821189284324646 + 1.0 * 6.807907581329346
Epoch 70, val loss: 1.8275455236434937
Epoch 80, training loss: 8.435955047607422 = 1.8083618879318237 + 1.0 * 6.627593040466309
Epoch 80, val loss: 1.8148044347763062
Epoch 90, training loss: 8.320066452026367 = 1.7958436012268066 + 1.0 * 6.5242228507995605
Epoch 90, val loss: 1.8018940687179565
Epoch 100, training loss: 8.240455627441406 = 1.7835047245025635 + 1.0 * 6.456950664520264
Epoch 100, val loss: 1.7892545461654663
Epoch 110, training loss: 8.180668830871582 = 1.7714744806289673 + 1.0 * 6.409193992614746
Epoch 110, val loss: 1.7770596742630005
Epoch 120, training loss: 8.129932403564453 = 1.7589737176895142 + 1.0 * 6.3709588050842285
Epoch 120, val loss: 1.764673113822937
Epoch 130, training loss: 8.083189964294434 = 1.7450172901153564 + 1.0 * 6.338172435760498
Epoch 130, val loss: 1.7514920234680176
Epoch 140, training loss: 8.037128448486328 = 1.7292356491088867 + 1.0 * 6.307892799377441
Epoch 140, val loss: 1.7375158071517944
Epoch 150, training loss: 7.993855953216553 = 1.7107582092285156 + 1.0 * 6.283097743988037
Epoch 150, val loss: 1.7218446731567383
Epoch 160, training loss: 7.950691223144531 = 1.6881482601165771 + 1.0 * 6.262543201446533
Epoch 160, val loss: 1.7031837701797485
Epoch 170, training loss: 7.905551910400391 = 1.6601225137710571 + 1.0 * 6.245429515838623
Epoch 170, val loss: 1.6803706884384155
Epoch 180, training loss: 7.855400085449219 = 1.625524640083313 + 1.0 * 6.229875564575195
Epoch 180, val loss: 1.6520966291427612
Epoch 190, training loss: 7.800652503967285 = 1.5828500986099243 + 1.0 * 6.21780252456665
Epoch 190, val loss: 1.616990089416504
Epoch 200, training loss: 7.73860502243042 = 1.5319503545761108 + 1.0 * 6.2066545486450195
Epoch 200, val loss: 1.5748875141143799
Epoch 210, training loss: 7.670851707458496 = 1.4730327129364014 + 1.0 * 6.197819232940674
Epoch 210, val loss: 1.5261777639389038
Epoch 220, training loss: 7.600618839263916 = 1.4076327085494995 + 1.0 * 6.192986011505127
Epoch 220, val loss: 1.4725970029830933
Epoch 230, training loss: 7.526969909667969 = 1.3414281606674194 + 1.0 * 6.18554162979126
Epoch 230, val loss: 1.4197231531143188
Epoch 240, training loss: 7.454715728759766 = 1.2761270999908447 + 1.0 * 6.1785888671875
Epoch 240, val loss: 1.3683199882507324
Epoch 250, training loss: 7.3850884437561035 = 1.2123252153396606 + 1.0 * 6.172763347625732
Epoch 250, val loss: 1.3187322616577148
Epoch 260, training loss: 7.317704200744629 = 1.1508270502090454 + 1.0 * 6.166877269744873
Epoch 260, val loss: 1.2720069885253906
Epoch 270, training loss: 7.256961822509766 = 1.092775583267212 + 1.0 * 6.164186477661133
Epoch 270, val loss: 1.2286862134933472
Epoch 280, training loss: 7.195619106292725 = 1.0389070510864258 + 1.0 * 6.156712055206299
Epoch 280, val loss: 1.1888883113861084
Epoch 290, training loss: 7.149709224700928 = 0.9883454442024231 + 1.0 * 6.16136360168457
Epoch 290, val loss: 1.1517693996429443
Epoch 300, training loss: 7.090831756591797 = 0.9424415230751038 + 1.0 * 6.148390293121338
Epoch 300, val loss: 1.1183853149414062
Epoch 310, training loss: 7.043170928955078 = 0.8996149301528931 + 1.0 * 6.143556118011475
Epoch 310, val loss: 1.0873686075210571
Epoch 320, training loss: 6.998085021972656 = 0.8586931824684143 + 1.0 * 6.139391899108887
Epoch 320, val loss: 1.0578837394714355
Epoch 330, training loss: 6.954272270202637 = 0.8191425800323486 + 1.0 * 6.135129451751709
Epoch 330, val loss: 1.0294742584228516
Epoch 340, training loss: 6.922041893005371 = 0.7806986570358276 + 1.0 * 6.141343116760254
Epoch 340, val loss: 1.0019655227661133
Epoch 350, training loss: 6.873720645904541 = 0.7441256642341614 + 1.0 * 6.129594802856445
Epoch 350, val loss: 0.9759230017662048
Epoch 360, training loss: 6.834348201751709 = 0.7086841464042664 + 1.0 * 6.125664234161377
Epoch 360, val loss: 0.9510424733161926
Epoch 370, training loss: 6.796546459197998 = 0.6741053462028503 + 1.0 * 6.122441291809082
Epoch 370, val loss: 0.9269346594810486
Epoch 380, training loss: 6.766556739807129 = 0.640588641166687 + 1.0 * 6.125967979431152
Epoch 380, val loss: 0.9038367867469788
Epoch 390, training loss: 6.728528022766113 = 0.6089935302734375 + 1.0 * 6.119534492492676
Epoch 390, val loss: 0.8825660347938538
Epoch 400, training loss: 6.693679332733154 = 0.5788722634315491 + 1.0 * 6.11480712890625
Epoch 400, val loss: 0.862910270690918
Epoch 410, training loss: 6.661861896514893 = 0.5500479340553284 + 1.0 * 6.111814022064209
Epoch 410, val loss: 0.8443777561187744
Epoch 420, training loss: 6.640799045562744 = 0.5226815342903137 + 1.0 * 6.118117332458496
Epoch 420, val loss: 0.8274354934692383
Epoch 430, training loss: 6.6060895919799805 = 0.4970158636569977 + 1.0 * 6.109073638916016
Epoch 430, val loss: 0.8121793270111084
Epoch 440, training loss: 6.577484130859375 = 0.47273126244544983 + 1.0 * 6.104753017425537
Epoch 440, val loss: 0.7985740303993225
Epoch 450, training loss: 6.554256439208984 = 0.4495905637741089 + 1.0 * 6.104665756225586
Epoch 450, val loss: 0.7862130403518677
Epoch 460, training loss: 6.528295993804932 = 0.4275713860988617 + 1.0 * 6.100724697113037
Epoch 460, val loss: 0.7751191258430481
Epoch 470, training loss: 6.50437593460083 = 0.4065212309360504 + 1.0 * 6.0978546142578125
Epoch 470, val loss: 0.7654404640197754
Epoch 480, training loss: 6.485439300537109 = 0.38619229197502136 + 1.0 * 6.099246978759766
Epoch 480, val loss: 0.7566991448402405
Epoch 490, training loss: 6.460675239562988 = 0.36676138639450073 + 1.0 * 6.093914031982422
Epoch 490, val loss: 0.7490224242210388
Epoch 500, training loss: 6.439873695373535 = 0.34810927510261536 + 1.0 * 6.091764450073242
Epoch 500, val loss: 0.7426358461380005
Epoch 510, training loss: 6.422203540802002 = 0.33010661602020264 + 1.0 * 6.09209680557251
Epoch 510, val loss: 0.7371307015419006
Epoch 520, training loss: 6.402328014373779 = 0.3128376603126526 + 1.0 * 6.0894904136657715
Epoch 520, val loss: 0.7326406836509705
Epoch 530, training loss: 6.384594917297363 = 0.29645293951034546 + 1.0 * 6.088141918182373
Epoch 530, val loss: 0.7292285561561584
Epoch 540, training loss: 6.366736888885498 = 0.2808319330215454 + 1.0 * 6.085905075073242
Epoch 540, val loss: 0.7267189025878906
Epoch 550, training loss: 6.348030090332031 = 0.2659108638763428 + 1.0 * 6.082119464874268
Epoch 550, val loss: 0.7249720692634583
Epoch 560, training loss: 6.337036609649658 = 0.2516173720359802 + 1.0 * 6.085419178009033
Epoch 560, val loss: 0.7237262725830078
Epoch 570, training loss: 6.31998348236084 = 0.23806583881378174 + 1.0 * 6.081917762756348
Epoch 570, val loss: 0.7229757905006409
Epoch 580, training loss: 6.307679653167725 = 0.22519908845424652 + 1.0 * 6.082480430603027
Epoch 580, val loss: 0.7227755784988403
Epoch 590, training loss: 6.289610385894775 = 0.21298807859420776 + 1.0 * 6.076622486114502
Epoch 590, val loss: 0.7229010462760925
Epoch 600, training loss: 6.276291370391846 = 0.20129495859146118 + 1.0 * 6.074996471405029
Epoch 600, val loss: 0.7233697175979614
Epoch 610, training loss: 6.27168083190918 = 0.19011105597019196 + 1.0 * 6.081569671630859
Epoch 610, val loss: 0.724001944065094
Epoch 620, training loss: 6.252251625061035 = 0.17946167290210724 + 1.0 * 6.072790145874023
Epoch 620, val loss: 0.7249030470848083
Epoch 630, training loss: 6.239193439483643 = 0.16934117674827576 + 1.0 * 6.069852352142334
Epoch 630, val loss: 0.7262012958526611
Epoch 640, training loss: 6.231318473815918 = 0.1597079038619995 + 1.0 * 6.071610450744629
Epoch 640, val loss: 0.7275629043579102
Epoch 650, training loss: 6.225837707519531 = 0.1506335288286209 + 1.0 * 6.075204372406006
Epoch 650, val loss: 0.728931188583374
Epoch 660, training loss: 6.211052894592285 = 0.14213919639587402 + 1.0 * 6.068913459777832
Epoch 660, val loss: 0.7308638095855713
Epoch 670, training loss: 6.201108932495117 = 0.13416315615177155 + 1.0 * 6.066945552825928
Epoch 670, val loss: 0.7328041791915894
Epoch 680, training loss: 6.192855358123779 = 0.12668974697589874 + 1.0 * 6.066165447235107
Epoch 680, val loss: 0.7349162101745605
Epoch 690, training loss: 6.183625221252441 = 0.11970651149749756 + 1.0 * 6.063918590545654
Epoch 690, val loss: 0.7374364137649536
Epoch 700, training loss: 6.1747636795043945 = 0.11318789422512054 + 1.0 * 6.061575889587402
Epoch 700, val loss: 0.7399043440818787
Epoch 710, training loss: 6.166991233825684 = 0.10712213814258575 + 1.0 * 6.059869289398193
Epoch 710, val loss: 0.7427331805229187
Epoch 720, training loss: 6.164478302001953 = 0.10146978497505188 + 1.0 * 6.0630083084106445
Epoch 720, val loss: 0.7456679940223694
Epoch 730, training loss: 6.15484094619751 = 0.09622940421104431 + 1.0 * 6.0586113929748535
Epoch 730, val loss: 0.7487108707427979
Epoch 740, training loss: 6.1492695808410645 = 0.09134382754564285 + 1.0 * 6.057925701141357
Epoch 740, val loss: 0.752078115940094
Epoch 750, training loss: 6.150238037109375 = 0.08679220825433731 + 1.0 * 6.063446044921875
Epoch 750, val loss: 0.7553909420967102
Epoch 760, training loss: 6.138086318969727 = 0.08255662024021149 + 1.0 * 6.055529594421387
Epoch 760, val loss: 0.7588179111480713
Epoch 770, training loss: 6.133385181427002 = 0.07861315459012985 + 1.0 * 6.054771900177002
Epoch 770, val loss: 0.7625033855438232
Epoch 780, training loss: 6.128255844116211 = 0.07492904365062714 + 1.0 * 6.053326606750488
Epoch 780, val loss: 0.7661563754081726
Epoch 790, training loss: 6.126528739929199 = 0.07148360460996628 + 1.0 * 6.055045127868652
Epoch 790, val loss: 0.7698704600334167
Epoch 800, training loss: 6.119711875915527 = 0.06826505064964294 + 1.0 * 6.051446914672852
Epoch 800, val loss: 0.7735117673873901
Epoch 810, training loss: 6.114792346954346 = 0.06526114046573639 + 1.0 * 6.049530982971191
Epoch 810, val loss: 0.7773740887641907
Epoch 820, training loss: 6.111060619354248 = 0.062442004680633545 + 1.0 * 6.048618793487549
Epoch 820, val loss: 0.7811980843544006
Epoch 830, training loss: 6.109185218811035 = 0.05978979170322418 + 1.0 * 6.049395561218262
Epoch 830, val loss: 0.7849994897842407
Epoch 840, training loss: 6.1081671714782715 = 0.05729532241821289 + 1.0 * 6.050871849060059
Epoch 840, val loss: 0.7888490557670593
Epoch 850, training loss: 6.103518962860107 = 0.05495300143957138 + 1.0 * 6.048565864562988
Epoch 850, val loss: 0.7926725745201111
Epoch 860, training loss: 6.098562717437744 = 0.0527501180768013 + 1.0 * 6.045812606811523
Epoch 860, val loss: 0.7965028882026672
Epoch 870, training loss: 6.094335079193115 = 0.05066850036382675 + 1.0 * 6.043666362762451
Epoch 870, val loss: 0.8003670573234558
Epoch 880, training loss: 6.097289562225342 = 0.0486932136118412 + 1.0 * 6.048596382141113
Epoch 880, val loss: 0.8041006326675415
Epoch 890, training loss: 6.091977119445801 = 0.04683661460876465 + 1.0 * 6.045140743255615
Epoch 890, val loss: 0.8077961206436157
Epoch 900, training loss: 6.0862226486206055 = 0.04508830979466438 + 1.0 * 6.041134357452393
Epoch 900, val loss: 0.8117387294769287
Epoch 910, training loss: 6.083644390106201 = 0.04342671111226082 + 1.0 * 6.040217876434326
Epoch 910, val loss: 0.8155560493469238
Epoch 920, training loss: 6.081245422363281 = 0.04183958098292351 + 1.0 * 6.039405822753906
Epoch 920, val loss: 0.8192329406738281
Epoch 930, training loss: 6.095716953277588 = 0.040333013981580734 + 1.0 * 6.055384159088135
Epoch 930, val loss: 0.8228131532669067
Epoch 940, training loss: 6.082919120788574 = 0.038914185017347336 + 1.0 * 6.044004917144775
Epoch 940, val loss: 0.8263901472091675
Epoch 950, training loss: 6.076442718505859 = 0.03757611662149429 + 1.0 * 6.0388665199279785
Epoch 950, val loss: 0.8302612900733948
Epoch 960, training loss: 6.07307243347168 = 0.0362941212952137 + 1.0 * 6.036778450012207
Epoch 960, val loss: 0.8337892889976501
Epoch 970, training loss: 6.074147701263428 = 0.035065554082393646 + 1.0 * 6.039082050323486
Epoch 970, val loss: 0.8372905850410461
Epoch 980, training loss: 6.069971561431885 = 0.03389623761177063 + 1.0 * 6.036075115203857
Epoch 980, val loss: 0.8406625986099243
Epoch 990, training loss: 6.067997932434082 = 0.032788604497909546 + 1.0 * 6.0352091789245605
Epoch 990, val loss: 0.844410240650177
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8561
Flip ASR: 0.8267/225 nodes
The final ASR:0.77737, 0.07985, Accuracy:0.82716, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9442])
updated graph: torch.Size([2, 10492])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97417, 0.00000, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.329216957092285 = 1.9553672075271606 + 1.0 * 8.373849868774414
Epoch 0, val loss: 1.950896143913269
Epoch 10, training loss: 10.31824016571045 = 1.9447996616363525 + 1.0 * 8.373440742492676
Epoch 10, val loss: 1.9407979249954224
Epoch 20, training loss: 10.302379608154297 = 1.9316630363464355 + 1.0 * 8.370716094970703
Epoch 20, val loss: 1.9278150796890259
Epoch 30, training loss: 10.260841369628906 = 1.9135642051696777 + 1.0 * 8.34727668762207
Epoch 30, val loss: 1.9097615480422974
Epoch 40, training loss: 9.985315322875977 = 1.8909916877746582 + 1.0 * 8.09432315826416
Epoch 40, val loss: 1.8875994682312012
Epoch 50, training loss: 9.11215877532959 = 1.8697575330734253 + 1.0 * 7.242401123046875
Epoch 50, val loss: 1.8675118684768677
Epoch 60, training loss: 8.733933448791504 = 1.8573968410491943 + 1.0 * 6.8765363693237305
Epoch 60, val loss: 1.85627019405365
Epoch 70, training loss: 8.541784286499023 = 1.8462414741516113 + 1.0 * 6.695542812347412
Epoch 70, val loss: 1.8453240394592285
Epoch 80, training loss: 8.41307258605957 = 1.834874153137207 + 1.0 * 6.5781989097595215
Epoch 80, val loss: 1.8347253799438477
Epoch 90, training loss: 8.315617561340332 = 1.8227778673171997 + 1.0 * 6.492839813232422
Epoch 90, val loss: 1.823702096939087
Epoch 100, training loss: 8.248819351196289 = 1.811237096786499 + 1.0 * 6.437582492828369
Epoch 100, val loss: 1.8135420083999634
Epoch 110, training loss: 8.195878982543945 = 1.8001867532730103 + 1.0 * 6.395691871643066
Epoch 110, val loss: 1.803999900817871
Epoch 120, training loss: 8.14910888671875 = 1.7895336151123047 + 1.0 * 6.359575271606445
Epoch 120, val loss: 1.7949273586273193
Epoch 130, training loss: 8.107182502746582 = 1.7787936925888062 + 1.0 * 6.328388690948486
Epoch 130, val loss: 1.7859071493148804
Epoch 140, training loss: 8.069957733154297 = 1.7672313451766968 + 1.0 * 6.3027262687683105
Epoch 140, val loss: 1.7763174772262573
Epoch 150, training loss: 8.035473823547363 = 1.7541043758392334 + 1.0 * 6.281369686126709
Epoch 150, val loss: 1.7656288146972656
Epoch 160, training loss: 8.002756118774414 = 1.7386888265609741 + 1.0 * 6.26406717300415
Epoch 160, val loss: 1.7532378435134888
Epoch 170, training loss: 7.968458652496338 = 1.720319390296936 + 1.0 * 6.248139381408691
Epoch 170, val loss: 1.7385175228118896
Epoch 180, training loss: 7.932933807373047 = 1.6980788707733154 + 1.0 * 6.2348551750183105
Epoch 180, val loss: 1.7207350730895996
Epoch 190, training loss: 7.894392013549805 = 1.6709893941879272 + 1.0 * 6.223402500152588
Epoch 190, val loss: 1.698966383934021
Epoch 200, training loss: 7.851253509521484 = 1.6382348537445068 + 1.0 * 6.213018894195557
Epoch 200, val loss: 1.6725807189941406
Epoch 210, training loss: 7.802323341369629 = 1.5989524126052856 + 1.0 * 6.203371047973633
Epoch 210, val loss: 1.6409529447555542
Epoch 220, training loss: 7.748453140258789 = 1.5529329776763916 + 1.0 * 6.195520401000977
Epoch 220, val loss: 1.6038455963134766
Epoch 230, training loss: 7.689235687255859 = 1.5006951093673706 + 1.0 * 6.188540458679199
Epoch 230, val loss: 1.561983346939087
Epoch 240, training loss: 7.627862930297852 = 1.4433526992797852 + 1.0 * 6.184510231018066
Epoch 240, val loss: 1.5165355205535889
Epoch 250, training loss: 7.565375804901123 = 1.3838969469070435 + 1.0 * 6.181478977203369
Epoch 250, val loss: 1.4702168703079224
Epoch 260, training loss: 7.498882293701172 = 1.3255457878112793 + 1.0 * 6.173336505889893
Epoch 260, val loss: 1.4253820180892944
Epoch 270, training loss: 7.435351371765137 = 1.2685455083847046 + 1.0 * 6.166805744171143
Epoch 270, val loss: 1.3822816610336304
Epoch 280, training loss: 7.3765974044799805 = 1.2132418155670166 + 1.0 * 6.163355827331543
Epoch 280, val loss: 1.3412818908691406
Epoch 290, training loss: 7.320399761199951 = 1.1608362197875977 + 1.0 * 6.1595635414123535
Epoch 290, val loss: 1.3030949831008911
Epoch 300, training loss: 7.263406753540039 = 1.1115772724151611 + 1.0 * 6.151829719543457
Epoch 300, val loss: 1.2677644491195679
Epoch 310, training loss: 7.211350440979004 = 1.0648084878921509 + 1.0 * 6.146542072296143
Epoch 310, val loss: 1.2345168590545654
Epoch 320, training loss: 7.170659065246582 = 1.0205576419830322 + 1.0 * 6.150101184844971
Epoch 320, val loss: 1.2033990621566772
Epoch 330, training loss: 7.119802951812744 = 0.9798382520675659 + 1.0 * 6.139964580535889
Epoch 330, val loss: 1.1749285459518433
Epoch 340, training loss: 7.075780868530273 = 0.9418332576751709 + 1.0 * 6.133947849273682
Epoch 340, val loss: 1.14842689037323
Epoch 350, training loss: 7.037478446960449 = 0.9059204459190369 + 1.0 * 6.131557941436768
Epoch 350, val loss: 1.1235507726669312
Epoch 360, training loss: 7.000827789306641 = 0.8719664812088013 + 1.0 * 6.128861427307129
Epoch 360, val loss: 1.1001743078231812
Epoch 370, training loss: 6.963727951049805 = 0.839805006980896 + 1.0 * 6.123922824859619
Epoch 370, val loss: 1.078112244606018
Epoch 380, training loss: 6.928506374359131 = 0.8089331388473511 + 1.0 * 6.11957311630249
Epoch 380, val loss: 1.0569827556610107
Epoch 390, training loss: 6.897123336791992 = 0.7791483402252197 + 1.0 * 6.117975234985352
Epoch 390, val loss: 1.036632776260376
Epoch 400, training loss: 6.865017890930176 = 0.7504100799560547 + 1.0 * 6.114607810974121
Epoch 400, val loss: 1.0169237852096558
Epoch 410, training loss: 6.832138538360596 = 0.7221769094467163 + 1.0 * 6.10996150970459
Epoch 410, val loss: 0.9975733757019043
Epoch 420, training loss: 6.812584400177002 = 0.6940763592720032 + 1.0 * 6.1185078620910645
Epoch 420, val loss: 0.9780893921852112
Epoch 430, training loss: 6.773503303527832 = 0.6663596034049988 + 1.0 * 6.107143878936768
Epoch 430, val loss: 0.9588239789009094
Epoch 440, training loss: 6.741220474243164 = 0.6387274861335754 + 1.0 * 6.102492809295654
Epoch 440, val loss: 0.9399251341819763
Epoch 450, training loss: 6.71174430847168 = 0.6109806895256042 + 1.0 * 6.10076379776001
Epoch 450, val loss: 0.9212521314620972
Epoch 460, training loss: 6.683873176574707 = 0.5832907557487488 + 1.0 * 6.100582599639893
Epoch 460, val loss: 0.9030891060829163
Epoch 470, training loss: 6.655678749084473 = 0.5559658408164978 + 1.0 * 6.09971284866333
Epoch 470, val loss: 0.8857408761978149
Epoch 480, training loss: 6.624283790588379 = 0.5287941098213196 + 1.0 * 6.095489501953125
Epoch 480, val loss: 0.8690512776374817
Epoch 490, training loss: 6.5965471267700195 = 0.5018727779388428 + 1.0 * 6.094674587249756
Epoch 490, val loss: 0.8531057834625244
Epoch 500, training loss: 6.568028926849365 = 0.47544118762016296 + 1.0 * 6.092587947845459
Epoch 500, val loss: 0.8381180167198181
Epoch 510, training loss: 6.544317245483398 = 0.4495929777622223 + 1.0 * 6.094724178314209
Epoch 510, val loss: 0.8239588141441345
Epoch 520, training loss: 6.514246463775635 = 0.4244672358036041 + 1.0 * 6.089779376983643
Epoch 520, val loss: 0.8108694553375244
Epoch 530, training loss: 6.4867777824401855 = 0.3998652696609497 + 1.0 * 6.086912631988525
Epoch 530, val loss: 0.7985346913337708
Epoch 540, training loss: 6.466658115386963 = 0.37579983472824097 + 1.0 * 6.090858459472656
Epoch 540, val loss: 0.7869842648506165
Epoch 550, training loss: 6.441529750823975 = 0.35261452198028564 + 1.0 * 6.0889153480529785
Epoch 550, val loss: 0.7763330936431885
Epoch 560, training loss: 6.420589447021484 = 0.3302336037158966 + 1.0 * 6.09035587310791
Epoch 560, val loss: 0.7666097283363342
Epoch 570, training loss: 6.393013954162598 = 0.3088937997817993 + 1.0 * 6.084120273590088
Epoch 570, val loss: 0.7577956914901733
Epoch 580, training loss: 6.368226051330566 = 0.2884822487831116 + 1.0 * 6.0797438621521
Epoch 580, val loss: 0.7499771118164062
Epoch 590, training loss: 6.349549293518066 = 0.2689972221851349 + 1.0 * 6.080552101135254
Epoch 590, val loss: 0.7431026697158813
Epoch 600, training loss: 6.327993869781494 = 0.2505664527416229 + 1.0 * 6.077427387237549
Epoch 600, val loss: 0.7372213006019592
Epoch 610, training loss: 6.309953689575195 = 0.23325161635875702 + 1.0 * 6.076702117919922
Epoch 610, val loss: 0.7323364615440369
Epoch 620, training loss: 6.296071529388428 = 0.21706852316856384 + 1.0 * 6.079002857208252
Epoch 620, val loss: 0.7284305095672607
Epoch 630, training loss: 6.275373458862305 = 0.2020728588104248 + 1.0 * 6.073300361633301
Epoch 630, val loss: 0.7254626750946045
Epoch 640, training loss: 6.261548042297363 = 0.18820016086101532 + 1.0 * 6.073348045349121
Epoch 640, val loss: 0.7234542965888977
Epoch 650, training loss: 6.24932861328125 = 0.17542754113674164 + 1.0 * 6.073901176452637
Epoch 650, val loss: 0.7222592830657959
Epoch 660, training loss: 6.238414764404297 = 0.16371579468250275 + 1.0 * 6.0746989250183105
Epoch 660, val loss: 0.7218684554100037
Epoch 670, training loss: 6.224001407623291 = 0.15300817787647247 + 1.0 * 6.070993423461914
Epoch 670, val loss: 0.7222099900245667
Epoch 680, training loss: 6.217442035675049 = 0.1432037651538849 + 1.0 * 6.074238300323486
Epoch 680, val loss: 0.7232176065444946
Epoch 690, training loss: 6.200633525848389 = 0.13426755368709564 + 1.0 * 6.066366195678711
Epoch 690, val loss: 0.7248371839523315
Epoch 700, training loss: 6.190310478210449 = 0.12603142857551575 + 1.0 * 6.064279079437256
Epoch 700, val loss: 0.7270175218582153
Epoch 710, training loss: 6.18864631652832 = 0.11845040321350098 + 1.0 * 6.070196151733398
Epoch 710, val loss: 0.7297125458717346
Epoch 720, training loss: 6.18305778503418 = 0.11154662817716599 + 1.0 * 6.071511268615723
Epoch 720, val loss: 0.7327821850776672
Epoch 730, training loss: 6.167718410491943 = 0.1052529364824295 + 1.0 * 6.062465667724609
Epoch 730, val loss: 0.736184298992157
Epoch 740, training loss: 6.159819602966309 = 0.0994635671377182 + 1.0 * 6.060356140136719
Epoch 740, val loss: 0.7399770617485046
Epoch 750, training loss: 6.15791130065918 = 0.09410382062196732 + 1.0 * 6.063807487487793
Epoch 750, val loss: 0.7440592646598816
Epoch 760, training loss: 6.150572776794434 = 0.08916915953159332 + 1.0 * 6.061403751373291
Epoch 760, val loss: 0.748376190662384
Epoch 770, training loss: 6.142433166503906 = 0.084626704454422 + 1.0 * 6.057806491851807
Epoch 770, val loss: 0.752899706363678
Epoch 780, training loss: 6.139449119567871 = 0.08041401952505112 + 1.0 * 6.059035301208496
Epoch 780, val loss: 0.7576369643211365
Epoch 790, training loss: 6.134354114532471 = 0.07650967687368393 + 1.0 * 6.057844638824463
Epoch 790, val loss: 0.7624749541282654
Epoch 800, training loss: 6.130496978759766 = 0.0729033425450325 + 1.0 * 6.057593822479248
Epoch 800, val loss: 0.7674380540847778
Epoch 810, training loss: 6.126241683959961 = 0.06954476237297058 + 1.0 * 6.056696891784668
Epoch 810, val loss: 0.7724858522415161
Epoch 820, training loss: 6.1186604499816895 = 0.066415935754776 + 1.0 * 6.052244663238525
Epoch 820, val loss: 0.7776342630386353
Epoch 830, training loss: 6.120453357696533 = 0.06348550319671631 + 1.0 * 6.056967735290527
Epoch 830, val loss: 0.7828486561775208
Epoch 840, training loss: 6.112878799438477 = 0.06075344979763031 + 1.0 * 6.052125453948975
Epoch 840, val loss: 0.7880577445030212
Epoch 850, training loss: 6.113964557647705 = 0.05820203572511673 + 1.0 * 6.05576229095459
Epoch 850, val loss: 0.7932915687561035
Epoch 860, training loss: 6.10835075378418 = 0.05582432821393013 + 1.0 * 6.052526473999023
Epoch 860, val loss: 0.7984884977340698
Epoch 870, training loss: 6.102928638458252 = 0.05360138788819313 + 1.0 * 6.049327373504639
Epoch 870, val loss: 0.8037053346633911
Epoch 880, training loss: 6.099112033843994 = 0.05150375887751579 + 1.0 * 6.047608375549316
Epoch 880, val loss: 0.808928906917572
Epoch 890, training loss: 6.099209308624268 = 0.04951697215437889 + 1.0 * 6.049692153930664
Epoch 890, val loss: 0.8141589164733887
Epoch 900, training loss: 6.095348358154297 = 0.04764518514275551 + 1.0 * 6.047703266143799
Epoch 900, val loss: 0.8193543553352356
Epoch 910, training loss: 6.101447582244873 = 0.04589308425784111 + 1.0 * 6.055554389953613
Epoch 910, val loss: 0.8245031237602234
Epoch 920, training loss: 6.091365814208984 = 0.04424148052930832 + 1.0 * 6.04712438583374
Epoch 920, val loss: 0.8295316696166992
Epoch 930, training loss: 6.088100910186768 = 0.04268181696534157 + 1.0 * 6.045419216156006
Epoch 930, val loss: 0.8345282077789307
Epoch 940, training loss: 6.0881428718566895 = 0.041197508573532104 + 1.0 * 6.046945571899414
Epoch 940, val loss: 0.8395229578018188
Epoch 950, training loss: 6.083372592926025 = 0.0397912822663784 + 1.0 * 6.043581485748291
Epoch 950, val loss: 0.8445010185241699
Epoch 960, training loss: 6.0854387283325195 = 0.03846026584506035 + 1.0 * 6.04697847366333
Epoch 960, val loss: 0.8494542837142944
Epoch 970, training loss: 6.0788493156433105 = 0.03719642013311386 + 1.0 * 6.041652679443359
Epoch 970, val loss: 0.8542878031730652
Epoch 980, training loss: 6.076573848724365 = 0.03599579632282257 + 1.0 * 6.0405778884887695
Epoch 980, val loss: 0.8591209053993225
Epoch 990, training loss: 6.079334259033203 = 0.03484844043850899 + 1.0 * 6.044486045837402
Epoch 990, val loss: 0.8639447689056396
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.5941
Flip ASR: 0.5244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.344145774841309 = 1.970290184020996 + 1.0 * 8.373855590820312
Epoch 0, val loss: 1.9688971042633057
Epoch 10, training loss: 10.332695960998535 = 1.959437608718872 + 1.0 * 8.373258590698242
Epoch 10, val loss: 1.9575579166412354
Epoch 20, training loss: 10.315731048583984 = 1.9460957050323486 + 1.0 * 8.369635581970215
Epoch 20, val loss: 1.9427472352981567
Epoch 30, training loss: 10.27626895904541 = 1.9280977249145508 + 1.0 * 8.34817123413086
Epoch 30, val loss: 1.9220271110534668
Epoch 40, training loss: 10.098808288574219 = 1.9061764478683472 + 1.0 * 8.192631721496582
Epoch 40, val loss: 1.8969229459762573
Epoch 50, training loss: 9.581979751586914 = 1.8844752311706543 + 1.0 * 7.697504043579102
Epoch 50, val loss: 1.8724372386932373
Epoch 60, training loss: 9.12926959991455 = 1.8678061962127686 + 1.0 * 7.261463642120361
Epoch 60, val loss: 1.8555463552474976
Epoch 70, training loss: 8.723434448242188 = 1.8530638217926025 + 1.0 * 6.870370388031006
Epoch 70, val loss: 1.8412662744522095
Epoch 80, training loss: 8.539529800415039 = 1.8394712209701538 + 1.0 * 6.700058937072754
Epoch 80, val loss: 1.8282305002212524
Epoch 90, training loss: 8.410737037658691 = 1.82520592212677 + 1.0 * 6.585530757904053
Epoch 90, val loss: 1.814018726348877
Epoch 100, training loss: 8.3101224899292 = 1.8109685182571411 + 1.0 * 6.4991536140441895
Epoch 100, val loss: 1.7995585203170776
Epoch 110, training loss: 8.236410140991211 = 1.7975492477416992 + 1.0 * 6.438860893249512
Epoch 110, val loss: 1.786218285560608
Epoch 120, training loss: 8.180859565734863 = 1.784711241722107 + 1.0 * 6.396148204803467
Epoch 120, val loss: 1.7734514474868774
Epoch 130, training loss: 8.136585235595703 = 1.7717927694320679 + 1.0 * 6.364792346954346
Epoch 130, val loss: 1.7605552673339844
Epoch 140, training loss: 8.094718933105469 = 1.7587970495224 + 1.0 * 6.335921764373779
Epoch 140, val loss: 1.7482244968414307
Epoch 150, training loss: 8.057710647583008 = 1.7454555034637451 + 1.0 * 6.312255382537842
Epoch 150, val loss: 1.7359980344772339
Epoch 160, training loss: 8.021814346313477 = 1.7309480905532837 + 1.0 * 6.290866374969482
Epoch 160, val loss: 1.723479151725769
Epoch 170, training loss: 7.986913681030273 = 1.7143276929855347 + 1.0 * 6.272585868835449
Epoch 170, val loss: 1.7098356485366821
Epoch 180, training loss: 7.952110290527344 = 1.6950891017913818 + 1.0 * 6.257020950317383
Epoch 180, val loss: 1.694517970085144
Epoch 190, training loss: 7.915669918060303 = 1.6727279424667358 + 1.0 * 6.242941856384277
Epoch 190, val loss: 1.6770684719085693
Epoch 200, training loss: 7.876221656799316 = 1.6462613344192505 + 1.0 * 6.2299604415893555
Epoch 200, val loss: 1.656438946723938
Epoch 210, training loss: 7.83774471282959 = 1.6146458387374878 + 1.0 * 6.2230987548828125
Epoch 210, val loss: 1.6316536664962769
Epoch 220, training loss: 7.78721284866333 = 1.5775967836380005 + 1.0 * 6.209616184234619
Epoch 220, val loss: 1.6025216579437256
Epoch 230, training loss: 7.735969543457031 = 1.5350111722946167 + 1.0 * 6.200958251953125
Epoch 230, val loss: 1.5687705278396606
Epoch 240, training loss: 7.680988788604736 = 1.4872077703475952 + 1.0 * 6.193780899047852
Epoch 240, val loss: 1.5308667421340942
Epoch 250, training loss: 7.626756191253662 = 1.4357587099075317 + 1.0 * 6.19099760055542
Epoch 250, val loss: 1.4902617931365967
Epoch 260, training loss: 7.569769859313965 = 1.3842898607254028 + 1.0 * 6.185480117797852
Epoch 260, val loss: 1.4504913091659546
Epoch 270, training loss: 7.51204252243042 = 1.3339942693710327 + 1.0 * 6.178048133850098
Epoch 270, val loss: 1.4121843576431274
Epoch 280, training loss: 7.456530570983887 = 1.2849061489105225 + 1.0 * 6.171624183654785
Epoch 280, val loss: 1.375354290008545
Epoch 290, training loss: 7.40371036529541 = 1.2372016906738281 + 1.0 * 6.166508674621582
Epoch 290, val loss: 1.3402013778686523
Epoch 300, training loss: 7.354662895202637 = 1.1910325288772583 + 1.0 * 6.163630485534668
Epoch 300, val loss: 1.3066750764846802
Epoch 310, training loss: 7.30497932434082 = 1.146855115890503 + 1.0 * 6.1581244468688965
Epoch 310, val loss: 1.2750886678695679
Epoch 320, training loss: 7.256805419921875 = 1.1037181615829468 + 1.0 * 6.153087139129639
Epoch 320, val loss: 1.2444076538085938
Epoch 330, training loss: 7.216263771057129 = 1.0610421895980835 + 1.0 * 6.155221462249756
Epoch 330, val loss: 1.214294672012329
Epoch 340, training loss: 7.1661200523376465 = 1.0195223093032837 + 1.0 * 6.146597862243652
Epoch 340, val loss: 1.1851680278778076
Epoch 350, training loss: 7.121537208557129 = 0.978895902633667 + 1.0 * 6.142641544342041
Epoch 350, val loss: 1.1568652391433716
Epoch 360, training loss: 7.078642845153809 = 0.9389952421188354 + 1.0 * 6.139647483825684
Epoch 360, val loss: 1.129205346107483
Epoch 370, training loss: 7.0399580001831055 = 0.900497555732727 + 1.0 * 6.139460563659668
Epoch 370, val loss: 1.1025816202163696
Epoch 380, training loss: 6.996976375579834 = 0.8636754155158997 + 1.0 * 6.13330078125
Epoch 380, val loss: 1.0774787664413452
Epoch 390, training loss: 6.957843780517578 = 0.8281839489936829 + 1.0 * 6.129659652709961
Epoch 390, val loss: 1.0534381866455078
Epoch 400, training loss: 6.924736499786377 = 0.7937648892402649 + 1.0 * 6.130971431732178
Epoch 400, val loss: 1.0303457975387573
Epoch 410, training loss: 6.886501789093018 = 0.7607541680335999 + 1.0 * 6.1257476806640625
Epoch 410, val loss: 1.008185625076294
Epoch 420, training loss: 6.851680755615234 = 0.7289116978645325 + 1.0 * 6.122768878936768
Epoch 420, val loss: 0.987278938293457
Epoch 430, training loss: 6.81830358505249 = 0.6977794766426086 + 1.0 * 6.120523929595947
Epoch 430, val loss: 0.9671619534492493
Epoch 440, training loss: 6.786781311035156 = 0.6673535704612732 + 1.0 * 6.119427680969238
Epoch 440, val loss: 0.9476239681243896
Epoch 450, training loss: 6.754218578338623 = 0.637940526008606 + 1.0 * 6.116278171539307
Epoch 450, val loss: 0.9293000102043152
Epoch 460, training loss: 6.723042964935303 = 0.6092951893806458 + 1.0 * 6.113747596740723
Epoch 460, val loss: 0.911689281463623
Epoch 470, training loss: 6.692958354949951 = 0.5813255906105042 + 1.0 * 6.111632823944092
Epoch 470, val loss: 0.8949951529502869
Epoch 480, training loss: 6.667841911315918 = 0.5543762445449829 + 1.0 * 6.113465785980225
Epoch 480, val loss: 0.8793929219245911
Epoch 490, training loss: 6.636948585510254 = 0.5285307765007019 + 1.0 * 6.108417987823486
Epoch 490, val loss: 0.8649750351905823
Epoch 500, training loss: 6.610396385192871 = 0.5034975409507751 + 1.0 * 6.106898784637451
Epoch 500, val loss: 0.8516014218330383
Epoch 510, training loss: 6.589354038238525 = 0.47945335507392883 + 1.0 * 6.10990047454834
Epoch 510, val loss: 0.8393067717552185
Epoch 520, training loss: 6.560174942016602 = 0.45673930644989014 + 1.0 * 6.103435516357422
Epoch 520, val loss: 0.8284043073654175
Epoch 530, training loss: 6.534689426422119 = 0.4348950982093811 + 1.0 * 6.099794387817383
Epoch 530, val loss: 0.8184910416603088
Epoch 540, training loss: 6.511383533477783 = 0.413912832736969 + 1.0 * 6.097470760345459
Epoch 540, val loss: 0.8096570372581482
Epoch 550, training loss: 6.501481533050537 = 0.39382651448249817 + 1.0 * 6.107655048370361
Epoch 550, val loss: 0.8019121289253235
Epoch 560, training loss: 6.476926326751709 = 0.374994695186615 + 1.0 * 6.101931571960449
Epoch 560, val loss: 0.7953609228134155
Epoch 570, training loss: 6.451437950134277 = 0.3571464419364929 + 1.0 * 6.094291687011719
Epoch 570, val loss: 0.7902474403381348
Epoch 580, training loss: 6.432168960571289 = 0.34007319808006287 + 1.0 * 6.092095851898193
Epoch 580, val loss: 0.7860711812973022
Epoch 590, training loss: 6.418218612670898 = 0.32368791103363037 + 1.0 * 6.0945305824279785
Epoch 590, val loss: 0.7827484607696533
Epoch 600, training loss: 6.396271705627441 = 0.3080570101737976 + 1.0 * 6.088214874267578
Epoch 600, val loss: 0.7803617119789124
Epoch 610, training loss: 6.3827056884765625 = 0.2929519712924957 + 1.0 * 6.0897536277771
Epoch 610, val loss: 0.7788528203964233
Epoch 620, training loss: 6.363690376281738 = 0.2783629894256592 + 1.0 * 6.0853271484375
Epoch 620, val loss: 0.7780447602272034
Epoch 630, training loss: 6.349503993988037 = 0.2642313838005066 + 1.0 * 6.085272789001465
Epoch 630, val loss: 0.7780331969261169
Epoch 640, training loss: 6.334292888641357 = 0.2505493760108948 + 1.0 * 6.083743572235107
Epoch 640, val loss: 0.7783746123313904
Epoch 650, training loss: 6.320622444152832 = 0.23737835884094238 + 1.0 * 6.083244323730469
Epoch 650, val loss: 0.7795320153236389
Epoch 660, training loss: 6.307830333709717 = 0.22467944025993347 + 1.0 * 6.083150863647461
Epoch 660, val loss: 0.7810927033424377
Epoch 670, training loss: 6.291464805603027 = 0.2124941349029541 + 1.0 * 6.078970432281494
Epoch 670, val loss: 0.783219575881958
Epoch 680, training loss: 6.295597553253174 = 0.20087261497974396 + 1.0 * 6.094725131988525
Epoch 680, val loss: 0.7858301401138306
Epoch 690, training loss: 6.271450996398926 = 0.19003978371620178 + 1.0 * 6.081411361694336
Epoch 690, val loss: 0.7889702320098877
Epoch 700, training loss: 6.255721092224121 = 0.17983850836753845 + 1.0 * 6.075882434844971
Epoch 700, val loss: 0.7928548455238342
Epoch 710, training loss: 6.244028568267822 = 0.17019551992416382 + 1.0 * 6.073832988739014
Epoch 710, val loss: 0.7971200942993164
Epoch 720, training loss: 6.2338151931762695 = 0.16110605001449585 + 1.0 * 6.072709083557129
Epoch 720, val loss: 0.8019694089889526
Epoch 730, training loss: 6.239386081695557 = 0.15256965160369873 + 1.0 * 6.086816310882568
Epoch 730, val loss: 0.8072283267974854
Epoch 740, training loss: 6.218738079071045 = 0.14469540119171143 + 1.0 * 6.074042797088623
Epoch 740, val loss: 0.8127915263175964
Epoch 750, training loss: 6.207808017730713 = 0.13733699917793274 + 1.0 * 6.070470809936523
Epoch 750, val loss: 0.818851888179779
Epoch 760, training loss: 6.200750350952148 = 0.13041625916957855 + 1.0 * 6.070333957672119
Epoch 760, val loss: 0.825176477432251
Epoch 770, training loss: 6.191608905792236 = 0.12390556186437607 + 1.0 * 6.0677032470703125
Epoch 770, val loss: 0.8317509889602661
Epoch 780, training loss: 6.191996097564697 = 0.1177612692117691 + 1.0 * 6.074234962463379
Epoch 780, val loss: 0.8385549783706665
Epoch 790, training loss: 6.180207252502441 = 0.11197718977928162 + 1.0 * 6.068230152130127
Epoch 790, val loss: 0.8457139730453491
Epoch 800, training loss: 6.1782402992248535 = 0.10652351379394531 + 1.0 * 6.071716785430908
Epoch 800, val loss: 0.8528876304626465
Epoch 810, training loss: 6.166264533996582 = 0.10136295109987259 + 1.0 * 6.064901351928711
Epoch 810, val loss: 0.8603094816207886
Epoch 820, training loss: 6.159308910369873 = 0.09647156298160553 + 1.0 * 6.06283712387085
Epoch 820, val loss: 0.8679562211036682
Epoch 830, training loss: 6.153628826141357 = 0.09183984994888306 + 1.0 * 6.061789035797119
Epoch 830, val loss: 0.8756904602050781
Epoch 840, training loss: 6.149964332580566 = 0.08744797110557556 + 1.0 * 6.062516212463379
Epoch 840, val loss: 0.8835291266441345
Epoch 850, training loss: 6.150320053100586 = 0.08329509198665619 + 1.0 * 6.067025184631348
Epoch 850, val loss: 0.8912721276283264
Epoch 860, training loss: 6.1443963050842285 = 0.07944788038730621 + 1.0 * 6.064948558807373
Epoch 860, val loss: 0.8991709351539612
Epoch 870, training loss: 6.1377081871032715 = 0.07582175731658936 + 1.0 * 6.061886310577393
Epoch 870, val loss: 0.9070725440979004
Epoch 880, training loss: 6.13484001159668 = 0.07238613814115524 + 1.0 * 6.062453746795654
Epoch 880, val loss: 0.9149598479270935
Epoch 890, training loss: 6.126575469970703 = 0.06912963837385178 + 1.0 * 6.057446002960205
Epoch 890, val loss: 0.9226957559585571
Epoch 900, training loss: 6.122908115386963 = 0.06605877727270126 + 1.0 * 6.056849479675293
Epoch 900, val loss: 0.9306415319442749
Epoch 910, training loss: 6.126033306121826 = 0.06314391642808914 + 1.0 * 6.062889575958252
Epoch 910, val loss: 0.9384892582893372
Epoch 920, training loss: 6.1162190437316895 = 0.060380782932043076 + 1.0 * 6.055838108062744
Epoch 920, val loss: 0.9460340142250061
Epoch 930, training loss: 6.1126227378845215 = 0.057787638157606125 + 1.0 * 6.054835319519043
Epoch 930, val loss: 0.9538121819496155
Epoch 940, training loss: 6.108872413635254 = 0.05533282831311226 + 1.0 * 6.053539752960205
Epoch 940, val loss: 0.9615654945373535
Epoch 950, training loss: 6.110685348510742 = 0.05300959572196007 + 1.0 * 6.057675838470459
Epoch 950, val loss: 0.9692090153694153
Epoch 960, training loss: 6.110681056976318 = 0.050820574164390564 + 1.0 * 6.059860706329346
Epoch 960, val loss: 0.9766219258308411
Epoch 970, training loss: 6.101673603057861 = 0.04877594858407974 + 1.0 * 6.0528974533081055
Epoch 970, val loss: 0.9841572046279907
Epoch 980, training loss: 6.09791374206543 = 0.04684416577219963 + 1.0 * 6.051069736480713
Epoch 980, val loss: 0.9916884899139404
Epoch 990, training loss: 6.098625183105469 = 0.04501425847411156 + 1.0 * 6.053610801696777
Epoch 990, val loss: 0.9990206360816956
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.5609
Flip ASR: 0.5200/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32193374633789 = 1.948085904121399 + 1.0 * 8.373847961425781
Epoch 0, val loss: 1.9509832859039307
Epoch 10, training loss: 10.31234359741211 = 1.9388562440872192 + 1.0 * 8.37348747253418
Epoch 10, val loss: 1.941666841506958
Epoch 20, training loss: 10.298946380615234 = 1.9277032613754272 + 1.0 * 8.371243476867676
Epoch 20, val loss: 1.9297281503677368
Epoch 30, training loss: 10.267991065979004 = 1.9124854803085327 + 1.0 * 8.35550594329834
Epoch 30, val loss: 1.9127111434936523
Epoch 40, training loss: 10.144441604614258 = 1.893420934677124 + 1.0 * 8.251020431518555
Epoch 40, val loss: 1.8922491073608398
Epoch 50, training loss: 9.635034561157227 = 1.8736584186553955 + 1.0 * 7.761375904083252
Epoch 50, val loss: 1.8718217611312866
Epoch 60, training loss: 9.141047477722168 = 1.856547474861145 + 1.0 * 7.284499645233154
Epoch 60, val loss: 1.8555176258087158
Epoch 70, training loss: 8.690250396728516 = 1.8439407348632812 + 1.0 * 6.846309661865234
Epoch 70, val loss: 1.8433269262313843
Epoch 80, training loss: 8.482980728149414 = 1.832187294960022 + 1.0 * 6.650793552398682
Epoch 80, val loss: 1.8320966958999634
Epoch 90, training loss: 8.356667518615723 = 1.8205516338348389 + 1.0 * 6.536116123199463
Epoch 90, val loss: 1.8206144571304321
Epoch 100, training loss: 8.265865325927734 = 1.809815526008606 + 1.0 * 6.45604944229126
Epoch 100, val loss: 1.8097224235534668
Epoch 110, training loss: 8.205160140991211 = 1.7995946407318115 + 1.0 * 6.40556526184082
Epoch 110, val loss: 1.7995513677597046
Epoch 120, training loss: 8.154167175292969 = 1.7898712158203125 + 1.0 * 6.364295482635498
Epoch 120, val loss: 1.7899301052093506
Epoch 130, training loss: 8.10995101928711 = 1.7802610397338867 + 1.0 * 6.329689979553223
Epoch 130, val loss: 1.7808476686477661
Epoch 140, training loss: 8.072152137756348 = 1.7702827453613281 + 1.0 * 6.3018693923950195
Epoch 140, val loss: 1.7717500925064087
Epoch 150, training loss: 8.03996467590332 = 1.7593988180160522 + 1.0 * 6.2805657386779785
Epoch 150, val loss: 1.7624315023422241
Epoch 160, training loss: 8.009129524230957 = 1.7471152544021606 + 1.0 * 6.262013912200928
Epoch 160, val loss: 1.7522093057632446
Epoch 170, training loss: 7.9791259765625 = 1.7327353954315186 + 1.0 * 6.246390342712402
Epoch 170, val loss: 1.740644097328186
Epoch 180, training loss: 7.94926643371582 = 1.715627908706665 + 1.0 * 6.233638763427734
Epoch 180, val loss: 1.7270442247390747
Epoch 190, training loss: 7.918758869171143 = 1.6952744722366333 + 1.0 * 6.223484516143799
Epoch 190, val loss: 1.7111332416534424
Epoch 200, training loss: 7.884176731109619 = 1.6709822416305542 + 1.0 * 6.213194370269775
Epoch 200, val loss: 1.6921902894973755
Epoch 210, training loss: 7.847296237945557 = 1.6417460441589355 + 1.0 * 6.205550193786621
Epoch 210, val loss: 1.66938054561615
Epoch 220, training loss: 7.8046064376831055 = 1.6065049171447754 + 1.0 * 6.19810152053833
Epoch 220, val loss: 1.6417051553726196
Epoch 230, training loss: 7.757059097290039 = 1.5641446113586426 + 1.0 * 6.1929144859313965
Epoch 230, val loss: 1.608304500579834
Epoch 240, training loss: 7.703426361083984 = 1.5140361785888672 + 1.0 * 6.189390182495117
Epoch 240, val loss: 1.5687942504882812
Epoch 250, training loss: 7.63899040222168 = 1.4562612771987915 + 1.0 * 6.182729244232178
Epoch 250, val loss: 1.5232164859771729
Epoch 260, training loss: 7.570826530456543 = 1.3910595178604126 + 1.0 * 6.17976713180542
Epoch 260, val loss: 1.4716711044311523
Epoch 270, training loss: 7.497243881225586 = 1.3208152055740356 + 1.0 * 6.17642879486084
Epoch 270, val loss: 1.4158161878585815
Epoch 280, training loss: 7.421789646148682 = 1.247634768486023 + 1.0 * 6.174154758453369
Epoch 280, val loss: 1.3582278490066528
Epoch 290, training loss: 7.34517240524292 = 1.1753281354904175 + 1.0 * 6.169844150543213
Epoch 290, val loss: 1.3016284704208374
Epoch 300, training loss: 7.271216869354248 = 1.1053880453109741 + 1.0 * 6.165828704833984
Epoch 300, val loss: 1.2472403049468994
Epoch 310, training loss: 7.204507350921631 = 1.0399097204208374 + 1.0 * 6.164597511291504
Epoch 310, val loss: 1.1968884468078613
Epoch 320, training loss: 7.139853477478027 = 0.9801121354103088 + 1.0 * 6.159741401672363
Epoch 320, val loss: 1.1515498161315918
Epoch 330, training loss: 7.080427646636963 = 0.924570620059967 + 1.0 * 6.155857086181641
Epoch 330, val loss: 1.1099166870117188
Epoch 340, training loss: 7.03504753112793 = 0.8723596930503845 + 1.0 * 6.1626877784729
Epoch 340, val loss: 1.0715506076812744
Epoch 350, training loss: 6.97458028793335 = 0.8239455223083496 + 1.0 * 6.150634765625
Epoch 350, val loss: 1.0362036228179932
Epoch 360, training loss: 6.923721790313721 = 0.7778356671333313 + 1.0 * 6.145885944366455
Epoch 360, val loss: 1.002930998802185
Epoch 370, training loss: 6.8758769035339355 = 0.7329994440078735 + 1.0 * 6.142877578735352
Epoch 370, val loss: 0.9708898663520813
Epoch 380, training loss: 6.834681987762451 = 0.689601480960846 + 1.0 * 6.14508056640625
Epoch 380, val loss: 0.9400445818901062
Epoch 390, training loss: 6.787711143493652 = 0.6480149030685425 + 1.0 * 6.13969612121582
Epoch 390, val loss: 0.9104300141334534
Epoch 400, training loss: 6.7419023513793945 = 0.6078317165374756 + 1.0 * 6.13407039642334
Epoch 400, val loss: 0.8817449808120728
Epoch 410, training loss: 6.714405059814453 = 0.5690081119537354 + 1.0 * 6.145397186279297
Epoch 410, val loss: 0.8540700674057007
Epoch 420, training loss: 6.665128707885742 = 0.5322886109352112 + 1.0 * 6.132840156555176
Epoch 420, val loss: 0.8282190561294556
Epoch 430, training loss: 6.6250762939453125 = 0.4976327121257782 + 1.0 * 6.127443790435791
Epoch 430, val loss: 0.8040421605110168
Epoch 440, training loss: 6.588748931884766 = 0.464741051197052 + 1.0 * 6.124007701873779
Epoch 440, val loss: 0.7812999486923218
Epoch 450, training loss: 6.555696487426758 = 0.43356648087501526 + 1.0 * 6.122129917144775
Epoch 450, val loss: 0.7600944638252258
Epoch 460, training loss: 6.531417369842529 = 0.4045744240283966 + 1.0 * 6.126842975616455
Epoch 460, val loss: 0.7405601143836975
Epoch 470, training loss: 6.498642921447754 = 0.37770649790763855 + 1.0 * 6.120936393737793
Epoch 470, val loss: 0.7229244112968445
Epoch 480, training loss: 6.466777324676514 = 0.35250845551490784 + 1.0 * 6.114268779754639
Epoch 480, val loss: 0.7069715857505798
Epoch 490, training loss: 6.447782039642334 = 0.3287174105644226 + 1.0 * 6.119064807891846
Epoch 490, val loss: 0.6925609111785889
Epoch 500, training loss: 6.418349266052246 = 0.3065297603607178 + 1.0 * 6.111819267272949
Epoch 500, val loss: 0.6794730424880981
Epoch 510, training loss: 6.394073009490967 = 0.2857093811035156 + 1.0 * 6.108363628387451
Epoch 510, val loss: 0.66790372133255
Epoch 520, training loss: 6.372165679931641 = 0.2661145031452179 + 1.0 * 6.106050968170166
Epoch 520, val loss: 0.6576051712036133
Epoch 530, training loss: 6.3543901443481445 = 0.2476043403148651 + 1.0 * 6.106785774230957
Epoch 530, val loss: 0.6485673785209656
Epoch 540, training loss: 6.345216751098633 = 0.23040896654129028 + 1.0 * 6.114807605743408
Epoch 540, val loss: 0.6407244801521301
Epoch 550, training loss: 6.315882682800293 = 0.2145620882511139 + 1.0 * 6.101320743560791
Epoch 550, val loss: 0.6339616775512695
Epoch 560, training loss: 6.299052715301514 = 0.19976462423801422 + 1.0 * 6.099287986755371
Epoch 560, val loss: 0.628409743309021
Epoch 570, training loss: 6.283358097076416 = 0.18594412505626678 + 1.0 * 6.097414016723633
Epoch 570, val loss: 0.6237795352935791
Epoch 580, training loss: 6.27711820602417 = 0.17310716211795807 + 1.0 * 6.104011058807373
Epoch 580, val loss: 0.6199987530708313
Epoch 590, training loss: 6.254774570465088 = 0.1612745225429535 + 1.0 * 6.093500137329102
Epoch 590, val loss: 0.6170437335968018
Epoch 600, training loss: 6.243442058563232 = 0.15035289525985718 + 1.0 * 6.0930891036987305
Epoch 600, val loss: 0.6150183081626892
Epoch 610, training loss: 6.234968185424805 = 0.14030194282531738 + 1.0 * 6.094666481018066
Epoch 610, val loss: 0.6136276125907898
Epoch 620, training loss: 6.2243523597717285 = 0.13107097148895264 + 1.0 * 6.093281269073486
Epoch 620, val loss: 0.6128485202789307
Epoch 630, training loss: 6.2110276222229 = 0.12269119918346405 + 1.0 * 6.08833646774292
Epoch 630, val loss: 0.6125357747077942
Epoch 640, training loss: 6.200867652893066 = 0.11497852951288223 + 1.0 * 6.0858893394470215
Epoch 640, val loss: 0.612827718257904
Epoch 650, training loss: 6.200109004974365 = 0.10787668824195862 + 1.0 * 6.0922322273254395
Epoch 650, val loss: 0.6135267615318298
Epoch 660, training loss: 6.193186283111572 = 0.10138258337974548 + 1.0 * 6.091803550720215
Epoch 660, val loss: 0.614705502986908
Epoch 670, training loss: 6.180603504180908 = 0.09549293667078018 + 1.0 * 6.085110664367676
Epoch 670, val loss: 0.6161019206047058
Epoch 680, training loss: 6.171858310699463 = 0.09005279839038849 + 1.0 * 6.08180570602417
Epoch 680, val loss: 0.6178474426269531
Epoch 690, training loss: 6.172092914581299 = 0.08502516150474548 + 1.0 * 6.087067604064941
Epoch 690, val loss: 0.6199076771736145
Epoch 700, training loss: 6.157865047454834 = 0.08040332794189453 + 1.0 * 6.0774617195129395
Epoch 700, val loss: 0.6221050024032593
Epoch 710, training loss: 6.153501033782959 = 0.07613039761781693 + 1.0 * 6.077370643615723
Epoch 710, val loss: 0.6245027184486389
Epoch 720, training loss: 6.1496405601501465 = 0.07215423136949539 + 1.0 * 6.077486515045166
Epoch 720, val loss: 0.6271510720252991
Epoch 730, training loss: 6.146224498748779 = 0.06849081814289093 + 1.0 * 6.077733516693115
Epoch 730, val loss: 0.6300199627876282
Epoch 740, training loss: 6.138885498046875 = 0.06511642783880234 + 1.0 * 6.0737690925598145
Epoch 740, val loss: 0.6327776908874512
Epoch 750, training loss: 6.135373115539551 = 0.06197528913617134 + 1.0 * 6.073397636413574
Epoch 750, val loss: 0.6357548832893372
Epoch 760, training loss: 6.137234210968018 = 0.059038881212472916 + 1.0 * 6.078195095062256
Epoch 760, val loss: 0.6389093399047852
Epoch 770, training loss: 6.126614570617676 = 0.05632525309920311 + 1.0 * 6.070289134979248
Epoch 770, val loss: 0.6419942378997803
Epoch 780, training loss: 6.123326778411865 = 0.05378074571490288 + 1.0 * 6.069546222686768
Epoch 780, val loss: 0.6451840996742249
Epoch 790, training loss: 6.142604351043701 = 0.051404763013124466 + 1.0 * 6.0911993980407715
Epoch 790, val loss: 0.6484998464584351
Epoch 800, training loss: 6.12005615234375 = 0.049196917563676834 + 1.0 * 6.070859432220459
Epoch 800, val loss: 0.6517507433891296
Epoch 810, training loss: 6.115235805511475 = 0.04714798182249069 + 1.0 * 6.068088054656982
Epoch 810, val loss: 0.6549197435379028
Epoch 820, training loss: 6.110511302947998 = 0.04521448537707329 + 1.0 * 6.065296649932861
Epoch 820, val loss: 0.6582762598991394
Epoch 830, training loss: 6.107913494110107 = 0.043377239257097244 + 1.0 * 6.064536094665527
Epoch 830, val loss: 0.6616607904434204
Epoch 840, training loss: 6.114980220794678 = 0.041640911251306534 + 1.0 * 6.073339462280273
Epoch 840, val loss: 0.6650873422622681
Epoch 850, training loss: 6.106329917907715 = 0.040018074214458466 + 1.0 * 6.066311836242676
Epoch 850, val loss: 0.6684632301330566
Epoch 860, training loss: 6.101861953735352 = 0.03848716989159584 + 1.0 * 6.063374996185303
Epoch 860, val loss: 0.6718370914459229
Epoch 870, training loss: 6.105954170227051 = 0.03704118728637695 + 1.0 * 6.068912982940674
Epoch 870, val loss: 0.6753172874450684
Epoch 880, training loss: 6.101724147796631 = 0.035669684410095215 + 1.0 * 6.066054344177246
Epoch 880, val loss: 0.6787999868392944
Epoch 890, training loss: 6.09671688079834 = 0.03439198061823845 + 1.0 * 6.0623250007629395
Epoch 890, val loss: 0.6820827722549438
Epoch 900, training loss: 6.092728137969971 = 0.033173758536577225 + 1.0 * 6.059554576873779
Epoch 900, val loss: 0.685486912727356
Epoch 910, training loss: 6.091289520263672 = 0.032013483345508575 + 1.0 * 6.059276103973389
Epoch 910, val loss: 0.688945472240448
Epoch 920, training loss: 6.090016841888428 = 0.030909888446331024 + 1.0 * 6.059106826782227
Epoch 920, val loss: 0.6923232078552246
Epoch 930, training loss: 6.089437007904053 = 0.0298761073499918 + 1.0 * 6.059560775756836
Epoch 930, val loss: 0.695669412612915
Epoch 940, training loss: 6.085931301116943 = 0.028891315683722496 + 1.0 * 6.057040214538574
Epoch 940, val loss: 0.6989428400993347
Epoch 950, training loss: 6.088515758514404 = 0.02795155718922615 + 1.0 * 6.060564041137695
Epoch 950, val loss: 0.702340304851532
Epoch 960, training loss: 6.083748817443848 = 0.027067529037594795 + 1.0 * 6.056681156158447
Epoch 960, val loss: 0.7055640816688538
Epoch 970, training loss: 6.086208343505859 = 0.026218025013804436 + 1.0 * 6.059990406036377
Epoch 970, val loss: 0.7088062763214111
Epoch 980, training loss: 6.079835414886475 = 0.02541714906692505 + 1.0 * 6.054418087005615
Epoch 980, val loss: 0.7120600342750549
Epoch 990, training loss: 6.077219009399414 = 0.024648675695061684 + 1.0 * 6.052570343017578
Epoch 990, val loss: 0.7151584029197693
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.71218, 0.19096, Accuracy:0.81605, 0.02188
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9524])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.32522201538086 = 1.951326847076416 + 1.0 * 8.373894691467285
Epoch 0, val loss: 1.9523134231567383
Epoch 10, training loss: 10.314645767211914 = 1.941007375717163 + 1.0 * 8.373638153076172
Epoch 10, val loss: 1.9422489404678345
Epoch 20, training loss: 10.300224304199219 = 1.9283860921859741 + 1.0 * 8.371838569641113
Epoch 20, val loss: 1.9294297695159912
Epoch 30, training loss: 10.26904582977295 = 1.9108120203018188 + 1.0 * 8.358233451843262
Epoch 30, val loss: 1.9111448526382446
Epoch 40, training loss: 10.151529312133789 = 1.8873485326766968 + 1.0 * 8.264181137084961
Epoch 40, val loss: 1.8875654935836792
Epoch 50, training loss: 9.675775527954102 = 1.8630108833312988 + 1.0 * 7.812764644622803
Epoch 50, val loss: 1.8644572496414185
Epoch 60, training loss: 9.182920455932617 = 1.8430390357971191 + 1.0 * 7.339881420135498
Epoch 60, val loss: 1.8463878631591797
Epoch 70, training loss: 8.706562995910645 = 1.8271831274032593 + 1.0 * 6.879379749298096
Epoch 70, val loss: 1.8320841789245605
Epoch 80, training loss: 8.481575012207031 = 1.8127944469451904 + 1.0 * 6.66878080368042
Epoch 80, val loss: 1.8188141584396362
Epoch 90, training loss: 8.362598419189453 = 1.7954124212265015 + 1.0 * 6.567185878753662
Epoch 90, val loss: 1.802781581878662
Epoch 100, training loss: 8.26518440246582 = 1.7761907577514648 + 1.0 * 6.4889936447143555
Epoch 100, val loss: 1.785441279411316
Epoch 110, training loss: 8.186519622802734 = 1.7572627067565918 + 1.0 * 6.429256439208984
Epoch 110, val loss: 1.7685942649841309
Epoch 120, training loss: 8.121849060058594 = 1.7379895448684692 + 1.0 * 6.383859634399414
Epoch 120, val loss: 1.751433253288269
Epoch 130, training loss: 8.064681053161621 = 1.7166788578033447 + 1.0 * 6.3480024337768555
Epoch 130, val loss: 1.7323057651519775
Epoch 140, training loss: 8.011919975280762 = 1.6922430992126465 + 1.0 * 6.319676876068115
Epoch 140, val loss: 1.7104915380477905
Epoch 150, training loss: 7.959709644317627 = 1.6642255783081055 + 1.0 * 6.2954840660095215
Epoch 150, val loss: 1.685678243637085
Epoch 160, training loss: 7.907635688781738 = 1.63175368309021 + 1.0 * 6.275882244110107
Epoch 160, val loss: 1.6570179462432861
Epoch 170, training loss: 7.855334281921387 = 1.5941449403762817 + 1.0 * 6.2611894607543945
Epoch 170, val loss: 1.6240752935409546
Epoch 180, training loss: 7.797682762145996 = 1.551720380783081 + 1.0 * 6.245962142944336
Epoch 180, val loss: 1.5872077941894531
Epoch 190, training loss: 7.737398147583008 = 1.5038694143295288 + 1.0 * 6.2335286140441895
Epoch 190, val loss: 1.5460773706436157
Epoch 200, training loss: 7.68079948425293 = 1.4511940479278564 + 1.0 * 6.229605197906494
Epoch 200, val loss: 1.5015614032745361
Epoch 210, training loss: 7.610537528991699 = 1.3956198692321777 + 1.0 * 6.2149176597595215
Epoch 210, val loss: 1.4554193019866943
Epoch 220, training loss: 7.544276714324951 = 1.3377281427383423 + 1.0 * 6.206548690795898
Epoch 220, val loss: 1.4082775115966797
Epoch 230, training loss: 7.4780049324035645 = 1.278746485710144 + 1.0 * 6.199258327484131
Epoch 230, val loss: 1.361567497253418
Epoch 240, training loss: 7.4156718254089355 = 1.2205029726028442 + 1.0 * 6.195168972015381
Epoch 240, val loss: 1.3167250156402588
Epoch 250, training loss: 7.350346565246582 = 1.1640444993972778 + 1.0 * 6.186302185058594
Epoch 250, val loss: 1.2742102146148682
Epoch 260, training loss: 7.291400909423828 = 1.1093792915344238 + 1.0 * 6.182021617889404
Epoch 260, val loss: 1.2339552640914917
Epoch 270, training loss: 7.232933044433594 = 1.0571626424789429 + 1.0 * 6.175770282745361
Epoch 270, val loss: 1.1961866617202759
Epoch 280, training loss: 7.176837921142578 = 1.0073909759521484 + 1.0 * 6.16944694519043
Epoch 280, val loss: 1.160329818725586
Epoch 290, training loss: 7.124594688415527 = 0.9595859050750732 + 1.0 * 6.165008544921875
Epoch 290, val loss: 1.1261062622070312
Epoch 300, training loss: 7.077239990234375 = 0.9140483140945435 + 1.0 * 6.163191795349121
Epoch 300, val loss: 1.093832015991211
Epoch 310, training loss: 7.028710842132568 = 0.8708019256591797 + 1.0 * 6.157908916473389
Epoch 310, val loss: 1.0634130239486694
Epoch 320, training loss: 6.981511116027832 = 0.8293916583061218 + 1.0 * 6.1521196365356445
Epoch 320, val loss: 1.0347355604171753
Epoch 330, training loss: 6.939116477966309 = 0.7897989749908447 + 1.0 * 6.149317741394043
Epoch 330, val loss: 1.0077998638153076
Epoch 340, training loss: 6.89742374420166 = 0.7523921132087708 + 1.0 * 6.145031452178955
Epoch 340, val loss: 0.9830636382102966
Epoch 350, training loss: 6.8581223487854 = 0.7167403101921082 + 1.0 * 6.141382217407227
Epoch 350, val loss: 0.9602597951889038
Epoch 360, training loss: 6.825827121734619 = 0.6827542185783386 + 1.0 * 6.143073081970215
Epoch 360, val loss: 0.9392703771591187
Epoch 370, training loss: 6.787332534790039 = 0.6505962610244751 + 1.0 * 6.1367363929748535
Epoch 370, val loss: 0.9200728535652161
Epoch 380, training loss: 6.75240421295166 = 0.6198468804359436 + 1.0 * 6.132557392120361
Epoch 380, val loss: 0.9023751020431519
Epoch 390, training loss: 6.726578235626221 = 0.5901774168014526 + 1.0 * 6.1364006996154785
Epoch 390, val loss: 0.8858730792999268
Epoch 400, training loss: 6.691694259643555 = 0.5622022747993469 + 1.0 * 6.129491806030273
Epoch 400, val loss: 0.8708247542381287
Epoch 410, training loss: 6.6603827476501465 = 0.5352105498313904 + 1.0 * 6.125172138214111
Epoch 410, val loss: 0.8569192886352539
Epoch 420, training loss: 6.632252216339111 = 0.5090884566307068 + 1.0 * 6.12316370010376
Epoch 420, val loss: 0.8440017104148865
Epoch 430, training loss: 6.604105472564697 = 0.48382478952407837 + 1.0 * 6.120280742645264
Epoch 430, val loss: 0.8320444226264954
Epoch 440, training loss: 6.581418514251709 = 0.4592418670654297 + 1.0 * 6.122176647186279
Epoch 440, val loss: 0.8210977911949158
Epoch 450, training loss: 6.55156135559082 = 0.4352119266986847 + 1.0 * 6.116349220275879
Epoch 450, val loss: 0.8111270666122437
Epoch 460, training loss: 6.525639057159424 = 0.4114695191383362 + 1.0 * 6.114169597625732
Epoch 460, val loss: 0.8020777106285095
Epoch 470, training loss: 6.504154205322266 = 0.38800641894340515 + 1.0 * 6.116147994995117
Epoch 470, val loss: 0.7939620018005371
Epoch 480, training loss: 6.477230548858643 = 0.3649080693721771 + 1.0 * 6.1123223304748535
Epoch 480, val loss: 0.7869419455528259
Epoch 490, training loss: 6.450277328491211 = 0.34218811988830566 + 1.0 * 6.108089447021484
Epoch 490, val loss: 0.7810993194580078
Epoch 500, training loss: 6.435270309448242 = 0.31995126605033875 + 1.0 * 6.11531925201416
Epoch 500, val loss: 0.7764440774917603
Epoch 510, training loss: 6.4049201011657715 = 0.29866060614585876 + 1.0 * 6.106259346008301
Epoch 510, val loss: 0.7731891870498657
Epoch 520, training loss: 6.381961822509766 = 0.2783072590827942 + 1.0 * 6.103654384613037
Epoch 520, val loss: 0.7713120579719543
Epoch 530, training loss: 6.360169410705566 = 0.25906017422676086 + 1.0 * 6.101109027862549
Epoch 530, val loss: 0.7708182334899902
Epoch 540, training loss: 6.3499884605407715 = 0.24103912711143494 + 1.0 * 6.108949184417725
Epoch 540, val loss: 0.7715761661529541
Epoch 550, training loss: 6.323998928070068 = 0.22444452345371246 + 1.0 * 6.099554538726807
Epoch 550, val loss: 0.7737213373184204
Epoch 560, training loss: 6.3115339279174805 = 0.2091820240020752 + 1.0 * 6.102351665496826
Epoch 560, val loss: 0.7770044207572937
Epoch 570, training loss: 6.292583465576172 = 0.1952967643737793 + 1.0 * 6.097286701202393
Epoch 570, val loss: 0.781197190284729
Epoch 580, training loss: 6.280877590179443 = 0.182582288980484 + 1.0 * 6.098295211791992
Epoch 580, val loss: 0.7862510681152344
Epoch 590, training loss: 6.263721466064453 = 0.171016126871109 + 1.0 * 6.092705249786377
Epoch 590, val loss: 0.7919469475746155
Epoch 600, training loss: 6.251559257507324 = 0.16043752431869507 + 1.0 * 6.091121673583984
Epoch 600, val loss: 0.7983466386795044
Epoch 610, training loss: 6.246387481689453 = 0.15076744556427002 + 1.0 * 6.095620155334473
Epoch 610, val loss: 0.8052781224250793
Epoch 620, training loss: 6.234697341918945 = 0.14193177223205566 + 1.0 * 6.0927653312683105
Epoch 620, val loss: 0.8125695586204529
Epoch 630, training loss: 6.222506523132324 = 0.1338295191526413 + 1.0 * 6.088676929473877
Epoch 630, val loss: 0.8201518058776855
Epoch 640, training loss: 6.212184429168701 = 0.12638534605503082 + 1.0 * 6.085799217224121
Epoch 640, val loss: 0.8280724883079529
Epoch 650, training loss: 6.207442283630371 = 0.11950194835662842 + 1.0 * 6.087940216064453
Epoch 650, val loss: 0.8361635208129883
Epoch 660, training loss: 6.196706771850586 = 0.11315903812646866 + 1.0 * 6.083547592163086
Epoch 660, val loss: 0.8445316553115845
Epoch 670, training loss: 6.190178394317627 = 0.10727831721305847 + 1.0 * 6.082900047302246
Epoch 670, val loss: 0.8529476523399353
Epoch 680, training loss: 6.183140754699707 = 0.10183140635490417 + 1.0 * 6.0813093185424805
Epoch 680, val loss: 0.8615668416023254
Epoch 690, training loss: 6.179723262786865 = 0.0967644527554512 + 1.0 * 6.082958698272705
Epoch 690, val loss: 0.870193600654602
Epoch 700, training loss: 6.169199466705322 = 0.09205621480941772 + 1.0 * 6.07714319229126
Epoch 700, val loss: 0.8788600564002991
Epoch 710, training loss: 6.164658546447754 = 0.08766305446624756 + 1.0 * 6.076995372772217
Epoch 710, val loss: 0.8875482082366943
Epoch 720, training loss: 6.16591215133667 = 0.08356481790542603 + 1.0 * 6.082347393035889
Epoch 720, val loss: 0.8963600397109985
Epoch 730, training loss: 6.155294895172119 = 0.07972151041030884 + 1.0 * 6.075573444366455
Epoch 730, val loss: 0.9049105048179626
Epoch 740, training loss: 6.149271488189697 = 0.07613702863454819 + 1.0 * 6.073134422302246
Epoch 740, val loss: 0.9136481285095215
Epoch 750, training loss: 6.144341945648193 = 0.07275974750518799 + 1.0 * 6.071582317352295
Epoch 750, val loss: 0.9223040342330933
Epoch 760, training loss: 6.145989418029785 = 0.06958319991827011 + 1.0 * 6.076406002044678
Epoch 760, val loss: 0.9308696389198303
Epoch 770, training loss: 6.140713214874268 = 0.06661156564950943 + 1.0 * 6.074101448059082
Epoch 770, val loss: 0.9395004510879517
Epoch 780, training loss: 6.133004188537598 = 0.06382344663143158 + 1.0 * 6.069180965423584
Epoch 780, val loss: 0.9480013847351074
Epoch 790, training loss: 6.128551959991455 = 0.06119189411401749 + 1.0 * 6.067359924316406
Epoch 790, val loss: 0.9564011096954346
Epoch 800, training loss: 6.124833583831787 = 0.05870571732521057 + 1.0 * 6.066127777099609
Epoch 800, val loss: 0.9647698402404785
Epoch 810, training loss: 6.133388042449951 = 0.05634675174951553 + 1.0 * 6.077041149139404
Epoch 810, val loss: 0.972976803779602
Epoch 820, training loss: 6.122704982757568 = 0.05414006486535072 + 1.0 * 6.0685648918151855
Epoch 820, val loss: 0.9812940955162048
Epoch 830, training loss: 6.116068363189697 = 0.05203824117779732 + 1.0 * 6.064030170440674
Epoch 830, val loss: 0.9893746972084045
Epoch 840, training loss: 6.11639928817749 = 0.050052888691425323 + 1.0 * 6.066346168518066
Epoch 840, val loss: 0.9974839091300964
Epoch 850, training loss: 6.109421253204346 = 0.048167020082473755 + 1.0 * 6.061254024505615
Epoch 850, val loss: 1.0055055618286133
Epoch 860, training loss: 6.107518196105957 = 0.04637797921895981 + 1.0 * 6.061140060424805
Epoch 860, val loss: 1.0133934020996094
Epoch 870, training loss: 6.112600803375244 = 0.04468148201704025 + 1.0 * 6.0679192543029785
Epoch 870, val loss: 1.021276593208313
Epoch 880, training loss: 6.110525608062744 = 0.043065279722213745 + 1.0 * 6.067460536956787
Epoch 880, val loss: 1.0288554430007935
Epoch 890, training loss: 6.100251197814941 = 0.041542522609233856 + 1.0 * 6.058708667755127
Epoch 890, val loss: 1.0365720987319946
Epoch 900, training loss: 6.09715461730957 = 0.04008984565734863 + 1.0 * 6.057064533233643
Epoch 900, val loss: 1.0442196130752563
Epoch 910, training loss: 6.096286773681641 = 0.03870139643549919 + 1.0 * 6.0575852394104
Epoch 910, val loss: 1.0516926050186157
Epoch 920, training loss: 6.099817276000977 = 0.037380918860435486 + 1.0 * 6.062436580657959
Epoch 920, val loss: 1.0592052936553955
Epoch 930, training loss: 6.091317176818848 = 0.03612237796187401 + 1.0 * 6.055194854736328
Epoch 930, val loss: 1.0664058923721313
Epoch 940, training loss: 6.089433193206787 = 0.034926872700452805 + 1.0 * 6.054506301879883
Epoch 940, val loss: 1.073708415031433
Epoch 950, training loss: 6.090595722198486 = 0.033784329891204834 + 1.0 * 6.056811332702637
Epoch 950, val loss: 1.081034779548645
Epoch 960, training loss: 6.085376739501953 = 0.032690756022930145 + 1.0 * 6.0526862144470215
Epoch 960, val loss: 1.0879831314086914
Epoch 970, training loss: 6.086540222167969 = 0.03165352717041969 + 1.0 * 6.054886817932129
Epoch 970, val loss: 1.095010757446289
Epoch 980, training loss: 6.084886074066162 = 0.030660418793559074 + 1.0 * 6.054225444793701
Epoch 980, val loss: 1.1019384860992432
Epoch 990, training loss: 6.080178737640381 = 0.029712270945310593 + 1.0 * 6.050466537475586
Epoch 990, val loss: 1.1088969707489014
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.308378219604492 = 1.934468388557434 + 1.0 * 8.373909950256348
Epoch 0, val loss: 1.9404760599136353
Epoch 10, training loss: 10.29845905303955 = 1.9248294830322266 + 1.0 * 8.373629570007324
Epoch 10, val loss: 1.9306204319000244
Epoch 20, training loss: 10.284680366516113 = 1.912768006324768 + 1.0 * 8.371912002563477
Epoch 20, val loss: 1.9181486368179321
Epoch 30, training loss: 10.255484580993652 = 1.8958532810211182 + 1.0 * 8.359631538391113
Epoch 30, val loss: 1.9006808996200562
Epoch 40, training loss: 10.156258583068848 = 1.872776746749878 + 1.0 * 8.28348159790039
Epoch 40, val loss: 1.877944827079773
Epoch 50, training loss: 9.657459259033203 = 1.8476059436798096 + 1.0 * 7.809853553771973
Epoch 50, val loss: 1.8543510437011719
Epoch 60, training loss: 9.17461109161377 = 1.824562430381775 + 1.0 * 7.350049018859863
Epoch 60, val loss: 1.8339016437530518
Epoch 70, training loss: 8.827252388000488 = 1.808738112449646 + 1.0 * 7.018514633178711
Epoch 70, val loss: 1.8194432258605957
Epoch 80, training loss: 8.624397277832031 = 1.7940592765808105 + 1.0 * 6.8303375244140625
Epoch 80, val loss: 1.8050613403320312
Epoch 90, training loss: 8.480592727661133 = 1.7790333032608032 + 1.0 * 6.701559543609619
Epoch 90, val loss: 1.7903810739517212
Epoch 100, training loss: 8.37144947052002 = 1.763882040977478 + 1.0 * 6.607567310333252
Epoch 100, val loss: 1.7762893438339233
Epoch 110, training loss: 8.276628494262695 = 1.7482112646102905 + 1.0 * 6.528417110443115
Epoch 110, val loss: 1.7620790004730225
Epoch 120, training loss: 8.203042030334473 = 1.7314963340759277 + 1.0 * 6.471545696258545
Epoch 120, val loss: 1.7470157146453857
Epoch 130, training loss: 8.135906219482422 = 1.7128489017486572 + 1.0 * 6.4230570793151855
Epoch 130, val loss: 1.7304905652999878
Epoch 140, training loss: 8.081486701965332 = 1.6910139322280884 + 1.0 * 6.390472412109375
Epoch 140, val loss: 1.7117079496383667
Epoch 150, training loss: 8.028003692626953 = 1.6653685569763184 + 1.0 * 6.362635612487793
Epoch 150, val loss: 1.6901700496673584
Epoch 160, training loss: 7.973741054534912 = 1.635238766670227 + 1.0 * 6.338502407073975
Epoch 160, val loss: 1.6653536558151245
Epoch 170, training loss: 7.916319370269775 = 1.5997575521469116 + 1.0 * 6.316561698913574
Epoch 170, val loss: 1.636266827583313
Epoch 180, training loss: 7.855803489685059 = 1.5587600469589233 + 1.0 * 6.297043323516846
Epoch 180, val loss: 1.60309636592865
Epoch 190, training loss: 7.793749809265137 = 1.5122674703598022 + 1.0 * 6.281482219696045
Epoch 190, val loss: 1.5655949115753174
Epoch 200, training loss: 7.726632595062256 = 1.4604249000549316 + 1.0 * 6.266207695007324
Epoch 200, val loss: 1.5241432189941406
Epoch 210, training loss: 7.657942771911621 = 1.4035680294036865 + 1.0 * 6.2543745040893555
Epoch 210, val loss: 1.4788775444030762
Epoch 220, training loss: 7.584933757781982 = 1.3433932065963745 + 1.0 * 6.241540431976318
Epoch 220, val loss: 1.4314173460006714
Epoch 230, training loss: 7.5110626220703125 = 1.2806107997894287 + 1.0 * 6.230451583862305
Epoch 230, val loss: 1.3823330402374268
Epoch 240, training loss: 7.441128253936768 = 1.2160195112228394 + 1.0 * 6.225108623504639
Epoch 240, val loss: 1.3320872783660889
Epoch 250, training loss: 7.365289688110352 = 1.1517951488494873 + 1.0 * 6.213494300842285
Epoch 250, val loss: 1.2821800708770752
Epoch 260, training loss: 7.293820858001709 = 1.0880922079086304 + 1.0 * 6.205728530883789
Epoch 260, val loss: 1.2327475547790527
Epoch 270, training loss: 7.225992202758789 = 1.0261605978012085 + 1.0 * 6.199831485748291
Epoch 270, val loss: 1.184696912765503
Epoch 280, training loss: 7.159295082092285 = 0.9669727087020874 + 1.0 * 6.192322254180908
Epoch 280, val loss: 1.138720989227295
Epoch 290, training loss: 7.105244159698486 = 0.9106460213661194 + 1.0 * 6.194598197937012
Epoch 290, val loss: 1.094763994216919
Epoch 300, training loss: 7.040849208831787 = 0.8584904074668884 + 1.0 * 6.182358741760254
Epoch 300, val loss: 1.0538763999938965
Epoch 310, training loss: 6.9845404624938965 = 0.8097671866416931 + 1.0 * 6.174773216247559
Epoch 310, val loss: 1.0158743858337402
Epoch 320, training loss: 6.941339492797852 = 0.7642993330955505 + 1.0 * 6.177040100097656
Epoch 320, val loss: 0.9804264903068542
Epoch 330, training loss: 6.890247821807861 = 0.7223986387252808 + 1.0 * 6.167849063873291
Epoch 330, val loss: 0.9479621052742004
Epoch 340, training loss: 6.84559440612793 = 0.6833299994468689 + 1.0 * 6.162264347076416
Epoch 340, val loss: 0.9181750416755676
Epoch 350, training loss: 6.8047709465026855 = 0.6467357873916626 + 1.0 * 6.1580352783203125
Epoch 350, val loss: 0.8907891511917114
Epoch 360, training loss: 6.768024444580078 = 0.6123417019844055 + 1.0 * 6.155682563781738
Epoch 360, val loss: 0.8659259080886841
Epoch 370, training loss: 6.730008602142334 = 0.579626739025116 + 1.0 * 6.150382041931152
Epoch 370, val loss: 0.8431024551391602
Epoch 380, training loss: 6.700544357299805 = 0.5483232736587524 + 1.0 * 6.152221202850342
Epoch 380, val loss: 0.8221535682678223
Epoch 390, training loss: 6.66314172744751 = 0.5185909271240234 + 1.0 * 6.144550800323486
Epoch 390, val loss: 0.8032320141792297
Epoch 400, training loss: 6.631694316864014 = 0.4901033043861389 + 1.0 * 6.1415910720825195
Epoch 400, val loss: 0.78621906042099
Epoch 410, training loss: 6.600477695465088 = 0.4625956118106842 + 1.0 * 6.137882232666016
Epoch 410, val loss: 0.770714521408081
Epoch 420, training loss: 6.579132556915283 = 0.4360356628894806 + 1.0 * 6.143096923828125
Epoch 420, val loss: 0.7566832900047302
Epoch 430, training loss: 6.5471391677856445 = 0.4108278751373291 + 1.0 * 6.1363115310668945
Epoch 430, val loss: 0.7441989183425903
Epoch 440, training loss: 6.518646717071533 = 0.38683122396469116 + 1.0 * 6.131815433502197
Epoch 440, val loss: 0.7336975336074829
Epoch 450, training loss: 6.492781162261963 = 0.3637360632419586 + 1.0 * 6.129045009613037
Epoch 450, val loss: 0.7243555188179016
Epoch 460, training loss: 6.468006610870361 = 0.3414634168148041 + 1.0 * 6.126543045043945
Epoch 460, val loss: 0.7161946892738342
Epoch 470, training loss: 6.447258472442627 = 0.3201080858707428 + 1.0 * 6.127150535583496
Epoch 470, val loss: 0.7092561721801758
Epoch 480, training loss: 6.422633171081543 = 0.2998397648334503 + 1.0 * 6.122793197631836
Epoch 480, val loss: 0.7036156058311462
Epoch 490, training loss: 6.400700569152832 = 0.2804892063140869 + 1.0 * 6.120211124420166
Epoch 490, val loss: 0.6992191076278687
Epoch 500, training loss: 6.382224082946777 = 0.2620258331298828 + 1.0 * 6.1201982498168945
Epoch 500, val loss: 0.69573974609375
Epoch 510, training loss: 6.362161159515381 = 0.24455685913562775 + 1.0 * 6.1176042556762695
Epoch 510, val loss: 0.6932945251464844
Epoch 520, training loss: 6.343911170959473 = 0.2281228005886078 + 1.0 * 6.115788459777832
Epoch 520, val loss: 0.6919770836830139
Epoch 530, training loss: 6.325798511505127 = 0.21266265213489532 + 1.0 * 6.113135814666748
Epoch 530, val loss: 0.6915179491043091
Epoch 540, training loss: 6.316100120544434 = 0.19821391999721527 + 1.0 * 6.117886066436768
Epoch 540, val loss: 0.6918583512306213
Epoch 550, training loss: 6.307656288146973 = 0.18485473096370697 + 1.0 * 6.122801780700684
Epoch 550, val loss: 0.6930631995201111
Epoch 560, training loss: 6.284328937530518 = 0.17266319692134857 + 1.0 * 6.111665725708008
Epoch 560, val loss: 0.6950032711029053
Epoch 570, training loss: 6.269632339477539 = 0.16144415736198425 + 1.0 * 6.108188152313232
Epoch 570, val loss: 0.6977913975715637
Epoch 580, training loss: 6.256007671356201 = 0.15107361972332 + 1.0 * 6.104934215545654
Epoch 580, val loss: 0.7010096907615662
Epoch 590, training loss: 6.244915008544922 = 0.1414853185415268 + 1.0 * 6.103429794311523
Epoch 590, val loss: 0.7048251032829285
Epoch 600, training loss: 6.234943866729736 = 0.1326899379491806 + 1.0 * 6.1022539138793945
Epoch 600, val loss: 0.7090405225753784
Epoch 610, training loss: 6.22680139541626 = 0.12466856837272644 + 1.0 * 6.102132797241211
Epoch 610, val loss: 0.7137834429740906
Epoch 620, training loss: 6.218128681182861 = 0.11729911714792252 + 1.0 * 6.100829601287842
Epoch 620, val loss: 0.7189248204231262
Epoch 630, training loss: 6.209150314331055 = 0.11051348596811295 + 1.0 * 6.098636627197266
Epoch 630, val loss: 0.7242906093597412
Epoch 640, training loss: 6.203880310058594 = 0.10427457839250565 + 1.0 * 6.099605560302734
Epoch 640, val loss: 0.729778528213501
Epoch 650, training loss: 6.196573734283447 = 0.09856251627206802 + 1.0 * 6.098011016845703
Epoch 650, val loss: 0.7356286644935608
Epoch 660, training loss: 6.186146259307861 = 0.09327982366085052 + 1.0 * 6.09286642074585
Epoch 660, val loss: 0.7416942715644836
Epoch 670, training loss: 6.1852521896362305 = 0.08837304264307022 + 1.0 * 6.096879005432129
Epoch 670, val loss: 0.7477079033851624
Epoch 680, training loss: 6.177312850952148 = 0.08382495492696762 + 1.0 * 6.093487739562988
Epoch 680, val loss: 0.7538699507713318
Epoch 690, training loss: 6.170330047607422 = 0.07961033284664154 + 1.0 * 6.090719699859619
Epoch 690, val loss: 0.7602614760398865
Epoch 700, training loss: 6.164128303527832 = 0.07567616552114487 + 1.0 * 6.088452339172363
Epoch 700, val loss: 0.7666059136390686
Epoch 710, training loss: 6.166168212890625 = 0.07200486212968826 + 1.0 * 6.094163417816162
Epoch 710, val loss: 0.7729107737541199
Epoch 720, training loss: 6.153566360473633 = 0.06859516352415085 + 1.0 * 6.0849714279174805
Epoch 720, val loss: 0.7792745232582092
Epoch 730, training loss: 6.149899959564209 = 0.06541590392589569 + 1.0 * 6.084484100341797
Epoch 730, val loss: 0.7858375310897827
Epoch 740, training loss: 6.145209789276123 = 0.06242462992668152 + 1.0 * 6.082785129547119
Epoch 740, val loss: 0.7922763228416443
Epoch 750, training loss: 6.147776126861572 = 0.05961186811327934 + 1.0 * 6.088164329528809
Epoch 750, val loss: 0.7986581325531006
Epoch 760, training loss: 6.144139289855957 = 0.05698780342936516 + 1.0 * 6.087151527404785
Epoch 760, val loss: 0.8050001859664917
Epoch 770, training loss: 6.133460521697998 = 0.05453546345233917 + 1.0 * 6.078925132751465
Epoch 770, val loss: 0.8115242719650269
Epoch 780, training loss: 6.1310200691223145 = 0.05222420021891594 + 1.0 * 6.078795909881592
Epoch 780, val loss: 0.8179388642311096
Epoch 790, training loss: 6.136662006378174 = 0.050044503062963486 + 1.0 * 6.086617469787598
Epoch 790, val loss: 0.8242160081863403
Epoch 800, training loss: 6.125668525695801 = 0.04800790175795555 + 1.0 * 6.07766056060791
Epoch 800, val loss: 0.8304886221885681
Epoch 810, training loss: 6.122300148010254 = 0.046090248972177505 + 1.0 * 6.076210021972656
Epoch 810, val loss: 0.8369917869567871
Epoch 820, training loss: 6.118269920349121 = 0.04427464306354523 + 1.0 * 6.073995113372803
Epoch 820, val loss: 0.8432430624961853
Epoch 830, training loss: 6.12126350402832 = 0.042554959654808044 + 1.0 * 6.078708648681641
Epoch 830, val loss: 0.8494405150413513
Epoch 840, training loss: 6.114950180053711 = 0.04093340039253235 + 1.0 * 6.074016571044922
Epoch 840, val loss: 0.8556584119796753
Epoch 850, training loss: 6.1147685050964355 = 0.039402272552251816 + 1.0 * 6.075366020202637
Epoch 850, val loss: 0.8618723750114441
Epoch 860, training loss: 6.109867095947266 = 0.03795246779918671 + 1.0 * 6.0719146728515625
Epoch 860, val loss: 0.8678812384605408
Epoch 870, training loss: 6.106586933135986 = 0.0365871898829937 + 1.0 * 6.069999694824219
Epoch 870, val loss: 0.8740137815475464
Epoch 880, training loss: 6.102816581726074 = 0.03528609126806259 + 1.0 * 6.067530632019043
Epoch 880, val loss: 0.8800715208053589
Epoch 890, training loss: 6.111909866333008 = 0.034048035740852356 + 1.0 * 6.077861785888672
Epoch 890, val loss: 0.8859713077545166
Epoch 900, training loss: 6.102116107940674 = 0.03288090229034424 + 1.0 * 6.069235324859619
Epoch 900, val loss: 0.891819179058075
Epoch 910, training loss: 6.098427772521973 = 0.03177144378423691 + 1.0 * 6.066656112670898
Epoch 910, val loss: 0.8977938890457153
Epoch 920, training loss: 6.099294185638428 = 0.030716851353645325 + 1.0 * 6.068577289581299
Epoch 920, val loss: 0.903455376625061
Epoch 930, training loss: 6.093446254730225 = 0.02971678227186203 + 1.0 * 6.063729286193848
Epoch 930, val loss: 0.909095048904419
Epoch 940, training loss: 6.090903282165527 = 0.0287639070302248 + 1.0 * 6.062139511108398
Epoch 940, val loss: 0.9148832559585571
Epoch 950, training loss: 6.091182708740234 = 0.027852745726704597 + 1.0 * 6.063330173492432
Epoch 950, val loss: 0.920496940612793
Epoch 960, training loss: 6.094396114349365 = 0.02698298543691635 + 1.0 * 6.067413330078125
Epoch 960, val loss: 0.9258499145507812
Epoch 970, training loss: 6.087307453155518 = 0.026159845292568207 + 1.0 * 6.061147689819336
Epoch 970, val loss: 0.931380033493042
Epoch 980, training loss: 6.085579872131348 = 0.025374004617333412 + 1.0 * 6.060205936431885
Epoch 980, val loss: 0.9369179606437683
Epoch 990, training loss: 6.086519718170166 = 0.02461792156100273 + 1.0 * 6.061901569366455
Epoch 990, val loss: 0.9422398209571838
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.32032585144043 = 1.946433663368225 + 1.0 * 8.373891830444336
Epoch 0, val loss: 1.9430092573165894
Epoch 10, training loss: 10.31017017364502 = 1.9366577863693237 + 1.0 * 8.373512268066406
Epoch 10, val loss: 1.9340779781341553
Epoch 20, training loss: 10.295117378234863 = 1.9242475032806396 + 1.0 * 8.370869636535645
Epoch 20, val loss: 1.9223781824111938
Epoch 30, training loss: 10.258211135864258 = 1.906657338142395 + 1.0 * 8.351553916931152
Epoch 30, val loss: 1.9056075811386108
Epoch 40, training loss: 10.110912322998047 = 1.8836922645568848 + 1.0 * 8.22722053527832
Epoch 40, val loss: 1.8845287561416626
Epoch 50, training loss: 9.58686351776123 = 1.8586920499801636 + 1.0 * 7.7281718254089355
Epoch 50, val loss: 1.862181544303894
Epoch 60, training loss: 9.172245025634766 = 1.8364338874816895 + 1.0 * 7.335811138153076
Epoch 60, val loss: 1.8429160118103027
Epoch 70, training loss: 8.864521026611328 = 1.8192778825759888 + 1.0 * 7.045243263244629
Epoch 70, val loss: 1.8274708986282349
Epoch 80, training loss: 8.669954299926758 = 1.8001375198364258 + 1.0 * 6.869816780090332
Epoch 80, val loss: 1.8100749254226685
Epoch 90, training loss: 8.53211784362793 = 1.7790995836257935 + 1.0 * 6.753017902374268
Epoch 90, val loss: 1.7922022342681885
Epoch 100, training loss: 8.407804489135742 = 1.7585725784301758 + 1.0 * 6.649231433868408
Epoch 100, val loss: 1.7754145860671997
Epoch 110, training loss: 8.299497604370117 = 1.738781213760376 + 1.0 * 6.560716152191162
Epoch 110, val loss: 1.758633017539978
Epoch 120, training loss: 8.208870887756348 = 1.7177380323410034 + 1.0 * 6.491133213043213
Epoch 120, val loss: 1.7402523756027222
Epoch 130, training loss: 8.134610176086426 = 1.6943610906600952 + 1.0 * 6.440248966217041
Epoch 130, val loss: 1.719897747039795
Epoch 140, training loss: 8.068406105041504 = 1.66750967502594 + 1.0 * 6.4008965492248535
Epoch 140, val loss: 1.6968512535095215
Epoch 150, training loss: 8.007637023925781 = 1.6364104747772217 + 1.0 * 6.3712263107299805
Epoch 150, val loss: 1.6704318523406982
Epoch 160, training loss: 7.948567867279053 = 1.6007126569747925 + 1.0 * 6.347855091094971
Epoch 160, val loss: 1.6404682397842407
Epoch 170, training loss: 7.888326644897461 = 1.5604989528656006 + 1.0 * 6.327827453613281
Epoch 170, val loss: 1.6069997549057007
Epoch 180, training loss: 7.828113555908203 = 1.515810251235962 + 1.0 * 6.31230354309082
Epoch 180, val loss: 1.570088267326355
Epoch 190, training loss: 7.764348030090332 = 1.4680328369140625 + 1.0 * 6.2963151931762695
Epoch 190, val loss: 1.5308492183685303
Epoch 200, training loss: 7.699541091918945 = 1.41754150390625 + 1.0 * 6.281999588012695
Epoch 200, val loss: 1.4895970821380615
Epoch 210, training loss: 7.635716438293457 = 1.3650479316711426 + 1.0 * 6.2706685066223145
Epoch 210, val loss: 1.4471383094787598
Epoch 220, training loss: 7.568345546722412 = 1.3120427131652832 + 1.0 * 6.256302833557129
Epoch 220, val loss: 1.4045417308807373
Epoch 230, training loss: 7.503513813018799 = 1.258454442024231 + 1.0 * 6.245059490203857
Epoch 230, val loss: 1.3617477416992188
Epoch 240, training loss: 7.44185209274292 = 1.2052316665649414 + 1.0 * 6.2366204261779785
Epoch 240, val loss: 1.3196029663085938
Epoch 250, training loss: 7.3778157234191895 = 1.1532081365585327 + 1.0 * 6.224607467651367
Epoch 250, val loss: 1.278855800628662
Epoch 260, training loss: 7.318114280700684 = 1.1026999950408936 + 1.0 * 6.215414524078369
Epoch 260, val loss: 1.2394787073135376
Epoch 270, training loss: 7.260613441467285 = 1.0533688068389893 + 1.0 * 6.207244873046875
Epoch 270, val loss: 1.201342225074768
Epoch 280, training loss: 7.208976745605469 = 1.0055679082870483 + 1.0 * 6.203408718109131
Epoch 280, val loss: 1.164759874343872
Epoch 290, training loss: 7.154287338256836 = 0.9596013426780701 + 1.0 * 6.194685935974121
Epoch 290, val loss: 1.1297686100006104
Epoch 300, training loss: 7.1018385887146 = 0.9146910905838013 + 1.0 * 6.187147617340088
Epoch 300, val loss: 1.0959300994873047
Epoch 310, training loss: 7.05892276763916 = 0.870398759841919 + 1.0 * 6.188523769378662
Epoch 310, val loss: 1.062721610069275
Epoch 320, training loss: 7.005656719207764 = 0.82679283618927 + 1.0 * 6.178864002227783
Epoch 320, val loss: 1.0302573442459106
Epoch 330, training loss: 6.956099033355713 = 0.7837238907814026 + 1.0 * 6.172375202178955
Epoch 330, val loss: 0.9984377026557922
Epoch 340, training loss: 6.912905693054199 = 0.7411803007125854 + 1.0 * 6.171725273132324
Epoch 340, val loss: 0.9672267436981201
Epoch 350, training loss: 6.865533351898193 = 0.6997919082641602 + 1.0 * 6.165741443634033
Epoch 350, val loss: 0.9372564554214478
Epoch 360, training loss: 6.821002006530762 = 0.6602623462677002 + 1.0 * 6.160739898681641
Epoch 360, val loss: 0.9090947508811951
Epoch 370, training loss: 6.786708831787109 = 0.6229261159896851 + 1.0 * 6.163782596588135
Epoch 370, val loss: 0.8829858303070068
Epoch 380, training loss: 6.742591857910156 = 0.5883519649505615 + 1.0 * 6.154240131378174
Epoch 380, val loss: 0.8595956563949585
Epoch 390, training loss: 6.705928325653076 = 0.5565792918205261 + 1.0 * 6.149349212646484
Epoch 390, val loss: 0.8389854431152344
Epoch 400, training loss: 6.6789374351501465 = 0.5275086760520935 + 1.0 * 6.151428699493408
Epoch 400, val loss: 0.8210759162902832
Epoch 410, training loss: 6.650962829589844 = 0.5011235475540161 + 1.0 * 6.149839401245117
Epoch 410, val loss: 0.8058401942253113
Epoch 420, training loss: 6.6182861328125 = 0.47728949785232544 + 1.0 * 6.14099645614624
Epoch 420, val loss: 0.7930560111999512
Epoch 430, training loss: 6.591999530792236 = 0.4554549753665924 + 1.0 * 6.136544704437256
Epoch 430, val loss: 0.7823719382286072
Epoch 440, training loss: 6.568737983703613 = 0.4351893961429596 + 1.0 * 6.133548736572266
Epoch 440, val loss: 0.7732662558555603
Epoch 450, training loss: 6.554629802703857 = 0.4161590039730072 + 1.0 * 6.138470649719238
Epoch 450, val loss: 0.7654129862785339
Epoch 460, training loss: 6.528362274169922 = 0.39841797947883606 + 1.0 * 6.129944324493408
Epoch 460, val loss: 0.7586438655853271
Epoch 470, training loss: 6.507328510284424 = 0.3816055953502655 + 1.0 * 6.125722885131836
Epoch 470, val loss: 0.7528905868530273
Epoch 480, training loss: 6.488762855529785 = 0.36544764041900635 + 1.0 * 6.123315334320068
Epoch 480, val loss: 0.7477554678916931
Epoch 490, training loss: 6.473097801208496 = 0.34978413581848145 + 1.0 * 6.1233134269714355
Epoch 490, val loss: 0.7430911660194397
Epoch 500, training loss: 6.463590621948242 = 0.3346194624900818 + 1.0 * 6.128971099853516
Epoch 500, val loss: 0.7387461066246033
Epoch 510, training loss: 6.438817501068115 = 0.31991735100746155 + 1.0 * 6.118900299072266
Epoch 510, val loss: 0.7349298596382141
Epoch 520, training loss: 6.420650482177734 = 0.30554404854774475 + 1.0 * 6.115106582641602
Epoch 520, val loss: 0.7314451336860657
Epoch 530, training loss: 6.42072868347168 = 0.2914343476295471 + 1.0 * 6.129294395446777
Epoch 530, val loss: 0.728131890296936
Epoch 540, training loss: 6.39240026473999 = 0.2777860462665558 + 1.0 * 6.114614009857178
Epoch 540, val loss: 0.7250654697418213
Epoch 550, training loss: 6.374679088592529 = 0.2644278109073639 + 1.0 * 6.110251426696777
Epoch 550, val loss: 0.7223652601242065
Epoch 560, training loss: 6.358992576599121 = 0.25135329365730286 + 1.0 * 6.107639312744141
Epoch 560, val loss: 0.7198541164398193
Epoch 570, training loss: 6.358705520629883 = 0.2385522723197937 + 1.0 * 6.120153427124023
Epoch 570, val loss: 0.7174814343452454
Epoch 580, training loss: 6.331861972808838 = 0.22617000341415405 + 1.0 * 6.105691909790039
Epoch 580, val loss: 0.7153046727180481
Epoch 590, training loss: 6.317925453186035 = 0.21413767337799072 + 1.0 * 6.103787899017334
Epoch 590, val loss: 0.7134943008422852
Epoch 600, training loss: 6.306414604187012 = 0.20247550308704376 + 1.0 * 6.103939056396484
Epoch 600, val loss: 0.7119190692901611
Epoch 610, training loss: 6.2920756340026855 = 0.19122876226902008 + 1.0 * 6.100846767425537
Epoch 610, val loss: 0.710449755191803
Epoch 620, training loss: 6.280942440032959 = 0.18046170473098755 + 1.0 * 6.100480556488037
Epoch 620, val loss: 0.7094123363494873
Epoch 630, training loss: 6.26706075668335 = 0.1701149046421051 + 1.0 * 6.096945762634277
Epoch 630, val loss: 0.7086454629898071
Epoch 640, training loss: 6.262045860290527 = 0.1602514535188675 + 1.0 * 6.101794242858887
Epoch 640, val loss: 0.7081651091575623
Epoch 650, training loss: 6.251578330993652 = 0.15088705718517303 + 1.0 * 6.100691318511963
Epoch 650, val loss: 0.707902729511261
Epoch 660, training loss: 6.236213684082031 = 0.14207209646701813 + 1.0 * 6.094141483306885
Epoch 660, val loss: 0.7079607844352722
Epoch 670, training loss: 6.225558757781982 = 0.13376910984516144 + 1.0 * 6.091789722442627
Epoch 670, val loss: 0.7083641886711121
Epoch 680, training loss: 6.216712474822998 = 0.1259404420852661 + 1.0 * 6.0907721519470215
Epoch 680, val loss: 0.7090619802474976
Epoch 690, training loss: 6.218292236328125 = 0.11860230565071106 + 1.0 * 6.099689960479736
Epoch 690, val loss: 0.7100024223327637
Epoch 700, training loss: 6.20424222946167 = 0.11173957586288452 + 1.0 * 6.092502593994141
Epoch 700, val loss: 0.7111730575561523
Epoch 710, training loss: 6.192262172698975 = 0.1053505390882492 + 1.0 * 6.086911678314209
Epoch 710, val loss: 0.7126787304878235
Epoch 720, training loss: 6.185359954833984 = 0.09937819838523865 + 1.0 * 6.085981845855713
Epoch 720, val loss: 0.714462161064148
Epoch 730, training loss: 6.1857523918151855 = 0.09379726648330688 + 1.0 * 6.091955184936523
Epoch 730, val loss: 0.7164869904518127
Epoch 740, training loss: 6.182445049285889 = 0.0886230319738388 + 1.0 * 6.093822002410889
Epoch 740, val loss: 0.7187173366546631
Epoch 750, training loss: 6.16664457321167 = 0.08378039300441742 + 1.0 * 6.082864284515381
Epoch 750, val loss: 0.7211204767227173
Epoch 760, training loss: 6.160671234130859 = 0.07928384840488434 + 1.0 * 6.081387519836426
Epoch 760, val loss: 0.7237804532051086
Epoch 770, training loss: 6.154837608337402 = 0.07507225126028061 + 1.0 * 6.079765319824219
Epoch 770, val loss: 0.7266523838043213
Epoch 780, training loss: 6.159667491912842 = 0.07113778591156006 + 1.0 * 6.088529586791992
Epoch 780, val loss: 0.7297022342681885
Epoch 790, training loss: 6.14861536026001 = 0.06747125834226608 + 1.0 * 6.081144332885742
Epoch 790, val loss: 0.7328049540519714
Epoch 800, training loss: 6.142382621765137 = 0.06405165046453476 + 1.0 * 6.078330993652344
Epoch 800, val loss: 0.7361692190170288
Epoch 810, training loss: 6.139846324920654 = 0.0608634315431118 + 1.0 * 6.078982830047607
Epoch 810, val loss: 0.7396865487098694
Epoch 820, training loss: 6.131612777709961 = 0.057877067476511 + 1.0 * 6.07373571395874
Epoch 820, val loss: 0.7432244420051575
Epoch 830, training loss: 6.128509998321533 = 0.0550924688577652 + 1.0 * 6.073417663574219
Epoch 830, val loss: 0.746929407119751
Epoch 840, training loss: 6.124830722808838 = 0.052477769553661346 + 1.0 * 6.072352886199951
Epoch 840, val loss: 0.7507602572441101
Epoch 850, training loss: 6.128965854644775 = 0.0500318743288517 + 1.0 * 6.078934192657471
Epoch 850, val loss: 0.7546043992042542
Epoch 860, training loss: 6.119178771972656 = 0.04774630442261696 + 1.0 * 6.071432590484619
Epoch 860, val loss: 0.7585508823394775
Epoch 870, training loss: 6.115271091461182 = 0.04560608044266701 + 1.0 * 6.06966495513916
Epoch 870, val loss: 0.7626007199287415
Epoch 880, training loss: 6.1214704513549805 = 0.04359794408082962 + 1.0 * 6.077872276306152
Epoch 880, val loss: 0.7666671276092529
Epoch 890, training loss: 6.108153343200684 = 0.041711170226335526 + 1.0 * 6.066442012786865
Epoch 890, val loss: 0.7707545757293701
Epoch 900, training loss: 6.106457233428955 = 0.0399395152926445 + 1.0 * 6.0665178298950195
Epoch 900, val loss: 0.7749290466308594
Epoch 910, training loss: 6.113351345062256 = 0.038267459720373154 + 1.0 * 6.0750837326049805
Epoch 910, val loss: 0.7791045308113098
Epoch 920, training loss: 6.108412742614746 = 0.036715444177389145 + 1.0 * 6.071697235107422
Epoch 920, val loss: 0.7833343148231506
Epoch 930, training loss: 6.098135471343994 = 0.03523828461766243 + 1.0 * 6.062897205352783
Epoch 930, val loss: 0.787531852722168
Epoch 940, training loss: 6.09626579284668 = 0.033849555999040604 + 1.0 * 6.062416076660156
Epoch 940, val loss: 0.7917602062225342
Epoch 950, training loss: 6.097836971282959 = 0.03253879025578499 + 1.0 * 6.065298080444336
Epoch 950, val loss: 0.796061635017395
Epoch 960, training loss: 6.092894077301025 = 0.03129694610834122 + 1.0 * 6.0615973472595215
Epoch 960, val loss: 0.8001936674118042
Epoch 970, training loss: 6.092204570770264 = 0.03013009950518608 + 1.0 * 6.062074661254883
Epoch 970, val loss: 0.8044166564941406
Epoch 980, training loss: 6.095943450927734 = 0.02902478165924549 + 1.0 * 6.066918849945068
Epoch 980, val loss: 0.8085722327232361
Epoch 990, training loss: 6.089149475097656 = 0.027979526668787003 + 1.0 * 6.0611701011657715
Epoch 990, val loss: 0.8127357363700867
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6125
Flip ASR: 0.5467/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.305209159851074 = 1.931307077407837 + 1.0 * 8.373902320861816
Epoch 0, val loss: 1.9289082288742065
Epoch 10, training loss: 10.2944974899292 = 1.9209433794021606 + 1.0 * 8.373554229736328
Epoch 10, val loss: 1.9183510541915894
Epoch 20, training loss: 10.278590202331543 = 1.9076039791107178 + 1.0 * 8.370985984802246
Epoch 20, val loss: 1.9048326015472412
Epoch 30, training loss: 10.239166259765625 = 1.8887228965759277 + 1.0 * 8.350442886352539
Epoch 30, val loss: 1.8859907388687134
Epoch 40, training loss: 10.070685386657715 = 1.8643933534622192 + 1.0 * 8.206292152404785
Epoch 40, val loss: 1.8631196022033691
Epoch 50, training loss: 9.650869369506836 = 1.8386054039001465 + 1.0 * 7.812263488769531
Epoch 50, val loss: 1.839694619178772
Epoch 60, training loss: 9.277230262756348 = 1.8171265125274658 + 1.0 * 7.460103511810303
Epoch 60, val loss: 1.8204096555709839
Epoch 70, training loss: 8.840554237365723 = 1.8035962581634521 + 1.0 * 7.036957740783691
Epoch 70, val loss: 1.807931900024414
Epoch 80, training loss: 8.590373992919922 = 1.7915666103363037 + 1.0 * 6.798807144165039
Epoch 80, val loss: 1.796311616897583
Epoch 90, training loss: 8.416417121887207 = 1.7773479223251343 + 1.0 * 6.639069080352783
Epoch 90, val loss: 1.7826834917068481
Epoch 100, training loss: 8.308302879333496 = 1.761611819267273 + 1.0 * 6.546690940856934
Epoch 100, val loss: 1.7680996656417847
Epoch 110, training loss: 8.230588912963867 = 1.7443121671676636 + 1.0 * 6.486276626586914
Epoch 110, val loss: 1.7522579431533813
Epoch 120, training loss: 8.169017791748047 = 1.724712610244751 + 1.0 * 6.444305419921875
Epoch 120, val loss: 1.7345706224441528
Epoch 130, training loss: 8.109231948852539 = 1.7024521827697754 + 1.0 * 6.406780242919922
Epoch 130, val loss: 1.7147839069366455
Epoch 140, training loss: 8.055830955505371 = 1.6765379905700684 + 1.0 * 6.379292964935303
Epoch 140, val loss: 1.6922378540039062
Epoch 150, training loss: 8.00216293334961 = 1.6463732719421387 + 1.0 * 6.355789661407471
Epoch 150, val loss: 1.6665687561035156
Epoch 160, training loss: 7.945126533508301 = 1.612111210823059 + 1.0 * 6.333015441894531
Epoch 160, val loss: 1.637574315071106
Epoch 170, training loss: 7.887332916259766 = 1.5734151601791382 + 1.0 * 6.313917636871338
Epoch 170, val loss: 1.6048696041107178
Epoch 180, training loss: 7.830519199371338 = 1.5303417444229126 + 1.0 * 6.300177574157715
Epoch 180, val loss: 1.568649172782898
Epoch 190, training loss: 7.767168045043945 = 1.4844861030578613 + 1.0 * 6.282681941986084
Epoch 190, val loss: 1.530311107635498
Epoch 200, training loss: 7.704998016357422 = 1.4368027448654175 + 1.0 * 6.268195152282715
Epoch 200, val loss: 1.49103581905365
Epoch 210, training loss: 7.644172668457031 = 1.3883171081542969 + 1.0 * 6.255855560302734
Epoch 210, val loss: 1.4517309665679932
Epoch 220, training loss: 7.583432674407959 = 1.339099407196045 + 1.0 * 6.244333267211914
Epoch 220, val loss: 1.4126163721084595
Epoch 230, training loss: 7.526733875274658 = 1.289748191833496 + 1.0 * 6.236985683441162
Epoch 230, val loss: 1.3743067979812622
Epoch 240, training loss: 7.468293190002441 = 1.2413674592971802 + 1.0 * 6.226925849914551
Epoch 240, val loss: 1.337276816368103
Epoch 250, training loss: 7.412087917327881 = 1.1929636001586914 + 1.0 * 6.2191243171691895
Epoch 250, val loss: 1.3008191585540771
Epoch 260, training loss: 7.35670280456543 = 1.1442179679870605 + 1.0 * 6.212484836578369
Epoch 260, val loss: 1.2643485069274902
Epoch 270, training loss: 7.30802059173584 = 1.0949902534484863 + 1.0 * 6.2130303382873535
Epoch 270, val loss: 1.227848768234253
Epoch 280, training loss: 7.248647689819336 = 1.046133279800415 + 1.0 * 6.2025146484375
Epoch 280, val loss: 1.1916433572769165
Epoch 290, training loss: 7.19802188873291 = 0.9977240562438965 + 1.0 * 6.200297832489014
Epoch 290, val loss: 1.155945897102356
Epoch 300, training loss: 7.142591953277588 = 0.9501187205314636 + 1.0 * 6.192473411560059
Epoch 300, val loss: 1.1208434104919434
Epoch 310, training loss: 7.094029903411865 = 0.9034577012062073 + 1.0 * 6.190572261810303
Epoch 310, val loss: 1.0864977836608887
Epoch 320, training loss: 7.0424017906188965 = 0.8585313558578491 + 1.0 * 6.183870315551758
Epoch 320, val loss: 1.0536255836486816
Epoch 330, training loss: 6.994628429412842 = 0.8156973123550415 + 1.0 * 6.17893123626709
Epoch 330, val loss: 1.022605061531067
Epoch 340, training loss: 6.9518513679504395 = 0.7750502824783325 + 1.0 * 6.1768012046813965
Epoch 340, val loss: 0.9936506748199463
Epoch 350, training loss: 6.910874366760254 = 0.7369697093963623 + 1.0 * 6.1739044189453125
Epoch 350, val loss: 0.9672565460205078
Epoch 360, training loss: 6.876749038696289 = 0.7016537189483643 + 1.0 * 6.175095558166504
Epoch 360, val loss: 0.9436624050140381
Epoch 370, training loss: 6.833466053009033 = 0.6690570712089539 + 1.0 * 6.164409160614014
Epoch 370, val loss: 0.9228087067604065
Epoch 380, training loss: 6.797947406768799 = 0.6385872960090637 + 1.0 * 6.159359931945801
Epoch 380, val loss: 0.904392659664154
Epoch 390, training loss: 6.775007724761963 = 0.6098893284797668 + 1.0 * 6.165118217468262
Epoch 390, val loss: 0.8880457282066345
Epoch 400, training loss: 6.7378082275390625 = 0.5833130478858948 + 1.0 * 6.1544952392578125
Epoch 400, val loss: 0.8737614750862122
Epoch 410, training loss: 6.709476470947266 = 0.5584386587142944 + 1.0 * 6.151037693023682
Epoch 410, val loss: 0.8616140484809875
Epoch 420, training loss: 6.680578231811523 = 0.5349177122116089 + 1.0 * 6.145660400390625
Epoch 420, val loss: 0.8510592579841614
Epoch 430, training loss: 6.656035423278809 = 0.5125390291213989 + 1.0 * 6.143496513366699
Epoch 430, val loss: 0.8420114517211914
Epoch 440, training loss: 6.63153076171875 = 0.4912986755371094 + 1.0 * 6.140232086181641
Epoch 440, val loss: 0.8343865871429443
Epoch 450, training loss: 6.609830856323242 = 0.47111669182777405 + 1.0 * 6.13871431350708
Epoch 450, val loss: 0.8280801773071289
Epoch 460, training loss: 6.588835716247559 = 0.4518674910068512 + 1.0 * 6.13696813583374
Epoch 460, val loss: 0.8228545188903809
Epoch 470, training loss: 6.568634986877441 = 0.43359482288360596 + 1.0 * 6.135040283203125
Epoch 470, val loss: 0.8188427686691284
Epoch 480, training loss: 6.547186374664307 = 0.4160062074661255 + 1.0 * 6.131180286407471
Epoch 480, val loss: 0.8159409165382385
Epoch 490, training loss: 6.526778221130371 = 0.39898228645324707 + 1.0 * 6.127795696258545
Epoch 490, val loss: 0.8139327168464661
Epoch 500, training loss: 6.510995388031006 = 0.38250306248664856 + 1.0 * 6.12849235534668
Epoch 500, val loss: 0.8128001093864441
Epoch 510, training loss: 6.497267723083496 = 0.3666653335094452 + 1.0 * 6.1306023597717285
Epoch 510, val loss: 0.8124034404754639
Epoch 520, training loss: 6.473411560058594 = 0.35162678360939026 + 1.0 * 6.121784687042236
Epoch 520, val loss: 0.8130066990852356
Epoch 530, training loss: 6.456911087036133 = 0.33718714118003845 + 1.0 * 6.119723796844482
Epoch 530, val loss: 0.8143802285194397
Epoch 540, training loss: 6.440913200378418 = 0.323346883058548 + 1.0 * 6.117566108703613
Epoch 540, val loss: 0.8164775371551514
Epoch 550, training loss: 6.4357829093933105 = 0.31011855602264404 + 1.0 * 6.125664234161377
Epoch 550, val loss: 0.8193470239639282
Epoch 560, training loss: 6.413294792175293 = 0.29764580726623535 + 1.0 * 6.115649223327637
Epoch 560, val loss: 0.822632372379303
Epoch 570, training loss: 6.3987507820129395 = 0.2858375906944275 + 1.0 * 6.112913131713867
Epoch 570, val loss: 0.826662003993988
Epoch 580, training loss: 6.385834217071533 = 0.2745901644229889 + 1.0 * 6.111244201660156
Epoch 580, val loss: 0.8312146067619324
Epoch 590, training loss: 6.378767490386963 = 0.263857901096344 + 1.0 * 6.114909648895264
Epoch 590, val loss: 0.8362076878547668
Epoch 600, training loss: 6.3667097091674805 = 0.2536395192146301 + 1.0 * 6.113070011138916
Epoch 600, val loss: 0.8416349291801453
Epoch 610, training loss: 6.351302146911621 = 0.24382156133651733 + 1.0 * 6.107480525970459
Epoch 610, val loss: 0.8473066687583923
Epoch 620, training loss: 6.3412909507751465 = 0.23438787460327148 + 1.0 * 6.106903076171875
Epoch 620, val loss: 0.8532693982124329
Epoch 630, training loss: 6.329684257507324 = 0.2252565622329712 + 1.0 * 6.104427814483643
Epoch 630, val loss: 0.8594865202903748
Epoch 640, training loss: 6.322168350219727 = 0.21636654436588287 + 1.0 * 6.105801582336426
Epoch 640, val loss: 0.8658849000930786
Epoch 650, training loss: 6.308164596557617 = 0.20770251750946045 + 1.0 * 6.100461959838867
Epoch 650, val loss: 0.8724740743637085
Epoch 660, training loss: 6.305213928222656 = 0.1992269903421402 + 1.0 * 6.105987071990967
Epoch 660, val loss: 0.8792759776115417
Epoch 670, training loss: 6.292726516723633 = 0.19092795252799988 + 1.0 * 6.1017985343933105
Epoch 670, val loss: 0.8860370516777039
Epoch 680, training loss: 6.280473709106445 = 0.18281835317611694 + 1.0 * 6.097655296325684
Epoch 680, val loss: 0.8931228518486023
Epoch 690, training loss: 6.269479751586914 = 0.17486627399921417 + 1.0 * 6.094613552093506
Epoch 690, val loss: 0.9003372192382812
Epoch 700, training loss: 6.265729904174805 = 0.16707131266593933 + 1.0 * 6.098658561706543
Epoch 700, val loss: 0.9076810479164124
Epoch 710, training loss: 6.254748344421387 = 0.1595289558172226 + 1.0 * 6.095219612121582
Epoch 710, val loss: 0.9150972962379456
Epoch 720, training loss: 6.245124340057373 = 0.15218837559223175 + 1.0 * 6.092936038970947
Epoch 720, val loss: 0.9226202368736267
Epoch 730, training loss: 6.24260139465332 = 0.1451178640127182 + 1.0 * 6.0974836349487305
Epoch 730, val loss: 0.9301294088363647
Epoch 740, training loss: 6.228963851928711 = 0.13833346962928772 + 1.0 * 6.090630531311035
Epoch 740, val loss: 0.9376983046531677
Epoch 750, training loss: 6.221097469329834 = 0.1318187415599823 + 1.0 * 6.089278697967529
Epoch 750, val loss: 0.9452327489852905
Epoch 760, training loss: 6.215257167816162 = 0.1255812793970108 + 1.0 * 6.0896759033203125
Epoch 760, val loss: 0.9527527689933777
Epoch 770, training loss: 6.212116718292236 = 0.11964470893144608 + 1.0 * 6.092472076416016
Epoch 770, val loss: 0.9601504802703857
Epoch 780, training loss: 6.199965476989746 = 0.11397199332714081 + 1.0 * 6.08599328994751
Epoch 780, val loss: 0.96736079454422
Epoch 790, training loss: 6.191648006439209 = 0.10857247561216354 + 1.0 * 6.083075523376465
Epoch 790, val loss: 0.9746155738830566
Epoch 800, training loss: 6.1921706199646 = 0.10341878235340118 + 1.0 * 6.088751792907715
Epoch 800, val loss: 0.9817233681678772
Epoch 810, training loss: 6.180447101593018 = 0.0985221415758133 + 1.0 * 6.081924915313721
Epoch 810, val loss: 0.9887111186981201
Epoch 820, training loss: 6.174161434173584 = 0.09386847913265228 + 1.0 * 6.08029317855835
Epoch 820, val loss: 0.9957160353660583
Epoch 830, training loss: 6.172112464904785 = 0.08942588418722153 + 1.0 * 6.082686424255371
Epoch 830, val loss: 1.0025755167007446
Epoch 840, training loss: 6.165433883666992 = 0.08522000908851624 + 1.0 * 6.080214023590088
Epoch 840, val loss: 1.009425401687622
Epoch 850, training loss: 6.167796611785889 = 0.08124960958957672 + 1.0 * 6.086546897888184
Epoch 850, val loss: 1.0161162614822388
Epoch 860, training loss: 6.155135154724121 = 0.07749781012535095 + 1.0 * 6.077637195587158
Epoch 860, val loss: 1.0228214263916016
Epoch 870, training loss: 6.149538040161133 = 0.07393931597471237 + 1.0 * 6.07559871673584
Epoch 870, val loss: 1.0296059846878052
Epoch 880, training loss: 6.146887302398682 = 0.07056776434183121 + 1.0 * 6.076319694519043
Epoch 880, val loss: 1.0364078283309937
Epoch 890, training loss: 6.141965866088867 = 0.06737794727087021 + 1.0 * 6.074587821960449
Epoch 890, val loss: 1.0430397987365723
Epoch 900, training loss: 6.138134002685547 = 0.0643642246723175 + 1.0 * 6.073769569396973
Epoch 900, val loss: 1.0497876405715942
Epoch 910, training loss: 6.13437032699585 = 0.06152517721056938 + 1.0 * 6.072844982147217
Epoch 910, val loss: 1.0565638542175293
Epoch 920, training loss: 6.131507873535156 = 0.05884948745369911 + 1.0 * 6.072658538818359
Epoch 920, val loss: 1.0633703470230103
Epoch 930, training loss: 6.127191066741943 = 0.056334491819143295 + 1.0 * 6.07085657119751
Epoch 930, val loss: 1.0701409578323364
Epoch 940, training loss: 6.137989044189453 = 0.053980808705091476 + 1.0 * 6.08400821685791
Epoch 940, val loss: 1.0769238471984863
Epoch 950, training loss: 6.121524333953857 = 0.05176562815904617 + 1.0 * 6.069758892059326
Epoch 950, val loss: 1.0835949182510376
Epoch 960, training loss: 6.1171135902404785 = 0.04968554526567459 + 1.0 * 6.067428112030029
Epoch 960, val loss: 1.0903962850570679
Epoch 970, training loss: 6.1131672859191895 = 0.04771927744150162 + 1.0 * 6.065447807312012
Epoch 970, val loss: 1.0972294807434082
Epoch 980, training loss: 6.117948532104492 = 0.04585528373718262 + 1.0 * 6.0720930099487305
Epoch 980, val loss: 1.1039313077926636
Epoch 990, training loss: 6.11799955368042 = 0.044120389968156815 + 1.0 * 6.073879241943359
Epoch 990, val loss: 1.1105461120605469
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.6458
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.326299667358398 = 1.9523811340332031 + 1.0 * 8.373918533325195
Epoch 0, val loss: 1.9485259056091309
Epoch 10, training loss: 10.315418243408203 = 1.9417778253555298 + 1.0 * 8.373640060424805
Epoch 10, val loss: 1.9382972717285156
Epoch 20, training loss: 10.29990005493164 = 1.9284231662750244 + 1.0 * 8.371477127075195
Epoch 20, val loss: 1.9249428510665894
Epoch 30, training loss: 10.264067649841309 = 1.9093655347824097 + 1.0 * 8.35470199584961
Epoch 30, val loss: 1.905517578125
Epoch 40, training loss: 10.125554084777832 = 1.8834596872329712 + 1.0 * 8.242094039916992
Epoch 40, val loss: 1.8798948526382446
Epoch 50, training loss: 9.699706077575684 = 1.8549818992614746 + 1.0 * 7.844724178314209
Epoch 50, val loss: 1.8527930974960327
Epoch 60, training loss: 9.32970905303955 = 1.831134557723999 + 1.0 * 7.498574733734131
Epoch 60, val loss: 1.831335186958313
Epoch 70, training loss: 8.880544662475586 = 1.8135582208633423 + 1.0 * 7.066986560821533
Epoch 70, val loss: 1.8153797388076782
Epoch 80, training loss: 8.593240737915039 = 1.7985937595367432 + 1.0 * 6.794647216796875
Epoch 80, val loss: 1.8021469116210938
Epoch 90, training loss: 8.45332145690918 = 1.7806620597839355 + 1.0 * 6.672659873962402
Epoch 90, val loss: 1.786576747894287
Epoch 100, training loss: 8.35223388671875 = 1.76023530960083 + 1.0 * 6.59199857711792
Epoch 100, val loss: 1.7695930004119873
Epoch 110, training loss: 8.277100563049316 = 1.7402212619781494 + 1.0 * 6.536879539489746
Epoch 110, val loss: 1.7531423568725586
Epoch 120, training loss: 8.210590362548828 = 1.7197479009628296 + 1.0 * 6.490842819213867
Epoch 120, val loss: 1.736025333404541
Epoch 130, training loss: 8.146048545837402 = 1.6972668170928955 + 1.0 * 6.448781490325928
Epoch 130, val loss: 1.7173864841461182
Epoch 140, training loss: 8.087507247924805 = 1.6720699071884155 + 1.0 * 6.4154372215271
Epoch 140, val loss: 1.6970422267913818
Epoch 150, training loss: 8.02316665649414 = 1.6436914205551147 + 1.0 * 6.3794755935668945
Epoch 150, val loss: 1.6745258569717407
Epoch 160, training loss: 7.964960098266602 = 1.6112662553787231 + 1.0 * 6.353693962097168
Epoch 160, val loss: 1.649021863937378
Epoch 170, training loss: 7.906196117401123 = 1.5740615129470825 + 1.0 * 6.33213472366333
Epoch 170, val loss: 1.6198633909225464
Epoch 180, training loss: 7.84604549407959 = 1.532267451286316 + 1.0 * 6.313777923583984
Epoch 180, val loss: 1.5871509313583374
Epoch 190, training loss: 7.7840576171875 = 1.486428141593933 + 1.0 * 6.297629356384277
Epoch 190, val loss: 1.5513614416122437
Epoch 200, training loss: 7.719212055206299 = 1.4367529153823853 + 1.0 * 6.282459259033203
Epoch 200, val loss: 1.5128144025802612
Epoch 210, training loss: 7.654901504516602 = 1.3843793869018555 + 1.0 * 6.270522117614746
Epoch 210, val loss: 1.4725862741470337
Epoch 220, training loss: 7.588741779327393 = 1.3308428525924683 + 1.0 * 6.257898807525635
Epoch 220, val loss: 1.431818962097168
Epoch 230, training loss: 7.523523330688477 = 1.2766495943069458 + 1.0 * 6.24687385559082
Epoch 230, val loss: 1.390931248664856
Epoch 240, training loss: 7.4646077156066895 = 1.2230406999588013 + 1.0 * 6.241567134857178
Epoch 240, val loss: 1.3512699604034424
Epoch 250, training loss: 7.401126861572266 = 1.1718478202819824 + 1.0 * 6.229279041290283
Epoch 250, val loss: 1.3137779235839844
Epoch 260, training loss: 7.34653902053833 = 1.1230896711349487 + 1.0 * 6.223449230194092
Epoch 260, val loss: 1.2785991430282593
Epoch 270, training loss: 7.2926411628723145 = 1.077404499053955 + 1.0 * 6.215236663818359
Epoch 270, val loss: 1.246265172958374
Epoch 280, training loss: 7.242266654968262 = 1.0347363948822021 + 1.0 * 6.2075300216674805
Epoch 280, val loss: 1.2163246870040894
Epoch 290, training loss: 7.208796977996826 = 0.994544506072998 + 1.0 * 6.214252471923828
Epoch 290, val loss: 1.1886729001998901
Epoch 300, training loss: 7.153113842010498 = 0.9570861458778381 + 1.0 * 6.196027755737305
Epoch 300, val loss: 1.1629976034164429
Epoch 310, training loss: 7.112797737121582 = 0.9213172197341919 + 1.0 * 6.19148063659668
Epoch 310, val loss: 1.1387838125228882
Epoch 320, training loss: 7.071366310119629 = 0.8863334059715271 + 1.0 * 6.185032844543457
Epoch 320, val loss: 1.1153517961502075
Epoch 330, training loss: 7.040961742401123 = 0.8515793085098267 + 1.0 * 6.189382553100586
Epoch 330, val loss: 1.09232497215271
Epoch 340, training loss: 6.994974136352539 = 0.8171868920326233 + 1.0 * 6.1777873039245605
Epoch 340, val loss: 1.069561243057251
Epoch 350, training loss: 6.955872058868408 = 0.782565176486969 + 1.0 * 6.173306941986084
Epoch 350, val loss: 1.0468846559524536
Epoch 360, training loss: 6.922845363616943 = 0.7476420998573303 + 1.0 * 6.175203323364258
Epoch 360, val loss: 1.0241189002990723
Epoch 370, training loss: 6.879640102386475 = 0.7126312851905823 + 1.0 * 6.167008876800537
Epoch 370, val loss: 1.0013022422790527
Epoch 380, training loss: 6.839282512664795 = 0.677294135093689 + 1.0 * 6.161988258361816
Epoch 380, val loss: 0.9786345362663269
Epoch 390, training loss: 6.799990177154541 = 0.641753077507019 + 1.0 * 6.158236980438232
Epoch 390, val loss: 0.9559544324874878
Epoch 400, training loss: 6.77569055557251 = 0.6064023971557617 + 1.0 * 6.169288158416748
Epoch 400, val loss: 0.9335553050041199
Epoch 410, training loss: 6.727521896362305 = 0.5724169611930847 + 1.0 * 6.155105113983154
Epoch 410, val loss: 0.9121046662330627
Epoch 420, training loss: 6.689236164093018 = 0.5395417809486389 + 1.0 * 6.149694442749023
Epoch 420, val loss: 0.8919807076454163
Epoch 430, training loss: 6.655036449432373 = 0.5077878832817078 + 1.0 * 6.1472487449646
Epoch 430, val loss: 0.8730237483978271
Epoch 440, training loss: 6.627124309539795 = 0.47741183638572693 + 1.0 * 6.149712562561035
Epoch 440, val loss: 0.8552061915397644
Epoch 450, training loss: 6.593377113342285 = 0.4487874209880829 + 1.0 * 6.144589900970459
Epoch 450, val loss: 0.8391009569168091
Epoch 460, training loss: 6.561625003814697 = 0.4217084050178528 + 1.0 * 6.13991641998291
Epoch 460, val loss: 0.8246371746063232
Epoch 470, training loss: 6.534691333770752 = 0.3961222767829895 + 1.0 * 6.138568878173828
Epoch 470, val loss: 0.8114542961120605
Epoch 480, training loss: 6.50776481628418 = 0.3721080422401428 + 1.0 * 6.135656833648682
Epoch 480, val loss: 0.7999300360679626
Epoch 490, training loss: 6.481619834899902 = 0.34952041506767273 + 1.0 * 6.132099628448486
Epoch 490, val loss: 0.7899819612503052
Epoch 500, training loss: 6.4648308753967285 = 0.32821398973464966 + 1.0 * 6.1366167068481445
Epoch 500, val loss: 0.7813969254493713
Epoch 510, training loss: 6.439358711242676 = 0.30818426609039307 + 1.0 * 6.131174564361572
Epoch 510, val loss: 0.77408367395401
Epoch 520, training loss: 6.415071487426758 = 0.2893946170806885 + 1.0 * 6.12567663192749
Epoch 520, val loss: 0.7681460380554199
Epoch 530, training loss: 6.395883083343506 = 0.27173563838005066 + 1.0 * 6.124147415161133
Epoch 530, val loss: 0.7634391188621521
Epoch 540, training loss: 6.382909774780273 = 0.2551227807998657 + 1.0 * 6.127787113189697
Epoch 540, val loss: 0.7598501443862915
Epoch 550, training loss: 6.361522674560547 = 0.23956570029258728 + 1.0 * 6.121956825256348
Epoch 550, val loss: 0.7572540044784546
Epoch 560, training loss: 6.349494457244873 = 0.22493037581443787 + 1.0 * 6.124564170837402
Epoch 560, val loss: 0.7555765509605408
Epoch 570, training loss: 6.3287835121154785 = 0.21121104061603546 + 1.0 * 6.11757230758667
Epoch 570, val loss: 0.7549303770065308
Epoch 580, training loss: 6.311261177062988 = 0.19835616648197174 + 1.0 * 6.112905025482178
Epoch 580, val loss: 0.7550415992736816
Epoch 590, training loss: 6.303547382354736 = 0.18625646829605103 + 1.0 * 6.11729097366333
Epoch 590, val loss: 0.7558416724205017
Epoch 600, training loss: 6.291844844818115 = 0.17496126890182495 + 1.0 * 6.116883754730225
Epoch 600, val loss: 0.7572103142738342
Epoch 610, training loss: 6.2739996910095215 = 0.16446726024150848 + 1.0 * 6.109532356262207
Epoch 610, val loss: 0.7591909766197205
Epoch 620, training loss: 6.261234760284424 = 0.1546214073896408 + 1.0 * 6.1066131591796875
Epoch 620, val loss: 0.7617802023887634
Epoch 630, training loss: 6.251616477966309 = 0.14541451632976532 + 1.0 * 6.106202125549316
Epoch 630, val loss: 0.7648383378982544
Epoch 640, training loss: 6.242013931274414 = 0.13687439262866974 + 1.0 * 6.10513973236084
Epoch 640, val loss: 0.7683423757553101
Epoch 650, training loss: 6.230203628540039 = 0.12889522314071655 + 1.0 * 6.101308345794678
Epoch 650, val loss: 0.7723597884178162
Epoch 660, training loss: 6.232303142547607 = 0.12143459916114807 + 1.0 * 6.110868453979492
Epoch 660, val loss: 0.77681565284729
Epoch 670, training loss: 6.21272087097168 = 0.11449522525072098 + 1.0 * 6.0982255935668945
Epoch 670, val loss: 0.7815033197402954
Epoch 680, training loss: 6.207954406738281 = 0.10802903026342392 + 1.0 * 6.099925518035889
Epoch 680, val loss: 0.7865697741508484
Epoch 690, training loss: 6.198404788970947 = 0.10200982540845871 + 1.0 * 6.096395015716553
Epoch 690, val loss: 0.7918485999107361
Epoch 700, training loss: 6.19126558303833 = 0.09642446786165237 + 1.0 * 6.094841003417969
Epoch 700, val loss: 0.7974161505699158
Epoch 710, training loss: 6.184637546539307 = 0.09119445085525513 + 1.0 * 6.093442916870117
Epoch 710, val loss: 0.8032103776931763
Epoch 720, training loss: 6.187485218048096 = 0.08630668371915817 + 1.0 * 6.1011786460876465
Epoch 720, val loss: 0.8091514706611633
Epoch 730, training loss: 6.175905704498291 = 0.08175114542245865 + 1.0 * 6.094154357910156
Epoch 730, val loss: 0.8150738477706909
Epoch 740, training loss: 6.167702674865723 = 0.07752542942762375 + 1.0 * 6.090177059173584
Epoch 740, val loss: 0.8211250305175781
Epoch 750, training loss: 6.161925315856934 = 0.07357326149940491 + 1.0 * 6.088352203369141
Epoch 750, val loss: 0.8273063898086548
Epoch 760, training loss: 6.159302711486816 = 0.0698634535074234 + 1.0 * 6.089439392089844
Epoch 760, val loss: 0.8335598111152649
Epoch 770, training loss: 6.15175724029541 = 0.06641225516796112 + 1.0 * 6.0853447914123535
Epoch 770, val loss: 0.8397989869117737
Epoch 780, training loss: 6.146668910980225 = 0.0631842315196991 + 1.0 * 6.083484649658203
Epoch 780, val loss: 0.846062183380127
Epoch 790, training loss: 6.146209716796875 = 0.06015697866678238 + 1.0 * 6.086052894592285
Epoch 790, val loss: 0.8523696064949036
Epoch 800, training loss: 6.142722129821777 = 0.05733548477292061 + 1.0 * 6.085386753082275
Epoch 800, val loss: 0.8586576581001282
Epoch 810, training loss: 6.136753082275391 = 0.054701272398233414 + 1.0 * 6.082051753997803
Epoch 810, val loss: 0.8648511171340942
Epoch 820, training loss: 6.135519504547119 = 0.052228666841983795 + 1.0 * 6.083291053771973
Epoch 820, val loss: 0.8711456060409546
Epoch 830, training loss: 6.128241062164307 = 0.04991312325000763 + 1.0 * 6.0783281326293945
Epoch 830, val loss: 0.8773896098136902
Epoch 840, training loss: 6.126114368438721 = 0.0477399118244648 + 1.0 * 6.07837438583374
Epoch 840, val loss: 0.883632481098175
Epoch 850, training loss: 6.13427209854126 = 0.045694924890995026 + 1.0 * 6.0885772705078125
Epoch 850, val loss: 0.889855146408081
Epoch 860, training loss: 6.120824337005615 = 0.04377475753426552 + 1.0 * 6.077049732208252
Epoch 860, val loss: 0.895892322063446
Epoch 870, training loss: 6.119043350219727 = 0.04197297990322113 + 1.0 * 6.077070236206055
Epoch 870, val loss: 0.9019516706466675
Epoch 880, training loss: 6.119939804077148 = 0.04027784243226051 + 1.0 * 6.079661846160889
Epoch 880, val loss: 0.907960057258606
Epoch 890, training loss: 6.113757133483887 = 0.0386754609644413 + 1.0 * 6.075081825256348
Epoch 890, val loss: 0.9138851761817932
Epoch 900, training loss: 6.109460830688477 = 0.03716734051704407 + 1.0 * 6.072293281555176
Epoch 900, val loss: 0.9198068976402283
Epoch 910, training loss: 6.107837677001953 = 0.03573475405573845 + 1.0 * 6.072103023529053
Epoch 910, val loss: 0.9257270693778992
Epoch 920, training loss: 6.110599517822266 = 0.03437904268503189 + 1.0 * 6.076220512390137
Epoch 920, val loss: 0.9315747022628784
Epoch 930, training loss: 6.104875087738037 = 0.03310684114694595 + 1.0 * 6.071768283843994
Epoch 930, val loss: 0.9373234510421753
Epoch 940, training loss: 6.107168674468994 = 0.031904250383377075 + 1.0 * 6.0752644538879395
Epoch 940, val loss: 0.9430045485496521
Epoch 950, training loss: 6.099176406860352 = 0.030766164883971214 + 1.0 * 6.068410396575928
Epoch 950, val loss: 0.948527455329895
Epoch 960, training loss: 6.096594333648682 = 0.029687078669667244 + 1.0 * 6.0669074058532715
Epoch 960, val loss: 0.9540997743606567
Epoch 970, training loss: 6.096548557281494 = 0.028659794479608536 + 1.0 * 6.0678887367248535
Epoch 970, val loss: 0.9596669673919678
Epoch 980, training loss: 6.099959373474121 = 0.027684582397341728 + 1.0 * 6.072274684906006
Epoch 980, val loss: 0.9651179313659668
Epoch 990, training loss: 6.093305587768555 = 0.026758700609207153 + 1.0 * 6.06654691696167
Epoch 990, val loss: 0.9704554677009583
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8229
Flip ASR: 0.7911/225 nodes
The final ASR:0.69373, 0.09232, Accuracy:0.80000, 0.02362
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10446])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.337202072143555 = 1.9633420705795288 + 1.0 * 8.373860359191895
Epoch 0, val loss: 1.9673043489456177
Epoch 10, training loss: 10.325681686401367 = 1.9522329568862915 + 1.0 * 8.373448371887207
Epoch 10, val loss: 1.9558302164077759
Epoch 20, training loss: 10.308990478515625 = 1.938185214996338 + 1.0 * 8.370804786682129
Epoch 20, val loss: 1.941148281097412
Epoch 30, training loss: 10.270054817199707 = 1.9187325239181519 + 1.0 * 8.351322174072266
Epoch 30, val loss: 1.9209685325622559
Epoch 40, training loss: 10.08280086517334 = 1.8941402435302734 + 1.0 * 8.188660621643066
Epoch 40, val loss: 1.8964871168136597
Epoch 50, training loss: 9.364348411560059 = 1.8681999444961548 + 1.0 * 7.496148586273193
Epoch 50, val loss: 1.8720356225967407
Epoch 60, training loss: 8.882034301757812 = 1.8537687063217163 + 1.0 * 7.028265953063965
Epoch 60, val loss: 1.8601096868515015
Epoch 70, training loss: 8.536727905273438 = 1.8435360193252563 + 1.0 * 6.6931915283203125
Epoch 70, val loss: 1.851499319076538
Epoch 80, training loss: 8.353798866271973 = 1.8333176374435425 + 1.0 * 6.520481109619141
Epoch 80, val loss: 1.8419865369796753
Epoch 90, training loss: 8.239766120910645 = 1.8207838535308838 + 1.0 * 6.41898250579834
Epoch 90, val loss: 1.8299516439437866
Epoch 100, training loss: 8.16240119934082 = 1.8081727027893066 + 1.0 * 6.354228496551514
Epoch 100, val loss: 1.8179473876953125
Epoch 110, training loss: 8.109341621398926 = 1.7962603569030762 + 1.0 * 6.31308126449585
Epoch 110, val loss: 1.806769847869873
Epoch 120, training loss: 8.06689453125 = 1.7848347425460815 + 1.0 * 6.282060146331787
Epoch 120, val loss: 1.7960911989212036
Epoch 130, training loss: 8.030323028564453 = 1.773472785949707 + 1.0 * 6.256849765777588
Epoch 130, val loss: 1.7856234312057495
Epoch 140, training loss: 7.997827529907227 = 1.761406421661377 + 1.0 * 6.23642110824585
Epoch 140, val loss: 1.7747329473495483
Epoch 150, training loss: 7.96766471862793 = 1.7477749586105347 + 1.0 * 6.2198896408081055
Epoch 150, val loss: 1.7629435062408447
Epoch 160, training loss: 7.93746280670166 = 1.731965184211731 + 1.0 * 6.205497741699219
Epoch 160, val loss: 1.7497299909591675
Epoch 170, training loss: 7.90902042388916 = 1.7132999897003174 + 1.0 * 6.195720195770264
Epoch 170, val loss: 1.7344975471496582
Epoch 180, training loss: 7.87558126449585 = 1.6912918090820312 + 1.0 * 6.184289455413818
Epoch 180, val loss: 1.7168458700180054
Epoch 190, training loss: 7.839171409606934 = 1.6648995876312256 + 1.0 * 6.174272060394287
Epoch 190, val loss: 1.6957415342330933
Epoch 200, training loss: 7.800251007080078 = 1.6329467296600342 + 1.0 * 6.167304039001465
Epoch 200, val loss: 1.6702427864074707
Epoch 210, training loss: 7.756983757019043 = 1.5948396921157837 + 1.0 * 6.162144184112549
Epoch 210, val loss: 1.6398059129714966
Epoch 220, training loss: 7.7049760818481445 = 1.5499026775360107 + 1.0 * 6.155073642730713
Epoch 220, val loss: 1.603764533996582
Epoch 230, training loss: 7.647341728210449 = 1.4970697164535522 + 1.0 * 6.150271892547607
Epoch 230, val loss: 1.561582326889038
Epoch 240, training loss: 7.582945823669434 = 1.437816858291626 + 1.0 * 6.1451287269592285
Epoch 240, val loss: 1.5141688585281372
Epoch 250, training loss: 7.5146307945251465 = 1.3737069368362427 + 1.0 * 6.140923976898193
Epoch 250, val loss: 1.4630024433135986
Epoch 260, training loss: 7.446885585784912 = 1.3065966367721558 + 1.0 * 6.140288829803467
Epoch 260, val loss: 1.409420132637024
Epoch 270, training loss: 7.373456954956055 = 1.240107774734497 + 1.0 * 6.1333489418029785
Epoch 270, val loss: 1.356295108795166
Epoch 280, training loss: 7.304437160491943 = 1.1753491163253784 + 1.0 * 6.129087924957275
Epoch 280, val loss: 1.304391860961914
Epoch 290, training loss: 7.245415210723877 = 1.1133780479431152 + 1.0 * 6.132037162780762
Epoch 290, val loss: 1.2547204494476318
Epoch 300, training loss: 7.179425239562988 = 1.056113362312317 + 1.0 * 6.123311996459961
Epoch 300, val loss: 1.209229588508606
Epoch 310, training loss: 7.122113227844238 = 1.0025849342346191 + 1.0 * 6.119528293609619
Epoch 310, val loss: 1.1668779850006104
Epoch 320, training loss: 7.068285942077637 = 0.9526021480560303 + 1.0 * 6.115683555603027
Epoch 320, val loss: 1.1276803016662598
Epoch 330, training loss: 7.020198345184326 = 0.9057726860046387 + 1.0 * 6.1144256591796875
Epoch 330, val loss: 1.0911076068878174
Epoch 340, training loss: 6.972125053405762 = 0.8615405559539795 + 1.0 * 6.110584259033203
Epoch 340, val loss: 1.057188868522644
Epoch 350, training loss: 6.927839279174805 = 0.819655179977417 + 1.0 * 6.108184337615967
Epoch 350, val loss: 1.0254276990890503
Epoch 360, training loss: 6.885217189788818 = 0.7798933982849121 + 1.0 * 6.105323791503906
Epoch 360, val loss: 0.9957587122917175
Epoch 370, training loss: 6.844757080078125 = 0.7422281503677368 + 1.0 * 6.102529048919678
Epoch 370, val loss: 0.9684014320373535
Epoch 380, training loss: 6.805199146270752 = 0.7063603401184082 + 1.0 * 6.098838806152344
Epoch 380, val loss: 0.943125307559967
Epoch 390, training loss: 6.772026062011719 = 0.6718599796295166 + 1.0 * 6.100165843963623
Epoch 390, val loss: 0.9196306467056274
Epoch 400, training loss: 6.734917640686035 = 0.6389883160591125 + 1.0 * 6.095929145812988
Epoch 400, val loss: 0.8981139659881592
Epoch 410, training loss: 6.700305938720703 = 0.6075929999351501 + 1.0 * 6.092712879180908
Epoch 410, val loss: 0.8785899877548218
Epoch 420, training loss: 6.665889739990234 = 0.5771167278289795 + 1.0 * 6.088772773742676
Epoch 420, val loss: 0.8603020906448364
Epoch 430, training loss: 6.633857727050781 = 0.547260582447052 + 1.0 * 6.086596965789795
Epoch 430, val loss: 0.8430055379867554
Epoch 440, training loss: 6.607274055480957 = 0.5183150172233582 + 1.0 * 6.088959217071533
Epoch 440, val loss: 0.8267312049865723
Epoch 450, training loss: 6.576320648193359 = 0.490663081407547 + 1.0 * 6.085657596588135
Epoch 450, val loss: 0.8120460510253906
Epoch 460, training loss: 6.546220302581787 = 0.4639844298362732 + 1.0 * 6.082235813140869
Epoch 460, val loss: 0.7986637949943542
Epoch 470, training loss: 6.525136470794678 = 0.4384221136569977 + 1.0 * 6.086714267730713
Epoch 470, val loss: 0.786536455154419
Epoch 480, training loss: 6.495190143585205 = 0.414272278547287 + 1.0 * 6.080917835235596
Epoch 480, val loss: 0.775941014289856
Epoch 490, training loss: 6.468481540679932 = 0.3913015127182007 + 1.0 * 6.077179908752441
Epoch 490, val loss: 0.7668561935424805
Epoch 500, training loss: 6.447296142578125 = 0.3694237172603607 + 1.0 * 6.077872276306152
Epoch 500, val loss: 0.7589560151100159
Epoch 510, training loss: 6.426583766937256 = 0.3486602008342743 + 1.0 * 6.077923774719238
Epoch 510, val loss: 0.7522247433662415
Epoch 520, training loss: 6.400309085845947 = 0.3289918005466461 + 1.0 * 6.071317195892334
Epoch 520, val loss: 0.7466253638267517
Epoch 530, training loss: 6.386013031005859 = 0.31023481488227844 + 1.0 * 6.075778007507324
Epoch 530, val loss: 0.7419440150260925
Epoch 540, training loss: 6.3679399490356445 = 0.29248157143592834 + 1.0 * 6.075458526611328
Epoch 540, val loss: 0.7380786538124084
Epoch 550, training loss: 6.342798709869385 = 0.27558764815330505 + 1.0 * 6.067211151123047
Epoch 550, val loss: 0.7350627779960632
Epoch 560, training loss: 6.327756404876709 = 0.25943052768707275 + 1.0 * 6.068325996398926
Epoch 560, val loss: 0.732729434967041
Epoch 570, training loss: 6.308948040008545 = 0.24396821856498718 + 1.0 * 6.0649800300598145
Epoch 570, val loss: 0.7309383153915405
Epoch 580, training loss: 6.293424129486084 = 0.22920306026935577 + 1.0 * 6.064220905303955
Epoch 580, val loss: 0.7298206686973572
Epoch 590, training loss: 6.281428337097168 = 0.21510618925094604 + 1.0 * 6.066322326660156
Epoch 590, val loss: 0.7292051911354065
Epoch 600, training loss: 6.268877029418945 = 0.20174168050289154 + 1.0 * 6.067135334014893
Epoch 600, val loss: 0.7290104627609253
Epoch 610, training loss: 6.249497890472412 = 0.18915770947933197 + 1.0 * 6.060340404510498
Epoch 610, val loss: 0.7293012738227844
Epoch 620, training loss: 6.2357177734375 = 0.17729797959327698 + 1.0 * 6.058419704437256
Epoch 620, val loss: 0.730121374130249
Epoch 630, training loss: 6.22775936126709 = 0.1661388874053955 + 1.0 * 6.061620712280273
Epoch 630, val loss: 0.7313578128814697
Epoch 640, training loss: 6.214330196380615 = 0.15573650598526 + 1.0 * 6.05859375
Epoch 640, val loss: 0.7330490350723267
Epoch 650, training loss: 6.2024102210998535 = 0.14605073630809784 + 1.0 * 6.05635929107666
Epoch 650, val loss: 0.735167920589447
Epoch 660, training loss: 6.197286128997803 = 0.13708433508872986 + 1.0 * 6.060201644897461
Epoch 660, val loss: 0.7376375198364258
Epoch 670, training loss: 6.1836113929748535 = 0.1287965327501297 + 1.0 * 6.05481481552124
Epoch 670, val loss: 0.7404752373695374
Epoch 680, training loss: 6.172276973724365 = 0.12111826986074448 + 1.0 * 6.051158905029297
Epoch 680, val loss: 0.7437781691551208
Epoch 690, training loss: 6.172593593597412 = 0.11397642642259598 + 1.0 * 6.058617115020752
Epoch 690, val loss: 0.7473985552787781
Epoch 700, training loss: 6.158058166503906 = 0.10739848017692566 + 1.0 * 6.050659656524658
Epoch 700, val loss: 0.7513071894645691
Epoch 710, training loss: 6.150876522064209 = 0.10129883140325546 + 1.0 * 6.049577713012695
Epoch 710, val loss: 0.7555888295173645
Epoch 720, training loss: 6.144018173217773 = 0.09563825279474258 + 1.0 * 6.048379898071289
Epoch 720, val loss: 0.7600905299186707
Epoch 730, training loss: 6.137120723724365 = 0.09040246158838272 + 1.0 * 6.046718120574951
Epoch 730, val loss: 0.7648465037345886
Epoch 740, training loss: 6.129789352416992 = 0.08555345982313156 + 1.0 * 6.044235706329346
Epoch 740, val loss: 0.7698971629142761
Epoch 750, training loss: 6.125594139099121 = 0.08103664964437485 + 1.0 * 6.044557571411133
Epoch 750, val loss: 0.7751257419586182
Epoch 760, training loss: 6.12000036239624 = 0.07683297246694565 + 1.0 * 6.043167591094971
Epoch 760, val loss: 0.7804069519042969
Epoch 770, training loss: 6.116323947906494 = 0.07292439043521881 + 1.0 * 6.043399333953857
Epoch 770, val loss: 0.7858970761299133
Epoch 780, training loss: 6.111015796661377 = 0.06929337978363037 + 1.0 * 6.041722297668457
Epoch 780, val loss: 0.7913247346878052
Epoch 790, training loss: 6.107087135314941 = 0.06592342257499695 + 1.0 * 6.041163921356201
Epoch 790, val loss: 0.7968344688415527
Epoch 800, training loss: 6.102787494659424 = 0.06277646869421005 + 1.0 * 6.040010929107666
Epoch 800, val loss: 0.8024677634239197
Epoch 810, training loss: 6.098627090454102 = 0.05983268842101097 + 1.0 * 6.03879451751709
Epoch 810, val loss: 0.8080506920814514
Epoch 820, training loss: 6.093880653381348 = 0.05708317831158638 + 1.0 * 6.036797523498535
Epoch 820, val loss: 0.8137753009796143
Epoch 830, training loss: 6.094121932983398 = 0.054504334926605225 + 1.0 * 6.039617538452148
Epoch 830, val loss: 0.819563090801239
Epoch 840, training loss: 6.093277931213379 = 0.052094005048274994 + 1.0 * 6.041183948516846
Epoch 840, val loss: 0.8251937627792358
Epoch 850, training loss: 6.087676525115967 = 0.04984384775161743 + 1.0 * 6.037832736968994
Epoch 850, val loss: 0.8308737277984619
Epoch 860, training loss: 6.083832263946533 = 0.04774846136569977 + 1.0 * 6.036083698272705
Epoch 860, val loss: 0.8365273475646973
Epoch 870, training loss: 6.0791215896606445 = 0.04578069597482681 + 1.0 * 6.033340930938721
Epoch 870, val loss: 0.8421525955200195
Epoch 880, training loss: 6.07619047164917 = 0.04392349720001221 + 1.0 * 6.032267093658447
Epoch 880, val loss: 0.8477998971939087
Epoch 890, training loss: 6.073886871337891 = 0.042168088257312775 + 1.0 * 6.031718730926514
Epoch 890, val loss: 0.8534597158432007
Epoch 900, training loss: 6.0814104080200195 = 0.0405091755092144 + 1.0 * 6.040901184082031
Epoch 900, val loss: 0.8590142130851746
Epoch 910, training loss: 6.069985866546631 = 0.038959719240665436 + 1.0 * 6.031026363372803
Epoch 910, val loss: 0.8645088076591492
Epoch 920, training loss: 6.072003364562988 = 0.03750088810920715 + 1.0 * 6.0345025062561035
Epoch 920, val loss: 0.8700150847434998
Epoch 930, training loss: 6.06700325012207 = 0.036123648285865784 + 1.0 * 6.030879497528076
Epoch 930, val loss: 0.8753706812858582
Epoch 940, training loss: 6.063539981842041 = 0.03482085093855858 + 1.0 * 6.028718948364258
Epoch 940, val loss: 0.8807236552238464
Epoch 950, training loss: 6.06329345703125 = 0.033585067838430405 + 1.0 * 6.029708385467529
Epoch 950, val loss: 0.8860227465629578
Epoch 960, training loss: 6.0610833168029785 = 0.032413773238658905 + 1.0 * 6.028669357299805
Epoch 960, val loss: 0.8913096785545349
Epoch 970, training loss: 6.0663886070251465 = 0.031304895877838135 + 1.0 * 6.035083770751953
Epoch 970, val loss: 0.8964877724647522
Epoch 980, training loss: 6.056605339050293 = 0.030255325138568878 + 1.0 * 6.026350021362305
Epoch 980, val loss: 0.9015988111495972
Epoch 990, training loss: 6.053971290588379 = 0.029262881726026535 + 1.0 * 6.024708271026611
Epoch 990, val loss: 0.9067752361297607
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6125
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.321575164794922 = 1.9478261470794678 + 1.0 * 8.373748779296875
Epoch 0, val loss: 1.9419772624969482
Epoch 10, training loss: 10.310171127319336 = 1.9372495412826538 + 1.0 * 8.37292194366455
Epoch 10, val loss: 1.931776762008667
Epoch 20, training loss: 10.291728973388672 = 1.9241992235183716 + 1.0 * 8.36752986907959
Epoch 20, val loss: 1.9186996221542358
Epoch 30, training loss: 10.232744216918945 = 1.906724452972412 + 1.0 * 8.326020240783691
Epoch 30, val loss: 1.9009679555892944
Epoch 40, training loss: 9.860650062561035 = 1.886697769165039 + 1.0 * 7.973952293395996
Epoch 40, val loss: 1.8811590671539307
Epoch 50, training loss: 8.995655059814453 = 1.867594838142395 + 1.0 * 7.1280598640441895
Epoch 50, val loss: 1.8625736236572266
Epoch 60, training loss: 8.625359535217285 = 1.8523443937301636 + 1.0 * 6.77301549911499
Epoch 60, val loss: 1.8481563329696655
Epoch 70, training loss: 8.441078186035156 = 1.8388383388519287 + 1.0 * 6.602239608764648
Epoch 70, val loss: 1.8348915576934814
Epoch 80, training loss: 8.347986221313477 = 1.8270207643508911 + 1.0 * 6.520965576171875
Epoch 80, val loss: 1.823415756225586
Epoch 90, training loss: 8.27232837677002 = 1.8151218891143799 + 1.0 * 6.4572062492370605
Epoch 90, val loss: 1.811734914779663
Epoch 100, training loss: 8.20087718963623 = 1.8038430213928223 + 1.0 * 6.397034168243408
Epoch 100, val loss: 1.800946593284607
Epoch 110, training loss: 8.139676094055176 = 1.7932475805282593 + 1.0 * 6.346428394317627
Epoch 110, val loss: 1.790862798690796
Epoch 120, training loss: 8.089143753051758 = 1.783055305480957 + 1.0 * 6.306088447570801
Epoch 120, val loss: 1.7810674905776978
Epoch 130, training loss: 8.044858932495117 = 1.772179365158081 + 1.0 * 6.272679328918457
Epoch 130, val loss: 1.7706503868103027
Epoch 140, training loss: 8.00678825378418 = 1.7598198652267456 + 1.0 * 6.2469682693481445
Epoch 140, val loss: 1.7591793537139893
Epoch 150, training loss: 7.971697807312012 = 1.7453092336654663 + 1.0 * 6.226388454437256
Epoch 150, val loss: 1.7461382150650024
Epoch 160, training loss: 7.937044620513916 = 1.7281666994094849 + 1.0 * 6.208878040313721
Epoch 160, val loss: 1.731112003326416
Epoch 170, training loss: 7.902024269104004 = 1.7077195644378662 + 1.0 * 6.194304466247559
Epoch 170, val loss: 1.713716745376587
Epoch 180, training loss: 7.864278316497803 = 1.683143138885498 + 1.0 * 6.181135177612305
Epoch 180, val loss: 1.6929664611816406
Epoch 190, training loss: 7.823178291320801 = 1.6529154777526855 + 1.0 * 6.170262813568115
Epoch 190, val loss: 1.667710304260254
Epoch 200, training loss: 7.777216911315918 = 1.6156638860702515 + 1.0 * 6.161552906036377
Epoch 200, val loss: 1.6366773843765259
Epoch 210, training loss: 7.724895000457764 = 1.5703986883163452 + 1.0 * 6.154496192932129
Epoch 210, val loss: 1.5992006063461304
Epoch 220, training loss: 7.666278839111328 = 1.5174363851547241 + 1.0 * 6.1488423347473145
Epoch 220, val loss: 1.555652379989624
Epoch 230, training loss: 7.60092830657959 = 1.4576306343078613 + 1.0 * 6.1432976722717285
Epoch 230, val loss: 1.5070101022720337
Epoch 240, training loss: 7.531458854675293 = 1.3926494121551514 + 1.0 * 6.1388092041015625
Epoch 240, val loss: 1.4549586772918701
Epoch 250, training loss: 7.4638824462890625 = 1.3261083364486694 + 1.0 * 6.1377739906311035
Epoch 250, val loss: 1.4028629064559937
Epoch 260, training loss: 7.392895698547363 = 1.2619612216949463 + 1.0 * 6.130934715270996
Epoch 260, val loss: 1.3537490367889404
Epoch 270, training loss: 7.327292442321777 = 1.2005045413970947 + 1.0 * 6.1267876625061035
Epoch 270, val loss: 1.3071283102035522
Epoch 280, training loss: 7.268258094787598 = 1.1419169902801514 + 1.0 * 6.126341342926025
Epoch 280, val loss: 1.2633295059204102
Epoch 290, training loss: 7.204577922821045 = 1.0865811109542847 + 1.0 * 6.117996692657471
Epoch 290, val loss: 1.2225152254104614
Epoch 300, training loss: 7.150900840759277 = 1.0336577892303467 + 1.0 * 6.11724328994751
Epoch 300, val loss: 1.1837950944900513
Epoch 310, training loss: 7.0975260734558105 = 0.9834380149841309 + 1.0 * 6.11408805847168
Epoch 310, val loss: 1.147073745727539
Epoch 320, training loss: 7.043332099914551 = 0.9353048205375671 + 1.0 * 6.108027458190918
Epoch 320, val loss: 1.1117234230041504
Epoch 330, training loss: 6.99334192276001 = 0.8883419632911682 + 1.0 * 6.105000019073486
Epoch 330, val loss: 1.0770701169967651
Epoch 340, training loss: 6.949501037597656 = 0.8426323533058167 + 1.0 * 6.106868743896484
Epoch 340, val loss: 1.0433015823364258
Epoch 350, training loss: 6.899096488952637 = 0.798377275466919 + 1.0 * 6.100718975067139
Epoch 350, val loss: 1.010455846786499
Epoch 360, training loss: 6.8533830642700195 = 0.7549387812614441 + 1.0 * 6.09844446182251
Epoch 360, val loss: 0.9780179858207703
Epoch 370, training loss: 6.811623573303223 = 0.7126692533493042 + 1.0 * 6.098954200744629
Epoch 370, val loss: 0.9462723731994629
Epoch 380, training loss: 6.767448425292969 = 0.6720468401908875 + 1.0 * 6.095401763916016
Epoch 380, val loss: 0.9159291982650757
Epoch 390, training loss: 6.7250823974609375 = 0.6332113742828369 + 1.0 * 6.0918707847595215
Epoch 390, val loss: 0.8870017528533936
Epoch 400, training loss: 6.686212539672852 = 0.596225917339325 + 1.0 * 6.089986801147461
Epoch 400, val loss: 0.8598302006721497
Epoch 410, training loss: 6.6564459800720215 = 0.5615290403366089 + 1.0 * 6.094916820526123
Epoch 410, val loss: 0.8348464369773865
Epoch 420, training loss: 6.617074966430664 = 0.5294291973114014 + 1.0 * 6.087645530700684
Epoch 420, val loss: 0.8122039437294006
Epoch 430, training loss: 6.583437442779541 = 0.4993276596069336 + 1.0 * 6.084109783172607
Epoch 430, val loss: 0.791608989238739
Epoch 440, training loss: 6.553976535797119 = 0.470934122800827 + 1.0 * 6.083042621612549
Epoch 440, val loss: 0.7728754878044128
Epoch 450, training loss: 6.531486511230469 = 0.44437357783317566 + 1.0 * 6.087112903594971
Epoch 450, val loss: 0.7559705972671509
Epoch 460, training loss: 6.4989333152771 = 0.41946446895599365 + 1.0 * 6.079468727111816
Epoch 460, val loss: 0.7409965395927429
Epoch 470, training loss: 6.473608016967773 = 0.3958851397037506 + 1.0 * 6.077723026275635
Epoch 470, val loss: 0.727502167224884
Epoch 480, training loss: 6.449166297912598 = 0.3734719753265381 + 1.0 * 6.0756940841674805
Epoch 480, val loss: 0.715360701084137
Epoch 490, training loss: 6.431423187255859 = 0.35217201709747314 + 1.0 * 6.079251289367676
Epoch 490, val loss: 0.7045149207115173
Epoch 500, training loss: 6.404990196228027 = 0.33215537667274475 + 1.0 * 6.0728349685668945
Epoch 500, val loss: 0.6948702335357666
Epoch 510, training loss: 6.383975982666016 = 0.31317317485809326 + 1.0 * 6.070802688598633
Epoch 510, val loss: 0.6864397525787354
Epoch 520, training loss: 6.366844177246094 = 0.29510697722435 + 1.0 * 6.071737289428711
Epoch 520, val loss: 0.679044246673584
Epoch 530, training loss: 6.3519062995910645 = 0.27809351682662964 + 1.0 * 6.073812961578369
Epoch 530, val loss: 0.6726574301719666
Epoch 540, training loss: 6.329253673553467 = 0.2621138393878937 + 1.0 * 6.067139625549316
Epoch 540, val loss: 0.6672893762588501
Epoch 550, training loss: 6.312563419342041 = 0.2469860315322876 + 1.0 * 6.065577507019043
Epoch 550, val loss: 0.6627952456474304
Epoch 560, training loss: 6.298499584197998 = 0.23267944157123566 + 1.0 * 6.065820217132568
Epoch 560, val loss: 0.6591107845306396
Epoch 570, training loss: 6.281601428985596 = 0.21920666098594666 + 1.0 * 6.062394618988037
Epoch 570, val loss: 0.6562123894691467
Epoch 580, training loss: 6.270893573760986 = 0.20648859441280365 + 1.0 * 6.0644049644470215
Epoch 580, val loss: 0.6540151238441467
Epoch 590, training loss: 6.259632110595703 = 0.19456851482391357 + 1.0 * 6.0650634765625
Epoch 590, val loss: 0.6524234414100647
Epoch 600, training loss: 6.242674827575684 = 0.1833728551864624 + 1.0 * 6.059301853179932
Epoch 600, val loss: 0.6514301300048828
Epoch 610, training loss: 6.229493618011475 = 0.17283658683300018 + 1.0 * 6.056656837463379
Epoch 610, val loss: 0.651003897190094
Epoch 620, training loss: 6.227367877960205 = 0.1629132777452469 + 1.0 * 6.064454555511475
Epoch 620, val loss: 0.6510786414146423
Epoch 630, training loss: 6.210479736328125 = 0.15366554260253906 + 1.0 * 6.056814193725586
Epoch 630, val loss: 0.6515281200408936
Epoch 640, training loss: 6.1992998123168945 = 0.1450376957654953 + 1.0 * 6.054262161254883
Epoch 640, val loss: 0.6524162888526917
Epoch 650, training loss: 6.191162586212158 = 0.1369396597146988 + 1.0 * 6.05422306060791
Epoch 650, val loss: 0.6537028551101685
Epoch 660, training loss: 6.182467937469482 = 0.12936531007289886 + 1.0 * 6.053102493286133
Epoch 660, val loss: 0.6553397178649902
Epoch 670, training loss: 6.1748762130737305 = 0.12231941521167755 + 1.0 * 6.052556991577148
Epoch 670, val loss: 0.6572359800338745
Epoch 680, training loss: 6.165205478668213 = 0.11574706435203552 + 1.0 * 6.0494585037231445
Epoch 680, val loss: 0.6594403386116028
Epoch 690, training loss: 6.159358978271484 = 0.1095942035317421 + 1.0 * 6.049764633178711
Epoch 690, val loss: 0.6618630886077881
Epoch 700, training loss: 6.154300212860107 = 0.10386236757040024 + 1.0 * 6.050437927246094
Epoch 700, val loss: 0.6645284295082092
Epoch 710, training loss: 6.146077632904053 = 0.0985443964600563 + 1.0 * 6.04753303527832
Epoch 710, val loss: 0.6673402786254883
Epoch 720, training loss: 6.13933801651001 = 0.09356249868869781 + 1.0 * 6.045775413513184
Epoch 720, val loss: 0.6703661680221558
Epoch 730, training loss: 6.143668174743652 = 0.0888894721865654 + 1.0 * 6.054778575897217
Epoch 730, val loss: 0.6735751628875732
Epoch 740, training loss: 6.1319990158081055 = 0.08453407883644104 + 1.0 * 6.047464847564697
Epoch 740, val loss: 0.6768825650215149
Epoch 750, training loss: 6.123058319091797 = 0.08047633618116379 + 1.0 * 6.042582035064697
Epoch 750, val loss: 0.6803218126296997
Epoch 760, training loss: 6.118723392486572 = 0.07665346562862396 + 1.0 * 6.042069911956787
Epoch 760, val loss: 0.6838961839675903
Epoch 770, training loss: 6.121806621551514 = 0.07304982841014862 + 1.0 * 6.0487565994262695
Epoch 770, val loss: 0.687568724155426
Epoch 780, training loss: 6.114840984344482 = 0.06969506293535233 + 1.0 * 6.0451459884643555
Epoch 780, val loss: 0.6912529468536377
Epoch 790, training loss: 6.107088088989258 = 0.06656179577112198 + 1.0 * 6.040526390075684
Epoch 790, val loss: 0.6950228810310364
Epoch 800, training loss: 6.102413654327393 = 0.06360416859388351 + 1.0 * 6.038809299468994
Epoch 800, val loss: 0.6989262700080872
Epoch 810, training loss: 6.102262496948242 = 0.06080930680036545 + 1.0 * 6.0414533615112305
Epoch 810, val loss: 0.7028747797012329
Epoch 820, training loss: 6.095789909362793 = 0.05818090960383415 + 1.0 * 6.037609100341797
Epoch 820, val loss: 0.7068544030189514
Epoch 830, training loss: 6.093239784240723 = 0.055700916796922684 + 1.0 * 6.037539005279541
Epoch 830, val loss: 0.7109300494194031
Epoch 840, training loss: 6.092202663421631 = 0.053362201899290085 + 1.0 * 6.038840293884277
Epoch 840, val loss: 0.7150295972824097
Epoch 850, training loss: 6.090462684631348 = 0.051158756017684937 + 1.0 * 6.039303779602051
Epoch 850, val loss: 0.7190938591957092
Epoch 860, training loss: 6.085720062255859 = 0.04908846691250801 + 1.0 * 6.0366315841674805
Epoch 860, val loss: 0.7231879234313965
Epoch 870, training loss: 6.082391738891602 = 0.047127775847911835 + 1.0 * 6.035264015197754
Epoch 870, val loss: 0.727311372756958
Epoch 880, training loss: 6.085910320281982 = 0.04527619481086731 + 1.0 * 6.0406341552734375
Epoch 880, val loss: 0.731435239315033
Epoch 890, training loss: 6.075891971588135 = 0.04353201389312744 + 1.0 * 6.032360076904297
Epoch 890, val loss: 0.7355219125747681
Epoch 900, training loss: 6.074026584625244 = 0.041881807148456573 + 1.0 * 6.032144546508789
Epoch 900, val loss: 0.7396461367607117
Epoch 910, training loss: 6.073174953460693 = 0.04031293839216232 + 1.0 * 6.032862186431885
Epoch 910, val loss: 0.7437474131584167
Epoch 920, training loss: 6.079680442810059 = 0.03882894665002823 + 1.0 * 6.040851593017578
Epoch 920, val loss: 0.7478164434432983
Epoch 930, training loss: 6.070854187011719 = 0.03743479400873184 + 1.0 * 6.033419609069824
Epoch 930, val loss: 0.75179523229599
Epoch 940, training loss: 6.066216468811035 = 0.03611810877919197 + 1.0 * 6.0300984382629395
Epoch 940, val loss: 0.7558152079582214
Epoch 950, training loss: 6.06330680847168 = 0.03485911339521408 + 1.0 * 6.02844762802124
Epoch 950, val loss: 0.7598543763160706
Epoch 960, training loss: 6.062668800354004 = 0.033655017614364624 + 1.0 * 6.029013633728027
Epoch 960, val loss: 0.7638781070709229
Epoch 970, training loss: 6.061369895935059 = 0.032513950020074844 + 1.0 * 6.028855800628662
Epoch 970, val loss: 0.7678626775741577
Epoch 980, training loss: 6.060586929321289 = 0.03144039213657379 + 1.0 * 6.029146671295166
Epoch 980, val loss: 0.7717262506484985
Epoch 990, training loss: 6.058023452758789 = 0.0304189994931221 + 1.0 * 6.027604579925537
Epoch 990, val loss: 0.7756365537643433
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7675
Flip ASR: 0.7244/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.309720039367676 = 1.935917615890503 + 1.0 * 8.373802185058594
Epoch 0, val loss: 1.931045651435852
Epoch 10, training loss: 10.299577713012695 = 1.9264143705368042 + 1.0 * 8.373163223266602
Epoch 10, val loss: 1.921758770942688
Epoch 20, training loss: 10.28378963470459 = 1.914829969406128 + 1.0 * 8.368959426879883
Epoch 20, val loss: 1.9098454713821411
Epoch 30, training loss: 10.240926742553711 = 1.8989416360855103 + 1.0 * 8.341984748840332
Epoch 30, val loss: 1.8931316137313843
Epoch 40, training loss: 9.99815559387207 = 1.8791767358779907 + 1.0 * 8.118978500366211
Epoch 40, val loss: 1.87285315990448
Epoch 50, training loss: 9.041187286376953 = 1.8579130172729492 + 1.0 * 7.183273792266846
Epoch 50, val loss: 1.8512279987335205
Epoch 60, training loss: 8.818334579467773 = 1.8384473323822021 + 1.0 * 6.979887008666992
Epoch 60, val loss: 1.8315019607543945
Epoch 70, training loss: 8.599737167358398 = 1.822745442390442 + 1.0 * 6.776991367340088
Epoch 70, val loss: 1.8156002759933472
Epoch 80, training loss: 8.456823348999023 = 1.8090009689331055 + 1.0 * 6.64782190322876
Epoch 80, val loss: 1.8016413450241089
Epoch 90, training loss: 8.3419189453125 = 1.7950608730316162 + 1.0 * 6.546858310699463
Epoch 90, val loss: 1.787771224975586
Epoch 100, training loss: 8.254711151123047 = 1.7810570001602173 + 1.0 * 6.473653793334961
Epoch 100, val loss: 1.774003505706787
Epoch 110, training loss: 8.19374942779541 = 1.7668615579605103 + 1.0 * 6.4268879890441895
Epoch 110, val loss: 1.7600430250167847
Epoch 120, training loss: 8.140031814575195 = 1.751239538192749 + 1.0 * 6.388792514801025
Epoch 120, val loss: 1.7447142601013184
Epoch 130, training loss: 8.091694831848145 = 1.7337534427642822 + 1.0 * 6.357941627502441
Epoch 130, val loss: 1.728268027305603
Epoch 140, training loss: 8.045750617980957 = 1.713904857635498 + 1.0 * 6.331845760345459
Epoch 140, val loss: 1.710556983947754
Epoch 150, training loss: 7.996959686279297 = 1.6906578540802002 + 1.0 * 6.306302070617676
Epoch 150, val loss: 1.6910500526428223
Epoch 160, training loss: 7.9439873695373535 = 1.663162112236023 + 1.0 * 6.280825138092041
Epoch 160, val loss: 1.668548583984375
Epoch 170, training loss: 7.8873820304870605 = 1.6302250623703003 + 1.0 * 6.257156848907471
Epoch 170, val loss: 1.642471194267273
Epoch 180, training loss: 7.82603120803833 = 1.590771198272705 + 1.0 * 6.235260009765625
Epoch 180, val loss: 1.6113972663879395
Epoch 190, training loss: 7.763068199157715 = 1.543563961982727 + 1.0 * 6.219504356384277
Epoch 190, val loss: 1.5743976831436157
Epoch 200, training loss: 7.695251941680908 = 1.4891976118087769 + 1.0 * 6.206054210662842
Epoch 200, val loss: 1.5320065021514893
Epoch 210, training loss: 7.627758026123047 = 1.4308874607086182 + 1.0 * 6.196870803833008
Epoch 210, val loss: 1.4874540567398071
Epoch 220, training loss: 7.557328224182129 = 1.370864987373352 + 1.0 * 6.186463356018066
Epoch 220, val loss: 1.442238450050354
Epoch 230, training loss: 7.487712383270264 = 1.310250163078308 + 1.0 * 6.177462100982666
Epoch 230, val loss: 1.3971681594848633
Epoch 240, training loss: 7.42008113861084 = 1.2501739263534546 + 1.0 * 6.169907093048096
Epoch 240, val loss: 1.3534319400787354
Epoch 250, training loss: 7.358525276184082 = 1.1924840211868286 + 1.0 * 6.166041374206543
Epoch 250, val loss: 1.3124839067459106
Epoch 260, training loss: 7.296107769012451 = 1.1378649473190308 + 1.0 * 6.158242702484131
Epoch 260, val loss: 1.2742822170257568
Epoch 270, training loss: 7.235970497131348 = 1.084298849105835 + 1.0 * 6.151671409606934
Epoch 270, val loss: 1.237349510192871
Epoch 280, training loss: 7.179317474365234 = 1.0313565731048584 + 1.0 * 6.147960662841797
Epoch 280, val loss: 1.2011572122573853
Epoch 290, training loss: 7.1210222244262695 = 0.9791615009307861 + 1.0 * 6.141860485076904
Epoch 290, val loss: 1.1651750802993774
Epoch 300, training loss: 7.063290596008301 = 0.9268630146980286 + 1.0 * 6.136427402496338
Epoch 300, val loss: 1.1288459300994873
Epoch 310, training loss: 7.006289005279541 = 0.8746032118797302 + 1.0 * 6.131685733795166
Epoch 310, val loss: 1.0922682285308838
Epoch 320, training loss: 6.963101863861084 = 0.823722779750824 + 1.0 * 6.139379024505615
Epoch 320, val loss: 1.0563768148422241
Epoch 330, training loss: 6.901125431060791 = 0.7760352492332458 + 1.0 * 6.1250901222229
Epoch 330, val loss: 1.0226513147354126
Epoch 340, training loss: 6.853570938110352 = 0.7304986119270325 + 1.0 * 6.123072147369385
Epoch 340, val loss: 0.9906215071678162
Epoch 350, training loss: 6.805072784423828 = 0.6868240237236023 + 1.0 * 6.11824893951416
Epoch 350, val loss: 0.9601098895072937
Epoch 360, training loss: 6.762526988983154 = 0.6450020670890808 + 1.0 * 6.117525100708008
Epoch 360, val loss: 0.9314218759536743
Epoch 370, training loss: 6.71934175491333 = 0.6057077646255493 + 1.0 * 6.11363410949707
Epoch 370, val loss: 0.9053276777267456
Epoch 380, training loss: 6.680903911590576 = 0.568938136100769 + 1.0 * 6.111965656280518
Epoch 380, val loss: 0.8817927241325378
Epoch 390, training loss: 6.641164302825928 = 0.533879816532135 + 1.0 * 6.1072845458984375
Epoch 390, val loss: 0.8603575825691223
Epoch 400, training loss: 6.617310047149658 = 0.5004379153251648 + 1.0 * 6.116872310638428
Epoch 400, val loss: 0.8410292863845825
Epoch 410, training loss: 6.574268341064453 = 0.46909621357917786 + 1.0 * 6.105172157287598
Epoch 410, val loss: 0.8241310715675354
Epoch 420, training loss: 6.540094375610352 = 0.4394199848175049 + 1.0 * 6.100674629211426
Epoch 420, val loss: 0.8094227313995361
Epoch 430, training loss: 6.511346340179443 = 0.41121235489845276 + 1.0 * 6.100133895874023
Epoch 430, val loss: 0.7965250611305237
Epoch 440, training loss: 6.485957622528076 = 0.38487401604652405 + 1.0 * 6.101083755493164
Epoch 440, val loss: 0.7855091691017151
Epoch 450, training loss: 6.454459190368652 = 0.3603387773036957 + 1.0 * 6.094120502471924
Epoch 450, val loss: 0.7763723731040955
Epoch 460, training loss: 6.428859710693359 = 0.33729636669158936 + 1.0 * 6.0915632247924805
Epoch 460, val loss: 0.768648624420166
Epoch 470, training loss: 6.404679298400879 = 0.3156694173812866 + 1.0 * 6.089009761810303
Epoch 470, val loss: 0.7621886134147644
Epoch 480, training loss: 6.389086723327637 = 0.2954789996147156 + 1.0 * 6.0936079025268555
Epoch 480, val loss: 0.7569282054901123
Epoch 490, training loss: 6.362976551055908 = 0.2769886255264282 + 1.0 * 6.0859880447387695
Epoch 490, val loss: 0.752958357334137
Epoch 500, training loss: 6.344550609588623 = 0.2598009705543518 + 1.0 * 6.084749698638916
Epoch 500, val loss: 0.7501481771469116
Epoch 510, training loss: 6.332724571228027 = 0.24374833703041077 + 1.0 * 6.0889763832092285
Epoch 510, val loss: 0.7481778264045715
Epoch 520, training loss: 6.312740802764893 = 0.22899527847766876 + 1.0 * 6.08374547958374
Epoch 520, val loss: 0.747087299823761
Epoch 530, training loss: 6.2944111824035645 = 0.21529467403888702 + 1.0 * 6.079116344451904
Epoch 530, val loss: 0.7468755841255188
Epoch 540, training loss: 6.294284820556641 = 0.20251139998435974 + 1.0 * 6.091773509979248
Epoch 540, val loss: 0.7472384572029114
Epoch 550, training loss: 6.2692341804504395 = 0.19076333940029144 + 1.0 * 6.078470706939697
Epoch 550, val loss: 0.7480695247650146
Epoch 560, training loss: 6.253113746643066 = 0.17984578013420105 + 1.0 * 6.073267936706543
Epoch 560, val loss: 0.7495455741882324
Epoch 570, training loss: 6.241006374359131 = 0.16961337625980377 + 1.0 * 6.071393013000488
Epoch 570, val loss: 0.7514253258705139
Epoch 580, training loss: 6.2316131591796875 = 0.16001947224140167 + 1.0 * 6.071593761444092
Epoch 580, val loss: 0.7537704706192017
Epoch 590, training loss: 6.228888034820557 = 0.15109513700008392 + 1.0 * 6.077793121337891
Epoch 590, val loss: 0.7563912272453308
Epoch 600, training loss: 6.212365627288818 = 0.14280761778354645 + 1.0 * 6.069558143615723
Epoch 600, val loss: 0.7594014406204224
Epoch 610, training loss: 6.202184200286865 = 0.13506269454956055 + 1.0 * 6.067121505737305
Epoch 610, val loss: 0.7627742290496826
Epoch 620, training loss: 6.195346355438232 = 0.1277805119752884 + 1.0 * 6.06756591796875
Epoch 620, val loss: 0.7663708329200745
Epoch 630, training loss: 6.186896324157715 = 0.12097994983196259 + 1.0 * 6.065916538238525
Epoch 630, val loss: 0.770179033279419
Epoch 640, training loss: 6.177671432495117 = 0.11463860422372818 + 1.0 * 6.063032627105713
Epoch 640, val loss: 0.774346113204956
Epoch 650, training loss: 6.169421672821045 = 0.10868281126022339 + 1.0 * 6.060739040374756
Epoch 650, val loss: 0.7786653637886047
Epoch 660, training loss: 6.175022602081299 = 0.10309042781591415 + 1.0 * 6.071932315826416
Epoch 660, val loss: 0.7831834554672241
Epoch 670, training loss: 6.156347751617432 = 0.09788322448730469 + 1.0 * 6.058464527130127
Epoch 670, val loss: 0.7879453897476196
Epoch 680, training loss: 6.15103006362915 = 0.09300967305898666 + 1.0 * 6.05802059173584
Epoch 680, val loss: 0.7929340600967407
Epoch 690, training loss: 6.144291400909424 = 0.08842094987630844 + 1.0 * 6.055870532989502
Epoch 690, val loss: 0.7980876564979553
Epoch 700, training loss: 6.147843360900879 = 0.0841047391295433 + 1.0 * 6.063738822937012
Epoch 700, val loss: 0.8033760190010071
Epoch 710, training loss: 6.137484073638916 = 0.08007289469242096 + 1.0 * 6.057411193847656
Epoch 710, val loss: 0.8088316917419434
Epoch 720, training loss: 6.12986421585083 = 0.07630303502082825 + 1.0 * 6.053561210632324
Epoch 720, val loss: 0.814434826374054
Epoch 730, training loss: 6.124849319458008 = 0.07275021076202393 + 1.0 * 6.052099227905273
Epoch 730, val loss: 0.8200787305831909
Epoch 740, training loss: 6.129876613616943 = 0.06941767036914825 + 1.0 * 6.060459136962891
Epoch 740, val loss: 0.8256885409355164
Epoch 750, training loss: 6.119454383850098 = 0.06631886959075928 + 1.0 * 6.053135395050049
Epoch 750, val loss: 0.8313380479812622
Epoch 760, training loss: 6.113045692443848 = 0.06341828405857086 + 1.0 * 6.049627304077148
Epoch 760, val loss: 0.8371716141700745
Epoch 770, training loss: 6.1082763671875 = 0.060669202357530594 + 1.0 * 6.047606945037842
Epoch 770, val loss: 0.8429151177406311
Epoch 780, training loss: 6.104443073272705 = 0.05806597322225571 + 1.0 * 6.046377182006836
Epoch 780, val loss: 0.8487236499786377
Epoch 790, training loss: 6.1041131019592285 = 0.055604998022317886 + 1.0 * 6.048508167266846
Epoch 790, val loss: 0.8546873331069946
Epoch 800, training loss: 6.099445343017578 = 0.05328766256570816 + 1.0 * 6.0461578369140625
Epoch 800, val loss: 0.8604447841644287
Epoch 810, training loss: 6.098184585571289 = 0.05113116279244423 + 1.0 * 6.047053337097168
Epoch 810, val loss: 0.8663425445556641
Epoch 820, training loss: 6.092394828796387 = 0.04909069091081619 + 1.0 * 6.043303966522217
Epoch 820, val loss: 0.8722413182258606
Epoch 830, training loss: 6.094037055969238 = 0.04715441167354584 + 1.0 * 6.046882629394531
Epoch 830, val loss: 0.8780192136764526
Epoch 840, training loss: 6.088843822479248 = 0.0453261062502861 + 1.0 * 6.043517589569092
Epoch 840, val loss: 0.8838195204734802
Epoch 850, training loss: 6.090530872344971 = 0.04359837621450424 + 1.0 * 6.046932697296143
Epoch 850, val loss: 0.8896403908729553
Epoch 860, training loss: 6.083378791809082 = 0.04196947440505028 + 1.0 * 6.041409492492676
Epoch 860, val loss: 0.8954055905342102
Epoch 870, training loss: 6.080931663513184 = 0.0404217392206192 + 1.0 * 6.0405097007751465
Epoch 870, val loss: 0.9011889100074768
Epoch 880, training loss: 6.080843448638916 = 0.038953762501478195 + 1.0 * 6.041889667510986
Epoch 880, val loss: 0.9068630337715149
Epoch 890, training loss: 6.076066970825195 = 0.03756927326321602 + 1.0 * 6.0384979248046875
Epoch 890, val loss: 0.9124929308891296
Epoch 900, training loss: 6.074628829956055 = 0.036254435777664185 + 1.0 * 6.038374423980713
Epoch 900, val loss: 0.9181151390075684
Epoch 910, training loss: 6.088860034942627 = 0.035003501921892166 + 1.0 * 6.053856372833252
Epoch 910, val loss: 0.9235630631446838
Epoch 920, training loss: 6.074228286743164 = 0.03382626548409462 + 1.0 * 6.040401935577393
Epoch 920, val loss: 0.9288610816001892
Epoch 930, training loss: 6.06915807723999 = 0.03271765634417534 + 1.0 * 6.036440372467041
Epoch 930, val loss: 0.9343787431716919
Epoch 940, training loss: 6.066328525543213 = 0.03165486454963684 + 1.0 * 6.034673690795898
Epoch 940, val loss: 0.9397688508033752
Epoch 950, training loss: 6.067910671234131 = 0.030636832118034363 + 1.0 * 6.03727388381958
Epoch 950, val loss: 0.9450013041496277
Epoch 960, training loss: 6.0638227462768555 = 0.029665248468518257 + 1.0 * 6.0341572761535645
Epoch 960, val loss: 0.950167715549469
Epoch 970, training loss: 6.061498165130615 = 0.028744729235768318 + 1.0 * 6.0327534675598145
Epoch 970, val loss: 0.9554190039634705
Epoch 980, training loss: 6.060671329498291 = 0.027864806354045868 + 1.0 * 6.032806396484375
Epoch 980, val loss: 0.9606395363807678
Epoch 990, training loss: 6.067920207977295 = 0.02701789140701294 + 1.0 * 6.040902137756348
Epoch 990, val loss: 0.9657149910926819
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7380
Flip ASR: 0.7022/225 nodes
The final ASR:0.70603, 0.06719, Accuracy:0.81975, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9524])
updated graph: torch.Size([2, 10566])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00522, Accuracy:0.82716, 0.00349
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.313945770263672 = 1.9401888847351074 + 1.0 * 8.373757362365723
Epoch 0, val loss: 1.9468162059783936
Epoch 10, training loss: 10.303914070129395 = 1.9307243824005127 + 1.0 * 8.373189926147461
Epoch 10, val loss: 1.9370657205581665
Epoch 20, training loss: 10.288667678833008 = 1.9194304943084717 + 1.0 * 8.369236946105957
Epoch 20, val loss: 1.9251765012741089
Epoch 30, training loss: 10.242523193359375 = 1.903991460800171 + 1.0 * 8.338531494140625
Epoch 30, val loss: 1.909082055091858
Epoch 40, training loss: 9.96080207824707 = 1.8850828409194946 + 1.0 * 8.075718879699707
Epoch 40, val loss: 1.8907780647277832
Epoch 50, training loss: 8.979456901550293 = 1.8654536008834839 + 1.0 * 7.1140031814575195
Epoch 50, val loss: 1.8719950914382935
Epoch 60, training loss: 8.626845359802246 = 1.851596474647522 + 1.0 * 6.775249004364014
Epoch 60, val loss: 1.859520435333252
Epoch 70, training loss: 8.454483032226562 = 1.8397433757781982 + 1.0 * 6.614739418029785
Epoch 70, val loss: 1.8478471040725708
Epoch 80, training loss: 8.341938018798828 = 1.8285802602767944 + 1.0 * 6.513358116149902
Epoch 80, val loss: 1.836974024772644
Epoch 90, training loss: 8.246688842773438 = 1.8176618814468384 + 1.0 * 6.429027080535889
Epoch 90, val loss: 1.826168417930603
Epoch 100, training loss: 8.177297592163086 = 1.8074431419372559 + 1.0 * 6.36985445022583
Epoch 100, val loss: 1.8162459135055542
Epoch 110, training loss: 8.123452186584473 = 1.797408938407898 + 1.0 * 6.326043605804443
Epoch 110, val loss: 1.8064435720443726
Epoch 120, training loss: 8.0811185836792 = 1.7870789766311646 + 1.0 * 6.294039726257324
Epoch 120, val loss: 1.7963464260101318
Epoch 130, training loss: 8.046089172363281 = 1.775890827178955 + 1.0 * 6.270198822021484
Epoch 130, val loss: 1.7856348752975464
Epoch 140, training loss: 8.01241683959961 = 1.7634474039077759 + 1.0 * 6.248969554901123
Epoch 140, val loss: 1.7741323709487915
Epoch 150, training loss: 7.980277061462402 = 1.7492387294769287 + 1.0 * 6.2310380935668945
Epoch 150, val loss: 1.7614682912826538
Epoch 160, training loss: 7.946828365325928 = 1.7327089309692383 + 1.0 * 6.2141194343566895
Epoch 160, val loss: 1.7472891807556152
Epoch 170, training loss: 7.9115519523620605 = 1.7128461599349976 + 1.0 * 6.198705673217773
Epoch 170, val loss: 1.7305727005004883
Epoch 180, training loss: 7.87429141998291 = 1.6883037090301514 + 1.0 * 6.18598747253418
Epoch 180, val loss: 1.710179328918457
Epoch 190, training loss: 7.8331828117370605 = 1.6578611135482788 + 1.0 * 6.175321578979492
Epoch 190, val loss: 1.6849275827407837
Epoch 200, training loss: 7.786802291870117 = 1.6203887462615967 + 1.0 * 6.1664137840271
Epoch 200, val loss: 1.6537564992904663
Epoch 210, training loss: 7.733019828796387 = 1.5748791694641113 + 1.0 * 6.158140659332275
Epoch 210, val loss: 1.6159015893936157
Epoch 220, training loss: 7.675886631011963 = 1.5224460363388062 + 1.0 * 6.153440475463867
Epoch 220, val loss: 1.5728192329406738
Epoch 230, training loss: 7.611730098724365 = 1.4647908210754395 + 1.0 * 6.146939277648926
Epoch 230, val loss: 1.5255582332611084
Epoch 240, training loss: 7.544879913330078 = 1.4027522802352905 + 1.0 * 6.142127513885498
Epoch 240, val loss: 1.475361943244934
Epoch 250, training loss: 7.475959300994873 = 1.3385781049728394 + 1.0 * 6.137381076812744
Epoch 250, val loss: 1.4245338439941406
Epoch 260, training loss: 7.411012649536133 = 1.275124192237854 + 1.0 * 6.135888576507568
Epoch 260, val loss: 1.3756208419799805
Epoch 270, training loss: 7.343910217285156 = 1.2149527072906494 + 1.0 * 6.128957271575928
Epoch 270, val loss: 1.3304046392440796
Epoch 280, training loss: 7.2830681800842285 = 1.157395839691162 + 1.0 * 6.125672340393066
Epoch 280, val loss: 1.2880305051803589
Epoch 290, training loss: 7.22613525390625 = 1.102822184562683 + 1.0 * 6.123312950134277
Epoch 290, val loss: 1.2485145330429077
Epoch 300, training loss: 7.169529438018799 = 1.0514825582504272 + 1.0 * 6.118046760559082
Epoch 300, val loss: 1.2117253541946411
Epoch 310, training loss: 7.123420238494873 = 1.0028079748153687 + 1.0 * 6.120612144470215
Epoch 310, val loss: 1.1770261526107788
Epoch 320, training loss: 7.067574501037598 = 0.9574265480041504 + 1.0 * 6.110147953033447
Epoch 320, val loss: 1.1449072360992432
Epoch 330, training loss: 7.022487640380859 = 0.9144288897514343 + 1.0 * 6.108058929443359
Epoch 330, val loss: 1.1143985986709595
Epoch 340, training loss: 6.9774017333984375 = 0.8732454776763916 + 1.0 * 6.104156017303467
Epoch 340, val loss: 1.0851212739944458
Epoch 350, training loss: 6.937788009643555 = 0.8336039781570435 + 1.0 * 6.104184150695801
Epoch 350, val loss: 1.0569002628326416
Epoch 360, training loss: 6.899467468261719 = 0.7956976294517517 + 1.0 * 6.103769779205322
Epoch 360, val loss: 1.0299031734466553
Epoch 370, training loss: 6.856827259063721 = 0.7591819763183594 + 1.0 * 6.097645282745361
Epoch 370, val loss: 1.0036687850952148
Epoch 380, training loss: 6.818729400634766 = 0.7234602570533752 + 1.0 * 6.095269203186035
Epoch 380, val loss: 0.9780958294868469
Epoch 390, training loss: 6.783456802368164 = 0.6881738901138306 + 1.0 * 6.095283031463623
Epoch 390, val loss: 0.9528709053993225
Epoch 400, training loss: 6.747310638427734 = 0.6536788940429688 + 1.0 * 6.093631744384766
Epoch 400, val loss: 0.9284731149673462
Epoch 410, training loss: 6.7093682289123535 = 0.6199656128883362 + 1.0 * 6.089402675628662
Epoch 410, val loss: 0.9050582051277161
Epoch 420, training loss: 6.677224159240723 = 0.5867604613304138 + 1.0 * 6.090463638305664
Epoch 420, val loss: 0.8824806213378906
Epoch 430, training loss: 6.64396858215332 = 0.554276168346405 + 1.0 * 6.08969259262085
Epoch 430, val loss: 0.8612381815910339
Epoch 440, training loss: 6.608477592468262 = 0.5226032137870789 + 1.0 * 6.085874557495117
Epoch 440, val loss: 0.841370165348053
Epoch 450, training loss: 6.574770927429199 = 0.4916135370731354 + 1.0 * 6.083157539367676
Epoch 450, val loss: 0.8228972554206848
Epoch 460, training loss: 6.5408806800842285 = 0.46140164136886597 + 1.0 * 6.079479217529297
Epoch 460, val loss: 0.8060036301612854
Epoch 470, training loss: 6.509609222412109 = 0.431893914937973 + 1.0 * 6.0777153968811035
Epoch 470, val loss: 0.7906657457351685
Epoch 480, training loss: 6.484670162200928 = 0.4032781720161438 + 1.0 * 6.08139181137085
Epoch 480, val loss: 0.7769360542297363
Epoch 490, training loss: 6.452735424041748 = 0.37616434693336487 + 1.0 * 6.076570987701416
Epoch 490, val loss: 0.7653829455375671
Epoch 500, training loss: 6.4243316650390625 = 0.350322425365448 + 1.0 * 6.074009418487549
Epoch 500, val loss: 0.7557092905044556
Epoch 510, training loss: 6.397150039672852 = 0.32570111751556396 + 1.0 * 6.071448802947998
Epoch 510, val loss: 0.7475466132164001
Epoch 520, training loss: 6.379565715789795 = 0.3025159239768982 + 1.0 * 6.077049732208252
Epoch 520, val loss: 0.7410926818847656
Epoch 530, training loss: 6.349425792694092 = 0.2809985876083374 + 1.0 * 6.068427085876465
Epoch 530, val loss: 0.7363677024841309
Epoch 540, training loss: 6.330041885375977 = 0.2610393166542053 + 1.0 * 6.069002628326416
Epoch 540, val loss: 0.7330980896949768
Epoch 550, training loss: 6.3095269203186035 = 0.24259082973003387 + 1.0 * 6.066936016082764
Epoch 550, val loss: 0.730974018573761
Epoch 560, training loss: 6.29433536529541 = 0.22569233179092407 + 1.0 * 6.068643093109131
Epoch 560, val loss: 0.7301340699195862
Epoch 570, training loss: 6.274145603179932 = 0.2103773057460785 + 1.0 * 6.06376838684082
Epoch 570, val loss: 0.730445384979248
Epoch 580, training loss: 6.2573323249816895 = 0.19636400043964386 + 1.0 * 6.060968399047852
Epoch 580, val loss: 0.7316355109214783
Epoch 590, training loss: 6.243037700653076 = 0.18348006904125214 + 1.0 * 6.0595574378967285
Epoch 590, val loss: 0.733512282371521
Epoch 600, training loss: 6.238128662109375 = 0.17167361080646515 + 1.0 * 6.066454887390137
Epoch 600, val loss: 0.736014187335968
Epoch 610, training loss: 6.223634719848633 = 0.16099895536899567 + 1.0 * 6.062635898590088
Epoch 610, val loss: 0.7392701506614685
Epoch 620, training loss: 6.208303451538086 = 0.15125282108783722 + 1.0 * 6.057050704956055
Epoch 620, val loss: 0.7431254386901855
Epoch 630, training loss: 6.19704532623291 = 0.14226102828979492 + 1.0 * 6.054784297943115
Epoch 630, val loss: 0.7472145557403564
Epoch 640, training loss: 6.194928169250488 = 0.1339670568704605 + 1.0 * 6.0609612464904785
Epoch 640, val loss: 0.7517773509025574
Epoch 650, training loss: 6.182993412017822 = 0.12635919451713562 + 1.0 * 6.056634426116943
Epoch 650, val loss: 0.7566996812820435
Epoch 660, training loss: 6.171873569488525 = 0.11935950070619583 + 1.0 * 6.05251407623291
Epoch 660, val loss: 0.7618592977523804
Epoch 670, training loss: 6.164892196655273 = 0.11291590332984924 + 1.0 * 6.051976203918457
Epoch 670, val loss: 0.7673236727714539
Epoch 680, training loss: 6.156373023986816 = 0.1069430261850357 + 1.0 * 6.049429893493652
Epoch 680, val loss: 0.7730108499526978
Epoch 690, training loss: 6.15287446975708 = 0.10140043497085571 + 1.0 * 6.051474094390869
Epoch 690, val loss: 0.778714120388031
Epoch 700, training loss: 6.143315315246582 = 0.09627865254878998 + 1.0 * 6.047036647796631
Epoch 700, val loss: 0.7847288250923157
Epoch 710, training loss: 6.145801067352295 = 0.09150702506303787 + 1.0 * 6.054294109344482
Epoch 710, val loss: 0.7907572388648987
Epoch 720, training loss: 6.134982585906982 = 0.08708305656909943 + 1.0 * 6.0478997230529785
Epoch 720, val loss: 0.7967458367347717
Epoch 730, training loss: 6.126775741577148 = 0.08296996355056763 + 1.0 * 6.0438055992126465
Epoch 730, val loss: 0.803072988986969
Epoch 740, training loss: 6.123318672180176 = 0.07911571115255356 + 1.0 * 6.04420280456543
Epoch 740, val loss: 0.8092285394668579
Epoch 750, training loss: 6.12176513671875 = 0.07551141083240509 + 1.0 * 6.046253681182861
Epoch 750, val loss: 0.8154128193855286
Epoch 760, training loss: 6.113693714141846 = 0.07215508818626404 + 1.0 * 6.041538715362549
Epoch 760, val loss: 0.8217266798019409
Epoch 770, training loss: 6.120189666748047 = 0.06901542097330093 + 1.0 * 6.051174163818359
Epoch 770, val loss: 0.827906608581543
Epoch 780, training loss: 6.107181072235107 = 0.06608600914478302 + 1.0 * 6.04109525680542
Epoch 780, val loss: 0.8340650200843811
Epoch 790, training loss: 6.102095127105713 = 0.06333547830581665 + 1.0 * 6.038759708404541
Epoch 790, val loss: 0.8403554558753967
Epoch 800, training loss: 6.103649139404297 = 0.06073673814535141 + 1.0 * 6.042912483215332
Epoch 800, val loss: 0.8464695811271667
Epoch 810, training loss: 6.095417499542236 = 0.058298416435718536 + 1.0 * 6.037118911743164
Epoch 810, val loss: 0.8524506688117981
Epoch 820, training loss: 6.0918288230896 = 0.056003302335739136 + 1.0 * 6.035825729370117
Epoch 820, val loss: 0.8587016463279724
Epoch 830, training loss: 6.088747501373291 = 0.053828269243240356 + 1.0 * 6.034919261932373
Epoch 830, val loss: 0.8647223711013794
Epoch 840, training loss: 6.103448867797852 = 0.05176825076341629 + 1.0 * 6.051680564880371
Epoch 840, val loss: 0.8706058263778687
Epoch 850, training loss: 6.08881950378418 = 0.04984358325600624 + 1.0 * 6.038975715637207
Epoch 850, val loss: 0.8763917088508606
Epoch 860, training loss: 6.080923557281494 = 0.04803495109081268 + 1.0 * 6.032888412475586
Epoch 860, val loss: 0.8824196457862854
Epoch 870, training loss: 6.079854965209961 = 0.04631630703806877 + 1.0 * 6.033538818359375
Epoch 870, val loss: 0.8881449103355408
Epoch 880, training loss: 6.079174041748047 = 0.04467787966132164 + 1.0 * 6.034496307373047
Epoch 880, val loss: 0.8937641382217407
Epoch 890, training loss: 6.0760369300842285 = 0.04312239587306976 + 1.0 * 6.032914638519287
Epoch 890, val loss: 0.8995454907417297
Epoch 900, training loss: 6.072737693786621 = 0.04164540022611618 + 1.0 * 6.031092166900635
Epoch 900, val loss: 0.905112624168396
Epoch 910, training loss: 6.069509983062744 = 0.04024329036474228 + 1.0 * 6.029266834259033
Epoch 910, val loss: 0.9107696413993835
Epoch 920, training loss: 6.075566291809082 = 0.03890693560242653 + 1.0 * 6.036659240722656
Epoch 920, val loss: 0.9163928031921387
Epoch 930, training loss: 6.0699591636657715 = 0.03763844072818756 + 1.0 * 6.032320499420166
Epoch 930, val loss: 0.9215931296348572
Epoch 940, training loss: 6.065017223358154 = 0.03643518313765526 + 1.0 * 6.0285820960998535
Epoch 940, val loss: 0.9272623062133789
Epoch 950, training loss: 6.0665974617004395 = 0.035284556448459625 + 1.0 * 6.031312942504883
Epoch 950, val loss: 0.932574987411499
Epoch 960, training loss: 6.061685085296631 = 0.034182462841272354 + 1.0 * 6.027502536773682
Epoch 960, val loss: 0.9377202391624451
Epoch 970, training loss: 6.059825897216797 = 0.03313944861292839 + 1.0 * 6.026686668395996
Epoch 970, val loss: 0.9431542754173279
Epoch 980, training loss: 6.057673931121826 = 0.0321371890604496 + 1.0 * 6.02553653717041
Epoch 980, val loss: 0.9484127759933472
Epoch 990, training loss: 6.066952705383301 = 0.03117331862449646 + 1.0 * 6.0357794761657715
Epoch 990, val loss: 0.9534863233566284
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6900
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.310096740722656 = 1.9363152980804443 + 1.0 * 8.373781204223633
Epoch 0, val loss: 1.9263666868209839
Epoch 10, training loss: 10.2999906539917 = 1.9268499612808228 + 1.0 * 8.373140335083008
Epoch 10, val loss: 1.9174575805664062
Epoch 20, training loss: 10.284244537353516 = 1.9153401851654053 + 1.0 * 8.368904113769531
Epoch 20, val loss: 1.9063012599945068
Epoch 30, training loss: 10.239029884338379 = 1.8996964693069458 + 1.0 * 8.339333534240723
Epoch 30, val loss: 1.8909790515899658
Epoch 40, training loss: 10.014347076416016 = 1.8804553747177124 + 1.0 * 8.133892059326172
Epoch 40, val loss: 1.87260901927948
Epoch 50, training loss: 9.158096313476562 = 1.8610388040542603 + 1.0 * 7.297057628631592
Epoch 50, val loss: 1.8541752099990845
Epoch 60, training loss: 8.798259735107422 = 1.8455168008804321 + 1.0 * 6.952742576599121
Epoch 60, val loss: 1.8399548530578613
Epoch 70, training loss: 8.597097396850586 = 1.8322240114212036 + 1.0 * 6.764873027801514
Epoch 70, val loss: 1.8270344734191895
Epoch 80, training loss: 8.437677383422852 = 1.8190436363220215 + 1.0 * 6.61863374710083
Epoch 80, val loss: 1.814439296722412
Epoch 90, training loss: 8.316291809082031 = 1.805878758430481 + 1.0 * 6.510412693023682
Epoch 90, val loss: 1.8017369508743286
Epoch 100, training loss: 8.245939254760742 = 1.7926822900772095 + 1.0 * 6.453256607055664
Epoch 100, val loss: 1.7895066738128662
Epoch 110, training loss: 8.185175895690918 = 1.779274582862854 + 1.0 * 6.4059014320373535
Epoch 110, val loss: 1.777355432510376
Epoch 120, training loss: 8.131526947021484 = 1.7654136419296265 + 1.0 * 6.366113185882568
Epoch 120, val loss: 1.765049695968628
Epoch 130, training loss: 8.08004379272461 = 1.7503063678741455 + 1.0 * 6.329737186431885
Epoch 130, val loss: 1.7519723176956177
Epoch 140, training loss: 8.031761169433594 = 1.732875943183899 + 1.0 * 6.298885345458984
Epoch 140, val loss: 1.7372208833694458
Epoch 150, training loss: 7.984955310821533 = 1.7116562128067017 + 1.0 * 6.273299217224121
Epoch 150, val loss: 1.719634771347046
Epoch 160, training loss: 7.937102794647217 = 1.6855854988098145 + 1.0 * 6.251517295837402
Epoch 160, val loss: 1.6982837915420532
Epoch 170, training loss: 7.886636734008789 = 1.6529723405838013 + 1.0 * 6.233664512634277
Epoch 170, val loss: 1.6717145442962646
Epoch 180, training loss: 7.831669330596924 = 1.6133127212524414 + 1.0 * 6.218356609344482
Epoch 180, val loss: 1.6396993398666382
Epoch 190, training loss: 7.771683692932129 = 1.566146731376648 + 1.0 * 6.205536842346191
Epoch 190, val loss: 1.601469874382019
Epoch 200, training loss: 7.705333709716797 = 1.5109349489212036 + 1.0 * 6.194398880004883
Epoch 200, val loss: 1.5567960739135742
Epoch 210, training loss: 7.635472297668457 = 1.4505069255828857 + 1.0 * 6.184965133666992
Epoch 210, val loss: 1.5082975625991821
Epoch 220, training loss: 7.5632171630859375 = 1.387540578842163 + 1.0 * 6.175676345825195
Epoch 220, val loss: 1.4582754373550415
Epoch 230, training loss: 7.494858264923096 = 1.3246046304702759 + 1.0 * 6.170253753662109
Epoch 230, val loss: 1.4090129137039185
Epoch 240, training loss: 7.427311897277832 = 1.264744520187378 + 1.0 * 6.162567138671875
Epoch 240, val loss: 1.3630986213684082
Epoch 250, training loss: 7.362512111663818 = 1.208704948425293 + 1.0 * 6.153807163238525
Epoch 250, val loss: 1.3210134506225586
Epoch 260, training loss: 7.303092956542969 = 1.1555964946746826 + 1.0 * 6.147496700286865
Epoch 260, val loss: 1.2816832065582275
Epoch 270, training loss: 7.248392581939697 = 1.1052865982055664 + 1.0 * 6.143105983734131
Epoch 270, val loss: 1.244834303855896
Epoch 280, training loss: 7.192808628082275 = 1.057315707206726 + 1.0 * 6.13549280166626
Epoch 280, val loss: 1.2101434469223022
Epoch 290, training loss: 7.141556739807129 = 1.0109782218933105 + 1.0 * 6.130578517913818
Epoch 290, val loss: 1.1768531799316406
Epoch 300, training loss: 7.097651481628418 = 0.9660822153091431 + 1.0 * 6.1315693855285645
Epoch 300, val loss: 1.1447259187698364
Epoch 310, training loss: 7.044454574584961 = 0.9229819178581238 + 1.0 * 6.1214728355407715
Epoch 310, val loss: 1.1138951778411865
Epoch 320, training loss: 6.997373580932617 = 0.8810074329376221 + 1.0 * 6.116366386413574
Epoch 320, val loss: 1.0839293003082275
Epoch 330, training loss: 6.952800750732422 = 0.8397391438484192 + 1.0 * 6.113061428070068
Epoch 330, val loss: 1.054442286491394
Epoch 340, training loss: 6.917150497436523 = 0.7997623682022095 + 1.0 * 6.1173882484436035
Epoch 340, val loss: 1.0260072946548462
Epoch 350, training loss: 6.869379043579102 = 0.7617887854576111 + 1.0 * 6.107590198516846
Epoch 350, val loss: 0.9991874694824219
Epoch 360, training loss: 6.828023910522461 = 0.7250295877456665 + 1.0 * 6.102994441986084
Epoch 360, val loss: 0.973558783531189
Epoch 370, training loss: 6.790746212005615 = 0.689419686794281 + 1.0 * 6.1013264656066895
Epoch 370, val loss: 0.949118971824646
Epoch 380, training loss: 6.756160259246826 = 0.655433714389801 + 1.0 * 6.10072660446167
Epoch 380, val loss: 0.9263748526573181
Epoch 390, training loss: 6.71859884262085 = 0.6230337023735046 + 1.0 * 6.095565319061279
Epoch 390, val loss: 0.9055934548377991
Epoch 400, training loss: 6.686020374298096 = 0.5917277932167053 + 1.0 * 6.094292640686035
Epoch 400, val loss: 0.8862249255180359
Epoch 410, training loss: 6.654413223266602 = 0.5615851879119873 + 1.0 * 6.092827796936035
Epoch 410, val loss: 0.8683015704154968
Epoch 420, training loss: 6.619730472564697 = 0.532275378704071 + 1.0 * 6.0874552726745605
Epoch 420, val loss: 0.8517955541610718
Epoch 430, training loss: 6.589893341064453 = 0.5035423636436462 + 1.0 * 6.086350917816162
Epoch 430, val loss: 0.8362926840782166
Epoch 440, training loss: 6.560443878173828 = 0.475507915019989 + 1.0 * 6.084936141967773
Epoch 440, val loss: 0.821677029132843
Epoch 450, training loss: 6.531890869140625 = 0.4483048915863037 + 1.0 * 6.083585739135742
Epoch 450, val loss: 0.8083322048187256
Epoch 460, training loss: 6.502113342285156 = 0.42200565338134766 + 1.0 * 6.080107688903809
Epoch 460, val loss: 0.7959297299385071
Epoch 470, training loss: 6.477207660675049 = 0.39659491181373596 + 1.0 * 6.080612659454346
Epoch 470, val loss: 0.7844808101654053
Epoch 480, training loss: 6.450183868408203 = 0.3723088502883911 + 1.0 * 6.077875137329102
Epoch 480, val loss: 0.7741427421569824
Epoch 490, training loss: 6.430082321166992 = 0.3493228852748871 + 1.0 * 6.080759525299072
Epoch 490, val loss: 0.7650829553604126
Epoch 500, training loss: 6.402569770812988 = 0.32764384150505066 + 1.0 * 6.074925899505615
Epoch 500, val loss: 0.7571806311607361
Epoch 510, training loss: 6.3852386474609375 = 0.30725419521331787 + 1.0 * 6.07798433303833
Epoch 510, val loss: 0.7504981756210327
Epoch 520, training loss: 6.361319065093994 = 0.28825050592422485 + 1.0 * 6.073068618774414
Epoch 520, val loss: 0.7447983026504517
Epoch 530, training loss: 6.340240001678467 = 0.2704882025718689 + 1.0 * 6.069751739501953
Epoch 530, val loss: 0.7404363751411438
Epoch 540, training loss: 6.320817947387695 = 0.25379666686058044 + 1.0 * 6.067021369934082
Epoch 540, val loss: 0.7369099259376526
Epoch 550, training loss: 6.316022872924805 = 0.23822522163391113 + 1.0 * 6.0777974128723145
Epoch 550, val loss: 0.7342085838317871
Epoch 560, training loss: 6.292092323303223 = 0.22379465401172638 + 1.0 * 6.068297863006592
Epoch 560, val loss: 0.7324492335319519
Epoch 570, training loss: 6.274469375610352 = 0.21032370626926422 + 1.0 * 6.064145565032959
Epoch 570, val loss: 0.7315293550491333
Epoch 580, training loss: 6.262665271759033 = 0.1977086365222931 + 1.0 * 6.0649566650390625
Epoch 580, val loss: 0.7311425805091858
Epoch 590, training loss: 6.250242233276367 = 0.18597006797790527 + 1.0 * 6.064271926879883
Epoch 590, val loss: 0.7314236760139465
Epoch 600, training loss: 6.235781192779541 = 0.17508940398693085 + 1.0 * 6.060691833496094
Epoch 600, val loss: 0.7321998476982117
Epoch 610, training loss: 6.224926471710205 = 0.1650184690952301 + 1.0 * 6.059907913208008
Epoch 610, val loss: 0.7336597442626953
Epoch 620, training loss: 6.213481426239014 = 0.15560248494148254 + 1.0 * 6.0578789710998535
Epoch 620, val loss: 0.7355700731277466
Epoch 630, training loss: 6.202317237854004 = 0.146767258644104 + 1.0 * 6.0555500984191895
Epoch 630, val loss: 0.7378299832344055
Epoch 640, training loss: 6.192732334136963 = 0.138466939330101 + 1.0 * 6.05426549911499
Epoch 640, val loss: 0.7405902147293091
Epoch 650, training loss: 6.202141761779785 = 0.13068276643753052 + 1.0 * 6.07145881652832
Epoch 650, val loss: 0.7436496019363403
Epoch 660, training loss: 6.182497978210449 = 0.12349400669336319 + 1.0 * 6.059003829956055
Epoch 660, val loss: 0.7467811703681946
Epoch 670, training loss: 6.169713020324707 = 0.11679118871688843 + 1.0 * 6.052921772003174
Epoch 670, val loss: 0.7504094243049622
Epoch 680, training loss: 6.160477161407471 = 0.11051160842180252 + 1.0 * 6.0499653816223145
Epoch 680, val loss: 0.7541340589523315
Epoch 690, training loss: 6.156305313110352 = 0.10461483150720596 + 1.0 * 6.051690578460693
Epoch 690, val loss: 0.7581295967102051
Epoch 700, training loss: 6.147938251495361 = 0.09910175949335098 + 1.0 * 6.048836708068848
Epoch 700, val loss: 0.7622233629226685
Epoch 710, training loss: 6.152672290802002 = 0.09396621584892273 + 1.0 * 6.058706283569336
Epoch 710, val loss: 0.7665267586708069
Epoch 720, training loss: 6.137233734130859 = 0.08919098228216171 + 1.0 * 6.0480427742004395
Epoch 720, val loss: 0.7709429860115051
Epoch 730, training loss: 6.130092620849609 = 0.08471601456403732 + 1.0 * 6.045376777648926
Epoch 730, val loss: 0.7755266427993774
Epoch 740, training loss: 6.126173973083496 = 0.08050806820392609 + 1.0 * 6.045665740966797
Epoch 740, val loss: 0.7800998687744141
Epoch 750, training loss: 6.1225433349609375 = 0.07656368613243103 + 1.0 * 6.0459794998168945
Epoch 750, val loss: 0.7847395539283752
Epoch 760, training loss: 6.115896701812744 = 0.07287482917308807 + 1.0 * 6.0430216789245605
Epoch 760, val loss: 0.7896153926849365
Epoch 770, training loss: 6.111265182495117 = 0.06940707564353943 + 1.0 * 6.041858196258545
Epoch 770, val loss: 0.7944910526275635
Epoch 780, training loss: 6.108297348022461 = 0.0661383792757988 + 1.0 * 6.042159080505371
Epoch 780, val loss: 0.7993353009223938
Epoch 790, training loss: 6.107186317443848 = 0.06307630240917206 + 1.0 * 6.04410982131958
Epoch 790, val loss: 0.804086446762085
Epoch 800, training loss: 6.09993314743042 = 0.060226984322071075 + 1.0 * 6.039706230163574
Epoch 800, val loss: 0.8090914487838745
Epoch 810, training loss: 6.096456527709961 = 0.05754151567816734 + 1.0 * 6.038915157318115
Epoch 810, val loss: 0.8140343427658081
Epoch 820, training loss: 6.1007490158081055 = 0.05500692501664162 + 1.0 * 6.045742034912109
Epoch 820, val loss: 0.8187958598136902
Epoch 830, training loss: 6.091314315795898 = 0.052632205188274384 + 1.0 * 6.038681983947754
Epoch 830, val loss: 0.823697030544281
Epoch 840, training loss: 6.0868425369262695 = 0.050392087548971176 + 1.0 * 6.036450386047363
Epoch 840, val loss: 0.828710675239563
Epoch 850, training loss: 6.085014343261719 = 0.04827189818024635 + 1.0 * 6.036742210388184
Epoch 850, val loss: 0.833551824092865
Epoch 860, training loss: 6.084670066833496 = 0.04627430438995361 + 1.0 * 6.038395881652832
Epoch 860, val loss: 0.8382323384284973
Epoch 870, training loss: 6.081353187561035 = 0.04440959542989731 + 1.0 * 6.036943435668945
Epoch 870, val loss: 0.8431480526924133
Epoch 880, training loss: 6.075509071350098 = 0.0426497682929039 + 1.0 * 6.0328593254089355
Epoch 880, val loss: 0.8480433225631714
Epoch 890, training loss: 6.074177265167236 = 0.040975116193294525 + 1.0 * 6.033202171325684
Epoch 890, val loss: 0.8527471423149109
Epoch 900, training loss: 6.079668045043945 = 0.03938741609454155 + 1.0 * 6.040280818939209
Epoch 900, val loss: 0.8573383688926697
Epoch 910, training loss: 6.072620868682861 = 0.0378977470099926 + 1.0 * 6.034723281860352
Epoch 910, val loss: 0.8620768189430237
Epoch 920, training loss: 6.068664073944092 = 0.036495231091976166 + 1.0 * 6.032168865203857
Epoch 920, val loss: 0.8669036030769348
Epoch 930, training loss: 6.0675177574157715 = 0.03516587242484093 + 1.0 * 6.032351970672607
Epoch 930, val loss: 0.8714570999145508
Epoch 940, training loss: 6.065919876098633 = 0.033901821821928024 + 1.0 * 6.032018184661865
Epoch 940, val loss: 0.8759093880653381
Epoch 950, training loss: 6.062856197357178 = 0.03270270675420761 + 1.0 * 6.030153274536133
Epoch 950, val loss: 0.8804810047149658
Epoch 960, training loss: 6.061895370483398 = 0.03156764432787895 + 1.0 * 6.030327796936035
Epoch 960, val loss: 0.8850875496864319
Epoch 970, training loss: 6.05932092666626 = 0.03048715554177761 + 1.0 * 6.028833866119385
Epoch 970, val loss: 0.8895161747932434
Epoch 980, training loss: 6.063179016113281 = 0.02946077473461628 + 1.0 * 6.033718109130859
Epoch 980, val loss: 0.8939487338066101
Epoch 990, training loss: 6.056702136993408 = 0.028482703492045403 + 1.0 * 6.028219223022461
Epoch 990, val loss: 0.8982570767402649
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7601
Flip ASR: 0.7244/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320971488952637 = 1.9471218585968018 + 1.0 * 8.373849868774414
Epoch 0, val loss: 1.9478673934936523
Epoch 10, training loss: 10.31076717376709 = 1.937317967414856 + 1.0 * 8.373449325561523
Epoch 10, val loss: 1.9377714395523071
Epoch 20, training loss: 10.29615592956543 = 1.925205111503601 + 1.0 * 8.370950698852539
Epoch 20, val loss: 1.9250985383987427
Epoch 30, training loss: 10.261640548706055 = 1.9081525802612305 + 1.0 * 8.353487968444824
Epoch 30, val loss: 1.9071248769760132
Epoch 40, training loss: 10.113668441772461 = 1.8856576681137085 + 1.0 * 8.228011131286621
Epoch 40, val loss: 1.8841469287872314
Epoch 50, training loss: 9.402710914611816 = 1.8622570037841797 + 1.0 * 7.540453910827637
Epoch 50, val loss: 1.8612605333328247
Epoch 60, training loss: 8.944021224975586 = 1.844421625137329 + 1.0 * 7.099599361419678
Epoch 60, val loss: 1.8445613384246826
Epoch 70, training loss: 8.641419410705566 = 1.8317878246307373 + 1.0 * 6.809631824493408
Epoch 70, val loss: 1.831722378730774
Epoch 80, training loss: 8.453977584838867 = 1.8188576698303223 + 1.0 * 6.635119438171387
Epoch 80, val loss: 1.8187108039855957
Epoch 90, training loss: 8.339521408081055 = 1.80489981174469 + 1.0 * 6.534621238708496
Epoch 90, val loss: 1.8049612045288086
Epoch 100, training loss: 8.260616302490234 = 1.7910486459732056 + 1.0 * 6.46956729888916
Epoch 100, val loss: 1.7917509078979492
Epoch 110, training loss: 8.196147918701172 = 1.7776286602020264 + 1.0 * 6.418519496917725
Epoch 110, val loss: 1.7791188955307007
Epoch 120, training loss: 8.138248443603516 = 1.764155387878418 + 1.0 * 6.374093055725098
Epoch 120, val loss: 1.766717553138733
Epoch 130, training loss: 8.079354286193848 = 1.7501628398895264 + 1.0 * 6.329191207885742
Epoch 130, val loss: 1.7542215585708618
Epoch 140, training loss: 8.029940605163574 = 1.7342053651809692 + 1.0 * 6.295734882354736
Epoch 140, val loss: 1.7403454780578613
Epoch 150, training loss: 7.988322734832764 = 1.7150053977966309 + 1.0 * 6.273317337036133
Epoch 150, val loss: 1.7241549491882324
Epoch 160, training loss: 7.946810722351074 = 1.6916266679763794 + 1.0 * 6.255184173583984
Epoch 160, val loss: 1.7049527168273926
Epoch 170, training loss: 7.902951717376709 = 1.6631793975830078 + 1.0 * 6.239772319793701
Epoch 170, val loss: 1.6819573640823364
Epoch 180, training loss: 7.8551225662231445 = 1.628273606300354 + 1.0 * 6.22684907913208
Epoch 180, val loss: 1.6540040969848633
Epoch 190, training loss: 7.800497055053711 = 1.5852408409118652 + 1.0 * 6.215256214141846
Epoch 190, val loss: 1.6197128295898438
Epoch 200, training loss: 7.7391252517700195 = 1.5331313610076904 + 1.0 * 6.20599365234375
Epoch 200, val loss: 1.578360676765442
Epoch 210, training loss: 7.671333312988281 = 1.4736356735229492 + 1.0 * 6.197697639465332
Epoch 210, val loss: 1.5314158201217651
Epoch 220, training loss: 7.596392631530762 = 1.4072493314743042 + 1.0 * 6.189143180847168
Epoch 220, val loss: 1.4796574115753174
Epoch 230, training loss: 7.5169219970703125 = 1.3357330560684204 + 1.0 * 6.181189060211182
Epoch 230, val loss: 1.4247740507125854
Epoch 240, training loss: 7.437576770782471 = 1.2628473043441772 + 1.0 * 6.174729347229004
Epoch 240, val loss: 1.3697867393493652
Epoch 250, training loss: 7.365094184875488 = 1.1942484378814697 + 1.0 * 6.170845985412598
Epoch 250, val loss: 1.318673014640808
Epoch 260, training loss: 7.289409637451172 = 1.1285830736160278 + 1.0 * 6.160826683044434
Epoch 260, val loss: 1.2700614929199219
Epoch 270, training loss: 7.220399379730225 = 1.0663059949874878 + 1.0 * 6.154093265533447
Epoch 270, val loss: 1.224198579788208
Epoch 280, training loss: 7.163497447967529 = 1.007802963256836 + 1.0 * 6.155694484710693
Epoch 280, val loss: 1.1813243627548218
Epoch 290, training loss: 7.097219467163086 = 0.9546927809715271 + 1.0 * 6.142526626586914
Epoch 290, val loss: 1.1429001092910767
Epoch 300, training loss: 7.042957782745361 = 0.9059004783630371 + 1.0 * 6.137057304382324
Epoch 300, val loss: 1.1080353260040283
Epoch 310, training loss: 6.992696762084961 = 0.8602080345153809 + 1.0 * 6.13248872756958
Epoch 310, val loss: 1.0759905576705933
Epoch 320, training loss: 6.943477630615234 = 0.8174183964729309 + 1.0 * 6.126059055328369
Epoch 320, val loss: 1.0464234352111816
Epoch 330, training loss: 6.905972480773926 = 0.7768704891204834 + 1.0 * 6.129101753234863
Epoch 330, val loss: 1.0187665224075317
Epoch 340, training loss: 6.858924865722656 = 0.739315390586853 + 1.0 * 6.119609355926514
Epoch 340, val loss: 0.9935994744300842
Epoch 350, training loss: 6.8184967041015625 = 0.7042402625083923 + 1.0 * 6.114256381988525
Epoch 350, val loss: 0.9708666801452637
Epoch 360, training loss: 6.780698776245117 = 0.6711735725402832 + 1.0 * 6.109525203704834
Epoch 360, val loss: 0.949981153011322
Epoch 370, training loss: 6.7502336502075195 = 0.640117883682251 + 1.0 * 6.1101155281066895
Epoch 370, val loss: 0.9312865734100342
Epoch 380, training loss: 6.714357852935791 = 0.6110600233078003 + 1.0 * 6.103297710418701
Epoch 380, val loss: 0.9148282408714294
Epoch 390, training loss: 6.683535099029541 = 0.5830823183059692 + 1.0 * 6.100452899932861
Epoch 390, val loss: 0.899800717830658
Epoch 400, training loss: 6.652935028076172 = 0.5558447241783142 + 1.0 * 6.097090244293213
Epoch 400, val loss: 0.8859623074531555
Epoch 410, training loss: 6.624260425567627 = 0.5292619466781616 + 1.0 * 6.094998359680176
Epoch 410, val loss: 0.8734883069992065
Epoch 420, training loss: 6.59852409362793 = 0.5030797123908997 + 1.0 * 6.095444202423096
Epoch 420, val loss: 0.862048327922821
Epoch 430, training loss: 6.571659564971924 = 0.4776209890842438 + 1.0 * 6.094038486480713
Epoch 430, val loss: 0.8516273498535156
Epoch 440, training loss: 6.542201519012451 = 0.4528527855873108 + 1.0 * 6.089348793029785
Epoch 440, val loss: 0.842642605304718
Epoch 450, training loss: 6.515622615814209 = 0.4286504089832306 + 1.0 * 6.086972236633301
Epoch 450, val loss: 0.8344964981079102
Epoch 460, training loss: 6.50951623916626 = 0.4050750434398651 + 1.0 * 6.104441165924072
Epoch 460, val loss: 0.8272190690040588
Epoch 470, training loss: 6.465942859649658 = 0.38259416818618774 + 1.0 * 6.083348751068115
Epoch 470, val loss: 0.821239709854126
Epoch 480, training loss: 6.442792892456055 = 0.3609040081501007 + 1.0 * 6.081888675689697
Epoch 480, val loss: 0.8161594867706299
Epoch 490, training loss: 6.41966438293457 = 0.3398837149143219 + 1.0 * 6.079780578613281
Epoch 490, val loss: 0.8117004632949829
Epoch 500, training loss: 6.397010326385498 = 0.3195587396621704 + 1.0 * 6.077451705932617
Epoch 500, val loss: 0.8081619143486023
Epoch 510, training loss: 6.396395683288574 = 0.29997700452804565 + 1.0 * 6.096418857574463
Epoch 510, val loss: 0.8054153919219971
Epoch 520, training loss: 6.3607072830200195 = 0.28150177001953125 + 1.0 * 6.079205513000488
Epoch 520, val loss: 0.8034881353378296
Epoch 530, training loss: 6.341053009033203 = 0.2640129625797272 + 1.0 * 6.077040195465088
Epoch 530, val loss: 0.8025243878364563
Epoch 540, training loss: 6.319364547729492 = 0.2474859058856964 + 1.0 * 6.071878433227539
Epoch 540, val loss: 0.8021703362464905
Epoch 550, training loss: 6.303421497344971 = 0.23190586268901825 + 1.0 * 6.0715155601501465
Epoch 550, val loss: 0.8026804327964783
Epoch 560, training loss: 6.288040637969971 = 0.2172520011663437 + 1.0 * 6.070788860321045
Epoch 560, val loss: 0.8039419651031494
Epoch 570, training loss: 6.2718329429626465 = 0.20359282195568085 + 1.0 * 6.068240165710449
Epoch 570, val loss: 0.8058164715766907
Epoch 580, training loss: 6.2716569900512695 = 0.19093507528305054 + 1.0 * 6.080721855163574
Epoch 580, val loss: 0.8083534240722656
Epoch 590, training loss: 6.245988368988037 = 0.1793014258146286 + 1.0 * 6.066687107086182
Epoch 590, val loss: 0.8114585876464844
Epoch 600, training loss: 6.232527256011963 = 0.16854576766490936 + 1.0 * 6.063981533050537
Epoch 600, val loss: 0.8152903318405151
Epoch 610, training loss: 6.221841335296631 = 0.15856511890888214 + 1.0 * 6.063276290893555
Epoch 610, val loss: 0.8195701837539673
Epoch 620, training loss: 6.213729381561279 = 0.1493297815322876 + 1.0 * 6.064399719238281
Epoch 620, val loss: 0.8241928815841675
Epoch 630, training loss: 6.203785419464111 = 0.14084163308143616 + 1.0 * 6.062943935394287
Epoch 630, val loss: 0.8292996883392334
Epoch 640, training loss: 6.193090438842773 = 0.13298627734184265 + 1.0 * 6.0601043701171875
Epoch 640, val loss: 0.8348150849342346
Epoch 650, training loss: 6.184679985046387 = 0.12568631768226624 + 1.0 * 6.058993816375732
Epoch 650, val loss: 0.8406433463096619
Epoch 660, training loss: 6.1802592277526855 = 0.11890614032745361 + 1.0 * 6.0613532066345215
Epoch 660, val loss: 0.8466677069664001
Epoch 670, training loss: 6.170162677764893 = 0.11262887716293335 + 1.0 * 6.0575337409973145
Epoch 670, val loss: 0.8529289960861206
Epoch 680, training loss: 6.162515640258789 = 0.10679435729980469 + 1.0 * 6.055721282958984
Epoch 680, val loss: 0.859449028968811
Epoch 690, training loss: 6.155836582183838 = 0.10133668780326843 + 1.0 * 6.054500102996826
Epoch 690, val loss: 0.866150438785553
Epoch 700, training loss: 6.162323951721191 = 0.09623496234416962 + 1.0 * 6.066089153289795
Epoch 700, val loss: 0.8729968667030334
Epoch 710, training loss: 6.144683361053467 = 0.09150656312704086 + 1.0 * 6.0531768798828125
Epoch 710, val loss: 0.8798046112060547
Epoch 720, training loss: 6.138280868530273 = 0.08709431439638138 + 1.0 * 6.051186561584473
Epoch 720, val loss: 0.8869209289550781
Epoch 730, training loss: 6.132374286651611 = 0.08294551819562912 + 1.0 * 6.049428939819336
Epoch 730, val loss: 0.8939768075942993
Epoch 740, training loss: 6.131768226623535 = 0.07904236763715744 + 1.0 * 6.052725791931152
Epoch 740, val loss: 0.9012000560760498
Epoch 750, training loss: 6.129260063171387 = 0.07539177685976028 + 1.0 * 6.053868293762207
Epoch 750, val loss: 0.9083392024040222
Epoch 760, training loss: 6.121187686920166 = 0.0719774067401886 + 1.0 * 6.049210071563721
Epoch 760, val loss: 0.9156156182289124
Epoch 770, training loss: 6.116916656494141 = 0.06876479834318161 + 1.0 * 6.048151969909668
Epoch 770, val loss: 0.922928512096405
Epoch 780, training loss: 6.115072727203369 = 0.0657360702753067 + 1.0 * 6.0493364334106445
Epoch 780, val loss: 0.9302763938903809
Epoch 790, training loss: 6.107968330383301 = 0.06288791447877884 + 1.0 * 6.045080184936523
Epoch 790, val loss: 0.9375572204589844
Epoch 800, training loss: 6.104158878326416 = 0.06020336225628853 + 1.0 * 6.043955326080322
Epoch 800, val loss: 0.9450268149375916
Epoch 810, training loss: 6.106356143951416 = 0.05766861513257027 + 1.0 * 6.04868745803833
Epoch 810, val loss: 0.9523347020149231
Epoch 820, training loss: 6.100765228271484 = 0.05528441071510315 + 1.0 * 6.045480728149414
Epoch 820, val loss: 0.9596499800682068
Epoch 830, training loss: 6.096192836761475 = 0.053040191531181335 + 1.0 * 6.043152809143066
Epoch 830, val loss: 0.9670071005821228
Epoch 840, training loss: 6.09283971786499 = 0.05091957747936249 + 1.0 * 6.041920185089111
Epoch 840, val loss: 0.9743040800094604
Epoch 850, training loss: 6.089169979095459 = 0.04892021045088768 + 1.0 * 6.040249824523926
Epoch 850, val loss: 0.9816077351570129
Epoch 860, training loss: 6.087863445281982 = 0.047024887055158615 + 1.0 * 6.040838718414307
Epoch 860, val loss: 0.9889293313026428
Epoch 870, training loss: 6.087123870849609 = 0.045226819813251495 + 1.0 * 6.041896820068359
Epoch 870, val loss: 0.9960408806800842
Epoch 880, training loss: 6.081951141357422 = 0.043529558926820755 + 1.0 * 6.038421630859375
Epoch 880, val loss: 1.0032316446304321
Epoch 890, training loss: 6.081913948059082 = 0.041917916387319565 + 1.0 * 6.039996147155762
Epoch 890, val loss: 1.010406494140625
Epoch 900, training loss: 6.077337265014648 = 0.04038771241903305 + 1.0 * 6.036949634552002
Epoch 900, val loss: 1.0175193548202515
Epoch 910, training loss: 6.075181007385254 = 0.03893494978547096 + 1.0 * 6.036245822906494
Epoch 910, val loss: 1.024657130241394
Epoch 920, training loss: 6.078047275543213 = 0.037551965564489365 + 1.0 * 6.0404953956604
Epoch 920, val loss: 1.0315520763397217
Epoch 930, training loss: 6.07065486907959 = 0.03624505177140236 + 1.0 * 6.034409999847412
Epoch 930, val loss: 1.0384081602096558
Epoch 940, training loss: 6.069970607757568 = 0.03500480577349663 + 1.0 * 6.034965991973877
Epoch 940, val loss: 1.0453181266784668
Epoch 950, training loss: 6.0744123458862305 = 0.03382079675793648 + 1.0 * 6.040591716766357
Epoch 950, val loss: 1.0519686937332153
Epoch 960, training loss: 6.067850589752197 = 0.032695792615413666 + 1.0 * 6.035154819488525
Epoch 960, val loss: 1.0586116313934326
Epoch 970, training loss: 6.06491756439209 = 0.03162835165858269 + 1.0 * 6.033289432525635
Epoch 970, val loss: 1.0652978420257568
Epoch 980, training loss: 6.06462287902832 = 0.030606217682361603 + 1.0 * 6.0340166091918945
Epoch 980, val loss: 1.0717830657958984
Epoch 990, training loss: 6.061613082885742 = 0.02963232435286045 + 1.0 * 6.031980991363525
Epoch 990, val loss: 1.0782444477081299
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.8708
Flip ASR: 0.8444/225 nodes
The final ASR:0.77368, 0.07443, Accuracy:0.80864, 0.01364
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11568])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10502])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83210, 0.00873
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.335160255432129 = 1.9612960815429688 + 1.0 * 8.37386417388916
Epoch 0, val loss: 1.9656871557235718
Epoch 10, training loss: 10.324588775634766 = 1.95111083984375 + 1.0 * 8.373477935791016
Epoch 10, val loss: 1.9556498527526855
Epoch 20, training loss: 10.309633255004883 = 1.9385230541229248 + 1.0 * 8.371109962463379
Epoch 20, val loss: 1.9428740739822388
Epoch 30, training loss: 10.274919509887695 = 1.9212496280670166 + 1.0 * 8.353670120239258
Epoch 30, val loss: 1.925038456916809
Epoch 40, training loss: 10.130208969116211 = 1.898568034172058 + 1.0 * 8.231640815734863
Epoch 40, val loss: 1.902262568473816
Epoch 50, training loss: 9.542808532714844 = 1.8731989860534668 + 1.0 * 7.669609069824219
Epoch 50, val loss: 1.877533197402954
Epoch 60, training loss: 9.056807518005371 = 1.8520355224609375 + 1.0 * 7.204771995544434
Epoch 60, val loss: 1.8588396310806274
Epoch 70, training loss: 8.675163269042969 = 1.8388416767120361 + 1.0 * 6.8363213539123535
Epoch 70, val loss: 1.846742868423462
Epoch 80, training loss: 8.463592529296875 = 1.825998067855835 + 1.0 * 6.637594699859619
Epoch 80, val loss: 1.8346551656723022
Epoch 90, training loss: 8.326858520507812 = 1.8139002323150635 + 1.0 * 6.512958526611328
Epoch 90, val loss: 1.8229950666427612
Epoch 100, training loss: 8.239419937133789 = 1.8016815185546875 + 1.0 * 6.43773889541626
Epoch 100, val loss: 1.8117740154266357
Epoch 110, training loss: 8.180008888244629 = 1.789935827255249 + 1.0 * 6.390073299407959
Epoch 110, val loss: 1.8011384010314941
Epoch 120, training loss: 8.12811279296875 = 1.7786860466003418 + 1.0 * 6.34942626953125
Epoch 120, val loss: 1.7908918857574463
Epoch 130, training loss: 8.081169128417969 = 1.7674802541732788 + 1.0 * 6.313689231872559
Epoch 130, val loss: 1.7807815074920654
Epoch 140, training loss: 8.041672706604004 = 1.755280613899231 + 1.0 * 6.286391735076904
Epoch 140, val loss: 1.770097017288208
Epoch 150, training loss: 8.002400398254395 = 1.7412205934524536 + 1.0 * 6.261179447174072
Epoch 150, val loss: 1.7582546472549438
Epoch 160, training loss: 7.965026378631592 = 1.7246454954147339 + 1.0 * 6.240380764007568
Epoch 160, val loss: 1.7446426153182983
Epoch 170, training loss: 7.93007755279541 = 1.704835057258606 + 1.0 * 6.225242614746094
Epoch 170, val loss: 1.7285797595977783
Epoch 180, training loss: 7.891000747680664 = 1.6811856031417847 + 1.0 * 6.20981502532959
Epoch 180, val loss: 1.7095777988433838
Epoch 190, training loss: 7.849301815032959 = 1.6528335809707642 + 1.0 * 6.196468353271484
Epoch 190, val loss: 1.6868213415145874
Epoch 200, training loss: 7.8043365478515625 = 1.618678331375122 + 1.0 * 6.185657978057861
Epoch 200, val loss: 1.659292459487915
Epoch 210, training loss: 7.754454612731934 = 1.5777032375335693 + 1.0 * 6.176751613616943
Epoch 210, val loss: 1.626250147819519
Epoch 220, training loss: 7.700002193450928 = 1.5299967527389526 + 1.0 * 6.1700053215026855
Epoch 220, val loss: 1.587950348854065
Epoch 230, training loss: 7.638885498046875 = 1.476422905921936 + 1.0 * 6.1624627113342285
Epoch 230, val loss: 1.5447697639465332
Epoch 240, training loss: 7.572880268096924 = 1.4171556234359741 + 1.0 * 6.15572452545166
Epoch 240, val loss: 1.497024416923523
Epoch 250, training loss: 7.505429267883301 = 1.3540107011795044 + 1.0 * 6.151418685913086
Epoch 250, val loss: 1.4466153383255005
Epoch 260, training loss: 7.436213493347168 = 1.289555311203003 + 1.0 * 6.146657943725586
Epoch 260, val loss: 1.3953168392181396
Epoch 270, training loss: 7.365411758422852 = 1.2250347137451172 + 1.0 * 6.140377044677734
Epoch 270, val loss: 1.3438888788223267
Epoch 280, training loss: 7.298384666442871 = 1.1615923643112183 + 1.0 * 6.136792182922363
Epoch 280, val loss: 1.2935937643051147
Epoch 290, training loss: 7.237079620361328 = 1.10099458694458 + 1.0 * 6.136085033416748
Epoch 290, val loss: 1.2456722259521484
Epoch 300, training loss: 7.174117565155029 = 1.0440950393676758 + 1.0 * 6.1300225257873535
Epoch 300, val loss: 1.2007591724395752
Epoch 310, training loss: 7.1147284507751465 = 0.9898210167884827 + 1.0 * 6.124907493591309
Epoch 310, val loss: 1.1578677892684937
Epoch 320, training loss: 7.070036888122559 = 0.9381605386734009 + 1.0 * 6.131876468658447
Epoch 320, val loss: 1.1170448064804077
Epoch 330, training loss: 7.010326862335205 = 0.8898797631263733 + 1.0 * 6.120447158813477
Epoch 330, val loss: 1.079046607017517
Epoch 340, training loss: 6.960505485534668 = 0.8443213105201721 + 1.0 * 6.116184234619141
Epoch 340, val loss: 1.0433719158172607
Epoch 350, training loss: 6.915321350097656 = 0.8010473251342773 + 1.0 * 6.114274024963379
Epoch 350, val loss: 1.0095781087875366
Epoch 360, training loss: 6.87171745300293 = 0.7606326937675476 + 1.0 * 6.111084938049316
Epoch 360, val loss: 0.978438675403595
Epoch 370, training loss: 6.830940246582031 = 0.72307288646698 + 1.0 * 6.107867240905762
Epoch 370, val loss: 0.9503235816955566
Epoch 380, training loss: 6.796075820922852 = 0.6876552700996399 + 1.0 * 6.108420372009277
Epoch 380, val loss: 0.924540638923645
Epoch 390, training loss: 6.760449409484863 = 0.6543822884559631 + 1.0 * 6.106067180633545
Epoch 390, val loss: 0.9013352394104004
Epoch 400, training loss: 6.727807521820068 = 0.6231823563575745 + 1.0 * 6.104625225067139
Epoch 400, val loss: 0.8805191516876221
Epoch 410, training loss: 6.692845344543457 = 0.5935728549957275 + 1.0 * 6.09927225112915
Epoch 410, val loss: 0.8619611263275146
Epoch 420, training loss: 6.661282539367676 = 0.565194845199585 + 1.0 * 6.09608793258667
Epoch 420, val loss: 0.8451477289199829
Epoch 430, training loss: 6.639482021331787 = 0.5379301905632019 + 1.0 * 6.1015520095825195
Epoch 430, val loss: 0.8300086259841919
Epoch 440, training loss: 6.60713529586792 = 0.5119947195053101 + 1.0 * 6.09514045715332
Epoch 440, val loss: 0.8166596293449402
Epoch 450, training loss: 6.578726291656494 = 0.48724108934402466 + 1.0 * 6.091485023498535
Epoch 450, val loss: 0.8049777150154114
Epoch 460, training loss: 6.5534820556640625 = 0.46341246366500854 + 1.0 * 6.090069770812988
Epoch 460, val loss: 0.7946149110794067
Epoch 470, training loss: 6.534631729125977 = 0.4405687153339386 + 1.0 * 6.094062805175781
Epoch 470, val loss: 0.7854982614517212
Epoch 480, training loss: 6.506466388702393 = 0.4189598858356476 + 1.0 * 6.087506294250488
Epoch 480, val loss: 0.7779361605644226
Epoch 490, training loss: 6.48390531539917 = 0.39833495020866394 + 1.0 * 6.085570335388184
Epoch 490, val loss: 0.7717381119728088
Epoch 500, training loss: 6.464031219482422 = 0.37857043743133545 + 1.0 * 6.085460662841797
Epoch 500, val loss: 0.766689121723175
Epoch 510, training loss: 6.443404197692871 = 0.35977688431739807 + 1.0 * 6.083627223968506
Epoch 510, val loss: 0.762824296951294
Epoch 520, training loss: 6.426158428192139 = 0.34189853072166443 + 1.0 * 6.084259986877441
Epoch 520, val loss: 0.7601547241210938
Epoch 530, training loss: 6.404901027679443 = 0.3248734772205353 + 1.0 * 6.0800275802612305
Epoch 530, val loss: 0.7584642171859741
Epoch 540, training loss: 6.387507438659668 = 0.3086542785167694 + 1.0 * 6.078853130340576
Epoch 540, val loss: 0.7578424215316772
Epoch 550, training loss: 6.379160404205322 = 0.29314595460891724 + 1.0 * 6.086014270782471
Epoch 550, val loss: 0.7580726742744446
Epoch 560, training loss: 6.357698440551758 = 0.2785654664039612 + 1.0 * 6.079133033752441
Epoch 560, val loss: 0.759051501750946
Epoch 570, training loss: 6.340081691741943 = 0.2647792398929596 + 1.0 * 6.075302600860596
Epoch 570, val loss: 0.7610400319099426
Epoch 580, training loss: 6.325250625610352 = 0.251627653837204 + 1.0 * 6.073623180389404
Epoch 580, val loss: 0.7636446952819824
Epoch 590, training loss: 6.312115669250488 = 0.2390802949666977 + 1.0 * 6.07303524017334
Epoch 590, val loss: 0.7670118808746338
Epoch 600, training loss: 6.304264068603516 = 0.2272072434425354 + 1.0 * 6.077056884765625
Epoch 600, val loss: 0.7709667086601257
Epoch 610, training loss: 6.288137912750244 = 0.21602213382720947 + 1.0 * 6.072115898132324
Epoch 610, val loss: 0.7756134271621704
Epoch 620, training loss: 6.274866104125977 = 0.20537474751472473 + 1.0 * 6.069491386413574
Epoch 620, val loss: 0.7807492017745972
Epoch 630, training loss: 6.273624420166016 = 0.1952400952577591 + 1.0 * 6.0783843994140625
Epoch 630, val loss: 0.7863187789916992
Epoch 640, training loss: 6.253539085388184 = 0.18569537997245789 + 1.0 * 6.067843914031982
Epoch 640, val loss: 0.7921987771987915
Epoch 650, training loss: 6.243927478790283 = 0.17665940523147583 + 1.0 * 6.067267894744873
Epoch 650, val loss: 0.7985661625862122
Epoch 660, training loss: 6.237242698669434 = 0.16808933019638062 + 1.0 * 6.069153308868408
Epoch 660, val loss: 0.8050898313522339
Epoch 670, training loss: 6.223879337310791 = 0.15997852385044098 + 1.0 * 6.063900947570801
Epoch 670, val loss: 0.8119828701019287
Epoch 680, training loss: 6.214797496795654 = 0.15227408707141876 + 1.0 * 6.062523365020752
Epoch 680, val loss: 0.8192592263221741
Epoch 690, training loss: 6.214399337768555 = 0.144952192902565 + 1.0 * 6.069447040557861
Epoch 690, val loss: 0.8266738057136536
Epoch 700, training loss: 6.197745323181152 = 0.13804306089878082 + 1.0 * 6.059702396392822
Epoch 700, val loss: 0.8343066573143005
Epoch 710, training loss: 6.192234516143799 = 0.1314929723739624 + 1.0 * 6.060741424560547
Epoch 710, val loss: 0.8421862721443176
Epoch 720, training loss: 6.183605194091797 = 0.12529945373535156 + 1.0 * 6.058305740356445
Epoch 720, val loss: 0.8500560522079468
Epoch 730, training loss: 6.178288459777832 = 0.11945317685604095 + 1.0 * 6.058835506439209
Epoch 730, val loss: 0.8581970930099487
Epoch 740, training loss: 6.174341678619385 = 0.11391697078943253 + 1.0 * 6.0604248046875
Epoch 740, val loss: 0.8663487434387207
Epoch 750, training loss: 6.163706302642822 = 0.10868968814611435 + 1.0 * 6.05501651763916
Epoch 750, val loss: 0.8746078610420227
Epoch 760, training loss: 6.158189296722412 = 0.10372601449489594 + 1.0 * 6.0544633865356445
Epoch 760, val loss: 0.8830230236053467
Epoch 770, training loss: 6.156174182891846 = 0.0990220233798027 + 1.0 * 6.057152271270752
Epoch 770, val loss: 0.8914825916290283
Epoch 780, training loss: 6.15391206741333 = 0.09458291530609131 + 1.0 * 6.059329032897949
Epoch 780, val loss: 0.8997459411621094
Epoch 790, training loss: 6.142350196838379 = 0.09040552377700806 + 1.0 * 6.051944732666016
Epoch 790, val loss: 0.9081692695617676
Epoch 800, training loss: 6.144325256347656 = 0.0864473283290863 + 1.0 * 6.057878017425537
Epoch 800, val loss: 0.916566789150238
Epoch 810, training loss: 6.134267807006836 = 0.08270452171564102 + 1.0 * 6.051563262939453
Epoch 810, val loss: 0.9248981475830078
Epoch 820, training loss: 6.128255367279053 = 0.07915782928466797 + 1.0 * 6.049097537994385
Epoch 820, val loss: 0.9334052801132202
Epoch 830, training loss: 6.127462387084961 = 0.0757836252450943 + 1.0 * 6.051678657531738
Epoch 830, val loss: 0.9418796300888062
Epoch 840, training loss: 6.122788429260254 = 0.07259944826364517 + 1.0 * 6.050189018249512
Epoch 840, val loss: 0.9502074122428894
Epoch 850, training loss: 6.117787837982178 = 0.06958827376365662 + 1.0 * 6.048199653625488
Epoch 850, val loss: 0.9586746692657471
Epoch 860, training loss: 6.111969470977783 = 0.06673231720924377 + 1.0 * 6.045237064361572
Epoch 860, val loss: 0.9671714305877686
Epoch 870, training loss: 6.110825061798096 = 0.06401601433753967 + 1.0 * 6.046809196472168
Epoch 870, val loss: 0.9756518602371216
Epoch 880, training loss: 6.106019496917725 = 0.06143874675035477 + 1.0 * 6.044580936431885
Epoch 880, val loss: 0.9840927720069885
Epoch 890, training loss: 6.1068196296691895 = 0.05899758264422417 + 1.0 * 6.047821998596191
Epoch 890, val loss: 0.9924680590629578
Epoch 900, training loss: 6.101438999176025 = 0.056686799973249435 + 1.0 * 6.04475212097168
Epoch 900, val loss: 1.0008211135864258
Epoch 910, training loss: 6.096618175506592 = 0.05449441820383072 + 1.0 * 6.042123794555664
Epoch 910, val loss: 1.0092018842697144
Epoch 920, training loss: 6.092738628387451 = 0.052404992282390594 + 1.0 * 6.0403337478637695
Epoch 920, val loss: 1.01760995388031
Epoch 930, training loss: 6.0910749435424805 = 0.05041028559207916 + 1.0 * 6.0406646728515625
Epoch 930, val loss: 1.0260461568832397
Epoch 940, training loss: 6.091282844543457 = 0.04851146414875984 + 1.0 * 6.042771339416504
Epoch 940, val loss: 1.034295678138733
Epoch 950, training loss: 6.0867533683776855 = 0.04671727120876312 + 1.0 * 6.040036201477051
Epoch 950, val loss: 1.0425469875335693
Epoch 960, training loss: 6.086613655090332 = 0.045010756701231 + 1.0 * 6.041603088378906
Epoch 960, val loss: 1.050786018371582
Epoch 970, training loss: 6.081218242645264 = 0.0433841235935688 + 1.0 * 6.037834167480469
Epoch 970, val loss: 1.058964729309082
Epoch 980, training loss: 6.080402851104736 = 0.04183713719248772 + 1.0 * 6.038565635681152
Epoch 980, val loss: 1.067033290863037
Epoch 990, training loss: 6.075895309448242 = 0.04036775976419449 + 1.0 * 6.03552770614624
Epoch 990, val loss: 1.0750367641448975
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.4576
Flip ASR: 0.3556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.313250541687012 = 1.9394510984420776 + 1.0 * 8.373799324035645
Epoch 0, val loss: 1.9466471672058105
Epoch 10, training loss: 10.302026748657227 = 1.929632306098938 + 1.0 * 8.372394561767578
Epoch 10, val loss: 1.9359768629074097
Epoch 20, training loss: 10.282840728759766 = 1.9178522825241089 + 1.0 * 8.364988327026367
Epoch 20, val loss: 1.922668695449829
Epoch 30, training loss: 10.226262092590332 = 1.9023609161376953 + 1.0 * 8.323901176452637
Epoch 30, val loss: 1.9048346281051636
Epoch 40, training loss: 9.869553565979004 = 1.884351134300232 + 1.0 * 7.985202312469482
Epoch 40, val loss: 1.8840135335922241
Epoch 50, training loss: 9.223774909973145 = 1.8648079633712769 + 1.0 * 7.358967304229736
Epoch 50, val loss: 1.861889123916626
Epoch 60, training loss: 8.957738876342773 = 1.847926139831543 + 1.0 * 7.109812259674072
Epoch 60, val loss: 1.844741702079773
Epoch 70, training loss: 8.70191478729248 = 1.835587978363037 + 1.0 * 6.866326808929443
Epoch 70, val loss: 1.8322347402572632
Epoch 80, training loss: 8.49095630645752 = 1.8261384963989258 + 1.0 * 6.664817810058594
Epoch 80, val loss: 1.8216030597686768
Epoch 90, training loss: 8.364882469177246 = 1.8157320022583008 + 1.0 * 6.549150466918945
Epoch 90, val loss: 1.8097577095031738
Epoch 100, training loss: 8.265974044799805 = 1.8047800064086914 + 1.0 * 6.461194038391113
Epoch 100, val loss: 1.7979638576507568
Epoch 110, training loss: 8.186822891235352 = 1.7942687273025513 + 1.0 * 6.39255428314209
Epoch 110, val loss: 1.7871580123901367
Epoch 120, training loss: 8.124061584472656 = 1.7841061353683472 + 1.0 * 6.339955806732178
Epoch 120, val loss: 1.7768810987472534
Epoch 130, training loss: 8.07604694366455 = 1.7733983993530273 + 1.0 * 6.302648544311523
Epoch 130, val loss: 1.7662380933761597
Epoch 140, training loss: 8.03764820098877 = 1.7615282535552979 + 1.0 * 6.276120185852051
Epoch 140, val loss: 1.7547494173049927
Epoch 150, training loss: 8.003077507019043 = 1.7481017112731934 + 1.0 * 6.25497579574585
Epoch 150, val loss: 1.742496371269226
Epoch 160, training loss: 7.969201564788818 = 1.7325501441955566 + 1.0 * 6.236651420593262
Epoch 160, val loss: 1.7289003133773804
Epoch 170, training loss: 7.935581207275391 = 1.7140660285949707 + 1.0 * 6.22151517868042
Epoch 170, val loss: 1.7131907939910889
Epoch 180, training loss: 7.900404930114746 = 1.6918455362319946 + 1.0 * 6.208559513092041
Epoch 180, val loss: 1.694874882698059
Epoch 190, training loss: 7.8600239753723145 = 1.6644772291183472 + 1.0 * 6.195546627044678
Epoch 190, val loss: 1.6724731922149658
Epoch 200, training loss: 7.814785480499268 = 1.6304131746292114 + 1.0 * 6.184372425079346
Epoch 200, val loss: 1.644555926322937
Epoch 210, training loss: 7.768405914306641 = 1.5885353088378906 + 1.0 * 6.17987060546875
Epoch 210, val loss: 1.6105095148086548
Epoch 220, training loss: 7.70870304107666 = 1.5400961637496948 + 1.0 * 6.168606758117676
Epoch 220, val loss: 1.5710997581481934
Epoch 230, training loss: 7.648324489593506 = 1.4857081174850464 + 1.0 * 6.16261625289917
Epoch 230, val loss: 1.5273765325546265
Epoch 240, training loss: 7.598236083984375 = 1.4282538890838623 + 1.0 * 6.169981956481934
Epoch 240, val loss: 1.4818037748336792
Epoch 250, training loss: 7.5300164222717285 = 1.3737268447875977 + 1.0 * 6.156289577484131
Epoch 250, val loss: 1.4384976625442505
Epoch 260, training loss: 7.470474720001221 = 1.323508381843567 + 1.0 * 6.146966457366943
Epoch 260, val loss: 1.3988145589828491
Epoch 270, training loss: 7.420128345489502 = 1.278383731842041 + 1.0 * 6.141744613647461
Epoch 270, val loss: 1.3637362718582153
Epoch 280, training loss: 7.3878912925720215 = 1.2382850646972656 + 1.0 * 6.149606227874756
Epoch 280, val loss: 1.3333148956298828
Epoch 290, training loss: 7.33507776260376 = 1.2032076120376587 + 1.0 * 6.131870269775391
Epoch 290, val loss: 1.3073879480361938
Epoch 300, training loss: 7.2985053062438965 = 1.1710244417190552 + 1.0 * 6.127480983734131
Epoch 300, val loss: 1.2841105461120605
Epoch 310, training loss: 7.263429641723633 = 1.140048623085022 + 1.0 * 6.1233811378479
Epoch 310, val loss: 1.2621281147003174
Epoch 320, training loss: 7.233457088470459 = 1.1090186834335327 + 1.0 * 6.124438285827637
Epoch 320, val loss: 1.2402455806732178
Epoch 330, training loss: 7.197370529174805 = 1.0773134231567383 + 1.0 * 6.120057106018066
Epoch 330, val loss: 1.2180237770080566
Epoch 340, training loss: 7.157731533050537 = 1.0443620681762695 + 1.0 * 6.113369464874268
Epoch 340, val loss: 1.1945312023162842
Epoch 350, training loss: 7.120877742767334 = 1.0092039108276367 + 1.0 * 6.111673831939697
Epoch 350, val loss: 1.169152855873108
Epoch 360, training loss: 7.079493999481201 = 0.9711272120475769 + 1.0 * 6.108366966247559
Epoch 360, val loss: 1.1412582397460938
Epoch 370, training loss: 7.035411834716797 = 0.929892361164093 + 1.0 * 6.1055192947387695
Epoch 370, val loss: 1.1103134155273438
Epoch 380, training loss: 7.004606246948242 = 0.8864037394523621 + 1.0 * 6.1182026863098145
Epoch 380, val loss: 1.077673316001892
Epoch 390, training loss: 6.945428848266602 = 0.8436456918716431 + 1.0 * 6.101783275604248
Epoch 390, val loss: 1.0454833507537842
Epoch 400, training loss: 6.902512550354004 = 0.8018834590911865 + 1.0 * 6.1006293296813965
Epoch 400, val loss: 1.0135115385055542
Epoch 410, training loss: 6.858340740203857 = 0.761868953704834 + 1.0 * 6.096471786499023
Epoch 410, val loss: 0.9829350113868713
Epoch 420, training loss: 6.818558692932129 = 0.7242130041122437 + 1.0 * 6.094345569610596
Epoch 420, val loss: 0.9543748497962952
Epoch 430, training loss: 6.784912109375 = 0.6897194385528564 + 1.0 * 6.0951924324035645
Epoch 430, val loss: 0.9287040829658508
Epoch 440, training loss: 6.751626014709473 = 0.6588186621665955 + 1.0 * 6.092807292938232
Epoch 440, val loss: 0.9061365127563477
Epoch 450, training loss: 6.7187089920043945 = 0.6307157278060913 + 1.0 * 6.087993144989014
Epoch 450, val loss: 0.8863582611083984
Epoch 460, training loss: 6.689858436584473 = 0.6047725081443787 + 1.0 * 6.085085868835449
Epoch 460, val loss: 0.8689915537834167
Epoch 470, training loss: 6.664968967437744 = 0.5805641412734985 + 1.0 * 6.084404945373535
Epoch 470, val loss: 0.8536311984062195
Epoch 480, training loss: 6.639461994171143 = 0.5579070448875427 + 1.0 * 6.081554889678955
Epoch 480, val loss: 0.8399576544761658
Epoch 490, training loss: 6.617152690887451 = 0.5361274480819702 + 1.0 * 6.081025123596191
Epoch 490, val loss: 0.8275692462921143
Epoch 500, training loss: 6.595263481140137 = 0.514934241771698 + 1.0 * 6.080329418182373
Epoch 500, val loss: 0.816242516040802
Epoch 510, training loss: 6.572510242462158 = 0.49422356486320496 + 1.0 * 6.078286647796631
Epoch 510, val loss: 0.8056515455245972
Epoch 520, training loss: 6.549223899841309 = 0.4737398624420166 + 1.0 * 6.075483798980713
Epoch 520, val loss: 0.7957113981246948
Epoch 530, training loss: 6.527937412261963 = 0.4534328877925873 + 1.0 * 6.074504375457764
Epoch 530, val loss: 0.7862745523452759
Epoch 540, training loss: 6.506394386291504 = 0.4333032965660095 + 1.0 * 6.07309103012085
Epoch 540, val loss: 0.7773141860961914
Epoch 550, training loss: 6.484411716461182 = 0.41348034143447876 + 1.0 * 6.070931434631348
Epoch 550, val loss: 0.7689239382743835
Epoch 560, training loss: 6.463803291320801 = 0.3938758075237274 + 1.0 * 6.06992769241333
Epoch 560, val loss: 0.7610799074172974
Epoch 570, training loss: 6.462865829467773 = 0.374526709318161 + 1.0 * 6.088339328765869
Epoch 570, val loss: 0.7537198662757874
Epoch 580, training loss: 6.424004554748535 = 0.35559484362602234 + 1.0 * 6.0684099197387695
Epoch 580, val loss: 0.7467791438102722
Epoch 590, training loss: 6.404906749725342 = 0.33713236451148987 + 1.0 * 6.067774295806885
Epoch 590, val loss: 0.7403081059455872
Epoch 600, training loss: 6.384997367858887 = 0.3190186321735382 + 1.0 * 6.065978527069092
Epoch 600, val loss: 0.7343485951423645
Epoch 610, training loss: 6.365322589874268 = 0.30125731229782104 + 1.0 * 6.064065456390381
Epoch 610, val loss: 0.7287310361862183
Epoch 620, training loss: 6.346616744995117 = 0.2838461101055145 + 1.0 * 6.062770843505859
Epoch 620, val loss: 0.7234238386154175
Epoch 630, training loss: 6.351208686828613 = 0.26690083742141724 + 1.0 * 6.084307670593262
Epoch 630, val loss: 0.718383252620697
Epoch 640, training loss: 6.316995143890381 = 0.25059619545936584 + 1.0 * 6.066399097442627
Epoch 640, val loss: 0.7136108875274658
Epoch 650, training loss: 6.297216415405273 = 0.23495268821716309 + 1.0 * 6.062263488769531
Epoch 650, val loss: 0.7091318368911743
Epoch 660, training loss: 6.279470920562744 = 0.2199375480413437 + 1.0 * 6.059533596038818
Epoch 660, val loss: 0.7050741314888
Epoch 670, training loss: 6.265556335449219 = 0.2055819183588028 + 1.0 * 6.059974193572998
Epoch 670, val loss: 0.7013198733329773
Epoch 680, training loss: 6.268334865570068 = 0.19218207895755768 + 1.0 * 6.076152801513672
Epoch 680, val loss: 0.6978148818016052
Epoch 690, training loss: 6.240379333496094 = 0.17972621321678162 + 1.0 * 6.060653209686279
Epoch 690, val loss: 0.6947008967399597
Epoch 700, training loss: 6.224425315856934 = 0.16815106570720673 + 1.0 * 6.0562744140625
Epoch 700, val loss: 0.692048192024231
Epoch 710, training loss: 6.211877346038818 = 0.15736086666584015 + 1.0 * 6.054516315460205
Epoch 710, val loss: 0.6899362206459045
Epoch 720, training loss: 6.2001118659973145 = 0.14730961620807648 + 1.0 * 6.052802085876465
Epoch 720, val loss: 0.688214898109436
Epoch 730, training loss: 6.189670562744141 = 0.1379607915878296 + 1.0 * 6.0517096519470215
Epoch 730, val loss: 0.6869495511054993
Epoch 740, training loss: 6.194386005401611 = 0.1292928010225296 + 1.0 * 6.065093040466309
Epoch 740, val loss: 0.6861540675163269
Epoch 750, training loss: 6.171474456787109 = 0.12133333832025528 + 1.0 * 6.050141334533691
Epoch 750, val loss: 0.6856797933578491
Epoch 760, training loss: 6.177430629730225 = 0.1139943078160286 + 1.0 * 6.063436508178711
Epoch 760, val loss: 0.6855728030204773
Epoch 770, training loss: 6.1632866859436035 = 0.10730095952749252 + 1.0 * 6.055985927581787
Epoch 770, val loss: 0.6858920454978943
Epoch 780, training loss: 6.149648189544678 = 0.10112220048904419 + 1.0 * 6.048525810241699
Epoch 780, val loss: 0.686392605304718
Epoch 790, training loss: 6.142773151397705 = 0.09539421647787094 + 1.0 * 6.047379016876221
Epoch 790, val loss: 0.6872807145118713
Epoch 800, training loss: 6.135380744934082 = 0.09006603807210922 + 1.0 * 6.045314788818359
Epoch 800, val loss: 0.6884781718254089
Epoch 810, training loss: 6.129423141479492 = 0.08509121835231781 + 1.0 * 6.044332027435303
Epoch 810, val loss: 0.6899276375770569
Epoch 820, training loss: 6.125491619110107 = 0.08044993132352829 + 1.0 * 6.045041561126709
Epoch 820, val loss: 0.6916029453277588
Epoch 830, training loss: 6.121989727020264 = 0.07614687085151672 + 1.0 * 6.04584264755249
Epoch 830, val loss: 0.6934155225753784
Epoch 840, training loss: 6.120242118835449 = 0.07217223197221756 + 1.0 * 6.048069953918457
Epoch 840, val loss: 0.6953155994415283
Epoch 850, training loss: 6.112265586853027 = 0.06848690658807755 + 1.0 * 6.043778896331787
Epoch 850, val loss: 0.6973676681518555
Epoch 860, training loss: 6.105290412902832 = 0.06504710763692856 + 1.0 * 6.040243148803711
Epoch 860, val loss: 0.6996015310287476
Epoch 870, training loss: 6.102889060974121 = 0.0618305504322052 + 1.0 * 6.041058540344238
Epoch 870, val loss: 0.7019839286804199
Epoch 880, training loss: 6.09980583190918 = 0.05883020535111427 + 1.0 * 6.040975570678711
Epoch 880, val loss: 0.7044572830200195
Epoch 890, training loss: 6.096132755279541 = 0.056044355034828186 + 1.0 * 6.040088176727295
Epoch 890, val loss: 0.7069863080978394
Epoch 900, training loss: 6.092473983764648 = 0.05344154313206673 + 1.0 * 6.039032459259033
Epoch 900, val loss: 0.7095449566841125
Epoch 910, training loss: 6.0879998207092285 = 0.050998494029045105 + 1.0 * 6.037001132965088
Epoch 910, val loss: 0.712286651134491
Epoch 920, training loss: 6.098650932312012 = 0.048701994121074677 + 1.0 * 6.0499491691589355
Epoch 920, val loss: 0.715063750743866
Epoch 930, training loss: 6.083620548248291 = 0.046564050018787384 + 1.0 * 6.0370564460754395
Epoch 930, val loss: 0.717910647392273
Epoch 940, training loss: 6.078979969024658 = 0.044560227543115616 + 1.0 * 6.034419536590576
Epoch 940, val loss: 0.7207008600234985
Epoch 950, training loss: 6.076627254486084 = 0.04267087206244469 + 1.0 * 6.033956527709961
Epoch 950, val loss: 0.7235974073410034
Epoch 960, training loss: 6.084522247314453 = 0.04089102894067764 + 1.0 * 6.043631076812744
Epoch 960, val loss: 0.7265670895576477
Epoch 970, training loss: 6.078539848327637 = 0.03922504931688309 + 1.0 * 6.0393147468566895
Epoch 970, val loss: 0.7295419573783875
Epoch 980, training loss: 6.072119235992432 = 0.037667326629161835 + 1.0 * 6.034451961517334
Epoch 980, val loss: 0.732434093952179
Epoch 990, training loss: 6.072140216827393 = 0.03620326519012451 + 1.0 * 6.0359368324279785
Epoch 990, val loss: 0.7353812456130981
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.1808
Flip ASR: 0.2000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.338846206665039 = 1.9649537801742554 + 1.0 * 8.373892784118652
Epoch 0, val loss: 1.9547476768493652
Epoch 10, training loss: 10.326594352722168 = 1.9530503749847412 + 1.0 * 8.373543739318848
Epoch 10, val loss: 1.9434934854507446
Epoch 20, training loss: 10.31004810333252 = 1.9387725591659546 + 1.0 * 8.371275901794434
Epoch 20, val loss: 1.9295543432235718
Epoch 30, training loss: 10.270878791809082 = 1.919628381729126 + 1.0 * 8.351250648498535
Epoch 30, val loss: 1.9105685949325562
Epoch 40, training loss: 10.033447265625 = 1.895987868309021 + 1.0 * 8.137459754943848
Epoch 40, val loss: 1.8879810571670532
Epoch 50, training loss: 9.164640426635742 = 1.874558687210083 + 1.0 * 7.29008150100708
Epoch 50, val loss: 1.8688485622406006
Epoch 60, training loss: 8.722650527954102 = 1.8606867790222168 + 1.0 * 6.861964225769043
Epoch 60, val loss: 1.856231927871704
Epoch 70, training loss: 8.461684226989746 = 1.8482364416122437 + 1.0 * 6.613447666168213
Epoch 70, val loss: 1.844581961631775
Epoch 80, training loss: 8.318660736083984 = 1.8364676237106323 + 1.0 * 6.4821929931640625
Epoch 80, val loss: 1.8336384296417236
Epoch 90, training loss: 8.224297523498535 = 1.8242483139038086 + 1.0 * 6.400049209594727
Epoch 90, val loss: 1.82248055934906
Epoch 100, training loss: 8.156180381774902 = 1.812299370765686 + 1.0 * 6.343881130218506
Epoch 100, val loss: 1.8117164373397827
Epoch 110, training loss: 8.101179122924805 = 1.801011085510254 + 1.0 * 6.300168037414551
Epoch 110, val loss: 1.8017370700836182
Epoch 120, training loss: 8.056069374084473 = 1.7902195453643799 + 1.0 * 6.265850067138672
Epoch 120, val loss: 1.792328119277954
Epoch 130, training loss: 8.020363807678223 = 1.7795253992080688 + 1.0 * 6.240838050842285
Epoch 130, val loss: 1.7830297946929932
Epoch 140, training loss: 7.9879961013793945 = 1.7683316469192505 + 1.0 * 6.219664573669434
Epoch 140, val loss: 1.773455023765564
Epoch 150, training loss: 7.959652900695801 = 1.755994200706482 + 1.0 * 6.203658580780029
Epoch 150, val loss: 1.7630873918533325
Epoch 160, training loss: 7.931966304779053 = 1.7418912649154663 + 1.0 * 6.190074920654297
Epoch 160, val loss: 1.751512050628662
Epoch 170, training loss: 7.905134201049805 = 1.7253323793411255 + 1.0 * 6.179801940917969
Epoch 170, val loss: 1.7381412982940674
Epoch 180, training loss: 7.877330303192139 = 1.7058151960372925 + 1.0 * 6.171514987945557
Epoch 180, val loss: 1.722517490386963
Epoch 190, training loss: 7.84446907043457 = 1.6825547218322754 + 1.0 * 6.161914348602295
Epoch 190, val loss: 1.7041033506393433
Epoch 200, training loss: 7.809842586517334 = 1.6548161506652832 + 1.0 * 6.155026435852051
Epoch 200, val loss: 1.6821070909500122
Epoch 210, training loss: 7.770055294036865 = 1.6217646598815918 + 1.0 * 6.148290634155273
Epoch 210, val loss: 1.6559803485870361
Epoch 220, training loss: 7.730584144592285 = 1.5828675031661987 + 1.0 * 6.147716522216797
Epoch 220, val loss: 1.6252055168151855
Epoch 230, training loss: 7.677995204925537 = 1.538840413093567 + 1.0 * 6.13915491104126
Epoch 230, val loss: 1.5905005931854248
Epoch 240, training loss: 7.624423027038574 = 1.4898861646652222 + 1.0 * 6.1345367431640625
Epoch 240, val loss: 1.5520286560058594
Epoch 250, training loss: 7.567680835723877 = 1.4368151426315308 + 1.0 * 6.130865573883057
Epoch 250, val loss: 1.5105681419372559
Epoch 260, training loss: 7.512676239013672 = 1.3811194896697998 + 1.0 * 6.131556987762451
Epoch 260, val loss: 1.467801809310913
Epoch 270, training loss: 7.450189590454102 = 1.3251429796218872 + 1.0 * 6.125046730041504
Epoch 270, val loss: 1.4254733324050903
Epoch 280, training loss: 7.3945817947387695 = 1.2691457271575928 + 1.0 * 6.125436305999756
Epoch 280, val loss: 1.3835477828979492
Epoch 290, training loss: 7.333548545837402 = 1.2143871784210205 + 1.0 * 6.119161128997803
Epoch 290, val loss: 1.3431422710418701
Epoch 300, training loss: 7.2765913009643555 = 1.1613447666168213 + 1.0 * 6.115246772766113
Epoch 300, val loss: 1.3041372299194336
Epoch 310, training loss: 7.222497463226318 = 1.1100096702575684 + 1.0 * 6.11248779296875
Epoch 310, val loss: 1.2664473056793213
Epoch 320, training loss: 7.177136421203613 = 1.0611830949783325 + 1.0 * 6.11595344543457
Epoch 320, val loss: 1.2306602001190186
Epoch 330, training loss: 7.122597694396973 = 1.0157742500305176 + 1.0 * 6.106823444366455
Epoch 330, val loss: 1.1975078582763672
Epoch 340, training loss: 7.076290607452393 = 0.9727911949157715 + 1.0 * 6.103499412536621
Epoch 340, val loss: 1.1662702560424805
Epoch 350, training loss: 7.037403106689453 = 0.931729793548584 + 1.0 * 6.105673313140869
Epoch 350, val loss: 1.1367120742797852
Epoch 360, training loss: 6.990673065185547 = 0.8927607536315918 + 1.0 * 6.097912311553955
Epoch 360, val loss: 1.1087244749069214
Epoch 370, training loss: 6.951422214508057 = 0.8551802635192871 + 1.0 * 6.0962419509887695
Epoch 370, val loss: 1.08222234249115
Epoch 380, training loss: 6.914279937744141 = 0.8182011842727661 + 1.0 * 6.096078872680664
Epoch 380, val loss: 1.0560797452926636
Epoch 390, training loss: 6.876010894775391 = 0.7817646265029907 + 1.0 * 6.0942463874816895
Epoch 390, val loss: 1.0306638479232788
Epoch 400, training loss: 6.839179039001465 = 0.7459699511528015 + 1.0 * 6.093209266662598
Epoch 400, val loss: 1.0057344436645508
Epoch 410, training loss: 6.799293518066406 = 0.7106785178184509 + 1.0 * 6.0886149406433105
Epoch 410, val loss: 0.98140949010849
Epoch 420, training loss: 6.762217998504639 = 0.6755282282829285 + 1.0 * 6.0866899490356445
Epoch 420, val loss: 0.9572374224662781
Epoch 430, training loss: 6.724923133850098 = 0.6407612562179565 + 1.0 * 6.084161758422852
Epoch 430, val loss: 0.9335036873817444
Epoch 440, training loss: 6.69487190246582 = 0.6065301895141602 + 1.0 * 6.08834171295166
Epoch 440, val loss: 0.9104835987091064
Epoch 450, training loss: 6.654353618621826 = 0.5729110836982727 + 1.0 * 6.081442356109619
Epoch 450, val loss: 0.8883949518203735
Epoch 460, training loss: 6.623941898345947 = 0.5398862957954407 + 1.0 * 6.084055423736572
Epoch 460, val loss: 0.8672858476638794
Epoch 470, training loss: 6.588599681854248 = 0.5076994299888611 + 1.0 * 6.080900192260742
Epoch 470, val loss: 0.8473304510116577
Epoch 480, training loss: 6.552712440490723 = 0.476342111825943 + 1.0 * 6.0763702392578125
Epoch 480, val loss: 0.8289077877998352
Epoch 490, training loss: 6.52230167388916 = 0.44573020935058594 + 1.0 * 6.076571464538574
Epoch 490, val loss: 0.8117092847824097
Epoch 500, training loss: 6.498067855834961 = 0.41622740030288696 + 1.0 * 6.081840515136719
Epoch 500, val loss: 0.7958412170410156
Epoch 510, training loss: 6.4632158279418945 = 0.38810575008392334 + 1.0 * 6.075109958648682
Epoch 510, val loss: 0.7817218899726868
Epoch 520, training loss: 6.432306289672852 = 0.3611924648284912 + 1.0 * 6.0711140632629395
Epoch 520, val loss: 0.769061267375946
Epoch 530, training loss: 6.404953956604004 = 0.3354879319667816 + 1.0 * 6.0694661140441895
Epoch 530, val loss: 0.757614016532898
Epoch 540, training loss: 6.386554718017578 = 0.31111034750938416 + 1.0 * 6.075444221496582
Epoch 540, val loss: 0.7474362254142761
Epoch 550, training loss: 6.357302665710449 = 0.2883922755718231 + 1.0 * 6.068910598754883
Epoch 550, val loss: 0.7387140989303589
Epoch 560, training loss: 6.3314642906188965 = 0.2672288119792938 + 1.0 * 6.064235687255859
Epoch 560, val loss: 0.7313562035560608
Epoch 570, training loss: 6.317404270172119 = 0.24754610657691956 + 1.0 * 6.069858074188232
Epoch 570, val loss: 0.7250261306762695
Epoch 580, training loss: 6.295220851898193 = 0.2295204997062683 + 1.0 * 6.065700531005859
Epoch 580, val loss: 0.7198513746261597
Epoch 590, training loss: 6.274041175842285 = 0.21311259269714355 + 1.0 * 6.060928821563721
Epoch 590, val loss: 0.7161022424697876
Epoch 600, training loss: 6.257148265838623 = 0.1980425864458084 + 1.0 * 6.05910587310791
Epoch 600, val loss: 0.7132315635681152
Epoch 610, training loss: 6.25070858001709 = 0.18425482511520386 + 1.0 * 6.06645393371582
Epoch 610, val loss: 0.7113162279129028
Epoch 620, training loss: 6.233948230743408 = 0.17176871001720428 + 1.0 * 6.0621795654296875
Epoch 620, val loss: 0.7104628682136536
Epoch 630, training loss: 6.21657657623291 = 0.16042542457580566 + 1.0 * 6.056150913238525
Epoch 630, val loss: 0.7104877233505249
Epoch 640, training loss: 6.2081098556518555 = 0.15006321668624878 + 1.0 * 6.058046817779541
Epoch 640, val loss: 0.7112263441085815
Epoch 650, training loss: 6.199441432952881 = 0.1406065821647644 + 1.0 * 6.058835029602051
Epoch 650, val loss: 0.7125946879386902
Epoch 660, training loss: 6.185669422149658 = 0.1320023536682129 + 1.0 * 6.053667068481445
Epoch 660, val loss: 0.7147279381752014
Epoch 670, training loss: 6.177371501922607 = 0.12409214675426483 + 1.0 * 6.053279399871826
Epoch 670, val loss: 0.7173166871070862
Epoch 680, training loss: 6.169732093811035 = 0.11683674901723862 + 1.0 * 6.052895545959473
Epoch 680, val loss: 0.7203165888786316
Epoch 690, training loss: 6.162820816040039 = 0.11019444465637207 + 1.0 * 6.052626132965088
Epoch 690, val loss: 0.7237442135810852
Epoch 700, training loss: 6.151780605316162 = 0.10409031808376312 + 1.0 * 6.047690391540527
Epoch 700, val loss: 0.7275794744491577
Epoch 710, training loss: 6.145172595977783 = 0.09843512624502182 + 1.0 * 6.0467376708984375
Epoch 710, val loss: 0.7316442728042603
Epoch 720, training loss: 6.148592948913574 = 0.09320440143346786 + 1.0 * 6.055388450622559
Epoch 720, val loss: 0.7358735799789429
Epoch 730, training loss: 6.132590293884277 = 0.08838623017072678 + 1.0 * 6.044204235076904
Epoch 730, val loss: 0.7403554916381836
Epoch 740, training loss: 6.131694793701172 = 0.0839325413107872 + 1.0 * 6.047762393951416
Epoch 740, val loss: 0.7451412677764893
Epoch 750, training loss: 6.122376918792725 = 0.07980494946241379 + 1.0 * 6.042572021484375
Epoch 750, val loss: 0.7497748732566833
Epoch 760, training loss: 6.118470191955566 = 0.07598226517438889 + 1.0 * 6.042488098144531
Epoch 760, val loss: 0.7546738982200623
Epoch 770, training loss: 6.114508628845215 = 0.07240946590900421 + 1.0 * 6.0420989990234375
Epoch 770, val loss: 0.7596585154533386
Epoch 780, training loss: 6.111598491668701 = 0.06907815486192703 + 1.0 * 6.042520523071289
Epoch 780, val loss: 0.7645496726036072
Epoch 790, training loss: 6.107218265533447 = 0.0659978985786438 + 1.0 * 6.041220188140869
Epoch 790, val loss: 0.7695860862731934
Epoch 800, training loss: 6.1024346351623535 = 0.06312273442745209 + 1.0 * 6.03931188583374
Epoch 800, val loss: 0.7747044563293457
Epoch 810, training loss: 6.102924823760986 = 0.06042275205254555 + 1.0 * 6.042501926422119
Epoch 810, val loss: 0.7797373533248901
Epoch 820, training loss: 6.097305774688721 = 0.057891979813575745 + 1.0 * 6.039413928985596
Epoch 820, val loss: 0.7847297787666321
Epoch 830, training loss: 6.0946125984191895 = 0.05552595853805542 + 1.0 * 6.039086818695068
Epoch 830, val loss: 0.7897958159446716
Epoch 840, training loss: 6.091359615325928 = 0.053303468972444534 + 1.0 * 6.038056373596191
Epoch 840, val loss: 0.7947580814361572
Epoch 850, training loss: 6.088500022888184 = 0.051214225590229034 + 1.0 * 6.037285804748535
Epoch 850, val loss: 0.7997947931289673
Epoch 860, training loss: 6.086617469787598 = 0.04924876615405083 + 1.0 * 6.0373687744140625
Epoch 860, val loss: 0.8047506213188171
Epoch 870, training loss: 6.087368488311768 = 0.04739470034837723 + 1.0 * 6.039973735809326
Epoch 870, val loss: 0.8095554113388062
Epoch 880, training loss: 6.078810691833496 = 0.045659735798835754 + 1.0 * 6.033151149749756
Epoch 880, val loss: 0.8144325613975525
Epoch 890, training loss: 6.075768947601318 = 0.04401667043566704 + 1.0 * 6.031752109527588
Epoch 890, val loss: 0.819336473941803
Epoch 900, training loss: 6.08070707321167 = 0.04245844483375549 + 1.0 * 6.038248538970947
Epoch 900, val loss: 0.8241033554077148
Epoch 910, training loss: 6.073554039001465 = 0.04098634421825409 + 1.0 * 6.032567501068115
Epoch 910, val loss: 0.8287235498428345
Epoch 920, training loss: 6.078291893005371 = 0.039594147354364395 + 1.0 * 6.038697719573975
Epoch 920, val loss: 0.8334898352622986
Epoch 930, training loss: 6.068929672241211 = 0.03827938437461853 + 1.0 * 6.0306501388549805
Epoch 930, val loss: 0.8380622267723083
Epoch 940, training loss: 6.064661979675293 = 0.03702935203909874 + 1.0 * 6.027632713317871
Epoch 940, val loss: 0.8427566885948181
Epoch 950, training loss: 6.062458038330078 = 0.035833775997161865 + 1.0 * 6.0266242027282715
Epoch 950, val loss: 0.8473930358886719
Epoch 960, training loss: 6.082831859588623 = 0.03469369187951088 + 1.0 * 6.04813814163208
Epoch 960, val loss: 0.8518287539482117
Epoch 970, training loss: 6.060005187988281 = 0.03361097723245621 + 1.0 * 6.026394367218018
Epoch 970, val loss: 0.8561618328094482
Epoch 980, training loss: 6.0612287521362305 = 0.032594963908195496 + 1.0 * 6.0286335945129395
Epoch 980, val loss: 0.8607797622680664
Epoch 990, training loss: 6.058971405029297 = 0.03161870688199997 + 1.0 * 6.027352809906006
Epoch 990, val loss: 0.8650697469711304
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9373
Flip ASR: 0.9244/225 nodes
The final ASR:0.52522, 0.31251, Accuracy:0.81728, 0.01522
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9458])
updated graph: torch.Size([2, 10512])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00627, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.314661026000977 = 1.9407395124435425 + 1.0 * 8.373921394348145
Epoch 0, val loss: 1.9527795314788818
Epoch 10, training loss: 10.305072784423828 = 1.931417465209961 + 1.0 * 8.373655319213867
Epoch 10, val loss: 1.943289041519165
Epoch 20, training loss: 10.291447639465332 = 1.9197235107421875 + 1.0 * 8.371724128723145
Epoch 20, val loss: 1.9310343265533447
Epoch 30, training loss: 10.25877857208252 = 1.903321623802185 + 1.0 * 8.355457305908203
Epoch 30, val loss: 1.9137694835662842
Epoch 40, training loss: 10.115055084228516 = 1.881437063217163 + 1.0 * 8.233617782592773
Epoch 40, val loss: 1.8916239738464355
Epoch 50, training loss: 9.617514610290527 = 1.8583110570907593 + 1.0 * 7.759203910827637
Epoch 50, val loss: 1.8689407110214233
Epoch 60, training loss: 9.170801162719727 = 1.8395260572433472 + 1.0 * 7.33127498626709
Epoch 60, val loss: 1.851336121559143
Epoch 70, training loss: 8.741437911987305 = 1.825386643409729 + 1.0 * 6.916051387786865
Epoch 70, val loss: 1.837317943572998
Epoch 80, training loss: 8.52488899230957 = 1.8111178874969482 + 1.0 * 6.713770866394043
Epoch 80, val loss: 1.8234480619430542
Epoch 90, training loss: 8.398015022277832 = 1.7942224740982056 + 1.0 * 6.603792667388916
Epoch 90, val loss: 1.8072688579559326
Epoch 100, training loss: 8.287436485290527 = 1.775913119316101 + 1.0 * 6.511523723602295
Epoch 100, val loss: 1.7896358966827393
Epoch 110, training loss: 8.206404685974121 = 1.7573784589767456 + 1.0 * 6.449026107788086
Epoch 110, val loss: 1.7720681428909302
Epoch 120, training loss: 8.137022972106934 = 1.7378045320510864 + 1.0 * 6.3992180824279785
Epoch 120, val loss: 1.7544597387313843
Epoch 130, training loss: 8.077862739562988 = 1.7158480882644653 + 1.0 * 6.3620147705078125
Epoch 130, val loss: 1.7354754209518433
Epoch 140, training loss: 8.019816398620605 = 1.6907554864883423 + 1.0 * 6.3290605545043945
Epoch 140, val loss: 1.713930606842041
Epoch 150, training loss: 7.966431617736816 = 1.6614404916763306 + 1.0 * 6.304991245269775
Epoch 150, val loss: 1.6891549825668335
Epoch 160, training loss: 7.912137031555176 = 1.6269350051879883 + 1.0 * 6.2852020263671875
Epoch 160, val loss: 1.6601779460906982
Epoch 170, training loss: 7.854183673858643 = 1.5865329504013062 + 1.0 * 6.267650604248047
Epoch 170, val loss: 1.6266642808914185
Epoch 180, training loss: 7.795805931091309 = 1.5396252870559692 + 1.0 * 6.256180763244629
Epoch 180, val loss: 1.5879220962524414
Epoch 190, training loss: 7.727663993835449 = 1.4872016906738281 + 1.0 * 6.240462303161621
Epoch 190, val loss: 1.5447279214859009
Epoch 200, training loss: 7.659843921661377 = 1.4292353391647339 + 1.0 * 6.2306084632873535
Epoch 200, val loss: 1.4972416162490845
Epoch 210, training loss: 7.588375091552734 = 1.3670551776885986 + 1.0 * 6.221320152282715
Epoch 210, val loss: 1.4465168714523315
Epoch 220, training loss: 7.515598297119141 = 1.3020598888397217 + 1.0 * 6.213538646697998
Epoch 220, val loss: 1.394056797027588
Epoch 230, training loss: 7.44608736038208 = 1.2369481325149536 + 1.0 * 6.209139347076416
Epoch 230, val loss: 1.3422209024429321
Epoch 240, training loss: 7.37333869934082 = 1.173716425895691 + 1.0 * 6.19962215423584
Epoch 240, val loss: 1.2923716306686401
Epoch 250, training loss: 7.305722713470459 = 1.1124721765518188 + 1.0 * 6.19325065612793
Epoch 250, val loss: 1.244551658630371
Epoch 260, training loss: 7.248251914978027 = 1.053528070449829 + 1.0 * 6.194723606109619
Epoch 260, val loss: 1.1990687847137451
Epoch 270, training loss: 7.184675693511963 = 0.9991003274917603 + 1.0 * 6.185575485229492
Epoch 270, val loss: 1.1569318771362305
Epoch 280, training loss: 7.125511169433594 = 0.9482100009918213 + 1.0 * 6.177301406860352
Epoch 280, val loss: 1.1180263757705688
Epoch 290, training loss: 7.071484565734863 = 0.8998866677284241 + 1.0 * 6.171597957611084
Epoch 290, val loss: 1.0812532901763916
Epoch 300, training loss: 7.029706954956055 = 0.8540403246879578 + 1.0 * 6.175666809082031
Epoch 300, val loss: 1.0464023351669312
Epoch 310, training loss: 6.976121425628662 = 0.8111339807510376 + 1.0 * 6.164987564086914
Epoch 310, val loss: 1.014333963394165
Epoch 320, training loss: 6.929137706756592 = 0.7708330154418945 + 1.0 * 6.158304691314697
Epoch 320, val loss: 0.9847549796104431
Epoch 330, training loss: 6.894582748413086 = 0.7328349947929382 + 1.0 * 6.161747932434082
Epoch 330, val loss: 0.957282304763794
Epoch 340, training loss: 6.8492536544799805 = 0.6976166367530823 + 1.0 * 6.151637077331543
Epoch 340, val loss: 0.9324547648429871
Epoch 350, training loss: 6.811337471008301 = 0.6648436784744263 + 1.0 * 6.146493911743164
Epoch 350, val loss: 0.9101946353912354
Epoch 360, training loss: 6.777612686157227 = 0.6342194676399231 + 1.0 * 6.143393039703369
Epoch 360, val loss: 0.8901755809783936
Epoch 370, training loss: 6.758158206939697 = 0.6059460639953613 + 1.0 * 6.152212142944336
Epoch 370, val loss: 0.8724814653396606
Epoch 380, training loss: 6.715846538543701 = 0.5801089406013489 + 1.0 * 6.135737419128418
Epoch 380, val loss: 0.8572807312011719
Epoch 390, training loss: 6.688920497894287 = 0.5560879707336426 + 1.0 * 6.1328325271606445
Epoch 390, val loss: 0.8440653681755066
Epoch 400, training loss: 6.662105560302734 = 0.5335463881492615 + 1.0 * 6.128559112548828
Epoch 400, val loss: 0.8325642347335815
Epoch 410, training loss: 6.645587921142578 = 0.5122660994529724 + 1.0 * 6.133321762084961
Epoch 410, val loss: 0.822568416595459
Epoch 420, training loss: 6.616054534912109 = 0.4924278259277344 + 1.0 * 6.123626708984375
Epoch 420, val loss: 0.8137986063957214
Epoch 430, training loss: 6.595335960388184 = 0.4735810458660126 + 1.0 * 6.121755123138428
Epoch 430, val loss: 0.806291401386261
Epoch 440, training loss: 6.573998928070068 = 0.4554227590560913 + 1.0 * 6.1185760498046875
Epoch 440, val loss: 0.7996150255203247
Epoch 450, training loss: 6.562467098236084 = 0.4377405047416687 + 1.0 * 6.12472677230835
Epoch 450, val loss: 0.7935857772827148
Epoch 460, training loss: 6.534150123596191 = 0.42053350806236267 + 1.0 * 6.113616466522217
Epoch 460, val loss: 0.7881155610084534
Epoch 470, training loss: 6.516148567199707 = 0.40360525250434875 + 1.0 * 6.112543106079102
Epoch 470, val loss: 0.7831595540046692
Epoch 480, training loss: 6.499833583831787 = 0.386893093585968 + 1.0 * 6.112940311431885
Epoch 480, val loss: 0.7785691022872925
Epoch 490, training loss: 6.480798721313477 = 0.3704049587249756 + 1.0 * 6.110393524169922
Epoch 490, val loss: 0.7744280099868774
Epoch 500, training loss: 6.463900566101074 = 0.35424360632896423 + 1.0 * 6.109656810760498
Epoch 500, val loss: 0.7707284092903137
Epoch 510, training loss: 6.4422197341918945 = 0.3383810222148895 + 1.0 * 6.103838920593262
Epoch 510, val loss: 0.7675248980522156
Epoch 520, training loss: 6.424556255340576 = 0.3228156268596649 + 1.0 * 6.101740837097168
Epoch 520, val loss: 0.7647877335548401
Epoch 530, training loss: 6.415572643280029 = 0.3076552450656891 + 1.0 * 6.107917308807373
Epoch 530, val loss: 0.7624952793121338
Epoch 540, training loss: 6.395244121551514 = 0.2929249405860901 + 1.0 * 6.102319240570068
Epoch 540, val loss: 0.7606870532035828
Epoch 550, training loss: 6.375443935394287 = 0.27873820066452026 + 1.0 * 6.096705913543701
Epoch 550, val loss: 0.7592987418174744
Epoch 560, training loss: 6.360662460327148 = 0.2650073766708374 + 1.0 * 6.0956549644470215
Epoch 560, val loss: 0.7582956552505493
Epoch 570, training loss: 6.347886562347412 = 0.2517464756965637 + 1.0 * 6.096139907836914
Epoch 570, val loss: 0.7576408386230469
Epoch 580, training loss: 6.331839084625244 = 0.23899024724960327 + 1.0 * 6.092848777770996
Epoch 580, val loss: 0.7573258280754089
Epoch 590, training loss: 6.317756175994873 = 0.22666899859905243 + 1.0 * 6.091087341308594
Epoch 590, val loss: 0.7573217153549194
Epoch 600, training loss: 6.306433200836182 = 0.21478863060474396 + 1.0 * 6.091644763946533
Epoch 600, val loss: 0.7576022744178772
Epoch 610, training loss: 6.291207313537598 = 0.20332475006580353 + 1.0 * 6.0878825187683105
Epoch 610, val loss: 0.7580658793449402
Epoch 620, training loss: 6.283668518066406 = 0.19233593344688416 + 1.0 * 6.09133243560791
Epoch 620, val loss: 0.7588582038879395
Epoch 630, training loss: 6.267314910888672 = 0.18183588981628418 + 1.0 * 6.085479259490967
Epoch 630, val loss: 0.7598958015441895
Epoch 640, training loss: 6.261145114898682 = 0.1717529445886612 + 1.0 * 6.089392185211182
Epoch 640, val loss: 0.761222243309021
Epoch 650, training loss: 6.252227783203125 = 0.1622210592031479 + 1.0 * 6.0900068283081055
Epoch 650, val loss: 0.7627007961273193
Epoch 660, training loss: 6.234838962554932 = 0.15315945446491241 + 1.0 * 6.081679344177246
Epoch 660, val loss: 0.7644805312156677
Epoch 670, training loss: 6.222833156585693 = 0.14455783367156982 + 1.0 * 6.078275203704834
Epoch 670, val loss: 0.7665526866912842
Epoch 680, training loss: 6.215714931488037 = 0.1363961547613144 + 1.0 * 6.079319000244141
Epoch 680, val loss: 0.768869936466217
Epoch 690, training loss: 6.2059831619262695 = 0.12870554625988007 + 1.0 * 6.077277660369873
Epoch 690, val loss: 0.7713590264320374
Epoch 700, training loss: 6.197356224060059 = 0.12148712575435638 + 1.0 * 6.075869083404541
Epoch 700, val loss: 0.7740699648857117
Epoch 710, training loss: 6.190802574157715 = 0.1146930605173111 + 1.0 * 6.076109409332275
Epoch 710, val loss: 0.7770817279815674
Epoch 720, training loss: 6.182270526885986 = 0.10831287503242493 + 1.0 * 6.073957443237305
Epoch 720, val loss: 0.7802926898002625
Epoch 730, training loss: 6.175657272338867 = 0.10234370082616806 + 1.0 * 6.0733137130737305
Epoch 730, val loss: 0.7837193608283997
Epoch 740, training loss: 6.167646408081055 = 0.09674958139657974 + 1.0 * 6.070896625518799
Epoch 740, val loss: 0.7873938679695129
Epoch 750, training loss: 6.178324222564697 = 0.09149564802646637 + 1.0 * 6.086828708648682
Epoch 750, val loss: 0.7912064790725708
Epoch 760, training loss: 6.156972885131836 = 0.08661071211099625 + 1.0 * 6.070362091064453
Epoch 760, val loss: 0.7951906323432922
Epoch 770, training loss: 6.149955749511719 = 0.08204857259988785 + 1.0 * 6.067907333374023
Epoch 770, val loss: 0.7993997931480408
Epoch 780, training loss: 6.1474928855896 = 0.0777718797326088 + 1.0 * 6.069721221923828
Epoch 780, val loss: 0.8037918210029602
Epoch 790, training loss: 6.143026828765869 = 0.07379540055990219 + 1.0 * 6.0692315101623535
Epoch 790, val loss: 0.8082358241081238
Epoch 800, training loss: 6.13894510269165 = 0.07008515298366547 + 1.0 * 6.068860054016113
Epoch 800, val loss: 0.8127571940422058
Epoch 810, training loss: 6.1299614906311035 = 0.06662506610155106 + 1.0 * 6.063336372375488
Epoch 810, val loss: 0.8174420595169067
Epoch 820, training loss: 6.126670837402344 = 0.06337965279817581 + 1.0 * 6.063291072845459
Epoch 820, val loss: 0.8222276568412781
Epoch 830, training loss: 6.132504463195801 = 0.060337115079164505 + 1.0 * 6.07216739654541
Epoch 830, val loss: 0.827066957950592
Epoch 840, training loss: 6.123412132263184 = 0.05750596523284912 + 1.0 * 6.065906047821045
Epoch 840, val loss: 0.831940233707428
Epoch 850, training loss: 6.116113662719727 = 0.05484655871987343 + 1.0 * 6.061266899108887
Epoch 850, val loss: 0.8368316292762756
Epoch 860, training loss: 6.112032890319824 = 0.052360694855451584 + 1.0 * 6.0596723556518555
Epoch 860, val loss: 0.8417891263961792
Epoch 870, training loss: 6.113661289215088 = 0.050025660544633865 + 1.0 * 6.06363582611084
Epoch 870, val loss: 0.8467798233032227
Epoch 880, training loss: 6.106909275054932 = 0.0478336401283741 + 1.0 * 6.059075832366943
Epoch 880, val loss: 0.8517225980758667
Epoch 890, training loss: 6.111856460571289 = 0.04578970745205879 + 1.0 * 6.066066741943359
Epoch 890, val loss: 0.8566997647285461
Epoch 900, training loss: 6.10420560836792 = 0.043857965618371964 + 1.0 * 6.060347557067871
Epoch 900, val loss: 0.8615356087684631
Epoch 910, training loss: 6.097833633422852 = 0.042055509984493256 + 1.0 * 6.0557780265808105
Epoch 910, val loss: 0.866450309753418
Epoch 920, training loss: 6.0965752601623535 = 0.04035064950585365 + 1.0 * 6.056224822998047
Epoch 920, val loss: 0.871357798576355
Epoch 930, training loss: 6.097757339477539 = 0.03874409198760986 + 1.0 * 6.059013366699219
Epoch 930, val loss: 0.8762161731719971
Epoch 940, training loss: 6.0963616371154785 = 0.03723832219839096 + 1.0 * 6.059123516082764
Epoch 940, val loss: 0.8810173869132996
Epoch 950, training loss: 6.087846279144287 = 0.03581061214208603 + 1.0 * 6.052035808563232
Epoch 950, val loss: 0.8857587575912476
Epoch 960, training loss: 6.085900783538818 = 0.034463874995708466 + 1.0 * 6.051436901092529
Epoch 960, val loss: 0.8904916644096375
Epoch 970, training loss: 6.08444881439209 = 0.03318930044770241 + 1.0 * 6.051259517669678
Epoch 970, val loss: 0.8952583074569702
Epoch 980, training loss: 6.089099407196045 = 0.03197862580418587 + 1.0 * 6.0571208000183105
Epoch 980, val loss: 0.8999646306037903
Epoch 990, training loss: 6.08502721786499 = 0.030833875760436058 + 1.0 * 6.054193496704102
Epoch 990, val loss: 0.9045669436454773
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.337541580200195 = 1.9636465311050415 + 1.0 * 8.373894691467285
Epoch 0, val loss: 1.9553039073944092
Epoch 10, training loss: 10.326666831970215 = 1.953068494796753 + 1.0 * 8.373598098754883
Epoch 10, val loss: 1.945383906364441
Epoch 20, training loss: 10.311338424682617 = 1.940131425857544 + 1.0 * 8.371207237243652
Epoch 20, val loss: 1.932814121246338
Epoch 30, training loss: 10.27521800994873 = 1.92220139503479 + 1.0 * 8.35301685333252
Epoch 30, val loss: 1.9150772094726562
Epoch 40, training loss: 10.137353897094727 = 1.898789882659912 + 1.0 * 8.238564491271973
Epoch 40, val loss: 1.8924269676208496
Epoch 50, training loss: 9.659502983093262 = 1.8736618757247925 + 1.0 * 7.78584098815918
Epoch 50, val loss: 1.8687046766281128
Epoch 60, training loss: 9.281773567199707 = 1.8495614528656006 + 1.0 * 7.432211875915527
Epoch 60, val loss: 1.8469569683074951
Epoch 70, training loss: 8.878085136413574 = 1.829659342765808 + 1.0 * 7.048426151275635
Epoch 70, val loss: 1.8286656141281128
Epoch 80, training loss: 8.619049072265625 = 1.8110226392745972 + 1.0 * 6.8080267906188965
Epoch 80, val loss: 1.8118730783462524
Epoch 90, training loss: 8.450054168701172 = 1.7922043800354004 + 1.0 * 6.6578497886657715
Epoch 90, val loss: 1.7955148220062256
Epoch 100, training loss: 8.332379341125488 = 1.7735960483551025 + 1.0 * 6.558783054351807
Epoch 100, val loss: 1.7803643941879272
Epoch 110, training loss: 8.23544692993164 = 1.7556242942810059 + 1.0 * 6.479822158813477
Epoch 110, val loss: 1.7661210298538208
Epoch 120, training loss: 8.167075157165527 = 1.7370280027389526 + 1.0 * 6.430047512054443
Epoch 120, val loss: 1.7511214017868042
Epoch 130, training loss: 8.110618591308594 = 1.7164541482925415 + 1.0 * 6.394164085388184
Epoch 130, val loss: 1.7343957424163818
Epoch 140, training loss: 8.058220863342285 = 1.6930807828903198 + 1.0 * 6.365140438079834
Epoch 140, val loss: 1.7153549194335938
Epoch 150, training loss: 8.008516311645508 = 1.6661142110824585 + 1.0 * 6.342402458190918
Epoch 150, val loss: 1.6934986114501953
Epoch 160, training loss: 7.953404426574707 = 1.6351755857467651 + 1.0 * 6.318228721618652
Epoch 160, val loss: 1.6686866283416748
Epoch 170, training loss: 7.897520542144775 = 1.5994046926498413 + 1.0 * 6.2981157302856445
Epoch 170, val loss: 1.6398528814315796
Epoch 180, training loss: 7.842484474182129 = 1.5577811002731323 + 1.0 * 6.284703254699707
Epoch 180, val loss: 1.6063697338104248
Epoch 190, training loss: 7.779270172119141 = 1.5107580423355103 + 1.0 * 6.26851224899292
Epoch 190, val loss: 1.5684906244277954
Epoch 200, training loss: 7.714015007019043 = 1.4582884311676025 + 1.0 * 6.2557268142700195
Epoch 200, val loss: 1.5263562202453613
Epoch 210, training loss: 7.646169662475586 = 1.4015063047409058 + 1.0 * 6.244663238525391
Epoch 210, val loss: 1.4813201427459717
Epoch 220, training loss: 7.57780647277832 = 1.3427679538726807 + 1.0 * 6.2350382804870605
Epoch 220, val loss: 1.4352704286575317
Epoch 230, training loss: 7.507515907287598 = 1.2830488681793213 + 1.0 * 6.224466800689697
Epoch 230, val loss: 1.3889563083648682
Epoch 240, training loss: 7.4438323974609375 = 1.223914384841919 + 1.0 * 6.2199177742004395
Epoch 240, val loss: 1.3438284397125244
Epoch 250, training loss: 7.37553071975708 = 1.1673239469528198 + 1.0 * 6.208206653594971
Epoch 250, val loss: 1.301448106765747
Epoch 260, training loss: 7.31494665145874 = 1.1133575439453125 + 1.0 * 6.201589107513428
Epoch 260, val loss: 1.261846661567688
Epoch 270, training loss: 7.255941390991211 = 1.0621014833450317 + 1.0 * 6.193840026855469
Epoch 270, val loss: 1.2249853610992432
Epoch 280, training loss: 7.202551364898682 = 1.0135289430618286 + 1.0 * 6.189022541046143
Epoch 280, val loss: 1.1909620761871338
Epoch 290, training loss: 7.150847434997559 = 0.967684805393219 + 1.0 * 6.183162689208984
Epoch 290, val loss: 1.1592203378677368
Epoch 300, training loss: 7.099217891693115 = 0.9237874746322632 + 1.0 * 6.1754302978515625
Epoch 300, val loss: 1.1296472549438477
Epoch 310, training loss: 7.061968803405762 = 0.8815036416053772 + 1.0 * 6.180465221405029
Epoch 310, val loss: 1.1016623973846436
Epoch 320, training loss: 7.011000156402588 = 0.8417393565177917 + 1.0 * 6.1692609786987305
Epoch 320, val loss: 1.075285792350769
Epoch 330, training loss: 6.966298580169678 = 0.8037523627281189 + 1.0 * 6.162546157836914
Epoch 330, val loss: 1.0508921146392822
Epoch 340, training loss: 6.925660610198975 = 0.7671380043029785 + 1.0 * 6.158522605895996
Epoch 340, val loss: 1.0275956392288208
Epoch 350, training loss: 6.891256332397461 = 0.7321752905845642 + 1.0 * 6.159080982208252
Epoch 350, val loss: 1.0056239366531372
Epoch 360, training loss: 6.850543975830078 = 0.6992495059967041 + 1.0 * 6.151294708251953
Epoch 360, val loss: 0.9854752421379089
Epoch 370, training loss: 6.815408706665039 = 0.6677738428115845 + 1.0 * 6.147634983062744
Epoch 370, val loss: 0.9667471647262573
Epoch 380, training loss: 6.781396389007568 = 0.6377105116844177 + 1.0 * 6.143685817718506
Epoch 380, val loss: 0.9493264555931091
Epoch 390, training loss: 6.7558441162109375 = 0.6089710593223572 + 1.0 * 6.1468729972839355
Epoch 390, val loss: 0.9335559606552124
Epoch 400, training loss: 6.718262672424316 = 0.5815687775611877 + 1.0 * 6.136693954467773
Epoch 400, val loss: 0.9188253879547119
Epoch 410, training loss: 6.688299655914307 = 0.5551732778549194 + 1.0 * 6.133126258850098
Epoch 410, val loss: 0.9055593013763428
Epoch 420, training loss: 6.659425258636475 = 0.5294813513755798 + 1.0 * 6.12994384765625
Epoch 420, val loss: 0.8931325674057007
Epoch 430, training loss: 6.6357269287109375 = 0.5045044422149658 + 1.0 * 6.131222724914551
Epoch 430, val loss: 0.8814015984535217
Epoch 440, training loss: 6.606217861175537 = 0.48037099838256836 + 1.0 * 6.125846862792969
Epoch 440, val loss: 0.8708902597427368
Epoch 450, training loss: 6.57939338684082 = 0.4569067656993866 + 1.0 * 6.122486591339111
Epoch 450, val loss: 0.8615128397941589
Epoch 460, training loss: 6.555253505706787 = 0.4340481758117676 + 1.0 * 6.1212053298950195
Epoch 460, val loss: 0.852823793888092
Epoch 470, training loss: 6.535852432250977 = 0.4120662808418274 + 1.0 * 6.123785972595215
Epoch 470, val loss: 0.845037579536438
Epoch 480, training loss: 6.5077290534973145 = 0.3908459544181824 + 1.0 * 6.116883277893066
Epoch 480, val loss: 0.8386754989624023
Epoch 490, training loss: 6.4839863777160645 = 0.3702937662601471 + 1.0 * 6.113692760467529
Epoch 490, val loss: 0.8331162929534912
Epoch 500, training loss: 6.4727935791015625 = 0.35033348202705383 + 1.0 * 6.122459888458252
Epoch 500, val loss: 0.8282305598258972
Epoch 510, training loss: 6.443405628204346 = 0.33109939098358154 + 1.0 * 6.112306118011475
Epoch 510, val loss: 0.8242995142936707
Epoch 520, training loss: 6.420400142669678 = 0.31237712502479553 + 1.0 * 6.108023166656494
Epoch 520, val loss: 0.821412205696106
Epoch 530, training loss: 6.401162147521973 = 0.294135183095932 + 1.0 * 6.107027053833008
Epoch 530, val loss: 0.8189509510993958
Epoch 540, training loss: 6.380912780761719 = 0.2764670252799988 + 1.0 * 6.104445934295654
Epoch 540, val loss: 0.8169491291046143
Epoch 550, training loss: 6.370700836181641 = 0.25940704345703125 + 1.0 * 6.111293792724609
Epoch 550, val loss: 0.8158750534057617
Epoch 560, training loss: 6.344783782958984 = 0.24310334026813507 + 1.0 * 6.101680278778076
Epoch 560, val loss: 0.8150964975357056
Epoch 570, training loss: 6.327073574066162 = 0.22748486697673798 + 1.0 * 6.099588871002197
Epoch 570, val loss: 0.8151788115501404
Epoch 580, training loss: 6.310434341430664 = 0.21261383593082428 + 1.0 * 6.097820281982422
Epoch 580, val loss: 0.8157344460487366
Epoch 590, training loss: 6.29752779006958 = 0.1985354721546173 + 1.0 * 6.098992347717285
Epoch 590, val loss: 0.8169143199920654
Epoch 600, training loss: 6.289449691772461 = 0.185453861951828 + 1.0 * 6.1039958000183105
Epoch 600, val loss: 0.8183455467224121
Epoch 610, training loss: 6.267353057861328 = 0.17334546148777008 + 1.0 * 6.09400749206543
Epoch 610, val loss: 0.8209980726242065
Epoch 620, training loss: 6.254178524017334 = 0.16214586794376373 + 1.0 * 6.092032432556152
Epoch 620, val loss: 0.8240513205528259
Epoch 630, training loss: 6.246470928192139 = 0.1517699956893921 + 1.0 * 6.094700813293457
Epoch 630, val loss: 0.8273316025733948
Epoch 640, training loss: 6.231820106506348 = 0.14226222038269043 + 1.0 * 6.089558124542236
Epoch 640, val loss: 0.8313069343566895
Epoch 650, training loss: 6.221616268157959 = 0.13349312543869019 + 1.0 * 6.088123321533203
Epoch 650, val loss: 0.835938572883606
Epoch 660, training loss: 6.215560436248779 = 0.12544384598731995 + 1.0 * 6.090116500854492
Epoch 660, val loss: 0.8406679630279541
Epoch 670, training loss: 6.207747459411621 = 0.11803610622882843 + 1.0 * 6.0897111892700195
Epoch 670, val loss: 0.8458398580551147
Epoch 680, training loss: 6.195490837097168 = 0.11123530566692352 + 1.0 * 6.084255695343018
Epoch 680, val loss: 0.8513463735580444
Epoch 690, training loss: 6.186331748962402 = 0.10494738072156906 + 1.0 * 6.081384181976318
Epoch 690, val loss: 0.8572032451629639
Epoch 700, training loss: 6.180490970611572 = 0.09913891553878784 + 1.0 * 6.081352233886719
Epoch 700, val loss: 0.863355278968811
Epoch 710, training loss: 6.178408145904541 = 0.09374848753213882 + 1.0 * 6.084659576416016
Epoch 710, val loss: 0.8694290518760681
Epoch 720, training loss: 6.167843818664551 = 0.08880139142274857 + 1.0 * 6.079042434692383
Epoch 720, val loss: 0.8760582208633423
Epoch 730, training loss: 6.160315036773682 = 0.08420418202877045 + 1.0 * 6.07611083984375
Epoch 730, val loss: 0.8830028772354126
Epoch 740, training loss: 6.159178733825684 = 0.07992762327194214 + 1.0 * 6.079251289367676
Epoch 740, val loss: 0.8898547291755676
Epoch 750, training loss: 6.158336162567139 = 0.0759684219956398 + 1.0 * 6.082367897033691
Epoch 750, val loss: 0.8965501189231873
Epoch 760, training loss: 6.145846366882324 = 0.07230530679225922 + 1.0 * 6.073541164398193
Epoch 760, val loss: 0.9037855863571167
Epoch 770, training loss: 6.141176700592041 = 0.06889393925666809 + 1.0 * 6.072282791137695
Epoch 770, val loss: 0.9110733270645142
Epoch 780, training loss: 6.136081218719482 = 0.06569556891918182 + 1.0 * 6.070385456085205
Epoch 780, val loss: 0.9181426167488098
Epoch 790, training loss: 6.143209457397461 = 0.06270787864923477 + 1.0 * 6.080501556396484
Epoch 790, val loss: 0.9251384735107422
Epoch 800, training loss: 6.130193710327148 = 0.05991349369287491 + 1.0 * 6.070280075073242
Epoch 800, val loss: 0.9323658347129822
Epoch 810, training loss: 6.1266326904296875 = 0.05730845034122467 + 1.0 * 6.069324016571045
Epoch 810, val loss: 0.9397724866867065
Epoch 820, training loss: 6.122210502624512 = 0.05486356467008591 + 1.0 * 6.067347049713135
Epoch 820, val loss: 0.9468652606010437
Epoch 830, training loss: 6.120937347412109 = 0.052559807896614075 + 1.0 * 6.068377494812012
Epoch 830, val loss: 0.9541130661964417
Epoch 840, training loss: 6.114282131195068 = 0.0504068024456501 + 1.0 * 6.063875198364258
Epoch 840, val loss: 0.9611603021621704
Epoch 850, training loss: 6.116699695587158 = 0.04837641492486 + 1.0 * 6.068323135375977
Epoch 850, val loss: 0.968483567237854
Epoch 860, training loss: 6.1097002029418945 = 0.04646819829940796 + 1.0 * 6.063231945037842
Epoch 860, val loss: 0.975214421749115
Epoch 870, training loss: 6.1054253578186035 = 0.04467290639877319 + 1.0 * 6.0607523918151855
Epoch 870, val loss: 0.9826012253761292
Epoch 880, training loss: 6.108907699584961 = 0.042974330484867096 + 1.0 * 6.0659332275390625
Epoch 880, val loss: 0.9895489811897278
Epoch 890, training loss: 6.103729248046875 = 0.04137350991368294 + 1.0 * 6.0623555183410645
Epoch 890, val loss: 0.9962685108184814
Epoch 900, training loss: 6.101736068725586 = 0.03986009210348129 + 1.0 * 6.061875820159912
Epoch 900, val loss: 1.0032256841659546
Epoch 910, training loss: 6.096285343170166 = 0.0384264774620533 + 1.0 * 6.057858943939209
Epoch 910, val loss: 1.0101096630096436
Epoch 920, training loss: 6.102824687957764 = 0.03706934675574303 + 1.0 * 6.065755367279053
Epoch 920, val loss: 1.0169060230255127
Epoch 930, training loss: 6.095525741577148 = 0.03577598184347153 + 1.0 * 6.059749603271484
Epoch 930, val loss: 1.0233893394470215
Epoch 940, training loss: 6.090688228607178 = 0.03455879166722298 + 1.0 * 6.056129455566406
Epoch 940, val loss: 1.0304007530212402
Epoch 950, training loss: 6.096739768981934 = 0.03339896351099014 + 1.0 * 6.063340663909912
Epoch 950, val loss: 1.0367149114608765
Epoch 960, training loss: 6.087515830993652 = 0.03229688107967377 + 1.0 * 6.0552191734313965
Epoch 960, val loss: 1.0432038307189941
Epoch 970, training loss: 6.083468437194824 = 0.03125235065817833 + 1.0 * 6.052216053009033
Epoch 970, val loss: 1.0500134229660034
Epoch 980, training loss: 6.083254814147949 = 0.030250877141952515 + 1.0 * 6.053003787994385
Epoch 980, val loss: 1.0563757419586182
Epoch 990, training loss: 6.0837178230285645 = 0.02929808385670185 + 1.0 * 6.05441951751709
Epoch 990, val loss: 1.0624243021011353
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.307328224182129 = 1.9333932399749756 + 1.0 * 8.373934745788574
Epoch 0, val loss: 1.9205163717269897
Epoch 10, training loss: 10.297270774841309 = 1.923604965209961 + 1.0 * 8.373665809631348
Epoch 10, val loss: 1.911514401435852
Epoch 20, training loss: 10.282853126525879 = 1.9113013744354248 + 1.0 * 8.371551513671875
Epoch 20, val loss: 1.899882435798645
Epoch 30, training loss: 10.24860668182373 = 1.894118070602417 + 1.0 * 8.354488372802734
Epoch 30, val loss: 1.8835086822509766
Epoch 40, training loss: 10.112001419067383 = 1.871417760848999 + 1.0 * 8.240583419799805
Epoch 40, val loss: 1.8625569343566895
Epoch 50, training loss: 9.659271240234375 = 1.84621262550354 + 1.0 * 7.813058376312256
Epoch 50, val loss: 1.8404021263122559
Epoch 60, training loss: 9.207764625549316 = 1.8259365558624268 + 1.0 * 7.381828308105469
Epoch 60, val loss: 1.8238587379455566
Epoch 70, training loss: 8.743759155273438 = 1.8094388246536255 + 1.0 * 6.934319972991943
Epoch 70, val loss: 1.809908151626587
Epoch 80, training loss: 8.525826454162598 = 1.7935513257980347 + 1.0 * 6.732275009155273
Epoch 80, val loss: 1.7965314388275146
Epoch 90, training loss: 8.405288696289062 = 1.775608777999878 + 1.0 * 6.629680156707764
Epoch 90, val loss: 1.7822080850601196
Epoch 100, training loss: 8.31660270690918 = 1.756514072418213 + 1.0 * 6.560088157653809
Epoch 100, val loss: 1.7674670219421387
Epoch 110, training loss: 8.244783401489258 = 1.7365890741348267 + 1.0 * 6.508194446563721
Epoch 110, val loss: 1.7519230842590332
Epoch 120, training loss: 8.179749488830566 = 1.7151559591293335 + 1.0 * 6.464593410491943
Epoch 120, val loss: 1.7349570989608765
Epoch 130, training loss: 8.120584487915039 = 1.6910384893417358 + 1.0 * 6.429545879364014
Epoch 130, val loss: 1.7158454656600952
Epoch 140, training loss: 8.057653427124023 = 1.6638890504837036 + 1.0 * 6.393764019012451
Epoch 140, val loss: 1.6943159103393555
Epoch 150, training loss: 7.994501113891602 = 1.6326978206634521 + 1.0 * 6.36180305480957
Epoch 150, val loss: 1.6695119142532349
Epoch 160, training loss: 7.9306111335754395 = 1.5965385437011719 + 1.0 * 6.334072589874268
Epoch 160, val loss: 1.640690565109253
Epoch 170, training loss: 7.866206169128418 = 1.5547195672988892 + 1.0 * 6.311486721038818
Epoch 170, val loss: 1.6072834730148315
Epoch 180, training loss: 7.800329208374023 = 1.5071483850479126 + 1.0 * 6.2931809425354
Epoch 180, val loss: 1.5691514015197754
Epoch 190, training loss: 7.733929634094238 = 1.4535133838653564 + 1.0 * 6.280416011810303
Epoch 190, val loss: 1.5260480642318726
Epoch 200, training loss: 7.660247325897217 = 1.3959145545959473 + 1.0 * 6.2643327713012695
Epoch 200, val loss: 1.4799519777297974
Epoch 210, training loss: 7.586071014404297 = 1.3353185653686523 + 1.0 * 6.2507524490356445
Epoch 210, val loss: 1.4317200183868408
Epoch 220, training loss: 7.510774612426758 = 1.2722280025482178 + 1.0 * 6.238546848297119
Epoch 220, val loss: 1.3818025588989258
Epoch 230, training loss: 7.437411308288574 = 1.2078996896743774 + 1.0 * 6.229511737823486
Epoch 230, val loss: 1.3314510583877563
Epoch 240, training loss: 7.366067886352539 = 1.1445375680923462 + 1.0 * 6.221530437469482
Epoch 240, val loss: 1.2828023433685303
Epoch 250, training loss: 7.300202369689941 = 1.0834263563156128 + 1.0 * 6.216775894165039
Epoch 250, val loss: 1.2364875078201294
Epoch 260, training loss: 7.231794357299805 = 1.0255321264266968 + 1.0 * 6.206262111663818
Epoch 260, val loss: 1.1934030055999756
Epoch 270, training loss: 7.168849468231201 = 0.9706390500068665 + 1.0 * 6.1982102394104
Epoch 270, val loss: 1.153222918510437
Epoch 280, training loss: 7.110139846801758 = 0.9186536073684692 + 1.0 * 6.191486358642578
Epoch 280, val loss: 1.1156936883926392
Epoch 290, training loss: 7.055337905883789 = 0.8696509003639221 + 1.0 * 6.185687065124512
Epoch 290, val loss: 1.080675721168518
Epoch 300, training loss: 7.0031938552856445 = 0.8228821754455566 + 1.0 * 6.180311679840088
Epoch 300, val loss: 1.0477240085601807
Epoch 310, training loss: 6.953069686889648 = 0.7785329818725586 + 1.0 * 6.17453670501709
Epoch 310, val loss: 1.0168848037719727
Epoch 320, training loss: 6.910204887390137 = 0.7368715405464172 + 1.0 * 6.173333168029785
Epoch 320, val loss: 0.988413393497467
Epoch 330, training loss: 6.864461898803711 = 0.6979196071624756 + 1.0 * 6.166542053222656
Epoch 330, val loss: 0.9622304439544678
Epoch 340, training loss: 6.822134494781494 = 0.6611122488975525 + 1.0 * 6.161022186279297
Epoch 340, val loss: 0.9380577206611633
Epoch 350, training loss: 6.784717082977295 = 0.6262000203132629 + 1.0 * 6.158516883850098
Epoch 350, val loss: 0.9157480597496033
Epoch 360, training loss: 6.748511791229248 = 0.5932031273841858 + 1.0 * 6.155308723449707
Epoch 360, val loss: 0.8954073786735535
Epoch 370, training loss: 6.712510108947754 = 0.5622230768203735 + 1.0 * 6.15028715133667
Epoch 370, val loss: 0.8771138787269592
Epoch 380, training loss: 6.679620742797852 = 0.5328142046928406 + 1.0 * 6.146806716918945
Epoch 380, val loss: 0.860630989074707
Epoch 390, training loss: 6.64758825302124 = 0.504693329334259 + 1.0 * 6.142894744873047
Epoch 390, val loss: 0.8456704616546631
Epoch 400, training loss: 6.619027137756348 = 0.477937251329422 + 1.0 * 6.141089916229248
Epoch 400, val loss: 0.8322367072105408
Epoch 410, training loss: 6.597414493560791 = 0.4526127874851227 + 1.0 * 6.144801616668701
Epoch 410, val loss: 0.820260763168335
Epoch 420, training loss: 6.563140392303467 = 0.4284765422344208 + 1.0 * 6.134664058685303
Epoch 420, val loss: 0.8096302151679993
Epoch 430, training loss: 6.536059379577637 = 0.4052789509296417 + 1.0 * 6.130780220031738
Epoch 430, val loss: 0.8000865578651428
Epoch 440, training loss: 6.510430335998535 = 0.38280582427978516 + 1.0 * 6.12762451171875
Epoch 440, val loss: 0.7914165258407593
Epoch 450, training loss: 6.496016025543213 = 0.36095577478408813 + 1.0 * 6.1350603103637695
Epoch 450, val loss: 0.7835544943809509
Epoch 460, training loss: 6.46381139755249 = 0.3400193750858307 + 1.0 * 6.1237921714782715
Epoch 460, val loss: 0.7764961123466492
Epoch 470, training loss: 6.4426188468933105 = 0.3197954595088959 + 1.0 * 6.122823238372803
Epoch 470, val loss: 0.7701797485351562
Epoch 480, training loss: 6.419638156890869 = 0.3001759648323059 + 1.0 * 6.119462013244629
Epoch 480, val loss: 0.764478862285614
Epoch 490, training loss: 6.398502349853516 = 0.28122466802597046 + 1.0 * 6.1172776222229
Epoch 490, val loss: 0.7594242691993713
Epoch 500, training loss: 6.379621505737305 = 0.2631036937236786 + 1.0 * 6.116518020629883
Epoch 500, val loss: 0.7551281452178955
Epoch 510, training loss: 6.359404563903809 = 0.2458081990480423 + 1.0 * 6.113596439361572
Epoch 510, val loss: 0.7515449523925781
Epoch 520, training loss: 6.33914041519165 = 0.22945155203342438 + 1.0 * 6.109688758850098
Epoch 520, val loss: 0.7487602829933167
Epoch 530, training loss: 6.324574947357178 = 0.2140994817018509 + 1.0 * 6.110475540161133
Epoch 530, val loss: 0.746835470199585
Epoch 540, training loss: 6.310490131378174 = 0.1998370885848999 + 1.0 * 6.110652923583984
Epoch 540, val loss: 0.7458013892173767
Epoch 550, training loss: 6.290699481964111 = 0.18669913709163666 + 1.0 * 6.104000568389893
Epoch 550, val loss: 0.7456834316253662
Epoch 560, training loss: 6.277429103851318 = 0.17460893094539642 + 1.0 * 6.10282039642334
Epoch 560, val loss: 0.7463890314102173
Epoch 570, training loss: 6.265195369720459 = 0.16354134678840637 + 1.0 * 6.101654052734375
Epoch 570, val loss: 0.7479154467582703
Epoch 580, training loss: 6.251636028289795 = 0.15337471663951874 + 1.0 * 6.09826135635376
Epoch 580, val loss: 0.7501545548439026
Epoch 590, training loss: 6.249869346618652 = 0.14401431381702423 + 1.0 * 6.1058549880981445
Epoch 590, val loss: 0.7530899047851562
Epoch 600, training loss: 6.232224941253662 = 0.1354525238275528 + 1.0 * 6.096772193908691
Epoch 600, val loss: 0.7566164135932922
Epoch 610, training loss: 6.223196983337402 = 0.12759049236774445 + 1.0 * 6.095606327056885
Epoch 610, val loss: 0.7606711387634277
Epoch 620, training loss: 6.212428092956543 = 0.12032024562358856 + 1.0 * 6.092107772827148
Epoch 620, val loss: 0.7651826739311218
Epoch 630, training loss: 6.213620662689209 = 0.1135842427611351 + 1.0 * 6.10003662109375
Epoch 630, val loss: 0.770141065120697
Epoch 640, training loss: 6.201838970184326 = 0.10740963369607925 + 1.0 * 6.0944294929504395
Epoch 640, val loss: 0.7754591107368469
Epoch 650, training loss: 6.190850734710693 = 0.10166218876838684 + 1.0 * 6.089188575744629
Epoch 650, val loss: 0.7811195850372314
Epoch 660, training loss: 6.182894706726074 = 0.09633754193782806 + 1.0 * 6.086557388305664
Epoch 660, val loss: 0.7870335578918457
Epoch 670, training loss: 6.190849781036377 = 0.0913674384355545 + 1.0 * 6.099482536315918
Epoch 670, val loss: 0.7932013869285583
Epoch 680, training loss: 6.171473026275635 = 0.08675874769687653 + 1.0 * 6.084714412689209
Epoch 680, val loss: 0.7994549870491028
Epoch 690, training loss: 6.167403697967529 = 0.08247331529855728 + 1.0 * 6.084930419921875
Epoch 690, val loss: 0.8059475421905518
Epoch 700, training loss: 6.159902095794678 = 0.07846257090568542 + 1.0 * 6.08143949508667
Epoch 700, val loss: 0.8124951720237732
Epoch 710, training loss: 6.1545610427856445 = 0.07470757514238358 + 1.0 * 6.079853534698486
Epoch 710, val loss: 0.8192163109779358
Epoch 720, training loss: 6.150515556335449 = 0.07118022441864014 + 1.0 * 6.0793352127075195
Epoch 720, val loss: 0.8260838389396667
Epoch 730, training loss: 6.166131496429443 = 0.06785859912633896 + 1.0 * 6.098272800445557
Epoch 730, val loss: 0.8329753279685974
Epoch 740, training loss: 6.144096851348877 = 0.06478364020586014 + 1.0 * 6.079313278198242
Epoch 740, val loss: 0.8399312496185303
Epoch 750, training loss: 6.137304782867432 = 0.0618838369846344 + 1.0 * 6.07542085647583
Epoch 750, val loss: 0.8469074368476868
Epoch 760, training loss: 6.133556842803955 = 0.05914674699306488 + 1.0 * 6.0744099617004395
Epoch 760, val loss: 0.8539186120033264
Epoch 770, training loss: 6.129062652587891 = 0.05656444653868675 + 1.0 * 6.072498321533203
Epoch 770, val loss: 0.8610655665397644
Epoch 780, training loss: 6.128139495849609 = 0.05411992222070694 + 1.0 * 6.074019432067871
Epoch 780, val loss: 0.8682236671447754
Epoch 790, training loss: 6.127690315246582 = 0.05181315541267395 + 1.0 * 6.0758771896362305
Epoch 790, val loss: 0.8753120303153992
Epoch 800, training loss: 6.119200229644775 = 0.04964780434966087 + 1.0 * 6.069552421569824
Epoch 800, val loss: 0.8823555707931519
Epoch 810, training loss: 6.116273880004883 = 0.04760121926665306 + 1.0 * 6.0686726570129395
Epoch 810, val loss: 0.8893815875053406
Epoch 820, training loss: 6.1207499504089355 = 0.045658938586711884 + 1.0 * 6.0750908851623535
Epoch 820, val loss: 0.8964009284973145
Epoch 830, training loss: 6.1200456619262695 = 0.04383038356900215 + 1.0 * 6.0762152671813965
Epoch 830, val loss: 0.9034332633018494
Epoch 840, training loss: 6.109573841094971 = 0.04210776090621948 + 1.0 * 6.0674662590026855
Epoch 840, val loss: 0.9103670716285706
Epoch 850, training loss: 6.106166362762451 = 0.04047384485602379 + 1.0 * 6.06569242477417
Epoch 850, val loss: 0.917262077331543
Epoch 860, training loss: 6.109611988067627 = 0.03892294690012932 + 1.0 * 6.0706892013549805
Epoch 860, val loss: 0.9241020083427429
Epoch 870, training loss: 6.102491855621338 = 0.037460654973983765 + 1.0 * 6.065031051635742
Epoch 870, val loss: 0.9309250712394714
Epoch 880, training loss: 6.100274085998535 = 0.03606589511036873 + 1.0 * 6.064208030700684
Epoch 880, val loss: 0.9376736283302307
Epoch 890, training loss: 6.101895332336426 = 0.034745123237371445 + 1.0 * 6.067150115966797
Epoch 890, val loss: 0.9444053173065186
Epoch 900, training loss: 6.097863674163818 = 0.033485472202301025 + 1.0 * 6.064378261566162
Epoch 900, val loss: 0.9510132670402527
Epoch 910, training loss: 6.092422962188721 = 0.032297924160957336 + 1.0 * 6.06012487411499
Epoch 910, val loss: 0.9575438499450684
Epoch 920, training loss: 6.089972972869873 = 0.031164679676294327 + 1.0 * 6.058808326721191
Epoch 920, val loss: 0.9640321135520935
Epoch 930, training loss: 6.089816093444824 = 0.030085882171988487 + 1.0 * 6.059730052947998
Epoch 930, val loss: 0.9704917669296265
Epoch 940, training loss: 6.088988304138184 = 0.02905873954296112 + 1.0 * 6.059929370880127
Epoch 940, val loss: 0.9767868518829346
Epoch 950, training loss: 6.084821701049805 = 0.02808484621345997 + 1.0 * 6.056736946105957
Epoch 950, val loss: 0.9830783605575562
Epoch 960, training loss: 6.084277153015137 = 0.027154533192515373 + 1.0 * 6.057122707366943
Epoch 960, val loss: 0.9892410635948181
Epoch 970, training loss: 6.084298610687256 = 0.026267550885677338 + 1.0 * 6.05803108215332
Epoch 970, val loss: 0.9953575134277344
Epoch 980, training loss: 6.0814714431762695 = 0.025424214079976082 + 1.0 * 6.056047439575195
Epoch 980, val loss: 1.0013818740844727
Epoch 990, training loss: 6.085084915161133 = 0.024617252871394157 + 1.0 * 6.060467720031738
Epoch 990, val loss: 1.0073782205581665
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.345195770263672 = 1.9713844060897827 + 1.0 * 8.373811721801758
Epoch 0, val loss: 1.967545747756958
Epoch 10, training loss: 10.333236694335938 = 1.9599474668502808 + 1.0 * 8.373289108276367
Epoch 10, val loss: 1.9568756818771362
Epoch 20, training loss: 10.315912246704102 = 1.9458301067352295 + 1.0 * 8.370081901550293
Epoch 20, val loss: 1.943283200263977
Epoch 30, training loss: 10.27465534210205 = 1.926390290260315 + 1.0 * 8.348264694213867
Epoch 30, val loss: 1.9241995811462402
Epoch 40, training loss: 10.109161376953125 = 1.9017037153244019 + 1.0 * 8.207457542419434
Epoch 40, val loss: 1.9009959697723389
Epoch 50, training loss: 9.569849014282227 = 1.8753552436828613 + 1.0 * 7.694493770599365
Epoch 50, val loss: 1.876315951347351
Epoch 60, training loss: 9.17332649230957 = 1.8530120849609375 + 1.0 * 7.320313930511475
Epoch 60, val loss: 1.8565325736999512
Epoch 70, training loss: 8.716962814331055 = 1.8390705585479736 + 1.0 * 6.877892017364502
Epoch 70, val loss: 1.84489905834198
Epoch 80, training loss: 8.505603790283203 = 1.8246631622314453 + 1.0 * 6.680941104888916
Epoch 80, val loss: 1.8326468467712402
Epoch 90, training loss: 8.377889633178711 = 1.8094438314437866 + 1.0 * 6.568445682525635
Epoch 90, val loss: 1.8195106983184814
Epoch 100, training loss: 8.27556037902832 = 1.7948246002197266 + 1.0 * 6.480735778808594
Epoch 100, val loss: 1.8072593212127686
Epoch 110, training loss: 8.198301315307617 = 1.7824442386627197 + 1.0 * 6.415856838226318
Epoch 110, val loss: 1.7967612743377686
Epoch 120, training loss: 8.139633178710938 = 1.7712117433547974 + 1.0 * 6.36842155456543
Epoch 120, val loss: 1.7867779731750488
Epoch 130, training loss: 8.088239669799805 = 1.759211778640747 + 1.0 * 6.329028129577637
Epoch 130, val loss: 1.7761282920837402
Epoch 140, training loss: 8.042898178100586 = 1.7454276084899902 + 1.0 * 6.297471046447754
Epoch 140, val loss: 1.7644017934799194
Epoch 150, training loss: 8.002710342407227 = 1.7295079231262207 + 1.0 * 6.273201942443848
Epoch 150, val loss: 1.751375436782837
Epoch 160, training loss: 7.964577674865723 = 1.7105858325958252 + 1.0 * 6.253991603851318
Epoch 160, val loss: 1.7362102270126343
Epoch 170, training loss: 7.927760124206543 = 1.687816858291626 + 1.0 * 6.239943027496338
Epoch 170, val loss: 1.7180209159851074
Epoch 180, training loss: 7.885228157043457 = 1.6605899333953857 + 1.0 * 6.22463846206665
Epoch 180, val loss: 1.6963011026382446
Epoch 190, training loss: 7.840304374694824 = 1.6277344226837158 + 1.0 * 6.212569713592529
Epoch 190, val loss: 1.6699999570846558
Epoch 200, training loss: 7.790415287017822 = 1.5887818336486816 + 1.0 * 6.201633453369141
Epoch 200, val loss: 1.63888680934906
Epoch 210, training loss: 7.737886428833008 = 1.5443600416183472 + 1.0 * 6.193526268005371
Epoch 210, val loss: 1.603521466255188
Epoch 220, training loss: 7.680734157562256 = 1.496533989906311 + 1.0 * 6.184200286865234
Epoch 220, val loss: 1.5655254125595093
Epoch 230, training loss: 7.623370170593262 = 1.446260690689087 + 1.0 * 6.177109718322754
Epoch 230, val loss: 1.5257188081741333
Epoch 240, training loss: 7.566126823425293 = 1.3951456546783447 + 1.0 * 6.170980930328369
Epoch 240, val loss: 1.4857193231582642
Epoch 250, training loss: 7.509255886077881 = 1.3453317880630493 + 1.0 * 6.163924217224121
Epoch 250, val loss: 1.4474953413009644
Epoch 260, training loss: 7.455585479736328 = 1.2971352338790894 + 1.0 * 6.158450126647949
Epoch 260, val loss: 1.410797357559204
Epoch 270, training loss: 7.403246879577637 = 1.2499090433120728 + 1.0 * 6.1533379554748535
Epoch 270, val loss: 1.3752555847167969
Epoch 280, training loss: 7.352750778198242 = 1.203749418258667 + 1.0 * 6.149001598358154
Epoch 280, val loss: 1.340666651725769
Epoch 290, training loss: 7.300578594207764 = 1.158375859260559 + 1.0 * 6.142202854156494
Epoch 290, val loss: 1.3069515228271484
Epoch 300, training loss: 7.254199028015137 = 1.1133763790130615 + 1.0 * 6.140822887420654
Epoch 300, val loss: 1.2737493515014648
Epoch 310, training loss: 7.203536510467529 = 1.0694985389709473 + 1.0 * 6.134037971496582
Epoch 310, val loss: 1.2414400577545166
Epoch 320, training loss: 7.156525135040283 = 1.0268136262893677 + 1.0 * 6.129711627960205
Epoch 320, val loss: 1.2102437019348145
Epoch 330, training loss: 7.115772247314453 = 0.9852111339569092 + 1.0 * 6.130560874938965
Epoch 330, val loss: 1.179923176765442
Epoch 340, training loss: 7.068686485290527 = 0.9453074932098389 + 1.0 * 6.123379230499268
Epoch 340, val loss: 1.1508705615997314
Epoch 350, training loss: 7.027130603790283 = 0.9066045880317688 + 1.0 * 6.12052583694458
Epoch 350, val loss: 1.122854471206665
Epoch 360, training loss: 6.989716053009033 = 0.8685712814331055 + 1.0 * 6.121144771575928
Epoch 360, val loss: 1.0952463150024414
Epoch 370, training loss: 6.9454827308654785 = 0.8311675190925598 + 1.0 * 6.114315032958984
Epoch 370, val loss: 1.0679984092712402
Epoch 380, training loss: 6.914412975311279 = 0.7939397096633911 + 1.0 * 6.120473384857178
Epoch 380, val loss: 1.040714144706726
Epoch 390, training loss: 6.869128704071045 = 0.7570868134498596 + 1.0 * 6.11204195022583
Epoch 390, val loss: 1.013784408569336
Epoch 400, training loss: 6.828740119934082 = 0.720372200012207 + 1.0 * 6.108367919921875
Epoch 400, val loss: 0.9869967699050903
Epoch 410, training loss: 6.791442394256592 = 0.6836861371994019 + 1.0 * 6.1077561378479
Epoch 410, val loss: 0.9602034687995911
Epoch 420, training loss: 6.752625465393066 = 0.6477854251861572 + 1.0 * 6.104840278625488
Epoch 420, val loss: 0.9339174628257751
Epoch 430, training loss: 6.716202735900879 = 0.6128764152526855 + 1.0 * 6.103326320648193
Epoch 430, val loss: 0.908586859703064
Epoch 440, training loss: 6.679316520690918 = 0.579193651676178 + 1.0 * 6.100122928619385
Epoch 440, val loss: 0.8841315507888794
Epoch 450, training loss: 6.644952297210693 = 0.5471013188362122 + 1.0 * 6.097850799560547
Epoch 450, val loss: 0.8610416650772095
Epoch 460, training loss: 6.6141228675842285 = 0.5169649720191956 + 1.0 * 6.097157955169678
Epoch 460, val loss: 0.8396973013877869
Epoch 470, training loss: 6.582865238189697 = 0.48888450860977173 + 1.0 * 6.09398078918457
Epoch 470, val loss: 0.8201959729194641
Epoch 480, training loss: 6.555934906005859 = 0.4627126455307007 + 1.0 * 6.093222141265869
Epoch 480, val loss: 0.8025603890419006
Epoch 490, training loss: 6.527524471282959 = 0.4384208619594574 + 1.0 * 6.089103698730469
Epoch 490, val loss: 0.7867808938026428
Epoch 500, training loss: 6.5076823234558105 = 0.41547852754592896 + 1.0 * 6.092203617095947
Epoch 500, val loss: 0.7724464535713196
Epoch 510, training loss: 6.479691982269287 = 0.3936219811439514 + 1.0 * 6.0860700607299805
Epoch 510, val loss: 0.7592730522155762
Epoch 520, training loss: 6.457188606262207 = 0.37239229679107666 + 1.0 * 6.08479642868042
Epoch 520, val loss: 0.7469204664230347
Epoch 530, training loss: 6.434328556060791 = 0.35143643617630005 + 1.0 * 6.082891941070557
Epoch 530, val loss: 0.7350576519966125
Epoch 540, training loss: 6.411337852478027 = 0.33067435026168823 + 1.0 * 6.080663681030273
Epoch 540, val loss: 0.7235653400421143
Epoch 550, training loss: 6.391895771026611 = 0.3101061284542084 + 1.0 * 6.081789493560791
Epoch 550, val loss: 0.7125850915908813
Epoch 560, training loss: 6.369657039642334 = 0.2897418439388275 + 1.0 * 6.0799150466918945
Epoch 560, val loss: 0.7020953297615051
Epoch 570, training loss: 6.346042156219482 = 0.2697790265083313 + 1.0 * 6.076262950897217
Epoch 570, val loss: 0.6920797824859619
Epoch 580, training loss: 6.3266096115112305 = 0.25039002299308777 + 1.0 * 6.07621955871582
Epoch 580, val loss: 0.6828017830848694
Epoch 590, training loss: 6.306527614593506 = 0.23181749880313873 + 1.0 * 6.074709892272949
Epoch 590, val loss: 0.6743448376655579
Epoch 600, training loss: 6.289282321929932 = 0.2143082469701767 + 1.0 * 6.074974060058594
Epoch 600, val loss: 0.6669286489486694
Epoch 610, training loss: 6.269282817840576 = 0.1979924440383911 + 1.0 * 6.071290493011475
Epoch 610, val loss: 0.6605607867240906
Epoch 620, training loss: 6.252989292144775 = 0.1828581988811493 + 1.0 * 6.070131301879883
Epoch 620, val loss: 0.655195415019989
Epoch 630, training loss: 6.239185333251953 = 0.16900210082530975 + 1.0 * 6.070183277130127
Epoch 630, val loss: 0.6507595181465149
Epoch 640, training loss: 6.224647045135498 = 0.15642771124839783 + 1.0 * 6.068219184875488
Epoch 640, val loss: 0.6474670171737671
Epoch 650, training loss: 6.221301555633545 = 0.145003542304039 + 1.0 * 6.076298236846924
Epoch 650, val loss: 0.6450833082199097
Epoch 660, training loss: 6.20115327835083 = 0.1347329169511795 + 1.0 * 6.066420555114746
Epoch 660, val loss: 0.6435095071792603
Epoch 670, training loss: 6.1886515617370605 = 0.1254575401544571 + 1.0 * 6.0631937980651855
Epoch 670, val loss: 0.642927885055542
Epoch 680, training loss: 6.181751251220703 = 0.11704003810882568 + 1.0 * 6.064711093902588
Epoch 680, val loss: 0.6430840492248535
Epoch 690, training loss: 6.171064376831055 = 0.10942720621824265 + 1.0 * 6.0616374015808105
Epoch 690, val loss: 0.6438528895378113
Epoch 700, training loss: 6.163727760314941 = 0.1025409922003746 + 1.0 * 6.061186790466309
Epoch 700, val loss: 0.6452774405479431
Epoch 710, training loss: 6.1550092697143555 = 0.09628034383058548 + 1.0 * 6.0587286949157715
Epoch 710, val loss: 0.6472545266151428
Epoch 720, training loss: 6.151893138885498 = 0.09057516604661942 + 1.0 * 6.0613179206848145
Epoch 720, val loss: 0.6495847105979919
Epoch 730, training loss: 6.145077705383301 = 0.08540350198745728 + 1.0 * 6.059674263000488
Epoch 730, val loss: 0.6522426009178162
Epoch 740, training loss: 6.139312744140625 = 0.08067429065704346 + 1.0 * 6.058638572692871
Epoch 740, val loss: 0.6552717089653015
Epoch 750, training loss: 6.131412982940674 = 0.07633714377880096 + 1.0 * 6.055075645446777
Epoch 750, val loss: 0.6584980487823486
Epoch 760, training loss: 6.1261372566223145 = 0.07233328372240067 + 1.0 * 6.05380392074585
Epoch 760, val loss: 0.6619856357574463
Epoch 770, training loss: 6.124450206756592 = 0.06863230466842651 + 1.0 * 6.0558180809021
Epoch 770, val loss: 0.6655703186988831
Epoch 780, training loss: 6.117833614349365 = 0.0652211457490921 + 1.0 * 6.0526123046875
Epoch 780, val loss: 0.6693070530891418
Epoch 790, training loss: 6.113068103790283 = 0.06205687299370766 + 1.0 * 6.051011085510254
Epoch 790, val loss: 0.6732378602027893
Epoch 800, training loss: 6.113775730133057 = 0.05910582095384598 + 1.0 * 6.0546698570251465
Epoch 800, val loss: 0.6772032976150513
Epoch 810, training loss: 6.105170249938965 = 0.056360259652137756 + 1.0 * 6.048810005187988
Epoch 810, val loss: 0.6812132000923157
Epoch 820, training loss: 6.105026721954346 = 0.053794488310813904 + 1.0 * 6.05123233795166
Epoch 820, val loss: 0.6853908896446228
Epoch 830, training loss: 6.105745315551758 = 0.051404982805252075 + 1.0 * 6.054340362548828
Epoch 830, val loss: 0.6894711256027222
Epoch 840, training loss: 6.094270706176758 = 0.04918678477406502 + 1.0 * 6.045083999633789
Epoch 840, val loss: 0.693632185459137
Epoch 850, training loss: 6.092255592346191 = 0.047104671597480774 + 1.0 * 6.0451507568359375
Epoch 850, val loss: 0.6978943943977356
Epoch 860, training loss: 6.088510036468506 = 0.045133061707019806 + 1.0 * 6.043376922607422
Epoch 860, val loss: 0.7020951509475708
Epoch 870, training loss: 6.094442367553711 = 0.04326923191547394 + 1.0 * 6.051173210144043
Epoch 870, val loss: 0.7062894105911255
Epoch 880, training loss: 6.090054512023926 = 0.041520461440086365 + 1.0 * 6.048533916473389
Epoch 880, val loss: 0.7104805111885071
Epoch 890, training loss: 6.0819220542907715 = 0.0398796871304512 + 1.0 * 6.042042255401611
Epoch 890, val loss: 0.7147015333175659
Epoch 900, training loss: 6.078717231750488 = 0.03833454102277756 + 1.0 * 6.0403828620910645
Epoch 900, val loss: 0.7189092040061951
Epoch 910, training loss: 6.078675270080566 = 0.03686738759279251 + 1.0 * 6.041807651519775
Epoch 910, val loss: 0.7230716347694397
Epoch 920, training loss: 6.07681941986084 = 0.03547929599881172 + 1.0 * 6.041340351104736
Epoch 920, val loss: 0.727114200592041
Epoch 930, training loss: 6.072393894195557 = 0.03417721390724182 + 1.0 * 6.038216590881348
Epoch 930, val loss: 0.7312473058700562
Epoch 940, training loss: 6.070955276489258 = 0.03294362872838974 + 1.0 * 6.03801155090332
Epoch 940, val loss: 0.7353964447975159
Epoch 950, training loss: 6.074958324432373 = 0.031767476350069046 + 1.0 * 6.043190956115723
Epoch 950, val loss: 0.7394476532936096
Epoch 960, training loss: 6.070106506347656 = 0.030654780566692352 + 1.0 * 6.039451599121094
Epoch 960, val loss: 0.7434291839599609
Epoch 970, training loss: 6.069537162780762 = 0.02960696630179882 + 1.0 * 6.03993034362793
Epoch 970, val loss: 0.7474742531776428
Epoch 980, training loss: 6.065887451171875 = 0.028612658381462097 + 1.0 * 6.0372748374938965
Epoch 980, val loss: 0.751468300819397
Epoch 990, training loss: 6.062840461730957 = 0.027665605768561363 + 1.0 * 6.03517484664917
Epoch 990, val loss: 0.7553644180297852
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.318772315979004 = 1.9449856281280518 + 1.0 * 8.373786926269531
Epoch 0, val loss: 1.9396685361862183
Epoch 10, training loss: 10.308147430419922 = 1.9347898960113525 + 1.0 * 8.373357772827148
Epoch 10, val loss: 1.9298955202102661
Epoch 20, training loss: 10.293127059936523 = 1.9224703311920166 + 1.0 * 8.370656967163086
Epoch 20, val loss: 1.9181550741195679
Epoch 30, training loss: 10.257603645324707 = 1.9054158926010132 + 1.0 * 8.352188110351562
Epoch 30, val loss: 1.9019994735717773
Epoch 40, training loss: 10.101006507873535 = 1.8829656839370728 + 1.0 * 8.218040466308594
Epoch 40, val loss: 1.8813532590866089
Epoch 50, training loss: 9.399913787841797 = 1.8583637475967407 + 1.0 * 7.5415496826171875
Epoch 50, val loss: 1.8586333990097046
Epoch 60, training loss: 8.937113761901855 = 1.8368619680404663 + 1.0 * 7.1002516746521
Epoch 60, val loss: 1.8392175436019897
Epoch 70, training loss: 8.633211135864258 = 1.821803331375122 + 1.0 * 6.811407566070557
Epoch 70, val loss: 1.824326753616333
Epoch 80, training loss: 8.435208320617676 = 1.807864785194397 + 1.0 * 6.62734317779541
Epoch 80, val loss: 1.811531662940979
Epoch 90, training loss: 8.344327926635742 = 1.79475736618042 + 1.0 * 6.549570083618164
Epoch 90, val loss: 1.7999070882797241
Epoch 100, training loss: 8.267552375793457 = 1.7812693119049072 + 1.0 * 6.486283302307129
Epoch 100, val loss: 1.7882219552993774
Epoch 110, training loss: 8.205443382263184 = 1.768393635749817 + 1.0 * 6.437049865722656
Epoch 110, val loss: 1.7768898010253906
Epoch 120, training loss: 8.14506721496582 = 1.7550528049468994 + 1.0 * 6.3900146484375
Epoch 120, val loss: 1.7653446197509766
Epoch 130, training loss: 8.089794158935547 = 1.7405118942260742 + 1.0 * 6.349282264709473
Epoch 130, val loss: 1.7529569864273071
Epoch 140, training loss: 8.044515609741211 = 1.7233482599258423 + 1.0 * 6.3211669921875
Epoch 140, val loss: 1.7385472059249878
Epoch 150, training loss: 8.000652313232422 = 1.7020869255065918 + 1.0 * 6.298565864562988
Epoch 150, val loss: 1.720831036567688
Epoch 160, training loss: 7.955025672912598 = 1.675663709640503 + 1.0 * 6.279361724853516
Epoch 160, val loss: 1.698941707611084
Epoch 170, training loss: 7.905959606170654 = 1.6434139013290405 + 1.0 * 6.262545585632324
Epoch 170, val loss: 1.6725611686706543
Epoch 180, training loss: 7.851709365844727 = 1.6040446758270264 + 1.0 * 6.247664928436279
Epoch 180, val loss: 1.6402347087860107
Epoch 190, training loss: 7.790315628051758 = 1.5559775829315186 + 1.0 * 6.23433780670166
Epoch 190, val loss: 1.600493311882019
Epoch 200, training loss: 7.721973419189453 = 1.4996002912521362 + 1.0 * 6.222373008728027
Epoch 200, val loss: 1.554193139076233
Epoch 210, training loss: 7.648022651672363 = 1.43621826171875 + 1.0 * 6.211804389953613
Epoch 210, val loss: 1.5018953084945679
Epoch 220, training loss: 7.572278022766113 = 1.3684674501419067 + 1.0 * 6.203810691833496
Epoch 220, val loss: 1.4459527730941772
Epoch 230, training loss: 7.494434356689453 = 1.2995449304580688 + 1.0 * 6.194889545440674
Epoch 230, val loss: 1.3892937898635864
Epoch 240, training loss: 7.418949127197266 = 1.231609582901001 + 1.0 * 6.1873393058776855
Epoch 240, val loss: 1.3335809707641602
Epoch 250, training loss: 7.348721981048584 = 1.1665242910385132 + 1.0 * 6.182197570800781
Epoch 250, val loss: 1.2802592515945435
Epoch 260, training loss: 7.279815673828125 = 1.106579065322876 + 1.0 * 6.17323637008667
Epoch 260, val loss: 1.2314369678497314
Epoch 270, training loss: 7.2186994552612305 = 1.0513644218444824 + 1.0 * 6.167335033416748
Epoch 270, val loss: 1.1872026920318604
Epoch 280, training loss: 7.16464900970459 = 1.0009729862213135 + 1.0 * 6.163675785064697
Epoch 280, val loss: 1.1476832628250122
Epoch 290, training loss: 7.110435485839844 = 0.95513916015625 + 1.0 * 6.155296325683594
Epoch 290, val loss: 1.1130554676055908
Epoch 300, training loss: 7.062353610992432 = 0.9126660227775574 + 1.0 * 6.149687767028809
Epoch 300, val loss: 1.0820536613464355
Epoch 310, training loss: 7.020787715911865 = 0.8732896447181702 + 1.0 * 6.14749813079834
Epoch 310, val loss: 1.054516077041626
Epoch 320, training loss: 6.977870941162109 = 0.8371004462242126 + 1.0 * 6.140770435333252
Epoch 320, val loss: 1.0304267406463623
Epoch 330, training loss: 6.9384846687316895 = 0.8029426336288452 + 1.0 * 6.135541915893555
Epoch 330, val loss: 1.0087082386016846
Epoch 340, training loss: 6.900944709777832 = 0.7701618075370789 + 1.0 * 6.1307830810546875
Epoch 340, val loss: 0.9884805083274841
Epoch 350, training loss: 6.867932319641113 = 0.7383838295936584 + 1.0 * 6.1295485496521
Epoch 350, val loss: 0.9695683121681213
Epoch 360, training loss: 6.835390567779541 = 0.7076060175895691 + 1.0 * 6.127784729003906
Epoch 360, val loss: 0.9519246220588684
Epoch 370, training loss: 6.798859596252441 = 0.6780114769935608 + 1.0 * 6.120848178863525
Epoch 370, val loss: 0.9354287385940552
Epoch 380, training loss: 6.765132904052734 = 0.6489689350128174 + 1.0 * 6.116163730621338
Epoch 380, val loss: 0.9195534586906433
Epoch 390, training loss: 6.73306941986084 = 0.6202397346496582 + 1.0 * 6.112829685211182
Epoch 390, val loss: 0.904166042804718
Epoch 400, training loss: 6.707006931304932 = 0.5919040441513062 + 1.0 * 6.115102767944336
Epoch 400, val loss: 0.8892320394515991
Epoch 410, training loss: 6.673971652984619 = 0.5644171833992004 + 1.0 * 6.109554290771484
Epoch 410, val loss: 0.8749053478240967
Epoch 420, training loss: 6.645585060119629 = 0.5377100110054016 + 1.0 * 6.107874870300293
Epoch 420, val loss: 0.8611982464790344
Epoch 430, training loss: 6.6181559562683105 = 0.512120246887207 + 1.0 * 6.1060357093811035
Epoch 430, val loss: 0.8482459783554077
Epoch 440, training loss: 6.588594436645508 = 0.4876595437526703 + 1.0 * 6.100934982299805
Epoch 440, val loss: 0.836412787437439
Epoch 450, training loss: 6.561830997467041 = 0.4641748368740082 + 1.0 * 6.09765625
Epoch 450, val loss: 0.8253064751625061
Epoch 460, training loss: 6.540384292602539 = 0.44183290004730225 + 1.0 * 6.098551273345947
Epoch 460, val loss: 0.8150238394737244
Epoch 470, training loss: 6.5167036056518555 = 0.4208693206310272 + 1.0 * 6.095834255218506
Epoch 470, val loss: 0.8059910535812378
Epoch 480, training loss: 6.49227237701416 = 0.4009382724761963 + 1.0 * 6.091334342956543
Epoch 480, val loss: 0.7976974248886108
Epoch 490, training loss: 6.473365306854248 = 0.38191351294517517 + 1.0 * 6.091451644897461
Epoch 490, val loss: 0.7901463508605957
Epoch 500, training loss: 6.4532952308654785 = 0.3638676106929779 + 1.0 * 6.089427471160889
Epoch 500, val loss: 0.7834910154342651
Epoch 510, training loss: 6.437239646911621 = 0.3467404246330261 + 1.0 * 6.090499401092529
Epoch 510, val loss: 0.777668833732605
Epoch 520, training loss: 6.416814804077148 = 0.3304350674152374 + 1.0 * 6.086379528045654
Epoch 520, val loss: 0.7722070217132568
Epoch 530, training loss: 6.398914813995361 = 0.31482475996017456 + 1.0 * 6.084090232849121
Epoch 530, val loss: 0.7674793601036072
Epoch 540, training loss: 6.386505126953125 = 0.2998269498348236 + 1.0 * 6.0866780281066895
Epoch 540, val loss: 0.7629162669181824
Epoch 550, training loss: 6.367119789123535 = 0.2854073941707611 + 1.0 * 6.081712245941162
Epoch 550, val loss: 0.7590928077697754
Epoch 560, training loss: 6.35024356842041 = 0.27131861448287964 + 1.0 * 6.078925132751465
Epoch 560, val loss: 0.7554194927215576
Epoch 570, training loss: 6.34373140335083 = 0.25748851895332336 + 1.0 * 6.08624267578125
Epoch 570, val loss: 0.7521237134933472
Epoch 580, training loss: 6.3206658363342285 = 0.2440415620803833 + 1.0 * 6.076624393463135
Epoch 580, val loss: 0.7491777539253235
Epoch 590, training loss: 6.306528568267822 = 0.2309342622756958 + 1.0 * 6.075594425201416
Epoch 590, val loss: 0.7467707395553589
Epoch 600, training loss: 6.300308704376221 = 0.2182074636220932 + 1.0 * 6.082101345062256
Epoch 600, val loss: 0.7447223663330078
Epoch 610, training loss: 6.281448841094971 = 0.20609186589717865 + 1.0 * 6.075356960296631
Epoch 610, val loss: 0.7432452440261841
Epoch 620, training loss: 6.2674736976623535 = 0.1946234405040741 + 1.0 * 6.072850227355957
Epoch 620, val loss: 0.7424123287200928
Epoch 630, training loss: 6.254271507263184 = 0.18380120396614075 + 1.0 * 6.070470333099365
Epoch 630, val loss: 0.7421819567680359
Epoch 640, training loss: 6.241753101348877 = 0.1736183613538742 + 1.0 * 6.068134784698486
Epoch 640, val loss: 0.7425190210342407
Epoch 650, training loss: 6.241185188293457 = 0.16407419741153717 + 1.0 * 6.077110767364502
Epoch 650, val loss: 0.7434089183807373
Epoch 660, training loss: 6.2219624519348145 = 0.15525954961776733 + 1.0 * 6.066702842712402
Epoch 660, val loss: 0.744804322719574
Epoch 670, training loss: 6.215957164764404 = 0.14706555008888245 + 1.0 * 6.068891525268555
Epoch 670, val loss: 0.7467954754829407
Epoch 680, training loss: 6.205243110656738 = 0.13943743705749512 + 1.0 * 6.065805912017822
Epoch 680, val loss: 0.7488738894462585
Epoch 690, training loss: 6.19659423828125 = 0.1323011964559555 + 1.0 * 6.064292907714844
Epoch 690, val loss: 0.7516325116157532
Epoch 700, training loss: 6.188552379608154 = 0.12558192014694214 + 1.0 * 6.0629706382751465
Epoch 700, val loss: 0.754546046257019
Epoch 710, training loss: 6.185282230377197 = 0.11926911026239395 + 1.0 * 6.066013336181641
Epoch 710, val loss: 0.7576950788497925
Epoch 720, training loss: 6.176758766174316 = 0.11337193846702576 + 1.0 * 6.063386917114258
Epoch 720, val loss: 0.7610668540000916
Epoch 730, training loss: 6.168407440185547 = 0.10784068703651428 + 1.0 * 6.0605669021606445
Epoch 730, val loss: 0.7648081183433533
Epoch 740, training loss: 6.160163879394531 = 0.10261120647192001 + 1.0 * 6.057552814483643
Epoch 740, val loss: 0.7687197923660278
Epoch 750, training loss: 6.156188488006592 = 0.0976606160402298 + 1.0 * 6.058527946472168
Epoch 750, val loss: 0.7728277444839478
Epoch 760, training loss: 6.149327278137207 = 0.0930013582110405 + 1.0 * 6.056325912475586
Epoch 760, val loss: 0.7769795656204224
Epoch 770, training loss: 6.145514011383057 = 0.08861833065748215 + 1.0 * 6.056895732879639
Epoch 770, val loss: 0.7815176844596863
Epoch 780, training loss: 6.1409430503845215 = 0.0844951942563057 + 1.0 * 6.056447982788086
Epoch 780, val loss: 0.7859361171722412
Epoch 790, training loss: 6.134521007537842 = 0.08061486482620239 + 1.0 * 6.053905963897705
Epoch 790, val loss: 0.7906433343887329
Epoch 800, training loss: 6.129570960998535 = 0.07694489508867264 + 1.0 * 6.052626132965088
Epoch 800, val loss: 0.7954846620559692
Epoch 810, training loss: 6.133798122406006 = 0.07346929609775543 + 1.0 * 6.060328960418701
Epoch 810, val loss: 0.8002095222473145
Epoch 820, training loss: 6.121302604675293 = 0.0702136754989624 + 1.0 * 6.051088809967041
Epoch 820, val loss: 0.8049633502960205
Epoch 830, training loss: 6.117086887359619 = 0.0671544075012207 + 1.0 * 6.049932479858398
Epoch 830, val loss: 0.8100941777229309
Epoch 840, training loss: 6.113418102264404 = 0.06425178050994873 + 1.0 * 6.049166202545166
Epoch 840, val loss: 0.8151760101318359
Epoch 850, training loss: 6.109632968902588 = 0.061499837785959244 + 1.0 * 6.04813289642334
Epoch 850, val loss: 0.8202673196792603
Epoch 860, training loss: 6.115804672241211 = 0.0588938407599926 + 1.0 * 6.056910991668701
Epoch 860, val loss: 0.8253650069236755
Epoch 870, training loss: 6.10920524597168 = 0.056447532027959824 + 1.0 * 6.052757740020752
Epoch 870, val loss: 0.8303589224815369
Epoch 880, training loss: 6.10087776184082 = 0.05414511263370514 + 1.0 * 6.046732425689697
Epoch 880, val loss: 0.8355614542961121
Epoch 890, training loss: 6.09699010848999 = 0.05196082592010498 + 1.0 * 6.045029163360596
Epoch 890, val loss: 0.8407704830169678
Epoch 900, training loss: 6.095640182495117 = 0.04988429322838783 + 1.0 * 6.045755863189697
Epoch 900, val loss: 0.8458977341651917
Epoch 910, training loss: 6.09291410446167 = 0.0479167178273201 + 1.0 * 6.044997215270996
Epoch 910, val loss: 0.8509160876274109
Epoch 920, training loss: 6.092560768127441 = 0.046061933040618896 + 1.0 * 6.046498775482178
Epoch 920, val loss: 0.8559079766273499
Epoch 930, training loss: 6.08776330947876 = 0.044318705797195435 + 1.0 * 6.043444633483887
Epoch 930, val loss: 0.8609768748283386
Epoch 940, training loss: 6.0841064453125 = 0.04266485571861267 + 1.0 * 6.041441440582275
Epoch 940, val loss: 0.8661187291145325
Epoch 950, training loss: 6.08373498916626 = 0.041084956377744675 + 1.0 * 6.04265022277832
Epoch 950, val loss: 0.8710784912109375
Epoch 960, training loss: 6.079467296600342 = 0.03958461061120033 + 1.0 * 6.039882659912109
Epoch 960, val loss: 0.8758435249328613
Epoch 970, training loss: 6.07724142074585 = 0.038166459649801254 + 1.0 * 6.039074897766113
Epoch 970, val loss: 0.8809173107147217
Epoch 980, training loss: 6.075340747833252 = 0.03681255504488945 + 1.0 * 6.038527965545654
Epoch 980, val loss: 0.8858680129051208
Epoch 990, training loss: 6.078338146209717 = 0.035521090030670166 + 1.0 * 6.042817115783691
Epoch 990, val loss: 0.890789806842804
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7343
Flip ASR: 0.6800/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.308521270751953 = 1.9347237348556519 + 1.0 * 8.373797416687012
Epoch 0, val loss: 1.9302393198013306
Epoch 10, training loss: 10.297950744628906 = 1.924615740776062 + 1.0 * 8.373334884643555
Epoch 10, val loss: 1.920369267463684
Epoch 20, training loss: 10.283308029174805 = 1.912529706954956 + 1.0 * 8.37077808380127
Epoch 20, val loss: 1.908172607421875
Epoch 30, training loss: 10.249648094177246 = 1.8961633443832397 + 1.0 * 8.353485107421875
Epoch 30, val loss: 1.8912094831466675
Epoch 40, training loss: 10.115846633911133 = 1.8754079341888428 + 1.0 * 8.240438461303711
Epoch 40, val loss: 1.8700662851333618
Epoch 50, training loss: 9.470535278320312 = 1.8542803525924683 + 1.0 * 7.616255283355713
Epoch 50, val loss: 1.8484607934951782
Epoch 60, training loss: 8.947197914123535 = 1.8348777294158936 + 1.0 * 7.112320423126221
Epoch 60, val loss: 1.8292157649993896
Epoch 70, training loss: 8.60719108581543 = 1.8206382989883423 + 1.0 * 6.786552429199219
Epoch 70, val loss: 1.81402587890625
Epoch 80, training loss: 8.47677230834961 = 1.8056936264038086 + 1.0 * 6.671079158782959
Epoch 80, val loss: 1.7985177040100098
Epoch 90, training loss: 8.364294052124023 = 1.7918715476989746 + 1.0 * 6.572422504425049
Epoch 90, val loss: 1.7845174074172974
Epoch 100, training loss: 8.263538360595703 = 1.778782844543457 + 1.0 * 6.484755992889404
Epoch 100, val loss: 1.7714495658874512
Epoch 110, training loss: 8.18886947631836 = 1.7661892175674438 + 1.0 * 6.422680377960205
Epoch 110, val loss: 1.7589257955551147
Epoch 120, training loss: 8.130157470703125 = 1.7525334358215332 + 1.0 * 6.377623558044434
Epoch 120, val loss: 1.7457752227783203
Epoch 130, training loss: 8.078572273254395 = 1.7372486591339111 + 1.0 * 6.341323375701904
Epoch 130, val loss: 1.731575846672058
Epoch 140, training loss: 8.031667709350586 = 1.7196414470672607 + 1.0 * 6.312026500701904
Epoch 140, val loss: 1.7159993648529053
Epoch 150, training loss: 7.985513687133789 = 1.6989761590957642 + 1.0 * 6.2865376472473145
Epoch 150, val loss: 1.698508381843567
Epoch 160, training loss: 7.938899993896484 = 1.6741719245910645 + 1.0 * 6.26472806930542
Epoch 160, val loss: 1.6782721281051636
Epoch 170, training loss: 7.88939094543457 = 1.643669605255127 + 1.0 * 6.245721340179443
Epoch 170, val loss: 1.6539372205734253
Epoch 180, training loss: 7.835972785949707 = 1.6063182353973389 + 1.0 * 6.229654788970947
Epoch 180, val loss: 1.6244713068008423
Epoch 190, training loss: 7.778632164001465 = 1.5613936185836792 + 1.0 * 6.217238426208496
Epoch 190, val loss: 1.5892245769500732
Epoch 200, training loss: 7.713281154632568 = 1.5090810060501099 + 1.0 * 6.204200267791748
Epoch 200, val loss: 1.5485267639160156
Epoch 210, training loss: 7.644408226013184 = 1.4499256610870361 + 1.0 * 6.194482326507568
Epoch 210, val loss: 1.5026450157165527
Epoch 220, training loss: 7.575002670288086 = 1.3857381343841553 + 1.0 * 6.189264297485352
Epoch 220, val loss: 1.453268051147461
Epoch 230, training loss: 7.501309394836426 = 1.3207991123199463 + 1.0 * 6.1805100440979
Epoch 230, val loss: 1.4038037061691284
Epoch 240, training loss: 7.430837154388428 = 1.256642460823059 + 1.0 * 6.174194812774658
Epoch 240, val loss: 1.3552747964859009
Epoch 250, training loss: 7.3610429763793945 = 1.1935374736785889 + 1.0 * 6.167505741119385
Epoch 250, val loss: 1.307923674583435
Epoch 260, training loss: 7.2973480224609375 = 1.1321219205856323 + 1.0 * 6.165225982666016
Epoch 260, val loss: 1.2621928453445435
Epoch 270, training loss: 7.230467796325684 = 1.0738786458969116 + 1.0 * 6.156589031219482
Epoch 270, val loss: 1.2190320491790771
Epoch 280, training loss: 7.169255256652832 = 1.0177570581436157 + 1.0 * 6.151498317718506
Epoch 280, val loss: 1.1775577068328857
Epoch 290, training loss: 7.115060806274414 = 0.9636229276657104 + 1.0 * 6.151437759399414
Epoch 290, val loss: 1.1376240253448486
Epoch 300, training loss: 7.055718421936035 = 0.912696123123169 + 1.0 * 6.143022537231445
Epoch 300, val loss: 1.0998458862304688
Epoch 310, training loss: 7.002408981323242 = 0.8644142746925354 + 1.0 * 6.137994766235352
Epoch 310, val loss: 1.0642311573028564
Epoch 320, training loss: 6.952059268951416 = 0.8186083436012268 + 1.0 * 6.133450984954834
Epoch 320, val loss: 1.0305273532867432
Epoch 330, training loss: 6.91129732131958 = 0.7754208445549011 + 1.0 * 6.135876655578613
Epoch 330, val loss: 0.9989849925041199
Epoch 340, training loss: 6.866406440734863 = 0.7360512018203735 + 1.0 * 6.130355358123779
Epoch 340, val loss: 0.9705442190170288
Epoch 350, training loss: 6.823423385620117 = 0.6991816759109497 + 1.0 * 6.124241828918457
Epoch 350, val loss: 0.9446753859519958
Epoch 360, training loss: 6.785280704498291 = 0.6644964218139648 + 1.0 * 6.120784282684326
Epoch 360, val loss: 0.9206650257110596
Epoch 370, training loss: 6.749505043029785 = 0.6319690942764282 + 1.0 * 6.1175360679626465
Epoch 370, val loss: 0.8987240195274353
Epoch 380, training loss: 6.716588020324707 = 0.6013449430465698 + 1.0 * 6.115242958068848
Epoch 380, val loss: 0.8788983821868896
Epoch 390, training loss: 6.683980464935303 = 0.5723939538002014 + 1.0 * 6.111586570739746
Epoch 390, val loss: 0.8605738282203674
Epoch 400, training loss: 6.654271602630615 = 0.5448669195175171 + 1.0 * 6.109404563903809
Epoch 400, val loss: 0.8438177108764648
Epoch 410, training loss: 6.625342845916748 = 0.5185181498527527 + 1.0 * 6.10682487487793
Epoch 410, val loss: 0.828419029712677
Epoch 420, training loss: 6.596986293792725 = 0.49322861433029175 + 1.0 * 6.103757858276367
Epoch 420, val loss: 0.8141189217567444
Epoch 430, training loss: 6.572293281555176 = 0.46879473328590393 + 1.0 * 6.103498458862305
Epoch 430, val loss: 0.800844669342041
Epoch 440, training loss: 6.545387268066406 = 0.44516706466674805 + 1.0 * 6.100220203399658
Epoch 440, val loss: 0.7884002327919006
Epoch 450, training loss: 6.520007610321045 = 0.4219723641872406 + 1.0 * 6.0980353355407715
Epoch 450, val loss: 0.7766878604888916
Epoch 460, training loss: 6.4958038330078125 = 0.3991861939430237 + 1.0 * 6.096617698669434
Epoch 460, val loss: 0.7655948996543884
Epoch 470, training loss: 6.472813606262207 = 0.376905232667923 + 1.0 * 6.095908164978027
Epoch 470, val loss: 0.7550650835037231
Epoch 480, training loss: 6.447830677032471 = 0.3550068736076355 + 1.0 * 6.0928239822387695
Epoch 480, val loss: 0.7449949383735657
Epoch 490, training loss: 6.427682876586914 = 0.3335367739200592 + 1.0 * 6.094146251678467
Epoch 490, val loss: 0.7355265021324158
Epoch 500, training loss: 6.404765605926514 = 0.31273171305656433 + 1.0 * 6.092033863067627
Epoch 500, val loss: 0.7266925573348999
Epoch 510, training loss: 6.3806891441345215 = 0.2926713526248932 + 1.0 * 6.08801794052124
Epoch 510, val loss: 0.7185453176498413
Epoch 520, training loss: 6.359258651733398 = 0.2734963893890381 + 1.0 * 6.0857625007629395
Epoch 520, val loss: 0.7111730575561523
Epoch 530, training loss: 6.34083366394043 = 0.2553160786628723 + 1.0 * 6.085517406463623
Epoch 530, val loss: 0.7045955657958984
Epoch 540, training loss: 6.326049327850342 = 0.2383216917514801 + 1.0 * 6.0877275466918945
Epoch 540, val loss: 0.6988442540168762
Epoch 550, training loss: 6.3043975830078125 = 0.22257781028747559 + 1.0 * 6.081820011138916
Epoch 550, val loss: 0.6939968466758728
Epoch 560, training loss: 6.287749767303467 = 0.2079981416463852 + 1.0 * 6.079751491546631
Epoch 560, val loss: 0.6899626851081848
Epoch 570, training loss: 6.277712821960449 = 0.19454452395439148 + 1.0 * 6.0831685066223145
Epoch 570, val loss: 0.6869518756866455
Epoch 580, training loss: 6.262846946716309 = 0.18229293823242188 + 1.0 * 6.080554008483887
Epoch 580, val loss: 0.6846261024475098
Epoch 590, training loss: 6.246347427368164 = 0.1710508018732071 + 1.0 * 6.075296401977539
Epoch 590, val loss: 0.6830832362174988
Epoch 600, training loss: 6.236457824707031 = 0.16069602966308594 + 1.0 * 6.075761795043945
Epoch 600, val loss: 0.6821706891059875
Epoch 610, training loss: 6.222770690917969 = 0.15119346976280212 + 1.0 * 6.071577072143555
Epoch 610, val loss: 0.6819368004798889
Epoch 620, training loss: 6.216728210449219 = 0.14245104789733887 + 1.0 * 6.074277400970459
Epoch 620, val loss: 0.6821610331535339
Epoch 630, training loss: 6.207833766937256 = 0.13439969718456268 + 1.0 * 6.073433876037598
Epoch 630, val loss: 0.682732105255127
Epoch 640, training loss: 6.194826602935791 = 0.12697474658489227 + 1.0 * 6.067852020263672
Epoch 640, val loss: 0.6838791370391846
Epoch 650, training loss: 6.186513423919678 = 0.12007666379213333 + 1.0 * 6.066436767578125
Epoch 650, val loss: 0.6854954957962036
Epoch 660, training loss: 6.186023235321045 = 0.1136498749256134 + 1.0 * 6.072373390197754
Epoch 660, val loss: 0.6874570250511169
Epoch 670, training loss: 6.176645755767822 = 0.10769812017679214 + 1.0 * 6.068947792053223
Epoch 670, val loss: 0.6896049380302429
Epoch 680, training loss: 6.166473865509033 = 0.1021350622177124 + 1.0 * 6.064338684082031
Epoch 680, val loss: 0.6920720934867859
Epoch 690, training loss: 6.158579349517822 = 0.09696470946073532 + 1.0 * 6.061614513397217
Epoch 690, val loss: 0.6947829127311707
Epoch 700, training loss: 6.153934001922607 = 0.09210794419050217 + 1.0 * 6.061826229095459
Epoch 700, val loss: 0.6976915597915649
Epoch 710, training loss: 6.148970603942871 = 0.08755148202180862 + 1.0 * 6.0614190101623535
Epoch 710, val loss: 0.7007458806037903
Epoch 720, training loss: 6.141948699951172 = 0.08330100029706955 + 1.0 * 6.058647632598877
Epoch 720, val loss: 0.7039790153503418
Epoch 730, training loss: 6.1403374671936035 = 0.07929115742444992 + 1.0 * 6.061046123504639
Epoch 730, val loss: 0.7073374390602112
Epoch 740, training loss: 6.134210586547852 = 0.07554148137569427 + 1.0 * 6.058669090270996
Epoch 740, val loss: 0.7108398675918579
Epoch 750, training loss: 6.126955509185791 = 0.07202183455228806 + 1.0 * 6.054933547973633
Epoch 750, val loss: 0.7143753170967102
Epoch 760, training loss: 6.124223709106445 = 0.06869810074567795 + 1.0 * 6.055525779724121
Epoch 760, val loss: 0.718072772026062
Epoch 770, training loss: 6.121365547180176 = 0.0655883401632309 + 1.0 * 6.055777072906494
Epoch 770, val loss: 0.7217943668365479
Epoch 780, training loss: 6.116436004638672 = 0.06269341707229614 + 1.0 * 6.053742408752441
Epoch 780, val loss: 0.7255502343177795
Epoch 790, training loss: 6.111907005310059 = 0.05997026339173317 + 1.0 * 6.051936626434326
Epoch 790, val loss: 0.7293997406959534
Epoch 800, training loss: 6.1073503494262695 = 0.057390760630369186 + 1.0 * 6.049959659576416
Epoch 800, val loss: 0.7333099246025085
Epoch 810, training loss: 6.115429878234863 = 0.054951369762420654 + 1.0 * 6.060478687286377
Epoch 810, val loss: 0.7372087836265564
Epoch 820, training loss: 6.1031341552734375 = 0.05266039818525314 + 1.0 * 6.050473690032959
Epoch 820, val loss: 0.7410426139831543
Epoch 830, training loss: 6.100993633270264 = 0.05051231384277344 + 1.0 * 6.05048131942749
Epoch 830, val loss: 0.7449633479118347
Epoch 840, training loss: 6.094968795776367 = 0.048494063317775726 + 1.0 * 6.046474933624268
Epoch 840, val loss: 0.7488099336624146
Epoch 850, training loss: 6.092930793762207 = 0.046591196209192276 + 1.0 * 6.046339511871338
Epoch 850, val loss: 0.7527031302452087
Epoch 860, training loss: 6.0894975662231445 = 0.044780246913433075 + 1.0 * 6.044717311859131
Epoch 860, val loss: 0.756626307964325
Epoch 870, training loss: 6.088206768035889 = 0.043061595410108566 + 1.0 * 6.045145034790039
Epoch 870, val loss: 0.760533332824707
Epoch 880, training loss: 6.086006164550781 = 0.04144240543246269 + 1.0 * 6.0445637702941895
Epoch 880, val loss: 0.7643678188323975
Epoch 890, training loss: 6.084863662719727 = 0.03992485627532005 + 1.0 * 6.044939041137695
Epoch 890, val loss: 0.7681558728218079
Epoch 900, training loss: 6.080347061157227 = 0.03848874941468239 + 1.0 * 6.041858196258545
Epoch 900, val loss: 0.7720301747322083
Epoch 910, training loss: 6.078162670135498 = 0.03712213411927223 + 1.0 * 6.041040420532227
Epoch 910, val loss: 0.7759066224098206
Epoch 920, training loss: 6.087849140167236 = 0.03582152724266052 + 1.0 * 6.052027702331543
Epoch 920, val loss: 0.7797118425369263
Epoch 930, training loss: 6.078874588012695 = 0.03459765017032623 + 1.0 * 6.044276714324951
Epoch 930, val loss: 0.783439576625824
Epoch 940, training loss: 6.072868347167969 = 0.03343665972352028 + 1.0 * 6.039431571960449
Epoch 940, val loss: 0.7871567606925964
Epoch 950, training loss: 6.071292400360107 = 0.032328780740499496 + 1.0 * 6.038963794708252
Epoch 950, val loss: 0.7909123301506042
Epoch 960, training loss: 6.077451229095459 = 0.031269751489162445 + 1.0 * 6.046181678771973
Epoch 960, val loss: 0.794589102268219
Epoch 970, training loss: 6.070262908935547 = 0.030263956636190414 + 1.0 * 6.039999008178711
Epoch 970, val loss: 0.7981738448143005
Epoch 980, training loss: 6.068869590759277 = 0.02930508367717266 + 1.0 * 6.039564609527588
Epoch 980, val loss: 0.8017736673355103
Epoch 990, training loss: 6.065600395202637 = 0.02839190699160099 + 1.0 * 6.037208557128906
Epoch 990, val loss: 0.805338442325592
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.343536376953125 = 1.9698302745819092 + 1.0 * 8.373705863952637
Epoch 0, val loss: 1.9706203937530518
Epoch 10, training loss: 10.330649375915527 = 1.9578826427459717 + 1.0 * 8.372766494750977
Epoch 10, val loss: 1.9585447311401367
Epoch 20, training loss: 10.310763359069824 = 1.943182110786438 + 1.0 * 8.367581367492676
Epoch 20, val loss: 1.9427878856658936
Epoch 30, training loss: 10.260000228881836 = 1.9230035543441772 + 1.0 * 8.336997032165527
Epoch 30, val loss: 1.9204391241073608
Epoch 40, training loss: 9.99491024017334 = 1.899009346961975 + 1.0 * 8.095900535583496
Epoch 40, val loss: 1.8949744701385498
Epoch 50, training loss: 9.033196449279785 = 1.8753892183303833 + 1.0 * 7.157807350158691
Epoch 50, val loss: 1.8703043460845947
Epoch 60, training loss: 8.700807571411133 = 1.8565454483032227 + 1.0 * 6.844262599945068
Epoch 60, val loss: 1.8512531518936157
Epoch 70, training loss: 8.46888542175293 = 1.8407418727874756 + 1.0 * 6.628143310546875
Epoch 70, val loss: 1.834956407546997
Epoch 80, training loss: 8.360864639282227 = 1.825852870941162 + 1.0 * 6.5350117683410645
Epoch 80, val loss: 1.8192636966705322
Epoch 90, training loss: 8.281246185302734 = 1.8094128370285034 + 1.0 * 6.471833229064941
Epoch 90, val loss: 1.8018853664398193
Epoch 100, training loss: 8.208331108093262 = 1.791924238204956 + 1.0 * 6.416406631469727
Epoch 100, val loss: 1.7840662002563477
Epoch 110, training loss: 8.14773178100586 = 1.7751888036727905 + 1.0 * 6.372542858123779
Epoch 110, val loss: 1.7675193548202515
Epoch 120, training loss: 8.09062385559082 = 1.7593441009521484 + 1.0 * 6.331279277801514
Epoch 120, val loss: 1.7523291110992432
Epoch 130, training loss: 8.038398742675781 = 1.7432515621185303 + 1.0 * 6.295146942138672
Epoch 130, val loss: 1.7373305559158325
Epoch 140, training loss: 7.992222309112549 = 1.7257013320922852 + 1.0 * 6.266520977020264
Epoch 140, val loss: 1.7216442823410034
Epoch 150, training loss: 7.950528144836426 = 1.705368995666504 + 1.0 * 6.245159149169922
Epoch 150, val loss: 1.7044206857681274
Epoch 160, training loss: 7.907325267791748 = 1.6813921928405762 + 1.0 * 6.225933074951172
Epoch 160, val loss: 1.6847805976867676
Epoch 170, training loss: 7.863674163818359 = 1.6523170471191406 + 1.0 * 6.211357116699219
Epoch 170, val loss: 1.6616153717041016
Epoch 180, training loss: 7.816082954406738 = 1.6168601512908936 + 1.0 * 6.199223041534424
Epoch 180, val loss: 1.6337820291519165
Epoch 190, training loss: 7.766505718231201 = 1.5744863748550415 + 1.0 * 6.192019462585449
Epoch 190, val loss: 1.6007193326950073
Epoch 200, training loss: 7.708986759185791 = 1.5267772674560547 + 1.0 * 6.182209491729736
Epoch 200, val loss: 1.5637786388397217
Epoch 210, training loss: 7.649389266967773 = 1.4745522737503052 + 1.0 * 6.174837112426758
Epoch 210, val loss: 1.5236761569976807
Epoch 220, training loss: 7.587498188018799 = 1.4188942909240723 + 1.0 * 6.168603897094727
Epoch 220, val loss: 1.4813580513000488
Epoch 230, training loss: 7.5271315574646 = 1.3631948232650757 + 1.0 * 6.163936614990234
Epoch 230, val loss: 1.4402748346328735
Epoch 240, training loss: 7.468363285064697 = 1.3099337816238403 + 1.0 * 6.1584296226501465
Epoch 240, val loss: 1.4016640186309814
Epoch 250, training loss: 7.413367748260498 = 1.2597178220748901 + 1.0 * 6.153649806976318
Epoch 250, val loss: 1.366050362586975
Epoch 260, training loss: 7.359316825866699 = 1.2122633457183838 + 1.0 * 6.1470537185668945
Epoch 260, val loss: 1.3331650495529175
Epoch 270, training loss: 7.308279037475586 = 1.166294813156128 + 1.0 * 6.141983985900879
Epoch 270, val loss: 1.3017123937606812
Epoch 280, training loss: 7.258895397186279 = 1.1211929321289062 + 1.0 * 6.137702465057373
Epoch 280, val loss: 1.2711867094039917
Epoch 290, training loss: 7.210083961486816 = 1.0765597820281982 + 1.0 * 6.133524417877197
Epoch 290, val loss: 1.2411792278289795
Epoch 300, training loss: 7.159830093383789 = 1.0314061641693115 + 1.0 * 6.128423690795898
Epoch 300, val loss: 1.2105703353881836
Epoch 310, training loss: 7.113026142120361 = 0.9854422211647034 + 1.0 * 6.127583980560303
Epoch 310, val loss: 1.1792734861373901
Epoch 320, training loss: 7.064643859863281 = 0.939397931098938 + 1.0 * 6.125246047973633
Epoch 320, val loss: 1.1480635404586792
Epoch 330, training loss: 7.012373447418213 = 0.8934174180030823 + 1.0 * 6.118956089019775
Epoch 330, val loss: 1.1166369915008545
Epoch 340, training loss: 6.962438583374023 = 0.8470044732093811 + 1.0 * 6.115434169769287
Epoch 340, val loss: 1.084986925125122
Epoch 350, training loss: 6.917579174041748 = 0.8005141019821167 + 1.0 * 6.117064952850342
Epoch 350, val loss: 1.0537207126617432
Epoch 360, training loss: 6.869238376617432 = 0.7555761337280273 + 1.0 * 6.113662242889404
Epoch 360, val loss: 1.0239514112472534
Epoch 370, training loss: 6.819790840148926 = 0.711487889289856 + 1.0 * 6.108303070068359
Epoch 370, val loss: 0.9953780770301819
Epoch 380, training loss: 6.773804664611816 = 0.6683902144432068 + 1.0 * 6.105414390563965
Epoch 380, val loss: 0.9681641459465027
Epoch 390, training loss: 6.74104118347168 = 0.6272196173667908 + 1.0 * 6.113821506500244
Epoch 390, val loss: 0.9432966709136963
Epoch 400, training loss: 6.691049575805664 = 0.5892726182937622 + 1.0 * 6.101777076721191
Epoch 400, val loss: 0.9212489724159241
Epoch 410, training loss: 6.652372360229492 = 0.5537610650062561 + 1.0 * 6.098611354827881
Epoch 410, val loss: 0.9018990993499756
Epoch 420, training loss: 6.616039276123047 = 0.5206295847892761 + 1.0 * 6.095409870147705
Epoch 420, val loss: 0.8851330280303955
Epoch 430, training loss: 6.583928108215332 = 0.4898986220359802 + 1.0 * 6.094029426574707
Epoch 430, val loss: 0.8712016344070435
Epoch 440, training loss: 6.562135219573975 = 0.46185973286628723 + 1.0 * 6.10027551651001
Epoch 440, val loss: 0.8599035143852234
Epoch 450, training loss: 6.526451110839844 = 0.4363488256931305 + 1.0 * 6.090102195739746
Epoch 450, val loss: 0.8513529896736145
Epoch 460, training loss: 6.499272346496582 = 0.41274094581604004 + 1.0 * 6.086531162261963
Epoch 460, val loss: 0.8447489738464355
Epoch 470, training loss: 6.484373092651367 = 0.3907696008682251 + 1.0 * 6.093603610992432
Epoch 470, val loss: 0.8398457765579224
Epoch 480, training loss: 6.458249092102051 = 0.3705558180809021 + 1.0 * 6.087693214416504
Epoch 480, val loss: 0.8363674283027649
Epoch 490, training loss: 6.433674335479736 = 0.3518160879611969 + 1.0 * 6.081858158111572
Epoch 490, val loss: 0.8343421220779419
Epoch 500, training loss: 6.414176940917969 = 0.3341588079929352 + 1.0 * 6.080018043518066
Epoch 500, val loss: 0.833029568195343
Epoch 510, training loss: 6.397172451019287 = 0.3174145817756653 + 1.0 * 6.0797576904296875
Epoch 510, val loss: 0.8325154781341553
Epoch 520, training loss: 6.378468990325928 = 0.30159956216812134 + 1.0 * 6.076869487762451
Epoch 520, val loss: 0.8326013088226318
Epoch 530, training loss: 6.363966464996338 = 0.2866363227367401 + 1.0 * 6.077330112457275
Epoch 530, val loss: 0.8333605527877808
Epoch 540, training loss: 6.346747398376465 = 0.2724035978317261 + 1.0 * 6.074343681335449
Epoch 540, val loss: 0.8345063924789429
Epoch 550, training loss: 6.334246635437012 = 0.25883999466896057 + 1.0 * 6.075406551361084
Epoch 550, val loss: 0.8359623551368713
Epoch 560, training loss: 6.317568778991699 = 0.24598032236099243 + 1.0 * 6.071588516235352
Epoch 560, val loss: 0.8379156589508057
Epoch 570, training loss: 6.304179668426514 = 0.2336970418691635 + 1.0 * 6.0704827308654785
Epoch 570, val loss: 0.8401925563812256
Epoch 580, training loss: 6.297138690948486 = 0.22199156880378723 + 1.0 * 6.0751471519470215
Epoch 580, val loss: 0.842612087726593
Epoch 590, training loss: 6.281026840209961 = 0.21090945601463318 + 1.0 * 6.070117473602295
Epoch 590, val loss: 0.8453521132469177
Epoch 600, training loss: 6.267221927642822 = 0.20039106905460358 + 1.0 * 6.066830635070801
Epoch 600, val loss: 0.8483492732048035
Epoch 610, training loss: 6.255448341369629 = 0.190354585647583 + 1.0 * 6.065093994140625
Epoch 610, val loss: 0.8514400124549866
Epoch 620, training loss: 6.248107433319092 = 0.18077965080738068 + 1.0 * 6.067327976226807
Epoch 620, val loss: 0.8547382354736328
Epoch 630, training loss: 6.235616683959961 = 0.17169374227523804 + 1.0 * 6.063922882080078
Epoch 630, val loss: 0.8581891059875488
Epoch 640, training loss: 6.2260918617248535 = 0.16307900846004486 + 1.0 * 6.063013076782227
Epoch 640, val loss: 0.8617951273918152
Epoch 650, training loss: 6.218818664550781 = 0.15490984916687012 + 1.0 * 6.063908576965332
Epoch 650, val loss: 0.8654507398605347
Epoch 660, training loss: 6.206977367401123 = 0.14717704057693481 + 1.0 * 6.059800148010254
Epoch 660, val loss: 0.8692507743835449
Epoch 670, training loss: 6.199422359466553 = 0.13985027372837067 + 1.0 * 6.059572219848633
Epoch 670, val loss: 0.8730847835540771
Epoch 680, training loss: 6.189589977264404 = 0.13290371000766754 + 1.0 * 6.0566864013671875
Epoch 680, val loss: 0.876997709274292
Epoch 690, training loss: 6.193150043487549 = 0.1263243556022644 + 1.0 * 6.066825866699219
Epoch 690, val loss: 0.8809438347816467
Epoch 700, training loss: 6.1762800216674805 = 0.12012778967618942 + 1.0 * 6.05615234375
Epoch 700, val loss: 0.8849586844444275
Epoch 710, training loss: 6.16815185546875 = 0.11427737027406693 + 1.0 * 6.053874492645264
Epoch 710, val loss: 0.8891357779502869
Epoch 720, training loss: 6.1670756340026855 = 0.10873416066169739 + 1.0 * 6.0583415031433105
Epoch 720, val loss: 0.8932703137397766
Epoch 730, training loss: 6.1603240966796875 = 0.10351589322090149 + 1.0 * 6.056807994842529
Epoch 730, val loss: 0.8974117636680603
Epoch 740, training loss: 6.1487884521484375 = 0.09860426187515259 + 1.0 * 6.05018424987793
Epoch 740, val loss: 0.9018463492393494
Epoch 750, training loss: 6.1439971923828125 = 0.09395547956228256 + 1.0 * 6.050041675567627
Epoch 750, val loss: 0.9062682390213013
Epoch 760, training loss: 6.142070293426514 = 0.08956882357597351 + 1.0 * 6.052501678466797
Epoch 760, val loss: 0.9105761647224426
Epoch 770, training loss: 6.1340107917785645 = 0.08545146137475967 + 1.0 * 6.048559188842773
Epoch 770, val loss: 0.9151878356933594
Epoch 780, training loss: 6.12797212600708 = 0.0815633088350296 + 1.0 * 6.046408653259277
Epoch 780, val loss: 0.9198199510574341
Epoch 790, training loss: 6.122926712036133 = 0.0778842568397522 + 1.0 * 6.045042514801025
Epoch 790, val loss: 0.92442387342453
Epoch 800, training loss: 6.144334316253662 = 0.07440605759620667 + 1.0 * 6.069928169250488
Epoch 800, val loss: 0.929000198841095
Epoch 810, training loss: 6.119378566741943 = 0.07116357237100601 + 1.0 * 6.048214912414551
Epoch 810, val loss: 0.9335569739341736
Epoch 820, training loss: 6.112065315246582 = 0.0681229755282402 + 1.0 * 6.043942451477051
Epoch 820, val loss: 0.9384335279464722
Epoch 830, training loss: 6.107339382171631 = 0.06524772197008133 + 1.0 * 6.0420918464660645
Epoch 830, val loss: 0.9430530667304993
Epoch 840, training loss: 6.11572790145874 = 0.06252428889274597 + 1.0 * 6.053203582763672
Epoch 840, val loss: 0.9477105140686035
Epoch 850, training loss: 6.104114055633545 = 0.05996077507734299 + 1.0 * 6.044153213500977
Epoch 850, val loss: 0.9524630904197693
Epoch 860, training loss: 6.0978779792785645 = 0.057548828423023224 + 1.0 * 6.0403289794921875
Epoch 860, val loss: 0.9574248194694519
Epoch 870, training loss: 6.096249580383301 = 0.05525858327746391 + 1.0 * 6.040990829467773
Epoch 870, val loss: 0.9622161984443665
Epoch 880, training loss: 6.0930705070495605 = 0.05309540778398514 + 1.0 * 6.039975166320801
Epoch 880, val loss: 0.9669324159622192
Epoch 890, training loss: 6.08976936340332 = 0.05105443671345711 + 1.0 * 6.03871488571167
Epoch 890, val loss: 0.9719127416610718
Epoch 900, training loss: 6.086508750915527 = 0.049116943031549454 + 1.0 * 6.037391662597656
Epoch 900, val loss: 0.9768222570419312
Epoch 910, training loss: 6.090611457824707 = 0.047280263155698776 + 1.0 * 6.043331146240234
Epoch 910, val loss: 0.9816673994064331
Epoch 920, training loss: 6.085015296936035 = 0.04554280638694763 + 1.0 * 6.039472579956055
Epoch 920, val loss: 0.9863976240158081
Epoch 930, training loss: 6.083983421325684 = 0.043910231441259384 + 1.0 * 6.040073394775391
Epoch 930, val loss: 0.9913448691368103
Epoch 940, training loss: 6.076498985290527 = 0.0423629954457283 + 1.0 * 6.034135818481445
Epoch 940, val loss: 0.9961940050125122
Epoch 950, training loss: 6.074535369873047 = 0.04088866710662842 + 1.0 * 6.033646583557129
Epoch 950, val loss: 1.001044750213623
Epoch 960, training loss: 6.0731635093688965 = 0.03948391228914261 + 1.0 * 6.033679485321045
Epoch 960, val loss: 1.0058488845825195
Epoch 970, training loss: 6.073435306549072 = 0.038146477192640305 + 1.0 * 6.0352888107299805
Epoch 970, val loss: 1.010635256767273
Epoch 980, training loss: 6.075068473815918 = 0.03687676414847374 + 1.0 * 6.038191795349121
Epoch 980, val loss: 1.0153684616088867
Epoch 990, training loss: 6.066867828369141 = 0.035672932863235474 + 1.0 * 6.031194686889648
Epoch 990, val loss: 1.020195722579956
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.5904
Flip ASR: 0.5511/225 nodes
The final ASR:0.71956, 0.09997, Accuracy:0.80247, 0.01062
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10544])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00348, Accuracy:0.82840, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.314888000488281 = 1.940999150276184 + 1.0 * 8.373888969421387
Epoch 0, val loss: 1.940780520439148
Epoch 10, training loss: 10.304591178894043 = 1.930927038192749 + 1.0 * 8.373663902282715
Epoch 10, val loss: 1.9309319257736206
Epoch 20, training loss: 10.291271209716797 = 1.91890549659729 + 1.0 * 8.372365951538086
Epoch 20, val loss: 1.9191279411315918
Epoch 30, training loss: 10.265762329101562 = 1.9024745225906372 + 1.0 * 8.363287925720215
Epoch 30, val loss: 1.9033361673355103
Epoch 40, training loss: 10.17168140411377 = 1.8801931142807007 + 1.0 * 8.291488647460938
Epoch 40, val loss: 1.882561206817627
Epoch 50, training loss: 9.602783203125 = 1.854218602180481 + 1.0 * 7.748564720153809
Epoch 50, val loss: 1.8584496974945068
Epoch 60, training loss: 9.037351608276367 = 1.830986738204956 + 1.0 * 7.20636510848999
Epoch 60, val loss: 1.8383233547210693
Epoch 70, training loss: 8.726363182067871 = 1.8165175914764404 + 1.0 * 6.90984582901001
Epoch 70, val loss: 1.8253679275512695
Epoch 80, training loss: 8.58549976348877 = 1.8031914234161377 + 1.0 * 6.782308101654053
Epoch 80, val loss: 1.8132425546646118
Epoch 90, training loss: 8.498289108276367 = 1.790061354637146 + 1.0 * 6.70822811126709
Epoch 90, val loss: 1.801200032234192
Epoch 100, training loss: 8.40849494934082 = 1.7771743535995483 + 1.0 * 6.631320476531982
Epoch 100, val loss: 1.789372444152832
Epoch 110, training loss: 8.316314697265625 = 1.7653484344482422 + 1.0 * 6.550966739654541
Epoch 110, val loss: 1.7785699367523193
Epoch 120, training loss: 8.241107940673828 = 1.7531579732894897 + 1.0 * 6.487950325012207
Epoch 120, val loss: 1.7676182985305786
Epoch 130, training loss: 8.182673454284668 = 1.7385655641555786 + 1.0 * 6.444108009338379
Epoch 130, val loss: 1.7547037601470947
Epoch 140, training loss: 8.126206398010254 = 1.7211025953292847 + 1.0 * 6.405104160308838
Epoch 140, val loss: 1.7400189638137817
Epoch 150, training loss: 8.069252014160156 = 1.7004202604293823 + 1.0 * 6.368832111358643
Epoch 150, val loss: 1.7229775190353394
Epoch 160, training loss: 8.015761375427246 = 1.675731897354126 + 1.0 * 6.340029239654541
Epoch 160, val loss: 1.7028712034225464
Epoch 170, training loss: 7.9627685546875 = 1.6453640460968018 + 1.0 * 6.317404747009277
Epoch 170, val loss: 1.6781648397445679
Epoch 180, training loss: 7.907066345214844 = 1.6078283786773682 + 1.0 * 6.299238204956055
Epoch 180, val loss: 1.647372841835022
Epoch 190, training loss: 7.849358558654785 = 1.5620366334915161 + 1.0 * 6.287322044372559
Epoch 190, val loss: 1.6096185445785522
Epoch 200, training loss: 7.780295372009277 = 1.5087326765060425 + 1.0 * 6.271562576293945
Epoch 200, val loss: 1.566012978553772
Epoch 210, training loss: 7.708100318908691 = 1.4482989311218262 + 1.0 * 6.259801387786865
Epoch 210, val loss: 1.5163171291351318
Epoch 220, training loss: 7.631844520568848 = 1.381983995437622 + 1.0 * 6.2498602867126465
Epoch 220, val loss: 1.4616447687149048
Epoch 230, training loss: 7.55446195602417 = 1.3136528730392456 + 1.0 * 6.240808963775635
Epoch 230, val loss: 1.4061834812164307
Epoch 240, training loss: 7.478193759918213 = 1.246058464050293 + 1.0 * 6.23213529586792
Epoch 240, val loss: 1.3513429164886475
Epoch 250, training loss: 7.401772975921631 = 1.1795449256896973 + 1.0 * 6.222228050231934
Epoch 250, val loss: 1.2983516454696655
Epoch 260, training loss: 7.330651760101318 = 1.1151418685913086 + 1.0 * 6.21550989151001
Epoch 260, val loss: 1.2479897737503052
Epoch 270, training loss: 7.261744022369385 = 1.0544513463974 + 1.0 * 6.207292556762695
Epoch 270, val loss: 1.2013241052627563
Epoch 280, training loss: 7.199794292449951 = 0.996487021446228 + 1.0 * 6.203307151794434
Epoch 280, val loss: 1.1571223735809326
Epoch 290, training loss: 7.136544704437256 = 0.941796600818634 + 1.0 * 6.1947479248046875
Epoch 290, val loss: 1.115830659866333
Epoch 300, training loss: 7.076746463775635 = 0.8897859454154968 + 1.0 * 6.186960697174072
Epoch 300, val loss: 1.076796054840088
Epoch 310, training loss: 7.024698734283447 = 0.8400000929832458 + 1.0 * 6.184698581695557
Epoch 310, val loss: 1.0393564701080322
Epoch 320, training loss: 6.971484184265137 = 0.793274462223053 + 1.0 * 6.1782097816467285
Epoch 320, val loss: 1.0042848587036133
Epoch 330, training loss: 6.921680450439453 = 0.749352753162384 + 1.0 * 6.172327518463135
Epoch 330, val loss: 0.971705436706543
Epoch 340, training loss: 6.879968643188477 = 0.7077319622039795 + 1.0 * 6.172236919403076
Epoch 340, val loss: 0.9412530660629272
Epoch 350, training loss: 6.8317084312438965 = 0.6686624884605408 + 1.0 * 6.163045883178711
Epoch 350, val loss: 0.9132924675941467
Epoch 360, training loss: 6.790525913238525 = 0.6315574049949646 + 1.0 * 6.158968448638916
Epoch 360, val loss: 0.8873797655105591
Epoch 370, training loss: 6.762078762054443 = 0.5960627198219299 + 1.0 * 6.166016101837158
Epoch 370, val loss: 0.8634507060050964
Epoch 380, training loss: 6.715970039367676 = 0.562804639339447 + 1.0 * 6.153165340423584
Epoch 380, val loss: 0.8417373895645142
Epoch 390, training loss: 6.680818557739258 = 0.5312609672546387 + 1.0 * 6.149557590484619
Epoch 390, val loss: 0.8220716118812561
Epoch 400, training loss: 6.646384239196777 = 0.5008556842803955 + 1.0 * 6.145528316497803
Epoch 400, val loss: 0.8037906289100647
Epoch 410, training loss: 6.614192008972168 = 0.4717239439487457 + 1.0 * 6.142467975616455
Epoch 410, val loss: 0.7869002819061279
Epoch 420, training loss: 6.585970878601074 = 0.4441712200641632 + 1.0 * 6.141799449920654
Epoch 420, val loss: 0.7717679142951965
Epoch 430, training loss: 6.554630756378174 = 0.4177352786064148 + 1.0 * 6.136895656585693
Epoch 430, val loss: 0.7579289674758911
Epoch 440, training loss: 6.5258331298828125 = 0.3922869861125946 + 1.0 * 6.133546352386475
Epoch 440, val loss: 0.7452486753463745
Epoch 450, training loss: 6.513889789581299 = 0.36787474155426025 + 1.0 * 6.146015167236328
Epoch 450, val loss: 0.7338255643844604
Epoch 460, training loss: 6.477377891540527 = 0.3448306918144226 + 1.0 * 6.132547378540039
Epoch 460, val loss: 0.7237064242362976
Epoch 470, training loss: 6.452049255371094 = 0.3229770362377167 + 1.0 * 6.129072189331055
Epoch 470, val loss: 0.7148836851119995
Epoch 480, training loss: 6.428159236907959 = 0.3023073971271515 + 1.0 * 6.125851631164551
Epoch 480, val loss: 0.7071916460990906
Epoch 490, training loss: 6.4056715965271 = 0.2827381193637848 + 1.0 * 6.122933387756348
Epoch 490, val loss: 0.7006556391716003
Epoch 500, training loss: 6.390260696411133 = 0.2642240524291992 + 1.0 * 6.126036643981934
Epoch 500, val loss: 0.6951696872711182
Epoch 510, training loss: 6.366962909698486 = 0.24695689976215363 + 1.0 * 6.120006084442139
Epoch 510, val loss: 0.6908095479011536
Epoch 520, training loss: 6.348178863525391 = 0.23090745508670807 + 1.0 * 6.117271423339844
Epoch 520, val loss: 0.6875863671302795
Epoch 530, training loss: 6.329385757446289 = 0.21583189070224762 + 1.0 * 6.113554000854492
Epoch 530, val loss: 0.6853234171867371
Epoch 540, training loss: 6.315096378326416 = 0.20167109370231628 + 1.0 * 6.113425254821777
Epoch 540, val loss: 0.6839536428451538
Epoch 550, training loss: 6.302330493927002 = 0.1885063797235489 + 1.0 * 6.113823890686035
Epoch 550, val loss: 0.6834356188774109
Epoch 560, training loss: 6.289328575134277 = 0.17640449106693268 + 1.0 * 6.112924098968506
Epoch 560, val loss: 0.6837314367294312
Epoch 570, training loss: 6.272580146789551 = 0.16525313258171082 + 1.0 * 6.107326984405518
Epoch 570, val loss: 0.6847564578056335
Epoch 580, training loss: 6.25968599319458 = 0.15489265322685242 + 1.0 * 6.104793548583984
Epoch 580, val loss: 0.6864162087440491
Epoch 590, training loss: 6.255741119384766 = 0.14527535438537598 + 1.0 * 6.1104655265808105
Epoch 590, val loss: 0.6886199712753296
Epoch 600, training loss: 6.240493297576904 = 0.13644838333129883 + 1.0 * 6.1040449142456055
Epoch 600, val loss: 0.6913570165634155
Epoch 610, training loss: 6.227409362792969 = 0.1283094882965088 + 1.0 * 6.099099636077881
Epoch 610, val loss: 0.6945713758468628
Epoch 620, training loss: 6.222571849822998 = 0.1207633689045906 + 1.0 * 6.101808547973633
Epoch 620, val loss: 0.6981883645057678
Epoch 630, training loss: 6.214441299438477 = 0.1138162612915039 + 1.0 * 6.100625038146973
Epoch 630, val loss: 0.7021265625953674
Epoch 640, training loss: 6.211254596710205 = 0.10746563225984573 + 1.0 * 6.10378885269165
Epoch 640, val loss: 0.7063503861427307
Epoch 650, training loss: 6.196591854095459 = 0.10163339972496033 + 1.0 * 6.094958305358887
Epoch 650, val loss: 0.7108094692230225
Epoch 660, training loss: 6.187148571014404 = 0.09622524678707123 + 1.0 * 6.090923309326172
Epoch 660, val loss: 0.7155402302742004
Epoch 670, training loss: 6.180230140686035 = 0.09119042754173279 + 1.0 * 6.0890398025512695
Epoch 670, val loss: 0.7204524874687195
Epoch 680, training loss: 6.184039115905762 = 0.08650650829076767 + 1.0 * 6.097532749176025
Epoch 680, val loss: 0.7255659103393555
Epoch 690, training loss: 6.171123027801514 = 0.08218318223953247 + 1.0 * 6.088939666748047
Epoch 690, val loss: 0.730774998664856
Epoch 700, training loss: 6.165077209472656 = 0.07818058133125305 + 1.0 * 6.0868964195251465
Epoch 700, val loss: 0.7360645532608032
Epoch 710, training loss: 6.167976379394531 = 0.07443861663341522 + 1.0 * 6.0935378074646
Epoch 710, val loss: 0.7415032982826233
Epoch 720, training loss: 6.157865524291992 = 0.07097981870174408 + 1.0 * 6.086885929107666
Epoch 720, val loss: 0.7468273043632507
Epoch 730, training loss: 6.148491859436035 = 0.06775029003620148 + 1.0 * 6.0807414054870605
Epoch 730, val loss: 0.7522842884063721
Epoch 740, training loss: 6.145472526550293 = 0.06471486389636993 + 1.0 * 6.0807576179504395
Epoch 740, val loss: 0.7578197121620178
Epoch 750, training loss: 6.143154621124268 = 0.06188889220356941 + 1.0 * 6.081265926361084
Epoch 750, val loss: 0.7633377313613892
Epoch 760, training loss: 6.137309551239014 = 0.059257082641124725 + 1.0 * 6.078052520751953
Epoch 760, val loss: 0.7687644958496094
Epoch 770, training loss: 6.133538722991943 = 0.056786682456731796 + 1.0 * 6.076752185821533
Epoch 770, val loss: 0.7742518186569214
Epoch 780, training loss: 6.133810520172119 = 0.054452940821647644 + 1.0 * 6.079357624053955
Epoch 780, val loss: 0.7797633409500122
Epoch 790, training loss: 6.12981653213501 = 0.05226771906018257 + 1.0 * 6.077548980712891
Epoch 790, val loss: 0.7852285504341125
Epoch 800, training loss: 6.124269008636475 = 0.050220802426338196 + 1.0 * 6.074048042297363
Epoch 800, val loss: 0.7905917167663574
Epoch 810, training loss: 6.1195387840271 = 0.04828178510069847 + 1.0 * 6.0712571144104
Epoch 810, val loss: 0.7960553169250488
Epoch 820, training loss: 6.130294322967529 = 0.04643826186656952 + 1.0 * 6.083856105804443
Epoch 820, val loss: 0.8014375567436218
Epoch 830, training loss: 6.115686416625977 = 0.044716592878103256 + 1.0 * 6.070970058441162
Epoch 830, val loss: 0.8067575097084045
Epoch 840, training loss: 6.112880706787109 = 0.043095413595438004 + 1.0 * 6.069785118103027
Epoch 840, val loss: 0.8120406866073608
Epoch 850, training loss: 6.109983444213867 = 0.0415479838848114 + 1.0 * 6.0684356689453125
Epoch 850, val loss: 0.8173409104347229
Epoch 860, training loss: 6.111990451812744 = 0.04007779806852341 + 1.0 * 6.07191276550293
Epoch 860, val loss: 0.8225805759429932
Epoch 870, training loss: 6.1056437492370605 = 0.038694750517606735 + 1.0 * 6.066948890686035
Epoch 870, val loss: 0.8277384638786316
Epoch 880, training loss: 6.1064324378967285 = 0.0373825766146183 + 1.0 * 6.069049835205078
Epoch 880, val loss: 0.8328705430030823
Epoch 890, training loss: 6.100222587585449 = 0.0361400805413723 + 1.0 * 6.064082622528076
Epoch 890, val loss: 0.8379432559013367
Epoch 900, training loss: 6.098354339599609 = 0.03495189547538757 + 1.0 * 6.0634026527404785
Epoch 900, val loss: 0.843040406703949
Epoch 910, training loss: 6.101025581359863 = 0.03381464630365372 + 1.0 * 6.067211151123047
Epoch 910, val loss: 0.8480916619300842
Epoch 920, training loss: 6.096362113952637 = 0.032733384519815445 + 1.0 * 6.063628673553467
Epoch 920, val loss: 0.8531531095504761
Epoch 930, training loss: 6.0961833000183105 = 0.03170699626207352 + 1.0 * 6.064476490020752
Epoch 930, val loss: 0.8580650687217712
Epoch 940, training loss: 6.09175443649292 = 0.030725929886102676 + 1.0 * 6.061028480529785
Epoch 940, val loss: 0.8630327582359314
Epoch 950, training loss: 6.09104061126709 = 0.02978706732392311 + 1.0 * 6.061253547668457
Epoch 950, val loss: 0.8679454922676086
Epoch 960, training loss: 6.089580535888672 = 0.028890209272503853 + 1.0 * 6.060690402984619
Epoch 960, val loss: 0.8727881908416748
Epoch 970, training loss: 6.088636875152588 = 0.02803821861743927 + 1.0 * 6.060598850250244
Epoch 970, val loss: 0.8775860667228699
Epoch 980, training loss: 6.087913513183594 = 0.02722463570535183 + 1.0 * 6.0606889724731445
Epoch 980, val loss: 0.8822892904281616
Epoch 990, training loss: 6.082640647888184 = 0.02644910104572773 + 1.0 * 6.056191444396973
Epoch 990, val loss: 0.8869892358779907
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5683
Flip ASR: 0.4889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.307746887207031 = 1.933967113494873 + 1.0 * 8.373780250549316
Epoch 0, val loss: 1.9356108903884888
Epoch 10, training loss: 10.296266555786133 = 1.9233850240707397 + 1.0 * 8.372881889343262
Epoch 10, val loss: 1.923375129699707
Epoch 20, training loss: 10.281167984008789 = 1.9100466966629028 + 1.0 * 8.371121406555176
Epoch 20, val loss: 1.9064842462539673
Epoch 30, training loss: 10.250537872314453 = 1.8921130895614624 + 1.0 * 8.35842514038086
Epoch 30, val loss: 1.8830004930496216
Epoch 40, training loss: 10.159159660339355 = 1.870707392692566 + 1.0 * 8.2884521484375
Epoch 40, val loss: 1.8560999631881714
Epoch 50, training loss: 9.794014930725098 = 1.8503085374832153 + 1.0 * 7.943706035614014
Epoch 50, val loss: 1.8321750164031982
Epoch 60, training loss: 9.279167175292969 = 1.8313380479812622 + 1.0 * 7.447828769683838
Epoch 60, val loss: 1.8126362562179565
Epoch 70, training loss: 8.806808471679688 = 1.8151761293411255 + 1.0 * 6.991631984710693
Epoch 70, val loss: 1.796991229057312
Epoch 80, training loss: 8.606781005859375 = 1.7992370128631592 + 1.0 * 6.807543754577637
Epoch 80, val loss: 1.7814286947250366
Epoch 90, training loss: 8.476418495178223 = 1.784442663192749 + 1.0 * 6.691976070404053
Epoch 90, val loss: 1.7675219774246216
Epoch 100, training loss: 8.380331039428711 = 1.7711127996444702 + 1.0 * 6.609218120574951
Epoch 100, val loss: 1.7552028894424438
Epoch 110, training loss: 8.302021980285645 = 1.7578048706054688 + 1.0 * 6.544217109680176
Epoch 110, val loss: 1.7429015636444092
Epoch 120, training loss: 8.226130485534668 = 1.7438710927963257 + 1.0 * 6.482259273529053
Epoch 120, val loss: 1.7302855253219604
Epoch 130, training loss: 8.160879135131836 = 1.7291364669799805 + 1.0 * 6.431743144989014
Epoch 130, val loss: 1.7175824642181396
Epoch 140, training loss: 8.104937553405762 = 1.7123785018920898 + 1.0 * 6.392559051513672
Epoch 140, val loss: 1.7032057046890259
Epoch 150, training loss: 8.053458213806152 = 1.6920477151870728 + 1.0 * 6.361410140991211
Epoch 150, val loss: 1.6861971616744995
Epoch 160, training loss: 8.000505447387695 = 1.6672964096069336 + 1.0 * 6.33320951461792
Epoch 160, val loss: 1.6658958196640015
Epoch 170, training loss: 7.947750568389893 = 1.6374112367630005 + 1.0 * 6.310339450836182
Epoch 170, val loss: 1.6415197849273682
Epoch 180, training loss: 7.890726089477539 = 1.6010777950286865 + 1.0 * 6.289648532867432
Epoch 180, val loss: 1.6120115518569946
Epoch 190, training loss: 7.828874111175537 = 1.5566835403442383 + 1.0 * 6.272190570831299
Epoch 190, val loss: 1.5758596658706665
Epoch 200, training loss: 7.76624870300293 = 1.5035308599472046 + 1.0 * 6.2627177238464355
Epoch 200, val loss: 1.532850742340088
Epoch 210, training loss: 7.690681457519531 = 1.444219946861267 + 1.0 * 6.246461391448975
Epoch 210, val loss: 1.4853301048278809
Epoch 220, training loss: 7.617569923400879 = 1.3797199726104736 + 1.0 * 6.237850189208984
Epoch 220, val loss: 1.4344435930252075
Epoch 230, training loss: 7.5418901443481445 = 1.3135225772857666 + 1.0 * 6.228367328643799
Epoch 230, val loss: 1.3828691244125366
Epoch 240, training loss: 7.4655842781066895 = 1.2475662231445312 + 1.0 * 6.218018054962158
Epoch 240, val loss: 1.3324438333511353
Epoch 250, training loss: 7.394761085510254 = 1.1836885213851929 + 1.0 * 6.2110724449157715
Epoch 250, val loss: 1.2842715978622437
Epoch 260, training loss: 7.326830863952637 = 1.123866319656372 + 1.0 * 6.202964782714844
Epoch 260, val loss: 1.2402150630950928
Epoch 270, training loss: 7.266937255859375 = 1.0693011283874512 + 1.0 * 6.197636127471924
Epoch 270, val loss: 1.2001866102218628
Epoch 280, training loss: 7.209402084350586 = 1.0200212001800537 + 1.0 * 6.189380645751953
Epoch 280, val loss: 1.164610505104065
Epoch 290, training loss: 7.155903339385986 = 0.9745641946792603 + 1.0 * 6.181339263916016
Epoch 290, val loss: 1.1320784091949463
Epoch 300, training loss: 7.111659526824951 = 0.9320425987243652 + 1.0 * 6.179616928100586
Epoch 300, val loss: 1.1018600463867188
Epoch 310, training loss: 7.065954208374023 = 0.8919969201087952 + 1.0 * 6.173957347869873
Epoch 310, val loss: 1.07352614402771
Epoch 320, training loss: 7.021686553955078 = 0.8542923927307129 + 1.0 * 6.167394161224365
Epoch 320, val loss: 1.0468287467956543
Epoch 330, training loss: 6.978600978851318 = 0.8178356885910034 + 1.0 * 6.160765171051025
Epoch 330, val loss: 1.0212997198104858
Epoch 340, training loss: 6.936303615570068 = 0.781737744808197 + 1.0 * 6.154565811157227
Epoch 340, val loss: 0.9961369037628174
Epoch 350, training loss: 6.897948741912842 = 0.7458415031433105 + 1.0 * 6.152107238769531
Epoch 350, val loss: 0.9711910486221313
Epoch 360, training loss: 6.857668876647949 = 0.710809588432312 + 1.0 * 6.146859169006348
Epoch 360, val loss: 0.9468514919281006
Epoch 370, training loss: 6.819092750549316 = 0.6768719553947449 + 1.0 * 6.142220973968506
Epoch 370, val loss: 0.9236847162246704
Epoch 380, training loss: 6.782429218292236 = 0.6438433527946472 + 1.0 * 6.138586044311523
Epoch 380, val loss: 0.9013878703117371
Epoch 390, training loss: 6.754715442657471 = 0.6121876835823059 + 1.0 * 6.1425275802612305
Epoch 390, val loss: 0.8802675604820251
Epoch 400, training loss: 6.716073036193848 = 0.5825008749961853 + 1.0 * 6.133572101593018
Epoch 400, val loss: 0.8612490296363831
Epoch 410, training loss: 6.6836957931518555 = 0.5542376041412354 + 1.0 * 6.129457950592041
Epoch 410, val loss: 0.8437430262565613
Epoch 420, training loss: 6.653125762939453 = 0.5272551774978638 + 1.0 * 6.125870704650879
Epoch 420, val loss: 0.8276383280754089
Epoch 430, training loss: 6.62685489654541 = 0.5015047192573547 + 1.0 * 6.125349998474121
Epoch 430, val loss: 0.812984824180603
Epoch 440, training loss: 6.603292942047119 = 0.4773802161216736 + 1.0 * 6.125912666320801
Epoch 440, val loss: 0.7999975085258484
Epoch 450, training loss: 6.57327127456665 = 0.4546829164028168 + 1.0 * 6.118588447570801
Epoch 450, val loss: 0.7888850569725037
Epoch 460, training loss: 6.550901889801025 = 0.4331422746181488 + 1.0 * 6.117759704589844
Epoch 460, val loss: 0.7790911793708801
Epoch 470, training loss: 6.528392791748047 = 0.412750244140625 + 1.0 * 6.115642547607422
Epoch 470, val loss: 0.7705984711647034
Epoch 480, training loss: 6.507644176483154 = 0.3935025930404663 + 1.0 * 6.114141464233398
Epoch 480, val loss: 0.7634413838386536
Epoch 490, training loss: 6.487489700317383 = 0.375336617231369 + 1.0 * 6.112153053283691
Epoch 490, val loss: 0.7575879693031311
Epoch 500, training loss: 6.466627597808838 = 0.3579655587673187 + 1.0 * 6.108662128448486
Epoch 500, val loss: 0.7526623010635376
Epoch 510, training loss: 6.447740077972412 = 0.3413505554199219 + 1.0 * 6.10638952255249
Epoch 510, val loss: 0.7486185431480408
Epoch 520, training loss: 6.446011543273926 = 0.3254392147064209 + 1.0 * 6.120572090148926
Epoch 520, val loss: 0.7453566789627075
Epoch 530, training loss: 6.418732643127441 = 0.31039735674858093 + 1.0 * 6.108335494995117
Epoch 530, val loss: 0.7429090142250061
Epoch 540, training loss: 6.398753643035889 = 0.2959772050380707 + 1.0 * 6.102776527404785
Epoch 540, val loss: 0.7412483096122742
Epoch 550, training loss: 6.382350444793701 = 0.2820572853088379 + 1.0 * 6.100293159484863
Epoch 550, val loss: 0.7400875091552734
Epoch 560, training loss: 6.3805108070373535 = 0.26863306760787964 + 1.0 * 6.111877918243408
Epoch 560, val loss: 0.7393327355384827
Epoch 570, training loss: 6.355973243713379 = 0.25591909885406494 + 1.0 * 6.1000542640686035
Epoch 570, val loss: 0.739192545413971
Epoch 580, training loss: 6.339766025543213 = 0.24369214475154877 + 1.0 * 6.096074104309082
Epoch 580, val loss: 0.7397008538246155
Epoch 590, training loss: 6.326167583465576 = 0.231840580701828 + 1.0 * 6.094326972961426
Epoch 590, val loss: 0.7405162453651428
Epoch 600, training loss: 6.327682971954346 = 0.22041355073451996 + 1.0 * 6.107269287109375
Epoch 600, val loss: 0.7416871190071106
Epoch 610, training loss: 6.3025221824646 = 0.2095630019903183 + 1.0 * 6.092959403991699
Epoch 610, val loss: 0.7433170676231384
Epoch 620, training loss: 6.290101528167725 = 0.19918109476566315 + 1.0 * 6.090920448303223
Epoch 620, val loss: 0.7454929351806641
Epoch 630, training loss: 6.27788782119751 = 0.18920616805553436 + 1.0 * 6.088681697845459
Epoch 630, val loss: 0.7479471564292908
Epoch 640, training loss: 6.276546955108643 = 0.17965403199195862 + 1.0 * 6.096892833709717
Epoch 640, val loss: 0.7506930828094482
Epoch 650, training loss: 6.262039661407471 = 0.17060033977031708 + 1.0 * 6.091439247131348
Epoch 650, val loss: 0.7536681294441223
Epoch 660, training loss: 6.2485671043396 = 0.1620442420244217 + 1.0 * 6.086523056030273
Epoch 660, val loss: 0.7572404146194458
Epoch 670, training loss: 6.240243911743164 = 0.1538865566253662 + 1.0 * 6.086357116699219
Epoch 670, val loss: 0.7611427307128906
Epoch 680, training loss: 6.229302406311035 = 0.14614851772785187 + 1.0 * 6.08315372467041
Epoch 680, val loss: 0.7652844190597534
Epoch 690, training loss: 6.219405174255371 = 0.13882367312908173 + 1.0 * 6.0805816650390625
Epoch 690, val loss: 0.7697384357452393
Epoch 700, training loss: 6.225653648376465 = 0.13187122344970703 + 1.0 * 6.093782424926758
Epoch 700, val loss: 0.7742847800254822
Epoch 710, training loss: 6.204648494720459 = 0.12536616623401642 + 1.0 * 6.079282283782959
Epoch 710, val loss: 0.778954803943634
Epoch 720, training loss: 6.197247505187988 = 0.11924612522125244 + 1.0 * 6.078001499176025
Epoch 720, val loss: 0.7840014696121216
Epoch 730, training loss: 6.190414905548096 = 0.1134585589170456 + 1.0 * 6.076956272125244
Epoch 730, val loss: 0.7891849279403687
Epoch 740, training loss: 6.193510055541992 = 0.10799834877252579 + 1.0 * 6.085511684417725
Epoch 740, val loss: 0.7944292426109314
Epoch 750, training loss: 6.178417205810547 = 0.10286778211593628 + 1.0 * 6.075549602508545
Epoch 750, val loss: 0.7996590733528137
Epoch 760, training loss: 6.172158241271973 = 0.0980556383728981 + 1.0 * 6.074102401733398
Epoch 760, val loss: 0.8051719069480896
Epoch 770, training loss: 6.167037010192871 = 0.09351051598787308 + 1.0 * 6.073526382446289
Epoch 770, val loss: 0.810766875743866
Epoch 780, training loss: 6.164533615112305 = 0.08921927213668823 + 1.0 * 6.075314521789551
Epoch 780, val loss: 0.816377580165863
Epoch 790, training loss: 6.155465126037598 = 0.08518767356872559 + 1.0 * 6.070277214050293
Epoch 790, val loss: 0.8220289349555969
Epoch 800, training loss: 6.150736331939697 = 0.08139539510011673 + 1.0 * 6.069340705871582
Epoch 800, val loss: 0.8278605341911316
Epoch 810, training loss: 6.147339344024658 = 0.0778094083070755 + 1.0 * 6.069530010223389
Epoch 810, val loss: 0.8336470127105713
Epoch 820, training loss: 6.1463117599487305 = 0.07442091405391693 + 1.0 * 6.071890830993652
Epoch 820, val loss: 0.839419424533844
Epoch 830, training loss: 6.137577056884766 = 0.07123055309057236 + 1.0 * 6.066346645355225
Epoch 830, val loss: 0.8452057838439941
Epoch 840, training loss: 6.133608341217041 = 0.0682433694601059 + 1.0 * 6.065364837646484
Epoch 840, val loss: 0.8510805368423462
Epoch 850, training loss: 6.129330158233643 = 0.06542058289051056 + 1.0 * 6.063909530639648
Epoch 850, val loss: 0.8569433093070984
Epoch 860, training loss: 6.127658843994141 = 0.06275179237127304 + 1.0 * 6.064907073974609
Epoch 860, val loss: 0.8628368973731995
Epoch 870, training loss: 6.131060600280762 = 0.060238227248191833 + 1.0 * 6.070822238922119
Epoch 870, val loss: 0.8686962723731995
Epoch 880, training loss: 6.118582248687744 = 0.05787058174610138 + 1.0 * 6.060711860656738
Epoch 880, val loss: 0.8744710683822632
Epoch 890, training loss: 6.115753650665283 = 0.055641647428274155 + 1.0 * 6.060111999511719
Epoch 890, val loss: 0.8803756237030029
Epoch 900, training loss: 6.113137722015381 = 0.053519684821367264 + 1.0 * 6.05961799621582
Epoch 900, val loss: 0.8862195014953613
Epoch 910, training loss: 6.114340305328369 = 0.05149882286787033 + 1.0 * 6.062841415405273
Epoch 910, val loss: 0.8920058608055115
Epoch 920, training loss: 6.107276439666748 = 0.049593765288591385 + 1.0 * 6.057682514190674
Epoch 920, val loss: 0.8978004455566406
Epoch 930, training loss: 6.1036458015441895 = 0.047791656106710434 + 1.0 * 6.055854320526123
Epoch 930, val loss: 0.9036045074462891
Epoch 940, training loss: 6.1057820320129395 = 0.04608149081468582 + 1.0 * 6.0597004890441895
Epoch 940, val loss: 0.9094327092170715
Epoch 950, training loss: 6.100241661071777 = 0.044461559504270554 + 1.0 * 6.055779933929443
Epoch 950, val loss: 0.9151655435562134
Epoch 960, training loss: 6.099228858947754 = 0.04292675107717514 + 1.0 * 6.056302070617676
Epoch 960, val loss: 0.9208955764770508
Epoch 970, training loss: 6.097723007202148 = 0.041467923671007156 + 1.0 * 6.056254863739014
Epoch 970, val loss: 0.9266073703765869
Epoch 980, training loss: 6.096578598022461 = 0.04008062556385994 + 1.0 * 6.056498050689697
Epoch 980, val loss: 0.9322983622550964
Epoch 990, training loss: 6.091042995452881 = 0.038763996213674545 + 1.0 * 6.052278995513916
Epoch 990, val loss: 0.9378905892372131
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.1624
Flip ASR: 0.1822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320249557495117 = 1.946398138999939 + 1.0 * 8.373851776123047
Epoch 0, val loss: 1.9547756910324097
Epoch 10, training loss: 10.310583114624023 = 1.9372634887695312 + 1.0 * 8.373319625854492
Epoch 10, val loss: 1.9456255435943604
Epoch 20, training loss: 10.295475006103516 = 1.925809383392334 + 1.0 * 8.36966609954834
Epoch 20, val loss: 1.9336580038070679
Epoch 30, training loss: 10.257381439208984 = 1.9099736213684082 + 1.0 * 8.347408294677734
Epoch 30, val loss: 1.9167792797088623
Epoch 40, training loss: 10.06676959991455 = 1.8895800113677979 + 1.0 * 8.177189826965332
Epoch 40, val loss: 1.8955738544464111
Epoch 50, training loss: 9.23038387298584 = 1.8672714233398438 + 1.0 * 7.363112449645996
Epoch 50, val loss: 1.8725500106811523
Epoch 60, training loss: 8.916876792907715 = 1.848053216934204 + 1.0 * 7.068823337554932
Epoch 60, val loss: 1.8528740406036377
Epoch 70, training loss: 8.656517028808594 = 1.8318378925323486 + 1.0 * 6.824679374694824
Epoch 70, val loss: 1.8368574380874634
Epoch 80, training loss: 8.488712310791016 = 1.8166906833648682 + 1.0 * 6.672021865844727
Epoch 80, val loss: 1.8212307691574097
Epoch 90, training loss: 8.368391990661621 = 1.8018735647201538 + 1.0 * 6.566518783569336
Epoch 90, val loss: 1.8055368661880493
Epoch 100, training loss: 8.279375076293945 = 1.7876784801483154 + 1.0 * 6.491696834564209
Epoch 100, val loss: 1.7900218963623047
Epoch 110, training loss: 8.216985702514648 = 1.7730473279953003 + 1.0 * 6.443938732147217
Epoch 110, val loss: 1.7745016813278198
Epoch 120, training loss: 8.163256645202637 = 1.757035493850708 + 1.0 * 6.40622091293335
Epoch 120, val loss: 1.7580597400665283
Epoch 130, training loss: 8.11378002166748 = 1.7400840520858765 + 1.0 * 6.3736958503723145
Epoch 130, val loss: 1.741333246231079
Epoch 140, training loss: 8.06763744354248 = 1.7216500043869019 + 1.0 * 6.345987319946289
Epoch 140, val loss: 1.7242122888565063
Epoch 150, training loss: 8.023192405700684 = 1.7000142335891724 + 1.0 * 6.323177814483643
Epoch 150, val loss: 1.7055389881134033
Epoch 160, training loss: 7.97785758972168 = 1.6740314960479736 + 1.0 * 6.303826332092285
Epoch 160, val loss: 1.6842612028121948
Epoch 170, training loss: 7.928055763244629 = 1.6429423093795776 + 1.0 * 6.285113334655762
Epoch 170, val loss: 1.6596729755401611
Epoch 180, training loss: 7.87366247177124 = 1.6049500703811646 + 1.0 * 6.268712520599365
Epoch 180, val loss: 1.6300158500671387
Epoch 190, training loss: 7.8135528564453125 = 1.5583655834197998 + 1.0 * 6.255187034606934
Epoch 190, val loss: 1.5936858654022217
Epoch 200, training loss: 7.74787712097168 = 1.5038483142852783 + 1.0 * 6.244028568267822
Epoch 200, val loss: 1.551479458808899
Epoch 210, training loss: 7.673023223876953 = 1.4422255754470825 + 1.0 * 6.23079776763916
Epoch 210, val loss: 1.5037323236465454
Epoch 220, training loss: 7.594993591308594 = 1.3739038705825806 + 1.0 * 6.221089839935303
Epoch 220, val loss: 1.4508662223815918
Epoch 230, training loss: 7.513132572174072 = 1.3016725778579712 + 1.0 * 6.211460113525391
Epoch 230, val loss: 1.3953951597213745
Epoch 240, training loss: 7.431438446044922 = 1.2291656732559204 + 1.0 * 6.202272891998291
Epoch 240, val loss: 1.3402423858642578
Epoch 250, training loss: 7.354140758514404 = 1.1583493947982788 + 1.0 * 6.195791244506836
Epoch 250, val loss: 1.2867313623428345
Epoch 260, training loss: 7.280060291290283 = 1.091619610786438 + 1.0 * 6.188440799713135
Epoch 260, val loss: 1.236900806427002
Epoch 270, training loss: 7.2100090980529785 = 1.0298652648925781 + 1.0 * 6.1801438331604
Epoch 270, val loss: 1.191285252571106
Epoch 280, training loss: 7.1482086181640625 = 0.9723330736160278 + 1.0 * 6.175875663757324
Epoch 280, val loss: 1.149547815322876
Epoch 290, training loss: 7.091482639312744 = 0.9201593995094299 + 1.0 * 6.171323299407959
Epoch 290, val loss: 1.1127442121505737
Epoch 300, training loss: 7.036088943481445 = 0.872667133808136 + 1.0 * 6.163421630859375
Epoch 300, val loss: 1.080398440361023
Epoch 310, training loss: 6.985021591186523 = 0.8283634781837463 + 1.0 * 6.156658172607422
Epoch 310, val loss: 1.0514469146728516
Epoch 320, training loss: 6.956723690032959 = 0.7867254018783569 + 1.0 * 6.1699981689453125
Epoch 320, val loss: 1.025540828704834
Epoch 330, training loss: 6.896705150604248 = 0.7486283183097839 + 1.0 * 6.148077011108398
Epoch 330, val loss: 1.0028806924819946
Epoch 340, training loss: 6.856743812561035 = 0.7127712965011597 + 1.0 * 6.143972396850586
Epoch 340, val loss: 0.9829160571098328
Epoch 350, training loss: 6.817200183868408 = 0.678313136100769 + 1.0 * 6.13888692855835
Epoch 350, val loss: 0.9644092917442322
Epoch 360, training loss: 6.780531406402588 = 0.6449665427207947 + 1.0 * 6.135564804077148
Epoch 360, val loss: 0.9468690156936646
Epoch 370, training loss: 6.745847702026367 = 0.6129524111747742 + 1.0 * 6.132895469665527
Epoch 370, val loss: 0.9304970502853394
Epoch 380, training loss: 6.712477207183838 = 0.5823506712913513 + 1.0 * 6.130126476287842
Epoch 380, val loss: 0.9153124690055847
Epoch 390, training loss: 6.678463935852051 = 0.5525155663490295 + 1.0 * 6.125948429107666
Epoch 390, val loss: 0.9009969234466553
Epoch 400, training loss: 6.65110445022583 = 0.5238675475120544 + 1.0 * 6.127236843109131
Epoch 400, val loss: 0.8877114653587341
Epoch 410, training loss: 6.6153154373168945 = 0.4965845048427582 + 1.0 * 6.1187310218811035
Epoch 410, val loss: 0.8756920099258423
Epoch 420, training loss: 6.5853681564331055 = 0.47035735845565796 + 1.0 * 6.115010738372803
Epoch 420, val loss: 0.8648591041564941
Epoch 430, training loss: 6.571091651916504 = 0.4452892243862152 + 1.0 * 6.125802516937256
Epoch 430, val loss: 0.8552926182746887
Epoch 440, training loss: 6.534517288208008 = 0.42161017656326294 + 1.0 * 6.1129069328308105
Epoch 440, val loss: 0.8473542928695679
Epoch 450, training loss: 6.508337497711182 = 0.39911046624183655 + 1.0 * 6.109227180480957
Epoch 450, val loss: 0.8409008979797363
Epoch 460, training loss: 6.483640193939209 = 0.3776612877845764 + 1.0 * 6.105978965759277
Epoch 460, val loss: 0.8357629776000977
Epoch 470, training loss: 6.468191623687744 = 0.3572342097759247 + 1.0 * 6.110957622528076
Epoch 470, val loss: 0.8320546746253967
Epoch 480, training loss: 6.4425201416015625 = 0.3380424380302429 + 1.0 * 6.104477882385254
Epoch 480, val loss: 0.8295489549636841
Epoch 490, training loss: 6.420230865478516 = 0.31995531916618347 + 1.0 * 6.10027551651001
Epoch 490, val loss: 0.8283244967460632
Epoch 500, training loss: 6.403176307678223 = 0.30287840962409973 + 1.0 * 6.100297927856445
Epoch 500, val loss: 0.8280786871910095
Epoch 510, training loss: 6.384933948516846 = 0.28686031699180603 + 1.0 * 6.098073482513428
Epoch 510, val loss: 0.828677237033844
Epoch 520, training loss: 6.366311550140381 = 0.27186670899391174 + 1.0 * 6.094444751739502
Epoch 520, val loss: 0.8301817774772644
Epoch 530, training loss: 6.350902557373047 = 0.2578592896461487 + 1.0 * 6.093043327331543
Epoch 530, val loss: 0.8324272632598877
Epoch 540, training loss: 6.342348098754883 = 0.24475355446338654 + 1.0 * 6.097594738006592
Epoch 540, val loss: 0.8352093696594238
Epoch 550, training loss: 6.326087951660156 = 0.23264268040657043 + 1.0 * 6.093445301055908
Epoch 550, val loss: 0.8385745882987976
Epoch 560, training loss: 6.3091278076171875 = 0.22134728729724884 + 1.0 * 6.087780475616455
Epoch 560, val loss: 0.8424544930458069
Epoch 570, training loss: 6.300170421600342 = 0.21074740588665009 + 1.0 * 6.089423179626465
Epoch 570, val loss: 0.8467391133308411
Epoch 580, training loss: 6.28695821762085 = 0.20083248615264893 + 1.0 * 6.08612585067749
Epoch 580, val loss: 0.8512310981750488
Epoch 590, training loss: 6.276926517486572 = 0.19157230854034424 + 1.0 * 6.085354328155518
Epoch 590, val loss: 0.8563101291656494
Epoch 600, training loss: 6.26893424987793 = 0.182861328125 + 1.0 * 6.08607292175293
Epoch 600, val loss: 0.8615487217903137
Epoch 610, training loss: 6.257658958435059 = 0.1746678501367569 + 1.0 * 6.082991123199463
Epoch 610, val loss: 0.8670371770858765
Epoch 620, training loss: 6.24774169921875 = 0.166963130235672 + 1.0 * 6.0807785987854
Epoch 620, val loss: 0.872840404510498
Epoch 630, training loss: 6.238061904907227 = 0.1596977710723877 + 1.0 * 6.078364372253418
Epoch 630, val loss: 0.8787345886230469
Epoch 640, training loss: 6.229342460632324 = 0.15282373130321503 + 1.0 * 6.076518535614014
Epoch 640, val loss: 0.8849580883979797
Epoch 650, training loss: 6.224283695220947 = 0.1462670862674713 + 1.0 * 6.078016757965088
Epoch 650, val loss: 0.8912044763565063
Epoch 660, training loss: 6.221844673156738 = 0.1400473415851593 + 1.0 * 6.081797122955322
Epoch 660, val loss: 0.8974137902259827
Epoch 670, training loss: 6.210139274597168 = 0.1341867297887802 + 1.0 * 6.075952529907227
Epoch 670, val loss: 0.9037818908691406
Epoch 680, training loss: 6.200790882110596 = 0.12862609326839447 + 1.0 * 6.072165012359619
Epoch 680, val loss: 0.910394012928009
Epoch 690, training loss: 6.1931047439575195 = 0.12328697741031647 + 1.0 * 6.069817543029785
Epoch 690, val loss: 0.9170470237731934
Epoch 700, training loss: 6.1915740966796875 = 0.11817451566457748 + 1.0 * 6.073399543762207
Epoch 700, val loss: 0.9238273501396179
Epoch 710, training loss: 6.1865153312683105 = 0.11328712105751038 + 1.0 * 6.073228359222412
Epoch 710, val loss: 0.930645227432251
Epoch 720, training loss: 6.17812967300415 = 0.10859764367341995 + 1.0 * 6.0695319175720215
Epoch 720, val loss: 0.9375980496406555
Epoch 730, training loss: 6.170577049255371 = 0.10411296784877777 + 1.0 * 6.066463947296143
Epoch 730, val loss: 0.9445249438285828
Epoch 740, training loss: 6.164215087890625 = 0.09979916363954544 + 1.0 * 6.06441593170166
Epoch 740, val loss: 0.9515923261642456
Epoch 750, training loss: 6.159191608428955 = 0.0956416130065918 + 1.0 * 6.063549995422363
Epoch 750, val loss: 0.9587410092353821
Epoch 760, training loss: 6.168701648712158 = 0.09162548184394836 + 1.0 * 6.077075958251953
Epoch 760, val loss: 0.965741753578186
Epoch 770, training loss: 6.152170658111572 = 0.0877968966960907 + 1.0 * 6.064373970031738
Epoch 770, val loss: 0.9728432893753052
Epoch 780, training loss: 6.1464009284973145 = 0.08413799107074738 + 1.0 * 6.062263011932373
Epoch 780, val loss: 0.9801669716835022
Epoch 790, training loss: 6.1400556564331055 = 0.08061391860246658 + 1.0 * 6.059441566467285
Epoch 790, val loss: 0.9874652028083801
Epoch 800, training loss: 6.1470513343811035 = 0.07721257954835892 + 1.0 * 6.069838523864746
Epoch 800, val loss: 0.9947797060012817
Epoch 810, training loss: 6.135634899139404 = 0.0739610493183136 + 1.0 * 6.061673641204834
Epoch 810, val loss: 1.0020219087600708
Epoch 820, training loss: 6.131389617919922 = 0.07083283364772797 + 1.0 * 6.060556888580322
Epoch 820, val loss: 1.0094407796859741
Epoch 830, training loss: 6.125104904174805 = 0.0678192600607872 + 1.0 * 6.057285785675049
Epoch 830, val loss: 1.016790747642517
Epoch 840, training loss: 6.1196608543396 = 0.06492406129837036 + 1.0 * 6.054736614227295
Epoch 840, val loss: 1.0242564678192139
Epoch 850, training loss: 6.118431568145752 = 0.06214911863207817 + 1.0 * 6.0562825202941895
Epoch 850, val loss: 1.0318259000778198
Epoch 860, training loss: 6.112850666046143 = 0.0595257431268692 + 1.0 * 6.0533246994018555
Epoch 860, val loss: 1.0393292903900146
Epoch 870, training loss: 6.110877990722656 = 0.05706784501671791 + 1.0 * 6.053810119628906
Epoch 870, val loss: 1.0471367835998535
Epoch 880, training loss: 6.119650840759277 = 0.05476628243923187 + 1.0 * 6.064884662628174
Epoch 880, val loss: 1.0548617839813232
Epoch 890, training loss: 6.108259201049805 = 0.05263461917638779 + 1.0 * 6.055624485015869
Epoch 890, val loss: 1.0627703666687012
Epoch 900, training loss: 6.1014084815979 = 0.05064428970217705 + 1.0 * 6.050764083862305
Epoch 900, val loss: 1.0710101127624512
Epoch 910, training loss: 6.098008632659912 = 0.048761527985334396 + 1.0 * 6.0492472648620605
Epoch 910, val loss: 1.0792596340179443
Epoch 920, training loss: 6.095963954925537 = 0.04698256030678749 + 1.0 * 6.048981189727783
Epoch 920, val loss: 1.0875872373580933
Epoch 930, training loss: 6.101850509643555 = 0.04530271887779236 + 1.0 * 6.05654764175415
Epoch 930, val loss: 1.0958272218704224
Epoch 940, training loss: 6.0927252769470215 = 0.04371855407953262 + 1.0 * 6.049006938934326
Epoch 940, val loss: 1.1041086912155151
Epoch 950, training loss: 6.091001987457275 = 0.04222190007567406 + 1.0 * 6.0487799644470215
Epoch 950, val loss: 1.112460970878601
Epoch 960, training loss: 6.087338924407959 = 0.04079820588231087 + 1.0 * 6.0465407371521
Epoch 960, val loss: 1.1208029985427856
Epoch 970, training loss: 6.090257167816162 = 0.039441950619220734 + 1.0 * 6.050815105438232
Epoch 970, val loss: 1.1290339231491089
Epoch 980, training loss: 6.083359241485596 = 0.03814901039004326 + 1.0 * 6.045210361480713
Epoch 980, val loss: 1.1371527910232544
Epoch 990, training loss: 6.083104133605957 = 0.0369226336479187 + 1.0 * 6.046181678771973
Epoch 990, val loss: 1.1452903747558594
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8487
Flip ASR: 0.8178/225 nodes
The final ASR:0.52645, 0.28176, Accuracy:0.80864, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9548])
updated graph: torch.Size([2, 10610])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98647, 0.00627, Accuracy:0.83210, 0.00175
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.338266372680664 = 1.9643807411193848 + 1.0 * 8.373885154724121
Epoch 0, val loss: 1.9640774726867676
Epoch 10, training loss: 10.32713794708252 = 1.9536610841751099 + 1.0 * 8.3734769821167
Epoch 10, val loss: 1.9534690380096436
Epoch 20, training loss: 10.310674667358398 = 1.9399973154067993 + 1.0 * 8.37067699432373
Epoch 20, val loss: 1.9398930072784424
Epoch 30, training loss: 10.26926040649414 = 1.920436978340149 + 1.0 * 8.348823547363281
Epoch 30, val loss: 1.920582890510559
Epoch 40, training loss: 10.074077606201172 = 1.8944388628005981 + 1.0 * 8.179638862609863
Epoch 40, val loss: 1.8955676555633545
Epoch 50, training loss: 9.440577507019043 = 1.8676426410675049 + 1.0 * 7.572935104370117
Epoch 50, val loss: 1.8704439401626587
Epoch 60, training loss: 8.964425086975098 = 1.8493033647537231 + 1.0 * 7.115121364593506
Epoch 60, val loss: 1.854472041130066
Epoch 70, training loss: 8.64879322052002 = 1.833978295326233 + 1.0 * 6.814814567565918
Epoch 70, val loss: 1.8409029245376587
Epoch 80, training loss: 8.464187622070312 = 1.8184170722961426 + 1.0 * 6.64577054977417
Epoch 80, val loss: 1.8272227048873901
Epoch 90, training loss: 8.323203086853027 = 1.8010879755020142 + 1.0 * 6.522115230560303
Epoch 90, val loss: 1.8126803636550903
Epoch 100, training loss: 8.239395141601562 = 1.7832810878753662 + 1.0 * 6.456114292144775
Epoch 100, val loss: 1.7979615926742554
Epoch 110, training loss: 8.174690246582031 = 1.765084981918335 + 1.0 * 6.409605026245117
Epoch 110, val loss: 1.7825899124145508
Epoch 120, training loss: 8.119796752929688 = 1.7461447715759277 + 1.0 * 6.373651504516602
Epoch 120, val loss: 1.7663500308990479
Epoch 130, training loss: 8.070453643798828 = 1.7255516052246094 + 1.0 * 6.344902038574219
Epoch 130, val loss: 1.7486289739608765
Epoch 140, training loss: 8.022662162780762 = 1.7021294832229614 + 1.0 * 6.32053279876709
Epoch 140, val loss: 1.7286816835403442
Epoch 150, training loss: 7.973583221435547 = 1.6753251552581787 + 1.0 * 6.298257827758789
Epoch 150, val loss: 1.7060987949371338
Epoch 160, training loss: 7.923807621002197 = 1.6445560455322266 + 1.0 * 6.279251575469971
Epoch 160, val loss: 1.6802825927734375
Epoch 170, training loss: 7.873604774475098 = 1.6091865301132202 + 1.0 * 6.264418125152588
Epoch 170, val loss: 1.6506145000457764
Epoch 180, training loss: 7.818166255950928 = 1.569108486175537 + 1.0 * 6.249057769775391
Epoch 180, val loss: 1.6169722080230713
Epoch 190, training loss: 7.761499404907227 = 1.524604082107544 + 1.0 * 6.2368950843811035
Epoch 190, val loss: 1.5794967412948608
Epoch 200, training loss: 7.70177698135376 = 1.4758888483047485 + 1.0 * 6.225888252258301
Epoch 200, val loss: 1.5383150577545166
Epoch 210, training loss: 7.64346981048584 = 1.4236276149749756 + 1.0 * 6.219842433929443
Epoch 210, val loss: 1.4942055940628052
Epoch 220, training loss: 7.57853889465332 = 1.369389295578003 + 1.0 * 6.2091498374938965
Epoch 220, val loss: 1.4486793279647827
Epoch 230, training loss: 7.517083168029785 = 1.3144502639770508 + 1.0 * 6.202632904052734
Epoch 230, val loss: 1.403075098991394
Epoch 240, training loss: 7.455245494842529 = 1.2601776123046875 + 1.0 * 6.195067882537842
Epoch 240, val loss: 1.3587064743041992
Epoch 250, training loss: 7.396150588989258 = 1.2072113752365112 + 1.0 * 6.188939094543457
Epoch 250, val loss: 1.3161256313323975
Epoch 260, training loss: 7.339685916900635 = 1.1556472778320312 + 1.0 * 6.1840386390686035
Epoch 260, val loss: 1.2756541967391968
Epoch 270, training loss: 7.286518096923828 = 1.1065233945846558 + 1.0 * 6.179994583129883
Epoch 270, val loss: 1.2378785610198975
Epoch 280, training loss: 7.233658790588379 = 1.0599578619003296 + 1.0 * 6.17370080947876
Epoch 280, val loss: 1.2030456066131592
Epoch 290, training loss: 7.184328556060791 = 1.015763282775879 + 1.0 * 6.168565273284912
Epoch 290, val loss: 1.1706328392028809
Epoch 300, training loss: 7.139386177062988 = 0.9734194278717041 + 1.0 * 6.165966987609863
Epoch 300, val loss: 1.1400970220565796
Epoch 310, training loss: 7.0934977531433105 = 0.9327635765075684 + 1.0 * 6.160734176635742
Epoch 310, val loss: 1.1110446453094482
Epoch 320, training loss: 7.049693584442139 = 0.893297553062439 + 1.0 * 6.15639591217041
Epoch 320, val loss: 1.0832281112670898
Epoch 330, training loss: 7.011375427246094 = 0.8546153903007507 + 1.0 * 6.156760215759277
Epoch 330, val loss: 1.0562609434127808
Epoch 340, training loss: 6.967302322387695 = 0.8168831467628479 + 1.0 * 6.150419235229492
Epoch 340, val loss: 1.0299351215362549
Epoch 350, training loss: 6.925657749176025 = 0.7797167897224426 + 1.0 * 6.145940780639648
Epoch 350, val loss: 1.0041460990905762
Epoch 360, training loss: 6.893719673156738 = 0.7430388927459717 + 1.0 * 6.150681018829346
Epoch 360, val loss: 0.9787021279335022
Epoch 370, training loss: 6.847356796264648 = 0.7073099613189697 + 1.0 * 6.1400465965271
Epoch 370, val loss: 0.9540170431137085
Epoch 380, training loss: 6.809025287628174 = 0.6724885106086731 + 1.0 * 6.136536598205566
Epoch 380, val loss: 0.9302343726158142
Epoch 390, training loss: 6.777825832366943 = 0.6386560797691345 + 1.0 * 6.139169692993164
Epoch 390, val loss: 0.9071704745292664
Epoch 400, training loss: 6.740044593811035 = 0.606318473815918 + 1.0 * 6.133726119995117
Epoch 400, val loss: 0.8852758407592773
Epoch 410, training loss: 6.706610679626465 = 0.5754534006118774 + 1.0 * 6.131157398223877
Epoch 410, val loss: 0.8648207783699036
Epoch 420, training loss: 6.673369407653809 = 0.5462764501571655 + 1.0 * 6.1270928382873535
Epoch 420, val loss: 0.8458921313285828
Epoch 430, training loss: 6.6423211097717285 = 0.5186701416969299 + 1.0 * 6.123651027679443
Epoch 430, val loss: 0.8285159468650818
Epoch 440, training loss: 6.6151933670043945 = 0.49257898330688477 + 1.0 * 6.12261438369751
Epoch 440, val loss: 0.8126437664031982
Epoch 450, training loss: 6.586206436157227 = 0.467843234539032 + 1.0 * 6.118363380432129
Epoch 450, val loss: 0.7982248663902283
Epoch 460, training loss: 6.563040256500244 = 0.44435641169548035 + 1.0 * 6.118683815002441
Epoch 460, val loss: 0.7851167917251587
Epoch 470, training loss: 6.541775226593018 = 0.4219471216201782 + 1.0 * 6.119828224182129
Epoch 470, val loss: 0.7732530832290649
Epoch 480, training loss: 6.51405668258667 = 0.40066230297088623 + 1.0 * 6.113394260406494
Epoch 480, val loss: 0.7623629570007324
Epoch 490, training loss: 6.488775253295898 = 0.3800845742225647 + 1.0 * 6.1086907386779785
Epoch 490, val loss: 0.752502977848053
Epoch 500, training loss: 6.467249393463135 = 0.36008089780807495 + 1.0 * 6.107168674468994
Epoch 500, val loss: 0.7433401346206665
Epoch 510, training loss: 6.446877479553223 = 0.34065011143684387 + 1.0 * 6.106227397918701
Epoch 510, val loss: 0.734731912612915
Epoch 520, training loss: 6.432156085968018 = 0.3218766748905182 + 1.0 * 6.110279560089111
Epoch 520, val loss: 0.7267416715621948
Epoch 530, training loss: 6.405862331390381 = 0.30372920632362366 + 1.0 * 6.102133274078369
Epoch 530, val loss: 0.7193892598152161
Epoch 540, training loss: 6.3857879638671875 = 0.2861199676990509 + 1.0 * 6.099668025970459
Epoch 540, val loss: 0.7125481367111206
Epoch 550, training loss: 6.367262840270996 = 0.2690705358982086 + 1.0 * 6.09819221496582
Epoch 550, val loss: 0.7062315344810486
Epoch 560, training loss: 6.348740577697754 = 0.2526978850364685 + 1.0 * 6.096042633056641
Epoch 560, val loss: 0.7004093527793884
Epoch 570, training loss: 6.3351149559021 = 0.23705479502677917 + 1.0 * 6.098060131072998
Epoch 570, val loss: 0.6954207420349121
Epoch 580, training loss: 6.31518030166626 = 0.2221788465976715 + 1.0 * 6.093001365661621
Epoch 580, val loss: 0.6911137700080872
Epoch 590, training loss: 6.306058406829834 = 0.20804090797901154 + 1.0 * 6.098017692565918
Epoch 590, val loss: 0.6874374747276306
Epoch 600, training loss: 6.284951210021973 = 0.19477836787700653 + 1.0 * 6.09017276763916
Epoch 600, val loss: 0.6844225525856018
Epoch 610, training loss: 6.269687175750732 = 0.18225711584091187 + 1.0 * 6.087430000305176
Epoch 610, val loss: 0.6822495460510254
Epoch 620, training loss: 6.257923603057861 = 0.17049634456634521 + 1.0 * 6.087427139282227
Epoch 620, val loss: 0.680676281452179
Epoch 630, training loss: 6.252169609069824 = 0.15951766073703766 + 1.0 * 6.092651844024658
Epoch 630, val loss: 0.6797520518302917
Epoch 640, training loss: 6.2359747886657715 = 0.14935319125652313 + 1.0 * 6.0866217613220215
Epoch 640, val loss: 0.6795169115066528
Epoch 650, training loss: 6.2216105461120605 = 0.139887273311615 + 1.0 * 6.081723213195801
Epoch 650, val loss: 0.6800206899642944
Epoch 660, training loss: 6.219924449920654 = 0.13103923201560974 + 1.0 * 6.088885307312012
Epoch 660, val loss: 0.6811420321464539
Epoch 670, training loss: 6.2082929611206055 = 0.12289583683013916 + 1.0 * 6.085397243499756
Epoch 670, val loss: 0.6825821399688721
Epoch 680, training loss: 6.197015762329102 = 0.11531636863946915 + 1.0 * 6.081699371337891
Epoch 680, val loss: 0.6846888661384583
Epoch 690, training loss: 6.186716079711914 = 0.10828614979982376 + 1.0 * 6.078429698944092
Epoch 690, val loss: 0.6873538494110107
Epoch 700, training loss: 6.177101135253906 = 0.10175217688083649 + 1.0 * 6.075348854064941
Epoch 700, val loss: 0.6904671788215637
Epoch 710, training loss: 6.18278169631958 = 0.09566936641931534 + 1.0 * 6.0871124267578125
Epoch 710, val loss: 0.6939802169799805
Epoch 720, training loss: 6.165546417236328 = 0.09003963321447372 + 1.0 * 6.075506687164307
Epoch 720, val loss: 0.6977240443229675
Epoch 730, training loss: 6.1592888832092285 = 0.08483424037694931 + 1.0 * 6.0744547843933105
Epoch 730, val loss: 0.7017449140548706
Epoch 740, training loss: 6.154500961303711 = 0.08000719547271729 + 1.0 * 6.074493885040283
Epoch 740, val loss: 0.7060965299606323
Epoch 750, training loss: 6.149401664733887 = 0.075536347925663 + 1.0 * 6.0738654136657715
Epoch 750, val loss: 0.7106557488441467
Epoch 760, training loss: 6.142343997955322 = 0.07140142470598221 + 1.0 * 6.070942401885986
Epoch 760, val loss: 0.7152948975563049
Epoch 770, training loss: 6.137219429016113 = 0.06757199019193649 + 1.0 * 6.069647312164307
Epoch 770, val loss: 0.7200947999954224
Epoch 780, training loss: 6.135797023773193 = 0.06401946395635605 + 1.0 * 6.07177734375
Epoch 780, val loss: 0.724975049495697
Epoch 790, training loss: 6.128852367401123 = 0.06072750687599182 + 1.0 * 6.068124771118164
Epoch 790, val loss: 0.729934573173523
Epoch 800, training loss: 6.1275410652160645 = 0.0576864592730999 + 1.0 * 6.069854736328125
Epoch 800, val loss: 0.7348222732543945
Epoch 810, training loss: 6.122698783874512 = 0.05486209690570831 + 1.0 * 6.067836761474609
Epoch 810, val loss: 0.7397755980491638
Epoch 820, training loss: 6.114984035491943 = 0.052236977964639664 + 1.0 * 6.062747001647949
Epoch 820, val loss: 0.7447241544723511
Epoch 830, training loss: 6.112608432769775 = 0.04978472366929054 + 1.0 * 6.06282377243042
Epoch 830, val loss: 0.749701738357544
Epoch 840, training loss: 6.117343902587891 = 0.04749186336994171 + 1.0 * 6.069851875305176
Epoch 840, val loss: 0.7546281814575195
Epoch 850, training loss: 6.108044624328613 = 0.045355744659900665 + 1.0 * 6.062688827514648
Epoch 850, val loss: 0.75948566198349
Epoch 860, training loss: 6.102242469787598 = 0.04336277395486832 + 1.0 * 6.058879852294922
Epoch 860, val loss: 0.7643260955810547
Epoch 870, training loss: 6.105198383331299 = 0.04148930311203003 + 1.0 * 6.063709259033203
Epoch 870, val loss: 0.7692333459854126
Epoch 880, training loss: 6.100348949432373 = 0.039731163531541824 + 1.0 * 6.060617923736572
Epoch 880, val loss: 0.7740120887756348
Epoch 890, training loss: 6.1022210121154785 = 0.03808722272515297 + 1.0 * 6.064133644104004
Epoch 890, val loss: 0.7786991596221924
Epoch 900, training loss: 6.093560218811035 = 0.036543671041727066 + 1.0 * 6.057016372680664
Epoch 900, val loss: 0.7834012508392334
Epoch 910, training loss: 6.09028434753418 = 0.035084422677755356 + 1.0 * 6.055200099945068
Epoch 910, val loss: 0.7881321907043457
Epoch 920, training loss: 6.088411808013916 = 0.03370317444205284 + 1.0 * 6.054708480834961
Epoch 920, val loss: 0.792832612991333
Epoch 930, training loss: 6.094316482543945 = 0.0323973186314106 + 1.0 * 6.061919212341309
Epoch 930, val loss: 0.7974609136581421
Epoch 940, training loss: 6.095580101013184 = 0.031175797805190086 + 1.0 * 6.064404487609863
Epoch 940, val loss: 0.8019499182701111
Epoch 950, training loss: 6.082441329956055 = 0.03002196177840233 + 1.0 * 6.052419185638428
Epoch 950, val loss: 0.8063871264457703
Epoch 960, training loss: 6.080996513366699 = 0.02893136627972126 + 1.0 * 6.052065372467041
Epoch 960, val loss: 0.8108547925949097
Epoch 970, training loss: 6.078319072723389 = 0.027893133461475372 + 1.0 * 6.050426006317139
Epoch 970, val loss: 0.8153514266014099
Epoch 980, training loss: 6.084036350250244 = 0.026905758306384087 + 1.0 * 6.057130813598633
Epoch 980, val loss: 0.8197780251502991
Epoch 990, training loss: 6.078000545501709 = 0.025968749076128006 + 1.0 * 6.052031993865967
Epoch 990, val loss: 0.8240618109703064
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6937
Flip ASR: 0.6400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.313360214233398 = 1.9394927024841309 + 1.0 * 8.373867988586426
Epoch 0, val loss: 1.9330779314041138
Epoch 10, training loss: 10.303425788879395 = 1.929883599281311 + 1.0 * 8.373541831970215
Epoch 10, val loss: 1.9238744974136353
Epoch 20, training loss: 10.289297103881836 = 1.9180541038513184 + 1.0 * 8.37124252319336
Epoch 20, val loss: 1.912149429321289
Epoch 30, training loss: 10.25651741027832 = 1.9013521671295166 + 1.0 * 8.355165481567383
Epoch 30, val loss: 1.895295262336731
Epoch 40, training loss: 10.137147903442383 = 1.878771424293518 + 1.0 * 8.258376121520996
Epoch 40, val loss: 1.8733208179473877
Epoch 50, training loss: 9.694790840148926 = 1.8535183668136597 + 1.0 * 7.841272830963135
Epoch 50, val loss: 1.8493387699127197
Epoch 60, training loss: 9.30757999420166 = 1.8281770944595337 + 1.0 * 7.479403018951416
Epoch 60, val loss: 1.8257715702056885
Epoch 70, training loss: 8.895565032958984 = 1.809093713760376 + 1.0 * 7.0864715576171875
Epoch 70, val loss: 1.808318853378296
Epoch 80, training loss: 8.650421142578125 = 1.7929738759994507 + 1.0 * 6.857447147369385
Epoch 80, val loss: 1.7932018041610718
Epoch 90, training loss: 8.481257438659668 = 1.7741577625274658 + 1.0 * 6.707099437713623
Epoch 90, val loss: 1.7762422561645508
Epoch 100, training loss: 8.3515043258667 = 1.7543423175811768 + 1.0 * 6.597161769866943
Epoch 100, val loss: 1.7593971490859985
Epoch 110, training loss: 8.257258415222168 = 1.7353218793869019 + 1.0 * 6.521936893463135
Epoch 110, val loss: 1.7429908514022827
Epoch 120, training loss: 8.178887367248535 = 1.7140278816223145 + 1.0 * 6.464859485626221
Epoch 120, val loss: 1.724421739578247
Epoch 130, training loss: 8.10969066619873 = 1.6887682676315308 + 1.0 * 6.420922756195068
Epoch 130, val loss: 1.7026727199554443
Epoch 140, training loss: 8.042403221130371 = 1.6592686176300049 + 1.0 * 6.383134365081787
Epoch 140, val loss: 1.6777406930923462
Epoch 150, training loss: 7.9760422706604 = 1.624390959739685 + 1.0 * 6.351651191711426
Epoch 150, val loss: 1.648738980293274
Epoch 160, training loss: 7.908134460449219 = 1.5837092399597168 + 1.0 * 6.324425220489502
Epoch 160, val loss: 1.6152328252792358
Epoch 170, training loss: 7.839724063873291 = 1.5371233224868774 + 1.0 * 6.302600860595703
Epoch 170, val loss: 1.5769795179367065
Epoch 180, training loss: 7.773841381072998 = 1.4853488206863403 + 1.0 * 6.288492679595947
Epoch 180, val loss: 1.5349066257476807
Epoch 190, training loss: 7.703413009643555 = 1.431546688079834 + 1.0 * 6.271866321563721
Epoch 190, val loss: 1.4917266368865967
Epoch 200, training loss: 7.635453224182129 = 1.3772273063659668 + 1.0 * 6.258225917816162
Epoch 200, val loss: 1.448815941810608
Epoch 210, training loss: 7.570847988128662 = 1.3237547874450684 + 1.0 * 6.247093200683594
Epoch 210, val loss: 1.4075819253921509
Epoch 220, training loss: 7.511167526245117 = 1.2728523015975952 + 1.0 * 6.238315105438232
Epoch 220, val loss: 1.3693041801452637
Epoch 230, training loss: 7.454880237579346 = 1.2248296737670898 + 1.0 * 6.230050563812256
Epoch 230, val loss: 1.3339804410934448
Epoch 240, training loss: 7.401793956756592 = 1.17889404296875 + 1.0 * 6.222899913787842
Epoch 240, val loss: 1.300532579421997
Epoch 250, training loss: 7.348362922668457 = 1.134507179260254 + 1.0 * 6.213855743408203
Epoch 250, val loss: 1.268526554107666
Epoch 260, training loss: 7.298371315002441 = 1.0909740924835205 + 1.0 * 6.2073974609375
Epoch 260, val loss: 1.2374638319015503
Epoch 270, training loss: 7.2504425048828125 = 1.0478166341781616 + 1.0 * 6.202625751495361
Epoch 270, val loss: 1.2065913677215576
Epoch 280, training loss: 7.200242042541504 = 1.0053319931030273 + 1.0 * 6.194910049438477
Epoch 280, val loss: 1.176099419593811
Epoch 290, training loss: 7.152119159698486 = 0.9634070992469788 + 1.0 * 6.188712120056152
Epoch 290, val loss: 1.1460050344467163
Epoch 300, training loss: 7.106376647949219 = 0.9218026995658875 + 1.0 * 6.184574127197266
Epoch 300, val loss: 1.1159367561340332
Epoch 310, training loss: 7.060622215270996 = 0.8808817863464355 + 1.0 * 6.1797404289245605
Epoch 310, val loss: 1.0863854885101318
Epoch 320, training loss: 7.0138444900512695 = 0.8408583998680115 + 1.0 * 6.172986030578613
Epoch 320, val loss: 1.0575950145721436
Epoch 330, training loss: 6.972023963928223 = 0.8015701770782471 + 1.0 * 6.1704535484313965
Epoch 330, val loss: 1.0293793678283691
Epoch 340, training loss: 6.928961753845215 = 0.7636021971702576 + 1.0 * 6.1653594970703125
Epoch 340, val loss: 1.0021294355392456
Epoch 350, training loss: 6.892335414886475 = 0.7271939516067505 + 1.0 * 6.165141582489014
Epoch 350, val loss: 0.9763075113296509
Epoch 360, training loss: 6.848045349121094 = 0.692659854888916 + 1.0 * 6.155385494232178
Epoch 360, val loss: 0.9519829750061035
Epoch 370, training loss: 6.811491012573242 = 0.6597011089324951 + 1.0 * 6.151790142059326
Epoch 370, val loss: 0.9291523694992065
Epoch 380, training loss: 6.779499530792236 = 0.6282581686973572 + 1.0 * 6.151241302490234
Epoch 380, val loss: 0.9078664183616638
Epoch 390, training loss: 6.744176387786865 = 0.5985836386680603 + 1.0 * 6.14559268951416
Epoch 390, val loss: 0.8884053230285645
Epoch 400, training loss: 6.715571880340576 = 0.5704978704452515 + 1.0 * 6.145073890686035
Epoch 400, val loss: 0.8704299330711365
Epoch 410, training loss: 6.684407711029053 = 0.5439561605453491 + 1.0 * 6.140451431274414
Epoch 410, val loss: 0.853801429271698
Epoch 420, training loss: 6.654204368591309 = 0.5187902450561523 + 1.0 * 6.135414123535156
Epoch 420, val loss: 0.838394045829773
Epoch 430, training loss: 6.627349853515625 = 0.49468010663986206 + 1.0 * 6.132669925689697
Epoch 430, val loss: 0.8240205645561218
Epoch 440, training loss: 6.6069841384887695 = 0.4715518057346344 + 1.0 * 6.135432243347168
Epoch 440, val loss: 0.8105441927909851
Epoch 450, training loss: 6.580508708953857 = 0.4495479464530945 + 1.0 * 6.130960941314697
Epoch 450, val loss: 0.7980930805206299
Epoch 460, training loss: 6.555115222930908 = 0.42846646904945374 + 1.0 * 6.126648902893066
Epoch 460, val loss: 0.7864270210266113
Epoch 470, training loss: 6.530808925628662 = 0.4080800414085388 + 1.0 * 6.1227288246154785
Epoch 470, val loss: 0.7755005359649658
Epoch 480, training loss: 6.5221452713012695 = 0.3882893919944763 + 1.0 * 6.133855819702148
Epoch 480, val loss: 0.7652260065078735
Epoch 490, training loss: 6.491227626800537 = 0.36918604373931885 + 1.0 * 6.122041702270508
Epoch 490, val loss: 0.7555515766143799
Epoch 500, training loss: 6.466424942016602 = 0.35059431195259094 + 1.0 * 6.115830421447754
Epoch 500, val loss: 0.7463880777359009
Epoch 510, training loss: 6.447799205780029 = 0.33238694071769714 + 1.0 * 6.11541223526001
Epoch 510, val loss: 0.737684965133667
Epoch 520, training loss: 6.434478759765625 = 0.314626008272171 + 1.0 * 6.119852542877197
Epoch 520, val loss: 0.7294018268585205
Epoch 530, training loss: 6.409771919250488 = 0.29735541343688965 + 1.0 * 6.112416744232178
Epoch 530, val loss: 0.7214770913124084
Epoch 540, training loss: 6.389185428619385 = 0.2805067300796509 + 1.0 * 6.108678817749023
Epoch 540, val loss: 0.7140803933143616
Epoch 550, training loss: 6.374692916870117 = 0.2641260623931885 + 1.0 * 6.110567092895508
Epoch 550, val loss: 0.7071904540061951
Epoch 560, training loss: 6.3562541007995605 = 0.2483188509941101 + 1.0 * 6.107935428619385
Epoch 560, val loss: 0.700756847858429
Epoch 570, training loss: 6.337657928466797 = 0.23315367102622986 + 1.0 * 6.104504108428955
Epoch 570, val loss: 0.6949259042739868
Epoch 580, training loss: 6.326524257659912 = 0.2186296582221985 + 1.0 * 6.107894420623779
Epoch 580, val loss: 0.6896551847457886
Epoch 590, training loss: 6.306511878967285 = 0.2049400508403778 + 1.0 * 6.101572036743164
Epoch 590, val loss: 0.6851915121078491
Epoch 600, training loss: 6.292128562927246 = 0.19205313920974731 + 1.0 * 6.1000752449035645
Epoch 600, val loss: 0.6814471483230591
Epoch 610, training loss: 6.278316020965576 = 0.18000560998916626 + 1.0 * 6.098310470581055
Epoch 610, val loss: 0.678412914276123
Epoch 620, training loss: 6.264561176300049 = 0.16874855756759644 + 1.0 * 6.095812797546387
Epoch 620, val loss: 0.6761459112167358
Epoch 630, training loss: 6.256717205047607 = 0.15823566913604736 + 1.0 * 6.09848165512085
Epoch 630, val loss: 0.6746229529380798
Epoch 640, training loss: 6.245108604431152 = 0.1484997719526291 + 1.0 * 6.096608638763428
Epoch 640, val loss: 0.6738880276679993
Epoch 650, training loss: 6.231579303741455 = 0.13944987952709198 + 1.0 * 6.092129230499268
Epoch 650, val loss: 0.6735987067222595
Epoch 660, training loss: 6.2221198081970215 = 0.1310267448425293 + 1.0 * 6.091093063354492
Epoch 660, val loss: 0.6740399599075317
Epoch 670, training loss: 6.213681221008301 = 0.12321167439222336 + 1.0 * 6.0904693603515625
Epoch 670, val loss: 0.6751129031181335
Epoch 680, training loss: 6.205193519592285 = 0.11599026620388031 + 1.0 * 6.089203357696533
Epoch 680, val loss: 0.676527738571167
Epoch 690, training loss: 6.196309566497803 = 0.10926112532615662 + 1.0 * 6.087048530578613
Epoch 690, val loss: 0.6784395575523376
Epoch 700, training loss: 6.192166328430176 = 0.10296446084976196 + 1.0 * 6.089201927185059
Epoch 700, val loss: 0.6808561086654663
Epoch 710, training loss: 6.181573390960693 = 0.09707224369049072 + 1.0 * 6.084501266479492
Epoch 710, val loss: 0.6835963726043701
Epoch 720, training loss: 6.174646854400635 = 0.09157223999500275 + 1.0 * 6.083074569702148
Epoch 720, val loss: 0.6865984201431274
Epoch 730, training loss: 6.170382976531982 = 0.08641207218170166 + 1.0 * 6.08397102355957
Epoch 730, val loss: 0.6899043917655945
Epoch 740, training loss: 6.164746284484863 = 0.08161615580320358 + 1.0 * 6.083130359649658
Epoch 740, val loss: 0.6933979392051697
Epoch 750, training loss: 6.158063888549805 = 0.07711604982614517 + 1.0 * 6.0809478759765625
Epoch 750, val loss: 0.6971662640571594
Epoch 760, training loss: 6.151035785675049 = 0.07290974259376526 + 1.0 * 6.078125953674316
Epoch 760, val loss: 0.7010526061058044
Epoch 770, training loss: 6.148042678833008 = 0.06896436959505081 + 1.0 * 6.079078197479248
Epoch 770, val loss: 0.7051790356636047
Epoch 780, training loss: 6.142832279205322 = 0.06528893858194351 + 1.0 * 6.077543258666992
Epoch 780, val loss: 0.7093848586082458
Epoch 790, training loss: 6.138258457183838 = 0.061867788434028625 + 1.0 * 6.076390743255615
Epoch 790, val loss: 0.7136269807815552
Epoch 800, training loss: 6.141363620758057 = 0.05869390442967415 + 1.0 * 6.082669734954834
Epoch 800, val loss: 0.7179844975471497
Epoch 810, training loss: 6.128920078277588 = 0.05575315281748772 + 1.0 * 6.073166847229004
Epoch 810, val loss: 0.7222958207130432
Epoch 820, training loss: 6.125668048858643 = 0.05302787199616432 + 1.0 * 6.0726399421691895
Epoch 820, val loss: 0.7266502380371094
Epoch 830, training loss: 6.123443126678467 = 0.05048804730176926 + 1.0 * 6.072955131530762
Epoch 830, val loss: 0.7310935258865356
Epoch 840, training loss: 6.118284225463867 = 0.04813579097390175 + 1.0 * 6.070148468017578
Epoch 840, val loss: 0.7355688214302063
Epoch 850, training loss: 6.1153717041015625 = 0.04594588652253151 + 1.0 * 6.069425582885742
Epoch 850, val loss: 0.7398588061332703
Epoch 860, training loss: 6.121281147003174 = 0.0439080074429512 + 1.0 * 6.077373027801514
Epoch 860, val loss: 0.7442367672920227
Epoch 870, training loss: 6.112947463989258 = 0.04200257360935211 + 1.0 * 6.070944786071777
Epoch 870, val loss: 0.7486124038696289
Epoch 880, training loss: 6.107401371002197 = 0.040229443460702896 + 1.0 * 6.067172050476074
Epoch 880, val loss: 0.7527821660041809
Epoch 890, training loss: 6.104233264923096 = 0.03856446593999863 + 1.0 * 6.06566858291626
Epoch 890, val loss: 0.7570408582687378
Epoch 900, training loss: 6.110786437988281 = 0.036997150629758835 + 1.0 * 6.073789119720459
Epoch 900, val loss: 0.7613328099250793
Epoch 910, training loss: 6.102829456329346 = 0.03553564473986626 + 1.0 * 6.067293643951416
Epoch 910, val loss: 0.7655583024024963
Epoch 920, training loss: 6.0994133949279785 = 0.03416427969932556 + 1.0 * 6.065248966217041
Epoch 920, val loss: 0.7696189880371094
Epoch 930, training loss: 6.0949788093566895 = 0.03286731243133545 + 1.0 * 6.0621113777160645
Epoch 930, val loss: 0.7737135887145996
Epoch 940, training loss: 6.093459129333496 = 0.03164009377360344 + 1.0 * 6.061819076538086
Epoch 940, val loss: 0.7778220176696777
Epoch 950, training loss: 6.097867965698242 = 0.03047948330640793 + 1.0 * 6.067388534545898
Epoch 950, val loss: 0.7820020914077759
Epoch 960, training loss: 6.095429420471191 = 0.0293926652520895 + 1.0 * 6.066036701202393
Epoch 960, val loss: 0.7860302329063416
Epoch 970, training loss: 6.087961196899414 = 0.02836274355649948 + 1.0 * 6.059598445892334
Epoch 970, val loss: 0.7898741960525513
Epoch 980, training loss: 6.087233066558838 = 0.02738562971353531 + 1.0 * 6.059847354888916
Epoch 980, val loss: 0.7938460111618042
Epoch 990, training loss: 6.086283206939697 = 0.026462355628609657 + 1.0 * 6.059820652008057
Epoch 990, val loss: 0.7978325486183167
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6900
Flip ASR: 0.6400/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.319398880004883 = 1.9454891681671143 + 1.0 * 8.373909950256348
Epoch 0, val loss: 1.945191740989685
Epoch 10, training loss: 10.309450149536133 = 1.9358149766921997 + 1.0 * 8.373635292053223
Epoch 10, val loss: 1.9350746870040894
Epoch 20, training loss: 10.295599937438965 = 1.9236695766448975 + 1.0 * 8.371930122375488
Epoch 20, val loss: 1.9224328994750977
Epoch 30, training loss: 10.265745162963867 = 1.9065648317337036 + 1.0 * 8.359180450439453
Epoch 30, val loss: 1.9045344591140747
Epoch 40, training loss: 10.156695365905762 = 1.8834172487258911 + 1.0 * 8.27327823638916
Epoch 40, val loss: 1.8808460235595703
Epoch 50, training loss: 9.68136978149414 = 1.8590307235717773 + 1.0 * 7.822338581085205
Epoch 50, val loss: 1.85683274269104
Epoch 60, training loss: 9.236066818237305 = 1.835810899734497 + 1.0 * 7.400256156921387
Epoch 60, val loss: 1.8352018594741821
Epoch 70, training loss: 8.845112800598145 = 1.8169305324554443 + 1.0 * 7.028182506561279
Epoch 70, val loss: 1.818000078201294
Epoch 80, training loss: 8.576984405517578 = 1.800345778465271 + 1.0 * 6.776638984680176
Epoch 80, val loss: 1.8022205829620361
Epoch 90, training loss: 8.405667304992676 = 1.7845438718795776 + 1.0 * 6.621123790740967
Epoch 90, val loss: 1.7871538400650024
Epoch 100, training loss: 8.308151245117188 = 1.7666796445846558 + 1.0 * 6.5414719581604
Epoch 100, val loss: 1.770477533340454
Epoch 110, training loss: 8.227466583251953 = 1.7471293210983276 + 1.0 * 6.480337142944336
Epoch 110, val loss: 1.752526044845581
Epoch 120, training loss: 8.162495613098145 = 1.7266985177993774 + 1.0 * 6.435797214508057
Epoch 120, val loss: 1.7340495586395264
Epoch 130, training loss: 8.104593276977539 = 1.7046314477920532 + 1.0 * 6.399961948394775
Epoch 130, val loss: 1.714505672454834
Epoch 140, training loss: 8.051263809204102 = 1.6793016195297241 + 1.0 * 6.371962547302246
Epoch 140, val loss: 1.692473292350769
Epoch 150, training loss: 7.99932861328125 = 1.6495771408081055 + 1.0 * 6.3497514724731445
Epoch 150, val loss: 1.6672005653381348
Epoch 160, training loss: 7.943402290344238 = 1.6147711277008057 + 1.0 * 6.3286309242248535
Epoch 160, val loss: 1.638076901435852
Epoch 170, training loss: 7.886596202850342 = 1.5742250680923462 + 1.0 * 6.312371253967285
Epoch 170, val loss: 1.604665756225586
Epoch 180, training loss: 7.8231587409973145 = 1.5277109146118164 + 1.0 * 6.295447826385498
Epoch 180, val loss: 1.566389560699463
Epoch 190, training loss: 7.756403923034668 = 1.4744212627410889 + 1.0 * 6.281982421875
Epoch 190, val loss: 1.522774577140808
Epoch 200, training loss: 7.685261249542236 = 1.4142303466796875 + 1.0 * 6.271030902862549
Epoch 200, val loss: 1.4741350412368774
Epoch 210, training loss: 7.611959457397461 = 1.3493869304656982 + 1.0 * 6.262572288513184
Epoch 210, val loss: 1.4226367473602295
Epoch 220, training loss: 7.533785820007324 = 1.2813315391540527 + 1.0 * 6.2524542808532715
Epoch 220, val loss: 1.3688268661499023
Epoch 230, training loss: 7.454773902893066 = 1.210831880569458 + 1.0 * 6.2439422607421875
Epoch 230, val loss: 1.3136730194091797
Epoch 240, training loss: 7.380101680755615 = 1.1398001909255981 + 1.0 * 6.240301609039307
Epoch 240, val loss: 1.258790373802185
Epoch 250, training loss: 7.301682472229004 = 1.0712321996688843 + 1.0 * 6.23045015335083
Epoch 250, val loss: 1.206754446029663
Epoch 260, training loss: 7.233850479125977 = 1.006416916847229 + 1.0 * 6.227433681488037
Epoch 260, val loss: 1.158539056777954
Epoch 270, training loss: 7.163914680480957 = 0.946380078792572 + 1.0 * 6.21753454208374
Epoch 270, val loss: 1.1148873567581177
Epoch 280, training loss: 7.101606845855713 = 0.8904658555984497 + 1.0 * 6.211141109466553
Epoch 280, val loss: 1.0749722719192505
Epoch 290, training loss: 7.043787002563477 = 0.8382692337036133 + 1.0 * 6.205517768859863
Epoch 290, val loss: 1.0385096073150635
Epoch 300, training loss: 6.9928202629089355 = 0.7899693846702576 + 1.0 * 6.202850818634033
Epoch 300, val loss: 1.0056405067443848
Epoch 310, training loss: 6.942856788635254 = 0.7456364631652832 + 1.0 * 6.197220325469971
Epoch 310, val loss: 0.9761507511138916
Epoch 320, training loss: 6.8949809074401855 = 0.704606831073761 + 1.0 * 6.19037389755249
Epoch 320, val loss: 0.9495081901550293
Epoch 330, training loss: 6.853618621826172 = 0.6662015914916992 + 1.0 * 6.187417030334473
Epoch 330, val loss: 0.9252893328666687
Epoch 340, training loss: 6.811462879180908 = 0.6302587389945984 + 1.0 * 6.181204319000244
Epoch 340, val loss: 0.9032171368598938
Epoch 350, training loss: 6.774232387542725 = 0.5963982343673706 + 1.0 * 6.1778340339660645
Epoch 350, val loss: 0.8831401467323303
Epoch 360, training loss: 6.746233940124512 = 0.5644158720970154 + 1.0 * 6.181818008422852
Epoch 360, val loss: 0.8648041486740112
Epoch 370, training loss: 6.706026077270508 = 0.534376859664917 + 1.0 * 6.17164945602417
Epoch 370, val loss: 0.8482896685600281
Epoch 380, training loss: 6.672051906585693 = 0.5060490369796753 + 1.0 * 6.1660027503967285
Epoch 380, val loss: 0.83339923620224
Epoch 390, training loss: 6.643475532531738 = 0.4790884256362915 + 1.0 * 6.164387226104736
Epoch 390, val loss: 0.819961428642273
Epoch 400, training loss: 6.614314079284668 = 0.45346277952194214 + 1.0 * 6.16085147857666
Epoch 400, val loss: 0.8079108595848083
Epoch 410, training loss: 6.590692520141602 = 0.42929694056510925 + 1.0 * 6.16139554977417
Epoch 410, val loss: 0.7972034811973572
Epoch 420, training loss: 6.56125020980835 = 0.4065166413784027 + 1.0 * 6.154733657836914
Epoch 420, val loss: 0.7877693176269531
Epoch 430, training loss: 6.535264015197754 = 0.38482457399368286 + 1.0 * 6.150439262390137
Epoch 430, val loss: 0.7794539928436279
Epoch 440, training loss: 6.516848564147949 = 0.36414140462875366 + 1.0 * 6.152707099914551
Epoch 440, val loss: 0.7721563577651978
Epoch 450, training loss: 6.494084358215332 = 0.34469354152679443 + 1.0 * 6.149390697479248
Epoch 450, val loss: 0.7658475637435913
Epoch 460, training loss: 6.470274925231934 = 0.32625773549079895 + 1.0 * 6.144017219543457
Epoch 460, val loss: 0.7604346871376038
Epoch 470, training loss: 6.448474884033203 = 0.308682918548584 + 1.0 * 6.139791965484619
Epoch 470, val loss: 0.7557997107505798
Epoch 480, training loss: 6.4347639083862305 = 0.2919071316719055 + 1.0 * 6.142856597900391
Epoch 480, val loss: 0.7519040703773499
Epoch 490, training loss: 6.414680004119873 = 0.2760380208492279 + 1.0 * 6.138641834259033
Epoch 490, val loss: 0.7487316131591797
Epoch 500, training loss: 6.394628047943115 = 0.2609693109989166 + 1.0 * 6.1336588859558105
Epoch 500, val loss: 0.7462501525878906
Epoch 510, training loss: 6.377139568328857 = 0.24659763276576996 + 1.0 * 6.130541801452637
Epoch 510, val loss: 0.7443656921386719
Epoch 520, training loss: 6.362830638885498 = 0.2328680008649826 + 1.0 * 6.12996244430542
Epoch 520, val loss: 0.7430490851402283
Epoch 530, training loss: 6.355687618255615 = 0.2198622077703476 + 1.0 * 6.1358256340026855
Epoch 530, val loss: 0.7422951459884644
Epoch 540, training loss: 6.3351874351501465 = 0.20762355625629425 + 1.0 * 6.127563953399658
Epoch 540, val loss: 0.7420186400413513
Epoch 550, training loss: 6.319156646728516 = 0.19600939750671387 + 1.0 * 6.123147487640381
Epoch 550, val loss: 0.7422397136688232
Epoch 560, training loss: 6.327337265014648 = 0.18496662378311157 + 1.0 * 6.142370700836182
Epoch 560, val loss: 0.7429273724555969
Epoch 570, training loss: 6.2987213134765625 = 0.17468677461147308 + 1.0 * 6.124034404754639
Epoch 570, val loss: 0.7440359592437744
Epoch 580, training loss: 6.283717155456543 = 0.16500580310821533 + 1.0 * 6.118711471557617
Epoch 580, val loss: 0.745510995388031
Epoch 590, training loss: 6.271627426147461 = 0.155869722366333 + 1.0 * 6.115757942199707
Epoch 590, val loss: 0.7474257349967957
Epoch 600, training loss: 6.260812759399414 = 0.14725226163864136 + 1.0 * 6.113560676574707
Epoch 600, val loss: 0.7497677206993103
Epoch 610, training loss: 6.259891033172607 = 0.1391666829586029 + 1.0 * 6.120724201202393
Epoch 610, val loss: 0.7525013089179993
Epoch 620, training loss: 6.244297504425049 = 0.13166186213493347 + 1.0 * 6.112635612487793
Epoch 620, val loss: 0.755524218082428
Epoch 630, training loss: 6.233189582824707 = 0.12466742098331451 + 1.0 * 6.108521938323975
Epoch 630, val loss: 0.7587817311286926
Epoch 640, training loss: 6.225336074829102 = 0.11811001598834991 + 1.0 * 6.1072258949279785
Epoch 640, val loss: 0.7624104022979736
Epoch 650, training loss: 6.218459129333496 = 0.11195333302021027 + 1.0 * 6.106505870819092
Epoch 650, val loss: 0.7663804888725281
Epoch 660, training loss: 6.214078426361084 = 0.10619095712900162 + 1.0 * 6.107887268066406
Epoch 660, val loss: 0.7706038355827332
Epoch 670, training loss: 6.211116313934326 = 0.10081470012664795 + 1.0 * 6.110301494598389
Epoch 670, val loss: 0.7750187516212463
Epoch 680, training loss: 6.196954250335693 = 0.09580297768115997 + 1.0 * 6.101151466369629
Epoch 680, val loss: 0.779679536819458
Epoch 690, training loss: 6.1929192543029785 = 0.0911116823554039 + 1.0 * 6.101807594299316
Epoch 690, val loss: 0.7845126390457153
Epoch 700, training loss: 6.1869096755981445 = 0.08672536909580231 + 1.0 * 6.100184440612793
Epoch 700, val loss: 0.7895228862762451
Epoch 710, training loss: 6.182902812957764 = 0.08262524008750916 + 1.0 * 6.100277423858643
Epoch 710, val loss: 0.7947518825531006
Epoch 720, training loss: 6.176035404205322 = 0.07879417389631271 + 1.0 * 6.097241401672363
Epoch 720, val loss: 0.8000766634941101
Epoch 730, training loss: 6.170368671417236 = 0.07518865168094635 + 1.0 * 6.095180034637451
Epoch 730, val loss: 0.8055398464202881
Epoch 740, training loss: 6.1718525886535645 = 0.07180392742156982 + 1.0 * 6.100048542022705
Epoch 740, val loss: 0.8112434148788452
Epoch 750, training loss: 6.165797233581543 = 0.06862769275903702 + 1.0 * 6.097169399261475
Epoch 750, val loss: 0.8168865442276001
Epoch 760, training loss: 6.156427383422852 = 0.06567206978797913 + 1.0 * 6.090755462646484
Epoch 760, val loss: 0.8225908279418945
Epoch 770, training loss: 6.152478218078613 = 0.06288440525531769 + 1.0 * 6.089593887329102
Epoch 770, val loss: 0.8284252882003784
Epoch 780, training loss: 6.151276111602783 = 0.060250215232372284 + 1.0 * 6.0910258293151855
Epoch 780, val loss: 0.8343316912651062
Epoch 790, training loss: 6.145211696624756 = 0.057767223566770554 + 1.0 * 6.087444305419922
Epoch 790, val loss: 0.8402663469314575
Epoch 800, training loss: 6.142170429229736 = 0.05543006584048271 + 1.0 * 6.086740493774414
Epoch 800, val loss: 0.8461949229240417
Epoch 810, training loss: 6.144800662994385 = 0.0532284751534462 + 1.0 * 6.091572284698486
Epoch 810, val loss: 0.8522049784660339
Epoch 820, training loss: 6.136344909667969 = 0.05114234611392021 + 1.0 * 6.085202693939209
Epoch 820, val loss: 0.8581705093383789
Epoch 830, training loss: 6.131629467010498 = 0.04917909950017929 + 1.0 * 6.0824503898620605
Epoch 830, val loss: 0.8641803860664368
Epoch 840, training loss: 6.142951011657715 = 0.04731423035264015 + 1.0 * 6.09563684463501
Epoch 840, val loss: 0.870261013507843
Epoch 850, training loss: 6.127923011779785 = 0.04556629806756973 + 1.0 * 6.082356929779053
Epoch 850, val loss: 0.8762626647949219
Epoch 860, training loss: 6.123956680297852 = 0.04390862211585045 + 1.0 * 6.080048084259033
Epoch 860, val loss: 0.8821077346801758
Epoch 870, training loss: 6.121345520019531 = 0.04233401641249657 + 1.0 * 6.0790114402771
Epoch 870, val loss: 0.8880957961082458
Epoch 880, training loss: 6.122159004211426 = 0.040838856250047684 + 1.0 * 6.081320285797119
Epoch 880, val loss: 0.8941037654876709
Epoch 890, training loss: 6.116515636444092 = 0.039417244493961334 + 1.0 * 6.077098369598389
Epoch 890, val loss: 0.9000484347343445
Epoch 900, training loss: 6.1134114265441895 = 0.038065578788518906 + 1.0 * 6.075345993041992
Epoch 900, val loss: 0.905919075012207
Epoch 910, training loss: 6.1144795417785645 = 0.03678261116147041 + 1.0 * 6.077696800231934
Epoch 910, val loss: 0.911843478679657
Epoch 920, training loss: 6.112189292907715 = 0.0355709008872509 + 1.0 * 6.076618194580078
Epoch 920, val loss: 0.9176126718521118
Epoch 930, training loss: 6.107290744781494 = 0.03441334888339043 + 1.0 * 6.072877407073975
Epoch 930, val loss: 0.9233078360557556
Epoch 940, training loss: 6.103772163391113 = 0.0333046093583107 + 1.0 * 6.070467472076416
Epoch 940, val loss: 0.9290565252304077
Epoch 950, training loss: 6.103586673736572 = 0.03224337100982666 + 1.0 * 6.071343421936035
Epoch 950, val loss: 0.9347990155220032
Epoch 960, training loss: 6.101805686950684 = 0.031235747039318085 + 1.0 * 6.07056999206543
Epoch 960, val loss: 0.9405146837234497
Epoch 970, training loss: 6.100949287414551 = 0.030275436118245125 + 1.0 * 6.070673942565918
Epoch 970, val loss: 0.9460825324058533
Epoch 980, training loss: 6.099296569824219 = 0.02935964986681938 + 1.0 * 6.069936752319336
Epoch 980, val loss: 0.9515817165374756
Epoch 990, training loss: 6.098293781280518 = 0.028484435752034187 + 1.0 * 6.069809436798096
Epoch 990, val loss: 0.9571162462234497
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7786
Flip ASR: 0.7333/225 nodes
The final ASR:0.72079, 0.04091, Accuracy:0.80988, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11600])
remove edge: torch.Size([2, 9538])
updated graph: torch.Size([2, 10582])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98155, 0.01044, Accuracy:0.83457, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.322063446044922 = 1.9481801986694336 + 1.0 * 8.373883247375488
Epoch 0, val loss: 1.9448398351669312
Epoch 10, training loss: 10.311779022216797 = 1.9382025003433228 + 1.0 * 8.373576164245605
Epoch 10, val loss: 1.9349794387817383
Epoch 20, training loss: 10.297396659851074 = 1.9259012937545776 + 1.0 * 8.371495246887207
Epoch 20, val loss: 1.9228730201721191
Epoch 30, training loss: 10.264405250549316 = 1.9085521697998047 + 1.0 * 8.355853080749512
Epoch 30, val loss: 1.9059240818023682
Epoch 40, training loss: 10.12409496307373 = 1.8854678869247437 + 1.0 * 8.238627433776855
Epoch 40, val loss: 1.883995771408081
Epoch 50, training loss: 9.397302627563477 = 1.8617782592773438 + 1.0 * 7.535524845123291
Epoch 50, val loss: 1.8617576360702515
Epoch 60, training loss: 9.029004096984863 = 1.8418322801589966 + 1.0 * 7.187171936035156
Epoch 60, val loss: 1.8441729545593262
Epoch 70, training loss: 8.713907241821289 = 1.8255951404571533 + 1.0 * 6.888311862945557
Epoch 70, val loss: 1.8290936946868896
Epoch 80, training loss: 8.48377513885498 = 1.8096585273742676 + 1.0 * 6.674116611480713
Epoch 80, val loss: 1.8146353960037231
Epoch 90, training loss: 8.369817733764648 = 1.7938079833984375 + 1.0 * 6.576009273529053
Epoch 90, val loss: 1.8003900051116943
Epoch 100, training loss: 8.276163101196289 = 1.7773302793502808 + 1.0 * 6.498833179473877
Epoch 100, val loss: 1.7863010168075562
Epoch 110, training loss: 8.200451850891113 = 1.7609063386917114 + 1.0 * 6.439545631408691
Epoch 110, val loss: 1.7722386121749878
Epoch 120, training loss: 8.136234283447266 = 1.7441110610961914 + 1.0 * 6.392122745513916
Epoch 120, val loss: 1.757646918296814
Epoch 130, training loss: 8.078803062438965 = 1.7256410121917725 + 1.0 * 6.3531622886657715
Epoch 130, val loss: 1.7417750358581543
Epoch 140, training loss: 8.029419898986816 = 1.7042711973190308 + 1.0 * 6.325148582458496
Epoch 140, val loss: 1.723905086517334
Epoch 150, training loss: 7.98115348815918 = 1.6790692806243896 + 1.0 * 6.302083969116211
Epoch 150, val loss: 1.7031298875808716
Epoch 160, training loss: 7.934957027435303 = 1.6491470336914062 + 1.0 * 6.2858099937438965
Epoch 160, val loss: 1.6784963607788086
Epoch 170, training loss: 7.883876323699951 = 1.613989233970642 + 1.0 * 6.2698869705200195
Epoch 170, val loss: 1.6494472026824951
Epoch 180, training loss: 7.829787731170654 = 1.572980284690857 + 1.0 * 6.256807327270508
Epoch 180, val loss: 1.6152993440628052
Epoch 190, training loss: 7.7720537185668945 = 1.5257751941680908 + 1.0 * 6.246278285980225
Epoch 190, val loss: 1.5757405757904053
Epoch 200, training loss: 7.710496425628662 = 1.4730867147445679 + 1.0 * 6.237409591674805
Epoch 200, val loss: 1.531673789024353
Epoch 210, training loss: 7.645652770996094 = 1.4166525602340698 + 1.0 * 6.229000091552734
Epoch 210, val loss: 1.4847791194915771
Epoch 220, training loss: 7.578227996826172 = 1.357558250427246 + 1.0 * 6.220669746398926
Epoch 220, val loss: 1.43561589717865
Epoch 230, training loss: 7.513571739196777 = 1.2970247268676758 + 1.0 * 6.216547012329102
Epoch 230, val loss: 1.385657787322998
Epoch 240, training loss: 7.4468278884887695 = 1.2378789186477661 + 1.0 * 6.208949089050293
Epoch 240, val loss: 1.3378450870513916
Epoch 250, training loss: 7.38064432144165 = 1.1808615922927856 + 1.0 * 6.199782848358154
Epoch 250, val loss: 1.2924127578735352
Epoch 260, training loss: 7.319631576538086 = 1.1257458925247192 + 1.0 * 6.193885803222656
Epoch 260, val loss: 1.2491052150726318
Epoch 270, training loss: 7.267201900482178 = 1.072597861289978 + 1.0 * 6.19460391998291
Epoch 270, val loss: 1.2079598903656006
Epoch 280, training loss: 7.207437515258789 = 1.022571325302124 + 1.0 * 6.184866428375244
Epoch 280, val loss: 1.1697025299072266
Epoch 290, training loss: 7.153458118438721 = 0.9750045537948608 + 1.0 * 6.17845344543457
Epoch 290, val loss: 1.1336241960525513
Epoch 300, training loss: 7.102663040161133 = 0.9290556311607361 + 1.0 * 6.173607349395752
Epoch 300, val loss: 1.0988463163375854
Epoch 310, training loss: 7.057971000671387 = 0.8842494487762451 + 1.0 * 6.173721790313721
Epoch 310, val loss: 1.0650612115859985
Epoch 320, training loss: 7.00606632232666 = 0.8411096930503845 + 1.0 * 6.164956569671631
Epoch 320, val loss: 1.0324194431304932
Epoch 330, training loss: 6.960062503814697 = 0.7991418242454529 + 1.0 * 6.1609206199646
Epoch 330, val loss: 1.0007950067520142
Epoch 340, training loss: 6.915822982788086 = 0.7581337094306946 + 1.0 * 6.157689094543457
Epoch 340, val loss: 0.9699790477752686
Epoch 350, training loss: 6.87361478805542 = 0.7185091972351074 + 1.0 * 6.1551055908203125
Epoch 350, val loss: 0.9403255581855774
Epoch 360, training loss: 6.830787658691406 = 0.6804558634757996 + 1.0 * 6.150331974029541
Epoch 360, val loss: 0.912152111530304
Epoch 370, training loss: 6.792046070098877 = 0.643886387348175 + 1.0 * 6.148159503936768
Epoch 370, val loss: 0.8854803442955017
Epoch 380, training loss: 6.756581783294678 = 0.6092666983604431 + 1.0 * 6.14731502532959
Epoch 380, val loss: 0.8605481386184692
Epoch 390, training loss: 6.717684745788574 = 0.5766107439994812 + 1.0 * 6.141074180603027
Epoch 390, val loss: 0.8375424146652222
Epoch 400, training loss: 6.687741756439209 = 0.5456641316413879 + 1.0 * 6.142077445983887
Epoch 400, val loss: 0.8162681460380554
Epoch 410, training loss: 6.652524948120117 = 0.5163838863372803 + 1.0 * 6.136140823364258
Epoch 410, val loss: 0.7965476512908936
Epoch 420, training loss: 6.620849132537842 = 0.4884801208972931 + 1.0 * 6.132369041442871
Epoch 420, val loss: 0.7783834338188171
Epoch 430, training loss: 6.590912342071533 = 0.4617864191532135 + 1.0 * 6.129126071929932
Epoch 430, val loss: 0.7614794969558716
Epoch 440, training loss: 6.571486473083496 = 0.436137318611145 + 1.0 * 6.135349273681641
Epoch 440, val loss: 0.7458495497703552
Epoch 450, training loss: 6.538309097290039 = 0.41162124276161194 + 1.0 * 6.126688003540039
Epoch 450, val loss: 0.7312281131744385
Epoch 460, training loss: 6.5105791091918945 = 0.38792499899864197 + 1.0 * 6.122653961181641
Epoch 460, val loss: 0.7177674174308777
Epoch 470, training loss: 6.484280109405518 = 0.3648354113101959 + 1.0 * 6.119444847106934
Epoch 470, val loss: 0.7050568461418152
Epoch 480, training loss: 6.462892532348633 = 0.34231770038604736 + 1.0 * 6.120574951171875
Epoch 480, val loss: 0.6930192112922668
Epoch 490, training loss: 6.438714504241943 = 0.32057011127471924 + 1.0 * 6.118144512176514
Epoch 490, val loss: 0.6817444562911987
Epoch 500, training loss: 6.413625240325928 = 0.29976922273635864 + 1.0 * 6.113855838775635
Epoch 500, val loss: 0.6713107824325562
Epoch 510, training loss: 6.391844749450684 = 0.2798311114311218 + 1.0 * 6.112013816833496
Epoch 510, val loss: 0.6615598797798157
Epoch 520, training loss: 6.370701789855957 = 0.26088789105415344 + 1.0 * 6.109813690185547
Epoch 520, val loss: 0.6525562405586243
Epoch 530, training loss: 6.359195709228516 = 0.24309931695461273 + 1.0 * 6.116096496582031
Epoch 530, val loss: 0.6444589495658875
Epoch 540, training loss: 6.333952903747559 = 0.22650252282619476 + 1.0 * 6.107450485229492
Epoch 540, val loss: 0.6371975541114807
Epoch 550, training loss: 6.315037250518799 = 0.21098986268043518 + 1.0 * 6.1040472984313965
Epoch 550, val loss: 0.6307613253593445
Epoch 560, training loss: 6.298587322235107 = 0.19654737412929535 + 1.0 * 6.102039813995361
Epoch 560, val loss: 0.6250948309898376
Epoch 570, training loss: 6.287420749664307 = 0.18316952884197235 + 1.0 * 6.104251384735107
Epoch 570, val loss: 0.620206892490387
Epoch 580, training loss: 6.2718634605407715 = 0.1709056794643402 + 1.0 * 6.100957870483398
Epoch 580, val loss: 0.6161109805107117
Epoch 590, training loss: 6.269313812255859 = 0.15960019826889038 + 1.0 * 6.109713554382324
Epoch 590, val loss: 0.6127930879592896
Epoch 600, training loss: 6.245092868804932 = 0.149257093667984 + 1.0 * 6.0958356857299805
Epoch 600, val loss: 0.6100997924804688
Epoch 610, training loss: 6.232923984527588 = 0.1397317498922348 + 1.0 * 6.093192100524902
Epoch 610, val loss: 0.6081743240356445
Epoch 620, training loss: 6.2225165367126465 = 0.13094528019428253 + 1.0 * 6.09157133102417
Epoch 620, val loss: 0.606848418712616
Epoch 630, training loss: 6.2213969230651855 = 0.12285684049129486 + 1.0 * 6.098540306091309
Epoch 630, val loss: 0.6060589551925659
Epoch 640, training loss: 6.20576810836792 = 0.11545045673847198 + 1.0 * 6.090317726135254
Epoch 640, val loss: 0.6058008074760437
Epoch 650, training loss: 6.195279598236084 = 0.10862670838832855 + 1.0 * 6.086652755737305
Epoch 650, val loss: 0.6061153411865234
Epoch 660, training loss: 6.18934965133667 = 0.10232412070035934 + 1.0 * 6.0870256423950195
Epoch 660, val loss: 0.6068772077560425
Epoch 670, training loss: 6.185068130493164 = 0.09650556743144989 + 1.0 * 6.088562488555908
Epoch 670, val loss: 0.6079406142234802
Epoch 680, training loss: 6.174439430236816 = 0.09118490666151047 + 1.0 * 6.083254337310791
Epoch 680, val loss: 0.609442949295044
Epoch 690, training loss: 6.1676926612854 = 0.0862492024898529 + 1.0 * 6.0814433097839355
Epoch 690, val loss: 0.6113172173500061
Epoch 700, training loss: 6.161253929138184 = 0.08165847510099411 + 1.0 * 6.079595565795898
Epoch 700, val loss: 0.613480806350708
Epoch 710, training loss: 6.173691749572754 = 0.07739710062742233 + 1.0 * 6.09629487991333
Epoch 710, val loss: 0.6158880591392517
Epoch 720, training loss: 6.15122127532959 = 0.07345947623252869 + 1.0 * 6.077761650085449
Epoch 720, val loss: 0.6185075044631958
Epoch 730, training loss: 6.1467695236206055 = 0.0697973445057869 + 1.0 * 6.076972007751465
Epoch 730, val loss: 0.6214292049407959
Epoch 740, training loss: 6.141628742218018 = 0.06638278067111969 + 1.0 * 6.0752458572387695
Epoch 740, val loss: 0.6245394945144653
Epoch 750, training loss: 6.138809680938721 = 0.06319861114025116 + 1.0 * 6.075611114501953
Epoch 750, val loss: 0.6277546882629395
Epoch 760, training loss: 6.132824897766113 = 0.0602322593331337 + 1.0 * 6.072592735290527
Epoch 760, val loss: 0.6311342716217041
Epoch 770, training loss: 6.13059663772583 = 0.057458121329545975 + 1.0 * 6.07313871383667
Epoch 770, val loss: 0.6347019672393799
Epoch 780, training loss: 6.127012729644775 = 0.054859187453985214 + 1.0 * 6.072153568267822
Epoch 780, val loss: 0.6383143663406372
Epoch 790, training loss: 6.122102737426758 = 0.05243195593357086 + 1.0 * 6.069670677185059
Epoch 790, val loss: 0.6420556902885437
Epoch 800, training loss: 6.139997959136963 = 0.05015258118510246 + 1.0 * 6.089845180511475
Epoch 800, val loss: 0.6458991765975952
Epoch 810, training loss: 6.118051052093506 = 0.04802898317575455 + 1.0 * 6.070022106170654
Epoch 810, val loss: 0.6495771408081055
Epoch 820, training loss: 6.111867427825928 = 0.0460471548140049 + 1.0 * 6.065820217132568
Epoch 820, val loss: 0.65349942445755
Epoch 830, training loss: 6.1087751388549805 = 0.04417881369590759 + 1.0 * 6.064596176147461
Epoch 830, val loss: 0.657486081123352
Epoch 840, training loss: 6.105904579162598 = 0.042409077286720276 + 1.0 * 6.063495635986328
Epoch 840, val loss: 0.6614862680435181
Epoch 850, training loss: 6.10445499420166 = 0.04073387011885643 + 1.0 * 6.063721179962158
Epoch 850, val loss: 0.6655201315879822
Epoch 860, training loss: 6.1065826416015625 = 0.03915668651461601 + 1.0 * 6.067425727844238
Epoch 860, val loss: 0.6695162653923035
Epoch 870, training loss: 6.10440731048584 = 0.037670332938432693 + 1.0 * 6.066737174987793
Epoch 870, val loss: 0.6734775900840759
Epoch 880, training loss: 6.098850250244141 = 0.036278799176216125 + 1.0 * 6.0625715255737305
Epoch 880, val loss: 0.6775857210159302
Epoch 890, training loss: 6.095254421234131 = 0.034953657537698746 + 1.0 * 6.060300827026367
Epoch 890, val loss: 0.6817062497138977
Epoch 900, training loss: 6.107353687286377 = 0.03369932249188423 + 1.0 * 6.0736541748046875
Epoch 900, val loss: 0.685741662979126
Epoch 910, training loss: 6.091564178466797 = 0.03251713514328003 + 1.0 * 6.059047222137451
Epoch 910, val loss: 0.6896886229515076
Epoch 920, training loss: 6.089006423950195 = 0.03139658272266388 + 1.0 * 6.057610034942627
Epoch 920, val loss: 0.69378262758255
Epoch 930, training loss: 6.087227821350098 = 0.030325941741466522 + 1.0 * 6.056901931762695
Epoch 930, val loss: 0.6978747844696045
Epoch 940, training loss: 6.094987869262695 = 0.02930949628353119 + 1.0 * 6.065678596496582
Epoch 940, val loss: 0.701869010925293
Epoch 950, training loss: 6.088490962982178 = 0.028340918943285942 + 1.0 * 6.060150146484375
Epoch 950, val loss: 0.7057821750640869
Epoch 960, training loss: 6.08212423324585 = 0.027427133172750473 + 1.0 * 6.054697036743164
Epoch 960, val loss: 0.7097954750061035
Epoch 970, training loss: 6.080591678619385 = 0.02655506320297718 + 1.0 * 6.054036617279053
Epoch 970, val loss: 0.7137921452522278
Epoch 980, training loss: 6.09034538269043 = 0.0257182065397501 + 1.0 * 6.064627170562744
Epoch 980, val loss: 0.7177368998527527
Epoch 990, training loss: 6.082608699798584 = 0.024929162114858627 + 1.0 * 6.057679653167725
Epoch 990, val loss: 0.7215592265129089
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8556
Overall ASR: 0.5314
Flip ASR: 0.4400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.31106185913086 = 1.9372042417526245 + 1.0 * 8.373857498168945
Epoch 0, val loss: 1.943456768989563
Epoch 10, training loss: 10.300164222717285 = 1.9270249605178833 + 1.0 * 8.373139381408691
Epoch 10, val loss: 1.9325122833251953
Epoch 20, training loss: 10.283535957336426 = 1.9141337871551514 + 1.0 * 8.369401931762695
Epoch 20, val loss: 1.9186625480651855
Epoch 30, training loss: 10.244214057922363 = 1.8960224390029907 + 1.0 * 8.348191261291504
Epoch 30, val loss: 1.8993573188781738
Epoch 40, training loss: 10.097514152526855 = 1.8727062940597534 + 1.0 * 8.224807739257812
Epoch 40, val loss: 1.8753435611724854
Epoch 50, training loss: 9.608298301696777 = 1.84766685962677 + 1.0 * 7.760631561279297
Epoch 50, val loss: 1.8498245477676392
Epoch 60, training loss: 9.30926513671875 = 1.824641227722168 + 1.0 * 7.484623432159424
Epoch 60, val loss: 1.8268316984176636
Epoch 70, training loss: 8.925544738769531 = 1.806627631187439 + 1.0 * 7.118916988372803
Epoch 70, val loss: 1.8085132837295532
Epoch 80, training loss: 8.683490753173828 = 1.7904272079467773 + 1.0 * 6.893063545227051
Epoch 80, val loss: 1.7907025814056396
Epoch 90, training loss: 8.567383766174316 = 1.7705516815185547 + 1.0 * 6.796832084655762
Epoch 90, val loss: 1.7694807052612305
Epoch 100, training loss: 8.452881813049316 = 1.7495543956756592 + 1.0 * 6.703327655792236
Epoch 100, val loss: 1.7491874694824219
Epoch 110, training loss: 8.351061820983887 = 1.7305288314819336 + 1.0 * 6.620532989501953
Epoch 110, val loss: 1.7313358783721924
Epoch 120, training loss: 8.252552032470703 = 1.7107027769088745 + 1.0 * 6.541849613189697
Epoch 120, val loss: 1.7124711275100708
Epoch 130, training loss: 8.167898178100586 = 1.6876018047332764 + 1.0 * 6.4802961349487305
Epoch 130, val loss: 1.6918548345565796
Epoch 140, training loss: 8.088735580444336 = 1.6600122451782227 + 1.0 * 6.4287238121032715
Epoch 140, val loss: 1.6693000793457031
Epoch 150, training loss: 8.015724182128906 = 1.6280449628829956 + 1.0 * 6.387679576873779
Epoch 150, val loss: 1.6439566612243652
Epoch 160, training loss: 7.946457386016846 = 1.5905815362930298 + 1.0 * 6.3558759689331055
Epoch 160, val loss: 1.6140345335006714
Epoch 170, training loss: 7.874574661254883 = 1.547130823135376 + 1.0 * 6.327443599700928
Epoch 170, val loss: 1.5790780782699585
Epoch 180, training loss: 7.806843280792236 = 1.4980262517929077 + 1.0 * 6.308816909790039
Epoch 180, val loss: 1.5399540662765503
Epoch 190, training loss: 7.732074737548828 = 1.4463838338851929 + 1.0 * 6.285690784454346
Epoch 190, val loss: 1.498740315437317
Epoch 200, training loss: 7.6631927490234375 = 1.3934447765350342 + 1.0 * 6.269748210906982
Epoch 200, val loss: 1.4569625854492188
Epoch 210, training loss: 7.60344934463501 = 1.3408204317092896 + 1.0 * 6.26262903213501
Epoch 210, val loss: 1.4156720638275146
Epoch 220, training loss: 7.5387773513793945 = 1.292068362236023 + 1.0 * 6.246708869934082
Epoch 220, val loss: 1.3777271509170532
Epoch 230, training loss: 7.480690002441406 = 1.2464687824249268 + 1.0 * 6.234221458435059
Epoch 230, val loss: 1.3428047895431519
Epoch 240, training loss: 7.426797866821289 = 1.2032839059829712 + 1.0 * 6.223514080047607
Epoch 240, val loss: 1.3101023435592651
Epoch 250, training loss: 7.376092433929443 = 1.1618801355361938 + 1.0 * 6.214212417602539
Epoch 250, val loss: 1.279358983039856
Epoch 260, training loss: 7.329291343688965 = 1.1216446161270142 + 1.0 * 6.20764684677124
Epoch 260, val loss: 1.250052809715271
Epoch 270, training loss: 7.2843523025512695 = 1.0829702615737915 + 1.0 * 6.201382160186768
Epoch 270, val loss: 1.222162127494812
Epoch 280, training loss: 7.237763404846191 = 1.0453413724899292 + 1.0 * 6.192421913146973
Epoch 280, val loss: 1.1955194473266602
Epoch 290, training loss: 7.193089485168457 = 1.0080842971801758 + 1.0 * 6.185005187988281
Epoch 290, val loss: 1.16934072971344
Epoch 300, training loss: 7.1498308181762695 = 0.9711096882820129 + 1.0 * 6.178720951080322
Epoch 300, val loss: 1.143693208694458
Epoch 310, training loss: 7.109139442443848 = 0.934712827205658 + 1.0 * 6.174426555633545
Epoch 310, val loss: 1.118773102760315
Epoch 320, training loss: 7.069695472717285 = 0.8993085622787476 + 1.0 * 6.170386791229248
Epoch 320, val loss: 1.0951675176620483
Epoch 330, training loss: 7.032074928283691 = 0.8646293878555298 + 1.0 * 6.167445659637451
Epoch 330, val loss: 1.0726114511489868
Epoch 340, training loss: 6.992264747619629 = 0.8311336040496826 + 1.0 * 6.161130905151367
Epoch 340, val loss: 1.0511459112167358
Epoch 350, training loss: 6.956284523010254 = 0.7986001372337341 + 1.0 * 6.157684326171875
Epoch 350, val loss: 1.0309780836105347
Epoch 360, training loss: 6.9192986488342285 = 0.7666010856628418 + 1.0 * 6.152697563171387
Epoch 360, val loss: 1.0113143920898438
Epoch 370, training loss: 6.885801315307617 = 0.7349122762680054 + 1.0 * 6.150888919830322
Epoch 370, val loss: 0.9921059012413025
Epoch 380, training loss: 6.851252555847168 = 0.7037878036499023 + 1.0 * 6.147464752197266
Epoch 380, val loss: 0.9732316732406616
Epoch 390, training loss: 6.815284729003906 = 0.6730635166168213 + 1.0 * 6.142220973968506
Epoch 390, val loss: 0.9548115134239197
Epoch 400, training loss: 6.789406776428223 = 0.6426995396614075 + 1.0 * 6.146707057952881
Epoch 400, val loss: 0.9365426301956177
Epoch 410, training loss: 6.750579833984375 = 0.6130790710449219 + 1.0 * 6.137500762939453
Epoch 410, val loss: 0.9188517332077026
Epoch 420, training loss: 6.71823263168335 = 0.5839576721191406 + 1.0 * 6.134274959564209
Epoch 420, val loss: 0.901819109916687
Epoch 430, training loss: 6.686555862426758 = 0.5553185939788818 + 1.0 * 6.131237030029297
Epoch 430, val loss: 0.8853710889816284
Epoch 440, training loss: 6.6609787940979 = 0.5274201035499573 + 1.0 * 6.133558750152588
Epoch 440, val loss: 0.8698683381080627
Epoch 450, training loss: 6.629091262817383 = 0.5005329251289368 + 1.0 * 6.128558158874512
Epoch 450, val loss: 0.8557055592536926
Epoch 460, training loss: 6.6000447273254395 = 0.4744878113269806 + 1.0 * 6.125556945800781
Epoch 460, val loss: 0.8428872227668762
Epoch 470, training loss: 6.573681354522705 = 0.44934457540512085 + 1.0 * 6.1243367195129395
Epoch 470, val loss: 0.8312274217605591
Epoch 480, training loss: 6.544905662536621 = 0.4251396656036377 + 1.0 * 6.119765758514404
Epoch 480, val loss: 0.8213148713111877
Epoch 490, training loss: 6.520656585693359 = 0.4017290472984314 + 1.0 * 6.118927478790283
Epoch 490, val loss: 0.8126747608184814
Epoch 500, training loss: 6.496991157531738 = 0.3792388439178467 + 1.0 * 6.117752552032471
Epoch 500, val loss: 0.805415153503418
Epoch 510, training loss: 6.469756126403809 = 0.35765939950942993 + 1.0 * 6.112096786499023
Epoch 510, val loss: 0.7995123267173767
Epoch 520, training loss: 6.449912071228027 = 0.3368639647960663 + 1.0 * 6.113048076629639
Epoch 520, val loss: 0.79468834400177
Epoch 530, training loss: 6.434472560882568 = 0.3169521987438202 + 1.0 * 6.117520332336426
Epoch 530, val loss: 0.79090416431427
Epoch 540, training loss: 6.403714656829834 = 0.2980037331581116 + 1.0 * 6.105710983276367
Epoch 540, val loss: 0.7882281541824341
Epoch 550, training loss: 6.384824752807617 = 0.27992263436317444 + 1.0 * 6.104902267456055
Epoch 550, val loss: 0.7866055369377136
Epoch 560, training loss: 6.3703179359436035 = 0.26266834139823914 + 1.0 * 6.107649803161621
Epoch 560, val loss: 0.7856956124305725
Epoch 570, training loss: 6.348574161529541 = 0.24631436169147491 + 1.0 * 6.102259635925293
Epoch 570, val loss: 0.7856127023696899
Epoch 580, training loss: 6.329989910125732 = 0.23085656762123108 + 1.0 * 6.099133491516113
Epoch 580, val loss: 0.7865411639213562
Epoch 590, training loss: 6.315464496612549 = 0.21628691256046295 + 1.0 * 6.099177360534668
Epoch 590, val loss: 0.7879933714866638
Epoch 600, training loss: 6.3054938316345215 = 0.20267581939697266 + 1.0 * 6.102818012237549
Epoch 600, val loss: 0.7902653813362122
Epoch 610, training loss: 6.287969589233398 = 0.19006644189357758 + 1.0 * 6.097903251647949
Epoch 610, val loss: 0.7931882739067078
Epoch 620, training loss: 6.2728495597839355 = 0.17836736142635345 + 1.0 * 6.094482421875
Epoch 620, val loss: 0.7967990040779114
Epoch 630, training loss: 6.2589111328125 = 0.16743208467960358 + 1.0 * 6.0914788246154785
Epoch 630, val loss: 0.8009193539619446
Epoch 640, training loss: 6.246153831481934 = 0.15717992186546326 + 1.0 * 6.0889739990234375
Epoch 640, val loss: 0.8055350184440613
Epoch 650, training loss: 6.248955726623535 = 0.14756420254707336 + 1.0 * 6.101391315460205
Epoch 650, val loss: 0.8105575442314148
Epoch 660, training loss: 6.2263922691345215 = 0.13870331645011902 + 1.0 * 6.08768892288208
Epoch 660, val loss: 0.8157884478569031
Epoch 670, training loss: 6.215719699859619 = 0.13049066066741943 + 1.0 * 6.08522891998291
Epoch 670, val loss: 0.8218584060668945
Epoch 680, training loss: 6.205997943878174 = 0.12281052023172379 + 1.0 * 6.083187580108643
Epoch 680, val loss: 0.8280694484710693
Epoch 690, training loss: 6.197879314422607 = 0.11560705304145813 + 1.0 * 6.082272052764893
Epoch 690, val loss: 0.8344563245773315
Epoch 700, training loss: 6.199240207672119 = 0.10886292159557343 + 1.0 * 6.090377330780029
Epoch 700, val loss: 0.8410385251045227
Epoch 710, training loss: 6.186445236206055 = 0.10259893536567688 + 1.0 * 6.083846092224121
Epoch 710, val loss: 0.8479562997817993
Epoch 720, training loss: 6.175911903381348 = 0.09672350436449051 + 1.0 * 6.079188346862793
Epoch 720, val loss: 0.8552664518356323
Epoch 730, training loss: 6.169981479644775 = 0.0911410003900528 + 1.0 * 6.078840255737305
Epoch 730, val loss: 0.8626077771186829
Epoch 740, training loss: 6.162239074707031 = 0.08581141382455826 + 1.0 * 6.076427459716797
Epoch 740, val loss: 0.8700742125511169
Epoch 750, training loss: 6.165477752685547 = 0.08070025593042374 + 1.0 * 6.084777355194092
Epoch 750, val loss: 0.877792239189148
Epoch 760, training loss: 6.1512770652771 = 0.07578568905591965 + 1.0 * 6.075491428375244
Epoch 760, val loss: 0.8858213424682617
Epoch 770, training loss: 6.143868446350098 = 0.07108200341463089 + 1.0 * 6.072786331176758
Epoch 770, val loss: 0.8939604163169861
Epoch 780, training loss: 6.139365196228027 = 0.06653932482004166 + 1.0 * 6.072825908660889
Epoch 780, val loss: 0.9021427035331726
Epoch 790, training loss: 6.134568214416504 = 0.06229455769062042 + 1.0 * 6.0722737312316895
Epoch 790, val loss: 0.9102133512496948
Epoch 800, training loss: 6.130038738250732 = 0.058553874492645264 + 1.0 * 6.0714850425720215
Epoch 800, val loss: 0.9186034202575684
Epoch 810, training loss: 6.123354911804199 = 0.05526965484023094 + 1.0 * 6.068085193634033
Epoch 810, val loss: 0.9270501136779785
Epoch 820, training loss: 6.121943950653076 = 0.05233467370271683 + 1.0 * 6.06960916519165
Epoch 820, val loss: 0.9353987574577332
Epoch 830, training loss: 6.116382598876953 = 0.04967280849814415 + 1.0 * 6.066709995269775
Epoch 830, val loss: 0.9437154531478882
Epoch 840, training loss: 6.117053031921387 = 0.0472395159304142 + 1.0 * 6.0698137283325195
Epoch 840, val loss: 0.9523304104804993
Epoch 850, training loss: 6.112059593200684 = 0.04498988389968872 + 1.0 * 6.0670695304870605
Epoch 850, val loss: 0.9607828855514526
Epoch 860, training loss: 6.108238697052002 = 0.042912557721138 + 1.0 * 6.06532621383667
Epoch 860, val loss: 0.969098687171936
Epoch 870, training loss: 6.10606050491333 = 0.04098682105541229 + 1.0 * 6.065073490142822
Epoch 870, val loss: 0.9774978160858154
Epoch 880, training loss: 6.103573799133301 = 0.03920083865523338 + 1.0 * 6.064373016357422
Epoch 880, val loss: 0.9855809211730957
Epoch 890, training loss: 6.1017255783081055 = 0.0375431589782238 + 1.0 * 6.064182281494141
Epoch 890, val loss: 0.993700385093689
Epoch 900, training loss: 6.099032878875732 = 0.03599846735596657 + 1.0 * 6.063034534454346
Epoch 900, val loss: 1.0018501281738281
Epoch 910, training loss: 6.095062255859375 = 0.034555673599243164 + 1.0 * 6.060506343841553
Epoch 910, val loss: 1.0099295377731323
Epoch 920, training loss: 6.092442035675049 = 0.03319961205124855 + 1.0 * 6.059242248535156
Epoch 920, val loss: 1.0179158449172974
Epoch 930, training loss: 6.093930244445801 = 0.03192412108182907 + 1.0 * 6.062005996704102
Epoch 930, val loss: 1.0257583856582642
Epoch 940, training loss: 6.091463088989258 = 0.030726488679647446 + 1.0 * 6.060736656188965
Epoch 940, val loss: 1.0334285497665405
Epoch 950, training loss: 6.09134578704834 = 0.029602093622088432 + 1.0 * 6.06174373626709
Epoch 950, val loss: 1.0410696268081665
Epoch 960, training loss: 6.084071636199951 = 0.028545701876282692 + 1.0 * 6.055525779724121
Epoch 960, val loss: 1.04866623878479
Epoch 970, training loss: 6.0822224617004395 = 0.027542373165488243 + 1.0 * 6.054679870605469
Epoch 970, val loss: 1.0561819076538086
Epoch 980, training loss: 6.08140230178833 = 0.026586482301354408 + 1.0 * 6.054815769195557
Epoch 980, val loss: 1.0635677576065063
Epoch 990, training loss: 6.082435131072998 = 0.02568104676902294 + 1.0 * 6.056754112243652
Epoch 990, val loss: 1.0706204175949097
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.310044288635254 = 1.9362297058105469 + 1.0 * 8.373814582824707
Epoch 0, val loss: 1.9178541898727417
Epoch 10, training loss: 10.299644470214844 = 1.9263361692428589 + 1.0 * 8.373308181762695
Epoch 10, val loss: 1.9082415103912354
Epoch 20, training loss: 10.284194946289062 = 1.9144090414047241 + 1.0 * 8.369786262512207
Epoch 20, val loss: 1.8961611986160278
Epoch 30, training loss: 10.245203018188477 = 1.898309350013733 + 1.0 * 8.346893310546875
Epoch 30, val loss: 1.8794564008712769
Epoch 40, training loss: 10.081780433654785 = 1.8786261081695557 + 1.0 * 8.203154563903809
Epoch 40, val loss: 1.859796404838562
Epoch 50, training loss: 9.375471115112305 = 1.8585824966430664 + 1.0 * 7.516888618469238
Epoch 50, val loss: 1.8403306007385254
Epoch 60, training loss: 9.008365631103516 = 1.8417123556137085 + 1.0 * 7.166653633117676
Epoch 60, val loss: 1.824923038482666
Epoch 70, training loss: 8.745495796203613 = 1.8243770599365234 + 1.0 * 6.92111873626709
Epoch 70, val loss: 1.8085600137710571
Epoch 80, training loss: 8.57010269165039 = 1.805819034576416 + 1.0 * 6.764284133911133
Epoch 80, val loss: 1.7911207675933838
Epoch 90, training loss: 8.44438648223877 = 1.7883880138397217 + 1.0 * 6.655998706817627
Epoch 90, val loss: 1.775376319885254
Epoch 100, training loss: 8.342232704162598 = 1.7724825143814087 + 1.0 * 6.56974983215332
Epoch 100, val loss: 1.7615541219711304
Epoch 110, training loss: 8.255756378173828 = 1.7556132078170776 + 1.0 * 6.500143051147461
Epoch 110, val loss: 1.7472490072250366
Epoch 120, training loss: 8.180269241333008 = 1.7370635271072388 + 1.0 * 6.443205833435059
Epoch 120, val loss: 1.731902003288269
Epoch 130, training loss: 8.114245414733887 = 1.7167383432388306 + 1.0 * 6.397507190704346
Epoch 130, val loss: 1.715157389640808
Epoch 140, training loss: 8.053749084472656 = 1.693611741065979 + 1.0 * 6.360136985778809
Epoch 140, val loss: 1.6961168050765991
Epoch 150, training loss: 7.9980788230896 = 1.6666191816329956 + 1.0 * 6.3314595222473145
Epoch 150, val loss: 1.6741039752960205
Epoch 160, training loss: 7.9409966468811035 = 1.6347466707229614 + 1.0 * 6.306250095367432
Epoch 160, val loss: 1.648054599761963
Epoch 170, training loss: 7.881557464599609 = 1.5967990159988403 + 1.0 * 6.284758567810059
Epoch 170, val loss: 1.6172343492507935
Epoch 180, training loss: 7.820309638977051 = 1.552191138267517 + 1.0 * 6.268118381500244
Epoch 180, val loss: 1.5812418460845947
Epoch 190, training loss: 7.754093170166016 = 1.500944972038269 + 1.0 * 6.253148078918457
Epoch 190, val loss: 1.5403882265090942
Epoch 200, training loss: 7.685742378234863 = 1.4438625574111938 + 1.0 * 6.241879940032959
Epoch 200, val loss: 1.4956868886947632
Epoch 210, training loss: 7.613344669342041 = 1.382189393043518 + 1.0 * 6.2311553955078125
Epoch 210, val loss: 1.4480043649673462
Epoch 220, training loss: 7.538769245147705 = 1.3163901567459106 + 1.0 * 6.222379207611084
Epoch 220, val loss: 1.3981889486312866
Epoch 230, training loss: 7.465967655181885 = 1.2479548454284668 + 1.0 * 6.218012809753418
Epoch 230, val loss: 1.347536563873291
Epoch 240, training loss: 7.387709617614746 = 1.1800328493118286 + 1.0 * 6.207676887512207
Epoch 240, val loss: 1.297986626625061
Epoch 250, training loss: 7.31468391418457 = 1.1131824254989624 + 1.0 * 6.201501369476318
Epoch 250, val loss: 1.249724268913269
Epoch 260, training loss: 7.245983600616455 = 1.0488373041152954 + 1.0 * 6.197146415710449
Epoch 260, val loss: 1.2035446166992188
Epoch 270, training loss: 7.17719030380249 = 0.9877046346664429 + 1.0 * 6.189485549926758
Epoch 270, val loss: 1.159316897392273
Epoch 280, training loss: 7.112876892089844 = 0.9292474985122681 + 1.0 * 6.183629512786865
Epoch 280, val loss: 1.116931676864624
Epoch 290, training loss: 7.05472993850708 = 0.8735043406486511 + 1.0 * 6.181225776672363
Epoch 290, val loss: 1.0766122341156006
Epoch 300, training loss: 6.994148254394531 = 0.8206231594085693 + 1.0 * 6.173524856567383
Epoch 300, val loss: 1.0382667779922485
Epoch 310, training loss: 6.94010066986084 = 0.7702159881591797 + 1.0 * 6.16988468170166
Epoch 310, val loss: 1.0018653869628906
Epoch 320, training loss: 6.889143466949463 = 0.7227168679237366 + 1.0 * 6.166426658630371
Epoch 320, val loss: 0.9679347276687622
Epoch 330, training loss: 6.838923454284668 = 0.6786184310913086 + 1.0 * 6.160305023193359
Epoch 330, val loss: 0.9366032481193542
Epoch 340, training loss: 6.792896270751953 = 0.6372547149658203 + 1.0 * 6.155641555786133
Epoch 340, val loss: 0.9078684449195862
Epoch 350, training loss: 6.75307035446167 = 0.5987300276756287 + 1.0 * 6.1543402671813965
Epoch 350, val loss: 0.8815305829048157
Epoch 360, training loss: 6.712574481964111 = 0.5630602836608887 + 1.0 * 6.149514198303223
Epoch 360, val loss: 0.8578345775604248
Epoch 370, training loss: 6.674590587615967 = 0.5297954678535461 + 1.0 * 6.144794940948486
Epoch 370, val loss: 0.8364044427871704
Epoch 380, training loss: 6.640067100524902 = 0.4986652135848999 + 1.0 * 6.141401767730713
Epoch 380, val loss: 0.816918134689331
Epoch 390, training loss: 6.608357906341553 = 0.469485342502594 + 1.0 * 6.1388726234436035
Epoch 390, val loss: 0.7994208335876465
Epoch 400, training loss: 6.57826566696167 = 0.44205114245414734 + 1.0 * 6.136214733123779
Epoch 400, val loss: 0.7836968302726746
Epoch 410, training loss: 6.548532485961914 = 0.4160385727882385 + 1.0 * 6.13249397277832
Epoch 410, val loss: 0.7696815729141235
Epoch 420, training loss: 6.5213775634765625 = 0.39152947068214417 + 1.0 * 6.129848003387451
Epoch 420, val loss: 0.7573120594024658
Epoch 430, training loss: 6.494756698608398 = 0.36835217475891113 + 1.0 * 6.126404285430908
Epoch 430, val loss: 0.7463215589523315
Epoch 440, training loss: 6.470217704772949 = 0.34628939628601074 + 1.0 * 6.123928070068359
Epoch 440, val loss: 0.7366598844528198
Epoch 450, training loss: 6.449516296386719 = 0.3253975212574005 + 1.0 * 6.124118804931641
Epoch 450, val loss: 0.7281618714332581
Epoch 460, training loss: 6.4248833656311035 = 0.3057309687137604 + 1.0 * 6.119152545928955
Epoch 460, val loss: 0.7208290100097656
Epoch 470, training loss: 6.403725624084473 = 0.2871180474758148 + 1.0 * 6.116607666015625
Epoch 470, val loss: 0.7146511673927307
Epoch 480, training loss: 6.392883777618408 = 0.2694794237613678 + 1.0 * 6.123404502868652
Epoch 480, val loss: 0.7095248103141785
Epoch 490, training loss: 6.370128154754639 = 0.253048837184906 + 1.0 * 6.117079257965088
Epoch 490, val loss: 0.7053602933883667
Epoch 500, training loss: 6.348870277404785 = 0.23762425780296326 + 1.0 * 6.111246109008789
Epoch 500, val loss: 0.7020640969276428
Epoch 510, training loss: 6.330989837646484 = 0.2230757474899292 + 1.0 * 6.107913970947266
Epoch 510, val loss: 0.6998165845870972
Epoch 520, training loss: 6.320307731628418 = 0.20932240784168243 + 1.0 * 6.110985279083252
Epoch 520, val loss: 0.6984409093856812
Epoch 530, training loss: 6.300624847412109 = 0.1965019702911377 + 1.0 * 6.104122638702393
Epoch 530, val loss: 0.6977856755256653
Epoch 540, training loss: 6.289943218231201 = 0.18449348211288452 + 1.0 * 6.105449676513672
Epoch 540, val loss: 0.6979372501373291
Epoch 550, training loss: 6.277766704559326 = 0.17325378954410553 + 1.0 * 6.104512691497803
Epoch 550, val loss: 0.6988999843597412
Epoch 560, training loss: 6.262589454650879 = 0.16283810138702393 + 1.0 * 6.0997514724731445
Epoch 560, val loss: 0.7003747820854187
Epoch 570, training loss: 6.249851703643799 = 0.1530555933713913 + 1.0 * 6.096796035766602
Epoch 570, val loss: 0.7025904655456543
Epoch 580, training loss: 6.241833686828613 = 0.14386723935604095 + 1.0 * 6.09796667098999
Epoch 580, val loss: 0.7054105997085571
Epoch 590, training loss: 6.23245906829834 = 0.1353342980146408 + 1.0 * 6.0971245765686035
Epoch 590, val loss: 0.7086446285247803
Epoch 600, training loss: 6.221420764923096 = 0.12735706567764282 + 1.0 * 6.094063758850098
Epoch 600, val loss: 0.7122347354888916
Epoch 610, training loss: 6.211232662200928 = 0.11992397904396057 + 1.0 * 6.09130859375
Epoch 610, val loss: 0.716339647769928
Epoch 620, training loss: 6.2056565284729 = 0.11299144476652145 + 1.0 * 6.092665195465088
Epoch 620, val loss: 0.7207691073417664
Epoch 630, training loss: 6.1968607902526855 = 0.10654947906732559 + 1.0 * 6.090311527252197
Epoch 630, val loss: 0.7254563570022583
Epoch 640, training loss: 6.187016487121582 = 0.1005796417593956 + 1.0 * 6.086436748504639
Epoch 640, val loss: 0.7302626371383667
Epoch 650, training loss: 6.179536819458008 = 0.09500908851623535 + 1.0 * 6.084527492523193
Epoch 650, val loss: 0.7353900671005249
Epoch 660, training loss: 6.1801652908325195 = 0.089846670627594 + 1.0 * 6.09031867980957
Epoch 660, val loss: 0.7407190799713135
Epoch 670, training loss: 6.1685686111450195 = 0.08506621420383453 + 1.0 * 6.083502292633057
Epoch 670, val loss: 0.7460163235664368
Epoch 680, training loss: 6.160001277923584 = 0.08062426000833511 + 1.0 * 6.079377174377441
Epoch 680, val loss: 0.7515256404876709
Epoch 690, training loss: 6.1567864418029785 = 0.07647604495286942 + 1.0 * 6.080310344696045
Epoch 690, val loss: 0.7572822570800781
Epoch 700, training loss: 6.151544570922852 = 0.07262058556079865 + 1.0 * 6.078924179077148
Epoch 700, val loss: 0.7630071043968201
Epoch 710, training loss: 6.147334575653076 = 0.06905018538236618 + 1.0 * 6.07828426361084
Epoch 710, val loss: 0.768703043460846
Epoch 720, training loss: 6.140394687652588 = 0.06572188436985016 + 1.0 * 6.074672698974609
Epoch 720, val loss: 0.7745205163955688
Epoch 730, training loss: 6.14279317855835 = 0.06261539459228516 + 1.0 * 6.0801777839660645
Epoch 730, val loss: 0.7804169058799744
Epoch 740, training loss: 6.133642196655273 = 0.05971755459904671 + 1.0 * 6.073924541473389
Epoch 740, val loss: 0.7862431406974792
Epoch 750, training loss: 6.133367538452148 = 0.05701446905732155 + 1.0 * 6.076353073120117
Epoch 750, val loss: 0.792133629322052
Epoch 760, training loss: 6.126206398010254 = 0.05447110906243324 + 1.0 * 6.071735382080078
Epoch 760, val loss: 0.7979437112808228
Epoch 770, training loss: 6.120879650115967 = 0.05211329087615013 + 1.0 * 6.0687665939331055
Epoch 770, val loss: 0.8037208914756775
Epoch 780, training loss: 6.117400646209717 = 0.04987707734107971 + 1.0 * 6.06752347946167
Epoch 780, val loss: 0.8095822334289551
Epoch 790, training loss: 6.116972923278809 = 0.04778851941227913 + 1.0 * 6.069184303283691
Epoch 790, val loss: 0.8154301047325134
Epoch 800, training loss: 6.111573219299316 = 0.04582885652780533 + 1.0 * 6.065744400024414
Epoch 800, val loss: 0.8210804462432861
Epoch 810, training loss: 6.1075005531311035 = 0.04398185759782791 + 1.0 * 6.063518524169922
Epoch 810, val loss: 0.8267950415611267
Epoch 820, training loss: 6.108214855194092 = 0.042234551161527634 + 1.0 * 6.065980434417725
Epoch 820, val loss: 0.832582950592041
Epoch 830, training loss: 6.109591484069824 = 0.04059958457946777 + 1.0 * 6.0689921379089355
Epoch 830, val loss: 0.8382142186164856
Epoch 840, training loss: 6.103000640869141 = 0.03906228020787239 + 1.0 * 6.063938140869141
Epoch 840, val loss: 0.8436819314956665
Epoch 850, training loss: 6.100399494171143 = 0.03760649636387825 + 1.0 * 6.062792778015137
Epoch 850, val loss: 0.8492366671562195
Epoch 860, training loss: 6.0956950187683105 = 0.03623528778553009 + 1.0 * 6.059459686279297
Epoch 860, val loss: 0.8547505140304565
Epoch 870, training loss: 6.093364238739014 = 0.03492891415953636 + 1.0 * 6.058435440063477
Epoch 870, val loss: 0.8602170348167419
Epoch 880, training loss: 6.0959086418151855 = 0.03369175270199776 + 1.0 * 6.062216758728027
Epoch 880, val loss: 0.8656548261642456
Epoch 890, training loss: 6.092602252960205 = 0.03251365199685097 + 1.0 * 6.060088634490967
Epoch 890, val loss: 0.8709056973457336
Epoch 900, training loss: 6.087817192077637 = 0.03141385689377785 + 1.0 * 6.056403160095215
Epoch 900, val loss: 0.8760795593261719
Epoch 910, training loss: 6.0849761962890625 = 0.03035532496869564 + 1.0 * 6.054620742797852
Epoch 910, val loss: 0.88129723072052
Epoch 920, training loss: 6.082597732543945 = 0.029350971803069115 + 1.0 * 6.053246974945068
Epoch 920, val loss: 0.8865405321121216
Epoch 930, training loss: 6.087970733642578 = 0.028388651087880135 + 1.0 * 6.059582233428955
Epoch 930, val loss: 0.8916707038879395
Epoch 940, training loss: 6.088780879974365 = 0.02747976966202259 + 1.0 * 6.061301231384277
Epoch 940, val loss: 0.8967385292053223
Epoch 950, training loss: 6.079404354095459 = 0.026620380580425262 + 1.0 * 6.052783966064453
Epoch 950, val loss: 0.901600182056427
Epoch 960, training loss: 6.07535982131958 = 0.025798553600907326 + 1.0 * 6.049561500549316
Epoch 960, val loss: 0.9065731763839722
Epoch 970, training loss: 6.07429838180542 = 0.025010449811816216 + 1.0 * 6.049287796020508
Epoch 970, val loss: 0.9115816354751587
Epoch 980, training loss: 6.078466415405273 = 0.024255573749542236 + 1.0 * 6.054210662841797
Epoch 980, val loss: 0.9165123105049133
Epoch 990, training loss: 6.074268341064453 = 0.023535963147878647 + 1.0 * 6.050732612609863
Epoch 990, val loss: 0.9212656021118164
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7306
Flip ASR: 0.6889/225 nodes
The final ASR:0.74908, 0.18575, Accuracy:0.83210, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11678])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10622])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9520
Flip ASR: 0.9422/225 nodes
The final ASR:0.97048, 0.01381, Accuracy:0.83457, 0.00924
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.341039657592773 = 1.967123031616211 + 1.0 * 8.373916625976562
Epoch 0, val loss: 1.9617191553115845
Epoch 10, training loss: 10.32983684539795 = 1.9561930894851685 + 1.0 * 8.37364387512207
Epoch 10, val loss: 1.9509491920471191
Epoch 20, training loss: 10.314520835876465 = 1.9430125951766968 + 1.0 * 8.371508598327637
Epoch 20, val loss: 1.937902569770813
Epoch 30, training loss: 10.278860092163086 = 1.9250195026397705 + 1.0 * 8.353840827941895
Epoch 30, val loss: 1.9199320077896118
Epoch 40, training loss: 10.131377220153809 = 1.9016636610031128 + 1.0 * 8.229713439941406
Epoch 40, val loss: 1.8970917463302612
Epoch 50, training loss: 9.658084869384766 = 1.876222014427185 + 1.0 * 7.781862735748291
Epoch 50, val loss: 1.8727407455444336
Epoch 60, training loss: 9.216653823852539 = 1.8544691801071167 + 1.0 * 7.362184524536133
Epoch 60, val loss: 1.8531237840652466
Epoch 70, training loss: 8.823015213012695 = 1.837799072265625 + 1.0 * 6.985215663909912
Epoch 70, val loss: 1.8381578922271729
Epoch 80, training loss: 8.588361740112305 = 1.8201384544372559 + 1.0 * 6.768222808837891
Epoch 80, val loss: 1.8225077390670776
Epoch 90, training loss: 8.443068504333496 = 1.799114465713501 + 1.0 * 6.643954277038574
Epoch 90, val loss: 1.8043842315673828
Epoch 100, training loss: 8.349884033203125 = 1.7771596908569336 + 1.0 * 6.572724342346191
Epoch 100, val loss: 1.7857199907302856
Epoch 110, training loss: 8.267802238464355 = 1.755841612815857 + 1.0 * 6.511960983276367
Epoch 110, val loss: 1.7678529024124146
Epoch 120, training loss: 8.200335502624512 = 1.7347559928894043 + 1.0 * 6.465579509735107
Epoch 120, val loss: 1.7503937482833862
Epoch 130, training loss: 8.13727855682373 = 1.7124372720718384 + 1.0 * 6.424841403961182
Epoch 130, val loss: 1.7320650815963745
Epoch 140, training loss: 8.07955551147461 = 1.6875088214874268 + 1.0 * 6.3920464515686035
Epoch 140, val loss: 1.7117894887924194
Epoch 150, training loss: 8.026799201965332 = 1.6591277122497559 + 1.0 * 6.367671489715576
Epoch 150, val loss: 1.6888700723648071
Epoch 160, training loss: 7.970064640045166 = 1.6269311904907227 + 1.0 * 6.343133449554443
Epoch 160, val loss: 1.6627826690673828
Epoch 170, training loss: 7.9129838943481445 = 1.5901703834533691 + 1.0 * 6.322813510894775
Epoch 170, val loss: 1.6329518556594849
Epoch 180, training loss: 7.859976768493652 = 1.5482548475265503 + 1.0 * 6.3117218017578125
Epoch 180, val loss: 1.599002718925476
Epoch 190, training loss: 7.79463529586792 = 1.5020924806594849 + 1.0 * 6.292542934417725
Epoch 190, val loss: 1.5617445707321167
Epoch 200, training loss: 7.732393741607666 = 1.4517582654953003 + 1.0 * 6.280635356903076
Epoch 200, val loss: 1.521384835243225
Epoch 210, training loss: 7.667552947998047 = 1.397773265838623 + 1.0 * 6.269779682159424
Epoch 210, val loss: 1.4783388376235962
Epoch 220, training loss: 7.602054595947266 = 1.3418586254119873 + 1.0 * 6.260196208953857
Epoch 220, val loss: 1.4345686435699463
Epoch 230, training loss: 7.534106731414795 = 1.2853717803955078 + 1.0 * 6.248734951019287
Epoch 230, val loss: 1.390968918800354
Epoch 240, training loss: 7.474820137023926 = 1.2294368743896484 + 1.0 * 6.245383262634277
Epoch 240, val loss: 1.348546028137207
Epoch 250, training loss: 7.408590793609619 = 1.1764603853225708 + 1.0 * 6.232130527496338
Epoch 250, val loss: 1.3089864253997803
Epoch 260, training loss: 7.350164413452148 = 1.126844048500061 + 1.0 * 6.223320484161377
Epoch 260, val loss: 1.2725881338119507
Epoch 270, training loss: 7.29640531539917 = 1.080578327178955 + 1.0 * 6.215826988220215
Epoch 270, val loss: 1.2392510175704956
Epoch 280, training loss: 7.25300407409668 = 1.0380635261535645 + 1.0 * 6.214940547943115
Epoch 280, val loss: 1.2093753814697266
Epoch 290, training loss: 7.201626777648926 = 0.9993084073066711 + 1.0 * 6.20231819152832
Epoch 290, val loss: 1.182831048965454
Epoch 300, training loss: 7.1613593101501465 = 0.9630871415138245 + 1.0 * 6.198272228240967
Epoch 300, val loss: 1.1586472988128662
Epoch 310, training loss: 7.121248722076416 = 0.928767204284668 + 1.0 * 6.192481517791748
Epoch 310, val loss: 1.1360726356506348
Epoch 320, training loss: 7.081794738769531 = 0.8954634070396423 + 1.0 * 6.186331272125244
Epoch 320, val loss: 1.1145230531692505
Epoch 330, training loss: 7.045183181762695 = 0.8625684976577759 + 1.0 * 6.182614803314209
Epoch 330, val loss: 1.0934427976608276
Epoch 340, training loss: 7.006195545196533 = 0.8297033309936523 + 1.0 * 6.176492214202881
Epoch 340, val loss: 1.0723657608032227
Epoch 350, training loss: 6.97103214263916 = 0.7962756752967834 + 1.0 * 6.1747565269470215
Epoch 350, val loss: 1.0510475635528564
Epoch 360, training loss: 6.933136463165283 = 0.7624422311782837 + 1.0 * 6.170694351196289
Epoch 360, val loss: 1.0294413566589355
Epoch 370, training loss: 6.894539833068848 = 0.7280831336975098 + 1.0 * 6.166456699371338
Epoch 370, val loss: 1.0076956748962402
Epoch 380, training loss: 6.855157852172852 = 0.6935102343559265 + 1.0 * 6.161647796630859
Epoch 380, val loss: 0.9859858751296997
Epoch 390, training loss: 6.81773567199707 = 0.658974289894104 + 1.0 * 6.158761501312256
Epoch 390, val loss: 0.9645695686340332
Epoch 400, training loss: 6.783252716064453 = 0.624711811542511 + 1.0 * 6.158540725708008
Epoch 400, val loss: 0.9435532093048096
Epoch 410, training loss: 6.744970321655273 = 0.5910561084747314 + 1.0 * 6.153913974761963
Epoch 410, val loss: 0.923364520072937
Epoch 420, training loss: 6.707013130187988 = 0.5583971738815308 + 1.0 * 6.148615837097168
Epoch 420, val loss: 0.904460608959198
Epoch 430, training loss: 6.682609558105469 = 0.5267722010612488 + 1.0 * 6.155837535858154
Epoch 430, val loss: 0.8867897391319275
Epoch 440, training loss: 6.6416850090026855 = 0.4966191351413727 + 1.0 * 6.145065784454346
Epoch 440, val loss: 0.8707789778709412
Epoch 450, training loss: 6.610002517700195 = 0.46792224049568176 + 1.0 * 6.142080307006836
Epoch 450, val loss: 0.8567537665367126
Epoch 460, training loss: 6.581693649291992 = 0.44055798649787903 + 1.0 * 6.1411356925964355
Epoch 460, val loss: 0.8441792130470276
Epoch 470, training loss: 6.556183815002441 = 0.4145154654979706 + 1.0 * 6.141668319702148
Epoch 470, val loss: 0.8332339525222778
Epoch 480, training loss: 6.524576187133789 = 0.389835000038147 + 1.0 * 6.134741306304932
Epoch 480, val loss: 0.8240125775337219
Epoch 490, training loss: 6.498989105224609 = 0.36633506417274475 + 1.0 * 6.132654190063477
Epoch 490, val loss: 0.8161882758140564
Epoch 500, training loss: 6.47848653793335 = 0.34403306245803833 + 1.0 * 6.134453296661377
Epoch 500, val loss: 0.8094924688339233
Epoch 510, training loss: 6.4502339363098145 = 0.32311439514160156 + 1.0 * 6.127119541168213
Epoch 510, val loss: 0.8042426705360413
Epoch 520, training loss: 6.427530765533447 = 0.303222119808197 + 1.0 * 6.1243085861206055
Epoch 520, val loss: 0.8000625967979431
Epoch 530, training loss: 6.406551837921143 = 0.28424760699272156 + 1.0 * 6.122304439544678
Epoch 530, val loss: 0.796668529510498
Epoch 540, training loss: 6.394357204437256 = 0.2661799490451813 + 1.0 * 6.128177165985107
Epoch 540, val loss: 0.7940234541893005
Epoch 550, training loss: 6.3694000244140625 = 0.24912667274475098 + 1.0 * 6.120273590087891
Epoch 550, val loss: 0.7922742366790771
Epoch 560, training loss: 6.353273868560791 = 0.2330131083726883 + 1.0 * 6.120260715484619
Epoch 560, val loss: 0.7912611365318298
Epoch 570, training loss: 6.332859039306641 = 0.217869833111763 + 1.0 * 6.114989280700684
Epoch 570, val loss: 0.7909523248672485
Epoch 580, training loss: 6.316222667694092 = 0.2035677582025528 + 1.0 * 6.112654685974121
Epoch 580, val loss: 0.7912815809249878
Epoch 590, training loss: 6.305534839630127 = 0.19011792540550232 + 1.0 * 6.115417003631592
Epoch 590, val loss: 0.7923346757888794
Epoch 600, training loss: 6.292159080505371 = 0.17760968208312988 + 1.0 * 6.11454963684082
Epoch 600, val loss: 0.7939032912254333
Epoch 610, training loss: 6.280128002166748 = 0.16598904132843018 + 1.0 * 6.114139080047607
Epoch 610, val loss: 0.7963258624076843
Epoch 620, training loss: 6.26623010635376 = 0.15524813532829285 + 1.0 * 6.1109819412231445
Epoch 620, val loss: 0.7992025017738342
Epoch 630, training loss: 6.24904203414917 = 0.14532192051410675 + 1.0 * 6.103720188140869
Epoch 630, val loss: 0.802868664264679
Epoch 640, training loss: 6.237923622131348 = 0.13611355423927307 + 1.0 * 6.101809978485107
Epoch 640, val loss: 0.8071490526199341
Epoch 650, training loss: 6.236700534820557 = 0.12759172916412354 + 1.0 * 6.109108924865723
Epoch 650, val loss: 0.8119736313819885
Epoch 660, training loss: 6.222444534301758 = 0.11977741122245789 + 1.0 * 6.102667331695557
Epoch 660, val loss: 0.8171375393867493
Epoch 670, training loss: 6.209853649139404 = 0.11262113600969315 + 1.0 * 6.097232341766357
Epoch 670, val loss: 0.8229793906211853
Epoch 680, training loss: 6.202424049377441 = 0.10599680244922638 + 1.0 * 6.0964274406433105
Epoch 680, val loss: 0.8291674852371216
Epoch 690, training loss: 6.194763660430908 = 0.09984996169805527 + 1.0 * 6.094913482666016
Epoch 690, val loss: 0.8356878757476807
Epoch 700, training loss: 6.192766189575195 = 0.09417279809713364 + 1.0 * 6.098593235015869
Epoch 700, val loss: 0.8424369096755981
Epoch 710, training loss: 6.185273170471191 = 0.08895532041788101 + 1.0 * 6.096317768096924
Epoch 710, val loss: 0.8494405746459961
Epoch 720, training loss: 6.1752777099609375 = 0.08412003517150879 + 1.0 * 6.091157913208008
Epoch 720, val loss: 0.8567558526992798
Epoch 730, training loss: 6.179223537445068 = 0.07962280511856079 + 1.0 * 6.099600791931152
Epoch 730, val loss: 0.8641794919967651
Epoch 740, training loss: 6.170454502105713 = 0.07545589655637741 + 1.0 * 6.094998836517334
Epoch 740, val loss: 0.8715067505836487
Epoch 750, training loss: 6.15874719619751 = 0.07159171998500824 + 1.0 * 6.087155342102051
Epoch 750, val loss: 0.8792091012001038
Epoch 760, training loss: 6.1538567543029785 = 0.06798361986875534 + 1.0 * 6.085873126983643
Epoch 760, val loss: 0.8869680166244507
Epoch 770, training loss: 6.149936199188232 = 0.06460635364055634 + 1.0 * 6.085330009460449
Epoch 770, val loss: 0.8947190642356873
Epoch 780, training loss: 6.146716594696045 = 0.06145794317126274 + 1.0 * 6.085258483886719
Epoch 780, val loss: 0.9024671912193298
Epoch 790, training loss: 6.149774074554443 = 0.05852814018726349 + 1.0 * 6.091246128082275
Epoch 790, val loss: 0.9102054834365845
Epoch 800, training loss: 6.139039993286133 = 0.05579940229654312 + 1.0 * 6.083240509033203
Epoch 800, val loss: 0.9180285930633545
Epoch 810, training loss: 6.1335225105285645 = 0.05323755741119385 + 1.0 * 6.08028507232666
Epoch 810, val loss: 0.9258258938789368
Epoch 820, training loss: 6.129228591918945 = 0.05083248391747475 + 1.0 * 6.078396320343018
Epoch 820, val loss: 0.9335997700691223
Epoch 830, training loss: 6.140100955963135 = 0.04857564717531204 + 1.0 * 6.091525077819824
Epoch 830, val loss: 0.9412643909454346
Epoch 840, training loss: 6.128086566925049 = 0.046472229063510895 + 1.0 * 6.0816144943237305
Epoch 840, val loss: 0.948752760887146
Epoch 850, training loss: 6.119641304016113 = 0.04449848085641861 + 1.0 * 6.075142860412598
Epoch 850, val loss: 0.9563944339752197
Epoch 860, training loss: 6.116819858551025 = 0.04264044016599655 + 1.0 * 6.074179649353027
Epoch 860, val loss: 0.9639537930488586
Epoch 870, training loss: 6.117700099945068 = 0.04088776186108589 + 1.0 * 6.076812267303467
Epoch 870, val loss: 0.9713412523269653
Epoch 880, training loss: 6.127028465270996 = 0.03925953060388565 + 1.0 * 6.087769031524658
Epoch 880, val loss: 0.9782881140708923
Epoch 890, training loss: 6.113694190979004 = 0.03773490712046623 + 1.0 * 6.075959205627441
Epoch 890, val loss: 0.98542720079422
Epoch 900, training loss: 6.108490467071533 = 0.0362972654402256 + 1.0 * 6.072193145751953
Epoch 900, val loss: 0.9926496744155884
Epoch 910, training loss: 6.104722499847412 = 0.03493537753820419 + 1.0 * 6.06978702545166
Epoch 910, val loss: 0.9996090531349182
Epoch 920, training loss: 6.102688312530518 = 0.033642757683992386 + 1.0 * 6.069045543670654
Epoch 920, val loss: 1.0064268112182617
Epoch 930, training loss: 6.105766773223877 = 0.032425180077552795 + 1.0 * 6.073341369628906
Epoch 930, val loss: 1.013047456741333
Epoch 940, training loss: 6.100686073303223 = 0.03128019720315933 + 1.0 * 6.069406032562256
Epoch 940, val loss: 1.0196417570114136
Epoch 950, training loss: 6.0968732833862305 = 0.030196992680430412 + 1.0 * 6.066676139831543
Epoch 950, val loss: 1.0263302326202393
Epoch 960, training loss: 6.1055426597595215 = 0.0291688721626997 + 1.0 * 6.07637357711792
Epoch 960, val loss: 1.0327529907226562
Epoch 970, training loss: 6.0962910652160645 = 0.02819480001926422 + 1.0 * 6.068096160888672
Epoch 970, val loss: 1.038939356803894
Epoch 980, training loss: 6.091683387756348 = 0.02726837806403637 + 1.0 * 6.064414978027344
Epoch 980, val loss: 1.0452601909637451
Epoch 990, training loss: 6.0913310050964355 = 0.026385396718978882 + 1.0 * 6.064945697784424
Epoch 990, val loss: 1.0514369010925293
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7897
Flip ASR: 0.7467/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.31758975982666 = 1.943678855895996 + 1.0 * 8.373910903930664
Epoch 0, val loss: 1.933447241783142
Epoch 10, training loss: 10.306317329406738 = 1.9328802824020386 + 1.0 * 8.37343692779541
Epoch 10, val loss: 1.9223121404647827
Epoch 20, training loss: 10.290470123291016 = 1.9196844100952148 + 1.0 * 8.3707857131958
Epoch 20, val loss: 1.9079838991165161
Epoch 30, training loss: 10.254077911376953 = 1.9015004634857178 + 1.0 * 8.352577209472656
Epoch 30, val loss: 1.8877434730529785
Epoch 40, training loss: 10.119974136352539 = 1.878023386001587 + 1.0 * 8.241950988769531
Epoch 40, val loss: 1.8625257015228271
Epoch 50, training loss: 9.769170761108398 = 1.854303240776062 + 1.0 * 7.914867877960205
Epoch 50, val loss: 1.8380064964294434
Epoch 60, training loss: 9.36373519897461 = 1.833520770072937 + 1.0 * 7.530214309692383
Epoch 60, val loss: 1.8174389600753784
Epoch 70, training loss: 8.851725578308105 = 1.815643548965454 + 1.0 * 7.0360822677612305
Epoch 70, val loss: 1.7996906042099
Epoch 80, training loss: 8.5374755859375 = 1.8001409769058228 + 1.0 * 6.737334728240967
Epoch 80, val loss: 1.7844772338867188
Epoch 90, training loss: 8.388436317443848 = 1.7832204103469849 + 1.0 * 6.605216026306152
Epoch 90, val loss: 1.7683709859848022
Epoch 100, training loss: 8.290923118591309 = 1.7635014057159424 + 1.0 * 6.527421951293945
Epoch 100, val loss: 1.75054931640625
Epoch 110, training loss: 8.213682174682617 = 1.7427146434783936 + 1.0 * 6.470967769622803
Epoch 110, val loss: 1.7327508926391602
Epoch 120, training loss: 8.15160083770752 = 1.7214173078536987 + 1.0 * 6.430183410644531
Epoch 120, val loss: 1.7148823738098145
Epoch 130, training loss: 8.094207763671875 = 1.6983866691589355 + 1.0 * 6.395821571350098
Epoch 130, val loss: 1.6956157684326172
Epoch 140, training loss: 8.03835391998291 = 1.6722649335861206 + 1.0 * 6.366089344024658
Epoch 140, val loss: 1.6739215850830078
Epoch 150, training loss: 7.983741283416748 = 1.6424775123596191 + 1.0 * 6.341263771057129
Epoch 150, val loss: 1.6494561433792114
Epoch 160, training loss: 7.931231498718262 = 1.609067440032959 + 1.0 * 6.322164058685303
Epoch 160, val loss: 1.6221081018447876
Epoch 170, training loss: 7.870908260345459 = 1.571661114692688 + 1.0 * 6.2992472648620605
Epoch 170, val loss: 1.5916829109191895
Epoch 180, training loss: 7.813587188720703 = 1.53013014793396 + 1.0 * 6.283456802368164
Epoch 180, val loss: 1.5578693151474
Epoch 190, training loss: 7.754556179046631 = 1.484517216682434 + 1.0 * 6.270039081573486
Epoch 190, val loss: 1.5206975936889648
Epoch 200, training loss: 7.693843841552734 = 1.4354861974716187 + 1.0 * 6.258357524871826
Epoch 200, val loss: 1.4808664321899414
Epoch 210, training loss: 7.634420394897461 = 1.384597659111023 + 1.0 * 6.249822616577148
Epoch 210, val loss: 1.4397201538085938
Epoch 220, training loss: 7.575560569763184 = 1.334001064300537 + 1.0 * 6.2415595054626465
Epoch 220, val loss: 1.3991875648498535
Epoch 230, training loss: 7.516781806945801 = 1.2841371297836304 + 1.0 * 6.232644557952881
Epoch 230, val loss: 1.359480619430542
Epoch 240, training loss: 7.460643768310547 = 1.2354753017425537 + 1.0 * 6.225168228149414
Epoch 240, val loss: 1.3211613893508911
Epoch 250, training loss: 7.406374931335449 = 1.1883071660995483 + 1.0 * 6.218067646026611
Epoch 250, val loss: 1.2842180728912354
Epoch 260, training loss: 7.353349685668945 = 1.142249345779419 + 1.0 * 6.2111005783081055
Epoch 260, val loss: 1.2490112781524658
Epoch 270, training loss: 7.302894592285156 = 1.0975083112716675 + 1.0 * 6.205386161804199
Epoch 270, val loss: 1.214892864227295
Epoch 280, training loss: 7.258827209472656 = 1.053276538848877 + 1.0 * 6.205550670623779
Epoch 280, val loss: 1.18178129196167
Epoch 290, training loss: 7.204867839813232 = 1.010045051574707 + 1.0 * 6.194822788238525
Epoch 290, val loss: 1.149667501449585
Epoch 300, training loss: 7.156466007232666 = 0.967387318611145 + 1.0 * 6.1890788078308105
Epoch 300, val loss: 1.1184600591659546
Epoch 310, training loss: 7.108066558837891 = 0.9254853129386902 + 1.0 * 6.182581424713135
Epoch 310, val loss: 1.0878829956054688
Epoch 320, training loss: 7.0625996589660645 = 0.8843427896499634 + 1.0 * 6.178256988525391
Epoch 320, val loss: 1.058026909828186
Epoch 330, training loss: 7.0204973220825195 = 0.8440377116203308 + 1.0 * 6.176459789276123
Epoch 330, val loss: 1.029089093208313
Epoch 340, training loss: 6.974524021148682 = 0.8051853179931641 + 1.0 * 6.169338703155518
Epoch 340, val loss: 1.0012695789337158
Epoch 350, training loss: 6.933860778808594 = 0.7676984071731567 + 1.0 * 6.166162490844727
Epoch 350, val loss: 0.9746950268745422
Epoch 360, training loss: 6.8986029624938965 = 0.7320026159286499 + 1.0 * 6.166600227355957
Epoch 360, val loss: 0.949437141418457
Epoch 370, training loss: 6.858410835266113 = 0.6980910897254944 + 1.0 * 6.160319805145264
Epoch 370, val loss: 0.9257411956787109
Epoch 380, training loss: 6.821871757507324 = 0.6663119196891785 + 1.0 * 6.15556001663208
Epoch 380, val loss: 0.9035429358482361
Epoch 390, training loss: 6.789003372192383 = 0.6362703442573547 + 1.0 * 6.152732849121094
Epoch 390, val loss: 0.8828839659690857
Epoch 400, training loss: 6.758618354797363 = 0.6079642176628113 + 1.0 * 6.150654315948486
Epoch 400, val loss: 0.8635379076004028
Epoch 410, training loss: 6.728971481323242 = 0.5812517404556274 + 1.0 * 6.147719860076904
Epoch 410, val loss: 0.8456544280052185
Epoch 420, training loss: 6.6992411613464355 = 0.5558844208717346 + 1.0 * 6.143356800079346
Epoch 420, val loss: 0.8292152285575867
Epoch 430, training loss: 6.6760406494140625 = 0.5318555235862732 + 1.0 * 6.1441850662231445
Epoch 430, val loss: 0.8139286637306213
Epoch 440, training loss: 6.646188735961914 = 0.508978009223938 + 1.0 * 6.137210845947266
Epoch 440, val loss: 0.7999362349510193
Epoch 450, training loss: 6.62365198135376 = 0.4870424270629883 + 1.0 * 6.1366095542907715
Epoch 450, val loss: 0.7869783639907837
Epoch 460, training loss: 6.5988569259643555 = 0.465938001871109 + 1.0 * 6.132918834686279
Epoch 460, val loss: 0.7751404047012329
Epoch 470, training loss: 6.577162265777588 = 0.4457704722881317 + 1.0 * 6.131392002105713
Epoch 470, val loss: 0.7642678022384644
Epoch 480, training loss: 6.553075313568115 = 0.4261835217475891 + 1.0 * 6.126891613006592
Epoch 480, val loss: 0.7543028593063354
Epoch 490, training loss: 6.539529323577881 = 0.40715378522872925 + 1.0 * 6.132375717163086
Epoch 490, val loss: 0.7450490593910217
Epoch 500, training loss: 6.515896797180176 = 0.38872528076171875 + 1.0 * 6.127171516418457
Epoch 500, val loss: 0.7365283966064453
Epoch 510, training loss: 6.4931230545043945 = 0.37099388241767883 + 1.0 * 6.122128963470459
Epoch 510, val loss: 0.7288889288902283
Epoch 520, training loss: 6.473686218261719 = 0.35370251536369324 + 1.0 * 6.119983673095703
Epoch 520, val loss: 0.7218916416168213
Epoch 530, training loss: 6.458076477050781 = 0.3368822932243347 + 1.0 * 6.121194362640381
Epoch 530, val loss: 0.7154056429862976
Epoch 540, training loss: 6.441626071929932 = 0.32059523463249207 + 1.0 * 6.121030807495117
Epoch 540, val loss: 0.7093744277954102
Epoch 550, training loss: 6.419271469116211 = 0.3047240078449249 + 1.0 * 6.114547252655029
Epoch 550, val loss: 0.7039768695831299
Epoch 560, training loss: 6.402724266052246 = 0.2894046902656555 + 1.0 * 6.113319396972656
Epoch 560, val loss: 0.6991850137710571
Epoch 570, training loss: 6.384122371673584 = 0.27456408739089966 + 1.0 * 6.10955810546875
Epoch 570, val loss: 0.6950159668922424
Epoch 580, training loss: 6.380398750305176 = 0.26019078493118286 + 1.0 * 6.120207786560059
Epoch 580, val loss: 0.6915110349655151
Epoch 590, training loss: 6.355469703674316 = 0.2464795559644699 + 1.0 * 6.10899019241333
Epoch 590, val loss: 0.6885318756103516
Epoch 600, training loss: 6.34074592590332 = 0.233323872089386 + 1.0 * 6.107421875
Epoch 600, val loss: 0.686286211013794
Epoch 610, training loss: 6.328330993652344 = 0.22077246010303497 + 1.0 * 6.107558727264404
Epoch 610, val loss: 0.6846060752868652
Epoch 620, training loss: 6.316003799438477 = 0.20875583589076996 + 1.0 * 6.107247829437256
Epoch 620, val loss: 0.6835463643074036
Epoch 630, training loss: 6.298376083374023 = 0.19733773171901703 + 1.0 * 6.101038455963135
Epoch 630, val loss: 0.6830691695213318
Epoch 640, training loss: 6.2860612869262695 = 0.1864725798368454 + 1.0 * 6.099588871002197
Epoch 640, val loss: 0.6830715537071228
Epoch 650, training loss: 6.288619041442871 = 0.176236093044281 + 1.0 * 6.112382888793945
Epoch 650, val loss: 0.6836884617805481
Epoch 660, training loss: 6.262852668762207 = 0.16657625138759613 + 1.0 * 6.09627628326416
Epoch 660, val loss: 0.6848462224006653
Epoch 670, training loss: 6.253243923187256 = 0.15744273364543915 + 1.0 * 6.09580135345459
Epoch 670, val loss: 0.6865940690040588
Epoch 680, training loss: 6.2451629638671875 = 0.14882288873195648 + 1.0 * 6.096340179443359
Epoch 680, val loss: 0.6886541843414307
Epoch 690, training loss: 6.2354207038879395 = 0.14063787460327148 + 1.0 * 6.094782829284668
Epoch 690, val loss: 0.6910806894302368
Epoch 700, training loss: 6.2262396812438965 = 0.13287566602230072 + 1.0 * 6.093364238739014
Epoch 700, val loss: 0.6942198276519775
Epoch 710, training loss: 6.215639114379883 = 0.1255049705505371 + 1.0 * 6.090134143829346
Epoch 710, val loss: 0.6975279450416565
Epoch 720, training loss: 6.224035739898682 = 0.11849386990070343 + 1.0 * 6.105541706085205
Epoch 720, val loss: 0.7011501789093018
Epoch 730, training loss: 6.200354099273682 = 0.11181524395942688 + 1.0 * 6.088538646697998
Epoch 730, val loss: 0.7051177024841309
Epoch 740, training loss: 6.193149089813232 = 0.10564316809177399 + 1.0 * 6.08750581741333
Epoch 740, val loss: 0.7095203399658203
Epoch 750, training loss: 6.185408592224121 = 0.09996051341295242 + 1.0 * 6.085448265075684
Epoch 750, val loss: 0.7142906785011292
Epoch 760, training loss: 6.183746337890625 = 0.09471341967582703 + 1.0 * 6.089033126831055
Epoch 760, val loss: 0.7193174362182617
Epoch 770, training loss: 6.176242351531982 = 0.08984178304672241 + 1.0 * 6.086400508880615
Epoch 770, val loss: 0.7245298027992249
Epoch 780, training loss: 6.172625541687012 = 0.08534963428974152 + 1.0 * 6.087275981903076
Epoch 780, val loss: 0.7300731539726257
Epoch 790, training loss: 6.162581920623779 = 0.08116932213306427 + 1.0 * 6.0814127922058105
Epoch 790, val loss: 0.7357603907585144
Epoch 800, training loss: 6.157041072845459 = 0.07726763188838959 + 1.0 * 6.079773426055908
Epoch 800, val loss: 0.7416216731071472
Epoch 810, training loss: 6.157191276550293 = 0.07361762225627899 + 1.0 * 6.083573818206787
Epoch 810, val loss: 0.7476112842559814
Epoch 820, training loss: 6.15595817565918 = 0.07021373510360718 + 1.0 * 6.085744380950928
Epoch 820, val loss: 0.7535547018051147
Epoch 830, training loss: 6.146643161773682 = 0.0670384094119072 + 1.0 * 6.079604625701904
Epoch 830, val loss: 0.7597295045852661
Epoch 840, training loss: 6.140626430511475 = 0.06406477838754654 + 1.0 * 6.076561450958252
Epoch 840, val loss: 0.765968382358551
Epoch 850, training loss: 6.138350486755371 = 0.061268147081136703 + 1.0 * 6.07708215713501
Epoch 850, val loss: 0.7722346782684326
Epoch 860, training loss: 6.134164810180664 = 0.05863511189818382 + 1.0 * 6.0755295753479
Epoch 860, val loss: 0.7783117890357971
Epoch 870, training loss: 6.129870891571045 = 0.05615594610571861 + 1.0 * 6.073714733123779
Epoch 870, val loss: 0.7846846580505371
Epoch 880, training loss: 6.1268815994262695 = 0.053819503635168076 + 1.0 * 6.073061943054199
Epoch 880, val loss: 0.7909818291664124
Epoch 890, training loss: 6.128721237182617 = 0.05161280184984207 + 1.0 * 6.077108383178711
Epoch 890, val loss: 0.7972314953804016
Epoch 900, training loss: 6.122466564178467 = 0.04953516647219658 + 1.0 * 6.072931289672852
Epoch 900, val loss: 0.8034022450447083
Epoch 910, training loss: 6.124500751495361 = 0.04757440835237503 + 1.0 * 6.076926231384277
Epoch 910, val loss: 0.8096503019332886
Epoch 920, training loss: 6.118432521820068 = 0.04572241008281708 + 1.0 * 6.072710037231445
Epoch 920, val loss: 0.8158705830574036
Epoch 930, training loss: 6.111834526062012 = 0.04397120699286461 + 1.0 * 6.067863464355469
Epoch 930, val loss: 0.8221657872200012
Epoch 940, training loss: 6.109516143798828 = 0.042313553392887115 + 1.0 * 6.067202568054199
Epoch 940, val loss: 0.8283969759941101
Epoch 950, training loss: 6.122486591339111 = 0.04075004160404205 + 1.0 * 6.0817365646362305
Epoch 950, val loss: 0.8344618082046509
Epoch 960, training loss: 6.108397483825684 = 0.03924538567662239 + 1.0 * 6.069151878356934
Epoch 960, val loss: 0.8405189514160156
Epoch 970, training loss: 6.10277795791626 = 0.03784383833408356 + 1.0 * 6.064934253692627
Epoch 970, val loss: 0.846756637096405
Epoch 980, training loss: 6.1013569831848145 = 0.03650025278329849 + 1.0 * 6.06485652923584
Epoch 980, val loss: 0.8528217673301697
Epoch 990, training loss: 6.100965976715088 = 0.0352306067943573 + 1.0 * 6.065735340118408
Epoch 990, val loss: 0.8586825728416443
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7417
Flip ASR: 0.6978/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.312997817993164 = 1.9390672445297241 + 1.0 * 8.373930931091309
Epoch 0, val loss: 1.945652961730957
Epoch 10, training loss: 10.303175926208496 = 1.9294880628585815 + 1.0 * 8.373687744140625
Epoch 10, val loss: 1.935678482055664
Epoch 20, training loss: 10.289373397827148 = 1.9175218343734741 + 1.0 * 8.371851921081543
Epoch 20, val loss: 1.92314612865448
Epoch 30, training loss: 10.2583646774292 = 1.9005558490753174 + 1.0 * 8.357809066772461
Epoch 30, val loss: 1.9053505659103394
Epoch 40, training loss: 10.141282081604004 = 1.8774452209472656 + 1.0 * 8.263836860656738
Epoch 40, val loss: 1.8818515539169312
Epoch 50, training loss: 9.530542373657227 = 1.8533401489257812 + 1.0 * 7.677202224731445
Epoch 50, val loss: 1.8589221239089966
Epoch 60, training loss: 9.078155517578125 = 1.8348520994186401 + 1.0 * 7.243303298950195
Epoch 60, val loss: 1.8416755199432373
Epoch 70, training loss: 8.784549713134766 = 1.8189399242401123 + 1.0 * 6.965609550476074
Epoch 70, val loss: 1.8261638879776
Epoch 80, training loss: 8.615218162536621 = 1.8027364015579224 + 1.0 * 6.812481880187988
Epoch 80, val loss: 1.810294508934021
Epoch 90, training loss: 8.497543334960938 = 1.7842196226119995 + 1.0 * 6.713324069976807
Epoch 90, val loss: 1.792513132095337
Epoch 100, training loss: 8.398882865905762 = 1.7650665044784546 + 1.0 * 6.633816719055176
Epoch 100, val loss: 1.7748688459396362
Epoch 110, training loss: 8.300248146057129 = 1.7453949451446533 + 1.0 * 6.5548529624938965
Epoch 110, val loss: 1.7573868036270142
Epoch 120, training loss: 8.215131759643555 = 1.723824381828308 + 1.0 * 6.491307258605957
Epoch 120, val loss: 1.7384675741195679
Epoch 130, training loss: 8.143644332885742 = 1.6989318132400513 + 1.0 * 6.444712162017822
Epoch 130, val loss: 1.7167742252349854
Epoch 140, training loss: 8.07681941986084 = 1.669556975364685 + 1.0 * 6.407262325286865
Epoch 140, val loss: 1.6916158199310303
Epoch 150, training loss: 8.013717651367188 = 1.6350390911102295 + 1.0 * 6.378678321838379
Epoch 150, val loss: 1.662404179573059
Epoch 160, training loss: 7.948005199432373 = 1.595473289489746 + 1.0 * 6.352531909942627
Epoch 160, val loss: 1.6295043230056763
Epoch 170, training loss: 7.880606651306152 = 1.5500833988189697 + 1.0 * 6.3305230140686035
Epoch 170, val loss: 1.592186689376831
Epoch 180, training loss: 7.811897277832031 = 1.4997849464416504 + 1.0 * 6.312112331390381
Epoch 180, val loss: 1.5516791343688965
Epoch 190, training loss: 7.7409281730651855 = 1.4468144178390503 + 1.0 * 6.294113636016846
Epoch 190, val loss: 1.5096849203109741
Epoch 200, training loss: 7.670901298522949 = 1.3916770219802856 + 1.0 * 6.279224395751953
Epoch 200, val loss: 1.4668478965759277
Epoch 210, training loss: 7.603850841522217 = 1.336005687713623 + 1.0 * 6.267845153808594
Epoch 210, val loss: 1.4246208667755127
Epoch 220, training loss: 7.539239883422852 = 1.2817033529281616 + 1.0 * 6.2575364112854
Epoch 220, val loss: 1.384160041809082
Epoch 230, training loss: 7.474062919616699 = 1.2281733751296997 + 1.0 * 6.245889663696289
Epoch 230, val loss: 1.3450819253921509
Epoch 240, training loss: 7.41212797164917 = 1.1751025915145874 + 1.0 * 6.237025260925293
Epoch 240, val loss: 1.3068023920059204
Epoch 250, training loss: 7.353738307952881 = 1.122442364692688 + 1.0 * 6.231296062469482
Epoch 250, val loss: 1.2694311141967773
Epoch 260, training loss: 7.293729305267334 = 1.0704365968704224 + 1.0 * 6.223292827606201
Epoch 260, val loss: 1.2326467037200928
Epoch 270, training loss: 7.2321624755859375 = 1.017986536026001 + 1.0 * 6.214175701141357
Epoch 270, val loss: 1.195717453956604
Epoch 280, training loss: 7.187222957611084 = 0.9647887349128723 + 1.0 * 6.222434043884277
Epoch 280, val loss: 1.1584172248840332
Epoch 290, training loss: 7.115076065063477 = 0.9129956364631653 + 1.0 * 6.202080249786377
Epoch 290, val loss: 1.1220163106918335
Epoch 300, training loss: 7.061478137969971 = 0.8624425530433655 + 1.0 * 6.19903564453125
Epoch 300, val loss: 1.0873972177505493
Epoch 310, training loss: 7.003543853759766 = 0.8130552172660828 + 1.0 * 6.190488815307617
Epoch 310, val loss: 1.0541013479232788
Epoch 320, training loss: 6.950126647949219 = 0.7652990818023682 + 1.0 * 6.1848273277282715
Epoch 320, val loss: 1.0225502252578735
Epoch 330, training loss: 6.901910781860352 = 0.7196215391159058 + 1.0 * 6.182289123535156
Epoch 330, val loss: 0.9932565093040466
Epoch 340, training loss: 6.854429244995117 = 0.6776069402694702 + 1.0 * 6.176822185516357
Epoch 340, val loss: 0.9671872854232788
Epoch 350, training loss: 6.8107171058654785 = 0.6391810774803162 + 1.0 * 6.171535968780518
Epoch 350, val loss: 0.944476842880249
Epoch 360, training loss: 6.770894527435303 = 0.6037288308143616 + 1.0 * 6.167165756225586
Epoch 360, val loss: 0.9245291352272034
Epoch 370, training loss: 6.733859062194824 = 0.5708324313163757 + 1.0 * 6.163026809692383
Epoch 370, val loss: 0.9070311784744263
Epoch 380, training loss: 6.7072601318359375 = 0.5407037734985352 + 1.0 * 6.166556358337402
Epoch 380, val loss: 0.8918846845626831
Epoch 390, training loss: 6.668628692626953 = 0.5129845142364502 + 1.0 * 6.155643939971924
Epoch 390, val loss: 0.8791249990463257
Epoch 400, training loss: 6.639183521270752 = 0.48716065287590027 + 1.0 * 6.152022838592529
Epoch 400, val loss: 0.8683258891105652
Epoch 410, training loss: 6.612941741943359 = 0.46283936500549316 + 1.0 * 6.150102138519287
Epoch 410, val loss: 0.8591346740722656
Epoch 420, training loss: 6.591394424438477 = 0.43992045521736145 + 1.0 * 6.1514739990234375
Epoch 420, val loss: 0.8514454364776611
Epoch 430, training loss: 6.561289310455322 = 0.41831567883491516 + 1.0 * 6.14297342300415
Epoch 430, val loss: 0.8450707197189331
Epoch 440, training loss: 6.536166191101074 = 0.397625207901001 + 1.0 * 6.138540744781494
Epoch 440, val loss: 0.8397706151008606
Epoch 450, training loss: 6.526407241821289 = 0.37775206565856934 + 1.0 * 6.148655414581299
Epoch 450, val loss: 0.8353831768035889
Epoch 460, training loss: 6.4951677322387695 = 0.35886821150779724 + 1.0 * 6.1362996101379395
Epoch 460, val loss: 0.8318417072296143
Epoch 470, training loss: 6.470638275146484 = 0.34067991375923157 + 1.0 * 6.129958152770996
Epoch 470, val loss: 0.8291160464286804
Epoch 480, training loss: 6.4508843421936035 = 0.32308557629585266 + 1.0 * 6.127798557281494
Epoch 480, val loss: 0.8271142840385437
Epoch 490, training loss: 6.453476905822754 = 0.30603983998298645 + 1.0 * 6.14743709564209
Epoch 490, val loss: 0.8258792757987976
Epoch 500, training loss: 6.422766208648682 = 0.28995099663734436 + 1.0 * 6.132815361022949
Epoch 500, val loss: 0.8252279162406921
Epoch 510, training loss: 6.395118713378906 = 0.2745753824710846 + 1.0 * 6.120543479919434
Epoch 510, val loss: 0.8253530263900757
Epoch 520, training loss: 6.378703594207764 = 0.2598128914833069 + 1.0 * 6.118890762329102
Epoch 520, val loss: 0.8263342976570129
Epoch 530, training loss: 6.362722396850586 = 0.24566102027893066 + 1.0 * 6.117061614990234
Epoch 530, val loss: 0.8279810547828674
Epoch 540, training loss: 6.347938060760498 = 0.23226581513881683 + 1.0 * 6.1156721115112305
Epoch 540, val loss: 0.8302527666091919
Epoch 550, training loss: 6.331862449645996 = 0.2196349948644638 + 1.0 * 6.112227439880371
Epoch 550, val loss: 0.8333604335784912
Epoch 560, training loss: 6.318635940551758 = 0.20765693485736847 + 1.0 * 6.110979080200195
Epoch 560, val loss: 0.8372446894645691
Epoch 570, training loss: 6.30877161026001 = 0.1962900012731552 + 1.0 * 6.112481594085693
Epoch 570, val loss: 0.8417738080024719
Epoch 580, training loss: 6.302842140197754 = 0.18567776679992676 + 1.0 * 6.117164134979248
Epoch 580, val loss: 0.8467220664024353
Epoch 590, training loss: 6.283619403839111 = 0.17568741738796234 + 1.0 * 6.107932090759277
Epoch 590, val loss: 0.8523280024528503
Epoch 600, training loss: 6.269989490509033 = 0.1662810742855072 + 1.0 * 6.103708267211914
Epoch 600, val loss: 0.8585487604141235
Epoch 610, training loss: 6.259088039398193 = 0.1574147492647171 + 1.0 * 6.101673126220703
Epoch 610, val loss: 0.8652661442756653
Epoch 620, training loss: 6.262442111968994 = 0.1491003930568695 + 1.0 * 6.113341808319092
Epoch 620, val loss: 0.8723838925361633
Epoch 630, training loss: 6.245408058166504 = 0.14136473834514618 + 1.0 * 6.104043483734131
Epoch 630, val loss: 0.8799777626991272
Epoch 640, training loss: 6.232609272003174 = 0.134137824177742 + 1.0 * 6.098471641540527
Epoch 640, val loss: 0.8879326581954956
Epoch 650, training loss: 6.223390579223633 = 0.1273443102836609 + 1.0 * 6.096046447753906
Epoch 650, val loss: 0.896284818649292
Epoch 660, training loss: 6.215233325958252 = 0.12096050381660461 + 1.0 * 6.094272613525391
Epoch 660, val loss: 0.9049332737922668
Epoch 670, training loss: 6.217712879180908 = 0.11496644467115402 + 1.0 * 6.102746486663818
Epoch 670, val loss: 0.9138882160186768
Epoch 680, training loss: 6.203089714050293 = 0.10941124707460403 + 1.0 * 6.0936784744262695
Epoch 680, val loss: 0.9228818416595459
Epoch 690, training loss: 6.195175647735596 = 0.1042150929570198 + 1.0 * 6.090960502624512
Epoch 690, val loss: 0.9321571588516235
Epoch 700, training loss: 6.1884002685546875 = 0.09932979941368103 + 1.0 * 6.0890703201293945
Epoch 700, val loss: 0.9415574073791504
Epoch 710, training loss: 6.181743621826172 = 0.09472024440765381 + 1.0 * 6.0870232582092285
Epoch 710, val loss: 0.9510691165924072
Epoch 720, training loss: 6.188909530639648 = 0.0903661772608757 + 1.0 * 6.098543167114258
Epoch 720, val loss: 0.9607532620429993
Epoch 730, training loss: 6.1765265464782715 = 0.0863262265920639 + 1.0 * 6.090200424194336
Epoch 730, val loss: 0.9703805446624756
Epoch 740, training loss: 6.168463230133057 = 0.08252742886543274 + 1.0 * 6.085935592651367
Epoch 740, val loss: 0.9801357984542847
Epoch 750, training loss: 6.161814212799072 = 0.07893509417772293 + 1.0 * 6.082879066467285
Epoch 750, val loss: 0.9900091290473938
Epoch 760, training loss: 6.158384799957275 = 0.07553345710039139 + 1.0 * 6.082851409912109
Epoch 760, val loss: 0.9998729825019836
Epoch 770, training loss: 6.155803680419922 = 0.07232670485973358 + 1.0 * 6.083477020263672
Epoch 770, val loss: 1.0097289085388184
Epoch 780, training loss: 6.151078701019287 = 0.06931416690349579 + 1.0 * 6.0817646980285645
Epoch 780, val loss: 1.0195918083190918
Epoch 790, training loss: 6.145980358123779 = 0.06646274775266647 + 1.0 * 6.079517841339111
Epoch 790, val loss: 1.0294630527496338
Epoch 800, training loss: 6.145213603973389 = 0.06374920904636383 + 1.0 * 6.0814642906188965
Epoch 800, val loss: 1.0392966270446777
Epoch 810, training loss: 6.138641834259033 = 0.06117993965744972 + 1.0 * 6.0774617195129395
Epoch 810, val loss: 1.0491689443588257
Epoch 820, training loss: 6.135407447814941 = 0.05874873325228691 + 1.0 * 6.076658725738525
Epoch 820, val loss: 1.0589152574539185
Epoch 830, training loss: 6.131720066070557 = 0.05643186718225479 + 1.0 * 6.07528829574585
Epoch 830, val loss: 1.0687694549560547
Epoch 840, training loss: 6.137352466583252 = 0.054235637187957764 + 1.0 * 6.0831170082092285
Epoch 840, val loss: 1.0785071849822998
Epoch 850, training loss: 6.128976821899414 = 0.05215045437216759 + 1.0 * 6.076826572418213
Epoch 850, val loss: 1.0881770849227905
Epoch 860, training loss: 6.12275505065918 = 0.050177209079265594 + 1.0 * 6.072577953338623
Epoch 860, val loss: 1.0977963209152222
Epoch 870, training loss: 6.125369071960449 = 0.04829617589712143 + 1.0 * 6.077073097229004
Epoch 870, val loss: 1.1073532104492188
Epoch 880, training loss: 6.119558811187744 = 0.04650785028934479 + 1.0 * 6.0730509757995605
Epoch 880, val loss: 1.116754174232483
Epoch 890, training loss: 6.115712642669678 = 0.04480459541082382 + 1.0 * 6.070908069610596
Epoch 890, val loss: 1.1261051893234253
Epoch 900, training loss: 6.127086639404297 = 0.04318481683731079 + 1.0 * 6.083901882171631
Epoch 900, val loss: 1.135363221168518
Epoch 910, training loss: 6.11138391494751 = 0.04165461286902428 + 1.0 * 6.069729328155518
Epoch 910, val loss: 1.1444376707077026
Epoch 920, training loss: 6.1079840660095215 = 0.040189310908317566 + 1.0 * 6.0677947998046875
Epoch 920, val loss: 1.1535311937332153
Epoch 930, training loss: 6.104808807373047 = 0.038790784776210785 + 1.0 * 6.066018104553223
Epoch 930, val loss: 1.1625393629074097
Epoch 940, training loss: 6.109772682189941 = 0.0374523401260376 + 1.0 * 6.072320461273193
Epoch 940, val loss: 1.1714767217636108
Epoch 950, training loss: 6.102820873260498 = 0.0361778549849987 + 1.0 * 6.066643238067627
Epoch 950, val loss: 1.180261254310608
Epoch 960, training loss: 6.101152420043945 = 0.03496463596820831 + 1.0 * 6.066187858581543
Epoch 960, val loss: 1.1889560222625732
Epoch 970, training loss: 6.102140426635742 = 0.03380541130900383 + 1.0 * 6.068335056304932
Epoch 970, val loss: 1.1975222826004028
Epoch 980, training loss: 6.095164775848389 = 0.032707590609788895 + 1.0 * 6.062457084655762
Epoch 980, val loss: 1.2059593200683594
Epoch 990, training loss: 6.093800067901611 = 0.031656187027692795 + 1.0 * 6.062143802642822
Epoch 990, val loss: 1.2143290042877197
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7306
Flip ASR: 0.6889/225 nodes
The final ASR:0.75400, 0.02562, Accuracy:0.80988, 0.01062
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11626])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10580])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.311399459838867 = 1.9375907182693481 + 1.0 * 8.373808860778809
Epoch 0, val loss: 1.9333292245864868
Epoch 10, training loss: 10.301366806030273 = 1.928026556968689 + 1.0 * 8.373340606689453
Epoch 10, val loss: 1.9243413209915161
Epoch 20, training loss: 10.28648853302002 = 1.9160470962524414 + 1.0 * 8.370441436767578
Epoch 20, val loss: 1.9127233028411865
Epoch 30, training loss: 10.24936580657959 = 1.899367332458496 + 1.0 * 8.349998474121094
Epoch 30, val loss: 1.896361231803894
Epoch 40, training loss: 10.084207534790039 = 1.8785842657089233 + 1.0 * 8.205623626708984
Epoch 40, val loss: 1.8766065835952759
Epoch 50, training loss: 9.533868789672852 = 1.855168104171753 + 1.0 * 7.6787004470825195
Epoch 50, val loss: 1.8539670705795288
Epoch 60, training loss: 9.107584953308105 = 1.8357449769973755 + 1.0 * 7.2718400955200195
Epoch 60, val loss: 1.837261438369751
Epoch 70, training loss: 8.66437816619873 = 1.8257942199707031 + 1.0 * 6.838583946228027
Epoch 70, val loss: 1.8291833400726318
Epoch 80, training loss: 8.44464111328125 = 1.8175374269485474 + 1.0 * 6.627103328704834
Epoch 80, val loss: 1.8221417665481567
Epoch 90, training loss: 8.322357177734375 = 1.8069264888763428 + 1.0 * 6.515430927276611
Epoch 90, val loss: 1.8120895624160767
Epoch 100, training loss: 8.234667778015137 = 1.7949631214141846 + 1.0 * 6.439704418182373
Epoch 100, val loss: 1.8010149002075195
Epoch 110, training loss: 8.171537399291992 = 1.783687949180603 + 1.0 * 6.387849807739258
Epoch 110, val loss: 1.7908895015716553
Epoch 120, training loss: 8.122480392456055 = 1.7730224132537842 + 1.0 * 6.349457740783691
Epoch 120, val loss: 1.7813678979873657
Epoch 130, training loss: 8.079032897949219 = 1.7615994215011597 + 1.0 * 6.3174333572387695
Epoch 130, val loss: 1.7712593078613281
Epoch 140, training loss: 8.040190696716309 = 1.748369812965393 + 1.0 * 6.291820526123047
Epoch 140, val loss: 1.7597442865371704
Epoch 150, training loss: 8.005681991577148 = 1.7325198650360107 + 1.0 * 6.273161888122559
Epoch 150, val loss: 1.746336579322815
Epoch 160, training loss: 7.967804431915283 = 1.7134203910827637 + 1.0 * 6.2543840408325195
Epoch 160, val loss: 1.7302618026733398
Epoch 170, training loss: 7.928992748260498 = 1.6897072792053223 + 1.0 * 6.239285469055176
Epoch 170, val loss: 1.710471749305725
Epoch 180, training loss: 7.886748790740967 = 1.6601587533950806 + 1.0 * 6.226590156555176
Epoch 180, val loss: 1.6857147216796875
Epoch 190, training loss: 7.838760852813721 = 1.6236506700515747 + 1.0 * 6.2151103019714355
Epoch 190, val loss: 1.6550068855285645
Epoch 200, training loss: 7.7857666015625 = 1.5791857242584229 + 1.0 * 6.206580638885498
Epoch 200, val loss: 1.617851734161377
Epoch 210, training loss: 7.725330829620361 = 1.5279029607772827 + 1.0 * 6.197427749633789
Epoch 210, val loss: 1.5753729343414307
Epoch 220, training loss: 7.66140079498291 = 1.4712810516357422 + 1.0 * 6.190119743347168
Epoch 220, val loss: 1.5289582014083862
Epoch 230, training loss: 7.599388122558594 = 1.411678671836853 + 1.0 * 6.187709331512451
Epoch 230, val loss: 1.4809346199035645
Epoch 240, training loss: 7.532232761383057 = 1.3534188270568848 + 1.0 * 6.178813934326172
Epoch 240, val loss: 1.4350805282592773
Epoch 250, training loss: 7.470281600952148 = 1.2976198196411133 + 1.0 * 6.172661781311035
Epoch 250, val loss: 1.3921738862991333
Epoch 260, training loss: 7.415834426879883 = 1.2451512813568115 + 1.0 * 6.170682907104492
Epoch 260, val loss: 1.353123664855957
Epoch 270, training loss: 7.360045433044434 = 1.1969367265701294 + 1.0 * 6.163108825683594
Epoch 270, val loss: 1.318459391593933
Epoch 280, training loss: 7.308113098144531 = 1.151610255241394 + 1.0 * 6.156502723693848
Epoch 280, val loss: 1.2867273092269897
Epoch 290, training loss: 7.261688232421875 = 1.1076847314834595 + 1.0 * 6.154003620147705
Epoch 290, val loss: 1.2564387321472168
Epoch 300, training loss: 7.213570594787598 = 1.0651702880859375 + 1.0 * 6.14840030670166
Epoch 300, val loss: 1.2271515130996704
Epoch 310, training loss: 7.166317939758301 = 1.0233583450317383 + 1.0 * 6.1429595947265625
Epoch 310, val loss: 1.1985571384429932
Epoch 320, training loss: 7.122786998748779 = 0.9818071722984314 + 1.0 * 6.140979766845703
Epoch 320, val loss: 1.1697866916656494
Epoch 330, training loss: 7.078507423400879 = 0.9410856366157532 + 1.0 * 6.137421607971191
Epoch 330, val loss: 1.1413630247116089
Epoch 340, training loss: 7.03394889831543 = 0.9011778235435486 + 1.0 * 6.132771015167236
Epoch 340, val loss: 1.1133203506469727
Epoch 350, training loss: 6.992183685302734 = 0.8618158102035522 + 1.0 * 6.130367755889893
Epoch 350, val loss: 1.0854480266571045
Epoch 360, training loss: 6.951900005340576 = 0.8234094977378845 + 1.0 * 6.128490447998047
Epoch 360, val loss: 1.0580165386199951
Epoch 370, training loss: 6.910247325897217 = 0.7861090302467346 + 1.0 * 6.124138355255127
Epoch 370, val loss: 1.031262755393982
Epoch 380, training loss: 6.870039939880371 = 0.7492967247962952 + 1.0 * 6.120743274688721
Epoch 380, val loss: 1.0046910047531128
Epoch 390, training loss: 6.837667465209961 = 0.7129862308502197 + 1.0 * 6.124680995941162
Epoch 390, val loss: 0.9783800840377808
Epoch 400, training loss: 6.797702789306641 = 0.6778239011764526 + 1.0 * 6.119878768920898
Epoch 400, val loss: 0.9527426362037659
Epoch 410, training loss: 6.759385108947754 = 0.6439058780670166 + 1.0 * 6.115478992462158
Epoch 410, val loss: 0.9282897710800171
Epoch 420, training loss: 6.723315238952637 = 0.611296534538269 + 1.0 * 6.112018585205078
Epoch 420, val loss: 0.905000627040863
Epoch 430, training loss: 6.69317102432251 = 0.5800604224205017 + 1.0 * 6.113110542297363
Epoch 430, val loss: 0.8831872344017029
Epoch 440, training loss: 6.662407398223877 = 0.5505315661430359 + 1.0 * 6.111876010894775
Epoch 440, val loss: 0.8632615804672241
Epoch 450, training loss: 6.628434658050537 = 0.523016095161438 + 1.0 * 6.105418682098389
Epoch 450, val loss: 0.8454473614692688
Epoch 460, training loss: 6.602085113525391 = 0.4971616268157959 + 1.0 * 6.104923725128174
Epoch 460, val loss: 0.8294963836669922
Epoch 470, training loss: 6.577768802642822 = 0.47287517786026 + 1.0 * 6.104893684387207
Epoch 470, val loss: 0.8152856230735779
Epoch 480, training loss: 6.552716255187988 = 0.45022597908973694 + 1.0 * 6.102490425109863
Epoch 480, val loss: 0.8028298020362854
Epoch 490, training loss: 6.527431964874268 = 0.4287398159503937 + 1.0 * 6.098691940307617
Epoch 490, val loss: 0.7919115424156189
Epoch 500, training loss: 6.5077595710754395 = 0.4082421362400055 + 1.0 * 6.099517345428467
Epoch 500, val loss: 0.7821421027183533
Epoch 510, training loss: 6.484164237976074 = 0.388600617647171 + 1.0 * 6.0955634117126465
Epoch 510, val loss: 0.7734240889549255
Epoch 520, training loss: 6.463377952575684 = 0.3696427345275879 + 1.0 * 6.093735218048096
Epoch 520, val loss: 0.7657605409622192
Epoch 530, training loss: 6.446982383728027 = 0.3512328565120697 + 1.0 * 6.095749378204346
Epoch 530, val loss: 0.7589269280433655
Epoch 540, training loss: 6.4243998527526855 = 0.33353152871131897 + 1.0 * 6.0908684730529785
Epoch 540, val loss: 0.752936840057373
Epoch 550, training loss: 6.404921054840088 = 0.3163274824619293 + 1.0 * 6.088593482971191
Epoch 550, val loss: 0.7477359175682068
Epoch 560, training loss: 6.393881320953369 = 0.29959365725517273 + 1.0 * 6.094287872314453
Epoch 560, val loss: 0.743148148059845
Epoch 570, training loss: 6.3707146644592285 = 0.28336429595947266 + 1.0 * 6.087350368499756
Epoch 570, val loss: 0.7391780614852905
Epoch 580, training loss: 6.3520402908325195 = 0.267670601606369 + 1.0 * 6.084369659423828
Epoch 580, val loss: 0.7357432246208191
Epoch 590, training loss: 6.336379051208496 = 0.2524363100528717 + 1.0 * 6.083942890167236
Epoch 590, val loss: 0.7326750159263611
Epoch 600, training loss: 6.322726249694824 = 0.23772960901260376 + 1.0 * 6.084996700286865
Epoch 600, val loss: 0.7299787402153015
Epoch 610, training loss: 6.305456161499023 = 0.22368541359901428 + 1.0 * 6.081770896911621
Epoch 610, val loss: 0.7276920080184937
Epoch 620, training loss: 6.289534568786621 = 0.21020175516605377 + 1.0 * 6.0793328285217285
Epoch 620, val loss: 0.7257557511329651
Epoch 630, training loss: 6.2755632400512695 = 0.19724330306053162 + 1.0 * 6.078320026397705
Epoch 630, val loss: 0.7241334319114685
Epoch 640, training loss: 6.26473331451416 = 0.1848931908607483 + 1.0 * 6.079840183258057
Epoch 640, val loss: 0.7229104042053223
Epoch 650, training loss: 6.249895095825195 = 0.17323923110961914 + 1.0 * 6.076655864715576
Epoch 650, val loss: 0.7220334410667419
Epoch 660, training loss: 6.245175361633301 = 0.1622365564107895 + 1.0 * 6.0829386711120605
Epoch 660, val loss: 0.7215732336044312
Epoch 670, training loss: 6.22861385345459 = 0.1520472913980484 + 1.0 * 6.076566696166992
Epoch 670, val loss: 0.7214414477348328
Epoch 680, training loss: 6.216111660003662 = 0.14256201684474945 + 1.0 * 6.073549747467041
Epoch 680, val loss: 0.721855640411377
Epoch 690, training loss: 6.2043023109436035 = 0.13373886048793793 + 1.0 * 6.070563316345215
Epoch 690, val loss: 0.7226254940032959
Epoch 700, training loss: 6.204895973205566 = 0.12555798888206482 + 1.0 * 6.079338073730469
Epoch 700, val loss: 0.7238368391990662
Epoch 710, training loss: 6.193248271942139 = 0.11801332980394363 + 1.0 * 6.075234889984131
Epoch 710, val loss: 0.7253245115280151
Epoch 720, training loss: 6.178493976593018 = 0.11110785603523254 + 1.0 * 6.067386150360107
Epoch 720, val loss: 0.7273168563842773
Epoch 730, training loss: 6.17197322845459 = 0.10471547394990921 + 1.0 * 6.067257881164551
Epoch 730, val loss: 0.7297104001045227
Epoch 740, training loss: 6.170751571655273 = 0.09879609197378159 + 1.0 * 6.071955680847168
Epoch 740, val loss: 0.7324292659759521
Epoch 750, training loss: 6.158799171447754 = 0.09333564341068268 + 1.0 * 6.065463542938232
Epoch 750, val loss: 0.735459566116333
Epoch 760, training loss: 6.152126312255859 = 0.08829005062580109 + 1.0 * 6.063836097717285
Epoch 760, val loss: 0.738745391368866
Epoch 770, training loss: 6.148247241973877 = 0.08361031860113144 + 1.0 * 6.064636707305908
Epoch 770, val loss: 0.7422305345535278
Epoch 780, training loss: 6.142561912536621 = 0.07930111885070801 + 1.0 * 6.063260555267334
Epoch 780, val loss: 0.7458850145339966
Epoch 790, training loss: 6.137588024139404 = 0.07529472559690475 + 1.0 * 6.062293529510498
Epoch 790, val loss: 0.7497973442077637
Epoch 800, training loss: 6.132686614990234 = 0.07157211005687714 + 1.0 * 6.061114311218262
Epoch 800, val loss: 0.7538167834281921
Epoch 810, training loss: 6.126764297485352 = 0.06811767816543579 + 1.0 * 6.0586466789245605
Epoch 810, val loss: 0.7579646110534668
Epoch 820, training loss: 6.122622489929199 = 0.06487784534692764 + 1.0 * 6.05774450302124
Epoch 820, val loss: 0.7622283101081848
Epoch 830, training loss: 6.123789310455322 = 0.06185152381658554 + 1.0 * 6.0619378089904785
Epoch 830, val loss: 0.7665926814079285
Epoch 840, training loss: 6.118068695068359 = 0.05903032422065735 + 1.0 * 6.059038162231445
Epoch 840, val loss: 0.7709977030754089
Epoch 850, training loss: 6.1168951988220215 = 0.05639718845486641 + 1.0 * 6.060498237609863
Epoch 850, val loss: 0.7754162549972534
Epoch 860, training loss: 6.108686923980713 = 0.053945910185575485 + 1.0 * 6.054740905761719
Epoch 860, val loss: 0.7798870801925659
Epoch 870, training loss: 6.105264186859131 = 0.05163593962788582 + 1.0 * 6.053628444671631
Epoch 870, val loss: 0.7843949794769287
Epoch 880, training loss: 6.102535724639893 = 0.04945757985115051 + 1.0 * 6.0530781745910645
Epoch 880, val loss: 0.7889131903648376
Epoch 890, training loss: 6.10631799697876 = 0.047408320009708405 + 1.0 * 6.058909893035889
Epoch 890, val loss: 0.7934390306472778
Epoch 900, training loss: 6.098287105560303 = 0.04547441750764847 + 1.0 * 6.052812576293945
Epoch 900, val loss: 0.7979244589805603
Epoch 910, training loss: 6.096566200256348 = 0.043659720569849014 + 1.0 * 6.052906513214111
Epoch 910, val loss: 0.8025025725364685
Epoch 920, training loss: 6.092019081115723 = 0.04194645583629608 + 1.0 * 6.05007266998291
Epoch 920, val loss: 0.8070115447044373
Epoch 930, training loss: 6.089102268218994 = 0.04033255949616432 + 1.0 * 6.048769474029541
Epoch 930, val loss: 0.8114991784095764
Epoch 940, training loss: 6.089728832244873 = 0.038806550204753876 + 1.0 * 6.050922393798828
Epoch 940, val loss: 0.8160502314567566
Epoch 950, training loss: 6.085982799530029 = 0.03736092150211334 + 1.0 * 6.048621654510498
Epoch 950, val loss: 0.82049161195755
Epoch 960, training loss: 6.08240270614624 = 0.03599655255675316 + 1.0 * 6.046406269073486
Epoch 960, val loss: 0.8249512910842896
Epoch 970, training loss: 6.07975435256958 = 0.03469983860850334 + 1.0 * 6.0450544357299805
Epoch 970, val loss: 0.8294100165367126
Epoch 980, training loss: 6.087131977081299 = 0.033469248563051224 + 1.0 * 6.0536627769470215
Epoch 980, val loss: 0.8338317275047302
Epoch 990, training loss: 6.081604480743408 = 0.03229845315217972 + 1.0 * 6.0493059158325195
Epoch 990, val loss: 0.8380536437034607
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7048
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.30131721496582 = 1.9274859428405762 + 1.0 * 8.373831748962402
Epoch 0, val loss: 1.9266881942749023
Epoch 10, training loss: 10.291346549987793 = 1.9179840087890625 + 1.0 * 8.37336254119873
Epoch 10, val loss: 1.9175218343734741
Epoch 20, training loss: 10.276594161987305 = 1.9059302806854248 + 1.0 * 8.3706636428833
Epoch 20, val loss: 1.9055675268173218
Epoch 30, training loss: 10.240228652954102 = 1.8888391256332397 + 1.0 * 8.35138988494873
Epoch 30, val loss: 1.8884514570236206
Epoch 40, training loss: 10.045438766479492 = 1.866599202156067 + 1.0 * 8.178839683532715
Epoch 40, val loss: 1.8668732643127441
Epoch 50, training loss: 9.343218803405762 = 1.8424279689788818 + 1.0 * 7.500791072845459
Epoch 50, val loss: 1.8436487913131714
Epoch 60, training loss: 9.015519142150879 = 1.8249053955078125 + 1.0 * 7.190613746643066
Epoch 60, val loss: 1.826768398284912
Epoch 70, training loss: 8.6592378616333 = 1.8162730932235718 + 1.0 * 6.8429646492004395
Epoch 70, val loss: 1.8182804584503174
Epoch 80, training loss: 8.477612495422363 = 1.8059945106506348 + 1.0 * 6.6716179847717285
Epoch 80, val loss: 1.808043360710144
Epoch 90, training loss: 8.346627235412598 = 1.7935521602630615 + 1.0 * 6.553074836730957
Epoch 90, val loss: 1.7958849668502808
Epoch 100, training loss: 8.259184837341309 = 1.7807011604309082 + 1.0 * 6.4784836769104
Epoch 100, val loss: 1.7834320068359375
Epoch 110, training loss: 8.187862396240234 = 1.7682467699050903 + 1.0 * 6.419615268707275
Epoch 110, val loss: 1.771285891532898
Epoch 120, training loss: 8.132977485656738 = 1.754448652267456 + 1.0 * 6.378528594970703
Epoch 120, val loss: 1.7580443620681763
Epoch 130, training loss: 8.080150604248047 = 1.7377148866653442 + 1.0 * 6.342435836791992
Epoch 130, val loss: 1.742658019065857
Epoch 140, training loss: 8.031054496765137 = 1.7172315120697021 + 1.0 * 6.3138227462768555
Epoch 140, val loss: 1.724624514579773
Epoch 150, training loss: 7.983669281005859 = 1.6921098232269287 + 1.0 * 6.29155969619751
Epoch 150, val loss: 1.7030842304229736
Epoch 160, training loss: 7.934064865112305 = 1.6611782312393188 + 1.0 * 6.272886753082275
Epoch 160, val loss: 1.6773372888565063
Epoch 170, training loss: 7.880687236785889 = 1.6229901313781738 + 1.0 * 6.257697105407715
Epoch 170, val loss: 1.6457269191741943
Epoch 180, training loss: 7.821681976318359 = 1.5764285326004028 + 1.0 * 6.245253562927246
Epoch 180, val loss: 1.607450246810913
Epoch 190, training loss: 7.7563018798828125 = 1.5223467350006104 + 1.0 * 6.233954906463623
Epoch 190, val loss: 1.5633370876312256
Epoch 200, training loss: 7.688623428344727 = 1.4630773067474365 + 1.0 * 6.225545883178711
Epoch 200, val loss: 1.5157009363174438
Epoch 210, training loss: 7.619001388549805 = 1.403336763381958 + 1.0 * 6.215664386749268
Epoch 210, val loss: 1.4684380292892456
Epoch 220, training loss: 7.55794095993042 = 1.3462144136428833 + 1.0 * 6.211726665496826
Epoch 220, val loss: 1.4240864515304565
Epoch 230, training loss: 7.494296550750732 = 1.2948955297470093 + 1.0 * 6.199400901794434
Epoch 230, val loss: 1.3850733041763306
Epoch 240, training loss: 7.440135478973389 = 1.2490979433059692 + 1.0 * 6.191037654876709
Epoch 240, val loss: 1.3513389825820923
Epoch 250, training loss: 7.396768569946289 = 1.208335041999817 + 1.0 * 6.188433647155762
Epoch 250, val loss: 1.3224154710769653
Epoch 260, training loss: 7.348974227905273 = 1.1716039180755615 + 1.0 * 6.177370071411133
Epoch 260, val loss: 1.2973383665084839
Epoch 270, training loss: 7.3069024085998535 = 1.136845588684082 + 1.0 * 6.1700568199157715
Epoch 270, val loss: 1.2740001678466797
Epoch 280, training loss: 7.270197868347168 = 1.103125810623169 + 1.0 * 6.16707181930542
Epoch 280, val loss: 1.2517975568771362
Epoch 290, training loss: 7.230067729949951 = 1.0699819326400757 + 1.0 * 6.160085678100586
Epoch 290, val loss: 1.2301080226898193
Epoch 300, training loss: 7.190760612487793 = 1.0366512537002563 + 1.0 * 6.154109477996826
Epoch 300, val loss: 1.2082571983337402
Epoch 310, training loss: 7.152463912963867 = 1.0029950141906738 + 1.0 * 6.149468898773193
Epoch 310, val loss: 1.1861447095870972
Epoch 320, training loss: 7.124624729156494 = 0.9695859551429749 + 1.0 * 6.155038833618164
Epoch 320, val loss: 1.1641730070114136
Epoch 330, training loss: 7.079919338226318 = 0.9368906617164612 + 1.0 * 6.143028736114502
Epoch 330, val loss: 1.1428807973861694
Epoch 340, training loss: 7.042835235595703 = 0.9046018719673157 + 1.0 * 6.138233184814453
Epoch 340, val loss: 1.1217366456985474
Epoch 350, training loss: 7.008657455444336 = 0.8724097013473511 + 1.0 * 6.136247634887695
Epoch 350, val loss: 1.1010535955429077
Epoch 360, training loss: 6.979823589324951 = 0.8403909802436829 + 1.0 * 6.139432430267334
Epoch 360, val loss: 1.080901861190796
Epoch 370, training loss: 6.940342426300049 = 0.8086041808128357 + 1.0 * 6.131738185882568
Epoch 370, val loss: 1.0611793994903564
Epoch 380, training loss: 6.90301513671875 = 0.7763159275054932 + 1.0 * 6.126698970794678
Epoch 380, val loss: 1.0412817001342773
Epoch 390, training loss: 6.866929054260254 = 0.7431177496910095 + 1.0 * 6.1238112449646
Epoch 390, val loss: 1.0210903882980347
Epoch 400, training loss: 6.829885482788086 = 0.7090917229652405 + 1.0 * 6.12079381942749
Epoch 400, val loss: 1.0005173683166504
Epoch 410, training loss: 6.795569896697998 = 0.674655020236969 + 1.0 * 6.120914936065674
Epoch 410, val loss: 0.9800070524215698
Epoch 420, training loss: 6.759178161621094 = 0.6402151584625244 + 1.0 * 6.11896276473999
Epoch 420, val loss: 0.9599500894546509
Epoch 430, training loss: 6.720296382904053 = 0.6057888269424438 + 1.0 * 6.114507675170898
Epoch 430, val loss: 0.9404329061508179
Epoch 440, training loss: 6.684697151184082 = 0.5717609524726868 + 1.0 * 6.112936019897461
Epoch 440, val loss: 0.9217626452445984
Epoch 450, training loss: 6.649938583374023 = 0.5386925339698792 + 1.0 * 6.111246109008789
Epoch 450, val loss: 0.9046406745910645
Epoch 460, training loss: 6.619236469268799 = 0.5069783329963684 + 1.0 * 6.112257957458496
Epoch 460, val loss: 0.8890818953514099
Epoch 470, training loss: 6.584481239318848 = 0.47716864943504333 + 1.0 * 6.1073126792907715
Epoch 470, val loss: 0.8758404850959778
Epoch 480, training loss: 6.553326606750488 = 0.44903871417045593 + 1.0 * 6.104288101196289
Epoch 480, val loss: 0.8645556569099426
Epoch 490, training loss: 6.529746055603027 = 0.4227331876754761 + 1.0 * 6.107012748718262
Epoch 490, val loss: 0.8551468253135681
Epoch 500, training loss: 6.498936653137207 = 0.3983080983161926 + 1.0 * 6.10062837600708
Epoch 500, val loss: 0.8481118679046631
Epoch 510, training loss: 6.472553730010986 = 0.37539660930633545 + 1.0 * 6.097157001495361
Epoch 510, val loss: 0.8424988985061646
Epoch 520, training loss: 6.452070236206055 = 0.35379505157470703 + 1.0 * 6.098275184631348
Epoch 520, val loss: 0.8384150862693787
Epoch 530, training loss: 6.431697368621826 = 0.3335227966308594 + 1.0 * 6.098174571990967
Epoch 530, val loss: 0.8356472253799438
Epoch 540, training loss: 6.406161308288574 = 0.31446781754493713 + 1.0 * 6.09169340133667
Epoch 540, val loss: 0.8339007496833801
Epoch 550, training loss: 6.386122703552246 = 0.29641032218933105 + 1.0 * 6.089712142944336
Epoch 550, val loss: 0.8330585956573486
Epoch 560, training loss: 6.381142616271973 = 0.27932244539260864 + 1.0 * 6.10181999206543
Epoch 560, val loss: 0.8329589366912842
Epoch 570, training loss: 6.352662086486816 = 0.263260155916214 + 1.0 * 6.089401721954346
Epoch 570, val loss: 0.8334499597549438
Epoch 580, training loss: 6.334288597106934 = 0.24813736975193024 + 1.0 * 6.086151123046875
Epoch 580, val loss: 0.8346966505050659
Epoch 590, training loss: 6.323210716247559 = 0.23379796743392944 + 1.0 * 6.089412689208984
Epoch 590, val loss: 0.8361911177635193
Epoch 600, training loss: 6.307953834533691 = 0.22033151984214783 + 1.0 * 6.087622165679932
Epoch 600, val loss: 0.8380556702613831
Epoch 610, training loss: 6.290616035461426 = 0.20764435827732086 + 1.0 * 6.082971572875977
Epoch 610, val loss: 0.8404152393341064
Epoch 620, training loss: 6.275821685791016 = 0.19567014276981354 + 1.0 * 6.080151557922363
Epoch 620, val loss: 0.8429919481277466
Epoch 630, training loss: 6.264305591583252 = 0.18432728946208954 + 1.0 * 6.0799784660339355
Epoch 630, val loss: 0.8458480834960938
Epoch 640, training loss: 6.250299453735352 = 0.17361479997634888 + 1.0 * 6.076684474945068
Epoch 640, val loss: 0.8489645719528198
Epoch 650, training loss: 6.244576454162598 = 0.16352880001068115 + 1.0 * 6.081047534942627
Epoch 650, val loss: 0.8522199988365173
Epoch 660, training loss: 6.231182098388672 = 0.1541118323802948 + 1.0 * 6.077070236206055
Epoch 660, val loss: 0.8557173013687134
Epoch 670, training loss: 6.218505859375 = 0.14525596797466278 + 1.0 * 6.073249816894531
Epoch 670, val loss: 0.8595132231712341
Epoch 680, training loss: 6.21499490737915 = 0.13695576786994934 + 1.0 * 6.078039169311523
Epoch 680, val loss: 0.8632280230522156
Epoch 690, training loss: 6.200900077819824 = 0.12920033931732178 + 1.0 * 6.071699619293213
Epoch 690, val loss: 0.8672922253608704
Epoch 700, training loss: 6.1932148933410645 = 0.12192462384700775 + 1.0 * 6.071290493011475
Epoch 700, val loss: 0.8715145587921143
Epoch 710, training loss: 6.188136100769043 = 0.11514107882976532 + 1.0 * 6.072995185852051
Epoch 710, val loss: 0.8756315112113953
Epoch 720, training loss: 6.178430557250977 = 0.10884284228086472 + 1.0 * 6.069587707519531
Epoch 720, val loss: 0.8802008628845215
Epoch 730, training loss: 6.168348789215088 = 0.1029752567410469 + 1.0 * 6.065373420715332
Epoch 730, val loss: 0.8848091959953308
Epoch 740, training loss: 6.162104606628418 = 0.09748615324497223 + 1.0 * 6.0646185874938965
Epoch 740, val loss: 0.8895160555839539
Epoch 750, training loss: 6.170476913452148 = 0.0923578217625618 + 1.0 * 6.078119277954102
Epoch 750, val loss: 0.8942779898643494
Epoch 760, training loss: 6.155384540557861 = 0.08764184266328812 + 1.0 * 6.067742824554443
Epoch 760, val loss: 0.8990281224250793
Epoch 770, training loss: 6.145970821380615 = 0.0832739993929863 + 1.0 * 6.062696933746338
Epoch 770, val loss: 0.9041758179664612
Epoch 780, training loss: 6.139437198638916 = 0.0791986957192421 + 1.0 * 6.060238361358643
Epoch 780, val loss: 0.9091808199882507
Epoch 790, training loss: 6.135384559631348 = 0.07538475841283798 + 1.0 * 6.059999942779541
Epoch 790, val loss: 0.9143509268760681
Epoch 800, training loss: 6.135648727416992 = 0.07181857526302338 + 1.0 * 6.063830375671387
Epoch 800, val loss: 0.919589102268219
Epoch 810, training loss: 6.127115726470947 = 0.06848833709955215 + 1.0 * 6.058627605438232
Epoch 810, val loss: 0.9249556660652161
Epoch 820, training loss: 6.126128673553467 = 0.0653722807765007 + 1.0 * 6.060756206512451
Epoch 820, val loss: 0.9303860068321228
Epoch 830, training loss: 6.126551628112793 = 0.062477223575115204 + 1.0 * 6.064074516296387
Epoch 830, val loss: 0.935619592666626
Epoch 840, training loss: 6.116103172302246 = 0.05977460741996765 + 1.0 * 6.056328773498535
Epoch 840, val loss: 0.9412934184074402
Epoch 850, training loss: 6.114211082458496 = 0.057236652821302414 + 1.0 * 6.056974411010742
Epoch 850, val loss: 0.9467573165893555
Epoch 860, training loss: 6.1075944900512695 = 0.054850902408361435 + 1.0 * 6.052743434906006
Epoch 860, val loss: 0.9521391987800598
Epoch 870, training loss: 6.1072187423706055 = 0.05260090529918671 + 1.0 * 6.054617881774902
Epoch 870, val loss: 0.9577602744102478
Epoch 880, training loss: 6.1042914390563965 = 0.05048065632581711 + 1.0 * 6.0538105964660645
Epoch 880, val loss: 0.9632436037063599
Epoch 890, training loss: 6.098907947540283 = 0.04848527908325195 + 1.0 * 6.050422668457031
Epoch 890, val loss: 0.968754768371582
Epoch 900, training loss: 6.096053600311279 = 0.04660465940833092 + 1.0 * 6.0494489669799805
Epoch 900, val loss: 0.9744582772254944
Epoch 910, training loss: 6.099713325500488 = 0.04482583701610565 + 1.0 * 6.054887294769287
Epoch 910, val loss: 0.9800227284431458
Epoch 920, training loss: 6.093272686004639 = 0.04314308986067772 + 1.0 * 6.050129413604736
Epoch 920, val loss: 0.985442578792572
Epoch 930, training loss: 6.090386867523193 = 0.04155813902616501 + 1.0 * 6.048828601837158
Epoch 930, val loss: 0.9911573529243469
Epoch 940, training loss: 6.094589710235596 = 0.040047064423561096 + 1.0 * 6.054542541503906
Epoch 940, val loss: 0.99661785364151
Epoch 950, training loss: 6.086894512176514 = 0.03862600773572922 + 1.0 * 6.0482683181762695
Epoch 950, val loss: 1.0019874572753906
Epoch 960, training loss: 6.087297439575195 = 0.0372704416513443 + 1.0 * 6.050026893615723
Epoch 960, val loss: 1.0076099634170532
Epoch 970, training loss: 6.081172943115234 = 0.03598620370030403 + 1.0 * 6.045186519622803
Epoch 970, val loss: 1.012902021408081
Epoch 980, training loss: 6.078741550445557 = 0.03476513922214508 + 1.0 * 6.043976306915283
Epoch 980, val loss: 1.0185155868530273
Epoch 990, training loss: 6.080857276916504 = 0.03360085189342499 + 1.0 * 6.0472564697265625
Epoch 990, val loss: 1.0239160060882568
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.6679
Flip ASR: 0.6178/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.314740180969238 = 1.9409904479980469 + 1.0 * 8.373749732971191
Epoch 0, val loss: 1.9305472373962402
Epoch 10, training loss: 10.302995681762695 = 1.929973840713501 + 1.0 * 8.373022079467773
Epoch 10, val loss: 1.9199074506759644
Epoch 20, training loss: 10.284125328063965 = 1.9161880016326904 + 1.0 * 8.367937088012695
Epoch 20, val loss: 1.906214952468872
Epoch 30, training loss: 10.225092887878418 = 1.897469162940979 + 1.0 * 8.32762336730957
Epoch 30, val loss: 1.8874893188476562
Epoch 40, training loss: 9.7284517288208 = 1.8772071599960327 + 1.0 * 7.851244926452637
Epoch 40, val loss: 1.8681105375289917
Epoch 50, training loss: 8.999917984008789 = 1.861130714416504 + 1.0 * 7.138787269592285
Epoch 50, val loss: 1.85382878780365
Epoch 60, training loss: 8.65109920501709 = 1.8501161336898804 + 1.0 * 6.80098295211792
Epoch 60, val loss: 1.8436243534088135
Epoch 70, training loss: 8.443405151367188 = 1.8401048183441162 + 1.0 * 6.603300094604492
Epoch 70, val loss: 1.8344063758850098
Epoch 80, training loss: 8.316041946411133 = 1.8301076889038086 + 1.0 * 6.485934257507324
Epoch 80, val loss: 1.8252421617507935
Epoch 90, training loss: 8.226051330566406 = 1.8202677965164185 + 1.0 * 6.405783176422119
Epoch 90, val loss: 1.8164780139923096
Epoch 100, training loss: 8.158856391906738 = 1.8106037378311157 + 1.0 * 6.348252773284912
Epoch 100, val loss: 1.8080668449401855
Epoch 110, training loss: 8.109367370605469 = 1.8012285232543945 + 1.0 * 6.308138847351074
Epoch 110, val loss: 1.8000924587249756
Epoch 120, training loss: 8.070576667785645 = 1.7920173406600952 + 1.0 * 6.27855920791626
Epoch 120, val loss: 1.7923550605773926
Epoch 130, training loss: 8.035116195678711 = 1.7825145721435547 + 1.0 * 6.2526021003723145
Epoch 130, val loss: 1.7844382524490356
Epoch 140, training loss: 8.004161834716797 = 1.7720775604248047 + 1.0 * 6.232083797454834
Epoch 140, val loss: 1.7758374214172363
Epoch 150, training loss: 7.976100921630859 = 1.7601765394210815 + 1.0 * 6.215924263000488
Epoch 150, val loss: 1.7661008834838867
Epoch 160, training loss: 7.947324752807617 = 1.746394395828247 + 1.0 * 6.200930118560791
Epoch 160, val loss: 1.7549670934677124
Epoch 170, training loss: 7.918193340301514 = 1.7301644086837769 + 1.0 * 6.188028812408447
Epoch 170, val loss: 1.7419915199279785
Epoch 180, training loss: 7.8875908851623535 = 1.7106434106826782 + 1.0 * 6.176947593688965
Epoch 180, val loss: 1.7265188694000244
Epoch 190, training loss: 7.8550028800964355 = 1.6871256828308105 + 1.0 * 6.167877197265625
Epoch 190, val loss: 1.7079766988754272
Epoch 200, training loss: 7.817548751831055 = 1.6588280200958252 + 1.0 * 6.158720970153809
Epoch 200, val loss: 1.6855762004852295
Epoch 210, training loss: 7.776093006134033 = 1.624822974205017 + 1.0 * 6.151269912719727
Epoch 210, val loss: 1.6586359739303589
Epoch 220, training loss: 7.7293195724487305 = 1.584346055984497 + 1.0 * 6.144973278045654
Epoch 220, val loss: 1.6264103651046753
Epoch 230, training loss: 7.6794962882995605 = 1.5373643636703491 + 1.0 * 6.142131805419922
Epoch 230, val loss: 1.5890604257583618
Epoch 240, training loss: 7.62101411819458 = 1.4854516983032227 + 1.0 * 6.135562419891357
Epoch 240, val loss: 1.5477911233901978
Epoch 250, training loss: 7.561936855316162 = 1.4296983480453491 + 1.0 * 6.132238388061523
Epoch 250, val loss: 1.5037585496902466
Epoch 260, training loss: 7.501422882080078 = 1.3724031448364258 + 1.0 * 6.129019737243652
Epoch 260, val loss: 1.4592630863189697
Epoch 270, training loss: 7.440645694732666 = 1.3153963088989258 + 1.0 * 6.12524938583374
Epoch 270, val loss: 1.4154843091964722
Epoch 280, training loss: 7.384920120239258 = 1.259292483329773 + 1.0 * 6.125627517700195
Epoch 280, val loss: 1.3731962442398071
Epoch 290, training loss: 7.324161052703857 = 1.2053803205490112 + 1.0 * 6.118780612945557
Epoch 290, val loss: 1.333033800125122
Epoch 300, training loss: 7.268726348876953 = 1.1532113552093506 + 1.0 * 6.115514755249023
Epoch 300, val loss: 1.2942358255386353
Epoch 310, training loss: 7.215631008148193 = 1.1025546789169312 + 1.0 * 6.113076210021973
Epoch 310, val loss: 1.2566860914230347
Epoch 320, training loss: 7.161900043487549 = 1.053524136543274 + 1.0 * 6.1083760261535645
Epoch 320, val loss: 1.220295786857605
Epoch 330, training loss: 7.110775470733643 = 1.0056151151657104 + 1.0 * 6.105160236358643
Epoch 330, val loss: 1.1846158504486084
Epoch 340, training loss: 7.068528175354004 = 0.9588354825973511 + 1.0 * 6.109692573547363
Epoch 340, val loss: 1.1496986150741577
Epoch 350, training loss: 7.014747142791748 = 0.9139354825019836 + 1.0 * 6.10081148147583
Epoch 350, val loss: 1.115952491760254
Epoch 360, training loss: 6.966872215270996 = 0.8699964880943298 + 1.0 * 6.0968756675720215
Epoch 360, val loss: 1.0829817056655884
Epoch 370, training loss: 6.923424243927002 = 0.8265402913093567 + 1.0 * 6.096883773803711
Epoch 370, val loss: 1.0502971410751343
Epoch 380, training loss: 6.878078460693359 = 0.7839170098304749 + 1.0 * 6.094161510467529
Epoch 380, val loss: 1.018350601196289
Epoch 390, training loss: 6.833232879638672 = 0.7422927618026733 + 1.0 * 6.090939998626709
Epoch 390, val loss: 0.9872791171073914
Epoch 400, training loss: 6.79049825668335 = 0.7014976143836975 + 1.0 * 6.089000701904297
Epoch 400, val loss: 0.9569443464279175
Epoch 410, training loss: 6.7540669441223145 = 0.6620063185691833 + 1.0 * 6.092060565948486
Epoch 410, val loss: 0.9280207753181458
Epoch 420, training loss: 6.710738182067871 = 0.6245148777961731 + 1.0 * 6.086223125457764
Epoch 420, val loss: 0.9010480046272278
Epoch 430, training loss: 6.67139196395874 = 0.588645875453949 + 1.0 * 6.0827460289001465
Epoch 430, val loss: 0.8760143518447876
Epoch 440, training loss: 6.636702537536621 = 0.5544480681419373 + 1.0 * 6.082254409790039
Epoch 440, val loss: 0.8529720306396484
Epoch 450, training loss: 6.605997085571289 = 0.5223939418792725 + 1.0 * 6.0836029052734375
Epoch 450, val loss: 0.8323020935058594
Epoch 460, training loss: 6.5702223777771 = 0.4924827814102173 + 1.0 * 6.077739715576172
Epoch 460, val loss: 0.8143697381019592
Epoch 470, training loss: 6.546127796173096 = 0.46445193886756897 + 1.0 * 6.081676006317139
Epoch 470, val loss: 0.7985954284667969
Epoch 480, training loss: 6.51334285736084 = 0.4385792315006256 + 1.0 * 6.074763774871826
Epoch 480, val loss: 0.7852950692176819
Epoch 490, training loss: 6.488061428070068 = 0.4146309494972229 + 1.0 * 6.07343053817749
Epoch 490, val loss: 0.7743774652481079
Epoch 500, training loss: 6.462961673736572 = 0.39213988184928894 + 1.0 * 6.070821762084961
Epoch 500, val loss: 0.7651580572128296
Epoch 510, training loss: 6.444997787475586 = 0.3708898723125458 + 1.0 * 6.074108123779297
Epoch 510, val loss: 0.7574873566627502
Epoch 520, training loss: 6.422296524047852 = 0.35096707940101624 + 1.0 * 6.071329593658447
Epoch 520, val loss: 0.7510061860084534
Epoch 530, training loss: 6.3996734619140625 = 0.3321232497692108 + 1.0 * 6.067550182342529
Epoch 530, val loss: 0.7457648515701294
Epoch 540, training loss: 6.379693508148193 = 0.3140694200992584 + 1.0 * 6.065624237060547
Epoch 540, val loss: 0.7412860989570618
Epoch 550, training loss: 6.367521286010742 = 0.2967321276664734 + 1.0 * 6.070789337158203
Epoch 550, val loss: 0.7374258041381836
Epoch 560, training loss: 6.34829044342041 = 0.28018346428871155 + 1.0 * 6.0681071281433105
Epoch 560, val loss: 0.7343339323997498
Epoch 570, training loss: 6.326574325561523 = 0.2643159031867981 + 1.0 * 6.062258243560791
Epoch 570, val loss: 0.7317017912864685
Epoch 580, training loss: 6.310791015625 = 0.2490111142396927 + 1.0 * 6.061779975891113
Epoch 580, val loss: 0.7294312715530396
Epoch 590, training loss: 6.294662952423096 = 0.23433691263198853 + 1.0 * 6.060326099395752
Epoch 590, val loss: 0.727446973323822
Epoch 600, training loss: 6.280149459838867 = 0.22038084268569946 + 1.0 * 6.0597686767578125
Epoch 600, val loss: 0.725941002368927
Epoch 610, training loss: 6.265434265136719 = 0.20705470442771912 + 1.0 * 6.058379650115967
Epoch 610, val loss: 0.7247568368911743
Epoch 620, training loss: 6.251733303070068 = 0.19444730877876282 + 1.0 * 6.057285785675049
Epoch 620, val loss: 0.7237176895141602
Epoch 630, training loss: 6.23745584487915 = 0.18260620534420013 + 1.0 * 6.054849624633789
Epoch 630, val loss: 0.7233401536941528
Epoch 640, training loss: 6.225287437438965 = 0.17143568396568298 + 1.0 * 6.05385160446167
Epoch 640, val loss: 0.7232360243797302
Epoch 650, training loss: 6.220839023590088 = 0.1608969122171402 + 1.0 * 6.059942245483398
Epoch 650, val loss: 0.7234121561050415
Epoch 660, training loss: 6.209626197814941 = 0.15108665823936462 + 1.0 * 6.058539390563965
Epoch 660, val loss: 0.7238442301750183
Epoch 670, training loss: 6.193698883056641 = 0.1419377475976944 + 1.0 * 6.051761150360107
Epoch 670, val loss: 0.7247833609580994
Epoch 680, training loss: 6.18356466293335 = 0.13337033987045288 + 1.0 * 6.050194263458252
Epoch 680, val loss: 0.725928008556366
Epoch 690, training loss: 6.179027080535889 = 0.1253851354122162 + 1.0 * 6.0536417961120605
Epoch 690, val loss: 0.7272682785987854
Epoch 700, training loss: 6.166897296905518 = 0.11799684166908264 + 1.0 * 6.048900604248047
Epoch 700, val loss: 0.7290534973144531
Epoch 710, training loss: 6.167139530181885 = 0.11113277822732925 + 1.0 * 6.056006908416748
Epoch 710, val loss: 0.7310610413551331
Epoch 720, training loss: 6.15179443359375 = 0.10479868203401566 + 1.0 * 6.046995639801025
Epoch 720, val loss: 0.7332726120948792
Epoch 730, training loss: 6.144371509552002 = 0.0988989993929863 + 1.0 * 6.045472621917725
Epoch 730, val loss: 0.7357398867607117
Epoch 740, training loss: 6.139001846313477 = 0.09339209645986557 + 1.0 * 6.045609951019287
Epoch 740, val loss: 0.7383580803871155
Epoch 750, training loss: 6.133132457733154 = 0.08828625828027725 + 1.0 * 6.044846057891846
Epoch 750, val loss: 0.7411046624183655
Epoch 760, training loss: 6.128293991088867 = 0.0835624411702156 + 1.0 * 6.044731616973877
Epoch 760, val loss: 0.7441503405570984
Epoch 770, training loss: 6.12175178527832 = 0.0791686475276947 + 1.0 * 6.042582988739014
Epoch 770, val loss: 0.7473600506782532
Epoch 780, training loss: 6.123244762420654 = 0.07507633417844772 + 1.0 * 6.048168659210205
Epoch 780, val loss: 0.7505843043327332
Epoch 790, training loss: 6.112875938415527 = 0.07126492261886597 + 1.0 * 6.041611194610596
Epoch 790, val loss: 0.7539671659469604
Epoch 800, training loss: 6.107514381408691 = 0.06771773099899292 + 1.0 * 6.039796829223633
Epoch 800, val loss: 0.7575161457061768
Epoch 810, training loss: 6.1129326820373535 = 0.06439679116010666 + 1.0 * 6.0485358238220215
Epoch 810, val loss: 0.7610457539558411
Epoch 820, training loss: 6.103501796722412 = 0.06131770834326744 + 1.0 * 6.042183876037598
Epoch 820, val loss: 0.7645158767700195
Epoch 830, training loss: 6.0978803634643555 = 0.05845668539404869 + 1.0 * 6.03942346572876
Epoch 830, val loss: 0.7681723237037659
Epoch 840, training loss: 6.092509746551514 = 0.055772565305233 + 1.0 * 6.036736965179443
Epoch 840, val loss: 0.7718635201454163
Epoch 850, training loss: 6.090265274047852 = 0.053252898156642914 + 1.0 * 6.037012577056885
Epoch 850, val loss: 0.7755597829818726
Epoch 860, training loss: 6.090511322021484 = 0.05089033395051956 + 1.0 * 6.039620876312256
Epoch 860, val loss: 0.7792081236839294
Epoch 870, training loss: 6.082684516906738 = 0.04868577420711517 + 1.0 * 6.033998966217041
Epoch 870, val loss: 0.7829164862632751
Epoch 880, training loss: 6.080399036407471 = 0.04661586880683899 + 1.0 * 6.033782958984375
Epoch 880, val loss: 0.7867113351821899
Epoch 890, training loss: 6.077502250671387 = 0.0446619838476181 + 1.0 * 6.032840251922607
Epoch 890, val loss: 0.790461003780365
Epoch 900, training loss: 6.093303680419922 = 0.04282364994287491 + 1.0 * 6.050479888916016
Epoch 900, val loss: 0.7941708564758301
Epoch 910, training loss: 6.07309103012085 = 0.04110688716173172 + 1.0 * 6.031984329223633
Epoch 910, val loss: 0.7978178262710571
Epoch 920, training loss: 6.071577548980713 = 0.039500270038843155 + 1.0 * 6.032077312469482
Epoch 920, val loss: 0.8016948103904724
Epoch 930, training loss: 6.068346977233887 = 0.037974875420331955 + 1.0 * 6.030372142791748
Epoch 930, val loss: 0.805424153804779
Epoch 940, training loss: 6.078482627868652 = 0.03652851656079292 + 1.0 * 6.041954040527344
Epoch 940, val loss: 0.8091298341751099
Epoch 950, training loss: 6.067514419555664 = 0.03517044335603714 + 1.0 * 6.032343864440918
Epoch 950, val loss: 0.8127551078796387
Epoch 960, training loss: 6.062467098236084 = 0.033891379833221436 + 1.0 * 6.028575897216797
Epoch 960, val loss: 0.8165537714958191
Epoch 970, training loss: 6.06165885925293 = 0.032674092799425125 + 1.0 * 6.028984546661377
Epoch 970, val loss: 0.8202353119850159
Epoch 980, training loss: 6.063073635101318 = 0.03151664510369301 + 1.0 * 6.031557083129883
Epoch 980, val loss: 0.8238458037376404
Epoch 990, training loss: 6.057430744171143 = 0.030418751761317253 + 1.0 * 6.027011871337891
Epoch 990, val loss: 0.8275137543678284
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9520
Flip ASR: 0.9422/225 nodes
The final ASR:0.77491, 0.12615, Accuracy:0.81358, 0.01552
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11646])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10582])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00627, Accuracy:0.83457, 0.00924
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.30010986328125 = 1.9263285398483276 + 1.0 * 8.373781204223633
Epoch 0, val loss: 1.9285094738006592
Epoch 10, training loss: 10.290210723876953 = 1.9170317649841309 + 1.0 * 8.37317943572998
Epoch 10, val loss: 1.9189566373825073
Epoch 20, training loss: 10.275017738342285 = 1.905631184577942 + 1.0 * 8.369386672973633
Epoch 20, val loss: 1.9071660041809082
Epoch 30, training loss: 10.228592872619629 = 1.8902417421340942 + 1.0 * 8.338351249694824
Epoch 30, val loss: 1.8913389444351196
Epoch 40, training loss: 9.878583908081055 = 1.8715139627456665 + 1.0 * 8.00706958770752
Epoch 40, val loss: 1.872685432434082
Epoch 50, training loss: 9.109048843383789 = 1.8522666692733765 + 1.0 * 7.256782054901123
Epoch 50, val loss: 1.8544096946716309
Epoch 60, training loss: 8.667570114135742 = 1.8399040699005127 + 1.0 * 6.827666282653809
Epoch 60, val loss: 1.842671275138855
Epoch 70, training loss: 8.442596435546875 = 1.8306083679199219 + 1.0 * 6.611988544464111
Epoch 70, val loss: 1.8330590724945068
Epoch 80, training loss: 8.304012298583984 = 1.8220945596694946 + 1.0 * 6.481917858123779
Epoch 80, val loss: 1.8246268033981323
Epoch 90, training loss: 8.217179298400879 = 1.8131438493728638 + 1.0 * 6.404035568237305
Epoch 90, val loss: 1.816141128540039
Epoch 100, training loss: 8.154396057128906 = 1.8039019107818604 + 1.0 * 6.350494384765625
Epoch 100, val loss: 1.807535171508789
Epoch 110, training loss: 8.10602855682373 = 1.7947146892547607 + 1.0 * 6.311313629150391
Epoch 110, val loss: 1.7989628314971924
Epoch 120, training loss: 8.06645679473877 = 1.7854408025741577 + 1.0 * 6.281015872955322
Epoch 120, val loss: 1.7903510332107544
Epoch 130, training loss: 8.032753944396973 = 1.7756783962249756 + 1.0 * 6.257075786590576
Epoch 130, val loss: 1.7815684080123901
Epoch 140, training loss: 8.000412940979004 = 1.7649812698364258 + 1.0 * 6.235431671142578
Epoch 140, val loss: 1.7722949981689453
Epoch 150, training loss: 7.969820976257324 = 1.7527600526809692 + 1.0 * 6.2170610427856445
Epoch 150, val loss: 1.7619917392730713
Epoch 160, training loss: 7.940103530883789 = 1.7383264303207397 + 1.0 * 6.20177698135376
Epoch 160, val loss: 1.7501660585403442
Epoch 170, training loss: 7.910104274749756 = 1.7209148406982422 + 1.0 * 6.189189434051514
Epoch 170, val loss: 1.7361685037612915
Epoch 180, training loss: 7.878293037414551 = 1.6997849941253662 + 1.0 * 6.1785078048706055
Epoch 180, val loss: 1.7193763256072998
Epoch 190, training loss: 7.843050479888916 = 1.673763394355774 + 1.0 * 6.169287204742432
Epoch 190, val loss: 1.6987957954406738
Epoch 200, training loss: 7.8048224449157715 = 1.6414399147033691 + 1.0 * 6.163382530212402
Epoch 200, val loss: 1.6733406782150269
Epoch 210, training loss: 7.757795333862305 = 1.6021397113800049 + 1.0 * 6.155655860900879
Epoch 210, val loss: 1.642282485961914
Epoch 220, training loss: 7.704102516174316 = 1.554805040359497 + 1.0 * 6.149297714233398
Epoch 220, val loss: 1.6046333312988281
Epoch 230, training loss: 7.642761707305908 = 1.4986478090286255 + 1.0 * 6.144114017486572
Epoch 230, val loss: 1.5596096515655518
Epoch 240, training loss: 7.574084758758545 = 1.4343119859695435 + 1.0 * 6.139772891998291
Epoch 240, val loss: 1.5077613592147827
Epoch 250, training loss: 7.500214576721191 = 1.3641180992126465 + 1.0 * 6.136096477508545
Epoch 250, val loss: 1.450789451599121
Epoch 260, training loss: 7.426191806793213 = 1.2908846139907837 + 1.0 * 6.135307312011719
Epoch 260, val loss: 1.3912373781204224
Epoch 270, training loss: 7.348724365234375 = 1.2185337543487549 + 1.0 * 6.130190849304199
Epoch 270, val loss: 1.3321878910064697
Epoch 280, training loss: 7.274044036865234 = 1.1475551128387451 + 1.0 * 6.126489162445068
Epoch 280, val loss: 1.2745578289031982
Epoch 290, training loss: 7.203937530517578 = 1.0789167881011963 + 1.0 * 6.125020503997803
Epoch 290, val loss: 1.2190996408462524
Epoch 300, training loss: 7.137606620788574 = 1.0146673917770386 + 1.0 * 6.122939109802246
Epoch 300, val loss: 1.1677370071411133
Epoch 310, training loss: 7.074039459228516 = 0.955129086971283 + 1.0 * 6.118910312652588
Epoch 310, val loss: 1.1204971075057983
Epoch 320, training loss: 7.0172224044799805 = 0.8999886512756348 + 1.0 * 6.117233753204346
Epoch 320, val loss: 1.0772044658660889
Epoch 330, training loss: 6.962031364440918 = 0.8492003679275513 + 1.0 * 6.112831115722656
Epoch 330, val loss: 1.038063645362854
Epoch 340, training loss: 6.910694599151611 = 0.8022651076316833 + 1.0 * 6.108429431915283
Epoch 340, val loss: 1.002427101135254
Epoch 350, training loss: 6.866038799285889 = 0.7589936852455139 + 1.0 * 6.1070451736450195
Epoch 350, val loss: 0.9702497720718384
Epoch 360, training loss: 6.822267532348633 = 0.7193827629089355 + 1.0 * 6.102884769439697
Epoch 360, val loss: 0.9416054487228394
Epoch 370, training loss: 6.781686782836914 = 0.6824840903282166 + 1.0 * 6.099202632904053
Epoch 370, val loss: 0.9157743453979492
Epoch 380, training loss: 6.749457836151123 = 0.6480089426040649 + 1.0 * 6.101449012756348
Epoch 380, val loss: 0.8926041126251221
Epoch 390, training loss: 6.71160888671875 = 0.6163437962532043 + 1.0 * 6.095264911651611
Epoch 390, val loss: 0.8723656535148621
Epoch 400, training loss: 6.677929401397705 = 0.5867045521736145 + 1.0 * 6.091224670410156
Epoch 400, val loss: 0.8543694019317627
Epoch 410, training loss: 6.655973434448242 = 0.5586705207824707 + 1.0 * 6.0973029136657715
Epoch 410, val loss: 0.8382566571235657
Epoch 420, training loss: 6.624471187591553 = 0.5323895215988159 + 1.0 * 6.092081546783447
Epoch 420, val loss: 0.8241853713989258
Epoch 430, training loss: 6.594543933868408 = 0.5076697468757629 + 1.0 * 6.086874008178711
Epoch 430, val loss: 0.8117765188217163
Epoch 440, training loss: 6.5675048828125 = 0.4841476380825043 + 1.0 * 6.083357334136963
Epoch 440, val loss: 0.8008053302764893
Epoch 450, training loss: 6.547790050506592 = 0.4616977274417877 + 1.0 * 6.086092472076416
Epoch 450, val loss: 0.7911009788513184
Epoch 460, training loss: 6.522242546081543 = 0.44040432572364807 + 1.0 * 6.081838130950928
Epoch 460, val loss: 0.7826697826385498
Epoch 470, training loss: 6.504397392272949 = 0.42016586661338806 + 1.0 * 6.084231376647949
Epoch 470, val loss: 0.7753351926803589
Epoch 480, training loss: 6.480245113372803 = 0.4008879065513611 + 1.0 * 6.079357147216797
Epoch 480, val loss: 0.7690020799636841
Epoch 490, training loss: 6.457862854003906 = 0.3824293911457062 + 1.0 * 6.075433254241943
Epoch 490, val loss: 0.7635777592658997
Epoch 500, training loss: 6.439009189605713 = 0.36464163661003113 + 1.0 * 6.074367523193359
Epoch 500, val loss: 0.7589585185050964
Epoch 510, training loss: 6.420968532562256 = 0.34750378131866455 + 1.0 * 6.073464870452881
Epoch 510, val loss: 0.7551231384277344
Epoch 520, training loss: 6.4017133712768555 = 0.3311023414134979 + 1.0 * 6.070611000061035
Epoch 520, val loss: 0.7520816326141357
Epoch 530, training loss: 6.384696960449219 = 0.3153705894947052 + 1.0 * 6.069326400756836
Epoch 530, val loss: 0.7497977614402771
Epoch 540, training loss: 6.36827278137207 = 0.300198495388031 + 1.0 * 6.0680742263793945
Epoch 540, val loss: 0.7482512593269348
Epoch 550, training loss: 6.367212772369385 = 0.2856321930885315 + 1.0 * 6.081580638885498
Epoch 550, val loss: 0.7474179863929749
Epoch 560, training loss: 6.341243267059326 = 0.2718084156513214 + 1.0 * 6.069434642791748
Epoch 560, val loss: 0.747353732585907
Epoch 570, training loss: 6.332942485809326 = 0.25861936807632446 + 1.0 * 6.0743231773376465
Epoch 570, val loss: 0.7479273080825806
Epoch 580, training loss: 6.3094587326049805 = 0.2460799217224121 + 1.0 * 6.063378810882568
Epoch 580, val loss: 0.7491549253463745
Epoch 590, training loss: 6.295913219451904 = 0.2341117262840271 + 1.0 * 6.061801433563232
Epoch 590, val loss: 0.7510334253311157
Epoch 600, training loss: 6.283901691436768 = 0.222676619887352 + 1.0 * 6.061224937438965
Epoch 600, val loss: 0.753494143486023
Epoch 610, training loss: 6.273489475250244 = 0.2118460088968277 + 1.0 * 6.061643600463867
Epoch 610, val loss: 0.7564165592193604
Epoch 620, training loss: 6.261221885681152 = 0.2016439288854599 + 1.0 * 6.059577941894531
Epoch 620, val loss: 0.759856104850769
Epoch 630, training loss: 6.250483512878418 = 0.1919393390417099 + 1.0 * 6.058544158935547
Epoch 630, val loss: 0.7636942267417908
Epoch 640, training loss: 6.2448859214782715 = 0.18270598351955414 + 1.0 * 6.062180042266846
Epoch 640, val loss: 0.7679117321968079
Epoch 650, training loss: 6.23540735244751 = 0.174013152718544 + 1.0 * 6.061394214630127
Epoch 650, val loss: 0.7723125219345093
Epoch 660, training loss: 6.223346710205078 = 0.1658097207546234 + 1.0 * 6.057537078857422
Epoch 660, val loss: 0.777125895023346
Epoch 670, training loss: 6.213443279266357 = 0.15802796185016632 + 1.0 * 6.055415153503418
Epoch 670, val loss: 0.7821155786514282
Epoch 680, training loss: 6.205977439880371 = 0.15062646567821503 + 1.0 * 6.0553507804870605
Epoch 680, val loss: 0.7874135375022888
Epoch 690, training loss: 6.201234340667725 = 0.14361774921417236 + 1.0 * 6.057616710662842
Epoch 690, val loss: 0.7928124666213989
Epoch 700, training loss: 6.189435005187988 = 0.13701337575912476 + 1.0 * 6.052421569824219
Epoch 700, val loss: 0.7984934449195862
Epoch 710, training loss: 6.181177139282227 = 0.13074803352355957 + 1.0 * 6.050428867340088
Epoch 710, val loss: 0.8043228387832642
Epoch 720, training loss: 6.177575588226318 = 0.12480278313159943 + 1.0 * 6.0527729988098145
Epoch 720, val loss: 0.8103203773498535
Epoch 730, training loss: 6.171202182769775 = 0.11919182538986206 + 1.0 * 6.052010536193848
Epoch 730, val loss: 0.8163083791732788
Epoch 740, training loss: 6.163550853729248 = 0.11389344930648804 + 1.0 * 6.049657344818115
Epoch 740, val loss: 0.8225787281990051
Epoch 750, training loss: 6.155387878417969 = 0.10886919498443604 + 1.0 * 6.046518802642822
Epoch 750, val loss: 0.8289280533790588
Epoch 760, training loss: 6.1510186195373535 = 0.10409089922904968 + 1.0 * 6.0469279289245605
Epoch 760, val loss: 0.8353952765464783
Epoch 770, training loss: 6.150505065917969 = 0.09958530217409134 + 1.0 * 6.050919532775879
Epoch 770, val loss: 0.841754138469696
Epoch 780, training loss: 6.142033100128174 = 0.09540119022130966 + 1.0 * 6.046631813049316
Epoch 780, val loss: 0.8483158946037292
Epoch 790, training loss: 6.136059761047363 = 0.09144072979688644 + 1.0 * 6.044619083404541
Epoch 790, val loss: 0.8549879789352417
Epoch 800, training loss: 6.130902290344238 = 0.08767340332269669 + 1.0 * 6.043229103088379
Epoch 800, val loss: 0.8616890907287598
Epoch 810, training loss: 6.1262898445129395 = 0.08409464359283447 + 1.0 * 6.0421953201293945
Epoch 810, val loss: 0.8685288429260254
Epoch 820, training loss: 6.135307312011719 = 0.08070056140422821 + 1.0 * 6.054606914520264
Epoch 820, val loss: 0.875281572341919
Epoch 830, training loss: 6.120627403259277 = 0.07751350849866867 + 1.0 * 6.043113708496094
Epoch 830, val loss: 0.8821561932563782
Epoch 840, training loss: 6.11458158493042 = 0.0744870975613594 + 1.0 * 6.040094375610352
Epoch 840, val loss: 0.8891468048095703
Epoch 850, training loss: 6.11212682723999 = 0.07159724086523056 + 1.0 * 6.040529727935791
Epoch 850, val loss: 0.8961278200149536
Epoch 860, training loss: 6.110114574432373 = 0.06884964555501938 + 1.0 * 6.04126501083374
Epoch 860, val loss: 0.9029920697212219
Epoch 870, training loss: 6.104462623596191 = 0.06624284386634827 + 1.0 * 6.038219928741455
Epoch 870, val loss: 0.9099072217941284
Epoch 880, training loss: 6.106011867523193 = 0.06375493109226227 + 1.0 * 6.042256832122803
Epoch 880, val loss: 0.9167290925979614
Epoch 890, training loss: 6.098616600036621 = 0.06138255074620247 + 1.0 * 6.037233829498291
Epoch 890, val loss: 0.9235213398933411
Epoch 900, training loss: 6.097269535064697 = 0.05911876633763313 + 1.0 * 6.038150787353516
Epoch 900, val loss: 0.9303451180458069
Epoch 910, training loss: 6.09498405456543 = 0.056953705847263336 + 1.0 * 6.03803014755249
Epoch 910, val loss: 0.9368850588798523
Epoch 920, training loss: 6.090493202209473 = 0.05489618331193924 + 1.0 * 6.03559684753418
Epoch 920, val loss: 0.9435426592826843
Epoch 930, training loss: 6.08743143081665 = 0.05292234197258949 + 1.0 * 6.034509181976318
Epoch 930, val loss: 0.9500904083251953
Epoch 940, training loss: 6.0846147537231445 = 0.05102419853210449 + 1.0 * 6.033590316772461
Epoch 940, val loss: 0.9566761255264282
Epoch 950, training loss: 6.090843677520752 = 0.049203209578990936 + 1.0 * 6.041640281677246
Epoch 950, val loss: 0.9631226658821106
Epoch 960, training loss: 6.085756301879883 = 0.047470591962337494 + 1.0 * 6.038285732269287
Epoch 960, val loss: 0.9694193601608276
Epoch 970, training loss: 6.078979969024658 = 0.045813411474227905 + 1.0 * 6.033166408538818
Epoch 970, val loss: 0.9757887125015259
Epoch 980, training loss: 6.078106880187988 = 0.0442289337515831 + 1.0 * 6.033877849578857
Epoch 980, val loss: 0.981984555721283
Epoch 990, training loss: 6.073624610900879 = 0.04271699860692024 + 1.0 * 6.03090763092041
Epoch 990, val loss: 0.9882750511169434
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5867
Flip ASR: 0.5244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.336725234985352 = 1.9629838466644287 + 1.0 * 8.373741149902344
Epoch 0, val loss: 1.9619672298431396
Epoch 10, training loss: 10.32510757446289 = 1.9521902799606323 + 1.0 * 8.372917175292969
Epoch 10, val loss: 1.9510324001312256
Epoch 20, training loss: 10.306039810180664 = 1.93867826461792 + 1.0 * 8.367362022399902
Epoch 20, val loss: 1.9367895126342773
Epoch 30, training loss: 10.251975059509277 = 1.920479655265808 + 1.0 * 8.33149528503418
Epoch 30, val loss: 1.9172616004943848
Epoch 40, training loss: 9.982192039489746 = 1.900247573852539 + 1.0 * 8.081944465637207
Epoch 40, val loss: 1.8964197635650635
Epoch 50, training loss: 9.143938064575195 = 1.882386326789856 + 1.0 * 7.261551856994629
Epoch 50, val loss: 1.8784154653549194
Epoch 60, training loss: 8.67134952545166 = 1.8686223030090332 + 1.0 * 6.802727222442627
Epoch 60, val loss: 1.8647438287734985
Epoch 70, training loss: 8.484752655029297 = 1.8562211990356445 + 1.0 * 6.6285319328308105
Epoch 70, val loss: 1.8524941205978394
Epoch 80, training loss: 8.37417221069336 = 1.842379093170166 + 1.0 * 6.531792640686035
Epoch 80, val loss: 1.8384333848953247
Epoch 90, training loss: 8.273560523986816 = 1.8283026218414307 + 1.0 * 6.445258140563965
Epoch 90, val loss: 1.8244154453277588
Epoch 100, training loss: 8.200101852416992 = 1.8147504329681396 + 1.0 * 6.385351181030273
Epoch 100, val loss: 1.8106662034988403
Epoch 110, training loss: 8.141128540039062 = 1.8011481761932373 + 1.0 * 6.339980125427246
Epoch 110, val loss: 1.7970417737960815
Epoch 120, training loss: 8.092731475830078 = 1.7875665426254272 + 1.0 * 6.3051652908325195
Epoch 120, val loss: 1.783656358718872
Epoch 130, training loss: 8.051839828491211 = 1.7741823196411133 + 1.0 * 6.277657508850098
Epoch 130, val loss: 1.7708189487457275
Epoch 140, training loss: 8.013910293579102 = 1.7603068351745605 + 1.0 * 6.253603935241699
Epoch 140, val loss: 1.7580156326293945
Epoch 150, training loss: 7.978538990020752 = 1.744953989982605 + 1.0 * 6.233584880828857
Epoch 150, val loss: 1.7440979480743408
Epoch 160, training loss: 7.945834159851074 = 1.7269952297210693 + 1.0 * 6.218839168548584
Epoch 160, val loss: 1.7284021377563477
Epoch 170, training loss: 7.911279201507568 = 1.705901026725769 + 1.0 * 6.20537805557251
Epoch 170, val loss: 1.7103570699691772
Epoch 180, training loss: 7.874607086181641 = 1.6808269023895264 + 1.0 * 6.193780422210693
Epoch 180, val loss: 1.6893179416656494
Epoch 190, training loss: 7.834498405456543 = 1.6506621837615967 + 1.0 * 6.183835983276367
Epoch 190, val loss: 1.664293646812439
Epoch 200, training loss: 7.789838790893555 = 1.614190697669983 + 1.0 * 6.175648212432861
Epoch 200, val loss: 1.6342601776123047
Epoch 210, training loss: 7.741161346435547 = 1.5710711479187012 + 1.0 * 6.170090198516846
Epoch 210, val loss: 1.5989768505096436
Epoch 220, training loss: 7.683955192565918 = 1.5215463638305664 + 1.0 * 6.162408828735352
Epoch 220, val loss: 1.558853030204773
Epoch 230, training loss: 7.62304162979126 = 1.465872883796692 + 1.0 * 6.157168865203857
Epoch 230, val loss: 1.5142016410827637
Epoch 240, training loss: 7.5584235191345215 = 1.40496027469635 + 1.0 * 6.153463363647461
Epoch 240, val loss: 1.465620517730713
Epoch 250, training loss: 7.492359638214111 = 1.3409227132797241 + 1.0 * 6.151436805725098
Epoch 250, val loss: 1.4149508476257324
Epoch 260, training loss: 7.421787261962891 = 1.2763807773590088 + 1.0 * 6.145406723022461
Epoch 260, val loss: 1.3649307489395142
Epoch 270, training loss: 7.35321044921875 = 1.2130032777786255 + 1.0 * 6.140207290649414
Epoch 270, val loss: 1.315659523010254
Epoch 280, training loss: 7.286187171936035 = 1.151078462600708 + 1.0 * 6.135108470916748
Epoch 280, val loss: 1.2675977945327759
Epoch 290, training loss: 7.235507488250732 = 1.0917364358901978 + 1.0 * 6.143771171569824
Epoch 290, val loss: 1.2217940092086792
Epoch 300, training loss: 7.167333602905273 = 1.0374956130981445 + 1.0 * 6.129837989807129
Epoch 300, val loss: 1.1796280145645142
Epoch 310, training loss: 7.110447883605957 = 0.9870831370353699 + 1.0 * 6.1233649253845215
Epoch 310, val loss: 1.1408131122589111
Epoch 320, training loss: 7.069676399230957 = 0.9396358728408813 + 1.0 * 6.130040645599365
Epoch 320, val loss: 1.1047019958496094
Epoch 330, training loss: 7.014053821563721 = 0.8957187533378601 + 1.0 * 6.118335247039795
Epoch 330, val loss: 1.0714633464813232
Epoch 340, training loss: 6.966840744018555 = 0.8542772531509399 + 1.0 * 6.112563610076904
Epoch 340, val loss: 1.040569543838501
Epoch 350, training loss: 6.938618183135986 = 0.8149285912513733 + 1.0 * 6.123689651489258
Epoch 350, val loss: 1.0115957260131836
Epoch 360, training loss: 6.886761665344238 = 0.7782522439956665 + 1.0 * 6.108509540557861
Epoch 360, val loss: 0.9851551055908203
Epoch 370, training loss: 6.848781585693359 = 0.7438464164733887 + 1.0 * 6.104935169219971
Epoch 370, val loss: 0.9607663750648499
Epoch 380, training loss: 6.813390731811523 = 0.711377739906311 + 1.0 * 6.102013111114502
Epoch 380, val loss: 0.9379341006278992
Epoch 390, training loss: 6.78459358215332 = 0.680873453617096 + 1.0 * 6.103720188140869
Epoch 390, val loss: 0.9169339537620544
Epoch 400, training loss: 6.74906063079834 = 0.6524921655654907 + 1.0 * 6.096568584442139
Epoch 400, val loss: 0.8979639410972595
Epoch 410, training loss: 6.719400405883789 = 0.6255214214324951 + 1.0 * 6.093878746032715
Epoch 410, val loss: 0.8806257247924805
Epoch 420, training loss: 6.699045658111572 = 0.5996569991111755 + 1.0 * 6.099388599395752
Epoch 420, val loss: 0.8645049333572388
Epoch 430, training loss: 6.664558410644531 = 0.5749562382698059 + 1.0 * 6.089601993560791
Epoch 430, val loss: 0.8495368361473083
Epoch 440, training loss: 6.645519256591797 = 0.5509885549545288 + 1.0 * 6.0945305824279785
Epoch 440, val loss: 0.8355295658111572
Epoch 450, training loss: 6.613298416137695 = 0.5277221202850342 + 1.0 * 6.085576057434082
Epoch 450, val loss: 0.822270929813385
Epoch 460, training loss: 6.5921220779418945 = 0.5049870610237122 + 1.0 * 6.087134838104248
Epoch 460, val loss: 0.8097615242004395
Epoch 470, training loss: 6.5657734870910645 = 0.48272502422332764 + 1.0 * 6.083048343658447
Epoch 470, val loss: 0.7978877425193787
Epoch 480, training loss: 6.550912857055664 = 0.4609455466270447 + 1.0 * 6.089967250823975
Epoch 480, val loss: 0.7866203784942627
Epoch 490, training loss: 6.5181193351745605 = 0.4396890103816986 + 1.0 * 6.07843017578125
Epoch 490, val loss: 0.7760263085365295
Epoch 500, training loss: 6.49600076675415 = 0.4190046191215515 + 1.0 * 6.076996326446533
Epoch 500, val loss: 0.7660378813743591
Epoch 510, training loss: 6.475546360015869 = 0.3988809287548065 + 1.0 * 6.07666540145874
Epoch 510, val loss: 0.7565812468528748
Epoch 520, training loss: 6.464951038360596 = 0.37949705123901367 + 1.0 * 6.085453987121582
Epoch 520, val loss: 0.7478381991386414
Epoch 530, training loss: 6.436746597290039 = 0.361150324344635 + 1.0 * 6.075596332550049
Epoch 530, val loss: 0.7398704290390015
Epoch 540, training loss: 6.414870262145996 = 0.34363624453544617 + 1.0 * 6.071234226226807
Epoch 540, val loss: 0.7326750755310059
Epoch 550, training loss: 6.397244453430176 = 0.32686373591423035 + 1.0 * 6.070380687713623
Epoch 550, val loss: 0.7261967062950134
Epoch 560, training loss: 6.380265235900879 = 0.31097957491874695 + 1.0 * 6.069285869598389
Epoch 560, val loss: 0.7205005288124084
Epoch 570, training loss: 6.3659443855285645 = 0.29604578018188477 + 1.0 * 6.06989860534668
Epoch 570, val loss: 0.7156168222427368
Epoch 580, training loss: 6.3495330810546875 = 0.28196343779563904 + 1.0 * 6.067569732666016
Epoch 580, val loss: 0.7115337252616882
Epoch 590, training loss: 6.333718776702881 = 0.26861676573753357 + 1.0 * 6.0651021003723145
Epoch 590, val loss: 0.7082735300064087
Epoch 600, training loss: 6.325835704803467 = 0.2559344470500946 + 1.0 * 6.069901466369629
Epoch 600, val loss: 0.7056519985198975
Epoch 610, training loss: 6.310730934143066 = 0.2439928948879242 + 1.0 * 6.066738128662109
Epoch 610, val loss: 0.7037784457206726
Epoch 620, training loss: 6.2968034744262695 = 0.2327442765235901 + 1.0 * 6.064059257507324
Epoch 620, val loss: 0.702518105506897
Epoch 630, training loss: 6.282321453094482 = 0.22210995852947235 + 1.0 * 6.060211658477783
Epoch 630, val loss: 0.7018318176269531
Epoch 640, training loss: 6.273936748504639 = 0.21200279891490936 + 1.0 * 6.061933994293213
Epoch 640, val loss: 0.7016420364379883
Epoch 650, training loss: 6.2644476890563965 = 0.20245923101902008 + 1.0 * 6.061988353729248
Epoch 650, val loss: 0.7018897533416748
Epoch 660, training loss: 6.250665664672852 = 0.19345270097255707 + 1.0 * 6.057212829589844
Epoch 660, val loss: 0.7025858163833618
Epoch 670, training loss: 6.241567134857178 = 0.18489350378513336 + 1.0 * 6.056673526763916
Epoch 670, val loss: 0.7037535905838013
Epoch 680, training loss: 6.2352519035339355 = 0.17675282061100006 + 1.0 * 6.058498859405518
Epoch 680, val loss: 0.7053786516189575
Epoch 690, training loss: 6.225253105163574 = 0.16900508105754852 + 1.0 * 6.056248188018799
Epoch 690, val loss: 0.7073547840118408
Epoch 700, training loss: 6.223006248474121 = 0.16167901456356049 + 1.0 * 6.0613274574279785
Epoch 700, val loss: 0.7097164392471313
Epoch 710, training loss: 6.207668304443359 = 0.15475845336914062 + 1.0 * 6.052909851074219
Epoch 710, val loss: 0.7123785614967346
Epoch 720, training loss: 6.199576377868652 = 0.14818075299263 + 1.0 * 6.051395416259766
Epoch 720, val loss: 0.7154160141944885
Epoch 730, training loss: 6.194942474365234 = 0.14192937314510345 + 1.0 * 6.053013324737549
Epoch 730, val loss: 0.7188372015953064
Epoch 740, training loss: 6.191827297210693 = 0.13601075112819672 + 1.0 * 6.055816650390625
Epoch 740, val loss: 0.7225438952445984
Epoch 750, training loss: 6.181117057800293 = 0.13043081760406494 + 1.0 * 6.050686359405518
Epoch 750, val loss: 0.7265226244926453
Epoch 760, training loss: 6.172395706176758 = 0.12513470649719238 + 1.0 * 6.047260761260986
Epoch 760, val loss: 0.7308216691017151
Epoch 770, training loss: 6.169965744018555 = 0.12005317956209183 + 1.0 * 6.049912452697754
Epoch 770, val loss: 0.7353610992431641
Epoch 780, training loss: 6.162264823913574 = 0.1152254194021225 + 1.0 * 6.04703950881958
Epoch 780, val loss: 0.740078866481781
Epoch 790, training loss: 6.16089391708374 = 0.11061005294322968 + 1.0 * 6.050283908843994
Epoch 790, val loss: 0.7450103759765625
Epoch 800, training loss: 6.151276588439941 = 0.10620265454053879 + 1.0 * 6.045073986053467
Epoch 800, val loss: 0.7501258850097656
Epoch 810, training loss: 6.145679473876953 = 0.10195519775152206 + 1.0 * 6.043724060058594
Epoch 810, val loss: 0.7554512023925781
Epoch 820, training loss: 6.1418776512146 = 0.09785590320825577 + 1.0 * 6.0440216064453125
Epoch 820, val loss: 0.7609992623329163
Epoch 830, training loss: 6.141326427459717 = 0.09393294900655746 + 1.0 * 6.047393321990967
Epoch 830, val loss: 0.7667357921600342
Epoch 840, training loss: 6.134743690490723 = 0.09019873291254044 + 1.0 * 6.0445451736450195
Epoch 840, val loss: 0.7725401520729065
Epoch 850, training loss: 6.127561092376709 = 0.08660708367824554 + 1.0 * 6.040954113006592
Epoch 850, val loss: 0.7785226702690125
Epoch 860, training loss: 6.122683048248291 = 0.08313846588134766 + 1.0 * 6.039544582366943
Epoch 860, val loss: 0.7847141027450562
Epoch 870, training loss: 6.122340679168701 = 0.0797819271683693 + 1.0 * 6.042558670043945
Epoch 870, val loss: 0.7911000847816467
Epoch 880, training loss: 6.120360851287842 = 0.07651787251234055 + 1.0 * 6.043842792510986
Epoch 880, val loss: 0.797499418258667
Epoch 890, training loss: 6.112154483795166 = 0.07336415350437164 + 1.0 * 6.038790225982666
Epoch 890, val loss: 0.8039121627807617
Epoch 900, training loss: 6.10833740234375 = 0.07029835134744644 + 1.0 * 6.038039207458496
Epoch 900, val loss: 0.8104875087738037
Epoch 910, training loss: 6.106571674346924 = 0.06731735169887543 + 1.0 * 6.039254188537598
Epoch 910, val loss: 0.8172147274017334
Epoch 920, training loss: 6.100346565246582 = 0.0644536241889 + 1.0 * 6.035892963409424
Epoch 920, val loss: 0.8239741325378418
Epoch 930, training loss: 6.096992015838623 = 0.06169187277555466 + 1.0 * 6.035300254821777
Epoch 930, val loss: 0.8307883143424988
Epoch 940, training loss: 6.094025135040283 = 0.05904461070895195 + 1.0 * 6.034980297088623
Epoch 940, val loss: 0.8377487659454346
Epoch 950, training loss: 6.096768379211426 = 0.056539587676525116 + 1.0 * 6.040228843688965
Epoch 950, val loss: 0.8447942733764648
Epoch 960, training loss: 6.092329025268555 = 0.05421130359172821 + 1.0 * 6.0381178855896
Epoch 960, val loss: 0.8518284559249878
Epoch 970, training loss: 6.0854597091674805 = 0.05204014480113983 + 1.0 * 6.033419609069824
Epoch 970, val loss: 0.858832061290741
Epoch 980, training loss: 6.082404613494873 = 0.0499933660030365 + 1.0 * 6.032411098480225
Epoch 980, val loss: 0.8658910393714905
Epoch 990, training loss: 6.080832481384277 = 0.04806319251656532 + 1.0 * 6.032769203186035
Epoch 990, val loss: 0.8729773163795471
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5683
Flip ASR: 0.5156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.318915367126465 = 1.9451340436935425 + 1.0 * 8.373781204223633
Epoch 0, val loss: 1.9369299411773682
Epoch 10, training loss: 10.308287620544434 = 1.9349876642227173 + 1.0 * 8.373299598693848
Epoch 10, val loss: 1.927245855331421
Epoch 20, training loss: 10.293570518493652 = 1.9230178594589233 + 1.0 * 8.370553016662598
Epoch 20, val loss: 1.9153591394424438
Epoch 30, training loss: 10.259350776672363 = 1.9070066213607788 + 1.0 * 8.352344512939453
Epoch 30, val loss: 1.8990815877914429
Epoch 40, training loss: 10.106173515319824 = 1.8868638277053833 + 1.0 * 8.21930980682373
Epoch 40, val loss: 1.8789403438568115
Epoch 50, training loss: 9.399969100952148 = 1.8665833473205566 + 1.0 * 7.533385276794434
Epoch 50, val loss: 1.859030842781067
Epoch 60, training loss: 8.911168098449707 = 1.848070502281189 + 1.0 * 7.0630974769592285
Epoch 60, val loss: 1.84171724319458
Epoch 70, training loss: 8.61483383178711 = 1.834017276763916 + 1.0 * 6.780817031860352
Epoch 70, val loss: 1.8286521434783936
Epoch 80, training loss: 8.467936515808105 = 1.8200961351394653 + 1.0 * 6.64784049987793
Epoch 80, val loss: 1.815259575843811
Epoch 90, training loss: 8.368144035339355 = 1.8064532279968262 + 1.0 * 6.561690807342529
Epoch 90, val loss: 1.8025736808776855
Epoch 100, training loss: 8.280082702636719 = 1.7933664321899414 + 1.0 * 6.4867167472839355
Epoch 100, val loss: 1.790803074836731
Epoch 110, training loss: 8.201647758483887 = 1.7809451818466187 + 1.0 * 6.420702934265137
Epoch 110, val loss: 1.780388593673706
Epoch 120, training loss: 8.136434555053711 = 1.76824951171875 + 1.0 * 6.368185520172119
Epoch 120, val loss: 1.7697521448135376
Epoch 130, training loss: 8.084185600280762 = 1.7542868852615356 + 1.0 * 6.329898834228516
Epoch 130, val loss: 1.7577191591262817
Epoch 140, training loss: 8.034256935119629 = 1.7381929159164429 + 1.0 * 6.296064376831055
Epoch 140, val loss: 1.7439346313476562
Epoch 150, training loss: 7.988945484161377 = 1.719232439994812 + 1.0 * 6.269712924957275
Epoch 150, val loss: 1.7282154560089111
Epoch 160, training loss: 7.94568395614624 = 1.696245789527893 + 1.0 * 6.249438285827637
Epoch 160, val loss: 1.7095800638198853
Epoch 170, training loss: 7.901283264160156 = 1.6682965755462646 + 1.0 * 6.232986927032471
Epoch 170, val loss: 1.687157392501831
Epoch 180, training loss: 7.85404109954834 = 1.6341819763183594 + 1.0 * 6.2198591232299805
Epoch 180, val loss: 1.659864902496338
Epoch 190, training loss: 7.801090240478516 = 1.5923792123794556 + 1.0 * 6.20871114730835
Epoch 190, val loss: 1.6265852451324463
Epoch 200, training loss: 7.7415971755981445 = 1.5419822931289673 + 1.0 * 6.199615001678467
Epoch 200, val loss: 1.5866972208023071
Epoch 210, training loss: 7.6746826171875 = 1.4833866357803345 + 1.0 * 6.191296100616455
Epoch 210, val loss: 1.540404200553894
Epoch 220, training loss: 7.60182523727417 = 1.4168630838394165 + 1.0 * 6.184962272644043
Epoch 220, val loss: 1.4880294799804688
Epoch 230, training loss: 7.52301549911499 = 1.3447613716125488 + 1.0 * 6.178254127502441
Epoch 230, val loss: 1.4312784671783447
Epoch 240, training loss: 7.443934440612793 = 1.2697113752365112 + 1.0 * 6.174222946166992
Epoch 240, val loss: 1.3724398612976074
Epoch 250, training loss: 7.36305046081543 = 1.195907473564148 + 1.0 * 6.167142868041992
Epoch 250, val loss: 1.3147739171981812
Epoch 260, training loss: 7.28831672668457 = 1.12466561794281 + 1.0 * 6.163650989532471
Epoch 260, val loss: 1.2593331336975098
Epoch 270, training loss: 7.215351581573486 = 1.0584174394607544 + 1.0 * 6.1569342613220215
Epoch 270, val loss: 1.208019495010376
Epoch 280, training loss: 7.148987770080566 = 0.9972988963127136 + 1.0 * 6.151689052581787
Epoch 280, val loss: 1.1609293222427368
Epoch 290, training loss: 7.087836742401123 = 0.940710186958313 + 1.0 * 6.1471266746521
Epoch 290, val loss: 1.1178021430969238
Epoch 300, training loss: 7.035251617431641 = 0.8888957500457764 + 1.0 * 6.146356105804443
Epoch 300, val loss: 1.0789481401443481
Epoch 310, training loss: 6.978785514831543 = 0.8419422507286072 + 1.0 * 6.136843204498291
Epoch 310, val loss: 1.044294834136963
Epoch 320, training loss: 6.931491851806641 = 0.7984703779220581 + 1.0 * 6.133021354675293
Epoch 320, val loss: 1.0128310918807983
Epoch 330, training loss: 6.895717144012451 = 0.7578638195991516 + 1.0 * 6.137853145599365
Epoch 330, val loss: 0.9839096665382385
Epoch 340, training loss: 6.8483567237854 = 0.720607578754425 + 1.0 * 6.127748966217041
Epoch 340, val loss: 0.9579611420631409
Epoch 350, training loss: 6.813692092895508 = 0.6858737468719482 + 1.0 * 6.1278181076049805
Epoch 350, val loss: 0.9344411492347717
Epoch 360, training loss: 6.771544456481934 = 0.6534215807914734 + 1.0 * 6.1181230545043945
Epoch 360, val loss: 0.9129272103309631
Epoch 370, training loss: 6.737730503082275 = 0.6226779222488403 + 1.0 * 6.115052700042725
Epoch 370, val loss: 0.893079936504364
Epoch 380, training loss: 6.706341743469238 = 0.5933206081390381 + 1.0 * 6.113020896911621
Epoch 380, val loss: 0.8745623230934143
Epoch 390, training loss: 6.674544334411621 = 0.5655096173286438 + 1.0 * 6.109034538269043
Epoch 390, val loss: 0.8575960993766785
Epoch 400, training loss: 6.647214412689209 = 0.5391219854354858 + 1.0 * 6.108092308044434
Epoch 400, val loss: 0.8421661853790283
Epoch 410, training loss: 6.616846084594727 = 0.5135657787322998 + 1.0 * 6.103280067443848
Epoch 410, val loss: 0.8278110027313232
Epoch 420, training loss: 6.5907206535339355 = 0.48879435658454895 + 1.0 * 6.101926326751709
Epoch 420, val loss: 0.8143973350524902
Epoch 430, training loss: 6.564453125 = 0.46484845876693726 + 1.0 * 6.099604606628418
Epoch 430, val loss: 0.8021070957183838
Epoch 440, training loss: 6.544622898101807 = 0.44142642617225647 + 1.0 * 6.103196620941162
Epoch 440, val loss: 0.7907411456108093
Epoch 450, training loss: 6.51623010635376 = 0.4187285006046295 + 1.0 * 6.097501754760742
Epoch 450, val loss: 0.7803177833557129
Epoch 460, training loss: 6.4897990226745605 = 0.3964889347553253 + 1.0 * 6.0933098793029785
Epoch 460, val loss: 0.7709748148918152
Epoch 470, training loss: 6.464986801147461 = 0.37457817792892456 + 1.0 * 6.090408802032471
Epoch 470, val loss: 0.7624332904815674
Epoch 480, training loss: 6.460498332977295 = 0.35304781794548035 + 1.0 * 6.107450485229492
Epoch 480, val loss: 0.7547004222869873
Epoch 490, training loss: 6.423810958862305 = 0.3323824107646942 + 1.0 * 6.091428756713867
Epoch 490, val loss: 0.7479991912841797
Epoch 500, training loss: 6.39866304397583 = 0.31236210465431213 + 1.0 * 6.086300849914551
Epoch 500, val loss: 0.7421939373016357
Epoch 510, training loss: 6.377410411834717 = 0.2929881513118744 + 1.0 * 6.0844221115112305
Epoch 510, val loss: 0.7369933724403381
Epoch 520, training loss: 6.362663269042969 = 0.2744210362434387 + 1.0 * 6.088242053985596
Epoch 520, val loss: 0.7324448823928833
Epoch 530, training loss: 6.341014862060547 = 0.25689658522605896 + 1.0 * 6.084118366241455
Epoch 530, val loss: 0.7286012172698975
Epoch 540, training loss: 6.320613384246826 = 0.24025508761405945 + 1.0 * 6.080358505249023
Epoch 540, val loss: 0.7253966331481934
Epoch 550, training loss: 6.312232494354248 = 0.22450348734855652 + 1.0 * 6.087728977203369
Epoch 550, val loss: 0.7226786017417908
Epoch 560, training loss: 6.288622856140137 = 0.2098933905363083 + 1.0 * 6.078729629516602
Epoch 560, val loss: 0.7205960154533386
Epoch 570, training loss: 6.273127555847168 = 0.19627712666988373 + 1.0 * 6.076850414276123
Epoch 570, val loss: 0.7191588282585144
Epoch 580, training loss: 6.258045673370361 = 0.18350805342197418 + 1.0 * 6.074537754058838
Epoch 580, val loss: 0.7182539701461792
Epoch 590, training loss: 6.252400875091553 = 0.17156469821929932 + 1.0 * 6.080836296081543
Epoch 590, val loss: 0.717822790145874
Epoch 600, training loss: 6.2379374504089355 = 0.16050255298614502 + 1.0 * 6.07743501663208
Epoch 600, val loss: 0.7178168892860413
Epoch 610, training loss: 6.221754550933838 = 0.15026912093162537 + 1.0 * 6.07148551940918
Epoch 610, val loss: 0.7183696627616882
Epoch 620, training loss: 6.210225582122803 = 0.1407451033592224 + 1.0 * 6.0694804191589355
Epoch 620, val loss: 0.7193434834480286
Epoch 630, training loss: 6.204134941101074 = 0.13189592957496643 + 1.0 * 6.072238922119141
Epoch 630, val loss: 0.7206742763519287
Epoch 640, training loss: 6.193030834197998 = 0.12373660504817963 + 1.0 * 6.069294452667236
Epoch 640, val loss: 0.7224079370498657
Epoch 650, training loss: 6.182607650756836 = 0.11618536710739136 + 1.0 * 6.066422462463379
Epoch 650, val loss: 0.7245959043502808
Epoch 660, training loss: 6.178200721740723 = 0.10918276011943817 + 1.0 * 6.0690178871154785
Epoch 660, val loss: 0.7271381616592407
Epoch 670, training loss: 6.1695685386657715 = 0.10271823406219482 + 1.0 * 6.066850185394287
Epoch 670, val loss: 0.7299083471298218
Epoch 680, training loss: 6.1594648361206055 = 0.09675976634025574 + 1.0 * 6.062705039978027
Epoch 680, val loss: 0.7330578565597534
Epoch 690, training loss: 6.152513027191162 = 0.09122475981712341 + 1.0 * 6.061288356781006
Epoch 690, val loss: 0.736449658870697
Epoch 700, training loss: 6.149012088775635 = 0.08607657998800278 + 1.0 * 6.0629353523254395
Epoch 700, val loss: 0.7400924563407898
Epoch 710, training loss: 6.144547462463379 = 0.08132229745388031 + 1.0 * 6.063225269317627
Epoch 710, val loss: 0.7439318895339966
Epoch 720, training loss: 6.1351094245910645 = 0.07692345976829529 + 1.0 * 6.058186054229736
Epoch 720, val loss: 0.7479263544082642
Epoch 730, training loss: 6.138797283172607 = 0.0728430449962616 + 1.0 * 6.065954208374023
Epoch 730, val loss: 0.7521020770072937
Epoch 740, training loss: 6.127249240875244 = 0.06906852126121521 + 1.0 * 6.058180809020996
Epoch 740, val loss: 0.756336510181427
Epoch 750, training loss: 6.120681285858154 = 0.06556130945682526 + 1.0 * 6.05511999130249
Epoch 750, val loss: 0.7608275413513184
Epoch 760, training loss: 6.11879825592041 = 0.0622866153717041 + 1.0 * 6.056511402130127
Epoch 760, val loss: 0.7654098868370056
Epoch 770, training loss: 6.1151227951049805 = 0.059241313487291336 + 1.0 * 6.055881500244141
Epoch 770, val loss: 0.7699341773986816
Epoch 780, training loss: 6.112611293792725 = 0.05642412230372429 + 1.0 * 6.056187152862549
Epoch 780, val loss: 0.7746214866638184
Epoch 790, training loss: 6.105278491973877 = 0.053793661296367645 + 1.0 * 6.051485061645508
Epoch 790, val loss: 0.7793582677841187
Epoch 800, training loss: 6.104548931121826 = 0.051328033208847046 + 1.0 * 6.053220748901367
Epoch 800, val loss: 0.7841647267341614
Epoch 810, training loss: 6.099003314971924 = 0.049019705504179 + 1.0 * 6.049983501434326
Epoch 810, val loss: 0.7889145612716675
Epoch 820, training loss: 6.100051403045654 = 0.04686134308576584 + 1.0 * 6.053190231323242
Epoch 820, val loss: 0.7937514185905457
Epoch 830, training loss: 6.099749565124512 = 0.044846151024103165 + 1.0 * 6.054903507232666
Epoch 830, val loss: 0.7984030842781067
Epoch 840, training loss: 6.092297077178955 = 0.04297587648034096 + 1.0 * 6.049321174621582
Epoch 840, val loss: 0.8031891584396362
Epoch 850, training loss: 6.087836742401123 = 0.04121369123458862 + 1.0 * 6.046623229980469
Epoch 850, val loss: 0.8080901503562927
Epoch 860, training loss: 6.084923267364502 = 0.039550021290779114 + 1.0 * 6.045373439788818
Epoch 860, val loss: 0.8129875063896179
Epoch 870, training loss: 6.082509517669678 = 0.03797454386949539 + 1.0 * 6.044535160064697
Epoch 870, val loss: 0.8178086280822754
Epoch 880, training loss: 6.093060493469238 = 0.03648937866091728 + 1.0 * 6.056571006774902
Epoch 880, val loss: 0.8225836753845215
Epoch 890, training loss: 6.080206871032715 = 0.03509874641895294 + 1.0 * 6.045108318328857
Epoch 890, val loss: 0.8271745443344116
Epoch 900, training loss: 6.079737186431885 = 0.03379051759839058 + 1.0 * 6.0459465980529785
Epoch 900, val loss: 0.8318938612937927
Epoch 910, training loss: 6.074418544769287 = 0.032555222511291504 + 1.0 * 6.041863441467285
Epoch 910, val loss: 0.8365952372550964
Epoch 920, training loss: 6.074459552764893 = 0.0313837006688118 + 1.0 * 6.043076038360596
Epoch 920, val loss: 0.841293215751648
Epoch 930, training loss: 6.070809364318848 = 0.03027532808482647 + 1.0 * 6.040534019470215
Epoch 930, val loss: 0.8458131551742554
Epoch 940, training loss: 6.070694923400879 = 0.029229991137981415 + 1.0 * 6.041464805603027
Epoch 940, val loss: 0.8503676652908325
Epoch 950, training loss: 6.070899486541748 = 0.028239363804459572 + 1.0 * 6.042660236358643
Epoch 950, val loss: 0.8549014925956726
Epoch 960, training loss: 6.068406105041504 = 0.027301236987113953 + 1.0 * 6.041104793548584
Epoch 960, val loss: 0.8594377040863037
Epoch 970, training loss: 6.069567680358887 = 0.026411322876811028 + 1.0 * 6.043156147003174
Epoch 970, val loss: 0.8638289570808411
Epoch 980, training loss: 6.064311981201172 = 0.025564994663000107 + 1.0 * 6.0387468338012695
Epoch 980, val loss: 0.868223249912262
Epoch 990, training loss: 6.062021732330322 = 0.02476208657026291 + 1.0 * 6.037259578704834
Epoch 990, val loss: 0.8726763129234314
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8524
Flip ASR: 0.8222/225 nodes
The final ASR:0.66913, 0.12981, Accuracy:0.80988, 0.01145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9430])
updated graph: torch.Size([2, 10464])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.321985244750977 = 1.9481714963912964 + 1.0 * 8.37381362915039
Epoch 0, val loss: 1.9426627159118652
Epoch 10, training loss: 10.310600280761719 = 1.9373723268508911 + 1.0 * 8.373228073120117
Epoch 10, val loss: 1.932592511177063
Epoch 20, training loss: 10.293488502502441 = 1.9238938093185425 + 1.0 * 8.36959457397461
Epoch 20, val loss: 1.9195730686187744
Epoch 30, training loss: 10.248985290527344 = 1.9053921699523926 + 1.0 * 8.34359359741211
Epoch 30, val loss: 1.901590347290039
Epoch 40, training loss: 10.007003784179688 = 1.8832708597183228 + 1.0 * 8.123732566833496
Epoch 40, val loss: 1.880981206893921
Epoch 50, training loss: 9.097155570983887 = 1.8605854511260986 + 1.0 * 7.236570358276367
Epoch 50, val loss: 1.8599594831466675
Epoch 60, training loss: 8.808938026428223 = 1.844783067703247 + 1.0 * 6.964155197143555
Epoch 60, val loss: 1.8457871675491333
Epoch 70, training loss: 8.608839988708496 = 1.832086443901062 + 1.0 * 6.7767534255981445
Epoch 70, val loss: 1.8335472345352173
Epoch 80, training loss: 8.477741241455078 = 1.8187941312789917 + 1.0 * 6.658947467803955
Epoch 80, val loss: 1.8213385343551636
Epoch 90, training loss: 8.386746406555176 = 1.8066200017929077 + 1.0 * 6.580126762390137
Epoch 90, val loss: 1.810081958770752
Epoch 100, training loss: 8.315850257873535 = 1.7940856218338013 + 1.0 * 6.521764278411865
Epoch 100, val loss: 1.7990761995315552
Epoch 110, training loss: 8.252201080322266 = 1.782008409500122 + 1.0 * 6.470192909240723
Epoch 110, val loss: 1.7885981798171997
Epoch 120, training loss: 8.195517539978027 = 1.7696329355239868 + 1.0 * 6.42588472366333
Epoch 120, val loss: 1.777852177619934
Epoch 130, training loss: 8.143051147460938 = 1.7559804916381836 + 1.0 * 6.387070655822754
Epoch 130, val loss: 1.766050100326538
Epoch 140, training loss: 8.092774391174316 = 1.7401670217514038 + 1.0 * 6.352607727050781
Epoch 140, val loss: 1.7526819705963135
Epoch 150, training loss: 8.045156478881836 = 1.7213495969772339 + 1.0 * 6.323807239532471
Epoch 150, val loss: 1.7369824647903442
Epoch 160, training loss: 7.99790620803833 = 1.6983790397644043 + 1.0 * 6.299527168273926
Epoch 160, val loss: 1.7181576490402222
Epoch 170, training loss: 7.95188570022583 = 1.6699730157852173 + 1.0 * 6.281912803649902
Epoch 170, val loss: 1.6948548555374146
Epoch 180, training loss: 7.901092052459717 = 1.6351704597473145 + 1.0 * 6.265921592712402
Epoch 180, val loss: 1.6664031744003296
Epoch 190, training loss: 7.845487594604492 = 1.5931915044784546 + 1.0 * 6.252295970916748
Epoch 190, val loss: 1.632053017616272
Epoch 200, training loss: 7.7860541343688965 = 1.5442471504211426 + 1.0 * 6.241806983947754
Epoch 200, val loss: 1.592297077178955
Epoch 210, training loss: 7.722528457641602 = 1.490910291671753 + 1.0 * 6.2316179275512695
Epoch 210, val loss: 1.5494003295898438
Epoch 220, training loss: 7.655889511108398 = 1.4342607259750366 + 1.0 * 6.221628665924072
Epoch 220, val loss: 1.504148006439209
Epoch 230, training loss: 7.592385292053223 = 1.3767807483673096 + 1.0 * 6.215604782104492
Epoch 230, val loss: 1.4593472480773926
Epoch 240, training loss: 7.526422023773193 = 1.320517659187317 + 1.0 * 6.205904483795166
Epoch 240, val loss: 1.4163051843643188
Epoch 250, training loss: 7.466238498687744 = 1.2660263776779175 + 1.0 * 6.200212001800537
Epoch 250, val loss: 1.3754682540893555
Epoch 260, training loss: 7.406642436981201 = 1.2141228914260864 + 1.0 * 6.192519664764404
Epoch 260, val loss: 1.3374370336532593
Epoch 270, training loss: 7.3501811027526855 = 1.1638450622558594 + 1.0 * 6.186336040496826
Epoch 270, val loss: 1.3010019063949585
Epoch 280, training loss: 7.301413536071777 = 1.1144301891326904 + 1.0 * 6.186983585357666
Epoch 280, val loss: 1.2655625343322754
Epoch 290, training loss: 7.244069576263428 = 1.0665487051010132 + 1.0 * 6.177520751953125
Epoch 290, val loss: 1.2312736511230469
Epoch 300, training loss: 7.190560817718506 = 1.0187644958496094 + 1.0 * 6.1717963218688965
Epoch 300, val loss: 1.1969987154006958
Epoch 310, training loss: 7.137669086456299 = 0.9705072641372681 + 1.0 * 6.16716194152832
Epoch 310, val loss: 1.162105679512024
Epoch 320, training loss: 7.093169212341309 = 0.9217628240585327 + 1.0 * 6.171406269073486
Epoch 320, val loss: 1.1267484426498413
Epoch 330, training loss: 7.036397933959961 = 0.8747634291648865 + 1.0 * 6.16163444519043
Epoch 330, val loss: 1.0916308164596558
Epoch 340, training loss: 6.986560344696045 = 0.8293570280075073 + 1.0 * 6.157203197479248
Epoch 340, val loss: 1.0576962232589722
Epoch 350, training loss: 6.93883752822876 = 0.7853829264640808 + 1.0 * 6.153454780578613
Epoch 350, val loss: 1.0241799354553223
Epoch 360, training loss: 6.89272928237915 = 0.7431499361991882 + 1.0 * 6.1495795249938965
Epoch 360, val loss: 0.9919986128807068
Epoch 370, training loss: 6.8495659828186035 = 0.7031902074813843 + 1.0 * 6.14637565612793
Epoch 370, val loss: 0.9615627527236938
Epoch 380, training loss: 6.811147689819336 = 0.6660731434822083 + 1.0 * 6.145074367523193
Epoch 380, val loss: 0.9337371587753296
Epoch 390, training loss: 6.771210670471191 = 0.6311163306236267 + 1.0 * 6.14009428024292
Epoch 390, val loss: 0.9081193804740906
Epoch 400, training loss: 6.734672546386719 = 0.5979316830635071 + 1.0 * 6.136740684509277
Epoch 400, val loss: 0.8844729661941528
Epoch 410, training loss: 6.703207015991211 = 0.5664964318275452 + 1.0 * 6.1367106437683105
Epoch 410, val loss: 0.8627041578292847
Epoch 420, training loss: 6.6684675216674805 = 0.5366137623786926 + 1.0 * 6.1318535804748535
Epoch 420, val loss: 0.842818021774292
Epoch 430, training loss: 6.635819435119629 = 0.5079327821731567 + 1.0 * 6.127886772155762
Epoch 430, val loss: 0.8244116902351379
Epoch 440, training loss: 6.608785152435303 = 0.4804801940917969 + 1.0 * 6.128304958343506
Epoch 440, val loss: 0.8074555993080139
Epoch 450, training loss: 6.578629016876221 = 0.4544319808483124 + 1.0 * 6.124197006225586
Epoch 450, val loss: 0.7921707034111023
Epoch 460, training loss: 6.548926830291748 = 0.42944926023483276 + 1.0 * 6.11947774887085
Epoch 460, val loss: 0.7781737446784973
Epoch 470, training loss: 6.527939796447754 = 0.40544867515563965 + 1.0 * 6.122491359710693
Epoch 470, val loss: 0.7654116153717041
Epoch 480, training loss: 6.500670909881592 = 0.38250863552093506 + 1.0 * 6.118162155151367
Epoch 480, val loss: 0.754033088684082
Epoch 490, training loss: 6.475133419036865 = 0.36065909266471863 + 1.0 * 6.114474296569824
Epoch 490, val loss: 0.7439724802970886
Epoch 500, training loss: 6.450780868530273 = 0.33978429436683655 + 1.0 * 6.110996723175049
Epoch 500, val loss: 0.7350959181785583
Epoch 510, training loss: 6.4378342628479 = 0.31974896788597107 + 1.0 * 6.1180853843688965
Epoch 510, val loss: 0.7273197174072266
Epoch 520, training loss: 6.407890319824219 = 0.30075135827064514 + 1.0 * 6.1071391105651855
Epoch 520, val loss: 0.7206804752349854
Epoch 530, training loss: 6.386939525604248 = 0.28274592757225037 + 1.0 * 6.104193687438965
Epoch 530, val loss: 0.7152761220932007
Epoch 540, training loss: 6.367432594299316 = 0.26556023955345154 + 1.0 * 6.101872444152832
Epoch 540, val loss: 0.7109022736549377
Epoch 550, training loss: 6.356837272644043 = 0.2491523027420044 + 1.0 * 6.107685089111328
Epoch 550, val loss: 0.70750892162323
Epoch 560, training loss: 6.333063125610352 = 0.23377229273319244 + 1.0 * 6.09929084777832
Epoch 560, val loss: 0.705121636390686
Epoch 570, training loss: 6.316399097442627 = 0.2192307859659195 + 1.0 * 6.097168445587158
Epoch 570, val loss: 0.7038671970367432
Epoch 580, training loss: 6.306087970733643 = 0.20546042919158936 + 1.0 * 6.100627422332764
Epoch 580, val loss: 0.703434407711029
Epoch 590, training loss: 6.289158344268799 = 0.19247759878635406 + 1.0 * 6.096680641174316
Epoch 590, val loss: 0.7037792205810547
Epoch 600, training loss: 6.275545120239258 = 0.180280402302742 + 1.0 * 6.095264911651611
Epoch 600, val loss: 0.7051249146461487
Epoch 610, training loss: 6.262758731842041 = 0.16884364187717438 + 1.0 * 6.093914985656738
Epoch 610, val loss: 0.707225501537323
Epoch 620, training loss: 6.248237133026123 = 0.15808238089084625 + 1.0 * 6.090154647827148
Epoch 620, val loss: 0.710157573223114
Epoch 630, training loss: 6.237973213195801 = 0.14800380170345306 + 1.0 * 6.089969635009766
Epoch 630, val loss: 0.7138019800186157
Epoch 640, training loss: 6.228676795959473 = 0.1386149525642395 + 1.0 * 6.090061664581299
Epoch 640, val loss: 0.7180789113044739
Epoch 650, training loss: 6.217291831970215 = 0.12989093363285065 + 1.0 * 6.087400913238525
Epoch 650, val loss: 0.7228283882141113
Epoch 660, training loss: 6.206223011016846 = 0.12178768962621689 + 1.0 * 6.08443546295166
Epoch 660, val loss: 0.7281844019889832
Epoch 670, training loss: 6.203419208526611 = 0.11424432694911957 + 1.0 * 6.089174747467041
Epoch 670, val loss: 0.7339656352996826
Epoch 680, training loss: 6.193146705627441 = 0.10726810991764069 + 1.0 * 6.085878372192383
Epoch 680, val loss: 0.7400352358818054
Epoch 690, training loss: 6.188064098358154 = 0.1008516177535057 + 1.0 * 6.087212562561035
Epoch 690, val loss: 0.7465804219245911
Epoch 700, training loss: 6.176122188568115 = 0.09492530673742294 + 1.0 * 6.0811967849731445
Epoch 700, val loss: 0.7533260583877563
Epoch 710, training loss: 6.167776584625244 = 0.08945310115814209 + 1.0 * 6.0783233642578125
Epoch 710, val loss: 0.7604018449783325
Epoch 720, training loss: 6.164915084838867 = 0.08436667174100876 + 1.0 * 6.080548286437988
Epoch 720, val loss: 0.7676578760147095
Epoch 730, training loss: 6.160480499267578 = 0.07969341427087784 + 1.0 * 6.080787181854248
Epoch 730, val loss: 0.7747880220413208
Epoch 740, training loss: 6.149810314178467 = 0.07539226114749908 + 1.0 * 6.074418067932129
Epoch 740, val loss: 0.7822397351264954
Epoch 750, training loss: 6.144397735595703 = 0.07140097767114639 + 1.0 * 6.072996616363525
Epoch 750, val loss: 0.7898092865943909
Epoch 760, training loss: 6.149505138397217 = 0.06769760698080063 + 1.0 * 6.081807613372803
Epoch 760, val loss: 0.7972988486289978
Epoch 770, training loss: 6.134838104248047 = 0.06427565962076187 + 1.0 * 6.070562362670898
Epoch 770, val loss: 0.8048673868179321
Epoch 780, training loss: 6.132874965667725 = 0.06110137701034546 + 1.0 * 6.071773529052734
Epoch 780, val loss: 0.8125311136245728
Epoch 790, training loss: 6.126547336578369 = 0.05814864858984947 + 1.0 * 6.068398475646973
Epoch 790, val loss: 0.820059061050415
Epoch 800, training loss: 6.124708652496338 = 0.055406343191862106 + 1.0 * 6.069302082061768
Epoch 800, val loss: 0.8276618719100952
Epoch 810, training loss: 6.120316982269287 = 0.05284351482987404 + 1.0 * 6.067473411560059
Epoch 810, val loss: 0.8353374004364014
Epoch 820, training loss: 6.117171287536621 = 0.05044523626565933 + 1.0 * 6.066726207733154
Epoch 820, val loss: 0.8427065014839172
Epoch 830, training loss: 6.11536979675293 = 0.048219263553619385 + 1.0 * 6.067150592803955
Epoch 830, val loss: 0.8502119183540344
Epoch 840, training loss: 6.10934591293335 = 0.04613099619746208 + 1.0 * 6.0632147789001465
Epoch 840, val loss: 0.8577116131782532
Epoch 850, training loss: 6.107588768005371 = 0.04415789619088173 + 1.0 * 6.0634307861328125
Epoch 850, val loss: 0.8650915622711182
Epoch 860, training loss: 6.107762336730957 = 0.04230436682701111 + 1.0 * 6.065457820892334
Epoch 860, val loss: 0.8723012804985046
Epoch 870, training loss: 6.102826118469238 = 0.040579166263341904 + 1.0 * 6.062246799468994
Epoch 870, val loss: 0.8794931173324585
Epoch 880, training loss: 6.099770545959473 = 0.03895411640405655 + 1.0 * 6.060816287994385
Epoch 880, val loss: 0.8866360187530518
Epoch 890, training loss: 6.104875087738037 = 0.037415843456983566 + 1.0 * 6.0674591064453125
Epoch 890, val loss: 0.8936211466789246
Epoch 900, training loss: 6.09930944442749 = 0.03597097843885422 + 1.0 * 6.063338279724121
Epoch 900, val loss: 0.9004062414169312
Epoch 910, training loss: 6.094357013702393 = 0.03461379185318947 + 1.0 * 6.059743404388428
Epoch 910, val loss: 0.9072173833847046
Epoch 920, training loss: 6.0911078453063965 = 0.03333141654729843 + 1.0 * 6.05777645111084
Epoch 920, val loss: 0.9140297174453735
Epoch 930, training loss: 6.0886616706848145 = 0.032113295048475266 + 1.0 * 6.056548595428467
Epoch 930, val loss: 0.9206991195678711
Epoch 940, training loss: 6.091415882110596 = 0.030958019196987152 + 1.0 * 6.060457706451416
Epoch 940, val loss: 0.927277147769928
Epoch 950, training loss: 6.087524890899658 = 0.029864268377423286 + 1.0 * 6.0576605796813965
Epoch 950, val loss: 0.9337680339813232
Epoch 960, training loss: 6.085015773773193 = 0.0288346316665411 + 1.0 * 6.056180953979492
Epoch 960, val loss: 0.9401925206184387
Epoch 970, training loss: 6.082223415374756 = 0.02785484679043293 + 1.0 * 6.054368495941162
Epoch 970, val loss: 0.9466192126274109
Epoch 980, training loss: 6.084062099456787 = 0.026923468336462975 + 1.0 * 6.057138442993164
Epoch 980, val loss: 0.9527880549430847
Epoch 990, training loss: 6.08047342300415 = 0.026045391336083412 + 1.0 * 6.0544281005859375
Epoch 990, val loss: 0.9588950872421265
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.7306
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.311686515808105 = 1.937806248664856 + 1.0 * 8.373880386352539
Epoch 0, val loss: 1.9369562864303589
Epoch 10, training loss: 10.301448822021484 = 1.927856683731079 + 1.0 * 8.373592376708984
Epoch 10, val loss: 1.926142930984497
Epoch 20, training loss: 10.287346839904785 = 1.9153252840042114 + 1.0 * 8.372021675109863
Epoch 20, val loss: 1.9121527671813965
Epoch 30, training loss: 10.259380340576172 = 1.8974277973175049 + 1.0 * 8.361952781677246
Epoch 30, val loss: 1.8919447660446167
Epoch 40, training loss: 10.169279098510742 = 1.873012900352478 + 1.0 * 8.296266555786133
Epoch 40, val loss: 1.8653175830841064
Epoch 50, training loss: 9.694175720214844 = 1.8470463752746582 + 1.0 * 7.847128868103027
Epoch 50, val loss: 1.8387458324432373
Epoch 60, training loss: 9.033917427062988 = 1.8270034790039062 + 1.0 * 7.206913948059082
Epoch 60, val loss: 1.8197705745697021
Epoch 70, training loss: 8.678963661193848 = 1.8147168159484863 + 1.0 * 6.864246845245361
Epoch 70, val loss: 1.8079612255096436
Epoch 80, training loss: 8.482367515563965 = 1.8024132251739502 + 1.0 * 6.6799540519714355
Epoch 80, val loss: 1.795797348022461
Epoch 90, training loss: 8.369566917419434 = 1.7916866540908813 + 1.0 * 6.577879905700684
Epoch 90, val loss: 1.7851213216781616
Epoch 100, training loss: 8.281925201416016 = 1.7804540395736694 + 1.0 * 6.501471042633057
Epoch 100, val loss: 1.7741622924804688
Epoch 110, training loss: 8.213540077209473 = 1.7687652111053467 + 1.0 * 6.444774627685547
Epoch 110, val loss: 1.7632238864898682
Epoch 120, training loss: 8.158236503601074 = 1.756033182144165 + 1.0 * 6.40220308303833
Epoch 120, val loss: 1.7517547607421875
Epoch 130, training loss: 8.110336303710938 = 1.7416659593582153 + 1.0 * 6.3686699867248535
Epoch 130, val loss: 1.7392535209655762
Epoch 140, training loss: 8.064166069030762 = 1.7248793840408325 + 1.0 * 6.339286804199219
Epoch 140, val loss: 1.7249422073364258
Epoch 150, training loss: 8.02108097076416 = 1.7046518325805664 + 1.0 * 6.316429138183594
Epoch 150, val loss: 1.7080029249191284
Epoch 160, training loss: 7.978667736053467 = 1.6798839569091797 + 1.0 * 6.298783779144287
Epoch 160, val loss: 1.6874216794967651
Epoch 170, training loss: 7.930050373077393 = 1.6499477624893188 + 1.0 * 6.280102729797363
Epoch 170, val loss: 1.6627801656723022
Epoch 180, training loss: 7.87763786315918 = 1.613390564918518 + 1.0 * 6.264247417449951
Epoch 180, val loss: 1.63277006149292
Epoch 190, training loss: 7.821601867675781 = 1.5694257020950317 + 1.0 * 6.252176284790039
Epoch 190, val loss: 1.596733570098877
Epoch 200, training loss: 7.758755683898926 = 1.5181959867477417 + 1.0 * 6.2405595779418945
Epoch 200, val loss: 1.5547609329223633
Epoch 210, training loss: 7.69671630859375 = 1.4607305526733398 + 1.0 * 6.23598575592041
Epoch 210, val loss: 1.5082998275756836
Epoch 220, training loss: 7.624285697937012 = 1.4010276794433594 + 1.0 * 6.223258018493652
Epoch 220, val loss: 1.461092472076416
Epoch 230, training loss: 7.556450843811035 = 1.341422200202942 + 1.0 * 6.215028762817383
Epoch 230, val loss: 1.4149293899536133
Epoch 240, training loss: 7.492157459259033 = 1.2833013534545898 + 1.0 * 6.208856105804443
Epoch 240, val loss: 1.3710287809371948
Epoch 250, training loss: 7.431967735290527 = 1.2288634777069092 + 1.0 * 6.203104019165039
Epoch 250, val loss: 1.3311266899108887
Epoch 260, training loss: 7.372495651245117 = 1.1778734922409058 + 1.0 * 6.194622039794922
Epoch 260, val loss: 1.2942384481430054
Epoch 270, training loss: 7.317261219024658 = 1.1286802291870117 + 1.0 * 6.1885809898376465
Epoch 270, val loss: 1.2591742277145386
Epoch 280, training loss: 7.267282009124756 = 1.0807899236679077 + 1.0 * 6.186491966247559
Epoch 280, val loss: 1.225406527519226
Epoch 290, training loss: 7.212363243103027 = 1.0336668491363525 + 1.0 * 6.178696155548096
Epoch 290, val loss: 1.192295789718628
Epoch 300, training loss: 7.160609722137451 = 0.9862327575683594 + 1.0 * 6.174376964569092
Epoch 300, val loss: 1.1588613986968994
Epoch 310, training loss: 7.1100239753723145 = 0.9388695955276489 + 1.0 * 6.171154499053955
Epoch 310, val loss: 1.125273585319519
Epoch 320, training loss: 7.0569634437561035 = 0.8919194936752319 + 1.0 * 6.165043830871582
Epoch 320, val loss: 1.091996192932129
Epoch 330, training loss: 7.006353378295898 = 0.8457857966423035 + 1.0 * 6.160567760467529
Epoch 330, val loss: 1.0590596199035645
Epoch 340, training loss: 6.970712661743164 = 0.8013198971748352 + 1.0 * 6.1693925857543945
Epoch 340, val loss: 1.0272347927093506
Epoch 350, training loss: 6.913744926452637 = 0.7602429389953613 + 1.0 * 6.153501987457275
Epoch 350, val loss: 0.9980096817016602
Epoch 360, training loss: 6.872442245483398 = 0.7223195433616638 + 1.0 * 6.15012264251709
Epoch 360, val loss: 0.9712306261062622
Epoch 370, training loss: 6.83795166015625 = 0.6871731281280518 + 1.0 * 6.150778770446777
Epoch 370, val loss: 0.9468227624893188
Epoch 380, training loss: 6.797795295715332 = 0.6549705862998962 + 1.0 * 6.142824649810791
Epoch 380, val loss: 0.9247744083404541
Epoch 390, training loss: 6.765377521514893 = 0.6249828934669495 + 1.0 * 6.140394687652588
Epoch 390, val loss: 0.9047135710716248
Epoch 400, training loss: 6.732894420623779 = 0.5965967774391174 + 1.0 * 6.136297702789307
Epoch 400, val loss: 0.8861541748046875
Epoch 410, training loss: 6.703030109405518 = 0.5692147016525269 + 1.0 * 6.133815288543701
Epoch 410, val loss: 0.8687350749969482
Epoch 420, training loss: 6.67940092086792 = 0.5423080325126648 + 1.0 * 6.1370930671691895
Epoch 420, val loss: 0.8519097566604614
Epoch 430, training loss: 6.644733428955078 = 0.5158492922782898 + 1.0 * 6.128884315490723
Epoch 430, val loss: 0.835808277130127
Epoch 440, training loss: 6.615817546844482 = 0.489621102809906 + 1.0 * 6.126196384429932
Epoch 440, val loss: 0.8201025724411011
Epoch 450, training loss: 6.586047172546387 = 0.4632636606693268 + 1.0 * 6.122783660888672
Epoch 450, val loss: 0.8046779036521912
Epoch 460, training loss: 6.567307472229004 = 0.4368032217025757 + 1.0 * 6.130504131317139
Epoch 460, val loss: 0.7896215915679932
Epoch 470, training loss: 6.531365871429443 = 0.41075795888900757 + 1.0 * 6.120607852935791
Epoch 470, val loss: 0.7752456665039062
Epoch 480, training loss: 6.501541614532471 = 0.38511332869529724 + 1.0 * 6.116428375244141
Epoch 480, val loss: 0.7617151141166687
Epoch 490, training loss: 6.477858543395996 = 0.35987183451652527 + 1.0 * 6.117986679077148
Epoch 490, val loss: 0.7489405870437622
Epoch 500, training loss: 6.448916435241699 = 0.335382878780365 + 1.0 * 6.1135334968566895
Epoch 500, val loss: 0.7372860312461853
Epoch 510, training loss: 6.429323673248291 = 0.3117959797382355 + 1.0 * 6.117527484893799
Epoch 510, val loss: 0.726927638053894
Epoch 520, training loss: 6.398972034454346 = 0.28946828842163086 + 1.0 * 6.109503746032715
Epoch 520, val loss: 0.7180647850036621
Epoch 530, training loss: 6.379647731781006 = 0.2683722674846649 + 1.0 * 6.111275672912598
Epoch 530, val loss: 0.710658073425293
Epoch 540, training loss: 6.3568572998046875 = 0.24868112802505493 + 1.0 * 6.108176231384277
Epoch 540, val loss: 0.7047485113143921
Epoch 550, training loss: 6.336014747619629 = 0.2304295003414154 + 1.0 * 6.105585098266602
Epoch 550, val loss: 0.7001320719718933
Epoch 560, training loss: 6.314672470092773 = 0.2135339230298996 + 1.0 * 6.101138591766357
Epoch 560, val loss: 0.6967379450798035
Epoch 570, training loss: 6.3140411376953125 = 0.19797229766845703 + 1.0 * 6.1160688400268555
Epoch 570, val loss: 0.6945472359657288
Epoch 580, training loss: 6.283954620361328 = 0.183933824300766 + 1.0 * 6.100020885467529
Epoch 580, val loss: 0.6932275295257568
Epoch 590, training loss: 6.26718282699585 = 0.1711280643939972 + 1.0 * 6.096054553985596
Epoch 590, val loss: 0.6930355429649353
Epoch 600, training loss: 6.25335693359375 = 0.159387469291687 + 1.0 * 6.093969345092773
Epoch 600, val loss: 0.6936221122741699
Epoch 610, training loss: 6.242576599121094 = 0.14868806302547455 + 1.0 * 6.093888759613037
Epoch 610, val loss: 0.6948894262313843
Epoch 620, training loss: 6.231081008911133 = 0.13904470205307007 + 1.0 * 6.092036247253418
Epoch 620, val loss: 0.6967114210128784
Epoch 630, training loss: 6.2203216552734375 = 0.13022778928279877 + 1.0 * 6.090094089508057
Epoch 630, val loss: 0.6989830136299133
Epoch 640, training loss: 6.210968494415283 = 0.1221117302775383 + 1.0 * 6.0888566970825195
Epoch 640, val loss: 0.7017782330513
Epoch 650, training loss: 6.20130729675293 = 0.11467090249061584 + 1.0 * 6.086636543273926
Epoch 650, val loss: 0.7049108147621155
Epoch 660, training loss: 6.193891525268555 = 0.10786942392587662 + 1.0 * 6.086021900177002
Epoch 660, val loss: 0.7083210945129395
Epoch 670, training loss: 6.194710731506348 = 0.10160257667303085 + 1.0 * 6.093108177185059
Epoch 670, val loss: 0.7120116949081421
Epoch 680, training loss: 6.179691791534424 = 0.09586254507303238 + 1.0 * 6.083829402923584
Epoch 680, val loss: 0.7160014510154724
Epoch 690, training loss: 6.17167329788208 = 0.09056592732667923 + 1.0 * 6.081107139587402
Epoch 690, val loss: 0.7201639413833618
Epoch 700, training loss: 6.170180320739746 = 0.08564388006925583 + 1.0 * 6.084536552429199
Epoch 700, val loss: 0.7245416045188904
Epoch 710, training loss: 6.161238670349121 = 0.0811084434390068 + 1.0 * 6.080130100250244
Epoch 710, val loss: 0.7291198372840881
Epoch 720, training loss: 6.158610820770264 = 0.07691486924886703 + 1.0 * 6.081696033477783
Epoch 720, val loss: 0.7337402701377869
Epoch 730, training loss: 6.149965286254883 = 0.07303592562675476 + 1.0 * 6.076929569244385
Epoch 730, val loss: 0.7383927702903748
Epoch 740, training loss: 6.144131660461426 = 0.06943551450967789 + 1.0 * 6.074696063995361
Epoch 740, val loss: 0.743212103843689
Epoch 750, training loss: 6.1388654708862305 = 0.06606855988502502 + 1.0 * 6.072796821594238
Epoch 750, val loss: 0.7480757832527161
Epoch 760, training loss: 6.146964073181152 = 0.0629185289144516 + 1.0 * 6.08404541015625
Epoch 760, val loss: 0.7530292868614197
Epoch 770, training loss: 6.132476806640625 = 0.06000332161784172 + 1.0 * 6.072473526000977
Epoch 770, val loss: 0.7579006552696228
Epoch 780, training loss: 6.127536773681641 = 0.05730824172496796 + 1.0 * 6.070228576660156
Epoch 780, val loss: 0.7628767490386963
Epoch 790, training loss: 6.123537063598633 = 0.054777830839157104 + 1.0 * 6.068759441375732
Epoch 790, val loss: 0.7678111791610718
Epoch 800, training loss: 6.121204853057861 = 0.05239178612828255 + 1.0 * 6.068812847137451
Epoch 800, val loss: 0.772845983505249
Epoch 810, training loss: 6.128685474395752 = 0.05015258118510246 + 1.0 * 6.078532695770264
Epoch 810, val loss: 0.7778075337409973
Epoch 820, training loss: 6.116109371185303 = 0.048069048672914505 + 1.0 * 6.068040370941162
Epoch 820, val loss: 0.7827432155609131
Epoch 830, training loss: 6.112608432769775 = 0.04612128064036369 + 1.0 * 6.0664873123168945
Epoch 830, val loss: 0.7876595854759216
Epoch 840, training loss: 6.107760906219482 = 0.04427545517683029 + 1.0 * 6.063485622406006
Epoch 840, val loss: 0.7925883531570435
Epoch 850, training loss: 6.105700492858887 = 0.04252631589770317 + 1.0 * 6.063174247741699
Epoch 850, val loss: 0.797541081905365
Epoch 860, training loss: 6.120968341827393 = 0.040875207632780075 + 1.0 * 6.080092906951904
Epoch 860, val loss: 0.8024168610572815
Epoch 870, training loss: 6.100529193878174 = 0.039324626326560974 + 1.0 * 6.061204433441162
Epoch 870, val loss: 0.8072317838668823
Epoch 880, training loss: 6.100137710571289 = 0.03786838427186012 + 1.0 * 6.06226921081543
Epoch 880, val loss: 0.812033474445343
Epoch 890, training loss: 6.096187591552734 = 0.03648185357451439 + 1.0 * 6.05970573425293
Epoch 890, val loss: 0.8168179988861084
Epoch 900, training loss: 6.104325771331787 = 0.035163797438144684 + 1.0 * 6.069161891937256
Epoch 900, val loss: 0.8215393424034119
Epoch 910, training loss: 6.095108509063721 = 0.03391999006271362 + 1.0 * 6.061188697814941
Epoch 910, val loss: 0.8263556957244873
Epoch 920, training loss: 6.097917556762695 = 0.0327439121901989 + 1.0 * 6.065173625946045
Epoch 920, val loss: 0.831002950668335
Epoch 930, training loss: 6.094559669494629 = 0.03163065016269684 + 1.0 * 6.062929153442383
Epoch 930, val loss: 0.835659384727478
Epoch 940, training loss: 6.088372707366943 = 0.030574802309274673 + 1.0 * 6.057797908782959
Epoch 940, val loss: 0.8403458595275879
Epoch 950, training loss: 6.0858259201049805 = 0.029571058228611946 + 1.0 * 6.056254863739014
Epoch 950, val loss: 0.8449623584747314
Epoch 960, training loss: 6.0835065841674805 = 0.028610851615667343 + 1.0 * 6.054895877838135
Epoch 960, val loss: 0.8495227694511414
Epoch 970, training loss: 6.0856170654296875 = 0.027692360803484917 + 1.0 * 6.057924747467041
Epoch 970, val loss: 0.8541106581687927
Epoch 980, training loss: 6.081417560577393 = 0.026817679405212402 + 1.0 * 6.054599761962891
Epoch 980, val loss: 0.8586485981941223
Epoch 990, training loss: 6.084259033203125 = 0.025988232344388962 + 1.0 * 6.0582709312438965
Epoch 990, val loss: 0.8631843328475952
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6531
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.312812805175781 = 1.9389585256576538 + 1.0 * 8.373854637145996
Epoch 0, val loss: 1.9390143156051636
Epoch 10, training loss: 10.302576065063477 = 1.9290883541107178 + 1.0 * 8.37348747253418
Epoch 10, val loss: 1.9293149709701538
Epoch 20, training loss: 10.288354873657227 = 1.9169423580169678 + 1.0 * 8.37141227722168
Epoch 20, val loss: 1.916845679283142
Epoch 30, training loss: 10.258260726928711 = 1.9001247882843018 + 1.0 * 8.358136177062988
Epoch 30, val loss: 1.8994543552398682
Epoch 40, training loss: 10.140254974365234 = 1.8780781030654907 + 1.0 * 8.262176513671875
Epoch 40, val loss: 1.8775063753128052
Epoch 50, training loss: 9.471935272216797 = 1.8550541400909424 + 1.0 * 7.616880893707275
Epoch 50, val loss: 1.8550044298171997
Epoch 60, training loss: 8.988317489624023 = 1.8365225791931152 + 1.0 * 7.15179443359375
Epoch 60, val loss: 1.8378771543502808
Epoch 70, training loss: 8.706968307495117 = 1.8238791227340698 + 1.0 * 6.883089065551758
Epoch 70, val loss: 1.8257533311843872
Epoch 80, training loss: 8.549357414245605 = 1.810828685760498 + 1.0 * 6.738528728485107
Epoch 80, val loss: 1.8127883672714233
Epoch 90, training loss: 8.441390991210938 = 1.7972830533981323 + 1.0 * 6.644108295440674
Epoch 90, val loss: 1.799653172492981
Epoch 100, training loss: 8.365405082702637 = 1.78416907787323 + 1.0 * 6.581236362457275
Epoch 100, val loss: 1.7869393825531006
Epoch 110, training loss: 8.296083450317383 = 1.7715591192245483 + 1.0 * 6.524524211883545
Epoch 110, val loss: 1.7751128673553467
Epoch 120, training loss: 8.227046966552734 = 1.7589449882507324 + 1.0 * 6.468101501464844
Epoch 120, val loss: 1.7635270357131958
Epoch 130, training loss: 8.162699699401855 = 1.7452313899993896 + 1.0 * 6.417468070983887
Epoch 130, val loss: 1.751033067703247
Epoch 140, training loss: 8.110267639160156 = 1.7292771339416504 + 1.0 * 6.380990028381348
Epoch 140, val loss: 1.7365689277648926
Epoch 150, training loss: 8.05993938446045 = 1.7099993228912354 + 1.0 * 6.349939823150635
Epoch 150, val loss: 1.7194507122039795
Epoch 160, training loss: 8.010820388793945 = 1.6865770816802979 + 1.0 * 6.324243068695068
Epoch 160, val loss: 1.699103832244873
Epoch 170, training loss: 7.961124420166016 = 1.6576604843139648 + 1.0 * 6.303463935852051
Epoch 170, val loss: 1.6745178699493408
Epoch 180, training loss: 7.910767078399658 = 1.6216527223587036 + 1.0 * 6.289114475250244
Epoch 180, val loss: 1.644363284111023
Epoch 190, training loss: 7.849795341491699 = 1.5783940553665161 + 1.0 * 6.271401405334473
Epoch 190, val loss: 1.607838749885559
Epoch 200, training loss: 7.785755634307861 = 1.5261691808700562 + 1.0 * 6.259586334228516
Epoch 200, val loss: 1.5639535188674927
Epoch 210, training loss: 7.716125965118408 = 1.4646011590957642 + 1.0 * 6.251524925231934
Epoch 210, val loss: 1.5125465393066406
Epoch 220, training loss: 7.638329982757568 = 1.3959754705429077 + 1.0 * 6.242354393005371
Epoch 220, val loss: 1.4555755853652954
Epoch 230, training loss: 7.559329986572266 = 1.3222522735595703 + 1.0 * 6.237077713012695
Epoch 230, val loss: 1.3950564861297607
Epoch 240, training loss: 7.477165222167969 = 1.2474753856658936 + 1.0 * 6.229690074920654
Epoch 240, val loss: 1.3346689939498901
Epoch 250, training loss: 7.398550033569336 = 1.1739869117736816 + 1.0 * 6.224563121795654
Epoch 250, val loss: 1.275968074798584
Epoch 260, training loss: 7.325160503387451 = 1.1044522523880005 + 1.0 * 6.22070837020874
Epoch 260, val loss: 1.2216482162475586
Epoch 270, training loss: 7.252824783325195 = 1.0405669212341309 + 1.0 * 6.2122578620910645
Epoch 270, val loss: 1.1724096536636353
Epoch 280, training loss: 7.19061803817749 = 0.9819086790084839 + 1.0 * 6.208709239959717
Epoch 280, val loss: 1.1278351545333862
Epoch 290, training loss: 7.131741523742676 = 0.9292177557945251 + 1.0 * 6.202523708343506
Epoch 290, val loss: 1.0883668661117554
Epoch 300, training loss: 7.075925350189209 = 0.8814581632614136 + 1.0 * 6.194467067718506
Epoch 300, val loss: 1.053380012512207
Epoch 310, training loss: 7.031911849975586 = 0.8375053405761719 + 1.0 * 6.194406509399414
Epoch 310, val loss: 1.0216461420059204
Epoch 320, training loss: 6.982491970062256 = 0.7971206307411194 + 1.0 * 6.185371398925781
Epoch 320, val loss: 0.9932749271392822
Epoch 330, training loss: 6.941797733306885 = 0.7593091130256653 + 1.0 * 6.182488441467285
Epoch 330, val loss: 0.966919481754303
Epoch 340, training loss: 6.898047924041748 = 0.7235794067382812 + 1.0 * 6.174468517303467
Epoch 340, val loss: 0.9425273537635803
Epoch 350, training loss: 6.858706474304199 = 0.6893186569213867 + 1.0 * 6.1693878173828125
Epoch 350, val loss: 0.9196253418922424
Epoch 360, training loss: 6.826324939727783 = 0.6558939814567566 + 1.0 * 6.170431137084961
Epoch 360, val loss: 0.8974592685699463
Epoch 370, training loss: 6.785140037536621 = 0.6233747005462646 + 1.0 * 6.161765098571777
Epoch 370, val loss: 0.8761606812477112
Epoch 380, training loss: 6.748603820800781 = 0.5915628671646118 + 1.0 * 6.157041072845459
Epoch 380, val loss: 0.8556637167930603
Epoch 390, training loss: 6.714138984680176 = 0.56059330701828 + 1.0 * 6.15354585647583
Epoch 390, val loss: 0.8359224796295166
Epoch 400, training loss: 6.6860198974609375 = 0.5305435657501221 + 1.0 * 6.1554765701293945
Epoch 400, val loss: 0.816950261592865
Epoch 410, training loss: 6.6482977867126465 = 0.5018537640571594 + 1.0 * 6.146443843841553
Epoch 410, val loss: 0.7992792129516602
Epoch 420, training loss: 6.616460800170898 = 0.47425779700279236 + 1.0 * 6.142202854156494
Epoch 420, val loss: 0.7826429009437561
Epoch 430, training loss: 6.587186336517334 = 0.4476664960384369 + 1.0 * 6.139519691467285
Epoch 430, val loss: 0.7670276761054993
Epoch 440, training loss: 6.563742637634277 = 0.42234110832214355 + 1.0 * 6.141401290893555
Epoch 440, val loss: 0.7525731921195984
Epoch 450, training loss: 6.534985542297363 = 0.3984408676624298 + 1.0 * 6.136544704437256
Epoch 450, val loss: 0.7395970225334167
Epoch 460, training loss: 6.506436347961426 = 0.37564149498939514 + 1.0 * 6.130795001983643
Epoch 460, val loss: 0.7276591062545776
Epoch 470, training loss: 6.49107551574707 = 0.35379576683044434 + 1.0 * 6.137279987335205
Epoch 470, val loss: 0.7166559100151062
Epoch 480, training loss: 6.462871551513672 = 0.33302369713783264 + 1.0 * 6.129848003387451
Epoch 480, val loss: 0.7066745758056641
Epoch 490, training loss: 6.438231468200684 = 0.3133509159088135 + 1.0 * 6.124880790710449
Epoch 490, val loss: 0.6976498365402222
Epoch 500, training loss: 6.415350437164307 = 0.2945925295352936 + 1.0 * 6.120758056640625
Epoch 500, val loss: 0.689518928527832
Epoch 510, training loss: 6.395012855529785 = 0.2765817642211914 + 1.0 * 6.118431091308594
Epoch 510, val loss: 0.6821693778038025
Epoch 520, training loss: 6.391310691833496 = 0.2593116760253906 + 1.0 * 6.1319990158081055
Epoch 520, val loss: 0.6755942702293396
Epoch 530, training loss: 6.359736442565918 = 0.24305380880832672 + 1.0 * 6.116682529449463
Epoch 530, val loss: 0.6697513461112976
Epoch 540, training loss: 6.339379787445068 = 0.22757655382156372 + 1.0 * 6.11180305480957
Epoch 540, val loss: 0.6647637486457825
Epoch 550, training loss: 6.329592704772949 = 0.21283777058124542 + 1.0 * 6.11675500869751
Epoch 550, val loss: 0.6604928374290466
Epoch 560, training loss: 6.309382915496826 = 0.19894708693027496 + 1.0 * 6.110435962677002
Epoch 560, val loss: 0.6569581627845764
Epoch 570, training loss: 6.293264865875244 = 0.18587857484817505 + 1.0 * 6.107386112213135
Epoch 570, val loss: 0.6541694402694702
Epoch 580, training loss: 6.278192520141602 = 0.17356324195861816 + 1.0 * 6.1046295166015625
Epoch 580, val loss: 0.6520569324493408
Epoch 590, training loss: 6.275188446044922 = 0.16201555728912354 + 1.0 * 6.113173007965088
Epoch 590, val loss: 0.6505702137947083
Epoch 600, training loss: 6.257364749908447 = 0.15134267508983612 + 1.0 * 6.106021881103516
Epoch 600, val loss: 0.6497058868408203
Epoch 610, training loss: 6.242083549499512 = 0.14141491055488586 + 1.0 * 6.100668430328369
Epoch 610, val loss: 0.6495198011398315
Epoch 620, training loss: 6.232639789581299 = 0.13216176629066467 + 1.0 * 6.100478172302246
Epoch 620, val loss: 0.6499186754226685
Epoch 630, training loss: 6.219453811645508 = 0.123593270778656 + 1.0 * 6.095860481262207
Epoch 630, val loss: 0.6508032083511353
Epoch 640, training loss: 6.209828853607178 = 0.11565999686717987 + 1.0 * 6.094168663024902
Epoch 640, val loss: 0.652152955532074
Epoch 650, training loss: 6.212612152099609 = 0.10832133889198303 + 1.0 * 6.104290962219238
Epoch 650, val loss: 0.653922438621521
Epoch 660, training loss: 6.193995475769043 = 0.10162191092967987 + 1.0 * 6.092373371124268
Epoch 660, val loss: 0.6560046076774597
Epoch 670, training loss: 6.184730529785156 = 0.09550411254167557 + 1.0 * 6.089226245880127
Epoch 670, val loss: 0.6583947539329529
Epoch 680, training loss: 6.17720890045166 = 0.08985640108585358 + 1.0 * 6.087352275848389
Epoch 680, val loss: 0.6611284613609314
Epoch 690, training loss: 6.17244815826416 = 0.08462774753570557 + 1.0 * 6.087820529937744
Epoch 690, val loss: 0.6641651391983032
Epoch 700, training loss: 6.168558597564697 = 0.07982555031776428 + 1.0 * 6.088733196258545
Epoch 700, val loss: 0.6673447489738464
Epoch 710, training loss: 6.159752368927002 = 0.07544699311256409 + 1.0 * 6.084305286407471
Epoch 710, val loss: 0.6706336140632629
Epoch 720, training loss: 6.161149978637695 = 0.07140849530696869 + 1.0 * 6.0897417068481445
Epoch 720, val loss: 0.6741088628768921
Epoch 730, training loss: 6.150615215301514 = 0.06769384443759918 + 1.0 * 6.082921504974365
Epoch 730, val loss: 0.677723228931427
Epoch 740, training loss: 6.144424915313721 = 0.06424376368522644 + 1.0 * 6.080181121826172
Epoch 740, val loss: 0.6815159320831299
Epoch 750, training loss: 6.151371479034424 = 0.06104555353522301 + 1.0 * 6.090325832366943
Epoch 750, val loss: 0.685439944267273
Epoch 760, training loss: 6.1375346183776855 = 0.058102019131183624 + 1.0 * 6.079432487487793
Epoch 760, val loss: 0.6893242597579956
Epoch 770, training loss: 6.132389545440674 = 0.05537675321102142 + 1.0 * 6.07701301574707
Epoch 770, val loss: 0.6932868957519531
Epoch 780, training loss: 6.129341125488281 = 0.05282977223396301 + 1.0 * 6.076511383056641
Epoch 780, val loss: 0.6973341107368469
Epoch 790, training loss: 6.125244140625 = 0.050457753241062164 + 1.0 * 6.074786186218262
Epoch 790, val loss: 0.7013609409332275
Epoch 800, training loss: 6.122673034667969 = 0.04827163368463516 + 1.0 * 6.074401378631592
Epoch 800, val loss: 0.705333411693573
Epoch 810, training loss: 6.1179351806640625 = 0.04622340947389603 + 1.0 * 6.071711540222168
Epoch 810, val loss: 0.709359347820282
Epoch 820, training loss: 6.114352703094482 = 0.04429055005311966 + 1.0 * 6.070062160491943
Epoch 820, val loss: 0.7134678363800049
Epoch 830, training loss: 6.130781173706055 = 0.04247787967324257 + 1.0 * 6.088303089141846
Epoch 830, val loss: 0.7176101803779602
Epoch 840, training loss: 6.110928535461426 = 0.04079116880893707 + 1.0 * 6.0701375007629395
Epoch 840, val loss: 0.7216302156448364
Epoch 850, training loss: 6.106565952301025 = 0.03921694681048393 + 1.0 * 6.067348957061768
Epoch 850, val loss: 0.7256054878234863
Epoch 860, training loss: 6.104400634765625 = 0.037726614624261856 + 1.0 * 6.06667423248291
Epoch 860, val loss: 0.729632556438446
Epoch 870, training loss: 6.112001895904541 = 0.036313895136117935 + 1.0 * 6.075687885284424
Epoch 870, val loss: 0.7336728572845459
Epoch 880, training loss: 6.106201171875 = 0.03498854488134384 + 1.0 * 6.0712127685546875
Epoch 880, val loss: 0.7376306653022766
Epoch 890, training loss: 6.097429275512695 = 0.03374066203832626 + 1.0 * 6.0636887550354
Epoch 890, val loss: 0.741537868976593
Epoch 900, training loss: 6.0960001945495605 = 0.032559484243392944 + 1.0 * 6.063440799713135
Epoch 900, val loss: 0.7454383373260498
Epoch 910, training loss: 6.096225738525391 = 0.03143501654267311 + 1.0 * 6.064790725708008
Epoch 910, val loss: 0.7493417263031006
Epoch 920, training loss: 6.09211540222168 = 0.0303674153983593 + 1.0 * 6.061748027801514
Epoch 920, val loss: 0.7532204389572144
Epoch 930, training loss: 6.092803478240967 = 0.02935737557709217 + 1.0 * 6.063446044921875
Epoch 930, val loss: 0.7570661902427673
Epoch 940, training loss: 6.095747470855713 = 0.028395475819706917 + 1.0 * 6.067351818084717
Epoch 940, val loss: 0.7608745694160461
Epoch 950, training loss: 6.092026233673096 = 0.027491863816976547 + 1.0 * 6.0645341873168945
Epoch 950, val loss: 0.7646214365959167
Epoch 960, training loss: 6.0850348472595215 = 0.026637446135282516 + 1.0 * 6.05839729309082
Epoch 960, val loss: 0.7683144807815552
Epoch 970, training loss: 6.082929611206055 = 0.02581995725631714 + 1.0 * 6.057109832763672
Epoch 970, val loss: 0.7720339894294739
Epoch 980, training loss: 6.0837860107421875 = 0.02503582090139389 + 1.0 * 6.058750152587891
Epoch 980, val loss: 0.7757569551467896
Epoch 990, training loss: 6.081806659698486 = 0.02428727224469185 + 1.0 * 6.057519435882568
Epoch 990, val loss: 0.7794115543365479
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.9299
Flip ASR: 0.9156/225 nodes
The final ASR:0.77122, 0.11657, Accuracy:0.83704, 0.01386
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10598])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.317715644836426 = 1.9438058137893677 + 1.0 * 8.373909950256348
Epoch 0, val loss: 1.9465279579162598
Epoch 10, training loss: 10.307308197021484 = 1.9336680173873901 + 1.0 * 8.373640060424805
Epoch 10, val loss: 1.9368677139282227
Epoch 20, training loss: 10.292398452758789 = 1.9208027124404907 + 1.0 * 8.37159538269043
Epoch 20, val loss: 1.924143671989441
Epoch 30, training loss: 10.258077621459961 = 1.9022740125656128 + 1.0 * 8.355803489685059
Epoch 30, val loss: 1.905430555343628
Epoch 40, training loss: 10.133328437805176 = 1.8775588274002075 + 1.0 * 8.255769729614258
Epoch 40, val loss: 1.8813250064849854
Epoch 50, training loss: 9.726323127746582 = 1.851308822631836 + 1.0 * 7.875014305114746
Epoch 50, val loss: 1.8569087982177734
Epoch 60, training loss: 9.375602722167969 = 1.8283358812332153 + 1.0 * 7.547266960144043
Epoch 60, val loss: 1.8372443914413452
Epoch 70, training loss: 8.915287017822266 = 1.8114795684814453 + 1.0 * 7.103806972503662
Epoch 70, val loss: 1.8227598667144775
Epoch 80, training loss: 8.63267993927002 = 1.7993265390396118 + 1.0 * 6.833353519439697
Epoch 80, val loss: 1.8122881650924683
Epoch 90, training loss: 8.433462142944336 = 1.7861931324005127 + 1.0 * 6.647269248962402
Epoch 90, val loss: 1.80112624168396
Epoch 100, training loss: 8.294119834899902 = 1.7715764045715332 + 1.0 * 6.522543430328369
Epoch 100, val loss: 1.788670301437378
Epoch 110, training loss: 8.210103034973145 = 1.7556679248809814 + 1.0 * 6.454434871673584
Epoch 110, val loss: 1.7746535539627075
Epoch 120, training loss: 8.14437198638916 = 1.7378352880477905 + 1.0 * 6.40653657913208
Epoch 120, val loss: 1.758812665939331
Epoch 130, training loss: 8.088743209838867 = 1.717631459236145 + 1.0 * 6.371111869812012
Epoch 130, val loss: 1.7410385608673096
Epoch 140, training loss: 8.036737442016602 = 1.6946110725402832 + 1.0 * 6.34212589263916
Epoch 140, val loss: 1.7213225364685059
Epoch 150, training loss: 7.986517906188965 = 1.6684130430221558 + 1.0 * 6.3181047439575195
Epoch 150, val loss: 1.6992908716201782
Epoch 160, training loss: 7.935849189758301 = 1.6383787393569946 + 1.0 * 6.297470569610596
Epoch 160, val loss: 1.6741886138916016
Epoch 170, training loss: 7.888208866119385 = 1.6039541959762573 + 1.0 * 6.284254550933838
Epoch 170, val loss: 1.6456035375595093
Epoch 180, training loss: 7.831595420837402 = 1.5653221607208252 + 1.0 * 6.266273021697998
Epoch 180, val loss: 1.6135141849517822
Epoch 190, training loss: 7.774980545043945 = 1.5220248699188232 + 1.0 * 6.252955436706543
Epoch 190, val loss: 1.5776562690734863
Epoch 200, training loss: 7.715627670288086 = 1.474013328552246 + 1.0 * 6.24161434173584
Epoch 200, val loss: 1.5381702184677124
Epoch 210, training loss: 7.6629557609558105 = 1.4219613075256348 + 1.0 * 6.240994453430176
Epoch 210, val loss: 1.4957438707351685
Epoch 220, training loss: 7.591798782348633 = 1.368450403213501 + 1.0 * 6.223348140716553
Epoch 220, val loss: 1.4525063037872314
Epoch 230, training loss: 7.530001163482666 = 1.3138242959976196 + 1.0 * 6.216176986694336
Epoch 230, val loss: 1.4089083671569824
Epoch 240, training loss: 7.468033313751221 = 1.2588251829147339 + 1.0 * 6.209208011627197
Epoch 240, val loss: 1.365760087966919
Epoch 250, training loss: 7.408891201019287 = 1.2052693367004395 + 1.0 * 6.203621864318848
Epoch 250, val loss: 1.3244463205337524
Epoch 260, training loss: 7.349095344543457 = 1.153595209121704 + 1.0 * 6.195499897003174
Epoch 260, val loss: 1.285249948501587
Epoch 270, training loss: 7.293773651123047 = 1.1037310361862183 + 1.0 * 6.190042495727539
Epoch 270, val loss: 1.2480111122131348
Epoch 280, training loss: 7.244076728820801 = 1.0561964511871338 + 1.0 * 6.187880039215088
Epoch 280, val loss: 1.2128069400787354
Epoch 290, training loss: 7.190349578857422 = 1.0109323263168335 + 1.0 * 6.179417133331299
Epoch 290, val loss: 1.1795421838760376
Epoch 300, training loss: 7.141016960144043 = 0.9672847986221313 + 1.0 * 6.173732280731201
Epoch 300, val loss: 1.1477088928222656
Epoch 310, training loss: 7.094927787780762 = 0.9249823689460754 + 1.0 * 6.169945240020752
Epoch 310, val loss: 1.1167551279067993
Epoch 320, training loss: 7.053925514221191 = 0.8839461207389832 + 1.0 * 6.169979572296143
Epoch 320, val loss: 1.086640477180481
Epoch 330, training loss: 7.006202220916748 = 0.844038188457489 + 1.0 * 6.162164211273193
Epoch 330, val loss: 1.0572571754455566
Epoch 340, training loss: 6.961580753326416 = 0.8045554161071777 + 1.0 * 6.157025337219238
Epoch 340, val loss: 1.0281513929367065
Epoch 350, training loss: 6.918148994445801 = 0.7652513384819031 + 1.0 * 6.152897834777832
Epoch 350, val loss: 0.9991716146469116
Epoch 360, training loss: 6.882744789123535 = 0.7263211011886597 + 1.0 * 6.156423568725586
Epoch 360, val loss: 0.9706428647041321
Epoch 370, training loss: 6.837165832519531 = 0.6883158087730408 + 1.0 * 6.148849964141846
Epoch 370, val loss: 0.9432079792022705
Epoch 380, training loss: 6.795236110687256 = 0.6516137719154358 + 1.0 * 6.143622398376465
Epoch 380, val loss: 0.9172965884208679
Epoch 390, training loss: 6.764738082885742 = 0.6163594722747803 + 1.0 * 6.148378849029541
Epoch 390, val loss: 0.8932666778564453
Epoch 400, training loss: 6.7212419509887695 = 0.583275318145752 + 1.0 * 6.137966632843018
Epoch 400, val loss: 0.8714753985404968
Epoch 410, training loss: 6.687522888183594 = 0.5521077513694763 + 1.0 * 6.135415077209473
Epoch 410, val loss: 0.8521787524223328
Epoch 420, training loss: 6.655018329620361 = 0.5226787328720093 + 1.0 * 6.1323394775390625
Epoch 420, val loss: 0.8351146578788757
Epoch 430, training loss: 6.625003337860107 = 0.4949522912502289 + 1.0 * 6.130051136016846
Epoch 430, val loss: 0.8200798034667969
Epoch 440, training loss: 6.5998921394348145 = 0.4688643217086792 + 1.0 * 6.131027698516846
Epoch 440, val loss: 0.8071169257164001
Epoch 450, training loss: 6.570441722869873 = 0.44422170519828796 + 1.0 * 6.126220226287842
Epoch 450, val loss: 0.7959014177322388
Epoch 460, training loss: 6.543595790863037 = 0.4207592010498047 + 1.0 * 6.122836589813232
Epoch 460, val loss: 0.7861821055412292
Epoch 470, training loss: 6.521157741546631 = 0.3983093798160553 + 1.0 * 6.1228485107421875
Epoch 470, val loss: 0.777715265750885
Epoch 480, training loss: 6.50088357925415 = 0.3768734633922577 + 1.0 * 6.12401008605957
Epoch 480, val loss: 0.7704080939292908
Epoch 490, training loss: 6.473535060882568 = 0.3563386797904968 + 1.0 * 6.117196559906006
Epoch 490, val loss: 0.7641814947128296
Epoch 500, training loss: 6.451615333557129 = 0.3366966247558594 + 1.0 * 6.1149187088012695
Epoch 500, val loss: 0.7588692307472229
Epoch 510, training loss: 6.436641693115234 = 0.3178226947784424 + 1.0 * 6.118818759918213
Epoch 510, val loss: 0.7543089985847473
Epoch 520, training loss: 6.412849426269531 = 0.2997739315032959 + 1.0 * 6.113075256347656
Epoch 520, val loss: 0.7505021691322327
Epoch 530, training loss: 6.3914313316345215 = 0.2825126349925995 + 1.0 * 6.1089186668396
Epoch 530, val loss: 0.7474607825279236
Epoch 540, training loss: 6.374112129211426 = 0.26598116755485535 + 1.0 * 6.108130931854248
Epoch 540, val loss: 0.7450482845306396
Epoch 550, training loss: 6.363828182220459 = 0.2502216398715973 + 1.0 * 6.1136064529418945
Epoch 550, val loss: 0.7431578636169434
Epoch 560, training loss: 6.339665412902832 = 0.23533867299556732 + 1.0 * 6.1043267250061035
Epoch 560, val loss: 0.7420036196708679
Epoch 570, training loss: 6.3238725662231445 = 0.22120921313762665 + 1.0 * 6.102663516998291
Epoch 570, val loss: 0.741398811340332
Epoch 580, training loss: 6.307847023010254 = 0.2077629566192627 + 1.0 * 6.100083827972412
Epoch 580, val loss: 0.7412785291671753
Epoch 590, training loss: 6.295614719390869 = 0.19499154388904572 + 1.0 * 6.10062313079834
Epoch 590, val loss: 0.7416937351226807
Epoch 600, training loss: 6.283280849456787 = 0.18291589617729187 + 1.0 * 6.100365161895752
Epoch 600, val loss: 0.7425862550735474
Epoch 610, training loss: 6.269008636474609 = 0.1715821772813797 + 1.0 * 6.097426414489746
Epoch 610, val loss: 0.7438725829124451
Epoch 620, training loss: 6.255843162536621 = 0.16094054281711578 + 1.0 * 6.094902515411377
Epoch 620, val loss: 0.7456121444702148
Epoch 630, training loss: 6.244120121002197 = 0.15092211961746216 + 1.0 * 6.093197822570801
Epoch 630, val loss: 0.7476994395256042
Epoch 640, training loss: 6.243824005126953 = 0.1415300965309143 + 1.0 * 6.102293968200684
Epoch 640, val loss: 0.7501694560050964
Epoch 650, training loss: 6.227215766906738 = 0.1327214241027832 + 1.0 * 6.094494342803955
Epoch 650, val loss: 0.7528281807899475
Epoch 660, training loss: 6.21315336227417 = 0.12453456223011017 + 1.0 * 6.088618755340576
Epoch 660, val loss: 0.7559853792190552
Epoch 670, training loss: 6.2068095207214355 = 0.11687137186527252 + 1.0 * 6.089938163757324
Epoch 670, val loss: 0.7593443393707275
Epoch 680, training loss: 6.197399139404297 = 0.10974042862653732 + 1.0 * 6.087658882141113
Epoch 680, val loss: 0.7628721594810486
Epoch 690, training loss: 6.188832759857178 = 0.10311456769704819 + 1.0 * 6.085718154907227
Epoch 690, val loss: 0.7667499780654907
Epoch 700, training loss: 6.190933704376221 = 0.09695076197385788 + 1.0 * 6.093983173370361
Epoch 700, val loss: 0.7707119584083557
Epoch 710, training loss: 6.174738883972168 = 0.09124473482370377 + 1.0 * 6.083494186401367
Epoch 710, val loss: 0.7747677564620972
Epoch 720, training loss: 6.168243408203125 = 0.08595231920480728 + 1.0 * 6.082291126251221
Epoch 720, val loss: 0.7790755033493042
Epoch 730, training loss: 6.170763969421387 = 0.0810418426990509 + 1.0 * 6.089722156524658
Epoch 730, val loss: 0.7834881544113159
Epoch 740, training loss: 6.160043716430664 = 0.07647373527288437 + 1.0 * 6.0835700035095215
Epoch 740, val loss: 0.7878814339637756
Epoch 750, training loss: 6.149878978729248 = 0.07226209342479706 + 1.0 * 6.0776166915893555
Epoch 750, val loss: 0.7925558090209961
Epoch 760, training loss: 6.1452107429504395 = 0.0683383122086525 + 1.0 * 6.0768723487854
Epoch 760, val loss: 0.7972764372825623
Epoch 770, training loss: 6.1481218338012695 = 0.06468581408262253 + 1.0 * 6.083436012268066
Epoch 770, val loss: 0.8020333647727966
Epoch 780, training loss: 6.142879009246826 = 0.06130216270685196 + 1.0 * 6.081576824188232
Epoch 780, val loss: 0.8067012429237366
Epoch 790, training loss: 6.133626461029053 = 0.05818367004394531 + 1.0 * 6.075442790985107
Epoch 790, val loss: 0.8116287589073181
Epoch 800, training loss: 6.127832889556885 = 0.05527811497449875 + 1.0 * 6.072554588317871
Epoch 800, val loss: 0.8164559602737427
Epoch 810, training loss: 6.125170707702637 = 0.0525662861764431 + 1.0 * 6.072604656219482
Epoch 810, val loss: 0.821304976940155
Epoch 820, training loss: 6.1235785484313965 = 0.050041213631629944 + 1.0 * 6.073537349700928
Epoch 820, val loss: 0.8260560631752014
Epoch 830, training loss: 6.120639324188232 = 0.04769385606050491 + 1.0 * 6.072945594787598
Epoch 830, val loss: 0.8309130668640137
Epoch 840, training loss: 6.116578102111816 = 0.045505523681640625 + 1.0 * 6.071072578430176
Epoch 840, val loss: 0.8356435298919678
Epoch 850, training loss: 6.113015174865723 = 0.043457116931676865 + 1.0 * 6.069558143615723
Epoch 850, val loss: 0.8404219150543213
Epoch 860, training loss: 6.1086297035217285 = 0.04154512286186218 + 1.0 * 6.067084789276123
Epoch 860, val loss: 0.8451597094535828
Epoch 870, training loss: 6.110032558441162 = 0.03975026682019234 + 1.0 * 6.070282459259033
Epoch 870, val loss: 0.8498959541320801
Epoch 880, training loss: 6.108832359313965 = 0.03807028383016586 + 1.0 * 6.0707621574401855
Epoch 880, val loss: 0.8543257713317871
Epoch 890, training loss: 6.101340293884277 = 0.03649983927607536 + 1.0 * 6.064840316772461
Epoch 890, val loss: 0.8590529561042786
Epoch 900, training loss: 6.105640411376953 = 0.03502197936177254 + 1.0 * 6.070618629455566
Epoch 900, val loss: 0.8635364770889282
Epoch 910, training loss: 6.097692012786865 = 0.03364383056759834 + 1.0 * 6.0640482902526855
Epoch 910, val loss: 0.8679412007331848
Epoch 920, training loss: 6.095102310180664 = 0.032338716089725494 + 1.0 * 6.062763690948486
Epoch 920, val loss: 0.872425377368927
Epoch 930, training loss: 6.092358112335205 = 0.031112492084503174 + 1.0 * 6.061245441436768
Epoch 930, val loss: 0.8768859505653381
Epoch 940, training loss: 6.097045421600342 = 0.02995103970170021 + 1.0 * 6.067094326019287
Epoch 940, val loss: 0.8812386989593506
Epoch 950, training loss: 6.092355728149414 = 0.028853461146354675 + 1.0 * 6.063502311706543
Epoch 950, val loss: 0.885367214679718
Epoch 960, training loss: 6.08854341506958 = 0.027822090312838554 + 1.0 * 6.060721397399902
Epoch 960, val loss: 0.8897269368171692
Epoch 970, training loss: 6.087763786315918 = 0.026842482388019562 + 1.0 * 6.0609211921691895
Epoch 970, val loss: 0.8939018845558167
Epoch 980, training loss: 6.085358142852783 = 0.025914808735251427 + 1.0 * 6.059443473815918
Epoch 980, val loss: 0.8979922533035278
Epoch 990, training loss: 6.0867438316345215 = 0.025040380656719208 + 1.0 * 6.061703681945801
Epoch 990, val loss: 0.9021187424659729
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.6827
Flip ASR: 0.6178/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.324053764343262 = 1.9502264261245728 + 1.0 * 8.37382698059082
Epoch 0, val loss: 1.9477009773254395
Epoch 10, training loss: 10.312626838684082 = 1.939568281173706 + 1.0 * 8.373058319091797
Epoch 10, val loss: 1.9363042116165161
Epoch 20, training loss: 10.294717788696289 = 1.9263339042663574 + 1.0 * 8.36838436126709
Epoch 20, val loss: 1.9210959672927856
Epoch 30, training loss: 10.250629425048828 = 1.9082930088043213 + 1.0 * 8.342336654663086
Epoch 30, val loss: 1.8996073007583618
Epoch 40, training loss: 10.079813957214355 = 1.8868273496627808 + 1.0 * 8.192986488342285
Epoch 40, val loss: 1.874772310256958
Epoch 50, training loss: 9.563334465026855 = 1.8654274940490723 + 1.0 * 7.697906970977783
Epoch 50, val loss: 1.8505511283874512
Epoch 60, training loss: 9.131229400634766 = 1.844407081604004 + 1.0 * 7.28682279586792
Epoch 60, val loss: 1.828270435333252
Epoch 70, training loss: 8.824822425842285 = 1.8277417421340942 + 1.0 * 6.997080326080322
Epoch 70, val loss: 1.8110336065292358
Epoch 80, training loss: 8.598942756652832 = 1.8109060525894165 + 1.0 * 6.788036823272705
Epoch 80, val loss: 1.7945122718811035
Epoch 90, training loss: 8.453545570373535 = 1.7944992780685425 + 1.0 * 6.659046649932861
Epoch 90, val loss: 1.7789875268936157
Epoch 100, training loss: 8.33224105834961 = 1.7788599729537964 + 1.0 * 6.553381443023682
Epoch 100, val loss: 1.7648755311965942
Epoch 110, training loss: 8.24699878692627 = 1.76343834400177 + 1.0 * 6.483560562133789
Epoch 110, val loss: 1.7509338855743408
Epoch 120, training loss: 8.17690658569336 = 1.7477246522903442 + 1.0 * 6.4291815757751465
Epoch 120, val loss: 1.7368671894073486
Epoch 130, training loss: 8.11938762664795 = 1.7311184406280518 + 1.0 * 6.388268947601318
Epoch 130, val loss: 1.722078561782837
Epoch 140, training loss: 8.066695213317871 = 1.7126315832138062 + 1.0 * 6.354063510894775
Epoch 140, val loss: 1.7060520648956299
Epoch 150, training loss: 8.01711368560791 = 1.6915372610092163 + 1.0 * 6.325576305389404
Epoch 150, val loss: 1.6883682012557983
Epoch 160, training loss: 7.972662448883057 = 1.6669350862503052 + 1.0 * 6.305727481842041
Epoch 160, val loss: 1.66817307472229
Epoch 170, training loss: 7.924966812133789 = 1.638096570968628 + 1.0 * 6.28687047958374
Epoch 170, val loss: 1.6449977159500122
Epoch 180, training loss: 7.876256942749023 = 1.604217529296875 + 1.0 * 6.272039413452148
Epoch 180, val loss: 1.6179983615875244
Epoch 190, training loss: 7.824028015136719 = 1.5646569728851318 + 1.0 * 6.259371280670166
Epoch 190, val loss: 1.5866280794143677
Epoch 200, training loss: 7.767605304718018 = 1.5190194845199585 + 1.0 * 6.2485857009887695
Epoch 200, val loss: 1.5504802465438843
Epoch 210, training loss: 7.707364082336426 = 1.467577576637268 + 1.0 * 6.239786624908447
Epoch 210, val loss: 1.5101491212844849
Epoch 220, training loss: 7.643056392669678 = 1.411814570426941 + 1.0 * 6.231241703033447
Epoch 220, val loss: 1.466761827468872
Epoch 230, training loss: 7.576302528381348 = 1.352782130241394 + 1.0 * 6.223520278930664
Epoch 230, val loss: 1.421218752861023
Epoch 240, training loss: 7.50886869430542 = 1.292019248008728 + 1.0 * 6.216849327087402
Epoch 240, val loss: 1.375117540359497
Epoch 250, training loss: 7.441409587860107 = 1.2311867475509644 + 1.0 * 6.2102227210998535
Epoch 250, val loss: 1.3293730020523071
Epoch 260, training loss: 7.377074241638184 = 1.1705538034439087 + 1.0 * 6.2065205574035645
Epoch 260, val loss: 1.284441351890564
Epoch 270, training loss: 7.31068754196167 = 1.1115564107894897 + 1.0 * 6.199131011962891
Epoch 270, val loss: 1.2408794164657593
Epoch 280, training loss: 7.246273517608643 = 1.0538991689682007 + 1.0 * 6.192374229431152
Epoch 280, val loss: 1.1985975503921509
Epoch 290, training loss: 7.189565181732178 = 0.9976111054420471 + 1.0 * 6.191954135894775
Epoch 290, val loss: 1.157278060913086
Epoch 300, training loss: 7.126286506652832 = 0.9432554244995117 + 1.0 * 6.18303108215332
Epoch 300, val loss: 1.1177160739898682
Epoch 310, training loss: 7.068087577819824 = 0.8908773064613342 + 1.0 * 6.177210330963135
Epoch 310, val loss: 1.0795962810516357
Epoch 320, training loss: 7.0190958976745605 = 0.8403196334838867 + 1.0 * 6.178776264190674
Epoch 320, val loss: 1.0430494546890259
Epoch 330, training loss: 6.962096214294434 = 0.7924764156341553 + 1.0 * 6.169620037078857
Epoch 330, val loss: 1.0084855556488037
Epoch 340, training loss: 6.911422252655029 = 0.7470961213111877 + 1.0 * 6.164326190948486
Epoch 340, val loss: 0.9758691787719727
Epoch 350, training loss: 6.864328384399414 = 0.7044009566307068 + 1.0 * 6.1599273681640625
Epoch 350, val loss: 0.9454506635665894
Epoch 360, training loss: 6.82108211517334 = 0.6646767854690552 + 1.0 * 6.156405448913574
Epoch 360, val loss: 0.91757732629776
Epoch 370, training loss: 6.779051780700684 = 0.6270010471343994 + 1.0 * 6.152050971984863
Epoch 370, val loss: 0.8915255069732666
Epoch 380, training loss: 6.740765571594238 = 0.5910689830780029 + 1.0 * 6.149696350097656
Epoch 380, val loss: 0.8674103021621704
Epoch 390, training loss: 6.70338249206543 = 0.5575045347213745 + 1.0 * 6.145877838134766
Epoch 390, val loss: 0.8464581370353699
Epoch 400, training loss: 6.668525695800781 = 0.526602566242218 + 1.0 * 6.141922950744629
Epoch 400, val loss: 0.8283172845840454
Epoch 410, training loss: 6.635595798492432 = 0.4974738657474518 + 1.0 * 6.138122081756592
Epoch 410, val loss: 0.8113817572593689
Epoch 420, training loss: 6.621710300445557 = 0.47007548809051514 + 1.0 * 6.151634693145752
Epoch 420, val loss: 0.7965026497840881
Epoch 430, training loss: 6.57820463180542 = 0.44458040595054626 + 1.0 * 6.133624076843262
Epoch 430, val loss: 0.7835512757301331
Epoch 440, training loss: 6.550941467285156 = 0.4207633137702942 + 1.0 * 6.130177974700928
Epoch 440, val loss: 0.7721267938613892
Epoch 450, training loss: 6.5254225730896 = 0.3982316553592682 + 1.0 * 6.127191066741943
Epoch 450, val loss: 0.7621000409126282
Epoch 460, training loss: 6.501552104949951 = 0.376793771982193 + 1.0 * 6.124758243560791
Epoch 460, val loss: 0.7531242966651917
Epoch 470, training loss: 6.484553813934326 = 0.3565012216567993 + 1.0 * 6.128052711486816
Epoch 470, val loss: 0.7456014752388
Epoch 480, training loss: 6.457932949066162 = 0.3374757468700409 + 1.0 * 6.120457172393799
Epoch 480, val loss: 0.7392342686653137
Epoch 490, training loss: 6.437833309173584 = 0.3195662796497345 + 1.0 * 6.118267059326172
Epoch 490, val loss: 0.7339703440666199
Epoch 500, training loss: 6.420467376708984 = 0.3027065098285675 + 1.0 * 6.11776065826416
Epoch 500, val loss: 0.7295979261398315
Epoch 510, training loss: 6.405066013336182 = 0.28692367672920227 + 1.0 * 6.118142127990723
Epoch 510, val loss: 0.726176381111145
Epoch 520, training loss: 6.383873462677002 = 0.272311270236969 + 1.0 * 6.111562252044678
Epoch 520, val loss: 0.7235386967658997
Epoch 530, training loss: 6.368658065795898 = 0.25859636068344116 + 1.0 * 6.1100616455078125
Epoch 530, val loss: 0.7216803431510925
Epoch 540, training loss: 6.359198570251465 = 0.24565836787223816 + 1.0 * 6.113540172576904
Epoch 540, val loss: 0.720488429069519
Epoch 550, training loss: 6.342027187347412 = 0.23347631096839905 + 1.0 * 6.108551025390625
Epoch 550, val loss: 0.7197237610816956
Epoch 560, training loss: 6.327313423156738 = 0.2219373732805252 + 1.0 * 6.105376243591309
Epoch 560, val loss: 0.7193888425827026
Epoch 570, training loss: 6.316042900085449 = 0.21088001132011414 + 1.0 * 6.105163097381592
Epoch 570, val loss: 0.7195635437965393
Epoch 580, training loss: 6.3021769523620605 = 0.20027665793895721 + 1.0 * 6.101900100708008
Epoch 580, val loss: 0.7200353741645813
Epoch 590, training loss: 6.29378604888916 = 0.19002532958984375 + 1.0 * 6.103760719299316
Epoch 590, val loss: 0.7206605672836304
Epoch 600, training loss: 6.279754161834717 = 0.18002696335315704 + 1.0 * 6.099727153778076
Epoch 600, val loss: 0.7215194702148438
Epoch 610, training loss: 6.2673492431640625 = 0.17032994329929352 + 1.0 * 6.097019195556641
Epoch 610, val loss: 0.7224566340446472
Epoch 620, training loss: 6.254926681518555 = 0.16079887747764587 + 1.0 * 6.094127655029297
Epoch 620, val loss: 0.7234818339347839
Epoch 630, training loss: 6.244912624359131 = 0.15144895017147064 + 1.0 * 6.093463897705078
Epoch 630, val loss: 0.7246377468109131
Epoch 640, training loss: 6.238008975982666 = 0.1423531323671341 + 1.0 * 6.095655918121338
Epoch 640, val loss: 0.7258287668228149
Epoch 650, training loss: 6.226701736450195 = 0.13359981775283813 + 1.0 * 6.093101978302002
Epoch 650, val loss: 0.7270530462265015
Epoch 660, training loss: 6.2186760902404785 = 0.1251809448003769 + 1.0 * 6.0934953689575195
Epoch 660, val loss: 0.7284657955169678
Epoch 670, training loss: 6.204892158508301 = 0.11714380234479904 + 1.0 * 6.0877485275268555
Epoch 670, val loss: 0.7300376296043396
Epoch 680, training loss: 6.195768356323242 = 0.10952404886484146 + 1.0 * 6.086244106292725
Epoch 680, val loss: 0.7318663597106934
Epoch 690, training loss: 6.189505100250244 = 0.10235252231359482 + 1.0 * 6.087152481079102
Epoch 690, val loss: 0.7338793277740479
Epoch 700, training loss: 6.181941986083984 = 0.09563980251550674 + 1.0 * 6.086302280426025
Epoch 700, val loss: 0.7361558675765991
Epoch 710, training loss: 6.174626350402832 = 0.08945786952972412 + 1.0 * 6.085168361663818
Epoch 710, val loss: 0.7385140657424927
Epoch 720, training loss: 6.166456699371338 = 0.08372791856527328 + 1.0 * 6.082728862762451
Epoch 720, val loss: 0.7412262558937073
Epoch 730, training loss: 6.162646770477295 = 0.07844728231430054 + 1.0 * 6.08419942855835
Epoch 730, val loss: 0.7440710067749023
Epoch 740, training loss: 6.153106212615967 = 0.07359995692968369 + 1.0 * 6.0795063972473145
Epoch 740, val loss: 0.7470369935035706
Epoch 750, training loss: 6.146881580352783 = 0.06912308186292648 + 1.0 * 6.077758312225342
Epoch 750, val loss: 0.7502058148384094
Epoch 760, training loss: 6.144408702850342 = 0.0649867132306099 + 1.0 * 6.0794219970703125
Epoch 760, val loss: 0.7534703016281128
Epoch 770, training loss: 6.143033504486084 = 0.061221685260534286 + 1.0 * 6.081811904907227
Epoch 770, val loss: 0.7567813992500305
Epoch 780, training loss: 6.131540298461914 = 0.05778384953737259 + 1.0 * 6.073756217956543
Epoch 780, val loss: 0.7601332068443298
Epoch 790, training loss: 6.127009868621826 = 0.05461440607905388 + 1.0 * 6.072395324707031
Epoch 790, val loss: 0.7636635899543762
Epoch 800, training loss: 6.1230902671813965 = 0.051685381680727005 + 1.0 * 6.071404933929443
Epoch 800, val loss: 0.7672511339187622
Epoch 810, training loss: 6.129712104797363 = 0.048977624624967575 + 1.0 * 6.0807342529296875
Epoch 810, val loss: 0.7708591222763062
Epoch 820, training loss: 6.118983268737793 = 0.04647745564579964 + 1.0 * 6.072505950927734
Epoch 820, val loss: 0.7744702696800232
Epoch 830, training loss: 6.113897800445557 = 0.04418044909834862 + 1.0 * 6.0697174072265625
Epoch 830, val loss: 0.7780521512031555
Epoch 840, training loss: 6.109766960144043 = 0.042046334594488144 + 1.0 * 6.067720413208008
Epoch 840, val loss: 0.7816995978355408
Epoch 850, training loss: 6.111616611480713 = 0.04005562514066696 + 1.0 * 6.071560859680176
Epoch 850, val loss: 0.7853886485099792
Epoch 860, training loss: 6.113306522369385 = 0.038213152438402176 + 1.0 * 6.0750932693481445
Epoch 860, val loss: 0.7889642119407654
Epoch 870, training loss: 6.102223873138428 = 0.036508895456790924 + 1.0 * 6.0657148361206055
Epoch 870, val loss: 0.7925033569335938
Epoch 880, training loss: 6.099070072174072 = 0.034912724047899246 + 1.0 * 6.064157485961914
Epoch 880, val loss: 0.7960681915283203
Epoch 890, training loss: 6.0953049659729 = 0.033419087529182434 + 1.0 * 6.061885833740234
Epoch 890, val loss: 0.7996227741241455
Epoch 900, training loss: 6.093177795410156 = 0.03201441839337349 + 1.0 * 6.061163425445557
Epoch 900, val loss: 0.8031642436981201
Epoch 910, training loss: 6.10072660446167 = 0.030701374635100365 + 1.0 * 6.070025444030762
Epoch 910, val loss: 0.8066502213478088
Epoch 920, training loss: 6.092111587524414 = 0.02947205677628517 + 1.0 * 6.0626397132873535
Epoch 920, val loss: 0.8100292682647705
Epoch 930, training loss: 6.086884021759033 = 0.028322959318757057 + 1.0 * 6.058560848236084
Epoch 930, val loss: 0.8134136199951172
Epoch 940, training loss: 6.085859775543213 = 0.027240367606282234 + 1.0 * 6.058619499206543
Epoch 940, val loss: 0.8167977333068848
Epoch 950, training loss: 6.09100866317749 = 0.026221297681331635 + 1.0 * 6.0647873878479
Epoch 950, val loss: 0.820138692855835
Epoch 960, training loss: 6.084048748016357 = 0.025251196697354317 + 1.0 * 6.058797359466553
Epoch 960, val loss: 0.8233861327171326
Epoch 970, training loss: 6.081570625305176 = 0.02434387430548668 + 1.0 * 6.057226657867432
Epoch 970, val loss: 0.8266392946243286
Epoch 980, training loss: 6.0807085037231445 = 0.023479146882891655 + 1.0 * 6.057229518890381
Epoch 980, val loss: 0.8298764824867249
Epoch 990, training loss: 6.07652473449707 = 0.02266666106879711 + 1.0 * 6.053858280181885
Epoch 990, val loss: 0.8330413699150085
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8044
Flip ASR: 0.7644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.321874618530273 = 1.9480445384979248 + 1.0 * 8.37382984161377
Epoch 0, val loss: 1.943068027496338
Epoch 10, training loss: 10.31058406829834 = 1.9373447895050049 + 1.0 * 8.373239517211914
Epoch 10, val loss: 1.9323238134384155
Epoch 20, training loss: 10.292654037475586 = 1.92360520362854 + 1.0 * 8.369049072265625
Epoch 20, val loss: 1.9185386896133423
Epoch 30, training loss: 10.24435806274414 = 1.9043831825256348 + 1.0 * 8.339974403381348
Epoch 30, val loss: 1.8994721174240112
Epoch 40, training loss: 10.032190322875977 = 1.8809760808944702 + 1.0 * 8.151214599609375
Epoch 40, val loss: 1.8771185874938965
Epoch 50, training loss: 9.59998893737793 = 1.85432767868042 + 1.0 * 7.745660781860352
Epoch 50, val loss: 1.8521360158920288
Epoch 60, training loss: 9.306564331054688 = 1.831891655921936 + 1.0 * 7.474672317504883
Epoch 60, val loss: 1.832131266593933
Epoch 70, training loss: 8.890119552612305 = 1.815539836883545 + 1.0 * 7.074580192565918
Epoch 70, val loss: 1.8177281618118286
Epoch 80, training loss: 8.622396469116211 = 1.8026187419891357 + 1.0 * 6.819777965545654
Epoch 80, val loss: 1.806205153465271
Epoch 90, training loss: 8.435956001281738 = 1.7871114015579224 + 1.0 * 6.6488447189331055
Epoch 90, val loss: 1.792744517326355
Epoch 100, training loss: 8.30801010131836 = 1.7710323333740234 + 1.0 * 6.536978244781494
Epoch 100, val loss: 1.7788012027740479
Epoch 110, training loss: 8.211332321166992 = 1.7547035217285156 + 1.0 * 6.456629276275635
Epoch 110, val loss: 1.7642935514450073
Epoch 120, training loss: 8.136165618896484 = 1.7371058464050293 + 1.0 * 6.399059295654297
Epoch 120, val loss: 1.7484904527664185
Epoch 130, training loss: 8.076035499572754 = 1.7170168161392212 + 1.0 * 6.359018325805664
Epoch 130, val loss: 1.7305980920791626
Epoch 140, training loss: 8.021347999572754 = 1.6935207843780518 + 1.0 * 6.327827453613281
Epoch 140, val loss: 1.710081696510315
Epoch 150, training loss: 7.96807336807251 = 1.665951132774353 + 1.0 * 6.302122116088867
Epoch 150, val loss: 1.6863806247711182
Epoch 160, training loss: 7.917375564575195 = 1.634048581123352 + 1.0 * 6.283327102661133
Epoch 160, val loss: 1.6592391729354858
Epoch 170, training loss: 7.861955642700195 = 1.5982615947723389 + 1.0 * 6.263693809509277
Epoch 170, val loss: 1.6288731098175049
Epoch 180, training loss: 7.805706977844238 = 1.5577336549758911 + 1.0 * 6.247973442077637
Epoch 180, val loss: 1.5946285724639893
Epoch 190, training loss: 7.746812343597412 = 1.5122548341751099 + 1.0 * 6.234557628631592
Epoch 190, val loss: 1.5562762022018433
Epoch 200, training loss: 7.6880388259887695 = 1.4625993967056274 + 1.0 * 6.225439548492432
Epoch 200, val loss: 1.514633059501648
Epoch 210, training loss: 7.623908042907715 = 1.4096543788909912 + 1.0 * 6.214253902435303
Epoch 210, val loss: 1.47067391872406
Epoch 220, training loss: 7.5616984367370605 = 1.3538414239883423 + 1.0 * 6.207857131958008
Epoch 220, val loss: 1.424974799156189
Epoch 230, training loss: 7.4948906898498535 = 1.2966192960739136 + 1.0 * 6.19827127456665
Epoch 230, val loss: 1.378610610961914
Epoch 240, training loss: 7.429666519165039 = 1.2381956577301025 + 1.0 * 6.191471099853516
Epoch 240, val loss: 1.3319261074066162
Epoch 250, training loss: 7.365315914154053 = 1.1790374517440796 + 1.0 * 6.186278343200684
Epoch 250, val loss: 1.2851802110671997
Epoch 260, training loss: 7.300683498382568 = 1.1197433471679688 + 1.0 * 6.1809401512146
Epoch 260, val loss: 1.2386934757232666
Epoch 270, training loss: 7.23703670501709 = 1.0606491565704346 + 1.0 * 6.176387786865234
Epoch 270, val loss: 1.1927509307861328
Epoch 280, training loss: 7.176574230194092 = 1.002386450767517 + 1.0 * 6.174187660217285
Epoch 280, val loss: 1.1476635932922363
Epoch 290, training loss: 7.1124653816223145 = 0.9458391070365906 + 1.0 * 6.166626453399658
Epoch 290, val loss: 1.1041390895843506
Epoch 300, training loss: 7.054031848907471 = 0.8909183740615845 + 1.0 * 6.163113594055176
Epoch 300, val loss: 1.0622155666351318
Epoch 310, training loss: 6.998882293701172 = 0.8381584882736206 + 1.0 * 6.160723686218262
Epoch 310, val loss: 1.022141933441162
Epoch 320, training loss: 6.944090366363525 = 0.7880651354789734 + 1.0 * 6.156025409698486
Epoch 320, val loss: 0.9844008684158325
Epoch 330, training loss: 6.892404556274414 = 0.7406031489372253 + 1.0 * 6.151801586151123
Epoch 330, val loss: 0.9489904046058655
Epoch 340, training loss: 6.8516974449157715 = 0.6957296133041382 + 1.0 * 6.155967712402344
Epoch 340, val loss: 0.9159412980079651
Epoch 350, training loss: 6.799590110778809 = 0.6538906693458557 + 1.0 * 6.145699501037598
Epoch 350, val loss: 0.885563313961029
Epoch 360, training loss: 6.756900310516357 = 0.6146916151046753 + 1.0 * 6.142208576202393
Epoch 360, val loss: 0.8577800989151001
Epoch 370, training loss: 6.722630977630615 = 0.5779613852500916 + 1.0 * 6.144669532775879
Epoch 370, val loss: 0.8324383497238159
Epoch 380, training loss: 6.680908679962158 = 0.5439505577087402 + 1.0 * 6.136958122253418
Epoch 380, val loss: 0.8098011016845703
Epoch 390, training loss: 6.645729064941406 = 0.5120975971221924 + 1.0 * 6.133631706237793
Epoch 390, val loss: 0.7895638942718506
Epoch 400, training loss: 6.61372709274292 = 0.4820718765258789 + 1.0 * 6.131655216217041
Epoch 400, val loss: 0.7713429927825928
Epoch 410, training loss: 6.5916428565979 = 0.45392921566963196 + 1.0 * 6.137713432312012
Epoch 410, val loss: 0.7550898194313049
Epoch 420, training loss: 6.553091049194336 = 0.42758670449256897 + 1.0 * 6.125504493713379
Epoch 420, val loss: 0.7407821416854858
Epoch 430, training loss: 6.5250678062438965 = 0.4025384783744812 + 1.0 * 6.12252950668335
Epoch 430, val loss: 0.7280404567718506
Epoch 440, training loss: 6.498262405395508 = 0.3785914182662964 + 1.0 * 6.119670867919922
Epoch 440, val loss: 0.7166396975517273
Epoch 450, training loss: 6.486032962799072 = 0.3557117283344269 + 1.0 * 6.130321025848389
Epoch 450, val loss: 0.706478476524353
Epoch 460, training loss: 6.450136661529541 = 0.33427056670188904 + 1.0 * 6.115866184234619
Epoch 460, val loss: 0.697709858417511
Epoch 470, training loss: 6.4280781745910645 = 0.3139207661151886 + 1.0 * 6.114157199859619
Epoch 470, val loss: 0.6902123689651489
Epoch 480, training loss: 6.405304908752441 = 0.2945597767829895 + 1.0 * 6.110744953155518
Epoch 480, val loss: 0.6838253736495972
Epoch 490, training loss: 6.389377117156982 = 0.276195228099823 + 1.0 * 6.113182067871094
Epoch 490, val loss: 0.678484320640564
Epoch 500, training loss: 6.3693623542785645 = 0.25891056656837463 + 1.0 * 6.110451698303223
Epoch 500, val loss: 0.6741912364959717
Epoch 510, training loss: 6.347346305847168 = 0.2426537573337555 + 1.0 * 6.104692459106445
Epoch 510, val loss: 0.6708457469940186
Epoch 520, training loss: 6.3381171226501465 = 0.2273540049791336 + 1.0 * 6.110763072967529
Epoch 520, val loss: 0.6684972047805786
Epoch 530, training loss: 6.315364360809326 = 0.21313031017780304 + 1.0 * 6.10223388671875
Epoch 530, val loss: 0.6670575141906738
Epoch 540, training loss: 6.297718048095703 = 0.19979597628116608 + 1.0 * 6.097921848297119
Epoch 540, val loss: 0.6664368510246277
Epoch 550, training loss: 6.287701606750488 = 0.1872866153717041 + 1.0 * 6.100414752960205
Epoch 550, val loss: 0.6666041016578674
Epoch 560, training loss: 6.279054641723633 = 0.17562933266162872 + 1.0 * 6.1034255027771
Epoch 560, val loss: 0.6675091981887817
Epoch 570, training loss: 6.259893417358398 = 0.16486859321594238 + 1.0 * 6.095025062561035
Epoch 570, val loss: 0.6690106391906738
Epoch 580, training loss: 6.246884346008301 = 0.15483210980892181 + 1.0 * 6.092052459716797
Epoch 580, val loss: 0.6711089015007019
Epoch 590, training loss: 6.2397003173828125 = 0.14546526968479156 + 1.0 * 6.094234943389893
Epoch 590, val loss: 0.6737508177757263
Epoch 600, training loss: 6.226849555969238 = 0.13673998415470123 + 1.0 * 6.090109348297119
Epoch 600, val loss: 0.6768509745597839
Epoch 610, training loss: 6.220716953277588 = 0.12862758338451385 + 1.0 * 6.0920891761779785
Epoch 610, val loss: 0.6803585290908813
Epoch 620, training loss: 6.208359241485596 = 0.121107317507267 + 1.0 * 6.087252140045166
Epoch 620, val loss: 0.6841920614242554
Epoch 630, training loss: 6.199105262756348 = 0.11412006616592407 + 1.0 * 6.084985256195068
Epoch 630, val loss: 0.6883417367935181
Epoch 640, training loss: 6.192130088806152 = 0.10763417929410934 + 1.0 * 6.084496021270752
Epoch 640, val loss: 0.6927451491355896
Epoch 650, training loss: 6.181948184967041 = 0.10163190960884094 + 1.0 * 6.080316066741943
Epoch 650, val loss: 0.6972852945327759
Epoch 660, training loss: 6.175622463226318 = 0.09603547304868698 + 1.0 * 6.079586982727051
Epoch 660, val loss: 0.701996386051178
Epoch 670, training loss: 6.173139572143555 = 0.09080992639064789 + 1.0 * 6.082329750061035
Epoch 670, val loss: 0.7068457007408142
Epoch 680, training loss: 6.167852878570557 = 0.08597014099359512 + 1.0 * 6.081882953643799
Epoch 680, val loss: 0.7117771506309509
Epoch 690, training loss: 6.156723976135254 = 0.08148244023323059 + 1.0 * 6.075241565704346
Epoch 690, val loss: 0.7167565822601318
Epoch 700, training loss: 6.149781703948975 = 0.07728607952594757 + 1.0 * 6.072495460510254
Epoch 700, val loss: 0.7218005061149597
Epoch 710, training loss: 6.147725582122803 = 0.0733659416437149 + 1.0 * 6.07435941696167
Epoch 710, val loss: 0.7268769145011902
Epoch 720, training loss: 6.146175384521484 = 0.069719597697258 + 1.0 * 6.076455593109131
Epoch 720, val loss: 0.7319290041923523
Epoch 730, training loss: 6.1358208656311035 = 0.06633694469928741 + 1.0 * 6.069483757019043
Epoch 730, val loss: 0.736946165561676
Epoch 740, training loss: 6.131618499755859 = 0.06317400932312012 + 1.0 * 6.06844425201416
Epoch 740, val loss: 0.7419834136962891
Epoch 750, training loss: 6.131340026855469 = 0.06020873039960861 + 1.0 * 6.071131229400635
Epoch 750, val loss: 0.7470188140869141
Epoch 760, training loss: 6.124312877655029 = 0.05742645636200905 + 1.0 * 6.0668864250183105
Epoch 760, val loss: 0.7520254254341125
Epoch 770, training loss: 6.1252641677856445 = 0.05483568459749222 + 1.0 * 6.070428371429443
Epoch 770, val loss: 0.7569732069969177
Epoch 780, training loss: 6.117996692657471 = 0.052400894463062286 + 1.0 * 6.065595626831055
Epoch 780, val loss: 0.7619056701660156
Epoch 790, training loss: 6.115715026855469 = 0.050122588872909546 + 1.0 * 6.065592288970947
Epoch 790, val loss: 0.7668269872665405
Epoch 800, training loss: 6.111123561859131 = 0.047991614788770676 + 1.0 * 6.063131809234619
Epoch 800, val loss: 0.7716942429542542
Epoch 810, training loss: 6.1062164306640625 = 0.045972682535648346 + 1.0 * 6.060243606567383
Epoch 810, val loss: 0.7765612602233887
Epoch 820, training loss: 6.105980396270752 = 0.04406763240695 + 1.0 * 6.061912536621094
Epoch 820, val loss: 0.7814109325408936
Epoch 830, training loss: 6.107173919677734 = 0.04228180646896362 + 1.0 * 6.064892292022705
Epoch 830, val loss: 0.7862184643745422
Epoch 840, training loss: 6.099092483520508 = 0.040593717247247696 + 1.0 * 6.058498859405518
Epoch 840, val loss: 0.7909772396087646
Epoch 850, training loss: 6.096606731414795 = 0.03899846971035004 + 1.0 * 6.057608127593994
Epoch 850, val loss: 0.7956752181053162
Epoch 860, training loss: 6.120467662811279 = 0.03749319538474083 + 1.0 * 6.082974433898926
Epoch 860, val loss: 0.8003588318824768
Epoch 870, training loss: 6.091374397277832 = 0.0360904186964035 + 1.0 * 6.055284023284912
Epoch 870, val loss: 0.804992139339447
Epoch 880, training loss: 6.088551998138428 = 0.03476359695196152 + 1.0 * 6.053788185119629
Epoch 880, val loss: 0.8095028400421143
Epoch 890, training loss: 6.086322784423828 = 0.03349682688713074 + 1.0 * 6.052825927734375
Epoch 890, val loss: 0.8140023946762085
Epoch 900, training loss: 6.084110736846924 = 0.03229021653532982 + 1.0 * 6.051820755004883
Epoch 900, val loss: 0.8184627294540405
Epoch 910, training loss: 6.084118843078613 = 0.031139185652136803 + 1.0 * 6.052979469299316
Epoch 910, val loss: 0.8228780031204224
Epoch 920, training loss: 6.081485271453857 = 0.030052771791815758 + 1.0 * 6.0514326095581055
Epoch 920, val loss: 0.8272744417190552
Epoch 930, training loss: 6.083122253417969 = 0.029027041047811508 + 1.0 * 6.054095268249512
Epoch 930, val loss: 0.8315781354904175
Epoch 940, training loss: 6.078609466552734 = 0.02805161662399769 + 1.0 * 6.050557613372803
Epoch 940, val loss: 0.8358427882194519
Epoch 950, training loss: 6.075724124908447 = 0.027119934558868408 + 1.0 * 6.0486040115356445
Epoch 950, val loss: 0.8400678634643555
Epoch 960, training loss: 6.076812267303467 = 0.02623109519481659 + 1.0 * 6.050580978393555
Epoch 960, val loss: 0.8442556262016296
Epoch 970, training loss: 6.07757043838501 = 0.025388142094016075 + 1.0 * 6.052182197570801
Epoch 970, val loss: 0.8484081029891968
Epoch 980, training loss: 6.073132514953613 = 0.024584803730249405 + 1.0 * 6.048547744750977
Epoch 980, val loss: 0.8524793386459351
Epoch 990, training loss: 6.074309825897217 = 0.02382303588092327 + 1.0 * 6.0504865646362305
Epoch 990, val loss: 0.8565019369125366
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8118
Flip ASR: 0.7733/225 nodes
The final ASR:0.76630, 0.05922, Accuracy:0.80494, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.316375732421875 = 1.9425495862960815 + 1.0 * 8.373826026916504
Epoch 0, val loss: 1.937678337097168
Epoch 10, training loss: 10.305279731750488 = 1.931864619255066 + 1.0 * 8.373414993286133
Epoch 10, val loss: 1.9277528524398804
Epoch 20, training loss: 10.289451599121094 = 1.9187369346618652 + 1.0 * 8.370715141296387
Epoch 20, val loss: 1.915094017982483
Epoch 30, training loss: 10.253427505493164 = 1.9001150131225586 + 1.0 * 8.353312492370605
Epoch 30, val loss: 1.8967504501342773
Epoch 40, training loss: 10.130195617675781 = 1.8752226829528809 + 1.0 * 8.254973411560059
Epoch 40, val loss: 1.8731813430786133
Epoch 50, training loss: 9.658919334411621 = 1.84902024269104 + 1.0 * 7.809898853302002
Epoch 50, val loss: 1.8489861488342285
Epoch 60, training loss: 9.297957420349121 = 1.8244516849517822 + 1.0 * 7.473505973815918
Epoch 60, val loss: 1.8270989656448364
Epoch 70, training loss: 8.885112762451172 = 1.8083865642547607 + 1.0 * 7.07672643661499
Epoch 70, val loss: 1.8138203620910645
Epoch 80, training loss: 8.568732261657715 = 1.7967246770858765 + 1.0 * 6.772007942199707
Epoch 80, val loss: 1.8043413162231445
Epoch 90, training loss: 8.403401374816895 = 1.7831753492355347 + 1.0 * 6.6202263832092285
Epoch 90, val loss: 1.7929543256759644
Epoch 100, training loss: 8.305243492126465 = 1.7660773992538452 + 1.0 * 6.53916597366333
Epoch 100, val loss: 1.7786805629730225
Epoch 110, training loss: 8.222211837768555 = 1.7483458518981934 + 1.0 * 6.473865509033203
Epoch 110, val loss: 1.7637858390808105
Epoch 120, training loss: 8.152236938476562 = 1.729590892791748 + 1.0 * 6.422645568847656
Epoch 120, val loss: 1.74794340133667
Epoch 130, training loss: 8.087302207946777 = 1.7080360651016235 + 1.0 * 6.379265785217285
Epoch 130, val loss: 1.7299368381500244
Epoch 140, training loss: 8.026623725891113 = 1.6830284595489502 + 1.0 * 6.343595504760742
Epoch 140, val loss: 1.7093709707260132
Epoch 150, training loss: 7.969350814819336 = 1.6535422801971436 + 1.0 * 6.315808296203613
Epoch 150, val loss: 1.6854480504989624
Epoch 160, training loss: 7.913680076599121 = 1.6187299489974976 + 1.0 * 6.294950008392334
Epoch 160, val loss: 1.6572208404541016
Epoch 170, training loss: 7.855305194854736 = 1.5788822174072266 + 1.0 * 6.27642297744751
Epoch 170, val loss: 1.6249208450317383
Epoch 180, training loss: 7.795490264892578 = 1.5345180034637451 + 1.0 * 6.260972499847412
Epoch 180, val loss: 1.5889124870300293
Epoch 190, training loss: 7.738481044769287 = 1.4865015745162964 + 1.0 * 6.251979351043701
Epoch 190, val loss: 1.5503737926483154
Epoch 200, training loss: 7.6759233474731445 = 1.4378526210784912 + 1.0 * 6.238070964813232
Epoch 200, val loss: 1.5118374824523926
Epoch 210, training loss: 7.615738868713379 = 1.3890292644500732 + 1.0 * 6.226709365844727
Epoch 210, val loss: 1.4736775159835815
Epoch 220, training loss: 7.558955192565918 = 1.340698003768921 + 1.0 * 6.218256950378418
Epoch 220, val loss: 1.4367848634719849
Epoch 230, training loss: 7.504366874694824 = 1.293805718421936 + 1.0 * 6.210561275482178
Epoch 230, val loss: 1.401831865310669
Epoch 240, training loss: 7.453370571136475 = 1.2483725547790527 + 1.0 * 6.204998016357422
Epoch 240, val loss: 1.3685777187347412
Epoch 250, training loss: 7.400963306427002 = 1.2045232057571411 + 1.0 * 6.19644021987915
Epoch 250, val loss: 1.3370858430862427
Epoch 260, training loss: 7.352475166320801 = 1.1614099740982056 + 1.0 * 6.191065311431885
Epoch 260, val loss: 1.3065567016601562
Epoch 270, training loss: 7.304163455963135 = 1.1187772750854492 + 1.0 * 6.1853861808776855
Epoch 270, val loss: 1.2764968872070312
Epoch 280, training loss: 7.257631301879883 = 1.0762693881988525 + 1.0 * 6.181362152099609
Epoch 280, val loss: 1.2466332912445068
Epoch 290, training loss: 7.208317756652832 = 1.033825159072876 + 1.0 * 6.174492835998535
Epoch 290, val loss: 1.216740369796753
Epoch 300, training loss: 7.1608991622924805 = 0.9912348389625549 + 1.0 * 6.16966438293457
Epoch 300, val loss: 1.1865124702453613
Epoch 310, training loss: 7.117331027984619 = 0.9485947489738464 + 1.0 * 6.168736457824707
Epoch 310, val loss: 1.1560630798339844
Epoch 320, training loss: 7.067986488342285 = 0.9064992666244507 + 1.0 * 6.161487102508545
Epoch 320, val loss: 1.1258529424667358
Epoch 330, training loss: 7.027894973754883 = 0.8648688793182373 + 1.0 * 6.163026332855225
Epoch 330, val loss: 1.0961053371429443
Epoch 340, training loss: 6.981145858764648 = 0.8245909214019775 + 1.0 * 6.156554698944092
Epoch 340, val loss: 1.0672669410705566
Epoch 350, training loss: 6.936859607696533 = 0.7855274081230164 + 1.0 * 6.151332378387451
Epoch 350, val loss: 1.0398424863815308
Epoch 360, training loss: 6.89588737487793 = 0.7475641965866089 + 1.0 * 6.148323059082031
Epoch 360, val loss: 1.013628602027893
Epoch 370, training loss: 6.859714984893799 = 0.7110961079597473 + 1.0 * 6.148618698120117
Epoch 370, val loss: 0.9889673590660095
Epoch 380, training loss: 6.818999767303467 = 0.6761270761489868 + 1.0 * 6.1428728103637695
Epoch 380, val loss: 0.9661417603492737
Epoch 390, training loss: 6.781899452209473 = 0.6423425674438477 + 1.0 * 6.139556884765625
Epoch 390, val loss: 0.9447692632675171
Epoch 400, training loss: 6.75214147567749 = 0.609983503818512 + 1.0 * 6.142158031463623
Epoch 400, val loss: 0.924835741519928
Epoch 410, training loss: 6.71492338180542 = 0.5792398452758789 + 1.0 * 6.135683536529541
Epoch 410, val loss: 0.9067171216011047
Epoch 420, training loss: 6.681095600128174 = 0.5497899055480957 + 1.0 * 6.131305694580078
Epoch 420, val loss: 0.8900750279426575
Epoch 430, training loss: 6.652278900146484 = 0.5215827822685242 + 1.0 * 6.1306962966918945
Epoch 430, val loss: 0.87477707862854
Epoch 440, training loss: 6.62777042388916 = 0.4949284791946411 + 1.0 * 6.132842063903809
Epoch 440, val loss: 0.8609687685966492
Epoch 450, training loss: 6.595042705535889 = 0.4698573648929596 + 1.0 * 6.125185489654541
Epoch 450, val loss: 0.8488141298294067
Epoch 460, training loss: 6.569267272949219 = 0.4460805654525757 + 1.0 * 6.1231865882873535
Epoch 460, val loss: 0.838079571723938
Epoch 470, training loss: 6.543214797973633 = 0.4235592782497406 + 1.0 * 6.119655609130859
Epoch 470, val loss: 0.8286116719245911
Epoch 480, training loss: 6.519084930419922 = 0.4022672176361084 + 1.0 * 6.116817474365234
Epoch 480, val loss: 0.8205054402351379
Epoch 490, training loss: 6.500056266784668 = 0.3821243941783905 + 1.0 * 6.117931842803955
Epoch 490, val loss: 0.8135649561882019
Epoch 500, training loss: 6.477668285369873 = 0.3632030487060547 + 1.0 * 6.114465236663818
Epoch 500, val loss: 0.807681143283844
Epoch 510, training loss: 6.456532001495361 = 0.3454311788082123 + 1.0 * 6.111100673675537
Epoch 510, val loss: 0.8030461072921753
Epoch 520, training loss: 6.443592548370361 = 0.32871997356414795 + 1.0 * 6.114872455596924
Epoch 520, val loss: 0.7993866801261902
Epoch 530, training loss: 6.424175262451172 = 0.31314417719841003 + 1.0 * 6.1110310554504395
Epoch 530, val loss: 0.7965375781059265
Epoch 540, training loss: 6.404325485229492 = 0.29855847358703613 + 1.0 * 6.105767250061035
Epoch 540, val loss: 0.7947380542755127
Epoch 550, training loss: 6.387490272521973 = 0.2847917079925537 + 1.0 * 6.102698802947998
Epoch 550, val loss: 0.7936649322509766
Epoch 560, training loss: 6.379235744476318 = 0.27172836661338806 + 1.0 * 6.107507228851318
Epoch 560, val loss: 0.7932351231575012
Epoch 570, training loss: 6.36487340927124 = 0.25930488109588623 + 1.0 * 6.1055684089660645
Epoch 570, val loss: 0.7933713793754578
Epoch 580, training loss: 6.348625183105469 = 0.24745436012744904 + 1.0 * 6.101171016693115
Epoch 580, val loss: 0.793809711933136
Epoch 590, training loss: 6.33114767074585 = 0.236001119017601 + 1.0 * 6.095146656036377
Epoch 590, val loss: 0.7947494983673096
Epoch 600, training loss: 6.317962169647217 = 0.22474053502082825 + 1.0 * 6.093221664428711
Epoch 600, val loss: 0.7959640026092529
Epoch 610, training loss: 6.305305480957031 = 0.2135547250509262 + 1.0 * 6.091750621795654
Epoch 610, val loss: 0.797420859336853
Epoch 620, training loss: 6.292973041534424 = 0.20240755379199982 + 1.0 * 6.0905656814575195
Epoch 620, val loss: 0.7989787459373474
Epoch 630, training loss: 6.287272930145264 = 0.19130434095859528 + 1.0 * 6.095968723297119
Epoch 630, val loss: 0.800677478313446
Epoch 640, training loss: 6.269172191619873 = 0.18023724853992462 + 1.0 * 6.088934898376465
Epoch 640, val loss: 0.8024935126304626
Epoch 650, training loss: 6.254769802093506 = 0.16921772062778473 + 1.0 * 6.085552215576172
Epoch 650, val loss: 0.8045114874839783
Epoch 660, training loss: 6.242377281188965 = 0.1583436131477356 + 1.0 * 6.084033489227295
Epoch 660, val loss: 0.806764543056488
Epoch 670, training loss: 6.251474857330322 = 0.14778916537761688 + 1.0 * 6.1036858558654785
Epoch 670, val loss: 0.8092097043991089
Epoch 680, training loss: 6.222235679626465 = 0.13784849643707275 + 1.0 * 6.084387302398682
Epoch 680, val loss: 0.8118498921394348
Epoch 690, training loss: 6.208925724029541 = 0.12850219011306763 + 1.0 * 6.080423355102539
Epoch 690, val loss: 0.8149349689483643
Epoch 700, training loss: 6.198943138122559 = 0.11978308856487274 + 1.0 * 6.079160213470459
Epoch 700, val loss: 0.8183996677398682
Epoch 710, training loss: 6.197697162628174 = 0.11173392832279205 + 1.0 * 6.085963249206543
Epoch 710, val loss: 0.8221641778945923
Epoch 720, training loss: 6.181674957275391 = 0.1043870821595192 + 1.0 * 6.077287673950195
Epoch 720, val loss: 0.8262341022491455
Epoch 730, training loss: 6.17453670501709 = 0.09765762835741043 + 1.0 * 6.076879024505615
Epoch 730, val loss: 0.8306116461753845
Epoch 740, training loss: 6.16871452331543 = 0.09151716530323029 + 1.0 * 6.077197551727295
Epoch 740, val loss: 0.8352380990982056
Epoch 750, training loss: 6.162557601928711 = 0.08591658622026443 + 1.0 * 6.076641082763672
Epoch 750, val loss: 0.8401046991348267
Epoch 760, training loss: 6.152403831481934 = 0.08080895990133286 + 1.0 * 6.071594715118408
Epoch 760, val loss: 0.8451571464538574
Epoch 770, training loss: 6.148791790008545 = 0.07612702250480652 + 1.0 * 6.072664737701416
Epoch 770, val loss: 0.8504220843315125
Epoch 780, training loss: 6.144107341766357 = 0.0718393325805664 + 1.0 * 6.072268009185791
Epoch 780, val loss: 0.855799674987793
Epoch 790, training loss: 6.137470722198486 = 0.06790830940008163 + 1.0 * 6.0695624351501465
Epoch 790, val loss: 0.8613402247428894
Epoch 800, training loss: 6.132540702819824 = 0.06427343934774399 + 1.0 * 6.068267345428467
Epoch 800, val loss: 0.8669926524162292
Epoch 810, training loss: 6.129302024841309 = 0.06091270223259926 + 1.0 * 6.068389415740967
Epoch 810, val loss: 0.8727661967277527
Epoch 820, training loss: 6.129024028778076 = 0.057813286781311035 + 1.0 * 6.071210861206055
Epoch 820, val loss: 0.8785806894302368
Epoch 830, training loss: 6.123572826385498 = 0.05496054142713547 + 1.0 * 6.068612098693848
Epoch 830, val loss: 0.8843898177146912
Epoch 840, training loss: 6.116297721862793 = 0.0523081049323082 + 1.0 * 6.063989639282227
Epoch 840, val loss: 0.8903142809867859
Epoch 850, training loss: 6.121392726898193 = 0.04983383044600487 + 1.0 * 6.071558952331543
Epoch 850, val loss: 0.8962717056274414
Epoch 860, training loss: 6.11566686630249 = 0.04754325747489929 + 1.0 * 6.068123817443848
Epoch 860, val loss: 0.9021894335746765
Epoch 870, training loss: 6.10775899887085 = 0.04540450870990753 + 1.0 * 6.062354564666748
Epoch 870, val loss: 0.908151388168335
Epoch 880, training loss: 6.105634689331055 = 0.04340151697397232 + 1.0 * 6.062232971191406
Epoch 880, val loss: 0.9141187071800232
Epoch 890, training loss: 6.102676868438721 = 0.041526731103658676 + 1.0 * 6.061150074005127
Epoch 890, val loss: 0.9200074076652527
Epoch 900, training loss: 6.103392124176025 = 0.03976995497941971 + 1.0 * 6.063621997833252
Epoch 900, val loss: 0.9259084463119507
Epoch 910, training loss: 6.096643924713135 = 0.038129571825265884 + 1.0 * 6.05851411819458
Epoch 910, val loss: 0.931816816329956
Epoch 920, training loss: 6.0946784019470215 = 0.036584898829460144 + 1.0 * 6.058093547821045
Epoch 920, val loss: 0.9376834034919739
Epoch 930, training loss: 6.091973304748535 = 0.03512560948729515 + 1.0 * 6.05684757232666
Epoch 930, val loss: 0.9435745477676392
Epoch 940, training loss: 6.097313404083252 = 0.033748313784599304 + 1.0 * 6.063565254211426
Epoch 940, val loss: 0.9494425654411316
Epoch 950, training loss: 6.096018314361572 = 0.032460689544677734 + 1.0 * 6.0635576248168945
Epoch 950, val loss: 0.9553040862083435
Epoch 960, training loss: 6.085971355438232 = 0.03124341182410717 + 1.0 * 6.054728031158447
Epoch 960, val loss: 0.9610245227813721
Epoch 970, training loss: 6.084511756896973 = 0.030092714354395866 + 1.0 * 6.054419040679932
Epoch 970, val loss: 0.966736376285553
Epoch 980, training loss: 6.085431098937988 = 0.029001403599977493 + 1.0 * 6.056429862976074
Epoch 980, val loss: 0.9724428653717041
Epoch 990, training loss: 6.081046104431152 = 0.02796933799982071 + 1.0 * 6.05307674407959
Epoch 990, val loss: 0.9781184196472168
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.321693420410156 = 1.9478116035461426 + 1.0 * 8.373881340026855
Epoch 0, val loss: 1.9407124519348145
Epoch 10, training loss: 10.310576438903809 = 1.937199354171753 + 1.0 * 8.373376846313477
Epoch 10, val loss: 1.9311667680740356
Epoch 20, training loss: 10.293858528137207 = 1.9240137338638306 + 1.0 * 8.369844436645508
Epoch 20, val loss: 1.919113278388977
Epoch 30, training loss: 10.25212287902832 = 1.905964732170105 + 1.0 * 8.346158027648926
Epoch 30, val loss: 1.9024068117141724
Epoch 40, training loss: 10.082484245300293 = 1.884047508239746 + 1.0 * 8.198436737060547
Epoch 40, val loss: 1.8823155164718628
Epoch 50, training loss: 9.593706130981445 = 1.8600564002990723 + 1.0 * 7.733650207519531
Epoch 50, val loss: 1.860065221786499
Epoch 60, training loss: 9.189228057861328 = 1.8384042978286743 + 1.0 * 7.350823402404785
Epoch 60, val loss: 1.8409767150878906
Epoch 70, training loss: 8.862037658691406 = 1.8226763010025024 + 1.0 * 7.039361000061035
Epoch 70, val loss: 1.8272806406021118
Epoch 80, training loss: 8.66372013092041 = 1.8052808046340942 + 1.0 * 6.8584394454956055
Epoch 80, val loss: 1.8123232126235962
Epoch 90, training loss: 8.510519027709961 = 1.7867705821990967 + 1.0 * 6.723748683929443
Epoch 90, val loss: 1.7971082925796509
Epoch 100, training loss: 8.391923904418945 = 1.7682069540023804 + 1.0 * 6.623717308044434
Epoch 100, val loss: 1.7818480730056763
Epoch 110, training loss: 8.287945747375488 = 1.7487870454788208 + 1.0 * 6.539158344268799
Epoch 110, val loss: 1.7653326988220215
Epoch 120, training loss: 8.207662582397461 = 1.7279822826385498 + 1.0 * 6.47968053817749
Epoch 120, val loss: 1.7473129034042358
Epoch 130, training loss: 8.143067359924316 = 1.7047994136810303 + 1.0 * 6.438267707824707
Epoch 130, val loss: 1.727477788925171
Epoch 140, training loss: 8.074546813964844 = 1.6786061525344849 + 1.0 * 6.395940780639648
Epoch 140, val loss: 1.7053906917572021
Epoch 150, training loss: 8.014566421508789 = 1.6482828855514526 + 1.0 * 6.366283416748047
Epoch 150, val loss: 1.6800992488861084
Epoch 160, training loss: 7.953736305236816 = 1.6127859354019165 + 1.0 * 6.3409504890441895
Epoch 160, val loss: 1.6505835056304932
Epoch 170, training loss: 7.891969680786133 = 1.5717904567718506 + 1.0 * 6.320179462432861
Epoch 170, val loss: 1.6165953874588013
Epoch 180, training loss: 7.829710006713867 = 1.526026964187622 + 1.0 * 6.303683280944824
Epoch 180, val loss: 1.5791126489639282
Epoch 190, training loss: 7.766170978546143 = 1.4770336151123047 + 1.0 * 6.289137363433838
Epoch 190, val loss: 1.5389974117279053
Epoch 200, training loss: 7.701480865478516 = 1.4250621795654297 + 1.0 * 6.276418685913086
Epoch 200, val loss: 1.4965496063232422
Epoch 210, training loss: 7.640337944030762 = 1.3713855743408203 + 1.0 * 6.268952369689941
Epoch 210, val loss: 1.4530216455459595
Epoch 220, training loss: 7.574441909790039 = 1.317993402481079 + 1.0 * 6.256448745727539
Epoch 220, val loss: 1.410231113433838
Epoch 230, training loss: 7.511585235595703 = 1.2649778127670288 + 1.0 * 6.246607303619385
Epoch 230, val loss: 1.3683727979660034
Epoch 240, training loss: 7.451799392700195 = 1.21279776096344 + 1.0 * 6.239001750946045
Epoch 240, val loss: 1.3278714418411255
Epoch 250, training loss: 7.393003463745117 = 1.1621534824371338 + 1.0 * 6.2308502197265625
Epoch 250, val loss: 1.288985252380371
Epoch 260, training loss: 7.335196495056152 = 1.1122187376022339 + 1.0 * 6.222977638244629
Epoch 260, val loss: 1.2510408163070679
Epoch 270, training loss: 7.284430980682373 = 1.0627435445785522 + 1.0 * 6.221687316894531
Epoch 270, val loss: 1.2137415409088135
Epoch 280, training loss: 7.226193904876709 = 1.0144098997116089 + 1.0 * 6.2117838859558105
Epoch 280, val loss: 1.1772960424423218
Epoch 290, training loss: 7.1713738441467285 = 0.9668280482292175 + 1.0 * 6.204545974731445
Epoch 290, val loss: 1.1417548656463623
Epoch 300, training loss: 7.123622894287109 = 0.920205295085907 + 1.0 * 6.203417778015137
Epoch 300, val loss: 1.1071138381958008
Epoch 310, training loss: 7.070162773132324 = 0.8754000663757324 + 1.0 * 6.194762706756592
Epoch 310, val loss: 1.0739809274673462
Epoch 320, training loss: 7.022064685821533 = 0.8325791954994202 + 1.0 * 6.189485549926758
Epoch 320, val loss: 1.0428963899612427
Epoch 330, training loss: 6.9771013259887695 = 0.7918466329574585 + 1.0 * 6.1852545738220215
Epoch 330, val loss: 1.0138146877288818
Epoch 340, training loss: 6.936114311218262 = 0.7533973455429077 + 1.0 * 6.1827168464660645
Epoch 340, val loss: 0.9869300127029419
Epoch 350, training loss: 6.894894599914551 = 0.7173646092414856 + 1.0 * 6.177529811859131
Epoch 350, val loss: 0.9622652530670166
Epoch 360, training loss: 6.855013847351074 = 0.6834251880645752 + 1.0 * 6.171588897705078
Epoch 360, val loss: 0.9395170211791992
Epoch 370, training loss: 6.823402404785156 = 0.6511489152908325 + 1.0 * 6.172253608703613
Epoch 370, val loss: 0.9183759689331055
Epoch 380, training loss: 6.784860610961914 = 0.6206434369087219 + 1.0 * 6.164216995239258
Epoch 380, val loss: 0.8987475633621216
Epoch 390, training loss: 6.7526960372924805 = 0.5916686654090881 + 1.0 * 6.161027431488037
Epoch 390, val loss: 0.8805795907974243
Epoch 400, training loss: 6.722565174102783 = 0.564134418964386 + 1.0 * 6.158430576324463
Epoch 400, val loss: 0.8636527061462402
Epoch 410, training loss: 6.692734241485596 = 0.5381889343261719 + 1.0 * 6.154545307159424
Epoch 410, val loss: 0.8481721878051758
Epoch 420, training loss: 6.664304256439209 = 0.5135661959648132 + 1.0 * 6.15073823928833
Epoch 420, val loss: 0.8341380953788757
Epoch 430, training loss: 6.637697696685791 = 0.49015673995018005 + 1.0 * 6.147541046142578
Epoch 430, val loss: 0.821384072303772
Epoch 440, training loss: 6.618401050567627 = 0.46796655654907227 + 1.0 * 6.150434494018555
Epoch 440, val loss: 0.8099539279937744
Epoch 450, training loss: 6.590536117553711 = 0.4472186863422394 + 1.0 * 6.143317222595215
Epoch 450, val loss: 0.8000465035438538
Epoch 460, training loss: 6.567756175994873 = 0.42756688594818115 + 1.0 * 6.140189170837402
Epoch 460, val loss: 0.7914939522743225
Epoch 470, training loss: 6.545881271362305 = 0.4087340831756592 + 1.0 * 6.137147426605225
Epoch 470, val loss: 0.7840990424156189
Epoch 480, training loss: 6.530949592590332 = 0.39056825637817383 + 1.0 * 6.140381336212158
Epoch 480, val loss: 0.7777754068374634
Epoch 490, training loss: 6.505068302154541 = 0.37298452854156494 + 1.0 * 6.132083892822266
Epoch 490, val loss: 0.7725094556808472
Epoch 500, training loss: 6.493620872497559 = 0.35579389333724976 + 1.0 * 6.137826919555664
Epoch 500, val loss: 0.7682631611824036
Epoch 510, training loss: 6.467287063598633 = 0.3388381004333496 + 1.0 * 6.128448963165283
Epoch 510, val loss: 0.7649736404418945
Epoch 520, training loss: 6.449493885040283 = 0.322031170129776 + 1.0 * 6.127462863922119
Epoch 520, val loss: 0.7626195549964905
Epoch 530, training loss: 6.429546356201172 = 0.3052576184272766 + 1.0 * 6.124288558959961
Epoch 530, val loss: 0.7610204219818115
Epoch 540, training loss: 6.410759925842285 = 0.28850439190864563 + 1.0 * 6.122255325317383
Epoch 540, val loss: 0.7601848244667053
Epoch 550, training loss: 6.391855239868164 = 0.2716856002807617 + 1.0 * 6.120169639587402
Epoch 550, val loss: 0.760010302066803
Epoch 560, training loss: 6.37680196762085 = 0.25502529740333557 + 1.0 * 6.121776580810547
Epoch 560, val loss: 0.760356605052948
Epoch 570, training loss: 6.355957984924316 = 0.23875296115875244 + 1.0 * 6.1172051429748535
Epoch 570, val loss: 0.7615483403205872
Epoch 580, training loss: 6.338004112243652 = 0.2229764759540558 + 1.0 * 6.11502742767334
Epoch 580, val loss: 0.7636029124259949
Epoch 590, training loss: 6.327420234680176 = 0.2078455239534378 + 1.0 * 6.119574546813965
Epoch 590, val loss: 0.7664223313331604
Epoch 600, training loss: 6.308628559112549 = 0.1936550736427307 + 1.0 * 6.114973545074463
Epoch 600, val loss: 0.7698362469673157
Epoch 610, training loss: 6.291784763336182 = 0.18041133880615234 + 1.0 * 6.111373424530029
Epoch 610, val loss: 0.7743347883224487
Epoch 620, training loss: 6.276519775390625 = 0.16812516748905182 + 1.0 * 6.108394622802734
Epoch 620, val loss: 0.7796385884284973
Epoch 630, training loss: 6.27045202255249 = 0.1568031907081604 + 1.0 * 6.113648891448975
Epoch 630, val loss: 0.7855113744735718
Epoch 640, training loss: 6.251962661743164 = 0.14642834663391113 + 1.0 * 6.105534553527832
Epoch 640, val loss: 0.7920897006988525
Epoch 650, training loss: 6.241163730621338 = 0.13690458238124847 + 1.0 * 6.104259014129639
Epoch 650, val loss: 0.7992365956306458
Epoch 660, training loss: 6.243334770202637 = 0.12815555930137634 + 1.0 * 6.115179061889648
Epoch 660, val loss: 0.8066619038581848
Epoch 670, training loss: 6.221327304840088 = 0.12018682807683945 + 1.0 * 6.10114049911499
Epoch 670, val loss: 0.8144500851631165
Epoch 680, training loss: 6.212791919708252 = 0.1128399521112442 + 1.0 * 6.09995174407959
Epoch 680, val loss: 0.822723388671875
Epoch 690, training loss: 6.214296817779541 = 0.1060776337981224 + 1.0 * 6.108219146728516
Epoch 690, val loss: 0.8311114311218262
Epoch 700, training loss: 6.196261882781982 = 0.09988007694482803 + 1.0 * 6.096381664276123
Epoch 700, val loss: 0.8396229147911072
Epoch 710, training loss: 6.1897172927856445 = 0.09415367990732193 + 1.0 * 6.0955634117126465
Epoch 710, val loss: 0.8484545946121216
Epoch 720, training loss: 6.1860198974609375 = 0.08885751664638519 + 1.0 * 6.097162246704102
Epoch 720, val loss: 0.8573365807533264
Epoch 730, training loss: 6.178537845611572 = 0.08397497236728668 + 1.0 * 6.094563007354736
Epoch 730, val loss: 0.8660925626754761
Epoch 740, training loss: 6.171799182891846 = 0.0794597640633583 + 1.0 * 6.092339515686035
Epoch 740, val loss: 0.8751222491264343
Epoch 750, training loss: 6.165472030639648 = 0.07527422904968262 + 1.0 * 6.090197563171387
Epoch 750, val loss: 0.8841039538383484
Epoch 760, training loss: 6.167344570159912 = 0.07139725238084793 + 1.0 * 6.095947265625
Epoch 760, val loss: 0.892934262752533
Epoch 770, training loss: 6.156056880950928 = 0.06779737025499344 + 1.0 * 6.088259696960449
Epoch 770, val loss: 0.9017402529716492
Epoch 780, training loss: 6.151333808898926 = 0.06444475799798965 + 1.0 * 6.086889266967773
Epoch 780, val loss: 0.9105708003044128
Epoch 790, training loss: 6.147031307220459 = 0.06131530553102493 + 1.0 * 6.0857157707214355
Epoch 790, val loss: 0.9193945527076721
Epoch 800, training loss: 6.1447577476501465 = 0.05839673802256584 + 1.0 * 6.086360931396484
Epoch 800, val loss: 0.9278972744941711
Epoch 810, training loss: 6.141423225402832 = 0.055679623037576675 + 1.0 * 6.085743427276611
Epoch 810, val loss: 0.9366068243980408
Epoch 820, training loss: 6.1351318359375 = 0.05313402786850929 + 1.0 * 6.081997871398926
Epoch 820, val loss: 0.9452354311943054
Epoch 830, training loss: 6.132613658905029 = 0.05074416846036911 + 1.0 * 6.081869602203369
Epoch 830, val loss: 0.953765869140625
Epoch 840, training loss: 6.129018783569336 = 0.04850780591368675 + 1.0 * 6.080511093139648
Epoch 840, val loss: 0.9619458317756653
Epoch 850, training loss: 6.128182888031006 = 0.04641296714544296 + 1.0 * 6.081769943237305
Epoch 850, val loss: 0.9704000949859619
Epoch 860, training loss: 6.122405529022217 = 0.04444311931729317 + 1.0 * 6.077962398529053
Epoch 860, val loss: 0.9787744879722595
Epoch 870, training loss: 6.121652126312256 = 0.04258628562092781 + 1.0 * 6.079065799713135
Epoch 870, val loss: 0.9870058298110962
Epoch 880, training loss: 6.117644309997559 = 0.04083922877907753 + 1.0 * 6.076805114746094
Epoch 880, val loss: 0.9950188398361206
Epoch 890, training loss: 6.115975856781006 = 0.03919278457760811 + 1.0 * 6.076783180236816
Epoch 890, val loss: 1.0030853748321533
Epoch 900, training loss: 6.115548610687256 = 0.0376412607729435 + 1.0 * 6.077907562255859
Epoch 900, val loss: 1.0111618041992188
Epoch 910, training loss: 6.109394550323486 = 0.03617383539676666 + 1.0 * 6.073220729827881
Epoch 910, val loss: 1.019098162651062
Epoch 920, training loss: 6.115582466125488 = 0.034787122160196304 + 1.0 * 6.0807952880859375
Epoch 920, val loss: 1.0268867015838623
Epoch 930, training loss: 6.1087141036987305 = 0.03348824754357338 + 1.0 * 6.075225830078125
Epoch 930, val loss: 1.034669041633606
Epoch 940, training loss: 6.104071140289307 = 0.03225105255842209 + 1.0 * 6.071820259094238
Epoch 940, val loss: 1.0423505306243896
Epoch 950, training loss: 6.10164737701416 = 0.0310823917388916 + 1.0 * 6.070565223693848
Epoch 950, val loss: 1.0500311851501465
Epoch 960, training loss: 6.103888988494873 = 0.02997191622853279 + 1.0 * 6.073916912078857
Epoch 960, val loss: 1.0575872659683228
Epoch 970, training loss: 6.097983360290527 = 0.02891930751502514 + 1.0 * 6.069064140319824
Epoch 970, val loss: 1.0649069547653198
Epoch 980, training loss: 6.096099853515625 = 0.027919769287109375 + 1.0 * 6.068180084228516
Epoch 980, val loss: 1.0723627805709839
Epoch 990, training loss: 6.094874382019043 = 0.026969874277710915 + 1.0 * 6.067904472351074
Epoch 990, val loss: 1.0796535015106201
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5240
Flip ASR: 0.4356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.306650161743164 = 1.9327282905578613 + 1.0 * 8.373921394348145
Epoch 0, val loss: 1.9274438619613647
Epoch 10, training loss: 10.29658317565918 = 1.9229013919830322 + 1.0 * 8.373682022094727
Epoch 10, val loss: 1.9169503450393677
Epoch 20, training loss: 10.282901763916016 = 1.9109936952590942 + 1.0 * 8.371908187866211
Epoch 20, val loss: 1.904199242591858
Epoch 30, training loss: 10.252166748046875 = 1.8946890830993652 + 1.0 * 8.357478141784668
Epoch 30, val loss: 1.8865787982940674
Epoch 40, training loss: 10.125017166137695 = 1.8735648393630981 + 1.0 * 8.251452445983887
Epoch 40, val loss: 1.86445951461792
Epoch 50, training loss: 9.653599739074707 = 1.852946400642395 + 1.0 * 7.800652980804443
Epoch 50, val loss: 1.844020962715149
Epoch 60, training loss: 9.147100448608398 = 1.8364927768707275 + 1.0 * 7.310607433319092
Epoch 60, val loss: 1.82820725440979
Epoch 70, training loss: 8.794026374816895 = 1.8232828378677368 + 1.0 * 6.970743656158447
Epoch 70, val loss: 1.8153952360153198
Epoch 80, training loss: 8.59688949584961 = 1.8096178770065308 + 1.0 * 6.787271976470947
Epoch 80, val loss: 1.8020248413085938
Epoch 90, training loss: 8.467823028564453 = 1.793165922164917 + 1.0 * 6.674657344818115
Epoch 90, val loss: 1.7861212491989136
Epoch 100, training loss: 8.359855651855469 = 1.7749545574188232 + 1.0 * 6.584900856018066
Epoch 100, val loss: 1.768875002861023
Epoch 110, training loss: 8.264365196228027 = 1.7554994821548462 + 1.0 * 6.508865833282471
Epoch 110, val loss: 1.7507988214492798
Epoch 120, training loss: 8.189655303955078 = 1.7340866327285767 + 1.0 * 6.455568313598633
Epoch 120, val loss: 1.7311255931854248
Epoch 130, training loss: 8.12410831451416 = 1.709775447845459 + 1.0 * 6.414332866668701
Epoch 130, val loss: 1.70920991897583
Epoch 140, training loss: 8.06154727935791 = 1.6821038722991943 + 1.0 * 6.379443168640137
Epoch 140, val loss: 1.6847420930862427
Epoch 150, training loss: 8.000587463378906 = 1.6502115726470947 + 1.0 * 6.350376129150391
Epoch 150, val loss: 1.656980276107788
Epoch 160, training loss: 7.9385271072387695 = 1.6134308576583862 + 1.0 * 6.325096130371094
Epoch 160, val loss: 1.6255253553390503
Epoch 170, training loss: 7.87549352645874 = 1.5708414316177368 + 1.0 * 6.304652214050293
Epoch 170, val loss: 1.5898056030273438
Epoch 180, training loss: 7.808436870574951 = 1.5220037698745728 + 1.0 * 6.286433219909668
Epoch 180, val loss: 1.549639105796814
Epoch 190, training loss: 7.740623474121094 = 1.4674583673477173 + 1.0 * 6.273165225982666
Epoch 190, val loss: 1.5055752992630005
Epoch 200, training loss: 7.668885231018066 = 1.4088857173919678 + 1.0 * 6.2599992752075195
Epoch 200, val loss: 1.4588651657104492
Epoch 210, training loss: 7.599612712860107 = 1.3473089933395386 + 1.0 * 6.252303600311279
Epoch 210, val loss: 1.410627007484436
Epoch 220, training loss: 7.523653030395508 = 1.2846499681472778 + 1.0 * 6.2390031814575195
Epoch 220, val loss: 1.3623672723770142
Epoch 230, training loss: 7.452338695526123 = 1.2219659090042114 + 1.0 * 6.230372905731201
Epoch 230, val loss: 1.314502239227295
Epoch 240, training loss: 7.388676643371582 = 1.1598995923995972 + 1.0 * 6.228776931762695
Epoch 240, val loss: 1.2674288749694824
Epoch 250, training loss: 7.319865703582764 = 1.100213646888733 + 1.0 * 6.21965217590332
Epoch 250, val loss: 1.2221555709838867
Epoch 260, training loss: 7.251500129699707 = 1.0421382188796997 + 1.0 * 6.209362030029297
Epoch 260, val loss: 1.1776992082595825
Epoch 270, training loss: 7.188573360443115 = 0.9849419593811035 + 1.0 * 6.203631401062012
Epoch 270, val loss: 1.133915662765503
Epoch 280, training loss: 7.134207248687744 = 0.9288566708564758 + 1.0 * 6.205350399017334
Epoch 280, val loss: 1.090922474861145
Epoch 290, training loss: 7.0697431564331055 = 0.8753520250320435 + 1.0 * 6.194391250610352
Epoch 290, val loss: 1.049580454826355
Epoch 300, training loss: 7.011832237243652 = 0.8243452906608582 + 1.0 * 6.1874871253967285
Epoch 300, val loss: 1.0102636814117432
Epoch 310, training loss: 6.962221145629883 = 0.7760463953018188 + 1.0 * 6.1861748695373535
Epoch 310, val loss: 0.973244309425354
Epoch 320, training loss: 6.9106316566467285 = 0.7310636639595032 + 1.0 * 6.179567813873291
Epoch 320, val loss: 0.9390352964401245
Epoch 330, training loss: 6.8664655685424805 = 0.6890913844108582 + 1.0 * 6.177374362945557
Epoch 330, val loss: 0.9079671502113342
Epoch 340, training loss: 6.8207502365112305 = 0.6502659916877747 + 1.0 * 6.1704840660095215
Epoch 340, val loss: 0.8796694278717041
Epoch 350, training loss: 6.779022216796875 = 0.6137511730194092 + 1.0 * 6.165270805358887
Epoch 350, val loss: 0.8538876175880432
Epoch 360, training loss: 6.748895168304443 = 0.5792785286903381 + 1.0 * 6.16961669921875
Epoch 360, val loss: 0.830173909664154
Epoch 370, training loss: 6.706978797912598 = 0.5466760396957397 + 1.0 * 6.160302639007568
Epoch 370, val loss: 0.8085375428199768
Epoch 380, training loss: 6.670820236206055 = 0.5155736804008484 + 1.0 * 6.155246734619141
Epoch 380, val loss: 0.7886654138565063
Epoch 390, training loss: 6.636874675750732 = 0.4856587052345276 + 1.0 * 6.15121603012085
Epoch 390, val loss: 0.7703600525856018
Epoch 400, training loss: 6.622381687164307 = 0.456756591796875 + 1.0 * 6.165625095367432
Epoch 400, val loss: 0.7536166906356812
Epoch 410, training loss: 6.579652309417725 = 0.42942848801612854 + 1.0 * 6.150223731994629
Epoch 410, val loss: 0.7382989525794983
Epoch 420, training loss: 6.547547817230225 = 0.4033941328525543 + 1.0 * 6.144153594970703
Epoch 420, val loss: 0.7247347235679626
Epoch 430, training loss: 6.5184006690979 = 0.3785933256149292 + 1.0 * 6.139807224273682
Epoch 430, val loss: 0.7127241492271423
Epoch 440, training loss: 6.500269889831543 = 0.3550437390804291 + 1.0 * 6.145226001739502
Epoch 440, val loss: 0.7020541429519653
Epoch 450, training loss: 6.469870090484619 = 0.33298584818840027 + 1.0 * 6.1368842124938965
Epoch 450, val loss: 0.692691445350647
Epoch 460, training loss: 6.445948600769043 = 0.31237098574638367 + 1.0 * 6.133577823638916
Epoch 460, val loss: 0.6845483779907227
Epoch 470, training loss: 6.423500061035156 = 0.29307347536087036 + 1.0 * 6.130426406860352
Epoch 470, val loss: 0.6776146292686462
Epoch 480, training loss: 6.405640602111816 = 0.27505022287368774 + 1.0 * 6.130590438842773
Epoch 480, val loss: 0.671710729598999
Epoch 490, training loss: 6.3849382400512695 = 0.2582978904247284 + 1.0 * 6.126640319824219
Epoch 490, val loss: 0.666804850101471
Epoch 500, training loss: 6.370903968811035 = 0.2428099811077118 + 1.0 * 6.12809419631958
Epoch 500, val loss: 0.6629317402839661
Epoch 510, training loss: 6.352370262145996 = 0.22854089736938477 + 1.0 * 6.123829364776611
Epoch 510, val loss: 0.659848690032959
Epoch 520, training loss: 6.3340325355529785 = 0.2153363823890686 + 1.0 * 6.118696212768555
Epoch 520, val loss: 0.6576923131942749
Epoch 530, training loss: 6.321174144744873 = 0.20309369266033173 + 1.0 * 6.1180806159973145
Epoch 530, val loss: 0.656265914440155
Epoch 540, training loss: 6.309196949005127 = 0.19178369641304016 + 1.0 * 6.11741304397583
Epoch 540, val loss: 0.6554902195930481
Epoch 550, training loss: 6.298194408416748 = 0.18137258291244507 + 1.0 * 6.116821765899658
Epoch 550, val loss: 0.6553622484207153
Epoch 560, training loss: 6.283447265625 = 0.17174255847930908 + 1.0 * 6.1117048263549805
Epoch 560, val loss: 0.6557851433753967
Epoch 570, training loss: 6.278383255004883 = 0.16282373666763306 + 1.0 * 6.1155595779418945
Epoch 570, val loss: 0.6567136645317078
Epoch 580, training loss: 6.262957572937012 = 0.15455971658229828 + 1.0 * 6.108397960662842
Epoch 580, val loss: 0.6580978035926819
Epoch 590, training loss: 6.254183769226074 = 0.14691892266273499 + 1.0 * 6.107264995574951
Epoch 590, val loss: 0.6598342657089233
Epoch 600, training loss: 6.244742393493652 = 0.13980072736740112 + 1.0 * 6.1049418449401855
Epoch 600, val loss: 0.6619939804077148
Epoch 610, training loss: 6.242318630218506 = 0.13314753770828247 + 1.0 * 6.109170913696289
Epoch 610, val loss: 0.6644544005393982
Epoch 620, training loss: 6.233976364135742 = 0.1269860565662384 + 1.0 * 6.106990337371826
Epoch 620, val loss: 0.6670777201652527
Epoch 630, training loss: 6.22147798538208 = 0.12122424691915512 + 1.0 * 6.100253582000732
Epoch 630, val loss: 0.6699293851852417
Epoch 640, training loss: 6.214200973510742 = 0.11582193523645401 + 1.0 * 6.098379135131836
Epoch 640, val loss: 0.6730088591575623
Epoch 650, training loss: 6.218955993652344 = 0.11075621098279953 + 1.0 * 6.108199596405029
Epoch 650, val loss: 0.6763072609901428
Epoch 660, training loss: 6.210662841796875 = 0.10605446994304657 + 1.0 * 6.104608535766602
Epoch 660, val loss: 0.6795702576637268
Epoch 670, training loss: 6.195910930633545 = 0.10164619982242584 + 1.0 * 6.094264507293701
Epoch 670, val loss: 0.682999849319458
Epoch 680, training loss: 6.190242767333984 = 0.09749288111925125 + 1.0 * 6.092750072479248
Epoch 680, val loss: 0.6866374015808105
Epoch 690, training loss: 6.185133457183838 = 0.0935642197728157 + 1.0 * 6.091569423675537
Epoch 690, val loss: 0.6903454661369324
Epoch 700, training loss: 6.191445827484131 = 0.08984030038118362 + 1.0 * 6.101605415344238
Epoch 700, val loss: 0.6941622495651245
Epoch 710, training loss: 6.176760673522949 = 0.08634030073881149 + 1.0 * 6.090420246124268
Epoch 710, val loss: 0.6979448199272156
Epoch 720, training loss: 6.1704630851745605 = 0.0830167829990387 + 1.0 * 6.087446212768555
Epoch 720, val loss: 0.7018176317214966
Epoch 730, training loss: 6.176039695739746 = 0.0798676460981369 + 1.0 * 6.096171855926514
Epoch 730, val loss: 0.705736517906189
Epoch 740, training loss: 6.168435096740723 = 0.07688293606042862 + 1.0 * 6.091552257537842
Epoch 740, val loss: 0.7097806930541992
Epoch 750, training loss: 6.159361362457275 = 0.07406462728977203 + 1.0 * 6.085296630859375
Epoch 750, val loss: 0.7137474417686462
Epoch 760, training loss: 6.154660701751709 = 0.0713747888803482 + 1.0 * 6.083285808563232
Epoch 760, val loss: 0.717803955078125
Epoch 770, training loss: 6.150951385498047 = 0.06880496442317963 + 1.0 * 6.082146644592285
Epoch 770, val loss: 0.7218896746635437
Epoch 780, training loss: 6.153005123138428 = 0.06634560227394104 + 1.0 * 6.0866594314575195
Epoch 780, val loss: 0.7260258197784424
Epoch 790, training loss: 6.1503705978393555 = 0.06399137526750565 + 1.0 * 6.086379051208496
Epoch 790, val loss: 0.7300322651863098
Epoch 800, training loss: 6.1408843994140625 = 0.061727482825517654 + 1.0 * 6.079156875610352
Epoch 800, val loss: 0.7341170310974121
Epoch 810, training loss: 6.1380462646484375 = 0.05955160781741142 + 1.0 * 6.078494548797607
Epoch 810, val loss: 0.738153874874115
Epoch 820, training loss: 6.137686252593994 = 0.05744948238134384 + 1.0 * 6.080236911773682
Epoch 820, val loss: 0.7422884702682495
Epoch 830, training loss: 6.136599540710449 = 0.05542807653546333 + 1.0 * 6.08117151260376
Epoch 830, val loss: 0.7464001774787903
Epoch 840, training loss: 6.1303300857543945 = 0.053486887365579605 + 1.0 * 6.07684326171875
Epoch 840, val loss: 0.7504553198814392
Epoch 850, training loss: 6.12641716003418 = 0.051605094224214554 + 1.0 * 6.074811935424805
Epoch 850, val loss: 0.7546631693840027
Epoch 860, training loss: 6.12401008605957 = 0.04978436231613159 + 1.0 * 6.074225902557373
Epoch 860, val loss: 0.7587949633598328
Epoch 870, training loss: 6.127474308013916 = 0.048018649220466614 + 1.0 * 6.079455852508545
Epoch 870, val loss: 0.7629600167274475
Epoch 880, training loss: 6.122167587280273 = 0.0463053397834301 + 1.0 * 6.075862407684326
Epoch 880, val loss: 0.7670285701751709
Epoch 890, training loss: 6.1165900230407715 = 0.0446486622095108 + 1.0 * 6.071941375732422
Epoch 890, val loss: 0.7710760831832886
Epoch 900, training loss: 6.111322402954102 = 0.04303570091724396 + 1.0 * 6.068286895751953
Epoch 900, val loss: 0.7751765847206116
Epoch 910, training loss: 6.110079288482666 = 0.041462622582912445 + 1.0 * 6.06861686706543
Epoch 910, val loss: 0.7793319821357727
Epoch 920, training loss: 6.1166510581970215 = 0.039935093373060226 + 1.0 * 6.07671594619751
Epoch 920, val loss: 0.7835033535957336
Epoch 930, training loss: 6.111761569976807 = 0.03844446316361427 + 1.0 * 6.073317050933838
Epoch 930, val loss: 0.7875993251800537
Epoch 940, training loss: 6.105658531188965 = 0.03700264170765877 + 1.0 * 6.068655967712402
Epoch 940, val loss: 0.7915843725204468
Epoch 950, training loss: 6.101368427276611 = 0.03561744838953018 + 1.0 * 6.065751075744629
Epoch 950, val loss: 0.795702338218689
Epoch 960, training loss: 6.097896575927734 = 0.0342831127345562 + 1.0 * 6.063613414764404
Epoch 960, val loss: 0.7997725605964661
Epoch 970, training loss: 6.099423885345459 = 0.033002208918333054 + 1.0 * 6.0664215087890625
Epoch 970, val loss: 0.8038852214813232
Epoch 980, training loss: 6.096152305603027 = 0.031780656427145004 + 1.0 * 6.064371585845947
Epoch 980, val loss: 0.8080137372016907
Epoch 990, training loss: 6.100956439971924 = 0.030618758872151375 + 1.0 * 6.070337772369385
Epoch 990, val loss: 0.8120492100715637
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.5867
Flip ASR: 0.5600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.317240715026855 = 1.9433660507202148 + 1.0 * 8.37387466430664
Epoch 0, val loss: 1.9465612173080444
Epoch 10, training loss: 10.306501388549805 = 1.9330283403396606 + 1.0 * 8.373473167419434
Epoch 10, val loss: 1.9363173246383667
Epoch 20, training loss: 10.290489196777344 = 1.920037031173706 + 1.0 * 8.370451927185059
Epoch 20, val loss: 1.9230369329452515
Epoch 30, training loss: 10.249439239501953 = 1.9016348123550415 + 1.0 * 8.347804069519043
Epoch 30, val loss: 1.9040452241897583
Epoch 40, training loss: 10.056707382202148 = 1.878138780593872 + 1.0 * 8.178568840026855
Epoch 40, val loss: 1.8811382055282593
Epoch 50, training loss: 9.588167190551758 = 1.8533456325531006 + 1.0 * 7.734821796417236
Epoch 50, val loss: 1.8576675653457642
Epoch 60, training loss: 9.22629165649414 = 1.83228600025177 + 1.0 * 7.39400577545166
Epoch 60, val loss: 1.8378705978393555
Epoch 70, training loss: 8.827263832092285 = 1.815000057220459 + 1.0 * 7.012263774871826
Epoch 70, val loss: 1.822465419769287
Epoch 80, training loss: 8.56757926940918 = 1.8001952171325684 + 1.0 * 6.767383575439453
Epoch 80, val loss: 1.8089423179626465
Epoch 90, training loss: 8.423511505126953 = 1.783244013786316 + 1.0 * 6.640267372131348
Epoch 90, val loss: 1.7929472923278809
Epoch 100, training loss: 8.33456802368164 = 1.765272617340088 + 1.0 * 6.569295883178711
Epoch 100, val loss: 1.776098608970642
Epoch 110, training loss: 8.25234603881836 = 1.7470604181289673 + 1.0 * 6.505285263061523
Epoch 110, val loss: 1.7592130899429321
Epoch 120, training loss: 8.183201789855957 = 1.7282418012619019 + 1.0 * 6.454959869384766
Epoch 120, val loss: 1.7417833805084229
Epoch 130, training loss: 8.119794845581055 = 1.707337498664856 + 1.0 * 6.412457466125488
Epoch 130, val loss: 1.7226852178573608
Epoch 140, training loss: 8.05964469909668 = 1.683127760887146 + 1.0 * 6.376516819000244
Epoch 140, val loss: 1.7012380361557007
Epoch 150, training loss: 8.001270294189453 = 1.6551576852798462 + 1.0 * 6.346112251281738
Epoch 150, val loss: 1.676952838897705
Epoch 160, training loss: 7.944974422454834 = 1.6231002807617188 + 1.0 * 6.321874141693115
Epoch 160, val loss: 1.6493958234786987
Epoch 170, training loss: 7.887299537658691 = 1.5861690044403076 + 1.0 * 6.301130771636963
Epoch 170, val loss: 1.617774486541748
Epoch 180, training loss: 7.837645530700684 = 1.544037103652954 + 1.0 * 6.293608665466309
Epoch 180, val loss: 1.5819109678268433
Epoch 190, training loss: 7.77208137512207 = 1.4981240034103394 + 1.0 * 6.273957252502441
Epoch 190, val loss: 1.5430822372436523
Epoch 200, training loss: 7.707998752593994 = 1.448263168334961 + 1.0 * 6.259735584259033
Epoch 200, val loss: 1.50162672996521
Epoch 210, training loss: 7.643868923187256 = 1.395127773284912 + 1.0 * 6.248741149902344
Epoch 210, val loss: 1.4582350254058838
Epoch 220, training loss: 7.57872200012207 = 1.339436411857605 + 1.0 * 6.239285469055176
Epoch 220, val loss: 1.4136687517166138
Epoch 230, training loss: 7.5149126052856445 = 1.2821472883224487 + 1.0 * 6.232765197753906
Epoch 230, val loss: 1.3688817024230957
Epoch 240, training loss: 7.449333667755127 = 1.2249449491500854 + 1.0 * 6.224388599395752
Epoch 240, val loss: 1.3250144720077515
Epoch 250, training loss: 7.385143280029297 = 1.1682713031768799 + 1.0 * 6.216872215270996
Epoch 250, val loss: 1.2823485136032104
Epoch 260, training loss: 7.32548713684082 = 1.1126954555511475 + 1.0 * 6.212791919708252
Epoch 260, val loss: 1.241240382194519
Epoch 270, training loss: 7.263644218444824 = 1.0589635372161865 + 1.0 * 6.204680442810059
Epoch 270, val loss: 1.2018719911575317
Epoch 280, training loss: 7.205032825469971 = 1.0066672563552856 + 1.0 * 6.198365688323975
Epoch 280, val loss: 1.163895845413208
Epoch 290, training loss: 7.152634620666504 = 0.955803394317627 + 1.0 * 6.196831226348877
Epoch 290, val loss: 1.1272947788238525
Epoch 300, training loss: 7.096919536590576 = 0.9071568250656128 + 1.0 * 6.189762592315674
Epoch 300, val loss: 1.0923471450805664
Epoch 310, training loss: 7.044515609741211 = 0.8603711724281311 + 1.0 * 6.184144496917725
Epoch 310, val loss: 1.0589686632156372
Epoch 320, training loss: 6.994384765625 = 0.8151432275772095 + 1.0 * 6.17924165725708
Epoch 320, val loss: 1.026800274848938
Epoch 330, training loss: 6.9485907554626465 = 0.7715133428573608 + 1.0 * 6.177077293395996
Epoch 330, val loss: 0.9958686232566833
Epoch 340, training loss: 6.903872013092041 = 0.7301427125930786 + 1.0 * 6.173729419708252
Epoch 340, val loss: 0.9666363596916199
Epoch 350, training loss: 6.857422828674316 = 0.690439760684967 + 1.0 * 6.166983127593994
Epoch 350, val loss: 0.938873291015625
Epoch 360, training loss: 6.816102027893066 = 0.6522886157035828 + 1.0 * 6.163813591003418
Epoch 360, val loss: 0.912409245967865
Epoch 370, training loss: 6.78562593460083 = 0.6159237623214722 + 1.0 * 6.169702053070068
Epoch 370, val loss: 0.8874105215072632
Epoch 380, training loss: 6.741926193237305 = 0.5818504095077515 + 1.0 * 6.160075664520264
Epoch 380, val loss: 0.8645038604736328
Epoch 390, training loss: 6.704413414001465 = 0.5497406721115112 + 1.0 * 6.154672622680664
Epoch 390, val loss: 0.8434904217720032
Epoch 400, training loss: 6.669393539428711 = 0.5192582011222839 + 1.0 * 6.150135517120361
Epoch 400, val loss: 0.8242242932319641
Epoch 410, training loss: 6.638831615447998 = 0.4902016222476959 + 1.0 * 6.148630142211914
Epoch 410, val loss: 0.8066645264625549
Epoch 420, training loss: 6.617321968078613 = 0.4627736210823059 + 1.0 * 6.154548168182373
Epoch 420, val loss: 0.7906832098960876
Epoch 430, training loss: 6.5786309242248535 = 0.4367032051086426 + 1.0 * 6.141927719116211
Epoch 430, val loss: 0.7764568328857422
Epoch 440, training loss: 6.551591396331787 = 0.4117235243320465 + 1.0 * 6.139867782592773
Epoch 440, val loss: 0.7635773420333862
Epoch 450, training loss: 6.525634288787842 = 0.3876345455646515 + 1.0 * 6.137999534606934
Epoch 450, val loss: 0.7518408894538879
Epoch 460, training loss: 6.5048723220825195 = 0.3644181191921234 + 1.0 * 6.140454292297363
Epoch 460, val loss: 0.7411560416221619
Epoch 470, training loss: 6.475757122039795 = 0.34217315912246704 + 1.0 * 6.133584022521973
Epoch 470, val loss: 0.7316153049468994
Epoch 480, training loss: 6.450012683868408 = 0.32076600193977356 + 1.0 * 6.129246711730957
Epoch 480, val loss: 0.7230457663536072
Epoch 490, training loss: 6.429276943206787 = 0.30021658539772034 + 1.0 * 6.1290602684021
Epoch 490, val loss: 0.7154268622398376
Epoch 500, training loss: 6.411642551422119 = 0.2807033360004425 + 1.0 * 6.13093900680542
Epoch 500, val loss: 0.7088425159454346
Epoch 510, training loss: 6.386499881744385 = 0.2623335123062134 + 1.0 * 6.124166488647461
Epoch 510, val loss: 0.7033472061157227
Epoch 520, training loss: 6.365514755249023 = 0.24502339959144592 + 1.0 * 6.1204915046691895
Epoch 520, val loss: 0.6989307999610901
Epoch 530, training loss: 6.358725547790527 = 0.22881649434566498 + 1.0 * 6.129909038543701
Epoch 530, val loss: 0.6954932808876038
Epoch 540, training loss: 6.334449768066406 = 0.21379512548446655 + 1.0 * 6.120654582977295
Epoch 540, val loss: 0.6930860877037048
Epoch 550, training loss: 6.315774440765381 = 0.1998627632856369 + 1.0 * 6.115911483764648
Epoch 550, val loss: 0.6917241811752319
Epoch 560, training loss: 6.301506996154785 = 0.18690335750579834 + 1.0 * 6.114603519439697
Epoch 560, val loss: 0.6912710666656494
Epoch 570, training loss: 6.291231155395508 = 0.1749243587255478 + 1.0 * 6.116306781768799
Epoch 570, val loss: 0.691588282585144
Epoch 580, training loss: 6.275619029998779 = 0.16390405595302582 + 1.0 * 6.111714839935303
Epoch 580, val loss: 0.6927070617675781
Epoch 590, training loss: 6.261690616607666 = 0.15373212099075317 + 1.0 * 6.1079583168029785
Epoch 590, val loss: 0.6945075988769531
Epoch 600, training loss: 6.2511491775512695 = 0.14429891109466553 + 1.0 * 6.1068501472473145
Epoch 600, val loss: 0.6969161629676819
Epoch 610, training loss: 6.242392539978027 = 0.13556760549545288 + 1.0 * 6.10682487487793
Epoch 610, val loss: 0.699799120426178
Epoch 620, training loss: 6.231886386871338 = 0.12751686573028564 + 1.0 * 6.104369640350342
Epoch 620, val loss: 0.7031281590461731
Epoch 630, training loss: 6.222107410430908 = 0.1200735792517662 + 1.0 * 6.102033615112305
Epoch 630, val loss: 0.7067839503288269
Epoch 640, training loss: 6.211644172668457 = 0.1131700724363327 + 1.0 * 6.098474025726318
Epoch 640, val loss: 0.7108275890350342
Epoch 650, training loss: 6.212826728820801 = 0.10675222426652908 + 1.0 * 6.106074333190918
Epoch 650, val loss: 0.7151193618774414
Epoch 660, training loss: 6.200491905212402 = 0.10082940012216568 + 1.0 * 6.0996623039245605
Epoch 660, val loss: 0.7196272015571594
Epoch 670, training loss: 6.1911821365356445 = 0.09536829590797424 + 1.0 * 6.095813751220703
Epoch 670, val loss: 0.7243083715438843
Epoch 680, training loss: 6.184396266937256 = 0.09030047804117203 + 1.0 * 6.094095706939697
Epoch 680, val loss: 0.7292452454566956
Epoch 690, training loss: 6.1776885986328125 = 0.08557284623384476 + 1.0 * 6.092115879058838
Epoch 690, val loss: 0.7342996001243591
Epoch 700, training loss: 6.176036834716797 = 0.08117058128118515 + 1.0 * 6.0948662757873535
Epoch 700, val loss: 0.7395025491714478
Epoch 710, training loss: 6.1666717529296875 = 0.07706186175346375 + 1.0 * 6.0896100997924805
Epoch 710, val loss: 0.7447761297225952
Epoch 720, training loss: 6.161669731140137 = 0.07324278354644775 + 1.0 * 6.0884270668029785
Epoch 720, val loss: 0.750170111656189
Epoch 730, training loss: 6.160419940948486 = 0.06967993080615997 + 1.0 * 6.090740203857422
Epoch 730, val loss: 0.7556033134460449
Epoch 740, training loss: 6.152839183807373 = 0.06635977327823639 + 1.0 * 6.086479187011719
Epoch 740, val loss: 0.7610825300216675
Epoch 750, training loss: 6.146233558654785 = 0.06326129287481308 + 1.0 * 6.082972049713135
Epoch 750, val loss: 0.7666798830032349
Epoch 760, training loss: 6.142752647399902 = 0.06035032868385315 + 1.0 * 6.082402229309082
Epoch 760, val loss: 0.772325336933136
Epoch 770, training loss: 6.140803337097168 = 0.057621683925390244 + 1.0 * 6.083181858062744
Epoch 770, val loss: 0.777969479560852
Epoch 780, training loss: 6.138176441192627 = 0.05506783723831177 + 1.0 * 6.083108425140381
Epoch 780, val loss: 0.7836229801177979
Epoch 790, training loss: 6.130692005157471 = 0.05268044024705887 + 1.0 * 6.078011512756348
Epoch 790, val loss: 0.7893252372741699
Epoch 800, training loss: 6.1285600662231445 = 0.05044499784708023 + 1.0 * 6.078114986419678
Epoch 800, val loss: 0.7950431704521179
Epoch 810, training loss: 6.1245646476745605 = 0.04833795502781868 + 1.0 * 6.076226711273193
Epoch 810, val loss: 0.800807774066925
Epoch 820, training loss: 6.131345748901367 = 0.04635465517640114 + 1.0 * 6.084990978240967
Epoch 820, val loss: 0.8065255284309387
Epoch 830, training loss: 6.11968994140625 = 0.044477298855781555 + 1.0 * 6.075212478637695
Epoch 830, val loss: 0.8121739029884338
Epoch 840, training loss: 6.1149396896362305 = 0.04271828010678291 + 1.0 * 6.072221279144287
Epoch 840, val loss: 0.817926824092865
Epoch 850, training loss: 6.121681213378906 = 0.04105287045240402 + 1.0 * 6.080628395080566
Epoch 850, val loss: 0.8236499428749084
Epoch 860, training loss: 6.114195823669434 = 0.039478905498981476 + 1.0 * 6.074717044830322
Epoch 860, val loss: 0.8292630910873413
Epoch 870, training loss: 6.109108924865723 = 0.03799798712134361 + 1.0 * 6.071110725402832
Epoch 870, val loss: 0.8349428772926331
Epoch 880, training loss: 6.108478546142578 = 0.036593250930309296 + 1.0 * 6.071885108947754
Epoch 880, val loss: 0.8405205607414246
Epoch 890, training loss: 6.110039234161377 = 0.03526236116886139 + 1.0 * 6.074776649475098
Epoch 890, val loss: 0.8461045026779175
Epoch 900, training loss: 6.1034111976623535 = 0.03400861471891403 + 1.0 * 6.069402694702148
Epoch 900, val loss: 0.851599395275116
Epoch 910, training loss: 6.102827548980713 = 0.03281718119978905 + 1.0 * 6.070010185241699
Epoch 910, val loss: 0.857120156288147
Epoch 920, training loss: 6.097412109375 = 0.03168473392724991 + 1.0 * 6.065727233886719
Epoch 920, val loss: 0.8625484704971313
Epoch 930, training loss: 6.09494686126709 = 0.03060717135667801 + 1.0 * 6.064339637756348
Epoch 930, val loss: 0.8679793477058411
Epoch 940, training loss: 6.095183849334717 = 0.029580630362033844 + 1.0 * 6.065603256225586
Epoch 940, val loss: 0.8733706474304199
Epoch 950, training loss: 6.09840202331543 = 0.028605369850993156 + 1.0 * 6.069796562194824
Epoch 950, val loss: 0.8787367939949036
Epoch 960, training loss: 6.094876289367676 = 0.027681956067681313 + 1.0 * 6.06719446182251
Epoch 960, val loss: 0.8840266466140747
Epoch 970, training loss: 6.087346076965332 = 0.026805078610777855 + 1.0 * 6.060541152954102
Epoch 970, val loss: 0.8893147706985474
Epoch 980, training loss: 6.085933208465576 = 0.02596662938594818 + 1.0 * 6.059966564178467
Epoch 980, val loss: 0.894620418548584
Epoch 990, training loss: 6.0923943519592285 = 0.025163285434246063 + 1.0 * 6.067231178283691
Epoch 990, val loss: 0.8998091816902161
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8782
Flip ASR: 0.8533/225 nodes
The final ASR:0.66298, 0.15435, Accuracy:0.80617, 0.01429
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9426])
updated graph: torch.Size([2, 10478])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.97540, 0.01058, Accuracy:0.83457, 0.01222
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.318540573120117 = 1.9446853399276733 + 1.0 * 8.373855590820312
Epoch 0, val loss: 1.951696753501892
Epoch 10, training loss: 10.307795524597168 = 1.9343751668930054 + 1.0 * 8.373420715332031
Epoch 10, val loss: 1.9410934448242188
Epoch 20, training loss: 10.29204273223877 = 1.9210808277130127 + 1.0 * 8.370962142944336
Epoch 20, val loss: 1.9273767471313477
Epoch 30, training loss: 10.25670337677002 = 1.9024025201797485 + 1.0 * 8.354300498962402
Epoch 30, val loss: 1.9081952571868896
Epoch 40, training loss: 10.113240242004395 = 1.8785226345062256 + 1.0 * 8.23471736907959
Epoch 40, val loss: 1.8849800825119019
Epoch 50, training loss: 9.44227123260498 = 1.8537760972976685 + 1.0 * 7.588495254516602
Epoch 50, val loss: 1.8617424964904785
Epoch 60, training loss: 8.941925048828125 = 1.8345991373062134 + 1.0 * 7.107326030731201
Epoch 60, val loss: 1.8448576927185059
Epoch 70, training loss: 8.6348876953125 = 1.821353793144226 + 1.0 * 6.813533782958984
Epoch 70, val loss: 1.8327736854553223
Epoch 80, training loss: 8.480984687805176 = 1.807992935180664 + 1.0 * 6.672991752624512
Epoch 80, val loss: 1.8198084831237793
Epoch 90, training loss: 8.383736610412598 = 1.7940092086791992 + 1.0 * 6.589727401733398
Epoch 90, val loss: 1.8065192699432373
Epoch 100, training loss: 8.305179595947266 = 1.7809431552886963 + 1.0 * 6.52423620223999
Epoch 100, val loss: 1.7944716215133667
Epoch 110, training loss: 8.229269981384277 = 1.7695726156234741 + 1.0 * 6.459697723388672
Epoch 110, val loss: 1.7842059135437012
Epoch 120, training loss: 8.168327331542969 = 1.7584872245788574 + 1.0 * 6.409839630126953
Epoch 120, val loss: 1.7742277383804321
Epoch 130, training loss: 8.116872787475586 = 1.7459031343460083 + 1.0 * 6.370969772338867
Epoch 130, val loss: 1.7631573677062988
Epoch 140, training loss: 8.06871509552002 = 1.7310292720794678 + 1.0 * 6.337686061859131
Epoch 140, val loss: 1.7505838871002197
Epoch 150, training loss: 8.025557518005371 = 1.713289499282837 + 1.0 * 6.312268257141113
Epoch 150, val loss: 1.7360807657241821
Epoch 160, training loss: 7.982383728027344 = 1.6923764944076538 + 1.0 * 6.2900071144104
Epoch 160, val loss: 1.7192845344543457
Epoch 170, training loss: 7.941731929779053 = 1.6672779321670532 + 1.0 * 6.274454116821289
Epoch 170, val loss: 1.6992954015731812
Epoch 180, training loss: 7.89600944519043 = 1.6369268894195557 + 1.0 * 6.259082794189453
Epoch 180, val loss: 1.6752679347991943
Epoch 190, training loss: 7.846324920654297 = 1.6003069877624512 + 1.0 * 6.246017932891846
Epoch 190, val loss: 1.6463924646377563
Epoch 200, training loss: 7.793039321899414 = 1.5567679405212402 + 1.0 * 6.236271381378174
Epoch 200, val loss: 1.6121556758880615
Epoch 210, training loss: 7.730625629425049 = 1.506803035736084 + 1.0 * 6.223822593688965
Epoch 210, val loss: 1.5728267431259155
Epoch 220, training loss: 7.666033744812012 = 1.4505436420440674 + 1.0 * 6.215490341186523
Epoch 220, val loss: 1.5287952423095703
Epoch 230, training loss: 7.595778465270996 = 1.3896757364273071 + 1.0 * 6.2061028480529785
Epoch 230, val loss: 1.4813971519470215
Epoch 240, training loss: 7.525835990905762 = 1.3255093097686768 + 1.0 * 6.200326919555664
Epoch 240, val loss: 1.4316354990005493
Epoch 250, training loss: 7.452679634094238 = 1.259978175163269 + 1.0 * 6.19270133972168
Epoch 250, val loss: 1.3810588121414185
Epoch 260, training loss: 7.380306243896484 = 1.1942178010940552 + 1.0 * 6.186088562011719
Epoch 260, val loss: 1.3305726051330566
Epoch 270, training loss: 7.311975002288818 = 1.1302495002746582 + 1.0 * 6.18172550201416
Epoch 270, val loss: 1.2815607786178589
Epoch 280, training loss: 7.243112564086914 = 1.0688707828521729 + 1.0 * 6.174241542816162
Epoch 280, val loss: 1.2342761754989624
Epoch 290, training loss: 7.178617477416992 = 1.0099290609359741 + 1.0 * 6.1686882972717285
Epoch 290, val loss: 1.1887379884719849
Epoch 300, training loss: 7.1200032234191895 = 0.9544086456298828 + 1.0 * 6.165594577789307
Epoch 300, val loss: 1.1458410024642944
Epoch 310, training loss: 7.061667442321777 = 0.9024239182472229 + 1.0 * 6.159243583679199
Epoch 310, val loss: 1.1059187650680542
Epoch 320, training loss: 7.006496429443359 = 0.8527548909187317 + 1.0 * 6.153741359710693
Epoch 320, val loss: 1.067784070968628
Epoch 330, training loss: 6.955086708068848 = 0.8052531480789185 + 1.0 * 6.149833679199219
Epoch 330, val loss: 1.0314254760742188
Epoch 340, training loss: 6.905333995819092 = 0.7602497339248657 + 1.0 * 6.145084381103516
Epoch 340, val loss: 0.9972649216651917
Epoch 350, training loss: 6.862423419952393 = 0.7172721028327942 + 1.0 * 6.145151138305664
Epoch 350, val loss: 0.9645941853523254
Epoch 360, training loss: 6.815171718597412 = 0.6766775250434875 + 1.0 * 6.13849401473999
Epoch 360, val loss: 0.9338334798812866
Epoch 370, training loss: 6.7728447914123535 = 0.6383959054946899 + 1.0 * 6.134449005126953
Epoch 370, val loss: 0.9050770401954651
Epoch 380, training loss: 6.73856258392334 = 0.6022445559501648 + 1.0 * 6.136318206787109
Epoch 380, val loss: 0.8780946731567383
Epoch 390, training loss: 6.698460102081299 = 0.5685098767280579 + 1.0 * 6.129950046539307
Epoch 390, val loss: 0.8535612225532532
Epoch 400, training loss: 6.661855220794678 = 0.5369114875793457 + 1.0 * 6.124943733215332
Epoch 400, val loss: 0.8311033248901367
Epoch 410, training loss: 6.6342339515686035 = 0.5072807669639587 + 1.0 * 6.126953125
Epoch 410, val loss: 0.810708224773407
Epoch 420, training loss: 6.599634647369385 = 0.4796234369277954 + 1.0 * 6.120011329650879
Epoch 420, val loss: 0.7925102710723877
Epoch 430, training loss: 6.5718183517456055 = 0.4536873996257782 + 1.0 * 6.118131160736084
Epoch 430, val loss: 0.7762511968612671
Epoch 440, training loss: 6.548662185668945 = 0.42929908633232117 + 1.0 * 6.119363307952881
Epoch 440, val loss: 0.7617741227149963
Epoch 450, training loss: 6.521768569946289 = 0.40647733211517334 + 1.0 * 6.115291118621826
Epoch 450, val loss: 0.749034583568573
Epoch 460, training loss: 6.49503755569458 = 0.3849957287311554 + 1.0 * 6.110041618347168
Epoch 460, val loss: 0.737824022769928
Epoch 470, training loss: 6.471083641052246 = 0.3645126521587372 + 1.0 * 6.106571197509766
Epoch 470, val loss: 0.7276654839515686
Epoch 480, training loss: 6.450566291809082 = 0.34489572048187256 + 1.0 * 6.10567045211792
Epoch 480, val loss: 0.71849524974823
Epoch 490, training loss: 6.438817024230957 = 0.32629677653312683 + 1.0 * 6.112520217895508
Epoch 490, val loss: 0.7102193832397461
Epoch 500, training loss: 6.40873908996582 = 0.3086642324924469 + 1.0 * 6.100074768066406
Epoch 500, val loss: 0.7029025554656982
Epoch 510, training loss: 6.391147613525391 = 0.29168906807899475 + 1.0 * 6.099458694458008
Epoch 510, val loss: 0.6960831880569458
Epoch 520, training loss: 6.374256610870361 = 0.2752414047718048 + 1.0 * 6.099015235900879
Epoch 520, val loss: 0.6897160410881042
Epoch 530, training loss: 6.361047744750977 = 0.25949281454086304 + 1.0 * 6.101554870605469
Epoch 530, val loss: 0.6839279532432556
Epoch 540, training loss: 6.339694023132324 = 0.24442699551582336 + 1.0 * 6.095266819000244
Epoch 540, val loss: 0.6787312030792236
Epoch 550, training loss: 6.322884559631348 = 0.22995711863040924 + 1.0 * 6.0929274559021
Epoch 550, val loss: 0.6739840507507324
Epoch 560, training loss: 6.307586669921875 = 0.2161891609430313 + 1.0 * 6.091397285461426
Epoch 560, val loss: 0.6697679758071899
Epoch 570, training loss: 6.292584419250488 = 0.2031545490026474 + 1.0 * 6.08942985534668
Epoch 570, val loss: 0.6661153435707092
Epoch 580, training loss: 6.283482551574707 = 0.19083666801452637 + 1.0 * 6.09264612197876
Epoch 580, val loss: 0.6630359888076782
Epoch 590, training loss: 6.266176223754883 = 0.17931590974330902 + 1.0 * 6.086860179901123
Epoch 590, val loss: 0.6605807542800903
Epoch 600, training loss: 6.254612922668457 = 0.16849066317081451 + 1.0 * 6.086122035980225
Epoch 600, val loss: 0.658751904964447
Epoch 610, training loss: 6.241582870483398 = 0.15838666260242462 + 1.0 * 6.08319616317749
Epoch 610, val loss: 0.6574101448059082
Epoch 620, training loss: 6.231006622314453 = 0.14898735284805298 + 1.0 * 6.082019329071045
Epoch 620, val loss: 0.6566960215568542
Epoch 630, training loss: 6.220702171325684 = 0.140201598405838 + 1.0 * 6.080500602722168
Epoch 630, val loss: 0.6564440727233887
Epoch 640, training loss: 6.215771198272705 = 0.13203242421150208 + 1.0 * 6.083738803863525
Epoch 640, val loss: 0.6566537022590637
Epoch 650, training loss: 6.205174922943115 = 0.12449123710393906 + 1.0 * 6.080683708190918
Epoch 650, val loss: 0.6573071479797363
Epoch 660, training loss: 6.194538593292236 = 0.11749612540006638 + 1.0 * 6.077042579650879
Epoch 660, val loss: 0.6584442257881165
Epoch 670, training loss: 6.191274642944336 = 0.11096840351819992 + 1.0 * 6.080306053161621
Epoch 670, val loss: 0.659980297088623
Epoch 680, training loss: 6.182927131652832 = 0.10492768883705139 + 1.0 * 6.077999591827393
Epoch 680, val loss: 0.6618640422821045
Epoch 690, training loss: 6.172291278839111 = 0.09932457655668259 + 1.0 * 6.072966575622559
Epoch 690, val loss: 0.664054811000824
Epoch 700, training loss: 6.166201591491699 = 0.09409594535827637 + 1.0 * 6.072105884552002
Epoch 700, val loss: 0.6666063666343689
Epoch 710, training loss: 6.163659572601318 = 0.08922324329614639 + 1.0 * 6.074436187744141
Epoch 710, val loss: 0.6693530678749084
Epoch 720, training loss: 6.155700206756592 = 0.08470616489648819 + 1.0 * 6.070993900299072
Epoch 720, val loss: 0.6723347306251526
Epoch 730, training loss: 6.15000581741333 = 0.0804930105805397 + 1.0 * 6.069512844085693
Epoch 730, val loss: 0.6755942702293396
Epoch 740, training loss: 6.151873588562012 = 0.07655459642410278 + 1.0 * 6.075318813323975
Epoch 740, val loss: 0.6789749264717102
Epoch 750, training loss: 6.140720844268799 = 0.07288669794797897 + 1.0 * 6.067834377288818
Epoch 750, val loss: 0.6825251579284668
Epoch 760, training loss: 6.140487194061279 = 0.06945881992578506 + 1.0 * 6.071028232574463
Epoch 760, val loss: 0.6862040758132935
Epoch 770, training loss: 6.132787704467773 = 0.06625984609127045 + 1.0 * 6.066527843475342
Epoch 770, val loss: 0.6900412440299988
Epoch 780, training loss: 6.126556396484375 = 0.06324808299541473 + 1.0 * 6.063308238983154
Epoch 780, val loss: 0.6939436793327332
Epoch 790, training loss: 6.126048564910889 = 0.06041250750422478 + 1.0 * 6.065636157989502
Epoch 790, val loss: 0.697966992855072
Epoch 800, training loss: 6.124766826629639 = 0.05776401609182358 + 1.0 * 6.067002773284912
Epoch 800, val loss: 0.7021166086196899
Epoch 810, training loss: 6.115884304046631 = 0.05528794229030609 + 1.0 * 6.060596466064453
Epoch 810, val loss: 0.706180214881897
Epoch 820, training loss: 6.1148223876953125 = 0.052957043051719666 + 1.0 * 6.061865329742432
Epoch 820, val loss: 0.7103103995323181
Epoch 830, training loss: 6.111734867095947 = 0.05076118931174278 + 1.0 * 6.060973644256592
Epoch 830, val loss: 0.7144587635993958
Epoch 840, training loss: 6.108288288116455 = 0.04869223013520241 + 1.0 * 6.059596061706543
Epoch 840, val loss: 0.7186585664749146
Epoch 850, training loss: 6.110584735870361 = 0.046744342893362045 + 1.0 * 6.063840389251709
Epoch 850, val loss: 0.7228875756263733
Epoch 860, training loss: 6.104150295257568 = 0.044908370822668076 + 1.0 * 6.059241771697998
Epoch 860, val loss: 0.7269871830940247
Epoch 870, training loss: 6.099666595458984 = 0.043181903660297394 + 1.0 * 6.056484699249268
Epoch 870, val loss: 0.731222927570343
Epoch 880, training loss: 6.102363109588623 = 0.04154074937105179 + 1.0 * 6.060822486877441
Epoch 880, val loss: 0.7354235649108887
Epoch 890, training loss: 6.095820903778076 = 0.039996061474084854 + 1.0 * 6.0558247566223145
Epoch 890, val loss: 0.7395095825195312
Epoch 900, training loss: 6.092413425445557 = 0.03853374347090721 + 1.0 * 6.053879737854004
Epoch 900, val loss: 0.7437149882316589
Epoch 910, training loss: 6.091146469116211 = 0.03714197129011154 + 1.0 * 6.054004669189453
Epoch 910, val loss: 0.7478383183479309
Epoch 920, training loss: 6.0890936851501465 = 0.03582320734858513 + 1.0 * 6.05327033996582
Epoch 920, val loss: 0.751966118812561
Epoch 930, training loss: 6.092463493347168 = 0.03458239138126373 + 1.0 * 6.057880878448486
Epoch 930, val loss: 0.7560402750968933
Epoch 940, training loss: 6.083995342254639 = 0.03340776637196541 + 1.0 * 6.0505876541137695
Epoch 940, val loss: 0.7600439190864563
Epoch 950, training loss: 6.082090377807617 = 0.03228765353560448 + 1.0 * 6.049802780151367
Epoch 950, val loss: 0.7640807032585144
Epoch 960, training loss: 6.080871105194092 = 0.031216638162732124 + 1.0 * 6.049654483795166
Epoch 960, val loss: 0.768151581287384
Epoch 970, training loss: 6.0866923332214355 = 0.030197210609912872 + 1.0 * 6.056495189666748
Epoch 970, val loss: 0.772108793258667
Epoch 980, training loss: 6.07713508605957 = 0.029232202097773552 + 1.0 * 6.047903060913086
Epoch 980, val loss: 0.776058554649353
Epoch 990, training loss: 6.074529647827148 = 0.028314482420682907 + 1.0 * 6.046215057373047
Epoch 990, val loss: 0.7799980640411377
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.7380
Flip ASR: 0.6844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.329967498779297 = 1.9561593532562256 + 1.0 * 8.373807907104492
Epoch 0, val loss: 1.9643574953079224
Epoch 10, training loss: 10.319347381591797 = 1.9460185766220093 + 1.0 * 8.373329162597656
Epoch 10, val loss: 1.9539313316345215
Epoch 20, training loss: 10.303784370422363 = 1.9332857131958008 + 1.0 * 8.370498657226562
Epoch 20, val loss: 1.9406569004058838
Epoch 30, training loss: 10.266745567321777 = 1.9151897430419922 + 1.0 * 8.351555824279785
Epoch 30, val loss: 1.921971082687378
Epoch 40, training loss: 10.100629806518555 = 1.8920974731445312 + 1.0 * 8.208532333374023
Epoch 40, val loss: 1.8994805812835693
Epoch 50, training loss: 9.468337059020996 = 1.8674697875976562 + 1.0 * 7.60086727142334
Epoch 50, val loss: 1.8753662109375
Epoch 60, training loss: 9.155835151672363 = 1.8474562168121338 + 1.0 * 7.30837869644165
Epoch 60, val loss: 1.856076717376709
Epoch 70, training loss: 8.774843215942383 = 1.834630012512207 + 1.0 * 6.940213680267334
Epoch 70, val loss: 1.8431729078292847
Epoch 80, training loss: 8.526533126831055 = 1.823508381843567 + 1.0 * 6.703024387359619
Epoch 80, val loss: 1.831642746925354
Epoch 90, training loss: 8.369735717773438 = 1.8105943202972412 + 1.0 * 6.559141159057617
Epoch 90, val loss: 1.8185067176818848
Epoch 100, training loss: 8.273509979248047 = 1.7978320121765137 + 1.0 * 6.475677967071533
Epoch 100, val loss: 1.8054428100585938
Epoch 110, training loss: 8.19900131225586 = 1.7858542203903198 + 1.0 * 6.41314697265625
Epoch 110, val loss: 1.7927285432815552
Epoch 120, training loss: 8.1388578414917 = 1.7742550373077393 + 1.0 * 6.364602565765381
Epoch 120, val loss: 1.7801779508590698
Epoch 130, training loss: 8.090631484985352 = 1.761974573135376 + 1.0 * 6.3286566734313965
Epoch 130, val loss: 1.7673540115356445
Epoch 140, training loss: 8.050036430358887 = 1.748184323310852 + 1.0 * 6.301851749420166
Epoch 140, val loss: 1.754073143005371
Epoch 150, training loss: 8.010833740234375 = 1.7325420379638672 + 1.0 * 6.278291702270508
Epoch 150, val loss: 1.7400715351104736
Epoch 160, training loss: 7.973374366760254 = 1.7142268419265747 + 1.0 * 6.259147644042969
Epoch 160, val loss: 1.7244665622711182
Epoch 170, training loss: 7.935244560241699 = 1.692225694656372 + 1.0 * 6.243018627166748
Epoch 170, val loss: 1.7062281370162964
Epoch 180, training loss: 7.896827220916748 = 1.6657590866088867 + 1.0 * 6.231068134307861
Epoch 180, val loss: 1.6845577955245972
Epoch 190, training loss: 7.851739883422852 = 1.6341140270233154 + 1.0 * 6.217625617980957
Epoch 190, val loss: 1.6588327884674072
Epoch 200, training loss: 7.803471565246582 = 1.5961780548095703 + 1.0 * 6.207293510437012
Epoch 200, val loss: 1.6279569864273071
Epoch 210, training loss: 7.753026962280273 = 1.5514321327209473 + 1.0 * 6.201594829559326
Epoch 210, val loss: 1.5916532278060913
Epoch 220, training loss: 7.694797515869141 = 1.5015777349472046 + 1.0 * 6.1932196617126465
Epoch 220, val loss: 1.5514914989471436
Epoch 230, training loss: 7.632403373718262 = 1.447444200515747 + 1.0 * 6.1849589347839355
Epoch 230, val loss: 1.5078754425048828
Epoch 240, training loss: 7.569789409637451 = 1.3904814720153809 + 1.0 * 6.17930793762207
Epoch 240, val loss: 1.4621175527572632
Epoch 250, training loss: 7.509683609008789 = 1.3325190544128418 + 1.0 * 6.177164554595947
Epoch 250, val loss: 1.4159444570541382
Epoch 260, training loss: 7.448842525482178 = 1.2758092880249023 + 1.0 * 6.173033237457275
Epoch 260, val loss: 1.3710861206054688
Epoch 270, training loss: 7.385977745056152 = 1.2211742401123047 + 1.0 * 6.164803504943848
Epoch 270, val loss: 1.3280994892120361
Epoch 280, training loss: 7.326204299926758 = 1.1673599481582642 + 1.0 * 6.158844470977783
Epoch 280, val loss: 1.2857502698898315
Epoch 290, training loss: 7.266810417175293 = 1.113337755203247 + 1.0 * 6.153472423553467
Epoch 290, val loss: 1.2432670593261719
Epoch 300, training loss: 7.208966255187988 = 1.058706283569336 + 1.0 * 6.150259971618652
Epoch 300, val loss: 1.2004001140594482
Epoch 310, training loss: 7.152102947235107 = 1.004679799079895 + 1.0 * 6.147423267364502
Epoch 310, val loss: 1.15803861618042
Epoch 320, training loss: 7.092494964599609 = 0.9509825110435486 + 1.0 * 6.141512393951416
Epoch 320, val loss: 1.115712285041809
Epoch 330, training loss: 7.038936614990234 = 0.8978316187858582 + 1.0 * 6.1411051750183105
Epoch 330, val loss: 1.0739320516586304
Epoch 340, training loss: 6.98131799697876 = 0.8467456698417664 + 1.0 * 6.134572505950928
Epoch 340, val loss: 1.0338714122772217
Epoch 350, training loss: 6.930750846862793 = 0.7985761165618896 + 1.0 * 6.132174491882324
Epoch 350, val loss: 0.9962377548217773
Epoch 360, training loss: 6.888134002685547 = 0.7540671825408936 + 1.0 * 6.134066581726074
Epoch 360, val loss: 0.9618846774101257
Epoch 370, training loss: 6.838834285736084 = 0.7140225768089294 + 1.0 * 6.12481164932251
Epoch 370, val loss: 0.9316574335098267
Epoch 380, training loss: 6.798104286193848 = 0.6775931715965271 + 1.0 * 6.120511054992676
Epoch 380, val loss: 0.9051052927970886
Epoch 390, training loss: 6.769047260284424 = 0.6441454291343689 + 1.0 * 6.12490177154541
Epoch 390, val loss: 0.8817611336708069
Epoch 400, training loss: 6.729978084564209 = 0.6138168573379517 + 1.0 * 6.116161346435547
Epoch 400, val loss: 0.8615949153900146
Epoch 410, training loss: 6.698716163635254 = 0.5859262943267822 + 1.0 * 6.112790107727051
Epoch 410, val loss: 0.8441567420959473
Epoch 420, training loss: 6.6697540283203125 = 0.5598244071006775 + 1.0 * 6.10992956161499
Epoch 420, val loss: 0.8287942409515381
Epoch 430, training loss: 6.645927429199219 = 0.53557288646698 + 1.0 * 6.110354423522949
Epoch 430, val loss: 0.8154694437980652
Epoch 440, training loss: 6.6171555519104 = 0.5130586624145508 + 1.0 * 6.10409688949585
Epoch 440, val loss: 0.8040005564689636
Epoch 450, training loss: 6.597884178161621 = 0.49189695715904236 + 1.0 * 6.105987071990967
Epoch 450, val loss: 0.7940784096717834
Epoch 460, training loss: 6.571377754211426 = 0.4720335900783539 + 1.0 * 6.099344253540039
Epoch 460, val loss: 0.785685658454895
Epoch 470, training loss: 6.55145263671875 = 0.45324093103408813 + 1.0 * 6.098211765289307
Epoch 470, val loss: 0.7785837054252625
Epoch 480, training loss: 6.5334906578063965 = 0.4355066120624542 + 1.0 * 6.0979838371276855
Epoch 480, val loss: 0.7726107239723206
Epoch 490, training loss: 6.511197090148926 = 0.4187436103820801 + 1.0 * 6.092453479766846
Epoch 490, val loss: 0.7677501440048218
Epoch 500, training loss: 6.493473529815674 = 0.40269482135772705 + 1.0 * 6.090778827667236
Epoch 500, val loss: 0.7637414336204529
Epoch 510, training loss: 6.480494499206543 = 0.38735851645469666 + 1.0 * 6.093135833740234
Epoch 510, val loss: 0.7605013847351074
Epoch 520, training loss: 6.461670875549316 = 0.3728679418563843 + 1.0 * 6.088802814483643
Epoch 520, val loss: 0.7579883933067322
Epoch 530, training loss: 6.443995952606201 = 0.35897332429885864 + 1.0 * 6.085022449493408
Epoch 530, val loss: 0.7560688257217407
Epoch 540, training loss: 6.428788661956787 = 0.3455410599708557 + 1.0 * 6.083247661590576
Epoch 540, val loss: 0.7546449303627014
Epoch 550, training loss: 6.428526401519775 = 0.3325657844543457 + 1.0 * 6.09596061706543
Epoch 550, val loss: 0.7536614537239075
Epoch 560, training loss: 6.403033256530762 = 0.32027870416641235 + 1.0 * 6.082754611968994
Epoch 560, val loss: 0.7530211806297302
Epoch 570, training loss: 6.389316082000732 = 0.30847761034965515 + 1.0 * 6.080838680267334
Epoch 570, val loss: 0.7526575326919556
Epoch 580, training loss: 6.374874591827393 = 0.29699453711509705 + 1.0 * 6.077879905700684
Epoch 580, val loss: 0.7525393962860107
Epoch 590, training loss: 6.36141300201416 = 0.2857816517353058 + 1.0 * 6.075631141662598
Epoch 590, val loss: 0.7526004314422607
Epoch 600, training loss: 6.356391906738281 = 0.2748068571090698 + 1.0 * 6.081584930419922
Epoch 600, val loss: 0.7528526782989502
Epoch 610, training loss: 6.342540740966797 = 0.2641914188861847 + 1.0 * 6.0783491134643555
Epoch 610, val loss: 0.7532234191894531
Epoch 620, training loss: 6.326197624206543 = 0.2538135051727295 + 1.0 * 6.072383880615234
Epoch 620, val loss: 0.7536860108375549
Epoch 630, training loss: 6.323101043701172 = 0.24361687898635864 + 1.0 * 6.079483985900879
Epoch 630, val loss: 0.754163384437561
Epoch 640, training loss: 6.303785800933838 = 0.23367641866207123 + 1.0 * 6.0701093673706055
Epoch 640, val loss: 0.7546173930168152
Epoch 650, training loss: 6.299145221710205 = 0.2239171415567398 + 1.0 * 6.075228214263916
Epoch 650, val loss: 0.7551192045211792
Epoch 660, training loss: 6.285622596740723 = 0.21437574923038483 + 1.0 * 6.07124662399292
Epoch 660, val loss: 0.7556512355804443
Epoch 670, training loss: 6.27213716506958 = 0.20503537356853485 + 1.0 * 6.067101955413818
Epoch 670, val loss: 0.7561647891998291
Epoch 680, training loss: 6.26552677154541 = 0.1958789825439453 + 1.0 * 6.069647789001465
Epoch 680, val loss: 0.7568328976631165
Epoch 690, training loss: 6.253607749938965 = 0.1869840770959854 + 1.0 * 6.066623687744141
Epoch 690, val loss: 0.7575467228889465
Epoch 700, training loss: 6.245612144470215 = 0.17836081981658936 + 1.0 * 6.067251205444336
Epoch 700, val loss: 0.758373498916626
Epoch 710, training loss: 6.233126640319824 = 0.17003101110458374 + 1.0 * 6.063095569610596
Epoch 710, val loss: 0.7593463063240051
Epoch 720, training loss: 6.22791862487793 = 0.16197717189788818 + 1.0 * 6.065941333770752
Epoch 720, val loss: 0.7605051398277283
Epoch 730, training loss: 6.219464302062988 = 0.1542544960975647 + 1.0 * 6.065209865570068
Epoch 730, val loss: 0.761806309223175
Epoch 740, training loss: 6.2084784507751465 = 0.14692790806293488 + 1.0 * 6.061550617218018
Epoch 740, val loss: 0.7632957696914673
Epoch 750, training loss: 6.198631286621094 = 0.13992071151733398 + 1.0 * 6.05871057510376
Epoch 750, val loss: 0.7649768590927124
Epoch 760, training loss: 6.190159797668457 = 0.1331997662782669 + 1.0 * 6.056960105895996
Epoch 760, val loss: 0.7669007182121277
Epoch 770, training loss: 6.183927536010742 = 0.12680506706237793 + 1.0 * 6.057122707366943
Epoch 770, val loss: 0.769028902053833
Epoch 780, training loss: 6.185327529907227 = 0.12074128538370132 + 1.0 * 6.064586162567139
Epoch 780, val loss: 0.7713900804519653
Epoch 790, training loss: 6.173408031463623 = 0.11508359760046005 + 1.0 * 6.058324337005615
Epoch 790, val loss: 0.773813784122467
Epoch 800, training loss: 6.165028095245361 = 0.1097460687160492 + 1.0 * 6.055282115936279
Epoch 800, val loss: 0.7764699459075928
Epoch 810, training loss: 6.160002708435059 = 0.10468599200248718 + 1.0 * 6.055316925048828
Epoch 810, val loss: 0.7794384956359863
Epoch 820, training loss: 6.15200138092041 = 0.09987431764602661 + 1.0 * 6.052126884460449
Epoch 820, val loss: 0.7825321555137634
Epoch 830, training loss: 6.1490044593811035 = 0.09531520307064056 + 1.0 * 6.053689479827881
Epoch 830, val loss: 0.7857120633125305
Epoch 840, training loss: 6.144225597381592 = 0.09095723927021027 + 1.0 * 6.0532684326171875
Epoch 840, val loss: 0.7891334891319275
Epoch 850, training loss: 6.13739538192749 = 0.08679425716400146 + 1.0 * 6.050601005554199
Epoch 850, val loss: 0.7927272319793701
Epoch 860, training loss: 6.133218765258789 = 0.08281964063644409 + 1.0 * 6.050399303436279
Epoch 860, val loss: 0.7964076399803162
Epoch 870, training loss: 6.127078533172607 = 0.07903191447257996 + 1.0 * 6.048046588897705
Epoch 870, val loss: 0.8002458810806274
Epoch 880, training loss: 6.128654956817627 = 0.07542335242033005 + 1.0 * 6.053231716156006
Epoch 880, val loss: 0.8043185472488403
Epoch 890, training loss: 6.121349334716797 = 0.07199937105178833 + 1.0 * 6.049349784851074
Epoch 890, val loss: 0.8084896802902222
Epoch 900, training loss: 6.12289571762085 = 0.06876706331968307 + 1.0 * 6.054128646850586
Epoch 900, val loss: 0.8127697110176086
Epoch 910, training loss: 6.116372585296631 = 0.06574756652116776 + 1.0 * 6.050624847412109
Epoch 910, val loss: 0.81714928150177
Epoch 920, training loss: 6.108246803283691 = 0.06289999932050705 + 1.0 * 6.045346736907959
Epoch 920, val loss: 0.8215230107307434
Epoch 930, training loss: 6.1045942306518555 = 0.06024257838726044 + 1.0 * 6.044351577758789
Epoch 930, val loss: 0.8260054588317871
Epoch 940, training loss: 6.101916313171387 = 0.05776309221982956 + 1.0 * 6.044153213500977
Epoch 940, val loss: 0.8306164145469666
Epoch 950, training loss: 6.103083610534668 = 0.05546920746564865 + 1.0 * 6.047614574432373
Epoch 950, val loss: 0.8352831602096558
Epoch 960, training loss: 6.095999717712402 = 0.053354546427726746 + 1.0 * 6.04264497756958
Epoch 960, val loss: 0.8399308323860168
Epoch 970, training loss: 6.094200611114502 = 0.05138859525322914 + 1.0 * 6.042811870574951
Epoch 970, val loss: 0.8446813225746155
Epoch 980, training loss: 6.09158182144165 = 0.049551498144865036 + 1.0 * 6.042030334472656
Epoch 980, val loss: 0.8494621515274048
Epoch 990, training loss: 6.089297294616699 = 0.04783134162425995 + 1.0 * 6.041465759277344
Epoch 990, val loss: 0.8541786074638367
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.7011
Flip ASR: 0.6533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.305281639099121 = 1.9314907789230347 + 1.0 * 8.373790740966797
Epoch 0, val loss: 1.9340217113494873
Epoch 10, training loss: 10.294778823852539 = 1.921617031097412 + 1.0 * 8.373161315917969
Epoch 10, val loss: 1.9243234395980835
Epoch 20, training loss: 10.278773307800293 = 1.9094969034194946 + 1.0 * 8.36927604675293
Epoch 20, val loss: 1.9119411706924438
Epoch 30, training loss: 10.23801040649414 = 1.892884373664856 + 1.0 * 8.345126152038574
Epoch 30, val loss: 1.8947430849075317
Epoch 40, training loss: 10.040396690368652 = 1.8730039596557617 + 1.0 * 8.16739273071289
Epoch 40, val loss: 1.8746347427368164
Epoch 50, training loss: 9.527938842773438 = 1.8515554666519165 + 1.0 * 7.676383018493652
Epoch 50, val loss: 1.852673053741455
Epoch 60, training loss: 9.244942665100098 = 1.833070158958435 + 1.0 * 7.411872863769531
Epoch 60, val loss: 1.8344988822937012
Epoch 70, training loss: 8.829304695129395 = 1.8200137615203857 + 1.0 * 7.009291172027588
Epoch 70, val loss: 1.8217171430587769
Epoch 80, training loss: 8.528303146362305 = 1.8106544017791748 + 1.0 * 6.717648506164551
Epoch 80, val loss: 1.8115649223327637
Epoch 90, training loss: 8.380165100097656 = 1.8000741004943848 + 1.0 * 6.58009147644043
Epoch 90, val loss: 1.8009955883026123
Epoch 100, training loss: 8.269654273986816 = 1.7888802289962769 + 1.0 * 6.480774402618408
Epoch 100, val loss: 1.7898163795471191
Epoch 110, training loss: 8.199478149414062 = 1.7771918773651123 + 1.0 * 6.422286510467529
Epoch 110, val loss: 1.7772995233535767
Epoch 120, training loss: 8.145469665527344 = 1.7647325992584229 + 1.0 * 6.380736827850342
Epoch 120, val loss: 1.7638235092163086
Epoch 130, training loss: 8.098363876342773 = 1.7509056329727173 + 1.0 * 6.3474578857421875
Epoch 130, val loss: 1.7497626543045044
Epoch 140, training loss: 8.055782318115234 = 1.73506498336792 + 1.0 * 6.320716857910156
Epoch 140, val loss: 1.7347241640090942
Epoch 150, training loss: 8.015276908874512 = 1.7165074348449707 + 1.0 * 6.298769474029541
Epoch 150, val loss: 1.7181740999221802
Epoch 160, training loss: 7.9721269607543945 = 1.6947476863861084 + 1.0 * 6.277379512786865
Epoch 160, val loss: 1.699320673942566
Epoch 170, training loss: 7.931527137756348 = 1.668500542640686 + 1.0 * 6.263026714324951
Epoch 170, val loss: 1.6773006916046143
Epoch 180, training loss: 7.881122589111328 = 1.637060523033142 + 1.0 * 6.2440619468688965
Epoch 180, val loss: 1.6514893770217896
Epoch 190, training loss: 7.8294219970703125 = 1.599214792251587 + 1.0 * 6.230207443237305
Epoch 190, val loss: 1.620786428451538
Epoch 200, training loss: 7.772629737854004 = 1.554182529449463 + 1.0 * 6.218447208404541
Epoch 200, val loss: 1.5845613479614258
Epoch 210, training loss: 7.709901332855225 = 1.5025407075881958 + 1.0 * 6.207360744476318
Epoch 210, val loss: 1.5432597398757935
Epoch 220, training loss: 7.644978046417236 = 1.4445749521255493 + 1.0 * 6.200403213500977
Epoch 220, val loss: 1.4976208209991455
Epoch 230, training loss: 7.573817253112793 = 1.3826429843902588 + 1.0 * 6.191174507141113
Epoch 230, val loss: 1.449264645576477
Epoch 240, training loss: 7.500422477722168 = 1.3177993297576904 + 1.0 * 6.182623386383057
Epoch 240, val loss: 1.3992936611175537
Epoch 250, training loss: 7.430490016937256 = 1.2513885498046875 + 1.0 * 6.179101467132568
Epoch 250, val loss: 1.3491829633712769
Epoch 260, training loss: 7.358631134033203 = 1.1857378482818604 + 1.0 * 6.172893047332764
Epoch 260, val loss: 1.3001611232757568
Epoch 270, training loss: 7.2874932289123535 = 1.1220508813858032 + 1.0 * 6.16544246673584
Epoch 270, val loss: 1.2528302669525146
Epoch 280, training loss: 7.219577312469482 = 1.0603667497634888 + 1.0 * 6.159210681915283
Epoch 280, val loss: 1.2075406312942505
Epoch 290, training loss: 7.160855770111084 = 1.002447247505188 + 1.0 * 6.1584086418151855
Epoch 290, val loss: 1.1656620502471924
Epoch 300, training loss: 7.098841190338135 = 0.9490651488304138 + 1.0 * 6.149775981903076
Epoch 300, val loss: 1.1277236938476562
Epoch 310, training loss: 7.051040172576904 = 0.8998427987098694 + 1.0 * 6.15119743347168
Epoch 310, val loss: 1.0932790040969849
Epoch 320, training loss: 6.994444370269775 = 0.8550634980201721 + 1.0 * 6.139380931854248
Epoch 320, val loss: 1.0628663301467896
Epoch 330, training loss: 6.948063373565674 = 0.8140503764152527 + 1.0 * 6.1340131759643555
Epoch 330, val loss: 1.036102056503296
Epoch 340, training loss: 6.914198875427246 = 0.7760549187660217 + 1.0 * 6.138144016265869
Epoch 340, val loss: 1.0121937990188599
Epoch 350, training loss: 6.867334365844727 = 0.7410185933113098 + 1.0 * 6.126315593719482
Epoch 350, val loss: 0.9910473823547363
Epoch 360, training loss: 6.8309526443481445 = 0.7080919146537781 + 1.0 * 6.122860908508301
Epoch 360, val loss: 0.9722121357917786
Epoch 370, training loss: 6.799868106842041 = 0.677030086517334 + 1.0 * 6.122838020324707
Epoch 370, val loss: 0.9550999402999878
Epoch 380, training loss: 6.764894008636475 = 0.6476849913597107 + 1.0 * 6.117208957672119
Epoch 380, val loss: 0.9395796656608582
Epoch 390, training loss: 6.732327461242676 = 0.6194060444831848 + 1.0 * 6.112921237945557
Epoch 390, val loss: 0.9250384569168091
Epoch 400, training loss: 6.70756721496582 = 0.5918983817100525 + 1.0 * 6.115668773651123
Epoch 400, val loss: 0.9110631346702576
Epoch 410, training loss: 6.673006057739258 = 0.5652945041656494 + 1.0 * 6.1077117919921875
Epoch 410, val loss: 0.8978819847106934
Epoch 420, training loss: 6.645633220672607 = 0.5391290783882141 + 1.0 * 6.106503963470459
Epoch 420, val loss: 0.8852800726890564
Epoch 430, training loss: 6.618849754333496 = 0.5134516954421997 + 1.0 * 6.105398178100586
Epoch 430, val loss: 0.873117983341217
Epoch 440, training loss: 6.5916924476623535 = 0.4882698059082031 + 1.0 * 6.10342264175415
Epoch 440, val loss: 0.8617579936981201
Epoch 450, training loss: 6.561680793762207 = 0.46362748742103577 + 1.0 * 6.098053455352783
Epoch 450, val loss: 0.8510631322860718
Epoch 460, training loss: 6.534474849700928 = 0.4394761025905609 + 1.0 * 6.094998836517334
Epoch 460, val loss: 0.8412670493125916
Epoch 470, training loss: 6.513275623321533 = 0.41588112711906433 + 1.0 * 6.0973944664001465
Epoch 470, val loss: 0.8324716091156006
Epoch 480, training loss: 6.491281986236572 = 0.39340418577194214 + 1.0 * 6.0978779792785645
Epoch 480, val loss: 0.8249514102935791
Epoch 490, training loss: 6.464684009552002 = 0.3721378445625305 + 1.0 * 6.092545986175537
Epoch 490, val loss: 0.8191327452659607
Epoch 500, training loss: 6.440896987915039 = 0.35189780592918396 + 1.0 * 6.088999271392822
Epoch 500, val loss: 0.8146615028381348
Epoch 510, training loss: 6.4252800941467285 = 0.33270663022994995 + 1.0 * 6.092573642730713
Epoch 510, val loss: 0.8114258646965027
Epoch 520, training loss: 6.400373935699463 = 0.3147285580635071 + 1.0 * 6.0856451988220215
Epoch 520, val loss: 0.8095202445983887
Epoch 530, training loss: 6.381588935852051 = 0.29788151383399963 + 1.0 * 6.083707332611084
Epoch 530, val loss: 0.8087084293365479
Epoch 540, training loss: 6.3640031814575195 = 0.28211185336112976 + 1.0 * 6.0818915367126465
Epoch 540, val loss: 0.8089894652366638
Epoch 550, training loss: 6.3476033210754395 = 0.2672577500343323 + 1.0 * 6.080345630645752
Epoch 550, val loss: 0.8101009130477905
Epoch 560, training loss: 6.3460588455200195 = 0.2532522976398468 + 1.0 * 6.092806339263916
Epoch 560, val loss: 0.8117972016334534
Epoch 570, training loss: 6.3181843757629395 = 0.240220844745636 + 1.0 * 6.077963352203369
Epoch 570, val loss: 0.8141945004463196
Epoch 580, training loss: 6.303614139556885 = 0.2279587984085083 + 1.0 * 6.075655460357666
Epoch 580, val loss: 0.8172408938407898
Epoch 590, training loss: 6.294688701629639 = 0.21642212569713593 + 1.0 * 6.078266620635986
Epoch 590, val loss: 0.8205949664115906
Epoch 600, training loss: 6.278757095336914 = 0.20558235049247742 + 1.0 * 6.073174953460693
Epoch 600, val loss: 0.8244935274124146
Epoch 610, training loss: 6.278041362762451 = 0.19535760581493378 + 1.0 * 6.082683563232422
Epoch 610, val loss: 0.8287624716758728
Epoch 620, training loss: 6.257518768310547 = 0.18574704229831696 + 1.0 * 6.071771621704102
Epoch 620, val loss: 0.8332394361495972
Epoch 630, training loss: 6.246450901031494 = 0.1767093688249588 + 1.0 * 6.069741725921631
Epoch 630, val loss: 0.8382335305213928
Epoch 640, training loss: 6.235538005828857 = 0.16814389824867249 + 1.0 * 6.067394256591797
Epoch 640, val loss: 0.8434262871742249
Epoch 650, training loss: 6.24657678604126 = 0.16005472838878632 + 1.0 * 6.086522102355957
Epoch 650, val loss: 0.8487629890441895
Epoch 660, training loss: 6.224392890930176 = 0.1525474488735199 + 1.0 * 6.071845531463623
Epoch 660, val loss: 0.8543400168418884
Epoch 670, training loss: 6.211348533630371 = 0.1454523354768753 + 1.0 * 6.065896034240723
Epoch 670, val loss: 0.8602643609046936
Epoch 680, training loss: 6.202357769012451 = 0.13871555030345917 + 1.0 * 6.0636420249938965
Epoch 680, val loss: 0.8663109540939331
Epoch 690, training loss: 6.200042724609375 = 0.13234415650367737 + 1.0 * 6.0676984786987305
Epoch 690, val loss: 0.8724141716957092
Epoch 700, training loss: 6.188182353973389 = 0.12634329497814178 + 1.0 * 6.0618391036987305
Epoch 700, val loss: 0.8787708878517151
Epoch 710, training loss: 6.181346416473389 = 0.12066364288330078 + 1.0 * 6.060682773590088
Epoch 710, val loss: 0.8853515386581421
Epoch 720, training loss: 6.181639194488525 = 0.11527962982654572 + 1.0 * 6.066359519958496
Epoch 720, val loss: 0.8919782638549805
Epoch 730, training loss: 6.1727190017700195 = 0.11022871732711792 + 1.0 * 6.062490463256836
Epoch 730, val loss: 0.8987491726875305
Epoch 740, training loss: 6.162252902984619 = 0.10545731335878372 + 1.0 * 6.056795597076416
Epoch 740, val loss: 0.9057948589324951
Epoch 750, training loss: 6.157378673553467 = 0.10092892497777939 + 1.0 * 6.056449890136719
Epoch 750, val loss: 0.9129522442817688
Epoch 760, training loss: 6.151981830596924 = 0.09662462770938873 + 1.0 * 6.055356979370117
Epoch 760, val loss: 0.9201977849006653
Epoch 770, training loss: 6.159721851348877 = 0.09253603219985962 + 1.0 * 6.067185878753662
Epoch 770, val loss: 0.9273857474327087
Epoch 780, training loss: 6.143775939941406 = 0.08871068805456161 + 1.0 * 6.055065155029297
Epoch 780, val loss: 0.9348371028900146
Epoch 790, training loss: 6.138100624084473 = 0.08507438004016876 + 1.0 * 6.05302619934082
Epoch 790, val loss: 0.9424917697906494
Epoch 800, training loss: 6.134178161621094 = 0.08161167800426483 + 1.0 * 6.0525665283203125
Epoch 800, val loss: 0.9501268267631531
Epoch 810, training loss: 6.133757591247559 = 0.07832267880439758 + 1.0 * 6.055434703826904
Epoch 810, val loss: 0.9577045440673828
Epoch 820, training loss: 6.127999782562256 = 0.07521316409111023 + 1.0 * 6.052786827087402
Epoch 820, val loss: 0.9653351902961731
Epoch 830, training loss: 6.1236114501953125 = 0.07225975394248962 + 1.0 * 6.051351547241211
Epoch 830, val loss: 0.9730820059776306
Epoch 840, training loss: 6.121402740478516 = 0.0694553479552269 + 1.0 * 6.051947593688965
Epoch 840, val loss: 0.9807135462760925
Epoch 850, training loss: 6.1171088218688965 = 0.06679708510637283 + 1.0 * 6.05031156539917
Epoch 850, val loss: 0.988515317440033
Epoch 860, training loss: 6.11350679397583 = 0.06425555795431137 + 1.0 * 6.049251079559326
Epoch 860, val loss: 0.9961932301521301
Epoch 870, training loss: 6.109644889831543 = 0.061840202659368515 + 1.0 * 6.047804832458496
Epoch 870, val loss: 1.0038012266159058
Epoch 880, training loss: 6.107524871826172 = 0.05954301729798317 + 1.0 * 6.0479817390441895
Epoch 880, val loss: 1.0114778280258179
Epoch 890, training loss: 6.102006435394287 = 0.05735960602760315 + 1.0 * 6.044646739959717
Epoch 890, val loss: 1.0191313028335571
Epoch 900, training loss: 6.099940776824951 = 0.05527602881193161 + 1.0 * 6.0446648597717285
Epoch 900, val loss: 1.0268206596374512
Epoch 910, training loss: 6.102971076965332 = 0.0532882921397686 + 1.0 * 6.0496826171875
Epoch 910, val loss: 1.0344151258468628
Epoch 920, training loss: 6.10220193862915 = 0.05139818415045738 + 1.0 * 6.0508036613464355
Epoch 920, val loss: 1.0418328046798706
Epoch 930, training loss: 6.09359884262085 = 0.04960266128182411 + 1.0 * 6.043996334075928
Epoch 930, val loss: 1.0491396188735962
Epoch 940, training loss: 6.089565277099609 = 0.04789330065250397 + 1.0 * 6.0416717529296875
Epoch 940, val loss: 1.056625485420227
Epoch 950, training loss: 6.086731433868408 = 0.04625614732503891 + 1.0 * 6.040475368499756
Epoch 950, val loss: 1.0640016794204712
Epoch 960, training loss: 6.087007999420166 = 0.04468679055571556 + 1.0 * 6.04232120513916
Epoch 960, val loss: 1.071332573890686
Epoch 970, training loss: 6.083333492279053 = 0.0431906022131443 + 1.0 * 6.040143013000488
Epoch 970, val loss: 1.078442931175232
Epoch 980, training loss: 6.0804338455200195 = 0.041765522211790085 + 1.0 * 6.038668155670166
Epoch 980, val loss: 1.0857467651367188
Epoch 990, training loss: 6.085662364959717 = 0.04040096700191498 + 1.0 * 6.045261383056641
Epoch 990, val loss: 1.0930099487304688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.8635
Flip ASR: 0.8356/225 nodes
The final ASR:0.76753, 0.06949, Accuracy:0.80247, 0.02463
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9570])
updated graph: torch.Size([2, 10626])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97294, 0.00174, Accuracy:0.83580, 0.00761
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.31783676147461 = 1.944140076637268 + 1.0 * 8.373696327209473
Epoch 0, val loss: 1.95364511013031
Epoch 10, training loss: 10.306940078735352 = 1.9343317747116089 + 1.0 * 8.372608184814453
Epoch 10, val loss: 1.9439984560012817
Epoch 20, training loss: 10.287399291992188 = 1.922092080116272 + 1.0 * 8.365306854248047
Epoch 20, val loss: 1.9319102764129639
Epoch 30, training loss: 10.219797134399414 = 1.9059242010116577 + 1.0 * 8.313873291015625
Epoch 30, val loss: 1.9161241054534912
Epoch 40, training loss: 9.744667053222656 = 1.8890641927719116 + 1.0 * 7.855602741241455
Epoch 40, val loss: 1.9001820087432861
Epoch 50, training loss: 8.987923622131348 = 1.8725217580795288 + 1.0 * 7.115401744842529
Epoch 50, val loss: 1.8843196630477905
Epoch 60, training loss: 8.613481521606445 = 1.8600174188613892 + 1.0 * 6.7534637451171875
Epoch 60, val loss: 1.8717995882034302
Epoch 70, training loss: 8.447063446044922 = 1.848504662513733 + 1.0 * 6.5985589027404785
Epoch 70, val loss: 1.8603743314743042
Epoch 80, training loss: 8.338359832763672 = 1.8380714654922485 + 1.0 * 6.500288486480713
Epoch 80, val loss: 1.8499449491500854
Epoch 90, training loss: 8.26093578338623 = 1.8272372484207153 + 1.0 * 6.4336981773376465
Epoch 90, val loss: 1.8392678499221802
Epoch 100, training loss: 8.200294494628906 = 1.8160232305526733 + 1.0 * 6.384271621704102
Epoch 100, val loss: 1.8281879425048828
Epoch 110, training loss: 8.147505760192871 = 1.8049806356430054 + 1.0 * 6.342525005340576
Epoch 110, val loss: 1.8171532154083252
Epoch 120, training loss: 8.098739624023438 = 1.7943133115768433 + 1.0 * 6.304426193237305
Epoch 120, val loss: 1.8065358400344849
Epoch 130, training loss: 8.057847023010254 = 1.783729910850525 + 1.0 * 6.274117469787598
Epoch 130, val loss: 1.7961654663085938
Epoch 140, training loss: 8.02340316772461 = 1.77247953414917 + 1.0 * 6.2509236335754395
Epoch 140, val loss: 1.7856234312057495
Epoch 150, training loss: 7.988918781280518 = 1.7599525451660156 + 1.0 * 6.228966236114502
Epoch 150, val loss: 1.7745814323425293
Epoch 160, training loss: 7.957230567932129 = 1.745565414428711 + 1.0 * 6.211665153503418
Epoch 160, val loss: 1.7624038457870483
Epoch 170, training loss: 7.925743103027344 = 1.7285634279251099 + 1.0 * 6.197179794311523
Epoch 170, val loss: 1.7484409809112549
Epoch 180, training loss: 7.895931720733643 = 1.7082558870315552 + 1.0 * 6.187675952911377
Epoch 180, val loss: 1.7321465015411377
Epoch 190, training loss: 7.859282493591309 = 1.6840672492980957 + 1.0 * 6.175215244293213
Epoch 190, val loss: 1.7129840850830078
Epoch 200, training loss: 7.820586204528809 = 1.6548651456832886 + 1.0 * 6.1657209396362305
Epoch 200, val loss: 1.6899163722991943
Epoch 210, training loss: 7.779961585998535 = 1.6194652318954468 + 1.0 * 6.160496234893799
Epoch 210, val loss: 1.6618256568908691
Epoch 220, training loss: 7.7293596267700195 = 1.577467441558838 + 1.0 * 6.151892185211182
Epoch 220, val loss: 1.6288158893585205
Epoch 230, training loss: 7.67485237121582 = 1.5284833908081055 + 1.0 * 6.146368980407715
Epoch 230, val loss: 1.5901713371276855
Epoch 240, training loss: 7.6176652908325195 = 1.4727389812469482 + 1.0 * 6.14492654800415
Epoch 240, val loss: 1.5462443828582764
Epoch 250, training loss: 7.552849769592285 = 1.4129811525344849 + 1.0 * 6.13986873626709
Epoch 250, val loss: 1.4996061325073242
Epoch 260, training loss: 7.485924243927002 = 1.3506293296813965 + 1.0 * 6.1352949142456055
Epoch 260, val loss: 1.4506480693817139
Epoch 270, training loss: 7.421916961669922 = 1.2869253158569336 + 1.0 * 6.134991645812988
Epoch 270, val loss: 1.40091872215271
Epoch 280, training loss: 7.3524556159973145 = 1.2244205474853516 + 1.0 * 6.128035068511963
Epoch 280, val loss: 1.3525460958480835
Epoch 290, training loss: 7.2884979248046875 = 1.1642448902130127 + 1.0 * 6.124252796173096
Epoch 290, val loss: 1.3060647249221802
Epoch 300, training loss: 7.238739967346191 = 1.1073390245437622 + 1.0 * 6.131401062011719
Epoch 300, val loss: 1.2624493837356567
Epoch 310, training loss: 7.174384117126465 = 1.0559360980987549 + 1.0 * 6.118448257446289
Epoch 310, val loss: 1.2232880592346191
Epoch 320, training loss: 7.123373508453369 = 1.0087612867355347 + 1.0 * 6.114612102508545
Epoch 320, val loss: 1.1874974966049194
Epoch 330, training loss: 7.075062274932861 = 0.9647050499916077 + 1.0 * 6.110357284545898
Epoch 330, val loss: 1.154517412185669
Epoch 340, training loss: 7.032678127288818 = 0.9233708381652832 + 1.0 * 6.109307289123535
Epoch 340, val loss: 1.1238161325454712
Epoch 350, training loss: 6.9902024269104 = 0.8843643069267273 + 1.0 * 6.105838298797607
Epoch 350, val loss: 1.0953036546707153
Epoch 360, training loss: 6.949268341064453 = 0.8467447757720947 + 1.0 * 6.1025238037109375
Epoch 360, val loss: 1.0676549673080444
Epoch 370, training loss: 6.908877372741699 = 0.8094884753227234 + 1.0 * 6.09938907623291
Epoch 370, val loss: 1.0404449701309204
Epoch 380, training loss: 6.87322473526001 = 0.7724646329879761 + 1.0 * 6.100759983062744
Epoch 380, val loss: 1.0131428241729736
Epoch 390, training loss: 6.83189058303833 = 0.7357184290885925 + 1.0 * 6.096172332763672
Epoch 390, val loss: 0.986225962638855
Epoch 400, training loss: 6.7917351722717285 = 0.6987596750259399 + 1.0 * 6.092975616455078
Epoch 400, val loss: 0.9590438008308411
Epoch 410, training loss: 6.75334358215332 = 0.6614938378334045 + 1.0 * 6.0918498039245605
Epoch 410, val loss: 0.9315892457962036
Epoch 420, training loss: 6.716118335723877 = 0.6242150068283081 + 1.0 * 6.091903209686279
Epoch 420, val loss: 0.9042769074440002
Epoch 430, training loss: 6.675756931304932 = 0.5871258974075317 + 1.0 * 6.0886311531066895
Epoch 430, val loss: 0.8775225877761841
Epoch 440, training loss: 6.638948917388916 = 0.5501083731651306 + 1.0 * 6.088840484619141
Epoch 440, val loss: 0.8512986302375793
Epoch 450, training loss: 6.59853982925415 = 0.5134444236755371 + 1.0 * 6.085095405578613
Epoch 450, val loss: 0.825984001159668
Epoch 460, training loss: 6.563211917877197 = 0.47737863659858704 + 1.0 * 6.0858330726623535
Epoch 460, val loss: 0.8017816543579102
Epoch 470, training loss: 6.527971267700195 = 0.44245052337646484 + 1.0 * 6.0855207443237305
Epoch 470, val loss: 0.7794499397277832
Epoch 480, training loss: 6.488584041595459 = 0.40894779562950134 + 1.0 * 6.079636096954346
Epoch 480, val loss: 0.7591307759284973
Epoch 490, training loss: 6.4582037925720215 = 0.3768775165081024 + 1.0 * 6.081326484680176
Epoch 490, val loss: 0.7407764792442322
Epoch 500, training loss: 6.42457914352417 = 0.3466423749923706 + 1.0 * 6.07793664932251
Epoch 500, val loss: 0.7245521545410156
Epoch 510, training loss: 6.396668910980225 = 0.3182969391345978 + 1.0 * 6.078372001647949
Epoch 510, val loss: 0.710709273815155
Epoch 520, training loss: 6.365305423736572 = 0.2920512855052948 + 1.0 * 6.073254108428955
Epoch 520, val loss: 0.6989298462867737
Epoch 530, training loss: 6.34248161315918 = 0.26777058839797974 + 1.0 * 6.074710845947266
Epoch 530, val loss: 0.6891679763793945
Epoch 540, training loss: 6.32110595703125 = 0.2456469088792801 + 1.0 * 6.075459003448486
Epoch 540, val loss: 0.6814034581184387
Epoch 550, training loss: 6.295584201812744 = 0.225811168551445 + 1.0 * 6.069773197174072
Epoch 550, val loss: 0.675423264503479
Epoch 560, training loss: 6.27503776550293 = 0.20782513916492462 + 1.0 * 6.0672125816345215
Epoch 560, val loss: 0.6710341572761536
Epoch 570, training loss: 6.256393909454346 = 0.191519632935524 + 1.0 * 6.064874172210693
Epoch 570, val loss: 0.6679866909980774
Epoch 580, training loss: 6.253388404846191 = 0.1767968386411667 + 1.0 * 6.076591491699219
Epoch 580, val loss: 0.6661851406097412
Epoch 590, training loss: 6.2301225662231445 = 0.16381040215492249 + 1.0 * 6.066312313079834
Epoch 590, val loss: 0.6653850674629211
Epoch 600, training loss: 6.215169906616211 = 0.15221869945526123 + 1.0 * 6.06295108795166
Epoch 600, val loss: 0.6656840443611145
Epoch 610, training loss: 6.200750827789307 = 0.14174537360668182 + 1.0 * 6.059005260467529
Epoch 610, val loss: 0.6667768955230713
Epoch 620, training loss: 6.1897430419921875 = 0.1322423368692398 + 1.0 * 6.057500839233398
Epoch 620, val loss: 0.6686133146286011
Epoch 630, training loss: 6.187807083129883 = 0.1236567497253418 + 1.0 * 6.064150333404541
Epoch 630, val loss: 0.6710421442985535
Epoch 640, training loss: 6.172394275665283 = 0.11601429432630539 + 1.0 * 6.056379795074463
Epoch 640, val loss: 0.6740069389343262
Epoch 650, training loss: 6.164697647094727 = 0.10906651616096497 + 1.0 * 6.055631160736084
Epoch 650, val loss: 0.6775013208389282
Epoch 660, training loss: 6.156155586242676 = 0.10268401354551315 + 1.0 * 6.053471565246582
Epoch 660, val loss: 0.6813079118728638
Epoch 670, training loss: 6.149239540100098 = 0.09685143828392029 + 1.0 * 6.0523881912231445
Epoch 670, val loss: 0.6852980852127075
Epoch 680, training loss: 6.145633697509766 = 0.09153833985328674 + 1.0 * 6.054095268249512
Epoch 680, val loss: 0.6896877288818359
Epoch 690, training loss: 6.136401653289795 = 0.08664198219776154 + 1.0 * 6.049759864807129
Epoch 690, val loss: 0.6943145990371704
Epoch 700, training loss: 6.130221366882324 = 0.08210231363773346 + 1.0 * 6.048119068145752
Epoch 700, val loss: 0.6990910172462463
Epoch 710, training loss: 6.126673698425293 = 0.07788483053445816 + 1.0 * 6.048789024353027
Epoch 710, val loss: 0.7040706872940063
Epoch 720, training loss: 6.122994899749756 = 0.07399698346853256 + 1.0 * 6.04899787902832
Epoch 720, val loss: 0.7090893983840942
Epoch 730, training loss: 6.116078853607178 = 0.07042267918586731 + 1.0 * 6.045656204223633
Epoch 730, val loss: 0.7142835855484009
Epoch 740, training loss: 6.11488151550293 = 0.067095547914505 + 1.0 * 6.047785758972168
Epoch 740, val loss: 0.7195014953613281
Epoch 750, training loss: 6.107203483581543 = 0.06399846822023392 + 1.0 * 6.0432047843933105
Epoch 750, val loss: 0.7247170805931091
Epoch 760, training loss: 6.103374481201172 = 0.06111136078834534 + 1.0 * 6.042263031005859
Epoch 760, val loss: 0.7300910353660583
Epoch 770, training loss: 6.101718902587891 = 0.05840852111577988 + 1.0 * 6.043310165405273
Epoch 770, val loss: 0.7354164719581604
Epoch 780, training loss: 6.099728107452393 = 0.055888429284095764 + 1.0 * 6.043839454650879
Epoch 780, val loss: 0.7406044006347656
Epoch 790, training loss: 6.095832824707031 = 0.05355098843574524 + 1.0 * 6.042281627655029
Epoch 790, val loss: 0.7458431720733643
Epoch 800, training loss: 6.090277194976807 = 0.05135519802570343 + 1.0 * 6.03892183303833
Epoch 800, val loss: 0.7511294484138489
Epoch 810, training loss: 6.0953898429870605 = 0.049284059554338455 + 1.0 * 6.046105861663818
Epoch 810, val loss: 0.7562572360038757
Epoch 820, training loss: 6.088043689727783 = 0.04734478518366814 + 1.0 * 6.040699005126953
Epoch 820, val loss: 0.7613499164581299
Epoch 830, training loss: 6.082793712615967 = 0.045526761561632156 + 1.0 * 6.037266731262207
Epoch 830, val loss: 0.7665089964866638
Epoch 840, training loss: 6.079618453979492 = 0.043800465762615204 + 1.0 * 6.035818099975586
Epoch 840, val loss: 0.7716096639633179
Epoch 850, training loss: 6.090062618255615 = 0.04217025265097618 + 1.0 * 6.0478925704956055
Epoch 850, val loss: 0.7765610218048096
Epoch 860, training loss: 6.074779510498047 = 0.04065101221203804 + 1.0 * 6.034128665924072
Epoch 860, val loss: 0.7814012765884399
Epoch 870, training loss: 6.073679447174072 = 0.039211343973875046 + 1.0 * 6.034468173980713
Epoch 870, val loss: 0.786369264125824
Epoch 880, training loss: 6.0776543617248535 = 0.03783927485346794 + 1.0 * 6.0398149490356445
Epoch 880, val loss: 0.7911427617073059
Epoch 890, training loss: 6.070854187011719 = 0.036548275500535965 + 1.0 * 6.034306049346924
Epoch 890, val loss: 0.7958346605300903
Epoch 900, training loss: 6.0671706199646 = 0.035324301570653915 + 1.0 * 6.031846523284912
Epoch 900, val loss: 0.8005889654159546
Epoch 910, training loss: 6.067775249481201 = 0.03415679559111595 + 1.0 * 6.033618450164795
Epoch 910, val loss: 0.8052483201026917
Epoch 920, training loss: 6.06554651260376 = 0.03305182233452797 + 1.0 * 6.03249454498291
Epoch 920, val loss: 0.8096997737884521
Epoch 930, training loss: 6.0623674392700195 = 0.03200835362076759 + 1.0 * 6.030359268188477
Epoch 930, val loss: 0.8142905831336975
Epoch 940, training loss: 6.059170246124268 = 0.031012171879410744 + 1.0 * 6.028158187866211
Epoch 940, val loss: 0.8187833428382874
Epoch 950, training loss: 6.05892276763916 = 0.030054131522774696 + 1.0 * 6.028868675231934
Epoch 950, val loss: 0.8231635689735413
Epoch 960, training loss: 6.060484886169434 = 0.02914314903318882 + 1.0 * 6.031341552734375
Epoch 960, val loss: 0.827390730381012
Epoch 970, training loss: 6.0601654052734375 = 0.028285475447773933 + 1.0 * 6.031879901885986
Epoch 970, val loss: 0.8316750526428223
Epoch 980, training loss: 6.054496765136719 = 0.027465853840112686 + 1.0 * 6.027030944824219
Epoch 980, val loss: 0.8358395099639893
Epoch 990, training loss: 6.052481174468994 = 0.026683080941438675 + 1.0 * 6.025798320770264
Epoch 990, val loss: 0.840068519115448
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.315025329589844 = 1.9411845207214355 + 1.0 * 8.373841285705566
Epoch 0, val loss: 1.9465168714523315
Epoch 10, training loss: 10.305120468139648 = 1.9316576719284058 + 1.0 * 8.373462677001953
Epoch 10, val loss: 1.937097191810608
Epoch 20, training loss: 10.291409492492676 = 1.920119047164917 + 1.0 * 8.37129020690918
Epoch 20, val loss: 1.925480842590332
Epoch 30, training loss: 10.26040267944336 = 1.904447317123413 + 1.0 * 8.355955123901367
Epoch 30, val loss: 1.9097305536270142
Epoch 40, training loss: 10.118852615356445 = 1.8843015432357788 + 1.0 * 8.234551429748535
Epoch 40, val loss: 1.8899610042572021
Epoch 50, training loss: 9.334025382995605 = 1.8635265827178955 + 1.0 * 7.470499038696289
Epoch 50, val loss: 1.8703051805496216
Epoch 60, training loss: 8.956789016723633 = 1.8478612899780273 + 1.0 * 7.108928203582764
Epoch 60, val loss: 1.8563545942306519
Epoch 70, training loss: 8.672307968139648 = 1.8354194164276123 + 1.0 * 6.836888790130615
Epoch 70, val loss: 1.8445568084716797
Epoch 80, training loss: 8.492515563964844 = 1.8224629163742065 + 1.0 * 6.670052528381348
Epoch 80, val loss: 1.8319846391677856
Epoch 90, training loss: 8.380003929138184 = 1.8103952407836914 + 1.0 * 6.569608688354492
Epoch 90, val loss: 1.8205674886703491
Epoch 100, training loss: 8.292878150939941 = 1.7977906465530396 + 1.0 * 6.495087623596191
Epoch 100, val loss: 1.8091685771942139
Epoch 110, training loss: 8.224565505981445 = 1.7858185768127441 + 1.0 * 6.438747406005859
Epoch 110, val loss: 1.7984126806259155
Epoch 120, training loss: 8.1696138381958 = 1.7742383480072021 + 1.0 * 6.395375728607178
Epoch 120, val loss: 1.7877627611160278
Epoch 130, training loss: 8.126093864440918 = 1.761980414390564 + 1.0 * 6.364113807678223
Epoch 130, val loss: 1.7764586210250854
Epoch 140, training loss: 8.083142280578613 = 1.7482450008392334 + 1.0 * 6.334897518157959
Epoch 140, val loss: 1.7641173601150513
Epoch 150, training loss: 8.04515552520752 = 1.7323259115219116 + 1.0 * 6.312829494476318
Epoch 150, val loss: 1.7506381273269653
Epoch 160, training loss: 8.00770378112793 = 1.7136987447738647 + 1.0 * 6.294004917144775
Epoch 160, val loss: 1.735413670539856
Epoch 170, training loss: 7.968078136444092 = 1.6917213201522827 + 1.0 * 6.2763566970825195
Epoch 170, val loss: 1.7177183628082275
Epoch 180, training loss: 7.9257001876831055 = 1.6652562618255615 + 1.0 * 6.260444164276123
Epoch 180, val loss: 1.6965237855911255
Epoch 190, training loss: 7.879162311553955 = 1.6329940557479858 + 1.0 * 6.24616813659668
Epoch 190, val loss: 1.6706665754318237
Epoch 200, training loss: 7.829416275024414 = 1.5937656164169312 + 1.0 * 6.235650539398193
Epoch 200, val loss: 1.639196515083313
Epoch 210, training loss: 7.7711334228515625 = 1.5472792387008667 + 1.0 * 6.223854064941406
Epoch 210, val loss: 1.6019790172576904
Epoch 220, training loss: 7.7082600593566895 = 1.4930142164230347 + 1.0 * 6.215245723724365
Epoch 220, val loss: 1.5582243204116821
Epoch 230, training loss: 7.641190528869629 = 1.4312790632247925 + 1.0 * 6.209911346435547
Epoch 230, val loss: 1.5081766843795776
Epoch 240, training loss: 7.566312789916992 = 1.3648560047149658 + 1.0 * 6.201456546783447
Epoch 240, val loss: 1.4542156457901
Epoch 250, training loss: 7.492234230041504 = 1.2953674793243408 + 1.0 * 6.196866989135742
Epoch 250, val loss: 1.3977768421173096
Epoch 260, training loss: 7.417393207550049 = 1.2254563570022583 + 1.0 * 6.19193696975708
Epoch 260, val loss: 1.3410544395446777
Epoch 270, training loss: 7.342580318450928 = 1.1569236516952515 + 1.0 * 6.185656547546387
Epoch 270, val loss: 1.2854312658309937
Epoch 280, training loss: 7.272077560424805 = 1.0907295942306519 + 1.0 * 6.181347846984863
Epoch 280, val loss: 1.2317883968353271
Epoch 290, training loss: 7.207735061645508 = 1.0280873775482178 + 1.0 * 6.179647922515869
Epoch 290, val loss: 1.1814746856689453
Epoch 300, training loss: 7.143016338348389 = 0.9700927734375 + 1.0 * 6.172923564910889
Epoch 300, val loss: 1.1355220079421997
Epoch 310, training loss: 7.082839012145996 = 0.9153468608856201 + 1.0 * 6.167491912841797
Epoch 310, val loss: 1.0924925804138184
Epoch 320, training loss: 7.028842449188232 = 0.86373370885849 + 1.0 * 6.165108680725098
Epoch 320, val loss: 1.0524107217788696
Epoch 330, training loss: 6.9788498878479 = 0.8161224126815796 + 1.0 * 6.162727355957031
Epoch 330, val loss: 1.0158958435058594
Epoch 340, training loss: 6.927711009979248 = 0.7712732553482056 + 1.0 * 6.156437873840332
Epoch 340, val loss: 0.981453537940979
Epoch 350, training loss: 6.8806281089782715 = 0.7284305691719055 + 1.0 * 6.152197360992432
Epoch 350, val loss: 0.9487912654876709
Epoch 360, training loss: 6.840145111083984 = 0.6879700422286987 + 1.0 * 6.152174949645996
Epoch 360, val loss: 0.9180623292922974
Epoch 370, training loss: 6.797335624694824 = 0.6503825783729553 + 1.0 * 6.146953105926514
Epoch 370, val loss: 0.8896757960319519
Epoch 380, training loss: 6.756174564361572 = 0.6150913238525391 + 1.0 * 6.141083240509033
Epoch 380, val loss: 0.8632052540779114
Epoch 390, training loss: 6.731674671173096 = 0.5820898413658142 + 1.0 * 6.149584770202637
Epoch 390, val loss: 0.8387481570243835
Epoch 400, training loss: 6.689862251281738 = 0.5518602728843689 + 1.0 * 6.138001918792725
Epoch 400, val loss: 0.8169541954994202
Epoch 410, training loss: 6.655779838562012 = 0.5239464044570923 + 1.0 * 6.131833553314209
Epoch 410, val loss: 0.7974295616149902
Epoch 420, training loss: 6.6291632652282715 = 0.49801021814346313 + 1.0 * 6.131153106689453
Epoch 420, val loss: 0.7799417972564697
Epoch 430, training loss: 6.600760459899902 = 0.4741966724395752 + 1.0 * 6.126564025878906
Epoch 430, val loss: 0.7647449374198914
Epoch 440, training loss: 6.577781677246094 = 0.4522416591644287 + 1.0 * 6.125539779663086
Epoch 440, val loss: 0.7518001794815063
Epoch 450, training loss: 6.556873798370361 = 0.43162408471107483 + 1.0 * 6.125249862670898
Epoch 450, val loss: 0.740432620048523
Epoch 460, training loss: 6.534518718719482 = 0.4124423563480377 + 1.0 * 6.122076511383057
Epoch 460, val loss: 0.7304454445838928
Epoch 470, training loss: 6.510588645935059 = 0.3942568302154541 + 1.0 * 6.116332054138184
Epoch 470, val loss: 0.7219526767730713
Epoch 480, training loss: 6.491079807281494 = 0.3768399953842163 + 1.0 * 6.114239692687988
Epoch 480, val loss: 0.7144357562065125
Epoch 490, training loss: 6.472705841064453 = 0.36017632484436035 + 1.0 * 6.112529277801514
Epoch 490, val loss: 0.7077381610870361
Epoch 500, training loss: 6.456150054931641 = 0.3442152440547943 + 1.0 * 6.111934661865234
Epoch 500, val loss: 0.7019219398498535
Epoch 510, training loss: 6.43928337097168 = 0.3287227749824524 + 1.0 * 6.110560417175293
Epoch 510, val loss: 0.6966350078582764
Epoch 520, training loss: 6.421012878417969 = 0.3135901987552643 + 1.0 * 6.107422828674316
Epoch 520, val loss: 0.6918632984161377
Epoch 530, training loss: 6.4044952392578125 = 0.29876232147216797 + 1.0 * 6.1057329177856445
Epoch 530, val loss: 0.6874004006385803
Epoch 540, training loss: 6.388721466064453 = 0.28424257040023804 + 1.0 * 6.10447883605957
Epoch 540, val loss: 0.6833720207214355
Epoch 550, training loss: 6.370728969573975 = 0.2699848413467407 + 1.0 * 6.100744247436523
Epoch 550, val loss: 0.6796849966049194
Epoch 560, training loss: 6.355588436126709 = 0.2559646964073181 + 1.0 * 6.099623680114746
Epoch 560, val loss: 0.6762435436248779
Epoch 570, training loss: 6.350775241851807 = 0.24229520559310913 + 1.0 * 6.108479976654053
Epoch 570, val loss: 0.6731180548667908
Epoch 580, training loss: 6.328898906707764 = 0.2292371392250061 + 1.0 * 6.099661827087402
Epoch 580, val loss: 0.6703501343727112
Epoch 590, training loss: 6.311153411865234 = 0.21658426523208618 + 1.0 * 6.094569206237793
Epoch 590, val loss: 0.6680408716201782
Epoch 600, training loss: 6.296859264373779 = 0.20433461666107178 + 1.0 * 6.092524528503418
Epoch 600, val loss: 0.6659916043281555
Epoch 610, training loss: 6.289769172668457 = 0.19252215325832367 + 1.0 * 6.097247123718262
Epoch 610, val loss: 0.6642550230026245
Epoch 620, training loss: 6.2766008377075195 = 0.18134622275829315 + 1.0 * 6.095254421234131
Epoch 620, val loss: 0.6629208326339722
Epoch 630, training loss: 6.262028694152832 = 0.17081384360790253 + 1.0 * 6.091214656829834
Epoch 630, val loss: 0.6620544791221619
Epoch 640, training loss: 6.2486467361450195 = 0.1608230024576187 + 1.0 * 6.087823867797852
Epoch 640, val loss: 0.6615850925445557
Epoch 650, training loss: 6.246163845062256 = 0.15137290954589844 + 1.0 * 6.094790935516357
Epoch 650, val loss: 0.6614908576011658
Epoch 660, training loss: 6.230616569519043 = 0.14256566762924194 + 1.0 * 6.088050842285156
Epoch 660, val loss: 0.6616498827934265
Epoch 670, training loss: 6.219258785247803 = 0.1343458890914917 + 1.0 * 6.0849127769470215
Epoch 670, val loss: 0.6622633934020996
Epoch 680, training loss: 6.218579292297363 = 0.12662135064601898 + 1.0 * 6.091958045959473
Epoch 680, val loss: 0.6632439494132996
Epoch 690, training loss: 6.2075653076171875 = 0.11941055953502655 + 1.0 * 6.0881547927856445
Epoch 690, val loss: 0.6644389033317566
Epoch 700, training loss: 6.192842960357666 = 0.11271578073501587 + 1.0 * 6.080127239227295
Epoch 700, val loss: 0.6660106778144836
Epoch 710, training loss: 6.184678554534912 = 0.10642050206661224 + 1.0 * 6.078258037567139
Epoch 710, val loss: 0.6679105162620544
Epoch 720, training loss: 6.201609134674072 = 0.10054481029510498 + 1.0 * 6.101064205169678
Epoch 720, val loss: 0.669988214969635
Epoch 730, training loss: 6.175705909729004 = 0.09513738751411438 + 1.0 * 6.080568313598633
Epoch 730, val loss: 0.6722531318664551
Epoch 740, training loss: 6.165063381195068 = 0.09008818864822388 + 1.0 * 6.07497501373291
Epoch 740, val loss: 0.674826443195343
Epoch 750, training loss: 6.159845352172852 = 0.08535382896661758 + 1.0 * 6.074491500854492
Epoch 750, val loss: 0.6775981783866882
Epoch 760, training loss: 6.154569625854492 = 0.080908864736557 + 1.0 * 6.073660850524902
Epoch 760, val loss: 0.6805101037025452
Epoch 770, training loss: 6.1533050537109375 = 0.07677993923425674 + 1.0 * 6.0765252113342285
Epoch 770, val loss: 0.6834231019020081
Epoch 780, training loss: 6.1454386711120605 = 0.07297316193580627 + 1.0 * 6.072465419769287
Epoch 780, val loss: 0.6864976286888123
Epoch 790, training loss: 6.139749526977539 = 0.06941179931163788 + 1.0 * 6.070337772369385
Epoch 790, val loss: 0.6897831559181213
Epoch 800, training loss: 6.135609149932861 = 0.0660615935921669 + 1.0 * 6.069547653198242
Epoch 800, val loss: 0.693156361579895
Epoch 810, training loss: 6.141744613647461 = 0.06292340159416199 + 1.0 * 6.078821182250977
Epoch 810, val loss: 0.6965680122375488
Epoch 820, training loss: 6.132754802703857 = 0.060003943741321564 + 1.0 * 6.072751045227051
Epoch 820, val loss: 0.6999614834785461
Epoch 830, training loss: 6.126342296600342 = 0.057286836206912994 + 1.0 * 6.069055557250977
Epoch 830, val loss: 0.7034307718276978
Epoch 840, training loss: 6.119884490966797 = 0.05474095046520233 + 1.0 * 6.065143585205078
Epoch 840, val loss: 0.7070286870002747
Epoch 850, training loss: 6.1177144050598145 = 0.0523357167840004 + 1.0 * 6.065378665924072
Epoch 850, val loss: 0.7106867432594299
Epoch 860, training loss: 6.122011184692383 = 0.05006835237145424 + 1.0 * 6.0719428062438965
Epoch 860, val loss: 0.7143823504447937
Epoch 870, training loss: 6.121922492980957 = 0.04794861748814583 + 1.0 * 6.073973655700684
Epoch 870, val loss: 0.7179175019264221
Epoch 880, training loss: 6.108220100402832 = 0.045981429517269135 + 1.0 * 6.062238693237305
Epoch 880, val loss: 0.7215539216995239
Epoch 890, training loss: 6.10711669921875 = 0.04412185028195381 + 1.0 * 6.062994956970215
Epoch 890, val loss: 0.72526615858078
Epoch 900, training loss: 6.104103088378906 = 0.04235726222395897 + 1.0 * 6.061745643615723
Epoch 900, val loss: 0.7289880514144897
Epoch 910, training loss: 6.106065273284912 = 0.04068930447101593 + 1.0 * 6.065375804901123
Epoch 910, val loss: 0.7326773405075073
Epoch 920, training loss: 6.100171089172363 = 0.03912060707807541 + 1.0 * 6.0610504150390625
Epoch 920, val loss: 0.7363374829292297
Epoch 930, training loss: 6.095451831817627 = 0.03763483837246895 + 1.0 * 6.057816982269287
Epoch 930, val loss: 0.740079402923584
Epoch 940, training loss: 6.099418640136719 = 0.03622113913297653 + 1.0 * 6.063197612762451
Epoch 940, val loss: 0.7437996864318848
Epoch 950, training loss: 6.102382183074951 = 0.03489261120557785 + 1.0 * 6.0674896240234375
Epoch 950, val loss: 0.7473415732383728
Epoch 960, training loss: 6.08968448638916 = 0.03365525230765343 + 1.0 * 6.056029319763184
Epoch 960, val loss: 0.750954806804657
Epoch 970, training loss: 6.088841915130615 = 0.032477542757987976 + 1.0 * 6.0563645362854
Epoch 970, val loss: 0.754608154296875
Epoch 980, training loss: 6.0861053466796875 = 0.03134884685277939 + 1.0 * 6.0547566413879395
Epoch 980, val loss: 0.7582662105560303
Epoch 990, training loss: 6.093810558319092 = 0.030276980251073837 + 1.0 * 6.063533782958984
Epoch 990, val loss: 0.7618442177772522
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.31967830657959 = 1.9458436965942383 + 1.0 * 8.373834609985352
Epoch 0, val loss: 1.9541786909103394
Epoch 10, training loss: 10.30960750579834 = 1.936287522315979 + 1.0 * 8.373319625854492
Epoch 10, val loss: 1.9443576335906982
Epoch 20, training loss: 10.2945556640625 = 1.9243485927581787 + 1.0 * 8.370206832885742
Epoch 20, val loss: 1.931894063949585
Epoch 30, training loss: 10.257769584655762 = 1.9075143337249756 + 1.0 * 8.350255012512207
Epoch 30, val loss: 1.9143943786621094
Epoch 40, training loss: 10.106498718261719 = 1.8857120275497437 + 1.0 * 8.220787048339844
Epoch 40, val loss: 1.8926640748977661
Epoch 50, training loss: 9.557286262512207 = 1.860926866531372 + 1.0 * 7.696359634399414
Epoch 50, val loss: 1.8685007095336914
Epoch 60, training loss: 9.109155654907227 = 1.839604139328003 + 1.0 * 7.269551753997803
Epoch 60, val loss: 1.8494952917099
Epoch 70, training loss: 8.715825080871582 = 1.824084997177124 + 1.0 * 6.891740322113037
Epoch 70, val loss: 1.8352062702178955
Epoch 80, training loss: 8.528523445129395 = 1.8083983659744263 + 1.0 * 6.720125198364258
Epoch 80, val loss: 1.820073127746582
Epoch 90, training loss: 8.420476913452148 = 1.791506290435791 + 1.0 * 6.628971099853516
Epoch 90, val loss: 1.8040188550949097
Epoch 100, training loss: 8.336244583129883 = 1.7738251686096191 + 1.0 * 6.5624189376831055
Epoch 100, val loss: 1.7875467538833618
Epoch 110, training loss: 8.25523567199707 = 1.7567135095596313 + 1.0 * 6.49852180480957
Epoch 110, val loss: 1.7718408107757568
Epoch 120, training loss: 8.188226699829102 = 1.7393031120300293 + 1.0 * 6.448923110961914
Epoch 120, val loss: 1.7563046216964722
Epoch 130, training loss: 8.124337196350098 = 1.719688057899475 + 1.0 * 6.404649257659912
Epoch 130, val loss: 1.7395910024642944
Epoch 140, training loss: 8.067224502563477 = 1.6964776515960693 + 1.0 * 6.370747089385986
Epoch 140, val loss: 1.720431923866272
Epoch 150, training loss: 8.012471199035645 = 1.6689956188201904 + 1.0 * 6.343475818634033
Epoch 150, val loss: 1.697986125946045
Epoch 160, training loss: 7.957192420959473 = 1.6363617181777954 + 1.0 * 6.320830821990967
Epoch 160, val loss: 1.6714694499969482
Epoch 170, training loss: 7.901647090911865 = 1.5979704856872559 + 1.0 * 6.303676605224609
Epoch 170, val loss: 1.6404262781143188
Epoch 180, training loss: 7.838615417480469 = 1.553999900817871 + 1.0 * 6.284615516662598
Epoch 180, val loss: 1.6050795316696167
Epoch 190, training loss: 7.772852897644043 = 1.5046677589416504 + 1.0 * 6.268185138702393
Epoch 190, val loss: 1.5652680397033691
Epoch 200, training loss: 7.703658580780029 = 1.4497703313827515 + 1.0 * 6.253888130187988
Epoch 200, val loss: 1.5210293531417847
Epoch 210, training loss: 7.635549545288086 = 1.3903584480285645 + 1.0 * 6.2451910972595215
Epoch 210, val loss: 1.473314881324768
Epoch 220, training loss: 7.56160831451416 = 1.328852891921997 + 1.0 * 6.232755661010742
Epoch 220, val loss: 1.4243438243865967
Epoch 230, training loss: 7.490049839019775 = 1.2662105560302734 + 1.0 * 6.223839282989502
Epoch 230, val loss: 1.374564528465271
Epoch 240, training loss: 7.421319484710693 = 1.2034927606582642 + 1.0 * 6.217826843261719
Epoch 240, val loss: 1.32512366771698
Epoch 250, training loss: 7.352088451385498 = 1.1429311037063599 + 1.0 * 6.209157466888428
Epoch 250, val loss: 1.2779390811920166
Epoch 260, training loss: 7.287761688232422 = 1.0851486921310425 + 1.0 * 6.20261287689209
Epoch 260, val loss: 1.233516812324524
Epoch 270, training loss: 7.225826263427734 = 1.0299190282821655 + 1.0 * 6.195907115936279
Epoch 270, val loss: 1.1915404796600342
Epoch 280, training loss: 7.171235084533691 = 0.9773519039154053 + 1.0 * 6.193883419036865
Epoch 280, val loss: 1.151857852935791
Epoch 290, training loss: 7.114184856414795 = 0.9277791380882263 + 1.0 * 6.186405658721924
Epoch 290, val loss: 1.1147762537002563
Epoch 300, training loss: 7.0612664222717285 = 0.8804396390914917 + 1.0 * 6.180826663970947
Epoch 300, val loss: 1.0793702602386475
Epoch 310, training loss: 7.009618759155273 = 0.8342342972755432 + 1.0 * 6.175384521484375
Epoch 310, val loss: 1.044648289680481
Epoch 320, training loss: 6.964319705963135 = 0.7887389063835144 + 1.0 * 6.175580978393555
Epoch 320, val loss: 1.010118842124939
Epoch 330, training loss: 6.913918495178223 = 0.7446111440658569 + 1.0 * 6.169307231903076
Epoch 330, val loss: 0.9762636423110962
Epoch 340, training loss: 6.866408348083496 = 0.7019602656364441 + 1.0 * 6.164448261260986
Epoch 340, val loss: 0.9435262680053711
Epoch 350, training loss: 6.8203582763671875 = 0.6604065299034119 + 1.0 * 6.159951686859131
Epoch 350, val loss: 0.9114235043525696
Epoch 360, training loss: 6.784031867980957 = 0.6202064156532288 + 1.0 * 6.163825511932373
Epoch 360, val loss: 0.8804174065589905
Epoch 370, training loss: 6.73625373840332 = 0.5822300910949707 + 1.0 * 6.15402364730835
Epoch 370, val loss: 0.8512860536575317
Epoch 380, training loss: 6.696581840515137 = 0.5464060306549072 + 1.0 * 6.150176048278809
Epoch 380, val loss: 0.8244906663894653
Epoch 390, training loss: 6.659668922424316 = 0.5126423835754395 + 1.0 * 6.147026538848877
Epoch 390, val loss: 0.7999600172042847
Epoch 400, training loss: 6.6267242431640625 = 0.4810599982738495 + 1.0 * 6.145664215087891
Epoch 400, val loss: 0.7779945731163025
Epoch 410, training loss: 6.596072673797607 = 0.45180171728134155 + 1.0 * 6.144270896911621
Epoch 410, val loss: 0.7588313817977905
Epoch 420, training loss: 6.562540054321289 = 0.42426908016204834 + 1.0 * 6.138270854949951
Epoch 420, val loss: 0.7419795989990234
Epoch 430, training loss: 6.534930229187012 = 0.3980710208415985 + 1.0 * 6.13685941696167
Epoch 430, val loss: 0.7271312475204468
Epoch 440, training loss: 6.511150360107422 = 0.37318989634513855 + 1.0 * 6.137960433959961
Epoch 440, val loss: 0.714232325553894
Epoch 450, training loss: 6.480869770050049 = 0.3495255410671234 + 1.0 * 6.131344318389893
Epoch 450, val loss: 0.7031416296958923
Epoch 460, training loss: 6.454801559448242 = 0.3267347514629364 + 1.0 * 6.1280670166015625
Epoch 460, val loss: 0.6934590339660645
Epoch 470, training loss: 6.444969177246094 = 0.3047815263271332 + 1.0 * 6.140187740325928
Epoch 470, val loss: 0.6850027441978455
Epoch 480, training loss: 6.411920547485352 = 0.28388574719429016 + 1.0 * 6.128034591674805
Epoch 480, val loss: 0.6779101490974426
Epoch 490, training loss: 6.385777950286865 = 0.26400259137153625 + 1.0 * 6.121775150299072
Epoch 490, val loss: 0.6721769571304321
Epoch 500, training loss: 6.3643574714660645 = 0.24504753947257996 + 1.0 * 6.119309902191162
Epoch 500, val loss: 0.6675496697425842
Epoch 510, training loss: 6.344330787658691 = 0.2270490974187851 + 1.0 * 6.117281913757324
Epoch 510, val loss: 0.664166271686554
Epoch 520, training loss: 6.328860282897949 = 0.21019434928894043 + 1.0 * 6.118666172027588
Epoch 520, val loss: 0.6619009971618652
Epoch 530, training loss: 6.311917781829834 = 0.19467462599277496 + 1.0 * 6.11724328994751
Epoch 530, val loss: 0.6609349846839905
Epoch 540, training loss: 6.294623851776123 = 0.18043576180934906 + 1.0 * 6.114188194274902
Epoch 540, val loss: 0.661094069480896
Epoch 550, training loss: 6.278931140899658 = 0.16738936305046082 + 1.0 * 6.111541748046875
Epoch 550, val loss: 0.6623128056526184
Epoch 560, training loss: 6.264245986938477 = 0.15547531843185425 + 1.0 * 6.108770847320557
Epoch 560, val loss: 0.6644801497459412
Epoch 570, training loss: 6.252536773681641 = 0.14462952315807343 + 1.0 * 6.107907295227051
Epoch 570, val loss: 0.6675161123275757
Epoch 580, training loss: 6.240694522857666 = 0.13476574420928955 + 1.0 * 6.105928897857666
Epoch 580, val loss: 0.6713578104972839
Epoch 590, training loss: 6.25017786026001 = 0.1257462501525879 + 1.0 * 6.124431610107422
Epoch 590, val loss: 0.6758078932762146
Epoch 600, training loss: 6.222541332244873 = 0.11766549944877625 + 1.0 * 6.1048760414123535
Epoch 600, val loss: 0.6806390881538391
Epoch 610, training loss: 6.210813522338867 = 0.11028160899877548 + 1.0 * 6.100532054901123
Epoch 610, val loss: 0.686019241809845
Epoch 620, training loss: 6.201533794403076 = 0.10348032414913177 + 1.0 * 6.098053455352783
Epoch 620, val loss: 0.6917646527290344
Epoch 630, training loss: 6.193116664886475 = 0.09719766676425934 + 1.0 * 6.095919132232666
Epoch 630, val loss: 0.6979230642318726
Epoch 640, training loss: 6.197495937347412 = 0.09140834212303162 + 1.0 * 6.106087684631348
Epoch 640, val loss: 0.7043421268463135
Epoch 650, training loss: 6.182989120483398 = 0.08612795174121857 + 1.0 * 6.096861362457275
Epoch 650, val loss: 0.7108942270278931
Epoch 660, training loss: 6.174462795257568 = 0.08125974237918854 + 1.0 * 6.093203067779541
Epoch 660, val loss: 0.7176538109779358
Epoch 670, training loss: 6.171866416931152 = 0.0767502561211586 + 1.0 * 6.095116138458252
Epoch 670, val loss: 0.7244664430618286
Epoch 680, training loss: 6.161930084228516 = 0.07257295399904251 + 1.0 * 6.089356899261475
Epoch 680, val loss: 0.7314174175262451
Epoch 690, training loss: 6.158217430114746 = 0.06869185715913773 + 1.0 * 6.0895256996154785
Epoch 690, val loss: 0.7385568618774414
Epoch 700, training loss: 6.153158187866211 = 0.06509405374526978 + 1.0 * 6.088064193725586
Epoch 700, val loss: 0.7456258535385132
Epoch 710, training loss: 6.147220134735107 = 0.06176441162824631 + 1.0 * 6.085455894470215
Epoch 710, val loss: 0.7527379989624023
Epoch 720, training loss: 6.142834663391113 = 0.058658383786678314 + 1.0 * 6.084176063537598
Epoch 720, val loss: 0.7599838972091675
Epoch 730, training loss: 6.150538444519043 = 0.05576582998037338 + 1.0 * 6.094772815704346
Epoch 730, val loss: 0.7670820355415344
Epoch 740, training loss: 6.136518478393555 = 0.053081244230270386 + 1.0 * 6.083437442779541
Epoch 740, val loss: 0.7740895748138428
Epoch 750, training loss: 6.132115364074707 = 0.0505913607776165 + 1.0 * 6.081523895263672
Epoch 750, val loss: 0.7811039090156555
Epoch 760, training loss: 6.127800464630127 = 0.048254698514938354 + 1.0 * 6.079545974731445
Epoch 760, val loss: 0.788092851638794
Epoch 770, training loss: 6.130770683288574 = 0.04606315866112709 + 1.0 * 6.084707736968994
Epoch 770, val loss: 0.7949742674827576
Epoch 780, training loss: 6.124070167541504 = 0.04403004050254822 + 1.0 * 6.080039978027344
Epoch 780, val loss: 0.8017045259475708
Epoch 790, training loss: 6.118899822235107 = 0.04213560372591019 + 1.0 * 6.076764106750488
Epoch 790, val loss: 0.8084257245063782
Epoch 800, training loss: 6.117269515991211 = 0.04035127907991409 + 1.0 * 6.076918125152588
Epoch 800, val loss: 0.8150207996368408
Epoch 810, training loss: 6.114642143249512 = 0.038675930351018906 + 1.0 * 6.0759663581848145
Epoch 810, val loss: 0.821475625038147
Epoch 820, training loss: 6.114579677581787 = 0.037106309086084366 + 1.0 * 6.077473163604736
Epoch 820, val loss: 0.8278595209121704
Epoch 830, training loss: 6.108405113220215 = 0.03563126549124718 + 1.0 * 6.0727739334106445
Epoch 830, val loss: 0.8340687155723572
Epoch 840, training loss: 6.106818199157715 = 0.03424562141299248 + 1.0 * 6.072572708129883
Epoch 840, val loss: 0.8402818441390991
Epoch 850, training loss: 6.11078405380249 = 0.03293883055448532 + 1.0 * 6.077845096588135
Epoch 850, val loss: 0.8462623953819275
Epoch 860, training loss: 6.102645397186279 = 0.0317128486931324 + 1.0 * 6.070932388305664
Epoch 860, val loss: 0.8521892428398132
Epoch 870, training loss: 6.109730243682861 = 0.030561063438653946 + 1.0 * 6.079169273376465
Epoch 870, val loss: 0.8580344319343567
Epoch 880, training loss: 6.100577354431152 = 0.029468826949596405 + 1.0 * 6.071108341217041
Epoch 880, val loss: 0.8635836839675903
Epoch 890, training loss: 6.09492301940918 = 0.0284427423030138 + 1.0 * 6.0664801597595215
Epoch 890, val loss: 0.8692781329154968
Epoch 900, training loss: 6.0924859046936035 = 0.02746330201625824 + 1.0 * 6.0650224685668945
Epoch 900, val loss: 0.8748728036880493
Epoch 910, training loss: 6.098121166229248 = 0.026527559384703636 + 1.0 * 6.071593761444092
Epoch 910, val loss: 0.8804033398628235
Epoch 920, training loss: 6.095882415771484 = 0.02565395087003708 + 1.0 * 6.070228576660156
Epoch 920, val loss: 0.8855710625648499
Epoch 930, training loss: 6.0869574546813965 = 0.024824392050504684 + 1.0 * 6.062132835388184
Epoch 930, val loss: 0.8908388614654541
Epoch 940, training loss: 6.086376667022705 = 0.024034732952713966 + 1.0 * 6.062342166900635
Epoch 940, val loss: 0.896045446395874
Epoch 950, training loss: 6.085959434509277 = 0.02327900193631649 + 1.0 * 6.062680244445801
Epoch 950, val loss: 0.9011712670326233
Epoch 960, training loss: 6.083225727081299 = 0.022558538243174553 + 1.0 * 6.060667037963867
Epoch 960, val loss: 0.9061222672462463
Epoch 970, training loss: 6.082943916320801 = 0.021878890693187714 + 1.0 * 6.061065196990967
Epoch 970, val loss: 0.9110972285270691
Epoch 980, training loss: 6.090282917022705 = 0.021231193095445633 + 1.0 * 6.069051742553711
Epoch 980, val loss: 0.9159965515136719
Epoch 990, training loss: 6.08306884765625 = 0.02061026729643345 + 1.0 * 6.062458515167236
Epoch 990, val loss: 0.9205590486526489
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.299324035644531 = 1.9255281686782837 + 1.0 * 8.373795509338379
Epoch 0, val loss: 1.9286268949508667
Epoch 10, training loss: 10.289260864257812 = 1.9161953926086426 + 1.0 * 8.373065948486328
Epoch 10, val loss: 1.9191185235977173
Epoch 20, training loss: 10.272876739501953 = 1.9047703742980957 + 1.0 * 8.368106842041016
Epoch 20, val loss: 1.9071874618530273
Epoch 30, training loss: 10.221189498901367 = 1.8895608186721802 + 1.0 * 8.331628799438477
Epoch 30, val loss: 1.8913865089416504
Epoch 40, training loss: 9.913389205932617 = 1.8717139959335327 + 1.0 * 8.041675567626953
Epoch 40, val loss: 1.8735754489898682
Epoch 50, training loss: 9.355445861816406 = 1.8519213199615479 + 1.0 * 7.5035247802734375
Epoch 50, val loss: 1.854547381401062
Epoch 60, training loss: 8.913751602172852 = 1.8372485637664795 + 1.0 * 7.076503276824951
Epoch 60, val loss: 1.8411246538162231
Epoch 70, training loss: 8.605361938476562 = 1.8258004188537598 + 1.0 * 6.779561519622803
Epoch 70, val loss: 1.8304505348205566
Epoch 80, training loss: 8.457630157470703 = 1.8147670030593872 + 1.0 * 6.642862796783447
Epoch 80, val loss: 1.819954514503479
Epoch 90, training loss: 8.352413177490234 = 1.8022139072418213 + 1.0 * 6.550199031829834
Epoch 90, val loss: 1.8081657886505127
Epoch 100, training loss: 8.271262168884277 = 1.7893844842910767 + 1.0 * 6.481877326965332
Epoch 100, val loss: 1.7964712381362915
Epoch 110, training loss: 8.21001148223877 = 1.7765191793441772 + 1.0 * 6.433492183685303
Epoch 110, val loss: 1.7849878072738647
Epoch 120, training loss: 8.161442756652832 = 1.762571930885315 + 1.0 * 6.398870468139648
Epoch 120, val loss: 1.7725721597671509
Epoch 130, training loss: 8.115943908691406 = 1.7468435764312744 + 1.0 * 6.369100093841553
Epoch 130, val loss: 1.7588293552398682
Epoch 140, training loss: 8.071310997009277 = 1.729053258895874 + 1.0 * 6.342257499694824
Epoch 140, val loss: 1.7434439659118652
Epoch 150, training loss: 8.03055191040039 = 1.7083427906036377 + 1.0 * 6.322208881378174
Epoch 150, val loss: 1.7258330583572388
Epoch 160, training loss: 7.980841636657715 = 1.6841808557510376 + 1.0 * 6.296660900115967
Epoch 160, val loss: 1.7054924964904785
Epoch 170, training loss: 7.933150768280029 = 1.6554688215255737 + 1.0 * 6.277681827545166
Epoch 170, val loss: 1.681387186050415
Epoch 180, training loss: 7.881350517272949 = 1.6207973957061768 + 1.0 * 6.260552883148193
Epoch 180, val loss: 1.6522871255874634
Epoch 190, training loss: 7.8266401290893555 = 1.5792368650436401 + 1.0 * 6.247403144836426
Epoch 190, val loss: 1.6173856258392334
Epoch 200, training loss: 7.763661861419678 = 1.5306038856506348 + 1.0 * 6.233057975769043
Epoch 200, val loss: 1.5764366388320923
Epoch 210, training loss: 7.6963114738464355 = 1.4742594957351685 + 1.0 * 6.222052097320557
Epoch 210, val loss: 1.5289664268493652
Epoch 220, training loss: 7.623056411743164 = 1.4102967977523804 + 1.0 * 6.212759494781494
Epoch 220, val loss: 1.4750895500183105
Epoch 230, training loss: 7.550444602966309 = 1.3409748077392578 + 1.0 * 6.209469795227051
Epoch 230, val loss: 1.4171931743621826
Epoch 240, training loss: 7.469501495361328 = 1.2702072858810425 + 1.0 * 6.199294090270996
Epoch 240, val loss: 1.358223557472229
Epoch 250, training loss: 7.390430927276611 = 1.198934555053711 + 1.0 * 6.1914963722229
Epoch 250, val loss: 1.299452304840088
Epoch 260, training loss: 7.3143415451049805 = 1.1288634538650513 + 1.0 * 6.185478210449219
Epoch 260, val loss: 1.2422878742218018
Epoch 270, training loss: 7.244625091552734 = 1.062123417854309 + 1.0 * 6.182501792907715
Epoch 270, val loss: 1.1886987686157227
Epoch 280, training loss: 7.1751227378845215 = 1.0004620552062988 + 1.0 * 6.174660682678223
Epoch 280, val loss: 1.1399990320205688
Epoch 290, training loss: 7.1127095222473145 = 0.9426246881484985 + 1.0 * 6.1700849533081055
Epoch 290, val loss: 1.0950058698654175
Epoch 300, training loss: 7.054253101348877 = 0.8881567716598511 + 1.0 * 6.166096210479736
Epoch 300, val loss: 1.0532852411270142
Epoch 310, training loss: 6.997724533081055 = 0.8373783826828003 + 1.0 * 6.160346031188965
Epoch 310, val loss: 1.014918565750122
Epoch 320, training loss: 6.945462703704834 = 0.7892361879348755 + 1.0 * 6.156226634979248
Epoch 320, val loss: 0.9791020154953003
Epoch 330, training loss: 6.8962578773498535 = 0.7437456250190735 + 1.0 * 6.152512073516846
Epoch 330, val loss: 0.945625901222229
Epoch 340, training loss: 6.8501877784729 = 0.7011891007423401 + 1.0 * 6.148998737335205
Epoch 340, val loss: 0.9148512482643127
Epoch 350, training loss: 6.8061909675598145 = 0.6610288619995117 + 1.0 * 6.145162105560303
Epoch 350, val loss: 0.8864672183990479
Epoch 360, training loss: 6.768677234649658 = 0.6233996152877808 + 1.0 * 6.145277500152588
Epoch 360, val loss: 0.8604153394699097
Epoch 370, training loss: 6.727236747741699 = 0.5883869528770447 + 1.0 * 6.13884973526001
Epoch 370, val loss: 0.8369835615158081
Epoch 380, training loss: 6.69238805770874 = 0.5554621815681458 + 1.0 * 6.13692569732666
Epoch 380, val loss: 0.8158839344978333
Epoch 390, training loss: 6.656201362609863 = 0.5246407985687256 + 1.0 * 6.131560325622559
Epoch 390, val loss: 0.7970268130302429
Epoch 400, training loss: 6.624209880828857 = 0.49572044610977173 + 1.0 * 6.1284894943237305
Epoch 400, val loss: 0.7804314494132996
Epoch 410, training loss: 6.595544338226318 = 0.4684423506259918 + 1.0 * 6.127101898193359
Epoch 410, val loss: 0.765731692314148
Epoch 420, training loss: 6.573549747467041 = 0.4428330957889557 + 1.0 * 6.130716800689697
Epoch 420, val loss: 0.7527413368225098
Epoch 430, training loss: 6.541589260101318 = 0.4189247786998749 + 1.0 * 6.122664451599121
Epoch 430, val loss: 0.7417162656784058
Epoch 440, training loss: 6.517411231994629 = 0.3963458836078644 + 1.0 * 6.121065139770508
Epoch 440, val loss: 0.732253909111023
Epoch 450, training loss: 6.490489482879639 = 0.37496116757392883 + 1.0 * 6.115528106689453
Epoch 450, val loss: 0.7240464091300964
Epoch 460, training loss: 6.467001914978027 = 0.35462531447410583 + 1.0 * 6.112376689910889
Epoch 460, val loss: 0.7170928120613098
Epoch 470, training loss: 6.452179908752441 = 0.33525389432907104 + 1.0 * 6.116926193237305
Epoch 470, val loss: 0.7111570835113525
Epoch 480, training loss: 6.426918983459473 = 0.31699225306510925 + 1.0 * 6.109926700592041
Epoch 480, val loss: 0.706373393535614
Epoch 490, training loss: 6.406018257141113 = 0.29953593015670776 + 1.0 * 6.10648250579834
Epoch 490, val loss: 0.7025856971740723
Epoch 500, training loss: 6.39021110534668 = 0.282889723777771 + 1.0 * 6.107321262359619
Epoch 500, val loss: 0.699487566947937
Epoch 510, training loss: 6.3717360496521 = 0.2670842409133911 + 1.0 * 6.104651927947998
Epoch 510, val loss: 0.6972371339797974
Epoch 520, training loss: 6.355494499206543 = 0.2520052492618561 + 1.0 * 6.103489398956299
Epoch 520, val loss: 0.6957882642745972
Epoch 530, training loss: 6.338322162628174 = 0.23765650391578674 + 1.0 * 6.10066556930542
Epoch 530, val loss: 0.6950350999832153
Epoch 540, training loss: 6.323080062866211 = 0.22398483753204346 + 1.0 * 6.099095344543457
Epoch 540, val loss: 0.6949086785316467
Epoch 550, training loss: 6.30851936340332 = 0.2110341489315033 + 1.0 * 6.097485065460205
Epoch 550, val loss: 0.6953031420707703
Epoch 560, training loss: 6.293430328369141 = 0.1987839788198471 + 1.0 * 6.094646453857422
Epoch 560, val loss: 0.6963070631027222
Epoch 570, training loss: 6.279617786407471 = 0.18715359270572662 + 1.0 * 6.092463970184326
Epoch 570, val loss: 0.6977338194847107
Epoch 580, training loss: 6.270813465118408 = 0.17614686489105225 + 1.0 * 6.094666481018066
Epoch 580, val loss: 0.6994451284408569
Epoch 590, training loss: 6.259885787963867 = 0.16578814387321472 + 1.0 * 6.09409761428833
Epoch 590, val loss: 0.7015330195426941
Epoch 600, training loss: 6.244877815246582 = 0.15605053305625916 + 1.0 * 6.088827133178711
Epoch 600, val loss: 0.7038853168487549
Epoch 610, training loss: 6.233092308044434 = 0.1469241827726364 + 1.0 * 6.08616828918457
Epoch 610, val loss: 0.7066423296928406
Epoch 620, training loss: 6.222846031188965 = 0.13833481073379517 + 1.0 * 6.0845112800598145
Epoch 620, val loss: 0.7095794081687927
Epoch 630, training loss: 6.218310832977295 = 0.13028381764888763 + 1.0 * 6.088027000427246
Epoch 630, val loss: 0.7126153111457825
Epoch 640, training loss: 6.204758644104004 = 0.12279807031154633 + 1.0 * 6.081960678100586
Epoch 640, val loss: 0.7159603238105774
Epoch 650, training loss: 6.2072601318359375 = 0.1157902255654335 + 1.0 * 6.091469764709473
Epoch 650, val loss: 0.7194543480873108
Epoch 660, training loss: 6.189769744873047 = 0.10930055379867554 + 1.0 * 6.080469131469727
Epoch 660, val loss: 0.7228626608848572
Epoch 670, training loss: 6.181271076202393 = 0.1032615453004837 + 1.0 * 6.078009605407715
Epoch 670, val loss: 0.7265719175338745
Epoch 680, training loss: 6.17440938949585 = 0.09761365503072739 + 1.0 * 6.07679557800293
Epoch 680, val loss: 0.7303752899169922
Epoch 690, training loss: 6.171523094177246 = 0.09234166890382767 + 1.0 * 6.07918119430542
Epoch 690, val loss: 0.7340442538261414
Epoch 700, training loss: 6.166314601898193 = 0.08744767308235168 + 1.0 * 6.078866958618164
Epoch 700, val loss: 0.7378677129745483
Epoch 710, training loss: 6.15853214263916 = 0.08288722485303879 + 1.0 * 6.0756449699401855
Epoch 710, val loss: 0.7417643070220947
Epoch 720, training loss: 6.150639533996582 = 0.07864800840616226 + 1.0 * 6.071991443634033
Epoch 720, val loss: 0.7457249164581299
Epoch 730, training loss: 6.145881652832031 = 0.07467316091060638 + 1.0 * 6.071208477020264
Epoch 730, val loss: 0.7496231198310852
Epoch 740, training loss: 6.145144939422607 = 0.07096333801746368 + 1.0 * 6.07418155670166
Epoch 740, val loss: 0.7534609436988831
Epoch 750, training loss: 6.137303829193115 = 0.06751981377601624 + 1.0 * 6.069784164428711
Epoch 750, val loss: 0.7573786377906799
Epoch 760, training loss: 6.132085800170898 = 0.06429631263017654 + 1.0 * 6.067789554595947
Epoch 760, val loss: 0.7613680958747864
Epoch 770, training loss: 6.132262706756592 = 0.06127511337399483 + 1.0 * 6.070987701416016
Epoch 770, val loss: 0.765292763710022
Epoch 780, training loss: 6.126349925994873 = 0.05844612792134285 + 1.0 * 6.067903995513916
Epoch 780, val loss: 0.769066572189331
Epoch 790, training loss: 6.121007442474365 = 0.05580625683069229 + 1.0 * 6.065201282501221
Epoch 790, val loss: 0.773007333278656
Epoch 800, training loss: 6.11680793762207 = 0.05331599712371826 + 1.0 * 6.0634918212890625
Epoch 800, val loss: 0.7768711447715759
Epoch 810, training loss: 6.121535778045654 = 0.05098233371973038 + 1.0 * 6.070553302764893
Epoch 810, val loss: 0.7805783152580261
Epoch 820, training loss: 6.11148738861084 = 0.04879200831055641 + 1.0 * 6.062695503234863
Epoch 820, val loss: 0.7843524217605591
Epoch 830, training loss: 6.108128547668457 = 0.04673205688595772 + 1.0 * 6.061396598815918
Epoch 830, val loss: 0.7882423996925354
Epoch 840, training loss: 6.104756832122803 = 0.044796135276556015 + 1.0 * 6.059960842132568
Epoch 840, val loss: 0.7919076681137085
Epoch 850, training loss: 6.101085186004639 = 0.04297403246164322 + 1.0 * 6.058111190795898
Epoch 850, val loss: 0.7956876158714294
Epoch 860, training loss: 6.098230361938477 = 0.04125012829899788 + 1.0 * 6.056980133056641
Epoch 860, val loss: 0.799473226070404
Epoch 870, training loss: 6.115980625152588 = 0.03962158039212227 + 1.0 * 6.076359272003174
Epoch 870, val loss: 0.80303555727005
Epoch 880, training loss: 6.094074726104736 = 0.03809497505426407 + 1.0 * 6.0559797286987305
Epoch 880, val loss: 0.8066413402557373
Epoch 890, training loss: 6.092412948608398 = 0.036655887961387634 + 1.0 * 6.05575704574585
Epoch 890, val loss: 0.8105303049087524
Epoch 900, training loss: 6.089596271514893 = 0.035291023552417755 + 1.0 * 6.054305076599121
Epoch 900, val loss: 0.8142610788345337
Epoch 910, training loss: 6.088553428649902 = 0.03399438038468361 + 1.0 * 6.054559230804443
Epoch 910, val loss: 0.8178883194923401
Epoch 920, training loss: 6.085765361785889 = 0.03276745602488518 + 1.0 * 6.052998065948486
Epoch 920, val loss: 0.8214171528816223
Epoch 930, training loss: 6.086513042449951 = 0.03161029517650604 + 1.0 * 6.05490255355835
Epoch 930, val loss: 0.8251604437828064
Epoch 940, training loss: 6.081717491149902 = 0.030511140823364258 + 1.0 * 6.051206588745117
Epoch 940, val loss: 0.8289056420326233
Epoch 950, training loss: 6.081387042999268 = 0.029466716572642326 + 1.0 * 6.051920413970947
Epoch 950, val loss: 0.8325245380401611
Epoch 960, training loss: 6.079996585845947 = 0.0284732598811388 + 1.0 * 6.051523208618164
Epoch 960, val loss: 0.8360626101493835
Epoch 970, training loss: 6.082058906555176 = 0.02752598747611046 + 1.0 * 6.054533004760742
Epoch 970, val loss: 0.8396551609039307
Epoch 980, training loss: 6.076690673828125 = 0.02663251757621765 + 1.0 * 6.050058364868164
Epoch 980, val loss: 0.8432850241661072
Epoch 990, training loss: 6.080044746398926 = 0.025779714807868004 + 1.0 * 6.054265022277832
Epoch 990, val loss: 0.8468279242515564
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5572
Flip ASR: 0.4711/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.313090324401855 = 1.939271092414856 + 1.0 * 8.373819351196289
Epoch 0, val loss: 1.9340919256210327
Epoch 10, training loss: 10.302104949951172 = 1.9288626909255981 + 1.0 * 8.373242378234863
Epoch 10, val loss: 1.9229910373687744
Epoch 20, training loss: 10.28514289855957 = 1.91581392288208 + 1.0 * 8.369328498840332
Epoch 20, val loss: 1.908732533454895
Epoch 30, training loss: 10.241270065307617 = 1.8978312015533447 + 1.0 * 8.343439102172852
Epoch 30, val loss: 1.889042854309082
Epoch 40, training loss: 10.041496276855469 = 1.8757824897766113 + 1.0 * 8.165714263916016
Epoch 40, val loss: 1.8660815954208374
Epoch 50, training loss: 9.366646766662598 = 1.854282259941101 + 1.0 * 7.512364864349365
Epoch 50, val loss: 1.8443760871887207
Epoch 60, training loss: 8.911632537841797 = 1.8383311033248901 + 1.0 * 7.073301315307617
Epoch 60, val loss: 1.8294748067855835
Epoch 70, training loss: 8.629012107849121 = 1.8247636556625366 + 1.0 * 6.804248809814453
Epoch 70, val loss: 1.8156795501708984
Epoch 80, training loss: 8.469545364379883 = 1.809659719467163 + 1.0 * 6.659885883331299
Epoch 80, val loss: 1.8002897500991821
Epoch 90, training loss: 8.34072494506836 = 1.7938556671142578 + 1.0 * 6.546868801116943
Epoch 90, val loss: 1.7845828533172607
Epoch 100, training loss: 8.246387481689453 = 1.778462290763855 + 1.0 * 6.467925548553467
Epoch 100, val loss: 1.769700527191162
Epoch 110, training loss: 8.173824310302734 = 1.7626584768295288 + 1.0 * 6.411165714263916
Epoch 110, val loss: 1.7545398473739624
Epoch 120, training loss: 8.114741325378418 = 1.7455672025680542 + 1.0 * 6.369174480438232
Epoch 120, val loss: 1.7381150722503662
Epoch 130, training loss: 8.062280654907227 = 1.7264500856399536 + 1.0 * 6.335830211639404
Epoch 130, val loss: 1.7201036214828491
Epoch 140, training loss: 8.014799118041992 = 1.7045998573303223 + 1.0 * 6.310199737548828
Epoch 140, val loss: 1.7002097368240356
Epoch 150, training loss: 7.968345642089844 = 1.6790813207626343 + 1.0 * 6.28926420211792
Epoch 150, val loss: 1.6777029037475586
Epoch 160, training loss: 7.922346115112305 = 1.6494765281677246 + 1.0 * 6.27286958694458
Epoch 160, val loss: 1.6523069143295288
Epoch 170, training loss: 7.874849319458008 = 1.615067720413208 + 1.0 * 6.259781360626221
Epoch 170, val loss: 1.6235451698303223
Epoch 180, training loss: 7.822235584259033 = 1.5755023956298828 + 1.0 * 6.24673318862915
Epoch 180, val loss: 1.5910176038742065
Epoch 190, training loss: 7.766496181488037 = 1.530387043952942 + 1.0 * 6.236109256744385
Epoch 190, val loss: 1.554537296295166
Epoch 200, training loss: 7.710559844970703 = 1.4806299209594727 + 1.0 * 6.2299299240112305
Epoch 200, val loss: 1.5151770114898682
Epoch 210, training loss: 7.647229194641113 = 1.4280487298965454 + 1.0 * 6.219180583953857
Epoch 210, val loss: 1.4742354154586792
Epoch 220, training loss: 7.584823131561279 = 1.372965693473816 + 1.0 * 6.211857318878174
Epoch 220, val loss: 1.4321027994155884
Epoch 230, training loss: 7.521431922912598 = 1.3162564039230347 + 1.0 * 6.205175399780273
Epoch 230, val loss: 1.3895353078842163
Epoch 240, training loss: 7.461045742034912 = 1.2594928741455078 + 1.0 * 6.201552867889404
Epoch 240, val loss: 1.3476728200912476
Epoch 250, training loss: 7.398252010345459 = 1.2038726806640625 + 1.0 * 6.1943793296813965
Epoch 250, val loss: 1.307282567024231
Epoch 260, training loss: 7.336483001708984 = 1.149185299873352 + 1.0 * 6.187297821044922
Epoch 260, val loss: 1.2675907611846924
Epoch 270, training loss: 7.2772746086120605 = 1.0950508117675781 + 1.0 * 6.182223796844482
Epoch 270, val loss: 1.2281681299209595
Epoch 280, training loss: 7.2259321212768555 = 1.0414174795150757 + 1.0 * 6.18451452255249
Epoch 280, val loss: 1.1889504194259644
Epoch 290, training loss: 7.164363861083984 = 0.9890786409378052 + 1.0 * 6.175285339355469
Epoch 290, val loss: 1.1503758430480957
Epoch 300, training loss: 7.106067657470703 = 0.9378756284713745 + 1.0 * 6.168191909790039
Epoch 300, val loss: 1.1127244234085083
Epoch 310, training loss: 7.056392669677734 = 0.8877003788948059 + 1.0 * 6.168692111968994
Epoch 310, val loss: 1.0758213996887207
Epoch 320, training loss: 7.00076961517334 = 0.8390439748764038 + 1.0 * 6.1617255210876465
Epoch 320, val loss: 1.0402053594589233
Epoch 330, training loss: 6.950606346130371 = 0.7924458980560303 + 1.0 * 6.15816068649292
Epoch 330, val loss: 1.006685733795166
Epoch 340, training loss: 6.901485443115234 = 0.7482442855834961 + 1.0 * 6.153241157531738
Epoch 340, val loss: 0.9754493832588196
Epoch 350, training loss: 6.8559160232543945 = 0.7063669562339783 + 1.0 * 6.1495490074157715
Epoch 350, val loss: 0.9467849135398865
Epoch 360, training loss: 6.820620059967041 = 0.6674320101737976 + 1.0 * 6.153188228607178
Epoch 360, val loss: 0.9210565090179443
Epoch 370, training loss: 6.776622772216797 = 0.6323721408843994 + 1.0 * 6.144250869750977
Epoch 370, val loss: 0.8989119529724121
Epoch 380, training loss: 6.740938663482666 = 0.6004438996315002 + 1.0 * 6.1404948234558105
Epoch 380, val loss: 0.8799065351486206
Epoch 390, training loss: 6.708518028259277 = 0.5712299346923828 + 1.0 * 6.1372880935668945
Epoch 390, val loss: 0.8635441660881042
Epoch 400, training loss: 6.677788257598877 = 0.5446357131004333 + 1.0 * 6.133152484893799
Epoch 400, val loss: 0.8496348261833191
Epoch 410, training loss: 6.652164459228516 = 0.5204718112945557 + 1.0 * 6.131692886352539
Epoch 410, val loss: 0.8381126523017883
Epoch 420, training loss: 6.628515243530273 = 0.4982457458972931 + 1.0 * 6.130269527435303
Epoch 420, val loss: 0.8283429145812988
Epoch 430, training loss: 6.605943202972412 = 0.47779062390327454 + 1.0 * 6.128152370452881
Epoch 430, val loss: 0.8201238512992859
Epoch 440, training loss: 6.581334590911865 = 0.45868006348609924 + 1.0 * 6.122654438018799
Epoch 440, val loss: 0.8132755160331726
Epoch 450, training loss: 6.5586323738098145 = 0.44056063890457153 + 1.0 * 6.118071556091309
Epoch 450, val loss: 0.8074798583984375
Epoch 460, training loss: 6.546694278717041 = 0.42327794432640076 + 1.0 * 6.123416423797607
Epoch 460, val loss: 0.8025257587432861
Epoch 470, training loss: 6.5232086181640625 = 0.4069821238517761 + 1.0 * 6.116226673126221
Epoch 470, val loss: 0.7985281348228455
Epoch 480, training loss: 6.5023698806762695 = 0.3911726176738739 + 1.0 * 6.111197471618652
Epoch 480, val loss: 0.7952402830123901
Epoch 490, training loss: 6.484827995300293 = 0.37568143010139465 + 1.0 * 6.109146595001221
Epoch 490, val loss: 0.7923657298088074
Epoch 500, training loss: 6.476129531860352 = 0.36048439145088196 + 1.0 * 6.115644931793213
Epoch 500, val loss: 0.7898336052894592
Epoch 510, training loss: 6.4529619216918945 = 0.345638245344162 + 1.0 * 6.10732364654541
Epoch 510, val loss: 0.7877365350723267
Epoch 520, training loss: 6.4334001541137695 = 0.3310636579990387 + 1.0 * 6.102336406707764
Epoch 520, val loss: 0.786177933216095
Epoch 530, training loss: 6.425864219665527 = 0.3167228698730469 + 1.0 * 6.1091413497924805
Epoch 530, val loss: 0.7850234508514404
Epoch 540, training loss: 6.402591228485107 = 0.30276617407798767 + 1.0 * 6.099824905395508
Epoch 540, val loss: 0.784186065196991
Epoch 550, training loss: 6.386699676513672 = 0.28914546966552734 + 1.0 * 6.0975542068481445
Epoch 550, val loss: 0.7837873101234436
Epoch 560, training loss: 6.371321201324463 = 0.27583828568458557 + 1.0 * 6.09548282623291
Epoch 560, val loss: 0.7837715744972229
Epoch 570, training loss: 6.356215000152588 = 0.2628224790096283 + 1.0 * 6.093392372131348
Epoch 570, val loss: 0.7841904163360596
Epoch 580, training loss: 6.347014427185059 = 0.2501883804798126 + 1.0 * 6.096826076507568
Epoch 580, val loss: 0.785041093826294
Epoch 590, training loss: 6.335831165313721 = 0.23804700374603271 + 1.0 * 6.097784042358398
Epoch 590, val loss: 0.7863301038742065
Epoch 600, training loss: 6.315607070922852 = 0.2262871414422989 + 1.0 * 6.089319705963135
Epoch 600, val loss: 0.7880487442016602
Epoch 610, training loss: 6.302389621734619 = 0.214848130941391 + 1.0 * 6.087541580200195
Epoch 610, val loss: 0.7902538180351257
Epoch 620, training loss: 6.291322231292725 = 0.20376873016357422 + 1.0 * 6.08755350112915
Epoch 620, val loss: 0.792925238609314
Epoch 630, training loss: 6.281286716461182 = 0.1931004822254181 + 1.0 * 6.088186264038086
Epoch 630, val loss: 0.7960651516914368
Epoch 640, training loss: 6.2669572830200195 = 0.18289653956890106 + 1.0 * 6.0840606689453125
Epoch 640, val loss: 0.799564778804779
Epoch 650, training loss: 6.256274700164795 = 0.17307163774967194 + 1.0 * 6.083202838897705
Epoch 650, val loss: 0.8034607768058777
Epoch 660, training loss: 6.247722625732422 = 0.16368307173252106 + 1.0 * 6.084039688110352
Epoch 660, val loss: 0.8077757358551025
Epoch 670, training loss: 6.238089084625244 = 0.15476709604263306 + 1.0 * 6.083322048187256
Epoch 670, val loss: 0.812522828578949
Epoch 680, training loss: 6.228320121765137 = 0.14634856581687927 + 1.0 * 6.081971645355225
Epoch 680, val loss: 0.8175365924835205
Epoch 690, training loss: 6.216493606567383 = 0.13838568329811096 + 1.0 * 6.078107833862305
Epoch 690, val loss: 0.8229300379753113
Epoch 700, training loss: 6.208285331726074 = 0.13081897795200348 + 1.0 * 6.0774664878845215
Epoch 700, val loss: 0.8286260962486267
Epoch 710, training loss: 6.208914756774902 = 0.12366664409637451 + 1.0 * 6.085247993469238
Epoch 710, val loss: 0.8346409201622009
Epoch 720, training loss: 6.19130802154541 = 0.11700887233018875 + 1.0 * 6.074299335479736
Epoch 720, val loss: 0.840873658657074
Epoch 730, training loss: 6.1861348152160645 = 0.110788494348526 + 1.0 * 6.07534646987915
Epoch 730, val loss: 0.8473483920097351
Epoch 740, training loss: 6.180315017700195 = 0.10497436672449112 + 1.0 * 6.075340747833252
Epoch 740, val loss: 0.8541205525398254
Epoch 750, training loss: 6.1760663986206055 = 0.09953156858682632 + 1.0 * 6.076534748077393
Epoch 750, val loss: 0.8610625267028809
Epoch 760, training loss: 6.169224262237549 = 0.09445127099752426 + 1.0 * 6.074772834777832
Epoch 760, val loss: 0.8681089878082275
Epoch 770, training loss: 6.1622819900512695 = 0.08971631526947021 + 1.0 * 6.07256555557251
Epoch 770, val loss: 0.8754058480262756
Epoch 780, training loss: 6.155107021331787 = 0.08530145138502121 + 1.0 * 6.06980562210083
Epoch 780, val loss: 0.8827900290489197
Epoch 790, training loss: 6.14923620223999 = 0.08117353916168213 + 1.0 * 6.068062782287598
Epoch 790, val loss: 0.8903358578681946
Epoch 800, training loss: 6.14847469329834 = 0.07730860263109207 + 1.0 * 6.071166038513184
Epoch 800, val loss: 0.898000955581665
Epoch 810, training loss: 6.146025657653809 = 0.07370323687791824 + 1.0 * 6.072322368621826
Epoch 810, val loss: 0.9057637453079224
Epoch 820, training loss: 6.1373677253723145 = 0.07032232731580734 + 1.0 * 6.067045211791992
Epoch 820, val loss: 0.9134544134140015
Epoch 830, training loss: 6.130921840667725 = 0.06715504825115204 + 1.0 * 6.063766956329346
Epoch 830, val loss: 0.9211223721504211
Epoch 840, training loss: 6.126677989959717 = 0.06416957825422287 + 1.0 * 6.062508583068848
Epoch 840, val loss: 0.9288418889045715
Epoch 850, training loss: 6.12579345703125 = 0.0613473616540432 + 1.0 * 6.064445972442627
Epoch 850, val loss: 0.9365404844284058
Epoch 860, training loss: 6.122603416442871 = 0.05869502201676369 + 1.0 * 6.063908576965332
Epoch 860, val loss: 0.9442885518074036
Epoch 870, training loss: 6.11929178237915 = 0.05620803311467171 + 1.0 * 6.063083648681641
Epoch 870, val loss: 0.9518725275993347
Epoch 880, training loss: 6.117163181304932 = 0.053862519562244415 + 1.0 * 6.063300609588623
Epoch 880, val loss: 0.9593944549560547
Epoch 890, training loss: 6.1099324226379395 = 0.05165466293692589 + 1.0 * 6.058277606964111
Epoch 890, val loss: 0.9668416380882263
Epoch 900, training loss: 6.108246803283691 = 0.04955853149294853 + 1.0 * 6.058688163757324
Epoch 900, val loss: 0.9742259979248047
Epoch 910, training loss: 6.107785224914551 = 0.047579701989889145 + 1.0 * 6.060205459594727
Epoch 910, val loss: 0.9815531373023987
Epoch 920, training loss: 6.106744289398193 = 0.04571548476815224 + 1.0 * 6.061028957366943
Epoch 920, val loss: 0.9887125492095947
Epoch 930, training loss: 6.0987043380737305 = 0.04395558685064316 + 1.0 * 6.05474853515625
Epoch 930, val loss: 0.9957044720649719
Epoch 940, training loss: 6.0973711013793945 = 0.04228523001074791 + 1.0 * 6.0550856590271
Epoch 940, val loss: 1.0026079416275024
Epoch 950, training loss: 6.098302364349365 = 0.040705353021621704 + 1.0 * 6.0575971603393555
Epoch 950, val loss: 1.009467601776123
Epoch 960, training loss: 6.093736171722412 = 0.03921067342162132 + 1.0 * 6.054525375366211
Epoch 960, val loss: 1.016193151473999
Epoch 970, training loss: 6.096394062042236 = 0.037784919142723083 + 1.0 * 6.0586090087890625
Epoch 970, val loss: 1.0227957963943481
Epoch 980, training loss: 6.0944013595581055 = 0.03644441068172455 + 1.0 * 6.057957172393799
Epoch 980, val loss: 1.029278039932251
Epoch 990, training loss: 6.088067054748535 = 0.03517361730337143 + 1.0 * 6.05289363861084
Epoch 990, val loss: 1.0355373620986938
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32452392578125 = 1.9506779909133911 + 1.0 * 8.373846054077148
Epoch 0, val loss: 1.9515032768249512
Epoch 10, training loss: 10.313872337341309 = 1.9405872821807861 + 1.0 * 8.373285293579102
Epoch 10, val loss: 1.9408667087554932
Epoch 20, training loss: 10.297371864318848 = 1.9273308515548706 + 1.0 * 8.370040893554688
Epoch 20, val loss: 1.9265164136886597
Epoch 30, training loss: 10.256762504577637 = 1.9083160161972046 + 1.0 * 8.3484468460083
Epoch 30, val loss: 1.9058785438537598
Epoch 40, training loss: 10.066892623901367 = 1.8843764066696167 + 1.0 * 8.182516098022461
Epoch 40, val loss: 1.8814058303833008
Epoch 50, training loss: 9.44337272644043 = 1.8593147993087769 + 1.0 * 7.5840582847595215
Epoch 50, val loss: 1.856677532196045
Epoch 60, training loss: 9.036786079406738 = 1.8390001058578491 + 1.0 * 7.197786331176758
Epoch 60, val loss: 1.837132215499878
Epoch 70, training loss: 8.765183448791504 = 1.823060154914856 + 1.0 * 6.9421234130859375
Epoch 70, val loss: 1.8214035034179688
Epoch 80, training loss: 8.57728099822998 = 1.8070437908172607 + 1.0 * 6.770236968994141
Epoch 80, val loss: 1.805735468864441
Epoch 90, training loss: 8.457676887512207 = 1.791752576828003 + 1.0 * 6.665924549102783
Epoch 90, val loss: 1.7910990715026855
Epoch 100, training loss: 8.35963249206543 = 1.7768797874450684 + 1.0 * 6.582752227783203
Epoch 100, val loss: 1.7770812511444092
Epoch 110, training loss: 8.283024787902832 = 1.7624379396438599 + 1.0 * 6.520586967468262
Epoch 110, val loss: 1.7636293172836304
Epoch 120, training loss: 8.213944435119629 = 1.7477285861968994 + 1.0 * 6.466216087341309
Epoch 120, val loss: 1.750004768371582
Epoch 130, training loss: 8.155778884887695 = 1.7319570779800415 + 1.0 * 6.423821449279785
Epoch 130, val loss: 1.7357035875320435
Epoch 140, training loss: 8.101031303405762 = 1.7142268419265747 + 1.0 * 6.386804580688477
Epoch 140, val loss: 1.7201274633407593
Epoch 150, training loss: 8.046796798706055 = 1.6936414241790771 + 1.0 * 6.353155612945557
Epoch 150, val loss: 1.7026516199111938
Epoch 160, training loss: 7.997255325317383 = 1.669442057609558 + 1.0 * 6.327813148498535
Epoch 160, val loss: 1.6825298070907593
Epoch 170, training loss: 7.941793441772461 = 1.6416661739349365 + 1.0 * 6.300127029418945
Epoch 170, val loss: 1.6596883535385132
Epoch 180, training loss: 7.889255046844482 = 1.6095986366271973 + 1.0 * 6.279656410217285
Epoch 180, val loss: 1.633373737335205
Epoch 190, training loss: 7.835792064666748 = 1.5724172592163086 + 1.0 * 6.2633748054504395
Epoch 190, val loss: 1.6030168533325195
Epoch 200, training loss: 7.779272079467773 = 1.5302437543869019 + 1.0 * 6.249028205871582
Epoch 200, val loss: 1.5686649084091187
Epoch 210, training loss: 7.721379280090332 = 1.4836809635162354 + 1.0 * 6.237698078155518
Epoch 210, val loss: 1.5311684608459473
Epoch 220, training loss: 7.660050868988037 = 1.4333797693252563 + 1.0 * 6.22667121887207
Epoch 220, val loss: 1.4911508560180664
Epoch 230, training loss: 7.601810932159424 = 1.3797131776809692 + 1.0 * 6.222097873687744
Epoch 230, val loss: 1.4488078355789185
Epoch 240, training loss: 7.535359859466553 = 1.3243681192398071 + 1.0 * 6.210991859436035
Epoch 240, val loss: 1.4054763317108154
Epoch 250, training loss: 7.470678806304932 = 1.2675801515579224 + 1.0 * 6.203098773956299
Epoch 250, val loss: 1.361180067062378
Epoch 260, training loss: 7.406887531280518 = 1.2098177671432495 + 1.0 * 6.1970696449279785
Epoch 260, val loss: 1.3161431550979614
Epoch 270, training loss: 7.34680700302124 = 1.1525006294250488 + 1.0 * 6.194306373596191
Epoch 270, val loss: 1.271777868270874
Epoch 280, training loss: 7.284466743469238 = 1.0968430042266846 + 1.0 * 6.187623500823975
Epoch 280, val loss: 1.228864312171936
Epoch 290, training loss: 7.223623752593994 = 1.042824387550354 + 1.0 * 6.18079948425293
Epoch 290, val loss: 1.1873961687088013
Epoch 300, training loss: 7.171963214874268 = 0.9904321432113647 + 1.0 * 6.181530952453613
Epoch 300, val loss: 1.147489309310913
Epoch 310, training loss: 7.114249229431152 = 0.9402073621749878 + 1.0 * 6.174041748046875
Epoch 310, val loss: 1.1095373630523682
Epoch 320, training loss: 7.060037612915039 = 0.8917160034179688 + 1.0 * 6.16832160949707
Epoch 320, val loss: 1.073165774345398
Epoch 330, training loss: 7.00938081741333 = 0.8445972800254822 + 1.0 * 6.164783477783203
Epoch 330, val loss: 1.0378602743148804
Epoch 340, training loss: 6.965346336364746 = 0.7994487881660461 + 1.0 * 6.165897369384766
Epoch 340, val loss: 1.0040080547332764
Epoch 350, training loss: 6.914470195770264 = 0.7567834854125977 + 1.0 * 6.157686710357666
Epoch 350, val loss: 0.9721630811691284
Epoch 360, training loss: 6.869551658630371 = 0.716223955154419 + 1.0 * 6.153327941894531
Epoch 360, val loss: 0.941905677318573
Epoch 370, training loss: 6.830938339233398 = 0.6778954863548279 + 1.0 * 6.153042793273926
Epoch 370, val loss: 0.9134553074836731
Epoch 380, training loss: 6.789722919464111 = 0.6423095464706421 + 1.0 * 6.14741325378418
Epoch 380, val loss: 0.8873433470726013
Epoch 390, training loss: 6.752089977264404 = 0.6088277101516724 + 1.0 * 6.1432623863220215
Epoch 390, val loss: 0.8632271885871887
Epoch 400, training loss: 6.717401027679443 = 0.5771032571792603 + 1.0 * 6.140297889709473
Epoch 400, val loss: 0.8408230543136597
Epoch 410, training loss: 6.693374156951904 = 0.5472347140312195 + 1.0 * 6.146139621734619
Epoch 410, val loss: 0.8202463984489441
Epoch 420, training loss: 6.6544976234436035 = 0.5193299651145935 + 1.0 * 6.135167598724365
Epoch 420, val loss: 0.8019003868103027
Epoch 430, training loss: 6.624838829040527 = 0.49282294511795044 + 1.0 * 6.132015705108643
Epoch 430, val loss: 0.785222589969635
Epoch 440, training loss: 6.602112770080566 = 0.46746817231178284 + 1.0 * 6.134644508361816
Epoch 440, val loss: 0.7699511647224426
Epoch 450, training loss: 6.570316314697266 = 0.4433876574039459 + 1.0 * 6.126928806304932
Epoch 450, val loss: 0.7562720775604248
Epoch 460, training loss: 6.544166088104248 = 0.4203595519065857 + 1.0 * 6.123806476593018
Epoch 460, val loss: 0.7441016435623169
Epoch 470, training loss: 6.520084381103516 = 0.39814990758895874 + 1.0 * 6.121934413909912
Epoch 470, val loss: 0.7330512404441833
Epoch 480, training loss: 6.499166965484619 = 0.37680190801620483 + 1.0 * 6.1223649978637695
Epoch 480, val loss: 0.7230800986289978
Epoch 490, training loss: 6.474304676055908 = 0.35636550188064575 + 1.0 * 6.117938995361328
Epoch 490, val loss: 0.7142652273178101
Epoch 500, training loss: 6.451718330383301 = 0.3366119861602783 + 1.0 * 6.115106105804443
Epoch 500, val loss: 0.7063186168670654
Epoch 510, training loss: 6.431469440460205 = 0.31757262349128723 + 1.0 * 6.11389684677124
Epoch 510, val loss: 0.6991919279098511
Epoch 520, training loss: 6.411526203155518 = 0.2993549704551697 + 1.0 * 6.112171173095703
Epoch 520, val loss: 0.6928600072860718
Epoch 530, training loss: 6.3910017013549805 = 0.28183549642562866 + 1.0 * 6.109166145324707
Epoch 530, val loss: 0.6872493624687195
Epoch 540, training loss: 6.3716607093811035 = 0.2649596631526947 + 1.0 * 6.106700897216797
Epoch 540, val loss: 0.6823074221611023
Epoch 550, training loss: 6.360797882080078 = 0.24893736839294434 + 1.0 * 6.111860275268555
Epoch 550, val loss: 0.6780064105987549
Epoch 560, training loss: 6.3424601554870605 = 0.23385511338710785 + 1.0 * 6.108604907989502
Epoch 560, val loss: 0.6743838787078857
Epoch 570, training loss: 6.321605682373047 = 0.21957339346408844 + 1.0 * 6.10203218460083
Epoch 570, val loss: 0.6713484525680542
Epoch 580, training loss: 6.309745788574219 = 0.2060527354478836 + 1.0 * 6.103693008422852
Epoch 580, val loss: 0.6688212156295776
Epoch 590, training loss: 6.2931389808654785 = 0.1933685541152954 + 1.0 * 6.099770545959473
Epoch 590, val loss: 0.6668673157691956
Epoch 600, training loss: 6.282294750213623 = 0.1814580112695694 + 1.0 * 6.100836753845215
Epoch 600, val loss: 0.6654541492462158
Epoch 610, training loss: 6.268119812011719 = 0.17030903697013855 + 1.0 * 6.097810745239258
Epoch 610, val loss: 0.6645839810371399
Epoch 620, training loss: 6.258661270141602 = 0.15988193452358246 + 1.0 * 6.098779201507568
Epoch 620, val loss: 0.6641605496406555
Epoch 630, training loss: 6.243599891662598 = 0.1501702070236206 + 1.0 * 6.0934295654296875
Epoch 630, val loss: 0.6642604470252991
Epoch 640, training loss: 6.23366641998291 = 0.14109724760055542 + 1.0 * 6.092569351196289
Epoch 640, val loss: 0.6647247076034546
Epoch 650, training loss: 6.2255353927612305 = 0.13264544308185577 + 1.0 * 6.092889785766602
Epoch 650, val loss: 0.665625810623169
Epoch 660, training loss: 6.218651294708252 = 0.12480190396308899 + 1.0 * 6.093849182128906
Epoch 660, val loss: 0.6668366193771362
Epoch 670, training loss: 6.207826614379883 = 0.11753695458173752 + 1.0 * 6.09028959274292
Epoch 670, val loss: 0.6684567332267761
Epoch 680, training loss: 6.196987152099609 = 0.11076697707176208 + 1.0 * 6.0862202644348145
Epoch 680, val loss: 0.6703574657440186
Epoch 690, training loss: 6.192080974578857 = 0.10446161031723022 + 1.0 * 6.087619304656982
Epoch 690, val loss: 0.6725218892097473
Epoch 700, training loss: 6.185011386871338 = 0.09861604124307632 + 1.0 * 6.086395263671875
Epoch 700, val loss: 0.6749251484870911
Epoch 710, training loss: 6.177983283996582 = 0.09320995211601257 + 1.0 * 6.084773540496826
Epoch 710, val loss: 0.6775984764099121
Epoch 720, training loss: 6.173781871795654 = 0.08817432075738907 + 1.0 * 6.085607528686523
Epoch 720, val loss: 0.680504322052002
Epoch 730, training loss: 6.169683456420898 = 0.08351977914571762 + 1.0 * 6.086163520812988
Epoch 730, val loss: 0.6834427714347839
Epoch 740, training loss: 6.157898426055908 = 0.07919317483901978 + 1.0 * 6.078705310821533
Epoch 740, val loss: 0.686621367931366
Epoch 750, training loss: 6.1523118019104 = 0.07514916360378265 + 1.0 * 6.077162742614746
Epoch 750, val loss: 0.6900010108947754
Epoch 760, training loss: 6.161783695220947 = 0.07136885076761246 + 1.0 * 6.090415000915527
Epoch 760, val loss: 0.6934744119644165
Epoch 770, training loss: 6.143967151641846 = 0.06787584722042084 + 1.0 * 6.076091289520264
Epoch 770, val loss: 0.6970311999320984
Epoch 780, training loss: 6.138516902923584 = 0.06460445374250412 + 1.0 * 6.073912620544434
Epoch 780, val loss: 0.7006862759590149
Epoch 790, training loss: 6.1354875564575195 = 0.06153377145528793 + 1.0 * 6.073953628540039
Epoch 790, val loss: 0.704457700252533
Epoch 800, training loss: 6.1315765380859375 = 0.058668289333581924 + 1.0 * 6.072908401489258
Epoch 800, val loss: 0.7083215713500977
Epoch 810, training loss: 6.129634857177734 = 0.0559968538582325 + 1.0 * 6.073637962341309
Epoch 810, val loss: 0.7121434807777405
Epoch 820, training loss: 6.125938892364502 = 0.053493209183216095 + 1.0 * 6.072445869445801
Epoch 820, val loss: 0.7160956263542175
Epoch 830, training loss: 6.121291637420654 = 0.051145363599061966 + 1.0 * 6.070146083831787
Epoch 830, val loss: 0.7200568318367004
Epoch 840, training loss: 6.120125770568848 = 0.04894375801086426 + 1.0 * 6.071181774139404
Epoch 840, val loss: 0.7240127921104431
Epoch 850, training loss: 6.117340087890625 = 0.046876560896635056 + 1.0 * 6.07046365737915
Epoch 850, val loss: 0.7280042171478271
Epoch 860, training loss: 6.113062381744385 = 0.0449301078915596 + 1.0 * 6.068132400512695
Epoch 860, val loss: 0.7320053577423096
Epoch 870, training loss: 6.112436294555664 = 0.04310472682118416 + 1.0 * 6.069331645965576
Epoch 870, val loss: 0.7359766364097595
Epoch 880, training loss: 6.108741283416748 = 0.0413941964507103 + 1.0 * 6.067347049713135
Epoch 880, val loss: 0.7399314045906067
Epoch 890, training loss: 6.104405879974365 = 0.039784323424100876 + 1.0 * 6.064621448516846
Epoch 890, val loss: 0.7439491152763367
Epoch 900, training loss: 6.103562831878662 = 0.038258831948041916 + 1.0 * 6.065303802490234
Epoch 900, val loss: 0.7479419112205505
Epoch 910, training loss: 6.100137710571289 = 0.03681313991546631 + 1.0 * 6.063324451446533
Epoch 910, val loss: 0.7519211769104004
Epoch 920, training loss: 6.106283664703369 = 0.03545159101486206 + 1.0 * 6.070832252502441
Epoch 920, val loss: 0.7558430433273315
Epoch 930, training loss: 6.095901966094971 = 0.034174274653196335 + 1.0 * 6.061727523803711
Epoch 930, val loss: 0.7597300410270691
Epoch 940, training loss: 6.0929856300354 = 0.032959144562482834 + 1.0 * 6.0600266456604
Epoch 940, val loss: 0.7636903524398804
Epoch 950, training loss: 6.090749740600586 = 0.03180074319243431 + 1.0 * 6.058948993682861
Epoch 950, val loss: 0.7676159143447876
Epoch 960, training loss: 6.097711563110352 = 0.03070172853767872 + 1.0 * 6.067009925842285
Epoch 960, val loss: 0.7715346813201904
Epoch 970, training loss: 6.087570667266846 = 0.029662957414984703 + 1.0 * 6.057907581329346
Epoch 970, val loss: 0.7753356695175171
Epoch 980, training loss: 6.086460590362549 = 0.028677769005298615 + 1.0 * 6.0577826499938965
Epoch 980, val loss: 0.7792080044746399
Epoch 990, training loss: 6.088054180145264 = 0.027740050107240677 + 1.0 * 6.060314178466797
Epoch 990, val loss: 0.7830213308334351
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7934
Flip ASR: 0.7556/225 nodes
The final ASR:0.77614, 0.17217, Accuracy:0.81111, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10532])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97540, 0.00627, Accuracy:0.84074, 0.00605
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.312051773071289 = 1.9381794929504395 + 1.0 * 8.373872756958008
Epoch 0, val loss: 1.9365882873535156
Epoch 10, training loss: 10.302318572998047 = 1.9288674592971802 + 1.0 * 8.373451232910156
Epoch 10, val loss: 1.9278039932250977
Epoch 20, training loss: 10.287721633911133 = 1.917372465133667 + 1.0 * 8.370348930358887
Epoch 20, val loss: 1.9164738655090332
Epoch 30, training loss: 10.24779224395752 = 1.9012529850006104 + 1.0 * 8.346539497375488
Epoch 30, val loss: 1.900141716003418
Epoch 40, training loss: 10.029489517211914 = 1.8811798095703125 + 1.0 * 8.148309707641602
Epoch 40, val loss: 1.8802732229232788
Epoch 50, training loss: 9.505165100097656 = 1.8604432344436646 + 1.0 * 7.644721984863281
Epoch 50, val loss: 1.8605982065200806
Epoch 60, training loss: 9.001832008361816 = 1.8451879024505615 + 1.0 * 7.156643867492676
Epoch 60, val loss: 1.8462384939193726
Epoch 70, training loss: 8.653614044189453 = 1.8335890769958496 + 1.0 * 6.820024490356445
Epoch 70, val loss: 1.8354741334915161
Epoch 80, training loss: 8.477499008178711 = 1.8204454183578491 + 1.0 * 6.6570539474487305
Epoch 80, val loss: 1.8231689929962158
Epoch 90, training loss: 8.351217269897461 = 1.8047786951065063 + 1.0 * 6.546438694000244
Epoch 90, val loss: 1.8086613416671753
Epoch 100, training loss: 8.264140129089355 = 1.7887591123580933 + 1.0 * 6.475380897521973
Epoch 100, val loss: 1.7940417528152466
Epoch 110, training loss: 8.196524620056152 = 1.7727361917495728 + 1.0 * 6.423788070678711
Epoch 110, val loss: 1.7795566320419312
Epoch 120, training loss: 8.143016815185547 = 1.7561266422271729 + 1.0 * 6.386889934539795
Epoch 120, val loss: 1.7649489641189575
Epoch 130, training loss: 8.095972061157227 = 1.7382103204727173 + 1.0 * 6.357761383056641
Epoch 130, val loss: 1.7495667934417725
Epoch 140, training loss: 8.052387237548828 = 1.7184476852416992 + 1.0 * 6.333940029144287
Epoch 140, val loss: 1.7328580617904663
Epoch 150, training loss: 8.008785247802734 = 1.6960045099258423 + 1.0 * 6.312780857086182
Epoch 150, val loss: 1.714124083518982
Epoch 160, training loss: 7.9667863845825195 = 1.6701161861419678 + 1.0 * 6.296670436859131
Epoch 160, val loss: 1.6928255558013916
Epoch 170, training loss: 7.920598983764648 = 1.640419840812683 + 1.0 * 6.280179023742676
Epoch 170, val loss: 1.668552279472351
Epoch 180, training loss: 7.873149871826172 = 1.6063743829727173 + 1.0 * 6.266775608062744
Epoch 180, val loss: 1.640889286994934
Epoch 190, training loss: 7.823557376861572 = 1.5674238204956055 + 1.0 * 6.256133556365967
Epoch 190, val loss: 1.609397053718567
Epoch 200, training loss: 7.769504547119141 = 1.5237228870391846 + 1.0 * 6.245781421661377
Epoch 200, val loss: 1.5742098093032837
Epoch 210, training loss: 7.713645935058594 = 1.4756945371627808 + 1.0 * 6.237951278686523
Epoch 210, val loss: 1.5358741283416748
Epoch 220, training loss: 7.6540656089782715 = 1.4245705604553223 + 1.0 * 6.229495048522949
Epoch 220, val loss: 1.4954543113708496
Epoch 230, training loss: 7.592060089111328 = 1.3710182905197144 + 1.0 * 6.221041679382324
Epoch 230, val loss: 1.4536460638046265
Epoch 240, training loss: 7.534473419189453 = 1.3159672021865845 + 1.0 * 6.218506336212158
Epoch 240, val loss: 1.4110840559005737
Epoch 250, training loss: 7.471359729766846 = 1.2613717317581177 + 1.0 * 6.209988117218018
Epoch 250, val loss: 1.3690377473831177
Epoch 260, training loss: 7.410067558288574 = 1.207573652267456 + 1.0 * 6.202494144439697
Epoch 260, val loss: 1.3279489278793335
Epoch 270, training loss: 7.352058410644531 = 1.1551250219345093 + 1.0 * 6.196933269500732
Epoch 270, val loss: 1.287962555885315
Epoch 280, training loss: 7.296805381774902 = 1.1045302152633667 + 1.0 * 6.192275047302246
Epoch 280, val loss: 1.249369740486145
Epoch 290, training loss: 7.242352485656738 = 1.0558475255966187 + 1.0 * 6.18650484085083
Epoch 290, val loss: 1.2126444578170776
Epoch 300, training loss: 7.195178508758545 = 1.009173035621643 + 1.0 * 6.186005592346191
Epoch 300, val loss: 1.1776458024978638
Epoch 310, training loss: 7.14198637008667 = 0.9646958708763123 + 1.0 * 6.177290439605713
Epoch 310, val loss: 1.1447216272354126
Epoch 320, training loss: 7.096236228942871 = 0.9219813346862793 + 1.0 * 6.174254894256592
Epoch 320, val loss: 1.1132029294967651
Epoch 330, training loss: 7.049464702606201 = 0.880165696144104 + 1.0 * 6.169299125671387
Epoch 330, val loss: 1.082559585571289
Epoch 340, training loss: 7.013449192047119 = 0.8389090299606323 + 1.0 * 6.174540042877197
Epoch 340, val loss: 1.052301049232483
Epoch 350, training loss: 6.963033676147461 = 0.7989485859870911 + 1.0 * 6.1640849113464355
Epoch 350, val loss: 1.022871494293213
Epoch 360, training loss: 6.9185309410095215 = 0.7596077919006348 + 1.0 * 6.158923149108887
Epoch 360, val loss: 0.9939968585968018
Epoch 370, training loss: 6.877223491668701 = 0.720860481262207 + 1.0 * 6.156363010406494
Epoch 370, val loss: 0.9655528664588928
Epoch 380, training loss: 6.8409953117370605 = 0.6829899549484253 + 1.0 * 6.158005237579346
Epoch 380, val loss: 0.9376649260520935
Epoch 390, training loss: 6.797338008880615 = 0.6466747522354126 + 1.0 * 6.150663375854492
Epoch 390, val loss: 0.9111877083778381
Epoch 400, training loss: 6.761869430541992 = 0.6121641993522644 + 1.0 * 6.149705410003662
Epoch 400, val loss: 0.8863664269447327
Epoch 410, training loss: 6.724727630615234 = 0.5791837573051453 + 1.0 * 6.145544052124023
Epoch 410, val loss: 0.8629973530769348
Epoch 420, training loss: 6.6928486824035645 = 0.5477245450019836 + 1.0 * 6.1451239585876465
Epoch 420, val loss: 0.8412153720855713
Epoch 430, training loss: 6.66118860244751 = 0.5180267691612244 + 1.0 * 6.143161773681641
Epoch 430, val loss: 0.8212574124336243
Epoch 440, training loss: 6.628485679626465 = 0.48999258875846863 + 1.0 * 6.138493061065674
Epoch 440, val loss: 0.8033413887023926
Epoch 450, training loss: 6.600930213928223 = 0.4633869528770447 + 1.0 * 6.137543201446533
Epoch 450, val loss: 0.7870804667472839
Epoch 460, training loss: 6.575904369354248 = 0.4382322430610657 + 1.0 * 6.137671947479248
Epoch 460, val loss: 0.7724341154098511
Epoch 470, training loss: 6.546197891235352 = 0.4144402742385864 + 1.0 * 6.131757736206055
Epoch 470, val loss: 0.7597412467002869
Epoch 480, training loss: 6.521424293518066 = 0.3917332887649536 + 1.0 * 6.129691123962402
Epoch 480, val loss: 0.7484496831893921
Epoch 490, training loss: 6.497364521026611 = 0.3700779974460602 + 1.0 * 6.127286434173584
Epoch 490, val loss: 0.7383403182029724
Epoch 500, training loss: 6.475561618804932 = 0.34947019815444946 + 1.0 * 6.126091480255127
Epoch 500, val loss: 0.7298484444618225
Epoch 510, training loss: 6.452661514282227 = 0.3296937644481659 + 1.0 * 6.122967720031738
Epoch 510, val loss: 0.7224588394165039
Epoch 520, training loss: 6.442349910736084 = 0.3106623888015747 + 1.0 * 6.131687641143799
Epoch 520, val loss: 0.7160622477531433
Epoch 530, training loss: 6.413388729095459 = 0.2924079895019531 + 1.0 * 6.120980739593506
Epoch 530, val loss: 0.7107021808624268
Epoch 540, training loss: 6.395788192749023 = 0.2748491168022156 + 1.0 * 6.120939254760742
Epoch 540, val loss: 0.7064211964607239
Epoch 550, training loss: 6.373801231384277 = 0.2579878270626068 + 1.0 * 6.115813255310059
Epoch 550, val loss: 0.7029275298118591
Epoch 560, training loss: 6.356746673583984 = 0.24178361892700195 + 1.0 * 6.114963054656982
Epoch 560, val loss: 0.7003430128097534
Epoch 570, training loss: 6.344880104064941 = 0.22625760734081268 + 1.0 * 6.118622303009033
Epoch 570, val loss: 0.6986091136932373
Epoch 580, training loss: 6.326277256011963 = 0.21149851381778717 + 1.0 * 6.114778518676758
Epoch 580, val loss: 0.6976487040519714
Epoch 590, training loss: 6.306806564331055 = 0.19757626950740814 + 1.0 * 6.1092305183410645
Epoch 590, val loss: 0.6975923776626587
Epoch 600, training loss: 6.296058654785156 = 0.1844031810760498 + 1.0 * 6.1116557121276855
Epoch 600, val loss: 0.6982854008674622
Epoch 610, training loss: 6.28323221206665 = 0.17207390069961548 + 1.0 * 6.11115837097168
Epoch 610, val loss: 0.6995421648025513
Epoch 620, training loss: 6.266167640686035 = 0.16056938469409943 + 1.0 * 6.105598449707031
Epoch 620, val loss: 0.7016621232032776
Epoch 630, training loss: 6.2524189949035645 = 0.1498105823993683 + 1.0 * 6.1026082038879395
Epoch 630, val loss: 0.7043312788009644
Epoch 640, training loss: 6.246384620666504 = 0.13975216448307037 + 1.0 * 6.106632232666016
Epoch 640, val loss: 0.707557201385498
Epoch 650, training loss: 6.236187934875488 = 0.13053657114505768 + 1.0 * 6.105651378631592
Epoch 650, val loss: 0.7110446691513062
Epoch 660, training loss: 6.2212958335876465 = 0.12201200425624847 + 1.0 * 6.099283695220947
Epoch 660, val loss: 0.7152093052864075
Epoch 670, training loss: 6.210848808288574 = 0.11412271112203598 + 1.0 * 6.096725940704346
Epoch 670, val loss: 0.7197172045707703
Epoch 680, training loss: 6.216244697570801 = 0.10682849586009979 + 1.0 * 6.1094160079956055
Epoch 680, val loss: 0.7245208621025085
Epoch 690, training loss: 6.193941593170166 = 0.10013917088508606 + 1.0 * 6.093802452087402
Epoch 690, val loss: 0.7294337749481201
Epoch 700, training loss: 6.187154769897461 = 0.09397181868553162 + 1.0 * 6.0931830406188965
Epoch 700, val loss: 0.7347883582115173
Epoch 710, training loss: 6.18212890625 = 0.08827099204063416 + 1.0 * 6.093857765197754
Epoch 710, val loss: 0.740276575088501
Epoch 720, training loss: 6.173341274261475 = 0.08302506804466248 + 1.0 * 6.090316295623779
Epoch 720, val loss: 0.7458725571632385
Epoch 730, training loss: 6.167823791503906 = 0.07818879187107086 + 1.0 * 6.089634895324707
Epoch 730, val loss: 0.7517401576042175
Epoch 740, training loss: 6.164851665496826 = 0.0737258642911911 + 1.0 * 6.091125965118408
Epoch 740, val loss: 0.7575784921646118
Epoch 750, training loss: 6.155290126800537 = 0.06960684061050415 + 1.0 * 6.085683345794678
Epoch 750, val loss: 0.7635400295257568
Epoch 760, training loss: 6.150354862213135 = 0.06580320000648499 + 1.0 * 6.084551811218262
Epoch 760, val loss: 0.7695723176002502
Epoch 770, training loss: 6.150587558746338 = 0.06228259205818176 + 1.0 * 6.0883049964904785
Epoch 770, val loss: 0.7756195068359375
Epoch 780, training loss: 6.1455488204956055 = 0.05904817581176758 + 1.0 * 6.086500644683838
Epoch 780, val loss: 0.7816992998123169
Epoch 790, training loss: 6.139598369598389 = 0.05604282766580582 + 1.0 * 6.083555698394775
Epoch 790, val loss: 0.7877577543258667
Epoch 800, training loss: 6.135275363922119 = 0.05325734615325928 + 1.0 * 6.08201789855957
Epoch 800, val loss: 0.7938629984855652
Epoch 810, training loss: 6.13236141204834 = 0.050676584243774414 + 1.0 * 6.0816850662231445
Epoch 810, val loss: 0.7999856472015381
Epoch 820, training loss: 6.130026817321777 = 0.04827060177922249 + 1.0 * 6.081756114959717
Epoch 820, val loss: 0.8059763312339783
Epoch 830, training loss: 6.123509883880615 = 0.04603315889835358 + 1.0 * 6.077476501464844
Epoch 830, val loss: 0.8120021820068359
Epoch 840, training loss: 6.120055198669434 = 0.04394582286477089 + 1.0 * 6.076109409332275
Epoch 840, val loss: 0.8180244565010071
Epoch 850, training loss: 6.121441841125488 = 0.0419953279197216 + 1.0 * 6.079446315765381
Epoch 850, val loss: 0.8239914774894714
Epoch 860, training loss: 6.11737060546875 = 0.04016691446304321 + 1.0 * 6.077203750610352
Epoch 860, val loss: 0.8297444581985474
Epoch 870, training loss: 6.115775108337402 = 0.03847220540046692 + 1.0 * 6.077302932739258
Epoch 870, val loss: 0.8356531262397766
Epoch 880, training loss: 6.109313488006592 = 0.03688506409525871 + 1.0 * 6.072428226470947
Epoch 880, val loss: 0.8413881063461304
Epoch 890, training loss: 6.106341361999512 = 0.0353899784386158 + 1.0 * 6.070951461791992
Epoch 890, val loss: 0.8471449017524719
Epoch 900, training loss: 6.104110240936279 = 0.033978596329689026 + 1.0 * 6.070131778717041
Epoch 900, val loss: 0.8528360724449158
Epoch 910, training loss: 6.110933303833008 = 0.032653093338012695 + 1.0 * 6.078279972076416
Epoch 910, val loss: 0.8584316968917847
Epoch 920, training loss: 6.1028151512146 = 0.03141099587082863 + 1.0 * 6.071403980255127
Epoch 920, val loss: 0.8638608455657959
Epoch 930, training loss: 6.099050998687744 = 0.030240342020988464 + 1.0 * 6.06881046295166
Epoch 930, val loss: 0.8693486452102661
Epoch 940, training loss: 6.099864959716797 = 0.02913772501051426 + 1.0 * 6.070727348327637
Epoch 940, val loss: 0.8746940493583679
Epoch 950, training loss: 6.094610691070557 = 0.028093693777918816 + 1.0 * 6.066516876220703
Epoch 950, val loss: 0.8800643086433411
Epoch 960, training loss: 6.0911688804626465 = 0.027103563770651817 + 1.0 * 6.064065456390381
Epoch 960, val loss: 0.8853950500488281
Epoch 970, training loss: 6.092499256134033 = 0.026164766401052475 + 1.0 * 6.0663347244262695
Epoch 970, val loss: 0.8906911611557007
Epoch 980, training loss: 6.094956398010254 = 0.025277582928538322 + 1.0 * 6.069678783416748
Epoch 980, val loss: 0.8957667946815491
Epoch 990, training loss: 6.088738918304443 = 0.024440718814730644 + 1.0 * 6.064298152923584
Epoch 990, val loss: 0.9008939862251282
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6863
Flip ASR: 0.6222/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.33125114440918 = 1.9573469161987305 + 1.0 * 8.37390422821045
Epoch 0, val loss: 1.9588632583618164
Epoch 10, training loss: 10.320718765258789 = 1.9470995664596558 + 1.0 * 8.373619079589844
Epoch 10, val loss: 1.9490333795547485
Epoch 20, training loss: 10.306413650512695 = 1.9347105026245117 + 1.0 * 8.371703147888184
Epoch 20, val loss: 1.9368125200271606
Epoch 30, training loss: 10.276153564453125 = 1.917778730392456 + 1.0 * 8.35837459564209
Epoch 30, val loss: 1.9198169708251953
Epoch 40, training loss: 10.173542976379395 = 1.8952158689498901 + 1.0 * 8.278326988220215
Epoch 40, val loss: 1.897746205329895
Epoch 50, training loss: 9.717804908752441 = 1.8702802658081055 + 1.0 * 7.847524642944336
Epoch 50, val loss: 1.8736592531204224
Epoch 60, training loss: 9.221115112304688 = 1.8481355905532837 + 1.0 * 7.372979164123535
Epoch 60, val loss: 1.8533183336257935
Epoch 70, training loss: 8.881766319274902 = 1.8314956426620483 + 1.0 * 7.050271034240723
Epoch 70, val loss: 1.8382991552352905
Epoch 80, training loss: 8.67197036743164 = 1.813768744468689 + 1.0 * 6.858201503753662
Epoch 80, val loss: 1.8221356868743896
Epoch 90, training loss: 8.530729293823242 = 1.7940946817398071 + 1.0 * 6.736634254455566
Epoch 90, val loss: 1.8041480779647827
Epoch 100, training loss: 8.413206100463867 = 1.7741923332214355 + 1.0 * 6.63901424407959
Epoch 100, val loss: 1.7865649461746216
Epoch 110, training loss: 8.321178436279297 = 1.7549012899398804 + 1.0 * 6.566277027130127
Epoch 110, val loss: 1.7701020240783691
Epoch 120, training loss: 8.248527526855469 = 1.734368920326233 + 1.0 * 6.514158725738525
Epoch 120, val loss: 1.7523682117462158
Epoch 130, training loss: 8.186501502990723 = 1.7110730409622192 + 1.0 * 6.475428581237793
Epoch 130, val loss: 1.7321935892105103
Epoch 140, training loss: 8.127801895141602 = 1.6845006942749023 + 1.0 * 6.443301200866699
Epoch 140, val loss: 1.7095046043395996
Epoch 150, training loss: 8.07122802734375 = 1.6536188125610352 + 1.0 * 6.417608737945557
Epoch 150, val loss: 1.6838107109069824
Epoch 160, training loss: 8.010995864868164 = 1.6178781986236572 + 1.0 * 6.393117904663086
Epoch 160, val loss: 1.6548011302947998
Epoch 170, training loss: 7.946629524230957 = 1.576729655265808 + 1.0 * 6.369899749755859
Epoch 170, val loss: 1.6219863891601562
Epoch 180, training loss: 7.879125595092773 = 1.5297291278839111 + 1.0 * 6.349396705627441
Epoch 180, val loss: 1.584687352180481
Epoch 190, training loss: 7.8091020584106445 = 1.4766273498535156 + 1.0 * 6.332474708557129
Epoch 190, val loss: 1.5426214933395386
Epoch 200, training loss: 7.7363739013671875 = 1.4188077449798584 + 1.0 * 6.317566394805908
Epoch 200, val loss: 1.4970718622207642
Epoch 210, training loss: 7.660859107971191 = 1.3575023412704468 + 1.0 * 6.303356647491455
Epoch 210, val loss: 1.449084997177124
Epoch 220, training loss: 7.587784290313721 = 1.2941831350326538 + 1.0 * 6.293601036071777
Epoch 220, val loss: 1.399931788444519
Epoch 230, training loss: 7.514837741851807 = 1.2312310934066772 + 1.0 * 6.28360652923584
Epoch 230, val loss: 1.3517402410507202
Epoch 240, training loss: 7.4451189041137695 = 1.1702090501785278 + 1.0 * 6.274909973144531
Epoch 240, val loss: 1.3053025007247925
Epoch 250, training loss: 7.381433010101318 = 1.1119422912597656 + 1.0 * 6.269490718841553
Epoch 250, val loss: 1.261370062828064
Epoch 260, training loss: 7.318410873413086 = 1.0576598644256592 + 1.0 * 6.260751247406006
Epoch 260, val loss: 1.2208045721054077
Epoch 270, training loss: 7.258729934692383 = 1.006807565689087 + 1.0 * 6.251922607421875
Epoch 270, val loss: 1.1834250688552856
Epoch 280, training loss: 7.210299491882324 = 0.9588435292243958 + 1.0 * 6.251455783843994
Epoch 280, val loss: 1.1487549543380737
Epoch 290, training loss: 7.155701637268066 = 0.9139888286590576 + 1.0 * 6.24171257019043
Epoch 290, val loss: 1.1171026229858398
Epoch 300, training loss: 7.105706214904785 = 0.8714821934700012 + 1.0 * 6.23422384262085
Epoch 300, val loss: 1.0878305435180664
Epoch 310, training loss: 7.061171054840088 = 0.8304734826087952 + 1.0 * 6.2306976318359375
Epoch 310, val loss: 1.0601953268051147
Epoch 320, training loss: 7.013488292694092 = 0.7910017371177673 + 1.0 * 6.22248649597168
Epoch 320, val loss: 1.0341970920562744
Epoch 330, training loss: 6.973989486694336 = 0.7528355121612549 + 1.0 * 6.221153736114502
Epoch 330, val loss: 1.0098685026168823
Epoch 340, training loss: 6.930144309997559 = 0.7166621088981628 + 1.0 * 6.21348237991333
Epoch 340, val loss: 0.987474799156189
Epoch 350, training loss: 6.890471458435059 = 0.6822074055671692 + 1.0 * 6.208263874053955
Epoch 350, val loss: 0.9672361016273499
Epoch 360, training loss: 6.8585100173950195 = 0.6493051648139954 + 1.0 * 6.20920467376709
Epoch 360, val loss: 0.9488159418106079
Epoch 370, training loss: 6.819131374359131 = 0.6182345151901245 + 1.0 * 6.200896739959717
Epoch 370, val loss: 0.9322896003723145
Epoch 380, training loss: 6.782942771911621 = 0.5887198448181152 + 1.0 * 6.194222927093506
Epoch 380, val loss: 0.9178342223167419
Epoch 390, training loss: 6.750865936279297 = 0.5604028701782227 + 1.0 * 6.190463066101074
Epoch 390, val loss: 0.9048582911491394
Epoch 400, training loss: 6.720878601074219 = 0.5333448052406311 + 1.0 * 6.187533855438232
Epoch 400, val loss: 0.8931741714477539
Epoch 410, training loss: 6.689899921417236 = 0.5074576735496521 + 1.0 * 6.1824421882629395
Epoch 410, val loss: 0.8829690217971802
Epoch 420, training loss: 6.661194324493408 = 0.4824466407299042 + 1.0 * 6.178747653961182
Epoch 420, val loss: 0.8739211559295654
Epoch 430, training loss: 6.641261100769043 = 0.4583291709423065 + 1.0 * 6.182931900024414
Epoch 430, val loss: 0.8656564950942993
Epoch 440, training loss: 6.607610702514648 = 0.4352079927921295 + 1.0 * 6.172402858734131
Epoch 440, val loss: 0.8586933016777039
Epoch 450, training loss: 6.581901550292969 = 0.41272857785224915 + 1.0 * 6.169172763824463
Epoch 450, val loss: 0.8526759743690491
Epoch 460, training loss: 6.557246208190918 = 0.39076289534568787 + 1.0 * 6.166483402252197
Epoch 460, val loss: 0.8473239541053772
Epoch 470, training loss: 6.5379180908203125 = 0.36939966678619385 + 1.0 * 6.168518543243408
Epoch 470, val loss: 0.8426491022109985
Epoch 480, training loss: 6.51061487197876 = 0.34871774911880493 + 1.0 * 6.1618971824646
Epoch 480, val loss: 0.8389477133750916
Epoch 490, training loss: 6.488068580627441 = 0.328655481338501 + 1.0 * 6.159412860870361
Epoch 490, val loss: 0.8359575867652893
Epoch 500, training loss: 6.466558933258057 = 0.3093031942844391 + 1.0 * 6.15725564956665
Epoch 500, val loss: 0.8335954546928406
Epoch 510, training loss: 6.448112487792969 = 0.2907695472240448 + 1.0 * 6.157342910766602
Epoch 510, val loss: 0.8318817615509033
Epoch 520, training loss: 6.4241557121276855 = 0.2730746567249298 + 1.0 * 6.151081085205078
Epoch 520, val loss: 0.8311007022857666
Epoch 530, training loss: 6.405012607574463 = 0.25616246461868286 + 1.0 * 6.148849964141846
Epoch 530, val loss: 0.830928385257721
Epoch 540, training loss: 6.3863844871521 = 0.24012689292430878 + 1.0 * 6.146257400512695
Epoch 540, val loss: 0.8313250541687012
Epoch 550, training loss: 6.371628284454346 = 0.2249913513660431 + 1.0 * 6.146636962890625
Epoch 550, val loss: 0.8324546813964844
Epoch 560, training loss: 6.353682041168213 = 0.21074514091014862 + 1.0 * 6.142936706542969
Epoch 560, val loss: 0.8340405225753784
Epoch 570, training loss: 6.336941719055176 = 0.19738157093524933 + 1.0 * 6.139560222625732
Epoch 570, val loss: 0.8363199234008789
Epoch 580, training loss: 6.3289361000061035 = 0.18486179411411285 + 1.0 * 6.144074440002441
Epoch 580, val loss: 0.8390476107597351
Epoch 590, training loss: 6.312367916107178 = 0.1732582002878189 + 1.0 * 6.1391096115112305
Epoch 590, val loss: 0.8421558141708374
Epoch 600, training loss: 6.297140598297119 = 0.1624860167503357 + 1.0 * 6.134654521942139
Epoch 600, val loss: 0.845834493637085
Epoch 610, training loss: 6.284671783447266 = 0.15245485305786133 + 1.0 * 6.132216930389404
Epoch 610, val loss: 0.8499497771263123
Epoch 620, training loss: 6.277646064758301 = 0.1431204080581665 + 1.0 * 6.134525775909424
Epoch 620, val loss: 0.8543952703475952
Epoch 630, training loss: 6.2718329429626465 = 0.13450565934181213 + 1.0 * 6.137327194213867
Epoch 630, val loss: 0.8591619729995728
Epoch 640, training loss: 6.253204345703125 = 0.12652981281280518 + 1.0 * 6.126674652099609
Epoch 640, val loss: 0.8643559813499451
Epoch 650, training loss: 6.244984149932861 = 0.11915235221385956 + 1.0 * 6.125831604003906
Epoch 650, val loss: 0.8699154257774353
Epoch 660, training loss: 6.24424409866333 = 0.11231344938278198 + 1.0 * 6.131930828094482
Epoch 660, val loss: 0.8756988644599915
Epoch 670, training loss: 6.231649398803711 = 0.10599338263273239 + 1.0 * 6.1256561279296875
Epoch 670, val loss: 0.8817859888076782
Epoch 680, training loss: 6.221913814544678 = 0.10012146085500717 + 1.0 * 6.121792316436768
Epoch 680, val loss: 0.8881770968437195
Epoch 690, training loss: 6.217048168182373 = 0.09466198086738586 + 1.0 * 6.1223859786987305
Epoch 690, val loss: 0.8947727084159851
Epoch 700, training loss: 6.209420204162598 = 0.08957832306623459 + 1.0 * 6.119842052459717
Epoch 700, val loss: 0.90145343542099
Epoch 710, training loss: 6.201952934265137 = 0.0848514661192894 + 1.0 * 6.117101669311523
Epoch 710, val loss: 0.9084147214889526
Epoch 720, training loss: 6.200191497802734 = 0.0804622620344162 + 1.0 * 6.119729042053223
Epoch 720, val loss: 0.9155292510986328
Epoch 730, training loss: 6.190692901611328 = 0.07639412581920624 + 1.0 * 6.1142988204956055
Epoch 730, val loss: 0.9227243661880493
Epoch 740, training loss: 6.18545389175415 = 0.07259908318519592 + 1.0 * 6.112854957580566
Epoch 740, val loss: 0.9302978515625
Epoch 750, training loss: 6.185837745666504 = 0.06906089931726456 + 1.0 * 6.116776943206787
Epoch 750, val loss: 0.9377332925796509
Epoch 760, training loss: 6.175589084625244 = 0.06576094776391983 + 1.0 * 6.109827995300293
Epoch 760, val loss: 0.9454371333122253
Epoch 770, training loss: 6.173539161682129 = 0.06267306953668594 + 1.0 * 6.110866069793701
Epoch 770, val loss: 0.9532569050788879
Epoch 780, training loss: 6.166640281677246 = 0.05978778004646301 + 1.0 * 6.1068525314331055
Epoch 780, val loss: 0.9610534906387329
Epoch 790, training loss: 6.163944721221924 = 0.0570843368768692 + 1.0 * 6.106860160827637
Epoch 790, val loss: 0.9690569639205933
Epoch 800, training loss: 6.16351318359375 = 0.05454705283045769 + 1.0 * 6.10896635055542
Epoch 800, val loss: 0.97710120677948
Epoch 810, training loss: 6.156152725219727 = 0.05216733738780022 + 1.0 * 6.10398530960083
Epoch 810, val loss: 0.9850947856903076
Epoch 820, training loss: 6.155873775482178 = 0.04992983862757683 + 1.0 * 6.1059441566467285
Epoch 820, val loss: 0.9932308793067932
Epoch 830, training loss: 6.152853488922119 = 0.047827742993831635 + 1.0 * 6.105025768280029
Epoch 830, val loss: 1.0011759996414185
Epoch 840, training loss: 6.149206638336182 = 0.045853909105062485 + 1.0 * 6.1033525466918945
Epoch 840, val loss: 1.0092350244522095
Epoch 850, training loss: 6.1433258056640625 = 0.04399239644408226 + 1.0 * 6.0993332862854
Epoch 850, val loss: 1.0174096822738647
Epoch 860, training loss: 6.140647888183594 = 0.04223368689417839 + 1.0 * 6.098414421081543
Epoch 860, val loss: 1.0254862308502197
Epoch 870, training loss: 6.145182132720947 = 0.04057353734970093 + 1.0 * 6.104608535766602
Epoch 870, val loss: 1.0334450006484985
Epoch 880, training loss: 6.138315677642822 = 0.03900109976530075 + 1.0 * 6.0993146896362305
Epoch 880, val loss: 1.0412431955337524
Epoch 890, training loss: 6.134589195251465 = 0.0375220812857151 + 1.0 * 6.097066879272461
Epoch 890, val loss: 1.0492427349090576
Epoch 900, training loss: 6.130591869354248 = 0.03611631691455841 + 1.0 * 6.094475746154785
Epoch 900, val loss: 1.0569746494293213
Epoch 910, training loss: 6.135951995849609 = 0.03478503227233887 + 1.0 * 6.10116720199585
Epoch 910, val loss: 1.0646294355392456
Epoch 920, training loss: 6.128871440887451 = 0.033529672771692276 + 1.0 * 6.095341682434082
Epoch 920, val loss: 1.072261095046997
Epoch 930, training loss: 6.124683380126953 = 0.032335758209228516 + 1.0 * 6.092347621917725
Epoch 930, val loss: 1.0798717737197876
Epoch 940, training loss: 6.122329235076904 = 0.031201645731925964 + 1.0 * 6.091127395629883
Epoch 940, val loss: 1.0874216556549072
Epoch 950, training loss: 6.122553825378418 = 0.030123762786388397 + 1.0 * 6.092430114746094
Epoch 950, val loss: 1.094730019569397
Epoch 960, training loss: 6.124162673950195 = 0.02909727394580841 + 1.0 * 6.095065593719482
Epoch 960, val loss: 1.1019823551177979
Epoch 970, training loss: 6.1193528175354 = 0.02812597155570984 + 1.0 * 6.091227054595947
Epoch 970, val loss: 1.1091586351394653
Epoch 980, training loss: 6.11352014541626 = 0.027199676260352135 + 1.0 * 6.086320400238037
Epoch 980, val loss: 1.1164215803146362
Epoch 990, training loss: 6.112593650817871 = 0.026314938440918922 + 1.0 * 6.086278915405273
Epoch 990, val loss: 1.1235235929489136
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7934
Flip ASR: 0.7556/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.310595512390137 = 1.9367693662643433 + 1.0 * 8.373826026916504
Epoch 0, val loss: 1.9294923543930054
Epoch 10, training loss: 10.299583435058594 = 1.9267213344573975 + 1.0 * 8.372861862182617
Epoch 10, val loss: 1.9192931652069092
Epoch 20, training loss: 10.284075736999512 = 1.91455078125 + 1.0 * 8.369524955749512
Epoch 20, val loss: 1.906030297279358
Epoch 30, training loss: 10.253853797912598 = 1.8978396654129028 + 1.0 * 8.356014251708984
Epoch 30, val loss: 1.887067437171936
Epoch 40, training loss: 10.156970977783203 = 1.8760921955108643 + 1.0 * 8.280879020690918
Epoch 40, val loss: 1.863058090209961
Epoch 50, training loss: 9.774041175842285 = 1.8543387651443481 + 1.0 * 7.919702529907227
Epoch 50, val loss: 1.839940071105957
Epoch 60, training loss: 9.224364280700684 = 1.8330904245376587 + 1.0 * 7.3912739753723145
Epoch 60, val loss: 1.8176237344741821
Epoch 70, training loss: 8.771687507629395 = 1.8129222393035889 + 1.0 * 6.958765029907227
Epoch 70, val loss: 1.7967618703842163
Epoch 80, training loss: 8.572237014770508 = 1.7940824031829834 + 1.0 * 6.7781548500061035
Epoch 80, val loss: 1.7777022123336792
Epoch 90, training loss: 8.436637878417969 = 1.7761411666870117 + 1.0 * 6.660496234893799
Epoch 90, val loss: 1.7598869800567627
Epoch 100, training loss: 8.332635879516602 = 1.75677490234375 + 1.0 * 6.575860977172852
Epoch 100, val loss: 1.7413241863250732
Epoch 110, training loss: 8.254436492919922 = 1.735867977142334 + 1.0 * 6.518568515777588
Epoch 110, val loss: 1.7220333814620972
Epoch 120, training loss: 8.18648910522461 = 1.7130305767059326 + 1.0 * 6.473458766937256
Epoch 120, val loss: 1.7015365362167358
Epoch 130, training loss: 8.124756813049316 = 1.6874336004257202 + 1.0 * 6.437323093414307
Epoch 130, val loss: 1.6790622472763062
Epoch 140, training loss: 8.067119598388672 = 1.6579580307006836 + 1.0 * 6.409161567687988
Epoch 140, val loss: 1.653738021850586
Epoch 150, training loss: 8.010187149047852 = 1.6236921548843384 + 1.0 * 6.3864946365356445
Epoch 150, val loss: 1.6246238946914673
Epoch 160, training loss: 7.947894096374512 = 1.5840953588485718 + 1.0 * 6.36379861831665
Epoch 160, val loss: 1.5917582511901855
Epoch 170, training loss: 7.882673263549805 = 1.5388485193252563 + 1.0 * 6.343824863433838
Epoch 170, val loss: 1.5545566082000732
Epoch 180, training loss: 7.814460754394531 = 1.4877504110336304 + 1.0 * 6.326710224151611
Epoch 180, val loss: 1.513291597366333
Epoch 190, training loss: 7.742648124694824 = 1.4319199323654175 + 1.0 * 6.310728073120117
Epoch 190, val loss: 1.4687782526016235
Epoch 200, training loss: 7.669670581817627 = 1.3720159530639648 + 1.0 * 6.297654628753662
Epoch 200, val loss: 1.4224106073379517
Epoch 210, training loss: 7.599305629730225 = 1.3094362020492554 + 1.0 * 6.28986930847168
Epoch 210, val loss: 1.3753894567489624
Epoch 220, training loss: 7.522040367126465 = 1.2462164163589478 + 1.0 * 6.275824069976807
Epoch 220, val loss: 1.32938814163208
Epoch 230, training loss: 7.447782039642334 = 1.18254554271698 + 1.0 * 6.2652363777160645
Epoch 230, val loss: 1.2841753959655762
Epoch 240, training loss: 7.3766021728515625 = 1.1189582347869873 + 1.0 * 6.257643699645996
Epoch 240, val loss: 1.2403391599655151
Epoch 250, training loss: 7.304811000823975 = 1.0565009117126465 + 1.0 * 6.248310089111328
Epoch 250, val loss: 1.1975902318954468
Epoch 260, training loss: 7.235477447509766 = 0.9951905012130737 + 1.0 * 6.240286827087402
Epoch 260, val loss: 1.1559430360794067
Epoch 270, training loss: 7.171117305755615 = 0.9356752634048462 + 1.0 * 6.235442161560059
Epoch 270, val loss: 1.1156858205795288
Epoch 280, training loss: 7.106369495391846 = 0.8795656561851501 + 1.0 * 6.226803779602051
Epoch 280, val loss: 1.0777497291564941
Epoch 290, training loss: 7.048247814178467 = 0.8277469277381897 + 1.0 * 6.220500946044922
Epoch 290, val loss: 1.0424751043319702
Epoch 300, training loss: 6.9945526123046875 = 0.7802838683128357 + 1.0 * 6.214268684387207
Epoch 300, val loss: 1.0101059675216675
Epoch 310, training loss: 6.944334983825684 = 0.7366900444030762 + 1.0 * 6.207644939422607
Epoch 310, val loss: 0.9803975224494934
Epoch 320, training loss: 6.904961585998535 = 0.6966613531112671 + 1.0 * 6.2083001136779785
Epoch 320, val loss: 0.9534847736358643
Epoch 330, training loss: 6.858035564422607 = 0.6603428721427917 + 1.0 * 6.19769287109375
Epoch 330, val loss: 0.9292882680892944
Epoch 340, training loss: 6.817983150482178 = 0.6266937851905823 + 1.0 * 6.19128942489624
Epoch 340, val loss: 0.907498300075531
Epoch 350, training loss: 6.781370639801025 = 0.5949855446815491 + 1.0 * 6.186385154724121
Epoch 350, val loss: 0.8875833749771118
Epoch 360, training loss: 6.754242897033691 = 0.564934253692627 + 1.0 * 6.1893086433410645
Epoch 360, val loss: 0.8694681525230408
Epoch 370, training loss: 6.714198112487793 = 0.5364033579826355 + 1.0 * 6.177794933319092
Epoch 370, val loss: 0.8529365658760071
Epoch 380, training loss: 6.682154178619385 = 0.5088269114494324 + 1.0 * 6.173327445983887
Epoch 380, val loss: 0.8379324078559875
Epoch 390, training loss: 6.654637813568115 = 0.48199155926704407 + 1.0 * 6.1726460456848145
Epoch 390, val loss: 0.8241755366325378
Epoch 400, training loss: 6.623996734619141 = 0.4559643268585205 + 1.0 * 6.168032169342041
Epoch 400, val loss: 0.8115457892417908
Epoch 410, training loss: 6.600632667541504 = 0.4309529960155487 + 1.0 * 6.169679641723633
Epoch 410, val loss: 0.8000510931015015
Epoch 420, training loss: 6.566564559936523 = 0.40689817070961 + 1.0 * 6.159666538238525
Epoch 420, val loss: 0.7901163697242737
Epoch 430, training loss: 6.5385212898254395 = 0.38373926281929016 + 1.0 * 6.154781818389893
Epoch 430, val loss: 0.7814823389053345
Epoch 440, training loss: 6.512824058532715 = 0.36146271228790283 + 1.0 * 6.151361465454102
Epoch 440, val loss: 0.7740749716758728
Epoch 450, training loss: 6.488940715789795 = 0.34007447957992554 + 1.0 * 6.148866176605225
Epoch 450, val loss: 0.7678380608558655
Epoch 460, training loss: 6.476254940032959 = 0.31971463561058044 + 1.0 * 6.156540393829346
Epoch 460, val loss: 0.7628635168075562
Epoch 470, training loss: 6.445780277252197 = 0.3007688820362091 + 1.0 * 6.1450114250183105
Epoch 470, val loss: 0.7588564157485962
Epoch 480, training loss: 6.424091339111328 = 0.28289780020713806 + 1.0 * 6.141193389892578
Epoch 480, val loss: 0.7561230659484863
Epoch 490, training loss: 6.4036970138549805 = 0.2659371495246887 + 1.0 * 6.137759685516357
Epoch 490, val loss: 0.7545550465583801
Epoch 500, training loss: 6.384837627410889 = 0.24985694885253906 + 1.0 * 6.13498067855835
Epoch 500, val loss: 0.753842830657959
Epoch 510, training loss: 6.373902797698975 = 0.23467981815338135 + 1.0 * 6.139223098754883
Epoch 510, val loss: 0.7540701031684875
Epoch 520, training loss: 6.35148286819458 = 0.22058910131454468 + 1.0 * 6.130893707275391
Epoch 520, val loss: 0.7550126910209656
Epoch 530, training loss: 6.336941719055176 = 0.20735704898834229 + 1.0 * 6.129584789276123
Epoch 530, val loss: 0.756829559803009
Epoch 540, training loss: 6.324978828430176 = 0.19491086900234222 + 1.0 * 6.130067825317383
Epoch 540, val loss: 0.7594618201255798
Epoch 550, training loss: 6.312335968017578 = 0.18325895071029663 + 1.0 * 6.129076957702637
Epoch 550, val loss: 0.7626329660415649
Epoch 560, training loss: 6.295356273651123 = 0.17241418361663818 + 1.0 * 6.122941970825195
Epoch 560, val loss: 0.7662662267684937
Epoch 570, training loss: 6.281707286834717 = 0.16223059594631195 + 1.0 * 6.119476795196533
Epoch 570, val loss: 0.7705325484275818
Epoch 580, training loss: 6.2761969566345215 = 0.15265816450119019 + 1.0 * 6.123538970947266
Epoch 580, val loss: 0.7752367258071899
Epoch 590, training loss: 6.2708516120910645 = 0.14367817342281342 + 1.0 * 6.12717342376709
Epoch 590, val loss: 0.7803037166595459
Epoch 600, training loss: 6.251208305358887 = 0.13536417484283447 + 1.0 * 6.115844249725342
Epoch 600, val loss: 0.7855949401855469
Epoch 610, training loss: 6.239680767059326 = 0.12755559384822845 + 1.0 * 6.112125396728516
Epoch 610, val loss: 0.7910568714141846
Epoch 620, training loss: 6.22994327545166 = 0.12020173668861389 + 1.0 * 6.109741687774658
Epoch 620, val loss: 0.7970409989356995
Epoch 630, training loss: 6.225167274475098 = 0.11329957842826843 + 1.0 * 6.111867904663086
Epoch 630, val loss: 0.8033027052879333
Epoch 640, training loss: 6.218982219696045 = 0.10685940831899643 + 1.0 * 6.112123012542725
Epoch 640, val loss: 0.8098489046096802
Epoch 650, training loss: 6.206165790557861 = 0.10089753568172455 + 1.0 * 6.105268478393555
Epoch 650, val loss: 0.8164597153663635
Epoch 660, training loss: 6.198852062225342 = 0.09531343728303909 + 1.0 * 6.103538513183594
Epoch 660, val loss: 0.823459267616272
Epoch 670, training loss: 6.200855255126953 = 0.09009206295013428 + 1.0 * 6.110763072967529
Epoch 670, val loss: 0.8307693600654602
Epoch 680, training loss: 6.1867194175720215 = 0.08524459600448608 + 1.0 * 6.101474761962891
Epoch 680, val loss: 0.8380645513534546
Epoch 690, training loss: 6.1795525550842285 = 0.08072022348642349 + 1.0 * 6.098832130432129
Epoch 690, val loss: 0.8456881046295166
Epoch 700, training loss: 6.179606914520264 = 0.07649201899766922 + 1.0 * 6.103115081787109
Epoch 700, val loss: 0.8534823656082153
Epoch 710, training loss: 6.174281597137451 = 0.07255594432353973 + 1.0 * 6.1017255783081055
Epoch 710, val loss: 0.8614320755004883
Epoch 720, training loss: 6.163242340087891 = 0.06891599297523499 + 1.0 * 6.094326496124268
Epoch 720, val loss: 0.8692826628684998
Epoch 730, training loss: 6.160233974456787 = 0.06550893932580948 + 1.0 * 6.094725131988525
Epoch 730, val loss: 0.8773939609527588
Epoch 740, training loss: 6.154162406921387 = 0.06233802065253258 + 1.0 * 6.091824531555176
Epoch 740, val loss: 0.8855802416801453
Epoch 750, training loss: 6.151001930236816 = 0.059389904141426086 + 1.0 * 6.091611862182617
Epoch 750, val loss: 0.8935169577598572
Epoch 760, training loss: 6.146700382232666 = 0.056630633771419525 + 1.0 * 6.090069770812988
Epoch 760, val loss: 0.9015886187553406
Epoch 770, training loss: 6.145960330963135 = 0.054050181061029434 + 1.0 * 6.091910362243652
Epoch 770, val loss: 0.9097752571105957
Epoch 780, training loss: 6.140448093414307 = 0.05164933577179909 + 1.0 * 6.088798522949219
Epoch 780, val loss: 0.9177103638648987
Epoch 790, training loss: 6.135013103485107 = 0.04938941448926926 + 1.0 * 6.085623741149902
Epoch 790, val loss: 0.9256795048713684
Epoch 800, training loss: 6.132108688354492 = 0.04725886881351471 + 1.0 * 6.084849834442139
Epoch 800, val loss: 0.9337419271469116
Epoch 810, training loss: 6.133553981781006 = 0.045265648514032364 + 1.0 * 6.088288307189941
Epoch 810, val loss: 0.9418380260467529
Epoch 820, training loss: 6.127620220184326 = 0.04338478296995163 + 1.0 * 6.084235668182373
Epoch 820, val loss: 0.9495858550071716
Epoch 830, training loss: 6.12270450592041 = 0.04161686450242996 + 1.0 * 6.081087589263916
Epoch 830, val loss: 0.9574110507965088
Epoch 840, training loss: 6.12031888961792 = 0.0399506539106369 + 1.0 * 6.0803680419921875
Epoch 840, val loss: 0.9652344584465027
Epoch 850, training loss: 6.127554893493652 = 0.03837524726986885 + 1.0 * 6.089179515838623
Epoch 850, val loss: 0.9729816317558289
Epoch 860, training loss: 6.117800235748291 = 0.03690288960933685 + 1.0 * 6.080897331237793
Epoch 860, val loss: 0.9805170297622681
Epoch 870, training loss: 6.11281681060791 = 0.03551037237048149 + 1.0 * 6.077306270599365
Epoch 870, val loss: 0.9879478216171265
Epoch 880, training loss: 6.111014366149902 = 0.03419099375605583 + 1.0 * 6.0768232345581055
Epoch 880, val loss: 0.9953882098197937
Epoch 890, training loss: 6.113712787628174 = 0.032943081110715866 + 1.0 * 6.0807695388793945
Epoch 890, val loss: 1.0028167963027954
Epoch 900, training loss: 6.109489440917969 = 0.03176121041178703 + 1.0 * 6.077728271484375
Epoch 900, val loss: 1.0100523233413696
Epoch 910, training loss: 6.106853485107422 = 0.030649835243821144 + 1.0 * 6.0762038230896
Epoch 910, val loss: 1.0171432495117188
Epoch 920, training loss: 6.101939678192139 = 0.029594661667943 + 1.0 * 6.072344779968262
Epoch 920, val loss: 1.0240473747253418
Epoch 930, training loss: 6.10025691986084 = 0.02858869358897209 + 1.0 * 6.0716681480407715
Epoch 930, val loss: 1.0309913158416748
Epoch 940, training loss: 6.0982279777526855 = 0.02762903831899166 + 1.0 * 6.07059907913208
Epoch 940, val loss: 1.037875771522522
Epoch 950, training loss: 6.104886054992676 = 0.0267183817923069 + 1.0 * 6.07816743850708
Epoch 950, val loss: 1.0447239875793457
Epoch 960, training loss: 6.097846031188965 = 0.025850772857666016 + 1.0 * 6.071995258331299
Epoch 960, val loss: 1.0513750314712524
Epoch 970, training loss: 6.09669303894043 = 0.025027362629771233 + 1.0 * 6.0716657638549805
Epoch 970, val loss: 1.0579173564910889
Epoch 980, training loss: 6.092170238494873 = 0.024243492633104324 + 1.0 * 6.06792688369751
Epoch 980, val loss: 1.06444251537323
Epoch 990, training loss: 6.0897932052612305 = 0.0234950240701437 + 1.0 * 6.066298007965088
Epoch 990, val loss: 1.0708903074264526
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.82657, 0.13018, Accuracy:0.80000, 0.00302
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10560])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.325676918029785 = 1.9518262147903442 + 1.0 * 8.37385082244873
Epoch 0, val loss: 1.9541696310043335
Epoch 10, training loss: 10.31538200378418 = 1.9419009685516357 + 1.0 * 8.373480796813965
Epoch 10, val loss: 1.944122076034546
Epoch 20, training loss: 10.301006317138672 = 1.9299300909042358 + 1.0 * 8.371076583862305
Epoch 20, val loss: 1.9317508935928345
Epoch 30, training loss: 10.266054153442383 = 1.9135563373565674 + 1.0 * 8.352498054504395
Epoch 30, val loss: 1.9147580862045288
Epoch 40, training loss: 10.086368560791016 = 1.892410397529602 + 1.0 * 8.193958282470703
Epoch 40, val loss: 1.893311619758606
Epoch 50, training loss: 9.121858596801758 = 1.8697556257247925 + 1.0 * 7.252103328704834
Epoch 50, val loss: 1.8709838390350342
Epoch 60, training loss: 8.79019546508789 = 1.852281928062439 + 1.0 * 6.937913417816162
Epoch 60, val loss: 1.8551242351531982
Epoch 70, training loss: 8.569852828979492 = 1.8391540050506592 + 1.0 * 6.730698585510254
Epoch 70, val loss: 1.8423572778701782
Epoch 80, training loss: 8.406702995300293 = 1.8243530988693237 + 1.0 * 6.58234977722168
Epoch 80, val loss: 1.8283733129501343
Epoch 90, training loss: 8.305644035339355 = 1.8115166425704956 + 1.0 * 6.49412727355957
Epoch 90, val loss: 1.8161697387695312
Epoch 100, training loss: 8.227795600891113 = 1.7987221479415894 + 1.0 * 6.429073810577393
Epoch 100, val loss: 1.8041640520095825
Epoch 110, training loss: 8.162174224853516 = 1.7868931293487549 + 1.0 * 6.37528133392334
Epoch 110, val loss: 1.7929154634475708
Epoch 120, training loss: 8.112627029418945 = 1.7748496532440186 + 1.0 * 6.337777614593506
Epoch 120, val loss: 1.7815802097320557
Epoch 130, training loss: 8.07335090637207 = 1.762163758277893 + 1.0 * 6.311186790466309
Epoch 130, val loss: 1.7701252698898315
Epoch 140, training loss: 8.037947654724121 = 1.7480567693710327 + 1.0 * 6.289890766143799
Epoch 140, val loss: 1.757927656173706
Epoch 150, training loss: 8.002904891967773 = 1.7319300174713135 + 1.0 * 6.270975112915039
Epoch 150, val loss: 1.7445100545883179
Epoch 160, training loss: 7.966460704803467 = 1.7131181955337524 + 1.0 * 6.253342628479004
Epoch 160, val loss: 1.7292654514312744
Epoch 170, training loss: 7.927736759185791 = 1.6906976699829102 + 1.0 * 6.237039089202881
Epoch 170, val loss: 1.7112786769866943
Epoch 180, training loss: 7.887174606323242 = 1.6639020442962646 + 1.0 * 6.223272800445557
Epoch 180, val loss: 1.6897881031036377
Epoch 190, training loss: 7.842556953430176 = 1.6321443319320679 + 1.0 * 6.210412502288818
Epoch 190, val loss: 1.6642422676086426
Epoch 200, training loss: 7.792856216430664 = 1.594369649887085 + 1.0 * 6.198486804962158
Epoch 200, val loss: 1.6337896585464478
Epoch 210, training loss: 7.74204683303833 = 1.5501059293746948 + 1.0 * 6.191940784454346
Epoch 210, val loss: 1.5981905460357666
Epoch 220, training loss: 7.681212425231934 = 1.5005017518997192 + 1.0 * 6.180710792541504
Epoch 220, val loss: 1.558180570602417
Epoch 230, training loss: 7.619491100311279 = 1.4454594850540161 + 1.0 * 6.174031734466553
Epoch 230, val loss: 1.5139811038970947
Epoch 240, training loss: 7.553980827331543 = 1.386040210723877 + 1.0 * 6.167940616607666
Epoch 240, val loss: 1.466599464416504
Epoch 250, training loss: 7.488004684448242 = 1.323961615562439 + 1.0 * 6.164042949676514
Epoch 250, val loss: 1.4176896810531616
Epoch 260, training loss: 7.418787479400635 = 1.260661244392395 + 1.0 * 6.158126354217529
Epoch 260, val loss: 1.3685797452926636
Epoch 270, training loss: 7.3523430824279785 = 1.1970739364624023 + 1.0 * 6.155269145965576
Epoch 270, val loss: 1.3197050094604492
Epoch 280, training loss: 7.284694671630859 = 1.134723424911499 + 1.0 * 6.1499714851379395
Epoch 280, val loss: 1.2720541954040527
Epoch 290, training loss: 7.219234466552734 = 1.0740526914596558 + 1.0 * 6.145181655883789
Epoch 290, val loss: 1.2258646488189697
Epoch 300, training loss: 7.154845237731934 = 1.0145310163497925 + 1.0 * 6.140314102172852
Epoch 300, val loss: 1.1806585788726807
Epoch 310, training loss: 7.093733787536621 = 0.9566761255264282 + 1.0 * 6.137057781219482
Epoch 310, val loss: 1.1367367506027222
Epoch 320, training loss: 7.0359697341918945 = 0.9012963771820068 + 1.0 * 6.134673595428467
Epoch 320, val loss: 1.0946656465530396
Epoch 330, training loss: 6.980564117431641 = 0.8478712439537048 + 1.0 * 6.132692813873291
Epoch 330, val loss: 1.0538908243179321
Epoch 340, training loss: 6.926947593688965 = 0.7965165376663208 + 1.0 * 6.130431175231934
Epoch 340, val loss: 1.0148314237594604
Epoch 350, training loss: 6.874087333679199 = 0.7485023736953735 + 1.0 * 6.125585079193115
Epoch 350, val loss: 0.9782472848892212
Epoch 360, training loss: 6.823697566986084 = 0.7036376595497131 + 1.0 * 6.120059967041016
Epoch 360, val loss: 0.9440955519676208
Epoch 370, training loss: 6.7782392501831055 = 0.6614618301391602 + 1.0 * 6.116777420043945
Epoch 370, val loss: 0.9121562242507935
Epoch 380, training loss: 6.744200706481934 = 0.6220016479492188 + 1.0 * 6.122199058532715
Epoch 380, val loss: 0.882634162902832
Epoch 390, training loss: 6.699449062347412 = 0.5859958529472351 + 1.0 * 6.113453388214111
Epoch 390, val loss: 0.8561948537826538
Epoch 400, training loss: 6.66281795501709 = 0.552832841873169 + 1.0 * 6.1099853515625
Epoch 400, val loss: 0.8326746821403503
Epoch 410, training loss: 6.634078502655029 = 0.5219168663024902 + 1.0 * 6.112161636352539
Epoch 410, val loss: 0.8116326332092285
Epoch 420, training loss: 6.600240707397461 = 0.4932135343551636 + 1.0 * 6.107027053833008
Epoch 420, val loss: 0.7929407358169556
Epoch 430, training loss: 6.568814754486084 = 0.4662242531776428 + 1.0 * 6.102590560913086
Epoch 430, val loss: 0.776439368724823
Epoch 440, training loss: 6.539382457733154 = 0.4404659569263458 + 1.0 * 6.098916530609131
Epoch 440, val loss: 0.7616941332817078
Epoch 450, training loss: 6.514175891876221 = 0.4157011806964874 + 1.0 * 6.098474502563477
Epoch 450, val loss: 0.7483974695205688
Epoch 460, training loss: 6.498185157775879 = 0.39187008142471313 + 1.0 * 6.1063151359558105
Epoch 460, val loss: 0.7365384101867676
Epoch 470, training loss: 6.463312149047852 = 0.3690369129180908 + 1.0 * 6.09427547454834
Epoch 470, val loss: 0.7260878682136536
Epoch 480, training loss: 6.438442707061768 = 0.3469778895378113 + 1.0 * 6.091464996337891
Epoch 480, val loss: 0.716946542263031
Epoch 490, training loss: 6.4135847091674805 = 0.3255694508552551 + 1.0 * 6.088015079498291
Epoch 490, val loss: 0.7089293599128723
Epoch 500, training loss: 6.410181522369385 = 0.30491378903388977 + 1.0 * 6.105267524719238
Epoch 500, val loss: 0.7020748853683472
Epoch 510, training loss: 6.376036167144775 = 0.2853875458240509 + 1.0 * 6.090648651123047
Epoch 510, val loss: 0.6965475678443909
Epoch 520, training loss: 6.351410865783691 = 0.26692283153533936 + 1.0 * 6.0844879150390625
Epoch 520, val loss: 0.6922896504402161
Epoch 530, training loss: 6.330745220184326 = 0.24941356480121613 + 1.0 * 6.081331729888916
Epoch 530, val loss: 0.6891335248947144
Epoch 540, training loss: 6.326644420623779 = 0.23297031223773956 + 1.0 * 6.093674182891846
Epoch 540, val loss: 0.6870608925819397
Epoch 550, training loss: 6.299549102783203 = 0.21777305006980896 + 1.0 * 6.081776142120361
Epoch 550, val loss: 0.6860931515693665
Epoch 560, training loss: 6.284047603607178 = 0.20369087159633636 + 1.0 * 6.080356597900391
Epoch 560, val loss: 0.6861083507537842
Epoch 570, training loss: 6.267658233642578 = 0.190691739320755 + 1.0 * 6.076966285705566
Epoch 570, val loss: 0.6869069337844849
Epoch 580, training loss: 6.257786273956299 = 0.17875197529792786 + 1.0 * 6.079034328460693
Epoch 580, val loss: 0.6885443329811096
Epoch 590, training loss: 6.242420196533203 = 0.16777294874191284 + 1.0 * 6.074647426605225
Epoch 590, val loss: 0.6909233331680298
Epoch 600, training loss: 6.229094505310059 = 0.1576538383960724 + 1.0 * 6.071440696716309
Epoch 600, val loss: 0.6939281821250916
Epoch 610, training loss: 6.222375392913818 = 0.1482876092195511 + 1.0 * 6.074087619781494
Epoch 610, val loss: 0.6974495649337769
Epoch 620, training loss: 6.215313911437988 = 0.13969899713993073 + 1.0 * 6.075614929199219
Epoch 620, val loss: 0.7015054225921631
Epoch 630, training loss: 6.200622081756592 = 0.13182620704174042 + 1.0 * 6.068795680999756
Epoch 630, val loss: 0.7059497833251953
Epoch 640, training loss: 6.192467212677002 = 0.12453670054674149 + 1.0 * 6.067930698394775
Epoch 640, val loss: 0.7107383608818054
Epoch 650, training loss: 6.182745456695557 = 0.11780314892530441 + 1.0 * 6.064942359924316
Epoch 650, val loss: 0.7157943248748779
Epoch 660, training loss: 6.1769118309021 = 0.11158391833305359 + 1.0 * 6.065328121185303
Epoch 660, val loss: 0.7211756706237793
Epoch 670, training loss: 6.177103042602539 = 0.10579631477594376 + 1.0 * 6.0713067054748535
Epoch 670, val loss: 0.7267658710479736
Epoch 680, training loss: 6.164287567138672 = 0.10043396800756454 + 1.0 * 6.063853740692139
Epoch 680, val loss: 0.7325419187545776
Epoch 690, training loss: 6.159939765930176 = 0.09542331099510193 + 1.0 * 6.064516544342041
Epoch 690, val loss: 0.7385356426239014
Epoch 700, training loss: 6.152068614959717 = 0.09075933694839478 + 1.0 * 6.061309337615967
Epoch 700, val loss: 0.7445803880691528
Epoch 710, training loss: 6.147000312805176 = 0.08641029894351959 + 1.0 * 6.060589790344238
Epoch 710, val loss: 0.7508334517478943
Epoch 720, training loss: 6.141796112060547 = 0.08234238624572754 + 1.0 * 6.05945348739624
Epoch 720, val loss: 0.7571520209312439
Epoch 730, training loss: 6.134550094604492 = 0.07853306084871292 + 1.0 * 6.05601692199707
Epoch 730, val loss: 0.7635797262191772
Epoch 740, training loss: 6.130152225494385 = 0.07494845241308212 + 1.0 * 6.055203914642334
Epoch 740, val loss: 0.7701046466827393
Epoch 750, training loss: 6.126457214355469 = 0.0715617910027504 + 1.0 * 6.054895401000977
Epoch 750, val loss: 0.7767379879951477
Epoch 760, training loss: 6.125119686126709 = 0.06838013231754303 + 1.0 * 6.056739330291748
Epoch 760, val loss: 0.7833120226860046
Epoch 770, training loss: 6.121817588806152 = 0.0654129832983017 + 1.0 * 6.0564045906066895
Epoch 770, val loss: 0.7899653911590576
Epoch 780, training loss: 6.113577365875244 = 0.06262048333883286 + 1.0 * 6.050956726074219
Epoch 780, val loss: 0.7967025637626648
Epoch 790, training loss: 6.111196517944336 = 0.05997109413146973 + 1.0 * 6.051225662231445
Epoch 790, val loss: 0.8034075498580933
Epoch 800, training loss: 6.123179912567139 = 0.05746721848845482 + 1.0 * 6.065712928771973
Epoch 800, val loss: 0.8100934624671936
Epoch 810, training loss: 6.111298084259033 = 0.055132582783699036 + 1.0 * 6.05616569519043
Epoch 810, val loss: 0.816717267036438
Epoch 820, training loss: 6.101789951324463 = 0.05294239521026611 + 1.0 * 6.048847675323486
Epoch 820, val loss: 0.8234142065048218
Epoch 830, training loss: 6.0980544090271 = 0.05086315795779228 + 1.0 * 6.047191143035889
Epoch 830, val loss: 0.8300459980964661
Epoch 840, training loss: 6.1091790199279785 = 0.048884257674217224 + 1.0 * 6.0602946281433105
Epoch 840, val loss: 0.8366467356681824
Epoch 850, training loss: 6.098204612731934 = 0.04702799767255783 + 1.0 * 6.05117654800415
Epoch 850, val loss: 0.8431249260902405
Epoch 860, training loss: 6.089685440063477 = 0.045272864401340485 + 1.0 * 6.044412612915039
Epoch 860, val loss: 0.8497143983840942
Epoch 870, training loss: 6.0876946449279785 = 0.04360254853963852 + 1.0 * 6.044092178344727
Epoch 870, val loss: 0.856223464012146
Epoch 880, training loss: 6.085109710693359 = 0.04201139509677887 + 1.0 * 6.043098449707031
Epoch 880, val loss: 0.862676739692688
Epoch 890, training loss: 6.083217620849609 = 0.040495339781045914 + 1.0 * 6.042722225189209
Epoch 890, val loss: 0.8691226243972778
Epoch 900, training loss: 6.093068599700928 = 0.03905714303255081 + 1.0 * 6.054011344909668
Epoch 900, val loss: 0.8753886818885803
Epoch 910, training loss: 6.0810723304748535 = 0.03770751506090164 + 1.0 * 6.043365001678467
Epoch 910, val loss: 0.8815277814865112
Epoch 920, training loss: 6.0785675048828125 = 0.036432355642318726 + 1.0 * 6.042135238647461
Epoch 920, val loss: 0.8876616954803467
Epoch 930, training loss: 6.076220512390137 = 0.03521757200360298 + 1.0 * 6.0410027503967285
Epoch 930, val loss: 0.8935479521751404
Epoch 940, training loss: 6.0763092041015625 = 0.03407135605812073 + 1.0 * 6.042237758636475
Epoch 940, val loss: 0.8993442058563232
Epoch 950, training loss: 6.071561336517334 = 0.03297962248325348 + 1.0 * 6.038581848144531
Epoch 950, val loss: 0.9052008986473083
Epoch 960, training loss: 6.069543838500977 = 0.03193189203739166 + 1.0 * 6.037611961364746
Epoch 960, val loss: 0.9109131693840027
Epoch 970, training loss: 6.072422981262207 = 0.030929233878850937 + 1.0 * 6.041493892669678
Epoch 970, val loss: 0.9165364503860474
Epoch 980, training loss: 6.072157382965088 = 0.02997450716793537 + 1.0 * 6.042182922363281
Epoch 980, val loss: 0.9220079779624939
Epoch 990, training loss: 6.066137790679932 = 0.029079316183924675 + 1.0 * 6.037058353424072
Epoch 990, val loss: 0.9274436235427856
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.324082374572754 = 1.9503419399261475 + 1.0 * 8.373740196228027
Epoch 0, val loss: 1.9442323446273804
Epoch 10, training loss: 10.313263893127441 = 1.9402412176132202 + 1.0 * 8.37302303314209
Epoch 10, val loss: 1.9348686933517456
Epoch 20, training loss: 10.295567512512207 = 1.9274741411209106 + 1.0 * 8.368093490600586
Epoch 20, val loss: 1.922500491142273
Epoch 30, training loss: 10.237201690673828 = 1.9095827341079712 + 1.0 * 8.327618598937988
Epoch 30, val loss: 1.9048916101455688
Epoch 40, training loss: 9.789569854736328 = 1.8885947465896606 + 1.0 * 7.900975227355957
Epoch 40, val loss: 1.8848775625228882
Epoch 50, training loss: 8.994392395019531 = 1.869027018547058 + 1.0 * 7.125365734100342
Epoch 50, val loss: 1.8670551776885986
Epoch 60, training loss: 8.637716293334961 = 1.8550512790679932 + 1.0 * 6.782664775848389
Epoch 60, val loss: 1.8536635637283325
Epoch 70, training loss: 8.458902359008789 = 1.8418948650360107 + 1.0 * 6.617007732391357
Epoch 70, val loss: 1.8412365913391113
Epoch 80, training loss: 8.341867446899414 = 1.829627275466919 + 1.0 * 6.512240409851074
Epoch 80, val loss: 1.8301692008972168
Epoch 90, training loss: 8.243818283081055 = 1.8177849054336548 + 1.0 * 6.426033020019531
Epoch 90, val loss: 1.8201147317886353
Epoch 100, training loss: 8.166719436645508 = 1.8069911003112793 + 1.0 * 6.3597283363342285
Epoch 100, val loss: 1.8108431100845337
Epoch 110, training loss: 8.106717109680176 = 1.797032117843628 + 1.0 * 6.309685230255127
Epoch 110, val loss: 1.802164912223816
Epoch 120, training loss: 8.062207221984863 = 1.787346601486206 + 1.0 * 6.274860858917236
Epoch 120, val loss: 1.7936075925827026
Epoch 130, training loss: 8.026734352111816 = 1.7770850658416748 + 1.0 * 6.2496490478515625
Epoch 130, val loss: 1.7845304012298584
Epoch 140, training loss: 7.9942426681518555 = 1.765695333480835 + 1.0 * 6.228547096252441
Epoch 140, val loss: 1.7745896577835083
Epoch 150, training loss: 7.964653491973877 = 1.7527031898498535 + 1.0 * 6.211950302124023
Epoch 150, val loss: 1.7635622024536133
Epoch 160, training loss: 7.933826446533203 = 1.7376220226287842 + 1.0 * 6.196204662322998
Epoch 160, val loss: 1.7511317729949951
Epoch 170, training loss: 7.902346611022949 = 1.7197874784469604 + 1.0 * 6.182559013366699
Epoch 170, val loss: 1.7366753816604614
Epoch 180, training loss: 7.869986534118652 = 1.6981167793273926 + 1.0 * 6.17186975479126
Epoch 180, val loss: 1.7192938327789307
Epoch 190, training loss: 7.834362030029297 = 1.6718547344207764 + 1.0 * 6.1625075340271
Epoch 190, val loss: 1.6982899904251099
Epoch 200, training loss: 7.79357385635376 = 1.63988196849823 + 1.0 * 6.15369176864624
Epoch 200, val loss: 1.672653079032898
Epoch 210, training loss: 7.747582912445068 = 1.6011239290237427 + 1.0 * 6.146459102630615
Epoch 210, val loss: 1.641359567642212
Epoch 220, training loss: 7.698669910430908 = 1.5548962354660034 + 1.0 * 6.143773555755615
Epoch 220, val loss: 1.6042383909225464
Epoch 230, training loss: 7.638251781463623 = 1.5023032426834106 + 1.0 * 6.135948657989502
Epoch 230, val loss: 1.561772346496582
Epoch 240, training loss: 7.574855804443359 = 1.4433512687683105 + 1.0 * 6.131504535675049
Epoch 240, val loss: 1.514137625694275
Epoch 250, training loss: 7.507352828979492 = 1.379348874092102 + 1.0 * 6.12800407409668
Epoch 250, val loss: 1.462536096572876
Epoch 260, training loss: 7.43781852722168 = 1.3125829696655273 + 1.0 * 6.125235557556152
Epoch 260, val loss: 1.409141182899475
Epoch 270, training loss: 7.371173858642578 = 1.2465059757232666 + 1.0 * 6.124668121337891
Epoch 270, val loss: 1.3566749095916748
Epoch 280, training loss: 7.300661563873291 = 1.1821600198745728 + 1.0 * 6.118501663208008
Epoch 280, val loss: 1.3059003353118896
Epoch 290, training loss: 7.235916614532471 = 1.1198506355285645 + 1.0 * 6.116065979003906
Epoch 290, val loss: 1.2573738098144531
Epoch 300, training loss: 7.1800432205200195 = 1.0608375072479248 + 1.0 * 6.119205474853516
Epoch 300, val loss: 1.2119711637496948
Epoch 310, training loss: 7.118984699249268 = 1.0065922737121582 + 1.0 * 6.112392425537109
Epoch 310, val loss: 1.170979619026184
Epoch 320, training loss: 7.062219619750977 = 0.956203818321228 + 1.0 * 6.106015682220459
Epoch 320, val loss: 1.1333342790603638
Epoch 330, training loss: 7.011456489562988 = 0.9086255431175232 + 1.0 * 6.10283088684082
Epoch 330, val loss: 1.0981348752975464
Epoch 340, training loss: 6.964941501617432 = 0.8636741042137146 + 1.0 * 6.101267337799072
Epoch 340, val loss: 1.0653057098388672
Epoch 350, training loss: 6.920042037963867 = 0.8216058015823364 + 1.0 * 6.09843635559082
Epoch 350, val loss: 1.0347992181777954
Epoch 360, training loss: 6.876585006713867 = 0.7814137935638428 + 1.0 * 6.095170974731445
Epoch 360, val loss: 1.0060114860534668
Epoch 370, training loss: 6.836949348449707 = 0.7431164383888245 + 1.0 * 6.093832969665527
Epoch 370, val loss: 0.9789263606071472
Epoch 380, training loss: 6.798162937164307 = 0.7071081399917603 + 1.0 * 6.091054916381836
Epoch 380, val loss: 0.9540036916732788
Epoch 390, training loss: 6.761966705322266 = 0.6726499795913696 + 1.0 * 6.0893168449401855
Epoch 390, val loss: 0.9304003119468689
Epoch 400, training loss: 6.728446006774902 = 0.6398417353630066 + 1.0 * 6.08860445022583
Epoch 400, val loss: 0.9083917737007141
Epoch 410, training loss: 6.693368434906006 = 0.6088164448738098 + 1.0 * 6.084551811218262
Epoch 410, val loss: 0.8879765272140503
Epoch 420, training loss: 6.662343978881836 = 0.5793275833129883 + 1.0 * 6.083016395568848
Epoch 420, val loss: 0.8690272569656372
Epoch 430, training loss: 6.633057594299316 = 0.5513537526130676 + 1.0 * 6.0817036628723145
Epoch 430, val loss: 0.8514597415924072
Epoch 440, training loss: 6.608185291290283 = 0.5249066948890686 + 1.0 * 6.083278656005859
Epoch 440, val loss: 0.8353599905967712
Epoch 450, training loss: 6.576946258544922 = 0.49990200996398926 + 1.0 * 6.0770440101623535
Epoch 450, val loss: 0.820716917514801
Epoch 460, training loss: 6.551215648651123 = 0.4761204421520233 + 1.0 * 6.075095176696777
Epoch 460, val loss: 0.8075014352798462
Epoch 470, training loss: 6.5290327072143555 = 0.4535243809223175 + 1.0 * 6.075508117675781
Epoch 470, val loss: 0.7955331802368164
Epoch 480, training loss: 6.504866123199463 = 0.4320475459098816 + 1.0 * 6.072818756103516
Epoch 480, val loss: 0.7850435972213745
Epoch 490, training loss: 6.481261730194092 = 0.41126763820648193 + 1.0 * 6.06999397277832
Epoch 490, val loss: 0.7754710912704468
Epoch 500, training loss: 6.4619011878967285 = 0.3909870684146881 + 1.0 * 6.070914268493652
Epoch 500, val loss: 0.7667668461799622
Epoch 510, training loss: 6.4440412521362305 = 0.37136611342430115 + 1.0 * 6.0726752281188965
Epoch 510, val loss: 0.7589909434318542
Epoch 520, training loss: 6.4174699783325195 = 0.3522603511810303 + 1.0 * 6.06520938873291
Epoch 520, val loss: 0.7521601915359497
Epoch 530, training loss: 6.40287446975708 = 0.33360761404037476 + 1.0 * 6.0692667961120605
Epoch 530, val loss: 0.7460309267044067
Epoch 540, training loss: 6.382857799530029 = 0.3155602216720581 + 1.0 * 6.067297458648682
Epoch 540, val loss: 0.7406437993049622
Epoch 550, training loss: 6.360509872436523 = 0.29808780550956726 + 1.0 * 6.062422275543213
Epoch 550, val loss: 0.7361232042312622
Epoch 560, training loss: 6.3404154777526855 = 0.2812019884586334 + 1.0 * 6.059213638305664
Epoch 560, val loss: 0.7323470115661621
Epoch 570, training loss: 6.330777645111084 = 0.2649247944355011 + 1.0 * 6.065852642059326
Epoch 570, val loss: 0.7292575240135193
Epoch 580, training loss: 6.306909561157227 = 0.24948352575302124 + 1.0 * 6.0574259757995605
Epoch 580, val loss: 0.7270383238792419
Epoch 590, training loss: 6.290249347686768 = 0.23482376337051392 + 1.0 * 6.055425643920898
Epoch 590, val loss: 0.7257183194160461
Epoch 600, training loss: 6.276155471801758 = 0.2209164798259735 + 1.0 * 6.055239200592041
Epoch 600, val loss: 0.7250857949256897
Epoch 610, training loss: 6.265180587768555 = 0.2078550010919571 + 1.0 * 6.05732536315918
Epoch 610, val loss: 0.7251896262168884
Epoch 620, training loss: 6.248579025268555 = 0.19571685791015625 + 1.0 * 6.052862167358398
Epoch 620, val loss: 0.7261220216751099
Epoch 630, training loss: 6.2355217933654785 = 0.18435075879096985 + 1.0 * 6.051170825958252
Epoch 630, val loss: 0.7277556657791138
Epoch 640, training loss: 6.233969211578369 = 0.17369665205478668 + 1.0 * 6.060272693634033
Epoch 640, val loss: 0.7299337983131409
Epoch 650, training loss: 6.215826988220215 = 0.1638428121805191 + 1.0 * 6.0519843101501465
Epoch 650, val loss: 0.7325936555862427
Epoch 660, training loss: 6.203188896179199 = 0.15469573438167572 + 1.0 * 6.048493385314941
Epoch 660, val loss: 0.7359804511070251
Epoch 670, training loss: 6.192739963531494 = 0.14612986147403717 + 1.0 * 6.046609878540039
Epoch 670, val loss: 0.7396999001502991
Epoch 680, training loss: 6.1832966804504395 = 0.13809947669506073 + 1.0 * 6.045197010040283
Epoch 680, val loss: 0.7438924908638
Epoch 690, training loss: 6.184421539306641 = 0.13061685860157013 + 1.0 * 6.053804874420166
Epoch 690, val loss: 0.7484146952629089
Epoch 700, training loss: 6.171326637268066 = 0.1237131878733635 + 1.0 * 6.047613620758057
Epoch 700, val loss: 0.7533159255981445
Epoch 710, training loss: 6.1603546142578125 = 0.11726724356412888 + 1.0 * 6.043087482452393
Epoch 710, val loss: 0.758603036403656
Epoch 720, training loss: 6.152694225311279 = 0.11121475696563721 + 1.0 * 6.041479587554932
Epoch 720, val loss: 0.7640350461006165
Epoch 730, training loss: 6.14975118637085 = 0.10553263127803802 + 1.0 * 6.04421854019165
Epoch 730, val loss: 0.7698196768760681
Epoch 740, training loss: 6.140834331512451 = 0.10022149235010147 + 1.0 * 6.040612697601318
Epoch 740, val loss: 0.7758005857467651
Epoch 750, training loss: 6.13884973526001 = 0.09524212777614594 + 1.0 * 6.043607711791992
Epoch 750, val loss: 0.7820684313774109
Epoch 760, training loss: 6.134304523468018 = 0.09061799198389053 + 1.0 * 6.043686389923096
Epoch 760, val loss: 0.7882373929023743
Epoch 770, training loss: 6.125001907348633 = 0.08629526197910309 + 1.0 * 6.0387067794799805
Epoch 770, val loss: 0.794775664806366
Epoch 780, training loss: 6.117755889892578 = 0.08222818374633789 + 1.0 * 6.03552770614624
Epoch 780, val loss: 0.8013603091239929
Epoch 790, training loss: 6.113043785095215 = 0.07838886976242065 + 1.0 * 6.0346550941467285
Epoch 790, val loss: 0.8079689741134644
Epoch 800, training loss: 6.118198871612549 = 0.07476645708084106 + 1.0 * 6.043432235717773
Epoch 800, val loss: 0.8146235346794128
Epoch 810, training loss: 6.112314701080322 = 0.07139509916305542 + 1.0 * 6.040919780731201
Epoch 810, val loss: 0.8211489319801331
Epoch 820, training loss: 6.10282039642334 = 0.06824353337287903 + 1.0 * 6.034576892852783
Epoch 820, val loss: 0.8281008005142212
Epoch 830, training loss: 6.09760856628418 = 0.06526759266853333 + 1.0 * 6.032341003417969
Epoch 830, val loss: 0.8348373174667358
Epoch 840, training loss: 6.093247413635254 = 0.06244632229208946 + 1.0 * 6.030801296234131
Epoch 840, val loss: 0.8414902091026306
Epoch 850, training loss: 6.091184139251709 = 0.0597740076482296 + 1.0 * 6.031410217285156
Epoch 850, val loss: 0.8482872247695923
Epoch 860, training loss: 6.0909647941589355 = 0.05725732818245888 + 1.0 * 6.033707618713379
Epoch 860, val loss: 0.8550171852111816
Epoch 870, training loss: 6.08507776260376 = 0.05488962680101395 + 1.0 * 6.030188083648682
Epoch 870, val loss: 0.8618541359901428
Epoch 880, training loss: 6.082966327667236 = 0.05266151204705238 + 1.0 * 6.030304908752441
Epoch 880, val loss: 0.868654191493988
Epoch 890, training loss: 6.079715251922607 = 0.050552815198898315 + 1.0 * 6.029162406921387
Epoch 890, val loss: 0.8753684759140015
Epoch 900, training loss: 6.083312511444092 = 0.04855547845363617 + 1.0 * 6.034757137298584
Epoch 900, val loss: 0.88190096616745
Epoch 910, training loss: 6.073340892791748 = 0.046674445271492004 + 1.0 * 6.026666641235352
Epoch 910, val loss: 0.8886025547981262
Epoch 920, training loss: 6.069986343383789 = 0.04489060118794441 + 1.0 * 6.0250959396362305
Epoch 920, val loss: 0.8952807188034058
Epoch 930, training loss: 6.069204330444336 = 0.04319017753005028 + 1.0 * 6.02601432800293
Epoch 930, val loss: 0.9017160534858704
Epoch 940, training loss: 6.0700154304504395 = 0.041581038385629654 + 1.0 * 6.0284342765808105
Epoch 940, val loss: 0.9080385565757751
Epoch 950, training loss: 6.0651044845581055 = 0.040065962821245193 + 1.0 * 6.025038719177246
Epoch 950, val loss: 0.9145819544792175
Epoch 960, training loss: 6.064842700958252 = 0.03862166777253151 + 1.0 * 6.026220798492432
Epoch 960, val loss: 0.9209752082824707
Epoch 970, training loss: 6.060189247131348 = 0.03725690767168999 + 1.0 * 6.022932529449463
Epoch 970, val loss: 0.9268931150436401
Epoch 980, training loss: 6.060194969177246 = 0.03596551716327667 + 1.0 * 6.024229526519775
Epoch 980, val loss: 0.9332380890846252
Epoch 990, training loss: 6.055736541748047 = 0.03473401069641113 + 1.0 * 6.021002769470215
Epoch 990, val loss: 0.9394035935401917
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.329195976257324 = 1.955341100692749 + 1.0 * 8.373854637145996
Epoch 0, val loss: 1.9643559455871582
Epoch 10, training loss: 10.3182954788208 = 1.9448168277740479 + 1.0 * 8.373478889465332
Epoch 10, val loss: 1.9539310932159424
Epoch 20, training loss: 10.30288314819336 = 1.931809425354004 + 1.0 * 8.371073722839355
Epoch 20, val loss: 1.9406579732894897
Epoch 30, training loss: 10.265009880065918 = 1.9138485193252563 + 1.0 * 8.351161003112793
Epoch 30, val loss: 1.92221999168396
Epoch 40, training loss: 10.060690879821777 = 1.8909646272659302 + 1.0 * 8.169726371765137
Epoch 40, val loss: 1.8995366096496582
Epoch 50, training loss: 9.376236915588379 = 1.8667070865631104 + 1.0 * 7.509530067443848
Epoch 50, val loss: 1.876183271408081
Epoch 60, training loss: 9.03581428527832 = 1.8495854139328003 + 1.0 * 7.1862287521362305
Epoch 60, val loss: 1.8606317043304443
Epoch 70, training loss: 8.748034477233887 = 1.8362841606140137 + 1.0 * 6.911750316619873
Epoch 70, val loss: 1.8480910062789917
Epoch 80, training loss: 8.55902099609375 = 1.8229154348373413 + 1.0 * 6.736105442047119
Epoch 80, val loss: 1.8355746269226074
Epoch 90, training loss: 8.420101165771484 = 1.8104991912841797 + 1.0 * 6.6096014976501465
Epoch 90, val loss: 1.8241251707077026
Epoch 100, training loss: 8.31641960144043 = 1.7991342544555664 + 1.0 * 6.517285346984863
Epoch 100, val loss: 1.8134045600891113
Epoch 110, training loss: 8.235733032226562 = 1.7887687683105469 + 1.0 * 6.446964263916016
Epoch 110, val loss: 1.803062915802002
Epoch 120, training loss: 8.174040794372559 = 1.7785264253616333 + 1.0 * 6.395514011383057
Epoch 120, val loss: 1.792759656906128
Epoch 130, training loss: 8.120874404907227 = 1.7676993608474731 + 1.0 * 6.353174686431885
Epoch 130, val loss: 1.7823407649993896
Epoch 140, training loss: 8.072868347167969 = 1.7557927370071411 + 1.0 * 6.317075729370117
Epoch 140, val loss: 1.7715805768966675
Epoch 150, training loss: 8.028517723083496 = 1.7423356771469116 + 1.0 * 6.286181926727295
Epoch 150, val loss: 1.7600748538970947
Epoch 160, training loss: 7.988333225250244 = 1.7267680168151855 + 1.0 * 6.261565208435059
Epoch 160, val loss: 1.7473156452178955
Epoch 170, training loss: 7.950826644897461 = 1.708868145942688 + 1.0 * 6.2419586181640625
Epoch 170, val loss: 1.7329657077789307
Epoch 180, training loss: 7.913694858551025 = 1.6877647638320923 + 1.0 * 6.225930213928223
Epoch 180, val loss: 1.716307520866394
Epoch 190, training loss: 7.875041484832764 = 1.6625970602035522 + 1.0 * 6.212444305419922
Epoch 190, val loss: 1.6964243650436401
Epoch 200, training loss: 7.834071636199951 = 1.6329970359802246 + 1.0 * 6.201074600219727
Epoch 200, val loss: 1.6731665134429932
Epoch 210, training loss: 7.7888078689575195 = 1.5985411405563354 + 1.0 * 6.1902666091918945
Epoch 210, val loss: 1.6461057662963867
Epoch 220, training loss: 7.740460395812988 = 1.558957576751709 + 1.0 * 6.181502819061279
Epoch 220, val loss: 1.6151002645492554
Epoch 230, training loss: 7.691474914550781 = 1.5145323276519775 + 1.0 * 6.176942825317383
Epoch 230, val loss: 1.5803630352020264
Epoch 240, training loss: 7.634795188903809 = 1.466766595840454 + 1.0 * 6.168028831481934
Epoch 240, val loss: 1.5430691242218018
Epoch 250, training loss: 7.57889461517334 = 1.4168970584869385 + 1.0 * 6.161997318267822
Epoch 250, val loss: 1.5044867992401123
Epoch 260, training loss: 7.522624969482422 = 1.3659260272979736 + 1.0 * 6.156698703765869
Epoch 260, val loss: 1.4651063680648804
Epoch 270, training loss: 7.466890335083008 = 1.3141391277313232 + 1.0 * 6.152751445770264
Epoch 270, val loss: 1.425175428390503
Epoch 280, training loss: 7.410737991333008 = 1.2624592781066895 + 1.0 * 6.148278713226318
Epoch 280, val loss: 1.3851217031478882
Epoch 290, training loss: 7.355342388153076 = 1.2110131978988647 + 1.0 * 6.144329071044922
Epoch 290, val loss: 1.3452261686325073
Epoch 300, training loss: 7.299878120422363 = 1.1605000495910645 + 1.0 * 6.139378070831299
Epoch 300, val loss: 1.305824875831604
Epoch 310, training loss: 7.245746612548828 = 1.1111506223678589 + 1.0 * 6.13459587097168
Epoch 310, val loss: 1.2671984434127808
Epoch 320, training loss: 7.19550895690918 = 1.0630148649215698 + 1.0 * 6.13249397277832
Epoch 320, val loss: 1.2295637130737305
Epoch 330, training loss: 7.145068168640137 = 1.0166722536087036 + 1.0 * 6.128396034240723
Epoch 330, val loss: 1.193271517753601
Epoch 340, training loss: 7.098001480102539 = 0.9721059203147888 + 1.0 * 6.1258955001831055
Epoch 340, val loss: 1.1582468748092651
Epoch 350, training loss: 7.05154275894165 = 0.9292308688163757 + 1.0 * 6.122312068939209
Epoch 350, val loss: 1.1247014999389648
Epoch 360, training loss: 7.005727291107178 = 0.8879069089889526 + 1.0 * 6.1178202629089355
Epoch 360, val loss: 1.092651128768921
Epoch 370, training loss: 6.965366363525391 = 0.8479394316673279 + 1.0 * 6.117426872253418
Epoch 370, val loss: 1.0618603229522705
Epoch 380, training loss: 6.924193382263184 = 0.8097420930862427 + 1.0 * 6.1144514083862305
Epoch 380, val loss: 1.032497763633728
Epoch 390, training loss: 6.8842997550964355 = 0.7731860876083374 + 1.0 * 6.111113548278809
Epoch 390, val loss: 1.005133032798767
Epoch 400, training loss: 6.849292755126953 = 0.7381901741027832 + 1.0 * 6.11110258102417
Epoch 400, val loss: 0.97954261302948
Epoch 410, training loss: 6.813403129577637 = 0.7048399448394775 + 1.0 * 6.10856294631958
Epoch 410, val loss: 0.9559328556060791
Epoch 420, training loss: 6.777680397033691 = 0.672934353351593 + 1.0 * 6.104745864868164
Epoch 420, val loss: 0.9342311024665833
Epoch 430, training loss: 6.744968414306641 = 0.6423668265342712 + 1.0 * 6.102601528167725
Epoch 430, val loss: 0.9144702553749084
Epoch 440, training loss: 6.715331077575684 = 0.6127851009368896 + 1.0 * 6.102545738220215
Epoch 440, val loss: 0.8963584303855896
Epoch 450, training loss: 6.6840291023254395 = 0.5840973854064941 + 1.0 * 6.099931716918945
Epoch 450, val loss: 0.8796588778495789
Epoch 460, training loss: 6.6521100997924805 = 0.5561249852180481 + 1.0 * 6.095984935760498
Epoch 460, val loss: 0.8643608093261719
Epoch 470, training loss: 6.626462936401367 = 0.528648316860199 + 1.0 * 6.097814559936523
Epoch 470, val loss: 0.8500427007675171
Epoch 480, training loss: 6.598211288452148 = 0.5018584132194519 + 1.0 * 6.096353054046631
Epoch 480, val loss: 0.8368167281150818
Epoch 490, training loss: 6.569786071777344 = 0.4758149981498718 + 1.0 * 6.093971252441406
Epoch 490, val loss: 0.8245185613632202
Epoch 500, training loss: 6.540705680847168 = 0.45060399174690247 + 1.0 * 6.090101718902588
Epoch 500, val loss: 0.8132449388504028
Epoch 510, training loss: 6.514540672302246 = 0.42617958784103394 + 1.0 * 6.0883612632751465
Epoch 510, val loss: 0.8030288219451904
Epoch 520, training loss: 6.4974493980407715 = 0.40264689922332764 + 1.0 * 6.094802379608154
Epoch 520, val loss: 0.7937506437301636
Epoch 530, training loss: 6.467870235443115 = 0.38033780455589294 + 1.0 * 6.0875325202941895
Epoch 530, val loss: 0.7856013178825378
Epoch 540, training loss: 6.442784786224365 = 0.358995258808136 + 1.0 * 6.083789348602295
Epoch 540, val loss: 0.7784983515739441
Epoch 550, training loss: 6.423519611358643 = 0.33859649300575256 + 1.0 * 6.084923267364502
Epoch 550, val loss: 0.7724441289901733
Epoch 560, training loss: 6.406791687011719 = 0.3194331228733063 + 1.0 * 6.087358474731445
Epoch 560, val loss: 0.7673333883285522
Epoch 570, training loss: 6.38068151473999 = 0.30147773027420044 + 1.0 * 6.0792036056518555
Epoch 570, val loss: 0.7636963725090027
Epoch 580, training loss: 6.362287521362305 = 0.2844793200492859 + 1.0 * 6.077808380126953
Epoch 580, val loss: 0.7610597014427185
Epoch 590, training loss: 6.345632076263428 = 0.2683676779270172 + 1.0 * 6.077264308929443
Epoch 590, val loss: 0.759518027305603
Epoch 600, training loss: 6.328307628631592 = 0.2531411945819855 + 1.0 * 6.07516622543335
Epoch 600, val loss: 0.7588348984718323
Epoch 610, training loss: 6.3153767585754395 = 0.23880510032176971 + 1.0 * 6.076571464538574
Epoch 610, val loss: 0.7590742111206055
Epoch 620, training loss: 6.299327373504639 = 0.22525747120380402 + 1.0 * 6.074069976806641
Epoch 620, val loss: 0.7600515484809875
Epoch 630, training loss: 6.285679817199707 = 0.21243168413639069 + 1.0 * 6.073247909545898
Epoch 630, val loss: 0.7618386149406433
Epoch 640, training loss: 6.273789405822754 = 0.2003134936094284 + 1.0 * 6.0734758377075195
Epoch 640, val loss: 0.7642617225646973
Epoch 650, training loss: 6.258745193481445 = 0.18887682259082794 + 1.0 * 6.069868564605713
Epoch 650, val loss: 0.7673037648200989
Epoch 660, training loss: 6.251956939697266 = 0.17807528376579285 + 1.0 * 6.07388162612915
Epoch 660, val loss: 0.7709059715270996
Epoch 670, training loss: 6.235430717468262 = 0.16790303587913513 + 1.0 * 6.067527770996094
Epoch 670, val loss: 0.7750557065010071
Epoch 680, training loss: 6.2239298820495605 = 0.15831047296524048 + 1.0 * 6.065619468688965
Epoch 680, val loss: 0.7797577381134033
Epoch 690, training loss: 6.225463390350342 = 0.14926807582378387 + 1.0 * 6.076195240020752
Epoch 690, val loss: 0.7847841382026672
Epoch 700, training loss: 6.206122875213623 = 0.140853151679039 + 1.0 * 6.065269947052002
Epoch 700, val loss: 0.7901738882064819
Epoch 710, training loss: 6.194193363189697 = 0.13294970989227295 + 1.0 * 6.061243534088135
Epoch 710, val loss: 0.7959113717079163
Epoch 720, training loss: 6.187027931213379 = 0.12550140917301178 + 1.0 * 6.061526298522949
Epoch 720, val loss: 0.801915168762207
Epoch 730, training loss: 6.180305004119873 = 0.11851303279399872 + 1.0 * 6.061791896820068
Epoch 730, val loss: 0.8080925345420837
Epoch 740, training loss: 6.172618389129639 = 0.11201658099889755 + 1.0 * 6.060601711273193
Epoch 740, val loss: 0.8144750595092773
Epoch 750, training loss: 6.164828777313232 = 0.10591372102499008 + 1.0 * 6.058915138244629
Epoch 750, val loss: 0.8210102319717407
Epoch 760, training loss: 6.1602983474731445 = 0.10019482672214508 + 1.0 * 6.060103416442871
Epoch 760, val loss: 0.82768315076828
Epoch 770, training loss: 6.152068614959717 = 0.09485391527414322 + 1.0 * 6.057214736938477
Epoch 770, val loss: 0.8344045281410217
Epoch 780, training loss: 6.146769046783447 = 0.08987836539745331 + 1.0 * 6.056890487670898
Epoch 780, val loss: 0.8411096930503845
Epoch 790, training loss: 6.141271591186523 = 0.08524865657091141 + 1.0 * 6.056023120880127
Epoch 790, val loss: 0.8478944301605225
Epoch 800, training loss: 6.133856296539307 = 0.080906443297863 + 1.0 * 6.052949905395508
Epoch 800, val loss: 0.8546905517578125
Epoch 810, training loss: 6.13853645324707 = 0.07683250308036804 + 1.0 * 6.061704158782959
Epoch 810, val loss: 0.8614675402641296
Epoch 820, training loss: 6.126930236816406 = 0.07303910702466965 + 1.0 * 6.053891181945801
Epoch 820, val loss: 0.8681579828262329
Epoch 830, training loss: 6.1246466636657715 = 0.0694970041513443 + 1.0 * 6.055149555206299
Epoch 830, val loss: 0.8748385310173035
Epoch 840, training loss: 6.115373134613037 = 0.06618275493383408 + 1.0 * 6.049190521240234
Epoch 840, val loss: 0.8814801573753357
Epoch 850, training loss: 6.111764430999756 = 0.06306898593902588 + 1.0 * 6.0486955642700195
Epoch 850, val loss: 0.8880695700645447
Epoch 860, training loss: 6.122490882873535 = 0.060149580240249634 + 1.0 * 6.062341213226318
Epoch 860, val loss: 0.8945648074150085
Epoch 870, training loss: 6.1048808097839355 = 0.057432692497968674 + 1.0 * 6.04744815826416
Epoch 870, val loss: 0.9008947610855103
Epoch 880, training loss: 6.10136079788208 = 0.054884351789951324 + 1.0 * 6.046476364135742
Epoch 880, val loss: 0.9071673154830933
Epoch 890, training loss: 6.097780704498291 = 0.05247646942734718 + 1.0 * 6.045304298400879
Epoch 890, val loss: 0.9134361743927002
Epoch 900, training loss: 6.102436542510986 = 0.050209660083055496 + 1.0 * 6.052227020263672
Epoch 900, val loss: 0.9196908473968506
Epoch 910, training loss: 6.104055404663086 = 0.04808344691991806 + 1.0 * 6.055972099304199
Epoch 910, val loss: 0.9257811903953552
Epoch 920, training loss: 6.092494010925293 = 0.0461028590798378 + 1.0 * 6.046391010284424
Epoch 920, val loss: 0.9317346215248108
Epoch 930, training loss: 6.086435317993164 = 0.044232986867427826 + 1.0 * 6.042202472686768
Epoch 930, val loss: 0.9376522898674011
Epoch 940, training loss: 6.084012985229492 = 0.04245848208665848 + 1.0 * 6.0415544509887695
Epoch 940, val loss: 0.9435622692108154
Epoch 950, training loss: 6.088351249694824 = 0.040780313313007355 + 1.0 * 6.047570705413818
Epoch 950, val loss: 0.9494738578796387
Epoch 960, training loss: 6.084171772003174 = 0.03920106217265129 + 1.0 * 6.044970512390137
Epoch 960, val loss: 0.9551486372947693
Epoch 970, training loss: 6.077839374542236 = 0.037720292806625366 + 1.0 * 6.040119171142578
Epoch 970, val loss: 0.9607919454574585
Epoch 980, training loss: 6.074596881866455 = 0.03631158918142319 + 1.0 * 6.038285255432129
Epoch 980, val loss: 0.9663854241371155
Epoch 990, training loss: 6.073067665100098 = 0.03497200831770897 + 1.0 * 6.038095474243164
Epoch 990, val loss: 0.9719166159629822
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.4945
Flip ASR: 0.4044/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320562362670898 = 1.9466928243637085 + 1.0 * 8.373869895935059
Epoch 0, val loss: 1.9512659311294556
Epoch 10, training loss: 10.308841705322266 = 1.9358530044555664 + 1.0 * 8.3729887008667
Epoch 10, val loss: 1.9394621849060059
Epoch 20, training loss: 10.291549682617188 = 1.9226311445236206 + 1.0 * 8.368918418884277
Epoch 20, val loss: 1.9239585399627686
Epoch 30, training loss: 10.258468627929688 = 1.9048423767089844 + 1.0 * 8.353626251220703
Epoch 30, val loss: 1.9022029638290405
Epoch 40, training loss: 10.104243278503418 = 1.882818579673767 + 1.0 * 8.22142505645752
Epoch 40, val loss: 1.8756638765335083
Epoch 50, training loss: 9.445091247558594 = 1.8595248460769653 + 1.0 * 7.58556604385376
Epoch 50, val loss: 1.8487615585327148
Epoch 60, training loss: 9.189260482788086 = 1.8375604152679443 + 1.0 * 7.351700305938721
Epoch 60, val loss: 1.8265166282653809
Epoch 70, training loss: 8.955183982849121 = 1.8217014074325562 + 1.0 * 7.133482456207275
Epoch 70, val loss: 1.8110426664352417
Epoch 80, training loss: 8.725493431091309 = 1.8082635402679443 + 1.0 * 6.917229652404785
Epoch 80, val loss: 1.7961267232894897
Epoch 90, training loss: 8.557012557983398 = 1.7958213090896606 + 1.0 * 6.761191368103027
Epoch 90, val loss: 1.7821364402770996
Epoch 100, training loss: 8.423619270324707 = 1.7837647199630737 + 1.0 * 6.639854907989502
Epoch 100, val loss: 1.7691210508346558
Epoch 110, training loss: 8.32117748260498 = 1.7718627452850342 + 1.0 * 6.549314498901367
Epoch 110, val loss: 1.7574552297592163
Epoch 120, training loss: 8.235880851745605 = 1.759456753730774 + 1.0 * 6.476424217224121
Epoch 120, val loss: 1.7456483840942383
Epoch 130, training loss: 8.163030624389648 = 1.746214509010315 + 1.0 * 6.416815757751465
Epoch 130, val loss: 1.7331351041793823
Epoch 140, training loss: 8.105140686035156 = 1.731135606765747 + 1.0 * 6.37400484085083
Epoch 140, val loss: 1.7190512418746948
Epoch 150, training loss: 8.053037643432617 = 1.7132742404937744 + 1.0 * 6.339763164520264
Epoch 150, val loss: 1.7028324604034424
Epoch 160, training loss: 8.008106231689453 = 1.6912798881530762 + 1.0 * 6.316826820373535
Epoch 160, val loss: 1.6835030317306519
Epoch 170, training loss: 7.963720798492432 = 1.6643847227096558 + 1.0 * 6.299335956573486
Epoch 170, val loss: 1.660606861114502
Epoch 180, training loss: 7.9167327880859375 = 1.6320757865905762 + 1.0 * 6.284657001495361
Epoch 180, val loss: 1.6333461999893188
Epoch 190, training loss: 7.864608287811279 = 1.5926274061203003 + 1.0 * 6.2719807624816895
Epoch 190, val loss: 1.6002781391143799
Epoch 200, training loss: 7.806155681610107 = 1.5447841882705688 + 1.0 * 6.261371612548828
Epoch 200, val loss: 1.5604745149612427
Epoch 210, training loss: 7.741115093231201 = 1.4883953332901 + 1.0 * 6.252719879150391
Epoch 210, val loss: 1.5140116214752197
Epoch 220, training loss: 7.671656608581543 = 1.4272949695587158 + 1.0 * 6.244361877441406
Epoch 220, val loss: 1.4650706052780151
Epoch 230, training loss: 7.600227355957031 = 1.3652678728103638 + 1.0 * 6.234959602355957
Epoch 230, val loss: 1.4161168336868286
Epoch 240, training loss: 7.531402111053467 = 1.3045234680175781 + 1.0 * 6.226878643035889
Epoch 240, val loss: 1.3691200017929077
Epoch 250, training loss: 7.472515106201172 = 1.2474136352539062 + 1.0 * 6.225101470947266
Epoch 250, val loss: 1.32626473903656
Epoch 260, training loss: 7.4076738357543945 = 1.1956229209899902 + 1.0 * 6.212050914764404
Epoch 260, val loss: 1.288542628288269
Epoch 270, training loss: 7.352606296539307 = 1.1475434303283691 + 1.0 * 6.2050628662109375
Epoch 270, val loss: 1.2539548873901367
Epoch 280, training loss: 7.298628330230713 = 1.1015881299972534 + 1.0 * 6.19704008102417
Epoch 280, val loss: 1.2215136289596558
Epoch 290, training loss: 7.247071743011475 = 1.0564712285995483 + 1.0 * 6.190600395202637
Epoch 290, val loss: 1.1898435354232788
Epoch 300, training loss: 7.195927143096924 = 1.012262225151062 + 1.0 * 6.183664798736572
Epoch 300, val loss: 1.1586933135986328
Epoch 310, training loss: 7.147623062133789 = 0.9686521887779236 + 1.0 * 6.178970813751221
Epoch 310, val loss: 1.127907156944275
Epoch 320, training loss: 7.105411052703857 = 0.9255663156509399 + 1.0 * 6.179844856262207
Epoch 320, val loss: 1.0972481966018677
Epoch 330, training loss: 7.05197811126709 = 0.8834822773933411 + 1.0 * 6.1684956550598145
Epoch 330, val loss: 1.0672985315322876
Epoch 340, training loss: 7.005925178527832 = 0.8422667980194092 + 1.0 * 6.163658142089844
Epoch 340, val loss: 1.037935733795166
Epoch 350, training loss: 6.962088108062744 = 0.8023476004600525 + 1.0 * 6.159740447998047
Epoch 350, val loss: 1.0095611810684204
Epoch 360, training loss: 6.919086456298828 = 0.7641932368278503 + 1.0 * 6.154893398284912
Epoch 360, val loss: 0.9827855825424194
Epoch 370, training loss: 6.882854461669922 = 0.7279432415962219 + 1.0 * 6.154911041259766
Epoch 370, val loss: 0.9577170014381409
Epoch 380, training loss: 6.844040870666504 = 0.6939719319343567 + 1.0 * 6.150068759918213
Epoch 380, val loss: 0.9346247911453247
Epoch 390, training loss: 6.805482864379883 = 0.6618650555610657 + 1.0 * 6.143617630004883
Epoch 390, val loss: 0.9135343432426453
Epoch 400, training loss: 6.771678924560547 = 0.6314224004745483 + 1.0 * 6.140256404876709
Epoch 400, val loss: 0.8942039608955383
Epoch 410, training loss: 6.747618675231934 = 0.6024565100669861 + 1.0 * 6.145162105560303
Epoch 410, val loss: 0.8764517903327942
Epoch 420, training loss: 6.713108539581299 = 0.5751122236251831 + 1.0 * 6.137996196746826
Epoch 420, val loss: 0.860527515411377
Epoch 430, training loss: 6.682619094848633 = 0.5491117835044861 + 1.0 * 6.133507251739502
Epoch 430, val loss: 0.8463391661643982
Epoch 440, training loss: 6.653502464294434 = 0.5241851806640625 + 1.0 * 6.129317283630371
Epoch 440, val loss: 0.8334675431251526
Epoch 450, training loss: 6.628018379211426 = 0.5001398921012878 + 1.0 * 6.127878665924072
Epoch 450, val loss: 0.8218992948532104
Epoch 460, training loss: 6.600809097290039 = 0.47704017162323 + 1.0 * 6.1237688064575195
Epoch 460, val loss: 0.8115940093994141
Epoch 470, training loss: 6.577109336853027 = 0.4548347592353821 + 1.0 * 6.122274398803711
Epoch 470, val loss: 0.802563488483429
Epoch 480, training loss: 6.554642200469971 = 0.4334503710269928 + 1.0 * 6.12119197845459
Epoch 480, val loss: 0.7946307063102722
Epoch 490, training loss: 6.534480094909668 = 0.41281864047050476 + 1.0 * 6.12166166305542
Epoch 490, val loss: 0.7877803444862366
Epoch 500, training loss: 6.508904457092285 = 0.3931243121623993 + 1.0 * 6.115780353546143
Epoch 500, val loss: 0.781960666179657
Epoch 510, training loss: 6.4901604652404785 = 0.37417763471603394 + 1.0 * 6.115983009338379
Epoch 510, val loss: 0.7771506905555725
Epoch 520, training loss: 6.474147319793701 = 0.35626891255378723 + 1.0 * 6.117878437042236
Epoch 520, val loss: 0.7731652855873108
Epoch 530, training loss: 6.449084758758545 = 0.3392163813114166 + 1.0 * 6.10986852645874
Epoch 530, val loss: 0.7701566219329834
Epoch 540, training loss: 6.429900169372559 = 0.32289910316467285 + 1.0 * 6.107000827789307
Epoch 540, val loss: 0.7678564190864563
Epoch 550, training loss: 6.412057399749756 = 0.30726054310798645 + 1.0 * 6.104796886444092
Epoch 550, val loss: 0.7662551999092102
Epoch 560, training loss: 6.409092426300049 = 0.2922731637954712 + 1.0 * 6.116819381713867
Epoch 560, val loss: 0.7652480602264404
Epoch 570, training loss: 6.385132312774658 = 0.2780538499355316 + 1.0 * 6.107078552246094
Epoch 570, val loss: 0.7646239399909973
Epoch 580, training loss: 6.369958400726318 = 0.26463595032691956 + 1.0 * 6.105322360992432
Epoch 580, val loss: 0.7644283771514893
Epoch 590, training loss: 6.353355884552002 = 0.2518591582775116 + 1.0 * 6.101496696472168
Epoch 590, val loss: 0.7647005915641785
Epoch 600, training loss: 6.337374687194824 = 0.2396870106458664 + 1.0 * 6.097687721252441
Epoch 600, val loss: 0.7654221057891846
Epoch 610, training loss: 6.32434606552124 = 0.22801390290260315 + 1.0 * 6.09633207321167
Epoch 610, val loss: 0.7664637565612793
Epoch 620, training loss: 6.315651893615723 = 0.21686559915542603 + 1.0 * 6.098786354064941
Epoch 620, val loss: 0.767756462097168
Epoch 630, training loss: 6.300571441650391 = 0.20630508661270142 + 1.0 * 6.094266414642334
Epoch 630, val loss: 0.76933753490448
Epoch 640, training loss: 6.295613765716553 = 0.19628877937793732 + 1.0 * 6.099325180053711
Epoch 640, val loss: 0.7711994647979736
Epoch 650, training loss: 6.277160167694092 = 0.1868145912885666 + 1.0 * 6.09034538269043
Epoch 650, val loss: 0.7732437252998352
Epoch 660, training loss: 6.26850700378418 = 0.17785212397575378 + 1.0 * 6.0906548500061035
Epoch 660, val loss: 0.7756009697914124
Epoch 670, training loss: 6.262248516082764 = 0.1693524718284607 + 1.0 * 6.092895984649658
Epoch 670, val loss: 0.7781951427459717
Epoch 680, training loss: 6.248342990875244 = 0.1613464057445526 + 1.0 * 6.086996555328369
Epoch 680, val loss: 0.7810626029968262
Epoch 690, training loss: 6.241339683532715 = 0.15376558899879456 + 1.0 * 6.087574005126953
Epoch 690, val loss: 0.7841904163360596
Epoch 700, training loss: 6.2320170402526855 = 0.1466367542743683 + 1.0 * 6.0853800773620605
Epoch 700, val loss: 0.7874812483787537
Epoch 710, training loss: 6.224117755889893 = 0.13993167877197266 + 1.0 * 6.08418607711792
Epoch 710, val loss: 0.790956974029541
Epoch 720, training loss: 6.215450286865234 = 0.13360914587974548 + 1.0 * 6.081840991973877
Epoch 720, val loss: 0.7947273850440979
Epoch 730, training loss: 6.212669849395752 = 0.12762047350406647 + 1.0 * 6.085049152374268
Epoch 730, val loss: 0.7986533641815186
Epoch 740, training loss: 6.205052852630615 = 0.12199147790670395 + 1.0 * 6.083061218261719
Epoch 740, val loss: 0.8026116490364075
Epoch 750, training loss: 6.196995258331299 = 0.1167064905166626 + 1.0 * 6.080288887023926
Epoch 750, val loss: 0.8068130612373352
Epoch 760, training loss: 6.1941657066345215 = 0.11171724647283554 + 1.0 * 6.082448482513428
Epoch 760, val loss: 0.8110758662223816
Epoch 770, training loss: 6.183076858520508 = 0.10700535029172897 + 1.0 * 6.076071739196777
Epoch 770, val loss: 0.8154709339141846
Epoch 780, training loss: 6.178657531738281 = 0.10253634303808212 + 1.0 * 6.0761213302612305
Epoch 780, val loss: 0.8200305700302124
Epoch 790, training loss: 6.173951148986816 = 0.09830061346292496 + 1.0 * 6.075650691986084
Epoch 790, val loss: 0.8246075510978699
Epoch 800, training loss: 6.167799472808838 = 0.09429042041301727 + 1.0 * 6.073509216308594
Epoch 800, val loss: 0.8292697668075562
Epoch 810, training loss: 6.162782669067383 = 0.09049522131681442 + 1.0 * 6.072287559509277
Epoch 810, val loss: 0.834041178226471
Epoch 820, training loss: 6.162918567657471 = 0.0868934839963913 + 1.0 * 6.076025009155273
Epoch 820, val loss: 0.8387957811355591
Epoch 830, training loss: 6.157310485839844 = 0.08350465446710587 + 1.0 * 6.073805809020996
Epoch 830, val loss: 0.8435565829277039
Epoch 840, training loss: 6.150330543518066 = 0.08029672503471375 + 1.0 * 6.070034027099609
Epoch 840, val loss: 0.8483292460441589
Epoch 850, training loss: 6.145050525665283 = 0.07725366950035095 + 1.0 * 6.06779670715332
Epoch 850, val loss: 0.8532095551490784
Epoch 860, training loss: 6.140171527862549 = 0.07434836775064468 + 1.0 * 6.065823078155518
Epoch 860, val loss: 0.8581426739692688
Epoch 870, training loss: 6.147893905639648 = 0.07157091796398163 + 1.0 * 6.07632303237915
Epoch 870, val loss: 0.8630640506744385
Epoch 880, training loss: 6.137428283691406 = 0.06895477324724197 + 1.0 * 6.0684733390808105
Epoch 880, val loss: 0.8679777979850769
Epoch 890, training loss: 6.13168478012085 = 0.06644723564386368 + 1.0 * 6.065237522125244
Epoch 890, val loss: 0.8728663921356201
Epoch 900, training loss: 6.12696647644043 = 0.06406459957361221 + 1.0 * 6.062901973724365
Epoch 900, val loss: 0.8778989315032959
Epoch 910, training loss: 6.1329665184021 = 0.06177491322159767 + 1.0 * 6.071191787719727
Epoch 910, val loss: 0.8828229308128357
Epoch 920, training loss: 6.124900817871094 = 0.05959406867623329 + 1.0 * 6.065306663513184
Epoch 920, val loss: 0.8876579999923706
Epoch 930, training loss: 6.119187355041504 = 0.05751664564013481 + 1.0 * 6.061670780181885
Epoch 930, val loss: 0.8925061225891113
Epoch 940, training loss: 6.117911338806152 = 0.055534061044454575 + 1.0 * 6.062377452850342
Epoch 940, val loss: 0.8973717093467712
Epoch 950, training loss: 6.112936973571777 = 0.05363320931792259 + 1.0 * 6.0593037605285645
Epoch 950, val loss: 0.902176022529602
Epoch 960, training loss: 6.109496116638184 = 0.0518130399286747 + 1.0 * 6.057682991027832
Epoch 960, val loss: 0.9069966077804565
Epoch 970, training loss: 6.112229824066162 = 0.050066445022821426 + 1.0 * 6.062163352966309
Epoch 970, val loss: 0.9118083715438843
Epoch 980, training loss: 6.105777740478516 = 0.04839744418859482 + 1.0 * 6.057380199432373
Epoch 980, val loss: 0.9165138602256775
Epoch 990, training loss: 6.102360248565674 = 0.04679761081933975 + 1.0 * 6.055562496185303
Epoch 990, val loss: 0.9212141633033752
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.1661
Flip ASR: 0.1867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.314615249633789 = 1.940707802772522 + 1.0 * 8.373907089233398
Epoch 0, val loss: 1.9420263767242432
Epoch 10, training loss: 10.304464340209961 = 1.930826187133789 + 1.0 * 8.373638153076172
Epoch 10, val loss: 1.932626485824585
Epoch 20, training loss: 10.290613174438477 = 1.918428659439087 + 1.0 * 8.372184753417969
Epoch 20, val loss: 1.9202512502670288
Epoch 30, training loss: 10.263261795043945 = 1.9008820056915283 + 1.0 * 8.362380027770996
Epoch 30, val loss: 1.902227759361267
Epoch 40, training loss: 10.164670944213867 = 1.8768055438995361 + 1.0 * 8.28786563873291
Epoch 40, val loss: 1.8779722452163696
Epoch 50, training loss: 9.610305786132812 = 1.8503468036651611 + 1.0 * 7.7599592208862305
Epoch 50, val loss: 1.8524268865585327
Epoch 60, training loss: 8.975702285766602 = 1.832107663154602 + 1.0 * 7.143594264984131
Epoch 60, val loss: 1.8358314037322998
Epoch 70, training loss: 8.708127975463867 = 1.819258689880371 + 1.0 * 6.888869762420654
Epoch 70, val loss: 1.8229038715362549
Epoch 80, training loss: 8.54472541809082 = 1.8055708408355713 + 1.0 * 6.73915433883667
Epoch 80, val loss: 1.809935450553894
Epoch 90, training loss: 8.410770416259766 = 1.7929826974868774 + 1.0 * 6.6177873611450195
Epoch 90, val loss: 1.7985644340515137
Epoch 100, training loss: 8.316664695739746 = 1.782299280166626 + 1.0 * 6.534365653991699
Epoch 100, val loss: 1.788939356803894
Epoch 110, training loss: 8.233684539794922 = 1.771923303604126 + 1.0 * 6.461761474609375
Epoch 110, val loss: 1.7794221639633179
Epoch 120, training loss: 8.170027732849121 = 1.760542631149292 + 1.0 * 6.409485340118408
Epoch 120, val loss: 1.7692878246307373
Epoch 130, training loss: 8.121676445007324 = 1.747182011604309 + 1.0 * 6.3744940757751465
Epoch 130, val loss: 1.757628321647644
Epoch 140, training loss: 8.079097747802734 = 1.7312231063842773 + 1.0 * 6.347875118255615
Epoch 140, val loss: 1.7441128492355347
Epoch 150, training loss: 8.038082122802734 = 1.7122100591659546 + 1.0 * 6.325872421264648
Epoch 150, val loss: 1.728521466255188
Epoch 160, training loss: 7.994563102722168 = 1.6895791292190552 + 1.0 * 6.304984092712402
Epoch 160, val loss: 1.7101722955703735
Epoch 170, training loss: 7.947671413421631 = 1.6623984575271606 + 1.0 * 6.28527307510376
Epoch 170, val loss: 1.6883703470230103
Epoch 180, training loss: 7.897289276123047 = 1.6295729875564575 + 1.0 * 6.267716407775879
Epoch 180, val loss: 1.6619840860366821
Epoch 190, training loss: 7.843180179595947 = 1.590391755104065 + 1.0 * 6.252788543701172
Epoch 190, val loss: 1.6303790807724
Epoch 200, training loss: 7.782553195953369 = 1.5445642471313477 + 1.0 * 6.2379889488220215
Epoch 200, val loss: 1.5934077501296997
Epoch 210, training loss: 7.7189435958862305 = 1.4924488067626953 + 1.0 * 6.226494789123535
Epoch 210, val loss: 1.5513185262680054
Epoch 220, training loss: 7.651855945587158 = 1.4349923133850098 + 1.0 * 6.216863632202148
Epoch 220, val loss: 1.5049493312835693
Epoch 230, training loss: 7.582305908203125 = 1.3737965822219849 + 1.0 * 6.20850944519043
Epoch 230, val loss: 1.4560922384262085
Epoch 240, training loss: 7.512572288513184 = 1.3112350702285767 + 1.0 * 6.2013373374938965
Epoch 240, val loss: 1.4065558910369873
Epoch 250, training loss: 7.441722393035889 = 1.2489676475524902 + 1.0 * 6.192754745483398
Epoch 250, val loss: 1.3578625917434692
Epoch 260, training loss: 7.380169868469238 = 1.1880650520324707 + 1.0 * 6.192104816436768
Epoch 260, val loss: 1.310814619064331
Epoch 270, training loss: 7.312291622161865 = 1.1305699348449707 + 1.0 * 6.1817216873168945
Epoch 270, val loss: 1.2668511867523193
Epoch 280, training loss: 7.24913215637207 = 1.0761723518371582 + 1.0 * 6.172959804534912
Epoch 280, val loss: 1.2254060506820679
Epoch 290, training loss: 7.199009895324707 = 1.024562954902649 + 1.0 * 6.174447059631348
Epoch 290, val loss: 1.1859569549560547
Epoch 300, training loss: 7.141165733337402 = 0.9763134121894836 + 1.0 * 6.164852142333984
Epoch 300, val loss: 1.1492961645126343
Epoch 310, training loss: 7.088749885559082 = 0.9307668209075928 + 1.0 * 6.15798282623291
Epoch 310, val loss: 1.1149028539657593
Epoch 320, training loss: 7.042725563049316 = 0.8871082067489624 + 1.0 * 6.1556172370910645
Epoch 320, val loss: 1.0817824602127075
Epoch 330, training loss: 6.995370864868164 = 0.8455914855003357 + 1.0 * 6.149779319763184
Epoch 330, val loss: 1.050287127494812
Epoch 340, training loss: 6.95247745513916 = 0.8060218691825867 + 1.0 * 6.146455764770508
Epoch 340, val loss: 1.020269513130188
Epoch 350, training loss: 6.909525394439697 = 0.7675656676292419 + 1.0 * 6.1419596672058105
Epoch 350, val loss: 0.9911524057388306
Epoch 360, training loss: 6.868070602416992 = 0.7299566864967346 + 1.0 * 6.138113975524902
Epoch 360, val loss: 0.9628299474716187
Epoch 370, training loss: 6.835539817810059 = 0.6933181881904602 + 1.0 * 6.142221450805664
Epoch 370, val loss: 0.9355264902114868
Epoch 380, training loss: 6.792160511016846 = 0.6583394408226013 + 1.0 * 6.1338210105896
Epoch 380, val loss: 0.9100937247276306
Epoch 390, training loss: 6.7547407150268555 = 0.6245870590209961 + 1.0 * 6.130153656005859
Epoch 390, val loss: 0.8863507509231567
Epoch 400, training loss: 6.719174861907959 = 0.5918115377426147 + 1.0 * 6.127363204956055
Epoch 400, val loss: 0.8640896081924438
Epoch 410, training loss: 6.690009593963623 = 0.5601102113723755 + 1.0 * 6.129899501800537
Epoch 410, val loss: 0.843521773815155
Epoch 420, training loss: 6.6557087898254395 = 0.5298951864242554 + 1.0 * 6.1258134841918945
Epoch 420, val loss: 0.8249905705451965
Epoch 430, training loss: 6.620824337005615 = 0.5006901025772095 + 1.0 * 6.120134353637695
Epoch 430, val loss: 0.8081007599830627
Epoch 440, training loss: 6.590447425842285 = 0.47218093276023865 + 1.0 * 6.118266582489014
Epoch 440, val loss: 0.7925419211387634
Epoch 450, training loss: 6.56062650680542 = 0.44441959261894226 + 1.0 * 6.116207122802734
Epoch 450, val loss: 0.7783651351928711
Epoch 460, training loss: 6.532853126525879 = 0.41755175590515137 + 1.0 * 6.115301132202148
Epoch 460, val loss: 0.7657208442687988
Epoch 470, training loss: 6.503317832946777 = 0.3913487195968628 + 1.0 * 6.111968994140625
Epoch 470, val loss: 0.7545008659362793
Epoch 480, training loss: 6.478173732757568 = 0.36595359444618225 + 1.0 * 6.112220287322998
Epoch 480, val loss: 0.7446717619895935
Epoch 490, training loss: 6.4524970054626465 = 0.34172573685646057 + 1.0 * 6.110771179199219
Epoch 490, val loss: 0.7362948656082153
Epoch 500, training loss: 6.4251298904418945 = 0.3184340000152588 + 1.0 * 6.106696128845215
Epoch 500, val loss: 0.729297399520874
Epoch 510, training loss: 6.403687000274658 = 0.2961548864841461 + 1.0 * 6.107532024383545
Epoch 510, val loss: 0.7234846949577332
Epoch 520, training loss: 6.382772445678711 = 0.2750844955444336 + 1.0 * 6.107687950134277
Epoch 520, val loss: 0.7189612984657288
Epoch 530, training loss: 6.359654426574707 = 0.25537872314453125 + 1.0 * 6.104275703430176
Epoch 530, val loss: 0.7156909704208374
Epoch 540, training loss: 6.336002826690674 = 0.23703187704086304 + 1.0 * 6.098970890045166
Epoch 540, val loss: 0.7135583758354187
Epoch 550, training loss: 6.3171186447143555 = 0.21997031569480896 + 1.0 * 6.097148418426514
Epoch 550, val loss: 0.7124311327934265
Epoch 560, training loss: 6.304482460021973 = 0.20425298810005188 + 1.0 * 6.100229263305664
Epoch 560, val loss: 0.7122268080711365
Epoch 570, training loss: 6.284845352172852 = 0.19000285863876343 + 1.0 * 6.094842433929443
Epoch 570, val loss: 0.712965726852417
Epoch 580, training loss: 6.269325256347656 = 0.17697253823280334 + 1.0 * 6.092352867126465
Epoch 580, val loss: 0.7146145105361938
Epoch 590, training loss: 6.262416362762451 = 0.1650344431400299 + 1.0 * 6.097382068634033
Epoch 590, val loss: 0.7169820666313171
Epoch 600, training loss: 6.2474799156188965 = 0.1541680246591568 + 1.0 * 6.093311786651611
Epoch 600, val loss: 0.7200039625167847
Epoch 610, training loss: 6.232908725738525 = 0.14426961541175842 + 1.0 * 6.088639259338379
Epoch 610, val loss: 0.7237170934677124
Epoch 620, training loss: 6.231871604919434 = 0.13523167371749878 + 1.0 * 6.096640110015869
Epoch 620, val loss: 0.7279314994812012
Epoch 630, training loss: 6.212885856628418 = 0.1270100325345993 + 1.0 * 6.085875988006592
Epoch 630, val loss: 0.7326068878173828
Epoch 640, training loss: 6.204058647155762 = 0.11949979513883591 + 1.0 * 6.084558963775635
Epoch 640, val loss: 0.7378155589103699
Epoch 650, training loss: 6.198684215545654 = 0.11261198669672012 + 1.0 * 6.0860724449157715
Epoch 650, val loss: 0.7433239221572876
Epoch 660, training loss: 6.1879377365112305 = 0.10632060468196869 + 1.0 * 6.08161735534668
Epoch 660, val loss: 0.7492161989212036
Epoch 670, training loss: 6.184666156768799 = 0.10054059326648712 + 1.0 * 6.084125518798828
Epoch 670, val loss: 0.7554290890693665
Epoch 680, training loss: 6.173637390136719 = 0.09522467851638794 + 1.0 * 6.0784125328063965
Epoch 680, val loss: 0.7618505358695984
Epoch 690, training loss: 6.167172431945801 = 0.0903129056096077 + 1.0 * 6.076859474182129
Epoch 690, val loss: 0.7685655355453491
Epoch 700, training loss: 6.167309284210205 = 0.0857565850019455 + 1.0 * 6.081552505493164
Epoch 700, val loss: 0.7754639387130737
Epoch 710, training loss: 6.164531707763672 = 0.0815717950463295 + 1.0 * 6.08296012878418
Epoch 710, val loss: 0.7824392318725586
Epoch 720, training loss: 6.151993274688721 = 0.07770621031522751 + 1.0 * 6.074286937713623
Epoch 720, val loss: 0.7896258234977722
Epoch 730, training loss: 6.1454362869262695 = 0.07409856468439102 + 1.0 * 6.071337699890137
Epoch 730, val loss: 0.7969329953193665
Epoch 740, training loss: 6.143984317779541 = 0.07071440666913986 + 1.0 * 6.073269844055176
Epoch 740, val loss: 0.804368793964386
Epoch 750, training loss: 6.140717029571533 = 0.06756361573934555 + 1.0 * 6.073153495788574
Epoch 750, val loss: 0.8117390871047974
Epoch 760, training loss: 6.133872032165527 = 0.06463880836963654 + 1.0 * 6.069233417510986
Epoch 760, val loss: 0.819300651550293
Epoch 770, training loss: 6.130342483520508 = 0.06189173832535744 + 1.0 * 6.068450927734375
Epoch 770, val loss: 0.8269284963607788
Epoch 780, training loss: 6.136694431304932 = 0.059299468994140625 + 1.0 * 6.077394962310791
Epoch 780, val loss: 0.834510326385498
Epoch 790, training loss: 6.124965190887451 = 0.05687759444117546 + 1.0 * 6.068087577819824
Epoch 790, val loss: 0.8420640230178833
Epoch 800, training loss: 6.119253158569336 = 0.05459917336702347 + 1.0 * 6.0646538734436035
Epoch 800, val loss: 0.8497526049613953
Epoch 810, training loss: 6.127161502838135 = 0.05244173854589462 + 1.0 * 6.0747199058532715
Epoch 810, val loss: 0.8573728799819946
Epoch 820, training loss: 6.1158342361450195 = 0.0504131093621254 + 1.0 * 6.065421104431152
Epoch 820, val loss: 0.8648672699928284
Epoch 830, training loss: 6.111159324645996 = 0.04850834980607033 + 1.0 * 6.06265115737915
Epoch 830, val loss: 0.8724741339683533
Epoch 840, training loss: 6.107700347900391 = 0.04669307917356491 + 1.0 * 6.061007499694824
Epoch 840, val loss: 0.8799896240234375
Epoch 850, training loss: 6.105572700500488 = 0.0449613481760025 + 1.0 * 6.060611248016357
Epoch 850, val loss: 0.88753342628479
Epoch 860, training loss: 6.110413074493408 = 0.043316084891557693 + 1.0 * 6.067097187042236
Epoch 860, val loss: 0.8949944972991943
Epoch 870, training loss: 6.102917671203613 = 0.041771404445171356 + 1.0 * 6.061146259307861
Epoch 870, val loss: 0.9023849368095398
Epoch 880, training loss: 6.0981831550598145 = 0.04030877351760864 + 1.0 * 6.0578742027282715
Epoch 880, val loss: 0.9098773002624512
Epoch 890, training loss: 6.096972942352295 = 0.03890600800514221 + 1.0 * 6.0580668449401855
Epoch 890, val loss: 0.9172756671905518
Epoch 900, training loss: 6.099613189697266 = 0.037567138671875 + 1.0 * 6.062046051025391
Epoch 900, val loss: 0.9245534539222717
Epoch 910, training loss: 6.094183921813965 = 0.03630046918988228 + 1.0 * 6.057883262634277
Epoch 910, val loss: 0.9317389726638794
Epoch 920, training loss: 6.09090518951416 = 0.035093531012535095 + 1.0 * 6.055811882019043
Epoch 920, val loss: 0.9388992786407471
Epoch 930, training loss: 6.0889363288879395 = 0.03393780440092087 + 1.0 * 6.054998397827148
Epoch 930, val loss: 0.9460285305976868
Epoch 940, training loss: 6.094479084014893 = 0.03283344954252243 + 1.0 * 6.0616455078125
Epoch 940, val loss: 0.9530642032623291
Epoch 950, training loss: 6.0874810218811035 = 0.03178587183356285 + 1.0 * 6.055695056915283
Epoch 950, val loss: 0.9600025415420532
Epoch 960, training loss: 6.0826568603515625 = 0.030786877498030663 + 1.0 * 6.051869869232178
Epoch 960, val loss: 0.9669567942619324
Epoch 970, training loss: 6.081314563751221 = 0.029826171696186066 + 1.0 * 6.051488399505615
Epoch 970, val loss: 0.9738499522209167
Epoch 980, training loss: 6.08593225479126 = 0.02890479378402233 + 1.0 * 6.057027339935303
Epoch 980, val loss: 0.9806293845176697
Epoch 990, training loss: 6.082539081573486 = 0.028029896318912506 + 1.0 * 6.054509162902832
Epoch 990, val loss: 0.987307071685791
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.9225
Flip ASR: 0.9067/225 nodes
The final ASR:0.52768, 0.30971, Accuracy:0.81481, 0.01512
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9560])
updated graph: torch.Size([2, 10606])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98647, 0.00758, Accuracy:0.82716, 0.00349
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.314738273620605 = 1.9408273696899414 + 1.0 * 8.373910903930664
Epoch 0, val loss: 1.9337741136550903
Epoch 10, training loss: 10.305039405822754 = 1.931391954421997 + 1.0 * 8.373647689819336
Epoch 10, val loss: 1.924225091934204
Epoch 20, training loss: 10.291387557983398 = 1.9195303916931152 + 1.0 * 8.371857643127441
Epoch 20, val loss: 1.912185788154602
Epoch 30, training loss: 10.260891914367676 = 1.9026516675949097 + 1.0 * 8.358240127563477
Epoch 30, val loss: 1.895135521888733
Epoch 40, training loss: 10.150165557861328 = 1.8795260190963745 + 1.0 * 8.270639419555664
Epoch 40, val loss: 1.8727608919143677
Epoch 50, training loss: 9.59586238861084 = 1.8545072078704834 + 1.0 * 7.7413554191589355
Epoch 50, val loss: 1.8496944904327393
Epoch 60, training loss: 9.073225021362305 = 1.8317896127700806 + 1.0 * 7.241435527801514
Epoch 60, val loss: 1.8300153017044067
Epoch 70, training loss: 8.77329158782959 = 1.8154181241989136 + 1.0 * 6.957873344421387
Epoch 70, val loss: 1.816025972366333
Epoch 80, training loss: 8.598912239074707 = 1.7987736463546753 + 1.0 * 6.800138473510742
Epoch 80, val loss: 1.801314353942871
Epoch 90, training loss: 8.465760231018066 = 1.7820334434509277 + 1.0 * 6.683726787567139
Epoch 90, val loss: 1.7873694896697998
Epoch 100, training loss: 8.364538192749023 = 1.7646199464797974 + 1.0 * 6.599918365478516
Epoch 100, val loss: 1.7731424570083618
Epoch 110, training loss: 8.272367477416992 = 1.7474188804626465 + 1.0 * 6.524948596954346
Epoch 110, val loss: 1.7592588663101196
Epoch 120, training loss: 8.198110580444336 = 1.7289197444915771 + 1.0 * 6.46919059753418
Epoch 120, val loss: 1.744336724281311
Epoch 130, training loss: 8.135095596313477 = 1.7076051235198975 + 1.0 * 6.427490234375
Epoch 130, val loss: 1.7268941402435303
Epoch 140, training loss: 8.07575511932373 = 1.682455062866211 + 1.0 * 6.3933000564575195
Epoch 140, val loss: 1.7061389684677124
Epoch 150, training loss: 8.017918586730957 = 1.6525564193725586 + 1.0 * 6.365362167358398
Epoch 150, val loss: 1.681623935699463
Epoch 160, training loss: 7.958502292633057 = 1.6174267530441284 + 1.0 * 6.341075420379639
Epoch 160, val loss: 1.6529775857925415
Epoch 170, training loss: 7.897039413452148 = 1.5762627124786377 + 1.0 * 6.320776462554932
Epoch 170, val loss: 1.6194697618484497
Epoch 180, training loss: 7.833547592163086 = 1.5285046100616455 + 1.0 * 6.3050432205200195
Epoch 180, val loss: 1.5806230306625366
Epoch 190, training loss: 7.765500545501709 = 1.4750696420669556 + 1.0 * 6.290431022644043
Epoch 190, val loss: 1.5373334884643555
Epoch 200, training loss: 7.695014953613281 = 1.4169502258300781 + 1.0 * 6.278064727783203
Epoch 200, val loss: 1.4907344579696655
Epoch 210, training loss: 7.625566482543945 = 1.3559590578079224 + 1.0 * 6.2696075439453125
Epoch 210, val loss: 1.4426020383834839
Epoch 220, training loss: 7.551661968231201 = 1.295102596282959 + 1.0 * 6.256559371948242
Epoch 220, val loss: 1.3953436613082886
Epoch 230, training loss: 7.481595993041992 = 1.2353236675262451 + 1.0 * 6.246272087097168
Epoch 230, val loss: 1.3496577739715576
Epoch 240, training loss: 7.41715145111084 = 1.1780935525894165 + 1.0 * 6.239058017730713
Epoch 240, val loss: 1.30720853805542
Epoch 250, training loss: 7.353710174560547 = 1.1248109340667725 + 1.0 * 6.2288994789123535
Epoch 250, val loss: 1.268280267715454
Epoch 260, training loss: 7.294500827789307 = 1.0748038291931152 + 1.0 * 6.219696998596191
Epoch 260, val loss: 1.232493281364441
Epoch 270, training loss: 7.248694896697998 = 1.0279184579849243 + 1.0 * 6.220776557922363
Epoch 270, val loss: 1.1995939016342163
Epoch 280, training loss: 7.192355155944824 = 0.9850536584854126 + 1.0 * 6.207301616668701
Epoch 280, val loss: 1.1698193550109863
Epoch 290, training loss: 7.14587926864624 = 0.9449113011360168 + 1.0 * 6.200967788696289
Epoch 290, val loss: 1.1422615051269531
Epoch 300, training loss: 7.104795932769775 = 0.9065110087394714 + 1.0 * 6.198285102844238
Epoch 300, val loss: 1.1158502101898193
Epoch 310, training loss: 7.059415340423584 = 0.8696044087409973 + 1.0 * 6.189810752868652
Epoch 310, val loss: 1.0902889966964722
Epoch 320, training loss: 7.018129348754883 = 0.8332407474517822 + 1.0 * 6.18488883972168
Epoch 320, val loss: 1.0648950338363647
Epoch 330, training loss: 6.978551387786865 = 0.7967702150344849 + 1.0 * 6.18178129196167
Epoch 330, val loss: 1.0391169786453247
Epoch 340, training loss: 6.940577030181885 = 0.7603051066398621 + 1.0 * 6.180272102355957
Epoch 340, val loss: 1.0130712985992432
Epoch 350, training loss: 6.900038242340088 = 0.7241492867469788 + 1.0 * 6.175889015197754
Epoch 350, val loss: 0.9871336817741394
Epoch 360, training loss: 6.857996940612793 = 0.6876226663589478 + 1.0 * 6.170374393463135
Epoch 360, val loss: 0.9609493613243103
Epoch 370, training loss: 6.820040225982666 = 0.650905191898346 + 1.0 * 6.169135093688965
Epoch 370, val loss: 0.9345778226852417
Epoch 380, training loss: 6.781184196472168 = 0.6144548058509827 + 1.0 * 6.16672945022583
Epoch 380, val loss: 0.9086869359016418
Epoch 390, training loss: 6.740425109863281 = 0.578561007976532 + 1.0 * 6.161864280700684
Epoch 390, val loss: 0.8836417198181152
Epoch 400, training loss: 6.71364688873291 = 0.5432693958282471 + 1.0 * 6.170377254486084
Epoch 400, val loss: 0.8596753478050232
Epoch 410, training loss: 6.6688032150268555 = 0.5093674063682556 + 1.0 * 6.159435749053955
Epoch 410, val loss: 0.8374118208885193
Epoch 420, training loss: 6.6300201416015625 = 0.47670742869377136 + 1.0 * 6.153312683105469
Epoch 420, val loss: 0.8171363472938538
Epoch 430, training loss: 6.594964027404785 = 0.4452970027923584 + 1.0 * 6.149667263031006
Epoch 430, val loss: 0.7986946105957031
Epoch 440, training loss: 6.578436851501465 = 0.41524016857147217 + 1.0 * 6.163196563720703
Epoch 440, val loss: 0.7822604179382324
Epoch 450, training loss: 6.538012504577637 = 0.38712039589881897 + 1.0 * 6.15089225769043
Epoch 450, val loss: 0.7680139541625977
Epoch 460, training loss: 6.502774715423584 = 0.3606623709201813 + 1.0 * 6.1421122550964355
Epoch 460, val loss: 0.7559442520141602
Epoch 470, training loss: 6.476149082183838 = 0.33570635318756104 + 1.0 * 6.140442848205566
Epoch 470, val loss: 0.7457315325737
Epoch 480, training loss: 6.450789451599121 = 0.31237906217575073 + 1.0 * 6.138410568237305
Epoch 480, val loss: 0.7372899055480957
Epoch 490, training loss: 6.425865173339844 = 0.29068872332572937 + 1.0 * 6.135176658630371
Epoch 490, val loss: 0.730674684047699
Epoch 500, training loss: 6.40588903427124 = 0.27050551772117615 + 1.0 * 6.135383605957031
Epoch 500, val loss: 0.7256799936294556
Epoch 510, training loss: 6.38218879699707 = 0.2517656683921814 + 1.0 * 6.130423069000244
Epoch 510, val loss: 0.7221540808677673
Epoch 520, training loss: 6.3735551834106445 = 0.23443642258644104 + 1.0 * 6.139118671417236
Epoch 520, val loss: 0.7200201153755188
Epoch 530, training loss: 6.347282409667969 = 0.21862754225730896 + 1.0 * 6.128654956817627
Epoch 530, val loss: 0.719154953956604
Epoch 540, training loss: 6.328384876251221 = 0.20407362282276154 + 1.0 * 6.124311447143555
Epoch 540, val loss: 0.7195745706558228
Epoch 550, training loss: 6.313234329223633 = 0.1906268298625946 + 1.0 * 6.122607707977295
Epoch 550, val loss: 0.7210490703582764
Epoch 560, training loss: 6.3018999099731445 = 0.17824816703796387 + 1.0 * 6.123651504516602
Epoch 560, val loss: 0.7234607934951782
Epoch 570, training loss: 6.285630226135254 = 0.16690422594547272 + 1.0 * 6.118725776672363
Epoch 570, val loss: 0.7267563343048096
Epoch 580, training loss: 6.273890495300293 = 0.15644846856594086 + 1.0 * 6.1174421310424805
Epoch 580, val loss: 0.7308751940727234
Epoch 590, training loss: 6.281269073486328 = 0.14683258533477783 + 1.0 * 6.13443660736084
Epoch 590, val loss: 0.735605776309967
Epoch 600, training loss: 6.254251480102539 = 0.13804589211940765 + 1.0 * 6.11620569229126
Epoch 600, val loss: 0.7407875061035156
Epoch 610, training loss: 6.243593215942383 = 0.12996765971183777 + 1.0 * 6.113625526428223
Epoch 610, val loss: 0.7465875148773193
Epoch 620, training loss: 6.242115020751953 = 0.1225089430809021 + 1.0 * 6.119606018066406
Epoch 620, val loss: 0.7527994513511658
Epoch 630, training loss: 6.225839614868164 = 0.11564915627241135 + 1.0 * 6.110190391540527
Epoch 630, val loss: 0.7593114972114563
Epoch 640, training loss: 6.222011566162109 = 0.10929228365421295 + 1.0 * 6.1127190589904785
Epoch 640, val loss: 0.7662140727043152
Epoch 650, training loss: 6.21263313293457 = 0.10339441150426865 + 1.0 * 6.109238624572754
Epoch 650, val loss: 0.773245632648468
Epoch 660, training loss: 6.2029314041137695 = 0.09793158620595932 + 1.0 * 6.105000019073486
Epoch 660, val loss: 0.7806010842323303
Epoch 670, training loss: 6.214654445648193 = 0.09284032881259918 + 1.0 * 6.121814250946045
Epoch 670, val loss: 0.7881022691726685
Epoch 680, training loss: 6.192887783050537 = 0.08814230561256409 + 1.0 * 6.104745388031006
Epoch 680, val loss: 0.7955881357192993
Epoch 690, training loss: 6.184883117675781 = 0.08377223461866379 + 1.0 * 6.101110935211182
Epoch 690, val loss: 0.8032844066619873
Epoch 700, training loss: 6.178782939910889 = 0.07967376708984375 + 1.0 * 6.099109172821045
Epoch 700, val loss: 0.81110018491745
Epoch 710, training loss: 6.1745219230651855 = 0.07582329213619232 + 1.0 * 6.098698616027832
Epoch 710, val loss: 0.8190131783485413
Epoch 720, training loss: 6.172567844390869 = 0.07223191112279892 + 1.0 * 6.100336074829102
Epoch 720, val loss: 0.8269095420837402
Epoch 730, training loss: 6.167906284332275 = 0.0688900277018547 + 1.0 * 6.099016189575195
Epoch 730, val loss: 0.8347712755203247
Epoch 740, training loss: 6.1607513427734375 = 0.06575947254896164 + 1.0 * 6.094991683959961
Epoch 740, val loss: 0.8426693677902222
Epoch 750, training loss: 6.156250476837158 = 0.0628143846988678 + 1.0 * 6.093436241149902
Epoch 750, val loss: 0.850593090057373
Epoch 760, training loss: 6.162522315979004 = 0.0600389689207077 + 1.0 * 6.10248327255249
Epoch 760, val loss: 0.8584969639778137
Epoch 770, training loss: 6.148524284362793 = 0.057454586029052734 + 1.0 * 6.09106969833374
Epoch 770, val loss: 0.8663760423660278
Epoch 780, training loss: 6.147474765777588 = 0.0550164133310318 + 1.0 * 6.092458248138428
Epoch 780, val loss: 0.8742861151695251
Epoch 790, training loss: 6.144865036010742 = 0.052726928144693375 + 1.0 * 6.092138290405273
Epoch 790, val loss: 0.8820119500160217
Epoch 800, training loss: 6.140446186065674 = 0.05058704689145088 + 1.0 * 6.0898590087890625
Epoch 800, val loss: 0.8896635174751282
Epoch 810, training loss: 6.136881351470947 = 0.04856632649898529 + 1.0 * 6.088315010070801
Epoch 810, val loss: 0.8973144292831421
Epoch 820, training loss: 6.133067607879639 = 0.04664946347475052 + 1.0 * 6.086418151855469
Epoch 820, val loss: 0.904979944229126
Epoch 830, training loss: 6.1343092918396 = 0.044839728623628616 + 1.0 * 6.0894694328308105
Epoch 830, val loss: 0.9125624895095825
Epoch 840, training loss: 6.127564907073975 = 0.04312823340296745 + 1.0 * 6.084436893463135
Epoch 840, val loss: 0.9200229048728943
Epoch 850, training loss: 6.13640832901001 = 0.041511185467243195 + 1.0 * 6.094897270202637
Epoch 850, val loss: 0.9275186061859131
Epoch 860, training loss: 6.1253862380981445 = 0.03999418392777443 + 1.0 * 6.085391998291016
Epoch 860, val loss: 0.9346486926078796
Epoch 870, training loss: 6.121038436889648 = 0.03856147453188896 + 1.0 * 6.08247709274292
Epoch 870, val loss: 0.9418496489524841
Epoch 880, training loss: 6.116833209991455 = 0.037197284400463104 + 1.0 * 6.079636096954346
Epoch 880, val loss: 0.9490390419960022
Epoch 890, training loss: 6.115686893463135 = 0.0358952060341835 + 1.0 * 6.07979154586792
Epoch 890, val loss: 0.9562129378318787
Epoch 900, training loss: 6.119311332702637 = 0.034660711884498596 + 1.0 * 6.08465051651001
Epoch 900, val loss: 0.9632545709609985
Epoch 910, training loss: 6.115978240966797 = 0.03349209204316139 + 1.0 * 6.082486152648926
Epoch 910, val loss: 0.9701139330863953
Epoch 920, training loss: 6.1097941398620605 = 0.0323823019862175 + 1.0 * 6.077411651611328
Epoch 920, val loss: 0.9769715666770935
Epoch 930, training loss: 6.107499122619629 = 0.0313279815018177 + 1.0 * 6.076170921325684
Epoch 930, val loss: 0.9838451147079468
Epoch 940, training loss: 6.116285800933838 = 0.030321599915623665 + 1.0 * 6.085964202880859
Epoch 940, val loss: 0.9905784130096436
Epoch 950, training loss: 6.105349063873291 = 0.029360927641391754 + 1.0 * 6.075988292694092
Epoch 950, val loss: 0.99718177318573
Epoch 960, training loss: 6.101946830749512 = 0.028451139107346535 + 1.0 * 6.073495864868164
Epoch 960, val loss: 1.0037983655929565
Epoch 970, training loss: 6.107870578765869 = 0.027575263753533363 + 1.0 * 6.080295085906982
Epoch 970, val loss: 1.0103247165679932
Epoch 980, training loss: 6.101914882659912 = 0.026746701449155807 + 1.0 * 6.075168132781982
Epoch 980, val loss: 1.0166763067245483
Epoch 990, training loss: 6.098254203796387 = 0.02595200017094612 + 1.0 * 6.072302341461182
Epoch 990, val loss: 1.0229980945587158
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.6974
Flip ASR: 0.6400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.310811996459961 = 1.9368959665298462 + 1.0 * 8.373915672302246
Epoch 0, val loss: 1.935257077217102
Epoch 10, training loss: 10.300167083740234 = 1.9265141487121582 + 1.0 * 8.373653411865234
Epoch 10, val loss: 1.92392897605896
Epoch 20, training loss: 10.285531997680664 = 1.9137651920318604 + 1.0 * 8.371767044067383
Epoch 20, val loss: 1.9099012613296509
Epoch 30, training loss: 10.252562522888184 = 1.8960707187652588 + 1.0 * 8.356492042541504
Epoch 30, val loss: 1.8904616832733154
Epoch 40, training loss: 10.112312316894531 = 1.8726476430892944 + 1.0 * 8.239665031433105
Epoch 40, val loss: 1.8655004501342773
Epoch 50, training loss: 9.50150203704834 = 1.8465920686721802 + 1.0 * 7.654909610748291
Epoch 50, val loss: 1.8387353420257568
Epoch 60, training loss: 9.064806938171387 = 1.82392418384552 + 1.0 * 7.240882396697998
Epoch 60, val loss: 1.8171753883361816
Epoch 70, training loss: 8.750164985656738 = 1.8086344003677368 + 1.0 * 6.941530704498291
Epoch 70, val loss: 1.8024903535842896
Epoch 80, training loss: 8.574251174926758 = 1.7923104763031006 + 1.0 * 6.781940937042236
Epoch 80, val loss: 1.7864936590194702
Epoch 90, training loss: 8.458931922912598 = 1.774891972541809 + 1.0 * 6.684040069580078
Epoch 90, val loss: 1.7707114219665527
Epoch 100, training loss: 8.360296249389648 = 1.756671667098999 + 1.0 * 6.60362434387207
Epoch 100, val loss: 1.7547441720962524
Epoch 110, training loss: 8.281941413879395 = 1.738406777381897 + 1.0 * 6.543534755706787
Epoch 110, val loss: 1.739029884338379
Epoch 120, training loss: 8.210853576660156 = 1.7186646461486816 + 1.0 * 6.492188453674316
Epoch 120, val loss: 1.7220529317855835
Epoch 130, training loss: 8.146610260009766 = 1.6960978507995605 + 1.0 * 6.450511932373047
Epoch 130, val loss: 1.7027627229690552
Epoch 140, training loss: 8.080810546875 = 1.6699258089065552 + 1.0 * 6.410884857177734
Epoch 140, val loss: 1.6806929111480713
Epoch 150, training loss: 8.018019676208496 = 1.6388124227523804 + 1.0 * 6.379207134246826
Epoch 150, val loss: 1.6547367572784424
Epoch 160, training loss: 7.953765392303467 = 1.6024655103683472 + 1.0 * 6.35129976272583
Epoch 160, val loss: 1.6247771978378296
Epoch 170, training loss: 7.888773441314697 = 1.5601242780685425 + 1.0 * 6.328649044036865
Epoch 170, val loss: 1.590001106262207
Epoch 180, training loss: 7.82052755355835 = 1.5108190774917603 + 1.0 * 6.309708595275879
Epoch 180, val loss: 1.549820065498352
Epoch 190, training loss: 7.752641201019287 = 1.4552497863769531 + 1.0 * 6.297391414642334
Epoch 190, val loss: 1.5051381587982178
Epoch 200, training loss: 7.676326751708984 = 1.3952348232269287 + 1.0 * 6.281091690063477
Epoch 200, val loss: 1.4573287963867188
Epoch 210, training loss: 7.600857734680176 = 1.3312257528305054 + 1.0 * 6.269631862640381
Epoch 210, val loss: 1.4069252014160156
Epoch 220, training loss: 7.528480052947998 = 1.2647422552108765 + 1.0 * 6.263737678527832
Epoch 220, val loss: 1.355445384979248
Epoch 230, training loss: 7.448859214782715 = 1.1990680694580078 + 1.0 * 6.249791145324707
Epoch 230, val loss: 1.3053687810897827
Epoch 240, training loss: 7.375521183013916 = 1.134907603263855 + 1.0 * 6.2406134605407715
Epoch 240, val loss: 1.2566063404083252
Epoch 250, training loss: 7.30423641204834 = 1.0724103450775146 + 1.0 * 6.231825828552246
Epoch 250, val loss: 1.2093974351882935
Epoch 260, training loss: 7.237290382385254 = 1.0124192237854004 + 1.0 * 6.2248711585998535
Epoch 260, val loss: 1.1643266677856445
Epoch 270, training loss: 7.175812244415283 = 0.955830454826355 + 1.0 * 6.219981670379639
Epoch 270, val loss: 1.1222268342971802
Epoch 280, training loss: 7.113569259643555 = 0.902709424495697 + 1.0 * 6.210859775543213
Epoch 280, val loss: 1.083235502243042
Epoch 290, training loss: 7.058281421661377 = 0.8523587584495544 + 1.0 * 6.205922603607178
Epoch 290, val loss: 1.04677414894104
Epoch 300, training loss: 7.0036163330078125 = 0.8051989078521729 + 1.0 * 6.1984171867370605
Epoch 300, val loss: 1.0129382610321045
Epoch 310, training loss: 6.954305648803711 = 0.7610164284706116 + 1.0 * 6.193289279937744
Epoch 310, val loss: 0.9817638397216797
Epoch 320, training loss: 6.907477378845215 = 0.7197890281677246 + 1.0 * 6.18768835067749
Epoch 320, val loss: 0.9531522393226624
Epoch 330, training loss: 6.865108013153076 = 0.6815516352653503 + 1.0 * 6.18355655670166
Epoch 330, val loss: 0.9272670745849609
Epoch 340, training loss: 6.823904991149902 = 0.6462239027023315 + 1.0 * 6.177680969238281
Epoch 340, val loss: 0.9039302468299866
Epoch 350, training loss: 6.786398410797119 = 0.6135104894638062 + 1.0 * 6.172887802124023
Epoch 350, val loss: 0.8829623460769653
Epoch 360, training loss: 6.75789737701416 = 0.5831315517425537 + 1.0 * 6.1747660636901855
Epoch 360, val loss: 0.8642452955245972
Epoch 370, training loss: 6.721806049346924 = 0.5550857782363892 + 1.0 * 6.166720390319824
Epoch 370, val loss: 0.8475992679595947
Epoch 380, training loss: 6.692151069641113 = 0.5288922786712646 + 1.0 * 6.1632585525512695
Epoch 380, val loss: 0.8329240679740906
Epoch 390, training loss: 6.662294387817383 = 0.5043475031852722 + 1.0 * 6.157947063446045
Epoch 390, val loss: 0.8198720812797546
Epoch 400, training loss: 6.638431549072266 = 0.48106855154037476 + 1.0 * 6.157362937927246
Epoch 400, val loss: 0.8083169460296631
Epoch 410, training loss: 6.609899044036865 = 0.45893430709838867 + 1.0 * 6.150964736938477
Epoch 410, val loss: 0.7981297373771667
Epoch 420, training loss: 6.584222793579102 = 0.43765988945961 + 1.0 * 6.1465630531311035
Epoch 420, val loss: 0.7892570495605469
Epoch 430, training loss: 6.567519187927246 = 0.41698479652404785 + 1.0 * 6.150534152984619
Epoch 430, val loss: 0.781497597694397
Epoch 440, training loss: 6.538582801818848 = 0.39703184366226196 + 1.0 * 6.1415510177612305
Epoch 440, val loss: 0.7749056816101074
Epoch 450, training loss: 6.514784812927246 = 0.37766584753990173 + 1.0 * 6.137118816375732
Epoch 450, val loss: 0.7695160508155823
Epoch 460, training loss: 6.493780136108398 = 0.35874515771865845 + 1.0 * 6.135035037994385
Epoch 460, val loss: 0.7650861144065857
Epoch 470, training loss: 6.47540283203125 = 0.3403228223323822 + 1.0 * 6.135079860687256
Epoch 470, val loss: 0.7614166736602783
Epoch 480, training loss: 6.453855514526367 = 0.3225167691707611 + 1.0 * 6.131338596343994
Epoch 480, val loss: 0.7586839199066162
Epoch 490, training loss: 6.432546615600586 = 0.3051821291446686 + 1.0 * 6.127364635467529
Epoch 490, val loss: 0.756870687007904
Epoch 500, training loss: 6.421863079071045 = 0.2882927656173706 + 1.0 * 6.133570194244385
Epoch 500, val loss: 0.7556421160697937
Epoch 510, training loss: 6.396536350250244 = 0.272113174200058 + 1.0 * 6.124423027038574
Epoch 510, val loss: 0.75503009557724
Epoch 520, training loss: 6.378674507141113 = 0.25654804706573486 + 1.0 * 6.122126579284668
Epoch 520, val loss: 0.7551205158233643
Epoch 530, training loss: 6.362055778503418 = 0.24161605536937714 + 1.0 * 6.120439529418945
Epoch 530, val loss: 0.7558059096336365
Epoch 540, training loss: 6.345455169677734 = 0.2274046391248703 + 1.0 * 6.118050575256348
Epoch 540, val loss: 0.7569562792778015
Epoch 550, training loss: 6.331740856170654 = 0.21401795744895935 + 1.0 * 6.117722988128662
Epoch 550, val loss: 0.7588725686073303
Epoch 560, training loss: 6.319231033325195 = 0.20140531659126282 + 1.0 * 6.117825508117676
Epoch 560, val loss: 0.7613670229911804
Epoch 570, training loss: 6.306400775909424 = 0.1895887702703476 + 1.0 * 6.116812229156494
Epoch 570, val loss: 0.7644296884536743
Epoch 580, training loss: 6.289983749389648 = 0.1785668581724167 + 1.0 * 6.111416816711426
Epoch 580, val loss: 0.767898440361023
Epoch 590, training loss: 6.291806697845459 = 0.16825473308563232 + 1.0 * 6.123551845550537
Epoch 590, val loss: 0.7718595862388611
Epoch 600, training loss: 6.268376350402832 = 0.15872958302497864 + 1.0 * 6.109646797180176
Epoch 600, val loss: 0.7763165831565857
Epoch 610, training loss: 6.256322383880615 = 0.149854376912117 + 1.0 * 6.106468200683594
Epoch 610, val loss: 0.7813363671302795
Epoch 620, training loss: 6.246064186096191 = 0.141574427485466 + 1.0 * 6.104489803314209
Epoch 620, val loss: 0.7866458296775818
Epoch 630, training loss: 6.251139163970947 = 0.1338578462600708 + 1.0 * 6.117281436920166
Epoch 630, val loss: 0.7922528982162476
Epoch 640, training loss: 6.228210926055908 = 0.12671306729316711 + 1.0 * 6.101497650146484
Epoch 640, val loss: 0.7981343269348145
Epoch 650, training loss: 6.220834255218506 = 0.12006539106369019 + 1.0 * 6.10076904296875
Epoch 650, val loss: 0.804471492767334
Epoch 660, training loss: 6.218077182769775 = 0.11385037004947662 + 1.0 * 6.104226589202881
Epoch 660, val loss: 0.8109264969825745
Epoch 670, training loss: 6.208784103393555 = 0.10809175670146942 + 1.0 * 6.100692272186279
Epoch 670, val loss: 0.8175917267799377
Epoch 680, training loss: 6.199160099029541 = 0.10270281136035919 + 1.0 * 6.096457481384277
Epoch 680, val loss: 0.8246200084686279
Epoch 690, training loss: 6.19309139251709 = 0.0976664200425148 + 1.0 * 6.095425128936768
Epoch 690, val loss: 0.8318187594413757
Epoch 700, training loss: 6.186673641204834 = 0.09296592324972153 + 1.0 * 6.09370756149292
Epoch 700, val loss: 0.8389941453933716
Epoch 710, training loss: 6.183454990386963 = 0.08858754485845566 + 1.0 * 6.09486722946167
Epoch 710, val loss: 0.8464547991752625
Epoch 720, training loss: 6.178061008453369 = 0.08448505401611328 + 1.0 * 6.093575954437256
Epoch 720, val loss: 0.8541589379310608
Epoch 730, training loss: 6.171817302703857 = 0.08063812553882599 + 1.0 * 6.091179370880127
Epoch 730, val loss: 0.8618406653404236
Epoch 740, training loss: 6.165749549865723 = 0.07702745497226715 + 1.0 * 6.088722229003906
Epoch 740, val loss: 0.8695895671844482
Epoch 750, training loss: 6.163866996765137 = 0.07362686097621918 + 1.0 * 6.090240001678467
Epoch 750, val loss: 0.8775262832641602
Epoch 760, training loss: 6.159089088439941 = 0.07043114304542542 + 1.0 * 6.088657855987549
Epoch 760, val loss: 0.8853638768196106
Epoch 770, training loss: 6.156494617462158 = 0.06742814928293228 + 1.0 * 6.089066505432129
Epoch 770, val loss: 0.8933902978897095
Epoch 780, training loss: 6.149885177612305 = 0.06460568308830261 + 1.0 * 6.08527946472168
Epoch 780, val loss: 0.9013426303863525
Epoch 790, training loss: 6.144559383392334 = 0.06194853037595749 + 1.0 * 6.082611083984375
Epoch 790, val loss: 0.9094258546829224
Epoch 800, training loss: 6.145045280456543 = 0.05943598225712776 + 1.0 * 6.085609436035156
Epoch 800, val loss: 0.9175912141799927
Epoch 810, training loss: 6.139764308929443 = 0.057061515748500824 + 1.0 * 6.08270263671875
Epoch 810, val loss: 0.9256263375282288
Epoch 820, training loss: 6.134917736053467 = 0.05481915548443794 + 1.0 * 6.080098628997803
Epoch 820, val loss: 0.9337024688720703
Epoch 830, training loss: 6.13083028793335 = 0.05269121006131172 + 1.0 * 6.078139305114746
Epoch 830, val loss: 0.9418846964836121
Epoch 840, training loss: 6.134274005889893 = 0.05066816881299019 + 1.0 * 6.083605766296387
Epoch 840, val loss: 0.9499503970146179
Epoch 850, training loss: 6.131049633026123 = 0.04875684157013893 + 1.0 * 6.082292556762695
Epoch 850, val loss: 0.9578555822372437
Epoch 860, training loss: 6.123558044433594 = 0.04694153368473053 + 1.0 * 6.076616287231445
Epoch 860, val loss: 0.9658666253089905
Epoch 870, training loss: 6.1210761070251465 = 0.045216575264930725 + 1.0 * 6.075859546661377
Epoch 870, val loss: 0.973895251750946
Epoch 880, training loss: 6.1200737953186035 = 0.043574586510658264 + 1.0 * 6.076498985290527
Epoch 880, val loss: 0.98179692029953
Epoch 890, training loss: 6.116920471191406 = 0.04201522096991539 + 1.0 * 6.0749053955078125
Epoch 890, val loss: 0.9897258877754211
Epoch 900, training loss: 6.117946624755859 = 0.040525883436203 + 1.0 * 6.077420711517334
Epoch 900, val loss: 0.9976675510406494
Epoch 910, training loss: 6.1120924949646 = 0.03911229223012924 + 1.0 * 6.0729804039001465
Epoch 910, val loss: 1.0053631067276
Epoch 920, training loss: 6.108240604400635 = 0.037765104323625565 + 1.0 * 6.0704755783081055
Epoch 920, val loss: 1.013344645500183
Epoch 930, training loss: 6.107956409454346 = 0.03647686541080475 + 1.0 * 6.071479320526123
Epoch 930, val loss: 1.0211573839187622
Epoch 940, training loss: 6.1062116622924805 = 0.03524375334382057 + 1.0 * 6.070967674255371
Epoch 940, val loss: 1.028774619102478
Epoch 950, training loss: 6.108091831207275 = 0.03406817466020584 + 1.0 * 6.074023723602295
Epoch 950, val loss: 1.036441683769226
Epoch 960, training loss: 6.101345062255859 = 0.0329459123313427 + 1.0 * 6.068398952484131
Epoch 960, val loss: 1.0441380739212036
Epoch 970, training loss: 6.098618507385254 = 0.03187589719891548 + 1.0 * 6.066742420196533
Epoch 970, val loss: 1.0519108772277832
Epoch 980, training loss: 6.095952987670898 = 0.030845068395137787 + 1.0 * 6.065107822418213
Epoch 980, val loss: 1.0594462156295776
Epoch 990, training loss: 6.098543167114258 = 0.029858041554689407 + 1.0 * 6.068685054779053
Epoch 990, val loss: 1.0668308734893799
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.7565
Flip ASR: 0.7156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.311247825622559 = 1.9373801946640015 + 1.0 * 8.373867988586426
Epoch 0, val loss: 1.932429552078247
Epoch 10, training loss: 10.299369812011719 = 1.926820158958435 + 1.0 * 8.372550010681152
Epoch 10, val loss: 1.9211480617523193
Epoch 20, training loss: 10.281585693359375 = 1.9140045642852783 + 1.0 * 8.367581367492676
Epoch 20, val loss: 1.9069907665252686
Epoch 30, training loss: 10.246491432189941 = 1.896335244178772 + 1.0 * 8.3501558303833
Epoch 30, val loss: 1.8870108127593994
Epoch 40, training loss: 10.11080265045166 = 1.873469591140747 + 1.0 * 8.237333297729492
Epoch 40, val loss: 1.8619189262390137
Epoch 50, training loss: 9.698688507080078 = 1.8509019613265991 + 1.0 * 7.847786903381348
Epoch 50, val loss: 1.8378825187683105
Epoch 60, training loss: 9.185340881347656 = 1.8304146528244019 + 1.0 * 7.354926586151123
Epoch 60, val loss: 1.8169162273406982
Epoch 70, training loss: 8.771947860717773 = 1.812479019165039 + 1.0 * 6.959469318389893
Epoch 70, val loss: 1.7987529039382935
Epoch 80, training loss: 8.528253555297852 = 1.7966312170028687 + 1.0 * 6.731622219085693
Epoch 80, val loss: 1.783382773399353
Epoch 90, training loss: 8.381612777709961 = 1.7805534601211548 + 1.0 * 6.6010589599609375
Epoch 90, val loss: 1.7682677507400513
Epoch 100, training loss: 8.286784172058105 = 1.763688325881958 + 1.0 * 6.523095607757568
Epoch 100, val loss: 1.7530115842819214
Epoch 110, training loss: 8.208145141601562 = 1.7467634677886963 + 1.0 * 6.461381435394287
Epoch 110, val loss: 1.7376521825790405
Epoch 120, training loss: 8.147621154785156 = 1.7287368774414062 + 1.0 * 6.418884754180908
Epoch 120, val loss: 1.7217129468917847
Epoch 130, training loss: 8.091168403625488 = 1.7088940143585205 + 1.0 * 6.382274150848389
Epoch 130, val loss: 1.704411506652832
Epoch 140, training loss: 8.038124084472656 = 1.6864285469055176 + 1.0 * 6.3516950607299805
Epoch 140, val loss: 1.6853123903274536
Epoch 150, training loss: 7.991486072540283 = 1.660814881324768 + 1.0 * 6.330671310424805
Epoch 150, val loss: 1.6642961502075195
Epoch 160, training loss: 7.937537670135498 = 1.6315973997116089 + 1.0 * 6.3059401512146
Epoch 160, val loss: 1.6409196853637695
Epoch 170, training loss: 7.885025978088379 = 1.5979745388031006 + 1.0 * 6.287051677703857
Epoch 170, val loss: 1.614383578300476
Epoch 180, training loss: 7.830216884613037 = 1.559327483177185 + 1.0 * 6.2708892822265625
Epoch 180, val loss: 1.5841801166534424
Epoch 190, training loss: 7.772280216217041 = 1.5153049230575562 + 1.0 * 6.256975173950195
Epoch 190, val loss: 1.5499988794326782
Epoch 200, training loss: 7.7108073234558105 = 1.4660487174987793 + 1.0 * 6.244758605957031
Epoch 200, val loss: 1.5123040676116943
Epoch 210, training loss: 7.656603813171387 = 1.412799596786499 + 1.0 * 6.243804454803467
Epoch 210, val loss: 1.4720033407211304
Epoch 220, training loss: 7.585300922393799 = 1.3587466478347778 + 1.0 * 6.2265543937683105
Epoch 220, val loss: 1.431667685508728
Epoch 230, training loss: 7.522101402282715 = 1.3045251369476318 + 1.0 * 6.217576026916504
Epoch 230, val loss: 1.3916679620742798
Epoch 240, training loss: 7.460498332977295 = 1.2506815195083618 + 1.0 * 6.209816932678223
Epoch 240, val loss: 1.35238516330719
Epoch 250, training loss: 7.4005446434021 = 1.197755217552185 + 1.0 * 6.202789306640625
Epoch 250, val loss: 1.3140745162963867
Epoch 260, training loss: 7.348182678222656 = 1.1460411548614502 + 1.0 * 6.202141284942627
Epoch 260, val loss: 1.2769107818603516
Epoch 270, training loss: 7.291596412658691 = 1.0968737602233887 + 1.0 * 6.194722652435303
Epoch 270, val loss: 1.2416828870773315
Epoch 280, training loss: 7.23569393157959 = 1.049981951713562 + 1.0 * 6.185711860656738
Epoch 280, val loss: 1.2080093622207642
Epoch 290, training loss: 7.186184883117676 = 1.0048205852508545 + 1.0 * 6.1813645362854
Epoch 290, val loss: 1.1752545833587646
Epoch 300, training loss: 7.137208938598633 = 0.9612782001495361 + 1.0 * 6.175930500030518
Epoch 300, val loss: 1.1433779001235962
Epoch 310, training loss: 7.091161251068115 = 0.9194826483726501 + 1.0 * 6.17167854309082
Epoch 310, val loss: 1.1126066446304321
Epoch 320, training loss: 7.045788764953613 = 0.878876805305481 + 1.0 * 6.166912078857422
Epoch 320, val loss: 1.0827863216400146
Epoch 330, training loss: 7.0064005851745605 = 0.8394737839698792 + 1.0 * 6.166926860809326
Epoch 330, val loss: 1.0537527799606323
Epoch 340, training loss: 6.963411331176758 = 0.8015084266662598 + 1.0 * 6.161902904510498
Epoch 340, val loss: 1.0255862474441528
Epoch 350, training loss: 6.922462463378906 = 0.7650850415229797 + 1.0 * 6.157377243041992
Epoch 350, val loss: 0.998690664768219
Epoch 360, training loss: 6.882905960083008 = 0.730369508266449 + 1.0 * 6.152536392211914
Epoch 360, val loss: 0.9733206033706665
Epoch 370, training loss: 6.84682035446167 = 0.6971222162246704 + 1.0 * 6.149698257446289
Epoch 370, val loss: 0.949309766292572
Epoch 380, training loss: 6.816747188568115 = 0.6654816269874573 + 1.0 * 6.151265621185303
Epoch 380, val loss: 0.9265643954277039
Epoch 390, training loss: 6.77754545211792 = 0.6354788541793823 + 1.0 * 6.142066478729248
Epoch 390, val loss: 0.9055129289627075
Epoch 400, training loss: 6.746445178985596 = 0.6069331169128418 + 1.0 * 6.139512062072754
Epoch 400, val loss: 0.8861386775970459
Epoch 410, training loss: 6.724686622619629 = 0.5797747373580933 + 1.0 * 6.144911766052246
Epoch 410, val loss: 0.8682528734207153
Epoch 420, training loss: 6.6904144287109375 = 0.5539844632148743 + 1.0 * 6.136429786682129
Epoch 420, val loss: 0.8519662618637085
Epoch 430, training loss: 6.667163372039795 = 0.5292876958847046 + 1.0 * 6.137875556945801
Epoch 430, val loss: 0.8371495008468628
Epoch 440, training loss: 6.635435104370117 = 0.5057249069213867 + 1.0 * 6.1297101974487305
Epoch 440, val loss: 0.8235688209533691
Epoch 450, training loss: 6.608067035675049 = 0.48295122385025024 + 1.0 * 6.125115871429443
Epoch 450, val loss: 0.8112534284591675
Epoch 460, training loss: 6.588780879974365 = 0.4608518183231354 + 1.0 * 6.127929210662842
Epoch 460, val loss: 0.7998050451278687
Epoch 470, training loss: 6.561864852905273 = 0.4394964277744293 + 1.0 * 6.122368335723877
Epoch 470, val loss: 0.7892600297927856
Epoch 480, training loss: 6.538112640380859 = 0.4186531901359558 + 1.0 * 6.119459629058838
Epoch 480, val loss: 0.7796322107315063
Epoch 490, training loss: 6.517524719238281 = 0.3983747363090515 + 1.0 * 6.119150161743164
Epoch 490, val loss: 0.7707023620605469
Epoch 500, training loss: 6.497023105621338 = 0.37866201996803284 + 1.0 * 6.118360996246338
Epoch 500, val loss: 0.7622458934783936
Epoch 510, training loss: 6.472406387329102 = 0.3596165180206299 + 1.0 * 6.112790107727051
Epoch 510, val loss: 0.7545704245567322
Epoch 520, training loss: 6.4517436027526855 = 0.34106048941612244 + 1.0 * 6.110682964324951
Epoch 520, val loss: 0.7475436925888062
Epoch 530, training loss: 6.436896800994873 = 0.3230522572994232 + 1.0 * 6.113844394683838
Epoch 530, val loss: 0.740875244140625
Epoch 540, training loss: 6.412820816040039 = 0.30569538474082947 + 1.0 * 6.107125282287598
Epoch 540, val loss: 0.7350139617919922
Epoch 550, training loss: 6.393555164337158 = 0.2889644503593445 + 1.0 * 6.104590892791748
Epoch 550, val loss: 0.729954183101654
Epoch 560, training loss: 6.376915454864502 = 0.27278807759284973 + 1.0 * 6.104127407073975
Epoch 560, val loss: 0.725561261177063
Epoch 570, training loss: 6.363866329193115 = 0.2572581171989441 + 1.0 * 6.1066083908081055
Epoch 570, val loss: 0.7217841744422913
Epoch 580, training loss: 6.342005729675293 = 0.2424364984035492 + 1.0 * 6.099569320678711
Epoch 580, val loss: 0.7187296748161316
Epoch 590, training loss: 6.327694892883301 = 0.22826899588108063 + 1.0 * 6.099425792694092
Epoch 590, val loss: 0.7165380120277405
Epoch 600, training loss: 6.316835403442383 = 0.21474207937717438 + 1.0 * 6.10209321975708
Epoch 600, val loss: 0.7150768637657166
Epoch 610, training loss: 6.296569347381592 = 0.20195230841636658 + 1.0 * 6.094616889953613
Epoch 610, val loss: 0.7143282294273376
Epoch 620, training loss: 6.286422252655029 = 0.189804807305336 + 1.0 * 6.096617221832275
Epoch 620, val loss: 0.7142952084541321
Epoch 630, training loss: 6.272378921508789 = 0.1783241629600525 + 1.0 * 6.094054698944092
Epoch 630, val loss: 0.7148982286453247
Epoch 640, training loss: 6.2606892585754395 = 0.16757214069366455 + 1.0 * 6.0931172370910645
Epoch 640, val loss: 0.7161766886711121
Epoch 650, training loss: 6.248305320739746 = 0.157520592212677 + 1.0 * 6.090784549713135
Epoch 650, val loss: 0.7181392312049866
Epoch 660, training loss: 6.2413225173950195 = 0.1481555849313736 + 1.0 * 6.093166828155518
Epoch 660, val loss: 0.7206254005432129
Epoch 670, training loss: 6.2322773933410645 = 0.1394585818052292 + 1.0 * 6.092818737030029
Epoch 670, val loss: 0.7235844135284424
Epoch 680, training loss: 6.220395088195801 = 0.13141989707946777 + 1.0 * 6.088974952697754
Epoch 680, val loss: 0.7268218994140625
Epoch 690, training loss: 6.209132194519043 = 0.12402669340372086 + 1.0 * 6.0851054191589355
Epoch 690, val loss: 0.7305659651756287
Epoch 700, training loss: 6.199244976043701 = 0.11714274436235428 + 1.0 * 6.082102298736572
Epoch 700, val loss: 0.734740674495697
Epoch 710, training loss: 6.1910600662231445 = 0.11074206233024597 + 1.0 * 6.080317974090576
Epoch 710, val loss: 0.7392863035202026
Epoch 720, training loss: 6.199742794036865 = 0.1047842800617218 + 1.0 * 6.094958305358887
Epoch 720, val loss: 0.7441229224205017
Epoch 730, training loss: 6.180294036865234 = 0.0992869958281517 + 1.0 * 6.08100700378418
Epoch 730, val loss: 0.7490106225013733
Epoch 740, training loss: 6.17325496673584 = 0.09417743235826492 + 1.0 * 6.07907772064209
Epoch 740, val loss: 0.7541982531547546
Epoch 750, training loss: 6.169693946838379 = 0.0894223228096962 + 1.0 * 6.0802717208862305
Epoch 750, val loss: 0.7595203518867493
Epoch 760, training loss: 6.161759853363037 = 0.08497488498687744 + 1.0 * 6.076785087585449
Epoch 760, val loss: 0.7649713754653931
Epoch 770, training loss: 6.15438985824585 = 0.08083336800336838 + 1.0 * 6.073556423187256
Epoch 770, val loss: 0.7706191539764404
Epoch 780, training loss: 6.156363487243652 = 0.07694463431835175 + 1.0 * 6.079418659210205
Epoch 780, val loss: 0.7763659954071045
Epoch 790, training loss: 6.150301933288574 = 0.07330016046762466 + 1.0 * 6.077001571655273
Epoch 790, val loss: 0.7820395827293396
Epoch 800, training loss: 6.140672206878662 = 0.06990984082221985 + 1.0 * 6.0707621574401855
Epoch 800, val loss: 0.7877727150917053
Epoch 810, training loss: 6.139989852905273 = 0.06672067940235138 + 1.0 * 6.073269367218018
Epoch 810, val loss: 0.793529748916626
Epoch 820, training loss: 6.134021759033203 = 0.06372591853141785 + 1.0 * 6.070295810699463
Epoch 820, val loss: 0.7992900013923645
Epoch 830, training loss: 6.1283769607543945 = 0.060917410999536514 + 1.0 * 6.067459583282471
Epoch 830, val loss: 0.8050416111946106
Epoch 840, training loss: 6.12726354598999 = 0.058265358209609985 + 1.0 * 6.068998336791992
Epoch 840, val loss: 0.8108715415000916
Epoch 850, training loss: 6.122701644897461 = 0.05576169863343239 + 1.0 * 6.066939830780029
Epoch 850, val loss: 0.8166336417198181
Epoch 860, training loss: 6.118712425231934 = 0.053396839648485184 + 1.0 * 6.0653157234191895
Epoch 860, val loss: 0.8223317861557007
Epoch 870, training loss: 6.119540691375732 = 0.051167428493499756 + 1.0 * 6.068373203277588
Epoch 870, val loss: 0.8280661702156067
Epoch 880, training loss: 6.114497661590576 = 0.0490754097700119 + 1.0 * 6.065422058105469
Epoch 880, val loss: 0.8337017297744751
Epoch 890, training loss: 6.109774112701416 = 0.04709470272064209 + 1.0 * 6.062679290771484
Epoch 890, val loss: 0.8392730355262756
Epoch 900, training loss: 6.109391689300537 = 0.04522475227713585 + 1.0 * 6.064167022705078
Epoch 900, val loss: 0.8448842763900757
Epoch 910, training loss: 6.104521751403809 = 0.043454188853502274 + 1.0 * 6.061067581176758
Epoch 910, val loss: 0.8504543900489807
Epoch 920, training loss: 6.107203483581543 = 0.041777871549129486 + 1.0 * 6.065425395965576
Epoch 920, val loss: 0.8559889793395996
Epoch 930, training loss: 6.098409175872803 = 0.04019668698310852 + 1.0 * 6.0582122802734375
Epoch 930, val loss: 0.8614708185195923
Epoch 940, training loss: 6.097659111022949 = 0.03869663551449776 + 1.0 * 6.058962345123291
Epoch 940, val loss: 0.8669480681419373
Epoch 950, training loss: 6.095388412475586 = 0.03727000951766968 + 1.0 * 6.0581183433532715
Epoch 950, val loss: 0.8723911643028259
Epoch 960, training loss: 6.0960493087768555 = 0.035918381065130234 + 1.0 * 6.060131072998047
Epoch 960, val loss: 0.8777375221252441
Epoch 970, training loss: 6.090019702911377 = 0.03463730588555336 + 1.0 * 6.055382251739502
Epoch 970, val loss: 0.8829563856124878
Epoch 980, training loss: 6.089141368865967 = 0.03342572599649429 + 1.0 * 6.055715560913086
Epoch 980, val loss: 0.8882167935371399
Epoch 990, training loss: 6.093183517456055 = 0.03227080777287483 + 1.0 * 6.060912609100342
Epoch 990, val loss: 0.8934404253959656
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.80935, 0.11901, Accuracy:0.80864, 0.02290
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10568])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00000, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.336536407470703 = 1.962660312652588 + 1.0 * 8.373876571655273
Epoch 0, val loss: 1.968150019645691
Epoch 10, training loss: 10.325462341308594 = 1.951944351196289 + 1.0 * 8.373517990112305
Epoch 10, val loss: 1.9580844640731812
Epoch 20, training loss: 10.309799194335938 = 1.9386239051818848 + 1.0 * 8.371174812316895
Epoch 20, val loss: 1.944915533065796
Epoch 30, training loss: 10.275094032287598 = 1.919867753982544 + 1.0 * 8.355226516723633
Epoch 30, val loss: 1.9257780313491821
Epoch 40, training loss: 10.160467147827148 = 1.8945670127868652 + 1.0 * 8.265900611877441
Epoch 40, val loss: 1.900694727897644
Epoch 50, training loss: 9.7464599609375 = 1.8672337532043457 + 1.0 * 7.8792266845703125
Epoch 50, val loss: 1.874397873878479
Epoch 60, training loss: 9.399628639221191 = 1.8430942296981812 + 1.0 * 7.556534290313721
Epoch 60, val loss: 1.8519532680511475
Epoch 70, training loss: 9.009492874145508 = 1.824910283088684 + 1.0 * 7.184582233428955
Epoch 70, val loss: 1.835018515586853
Epoch 80, training loss: 8.689902305603027 = 1.8105494976043701 + 1.0 * 6.879352569580078
Epoch 80, val loss: 1.8214219808578491
Epoch 90, training loss: 8.49008560180664 = 1.7952673435211182 + 1.0 * 6.694818496704102
Epoch 90, val loss: 1.8071260452270508
Epoch 100, training loss: 8.350545883178711 = 1.7778666019439697 + 1.0 * 6.572679042816162
Epoch 100, val loss: 1.7910635471343994
Epoch 110, training loss: 8.25449275970459 = 1.7604824304580688 + 1.0 * 6.4940104484558105
Epoch 110, val loss: 1.7746739387512207
Epoch 120, training loss: 8.189316749572754 = 1.7427412271499634 + 1.0 * 6.446575164794922
Epoch 120, val loss: 1.7578890323638916
Epoch 130, training loss: 8.133892059326172 = 1.723096489906311 + 1.0 * 6.410795211791992
Epoch 130, val loss: 1.7398391962051392
Epoch 140, training loss: 8.083466529846191 = 1.7008506059646606 + 1.0 * 6.382615566253662
Epoch 140, val loss: 1.7201809883117676
Epoch 150, training loss: 8.033679008483887 = 1.6754504442214966 + 1.0 * 6.3582282066345215
Epoch 150, val loss: 1.6982983350753784
Epoch 160, training loss: 7.983064651489258 = 1.6462664604187012 + 1.0 * 6.336798191070557
Epoch 160, val loss: 1.673553228378296
Epoch 170, training loss: 7.930876731872559 = 1.6120805740356445 + 1.0 * 6.318796157836914
Epoch 170, val loss: 1.6448439359664917
Epoch 180, training loss: 7.8748931884765625 = 1.5729954242706299 + 1.0 * 6.301898002624512
Epoch 180, val loss: 1.6123801469802856
Epoch 190, training loss: 7.816933631896973 = 1.5292890071868896 + 1.0 * 6.287644386291504
Epoch 190, val loss: 1.5766377449035645
Epoch 200, training loss: 7.755497455596924 = 1.4820040464401245 + 1.0 * 6.27349328994751
Epoch 200, val loss: 1.5387216806411743
Epoch 210, training loss: 7.693426609039307 = 1.4327524900436401 + 1.0 * 6.260673999786377
Epoch 210, val loss: 1.5003162622451782
Epoch 220, training loss: 7.634767055511475 = 1.3835316896438599 + 1.0 * 6.251235485076904
Epoch 220, val loss: 1.4631315469741821
Epoch 230, training loss: 7.575357437133789 = 1.3357722759246826 + 1.0 * 6.239584922790527
Epoch 230, val loss: 1.4280800819396973
Epoch 240, training loss: 7.519539833068848 = 1.2893216609954834 + 1.0 * 6.230218410491943
Epoch 240, val loss: 1.3948191404342651
Epoch 250, training loss: 7.468538761138916 = 1.2445558309555054 + 1.0 * 6.223982810974121
Epoch 250, val loss: 1.363389492034912
Epoch 260, training loss: 7.415903091430664 = 1.2010498046875 + 1.0 * 6.214853286743164
Epoch 260, val loss: 1.333195686340332
Epoch 270, training loss: 7.368518829345703 = 1.1587079763412476 + 1.0 * 6.209810733795166
Epoch 270, val loss: 1.303986668586731
Epoch 280, training loss: 7.318169116973877 = 1.116890549659729 + 1.0 * 6.2012786865234375
Epoch 280, val loss: 1.2749996185302734
Epoch 290, training loss: 7.269323348999023 = 1.0746610164642334 + 1.0 * 6.194662570953369
Epoch 290, val loss: 1.2455941438674927
Epoch 300, training loss: 7.230310916900635 = 1.0318070650100708 + 1.0 * 6.1985039710998535
Epoch 300, val loss: 1.2152750492095947
Epoch 310, training loss: 7.17474365234375 = 0.9892022609710693 + 1.0 * 6.185541152954102
Epoch 310, val loss: 1.1847779750823975
Epoch 320, training loss: 7.127007961273193 = 0.9465557932853699 + 1.0 * 6.180452346801758
Epoch 320, val loss: 1.1539298295974731
Epoch 330, training loss: 7.079670429229736 = 0.9038061499595642 + 1.0 * 6.175864219665527
Epoch 330, val loss: 1.1224912405014038
Epoch 340, training loss: 7.039811134338379 = 0.861343264579773 + 1.0 * 6.178467750549316
Epoch 340, val loss: 1.0908879041671753
Epoch 350, training loss: 6.989314079284668 = 0.8200929164886475 + 1.0 * 6.1692214012146
Epoch 350, val loss: 1.0599156618118286
Epoch 360, training loss: 6.945712089538574 = 0.7800621390342712 + 1.0 * 6.165649890899658
Epoch 360, val loss: 1.029744029045105
Epoch 370, training loss: 6.905575752258301 = 0.7415556311607361 + 1.0 * 6.16402006149292
Epoch 370, val loss: 1.0005780458450317
Epoch 380, training loss: 6.863719463348389 = 0.7047476768493652 + 1.0 * 6.158971786499023
Epoch 380, val loss: 0.9729572534561157
Epoch 390, training loss: 6.826112747192383 = 0.6694773435592651 + 1.0 * 6.156635284423828
Epoch 390, val loss: 0.9468607902526855
Epoch 400, training loss: 6.793169975280762 = 0.635923445224762 + 1.0 * 6.1572465896606445
Epoch 400, val loss: 0.9223456382751465
Epoch 410, training loss: 6.755305767059326 = 0.6039844751358032 + 1.0 * 6.1513214111328125
Epoch 410, val loss: 0.8995643258094788
Epoch 420, training loss: 6.721987247467041 = 0.5733426213264465 + 1.0 * 6.14864444732666
Epoch 420, val loss: 0.8782749176025391
Epoch 430, training loss: 6.6944451332092285 = 0.5440942645072937 + 1.0 * 6.150351047515869
Epoch 430, val loss: 0.8583719730377197
Epoch 440, training loss: 6.6610941886901855 = 0.5161305665969849 + 1.0 * 6.14496374130249
Epoch 440, val loss: 0.8398682475090027
Epoch 450, training loss: 6.629838466644287 = 0.4891038239002228 + 1.0 * 6.140734672546387
Epoch 450, val loss: 0.8224542140960693
Epoch 460, training loss: 6.612857341766357 = 0.46288377046585083 + 1.0 * 6.149973392486572
Epoch 460, val loss: 0.8059981465339661
Epoch 470, training loss: 6.579219818115234 = 0.43789342045783997 + 1.0 * 6.141326427459717
Epoch 470, val loss: 0.790680468082428
Epoch 480, training loss: 6.547900676727295 = 0.4138961732387543 + 1.0 * 6.134004592895508
Epoch 480, val loss: 0.776408314704895
Epoch 490, training loss: 6.523650169372559 = 0.39075419306755066 + 1.0 * 6.1328959465026855
Epoch 490, val loss: 0.7630096077919006
Epoch 500, training loss: 6.498579978942871 = 0.36853766441345215 + 1.0 * 6.130042552947998
Epoch 500, val loss: 0.7505554556846619
Epoch 510, training loss: 6.475511074066162 = 0.34724676609039307 + 1.0 * 6.128264427185059
Epoch 510, val loss: 0.7390594482421875
Epoch 520, training loss: 6.4586591720581055 = 0.3268454968929291 + 1.0 * 6.1318135261535645
Epoch 520, val loss: 0.7285166382789612
Epoch 530, training loss: 6.432718753814697 = 0.3074532449245453 + 1.0 * 6.125265598297119
Epoch 530, val loss: 0.7190342545509338
Epoch 540, training loss: 6.411894798278809 = 0.28900280594825745 + 1.0 * 6.122891902923584
Epoch 540, val loss: 0.7104557752609253
Epoch 550, training loss: 6.393597602844238 = 0.2714681327342987 + 1.0 * 6.122129440307617
Epoch 550, val loss: 0.7029701471328735
Epoch 560, training loss: 6.373139381408691 = 0.254871666431427 + 1.0 * 6.11826753616333
Epoch 560, val loss: 0.696524441242218
Epoch 570, training loss: 6.357916831970215 = 0.2390916645526886 + 1.0 * 6.1188249588012695
Epoch 570, val loss: 0.6910155415534973
Epoch 580, training loss: 6.344552516937256 = 0.22429604828357697 + 1.0 * 6.120256423950195
Epoch 580, val loss: 0.6865688562393188
Epoch 590, training loss: 6.32289457321167 = 0.2103859931230545 + 1.0 * 6.112508773803711
Epoch 590, val loss: 0.6828922033309937
Epoch 600, training loss: 6.306998252868652 = 0.19724583625793457 + 1.0 * 6.109752655029297
Epoch 600, val loss: 0.6801875233650208
Epoch 610, training loss: 6.293882846832275 = 0.18483616411685944 + 1.0 * 6.109046459197998
Epoch 610, val loss: 0.6784233450889587
Epoch 620, training loss: 6.2809529304504395 = 0.17325030267238617 + 1.0 * 6.107702732086182
Epoch 620, val loss: 0.677539050579071
Epoch 630, training loss: 6.2696638107299805 = 0.1624787449836731 + 1.0 * 6.107184886932373
Epoch 630, val loss: 0.6772580742835999
Epoch 640, training loss: 6.255849838256836 = 0.1524769514799118 + 1.0 * 6.103373050689697
Epoch 640, val loss: 0.6776887774467468
Epoch 650, training loss: 6.250302791595459 = 0.14318136870861053 + 1.0 * 6.107121467590332
Epoch 650, val loss: 0.6787626147270203
Epoch 660, training loss: 6.240428924560547 = 0.13461478054523468 + 1.0 * 6.105813980102539
Epoch 660, val loss: 0.6802787780761719
Epoch 670, training loss: 6.23234224319458 = 0.1267470121383667 + 1.0 * 6.105595111846924
Epoch 670, val loss: 0.6821802854537964
Epoch 680, training loss: 6.2165141105651855 = 0.1194969043135643 + 1.0 * 6.097017288208008
Epoch 680, val loss: 0.6843911409378052
Epoch 690, training loss: 6.207978248596191 = 0.11279629915952682 + 1.0 * 6.095181941986084
Epoch 690, val loss: 0.686862587928772
Epoch 700, training loss: 6.211027145385742 = 0.10660121589899063 + 1.0 * 6.10442590713501
Epoch 700, val loss: 0.6895946264266968
Epoch 710, training loss: 6.196447372436523 = 0.10093561559915543 + 1.0 * 6.0955119132995605
Epoch 710, val loss: 0.6925363540649414
Epoch 720, training loss: 6.189566135406494 = 0.09570835530757904 + 1.0 * 6.093857765197754
Epoch 720, val loss: 0.6954848170280457
Epoch 730, training loss: 6.1803998947143555 = 0.09085118025541306 + 1.0 * 6.089548587799072
Epoch 730, val loss: 0.6985860466957092
Epoch 740, training loss: 6.186337471008301 = 0.08633934706449509 + 1.0 * 6.0999979972839355
Epoch 740, val loss: 0.7018713355064392
Epoch 750, training loss: 6.172783374786377 = 0.08218501508235931 + 1.0 * 6.0905985832214355
Epoch 750, val loss: 0.7052225470542908
Epoch 760, training loss: 6.1645989418029785 = 0.07832091301679611 + 1.0 * 6.086277961730957
Epoch 760, val loss: 0.7085325121879578
Epoch 770, training loss: 6.15995979309082 = 0.07470536977052689 + 1.0 * 6.085254192352295
Epoch 770, val loss: 0.7119687795639038
Epoch 780, training loss: 6.1606574058532715 = 0.07132278382778168 + 1.0 * 6.089334487915039
Epoch 780, val loss: 0.7155440449714661
Epoch 790, training loss: 6.153748512268066 = 0.06817355751991272 + 1.0 * 6.085575103759766
Epoch 790, val loss: 0.7191310524940491
Epoch 800, training loss: 6.147799968719482 = 0.06521660834550858 + 1.0 * 6.082583427429199
Epoch 800, val loss: 0.7227430939674377
Epoch 810, training loss: 6.1436848640441895 = 0.062436673790216446 + 1.0 * 6.0812482833862305
Epoch 810, val loss: 0.7264517545700073
Epoch 820, training loss: 6.138391971588135 = 0.05982943996787071 + 1.0 * 6.0785627365112305
Epoch 820, val loss: 0.7302698493003845
Epoch 830, training loss: 6.13758659362793 = 0.05738170072436333 + 1.0 * 6.080204963684082
Epoch 830, val loss: 0.7339667677879333
Epoch 840, training loss: 6.130337715148926 = 0.05506652966141701 + 1.0 * 6.075271129608154
Epoch 840, val loss: 0.737709105014801
Epoch 850, training loss: 6.130573272705078 = 0.05287351459264755 + 1.0 * 6.077699661254883
Epoch 850, val loss: 0.7415509223937988
Epoch 860, training loss: 6.132324695587158 = 0.05082077533006668 + 1.0 * 6.081503868103027
Epoch 860, val loss: 0.745587170124054
Epoch 870, training loss: 6.121889114379883 = 0.04889017343521118 + 1.0 * 6.072999000549316
Epoch 870, val loss: 0.7493377327919006
Epoch 880, training loss: 6.118626594543457 = 0.04706016927957535 + 1.0 * 6.071566581726074
Epoch 880, val loss: 0.7530652284622192
Epoch 890, training loss: 6.116295337677002 = 0.04531829431653023 + 1.0 * 6.070977210998535
Epoch 890, val loss: 0.7569581270217896
Epoch 900, training loss: 6.120485305786133 = 0.04365910217165947 + 1.0 * 6.076826095581055
Epoch 900, val loss: 0.7609127163887024
Epoch 910, training loss: 6.120767116546631 = 0.04209847375750542 + 1.0 * 6.078668594360352
Epoch 910, val loss: 0.7650697827339172
Epoch 920, training loss: 6.108593463897705 = 0.040622778236866 + 1.0 * 6.0679707527160645
Epoch 920, val loss: 0.7689054012298584
Epoch 930, training loss: 6.107235431671143 = 0.03921549767255783 + 1.0 * 6.068019866943359
Epoch 930, val loss: 0.772716224193573
Epoch 940, training loss: 6.110442638397217 = 0.03787540644407272 + 1.0 * 6.072567462921143
Epoch 940, val loss: 0.7766615152359009
Epoch 950, training loss: 6.105019569396973 = 0.03659725934267044 + 1.0 * 6.068422317504883
Epoch 950, val loss: 0.7806500196456909
Epoch 960, training loss: 6.103337287902832 = 0.03538563475012779 + 1.0 * 6.067951679229736
Epoch 960, val loss: 0.7846271991729736
Epoch 970, training loss: 6.0996479988098145 = 0.03422614559531212 + 1.0 * 6.065422058105469
Epoch 970, val loss: 0.7885276079177856
Epoch 980, training loss: 6.096260070800781 = 0.03312121331691742 + 1.0 * 6.063138961791992
Epoch 980, val loss: 0.7924256920814514
Epoch 990, training loss: 6.105125427246094 = 0.03206687048077583 + 1.0 * 6.073058605194092
Epoch 990, val loss: 0.796474277973175
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.5756
Flip ASR: 0.4889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.306388854980469 = 1.932502269744873 + 1.0 * 8.373887062072754
Epoch 0, val loss: 1.9294227361679077
Epoch 10, training loss: 10.296154975891113 = 1.9226081371307373 + 1.0 * 8.373546600341797
Epoch 10, val loss: 1.9192428588867188
Epoch 20, training loss: 10.281296730041504 = 1.9101229906082153 + 1.0 * 8.371173858642578
Epoch 20, val loss: 1.9058477878570557
Epoch 30, training loss: 10.246196746826172 = 1.892531394958496 + 1.0 * 8.353665351867676
Epoch 30, val loss: 1.8865563869476318
Epoch 40, training loss: 10.105879783630371 = 1.8694285154342651 + 1.0 * 8.236451148986816
Epoch 40, val loss: 1.8624991178512573
Epoch 50, training loss: 9.552106857299805 = 1.8442482948303223 + 1.0 * 7.707858562469482
Epoch 50, val loss: 1.8372526168823242
Epoch 60, training loss: 9.07950210571289 = 1.8215223550796509 + 1.0 * 7.257979393005371
Epoch 60, val loss: 1.8164067268371582
Epoch 70, training loss: 8.756860733032227 = 1.8069360256195068 + 1.0 * 6.949924945831299
Epoch 70, val loss: 1.8031082153320312
Epoch 80, training loss: 8.593490600585938 = 1.7911947965621948 + 1.0 * 6.802296161651611
Epoch 80, val loss: 1.7878559827804565
Epoch 90, training loss: 8.446080207824707 = 1.7740824222564697 + 1.0 * 6.671997547149658
Epoch 90, val loss: 1.7712522745132446
Epoch 100, training loss: 8.346101760864258 = 1.756592869758606 + 1.0 * 6.589509010314941
Epoch 100, val loss: 1.7545866966247559
Epoch 110, training loss: 8.270243644714355 = 1.7381870746612549 + 1.0 * 6.5320563316345215
Epoch 110, val loss: 1.7373206615447998
Epoch 120, training loss: 8.195296287536621 = 1.7181153297424316 + 1.0 * 6.4771809577941895
Epoch 120, val loss: 1.719171166419983
Epoch 130, training loss: 8.127540588378906 = 1.6952852010726929 + 1.0 * 6.432255744934082
Epoch 130, val loss: 1.6991851329803467
Epoch 140, training loss: 8.065508842468262 = 1.6686933040618896 + 1.0 * 6.396815776824951
Epoch 140, val loss: 1.6763955354690552
Epoch 150, training loss: 8.000251770019531 = 1.6377716064453125 + 1.0 * 6.3624796867370605
Epoch 150, val loss: 1.650221347808838
Epoch 160, training loss: 7.9374566078186035 = 1.6010740995407104 + 1.0 * 6.3363823890686035
Epoch 160, val loss: 1.619263768196106
Epoch 170, training loss: 7.871402740478516 = 1.5587016344070435 + 1.0 * 6.312701225280762
Epoch 170, val loss: 1.5838371515274048
Epoch 180, training loss: 7.802927017211914 = 1.5105047225952148 + 1.0 * 6.292422294616699
Epoch 180, val loss: 1.5438613891601562
Epoch 190, training loss: 7.7342047691345215 = 1.4575861692428589 + 1.0 * 6.276618480682373
Epoch 190, val loss: 1.5006709098815918
Epoch 200, training loss: 7.66202449798584 = 1.4012494087219238 + 1.0 * 6.260775089263916
Epoch 200, val loss: 1.4555795192718506
Epoch 210, training loss: 7.593273162841797 = 1.342399001121521 + 1.0 * 6.250874042510986
Epoch 210, val loss: 1.4095582962036133
Epoch 220, training loss: 7.521891117095947 = 1.2829275131225586 + 1.0 * 6.238963603973389
Epoch 220, val loss: 1.3641833066940308
Epoch 230, training loss: 7.452254772186279 = 1.2233084440231323 + 1.0 * 6.228946208953857
Epoch 230, val loss: 1.319814920425415
Epoch 240, training loss: 7.391043663024902 = 1.164196491241455 + 1.0 * 6.226847171783447
Epoch 240, val loss: 1.2763676643371582
Epoch 250, training loss: 7.322548866271973 = 1.1069790124893188 + 1.0 * 6.215569972991943
Epoch 250, val loss: 1.23484468460083
Epoch 260, training loss: 7.25770378112793 = 1.0512961149215698 + 1.0 * 6.20640754699707
Epoch 260, val loss: 1.1945610046386719
Epoch 270, training loss: 7.197347640991211 = 0.9967726469039917 + 1.0 * 6.20057487487793
Epoch 270, val loss: 1.1549478769302368
Epoch 280, training loss: 7.13698673248291 = 0.943528413772583 + 1.0 * 6.193458557128906
Epoch 280, val loss: 1.1161651611328125
Epoch 290, training loss: 7.083878517150879 = 0.8920761346817017 + 1.0 * 6.191802501678467
Epoch 290, val loss: 1.078611969947815
Epoch 300, training loss: 7.026190757751465 = 0.843022882938385 + 1.0 * 6.183167934417725
Epoch 300, val loss: 1.0428240299224854
Epoch 310, training loss: 6.974184036254883 = 0.7962397336959839 + 1.0 * 6.177944183349609
Epoch 310, val loss: 1.008858561515808
Epoch 320, training loss: 6.9279985427856445 = 0.7524542212486267 + 1.0 * 6.175544261932373
Epoch 320, val loss: 0.9773502945899963
Epoch 330, training loss: 6.879886627197266 = 0.7119750380516052 + 1.0 * 6.167911529541016
Epoch 330, val loss: 0.9487332701683044
Epoch 340, training loss: 6.8373942375183105 = 0.6742590665817261 + 1.0 * 6.163135051727295
Epoch 340, val loss: 0.9227615594863892
Epoch 350, training loss: 6.798386573791504 = 0.6390019655227661 + 1.0 * 6.159384727478027
Epoch 350, val loss: 0.899239182472229
Epoch 360, training loss: 6.762925148010254 = 0.6063454151153564 + 1.0 * 6.156579494476318
Epoch 360, val loss: 0.8781914710998535
Epoch 370, training loss: 6.728466510772705 = 0.5759386420249939 + 1.0 * 6.152527809143066
Epoch 370, val loss: 0.8596925139427185
Epoch 380, training loss: 6.695038318634033 = 0.5469741821289062 + 1.0 * 6.148064136505127
Epoch 380, val loss: 0.8430493474006653
Epoch 390, training loss: 6.66916561126709 = 0.5191041827201843 + 1.0 * 6.15006160736084
Epoch 390, val loss: 0.8278284072875977
Epoch 400, training loss: 6.634449005126953 = 0.49231666326522827 + 1.0 * 6.14213228225708
Epoch 400, val loss: 0.8140510320663452
Epoch 410, training loss: 6.604721546173096 = 0.4661550521850586 + 1.0 * 6.138566493988037
Epoch 410, val loss: 0.8014824986457825
Epoch 420, training loss: 6.5792765617370605 = 0.4404664933681488 + 1.0 * 6.138810157775879
Epoch 420, val loss: 0.789842426776886
Epoch 430, training loss: 6.54877233505249 = 0.4152640998363495 + 1.0 * 6.133508205413818
Epoch 430, val loss: 0.7793183326721191
Epoch 440, training loss: 6.520782470703125 = 0.39058125019073486 + 1.0 * 6.13020133972168
Epoch 440, val loss: 0.769783079624176
Epoch 450, training loss: 6.50662088394165 = 0.3665063679218292 + 1.0 * 6.1401143074035645
Epoch 450, val loss: 0.7613439559936523
Epoch 460, training loss: 6.469727039337158 = 0.34355634450912476 + 1.0 * 6.126170635223389
Epoch 460, val loss: 0.7541253566741943
Epoch 470, training loss: 6.445995330810547 = 0.3215530812740326 + 1.0 * 6.124442100524902
Epoch 470, val loss: 0.7481831908226013
Epoch 480, training loss: 6.423736095428467 = 0.300601065158844 + 1.0 * 6.123135089874268
Epoch 480, val loss: 0.7432976365089417
Epoch 490, training loss: 6.400397777557373 = 0.28095582127571106 + 1.0 * 6.119441986083984
Epoch 490, val loss: 0.7394925951957703
Epoch 500, training loss: 6.380758285522461 = 0.2626410126686096 + 1.0 * 6.118117332458496
Epoch 500, val loss: 0.7369204759597778
Epoch 510, training loss: 6.3627519607543945 = 0.24562934041023254 + 1.0 * 6.117122650146484
Epoch 510, val loss: 0.7353371381759644
Epoch 520, training loss: 6.3498640060424805 = 0.2299504429101944 + 1.0 * 6.119913578033447
Epoch 520, val loss: 0.7345922589302063
Epoch 530, training loss: 6.328138828277588 = 0.21559308469295502 + 1.0 * 6.112545967102051
Epoch 530, val loss: 0.7347977161407471
Epoch 540, training loss: 6.311970233917236 = 0.20231841504573822 + 1.0 * 6.109652042388916
Epoch 540, val loss: 0.7358821034431458
Epoch 550, training loss: 6.303238868713379 = 0.19003358483314514 + 1.0 * 6.113205432891846
Epoch 550, val loss: 0.7376506328582764
Epoch 560, training loss: 6.293895244598389 = 0.1787169724702835 + 1.0 * 6.115178108215332
Epoch 560, val loss: 0.7400525212287903
Epoch 570, training loss: 6.278059005737305 = 0.16837087273597717 + 1.0 * 6.1096882820129395
Epoch 570, val loss: 0.7429297566413879
Epoch 580, training loss: 6.261682510375977 = 0.15882861614227295 + 1.0 * 6.102853775024414
Epoch 580, val loss: 0.7465059757232666
Epoch 590, training loss: 6.251269340515137 = 0.15002043545246124 + 1.0 * 6.101248741149902
Epoch 590, val loss: 0.7505791783332825
Epoch 600, training loss: 6.242412567138672 = 0.14190901815891266 + 1.0 * 6.100503444671631
Epoch 600, val loss: 0.7549377083778381
Epoch 610, training loss: 6.234529972076416 = 0.13449525833129883 + 1.0 * 6.100034713745117
Epoch 610, val loss: 0.7597986459732056
Epoch 620, training loss: 6.223729133605957 = 0.12764132022857666 + 1.0 * 6.09608793258667
Epoch 620, val loss: 0.7650249600410461
Epoch 630, training loss: 6.2162861824035645 = 0.12128531187772751 + 1.0 * 6.095000743865967
Epoch 630, val loss: 0.7704356908798218
Epoch 640, training loss: 6.2116851806640625 = 0.11539636552333832 + 1.0 * 6.096288681030273
Epoch 640, val loss: 0.7760313749313354
Epoch 650, training loss: 6.201920986175537 = 0.109961599111557 + 1.0 * 6.091959476470947
Epoch 650, val loss: 0.7818185091018677
Epoch 660, training loss: 6.1954145431518555 = 0.10489346832036972 + 1.0 * 6.090520858764648
Epoch 660, val loss: 0.7878736853599548
Epoch 670, training loss: 6.194721698760986 = 0.10014928877353668 + 1.0 * 6.0945725440979
Epoch 670, val loss: 0.7940201759338379
Epoch 680, training loss: 6.1917548179626465 = 0.09573812037706375 + 1.0 * 6.096016883850098
Epoch 680, val loss: 0.8000233769416809
Epoch 690, training loss: 6.179994583129883 = 0.09163133054971695 + 1.0 * 6.088363170623779
Epoch 690, val loss: 0.8063352108001709
Epoch 700, training loss: 6.172937870025635 = 0.08776122331619263 + 1.0 * 6.085176467895508
Epoch 700, val loss: 0.8127316832542419
Epoch 710, training loss: 6.1725029945373535 = 0.08409655094146729 + 1.0 * 6.088406562805176
Epoch 710, val loss: 0.8191459774971008
Epoch 720, training loss: 6.166988849639893 = 0.08065574616193771 + 1.0 * 6.086333274841309
Epoch 720, val loss: 0.8255754113197327
Epoch 730, training loss: 6.161571502685547 = 0.07740909606218338 + 1.0 * 6.08416223526001
Epoch 730, val loss: 0.8320888876914978
Epoch 740, training loss: 6.155735015869141 = 0.07434076070785522 + 1.0 * 6.081394195556641
Epoch 740, val loss: 0.8387592434883118
Epoch 750, training loss: 6.150748252868652 = 0.07140949368476868 + 1.0 * 6.079338550567627
Epoch 750, val loss: 0.8454070687294006
Epoch 760, training loss: 6.150325298309326 = 0.06861542910337448 + 1.0 * 6.081709861755371
Epoch 760, val loss: 0.8520978093147278
Epoch 770, training loss: 6.152286052703857 = 0.06596332788467407 + 1.0 * 6.086322784423828
Epoch 770, val loss: 0.8585873246192932
Epoch 780, training loss: 6.142786979675293 = 0.06345466524362564 + 1.0 * 6.07933235168457
Epoch 780, val loss: 0.8652705550193787
Epoch 790, training loss: 6.139882564544678 = 0.06106194481253624 + 1.0 * 6.078820705413818
Epoch 790, val loss: 0.8720078468322754
Epoch 800, training loss: 6.133646011352539 = 0.058771874755620956 + 1.0 * 6.074873924255371
Epoch 800, val loss: 0.8786660432815552
Epoch 810, training loss: 6.133820056915283 = 0.05657775327563286 + 1.0 * 6.077242374420166
Epoch 810, val loss: 0.8853772878646851
Epoch 820, training loss: 6.126809120178223 = 0.054486263543367386 + 1.0 * 6.072322845458984
Epoch 820, val loss: 0.8920890688896179
Epoch 830, training loss: 6.1248273849487305 = 0.05247655510902405 + 1.0 * 6.072350978851318
Epoch 830, val loss: 0.8989336490631104
Epoch 840, training loss: 6.130366802215576 = 0.05054399371147156 + 1.0 * 6.079823017120361
Epoch 840, val loss: 0.9056342244148254
Epoch 850, training loss: 6.121059417724609 = 0.04870149865746498 + 1.0 * 6.072358131408691
Epoch 850, val loss: 0.9122995734214783
Epoch 860, training loss: 6.114814281463623 = 0.0469316728413105 + 1.0 * 6.067882537841797
Epoch 860, val loss: 0.9190768003463745
Epoch 870, training loss: 6.115016460418701 = 0.04522509127855301 + 1.0 * 6.069791316986084
Epoch 870, val loss: 0.9258554577827454
Epoch 880, training loss: 6.113405704498291 = 0.04358810558915138 + 1.0 * 6.069817543029785
Epoch 880, val loss: 0.9324125647544861
Epoch 890, training loss: 6.110299587249756 = 0.04202120378613472 + 1.0 * 6.0682783126831055
Epoch 890, val loss: 0.9390360713005066
Epoch 900, training loss: 6.106730937957764 = 0.040519583970308304 + 1.0 * 6.066211223602295
Epoch 900, val loss: 0.9457156658172607
Epoch 910, training loss: 6.112706184387207 = 0.039064157754182816 + 1.0 * 6.073642253875732
Epoch 910, val loss: 0.9523493051528931
Epoch 920, training loss: 6.105506420135498 = 0.03767130523920059 + 1.0 * 6.067835330963135
Epoch 920, val loss: 0.9586819410324097
Epoch 930, training loss: 6.099385738372803 = 0.03633519634604454 + 1.0 * 6.063050746917725
Epoch 930, val loss: 0.9653109908103943
Epoch 940, training loss: 6.097984790802002 = 0.03505497798323631 + 1.0 * 6.062929630279541
Epoch 940, val loss: 0.9718954563140869
Epoch 950, training loss: 6.099237442016602 = 0.03383100777864456 + 1.0 * 6.065406322479248
Epoch 950, val loss: 0.978213369846344
Epoch 960, training loss: 6.092748165130615 = 0.0326690711081028 + 1.0 * 6.060079097747803
Epoch 960, val loss: 0.9846521615982056
Epoch 970, training loss: 6.090763092041016 = 0.031565673649311066 + 1.0 * 6.059197425842285
Epoch 970, val loss: 0.9911177754402161
Epoch 980, training loss: 6.089186191558838 = 0.03050828166306019 + 1.0 * 6.058677673339844
Epoch 980, val loss: 0.9975548386573792
Epoch 990, training loss: 6.097259521484375 = 0.029500503093004227 + 1.0 * 6.067759037017822
Epoch 990, val loss: 1.0037614107131958
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9373
Flip ASR: 0.9289/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.322859764099121 = 1.9489717483520508 + 1.0 * 8.37388801574707
Epoch 0, val loss: 1.9429258108139038
Epoch 10, training loss: 10.310434341430664 = 1.937663197517395 + 1.0 * 8.372771263122559
Epoch 10, val loss: 1.9306491613388062
Epoch 20, training loss: 10.29112434387207 = 1.9231669902801514 + 1.0 * 8.36795711517334
Epoch 20, val loss: 1.9137591123580933
Epoch 30, training loss: 10.258537292480469 = 1.9029066562652588 + 1.0 * 8.355630874633789
Epoch 30, val loss: 1.8890713453292847
Epoch 40, training loss: 10.131969451904297 = 1.8765721321105957 + 1.0 * 8.25539779663086
Epoch 40, val loss: 1.85771906375885
Epoch 50, training loss: 9.459508895874023 = 1.8504987955093384 + 1.0 * 7.609009742736816
Epoch 50, val loss: 1.828482747077942
Epoch 60, training loss: 9.22024917602539 = 1.8267247676849365 + 1.0 * 7.393524169921875
Epoch 60, val loss: 1.8054920434951782
Epoch 70, training loss: 8.98427963256836 = 1.8082913160324097 + 1.0 * 7.17598819732666
Epoch 70, val loss: 1.7890290021896362
Epoch 80, training loss: 8.7609281539917 = 1.7918457984924316 + 1.0 * 6.969082355499268
Epoch 80, val loss: 1.7751449346542358
Epoch 90, training loss: 8.555059432983398 = 1.7774525880813599 + 1.0 * 6.77760648727417
Epoch 90, val loss: 1.763543963432312
Epoch 100, training loss: 8.421930313110352 = 1.762938141822815 + 1.0 * 6.658991813659668
Epoch 100, val loss: 1.7518070936203003
Epoch 110, training loss: 8.315435409545898 = 1.7462472915649414 + 1.0 * 6.569188594818115
Epoch 110, val loss: 1.7382324934005737
Epoch 120, training loss: 8.23419189453125 = 1.7280148267745972 + 1.0 * 6.506176948547363
Epoch 120, val loss: 1.7237719297409058
Epoch 130, training loss: 8.167078018188477 = 1.7080224752426147 + 1.0 * 6.4590559005737305
Epoch 130, val loss: 1.7079532146453857
Epoch 140, training loss: 8.099811553955078 = 1.6854599714279175 + 1.0 * 6.414351463317871
Epoch 140, val loss: 1.6904488801956177
Epoch 150, training loss: 8.03626823425293 = 1.658877968788147 + 1.0 * 6.377390384674072
Epoch 150, val loss: 1.6697437763214111
Epoch 160, training loss: 7.973272323608398 = 1.6268296241760254 + 1.0 * 6.346442699432373
Epoch 160, val loss: 1.64518141746521
Epoch 170, training loss: 7.908533573150635 = 1.5890377759933472 + 1.0 * 6.319495677947998
Epoch 170, val loss: 1.6160752773284912
Epoch 180, training loss: 7.844838619232178 = 1.5450211763381958 + 1.0 * 6.2998175621032715
Epoch 180, val loss: 1.5828197002410889
Epoch 190, training loss: 7.776672840118408 = 1.4957990646362305 + 1.0 * 6.280873775482178
Epoch 190, val loss: 1.5459415912628174
Epoch 200, training loss: 7.711397171020508 = 1.4431666135787964 + 1.0 * 6.268230438232422
Epoch 200, val loss: 1.5072386264801025
Epoch 210, training loss: 7.643913269042969 = 1.3908309936523438 + 1.0 * 6.253082275390625
Epoch 210, val loss: 1.4698333740234375
Epoch 220, training loss: 7.5811567306518555 = 1.3404109477996826 + 1.0 * 6.240745544433594
Epoch 220, val loss: 1.4349554777145386
Epoch 230, training loss: 7.52470588684082 = 1.29367196559906 + 1.0 * 6.231033802032471
Epoch 230, val loss: 1.403969407081604
Epoch 240, training loss: 7.472199440002441 = 1.2519363164901733 + 1.0 * 6.2202630043029785
Epoch 240, val loss: 1.377221703529358
Epoch 250, training loss: 7.424504280090332 = 1.214142918586731 + 1.0 * 6.210361480712891
Epoch 250, val loss: 1.3536403179168701
Epoch 260, training loss: 7.381901264190674 = 1.1792683601379395 + 1.0 * 6.202632904052734
Epoch 260, val loss: 1.3322516679763794
Epoch 270, training loss: 7.339359760284424 = 1.1463947296142578 + 1.0 * 6.192965030670166
Epoch 270, val loss: 1.3121278285980225
Epoch 280, training loss: 7.2989044189453125 = 1.1142950057983398 + 1.0 * 6.184609413146973
Epoch 280, val loss: 1.2927085161209106
Epoch 290, training loss: 7.2602152824401855 = 1.082243800163269 + 1.0 * 6.177971363067627
Epoch 290, val loss: 1.273230791091919
Epoch 300, training loss: 7.230971336364746 = 1.0497149229049683 + 1.0 * 6.181256294250488
Epoch 300, val loss: 1.253262996673584
Epoch 310, training loss: 7.184328079223633 = 1.0167410373687744 + 1.0 * 6.1675872802734375
Epoch 310, val loss: 1.2326762676239014
Epoch 320, training loss: 7.1443328857421875 = 0.9828757047653198 + 1.0 * 6.161457061767578
Epoch 320, val loss: 1.2113032341003418
Epoch 330, training loss: 7.104521751403809 = 0.947981059551239 + 1.0 * 6.156540870666504
Epoch 330, val loss: 1.1890478134155273
Epoch 340, training loss: 7.066353797912598 = 0.9122222065925598 + 1.0 * 6.1541314125061035
Epoch 340, val loss: 1.1659363508224487
Epoch 350, training loss: 7.024904251098633 = 0.8763711452484131 + 1.0 * 6.148532867431641
Epoch 350, val loss: 1.1425247192382812
Epoch 360, training loss: 6.985093593597412 = 0.8405324816703796 + 1.0 * 6.144561290740967
Epoch 360, val loss: 1.11895751953125
Epoch 370, training loss: 6.94602632522583 = 0.8047356605529785 + 1.0 * 6.141290664672852
Epoch 370, val loss: 1.0953822135925293
Epoch 380, training loss: 6.9069390296936035 = 0.7694686651229858 + 1.0 * 6.137470245361328
Epoch 380, val loss: 1.0719588994979858
Epoch 390, training loss: 6.87083101272583 = 0.7349759340286255 + 1.0 * 6.135855197906494
Epoch 390, val loss: 1.0492548942565918
Epoch 400, training loss: 6.833502292633057 = 0.7010388970375061 + 1.0 * 6.132463455200195
Epoch 400, val loss: 1.0269107818603516
Epoch 410, training loss: 6.804044723510742 = 0.6675460338592529 + 1.0 * 6.13649845123291
Epoch 410, val loss: 1.0049188137054443
Epoch 420, training loss: 6.761914253234863 = 0.6347353458404541 + 1.0 * 6.127179145812988
Epoch 420, val loss: 0.9833580851554871
Epoch 430, training loss: 6.726914882659912 = 0.6024688482284546 + 1.0 * 6.124445915222168
Epoch 430, val loss: 0.9625269174575806
Epoch 440, training loss: 6.6962409019470215 = 0.5706619620323181 + 1.0 * 6.125578880310059
Epoch 440, val loss: 0.9420258402824402
Epoch 450, training loss: 6.667939186096191 = 0.539529025554657 + 1.0 * 6.128410339355469
Epoch 450, val loss: 0.9221522808074951
Epoch 460, training loss: 6.627687931060791 = 0.5092720985412598 + 1.0 * 6.118415832519531
Epoch 460, val loss: 0.9031933546066284
Epoch 470, training loss: 6.596860885620117 = 0.4799644947052002 + 1.0 * 6.116896152496338
Epoch 470, val loss: 0.8850601315498352
Epoch 480, training loss: 6.569474220275879 = 0.45152515172958374 + 1.0 * 6.11794900894165
Epoch 480, val loss: 0.8679568767547607
Epoch 490, training loss: 6.537686824798584 = 0.4243057668209076 + 1.0 * 6.1133809089660645
Epoch 490, val loss: 0.8520035147666931
Epoch 500, training loss: 6.507871150970459 = 0.39819973707199097 + 1.0 * 6.109671592712402
Epoch 500, val loss: 0.8375482559204102
Epoch 510, training loss: 6.482208728790283 = 0.3731115460395813 + 1.0 * 6.109097003936768
Epoch 510, val loss: 0.8242939710617065
Epoch 520, training loss: 6.458935737609863 = 0.34905555844306946 + 1.0 * 6.109879970550537
Epoch 520, val loss: 0.8124649524688721
Epoch 530, training loss: 6.44251823425293 = 0.3262265622615814 + 1.0 * 6.116291522979736
Epoch 530, val loss: 0.8019419312477112
Epoch 540, training loss: 6.410305023193359 = 0.30462944507598877 + 1.0 * 6.10567569732666
Epoch 540, val loss: 0.7931033372879028
Epoch 550, training loss: 6.385462760925293 = 0.2842393219470978 + 1.0 * 6.101223468780518
Epoch 550, val loss: 0.7855494618415833
Epoch 560, training loss: 6.365353584289551 = 0.2649688124656677 + 1.0 * 6.100384712219238
Epoch 560, val loss: 0.7793256640434265
Epoch 570, training loss: 6.352810382843018 = 0.24691733717918396 + 1.0 * 6.105893135070801
Epoch 570, val loss: 0.7743710875511169
Epoch 580, training loss: 6.331154823303223 = 0.23015856742858887 + 1.0 * 6.100996494293213
Epoch 580, val loss: 0.7707986831665039
Epoch 590, training loss: 6.3098907470703125 = 0.2146492302417755 + 1.0 * 6.095241546630859
Epoch 590, val loss: 0.7683212161064148
Epoch 600, training loss: 6.29523229598999 = 0.20027291774749756 + 1.0 * 6.094959259033203
Epoch 600, val loss: 0.7669159770011902
Epoch 610, training loss: 6.288021087646484 = 0.18707890808582306 + 1.0 * 6.100942134857178
Epoch 610, val loss: 0.7663745880126953
Epoch 620, training loss: 6.269103527069092 = 0.1750904619693756 + 1.0 * 6.094013214111328
Epoch 620, val loss: 0.7667689323425293
Epoch 630, training loss: 6.253949165344238 = 0.16407549381256104 + 1.0 * 6.089873790740967
Epoch 630, val loss: 0.7678894996643066
Epoch 640, training loss: 6.241455554962158 = 0.15394409000873566 + 1.0 * 6.0875115394592285
Epoch 640, val loss: 0.7696885466575623
Epoch 650, training loss: 6.252566814422607 = 0.144596129655838 + 1.0 * 6.107970714569092
Epoch 650, val loss: 0.7721188068389893
Epoch 660, training loss: 6.222311973571777 = 0.13613417744636536 + 1.0 * 6.086177825927734
Epoch 660, val loss: 0.7748090028762817
Epoch 670, training loss: 6.212000846862793 = 0.12835173308849335 + 1.0 * 6.083649158477783
Epoch 670, val loss: 0.7780709266662598
Epoch 680, training loss: 6.203579425811768 = 0.12116335332393646 + 1.0 * 6.08241605758667
Epoch 680, val loss: 0.7816148400306702
Epoch 690, training loss: 6.19612979888916 = 0.1145070344209671 + 1.0 * 6.08162260055542
Epoch 690, val loss: 0.7855186462402344
Epoch 700, training loss: 6.199230194091797 = 0.10834105312824249 + 1.0 * 6.090888977050781
Epoch 700, val loss: 0.7895787358283997
Epoch 710, training loss: 6.183745384216309 = 0.10263807326555252 + 1.0 * 6.081107139587402
Epoch 710, val loss: 0.7939236760139465
Epoch 720, training loss: 6.176126956939697 = 0.09736069291830063 + 1.0 * 6.078766345977783
Epoch 720, val loss: 0.7985016703605652
Epoch 730, training loss: 6.169183254241943 = 0.09244321286678314 + 1.0 * 6.076740264892578
Epoch 730, val loss: 0.8031646013259888
Epoch 740, training loss: 6.170398712158203 = 0.08784091472625732 + 1.0 * 6.082557678222656
Epoch 740, val loss: 0.8079881072044373
Epoch 750, training loss: 6.164261341094971 = 0.0835743397474289 + 1.0 * 6.080687046051025
Epoch 750, val loss: 0.8127568364143372
Epoch 760, training loss: 6.154293537139893 = 0.07959558069705963 + 1.0 * 6.074697971343994
Epoch 760, val loss: 0.8177509903907776
Epoch 770, training loss: 6.148134708404541 = 0.07586212456226349 + 1.0 * 6.072272777557373
Epoch 770, val loss: 0.8228034377098083
Epoch 780, training loss: 6.146612167358398 = 0.07235763221979141 + 1.0 * 6.074254512786865
Epoch 780, val loss: 0.8278641700744629
Epoch 790, training loss: 6.149027347564697 = 0.06907106935977936 + 1.0 * 6.0799560546875
Epoch 790, val loss: 0.8329204320907593
Epoch 800, training loss: 6.136392116546631 = 0.06599494814872742 + 1.0 * 6.07039737701416
Epoch 800, val loss: 0.8381177186965942
Epoch 810, training loss: 6.1317338943481445 = 0.06310462951660156 + 1.0 * 6.068629264831543
Epoch 810, val loss: 0.8433281779289246
Epoch 820, training loss: 6.131337642669678 = 0.06037919595837593 + 1.0 * 6.070958614349365
Epoch 820, val loss: 0.8485198020935059
Epoch 830, training loss: 6.128633499145508 = 0.05780638009309769 + 1.0 * 6.070827007293701
Epoch 830, val loss: 0.8537629246711731
Epoch 840, training loss: 6.122732162475586 = 0.05539640039205551 + 1.0 * 6.067335605621338
Epoch 840, val loss: 0.8588743209838867
Epoch 850, training loss: 6.119227409362793 = 0.053122662007808685 + 1.0 * 6.066104888916016
Epoch 850, val loss: 0.864173948764801
Epoch 860, training loss: 6.118061542510986 = 0.050970595329999924 + 1.0 * 6.06709098815918
Epoch 860, val loss: 0.8693366646766663
Epoch 870, training loss: 6.112781524658203 = 0.04893603175878525 + 1.0 * 6.063845634460449
Epoch 870, val loss: 0.8745395541191101
Epoch 880, training loss: 6.1110310554504395 = 0.047011882066726685 + 1.0 * 6.064019203186035
Epoch 880, val loss: 0.8797231912612915
Epoch 890, training loss: 6.109329700469971 = 0.04519347473978996 + 1.0 * 6.064136028289795
Epoch 890, val loss: 0.8848700523376465
Epoch 900, training loss: 6.115975856781006 = 0.04347069188952446 + 1.0 * 6.072504997253418
Epoch 900, val loss: 0.889983057975769
Epoch 910, training loss: 6.104072570800781 = 0.04185914993286133 + 1.0 * 6.06221342086792
Epoch 910, val loss: 0.8949366807937622
Epoch 920, training loss: 6.099040508270264 = 0.040322404354810715 + 1.0 * 6.058718204498291
Epoch 920, val loss: 0.9000592231750488
Epoch 930, training loss: 6.104555606842041 = 0.0388670489192009 + 1.0 * 6.065688610076904
Epoch 930, val loss: 0.9050734639167786
Epoch 940, training loss: 6.095029354095459 = 0.0374886654317379 + 1.0 * 6.0575408935546875
Epoch 940, val loss: 0.9099513292312622
Epoch 950, training loss: 6.0924811363220215 = 0.03617570921778679 + 1.0 * 6.056305408477783
Epoch 950, val loss: 0.9150189757347107
Epoch 960, training loss: 6.099011421203613 = 0.034923356026411057 + 1.0 * 6.064087867736816
Epoch 960, val loss: 0.9199894070625305
Epoch 970, training loss: 6.090409278869629 = 0.03374295309185982 + 1.0 * 6.056666374206543
Epoch 970, val loss: 0.9246629476547241
Epoch 980, training loss: 6.0872578620910645 = 0.032615773379802704 + 1.0 * 6.054642200469971
Epoch 980, val loss: 0.9296136498451233
Epoch 990, training loss: 6.085689544677734 = 0.03154284879565239 + 1.0 * 6.054146766662598
Epoch 990, val loss: 0.9344338774681091
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.3469
Flip ASR: 0.3422/225 nodes
The final ASR:0.61993, 0.24306, Accuracy:0.82099, 0.02058
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10600])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98155, 0.00301, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.327622413635254 = 1.9537080526351929 + 1.0 * 8.37391471862793
Epoch 0, val loss: 1.948388934135437
Epoch 10, training loss: 10.316229820251465 = 1.9425947666168213 + 1.0 * 8.373635292053223
Epoch 10, val loss: 1.938166618347168
Epoch 20, training loss: 10.30043888092041 = 1.928853988647461 + 1.0 * 8.37158489227295
Epoch 20, val loss: 1.925252079963684
Epoch 30, training loss: 10.264555931091309 = 1.9098161458969116 + 1.0 * 8.354740142822266
Epoch 30, val loss: 1.9070181846618652
Epoch 40, training loss: 10.110764503479004 = 1.8846238851547241 + 1.0 * 8.226140975952148
Epoch 40, val loss: 1.8833320140838623
Epoch 50, training loss: 9.619020462036133 = 1.8573498725891113 + 1.0 * 7.761670112609863
Epoch 50, val loss: 1.8585089445114136
Epoch 60, training loss: 9.299077033996582 = 1.8345693349838257 + 1.0 * 7.464507579803467
Epoch 60, val loss: 1.837889313697815
Epoch 70, training loss: 8.97219181060791 = 1.8152564764022827 + 1.0 * 7.156935214996338
Epoch 70, val loss: 1.821547031402588
Epoch 80, training loss: 8.650849342346191 = 1.7994798421859741 + 1.0 * 6.851369380950928
Epoch 80, val loss: 1.808815360069275
Epoch 90, training loss: 8.453390121459961 = 1.7814340591430664 + 1.0 * 6.6719560623168945
Epoch 90, val loss: 1.7938141822814941
Epoch 100, training loss: 8.313071250915527 = 1.761266827583313 + 1.0 * 6.551804065704346
Epoch 100, val loss: 1.7774224281311035
Epoch 110, training loss: 8.216015815734863 = 1.7412831783294678 + 1.0 * 6.474732875823975
Epoch 110, val loss: 1.7610760927200317
Epoch 120, training loss: 8.143216133117676 = 1.718919038772583 + 1.0 * 6.424296855926514
Epoch 120, val loss: 1.7425103187561035
Epoch 130, training loss: 8.081609725952148 = 1.6929287910461426 + 1.0 * 6.388681411743164
Epoch 130, val loss: 1.7209564447402954
Epoch 140, training loss: 8.019365310668945 = 1.6635264158248901 + 1.0 * 6.355839252471924
Epoch 140, val loss: 1.6964828968048096
Epoch 150, training loss: 7.95934534072876 = 1.6294206380844116 + 1.0 * 6.329924583435059
Epoch 150, val loss: 1.668267011642456
Epoch 160, training loss: 7.901688575744629 = 1.5897737741470337 + 1.0 * 6.311914920806885
Epoch 160, val loss: 1.635622262954712
Epoch 170, training loss: 7.835561752319336 = 1.5448483228683472 + 1.0 * 6.290713310241699
Epoch 170, val loss: 1.5988234281539917
Epoch 180, training loss: 7.770046234130859 = 1.4947952032089233 + 1.0 * 6.2752509117126465
Epoch 180, val loss: 1.5582219362258911
Epoch 190, training loss: 7.704456329345703 = 1.440807580947876 + 1.0 * 6.263648986816406
Epoch 190, val loss: 1.51499605178833
Epoch 200, training loss: 7.634708404541016 = 1.3847317695617676 + 1.0 * 6.249976634979248
Epoch 200, val loss: 1.4707424640655518
Epoch 210, training loss: 7.566608428955078 = 1.3275450468063354 + 1.0 * 6.239063262939453
Epoch 210, val loss: 1.426369071006775
Epoch 220, training loss: 7.504996299743652 = 1.2700289487838745 + 1.0 * 6.234967231750488
Epoch 220, val loss: 1.382673740386963
Epoch 230, training loss: 7.438290596008301 = 1.2146320343017578 + 1.0 * 6.223658561706543
Epoch 230, val loss: 1.3411741256713867
Epoch 240, training loss: 7.374420642852783 = 1.161376953125 + 1.0 * 6.213043689727783
Epoch 240, val loss: 1.3018690347671509
Epoch 250, training loss: 7.315887451171875 = 1.1099696159362793 + 1.0 * 6.205917835235596
Epoch 250, val loss: 1.2642985582351685
Epoch 260, training loss: 7.265637397766113 = 1.0607315301895142 + 1.0 * 6.204905986785889
Epoch 260, val loss: 1.2282928228378296
Epoch 270, training loss: 7.20728874206543 = 1.0139716863632202 + 1.0 * 6.19331693649292
Epoch 270, val loss: 1.1946734189987183
Epoch 280, training loss: 7.156322479248047 = 0.9689396023750305 + 1.0 * 6.187382698059082
Epoch 280, val loss: 1.1624033451080322
Epoch 290, training loss: 7.107696533203125 = 0.9251607656478882 + 1.0 * 6.182535648345947
Epoch 290, val loss: 1.1312240362167358
Epoch 300, training loss: 7.065337181091309 = 0.8827736377716064 + 1.0 * 6.182563304901123
Epoch 300, val loss: 1.1012039184570312
Epoch 310, training loss: 7.016948223114014 = 0.8420131802558899 + 1.0 * 6.1749348640441895
Epoch 310, val loss: 1.0727108716964722
Epoch 320, training loss: 6.971041202545166 = 0.8024516701698303 + 1.0 * 6.1685895919799805
Epoch 320, val loss: 1.0452136993408203
Epoch 330, training loss: 6.93316650390625 = 0.7640390396118164 + 1.0 * 6.169127464294434
Epoch 330, val loss: 1.0189120769500732
Epoch 340, training loss: 6.886775970458984 = 0.7270619869232178 + 1.0 * 6.1597137451171875
Epoch 340, val loss: 0.9939083456993103
Epoch 350, training loss: 6.847491264343262 = 0.6914036870002747 + 1.0 * 6.156087398529053
Epoch 350, val loss: 0.9702719449996948
Epoch 360, training loss: 6.816105842590332 = 0.6571325659751892 + 1.0 * 6.158973217010498
Epoch 360, val loss: 0.9479684233665466
Epoch 370, training loss: 6.774215221405029 = 0.6246222853660583 + 1.0 * 6.149592876434326
Epoch 370, val loss: 0.9273571968078613
Epoch 380, training loss: 6.739718914031982 = 0.5937412977218628 + 1.0 * 6.14597749710083
Epoch 380, val loss: 0.9083912372589111
Epoch 390, training loss: 6.708638668060303 = 0.5644088387489319 + 1.0 * 6.144229888916016
Epoch 390, val loss: 0.8910435438156128
Epoch 400, training loss: 6.676170825958252 = 0.5366565585136414 + 1.0 * 6.139514446258545
Epoch 400, val loss: 0.875353217124939
Epoch 410, training loss: 6.64602518081665 = 0.5103228688240051 + 1.0 * 6.135702133178711
Epoch 410, val loss: 0.8612370491027832
Epoch 420, training loss: 6.625946521759033 = 0.4852697253227234 + 1.0 * 6.140676975250244
Epoch 420, val loss: 0.8485668301582336
Epoch 430, training loss: 6.592606544494629 = 0.4615930914878845 + 1.0 * 6.1310133934021
Epoch 430, val loss: 0.8374428749084473
Epoch 440, training loss: 6.566359043121338 = 0.43918609619140625 + 1.0 * 6.127172946929932
Epoch 440, val loss: 0.8277834057807922
Epoch 450, training loss: 6.543796062469482 = 0.4177984893321991 + 1.0 * 6.125997543334961
Epoch 450, val loss: 0.8193975687026978
Epoch 460, training loss: 6.522425651550293 = 0.39744794368743896 + 1.0 * 6.1249775886535645
Epoch 460, val loss: 0.8122629523277283
Epoch 470, training loss: 6.499115943908691 = 0.378104031085968 + 1.0 * 6.121011734008789
Epoch 470, val loss: 0.8063970804214478
Epoch 480, training loss: 6.477231025695801 = 0.3595586121082306 + 1.0 * 6.117672443389893
Epoch 480, val loss: 0.8017207980155945
Epoch 490, training loss: 6.469851493835449 = 0.3417156934738159 + 1.0 * 6.128135681152344
Epoch 490, val loss: 0.7981067299842834
Epoch 500, training loss: 6.438920021057129 = 0.32465866208076477 + 1.0 * 6.114261150360107
Epoch 500, val loss: 0.7955418825149536
Epoch 510, training loss: 6.419745445251465 = 0.30827242136001587 + 1.0 * 6.111473083496094
Epoch 510, val loss: 0.7940451502799988
Epoch 520, training loss: 6.408487796783447 = 0.2925036549568176 + 1.0 * 6.115983963012695
Epoch 520, val loss: 0.793487548828125
Epoch 530, training loss: 6.387506484985352 = 0.2773786783218384 + 1.0 * 6.110127925872803
Epoch 530, val loss: 0.7939721941947937
Epoch 540, training loss: 6.3688507080078125 = 0.26288461685180664 + 1.0 * 6.105966091156006
Epoch 540, val loss: 0.7953185439109802
Epoch 550, training loss: 6.360668182373047 = 0.2489856481552124 + 1.0 * 6.111682415008545
Epoch 550, val loss: 0.7975448369979858
Epoch 560, training loss: 6.340571880340576 = 0.23575738072395325 + 1.0 * 6.104814529418945
Epoch 560, val loss: 0.8004232048988342
Epoch 570, training loss: 6.3262715339660645 = 0.22315600514411926 + 1.0 * 6.103115558624268
Epoch 570, val loss: 0.8040838241577148
Epoch 580, training loss: 6.314162254333496 = 0.21114559471607208 + 1.0 * 6.1030168533325195
Epoch 580, val loss: 0.8082787990570068
Epoch 590, training loss: 6.298312664031982 = 0.19972307980060577 + 1.0 * 6.0985894203186035
Epoch 590, val loss: 0.8129985928535461
Epoch 600, training loss: 6.284911155700684 = 0.18885566294193268 + 1.0 * 6.096055507659912
Epoch 600, val loss: 0.818125307559967
Epoch 610, training loss: 6.277686595916748 = 0.1784835159778595 + 1.0 * 6.099203109741211
Epoch 610, val loss: 0.8236487507820129
Epoch 620, training loss: 6.264706611633301 = 0.16863659024238586 + 1.0 * 6.096069812774658
Epoch 620, val loss: 0.8293545842170715
Epoch 630, training loss: 6.252856254577637 = 0.15930049121379852 + 1.0 * 6.093555927276611
Epoch 630, val loss: 0.8353738188743591
Epoch 640, training loss: 6.242541313171387 = 0.15044757723808289 + 1.0 * 6.0920939445495605
Epoch 640, val loss: 0.8415547609329224
Epoch 650, training loss: 6.236205577850342 = 0.14206893742084503 + 1.0 * 6.094136714935303
Epoch 650, val loss: 0.8478641510009766
Epoch 660, training loss: 6.224647045135498 = 0.13415703177452087 + 1.0 * 6.090489864349365
Epoch 660, val loss: 0.8543291091918945
Epoch 670, training loss: 6.21340274810791 = 0.12669220566749573 + 1.0 * 6.086710453033447
Epoch 670, val loss: 0.8609684109687805
Epoch 680, training loss: 6.2048468589782715 = 0.11965946853160858 + 1.0 * 6.0851874351501465
Epoch 680, val loss: 0.8677002191543579
Epoch 690, training loss: 6.203499794006348 = 0.1130513921380043 + 1.0 * 6.090448379516602
Epoch 690, val loss: 0.8744809031486511
Epoch 700, training loss: 6.1910552978515625 = 0.10687026381492615 + 1.0 * 6.0841851234436035
Epoch 700, val loss: 0.8812762498855591
Epoch 710, training loss: 6.18471622467041 = 0.1010887548327446 + 1.0 * 6.083627700805664
Epoch 710, val loss: 0.8881617784500122
Epoch 720, training loss: 6.176075458526611 = 0.09567718207836151 + 1.0 * 6.080398082733154
Epoch 720, val loss: 0.8951441645622253
Epoch 730, training loss: 6.170516490936279 = 0.09060871601104736 + 1.0 * 6.0799078941345215
Epoch 730, val loss: 0.9021573066711426
Epoch 740, training loss: 6.167398452758789 = 0.08587612211704254 + 1.0 * 6.081522464752197
Epoch 740, val loss: 0.9091339111328125
Epoch 750, training loss: 6.159623146057129 = 0.08145422488451004 + 1.0 * 6.078168869018555
Epoch 750, val loss: 0.916150689125061
Epoch 760, training loss: 6.155333518981934 = 0.0773385688662529 + 1.0 * 6.0779948234558105
Epoch 760, val loss: 0.9231274127960205
Epoch 770, training loss: 6.148801326751709 = 0.07348992675542831 + 1.0 * 6.075311183929443
Epoch 770, val loss: 0.9301499724388123
Epoch 780, training loss: 6.147817134857178 = 0.06989259272813797 + 1.0 * 6.077924728393555
Epoch 780, val loss: 0.9371289610862732
Epoch 790, training loss: 6.137820720672607 = 0.06653211265802383 + 1.0 * 6.071288585662842
Epoch 790, val loss: 0.9440609216690063
Epoch 800, training loss: 6.134649276733398 = 0.06338398903608322 + 1.0 * 6.07126522064209
Epoch 800, val loss: 0.9509625434875488
Epoch 810, training loss: 6.132869243621826 = 0.06043188273906708 + 1.0 * 6.072437286376953
Epoch 810, val loss: 0.9578222036361694
Epoch 820, training loss: 6.127920627593994 = 0.05765693634748459 + 1.0 * 6.070263862609863
Epoch 820, val loss: 0.964661180973053
Epoch 830, training loss: 6.123369216918945 = 0.055052123963832855 + 1.0 * 6.06831693649292
Epoch 830, val loss: 0.9714207649230957
Epoch 840, training loss: 6.121104717254639 = 0.0526076965034008 + 1.0 * 6.068497180938721
Epoch 840, val loss: 0.9781062602996826
Epoch 850, training loss: 6.115419864654541 = 0.05030858516693115 + 1.0 * 6.06511116027832
Epoch 850, val loss: 0.9847002029418945
Epoch 860, training loss: 6.120344638824463 = 0.04814204201102257 + 1.0 * 6.072202682495117
Epoch 860, val loss: 0.991279661655426
Epoch 870, training loss: 6.110492706298828 = 0.046103738248348236 + 1.0 * 6.064388751983643
Epoch 870, val loss: 0.997779130935669
Epoch 880, training loss: 6.1068549156188965 = 0.04418138787150383 + 1.0 * 6.062673568725586
Epoch 880, val loss: 1.0042765140533447
Epoch 890, training loss: 6.105490684509277 = 0.042361896485090256 + 1.0 * 6.06312894821167
Epoch 890, val loss: 1.0107183456420898
Epoch 900, training loss: 6.10865592956543 = 0.04064422473311424 + 1.0 * 6.06801176071167
Epoch 900, val loss: 1.0170570611953735
Epoch 910, training loss: 6.099647521972656 = 0.03902130946516991 + 1.0 * 6.060626029968262
Epoch 910, val loss: 1.0233696699142456
Epoch 920, training loss: 6.096197128295898 = 0.03749074786901474 + 1.0 * 6.058706283569336
Epoch 920, val loss: 1.0296446084976196
Epoch 930, training loss: 6.093880653381348 = 0.036037154495716095 + 1.0 * 6.0578436851501465
Epoch 930, val loss: 1.0358493328094482
Epoch 940, training loss: 6.101736545562744 = 0.03465694189071655 + 1.0 * 6.067079544067383
Epoch 940, val loss: 1.0420001745224
Epoch 950, training loss: 6.0891008377075195 = 0.03335639834403992 + 1.0 * 6.055744647979736
Epoch 950, val loss: 1.047974705696106
Epoch 960, training loss: 6.087857246398926 = 0.03212342783808708 + 1.0 * 6.055733680725098
Epoch 960, val loss: 1.0539708137512207
Epoch 970, training loss: 6.085503578186035 = 0.030951660126447678 + 1.0 * 6.05455207824707
Epoch 970, val loss: 1.0599066019058228
Epoch 980, training loss: 6.093751430511475 = 0.029837412759661674 + 1.0 * 6.063913822174072
Epoch 980, val loss: 1.0657541751861572
Epoch 990, training loss: 6.086371421813965 = 0.0287828017026186 + 1.0 * 6.057588577270508
Epoch 990, val loss: 1.0714722871780396
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8303
Flip ASR: 0.7956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.326216697692871 = 1.9523004293441772 + 1.0 * 8.373916625976562
Epoch 0, val loss: 1.9529744386672974
Epoch 10, training loss: 10.315529823303223 = 1.9418878555297852 + 1.0 * 8.373641967773438
Epoch 10, val loss: 1.9418818950653076
Epoch 20, training loss: 10.30105972290039 = 1.9293248653411865 + 1.0 * 8.371734619140625
Epoch 20, val loss: 1.9281905889511108
Epoch 30, training loss: 10.268817901611328 = 1.9121496677398682 + 1.0 * 8.356668472290039
Epoch 30, val loss: 1.909478783607483
Epoch 40, training loss: 10.149360656738281 = 1.8898147344589233 + 1.0 * 8.259546279907227
Epoch 40, val loss: 1.886271595954895
Epoch 50, training loss: 9.674381256103516 = 1.8683388233184814 + 1.0 * 7.806042671203613
Epoch 50, val loss: 1.8653199672698975
Epoch 60, training loss: 9.18679141998291 = 1.8498262166976929 + 1.0 * 7.336965084075928
Epoch 60, val loss: 1.8472853899002075
Epoch 70, training loss: 8.848383903503418 = 1.8333624601364136 + 1.0 * 7.015021800994873
Epoch 70, val loss: 1.8309168815612793
Epoch 80, training loss: 8.622161865234375 = 1.8157578706741333 + 1.0 * 6.806403636932373
Epoch 80, val loss: 1.8137097358703613
Epoch 90, training loss: 8.466875076293945 = 1.798405647277832 + 1.0 * 6.668468952178955
Epoch 90, val loss: 1.7968443632125854
Epoch 100, training loss: 8.349567413330078 = 1.780651569366455 + 1.0 * 6.568916320800781
Epoch 100, val loss: 1.7799700498580933
Epoch 110, training loss: 8.262513160705566 = 1.762383222579956 + 1.0 * 6.5001301765441895
Epoch 110, val loss: 1.7623708248138428
Epoch 120, training loss: 8.194852828979492 = 1.7432233095169067 + 1.0 * 6.451629638671875
Epoch 120, val loss: 1.7440000772476196
Epoch 130, training loss: 8.133772850036621 = 1.7226654291152954 + 1.0 * 6.411107540130615
Epoch 130, val loss: 1.7246770858764648
Epoch 140, training loss: 8.08100414276123 = 1.6997421979904175 + 1.0 * 6.381261825561523
Epoch 140, val loss: 1.7037626504898071
Epoch 150, training loss: 8.028605461120605 = 1.673409342765808 + 1.0 * 6.355196475982666
Epoch 150, val loss: 1.6803051233291626
Epoch 160, training loss: 7.976868152618408 = 1.6426995992660522 + 1.0 * 6.334168434143066
Epoch 160, val loss: 1.6534010171890259
Epoch 170, training loss: 7.923123359680176 = 1.6068308353424072 + 1.0 * 6.316292762756348
Epoch 170, val loss: 1.622438669204712
Epoch 180, training loss: 7.866516590118408 = 1.566097378730774 + 1.0 * 6.300419330596924
Epoch 180, val loss: 1.5876721143722534
Epoch 190, training loss: 7.8065619468688965 = 1.5202016830444336 + 1.0 * 6.286360263824463
Epoch 190, val loss: 1.5488981008529663
Epoch 200, training loss: 7.744460582733154 = 1.4697837829589844 + 1.0 * 6.27467679977417
Epoch 200, val loss: 1.5067218542099
Epoch 210, training loss: 7.6784162521362305 = 1.4160420894622803 + 1.0 * 6.262373924255371
Epoch 210, val loss: 1.4621810913085938
Epoch 220, training loss: 7.610566139221191 = 1.3598731756210327 + 1.0 * 6.250692844390869
Epoch 220, val loss: 1.4161897897720337
Epoch 230, training loss: 7.547679424285889 = 1.302538275718689 + 1.0 * 6.24514102935791
Epoch 230, val loss: 1.3701735734939575
Epoch 240, training loss: 7.478465557098389 = 1.2458381652832031 + 1.0 * 6.2326273918151855
Epoch 240, val loss: 1.3254040479660034
Epoch 250, training loss: 7.4126152992248535 = 1.1897664070129395 + 1.0 * 6.222848892211914
Epoch 250, val loss: 1.2817639112472534
Epoch 260, training loss: 7.353501319885254 = 1.134055256843567 + 1.0 * 6.219446182250977
Epoch 260, val loss: 1.238840937614441
Epoch 270, training loss: 7.290718078613281 = 1.079213261604309 + 1.0 * 6.211504936218262
Epoch 270, val loss: 1.1969566345214844
Epoch 280, training loss: 7.2278547286987305 = 1.0250622034072876 + 1.0 * 6.202792644500732
Epoch 280, val loss: 1.1559065580368042
Epoch 290, training loss: 7.167446613311768 = 0.97120201587677 + 1.0 * 6.196244716644287
Epoch 290, val loss: 1.115006446838379
Epoch 300, training loss: 7.119930744171143 = 0.9180165529251099 + 1.0 * 6.201914310455322
Epoch 300, val loss: 1.0746212005615234
Epoch 310, training loss: 7.054025650024414 = 0.8673872351646423 + 1.0 * 6.186638355255127
Epoch 310, val loss: 1.035919427871704
Epoch 320, training loss: 6.99991512298584 = 0.8191519379615784 + 1.0 * 6.180763244628906
Epoch 320, val loss: 0.9990326762199402
Epoch 330, training loss: 6.949000835418701 = 0.7735730409622192 + 1.0 * 6.1754279136657715
Epoch 330, val loss: 0.9642978310585022
Epoch 340, training loss: 6.908161640167236 = 0.7308339476585388 + 1.0 * 6.177327632904053
Epoch 340, val loss: 0.9320489764213562
Epoch 350, training loss: 6.859592437744141 = 0.6918461918830872 + 1.0 * 6.167746067047119
Epoch 350, val loss: 0.9028400778770447
Epoch 360, training loss: 6.819985389709473 = 0.6560025215148926 + 1.0 * 6.16398286819458
Epoch 360, val loss: 0.8766592741012573
Epoch 370, training loss: 6.782279968261719 = 0.622620701789856 + 1.0 * 6.159659385681152
Epoch 370, val loss: 0.8529850840568542
Epoch 380, training loss: 6.74900484085083 = 0.5912615060806274 + 1.0 * 6.157743453979492
Epoch 380, val loss: 0.8315386176109314
Epoch 390, training loss: 6.714940071105957 = 0.5617337226867676 + 1.0 * 6.1532063484191895
Epoch 390, val loss: 0.8121646642684937
Epoch 400, training loss: 6.686385154724121 = 0.5337030291557312 + 1.0 * 6.152682304382324
Epoch 400, val loss: 0.7946099638938904
Epoch 410, training loss: 6.654320240020752 = 0.5071504712104797 + 1.0 * 6.147169589996338
Epoch 410, val loss: 0.7787497043609619
Epoch 420, training loss: 6.62516450881958 = 0.4816683232784271 + 1.0 * 6.143496036529541
Epoch 420, val loss: 0.7643056511878967
Epoch 430, training loss: 6.597421646118164 = 0.45705732703208923 + 1.0 * 6.140364170074463
Epoch 430, val loss: 0.7512117028236389
Epoch 440, training loss: 6.575994968414307 = 0.43330416083335876 + 1.0 * 6.142690658569336
Epoch 440, val loss: 0.7394017577171326
Epoch 450, training loss: 6.553104877471924 = 0.4107063412666321 + 1.0 * 6.142398357391357
Epoch 450, val loss: 0.7288376688957214
Epoch 460, training loss: 6.522988796234131 = 0.3891879916191101 + 1.0 * 6.133800983428955
Epoch 460, val loss: 0.7196364402770996
Epoch 470, training loss: 6.498973846435547 = 0.36859914660453796 + 1.0 * 6.130374908447266
Epoch 470, val loss: 0.7116422653198242
Epoch 480, training loss: 6.479358673095703 = 0.3488633930683136 + 1.0 * 6.130495071411133
Epoch 480, val loss: 0.7048776745796204
Epoch 490, training loss: 6.457943916320801 = 0.3301410675048828 + 1.0 * 6.127802848815918
Epoch 490, val loss: 0.6991322040557861
Epoch 500, training loss: 6.437509059906006 = 0.31233784556388855 + 1.0 * 6.125171184539795
Epoch 500, val loss: 0.6946018934249878
Epoch 510, training loss: 6.418265342712402 = 0.2954042851924896 + 1.0 * 6.122860908508301
Epoch 510, val loss: 0.6910721659660339
Epoch 520, training loss: 6.40231466293335 = 0.27931058406829834 + 1.0 * 6.123003959655762
Epoch 520, val loss: 0.6886022686958313
Epoch 530, training loss: 6.3821821212768555 = 0.26408883929252625 + 1.0 * 6.118093490600586
Epoch 530, val loss: 0.6869252324104309
Epoch 540, training loss: 6.367993354797363 = 0.24969491362571716 + 1.0 * 6.118298530578613
Epoch 540, val loss: 0.6862101554870605
Epoch 550, training loss: 6.35178279876709 = 0.23611673712730408 + 1.0 * 6.115665912628174
Epoch 550, val loss: 0.6862340569496155
Epoch 560, training loss: 6.335561752319336 = 0.22326064109802246 + 1.0 * 6.112301349639893
Epoch 560, val loss: 0.6870418190956116
Epoch 570, training loss: 6.326338291168213 = 0.21114419400691986 + 1.0 * 6.115194320678711
Epoch 570, val loss: 0.688597559928894
Epoch 580, training loss: 6.3105692863464355 = 0.19980700314044952 + 1.0 * 6.110762119293213
Epoch 580, val loss: 0.6907062530517578
Epoch 590, training loss: 6.296299457550049 = 0.18914473056793213 + 1.0 * 6.107154846191406
Epoch 590, val loss: 0.6935238242149353
Epoch 600, training loss: 6.284080982208252 = 0.17909911274909973 + 1.0 * 6.104981899261475
Epoch 600, val loss: 0.6970228552818298
Epoch 610, training loss: 6.2777581214904785 = 0.1696687638759613 + 1.0 * 6.108089447021484
Epoch 610, val loss: 0.7010146975517273
Epoch 620, training loss: 6.265706539154053 = 0.1608867645263672 + 1.0 * 6.1048197746276855
Epoch 620, val loss: 0.705446720123291
Epoch 630, training loss: 6.254092216491699 = 0.15263615548610687 + 1.0 * 6.101456165313721
Epoch 630, val loss: 0.7103903889656067
Epoch 640, training loss: 6.254490852355957 = 0.14489613473415375 + 1.0 * 6.109594821929932
Epoch 640, val loss: 0.7157003879547119
Epoch 650, training loss: 6.237120151519775 = 0.13764820992946625 + 1.0 * 6.0994720458984375
Epoch 650, val loss: 0.7213725447654724
Epoch 660, training loss: 6.227155685424805 = 0.13083910942077637 + 1.0 * 6.096316814422607
Epoch 660, val loss: 0.7273849248886108
Epoch 670, training loss: 6.233034610748291 = 0.12444635480642319 + 1.0 * 6.108588218688965
Epoch 670, val loss: 0.7337113618850708
Epoch 680, training loss: 6.2139410972595215 = 0.11842414736747742 + 1.0 * 6.095517158508301
Epoch 680, val loss: 0.7402806282043457
Epoch 690, training loss: 6.205498695373535 = 0.11276476830244064 + 1.0 * 6.092733860015869
Epoch 690, val loss: 0.747105598449707
Epoch 700, training loss: 6.2027716636657715 = 0.10740745067596436 + 1.0 * 6.095364093780518
Epoch 700, val loss: 0.7542538046836853
Epoch 710, training loss: 6.19265604019165 = 0.10233994573354721 + 1.0 * 6.090316295623779
Epoch 710, val loss: 0.7614389061927795
Epoch 720, training loss: 6.1869659423828125 = 0.09754250198602676 + 1.0 * 6.089423656463623
Epoch 720, val loss: 0.7688639163970947
Epoch 730, training loss: 6.184439659118652 = 0.09297187626361847 + 1.0 * 6.09146785736084
Epoch 730, val loss: 0.7764793038368225
Epoch 740, training loss: 6.177332401275635 = 0.08862441778182983 + 1.0 * 6.08870792388916
Epoch 740, val loss: 0.7842437624931335
Epoch 750, training loss: 6.170155048370361 = 0.08448075503110886 + 1.0 * 6.085674285888672
Epoch 750, val loss: 0.7920755743980408
Epoch 760, training loss: 6.170820236206055 = 0.08052359521389008 + 1.0 * 6.090296745300293
Epoch 760, val loss: 0.8000969886779785
Epoch 770, training loss: 6.161500453948975 = 0.07676159590482712 + 1.0 * 6.084738731384277
Epoch 770, val loss: 0.8081114292144775
Epoch 780, training loss: 6.15520715713501 = 0.07315825670957565 + 1.0 * 6.0820488929748535
Epoch 780, val loss: 0.8162252306938171
Epoch 790, training loss: 6.152941703796387 = 0.06971952319145203 + 1.0 * 6.083222389221191
Epoch 790, val loss: 0.8244752287864685
Epoch 800, training loss: 6.149029731750488 = 0.06644154340028763 + 1.0 * 6.082588195800781
Epoch 800, val loss: 0.8327771425247192
Epoch 810, training loss: 6.147062301635742 = 0.06334486603736877 + 1.0 * 6.083717346191406
Epoch 810, val loss: 0.8411706686019897
Epoch 820, training loss: 6.142082691192627 = 0.06042017415165901 + 1.0 * 6.081662654876709
Epoch 820, val loss: 0.8495272994041443
Epoch 830, training loss: 6.137319564819336 = 0.05767141282558441 + 1.0 * 6.079648017883301
Epoch 830, val loss: 0.8579510450363159
Epoch 840, training loss: 6.131255149841309 = 0.05505538731813431 + 1.0 * 6.076199531555176
Epoch 840, val loss: 0.8664413690567017
Epoch 850, training loss: 6.134885311126709 = 0.052580852061510086 + 1.0 * 6.08230447769165
Epoch 850, val loss: 0.8749670386314392
Epoch 860, training loss: 6.127933979034424 = 0.05025036260485649 + 1.0 * 6.077683448791504
Epoch 860, val loss: 0.8834461569786072
Epoch 870, training loss: 6.130722522735596 = 0.0480533204972744 + 1.0 * 6.082669258117676
Epoch 870, val loss: 0.8919360041618347
Epoch 880, training loss: 6.120665550231934 = 0.04599911719560623 + 1.0 * 6.074666500091553
Epoch 880, val loss: 0.9002925157546997
Epoch 890, training loss: 6.117006778717041 = 0.04405961558222771 + 1.0 * 6.072947025299072
Epoch 890, val loss: 0.9086596965789795
Epoch 900, training loss: 6.113039970397949 = 0.042223162949085236 + 1.0 * 6.070816993713379
Epoch 900, val loss: 0.916998565196991
Epoch 910, training loss: 6.116951942443848 = 0.04048768803477287 + 1.0 * 6.0764641761779785
Epoch 910, val loss: 0.9252427816390991
Epoch 920, training loss: 6.114595413208008 = 0.03884724900126457 + 1.0 * 6.075747966766357
Epoch 920, val loss: 0.9333425164222717
Epoch 930, training loss: 6.108612060546875 = 0.03731352090835571 + 1.0 * 6.071298599243164
Epoch 930, val loss: 0.9414418935775757
Epoch 940, training loss: 6.109529972076416 = 0.03585944324731827 + 1.0 * 6.073670387268066
Epoch 940, val loss: 0.9494297504425049
Epoch 950, training loss: 6.103598594665527 = 0.03448561578989029 + 1.0 * 6.069112777709961
Epoch 950, val loss: 0.9571829438209534
Epoch 960, training loss: 6.101925849914551 = 0.03318900614976883 + 1.0 * 6.068737030029297
Epoch 960, val loss: 0.9650126099586487
Epoch 970, training loss: 6.1000590324401855 = 0.03195936605334282 + 1.0 * 6.068099498748779
Epoch 970, val loss: 0.9726698398590088
Epoch 980, training loss: 6.10316801071167 = 0.03079923428595066 + 1.0 * 6.072368621826172
Epoch 980, val loss: 0.9802084565162659
Epoch 990, training loss: 6.0949506759643555 = 0.029694361612200737 + 1.0 * 6.065256118774414
Epoch 990, val loss: 0.9876360893249512
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7011
Flip ASR: 0.6667/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.346549034118652 = 1.9726202487945557 + 1.0 * 8.373929023742676
Epoch 0, val loss: 1.978887677192688
Epoch 10, training loss: 10.334763526916504 = 1.9610906839370728 + 1.0 * 8.373672485351562
Epoch 10, val loss: 1.9667887687683105
Epoch 20, training loss: 10.318758010864258 = 1.9470582008361816 + 1.0 * 8.371700286865234
Epoch 20, val loss: 1.9516124725341797
Epoch 30, training loss: 10.284461975097656 = 1.927718997001648 + 1.0 * 8.356742858886719
Epoch 30, val loss: 1.930602788925171
Epoch 40, training loss: 10.165125846862793 = 1.9023641347885132 + 1.0 * 8.262762069702148
Epoch 40, val loss: 1.904306411743164
Epoch 50, training loss: 9.732154846191406 = 1.8756608963012695 + 1.0 * 7.856494426727295
Epoch 50, val loss: 1.8781923055648804
Epoch 60, training loss: 9.34028434753418 = 1.8543757200241089 + 1.0 * 7.4859089851379395
Epoch 60, val loss: 1.8592240810394287
Epoch 70, training loss: 9.01357650756836 = 1.8363405466079712 + 1.0 * 7.177236080169678
Epoch 70, val loss: 1.8423055410385132
Epoch 80, training loss: 8.741469383239746 = 1.8178801536560059 + 1.0 * 6.92358922958374
Epoch 80, val loss: 1.825015902519226
Epoch 90, training loss: 8.552196502685547 = 1.798621654510498 + 1.0 * 6.753575325012207
Epoch 90, val loss: 1.807207703590393
Epoch 100, training loss: 8.410526275634766 = 1.778889775276184 + 1.0 * 6.631636142730713
Epoch 100, val loss: 1.7890443801879883
Epoch 110, training loss: 8.310052871704102 = 1.7596254348754883 + 1.0 * 6.5504279136657715
Epoch 110, val loss: 1.7718019485473633
Epoch 120, training loss: 8.233055114746094 = 1.7395297288894653 + 1.0 * 6.49352502822876
Epoch 120, val loss: 1.7535912990570068
Epoch 130, training loss: 8.167804718017578 = 1.7175041437149048 + 1.0 * 6.450300693511963
Epoch 130, val loss: 1.7335695028305054
Epoch 140, training loss: 8.109769821166992 = 1.6935948133468628 + 1.0 * 6.41617488861084
Epoch 140, val loss: 1.7119083404541016
Epoch 150, training loss: 8.053460121154785 = 1.6671884059906006 + 1.0 * 6.386271953582764
Epoch 150, val loss: 1.6884512901306152
Epoch 160, training loss: 7.999312877655029 = 1.6373571157455444 + 1.0 * 6.361955642700195
Epoch 160, val loss: 1.662497878074646
Epoch 170, training loss: 7.943623065948486 = 1.6036087274551392 + 1.0 * 6.340014457702637
Epoch 170, val loss: 1.6335152387619019
Epoch 180, training loss: 7.888948440551758 = 1.5660043954849243 + 1.0 * 6.322944164276123
Epoch 180, val loss: 1.6015766859054565
Epoch 190, training loss: 7.828566551208496 = 1.5255615711212158 + 1.0 * 6.303005218505859
Epoch 190, val loss: 1.5675125122070312
Epoch 200, training loss: 7.768481254577637 = 1.4815905094146729 + 1.0 * 6.286890983581543
Epoch 200, val loss: 1.5308843851089478
Epoch 210, training loss: 7.705976486206055 = 1.4338200092315674 + 1.0 * 6.272156238555908
Epoch 210, val loss: 1.4915895462036133
Epoch 220, training loss: 7.650484085083008 = 1.3824032545089722 + 1.0 * 6.268080711364746
Epoch 220, val loss: 1.4499164819717407
Epoch 230, training loss: 7.580008029937744 = 1.3287601470947266 + 1.0 * 6.251247882843018
Epoch 230, val loss: 1.4069865942001343
Epoch 240, training loss: 7.512073516845703 = 1.2727348804473877 + 1.0 * 6.2393388748168945
Epoch 240, val loss: 1.3626705408096313
Epoch 250, training loss: 7.443883895874023 = 1.2139328718185425 + 1.0 * 6.229950904846191
Epoch 250, val loss: 1.3166555166244507
Epoch 260, training loss: 7.379920482635498 = 1.1538125276565552 + 1.0 * 6.226108074188232
Epoch 260, val loss: 1.270000696182251
Epoch 270, training loss: 7.309539794921875 = 1.0941011905670166 + 1.0 * 6.215438365936279
Epoch 270, val loss: 1.2239083051681519
Epoch 280, training loss: 7.242990493774414 = 1.0348248481750488 + 1.0 * 6.208165645599365
Epoch 280, val loss: 1.1782013177871704
Epoch 290, training loss: 7.178793430328369 = 0.976802408695221 + 1.0 * 6.201991081237793
Epoch 290, val loss: 1.1336263418197632
Epoch 300, training loss: 7.119126796722412 = 0.9213594198226929 + 1.0 * 6.19776725769043
Epoch 300, val loss: 1.091152310371399
Epoch 310, training loss: 7.061764240264893 = 0.8697806000709534 + 1.0 * 6.191983699798584
Epoch 310, val loss: 1.0519744157791138
Epoch 320, training loss: 7.007572650909424 = 0.8221107125282288 + 1.0 * 6.18546199798584
Epoch 320, val loss: 1.0162301063537598
Epoch 330, training loss: 6.958791255950928 = 0.7785449028015137 + 1.0 * 6.180246353149414
Epoch 330, val loss: 0.9840809106826782
Epoch 340, training loss: 6.915244102478027 = 0.7391988039016724 + 1.0 * 6.1760454177856445
Epoch 340, val loss: 0.9556537866592407
Epoch 350, training loss: 6.873463153839111 = 0.7031776309013367 + 1.0 * 6.170285701751709
Epoch 350, val loss: 0.9303063750267029
Epoch 360, training loss: 6.848082065582275 = 0.6699153184890747 + 1.0 * 6.17816686630249
Epoch 360, val loss: 0.9074731469154358
Epoch 370, training loss: 6.8045244216918945 = 0.6392790675163269 + 1.0 * 6.165245532989502
Epoch 370, val loss: 0.8869150280952454
Epoch 380, training loss: 6.768640041351318 = 0.6104713678359985 + 1.0 * 6.158168792724609
Epoch 380, val loss: 0.8683181405067444
Epoch 390, training loss: 6.737247467041016 = 0.58302903175354 + 1.0 * 6.1542181968688965
Epoch 390, val loss: 0.8512009978294373
Epoch 400, training loss: 6.713993549346924 = 0.556864857673645 + 1.0 * 6.157128810882568
Epoch 400, val loss: 0.8353843688964844
Epoch 410, training loss: 6.681432723999023 = 0.53200364112854 + 1.0 * 6.149428844451904
Epoch 410, val loss: 0.8209421634674072
Epoch 420, training loss: 6.652305603027344 = 0.5078946948051453 + 1.0 * 6.144411087036133
Epoch 420, val loss: 0.8076111674308777
Epoch 430, training loss: 6.6285319328308105 = 0.48444971442222595 + 1.0 * 6.144082069396973
Epoch 430, val loss: 0.7950868010520935
Epoch 440, training loss: 6.601805686950684 = 0.46181946992874146 + 1.0 * 6.139986038208008
Epoch 440, val loss: 0.78346186876297
Epoch 450, training loss: 6.576562404632568 = 0.4397268295288086 + 1.0 * 6.13683557510376
Epoch 450, val loss: 0.7728461027145386
Epoch 460, training loss: 6.550870895385742 = 0.4182972311973572 + 1.0 * 6.13257360458374
Epoch 460, val loss: 0.7628277540206909
Epoch 470, training loss: 6.528853893280029 = 0.3974657356739044 + 1.0 * 6.131388187408447
Epoch 470, val loss: 0.7536331415176392
Epoch 480, training loss: 6.505151748657227 = 0.3770761787891388 + 1.0 * 6.12807559967041
Epoch 480, val loss: 0.7451433539390564
Epoch 490, training loss: 6.4821062088012695 = 0.3571000099182129 + 1.0 * 6.125006198883057
Epoch 490, val loss: 0.7372358441352844
Epoch 500, training loss: 6.467541217803955 = 0.3376205861568451 + 1.0 * 6.129920482635498
Epoch 500, val loss: 0.7298588752746582
Epoch 510, training loss: 6.440755367279053 = 0.3188101649284363 + 1.0 * 6.121945381164551
Epoch 510, val loss: 0.7232138514518738
Epoch 520, training loss: 6.419198036193848 = 0.3006279766559601 + 1.0 * 6.118569850921631
Epoch 520, val loss: 0.7172664403915405
Epoch 530, training loss: 6.403460502624512 = 0.28306299448013306 + 1.0 * 6.120397567749023
Epoch 530, val loss: 0.7119488716125488
Epoch 540, training loss: 6.38702917098999 = 0.26636603474617004 + 1.0 * 6.120663166046143
Epoch 540, val loss: 0.7072243690490723
Epoch 550, training loss: 6.363274574279785 = 0.2504158616065979 + 1.0 * 6.112858772277832
Epoch 550, val loss: 0.7033804059028625
Epoch 560, training loss: 6.3530755043029785 = 0.2353062629699707 + 1.0 * 6.117769241333008
Epoch 560, val loss: 0.7003005146980286
Epoch 570, training loss: 6.335292816162109 = 0.2210918515920639 + 1.0 * 6.114201068878174
Epoch 570, val loss: 0.6978712677955627
Epoch 580, training loss: 6.3158745765686035 = 0.207758828997612 + 1.0 * 6.1081156730651855
Epoch 580, val loss: 0.6962699890136719
Epoch 590, training loss: 6.305233001708984 = 0.195205956697464 + 1.0 * 6.110026836395264
Epoch 590, val loss: 0.6954042315483093
Epoch 600, training loss: 6.2888898849487305 = 0.18349355459213257 + 1.0 * 6.105396270751953
Epoch 600, val loss: 0.695195198059082
Epoch 610, training loss: 6.276499271392822 = 0.17255738377571106 + 1.0 * 6.103941917419434
Epoch 610, val loss: 0.6956489086151123
Epoch 620, training loss: 6.263546466827393 = 0.16234441101551056 + 1.0 * 6.101202011108398
Epoch 620, val loss: 0.6967461705207825
Epoch 630, training loss: 6.259679317474365 = 0.15280136466026306 + 1.0 * 6.10687780380249
Epoch 630, val loss: 0.698386013507843
Epoch 640, training loss: 6.2483649253845215 = 0.14398165047168732 + 1.0 * 6.10438346862793
Epoch 640, val loss: 0.7005391120910645
Epoch 650, training loss: 6.233959674835205 = 0.13575269281864166 + 1.0 * 6.098206996917725
Epoch 650, val loss: 0.7032592296600342
Epoch 660, training loss: 6.2243571281433105 = 0.12808391451835632 + 1.0 * 6.096273422241211
Epoch 660, val loss: 0.7064502239227295
Epoch 670, training loss: 6.221638202667236 = 0.1209041029214859 + 1.0 * 6.100734233856201
Epoch 670, val loss: 0.7100262641906738
Epoch 680, training loss: 6.2104387283325195 = 0.1142728179693222 + 1.0 * 6.096166133880615
Epoch 680, val loss: 0.7140302062034607
Epoch 690, training loss: 6.200809955596924 = 0.10805489122867584 + 1.0 * 6.09275484085083
Epoch 690, val loss: 0.718363881111145
Epoch 700, training loss: 6.1927032470703125 = 0.10227099061012268 + 1.0 * 6.090432167053223
Epoch 700, val loss: 0.7230835556983948
Epoch 710, training loss: 6.195254802703857 = 0.09685879200696945 + 1.0 * 6.098395824432373
Epoch 710, val loss: 0.728033721446991
Epoch 720, training loss: 6.180180072784424 = 0.09182370454072952 + 1.0 * 6.0883564949035645
Epoch 720, val loss: 0.733190655708313
Epoch 730, training loss: 6.174716472625732 = 0.08712942153215408 + 1.0 * 6.087586879730225
Epoch 730, val loss: 0.738673210144043
Epoch 740, training loss: 6.174866676330566 = 0.0827353224158287 + 1.0 * 6.0921311378479
Epoch 740, val loss: 0.7443499565124512
Epoch 750, training loss: 6.162234783172607 = 0.07861737906932831 + 1.0 * 6.083617210388184
Epoch 750, val loss: 0.7501059770584106
Epoch 760, training loss: 6.157915115356445 = 0.07477470487356186 + 1.0 * 6.0831403732299805
Epoch 760, val loss: 0.7560697793960571
Epoch 770, training loss: 6.162667751312256 = 0.07117167860269547 + 1.0 * 6.091495990753174
Epoch 770, val loss: 0.762091338634491
Epoch 780, training loss: 6.150875091552734 = 0.06780890375375748 + 1.0 * 6.083065986633301
Epoch 780, val loss: 0.7683831453323364
Epoch 790, training loss: 6.1441426277160645 = 0.06463754922151566 + 1.0 * 6.07950496673584
Epoch 790, val loss: 0.7746827006340027
Epoch 800, training loss: 6.141783714294434 = 0.06165971979498863 + 1.0 * 6.0801239013671875
Epoch 800, val loss: 0.7811040878295898
Epoch 810, training loss: 6.137397766113281 = 0.058872804045677185 + 1.0 * 6.078525066375732
Epoch 810, val loss: 0.7874952554702759
Epoch 820, training loss: 6.133820056915283 = 0.056250348687171936 + 1.0 * 6.077569484710693
Epoch 820, val loss: 0.7939433455467224
Epoch 830, training loss: 6.129399299621582 = 0.05378703400492668 + 1.0 * 6.0756120681762695
Epoch 830, val loss: 0.8004378080368042
Epoch 840, training loss: 6.125001430511475 = 0.05146101117134094 + 1.0 * 6.073540210723877
Epoch 840, val loss: 0.8069791197776794
Epoch 850, training loss: 6.133080005645752 = 0.0492716059088707 + 1.0 * 6.083808422088623
Epoch 850, val loss: 0.8135448694229126
Epoch 860, training loss: 6.127264976501465 = 0.047203969210386276 + 1.0 * 6.080060958862305
Epoch 860, val loss: 0.8199042677879333
Epoch 870, training loss: 6.116182327270508 = 0.04527158662676811 + 1.0 * 6.070910930633545
Epoch 870, val loss: 0.8264239430427551
Epoch 880, training loss: 6.113683700561523 = 0.04344163089990616 + 1.0 * 6.070241928100586
Epoch 880, val loss: 0.8328716158866882
Epoch 890, training loss: 6.110665321350098 = 0.04170357808470726 + 1.0 * 6.0689616203308105
Epoch 890, val loss: 0.8392428755760193
Epoch 900, training loss: 6.11529541015625 = 0.04006386548280716 + 1.0 * 6.075231552124023
Epoch 900, val loss: 0.8454983234405518
Epoch 910, training loss: 6.107790946960449 = 0.03852534294128418 + 1.0 * 6.069265842437744
Epoch 910, val loss: 0.8517847061157227
Epoch 920, training loss: 6.10597562789917 = 0.037064626812934875 + 1.0 * 6.068911075592041
Epoch 920, val loss: 0.8580663204193115
Epoch 930, training loss: 6.102200508117676 = 0.03568117693066597 + 1.0 * 6.066519260406494
Epoch 930, val loss: 0.8641810417175293
Epoch 940, training loss: 6.099282741546631 = 0.03436781466007233 + 1.0 * 6.064914703369141
Epoch 940, val loss: 0.8703014254570007
Epoch 950, training loss: 6.1012349128723145 = 0.03311900794506073 + 1.0 * 6.068115711212158
Epoch 950, val loss: 0.8763236999511719
Epoch 960, training loss: 6.099168300628662 = 0.03193997964262962 + 1.0 * 6.067228317260742
Epoch 960, val loss: 0.8823720216751099
Epoch 970, training loss: 6.093996524810791 = 0.030823340639472008 + 1.0 * 6.063173294067383
Epoch 970, val loss: 0.8883174061775208
Epoch 980, training loss: 6.0919318199157715 = 0.02975541725754738 + 1.0 * 6.06217622756958
Epoch 980, val loss: 0.8942036628723145
Epoch 990, training loss: 6.102376461029053 = 0.02874307706952095 + 1.0 * 6.073633193969727
Epoch 990, val loss: 0.8999977111816406
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.8192
Flip ASR: 0.7911/225 nodes
The final ASR:0.78352, 0.05845, Accuracy:0.79012, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9550])
updated graph: torch.Size([2, 10636])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83210, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.327324867248535 = 1.9535118341445923 + 1.0 * 8.373812675476074
Epoch 0, val loss: 1.9502909183502197
Epoch 10, training loss: 10.316585540771484 = 1.9432554244995117 + 1.0 * 8.373330116271973
Epoch 10, val loss: 1.9408271312713623
Epoch 20, training loss: 10.300750732421875 = 1.9306660890579224 + 1.0 * 8.370084762573242
Epoch 20, val loss: 1.92892324924469
Epoch 30, training loss: 10.257622718811035 = 1.9136217832565308 + 1.0 * 8.344000816345215
Epoch 30, val loss: 1.9126408100128174
Epoch 40, training loss: 9.960977554321289 = 1.8932108879089355 + 1.0 * 8.067767143249512
Epoch 40, val loss: 1.8938419818878174
Epoch 50, training loss: 9.076394081115723 = 1.8739476203918457 + 1.0 * 7.202446460723877
Epoch 50, val loss: 1.8765541315078735
Epoch 60, training loss: 8.65025806427002 = 1.8621621131896973 + 1.0 * 6.788095951080322
Epoch 60, val loss: 1.8658901453018188
Epoch 70, training loss: 8.44566535949707 = 1.851203203201294 + 1.0 * 6.594461917877197
Epoch 70, val loss: 1.8559767007827759
Epoch 80, training loss: 8.34066104888916 = 1.8400202989578247 + 1.0 * 6.500640869140625
Epoch 80, val loss: 1.8459283113479614
Epoch 90, training loss: 8.257105827331543 = 1.8276349306106567 + 1.0 * 6.429471015930176
Epoch 90, val loss: 1.8350670337677002
Epoch 100, training loss: 8.189851760864258 = 1.8161803483963013 + 1.0 * 6.373671531677246
Epoch 100, val loss: 1.8249304294586182
Epoch 110, training loss: 8.135544776916504 = 1.8055498600006104 + 1.0 * 6.329995155334473
Epoch 110, val loss: 1.8154326677322388
Epoch 120, training loss: 8.091919898986816 = 1.7952367067337036 + 1.0 * 6.296682834625244
Epoch 120, val loss: 1.8058844804763794
Epoch 130, training loss: 8.05569076538086 = 1.7844467163085938 + 1.0 * 6.271243572235107
Epoch 130, val loss: 1.7958929538726807
Epoch 140, training loss: 8.023473739624023 = 1.7727537155151367 + 1.0 * 6.250720024108887
Epoch 140, val loss: 1.7852767705917358
Epoch 150, training loss: 7.9931793212890625 = 1.7597949504852295 + 1.0 * 6.233384609222412
Epoch 150, val loss: 1.773887038230896
Epoch 160, training loss: 7.962305545806885 = 1.7449626922607422 + 1.0 * 6.217342853546143
Epoch 160, val loss: 1.7612786293029785
Epoch 170, training loss: 7.931860446929932 = 1.7275794744491577 + 1.0 * 6.204280853271484
Epoch 170, val loss: 1.746910810470581
Epoch 180, training loss: 7.8990325927734375 = 1.7069518566131592 + 1.0 * 6.192080974578857
Epoch 180, val loss: 1.7301437854766846
Epoch 190, training loss: 7.863186359405518 = 1.68205726146698 + 1.0 * 6.181128978729248
Epoch 190, val loss: 1.7100552320480347
Epoch 200, training loss: 7.823553085327148 = 1.6516540050506592 + 1.0 * 6.171899318695068
Epoch 200, val loss: 1.6855610609054565
Epoch 210, training loss: 7.7828874588012695 = 1.6146385669708252 + 1.0 * 6.168249130249023
Epoch 210, val loss: 1.6556285619735718
Epoch 220, training loss: 7.729823589324951 = 1.5711336135864258 + 1.0 * 6.158689975738525
Epoch 220, val loss: 1.620267391204834
Epoch 230, training loss: 7.674006462097168 = 1.520815134048462 + 1.0 * 6.153191566467285
Epoch 230, val loss: 1.5792850255966187
Epoch 240, training loss: 7.615808963775635 = 1.4646459817886353 + 1.0 * 6.151163101196289
Epoch 240, val loss: 1.5336401462554932
Epoch 250, training loss: 7.5504608154296875 = 1.405227780342102 + 1.0 * 6.145233154296875
Epoch 250, val loss: 1.4851670265197754
Epoch 260, training loss: 7.485501289367676 = 1.3437931537628174 + 1.0 * 6.1417083740234375
Epoch 260, val loss: 1.4350627660751343
Epoch 270, training loss: 7.421976566314697 = 1.2825188636779785 + 1.0 * 6.139457702636719
Epoch 270, val loss: 1.3852616548538208
Epoch 280, training loss: 7.358782768249512 = 1.2231895923614502 + 1.0 * 6.135592937469482
Epoch 280, val loss: 1.3371686935424805
Epoch 290, training loss: 7.299528121948242 = 1.166774034500122 + 1.0 * 6.132754325866699
Epoch 290, val loss: 1.2921788692474365
Epoch 300, training loss: 7.242621898651123 = 1.1142643690109253 + 1.0 * 6.128357410430908
Epoch 300, val loss: 1.2506386041641235
Epoch 310, training loss: 7.188339710235596 = 1.0652523040771484 + 1.0 * 6.123087406158447
Epoch 310, val loss: 1.2124940156936646
Epoch 320, training loss: 7.1409454345703125 = 1.0192358493804932 + 1.0 * 6.121709823608398
Epoch 320, val loss: 1.1772544384002686
Epoch 330, training loss: 7.095608711242676 = 0.9760517477989197 + 1.0 * 6.119556903839111
Epoch 330, val loss: 1.1447546482086182
Epoch 340, training loss: 7.049114227294922 = 0.9351656436920166 + 1.0 * 6.113948822021484
Epoch 340, val loss: 1.1142727136611938
Epoch 350, training loss: 7.005594730377197 = 0.8950613141059875 + 1.0 * 6.110533237457275
Epoch 350, val loss: 1.0844707489013672
Epoch 360, training loss: 6.967772483825684 = 0.8549675941467285 + 1.0 * 6.112804889678955
Epoch 360, val loss: 1.0548983812332153
Epoch 370, training loss: 6.924515247344971 = 0.8155890107154846 + 1.0 * 6.108926296234131
Epoch 370, val loss: 1.0259145498275757
Epoch 380, training loss: 6.879982948303223 = 0.7765520811080933 + 1.0 * 6.10343074798584
Epoch 380, val loss: 0.9975035786628723
Epoch 390, training loss: 6.839065074920654 = 0.7374774813652039 + 1.0 * 6.101587772369385
Epoch 390, val loss: 0.9691886901855469
Epoch 400, training loss: 6.801255226135254 = 0.6987226009368896 + 1.0 * 6.102532863616943
Epoch 400, val loss: 0.9417293667793274
Epoch 410, training loss: 6.760489463806152 = 0.6614347100257874 + 1.0 * 6.09905481338501
Epoch 410, val loss: 0.9158946871757507
Epoch 420, training loss: 6.721589088439941 = 0.6259915232658386 + 1.0 * 6.095597743988037
Epoch 420, val loss: 0.8923311233520508
Epoch 430, training loss: 6.693983554840088 = 0.5923466682434082 + 1.0 * 6.10163688659668
Epoch 430, val loss: 0.8708113431930542
Epoch 440, training loss: 6.652660369873047 = 0.5609602928161621 + 1.0 * 6.091700077056885
Epoch 440, val loss: 0.85205078125
Epoch 450, training loss: 6.6212615966796875 = 0.531707763671875 + 1.0 * 6.0895538330078125
Epoch 450, val loss: 0.835846483707428
Epoch 460, training loss: 6.592937469482422 = 0.5042927265167236 + 1.0 * 6.088644981384277
Epoch 460, val loss: 0.821770429611206
Epoch 470, training loss: 6.565439224243164 = 0.4787440001964569 + 1.0 * 6.086695194244385
Epoch 470, val loss: 0.8098307251930237
Epoch 480, training loss: 6.542312145233154 = 0.4550509452819824 + 1.0 * 6.087261199951172
Epoch 480, val loss: 0.7997646331787109
Epoch 490, training loss: 6.516134262084961 = 0.4328296482563019 + 1.0 * 6.083304405212402
Epoch 490, val loss: 0.7911563515663147
Epoch 500, training loss: 6.49783182144165 = 0.4118025600910187 + 1.0 * 6.086029052734375
Epoch 500, val loss: 0.7837851643562317
Epoch 510, training loss: 6.473966598510742 = 0.39204713702201843 + 1.0 * 6.0819196701049805
Epoch 510, val loss: 0.7775681018829346
Epoch 520, training loss: 6.4527764320373535 = 0.37327486276626587 + 1.0 * 6.079501628875732
Epoch 520, val loss: 0.7723109722137451
Epoch 530, training loss: 6.432887554168701 = 0.35524675250053406 + 1.0 * 6.077641010284424
Epoch 530, val loss: 0.7677707076072693
Epoch 540, training loss: 6.4164862632751465 = 0.33792561292648315 + 1.0 * 6.078560829162598
Epoch 540, val loss: 0.7639172077178955
Epoch 550, training loss: 6.3966450691223145 = 0.32138368487358093 + 1.0 * 6.07526159286499
Epoch 550, val loss: 0.7608386278152466
Epoch 560, training loss: 6.378488540649414 = 0.3053930401802063 + 1.0 * 6.073095321655273
Epoch 560, val loss: 0.7582705616950989
Epoch 570, training loss: 6.361608028411865 = 0.2898479700088501 + 1.0 * 6.071760177612305
Epoch 570, val loss: 0.7561879754066467
Epoch 580, training loss: 6.348625659942627 = 0.2747293710708618 + 1.0 * 6.073896408081055
Epoch 580, val loss: 0.7545996308326721
Epoch 590, training loss: 6.339894771575928 = 0.2601603865623474 + 1.0 * 6.0797343254089355
Epoch 590, val loss: 0.7532874345779419
Epoch 600, training loss: 6.317161560058594 = 0.2461666613817215 + 1.0 * 6.070994853973389
Epoch 600, val loss: 0.7524418234825134
Epoch 610, training loss: 6.3009934425354 = 0.23268742859363556 + 1.0 * 6.068305969238281
Epoch 610, val loss: 0.7518959045410156
Epoch 620, training loss: 6.286232948303223 = 0.2196807712316513 + 1.0 * 6.06655216217041
Epoch 620, val loss: 0.7517004013061523
Epoch 630, training loss: 6.276115417480469 = 0.20720328390598297 + 1.0 * 6.068912029266357
Epoch 630, val loss: 0.751865565776825
Epoch 640, training loss: 6.259323596954346 = 0.19534814357757568 + 1.0 * 6.0639753341674805
Epoch 640, val loss: 0.7524203658103943
Epoch 650, training loss: 6.248424053192139 = 0.18404479324817657 + 1.0 * 6.0643792152404785
Epoch 650, val loss: 0.7533233165740967
Epoch 660, training loss: 6.237406253814697 = 0.1732873171567917 + 1.0 * 6.0641188621521
Epoch 660, val loss: 0.7545585632324219
Epoch 670, training loss: 6.227017402648926 = 0.1631380319595337 + 1.0 * 6.063879489898682
Epoch 670, val loss: 0.7561109066009521
Epoch 680, training loss: 6.215960502624512 = 0.15360215306282043 + 1.0 * 6.062358379364014
Epoch 680, val loss: 0.7578953504562378
Epoch 690, training loss: 6.204005718231201 = 0.14462731778621674 + 1.0 * 6.059378623962402
Epoch 690, val loss: 0.7601043581962585
Epoch 700, training loss: 6.194839000701904 = 0.13617122173309326 + 1.0 * 6.0586676597595215
Epoch 700, val loss: 0.7625508308410645
Epoch 710, training loss: 6.188366413116455 = 0.12823908030986786 + 1.0 * 6.060127258300781
Epoch 710, val loss: 0.7651671767234802
Epoch 720, training loss: 6.1781816482543945 = 0.12081369757652283 + 1.0 * 6.05736780166626
Epoch 720, val loss: 0.7680473327636719
Epoch 730, training loss: 6.170372009277344 = 0.1138782873749733 + 1.0 * 6.056493759155273
Epoch 730, val loss: 0.7710971236228943
Epoch 740, training loss: 6.163441181182861 = 0.10741903632879257 + 1.0 * 6.0560221672058105
Epoch 740, val loss: 0.7742238640785217
Epoch 750, training loss: 6.156177997589111 = 0.10142477601766586 + 1.0 * 6.054753303527832
Epoch 750, val loss: 0.7775575518608093
Epoch 760, training loss: 6.146570682525635 = 0.09582818299531937 + 1.0 * 6.0507426261901855
Epoch 760, val loss: 0.781040370464325
Epoch 770, training loss: 6.146155834197998 = 0.09058961272239685 + 1.0 * 6.055566310882568
Epoch 770, val loss: 0.7846393585205078
Epoch 780, training loss: 6.140842914581299 = 0.08573830872774124 + 1.0 * 6.055104732513428
Epoch 780, val loss: 0.7882221341133118
Epoch 790, training loss: 6.129471778869629 = 0.08122432976961136 + 1.0 * 6.048247337341309
Epoch 790, val loss: 0.7920346856117249
Epoch 800, training loss: 6.1251935958862305 = 0.07700647413730621 + 1.0 * 6.048187255859375
Epoch 800, val loss: 0.7958434224128723
Epoch 810, training loss: 6.128774642944336 = 0.07306790351867676 + 1.0 * 6.05570650100708
Epoch 810, val loss: 0.7996566295623779
Epoch 820, training loss: 6.117249965667725 = 0.06941734254360199 + 1.0 * 6.047832489013672
Epoch 820, val loss: 0.8035359978675842
Epoch 830, training loss: 6.1111040115356445 = 0.06601200997829437 + 1.0 * 6.0450921058654785
Epoch 830, val loss: 0.8075361251831055
Epoch 840, training loss: 6.107737064361572 = 0.06281797587871552 + 1.0 * 6.044919013977051
Epoch 840, val loss: 0.8114891052246094
Epoch 850, training loss: 6.107255935668945 = 0.05982593819499016 + 1.0 * 6.047430038452148
Epoch 850, val loss: 0.815376877784729
Epoch 860, training loss: 6.0998454093933105 = 0.05703100934624672 + 1.0 * 6.042814254760742
Epoch 860, val loss: 0.8193801641464233
Epoch 870, training loss: 6.096863269805908 = 0.05441167950630188 + 1.0 * 6.04245138168335
Epoch 870, val loss: 0.8233932852745056
Epoch 880, training loss: 6.097114562988281 = 0.051954012364149094 + 1.0 * 6.04516077041626
Epoch 880, val loss: 0.8272355198860168
Epoch 890, training loss: 6.090134620666504 = 0.04966267943382263 + 1.0 * 6.040472030639648
Epoch 890, val loss: 0.8311920166015625
Epoch 900, training loss: 6.088688373565674 = 0.04750967398285866 + 1.0 * 6.0411787033081055
Epoch 900, val loss: 0.8351424336433411
Epoch 910, training loss: 6.084111213684082 = 0.0454842746257782 + 1.0 * 6.0386271476745605
Epoch 910, val loss: 0.8390145897865295
Epoch 920, training loss: 6.081597328186035 = 0.04357755556702614 + 1.0 * 6.03801965713501
Epoch 920, val loss: 0.8429461717605591
Epoch 930, training loss: 6.079456806182861 = 0.04178232327103615 + 1.0 * 6.037674427032471
Epoch 930, val loss: 0.8466691374778748
Epoch 940, training loss: 6.078512668609619 = 0.040102675557136536 + 1.0 * 6.038410186767578
Epoch 940, val loss: 0.8505225777626038
Epoch 950, training loss: 6.074955463409424 = 0.03851921483874321 + 1.0 * 6.036436080932617
Epoch 950, val loss: 0.8543365597724915
Epoch 960, training loss: 6.0781965255737305 = 0.03702184557914734 + 1.0 * 6.04117488861084
Epoch 960, val loss: 0.8579519391059875
Epoch 970, training loss: 6.073252201080322 = 0.03561486676335335 + 1.0 * 6.037637233734131
Epoch 970, val loss: 0.8615817427635193
Epoch 980, training loss: 6.071184158325195 = 0.03428751602768898 + 1.0 * 6.036896705627441
Epoch 980, val loss: 0.8651809692382812
Epoch 990, training loss: 6.067286014556885 = 0.033035531640052795 + 1.0 * 6.034250259399414
Epoch 990, val loss: 0.8688281774520874
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6052
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.315242767333984 = 1.941480040550232 + 1.0 * 8.373763084411621
Epoch 0, val loss: 1.9350636005401611
Epoch 10, training loss: 10.303750991821289 = 1.931124210357666 + 1.0 * 8.372627258300781
Epoch 10, val loss: 1.9246402978897095
Epoch 20, training loss: 10.286041259765625 = 1.9188934564590454 + 1.0 * 8.367147445678711
Epoch 20, val loss: 1.9118069410324097
Epoch 30, training loss: 10.246649742126465 = 1.9026473760604858 + 1.0 * 8.344002723693848
Epoch 30, val loss: 1.8941856622695923
Epoch 40, training loss: 10.056184768676758 = 1.8829057216644287 + 1.0 * 8.17327880859375
Epoch 40, val loss: 1.8730034828186035
Epoch 50, training loss: 9.332019805908203 = 1.862136721611023 + 1.0 * 7.469882965087891
Epoch 50, val loss: 1.8501932621002197
Epoch 60, training loss: 9.124788284301758 = 1.8417421579360962 + 1.0 * 7.283046245574951
Epoch 60, val loss: 1.8298513889312744
Epoch 70, training loss: 8.881149291992188 = 1.826345443725586 + 1.0 * 7.054803848266602
Epoch 70, val loss: 1.814843773841858
Epoch 80, training loss: 8.61298656463623 = 1.8131712675094604 + 1.0 * 6.7998151779174805
Epoch 80, val loss: 1.801998496055603
Epoch 90, training loss: 8.417377471923828 = 1.8021581172943115 + 1.0 * 6.6152191162109375
Epoch 90, val loss: 1.7914029359817505
Epoch 100, training loss: 8.325363159179688 = 1.7884914875030518 + 1.0 * 6.536871433258057
Epoch 100, val loss: 1.777908205986023
Epoch 110, training loss: 8.248823165893555 = 1.7733492851257324 + 1.0 * 6.475473880767822
Epoch 110, val loss: 1.7635122537612915
Epoch 120, training loss: 8.18490982055664 = 1.7590546607971191 + 1.0 * 6.425854682922363
Epoch 120, val loss: 1.7501611709594727
Epoch 130, training loss: 8.127974510192871 = 1.7449989318847656 + 1.0 * 6.3829755783081055
Epoch 130, val loss: 1.7371814250946045
Epoch 140, training loss: 8.075286865234375 = 1.729344129562378 + 1.0 * 6.345942497253418
Epoch 140, val loss: 1.722953200340271
Epoch 150, training loss: 8.02549934387207 = 1.7108304500579834 + 1.0 * 6.314669132232666
Epoch 150, val loss: 1.7068312168121338
Epoch 160, training loss: 7.978559494018555 = 1.6883947849273682 + 1.0 * 6.290164470672607
Epoch 160, val loss: 1.6878976821899414
Epoch 170, training loss: 7.927334785461426 = 1.66126549243927 + 1.0 * 6.266069412231445
Epoch 170, val loss: 1.6651482582092285
Epoch 180, training loss: 7.874638080596924 = 1.6276963949203491 + 1.0 * 6.246941566467285
Epoch 180, val loss: 1.6372777223587036
Epoch 190, training loss: 7.820733070373535 = 1.5865345001220703 + 1.0 * 6.234198570251465
Epoch 190, val loss: 1.6031666994094849
Epoch 200, training loss: 7.758872985839844 = 1.5384714603424072 + 1.0 * 6.220401763916016
Epoch 200, val loss: 1.5638787746429443
Epoch 210, training loss: 7.6976470947265625 = 1.4848517179489136 + 1.0 * 6.212795257568359
Epoch 210, val loss: 1.5203683376312256
Epoch 220, training loss: 7.628999710083008 = 1.4285537004470825 + 1.0 * 6.200446128845215
Epoch 220, val loss: 1.4750785827636719
Epoch 230, training loss: 7.563567161560059 = 1.3718881607055664 + 1.0 * 6.191679000854492
Epoch 230, val loss: 1.4303182363510132
Epoch 240, training loss: 7.504188537597656 = 1.3172396421432495 + 1.0 * 6.186948776245117
Epoch 240, val loss: 1.3878916501998901
Epoch 250, training loss: 7.44773530960083 = 1.2680877447128296 + 1.0 * 6.179647445678711
Epoch 250, val loss: 1.3510472774505615
Epoch 260, training loss: 7.394346714019775 = 1.2241634130477905 + 1.0 * 6.170183181762695
Epoch 260, val loss: 1.3186882734298706
Epoch 270, training loss: 7.346779823303223 = 1.183327078819275 + 1.0 * 6.163452625274658
Epoch 270, val loss: 1.2891472578048706
Epoch 280, training loss: 7.301488399505615 = 1.1443175077438354 + 1.0 * 6.15717077255249
Epoch 280, val loss: 1.2614299058914185
Epoch 290, training loss: 7.257246494293213 = 1.1058470010757446 + 1.0 * 6.151399612426758
Epoch 290, val loss: 1.234462022781372
Epoch 300, training loss: 7.215865135192871 = 1.0671626329421997 + 1.0 * 6.148702621459961
Epoch 300, val loss: 1.2075552940368652
Epoch 310, training loss: 7.17507266998291 = 1.028382658958435 + 1.0 * 6.1466898918151855
Epoch 310, val loss: 1.1807570457458496
Epoch 320, training loss: 7.129830837249756 = 0.9891021847724915 + 1.0 * 6.14072847366333
Epoch 320, val loss: 1.1533786058425903
Epoch 330, training loss: 7.08524751663208 = 0.948870062828064 + 1.0 * 6.136377334594727
Epoch 330, val loss: 1.1252236366271973
Epoch 340, training loss: 7.042393207550049 = 0.9081906676292419 + 1.0 * 6.134202480316162
Epoch 340, val loss: 1.0966873168945312
Epoch 350, training loss: 7.0006818771362305 = 0.8678577542304993 + 1.0 * 6.132823944091797
Epoch 350, val loss: 1.0685405731201172
Epoch 360, training loss: 6.95567512512207 = 0.828508734703064 + 1.0 * 6.127166271209717
Epoch 360, val loss: 1.0412096977233887
Epoch 370, training loss: 6.913300037384033 = 0.7901400327682495 + 1.0 * 6.123159885406494
Epoch 370, val loss: 1.0147637128829956
Epoch 380, training loss: 6.875533580780029 = 0.7529700398445129 + 1.0 * 6.122563362121582
Epoch 380, val loss: 0.9895503520965576
Epoch 390, training loss: 6.8534440994262695 = 0.7176324129104614 + 1.0 * 6.135811805725098
Epoch 390, val loss: 0.9661456942558289
Epoch 400, training loss: 6.804035663604736 = 0.6846851110458374 + 1.0 * 6.119350433349609
Epoch 400, val loss: 0.9448996186256409
Epoch 410, training loss: 6.7661004066467285 = 0.653398871421814 + 1.0 * 6.112701416015625
Epoch 410, val loss: 0.9254135489463806
Epoch 420, training loss: 6.733119964599609 = 0.6233216524124146 + 1.0 * 6.109798431396484
Epoch 420, val loss: 0.9072231650352478
Epoch 430, training loss: 6.7018141746521 = 0.594255268573761 + 1.0 * 6.107558727264404
Epoch 430, val loss: 0.8903205990791321
Epoch 440, training loss: 6.677649974822998 = 0.566189169883728 + 1.0 * 6.1114606857299805
Epoch 440, val loss: 0.8745778799057007
Epoch 450, training loss: 6.643427848815918 = 0.5393603444099426 + 1.0 * 6.104067325592041
Epoch 450, val loss: 0.8601081967353821
Epoch 460, training loss: 6.6146745681762695 = 0.513512909412384 + 1.0 * 6.101161479949951
Epoch 460, val loss: 0.8467809557914734
Epoch 470, training loss: 6.5882673263549805 = 0.48833563923835754 + 1.0 * 6.099931716918945
Epoch 470, val loss: 0.8342874050140381
Epoch 480, training loss: 6.5654826164245605 = 0.4638427495956421 + 1.0 * 6.101639747619629
Epoch 480, val loss: 0.8226445317268372
Epoch 490, training loss: 6.537961006164551 = 0.44010379910469055 + 1.0 * 6.0978569984436035
Epoch 490, val loss: 0.8119879364967346
Epoch 500, training loss: 6.511065483093262 = 0.4171338975429535 + 1.0 * 6.093931674957275
Epoch 500, val loss: 0.8021571040153503
Epoch 510, training loss: 6.488651275634766 = 0.3949221074581146 + 1.0 * 6.093729019165039
Epoch 510, val loss: 0.7932779788970947
Epoch 520, training loss: 6.466418743133545 = 0.3735681176185608 + 1.0 * 6.092850685119629
Epoch 520, val loss: 0.7852461934089661
Epoch 530, training loss: 6.443290710449219 = 0.35318484902381897 + 1.0 * 6.090106010437012
Epoch 530, val loss: 0.7782654762268066
Epoch 540, training loss: 6.421009540557861 = 0.33379027247428894 + 1.0 * 6.08721923828125
Epoch 540, val loss: 0.7722821831703186
Epoch 550, training loss: 6.404575824737549 = 0.3154120445251465 + 1.0 * 6.089163780212402
Epoch 550, val loss: 0.7672684192657471
Epoch 560, training loss: 6.387594699859619 = 0.29815900325775146 + 1.0 * 6.089435577392578
Epoch 560, val loss: 0.7632428407669067
Epoch 570, training loss: 6.365196228027344 = 0.2820478677749634 + 1.0 * 6.08314847946167
Epoch 570, val loss: 0.7602573037147522
Epoch 580, training loss: 6.34791898727417 = 0.2669522166252136 + 1.0 * 6.080966949462891
Epoch 580, val loss: 0.7582136988639832
Epoch 590, training loss: 6.333009243011475 = 0.25273576378822327 + 1.0 * 6.080273628234863
Epoch 590, val loss: 0.7569561004638672
Epoch 600, training loss: 6.318095684051514 = 0.2394055426120758 + 1.0 * 6.078690052032471
Epoch 600, val loss: 0.7564353346824646
Epoch 610, training loss: 6.302419185638428 = 0.22695930302143097 + 1.0 * 6.075459957122803
Epoch 610, val loss: 0.7566247582435608
Epoch 620, training loss: 6.291032314300537 = 0.21528038382530212 + 1.0 * 6.075751781463623
Epoch 620, val loss: 0.7574602961540222
Epoch 630, training loss: 6.2806525230407715 = 0.20436789095401764 + 1.0 * 6.076284408569336
Epoch 630, val loss: 0.7586540579795837
Epoch 640, training loss: 6.266085147857666 = 0.1941920816898346 + 1.0 * 6.071893215179443
Epoch 640, val loss: 0.760554850101471
Epoch 650, training loss: 6.255662441253662 = 0.18463145196437836 + 1.0 * 6.071031093597412
Epoch 650, val loss: 0.7628175020217896
Epoch 660, training loss: 6.24919319152832 = 0.1756460815668106 + 1.0 * 6.073546886444092
Epoch 660, val loss: 0.7653734087944031
Epoch 670, training loss: 6.239348888397217 = 0.16722078621387482 + 1.0 * 6.0721282958984375
Epoch 670, val loss: 0.7682961821556091
Epoch 680, training loss: 6.22646427154541 = 0.15933825075626373 + 1.0 * 6.0671257972717285
Epoch 680, val loss: 0.771460771560669
Epoch 690, training loss: 6.218538761138916 = 0.15194261074066162 + 1.0 * 6.066596031188965
Epoch 690, val loss: 0.7748647928237915
Epoch 700, training loss: 6.213830947875977 = 0.14497080445289612 + 1.0 * 6.068860054016113
Epoch 700, val loss: 0.7785332798957825
Epoch 710, training loss: 6.202574253082275 = 0.13840022683143616 + 1.0 * 6.064174175262451
Epoch 710, val loss: 0.7824283242225647
Epoch 720, training loss: 6.198044300079346 = 0.13219976425170898 + 1.0 * 6.065844535827637
Epoch 720, val loss: 0.7863989472389221
Epoch 730, training loss: 6.188961982727051 = 0.1263519674539566 + 1.0 * 6.062610149383545
Epoch 730, val loss: 0.7905552387237549
Epoch 740, training loss: 6.1815505027771 = 0.12080943584442139 + 1.0 * 6.060740947723389
Epoch 740, val loss: 0.7948776483535767
Epoch 750, training loss: 6.178718566894531 = 0.11554374545812607 + 1.0 * 6.063174724578857
Epoch 750, val loss: 0.799314022064209
Epoch 760, training loss: 6.175145149230957 = 0.11054953187704086 + 1.0 * 6.064595699310303
Epoch 760, val loss: 0.8036901950836182
Epoch 770, training loss: 6.163324356079102 = 0.10584205389022827 + 1.0 * 6.0574822425842285
Epoch 770, val loss: 0.8081203699111938
Epoch 780, training loss: 6.157399654388428 = 0.1013844683766365 + 1.0 * 6.0560150146484375
Epoch 780, val loss: 0.8127777576446533
Epoch 790, training loss: 6.152470588684082 = 0.09712157398462296 + 1.0 * 6.055348873138428
Epoch 790, val loss: 0.8174914717674255
Epoch 800, training loss: 6.147000789642334 = 0.09302736073732376 + 1.0 * 6.053973197937012
Epoch 800, val loss: 0.8223095536231995
Epoch 810, training loss: 6.146452903747559 = 0.08910112082958221 + 1.0 * 6.057351589202881
Epoch 810, val loss: 0.8272107243537903
Epoch 820, training loss: 6.139898777008057 = 0.08536075055599213 + 1.0 * 6.054538249969482
Epoch 820, val loss: 0.8320304155349731
Epoch 830, training loss: 6.14236307144165 = 0.08177450299263 + 1.0 * 6.060588359832764
Epoch 830, val loss: 0.8369938731193542
Epoch 840, training loss: 6.133624076843262 = 0.07836747169494629 + 1.0 * 6.055256366729736
Epoch 840, val loss: 0.8417023420333862
Epoch 850, training loss: 6.125854015350342 = 0.07512129843235016 + 1.0 * 6.050732612609863
Epoch 850, val loss: 0.8466930389404297
Epoch 860, training loss: 6.121397018432617 = 0.07201791554689407 + 1.0 * 6.049378871917725
Epoch 860, val loss: 0.8516444563865662
Epoch 870, training loss: 6.126103401184082 = 0.06904973089694977 + 1.0 * 6.057053565979004
Epoch 870, val loss: 0.8566222190856934
Epoch 880, training loss: 6.116450309753418 = 0.06623000651597977 + 1.0 * 6.050220489501953
Epoch 880, val loss: 0.8615549802780151
Epoch 890, training loss: 6.110964298248291 = 0.06354598701000214 + 1.0 * 6.047418117523193
Epoch 890, val loss: 0.8666024804115295
Epoch 900, training loss: 6.1070685386657715 = 0.060982171446084976 + 1.0 * 6.046086311340332
Epoch 900, val loss: 0.8715434670448303
Epoch 910, training loss: 6.112293720245361 = 0.05854164808988571 + 1.0 * 6.0537519454956055
Epoch 910, val loss: 0.8764788508415222
Epoch 920, training loss: 6.10129451751709 = 0.05624299496412277 + 1.0 * 6.045051574707031
Epoch 920, val loss: 0.8813827633857727
Epoch 930, training loss: 6.10105037689209 = 0.05406629294157028 + 1.0 * 6.0469841957092285
Epoch 930, val loss: 0.8863345980644226
Epoch 940, training loss: 6.096688747406006 = 0.052010342478752136 + 1.0 * 6.044678211212158
Epoch 940, val loss: 0.8911095857620239
Epoch 950, training loss: 6.092686653137207 = 0.05006701499223709 + 1.0 * 6.042619705200195
Epoch 950, val loss: 0.8960387110710144
Epoch 960, training loss: 6.093308925628662 = 0.04821954667568207 + 1.0 * 6.045089244842529
Epoch 960, val loss: 0.900891125202179
Epoch 970, training loss: 6.087365627288818 = 0.04646224528551102 + 1.0 * 6.040903568267822
Epoch 970, val loss: 0.9057225584983826
Epoch 980, training loss: 6.086334228515625 = 0.04478520527482033 + 1.0 * 6.041549205780029
Epoch 980, val loss: 0.9106264710426331
Epoch 990, training loss: 6.0860819816589355 = 0.043188028037548065 + 1.0 * 6.042893886566162
Epoch 990, val loss: 0.9154114723205566
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.1919
Flip ASR: 0.2044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.31324577331543 = 1.9394323825836182 + 1.0 * 8.37381362915039
Epoch 0, val loss: 1.9338315725326538
Epoch 10, training loss: 10.303159713745117 = 1.9299265146255493 + 1.0 * 8.3732328414917
Epoch 10, val loss: 1.9248046875
Epoch 20, training loss: 10.287272453308105 = 1.9179455041885376 + 1.0 * 8.3693265914917
Epoch 20, val loss: 1.9132120609283447
Epoch 30, training loss: 10.243391036987305 = 1.9013394117355347 + 1.0 * 8.34205150604248
Epoch 30, val loss: 1.8970892429351807
Epoch 40, training loss: 10.036881446838379 = 1.8810147047042847 + 1.0 * 8.155866622924805
Epoch 40, val loss: 1.8780196905136108
Epoch 50, training loss: 9.493925094604492 = 1.858198881149292 + 1.0 * 7.635726451873779
Epoch 50, val loss: 1.8566513061523438
Epoch 60, training loss: 9.055717468261719 = 1.8408750295639038 + 1.0 * 7.214842796325684
Epoch 60, val loss: 1.8409937620162964
Epoch 70, training loss: 8.677155494689941 = 1.8304108381271362 + 1.0 * 6.846745014190674
Epoch 70, val loss: 1.8304882049560547
Epoch 80, training loss: 8.505940437316895 = 1.8178454637527466 + 1.0 * 6.688094615936279
Epoch 80, val loss: 1.81747567653656
Epoch 90, training loss: 8.391072273254395 = 1.8027909994125366 + 1.0 * 6.588281154632568
Epoch 90, val loss: 1.8025258779525757
Epoch 100, training loss: 8.30210018157959 = 1.7885935306549072 + 1.0 * 6.5135064125061035
Epoch 100, val loss: 1.789372205734253
Epoch 110, training loss: 8.219127655029297 = 1.776543140411377 + 1.0 * 6.442584037780762
Epoch 110, val loss: 1.778611421585083
Epoch 120, training loss: 8.152395248413086 = 1.765048623085022 + 1.0 * 6.387346267700195
Epoch 120, val loss: 1.7681372165679932
Epoch 130, training loss: 8.102680206298828 = 1.7521817684173584 + 1.0 * 6.350498199462891
Epoch 130, val loss: 1.7563345432281494
Epoch 140, training loss: 8.056513786315918 = 1.737348198890686 + 1.0 * 6.3191657066345215
Epoch 140, val loss: 1.7432552576065063
Epoch 150, training loss: 8.014485359191895 = 1.7203764915466309 + 1.0 * 6.294108867645264
Epoch 150, val loss: 1.7287100553512573
Epoch 160, training loss: 7.9734110832214355 = 1.7004474401474 + 1.0 * 6.272963523864746
Epoch 160, val loss: 1.7120771408081055
Epoch 170, training loss: 7.935080051422119 = 1.6765971183776855 + 1.0 * 6.258482933044434
Epoch 170, val loss: 1.6924420595169067
Epoch 180, training loss: 7.890187740325928 = 1.6482139825820923 + 1.0 * 6.241973876953125
Epoch 180, val loss: 1.6692818403244019
Epoch 190, training loss: 7.843097686767578 = 1.6142445802688599 + 1.0 * 6.228853225708008
Epoch 190, val loss: 1.6415904760360718
Epoch 200, training loss: 7.7912397384643555 = 1.5736477375030518 + 1.0 * 6.217592239379883
Epoch 200, val loss: 1.6085294485092163
Epoch 210, training loss: 7.734278678894043 = 1.5256842374801636 + 1.0 * 6.20859432220459
Epoch 210, val loss: 1.5695165395736694
Epoch 220, training loss: 7.670886516571045 = 1.470986008644104 + 1.0 * 6.1999006271362305
Epoch 220, val loss: 1.5253640413284302
Epoch 230, training loss: 7.603855133056641 = 1.41062593460083 + 1.0 * 6.1932291984558105
Epoch 230, val loss: 1.477026104927063
Epoch 240, training loss: 7.5333781242370605 = 1.3467693328857422 + 1.0 * 6.186608791351318
Epoch 240, val loss: 1.425963282585144
Epoch 250, training loss: 7.462449550628662 = 1.2813373804092407 + 1.0 * 6.181112289428711
Epoch 250, val loss: 1.3741921186447144
Epoch 260, training loss: 7.3931989669799805 = 1.216770887374878 + 1.0 * 6.176428318023682
Epoch 260, val loss: 1.3235968351364136
Epoch 270, training loss: 7.323607444763184 = 1.1543505191802979 + 1.0 * 6.169257164001465
Epoch 270, val loss: 1.2752487659454346
Epoch 280, training loss: 7.264620780944824 = 1.095086693763733 + 1.0 * 6.169534206390381
Epoch 280, val loss: 1.229644775390625
Epoch 290, training loss: 7.199894905090332 = 1.0405817031860352 + 1.0 * 6.159313201904297
Epoch 290, val loss: 1.187985897064209
Epoch 300, training loss: 7.143232822418213 = 0.9901084899902344 + 1.0 * 6.1531243324279785
Epoch 300, val loss: 1.1498371362686157
Epoch 310, training loss: 7.097151279449463 = 0.943206250667572 + 1.0 * 6.153944969177246
Epoch 310, val loss: 1.114803433418274
Epoch 320, training loss: 7.044754505157471 = 0.9002302885055542 + 1.0 * 6.144524097442627
Epoch 320, val loss: 1.0830011367797852
Epoch 330, training loss: 6.99995756149292 = 0.8599351644515991 + 1.0 * 6.140022277832031
Epoch 330, val loss: 1.0538058280944824
Epoch 340, training loss: 6.957602500915527 = 0.8216520547866821 + 1.0 * 6.135950565338135
Epoch 340, val loss: 1.026376485824585
Epoch 350, training loss: 6.918842315673828 = 0.7850251793861389 + 1.0 * 6.133817195892334
Epoch 350, val loss: 1.0004832744598389
Epoch 360, training loss: 6.880424976348877 = 0.7498273253440857 + 1.0 * 6.1305975914001465
Epoch 360, val loss: 0.9756733179092407
Epoch 370, training loss: 6.841178894042969 = 0.7155245542526245 + 1.0 * 6.125654220581055
Epoch 370, val loss: 0.9518128633499146
Epoch 380, training loss: 6.806920051574707 = 0.6817137002944946 + 1.0 * 6.125206470489502
Epoch 380, val loss: 0.9284569621086121
Epoch 390, training loss: 6.772711753845215 = 0.6486979722976685 + 1.0 * 6.124013900756836
Epoch 390, val loss: 0.9057260155677795
Epoch 400, training loss: 6.734184265136719 = 0.6167015433311462 + 1.0 * 6.117482662200928
Epoch 400, val loss: 0.8839908242225647
Epoch 410, training loss: 6.699355602264404 = 0.5854121446609497 + 1.0 * 6.113943576812744
Epoch 410, val loss: 0.8629568815231323
Epoch 420, training loss: 6.666896820068359 = 0.5548226833343506 + 1.0 * 6.11207389831543
Epoch 420, val loss: 0.8425379991531372
Epoch 430, training loss: 6.635566711425781 = 0.5252249836921692 + 1.0 * 6.110341548919678
Epoch 430, val loss: 0.823039710521698
Epoch 440, training loss: 6.609064102172852 = 0.4969203472137451 + 1.0 * 6.1121439933776855
Epoch 440, val loss: 0.804828941822052
Epoch 450, training loss: 6.57563591003418 = 0.4698966443538666 + 1.0 * 6.105739116668701
Epoch 450, val loss: 0.787924587726593
Epoch 460, training loss: 6.548533916473389 = 0.44391873478889465 + 1.0 * 6.104615211486816
Epoch 460, val loss: 0.7723181247711182
Epoch 470, training loss: 6.523289680480957 = 0.419014573097229 + 1.0 * 6.104275226593018
Epoch 470, val loss: 0.7578297853469849
Epoch 480, training loss: 6.493266582489014 = 0.39506959915161133 + 1.0 * 6.098196983337402
Epoch 480, val loss: 0.7445710897445679
Epoch 490, training loss: 6.468399524688721 = 0.37178367376327515 + 1.0 * 6.096615791320801
Epoch 490, val loss: 0.732257068157196
Epoch 500, training loss: 6.445915699005127 = 0.3491691052913666 + 1.0 * 6.096746444702148
Epoch 500, val loss: 0.720800518989563
Epoch 510, training loss: 6.424436569213867 = 0.3274320065975189 + 1.0 * 6.097004413604736
Epoch 510, val loss: 0.7102909088134766
Epoch 520, training loss: 6.39794397354126 = 0.30658113956451416 + 1.0 * 6.091362953186035
Epoch 520, val loss: 0.7008193135261536
Epoch 530, training loss: 6.374382019042969 = 0.28649452328681946 + 1.0 * 6.087887287139893
Epoch 530, val loss: 0.6921141147613525
Epoch 540, training loss: 6.35633659362793 = 0.2672429382801056 + 1.0 * 6.0890936851501465
Epoch 540, val loss: 0.6842024326324463
Epoch 550, training loss: 6.340303897857666 = 0.24909701943397522 + 1.0 * 6.091207027435303
Epoch 550, val loss: 0.6771774888038635
Epoch 560, training loss: 6.318861484527588 = 0.2321755290031433 + 1.0 * 6.086686134338379
Epoch 560, val loss: 0.6711928248405457
Epoch 570, training loss: 6.299259662628174 = 0.21641753613948822 + 1.0 * 6.0828423500061035
Epoch 570, val loss: 0.6661883592605591
Epoch 580, training loss: 6.283432483673096 = 0.20179635286331177 + 1.0 * 6.08163595199585
Epoch 580, val loss: 0.6620829105377197
Epoch 590, training loss: 6.27256441116333 = 0.18831989169120789 + 1.0 * 6.084244728088379
Epoch 590, val loss: 0.6587802171707153
Epoch 600, training loss: 6.258160591125488 = 0.17604316771030426 + 1.0 * 6.082117557525635
Epoch 600, val loss: 0.6562861800193787
Epoch 610, training loss: 6.241153717041016 = 0.16483306884765625 + 1.0 * 6.076320648193359
Epoch 610, val loss: 0.6546284556388855
Epoch 620, training loss: 6.237308502197266 = 0.15454980731010437 + 1.0 * 6.082758903503418
Epoch 620, val loss: 0.6536279916763306
Epoch 630, training loss: 6.221160888671875 = 0.14519956707954407 + 1.0 * 6.075961112976074
Epoch 630, val loss: 0.6532801985740662
Epoch 640, training loss: 6.214909076690674 = 0.13661867380142212 + 1.0 * 6.0782904624938965
Epoch 640, val loss: 0.653558075428009
Epoch 650, training loss: 6.202755451202393 = 0.12876833975315094 + 1.0 * 6.073987007141113
Epoch 650, val loss: 0.6542377471923828
Epoch 660, training loss: 6.191004276275635 = 0.12155668437480927 + 1.0 * 6.0694475173950195
Epoch 660, val loss: 0.6554281115531921
Epoch 670, training loss: 6.183323383331299 = 0.11486402899026871 + 1.0 * 6.068459510803223
Epoch 670, val loss: 0.6569787859916687
Epoch 680, training loss: 6.176024436950684 = 0.10869663208723068 + 1.0 * 6.067327976226807
Epoch 680, val loss: 0.658796489238739
Epoch 690, training loss: 6.168215274810791 = 0.10303783416748047 + 1.0 * 6.0651774406433105
Epoch 690, val loss: 0.6609912514686584
Epoch 700, training loss: 6.162293910980225 = 0.09777479618787766 + 1.0 * 6.064518928527832
Epoch 700, val loss: 0.6634935140609741
Epoch 710, training loss: 6.165388107299805 = 0.09285613894462585 + 1.0 * 6.0725321769714355
Epoch 710, val loss: 0.6661497354507446
Epoch 720, training loss: 6.152538299560547 = 0.08828525245189667 + 1.0 * 6.064252853393555
Epoch 720, val loss: 0.6689356565475464
Epoch 730, training loss: 6.144898414611816 = 0.08403708040714264 + 1.0 * 6.060861110687256
Epoch 730, val loss: 0.6720019578933716
Epoch 740, training loss: 6.149539947509766 = 0.08005403727293015 + 1.0 * 6.069486141204834
Epoch 740, val loss: 0.6751593351364136
Epoch 750, training loss: 6.134747505187988 = 0.07633929699659348 + 1.0 * 6.058408260345459
Epoch 750, val loss: 0.6784287691116333
Epoch 760, training loss: 6.130549907684326 = 0.07285218685865402 + 1.0 * 6.057697772979736
Epoch 760, val loss: 0.6819058656692505
Epoch 770, training loss: 6.1327924728393555 = 0.06956648081541061 + 1.0 * 6.063226222991943
Epoch 770, val loss: 0.685452401638031
Epoch 780, training loss: 6.12662935256958 = 0.06647855788469315 + 1.0 * 6.060150623321533
Epoch 780, val loss: 0.6889820098876953
Epoch 790, training loss: 6.11900520324707 = 0.06358841806650162 + 1.0 * 6.055416584014893
Epoch 790, val loss: 0.6926973462104797
Epoch 800, training loss: 6.115522384643555 = 0.060850877314805984 + 1.0 * 6.054671287536621
Epoch 800, val loss: 0.696452796459198
Epoch 810, training loss: 6.1122026443481445 = 0.05826873704791069 + 1.0 * 6.053934097290039
Epoch 810, val loss: 0.700240433216095
Epoch 820, training loss: 6.111008644104004 = 0.05584446340799332 + 1.0 * 6.055164337158203
Epoch 820, val loss: 0.7040886878967285
Epoch 830, training loss: 6.104884147644043 = 0.053559064865112305 + 1.0 * 6.051324844360352
Epoch 830, val loss: 0.7079540491104126
Epoch 840, training loss: 6.103378772735596 = 0.051395487040281296 + 1.0 * 6.05198335647583
Epoch 840, val loss: 0.711898148059845
Epoch 850, training loss: 6.102227210998535 = 0.0493537038564682 + 1.0 * 6.052873611450195
Epoch 850, val loss: 0.7157120108604431
Epoch 860, training loss: 6.097265720367432 = 0.047439999878406525 + 1.0 * 6.049825668334961
Epoch 860, val loss: 0.7195833921432495
Epoch 870, training loss: 6.092628479003906 = 0.045629218220710754 + 1.0 * 6.046999454498291
Epoch 870, val loss: 0.7235336899757385
Epoch 880, training loss: 6.089969158172607 = 0.043900057673454285 + 1.0 * 6.046069145202637
Epoch 880, val loss: 0.7274619936943054
Epoch 890, training loss: 6.097963333129883 = 0.04225970804691315 + 1.0 * 6.055703639984131
Epoch 890, val loss: 0.731246292591095
Epoch 900, training loss: 6.090498447418213 = 0.04073089733719826 + 1.0 * 6.04976749420166
Epoch 900, val loss: 0.7350327372550964
Epoch 910, training loss: 6.083198547363281 = 0.03928529471158981 + 1.0 * 6.0439133644104
Epoch 910, val loss: 0.7389259934425354
Epoch 920, training loss: 6.079963207244873 = 0.03789988532662392 + 1.0 * 6.042063236236572
Epoch 920, val loss: 0.742763876914978
Epoch 930, training loss: 6.078487396240234 = 0.036573298275470734 + 1.0 * 6.041913986206055
Epoch 930, val loss: 0.7465672492980957
Epoch 940, training loss: 6.0821614265441895 = 0.03531167283654213 + 1.0 * 6.046849727630615
Epoch 940, val loss: 0.7502604126930237
Epoch 950, training loss: 6.0777459144592285 = 0.03412754461169243 + 1.0 * 6.043618202209473
Epoch 950, val loss: 0.7539974451065063
Epoch 960, training loss: 6.072299957275391 = 0.033002037554979324 + 1.0 * 6.039298057556152
Epoch 960, val loss: 0.7577846646308899
Epoch 970, training loss: 6.075356960296631 = 0.03192142769694328 + 1.0 * 6.043435573577881
Epoch 970, val loss: 0.7615460157394409
Epoch 980, training loss: 6.069764137268066 = 0.03089219331741333 + 1.0 * 6.038871765136719
Epoch 980, val loss: 0.7651624083518982
Epoch 990, training loss: 6.0707197189331055 = 0.029917674139142036 + 1.0 * 6.040802001953125
Epoch 990, val loss: 0.7688753604888916
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8007
Flip ASR: 0.7600/225 nodes
The final ASR:0.53260, 0.25381, Accuracy:0.81728, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10574])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98032, 0.01141, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.318227767944336 = 1.9444736242294312 + 1.0 * 8.373754501342773
Epoch 0, val loss: 1.944392204284668
Epoch 10, training loss: 10.306209564208984 = 1.9331157207489014 + 1.0 * 8.373093605041504
Epoch 10, val loss: 1.932927131652832
Epoch 20, training loss: 10.28766918182373 = 1.9184503555297852 + 1.0 * 8.369218826293945
Epoch 20, val loss: 1.918107509613037
Epoch 30, training loss: 10.23847770690918 = 1.8976233005523682 + 1.0 * 8.34085464477539
Epoch 30, val loss: 1.8974425792694092
Epoch 40, training loss: 9.966473579406738 = 1.8730734586715698 + 1.0 * 8.093400001525879
Epoch 40, val loss: 1.8743488788604736
Epoch 50, training loss: 9.383543014526367 = 1.8483890295028687 + 1.0 * 7.535154342651367
Epoch 50, val loss: 1.8520057201385498
Epoch 60, training loss: 8.983261108398438 = 1.8317015171051025 + 1.0 * 7.151559829711914
Epoch 60, val loss: 1.8379933834075928
Epoch 70, training loss: 8.57722282409668 = 1.8224834203720093 + 1.0 * 6.754739761352539
Epoch 70, val loss: 1.8304170370101929
Epoch 80, training loss: 8.391072273254395 = 1.8144818544387817 + 1.0 * 6.576590538024902
Epoch 80, val loss: 1.8230412006378174
Epoch 90, training loss: 8.278676986694336 = 1.8033546209335327 + 1.0 * 6.475322246551514
Epoch 90, val loss: 1.8131078481674194
Epoch 100, training loss: 8.198200225830078 = 1.7920074462890625 + 1.0 * 6.406193256378174
Epoch 100, val loss: 1.8032268285751343
Epoch 110, training loss: 8.136967658996582 = 1.7817362546920776 + 1.0 * 6.355231761932373
Epoch 110, val loss: 1.7940868139266968
Epoch 120, training loss: 8.087324142456055 = 1.7715661525726318 + 1.0 * 6.315757751464844
Epoch 120, val loss: 1.7849658727645874
Epoch 130, training loss: 8.048118591308594 = 1.7602527141571045 + 1.0 * 6.28786563873291
Epoch 130, val loss: 1.7751301527023315
Epoch 140, training loss: 8.008796691894531 = 1.7473464012145996 + 1.0 * 6.261449813842773
Epoch 140, val loss: 1.7643569707870483
Epoch 150, training loss: 7.97363805770874 = 1.7323918342590332 + 1.0 * 6.241246223449707
Epoch 150, val loss: 1.7521973848342896
Epoch 160, training loss: 7.939421653747559 = 1.71454918384552 + 1.0 * 6.224872589111328
Epoch 160, val loss: 1.7379306554794312
Epoch 170, training loss: 7.903898239135742 = 1.692838430404663 + 1.0 * 6.211060047149658
Epoch 170, val loss: 1.72065007686615
Epoch 180, training loss: 7.866239070892334 = 1.6665621995925903 + 1.0 * 6.199676990509033
Epoch 180, val loss: 1.699830412864685
Epoch 190, training loss: 7.826476097106934 = 1.635533094406128 + 1.0 * 6.190943241119385
Epoch 190, val loss: 1.6752585172653198
Epoch 200, training loss: 7.780770778656006 = 1.5989890098571777 + 1.0 * 6.181781768798828
Epoch 200, val loss: 1.6461961269378662
Epoch 210, training loss: 7.733341217041016 = 1.5568127632141113 + 1.0 * 6.176528453826904
Epoch 210, val loss: 1.612618088722229
Epoch 220, training loss: 7.681812763214111 = 1.5112528800964355 + 1.0 * 6.170559883117676
Epoch 220, val loss: 1.5766735076904297
Epoch 230, training loss: 7.627045631408691 = 1.4635180234909058 + 1.0 * 6.163527488708496
Epoch 230, val loss: 1.5390139818191528
Epoch 240, training loss: 7.572656154632568 = 1.4145188331604004 + 1.0 * 6.158137321472168
Epoch 240, val loss: 1.5006940364837646
Epoch 250, training loss: 7.520440578460693 = 1.3655635118484497 + 1.0 * 6.154877185821533
Epoch 250, val loss: 1.4630842208862305
Epoch 260, training loss: 7.467207908630371 = 1.3182426691055298 + 1.0 * 6.148965358734131
Epoch 260, val loss: 1.4273252487182617
Epoch 270, training loss: 7.417865753173828 = 1.2727229595184326 + 1.0 * 6.145143032073975
Epoch 270, val loss: 1.3934661149978638
Epoch 280, training loss: 7.368019104003906 = 1.22890305519104 + 1.0 * 6.139115810394287
Epoch 280, val loss: 1.3610843420028687
Epoch 290, training loss: 7.320251941680908 = 1.1860510110855103 + 1.0 * 6.1342010498046875
Epoch 290, val loss: 1.3298213481903076
Epoch 300, training loss: 7.275984287261963 = 1.1445156335830688 + 1.0 * 6.131468772888184
Epoch 300, val loss: 1.2997329235076904
Epoch 310, training loss: 7.2300496101379395 = 1.1046329736709595 + 1.0 * 6.1254167556762695
Epoch 310, val loss: 1.2710224390029907
Epoch 320, training loss: 7.189114570617676 = 1.0665199756622314 + 1.0 * 6.122594356536865
Epoch 320, val loss: 1.2436761856079102
Epoch 330, training loss: 7.149878025054932 = 1.0305732488632202 + 1.0 * 6.119304656982422
Epoch 330, val loss: 1.217927098274231
Epoch 340, training loss: 7.110485076904297 = 0.996917188167572 + 1.0 * 6.11356782913208
Epoch 340, val loss: 1.1937634944915771
Epoch 350, training loss: 7.0775322914123535 = 0.9651986956596375 + 1.0 * 6.11233377456665
Epoch 350, val loss: 1.1710011959075928
Epoch 360, training loss: 7.04362154006958 = 0.9351004958152771 + 1.0 * 6.108520984649658
Epoch 360, val loss: 1.1495862007141113
Epoch 370, training loss: 7.011497497558594 = 0.9061420559883118 + 1.0 * 6.105355262756348
Epoch 370, val loss: 1.1289923191070557
Epoch 380, training loss: 6.983447551727295 = 0.8774057626724243 + 1.0 * 6.10604190826416
Epoch 380, val loss: 1.1085988283157349
Epoch 390, training loss: 6.947915554046631 = 0.8484405279159546 + 1.0 * 6.099474906921387
Epoch 390, val loss: 1.0880794525146484
Epoch 400, training loss: 6.915539741516113 = 0.8185648918151855 + 1.0 * 6.096974849700928
Epoch 400, val loss: 1.066970944404602
Epoch 410, training loss: 6.8885979652404785 = 0.7873843312263489 + 1.0 * 6.101213455200195
Epoch 410, val loss: 1.044803261756897
Epoch 420, training loss: 6.849774360656738 = 0.7553605437278748 + 1.0 * 6.094413757324219
Epoch 420, val loss: 1.0219612121582031
Epoch 430, training loss: 6.815227031707764 = 0.7226333618164062 + 1.0 * 6.092593669891357
Epoch 430, val loss: 0.998779833316803
Epoch 440, training loss: 6.779501438140869 = 0.6893625259399414 + 1.0 * 6.090138912200928
Epoch 440, val loss: 0.9751799702644348
Epoch 450, training loss: 6.748520851135254 = 0.6562961935997009 + 1.0 * 6.092224597930908
Epoch 450, val loss: 0.9518369436264038
Epoch 460, training loss: 6.712393283843994 = 0.6241227388381958 + 1.0 * 6.088270664215088
Epoch 460, val loss: 0.929502546787262
Epoch 470, training loss: 6.678607940673828 = 0.5931671857833862 + 1.0 * 6.085440635681152
Epoch 470, val loss: 0.9083162546157837
Epoch 480, training loss: 6.647344589233398 = 0.5638082027435303 + 1.0 * 6.083536148071289
Epoch 480, val loss: 0.8889407515525818
Epoch 490, training loss: 6.6212382316589355 = 0.5360398888587952 + 1.0 * 6.085198402404785
Epoch 490, val loss: 0.8712904453277588
Epoch 500, training loss: 6.595850944519043 = 0.5100486278533936 + 1.0 * 6.08580207824707
Epoch 500, val loss: 0.8554394841194153
Epoch 510, training loss: 6.565352916717529 = 0.48565155267715454 + 1.0 * 6.0797014236450195
Epoch 510, val loss: 0.8415642380714417
Epoch 520, training loss: 6.540051460266113 = 0.46251752972602844 + 1.0 * 6.077533721923828
Epoch 520, val loss: 0.8292573094367981
Epoch 530, training loss: 6.518240451812744 = 0.44031789898872375 + 1.0 * 6.077922344207764
Epoch 530, val loss: 0.8182998895645142
Epoch 540, training loss: 6.497975826263428 = 0.41885149478912354 + 1.0 * 6.079124450683594
Epoch 540, val loss: 0.8083432912826538
Epoch 550, training loss: 6.473780155181885 = 0.3981223404407501 + 1.0 * 6.075657844543457
Epoch 550, val loss: 0.7992545962333679
Epoch 560, training loss: 6.44977331161499 = 0.3777370750904083 + 1.0 * 6.072036266326904
Epoch 560, val loss: 0.7907642126083374
Epoch 570, training loss: 6.430703639984131 = 0.357716828584671 + 1.0 * 6.072986602783203
Epoch 570, val loss: 0.7827298045158386
Epoch 580, training loss: 6.4108076095581055 = 0.33810681104660034 + 1.0 * 6.0727009773254395
Epoch 580, val loss: 0.7752280235290527
Epoch 590, training loss: 6.395071983337402 = 0.31904172897338867 + 1.0 * 6.076030254364014
Epoch 590, val loss: 0.7681288123130798
Epoch 600, training loss: 6.370020866394043 = 0.300534188747406 + 1.0 * 6.069486618041992
Epoch 600, val loss: 0.7615894079208374
Epoch 610, training loss: 6.349404811859131 = 0.28244373202323914 + 1.0 * 6.066961288452148
Epoch 610, val loss: 0.7553183436393738
Epoch 620, training loss: 6.332627296447754 = 0.2647758424282074 + 1.0 * 6.067851543426514
Epoch 620, val loss: 0.749474823474884
Epoch 630, training loss: 6.320664882659912 = 0.24784812331199646 + 1.0 * 6.072816848754883
Epoch 630, val loss: 0.7439920902252197
Epoch 640, training loss: 6.29736328125 = 0.23163415491580963 + 1.0 * 6.065729141235352
Epoch 640, val loss: 0.7391676306724548
Epoch 650, training loss: 6.278665065765381 = 0.21612529456615448 + 1.0 * 6.062539577484131
Epoch 650, val loss: 0.7347863912582397
Epoch 660, training loss: 6.271477222442627 = 0.20137782394886017 + 1.0 * 6.070099353790283
Epoch 660, val loss: 0.730903685092926
Epoch 670, training loss: 6.249742031097412 = 0.1876184642314911 + 1.0 * 6.062123775482178
Epoch 670, val loss: 0.7275122404098511
Epoch 680, training loss: 6.234830379486084 = 0.1748088002204895 + 1.0 * 6.06002140045166
Epoch 680, val loss: 0.7247921228408813
Epoch 690, training loss: 6.222541332244873 = 0.16286996006965637 + 1.0 * 6.059671401977539
Epoch 690, val loss: 0.7225208878517151
Epoch 700, training loss: 6.213151931762695 = 0.15186603367328644 + 1.0 * 6.061285972595215
Epoch 700, val loss: 0.7207392454147339
Epoch 710, training loss: 6.199286460876465 = 0.14179983735084534 + 1.0 * 6.057486534118652
Epoch 710, val loss: 0.7196651697158813
Epoch 720, training loss: 6.188427448272705 = 0.1325296312570572 + 1.0 * 6.0558977127075195
Epoch 720, val loss: 0.719048798084259
Epoch 730, training loss: 6.179049968719482 = 0.12399354577064514 + 1.0 * 6.055056571960449
Epoch 730, val loss: 0.7189903259277344
Epoch 740, training loss: 6.1786394119262695 = 0.11618005484342575 + 1.0 * 6.062459468841553
Epoch 740, val loss: 0.719384491443634
Epoch 750, training loss: 6.166430473327637 = 0.10909605771303177 + 1.0 * 6.0573344230651855
Epoch 750, val loss: 0.7202199697494507
Epoch 760, training loss: 6.155887603759766 = 0.10261096805334091 + 1.0 * 6.053276538848877
Epoch 760, val loss: 0.7214180827140808
Epoch 770, training loss: 6.148808002471924 = 0.09663254767656326 + 1.0 * 6.052175521850586
Epoch 770, val loss: 0.722917914390564
Epoch 780, training loss: 6.142848491668701 = 0.09115733951330185 + 1.0 * 6.051691055297852
Epoch 780, val loss: 0.7247578501701355
Epoch 790, training loss: 6.136658191680908 = 0.08615853637456894 + 1.0 * 6.050499439239502
Epoch 790, val loss: 0.7268878221511841
Epoch 800, training loss: 6.130612373352051 = 0.08153540641069412 + 1.0 * 6.049077033996582
Epoch 800, val loss: 0.729220986366272
Epoch 810, training loss: 6.124561309814453 = 0.07724270969629288 + 1.0 * 6.047318458557129
Epoch 810, val loss: 0.7318069934844971
Epoch 820, training loss: 6.132218837738037 = 0.07326366007328033 + 1.0 * 6.058955192565918
Epoch 820, val loss: 0.7346187829971313
Epoch 830, training loss: 6.115757465362549 = 0.06958417594432831 + 1.0 * 6.046173095703125
Epoch 830, val loss: 0.7375855445861816
Epoch 840, training loss: 6.1118292808532715 = 0.06616685539484024 + 1.0 * 6.0456624031066895
Epoch 840, val loss: 0.7406784892082214
Epoch 850, training loss: 6.111608982086182 = 0.0629735067486763 + 1.0 * 6.048635482788086
Epoch 850, val loss: 0.7439116835594177
Epoch 860, training loss: 6.105510711669922 = 0.060005851089954376 + 1.0 * 6.045505046844482
Epoch 860, val loss: 0.7471680641174316
Epoch 870, training loss: 6.101025581359863 = 0.05723513290286064 + 1.0 * 6.043790340423584
Epoch 870, val loss: 0.7506427764892578
Epoch 880, training loss: 6.099340915679932 = 0.054636623710393906 + 1.0 * 6.044704437255859
Epoch 880, val loss: 0.7540796995162964
Epoch 890, training loss: 6.093364715576172 = 0.05221123620867729 + 1.0 * 6.041153430938721
Epoch 890, val loss: 0.7575891613960266
Epoch 900, training loss: 6.091004848480225 = 0.049939464777708054 + 1.0 * 6.041065216064453
Epoch 900, val loss: 0.7612273693084717
Epoch 910, training loss: 6.096264839172363 = 0.04780193790793419 + 1.0 * 6.048462867736816
Epoch 910, val loss: 0.764808177947998
Epoch 920, training loss: 6.088342189788818 = 0.04580792039632797 + 1.0 * 6.042534351348877
Epoch 920, val loss: 0.7683966159820557
Epoch 930, training loss: 6.082940578460693 = 0.043930765241384506 + 1.0 * 6.039010047912598
Epoch 930, val loss: 0.7720866799354553
Epoch 940, training loss: 6.085082530975342 = 0.042159393429756165 + 1.0 * 6.0429229736328125
Epoch 940, val loss: 0.7757115960121155
Epoch 950, training loss: 6.078668117523193 = 0.04049276188015938 + 1.0 * 6.038175582885742
Epoch 950, val loss: 0.7793723344802856
Epoch 960, training loss: 6.075531005859375 = 0.0389266237616539 + 1.0 * 6.036604404449463
Epoch 960, val loss: 0.7830495834350586
Epoch 970, training loss: 6.073581218719482 = 0.03743883594870567 + 1.0 * 6.036142349243164
Epoch 970, val loss: 0.7867159247398376
Epoch 980, training loss: 6.076810836791992 = 0.036034759134054184 + 1.0 * 6.040776252746582
Epoch 980, val loss: 0.790360689163208
Epoch 990, training loss: 6.069811820983887 = 0.034717924892902374 + 1.0 * 6.035093784332275
Epoch 990, val loss: 0.7940638065338135
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.4797
Flip ASR: 0.3867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.318940162658691 = 1.945216178894043 + 1.0 * 8.373723983764648
Epoch 0, val loss: 1.9409971237182617
Epoch 10, training loss: 10.307985305786133 = 1.9349696636199951 + 1.0 * 8.373015403747559
Epoch 10, val loss: 1.9307535886764526
Epoch 20, training loss: 10.291182518005371 = 1.9223965406417847 + 1.0 * 8.368785858154297
Epoch 20, val loss: 1.9181616306304932
Epoch 30, training loss: 10.247699737548828 = 1.9053263664245605 + 1.0 * 8.34237289428711
Epoch 30, val loss: 1.900899052619934
Epoch 40, training loss: 10.05600643157959 = 1.8853379487991333 + 1.0 * 8.170668601989746
Epoch 40, val loss: 1.881136178970337
Epoch 50, training loss: 9.320823669433594 = 1.8639713525772095 + 1.0 * 7.456852436065674
Epoch 50, val loss: 1.8597121238708496
Epoch 60, training loss: 8.85131549835205 = 1.8446431159973145 + 1.0 * 7.006672382354736
Epoch 60, val loss: 1.8406705856323242
Epoch 70, training loss: 8.617450714111328 = 1.8290793895721436 + 1.0 * 6.7883710861206055
Epoch 70, val loss: 1.825028896331787
Epoch 80, training loss: 8.49817943572998 = 1.8126410245895386 + 1.0 * 6.685538291931152
Epoch 80, val loss: 1.8089138269424438
Epoch 90, training loss: 8.418074607849121 = 1.796512484550476 + 1.0 * 6.621562480926514
Epoch 90, val loss: 1.793143391609192
Epoch 100, training loss: 8.337106704711914 = 1.781071424484253 + 1.0 * 6.55603551864624
Epoch 100, val loss: 1.778189778327942
Epoch 110, training loss: 8.259603500366211 = 1.7666778564453125 + 1.0 * 6.492926120758057
Epoch 110, val loss: 1.7642924785614014
Epoch 120, training loss: 8.190277099609375 = 1.7523547410964966 + 1.0 * 6.437922477722168
Epoch 120, val loss: 1.7510687112808228
Epoch 130, training loss: 8.126051902770996 = 1.736209750175476 + 1.0 * 6.389842510223389
Epoch 130, val loss: 1.7367548942565918
Epoch 140, training loss: 8.067264556884766 = 1.7172787189483643 + 1.0 * 6.3499860763549805
Epoch 140, val loss: 1.7205705642700195
Epoch 150, training loss: 8.016036987304688 = 1.6943793296813965 + 1.0 * 6.321658134460449
Epoch 150, val loss: 1.700839877128601
Epoch 160, training loss: 7.9632568359375 = 1.6658374071121216 + 1.0 * 6.297419548034668
Epoch 160, val loss: 1.6768264770507812
Epoch 170, training loss: 7.909870147705078 = 1.6305208206176758 + 1.0 * 6.279349327087402
Epoch 170, val loss: 1.6475155353546143
Epoch 180, training loss: 7.848957061767578 = 1.587707281112671 + 1.0 * 6.261249542236328
Epoch 180, val loss: 1.6120188236236572
Epoch 190, training loss: 7.783436298370361 = 1.5357736349105835 + 1.0 * 6.247662544250488
Epoch 190, val loss: 1.5692663192749023
Epoch 200, training loss: 7.710746765136719 = 1.4745818376541138 + 1.0 * 6.2361650466918945
Epoch 200, val loss: 1.5195006132125854
Epoch 210, training loss: 7.633440971374512 = 1.40716552734375 + 1.0 * 6.226275444030762
Epoch 210, val loss: 1.465356469154358
Epoch 220, training loss: 7.553969383239746 = 1.3364009857177734 + 1.0 * 6.217568397521973
Epoch 220, val loss: 1.4091681241989136
Epoch 230, training loss: 7.4733195304870605 = 1.2648452520370483 + 1.0 * 6.208474159240723
Epoch 230, val loss: 1.3528492450714111
Epoch 240, training loss: 7.400082588195801 = 1.1948310136795044 + 1.0 * 6.205251693725586
Epoch 240, val loss: 1.2983797788619995
Epoch 250, training loss: 7.3258185386657715 = 1.1295446157455444 + 1.0 * 6.1962738037109375
Epoch 250, val loss: 1.2478110790252686
Epoch 260, training loss: 7.25652551651001 = 1.0690555572509766 + 1.0 * 6.187469959259033
Epoch 260, val loss: 1.200748085975647
Epoch 270, training loss: 7.192713737487793 = 1.011817455291748 + 1.0 * 6.180896282196045
Epoch 270, val loss: 1.1562750339508057
Epoch 280, training loss: 7.134331226348877 = 0.9580969214439392 + 1.0 * 6.176234245300293
Epoch 280, val loss: 1.114560604095459
Epoch 290, training loss: 7.075448513031006 = 0.9075268507003784 + 1.0 * 6.167921543121338
Epoch 290, val loss: 1.0750675201416016
Epoch 300, training loss: 7.023812294006348 = 0.8589207530021667 + 1.0 * 6.164891719818115
Epoch 300, val loss: 1.0369176864624023
Epoch 310, training loss: 6.972633361816406 = 0.8128864765167236 + 1.0 * 6.1597466468811035
Epoch 310, val loss: 1.0006103515625
Epoch 320, training loss: 6.922782897949219 = 0.7688992023468018 + 1.0 * 6.153883934020996
Epoch 320, val loss: 0.9660264849662781
Epoch 330, training loss: 6.875771522521973 = 0.7266932725906372 + 1.0 * 6.149078369140625
Epoch 330, val loss: 0.9329473376274109
Epoch 340, training loss: 6.8358154296875 = 0.686607301235199 + 1.0 * 6.149208068847656
Epoch 340, val loss: 0.9019050598144531
Epoch 350, training loss: 6.792367935180664 = 0.6495962142944336 + 1.0 * 6.1427717208862305
Epoch 350, val loss: 0.873810350894928
Epoch 360, training loss: 6.752928733825684 = 0.6150107979774475 + 1.0 * 6.137917995452881
Epoch 360, val loss: 0.8484446406364441
Epoch 370, training loss: 6.720555305480957 = 0.5825475454330444 + 1.0 * 6.138007640838623
Epoch 370, val loss: 0.8256133794784546
Epoch 380, training loss: 6.685941219329834 = 0.5526556968688965 + 1.0 * 6.1332855224609375
Epoch 380, val loss: 0.8056872487068176
Epoch 390, training loss: 6.652011871337891 = 0.5248138308525085 + 1.0 * 6.127198219299316
Epoch 390, val loss: 0.7882579565048218
Epoch 400, training loss: 6.622374057769775 = 0.4984360635280609 + 1.0 * 6.123938083648682
Epoch 400, val loss: 0.7727769613265991
Epoch 410, training loss: 6.601090908050537 = 0.47325605154037476 + 1.0 * 6.127834796905518
Epoch 410, val loss: 0.7589356899261475
Epoch 420, training loss: 6.568741798400879 = 0.4494141936302185 + 1.0 * 6.119327545166016
Epoch 420, val loss: 0.7467079162597656
Epoch 430, training loss: 6.542654037475586 = 0.42667728662490845 + 1.0 * 6.115976810455322
Epoch 430, val loss: 0.7358529567718506
Epoch 440, training loss: 6.517892837524414 = 0.4047987759113312 + 1.0 * 6.113093852996826
Epoch 440, val loss: 0.7261743545532227
Epoch 450, training loss: 6.49798059463501 = 0.38383758068084717 + 1.0 * 6.114142894744873
Epoch 450, val loss: 0.7176182270050049
Epoch 460, training loss: 6.472953796386719 = 0.3640099763870239 + 1.0 * 6.108943939208984
Epoch 460, val loss: 0.710114061832428
Epoch 470, training loss: 6.451642990112305 = 0.34503665566444397 + 1.0 * 6.106606483459473
Epoch 470, val loss: 0.7036135196685791
Epoch 480, training loss: 6.430463790893555 = 0.3268186151981354 + 1.0 * 6.103645324707031
Epoch 480, val loss: 0.6980578899383545
Epoch 490, training loss: 6.423250675201416 = 0.30936145782470703 + 1.0 * 6.113889217376709
Epoch 490, val loss: 0.6934088468551636
Epoch 500, training loss: 6.392845153808594 = 0.2929322421550751 + 1.0 * 6.099913120269775
Epoch 500, val loss: 0.6895857453346252
Epoch 510, training loss: 6.37534236907959 = 0.2773103415966034 + 1.0 * 6.098031997680664
Epoch 510, val loss: 0.6866307854652405
Epoch 520, training loss: 6.362196922302246 = 0.2624645531177521 + 1.0 * 6.099732398986816
Epoch 520, val loss: 0.6846264600753784
Epoch 530, training loss: 6.343369483947754 = 0.24845059216022491 + 1.0 * 6.094918727874756
Epoch 530, val loss: 0.6833828687667847
Epoch 540, training loss: 6.328333854675293 = 0.23518072068691254 + 1.0 * 6.09315299987793
Epoch 540, val loss: 0.6829254031181335
Epoch 550, training loss: 6.320969104766846 = 0.2226325273513794 + 1.0 * 6.098336696624756
Epoch 550, val loss: 0.6832501292228699
Epoch 560, training loss: 6.301935195922852 = 0.2109236717224121 + 1.0 * 6.0910115242004395
Epoch 560, val loss: 0.6842632293701172
Epoch 570, training loss: 6.293640613555908 = 0.19992227852344513 + 1.0 * 6.093718528747559
Epoch 570, val loss: 0.6858984231948853
Epoch 580, training loss: 6.277741432189941 = 0.1896301805973053 + 1.0 * 6.088111400604248
Epoch 580, val loss: 0.6882447600364685
Epoch 590, training loss: 6.265527248382568 = 0.17999204993247986 + 1.0 * 6.085535049438477
Epoch 590, val loss: 0.6911285519599915
Epoch 600, training loss: 6.254784107208252 = 0.17093515396118164 + 1.0 * 6.08384895324707
Epoch 600, val loss: 0.6945956945419312
Epoch 610, training loss: 6.250067234039307 = 0.16249677538871765 + 1.0 * 6.087570667266846
Epoch 610, val loss: 0.6985177993774414
Epoch 620, training loss: 6.237358570098877 = 0.15472090244293213 + 1.0 * 6.082637786865234
Epoch 620, val loss: 0.7026531100273132
Epoch 630, training loss: 6.227732181549072 = 0.14746440947055817 + 1.0 * 6.080267906188965
Epoch 630, val loss: 0.7072042226791382
Epoch 640, training loss: 6.218918800354004 = 0.14064911007881165 + 1.0 * 6.0782694816589355
Epoch 640, val loss: 0.7122014760971069
Epoch 650, training loss: 6.211248874664307 = 0.1342371702194214 + 1.0 * 6.077011585235596
Epoch 650, val loss: 0.7174944281578064
Epoch 660, training loss: 6.207441806793213 = 0.12822426855564117 + 1.0 * 6.079217433929443
Epoch 660, val loss: 0.7230311036109924
Epoch 670, training loss: 6.199842929840088 = 0.12264282256364822 + 1.0 * 6.077199935913086
Epoch 670, val loss: 0.7286744713783264
Epoch 680, training loss: 6.1931939125061035 = 0.1174231693148613 + 1.0 * 6.075770854949951
Epoch 680, val loss: 0.7345266342163086
Epoch 690, training loss: 6.186107158660889 = 0.11251812428236008 + 1.0 * 6.073588848114014
Epoch 690, val loss: 0.7405904531478882
Epoch 700, training loss: 6.184106826782227 = 0.10790903121232986 + 1.0 * 6.076197624206543
Epoch 700, val loss: 0.74675452709198
Epoch 710, training loss: 6.175086975097656 = 0.1036088764667511 + 1.0 * 6.071477890014648
Epoch 710, val loss: 0.7530207633972168
Epoch 720, training loss: 6.168971538543701 = 0.09956948459148407 + 1.0 * 6.06940221786499
Epoch 720, val loss: 0.7592676877975464
Epoch 730, training loss: 6.1640424728393555 = 0.09575903415679932 + 1.0 * 6.068283557891846
Epoch 730, val loss: 0.7657018899917603
Epoch 740, training loss: 6.166329860687256 = 0.09216056019067764 + 1.0 * 6.074169158935547
Epoch 740, val loss: 0.7721975445747375
Epoch 750, training loss: 6.159201145172119 = 0.08879636228084564 + 1.0 * 6.070405006408691
Epoch 750, val loss: 0.7785806655883789
Epoch 760, training loss: 6.151736259460449 = 0.0856141597032547 + 1.0 * 6.066122055053711
Epoch 760, val loss: 0.7850073575973511
Epoch 770, training loss: 6.1465582847595215 = 0.08259198814630508 + 1.0 * 6.063966274261475
Epoch 770, val loss: 0.7915496826171875
Epoch 780, training loss: 6.145219326019287 = 0.07972043007612228 + 1.0 * 6.0654988288879395
Epoch 780, val loss: 0.7980893850326538
Epoch 790, training loss: 6.1499810218811035 = 0.07702461630105972 + 1.0 * 6.072956562042236
Epoch 790, val loss: 0.8045664429664612
Epoch 800, training loss: 6.139202117919922 = 0.07450062036514282 + 1.0 * 6.064701557159424
Epoch 800, val loss: 0.8107522130012512
Epoch 810, training loss: 6.133722305297852 = 0.07210204005241394 + 1.0 * 6.061620235443115
Epoch 810, val loss: 0.8170634508132935
Epoch 820, training loss: 6.129770278930664 = 0.06980772316455841 + 1.0 * 6.059962749481201
Epoch 820, val loss: 0.8234424591064453
Epoch 830, training loss: 6.126202583312988 = 0.0676126480102539 + 1.0 * 6.058589935302734
Epoch 830, val loss: 0.829771101474762
Epoch 840, training loss: 6.12526798248291 = 0.06551346927881241 + 1.0 * 6.059754371643066
Epoch 840, val loss: 0.8360695242881775
Epoch 850, training loss: 6.128232002258301 = 0.06352259963750839 + 1.0 * 6.064709186553955
Epoch 850, val loss: 0.8423073887825012
Epoch 860, training loss: 6.120201110839844 = 0.06164536625146866 + 1.0 * 6.058555603027344
Epoch 860, val loss: 0.8482967615127563
Epoch 870, training loss: 6.116201400756836 = 0.05985262989997864 + 1.0 * 6.05634880065918
Epoch 870, val loss: 0.8542510867118835
Epoch 880, training loss: 6.1141133308410645 = 0.058128610253334045 + 1.0 * 6.0559844970703125
Epoch 880, val loss: 0.8602514863014221
Epoch 890, training loss: 6.114017486572266 = 0.05647571012377739 + 1.0 * 6.057541847229004
Epoch 890, val loss: 0.866197943687439
Epoch 900, training loss: 6.1096367835998535 = 0.05488741025328636 + 1.0 * 6.054749488830566
Epoch 900, val loss: 0.8720830678939819
Epoch 910, training loss: 6.111804008483887 = 0.0533607192337513 + 1.0 * 6.058443069458008
Epoch 910, val loss: 0.8779357671737671
Epoch 920, training loss: 6.107930660247803 = 0.0518980510532856 + 1.0 * 6.056032657623291
Epoch 920, val loss: 0.8836129307746887
Epoch 930, training loss: 6.102339744567871 = 0.050500448793172836 + 1.0 * 6.051839351654053
Epoch 930, val loss: 0.8892239332199097
Epoch 940, training loss: 6.100356101989746 = 0.049152079969644547 + 1.0 * 6.051204204559326
Epoch 940, val loss: 0.8948221206665039
Epoch 950, training loss: 6.103489875793457 = 0.047848522663116455 + 1.0 * 6.055641174316406
Epoch 950, val loss: 0.9004129767417908
Epoch 960, training loss: 6.097774028778076 = 0.04659741371870041 + 1.0 * 6.05117654800415
Epoch 960, val loss: 0.9058389663696289
Epoch 970, training loss: 6.094078540802002 = 0.045396964997053146 + 1.0 * 6.048681735992432
Epoch 970, val loss: 0.9111734628677368
Epoch 980, training loss: 6.091719627380371 = 0.04423191398382187 + 1.0 * 6.047487735748291
Epoch 980, val loss: 0.9165633320808411
Epoch 990, training loss: 6.090294361114502 = 0.043098680675029755 + 1.0 * 6.047195911407471
Epoch 990, val loss: 0.9219282865524292
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8118
Flip ASR: 0.7733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.307525634765625 = 1.9337151050567627 + 1.0 * 8.373810768127441
Epoch 0, val loss: 1.9199697971343994
Epoch 10, training loss: 10.297651290893555 = 1.9243651628494263 + 1.0 * 8.373286247253418
Epoch 10, val loss: 1.9113032817840576
Epoch 20, training loss: 10.282925605773926 = 1.9124408960342407 + 1.0 * 8.370484352111816
Epoch 20, val loss: 1.9000047445297241
Epoch 30, training loss: 10.248771667480469 = 1.89558744430542 + 1.0 * 8.353184700012207
Epoch 30, val loss: 1.8840373754501343
Epoch 40, training loss: 10.11000919342041 = 1.873106598854065 + 1.0 * 8.236902236938477
Epoch 40, val loss: 1.8633506298065186
Epoch 50, training loss: 9.49435806274414 = 1.8479031324386597 + 1.0 * 7.64645528793335
Epoch 50, val loss: 1.8401644229888916
Epoch 60, training loss: 8.927953720092773 = 1.8278239965438843 + 1.0 * 7.1001296043396
Epoch 60, val loss: 1.8230061531066895
Epoch 70, training loss: 8.578872680664062 = 1.816455602645874 + 1.0 * 6.762416839599609
Epoch 70, val loss: 1.813416600227356
Epoch 80, training loss: 8.422751426696777 = 1.8046720027923584 + 1.0 * 6.618079662322998
Epoch 80, val loss: 1.8030349016189575
Epoch 90, training loss: 8.313474655151367 = 1.7925066947937012 + 1.0 * 6.520968437194824
Epoch 90, val loss: 1.792359709739685
Epoch 100, training loss: 8.238581657409668 = 1.7809697389602661 + 1.0 * 6.457612037658691
Epoch 100, val loss: 1.7823210954666138
Epoch 110, training loss: 8.178837776184082 = 1.7700318098068237 + 1.0 * 6.408805847167969
Epoch 110, val loss: 1.7726472616195679
Epoch 120, training loss: 8.12519359588623 = 1.758265733718872 + 1.0 * 6.366927623748779
Epoch 120, val loss: 1.7621915340423584
Epoch 130, training loss: 8.076904296875 = 1.7448806762695312 + 1.0 * 6.332024097442627
Epoch 130, val loss: 1.7504292726516724
Epoch 140, training loss: 8.03425121307373 = 1.7288943529129028 + 1.0 * 6.305356979370117
Epoch 140, val loss: 1.7367274761199951
Epoch 150, training loss: 7.994046211242676 = 1.7092812061309814 + 1.0 * 6.284765243530273
Epoch 150, val loss: 1.7204208374023438
Epoch 160, training loss: 7.952054977416992 = 1.6851933002471924 + 1.0 * 6.266861438751221
Epoch 160, val loss: 1.700793743133545
Epoch 170, training loss: 7.908512115478516 = 1.6554994583129883 + 1.0 * 6.253012657165527
Epoch 170, val loss: 1.6766927242279053
Epoch 180, training loss: 7.858875274658203 = 1.6196640729904175 + 1.0 * 6.239211082458496
Epoch 180, val loss: 1.6477105617523193
Epoch 190, training loss: 7.80397891998291 = 1.5769338607788086 + 1.0 * 6.227045059204102
Epoch 190, val loss: 1.6128250360488892
Epoch 200, training loss: 7.744900226593018 = 1.5268718004226685 + 1.0 * 6.218028545379639
Epoch 200, val loss: 1.5719271898269653
Epoch 210, training loss: 7.678514003753662 = 1.4709917306900024 + 1.0 * 6.207522392272949
Epoch 210, val loss: 1.5264019966125488
Epoch 220, training loss: 7.611728668212891 = 1.4103597402572632 + 1.0 * 6.201368808746338
Epoch 220, val loss: 1.4768836498260498
Epoch 230, training loss: 7.540407657623291 = 1.347330093383789 + 1.0 * 6.193077564239502
Epoch 230, val loss: 1.4256705045700073
Epoch 240, training loss: 7.469984531402588 = 1.2836823463439941 + 1.0 * 6.186302185058594
Epoch 240, val loss: 1.374442219734192
Epoch 250, training loss: 7.40058708190918 = 1.2203998565673828 + 1.0 * 6.180187225341797
Epoch 250, val loss: 1.323784351348877
Epoch 260, training loss: 7.33534049987793 = 1.1587674617767334 + 1.0 * 6.176573276519775
Epoch 260, val loss: 1.2750860452651978
Epoch 270, training loss: 7.26883602142334 = 1.1000479459762573 + 1.0 * 6.168787956237793
Epoch 270, val loss: 1.2289994955062866
Epoch 280, training loss: 7.211719512939453 = 1.043738603591919 + 1.0 * 6.167980670928955
Epoch 280, val loss: 1.1853431463241577
Epoch 290, training loss: 7.150925636291504 = 0.9910987019538879 + 1.0 * 6.159826755523682
Epoch 290, val loss: 1.1450228691101074
Epoch 300, training loss: 7.095345973968506 = 0.9414681792259216 + 1.0 * 6.1538777351379395
Epoch 300, val loss: 1.1073724031448364
Epoch 310, training loss: 7.0464324951171875 = 0.8945308327674866 + 1.0 * 6.151901721954346
Epoch 310, val loss: 1.0722509622573853
Epoch 320, training loss: 6.996924877166748 = 0.8507019281387329 + 1.0 * 6.146223068237305
Epoch 320, val loss: 1.0398591756820679
Epoch 330, training loss: 6.954062461853027 = 0.8093358874320984 + 1.0 * 6.144726753234863
Epoch 330, val loss: 1.0099345445632935
Epoch 340, training loss: 6.907962322235107 = 0.7707219123840332 + 1.0 * 6.137240409851074
Epoch 340, val loss: 0.9826114177703857
Epoch 350, training loss: 6.869999885559082 = 0.7343018651008606 + 1.0 * 6.135697841644287
Epoch 350, val loss: 0.9576162695884705
Epoch 360, training loss: 6.830903053283691 = 0.6999325156211853 + 1.0 * 6.130970478057861
Epoch 360, val loss: 0.934881329536438
Epoch 370, training loss: 6.795637130737305 = 0.6671920418739319 + 1.0 * 6.128445148468018
Epoch 370, val loss: 0.9141374826431274
Epoch 380, training loss: 6.760934352874756 = 0.6356290578842163 + 1.0 * 6.12530517578125
Epoch 380, val loss: 0.8950332999229431
Epoch 390, training loss: 6.725595951080322 = 0.6049885749816895 + 1.0 * 6.120607376098633
Epoch 390, val loss: 0.8772677183151245
Epoch 400, training loss: 6.694224834442139 = 0.5748535394668579 + 1.0 * 6.11937141418457
Epoch 400, val loss: 0.8603189587593079
Epoch 410, training loss: 6.664251804351807 = 0.5453448295593262 + 1.0 * 6.1189069747924805
Epoch 410, val loss: 0.8441363573074341
Epoch 420, training loss: 6.629418849945068 = 0.5163647532463074 + 1.0 * 6.113054275512695
Epoch 420, val loss: 0.8285173177719116
Epoch 430, training loss: 6.599639892578125 = 0.48765188455581665 + 1.0 * 6.111988067626953
Epoch 430, val loss: 0.8132874369621277
Epoch 440, training loss: 6.574014186859131 = 0.45964422821998596 + 1.0 * 6.114369869232178
Epoch 440, val loss: 0.7986541390419006
Epoch 450, training loss: 6.5387492179870605 = 0.4326063096523285 + 1.0 * 6.106142997741699
Epoch 450, val loss: 0.7850284576416016
Epoch 460, training loss: 6.5116353034973145 = 0.4063883125782013 + 1.0 * 6.1052470207214355
Epoch 460, val loss: 0.7721749544143677
Epoch 470, training loss: 6.483991622924805 = 0.3812394440174103 + 1.0 * 6.102752208709717
Epoch 470, val loss: 0.7605057954788208
Epoch 480, training loss: 6.458611965179443 = 0.35737094283103943 + 1.0 * 6.101241111755371
Epoch 480, val loss: 0.750344455242157
Epoch 490, training loss: 6.441368579864502 = 0.3347090184688568 + 1.0 * 6.106659412384033
Epoch 490, val loss: 0.7414606213569641
Epoch 500, training loss: 6.4112372398376465 = 0.31360918283462524 + 1.0 * 6.097628116607666
Epoch 500, val loss: 0.7341636419296265
Epoch 510, training loss: 6.388694763183594 = 0.2938157916069031 + 1.0 * 6.094879150390625
Epoch 510, val loss: 0.7283697724342346
Epoch 520, training loss: 6.373277187347412 = 0.27525949478149414 + 1.0 * 6.098017692565918
Epoch 520, val loss: 0.723760187625885
Epoch 530, training loss: 6.351265907287598 = 0.25796785950660706 + 1.0 * 6.093297958374023
Epoch 530, val loss: 0.7203245162963867
Epoch 540, training loss: 6.3341498374938965 = 0.2419021874666214 + 1.0 * 6.092247486114502
Epoch 540, val loss: 0.7180088758468628
Epoch 550, training loss: 6.31846809387207 = 0.22698670625686646 + 1.0 * 6.0914812088012695
Epoch 550, val loss: 0.7166295051574707
Epoch 560, training loss: 6.302883625030518 = 0.2132941633462906 + 1.0 * 6.089589595794678
Epoch 560, val loss: 0.7162002921104431
Epoch 570, training loss: 6.28652811050415 = 0.20060977339744568 + 1.0 * 6.085918426513672
Epoch 570, val loss: 0.7165561318397522
Epoch 580, training loss: 6.274389266967773 = 0.18891021609306335 + 1.0 * 6.085479259490967
Epoch 580, val loss: 0.7174543738365173
Epoch 590, training loss: 6.262211322784424 = 0.17814472317695618 + 1.0 * 6.084066390991211
Epoch 590, val loss: 0.71916264295578
Epoch 600, training loss: 6.248405933380127 = 0.16815407574176788 + 1.0 * 6.080251693725586
Epoch 600, val loss: 0.7214170694351196
Epoch 610, training loss: 6.252795696258545 = 0.15887178480625153 + 1.0 * 6.093924045562744
Epoch 610, val loss: 0.7240140438079834
Epoch 620, training loss: 6.229924201965332 = 0.1503506302833557 + 1.0 * 6.079573631286621
Epoch 620, val loss: 0.7271326780319214
Epoch 630, training loss: 6.218879222869873 = 0.14243507385253906 + 1.0 * 6.076444149017334
Epoch 630, val loss: 0.7307662963867188
Epoch 640, training loss: 6.215119361877441 = 0.13505488634109497 + 1.0 * 6.080064296722412
Epoch 640, val loss: 0.7346142530441284
Epoch 650, training loss: 6.205722808837891 = 0.1282060593366623 + 1.0 * 6.077516555786133
Epoch 650, val loss: 0.7386219501495361
Epoch 660, training loss: 6.1949944496154785 = 0.12185688316822052 + 1.0 * 6.0731377601623535
Epoch 660, val loss: 0.743255615234375
Epoch 670, training loss: 6.186957359313965 = 0.1158975288271904 + 1.0 * 6.071059703826904
Epoch 670, val loss: 0.7479493618011475
Epoch 680, training loss: 6.187626361846924 = 0.11030455678701401 + 1.0 * 6.077322006225586
Epoch 680, val loss: 0.7528978586196899
Epoch 690, training loss: 6.177480697631836 = 0.10509809851646423 + 1.0 * 6.07238245010376
Epoch 690, val loss: 0.7580006122589111
Epoch 700, training loss: 6.1682000160217285 = 0.10022257268428802 + 1.0 * 6.067977428436279
Epoch 700, val loss: 0.7633934020996094
Epoch 710, training loss: 6.1657209396362305 = 0.09563002735376358 + 1.0 * 6.0700907707214355
Epoch 710, val loss: 0.7688000798225403
Epoch 720, training loss: 6.166518688201904 = 0.09133906662464142 + 1.0 * 6.075179576873779
Epoch 720, val loss: 0.7742761969566345
Epoch 730, training loss: 6.151197910308838 = 0.08732369542121887 + 1.0 * 6.063874244689941
Epoch 730, val loss: 0.7800291776657104
Epoch 740, training loss: 6.147219181060791 = 0.08353651314973831 + 1.0 * 6.063682556152344
Epoch 740, val loss: 0.7858627438545227
Epoch 750, training loss: 6.141635894775391 = 0.0799456387758255 + 1.0 * 6.061690330505371
Epoch 750, val loss: 0.7917433381080627
Epoch 760, training loss: 6.147279262542725 = 0.07654698193073273 + 1.0 * 6.070732116699219
Epoch 760, val loss: 0.7977840900421143
Epoch 770, training loss: 6.1356635093688965 = 0.07335549592971802 + 1.0 * 6.062307834625244
Epoch 770, val loss: 0.8036591410636902
Epoch 780, training loss: 6.130594253540039 = 0.07034854590892792 + 1.0 * 6.060245513916016
Epoch 780, val loss: 0.8099368214607239
Epoch 790, training loss: 6.130890846252441 = 0.06749727576971054 + 1.0 * 6.063393592834473
Epoch 790, val loss: 0.816029965877533
Epoch 800, training loss: 6.125320911407471 = 0.06480343639850616 + 1.0 * 6.060517311096191
Epoch 800, val loss: 0.8220389485359192
Epoch 810, training loss: 6.120116233825684 = 0.062260981649160385 + 1.0 * 6.057855129241943
Epoch 810, val loss: 0.8283731937408447
Epoch 820, training loss: 6.12244987487793 = 0.05984603241086006 + 1.0 * 6.062603950500488
Epoch 820, val loss: 0.8344261050224304
Epoch 830, training loss: 6.112207412719727 = 0.05756904557347298 + 1.0 * 6.054638385772705
Epoch 830, val loss: 0.8406251668930054
Epoch 840, training loss: 6.110889434814453 = 0.05540088564157486 + 1.0 * 6.055488586425781
Epoch 840, val loss: 0.8468775749206543
Epoch 850, training loss: 6.107149124145508 = 0.05334057658910751 + 1.0 * 6.053808689117432
Epoch 850, val loss: 0.8528767824172974
Epoch 860, training loss: 6.106473922729492 = 0.0513901449739933 + 1.0 * 6.055083751678467
Epoch 860, val loss: 0.8590695858001709
Epoch 870, training loss: 6.100025177001953 = 0.04954056069254875 + 1.0 * 6.050484657287598
Epoch 870, val loss: 0.8651586771011353
Epoch 880, training loss: 6.099560260772705 = 0.04777676984667778 + 1.0 * 6.051783561706543
Epoch 880, val loss: 0.8712189793586731
Epoch 890, training loss: 6.099895000457764 = 0.046099111437797546 + 1.0 * 6.05379581451416
Epoch 890, val loss: 0.8771247863769531
Epoch 900, training loss: 6.092907428741455 = 0.044515300542116165 + 1.0 * 6.048392295837402
Epoch 900, val loss: 0.8832260966300964
Epoch 910, training loss: 6.096002578735352 = 0.043002426624298096 + 1.0 * 6.052999973297119
Epoch 910, val loss: 0.8891515135765076
Epoch 920, training loss: 6.091093063354492 = 0.04156321659684181 + 1.0 * 6.049530029296875
Epoch 920, val loss: 0.8949344754219055
Epoch 930, training loss: 6.086589813232422 = 0.040196098387241364 + 1.0 * 6.046393871307373
Epoch 930, val loss: 0.9008916616439819
Epoch 940, training loss: 6.084874629974365 = 0.03888415917754173 + 1.0 * 6.045990467071533
Epoch 940, val loss: 0.9066725969314575
Epoch 950, training loss: 6.086821556091309 = 0.03763037919998169 + 1.0 * 6.049190998077393
Epoch 950, val loss: 0.9123033881187439
Epoch 960, training loss: 6.081660747528076 = 0.03644184023141861 + 1.0 * 6.0452189445495605
Epoch 960, val loss: 0.9178423881530762
Epoch 970, training loss: 6.079139232635498 = 0.03531283512711525 + 1.0 * 6.043826580047607
Epoch 970, val loss: 0.9235503673553467
Epoch 980, training loss: 6.079738140106201 = 0.03422967717051506 + 1.0 * 6.04550838470459
Epoch 980, val loss: 0.9291558265686035
Epoch 990, training loss: 6.07639741897583 = 0.033192846924066544 + 1.0 * 6.0432047843933105
Epoch 990, val loss: 0.9344441294670105
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7638
Flip ASR: 0.7156/225 nodes
The final ASR:0.68512, 0.14656, Accuracy:0.82716, 0.00761
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10510])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98278, 0.00460, Accuracy:0.83086, 0.00349
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.316181182861328 = 1.9422831535339355 + 1.0 * 8.373897552490234
Epoch 0, val loss: 1.9314597845077515
Epoch 10, training loss: 10.305135726928711 = 1.9315192699432373 + 1.0 * 8.373616218566895
Epoch 10, val loss: 1.921738862991333
Epoch 20, training loss: 10.290290832519531 = 1.918259859085083 + 1.0 * 8.372031211853027
Epoch 20, val loss: 1.9095816612243652
Epoch 30, training loss: 10.259627342224121 = 1.9000657796859741 + 1.0 * 8.359561920166016
Epoch 30, val loss: 1.893024206161499
Epoch 40, training loss: 10.09840202331543 = 1.8765133619308472 + 1.0 * 8.221888542175293
Epoch 40, val loss: 1.8723639249801636
Epoch 50, training loss: 9.19953441619873 = 1.8535373210906982 + 1.0 * 7.345997333526611
Epoch 50, val loss: 1.8528313636779785
Epoch 60, training loss: 8.84543228149414 = 1.839704155921936 + 1.0 * 7.005727767944336
Epoch 60, val loss: 1.8413234949111938
Epoch 70, training loss: 8.638543128967285 = 1.8281207084655762 + 1.0 * 6.810422420501709
Epoch 70, val loss: 1.8313171863555908
Epoch 80, training loss: 8.475394248962402 = 1.8158493041992188 + 1.0 * 6.659544944763184
Epoch 80, val loss: 1.821696400642395
Epoch 90, training loss: 8.357879638671875 = 1.8049793243408203 + 1.0 * 6.5528998374938965
Epoch 90, val loss: 1.8134205341339111
Epoch 100, training loss: 8.277918815612793 = 1.7950690984725952 + 1.0 * 6.482849597930908
Epoch 100, val loss: 1.805867075920105
Epoch 110, training loss: 8.215387344360352 = 1.7853279113769531 + 1.0 * 6.430059432983398
Epoch 110, val loss: 1.797856092453003
Epoch 120, training loss: 8.162277221679688 = 1.7750478982925415 + 1.0 * 6.3872294425964355
Epoch 120, val loss: 1.7890042066574097
Epoch 130, training loss: 8.11502742767334 = 1.7636268138885498 + 1.0 * 6.351400852203369
Epoch 130, val loss: 1.779124140739441
Epoch 140, training loss: 8.073081016540527 = 1.7505152225494385 + 1.0 * 6.32256555557251
Epoch 140, val loss: 1.7679522037506104
Epoch 150, training loss: 8.032567024230957 = 1.7352463006973267 + 1.0 * 6.297320365905762
Epoch 150, val loss: 1.755278468132019
Epoch 160, training loss: 7.994214057922363 = 1.7170751094818115 + 1.0 * 6.277139186859131
Epoch 160, val loss: 1.7403512001037598
Epoch 170, training loss: 7.955488681793213 = 1.6952077150344849 + 1.0 * 6.260281085968018
Epoch 170, val loss: 1.7224982976913452
Epoch 180, training loss: 7.914268493652344 = 1.6686586141586304 + 1.0 * 6.245609760284424
Epoch 180, val loss: 1.7008540630340576
Epoch 190, training loss: 7.871604919433594 = 1.6365081071853638 + 1.0 * 6.2350969314575195
Epoch 190, val loss: 1.6745949983596802
Epoch 200, training loss: 7.822773456573486 = 1.5987147092819214 + 1.0 * 6.224058628082275
Epoch 200, val loss: 1.6437143087387085
Epoch 210, training loss: 7.771366596221924 = 1.5554357767105103 + 1.0 * 6.215930938720703
Epoch 210, val loss: 1.6083347797393799
Epoch 220, training loss: 7.716655731201172 = 1.5084275007247925 + 1.0 * 6.20822811126709
Epoch 220, val loss: 1.5702993869781494
Epoch 230, training loss: 7.659122467041016 = 1.4590821266174316 + 1.0 * 6.200040340423584
Epoch 230, val loss: 1.5304627418518066
Epoch 240, training loss: 7.601533889770508 = 1.4079172611236572 + 1.0 * 6.19361686706543
Epoch 240, val loss: 1.4891356229782104
Epoch 250, training loss: 7.546185493469238 = 1.3570137023925781 + 1.0 * 6.18917179107666
Epoch 250, val loss: 1.4488329887390137
Epoch 260, training loss: 7.489140510559082 = 1.307407259941101 + 1.0 * 6.181733131408691
Epoch 260, val loss: 1.409918189048767
Epoch 270, training loss: 7.434015274047852 = 1.2587355375289917 + 1.0 * 6.17527961730957
Epoch 270, val loss: 1.3722394704818726
Epoch 280, training loss: 7.381248474121094 = 1.2109438180923462 + 1.0 * 6.170304775238037
Epoch 280, val loss: 1.3357282876968384
Epoch 290, training loss: 7.3302693367004395 = 1.1644097566604614 + 1.0 * 6.165859699249268
Epoch 290, val loss: 1.3006640672683716
Epoch 300, training loss: 7.279558181762695 = 1.1184760332107544 + 1.0 * 6.1610822677612305
Epoch 300, val loss: 1.2666113376617432
Epoch 310, training loss: 7.229503631591797 = 1.0726184844970703 + 1.0 * 6.156885147094727
Epoch 310, val loss: 1.2329045534133911
Epoch 320, training loss: 7.1806793212890625 = 1.0271975994110107 + 1.0 * 6.153481960296631
Epoch 320, val loss: 1.1997531652450562
Epoch 330, training loss: 7.130892753601074 = 0.9825335741043091 + 1.0 * 6.148359298706055
Epoch 330, val loss: 1.1672900915145874
Epoch 340, training loss: 7.082581996917725 = 0.9384260177612305 + 1.0 * 6.144155979156494
Epoch 340, val loss: 1.1352583169937134
Epoch 350, training loss: 7.0450286865234375 = 0.8951024413108826 + 1.0 * 6.14992618560791
Epoch 350, val loss: 1.1040407419204712
Epoch 360, training loss: 6.993846893310547 = 0.8535259366035461 + 1.0 * 6.140320777893066
Epoch 360, val loss: 1.0745185613632202
Epoch 370, training loss: 6.948820114135742 = 0.8134167194366455 + 1.0 * 6.135403633117676
Epoch 370, val loss: 1.0464752912521362
Epoch 380, training loss: 6.907325744628906 = 0.7746633291244507 + 1.0 * 6.132662296295166
Epoch 380, val loss: 1.0198183059692383
Epoch 390, training loss: 6.866331577301025 = 0.7374200224876404 + 1.0 * 6.12891149520874
Epoch 390, val loss: 0.9951403737068176
Epoch 400, training loss: 6.827935695648193 = 0.7012344002723694 + 1.0 * 6.126701354980469
Epoch 400, val loss: 0.9717795848846436
Epoch 410, training loss: 6.7917256355285645 = 0.6661308407783508 + 1.0 * 6.125594615936279
Epoch 410, val loss: 0.9498432278633118
Epoch 420, training loss: 6.7542314529418945 = 0.6323995590209961 + 1.0 * 6.121831893920898
Epoch 420, val loss: 0.9296925663948059
Epoch 430, training loss: 6.7173991203308105 = 0.599876344203949 + 1.0 * 6.117522716522217
Epoch 430, val loss: 0.9109838008880615
Epoch 440, training loss: 6.690261363983154 = 0.5685712099075317 + 1.0 * 6.121690273284912
Epoch 440, val loss: 0.8936455845832825
Epoch 450, training loss: 6.652207851409912 = 0.5389693379402161 + 1.0 * 6.113238334655762
Epoch 450, val loss: 0.8781846761703491
Epoch 460, training loss: 6.622312068939209 = 0.5110187530517578 + 1.0 * 6.111293315887451
Epoch 460, val loss: 0.8647419810295105
Epoch 470, training loss: 6.596400737762451 = 0.48453670740127563 + 1.0 * 6.11186408996582
Epoch 470, val loss: 0.8529117703437805
Epoch 480, training loss: 6.57687520980835 = 0.45969244837760925 + 1.0 * 6.117182731628418
Epoch 480, val loss: 0.8428856730461121
Epoch 490, training loss: 6.544754981994629 = 0.43654966354370117 + 1.0 * 6.108205318450928
Epoch 490, val loss: 0.8348779082298279
Epoch 500, training loss: 6.518270492553711 = 0.4147905707359314 + 1.0 * 6.103479862213135
Epoch 500, val loss: 0.8282901048660278
Epoch 510, training loss: 6.496298313140869 = 0.39420077204704285 + 1.0 * 6.102097511291504
Epoch 510, val loss: 0.8228965401649475
Epoch 520, training loss: 6.474097728729248 = 0.3748165965080261 + 1.0 * 6.099281311035156
Epoch 520, val loss: 0.8186039924621582
Epoch 530, training loss: 6.4627180099487305 = 0.35667353868484497 + 1.0 * 6.106044292449951
Epoch 530, val loss: 0.8154135942459106
Epoch 540, training loss: 6.438014984130859 = 0.33964982628822327 + 1.0 * 6.098365306854248
Epoch 540, val loss: 0.813018798828125
Epoch 550, training loss: 6.421206474304199 = 0.3235754668712616 + 1.0 * 6.097630977630615
Epoch 550, val loss: 0.8113288283348083
Epoch 560, training loss: 6.403331756591797 = 0.30842143297195435 + 1.0 * 6.094910144805908
Epoch 560, val loss: 0.8100716471672058
Epoch 570, training loss: 6.388209819793701 = 0.29413554072380066 + 1.0 * 6.094074249267578
Epoch 570, val loss: 0.8094820380210876
Epoch 580, training loss: 6.373266220092773 = 0.2806217074394226 + 1.0 * 6.092644691467285
Epoch 580, val loss: 0.8093200325965881
Epoch 590, training loss: 6.357904434204102 = 0.2678211033344269 + 1.0 * 6.090083122253418
Epoch 590, val loss: 0.8096425533294678
Epoch 600, training loss: 6.344034194946289 = 0.2556191086769104 + 1.0 * 6.088415145874023
Epoch 600, val loss: 0.8103689551353455
Epoch 610, training loss: 6.336459159851074 = 0.2439556121826172 + 1.0 * 6.092503547668457
Epoch 610, val loss: 0.8114365339279175
Epoch 620, training loss: 6.322746276855469 = 0.2328277975320816 + 1.0 * 6.089918613433838
Epoch 620, val loss: 0.8127428293228149
Epoch 630, training loss: 6.311784744262695 = 0.22217346727848053 + 1.0 * 6.089611053466797
Epoch 630, val loss: 0.8144309520721436
Epoch 640, training loss: 6.296169757843018 = 0.2119482159614563 + 1.0 * 6.084221363067627
Epoch 640, val loss: 0.8164116740226746
Epoch 650, training loss: 6.283893585205078 = 0.20201729238033295 + 1.0 * 6.081876277923584
Epoch 650, val loss: 0.8186036348342896
Epoch 660, training loss: 6.273828029632568 = 0.19232982397079468 + 1.0 * 6.081498146057129
Epoch 660, val loss: 0.8209880590438843
Epoch 670, training loss: 6.26908016204834 = 0.18291734158992767 + 1.0 * 6.08616304397583
Epoch 670, val loss: 0.8233149647712708
Epoch 680, training loss: 6.254087448120117 = 0.1738363802433014 + 1.0 * 6.080251216888428
Epoch 680, val loss: 0.8261924386024475
Epoch 690, training loss: 6.242053508758545 = 0.1650124490261078 + 1.0 * 6.077041149139404
Epoch 690, val loss: 0.8292211294174194
Epoch 700, training loss: 6.232449531555176 = 0.156428724527359 + 1.0 * 6.07602071762085
Epoch 700, val loss: 0.8323363065719604
Epoch 710, training loss: 6.234020709991455 = 0.14813312888145447 + 1.0 * 6.085887432098389
Epoch 710, val loss: 0.8357124328613281
Epoch 720, training loss: 6.216625213623047 = 0.1401931792497635 + 1.0 * 6.076432228088379
Epoch 720, val loss: 0.8393040299415588
Epoch 730, training loss: 6.209038257598877 = 0.1326187402009964 + 1.0 * 6.076419353485107
Epoch 730, val loss: 0.8431886434555054
Epoch 740, training loss: 6.197791576385498 = 0.12541817128658295 + 1.0 * 6.072373390197754
Epoch 740, val loss: 0.8472890257835388
Epoch 750, training loss: 6.191762924194336 = 0.11857884377241135 + 1.0 * 6.073184013366699
Epoch 750, val loss: 0.8516570925712585
Epoch 760, training loss: 6.183766841888428 = 0.11212237179279327 + 1.0 * 6.071644306182861
Epoch 760, val loss: 0.8561118245124817
Epoch 770, training loss: 6.186966419219971 = 0.10605672001838684 + 1.0 * 6.080909729003906
Epoch 770, val loss: 0.8607110381126404
Epoch 780, training loss: 6.17115592956543 = 0.10041153430938721 + 1.0 * 6.070744514465332
Epoch 780, val loss: 0.865507960319519
Epoch 790, training loss: 6.1629958152771 = 0.09514134377241135 + 1.0 * 6.067854404449463
Epoch 790, val loss: 0.8706468343734741
Epoch 800, training loss: 6.155820846557617 = 0.09020129591226578 + 1.0 * 6.065619468688965
Epoch 800, val loss: 0.8757337927818298
Epoch 810, training loss: 6.152498722076416 = 0.08557109534740448 + 1.0 * 6.066927433013916
Epoch 810, val loss: 0.8809770941734314
Epoch 820, training loss: 6.149238109588623 = 0.08126021176576614 + 1.0 * 6.0679779052734375
Epoch 820, val loss: 0.8862631916999817
Epoch 830, training loss: 6.143838882446289 = 0.07726381719112396 + 1.0 * 6.066575050354004
Epoch 830, val loss: 0.8916849493980408
Epoch 840, training loss: 6.137332439422607 = 0.07354370504617691 + 1.0 * 6.063788890838623
Epoch 840, val loss: 0.8972461223602295
Epoch 850, training loss: 6.133588790893555 = 0.07007075101137161 + 1.0 * 6.063518047332764
Epoch 850, val loss: 0.902726411819458
Epoch 860, training loss: 6.1308512687683105 = 0.0668247863650322 + 1.0 * 6.064026355743408
Epoch 860, val loss: 0.9082289338111877
Epoch 870, training loss: 6.12457799911499 = 0.06379229575395584 + 1.0 * 6.06078577041626
Epoch 870, val loss: 0.9139018058776855
Epoch 880, training loss: 6.124022006988525 = 0.060946397483348846 + 1.0 * 6.063075542449951
Epoch 880, val loss: 0.9195149540901184
Epoch 890, training loss: 6.118501663208008 = 0.05828714370727539 + 1.0 * 6.060214519500732
Epoch 890, val loss: 0.925101101398468
Epoch 900, training loss: 6.115671634674072 = 0.055799923837184906 + 1.0 * 6.059871673583984
Epoch 900, val loss: 0.9308238625526428
Epoch 910, training loss: 6.109940052032471 = 0.05346648395061493 + 1.0 * 6.056473731994629
Epoch 910, val loss: 0.936520516872406
Epoch 920, training loss: 6.107970714569092 = 0.05126801133155823 + 1.0 * 6.056702613830566
Epoch 920, val loss: 0.9422657489776611
Epoch 930, training loss: 6.111268520355225 = 0.04919418692588806 + 1.0 * 6.062074184417725
Epoch 930, val loss: 0.9478355646133423
Epoch 940, training loss: 6.103260040283203 = 0.04725167527794838 + 1.0 * 6.056008338928223
Epoch 940, val loss: 0.953557014465332
Epoch 950, training loss: 6.101961135864258 = 0.045420560985803604 + 1.0 * 6.056540489196777
Epoch 950, val loss: 0.9592606425285339
Epoch 960, training loss: 6.0973591804504395 = 0.043693967163562775 + 1.0 * 6.0536651611328125
Epoch 960, val loss: 0.9648017883300781
Epoch 970, training loss: 6.094452381134033 = 0.042062971740961075 + 1.0 * 6.052389621734619
Epoch 970, val loss: 0.9704341292381287
Epoch 980, training loss: 6.092678070068359 = 0.04051632434129715 + 1.0 * 6.052161693572998
Epoch 980, val loss: 0.9759941101074219
Epoch 990, training loss: 6.099062919616699 = 0.03904811292886734 + 1.0 * 6.060014724731445
Epoch 990, val loss: 0.98137366771698
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.7269
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320703506469727 = 1.9468721151351929 + 1.0 * 8.373831748962402
Epoch 0, val loss: 1.9509466886520386
Epoch 10, training loss: 10.309823036193848 = 1.9364076852798462 + 1.0 * 8.373414993286133
Epoch 10, val loss: 1.9406017065048218
Epoch 20, training loss: 10.294633865356445 = 1.92372465133667 + 1.0 * 8.370908737182617
Epoch 20, val loss: 1.927613615989685
Epoch 30, training loss: 10.260184288024902 = 1.906596064567566 + 1.0 * 8.353588104248047
Epoch 30, val loss: 1.9098790884017944
Epoch 40, training loss: 10.105043411254883 = 1.8855304718017578 + 1.0 * 8.219512939453125
Epoch 40, val loss: 1.8885564804077148
Epoch 50, training loss: 9.287337303161621 = 1.865513563156128 + 1.0 * 7.421823501586914
Epoch 50, val loss: 1.868683934211731
Epoch 60, training loss: 8.803651809692383 = 1.8512389659881592 + 1.0 * 6.952413082122803
Epoch 60, val loss: 1.8544023036956787
Epoch 70, training loss: 8.5944242477417 = 1.8402620553970337 + 1.0 * 6.754162311553955
Epoch 70, val loss: 1.8424603939056396
Epoch 80, training loss: 8.449769020080566 = 1.8279894590377808 + 1.0 * 6.621779918670654
Epoch 80, val loss: 1.8299213647842407
Epoch 90, training loss: 8.326299667358398 = 1.8161733150482178 + 1.0 * 6.510126113891602
Epoch 90, val loss: 1.817867636680603
Epoch 100, training loss: 8.241202354431152 = 1.8043452501296997 + 1.0 * 6.436856746673584
Epoch 100, val loss: 1.8058009147644043
Epoch 110, training loss: 8.173888206481934 = 1.7910958528518677 + 1.0 * 6.382791996002197
Epoch 110, val loss: 1.7922958135604858
Epoch 120, training loss: 8.120277404785156 = 1.777140736579895 + 1.0 * 6.343136787414551
Epoch 120, val loss: 1.7781192064285278
Epoch 130, training loss: 8.077432632446289 = 1.762807011604309 + 1.0 * 6.3146257400512695
Epoch 130, val loss: 1.7638088464736938
Epoch 140, training loss: 8.038108825683594 = 1.7474886178970337 + 1.0 * 6.290619850158691
Epoch 140, val loss: 1.7491366863250732
Epoch 150, training loss: 8.000322341918945 = 1.7303309440612793 + 1.0 * 6.269990921020508
Epoch 150, val loss: 1.7334887981414795
Epoch 160, training loss: 7.9633917808532715 = 1.7105613946914673 + 1.0 * 6.252830505371094
Epoch 160, val loss: 1.7162922620773315
Epoch 170, training loss: 7.923977851867676 = 1.6873514652252197 + 1.0 * 6.236626148223877
Epoch 170, val loss: 1.6969215869903564
Epoch 180, training loss: 7.883614540100098 = 1.6597871780395508 + 1.0 * 6.223827362060547
Epoch 180, val loss: 1.6744810342788696
Epoch 190, training loss: 7.838773727416992 = 1.626551866531372 + 1.0 * 6.212221622467041
Epoch 190, val loss: 1.6478525400161743
Epoch 200, training loss: 7.789773464202881 = 1.5867432355880737 + 1.0 * 6.203030109405518
Epoch 200, val loss: 1.6163815259933472
Epoch 210, training loss: 7.736824035644531 = 1.540766954421997 + 1.0 * 6.196056842803955
Epoch 210, val loss: 1.580039620399475
Epoch 220, training loss: 7.677338600158691 = 1.488150954246521 + 1.0 * 6.189187526702881
Epoch 220, val loss: 1.5383623838424683
Epoch 230, training loss: 7.612542152404785 = 1.429557204246521 + 1.0 * 6.182984828948975
Epoch 230, val loss: 1.491931438446045
Epoch 240, training loss: 7.55303430557251 = 1.3673804998397827 + 1.0 * 6.1856536865234375
Epoch 240, val loss: 1.4426792860031128
Epoch 250, training loss: 7.47998046875 = 1.3056259155273438 + 1.0 * 6.174354553222656
Epoch 250, val loss: 1.3937623500823975
Epoch 260, training loss: 7.414807319641113 = 1.2453266382217407 + 1.0 * 6.169480800628662
Epoch 260, val loss: 1.345916509628296
Epoch 270, training loss: 7.353584289550781 = 1.1875501871109009 + 1.0 * 6.16603422164917
Epoch 270, val loss: 1.3003405332565308
Epoch 280, training loss: 7.298532485961914 = 1.1330540180206299 + 1.0 * 6.165478229522705
Epoch 280, val loss: 1.2578883171081543
Epoch 290, training loss: 7.241401195526123 = 1.0826719999313354 + 1.0 * 6.158729076385498
Epoch 290, val loss: 1.2189276218414307
Epoch 300, training loss: 7.186325550079346 = 1.0352636575698853 + 1.0 * 6.15106201171875
Epoch 300, val loss: 1.1825768947601318
Epoch 310, training loss: 7.1370368003845215 = 0.9898865818977356 + 1.0 * 6.147150039672852
Epoch 310, val loss: 1.1479352712631226
Epoch 320, training loss: 7.092523574829102 = 0.9465291500091553 + 1.0 * 6.145994186401367
Epoch 320, val loss: 1.1149924993515015
Epoch 330, training loss: 7.046117305755615 = 0.9051727652549744 + 1.0 * 6.140944480895996
Epoch 330, val loss: 1.0837609767913818
Epoch 340, training loss: 7.0005974769592285 = 0.864896297454834 + 1.0 * 6.1357011795043945
Epoch 340, val loss: 1.0533427000045776
Epoch 350, training loss: 6.971330642700195 = 0.8254249095916748 + 1.0 * 6.1459059715271
Epoch 350, val loss: 1.023762822151184
Epoch 360, training loss: 6.921268463134766 = 0.7877079844474792 + 1.0 * 6.133560657501221
Epoch 360, val loss: 0.9955319762229919
Epoch 370, training loss: 6.87827205657959 = 0.7515101432800293 + 1.0 * 6.1267619132995605
Epoch 370, val loss: 0.9687330722808838
Epoch 380, training loss: 6.840515613555908 = 0.7168155312538147 + 1.0 * 6.123700141906738
Epoch 380, val loss: 0.9433449506759644
Epoch 390, training loss: 6.805934906005859 = 0.6839486360549927 + 1.0 * 6.121986389160156
Epoch 390, val loss: 0.9196375012397766
Epoch 400, training loss: 6.773824691772461 = 0.6536574959754944 + 1.0 * 6.120167255401611
Epoch 400, val loss: 0.8981289863586426
Epoch 410, training loss: 6.742880344390869 = 0.626113772392273 + 1.0 * 6.116766452789307
Epoch 410, val loss: 0.8791214823722839
Epoch 420, training loss: 6.71380090713501 = 0.6005498766899109 + 1.0 * 6.113251209259033
Epoch 420, val loss: 0.8619298338890076
Epoch 430, training loss: 6.686633586883545 = 0.5766752362251282 + 1.0 * 6.109958171844482
Epoch 430, val loss: 0.8463521003723145
Epoch 440, training loss: 6.673994064331055 = 0.5542912483215332 + 1.0 * 6.1197028160095215
Epoch 440, val loss: 0.8324446678161621
Epoch 450, training loss: 6.6420063972473145 = 0.5334237217903137 + 1.0 * 6.108582496643066
Epoch 450, val loss: 0.8200212121009827
Epoch 460, training loss: 6.619506359100342 = 0.5137656927108765 + 1.0 * 6.105740547180176
Epoch 460, val loss: 0.8089315295219421
Epoch 470, training loss: 6.597860336303711 = 0.4950786232948303 + 1.0 * 6.102781772613525
Epoch 470, val loss: 0.7988989353179932
Epoch 480, training loss: 6.5773515701293945 = 0.47726932168006897 + 1.0 * 6.1000823974609375
Epoch 480, val loss: 0.7899446487426758
Epoch 490, training loss: 6.558516502380371 = 0.4601312577724457 + 1.0 * 6.098385334014893
Epoch 490, val loss: 0.7818013429641724
Epoch 500, training loss: 6.538804531097412 = 0.4435902237892151 + 1.0 * 6.095214366912842
Epoch 500, val loss: 0.7743915319442749
Epoch 510, training loss: 6.520908832550049 = 0.4275319278240204 + 1.0 * 6.093377113342285
Epoch 510, val loss: 0.7675718665122986
Epoch 520, training loss: 6.5055670738220215 = 0.41180795431137085 + 1.0 * 6.093759059906006
Epoch 520, val loss: 0.7613142728805542
Epoch 530, training loss: 6.488213062286377 = 0.39648371934890747 + 1.0 * 6.091729164123535
Epoch 530, val loss: 0.7555694580078125
Epoch 540, training loss: 6.469918727874756 = 0.38149935007095337 + 1.0 * 6.088419437408447
Epoch 540, val loss: 0.750343382358551
Epoch 550, training loss: 6.454414367675781 = 0.36673709750175476 + 1.0 * 6.087677478790283
Epoch 550, val loss: 0.7455736398696899
Epoch 560, training loss: 6.438130855560303 = 0.3522104322910309 + 1.0 * 6.085920333862305
Epoch 560, val loss: 0.741218090057373
Epoch 570, training loss: 6.421830654144287 = 0.33792921900749207 + 1.0 * 6.083901405334473
Epoch 570, val loss: 0.73734450340271
Epoch 580, training loss: 6.405242443084717 = 0.32381999492645264 + 1.0 * 6.081422328948975
Epoch 580, val loss: 0.7339187264442444
Epoch 590, training loss: 6.39772891998291 = 0.309850811958313 + 1.0 * 6.087878227233887
Epoch 590, val loss: 0.7307885885238647
Epoch 600, training loss: 6.378775119781494 = 0.29611894488334656 + 1.0 * 6.082656383514404
Epoch 600, val loss: 0.7280611395835876
Epoch 610, training loss: 6.361381530761719 = 0.28262844681739807 + 1.0 * 6.0787529945373535
Epoch 610, val loss: 0.7257087230682373
Epoch 620, training loss: 6.345580101013184 = 0.26927557587623596 + 1.0 * 6.0763044357299805
Epoch 620, val loss: 0.723677933216095
Epoch 630, training loss: 6.342902660369873 = 0.25608929991722107 + 1.0 * 6.086813449859619
Epoch 630, val loss: 0.7219544649124146
Epoch 640, training loss: 6.318758487701416 = 0.24320754408836365 + 1.0 * 6.0755510330200195
Epoch 640, val loss: 0.7206568121910095
Epoch 650, training loss: 6.3089213371276855 = 0.23064061999320984 + 1.0 * 6.078280925750732
Epoch 650, val loss: 0.7197681665420532
Epoch 660, training loss: 6.290999889373779 = 0.21844898164272308 + 1.0 * 6.0725507736206055
Epoch 660, val loss: 0.71920245885849
Epoch 670, training loss: 6.277699947357178 = 0.20655517280101776 + 1.0 * 6.0711445808410645
Epoch 670, val loss: 0.7189983129501343
Epoch 680, training loss: 6.2656049728393555 = 0.19503234326839447 + 1.0 * 6.070572853088379
Epoch 680, val loss: 0.7192771434783936
Epoch 690, training loss: 6.256777763366699 = 0.18384969234466553 + 1.0 * 6.072927951812744
Epoch 690, val loss: 0.7199700474739075
Epoch 700, training loss: 6.243679523468018 = 0.1730995923280716 + 1.0 * 6.070580005645752
Epoch 700, val loss: 0.7209802865982056
Epoch 710, training loss: 6.23081636428833 = 0.1628614366054535 + 1.0 * 6.067955017089844
Epoch 710, val loss: 0.7223761081695557
Epoch 720, training loss: 6.226757049560547 = 0.15325886011123657 + 1.0 * 6.073498249053955
Epoch 720, val loss: 0.724102258682251
Epoch 730, training loss: 6.213558673858643 = 0.14444778859615326 + 1.0 * 6.069110870361328
Epoch 730, val loss: 0.7261644601821899
Epoch 740, training loss: 6.200913906097412 = 0.13636241853237152 + 1.0 * 6.06455135345459
Epoch 740, val loss: 0.7286287546157837
Epoch 750, training loss: 6.192521572113037 = 0.12886321544647217 + 1.0 * 6.063658237457275
Epoch 750, val loss: 0.7313671112060547
Epoch 760, training loss: 6.187568187713623 = 0.12192541360855103 + 1.0 * 6.065642833709717
Epoch 760, val loss: 0.7342920899391174
Epoch 770, training loss: 6.178761005401611 = 0.1155071035027504 + 1.0 * 6.063253879547119
Epoch 770, val loss: 0.737484335899353
Epoch 780, training loss: 6.174939155578613 = 0.1095656007528305 + 1.0 * 6.065373420715332
Epoch 780, val loss: 0.7410069704055786
Epoch 790, training loss: 6.164577960968018 = 0.10405165702104568 + 1.0 * 6.060526371002197
Epoch 790, val loss: 0.7445717453956604
Epoch 800, training loss: 6.157647132873535 = 0.09891200810670853 + 1.0 * 6.058734893798828
Epoch 800, val loss: 0.7482975125312805
Epoch 810, training loss: 6.150357246398926 = 0.09410124272108078 + 1.0 * 6.05625581741333
Epoch 810, val loss: 0.7521685361862183
Epoch 820, training loss: 6.152521133422852 = 0.08958597481250763 + 1.0 * 6.0629353523254395
Epoch 820, val loss: 0.7561473846435547
Epoch 830, training loss: 6.147487163543701 = 0.08537661284208298 + 1.0 * 6.062110424041748
Epoch 830, val loss: 0.7598963975906372
Epoch 840, training loss: 6.135258674621582 = 0.08143860846757889 + 1.0 * 6.0538201332092285
Epoch 840, val loss: 0.7638921737670898
Epoch 850, training loss: 6.132897853851318 = 0.07771569490432739 + 1.0 * 6.055181980133057
Epoch 850, val loss: 0.7678722143173218
Epoch 860, training loss: 6.128615379333496 = 0.07420802116394043 + 1.0 * 6.054407119750977
Epoch 860, val loss: 0.7717879414558411
Epoch 870, training loss: 6.123289585113525 = 0.0709121897816658 + 1.0 * 6.052377223968506
Epoch 870, val loss: 0.7756825685501099
Epoch 880, training loss: 6.120378017425537 = 0.06779877841472626 + 1.0 * 6.052579402923584
Epoch 880, val loss: 0.7796132564544678
Epoch 890, training loss: 6.1158857345581055 = 0.06484301388263702 + 1.0 * 6.051042556762695
Epoch 890, val loss: 0.7835419178009033
Epoch 900, training loss: 6.112292766571045 = 0.06204938888549805 + 1.0 * 6.050243377685547
Epoch 900, val loss: 0.7874912619590759
Epoch 910, training loss: 6.113259315490723 = 0.059401579201221466 + 1.0 * 6.053857803344727
Epoch 910, val loss: 0.7914109826087952
Epoch 920, training loss: 6.108248710632324 = 0.056902430951595306 + 1.0 * 6.051346302032471
Epoch 920, val loss: 0.7952380180358887
Epoch 930, training loss: 6.102508068084717 = 0.05454254522919655 + 1.0 * 6.0479655265808105
Epoch 930, val loss: 0.7990249395370483
Epoch 940, training loss: 6.099889755249023 = 0.05230800062417984 + 1.0 * 6.047581672668457
Epoch 940, val loss: 0.8027327656745911
Epoch 950, training loss: 6.097832679748535 = 0.05018196254968643 + 1.0 * 6.0476508140563965
Epoch 950, val loss: 0.8064568042755127
Epoch 960, training loss: 6.096163749694824 = 0.048166897147893906 + 1.0 * 6.047996997833252
Epoch 960, val loss: 0.8101326823234558
Epoch 970, training loss: 6.092435836791992 = 0.04626404121518135 + 1.0 * 6.04617166519165
Epoch 970, val loss: 0.8136988282203674
Epoch 980, training loss: 6.088865280151367 = 0.044464364647865295 + 1.0 * 6.044400691986084
Epoch 980, val loss: 0.8172129988670349
Epoch 990, training loss: 6.0857930183410645 = 0.04276244714856148 + 1.0 * 6.043030738830566
Epoch 990, val loss: 0.8207546472549438
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.4354
Flip ASR: 0.3822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.328044891357422 = 1.9545652866363525 + 1.0 * 8.373479843139648
Epoch 0, val loss: 1.963820219039917
Epoch 10, training loss: 10.313447952270508 = 1.9431992769241333 + 1.0 * 8.370248794555664
Epoch 10, val loss: 1.9513819217681885
Epoch 20, training loss: 10.29526138305664 = 1.929293155670166 + 1.0 * 8.365968704223633
Epoch 20, val loss: 1.9345223903656006
Epoch 30, training loss: 10.26292610168457 = 1.9110150337219238 + 1.0 * 8.351911544799805
Epoch 30, val loss: 1.9123677015304565
Epoch 40, training loss: 10.112859725952148 = 1.89058518409729 + 1.0 * 8.222274780273438
Epoch 40, val loss: 1.8896147012710571
Epoch 50, training loss: 9.489869117736816 = 1.871962547302246 + 1.0 * 7.61790657043457
Epoch 50, val loss: 1.8696879148483276
Epoch 60, training loss: 9.015488624572754 = 1.8559532165527344 + 1.0 * 7.1595354080200195
Epoch 60, val loss: 1.8538320064544678
Epoch 70, training loss: 8.687199592590332 = 1.8416639566421509 + 1.0 * 6.8455352783203125
Epoch 70, val loss: 1.8396718502044678
Epoch 80, training loss: 8.468618392944336 = 1.8303200006484985 + 1.0 * 6.638298034667969
Epoch 80, val loss: 1.8282556533813477
Epoch 90, training loss: 8.35210132598877 = 1.8190900087356567 + 1.0 * 6.533010959625244
Epoch 90, val loss: 1.8165562152862549
Epoch 100, training loss: 8.26832389831543 = 1.8068698644638062 + 1.0 * 6.461453914642334
Epoch 100, val loss: 1.804065227508545
Epoch 110, training loss: 8.19985294342041 = 1.7947099208831787 + 1.0 * 6.405142784118652
Epoch 110, val loss: 1.7915829420089722
Epoch 120, training loss: 8.14570426940918 = 1.7832202911376953 + 1.0 * 6.362484455108643
Epoch 120, val loss: 1.780113935470581
Epoch 130, training loss: 8.100421905517578 = 1.7721271514892578 + 1.0 * 6.3282952308654785
Epoch 130, val loss: 1.7690006494522095
Epoch 140, training loss: 8.059793472290039 = 1.7604609727859497 + 1.0 * 6.299332618713379
Epoch 140, val loss: 1.7577085494995117
Epoch 150, training loss: 8.034671783447266 = 1.7478575706481934 + 1.0 * 6.286813735961914
Epoch 150, val loss: 1.746106743812561
Epoch 160, training loss: 7.9921183586120605 = 1.733911395072937 + 1.0 * 6.258206844329834
Epoch 160, val loss: 1.7340754270553589
Epoch 170, training loss: 7.957555770874023 = 1.7181411981582642 + 1.0 * 6.239414691925049
Epoch 170, val loss: 1.7211534976959229
Epoch 180, training loss: 7.9245710372924805 = 1.6995497941970825 + 1.0 * 6.2250213623046875
Epoch 180, val loss: 1.7063912153244019
Epoch 190, training loss: 7.890140056610107 = 1.677240014076233 + 1.0 * 6.212900161743164
Epoch 190, val loss: 1.6891573667526245
Epoch 200, training loss: 7.853049278259277 = 1.6504051685333252 + 1.0 * 6.202644348144531
Epoch 200, val loss: 1.6685153245925903
Epoch 210, training loss: 7.811079978942871 = 1.6177610158920288 + 1.0 * 6.193318843841553
Epoch 210, val loss: 1.643541932106018
Epoch 220, training loss: 7.767330646514893 = 1.5787506103515625 + 1.0 * 6.18858003616333
Epoch 220, val loss: 1.613720178604126
Epoch 230, training loss: 7.712348937988281 = 1.5338051319122314 + 1.0 * 6.178544044494629
Epoch 230, val loss: 1.5790694952011108
Epoch 240, training loss: 7.658698558807373 = 1.4833413362503052 + 1.0 * 6.175357341766357
Epoch 240, val loss: 1.5402095317840576
Epoch 250, training loss: 7.5966291427612305 = 1.4296038150787354 + 1.0 * 6.167025566101074
Epoch 250, val loss: 1.499324917793274
Epoch 260, training loss: 7.539709091186523 = 1.3750083446502686 + 1.0 * 6.164700984954834
Epoch 260, val loss: 1.4579510688781738
Epoch 270, training loss: 7.480507850646973 = 1.3223003149032593 + 1.0 * 6.158207416534424
Epoch 270, val loss: 1.417974591255188
Epoch 280, training loss: 7.426731109619141 = 1.2727453708648682 + 1.0 * 6.153985977172852
Epoch 280, val loss: 1.380960464477539
Epoch 290, training loss: 7.374993801116943 = 1.2270512580871582 + 1.0 * 6.147942543029785
Epoch 290, val loss: 1.3470336198806763
Epoch 300, training loss: 7.3271403312683105 = 1.1845623254776 + 1.0 * 6.142578125
Epoch 300, val loss: 1.3157432079315186
Epoch 310, training loss: 7.282423496246338 = 1.1445465087890625 + 1.0 * 6.137876987457275
Epoch 310, val loss: 1.2866108417510986
Epoch 320, training loss: 7.239806175231934 = 1.1060781478881836 + 1.0 * 6.13372802734375
Epoch 320, val loss: 1.2587890625
Epoch 330, training loss: 7.197591781616211 = 1.068011999130249 + 1.0 * 6.129579544067383
Epoch 330, val loss: 1.2312458753585815
Epoch 340, training loss: 7.156587600708008 = 1.0295850038528442 + 1.0 * 6.127002716064453
Epoch 340, val loss: 1.2035162448883057
Epoch 350, training loss: 7.1176533699035645 = 0.9905298352241516 + 1.0 * 6.1271233558654785
Epoch 350, val loss: 1.1751240491867065
Epoch 360, training loss: 7.0721893310546875 = 0.9509468674659729 + 1.0 * 6.121242523193359
Epoch 360, val loss: 1.1458086967468262
Epoch 370, training loss: 7.027831077575684 = 0.9103650450706482 + 1.0 * 6.117465972900391
Epoch 370, val loss: 1.115468144416809
Epoch 380, training loss: 6.990163326263428 = 0.8694822192192078 + 1.0 * 6.120681285858154
Epoch 380, val loss: 1.0845986604690552
Epoch 390, training loss: 6.942834377288818 = 0.8286610245704651 + 1.0 * 6.114173412322998
Epoch 390, val loss: 1.0534348487854004
Epoch 400, training loss: 6.897611618041992 = 0.7882956862449646 + 1.0 * 6.109315872192383
Epoch 400, val loss: 1.0222748517990112
Epoch 410, training loss: 6.855413436889648 = 0.7482446432113647 + 1.0 * 6.107168674468994
Epoch 410, val loss: 0.991375744342804
Epoch 420, training loss: 6.820310592651367 = 0.70893394947052 + 1.0 * 6.111376762390137
Epoch 420, val loss: 0.9609878063201904
Epoch 430, training loss: 6.774005889892578 = 0.6711334586143494 + 1.0 * 6.102872371673584
Epoch 430, val loss: 0.931769847869873
Epoch 440, training loss: 6.7374372482299805 = 0.6348140239715576 + 1.0 * 6.102622985839844
Epoch 440, val loss: 0.9041821360588074
Epoch 450, training loss: 6.702383995056152 = 0.6001467108726501 + 1.0 * 6.102237224578857
Epoch 450, val loss: 0.8783990144729614
Epoch 460, training loss: 6.666301250457764 = 0.5671280026435852 + 1.0 * 6.099173069000244
Epoch 460, val loss: 0.8546510338783264
Epoch 470, training loss: 6.6310930252075195 = 0.535742998123169 + 1.0 * 6.09535026550293
Epoch 470, val loss: 0.8329565525054932
Epoch 480, training loss: 6.601805686950684 = 0.5058876276016235 + 1.0 * 6.09591817855835
Epoch 480, val loss: 0.8133143782615662
Epoch 490, training loss: 6.572990894317627 = 0.4775231182575226 + 1.0 * 6.095467567443848
Epoch 490, val loss: 0.7957698106765747
Epoch 500, training loss: 6.54037618637085 = 0.45076078176498413 + 1.0 * 6.089615345001221
Epoch 500, val loss: 0.7803284525871277
Epoch 510, training loss: 6.5126543045043945 = 0.42530524730682373 + 1.0 * 6.087348937988281
Epoch 510, val loss: 0.7668673396110535
Epoch 520, training loss: 6.4885687828063965 = 0.401082307100296 + 1.0 * 6.087486267089844
Epoch 520, val loss: 0.7551190257072449
Epoch 530, training loss: 6.473180294036865 = 0.3782910704612732 + 1.0 * 6.094889163970947
Epoch 530, val loss: 0.7451260089874268
Epoch 540, training loss: 6.439903736114502 = 0.35702428221702576 + 1.0 * 6.082879543304443
Epoch 540, val loss: 0.7366701364517212
Epoch 550, training loss: 6.418927192687988 = 0.33695849776268005 + 1.0 * 6.081968784332275
Epoch 550, val loss: 0.7295543551445007
Epoch 560, training loss: 6.397917747497559 = 0.31797197461128235 + 1.0 * 6.0799455642700195
Epoch 560, val loss: 0.7235685586929321
Epoch 570, training loss: 6.393122673034668 = 0.30012792348861694 + 1.0 * 6.092994689941406
Epoch 570, val loss: 0.718582272529602
Epoch 580, training loss: 6.36503791809082 = 0.2835141122341156 + 1.0 * 6.081523895263672
Epoch 580, val loss: 0.7145637273788452
Epoch 590, training loss: 6.3438801765441895 = 0.2679187059402466 + 1.0 * 6.075961589813232
Epoch 590, val loss: 0.711450457572937
Epoch 600, training loss: 6.3275227546691895 = 0.25323694944381714 + 1.0 * 6.074285984039307
Epoch 600, val loss: 0.7089848518371582
Epoch 610, training loss: 6.312023162841797 = 0.23937563598155975 + 1.0 * 6.072647571563721
Epoch 610, val loss: 0.7072170972824097
Epoch 620, training loss: 6.306274890899658 = 0.22634437680244446 + 1.0 * 6.079930305480957
Epoch 620, val loss: 0.7060743570327759
Epoch 630, training loss: 6.287832260131836 = 0.21425549685955048 + 1.0 * 6.073576927185059
Epoch 630, val loss: 0.7055554389953613
Epoch 640, training loss: 6.272329807281494 = 0.20291337370872498 + 1.0 * 6.069416522979736
Epoch 640, val loss: 0.7057704925537109
Epoch 650, training loss: 6.261646747589111 = 0.19222748279571533 + 1.0 * 6.0694193840026855
Epoch 650, val loss: 0.7065851092338562
Epoch 660, training loss: 6.251497268676758 = 0.1821916401386261 + 1.0 * 6.069305419921875
Epoch 660, val loss: 0.7078639268875122
Epoch 670, training loss: 6.239400863647461 = 0.17281633615493774 + 1.0 * 6.066584587097168
Epoch 670, val loss: 0.7096290588378906
Epoch 680, training loss: 6.229942321777344 = 0.16398198902606964 + 1.0 * 6.06596040725708
Epoch 680, val loss: 0.7118695974349976
Epoch 690, training loss: 6.219180107116699 = 0.15566357970237732 + 1.0 * 6.063516616821289
Epoch 690, val loss: 0.7144634127616882
Epoch 700, training loss: 6.215188980102539 = 0.14782138168811798 + 1.0 * 6.0673675537109375
Epoch 700, val loss: 0.7173672914505005
Epoch 710, training loss: 6.207589149475098 = 0.14046117663383484 + 1.0 * 6.0671281814575195
Epoch 710, val loss: 0.7203061580657959
Epoch 720, training loss: 6.196211814880371 = 0.1336134970188141 + 1.0 * 6.06259822845459
Epoch 720, val loss: 0.7235479354858398
Epoch 730, training loss: 6.18646764755249 = 0.12715783715248108 + 1.0 * 6.059309959411621
Epoch 730, val loss: 0.7269811630249023
Epoch 740, training loss: 6.179267406463623 = 0.12105148285627365 + 1.0 * 6.058216094970703
Epoch 740, val loss: 0.7305098176002502
Epoch 750, training loss: 6.178513526916504 = 0.11527904123067856 + 1.0 * 6.063234329223633
Epoch 750, val loss: 0.7341386675834656
Epoch 760, training loss: 6.173617839813232 = 0.10986359417438507 + 1.0 * 6.063754081726074
Epoch 760, val loss: 0.7377702593803406
Epoch 770, training loss: 6.162160873413086 = 0.10477351397275925 + 1.0 * 6.057387351989746
Epoch 770, val loss: 0.7415242791175842
Epoch 780, training loss: 6.154379844665527 = 0.09996140003204346 + 1.0 * 6.054418563842773
Epoch 780, val loss: 0.7454160451889038
Epoch 790, training loss: 6.160774230957031 = 0.09540688991546631 + 1.0 * 6.065367221832275
Epoch 790, val loss: 0.7493289709091187
Epoch 800, training loss: 6.144752502441406 = 0.0911107137799263 + 1.0 * 6.0536417961120605
Epoch 800, val loss: 0.753180980682373
Epoch 810, training loss: 6.139449596405029 = 0.08705243468284607 + 1.0 * 6.05239725112915
Epoch 810, val loss: 0.7572198510169983
Epoch 820, training loss: 6.146756649017334 = 0.08321487158536911 + 1.0 * 6.063541889190674
Epoch 820, val loss: 0.7611620426177979
Epoch 830, training loss: 6.132263660430908 = 0.07961901277303696 + 1.0 * 6.052644729614258
Epoch 830, val loss: 0.7650927305221558
Epoch 840, training loss: 6.127469062805176 = 0.07621593028306961 + 1.0 * 6.051253318786621
Epoch 840, val loss: 0.7691980600357056
Epoch 850, training loss: 6.121154308319092 = 0.07297689467668533 + 1.0 * 6.048177242279053
Epoch 850, val loss: 0.7733448147773743
Epoch 860, training loss: 6.117178440093994 = 0.06989453732967377 + 1.0 * 6.047284126281738
Epoch 860, val loss: 0.7776039838790894
Epoch 870, training loss: 6.117694854736328 = 0.066963791847229 + 1.0 * 6.050731182098389
Epoch 870, val loss: 0.7819302678108215
Epoch 880, training loss: 6.117232322692871 = 0.06419116258621216 + 1.0 * 6.053040981292725
Epoch 880, val loss: 0.7862290143966675
Epoch 890, training loss: 6.1065521240234375 = 0.06157447397708893 + 1.0 * 6.04497766494751
Epoch 890, val loss: 0.7905505895614624
Epoch 900, training loss: 6.1073784828186035 = 0.05909058824181557 + 1.0 * 6.048287868499756
Epoch 900, val loss: 0.7949338555335999
Epoch 910, training loss: 6.103438854217529 = 0.05673579126596451 + 1.0 * 6.046702861785889
Epoch 910, val loss: 0.7992539405822754
Epoch 920, training loss: 6.100801944732666 = 0.05451009050011635 + 1.0 * 6.046291828155518
Epoch 920, val loss: 0.8036669492721558
Epoch 930, training loss: 6.09654426574707 = 0.05239124968647957 + 1.0 * 6.044153213500977
Epoch 930, val loss: 0.808097243309021
Epoch 940, training loss: 6.093475818634033 = 0.050382714718580246 + 1.0 * 6.043093204498291
Epoch 940, val loss: 0.8125307559967041
Epoch 950, training loss: 6.09097957611084 = 0.04848411679267883 + 1.0 * 6.042495250701904
Epoch 950, val loss: 0.8169799447059631
Epoch 960, training loss: 6.086911678314209 = 0.04667932167649269 + 1.0 * 6.040232181549072
Epoch 960, val loss: 0.8214998245239258
Epoch 970, training loss: 6.099968910217285 = 0.04495982453227043 + 1.0 * 6.055008888244629
Epoch 970, val loss: 0.8260082602500916
Epoch 980, training loss: 6.092496871948242 = 0.043344318866729736 + 1.0 * 6.049152374267578
Epoch 980, val loss: 0.8303450345993042
Epoch 990, training loss: 6.080878734588623 = 0.0418117456138134 + 1.0 * 6.039066791534424
Epoch 990, val loss: 0.8347644209861755
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.9815
Flip ASR: 0.9867/225 nodes
The final ASR:0.71464, 0.22312, Accuracy:0.80123, 0.01944
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9524])
updated graph: torch.Size([2, 10572])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98032, 0.00920, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.33946704864502 = 1.9655945301055908 + 1.0 * 8.373872756958008
Epoch 0, val loss: 1.9712251424789429
Epoch 10, training loss: 10.327699661254883 = 1.9541418552398682 + 1.0 * 8.373558044433594
Epoch 10, val loss: 1.9594234228134155
Epoch 20, training loss: 10.311667442321777 = 1.9401605129241943 + 1.0 * 8.371506690979004
Epoch 20, val loss: 1.9447087049484253
Epoch 30, training loss: 10.277325630187988 = 1.920747995376587 + 1.0 * 8.35657787322998
Epoch 30, val loss: 1.9242286682128906
Epoch 40, training loss: 10.144010543823242 = 1.8952994346618652 + 1.0 * 8.248711585998535
Epoch 40, val loss: 1.8985756635665894
Epoch 50, training loss: 9.602617263793945 = 1.8700098991394043 + 1.0 * 7.732607364654541
Epoch 50, val loss: 1.8743613958358765
Epoch 60, training loss: 9.05345344543457 = 1.8497796058654785 + 1.0 * 7.203673839569092
Epoch 60, val loss: 1.8564733266830444
Epoch 70, training loss: 8.689973831176758 = 1.833773136138916 + 1.0 * 6.856201171875
Epoch 70, val loss: 1.8417432308197021
Epoch 80, training loss: 8.527904510498047 = 1.8169803619384766 + 1.0 * 6.710923671722412
Epoch 80, val loss: 1.8261932134628296
Epoch 90, training loss: 8.413262367248535 = 1.7978047132492065 + 1.0 * 6.615457534790039
Epoch 90, val loss: 1.8090401887893677
Epoch 100, training loss: 8.317177772521973 = 1.7786365747451782 + 1.0 * 6.538540840148926
Epoch 100, val loss: 1.7925403118133545
Epoch 110, training loss: 8.237032890319824 = 1.7600390911102295 + 1.0 * 6.476993560791016
Epoch 110, val loss: 1.7764030694961548
Epoch 120, training loss: 8.169846534729004 = 1.7405811548233032 + 1.0 * 6.42926549911499
Epoch 120, val loss: 1.7593321800231934
Epoch 130, training loss: 8.111379623413086 = 1.7190158367156982 + 1.0 * 6.392364025115967
Epoch 130, val loss: 1.7406994104385376
Epoch 140, training loss: 8.05664348602295 = 1.69452702999115 + 1.0 * 6.36211633682251
Epoch 140, val loss: 1.7197598218917847
Epoch 150, training loss: 8.002668380737305 = 1.6658360958099365 + 1.0 * 6.336832523345947
Epoch 150, val loss: 1.6954642534255981
Epoch 160, training loss: 7.948953628540039 = 1.631960153579712 + 1.0 * 6.316993713378906
Epoch 160, val loss: 1.6668555736541748
Epoch 170, training loss: 7.8917083740234375 = 1.5925030708312988 + 1.0 * 6.299205303192139
Epoch 170, val loss: 1.6338127851486206
Epoch 180, training loss: 7.831926345825195 = 1.5468246936798096 + 1.0 * 6.285101413726807
Epoch 180, val loss: 1.5956718921661377
Epoch 190, training loss: 7.765824794769287 = 1.4943532943725586 + 1.0 * 6.2714715003967285
Epoch 190, val loss: 1.5520414113998413
Epoch 200, training loss: 7.69865608215332 = 1.4363445043563843 + 1.0 * 6.2623114585876465
Epoch 200, val loss: 1.5042011737823486
Epoch 210, training loss: 7.623661041259766 = 1.374444603919983 + 1.0 * 6.249216556549072
Epoch 210, val loss: 1.4536255598068237
Epoch 220, training loss: 7.549231052398682 = 1.309792399406433 + 1.0 * 6.239438533782959
Epoch 220, val loss: 1.401626706123352
Epoch 230, training loss: 7.476866722106934 = 1.2450261116027832 + 1.0 * 6.23184061050415
Epoch 230, val loss: 1.3503888845443726
Epoch 240, training loss: 7.405764102935791 = 1.1823683977127075 + 1.0 * 6.223395824432373
Epoch 240, val loss: 1.3018964529037476
Epoch 250, training loss: 7.3374481201171875 = 1.1222496032714844 + 1.0 * 6.215198516845703
Epoch 250, val loss: 1.256029725074768
Epoch 260, training loss: 7.278219223022461 = 1.0657626390457153 + 1.0 * 6.212456703186035
Epoch 260, val loss: 1.2135791778564453
Epoch 270, training loss: 7.215461254119873 = 1.0133785009384155 + 1.0 * 6.202082633972168
Epoch 270, val loss: 1.1744719743728638
Epoch 280, training loss: 7.158623695373535 = 0.9637882709503174 + 1.0 * 6.194835662841797
Epoch 280, val loss: 1.1377533674240112
Epoch 290, training loss: 7.107938766479492 = 0.9166404008865356 + 1.0 * 6.191298484802246
Epoch 290, val loss: 1.1031180620193481
Epoch 300, training loss: 7.057305335998535 = 0.8715248703956604 + 1.0 * 6.1857805252075195
Epoch 300, val loss: 1.0702584981918335
Epoch 310, training loss: 7.010120391845703 = 0.8279308080673218 + 1.0 * 6.182189464569092
Epoch 310, val loss: 1.0385563373565674
Epoch 320, training loss: 6.961407661437988 = 0.7857303619384766 + 1.0 * 6.175677299499512
Epoch 320, val loss: 1.0081229209899902
Epoch 330, training loss: 6.915707588195801 = 0.7446276545524597 + 1.0 * 6.171080112457275
Epoch 330, val loss: 0.9789578914642334
Epoch 340, training loss: 6.873593807220459 = 0.7050081491470337 + 1.0 * 6.168585777282715
Epoch 340, val loss: 0.9511517286300659
Epoch 350, training loss: 6.832266807556152 = 0.6673685312271118 + 1.0 * 6.16489839553833
Epoch 350, val loss: 0.9253448843955994
Epoch 360, training loss: 6.7905120849609375 = 0.6317484378814697 + 1.0 * 6.158763885498047
Epoch 360, val loss: 0.9016504287719727
Epoch 370, training loss: 6.760709762573242 = 0.5982367992401123 + 1.0 * 6.162473201751709
Epoch 370, val loss: 0.8800584077835083
Epoch 380, training loss: 6.72055721282959 = 0.567363440990448 + 1.0 * 6.153193950653076
Epoch 380, val loss: 0.8608533143997192
Epoch 390, training loss: 6.688842296600342 = 0.5387504696846008 + 1.0 * 6.150091648101807
Epoch 390, val loss: 0.8440977931022644
Epoch 400, training loss: 6.659489631652832 = 0.5121932625770569 + 1.0 * 6.14729642868042
Epoch 400, val loss: 0.8293970823287964
Epoch 410, training loss: 6.632335662841797 = 0.4874557554721832 + 1.0 * 6.1448798179626465
Epoch 410, val loss: 0.8166014552116394
Epoch 420, training loss: 6.607048511505127 = 0.4643729031085968 + 1.0 * 6.142675399780273
Epoch 420, val loss: 0.8055374622344971
Epoch 430, training loss: 6.582417011260986 = 0.44268086552619934 + 1.0 * 6.139736175537109
Epoch 430, val loss: 0.7960257530212402
Epoch 440, training loss: 6.559009552001953 = 0.4223524332046509 + 1.0 * 6.136657238006592
Epoch 440, val loss: 0.7878057360649109
Epoch 450, training loss: 6.5365729331970215 = 0.40314263105392456 + 1.0 * 6.133430480957031
Epoch 450, val loss: 0.7808372378349304
Epoch 460, training loss: 6.5143914222717285 = 0.3848896920681 + 1.0 * 6.129501819610596
Epoch 460, val loss: 0.774937093257904
Epoch 470, training loss: 6.493444442749023 = 0.36743679642677307 + 1.0 * 6.126007556915283
Epoch 470, val loss: 0.7699037790298462
Epoch 480, training loss: 6.475327491760254 = 0.350756973028183 + 1.0 * 6.124570369720459
Epoch 480, val loss: 0.7656532526016235
Epoch 490, training loss: 6.45680046081543 = 0.3346703350543976 + 1.0 * 6.122129917144775
Epoch 490, val loss: 0.7621416449546814
Epoch 500, training loss: 6.441440582275391 = 0.31920838356018066 + 1.0 * 6.122231960296631
Epoch 500, val loss: 0.7592538595199585
Epoch 510, training loss: 6.422906875610352 = 0.3042026460170746 + 1.0 * 6.118704319000244
Epoch 510, val loss: 0.7571218013763428
Epoch 520, training loss: 6.410577774047852 = 0.2897319793701172 + 1.0 * 6.120845794677734
Epoch 520, val loss: 0.7555793523788452
Epoch 530, training loss: 6.389699935913086 = 0.27581557631492615 + 1.0 * 6.113884449005127
Epoch 530, val loss: 0.7546564340591431
Epoch 540, training loss: 6.372550964355469 = 0.26227840781211853 + 1.0 * 6.110272407531738
Epoch 540, val loss: 0.7544562816619873
Epoch 550, training loss: 6.357865333557129 = 0.2491527497768402 + 1.0 * 6.108712673187256
Epoch 550, val loss: 0.7548288702964783
Epoch 560, training loss: 6.346322059631348 = 0.23643672466278076 + 1.0 * 6.109885215759277
Epoch 560, val loss: 0.7557428479194641
Epoch 570, training loss: 6.332576751708984 = 0.22424344718456268 + 1.0 * 6.108333110809326
Epoch 570, val loss: 0.7572668194770813
Epoch 580, training loss: 6.318206787109375 = 0.21248751878738403 + 1.0 * 6.105719089508057
Epoch 580, val loss: 0.7593623399734497
Epoch 590, training loss: 6.303922653198242 = 0.20118029415607452 + 1.0 * 6.1027421951293945
Epoch 590, val loss: 0.7620474696159363
Epoch 600, training loss: 6.290653705596924 = 0.19026321172714233 + 1.0 * 6.100390434265137
Epoch 600, val loss: 0.7652840614318848
Epoch 610, training loss: 6.287477016448975 = 0.17978018522262573 + 1.0 * 6.107697010040283
Epoch 610, val loss: 0.7690280079841614
Epoch 620, training loss: 6.271850109100342 = 0.1698102504014969 + 1.0 * 6.102039813995361
Epoch 620, val loss: 0.7732941508293152
Epoch 630, training loss: 6.257595062255859 = 0.16030040383338928 + 1.0 * 6.097294807434082
Epoch 630, val loss: 0.77821284532547
Epoch 640, training loss: 6.247312545776367 = 0.15120232105255127 + 1.0 * 6.0961103439331055
Epoch 640, val loss: 0.7836248874664307
Epoch 650, training loss: 6.237610340118408 = 0.1425745040178299 + 1.0 * 6.095036029815674
Epoch 650, val loss: 0.7893370389938354
Epoch 660, training loss: 6.227889060974121 = 0.13441112637519836 + 1.0 * 6.093477725982666
Epoch 660, val loss: 0.7956234812736511
Epoch 670, training loss: 6.216558456420898 = 0.1266891062259674 + 1.0 * 6.089869499206543
Epoch 670, val loss: 0.8023462295532227
Epoch 680, training loss: 6.21115779876709 = 0.11935913562774658 + 1.0 * 6.091798782348633
Epoch 680, val loss: 0.8093528747558594
Epoch 690, training loss: 6.202588081359863 = 0.11249038577079773 + 1.0 * 6.090097904205322
Epoch 690, val loss: 0.816710352897644
Epoch 700, training loss: 6.194164276123047 = 0.1060260757803917 + 1.0 * 6.088138103485107
Epoch 700, val loss: 0.8242346048355103
Epoch 710, training loss: 6.188918113708496 = 0.09999360889196396 + 1.0 * 6.088924407958984
Epoch 710, val loss: 0.8319812417030334
Epoch 720, training loss: 6.181857585906982 = 0.09435568749904633 + 1.0 * 6.0875020027160645
Epoch 720, val loss: 0.8398114442825317
Epoch 730, training loss: 6.1749067306518555 = 0.08910118788480759 + 1.0 * 6.085805416107178
Epoch 730, val loss: 0.8477174639701843
Epoch 740, training loss: 6.1729207038879395 = 0.08420635759830475 + 1.0 * 6.088714122772217
Epoch 740, val loss: 0.855643093585968
Epoch 750, training loss: 6.162789821624756 = 0.07967563718557358 + 1.0 * 6.083114147186279
Epoch 750, val loss: 0.8636042475700378
Epoch 760, training loss: 6.15504789352417 = 0.0754442885518074 + 1.0 * 6.079603672027588
Epoch 760, val loss: 0.8715697526931763
Epoch 770, training loss: 6.160916805267334 = 0.07151414453983307 + 1.0 * 6.089402675628662
Epoch 770, val loss: 0.8794026970863342
Epoch 780, training loss: 6.147602558135986 = 0.06784453988075256 + 1.0 * 6.079758167266846
Epoch 780, val loss: 0.887279748916626
Epoch 790, training loss: 6.141403675079346 = 0.06444474309682846 + 1.0 * 6.076959133148193
Epoch 790, val loss: 0.8951305150985718
Epoch 800, training loss: 6.141493320465088 = 0.0612628348171711 + 1.0 * 6.080230712890625
Epoch 800, val loss: 0.9029315710067749
Epoch 810, training loss: 6.138073921203613 = 0.05831681936979294 + 1.0 * 6.079757213592529
Epoch 810, val loss: 0.9104934334754944
Epoch 820, training loss: 6.129463195800781 = 0.05556729808449745 + 1.0 * 6.0738959312438965
Epoch 820, val loss: 0.9182441234588623
Epoch 830, training loss: 6.129844665527344 = 0.05300348624587059 + 1.0 * 6.076841354370117
Epoch 830, val loss: 0.9259151220321655
Epoch 840, training loss: 6.124120235443115 = 0.05061182379722595 + 1.0 * 6.073508262634277
Epoch 840, val loss: 0.9333666563034058
Epoch 850, training loss: 6.120057582855225 = 0.04837065190076828 + 1.0 * 6.071686744689941
Epoch 850, val loss: 0.9409414529800415
Epoch 860, training loss: 6.116265773773193 = 0.04626946151256561 + 1.0 * 6.069996356964111
Epoch 860, val loss: 0.9484632611274719
Epoch 870, training loss: 6.11675500869751 = 0.044292204082012177 + 1.0 * 6.072463035583496
Epoch 870, val loss: 0.9559177160263062
Epoch 880, training loss: 6.1114726066589355 = 0.042453113943338394 + 1.0 * 6.069019317626953
Epoch 880, val loss: 0.9631800055503845
Epoch 890, training loss: 6.113977909088135 = 0.04071918874979019 + 1.0 * 6.073258876800537
Epoch 890, val loss: 0.9704960584640503
Epoch 900, training loss: 6.107557773590088 = 0.039103925228118896 + 1.0 * 6.068453788757324
Epoch 900, val loss: 0.9776171445846558
Epoch 910, training loss: 6.10357141494751 = 0.037571992725133896 + 1.0 * 6.065999507904053
Epoch 910, val loss: 0.9848619103431702
Epoch 920, training loss: 6.102960109710693 = 0.036125365644693375 + 1.0 * 6.066834926605225
Epoch 920, val loss: 0.9919571280479431
Epoch 930, training loss: 6.098584175109863 = 0.03476608172059059 + 1.0 * 6.063817977905273
Epoch 930, val loss: 0.9990032315254211
Epoch 940, training loss: 6.099003791809082 = 0.03347916528582573 + 1.0 * 6.065524578094482
Epoch 940, val loss: 1.006041407585144
Epoch 950, training loss: 6.0969085693359375 = 0.032262977212667465 + 1.0 * 6.064645767211914
Epoch 950, val loss: 1.0128381252288818
Epoch 960, training loss: 6.09433126449585 = 0.03111308626830578 + 1.0 * 6.063218116760254
Epoch 960, val loss: 1.019700527191162
Epoch 970, training loss: 6.091462135314941 = 0.030026718974113464 + 1.0 * 6.061435222625732
Epoch 970, val loss: 1.0265403985977173
Epoch 980, training loss: 6.092950344085693 = 0.02899514138698578 + 1.0 * 6.063955307006836
Epoch 980, val loss: 1.033258080482483
Epoch 990, training loss: 6.09114933013916 = 0.02801494672894478 + 1.0 * 6.06313419342041
Epoch 990, val loss: 1.0398821830749512
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.7380
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.299334526062012 = 1.9257713556289673 + 1.0 * 8.373562812805176
Epoch 0, val loss: 1.9271687269210815
Epoch 10, training loss: 10.28464126586914 = 1.9140548706054688 + 1.0 * 8.370586395263672
Epoch 10, val loss: 1.9108755588531494
Epoch 20, training loss: 10.267271041870117 = 1.899510383605957 + 1.0 * 8.36776065826416
Epoch 20, val loss: 1.8895012140274048
Epoch 30, training loss: 10.233091354370117 = 1.883368968963623 + 1.0 * 8.349722862243652
Epoch 30, val loss: 1.866905927658081
Epoch 40, training loss: 10.104029655456543 = 1.8679312467575073 + 1.0 * 8.236098289489746
Epoch 40, val loss: 1.8478491306304932
Epoch 50, training loss: 9.657891273498535 = 1.8534213304519653 + 1.0 * 7.804470062255859
Epoch 50, val loss: 1.8324978351593018
Epoch 60, training loss: 9.190024375915527 = 1.8370567560195923 + 1.0 * 7.352967262268066
Epoch 60, val loss: 1.8185160160064697
Epoch 70, training loss: 8.8438720703125 = 1.8216683864593506 + 1.0 * 7.02220344543457
Epoch 70, val loss: 1.8060334920883179
Epoch 80, training loss: 8.575671195983887 = 1.8097721338272095 + 1.0 * 6.765898704528809
Epoch 80, val loss: 1.795479655265808
Epoch 90, training loss: 8.404979705810547 = 1.7984291315078735 + 1.0 * 6.606550693511963
Epoch 90, val loss: 1.7843917608261108
Epoch 100, training loss: 8.300823211669922 = 1.7853279113769531 + 1.0 * 6.5154948234558105
Epoch 100, val loss: 1.7720917463302612
Epoch 110, training loss: 8.229433059692383 = 1.7707456350326538 + 1.0 * 6.458687782287598
Epoch 110, val loss: 1.75881826877594
Epoch 120, training loss: 8.169235229492188 = 1.7550214529037476 + 1.0 * 6.41421365737915
Epoch 120, val loss: 1.7445648908615112
Epoch 130, training loss: 8.120710372924805 = 1.7380750179290771 + 1.0 * 6.382635593414307
Epoch 130, val loss: 1.7294330596923828
Epoch 140, training loss: 8.069136619567871 = 1.7196080684661865 + 1.0 * 6.349528789520264
Epoch 140, val loss: 1.7130508422851562
Epoch 150, training loss: 8.02330207824707 = 1.698459506034851 + 1.0 * 6.32484245300293
Epoch 150, val loss: 1.69484281539917
Epoch 160, training loss: 7.979003429412842 = 1.6738471984863281 + 1.0 * 6.305156230926514
Epoch 160, val loss: 1.6740608215332031
Epoch 170, training loss: 7.931395530700684 = 1.6449298858642578 + 1.0 * 6.286465644836426
Epoch 170, val loss: 1.6498444080352783
Epoch 180, training loss: 7.88166618347168 = 1.6106560230255127 + 1.0 * 6.271009922027588
Epoch 180, val loss: 1.6213775873184204
Epoch 190, training loss: 7.832067012786865 = 1.5704079866409302 + 1.0 * 6.261659145355225
Epoch 190, val loss: 1.5881068706512451
Epoch 200, training loss: 7.7710394859313965 = 1.5244393348693848 + 1.0 * 6.246600151062012
Epoch 200, val loss: 1.5503695011138916
Epoch 210, training loss: 7.7079572677612305 = 1.4729466438293457 + 1.0 * 6.235010623931885
Epoch 210, val loss: 1.5081678628921509
Epoch 220, training loss: 7.649197101593018 = 1.4165717363357544 + 1.0 * 6.232625484466553
Epoch 220, val loss: 1.462091326713562
Epoch 230, training loss: 7.578672409057617 = 1.358033537864685 + 1.0 * 6.220638751983643
Epoch 230, val loss: 1.4147869348526
Epoch 240, training loss: 7.508965969085693 = 1.2990821599960327 + 1.0 * 6.209883689880371
Epoch 240, val loss: 1.367209792137146
Epoch 250, training loss: 7.443727016448975 = 1.2406281232833862 + 1.0 * 6.203098773956299
Epoch 250, val loss: 1.3204506635665894
Epoch 260, training loss: 7.382009506225586 = 1.1841169595718384 + 1.0 * 6.197892665863037
Epoch 260, val loss: 1.2758203744888306
Epoch 270, training loss: 7.321703910827637 = 1.1306767463684082 + 1.0 * 6.1910271644592285
Epoch 270, val loss: 1.2341458797454834
Epoch 280, training loss: 7.264726161956787 = 1.0797761678695679 + 1.0 * 6.18494987487793
Epoch 280, val loss: 1.1947519779205322
Epoch 290, training loss: 7.210989952087402 = 1.0308818817138672 + 1.0 * 6.180108070373535
Epoch 290, val loss: 1.1571108102798462
Epoch 300, training loss: 7.160312175750732 = 0.9840874671936035 + 1.0 * 6.176224708557129
Epoch 300, val loss: 1.12138831615448
Epoch 310, training loss: 7.108405113220215 = 0.9392023682594299 + 1.0 * 6.16920280456543
Epoch 310, val loss: 1.0874557495117188
Epoch 320, training loss: 7.061582088470459 = 0.8956979513168335 + 1.0 * 6.165884017944336
Epoch 320, val loss: 1.0548784732818604
Epoch 330, training loss: 7.018973350524902 = 0.8536524772644043 + 1.0 * 6.165320873260498
Epoch 330, val loss: 1.0238975286483765
Epoch 340, training loss: 6.970629692077637 = 0.8130759596824646 + 1.0 * 6.157553672790527
Epoch 340, val loss: 0.9944696426391602
Epoch 350, training loss: 6.926149845123291 = 0.7735203504562378 + 1.0 * 6.152629375457764
Epoch 350, val loss: 0.9662030339241028
Epoch 360, training loss: 6.890045166015625 = 0.7349392771720886 + 1.0 * 6.155106067657471
Epoch 360, val loss: 0.9390285015106201
Epoch 370, training loss: 6.843664169311523 = 0.6976640224456787 + 1.0 * 6.146000385284424
Epoch 370, val loss: 0.9135748744010925
Epoch 380, training loss: 6.803985595703125 = 0.6618695259094238 + 1.0 * 6.142116069793701
Epoch 380, val loss: 0.8896920680999756
Epoch 390, training loss: 6.765824794769287 = 0.6272858381271362 + 1.0 * 6.138538837432861
Epoch 390, val loss: 0.8674793243408203
Epoch 400, training loss: 6.731403350830078 = 0.5940569639205933 + 1.0 * 6.137346267700195
Epoch 400, val loss: 0.8467559814453125
Epoch 410, training loss: 6.695947170257568 = 0.562653660774231 + 1.0 * 6.133293628692627
Epoch 410, val loss: 0.8279634714126587
Epoch 420, training loss: 6.6681060791015625 = 0.5328375697135925 + 1.0 * 6.135268688201904
Epoch 420, val loss: 0.8109551668167114
Epoch 430, training loss: 6.633750915527344 = 0.5045004487037659 + 1.0 * 6.129250526428223
Epoch 430, val loss: 0.7955478429794312
Epoch 440, training loss: 6.6024603843688965 = 0.4772949814796448 + 1.0 * 6.1251654624938965
Epoch 440, val loss: 0.7816358804702759
Epoch 450, training loss: 6.576327323913574 = 0.4512938857078552 + 1.0 * 6.125033378601074
Epoch 450, val loss: 0.7689887881278992
Epoch 460, training loss: 6.553837776184082 = 0.4265300929546356 + 1.0 * 6.127307891845703
Epoch 460, val loss: 0.757534384727478
Epoch 470, training loss: 6.5223846435546875 = 0.40301018953323364 + 1.0 * 6.1193742752075195
Epoch 470, val loss: 0.7475074529647827
Epoch 480, training loss: 6.496357440948486 = 0.3805214762687683 + 1.0 * 6.115836143493652
Epoch 480, val loss: 0.7385546565055847
Epoch 490, training loss: 6.476996898651123 = 0.3590182662010193 + 1.0 * 6.117978572845459
Epoch 490, val loss: 0.7307007312774658
Epoch 500, training loss: 6.454979419708252 = 0.33861786127090454 + 1.0 * 6.116361618041992
Epoch 500, val loss: 0.7238010764122009
Epoch 510, training loss: 6.429376602172852 = 0.3193356990814209 + 1.0 * 6.11004114151001
Epoch 510, val loss: 0.7180323004722595
Epoch 520, training loss: 6.40879487991333 = 0.30104395747184753 + 1.0 * 6.10775089263916
Epoch 520, val loss: 0.7131978273391724
Epoch 530, training loss: 6.396756172180176 = 0.28367504477500916 + 1.0 * 6.113080978393555
Epoch 530, val loss: 0.7092179656028748
Epoch 540, training loss: 6.374014854431152 = 0.2672538459300995 + 1.0 * 6.1067609786987305
Epoch 540, val loss: 0.7060707211494446
Epoch 550, training loss: 6.3615899085998535 = 0.2517762780189514 + 1.0 * 6.109813690185547
Epoch 550, val loss: 0.7037373185157776
Epoch 560, training loss: 6.338070392608643 = 0.23715804517269135 + 1.0 * 6.100912570953369
Epoch 560, val loss: 0.702253520488739
Epoch 570, training loss: 6.321865081787109 = 0.22332978248596191 + 1.0 * 6.098535060882568
Epoch 570, val loss: 0.701477587223053
Epoch 580, training loss: 6.306554794311523 = 0.21012704074382782 + 1.0 * 6.096427917480469
Epoch 580, val loss: 0.7013720870018005
Epoch 590, training loss: 6.297611236572266 = 0.19757716357707977 + 1.0 * 6.100034236907959
Epoch 590, val loss: 0.7018559575080872
Epoch 600, training loss: 6.285762310028076 = 0.18582946062088013 + 1.0 * 6.099932670593262
Epoch 600, val loss: 0.7027102708816528
Epoch 610, training loss: 6.268157958984375 = 0.17479926347732544 + 1.0 * 6.093358516693115
Epoch 610, val loss: 0.7041836977005005
Epoch 620, training loss: 6.256235122680664 = 0.16442902386188507 + 1.0 * 6.091805934906006
Epoch 620, val loss: 0.7062644362449646
Epoch 630, training loss: 6.254974842071533 = 0.15458707511425018 + 1.0 * 6.1003875732421875
Epoch 630, val loss: 0.7087495923042297
Epoch 640, training loss: 6.237378120422363 = 0.145456925034523 + 1.0 * 6.091921329498291
Epoch 640, val loss: 0.7115538716316223
Epoch 650, training loss: 6.224182605743408 = 0.1368679702281952 + 1.0 * 6.087314605712891
Epoch 650, val loss: 0.7148041725158691
Epoch 660, training loss: 6.216662406921387 = 0.1288764625787735 + 1.0 * 6.087785720825195
Epoch 660, val loss: 0.7183883190155029
Epoch 670, training loss: 6.204995155334473 = 0.12141721695661545 + 1.0 * 6.083578109741211
Epoch 670, val loss: 0.7221750617027283
Epoch 680, training loss: 6.199188709259033 = 0.11448164284229279 + 1.0 * 6.084707260131836
Epoch 680, val loss: 0.7262821793556213
Epoch 690, training loss: 6.190031051635742 = 0.10802217572927475 + 1.0 * 6.0820088386535645
Epoch 690, val loss: 0.7307273149490356
Epoch 700, training loss: 6.18850564956665 = 0.10205274820327759 + 1.0 * 6.086452960968018
Epoch 700, val loss: 0.7352989912033081
Epoch 710, training loss: 6.178126335144043 = 0.09653866291046143 + 1.0 * 6.081587791442871
Epoch 710, val loss: 0.7399485111236572
Epoch 720, training loss: 6.1704936027526855 = 0.09141011536121368 + 1.0 * 6.079083442687988
Epoch 720, val loss: 0.7448800802230835
Epoch 730, training loss: 6.1701860427856445 = 0.08662237972021103 + 1.0 * 6.083563804626465
Epoch 730, val loss: 0.7499421834945679
Epoch 740, training loss: 6.161725044250488 = 0.08219423145055771 + 1.0 * 6.079530715942383
Epoch 740, val loss: 0.7550248503684998
Epoch 750, training loss: 6.1550092697143555 = 0.07806894183158875 + 1.0 * 6.076940536499023
Epoch 750, val loss: 0.7602784037590027
Epoch 760, training loss: 6.148619651794434 = 0.07423382997512817 + 1.0 * 6.074385643005371
Epoch 760, val loss: 0.7655974626541138
Epoch 770, training loss: 6.150496006011963 = 0.07064060121774673 + 1.0 * 6.079855442047119
Epoch 770, val loss: 0.770911455154419
Epoch 780, training loss: 6.139052867889404 = 0.06731146574020386 + 1.0 * 6.071741580963135
Epoch 780, val loss: 0.7762184143066406
Epoch 790, training loss: 6.134928226470947 = 0.06419563293457031 + 1.0 * 6.070732593536377
Epoch 790, val loss: 0.7815966010093689
Epoch 800, training loss: 6.132318019866943 = 0.06126849725842476 + 1.0 * 6.071049690246582
Epoch 800, val loss: 0.7869881391525269
Epoch 810, training loss: 6.132772922515869 = 0.0585339218378067 + 1.0 * 6.0742387771606445
Epoch 810, val loss: 0.7923808693885803
Epoch 820, training loss: 6.124886512756348 = 0.05595698580145836 + 1.0 * 6.068929672241211
Epoch 820, val loss: 0.7976822257041931
Epoch 830, training loss: 6.120502471923828 = 0.053554560989141464 + 1.0 * 6.066947937011719
Epoch 830, val loss: 0.8030409812927246
Epoch 840, training loss: 6.1166791915893555 = 0.05127860978245735 + 1.0 * 6.06540060043335
Epoch 840, val loss: 0.8084449768066406
Epoch 850, training loss: 6.132218360900879 = 0.049141645431518555 + 1.0 * 6.0830769538879395
Epoch 850, val loss: 0.8137834072113037
Epoch 860, training loss: 6.115070343017578 = 0.04712777957320213 + 1.0 * 6.0679426193237305
Epoch 860, val loss: 0.818936824798584
Epoch 870, training loss: 6.10842227935791 = 0.045238930732011795 + 1.0 * 6.063183307647705
Epoch 870, val loss: 0.8241688013076782
Epoch 880, training loss: 6.105290412902832 = 0.04343797266483307 + 1.0 * 6.06185245513916
Epoch 880, val loss: 0.8294691443443298
Epoch 890, training loss: 6.109186172485352 = 0.04173904284834862 + 1.0 * 6.067447185516357
Epoch 890, val loss: 0.8347586989402771
Epoch 900, training loss: 6.105401039123535 = 0.04014416038990021 + 1.0 * 6.0652570724487305
Epoch 900, val loss: 0.8398353457450867
Epoch 910, training loss: 6.098578929901123 = 0.03865085914731026 + 1.0 * 6.059927940368652
Epoch 910, val loss: 0.8448478579521179
Epoch 920, training loss: 6.09555196762085 = 0.037234123796224594 + 1.0 * 6.0583176612854
Epoch 920, val loss: 0.8498824238777161
Epoch 930, training loss: 6.09769344329834 = 0.035886526107788086 + 1.0 * 6.061806678771973
Epoch 930, val loss: 0.8549124002456665
Epoch 940, training loss: 6.092559814453125 = 0.03462444990873337 + 1.0 * 6.0579352378845215
Epoch 940, val loss: 0.8597572445869446
Epoch 950, training loss: 6.093641757965088 = 0.033418815582990646 + 1.0 * 6.06022310256958
Epoch 950, val loss: 0.8645322322845459
Epoch 960, training loss: 6.088127613067627 = 0.0322781540453434 + 1.0 * 6.055849552154541
Epoch 960, val loss: 0.869340717792511
Epoch 970, training loss: 6.094046592712402 = 0.031193241477012634 + 1.0 * 6.0628533363342285
Epoch 970, val loss: 0.8741101622581482
Epoch 980, training loss: 6.086243629455566 = 0.030154455453157425 + 1.0 * 6.056089401245117
Epoch 980, val loss: 0.8787633776664734
Epoch 990, training loss: 6.082404136657715 = 0.029172632843255997 + 1.0 * 6.053231716156006
Epoch 990, val loss: 0.8834311366081238
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.4945
Flip ASR: 0.4978/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.335418701171875 = 1.9615179300308228 + 1.0 * 8.373900413513184
Epoch 0, val loss: 1.9645957946777344
Epoch 10, training loss: 10.323013305664062 = 1.9500632286071777 + 1.0 * 8.372950553894043
Epoch 10, val loss: 1.9513574838638306
Epoch 20, training loss: 10.304703712463379 = 1.9356005191802979 + 1.0 * 8.36910343170166
Epoch 20, val loss: 1.9333887100219727
Epoch 30, training loss: 10.272518157958984 = 1.9154727458953857 + 1.0 * 8.35704517364502
Epoch 30, val loss: 1.9079065322875977
Epoch 40, training loss: 10.179128646850586 = 1.889100193977356 + 1.0 * 8.29002857208252
Epoch 40, val loss: 1.8765604496002197
Epoch 50, training loss: 9.872568130493164 = 1.863027811050415 + 1.0 * 8.009540557861328
Epoch 50, val loss: 1.8484801054000854
Epoch 60, training loss: 9.562006950378418 = 1.8386400938034058 + 1.0 * 7.723367214202881
Epoch 60, val loss: 1.8241504430770874
Epoch 70, training loss: 9.132417678833008 = 1.8150018453598022 + 1.0 * 7.317416191101074
Epoch 70, val loss: 1.8012360334396362
Epoch 80, training loss: 8.765228271484375 = 1.7925459146499634 + 1.0 * 6.972681999206543
Epoch 80, val loss: 1.7800899744033813
Epoch 90, training loss: 8.51876449584961 = 1.7745643854141235 + 1.0 * 6.744200229644775
Epoch 90, val loss: 1.7632455825805664
Epoch 100, training loss: 8.395570755004883 = 1.7574234008789062 + 1.0 * 6.638147354125977
Epoch 100, val loss: 1.7474490404129028
Epoch 110, training loss: 8.308499336242676 = 1.7385278940200806 + 1.0 * 6.569971561431885
Epoch 110, val loss: 1.7303214073181152
Epoch 120, training loss: 8.24184799194336 = 1.717947006225586 + 1.0 * 6.523900985717773
Epoch 120, val loss: 1.7123525142669678
Epoch 130, training loss: 8.180551528930664 = 1.6957610845565796 + 1.0 * 6.484790325164795
Epoch 130, val loss: 1.6937155723571777
Epoch 140, training loss: 8.123039245605469 = 1.6711740493774414 + 1.0 * 6.451864719390869
Epoch 140, val loss: 1.673930048942566
Epoch 150, training loss: 8.060296058654785 = 1.643181324005127 + 1.0 * 6.417114734649658
Epoch 150, val loss: 1.651979684829712
Epoch 160, training loss: 8.001655578613281 = 1.610140323638916 + 1.0 * 6.391514778137207
Epoch 160, val loss: 1.6264278888702393
Epoch 170, training loss: 7.9400248527526855 = 1.5722283124923706 + 1.0 * 6.367796421051025
Epoch 170, val loss: 1.5974851846694946
Epoch 180, training loss: 7.875744819641113 = 1.529146432876587 + 1.0 * 6.3465986251831055
Epoch 180, val loss: 1.5648977756500244
Epoch 190, training loss: 7.808679580688477 = 1.4809424877166748 + 1.0 * 6.327736854553223
Epoch 190, val loss: 1.5290974378585815
Epoch 200, training loss: 7.7403178215026855 = 1.4282935857772827 + 1.0 * 6.312024116516113
Epoch 200, val loss: 1.4909508228302002
Epoch 210, training loss: 7.670632839202881 = 1.3733477592468262 + 1.0 * 6.297285079956055
Epoch 210, val loss: 1.4520972967147827
Epoch 220, training loss: 7.603590965270996 = 1.3174504041671753 + 1.0 * 6.286140441894531
Epoch 220, val loss: 1.4139463901519775
Epoch 230, training loss: 7.537625789642334 = 1.2618156671524048 + 1.0 * 6.275810241699219
Epoch 230, val loss: 1.3767191171646118
Epoch 240, training loss: 7.470876216888428 = 1.207140326499939 + 1.0 * 6.263735771179199
Epoch 240, val loss: 1.3410476446151733
Epoch 250, training loss: 7.406817436218262 = 1.1539082527160645 + 1.0 * 6.252909183502197
Epoch 250, val loss: 1.3061538934707642
Epoch 260, training loss: 7.34506893157959 = 1.1013810634613037 + 1.0 * 6.243687629699707
Epoch 260, val loss: 1.2717643976211548
Epoch 270, training loss: 7.290309906005859 = 1.0500890016555786 + 1.0 * 6.24022102355957
Epoch 270, val loss: 1.238141417503357
Epoch 280, training loss: 7.227869987487793 = 1.0007745027542114 + 1.0 * 6.227095603942871
Epoch 280, val loss: 1.2053390741348267
Epoch 290, training loss: 7.172471046447754 = 0.952541708946228 + 1.0 * 6.219929218292236
Epoch 290, val loss: 1.1730891466140747
Epoch 300, training loss: 7.118627071380615 = 0.9052724242210388 + 1.0 * 6.213354587554932
Epoch 300, val loss: 1.1413477659225464
Epoch 310, training loss: 7.066557884216309 = 0.8596621751785278 + 1.0 * 6.20689582824707
Epoch 310, val loss: 1.1108449697494507
Epoch 320, training loss: 7.0182204246521 = 0.8165125846862793 + 1.0 * 6.20170783996582
Epoch 320, val loss: 1.0820927619934082
Epoch 330, training loss: 6.971579074859619 = 0.7754656076431274 + 1.0 * 6.196113586425781
Epoch 330, val loss: 1.0550531148910522
Epoch 340, training loss: 6.9273681640625 = 0.736484944820404 + 1.0 * 6.190883159637451
Epoch 340, val loss: 1.0294815301895142
Epoch 350, training loss: 6.889618873596191 = 0.6997889876365662 + 1.0 * 6.1898298263549805
Epoch 350, val loss: 1.0054874420166016
Epoch 360, training loss: 6.849123477935791 = 0.665736734867096 + 1.0 * 6.18338680267334
Epoch 360, val loss: 0.9834511280059814
Epoch 370, training loss: 6.811041831970215 = 0.6338047385215759 + 1.0 * 6.177237033843994
Epoch 370, val loss: 0.9631766080856323
Epoch 380, training loss: 6.794796943664551 = 0.6039173603057861 + 1.0 * 6.190879821777344
Epoch 380, val loss: 0.9444212317466736
Epoch 390, training loss: 6.750391960144043 = 0.5760771036148071 + 1.0 * 6.174314975738525
Epoch 390, val loss: 0.9273732304573059
Epoch 400, training loss: 6.716695785522461 = 0.5499160289764404 + 1.0 * 6.1667799949646
Epoch 400, val loss: 0.9119465947151184
Epoch 410, training loss: 6.687475681304932 = 0.5251042246818542 + 1.0 * 6.162371635437012
Epoch 410, val loss: 0.897655725479126
Epoch 420, training loss: 6.659492015838623 = 0.5014472603797913 + 1.0 * 6.158044815063477
Epoch 420, val loss: 0.8843408823013306
Epoch 430, training loss: 6.659331321716309 = 0.4788194000720978 + 1.0 * 6.180511951446533
Epoch 430, val loss: 0.8718844652175903
Epoch 440, training loss: 6.612524509429932 = 0.45755428075790405 + 1.0 * 6.154970169067383
Epoch 440, val loss: 0.8601753115653992
Epoch 450, training loss: 6.587192535400391 = 0.4372461140155792 + 1.0 * 6.149946212768555
Epoch 450, val loss: 0.8494015336036682
Epoch 460, training loss: 6.5648393630981445 = 0.41766756772994995 + 1.0 * 6.147171974182129
Epoch 460, val loss: 0.8391785025596619
Epoch 470, training loss: 6.542557239532471 = 0.39858973026275635 + 1.0 * 6.143967628479004
Epoch 470, val loss: 0.829349935054779
Epoch 480, training loss: 6.523626327514648 = 0.37987321615219116 + 1.0 * 6.1437530517578125
Epoch 480, val loss: 0.8195695281028748
Epoch 490, training loss: 6.508665084838867 = 0.36163824796676636 + 1.0 * 6.147027015686035
Epoch 490, val loss: 0.8099720478057861
Epoch 500, training loss: 6.483311176300049 = 0.3438890874385834 + 1.0 * 6.1394219398498535
Epoch 500, val loss: 0.8006809949874878
Epoch 510, training loss: 6.461065769195557 = 0.32638609409332275 + 1.0 * 6.134679794311523
Epoch 510, val loss: 0.791815996170044
Epoch 520, training loss: 6.443256378173828 = 0.3092005252838135 + 1.0 * 6.134056091308594
Epoch 520, val loss: 0.7838423848152161
Epoch 530, training loss: 6.424942493438721 = 0.29265227913856506 + 1.0 * 6.132290363311768
Epoch 530, val loss: 0.7771042585372925
Epoch 540, training loss: 6.407599449157715 = 0.27698269486427307 + 1.0 * 6.130616664886475
Epoch 540, val loss: 0.7717069983482361
Epoch 550, training loss: 6.388690948486328 = 0.26212358474731445 + 1.0 * 6.126567363739014
Epoch 550, val loss: 0.7674937844276428
Epoch 560, training loss: 6.373361587524414 = 0.24798685312271118 + 1.0 * 6.125374794006348
Epoch 560, val loss: 0.7640045881271362
Epoch 570, training loss: 6.37010383605957 = 0.23453949391841888 + 1.0 * 6.13556432723999
Epoch 570, val loss: 0.7609833478927612
Epoch 580, training loss: 6.3458356857299805 = 0.2217462807893753 + 1.0 * 6.124089241027832
Epoch 580, val loss: 0.7586175799369812
Epoch 590, training loss: 6.33057975769043 = 0.2096899449825287 + 1.0 * 6.120889663696289
Epoch 590, val loss: 0.7567508816719055
Epoch 600, training loss: 6.3163042068481445 = 0.1982569396495819 + 1.0 * 6.11804723739624
Epoch 600, val loss: 0.7554603815078735
Epoch 610, training loss: 6.310307502746582 = 0.18745586276054382 + 1.0 * 6.122851848602295
Epoch 610, val loss: 0.7547863721847534
Epoch 620, training loss: 6.294404983520508 = 0.17730407416820526 + 1.0 * 6.117100715637207
Epoch 620, val loss: 0.754706859588623
Epoch 630, training loss: 6.280792236328125 = 0.16781358420848846 + 1.0 * 6.112978458404541
Epoch 630, val loss: 0.7551534175872803
Epoch 640, training loss: 6.2721781730651855 = 0.15891356766223907 + 1.0 * 6.113264560699463
Epoch 640, val loss: 0.7561444640159607
Epoch 650, training loss: 6.2617011070251465 = 0.15055115520954132 + 1.0 * 6.111149787902832
Epoch 650, val loss: 0.7576814889907837
Epoch 660, training loss: 6.259189128875732 = 0.14271728694438934 + 1.0 * 6.116471767425537
Epoch 660, val loss: 0.7595906853675842
Epoch 670, training loss: 6.241762161254883 = 0.13537831604480743 + 1.0 * 6.106383800506592
Epoch 670, val loss: 0.7618696689605713
Epoch 680, training loss: 6.234593868255615 = 0.12853966653347015 + 1.0 * 6.106054306030273
Epoch 680, val loss: 0.7645291090011597
Epoch 690, training loss: 6.225204944610596 = 0.12210314720869064 + 1.0 * 6.10310173034668
Epoch 690, val loss: 0.7676016688346863
Epoch 700, training loss: 6.2188191413879395 = 0.11603660136461258 + 1.0 * 6.102782726287842
Epoch 700, val loss: 0.7710133790969849
Epoch 710, training loss: 6.222114562988281 = 0.11032521724700928 + 1.0 * 6.111789226531982
Epoch 710, val loss: 0.7746009826660156
Epoch 720, training loss: 6.206645965576172 = 0.10499715059995651 + 1.0 * 6.101648807525635
Epoch 720, val loss: 0.7782965302467346
Epoch 730, training loss: 6.198578834533691 = 0.09998267143964767 + 1.0 * 6.098596096038818
Epoch 730, val loss: 0.782232940196991
Epoch 740, training loss: 6.20384407043457 = 0.09524937719106674 + 1.0 * 6.10859489440918
Epoch 740, val loss: 0.7864123582839966
Epoch 750, training loss: 6.189705848693848 = 0.09081766754388809 + 1.0 * 6.098888397216797
Epoch 750, val loss: 0.790545642375946
Epoch 760, training loss: 6.1815505027771 = 0.08661755174398422 + 1.0 * 6.094933032989502
Epoch 760, val loss: 0.7950143218040466
Epoch 770, training loss: 6.174035549163818 = 0.0826578289270401 + 1.0 * 6.0913777351379395
Epoch 770, val loss: 0.7995335459709167
Epoch 780, training loss: 6.172365188598633 = 0.07890943437814713 + 1.0 * 6.093455791473389
Epoch 780, val loss: 0.8041718602180481
Epoch 790, training loss: 6.167585849761963 = 0.07536140084266663 + 1.0 * 6.092224597930908
Epoch 790, val loss: 0.808858335018158
Epoch 800, training loss: 6.161741733551025 = 0.07202419638633728 + 1.0 * 6.089717388153076
Epoch 800, val loss: 0.8134756684303284
Epoch 810, training loss: 6.155907154083252 = 0.0688696950674057 + 1.0 * 6.087037563323975
Epoch 810, val loss: 0.8183009624481201
Epoch 820, training loss: 6.152754783630371 = 0.06588607281446457 + 1.0 * 6.086868762969971
Epoch 820, val loss: 0.8231319785118103
Epoch 830, training loss: 6.150121212005615 = 0.06306677311658859 + 1.0 * 6.087054252624512
Epoch 830, val loss: 0.8279225826263428
Epoch 840, training loss: 6.147441864013672 = 0.06040435656905174 + 1.0 * 6.087037563323975
Epoch 840, val loss: 0.8326445817947388
Epoch 850, training loss: 6.141996383666992 = 0.05789593607187271 + 1.0 * 6.084100246429443
Epoch 850, val loss: 0.8374286890029907
Epoch 860, training loss: 6.144051551818848 = 0.05552187189459801 + 1.0 * 6.088529586791992
Epoch 860, val loss: 0.8421767950057983
Epoch 870, training loss: 6.132965564727783 = 0.05326560139656067 + 1.0 * 6.079699993133545
Epoch 870, val loss: 0.8470115661621094
Epoch 880, training loss: 6.132411003112793 = 0.05112902820110321 + 1.0 * 6.081282138824463
Epoch 880, val loss: 0.8517767190933228
Epoch 890, training loss: 6.126918792724609 = 0.0491032674908638 + 1.0 * 6.077815532684326
Epoch 890, val loss: 0.8565139174461365
Epoch 900, training loss: 6.130778789520264 = 0.047178398817777634 + 1.0 * 6.0836005210876465
Epoch 900, val loss: 0.8612696528434753
Epoch 910, training loss: 6.126317024230957 = 0.04536031186580658 + 1.0 * 6.080956935882568
Epoch 910, val loss: 0.8659740090370178
Epoch 920, training loss: 6.11911678314209 = 0.043637070804834366 + 1.0 * 6.075479507446289
Epoch 920, val loss: 0.8705126047134399
Epoch 930, training loss: 6.117781639099121 = 0.0420030802488327 + 1.0 * 6.075778484344482
Epoch 930, val loss: 0.8750821948051453
Epoch 940, training loss: 6.1142754554748535 = 0.040450580418109894 + 1.0 * 6.073824882507324
Epoch 940, val loss: 0.879720151424408
Epoch 950, training loss: 6.114349365234375 = 0.038973428308963776 + 1.0 * 6.075376033782959
Epoch 950, val loss: 0.8842398524284363
Epoch 960, training loss: 6.108832359313965 = 0.03756887465715408 + 1.0 * 6.071263313293457
Epoch 960, val loss: 0.8887478709220886
Epoch 970, training loss: 6.107309818267822 = 0.036233775317668915 + 1.0 * 6.071075916290283
Epoch 970, val loss: 0.893252968788147
Epoch 980, training loss: 6.1151933670043945 = 0.03496214374899864 + 1.0 * 6.080231189727783
Epoch 980, val loss: 0.8977157473564148
Epoch 990, training loss: 6.104493618011475 = 0.033759601414203644 + 1.0 * 6.070734024047852
Epoch 990, val loss: 0.9019550681114197
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.73555, 0.19585, Accuracy:0.78765, 0.01259
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98401, 0.00627, Accuracy:0.83333, 0.00302
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.339729309082031 = 1.9658780097961426 + 1.0 * 8.373851776123047
Epoch 0, val loss: 1.9664123058319092
Epoch 10, training loss: 10.328099250793457 = 1.954642415046692 + 1.0 * 8.373456954956055
Epoch 10, val loss: 1.9560246467590332
Epoch 20, training loss: 10.311614990234375 = 1.9408129453659058 + 1.0 * 8.37080192565918
Epoch 20, val loss: 1.9428828954696655
Epoch 30, training loss: 10.272625923156738 = 1.9213887453079224 + 1.0 * 8.351237297058105
Epoch 30, val loss: 1.9241466522216797
Epoch 40, training loss: 10.095318794250488 = 1.8957226276397705 + 1.0 * 8.199596405029297
Epoch 40, val loss: 1.899969220161438
Epoch 50, training loss: 9.325736999511719 = 1.869217872619629 + 1.0 * 7.45651912689209
Epoch 50, val loss: 1.8755625486373901
Epoch 60, training loss: 8.884434700012207 = 1.850861668586731 + 1.0 * 7.033573150634766
Epoch 60, val loss: 1.8590672016143799
Epoch 70, training loss: 8.60815143585205 = 1.835876226425171 + 1.0 * 6.772275447845459
Epoch 70, val loss: 1.8453096151351929
Epoch 80, training loss: 8.433281898498535 = 1.8209184408187866 + 1.0 * 6.612363815307617
Epoch 80, val loss: 1.8321585655212402
Epoch 90, training loss: 8.306695938110352 = 1.8055059909820557 + 1.0 * 6.501189708709717
Epoch 90, val loss: 1.818886399269104
Epoch 100, training loss: 8.217984199523926 = 1.7902429103851318 + 1.0 * 6.427741527557373
Epoch 100, val loss: 1.8054414987564087
Epoch 110, training loss: 8.149834632873535 = 1.7747820615768433 + 1.0 * 6.3750529289245605
Epoch 110, val loss: 1.7915281057357788
Epoch 120, training loss: 8.093644142150879 = 1.758987307548523 + 1.0 * 6.334656715393066
Epoch 120, val loss: 1.7774542570114136
Epoch 130, training loss: 8.045979499816895 = 1.7420315742492676 + 1.0 * 6.303947925567627
Epoch 130, val loss: 1.7628498077392578
Epoch 140, training loss: 8.004273414611816 = 1.7229077816009521 + 1.0 * 6.281365871429443
Epoch 140, val loss: 1.747169852256775
Epoch 150, training loss: 7.9635796546936035 = 1.7008376121520996 + 1.0 * 6.262742042541504
Epoch 150, val loss: 1.7295185327529907
Epoch 160, training loss: 7.922361373901367 = 1.67493736743927 + 1.0 * 6.247424125671387
Epoch 160, val loss: 1.709081768989563
Epoch 170, training loss: 7.878931999206543 = 1.6445108652114868 + 1.0 * 6.234421253204346
Epoch 170, val loss: 1.6851544380187988
Epoch 180, training loss: 7.833817481994629 = 1.6088459491729736 + 1.0 * 6.224971294403076
Epoch 180, val loss: 1.6568788290023804
Epoch 190, training loss: 7.782320499420166 = 1.5685415267944336 + 1.0 * 6.213778972625732
Epoch 190, val loss: 1.625000238418579
Epoch 200, training loss: 7.728850364685059 = 1.523999571800232 + 1.0 * 6.204850673675537
Epoch 200, val loss: 1.5895731449127197
Epoch 210, training loss: 7.67234992980957 = 1.4757232666015625 + 1.0 * 6.196626663208008
Epoch 210, val loss: 1.5508977174758911
Epoch 220, training loss: 7.614466190338135 = 1.424864411354065 + 1.0 * 6.189601898193359
Epoch 220, val loss: 1.5100035667419434
Epoch 230, training loss: 7.559654712677002 = 1.3734335899353027 + 1.0 * 6.186221122741699
Epoch 230, val loss: 1.4686846733093262
Epoch 240, training loss: 7.499964237213135 = 1.3229349851608276 + 1.0 * 6.177029132843018
Epoch 240, val loss: 1.427976131439209
Epoch 250, training loss: 7.444706439971924 = 1.2731317281723022 + 1.0 * 6.171574592590332
Epoch 250, val loss: 1.387784719467163
Epoch 260, training loss: 7.395657062530518 = 1.224905014038086 + 1.0 * 6.170752048492432
Epoch 260, val loss: 1.3492494821548462
Epoch 270, training loss: 7.339902400970459 = 1.1787900924682617 + 1.0 * 6.161112308502197
Epoch 270, val loss: 1.3127477169036865
Epoch 280, training loss: 7.290511608123779 = 1.1342802047729492 + 1.0 * 6.15623140335083
Epoch 280, val loss: 1.2776046991348267
Epoch 290, training loss: 7.243793964385986 = 1.0912566184997559 + 1.0 * 6.1525373458862305
Epoch 290, val loss: 1.2439557313919067
Epoch 300, training loss: 7.2017822265625 = 1.050426721572876 + 1.0 * 6.151355743408203
Epoch 300, val loss: 1.2120953798294067
Epoch 310, training loss: 7.156373023986816 = 1.0116031169891357 + 1.0 * 6.14477014541626
Epoch 310, val loss: 1.1822714805603027
Epoch 320, training loss: 7.115772247314453 = 0.9742708802223206 + 1.0 * 6.141501426696777
Epoch 320, val loss: 1.1537443399429321
Epoch 330, training loss: 7.077779769897461 = 0.9382418990135193 + 1.0 * 6.139537811279297
Epoch 330, val loss: 1.1263154745101929
Epoch 340, training loss: 7.036369323730469 = 0.903083860874176 + 1.0 * 6.1332855224609375
Epoch 340, val loss: 1.0998138189315796
Epoch 350, training loss: 6.998311996459961 = 0.8681333065032959 + 1.0 * 6.130178928375244
Epoch 350, val loss: 1.0734617710113525
Epoch 360, training loss: 6.964139461517334 = 0.8329144716262817 + 1.0 * 6.131225109100342
Epoch 360, val loss: 1.046881079673767
Epoch 370, training loss: 6.923338890075684 = 0.7975479960441589 + 1.0 * 6.125791072845459
Epoch 370, val loss: 1.0201741456985474
Epoch 380, training loss: 6.883355140686035 = 0.761576771736145 + 1.0 * 6.12177848815918
Epoch 380, val loss: 0.9929295182228088
Epoch 390, training loss: 6.844326019287109 = 0.7248934507369995 + 1.0 * 6.11943244934082
Epoch 390, val loss: 0.964948296546936
Epoch 400, training loss: 6.806532859802246 = 0.687830924987793 + 1.0 * 6.118701934814453
Epoch 400, val loss: 0.9367300868034363
Epoch 410, training loss: 6.7680206298828125 = 0.6510902047157288 + 1.0 * 6.1169304847717285
Epoch 410, val loss: 0.9088600277900696
Epoch 420, training loss: 6.728640079498291 = 0.6151965260505676 + 1.0 * 6.113443374633789
Epoch 420, val loss: 0.8817898631095886
Epoch 430, training loss: 6.691469192504883 = 0.5803200006484985 + 1.0 * 6.111149311065674
Epoch 430, val loss: 0.8559889197349548
Epoch 440, training loss: 6.656016826629639 = 0.5466368794441223 + 1.0 * 6.109379768371582
Epoch 440, val loss: 0.8315643668174744
Epoch 450, training loss: 6.624866485595703 = 0.514502763748169 + 1.0 * 6.110363483428955
Epoch 450, val loss: 0.8089589476585388
Epoch 460, training loss: 6.588078498840332 = 0.48418933153152466 + 1.0 * 6.103888988494873
Epoch 460, val loss: 0.7885677218437195
Epoch 470, training loss: 6.557568073272705 = 0.45547446608543396 + 1.0 * 6.102093696594238
Epoch 470, val loss: 0.770046055316925
Epoch 480, training loss: 6.534450054168701 = 0.4282494783401489 + 1.0 * 6.106200695037842
Epoch 480, val loss: 0.7533086538314819
Epoch 490, training loss: 6.505416393280029 = 0.40261900424957275 + 1.0 * 6.102797508239746
Epoch 490, val loss: 0.738496720790863
Epoch 500, training loss: 6.478886604309082 = 0.37846770882606506 + 1.0 * 6.100419044494629
Epoch 500, val loss: 0.7254207730293274
Epoch 510, training loss: 6.4512176513671875 = 0.35578468441963196 + 1.0 * 6.095432758331299
Epoch 510, val loss: 0.7140551209449768
Epoch 520, training loss: 6.427069664001465 = 0.3343077301979065 + 1.0 * 6.092761993408203
Epoch 520, val loss: 0.703948974609375
Epoch 530, training loss: 6.408902645111084 = 0.3140300512313843 + 1.0 * 6.09487247467041
Epoch 530, val loss: 0.6950315833091736
Epoch 540, training loss: 6.387536525726318 = 0.29504460096359253 + 1.0 * 6.09249210357666
Epoch 540, val loss: 0.6874516606330872
Epoch 550, training loss: 6.367530345916748 = 0.2771850526332855 + 1.0 * 6.09034538269043
Epoch 550, val loss: 0.6809262037277222
Epoch 560, training loss: 6.346739768981934 = 0.26035240292549133 + 1.0 * 6.0863871574401855
Epoch 560, val loss: 0.6753537058830261
Epoch 570, training loss: 6.329344749450684 = 0.2444080412387848 + 1.0 * 6.084936618804932
Epoch 570, val loss: 0.6706996560096741
Epoch 580, training loss: 6.316002368927002 = 0.22934076189994812 + 1.0 * 6.0866618156433105
Epoch 580, val loss: 0.6666486859321594
Epoch 590, training loss: 6.303202152252197 = 0.21515773236751556 + 1.0 * 6.0880446434021
Epoch 590, val loss: 0.6634432077407837
Epoch 600, training loss: 6.287936687469482 = 0.20190474390983582 + 1.0 * 6.086031913757324
Epoch 600, val loss: 0.6608580350875854
Epoch 610, training loss: 6.268558502197266 = 0.1893492192029953 + 1.0 * 6.079209327697754
Epoch 610, val loss: 0.6588996052742004
Epoch 620, training loss: 6.2547287940979 = 0.17747947573661804 + 1.0 * 6.077249526977539
Epoch 620, val loss: 0.6573673486709595
Epoch 630, training loss: 6.241795539855957 = 0.16624364256858826 + 1.0 * 6.075551986694336
Epoch 630, val loss: 0.6563476920127869
Epoch 640, training loss: 6.2331862449646 = 0.15562498569488525 + 1.0 * 6.077561378479004
Epoch 640, val loss: 0.6557760834693909
Epoch 650, training loss: 6.2269978523254395 = 0.14567814767360687 + 1.0 * 6.081319808959961
Epoch 650, val loss: 0.6555202007293701
Epoch 660, training loss: 6.214404106140137 = 0.1364622265100479 + 1.0 * 6.07794189453125
Epoch 660, val loss: 0.6557786464691162
Epoch 670, training loss: 6.2001752853393555 = 0.12785668671131134 + 1.0 * 6.0723185539245605
Epoch 670, val loss: 0.6561895608901978
Epoch 680, training loss: 6.189934730529785 = 0.11984754353761673 + 1.0 * 6.07008695602417
Epoch 680, val loss: 0.6569839119911194
Epoch 690, training loss: 6.185515880584717 = 0.11238430440425873 + 1.0 * 6.073131561279297
Epoch 690, val loss: 0.6580758690834045
Epoch 700, training loss: 6.184453964233398 = 0.10549307614564896 + 1.0 * 6.07896089553833
Epoch 700, val loss: 0.6593903303146362
Epoch 710, training loss: 6.166666030883789 = 0.09916128218173981 + 1.0 * 6.0675048828125
Epoch 710, val loss: 0.6609512567520142
Epoch 720, training loss: 6.1585917472839355 = 0.09327897429466248 + 1.0 * 6.06531286239624
Epoch 720, val loss: 0.6627101898193359
Epoch 730, training loss: 6.159058094024658 = 0.08783973008394241 + 1.0 * 6.071218490600586
Epoch 730, val loss: 0.6646798253059387
Epoch 740, training loss: 6.153350830078125 = 0.0828109011054039 + 1.0 * 6.070539951324463
Epoch 740, val loss: 0.6665763854980469
Epoch 750, training loss: 6.140612602233887 = 0.07820189744234085 + 1.0 * 6.062410831451416
Epoch 750, val loss: 0.6688931584358215
Epoch 760, training loss: 6.134873390197754 = 0.07392462342977524 + 1.0 * 6.060948848724365
Epoch 760, val loss: 0.6713089942932129
Epoch 770, training loss: 6.130339622497559 = 0.06994282454252243 + 1.0 * 6.060396671295166
Epoch 770, val loss: 0.673832356929779
Epoch 780, training loss: 6.125428199768066 = 0.06626205891370773 + 1.0 * 6.059165954589844
Epoch 780, val loss: 0.6762710213661194
Epoch 790, training loss: 6.1215338706970215 = 0.06288322806358337 + 1.0 * 6.058650493621826
Epoch 790, val loss: 0.6788292527198792
Epoch 800, training loss: 6.116884231567383 = 0.059740036725997925 + 1.0 * 6.0571441650390625
Epoch 800, val loss: 0.6814927458763123
Epoch 810, training loss: 6.115644454956055 = 0.056804873049259186 + 1.0 * 6.058839797973633
Epoch 810, val loss: 0.6841649413108826
Epoch 820, training loss: 6.110674858093262 = 0.054083459079265594 + 1.0 * 6.056591510772705
Epoch 820, val loss: 0.6868685483932495
Epoch 830, training loss: 6.107097148895264 = 0.05154582858085632 + 1.0 * 6.055551528930664
Epoch 830, val loss: 0.68961501121521
Epoch 840, training loss: 6.102818965911865 = 0.04917703568935394 + 1.0 * 6.0536417961120605
Epoch 840, val loss: 0.6924654245376587
Epoch 850, training loss: 6.104711532592773 = 0.04696819558739662 + 1.0 * 6.057743549346924
Epoch 850, val loss: 0.6952458024024963
Epoch 860, training loss: 6.099308967590332 = 0.04490276798605919 + 1.0 * 6.05440616607666
Epoch 860, val loss: 0.6980747580528259
Epoch 870, training loss: 6.0938615798950195 = 0.04297216236591339 + 1.0 * 6.050889492034912
Epoch 870, val loss: 0.7009502649307251
Epoch 880, training loss: 6.092904090881348 = 0.04115423187613487 + 1.0 * 6.0517497062683105
Epoch 880, val loss: 0.7038106918334961
Epoch 890, training loss: 6.092224597930908 = 0.03944782540202141 + 1.0 * 6.05277681350708
Epoch 890, val loss: 0.7066110968589783
Epoch 900, training loss: 6.088133335113525 = 0.03785444796085358 + 1.0 * 6.050278663635254
Epoch 900, val loss: 0.7094616293907166
Epoch 910, training loss: 6.08387565612793 = 0.03635670244693756 + 1.0 * 6.047518730163574
Epoch 910, val loss: 0.7123644351959229
Epoch 920, training loss: 6.086940765380859 = 0.03494234383106232 + 1.0 * 6.051998615264893
Epoch 920, val loss: 0.7151980400085449
Epoch 930, training loss: 6.082427978515625 = 0.033606912940740585 + 1.0 * 6.048820972442627
Epoch 930, val loss: 0.7180250883102417
Epoch 940, training loss: 6.086171627044678 = 0.0323498472571373 + 1.0 * 6.053821563720703
Epoch 940, val loss: 0.7208399176597595
Epoch 950, training loss: 6.077406406402588 = 0.031175434589385986 + 1.0 * 6.046230792999268
Epoch 950, val loss: 0.7237597107887268
Epoch 960, training loss: 6.074735164642334 = 0.030053257942199707 + 1.0 * 6.044682025909424
Epoch 960, val loss: 0.7265418171882629
Epoch 970, training loss: 6.0798258781433105 = 0.028992442414164543 + 1.0 * 6.050833225250244
Epoch 970, val loss: 0.7292951345443726
Epoch 980, training loss: 6.072599411010742 = 0.027994031086564064 + 1.0 * 6.044605255126953
Epoch 980, val loss: 0.7321157455444336
Epoch 990, training loss: 6.069211006164551 = 0.027040230110287666 + 1.0 * 6.042171001434326
Epoch 990, val loss: 0.7349088191986084
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6421
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.311062812805176 = 1.9371583461761475 + 1.0 * 8.37390422821045
Epoch 0, val loss: 1.9309571981430054
Epoch 10, training loss: 10.300687789916992 = 1.9271589517593384 + 1.0 * 8.373528480529785
Epoch 10, val loss: 1.9206088781356812
Epoch 20, training loss: 10.285805702209473 = 1.9147155284881592 + 1.0 * 8.371089935302734
Epoch 20, val loss: 1.9070109128952026
Epoch 30, training loss: 10.251456260681152 = 1.8977057933807373 + 1.0 * 8.353750228881836
Epoch 30, val loss: 1.887865424156189
Epoch 40, training loss: 10.08094310760498 = 1.8761550188064575 + 1.0 * 8.204788208007812
Epoch 40, val loss: 1.864185094833374
Epoch 50, training loss: 9.201635360717773 = 1.8556902408599854 + 1.0 * 7.345945358276367
Epoch 50, val loss: 1.8423107862472534
Epoch 60, training loss: 8.76784610748291 = 1.8396004438400269 + 1.0 * 6.928246021270752
Epoch 60, val loss: 1.8261104822158813
Epoch 70, training loss: 8.579894065856934 = 1.823983907699585 + 1.0 * 6.755910396575928
Epoch 70, val loss: 1.8103173971176147
Epoch 80, training loss: 8.44986629486084 = 1.8083356618881226 + 1.0 * 6.641530513763428
Epoch 80, val loss: 1.7950959205627441
Epoch 90, training loss: 8.348650932312012 = 1.7923699617385864 + 1.0 * 6.556281089782715
Epoch 90, val loss: 1.7797722816467285
Epoch 100, training loss: 8.268025398254395 = 1.7764414548873901 + 1.0 * 6.491584300994873
Epoch 100, val loss: 1.764939308166504
Epoch 110, training loss: 8.200109481811523 = 1.7598799467086792 + 1.0 * 6.440229892730713
Epoch 110, val loss: 1.7498655319213867
Epoch 120, training loss: 8.141838073730469 = 1.7418649196624756 + 1.0 * 6.399972915649414
Epoch 120, val loss: 1.7337663173675537
Epoch 130, training loss: 8.089933395385742 = 1.7216978073120117 + 1.0 * 6.3682355880737305
Epoch 130, val loss: 1.7162952423095703
Epoch 140, training loss: 8.040750503540039 = 1.6988141536712646 + 1.0 * 6.3419365882873535
Epoch 140, val loss: 1.696881890296936
Epoch 150, training loss: 7.9904584884643555 = 1.6722347736358643 + 1.0 * 6.31822395324707
Epoch 150, val loss: 1.6747407913208008
Epoch 160, training loss: 7.939729690551758 = 1.6403369903564453 + 1.0 * 6.2993927001953125
Epoch 160, val loss: 1.6486815214157104
Epoch 170, training loss: 7.884617328643799 = 1.6024022102355957 + 1.0 * 6.282215118408203
Epoch 170, val loss: 1.6179981231689453
Epoch 180, training loss: 7.8261260986328125 = 1.5582743883132935 + 1.0 * 6.267851829528809
Epoch 180, val loss: 1.5826480388641357
Epoch 190, training loss: 7.7613115310668945 = 1.5081018209457397 + 1.0 * 6.253209590911865
Epoch 190, val loss: 1.5427738428115845
Epoch 200, training loss: 7.695393085479736 = 1.4529123306274414 + 1.0 * 6.242480754852295
Epoch 200, val loss: 1.499074935913086
Epoch 210, training loss: 7.628851890563965 = 1.3962470293045044 + 1.0 * 6.23260498046875
Epoch 210, val loss: 1.4547642469406128
Epoch 220, training loss: 7.560508728027344 = 1.3399937152862549 + 1.0 * 6.220515251159668
Epoch 220, val loss: 1.4113824367523193
Epoch 230, training loss: 7.495410919189453 = 1.2848267555236816 + 1.0 * 6.2105841636657715
Epoch 230, val loss: 1.369745135307312
Epoch 240, training loss: 7.44060754776001 = 1.2323867082595825 + 1.0 * 6.208220958709717
Epoch 240, val loss: 1.330742597579956
Epoch 250, training loss: 7.37858772277832 = 1.183133602142334 + 1.0 * 6.195454120635986
Epoch 250, val loss: 1.294752597808838
Epoch 260, training loss: 7.323940277099609 = 1.1360292434692383 + 1.0 * 6.187911033630371
Epoch 260, val loss: 1.2606568336486816
Epoch 270, training loss: 7.272030830383301 = 1.0900317430496216 + 1.0 * 6.181999206542969
Epoch 270, val loss: 1.227454662322998
Epoch 280, training loss: 7.2210307121276855 = 1.0453966856002808 + 1.0 * 6.175633907318115
Epoch 280, val loss: 1.1954338550567627
Epoch 290, training loss: 7.173151969909668 = 1.0019484758377075 + 1.0 * 6.17120361328125
Epoch 290, val loss: 1.1642292737960815
Epoch 300, training loss: 7.124533176422119 = 0.9589746594429016 + 1.0 * 6.165558338165283
Epoch 300, val loss: 1.1334879398345947
Epoch 310, training loss: 7.078763961791992 = 0.9167545437812805 + 1.0 * 6.162009239196777
Epoch 310, val loss: 1.1032443046569824
Epoch 320, training loss: 7.03132438659668 = 0.8753836750984192 + 1.0 * 6.155940532684326
Epoch 320, val loss: 1.0738492012023926
Epoch 330, training loss: 6.988376617431641 = 0.8351203799247742 + 1.0 * 6.153256416320801
Epoch 330, val loss: 1.0452957153320312
Epoch 340, training loss: 6.944116115570068 = 0.7964832186698914 + 1.0 * 6.147633075714111
Epoch 340, val loss: 1.0180250406265259
Epoch 350, training loss: 6.906032562255859 = 0.7594223618507385 + 1.0 * 6.146610260009766
Epoch 350, val loss: 0.9920151233673096
Epoch 360, training loss: 6.869938850402832 = 0.7245774865150452 + 1.0 * 6.145361423492432
Epoch 360, val loss: 0.9678050875663757
Epoch 370, training loss: 6.829746246337891 = 0.6920510530471802 + 1.0 * 6.1376953125
Epoch 370, val loss: 0.945697009563446
Epoch 380, training loss: 6.796181678771973 = 0.661497950553894 + 1.0 * 6.134683609008789
Epoch 380, val loss: 0.9254079461097717
Epoch 390, training loss: 6.7663044929504395 = 0.6327770352363586 + 1.0 * 6.1335272789001465
Epoch 390, val loss: 0.9068020582199097
Epoch 400, training loss: 6.7343597412109375 = 0.6059277653694153 + 1.0 * 6.128431797027588
Epoch 400, val loss: 0.8901773691177368
Epoch 410, training loss: 6.705315589904785 = 0.5803248882293701 + 1.0 * 6.124990940093994
Epoch 410, val loss: 0.8750580549240112
Epoch 420, training loss: 6.683582782745361 = 0.5556135177612305 + 1.0 * 6.127969264984131
Epoch 420, val loss: 0.8610585331916809
Epoch 430, training loss: 6.656700134277344 = 0.5318459272384644 + 1.0 * 6.12485408782959
Epoch 430, val loss: 0.8481137156486511
Epoch 440, training loss: 6.630298137664795 = 0.5090217590332031 + 1.0 * 6.121276378631592
Epoch 440, val loss: 0.8363173604011536
Epoch 450, training loss: 6.601933479309082 = 0.4867155849933624 + 1.0 * 6.115217685699463
Epoch 450, val loss: 0.825294554233551
Epoch 460, training loss: 6.577093601226807 = 0.46481773257255554 + 1.0 * 6.112276077270508
Epoch 460, val loss: 0.814850389957428
Epoch 470, training loss: 6.558080196380615 = 0.443234384059906 + 1.0 * 6.1148457527160645
Epoch 470, val loss: 0.8049997091293335
Epoch 480, training loss: 6.538612365722656 = 0.4222521185874939 + 1.0 * 6.116360187530518
Epoch 480, val loss: 0.7957813739776611
Epoch 490, training loss: 6.510771751403809 = 0.4018368124961853 + 1.0 * 6.1089348793029785
Epoch 490, val loss: 0.7873504757881165
Epoch 500, training loss: 6.4867095947265625 = 0.3820056617259979 + 1.0 * 6.104703903198242
Epoch 500, val loss: 0.7796584963798523
Epoch 510, training loss: 6.471576690673828 = 0.36274173855781555 + 1.0 * 6.108834743499756
Epoch 510, val loss: 0.772617518901825
Epoch 520, training loss: 6.445209503173828 = 0.3441661298274994 + 1.0 * 6.101043224334717
Epoch 520, val loss: 0.7663955092430115
Epoch 530, training loss: 6.433101654052734 = 0.3262578845024109 + 1.0 * 6.106843948364258
Epoch 530, val loss: 0.7610258460044861
Epoch 540, training loss: 6.408207416534424 = 0.30908191204071045 + 1.0 * 6.099125385284424
Epoch 540, val loss: 0.7564552426338196
Epoch 550, training loss: 6.389401912689209 = 0.292550653219223 + 1.0 * 6.096851348876953
Epoch 550, val loss: 0.7526848316192627
Epoch 560, training loss: 6.374277114868164 = 0.27663832902908325 + 1.0 * 6.0976386070251465
Epoch 560, val loss: 0.7494170665740967
Epoch 570, training loss: 6.355103969573975 = 0.2613791227340698 + 1.0 * 6.093724727630615
Epoch 570, val loss: 0.746832013130188
Epoch 580, training loss: 6.3378214836120605 = 0.2466457188129425 + 1.0 * 6.091175556182861
Epoch 580, val loss: 0.7448567152023315
Epoch 590, training loss: 6.337807655334473 = 0.23246142268180847 + 1.0 * 6.105346202850342
Epoch 590, val loss: 0.7434403300285339
Epoch 600, training loss: 6.310457706451416 = 0.21895915269851685 + 1.0 * 6.091498374938965
Epoch 600, val loss: 0.7426010370254517
Epoch 610, training loss: 6.293454647064209 = 0.20606271922588348 + 1.0 * 6.0873918533325195
Epoch 610, val loss: 0.7424488067626953
Epoch 620, training loss: 6.283944129943848 = 0.19373491406440735 + 1.0 * 6.090209007263184
Epoch 620, val loss: 0.7427929639816284
Epoch 630, training loss: 6.274879455566406 = 0.18209309875965118 + 1.0 * 6.0927863121032715
Epoch 630, val loss: 0.743438720703125
Epoch 640, training loss: 6.2550835609436035 = 0.17126190662384033 + 1.0 * 6.083821773529053
Epoch 640, val loss: 0.7447003722190857
Epoch 650, training loss: 6.243074417114258 = 0.16113798320293427 + 1.0 * 6.081936359405518
Epoch 650, val loss: 0.7464894652366638
Epoch 660, training loss: 6.2341203689575195 = 0.15171025693416595 + 1.0 * 6.0824103355407715
Epoch 660, val loss: 0.7486852407455444
Epoch 670, training loss: 6.225531101226807 = 0.14297683537006378 + 1.0 * 6.082554340362549
Epoch 670, val loss: 0.7512683272361755
Epoch 680, training loss: 6.216648101806641 = 0.13490784168243408 + 1.0 * 6.081740379333496
Epoch 680, val loss: 0.7542270421981812
Epoch 690, training loss: 6.2033915519714355 = 0.1273985356092453 + 1.0 * 6.075993061065674
Epoch 690, val loss: 0.7574973702430725
Epoch 700, training loss: 6.195400238037109 = 0.1203722134232521 + 1.0 * 6.075027942657471
Epoch 700, val loss: 0.7611057758331299
Epoch 710, training loss: 6.188083171844482 = 0.11382059752941132 + 1.0 * 6.074262619018555
Epoch 710, val loss: 0.7650327086448669
Epoch 720, training loss: 6.181936740875244 = 0.10774067789316177 + 1.0 * 6.074195861816406
Epoch 720, val loss: 0.7691664695739746
Epoch 730, training loss: 6.181528568267822 = 0.10214055329561234 + 1.0 * 6.07938814163208
Epoch 730, val loss: 0.7735449075698853
Epoch 740, training loss: 6.17024040222168 = 0.09696244448423386 + 1.0 * 6.073277950286865
Epoch 740, val loss: 0.7781634330749512
Epoch 750, training loss: 6.162197589874268 = 0.09214211255311966 + 1.0 * 6.0700554847717285
Epoch 750, val loss: 0.7830307483673096
Epoch 760, training loss: 6.164982795715332 = 0.0876421108841896 + 1.0 * 6.077340602874756
Epoch 760, val loss: 0.7880013585090637
Epoch 770, training loss: 6.151678562164307 = 0.08346996456384659 + 1.0 * 6.068208694458008
Epoch 770, val loss: 0.7930793762207031
Epoch 780, training loss: 6.1465067863464355 = 0.07955766469240189 + 1.0 * 6.066948890686035
Epoch 780, val loss: 0.7983174920082092
Epoch 790, training loss: 6.150149345397949 = 0.07589668780565262 + 1.0 * 6.074252605438232
Epoch 790, val loss: 0.8036139607429504
Epoch 800, training loss: 6.141521453857422 = 0.07246369123458862 + 1.0 * 6.069057941436768
Epoch 800, val loss: 0.8089354634284973
Epoch 810, training loss: 6.133840084075928 = 0.06925660371780396 + 1.0 * 6.0645833015441895
Epoch 810, val loss: 0.8143433928489685
Epoch 820, training loss: 6.128805160522461 = 0.06623852252960205 + 1.0 * 6.062566757202148
Epoch 820, val loss: 0.8197951316833496
Epoch 830, training loss: 6.133934020996094 = 0.06339683383703232 + 1.0 * 6.070537090301514
Epoch 830, val loss: 0.8252520561218262
Epoch 840, training loss: 6.1260881423950195 = 0.06072968617081642 + 1.0 * 6.065358638763428
Epoch 840, val loss: 0.8306798338890076
Epoch 850, training loss: 6.119992733001709 = 0.05822177231311798 + 1.0 * 6.061770915985107
Epoch 850, val loss: 0.8361251354217529
Epoch 860, training loss: 6.116270065307617 = 0.055859245359897614 + 1.0 * 6.060410976409912
Epoch 860, val loss: 0.8415898680686951
Epoch 870, training loss: 6.1150031089782715 = 0.053628481924533844 + 1.0 * 6.061374664306641
Epoch 870, val loss: 0.8470042943954468
Epoch 880, training loss: 6.1154704093933105 = 0.051528386771678925 + 1.0 * 6.063941955566406
Epoch 880, val loss: 0.8524149656295776
Epoch 890, training loss: 6.107522964477539 = 0.049549542367458344 + 1.0 * 6.057973384857178
Epoch 890, val loss: 0.8577948808670044
Epoch 900, training loss: 6.103620529174805 = 0.04767546430230141 + 1.0 * 6.055944919586182
Epoch 900, val loss: 0.8632048964500427
Epoch 910, training loss: 6.106293201446533 = 0.04590263217687607 + 1.0 * 6.060390472412109
Epoch 910, val loss: 0.8686175346374512
Epoch 920, training loss: 6.099362373352051 = 0.044216930866241455 + 1.0 * 6.055145263671875
Epoch 920, val loss: 0.8738802075386047
Epoch 930, training loss: 6.096542835235596 = 0.04262769967317581 + 1.0 * 6.053915023803711
Epoch 930, val loss: 0.8791978359222412
Epoch 940, training loss: 6.097726821899414 = 0.04111224040389061 + 1.0 * 6.056614398956299
Epoch 940, val loss: 0.8844917416572571
Epoch 950, training loss: 6.09307336807251 = 0.03967304155230522 + 1.0 * 6.05340051651001
Epoch 950, val loss: 0.8897335529327393
Epoch 960, training loss: 6.0935516357421875 = 0.03831430897116661 + 1.0 * 6.055237293243408
Epoch 960, val loss: 0.8949573040008545
Epoch 970, training loss: 6.08823823928833 = 0.037024520337581635 + 1.0 * 6.05121374130249
Epoch 970, val loss: 0.9001264572143555
Epoch 980, training loss: 6.085870742797852 = 0.03579889237880707 + 1.0 * 6.050071716308594
Epoch 980, val loss: 0.9052853584289551
Epoch 990, training loss: 6.08604097366333 = 0.034630514681339264 + 1.0 * 6.051410675048828
Epoch 990, val loss: 0.9104302525520325
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.1033
Flip ASR: 0.1067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.31570053100586 = 1.9418457746505737 + 1.0 * 8.373854637145996
Epoch 0, val loss: 1.9383494853973389
Epoch 10, training loss: 10.305366516113281 = 1.9318995475769043 + 1.0 * 8.373466491699219
Epoch 10, val loss: 1.9279389381408691
Epoch 20, training loss: 10.290525436401367 = 1.919400691986084 + 1.0 * 8.371124267578125
Epoch 20, val loss: 1.9146645069122314
Epoch 30, training loss: 10.255281448364258 = 1.9018207788467407 + 1.0 * 8.353460311889648
Epoch 30, val loss: 1.8959580659866333
Epoch 40, training loss: 10.099555969238281 = 1.8785367012023926 + 1.0 * 8.22101879119873
Epoch 40, val loss: 1.8718483448028564
Epoch 50, training loss: 9.321731567382812 = 1.853206992149353 + 1.0 * 7.46852445602417
Epoch 50, val loss: 1.8464001417160034
Epoch 60, training loss: 8.984097480773926 = 1.8330894708633423 + 1.0 * 7.151007652282715
Epoch 60, val loss: 1.8277692794799805
Epoch 70, training loss: 8.709932327270508 = 1.8160629272460938 + 1.0 * 6.893869876861572
Epoch 70, val loss: 1.81168794631958
Epoch 80, training loss: 8.489045143127441 = 1.7996371984481812 + 1.0 * 6.689407825469971
Epoch 80, val loss: 1.7963780164718628
Epoch 90, training loss: 8.373164176940918 = 1.7843621969223022 + 1.0 * 6.588802337646484
Epoch 90, val loss: 1.782175064086914
Epoch 100, training loss: 8.297191619873047 = 1.768349528312683 + 1.0 * 6.528842449188232
Epoch 100, val loss: 1.767563819885254
Epoch 110, training loss: 8.23293685913086 = 1.7514606714248657 + 1.0 * 6.481475830078125
Epoch 110, val loss: 1.7525213956832886
Epoch 120, training loss: 8.17324447631836 = 1.7335362434387207 + 1.0 * 6.439708232879639
Epoch 120, val loss: 1.7368944883346558
Epoch 130, training loss: 8.116510391235352 = 1.7134031057357788 + 1.0 * 6.403107166290283
Epoch 130, val loss: 1.7196502685546875
Epoch 140, training loss: 8.062178611755371 = 1.6899775266647339 + 1.0 * 6.372201442718506
Epoch 140, val loss: 1.7001229524612427
Epoch 150, training loss: 8.008262634277344 = 1.6628060340881348 + 1.0 * 6.345457077026367
Epoch 150, val loss: 1.6777522563934326
Epoch 160, training loss: 7.953011512756348 = 1.630744218826294 + 1.0 * 6.322267055511475
Epoch 160, val loss: 1.6514803171157837
Epoch 170, training loss: 7.895065784454346 = 1.5927354097366333 + 1.0 * 6.302330493927002
Epoch 170, val loss: 1.620247721672058
Epoch 180, training loss: 7.835066795349121 = 1.549165964126587 + 1.0 * 6.285900592803955
Epoch 180, val loss: 1.584791898727417
Epoch 190, training loss: 7.77018404006958 = 1.5001535415649414 + 1.0 * 6.270030498504639
Epoch 190, val loss: 1.5452086925506592
Epoch 200, training loss: 7.702207565307617 = 1.4455230236053467 + 1.0 * 6.25668478012085
Epoch 200, val loss: 1.50148344039917
Epoch 210, training loss: 7.635011196136475 = 1.386203646659851 + 1.0 * 6.248807430267334
Epoch 210, val loss: 1.4542773962020874
Epoch 220, training loss: 7.561946392059326 = 1.3249512910842896 + 1.0 * 6.236995220184326
Epoch 220, val loss: 1.4062997102737427
Epoch 230, training loss: 7.490533828735352 = 1.2629896402359009 + 1.0 * 6.22754430770874
Epoch 230, val loss: 1.358366847038269
Epoch 240, training loss: 7.423242568969727 = 1.2019144296646118 + 1.0 * 6.221328258514404
Epoch 240, val loss: 1.3120110034942627
Epoch 250, training loss: 7.356210231781006 = 1.1439785957336426 + 1.0 * 6.212231636047363
Epoch 250, val loss: 1.268588900566101
Epoch 260, training loss: 7.292974472045898 = 1.0888153314590454 + 1.0 * 6.204159259796143
Epoch 260, val loss: 1.2274912595748901
Epoch 270, training loss: 7.238543510437012 = 1.0362989902496338 + 1.0 * 6.202244281768799
Epoch 270, val loss: 1.1885119676589966
Epoch 280, training loss: 7.180659294128418 = 0.9870862364768982 + 1.0 * 6.193572998046875
Epoch 280, val loss: 1.1522750854492188
Epoch 290, training loss: 7.12553071975708 = 0.9400976896286011 + 1.0 * 6.1854329109191895
Epoch 290, val loss: 1.1176934242248535
Epoch 300, training loss: 7.075974464416504 = 0.8946573734283447 + 1.0 * 6.18131685256958
Epoch 300, val loss: 1.0841871500015259
Epoch 310, training loss: 7.02622127532959 = 0.850585401058197 + 1.0 * 6.175635814666748
Epoch 310, val loss: 1.051890254020691
Epoch 320, training loss: 6.978992462158203 = 0.8071940541267395 + 1.0 * 6.171798229217529
Epoch 320, val loss: 1.02020263671875
Epoch 330, training loss: 6.934008598327637 = 0.7648472785949707 + 1.0 * 6.169161319732666
Epoch 330, val loss: 0.9894008040428162
Epoch 340, training loss: 6.88613748550415 = 0.7235010862350464 + 1.0 * 6.1626362800598145
Epoch 340, val loss: 0.9597649574279785
Epoch 350, training loss: 6.842464447021484 = 0.6833818554878235 + 1.0 * 6.159082412719727
Epoch 350, val loss: 0.9312980771064758
Epoch 360, training loss: 6.7997355461120605 = 0.6446427702903748 + 1.0 * 6.155092716217041
Epoch 360, val loss: 0.904464602470398
Epoch 370, training loss: 6.759112358093262 = 0.6072038412094116 + 1.0 * 6.1519083976745605
Epoch 370, val loss: 0.8791486620903015
Epoch 380, training loss: 6.7265143394470215 = 0.5718206763267517 + 1.0 * 6.154693603515625
Epoch 380, val loss: 0.8560239672660828
Epoch 390, training loss: 6.685718059539795 = 0.5386017560958862 + 1.0 * 6.147116184234619
Epoch 390, val loss: 0.8354247808456421
Epoch 400, training loss: 6.648125648498535 = 0.5070855021476746 + 1.0 * 6.141040325164795
Epoch 400, val loss: 0.8168155550956726
Epoch 410, training loss: 6.615061283111572 = 0.47696617245674133 + 1.0 * 6.138094902038574
Epoch 410, val loss: 0.8000203371047974
Epoch 420, training loss: 6.589234352111816 = 0.4484666585922241 + 1.0 * 6.140767574310303
Epoch 420, val loss: 0.7851444482803345
Epoch 430, training loss: 6.55595064163208 = 0.42179355025291443 + 1.0 * 6.134157180786133
Epoch 430, val loss: 0.77254319190979
Epoch 440, training loss: 6.526307582855225 = 0.3965267837047577 + 1.0 * 6.1297807693481445
Epoch 440, val loss: 0.7617315649986267
Epoch 450, training loss: 6.500677108764648 = 0.3725195825099945 + 1.0 * 6.128157615661621
Epoch 450, val loss: 0.752431333065033
Epoch 460, training loss: 6.483386039733887 = 0.3499321937561035 + 1.0 * 6.133453845977783
Epoch 460, val loss: 0.7444987893104553
Epoch 470, training loss: 6.451830863952637 = 0.3288581371307373 + 1.0 * 6.1229729652404785
Epoch 470, val loss: 0.7382441759109497
Epoch 480, training loss: 6.430593490600586 = 0.3089439570903778 + 1.0 * 6.121649742126465
Epoch 480, val loss: 0.7330769300460815
Epoch 490, training loss: 6.408471584320068 = 0.2902359366416931 + 1.0 * 6.1182355880737305
Epoch 490, val loss: 0.7288545966148376
Epoch 500, training loss: 6.38900899887085 = 0.27261295914649963 + 1.0 * 6.116395950317383
Epoch 500, val loss: 0.7255414724349976
Epoch 510, training loss: 6.369588851928711 = 0.2561442255973816 + 1.0 * 6.113444805145264
Epoch 510, val loss: 0.7229034304618835
Epoch 520, training loss: 6.351577281951904 = 0.24081622064113617 + 1.0 * 6.1107611656188965
Epoch 520, val loss: 0.7212256789207458
Epoch 530, training loss: 6.334883213043213 = 0.22645904123783112 + 1.0 * 6.108424186706543
Epoch 530, val loss: 0.7202380895614624
Epoch 540, training loss: 6.321059703826904 = 0.2129928320646286 + 1.0 * 6.108067035675049
Epoch 540, val loss: 0.7198255062103271
Epoch 550, training loss: 6.3123555183410645 = 0.20048747956752777 + 1.0 * 6.111867904663086
Epoch 550, val loss: 0.7198927998542786
Epoch 560, training loss: 6.294297218322754 = 0.18892088532447815 + 1.0 * 6.105376243591309
Epoch 560, val loss: 0.7207552194595337
Epoch 570, training loss: 6.282548427581787 = 0.1781216412782669 + 1.0 * 6.104426860809326
Epoch 570, val loss: 0.722062349319458
Epoch 580, training loss: 6.26960563659668 = 0.16808095574378967 + 1.0 * 6.101524829864502
Epoch 580, val loss: 0.72361820936203
Epoch 590, training loss: 6.25781774520874 = 0.1587601900100708 + 1.0 * 6.099057674407959
Epoch 590, val loss: 0.7259328365325928
Epoch 600, training loss: 6.246804714202881 = 0.1500319242477417 + 1.0 * 6.09677267074585
Epoch 600, val loss: 0.7285094261169434
Epoch 610, training loss: 6.238593578338623 = 0.14192943274974823 + 1.0 * 6.096663951873779
Epoch 610, val loss: 0.7313447594642639
Epoch 620, training loss: 6.227181911468506 = 0.13439980149269104 + 1.0 * 6.092782020568848
Epoch 620, val loss: 0.734619677066803
Epoch 630, training loss: 6.219079971313477 = 0.12735766172409058 + 1.0 * 6.09172248840332
Epoch 630, val loss: 0.7382058501243591
Epoch 640, training loss: 6.211461544036865 = 0.12074405699968338 + 1.0 * 6.090717315673828
Epoch 640, val loss: 0.7419794201850891
Epoch 650, training loss: 6.207540512084961 = 0.11457483470439911 + 1.0 * 6.092965602874756
Epoch 650, val loss: 0.7457851767539978
Epoch 660, training loss: 6.198249340057373 = 0.10885043442249298 + 1.0 * 6.0893988609313965
Epoch 660, val loss: 0.7500430345535278
Epoch 670, training loss: 6.189937591552734 = 0.10347749292850494 + 1.0 * 6.086460113525391
Epoch 670, val loss: 0.7544565200805664
Epoch 680, training loss: 6.183417797088623 = 0.09841473400592804 + 1.0 * 6.085002899169922
Epoch 680, val loss: 0.7588788270950317
Epoch 690, training loss: 6.179748058319092 = 0.09367291629314423 + 1.0 * 6.086075305938721
Epoch 690, val loss: 0.7632673978805542
Epoch 700, training loss: 6.172261714935303 = 0.08924616873264313 + 1.0 * 6.083015441894531
Epoch 700, val loss: 0.767823338508606
Epoch 710, training loss: 6.16584587097168 = 0.08509223163127899 + 1.0 * 6.080753803253174
Epoch 710, val loss: 0.7726196646690369
Epoch 720, training loss: 6.160569190979004 = 0.08116547763347626 + 1.0 * 6.079403877258301
Epoch 720, val loss: 0.7773388028144836
Epoch 730, training loss: 6.170098781585693 = 0.07745387405157089 + 1.0 * 6.092644691467285
Epoch 730, val loss: 0.7820137739181519
Epoch 740, training loss: 6.1537675857543945 = 0.07398609071969986 + 1.0 * 6.079781532287598
Epoch 740, val loss: 0.7868266105651855
Epoch 750, training loss: 6.146881103515625 = 0.07070805132389069 + 1.0 * 6.076172828674316
Epoch 750, val loss: 0.7917196750640869
Epoch 760, training loss: 6.142704010009766 = 0.06760598719120026 + 1.0 * 6.075098037719727
Epoch 760, val loss: 0.796527087688446
Epoch 770, training loss: 6.158403396606445 = 0.0646735355257988 + 1.0 * 6.0937299728393555
Epoch 770, val loss: 0.8012666702270508
Epoch 780, training loss: 6.137664794921875 = 0.06193414703011513 + 1.0 * 6.075730800628662
Epoch 780, val loss: 0.8060040473937988
Epoch 790, training loss: 6.132083415985107 = 0.05935314670205116 + 1.0 * 6.07273006439209
Epoch 790, val loss: 0.8109187483787537
Epoch 800, training loss: 6.127935409545898 = 0.05690581351518631 + 1.0 * 6.0710296630859375
Epoch 800, val loss: 0.8156929016113281
Epoch 810, training loss: 6.130275249481201 = 0.05458737164735794 + 1.0 * 6.075687885284424
Epoch 810, val loss: 0.8203778862953186
Epoch 820, training loss: 6.122486591339111 = 0.0524028018116951 + 1.0 * 6.0700836181640625
Epoch 820, val loss: 0.8251762390136719
Epoch 830, training loss: 6.118610858917236 = 0.05032433941960335 + 1.0 * 6.068286418914795
Epoch 830, val loss: 0.8299784064292908
Epoch 840, training loss: 6.1157941818237305 = 0.048350054770708084 + 1.0 * 6.067444324493408
Epoch 840, val loss: 0.8347041010856628
Epoch 850, training loss: 6.113598823547363 = 0.04648268222808838 + 1.0 * 6.0671162605285645
Epoch 850, val loss: 0.8392485976219177
Epoch 860, training loss: 6.111505031585693 = 0.04473060742020607 + 1.0 * 6.066774368286133
Epoch 860, val loss: 0.8439224362373352
Epoch 870, training loss: 6.108248710632324 = 0.04306724667549133 + 1.0 * 6.065181255340576
Epoch 870, val loss: 0.8486268520355225
Epoch 880, training loss: 6.104644775390625 = 0.041480716317892075 + 1.0 * 6.063164234161377
Epoch 880, val loss: 0.8532067537307739
Epoch 890, training loss: 6.114507675170898 = 0.03997102752327919 + 1.0 * 6.0745368003845215
Epoch 890, val loss: 0.8577659726142883
Epoch 900, training loss: 6.101006507873535 = 0.038539666682481766 + 1.0 * 6.062466621398926
Epoch 900, val loss: 0.862200140953064
Epoch 910, training loss: 6.099432945251465 = 0.03718193992972374 + 1.0 * 6.062251091003418
Epoch 910, val loss: 0.8667804598808289
Epoch 920, training loss: 6.102772235870361 = 0.03588853031396866 + 1.0 * 6.066883563995361
Epoch 920, val loss: 0.8711711764335632
Epoch 930, training loss: 6.093842506408691 = 0.0346631221473217 + 1.0 * 6.059179306030273
Epoch 930, val loss: 0.8755928874015808
Epoch 940, training loss: 6.091599941253662 = 0.033493783324956894 + 1.0 * 6.058105945587158
Epoch 940, val loss: 0.8800046443939209
Epoch 950, training loss: 6.091956615447998 = 0.03237679973244667 + 1.0 * 6.059579849243164
Epoch 950, val loss: 0.8843653798103333
Epoch 960, training loss: 6.088916778564453 = 0.03131140395998955 + 1.0 * 6.057605266571045
Epoch 960, val loss: 0.8885996341705322
Epoch 970, training loss: 6.092984676361084 = 0.030299333855509758 + 1.0 * 6.062685489654541
Epoch 970, val loss: 0.8928703665733337
Epoch 980, training loss: 6.086625576019287 = 0.029333917424082756 + 1.0 * 6.057291507720947
Epoch 980, val loss: 0.8970248699188232
Epoch 990, training loss: 6.0834503173828125 = 0.028414269909262657 + 1.0 * 6.0550360679626465
Epoch 990, val loss: 0.9012974500656128
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7934
Flip ASR: 0.7556/225 nodes
The final ASR:0.51292, 0.29614, Accuracy:0.81481, 0.00907
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11576])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10536])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83704, 0.01090
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.32007884979248 = 1.9461606740951538 + 1.0 * 8.373918533325195
Epoch 0, val loss: 1.9444756507873535
Epoch 10, training loss: 10.30942153930664 = 1.935715913772583 + 1.0 * 8.373705863952637
Epoch 10, val loss: 1.9348376989364624
Epoch 20, training loss: 10.295232772827148 = 1.9230715036392212 + 1.0 * 8.372160911560059
Epoch 20, val loss: 1.9228004217147827
Epoch 30, training loss: 10.265070915222168 = 1.9054840803146362 + 1.0 * 8.359586715698242
Epoch 30, val loss: 1.9058587551116943
Epoch 40, training loss: 10.168044090270996 = 1.8817955255508423 + 1.0 * 8.286248207092285
Epoch 40, val loss: 1.8839205503463745
Epoch 50, training loss: 9.816783905029297 = 1.8572818040847778 + 1.0 * 7.959502220153809
Epoch 50, val loss: 1.862343668937683
Epoch 60, training loss: 9.468961715698242 = 1.8345305919647217 + 1.0 * 7.634430885314941
Epoch 60, val loss: 1.8431178331375122
Epoch 70, training loss: 8.977169036865234 = 1.8163334131240845 + 1.0 * 7.1608357429504395
Epoch 70, val loss: 1.8273600339889526
Epoch 80, training loss: 8.670269012451172 = 1.8000450134277344 + 1.0 * 6.8702239990234375
Epoch 80, val loss: 1.8127398490905762
Epoch 90, training loss: 8.498865127563477 = 1.7810925245285034 + 1.0 * 6.717772483825684
Epoch 90, val loss: 1.7960224151611328
Epoch 100, training loss: 8.366945266723633 = 1.761210560798645 + 1.0 * 6.605734348297119
Epoch 100, val loss: 1.779212236404419
Epoch 110, training loss: 8.272746086120605 = 1.7412289381027222 + 1.0 * 6.531517505645752
Epoch 110, val loss: 1.7621862888336182
Epoch 120, training loss: 8.201501846313477 = 1.7195500135421753 + 1.0 * 6.48195219039917
Epoch 120, val loss: 1.7433233261108398
Epoch 130, training loss: 8.13593578338623 = 1.6952649354934692 + 1.0 * 6.440670490264893
Epoch 130, val loss: 1.7221561670303345
Epoch 140, training loss: 8.075427055358887 = 1.6676199436187744 + 1.0 * 6.407806873321533
Epoch 140, val loss: 1.698512077331543
Epoch 150, training loss: 8.019340515136719 = 1.6357308626174927 + 1.0 * 6.383609294891357
Epoch 150, val loss: 1.67159903049469
Epoch 160, training loss: 7.957432746887207 = 1.5993624925613403 + 1.0 * 6.358070373535156
Epoch 160, val loss: 1.6410179138183594
Epoch 170, training loss: 7.89499568939209 = 1.5577497482299805 + 1.0 * 6.337245941162109
Epoch 170, val loss: 1.6061525344848633
Epoch 180, training loss: 7.8314642906188965 = 1.5107239484786987 + 1.0 * 6.320740222930908
Epoch 180, val loss: 1.566933512687683
Epoch 190, training loss: 7.765422821044922 = 1.4591801166534424 + 1.0 * 6.306242942810059
Epoch 190, val loss: 1.5241827964782715
Epoch 200, training loss: 7.697544574737549 = 1.403847336769104 + 1.0 * 6.293697357177734
Epoch 200, val loss: 1.4787474870681763
Epoch 210, training loss: 7.628225326538086 = 1.3462529182434082 + 1.0 * 6.281972408294678
Epoch 210, val loss: 1.432147741317749
Epoch 220, training loss: 7.5639214515686035 = 1.2879486083984375 + 1.0 * 6.275972843170166
Epoch 220, val loss: 1.3855078220367432
Epoch 230, training loss: 7.492101192474365 = 1.231116771697998 + 1.0 * 6.260984420776367
Epoch 230, val loss: 1.3411113023757935
Epoch 240, training loss: 7.428117752075195 = 1.176414132118225 + 1.0 * 6.25170373916626
Epoch 240, val loss: 1.298924207687378
Epoch 250, training loss: 7.368016242980957 = 1.1244453191757202 + 1.0 * 6.243570804595947
Epoch 250, val loss: 1.2594401836395264
Epoch 260, training loss: 7.313730239868164 = 1.0763453245162964 + 1.0 * 6.237384796142578
Epoch 260, val loss: 1.2237107753753662
Epoch 270, training loss: 7.261844635009766 = 1.03262197971344 + 1.0 * 6.229222774505615
Epoch 270, val loss: 1.191436767578125
Epoch 280, training loss: 7.212858200073242 = 0.9922794103622437 + 1.0 * 6.220578670501709
Epoch 280, val loss: 1.1622438430786133
Epoch 290, training loss: 7.168383598327637 = 0.954430341720581 + 1.0 * 6.213953018188477
Epoch 290, val loss: 1.135111927986145
Epoch 300, training loss: 7.128354549407959 = 0.9185196161270142 + 1.0 * 6.209835052490234
Epoch 300, val loss: 1.1095027923583984
Epoch 310, training loss: 7.088008880615234 = 0.8841211795806885 + 1.0 * 6.203887939453125
Epoch 310, val loss: 1.085368037223816
Epoch 320, training loss: 7.046612739562988 = 0.8502931594848633 + 1.0 * 6.196319580078125
Epoch 320, val loss: 1.0618306398391724
Epoch 330, training loss: 7.00773811340332 = 0.8166196346282959 + 1.0 * 6.1911187171936035
Epoch 330, val loss: 1.0385264158248901
Epoch 340, training loss: 6.970237731933594 = 0.7830516695976257 + 1.0 * 6.187186241149902
Epoch 340, val loss: 1.015533447265625
Epoch 350, training loss: 6.93268346786499 = 0.7497791051864624 + 1.0 * 6.182904243469238
Epoch 350, val loss: 0.9929695129394531
Epoch 360, training loss: 6.894891738891602 = 0.7166085839271545 + 1.0 * 6.178283214569092
Epoch 360, val loss: 0.9706916213035583
Epoch 370, training loss: 6.86129903793335 = 0.683577835559845 + 1.0 * 6.17772102355957
Epoch 370, val loss: 0.9486314654350281
Epoch 380, training loss: 6.823330879211426 = 0.6508830189704895 + 1.0 * 6.172447681427002
Epoch 380, val loss: 0.9270725250244141
Epoch 390, training loss: 6.785747528076172 = 0.618683397769928 + 1.0 * 6.167064189910889
Epoch 390, val loss: 0.9060336351394653
Epoch 400, training loss: 6.750309467315674 = 0.586944043636322 + 1.0 * 6.163365364074707
Epoch 400, val loss: 0.8856134414672852
Epoch 410, training loss: 6.724393844604492 = 0.5558180212974548 + 1.0 * 6.168575763702393
Epoch 410, val loss: 0.8660199046134949
Epoch 420, training loss: 6.6832051277160645 = 0.5255578756332397 + 1.0 * 6.157647132873535
Epoch 420, val loss: 0.8475229740142822
Epoch 430, training loss: 6.650028705596924 = 0.49616876244544983 + 1.0 * 6.153860092163086
Epoch 430, val loss: 0.8301076889038086
Epoch 440, training loss: 6.626668453216553 = 0.46766427159309387 + 1.0 * 6.159004211425781
Epoch 440, val loss: 0.8138288855552673
Epoch 450, training loss: 6.59013557434082 = 0.4403941035270691 + 1.0 * 6.1497416496276855
Epoch 450, val loss: 0.798738420009613
Epoch 460, training loss: 6.559911251068115 = 0.41423606872558594 + 1.0 * 6.145675182342529
Epoch 460, val loss: 0.7849385738372803
Epoch 470, training loss: 6.5358405113220215 = 0.38918963074684143 + 1.0 * 6.146650791168213
Epoch 470, val loss: 0.7723066210746765
Epoch 480, training loss: 6.51102352142334 = 0.3654409646987915 + 1.0 * 6.145582675933838
Epoch 480, val loss: 0.760884165763855
Epoch 490, training loss: 6.488948822021484 = 0.3429524302482605 + 1.0 * 6.145996570587158
Epoch 490, val loss: 0.7507708668708801
Epoch 500, training loss: 6.459377765655518 = 0.3216433823108673 + 1.0 * 6.137734413146973
Epoch 500, val loss: 0.7418297529220581
Epoch 510, training loss: 6.436021327972412 = 0.3014753460884094 + 1.0 * 6.134545803070068
Epoch 510, val loss: 0.7340164184570312
Epoch 520, training loss: 6.423537731170654 = 0.28231504559516907 + 1.0 * 6.1412224769592285
Epoch 520, val loss: 0.7273004055023193
Epoch 530, training loss: 6.394440174102783 = 0.26426300406455994 + 1.0 * 6.130177021026611
Epoch 530, val loss: 0.7216494083404541
Epoch 540, training loss: 6.375521659851074 = 0.24712112545967102 + 1.0 * 6.1284003257751465
Epoch 540, val loss: 0.7170200943946838
Epoch 550, training loss: 6.362181186676025 = 0.23083865642547607 + 1.0 * 6.13134241104126
Epoch 550, val loss: 0.7132489085197449
Epoch 560, training loss: 6.343343734741211 = 0.21541665494441986 + 1.0 * 6.127927303314209
Epoch 560, val loss: 0.710318386554718
Epoch 570, training loss: 6.3228864669799805 = 0.20089615881443024 + 1.0 * 6.121990203857422
Epoch 570, val loss: 0.7083067297935486
Epoch 580, training loss: 6.308028697967529 = 0.18716759979724884 + 1.0 * 6.120861053466797
Epoch 580, val loss: 0.7069446444511414
Epoch 590, training loss: 6.294255256652832 = 0.17423945665359497 + 1.0 * 6.120015621185303
Epoch 590, val loss: 0.70628821849823
Epoch 600, training loss: 6.280258655548096 = 0.16220563650131226 + 1.0 * 6.118052959442139
Epoch 600, val loss: 0.7064369320869446
Epoch 610, training loss: 6.266592025756836 = 0.15096250176429749 + 1.0 * 6.11562967300415
Epoch 610, val loss: 0.7072253227233887
Epoch 620, training loss: 6.265390396118164 = 0.14051231741905212 + 1.0 * 6.1248779296875
Epoch 620, val loss: 0.7085620164871216
Epoch 630, training loss: 6.243698596954346 = 0.13082639873027802 + 1.0 * 6.112872123718262
Epoch 630, val loss: 0.7105347514152527
Epoch 640, training loss: 6.231935024261475 = 0.12189202755689621 + 1.0 * 6.110043048858643
Epoch 640, val loss: 0.7131165266036987
Epoch 650, training loss: 6.2242560386657715 = 0.11362916976213455 + 1.0 * 6.110626697540283
Epoch 650, val loss: 0.7161409854888916
Epoch 660, training loss: 6.219951629638672 = 0.10603306442499161 + 1.0 * 6.113918781280518
Epoch 660, val loss: 0.7195254564285278
Epoch 670, training loss: 6.205759525299072 = 0.09909412264823914 + 1.0 * 6.10666561126709
Epoch 670, val loss: 0.723499059677124
Epoch 680, training loss: 6.196107387542725 = 0.09272120893001556 + 1.0 * 6.103386402130127
Epoch 680, val loss: 0.7277441620826721
Epoch 690, training loss: 6.194531440734863 = 0.08685650676488876 + 1.0 * 6.107675075531006
Epoch 690, val loss: 0.7323437929153442
Epoch 700, training loss: 6.185224533081055 = 0.08151581138372421 + 1.0 * 6.103708744049072
Epoch 700, val loss: 0.7371248602867126
Epoch 710, training loss: 6.175817012786865 = 0.07660456001758575 + 1.0 * 6.099212646484375
Epoch 710, val loss: 0.7422387599945068
Epoch 720, training loss: 6.173211574554443 = 0.07209434360265732 + 1.0 * 6.101117134094238
Epoch 720, val loss: 0.7474650740623474
Epoch 730, training loss: 6.171338081359863 = 0.06796619296073914 + 1.0 * 6.103372097015381
Epoch 730, val loss: 0.752784252166748
Epoch 740, training loss: 6.1599955558776855 = 0.06416576355695724 + 1.0 * 6.095829963684082
Epoch 740, val loss: 0.7583170533180237
Epoch 750, training loss: 6.154109001159668 = 0.06066081300377846 + 1.0 * 6.093448162078857
Epoch 750, val loss: 0.7639188170433044
Epoch 760, training loss: 6.149977207183838 = 0.05741306394338608 + 1.0 * 6.092564105987549
Epoch 760, val loss: 0.7695927023887634
Epoch 770, training loss: 6.148576259613037 = 0.05441397801041603 + 1.0 * 6.094162464141846
Epoch 770, val loss: 0.7752700448036194
Epoch 780, training loss: 6.145340442657471 = 0.05165552347898483 + 1.0 * 6.093685150146484
Epoch 780, val loss: 0.7809630632400513
Epoch 790, training loss: 6.138606071472168 = 0.04910022020339966 + 1.0 * 6.089505672454834
Epoch 790, val loss: 0.7867066264152527
Epoch 800, training loss: 6.1340508460998535 = 0.04672098159790039 + 1.0 * 6.087329864501953
Epoch 800, val loss: 0.7924837470054626
Epoch 810, training loss: 6.134446144104004 = 0.04449911043047905 + 1.0 * 6.08994722366333
Epoch 810, val loss: 0.7982791066169739
Epoch 820, training loss: 6.130651950836182 = 0.04243065044283867 + 1.0 * 6.088221073150635
Epoch 820, val loss: 0.803924024105072
Epoch 830, training loss: 6.127357006072998 = 0.04050881415605545 + 1.0 * 6.086848258972168
Epoch 830, val loss: 0.8096073269844055
Epoch 840, training loss: 6.121786117553711 = 0.038715146481990814 + 1.0 * 6.083070755004883
Epoch 840, val loss: 0.8152660131454468
Epoch 850, training loss: 6.121616363525391 = 0.037031129002571106 + 1.0 * 6.084585189819336
Epoch 850, val loss: 0.820898175239563
Epoch 860, training loss: 6.118675231933594 = 0.03546367585659027 + 1.0 * 6.083211421966553
Epoch 860, val loss: 0.8263322710990906
Epoch 870, training loss: 6.118574142456055 = 0.033998049795627594 + 1.0 * 6.08457612991333
Epoch 870, val loss: 0.8318685293197632
Epoch 880, training loss: 6.112195014953613 = 0.03262999653816223 + 1.0 * 6.079565048217773
Epoch 880, val loss: 0.8372722864151001
Epoch 890, training loss: 6.1110076904296875 = 0.03133854269981384 + 1.0 * 6.079668998718262
Epoch 890, val loss: 0.8425801396369934
Epoch 900, training loss: 6.111762523651123 = 0.030121950432658195 + 1.0 * 6.081640720367432
Epoch 900, val loss: 0.84787917137146
Epoch 910, training loss: 6.107769012451172 = 0.0289778970181942 + 1.0 * 6.07879114151001
Epoch 910, val loss: 0.8531624674797058
Epoch 920, training loss: 6.114001750946045 = 0.027899714186787605 + 1.0 * 6.08610200881958
Epoch 920, val loss: 0.8583221435546875
Epoch 930, training loss: 6.103318214416504 = 0.02687895856797695 + 1.0 * 6.076439380645752
Epoch 930, val loss: 0.8633544445037842
Epoch 940, training loss: 6.099903583526611 = 0.025920534506440163 + 1.0 * 6.073983192443848
Epoch 940, val loss: 0.868453860282898
Epoch 950, training loss: 6.098299026489258 = 0.02500973828136921 + 1.0 * 6.073289394378662
Epoch 950, val loss: 0.873464047908783
Epoch 960, training loss: 6.104273319244385 = 0.02414707839488983 + 1.0 * 6.0801262855529785
Epoch 960, val loss: 0.8783864378929138
Epoch 970, training loss: 6.102912425994873 = 0.023327328264713287 + 1.0 * 6.079585075378418
Epoch 970, val loss: 0.8831896781921387
Epoch 980, training loss: 6.093510150909424 = 0.022559408098459244 + 1.0 * 6.070950508117676
Epoch 980, val loss: 0.8880295157432556
Epoch 990, training loss: 6.093808650970459 = 0.02182910405099392 + 1.0 * 6.071979522705078
Epoch 990, val loss: 0.8927549719810486
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.5609
Flip ASR: 0.4844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32164478302002 = 1.9477382898330688 + 1.0 * 8.373906135559082
Epoch 0, val loss: 1.9524798393249512
Epoch 10, training loss: 10.310595512390137 = 1.9369966983795166 + 1.0 * 8.3735990524292
Epoch 10, val loss: 1.9412012100219727
Epoch 20, training loss: 10.294878005981445 = 1.923277497291565 + 1.0 * 8.371600151062012
Epoch 20, val loss: 1.9263865947723389
Epoch 30, training loss: 10.261427879333496 = 1.903747320175171 + 1.0 * 8.357680320739746
Epoch 30, val loss: 1.9048749208450317
Epoch 40, training loss: 10.150079727172852 = 1.8778908252716064 + 1.0 * 8.272189140319824
Epoch 40, val loss: 1.8773988485336304
Epoch 50, training loss: 9.770809173583984 = 1.8508291244506836 + 1.0 * 7.919980049133301
Epoch 50, val loss: 1.850447654724121
Epoch 60, training loss: 9.380196571350098 = 1.828701138496399 + 1.0 * 7.551495552062988
Epoch 60, val loss: 1.8293367624282837
Epoch 70, training loss: 8.879561424255371 = 1.8097692728042603 + 1.0 * 7.0697922706604
Epoch 70, val loss: 1.8111547231674194
Epoch 80, training loss: 8.592451095581055 = 1.7931439876556396 + 1.0 * 6.799307346343994
Epoch 80, val loss: 1.7944883108139038
Epoch 90, training loss: 8.4520845413208 = 1.7746526002883911 + 1.0 * 6.677432060241699
Epoch 90, val loss: 1.7758976221084595
Epoch 100, training loss: 8.345170021057129 = 1.7547886371612549 + 1.0 * 6.590381622314453
Epoch 100, val loss: 1.7565933465957642
Epoch 110, training loss: 8.250321388244629 = 1.7349767684936523 + 1.0 * 6.515344619750977
Epoch 110, val loss: 1.7381144762039185
Epoch 120, training loss: 8.171073913574219 = 1.7147966623306274 + 1.0 * 6.456276893615723
Epoch 120, val loss: 1.7197850942611694
Epoch 130, training loss: 8.103277206420898 = 1.6922000646591187 + 1.0 * 6.41107702255249
Epoch 130, val loss: 1.6999917030334473
Epoch 140, training loss: 8.044747352600098 = 1.6657572984695435 + 1.0 * 6.378990173339844
Epoch 140, val loss: 1.6772825717926025
Epoch 150, training loss: 7.990158557891846 = 1.634560227394104 + 1.0 * 6.355598449707031
Epoch 150, val loss: 1.650714635848999
Epoch 160, training loss: 7.932309150695801 = 1.598474144935608 + 1.0 * 6.333835124969482
Epoch 160, val loss: 1.6202495098114014
Epoch 170, training loss: 7.872231483459473 = 1.5571043491363525 + 1.0 * 6.315127372741699
Epoch 170, val loss: 1.5855768918991089
Epoch 180, training loss: 7.808920860290527 = 1.509735107421875 + 1.0 * 6.299185752868652
Epoch 180, val loss: 1.5463203191757202
Epoch 190, training loss: 7.743393421173096 = 1.4571434259414673 + 1.0 * 6.286250114440918
Epoch 190, val loss: 1.5030852556228638
Epoch 200, training loss: 7.673530578613281 = 1.4007847309112549 + 1.0 * 6.2727460861206055
Epoch 200, val loss: 1.4573309421539307
Epoch 210, training loss: 7.602898597717285 = 1.341246247291565 + 1.0 * 6.26165246963501
Epoch 210, val loss: 1.409516453742981
Epoch 220, training loss: 7.534886837005615 = 1.279884696006775 + 1.0 * 6.255002021789551
Epoch 220, val loss: 1.3608765602111816
Epoch 230, training loss: 7.4617600440979 = 1.2191085815429688 + 1.0 * 6.242651462554932
Epoch 230, val loss: 1.3136863708496094
Epoch 240, training loss: 7.394668102264404 = 1.1599148511886597 + 1.0 * 6.234753131866455
Epoch 240, val loss: 1.2682732343673706
Epoch 250, training loss: 7.330073833465576 = 1.102989673614502 + 1.0 * 6.227084159851074
Epoch 250, val loss: 1.2251098155975342
Epoch 260, training loss: 7.2777605056762695 = 1.0496137142181396 + 1.0 * 6.228147029876709
Epoch 260, val loss: 1.1850662231445312
Epoch 270, training loss: 7.21555233001709 = 1.0003597736358643 + 1.0 * 6.2151923179626465
Epoch 270, val loss: 1.1485358476638794
Epoch 280, training loss: 7.162054061889648 = 0.9541676044464111 + 1.0 * 6.207886219024658
Epoch 280, val loss: 1.114575743675232
Epoch 290, training loss: 7.11269474029541 = 0.9100373387336731 + 1.0 * 6.202657222747803
Epoch 290, val loss: 1.0824769735336304
Epoch 300, training loss: 7.072633743286133 = 0.8680189251899719 + 1.0 * 6.204614639282227
Epoch 300, val loss: 1.0519315004348755
Epoch 310, training loss: 7.021068096160889 = 0.8274348974227905 + 1.0 * 6.193633079528809
Epoch 310, val loss: 1.0229557752609253
Epoch 320, training loss: 6.976016521453857 = 0.7875828742980957 + 1.0 * 6.188433647155762
Epoch 320, val loss: 0.9945365786552429
Epoch 330, training loss: 6.932088375091553 = 0.7480630874633789 + 1.0 * 6.184025287628174
Epoch 330, val loss: 0.9662684798240662
Epoch 340, training loss: 6.890322208404541 = 0.7091327905654907 + 1.0 * 6.18118953704834
Epoch 340, val loss: 0.9384550452232361
Epoch 350, training loss: 6.848467826843262 = 0.6714112758636475 + 1.0 * 6.177056789398193
Epoch 350, val loss: 0.9116014838218689
Epoch 360, training loss: 6.808107376098633 = 0.6350599527359009 + 1.0 * 6.1730475425720215
Epoch 360, val loss: 0.8857317566871643
Epoch 370, training loss: 6.772560119628906 = 0.6003863215446472 + 1.0 * 6.172173976898193
Epoch 370, val loss: 0.8611533641815186
Epoch 380, training loss: 6.73435640335083 = 0.5677323937416077 + 1.0 * 6.166624069213867
Epoch 380, val loss: 0.838355302810669
Epoch 390, training loss: 6.700370788574219 = 0.5371927618980408 + 1.0 * 6.163177967071533
Epoch 390, val loss: 0.8174641728401184
Epoch 400, training loss: 6.674066066741943 = 0.5085870027542114 + 1.0 * 6.1654791831970215
Epoch 400, val loss: 0.7986843585968018
Epoch 410, training loss: 6.642088413238525 = 0.4821052551269531 + 1.0 * 6.159983158111572
Epoch 410, val loss: 0.7818186283111572
Epoch 420, training loss: 6.612111568450928 = 0.45713698863983154 + 1.0 * 6.154974460601807
Epoch 420, val loss: 0.7668572664260864
Epoch 430, training loss: 6.584659576416016 = 0.4334075450897217 + 1.0 * 6.151252269744873
Epoch 430, val loss: 0.7534769177436829
Epoch 440, training loss: 6.563251972198486 = 0.41083985567092896 + 1.0 * 6.152411937713623
Epoch 440, val loss: 0.741459846496582
Epoch 450, training loss: 6.536975860595703 = 0.3894588351249695 + 1.0 * 6.147517204284668
Epoch 450, val loss: 0.7309240698814392
Epoch 460, training loss: 6.515023231506348 = 0.36892110109329224 + 1.0 * 6.146101951599121
Epoch 460, val loss: 0.7216851711273193
Epoch 470, training loss: 6.49390172958374 = 0.34922274947166443 + 1.0 * 6.144679069519043
Epoch 470, val loss: 0.7135723829269409
Epoch 480, training loss: 6.469405651092529 = 0.3302701711654663 + 1.0 * 6.139135360717773
Epoch 480, val loss: 0.7065731883049011
Epoch 490, training loss: 6.448089599609375 = 0.31192436814308167 + 1.0 * 6.136165142059326
Epoch 490, val loss: 0.7005118727684021
Epoch 500, training loss: 6.432893753051758 = 0.2941608428955078 + 1.0 * 6.13873291015625
Epoch 500, val loss: 0.6953477263450623
Epoch 510, training loss: 6.413668632507324 = 0.2770974636077881 + 1.0 * 6.136571407318115
Epoch 510, val loss: 0.6909520626068115
Epoch 520, training loss: 6.391766548156738 = 0.260823518037796 + 1.0 * 6.1309428215026855
Epoch 520, val loss: 0.6875401735305786
Epoch 530, training loss: 6.374395847320557 = 0.24516242742538452 + 1.0 * 6.129233360290527
Epoch 530, val loss: 0.6849200129508972
Epoch 540, training loss: 6.3564839363098145 = 0.23012663424015045 + 1.0 * 6.126357078552246
Epoch 540, val loss: 0.6830266714096069
Epoch 550, training loss: 6.340332508087158 = 0.21576641499996185 + 1.0 * 6.124566078186035
Epoch 550, val loss: 0.6819210052490234
Epoch 560, training loss: 6.3326873779296875 = 0.20223559439182281 + 1.0 * 6.130451679229736
Epoch 560, val loss: 0.6815094351768494
Epoch 570, training loss: 6.3154144287109375 = 0.18968062102794647 + 1.0 * 6.125733852386475
Epoch 570, val loss: 0.6820024847984314
Epoch 580, training loss: 6.297708511352539 = 0.1779475212097168 + 1.0 * 6.119760990142822
Epoch 580, val loss: 0.6832804679870605
Epoch 590, training loss: 6.285055160522461 = 0.16698861122131348 + 1.0 * 6.118066787719727
Epoch 590, val loss: 0.685198187828064
Epoch 600, training loss: 6.283973693847656 = 0.15675915777683258 + 1.0 * 6.127214431762695
Epoch 600, val loss: 0.687744677066803
Epoch 610, training loss: 6.263006210327148 = 0.14742866158485413 + 1.0 * 6.115577697753906
Epoch 610, val loss: 0.6907519698143005
Epoch 620, training loss: 6.252381801605225 = 0.1388023942708969 + 1.0 * 6.113579273223877
Epoch 620, val loss: 0.6944106221199036
Epoch 630, training loss: 6.242436408996582 = 0.13084037601947784 + 1.0 * 6.11159610748291
Epoch 630, val loss: 0.6984819173812866
Epoch 640, training loss: 6.233163833618164 = 0.12346283346414566 + 1.0 * 6.109701156616211
Epoch 640, val loss: 0.7029083371162415
Epoch 650, training loss: 6.230526447296143 = 0.11663076281547546 + 1.0 * 6.113895893096924
Epoch 650, val loss: 0.7076341509819031
Epoch 660, training loss: 6.2206244468688965 = 0.1103455126285553 + 1.0 * 6.110279083251953
Epoch 660, val loss: 0.7126103639602661
Epoch 670, training loss: 6.212156295776367 = 0.10453558713197708 + 1.0 * 6.107620716094971
Epoch 670, val loss: 0.7178375720977783
Epoch 680, training loss: 6.204401016235352 = 0.099156953394413 + 1.0 * 6.105244159698486
Epoch 680, val loss: 0.7231882810592651
Epoch 690, training loss: 6.197690010070801 = 0.09416796267032623 + 1.0 * 6.103521823883057
Epoch 690, val loss: 0.7286848425865173
Epoch 700, training loss: 6.1973161697387695 = 0.08953675627708435 + 1.0 * 6.107779502868652
Epoch 700, val loss: 0.7342512011528015
Epoch 710, training loss: 6.186764240264893 = 0.08520760387182236 + 1.0 * 6.101556777954102
Epoch 710, val loss: 0.739840567111969
Epoch 720, training loss: 6.1800150871276855 = 0.08118841052055359 + 1.0 * 6.098826885223389
Epoch 720, val loss: 0.7455645203590393
Epoch 730, training loss: 6.189357280731201 = 0.07742749154567719 + 1.0 * 6.111929893493652
Epoch 730, val loss: 0.7512257695198059
Epoch 740, training loss: 6.171358108520508 = 0.0739201158285141 + 1.0 * 6.097437858581543
Epoch 740, val loss: 0.7568956017494202
Epoch 750, training loss: 6.165563583374023 = 0.07063125818967819 + 1.0 * 6.094932556152344
Epoch 750, val loss: 0.7626655697822571
Epoch 760, training loss: 6.161303520202637 = 0.06753740459680557 + 1.0 * 6.093766212463379
Epoch 760, val loss: 0.7683923840522766
Epoch 770, training loss: 6.163416862487793 = 0.06463111937046051 + 1.0 * 6.098785877227783
Epoch 770, val loss: 0.7740356922149658
Epoch 780, training loss: 6.155126571655273 = 0.06188862770795822 + 1.0 * 6.09323787689209
Epoch 780, val loss: 0.7797538638114929
Epoch 790, training loss: 6.153046131134033 = 0.05926863104104996 + 1.0 * 6.093777656555176
Epoch 790, val loss: 0.7854093909263611
Epoch 800, training loss: 6.145263195037842 = 0.05679234862327576 + 1.0 * 6.088470935821533
Epoch 800, val loss: 0.791006863117218
Epoch 810, training loss: 6.14199686050415 = 0.05443017557263374 + 1.0 * 6.08756685256958
Epoch 810, val loss: 0.7966175675392151
Epoch 820, training loss: 6.14801025390625 = 0.052179913967847824 + 1.0 * 6.09583044052124
Epoch 820, val loss: 0.8021273612976074
Epoch 830, training loss: 6.138950824737549 = 0.050049178302288055 + 1.0 * 6.088901519775391
Epoch 830, val loss: 0.8076943755149841
Epoch 840, training loss: 6.1328206062316895 = 0.04800976812839508 + 1.0 * 6.084810733795166
Epoch 840, val loss: 0.8132132291793823
Epoch 850, training loss: 6.1300048828125 = 0.04605765640735626 + 1.0 * 6.08394718170166
Epoch 850, val loss: 0.8186634182929993
Epoch 860, training loss: 6.13054084777832 = 0.04419044032692909 + 1.0 * 6.086350440979004
Epoch 860, val loss: 0.8240500688552856
Epoch 870, training loss: 6.126533031463623 = 0.04241069778800011 + 1.0 * 6.084122180938721
Epoch 870, val loss: 0.8295071125030518
Epoch 880, training loss: 6.128848075866699 = 0.04070240259170532 + 1.0 * 6.088145732879639
Epoch 880, val loss: 0.8347725868225098
Epoch 890, training loss: 6.117721080780029 = 0.03904808685183525 + 1.0 * 6.078672885894775
Epoch 890, val loss: 0.8400940895080566
Epoch 900, training loss: 6.11614990234375 = 0.037459131330251694 + 1.0 * 6.078691005706787
Epoch 900, val loss: 0.8454589247703552
Epoch 910, training loss: 6.113848686218262 = 0.035928383469581604 + 1.0 * 6.077920436859131
Epoch 910, val loss: 0.8507019877433777
Epoch 920, training loss: 6.117753982543945 = 0.03444995731115341 + 1.0 * 6.083303928375244
Epoch 920, val loss: 0.8558991551399231
Epoch 930, training loss: 6.1136369705200195 = 0.03304213285446167 + 1.0 * 6.080595016479492
Epoch 930, val loss: 0.8610620498657227
Epoch 940, training loss: 6.106852054595947 = 0.031694523990154266 + 1.0 * 6.075157642364502
Epoch 940, val loss: 0.8662881851196289
Epoch 950, training loss: 6.1066389083862305 = 0.030401062220335007 + 1.0 * 6.076237678527832
Epoch 950, val loss: 0.8714867234230042
Epoch 960, training loss: 6.105713844299316 = 0.029168901965022087 + 1.0 * 6.076544761657715
Epoch 960, val loss: 0.8765636086463928
Epoch 970, training loss: 6.102144718170166 = 0.02800440974533558 + 1.0 * 6.0741400718688965
Epoch 970, val loss: 0.8817821145057678
Epoch 980, training loss: 6.097658634185791 = 0.02690056711435318 + 1.0 * 6.070757865905762
Epoch 980, val loss: 0.8869715332984924
Epoch 990, training loss: 6.098027229309082 = 0.02584386058151722 + 1.0 * 6.072183132171631
Epoch 990, val loss: 0.892129123210907
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.325541496276855 = 1.9516876935958862 + 1.0 * 8.37385368347168
Epoch 0, val loss: 1.9538906812667847
Epoch 10, training loss: 10.31471061706543 = 1.9414715766906738 + 1.0 * 8.373238563537598
Epoch 10, val loss: 1.9439783096313477
Epoch 20, training loss: 10.29836654663086 = 1.929402470588684 + 1.0 * 8.368964195251465
Epoch 20, val loss: 1.9317559003829956
Epoch 30, training loss: 10.257261276245117 = 1.9134258031845093 + 1.0 * 8.343835830688477
Epoch 30, val loss: 1.9152847528457642
Epoch 40, training loss: 10.073974609375 = 1.8941504955291748 + 1.0 * 8.179823875427246
Epoch 40, val loss: 1.8959825038909912
Epoch 50, training loss: 9.634034156799316 = 1.8748325109481812 + 1.0 * 7.759202003479004
Epoch 50, val loss: 1.877030611038208
Epoch 60, training loss: 9.135298728942871 = 1.8591406345367432 + 1.0 * 7.276157855987549
Epoch 60, val loss: 1.8607221841812134
Epoch 70, training loss: 8.740730285644531 = 1.8424636125564575 + 1.0 * 6.898266315460205
Epoch 70, val loss: 1.8440836668014526
Epoch 80, training loss: 8.555923461914062 = 1.8259743452072144 + 1.0 * 6.729949474334717
Epoch 80, val loss: 1.8276993036270142
Epoch 90, training loss: 8.444601058959961 = 1.8080921173095703 + 1.0 * 6.636509418487549
Epoch 90, val loss: 1.8100796937942505
Epoch 100, training loss: 8.352535247802734 = 1.7889113426208496 + 1.0 * 6.563623905181885
Epoch 100, val loss: 1.791532278060913
Epoch 110, training loss: 8.27370834350586 = 1.7689132690429688 + 1.0 * 6.504794597625732
Epoch 110, val loss: 1.7723990678787231
Epoch 120, training loss: 8.20770263671875 = 1.747804045677185 + 1.0 * 6.459898471832275
Epoch 120, val loss: 1.7523527145385742
Epoch 130, training loss: 8.149160385131836 = 1.7246464490890503 + 1.0 * 6.424514293670654
Epoch 130, val loss: 1.7309024333953857
Epoch 140, training loss: 8.086084365844727 = 1.6991137266159058 + 1.0 * 6.3869709968566895
Epoch 140, val loss: 1.7080966234207153
Epoch 150, training loss: 8.02656364440918 = 1.6702139377593994 + 1.0 * 6.356349945068359
Epoch 150, val loss: 1.6833125352859497
Epoch 160, training loss: 7.973613739013672 = 1.6370441913604736 + 1.0 * 6.336569309234619
Epoch 160, val loss: 1.6555641889572144
Epoch 170, training loss: 7.912282943725586 = 1.5988057851791382 + 1.0 * 6.313477039337158
Epoch 170, val loss: 1.6240220069885254
Epoch 180, training loss: 7.850133895874023 = 1.5548951625823975 + 1.0 * 6.295238494873047
Epoch 180, val loss: 1.5879526138305664
Epoch 190, training loss: 7.785781383514404 = 1.504987120628357 + 1.0 * 6.280794143676758
Epoch 190, val loss: 1.5472068786621094
Epoch 200, training loss: 7.719267845153809 = 1.450509786605835 + 1.0 * 6.2687578201293945
Epoch 200, val loss: 1.5028353929519653
Epoch 210, training loss: 7.648650169372559 = 1.3924983739852905 + 1.0 * 6.2561516761779785
Epoch 210, val loss: 1.456037163734436
Epoch 220, training loss: 7.580516338348389 = 1.3323659896850586 + 1.0 * 6.24815034866333
Epoch 220, val loss: 1.407811164855957
Epoch 230, training loss: 7.509918212890625 = 1.2723020315170288 + 1.0 * 6.237616062164307
Epoch 230, val loss: 1.3602992296218872
Epoch 240, training loss: 7.4450554847717285 = 1.2135505676269531 + 1.0 * 6.231504917144775
Epoch 240, val loss: 1.314306616783142
Epoch 250, training loss: 7.378969192504883 = 1.1566798686981201 + 1.0 * 6.222289085388184
Epoch 250, val loss: 1.2705110311508179
Epoch 260, training loss: 7.320252895355225 = 1.1015995740890503 + 1.0 * 6.218653202056885
Epoch 260, val loss: 1.2288107872009277
Epoch 270, training loss: 7.258425235748291 = 1.0490622520446777 + 1.0 * 6.209362983703613
Epoch 270, val loss: 1.1895946264266968
Epoch 280, training loss: 7.201297283172607 = 0.9983196258544922 + 1.0 * 6.202977657318115
Epoch 280, val loss: 1.1523067951202393
Epoch 290, training loss: 7.151266098022461 = 0.9489909410476685 + 1.0 * 6.202275276184082
Epoch 290, val loss: 1.1165155172348022
Epoch 300, training loss: 7.094845294952393 = 0.9015796780586243 + 1.0 * 6.193265438079834
Epoch 300, val loss: 1.082565426826477
Epoch 310, training loss: 7.044105052947998 = 0.8557423949241638 + 1.0 * 6.1883625984191895
Epoch 310, val loss: 1.0501534938812256
Epoch 320, training loss: 7.00272274017334 = 0.8115304112434387 + 1.0 * 6.191192150115967
Epoch 320, val loss: 1.0193390846252441
Epoch 330, training loss: 6.949561595916748 = 0.7695366144180298 + 1.0 * 6.180025100708008
Epoch 330, val loss: 0.990595281124115
Epoch 340, training loss: 6.903870105743408 = 0.7297617197036743 + 1.0 * 6.174108505249023
Epoch 340, val loss: 0.9640393257141113
Epoch 350, training loss: 6.8675856590271 = 0.6923236846923828 + 1.0 * 6.175261974334717
Epoch 350, val loss: 0.9394697546958923
Epoch 360, training loss: 6.825080871582031 = 0.6575208902359009 + 1.0 * 6.16756010055542
Epoch 360, val loss: 0.9170198440551758
Epoch 370, training loss: 6.787792682647705 = 0.6250981092453003 + 1.0 * 6.162694454193115
Epoch 370, val loss: 0.8964224457740784
Epoch 380, training loss: 6.7615885734558105 = 0.5947478413581848 + 1.0 * 6.166840553283691
Epoch 380, val loss: 0.8775322437286377
Epoch 390, training loss: 6.727176189422607 = 0.5664708018302917 + 1.0 * 6.16070556640625
Epoch 390, val loss: 0.8603096008300781
Epoch 400, training loss: 6.692861557006836 = 0.5399328470230103 + 1.0 * 6.152928829193115
Epoch 400, val loss: 0.8445457220077515
Epoch 410, training loss: 6.664097785949707 = 0.5146121978759766 + 1.0 * 6.1494855880737305
Epoch 410, val loss: 0.8299373388290405
Epoch 420, training loss: 6.6412529945373535 = 0.4903813600540161 + 1.0 * 6.150871753692627
Epoch 420, val loss: 0.8163202404975891
Epoch 430, training loss: 6.611899375915527 = 0.4671952724456787 + 1.0 * 6.144704341888428
Epoch 430, val loss: 0.8036932945251465
Epoch 440, training loss: 6.586245059967041 = 0.44489946961402893 + 1.0 * 6.141345500946045
Epoch 440, val loss: 0.7918433547019958
Epoch 450, training loss: 6.561527729034424 = 0.42322519421577454 + 1.0 * 6.138302326202393
Epoch 450, val loss: 0.7807729244232178
Epoch 460, training loss: 6.539675712585449 = 0.40204307436943054 + 1.0 * 6.137632846832275
Epoch 460, val loss: 0.7703163027763367
Epoch 470, training loss: 6.523154258728027 = 0.38143905997276306 + 1.0 * 6.141715049743652
Epoch 470, val loss: 0.7604397535324097
Epoch 480, training loss: 6.493917465209961 = 0.3613951802253723 + 1.0 * 6.132522106170654
Epoch 480, val loss: 0.7513042092323303
Epoch 490, training loss: 6.4703497886657715 = 0.34182387590408325 + 1.0 * 6.128525733947754
Epoch 490, val loss: 0.7427945733070374
Epoch 500, training loss: 6.457846164703369 = 0.3226933479309082 + 1.0 * 6.135152816772461
Epoch 500, val loss: 0.7347865700721741
Epoch 510, training loss: 6.431644916534424 = 0.30426275730133057 + 1.0 * 6.127382278442383
Epoch 510, val loss: 0.7274907827377319
Epoch 520, training loss: 6.414951801300049 = 0.2865484356880188 + 1.0 * 6.128403186798096
Epoch 520, val loss: 0.720905065536499
Epoch 530, training loss: 6.391992092132568 = 0.2696476876735687 + 1.0 * 6.122344493865967
Epoch 530, val loss: 0.7149782180786133
Epoch 540, training loss: 6.372264862060547 = 0.25345897674560547 + 1.0 * 6.118805885314941
Epoch 540, val loss: 0.7096875905990601
Epoch 550, training loss: 6.3662872314453125 = 0.23807743191719055 + 1.0 * 6.128209590911865
Epoch 550, val loss: 0.705112099647522
Epoch 560, training loss: 6.339503288269043 = 0.22359991073608398 + 1.0 * 6.115903377532959
Epoch 560, val loss: 0.7012083530426025
Epoch 570, training loss: 6.327264785766602 = 0.21006497740745544 + 1.0 * 6.117199897766113
Epoch 570, val loss: 0.6980435252189636
Epoch 580, training loss: 6.3094096183776855 = 0.19747786223888397 + 1.0 * 6.111931800842285
Epoch 580, val loss: 0.6955263614654541
Epoch 590, training loss: 6.298618316650391 = 0.18573418259620667 + 1.0 * 6.112884044647217
Epoch 590, val loss: 0.6936346888542175
Epoch 600, training loss: 6.287271022796631 = 0.17478711903095245 + 1.0 * 6.112483978271484
Epoch 600, val loss: 0.6923096776008606
Epoch 610, training loss: 6.274712562561035 = 0.16463258862495422 + 1.0 * 6.110079765319824
Epoch 610, val loss: 0.6915425658226013
Epoch 620, training loss: 6.2612714767456055 = 0.1552397608757019 + 1.0 * 6.106031894683838
Epoch 620, val loss: 0.6913184523582458
Epoch 630, training loss: 6.258369445800781 = 0.14650069177150726 + 1.0 * 6.111868858337402
Epoch 630, val loss: 0.6915973424911499
Epoch 640, training loss: 6.245569229125977 = 0.13839709758758545 + 1.0 * 6.107172012329102
Epoch 640, val loss: 0.6922417879104614
Epoch 650, training loss: 6.234208106994629 = 0.13088375329971313 + 1.0 * 6.1033244132995605
Epoch 650, val loss: 0.6933106780052185
Epoch 660, training loss: 6.225302219390869 = 0.12393107265233994 + 1.0 * 6.1013712882995605
Epoch 660, val loss: 0.6947123408317566
Epoch 670, training loss: 6.216552734375 = 0.1174774169921875 + 1.0 * 6.0990753173828125
Epoch 670, val loss: 0.6964417099952698
Epoch 680, training loss: 6.20844841003418 = 0.11147036403417587 + 1.0 * 6.096978187561035
Epoch 680, val loss: 0.698499858379364
Epoch 690, training loss: 6.201075077056885 = 0.10583484172821045 + 1.0 * 6.095240116119385
Epoch 690, val loss: 0.7008079290390015
Epoch 700, training loss: 6.205406188964844 = 0.10057584941387177 + 1.0 * 6.104830265045166
Epoch 700, val loss: 0.703355610370636
Epoch 710, training loss: 6.1909685134887695 = 0.0956692323088646 + 1.0 * 6.095299243927002
Epoch 710, val loss: 0.7061241865158081
Epoch 720, training loss: 6.1828999519348145 = 0.09107786417007446 + 1.0 * 6.091822147369385
Epoch 720, val loss: 0.7090949416160583
Epoch 730, training loss: 6.181364059448242 = 0.08677560091018677 + 1.0 * 6.094588279724121
Epoch 730, val loss: 0.7122228741645813
Epoch 740, training loss: 6.177056789398193 = 0.08275361359119415 + 1.0 * 6.094303131103516
Epoch 740, val loss: 0.7155014872550964
Epoch 750, training loss: 6.16897439956665 = 0.07896742969751358 + 1.0 * 6.0900068283081055
Epoch 750, val loss: 0.7188531756401062
Epoch 760, training loss: 6.166110515594482 = 0.07542619854211807 + 1.0 * 6.090684413909912
Epoch 760, val loss: 0.7223870754241943
Epoch 770, training loss: 6.157927513122559 = 0.07208659499883652 + 1.0 * 6.085840702056885
Epoch 770, val loss: 0.7259611487388611
Epoch 780, training loss: 6.154765605926514 = 0.06894002109766006 + 1.0 * 6.085825443267822
Epoch 780, val loss: 0.7296343445777893
Epoch 790, training loss: 6.1513566970825195 = 0.06597180664539337 + 1.0 * 6.085384845733643
Epoch 790, val loss: 0.7334187030792236
Epoch 800, training loss: 6.152501106262207 = 0.06317213177680969 + 1.0 * 6.089328765869141
Epoch 800, val loss: 0.7372483015060425
Epoch 810, training loss: 6.144556999206543 = 0.060522183775901794 + 1.0 * 6.0840349197387695
Epoch 810, val loss: 0.7410480380058289
Epoch 820, training loss: 6.139919281005859 = 0.05803268030285835 + 1.0 * 6.0818867683410645
Epoch 820, val loss: 0.744999885559082
Epoch 830, training loss: 6.136123180389404 = 0.055673085153102875 + 1.0 * 6.080450057983398
Epoch 830, val loss: 0.7489672899246216
Epoch 840, training loss: 6.13532829284668 = 0.053439561277627945 + 1.0 * 6.081888675689697
Epoch 840, val loss: 0.7529042363166809
Epoch 850, training loss: 6.134676456451416 = 0.05132872983813286 + 1.0 * 6.083347797393799
Epoch 850, val loss: 0.7569171786308289
Epoch 860, training loss: 6.125230312347412 = 0.04931996390223503 + 1.0 * 6.075910568237305
Epoch 860, val loss: 0.760843813419342
Epoch 870, training loss: 6.122936725616455 = 0.0474201925098896 + 1.0 * 6.075516700744629
Epoch 870, val loss: 0.7648659348487854
Epoch 880, training loss: 6.135778427124023 = 0.04561357572674751 + 1.0 * 6.090164661407471
Epoch 880, val loss: 0.7688602805137634
Epoch 890, training loss: 6.118854522705078 = 0.043909408152103424 + 1.0 * 6.074944972991943
Epoch 890, val loss: 0.7728932499885559
Epoch 900, training loss: 6.116779327392578 = 0.04229025915265083 + 1.0 * 6.074489116668701
Epoch 900, val loss: 0.7769361138343811
Epoch 910, training loss: 6.119288921356201 = 0.040747568011283875 + 1.0 * 6.078541278839111
Epoch 910, val loss: 0.7809730172157288
Epoch 920, training loss: 6.111331462860107 = 0.039286497980356216 + 1.0 * 6.072044849395752
Epoch 920, val loss: 0.785005509853363
Epoch 930, training loss: 6.10864782333374 = 0.03789343684911728 + 1.0 * 6.070754528045654
Epoch 930, val loss: 0.7890412211418152
Epoch 940, training loss: 6.113790512084961 = 0.0365658663213253 + 1.0 * 6.0772247314453125
Epoch 940, val loss: 0.7930418848991394
Epoch 950, training loss: 6.106257438659668 = 0.035304956138134 + 1.0 * 6.070952415466309
Epoch 950, val loss: 0.797031044960022
Epoch 960, training loss: 6.101480484008789 = 0.03410699963569641 + 1.0 * 6.067373275756836
Epoch 960, val loss: 0.8010529279708862
Epoch 970, training loss: 6.100436210632324 = 0.032964520156383514 + 1.0 * 6.067471504211426
Epoch 970, val loss: 0.8050855994224548
Epoch 980, training loss: 6.1112895011901855 = 0.03187357634305954 + 1.0 * 6.079415798187256
Epoch 980, val loss: 0.8090643286705017
Epoch 990, training loss: 6.097551345825195 = 0.030826281756162643 + 1.0 * 6.066725254058838
Epoch 990, val loss: 0.8129265904426575
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.0332
Flip ASR: 0.0267/225 nodes
The final ASR:0.52768, 0.39088, Accuracy:0.79506, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11576])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10508])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.315814971923828 = 1.9420533180236816 + 1.0 * 8.373761177062988
Epoch 0, val loss: 1.941232681274414
Epoch 10, training loss: 10.304576873779297 = 1.9314029216766357 + 1.0 * 8.373173713684082
Epoch 10, val loss: 1.9314764738082886
Epoch 20, training loss: 10.287610054016113 = 1.9178658723831177 + 1.0 * 8.369744300842285
Epoch 20, val loss: 1.9189258813858032
Epoch 30, training loss: 10.244390487670898 = 1.898736596107483 + 1.0 * 8.345653533935547
Epoch 30, val loss: 1.9011971950531006
Epoch 40, training loss: 10.04479694366455 = 1.8750059604644775 + 1.0 * 8.169791221618652
Epoch 40, val loss: 1.879998803138733
Epoch 50, training loss: 9.315288543701172 = 1.849579095840454 + 1.0 * 7.465709209442139
Epoch 50, val loss: 1.8575230836868286
Epoch 60, training loss: 8.86253833770752 = 1.831915020942688 + 1.0 * 7.030623435974121
Epoch 60, val loss: 1.8425508737564087
Epoch 70, training loss: 8.609221458435059 = 1.8199338912963867 + 1.0 * 6.789287567138672
Epoch 70, val loss: 1.8313108682632446
Epoch 80, training loss: 8.476146697998047 = 1.8071637153625488 + 1.0 * 6.668982982635498
Epoch 80, val loss: 1.8198925256729126
Epoch 90, training loss: 8.378215789794922 = 1.794318437576294 + 1.0 * 6.583897590637207
Epoch 90, val loss: 1.8086634874343872
Epoch 100, training loss: 8.304187774658203 = 1.7825093269348145 + 1.0 * 6.5216779708862305
Epoch 100, val loss: 1.7982112169265747
Epoch 110, training loss: 8.235774993896484 = 1.77083158493042 + 1.0 * 6.4649434089660645
Epoch 110, val loss: 1.7873610258102417
Epoch 120, training loss: 8.176222801208496 = 1.7582567930221558 + 1.0 * 6.417966365814209
Epoch 120, val loss: 1.775590181350708
Epoch 130, training loss: 8.119242668151855 = 1.7444472312927246 + 1.0 * 6.374795436859131
Epoch 130, val loss: 1.7631161212921143
Epoch 140, training loss: 8.066471099853516 = 1.728758454322815 + 1.0 * 6.337712287902832
Epoch 140, val loss: 1.7495431900024414
Epoch 150, training loss: 8.01822280883789 = 1.7096728086471558 + 1.0 * 6.3085503578186035
Epoch 150, val loss: 1.7335401773452759
Epoch 160, training loss: 7.9709858894348145 = 1.6859617233276367 + 1.0 * 6.285024166107178
Epoch 160, val loss: 1.7140103578567505
Epoch 170, training loss: 7.92195987701416 = 1.657151699066162 + 1.0 * 6.264808177947998
Epoch 170, val loss: 1.6905627250671387
Epoch 180, training loss: 7.8710808753967285 = 1.6223031282424927 + 1.0 * 6.248777866363525
Epoch 180, val loss: 1.6622740030288696
Epoch 190, training loss: 7.8152875900268555 = 1.5804681777954102 + 1.0 * 6.234819412231445
Epoch 190, val loss: 1.6283332109451294
Epoch 200, training loss: 7.755908012390137 = 1.5323596000671387 + 1.0 * 6.223548412322998
Epoch 200, val loss: 1.5897995233535767
Epoch 210, training loss: 7.692834854125977 = 1.4793778657913208 + 1.0 * 6.213457107543945
Epoch 210, val loss: 1.547658920288086
Epoch 220, training loss: 7.625839710235596 = 1.42196524143219 + 1.0 * 6.203874588012695
Epoch 220, val loss: 1.5022815465927124
Epoch 230, training loss: 7.559573173522949 = 1.3624763488769531 + 1.0 * 6.197096824645996
Epoch 230, val loss: 1.4559277296066284
Epoch 240, training loss: 7.4921956062316895 = 1.3033651113510132 + 1.0 * 6.188830375671387
Epoch 240, val loss: 1.4101643562316895
Epoch 250, training loss: 7.426393508911133 = 1.2444262504577637 + 1.0 * 6.181967258453369
Epoch 250, val loss: 1.3645908832550049
Epoch 260, training loss: 7.364384174346924 = 1.1865240335464478 + 1.0 * 6.177860260009766
Epoch 260, val loss: 1.3201366662979126
Epoch 270, training loss: 7.301092147827148 = 1.1314009428024292 + 1.0 * 6.16969108581543
Epoch 270, val loss: 1.2782572507858276
Epoch 280, training loss: 7.242352485656738 = 1.0787975788116455 + 1.0 * 6.163554668426514
Epoch 280, val loss: 1.2384312152862549
Epoch 290, training loss: 7.191433429718018 = 1.0295753479003906 + 1.0 * 6.161858081817627
Epoch 290, val loss: 1.2015849351882935
Epoch 300, training loss: 7.136882781982422 = 0.9839950203895569 + 1.0 * 6.15288782119751
Epoch 300, val loss: 1.1679691076278687
Epoch 310, training loss: 7.090287208557129 = 0.9411159157752991 + 1.0 * 6.149171352386475
Epoch 310, val loss: 1.1365995407104492
Epoch 320, training loss: 7.047351837158203 = 0.9008667469024658 + 1.0 * 6.146485328674316
Epoch 320, val loss: 1.1075499057769775
Epoch 330, training loss: 7.004140377044678 = 0.8632180094718933 + 1.0 * 6.140922546386719
Epoch 330, val loss: 1.0807684659957886
Epoch 340, training loss: 6.963301181793213 = 0.8274868726730347 + 1.0 * 6.135814189910889
Epoch 340, val loss: 1.05610191822052
Epoch 350, training loss: 6.925163269042969 = 0.7933506965637207 + 1.0 * 6.131812572479248
Epoch 350, val loss: 1.0331023931503296
Epoch 360, training loss: 6.894949436187744 = 0.7607030272483826 + 1.0 * 6.134246349334717
Epoch 360, val loss: 1.0118756294250488
Epoch 370, training loss: 6.855714797973633 = 0.7298305630683899 + 1.0 * 6.125884056091309
Epoch 370, val loss: 0.992607057094574
Epoch 380, training loss: 6.821711540222168 = 0.6995986104011536 + 1.0 * 6.12211275100708
Epoch 380, val loss: 0.974342405796051
Epoch 390, training loss: 6.7900238037109375 = 0.669501543045044 + 1.0 * 6.1205220222473145
Epoch 390, val loss: 0.956725537776947
Epoch 400, training loss: 6.760232448577881 = 0.6395968198776245 + 1.0 * 6.120635509490967
Epoch 400, val loss: 0.9396313428878784
Epoch 410, training loss: 6.724806308746338 = 0.6097405552864075 + 1.0 * 6.115065574645996
Epoch 410, val loss: 0.923111081123352
Epoch 420, training loss: 6.6954851150512695 = 0.5798261761665344 + 1.0 * 6.115658760070801
Epoch 420, val loss: 0.9066550731658936
Epoch 430, training loss: 6.663364410400391 = 0.5503270626068115 + 1.0 * 6.113037586212158
Epoch 430, val loss: 0.8908364176750183
Epoch 440, training loss: 6.6295976638793945 = 0.5216494798660278 + 1.0 * 6.107948303222656
Epoch 440, val loss: 0.8761183023452759
Epoch 450, training loss: 6.599721908569336 = 0.4937584400177002 + 1.0 * 6.105963230133057
Epoch 450, val loss: 0.8624221086502075
Epoch 460, training loss: 6.575280666351318 = 0.46684879064559937 + 1.0 * 6.108431816101074
Epoch 460, val loss: 0.8500515818595886
Epoch 470, training loss: 6.542504787445068 = 0.44116708636283875 + 1.0 * 6.101337909698486
Epoch 470, val loss: 0.8390887975692749
Epoch 480, training loss: 6.517604351043701 = 0.41660282015800476 + 1.0 * 6.101001739501953
Epoch 480, val loss: 0.8296362161636353
Epoch 490, training loss: 6.491264820098877 = 0.39299145340919495 + 1.0 * 6.098273277282715
Epoch 490, val loss: 0.8212242126464844
Epoch 500, training loss: 6.467472076416016 = 0.370227575302124 + 1.0 * 6.0972442626953125
Epoch 500, val loss: 0.8138834238052368
Epoch 510, training loss: 6.445919036865234 = 0.3482280969619751 + 1.0 * 6.097691059112549
Epoch 510, val loss: 0.8074634075164795
Epoch 520, training loss: 6.4239420890808105 = 0.3271484673023224 + 1.0 * 6.0967936515808105
Epoch 520, val loss: 0.8015970587730408
Epoch 530, training loss: 6.398650646209717 = 0.3068973124027252 + 1.0 * 6.0917534828186035
Epoch 530, val loss: 0.7967879772186279
Epoch 540, training loss: 6.378973484039307 = 0.28738194704055786 + 1.0 * 6.0915913581848145
Epoch 540, val loss: 0.7922847867012024
Epoch 550, training loss: 6.358151435852051 = 0.2687408924102783 + 1.0 * 6.089410781860352
Epoch 550, val loss: 0.7885057330131531
Epoch 560, training loss: 6.3372883796691895 = 0.2510218024253845 + 1.0 * 6.08626651763916
Epoch 560, val loss: 0.7856336832046509
Epoch 570, training loss: 6.318897247314453 = 0.23417676985263824 + 1.0 * 6.084720611572266
Epoch 570, val loss: 0.7833722829818726
Epoch 580, training loss: 6.302783012390137 = 0.21831636130809784 + 1.0 * 6.084466457366943
Epoch 580, val loss: 0.7817028760910034
Epoch 590, training loss: 6.287438869476318 = 0.2035953849554062 + 1.0 * 6.08384370803833
Epoch 590, val loss: 0.7809357643127441
Epoch 600, training loss: 6.2753496170043945 = 0.18988454341888428 + 1.0 * 6.085464954376221
Epoch 600, val loss: 0.7808171510696411
Epoch 610, training loss: 6.257195472717285 = 0.17717596888542175 + 1.0 * 6.080019474029541
Epoch 610, val loss: 0.7812126874923706
Epoch 620, training loss: 6.243490695953369 = 0.16535627841949463 + 1.0 * 6.078134536743164
Epoch 620, val loss: 0.7824141979217529
Epoch 630, training loss: 6.239698886871338 = 0.15442419052124023 + 1.0 * 6.085274696350098
Epoch 630, val loss: 0.7839962244033813
Epoch 640, training loss: 6.218593597412109 = 0.14436651766300201 + 1.0 * 6.0742268562316895
Epoch 640, val loss: 0.7862364053726196
Epoch 650, training loss: 6.214802265167236 = 0.135101318359375 + 1.0 * 6.079700946807861
Epoch 650, val loss: 0.7890025973320007
Epoch 660, training loss: 6.20198392868042 = 0.12660300731658936 + 1.0 * 6.075380802154541
Epoch 660, val loss: 0.7919041514396667
Epoch 670, training loss: 6.190299034118652 = 0.11878371983766556 + 1.0 * 6.071515083312988
Epoch 670, val loss: 0.7955017685890198
Epoch 680, training loss: 6.190908432006836 = 0.11155255138874054 + 1.0 * 6.079355716705322
Epoch 680, val loss: 0.7991625070571899
Epoch 690, training loss: 6.174136638641357 = 0.1049468144774437 + 1.0 * 6.06919002532959
Epoch 690, val loss: 0.8032187223434448
Epoch 700, training loss: 6.166164398193359 = 0.09886373579502106 + 1.0 * 6.067300796508789
Epoch 700, val loss: 0.8077154755592346
Epoch 710, training loss: 6.159544467926025 = 0.0932290256023407 + 1.0 * 6.066315650939941
Epoch 710, val loss: 0.812234103679657
Epoch 720, training loss: 6.153630256652832 = 0.08804866671562195 + 1.0 * 6.065581798553467
Epoch 720, val loss: 0.8166527152061462
Epoch 730, training loss: 6.147853851318359 = 0.0833214670419693 + 1.0 * 6.064532279968262
Epoch 730, val loss: 0.8216506838798523
Epoch 740, training loss: 6.142140865325928 = 0.07894308120012283 + 1.0 * 6.063197612762451
Epoch 740, val loss: 0.8266732096672058
Epoch 750, training loss: 6.136331081390381 = 0.07486037909984589 + 1.0 * 6.0614705085754395
Epoch 750, val loss: 0.8316546678543091
Epoch 760, training loss: 6.132659912109375 = 0.07105407863855362 + 1.0 * 6.061605930328369
Epoch 760, val loss: 0.8368944525718689
Epoch 770, training loss: 6.128645896911621 = 0.06752414256334305 + 1.0 * 6.061121940612793
Epoch 770, val loss: 0.8418521881103516
Epoch 780, training loss: 6.123064041137695 = 0.06427038460969925 + 1.0 * 6.058793544769287
Epoch 780, val loss: 0.8472573161125183
Epoch 790, training loss: 6.119383335113525 = 0.06122733652591705 + 1.0 * 6.0581560134887695
Epoch 790, val loss: 0.8524892330169678
Epoch 800, training loss: 6.124808311462402 = 0.05837452784180641 + 1.0 * 6.066433906555176
Epoch 800, val loss: 0.8575791716575623
Epoch 810, training loss: 6.113778591156006 = 0.055716995149850845 + 1.0 * 6.058061599731445
Epoch 810, val loss: 0.8628169894218445
Epoch 820, training loss: 6.107800483703613 = 0.05322737619280815 + 1.0 * 6.054573059082031
Epoch 820, val loss: 0.8680490255355835
Epoch 830, training loss: 6.106761932373047 = 0.0508800745010376 + 1.0 * 6.055881977081299
Epoch 830, val loss: 0.8731680512428284
Epoch 840, training loss: 6.102167129516602 = 0.04868810251355171 + 1.0 * 6.053479194641113
Epoch 840, val loss: 0.8781919479370117
Epoch 850, training loss: 6.1077446937561035 = 0.04663417488336563 + 1.0 * 6.061110496520996
Epoch 850, val loss: 0.8833067417144775
Epoch 860, training loss: 6.0973711013793945 = 0.04470977932214737 + 1.0 * 6.052661418914795
Epoch 860, val loss: 0.8882644772529602
Epoch 870, training loss: 6.093599319458008 = 0.04289621859788895 + 1.0 * 6.050703048706055
Epoch 870, val loss: 0.8933568000793457
Epoch 880, training loss: 6.090826988220215 = 0.04117642343044281 + 1.0 * 6.0496506690979
Epoch 880, val loss: 0.8983296155929565
Epoch 890, training loss: 6.097522735595703 = 0.0395527258515358 + 1.0 * 6.05797004699707
Epoch 890, val loss: 0.9032328724861145
Epoch 900, training loss: 6.09128999710083 = 0.03802470117807388 + 1.0 * 6.05326509475708
Epoch 900, val loss: 0.9079040884971619
Epoch 910, training loss: 6.0843305587768555 = 0.0365937277674675 + 1.0 * 6.047736644744873
Epoch 910, val loss: 0.9129403233528137
Epoch 920, training loss: 6.083746433258057 = 0.03523324057459831 + 1.0 * 6.048513412475586
Epoch 920, val loss: 0.9176883697509766
Epoch 930, training loss: 6.079912185668945 = 0.033944424241781235 + 1.0 * 6.0459675788879395
Epoch 930, val loss: 0.9221086502075195
Epoch 940, training loss: 6.079642295837402 = 0.032736919820308685 + 1.0 * 6.046905517578125
Epoch 940, val loss: 0.926932692527771
Epoch 950, training loss: 6.076052188873291 = 0.03158503770828247 + 1.0 * 6.044466972351074
Epoch 950, val loss: 0.9315586090087891
Epoch 960, training loss: 6.083508491516113 = 0.03048735484480858 + 1.0 * 6.05302095413208
Epoch 960, val loss: 0.9360656142234802
Epoch 970, training loss: 6.079564571380615 = 0.029456837102770805 + 1.0 * 6.050107955932617
Epoch 970, val loss: 0.9402373433113098
Epoch 980, training loss: 6.070825576782227 = 0.028484582901000977 + 1.0 * 6.0423407554626465
Epoch 980, val loss: 0.9448749423027039
Epoch 990, training loss: 6.069451332092285 = 0.027553796768188477 + 1.0 * 6.041897296905518
Epoch 990, val loss: 0.9492530822753906
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6900
Flip ASR: 0.6356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.33230972290039 = 1.9585694074630737 + 1.0 * 8.373740196228027
Epoch 0, val loss: 1.957611322402954
Epoch 10, training loss: 10.321427345275879 = 1.9483346939086914 + 1.0 * 8.373092651367188
Epoch 10, val loss: 1.946940541267395
Epoch 20, training loss: 10.30509090423584 = 1.9356454610824585 + 1.0 * 8.36944580078125
Epoch 20, val loss: 1.9336624145507812
Epoch 30, training loss: 10.26388168334961 = 1.9180617332458496 + 1.0 * 8.345820426940918
Epoch 30, val loss: 1.9154433012008667
Epoch 40, training loss: 10.032289505004883 = 1.8964598178863525 + 1.0 * 8.13582992553711
Epoch 40, val loss: 1.8941607475280762
Epoch 50, training loss: 8.993234634399414 = 1.8754760026931763 + 1.0 * 7.117758750915527
Epoch 50, val loss: 1.8738877773284912
Epoch 60, training loss: 8.689421653747559 = 1.8593289852142334 + 1.0 * 6.830092906951904
Epoch 60, val loss: 1.857973337173462
Epoch 70, training loss: 8.49206256866455 = 1.8456840515136719 + 1.0 * 6.646378517150879
Epoch 70, val loss: 1.8442552089691162
Epoch 80, training loss: 8.373723983764648 = 1.8323580026626587 + 1.0 * 6.541366100311279
Epoch 80, val loss: 1.8308690786361694
Epoch 90, training loss: 8.288962364196777 = 1.8184937238693237 + 1.0 * 6.470468521118164
Epoch 90, val loss: 1.81708562374115
Epoch 100, training loss: 8.214316368103027 = 1.8044496774673462 + 1.0 * 6.409866809844971
Epoch 100, val loss: 1.8034907579421997
Epoch 110, training loss: 8.153315544128418 = 1.7908047437667847 + 1.0 * 6.362511157989502
Epoch 110, val loss: 1.7902418375015259
Epoch 120, training loss: 8.10400104522705 = 1.777026891708374 + 1.0 * 6.326973915100098
Epoch 120, val loss: 1.776786208152771
Epoch 130, training loss: 8.060325622558594 = 1.7623006105422974 + 1.0 * 6.298024654388428
Epoch 130, val loss: 1.7626632452011108
Epoch 140, training loss: 8.018939971923828 = 1.7461401224136353 + 1.0 * 6.272799968719482
Epoch 140, val loss: 1.7476446628570557
Epoch 150, training loss: 7.9793596267700195 = 1.7276113033294678 + 1.0 * 6.251748085021973
Epoch 150, val loss: 1.7309718132019043
Epoch 160, training loss: 7.941333770751953 = 1.705902338027954 + 1.0 * 6.235431671142578
Epoch 160, val loss: 1.7120459079742432
Epoch 170, training loss: 7.898941993713379 = 1.6802144050598145 + 1.0 * 6.2187275886535645
Epoch 170, val loss: 1.6903197765350342
Epoch 180, training loss: 7.85396146774292 = 1.6491738557815552 + 1.0 * 6.204787731170654
Epoch 180, val loss: 1.6645174026489258
Epoch 190, training loss: 7.804440498352051 = 1.6111820936203003 + 1.0 * 6.193258285522461
Epoch 190, val loss: 1.633164882659912
Epoch 200, training loss: 7.751739025115967 = 1.5650016069412231 + 1.0 * 6.186737537384033
Epoch 200, val loss: 1.5951924324035645
Epoch 210, training loss: 7.689146518707275 = 1.5126961469650269 + 1.0 * 6.176450252532959
Epoch 210, val loss: 1.5528545379638672
Epoch 220, training loss: 7.625611305236816 = 1.4555643796920776 + 1.0 * 6.170046806335449
Epoch 220, val loss: 1.5067399740219116
Epoch 230, training loss: 7.559648513793945 = 1.395424485206604 + 1.0 * 6.164224147796631
Epoch 230, val loss: 1.4583266973495483
Epoch 240, training loss: 7.495996952056885 = 1.3348445892333984 + 1.0 * 6.161152362823486
Epoch 240, val loss: 1.4103392362594604
Epoch 250, training loss: 7.432677745819092 = 1.2778692245483398 + 1.0 * 6.154808521270752
Epoch 250, val loss: 1.3659099340438843
Epoch 260, training loss: 7.373513221740723 = 1.2245105504989624 + 1.0 * 6.149002552032471
Epoch 260, val loss: 1.3247008323669434
Epoch 270, training loss: 7.319276809692383 = 1.1740779876708984 + 1.0 * 6.145198822021484
Epoch 270, val loss: 1.286009669303894
Epoch 280, training loss: 7.266363143920898 = 1.1269234418869019 + 1.0 * 6.139439582824707
Epoch 280, val loss: 1.250537395477295
Epoch 290, training loss: 7.217942237854004 = 1.0824339389801025 + 1.0 * 6.135508060455322
Epoch 290, val loss: 1.217287540435791
Epoch 300, training loss: 7.172255039215088 = 1.0396164655685425 + 1.0 * 6.132638454437256
Epoch 300, val loss: 1.1855511665344238
Epoch 310, training loss: 7.126077651977539 = 0.9989430904388428 + 1.0 * 6.127134323120117
Epoch 310, val loss: 1.155990719795227
Epoch 320, training loss: 7.083682060241699 = 0.960191011428833 + 1.0 * 6.123491287231445
Epoch 320, val loss: 1.1278654336929321
Epoch 330, training loss: 7.043512344360352 = 0.9229586124420166 + 1.0 * 6.120553970336914
Epoch 330, val loss: 1.1010934114456177
Epoch 340, training loss: 7.004888534545898 = 0.8871087431907654 + 1.0 * 6.117779731750488
Epoch 340, val loss: 1.0755285024642944
Epoch 350, training loss: 6.965855598449707 = 0.8518351316452026 + 1.0 * 6.114020347595215
Epoch 350, val loss: 1.050508737564087
Epoch 360, training loss: 6.930872917175293 = 0.8169500827789307 + 1.0 * 6.113922595977783
Epoch 360, val loss: 1.0257188081741333
Epoch 370, training loss: 6.893062114715576 = 0.7826947569847107 + 1.0 * 6.110367298126221
Epoch 370, val loss: 1.0014809370040894
Epoch 380, training loss: 6.855405807495117 = 0.7486610412597656 + 1.0 * 6.106744766235352
Epoch 380, val loss: 0.9772446155548096
Epoch 390, training loss: 6.823450088500977 = 0.7152454257011414 + 1.0 * 6.1082048416137695
Epoch 390, val loss: 0.9533140063285828
Epoch 400, training loss: 6.786593914031982 = 0.6833997964859009 + 1.0 * 6.103194236755371
Epoch 400, val loss: 0.9306237697601318
Epoch 410, training loss: 6.7526960372924805 = 0.6527529358863831 + 1.0 * 6.099943161010742
Epoch 410, val loss: 0.9089583158493042
Epoch 420, training loss: 6.721405982971191 = 0.6233279705047607 + 1.0 * 6.09807825088501
Epoch 420, val loss: 0.8883441686630249
Epoch 430, training loss: 6.691038131713867 = 0.5952314138412476 + 1.0 * 6.09580659866333
Epoch 430, val loss: 0.8690880537033081
Epoch 440, training loss: 6.664840221405029 = 0.568489134311676 + 1.0 * 6.096351146697998
Epoch 440, val loss: 0.8513458371162415
Epoch 450, training loss: 6.635714054107666 = 0.5426743626594543 + 1.0 * 6.093039512634277
Epoch 450, val loss: 0.8347126841545105
Epoch 460, training loss: 6.607806205749512 = 0.5173839330673218 + 1.0 * 6.0904221534729
Epoch 460, val loss: 0.8189519643783569
Epoch 470, training loss: 6.590602874755859 = 0.49258333444595337 + 1.0 * 6.098019599914551
Epoch 470, val loss: 0.8040403723716736
Epoch 480, training loss: 6.556615829467773 = 0.4684624969959259 + 1.0 * 6.08815336227417
Epoch 480, val loss: 0.7902042269706726
Epoch 490, training loss: 6.531355857849121 = 0.4448532164096832 + 1.0 * 6.086502552032471
Epoch 490, val loss: 0.7771027684211731
Epoch 500, training loss: 6.506229400634766 = 0.4216412603855133 + 1.0 * 6.084588050842285
Epoch 500, val loss: 0.7647330164909363
Epoch 510, training loss: 6.495579719543457 = 0.39899003505706787 + 1.0 * 6.0965895652771
Epoch 510, val loss: 0.7532025575637817
Epoch 520, training loss: 6.462527275085449 = 0.3772484064102173 + 1.0 * 6.0852789878845215
Epoch 520, val loss: 0.7425758838653564
Epoch 530, training loss: 6.437165260314941 = 0.35628801584243774 + 1.0 * 6.080877304077148
Epoch 530, val loss: 0.7327079772949219
Epoch 540, training loss: 6.4233856201171875 = 0.33606913685798645 + 1.0 * 6.087316513061523
Epoch 540, val loss: 0.7235757112503052
Epoch 550, training loss: 6.396209239959717 = 0.31678807735443115 + 1.0 * 6.079421043395996
Epoch 550, val loss: 0.7152989506721497
Epoch 560, training loss: 6.376511573791504 = 0.2983696162700653 + 1.0 * 6.078142166137695
Epoch 560, val loss: 0.7077426910400391
Epoch 570, training loss: 6.35891580581665 = 0.28075578808784485 + 1.0 * 6.078159809112549
Epoch 570, val loss: 0.7008748650550842
Epoch 580, training loss: 6.346994876861572 = 0.2642101049423218 + 1.0 * 6.082784652709961
Epoch 580, val loss: 0.6947675347328186
Epoch 590, training loss: 6.3226165771484375 = 0.24862125515937805 + 1.0 * 6.073995113372803
Epoch 590, val loss: 0.6894189715385437
Epoch 600, training loss: 6.306102752685547 = 0.23388716578483582 + 1.0 * 6.072215557098389
Epoch 600, val loss: 0.6847008466720581
Epoch 610, training loss: 6.290892124176025 = 0.21994951367378235 + 1.0 * 6.070942401885986
Epoch 610, val loss: 0.6806311011314392
Epoch 620, training loss: 6.2866363525390625 = 0.20682847499847412 + 1.0 * 6.079807758331299
Epoch 620, val loss: 0.6771966814994812
Epoch 630, training loss: 6.264727592468262 = 0.19461636245250702 + 1.0 * 6.070111274719238
Epoch 630, val loss: 0.6743372082710266
Epoch 640, training loss: 6.25360631942749 = 0.18320181965827942 + 1.0 * 6.070404529571533
Epoch 640, val loss: 0.6720794439315796
Epoch 650, training loss: 6.239169120788574 = 0.17255616188049316 + 1.0 * 6.06661319732666
Epoch 650, val loss: 0.6703094840049744
Epoch 660, training loss: 6.232050895690918 = 0.16261577606201172 + 1.0 * 6.069435119628906
Epoch 660, val loss: 0.669059157371521
Epoch 670, training loss: 6.222324848175049 = 0.15337294340133667 + 1.0 * 6.0689520835876465
Epoch 670, val loss: 0.6682578325271606
Epoch 680, training loss: 6.2095232009887695 = 0.14479131996631622 + 1.0 * 6.064732074737549
Epoch 680, val loss: 0.6679121255874634
Epoch 690, training loss: 6.202453136444092 = 0.13680069148540497 + 1.0 * 6.065652370452881
Epoch 690, val loss: 0.6678733825683594
Epoch 700, training loss: 6.190431118011475 = 0.12936356663703918 + 1.0 * 6.061067581176758
Epoch 700, val loss: 0.668206512928009
Epoch 710, training loss: 6.182188987731934 = 0.1223960816860199 + 1.0 * 6.059792995452881
Epoch 710, val loss: 0.6688637733459473
Epoch 720, training loss: 6.174952983856201 = 0.11585882306098938 + 1.0 * 6.059093952178955
Epoch 720, val loss: 0.6697970032691956
Epoch 730, training loss: 6.179968357086182 = 0.10971776396036148 + 1.0 * 6.070250511169434
Epoch 730, val loss: 0.6708934307098389
Epoch 740, training loss: 6.165168762207031 = 0.10403965413570404 + 1.0 * 6.061129093170166
Epoch 740, val loss: 0.6721874475479126
Epoch 750, training loss: 6.159444332122803 = 0.09870929270982742 + 1.0 * 6.06073522567749
Epoch 750, val loss: 0.6736799478530884
Epoch 760, training loss: 6.152493476867676 = 0.09372953325510025 + 1.0 * 6.0587639808654785
Epoch 760, val loss: 0.6751762628555298
Epoch 770, training loss: 6.143293380737305 = 0.08903244882822037 + 1.0 * 6.054260730743408
Epoch 770, val loss: 0.676878809928894
Epoch 780, training loss: 6.13724946975708 = 0.08460977673530579 + 1.0 * 6.052639484405518
Epoch 780, val loss: 0.678693413734436
Epoch 790, training loss: 6.133029937744141 = 0.08042465895414352 + 1.0 * 6.052605152130127
Epoch 790, val loss: 0.6806083917617798
Epoch 800, training loss: 6.1283650398254395 = 0.07650760561227798 + 1.0 * 6.0518574714660645
Epoch 800, val loss: 0.6825318336486816
Epoch 810, training loss: 6.124486923217773 = 0.07285572588443756 + 1.0 * 6.051630973815918
Epoch 810, val loss: 0.6845924854278564
Epoch 820, training loss: 6.119444847106934 = 0.06943379342556 + 1.0 * 6.050011157989502
Epoch 820, val loss: 0.6867585182189941
Epoch 830, training loss: 6.115760326385498 = 0.06621156632900238 + 1.0 * 6.049548625946045
Epoch 830, val loss: 0.688937246799469
Epoch 840, training loss: 6.112417697906494 = 0.06318113952875137 + 1.0 * 6.04923677444458
Epoch 840, val loss: 0.6910834908485413
Epoch 850, training loss: 6.108943462371826 = 0.06034570932388306 + 1.0 * 6.048597812652588
Epoch 850, val loss: 0.6933899521827698
Epoch 860, training loss: 6.104278564453125 = 0.05768849700689316 + 1.0 * 6.0465898513793945
Epoch 860, val loss: 0.6957499384880066
Epoch 870, training loss: 6.101771354675293 = 0.05518770217895508 + 1.0 * 6.046583652496338
Epoch 870, val loss: 0.6981168985366821
Epoch 880, training loss: 6.0984296798706055 = 0.05282912775874138 + 1.0 * 6.045600414276123
Epoch 880, val loss: 0.7004896402359009
Epoch 890, training loss: 6.104015827178955 = 0.0506146065890789 + 1.0 * 6.053400993347168
Epoch 890, val loss: 0.7029047012329102
Epoch 900, training loss: 6.095737457275391 = 0.04853556305170059 + 1.0 * 6.047202110290527
Epoch 900, val loss: 0.7052359580993652
Epoch 910, training loss: 6.088512897491455 = 0.04659189656376839 + 1.0 * 6.041921138763428
Epoch 910, val loss: 0.7077009081840515
Epoch 920, training loss: 6.0858073234558105 = 0.04474891349673271 + 1.0 * 6.041058540344238
Epoch 920, val loss: 0.7101662158966064
Epoch 930, training loss: 6.084582328796387 = 0.04299641028046608 + 1.0 * 6.041585922241211
Epoch 930, val loss: 0.7126747965812683
Epoch 940, training loss: 6.08554220199585 = 0.04134644567966461 + 1.0 * 6.044195652008057
Epoch 940, val loss: 0.715032160282135
Epoch 950, training loss: 6.079512596130371 = 0.039801232516765594 + 1.0 * 6.0397114753723145
Epoch 950, val loss: 0.7175278663635254
Epoch 960, training loss: 6.077321529388428 = 0.03834116458892822 + 1.0 * 6.038980484008789
Epoch 960, val loss: 0.7200480103492737
Epoch 970, training loss: 6.077054977416992 = 0.03694860637187958 + 1.0 * 6.040106296539307
Epoch 970, val loss: 0.7225085496902466
Epoch 980, training loss: 6.072456359863281 = 0.03562874346971512 + 1.0 * 6.036827564239502
Epoch 980, val loss: 0.7248742580413818
Epoch 990, training loss: 6.071905612945557 = 0.034383583813905716 + 1.0 * 6.037521839141846
Epoch 990, val loss: 0.7273861169815063
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6310
Flip ASR: 0.5911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320457458496094 = 1.9467086791992188 + 1.0 * 8.373748779296875
Epoch 0, val loss: 1.9516929388046265
Epoch 10, training loss: 10.309731483459473 = 1.9366302490234375 + 1.0 * 8.373101234436035
Epoch 10, val loss: 1.9414514303207397
Epoch 20, training loss: 10.293378829956055 = 1.9240503311157227 + 1.0 * 8.369328498840332
Epoch 20, val loss: 1.928192138671875
Epoch 30, training loss: 10.252326965332031 = 1.9065827131271362 + 1.0 * 8.345744132995605
Epoch 30, val loss: 1.909745693206787
Epoch 40, training loss: 10.06911563873291 = 1.885316252708435 + 1.0 * 8.183799743652344
Epoch 40, val loss: 1.888340711593628
Epoch 50, training loss: 9.369134902954102 = 1.863053798675537 + 1.0 * 7.506080627441406
Epoch 50, val loss: 1.8659611940383911
Epoch 60, training loss: 8.917736053466797 = 1.845317006111145 + 1.0 * 7.072418689727783
Epoch 60, val loss: 1.8494409322738647
Epoch 70, training loss: 8.650062561035156 = 1.8330864906311035 + 1.0 * 6.816976070404053
Epoch 70, val loss: 1.8374874591827393
Epoch 80, training loss: 8.492565155029297 = 1.8191455602645874 + 1.0 * 6.67341947555542
Epoch 80, val loss: 1.8232930898666382
Epoch 90, training loss: 8.373184204101562 = 1.8053979873657227 + 1.0 * 6.567786693572998
Epoch 90, val loss: 1.8098160028457642
Epoch 100, training loss: 8.27568531036377 = 1.793493628501892 + 1.0 * 6.482192039489746
Epoch 100, val loss: 1.7985777854919434
Epoch 110, training loss: 8.202829360961914 = 1.7829557657241821 + 1.0 * 6.419873237609863
Epoch 110, val loss: 1.788407802581787
Epoch 120, training loss: 8.14673900604248 = 1.7714197635650635 + 1.0 * 6.375319004058838
Epoch 120, val loss: 1.777133584022522
Epoch 130, training loss: 8.098551750183105 = 1.7582660913467407 + 1.0 * 6.340285778045654
Epoch 130, val loss: 1.76471745967865
Epoch 140, training loss: 8.055824279785156 = 1.7431045770645142 + 1.0 * 6.312719821929932
Epoch 140, val loss: 1.7511130571365356
Epoch 150, training loss: 8.016353607177734 = 1.7251996994018555 + 1.0 * 6.291154384613037
Epoch 150, val loss: 1.7357875108718872
Epoch 160, training loss: 7.973345756530762 = 1.7040002346038818 + 1.0 * 6.269345760345459
Epoch 160, val loss: 1.7181059122085571
Epoch 170, training loss: 7.929370403289795 = 1.6782450675964355 + 1.0 * 6.251125335693359
Epoch 170, val loss: 1.6969799995422363
Epoch 180, training loss: 7.8842291831970215 = 1.6465239524841309 + 1.0 * 6.237705230712891
Epoch 180, val loss: 1.6713063716888428
Epoch 190, training loss: 7.83205509185791 = 1.6080266237258911 + 1.0 * 6.224028587341309
Epoch 190, val loss: 1.6402809619903564
Epoch 200, training loss: 7.774892807006836 = 1.5612447261810303 + 1.0 * 6.213648319244385
Epoch 200, val loss: 1.6027425527572632
Epoch 210, training loss: 7.712837219238281 = 1.505206823348999 + 1.0 * 6.207630634307861
Epoch 210, val loss: 1.5578902959823608
Epoch 220, training loss: 7.6409101486206055 = 1.441437005996704 + 1.0 * 6.199472904205322
Epoch 220, val loss: 1.5068613290786743
Epoch 230, training loss: 7.5642290115356445 = 1.3711713552474976 + 1.0 * 6.193057537078857
Epoch 230, val loss: 1.4510248899459839
Epoch 240, training loss: 7.486181735992432 = 1.2975152730941772 + 1.0 * 6.188666343688965
Epoch 240, val loss: 1.39302659034729
Epoch 250, training loss: 7.407528877258301 = 1.2242337465286255 + 1.0 * 6.183295249938965
Epoch 250, val loss: 1.3354096412658691
Epoch 260, training loss: 7.331547260284424 = 1.153040885925293 + 1.0 * 6.178506374359131
Epoch 260, val loss: 1.2795263528823853
Epoch 270, training loss: 7.2588958740234375 = 1.0860586166381836 + 1.0 * 6.172837257385254
Epoch 270, val loss: 1.227118730545044
Epoch 280, training loss: 7.191455841064453 = 1.023328185081482 + 1.0 * 6.168127536773682
Epoch 280, val loss: 1.178070068359375
Epoch 290, training loss: 7.1322431564331055 = 0.9650452136993408 + 1.0 * 6.1671977043151855
Epoch 290, val loss: 1.1328277587890625
Epoch 300, training loss: 7.070887565612793 = 0.9119246006011963 + 1.0 * 6.158962726593018
Epoch 300, val loss: 1.0919644832611084
Epoch 310, training loss: 7.0148444175720215 = 0.8621845841407776 + 1.0 * 6.152659893035889
Epoch 310, val loss: 1.0542595386505127
Epoch 320, training loss: 6.9738569259643555 = 0.8152551054954529 + 1.0 * 6.158601760864258
Epoch 320, val loss: 1.0191293954849243
Epoch 330, training loss: 6.920560836791992 = 0.77198725938797 + 1.0 * 6.148573398590088
Epoch 330, val loss: 0.9875138998031616
Epoch 340, training loss: 6.872496128082275 = 0.7316817045211792 + 1.0 * 6.140814304351807
Epoch 340, val loss: 0.9590346813201904
Epoch 350, training loss: 6.831340789794922 = 0.6938216686248779 + 1.0 * 6.137518882751465
Epoch 350, val loss: 0.9331398606300354
Epoch 360, training loss: 6.7955498695373535 = 0.6585098505020142 + 1.0 * 6.137040138244629
Epoch 360, val loss: 0.9102236032485962
Epoch 370, training loss: 6.757386207580566 = 0.6255228519439697 + 1.0 * 6.131863594055176
Epoch 370, val loss: 0.8903839588165283
Epoch 380, training loss: 6.721607685089111 = 0.594098687171936 + 1.0 * 6.127509117126465
Epoch 380, val loss: 0.8727561235427856
Epoch 390, training loss: 6.691102504730225 = 0.5639669299125671 + 1.0 * 6.127135753631592
Epoch 390, val loss: 0.8570306897163391
Epoch 400, training loss: 6.656574249267578 = 0.5353281497955322 + 1.0 * 6.121246337890625
Epoch 400, val loss: 0.843532145023346
Epoch 410, training loss: 6.628493785858154 = 0.507749080657959 + 1.0 * 6.120744705200195
Epoch 410, val loss: 0.831703245639801
Epoch 420, training loss: 6.600878715515137 = 0.4814111590385437 + 1.0 * 6.119467735290527
Epoch 420, val loss: 0.8214289546012878
Epoch 430, training loss: 6.5702433586120605 = 0.45620840787887573 + 1.0 * 6.114035129547119
Epoch 430, val loss: 0.8127893805503845
Epoch 440, training loss: 6.544353485107422 = 0.4318610429763794 + 1.0 * 6.112492561340332
Epoch 440, val loss: 0.8052037358283997
Epoch 450, training loss: 6.526658058166504 = 0.40859952569007874 + 1.0 * 6.118058681488037
Epoch 450, val loss: 0.7987633347511292
Epoch 460, training loss: 6.4939985275268555 = 0.3864627778530121 + 1.0 * 6.1075358390808105
Epoch 460, val loss: 0.7938053011894226
Epoch 470, training loss: 6.470616340637207 = 0.3650902211666107 + 1.0 * 6.105525970458984
Epoch 470, val loss: 0.7896093130111694
Epoch 480, training loss: 6.447135925292969 = 0.34439191222190857 + 1.0 * 6.102744102478027
Epoch 480, val loss: 0.7863040566444397
Epoch 490, training loss: 6.428809642791748 = 0.32447102665901184 + 1.0 * 6.104338645935059
Epoch 490, val loss: 0.7838477492332458
Epoch 500, training loss: 6.405641555786133 = 0.3055240213871002 + 1.0 * 6.1001176834106445
Epoch 500, val loss: 0.7824310064315796
Epoch 510, training loss: 6.385214328765869 = 0.2873769700527191 + 1.0 * 6.097837448120117
Epoch 510, val loss: 0.7817395925521851
Epoch 520, training loss: 6.3798675537109375 = 0.2700357735157013 + 1.0 * 6.109831809997559
Epoch 520, val loss: 0.7816390991210938
Epoch 530, training loss: 6.355108737945557 = 0.2538321614265442 + 1.0 * 6.101276397705078
Epoch 530, val loss: 0.7821273803710938
Epoch 540, training loss: 6.331537246704102 = 0.23864439129829407 + 1.0 * 6.092892646789551
Epoch 540, val loss: 0.7835125923156738
Epoch 550, training loss: 6.3162031173706055 = 0.22433407604694366 + 1.0 * 6.091868877410889
Epoch 550, val loss: 0.7852927446365356
Epoch 560, training loss: 6.300693988800049 = 0.21088945865631104 + 1.0 * 6.089804649353027
Epoch 560, val loss: 0.7877184152603149
Epoch 570, training loss: 6.291811943054199 = 0.19840990006923676 + 1.0 * 6.093401908874512
Epoch 570, val loss: 0.7904554605484009
Epoch 580, training loss: 6.278802394866943 = 0.1869884878396988 + 1.0 * 6.091814041137695
Epoch 580, val loss: 0.7939360737800598
Epoch 590, training loss: 6.261579513549805 = 0.17635951936244965 + 1.0 * 6.085219860076904
Epoch 590, val loss: 0.7978448867797852
Epoch 600, training loss: 6.250749111175537 = 0.16642996668815613 + 1.0 * 6.084319114685059
Epoch 600, val loss: 0.8020264506340027
Epoch 610, training loss: 6.247403621673584 = 0.15715885162353516 + 1.0 * 6.090244770050049
Epoch 610, val loss: 0.8066117763519287
Epoch 620, training loss: 6.230077266693115 = 0.14864054322242737 + 1.0 * 6.081436634063721
Epoch 620, val loss: 0.8111422061920166
Epoch 630, training loss: 6.221652507781982 = 0.14076408743858337 + 1.0 * 6.080888271331787
Epoch 630, val loss: 0.8162792325019836
Epoch 640, training loss: 6.212481498718262 = 0.13340358436107635 + 1.0 * 6.07907772064209
Epoch 640, val loss: 0.8215067386627197
Epoch 650, training loss: 6.204201698303223 = 0.1265103965997696 + 1.0 * 6.077691078186035
Epoch 650, val loss: 0.8268975019454956
Epoch 660, training loss: 6.198736667633057 = 0.1200772076845169 + 1.0 * 6.078659534454346
Epoch 660, val loss: 0.8323652744293213
Epoch 670, training loss: 6.192802429199219 = 0.11410015821456909 + 1.0 * 6.078702449798584
Epoch 670, val loss: 0.8380419611930847
Epoch 680, training loss: 6.18212366104126 = 0.1085314154624939 + 1.0 * 6.073592185974121
Epoch 680, val loss: 0.8437830805778503
Epoch 690, training loss: 6.175578594207764 = 0.10332299768924713 + 1.0 * 6.072255611419678
Epoch 690, val loss: 0.8497047424316406
Epoch 700, training loss: 6.170179843902588 = 0.09842436015605927 + 1.0 * 6.071755409240723
Epoch 700, val loss: 0.8556857109069824
Epoch 710, training loss: 6.1651787757873535 = 0.09381513297557831 + 1.0 * 6.07136344909668
Epoch 710, val loss: 0.8617005348205566
Epoch 720, training loss: 6.1598310470581055 = 0.08949287235736847 + 1.0 * 6.070338249206543
Epoch 720, val loss: 0.8676570057868958
Epoch 730, training loss: 6.15591287612915 = 0.0854484960436821 + 1.0 * 6.070464611053467
Epoch 730, val loss: 0.8737765550613403
Epoch 740, training loss: 6.148245811462402 = 0.08164593577384949 + 1.0 * 6.0665998458862305
Epoch 740, val loss: 0.8799678087234497
Epoch 750, training loss: 6.145071029663086 = 0.07805031538009644 + 1.0 * 6.067020893096924
Epoch 750, val loss: 0.886169970035553
Epoch 760, training loss: 6.140159606933594 = 0.07466024160385132 + 1.0 * 6.065499305725098
Epoch 760, val loss: 0.8922392129898071
Epoch 770, training loss: 6.135027885437012 = 0.07147157937288284 + 1.0 * 6.06355619430542
Epoch 770, val loss: 0.898510217666626
Epoch 780, training loss: 6.144432067871094 = 0.06845950335264206 + 1.0 * 6.075972557067871
Epoch 780, val loss: 0.9046843647956848
Epoch 790, training loss: 6.128394603729248 = 0.0656297504901886 + 1.0 * 6.062764644622803
Epoch 790, val loss: 0.9107300043106079
Epoch 800, training loss: 6.124525547027588 = 0.06296027451753616 + 1.0 * 6.061565399169922
Epoch 800, val loss: 0.9169859886169434
Epoch 810, training loss: 6.121517658233643 = 0.06042058393359184 + 1.0 * 6.061097145080566
Epoch 810, val loss: 0.9231523275375366
Epoch 820, training loss: 6.117681980133057 = 0.05801279842853546 + 1.0 * 6.059669017791748
Epoch 820, val loss: 0.9292051196098328
Epoch 830, training loss: 6.115675926208496 = 0.05573825538158417 + 1.0 * 6.059937477111816
Epoch 830, val loss: 0.9354221224784851
Epoch 840, training loss: 6.111978530883789 = 0.05357944220304489 + 1.0 * 6.058399200439453
Epoch 840, val loss: 0.9415465593338013
Epoch 850, training loss: 6.108821392059326 = 0.051531750708818436 + 1.0 * 6.0572896003723145
Epoch 850, val loss: 0.9476693868637085
Epoch 860, training loss: 6.104931354522705 = 0.049588143825531006 + 1.0 * 6.055343151092529
Epoch 860, val loss: 0.9536470770835876
Epoch 870, training loss: 6.1037702560424805 = 0.04775118827819824 + 1.0 * 6.056019306182861
Epoch 870, val loss: 0.959691047668457
Epoch 880, training loss: 6.100083351135254 = 0.0460037998855114 + 1.0 * 6.054079532623291
Epoch 880, val loss: 0.9657824635505676
Epoch 890, training loss: 6.099632740020752 = 0.04433933272957802 + 1.0 * 6.055293560028076
Epoch 890, val loss: 0.9716944694519043
Epoch 900, training loss: 6.0998406410217285 = 0.04275727644562721 + 1.0 * 6.0570831298828125
Epoch 900, val loss: 0.9775581359863281
Epoch 910, training loss: 6.094446182250977 = 0.041260018944740295 + 1.0 * 6.053185939788818
Epoch 910, val loss: 0.9833003282546997
Epoch 920, training loss: 6.090061187744141 = 0.03983962535858154 + 1.0 * 6.0502214431762695
Epoch 920, val loss: 0.9891229867935181
Epoch 930, training loss: 6.088304042816162 = 0.0384843610227108 + 1.0 * 6.049819469451904
Epoch 930, val loss: 0.9949024319648743
Epoch 940, training loss: 6.091545104980469 = 0.03718670830130577 + 1.0 * 6.05435848236084
Epoch 940, val loss: 1.0005310773849487
Epoch 950, training loss: 6.092941761016846 = 0.03595493361353874 + 1.0 * 6.0569868087768555
Epoch 950, val loss: 1.0060369968414307
Epoch 960, training loss: 6.082293510437012 = 0.03478866070508957 + 1.0 * 6.047504901885986
Epoch 960, val loss: 1.0115997791290283
Epoch 970, training loss: 6.079154968261719 = 0.03367273509502411 + 1.0 * 6.045482158660889
Epoch 970, val loss: 1.0171167850494385
Epoch 980, training loss: 6.078892230987549 = 0.03260216861963272 + 1.0 * 6.046289920806885
Epoch 980, val loss: 1.0225509405136108
Epoch 990, training loss: 6.079704284667969 = 0.03158115968108177 + 1.0 * 6.048123359680176
Epoch 990, val loss: 1.027729868888855
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8044
Flip ASR: 0.7644/225 nodes
The final ASR:0.70849, 0.07200, Accuracy:0.81235, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10540])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.82840, 0.00175
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.329290390014648 = 1.9556142091751099 + 1.0 * 8.373676300048828
Epoch 0, val loss: 1.9498628377914429
Epoch 10, training loss: 10.318063735961914 = 1.945177674293518 + 1.0 * 8.372885704040527
Epoch 10, val loss: 1.939966082572937
Epoch 20, training loss: 10.300324440002441 = 1.9322713613510132 + 1.0 * 8.368053436279297
Epoch 20, val loss: 1.9271482229232788
Epoch 30, training loss: 10.250202178955078 = 1.9142417907714844 + 1.0 * 8.335960388183594
Epoch 30, val loss: 1.9089857339859009
Epoch 40, training loss: 9.984708786010742 = 1.8924928903579712 + 1.0 * 8.092215538024902
Epoch 40, val loss: 1.8877400159835815
Epoch 50, training loss: 9.131738662719727 = 1.8692387342453003 + 1.0 * 7.262499809265137
Epoch 50, val loss: 1.8649084568023682
Epoch 60, training loss: 8.767770767211914 = 1.8517683744430542 + 1.0 * 6.9160027503967285
Epoch 60, val loss: 1.8495471477508545
Epoch 70, training loss: 8.562126159667969 = 1.8375883102416992 + 1.0 * 6.724537372589111
Epoch 70, val loss: 1.8361566066741943
Epoch 80, training loss: 8.432621955871582 = 1.8214863538742065 + 1.0 * 6.611135959625244
Epoch 80, val loss: 1.821406602859497
Epoch 90, training loss: 8.346620559692383 = 1.8063806295394897 + 1.0 * 6.540240287780762
Epoch 90, val loss: 1.8079867362976074
Epoch 100, training loss: 8.263687133789062 = 1.79366934299469 + 1.0 * 6.470017910003662
Epoch 100, val loss: 1.797086477279663
Epoch 110, training loss: 8.193731307983398 = 1.7829102277755737 + 1.0 * 6.410820960998535
Epoch 110, val loss: 1.7877733707427979
Epoch 120, training loss: 8.136690139770508 = 1.7720741033554077 + 1.0 * 6.364616394042969
Epoch 120, val loss: 1.7781282663345337
Epoch 130, training loss: 8.088946342468262 = 1.7600011825561523 + 1.0 * 6.328945159912109
Epoch 130, val loss: 1.767418622970581
Epoch 140, training loss: 8.04808235168457 = 1.746039867401123 + 1.0 * 6.3020429611206055
Epoch 140, val loss: 1.7554049491882324
Epoch 150, training loss: 8.00786018371582 = 1.7296195030212402 + 1.0 * 6.278240203857422
Epoch 150, val loss: 1.7416483163833618
Epoch 160, training loss: 7.967944145202637 = 1.7098848819732666 + 1.0 * 6.258059501647949
Epoch 160, val loss: 1.725541591644287
Epoch 170, training loss: 7.926937580108643 = 1.68595552444458 + 1.0 * 6.2409820556640625
Epoch 170, val loss: 1.706118106842041
Epoch 180, training loss: 7.883234024047852 = 1.6568862199783325 + 1.0 * 6.226347923278809
Epoch 180, val loss: 1.6825820207595825
Epoch 190, training loss: 7.835275650024414 = 1.6214157342910767 + 1.0 * 6.213860034942627
Epoch 190, val loss: 1.6537888050079346
Epoch 200, training loss: 7.783761024475098 = 1.5782502889633179 + 1.0 * 6.20551061630249
Epoch 200, val loss: 1.618809461593628
Epoch 210, training loss: 7.722870826721191 = 1.5274935960769653 + 1.0 * 6.195377349853516
Epoch 210, val loss: 1.5777106285095215
Epoch 220, training loss: 7.656839847564697 = 1.469101071357727 + 1.0 * 6.18773889541626
Epoch 220, val loss: 1.5304672718048096
Epoch 230, training loss: 7.585713863372803 = 1.404390811920166 + 1.0 * 6.181323051452637
Epoch 230, val loss: 1.478314757347107
Epoch 240, training loss: 7.513627529144287 = 1.3365378379821777 + 1.0 * 6.177089691162109
Epoch 240, val loss: 1.4240977764129639
Epoch 250, training loss: 7.442183494567871 = 1.2704076766967773 + 1.0 * 6.171775817871094
Epoch 250, val loss: 1.3715338706970215
Epoch 260, training loss: 7.37206506729126 = 1.2066720724105835 + 1.0 * 6.165392875671387
Epoch 260, val loss: 1.3209644556045532
Epoch 270, training loss: 7.307626724243164 = 1.1472336053848267 + 1.0 * 6.160393238067627
Epoch 270, val loss: 1.274134635925293
Epoch 280, training loss: 7.248027324676514 = 1.0930341482162476 + 1.0 * 6.154993057250977
Epoch 280, val loss: 1.2318358421325684
Epoch 290, training loss: 7.19281005859375 = 1.04308021068573 + 1.0 * 6.1497297286987305
Epoch 290, val loss: 1.1931171417236328
Epoch 300, training loss: 7.1418070793151855 = 0.9966250658035278 + 1.0 * 6.145182132720947
Epoch 300, val loss: 1.1575629711151123
Epoch 310, training loss: 7.094031810760498 = 0.9528130292892456 + 1.0 * 6.141218662261963
Epoch 310, val loss: 1.1245136260986328
Epoch 320, training loss: 7.047630310058594 = 0.9106762409210205 + 1.0 * 6.136954307556152
Epoch 320, val loss: 1.0932447910308838
Epoch 330, training loss: 7.003318786621094 = 0.8697282075881958 + 1.0 * 6.1335906982421875
Epoch 330, val loss: 1.0632519721984863
Epoch 340, training loss: 6.958406448364258 = 0.8294273018836975 + 1.0 * 6.128979206085205
Epoch 340, val loss: 1.0343246459960938
Epoch 350, training loss: 6.9159159660339355 = 0.7891983389854431 + 1.0 * 6.126717567443848
Epoch 350, val loss: 1.0056216716766357
Epoch 360, training loss: 6.874683380126953 = 0.7493814826011658 + 1.0 * 6.125301837921143
Epoch 360, val loss: 0.9774692058563232
Epoch 370, training loss: 6.831758975982666 = 0.710658848285675 + 1.0 * 6.121099948883057
Epoch 370, val loss: 0.9505090713500977
Epoch 380, training loss: 6.790757179260254 = 0.6732460260391235 + 1.0 * 6.11751127243042
Epoch 380, val loss: 0.9250777959823608
Epoch 390, training loss: 6.759641647338867 = 0.637177050113678 + 1.0 * 6.122464656829834
Epoch 390, val loss: 0.9010728597640991
Epoch 400, training loss: 6.719077110290527 = 0.6034225821495056 + 1.0 * 6.115654468536377
Epoch 400, val loss: 0.8793742060661316
Epoch 410, training loss: 6.681499481201172 = 0.5717552900314331 + 1.0 * 6.109744071960449
Epoch 410, val loss: 0.8600601553916931
Epoch 420, training loss: 6.655581951141357 = 0.5419377088546753 + 1.0 * 6.113644123077393
Epoch 420, val loss: 0.8429189920425415
Epoch 430, training loss: 6.619359493255615 = 0.5140748023986816 + 1.0 * 6.105284690856934
Epoch 430, val loss: 0.8281452059745789
Epoch 440, training loss: 6.591968536376953 = 0.4877047836780548 + 1.0 * 6.104263782501221
Epoch 440, val loss: 0.8154628872871399
Epoch 450, training loss: 6.56531286239624 = 0.4626311659812927 + 1.0 * 6.102681636810303
Epoch 450, val loss: 0.8044524192810059
Epoch 460, training loss: 6.537820339202881 = 0.43862688541412354 + 1.0 * 6.099193572998047
Epoch 460, val loss: 0.7950671315193176
Epoch 470, training loss: 6.511843681335449 = 0.41550353169441223 + 1.0 * 6.096340179443359
Epoch 470, val loss: 0.7868819236755371
Epoch 480, training loss: 6.488686561584473 = 0.3931707441806793 + 1.0 * 6.095515727996826
Epoch 480, val loss: 0.7799929976463318
Epoch 490, training loss: 6.465350151062012 = 0.37149935960769653 + 1.0 * 6.093850612640381
Epoch 490, val loss: 0.7739201784133911
Epoch 500, training loss: 6.441459655761719 = 0.3504806160926819 + 1.0 * 6.090979099273682
Epoch 500, val loss: 0.7688992023468018
Epoch 510, training loss: 6.425092697143555 = 0.33003050088882446 + 1.0 * 6.095062255859375
Epoch 510, val loss: 0.7646204829216003
Epoch 520, training loss: 6.401928424835205 = 0.31030359864234924 + 1.0 * 6.091624736785889
Epoch 520, val loss: 0.7610501050949097
Epoch 530, training loss: 6.377062797546387 = 0.2912234961986542 + 1.0 * 6.08583927154541
Epoch 530, val loss: 0.7580373287200928
Epoch 540, training loss: 6.356039047241211 = 0.2726631164550781 + 1.0 * 6.083375930786133
Epoch 540, val loss: 0.7555678486824036
Epoch 550, training loss: 6.35060453414917 = 0.2546599805355072 + 1.0 * 6.095944404602051
Epoch 550, val loss: 0.7536460161209106
Epoch 560, training loss: 6.321496963500977 = 0.23751811683177948 + 1.0 * 6.083978652954102
Epoch 560, val loss: 0.7521494030952454
Epoch 570, training loss: 6.301666736602783 = 0.22118966281414032 + 1.0 * 6.080477237701416
Epoch 570, val loss: 0.751329243183136
Epoch 580, training loss: 6.286316871643066 = 0.20567522943019867 + 1.0 * 6.080641746520996
Epoch 580, val loss: 0.7509238123893738
Epoch 590, training loss: 6.26950740814209 = 0.19110913574695587 + 1.0 * 6.07839822769165
Epoch 590, val loss: 0.7511518597602844
Epoch 600, training loss: 6.2560038566589355 = 0.17759692668914795 + 1.0 * 6.078406810760498
Epoch 600, val loss: 0.7519910335540771
Epoch 610, training loss: 6.24049186706543 = 0.1651563048362732 + 1.0 * 6.075335502624512
Epoch 610, val loss: 0.7533833384513855
Epoch 620, training loss: 6.23018217086792 = 0.1537037491798401 + 1.0 * 6.076478481292725
Epoch 620, val loss: 0.7553508281707764
Epoch 630, training loss: 6.215468406677246 = 0.14325451850891113 + 1.0 * 6.072214126586914
Epoch 630, val loss: 0.7579323649406433
Epoch 640, training loss: 6.204065322875977 = 0.1337140053510666 + 1.0 * 6.0703511238098145
Epoch 640, val loss: 0.7610801458358765
Epoch 650, training loss: 6.197332382202148 = 0.12500086426734924 + 1.0 * 6.072331428527832
Epoch 650, val loss: 0.764747142791748
Epoch 660, training loss: 6.1878509521484375 = 0.1171131357550621 + 1.0 * 6.070737838745117
Epoch 660, val loss: 0.7688156962394714
Epoch 670, training loss: 6.176753044128418 = 0.1099519282579422 + 1.0 * 6.066801071166992
Epoch 670, val loss: 0.7733411192893982
Epoch 680, training loss: 6.174449920654297 = 0.10340724140405655 + 1.0 * 6.071042537689209
Epoch 680, val loss: 0.7781072854995728
Epoch 690, training loss: 6.163287162780762 = 0.09744248539209366 + 1.0 * 6.065844535827637
Epoch 690, val loss: 0.783202588558197
Epoch 700, training loss: 6.156608581542969 = 0.09197990596294403 + 1.0 * 6.064628601074219
Epoch 700, val loss: 0.788452684879303
Epoch 710, training loss: 6.148202419281006 = 0.08695973455905914 + 1.0 * 6.061242580413818
Epoch 710, val loss: 0.7939831614494324
Epoch 720, training loss: 6.143111705780029 = 0.08232156187295914 + 1.0 * 6.060790061950684
Epoch 720, val loss: 0.7996463775634766
Epoch 730, training loss: 6.143460750579834 = 0.07803831249475479 + 1.0 * 6.065422534942627
Epoch 730, val loss: 0.8053750991821289
Epoch 740, training loss: 6.135424613952637 = 0.07410212606191635 + 1.0 * 6.0613226890563965
Epoch 740, val loss: 0.8112362623214722
Epoch 750, training loss: 6.129518032073975 = 0.07047680765390396 + 1.0 * 6.0590410232543945
Epoch 750, val loss: 0.8171035051345825
Epoch 760, training loss: 6.123141765594482 = 0.06711708009243011 + 1.0 * 6.056024551391602
Epoch 760, val loss: 0.823012113571167
Epoch 770, training loss: 6.1218976974487305 = 0.06399112194776535 + 1.0 * 6.057906627655029
Epoch 770, val loss: 0.8289386034011841
Epoch 780, training loss: 6.114574909210205 = 0.06108417361974716 + 1.0 * 6.05349063873291
Epoch 780, val loss: 0.8349127769470215
Epoch 790, training loss: 6.111905097961426 = 0.05836452543735504 + 1.0 * 6.0535407066345215
Epoch 790, val loss: 0.8409069180488586
Epoch 800, training loss: 6.112706661224365 = 0.05582321062684059 + 1.0 * 6.056883335113525
Epoch 800, val loss: 0.8469114899635315
Epoch 810, training loss: 6.106783390045166 = 0.053456589579582214 + 1.0 * 6.053326606750488
Epoch 810, val loss: 0.8527886271476746
Epoch 820, training loss: 6.102380275726318 = 0.05124569311738014 + 1.0 * 6.0511345863342285
Epoch 820, val loss: 0.8587933778762817
Epoch 830, training loss: 6.101039409637451 = 0.049164220690727234 + 1.0 * 6.051875114440918
Epoch 830, val loss: 0.8646739721298218
Epoch 840, training loss: 6.096371650695801 = 0.04721122980117798 + 1.0 * 6.049160480499268
Epoch 840, val loss: 0.8706316947937012
Epoch 850, training loss: 6.098625183105469 = 0.04537511616945267 + 1.0 * 6.053249835968018
Epoch 850, val loss: 0.8764896988868713
Epoch 860, training loss: 6.093623161315918 = 0.043655380606651306 + 1.0 * 6.0499677658081055
Epoch 860, val loss: 0.8821864724159241
Epoch 870, training loss: 6.0877485275268555 = 0.04203549772500992 + 1.0 * 6.045712947845459
Epoch 870, val loss: 0.8880295753479004
Epoch 880, training loss: 6.08526611328125 = 0.04049862176179886 + 1.0 * 6.044767379760742
Epoch 880, val loss: 0.8936721086502075
Epoch 890, training loss: 6.08665657043457 = 0.03903820365667343 + 1.0 * 6.047618389129639
Epoch 890, val loss: 0.8993682265281677
Epoch 900, training loss: 6.082009315490723 = 0.0376591756939888 + 1.0 * 6.0443501472473145
Epoch 900, val loss: 0.904966413974762
Epoch 910, training loss: 6.080042362213135 = 0.03635543957352638 + 1.0 * 6.043686866760254
Epoch 910, val loss: 0.9105619192123413
Epoch 920, training loss: 6.08004903793335 = 0.03511890023946762 + 1.0 * 6.0449299812316895
Epoch 920, val loss: 0.9160557985305786
Epoch 930, training loss: 6.07661771774292 = 0.03394747152924538 + 1.0 * 6.042670249938965
Epoch 930, val loss: 0.921612024307251
Epoch 940, training loss: 6.078830718994141 = 0.03283027559518814 + 1.0 * 6.0460004806518555
Epoch 940, val loss: 0.9270287752151489
Epoch 950, training loss: 6.072324752807617 = 0.0317707434296608 + 1.0 * 6.040554046630859
Epoch 950, val loss: 0.932397723197937
Epoch 960, training loss: 6.069412708282471 = 0.03076038509607315 + 1.0 * 6.038652420043945
Epoch 960, val loss: 0.9377880096435547
Epoch 970, training loss: 6.069174289703369 = 0.029790330678224564 + 1.0 * 6.039383888244629
Epoch 970, val loss: 0.9431425333023071
Epoch 980, training loss: 6.066870212554932 = 0.028870796784758568 + 1.0 * 6.037999629974365
Epoch 980, val loss: 0.9483416676521301
Epoch 990, training loss: 6.067619323730469 = 0.028003890067338943 + 1.0 * 6.039615631103516
Epoch 990, val loss: 0.9536333084106445
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7122
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.311620712280273 = 1.9378273487091064 + 1.0 * 8.373793601989746
Epoch 0, val loss: 1.932041883468628
Epoch 10, training loss: 10.301169395446777 = 1.9279578924179077 + 1.0 * 8.373211860656738
Epoch 10, val loss: 1.9224112033843994
Epoch 20, training loss: 10.285433769226074 = 1.9156221151351929 + 1.0 * 8.36981201171875
Epoch 20, val loss: 1.9099676609039307
Epoch 30, training loss: 10.243633270263672 = 1.8987044095993042 + 1.0 * 8.344928741455078
Epoch 30, val loss: 1.8925985097885132
Epoch 40, training loss: 9.974977493286133 = 1.8781687021255493 + 1.0 * 8.096808433532715
Epoch 40, val loss: 1.8722106218338013
Epoch 50, training loss: 9.044898986816406 = 1.8579970598220825 + 1.0 * 7.186902046203613
Epoch 50, val loss: 1.8527002334594727
Epoch 60, training loss: 8.698527336120605 = 1.8452658653259277 + 1.0 * 6.853261470794678
Epoch 60, val loss: 1.8400288820266724
Epoch 70, training loss: 8.495838165283203 = 1.8343687057495117 + 1.0 * 6.661469459533691
Epoch 70, val loss: 1.8288602828979492
Epoch 80, training loss: 8.366859436035156 = 1.823554277420044 + 1.0 * 6.543304920196533
Epoch 80, val loss: 1.8178995847702026
Epoch 90, training loss: 8.27366828918457 = 1.8122360706329346 + 1.0 * 6.461431980133057
Epoch 90, val loss: 1.8067337274551392
Epoch 100, training loss: 8.201521873474121 = 1.8013241291046143 + 1.0 * 6.400197982788086
Epoch 100, val loss: 1.7958621978759766
Epoch 110, training loss: 8.142705917358398 = 1.790406346321106 + 1.0 * 6.352299690246582
Epoch 110, val loss: 1.7848612070083618
Epoch 120, training loss: 8.091042518615723 = 1.7793118953704834 + 1.0 * 6.31173038482666
Epoch 120, val loss: 1.7736561298370361
Epoch 130, training loss: 8.047161102294922 = 1.7673463821411133 + 1.0 * 6.279815196990967
Epoch 130, val loss: 1.7618709802627563
Epoch 140, training loss: 8.007237434387207 = 1.7537568807601929 + 1.0 * 6.253480434417725
Epoch 140, val loss: 1.7489980459213257
Epoch 150, training loss: 7.969321250915527 = 1.7378486394882202 + 1.0 * 6.231472492218018
Epoch 150, val loss: 1.7345691919326782
Epoch 160, training loss: 7.933229923248291 = 1.7189178466796875 + 1.0 * 6.2143120765686035
Epoch 160, val loss: 1.7180347442626953
Epoch 170, training loss: 7.895012378692627 = 1.6959795951843262 + 1.0 * 6.199032783508301
Epoch 170, val loss: 1.6984426975250244
Epoch 180, training loss: 7.856283187866211 = 1.667815089225769 + 1.0 * 6.188467979431152
Epoch 180, val loss: 1.674910068511963
Epoch 190, training loss: 7.813368797302246 = 1.633702039718628 + 1.0 * 6.179666996002197
Epoch 190, val loss: 1.646848201751709
Epoch 200, training loss: 7.76296329498291 = 1.5932177305221558 + 1.0 * 6.169745445251465
Epoch 200, val loss: 1.6137043237686157
Epoch 210, training loss: 7.708130836486816 = 1.5453765392303467 + 1.0 * 6.162754535675049
Epoch 210, val loss: 1.5747097730636597
Epoch 220, training loss: 7.648138999938965 = 1.490123987197876 + 1.0 * 6.158015251159668
Epoch 220, val loss: 1.5299961566925049
Epoch 230, training loss: 7.583073139190674 = 1.4301003217697144 + 1.0 * 6.15297269821167
Epoch 230, val loss: 1.4821224212646484
Epoch 240, training loss: 7.518993854522705 = 1.3678823709487915 + 1.0 * 6.151111602783203
Epoch 240, val loss: 1.4329410791397095
Epoch 250, training loss: 7.450039863586426 = 1.3062241077423096 + 1.0 * 6.143815994262695
Epoch 250, val loss: 1.3849488496780396
Epoch 260, training loss: 7.384696960449219 = 1.2457380294799805 + 1.0 * 6.138958930969238
Epoch 260, val loss: 1.3384109735488892
Epoch 270, training loss: 7.323328018188477 = 1.186812400817871 + 1.0 * 6.1365156173706055
Epoch 270, val loss: 1.2939270734786987
Epoch 280, training loss: 7.260998249053955 = 1.1300129890441895 + 1.0 * 6.130985260009766
Epoch 280, val loss: 1.2515982389450073
Epoch 290, training loss: 7.20158052444458 = 1.073670744895935 + 1.0 * 6.1279096603393555
Epoch 290, val loss: 1.2098687887191772
Epoch 300, training loss: 7.142996311187744 = 1.0182288885116577 + 1.0 * 6.124767303466797
Epoch 300, val loss: 1.1690260171890259
Epoch 310, training loss: 7.083250999450684 = 0.9635007381439209 + 1.0 * 6.119750499725342
Epoch 310, val loss: 1.129268765449524
Epoch 320, training loss: 7.029205799102783 = 0.9099487662315369 + 1.0 * 6.119256973266602
Epoch 320, val loss: 1.090707778930664
Epoch 330, training loss: 6.974527359008789 = 0.859229326248169 + 1.0 * 6.115298271179199
Epoch 330, val loss: 1.0546824932098389
Epoch 340, training loss: 6.923153877258301 = 0.812064528465271 + 1.0 * 6.11108922958374
Epoch 340, val loss: 1.021963357925415
Epoch 350, training loss: 6.875666618347168 = 0.7681384682655334 + 1.0 * 6.107528209686279
Epoch 350, val loss: 0.9922980666160583
Epoch 360, training loss: 6.8358283042907715 = 0.7272781729698181 + 1.0 * 6.108550071716309
Epoch 360, val loss: 0.9656201601028442
Epoch 370, training loss: 6.798002243041992 = 0.6896598935127258 + 1.0 * 6.108342170715332
Epoch 370, val loss: 0.9418833255767822
Epoch 380, training loss: 6.755459785461426 = 0.655029296875 + 1.0 * 6.100430488586426
Epoch 380, val loss: 0.9206743240356445
Epoch 390, training loss: 6.719845771789551 = 0.6225637197494507 + 1.0 * 6.0972819328308105
Epoch 390, val loss: 0.9015174508094788
Epoch 400, training loss: 6.688025951385498 = 0.5918799042701721 + 1.0 * 6.096146106719971
Epoch 400, val loss: 0.8839868903160095
Epoch 410, training loss: 6.660885810852051 = 0.5630608201026917 + 1.0 * 6.097825050354004
Epoch 410, val loss: 0.8684192299842834
Epoch 420, training loss: 6.626713275909424 = 0.5360916256904602 + 1.0 * 6.090621471405029
Epoch 420, val loss: 0.854501485824585
Epoch 430, training loss: 6.605112075805664 = 0.5104919075965881 + 1.0 * 6.094620227813721
Epoch 430, val loss: 0.8420671820640564
Epoch 440, training loss: 6.573802947998047 = 0.4864170551300049 + 1.0 * 6.087385654449463
Epoch 440, val loss: 0.8314398527145386
Epoch 450, training loss: 6.548553466796875 = 0.4637598991394043 + 1.0 * 6.084793567657471
Epoch 450, val loss: 0.8225430250167847
Epoch 460, training loss: 6.524687767028809 = 0.4422491788864136 + 1.0 * 6.0824384689331055
Epoch 460, val loss: 0.8150524497032166
Epoch 470, training loss: 6.509892463684082 = 0.42186400294303894 + 1.0 * 6.088028430938721
Epoch 470, val loss: 0.8091251850128174
Epoch 480, training loss: 6.482200622558594 = 0.40281662344932556 + 1.0 * 6.079383850097656
Epoch 480, val loss: 0.8046906590461731
Epoch 490, training loss: 6.462834358215332 = 0.38492077589035034 + 1.0 * 6.077913761138916
Epoch 490, val loss: 0.8016999959945679
Epoch 500, training loss: 6.4447150230407715 = 0.3680088222026825 + 1.0 * 6.076706409454346
Epoch 500, val loss: 0.799591064453125
Epoch 510, training loss: 6.426986217498779 = 0.35202649235725403 + 1.0 * 6.074959754943848
Epoch 510, val loss: 0.7985799312591553
Epoch 520, training loss: 6.409440994262695 = 0.33674561977386475 + 1.0 * 6.072695255279541
Epoch 520, val loss: 0.7983072996139526
Epoch 530, training loss: 6.4002685546875 = 0.3221376836299896 + 1.0 * 6.078130722045898
Epoch 530, val loss: 0.79860520362854
Epoch 540, training loss: 6.380285263061523 = 0.30820831656455994 + 1.0 * 6.072076797485352
Epoch 540, val loss: 0.7996944189071655
Epoch 550, training loss: 6.363451957702637 = 0.29474037885665894 + 1.0 * 6.068711757659912
Epoch 550, val loss: 0.8013226985931396
Epoch 560, training loss: 6.347954750061035 = 0.28155115246772766 + 1.0 * 6.066403388977051
Epoch 560, val loss: 0.8033369183540344
Epoch 570, training loss: 6.3339762687683105 = 0.26858723163604736 + 1.0 * 6.065389156341553
Epoch 570, val loss: 0.805770218372345
Epoch 580, training loss: 6.325283527374268 = 0.255891352891922 + 1.0 * 6.069392204284668
Epoch 580, val loss: 0.808402955532074
Epoch 590, training loss: 6.311810493469238 = 0.24357345700263977 + 1.0 * 6.068236827850342
Epoch 590, val loss: 0.8114117383956909
Epoch 600, training loss: 6.294163703918457 = 0.23146361112594604 + 1.0 * 6.062700271606445
Epoch 600, val loss: 0.8145984411239624
Epoch 610, training loss: 6.2817606925964355 = 0.21959275007247925 + 1.0 * 6.062168121337891
Epoch 610, val loss: 0.8180496692657471
Epoch 620, training loss: 6.26917028427124 = 0.20798435807228088 + 1.0 * 6.061185836791992
Epoch 620, val loss: 0.8216623663902283
Epoch 630, training loss: 6.25962495803833 = 0.19676391780376434 + 1.0 * 6.06286096572876
Epoch 630, val loss: 0.8256134986877441
Epoch 640, training loss: 6.247481822967529 = 0.1859554499387741 + 1.0 * 6.061526298522949
Epoch 640, val loss: 0.8296914100646973
Epoch 650, training loss: 6.23381233215332 = 0.17554515600204468 + 1.0 * 6.058267116546631
Epoch 650, val loss: 0.8341188430786133
Epoch 660, training loss: 6.221078872680664 = 0.16552633047103882 + 1.0 * 6.0555524826049805
Epoch 660, val loss: 0.8389022350311279
Epoch 670, training loss: 6.212070465087891 = 0.15596124529838562 + 1.0 * 6.056109428405762
Epoch 670, val loss: 0.8439418077468872
Epoch 680, training loss: 6.200540065765381 = 0.14694400131702423 + 1.0 * 6.053596019744873
Epoch 680, val loss: 0.8493004441261292
Epoch 690, training loss: 6.192544460296631 = 0.13861195743083954 + 1.0 * 6.0539326667785645
Epoch 690, val loss: 0.8554056882858276
Epoch 700, training loss: 6.1859822273254395 = 0.13096430897712708 + 1.0 * 6.055017948150635
Epoch 700, val loss: 0.8618817329406738
Epoch 710, training loss: 6.181240081787109 = 0.12397591769695282 + 1.0 * 6.05726432800293
Epoch 710, val loss: 0.8684524297714233
Epoch 720, training loss: 6.167827606201172 = 0.1175704374909401 + 1.0 * 6.050257205963135
Epoch 720, val loss: 0.8756030797958374
Epoch 730, training loss: 6.160068988800049 = 0.11162076890468597 + 1.0 * 6.048448085784912
Epoch 730, val loss: 0.8828760981559753
Epoch 740, training loss: 6.153360843658447 = 0.10607064515352249 + 1.0 * 6.047290325164795
Epoch 740, val loss: 0.8902570009231567
Epoch 750, training loss: 6.162643909454346 = 0.10088767856359482 + 1.0 * 6.061756134033203
Epoch 750, val loss: 0.8977182507514954
Epoch 760, training loss: 6.1426849365234375 = 0.09609843790531158 + 1.0 * 6.046586513519287
Epoch 760, val loss: 0.905272901058197
Epoch 770, training loss: 6.136595249176025 = 0.09163457155227661 + 1.0 * 6.0449604988098145
Epoch 770, val loss: 0.9130433797836304
Epoch 780, training loss: 6.131375789642334 = 0.08743173629045486 + 1.0 * 6.043943881988525
Epoch 780, val loss: 0.9206272959709167
Epoch 790, training loss: 6.128655910491943 = 0.08347845822572708 + 1.0 * 6.045177459716797
Epoch 790, val loss: 0.9283782839775085
Epoch 800, training loss: 6.128117561340332 = 0.07978471368551254 + 1.0 * 6.048332691192627
Epoch 800, val loss: 0.9359000325202942
Epoch 810, training loss: 6.117774963378906 = 0.07632389664649963 + 1.0 * 6.0414509773254395
Epoch 810, val loss: 0.9436686635017395
Epoch 820, training loss: 6.1136932373046875 = 0.07305405288934708 + 1.0 * 6.040639400482178
Epoch 820, val loss: 0.9513742923736572
Epoch 830, training loss: 6.112881660461426 = 0.06995716691017151 + 1.0 * 6.042924404144287
Epoch 830, val loss: 0.9589704871177673
Epoch 840, training loss: 6.1080145835876465 = 0.06704379618167877 + 1.0 * 6.040970802307129
Epoch 840, val loss: 0.9665532112121582
Epoch 850, training loss: 6.103417873382568 = 0.06429643929004669 + 1.0 * 6.039121627807617
Epoch 850, val loss: 0.9741781949996948
Epoch 860, training loss: 6.101463317871094 = 0.061697762459516525 + 1.0 * 6.039765357971191
Epoch 860, val loss: 0.9818292856216431
Epoch 870, training loss: 6.095966815948486 = 0.05923343822360039 + 1.0 * 6.036733150482178
Epoch 870, val loss: 0.9893805384635925
Epoch 880, training loss: 6.09499454498291 = 0.05689183622598648 + 1.0 * 6.038102626800537
Epoch 880, val loss: 0.9969135522842407
Epoch 890, training loss: 6.0938873291015625 = 0.05468253791332245 + 1.0 * 6.0392045974731445
Epoch 890, val loss: 1.00412917137146
Epoch 900, training loss: 6.088823318481445 = 0.05260409787297249 + 1.0 * 6.036219120025635
Epoch 900, val loss: 1.0115972757339478
Epoch 910, training loss: 6.083858489990234 = 0.05063043534755707 + 1.0 * 6.033227920532227
Epoch 910, val loss: 1.0189799070358276
Epoch 920, training loss: 6.081204414367676 = 0.04874341934919357 + 1.0 * 6.032461166381836
Epoch 920, val loss: 1.0261693000793457
Epoch 930, training loss: 6.083945274353027 = 0.046944599598646164 + 1.0 * 6.03700065612793
Epoch 930, val loss: 1.03345787525177
Epoch 940, training loss: 6.081736087799072 = 0.04524482414126396 + 1.0 * 6.036491394042969
Epoch 940, val loss: 1.0403393507003784
Epoch 950, training loss: 6.077576160430908 = 0.04364040493965149 + 1.0 * 6.033935546875
Epoch 950, val loss: 1.0474802255630493
Epoch 960, training loss: 6.073262691497803 = 0.0421147421002388 + 1.0 * 6.0311479568481445
Epoch 960, val loss: 1.0544602870941162
Epoch 970, training loss: 6.069866180419922 = 0.04065432399511337 + 1.0 * 6.02921199798584
Epoch 970, val loss: 1.0613441467285156
Epoch 980, training loss: 6.070643424987793 = 0.0392582006752491 + 1.0 * 6.03138542175293
Epoch 980, val loss: 1.0681864023208618
Epoch 990, training loss: 6.069229602813721 = 0.03792938217520714 + 1.0 * 6.031300067901611
Epoch 990, val loss: 1.0749305486679077
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7593
Overall ASR: 0.7786
Flip ASR: 0.7422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.304848670959473 = 1.9310405254364014 + 1.0 * 8.373807907104492
Epoch 0, val loss: 1.9316933155059814
Epoch 10, training loss: 10.294657707214355 = 1.9213849306106567 + 1.0 * 8.373272895812988
Epoch 10, val loss: 1.9225655794143677
Epoch 20, training loss: 10.279401779174805 = 1.9092637300491333 + 1.0 * 8.370138168334961
Epoch 20, val loss: 1.9107892513275146
Epoch 30, training loss: 10.239449501037598 = 1.8923646211624146 + 1.0 * 8.347084999084473
Epoch 30, val loss: 1.8942545652389526
Epoch 40, training loss: 10.01647663116455 = 1.8707481622695923 + 1.0 * 8.14572811126709
Epoch 40, val loss: 1.8740280866622925
Epoch 50, training loss: 9.266759872436523 = 1.8481258153915405 + 1.0 * 7.418633937835693
Epoch 50, val loss: 1.854262351989746
Epoch 60, training loss: 8.803156852722168 = 1.8333150148391724 + 1.0 * 6.969841480255127
Epoch 60, val loss: 1.8414244651794434
Epoch 70, training loss: 8.545056343078613 = 1.8233346939086914 + 1.0 * 6.721721649169922
Epoch 70, val loss: 1.8313864469528198
Epoch 80, training loss: 8.405912399291992 = 1.8120546340942383 + 1.0 * 6.593858242034912
Epoch 80, val loss: 1.8208450078964233
Epoch 90, training loss: 8.290421485900879 = 1.8008854389190674 + 1.0 * 6.489535808563232
Epoch 90, val loss: 1.8110599517822266
Epoch 100, training loss: 8.211373329162598 = 1.7911113500595093 + 1.0 * 6.420262336730957
Epoch 100, val loss: 1.802232265472412
Epoch 110, training loss: 8.151847839355469 = 1.7812790870666504 + 1.0 * 6.370569229125977
Epoch 110, val loss: 1.792745590209961
Epoch 120, training loss: 8.108311653137207 = 1.7706325054168701 + 1.0 * 6.337679386138916
Epoch 120, val loss: 1.7823518514633179
Epoch 130, training loss: 8.066225051879883 = 1.7588061094284058 + 1.0 * 6.307419300079346
Epoch 130, val loss: 1.77129328250885
Epoch 140, training loss: 8.029539108276367 = 1.7451744079589844 + 1.0 * 6.284364223480225
Epoch 140, val loss: 1.759211540222168
Epoch 150, training loss: 7.993709564208984 = 1.7288342714309692 + 1.0 * 6.264875411987305
Epoch 150, val loss: 1.7453536987304688
Epoch 160, training loss: 7.959401607513428 = 1.7090808153152466 + 1.0 * 6.250320911407471
Epoch 160, val loss: 1.7292091846466064
Epoch 170, training loss: 7.919404983520508 = 1.685497522354126 + 1.0 * 6.233907222747803
Epoch 170, val loss: 1.7100456953048706
Epoch 180, training loss: 7.876572608947754 = 1.6566699743270874 + 1.0 * 6.219902515411377
Epoch 180, val loss: 1.68686842918396
Epoch 190, training loss: 7.83209228515625 = 1.6216235160827637 + 1.0 * 6.210468769073486
Epoch 190, val loss: 1.6585800647735596
Epoch 200, training loss: 7.7780632972717285 = 1.5804057121276855 + 1.0 * 6.197657585144043
Epoch 200, val loss: 1.625367522239685
Epoch 210, training loss: 7.721310615539551 = 1.5328805446624756 + 1.0 * 6.188430309295654
Epoch 210, val loss: 1.5869066715240479
Epoch 220, training loss: 7.66322660446167 = 1.479792594909668 + 1.0 * 6.183434009552002
Epoch 220, val loss: 1.5440644025802612
Epoch 230, training loss: 7.599492073059082 = 1.424299955368042 + 1.0 * 6.175192356109619
Epoch 230, val loss: 1.4996778964996338
Epoch 240, training loss: 7.536488056182861 = 1.3675061464309692 + 1.0 * 6.168982028961182
Epoch 240, val loss: 1.4547208547592163
Epoch 250, training loss: 7.473435401916504 = 1.3105212450027466 + 1.0 * 6.162914276123047
Epoch 250, val loss: 1.4101840257644653
Epoch 260, training loss: 7.413259029388428 = 1.2544575929641724 + 1.0 * 6.158801555633545
Epoch 260, val loss: 1.366900086402893
Epoch 270, training loss: 7.35394287109375 = 1.2006715536117554 + 1.0 * 6.153271198272705
Epoch 270, val loss: 1.325835943222046
Epoch 280, training loss: 7.298517227172852 = 1.148875117301941 + 1.0 * 6.149641990661621
Epoch 280, val loss: 1.2865945100784302
Epoch 290, training loss: 7.242044448852539 = 1.0994088649749756 + 1.0 * 6.142635345458984
Epoch 290, val loss: 1.2495428323745728
Epoch 300, training loss: 7.191767692565918 = 1.0522563457489014 + 1.0 * 6.1395111083984375
Epoch 300, val loss: 1.214523434638977
Epoch 310, training loss: 7.144251823425293 = 1.0077741146087646 + 1.0 * 6.136477947235107
Epoch 310, val loss: 1.1821211576461792
Epoch 320, training loss: 7.095861911773682 = 0.9658218622207642 + 1.0 * 6.130040168762207
Epoch 320, val loss: 1.1518211364746094
Epoch 330, training loss: 7.052242755889893 = 0.9253994822502136 + 1.0 * 6.126843452453613
Epoch 330, val loss: 1.1227160692214966
Epoch 340, training loss: 7.008860111236572 = 0.8858709931373596 + 1.0 * 6.122989177703857
Epoch 340, val loss: 1.0944799184799194
Epoch 350, training loss: 6.9687700271606445 = 0.8467519283294678 + 1.0 * 6.122018337249756
Epoch 350, val loss: 1.0669405460357666
Epoch 360, training loss: 6.925224781036377 = 0.807855486869812 + 1.0 * 6.117369174957275
Epoch 360, val loss: 1.0398012399673462
Epoch 370, training loss: 6.88300085067749 = 0.7687360644340515 + 1.0 * 6.114264965057373
Epoch 370, val loss: 1.012888789176941
Epoch 380, training loss: 6.845537185668945 = 0.7298072576522827 + 1.0 * 6.115729808807373
Epoch 380, val loss: 0.9865485429763794
Epoch 390, training loss: 6.801592826843262 = 0.6916114687919617 + 1.0 * 6.109981536865234
Epoch 390, val loss: 0.9616502523422241
Epoch 400, training loss: 6.75995397567749 = 0.6540409922599792 + 1.0 * 6.105913162231445
Epoch 400, val loss: 0.9375597238540649
Epoch 410, training loss: 6.720745086669922 = 0.6174256205558777 + 1.0 * 6.1033196449279785
Epoch 410, val loss: 0.9149417877197266
Epoch 420, training loss: 6.696712493896484 = 0.5822372436523438 + 1.0 * 6.114475250244141
Epoch 420, val loss: 0.894239068031311
Epoch 430, training loss: 6.654458999633789 = 0.5496244430541992 + 1.0 * 6.10483455657959
Epoch 430, val loss: 0.8764688968658447
Epoch 440, training loss: 6.618793964385986 = 0.519378662109375 + 1.0 * 6.099415302276611
Epoch 440, val loss: 0.8617352247238159
Epoch 450, training loss: 6.586369514465332 = 0.4909660816192627 + 1.0 * 6.09540319442749
Epoch 450, val loss: 0.8491052389144897
Epoch 460, training loss: 6.557404041290283 = 0.4642302691936493 + 1.0 * 6.093173980712891
Epoch 460, val loss: 0.8389442563056946
Epoch 470, training loss: 6.537583827972412 = 0.43920478224754333 + 1.0 * 6.098379135131836
Epoch 470, val loss: 0.8307984471321106
Epoch 480, training loss: 6.506668567657471 = 0.41583529114723206 + 1.0 * 6.0908331871032715
Epoch 480, val loss: 0.8247805237770081
Epoch 490, training loss: 6.480762004852295 = 0.3936437666416168 + 1.0 * 6.087118148803711
Epoch 490, val loss: 0.8200790286064148
Epoch 500, training loss: 6.462611198425293 = 0.3725006580352783 + 1.0 * 6.0901103019714355
Epoch 500, val loss: 0.8165921568870544
Epoch 510, training loss: 6.4378862380981445 = 0.35242757201194763 + 1.0 * 6.085458755493164
Epoch 510, val loss: 0.8142588138580322
Epoch 520, training loss: 6.415528774261475 = 0.33324918150901794 + 1.0 * 6.082279682159424
Epoch 520, val loss: 0.812638521194458
Epoch 530, training loss: 6.39594841003418 = 0.31489625573158264 + 1.0 * 6.081052303314209
Epoch 530, val loss: 0.8116310834884644
Epoch 540, training loss: 6.376696586608887 = 0.2972583472728729 + 1.0 * 6.079438209533691
Epoch 540, val loss: 0.8111845254898071
Epoch 550, training loss: 6.35948371887207 = 0.2803492844104767 + 1.0 * 6.079134464263916
Epoch 550, val loss: 0.8111394047737122
Epoch 560, training loss: 6.343520164489746 = 0.2642660439014435 + 1.0 * 6.079254150390625
Epoch 560, val loss: 0.8115588426589966
Epoch 570, training loss: 6.323472023010254 = 0.2489885836839676 + 1.0 * 6.074483394622803
Epoch 570, val loss: 0.8122636675834656
Epoch 580, training loss: 6.308041095733643 = 0.23442910611629486 + 1.0 * 6.073612213134766
Epoch 580, val loss: 0.8132410049438477
Epoch 590, training loss: 6.298154354095459 = 0.22062434256076813 + 1.0 * 6.0775299072265625
Epoch 590, val loss: 0.8144615888595581
Epoch 600, training loss: 6.279419898986816 = 0.20765767991542816 + 1.0 * 6.0717620849609375
Epoch 600, val loss: 0.8161557912826538
Epoch 610, training loss: 6.265149116516113 = 0.19542641937732697 + 1.0 * 6.069722652435303
Epoch 610, val loss: 0.818070650100708
Epoch 620, training loss: 6.254199028015137 = 0.1838640719652176 + 1.0 * 6.0703349113464355
Epoch 620, val loss: 0.820215106010437
Epoch 630, training loss: 6.24465274810791 = 0.17306393384933472 + 1.0 * 6.07158899307251
Epoch 630, val loss: 0.8225430250167847
Epoch 640, training loss: 6.228323936462402 = 0.16296689212322235 + 1.0 * 6.065357208251953
Epoch 640, val loss: 0.8253565430641174
Epoch 650, training loss: 6.2201642990112305 = 0.1534947007894516 + 1.0 * 6.066669464111328
Epoch 650, val loss: 0.8282884955406189
Epoch 660, training loss: 6.214454174041748 = 0.14469845592975616 + 1.0 * 6.069755554199219
Epoch 660, val loss: 0.8313004374504089
Epoch 670, training loss: 6.198186874389648 = 0.13652296364307404 + 1.0 * 6.06166410446167
Epoch 670, val loss: 0.8349339962005615
Epoch 680, training loss: 6.189706802368164 = 0.12888137996196747 + 1.0 * 6.060825347900391
Epoch 680, val loss: 0.8385964035987854
Epoch 690, training loss: 6.18757963180542 = 0.12173934280872345 + 1.0 * 6.065840244293213
Epoch 690, val loss: 0.842444121837616
Epoch 700, training loss: 6.174831390380859 = 0.11509526520967484 + 1.0 * 6.059736251831055
Epoch 700, val loss: 0.8465735912322998
Epoch 710, training loss: 6.166868209838867 = 0.10889304429292679 + 1.0 * 6.0579752922058105
Epoch 710, val loss: 0.8508779406547546
Epoch 720, training loss: 6.164928436279297 = 0.10311155021190643 + 1.0 * 6.061816692352295
Epoch 720, val loss: 0.855192244052887
Epoch 730, training loss: 6.15374231338501 = 0.09773924946784973 + 1.0 * 6.056003093719482
Epoch 730, val loss: 0.8597580790519714
Epoch 740, training loss: 6.1484375 = 0.09275264292955399 + 1.0 * 6.055685043334961
Epoch 740, val loss: 0.8643931746482849
Epoch 750, training loss: 6.141944885253906 = 0.08811920881271362 + 1.0 * 6.053825855255127
Epoch 750, val loss: 0.8692342638969421
Epoch 760, training loss: 6.135471343994141 = 0.0837799534201622 + 1.0 * 6.05169153213501
Epoch 760, val loss: 0.8740882277488708
Epoch 770, training loss: 6.133444786071777 = 0.07971468567848206 + 1.0 * 6.053730010986328
Epoch 770, val loss: 0.8790178894996643
Epoch 780, training loss: 6.129705905914307 = 0.07592443376779556 + 1.0 * 6.053781509399414
Epoch 780, val loss: 0.8839259147644043
Epoch 790, training loss: 6.121371746063232 = 0.07239486277103424 + 1.0 * 6.048976898193359
Epoch 790, val loss: 0.8892511129379272
Epoch 800, training loss: 6.117810249328613 = 0.06908141076564789 + 1.0 * 6.048728942871094
Epoch 800, val loss: 0.8943850994110107
Epoch 810, training loss: 6.115658283233643 = 0.06597940623760223 + 1.0 * 6.049678802490234
Epoch 810, val loss: 0.8994019031524658
Epoch 820, training loss: 6.108989238739014 = 0.0630825087428093 + 1.0 * 6.0459065437316895
Epoch 820, val loss: 0.9047037959098816
Epoch 830, training loss: 6.106725692749023 = 0.06035897508263588 + 1.0 * 6.0463666915893555
Epoch 830, val loss: 0.9099870920181274
Epoch 840, training loss: 6.10530948638916 = 0.05780920013785362 + 1.0 * 6.047500133514404
Epoch 840, val loss: 0.9147936701774597
Epoch 850, training loss: 6.099905967712402 = 0.055431265383958817 + 1.0 * 6.0444746017456055
Epoch 850, val loss: 0.9201907515525818
Epoch 860, training loss: 6.0966572761535645 = 0.05319823697209358 + 1.0 * 6.043458938598633
Epoch 860, val loss: 0.9255303740501404
Epoch 870, training loss: 6.093093395233154 = 0.05107768252491951 + 1.0 * 6.042015552520752
Epoch 870, val loss: 0.9305901527404785
Epoch 880, training loss: 6.0989089012146 = 0.04907592013478279 + 1.0 * 6.049832820892334
Epoch 880, val loss: 0.9357094168663025
Epoch 890, training loss: 6.090888977050781 = 0.04718205705285072 + 1.0 * 6.043706893920898
Epoch 890, val loss: 0.9408287405967712
Epoch 900, training loss: 6.090065956115723 = 0.045406173914670944 + 1.0 * 6.044659614562988
Epoch 900, val loss: 0.9460484981536865
Epoch 910, training loss: 6.083187580108643 = 0.043726786971092224 + 1.0 * 6.0394606590271
Epoch 910, val loss: 0.9511460065841675
Epoch 920, training loss: 6.080443382263184 = 0.042135145515203476 + 1.0 * 6.038308143615723
Epoch 920, val loss: 0.9562849402427673
Epoch 930, training loss: 6.083576679229736 = 0.04062332212924957 + 1.0 * 6.0429534912109375
Epoch 930, val loss: 0.9612693786621094
Epoch 940, training loss: 6.077691555023193 = 0.03919592872262001 + 1.0 * 6.0384955406188965
Epoch 940, val loss: 0.9663029313087463
Epoch 950, training loss: 6.076454162597656 = 0.037840962409973145 + 1.0 * 6.038613319396973
Epoch 950, val loss: 0.9713747501373291
Epoch 960, training loss: 6.072616100311279 = 0.036556366831064224 + 1.0 * 6.036059856414795
Epoch 960, val loss: 0.9760546684265137
Epoch 970, training loss: 6.069888591766357 = 0.03534180670976639 + 1.0 * 6.034546852111816
Epoch 970, val loss: 0.9811567664146423
Epoch 980, training loss: 6.067835807800293 = 0.03418200463056564 + 1.0 * 6.033653736114502
Epoch 980, val loss: 0.9860090613365173
Epoch 990, training loss: 6.073753833770752 = 0.03307538107037544 + 1.0 * 6.04067850112915
Epoch 990, val loss: 0.9907414317131042
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8782
Flip ASR: 0.8533/225 nodes
The final ASR:0.78967, 0.06824, Accuracy:0.79012, 0.02188
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10452])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.82716, 0.00349
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.350435256958008 = 1.9765971899032593 + 1.0 * 8.373838424682617
Epoch 0, val loss: 1.9688483476638794
Epoch 10, training loss: 10.338020324707031 = 1.9647380113601685 + 1.0 * 8.373282432556152
Epoch 10, val loss: 1.9571024179458618
Epoch 20, training loss: 10.319513320922852 = 1.9495481252670288 + 1.0 * 8.369965553283691
Epoch 20, val loss: 1.9423086643218994
Epoch 30, training loss: 10.274828910827637 = 1.9285799264907837 + 1.0 * 8.346248626708984
Epoch 30, val loss: 1.9219990968704224
Epoch 40, training loss: 10.032489776611328 = 1.9026156663894653 + 1.0 * 8.129874229431152
Epoch 40, val loss: 1.8974074125289917
Epoch 50, training loss: 9.128023147583008 = 1.8765593767166138 + 1.0 * 7.251463413238525
Epoch 50, val loss: 1.87388277053833
Epoch 60, training loss: 8.769477844238281 = 1.8624391555786133 + 1.0 * 6.907038688659668
Epoch 60, val loss: 1.8610256910324097
Epoch 70, training loss: 8.554312705993652 = 1.8476673364639282 + 1.0 * 6.706645488739014
Epoch 70, val loss: 1.8474066257476807
Epoch 80, training loss: 8.412731170654297 = 1.8323643207550049 + 1.0 * 6.580367088317871
Epoch 80, val loss: 1.8342088460922241
Epoch 90, training loss: 8.317453384399414 = 1.8169093132019043 + 1.0 * 6.500544548034668
Epoch 90, val loss: 1.8213266134262085
Epoch 100, training loss: 8.24764633178711 = 1.802945613861084 + 1.0 * 6.444700717926025
Epoch 100, val loss: 1.8100204467773438
Epoch 110, training loss: 8.191120147705078 = 1.7900879383087158 + 1.0 * 6.401031970977783
Epoch 110, val loss: 1.7998046875
Epoch 120, training loss: 8.143745422363281 = 1.7777889966964722 + 1.0 * 6.3659563064575195
Epoch 120, val loss: 1.7899279594421387
Epoch 130, training loss: 8.106230735778809 = 1.765100121498108 + 1.0 * 6.341130256652832
Epoch 130, val loss: 1.779619812965393
Epoch 140, training loss: 8.064703941345215 = 1.7514430284500122 + 1.0 * 6.313261032104492
Epoch 140, val loss: 1.7684727907180786
Epoch 150, training loss: 8.027368545532227 = 1.735891342163086 + 1.0 * 6.291476726531982
Epoch 150, val loss: 1.755988359451294
Epoch 160, training loss: 7.989727973937988 = 1.7176188230514526 + 1.0 * 6.272109031677246
Epoch 160, val loss: 1.7414880990982056
Epoch 170, training loss: 7.956668376922607 = 1.695717692375183 + 1.0 * 6.260950565338135
Epoch 170, val loss: 1.7243064641952515
Epoch 180, training loss: 7.910799026489258 = 1.6700295209884644 + 1.0 * 6.240769386291504
Epoch 180, val loss: 1.7039306163787842
Epoch 190, training loss: 7.867758750915527 = 1.6394494771957397 + 1.0 * 6.228309154510498
Epoch 190, val loss: 1.679795742034912
Epoch 200, training loss: 7.82048225402832 = 1.602908730506897 + 1.0 * 6.217573642730713
Epoch 200, val loss: 1.6506413221359253
Epoch 210, training loss: 7.775493621826172 = 1.5596431493759155 + 1.0 * 6.215850353240967
Epoch 210, val loss: 1.6160738468170166
Epoch 220, training loss: 7.7146220207214355 = 1.5125292539596558 + 1.0 * 6.20209264755249
Epoch 220, val loss: 1.5783175230026245
Epoch 230, training loss: 7.656818866729736 = 1.4618293046951294 + 1.0 * 6.1949896812438965
Epoch 230, val loss: 1.5379468202590942
Epoch 240, training loss: 7.59723424911499 = 1.4084858894348145 + 1.0 * 6.188748359680176
Epoch 240, val loss: 1.4952213764190674
Epoch 250, training loss: 7.543579578399658 = 1.3540568351745605 + 1.0 * 6.189522743225098
Epoch 250, val loss: 1.4519840478897095
Epoch 260, training loss: 7.480278968811035 = 1.300442099571228 + 1.0 * 6.179836750030518
Epoch 260, val loss: 1.4099762439727783
Epoch 270, training loss: 7.420427322387695 = 1.2473138570785522 + 1.0 * 6.1731133460998535
Epoch 270, val loss: 1.3683247566223145
Epoch 280, training loss: 7.364477157592773 = 1.1945388317108154 + 1.0 * 6.169938087463379
Epoch 280, val loss: 1.3273969888687134
Epoch 290, training loss: 7.307690620422363 = 1.1430703401565552 + 1.0 * 6.164620399475098
Epoch 290, val loss: 1.2878901958465576
Epoch 300, training loss: 7.251567840576172 = 1.0926196575164795 + 1.0 * 6.1589484214782715
Epoch 300, val loss: 1.2495397329330444
Epoch 310, training loss: 7.196735858917236 = 1.0425816774368286 + 1.0 * 6.154154300689697
Epoch 310, val loss: 1.2117016315460205
Epoch 320, training loss: 7.149691104888916 = 0.9929252862930298 + 1.0 * 6.156765937805176
Epoch 320, val loss: 1.1743688583374023
Epoch 330, training loss: 7.092705249786377 = 0.9448515772819519 + 1.0 * 6.147853851318359
Epoch 330, val loss: 1.1381698846817017
Epoch 340, training loss: 7.046663284301758 = 0.898240864276886 + 1.0 * 6.1484222412109375
Epoch 340, val loss: 1.103392481803894
Epoch 350, training loss: 6.992180824279785 = 0.8535507917404175 + 1.0 * 6.138629913330078
Epoch 350, val loss: 1.0701806545257568
Epoch 360, training loss: 6.946072578430176 = 0.8105067014694214 + 1.0 * 6.135565757751465
Epoch 360, val loss: 1.038620114326477
Epoch 370, training loss: 6.9087233543396 = 0.7692474126815796 + 1.0 * 6.1394758224487305
Epoch 370, val loss: 1.0085809230804443
Epoch 380, training loss: 6.859419345855713 = 0.7305664420127869 + 1.0 * 6.128852844238281
Epoch 380, val loss: 0.9806599020957947
Epoch 390, training loss: 6.821568965911865 = 0.6943337321281433 + 1.0 * 6.127235412597656
Epoch 390, val loss: 0.9550666809082031
Epoch 400, training loss: 6.783267974853516 = 0.6605064868927002 + 1.0 * 6.1227617263793945
Epoch 400, val loss: 0.931765615940094
Epoch 410, training loss: 6.748007774353027 = 0.6287112832069397 + 1.0 * 6.119296550750732
Epoch 410, val loss: 0.9104483723640442
Epoch 420, training loss: 6.716907978057861 = 0.5988587737083435 + 1.0 * 6.118049144744873
Epoch 420, val loss: 0.8910598754882812
Epoch 430, training loss: 6.684988498687744 = 0.5710471272468567 + 1.0 * 6.113941192626953
Epoch 430, val loss: 0.8738580942153931
Epoch 440, training loss: 6.655815601348877 = 0.5447239875793457 + 1.0 * 6.111091613769531
Epoch 440, val loss: 0.8582096099853516
Epoch 450, training loss: 6.62951135635376 = 0.5197084546089172 + 1.0 * 6.109802722930908
Epoch 450, val loss: 0.8439654111862183
Epoch 460, training loss: 6.6037092208862305 = 0.4960380792617798 + 1.0 * 6.10767126083374
Epoch 460, val loss: 0.8312450647354126
Epoch 470, training loss: 6.579118251800537 = 0.4732980728149414 + 1.0 * 6.105820178985596
Epoch 470, val loss: 0.8197087645530701
Epoch 480, training loss: 6.555682182312012 = 0.45119744539260864 + 1.0 * 6.104484558105469
Epoch 480, val loss: 0.8091955184936523
Epoch 490, training loss: 6.53253173828125 = 0.42971235513687134 + 1.0 * 6.102819442749023
Epoch 490, val loss: 0.7995129823684692
Epoch 500, training loss: 6.511723518371582 = 0.40870675444602966 + 1.0 * 6.1030168533325195
Epoch 500, val loss: 0.7907962203025818
Epoch 510, training loss: 6.49040412902832 = 0.3882393538951874 + 1.0 * 6.1021647453308105
Epoch 510, val loss: 0.7827285528182983
Epoch 520, training loss: 6.463674068450928 = 0.3681345582008362 + 1.0 * 6.095539569854736
Epoch 520, val loss: 0.7756230235099792
Epoch 530, training loss: 6.444630146026611 = 0.34832608699798584 + 1.0 * 6.096303939819336
Epoch 530, val loss: 0.7691996693611145
Epoch 540, training loss: 6.4255571365356445 = 0.3289909362792969 + 1.0 * 6.096566200256348
Epoch 540, val loss: 0.7636863589286804
Epoch 550, training loss: 6.401028633117676 = 0.31020447611808777 + 1.0 * 6.090824127197266
Epoch 550, val loss: 0.7591934204101562
Epoch 560, training loss: 6.380961894989014 = 0.29195719957351685 + 1.0 * 6.0890045166015625
Epoch 560, val loss: 0.7554740309715271
Epoch 570, training loss: 6.363430500030518 = 0.27447158098220825 + 1.0 * 6.088958740234375
Epoch 570, val loss: 0.7526816129684448
Epoch 580, training loss: 6.348095893859863 = 0.2579728960990906 + 1.0 * 6.090123176574707
Epoch 580, val loss: 0.7509713768959045
Epoch 590, training loss: 6.327812194824219 = 0.24243684113025665 + 1.0 * 6.0853753089904785
Epoch 590, val loss: 0.7500075101852417
Epoch 600, training loss: 6.310304641723633 = 0.22781136631965637 + 1.0 * 6.082493305206299
Epoch 600, val loss: 0.7498346567153931
Epoch 610, training loss: 6.297518253326416 = 0.2140742689371109 + 1.0 * 6.083444118499756
Epoch 610, val loss: 0.7503672242164612
Epoch 620, training loss: 6.283844470977783 = 0.20126858353614807 + 1.0 * 6.082575798034668
Epoch 620, val loss: 0.7516195774078369
Epoch 630, training loss: 6.270036220550537 = 0.18942566215991974 + 1.0 * 6.080610752105713
Epoch 630, val loss: 0.7534984350204468
Epoch 640, training loss: 6.258755207061768 = 0.17844274640083313 + 1.0 * 6.080312252044678
Epoch 640, val loss: 0.7559953331947327
Epoch 650, training loss: 6.246549606323242 = 0.16827794909477234 + 1.0 * 6.078271865844727
Epoch 650, val loss: 0.7588042616844177
Epoch 660, training loss: 6.237382411956787 = 0.15887628495693207 + 1.0 * 6.078505992889404
Epoch 660, val loss: 0.7621275782585144
Epoch 670, training loss: 6.224225044250488 = 0.15014800429344177 + 1.0 * 6.074077129364014
Epoch 670, val loss: 0.7658235430717468
Epoch 680, training loss: 6.216223239898682 = 0.1420111507177353 + 1.0 * 6.074212074279785
Epoch 680, val loss: 0.7698339819908142
Epoch 690, training loss: 6.212690830230713 = 0.1344389021396637 + 1.0 * 6.078251838684082
Epoch 690, val loss: 0.774046003818512
Epoch 700, training loss: 6.199117660522461 = 0.127426877617836 + 1.0 * 6.071690559387207
Epoch 700, val loss: 0.7785077095031738
Epoch 710, training loss: 6.191009044647217 = 0.12090049684047699 + 1.0 * 6.070108413696289
Epoch 710, val loss: 0.7833126783370972
Epoch 720, training loss: 6.182121753692627 = 0.11478665471076965 + 1.0 * 6.06733512878418
Epoch 720, val loss: 0.788173258304596
Epoch 730, training loss: 6.179073810577393 = 0.10904804617166519 + 1.0 * 6.07002592086792
Epoch 730, val loss: 0.7932710647583008
Epoch 740, training loss: 6.179923057556152 = 0.1037006601691246 + 1.0 * 6.0762224197387695
Epoch 740, val loss: 0.7985126376152039
Epoch 750, training loss: 6.165240287780762 = 0.09874480217695236 + 1.0 * 6.066495418548584
Epoch 750, val loss: 0.8039361834526062
Epoch 760, training loss: 6.161649703979492 = 0.09409234672784805 + 1.0 * 6.067557334899902
Epoch 760, val loss: 0.8093363642692566
Epoch 770, training loss: 6.152490615844727 = 0.08972356468439102 + 1.0 * 6.062767028808594
Epoch 770, val loss: 0.8147915601730347
Epoch 780, training loss: 6.14757776260376 = 0.08561330288648605 + 1.0 * 6.061964511871338
Epoch 780, val loss: 0.8204349875450134
Epoch 790, training loss: 6.149644374847412 = 0.08173362165689468 + 1.0 * 6.067910671234131
Epoch 790, val loss: 0.8260766863822937
Epoch 800, training loss: 6.143919467926025 = 0.07809068262577057 + 1.0 * 6.065828800201416
Epoch 800, val loss: 0.8317148685455322
Epoch 810, training loss: 6.134062767028809 = 0.07467383146286011 + 1.0 * 6.059389114379883
Epoch 810, val loss: 0.8373463153839111
Epoch 820, training loss: 6.130571365356445 = 0.0714513435959816 + 1.0 * 6.059120178222656
Epoch 820, val loss: 0.8430410027503967
Epoch 830, training loss: 6.1259026527404785 = 0.06839877367019653 + 1.0 * 6.057503700256348
Epoch 830, val loss: 0.8487064838409424
Epoch 840, training loss: 6.132904052734375 = 0.06551236659288406 + 1.0 * 6.067391872406006
Epoch 840, val loss: 0.8543336391448975
Epoch 850, training loss: 6.12067174911499 = 0.06279875338077545 + 1.0 * 6.057872772216797
Epoch 850, val loss: 0.8598930835723877
Epoch 860, training loss: 6.116085529327393 = 0.060243941843509674 + 1.0 * 6.055841445922852
Epoch 860, val loss: 0.8655703067779541
Epoch 870, training loss: 6.110881328582764 = 0.05781545117497444 + 1.0 * 6.053065776824951
Epoch 870, val loss: 0.8710972666740417
Epoch 880, training loss: 6.124239921569824 = 0.05551009625196457 + 1.0 * 6.068729877471924
Epoch 880, val loss: 0.8765705823898315
Epoch 890, training loss: 6.1092705726623535 = 0.05333669111132622 + 1.0 * 6.055933952331543
Epoch 890, val loss: 0.8820530772209167
Epoch 900, training loss: 6.102960109710693 = 0.05128765106201172 + 1.0 * 6.051672458648682
Epoch 900, val loss: 0.8876873254776001
Epoch 910, training loss: 6.0994696617126465 = 0.04933200776576996 + 1.0 * 6.050137519836426
Epoch 910, val loss: 0.8930463790893555
Epoch 920, training loss: 6.1118879318237305 = 0.04746783897280693 + 1.0 * 6.064420223236084
Epoch 920, val loss: 0.8984130620956421
Epoch 930, training loss: 6.096041679382324 = 0.04570944979786873 + 1.0 * 6.050332069396973
Epoch 930, val loss: 0.9037452936172485
Epoch 940, training loss: 6.093184947967529 = 0.044039931148290634 + 1.0 * 6.049145221710205
Epoch 940, val loss: 0.9091624021530151
Epoch 950, training loss: 6.094946384429932 = 0.042444825172424316 + 1.0 * 6.052501678466797
Epoch 950, val loss: 0.9143410921096802
Epoch 960, training loss: 6.088974952697754 = 0.04092937335371971 + 1.0 * 6.048045635223389
Epoch 960, val loss: 0.9195511937141418
Epoch 970, training loss: 6.0899739265441895 = 0.039485957473516464 + 1.0 * 6.050487995147705
Epoch 970, val loss: 0.9247344136238098
Epoch 980, training loss: 6.084123134613037 = 0.038113683462142944 + 1.0 * 6.046009540557861
Epoch 980, val loss: 0.9298902153968811
Epoch 990, training loss: 6.081208229064941 = 0.03680504485964775 + 1.0 * 6.044403076171875
Epoch 990, val loss: 0.9350475072860718
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7269
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.323453903198242 = 1.949613094329834 + 1.0 * 8.37384033203125
Epoch 0, val loss: 1.951181173324585
Epoch 10, training loss: 10.31254768371582 = 1.9392225742340088 + 1.0 * 8.37332534790039
Epoch 10, val loss: 1.9407401084899902
Epoch 20, training loss: 10.296682357788086 = 1.92641019821167 + 1.0 * 8.370271682739258
Epoch 20, val loss: 1.927195429801941
Epoch 30, training loss: 10.257676124572754 = 1.908923864364624 + 1.0 * 8.34875202178955
Epoch 30, val loss: 1.9081827402114868
Epoch 40, training loss: 10.040594100952148 = 1.887391209602356 + 1.0 * 8.153203010559082
Epoch 40, val loss: 1.885340929031372
Epoch 50, training loss: 9.258402824401855 = 1.8661657571792603 + 1.0 * 7.392236709594727
Epoch 50, val loss: 1.864133596420288
Epoch 60, training loss: 8.809317588806152 = 1.85336172580719 + 1.0 * 6.955955982208252
Epoch 60, val loss: 1.851470708847046
Epoch 70, training loss: 8.550323486328125 = 1.841728925704956 + 1.0 * 6.708594799041748
Epoch 70, val loss: 1.8392553329467773
Epoch 80, training loss: 8.397398948669434 = 1.830712080001831 + 1.0 * 6.566686630249023
Epoch 80, val loss: 1.8279545307159424
Epoch 90, training loss: 8.282831192016602 = 1.818981409072876 + 1.0 * 6.463850021362305
Epoch 90, val loss: 1.815794825553894
Epoch 100, training loss: 8.205829620361328 = 1.8077350854873657 + 1.0 * 6.398094654083252
Epoch 100, val loss: 1.8038578033447266
Epoch 110, training loss: 8.1471586227417 = 1.796779990196228 + 1.0 * 6.35037899017334
Epoch 110, val loss: 1.7921020984649658
Epoch 120, training loss: 8.103818893432617 = 1.7858895063400269 + 1.0 * 6.317929744720459
Epoch 120, val loss: 1.780590534210205
Epoch 130, training loss: 8.062238693237305 = 1.7749452590942383 + 1.0 * 6.287293910980225
Epoch 130, val loss: 1.7693464756011963
Epoch 140, training loss: 8.02761173248291 = 1.763468623161316 + 1.0 * 6.264142990112305
Epoch 140, val loss: 1.7579354047775269
Epoch 150, training loss: 7.99526834487915 = 1.7509160041809082 + 1.0 * 6.244352340698242
Epoch 150, val loss: 1.7459794282913208
Epoch 160, training loss: 7.964394569396973 = 1.7367559671401978 + 1.0 * 6.2276387214660645
Epoch 160, val loss: 1.7330987453460693
Epoch 170, training loss: 7.942414283752441 = 1.720446228981018 + 1.0 * 6.221968173980713
Epoch 170, val loss: 1.7188843488693237
Epoch 180, training loss: 7.903825759887695 = 1.7017090320587158 + 1.0 * 6.2021164894104
Epoch 180, val loss: 1.703294277191162
Epoch 190, training loss: 7.872105121612549 = 1.6799325942993164 + 1.0 * 6.192172527313232
Epoch 190, val loss: 1.6854993104934692
Epoch 200, training loss: 7.837505340576172 = 1.6541146039962769 + 1.0 * 6.1833906173706055
Epoch 200, val loss: 1.6648030281066895
Epoch 210, training loss: 7.7989397048950195 = 1.6230672597885132 + 1.0 * 6.175872325897217
Epoch 210, val loss: 1.6400479078292847
Epoch 220, training loss: 7.756132125854492 = 1.5851753950119019 + 1.0 * 6.170956611633301
Epoch 220, val loss: 1.609928011894226
Epoch 230, training loss: 7.706207752227783 = 1.54129159450531 + 1.0 * 6.164916038513184
Epoch 230, val loss: 1.5753147602081299
Epoch 240, training loss: 7.651346683502197 = 1.4910058975219727 + 1.0 * 6.160340785980225
Epoch 240, val loss: 1.5355000495910645
Epoch 250, training loss: 7.590031623840332 = 1.4344534873962402 + 1.0 * 6.155578136444092
Epoch 250, val loss: 1.4907677173614502
Epoch 260, training loss: 7.525323867797852 = 1.3728477954864502 + 1.0 * 6.1524763107299805
Epoch 260, val loss: 1.4423370361328125
Epoch 270, training loss: 7.4588623046875 = 1.3094565868377686 + 1.0 * 6.149405479431152
Epoch 270, val loss: 1.3932887315750122
Epoch 280, training loss: 7.390844821929932 = 1.2466702461242676 + 1.0 * 6.144174575805664
Epoch 280, val loss: 1.345271348953247
Epoch 290, training loss: 7.326279163360596 = 1.1857128143310547 + 1.0 * 6.140566349029541
Epoch 290, val loss: 1.2988613843917847
Epoch 300, training loss: 7.265976428985596 = 1.1287461519241333 + 1.0 * 6.137230396270752
Epoch 300, val loss: 1.255806803703308
Epoch 310, training loss: 7.211637496948242 = 1.0772310495376587 + 1.0 * 6.134406566619873
Epoch 310, val loss: 1.216996669769287
Epoch 320, training loss: 7.1647186279296875 = 1.0315943956375122 + 1.0 * 6.133124351501465
Epoch 320, val loss: 1.1826047897338867
Epoch 330, training loss: 7.1172566413879395 = 0.9910893440246582 + 1.0 * 6.126167297363281
Epoch 330, val loss: 1.152375340461731
Epoch 340, training loss: 7.07577657699585 = 0.9542067646980286 + 1.0 * 6.121569633483887
Epoch 340, val loss: 1.125198483467102
Epoch 350, training loss: 7.042139053344727 = 0.9199016094207764 + 1.0 * 6.122237205505371
Epoch 350, val loss: 1.1004319190979004
Epoch 360, training loss: 7.003461837768555 = 0.8877817392349243 + 1.0 * 6.11568021774292
Epoch 360, val loss: 1.0776166915893555
Epoch 370, training loss: 6.969814300537109 = 0.8566553592681885 + 1.0 * 6.1131591796875
Epoch 370, val loss: 1.0556122064590454
Epoch 380, training loss: 6.935887336730957 = 0.8255029916763306 + 1.0 * 6.110384464263916
Epoch 380, val loss: 1.0337681770324707
Epoch 390, training loss: 6.904985427856445 = 0.7940480709075928 + 1.0 * 6.110937595367432
Epoch 390, val loss: 1.0117336511611938
Epoch 400, training loss: 6.869194030761719 = 0.7622262239456177 + 1.0 * 6.106967926025391
Epoch 400, val loss: 0.9894639849662781
Epoch 410, training loss: 6.833641052246094 = 0.7298198342323303 + 1.0 * 6.103821277618408
Epoch 410, val loss: 0.9666566252708435
Epoch 420, training loss: 6.808467864990234 = 0.6971719861030579 + 1.0 * 6.111295700073242
Epoch 420, val loss: 0.9434682130813599
Epoch 430, training loss: 6.771335601806641 = 0.6652713418006897 + 1.0 * 6.106064319610596
Epoch 430, val loss: 0.9209019541740417
Epoch 440, training loss: 6.733673095703125 = 0.6346054673194885 + 1.0 * 6.099067687988281
Epoch 440, val loss: 0.8992601633071899
Epoch 450, training loss: 6.7017436027526855 = 0.6049773097038269 + 1.0 * 6.096766471862793
Epoch 450, val loss: 0.8784029483795166
Epoch 460, training loss: 6.670815944671631 = 0.5764411687850952 + 1.0 * 6.094374656677246
Epoch 460, val loss: 0.8584267497062683
Epoch 470, training loss: 6.6458845138549805 = 0.5490155816078186 + 1.0 * 6.096868991851807
Epoch 470, val loss: 0.8395655155181885
Epoch 480, training loss: 6.615279674530029 = 0.52296382188797 + 1.0 * 6.092315673828125
Epoch 480, val loss: 0.8219039440155029
Epoch 490, training loss: 6.590910911560059 = 0.49816441535949707 + 1.0 * 6.092746734619141
Epoch 490, val loss: 0.8054389357566833
Epoch 500, training loss: 6.565150260925293 = 0.4742961525917053 + 1.0 * 6.090854167938232
Epoch 500, val loss: 0.7900288105010986
Epoch 510, training loss: 6.539505481719971 = 0.4512304961681366 + 1.0 * 6.088274955749512
Epoch 510, val loss: 0.7755207419395447
Epoch 520, training loss: 6.520646572113037 = 0.42872828245162964 + 1.0 * 6.091918468475342
Epoch 520, val loss: 0.7618152499198914
Epoch 530, training loss: 6.495781898498535 = 0.40684935450553894 + 1.0 * 6.088932514190674
Epoch 530, val loss: 0.7489464282989502
Epoch 540, training loss: 6.468889236450195 = 0.38554054498672485 + 1.0 * 6.083348751068115
Epoch 540, val loss: 0.7367148995399475
Epoch 550, training loss: 6.4486284255981445 = 0.3646204471588135 + 1.0 * 6.084007740020752
Epoch 550, val loss: 0.7250593900680542
Epoch 560, training loss: 6.4250807762146 = 0.34414198994636536 + 1.0 * 6.080938816070557
Epoch 560, val loss: 0.7139298915863037
Epoch 570, training loss: 6.405524253845215 = 0.3241190016269684 + 1.0 * 6.081405162811279
Epoch 570, val loss: 0.7033838629722595
Epoch 580, training loss: 6.385955810546875 = 0.30451473593711853 + 1.0 * 6.0814409255981445
Epoch 580, val loss: 0.6934335827827454
Epoch 590, training loss: 6.366927623748779 = 0.28549256920814514 + 1.0 * 6.081435203552246
Epoch 590, val loss: 0.6840786337852478
Epoch 600, training loss: 6.344052314758301 = 0.2670779526233673 + 1.0 * 6.076974391937256
Epoch 600, val loss: 0.6753196716308594
Epoch 610, training loss: 6.324995517730713 = 0.24924986064434052 + 1.0 * 6.075745582580566
Epoch 610, val loss: 0.6671999096870422
Epoch 620, training loss: 6.312397480010986 = 0.23218174278736115 + 1.0 * 6.080215930938721
Epoch 620, val loss: 0.6597502827644348
Epoch 630, training loss: 6.293652057647705 = 0.21601514518260956 + 1.0 * 6.07763671875
Epoch 630, val loss: 0.6530324220657349
Epoch 640, training loss: 6.274049758911133 = 0.2007897049188614 + 1.0 * 6.0732598304748535
Epoch 640, val loss: 0.647002100944519
Epoch 650, training loss: 6.2584757804870605 = 0.1864481419324875 + 1.0 * 6.072027683258057
Epoch 650, val loss: 0.6418002843856812
Epoch 660, training loss: 6.243724346160889 = 0.17308683693408966 + 1.0 * 6.0706377029418945
Epoch 660, val loss: 0.6373171210289001
Epoch 670, training loss: 6.2313714027404785 = 0.16072802245616913 + 1.0 * 6.070643424987793
Epoch 670, val loss: 0.6334904432296753
Epoch 680, training loss: 6.224111080169678 = 0.14932548999786377 + 1.0 * 6.0747857093811035
Epoch 680, val loss: 0.6304391026496887
Epoch 690, training loss: 6.208540916442871 = 0.13891857862472534 + 1.0 * 6.06962251663208
Epoch 690, val loss: 0.6281219720840454
Epoch 700, training loss: 6.195382595062256 = 0.12941519916057587 + 1.0 * 6.065967559814453
Epoch 700, val loss: 0.62635338306427
Epoch 710, training loss: 6.189452171325684 = 0.12079118937253952 + 1.0 * 6.068661212921143
Epoch 710, val loss: 0.6252036094665527
Epoch 720, training loss: 6.175020694732666 = 0.11294476687908173 + 1.0 * 6.062076091766357
Epoch 720, val loss: 0.6245618462562561
Epoch 730, training loss: 6.177459239959717 = 0.10579503327608109 + 1.0 * 6.071664333343506
Epoch 730, val loss: 0.6244861483573914
Epoch 740, training loss: 6.161985874176025 = 0.09926588833332062 + 1.0 * 6.062719821929932
Epoch 740, val loss: 0.6248065233230591
Epoch 750, training loss: 6.153228759765625 = 0.09333673864603043 + 1.0 * 6.059892177581787
Epoch 750, val loss: 0.6255139708518982
Epoch 760, training loss: 6.145829677581787 = 0.08787324279546738 + 1.0 * 6.057956218719482
Epoch 760, val loss: 0.6266543865203857
Epoch 770, training loss: 6.151019096374512 = 0.08285941928625107 + 1.0 * 6.068159580230713
Epoch 770, val loss: 0.6280766129493713
Epoch 780, training loss: 6.140007495880127 = 0.07828295230865479 + 1.0 * 6.061724662780762
Epoch 780, val loss: 0.6297702789306641
Epoch 790, training loss: 6.131106376647949 = 0.07409928739070892 + 1.0 * 6.057007312774658
Epoch 790, val loss: 0.6315444707870483
Epoch 800, training loss: 6.125157356262207 = 0.07023899257183075 + 1.0 * 6.05491828918457
Epoch 800, val loss: 0.6336021423339844
Epoch 810, training loss: 6.1209187507629395 = 0.06665347516536713 + 1.0 * 6.05426549911499
Epoch 810, val loss: 0.6358727216720581
Epoch 820, training loss: 6.119948863983154 = 0.06333093345165253 + 1.0 * 6.056617736816406
Epoch 820, val loss: 0.638256847858429
Epoch 830, training loss: 6.116837501525879 = 0.06026310473680496 + 1.0 * 6.05657434463501
Epoch 830, val loss: 0.6408103704452515
Epoch 840, training loss: 6.108950138092041 = 0.05742568150162697 + 1.0 * 6.051524639129639
Epoch 840, val loss: 0.6433789730072021
Epoch 850, training loss: 6.104332447052002 = 0.05478406697511673 + 1.0 * 6.049548149108887
Epoch 850, val loss: 0.6461142301559448
Epoch 860, training loss: 6.107238292694092 = 0.05230680853128433 + 1.0 * 6.054931640625
Epoch 860, val loss: 0.6489860415458679
Epoch 870, training loss: 6.098720073699951 = 0.05000563710927963 + 1.0 * 6.048714637756348
Epoch 870, val loss: 0.6520184278488159
Epoch 880, training loss: 6.095715522766113 = 0.04783724248409271 + 1.0 * 6.047878265380859
Epoch 880, val loss: 0.6549712419509888
Epoch 890, training loss: 6.0964860916137695 = 0.04581345617771149 + 1.0 * 6.05067253112793
Epoch 890, val loss: 0.6581704616546631
Epoch 900, training loss: 6.089184284210205 = 0.043924812227487564 + 1.0 * 6.045259475708008
Epoch 900, val loss: 0.6612658500671387
Epoch 910, training loss: 6.088199138641357 = 0.04214354604482651 + 1.0 * 6.046055793762207
Epoch 910, val loss: 0.6644139289855957
Epoch 920, training loss: 6.083618640899658 = 0.04047176241874695 + 1.0 * 6.043147087097168
Epoch 920, val loss: 0.667683482170105
Epoch 930, training loss: 6.082671642303467 = 0.03890051692724228 + 1.0 * 6.043771266937256
Epoch 930, val loss: 0.6709052324295044
Epoch 940, training loss: 6.081801891326904 = 0.0374189093708992 + 1.0 * 6.0443830490112305
Epoch 940, val loss: 0.6741363406181335
Epoch 950, training loss: 6.079073429107666 = 0.03602411225438118 + 1.0 * 6.043049335479736
Epoch 950, val loss: 0.6773792505264282
Epoch 960, training loss: 6.074676513671875 = 0.03470287099480629 + 1.0 * 6.039973735809326
Epoch 960, val loss: 0.6806526184082031
Epoch 970, training loss: 6.076396465301514 = 0.0334511436522007 + 1.0 * 6.042945384979248
Epoch 970, val loss: 0.6839816570281982
Epoch 980, training loss: 6.072664737701416 = 0.03226766362786293 + 1.0 * 6.0403971672058105
Epoch 980, val loss: 0.6873119473457336
Epoch 990, training loss: 6.07652473449707 = 0.031152445822954178 + 1.0 * 6.045372486114502
Epoch 990, val loss: 0.6905980706214905
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9299
Flip ASR: 0.9156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.321255683898926 = 1.9474027156829834 + 1.0 * 8.373852729797363
Epoch 0, val loss: 1.9547119140625
Epoch 10, training loss: 10.309639930725098 = 1.9362276792526245 + 1.0 * 8.373412132263184
Epoch 10, val loss: 1.9427754878997803
Epoch 20, training loss: 10.292739868164062 = 1.9216957092285156 + 1.0 * 8.371044158935547
Epoch 20, val loss: 1.926895022392273
Epoch 30, training loss: 10.25715446472168 = 1.9012397527694702 + 1.0 * 8.355915069580078
Epoch 30, val loss: 1.9043112993240356
Epoch 40, training loss: 10.121182441711426 = 1.8748725652694702 + 1.0 * 8.246310234069824
Epoch 40, val loss: 1.8761723041534424
Epoch 50, training loss: 9.357441902160645 = 1.8485292196273804 + 1.0 * 7.508912563323975
Epoch 50, val loss: 1.849224328994751
Epoch 60, training loss: 8.835577964782715 = 1.8330439329147339 + 1.0 * 7.00253438949585
Epoch 60, val loss: 1.8357056379318237
Epoch 70, training loss: 8.573417663574219 = 1.8225144147872925 + 1.0 * 6.750903606414795
Epoch 70, val loss: 1.8252873420715332
Epoch 80, training loss: 8.423174858093262 = 1.8103530406951904 + 1.0 * 6.61282205581665
Epoch 80, val loss: 1.8132426738739014
Epoch 90, training loss: 8.328272819519043 = 1.7983909845352173 + 1.0 * 6.529881477355957
Epoch 90, val loss: 1.800958275794983
Epoch 100, training loss: 8.25978946685791 = 1.787017583847046 + 1.0 * 6.472772121429443
Epoch 100, val loss: 1.7895126342773438
Epoch 110, training loss: 8.198563575744629 = 1.7762819528579712 + 1.0 * 6.422281265258789
Epoch 110, val loss: 1.7789241075515747
Epoch 120, training loss: 8.148978233337402 = 1.7654750347137451 + 1.0 * 6.383503437042236
Epoch 120, val loss: 1.7688387632369995
Epoch 130, training loss: 8.105359077453613 = 1.7535196542739868 + 1.0 * 6.351839065551758
Epoch 130, val loss: 1.758374810218811
Epoch 140, training loss: 8.065755844116211 = 1.7397031784057617 + 1.0 * 6.326053142547607
Epoch 140, val loss: 1.7468088865280151
Epoch 150, training loss: 8.028636932373047 = 1.7233741283416748 + 1.0 * 6.305263042449951
Epoch 150, val loss: 1.7335244417190552
Epoch 160, training loss: 7.992552757263184 = 1.7039625644683838 + 1.0 * 6.288589954376221
Epoch 160, val loss: 1.7179664373397827
Epoch 170, training loss: 7.954520225524902 = 1.6808050870895386 + 1.0 * 6.273715019226074
Epoch 170, val loss: 1.6995435953140259
Epoch 180, training loss: 7.9136857986450195 = 1.6531858444213867 + 1.0 * 6.260499954223633
Epoch 180, val loss: 1.677651047706604
Epoch 190, training loss: 7.869494438171387 = 1.620300054550171 + 1.0 * 6.249194145202637
Epoch 190, val loss: 1.6516450643539429
Epoch 200, training loss: 7.820371150970459 = 1.5813807249069214 + 1.0 * 6.238990306854248
Epoch 200, val loss: 1.6207596063613892
Epoch 210, training loss: 7.767657279968262 = 1.5360233783721924 + 1.0 * 6.23163366317749
Epoch 210, val loss: 1.5848325490951538
Epoch 220, training loss: 7.707964897155762 = 1.4851150512695312 + 1.0 * 6.2228498458862305
Epoch 220, val loss: 1.5443570613861084
Epoch 230, training loss: 7.645349979400635 = 1.429085612297058 + 1.0 * 6.216264247894287
Epoch 230, val loss: 1.5000056028366089
Epoch 240, training loss: 7.579982280731201 = 1.3695975542068481 + 1.0 * 6.210384845733643
Epoch 240, val loss: 1.4530760049819946
Epoch 250, training loss: 7.511959552764893 = 1.3085585832595825 + 1.0 * 6.2034010887146
Epoch 250, val loss: 1.4054430723190308
Epoch 260, training loss: 7.446000099182129 = 1.2470769882202148 + 1.0 * 6.198923110961914
Epoch 260, val loss: 1.3578909635543823
Epoch 270, training loss: 7.3809380531311035 = 1.1870495080947876 + 1.0 * 6.1938886642456055
Epoch 270, val loss: 1.3115999698638916
Epoch 280, training loss: 7.317859172821045 = 1.1296348571777344 + 1.0 * 6.1882243156433105
Epoch 280, val loss: 1.2672842741012573
Epoch 290, training loss: 7.258316993713379 = 1.0756518840789795 + 1.0 * 6.1826653480529785
Epoch 290, val loss: 1.2255468368530273
Epoch 300, training loss: 7.201657295227051 = 1.0249154567718506 + 1.0 * 6.176742076873779
Epoch 300, val loss: 1.1864341497421265
Epoch 310, training loss: 7.153413772583008 = 0.9768482446670532 + 1.0 * 6.176565647125244
Epoch 310, val loss: 1.1493779420852661
Epoch 320, training loss: 7.10128116607666 = 0.9315321445465088 + 1.0 * 6.169748783111572
Epoch 320, val loss: 1.1144134998321533
Epoch 330, training loss: 7.051635265350342 = 0.8880454301834106 + 1.0 * 6.163589954376221
Epoch 330, val loss: 1.080978512763977
Epoch 340, training loss: 7.008929252624512 = 0.8458152413368225 + 1.0 * 6.163114070892334
Epoch 340, val loss: 1.048643946647644
Epoch 350, training loss: 6.9610137939453125 = 0.8051108121871948 + 1.0 * 6.155902862548828
Epoch 350, val loss: 1.0177026987075806
Epoch 360, training loss: 6.916957855224609 = 0.7658498287200928 + 1.0 * 6.1511077880859375
Epoch 360, val loss: 0.9880749583244324
Epoch 370, training loss: 6.875229358673096 = 0.7280007600784302 + 1.0 * 6.147228717803955
Epoch 370, val loss: 0.9597249627113342
Epoch 380, training loss: 6.846189975738525 = 0.6922228932380676 + 1.0 * 6.153966903686523
Epoch 380, val loss: 0.9333093762397766
Epoch 390, training loss: 6.7993083000183105 = 0.6590413451194763 + 1.0 * 6.1402668952941895
Epoch 390, val loss: 0.9094120264053345
Epoch 400, training loss: 6.765897274017334 = 0.6275699734687805 + 1.0 * 6.138327121734619
Epoch 400, val loss: 0.8875873684883118
Epoch 410, training loss: 6.732964992523193 = 0.5976029634475708 + 1.0 * 6.135362148284912
Epoch 410, val loss: 0.8676214814186096
Epoch 420, training loss: 6.702919006347656 = 0.5692122578620911 + 1.0 * 6.133706569671631
Epoch 420, val loss: 0.8496851325035095
Epoch 430, training loss: 6.671902179718018 = 0.5416349768638611 + 1.0 * 6.130267143249512
Epoch 430, val loss: 0.8331425189971924
Epoch 440, training loss: 6.640950679779053 = 0.5146578550338745 + 1.0 * 6.126292705535889
Epoch 440, val loss: 0.8176811933517456
Epoch 450, training loss: 6.613945484161377 = 0.48827192187309265 + 1.0 * 6.125673770904541
Epoch 450, val loss: 0.8034707903862
Epoch 460, training loss: 6.592930793762207 = 0.462357759475708 + 1.0 * 6.130573272705078
Epoch 460, val loss: 0.7902407050132751
Epoch 470, training loss: 6.559254169464111 = 0.4372258186340332 + 1.0 * 6.122028350830078
Epoch 470, val loss: 0.7781808972358704
Epoch 480, training loss: 6.530867099761963 = 0.41286155581474304 + 1.0 * 6.118005752563477
Epoch 480, val loss: 0.7673664689064026
Epoch 490, training loss: 6.507086277008057 = 0.38926786184310913 + 1.0 * 6.117818355560303
Epoch 490, val loss: 0.7576152086257935
Epoch 500, training loss: 6.487805366516113 = 0.36673179268836975 + 1.0 * 6.1210737228393555
Epoch 500, val loss: 0.7489673495292664
Epoch 510, training loss: 6.459079265594482 = 0.34544867277145386 + 1.0 * 6.113630771636963
Epoch 510, val loss: 0.7417766451835632
Epoch 520, training loss: 6.4360551834106445 = 0.32524722814559937 + 1.0 * 6.1108078956604
Epoch 520, val loss: 0.7357935309410095
Epoch 530, training loss: 6.421280860900879 = 0.30611419677734375 + 1.0 * 6.115166664123535
Epoch 530, val loss: 0.7309136390686035
Epoch 540, training loss: 6.396366119384766 = 0.2881487011909485 + 1.0 * 6.108217239379883
Epoch 540, val loss: 0.7272843718528748
Epoch 550, training loss: 6.381226062774658 = 0.2712933123111725 + 1.0 * 6.109932899475098
Epoch 550, val loss: 0.7247934937477112
Epoch 560, training loss: 6.362691402435303 = 0.2554999589920044 + 1.0 * 6.107191562652588
Epoch 560, val loss: 0.7232498526573181
Epoch 570, training loss: 6.34913444519043 = 0.2406771332025528 + 1.0 * 6.108457088470459
Epoch 570, val loss: 0.7226307392120361
Epoch 580, training loss: 6.334304332733154 = 0.22682327032089233 + 1.0 * 6.107481002807617
Epoch 580, val loss: 0.7226794362068176
Epoch 590, training loss: 6.314159870147705 = 0.2138935774564743 + 1.0 * 6.100266456604004
Epoch 590, val loss: 0.7236788272857666
Epoch 600, training loss: 6.299314022064209 = 0.20173659920692444 + 1.0 * 6.0975775718688965
Epoch 600, val loss: 0.7254185080528259
Epoch 610, training loss: 6.290213584899902 = 0.1902754306793213 + 1.0 * 6.09993839263916
Epoch 610, val loss: 0.7278135418891907
Epoch 620, training loss: 6.275595188140869 = 0.1795215755701065 + 1.0 * 6.096073627471924
Epoch 620, val loss: 0.7307388186454773
Epoch 630, training loss: 6.273065567016602 = 0.1694750189781189 + 1.0 * 6.103590488433838
Epoch 630, val loss: 0.7342515587806702
Epoch 640, training loss: 6.251994609832764 = 0.1600853055715561 + 1.0 * 6.091909408569336
Epoch 640, val loss: 0.7382228970527649
Epoch 650, training loss: 6.243293285369873 = 0.15127335488796234 + 1.0 * 6.092020034790039
Epoch 650, val loss: 0.7427226901054382
Epoch 660, training loss: 6.233859062194824 = 0.14300654828548431 + 1.0 * 6.090852737426758
Epoch 660, val loss: 0.7475681304931641
Epoch 670, training loss: 6.2230634689331055 = 0.13525493443012238 + 1.0 * 6.087808609008789
Epoch 670, val loss: 0.7528599500656128
Epoch 680, training loss: 6.22063684463501 = 0.12797591090202332 + 1.0 * 6.092660903930664
Epoch 680, val loss: 0.7584296464920044
Epoch 690, training loss: 6.209245681762695 = 0.12117777019739151 + 1.0 * 6.088068008422852
Epoch 690, val loss: 0.7641358375549316
Epoch 700, training loss: 6.198670387268066 = 0.1148320809006691 + 1.0 * 6.08383846282959
Epoch 700, val loss: 0.7702528834342957
Epoch 710, training loss: 6.190671443939209 = 0.10886278748512268 + 1.0 * 6.081808567047119
Epoch 710, val loss: 0.776551365852356
Epoch 720, training loss: 6.191988468170166 = 0.10324236750602722 + 1.0 * 6.088746070861816
Epoch 720, val loss: 0.7830399870872498
Epoch 730, training loss: 6.1883111000061035 = 0.09800974279642105 + 1.0 * 6.090301513671875
Epoch 730, val loss: 0.789482831954956
Epoch 740, training loss: 6.175890922546387 = 0.09314186125993729 + 1.0 * 6.082748889923096
Epoch 740, val loss: 0.7962801456451416
Epoch 750, training loss: 6.166727066040039 = 0.0885547325015068 + 1.0 * 6.078172206878662
Epoch 750, val loss: 0.8031452894210815
Epoch 760, training loss: 6.1627068519592285 = 0.08422267436981201 + 1.0 * 6.078484058380127
Epoch 760, val loss: 0.8100399971008301
Epoch 770, training loss: 6.156001091003418 = 0.08016374707221985 + 1.0 * 6.075837135314941
Epoch 770, val loss: 0.8169912099838257
Epoch 780, training loss: 6.1552910804748535 = 0.07635398209095001 + 1.0 * 6.07893705368042
Epoch 780, val loss: 0.8241268992424011
Epoch 790, training loss: 6.145702362060547 = 0.0727754458785057 + 1.0 * 6.072926998138428
Epoch 790, val loss: 0.8311970233917236
Epoch 800, training loss: 6.141903877258301 = 0.06941559910774231 + 1.0 * 6.072488307952881
Epoch 800, val loss: 0.8384152054786682
Epoch 810, training loss: 6.139341354370117 = 0.0662442147731781 + 1.0 * 6.073097229003906
Epoch 810, val loss: 0.845629870891571
Epoch 820, training loss: 6.133983612060547 = 0.0632668137550354 + 1.0 * 6.070716857910156
Epoch 820, val loss: 0.8526511192321777
Epoch 830, training loss: 6.134139537811279 = 0.06048199161887169 + 1.0 * 6.073657512664795
Epoch 830, val loss: 0.8598387241363525
Epoch 840, training loss: 6.127595901489258 = 0.057868536561727524 + 1.0 * 6.069727420806885
Epoch 840, val loss: 0.866968035697937
Epoch 850, training loss: 6.123826026916504 = 0.05539843440055847 + 1.0 * 6.068427562713623
Epoch 850, val loss: 0.8740568161010742
Epoch 860, training loss: 6.119664192199707 = 0.05305793881416321 + 1.0 * 6.066606044769287
Epoch 860, val loss: 0.8810928463935852
Epoch 870, training loss: 6.121673107147217 = 0.0508449412882328 + 1.0 * 6.070827960968018
Epoch 870, val loss: 0.88812655210495
Epoch 880, training loss: 6.119710922241211 = 0.04876279458403587 + 1.0 * 6.070948123931885
Epoch 880, val loss: 0.8948654532432556
Epoch 890, training loss: 6.1121015548706055 = 0.04682207480072975 + 1.0 * 6.065279483795166
Epoch 890, val loss: 0.9017864465713501
Epoch 900, training loss: 6.108205795288086 = 0.04498358070850372 + 1.0 * 6.063222408294678
Epoch 900, val loss: 0.9085517525672913
Epoch 910, training loss: 6.106529712677002 = 0.04323265701532364 + 1.0 * 6.063297271728516
Epoch 910, val loss: 0.9151732325553894
Epoch 920, training loss: 6.106657028198242 = 0.04157917946577072 + 1.0 * 6.065077781677246
Epoch 920, val loss: 0.9216197729110718
Epoch 930, training loss: 6.105070114135742 = 0.04002946615219116 + 1.0 * 6.065040588378906
Epoch 930, val loss: 0.9280975461006165
Epoch 940, training loss: 6.101966857910156 = 0.038567688316106796 + 1.0 * 6.063399314880371
Epoch 940, val loss: 0.9345112442970276
Epoch 950, training loss: 6.096240520477295 = 0.037177491933107376 + 1.0 * 6.059062957763672
Epoch 950, val loss: 0.9407958388328552
Epoch 960, training loss: 6.096493244171143 = 0.035853903740644455 + 1.0 * 6.060639381408691
Epoch 960, val loss: 0.9471257925033569
Epoch 970, training loss: 6.09395694732666 = 0.034593552350997925 + 1.0 * 6.05936336517334
Epoch 970, val loss: 0.9532384872436523
Epoch 980, training loss: 6.100284576416016 = 0.03340020403265953 + 1.0 * 6.066884517669678
Epoch 980, val loss: 0.9593207240104675
Epoch 990, training loss: 6.0889201164245605 = 0.032272644340991974 + 1.0 * 6.056647300720215
Epoch 990, val loss: 0.9652663469314575
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8487
Flip ASR: 0.8267/225 nodes
The final ASR:0.83518, 0.08341, Accuracy:0.81852, 0.01571
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11582])
remove edge: torch.Size([2, 9432])
updated graph: torch.Size([2, 10458])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.334175109863281 = 1.9603327512741089 + 1.0 * 8.373842239379883
Epoch 0, val loss: 1.9636049270629883
Epoch 10, training loss: 10.323769569396973 = 1.9504300355911255 + 1.0 * 8.373339653015137
Epoch 10, val loss: 1.9535900354385376
Epoch 20, training loss: 10.307804107666016 = 1.9376317262649536 + 1.0 * 8.370172500610352
Epoch 20, val loss: 1.9404937028884888
Epoch 30, training loss: 10.26905632019043 = 1.9191317558288574 + 1.0 * 8.349924087524414
Epoch 30, val loss: 1.9216792583465576
Epoch 40, training loss: 10.128859519958496 = 1.8952215909957886 + 1.0 * 8.233637809753418
Epoch 40, val loss: 1.8989983797073364
Epoch 50, training loss: 9.744446754455566 = 1.869933843612671 + 1.0 * 7.874513149261475
Epoch 50, val loss: 1.875758171081543
Epoch 60, training loss: 9.372786521911621 = 1.8455134630203247 + 1.0 * 7.527272701263428
Epoch 60, val loss: 1.8540765047073364
Epoch 70, training loss: 8.918439865112305 = 1.8267549276351929 + 1.0 * 7.091684818267822
Epoch 70, val loss: 1.8381117582321167
Epoch 80, training loss: 8.638174057006836 = 1.8105862140655518 + 1.0 * 6.827587604522705
Epoch 80, val loss: 1.8244708776474
Epoch 90, training loss: 8.452444076538086 = 1.791788935661316 + 1.0 * 6.6606550216674805
Epoch 90, val loss: 1.8087666034698486
Epoch 100, training loss: 8.344917297363281 = 1.7729368209838867 + 1.0 * 6.571979999542236
Epoch 100, val loss: 1.7929894924163818
Epoch 110, training loss: 8.272834777832031 = 1.754197359085083 + 1.0 * 6.518637657165527
Epoch 110, val loss: 1.7767671346664429
Epoch 120, training loss: 8.214116096496582 = 1.7346168756484985 + 1.0 * 6.479499340057373
Epoch 120, val loss: 1.759719729423523
Epoch 130, training loss: 8.160062789916992 = 1.7132995128631592 + 1.0 * 6.446763038635254
Epoch 130, val loss: 1.7416174411773682
Epoch 140, training loss: 8.105870246887207 = 1.6893575191497803 + 1.0 * 6.416512966156006
Epoch 140, val loss: 1.7219510078430176
Epoch 150, training loss: 8.049783706665039 = 1.661969542503357 + 1.0 * 6.387814044952393
Epoch 150, val loss: 1.6998335123062134
Epoch 160, training loss: 7.991053104400635 = 1.6304926872253418 + 1.0 * 6.360560417175293
Epoch 160, val loss: 1.6745245456695557
Epoch 170, training loss: 7.933960437774658 = 1.594643235206604 + 1.0 * 6.339317321777344
Epoch 170, val loss: 1.6460297107696533
Epoch 180, training loss: 7.872711181640625 = 1.554608941078186 + 1.0 * 6.3181023597717285
Epoch 180, val loss: 1.6142305135726929
Epoch 190, training loss: 7.809174537658691 = 1.5095425844192505 + 1.0 * 6.2996320724487305
Epoch 190, val loss: 1.5784239768981934
Epoch 200, training loss: 7.744073867797852 = 1.4593610763549805 + 1.0 * 6.284712791442871
Epoch 200, val loss: 1.5385652780532837
Epoch 210, training loss: 7.6825175285339355 = 1.404741644859314 + 1.0 * 6.277775764465332
Epoch 210, val loss: 1.4955267906188965
Epoch 220, training loss: 7.609435081481934 = 1.348081350326538 + 1.0 * 6.261353492736816
Epoch 220, val loss: 1.4512704610824585
Epoch 230, training loss: 7.5415825843811035 = 1.2895854711532593 + 1.0 * 6.251996994018555
Epoch 230, val loss: 1.4061814546585083
Epoch 240, training loss: 7.474270820617676 = 1.2300282716751099 + 1.0 * 6.2442426681518555
Epoch 240, val loss: 1.360955834388733
Epoch 250, training loss: 7.409477710723877 = 1.1720093488693237 + 1.0 * 6.237468242645264
Epoch 250, val loss: 1.3173561096191406
Epoch 260, training loss: 7.3463544845581055 = 1.1161093711853027 + 1.0 * 6.230245113372803
Epoch 260, val loss: 1.2762118577957153
Epoch 270, training loss: 7.286123275756836 = 1.062475323677063 + 1.0 * 6.2236480712890625
Epoch 270, val loss: 1.2371118068695068
Epoch 280, training loss: 7.22874641418457 = 1.0112385749816895 + 1.0 * 6.217507839202881
Epoch 280, val loss: 1.2001477479934692
Epoch 290, training loss: 7.177206516265869 = 0.9626578688621521 + 1.0 * 6.214548587799072
Epoch 290, val loss: 1.165663719177246
Epoch 300, training loss: 7.125864505767822 = 0.9174500703811646 + 1.0 * 6.208414554595947
Epoch 300, val loss: 1.1338621377944946
Epoch 310, training loss: 7.0764851570129395 = 0.8745254874229431 + 1.0 * 6.201959609985352
Epoch 310, val loss: 1.1042512655258179
Epoch 320, training loss: 7.0362162590026855 = 0.8335899710655212 + 1.0 * 6.2026262283325195
Epoch 320, val loss: 1.0765173435211182
Epoch 330, training loss: 6.986997604370117 = 0.7946426868438721 + 1.0 * 6.192354679107666
Epoch 330, val loss: 1.0508631467819214
Epoch 340, training loss: 6.945509910583496 = 0.757348358631134 + 1.0 * 6.188161373138428
Epoch 340, val loss: 1.026958703994751
Epoch 350, training loss: 6.909228801727295 = 0.7219290733337402 + 1.0 * 6.187299728393555
Epoch 350, val loss: 1.0044995546340942
Epoch 360, training loss: 6.866775989532471 = 0.688257098197937 + 1.0 * 6.178518772125244
Epoch 360, val loss: 0.9840343594551086
Epoch 370, training loss: 6.830386161804199 = 0.6559924483299255 + 1.0 * 6.174393653869629
Epoch 370, val loss: 0.9649115800857544
Epoch 380, training loss: 6.799317359924316 = 0.6248559951782227 + 1.0 * 6.174461364746094
Epoch 380, val loss: 0.946926474571228
Epoch 390, training loss: 6.7654805183410645 = 0.5949901342391968 + 1.0 * 6.170490264892578
Epoch 390, val loss: 0.9299569129943848
Epoch 400, training loss: 6.730588436126709 = 0.5663995146751404 + 1.0 * 6.164188861846924
Epoch 400, val loss: 0.9139915704727173
Epoch 410, training loss: 6.698230743408203 = 0.5386360883712769 + 1.0 * 6.159594535827637
Epoch 410, val loss: 0.8989314436912537
Epoch 420, training loss: 6.671547889709473 = 0.5116286277770996 + 1.0 * 6.159919261932373
Epoch 420, val loss: 0.8845192193984985
Epoch 430, training loss: 6.642285346984863 = 0.48556047677993774 + 1.0 * 6.15672492980957
Epoch 430, val loss: 0.8708028197288513
Epoch 440, training loss: 6.611363887786865 = 0.4602046608924866 + 1.0 * 6.151159286499023
Epoch 440, val loss: 0.8581296801567078
Epoch 450, training loss: 6.583940505981445 = 0.43558812141418457 + 1.0 * 6.14835262298584
Epoch 450, val loss: 0.8463578224182129
Epoch 460, training loss: 6.559219837188721 = 0.4116278290748596 + 1.0 * 6.147592067718506
Epoch 460, val loss: 0.8352854251861572
Epoch 470, training loss: 6.533047676086426 = 0.38839489221572876 + 1.0 * 6.144652843475342
Epoch 470, val loss: 0.8253024220466614
Epoch 480, training loss: 6.5070624351501465 = 0.36572563648223877 + 1.0 * 6.141336917877197
Epoch 480, val loss: 0.8163064122200012
Epoch 490, training loss: 6.482261657714844 = 0.34358882904052734 + 1.0 * 6.138672828674316
Epoch 490, val loss: 0.8081629872322083
Epoch 500, training loss: 6.457270622253418 = 0.32200488448143005 + 1.0 * 6.135265827178955
Epoch 500, val loss: 0.801081120967865
Epoch 510, training loss: 6.43817138671875 = 0.30102089047431946 + 1.0 * 6.137150287628174
Epoch 510, val loss: 0.7950869202613831
Epoch 520, training loss: 6.41142463684082 = 0.2808345556259155 + 1.0 * 6.130589962005615
Epoch 520, val loss: 0.7902539372444153
Epoch 530, training loss: 6.394881725311279 = 0.261441171169281 + 1.0 * 6.1334404945373535
Epoch 530, val loss: 0.7865313291549683
Epoch 540, training loss: 6.372298717498779 = 0.24297291040420532 + 1.0 * 6.129325866699219
Epoch 540, val loss: 0.7839415669441223
Epoch 550, training loss: 6.353442192077637 = 0.22562524676322937 + 1.0 * 6.127817153930664
Epoch 550, val loss: 0.7825217843055725
Epoch 560, training loss: 6.332556247711182 = 0.20943918824195862 + 1.0 * 6.123116970062256
Epoch 560, val loss: 0.7821090221405029
Epoch 570, training loss: 6.313378810882568 = 0.19443421065807343 + 1.0 * 6.1189446449279785
Epoch 570, val loss: 0.782835841178894
Epoch 580, training loss: 6.298821449279785 = 0.1805805116891861 + 1.0 * 6.118240833282471
Epoch 580, val loss: 0.7845739126205444
Epoch 590, training loss: 6.293529033660889 = 0.16793201863765717 + 1.0 * 6.12559700012207
Epoch 590, val loss: 0.7871803045272827
Epoch 600, training loss: 6.270322799682617 = 0.15641926229000092 + 1.0 * 6.113903522491455
Epoch 600, val loss: 0.790645182132721
Epoch 610, training loss: 6.258055210113525 = 0.14598974585533142 + 1.0 * 6.112065315246582
Epoch 610, val loss: 0.7950109839439392
Epoch 620, training loss: 6.2470197677612305 = 0.13651259243488312 + 1.0 * 6.110507011413574
Epoch 620, val loss: 0.8000207543373108
Epoch 630, training loss: 6.23770809173584 = 0.12791337072849274 + 1.0 * 6.109794616699219
Epoch 630, val loss: 0.8055834770202637
Epoch 640, training loss: 6.226473808288574 = 0.12015911936759949 + 1.0 * 6.106314659118652
Epoch 640, val loss: 0.8116900324821472
Epoch 650, training loss: 6.21914529800415 = 0.11309470236301422 + 1.0 * 6.106050491333008
Epoch 650, val loss: 0.8182551264762878
Epoch 660, training loss: 6.209500312805176 = 0.10666292905807495 + 1.0 * 6.102837562561035
Epoch 660, val loss: 0.8250581622123718
Epoch 670, training loss: 6.204519271850586 = 0.10080679506063461 + 1.0 * 6.103712558746338
Epoch 670, val loss: 0.8321890830993652
Epoch 680, training loss: 6.195652484893799 = 0.0954488143324852 + 1.0 * 6.100203514099121
Epoch 680, val loss: 0.8394232988357544
Epoch 690, training loss: 6.188357353210449 = 0.09055226296186447 + 1.0 * 6.097805023193359
Epoch 690, val loss: 0.8468409776687622
Epoch 700, training loss: 6.184645175933838 = 0.08603774011135101 + 1.0 * 6.098607540130615
Epoch 700, val loss: 0.8543682098388672
Epoch 710, training loss: 6.178628921508789 = 0.08188249170780182 + 1.0 * 6.096746444702148
Epoch 710, val loss: 0.861801266670227
Epoch 720, training loss: 6.172383785247803 = 0.07805541157722473 + 1.0 * 6.0943284034729
Epoch 720, val loss: 0.8694286346435547
Epoch 730, training loss: 6.167669296264648 = 0.0744980052113533 + 1.0 * 6.093171119689941
Epoch 730, val loss: 0.8770555853843689
Epoch 740, training loss: 6.165308952331543 = 0.07119113206863403 + 1.0 * 6.094117641448975
Epoch 740, val loss: 0.8845085501670837
Epoch 750, training loss: 6.1590895652771 = 0.06809723377227783 + 1.0 * 6.090992450714111
Epoch 750, val loss: 0.8920612335205078
Epoch 760, training loss: 6.154958724975586 = 0.06521210074424744 + 1.0 * 6.089746475219727
Epoch 760, val loss: 0.8995621800422668
Epoch 770, training loss: 6.158721446990967 = 0.06250403821468353 + 1.0 * 6.096217632293701
Epoch 770, val loss: 0.906936526298523
Epoch 780, training loss: 6.147716045379639 = 0.05997198820114136 + 1.0 * 6.087744235992432
Epoch 780, val loss: 0.9143018126487732
Epoch 790, training loss: 6.145313262939453 = 0.057586461305618286 + 1.0 * 6.087726593017578
Epoch 790, val loss: 0.9215964674949646
Epoch 800, training loss: 6.139066219329834 = 0.05534059926867485 + 1.0 * 6.083725452423096
Epoch 800, val loss: 0.9288205504417419
Epoch 810, training loss: 6.146142482757568 = 0.053214386105537415 + 1.0 * 6.092927932739258
Epoch 810, val loss: 0.9359996914863586
Epoch 820, training loss: 6.137590408325195 = 0.05120169371366501 + 1.0 * 6.08638858795166
Epoch 820, val loss: 0.9430287480354309
Epoch 830, training loss: 6.128761291503906 = 0.049297135323286057 + 1.0 * 6.079463958740234
Epoch 830, val loss: 0.9501309990882874
Epoch 840, training loss: 6.129264831542969 = 0.047483548521995544 + 1.0 * 6.081781387329102
Epoch 840, val loss: 0.9571772813796997
Epoch 850, training loss: 6.12338924407959 = 0.045763637870550156 + 1.0 * 6.077625751495361
Epoch 850, val loss: 0.9640111923217773
Epoch 860, training loss: 6.1221184730529785 = 0.04412756860256195 + 1.0 * 6.077991008758545
Epoch 860, val loss: 0.9709386825561523
Epoch 870, training loss: 6.11929988861084 = 0.04257198050618172 + 1.0 * 6.076727867126465
Epoch 870, val loss: 0.9776907563209534
Epoch 880, training loss: 6.115877628326416 = 0.04108571261167526 + 1.0 * 6.07479190826416
Epoch 880, val loss: 0.9844327569007874
Epoch 890, training loss: 6.113972187042236 = 0.03966328501701355 + 1.0 * 6.0743088722229
Epoch 890, val loss: 0.9911500215530396
Epoch 900, training loss: 6.118576526641846 = 0.03830400109291077 + 1.0 * 6.080272674560547
Epoch 900, val loss: 0.997748613357544
Epoch 910, training loss: 6.112613201141357 = 0.037013065069913864 + 1.0 * 6.0756001472473145
Epoch 910, val loss: 1.004228115081787
Epoch 920, training loss: 6.107479572296143 = 0.03578285500407219 + 1.0 * 6.071696758270264
Epoch 920, val loss: 1.0107260942459106
Epoch 930, training loss: 6.105360507965088 = 0.034606754779815674 + 1.0 * 6.070753574371338
Epoch 930, val loss: 1.0171546936035156
Epoch 940, training loss: 6.11230993270874 = 0.03348146006464958 + 1.0 * 6.07882833480835
Epoch 940, val loss: 1.023444652557373
Epoch 950, training loss: 6.103936672210693 = 0.032403260469436646 + 1.0 * 6.071533203125
Epoch 950, val loss: 1.0296940803527832
Epoch 960, training loss: 6.1005401611328125 = 0.0313771590590477 + 1.0 * 6.069162845611572
Epoch 960, val loss: 1.0359835624694824
Epoch 970, training loss: 6.102639198303223 = 0.030390694737434387 + 1.0 * 6.072248458862305
Epoch 970, val loss: 1.0421142578125
Epoch 980, training loss: 6.099478721618652 = 0.029451359063386917 + 1.0 * 6.0700273513793945
Epoch 980, val loss: 1.0481089353561401
Epoch 990, training loss: 6.094213008880615 = 0.028556685894727707 + 1.0 * 6.0656561851501465
Epoch 990, val loss: 1.0541423559188843
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6716
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.318314552307129 = 1.9444631338119507 + 1.0 * 8.373851776123047
Epoch 0, val loss: 1.9432947635650635
Epoch 10, training loss: 10.307879447937012 = 1.9344710111618042 + 1.0 * 8.373408317565918
Epoch 10, val loss: 1.93308424949646
Epoch 20, training loss: 10.292452812194824 = 1.922126054763794 + 1.0 * 8.37032699584961
Epoch 20, val loss: 1.92041015625
Epoch 30, training loss: 10.253934860229492 = 1.9048622846603394 + 1.0 * 8.349072456359863
Epoch 30, val loss: 1.9028801918029785
Epoch 40, training loss: 10.072699546813965 = 1.8832473754882812 + 1.0 * 8.189452171325684
Epoch 40, val loss: 1.8818135261535645
Epoch 50, training loss: 9.468154907226562 = 1.8604923486709595 + 1.0 * 7.607662200927734
Epoch 50, val loss: 1.8604645729064941
Epoch 60, training loss: 8.990007400512695 = 1.84361732006073 + 1.0 * 7.146390438079834
Epoch 60, val loss: 1.84496009349823
Epoch 70, training loss: 8.654930114746094 = 1.830443263053894 + 1.0 * 6.824487209320068
Epoch 70, val loss: 1.8323376178741455
Epoch 80, training loss: 8.488774299621582 = 1.8164947032928467 + 1.0 * 6.672279357910156
Epoch 80, val loss: 1.818869709968567
Epoch 90, training loss: 8.36630916595459 = 1.8006591796875 + 1.0 * 6.56564998626709
Epoch 90, val loss: 1.8039977550506592
Epoch 100, training loss: 8.284188270568848 = 1.784925103187561 + 1.0 * 6.499263286590576
Epoch 100, val loss: 1.7892062664031982
Epoch 110, training loss: 8.22236156463623 = 1.7695221900939941 + 1.0 * 6.452839374542236
Epoch 110, val loss: 1.774535059928894
Epoch 120, training loss: 8.17117977142334 = 1.7537764310836792 + 1.0 * 6.417403697967529
Epoch 120, val loss: 1.7597033977508545
Epoch 130, training loss: 8.124767303466797 = 1.7369816303253174 + 1.0 * 6.387785911560059
Epoch 130, val loss: 1.7444177865982056
Epoch 140, training loss: 8.079395294189453 = 1.718371868133545 + 1.0 * 6.361023426055908
Epoch 140, val loss: 1.728262186050415
Epoch 150, training loss: 8.033370971679688 = 1.6971147060394287 + 1.0 * 6.33625602722168
Epoch 150, val loss: 1.7103248834609985
Epoch 160, training loss: 7.988883972167969 = 1.6725250482559204 + 1.0 * 6.316359043121338
Epoch 160, val loss: 1.6899809837341309
Epoch 170, training loss: 7.939505577087402 = 1.6437056064605713 + 1.0 * 6.295799732208252
Epoch 170, val loss: 1.666623830795288
Epoch 180, training loss: 7.889359951019287 = 1.6098653078079224 + 1.0 * 6.279494762420654
Epoch 180, val loss: 1.6393603086471558
Epoch 190, training loss: 7.837802886962891 = 1.5702965259552002 + 1.0 * 6.267506122589111
Epoch 190, val loss: 1.6076570749282837
Epoch 200, training loss: 7.77987813949585 = 1.5250438451766968 + 1.0 * 6.254834175109863
Epoch 200, val loss: 1.5717048645019531
Epoch 210, training loss: 7.718317985534668 = 1.4739149808883667 + 1.0 * 6.244402885437012
Epoch 210, val loss: 1.5313177108764648
Epoch 220, training loss: 7.653227806091309 = 1.4174145460128784 + 1.0 * 6.235813140869141
Epoch 220, val loss: 1.4869794845581055
Epoch 230, training loss: 7.586010932922363 = 1.3573120832443237 + 1.0 * 6.22869873046875
Epoch 230, val loss: 1.4399893283843994
Epoch 240, training loss: 7.5166425704956055 = 1.2952945232391357 + 1.0 * 6.221348285675049
Epoch 240, val loss: 1.3916255235671997
Epoch 250, training loss: 7.448269844055176 = 1.2339941263198853 + 1.0 * 6.21427583694458
Epoch 250, val loss: 1.3442769050598145
Epoch 260, training loss: 7.382544040679932 = 1.1752740144729614 + 1.0 * 6.20727014541626
Epoch 260, val loss: 1.2989639043807983
Epoch 270, training loss: 7.320045471191406 = 1.119053602218628 + 1.0 * 6.200991630554199
Epoch 270, val loss: 1.2558231353759766
Epoch 280, training loss: 7.263882637023926 = 1.0662857294082642 + 1.0 * 6.197597026824951
Epoch 280, val loss: 1.2155396938323975
Epoch 290, training loss: 7.205386161804199 = 1.016693353652954 + 1.0 * 6.188692569732666
Epoch 290, val loss: 1.1779239177703857
Epoch 300, training loss: 7.159127235412598 = 0.9691800475120544 + 1.0 * 6.189947128295898
Epoch 300, val loss: 1.1421360969543457
Epoch 310, training loss: 7.105188369750977 = 0.9238221645355225 + 1.0 * 6.181365966796875
Epoch 310, val loss: 1.1084771156311035
Epoch 320, training loss: 7.054352283477783 = 0.8799851536750793 + 1.0 * 6.1743669509887695
Epoch 320, val loss: 1.0765175819396973
Epoch 330, training loss: 7.007367134094238 = 0.8371163606643677 + 1.0 * 6.17025089263916
Epoch 330, val loss: 1.0455354452133179
Epoch 340, training loss: 6.964146614074707 = 0.7954633235931396 + 1.0 * 6.168683052062988
Epoch 340, val loss: 1.0156097412109375
Epoch 350, training loss: 6.919381141662598 = 0.7553673386573792 + 1.0 * 6.164013862609863
Epoch 350, val loss: 0.9872618317604065
Epoch 360, training loss: 6.875181674957275 = 0.7167461514472961 + 1.0 * 6.158435344696045
Epoch 360, val loss: 0.9602581858634949
Epoch 370, training loss: 6.839719295501709 = 0.6795166730880737 + 1.0 * 6.160202503204346
Epoch 370, val loss: 0.9345458745956421
Epoch 380, training loss: 6.7983808517456055 = 0.6445590257644653 + 1.0 * 6.15382194519043
Epoch 380, val loss: 0.9109655618667603
Epoch 390, training loss: 6.759843349456787 = 0.611383318901062 + 1.0 * 6.1484599113464355
Epoch 390, val loss: 0.8894191980361938
Epoch 400, training loss: 6.727167129516602 = 0.5797228217124939 + 1.0 * 6.147444248199463
Epoch 400, val loss: 0.86954265832901
Epoch 410, training loss: 6.694540500640869 = 0.5496315360069275 + 1.0 * 6.144908905029297
Epoch 410, val loss: 0.8516895174980164
Epoch 420, training loss: 6.6613006591796875 = 0.521056056022644 + 1.0 * 6.140244483947754
Epoch 420, val loss: 0.8359406590461731
Epoch 430, training loss: 6.636014461517334 = 0.4938085675239563 + 1.0 * 6.142205715179443
Epoch 430, val loss: 0.8220251798629761
Epoch 440, training loss: 6.604013919830322 = 0.46797576546669006 + 1.0 * 6.136038303375244
Epoch 440, val loss: 0.8101497292518616
Epoch 450, training loss: 6.575287818908691 = 0.4435498118400574 + 1.0 * 6.131738185882568
Epoch 450, val loss: 0.8002939820289612
Epoch 460, training loss: 6.551163673400879 = 0.4202995300292969 + 1.0 * 6.130864143371582
Epoch 460, val loss: 0.7920837998390198
Epoch 470, training loss: 6.530914306640625 = 0.3983191251754761 + 1.0 * 6.132595062255859
Epoch 470, val loss: 0.7855209112167358
Epoch 480, training loss: 6.502466201782227 = 0.37754184007644653 + 1.0 * 6.124924182891846
Epoch 480, val loss: 0.7805513143539429
Epoch 490, training loss: 6.480495929718018 = 0.35787734389305115 + 1.0 * 6.122618675231934
Epoch 490, val loss: 0.7768732905387878
Epoch 500, training loss: 6.467835426330566 = 0.3391900062561035 + 1.0 * 6.128645420074463
Epoch 500, val loss: 0.7742702960968018
Epoch 510, training loss: 6.444713115692139 = 0.32168829441070557 + 1.0 * 6.123024940490723
Epoch 510, val loss: 0.7726894617080688
Epoch 520, training loss: 6.421693801879883 = 0.30504879355430603 + 1.0 * 6.116644859313965
Epoch 520, val loss: 0.7720581293106079
Epoch 530, training loss: 6.40291166305542 = 0.28915223479270935 + 1.0 * 6.113759517669678
Epoch 530, val loss: 0.7719175219535828
Epoch 540, training loss: 6.385771751403809 = 0.2738896608352661 + 1.0 * 6.111882209777832
Epoch 540, val loss: 0.7722271680831909
Epoch 550, training loss: 6.371499538421631 = 0.25924697518348694 + 1.0 * 6.112252712249756
Epoch 550, val loss: 0.772855818271637
Epoch 560, training loss: 6.354811668395996 = 0.24535112082958221 + 1.0 * 6.109460353851318
Epoch 560, val loss: 0.7738198041915894
Epoch 570, training loss: 6.339802265167236 = 0.23200486600399017 + 1.0 * 6.107797622680664
Epoch 570, val loss: 0.7750871181488037
Epoch 580, training loss: 6.324028968811035 = 0.21912793815135956 + 1.0 * 6.10490083694458
Epoch 580, val loss: 0.7763789892196655
Epoch 590, training loss: 6.323227405548096 = 0.206710547208786 + 1.0 * 6.116517066955566
Epoch 590, val loss: 0.7777978181838989
Epoch 600, training loss: 6.296039581298828 = 0.19488990306854248 + 1.0 * 6.101149559020996
Epoch 600, val loss: 0.7793802618980408
Epoch 610, training loss: 6.284097194671631 = 0.18365781009197235 + 1.0 * 6.100439548492432
Epoch 610, val loss: 0.7814624905586243
Epoch 620, training loss: 6.271372318267822 = 0.17298224568367004 + 1.0 * 6.098390102386475
Epoch 620, val loss: 0.7835981845855713
Epoch 630, training loss: 6.259524345397949 = 0.16285188496112823 + 1.0 * 6.096672534942627
Epoch 630, val loss: 0.7859843969345093
Epoch 640, training loss: 6.260501861572266 = 0.15332503616809845 + 1.0 * 6.107176780700684
Epoch 640, val loss: 0.7886552810668945
Epoch 650, training loss: 6.238157272338867 = 0.14443978667259216 + 1.0 * 6.093717575073242
Epoch 650, val loss: 0.7916726469993591
Epoch 660, training loss: 6.229712009429932 = 0.13617351651191711 + 1.0 * 6.093538284301758
Epoch 660, val loss: 0.7950407862663269
Epoch 670, training loss: 6.2287278175354 = 0.12851108610630035 + 1.0 * 6.100216865539551
Epoch 670, val loss: 0.7985840439796448
Epoch 680, training loss: 6.211545467376709 = 0.12142591923475266 + 1.0 * 6.090119361877441
Epoch 680, val loss: 0.802460789680481
Epoch 690, training loss: 6.203866004943848 = 0.1148655042052269 + 1.0 * 6.089000701904297
Epoch 690, val loss: 0.8066971898078918
Epoch 700, training loss: 6.197510242462158 = 0.10877368599176407 + 1.0 * 6.088736534118652
Epoch 700, val loss: 0.8110732436180115
Epoch 710, training loss: 6.189509868621826 = 0.10311763733625412 + 1.0 * 6.086392402648926
Epoch 710, val loss: 0.815613329410553
Epoch 720, training loss: 6.189164638519287 = 0.0978788509964943 + 1.0 * 6.091285705566406
Epoch 720, val loss: 0.8204342722892761
Epoch 730, training loss: 6.179136276245117 = 0.09302376210689545 + 1.0 * 6.0861124992370605
Epoch 730, val loss: 0.8253573775291443
Epoch 740, training loss: 6.171168327331543 = 0.08849542587995529 + 1.0 * 6.082673072814941
Epoch 740, val loss: 0.8304873704910278
Epoch 750, training loss: 6.165933132171631 = 0.08426147699356079 + 1.0 * 6.081671714782715
Epoch 750, val loss: 0.835696280002594
Epoch 760, training loss: 6.161490440368652 = 0.08030541986227036 + 1.0 * 6.0811848640441895
Epoch 760, val loss: 0.8408174514770508
Epoch 770, training loss: 6.156713485717773 = 0.07663418352603912 + 1.0 * 6.080079078674316
Epoch 770, val loss: 0.8463562726974487
Epoch 780, training loss: 6.151567459106445 = 0.07318262755870819 + 1.0 * 6.078384876251221
Epoch 780, val loss: 0.8519067168235779
Epoch 790, training loss: 6.147318363189697 = 0.06991216540336609 + 1.0 * 6.077406406402588
Epoch 790, val loss: 0.8573830723762512
Epoch 800, training loss: 6.145860195159912 = 0.0668150931596756 + 1.0 * 6.079045295715332
Epoch 800, val loss: 0.8628177046775818
Epoch 810, training loss: 6.141221523284912 = 0.06389963626861572 + 1.0 * 6.077322006225586
Epoch 810, val loss: 0.8683823347091675
Epoch 820, training loss: 6.134690284729004 = 0.06112247332930565 + 1.0 * 6.073567867279053
Epoch 820, val loss: 0.8740554451942444
Epoch 830, training loss: 6.133068561553955 = 0.05847866088151932 + 1.0 * 6.074589729309082
Epoch 830, val loss: 0.8795983791351318
Epoch 840, training loss: 6.128970623016357 = 0.055955685675144196 + 1.0 * 6.073014736175537
Epoch 840, val loss: 0.8850775957107544
Epoch 850, training loss: 6.1249098777771 = 0.05355362221598625 + 1.0 * 6.071356296539307
Epoch 850, val loss: 0.8908039331436157
Epoch 860, training loss: 6.12363338470459 = 0.051272060722112656 + 1.0 * 6.072361469268799
Epoch 860, val loss: 0.8964099884033203
Epoch 870, training loss: 6.120433807373047 = 0.04910091310739517 + 1.0 * 6.071332931518555
Epoch 870, val loss: 0.9020088911056519
Epoch 880, training loss: 6.115116119384766 = 0.047040145844221115 + 1.0 * 6.068076133728027
Epoch 880, val loss: 0.9075842499732971
Epoch 890, training loss: 6.113945960998535 = 0.0450773611664772 + 1.0 * 6.068868637084961
Epoch 890, val loss: 0.9132172465324402
Epoch 900, training loss: 6.110825061798096 = 0.043217580765485764 + 1.0 * 6.067607402801514
Epoch 900, val loss: 0.9187726378440857
Epoch 910, training loss: 6.109214782714844 = 0.04145386442542076 + 1.0 * 6.067760944366455
Epoch 910, val loss: 0.9244424104690552
Epoch 920, training loss: 6.104350566864014 = 0.03979006037116051 + 1.0 * 6.064560413360596
Epoch 920, val loss: 0.9300799369812012
Epoch 930, training loss: 6.107851505279541 = 0.038207873702049255 + 1.0 * 6.069643497467041
Epoch 930, val loss: 0.9356726408004761
Epoch 940, training loss: 6.1037678718566895 = 0.03671768307685852 + 1.0 * 6.067049980163574
Epoch 940, val loss: 0.9413076639175415
Epoch 950, training loss: 6.099676132202148 = 0.035324010998010635 + 1.0 * 6.064352035522461
Epoch 950, val loss: 0.9469543695449829
Epoch 960, training loss: 6.097115993499756 = 0.03401212394237518 + 1.0 * 6.063103675842285
Epoch 960, val loss: 0.9525489211082458
Epoch 970, training loss: 6.095917224884033 = 0.032777704298496246 + 1.0 * 6.06313943862915
Epoch 970, val loss: 0.958122193813324
Epoch 980, training loss: 6.091288089752197 = 0.03162095323204994 + 1.0 * 6.059667110443115
Epoch 980, val loss: 0.9637206196784973
Epoch 990, training loss: 6.088900566101074 = 0.030523426830768585 + 1.0 * 6.058377265930176
Epoch 990, val loss: 0.9693533182144165
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.7196
Flip ASR: 0.6667/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.311992645263672 = 1.9381792545318604 + 1.0 * 8.37381362915039
Epoch 0, val loss: 1.9310171604156494
Epoch 10, training loss: 10.300167083740234 = 1.927111029624939 + 1.0 * 8.373056411743164
Epoch 10, val loss: 1.9200615882873535
Epoch 20, training loss: 10.281620025634766 = 1.9128775596618652 + 1.0 * 8.368742942810059
Epoch 20, val loss: 1.9055180549621582
Epoch 30, training loss: 10.234419822692871 = 1.893000841140747 + 1.0 * 8.341419219970703
Epoch 30, val loss: 1.8853238821029663
Epoch 40, training loss: 10.011621475219727 = 1.8694349527359009 + 1.0 * 8.142186164855957
Epoch 40, val loss: 1.8625526428222656
Epoch 50, training loss: 9.367982864379883 = 1.846086859703064 + 1.0 * 7.521895885467529
Epoch 50, val loss: 1.8402436971664429
Epoch 60, training loss: 9.017192840576172 = 1.828529715538025 + 1.0 * 7.188663005828857
Epoch 60, val loss: 1.822715163230896
Epoch 70, training loss: 8.742696762084961 = 1.8121228218078613 + 1.0 * 6.930573463439941
Epoch 70, val loss: 1.806177020072937
Epoch 80, training loss: 8.523019790649414 = 1.7955552339553833 + 1.0 * 6.72746467590332
Epoch 80, val loss: 1.7901750802993774
Epoch 90, training loss: 8.375975608825684 = 1.7788927555084229 + 1.0 * 6.59708309173584
Epoch 90, val loss: 1.774233341217041
Epoch 100, training loss: 8.27061653137207 = 1.760936975479126 + 1.0 * 6.509679794311523
Epoch 100, val loss: 1.7569961547851562
Epoch 110, training loss: 8.189876556396484 = 1.7411525249481201 + 1.0 * 6.448723793029785
Epoch 110, val loss: 1.738478422164917
Epoch 120, training loss: 8.122217178344727 = 1.7193868160247803 + 1.0 * 6.402830123901367
Epoch 120, val loss: 1.7187364101409912
Epoch 130, training loss: 8.064754486083984 = 1.6948214769363403 + 1.0 * 6.369933128356934
Epoch 130, val loss: 1.6971029043197632
Epoch 140, training loss: 8.005205154418945 = 1.666807770729065 + 1.0 * 6.338397026062012
Epoch 140, val loss: 1.6732505559921265
Epoch 150, training loss: 7.948914527893066 = 1.6348469257354736 + 1.0 * 6.314067363739014
Epoch 150, val loss: 1.646610975265503
Epoch 160, training loss: 7.893857002258301 = 1.598229169845581 + 1.0 * 6.295627593994141
Epoch 160, val loss: 1.6165791749954224
Epoch 170, training loss: 7.83278751373291 = 1.5572268962860107 + 1.0 * 6.2755608558654785
Epoch 170, val loss: 1.5835528373718262
Epoch 180, training loss: 7.773616790771484 = 1.5121681690216064 + 1.0 * 6.261448860168457
Epoch 180, val loss: 1.5477150678634644
Epoch 190, training loss: 7.712018966674805 = 1.4647958278656006 + 1.0 * 6.247222900390625
Epoch 190, val loss: 1.5107626914978027
Epoch 200, training loss: 7.651345729827881 = 1.4162343740463257 + 1.0 * 6.235111236572266
Epoch 200, val loss: 1.4735809564590454
Epoch 210, training loss: 7.591965198516846 = 1.367172360420227 + 1.0 * 6.224792957305908
Epoch 210, val loss: 1.4369635581970215
Epoch 220, training loss: 7.537520885467529 = 1.3199390172958374 + 1.0 * 6.217581748962402
Epoch 220, val loss: 1.4030498266220093
Epoch 230, training loss: 7.481393337249756 = 1.2753138542175293 + 1.0 * 6.206079483032227
Epoch 230, val loss: 1.3719865083694458
Epoch 240, training loss: 7.43281364440918 = 1.2325468063354492 + 1.0 * 6.2002668380737305
Epoch 240, val loss: 1.3428120613098145
Epoch 250, training loss: 7.383339881896973 = 1.1914827823638916 + 1.0 * 6.191856861114502
Epoch 250, val loss: 1.315353274345398
Epoch 260, training loss: 7.336743354797363 = 1.1514817476272583 + 1.0 * 6.1852617263793945
Epoch 260, val loss: 1.2887552976608276
Epoch 270, training loss: 7.290327548980713 = 1.1111907958984375 + 1.0 * 6.179136753082275
Epoch 270, val loss: 1.2619215250015259
Epoch 280, training loss: 7.24868106842041 = 1.0700345039367676 + 1.0 * 6.178646564483643
Epoch 280, val loss: 1.234029769897461
Epoch 290, training loss: 7.1977105140686035 = 1.027998924255371 + 1.0 * 6.169711589813232
Epoch 290, val loss: 1.2049833536148071
Epoch 300, training loss: 7.148715496063232 = 0.984677255153656 + 1.0 * 6.164038181304932
Epoch 300, val loss: 1.1746022701263428
Epoch 310, training loss: 7.099727630615234 = 0.940147340297699 + 1.0 * 6.159580230712891
Epoch 310, val loss: 1.1429648399353027
Epoch 320, training loss: 7.057458877563477 = 0.8952569365501404 + 1.0 * 6.162201881408691
Epoch 320, val loss: 1.110757827758789
Epoch 330, training loss: 7.003357887268066 = 0.8511052131652832 + 1.0 * 6.152252674102783
Epoch 330, val loss: 1.0791195631027222
Epoch 340, training loss: 6.956121444702148 = 0.8077731132507324 + 1.0 * 6.148348331451416
Epoch 340, val loss: 1.0482102632522583
Epoch 350, training loss: 6.914007186889648 = 0.765640139579773 + 1.0 * 6.148366928100586
Epoch 350, val loss: 1.0185033082962036
Epoch 360, training loss: 6.869570255279541 = 0.7257028818130493 + 1.0 * 6.143867492675781
Epoch 360, val loss: 0.9906421899795532
Epoch 370, training loss: 6.829758644104004 = 0.6880919337272644 + 1.0 * 6.141666889190674
Epoch 370, val loss: 0.9651229381561279
Epoch 380, training loss: 6.788733005523682 = 0.652928352355957 + 1.0 * 6.135804653167725
Epoch 380, val loss: 0.941838264465332
Epoch 390, training loss: 6.753865718841553 = 0.6198850274085999 + 1.0 * 6.133980751037598
Epoch 390, val loss: 0.9207572937011719
Epoch 400, training loss: 6.720198154449463 = 0.5890964269638062 + 1.0 * 6.131101608276367
Epoch 400, val loss: 0.901696503162384
Epoch 410, training loss: 6.688401699066162 = 0.5602056384086609 + 1.0 * 6.1281962394714355
Epoch 410, val loss: 0.88450688123703
Epoch 420, training loss: 6.657783508300781 = 0.5330840945243835 + 1.0 * 6.124699592590332
Epoch 420, val loss: 0.868882417678833
Epoch 430, training loss: 6.632016658782959 = 0.5074397921562195 + 1.0 * 6.124577045440674
Epoch 430, val loss: 0.8546679615974426
Epoch 440, training loss: 6.6048665046691895 = 0.48313552141189575 + 1.0 * 6.121730804443359
Epoch 440, val loss: 0.8416873812675476
Epoch 450, training loss: 6.576879501342773 = 0.4598534405231476 + 1.0 * 6.117025852203369
Epoch 450, val loss: 0.8298380374908447
Epoch 460, training loss: 6.556179523468018 = 0.4374251365661621 + 1.0 * 6.1187543869018555
Epoch 460, val loss: 0.8187777996063232
Epoch 470, training loss: 6.530025959014893 = 0.4157913625240326 + 1.0 * 6.114234447479248
Epoch 470, val loss: 0.8084911108016968
Epoch 480, training loss: 6.503959655761719 = 0.39475661516189575 + 1.0 * 6.109202861785889
Epoch 480, val loss: 0.7989881038665771
Epoch 490, training loss: 6.4817633628845215 = 0.37422555685043335 + 1.0 * 6.107537746429443
Epoch 490, val loss: 0.7900063991546631
Epoch 500, training loss: 6.469729423522949 = 0.3542630672454834 + 1.0 * 6.115466594696045
Epoch 500, val loss: 0.7816437482833862
Epoch 510, training loss: 6.4396071434021 = 0.33516401052474976 + 1.0 * 6.104443073272705
Epoch 510, val loss: 0.7739304900169373
Epoch 520, training loss: 6.420806884765625 = 0.3167490065097809 + 1.0 * 6.104057788848877
Epoch 520, val loss: 0.7670007348060608
Epoch 530, training loss: 6.399190425872803 = 0.2990495264530182 + 1.0 * 6.1001410484313965
Epoch 530, val loss: 0.7608265280723572
Epoch 540, training loss: 6.391887664794922 = 0.2820627689361572 + 1.0 * 6.109825134277344
Epoch 540, val loss: 0.755391001701355
Epoch 550, training loss: 6.3634161949157715 = 0.2660908102989197 + 1.0 * 6.097325325012207
Epoch 550, val loss: 0.7507274150848389
Epoch 560, training loss: 6.346524715423584 = 0.2510093152523041 + 1.0 * 6.095515251159668
Epoch 560, val loss: 0.7470484972000122
Epoch 570, training loss: 6.330962657928467 = 0.2367601990699768 + 1.0 * 6.094202518463135
Epoch 570, val loss: 0.7442381978034973
Epoch 580, training loss: 6.33002233505249 = 0.22334153950214386 + 1.0 * 6.106680870056152
Epoch 580, val loss: 0.7422937154769897
Epoch 590, training loss: 6.303044319152832 = 0.21089187264442444 + 1.0 * 6.0921525955200195
Epoch 590, val loss: 0.7411375045776367
Epoch 600, training loss: 6.290004253387451 = 0.1993073672056198 + 1.0 * 6.090696811676025
Epoch 600, val loss: 0.7408505082130432
Epoch 610, training loss: 6.277464866638184 = 0.18852417171001434 + 1.0 * 6.088940620422363
Epoch 610, val loss: 0.7413340210914612
Epoch 620, training loss: 6.265506267547607 = 0.17842064797878265 + 1.0 * 6.087085723876953
Epoch 620, val loss: 0.7425335645675659
Epoch 630, training loss: 6.254396438598633 = 0.16894452273845673 + 1.0 * 6.085452079772949
Epoch 630, val loss: 0.7443010807037354
Epoch 640, training loss: 6.2486748695373535 = 0.1600608080625534 + 1.0 * 6.088613986968994
Epoch 640, val loss: 0.7465690970420837
Epoch 650, training loss: 6.236814022064209 = 0.15176713466644287 + 1.0 * 6.085046768188477
Epoch 650, val loss: 0.749215841293335
Epoch 660, training loss: 6.22483491897583 = 0.1439916044473648 + 1.0 * 6.080843448638916
Epoch 660, val loss: 0.7521460652351379
Epoch 670, training loss: 6.218025207519531 = 0.13667702674865723 + 1.0 * 6.081348419189453
Epoch 670, val loss: 0.755434513092041
Epoch 680, training loss: 6.2126240730285645 = 0.12981246411800385 + 1.0 * 6.0828118324279785
Epoch 680, val loss: 0.7589510679244995
Epoch 690, training loss: 6.204697132110596 = 0.12342221289873123 + 1.0 * 6.08127498626709
Epoch 690, val loss: 0.7625472545623779
Epoch 700, training loss: 6.194581508636475 = 0.11744536459445953 + 1.0 * 6.077136039733887
Epoch 700, val loss: 0.7661192417144775
Epoch 710, training loss: 6.187800884246826 = 0.1118188202381134 + 1.0 * 6.075982093811035
Epoch 710, val loss: 0.7699717879295349
Epoch 720, training loss: 6.180622100830078 = 0.10649988800287247 + 1.0 * 6.074122428894043
Epoch 720, val loss: 0.7740059494972229
Epoch 730, training loss: 6.178257465362549 = 0.10145418345928192 + 1.0 * 6.076803207397461
Epoch 730, val loss: 0.7781285047531128
Epoch 740, training loss: 6.171179294586182 = 0.09667730331420898 + 1.0 * 6.074501991271973
Epoch 740, val loss: 0.7822079658508301
Epoch 750, training loss: 6.164699077606201 = 0.09215987473726273 + 1.0 * 6.072539329528809
Epoch 750, val loss: 0.7861539721488953
Epoch 760, training loss: 6.157456398010254 = 0.08785757422447205 + 1.0 * 6.06959867477417
Epoch 760, val loss: 0.7901986837387085
Epoch 770, training loss: 6.166779518127441 = 0.08374187350273132 + 1.0 * 6.083037853240967
Epoch 770, val loss: 0.7943075299263
Epoch 780, training loss: 6.150830268859863 = 0.07982303202152252 + 1.0 * 6.071007251739502
Epoch 780, val loss: 0.7981261014938354
Epoch 790, training loss: 6.14397668838501 = 0.07606785744428635 + 1.0 * 6.067908763885498
Epoch 790, val loss: 0.8018125295639038
Epoch 800, training loss: 6.138725280761719 = 0.07243731617927551 + 1.0 * 6.066287994384766
Epoch 800, val loss: 0.8055845499038696
Epoch 810, training loss: 6.139288902282715 = 0.068905308842659 + 1.0 * 6.070383548736572
Epoch 810, val loss: 0.809321403503418
Epoch 820, training loss: 6.131369113922119 = 0.06549388915300369 + 1.0 * 6.065875053405762
Epoch 820, val loss: 0.8130142688751221
Epoch 830, training loss: 6.128891468048096 = 0.06216839328408241 + 1.0 * 6.066722869873047
Epoch 830, val loss: 0.8165496587753296
Epoch 840, training loss: 6.122870922088623 = 0.05893513560295105 + 1.0 * 6.06393575668335
Epoch 840, val loss: 0.8201479911804199
Epoch 850, training loss: 6.11810302734375 = 0.05582243204116821 + 1.0 * 6.062280654907227
Epoch 850, val loss: 0.8238607048988342
Epoch 860, training loss: 6.118524074554443 = 0.05288675054907799 + 1.0 * 6.065637111663818
Epoch 860, val loss: 0.827833354473114
Epoch 870, training loss: 6.110808372497559 = 0.05017450451850891 + 1.0 * 6.060633659362793
Epoch 870, val loss: 0.8320015668869019
Epoch 880, training loss: 6.109189033508301 = 0.047696586698293686 + 1.0 * 6.061492443084717
Epoch 880, val loss: 0.8363754749298096
Epoch 890, training loss: 6.104826927185059 = 0.045438073575496674 + 1.0 * 6.059388637542725
Epoch 890, val loss: 0.840918242931366
Epoch 900, training loss: 6.101168632507324 = 0.043366938829422 + 1.0 * 6.057801723480225
Epoch 900, val loss: 0.8455264568328857
Epoch 910, training loss: 6.097804546356201 = 0.04146070405840874 + 1.0 * 6.056344032287598
Epoch 910, val loss: 0.8501579761505127
Epoch 920, training loss: 6.101076126098633 = 0.03968391567468643 + 1.0 * 6.061392307281494
Epoch 920, val loss: 0.8548609018325806
Epoch 930, training loss: 6.097428798675537 = 0.03804413974285126 + 1.0 * 6.059384822845459
Epoch 930, val loss: 0.8595876097679138
Epoch 940, training loss: 6.091124534606934 = 0.03651652857661247 + 1.0 * 6.05460786819458
Epoch 940, val loss: 0.8640584349632263
Epoch 950, training loss: 6.088407039642334 = 0.03508024662733078 + 1.0 * 6.053326606750488
Epoch 950, val loss: 0.8686763644218445
Epoch 960, training loss: 6.099331855773926 = 0.03372734412550926 + 1.0 * 6.0656046867370605
Epoch 960, val loss: 0.8733328580856323
Epoch 970, training loss: 6.086802005767822 = 0.032464634627103806 + 1.0 * 6.054337501525879
Epoch 970, val loss: 0.8779438138008118
Epoch 980, training loss: 6.081940650939941 = 0.031271904706954956 + 1.0 * 6.050668716430664
Epoch 980, val loss: 0.8824070692062378
Epoch 990, training loss: 6.084285736083984 = 0.030137984082102776 + 1.0 * 6.054147720336914
Epoch 990, val loss: 0.8870548009872437
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.5055
Flip ASR: 0.4533/225 nodes
The final ASR:0.63223, 0.09170, Accuracy:0.78889, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9422])
updated graph: torch.Size([2, 10444])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00758, Accuracy:0.83580, 0.00972
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.317709922790527 = 1.9439290761947632 + 1.0 * 8.373781204223633
Epoch 0, val loss: 1.9475547075271606
Epoch 10, training loss: 10.307788848876953 = 1.9346615076065063 + 1.0 * 8.373126983642578
Epoch 10, val loss: 1.938330054283142
Epoch 20, training loss: 10.291657447814941 = 1.9233362674713135 + 1.0 * 8.368321418762207
Epoch 20, val loss: 1.9270925521850586
Epoch 30, training loss: 10.237811088562012 = 1.9076786041259766 + 1.0 * 8.330132484436035
Epoch 30, val loss: 1.911758542060852
Epoch 40, training loss: 9.93900203704834 = 1.889073371887207 + 1.0 * 8.049928665161133
Epoch 40, val loss: 1.8938610553741455
Epoch 50, training loss: 9.393182754516602 = 1.870114803314209 + 1.0 * 7.523068428039551
Epoch 50, val loss: 1.8764739036560059
Epoch 60, training loss: 8.925944328308105 = 1.8546689748764038 + 1.0 * 7.07127571105957
Epoch 60, val loss: 1.8618762493133545
Epoch 70, training loss: 8.642169952392578 = 1.842151403427124 + 1.0 * 6.800018787384033
Epoch 70, val loss: 1.8494712114334106
Epoch 80, training loss: 8.463955879211426 = 1.8290369510650635 + 1.0 * 6.634919166564941
Epoch 80, val loss: 1.83707857131958
Epoch 90, training loss: 8.339689254760742 = 1.8144019842147827 + 1.0 * 6.525287628173828
Epoch 90, val loss: 1.8240745067596436
Epoch 100, training loss: 8.252507209777832 = 1.7996970415115356 + 1.0 * 6.452810287475586
Epoch 100, val loss: 1.8108701705932617
Epoch 110, training loss: 8.185559272766113 = 1.7852965593338013 + 1.0 * 6.400262355804443
Epoch 110, val loss: 1.7974703311920166
Epoch 120, training loss: 8.132181167602539 = 1.770612359046936 + 1.0 * 6.361568927764893
Epoch 120, val loss: 1.783555269241333
Epoch 130, training loss: 8.08533000946045 = 1.7548447847366333 + 1.0 * 6.330484867095947
Epoch 130, val loss: 1.768963098526001
Epoch 140, training loss: 8.044190406799316 = 1.7373758554458618 + 1.0 * 6.306814193725586
Epoch 140, val loss: 1.7533721923828125
Epoch 150, training loss: 8.000755310058594 = 1.7174831628799438 + 1.0 * 6.2832722663879395
Epoch 150, val loss: 1.735992431640625
Epoch 160, training loss: 7.958853721618652 = 1.6942185163497925 + 1.0 * 6.26463508605957
Epoch 160, val loss: 1.7161860466003418
Epoch 170, training loss: 7.918647766113281 = 1.6668659448623657 + 1.0 * 6.251781940460205
Epoch 170, val loss: 1.693211555480957
Epoch 180, training loss: 7.870360374450684 = 1.6346913576126099 + 1.0 * 6.235669136047363
Epoch 180, val loss: 1.6661704778671265
Epoch 190, training loss: 7.820530414581299 = 1.596827507019043 + 1.0 * 6.223702907562256
Epoch 190, val loss: 1.6344070434570312
Epoch 200, training loss: 7.768232822418213 = 1.5530039072036743 + 1.0 * 6.215229034423828
Epoch 200, val loss: 1.5978906154632568
Epoch 210, training loss: 7.7089433670043945 = 1.5036500692367554 + 1.0 * 6.20529317855835
Epoch 210, val loss: 1.5569573640823364
Epoch 220, training loss: 7.646292686462402 = 1.4490692615509033 + 1.0 * 6.19722318649292
Epoch 220, val loss: 1.5120656490325928
Epoch 230, training loss: 7.5856242179870605 = 1.3910459280014038 + 1.0 * 6.194578170776367
Epoch 230, val loss: 1.4651544094085693
Epoch 240, training loss: 7.517460346221924 = 1.3322515487670898 + 1.0 * 6.185208797454834
Epoch 240, val loss: 1.417597770690918
Epoch 250, training loss: 7.451953887939453 = 1.2727941274642944 + 1.0 * 6.179159641265869
Epoch 250, val loss: 1.3701335191726685
Epoch 260, training loss: 7.389299392700195 = 1.2131553888320923 + 1.0 * 6.176144123077393
Epoch 260, val loss: 1.3233058452606201
Epoch 270, training loss: 7.324443817138672 = 1.1545926332473755 + 1.0 * 6.169851303100586
Epoch 270, val loss: 1.2777944803237915
Epoch 280, training loss: 7.2624077796936035 = 1.0975794792175293 + 1.0 * 6.164828300476074
Epoch 280, val loss: 1.2341886758804321
Epoch 290, training loss: 7.202138423919678 = 1.0426554679870605 + 1.0 * 6.159482955932617
Epoch 290, val loss: 1.192588210105896
Epoch 300, training loss: 7.145730972290039 = 0.9894930124282837 + 1.0 * 6.156238079071045
Epoch 300, val loss: 1.1527934074401855
Epoch 310, training loss: 7.0916643142700195 = 0.9384507536888123 + 1.0 * 6.1532135009765625
Epoch 310, val loss: 1.1145440340042114
Epoch 320, training loss: 7.037128925323486 = 0.8894649744033813 + 1.0 * 6.1476640701293945
Epoch 320, val loss: 1.0779354572296143
Epoch 330, training loss: 6.988938808441162 = 0.8424148559570312 + 1.0 * 6.146523952484131
Epoch 330, val loss: 1.042801856994629
Epoch 340, training loss: 6.938766002655029 = 0.7974374890327454 + 1.0 * 6.14132833480835
Epoch 340, val loss: 1.009598731994629
Epoch 350, training loss: 6.895399570465088 = 0.7547764778137207 + 1.0 * 6.140623092651367
Epoch 350, val loss: 0.9781551957130432
Epoch 360, training loss: 6.848688125610352 = 0.7144944667816162 + 1.0 * 6.1341938972473145
Epoch 360, val loss: 0.9490346312522888
Epoch 370, training loss: 6.806701183319092 = 0.6764893531799316 + 1.0 * 6.13021183013916
Epoch 370, val loss: 0.9220626354217529
Epoch 380, training loss: 6.77100944519043 = 0.640646755695343 + 1.0 * 6.130362510681152
Epoch 380, val loss: 0.8969786763191223
Epoch 390, training loss: 6.734996795654297 = 0.607444703578949 + 1.0 * 6.127552032470703
Epoch 390, val loss: 0.8741863965988159
Epoch 400, training loss: 6.699808597564697 = 0.5764985084533691 + 1.0 * 6.123310089111328
Epoch 400, val loss: 0.8539889454841614
Epoch 410, training loss: 6.667420387268066 = 0.5478166937828064 + 1.0 * 6.119603633880615
Epoch 410, val loss: 0.8358273506164551
Epoch 420, training loss: 6.640209197998047 = 0.5211778283119202 + 1.0 * 6.1190314292907715
Epoch 420, val loss: 0.8197793364524841
Epoch 430, training loss: 6.610441207885742 = 0.496474951505661 + 1.0 * 6.113966464996338
Epoch 430, val loss: 0.8056827187538147
Epoch 440, training loss: 6.587284564971924 = 0.4734988808631897 + 1.0 * 6.113785743713379
Epoch 440, val loss: 0.793330729007721
Epoch 450, training loss: 6.568648338317871 = 0.45222926139831543 + 1.0 * 6.116419315338135
Epoch 450, val loss: 0.7826990485191345
Epoch 460, training loss: 6.5416178703308105 = 0.43261241912841797 + 1.0 * 6.109005451202393
Epoch 460, val loss: 0.7737442851066589
Epoch 470, training loss: 6.518706321716309 = 0.4142923653125763 + 1.0 * 6.104413986206055
Epoch 470, val loss: 0.7661110162734985
Epoch 480, training loss: 6.509427547454834 = 0.3969864249229431 + 1.0 * 6.112441062927246
Epoch 480, val loss: 0.7595437169075012
Epoch 490, training loss: 6.481447696685791 = 0.3808499872684479 + 1.0 * 6.100597858428955
Epoch 490, val loss: 0.7538909316062927
Epoch 500, training loss: 6.46383810043335 = 0.3654753863811493 + 1.0 * 6.098362922668457
Epoch 500, val loss: 0.7493076324462891
Epoch 510, training loss: 6.446547031402588 = 0.350625216960907 + 1.0 * 6.095921993255615
Epoch 510, val loss: 0.7453408241271973
Epoch 520, training loss: 6.442076206207275 = 0.33612191677093506 + 1.0 * 6.105954170227051
Epoch 520, val loss: 0.741952121257782
Epoch 530, training loss: 6.414442539215088 = 0.322028785943985 + 1.0 * 6.092413902282715
Epoch 530, val loss: 0.739019513130188
Epoch 540, training loss: 6.4013471603393555 = 0.308001846075058 + 1.0 * 6.0933451652526855
Epoch 540, val loss: 0.7365687489509583
Epoch 550, training loss: 6.38621187210083 = 0.29403865337371826 + 1.0 * 6.092173099517822
Epoch 550, val loss: 0.7343811988830566
Epoch 560, training loss: 6.368027210235596 = 0.2801201641559601 + 1.0 * 6.087906837463379
Epoch 560, val loss: 0.7325937747955322
Epoch 570, training loss: 6.352710723876953 = 0.26616376638412476 + 1.0 * 6.086546897888184
Epoch 570, val loss: 0.7311577200889587
Epoch 580, training loss: 6.341067314147949 = 0.25225088000297546 + 1.0 * 6.0888166427612305
Epoch 580, val loss: 0.7300757169723511
Epoch 590, training loss: 6.32474946975708 = 0.23853154480457306 + 1.0 * 6.086217880249023
Epoch 590, val loss: 0.7295609712600708
Epoch 600, training loss: 6.3098225593566895 = 0.22506749629974365 + 1.0 * 6.084754943847656
Epoch 600, val loss: 0.7296332716941833
Epoch 610, training loss: 6.294762134552002 = 0.21199242770671844 + 1.0 * 6.082769870758057
Epoch 610, val loss: 0.7302958965301514
Epoch 620, training loss: 6.280548095703125 = 0.19945968687534332 + 1.0 * 6.081088542938232
Epoch 620, val loss: 0.7317034006118774
Epoch 630, training loss: 6.269818305969238 = 0.18748623132705688 + 1.0 * 6.082332134246826
Epoch 630, val loss: 0.7339181900024414
Epoch 640, training loss: 6.259024620056152 = 0.17619235813617706 + 1.0 * 6.082832336425781
Epoch 640, val loss: 0.7368848919868469
Epoch 650, training loss: 6.243331432342529 = 0.16564176976680756 + 1.0 * 6.0776896476745605
Epoch 650, val loss: 0.7404544949531555
Epoch 660, training loss: 6.231943607330322 = 0.15574437379837036 + 1.0 * 6.076199054718018
Epoch 660, val loss: 0.744748592376709
Epoch 670, training loss: 6.224663257598877 = 0.1464834213256836 + 1.0 * 6.078179836273193
Epoch 670, val loss: 0.7496130466461182
Epoch 680, training loss: 6.212910175323486 = 0.13789506256580353 + 1.0 * 6.075015068054199
Epoch 680, val loss: 0.7550029754638672
Epoch 690, training loss: 6.2087483406066895 = 0.12986628711223602 + 1.0 * 6.078882217407227
Epoch 690, val loss: 0.7608883380889893
Epoch 700, training loss: 6.196064472198486 = 0.1224188581109047 + 1.0 * 6.07364559173584
Epoch 700, val loss: 0.7670207619667053
Epoch 710, training loss: 6.18549919128418 = 0.11547981202602386 + 1.0 * 6.070019245147705
Epoch 710, val loss: 0.7735814452171326
Epoch 720, training loss: 6.180353164672852 = 0.10900851339101791 + 1.0 * 6.07134485244751
Epoch 720, val loss: 0.7804393172264099
Epoch 730, training loss: 6.170792102813721 = 0.10297268629074097 + 1.0 * 6.067819595336914
Epoch 730, val loss: 0.7874662280082703
Epoch 740, training loss: 6.1640825271606445 = 0.09735728800296783 + 1.0 * 6.066725254058838
Epoch 740, val loss: 0.7947567701339722
Epoch 750, training loss: 6.161405563354492 = 0.09210821241140366 + 1.0 * 6.0692973136901855
Epoch 750, val loss: 0.8022046089172363
Epoch 760, training loss: 6.156811714172363 = 0.08722474426031113 + 1.0 * 6.069586753845215
Epoch 760, val loss: 0.809750497341156
Epoch 770, training loss: 6.147736549377441 = 0.08267499506473541 + 1.0 * 6.065061569213867
Epoch 770, val loss: 0.8173123002052307
Epoch 780, training loss: 6.142128944396973 = 0.07844097912311554 + 1.0 * 6.063687801361084
Epoch 780, val loss: 0.8249875903129578
Epoch 790, training loss: 6.136202335357666 = 0.07446840405464172 + 1.0 * 6.061733722686768
Epoch 790, val loss: 0.8327191472053528
Epoch 800, training loss: 6.1395182609558105 = 0.07074669748544693 + 1.0 * 6.0687713623046875
Epoch 800, val loss: 0.8404881954193115
Epoch 810, training loss: 6.131445407867432 = 0.06727638840675354 + 1.0 * 6.064168930053711
Epoch 810, val loss: 0.8481966853141785
Epoch 820, training loss: 6.124375820159912 = 0.06403730809688568 + 1.0 * 6.060338497161865
Epoch 820, val loss: 0.8558732867240906
Epoch 830, training loss: 6.119272232055664 = 0.06099684163928032 + 1.0 * 6.05827522277832
Epoch 830, val loss: 0.8635144233703613
Epoch 840, training loss: 6.11552619934082 = 0.05813957378268242 + 1.0 * 6.05738639831543
Epoch 840, val loss: 0.871131420135498
Epoch 850, training loss: 6.121252059936523 = 0.055471669882535934 + 1.0 * 6.065780162811279
Epoch 850, val loss: 0.8786997199058533
Epoch 860, training loss: 6.112020015716553 = 0.052957769483327866 + 1.0 * 6.059062480926514
Epoch 860, val loss: 0.8861181139945984
Epoch 870, training loss: 6.105552673339844 = 0.050620924681425095 + 1.0 * 6.054931640625
Epoch 870, val loss: 0.8935328125953674
Epoch 880, training loss: 6.10272741317749 = 0.048413652926683426 + 1.0 * 6.054313659667969
Epoch 880, val loss: 0.90085768699646
Epoch 890, training loss: 6.107771873474121 = 0.04633260890841484 + 1.0 * 6.061439037322998
Epoch 890, val loss: 0.9080970287322998
Epoch 900, training loss: 6.09920597076416 = 0.04438527673482895 + 1.0 * 6.054820537567139
Epoch 900, val loss: 0.9152663350105286
Epoch 910, training loss: 6.10245418548584 = 0.042548004537820816 + 1.0 * 6.059906005859375
Epoch 910, val loss: 0.922347903251648
Epoch 920, training loss: 6.09445333480835 = 0.0408279150724411 + 1.0 * 6.053625583648682
Epoch 920, val loss: 0.9292114973068237
Epoch 930, training loss: 6.089996814727783 = 0.03920534625649452 + 1.0 * 6.050791263580322
Epoch 930, val loss: 0.9361215233802795
Epoch 940, training loss: 6.088291645050049 = 0.037666887044906616 + 1.0 * 6.050624847412109
Epoch 940, val loss: 0.9429450631141663
Epoch 950, training loss: 6.089829921722412 = 0.0362115316092968 + 1.0 * 6.053618431091309
Epoch 950, val loss: 0.949678897857666
Epoch 960, training loss: 6.085123538970947 = 0.03484169393777847 + 1.0 * 6.050282001495361
Epoch 960, val loss: 0.9562671780586243
Epoch 970, training loss: 6.081536769866943 = 0.03355255350470543 + 1.0 * 6.0479841232299805
Epoch 970, val loss: 0.9628239274024963
Epoch 980, training loss: 6.078268527984619 = 0.03232652321457863 + 1.0 * 6.0459418296813965
Epoch 980, val loss: 0.969309389591217
Epoch 990, training loss: 6.078289985656738 = 0.03116031177341938 + 1.0 * 6.0471296310424805
Epoch 990, val loss: 0.9757242798805237
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6642
Flip ASR: 0.6000/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.323576927185059 = 1.9497052431106567 + 1.0 * 8.373871803283691
Epoch 0, val loss: 1.9532549381256104
Epoch 10, training loss: 10.313325881958008 = 1.939777135848999 + 1.0 * 8.37354850769043
Epoch 10, val loss: 1.9425410032272339
Epoch 20, training loss: 10.29936408996582 = 1.9279230833053589 + 1.0 * 8.371440887451172
Epoch 20, val loss: 1.9295941591262817
Epoch 30, training loss: 10.269139289855957 = 1.9116935729980469 + 1.0 * 8.35744571685791
Epoch 30, val loss: 1.9116308689117432
Epoch 40, training loss: 10.154830932617188 = 1.8904322385787964 + 1.0 * 8.264398574829102
Epoch 40, val loss: 1.888935923576355
Epoch 50, training loss: 9.67365550994873 = 1.8688442707061768 + 1.0 * 7.804811477661133
Epoch 50, val loss: 1.8664038181304932
Epoch 60, training loss: 9.156881332397461 = 1.8508899211883545 + 1.0 * 7.305991172790527
Epoch 60, val loss: 1.8482295274734497
Epoch 70, training loss: 8.76566219329834 = 1.835389494895935 + 1.0 * 6.930273056030273
Epoch 70, val loss: 1.8323312997817993
Epoch 80, training loss: 8.53577709197998 = 1.8225953578948975 + 1.0 * 6.713181495666504
Epoch 80, val loss: 1.8187696933746338
Epoch 90, training loss: 8.397749900817871 = 1.8091453313827515 + 1.0 * 6.58860445022583
Epoch 90, val loss: 1.8041218519210815
Epoch 100, training loss: 8.297795295715332 = 1.7938369512557983 + 1.0 * 6.503958225250244
Epoch 100, val loss: 1.7884455919265747
Epoch 110, training loss: 8.224542617797852 = 1.7782224416732788 + 1.0 * 6.446320056915283
Epoch 110, val loss: 1.7728004455566406
Epoch 120, training loss: 8.16474437713623 = 1.7620288133621216 + 1.0 * 6.402715682983398
Epoch 120, val loss: 1.7567660808563232
Epoch 130, training loss: 8.115341186523438 = 1.744789719581604 + 1.0 * 6.370551586151123
Epoch 130, val loss: 1.7399393320083618
Epoch 140, training loss: 8.0675048828125 = 1.7258296012878418 + 1.0 * 6.341675758361816
Epoch 140, val loss: 1.722051739692688
Epoch 150, training loss: 8.02268123626709 = 1.7040780782699585 + 1.0 * 6.318603038787842
Epoch 150, val loss: 1.7020994424819946
Epoch 160, training loss: 7.980578422546387 = 1.6785268783569336 + 1.0 * 6.302051544189453
Epoch 160, val loss: 1.6793789863586426
Epoch 170, training loss: 7.9327898025512695 = 1.6484593152999878 + 1.0 * 6.284330368041992
Epoch 170, val loss: 1.6532996892929077
Epoch 180, training loss: 7.884265899658203 = 1.612952470779419 + 1.0 * 6.271313190460205
Epoch 180, val loss: 1.6230700016021729
Epoch 190, training loss: 7.831284999847412 = 1.5714402198791504 + 1.0 * 6.259844779968262
Epoch 190, val loss: 1.588115930557251
Epoch 200, training loss: 7.772354602813721 = 1.5227535963058472 + 1.0 * 6.249600887298584
Epoch 200, val loss: 1.5475971698760986
Epoch 210, training loss: 7.706830978393555 = 1.4667565822601318 + 1.0 * 6.240074634552002
Epoch 210, val loss: 1.5015509128570557
Epoch 220, training loss: 7.637021064758301 = 1.4050952196121216 + 1.0 * 6.231925964355469
Epoch 220, val loss: 1.4520514011383057
Epoch 230, training loss: 7.566733360290527 = 1.3411691188812256 + 1.0 * 6.225564002990723
Epoch 230, val loss: 1.4017213582992554
Epoch 240, training loss: 7.4931440353393555 = 1.2765607833862305 + 1.0 * 6.216583251953125
Epoch 240, val loss: 1.351519227027893
Epoch 250, training loss: 7.433874607086182 = 1.2129493951797485 + 1.0 * 6.220925331115723
Epoch 250, val loss: 1.3033192157745361
Epoch 260, training loss: 7.35612678527832 = 1.1536948680877686 + 1.0 * 6.202431678771973
Epoch 260, val loss: 1.2591934204101562
Epoch 270, training loss: 7.294196605682373 = 1.09771728515625 + 1.0 * 6.196479320526123
Epoch 270, val loss: 1.2185064554214478
Epoch 280, training loss: 7.235830307006836 = 1.0447707176208496 + 1.0 * 6.191059589385986
Epoch 280, val loss: 1.1805576086044312
Epoch 290, training loss: 7.179610252380371 = 0.9949654936790466 + 1.0 * 6.18464469909668
Epoch 290, val loss: 1.1455097198486328
Epoch 300, training loss: 7.126110553741455 = 0.9475558400154114 + 1.0 * 6.178554534912109
Epoch 300, val loss: 1.1126545667648315
Epoch 310, training loss: 7.07830810546875 = 0.902780294418335 + 1.0 * 6.175528049468994
Epoch 310, val loss: 1.081722617149353
Epoch 320, training loss: 7.029172420501709 = 0.860662043094635 + 1.0 * 6.168510437011719
Epoch 320, val loss: 1.0532028675079346
Epoch 330, training loss: 6.984747409820557 = 0.8208510279655457 + 1.0 * 6.163896560668945
Epoch 330, val loss: 1.0266659259796143
Epoch 340, training loss: 6.945989608764648 = 0.7837406992912292 + 1.0 * 6.1622490882873535
Epoch 340, val loss: 1.0023102760314941
Epoch 350, training loss: 6.9058732986450195 = 0.7497286200523376 + 1.0 * 6.156144618988037
Epoch 350, val loss: 0.980629026889801
Epoch 360, training loss: 6.868496894836426 = 0.7182450890541077 + 1.0 * 6.150251865386963
Epoch 360, val loss: 0.9612870812416077
Epoch 370, training loss: 6.836340427398682 = 0.688967764377594 + 1.0 * 6.147372722625732
Epoch 370, val loss: 0.9441129565238953
Epoch 380, training loss: 6.810952663421631 = 0.6620249152183533 + 1.0 * 6.148927688598633
Epoch 380, val loss: 0.9290944337844849
Epoch 390, training loss: 6.777987957000732 = 0.6371297240257263 + 1.0 * 6.140858173370361
Epoch 390, val loss: 0.9161445498466492
Epoch 400, training loss: 6.749093532562256 = 0.613511860370636 + 1.0 * 6.1355814933776855
Epoch 400, val loss: 0.9045014381408691
Epoch 410, training loss: 6.734805583953857 = 0.5907523036003113 + 1.0 * 6.1440534591674805
Epoch 410, val loss: 0.8937601447105408
Epoch 420, training loss: 6.697909355163574 = 0.5685938000679016 + 1.0 * 6.129315376281738
Epoch 420, val loss: 0.8838284015655518
Epoch 430, training loss: 6.673210620880127 = 0.5467727780342102 + 1.0 * 6.126437664031982
Epoch 430, val loss: 0.87441086769104
Epoch 440, training loss: 6.655988693237305 = 0.5249956250190735 + 1.0 * 6.130992889404297
Epoch 440, val loss: 0.8653636574745178
Epoch 450, training loss: 6.625248908996582 = 0.5032667517662048 + 1.0 * 6.121982097625732
Epoch 450, val loss: 0.8567987680435181
Epoch 460, training loss: 6.600570201873779 = 0.4814731478691101 + 1.0 * 6.1190972328186035
Epoch 460, val loss: 0.8486968278884888
Epoch 470, training loss: 6.5819854736328125 = 0.4596670866012573 + 1.0 * 6.122318267822266
Epoch 470, val loss: 0.841033935546875
Epoch 480, training loss: 6.553962707519531 = 0.43818333745002747 + 1.0 * 6.115779399871826
Epoch 480, val loss: 0.8340970873832703
Epoch 490, training loss: 6.530145645141602 = 0.4168519973754883 + 1.0 * 6.113293647766113
Epoch 490, val loss: 0.828041672706604
Epoch 500, training loss: 6.5058817863464355 = 0.3956455588340759 + 1.0 * 6.110236167907715
Epoch 500, val loss: 0.8227852582931519
Epoch 510, training loss: 6.483461856842041 = 0.3746781647205353 + 1.0 * 6.108783721923828
Epoch 510, val loss: 0.8183207511901855
Epoch 520, training loss: 6.467844486236572 = 0.3541211187839508 + 1.0 * 6.113723278045654
Epoch 520, val loss: 0.8145959973335266
Epoch 530, training loss: 6.443642616271973 = 0.33429011702537537 + 1.0 * 6.1093525886535645
Epoch 530, val loss: 0.811846137046814
Epoch 540, training loss: 6.419827938079834 = 0.314970463514328 + 1.0 * 6.104857444763184
Epoch 540, val loss: 0.8100093603134155
Epoch 550, training loss: 6.40833044052124 = 0.2962692975997925 + 1.0 * 6.112061023712158
Epoch 550, val loss: 0.8089413046836853
Epoch 560, training loss: 6.3790974617004395 = 0.278423935174942 + 1.0 * 6.100673675537109
Epoch 560, val loss: 0.8084650039672852
Epoch 570, training loss: 6.361371994018555 = 0.26145562529563904 + 1.0 * 6.099916458129883
Epoch 570, val loss: 0.8087809085845947
Epoch 580, training loss: 6.352344036102295 = 0.24535129964351654 + 1.0 * 6.106992721557617
Epoch 580, val loss: 0.8097440600395203
Epoch 590, training loss: 6.3287034034729 = 0.23026560246944427 + 1.0 * 6.098437786102295
Epoch 590, val loss: 0.8114455938339233
Epoch 600, training loss: 6.3110504150390625 = 0.21616949141025543 + 1.0 * 6.094881057739258
Epoch 600, val loss: 0.8138381242752075
Epoch 610, training loss: 6.2960028648376465 = 0.20308342576026917 + 1.0 * 6.09291934967041
Epoch 610, val loss: 0.8168123364448547
Epoch 620, training loss: 6.283206939697266 = 0.1910107284784317 + 1.0 * 6.092195987701416
Epoch 620, val loss: 0.8204048871994019
Epoch 630, training loss: 6.270346641540527 = 0.1798102706670761 + 1.0 * 6.090536594390869
Epoch 630, val loss: 0.8246182203292847
Epoch 640, training loss: 6.2578125 = 0.16939522325992584 + 1.0 * 6.088417053222656
Epoch 640, val loss: 0.8293023109436035
Epoch 650, training loss: 6.246831893920898 = 0.15971167385578156 + 1.0 * 6.087120056152344
Epoch 650, val loss: 0.834408700466156
Epoch 660, training loss: 6.249785423278809 = 0.15071488916873932 + 1.0 * 6.0990705490112305
Epoch 660, val loss: 0.8398975729942322
Epoch 670, training loss: 6.232044219970703 = 0.14244620501995087 + 1.0 * 6.089598178863525
Epoch 670, val loss: 0.8456814885139465
Epoch 680, training loss: 6.219888687133789 = 0.13475564122200012 + 1.0 * 6.085133075714111
Epoch 680, val loss: 0.8518111109733582
Epoch 690, training loss: 6.210662841796875 = 0.1275629997253418 + 1.0 * 6.083099842071533
Epoch 690, val loss: 0.8582050204277039
Epoch 700, training loss: 6.209837913513184 = 0.12084756791591644 + 1.0 * 6.088990211486816
Epoch 700, val loss: 0.8647212982177734
Epoch 710, training loss: 6.1978607177734375 = 0.1145569309592247 + 1.0 * 6.083303928375244
Epoch 710, val loss: 0.871400773525238
Epoch 720, training loss: 6.19722843170166 = 0.10870027542114258 + 1.0 * 6.088528156280518
Epoch 720, val loss: 0.8782473802566528
Epoch 730, training loss: 6.181413173675537 = 0.1032157763838768 + 1.0 * 6.078197479248047
Epoch 730, val loss: 0.8852524161338806
Epoch 740, training loss: 6.177201271057129 = 0.09807469695806503 + 1.0 * 6.079126358032227
Epoch 740, val loss: 0.8924015760421753
Epoch 750, training loss: 6.170940399169922 = 0.09324809163808823 + 1.0 * 6.07769250869751
Epoch 750, val loss: 0.899558961391449
Epoch 760, training loss: 6.163506984710693 = 0.08871503919363022 + 1.0 * 6.07479190826416
Epoch 760, val loss: 0.906819224357605
Epoch 770, training loss: 6.15858268737793 = 0.08444802463054657 + 1.0 * 6.074134826660156
Epoch 770, val loss: 0.914185106754303
Epoch 780, training loss: 6.169610023498535 = 0.08043733239173889 + 1.0 * 6.089172840118408
Epoch 780, val loss: 0.921497106552124
Epoch 790, training loss: 6.149631023406982 = 0.07666642963886261 + 1.0 * 6.072964668273926
Epoch 790, val loss: 0.9287586808204651
Epoch 800, training loss: 6.144131183624268 = 0.07312817126512527 + 1.0 * 6.071002960205078
Epoch 800, val loss: 0.9362285137176514
Epoch 810, training loss: 6.140071868896484 = 0.06979360431432724 + 1.0 * 6.070278167724609
Epoch 810, val loss: 0.9437330961227417
Epoch 820, training loss: 6.14248514175415 = 0.06665142625570297 + 1.0 * 6.075833797454834
Epoch 820, val loss: 0.9511326551437378
Epoch 830, training loss: 6.131554126739502 = 0.06370626389980316 + 1.0 * 6.067847728729248
Epoch 830, val loss: 0.9585302472114563
Epoch 840, training loss: 6.127396106719971 = 0.06092797964811325 + 1.0 * 6.066468238830566
Epoch 840, val loss: 0.9659937024116516
Epoch 850, training loss: 6.1258039474487305 = 0.05830208212137222 + 1.0 * 6.067502021789551
Epoch 850, val loss: 0.9734682440757751
Epoch 860, training loss: 6.121222019195557 = 0.05583009868860245 + 1.0 * 6.065392017364502
Epoch 860, val loss: 0.980774462223053
Epoch 870, training loss: 6.123780250549316 = 0.05351194366812706 + 1.0 * 6.070268154144287
Epoch 870, val loss: 0.9881336092948914
Epoch 880, training loss: 6.114068508148193 = 0.051323410123586655 + 1.0 * 6.062745094299316
Epoch 880, val loss: 0.9954571723937988
Epoch 890, training loss: 6.111400127410889 = 0.049254659563302994 + 1.0 * 6.062145233154297
Epoch 890, val loss: 1.0027827024459839
Epoch 900, training loss: 6.109467506408691 = 0.04729440063238144 + 1.0 * 6.062172889709473
Epoch 900, val loss: 1.0100640058517456
Epoch 910, training loss: 6.114022731781006 = 0.04543938860297203 + 1.0 * 6.0685834884643555
Epoch 910, val loss: 1.0172085762023926
Epoch 920, training loss: 6.103461265563965 = 0.043695252388715744 + 1.0 * 6.059765815734863
Epoch 920, val loss: 1.0243136882781982
Epoch 930, training loss: 6.100646495819092 = 0.04203949123620987 + 1.0 * 6.05860710144043
Epoch 930, val loss: 1.031510353088379
Epoch 940, training loss: 6.099254131317139 = 0.040467411279678345 + 1.0 * 6.058786869049072
Epoch 940, val loss: 1.038624882698059
Epoch 950, training loss: 6.104996681213379 = 0.038975149393081665 + 1.0 * 6.06602144241333
Epoch 950, val loss: 1.045593023300171
Epoch 960, training loss: 6.098829746246338 = 0.037559594959020615 + 1.0 * 6.061270236968994
Epoch 960, val loss: 1.0523960590362549
Epoch 970, training loss: 6.093308448791504 = 0.03622594103217125 + 1.0 * 6.057082653045654
Epoch 970, val loss: 1.059210181236267
Epoch 980, training loss: 6.089725494384766 = 0.03495365008711815 + 1.0 * 6.054771900177002
Epoch 980, val loss: 1.066058874130249
Epoch 990, training loss: 6.094559669494629 = 0.03373928740620613 + 1.0 * 6.060820579528809
Epoch 990, val loss: 1.0727325677871704
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.7232
Flip ASR: 0.6844/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32686996459961 = 1.9529857635498047 + 1.0 * 8.373884201049805
Epoch 0, val loss: 1.9504096508026123
Epoch 10, training loss: 10.316433906555176 = 1.9429256916046143 + 1.0 * 8.37350845336914
Epoch 10, val loss: 1.9408541917800903
Epoch 20, training loss: 10.301483154296875 = 1.9303027391433716 + 1.0 * 8.371180534362793
Epoch 20, val loss: 1.9285054206848145
Epoch 30, training loss: 10.267965316772461 = 1.912637710571289 + 1.0 * 8.355327606201172
Epoch 30, val loss: 1.911001205444336
Epoch 40, training loss: 10.133384704589844 = 1.8892927169799805 + 1.0 * 8.244091987609863
Epoch 40, val loss: 1.8886438608169556
Epoch 50, training loss: 9.417009353637695 = 1.8637559413909912 + 1.0 * 7.553253650665283
Epoch 50, val loss: 1.8643734455108643
Epoch 60, training loss: 9.045402526855469 = 1.8409532308578491 + 1.0 * 7.204449653625488
Epoch 60, val loss: 1.8429639339447021
Epoch 70, training loss: 8.776113510131836 = 1.8215539455413818 + 1.0 * 6.954559803009033
Epoch 70, val loss: 1.8240716457366943
Epoch 80, training loss: 8.603423118591309 = 1.8007068634033203 + 1.0 * 6.802716255187988
Epoch 80, val loss: 1.8047627210617065
Epoch 90, training loss: 8.472792625427246 = 1.780254602432251 + 1.0 * 6.692537784576416
Epoch 90, val loss: 1.785764217376709
Epoch 100, training loss: 8.373089790344238 = 1.759374737739563 + 1.0 * 6.613715171813965
Epoch 100, val loss: 1.7668613195419312
Epoch 110, training loss: 8.287864685058594 = 1.7379577159881592 + 1.0 * 6.5499067306518555
Epoch 110, val loss: 1.7476439476013184
Epoch 120, training loss: 8.206972122192383 = 1.7154982089996338 + 1.0 * 6.49147367477417
Epoch 120, val loss: 1.7275255918502808
Epoch 130, training loss: 8.130536079406738 = 1.6907496452331543 + 1.0 * 6.439786434173584
Epoch 130, val loss: 1.7052351236343384
Epoch 140, training loss: 8.067094802856445 = 1.662243366241455 + 1.0 * 6.404850959777832
Epoch 140, val loss: 1.680497407913208
Epoch 150, training loss: 7.993310928344727 = 1.6307148933410645 + 1.0 * 6.362596035003662
Epoch 150, val loss: 1.653964877128601
Epoch 160, training loss: 7.926382064819336 = 1.5947027206420898 + 1.0 * 6.331679344177246
Epoch 160, val loss: 1.6244429349899292
Epoch 170, training loss: 7.860403537750244 = 1.5530933141708374 + 1.0 * 6.307310104370117
Epoch 170, val loss: 1.5907014608383179
Epoch 180, training loss: 7.792871475219727 = 1.505263090133667 + 1.0 * 6.2876081466674805
Epoch 180, val loss: 1.5520871877670288
Epoch 190, training loss: 7.72332239151001 = 1.4514670372009277 + 1.0 * 6.271855354309082
Epoch 190, val loss: 1.5091599225997925
Epoch 200, training loss: 7.653981685638428 = 1.3943625688552856 + 1.0 * 6.259619235992432
Epoch 200, val loss: 1.4645432233810425
Epoch 210, training loss: 7.584043502807617 = 1.3360531330108643 + 1.0 * 6.247990608215332
Epoch 210, val loss: 1.4193649291992188
Epoch 220, training loss: 7.514555931091309 = 1.2771079540252686 + 1.0 * 6.237448215484619
Epoch 220, val loss: 1.3744579553604126
Epoch 230, training loss: 7.448760032653809 = 1.218578577041626 + 1.0 * 6.230181694030762
Epoch 230, val loss: 1.3305798768997192
Epoch 240, training loss: 7.382693290710449 = 1.1623092889785767 + 1.0 * 6.220384120941162
Epoch 240, val loss: 1.2890868186950684
Epoch 250, training loss: 7.320345878601074 = 1.1075117588043213 + 1.0 * 6.212834358215332
Epoch 250, val loss: 1.2490803003311157
Epoch 260, training loss: 7.262111663818359 = 1.0536977052688599 + 1.0 * 6.208414077758789
Epoch 260, val loss: 1.2101476192474365
Epoch 270, training loss: 7.201818466186523 = 1.0012236833572388 + 1.0 * 6.200594902038574
Epoch 270, val loss: 1.1724038124084473
Epoch 280, training loss: 7.144411563873291 = 0.9498942494392395 + 1.0 * 6.194517135620117
Epoch 280, val loss: 1.1355912685394287
Epoch 290, training loss: 7.088856220245361 = 0.8996235132217407 + 1.0 * 6.18923282623291
Epoch 290, val loss: 1.0994107723236084
Epoch 300, training loss: 7.03870964050293 = 0.8516736626625061 + 1.0 * 6.187036037445068
Epoch 300, val loss: 1.064831018447876
Epoch 310, training loss: 6.986509799957275 = 0.8063498139381409 + 1.0 * 6.180160045623779
Epoch 310, val loss: 1.032278060913086
Epoch 320, training loss: 6.938147068023682 = 0.7632384896278381 + 1.0 * 6.174908638000488
Epoch 320, val loss: 1.0012288093566895
Epoch 330, training loss: 6.896406650543213 = 0.7221856713294983 + 1.0 * 6.174221038818359
Epoch 330, val loss: 0.971738338470459
Epoch 340, training loss: 6.851218223571777 = 0.6835053563117981 + 1.0 * 6.167712688446045
Epoch 340, val loss: 0.9442945122718811
Epoch 350, training loss: 6.80928373336792 = 0.6470096707344055 + 1.0 * 6.16227388381958
Epoch 350, val loss: 0.9188368320465088
Epoch 360, training loss: 6.771369934082031 = 0.6123315095901489 + 1.0 * 6.159038543701172
Epoch 360, val loss: 0.8951738476753235
Epoch 370, training loss: 6.742431640625 = 0.579820454120636 + 1.0 * 6.16261100769043
Epoch 370, val loss: 0.8733713030815125
Epoch 380, training loss: 6.700501441955566 = 0.5496176481246948 + 1.0 * 6.150883674621582
Epoch 380, val loss: 0.8539491891860962
Epoch 390, training loss: 6.669507026672363 = 0.5211435556411743 + 1.0 * 6.1483635902404785
Epoch 390, val loss: 0.8363500237464905
Epoch 400, training loss: 6.639810562133789 = 0.4941212832927704 + 1.0 * 6.145689487457275
Epoch 400, val loss: 0.8202515840530396
Epoch 410, training loss: 6.614803791046143 = 0.46869710087776184 + 1.0 * 6.146106719970703
Epoch 410, val loss: 0.8058344721794128
Epoch 420, training loss: 6.585022926330566 = 0.4448854923248291 + 1.0 * 6.140137195587158
Epoch 420, val loss: 0.7932912111282349
Epoch 430, training loss: 6.558725357055664 = 0.4221407175064087 + 1.0 * 6.136584758758545
Epoch 430, val loss: 0.7821147441864014
Epoch 440, training loss: 6.532993793487549 = 0.4002968668937683 + 1.0 * 6.132697105407715
Epoch 440, val loss: 0.7721022963523865
Epoch 450, training loss: 6.514827728271484 = 0.3793611526489258 + 1.0 * 6.135466575622559
Epoch 450, val loss: 0.7632727026939392
Epoch 460, training loss: 6.488796710968018 = 0.35964730381965637 + 1.0 * 6.129149436950684
Epoch 460, val loss: 0.7558187246322632
Epoch 470, training loss: 6.471290588378906 = 0.34078437089920044 + 1.0 * 6.1305060386657715
Epoch 470, val loss: 0.7495043873786926
Epoch 480, training loss: 6.446689128875732 = 0.32277917861938477 + 1.0 * 6.123909950256348
Epoch 480, val loss: 0.7441782355308533
Epoch 490, training loss: 6.4274373054504395 = 0.30559200048446655 + 1.0 * 6.121845245361328
Epoch 490, val loss: 0.7398505806922913
Epoch 500, training loss: 6.407287120819092 = 0.2891699969768524 + 1.0 * 6.118117332458496
Epoch 500, val loss: 0.7364417910575867
Epoch 510, training loss: 6.407810688018799 = 0.273512065410614 + 1.0 * 6.134298801422119
Epoch 510, val loss: 0.7338049411773682
Epoch 520, training loss: 6.378946304321289 = 0.25886404514312744 + 1.0 * 6.120082378387451
Epoch 520, val loss: 0.7319157719612122
Epoch 530, training loss: 6.358932971954346 = 0.24494439363479614 + 1.0 * 6.113988399505615
Epoch 530, val loss: 0.7308021783828735
Epoch 540, training loss: 6.342363357543945 = 0.23164665699005127 + 1.0 * 6.110716819763184
Epoch 540, val loss: 0.730207622051239
Epoch 550, training loss: 6.341458797454834 = 0.21894440054893494 + 1.0 * 6.122514247894287
Epoch 550, val loss: 0.7300702333450317
Epoch 560, training loss: 6.31726598739624 = 0.20699793100357056 + 1.0 * 6.1102681159973145
Epoch 560, val loss: 0.7303900718688965
Epoch 570, training loss: 6.300801753997803 = 0.1956406533718109 + 1.0 * 6.105161190032959
Epoch 570, val loss: 0.7312806248664856
Epoch 580, training loss: 6.293358325958252 = 0.18480215966701508 + 1.0 * 6.108556270599365
Epoch 580, val loss: 0.7325925827026367
Epoch 590, training loss: 6.27796745300293 = 0.17460677027702332 + 1.0 * 6.103360652923584
Epoch 590, val loss: 0.7342798709869385
Epoch 600, training loss: 6.2663421630859375 = 0.16503866016864777 + 1.0 * 6.101303577423096
Epoch 600, val loss: 0.7363799214363098
Epoch 610, training loss: 6.254598140716553 = 0.15600471198558807 + 1.0 * 6.098593235015869
Epoch 610, val loss: 0.7389287948608398
Epoch 620, training loss: 6.24397087097168 = 0.1474502980709076 + 1.0 * 6.09652042388916
Epoch 620, val loss: 0.7418020367622375
Epoch 630, training loss: 6.2429518699646 = 0.13937972486019135 + 1.0 * 6.103572368621826
Epoch 630, val loss: 0.7449665069580078
Epoch 640, training loss: 6.234036922454834 = 0.13187576830387115 + 1.0 * 6.102160930633545
Epoch 640, val loss: 0.7484301328659058
Epoch 650, training loss: 6.221487522125244 = 0.12486900389194489 + 1.0 * 6.09661865234375
Epoch 650, val loss: 0.7522345781326294
Epoch 660, training loss: 6.211000919342041 = 0.11833308637142181 + 1.0 * 6.092668056488037
Epoch 660, val loss: 0.7563410401344299
Epoch 670, training loss: 6.202020168304443 = 0.11218524724245071 + 1.0 * 6.089834690093994
Epoch 670, val loss: 0.760697066783905
Epoch 680, training loss: 6.200081825256348 = 0.10642652213573456 + 1.0 * 6.093655109405518
Epoch 680, val loss: 0.7652228474617004
Epoch 690, training loss: 6.191799640655518 = 0.1010780930519104 + 1.0 * 6.090721607208252
Epoch 690, val loss: 0.7699184417724609
Epoch 700, training loss: 6.185288906097412 = 0.0961068645119667 + 1.0 * 6.089181900024414
Epoch 700, val loss: 0.7747935056686401
Epoch 710, training loss: 6.1764302253723145 = 0.09149983525276184 + 1.0 * 6.084930419921875
Epoch 710, val loss: 0.7797249555587769
Epoch 720, training loss: 6.171097755432129 = 0.08718416094779968 + 1.0 * 6.083913803100586
Epoch 720, val loss: 0.7848644256591797
Epoch 730, training loss: 6.166488170623779 = 0.08313737064599991 + 1.0 * 6.083350658416748
Epoch 730, val loss: 0.7900410890579224
Epoch 740, training loss: 6.161391735076904 = 0.07936066389083862 + 1.0 * 6.08203125
Epoch 740, val loss: 0.7952083945274353
Epoch 750, training loss: 6.160403728485107 = 0.07583977282047272 + 1.0 * 6.084563732147217
Epoch 750, val loss: 0.8005183339118958
Epoch 760, training loss: 6.155429363250732 = 0.07254841178655624 + 1.0 * 6.082880973815918
Epoch 760, val loss: 0.8058980107307434
Epoch 770, training loss: 6.147564888000488 = 0.06946304440498352 + 1.0 * 6.078101634979248
Epoch 770, val loss: 0.8112727403640747
Epoch 780, training loss: 6.145514965057373 = 0.06655263155698776 + 1.0 * 6.078962326049805
Epoch 780, val loss: 0.8167314529418945
Epoch 790, training loss: 6.148543834686279 = 0.0638108029961586 + 1.0 * 6.084733009338379
Epoch 790, val loss: 0.822214663028717
Epoch 800, training loss: 6.138899326324463 = 0.0612584613263607 + 1.0 * 6.077641010284424
Epoch 800, val loss: 0.827603816986084
Epoch 810, training loss: 6.133336067199707 = 0.05884465202689171 + 1.0 * 6.074491500854492
Epoch 810, val loss: 0.833092451095581
Epoch 820, training loss: 6.131278038024902 = 0.056562621146440506 + 1.0 * 6.074715614318848
Epoch 820, val loss: 0.8385336995124817
Epoch 830, training loss: 6.129717826843262 = 0.054406557232141495 + 1.0 * 6.075311183929443
Epoch 830, val loss: 0.8439727425575256
Epoch 840, training loss: 6.1260857582092285 = 0.0523739717900753 + 1.0 * 6.07371187210083
Epoch 840, val loss: 0.849364697933197
Epoch 850, training loss: 6.123961448669434 = 0.05045054852962494 + 1.0 * 6.073511123657227
Epoch 850, val loss: 0.8547592163085938
Epoch 860, training loss: 6.118550777435303 = 0.04863137751817703 + 1.0 * 6.069919586181641
Epoch 860, val loss: 0.860098659992218
Epoch 870, training loss: 6.117978572845459 = 0.04690900817513466 + 1.0 * 6.071069717407227
Epoch 870, val loss: 0.8654240369796753
Epoch 880, training loss: 6.112180233001709 = 0.045268986374139786 + 1.0 * 6.066911220550537
Epoch 880, val loss: 0.8707675933837891
Epoch 890, training loss: 6.114762783050537 = 0.043709658086299896 + 1.0 * 6.0710530281066895
Epoch 890, val loss: 0.8760911226272583
Epoch 900, training loss: 6.110711574554443 = 0.042233433574438095 + 1.0 * 6.068478107452393
Epoch 900, val loss: 0.8812500834465027
Epoch 910, training loss: 6.10691499710083 = 0.0408286526799202 + 1.0 * 6.066086292266846
Epoch 910, val loss: 0.8864685297012329
Epoch 920, training loss: 6.102805137634277 = 0.03949129953980446 + 1.0 * 6.063313961029053
Epoch 920, val loss: 0.8916664123535156
Epoch 930, training loss: 6.108450889587402 = 0.038213301450014114 + 1.0 * 6.070237636566162
Epoch 930, val loss: 0.8968644738197327
Epoch 940, training loss: 6.100987434387207 = 0.03699975833296776 + 1.0 * 6.063987731933594
Epoch 940, val loss: 0.9019065499305725
Epoch 950, training loss: 6.100400447845459 = 0.03584056720137596 + 1.0 * 6.0645599365234375
Epoch 950, val loss: 0.9069753885269165
Epoch 960, training loss: 6.098026752471924 = 0.03473740443587303 + 1.0 * 6.063289165496826
Epoch 960, val loss: 0.912061870098114
Epoch 970, training loss: 6.092979431152344 = 0.033689528703689575 + 1.0 * 6.059289932250977
Epoch 970, val loss: 0.9169535040855408
Epoch 980, training loss: 6.091250896453857 = 0.03268050402402878 + 1.0 * 6.058570384979248
Epoch 980, val loss: 0.9219504594802856
Epoch 990, training loss: 6.091010093688965 = 0.03171195462346077 + 1.0 * 6.059298038482666
Epoch 990, val loss: 0.9269408583641052
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7749
Flip ASR: 0.7333/225 nodes
The final ASR:0.72079, 0.04523, Accuracy:0.80617, 0.02145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9504])
updated graph: torch.Size([2, 10540])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97417, 0.00000, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.31616497039795 = 1.9422857761383057 + 1.0 * 8.373879432678223
Epoch 0, val loss: 1.9446817636489868
Epoch 10, training loss: 10.30500316619873 = 1.9315284490585327 + 1.0 * 8.373475074768066
Epoch 10, val loss: 1.9336994886398315
Epoch 20, training loss: 10.288589477539062 = 1.9176135063171387 + 1.0 * 8.370976448059082
Epoch 20, val loss: 1.9191021919250488
Epoch 30, training loss: 10.25314712524414 = 1.8977891206741333 + 1.0 * 8.355358123779297
Epoch 30, val loss: 1.8983025550842285
Epoch 40, training loss: 10.1341552734375 = 1.871827483177185 + 1.0 * 8.262328147888184
Epoch 40, val loss: 1.8726587295532227
Epoch 50, training loss: 9.686768531799316 = 1.8448399305343628 + 1.0 * 7.841928958892822
Epoch 50, val loss: 1.8474854230880737
Epoch 60, training loss: 9.142361640930176 = 1.8238465785980225 + 1.0 * 7.318515300750732
Epoch 60, val loss: 1.8294655084609985
Epoch 70, training loss: 8.709936141967773 = 1.809286117553711 + 1.0 * 6.9006500244140625
Epoch 70, val loss: 1.8166630268096924
Epoch 80, training loss: 8.51610279083252 = 1.7943590879440308 + 1.0 * 6.721743583679199
Epoch 80, val loss: 1.8037090301513672
Epoch 90, training loss: 8.38746452331543 = 1.7775027751922607 + 1.0 * 6.60996150970459
Epoch 90, val loss: 1.7893662452697754
Epoch 100, training loss: 8.292425155639648 = 1.7604637145996094 + 1.0 * 6.531960964202881
Epoch 100, val loss: 1.7749593257904053
Epoch 110, training loss: 8.222382545471191 = 1.742516279220581 + 1.0 * 6.4798665046691895
Epoch 110, val loss: 1.7595092058181763
Epoch 120, training loss: 8.162062644958496 = 1.7222118377685547 + 1.0 * 6.439850807189941
Epoch 120, val loss: 1.7421866655349731
Epoch 130, training loss: 8.104190826416016 = 1.6989362239837646 + 1.0 * 6.40525484085083
Epoch 130, val loss: 1.7230112552642822
Epoch 140, training loss: 8.046579360961914 = 1.6721177101135254 + 1.0 * 6.374461650848389
Epoch 140, val loss: 1.7013648748397827
Epoch 150, training loss: 7.98846435546875 = 1.6406532526016235 + 1.0 * 6.347811222076416
Epoch 150, val loss: 1.6762166023254395
Epoch 160, training loss: 7.9305524826049805 = 1.6036230325698853 + 1.0 * 6.326929569244385
Epoch 160, val loss: 1.6466177701950073
Epoch 170, training loss: 7.869007587432861 = 1.561190128326416 + 1.0 * 6.307817459106445
Epoch 170, val loss: 1.6127851009368896
Epoch 180, training loss: 7.805843353271484 = 1.5126653909683228 + 1.0 * 6.293178081512451
Epoch 180, val loss: 1.5741583108901978
Epoch 190, training loss: 7.738986968994141 = 1.4597089290618896 + 1.0 * 6.279277801513672
Epoch 190, val loss: 1.5326248407363892
Epoch 200, training loss: 7.671490669250488 = 1.4031633138656616 + 1.0 * 6.268327236175537
Epoch 200, val loss: 1.4886500835418701
Epoch 210, training loss: 7.601483345031738 = 1.3439513444900513 + 1.0 * 6.257532119750977
Epoch 210, val loss: 1.4431931972503662
Epoch 220, training loss: 7.532837867736816 = 1.283756971359253 + 1.0 * 6.249081134796143
Epoch 220, val loss: 1.3977088928222656
Epoch 230, training loss: 7.468013286590576 = 1.225022315979004 + 1.0 * 6.242990970611572
Epoch 230, val loss: 1.353834867477417
Epoch 240, training loss: 7.401033401489258 = 1.1679173707962036 + 1.0 * 6.233116149902344
Epoch 240, val loss: 1.3114842176437378
Epoch 250, training loss: 7.338353633880615 = 1.1123863458633423 + 1.0 * 6.2259674072265625
Epoch 250, val loss: 1.2707401514053345
Epoch 260, training loss: 7.281157970428467 = 1.0587660074234009 + 1.0 * 6.2223920822143555
Epoch 260, val loss: 1.2316985130310059
Epoch 270, training loss: 7.221521854400635 = 1.007705569267273 + 1.0 * 6.213816165924072
Epoch 270, val loss: 1.1947394609451294
Epoch 280, training loss: 7.167054653167725 = 0.9589554071426392 + 1.0 * 6.208099365234375
Epoch 280, val loss: 1.1596015691757202
Epoch 290, training loss: 7.115434646606445 = 0.912219762802124 + 1.0 * 6.2032151222229
Epoch 290, val loss: 1.1259541511535645
Epoch 300, training loss: 7.064397811889648 = 0.8670785427093506 + 1.0 * 6.197319507598877
Epoch 300, val loss: 1.0937327146530151
Epoch 310, training loss: 7.016618728637695 = 0.8233034014701843 + 1.0 * 6.193315505981445
Epoch 310, val loss: 1.0626065731048584
Epoch 320, training loss: 6.970531940460205 = 0.7812342643737793 + 1.0 * 6.189297676086426
Epoch 320, val loss: 1.0328788757324219
Epoch 330, training loss: 6.929442405700684 = 0.740857720375061 + 1.0 * 6.188584804534912
Epoch 330, val loss: 1.0046794414520264
Epoch 340, training loss: 6.882530689239502 = 0.7024471163749695 + 1.0 * 6.180083751678467
Epoch 340, val loss: 0.9778989553451538
Epoch 350, training loss: 6.843175411224365 = 0.6658288240432739 + 1.0 * 6.177346706390381
Epoch 350, val loss: 0.9527736902236938
Epoch 360, training loss: 6.80414342880249 = 0.6313292384147644 + 1.0 * 6.17281436920166
Epoch 360, val loss: 0.9295229315757751
Epoch 370, training loss: 6.766399383544922 = 0.5989507436752319 + 1.0 * 6.1674485206604
Epoch 370, val loss: 0.9082961678504944
Epoch 380, training loss: 6.741943359375 = 0.5686783194541931 + 1.0 * 6.173264980316162
Epoch 380, val loss: 0.8890627026557922
Epoch 390, training loss: 6.704893112182617 = 0.5408129096031189 + 1.0 * 6.1640801429748535
Epoch 390, val loss: 0.8721072673797607
Epoch 400, training loss: 6.67315673828125 = 0.5151326060295105 + 1.0 * 6.158024311065674
Epoch 400, val loss: 0.8573251366615295
Epoch 410, training loss: 6.645225524902344 = 0.49130070209503174 + 1.0 * 6.153924942016602
Epoch 410, val loss: 0.8444241881370544
Epoch 420, training loss: 6.623227119445801 = 0.4690152108669281 + 1.0 * 6.15421199798584
Epoch 420, val loss: 0.8331883549690247
Epoch 430, training loss: 6.597538948059082 = 0.44805216789245605 + 1.0 * 6.149486541748047
Epoch 430, val loss: 0.8233256340026855
Epoch 440, training loss: 6.5814385414123535 = 0.42812883853912354 + 1.0 * 6.1533098220825195
Epoch 440, val loss: 0.8147144317626953
Epoch 450, training loss: 6.553903102874756 = 0.40902408957481384 + 1.0 * 6.14487886428833
Epoch 450, val loss: 0.8071564435958862
Epoch 460, training loss: 6.5311970710754395 = 0.39051446318626404 + 1.0 * 6.140682697296143
Epoch 460, val loss: 0.8004049062728882
Epoch 470, training loss: 6.509556770324707 = 0.3723995089530945 + 1.0 * 6.137157440185547
Epoch 470, val loss: 0.7943341135978699
Epoch 480, training loss: 6.492642879486084 = 0.3545670807361603 + 1.0 * 6.138075828552246
Epoch 480, val loss: 0.7889516353607178
Epoch 490, training loss: 6.474542617797852 = 0.3370915651321411 + 1.0 * 6.137451171875
Epoch 490, val loss: 0.7842358350753784
Epoch 500, training loss: 6.450599670410156 = 0.32006707787513733 + 1.0 * 6.130532741546631
Epoch 500, val loss: 0.7801864147186279
Epoch 510, training loss: 6.43170690536499 = 0.3034629821777344 + 1.0 * 6.128243923187256
Epoch 510, val loss: 0.7768757343292236
Epoch 520, training loss: 6.412560939788818 = 0.2872442305088043 + 1.0 * 6.125316619873047
Epoch 520, val loss: 0.7741464972496033
Epoch 530, training loss: 6.405694007873535 = 0.27143165469169617 + 1.0 * 6.134262561798096
Epoch 530, val loss: 0.7719374895095825
Epoch 540, training loss: 6.382058143615723 = 0.2561851143836975 + 1.0 * 6.12587308883667
Epoch 540, val loss: 0.770248532295227
Epoch 550, training loss: 6.362522602081299 = 0.24139362573623657 + 1.0 * 6.121129035949707
Epoch 550, val loss: 0.769101083278656
Epoch 560, training loss: 6.350852966308594 = 0.22702789306640625 + 1.0 * 6.1238250732421875
Epoch 560, val loss: 0.7683731317520142
Epoch 570, training loss: 6.333922863006592 = 0.21316462755203247 + 1.0 * 6.120758056640625
Epoch 570, val loss: 0.7682428956031799
Epoch 580, training loss: 6.314905643463135 = 0.1998804658651352 + 1.0 * 6.115025043487549
Epoch 580, val loss: 0.7685388326644897
Epoch 590, training loss: 6.300007343292236 = 0.18715137243270874 + 1.0 * 6.112855911254883
Epoch 590, val loss: 0.7694071531295776
Epoch 600, training loss: 6.286453723907471 = 0.17507512867450714 + 1.0 * 6.1113786697387695
Epoch 600, val loss: 0.7708178758621216
Epoch 610, training loss: 6.277132034301758 = 0.1637383997440338 + 1.0 * 6.113393783569336
Epoch 610, val loss: 0.7728214263916016
Epoch 620, training loss: 6.260985374450684 = 0.15313762426376343 + 1.0 * 6.107847690582275
Epoch 620, val loss: 0.775418221950531
Epoch 630, training loss: 6.250144004821777 = 0.1432373821735382 + 1.0 * 6.106906414031982
Epoch 630, val loss: 0.7785990834236145
Epoch 640, training loss: 6.238414287567139 = 0.13404904305934906 + 1.0 * 6.104365348815918
Epoch 640, val loss: 0.7824165225028992
Epoch 650, training loss: 6.232852458953857 = 0.12558546662330627 + 1.0 * 6.107266902923584
Epoch 650, val loss: 0.7866189479827881
Epoch 660, training loss: 6.220186710357666 = 0.11779291927814484 + 1.0 * 6.102393627166748
Epoch 660, val loss: 0.791417121887207
Epoch 670, training loss: 6.212177753448486 = 0.11061426252126694 + 1.0 * 6.101563453674316
Epoch 670, val loss: 0.7966569662094116
Epoch 680, training loss: 6.204378128051758 = 0.1040029302239418 + 1.0 * 6.100375175476074
Epoch 680, val loss: 0.8023567795753479
Epoch 690, training loss: 6.195615768432617 = 0.09792400896549225 + 1.0 * 6.097691535949707
Epoch 690, val loss: 0.808334231376648
Epoch 700, training loss: 6.187798976898193 = 0.09230127185583115 + 1.0 * 6.0954976081848145
Epoch 700, val loss: 0.8146641850471497
Epoch 710, training loss: 6.182650089263916 = 0.08708521723747253 + 1.0 * 6.095564842224121
Epoch 710, val loss: 0.8212893009185791
Epoch 720, training loss: 6.178674221038818 = 0.08227252215147018 + 1.0 * 6.096401691436768
Epoch 720, val loss: 0.8279680609703064
Epoch 730, training loss: 6.174169063568115 = 0.07783183455467224 + 1.0 * 6.09633731842041
Epoch 730, val loss: 0.8349998593330383
Epoch 740, training loss: 6.166437149047852 = 0.07372268289327621 + 1.0 * 6.092714309692383
Epoch 740, val loss: 0.8420355319976807
Epoch 750, training loss: 6.162274360656738 = 0.06989365071058273 + 1.0 * 6.092380523681641
Epoch 750, val loss: 0.8491613268852234
Epoch 760, training loss: 6.155073165893555 = 0.06632377952337265 + 1.0 * 6.088749408721924
Epoch 760, val loss: 0.8564623594284058
Epoch 770, training loss: 6.155776023864746 = 0.06299787014722824 + 1.0 * 6.092778205871582
Epoch 770, val loss: 0.8637496829032898
Epoch 780, training loss: 6.151443958282471 = 0.05990685150027275 + 1.0 * 6.091536998748779
Epoch 780, val loss: 0.8709511756896973
Epoch 790, training loss: 6.142189025878906 = 0.057017721235752106 + 1.0 * 6.085171222686768
Epoch 790, val loss: 0.878371000289917
Epoch 800, training loss: 6.138308048248291 = 0.0543186254799366 + 1.0 * 6.08398962020874
Epoch 800, val loss: 0.8856884837150574
Epoch 810, training loss: 6.141082763671875 = 0.05178914964199066 + 1.0 * 6.089293479919434
Epoch 810, val loss: 0.8929489850997925
Epoch 820, training loss: 6.135149002075195 = 0.049422916024923325 + 1.0 * 6.085726261138916
Epoch 820, val loss: 0.9002527594566345
Epoch 830, training loss: 6.129392147064209 = 0.047209303826093674 + 1.0 * 6.082182884216309
Epoch 830, val loss: 0.9075542092323303
Epoch 840, training loss: 6.12973690032959 = 0.04513049125671387 + 1.0 * 6.084606170654297
Epoch 840, val loss: 0.9148328900337219
Epoch 850, training loss: 6.122530460357666 = 0.043183013796806335 + 1.0 * 6.079347610473633
Epoch 850, val loss: 0.921909749507904
Epoch 860, training loss: 6.121692657470703 = 0.04135008156299591 + 1.0 * 6.080342769622803
Epoch 860, val loss: 0.9291175007820129
Epoch 870, training loss: 6.1198225021362305 = 0.03962577506899834 + 1.0 * 6.080196857452393
Epoch 870, val loss: 0.936204195022583
Epoch 880, training loss: 6.117292404174805 = 0.03800774738192558 + 1.0 * 6.07928466796875
Epoch 880, val loss: 0.9431318044662476
Epoch 890, training loss: 6.11208963394165 = 0.03648192435503006 + 1.0 * 6.075607776641846
Epoch 890, val loss: 0.9501816630363464
Epoch 900, training loss: 6.115184783935547 = 0.035042423754930496 + 1.0 * 6.080142498016357
Epoch 900, val loss: 0.9569729566574097
Epoch 910, training loss: 6.108956336975098 = 0.03368144854903221 + 1.0 * 6.07527494430542
Epoch 910, val loss: 0.9638259410858154
Epoch 920, training loss: 6.104745864868164 = 0.0323999784886837 + 1.0 * 6.072345733642578
Epoch 920, val loss: 0.9705877304077148
Epoch 930, training loss: 6.105169773101807 = 0.031181758269667625 + 1.0 * 6.07398796081543
Epoch 930, val loss: 0.977338433265686
Epoch 940, training loss: 6.101903915405273 = 0.03003077767789364 + 1.0 * 6.071873188018799
Epoch 940, val loss: 0.983843207359314
Epoch 950, training loss: 6.101539134979248 = 0.028944602236151695 + 1.0 * 6.07259464263916
Epoch 950, val loss: 0.9904888868331909
Epoch 960, training loss: 6.096706867218018 = 0.027918215841054916 + 1.0 * 6.068788528442383
Epoch 960, val loss: 0.9969859719276428
Epoch 970, training loss: 6.096540451049805 = 0.026938406750559807 + 1.0 * 6.069602012634277
Epoch 970, val loss: 1.003508448600769
Epoch 980, training loss: 6.0941548347473145 = 0.02600770629942417 + 1.0 * 6.0681471824646
Epoch 980, val loss: 1.009795904159546
Epoch 990, training loss: 6.093008995056152 = 0.025129185989499092 + 1.0 * 6.067879676818848
Epoch 990, val loss: 1.0161857604980469
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6089
Flip ASR: 0.5289/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.332009315490723 = 1.9581104516983032 + 1.0 * 8.37389850616455
Epoch 0, val loss: 1.9568804502487183
Epoch 10, training loss: 10.320666313171387 = 1.9472150802612305 + 1.0 * 8.373451232910156
Epoch 10, val loss: 1.9459364414215088
Epoch 20, training loss: 10.304485321044922 = 1.9335408210754395 + 1.0 * 8.370944023132324
Epoch 20, val loss: 1.9316143989562988
Epoch 30, training loss: 10.2712984085083 = 1.9143247604370117 + 1.0 * 8.356973648071289
Epoch 30, val loss: 1.910884141921997
Epoch 40, training loss: 10.150899887084961 = 1.8883432149887085 + 1.0 * 8.262557029724121
Epoch 40, val loss: 1.8834015130996704
Epoch 50, training loss: 9.550477981567383 = 1.8614230155944824 + 1.0 * 7.689054489135742
Epoch 50, val loss: 1.855330467224121
Epoch 60, training loss: 9.123568534851074 = 1.8385781049728394 + 1.0 * 7.284990310668945
Epoch 60, val loss: 1.8323085308074951
Epoch 70, training loss: 8.822876930236816 = 1.8185030221939087 + 1.0 * 7.004374027252197
Epoch 70, val loss: 1.812295913696289
Epoch 80, training loss: 8.595000267028809 = 1.7992416620254517 + 1.0 * 6.795758247375488
Epoch 80, val loss: 1.7941352128982544
Epoch 90, training loss: 8.45778751373291 = 1.7782224416732788 + 1.0 * 6.6795654296875
Epoch 90, val loss: 1.7750701904296875
Epoch 100, training loss: 8.357978820800781 = 1.7557300329208374 + 1.0 * 6.6022491455078125
Epoch 100, val loss: 1.7550336122512817
Epoch 110, training loss: 8.273366928100586 = 1.7329473495483398 + 1.0 * 6.540420055389404
Epoch 110, val loss: 1.7351583242416382
Epoch 120, training loss: 8.195868492126465 = 1.709351897239685 + 1.0 * 6.48651647567749
Epoch 120, val loss: 1.7147631645202637
Epoch 130, training loss: 8.128277778625488 = 1.6830179691314697 + 1.0 * 6.445260047912598
Epoch 130, val loss: 1.692447304725647
Epoch 140, training loss: 8.064133644104004 = 1.6527410745620728 + 1.0 * 6.4113922119140625
Epoch 140, val loss: 1.66697096824646
Epoch 150, training loss: 8.002176284790039 = 1.6172599792480469 + 1.0 * 6.38491678237915
Epoch 150, val loss: 1.6375372409820557
Epoch 160, training loss: 7.939453125 = 1.576421856880188 + 1.0 * 6.363031387329102
Epoch 160, val loss: 1.6042349338531494
Epoch 170, training loss: 7.873489856719971 = 1.5305700302124023 + 1.0 * 6.342919826507568
Epoch 170, val loss: 1.566864252090454
Epoch 180, training loss: 7.807216167449951 = 1.4800244569778442 + 1.0 * 6.3271918296813965
Epoch 180, val loss: 1.5262049436569214
Epoch 190, training loss: 7.73849630355835 = 1.4265326261520386 + 1.0 * 6.3119635581970215
Epoch 190, val loss: 1.4839518070220947
Epoch 200, training loss: 7.671985149383545 = 1.3721343278884888 + 1.0 * 6.299850940704346
Epoch 200, val loss: 1.4416325092315674
Epoch 210, training loss: 7.610750198364258 = 1.3188326358795166 + 1.0 * 6.29191780090332
Epoch 210, val loss: 1.4009830951690674
Epoch 220, training loss: 7.546878814697266 = 1.2680212259292603 + 1.0 * 6.278857707977295
Epoch 220, val loss: 1.3635364770889282
Epoch 230, training loss: 7.487571716308594 = 1.2194916009902954 + 1.0 * 6.268080234527588
Epoch 230, val loss: 1.3283501863479614
Epoch 240, training loss: 7.434232234954834 = 1.172458529472351 + 1.0 * 6.261773586273193
Epoch 240, val loss: 1.2946211099624634
Epoch 250, training loss: 7.378817558288574 = 1.126785397529602 + 1.0 * 6.252032279968262
Epoch 250, val loss: 1.2625601291656494
Epoch 260, training loss: 7.32572078704834 = 1.082079291343689 + 1.0 * 6.243641376495361
Epoch 260, val loss: 1.231236457824707
Epoch 270, training loss: 7.273871898651123 = 1.0374979972839355 + 1.0 * 6.2363739013671875
Epoch 270, val loss: 1.1998084783554077
Epoch 280, training loss: 7.225115776062012 = 0.9931726455688477 + 1.0 * 6.231943130493164
Epoch 280, val loss: 1.1687097549438477
Epoch 290, training loss: 7.172882080078125 = 0.9494931697845459 + 1.0 * 6.223388671875
Epoch 290, val loss: 1.1380398273468018
Epoch 300, training loss: 7.125267505645752 = 0.9060460925102234 + 1.0 * 6.219221591949463
Epoch 300, val loss: 1.1076537370681763
Epoch 310, training loss: 7.079121112823486 = 0.8639906048774719 + 1.0 * 6.21513032913208
Epoch 310, val loss: 1.0779844522476196
Epoch 320, training loss: 7.030588626861572 = 0.8233991265296936 + 1.0 * 6.207189559936523
Epoch 320, val loss: 1.0498536825180054
Epoch 330, training loss: 6.990367412567139 = 0.7845802903175354 + 1.0 * 6.205787181854248
Epoch 330, val loss: 1.0234076976776123
Epoch 340, training loss: 6.948276519775391 = 0.7483627200126648 + 1.0 * 6.19991397857666
Epoch 340, val loss: 0.9991388320922852
Epoch 350, training loss: 6.907096862792969 = 0.7144753932952881 + 1.0 * 6.192621231079102
Epoch 350, val loss: 0.9773038625717163
Epoch 360, training loss: 6.871699810028076 = 0.6828327775001526 + 1.0 * 6.188867092132568
Epoch 360, val loss: 0.9576239585876465
Epoch 370, training loss: 6.842904090881348 = 0.6534779071807861 + 1.0 * 6.189426422119141
Epoch 370, val loss: 0.9398550987243652
Epoch 380, training loss: 6.805587291717529 = 0.6260407567024231 + 1.0 * 6.179546356201172
Epoch 380, val loss: 0.9239917993545532
Epoch 390, training loss: 6.777181625366211 = 0.6000035405158997 + 1.0 * 6.177177906036377
Epoch 390, val loss: 0.9095237255096436
Epoch 400, training loss: 6.750227928161621 = 0.5751641392707825 + 1.0 * 6.175063610076904
Epoch 400, val loss: 0.8960483074188232
Epoch 410, training loss: 6.720881462097168 = 0.5512344837188721 + 1.0 * 6.169647216796875
Epoch 410, val loss: 0.8834704160690308
Epoch 420, training loss: 6.692754745483398 = 0.5280559659004211 + 1.0 * 6.164698600769043
Epoch 420, val loss: 0.8715584874153137
Epoch 430, training loss: 6.6700029373168945 = 0.5054656267166138 + 1.0 * 6.16453742980957
Epoch 430, val loss: 0.8602933287620544
Epoch 440, training loss: 6.6424360275268555 = 0.4834931790828705 + 1.0 * 6.158942699432373
Epoch 440, val loss: 0.8498255014419556
Epoch 450, training loss: 6.617873191833496 = 0.4621143341064453 + 1.0 * 6.155758857727051
Epoch 450, val loss: 0.8401574492454529
Epoch 460, training loss: 6.600528240203857 = 0.4413355588912964 + 1.0 * 6.1591925621032715
Epoch 460, val loss: 0.8312358260154724
Epoch 470, training loss: 6.571963310241699 = 0.42113202810287476 + 1.0 * 6.15083122253418
Epoch 470, val loss: 0.8232343792915344
Epoch 480, training loss: 6.547779560089111 = 0.40167856216430664 + 1.0 * 6.146100997924805
Epoch 480, val loss: 0.8163174390792847
Epoch 490, training loss: 6.541476249694824 = 0.38299837708473206 + 1.0 * 6.158477783203125
Epoch 490, val loss: 0.8104841113090515
Epoch 500, training loss: 6.506820201873779 = 0.365182489156723 + 1.0 * 6.141637802124023
Epoch 500, val loss: 0.8055689334869385
Epoch 510, training loss: 6.487500190734863 = 0.348123699426651 + 1.0 * 6.139376640319824
Epoch 510, val loss: 0.8016453981399536
Epoch 520, training loss: 6.469269275665283 = 0.33161675930023193 + 1.0 * 6.137652397155762
Epoch 520, val loss: 0.7984642386436462
Epoch 530, training loss: 6.450406551361084 = 0.3156183958053589 + 1.0 * 6.1347880363464355
Epoch 530, val loss: 0.7958584427833557
Epoch 540, training loss: 6.434427261352539 = 0.30015677213668823 + 1.0 * 6.134270668029785
Epoch 540, val loss: 0.7939704060554504
Epoch 550, training loss: 6.418638229370117 = 0.2851106822490692 + 1.0 * 6.133527755737305
Epoch 550, val loss: 0.7926905751228333
Epoch 560, training loss: 6.399486541748047 = 0.2704748809337616 + 1.0 * 6.129011631011963
Epoch 560, val loss: 0.7919443249702454
Epoch 570, training loss: 6.383664608001709 = 0.25622132420539856 + 1.0 * 6.127443313598633
Epoch 570, val loss: 0.7916433811187744
Epoch 580, training loss: 6.36771297454834 = 0.2423732578754425 + 1.0 * 6.125339508056641
Epoch 580, val loss: 0.7917728424072266
Epoch 590, training loss: 6.351320266723633 = 0.22895129024982452 + 1.0 * 6.122368812561035
Epoch 590, val loss: 0.7923866510391235
Epoch 600, training loss: 6.34513521194458 = 0.2160235494375229 + 1.0 * 6.1291117668151855
Epoch 600, val loss: 0.7934190034866333
Epoch 610, training loss: 6.325037002563477 = 0.20363830029964447 + 1.0 * 6.12139892578125
Epoch 610, val loss: 0.794625997543335
Epoch 620, training loss: 6.3112993240356445 = 0.19190940260887146 + 1.0 * 6.11939001083374
Epoch 620, val loss: 0.7963241934776306
Epoch 630, training loss: 6.296443939208984 = 0.18077535927295685 + 1.0 * 6.115668773651123
Epoch 630, val loss: 0.7984654903411865
Epoch 640, training loss: 6.290947437286377 = 0.170273557305336 + 1.0 * 6.120673656463623
Epoch 640, val loss: 0.8009812831878662
Epoch 650, training loss: 6.278547286987305 = 0.16047778725624084 + 1.0 * 6.118069648742676
Epoch 650, val loss: 0.8038355112075806
Epoch 660, training loss: 6.263769626617432 = 0.1513528823852539 + 1.0 * 6.112416744232178
Epoch 660, val loss: 0.8070487380027771
Epoch 670, training loss: 6.257154941558838 = 0.14284974336624146 + 1.0 * 6.114305019378662
Epoch 670, val loss: 0.8106147646903992
Epoch 680, training loss: 6.244266510009766 = 0.13491491973400116 + 1.0 * 6.109351634979248
Epoch 680, val loss: 0.8144733309745789
Epoch 690, training loss: 6.238857746124268 = 0.12754520773887634 + 1.0 * 6.111312389373779
Epoch 690, val loss: 0.8186916708946228
Epoch 700, training loss: 6.229772090911865 = 0.12071742117404938 + 1.0 * 6.1090545654296875
Epoch 700, val loss: 0.8231732249259949
Epoch 710, training loss: 6.221938610076904 = 0.11434749513864517 + 1.0 * 6.107591152191162
Epoch 710, val loss: 0.827849805355072
Epoch 720, training loss: 6.212635517120361 = 0.1084304228425026 + 1.0 * 6.104205131530762
Epoch 720, val loss: 0.8327944278717041
Epoch 730, training loss: 6.206298351287842 = 0.10291294008493423 + 1.0 * 6.1033854484558105
Epoch 730, val loss: 0.8379416465759277
Epoch 740, training loss: 6.202308654785156 = 0.09775473177433014 + 1.0 * 6.104553699493408
Epoch 740, val loss: 0.8432936072349548
Epoch 750, training loss: 6.193280220031738 = 0.09292852878570557 + 1.0 * 6.100351810455322
Epoch 750, val loss: 0.8487421274185181
Epoch 760, training loss: 6.192021369934082 = 0.08842048794031143 + 1.0 * 6.103600978851318
Epoch 760, val loss: 0.8542978763580322
Epoch 770, training loss: 6.18157958984375 = 0.08419650048017502 + 1.0 * 6.09738302230835
Epoch 770, val loss: 0.8599651455879211
Epoch 780, training loss: 6.187588691711426 = 0.08024568855762482 + 1.0 * 6.1073431968688965
Epoch 780, val loss: 0.8657696843147278
Epoch 790, training loss: 6.174792766571045 = 0.07652198523283005 + 1.0 * 6.098270893096924
Epoch 790, val loss: 0.8714539408683777
Epoch 800, training loss: 6.167294502258301 = 0.07304197549819946 + 1.0 * 6.094252586364746
Epoch 800, val loss: 0.8772518634796143
Epoch 810, training loss: 6.162549018859863 = 0.06974919140338898 + 1.0 * 6.092799663543701
Epoch 810, val loss: 0.8831104040145874
Epoch 820, training loss: 6.159045696258545 = 0.0666494369506836 + 1.0 * 6.092396259307861
Epoch 820, val loss: 0.8889827132225037
Epoch 830, training loss: 6.153557300567627 = 0.06373193114995956 + 1.0 * 6.08982515335083
Epoch 830, val loss: 0.8948187232017517
Epoch 840, training loss: 6.158631324768066 = 0.06098242104053497 + 1.0 * 6.097649097442627
Epoch 840, val loss: 0.900749683380127
Epoch 850, training loss: 6.147478103637695 = 0.05837870389223099 + 1.0 * 6.089099407196045
Epoch 850, val loss: 0.906553328037262
Epoch 860, training loss: 6.145493984222412 = 0.05592760443687439 + 1.0 * 6.089566230773926
Epoch 860, val loss: 0.91241455078125
Epoch 870, training loss: 6.145451068878174 = 0.053609397262334824 + 1.0 * 6.091841697692871
Epoch 870, val loss: 0.9181843996047974
Epoch 880, training loss: 6.136593818664551 = 0.0514146126806736 + 1.0 * 6.085179328918457
Epoch 880, val loss: 0.9239287376403809
Epoch 890, training loss: 6.132172584533691 = 0.04934386536478996 + 1.0 * 6.082828521728516
Epoch 890, val loss: 0.9296519160270691
Epoch 900, training loss: 6.1363325119018555 = 0.047380104660987854 + 1.0 * 6.088952541351318
Epoch 900, val loss: 0.9354009032249451
Epoch 910, training loss: 6.131786346435547 = 0.04551933705806732 + 1.0 * 6.086266994476318
Epoch 910, val loss: 0.9409525394439697
Epoch 920, training loss: 6.125566005706787 = 0.043760426342487335 + 1.0 * 6.08180570602417
Epoch 920, val loss: 0.946517825126648
Epoch 930, training loss: 6.122065544128418 = 0.042090412229299545 + 1.0 * 6.079975128173828
Epoch 930, val loss: 0.9520895481109619
Epoch 940, training loss: 6.1280012130737305 = 0.04050225019454956 + 1.0 * 6.087499141693115
Epoch 940, val loss: 0.9575591087341309
Epoch 950, training loss: 6.119330406188965 = 0.039007168263196945 + 1.0 * 6.080323219299316
Epoch 950, val loss: 0.9630023837089539
Epoch 960, training loss: 6.115516662597656 = 0.037579141557216644 + 1.0 * 6.077937602996826
Epoch 960, val loss: 0.9684417843818665
Epoch 970, training loss: 6.115661144256592 = 0.03622503578662872 + 1.0 * 6.079436302185059
Epoch 970, val loss: 0.973823606967926
Epoch 980, training loss: 6.112280368804932 = 0.03493485972285271 + 1.0 * 6.077345371246338
Epoch 980, val loss: 0.9790570139884949
Epoch 990, training loss: 6.1143412590026855 = 0.03370894491672516 + 1.0 * 6.080632209777832
Epoch 990, val loss: 0.9842009544372559
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.9446
Flip ASR: 0.9333/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.305618286132812 = 1.9317083358764648 + 1.0 * 8.373909950256348
Epoch 0, val loss: 1.9291406869888306
Epoch 10, training loss: 10.295869827270508 = 1.922252893447876 + 1.0 * 8.373617172241211
Epoch 10, val loss: 1.919389247894287
Epoch 20, training loss: 10.282272338867188 = 1.9105253219604492 + 1.0 * 8.371747016906738
Epoch 20, val loss: 1.9071168899536133
Epoch 30, training loss: 10.253196716308594 = 1.8943933248519897 + 1.0 * 8.358803749084473
Epoch 30, val loss: 1.8901675939559937
Epoch 40, training loss: 10.150735855102539 = 1.8733114004135132 + 1.0 * 8.277424812316895
Epoch 40, val loss: 1.8688262701034546
Epoch 50, training loss: 9.747314453125 = 1.8507224321365356 + 1.0 * 7.896592140197754
Epoch 50, val loss: 1.846796989440918
Epoch 60, training loss: 9.18917465209961 = 1.828656554222107 + 1.0 * 7.360518455505371
Epoch 60, val loss: 1.8262819051742554
Epoch 70, training loss: 8.835168838500977 = 1.811594009399414 + 1.0 * 7.023575305938721
Epoch 70, val loss: 1.8109114170074463
Epoch 80, training loss: 8.62934684753418 = 1.7963588237762451 + 1.0 * 6.8329877853393555
Epoch 80, val loss: 1.7961235046386719
Epoch 90, training loss: 8.501463890075684 = 1.7810312509536743 + 1.0 * 6.720432758331299
Epoch 90, val loss: 1.7815340757369995
Epoch 100, training loss: 8.390995979309082 = 1.7628483772277832 + 1.0 * 6.628147602081299
Epoch 100, val loss: 1.765235185623169
Epoch 110, training loss: 8.305008888244629 = 1.7441612482070923 + 1.0 * 6.560847759246826
Epoch 110, val loss: 1.7490744590759277
Epoch 120, training loss: 8.236001968383789 = 1.7241500616073608 + 1.0 * 6.511851787567139
Epoch 120, val loss: 1.7317219972610474
Epoch 130, training loss: 8.176871299743652 = 1.7008904218673706 + 1.0 * 6.475980758666992
Epoch 130, val loss: 1.7117654085159302
Epoch 140, training loss: 8.118705749511719 = 1.674013614654541 + 1.0 * 6.444692134857178
Epoch 140, val loss: 1.6888855695724487
Epoch 150, training loss: 8.058403015136719 = 1.6430424451828003 + 1.0 * 6.415360450744629
Epoch 150, val loss: 1.662887454032898
Epoch 160, training loss: 7.994690895080566 = 1.6070765256881714 + 1.0 * 6.3876142501831055
Epoch 160, val loss: 1.6328411102294922
Epoch 170, training loss: 7.930283546447754 = 1.5656321048736572 + 1.0 * 6.364651679992676
Epoch 170, val loss: 1.5985432863235474
Epoch 180, training loss: 7.862039566040039 = 1.5185695886611938 + 1.0 * 6.343470096588135
Epoch 180, val loss: 1.5596997737884521
Epoch 190, training loss: 7.794210433959961 = 1.465418815612793 + 1.0 * 6.328791618347168
Epoch 190, val loss: 1.5161278247833252
Epoch 200, training loss: 7.721436023712158 = 1.4068683385849 + 1.0 * 6.314567565917969
Epoch 200, val loss: 1.4682475328445435
Epoch 210, training loss: 7.64675235748291 = 1.3432035446166992 + 1.0 * 6.303548812866211
Epoch 210, val loss: 1.4165854454040527
Epoch 220, training loss: 7.571776866912842 = 1.276710033416748 + 1.0 * 6.295066833496094
Epoch 220, val loss: 1.363112449645996
Epoch 230, training loss: 7.494652271270752 = 1.2098835706710815 + 1.0 * 6.284768581390381
Epoch 230, val loss: 1.3099408149719238
Epoch 240, training loss: 7.419301509857178 = 1.1434205770492554 + 1.0 * 6.275880813598633
Epoch 240, val loss: 1.257292628288269
Epoch 250, training loss: 7.350398540496826 = 1.0781666040420532 + 1.0 * 6.2722320556640625
Epoch 250, val loss: 1.2058956623077393
Epoch 260, training loss: 7.278830528259277 = 1.0160746574401855 + 1.0 * 6.262755870819092
Epoch 260, val loss: 1.1572957038879395
Epoch 270, training loss: 7.209716320037842 = 0.957031786441803 + 1.0 * 6.252684593200684
Epoch 270, val loss: 1.1114599704742432
Epoch 280, training loss: 7.154517650604248 = 0.9009242057800293 + 1.0 * 6.253593444824219
Epoch 280, val loss: 1.0683854818344116
Epoch 290, training loss: 7.088317394256592 = 0.8491448760032654 + 1.0 * 6.239172458648682
Epoch 290, val loss: 1.02866530418396
Epoch 300, training loss: 7.031849384307861 = 0.8005135655403137 + 1.0 * 6.231335639953613
Epoch 300, val loss: 0.9918781518936157
Epoch 310, training loss: 6.980949878692627 = 0.7546482682228088 + 1.0 * 6.226301670074463
Epoch 310, val loss: 0.9576773047447205
Epoch 320, training loss: 6.933193206787109 = 0.7118963599205017 + 1.0 * 6.221296787261963
Epoch 320, val loss: 0.9260928630828857
Epoch 330, training loss: 6.885415077209473 = 0.6719914078712463 + 1.0 * 6.213423728942871
Epoch 330, val loss: 0.8972110152244568
Epoch 340, training loss: 6.847681045532227 = 0.6345903873443604 + 1.0 * 6.213090419769287
Epoch 340, val loss: 0.8707073330879211
Epoch 350, training loss: 6.80265474319458 = 0.599972128868103 + 1.0 * 6.2026824951171875
Epoch 350, val loss: 0.8466971516609192
Epoch 360, training loss: 6.765230178833008 = 0.5676121711730957 + 1.0 * 6.197618007659912
Epoch 360, val loss: 0.8251192569732666
Epoch 370, training loss: 6.7290802001953125 = 0.5371493697166443 + 1.0 * 6.191930770874023
Epoch 370, val loss: 0.8055356740951538
Epoch 380, training loss: 6.704400062561035 = 0.5084888935089111 + 1.0 * 6.195910930633545
Epoch 380, val loss: 0.7879865169525146
Epoch 390, training loss: 6.669859886169434 = 0.4821021258831024 + 1.0 * 6.187757968902588
Epoch 390, val loss: 0.7725607752799988
Epoch 400, training loss: 6.637833595275879 = 0.45734497904777527 + 1.0 * 6.180488586425781
Epoch 400, val loss: 0.7591559886932373
Epoch 410, training loss: 6.6100263595581055 = 0.4338879883289337 + 1.0 * 6.176138401031494
Epoch 410, val loss: 0.7473951578140259
Epoch 420, training loss: 6.586697101593018 = 0.4115827977657318 + 1.0 * 6.175114154815674
Epoch 420, val loss: 0.7370066046714783
Epoch 430, training loss: 6.559381484985352 = 0.3904080092906952 + 1.0 * 6.168973445892334
Epoch 430, val loss: 0.7279002070426941
Epoch 440, training loss: 6.5366902351379395 = 0.37013065814971924 + 1.0 * 6.16655969619751
Epoch 440, val loss: 0.7198169827461243
Epoch 450, training loss: 6.513385772705078 = 0.35058337450027466 + 1.0 * 6.162802219390869
Epoch 450, val loss: 0.7126396298408508
Epoch 460, training loss: 6.491415500640869 = 0.3317023813724518 + 1.0 * 6.159713268280029
Epoch 460, val loss: 0.7061351537704468
Epoch 470, training loss: 6.472412109375 = 0.31342613697052 + 1.0 * 6.1589860916137695
Epoch 470, val loss: 0.7002215385437012
Epoch 480, training loss: 6.451150417327881 = 0.295684814453125 + 1.0 * 6.155465602874756
Epoch 480, val loss: 0.6948140263557434
Epoch 490, training loss: 6.429777145385742 = 0.2784245014190674 + 1.0 * 6.151352882385254
Epoch 490, val loss: 0.6898865103721619
Epoch 500, training loss: 6.412479400634766 = 0.2615278661251068 + 1.0 * 6.150951385498047
Epoch 500, val loss: 0.6854164004325867
Epoch 510, training loss: 6.395986080169678 = 0.24508284032344818 + 1.0 * 6.150903224945068
Epoch 510, val loss: 0.6814334392547607
Epoch 520, training loss: 6.3733811378479 = 0.22917263209819794 + 1.0 * 6.1442084312438965
Epoch 520, val loss: 0.6780009269714355
Epoch 530, training loss: 6.356162071228027 = 0.21380029618740082 + 1.0 * 6.142361640930176
Epoch 530, val loss: 0.6750715374946594
Epoch 540, training loss: 6.342998027801514 = 0.1990066021680832 + 1.0 * 6.143991470336914
Epoch 540, val loss: 0.6726899147033691
Epoch 550, training loss: 6.327436447143555 = 0.18501748144626617 + 1.0 * 6.14241886138916
Epoch 550, val loss: 0.6709849834442139
Epoch 560, training loss: 6.310031890869141 = 0.17181816697120667 + 1.0 * 6.138213634490967
Epoch 560, val loss: 0.6698949933052063
Epoch 570, training loss: 6.29368782043457 = 0.15950711071491241 + 1.0 * 6.134180545806885
Epoch 570, val loss: 0.6695337295532227
Epoch 580, training loss: 6.279855251312256 = 0.14803943037986755 + 1.0 * 6.1318159103393555
Epoch 580, val loss: 0.6697859168052673
Epoch 590, training loss: 6.270696640014648 = 0.13749569654464722 + 1.0 * 6.1332011222839355
Epoch 590, val loss: 0.6706555485725403
Epoch 600, training loss: 6.255643367767334 = 0.1278848946094513 + 1.0 * 6.127758502960205
Epoch 600, val loss: 0.6721561551094055
Epoch 610, training loss: 6.258495807647705 = 0.11915823072195053 + 1.0 * 6.139337539672852
Epoch 610, val loss: 0.6742245554924011
Epoch 620, training loss: 6.237441539764404 = 0.11122650653123856 + 1.0 * 6.126214981079102
Epoch 620, val loss: 0.676845371723175
Epoch 630, training loss: 6.226742267608643 = 0.1040247231721878 + 1.0 * 6.122717380523682
Epoch 630, val loss: 0.6800091862678528
Epoch 640, training loss: 6.217931747436523 = 0.09743934869766235 + 1.0 * 6.120492458343506
Epoch 640, val loss: 0.683570921421051
Epoch 650, training loss: 6.2242655754089355 = 0.09144120663404465 + 1.0 * 6.132824420928955
Epoch 650, val loss: 0.6875344514846802
Epoch 660, training loss: 6.20663595199585 = 0.08599157631397247 + 1.0 * 6.120644569396973
Epoch 660, val loss: 0.6918307542800903
Epoch 670, training loss: 6.199153423309326 = 0.08100435882806778 + 1.0 * 6.118149280548096
Epoch 670, val loss: 0.6964942812919617
Epoch 680, training loss: 6.190873146057129 = 0.07643383741378784 + 1.0 * 6.114439487457275
Epoch 680, val loss: 0.7014104723930359
Epoch 690, training loss: 6.184659957885742 = 0.07223626971244812 + 1.0 * 6.112423896789551
Epoch 690, val loss: 0.7066035866737366
Epoch 700, training loss: 6.180200576782227 = 0.06835034489631653 + 1.0 * 6.111850261688232
Epoch 700, val loss: 0.7120369672775269
Epoch 710, training loss: 6.181082248687744 = 0.06476319581270218 + 1.0 * 6.116319179534912
Epoch 710, val loss: 0.7176225781440735
Epoch 720, training loss: 6.172812461853027 = 0.0614541657269001 + 1.0 * 6.111358165740967
Epoch 720, val loss: 0.7233827114105225
Epoch 730, training loss: 6.165258884429932 = 0.05838004872202873 + 1.0 * 6.106878757476807
Epoch 730, val loss: 0.7292777299880981
Epoch 740, training loss: 6.164765357971191 = 0.0555126778781414 + 1.0 * 6.109252452850342
Epoch 740, val loss: 0.73529452085495
Epoch 750, training loss: 6.163255214691162 = 0.052856482565402985 + 1.0 * 6.110398769378662
Epoch 750, val loss: 0.7413827776908875
Epoch 760, training loss: 6.153629302978516 = 0.050397440791130066 + 1.0 * 6.103231906890869
Epoch 760, val loss: 0.7475786805152893
Epoch 770, training loss: 6.151091575622559 = 0.04809165373444557 + 1.0 * 6.102999687194824
Epoch 770, val loss: 0.7538439631462097
Epoch 780, training loss: 6.154664993286133 = 0.045933693647384644 + 1.0 * 6.108731269836426
Epoch 780, val loss: 0.7601432204246521
Epoch 790, training loss: 6.146828651428223 = 0.04391476511955261 + 1.0 * 6.102913856506348
Epoch 790, val loss: 0.766471266746521
Epoch 800, training loss: 6.14207124710083 = 0.04202647507190704 + 1.0 * 6.1000447273254395
Epoch 800, val loss: 0.7728543281555176
Epoch 810, training loss: 6.143195152282715 = 0.040249455720186234 + 1.0 * 6.102945804595947
Epoch 810, val loss: 0.7792603373527527
Epoch 820, training loss: 6.137656211853027 = 0.03857951983809471 + 1.0 * 6.099076747894287
Epoch 820, val loss: 0.7855668663978577
Epoch 830, training loss: 6.133640766143799 = 0.03702138736844063 + 1.0 * 6.096619606018066
Epoch 830, val loss: 0.7919422388076782
Epoch 840, training loss: 6.130677223205566 = 0.035544998943805695 + 1.0 * 6.095132350921631
Epoch 840, val loss: 0.7982708811759949
Epoch 850, training loss: 6.13818883895874 = 0.034153666347265244 + 1.0 * 6.104035377502441
Epoch 850, val loss: 0.8045561909675598
Epoch 860, training loss: 6.126119613647461 = 0.032856330275535583 + 1.0 * 6.093263149261475
Epoch 860, val loss: 0.8108377456665039
Epoch 870, training loss: 6.128313064575195 = 0.03162414953112602 + 1.0 * 6.096688747406006
Epoch 870, val loss: 0.8170540928840637
Epoch 880, training loss: 6.121045112609863 = 0.03046870231628418 + 1.0 * 6.090576171875
Epoch 880, val loss: 0.8232356905937195
Epoch 890, training loss: 6.119626045227051 = 0.02937651239335537 + 1.0 * 6.090249538421631
Epoch 890, val loss: 0.8293715119361877
Epoch 900, training loss: 6.119333267211914 = 0.028337029740214348 + 1.0 * 6.090996265411377
Epoch 900, val loss: 0.8354566693305969
Epoch 910, training loss: 6.115636825561523 = 0.027354327961802483 + 1.0 * 6.088282585144043
Epoch 910, val loss: 0.8414580821990967
Epoch 920, training loss: 6.114305019378662 = 0.026419105008244514 + 1.0 * 6.087885856628418
Epoch 920, val loss: 0.8474279046058655
Epoch 930, training loss: 6.1142191886901855 = 0.025531958788633347 + 1.0 * 6.088687419891357
Epoch 930, val loss: 0.8534128665924072
Epoch 940, training loss: 6.1105427742004395 = 0.024689143523573875 + 1.0 * 6.085853576660156
Epoch 940, val loss: 0.859254002571106
Epoch 950, training loss: 6.110631942749023 = 0.023892717435956 + 1.0 * 6.0867390632629395
Epoch 950, val loss: 0.8650845289230347
Epoch 960, training loss: 6.107630729675293 = 0.023132825270295143 + 1.0 * 6.084497928619385
Epoch 960, val loss: 0.8707890510559082
Epoch 970, training loss: 6.108344554901123 = 0.02241138182580471 + 1.0 * 6.085933208465576
Epoch 970, val loss: 0.8764702081680298
Epoch 980, training loss: 6.10408878326416 = 0.021719666197896004 + 1.0 * 6.082369327545166
Epoch 980, val loss: 0.8821261525154114
Epoch 990, training loss: 6.1056952476501465 = 0.021061502397060394 + 1.0 * 6.084633827209473
Epoch 990, val loss: 0.8876963257789612
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8672
Flip ASR: 0.8400/225 nodes
The final ASR:0.80689, 0.14356, Accuracy:0.80494, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10528])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.3008394241333 = 1.9270819425582886 + 1.0 * 8.373757362365723
Epoch 0, val loss: 1.9254415035247803
Epoch 10, training loss: 10.290502548217773 = 1.9173814058303833 + 1.0 * 8.37312126159668
Epoch 10, val loss: 1.9158375263214111
Epoch 20, training loss: 10.274858474731445 = 1.9054923057556152 + 1.0 * 8.369366645812988
Epoch 20, val loss: 1.9037821292877197
Epoch 30, training loss: 10.232575416564941 = 1.8894150257110596 + 1.0 * 8.343160629272461
Epoch 30, val loss: 1.8875823020935059
Epoch 40, training loss: 9.989051818847656 = 1.8701388835906982 + 1.0 * 8.118912696838379
Epoch 40, val loss: 1.8689833879470825
Epoch 50, training loss: 9.256458282470703 = 1.8496708869934082 + 1.0 * 7.406787395477295
Epoch 50, val loss: 1.8501724004745483
Epoch 60, training loss: 8.967626571655273 = 1.835056185722351 + 1.0 * 7.132570743560791
Epoch 60, val loss: 1.8374086618423462
Epoch 70, training loss: 8.665327072143555 = 1.8249346017837524 + 1.0 * 6.840392589569092
Epoch 70, val loss: 1.8282917737960815
Epoch 80, training loss: 8.43994426727295 = 1.81486177444458 + 1.0 * 6.625082492828369
Epoch 80, val loss: 1.8194397687911987
Epoch 90, training loss: 8.322507858276367 = 1.805153489112854 + 1.0 * 6.517354488372803
Epoch 90, val loss: 1.8100353479385376
Epoch 100, training loss: 8.237287521362305 = 1.7940104007720947 + 1.0 * 6.443277359008789
Epoch 100, val loss: 1.7991300821304321
Epoch 110, training loss: 8.180187225341797 = 1.7832304239273071 + 1.0 * 6.396956920623779
Epoch 110, val loss: 1.7890704870224
Epoch 120, training loss: 8.132596969604492 = 1.7720704078674316 + 1.0 * 6.360527038574219
Epoch 120, val loss: 1.7791153192520142
Epoch 130, training loss: 8.09034538269043 = 1.7598174810409546 + 1.0 * 6.3305277824401855
Epoch 130, val loss: 1.768287181854248
Epoch 140, training loss: 8.050087928771973 = 1.7457278966903687 + 1.0 * 6.304360389709473
Epoch 140, val loss: 1.7560914754867554
Epoch 150, training loss: 8.011731147766113 = 1.729183316230774 + 1.0 * 6.282547950744629
Epoch 150, val loss: 1.7421669960021973
Epoch 160, training loss: 7.974430084228516 = 1.7094017267227173 + 1.0 * 6.265028476715088
Epoch 160, val loss: 1.7256510257720947
Epoch 170, training loss: 7.934869289398193 = 1.6848009824752808 + 1.0 * 6.250068187713623
Epoch 170, val loss: 1.705238699913025
Epoch 180, training loss: 7.891246795654297 = 1.6535753011703491 + 1.0 * 6.237671375274658
Epoch 180, val loss: 1.6793992519378662
Epoch 190, training loss: 7.842339515686035 = 1.6148145198822021 + 1.0 * 6.227524757385254
Epoch 190, val loss: 1.6471623182296753
Epoch 200, training loss: 7.785490036010742 = 1.5671656131744385 + 1.0 * 6.218324661254883
Epoch 200, val loss: 1.607648491859436
Epoch 210, training loss: 7.72183084487915 = 1.5097450017929077 + 1.0 * 6.212085723876953
Epoch 210, val loss: 1.560134768486023
Epoch 220, training loss: 7.649984359741211 = 1.4446889162063599 + 1.0 * 6.205295562744141
Epoch 220, val loss: 1.5068079233169556
Epoch 230, training loss: 7.573162078857422 = 1.374255895614624 + 1.0 * 6.198905944824219
Epoch 230, val loss: 1.4495086669921875
Epoch 240, training loss: 7.497260570526123 = 1.3015557527542114 + 1.0 * 6.195704936981201
Epoch 240, val loss: 1.3908616304397583
Epoch 250, training loss: 7.4218549728393555 = 1.2315561771392822 + 1.0 * 6.190298557281494
Epoch 250, val loss: 1.3357164859771729
Epoch 260, training loss: 7.347511291503906 = 1.165184497833252 + 1.0 * 6.182326793670654
Epoch 260, val loss: 1.2838506698608398
Epoch 270, training loss: 7.278267860412598 = 1.1018210649490356 + 1.0 * 6.176446914672852
Epoch 270, val loss: 1.2348105907440186
Epoch 280, training loss: 7.212571620941162 = 1.0412052869796753 + 1.0 * 6.171366214752197
Epoch 280, val loss: 1.18850576877594
Epoch 290, training loss: 7.151231288909912 = 0.9838701486587524 + 1.0 * 6.167361259460449
Epoch 290, val loss: 1.1451003551483154
Epoch 300, training loss: 7.091228485107422 = 0.9286490678787231 + 1.0 * 6.162579536437988
Epoch 300, val loss: 1.103432059288025
Epoch 310, training loss: 7.034158229827881 = 0.8764592409133911 + 1.0 * 6.157699108123779
Epoch 310, val loss: 1.0642420053482056
Epoch 320, training loss: 6.98102331161499 = 0.8276892304420471 + 1.0 * 6.153334140777588
Epoch 320, val loss: 1.0281715393066406
Epoch 330, training loss: 6.9384965896606445 = 0.7822999954223633 + 1.0 * 6.156196594238281
Epoch 330, val loss: 0.9949538707733154
Epoch 340, training loss: 6.886767864227295 = 0.7408623695373535 + 1.0 * 6.145905494689941
Epoch 340, val loss: 0.9653847217559814
Epoch 350, training loss: 6.842922210693359 = 0.7029167413711548 + 1.0 * 6.140005588531494
Epoch 350, val loss: 0.9391257762908936
Epoch 360, training loss: 6.808900356292725 = 0.6679624915122986 + 1.0 * 6.140937805175781
Epoch 360, val loss: 0.9158276319503784
Epoch 370, training loss: 6.768302917480469 = 0.6361210346221924 + 1.0 * 6.132181644439697
Epoch 370, val loss: 0.8956620097160339
Epoch 380, training loss: 6.735496520996094 = 0.6068724393844604 + 1.0 * 6.128623962402344
Epoch 380, val loss: 0.8780914545059204
Epoch 390, training loss: 6.707816123962402 = 0.5796509981155396 + 1.0 * 6.128165245056152
Epoch 390, val loss: 0.8626465797424316
Epoch 400, training loss: 6.677156925201416 = 0.5544983744621277 + 1.0 * 6.122658729553223
Epoch 400, val loss: 0.8492873311042786
Epoch 410, training loss: 6.651136875152588 = 0.5311008095741272 + 1.0 * 6.1200361251831055
Epoch 410, val loss: 0.837681770324707
Epoch 420, training loss: 6.63005256652832 = 0.508895754814148 + 1.0 * 6.121156692504883
Epoch 420, val loss: 0.827448844909668
Epoch 430, training loss: 6.6014628410339355 = 0.4877869784832001 + 1.0 * 6.113676071166992
Epoch 430, val loss: 0.8184115886688232
Epoch 440, training loss: 6.5839643478393555 = 0.46754521131515503 + 1.0 * 6.116419315338135
Epoch 440, val loss: 0.8105347752571106
Epoch 450, training loss: 6.557971000671387 = 0.44806209206581116 + 1.0 * 6.1099090576171875
Epoch 450, val loss: 0.8037732243537903
Epoch 460, training loss: 6.536363124847412 = 0.4292342960834503 + 1.0 * 6.107128620147705
Epoch 460, val loss: 0.7980823516845703
Epoch 470, training loss: 6.514920234680176 = 0.4108037054538727 + 1.0 * 6.104116439819336
Epoch 470, val loss: 0.7933170795440674
Epoch 480, training loss: 6.50199556350708 = 0.3927973210811615 + 1.0 * 6.109198093414307
Epoch 480, val loss: 0.789448082447052
Epoch 490, training loss: 6.477993011474609 = 0.37529510259628296 + 1.0 * 6.102697849273682
Epoch 490, val loss: 0.7865732312202454
Epoch 500, training loss: 6.457574367523193 = 0.35820135474205017 + 1.0 * 6.099372863769531
Epoch 500, val loss: 0.7846394181251526
Epoch 510, training loss: 6.448209762573242 = 0.341419517993927 + 1.0 * 6.106790065765381
Epoch 510, val loss: 0.7834892868995667
Epoch 520, training loss: 6.422338962554932 = 0.32505589723587036 + 1.0 * 6.097282886505127
Epoch 520, val loss: 0.7831579446792603
Epoch 530, training loss: 6.402772426605225 = 0.3090173602104187 + 1.0 * 6.09375524520874
Epoch 530, val loss: 0.783697783946991
Epoch 540, training loss: 6.386229038238525 = 0.2932402789592743 + 1.0 * 6.092988967895508
Epoch 540, val loss: 0.7849189639091492
Epoch 550, training loss: 6.371613502502441 = 0.27782049775123596 + 1.0 * 6.093792915344238
Epoch 550, val loss: 0.7867446541786194
Epoch 560, training loss: 6.353749752044678 = 0.2629111409187317 + 1.0 * 6.090838432312012
Epoch 560, val loss: 0.7892806529998779
Epoch 570, training loss: 6.336833477020264 = 0.24848996102809906 + 1.0 * 6.088343620300293
Epoch 570, val loss: 0.7923815250396729
Epoch 580, training loss: 6.320786476135254 = 0.23456072807312012 + 1.0 * 6.086225986480713
Epoch 580, val loss: 0.7960668206214905
Epoch 590, training loss: 6.308089256286621 = 0.22120574116706848 + 1.0 * 6.086883544921875
Epoch 590, val loss: 0.8003916144371033
Epoch 600, training loss: 6.302550315856934 = 0.20855712890625 + 1.0 * 6.093993186950684
Epoch 600, val loss: 0.8051798939704895
Epoch 610, training loss: 6.279768466949463 = 0.19674399495124817 + 1.0 * 6.083024501800537
Epoch 610, val loss: 0.8105518221855164
Epoch 620, training loss: 6.266430377960205 = 0.18564793467521667 + 1.0 * 6.080782413482666
Epoch 620, val loss: 0.8164914846420288
Epoch 630, training loss: 6.254788875579834 = 0.17520402371883392 + 1.0 * 6.079585075378418
Epoch 630, val loss: 0.8228760957717896
Epoch 640, training loss: 6.256176471710205 = 0.1654144674539566 + 1.0 * 6.090762138366699
Epoch 640, val loss: 0.8297266364097595
Epoch 650, training loss: 6.234742164611816 = 0.1563243567943573 + 1.0 * 6.078417778015137
Epoch 650, val loss: 0.8368246555328369
Epoch 660, training loss: 6.228459358215332 = 0.14784057438373566 + 1.0 * 6.080618858337402
Epoch 660, val loss: 0.8442316651344299
Epoch 670, training loss: 6.216063022613525 = 0.13990503549575806 + 1.0 * 6.076158046722412
Epoch 670, val loss: 0.8518865704536438
Epoch 680, training loss: 6.210824489593506 = 0.13247783482074738 + 1.0 * 6.0783467292785645
Epoch 680, val loss: 0.859822154045105
Epoch 690, training loss: 6.200361251831055 = 0.1255405992269516 + 1.0 * 6.074820518493652
Epoch 690, val loss: 0.8678708672523499
Epoch 700, training loss: 6.1909332275390625 = 0.11905080825090408 + 1.0 * 6.071882247924805
Epoch 700, val loss: 0.8761910200119019
Epoch 710, training loss: 6.183209419250488 = 0.11294513195753098 + 1.0 * 6.0702643394470215
Epoch 710, val loss: 0.8846830129623413
Epoch 720, training loss: 6.176493167877197 = 0.10718898475170135 + 1.0 * 6.0693039894104
Epoch 720, val loss: 0.8933514356613159
Epoch 730, training loss: 6.184460163116455 = 0.10175826400518417 + 1.0 * 6.082701683044434
Epoch 730, val loss: 0.9021492600440979
Epoch 740, training loss: 6.1653666496276855 = 0.09672116488218307 + 1.0 * 6.068645477294922
Epoch 740, val loss: 0.9109953045845032
Epoch 750, training loss: 6.158283233642578 = 0.09199214726686478 + 1.0 * 6.066290855407715
Epoch 750, val loss: 0.9201153516769409
Epoch 760, training loss: 6.1526103019714355 = 0.0875270664691925 + 1.0 * 6.065083026885986
Epoch 760, val loss: 0.9292396306991577
Epoch 770, training loss: 6.151007652282715 = 0.08330589532852173 + 1.0 * 6.067701816558838
Epoch 770, val loss: 0.9385232329368591
Epoch 780, training loss: 6.145821571350098 = 0.07934971898794174 + 1.0 * 6.066472053527832
Epoch 780, val loss: 0.9478029608726501
Epoch 790, training loss: 6.140482425689697 = 0.07563795894384384 + 1.0 * 6.064844608306885
Epoch 790, val loss: 0.9570152163505554
Epoch 800, training loss: 6.133746147155762 = 0.07214473932981491 + 1.0 * 6.061601638793945
Epoch 800, val loss: 0.9663496017456055
Epoch 810, training loss: 6.12988805770874 = 0.06883861124515533 + 1.0 * 6.061049461364746
Epoch 810, val loss: 0.9757882356643677
Epoch 820, training loss: 6.1297926902771 = 0.06570682674646378 + 1.0 * 6.064085960388184
Epoch 820, val loss: 0.9852409362792969
Epoch 830, training loss: 6.127707004547119 = 0.06277727335691452 + 1.0 * 6.064929962158203
Epoch 830, val loss: 0.9944924712181091
Epoch 840, training loss: 6.118406295776367 = 0.06002071872353554 + 1.0 * 6.058385372161865
Epoch 840, val loss: 1.0038425922393799
Epoch 850, training loss: 6.115221977233887 = 0.05742311850190163 + 1.0 * 6.057798862457275
Epoch 850, val loss: 1.0131462812423706
Epoch 860, training loss: 6.117334842681885 = 0.05496908724308014 + 1.0 * 6.062365531921387
Epoch 860, val loss: 1.0223573446273804
Epoch 870, training loss: 6.11319637298584 = 0.05266905203461647 + 1.0 * 6.060527324676514
Epoch 870, val loss: 1.0313694477081299
Epoch 880, training loss: 6.105767250061035 = 0.05050225183367729 + 1.0 * 6.055264949798584
Epoch 880, val loss: 1.0402873754501343
Epoch 890, training loss: 6.10294771194458 = 0.048456668853759766 + 1.0 * 6.05449104309082
Epoch 890, val loss: 1.049090027809143
Epoch 900, training loss: 6.10712194442749 = 0.04652388021349907 + 1.0 * 6.060597896575928
Epoch 900, val loss: 1.057677149772644
Epoch 910, training loss: 6.096961975097656 = 0.0447065494954586 + 1.0 * 6.052255630493164
Epoch 910, val loss: 1.0660991668701172
Epoch 920, training loss: 6.095185279846191 = 0.04298717901110649 + 1.0 * 6.0521979331970215
Epoch 920, val loss: 1.0745141506195068
Epoch 930, training loss: 6.093555927276611 = 0.041355960071086884 + 1.0 * 6.052199840545654
Epoch 930, val loss: 1.082804560661316
Epoch 940, training loss: 6.092992305755615 = 0.0398104153573513 + 1.0 * 6.053182125091553
Epoch 940, val loss: 1.0909364223480225
Epoch 950, training loss: 6.0923943519592285 = 0.038353051990270615 + 1.0 * 6.054041385650635
Epoch 950, val loss: 1.0989458560943604
Epoch 960, training loss: 6.086225986480713 = 0.036978427320718765 + 1.0 * 6.049247741699219
Epoch 960, val loss: 1.1067951917648315
Epoch 970, training loss: 6.0844645500183105 = 0.035669635981321335 + 1.0 * 6.048794746398926
Epoch 970, val loss: 1.1145596504211426
Epoch 980, training loss: 6.085970401763916 = 0.03442950174212456 + 1.0 * 6.051540851593018
Epoch 980, val loss: 1.1221742630004883
Epoch 990, training loss: 6.0820512771606445 = 0.03324729949235916 + 1.0 * 6.048803806304932
Epoch 990, val loss: 1.1296063661575317
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.6679
Flip ASR: 0.6000/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.306861877441406 = 1.9331027269363403 + 1.0 * 8.373759269714355
Epoch 0, val loss: 1.926797866821289
Epoch 10, training loss: 10.296433448791504 = 1.923380732536316 + 1.0 * 8.373052597045898
Epoch 10, val loss: 1.916853666305542
Epoch 20, training loss: 10.28027629852295 = 1.9114395380020142 + 1.0 * 8.368836402893066
Epoch 20, val loss: 1.904652714729309
Epoch 30, training loss: 10.235880851745605 = 1.8955862522125244 + 1.0 * 8.34029483795166
Epoch 30, val loss: 1.8884800672531128
Epoch 40, training loss: 9.955717086791992 = 1.8771610260009766 + 1.0 * 8.078556060791016
Epoch 40, val loss: 1.8702589273452759
Epoch 50, training loss: 9.034383773803711 = 1.8585114479064941 + 1.0 * 7.175872325897217
Epoch 50, val loss: 1.8522502183914185
Epoch 60, training loss: 8.701400756835938 = 1.8455206155776978 + 1.0 * 6.855879783630371
Epoch 60, val loss: 1.8400263786315918
Epoch 70, training loss: 8.492677688598633 = 1.833305835723877 + 1.0 * 6.659371376037598
Epoch 70, val loss: 1.827824592590332
Epoch 80, training loss: 8.364734649658203 = 1.821510672569275 + 1.0 * 6.543224334716797
Epoch 80, val loss: 1.816266417503357
Epoch 90, training loss: 8.277933120727539 = 1.8097715377807617 + 1.0 * 6.4681620597839355
Epoch 90, val loss: 1.804771065711975
Epoch 100, training loss: 8.207658767700195 = 1.7987010478973389 + 1.0 * 6.4089579582214355
Epoch 100, val loss: 1.7940040826797485
Epoch 110, training loss: 8.150175094604492 = 1.7877153158187866 + 1.0 * 6.362460136413574
Epoch 110, val loss: 1.7832590341567993
Epoch 120, training loss: 8.104297637939453 = 1.776231288909912 + 1.0 * 6.328066825866699
Epoch 120, val loss: 1.7720907926559448
Epoch 130, training loss: 8.06292724609375 = 1.7634961605072021 + 1.0 * 6.299430847167969
Epoch 130, val loss: 1.7598928213119507
Epoch 140, training loss: 8.025243759155273 = 1.7490283250808716 + 1.0 * 6.276215553283691
Epoch 140, val loss: 1.7464306354522705
Epoch 150, training loss: 7.9882965087890625 = 1.7322161197662354 + 1.0 * 6.256080627441406
Epoch 150, val loss: 1.7313892841339111
Epoch 160, training loss: 7.9521284103393555 = 1.7120468616485596 + 1.0 * 6.240081787109375
Epoch 160, val loss: 1.7138596773147583
Epoch 170, training loss: 7.9138994216918945 = 1.6875460147857666 + 1.0 * 6.226353168487549
Epoch 170, val loss: 1.6929429769515991
Epoch 180, training loss: 7.872632026672363 = 1.6574145555496216 + 1.0 * 6.215217590332031
Epoch 180, val loss: 1.6676212549209595
Epoch 190, training loss: 7.825573921203613 = 1.6198738813400269 + 1.0 * 6.205699920654297
Epoch 190, val loss: 1.6363012790679932
Epoch 200, training loss: 7.772243022918701 = 1.5735186338424683 + 1.0 * 6.198724269866943
Epoch 200, val loss: 1.5979429483413696
Epoch 210, training loss: 7.710878849029541 = 1.5183850526809692 + 1.0 * 6.192493915557861
Epoch 210, val loss: 1.5526927709579468
Epoch 220, training loss: 7.640770435333252 = 1.4552030563354492 + 1.0 * 6.185567378997803
Epoch 220, val loss: 1.5012551546096802
Epoch 230, training loss: 7.566168785095215 = 1.3854944705963135 + 1.0 * 6.180674076080322
Epoch 230, val loss: 1.4451404809951782
Epoch 240, training loss: 7.490328788757324 = 1.3131054639816284 + 1.0 * 6.177223205566406
Epoch 240, val loss: 1.3877382278442383
Epoch 250, training loss: 7.413656711578369 = 1.2424200773239136 + 1.0 * 6.171236515045166
Epoch 250, val loss: 1.3321455717086792
Epoch 260, training loss: 7.344170570373535 = 1.175369381904602 + 1.0 * 6.168801307678223
Epoch 260, val loss: 1.2800871133804321
Epoch 270, training loss: 7.275987148284912 = 1.1140958070755005 + 1.0 * 6.161891460418701
Epoch 270, val loss: 1.2329497337341309
Epoch 280, training loss: 7.214751720428467 = 1.0585622787475586 + 1.0 * 6.156189441680908
Epoch 280, val loss: 1.1909888982772827
Epoch 290, training loss: 7.1594438552856445 = 1.008151888847351 + 1.0 * 6.151291847229004
Epoch 290, val loss: 1.1534250974655151
Epoch 300, training loss: 7.110927104949951 = 0.9622656106948853 + 1.0 * 6.1486616134643555
Epoch 300, val loss: 1.1198492050170898
Epoch 310, training loss: 7.062681674957275 = 0.9198728203773499 + 1.0 * 6.14280891418457
Epoch 310, val loss: 1.0896369218826294
Epoch 320, training loss: 7.018399715423584 = 0.8799039125442505 + 1.0 * 6.138495922088623
Epoch 320, val loss: 1.0615390539169312
Epoch 330, training loss: 6.978593826293945 = 0.8417823314666748 + 1.0 * 6.13681173324585
Epoch 330, val loss: 1.0351409912109375
Epoch 340, training loss: 6.937785625457764 = 0.8053836822509766 + 1.0 * 6.132401943206787
Epoch 340, val loss: 1.0102707147598267
Epoch 350, training loss: 6.898505687713623 = 0.7701360583305359 + 1.0 * 6.1283698081970215
Epoch 350, val loss: 0.9864892959594727
Epoch 360, training loss: 6.866568565368652 = 0.7361481189727783 + 1.0 * 6.130420684814453
Epoch 360, val loss: 0.9636996984481812
Epoch 370, training loss: 6.828521728515625 = 0.703880786895752 + 1.0 * 6.124640941619873
Epoch 370, val loss: 0.9427391290664673
Epoch 380, training loss: 6.792541980743408 = 0.6732501983642578 + 1.0 * 6.11929178237915
Epoch 380, val loss: 0.9232363700866699
Epoch 390, training loss: 6.760804176330566 = 0.6440488696098328 + 1.0 * 6.116755485534668
Epoch 390, val loss: 0.9050499796867371
Epoch 400, training loss: 6.728588104248047 = 0.6160359978675842 + 1.0 * 6.112552165985107
Epoch 400, val loss: 0.8882910013198853
Epoch 410, training loss: 6.704212188720703 = 0.5891305804252625 + 1.0 * 6.115081787109375
Epoch 410, val loss: 0.8727840185165405
Epoch 420, training loss: 6.673280239105225 = 0.5635597109794617 + 1.0 * 6.109720706939697
Epoch 420, val loss: 0.8586537837982178
Epoch 430, training loss: 6.6458940505981445 = 0.5392069220542908 + 1.0 * 6.106687068939209
Epoch 430, val loss: 0.8460026979446411
Epoch 440, training loss: 6.619736671447754 = 0.5156962871551514 + 1.0 * 6.104040145874023
Epoch 440, val loss: 0.8345368504524231
Epoch 450, training loss: 6.598047733306885 = 0.4928610622882843 + 1.0 * 6.105186462402344
Epoch 450, val loss: 0.8241499662399292
Epoch 460, training loss: 6.572279930114746 = 0.4707934558391571 + 1.0 * 6.101486682891846
Epoch 460, val loss: 0.8150032162666321
Epoch 470, training loss: 6.5506367683410645 = 0.44952094554901123 + 1.0 * 6.101115703582764
Epoch 470, val loss: 0.8072012066841125
Epoch 480, training loss: 6.5247416496276855 = 0.42911067605018616 + 1.0 * 6.095631122589111
Epoch 480, val loss: 0.8007452487945557
Epoch 490, training loss: 6.503582954406738 = 0.40951380133628845 + 1.0 * 6.094069004058838
Epoch 490, val loss: 0.7955513000488281
Epoch 500, training loss: 6.482641220092773 = 0.3907814621925354 + 1.0 * 6.091859817504883
Epoch 500, val loss: 0.7914474606513977
Epoch 510, training loss: 6.463230133056641 = 0.3730713725090027 + 1.0 * 6.090158939361572
Epoch 510, val loss: 0.7886784672737122
Epoch 520, training loss: 6.445141315460205 = 0.35615259408950806 + 1.0 * 6.088988780975342
Epoch 520, val loss: 0.7868854403495789
Epoch 530, training loss: 6.436817169189453 = 0.33990371227264404 + 1.0 * 6.0969133377075195
Epoch 530, val loss: 0.7858946323394775
Epoch 540, training loss: 6.411229133605957 = 0.3245455324649811 + 1.0 * 6.086683750152588
Epoch 540, val loss: 0.7856518030166626
Epoch 550, training loss: 6.394501209259033 = 0.3099101781845093 + 1.0 * 6.084590911865234
Epoch 550, val loss: 0.7862635254859924
Epoch 560, training loss: 6.3785905838012695 = 0.2958490550518036 + 1.0 * 6.082741737365723
Epoch 560, val loss: 0.7873554825782776
Epoch 570, training loss: 6.367669582366943 = 0.2823101878166199 + 1.0 * 6.085359573364258
Epoch 570, val loss: 0.7889621257781982
Epoch 580, training loss: 6.3539886474609375 = 0.2693188190460205 + 1.0 * 6.084670066833496
Epoch 580, val loss: 0.7910683155059814
Epoch 590, training loss: 6.337252140045166 = 0.2569171190261841 + 1.0 * 6.0803351402282715
Epoch 590, val loss: 0.7934713959693909
Epoch 600, training loss: 6.322386264801025 = 0.24501152336597443 + 1.0 * 6.0773749351501465
Epoch 600, val loss: 0.7962737083435059
Epoch 610, training loss: 6.322137832641602 = 0.23357966542243958 + 1.0 * 6.088558197021484
Epoch 610, val loss: 0.7993661761283875
Epoch 620, training loss: 6.299603462219238 = 0.22272421419620514 + 1.0 * 6.076879024505615
Epoch 620, val loss: 0.8027400970458984
Epoch 630, training loss: 6.292193412780762 = 0.21240055561065674 + 1.0 * 6.0797929763793945
Epoch 630, val loss: 0.8064018487930298
Epoch 640, training loss: 6.276167869567871 = 0.2025773674249649 + 1.0 * 6.073590278625488
Epoch 640, val loss: 0.8102900385856628
Epoch 650, training loss: 6.264802932739258 = 0.19318147003650665 + 1.0 * 6.071621417999268
Epoch 650, val loss: 0.8144485354423523
Epoch 660, training loss: 6.255593776702881 = 0.18420575559139252 + 1.0 * 6.071388244628906
Epoch 660, val loss: 0.8188221454620361
Epoch 670, training loss: 6.249054431915283 = 0.1756659746170044 + 1.0 * 6.073388576507568
Epoch 670, val loss: 0.8232935070991516
Epoch 680, training loss: 6.239543437957764 = 0.16759620606899261 + 1.0 * 6.07194709777832
Epoch 680, val loss: 0.827873706817627
Epoch 690, training loss: 6.227490425109863 = 0.15994401276111603 + 1.0 * 6.067546367645264
Epoch 690, val loss: 0.832690954208374
Epoch 700, training loss: 6.230172634124756 = 0.15267843008041382 + 1.0 * 6.077494144439697
Epoch 700, val loss: 0.8374558091163635
Epoch 710, training loss: 6.2107648849487305 = 0.1458074450492859 + 1.0 * 6.064957618713379
Epoch 710, val loss: 0.8423770666122437
Epoch 720, training loss: 6.203791618347168 = 0.13928639888763428 + 1.0 * 6.064505100250244
Epoch 720, val loss: 0.8474636673927307
Epoch 730, training loss: 6.195982456207275 = 0.13307571411132812 + 1.0 * 6.062906742095947
Epoch 730, val loss: 0.8525835871696472
Epoch 740, training loss: 6.191277503967285 = 0.12718385457992554 + 1.0 * 6.064093589782715
Epoch 740, val loss: 0.857721209526062
Epoch 750, training loss: 6.18455171585083 = 0.12161745131015778 + 1.0 * 6.062934398651123
Epoch 750, val loss: 0.8630130887031555
Epoch 760, training loss: 6.177600860595703 = 0.11633901298046112 + 1.0 * 6.0612616539001465
Epoch 760, val loss: 0.8683648109436035
Epoch 770, training loss: 6.1728129386901855 = 0.11132072657346725 + 1.0 * 6.061492443084717
Epoch 770, val loss: 0.8736534118652344
Epoch 780, training loss: 6.169444561004639 = 0.1065630316734314 + 1.0 * 6.0628814697265625
Epoch 780, val loss: 0.8790060877799988
Epoch 790, training loss: 6.159345626831055 = 0.10206524282693863 + 1.0 * 6.057280540466309
Epoch 790, val loss: 0.8844237923622131
Epoch 800, training loss: 6.153235912322998 = 0.09778367727994919 + 1.0 * 6.055452346801758
Epoch 800, val loss: 0.8898693323135376
Epoch 810, training loss: 6.150106906890869 = 0.09370293468236923 + 1.0 * 6.056404113769531
Epoch 810, val loss: 0.895315408706665
Epoch 820, training loss: 6.144517421722412 = 0.0898212417960167 + 1.0 * 6.054696083068848
Epoch 820, val loss: 0.9006732106208801
Epoch 830, training loss: 6.144624710083008 = 0.08614744991064072 + 1.0 * 6.058477401733398
Epoch 830, val loss: 0.9060783982276917
Epoch 840, training loss: 6.136868000030518 = 0.08265849202871323 + 1.0 * 6.0542097091674805
Epoch 840, val loss: 0.9114155173301697
Epoch 850, training loss: 6.130049705505371 = 0.07935600727796555 + 1.0 * 6.050693511962891
Epoch 850, val loss: 0.9168230891227722
Epoch 860, training loss: 6.1265130043029785 = 0.07620315998792648 + 1.0 * 6.050309658050537
Epoch 860, val loss: 0.9221794605255127
Epoch 870, training loss: 6.138462066650391 = 0.07319489866495132 + 1.0 * 6.065267086029053
Epoch 870, val loss: 0.9274733662605286
Epoch 880, training loss: 6.119060039520264 = 0.070348359644413 + 1.0 * 6.048711776733398
Epoch 880, val loss: 0.9326333999633789
Epoch 890, training loss: 6.117427349090576 = 0.06764131784439087 + 1.0 * 6.04978609085083
Epoch 890, val loss: 0.9379777908325195
Epoch 900, training loss: 6.113869667053223 = 0.06505248695611954 + 1.0 * 6.048817157745361
Epoch 900, val loss: 0.9431838393211365
Epoch 910, training loss: 6.111913204193115 = 0.06258716434240341 + 1.0 * 6.049325942993164
Epoch 910, val loss: 0.9482883214950562
Epoch 920, training loss: 6.109282970428467 = 0.06024540960788727 + 1.0 * 6.049037456512451
Epoch 920, val loss: 0.9533941149711609
Epoch 930, training loss: 6.104255676269531 = 0.05801108106970787 + 1.0 * 6.0462446212768555
Epoch 930, val loss: 0.9584748148918152
Epoch 940, training loss: 6.102242469787598 = 0.055888593196868896 + 1.0 * 6.046353816986084
Epoch 940, val loss: 0.9635287523269653
Epoch 950, training loss: 6.098470687866211 = 0.05385779216885567 + 1.0 * 6.044612884521484
Epoch 950, val loss: 0.9684303402900696
Epoch 960, training loss: 6.097912311553955 = 0.05192260071635246 + 1.0 * 6.045989513397217
Epoch 960, val loss: 0.973422110080719
Epoch 970, training loss: 6.093631744384766 = 0.05007602274417877 + 1.0 * 6.043555736541748
Epoch 970, val loss: 0.9783100485801697
Epoch 980, training loss: 6.100332736968994 = 0.048312958329916 + 1.0 * 6.0520195960998535
Epoch 980, val loss: 0.9831051230430603
Epoch 990, training loss: 6.089530944824219 = 0.04664274677634239 + 1.0 * 6.042888164520264
Epoch 990, val loss: 0.9877973794937134
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.4982
Flip ASR: 0.4178/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.317740440368652 = 1.9442306756973267 + 1.0 * 8.373509407043457
Epoch 0, val loss: 1.9427956342697144
Epoch 10, training loss: 10.3037109375 = 1.9337410926818848 + 1.0 * 8.369970321655273
Epoch 10, val loss: 1.9303760528564453
Epoch 20, training loss: 10.284173011779785 = 1.9199696779251099 + 1.0 * 8.364203453063965
Epoch 20, val loss: 1.912033200263977
Epoch 30, training loss: 10.2333984375 = 1.902573585510254 + 1.0 * 8.330824851989746
Epoch 30, val loss: 1.8892687559127808
Epoch 40, training loss: 9.900341987609863 = 1.8863624334335327 + 1.0 * 8.0139799118042
Epoch 40, val loss: 1.8702466487884521
Epoch 50, training loss: 9.31014347076416 = 1.8723347187042236 + 1.0 * 7.437808990478516
Epoch 50, val loss: 1.8555341958999634
Epoch 60, training loss: 8.97481632232666 = 1.8579245805740356 + 1.0 * 7.116891860961914
Epoch 60, val loss: 1.8422966003417969
Epoch 70, training loss: 8.708612442016602 = 1.845765233039856 + 1.0 * 6.862847328186035
Epoch 70, val loss: 1.831030249595642
Epoch 80, training loss: 8.502238273620605 = 1.8354054689407349 + 1.0 * 6.66683292388916
Epoch 80, val loss: 1.8211677074432373
Epoch 90, training loss: 8.353649139404297 = 1.8258748054504395 + 1.0 * 6.527773857116699
Epoch 90, val loss: 1.8118276596069336
Epoch 100, training loss: 8.255683898925781 = 1.815633773803711 + 1.0 * 6.4400506019592285
Epoch 100, val loss: 1.8019800186157227
Epoch 110, training loss: 8.183151245117188 = 1.804431438446045 + 1.0 * 6.378720283508301
Epoch 110, val loss: 1.7914475202560425
Epoch 120, training loss: 8.130776405334473 = 1.7933130264282227 + 1.0 * 6.33746337890625
Epoch 120, val loss: 1.7809587717056274
Epoch 130, training loss: 8.085472106933594 = 1.7824045419692993 + 1.0 * 6.303067207336426
Epoch 130, val loss: 1.7708121538162231
Epoch 140, training loss: 8.04698657989502 = 1.771255373954773 + 1.0 * 6.275731563568115
Epoch 140, val loss: 1.760647177696228
Epoch 150, training loss: 8.01345443725586 = 1.7593157291412354 + 1.0 * 6.254138469696045
Epoch 150, val loss: 1.7501493692398071
Epoch 160, training loss: 7.981870651245117 = 1.7459763288497925 + 1.0 * 6.235894203186035
Epoch 160, val loss: 1.738861083984375
Epoch 170, training loss: 7.9536848068237305 = 1.7305564880371094 + 1.0 * 6.223128318786621
Epoch 170, val loss: 1.7263035774230957
Epoch 180, training loss: 7.920571804046631 = 1.7124816179275513 + 1.0 * 6.208090305328369
Epoch 180, val loss: 1.7119472026824951
Epoch 190, training loss: 7.886627197265625 = 1.6906980276107788 + 1.0 * 6.195929050445557
Epoch 190, val loss: 1.6949849128723145
Epoch 200, training loss: 7.849603652954102 = 1.663804531097412 + 1.0 * 6.1857991218566895
Epoch 200, val loss: 1.6742656230926514
Epoch 210, training loss: 7.8116607666015625 = 1.630259394645691 + 1.0 * 6.181401252746582
Epoch 210, val loss: 1.6484668254852295
Epoch 220, training loss: 7.760761260986328 = 1.5895742177963257 + 1.0 * 6.171186923980713
Epoch 220, val loss: 1.6172571182250977
Epoch 230, training loss: 7.705743789672852 = 1.5408508777618408 + 1.0 * 6.164892673492432
Epoch 230, val loss: 1.5798665285110474
Epoch 240, training loss: 7.645437240600586 = 1.484139084815979 + 1.0 * 6.1612982749938965
Epoch 240, val loss: 1.5360753536224365
Epoch 250, training loss: 7.579438209533691 = 1.422624111175537 + 1.0 * 6.156814098358154
Epoch 250, val loss: 1.4888758659362793
Epoch 260, training loss: 7.512247085571289 = 1.3599631786346436 + 1.0 * 6.152284145355225
Epoch 260, val loss: 1.4403579235076904
Epoch 270, training loss: 7.445235252380371 = 1.2981245517730713 + 1.0 * 6.147110939025879
Epoch 270, val loss: 1.392430305480957
Epoch 280, training loss: 7.385165214538574 = 1.2393423318862915 + 1.0 * 6.145823001861572
Epoch 280, val loss: 1.3470029830932617
Epoch 290, training loss: 7.327394485473633 = 1.185685157775879 + 1.0 * 6.141709327697754
Epoch 290, val loss: 1.3060650825500488
Epoch 300, training loss: 7.270331382751465 = 1.1362464427947998 + 1.0 * 6.134084701538086
Epoch 300, val loss: 1.2688812017440796
Epoch 310, training loss: 7.2188825607299805 = 1.0887600183486938 + 1.0 * 6.130122661590576
Epoch 310, val loss: 1.2337981462478638
Epoch 320, training loss: 7.17796516418457 = 1.042438268661499 + 1.0 * 6.13552713394165
Epoch 320, val loss: 1.2002826929092407
Epoch 330, training loss: 7.120321273803711 = 0.9970381259918213 + 1.0 * 6.1232829093933105
Epoch 330, val loss: 1.1675496101379395
Epoch 340, training loss: 7.072296142578125 = 0.9513794779777527 + 1.0 * 6.120916843414307
Epoch 340, val loss: 1.1348809003829956
Epoch 350, training loss: 7.022497653961182 = 0.9053131341934204 + 1.0 * 6.117184638977051
Epoch 350, val loss: 1.1019207239151
Epoch 360, training loss: 6.973786354064941 = 0.8591492176055908 + 1.0 * 6.11463737487793
Epoch 360, val loss: 1.0689783096313477
Epoch 370, training loss: 6.93745231628418 = 0.8141508102416992 + 1.0 * 6.1233015060424805
Epoch 370, val loss: 1.036744236946106
Epoch 380, training loss: 6.8833441734313965 = 0.7714880108833313 + 1.0 * 6.111855983734131
Epoch 380, val loss: 1.006440281867981
Epoch 390, training loss: 6.838418006896973 = 0.7308215498924255 + 1.0 * 6.107596397399902
Epoch 390, val loss: 0.9779066443443298
Epoch 400, training loss: 6.797553062438965 = 0.6920104026794434 + 1.0 * 6.1055426597595215
Epoch 400, val loss: 0.9511166214942932
Epoch 410, training loss: 6.764442443847656 = 0.655472457408905 + 1.0 * 6.1089701652526855
Epoch 410, val loss: 0.9264419078826904
Epoch 420, training loss: 6.7233123779296875 = 0.6214430332183838 + 1.0 * 6.101869583129883
Epoch 420, val loss: 0.9044556617736816
Epoch 430, training loss: 6.6870245933532715 = 0.5892862677574158 + 1.0 * 6.097738265991211
Epoch 430, val loss: 0.8845407366752625
Epoch 440, training loss: 6.657337188720703 = 0.5590067505836487 + 1.0 * 6.098330497741699
Epoch 440, val loss: 0.8665938377380371
Epoch 450, training loss: 6.627431869506836 = 0.5306867361068726 + 1.0 * 6.096745014190674
Epoch 450, val loss: 0.8508970737457275
Epoch 460, training loss: 6.594839572906494 = 0.5037661194801331 + 1.0 * 6.091073513031006
Epoch 460, val loss: 0.836982786655426
Epoch 470, training loss: 6.566503524780273 = 0.4781031906604767 + 1.0 * 6.088400363922119
Epoch 470, val loss: 0.8246945142745972
Epoch 480, training loss: 6.559775352478027 = 0.453635036945343 + 1.0 * 6.10614013671875
Epoch 480, val loss: 0.8141815662384033
Epoch 490, training loss: 6.519383430480957 = 0.43075940012931824 + 1.0 * 6.088624000549316
Epoch 490, val loss: 0.8054798245429993
Epoch 500, training loss: 6.493811130523682 = 0.40911394357681274 + 1.0 * 6.084697246551514
Epoch 500, val loss: 0.7983774542808533
Epoch 510, training loss: 6.471127986907959 = 0.388475239276886 + 1.0 * 6.082652568817139
Epoch 510, val loss: 0.7925905585289001
Epoch 520, training loss: 6.455397605895996 = 0.3687972128391266 + 1.0 * 6.086600303649902
Epoch 520, val loss: 0.7880004048347473
Epoch 530, training loss: 6.429642200469971 = 0.3501026928424835 + 1.0 * 6.0795392990112305
Epoch 530, val loss: 0.7845187187194824
Epoch 540, training loss: 6.410847187042236 = 0.3321908414363861 + 1.0 * 6.078656196594238
Epoch 540, val loss: 0.7819314002990723
Epoch 550, training loss: 6.39309024810791 = 0.3150361180305481 + 1.0 * 6.078053951263428
Epoch 550, val loss: 0.780108630657196
Epoch 560, training loss: 6.374547958374023 = 0.2986621558666229 + 1.0 * 6.075885772705078
Epoch 560, val loss: 0.7789723873138428
Epoch 570, training loss: 6.357279300689697 = 0.28289666771888733 + 1.0 * 6.074382781982422
Epoch 570, val loss: 0.7784085869789124
Epoch 580, training loss: 6.347652435302734 = 0.26773831248283386 + 1.0 * 6.079914093017578
Epoch 580, val loss: 0.7783351540565491
Epoch 590, training loss: 6.325584888458252 = 0.2532501220703125 + 1.0 * 6.0723347663879395
Epoch 590, val loss: 0.7787789106369019
Epoch 600, training loss: 6.309863567352295 = 0.2394268959760666 + 1.0 * 6.070436477661133
Epoch 600, val loss: 0.7796863913536072
Epoch 610, training loss: 6.305748462677002 = 0.2262401133775711 + 1.0 * 6.079508304595947
Epoch 610, val loss: 0.7810640931129456
Epoch 620, training loss: 6.2829108238220215 = 0.2138523906469345 + 1.0 * 6.069058418273926
Epoch 620, val loss: 0.7829286456108093
Epoch 630, training loss: 6.272892475128174 = 0.20217257738113403 + 1.0 * 6.0707197189331055
Epoch 630, val loss: 0.7852279543876648
Epoch 640, training loss: 6.256331443786621 = 0.19118459522724152 + 1.0 * 6.0651469230651855
Epoch 640, val loss: 0.7879845499992371
Epoch 650, training loss: 6.245915412902832 = 0.18085385859012604 + 1.0 * 6.065061569213867
Epoch 650, val loss: 0.7912310361862183
Epoch 660, training loss: 6.238434314727783 = 0.17114153504371643 + 1.0 * 6.0672926902771
Epoch 660, val loss: 0.7949028611183167
Epoch 670, training loss: 6.228783130645752 = 0.162090465426445 + 1.0 * 6.06669282913208
Epoch 670, val loss: 0.7990207672119141
Epoch 680, training loss: 6.215329170227051 = 0.15366028249263763 + 1.0 * 6.061668872833252
Epoch 680, val loss: 0.8034682869911194
Epoch 690, training loss: 6.213496208190918 = 0.1457807421684265 + 1.0 * 6.067715644836426
Epoch 690, val loss: 0.8082095384597778
Epoch 700, training loss: 6.197144985198975 = 0.13843177258968353 + 1.0 * 6.058713436126709
Epoch 700, val loss: 0.8132607340812683
Epoch 710, training loss: 6.189625263214111 = 0.13154181838035583 + 1.0 * 6.058083534240723
Epoch 710, val loss: 0.8186200857162476
Epoch 720, training loss: 6.193706512451172 = 0.12507322430610657 + 1.0 * 6.068633079528809
Epoch 720, val loss: 0.8242066502571106
Epoch 730, training loss: 6.177701473236084 = 0.1190313994884491 + 1.0 * 6.0586700439453125
Epoch 730, val loss: 0.8299580216407776
Epoch 740, training loss: 6.171926975250244 = 0.11336733400821686 + 1.0 * 6.058559417724609
Epoch 740, val loss: 0.835942804813385
Epoch 750, training loss: 6.161532402038574 = 0.108052559196949 + 1.0 * 6.0534796714782715
Epoch 750, val loss: 0.8419911861419678
Epoch 760, training loss: 6.1558942794799805 = 0.1030556783080101 + 1.0 * 6.0528388023376465
Epoch 760, val loss: 0.8482418060302734
Epoch 770, training loss: 6.151945114135742 = 0.09834089130163193 + 1.0 * 6.0536041259765625
Epoch 770, val loss: 0.8546150326728821
Epoch 780, training loss: 6.148821830749512 = 0.09389035403728485 + 1.0 * 6.054931640625
Epoch 780, val loss: 0.8610445261001587
Epoch 790, training loss: 6.139437675476074 = 0.08971232175827026 + 1.0 * 6.049725532531738
Epoch 790, val loss: 0.8675457835197449
Epoch 800, training loss: 6.138429641723633 = 0.08577151596546173 + 1.0 * 6.0526580810546875
Epoch 800, val loss: 0.8741260170936584
Epoch 810, training loss: 6.132894992828369 = 0.08205004036426544 + 1.0 * 6.050845146179199
Epoch 810, val loss: 0.8807063698768616
Epoch 820, training loss: 6.127879619598389 = 0.07853525131940842 + 1.0 * 6.049344539642334
Epoch 820, val loss: 0.8873631358146667
Epoch 830, training loss: 6.123996734619141 = 0.07520102709531784 + 1.0 * 6.048795700073242
Epoch 830, val loss: 0.8940556049346924
Epoch 840, training loss: 6.120361804962158 = 0.07205075770616531 + 1.0 * 6.048311233520508
Epoch 840, val loss: 0.9007155299186707
Epoch 850, training loss: 6.114546775817871 = 0.06907718628644943 + 1.0 * 6.045469760894775
Epoch 850, val loss: 0.907429575920105
Epoch 860, training loss: 6.110583782196045 = 0.0662570372223854 + 1.0 * 6.0443267822265625
Epoch 860, val loss: 0.9141612648963928
Epoch 870, training loss: 6.111513137817383 = 0.06357405334711075 + 1.0 * 6.047939300537109
Epoch 870, val loss: 0.9209513664245605
Epoch 880, training loss: 6.106380939483643 = 0.06103261932730675 + 1.0 * 6.045348167419434
Epoch 880, val loss: 0.9276363253593445
Epoch 890, training loss: 6.105233669281006 = 0.058628857135772705 + 1.0 * 6.046604633331299
Epoch 890, val loss: 0.9343162775039673
Epoch 900, training loss: 6.098579406738281 = 0.0563633032143116 + 1.0 * 6.0422163009643555
Epoch 900, val loss: 0.9409739375114441
Epoch 910, training loss: 6.096249580383301 = 0.05420829728245735 + 1.0 * 6.042041301727295
Epoch 910, val loss: 0.947611927986145
Epoch 920, training loss: 6.092737674713135 = 0.052159905433654785 + 1.0 * 6.0405778884887695
Epoch 920, val loss: 0.9542354941368103
Epoch 930, training loss: 6.090696334838867 = 0.050210535526275635 + 1.0 * 6.040485858917236
Epoch 930, val loss: 0.9608832597732544
Epoch 940, training loss: 6.0910539627075195 = 0.04835903272032738 + 1.0 * 6.042695045471191
Epoch 940, val loss: 0.9674428105354309
Epoch 950, training loss: 6.084839820861816 = 0.046605903655290604 + 1.0 * 6.038233757019043
Epoch 950, val loss: 0.9739497303962708
Epoch 960, training loss: 6.082074165344238 = 0.04494520276784897 + 1.0 * 6.037128925323486
Epoch 960, val loss: 0.9803513288497925
Epoch 970, training loss: 6.079524517059326 = 0.04335847496986389 + 1.0 * 6.036166191101074
Epoch 970, val loss: 0.986788809299469
Epoch 980, training loss: 6.081993579864502 = 0.04184260591864586 + 1.0 * 6.040151119232178
Epoch 980, val loss: 0.9931997656822205
Epoch 990, training loss: 6.075937747955322 = 0.04040220379829407 + 1.0 * 6.0355353355407715
Epoch 990, val loss: 0.9995946884155273
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.1808
Flip ASR: 0.2044/225 nodes
The final ASR:0.44895, 0.20187, Accuracy:0.77901, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10548])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.323643684387207 = 1.9498523473739624 + 1.0 * 8.373791694641113
Epoch 0, val loss: 1.950160026550293
Epoch 10, training loss: 10.313148498535156 = 1.939875841140747 + 1.0 * 8.373272895812988
Epoch 10, val loss: 1.9407780170440674
Epoch 20, training loss: 10.298428535461426 = 1.9279476404190063 + 1.0 * 8.37048053741455
Epoch 20, val loss: 1.9291560649871826
Epoch 30, training loss: 10.26344108581543 = 1.91200590133667 + 1.0 * 8.351435661315918
Epoch 30, val loss: 1.9133245944976807
Epoch 40, training loss: 10.081012725830078 = 1.8921122550964355 + 1.0 * 8.1889009475708
Epoch 40, val loss: 1.8938491344451904
Epoch 50, training loss: 9.254222869873047 = 1.8711159229278564 + 1.0 * 7.3831071853637695
Epoch 50, val loss: 1.8732656240463257
Epoch 60, training loss: 8.781928062438965 = 1.8556169271469116 + 1.0 * 6.926311016082764
Epoch 60, val loss: 1.8589847087860107
Epoch 70, training loss: 8.517894744873047 = 1.8446682691574097 + 1.0 * 6.673226356506348
Epoch 70, val loss: 1.847853183746338
Epoch 80, training loss: 8.377435684204102 = 1.8319483995437622 + 1.0 * 6.545486927032471
Epoch 80, val loss: 1.8354932069778442
Epoch 90, training loss: 8.2786283493042 = 1.8196738958358765 + 1.0 * 6.458954811096191
Epoch 90, val loss: 1.8236829042434692
Epoch 100, training loss: 8.20765495300293 = 1.8075207471847534 + 1.0 * 6.400134563446045
Epoch 100, val loss: 1.812150239944458
Epoch 110, training loss: 8.154780387878418 = 1.7957061529159546 + 1.0 * 6.359074592590332
Epoch 110, val loss: 1.8009084463119507
Epoch 120, training loss: 8.110254287719727 = 1.7840310335159302 + 1.0 * 6.326223373413086
Epoch 120, val loss: 1.7898669242858887
Epoch 130, training loss: 8.072066307067871 = 1.7719789743423462 + 1.0 * 6.300086975097656
Epoch 130, val loss: 1.7786316871643066
Epoch 140, training loss: 8.035683631896973 = 1.7591153383255005 + 1.0 * 6.276568412780762
Epoch 140, val loss: 1.7667596340179443
Epoch 150, training loss: 8.001016616821289 = 1.7445589303970337 + 1.0 * 6.256457805633545
Epoch 150, val loss: 1.7537373304367065
Epoch 160, training loss: 7.968908309936523 = 1.7272967100143433 + 1.0 * 6.241611480712891
Epoch 160, val loss: 1.7387741804122925
Epoch 170, training loss: 7.931976318359375 = 1.7066075801849365 + 1.0 * 6.225368976593018
Epoch 170, val loss: 1.7211909294128418
Epoch 180, training loss: 7.893848896026611 = 1.6814616918563843 + 1.0 * 6.2123870849609375
Epoch 180, val loss: 1.7000013589859009
Epoch 190, training loss: 7.853486061096191 = 1.6505423784255981 + 1.0 * 6.202943801879883
Epoch 190, val loss: 1.6739856004714966
Epoch 200, training loss: 7.80599308013916 = 1.613247036933899 + 1.0 * 6.192746162414551
Epoch 200, val loss: 1.6426645517349243
Epoch 210, training loss: 7.75337553024292 = 1.5686265230178833 + 1.0 * 6.184749126434326
Epoch 210, val loss: 1.6054269075393677
Epoch 220, training loss: 7.694271087646484 = 1.5160753726959229 + 1.0 * 6.178195476531982
Epoch 220, val loss: 1.5619895458221436
Epoch 230, training loss: 7.631930828094482 = 1.457166314125061 + 1.0 * 6.174764633178711
Epoch 230, val loss: 1.5138840675354004
Epoch 240, training loss: 7.561307907104492 = 1.3945711851119995 + 1.0 * 6.166736602783203
Epoch 240, val loss: 1.46339750289917
Epoch 250, training loss: 7.490860939025879 = 1.329384446144104 + 1.0 * 6.1614766120910645
Epoch 250, val loss: 1.4115369319915771
Epoch 260, training loss: 7.423376560211182 = 1.26394522190094 + 1.0 * 6.159431457519531
Epoch 260, val loss: 1.360396385192871
Epoch 270, training loss: 7.353341102600098 = 1.2012463808059692 + 1.0 * 6.152094841003418
Epoch 270, val loss: 1.3122761249542236
Epoch 280, training loss: 7.288939952850342 = 1.1414259672164917 + 1.0 * 6.1475138664245605
Epoch 280, val loss: 1.2665935754776
Epoch 290, training loss: 7.2304229736328125 = 1.0851764678955078 + 1.0 * 6.145246505737305
Epoch 290, val loss: 1.2241761684417725
Epoch 300, training loss: 7.172521114349365 = 1.0329070091247559 + 1.0 * 6.139614105224609
Epoch 300, val loss: 1.185192346572876
Epoch 310, training loss: 7.12079381942749 = 0.9845370650291443 + 1.0 * 6.136256694793701
Epoch 310, val loss: 1.1490432024002075
Epoch 320, training loss: 7.0700578689575195 = 0.9393801093101501 + 1.0 * 6.130677700042725
Epoch 320, val loss: 1.1155213117599487
Epoch 330, training loss: 7.023397922515869 = 0.8964839577674866 + 1.0 * 6.126914024353027
Epoch 330, val loss: 1.083493947982788
Epoch 340, training loss: 6.980987548828125 = 0.8555254340171814 + 1.0 * 6.125462055206299
Epoch 340, val loss: 1.0528455972671509
Epoch 350, training loss: 6.936888694763184 = 0.8165724277496338 + 1.0 * 6.120316505432129
Epoch 350, val loss: 1.0236696004867554
Epoch 360, training loss: 6.896585941314697 = 0.7788448929786682 + 1.0 * 6.117741107940674
Epoch 360, val loss: 0.9954456686973572
Epoch 370, training loss: 6.858509063720703 = 0.7424002289772034 + 1.0 * 6.1161088943481445
Epoch 370, val loss: 0.9679720401763916
Epoch 380, training loss: 6.818103313446045 = 0.7071628570556641 + 1.0 * 6.110940456390381
Epoch 380, val loss: 0.9417776465415955
Epoch 390, training loss: 6.785741806030273 = 0.6728331446647644 + 1.0 * 6.112908840179443
Epoch 390, val loss: 0.9164791107177734
Epoch 400, training loss: 6.746615409851074 = 0.6397278308868408 + 1.0 * 6.106887340545654
Epoch 400, val loss: 0.8925045728683472
Epoch 410, training loss: 6.711086750030518 = 0.6075311899185181 + 1.0 * 6.103555679321289
Epoch 410, val loss: 0.8699294924736023
Epoch 420, training loss: 6.681746959686279 = 0.5760810971260071 + 1.0 * 6.105665683746338
Epoch 420, val loss: 0.8485428690910339
Epoch 430, training loss: 6.647784233093262 = 0.5459011197090149 + 1.0 * 6.1018829345703125
Epoch 430, val loss: 0.8287487626075745
Epoch 440, training loss: 6.613093852996826 = 0.516830563545227 + 1.0 * 6.096263408660889
Epoch 440, val loss: 0.8108308911323547
Epoch 450, training loss: 6.584352970123291 = 0.4886073172092438 + 1.0 * 6.09574556350708
Epoch 450, val loss: 0.7941429018974304
Epoch 460, training loss: 6.5612897872924805 = 0.4615367650985718 + 1.0 * 6.099752902984619
Epoch 460, val loss: 0.7791544198989868
Epoch 470, training loss: 6.52720308303833 = 0.43588295578956604 + 1.0 * 6.091320037841797
Epoch 470, val loss: 0.7662163376808167
Epoch 480, training loss: 6.499144554138184 = 0.4113190770149231 + 1.0 * 6.087825298309326
Epoch 480, val loss: 0.7547155022621155
Epoch 490, training loss: 6.474245548248291 = 0.3878253996372223 + 1.0 * 6.086420059204102
Epoch 490, val loss: 0.7448228597640991
Epoch 500, training loss: 6.456790924072266 = 0.3655344545841217 + 1.0 * 6.091256618499756
Epoch 500, val loss: 0.7364910840988159
Epoch 510, training loss: 6.429661273956299 = 0.3447803258895874 + 1.0 * 6.084880828857422
Epoch 510, val loss: 0.7299101948738098
Epoch 520, training loss: 6.40783166885376 = 0.32519441843032837 + 1.0 * 6.082637310028076
Epoch 520, val loss: 0.7248049974441528
Epoch 530, training loss: 6.386678695678711 = 0.306537926197052 + 1.0 * 6.080140590667725
Epoch 530, val loss: 0.7207631468772888
Epoch 540, training loss: 6.3775739669799805 = 0.28877532482147217 + 1.0 * 6.088798522949219
Epoch 540, val loss: 0.7178518772125244
Epoch 550, training loss: 6.350218772888184 = 0.27202922105789185 + 1.0 * 6.078189373016357
Epoch 550, val loss: 0.7159912586212158
Epoch 560, training loss: 6.335683345794678 = 0.2562144696712494 + 1.0 * 6.079468727111816
Epoch 560, val loss: 0.7152072191238403
Epoch 570, training loss: 6.315446376800537 = 0.24126775562763214 + 1.0 * 6.074178695678711
Epoch 570, val loss: 0.7152794003486633
Epoch 580, training loss: 6.304884910583496 = 0.2271278202533722 + 1.0 * 6.077756881713867
Epoch 580, val loss: 0.7162566184997559
Epoch 590, training loss: 6.2878265380859375 = 0.21389992535114288 + 1.0 * 6.0739264488220215
Epoch 590, val loss: 0.7180235981941223
Epoch 600, training loss: 6.277122974395752 = 0.20150908827781677 + 1.0 * 6.075613975524902
Epoch 600, val loss: 0.7206613421440125
Epoch 610, training loss: 6.260570526123047 = 0.19001060724258423 + 1.0 * 6.070559978485107
Epoch 610, val loss: 0.7240166664123535
Epoch 620, training loss: 6.254809856414795 = 0.17925630509853363 + 1.0 * 6.0755534172058105
Epoch 620, val loss: 0.7280778884887695
Epoch 630, training loss: 6.236989498138428 = 0.16934072971343994 + 1.0 * 6.067648887634277
Epoch 630, val loss: 0.7327178120613098
Epoch 640, training loss: 6.225280284881592 = 0.1600957065820694 + 1.0 * 6.065184593200684
Epoch 640, val loss: 0.7380825877189636
Epoch 650, training loss: 6.215935707092285 = 0.15145617723464966 + 1.0 * 6.064479351043701
Epoch 650, val loss: 0.7439322471618652
Epoch 660, training loss: 6.20861291885376 = 0.14343826472759247 + 1.0 * 6.065174579620361
Epoch 660, val loss: 0.7501304745674133
Epoch 670, training loss: 6.198388576507568 = 0.13608278334140778 + 1.0 * 6.062305927276611
Epoch 670, val loss: 0.7569127678871155
Epoch 680, training loss: 6.189063549041748 = 0.12922368943691254 + 1.0 * 6.059839725494385
Epoch 680, val loss: 0.7640177011489868
Epoch 690, training loss: 6.181318283081055 = 0.12277179956436157 + 1.0 * 6.058546543121338
Epoch 690, val loss: 0.7712892889976501
Epoch 700, training loss: 6.1859869956970215 = 0.1167249009013176 + 1.0 * 6.0692620277404785
Epoch 700, val loss: 0.7788470983505249
Epoch 710, training loss: 6.169340133666992 = 0.11112853139638901 + 1.0 * 6.058211803436279
Epoch 710, val loss: 0.7865387797355652
Epoch 720, training loss: 6.162391185760498 = 0.1058739572763443 + 1.0 * 6.056517124176025
Epoch 720, val loss: 0.794495165348053
Epoch 730, training loss: 6.158016204833984 = 0.10092610865831375 + 1.0 * 6.0570902824401855
Epoch 730, val loss: 0.8024992346763611
Epoch 740, training loss: 6.150439262390137 = 0.09629128873348236 + 1.0 * 6.054148197174072
Epoch 740, val loss: 0.810535728931427
Epoch 750, training loss: 6.1512861251831055 = 0.09194420278072357 + 1.0 * 6.059341907501221
Epoch 750, val loss: 0.8187150359153748
Epoch 760, training loss: 6.140803813934326 = 0.08786524087190628 + 1.0 * 6.052938461303711
Epoch 760, val loss: 0.8268897533416748
Epoch 770, training loss: 6.134551525115967 = 0.0840110033750534 + 1.0 * 6.050540447235107
Epoch 770, val loss: 0.8351402878761292
Epoch 780, training loss: 6.130488395690918 = 0.08035992830991745 + 1.0 * 6.05012845993042
Epoch 780, val loss: 0.843427836894989
Epoch 790, training loss: 6.139533042907715 = 0.07691875100135803 + 1.0 * 6.062614440917969
Epoch 790, val loss: 0.8516134023666382
Epoch 800, training loss: 6.129754543304443 = 0.07369992882013321 + 1.0 * 6.056054592132568
Epoch 800, val loss: 0.8597409129142761
Epoch 810, training loss: 6.119281768798828 = 0.07069040834903717 + 1.0 * 6.048591136932373
Epoch 810, val loss: 0.8680663704872131
Epoch 820, training loss: 6.114248275756836 = 0.06782406568527222 + 1.0 * 6.046424388885498
Epoch 820, val loss: 0.8761863708496094
Epoch 830, training loss: 6.110184192657471 = 0.06509719043970108 + 1.0 * 6.045086860656738
Epoch 830, val loss: 0.8843684792518616
Epoch 840, training loss: 6.1175971031188965 = 0.06251295655965805 + 1.0 * 6.055084228515625
Epoch 840, val loss: 0.8925145864486694
Epoch 850, training loss: 6.1069746017456055 = 0.06007000803947449 + 1.0 * 6.046904563903809
Epoch 850, val loss: 0.9005401134490967
Epoch 860, training loss: 6.100712776184082 = 0.05777141824364662 + 1.0 * 6.042941570281982
Epoch 860, val loss: 0.9086573719978333
Epoch 870, training loss: 6.099567413330078 = 0.05558156967163086 + 1.0 * 6.043985843658447
Epoch 870, val loss: 0.9165956974029541
Epoch 880, training loss: 6.097275733947754 = 0.05350165441632271 + 1.0 * 6.043774127960205
Epoch 880, val loss: 0.9243763089179993
Epoch 890, training loss: 6.092816352844238 = 0.051545288413763046 + 1.0 * 6.041271209716797
Epoch 890, val loss: 0.9322651028633118
Epoch 900, training loss: 6.0910868644714355 = 0.04967588931322098 + 1.0 * 6.04141092300415
Epoch 900, val loss: 0.9400569200515747
Epoch 910, training loss: 6.0928473472595215 = 0.04789889603853226 + 1.0 * 6.044948577880859
Epoch 910, val loss: 0.9476104974746704
Epoch 920, training loss: 6.086394309997559 = 0.046222105622291565 + 1.0 * 6.040172100067139
Epoch 920, val loss: 0.9552761316299438
Epoch 930, training loss: 6.083255290985107 = 0.044619861990213394 + 1.0 * 6.03863525390625
Epoch 930, val loss: 0.9628611207008362
Epoch 940, training loss: 6.085518836975098 = 0.043090369552373886 + 1.0 * 6.042428493499756
Epoch 940, val loss: 0.9702854752540588
Epoch 950, training loss: 6.080842018127441 = 0.04163279011845589 + 1.0 * 6.039209365844727
Epoch 950, val loss: 0.9775450825691223
Epoch 960, training loss: 6.079063415527344 = 0.0402577705681324 + 1.0 * 6.0388054847717285
Epoch 960, val loss: 0.9849456548690796
Epoch 970, training loss: 6.078157901763916 = 0.038938235491514206 + 1.0 * 6.039219856262207
Epoch 970, val loss: 0.9921175241470337
Epoch 980, training loss: 6.072186470031738 = 0.0376792848110199 + 1.0 * 6.0345072746276855
Epoch 980, val loss: 0.9992918968200684
Epoch 990, training loss: 6.074960708618164 = 0.03647689148783684 + 1.0 * 6.038483619689941
Epoch 990, val loss: 1.0064274072647095
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6384
Flip ASR: 0.5689/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.311067581176758 = 1.9373281002044678 + 1.0 * 8.373739242553711
Epoch 0, val loss: 1.931501865386963
Epoch 10, training loss: 10.300742149353027 = 1.9277650117874146 + 1.0 * 8.372977256774902
Epoch 10, val loss: 1.9212325811386108
Epoch 20, training loss: 10.284504890441895 = 1.9158647060394287 + 1.0 * 8.368639945983887
Epoch 20, val loss: 1.9081743955612183
Epoch 30, training loss: 10.241154670715332 = 1.8997641801834106 + 1.0 * 8.341390609741211
Epoch 30, val loss: 1.8905799388885498
Epoch 40, training loss: 10.02371597290039 = 1.8808770179748535 + 1.0 * 8.142838478088379
Epoch 40, val loss: 1.871333360671997
Epoch 50, training loss: 9.239118576049805 = 1.8601486682891846 + 1.0 * 7.378969669342041
Epoch 50, val loss: 1.8503965139389038
Epoch 60, training loss: 8.819489479064941 = 1.8439228534698486 + 1.0 * 6.975566864013672
Epoch 60, val loss: 1.8356363773345947
Epoch 70, training loss: 8.561888694763184 = 1.8334134817123413 + 1.0 * 6.728475570678711
Epoch 70, val loss: 1.8256940841674805
Epoch 80, training loss: 8.430728912353516 = 1.8202564716339111 + 1.0 * 6.610472679138184
Epoch 80, val loss: 1.813171148300171
Epoch 90, training loss: 8.335643768310547 = 1.808153748512268 + 1.0 * 6.527490139007568
Epoch 90, val loss: 1.8018410205841064
Epoch 100, training loss: 8.253972053527832 = 1.7967454195022583 + 1.0 * 6.457226753234863
Epoch 100, val loss: 1.7912492752075195
Epoch 110, training loss: 8.188316345214844 = 1.7869247198104858 + 1.0 * 6.401391983032227
Epoch 110, val loss: 1.782067060470581
Epoch 120, training loss: 8.133135795593262 = 1.776929259300232 + 1.0 * 6.356206893920898
Epoch 120, val loss: 1.7730016708374023
Epoch 130, training loss: 8.085577964782715 = 1.765920877456665 + 1.0 * 6.319657325744629
Epoch 130, val loss: 1.7634507417678833
Epoch 140, training loss: 8.044071197509766 = 1.7536317110061646 + 1.0 * 6.290439605712891
Epoch 140, val loss: 1.753139615058899
Epoch 150, training loss: 8.005977630615234 = 1.7394074201583862 + 1.0 * 6.266570091247559
Epoch 150, val loss: 1.7414093017578125
Epoch 160, training loss: 7.969117164611816 = 1.7225404977798462 + 1.0 * 6.24657678604126
Epoch 160, val loss: 1.7277559041976929
Epoch 170, training loss: 7.933607578277588 = 1.7023199796676636 + 1.0 * 6.231287479400635
Epoch 170, val loss: 1.7115898132324219
Epoch 180, training loss: 7.89498233795166 = 1.6780836582183838 + 1.0 * 6.216898441314697
Epoch 180, val loss: 1.6923357248306274
Epoch 190, training loss: 7.854213714599609 = 1.6487082242965698 + 1.0 * 6.20550537109375
Epoch 190, val loss: 1.6690709590911865
Epoch 200, training loss: 7.808642387390137 = 1.6129071712493896 + 1.0 * 6.195735454559326
Epoch 200, val loss: 1.6406323909759521
Epoch 210, training loss: 7.757559776306152 = 1.56986403465271 + 1.0 * 6.187695503234863
Epoch 210, val loss: 1.6063591241836548
Epoch 220, training loss: 7.700326919555664 = 1.5195133686065674 + 1.0 * 6.180813789367676
Epoch 220, val loss: 1.5660680532455444
Epoch 230, training loss: 7.635985374450684 = 1.4613643884658813 + 1.0 * 6.174621105194092
Epoch 230, val loss: 1.51925528049469
Epoch 240, training loss: 7.565887451171875 = 1.396394968032837 + 1.0 * 6.169492721557617
Epoch 240, val loss: 1.466553807258606
Epoch 250, training loss: 7.492111682891846 = 1.3270745277404785 + 1.0 * 6.165037155151367
Epoch 250, val loss: 1.409868597984314
Epoch 260, training loss: 7.416880130767822 = 1.2558215856552124 + 1.0 * 6.16105842590332
Epoch 260, val loss: 1.3511823415756226
Epoch 270, training loss: 7.34316349029541 = 1.1858587265014648 + 1.0 * 6.157304763793945
Epoch 270, val loss: 1.293065071105957
Epoch 280, training loss: 7.270176410675049 = 1.1178041696548462 + 1.0 * 6.152372360229492
Epoch 280, val loss: 1.2363686561584473
Epoch 290, training loss: 7.213983535766602 = 1.0524007081985474 + 1.0 * 6.161582946777344
Epoch 290, val loss: 1.181726336479187
Epoch 300, training loss: 7.137847900390625 = 0.9915448427200317 + 1.0 * 6.146303176879883
Epoch 300, val loss: 1.1312487125396729
Epoch 310, training loss: 7.0747575759887695 = 0.9338372349739075 + 1.0 * 6.140920162200928
Epoch 310, val loss: 1.083104133605957
Epoch 320, training loss: 7.015067100524902 = 0.8780525326728821 + 1.0 * 6.137014389038086
Epoch 320, val loss: 1.0367201566696167
Epoch 330, training loss: 6.962696075439453 = 0.8241052031517029 + 1.0 * 6.1385908126831055
Epoch 330, val loss: 0.9921724796295166
Epoch 340, training loss: 6.908468723297119 = 0.7736383080482483 + 1.0 * 6.134830474853516
Epoch 340, val loss: 0.9508908987045288
Epoch 350, training loss: 6.854461669921875 = 0.7264419198036194 + 1.0 * 6.1280198097229
Epoch 350, val loss: 0.9128543734550476
Epoch 360, training loss: 6.8056488037109375 = 0.6819250583648682 + 1.0 * 6.12372350692749
Epoch 360, val loss: 0.8774803280830383
Epoch 370, training loss: 6.761108875274658 = 0.6400097012519836 + 1.0 * 6.12109899520874
Epoch 370, val loss: 0.8450093269348145
Epoch 380, training loss: 6.722964286804199 = 0.6009235978126526 + 1.0 * 6.122040748596191
Epoch 380, val loss: 0.815746009349823
Epoch 390, training loss: 6.682339191436768 = 0.5647806525230408 + 1.0 * 6.117558479309082
Epoch 390, val loss: 0.7897048592567444
Epoch 400, training loss: 6.644963264465332 = 0.53082674741745 + 1.0 * 6.114136695861816
Epoch 400, val loss: 0.7661835551261902
Epoch 410, training loss: 6.610581398010254 = 0.49875080585479736 + 1.0 * 6.111830711364746
Epoch 410, val loss: 0.7450401186943054
Epoch 420, training loss: 6.579174041748047 = 0.4682723879814148 + 1.0 * 6.110901832580566
Epoch 420, val loss: 0.7258919477462769
Epoch 430, training loss: 6.547030925750732 = 0.4391016960144043 + 1.0 * 6.107929229736328
Epoch 430, val loss: 0.7084764838218689
Epoch 440, training loss: 6.518595218658447 = 0.41110044717788696 + 1.0 * 6.107494831085205
Epoch 440, val loss: 0.6925762891769409
Epoch 450, training loss: 6.487986087799072 = 0.3842147886753082 + 1.0 * 6.103771209716797
Epoch 450, val loss: 0.6780994534492493
Epoch 460, training loss: 6.462137222290039 = 0.35835719108581543 + 1.0 * 6.103780269622803
Epoch 460, val loss: 0.6649807691574097
Epoch 470, training loss: 6.435369491577148 = 0.3336307108402252 + 1.0 * 6.101738929748535
Epoch 470, val loss: 0.6532413363456726
Epoch 480, training loss: 6.407717227935791 = 0.31009790301322937 + 1.0 * 6.097619533538818
Epoch 480, val loss: 0.6428372263908386
Epoch 490, training loss: 6.382574558258057 = 0.2875717878341675 + 1.0 * 6.0950026512146
Epoch 490, val loss: 0.6336798071861267
Epoch 500, training loss: 6.360751152038574 = 0.26628583669662476 + 1.0 * 6.094465255737305
Epoch 500, val loss: 0.6258548498153687
Epoch 510, training loss: 6.337379455566406 = 0.24641916155815125 + 1.0 * 6.090960502624512
Epoch 510, val loss: 0.6194255948066711
Epoch 520, training loss: 6.316694259643555 = 0.2277848720550537 + 1.0 * 6.08890962600708
Epoch 520, val loss: 0.6141653060913086
Epoch 530, training loss: 6.301787376403809 = 0.21051883697509766 + 1.0 * 6.091268539428711
Epoch 530, val loss: 0.6101610660552979
Epoch 540, training loss: 6.283268928527832 = 0.1947573572397232 + 1.0 * 6.0885114669799805
Epoch 540, val loss: 0.6072545647621155
Epoch 550, training loss: 6.264496326446533 = 0.18023918569087982 + 1.0 * 6.084257125854492
Epoch 550, val loss: 0.6054166555404663
Epoch 560, training loss: 6.26193904876709 = 0.16690029203891754 + 1.0 * 6.095038890838623
Epoch 560, val loss: 0.6046372056007385
Epoch 570, training loss: 6.236525058746338 = 0.15483716130256653 + 1.0 * 6.081687927246094
Epoch 570, val loss: 0.604776918888092
Epoch 580, training loss: 6.223227500915527 = 0.14388668537139893 + 1.0 * 6.079340934753418
Epoch 580, val loss: 0.6055914163589478
Epoch 590, training loss: 6.211634635925293 = 0.13388723134994507 + 1.0 * 6.077747344970703
Epoch 590, val loss: 0.6071919798851013
Epoch 600, training loss: 6.2011332511901855 = 0.12475274503231049 + 1.0 * 6.076380729675293
Epoch 600, val loss: 0.6094394326210022
Epoch 610, training loss: 6.19642972946167 = 0.1164780929684639 + 1.0 * 6.079951763153076
Epoch 610, val loss: 0.612251877784729
Epoch 620, training loss: 6.182715892791748 = 0.10908515751361847 + 1.0 * 6.0736308097839355
Epoch 620, val loss: 0.6153910160064697
Epoch 630, training loss: 6.174498558044434 = 0.10235007852315903 + 1.0 * 6.072148323059082
Epoch 630, val loss: 0.6187978982925415
Epoch 640, training loss: 6.167440891265869 = 0.09617292135953903 + 1.0 * 6.071268081665039
Epoch 640, val loss: 0.6225838661193848
Epoch 650, training loss: 6.165951251983643 = 0.09052060544490814 + 1.0 * 6.075430870056152
Epoch 650, val loss: 0.6267021298408508
Epoch 660, training loss: 6.154892921447754 = 0.08538882434368134 + 1.0 * 6.069504261016846
Epoch 660, val loss: 0.6309231519699097
Epoch 670, training loss: 6.150753498077393 = 0.08067655563354492 + 1.0 * 6.070076942443848
Epoch 670, val loss: 0.6353339552879333
Epoch 680, training loss: 6.148898124694824 = 0.0763523131608963 + 1.0 * 6.072546005249023
Epoch 680, val loss: 0.6398930549621582
Epoch 690, training loss: 6.139371871948242 = 0.07238949090242386 + 1.0 * 6.066982269287109
Epoch 690, val loss: 0.6444898247718811
Epoch 700, training loss: 6.133269786834717 = 0.06872498244047165 + 1.0 * 6.064544677734375
Epoch 700, val loss: 0.649202287197113
Epoch 710, training loss: 6.128627777099609 = 0.06531446427106857 + 1.0 * 6.0633134841918945
Epoch 710, val loss: 0.6540393233299255
Epoch 720, training loss: 6.130451202392578 = 0.06215672567486763 + 1.0 * 6.068294525146484
Epoch 720, val loss: 0.6589407324790955
Epoch 730, training loss: 6.122372150421143 = 0.059256430715322495 + 1.0 * 6.06311559677124
Epoch 730, val loss: 0.6636967062950134
Epoch 740, training loss: 6.115496635437012 = 0.05654983967542648 + 1.0 * 6.05894660949707
Epoch 740, val loss: 0.6684946417808533
Epoch 750, training loss: 6.114056587219238 = 0.054007332772016525 + 1.0 * 6.060049057006836
Epoch 750, val loss: 0.673401415348053
Epoch 760, training loss: 6.1135663986206055 = 0.051639460027217865 + 1.0 * 6.06192684173584
Epoch 760, val loss: 0.6783333420753479
Epoch 770, training loss: 6.107028484344482 = 0.0494498685002327 + 1.0 * 6.0575785636901855
Epoch 770, val loss: 0.6830850839614868
Epoch 780, training loss: 6.103307247161865 = 0.0473940335214138 + 1.0 * 6.05591344833374
Epoch 780, val loss: 0.6877899765968323
Epoch 790, training loss: 6.099892616271973 = 0.045447785407304764 + 1.0 * 6.054444789886475
Epoch 790, val loss: 0.6925961971282959
Epoch 800, training loss: 6.100696086883545 = 0.04361102730035782 + 1.0 * 6.057085037231445
Epoch 800, val loss: 0.6973620057106018
Epoch 810, training loss: 6.101866245269775 = 0.041897352784872055 + 1.0 * 6.059968948364258
Epoch 810, val loss: 0.7021527886390686
Epoch 820, training loss: 6.092200756072998 = 0.040300313383340836 + 1.0 * 6.051900386810303
Epoch 820, val loss: 0.7066648006439209
Epoch 830, training loss: 6.08976936340332 = 0.03878815844655037 + 1.0 * 6.050981044769287
Epoch 830, val loss: 0.711222767829895
Epoch 840, training loss: 6.089599609375 = 0.03735005110502243 + 1.0 * 6.052249431610107
Epoch 840, val loss: 0.7158448696136475
Epoch 850, training loss: 6.085300445556641 = 0.03599545732140541 + 1.0 * 6.049304962158203
Epoch 850, val loss: 0.7204059958457947
Epoch 860, training loss: 6.083935260772705 = 0.03471628949046135 + 1.0 * 6.049219131469727
Epoch 860, val loss: 0.7248303890228271
Epoch 870, training loss: 6.086994171142578 = 0.03350427374243736 + 1.0 * 6.053489685058594
Epoch 870, val loss: 0.7293096780776978
Epoch 880, training loss: 6.080581188201904 = 0.032359201461076736 + 1.0 * 6.048222064971924
Epoch 880, val loss: 0.7336878180503845
Epoch 890, training loss: 6.087132930755615 = 0.031272631138563156 + 1.0 * 6.05586051940918
Epoch 890, val loss: 0.7380236387252808
Epoch 900, training loss: 6.079534530639648 = 0.0302414633333683 + 1.0 * 6.049293041229248
Epoch 900, val loss: 0.7423613667488098
Epoch 910, training loss: 6.074498176574707 = 0.02926623821258545 + 1.0 * 6.045231819152832
Epoch 910, val loss: 0.7465359568595886
Epoch 920, training loss: 6.081697940826416 = 0.02833154797554016 + 1.0 * 6.053366184234619
Epoch 920, val loss: 0.7508304715156555
Epoch 930, training loss: 6.071915626525879 = 0.027447031810879707 + 1.0 * 6.044468402862549
Epoch 930, val loss: 0.7549985647201538
Epoch 940, training loss: 6.0692548751831055 = 0.026604915037751198 + 1.0 * 6.042649745941162
Epoch 940, val loss: 0.7590669393539429
Epoch 950, training loss: 6.067646026611328 = 0.025794098153710365 + 1.0 * 6.041851997375488
Epoch 950, val loss: 0.7632319331169128
Epoch 960, training loss: 6.074012756347656 = 0.025020809844136238 + 1.0 * 6.048992156982422
Epoch 960, val loss: 0.7673380970954895
Epoch 970, training loss: 6.068899631500244 = 0.024287322536110878 + 1.0 * 6.044612407684326
Epoch 970, val loss: 0.7713715434074402
Epoch 980, training loss: 6.064743518829346 = 0.023593075573444366 + 1.0 * 6.0411505699157715
Epoch 980, val loss: 0.7753271460533142
Epoch 990, training loss: 6.061732292175293 = 0.022923089563846588 + 1.0 * 6.038809299468994
Epoch 990, val loss: 0.779260516166687
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.7638
Flip ASR: 0.7156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320590019226074 = 1.9468812942504883 + 1.0 * 8.373708724975586
Epoch 0, val loss: 1.950063705444336
Epoch 10, training loss: 10.309712409973145 = 1.9367989301681519 + 1.0 * 8.372913360595703
Epoch 10, val loss: 1.9399093389511108
Epoch 20, training loss: 10.292712211608887 = 1.9243409633636475 + 1.0 * 8.36837100982666
Epoch 20, val loss: 1.9268059730529785
Epoch 30, training loss: 10.246634483337402 = 1.9073734283447266 + 1.0 * 8.339261054992676
Epoch 30, val loss: 1.9086484909057617
Epoch 40, training loss: 9.998945236206055 = 1.8870346546173096 + 1.0 * 8.111910820007324
Epoch 40, val loss: 1.887406349182129
Epoch 50, training loss: 9.175836563110352 = 1.8643901348114014 + 1.0 * 7.311446189880371
Epoch 50, val loss: 1.8638471364974976
Epoch 60, training loss: 8.829874038696289 = 1.847474217414856 + 1.0 * 6.982399940490723
Epoch 60, val loss: 1.8466012477874756
Epoch 70, training loss: 8.579416275024414 = 1.8333779573440552 + 1.0 * 6.74603796005249
Epoch 70, val loss: 1.8316819667816162
Epoch 80, training loss: 8.414022445678711 = 1.818917155265808 + 1.0 * 6.595105171203613
Epoch 80, val loss: 1.8172533512115479
Epoch 90, training loss: 8.305194854736328 = 1.8049488067626953 + 1.0 * 6.500246524810791
Epoch 90, val loss: 1.803122639656067
Epoch 100, training loss: 8.229179382324219 = 1.7909674644470215 + 1.0 * 6.438211441040039
Epoch 100, val loss: 1.7893370389938354
Epoch 110, training loss: 8.165743827819824 = 1.7771440744400024 + 1.0 * 6.388599395751953
Epoch 110, val loss: 1.775623083114624
Epoch 120, training loss: 8.113388061523438 = 1.7633261680603027 + 1.0 * 6.350061416625977
Epoch 120, val loss: 1.7617647647857666
Epoch 130, training loss: 8.066824913024902 = 1.7486116886138916 + 1.0 * 6.318212985992432
Epoch 130, val loss: 1.7472127676010132
Epoch 140, training loss: 8.023965835571289 = 1.7323994636535645 + 1.0 * 6.291566371917725
Epoch 140, val loss: 1.731882095336914
Epoch 150, training loss: 7.983941078186035 = 1.7139954566955566 + 1.0 * 6.2699456214904785
Epoch 150, val loss: 1.7153822183609009
Epoch 160, training loss: 7.9442853927612305 = 1.6923596858978271 + 1.0 * 6.251925945281982
Epoch 160, val loss: 1.6968849897384644
Epoch 170, training loss: 7.906460762023926 = 1.666407823562622 + 1.0 * 6.240052700042725
Epoch 170, val loss: 1.675410270690918
Epoch 180, training loss: 7.860488414764404 = 1.6354084014892578 + 1.0 * 6.2250800132751465
Epoch 180, val loss: 1.6504589319229126
Epoch 190, training loss: 7.812686920166016 = 1.5979604721069336 + 1.0 * 6.214726448059082
Epoch 190, val loss: 1.6205629110336304
Epoch 200, training loss: 7.75891637802124 = 1.5528769493103027 + 1.0 * 6.2060394287109375
Epoch 200, val loss: 1.5846328735351562
Epoch 210, training loss: 7.699023246765137 = 1.500180959701538 + 1.0 * 6.1988420486450195
Epoch 210, val loss: 1.5431804656982422
Epoch 220, training loss: 7.634885311126709 = 1.4416664838790894 + 1.0 * 6.19321870803833
Epoch 220, val loss: 1.4975529909133911
Epoch 230, training loss: 7.566141605377197 = 1.3784288167953491 + 1.0 * 6.187712669372559
Epoch 230, val loss: 1.44853675365448
Epoch 240, training loss: 7.497845649719238 = 1.3133554458618164 + 1.0 * 6.184490203857422
Epoch 240, val loss: 1.3985929489135742
Epoch 250, training loss: 7.4280290603637695 = 1.2494893074035645 + 1.0 * 6.178539752960205
Epoch 250, val loss: 1.350215196609497
Epoch 260, training loss: 7.35875940322876 = 1.1860952377319336 + 1.0 * 6.172664165496826
Epoch 260, val loss: 1.3025838136672974
Epoch 270, training loss: 7.2915544509887695 = 1.1231610774993896 + 1.0 * 6.168393611907959
Epoch 270, val loss: 1.2555201053619385
Epoch 280, training loss: 7.224598407745361 = 1.0613270998001099 + 1.0 * 6.163271427154541
Epoch 280, val loss: 1.2089709043502808
Epoch 290, training loss: 7.160191535949707 = 0.9998493194580078 + 1.0 * 6.160342216491699
Epoch 290, val loss: 1.1627296209335327
Epoch 300, training loss: 7.095797538757324 = 0.9399254322052002 + 1.0 * 6.155872344970703
Epoch 300, val loss: 1.1180596351623535
Epoch 310, training loss: 7.033181190490723 = 0.8824065327644348 + 1.0 * 6.1507744789123535
Epoch 310, val loss: 1.075518012046814
Epoch 320, training loss: 6.976179122924805 = 0.8274053931236267 + 1.0 * 6.148773670196533
Epoch 320, val loss: 1.0354599952697754
Epoch 330, training loss: 6.91981840133667 = 0.7766076326370239 + 1.0 * 6.1432108879089355
Epoch 330, val loss: 0.9990885257720947
Epoch 340, training loss: 6.870360374450684 = 0.7302200794219971 + 1.0 * 6.140140533447266
Epoch 340, val loss: 0.966852605342865
Epoch 350, training loss: 6.825462341308594 = 0.6877835392951965 + 1.0 * 6.137678623199463
Epoch 350, val loss: 0.9381188154220581
Epoch 360, training loss: 6.786365032196045 = 0.6496205925941467 + 1.0 * 6.136744499206543
Epoch 360, val loss: 0.9129163026809692
Epoch 370, training loss: 6.745204448699951 = 0.6155136823654175 + 1.0 * 6.129690647125244
Epoch 370, val loss: 0.8911741375923157
Epoch 380, training loss: 6.710147380828857 = 0.5847038626670837 + 1.0 * 6.125443458557129
Epoch 380, val loss: 0.8722080588340759
Epoch 390, training loss: 6.682940483093262 = 0.5567770600318909 + 1.0 * 6.126163482666016
Epoch 390, val loss: 0.8558000326156616
Epoch 400, training loss: 6.651736736297607 = 0.5315351486206055 + 1.0 * 6.120201587677002
Epoch 400, val loss: 0.841947615146637
Epoch 410, training loss: 6.625230312347412 = 0.5082303285598755 + 1.0 * 6.117000102996826
Epoch 410, val loss: 0.8299664855003357
Epoch 420, training loss: 6.602406978607178 = 0.4865279495716095 + 1.0 * 6.115879058837891
Epoch 420, val loss: 0.8194428086280823
Epoch 430, training loss: 6.577500343322754 = 0.466235876083374 + 1.0 * 6.111264228820801
Epoch 430, val loss: 0.8103615641593933
Epoch 440, training loss: 6.5569987297058105 = 0.4470430612564087 + 1.0 * 6.109955787658691
Epoch 440, val loss: 0.8022322058677673
Epoch 450, training loss: 6.534769058227539 = 0.4287891387939453 + 1.0 * 6.105979919433594
Epoch 450, val loss: 0.7951590418815613
Epoch 460, training loss: 6.514203071594238 = 0.41115307807922363 + 1.0 * 6.103050231933594
Epoch 460, val loss: 0.7887635231018066
Epoch 470, training loss: 6.495804309844971 = 0.3940649628639221 + 1.0 * 6.101739406585693
Epoch 470, val loss: 0.7828386425971985
Epoch 480, training loss: 6.476357460021973 = 0.37760549783706665 + 1.0 * 6.098752021789551
Epoch 480, val loss: 0.7775593996047974
Epoch 490, training loss: 6.458062648773193 = 0.36152806878089905 + 1.0 * 6.096534729003906
Epoch 490, val loss: 0.7727236747741699
Epoch 500, training loss: 6.44578742980957 = 0.34580370783805847 + 1.0 * 6.0999836921691895
Epoch 500, val loss: 0.7681739926338196
Epoch 510, training loss: 6.423685550689697 = 0.3304832875728607 + 1.0 * 6.093202114105225
Epoch 510, val loss: 0.7639970183372498
Epoch 520, training loss: 6.410830497741699 = 0.31555551290512085 + 1.0 * 6.095274925231934
Epoch 520, val loss: 0.7602023482322693
Epoch 530, training loss: 6.3919196128845215 = 0.30107977986335754 + 1.0 * 6.090839862823486
Epoch 530, val loss: 0.7566807866096497
Epoch 540, training loss: 6.375195026397705 = 0.2869361340999603 + 1.0 * 6.088258743286133
Epoch 540, val loss: 0.753413200378418
Epoch 550, training loss: 6.365800857543945 = 0.2731187641620636 + 1.0 * 6.092681884765625
Epoch 550, val loss: 0.7502612471580505
Epoch 560, training loss: 6.346319675445557 = 0.25978177785873413 + 1.0 * 6.086537837982178
Epoch 560, val loss: 0.7472383975982666
Epoch 570, training loss: 6.330086708068848 = 0.24679778516292572 + 1.0 * 6.08328914642334
Epoch 570, val loss: 0.7444000244140625
Epoch 580, training loss: 6.316687107086182 = 0.23413905501365662 + 1.0 * 6.082548141479492
Epoch 580, val loss: 0.7417593002319336
Epoch 590, training loss: 6.304971218109131 = 0.22192412614822388 + 1.0 * 6.083046913146973
Epoch 590, val loss: 0.7391896843910217
Epoch 600, training loss: 6.2899322509765625 = 0.2102455347776413 + 1.0 * 6.079686641693115
Epoch 600, val loss: 0.7369110584259033
Epoch 610, training loss: 6.276785373687744 = 0.19901534914970398 + 1.0 * 6.077770233154297
Epoch 610, val loss: 0.7349346876144409
Epoch 620, training loss: 6.276158332824707 = 0.18823003768920898 + 1.0 * 6.087928295135498
Epoch 620, val loss: 0.7331565022468567
Epoch 630, training loss: 6.2540669441223145 = 0.17810726165771484 + 1.0 * 6.0759596824646
Epoch 630, val loss: 0.7316608428955078
Epoch 640, training loss: 6.243319034576416 = 0.1685081422328949 + 1.0 * 6.074810981750488
Epoch 640, val loss: 0.7305284142494202
Epoch 650, training loss: 6.231825351715088 = 0.15940052270889282 + 1.0 * 6.07242488861084
Epoch 650, val loss: 0.7297828793525696
Epoch 660, training loss: 6.223604202270508 = 0.15079665184020996 + 1.0 * 6.072807312011719
Epoch 660, val loss: 0.7293248772621155
Epoch 670, training loss: 6.22224760055542 = 0.1427866667509079 + 1.0 * 6.079461097717285
Epoch 670, val loss: 0.7292003035545349
Epoch 680, training loss: 6.2077412605285645 = 0.13537590205669403 + 1.0 * 6.0723652839660645
Epoch 680, val loss: 0.729436457157135
Epoch 690, training loss: 6.197000503540039 = 0.1284152865409851 + 1.0 * 6.068585395812988
Epoch 690, val loss: 0.7300752401351929
Epoch 700, training loss: 6.18830680847168 = 0.12187401950359344 + 1.0 * 6.066432952880859
Epoch 700, val loss: 0.7309964895248413
Epoch 710, training loss: 6.192435264587402 = 0.11572451889514923 + 1.0 * 6.0767107009887695
Epoch 710, val loss: 0.7321337461471558
Epoch 720, training loss: 6.174932956695557 = 0.11002366989850998 + 1.0 * 6.0649094581604
Epoch 720, val loss: 0.7335302829742432
Epoch 730, training loss: 6.167834758758545 = 0.1046845093369484 + 1.0 * 6.063150405883789
Epoch 730, val loss: 0.735086977481842
Epoch 740, training loss: 6.161473274230957 = 0.09965738654136658 + 1.0 * 6.0618157386779785
Epoch 740, val loss: 0.7368729114532471
Epoch 750, training loss: 6.165870666503906 = 0.09490778297185898 + 1.0 * 6.070962905883789
Epoch 750, val loss: 0.7387818098068237
Epoch 760, training loss: 6.151859283447266 = 0.09047473222017288 + 1.0 * 6.061384677886963
Epoch 760, val loss: 0.7407177090644836
Epoch 770, training loss: 6.148160934448242 = 0.08628181368112564 + 1.0 * 6.0618791580200195
Epoch 770, val loss: 0.74282306432724
Epoch 780, training loss: 6.141062259674072 = 0.08231529593467712 + 1.0 * 6.058746814727783
Epoch 780, val loss: 0.7450134754180908
Epoch 790, training loss: 6.145228862762451 = 0.07857052981853485 + 1.0 * 6.0666584968566895
Epoch 790, val loss: 0.7472774386405945
Epoch 800, training loss: 6.133182525634766 = 0.07500963658094406 + 1.0 * 6.058172702789307
Epoch 800, val loss: 0.7495233416557312
Epoch 810, training loss: 6.126431941986084 = 0.07162459939718246 + 1.0 * 6.054807186126709
Epoch 810, val loss: 0.7518345713615417
Epoch 820, training loss: 6.121967792510986 = 0.06837540119886398 + 1.0 * 6.053592205047607
Epoch 820, val loss: 0.754244327545166
Epoch 830, training loss: 6.1333394050598145 = 0.06526583433151245 + 1.0 * 6.068073749542236
Epoch 830, val loss: 0.7566465139389038
Epoch 840, training loss: 6.114079475402832 = 0.062301285564899445 + 1.0 * 6.051778316497803
Epoch 840, val loss: 0.7589998841285706
Epoch 850, training loss: 6.1102447509765625 = 0.05944085121154785 + 1.0 * 6.0508036613464355
Epoch 850, val loss: 0.7613133788108826
Epoch 860, training loss: 6.110002517700195 = 0.0566561333835125 + 1.0 * 6.053346157073975
Epoch 860, val loss: 0.7636875510215759
Epoch 870, training loss: 6.10540246963501 = 0.053966011852025986 + 1.0 * 6.051436424255371
Epoch 870, val loss: 0.7660368084907532
Epoch 880, training loss: 6.100355625152588 = 0.051349472254514694 + 1.0 * 6.04900598526001
Epoch 880, val loss: 0.7683568596839905
Epoch 890, training loss: 6.096599102020264 = 0.04880331829190254 + 1.0 * 6.04779577255249
Epoch 890, val loss: 0.7707515954971313
Epoch 900, training loss: 6.09439754486084 = 0.046326104551553726 + 1.0 * 6.048071384429932
Epoch 900, val loss: 0.7731958627700806
Epoch 910, training loss: 6.095155239105225 = 0.04397111386060715 + 1.0 * 6.051184177398682
Epoch 910, val loss: 0.7756966352462769
Epoch 920, training loss: 6.08662748336792 = 0.041775234043598175 + 1.0 * 6.044852256774902
Epoch 920, val loss: 0.7782021164894104
Epoch 930, training loss: 6.084462642669678 = 0.03972916677594185 + 1.0 * 6.04473352432251
Epoch 930, val loss: 0.7809105515480042
Epoch 940, training loss: 6.080544948577881 = 0.0378461554646492 + 1.0 * 6.042698860168457
Epoch 940, val loss: 0.7838628888130188
Epoch 950, training loss: 6.0811991691589355 = 0.036121632903814316 + 1.0 * 6.045077323913574
Epoch 950, val loss: 0.7870107889175415
Epoch 960, training loss: 6.081686019897461 = 0.03456240892410278 + 1.0 * 6.047123432159424
Epoch 960, val loss: 0.7903278470039368
Epoch 970, training loss: 6.0743560791015625 = 0.03313300758600235 + 1.0 * 6.041223049163818
Epoch 970, val loss: 0.7936521172523499
Epoch 980, training loss: 6.072373867034912 = 0.031809039413928986 + 1.0 * 6.040565013885498
Epoch 980, val loss: 0.7971236705780029
Epoch 990, training loss: 6.070418834686279 = 0.0305732823908329 + 1.0 * 6.0398454666137695
Epoch 990, val loss: 0.8007054328918457
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5646
Flip ASR: 0.5156/225 nodes
The final ASR:0.65560, 0.08225, Accuracy:0.83210, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83086, 0.00761
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.338533401489258 = 1.9646782875061035 + 1.0 * 8.373855590820312
Epoch 0, val loss: 1.9594287872314453
Epoch 10, training loss: 10.32725715637207 = 1.953802466392517 + 1.0 * 8.373455047607422
Epoch 10, val loss: 1.9493849277496338
Epoch 20, training loss: 10.311771392822266 = 1.940571904182434 + 1.0 * 8.371199607849121
Epoch 20, val loss: 1.9367996454238892
Epoch 30, training loss: 10.27765941619873 = 1.9225361347198486 + 1.0 * 8.355123519897461
Epoch 30, val loss: 1.9194257259368896
Epoch 40, training loss: 10.102609634399414 = 1.8997118473052979 + 1.0 * 8.202898025512695
Epoch 40, val loss: 1.8980430364608765
Epoch 50, training loss: 9.077630043029785 = 1.8771827220916748 + 1.0 * 7.200447082519531
Epoch 50, val loss: 1.8770092725753784
Epoch 60, training loss: 8.75113582611084 = 1.8602814674377441 + 1.0 * 6.890854358673096
Epoch 60, val loss: 1.8620712757110596
Epoch 70, training loss: 8.57728385925293 = 1.8453940153121948 + 1.0 * 6.7318902015686035
Epoch 70, val loss: 1.8485010862350464
Epoch 80, training loss: 8.460479736328125 = 1.8296558856964111 + 1.0 * 6.630824089050293
Epoch 80, val loss: 1.8348220586776733
Epoch 90, training loss: 8.372591972351074 = 1.8150407075881958 + 1.0 * 6.55755090713501
Epoch 90, val loss: 1.822380542755127
Epoch 100, training loss: 8.306547164916992 = 1.800749659538269 + 1.0 * 6.505797386169434
Epoch 100, val loss: 1.8106913566589355
Epoch 110, training loss: 8.248063087463379 = 1.787559986114502 + 1.0 * 6.460503101348877
Epoch 110, val loss: 1.799973964691162
Epoch 120, training loss: 8.190787315368652 = 1.7752209901809692 + 1.0 * 6.4155659675598145
Epoch 120, val loss: 1.789867639541626
Epoch 130, training loss: 8.13880443572998 = 1.7628304958343506 + 1.0 * 6.375973701477051
Epoch 130, val loss: 1.7795374393463135
Epoch 140, training loss: 8.091521263122559 = 1.7491518259048462 + 1.0 * 6.342369079589844
Epoch 140, val loss: 1.7681328058242798
Epoch 150, training loss: 8.04862117767334 = 1.7332977056503296 + 1.0 * 6.315323829650879
Epoch 150, val loss: 1.755234956741333
Epoch 160, training loss: 8.008096694946289 = 1.7146124839782715 + 1.0 * 6.293484687805176
Epoch 160, val loss: 1.7403993606567383
Epoch 170, training loss: 7.9671831130981445 = 1.6921377182006836 + 1.0 * 6.275045394897461
Epoch 170, val loss: 1.72281813621521
Epoch 180, training loss: 7.924792289733887 = 1.6648991107940674 + 1.0 * 6.25989294052124
Epoch 180, val loss: 1.7015665769577026
Epoch 190, training loss: 7.879129409790039 = 1.632319688796997 + 1.0 * 6.246809482574463
Epoch 190, val loss: 1.6761138439178467
Epoch 200, training loss: 7.829224109649658 = 1.5935114622116089 + 1.0 * 6.23571252822876
Epoch 200, val loss: 1.6456263065338135
Epoch 210, training loss: 7.777338027954102 = 1.5481594800949097 + 1.0 * 6.229178428649902
Epoch 210, val loss: 1.6098743677139282
Epoch 220, training loss: 7.7164154052734375 = 1.4978697299957275 + 1.0 * 6.218545913696289
Epoch 220, val loss: 1.5701278448104858
Epoch 230, training loss: 7.654131889343262 = 1.4434700012207031 + 1.0 * 6.210661888122559
Epoch 230, val loss: 1.527158498764038
Epoch 240, training loss: 7.590558052062988 = 1.3864539861679077 + 1.0 * 6.204103946685791
Epoch 240, val loss: 1.482399821281433
Epoch 250, training loss: 7.530538558959961 = 1.3287855386734009 + 1.0 * 6.20175313949585
Epoch 250, val loss: 1.437574863433838
Epoch 260, training loss: 7.463604927062988 = 1.2717903852462769 + 1.0 * 6.191814422607422
Epoch 260, val loss: 1.3937811851501465
Epoch 270, training loss: 7.401750087738037 = 1.215156078338623 + 1.0 * 6.186594009399414
Epoch 270, val loss: 1.350299596786499
Epoch 280, training loss: 7.343239784240723 = 1.1593658924102783 + 1.0 * 6.183874130249023
Epoch 280, val loss: 1.3077119588851929
Epoch 290, training loss: 7.281748294830322 = 1.1058502197265625 + 1.0 * 6.17589807510376
Epoch 290, val loss: 1.2670010328292847
Epoch 300, training loss: 7.2275896072387695 = 1.05405855178833 + 1.0 * 6.1735310554504395
Epoch 300, val loss: 1.2276675701141357
Epoch 310, training loss: 7.172455310821533 = 1.0047537088394165 + 1.0 * 6.167701721191406
Epoch 310, val loss: 1.1904828548431396
Epoch 320, training loss: 7.119966506958008 = 0.9578627347946167 + 1.0 * 6.162103652954102
Epoch 320, val loss: 1.1553819179534912
Epoch 330, training loss: 7.0777363777160645 = 0.9125436544418335 + 1.0 * 6.165192604064941
Epoch 330, val loss: 1.1216942071914673
Epoch 340, training loss: 7.024499893188477 = 0.8693362474441528 + 1.0 * 6.155163764953613
Epoch 340, val loss: 1.0898722410202026
Epoch 350, training loss: 6.978164196014404 = 0.8275819420814514 + 1.0 * 6.150582313537598
Epoch 350, val loss: 1.0595667362213135
Epoch 360, training loss: 6.935275554656982 = 0.7870699167251587 + 1.0 * 6.148205757141113
Epoch 360, val loss: 1.030503511428833
Epoch 370, training loss: 6.8911848068237305 = 0.7481013536453247 + 1.0 * 6.143083572387695
Epoch 370, val loss: 1.0031672716140747
Epoch 380, training loss: 6.854619026184082 = 0.7106935977935791 + 1.0 * 6.143925189971924
Epoch 380, val loss: 0.9776917695999146
Epoch 390, training loss: 6.812627792358398 = 0.6753798127174377 + 1.0 * 6.1372480392456055
Epoch 390, val loss: 0.9545395374298096
Epoch 400, training loss: 6.775511741638184 = 0.6416480541229248 + 1.0 * 6.13386344909668
Epoch 400, val loss: 0.9334328770637512
Epoch 410, training loss: 6.740711688995361 = 0.6094858050346375 + 1.0 * 6.131226062774658
Epoch 410, val loss: 0.9142783284187317
Epoch 420, training loss: 6.7075018882751465 = 0.5790409445762634 + 1.0 * 6.128460884094238
Epoch 420, val loss: 0.8974905014038086
Epoch 430, training loss: 6.683046817779541 = 0.549871563911438 + 1.0 * 6.133175373077393
Epoch 430, val loss: 0.8823346495628357
Epoch 440, training loss: 6.644853591918945 = 0.522243857383728 + 1.0 * 6.122609615325928
Epoch 440, val loss: 0.8690422177314758
Epoch 450, training loss: 6.616283416748047 = 0.49591735005378723 + 1.0 * 6.120366096496582
Epoch 450, val loss: 0.8577252626419067
Epoch 460, training loss: 6.588932037353516 = 0.47068729996681213 + 1.0 * 6.118244647979736
Epoch 460, val loss: 0.8479177355766296
Epoch 470, training loss: 6.5810546875 = 0.4465261399745941 + 1.0 * 6.134528636932373
Epoch 470, val loss: 0.8396146297454834
Epoch 480, training loss: 6.545672416687012 = 0.42390820384025574 + 1.0 * 6.121764183044434
Epoch 480, val loss: 0.8328264951705933
Epoch 490, training loss: 6.5153021812438965 = 0.40249061584472656 + 1.0 * 6.11281156539917
Epoch 490, val loss: 0.8277549743652344
Epoch 500, training loss: 6.494339942932129 = 0.3820232152938843 + 1.0 * 6.112316608428955
Epoch 500, val loss: 0.8236633539199829
Epoch 510, training loss: 6.473089694976807 = 0.36253273487091064 + 1.0 * 6.1105570793151855
Epoch 510, val loss: 0.8206309080123901
Epoch 520, training loss: 6.453728675842285 = 0.34394845366477966 + 1.0 * 6.109780311584473
Epoch 520, val loss: 0.8188386559486389
Epoch 530, training loss: 6.434029579162598 = 0.3262065052986145 + 1.0 * 6.107822895050049
Epoch 530, val loss: 0.8178675770759583
Epoch 540, training loss: 6.412341594696045 = 0.30924782156944275 + 1.0 * 6.10309362411499
Epoch 540, val loss: 0.8179934024810791
Epoch 550, training loss: 6.402411460876465 = 0.292970210313797 + 1.0 * 6.10944128036499
Epoch 550, val loss: 0.818786084651947
Epoch 560, training loss: 6.377644062042236 = 0.27747735381126404 + 1.0 * 6.1001667976379395
Epoch 560, val loss: 0.8204617500305176
Epoch 570, training loss: 6.368793487548828 = 0.2626691460609436 + 1.0 * 6.106124401092529
Epoch 570, val loss: 0.8228915333747864
Epoch 580, training loss: 6.347196578979492 = 0.2486795037984848 + 1.0 * 6.098516941070557
Epoch 580, val loss: 0.8259779214859009
Epoch 590, training loss: 6.330624103546143 = 0.23543229699134827 + 1.0 * 6.095191955566406
Epoch 590, val loss: 0.8300092220306396
Epoch 600, training loss: 6.316102504730225 = 0.22286349534988403 + 1.0 * 6.093238830566406
Epoch 600, val loss: 0.8345903754234314
Epoch 610, training loss: 6.317282676696777 = 0.210959330201149 + 1.0 * 6.1063232421875
Epoch 610, val loss: 0.839824914932251
Epoch 620, training loss: 6.292454242706299 = 0.1999184638261795 + 1.0 * 6.092535972595215
Epoch 620, val loss: 0.845525860786438
Epoch 630, training loss: 6.280123233795166 = 0.18956951797008514 + 1.0 * 6.0905537605285645
Epoch 630, val loss: 0.8520681858062744
Epoch 640, training loss: 6.267752170562744 = 0.17983481287956238 + 1.0 * 6.087917327880859
Epoch 640, val loss: 0.8589808940887451
Epoch 650, training loss: 6.267119407653809 = 0.17068640887737274 + 1.0 * 6.096433162689209
Epoch 650, val loss: 0.866503894329071
Epoch 660, training loss: 6.2501702308654785 = 0.16217313706874847 + 1.0 * 6.087996959686279
Epoch 660, val loss: 0.8741664886474609
Epoch 670, training loss: 6.241851806640625 = 0.15423829853534698 + 1.0 * 6.087613582611084
Epoch 670, val loss: 0.8822695016860962
Epoch 680, training loss: 6.229910373687744 = 0.14680945873260498 + 1.0 * 6.08310079574585
Epoch 680, val loss: 0.8905525207519531
Epoch 690, training loss: 6.221653461456299 = 0.13981375098228455 + 1.0 * 6.081839561462402
Epoch 690, val loss: 0.8991521596908569
Epoch 700, training loss: 6.2157440185546875 = 0.1332254707813263 + 1.0 * 6.082518577575684
Epoch 700, val loss: 0.9078889489173889
Epoch 710, training loss: 6.211148738861084 = 0.1270325481891632 + 1.0 * 6.084115982055664
Epoch 710, val loss: 0.9168027639389038
Epoch 720, training loss: 6.201557159423828 = 0.12122150510549545 + 1.0 * 6.08033561706543
Epoch 720, val loss: 0.9256914258003235
Epoch 730, training loss: 6.192563056945801 = 0.11576682329177856 + 1.0 * 6.076796054840088
Epoch 730, val loss: 0.9348378777503967
Epoch 740, training loss: 6.185585975646973 = 0.11060762405395508 + 1.0 * 6.074978351593018
Epoch 740, val loss: 0.9439432621002197
Epoch 750, training loss: 6.181562423706055 = 0.10572420805692673 + 1.0 * 6.075838088989258
Epoch 750, val loss: 0.9531331658363342
Epoch 760, training loss: 6.178047180175781 = 0.10112498700618744 + 1.0 * 6.076922416687012
Epoch 760, val loss: 0.9620167016983032
Epoch 770, training loss: 6.1713547706604 = 0.09681998938322067 + 1.0 * 6.074534893035889
Epoch 770, val loss: 0.9712312817573547
Epoch 780, training loss: 6.163994312286377 = 0.09275294840335846 + 1.0 * 6.07124137878418
Epoch 780, val loss: 0.9802948832511902
Epoch 790, training loss: 6.160990238189697 = 0.08889585733413696 + 1.0 * 6.072094440460205
Epoch 790, val loss: 0.9892681241035461
Epoch 800, training loss: 6.156940460205078 = 0.08525587618350983 + 1.0 * 6.07168436050415
Epoch 800, val loss: 0.99806147813797
Epoch 810, training loss: 6.15134859085083 = 0.08182693272829056 + 1.0 * 6.069521427154541
Epoch 810, val loss: 1.0069994926452637
Epoch 820, training loss: 6.146292686462402 = 0.07858037203550339 + 1.0 * 6.067712306976318
Epoch 820, val loss: 1.015838384628296
Epoch 830, training loss: 6.142286777496338 = 0.07549392431974411 + 1.0 * 6.066792964935303
Epoch 830, val loss: 1.024592638015747
Epoch 840, training loss: 6.137829303741455 = 0.07256699353456497 + 1.0 * 6.065262317657471
Epoch 840, val loss: 1.0330829620361328
Epoch 850, training loss: 6.136565685272217 = 0.06980211287736893 + 1.0 * 6.066763401031494
Epoch 850, val loss: 1.0417596101760864
Epoch 860, training loss: 6.132205009460449 = 0.06717340648174286 + 1.0 * 6.0650315284729
Epoch 860, val loss: 1.0502532720565796
Epoch 870, training loss: 6.128332614898682 = 0.0646752193570137 + 1.0 * 6.063657283782959
Epoch 870, val loss: 1.0586318969726562
Epoch 880, training loss: 6.127143383026123 = 0.062299735844135284 + 1.0 * 6.064843654632568
Epoch 880, val loss: 1.0671870708465576
Epoch 890, training loss: 6.1255903244018555 = 0.06004659831523895 + 1.0 * 6.0655436515808105
Epoch 890, val loss: 1.0752933025360107
Epoch 900, training loss: 6.120823383331299 = 0.0579073503613472 + 1.0 * 6.062915802001953
Epoch 900, val loss: 1.083516001701355
Epoch 910, training loss: 6.115962982177734 = 0.05587504059076309 + 1.0 * 6.060088157653809
Epoch 910, val loss: 1.0916450023651123
Epoch 920, training loss: 6.116138935089111 = 0.053933192044496536 + 1.0 * 6.062205791473389
Epoch 920, val loss: 1.0995863676071167
Epoch 930, training loss: 6.112318515777588 = 0.05208844318985939 + 1.0 * 6.060230255126953
Epoch 930, val loss: 1.107338547706604
Epoch 940, training loss: 6.108039379119873 = 0.05033199116587639 + 1.0 * 6.0577073097229
Epoch 940, val loss: 1.1151710748672485
Epoch 950, training loss: 6.106045722961426 = 0.048655711114406586 + 1.0 * 6.057390213012695
Epoch 950, val loss: 1.1228394508361816
Epoch 960, training loss: 6.10656213760376 = 0.047051962465047836 + 1.0 * 6.059510231018066
Epoch 960, val loss: 1.1303026676177979
Epoch 970, training loss: 6.102161407470703 = 0.04552723467350006 + 1.0 * 6.056633949279785
Epoch 970, val loss: 1.1378813982009888
Epoch 980, training loss: 6.101315498352051 = 0.044066254049539566 + 1.0 * 6.057249069213867
Epoch 980, val loss: 1.145257592201233
Epoch 990, training loss: 6.096035957336426 = 0.04267124831676483 + 1.0 * 6.0533647537231445
Epoch 990, val loss: 1.15253484249115
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.4133
Flip ASR: 0.3111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.319478034973145 = 1.945692777633667 + 1.0 * 8.373785018920898
Epoch 0, val loss: 1.942896842956543
Epoch 10, training loss: 10.308225631713867 = 1.9351410865783691 + 1.0 * 8.373085021972656
Epoch 10, val loss: 1.932662844657898
Epoch 20, training loss: 10.290800094604492 = 1.9217143058776855 + 1.0 * 8.369086265563965
Epoch 20, val loss: 1.9191498756408691
Epoch 30, training loss: 10.243403434753418 = 1.9033292531967163 + 1.0 * 8.34007453918457
Epoch 30, val loss: 1.9005321264266968
Epoch 40, training loss: 9.962176322937012 = 1.8811081647872925 + 1.0 * 8.08106803894043
Epoch 40, val loss: 1.8786184787750244
Epoch 50, training loss: 9.170608520507812 = 1.8596137762069702 + 1.0 * 7.310995101928711
Epoch 50, val loss: 1.857375979423523
Epoch 60, training loss: 8.776073455810547 = 1.8460434675216675 + 1.0 * 6.93002986907959
Epoch 60, val loss: 1.8429290056228638
Epoch 70, training loss: 8.563946723937988 = 1.8337429761886597 + 1.0 * 6.730203628540039
Epoch 70, val loss: 1.8291404247283936
Epoch 80, training loss: 8.423299789428711 = 1.8218833208084106 + 1.0 * 6.601416110992432
Epoch 80, val loss: 1.8162510395050049
Epoch 90, training loss: 8.318902015686035 = 1.8099404573440552 + 1.0 * 6.5089616775512695
Epoch 90, val loss: 1.8034234046936035
Epoch 100, training loss: 8.244117736816406 = 1.7985544204711914 + 1.0 * 6.445562839508057
Epoch 100, val loss: 1.7910979986190796
Epoch 110, training loss: 8.186607360839844 = 1.7872610092163086 + 1.0 * 6.399346828460693
Epoch 110, val loss: 1.7788901329040527
Epoch 120, training loss: 8.139575004577637 = 1.7757798433303833 + 1.0 * 6.363794803619385
Epoch 120, val loss: 1.7667324542999268
Epoch 130, training loss: 8.096839904785156 = 1.7637650966644287 + 1.0 * 6.333074569702148
Epoch 130, val loss: 1.7544574737548828
Epoch 140, training loss: 8.05914306640625 = 1.75066077709198 + 1.0 * 6.308482646942139
Epoch 140, val loss: 1.7416975498199463
Epoch 150, training loss: 8.022239685058594 = 1.7358535528182983 + 1.0 * 6.286386489868164
Epoch 150, val loss: 1.727986454963684
Epoch 160, training loss: 7.988306045532227 = 1.718664288520813 + 1.0 * 6.269641876220703
Epoch 160, val loss: 1.7126948833465576
Epoch 170, training loss: 7.951817035675049 = 1.6986664533615112 + 1.0 * 6.253150463104248
Epoch 170, val loss: 1.6955751180648804
Epoch 180, training loss: 7.91392707824707 = 1.6750491857528687 + 1.0 * 6.238877773284912
Epoch 180, val loss: 1.6757867336273193
Epoch 190, training loss: 7.873456954956055 = 1.646499514579773 + 1.0 * 6.226957321166992
Epoch 190, val loss: 1.6521698236465454
Epoch 200, training loss: 7.827877521514893 = 1.6114131212234497 + 1.0 * 6.216464519500732
Epoch 200, val loss: 1.6233593225479126
Epoch 210, training loss: 7.778847694396973 = 1.5690314769744873 + 1.0 * 6.209815979003906
Epoch 210, val loss: 1.5888043642044067
Epoch 220, training loss: 7.722970485687256 = 1.5205368995666504 + 1.0 * 6.2024335861206055
Epoch 220, val loss: 1.5494284629821777
Epoch 230, training loss: 7.661007404327393 = 1.4660459756851196 + 1.0 * 6.1949615478515625
Epoch 230, val loss: 1.50520658493042
Epoch 240, training loss: 7.595190525054932 = 1.4060341119766235 + 1.0 * 6.189156532287598
Epoch 240, val loss: 1.456892490386963
Epoch 250, training loss: 7.526782035827637 = 1.3428118228912354 + 1.0 * 6.1839704513549805
Epoch 250, val loss: 1.4067436456680298
Epoch 260, training loss: 7.466632843017578 = 1.2799601554870605 + 1.0 * 6.186672687530518
Epoch 260, val loss: 1.3580013513565063
Epoch 270, training loss: 7.396939754486084 = 1.220328450202942 + 1.0 * 6.176611423492432
Epoch 270, val loss: 1.3122482299804688
Epoch 280, training loss: 7.3326592445373535 = 1.1628026962280273 + 1.0 * 6.169856548309326
Epoch 280, val loss: 1.2686201333999634
Epoch 290, training loss: 7.271620273590088 = 1.1070889234542847 + 1.0 * 6.164531230926514
Epoch 290, val loss: 1.226715087890625
Epoch 300, training loss: 7.213156700134277 = 1.053216576576233 + 1.0 * 6.159940242767334
Epoch 300, val loss: 1.1865429878234863
Epoch 310, training loss: 7.157543182373047 = 1.00177001953125 + 1.0 * 6.155773162841797
Epoch 310, val loss: 1.148482084274292
Epoch 320, training loss: 7.103627681732178 = 0.9522388577461243 + 1.0 * 6.151388645172119
Epoch 320, val loss: 1.1118887662887573
Epoch 330, training loss: 7.054753303527832 = 0.9045863151550293 + 1.0 * 6.150166988372803
Epoch 330, val loss: 1.0768738985061646
Epoch 340, training loss: 7.00425386428833 = 0.8594554662704468 + 1.0 * 6.144798278808594
Epoch 340, val loss: 1.0439059734344482
Epoch 350, training loss: 6.956656455993652 = 0.8165128827095032 + 1.0 * 6.140143394470215
Epoch 350, val loss: 1.0127421617507935
Epoch 360, training loss: 6.923250675201416 = 0.775836169719696 + 1.0 * 6.147414684295654
Epoch 360, val loss: 0.9834364056587219
Epoch 370, training loss: 6.871847152709961 = 0.7380943298339844 + 1.0 * 6.133752822875977
Epoch 370, val loss: 0.9567640423774719
Epoch 380, training loss: 6.8332743644714355 = 0.7027072310447693 + 1.0 * 6.1305670738220215
Epoch 380, val loss: 0.9322079420089722
Epoch 390, training loss: 6.797207832336426 = 0.6692509651184082 + 1.0 * 6.127956867218018
Epoch 390, val loss: 0.9094098210334778
Epoch 400, training loss: 6.76750373840332 = 0.6378729939460754 + 1.0 * 6.1296305656433105
Epoch 400, val loss: 0.8884339332580566
Epoch 410, training loss: 6.73183012008667 = 0.6084727644920349 + 1.0 * 6.12335729598999
Epoch 410, val loss: 0.8694062829017639
Epoch 420, training loss: 6.700407028198242 = 0.5807677507400513 + 1.0 * 6.1196393966674805
Epoch 420, val loss: 0.8519769310951233
Epoch 430, training loss: 6.671813488006592 = 0.5546199679374695 + 1.0 * 6.117193698883057
Epoch 430, val loss: 0.8360782861709595
Epoch 440, training loss: 6.64304256439209 = 0.529634952545166 + 1.0 * 6.113407611846924
Epoch 440, val loss: 0.821496307849884
Epoch 450, training loss: 6.6215901374816895 = 0.5057881474494934 + 1.0 * 6.115801811218262
Epoch 450, val loss: 0.8080226182937622
Epoch 460, training loss: 6.59358549118042 = 0.48309364914894104 + 1.0 * 6.110491752624512
Epoch 460, val loss: 0.7956963181495667
Epoch 470, training loss: 6.567150115966797 = 0.461125910282135 + 1.0 * 6.106024265289307
Epoch 470, val loss: 0.784315288066864
Epoch 480, training loss: 6.543593406677246 = 0.43968772888183594 + 1.0 * 6.10390567779541
Epoch 480, val loss: 0.7735888957977295
Epoch 490, training loss: 6.528902530670166 = 0.4187871515750885 + 1.0 * 6.1101155281066895
Epoch 490, val loss: 0.7635058760643005
Epoch 500, training loss: 6.499472618103027 = 0.3987314701080322 + 1.0 * 6.100741386413574
Epoch 500, val loss: 0.7542228698730469
Epoch 510, training loss: 6.478343963623047 = 0.379271924495697 + 1.0 * 6.099071979522705
Epoch 510, val loss: 0.7457334399223328
Epoch 520, training loss: 6.462169170379639 = 0.36029118299484253 + 1.0 * 6.1018781661987305
Epoch 520, val loss: 0.7377849221229553
Epoch 530, training loss: 6.442259788513184 = 0.3420349359512329 + 1.0 * 6.10022497177124
Epoch 530, val loss: 0.7305237650871277
Epoch 540, training loss: 6.417690277099609 = 0.32446035742759705 + 1.0 * 6.0932297706604
Epoch 540, val loss: 0.7241076231002808
Epoch 550, training loss: 6.403740882873535 = 0.30755966901779175 + 1.0 * 6.096181392669678
Epoch 550, val loss: 0.7184197306632996
Epoch 560, training loss: 6.3875603675842285 = 0.2914780378341675 + 1.0 * 6.0960822105407715
Epoch 560, val loss: 0.7134003043174744
Epoch 570, training loss: 6.366756439208984 = 0.2762000262737274 + 1.0 * 6.090556621551514
Epoch 570, val loss: 0.7093366384506226
Epoch 580, training loss: 6.34848690032959 = 0.26162922382354736 + 1.0 * 6.086857795715332
Epoch 580, val loss: 0.7059968113899231
Epoch 590, training loss: 6.333253383636475 = 0.247732475399971 + 1.0 * 6.0855207443237305
Epoch 590, val loss: 0.7033731937408447
Epoch 600, training loss: 6.326627254486084 = 0.2345355749130249 + 1.0 * 6.0920915603637695
Epoch 600, val loss: 0.7014739513397217
Epoch 610, training loss: 6.309138298034668 = 0.2221515029668808 + 1.0 * 6.086987018585205
Epoch 610, val loss: 0.7002405524253845
Epoch 620, training loss: 6.295413494110107 = 0.2105524092912674 + 1.0 * 6.0848612785339355
Epoch 620, val loss: 0.6996561288833618
Epoch 630, training loss: 6.280710697174072 = 0.1996714472770691 + 1.0 * 6.0810394287109375
Epoch 630, val loss: 0.6997212171554565
Epoch 640, training loss: 6.268383502960205 = 0.18935547769069672 + 1.0 * 6.079028129577637
Epoch 640, val loss: 0.7003977298736572
Epoch 650, training loss: 6.256626129150391 = 0.17953583598136902 + 1.0 * 6.077090263366699
Epoch 650, val loss: 0.7016065120697021
Epoch 660, training loss: 6.2507805824279785 = 0.17018814384937286 + 1.0 * 6.080592632293701
Epoch 660, val loss: 0.7033237814903259
Epoch 670, training loss: 6.242117881774902 = 0.16135035455226898 + 1.0 * 6.080767631530762
Epoch 670, val loss: 0.7055243253707886
Epoch 680, training loss: 6.226755142211914 = 0.15302927792072296 + 1.0 * 6.073725700378418
Epoch 680, val loss: 0.7081583738327026
Epoch 690, training loss: 6.21820592880249 = 0.14508694410324097 + 1.0 * 6.073119163513184
Epoch 690, val loss: 0.7112578749656677
Epoch 700, training loss: 6.223541259765625 = 0.13751454651355743 + 1.0 * 6.086026668548584
Epoch 700, val loss: 0.7146555781364441
Epoch 710, training loss: 6.206197738647461 = 0.13038796186447144 + 1.0 * 6.075809955596924
Epoch 710, val loss: 0.7183706164360046
Epoch 720, training loss: 6.193393707275391 = 0.12362471222877502 + 1.0 * 6.069768905639648
Epoch 720, val loss: 0.722394585609436
Epoch 730, training loss: 6.185088157653809 = 0.11717943847179413 + 1.0 * 6.067908763885498
Epoch 730, val loss: 0.7266205549240112
Epoch 740, training loss: 6.180293560028076 = 0.11105651408433914 + 1.0 * 6.069237232208252
Epoch 740, val loss: 0.7310096025466919
Epoch 750, training loss: 6.172163963317871 = 0.10534346848726273 + 1.0 * 6.0668206214904785
Epoch 750, val loss: 0.7356542348861694
Epoch 760, training loss: 6.168286323547363 = 0.10012441873550415 + 1.0 * 6.068161964416504
Epoch 760, val loss: 0.7402657270431519
Epoch 770, training loss: 6.160578727722168 = 0.09533996880054474 + 1.0 * 6.065238952636719
Epoch 770, val loss: 0.7449919581413269
Epoch 780, training loss: 6.16229772567749 = 0.09089367091655731 + 1.0 * 6.071403980255127
Epoch 780, val loss: 0.7497987747192383
Epoch 790, training loss: 6.1531829833984375 = 0.08676722645759583 + 1.0 * 6.066415786743164
Epoch 790, val loss: 0.7547591328620911
Epoch 800, training loss: 6.148861885070801 = 0.08292698860168457 + 1.0 * 6.065934658050537
Epoch 800, val loss: 0.7598389387130737
Epoch 810, training loss: 6.141887664794922 = 0.07932094484567642 + 1.0 * 6.062566757202148
Epoch 810, val loss: 0.7649705410003662
Epoch 820, training loss: 6.136246681213379 = 0.07592897117137909 + 1.0 * 6.060317516326904
Epoch 820, val loss: 0.7702139616012573
Epoch 830, training loss: 6.134246349334717 = 0.07271996885538101 + 1.0 * 6.061526298522949
Epoch 830, val loss: 0.7755398154258728
Epoch 840, training loss: 6.129536151885986 = 0.06970320641994476 + 1.0 * 6.05983304977417
Epoch 840, val loss: 0.7808480858802795
Epoch 850, training loss: 6.125009536743164 = 0.06686306744813919 + 1.0 * 6.0581464767456055
Epoch 850, val loss: 0.7862510681152344
Epoch 860, training loss: 6.127474308013916 = 0.06417661905288696 + 1.0 * 6.063297748565674
Epoch 860, val loss: 0.7916964888572693
Epoch 870, training loss: 6.119182586669922 = 0.061647679656744 + 1.0 * 6.057534694671631
Epoch 870, val loss: 0.7972246408462524
Epoch 880, training loss: 6.118991374969482 = 0.05925609543919563 + 1.0 * 6.059735298156738
Epoch 880, val loss: 0.802720308303833
Epoch 890, training loss: 6.111557960510254 = 0.05699275806546211 + 1.0 * 6.0545654296875
Epoch 890, val loss: 0.8082216382026672
Epoch 900, training loss: 6.110229969024658 = 0.05484369397163391 + 1.0 * 6.055386066436768
Epoch 900, val loss: 0.813728392124176
Epoch 910, training loss: 6.108936786651611 = 0.05280652269721031 + 1.0 * 6.056130409240723
Epoch 910, val loss: 0.8193025588989258
Epoch 920, training loss: 6.102304935455322 = 0.05087824910879135 + 1.0 * 6.051426887512207
Epoch 920, val loss: 0.8248245716094971
Epoch 930, training loss: 6.1003618240356445 = 0.04904254525899887 + 1.0 * 6.051319122314453
Epoch 930, val loss: 0.830402135848999
Epoch 940, training loss: 6.103174686431885 = 0.0472889319062233 + 1.0 * 6.0558857917785645
Epoch 940, val loss: 0.8358885049819946
Epoch 950, training loss: 6.097996234893799 = 0.045625291764736176 + 1.0 * 6.052371025085449
Epoch 950, val loss: 0.8414448499679565
Epoch 960, training loss: 6.095848560333252 = 0.04404660686850548 + 1.0 * 6.051802158355713
Epoch 960, val loss: 0.8470257520675659
Epoch 970, training loss: 6.091176509857178 = 0.04254573583602905 + 1.0 * 6.048630714416504
Epoch 970, val loss: 0.8525577187538147
Epoch 980, training loss: 6.088894367218018 = 0.04111262410879135 + 1.0 * 6.047781944274902
Epoch 980, val loss: 0.8580589890480042
Epoch 990, training loss: 6.090095043182373 = 0.039743006229400635 + 1.0 * 6.050352096557617
Epoch 990, val loss: 0.863635241985321
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6310
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.312482833862305 = 1.9391655921936035 + 1.0 * 8.37331771850586
Epoch 0, val loss: 1.9278281927108765
Epoch 10, training loss: 10.297345161437988 = 1.9267184734344482 + 1.0 * 8.370626449584961
Epoch 10, val loss: 1.9130067825317383
Epoch 20, training loss: 10.279394149780273 = 1.9114854335784912 + 1.0 * 8.367908477783203
Epoch 20, val loss: 1.8933159112930298
Epoch 30, training loss: 10.247411727905273 = 1.8926746845245361 + 1.0 * 8.354737281799316
Epoch 30, val loss: 1.8699324131011963
Epoch 40, training loss: 10.14197826385498 = 1.8717581033706665 + 1.0 * 8.270219802856445
Epoch 40, val loss: 1.8470672369003296
Epoch 50, training loss: 9.794600486755371 = 1.852103590965271 + 1.0 * 7.9424967765808105
Epoch 50, val loss: 1.8274259567260742
Epoch 60, training loss: 9.303793907165527 = 1.8350965976715088 + 1.0 * 7.468697547912598
Epoch 60, val loss: 1.8121682405471802
Epoch 70, training loss: 8.832489013671875 = 1.822559118270874 + 1.0 * 7.00993013381958
Epoch 70, val loss: 1.8015365600585938
Epoch 80, training loss: 8.58907413482666 = 1.8095629215240479 + 1.0 * 6.779510974884033
Epoch 80, val loss: 1.789965271949768
Epoch 90, training loss: 8.462696075439453 = 1.7959634065628052 + 1.0 * 6.666732311248779
Epoch 90, val loss: 1.7774074077606201
Epoch 100, training loss: 8.365381240844727 = 1.782972812652588 + 1.0 * 6.5824079513549805
Epoch 100, val loss: 1.7658123970031738
Epoch 110, training loss: 8.28207015991211 = 1.7720236778259277 + 1.0 * 6.51004695892334
Epoch 110, val loss: 1.7567590475082397
Epoch 120, training loss: 8.212080001831055 = 1.7618587017059326 + 1.0 * 6.450221061706543
Epoch 120, val loss: 1.7487130165100098
Epoch 130, training loss: 8.152997970581055 = 1.7511624097824097 + 1.0 * 6.4018354415893555
Epoch 130, val loss: 1.7401939630508423
Epoch 140, training loss: 8.105892181396484 = 1.7391705513000488 + 1.0 * 6.366721153259277
Epoch 140, val loss: 1.7306323051452637
Epoch 150, training loss: 8.06350040435791 = 1.7252323627471924 + 1.0 * 6.338268280029297
Epoch 150, val loss: 1.7196345329284668
Epoch 160, training loss: 8.025636672973633 = 1.7086647748947144 + 1.0 * 6.316972255706787
Epoch 160, val loss: 1.706616759300232
Epoch 170, training loss: 7.988185405731201 = 1.6888022422790527 + 1.0 * 6.299383163452148
Epoch 170, val loss: 1.6911171674728394
Epoch 180, training loss: 7.950234889984131 = 1.6644991636276245 + 1.0 * 6.285735607147217
Epoch 180, val loss: 1.6720434427261353
Epoch 190, training loss: 7.905752182006836 = 1.6349815130233765 + 1.0 * 6.27077054977417
Epoch 190, val loss: 1.648972988128662
Epoch 200, training loss: 7.858007431030273 = 1.5991765260696411 + 1.0 * 6.258831024169922
Epoch 200, val loss: 1.620851993560791
Epoch 210, training loss: 7.804096221923828 = 1.5561729669570923 + 1.0 * 6.247923374176025
Epoch 210, val loss: 1.5871331691741943
Epoch 220, training loss: 7.745622634887695 = 1.5059434175491333 + 1.0 * 6.239679336547852
Epoch 220, val loss: 1.5477486848831177
Epoch 230, training loss: 7.680577754974365 = 1.4504618644714355 + 1.0 * 6.23011589050293
Epoch 230, val loss: 1.5048350095748901
Epoch 240, training loss: 7.610836982727051 = 1.3909640312194824 + 1.0 * 6.219872951507568
Epoch 240, val loss: 1.4593679904937744
Epoch 250, training loss: 7.542715072631836 = 1.328678846359253 + 1.0 * 6.214035987854004
Epoch 250, val loss: 1.412469744682312
Epoch 260, training loss: 7.474578857421875 = 1.2664114236831665 + 1.0 * 6.208167552947998
Epoch 260, val loss: 1.3666151762008667
Epoch 270, training loss: 7.406589031219482 = 1.2059084177017212 + 1.0 * 6.200680732727051
Epoch 270, val loss: 1.3229868412017822
Epoch 280, training loss: 7.3426923751831055 = 1.1471765041351318 + 1.0 * 6.1955156326293945
Epoch 280, val loss: 1.2810596227645874
Epoch 290, training loss: 7.2794084548950195 = 1.0914617776870728 + 1.0 * 6.187946796417236
Epoch 290, val loss: 1.2415258884429932
Epoch 300, training loss: 7.221232891082764 = 1.0388816595077515 + 1.0 * 6.182351112365723
Epoch 300, val loss: 1.2045543193817139
Epoch 310, training loss: 7.165554046630859 = 0.9887206554412842 + 1.0 * 6.176833152770996
Epoch 310, val loss: 1.1694482564926147
Epoch 320, training loss: 7.116707801818848 = 0.9408864378929138 + 1.0 * 6.175821304321289
Epoch 320, val loss: 1.1360560655593872
Epoch 330, training loss: 7.065410614013672 = 0.8966158628463745 + 1.0 * 6.168794631958008
Epoch 330, val loss: 1.1051864624023438
Epoch 340, training loss: 7.016702175140381 = 0.854428768157959 + 1.0 * 6.162273406982422
Epoch 340, val loss: 1.0760029554367065
Epoch 350, training loss: 6.971314430236816 = 0.8140317797660828 + 1.0 * 6.157282829284668
Epoch 350, val loss: 1.0480594635009766
Epoch 360, training loss: 6.939290523529053 = 0.7754291892051697 + 1.0 * 6.163861274719238
Epoch 360, val loss: 1.0215845108032227
Epoch 370, training loss: 6.8931708335876465 = 0.7395508885383606 + 1.0 * 6.153619766235352
Epoch 370, val loss: 0.9971689581871033
Epoch 380, training loss: 6.849981307983398 = 0.7061182260513306 + 1.0 * 6.143863201141357
Epoch 380, val loss: 0.9749140739440918
Epoch 390, training loss: 6.815186500549316 = 0.6747162342071533 + 1.0 * 6.140470504760742
Epoch 390, val loss: 0.9544313549995422
Epoch 400, training loss: 6.791197299957275 = 0.6453709006309509 + 1.0 * 6.14582633972168
Epoch 400, val loss: 0.9357505440711975
Epoch 410, training loss: 6.7530622482299805 = 0.6182372570037842 + 1.0 * 6.134825229644775
Epoch 410, val loss: 0.9190389513969421
Epoch 420, training loss: 6.723015785217285 = 0.592451274394989 + 1.0 * 6.1305646896362305
Epoch 420, val loss: 0.9040017127990723
Epoch 430, training loss: 6.694367408752441 = 0.567599892616272 + 1.0 * 6.126767635345459
Epoch 430, val loss: 0.890131413936615
Epoch 440, training loss: 6.6814470291137695 = 0.5435523986816406 + 1.0 * 6.137894630432129
Epoch 440, val loss: 0.8773841261863708
Epoch 450, training loss: 6.645436763763428 = 0.5205187797546387 + 1.0 * 6.124917984008789
Epoch 450, val loss: 0.8659088611602783
Epoch 460, training loss: 6.619290351867676 = 0.4982309937477112 + 1.0 * 6.121059417724609
Epoch 460, val loss: 0.8558313250541687
Epoch 470, training loss: 6.595548629760742 = 0.47653651237487793 + 1.0 * 6.119012355804443
Epoch 470, val loss: 0.8467203378677368
Epoch 480, training loss: 6.570337772369385 = 0.4555041193962097 + 1.0 * 6.114833831787109
Epoch 480, val loss: 0.8387795686721802
Epoch 490, training loss: 6.549807071685791 = 0.43506482243537903 + 1.0 * 6.114742279052734
Epoch 490, val loss: 0.8319588303565979
Epoch 500, training loss: 6.528244972229004 = 0.41530293226242065 + 1.0 * 6.112942218780518
Epoch 500, val loss: 0.8261777758598328
Epoch 510, training loss: 6.504889965057373 = 0.3962722718715668 + 1.0 * 6.108617782592773
Epoch 510, val loss: 0.8216255903244019
Epoch 520, training loss: 6.484332084655762 = 0.3778388202190399 + 1.0 * 6.1064934730529785
Epoch 520, val loss: 0.8181031942367554
Epoch 530, training loss: 6.476224422454834 = 0.35997799038887024 + 1.0 * 6.116246223449707
Epoch 530, val loss: 0.8156301975250244
Epoch 540, training loss: 6.447350025177002 = 0.34284839034080505 + 1.0 * 6.104501724243164
Epoch 540, val loss: 0.8141594529151917
Epoch 550, training loss: 6.427382469177246 = 0.3263804316520691 + 1.0 * 6.101002216339111
Epoch 550, val loss: 0.813787579536438
Epoch 560, training loss: 6.412542819976807 = 0.31045398116111755 + 1.0 * 6.102088928222656
Epoch 560, val loss: 0.813836395740509
Epoch 570, training loss: 6.401524066925049 = 0.29517877101898193 + 1.0 * 6.106345176696777
Epoch 570, val loss: 0.8139753341674805
Epoch 580, training loss: 6.377845764160156 = 0.28064534068107605 + 1.0 * 6.097200393676758
Epoch 580, val loss: 0.8147301077842712
Epoch 590, training loss: 6.36135721206665 = 0.2667138874530792 + 1.0 * 6.0946431159973145
Epoch 590, val loss: 0.8158918619155884
Epoch 600, training loss: 6.345552921295166 = 0.2533775568008423 + 1.0 * 6.092175483703613
Epoch 600, val loss: 0.817352831363678
Epoch 610, training loss: 6.345364570617676 = 0.24064677953720093 + 1.0 * 6.10471773147583
Epoch 610, val loss: 0.8190451860427856
Epoch 620, training loss: 6.320667743682861 = 0.22860722243785858 + 1.0 * 6.092060565948486
Epoch 620, val loss: 0.8209998607635498
Epoch 630, training loss: 6.307727336883545 = 0.21721723675727844 + 1.0 * 6.09050989151001
Epoch 630, val loss: 0.8233121633529663
Epoch 640, training loss: 6.294701099395752 = 0.2064516246318817 + 1.0 * 6.088249683380127
Epoch 640, val loss: 0.825779914855957
Epoch 650, training loss: 6.286736965179443 = 0.19627121090888977 + 1.0 * 6.090465545654297
Epoch 650, val loss: 0.8285044431686401
Epoch 660, training loss: 6.271321773529053 = 0.18664269149303436 + 1.0 * 6.084679126739502
Epoch 660, val loss: 0.8314859867095947
Epoch 670, training loss: 6.260885715484619 = 0.17754128575325012 + 1.0 * 6.083344459533691
Epoch 670, val loss: 0.8346366286277771
Epoch 680, training loss: 6.2639055252075195 = 0.16896356642246246 + 1.0 * 6.094942092895508
Epoch 680, val loss: 0.8378666043281555
Epoch 690, training loss: 6.245811462402344 = 0.16098329424858093 + 1.0 * 6.0848283767700195
Epoch 690, val loss: 0.8412534594535828
Epoch 700, training loss: 6.233492851257324 = 0.15347284078598022 + 1.0 * 6.080019950866699
Epoch 700, val loss: 0.8449751734733582
Epoch 710, training loss: 6.224278450012207 = 0.1463640034198761 + 1.0 * 6.077914237976074
Epoch 710, val loss: 0.8487740755081177
Epoch 720, training loss: 6.216021537780762 = 0.1396171748638153 + 1.0 * 6.076404571533203
Epoch 720, val loss: 0.8526837825775146
Epoch 730, training loss: 6.217198371887207 = 0.13320250809192657 + 1.0 * 6.083995819091797
Epoch 730, val loss: 0.8567646145820618
Epoch 740, training loss: 6.208585262298584 = 0.12717336416244507 + 1.0 * 6.081411838531494
Epoch 740, val loss: 0.8606482744216919
Epoch 750, training loss: 6.196757793426514 = 0.1214757114648819 + 1.0 * 6.075282096862793
Epoch 750, val loss: 0.8646894693374634
Epoch 760, training loss: 6.18817663192749 = 0.11607632040977478 + 1.0 * 6.0721001625061035
Epoch 760, val loss: 0.8687861561775208
Epoch 770, training loss: 6.185153961181641 = 0.1109345331788063 + 1.0 * 6.074219226837158
Epoch 770, val loss: 0.8729485273361206
Epoch 780, training loss: 6.1764678955078125 = 0.10605500638484955 + 1.0 * 6.070413112640381
Epoch 780, val loss: 0.8769726157188416
Epoch 790, training loss: 6.171206951141357 = 0.10142900049686432 + 1.0 * 6.069777965545654
Epoch 790, val loss: 0.8812463879585266
Epoch 800, training loss: 6.1652679443359375 = 0.09706085920333862 + 1.0 * 6.068207263946533
Epoch 800, val loss: 0.8856158256530762
Epoch 810, training loss: 6.1617560386657715 = 0.09288813173770905 + 1.0 * 6.0688676834106445
Epoch 810, val loss: 0.8900334239006042
Epoch 820, training loss: 6.1580915451049805 = 0.088937908411026 + 1.0 * 6.069153785705566
Epoch 820, val loss: 0.8944119215011597
Epoch 830, training loss: 6.153156280517578 = 0.08520828187465668 + 1.0 * 6.067947864532471
Epoch 830, val loss: 0.8990470767021179
Epoch 840, training loss: 6.146910667419434 = 0.08167152106761932 + 1.0 * 6.065238952636719
Epoch 840, val loss: 0.9036464691162109
Epoch 850, training loss: 6.141535758972168 = 0.07830583304166794 + 1.0 * 6.063230037689209
Epoch 850, val loss: 0.9083548784255981
Epoch 860, training loss: 6.141507148742676 = 0.07509949058294296 + 1.0 * 6.066407680511475
Epoch 860, val loss: 0.9130843281745911
Epoch 870, training loss: 6.136755466461182 = 0.07205190509557724 + 1.0 * 6.064703464508057
Epoch 870, val loss: 0.9177952408790588
Epoch 880, training loss: 6.131422996520996 = 0.06916011869907379 + 1.0 * 6.062263011932373
Epoch 880, val loss: 0.9225478768348694
Epoch 890, training loss: 6.141127109527588 = 0.06641483306884766 + 1.0 * 6.07471227645874
Epoch 890, val loss: 0.9273211359977722
Epoch 900, training loss: 6.124267101287842 = 0.06382210552692413 + 1.0 * 6.0604448318481445
Epoch 900, val loss: 0.9320948123931885
Epoch 910, training loss: 6.119783878326416 = 0.06135730817914009 + 1.0 * 6.058426380157471
Epoch 910, val loss: 0.9370244145393372
Epoch 920, training loss: 6.1155829429626465 = 0.05900508165359497 + 1.0 * 6.056577682495117
Epoch 920, val loss: 0.9419686198234558
Epoch 930, training loss: 6.113436698913574 = 0.05674862489104271 + 1.0 * 6.05668830871582
Epoch 930, val loss: 0.9469826817512512
Epoch 940, training loss: 6.117319107055664 = 0.054584335535764694 + 1.0 * 6.062734603881836
Epoch 940, val loss: 0.9519261121749878
Epoch 950, training loss: 6.110276699066162 = 0.05254305899143219 + 1.0 * 6.057733535766602
Epoch 950, val loss: 0.9568726420402527
Epoch 960, training loss: 6.105689525604248 = 0.05062185227870941 + 1.0 * 6.055067539215088
Epoch 960, val loss: 0.9618303179740906
Epoch 970, training loss: 6.102966785430908 = 0.04879574477672577 + 1.0 * 6.054171085357666
Epoch 970, val loss: 0.9667983651161194
Epoch 980, training loss: 6.10051965713501 = 0.04704979434609413 + 1.0 * 6.053469657897949
Epoch 980, val loss: 0.9717739820480347
Epoch 990, training loss: 6.098790168762207 = 0.04538454860448837 + 1.0 * 6.05340576171875
Epoch 990, val loss: 0.9766469597816467
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.9779
Flip ASR: 0.9822/225 nodes
The final ASR:0.67405, 0.23249, Accuracy:0.81852, 0.00907
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10586])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97663, 0.00460, Accuracy:0.83210, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.31330394744873 = 1.9394232034683228 + 1.0 * 8.373880386352539
Epoch 0, val loss: 1.942265272140503
Epoch 10, training loss: 10.302508354187012 = 1.9289971590042114 + 1.0 * 8.37351131439209
Epoch 10, val loss: 1.9319437742233276
Epoch 20, training loss: 10.286993980407715 = 1.9157594442367554 + 1.0 * 8.371234893798828
Epoch 20, val loss: 1.918792963027954
Epoch 30, training loss: 10.253311157226562 = 1.8968186378479004 + 1.0 * 8.356492042541504
Epoch 30, val loss: 1.9002410173416138
Epoch 40, training loss: 10.139355659484863 = 1.8720062971115112 + 1.0 * 8.267349243164062
Epoch 40, val loss: 1.87716805934906
Epoch 50, training loss: 9.679091453552246 = 1.84575355052948 + 1.0 * 7.833337783813477
Epoch 50, val loss: 1.853336215019226
Epoch 60, training loss: 9.229818344116211 = 1.8228936195373535 + 1.0 * 7.406924724578857
Epoch 60, val loss: 1.83315110206604
Epoch 70, training loss: 8.84632396697998 = 1.8066238164901733 + 1.0 * 7.039700508117676
Epoch 70, val loss: 1.8185498714447021
Epoch 80, training loss: 8.612457275390625 = 1.7900893688201904 + 1.0 * 6.822368144989014
Epoch 80, val loss: 1.8030866384506226
Epoch 90, training loss: 8.466470718383789 = 1.7721751928329468 + 1.0 * 6.694295406341553
Epoch 90, val loss: 1.7868542671203613
Epoch 100, training loss: 8.35952377319336 = 1.753121018409729 + 1.0 * 6.606402397155762
Epoch 100, val loss: 1.7703033685684204
Epoch 110, training loss: 8.276616096496582 = 1.7330560684204102 + 1.0 * 6.543560028076172
Epoch 110, val loss: 1.752756953239441
Epoch 120, training loss: 8.203558921813965 = 1.7112044095993042 + 1.0 * 6.492354869842529
Epoch 120, val loss: 1.7333954572677612
Epoch 130, training loss: 8.136004447937012 = 1.6864808797836304 + 1.0 * 6.44952392578125
Epoch 130, val loss: 1.7116199731826782
Epoch 140, training loss: 8.072503089904785 = 1.6576272249221802 + 1.0 * 6.4148759841918945
Epoch 140, val loss: 1.6870814561843872
Epoch 150, training loss: 8.013040542602539 = 1.623778223991394 + 1.0 * 6.3892621994018555
Epoch 150, val loss: 1.6588516235351562
Epoch 160, training loss: 7.950704097747803 = 1.5851057767868042 + 1.0 * 6.365598201751709
Epoch 160, val loss: 1.6266753673553467
Epoch 170, training loss: 7.887326240539551 = 1.5413939952850342 + 1.0 * 6.345932483673096
Epoch 170, val loss: 1.5902212858200073
Epoch 180, training loss: 7.821694374084473 = 1.4931155443191528 + 1.0 * 6.328578948974609
Epoch 180, val loss: 1.5501713752746582
Epoch 190, training loss: 7.7540459632873535 = 1.4414925575256348 + 1.0 * 6.312553405761719
Epoch 190, val loss: 1.5076600313186646
Epoch 200, training loss: 7.688309669494629 = 1.3879306316375732 + 1.0 * 6.300379276275635
Epoch 200, val loss: 1.4642449617385864
Epoch 210, training loss: 7.6210174560546875 = 1.3344981670379639 + 1.0 * 6.286519527435303
Epoch 210, val loss: 1.421514868736267
Epoch 220, training loss: 7.555464744567871 = 1.2816078662872314 + 1.0 * 6.273857116699219
Epoch 220, val loss: 1.3798009157180786
Epoch 230, training loss: 7.4980316162109375 = 1.2299327850341797 + 1.0 * 6.268098831176758
Epoch 230, val loss: 1.339851975440979
Epoch 240, training loss: 7.434907913208008 = 1.180802583694458 + 1.0 * 6.254105567932129
Epoch 240, val loss: 1.3028169870376587
Epoch 250, training loss: 7.378681659698486 = 1.133177399635315 + 1.0 * 6.245504379272461
Epoch 250, val loss: 1.2672480344772339
Epoch 260, training loss: 7.322848320007324 = 1.0868432521820068 + 1.0 * 6.236004829406738
Epoch 260, val loss: 1.2329416275024414
Epoch 270, training loss: 7.2702741622924805 = 1.0414698123931885 + 1.0 * 6.228804588317871
Epoch 270, val loss: 1.1995282173156738
Epoch 280, training loss: 7.223164081573486 = 0.99700927734375 + 1.0 * 6.226154804229736
Epoch 280, val loss: 1.1668347120285034
Epoch 290, training loss: 7.169940948486328 = 0.9541237950325012 + 1.0 * 6.215816974639893
Epoch 290, val loss: 1.1351290941238403
Epoch 300, training loss: 7.1214165687561035 = 0.9122175574302673 + 1.0 * 6.209198951721191
Epoch 300, val loss: 1.1040314435958862
Epoch 310, training loss: 7.076133728027344 = 0.8710115551948547 + 1.0 * 6.205121994018555
Epoch 310, val loss: 1.07322359085083
Epoch 320, training loss: 7.031547546386719 = 0.8308384418487549 + 1.0 * 6.200709342956543
Epoch 320, val loss: 1.0429176092147827
Epoch 330, training loss: 6.985668659210205 = 0.7915119528770447 + 1.0 * 6.194156646728516
Epoch 330, val loss: 1.01309335231781
Epoch 340, training loss: 6.943922996520996 = 0.752859354019165 + 1.0 * 6.191063404083252
Epoch 340, val loss: 0.9835232496261597
Epoch 350, training loss: 6.90263032913208 = 0.7152690887451172 + 1.0 * 6.187361240386963
Epoch 350, val loss: 0.9546148180961609
Epoch 360, training loss: 6.862846374511719 = 0.6790763139724731 + 1.0 * 6.183770179748535
Epoch 360, val loss: 0.9267378449440002
Epoch 370, training loss: 6.822646141052246 = 0.6444995403289795 + 1.0 * 6.178146839141846
Epoch 370, val loss: 0.9002490639686584
Epoch 380, training loss: 6.7857985496521 = 0.6117694973945618 + 1.0 * 6.1740288734436035
Epoch 380, val loss: 0.8753510117530823
Epoch 390, training loss: 6.7512078285217285 = 0.5809052586555481 + 1.0 * 6.170302391052246
Epoch 390, val loss: 0.8522831797599792
Epoch 400, training loss: 6.728166580200195 = 0.5522520542144775 + 1.0 * 6.175914764404297
Epoch 400, val loss: 0.8313685059547424
Epoch 410, training loss: 6.691787242889404 = 0.5261359810829163 + 1.0 * 6.165651321411133
Epoch 410, val loss: 0.812969446182251
Epoch 420, training loss: 6.662233829498291 = 0.5019027590751648 + 1.0 * 6.1603312492370605
Epoch 420, val loss: 0.7967690229415894
Epoch 430, training loss: 6.636473178863525 = 0.47919976711273193 + 1.0 * 6.157273292541504
Epoch 430, val loss: 0.7823417782783508
Epoch 440, training loss: 6.6128830909729 = 0.45781323313713074 + 1.0 * 6.155069828033447
Epoch 440, val loss: 0.7695223689079285
Epoch 450, training loss: 6.588672161102295 = 0.437517374753952 + 1.0 * 6.1511549949646
Epoch 450, val loss: 0.758232593536377
Epoch 460, training loss: 6.567347049713135 = 0.4179069399833679 + 1.0 * 6.149440288543701
Epoch 460, val loss: 0.747992992401123
Epoch 470, training loss: 6.546954154968262 = 0.39876553416252136 + 1.0 * 6.148188591003418
Epoch 470, val loss: 0.7384846210479736
Epoch 480, training loss: 6.52497673034668 = 0.3799615800380707 + 1.0 * 6.145015239715576
Epoch 480, val loss: 0.7296284437179565
Epoch 490, training loss: 6.502015113830566 = 0.36116617918014526 + 1.0 * 6.1408491134643555
Epoch 490, val loss: 0.7212085723876953
Epoch 500, training loss: 6.487968921661377 = 0.3423446714878082 + 1.0 * 6.145624160766602
Epoch 500, val loss: 0.7130810022354126
Epoch 510, training loss: 6.461187839508057 = 0.32362037897109985 + 1.0 * 6.137567520141602
Epoch 510, val loss: 0.7052412629127502
Epoch 520, training loss: 6.438577175140381 = 0.3051008880138397 + 1.0 * 6.133476257324219
Epoch 520, val loss: 0.6978925466537476
Epoch 530, training loss: 6.417661666870117 = 0.28672948479652405 + 1.0 * 6.130932331085205
Epoch 530, val loss: 0.6908802390098572
Epoch 540, training loss: 6.398615837097168 = 0.268643319606781 + 1.0 * 6.129972457885742
Epoch 540, val loss: 0.6843166351318359
Epoch 550, training loss: 6.383349418640137 = 0.2511129379272461 + 1.0 * 6.132236480712891
Epoch 550, val loss: 0.6783133149147034
Epoch 560, training loss: 6.361672401428223 = 0.2345138043165207 + 1.0 * 6.1271586418151855
Epoch 560, val loss: 0.6731573343276978
Epoch 570, training loss: 6.3429694175720215 = 0.21879266202449799 + 1.0 * 6.124176979064941
Epoch 570, val loss: 0.6688228845596313
Epoch 580, training loss: 6.325429439544678 = 0.20405535399913788 + 1.0 * 6.121374130249023
Epoch 580, val loss: 0.6652503609657288
Epoch 590, training loss: 6.310769081115723 = 0.1903999149799347 + 1.0 * 6.120368957519531
Epoch 590, val loss: 0.6624936461448669
Epoch 600, training loss: 6.298025608062744 = 0.17791558802127838 + 1.0 * 6.120110034942627
Epoch 600, val loss: 0.6607136130332947
Epoch 610, training loss: 6.283942222595215 = 0.16640770435333252 + 1.0 * 6.117534637451172
Epoch 610, val loss: 0.6596968770027161
Epoch 620, training loss: 6.276846885681152 = 0.15578961372375488 + 1.0 * 6.121057510375977
Epoch 620, val loss: 0.659318745136261
Epoch 630, training loss: 6.262683868408203 = 0.1460179090499878 + 1.0 * 6.116665840148926
Epoch 630, val loss: 0.6596183776855469
Epoch 640, training loss: 6.248562812805176 = 0.13700786232948303 + 1.0 * 6.111555099487305
Epoch 640, val loss: 0.6606002449989319
Epoch 650, training loss: 6.241689682006836 = 0.12865526974201202 + 1.0 * 6.113034248352051
Epoch 650, val loss: 0.6621219515800476
Epoch 660, training loss: 6.232301712036133 = 0.12095627933740616 + 1.0 * 6.111345291137695
Epoch 660, val loss: 0.6641014814376831
Epoch 670, training loss: 6.220394134521484 = 0.11377834528684616 + 1.0 * 6.106616020202637
Epoch 670, val loss: 0.6665719151496887
Epoch 680, training loss: 6.211977005004883 = 0.10710960626602173 + 1.0 * 6.104867458343506
Epoch 680, val loss: 0.6694209575653076
Epoch 690, training loss: 6.213552951812744 = 0.10088244080543518 + 1.0 * 6.112670421600342
Epoch 690, val loss: 0.6725935339927673
Epoch 700, training loss: 6.198907375335693 = 0.09511128813028336 + 1.0 * 6.103796005249023
Epoch 700, val loss: 0.6761324405670166
Epoch 710, training loss: 6.193825721740723 = 0.08972708880901337 + 1.0 * 6.104098796844482
Epoch 710, val loss: 0.6799624562263489
Epoch 720, training loss: 6.184659481048584 = 0.08471879363059998 + 1.0 * 6.099940776824951
Epoch 720, val loss: 0.6840575337409973
Epoch 730, training loss: 6.178384304046631 = 0.08003149926662445 + 1.0 * 6.098352909088135
Epoch 730, val loss: 0.6884024143218994
Epoch 740, training loss: 6.184502124786377 = 0.07566031068563461 + 1.0 * 6.108841896057129
Epoch 740, val loss: 0.6930137276649475
Epoch 750, training loss: 6.167483806610107 = 0.07164745032787323 + 1.0 * 6.095836162567139
Epoch 750, val loss: 0.6977410912513733
Epoch 760, training loss: 6.163184642791748 = 0.06791158765554428 + 1.0 * 6.095273017883301
Epoch 760, val loss: 0.7026787400245667
Epoch 770, training loss: 6.156994819641113 = 0.06443223357200623 + 1.0 * 6.092562675476074
Epoch 770, val loss: 0.7077606320381165
Epoch 780, training loss: 6.157629013061523 = 0.06117921322584152 + 1.0 * 6.096449851989746
Epoch 780, val loss: 0.7129760384559631
Epoch 790, training loss: 6.153614044189453 = 0.058181073516607285 + 1.0 * 6.095432758331299
Epoch 790, val loss: 0.7182865142822266
Epoch 800, training loss: 6.145169734954834 = 0.05537404492497444 + 1.0 * 6.0897955894470215
Epoch 800, val loss: 0.7236542105674744
Epoch 810, training loss: 6.144471645355225 = 0.05276783928275108 + 1.0 * 6.09170389175415
Epoch 810, val loss: 0.7291069030761719
Epoch 820, training loss: 6.137567043304443 = 0.05033300071954727 + 1.0 * 6.087234020233154
Epoch 820, val loss: 0.7345646023750305
Epoch 830, training loss: 6.133243560791016 = 0.04805713891983032 + 1.0 * 6.08518648147583
Epoch 830, val loss: 0.7400913834571838
Epoch 840, training loss: 6.132005214691162 = 0.045930828899145126 + 1.0 * 6.086074352264404
Epoch 840, val loss: 0.7456592917442322
Epoch 850, training loss: 6.13176155090332 = 0.04393569380044937 + 1.0 * 6.087825775146484
Epoch 850, val loss: 0.751177191734314
Epoch 860, training loss: 6.12696647644043 = 0.04208364337682724 + 1.0 * 6.084882736206055
Epoch 860, val loss: 0.7566864490509033
Epoch 870, training loss: 6.122023582458496 = 0.040340226143598557 + 1.0 * 6.081683158874512
Epoch 870, val loss: 0.7622103691101074
Epoch 880, training loss: 6.120527267456055 = 0.038701675832271576 + 1.0 * 6.0818257331848145
Epoch 880, val loss: 0.7677441239356995
Epoch 890, training loss: 6.118743419647217 = 0.037166085094213486 + 1.0 * 6.081577301025391
Epoch 890, val loss: 0.773224413394928
Epoch 900, training loss: 6.115688800811768 = 0.03572104498744011 + 1.0 * 6.079967975616455
Epoch 900, val loss: 0.7786908149719238
Epoch 910, training loss: 6.11326265335083 = 0.034362439066171646 + 1.0 * 6.078900337219238
Epoch 910, val loss: 0.7841458320617676
Epoch 920, training loss: 6.112002849578857 = 0.03308038413524628 + 1.0 * 6.078922271728516
Epoch 920, val loss: 0.7895280718803406
Epoch 930, training loss: 6.111172199249268 = 0.03187214583158493 + 1.0 * 6.0792999267578125
Epoch 930, val loss: 0.7949106693267822
Epoch 940, training loss: 6.109893321990967 = 0.030725622549653053 + 1.0 * 6.07916784286499
Epoch 940, val loss: 0.8002076745033264
Epoch 950, training loss: 6.104839324951172 = 0.029650332406163216 + 1.0 * 6.075189113616943
Epoch 950, val loss: 0.805458664894104
Epoch 960, training loss: 6.101569175720215 = 0.028629012405872345 + 1.0 * 6.072940349578857
Epoch 960, val loss: 0.8106779456138611
Epoch 970, training loss: 6.104725360870361 = 0.027656154707074165 + 1.0 * 6.077069282531738
Epoch 970, val loss: 0.8158696889877319
Epoch 980, training loss: 6.0990424156188965 = 0.02673725038766861 + 1.0 * 6.072305202484131
Epoch 980, val loss: 0.8210436701774597
Epoch 990, training loss: 6.096683025360107 = 0.025861449539661407 + 1.0 * 6.070821762084961
Epoch 990, val loss: 0.8261560797691345
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.7380
Flip ASR: 0.6844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.324734687805176 = 1.9508962631225586 + 1.0 * 8.373838424682617
Epoch 0, val loss: 1.9562393426895142
Epoch 10, training loss: 10.313230514526367 = 1.9399027824401855 + 1.0 * 8.37332820892334
Epoch 10, val loss: 1.9444360733032227
Epoch 20, training loss: 10.296165466308594 = 1.925984263420105 + 1.0 * 8.3701810836792
Epoch 20, val loss: 1.9292973279953003
Epoch 30, training loss: 10.256207466125488 = 1.9065240621566772 + 1.0 * 8.34968376159668
Epoch 30, val loss: 1.908187747001648
Epoch 40, training loss: 10.100393295288086 = 1.881864070892334 + 1.0 * 8.21852970123291
Epoch 40, val loss: 1.8828448057174683
Epoch 50, training loss: 9.628396987915039 = 1.8559086322784424 + 1.0 * 7.772488117218018
Epoch 50, val loss: 1.8565829992294312
Epoch 60, training loss: 9.137998580932617 = 1.833784818649292 + 1.0 * 7.304214000701904
Epoch 60, val loss: 1.8353322744369507
Epoch 70, training loss: 8.723366737365723 = 1.8204584121704102 + 1.0 * 6.9029083251953125
Epoch 70, val loss: 1.8229774236679077
Epoch 80, training loss: 8.522342681884766 = 1.805220365524292 + 1.0 * 6.717122554779053
Epoch 80, val loss: 1.8081674575805664
Epoch 90, training loss: 8.384146690368652 = 1.7874603271484375 + 1.0 * 6.596686363220215
Epoch 90, val loss: 1.7908928394317627
Epoch 100, training loss: 8.28977108001709 = 1.7694084644317627 + 1.0 * 6.520362854003906
Epoch 100, val loss: 1.7738738059997559
Epoch 110, training loss: 8.215536117553711 = 1.7509379386901855 + 1.0 * 6.464598178863525
Epoch 110, val loss: 1.7570226192474365
Epoch 120, training loss: 8.155861854553223 = 1.7313445806503296 + 1.0 * 6.424517631530762
Epoch 120, val loss: 1.7394917011260986
Epoch 130, training loss: 8.096948623657227 = 1.7097312211990356 + 1.0 * 6.387217044830322
Epoch 130, val loss: 1.72063410282135
Epoch 140, training loss: 8.045034408569336 = 1.6850281953811646 + 1.0 * 6.360006332397461
Epoch 140, val loss: 1.6996567249298096
Epoch 150, training loss: 7.992525100708008 = 1.6564223766326904 + 1.0 * 6.3361029624938965
Epoch 150, val loss: 1.6759717464447021
Epoch 160, training loss: 7.942790985107422 = 1.6238118410110474 + 1.0 * 6.318979263305664
Epoch 160, val loss: 1.6495685577392578
Epoch 170, training loss: 7.8863019943237305 = 1.5875824689865112 + 1.0 * 6.29871940612793
Epoch 170, val loss: 1.620654821395874
Epoch 180, training loss: 7.829408168792725 = 1.5474472045898438 + 1.0 * 6.281960964202881
Epoch 180, val loss: 1.5890229940414429
Epoch 190, training loss: 7.772824287414551 = 1.5042548179626465 + 1.0 * 6.268569469451904
Epoch 190, val loss: 1.5555599927902222
Epoch 200, training loss: 7.715490341186523 = 1.4598352909088135 + 1.0 * 6.255654811859131
Epoch 200, val loss: 1.522087812423706
Epoch 210, training loss: 7.659192085266113 = 1.414857029914856 + 1.0 * 6.244335174560547
Epoch 210, val loss: 1.4889247417449951
Epoch 220, training loss: 7.6034836769104 = 1.3692593574523926 + 1.0 * 6.234224319458008
Epoch 220, val loss: 1.456093430519104
Epoch 230, training loss: 7.552090644836426 = 1.3233200311660767 + 1.0 * 6.228770732879639
Epoch 230, val loss: 1.4238566160202026
Epoch 240, training loss: 7.4957594871521 = 1.2778964042663574 + 1.0 * 6.217863082885742
Epoch 240, val loss: 1.3925849199295044
Epoch 250, training loss: 7.442403316497803 = 1.232561707496643 + 1.0 * 6.209841728210449
Epoch 250, val loss: 1.3618652820587158
Epoch 260, training loss: 7.391393184661865 = 1.1875957250595093 + 1.0 * 6.203797340393066
Epoch 260, val loss: 1.3319129943847656
Epoch 270, training loss: 7.340854644775391 = 1.143479347229004 + 1.0 * 6.197375297546387
Epoch 270, val loss: 1.302819848060608
Epoch 280, training loss: 7.292898178100586 = 1.1002962589263916 + 1.0 * 6.192602157592773
Epoch 280, val loss: 1.2747042179107666
Epoch 290, training loss: 7.244513034820557 = 1.058336853981018 + 1.0 * 6.186176300048828
Epoch 290, val loss: 1.2476812601089478
Epoch 300, training loss: 7.19950008392334 = 1.0177782773971558 + 1.0 * 6.1817216873168945
Epoch 300, val loss: 1.221674919128418
Epoch 310, training loss: 7.155269622802734 = 0.97886061668396 + 1.0 * 6.1764092445373535
Epoch 310, val loss: 1.1967300176620483
Epoch 320, training loss: 7.115523815155029 = 0.9410595893859863 + 1.0 * 6.174464225769043
Epoch 320, val loss: 1.1722931861877441
Epoch 330, training loss: 7.07307243347168 = 0.9040980339050293 + 1.0 * 6.16897439956665
Epoch 330, val loss: 1.1480071544647217
Epoch 340, training loss: 7.032036304473877 = 0.8676020503044128 + 1.0 * 6.164434432983398
Epoch 340, val loss: 1.1234898567199707
Epoch 350, training loss: 6.990857124328613 = 0.8310519456863403 + 1.0 * 6.1598052978515625
Epoch 350, val loss: 1.0984565019607544
Epoch 360, training loss: 6.954996109008789 = 0.7941192388534546 + 1.0 * 6.160876750946045
Epoch 360, val loss: 1.0726426839828491
Epoch 370, training loss: 6.911959648132324 = 0.7571948766708374 + 1.0 * 6.154764652252197
Epoch 370, val loss: 1.0462669134140015
Epoch 380, training loss: 6.870525360107422 = 0.7202554941177368 + 1.0 * 6.150269985198975
Epoch 380, val loss: 1.0195667743682861
Epoch 390, training loss: 6.830043315887451 = 0.6834317445755005 + 1.0 * 6.14661169052124
Epoch 390, val loss: 0.9926996231079102
Epoch 400, training loss: 6.792448043823242 = 0.6473111510276794 + 1.0 * 6.145136833190918
Epoch 400, val loss: 0.9663472771644592
Epoch 410, training loss: 6.756044387817383 = 0.6125567555427551 + 1.0 * 6.143487453460693
Epoch 410, val loss: 0.9412142038345337
Epoch 420, training loss: 6.717924118041992 = 0.5792896151542664 + 1.0 * 6.13863468170166
Epoch 420, val loss: 0.9174813628196716
Epoch 430, training loss: 6.682844638824463 = 0.5475398302078247 + 1.0 * 6.135304927825928
Epoch 430, val loss: 0.8954496383666992
Epoch 440, training loss: 6.6577277183532715 = 0.5173707008361816 + 1.0 * 6.14035701751709
Epoch 440, val loss: 0.8752992153167725
Epoch 450, training loss: 6.621879577636719 = 0.4889870584011078 + 1.0 * 6.132892608642578
Epoch 450, val loss: 0.8571493625640869
Epoch 460, training loss: 6.5910186767578125 = 0.4620823562145233 + 1.0 * 6.128936290740967
Epoch 460, val loss: 0.8408589363098145
Epoch 470, training loss: 6.563845157623291 = 0.4365503489971161 + 1.0 * 6.127295017242432
Epoch 470, val loss: 0.8263674378395081
Epoch 480, training loss: 6.53735876083374 = 0.4123947322368622 + 1.0 * 6.124964237213135
Epoch 480, val loss: 0.8138688206672668
Epoch 490, training loss: 6.51150369644165 = 0.3892595171928406 + 1.0 * 6.122244358062744
Epoch 490, val loss: 0.8029119372367859
Epoch 500, training loss: 6.496959686279297 = 0.367021769285202 + 1.0 * 6.129938125610352
Epoch 500, val loss: 0.7932002544403076
Epoch 510, training loss: 6.4651947021484375 = 0.34579768776893616 + 1.0 * 6.119397163391113
Epoch 510, val loss: 0.7847548723220825
Epoch 520, training loss: 6.441995620727539 = 0.32555413246154785 + 1.0 * 6.11644172668457
Epoch 520, val loss: 0.7777636051177979
Epoch 530, training loss: 6.420711517333984 = 0.3061406910419464 + 1.0 * 6.114570617675781
Epoch 530, val loss: 0.77175372838974
Epoch 540, training loss: 6.403157711029053 = 0.28761136531829834 + 1.0 * 6.115546226501465
Epoch 540, val loss: 0.7668067216873169
Epoch 550, training loss: 6.384621620178223 = 0.2700120210647583 + 1.0 * 6.114609718322754
Epoch 550, val loss: 0.7630311250686646
Epoch 560, training loss: 6.362034320831299 = 0.25340110063552856 + 1.0 * 6.108633041381836
Epoch 560, val loss: 0.7602583765983582
Epoch 570, training loss: 6.344571590423584 = 0.23764744400978088 + 1.0 * 6.106924057006836
Epoch 570, val loss: 0.7584482431411743
Epoch 580, training loss: 6.331923007965088 = 0.22277987003326416 + 1.0 * 6.109143257141113
Epoch 580, val loss: 0.7575591206550598
Epoch 590, training loss: 6.318536758422852 = 0.20884905755519867 + 1.0 * 6.109687805175781
Epoch 590, val loss: 0.7574862837791443
Epoch 600, training loss: 6.2990498542785645 = 0.19581909477710724 + 1.0 * 6.103230953216553
Epoch 600, val loss: 0.758303701877594
Epoch 610, training loss: 6.28840446472168 = 0.18358558416366577 + 1.0 * 6.104818820953369
Epoch 610, val loss: 0.7597625851631165
Epoch 620, training loss: 6.275190830230713 = 0.1721535623073578 + 1.0 * 6.103037357330322
Epoch 620, val loss: 0.7619577646255493
Epoch 630, training loss: 6.260214328765869 = 0.1615118682384491 + 1.0 * 6.098702430725098
Epoch 630, val loss: 0.7648415565490723
Epoch 640, training loss: 6.250298023223877 = 0.15157687664031982 + 1.0 * 6.098721027374268
Epoch 640, val loss: 0.7682943940162659
Epoch 650, training loss: 6.238044738769531 = 0.14234845340251923 + 1.0 * 6.095696449279785
Epoch 650, val loss: 0.7722262144088745
Epoch 660, training loss: 6.229791641235352 = 0.1337863653898239 + 1.0 * 6.096005439758301
Epoch 660, val loss: 0.7766595482826233
Epoch 670, training loss: 6.2289509773254395 = 0.12587794661521912 + 1.0 * 6.1030731201171875
Epoch 670, val loss: 0.7814613580703735
Epoch 680, training loss: 6.2121477127075195 = 0.11857368797063828 + 1.0 * 6.093574047088623
Epoch 680, val loss: 0.7867518067359924
Epoch 690, training loss: 6.201317310333252 = 0.11181843280792236 + 1.0 * 6.089498996734619
Epoch 690, val loss: 0.7924239635467529
Epoch 700, training loss: 6.194296836853027 = 0.10554076731204987 + 1.0 * 6.088756084442139
Epoch 700, val loss: 0.7982823848724365
Epoch 710, training loss: 6.187263011932373 = 0.09973574429750443 + 1.0 * 6.087527275085449
Epoch 710, val loss: 0.8042877912521362
Epoch 720, training loss: 6.182034969329834 = 0.0943818911910057 + 1.0 * 6.087653160095215
Epoch 720, val loss: 0.8107728362083435
Epoch 730, training loss: 6.174623489379883 = 0.08941122144460678 + 1.0 * 6.085212230682373
Epoch 730, val loss: 0.8174392580986023
Epoch 740, training loss: 6.175488471984863 = 0.08478941768407822 + 1.0 * 6.090699195861816
Epoch 740, val loss: 0.824094295501709
Epoch 750, training loss: 6.167271137237549 = 0.08052463084459305 + 1.0 * 6.086746692657471
Epoch 750, val loss: 0.8309555053710938
Epoch 760, training loss: 6.160141944885254 = 0.07656347006559372 + 1.0 * 6.083578586578369
Epoch 760, val loss: 0.8379553556442261
Epoch 770, training loss: 6.1546149253845215 = 0.0728759765625 + 1.0 * 6.0817389488220215
Epoch 770, val loss: 0.8449618220329285
Epoch 780, training loss: 6.150332450866699 = 0.06943801790475845 + 1.0 * 6.080894470214844
Epoch 780, val loss: 0.8519561290740967
Epoch 790, training loss: 6.144693851470947 = 0.06623361259698868 + 1.0 * 6.078460216522217
Epoch 790, val loss: 0.8591859936714172
Epoch 800, training loss: 6.158853054046631 = 0.06323803216218948 + 1.0 * 6.095614910125732
Epoch 800, val loss: 0.8663446307182312
Epoch 810, training loss: 6.135963439941406 = 0.06047073379158974 + 1.0 * 6.075492858886719
Epoch 810, val loss: 0.8732637166976929
Epoch 820, training loss: 6.133209705352783 = 0.057881951332092285 + 1.0 * 6.0753278732299805
Epoch 820, val loss: 0.8805257081985474
Epoch 830, training loss: 6.129321098327637 = 0.05544253811240196 + 1.0 * 6.073878765106201
Epoch 830, val loss: 0.8875529170036316
Epoch 840, training loss: 6.126143932342529 = 0.053138140588998795 + 1.0 * 6.073005676269531
Epoch 840, val loss: 0.8946828246116638
Epoch 850, training loss: 6.123997688293457 = 0.050975602120161057 + 1.0 * 6.07302188873291
Epoch 850, val loss: 0.9015920758247375
Epoch 860, training loss: 6.121821880340576 = 0.04895893111824989 + 1.0 * 6.0728631019592285
Epoch 860, val loss: 0.9087527394294739
Epoch 870, training loss: 6.11724853515625 = 0.0470564030110836 + 1.0 * 6.070192337036133
Epoch 870, val loss: 0.9157658219337463
Epoch 880, training loss: 6.116094589233398 = 0.04525606706738472 + 1.0 * 6.070838451385498
Epoch 880, val loss: 0.9226657152175903
Epoch 890, training loss: 6.111442565917969 = 0.043557438999414444 + 1.0 * 6.067884922027588
Epoch 890, val loss: 0.9294485449790955
Epoch 900, training loss: 6.11729097366333 = 0.041959114372730255 + 1.0 * 6.075331687927246
Epoch 900, val loss: 0.9363674521446228
Epoch 910, training loss: 6.110198020935059 = 0.04045708477497101 + 1.0 * 6.0697407722473145
Epoch 910, val loss: 0.9430027604103088
Epoch 920, training loss: 6.104180812835693 = 0.03903118520975113 + 1.0 * 6.065149784088135
Epoch 920, val loss: 0.9497997164726257
Epoch 930, training loss: 6.101669788360596 = 0.037675127387046814 + 1.0 * 6.063994884490967
Epoch 930, val loss: 0.9564370512962341
Epoch 940, training loss: 6.11345911026001 = 0.0363871268928051 + 1.0 * 6.0770721435546875
Epoch 940, val loss: 0.9629507660865784
Epoch 950, training loss: 6.099653720855713 = 0.03516814485192299 + 1.0 * 6.064485549926758
Epoch 950, val loss: 0.9693717956542969
Epoch 960, training loss: 6.096768856048584 = 0.03401574119925499 + 1.0 * 6.062753200531006
Epoch 960, val loss: 0.9759487509727478
Epoch 970, training loss: 6.097845554351807 = 0.032913412898778915 + 1.0 * 6.064932346343994
Epoch 970, val loss: 0.9823033213615417
Epoch 980, training loss: 6.094002723693848 = 0.03186800330877304 + 1.0 * 6.062134742736816
Epoch 980, val loss: 0.9883700013160706
Epoch 990, training loss: 6.092009544372559 = 0.03087443672120571 + 1.0 * 6.061135292053223
Epoch 990, val loss: 0.9948902130126953
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.7897
Flip ASR: 0.7467/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.31840705871582 = 1.9445874691009521 + 1.0 * 8.373819351196289
Epoch 0, val loss: 1.95388662815094
Epoch 10, training loss: 10.306954383850098 = 1.9342079162597656 + 1.0 * 8.372746467590332
Epoch 10, val loss: 1.9427719116210938
Epoch 20, training loss: 10.289326667785645 = 1.9210659265518188 + 1.0 * 8.368260383605957
Epoch 20, val loss: 1.9277169704437256
Epoch 30, training loss: 10.248520851135254 = 1.903019666671753 + 1.0 * 8.345500946044922
Epoch 30, val loss: 1.9063498973846436
Epoch 40, training loss: 10.097938537597656 = 1.8812041282653809 + 1.0 * 8.216734886169434
Epoch 40, val loss: 1.8810800313949585
Epoch 50, training loss: 9.692022323608398 = 1.8585089445114136 + 1.0 * 7.8335137367248535
Epoch 50, val loss: 1.855221152305603
Epoch 60, training loss: 9.17367935180664 = 1.8387058973312378 + 1.0 * 7.3349738121032715
Epoch 60, val loss: 1.8346216678619385
Epoch 70, training loss: 8.725163459777832 = 1.8229234218597412 + 1.0 * 6.902239799499512
Epoch 70, val loss: 1.8178318738937378
Epoch 80, training loss: 8.544561386108398 = 1.807631015777588 + 1.0 * 6.736930847167969
Epoch 80, val loss: 1.8017687797546387
Epoch 90, training loss: 8.428567886352539 = 1.7907687425613403 + 1.0 * 6.637799263000488
Epoch 90, val loss: 1.7841752767562866
Epoch 100, training loss: 8.333749771118164 = 1.7735562324523926 + 1.0 * 6.5601935386657715
Epoch 100, val loss: 1.7668004035949707
Epoch 110, training loss: 8.254175186157227 = 1.7571085691452026 + 1.0 * 6.497066974639893
Epoch 110, val loss: 1.7505241632461548
Epoch 120, training loss: 8.18789291381836 = 1.7404721975326538 + 1.0 * 6.447420597076416
Epoch 120, val loss: 1.7344844341278076
Epoch 130, training loss: 8.130955696105957 = 1.7224723100662231 + 1.0 * 6.408483028411865
Epoch 130, val loss: 1.7175533771514893
Epoch 140, training loss: 8.079078674316406 = 1.702149510383606 + 1.0 * 6.376928806304932
Epoch 140, val loss: 1.699143409729004
Epoch 150, training loss: 8.030790328979492 = 1.6787490844726562 + 1.0 * 6.352040767669678
Epoch 150, val loss: 1.6789220571517944
Epoch 160, training loss: 7.982209205627441 = 1.6516916751861572 + 1.0 * 6.330517768859863
Epoch 160, val loss: 1.656087040901184
Epoch 170, training loss: 7.933197498321533 = 1.6201157569885254 + 1.0 * 6.313081741333008
Epoch 170, val loss: 1.6299197673797607
Epoch 180, training loss: 7.880620002746582 = 1.5832202434539795 + 1.0 * 6.297399520874023
Epoch 180, val loss: 1.5998455286026
Epoch 190, training loss: 7.823766708374023 = 1.5399271249771118 + 1.0 * 6.283839702606201
Epoch 190, val loss: 1.5648565292358398
Epoch 200, training loss: 7.762110233306885 = 1.4896769523620605 + 1.0 * 6.272433280944824
Epoch 200, val loss: 1.524644136428833
Epoch 210, training loss: 7.6950178146362305 = 1.432774305343628 + 1.0 * 6.262243747711182
Epoch 210, val loss: 1.4791768789291382
Epoch 220, training loss: 7.621947288513184 = 1.369437336921692 + 1.0 * 6.252510070800781
Epoch 220, val loss: 1.4289946556091309
Epoch 230, training loss: 7.546638011932373 = 1.3012422323226929 + 1.0 * 6.245395660400391
Epoch 230, val loss: 1.3757117986679077
Epoch 240, training loss: 7.468462944030762 = 1.2314112186431885 + 1.0 * 6.237051486968994
Epoch 240, val loss: 1.3212765455245972
Epoch 250, training loss: 7.390219688415527 = 1.1610313653945923 + 1.0 * 6.229188442230225
Epoch 250, val loss: 1.2668836116790771
Epoch 260, training loss: 7.3172807693481445 = 1.0920202732086182 + 1.0 * 6.225260257720947
Epoch 260, val loss: 1.213638424873352
Epoch 270, training loss: 7.24370813369751 = 1.0264391899108887 + 1.0 * 6.217268943786621
Epoch 270, val loss: 1.1632020473480225
Epoch 280, training loss: 7.17490291595459 = 0.9643809795379639 + 1.0 * 6.210521697998047
Epoch 280, val loss: 1.1152267456054688
Epoch 290, training loss: 7.1172566413879395 = 0.9059451222419739 + 1.0 * 6.211311340332031
Epoch 290, val loss: 1.0702372789382935
Epoch 300, training loss: 7.0552144050598145 = 0.8524382710456848 + 1.0 * 6.202775955200195
Epoch 300, val loss: 1.0288830995559692
Epoch 310, training loss: 6.997115135192871 = 0.802260160446167 + 1.0 * 6.194855213165283
Epoch 310, val loss: 0.9903241395950317
Epoch 320, training loss: 6.944499969482422 = 0.7548902034759521 + 1.0 * 6.189609527587891
Epoch 320, val loss: 0.9543161988258362
Epoch 330, training loss: 6.898223876953125 = 0.7100024223327637 + 1.0 * 6.188221454620361
Epoch 330, val loss: 0.920892059803009
Epoch 340, training loss: 6.8564453125 = 0.668442964553833 + 1.0 * 6.188002109527588
Epoch 340, val loss: 0.8907656669616699
Epoch 350, training loss: 6.807748794555664 = 0.6297025084495544 + 1.0 * 6.178046226501465
Epoch 350, val loss: 0.8635905385017395
Epoch 360, training loss: 6.76532506942749 = 0.5929745435714722 + 1.0 * 6.1723504066467285
Epoch 360, val loss: 0.8389652371406555
Epoch 370, training loss: 6.7265825271606445 = 0.5578492283821106 + 1.0 * 6.1687331199646
Epoch 370, val loss: 0.8164248466491699
Epoch 380, training loss: 6.697271347045898 = 0.5244486331939697 + 1.0 * 6.172822952270508
Epoch 380, val loss: 0.7960794568061829
Epoch 390, training loss: 6.654521942138672 = 0.492963969707489 + 1.0 * 6.161558151245117
Epoch 390, val loss: 0.7777727246284485
Epoch 400, training loss: 6.621483325958252 = 0.46292635798454285 + 1.0 * 6.158556938171387
Epoch 400, val loss: 0.7613722085952759
Epoch 410, training loss: 6.58910608291626 = 0.434208482503891 + 1.0 * 6.154897689819336
Epoch 410, val loss: 0.7466869950294495
Epoch 420, training loss: 6.568147659301758 = 0.40697142481803894 + 1.0 * 6.1611762046813965
Epoch 420, val loss: 0.7336117625236511
Epoch 430, training loss: 6.532669544219971 = 0.3813644051551819 + 1.0 * 6.151305198669434
Epoch 430, val loss: 0.7223199605941772
Epoch 440, training loss: 6.503399848937988 = 0.3572472035884857 + 1.0 * 6.146152496337891
Epoch 440, val loss: 0.7127097249031067
Epoch 450, training loss: 6.490324974060059 = 0.33451321721076965 + 1.0 * 6.155811786651611
Epoch 450, val loss: 0.70455402135849
Epoch 460, training loss: 6.457198619842529 = 0.3133798837661743 + 1.0 * 6.1438188552856445
Epoch 460, val loss: 0.6977530121803284
Epoch 470, training loss: 6.432036399841309 = 0.293510377407074 + 1.0 * 6.13852596282959
Epoch 470, val loss: 0.6922373175621033
Epoch 480, training loss: 6.4115471839904785 = 0.27478140592575073 + 1.0 * 6.136765956878662
Epoch 480, val loss: 0.6879110336303711
Epoch 490, training loss: 6.3960041999816895 = 0.2573137879371643 + 1.0 * 6.13869047164917
Epoch 490, val loss: 0.6845335960388184
Epoch 500, training loss: 6.373464107513428 = 0.24101033806800842 + 1.0 * 6.132453918457031
Epoch 500, val loss: 0.6821143627166748
Epoch 510, training loss: 6.358846187591553 = 0.22579339146614075 + 1.0 * 6.133052825927734
Epoch 510, val loss: 0.6806203126907349
Epoch 520, training loss: 6.339164733886719 = 0.21161207556724548 + 1.0 * 6.127552509307861
Epoch 520, val loss: 0.6799489855766296
Epoch 530, training loss: 6.322629928588867 = 0.19837264716625214 + 1.0 * 6.1242570877075195
Epoch 530, val loss: 0.6799497604370117
Epoch 540, training loss: 6.312922954559326 = 0.18601496517658234 + 1.0 * 6.126907825469971
Epoch 540, val loss: 0.680515706539154
Epoch 550, training loss: 6.302032947540283 = 0.1745912879705429 + 1.0 * 6.127441883087158
Epoch 550, val loss: 0.6815782785415649
Epoch 560, training loss: 6.2829790115356445 = 0.1641257405281067 + 1.0 * 6.1188530921936035
Epoch 560, val loss: 0.6832103133201599
Epoch 570, training loss: 6.271248817443848 = 0.1544567495584488 + 1.0 * 6.11679220199585
Epoch 570, val loss: 0.6853846311569214
Epoch 580, training loss: 6.259616851806641 = 0.145475372672081 + 1.0 * 6.114141464233398
Epoch 580, val loss: 0.6879463195800781
Epoch 590, training loss: 6.261291980743408 = 0.1371472030878067 + 1.0 * 6.124144554138184
Epoch 590, val loss: 0.6908561587333679
Epoch 600, training loss: 6.243405342102051 = 0.1294941008090973 + 1.0 * 6.113911151885986
Epoch 600, val loss: 0.693976104259491
Epoch 610, training loss: 6.231780529022217 = 0.12239224463701248 + 1.0 * 6.10938835144043
Epoch 610, val loss: 0.6974380016326904
Epoch 620, training loss: 6.227050304412842 = 0.11575747281312943 + 1.0 * 6.111292839050293
Epoch 620, val loss: 0.7011570930480957
Epoch 630, training loss: 6.220334053039551 = 0.10956056416034698 + 1.0 * 6.11077356338501
Epoch 630, val loss: 0.7050113081932068
Epoch 640, training loss: 6.207831859588623 = 0.10380192846059799 + 1.0 * 6.104030132293701
Epoch 640, val loss: 0.7089528441429138
Epoch 650, training loss: 6.205484867095947 = 0.09838743507862091 + 1.0 * 6.107097625732422
Epoch 650, val loss: 0.7131484150886536
Epoch 660, training loss: 6.199857711791992 = 0.09329072386026382 + 1.0 * 6.106566905975342
Epoch 660, val loss: 0.7174397110939026
Epoch 670, training loss: 6.188724994659424 = 0.08851869404315948 + 1.0 * 6.10020637512207
Epoch 670, val loss: 0.7218896746635437
Epoch 680, training loss: 6.183718204498291 = 0.08400136232376099 + 1.0 * 6.099716663360596
Epoch 680, val loss: 0.7266169190406799
Epoch 690, training loss: 6.178384304046631 = 0.07974286377429962 + 1.0 * 6.098641395568848
Epoch 690, val loss: 0.7313604950904846
Epoch 700, training loss: 6.175881862640381 = 0.07572649419307709 + 1.0 * 6.100155353546143
Epoch 700, val loss: 0.7362490296363831
Epoch 710, training loss: 6.167942047119141 = 0.07196047157049179 + 1.0 * 6.095981597900391
Epoch 710, val loss: 0.741248607635498
Epoch 720, training loss: 6.161227226257324 = 0.06842907518148422 + 1.0 * 6.092798233032227
Epoch 720, val loss: 0.7465164065361023
Epoch 730, training loss: 6.1576690673828125 = 0.0651160255074501 + 1.0 * 6.09255313873291
Epoch 730, val loss: 0.7520157694816589
Epoch 740, training loss: 6.161041259765625 = 0.062025491148233414 + 1.0 * 6.099015712738037
Epoch 740, val loss: 0.7577073574066162
Epoch 750, training loss: 6.148128032684326 = 0.059148892760276794 + 1.0 * 6.088979244232178
Epoch 750, val loss: 0.763491153717041
Epoch 760, training loss: 6.144108772277832 = 0.05646664649248123 + 1.0 * 6.087642192840576
Epoch 760, val loss: 0.7694779634475708
Epoch 770, training loss: 6.1398234367370605 = 0.05394955351948738 + 1.0 * 6.085874080657959
Epoch 770, val loss: 0.7755714654922485
Epoch 780, training loss: 6.1464362144470215 = 0.05159498751163483 + 1.0 * 6.094841003417969
Epoch 780, val loss: 0.7818009257316589
Epoch 790, training loss: 6.13687801361084 = 0.049387428909540176 + 1.0 * 6.087490558624268
Epoch 790, val loss: 0.7879067063331604
Epoch 800, training loss: 6.13092565536499 = 0.047331031411886215 + 1.0 * 6.083594799041748
Epoch 800, val loss: 0.794048547744751
Epoch 810, training loss: 6.130777835845947 = 0.04539358243346214 + 1.0 * 6.085384368896484
Epoch 810, val loss: 0.8001668453216553
Epoch 820, training loss: 6.125006675720215 = 0.043568387627601624 + 1.0 * 6.081438064575195
Epoch 820, val loss: 0.80625319480896
Epoch 830, training loss: 6.125200271606445 = 0.041846029460430145 + 1.0 * 6.083354473114014
Epoch 830, val loss: 0.8123567700386047
Epoch 840, training loss: 6.120461463928223 = 0.04021941497921944 + 1.0 * 6.080242156982422
Epoch 840, val loss: 0.8183876872062683
Epoch 850, training loss: 6.117753505706787 = 0.03869294375181198 + 1.0 * 6.0790605545043945
Epoch 850, val loss: 0.8243889212608337
Epoch 860, training loss: 6.115991115570068 = 0.0372471883893013 + 1.0 * 6.078743934631348
Epoch 860, val loss: 0.8303340077400208
Epoch 870, training loss: 6.111109733581543 = 0.035885412245988846 + 1.0 * 6.07522439956665
Epoch 870, val loss: 0.8362427949905396
Epoch 880, training loss: 6.10767126083374 = 0.03459236025810242 + 1.0 * 6.0730791091918945
Epoch 880, val loss: 0.8421991467475891
Epoch 890, training loss: 6.108213424682617 = 0.03336123377084732 + 1.0 * 6.074851989746094
Epoch 890, val loss: 0.8480739593505859
Epoch 900, training loss: 6.105571269989014 = 0.032196931540966034 + 1.0 * 6.073374271392822
Epoch 900, val loss: 0.8538059592247009
Epoch 910, training loss: 6.103409290313721 = 0.03109803795814514 + 1.0 * 6.0723114013671875
Epoch 910, val loss: 0.8594557642936707
Epoch 920, training loss: 6.106154441833496 = 0.030049052089452744 + 1.0 * 6.07610559463501
Epoch 920, val loss: 0.8651151657104492
Epoch 930, training loss: 6.09838342666626 = 0.029057197272777557 + 1.0 * 6.069326400756836
Epoch 930, val loss: 0.8706217408180237
Epoch 940, training loss: 6.096452236175537 = 0.028107769787311554 + 1.0 * 6.068344593048096
Epoch 940, val loss: 0.8761337399482727
Epoch 950, training loss: 6.101207733154297 = 0.027203857898712158 + 1.0 * 6.07400369644165
Epoch 950, val loss: 0.8816077709197998
Epoch 960, training loss: 6.095625877380371 = 0.026347888633608818 + 1.0 * 6.069277763366699
Epoch 960, val loss: 0.8869228363037109
Epoch 970, training loss: 6.092291831970215 = 0.025532543659210205 + 1.0 * 6.06675910949707
Epoch 970, val loss: 0.8922212719917297
Epoch 980, training loss: 6.089413642883301 = 0.024751141667366028 + 1.0 * 6.064662456512451
Epoch 980, val loss: 0.8974908590316772
Epoch 990, training loss: 6.093400955200195 = 0.024005277082324028 + 1.0 * 6.069395542144775
Epoch 990, val loss: 0.9027299880981445
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6089
Flip ASR: 0.5689/225 nodes
The final ASR:0.71218, 0.07604, Accuracy:0.79877, 0.01944
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10624])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.83580, 0.00924
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.326598167419434 = 1.9526997804641724 + 1.0 * 8.37389850616455
Epoch 0, val loss: 1.9480122327804565
Epoch 10, training loss: 10.31557559967041 = 1.9419220685958862 + 1.0 * 8.373653411865234
Epoch 10, val loss: 1.938100814819336
Epoch 20, training loss: 10.300957679748535 = 1.9288171529769897 + 1.0 * 8.372140884399414
Epoch 20, val loss: 1.925673246383667
Epoch 30, training loss: 10.27267837524414 = 1.9108794927597046 + 1.0 * 8.361799240112305
Epoch 30, val loss: 1.9085317850112915
Epoch 40, training loss: 10.185493469238281 = 1.8864169120788574 + 1.0 * 8.299076080322266
Epoch 40, val loss: 1.8858476877212524
Epoch 50, training loss: 9.81141471862793 = 1.8594914674758911 + 1.0 * 7.951923370361328
Epoch 50, val loss: 1.8622636795043945
Epoch 60, training loss: 9.502264976501465 = 1.8342788219451904 + 1.0 * 7.6679863929748535
Epoch 60, val loss: 1.840850830078125
Epoch 70, training loss: 9.06176471710205 = 1.8137681484222412 + 1.0 * 7.247996807098389
Epoch 70, val loss: 1.8229916095733643
Epoch 80, training loss: 8.633170127868652 = 1.7978206872940063 + 1.0 * 6.835349082946777
Epoch 80, val loss: 1.809248447418213
Epoch 90, training loss: 8.442727088928223 = 1.7825846672058105 + 1.0 * 6.660142421722412
Epoch 90, val loss: 1.796082615852356
Epoch 100, training loss: 8.319475173950195 = 1.76385498046875 + 1.0 * 6.555619716644287
Epoch 100, val loss: 1.7804278135299683
Epoch 110, training loss: 8.2382173538208 = 1.7440906763076782 + 1.0 * 6.494126319885254
Epoch 110, val loss: 1.763649582862854
Epoch 120, training loss: 8.165899276733398 = 1.7243685722351074 + 1.0 * 6.441530227661133
Epoch 120, val loss: 1.745928406715393
Epoch 130, training loss: 8.101505279541016 = 1.7032756805419922 + 1.0 * 6.398230075836182
Epoch 130, val loss: 1.7265546321868896
Epoch 140, training loss: 8.042519569396973 = 1.6791980266571045 + 1.0 * 6.363321781158447
Epoch 140, val loss: 1.7048206329345703
Epoch 150, training loss: 7.987710952758789 = 1.651123046875 + 1.0 * 6.336587905883789
Epoch 150, val loss: 1.680021047592163
Epoch 160, training loss: 7.9327616691589355 = 1.6183615922927856 + 1.0 * 6.3144001960754395
Epoch 160, val loss: 1.6513404846191406
Epoch 170, training loss: 7.877833366394043 = 1.581047534942627 + 1.0 * 6.296785831451416
Epoch 170, val loss: 1.6188801527023315
Epoch 180, training loss: 7.820154666900635 = 1.5391477346420288 + 1.0 * 6.281006813049316
Epoch 180, val loss: 1.5823981761932373
Epoch 190, training loss: 7.7613372802734375 = 1.4925463199615479 + 1.0 * 6.268791198730469
Epoch 190, val loss: 1.5420899391174316
Epoch 200, training loss: 7.699413299560547 = 1.4428898096084595 + 1.0 * 6.256523609161377
Epoch 200, val loss: 1.4994792938232422
Epoch 210, training loss: 7.635932445526123 = 1.3906344175338745 + 1.0 * 6.245297908782959
Epoch 210, val loss: 1.4550397396087646
Epoch 220, training loss: 7.573411464691162 = 1.3367295265197754 + 1.0 * 6.236681938171387
Epoch 220, val loss: 1.4095654487609863
Epoch 230, training loss: 7.5100603103637695 = 1.2829654216766357 + 1.0 * 6.227094650268555
Epoch 230, val loss: 1.3651525974273682
Epoch 240, training loss: 7.448134899139404 = 1.229544758796692 + 1.0 * 6.218590259552002
Epoch 240, val loss: 1.3214612007141113
Epoch 250, training loss: 7.387190818786621 = 1.1760927438735962 + 1.0 * 6.2110981941223145
Epoch 250, val loss: 1.2781294584274292
Epoch 260, training loss: 7.333305835723877 = 1.1227394342422485 + 1.0 * 6.210566520690918
Epoch 260, val loss: 1.2353456020355225
Epoch 270, training loss: 7.269383430480957 = 1.0708932876586914 + 1.0 * 6.198490142822266
Epoch 270, val loss: 1.1938713788986206
Epoch 280, training loss: 7.21242618560791 = 1.02016282081604 + 1.0 * 6.192263126373291
Epoch 280, val loss: 1.1535437107086182
Epoch 290, training loss: 7.157894611358643 = 0.9706829190254211 + 1.0 * 6.187211513519287
Epoch 290, val loss: 1.1144498586654663
Epoch 300, training loss: 7.106225967407227 = 0.9233043789863586 + 1.0 * 6.182921409606934
Epoch 300, val loss: 1.0772216320037842
Epoch 310, training loss: 7.055147647857666 = 0.8787549138069153 + 1.0 * 6.176392555236816
Epoch 310, val loss: 1.042414665222168
Epoch 320, training loss: 7.011085033416748 = 0.8370401263237 + 1.0 * 6.174045085906982
Epoch 320, val loss: 1.0100160837173462
Epoch 330, training loss: 6.96661376953125 = 0.7982923984527588 + 1.0 * 6.168321132659912
Epoch 330, val loss: 0.98047935962677
Epoch 340, training loss: 6.9252729415893555 = 0.7624921798706055 + 1.0 * 6.16278076171875
Epoch 340, val loss: 0.9535369873046875
Epoch 350, training loss: 6.889772891998291 = 0.7291244268417358 + 1.0 * 6.160648345947266
Epoch 350, val loss: 0.9291358590126038
Epoch 360, training loss: 6.8525190353393555 = 0.6981966495513916 + 1.0 * 6.154322624206543
Epoch 360, val loss: 0.906919538974762
Epoch 370, training loss: 6.8189802169799805 = 0.6688524484634399 + 1.0 * 6.15012788772583
Epoch 370, val loss: 0.8865292072296143
Epoch 380, training loss: 6.7876176834106445 = 0.6405684947967529 + 1.0 * 6.147049427032471
Epoch 380, val loss: 0.8674564361572266
Epoch 390, training loss: 6.760419845581055 = 0.6131606698036194 + 1.0 * 6.14725923538208
Epoch 390, val loss: 0.8493397831916809
Epoch 400, training loss: 6.729015827178955 = 0.5865117311477661 + 1.0 * 6.1425042152404785
Epoch 400, val loss: 0.832103431224823
Epoch 410, training loss: 6.697831630706787 = 0.5601788759231567 + 1.0 * 6.13765287399292
Epoch 410, val loss: 0.8153178691864014
Epoch 420, training loss: 6.672872543334961 = 0.5339914560317993 + 1.0 * 6.138881206512451
Epoch 420, val loss: 0.7987792491912842
Epoch 430, training loss: 6.6405229568481445 = 0.5079290866851807 + 1.0 * 6.132594108581543
Epoch 430, val loss: 0.7825264930725098
Epoch 440, training loss: 6.613430976867676 = 0.481921911239624 + 1.0 * 6.131508827209473
Epoch 440, val loss: 0.766482949256897
Epoch 450, training loss: 6.585771083831787 = 0.4561005234718323 + 1.0 * 6.1296706199646
Epoch 450, val loss: 0.7506980895996094
Epoch 460, training loss: 6.557270526885986 = 0.430487722158432 + 1.0 * 6.1267828941345215
Epoch 460, val loss: 0.7354164123535156
Epoch 470, training loss: 6.532000541687012 = 0.40530529618263245 + 1.0 * 6.126695156097412
Epoch 470, val loss: 0.7207630276679993
Epoch 480, training loss: 6.500978946685791 = 0.3807888329029083 + 1.0 * 6.120190143585205
Epoch 480, val loss: 0.7071052193641663
Epoch 490, training loss: 6.475327491760254 = 0.3569665551185608 + 1.0 * 6.118360996246338
Epoch 490, val loss: 0.6945902705192566
Epoch 500, training loss: 6.456035614013672 = 0.3339405357837677 + 1.0 * 6.122095108032227
Epoch 500, val loss: 0.683242917060852
Epoch 510, training loss: 6.428465843200684 = 0.3119698166847229 + 1.0 * 6.1164960861206055
Epoch 510, val loss: 0.673250675201416
Epoch 520, training loss: 6.4088592529296875 = 0.2911379337310791 + 1.0 * 6.1177215576171875
Epoch 520, val loss: 0.6646396517753601
Epoch 530, training loss: 6.3836140632629395 = 0.2715698778629303 + 1.0 * 6.112044334411621
Epoch 530, val loss: 0.657416045665741
Epoch 540, training loss: 6.362072944641113 = 0.25305667519569397 + 1.0 * 6.109016418457031
Epoch 540, val loss: 0.6515105366706848
Epoch 550, training loss: 6.346330642700195 = 0.23568731546401978 + 1.0 * 6.11064338684082
Epoch 550, val loss: 0.6468184590339661
Epoch 560, training loss: 6.3375139236450195 = 0.2195730060338974 + 1.0 * 6.117940902709961
Epoch 560, val loss: 0.6432462930679321
Epoch 570, training loss: 6.31094217300415 = 0.20458847284317017 + 1.0 * 6.106353759765625
Epoch 570, val loss: 0.6407638788223267
Epoch 580, training loss: 6.292900562286377 = 0.19071832299232483 + 1.0 * 6.102182388305664
Epoch 580, val loss: 0.6392781138420105
Epoch 590, training loss: 6.283788681030273 = 0.1778142750263214 + 1.0 * 6.105974197387695
Epoch 590, val loss: 0.6386670470237732
Epoch 600, training loss: 6.269418716430664 = 0.16595734655857086 + 1.0 * 6.103461265563965
Epoch 600, val loss: 0.6388295292854309
Epoch 610, training loss: 6.253180980682373 = 0.1550038754940033 + 1.0 * 6.098176956176758
Epoch 610, val loss: 0.6397651433944702
Epoch 620, training loss: 6.2485527992248535 = 0.14486892521381378 + 1.0 * 6.103683948516846
Epoch 620, val loss: 0.641326367855072
Epoch 630, training loss: 6.232306957244873 = 0.1356164962053299 + 1.0 * 6.096690654754639
Epoch 630, val loss: 0.6434741616249084
Epoch 640, training loss: 6.2200493812561035 = 0.12701062858104706 + 1.0 * 6.093038558959961
Epoch 640, val loss: 0.6461527347564697
Epoch 650, training loss: 6.2165398597717285 = 0.11907695233821869 + 1.0 * 6.097463130950928
Epoch 650, val loss: 0.6493315100669861
Epoch 660, training loss: 6.205243110656738 = 0.11177948862314224 + 1.0 * 6.09346342086792
Epoch 660, val loss: 0.652800977230072
Epoch 670, training loss: 6.193897247314453 = 0.10506770014762878 + 1.0 * 6.088829517364502
Epoch 670, val loss: 0.6566515564918518
Epoch 680, training loss: 6.186699867248535 = 0.09884514659643173 + 1.0 * 6.087854862213135
Epoch 680, val loss: 0.6608064770698547
Epoch 690, training loss: 6.186017990112305 = 0.0930991917848587 + 1.0 * 6.092918872833252
Epoch 690, val loss: 0.6652026772499084
Epoch 700, training loss: 6.175009727478027 = 0.08778189867734909 + 1.0 * 6.087227821350098
Epoch 700, val loss: 0.6698001027107239
Epoch 710, training loss: 6.166106700897217 = 0.08286522328853607 + 1.0 * 6.0832414627075195
Epoch 710, val loss: 0.6745821833610535
Epoch 720, training loss: 6.1626410484313965 = 0.07830152660608292 + 1.0 * 6.084339618682861
Epoch 720, val loss: 0.67946857213974
Epoch 730, training loss: 6.158137798309326 = 0.07406451553106308 + 1.0 * 6.084073066711426
Epoch 730, val loss: 0.6844643354415894
Epoch 740, training loss: 6.150420188903809 = 0.07015299052000046 + 1.0 * 6.080267429351807
Epoch 740, val loss: 0.68949955701828
Epoch 750, training loss: 6.14749813079834 = 0.06650730222463608 + 1.0 * 6.080990791320801
Epoch 750, val loss: 0.6945486664772034
Epoch 760, training loss: 6.1411919593811035 = 0.06312000751495361 + 1.0 * 6.0780720710754395
Epoch 760, val loss: 0.6996378302574158
Epoch 770, training loss: 6.1387224197387695 = 0.059967391192913055 + 1.0 * 6.078754901885986
Epoch 770, val loss: 0.7047275304794312
Epoch 780, training loss: 6.1369547843933105 = 0.05702931806445122 + 1.0 * 6.079925537109375
Epoch 780, val loss: 0.7098072171211243
Epoch 790, training loss: 6.130784511566162 = 0.054308827966451645 + 1.0 * 6.076475620269775
Epoch 790, val loss: 0.7148354053497314
Epoch 800, training loss: 6.127280235290527 = 0.05176367610692978 + 1.0 * 6.075516700744629
Epoch 800, val loss: 0.7198237180709839
Epoch 810, training loss: 6.121250152587891 = 0.04939665645360947 + 1.0 * 6.0718536376953125
Epoch 810, val loss: 0.7247995734214783
Epoch 820, training loss: 6.121069431304932 = 0.04718083515763283 + 1.0 * 6.073888778686523
Epoch 820, val loss: 0.7297434210777283
Epoch 830, training loss: 6.116100311279297 = 0.04509942606091499 + 1.0 * 6.071001052856445
Epoch 830, val loss: 0.7346158623695374
Epoch 840, training loss: 6.111608505249023 = 0.043152883648872375 + 1.0 * 6.068455696105957
Epoch 840, val loss: 0.7394788265228271
Epoch 850, training loss: 6.109767913818359 = 0.04132553189992905 + 1.0 * 6.068442344665527
Epoch 850, val loss: 0.7443348169326782
Epoch 860, training loss: 6.123873710632324 = 0.039611004292964935 + 1.0 * 6.084262847900391
Epoch 860, val loss: 0.7491065859794617
Epoch 870, training loss: 6.10495138168335 = 0.03799840807914734 + 1.0 * 6.066953182220459
Epoch 870, val loss: 0.7537161707878113
Epoch 880, training loss: 6.102158546447754 = 0.03649413213133812 + 1.0 * 6.065664291381836
Epoch 880, val loss: 0.7583557963371277
Epoch 890, training loss: 6.099931240081787 = 0.03506827726960182 + 1.0 * 6.0648627281188965
Epoch 890, val loss: 0.7629641890525818
Epoch 900, training loss: 6.1094279289245605 = 0.03371770307421684 + 1.0 * 6.075710296630859
Epoch 900, val loss: 0.7674640417098999
Epoch 910, training loss: 6.094982147216797 = 0.03245769068598747 + 1.0 * 6.062524318695068
Epoch 910, val loss: 0.7719205617904663
Epoch 920, training loss: 6.093577861785889 = 0.03126106411218643 + 1.0 * 6.06231689453125
Epoch 920, val loss: 0.7763949632644653
Epoch 930, training loss: 6.094247341156006 = 0.0301258135586977 + 1.0 * 6.064121723175049
Epoch 930, val loss: 0.7807936072349548
Epoch 940, training loss: 6.092074871063232 = 0.029053453356027603 + 1.0 * 6.063021183013916
Epoch 940, val loss: 0.7850586771965027
Epoch 950, training loss: 6.088766098022461 = 0.02803710661828518 + 1.0 * 6.060729026794434
Epoch 950, val loss: 0.7893597483634949
Epoch 960, training loss: 6.0862226486206055 = 0.02707080729305744 + 1.0 * 6.059151649475098
Epoch 960, val loss: 0.7935888171195984
Epoch 970, training loss: 6.090205192565918 = 0.02615279145538807 + 1.0 * 6.064052581787109
Epoch 970, val loss: 0.7977613210678101
Epoch 980, training loss: 6.08782434463501 = 0.02529086358845234 + 1.0 * 6.062533378601074
Epoch 980, val loss: 0.801832914352417
Epoch 990, training loss: 6.0818772315979 = 0.02446369081735611 + 1.0 * 6.057413578033447
Epoch 990, val loss: 0.8058714866638184
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5867
Flip ASR: 0.5067/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.341776847839355 = 1.9679536819458008 + 1.0 * 8.373823165893555
Epoch 0, val loss: 1.9650180339813232
Epoch 10, training loss: 10.330574989318848 = 1.957335352897644 + 1.0 * 8.373239517211914
Epoch 10, val loss: 1.9544861316680908
Epoch 20, training loss: 10.314051628112793 = 1.9446042776107788 + 1.0 * 8.369447708129883
Epoch 20, val loss: 1.9413225650787354
Epoch 30, training loss: 10.27426815032959 = 1.9273748397827148 + 1.0 * 8.346893310546875
Epoch 30, val loss: 1.9229925870895386
Epoch 40, training loss: 10.06727409362793 = 1.906250238418579 + 1.0 * 8.16102409362793
Epoch 40, val loss: 1.9011874198913574
Epoch 50, training loss: 9.186634063720703 = 1.8853001594543457 + 1.0 * 7.301333427429199
Epoch 50, val loss: 1.880577564239502
Epoch 60, training loss: 8.803862571716309 = 1.8690838813781738 + 1.0 * 6.934778690338135
Epoch 60, val loss: 1.8650354146957397
Epoch 70, training loss: 8.56259822845459 = 1.8540418148040771 + 1.0 * 6.708556175231934
Epoch 70, val loss: 1.8504301309585571
Epoch 80, training loss: 8.432235717773438 = 1.8382744789123535 + 1.0 * 6.593960762023926
Epoch 80, val loss: 1.8350639343261719
Epoch 90, training loss: 8.330583572387695 = 1.8211599588394165 + 1.0 * 6.50942325592041
Epoch 90, val loss: 1.818412184715271
Epoch 100, training loss: 8.252090454101562 = 1.8041998147964478 + 1.0 * 6.447890758514404
Epoch 100, val loss: 1.8017692565917969
Epoch 110, training loss: 8.189251899719238 = 1.787225604057312 + 1.0 * 6.402026653289795
Epoch 110, val loss: 1.7851388454437256
Epoch 120, training loss: 8.13601016998291 = 1.769851803779602 + 1.0 * 6.3661580085754395
Epoch 120, val loss: 1.7681646347045898
Epoch 130, training loss: 8.088842391967773 = 1.7517962455749512 + 1.0 * 6.337046146392822
Epoch 130, val loss: 1.7507771253585815
Epoch 140, training loss: 8.044952392578125 = 1.732525110244751 + 1.0 * 6.312427043914795
Epoch 140, val loss: 1.7326788902282715
Epoch 150, training loss: 8.002897262573242 = 1.7111246585845947 + 1.0 * 6.291772365570068
Epoch 150, val loss: 1.7133865356445312
Epoch 160, training loss: 7.961690902709961 = 1.6870863437652588 + 1.0 * 6.274604320526123
Epoch 160, val loss: 1.6924632787704468
Epoch 170, training loss: 7.918882369995117 = 1.6596052646636963 + 1.0 * 6.259276866912842
Epoch 170, val loss: 1.669346809387207
Epoch 180, training loss: 7.87374210357666 = 1.627280354499817 + 1.0 * 6.246461868286133
Epoch 180, val loss: 1.6428643465042114
Epoch 190, training loss: 7.823760032653809 = 1.589219570159912 + 1.0 * 6.2345404624938965
Epoch 190, val loss: 1.6120986938476562
Epoch 200, training loss: 7.768496513366699 = 1.5441287755966187 + 1.0 * 6.224367618560791
Epoch 200, val loss: 1.5760220289230347
Epoch 210, training loss: 7.710614204406738 = 1.4917333126068115 + 1.0 * 6.218880653381348
Epoch 210, val loss: 1.5343279838562012
Epoch 220, training loss: 7.643570423126221 = 1.4338761568069458 + 1.0 * 6.2096943855285645
Epoch 220, val loss: 1.4886716604232788
Epoch 230, training loss: 7.572261810302734 = 1.371582269668579 + 1.0 * 6.200679779052734
Epoch 230, val loss: 1.4393608570098877
Epoch 240, training loss: 7.500617027282715 = 1.3058744668960571 + 1.0 * 6.194742679595947
Epoch 240, val loss: 1.3875489234924316
Epoch 250, training loss: 7.430858135223389 = 1.2399721145629883 + 1.0 * 6.1908860206604
Epoch 250, val loss: 1.3356961011886597
Epoch 260, training loss: 7.3607892990112305 = 1.175732135772705 + 1.0 * 6.185057163238525
Epoch 260, val loss: 1.2854374647140503
Epoch 270, training loss: 7.292390823364258 = 1.1140447854995728 + 1.0 * 6.178346157073975
Epoch 270, val loss: 1.237202763557434
Epoch 280, training loss: 7.228773593902588 = 1.0548211336135864 + 1.0 * 6.173952579498291
Epoch 280, val loss: 1.1911998987197876
Epoch 290, training loss: 7.167810916900635 = 0.9985479116439819 + 1.0 * 6.169262886047363
Epoch 290, val loss: 1.147676944732666
Epoch 300, training loss: 7.112018585205078 = 0.9452875852584839 + 1.0 * 6.166730880737305
Epoch 300, val loss: 1.1065995693206787
Epoch 310, training loss: 7.05487060546875 = 0.8955919742584229 + 1.0 * 6.159278869628906
Epoch 310, val loss: 1.0682226419448853
Epoch 320, training loss: 7.002925395965576 = 0.8484066724777222 + 1.0 * 6.1545186042785645
Epoch 320, val loss: 1.0319639444351196
Epoch 330, training loss: 6.95343542098999 = 0.8035558462142944 + 1.0 * 6.149879455566406
Epoch 330, val loss: 0.9976227879524231
Epoch 340, training loss: 6.908708095550537 = 0.7614839673042297 + 1.0 * 6.147223949432373
Epoch 340, val loss: 0.9654866456985474
Epoch 350, training loss: 6.868788242340088 = 0.7226113080978394 + 1.0 * 6.146176815032959
Epoch 350, val loss: 0.9361314177513123
Epoch 360, training loss: 6.825751304626465 = 0.6866976022720337 + 1.0 * 6.139053821563721
Epoch 360, val loss: 0.9093204140663147
Epoch 370, training loss: 6.794099807739258 = 0.653444230556488 + 1.0 * 6.140655517578125
Epoch 370, val loss: 0.8849035501480103
Epoch 380, training loss: 6.756906509399414 = 0.6230267882347107 + 1.0 * 6.133879661560059
Epoch 380, val loss: 0.8628642559051514
Epoch 390, training loss: 6.724120140075684 = 0.5946719646453857 + 1.0 * 6.129447937011719
Epoch 390, val loss: 0.8428068161010742
Epoch 400, training loss: 6.693851470947266 = 0.567898690700531 + 1.0 * 6.12595272064209
Epoch 400, val loss: 0.8243625164031982
Epoch 410, training loss: 6.669286251068115 = 0.5426401495933533 + 1.0 * 6.126646041870117
Epoch 410, val loss: 0.807395339012146
Epoch 420, training loss: 6.6393561363220215 = 0.5186852216720581 + 1.0 * 6.120670795440674
Epoch 420, val loss: 0.7917523384094238
Epoch 430, training loss: 6.612236022949219 = 0.4956761598587036 + 1.0 * 6.116559982299805
Epoch 430, val loss: 0.7771702408790588
Epoch 440, training loss: 6.588820934295654 = 0.4735185205936432 + 1.0 * 6.115302562713623
Epoch 440, val loss: 0.7635703086853027
Epoch 450, training loss: 6.573606014251709 = 0.4521602392196655 + 1.0 * 6.121445655822754
Epoch 450, val loss: 0.7509755492210388
Epoch 460, training loss: 6.541515350341797 = 0.4318396747112274 + 1.0 * 6.109675884246826
Epoch 460, val loss: 0.7393378019332886
Epoch 470, training loss: 6.520050525665283 = 0.4122762084007263 + 1.0 * 6.107774257659912
Epoch 470, val loss: 0.7287108302116394
Epoch 480, training loss: 6.497963905334473 = 0.39336955547332764 + 1.0 * 6.1045942306518555
Epoch 480, val loss: 0.7190331220626831
Epoch 490, training loss: 6.486043930053711 = 0.3751186430454254 + 1.0 * 6.110925197601318
Epoch 490, val loss: 0.7102609276771545
Epoch 500, training loss: 6.459451675415039 = 0.3576571047306061 + 1.0 * 6.101794719696045
Epoch 500, val loss: 0.7025371193885803
Epoch 510, training loss: 6.442083358764648 = 0.3410024344921112 + 1.0 * 6.101080894470215
Epoch 510, val loss: 0.6957820057868958
Epoch 520, training loss: 6.424919128417969 = 0.3251183032989502 + 1.0 * 6.099801063537598
Epoch 520, val loss: 0.6900385022163391
Epoch 530, training loss: 6.406456470489502 = 0.30997011065483093 + 1.0 * 6.096486568450928
Epoch 530, val loss: 0.6852443218231201
Epoch 540, training loss: 6.389679431915283 = 0.29550716280937195 + 1.0 * 6.094172477722168
Epoch 540, val loss: 0.6814386248588562
Epoch 550, training loss: 6.37857723236084 = 0.2816881537437439 + 1.0 * 6.096889019012451
Epoch 550, val loss: 0.6784709095954895
Epoch 560, training loss: 6.3624749183654785 = 0.26855581998825073 + 1.0 * 6.093919277191162
Epoch 560, val loss: 0.6763311624526978
Epoch 570, training loss: 6.345758438110352 = 0.2560405433177948 + 1.0 * 6.089717864990234
Epoch 570, val loss: 0.6750025153160095
Epoch 580, training loss: 6.3328070640563965 = 0.2440677136182785 + 1.0 * 6.088739395141602
Epoch 580, val loss: 0.6743924021720886
Epoch 590, training loss: 6.319046974182129 = 0.23264670372009277 + 1.0 * 6.086400508880615
Epoch 590, val loss: 0.6744247078895569
Epoch 600, training loss: 6.3088603019714355 = 0.22174696624279022 + 1.0 * 6.087113380432129
Epoch 600, val loss: 0.6751301884651184
Epoch 610, training loss: 6.297224998474121 = 0.21139346063137054 + 1.0 * 6.085831642150879
Epoch 610, val loss: 0.676426351070404
Epoch 620, training loss: 6.283614635467529 = 0.201525017619133 + 1.0 * 6.082089424133301
Epoch 620, val loss: 0.6783127188682556
Epoch 630, training loss: 6.273688793182373 = 0.19212114810943604 + 1.0 * 6.081567764282227
Epoch 630, val loss: 0.6806874871253967
Epoch 640, training loss: 6.267582416534424 = 0.18318112194538116 + 1.0 * 6.0844011306762695
Epoch 640, val loss: 0.6835855841636658
Epoch 650, training loss: 6.254343509674072 = 0.17462782561779022 + 1.0 * 6.079715728759766
Epoch 650, val loss: 0.6869127154350281
Epoch 660, training loss: 6.243710041046143 = 0.1664794385433197 + 1.0 * 6.077230453491211
Epoch 660, val loss: 0.6906695365905762
Epoch 670, training loss: 6.244328498840332 = 0.15869148075580597 + 1.0 * 6.085637092590332
Epoch 670, val loss: 0.6948171854019165
Epoch 680, training loss: 6.22524881362915 = 0.15129490196704865 + 1.0 * 6.073954105377197
Epoch 680, val loss: 0.6992474794387817
Epoch 690, training loss: 6.217207908630371 = 0.14419549703598022 + 1.0 * 6.073012351989746
Epoch 690, val loss: 0.7039974927902222
Epoch 700, training loss: 6.211031913757324 = 0.13735434412956238 + 1.0 * 6.0736775398254395
Epoch 700, val loss: 0.7091265916824341
Epoch 710, training loss: 6.2007880210876465 = 0.1308470368385315 + 1.0 * 6.06994104385376
Epoch 710, val loss: 0.7144980430603027
Epoch 720, training loss: 6.195565700531006 = 0.12459192425012589 + 1.0 * 6.070973873138428
Epoch 720, val loss: 0.7200153470039368
Epoch 730, training loss: 6.194582939147949 = 0.11848778277635574 + 1.0 * 6.076095104217529
Epoch 730, val loss: 0.7257575988769531
Epoch 740, training loss: 6.181090354919434 = 0.11255134642124176 + 1.0 * 6.068539142608643
Epoch 740, val loss: 0.7317016124725342
Epoch 750, training loss: 6.173511505126953 = 0.1066787838935852 + 1.0 * 6.066832542419434
Epoch 750, val loss: 0.7379161715507507
Epoch 760, training loss: 6.169386386871338 = 0.10088615864515305 + 1.0 * 6.06850004196167
Epoch 760, val loss: 0.7442986369132996
Epoch 770, training loss: 6.160057067871094 = 0.0951944887638092 + 1.0 * 6.0648627281188965
Epoch 770, val loss: 0.7507540583610535
Epoch 780, training loss: 6.153547286987305 = 0.08961059898138046 + 1.0 * 6.063936710357666
Epoch 780, val loss: 0.7573158144950867
Epoch 790, training loss: 6.150691032409668 = 0.08418700098991394 + 1.0 * 6.066504001617432
Epoch 790, val loss: 0.7640654444694519
Epoch 800, training loss: 6.14564847946167 = 0.07898294925689697 + 1.0 * 6.0666656494140625
Epoch 800, val loss: 0.770852267742157
Epoch 810, training loss: 6.138082981109619 = 0.07420284301042557 + 1.0 * 6.06387996673584
Epoch 810, val loss: 0.7777381539344788
Epoch 820, training loss: 6.129513740539551 = 0.06991056352853775 + 1.0 * 6.059603214263916
Epoch 820, val loss: 0.7848237156867981
Epoch 830, training loss: 6.1257100105285645 = 0.06605569273233414 + 1.0 * 6.059654235839844
Epoch 830, val loss: 0.7921901345252991
Epoch 840, training loss: 6.127377986907959 = 0.0625852644443512 + 1.0 * 6.064792633056641
Epoch 840, val loss: 0.7996622323989868
Epoch 850, training loss: 6.117607593536377 = 0.05946371704339981 + 1.0 * 6.0581440925598145
Epoch 850, val loss: 0.8070695400238037
Epoch 860, training loss: 6.113893985748291 = 0.05659273639321327 + 1.0 * 6.057301044464111
Epoch 860, val loss: 0.8144939541816711
Epoch 870, training loss: 6.114903450012207 = 0.0539475679397583 + 1.0 * 6.060956001281738
Epoch 870, val loss: 0.8219373226165771
Epoch 880, training loss: 6.107449531555176 = 0.051511120051145554 + 1.0 * 6.055938243865967
Epoch 880, val loss: 0.8291904926300049
Epoch 890, training loss: 6.103679656982422 = 0.04924571514129639 + 1.0 * 6.054433822631836
Epoch 890, val loss: 0.8363910913467407
Epoch 900, training loss: 6.106544017791748 = 0.04712923616170883 + 1.0 * 6.059414863586426
Epoch 900, val loss: 0.8435409665107727
Epoch 910, training loss: 6.100639820098877 = 0.04514920711517334 + 1.0 * 6.055490493774414
Epoch 910, val loss: 0.8505910634994507
Epoch 920, training loss: 6.098581790924072 = 0.04329851269721985 + 1.0 * 6.055283069610596
Epoch 920, val loss: 0.8575147986412048
Epoch 930, training loss: 6.093522071838379 = 0.04155831038951874 + 1.0 * 6.051963806152344
Epoch 930, val loss: 0.8643774390220642
Epoch 940, training loss: 6.091280460357666 = 0.03992251679301262 + 1.0 * 6.051357746124268
Epoch 940, val loss: 0.8711832761764526
Epoch 950, training loss: 6.088122367858887 = 0.03837554156780243 + 1.0 * 6.049746990203857
Epoch 950, val loss: 0.8779531717300415
Epoch 960, training loss: 6.093562602996826 = 0.03691747412085533 + 1.0 * 6.056644916534424
Epoch 960, val loss: 0.8846620917320251
Epoch 970, training loss: 6.083029747009277 = 0.035550929605960846 + 1.0 * 6.047478675842285
Epoch 970, val loss: 0.8911625146865845
Epoch 980, training loss: 6.081414222717285 = 0.03425751253962517 + 1.0 * 6.047156810760498
Epoch 980, val loss: 0.8976020216941833
Epoch 990, training loss: 6.083805561065674 = 0.033031560480594635 + 1.0 * 6.050774097442627
Epoch 990, val loss: 0.9040319323539734
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7675
Flip ASR: 0.7289/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.307673454284668 = 1.9340020418167114 + 1.0 * 8.373671531677246
Epoch 0, val loss: 1.9315155744552612
Epoch 10, training loss: 10.293791770935059 = 1.9225504398345947 + 1.0 * 8.371241569519043
Epoch 10, val loss: 1.9173927307128906
Epoch 20, training loss: 10.274053573608398 = 1.9075846672058105 + 1.0 * 8.36646842956543
Epoch 20, val loss: 1.8976705074310303
Epoch 30, training loss: 10.228976249694824 = 1.888020396232605 + 1.0 * 8.34095573425293
Epoch 30, val loss: 1.8727153539657593
Epoch 40, training loss: 9.979583740234375 = 1.8682001829147339 + 1.0 * 8.111383438110352
Epoch 40, val loss: 1.850564956665039
Epoch 50, training loss: 9.314665794372559 = 1.8492568731307983 + 1.0 * 7.465409278869629
Epoch 50, val loss: 1.8309491872787476
Epoch 60, training loss: 9.003096580505371 = 1.832909345626831 + 1.0 * 7.170187473297119
Epoch 60, val loss: 1.8153165578842163
Epoch 70, training loss: 8.743021011352539 = 1.8181148767471313 + 1.0 * 6.924905776977539
Epoch 70, val loss: 1.8014229536056519
Epoch 80, training loss: 8.496100425720215 = 1.806368112564087 + 1.0 * 6.689732551574707
Epoch 80, val loss: 1.7906084060668945
Epoch 90, training loss: 8.352992057800293 = 1.7950458526611328 + 1.0 * 6.55794620513916
Epoch 90, val loss: 1.7797383069992065
Epoch 100, training loss: 8.265018463134766 = 1.7816522121429443 + 1.0 * 6.4833664894104
Epoch 100, val loss: 1.7670332193374634
Epoch 110, training loss: 8.193961143493652 = 1.7670806646347046 + 1.0 * 6.426880359649658
Epoch 110, val loss: 1.7533103227615356
Epoch 120, training loss: 8.13552474975586 = 1.7519469261169434 + 1.0 * 6.383577346801758
Epoch 120, val loss: 1.739170789718628
Epoch 130, training loss: 8.087770462036133 = 1.735851764678955 + 1.0 * 6.351918697357178
Epoch 130, val loss: 1.72447669506073
Epoch 140, training loss: 8.041996955871582 = 1.7179125547409058 + 1.0 * 6.324084281921387
Epoch 140, val loss: 1.7083367109298706
Epoch 150, training loss: 7.998398303985596 = 1.6969938278198242 + 1.0 * 6.3014044761657715
Epoch 150, val loss: 1.6900660991668701
Epoch 160, training loss: 7.9562225341796875 = 1.6722571849822998 + 1.0 * 6.283965110778809
Epoch 160, val loss: 1.6688501834869385
Epoch 170, training loss: 7.909566402435303 = 1.643030047416687 + 1.0 * 6.266536235809326
Epoch 170, val loss: 1.6441792249679565
Epoch 180, training loss: 7.861084938049316 = 1.6084697246551514 + 1.0 * 6.252614974975586
Epoch 180, val loss: 1.6154743432998657
Epoch 190, training loss: 7.807956695556641 = 1.5677835941314697 + 1.0 * 6.240172863006592
Epoch 190, val loss: 1.5820626020431519
Epoch 200, training loss: 7.750693321228027 = 1.5211422443389893 + 1.0 * 6.229550838470459
Epoch 200, val loss: 1.544083833694458
Epoch 210, training loss: 7.688811302185059 = 1.469173789024353 + 1.0 * 6.219637393951416
Epoch 210, val loss: 1.5023921728134155
Epoch 220, training loss: 7.626680374145508 = 1.4126265048980713 + 1.0 * 6.214054107666016
Epoch 220, val loss: 1.4575883150100708
Epoch 230, training loss: 7.558938980102539 = 1.3536347150802612 + 1.0 * 6.205304145812988
Epoch 230, val loss: 1.4119269847869873
Epoch 240, training loss: 7.491519451141357 = 1.2935389280319214 + 1.0 * 6.1979804039001465
Epoch 240, val loss: 1.3660646677017212
Epoch 250, training loss: 7.42482852935791 = 1.233027696609497 + 1.0 * 6.191801071166992
Epoch 250, val loss: 1.320825219154358
Epoch 260, training loss: 7.361383438110352 = 1.1729720830917358 + 1.0 * 6.188411235809326
Epoch 260, val loss: 1.2767235040664673
Epoch 270, training loss: 7.298327922821045 = 1.1152606010437012 + 1.0 * 6.183067321777344
Epoch 270, val loss: 1.234686255455017
Epoch 280, training loss: 7.235967636108398 = 1.058593988418579 + 1.0 * 6.177373886108398
Epoch 280, val loss: 1.1939408779144287
Epoch 290, training loss: 7.175045490264893 = 1.0028080940246582 + 1.0 * 6.172237396240234
Epoch 290, val loss: 1.1539674997329712
Epoch 300, training loss: 7.131872653961182 = 0.9484245181083679 + 1.0 * 6.183448314666748
Epoch 300, val loss: 1.1152058839797974
Epoch 310, training loss: 7.06125020980835 = 0.8973051309585571 + 1.0 * 6.163945198059082
Epoch 310, val loss: 1.0791352987289429
Epoch 320, training loss: 7.008647918701172 = 0.8493821620941162 + 1.0 * 6.159265518188477
Epoch 320, val loss: 1.0458201169967651
Epoch 330, training loss: 6.96069860458374 = 0.8045883178710938 + 1.0 * 6.1561102867126465
Epoch 330, val loss: 1.0156036615371704
Epoch 340, training loss: 6.920499801635742 = 0.76348477602005 + 1.0 * 6.157014846801758
Epoch 340, val loss: 0.9884567856788635
Epoch 350, training loss: 6.875626564025879 = 0.7262526154518127 + 1.0 * 6.149374008178711
Epoch 350, val loss: 0.965006947517395
Epoch 360, training loss: 6.836617469787598 = 0.6923264861106873 + 1.0 * 6.144290924072266
Epoch 360, val loss: 0.944737434387207
Epoch 370, training loss: 6.810882568359375 = 0.6611800193786621 + 1.0 * 6.149702548980713
Epoch 370, val loss: 0.9272111654281616
Epoch 380, training loss: 6.7733049392700195 = 0.6328102350234985 + 1.0 * 6.1404948234558105
Epoch 380, val loss: 0.9121330976486206
Epoch 390, training loss: 6.742169380187988 = 0.606611967086792 + 1.0 * 6.135557651519775
Epoch 390, val loss: 0.899181067943573
Epoch 400, training loss: 6.714794158935547 = 0.5822721123695374 + 1.0 * 6.132522106170654
Epoch 400, val loss: 0.8879314661026001
Epoch 410, training loss: 6.68765926361084 = 0.5593150854110718 + 1.0 * 6.1283440589904785
Epoch 410, val loss: 0.8780574202537537
Epoch 420, training loss: 6.665465354919434 = 0.5376296639442444 + 1.0 * 6.127835750579834
Epoch 420, val loss: 0.8691794276237488
Epoch 430, training loss: 6.642396450042725 = 0.5171249508857727 + 1.0 * 6.125271320343018
Epoch 430, val loss: 0.8615222573280334
Epoch 440, training loss: 6.617563724517822 = 0.4974571466445923 + 1.0 * 6.1201066970825195
Epoch 440, val loss: 0.8546290397644043
Epoch 450, training loss: 6.596062660217285 = 0.47839847207069397 + 1.0 * 6.117664337158203
Epoch 450, val loss: 0.8483413457870483
Epoch 460, training loss: 6.57607364654541 = 0.4599916934967041 + 1.0 * 6.116081714630127
Epoch 460, val loss: 0.8425527811050415
Epoch 470, training loss: 6.555821418762207 = 0.4422358274459839 + 1.0 * 6.113585472106934
Epoch 470, val loss: 0.8374453783035278
Epoch 480, training loss: 6.53603458404541 = 0.42490363121032715 + 1.0 * 6.111131191253662
Epoch 480, val loss: 0.8326624035835266
Epoch 490, training loss: 6.518448829650879 = 0.4079684317111969 + 1.0 * 6.110480308532715
Epoch 490, val loss: 0.8282124996185303
Epoch 500, training loss: 6.500566482543945 = 0.391483873128891 + 1.0 * 6.1090826988220215
Epoch 500, val loss: 0.824135959148407
Epoch 510, training loss: 6.48067045211792 = 0.37532520294189453 + 1.0 * 6.105345249176025
Epoch 510, val loss: 0.8204514980316162
Epoch 520, training loss: 6.47314453125 = 0.35950252413749695 + 1.0 * 6.11364221572876
Epoch 520, val loss: 0.81702721118927
Epoch 530, training loss: 6.446712493896484 = 0.3439743220806122 + 1.0 * 6.102738380432129
Epoch 530, val loss: 0.814079999923706
Epoch 540, training loss: 6.430948257446289 = 0.32867002487182617 + 1.0 * 6.102278232574463
Epoch 540, val loss: 0.8113846182823181
Epoch 550, training loss: 6.411651611328125 = 0.3136604428291321 + 1.0 * 6.097990989685059
Epoch 550, val loss: 0.8089495897293091
Epoch 560, training loss: 6.395407676696777 = 0.2990027964115143 + 1.0 * 6.096405029296875
Epoch 560, val loss: 0.8072689771652222
Epoch 570, training loss: 6.386437892913818 = 0.28474998474121094 + 1.0 * 6.101687908172607
Epoch 570, val loss: 0.8062755465507507
Epoch 580, training loss: 6.3669538497924805 = 0.27103808522224426 + 1.0 * 6.095915794372559
Epoch 580, val loss: 0.806042492389679
Epoch 590, training loss: 6.3509135246276855 = 0.25779780745506287 + 1.0 * 6.09311580657959
Epoch 590, val loss: 0.8064393997192383
Epoch 600, training loss: 6.335446357727051 = 0.24496985971927643 + 1.0 * 6.0904765129089355
Epoch 600, val loss: 0.807453453540802
Epoch 610, training loss: 6.331369400024414 = 0.23258672654628754 + 1.0 * 6.098782539367676
Epoch 610, val loss: 0.8090340495109558
Epoch 620, training loss: 6.315235137939453 = 0.22087949514389038 + 1.0 * 6.094355583190918
Epoch 620, val loss: 0.8111073970794678
Epoch 630, training loss: 6.296749114990234 = 0.20966152846813202 + 1.0 * 6.087087631225586
Epoch 630, val loss: 0.8138390779495239
Epoch 640, training loss: 6.284056186676025 = 0.19893361628055573 + 1.0 * 6.085122585296631
Epoch 640, val loss: 0.8171500563621521
Epoch 650, training loss: 6.272517681121826 = 0.18867672979831696 + 1.0 * 6.083840847015381
Epoch 650, val loss: 0.8209412693977356
Epoch 660, training loss: 6.2636027336120605 = 0.17893141508102417 + 1.0 * 6.084671497344971
Epoch 660, val loss: 0.8252394795417786
Epoch 670, training loss: 6.251549243927002 = 0.16977056860923767 + 1.0 * 6.081778526306152
Epoch 670, val loss: 0.830024003982544
Epoch 680, training loss: 6.240863800048828 = 0.16108135879039764 + 1.0 * 6.079782485961914
Epoch 680, val loss: 0.8353310227394104
Epoch 690, training loss: 6.232291221618652 = 0.1528455913066864 + 1.0 * 6.079445838928223
Epoch 690, val loss: 0.8410777449607849
Epoch 700, training loss: 6.228137493133545 = 0.145076185464859 + 1.0 * 6.083061218261719
Epoch 700, val loss: 0.8471487760543823
Epoch 710, training loss: 6.219870567321777 = 0.13779886066913605 + 1.0 * 6.082071781158447
Epoch 710, val loss: 0.853668749332428
Epoch 720, training loss: 6.207423210144043 = 0.13099759817123413 + 1.0 * 6.076425552368164
Epoch 720, val loss: 0.8605330586433411
Epoch 730, training loss: 6.199773788452148 = 0.12455583363771439 + 1.0 * 6.0752177238464355
Epoch 730, val loss: 0.8677279949188232
Epoch 740, training loss: 6.192525386810303 = 0.11848372966051102 + 1.0 * 6.074041843414307
Epoch 740, val loss: 0.8752331733703613
Epoch 750, training loss: 6.192405700683594 = 0.11276572942733765 + 1.0 * 6.079639911651611
Epoch 750, val loss: 0.8829640746116638
Epoch 760, training loss: 6.180689811706543 = 0.10738012939691544 + 1.0 * 6.073309898376465
Epoch 760, val loss: 0.8909428119659424
Epoch 770, training loss: 6.172842979431152 = 0.10232970863580704 + 1.0 * 6.0705132484436035
Epoch 770, val loss: 0.8991303443908691
Epoch 780, training loss: 6.170148849487305 = 0.09755396842956543 + 1.0 * 6.07259464263916
Epoch 780, val loss: 0.9074869751930237
Epoch 790, training loss: 6.163882255554199 = 0.09306616336107254 + 1.0 * 6.0708160400390625
Epoch 790, val loss: 0.91591477394104
Epoch 800, training loss: 6.157198905944824 = 0.08885163813829422 + 1.0 * 6.068347454071045
Epoch 800, val loss: 0.9245192408561707
Epoch 810, training loss: 6.152642250061035 = 0.08487513661384583 + 1.0 * 6.067767143249512
Epoch 810, val loss: 0.933194100856781
Epoch 820, training loss: 6.157346248626709 = 0.08114487677812576 + 1.0 * 6.076201438903809
Epoch 820, val loss: 0.9418938755989075
Epoch 830, training loss: 6.14533805847168 = 0.07761623710393906 + 1.0 * 6.067721843719482
Epoch 830, val loss: 0.9506389498710632
Epoch 840, training loss: 6.1395955085754395 = 0.07430365681648254 + 1.0 * 6.065291881561279
Epoch 840, val loss: 0.9594215750694275
Epoch 850, training loss: 6.14552640914917 = 0.07117369025945663 + 1.0 * 6.074352741241455
Epoch 850, val loss: 0.9681816697120667
Epoch 860, training loss: 6.134437561035156 = 0.06823249906301498 + 1.0 * 6.066205024719238
Epoch 860, val loss: 0.9768664240837097
Epoch 870, training loss: 6.126760959625244 = 0.06545151025056839 + 1.0 * 6.061309337615967
Epoch 870, val loss: 0.9855844974517822
Epoch 880, training loss: 6.12470817565918 = 0.06281231343746185 + 1.0 * 6.061895847320557
Epoch 880, val loss: 0.9942719340324402
Epoch 890, training loss: 6.124042987823486 = 0.06031762808561325 + 1.0 * 6.063725471496582
Epoch 890, val loss: 1.0028839111328125
Epoch 900, training loss: 6.125238418579102 = 0.057962458580732346 + 1.0 * 6.0672760009765625
Epoch 900, val loss: 1.0114545822143555
Epoch 910, training loss: 6.1154022216796875 = 0.05572965368628502 + 1.0 * 6.0596723556518555
Epoch 910, val loss: 1.0198897123336792
Epoch 920, training loss: 6.112521648406982 = 0.05363333597779274 + 1.0 * 6.0588884353637695
Epoch 920, val loss: 1.0283477306365967
Epoch 930, training loss: 6.107706546783447 = 0.051632486283779144 + 1.0 * 6.056074142456055
Epoch 930, val loss: 1.0367900133132935
Epoch 940, training loss: 6.106982231140137 = 0.04972813278436661 + 1.0 * 6.057254314422607
Epoch 940, val loss: 1.0451568365097046
Epoch 950, training loss: 6.104142189025879 = 0.04792850837111473 + 1.0 * 6.056213855743408
Epoch 950, val loss: 1.0533620119094849
Epoch 960, training loss: 6.101441383361816 = 0.046221956610679626 + 1.0 * 6.055219650268555
Epoch 960, val loss: 1.0615514516830444
Epoch 970, training loss: 6.098949432373047 = 0.044599197804927826 + 1.0 * 6.05435037612915
Epoch 970, val loss: 1.0697749853134155
Epoch 980, training loss: 6.108282089233398 = 0.04305214434862137 + 1.0 * 6.065229892730713
Epoch 980, val loss: 1.0778584480285645
Epoch 990, training loss: 6.099372863769531 = 0.041590962558984756 + 1.0 * 6.05778169631958
Epoch 990, val loss: 1.0857876539230347
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.1771
Flip ASR: 0.2000/225 nodes
The final ASR:0.51046, 0.24699, Accuracy:0.79877, 0.01429
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10604])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98647, 0.00920, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.324507713317871 = 1.950594186782837 + 1.0 * 8.373913764953613
Epoch 0, val loss: 1.9508987665176392
Epoch 10, training loss: 10.313222885131836 = 1.9395852088928223 + 1.0 * 8.373637199401855
Epoch 10, val loss: 1.9402755498886108
Epoch 20, training loss: 10.297139167785645 = 1.9253989458084106 + 1.0 * 8.371740341186523
Epoch 20, val loss: 1.9261529445648193
Epoch 30, training loss: 10.263090133666992 = 1.9049582481384277 + 1.0 * 8.358132362365723
Epoch 30, val loss: 1.905440092086792
Epoch 40, training loss: 10.157135009765625 = 1.8776719570159912 + 1.0 * 8.279462814331055
Epoch 40, val loss: 1.8788845539093018
Epoch 50, training loss: 9.77857780456543 = 1.8491491079330444 + 1.0 * 7.929428577423096
Epoch 50, val loss: 1.8517791032791138
Epoch 60, training loss: 9.472413063049316 = 1.8229941129684448 + 1.0 * 7.649418830871582
Epoch 60, val loss: 1.8278002738952637
Epoch 70, training loss: 9.086568832397461 = 1.8021461963653564 + 1.0 * 7.284422874450684
Epoch 70, val loss: 1.8091907501220703
Epoch 80, training loss: 8.745704650878906 = 1.7864351272583008 + 1.0 * 6.9592695236206055
Epoch 80, val loss: 1.7957175970077515
Epoch 90, training loss: 8.555235862731934 = 1.7691876888275146 + 1.0 * 6.786048412322998
Epoch 90, val loss: 1.7805449962615967
Epoch 100, training loss: 8.420413970947266 = 1.7481698989868164 + 1.0 * 6.672244548797607
Epoch 100, val loss: 1.7625696659088135
Epoch 110, training loss: 8.323211669921875 = 1.7267651557922363 + 1.0 * 6.5964460372924805
Epoch 110, val loss: 1.7443903684616089
Epoch 120, training loss: 8.239608764648438 = 1.703595757484436 + 1.0 * 6.536013126373291
Epoch 120, val loss: 1.7245627641677856
Epoch 130, training loss: 8.163456916809082 = 1.6776396036148071 + 1.0 * 6.485816955566406
Epoch 130, val loss: 1.7026642560958862
Epoch 140, training loss: 8.095757484436035 = 1.6477152109146118 + 1.0 * 6.448042392730713
Epoch 140, val loss: 1.677811861038208
Epoch 150, training loss: 8.031235694885254 = 1.6134088039398193 + 1.0 * 6.4178266525268555
Epoch 150, val loss: 1.6495105028152466
Epoch 160, training loss: 7.962895393371582 = 1.5745424032211304 + 1.0 * 6.388352870941162
Epoch 160, val loss: 1.617675542831421
Epoch 170, training loss: 7.897067070007324 = 1.5314054489135742 + 1.0 * 6.36566162109375
Epoch 170, val loss: 1.5822277069091797
Epoch 180, training loss: 7.829526901245117 = 1.4853779077529907 + 1.0 * 6.344149112701416
Epoch 180, val loss: 1.5447452068328857
Epoch 190, training loss: 7.762090682983398 = 1.4374282360076904 + 1.0 * 6.324662208557129
Epoch 190, val loss: 1.5058833360671997
Epoch 200, training loss: 7.6966047286987305 = 1.3878741264343262 + 1.0 * 6.308730602264404
Epoch 200, val loss: 1.4659521579742432
Epoch 210, training loss: 7.634965419769287 = 1.3381825685501099 + 1.0 * 6.296782970428467
Epoch 210, val loss: 1.426430106163025
Epoch 220, training loss: 7.571493625640869 = 1.2890353202819824 + 1.0 * 6.282458305358887
Epoch 220, val loss: 1.3876980543136597
Epoch 230, training loss: 7.512465000152588 = 1.2406069040298462 + 1.0 * 6.271858215332031
Epoch 230, val loss: 1.350066900253296
Epoch 240, training loss: 7.455117225646973 = 1.1934521198272705 + 1.0 * 6.261665344238281
Epoch 240, val loss: 1.313659429550171
Epoch 250, training loss: 7.398456573486328 = 1.1473064422607422 + 1.0 * 6.251150131225586
Epoch 250, val loss: 1.2784173488616943
Epoch 260, training loss: 7.351171493530273 = 1.1023619174957275 + 1.0 * 6.248809814453125
Epoch 260, val loss: 1.244324803352356
Epoch 270, training loss: 7.296578407287598 = 1.0592873096466064 + 1.0 * 6.23729133605957
Epoch 270, val loss: 1.2117810249328613
Epoch 280, training loss: 7.244474411010742 = 1.0178498029708862 + 1.0 * 6.226624488830566
Epoch 280, val loss: 1.1807352304458618
Epoch 290, training loss: 7.202573776245117 = 0.9777194857597351 + 1.0 * 6.224854469299316
Epoch 290, val loss: 1.1507295370101929
Epoch 300, training loss: 7.152111530303955 = 0.9391484260559082 + 1.0 * 6.212963104248047
Epoch 300, val loss: 1.121964931488037
Epoch 310, training loss: 7.10867166519165 = 0.9016594290733337 + 1.0 * 6.207012176513672
Epoch 310, val loss: 1.0941431522369385
Epoch 320, training loss: 7.070315837860107 = 0.8649172186851501 + 1.0 * 6.2053985595703125
Epoch 320, val loss: 1.0670325756072998
Epoch 330, training loss: 7.025378704071045 = 0.8290228843688965 + 1.0 * 6.196355819702148
Epoch 330, val loss: 1.0406508445739746
Epoch 340, training loss: 6.988173007965088 = 0.7937529683113098 + 1.0 * 6.194419860839844
Epoch 340, val loss: 1.015087604522705
Epoch 350, training loss: 6.946376800537109 = 0.7591689229011536 + 1.0 * 6.1872076988220215
Epoch 350, val loss: 0.9902454614639282
Epoch 360, training loss: 6.916747570037842 = 0.7251724600791931 + 1.0 * 6.191575050354004
Epoch 360, val loss: 0.9662088751792908
Epoch 370, training loss: 6.872276306152344 = 0.6921889185905457 + 1.0 * 6.180087566375732
Epoch 370, val loss: 0.9430652856826782
Epoch 380, training loss: 6.835150241851807 = 0.6600731611251831 + 1.0 * 6.175076961517334
Epoch 380, val loss: 0.9208468794822693
Epoch 390, training loss: 6.799622058868408 = 0.6288700103759766 + 1.0 * 6.170752048492432
Epoch 390, val loss: 0.8996139168739319
Epoch 400, training loss: 6.771929740905762 = 0.5988749861717224 + 1.0 * 6.1730546951293945
Epoch 400, val loss: 0.8796224594116211
Epoch 410, training loss: 6.73784065246582 = 0.5705912709236145 + 1.0 * 6.1672492027282715
Epoch 410, val loss: 0.8611673712730408
Epoch 420, training loss: 6.705454349517822 = 0.5439275503158569 + 1.0 * 6.161526679992676
Epoch 420, val loss: 0.8443771004676819
Epoch 430, training loss: 6.677101135253906 = 0.5188825130462646 + 1.0 * 6.1582183837890625
Epoch 430, val loss: 0.8291874527931213
Epoch 440, training loss: 6.661045074462891 = 0.495334267616272 + 1.0 * 6.165710926055908
Epoch 440, val loss: 0.815526008605957
Epoch 450, training loss: 6.627595901489258 = 0.4734343886375427 + 1.0 * 6.15416145324707
Epoch 450, val loss: 0.8034586310386658
Epoch 460, training loss: 6.603172302246094 = 0.4529273509979248 + 1.0 * 6.150245189666748
Epoch 460, val loss: 0.7928912043571472
Epoch 470, training loss: 6.579913139343262 = 0.4334062337875366 + 1.0 * 6.1465067863464355
Epoch 470, val loss: 0.7834039926528931
Epoch 480, training loss: 6.569180488586426 = 0.41463255882263184 + 1.0 * 6.154547691345215
Epoch 480, val loss: 0.7748117446899414
Epoch 490, training loss: 6.538835048675537 = 0.39641040563583374 + 1.0 * 6.142424583435059
Epoch 490, val loss: 0.766742467880249
Epoch 500, training loss: 6.517814636230469 = 0.37841561436653137 + 1.0 * 6.13939905166626
Epoch 500, val loss: 0.7591813802719116
Epoch 510, training loss: 6.496865749359131 = 0.36036136746406555 + 1.0 * 6.136504173278809
Epoch 510, val loss: 0.7517850399017334
Epoch 520, training loss: 6.479876518249512 = 0.342075914144516 + 1.0 * 6.137800693511963
Epoch 520, val loss: 0.7444468140602112
Epoch 530, training loss: 6.460178852081299 = 0.3236198425292969 + 1.0 * 6.136559009552002
Epoch 530, val loss: 0.737147331237793
Epoch 540, training loss: 6.434937953948975 = 0.3049815595149994 + 1.0 * 6.129956245422363
Epoch 540, val loss: 0.7299226522445679
Epoch 550, training loss: 6.414403438568115 = 0.28610116243362427 + 1.0 * 6.128302097320557
Epoch 550, val loss: 0.7228997349739075
Epoch 560, training loss: 6.392768383026123 = 0.26717886328697205 + 1.0 * 6.125589370727539
Epoch 560, val loss: 0.7161439657211304
Epoch 570, training loss: 6.378178119659424 = 0.24853745102882385 + 1.0 * 6.129640579223633
Epoch 570, val loss: 0.7099676132202148
Epoch 580, training loss: 6.35721492767334 = 0.23054413497447968 + 1.0 * 6.126670837402344
Epoch 580, val loss: 0.7043901681900024
Epoch 590, training loss: 6.334824085235596 = 0.21339775621891022 + 1.0 * 6.121426105499268
Epoch 590, val loss: 0.699748694896698
Epoch 600, training loss: 6.316024303436279 = 0.1972821205854416 + 1.0 * 6.118741989135742
Epoch 600, val loss: 0.696081817150116
Epoch 610, training loss: 6.298203468322754 = 0.182296022772789 + 1.0 * 6.115907669067383
Epoch 610, val loss: 0.6935548186302185
Epoch 620, training loss: 6.296237468719482 = 0.16857585310935974 + 1.0 * 6.12766170501709
Epoch 620, val loss: 0.6921367049217224
Epoch 630, training loss: 6.273312091827393 = 0.15624205768108368 + 1.0 * 6.117070198059082
Epoch 630, val loss: 0.6916624307632446
Epoch 640, training loss: 6.2564287185668945 = 0.14510950446128845 + 1.0 * 6.111319065093994
Epoch 640, val loss: 0.6921897530555725
Epoch 650, training loss: 6.243814945220947 = 0.13503210246562958 + 1.0 * 6.108782768249512
Epoch 650, val loss: 0.6938000917434692
Epoch 660, training loss: 6.235045909881592 = 0.12588949501514435 + 1.0 * 6.109156608581543
Epoch 660, val loss: 0.6961770057678223
Epoch 670, training loss: 6.22495174407959 = 0.11762821674346924 + 1.0 * 6.10732364654541
Epoch 670, val loss: 0.6992138624191284
Epoch 680, training loss: 6.215869426727295 = 0.11016502976417542 + 1.0 * 6.105704307556152
Epoch 680, val loss: 0.7027643918991089
Epoch 690, training loss: 6.214648723602295 = 0.10338319838047028 + 1.0 * 6.111265659332275
Epoch 690, val loss: 0.7069281935691833
Epoch 700, training loss: 6.198720932006836 = 0.09720879793167114 + 1.0 * 6.1015119552612305
Epoch 700, val loss: 0.7114944458007812
Epoch 710, training loss: 6.1944780349731445 = 0.09152717143297195 + 1.0 * 6.1029510498046875
Epoch 710, val loss: 0.7164672017097473
Epoch 720, training loss: 6.183803081512451 = 0.08630479872226715 + 1.0 * 6.097498416900635
Epoch 720, val loss: 0.7216498255729675
Epoch 730, training loss: 6.177882671356201 = 0.08149103075265884 + 1.0 * 6.096391677856445
Epoch 730, val loss: 0.7270939350128174
Epoch 740, training loss: 6.174021244049072 = 0.07702379673719406 + 1.0 * 6.096997261047363
Epoch 740, val loss: 0.732757568359375
Epoch 750, training loss: 6.169201374053955 = 0.07289840281009674 + 1.0 * 6.0963029861450195
Epoch 750, val loss: 0.7385587692260742
Epoch 760, training loss: 6.161855697631836 = 0.0690532848238945 + 1.0 * 6.09280252456665
Epoch 760, val loss: 0.7445106506347656
Epoch 770, training loss: 6.158813953399658 = 0.06547799706459045 + 1.0 * 6.09333610534668
Epoch 770, val loss: 0.7506720423698425
Epoch 780, training loss: 6.151914119720459 = 0.06215095520019531 + 1.0 * 6.089763164520264
Epoch 780, val loss: 0.7569606304168701
Epoch 790, training loss: 6.153603553771973 = 0.05904151499271393 + 1.0 * 6.09456205368042
Epoch 790, val loss: 0.7633247375488281
Epoch 800, training loss: 6.145455837249756 = 0.056150343269109726 + 1.0 * 6.089305400848389
Epoch 800, val loss: 0.7697340250015259
Epoch 810, training loss: 6.140485763549805 = 0.053447458893060684 + 1.0 * 6.087038516998291
Epoch 810, val loss: 0.7761899828910828
Epoch 820, training loss: 6.136086940765381 = 0.050915494561195374 + 1.0 * 6.085171222686768
Epoch 820, val loss: 0.7827795743942261
Epoch 830, training loss: 6.1417622566223145 = 0.048540256917476654 + 1.0 * 6.093222141265869
Epoch 830, val loss: 0.7894155383110046
Epoch 840, training loss: 6.130239486694336 = 0.04633207619190216 + 1.0 * 6.083907604217529
Epoch 840, val loss: 0.7959046363830566
Epoch 850, training loss: 6.12525749206543 = 0.044253963977098465 + 1.0 * 6.081003665924072
Epoch 850, val loss: 0.802647590637207
Epoch 860, training loss: 6.122403621673584 = 0.04230031743645668 + 1.0 * 6.080103397369385
Epoch 860, val loss: 0.8093494772911072
Epoch 870, training loss: 6.1305670738220215 = 0.04046299681067467 + 1.0 * 6.090104103088379
Epoch 870, val loss: 0.8159049153327942
Epoch 880, training loss: 6.117560863494873 = 0.03875274583697319 + 1.0 * 6.078808307647705
Epoch 880, val loss: 0.8223512172698975
Epoch 890, training loss: 6.115254878997803 = 0.03714009374380112 + 1.0 * 6.078114986419678
Epoch 890, val loss: 0.8288828134536743
Epoch 900, training loss: 6.112804889678955 = 0.035619236528873444 + 1.0 * 6.07718563079834
Epoch 900, val loss: 0.8353623151779175
Epoch 910, training loss: 6.115391254425049 = 0.03419094160199165 + 1.0 * 6.081200122833252
Epoch 910, val loss: 0.8417033553123474
Epoch 920, training loss: 6.109038829803467 = 0.032848238945007324 + 1.0 * 6.07619047164917
Epoch 920, val loss: 0.8479701280593872
Epoch 930, training loss: 6.105811595916748 = 0.03157781809568405 + 1.0 * 6.0742340087890625
Epoch 930, val loss: 0.8542664647102356
Epoch 940, training loss: 6.10231351852417 = 0.030382903292775154 + 1.0 * 6.071930408477783
Epoch 940, val loss: 0.860439121723175
Epoch 950, training loss: 6.105027675628662 = 0.02925092726945877 + 1.0 * 6.07577657699585
Epoch 950, val loss: 0.8665419220924377
Epoch 960, training loss: 6.103519916534424 = 0.028184574097394943 + 1.0 * 6.075335502624512
Epoch 960, val loss: 0.872514009475708
Epoch 970, training loss: 6.099478721618652 = 0.02718016505241394 + 1.0 * 6.072298526763916
Epoch 970, val loss: 0.8783829212188721
Epoch 980, training loss: 6.101459980010986 = 0.026234548538923264 + 1.0 * 6.075225353240967
Epoch 980, val loss: 0.8841453790664673
Epoch 990, training loss: 6.094898223876953 = 0.025334831327199936 + 1.0 * 6.069563388824463
Epoch 990, val loss: 0.8898422718048096
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7528
Flip ASR: 0.7022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.299535751342773 = 1.9256788492202759 + 1.0 * 8.373856544494629
Epoch 0, val loss: 1.9249693155288696
Epoch 10, training loss: 10.288483619689941 = 1.9153938293457031 + 1.0 * 8.373089790344238
Epoch 10, val loss: 1.9130864143371582
Epoch 20, training loss: 10.27296257019043 = 1.9026641845703125 + 1.0 * 8.370298385620117
Epoch 20, val loss: 1.8974659442901611
Epoch 30, training loss: 10.2428560256958 = 1.884983777999878 + 1.0 * 8.357872009277344
Epoch 30, val loss: 1.8751704692840576
Epoch 40, training loss: 10.158278465270996 = 1.8617857694625854 + 1.0 * 8.296492576599121
Epoch 40, val loss: 1.8467663526535034
Epoch 50, training loss: 9.8746337890625 = 1.8380496501922607 + 1.0 * 8.03658390045166
Epoch 50, val loss: 1.8192939758300781
Epoch 60, training loss: 9.416312217712402 = 1.8163414001464844 + 1.0 * 7.599970817565918
Epoch 60, val loss: 1.7954338788986206
Epoch 70, training loss: 9.023233413696289 = 1.7972183227539062 + 1.0 * 7.226015090942383
Epoch 70, val loss: 1.775497317314148
Epoch 80, training loss: 8.752164840698242 = 1.7807074785232544 + 1.0 * 6.971457481384277
Epoch 80, val loss: 1.7583500146865845
Epoch 90, training loss: 8.5392484664917 = 1.7651883363723755 + 1.0 * 6.774060249328613
Epoch 90, val loss: 1.7429920434951782
Epoch 100, training loss: 8.403301239013672 = 1.7489436864852905 + 1.0 * 6.65435791015625
Epoch 100, val loss: 1.7275925874710083
Epoch 110, training loss: 8.310050010681152 = 1.7303673028945923 + 1.0 * 6.57968282699585
Epoch 110, val loss: 1.710949420928955
Epoch 120, training loss: 8.231498718261719 = 1.709594488143921 + 1.0 * 6.521903991699219
Epoch 120, val loss: 1.6931002140045166
Epoch 130, training loss: 8.161477088928223 = 1.6859349012374878 + 1.0 * 6.475542068481445
Epoch 130, val loss: 1.6734020709991455
Epoch 140, training loss: 8.095340728759766 = 1.6586484909057617 + 1.0 * 6.436691761016846
Epoch 140, val loss: 1.6511729955673218
Epoch 150, training loss: 8.031603813171387 = 1.6266032457351685 + 1.0 * 6.405000686645508
Epoch 150, val loss: 1.6254281997680664
Epoch 160, training loss: 7.968236446380615 = 1.5889086723327637 + 1.0 * 6.379327774047852
Epoch 160, val loss: 1.5952179431915283
Epoch 170, training loss: 7.903529644012451 = 1.545283317565918 + 1.0 * 6.358246326446533
Epoch 170, val loss: 1.5605309009552002
Epoch 180, training loss: 7.835533142089844 = 1.495790719985962 + 1.0 * 6.339742660522461
Epoch 180, val loss: 1.5211436748504639
Epoch 190, training loss: 7.766746520996094 = 1.4408035278320312 + 1.0 * 6.3259429931640625
Epoch 190, val loss: 1.477713704109192
Epoch 200, training loss: 7.694127082824707 = 1.3817273378372192 + 1.0 * 6.312399864196777
Epoch 200, val loss: 1.4321590662002563
Epoch 210, training loss: 7.620700359344482 = 1.3205901384353638 + 1.0 * 6.300110340118408
Epoch 210, val loss: 1.3855148553848267
Epoch 220, training loss: 7.548433303833008 = 1.2582309246063232 + 1.0 * 6.290202617645264
Epoch 220, val loss: 1.3387359380722046
Epoch 230, training loss: 7.476642608642578 = 1.1962825059890747 + 1.0 * 6.280360221862793
Epoch 230, val loss: 1.2937308549880981
Epoch 240, training loss: 7.40661096572876 = 1.1359777450561523 + 1.0 * 6.270633220672607
Epoch 240, val loss: 1.25053071975708
Epoch 250, training loss: 7.338858604431152 = 1.0772192478179932 + 1.0 * 6.26163911819458
Epoch 250, val loss: 1.2088663578033447
Epoch 260, training loss: 7.280679225921631 = 1.0202484130859375 + 1.0 * 6.260430812835693
Epoch 260, val loss: 1.1689491271972656
Epoch 270, training loss: 7.213878631591797 = 0.967173159122467 + 1.0 * 6.246705532073975
Epoch 270, val loss: 1.1311713457107544
Epoch 280, training loss: 7.154116630554199 = 0.9167535305023193 + 1.0 * 6.237363338470459
Epoch 280, val loss: 1.0954692363739014
Epoch 290, training loss: 7.098753929138184 = 0.8692271709442139 + 1.0 * 6.229526519775391
Epoch 290, val loss: 1.0618990659713745
Epoch 300, training loss: 7.054747581481934 = 0.8246632218360901 + 1.0 * 6.230084419250488
Epoch 300, val loss: 1.0307247638702393
Epoch 310, training loss: 7.001798152923584 = 0.7840641140937805 + 1.0 * 6.217733860015869
Epoch 310, val loss: 1.0023033618927002
Epoch 320, training loss: 6.957997798919678 = 0.7466907501220703 + 1.0 * 6.211307048797607
Epoch 320, val loss: 0.9765455722808838
Epoch 330, training loss: 6.919109344482422 = 0.7122603058815002 + 1.0 * 6.206849098205566
Epoch 330, val loss: 0.9533454179763794
Epoch 340, training loss: 6.879270076751709 = 0.6804308891296387 + 1.0 * 6.19883918762207
Epoch 340, val loss: 0.9323757886886597
Epoch 350, training loss: 6.844147205352783 = 0.6503118276596069 + 1.0 * 6.193835258483887
Epoch 350, val loss: 0.9132155179977417
Epoch 360, training loss: 6.813473701477051 = 0.6214938163757324 + 1.0 * 6.191979885101318
Epoch 360, val loss: 0.8954004645347595
Epoch 370, training loss: 6.781281471252441 = 0.593713104724884 + 1.0 * 6.187568187713623
Epoch 370, val loss: 0.8788411021232605
Epoch 380, training loss: 6.746833324432373 = 0.5665410757064819 + 1.0 * 6.180292129516602
Epoch 380, val loss: 0.8632743954658508
Epoch 390, training loss: 6.716285228729248 = 0.539612352848053 + 1.0 * 6.17667293548584
Epoch 390, val loss: 0.8485624194145203
Epoch 400, training loss: 6.687278747558594 = 0.5131841897964478 + 1.0 * 6.1740946769714355
Epoch 400, val loss: 0.8347543478012085
Epoch 410, training loss: 6.657787799835205 = 0.48760470747947693 + 1.0 * 6.170183181762695
Epoch 410, val loss: 0.822266697883606
Epoch 420, training loss: 6.629508018493652 = 0.46274256706237793 + 1.0 * 6.1667656898498535
Epoch 420, val loss: 0.8109926581382751
Epoch 430, training loss: 6.605689525604248 = 0.43860071897506714 + 1.0 * 6.167088985443115
Epoch 430, val loss: 0.8008160591125488
Epoch 440, training loss: 6.579995632171631 = 0.41542354226112366 + 1.0 * 6.164572238922119
Epoch 440, val loss: 0.7919365763664246
Epoch 450, training loss: 6.550297737121582 = 0.3931778073310852 + 1.0 * 6.1571197509765625
Epoch 450, val loss: 0.7843549847602844
Epoch 460, training loss: 6.527970314025879 = 0.3716447353363037 + 1.0 * 6.156325817108154
Epoch 460, val loss: 0.7779756188392639
Epoch 470, training loss: 6.502788066864014 = 0.350826233625412 + 1.0 * 6.151961803436279
Epoch 470, val loss: 0.7726823687553406
Epoch 480, training loss: 6.486356735229492 = 0.3308628797531128 + 1.0 * 6.15549373626709
Epoch 480, val loss: 0.7685670852661133
Epoch 490, training loss: 6.460852146148682 = 0.3115428686141968 + 1.0 * 6.149309158325195
Epoch 490, val loss: 0.7655107975006104
Epoch 500, training loss: 6.438329696655273 = 0.29284727573394775 + 1.0 * 6.145482540130615
Epoch 500, val loss: 0.7633323669433594
Epoch 510, training loss: 6.420883655548096 = 0.27457427978515625 + 1.0 * 6.1463093757629395
Epoch 510, val loss: 0.7618441581726074
Epoch 520, training loss: 6.399808883666992 = 0.25699859857559204 + 1.0 * 6.142810344696045
Epoch 520, val loss: 0.7610785961151123
Epoch 530, training loss: 6.381470680236816 = 0.240290105342865 + 1.0 * 6.141180515289307
Epoch 530, val loss: 0.7611252069473267
Epoch 540, training loss: 6.360615253448486 = 0.2247149497270584 + 1.0 * 6.135900497436523
Epoch 540, val loss: 0.7621472477912903
Epoch 550, training loss: 6.344266891479492 = 0.2102801501750946 + 1.0 * 6.133986949920654
Epoch 550, val loss: 0.7640997767448425
Epoch 560, training loss: 6.331099987030029 = 0.1969071328639984 + 1.0 * 6.134192943572998
Epoch 560, val loss: 0.7667573094367981
Epoch 570, training loss: 6.3175764083862305 = 0.18461884558200836 + 1.0 * 6.132957458496094
Epoch 570, val loss: 0.7698606252670288
Epoch 580, training loss: 6.303953170776367 = 0.17327465116977692 + 1.0 * 6.130678653717041
Epoch 580, val loss: 0.7734488248825073
Epoch 590, training loss: 6.290640354156494 = 0.16278916597366333 + 1.0 * 6.1278510093688965
Epoch 590, val loss: 0.777367889881134
Epoch 600, training loss: 6.277850151062012 = 0.15315690636634827 + 1.0 * 6.124693393707275
Epoch 600, val loss: 0.7816594839096069
Epoch 610, training loss: 6.268348693847656 = 0.14419147372245789 + 1.0 * 6.124157428741455
Epoch 610, val loss: 0.7861967086791992
Epoch 620, training loss: 6.256767749786377 = 0.13590705394744873 + 1.0 * 6.120860576629639
Epoch 620, val loss: 0.7911012768745422
Epoch 630, training loss: 6.251821041107178 = 0.12823550403118134 + 1.0 * 6.1235857009887695
Epoch 630, val loss: 0.7962422966957092
Epoch 640, training loss: 6.238836288452148 = 0.12113058567047119 + 1.0 * 6.117705821990967
Epoch 640, val loss: 0.8017281293869019
Epoch 650, training loss: 6.230249404907227 = 0.11453469842672348 + 1.0 * 6.1157145500183105
Epoch 650, val loss: 0.8075131177902222
Epoch 660, training loss: 6.2292280197143555 = 0.10837603360414505 + 1.0 * 6.120851993560791
Epoch 660, val loss: 0.8135352730751038
Epoch 670, training loss: 6.218827724456787 = 0.10269834846258163 + 1.0 * 6.116129398345947
Epoch 670, val loss: 0.8196585178375244
Epoch 680, training loss: 6.208047866821289 = 0.0973866656422615 + 1.0 * 6.110661029815674
Epoch 680, val loss: 0.8260939121246338
Epoch 690, training loss: 6.208171367645264 = 0.09243349730968475 + 1.0 * 6.1157379150390625
Epoch 690, val loss: 0.8327149748802185
Epoch 700, training loss: 6.197442054748535 = 0.08783777058124542 + 1.0 * 6.109604358673096
Epoch 700, val loss: 0.8393614292144775
Epoch 710, training loss: 6.1895270347595215 = 0.0835183784365654 + 1.0 * 6.106008529663086
Epoch 710, val loss: 0.8462981581687927
Epoch 720, training loss: 6.202325820922852 = 0.07947008311748505 + 1.0 * 6.1228556632995605
Epoch 720, val loss: 0.8533479571342468
Epoch 730, training loss: 6.179793357849121 = 0.07573886960744858 + 1.0 * 6.1040544509887695
Epoch 730, val loss: 0.860271692276001
Epoch 740, training loss: 6.173314094543457 = 0.07223398983478546 + 1.0 * 6.101079940795898
Epoch 740, val loss: 0.867376983165741
Epoch 750, training loss: 6.168022155761719 = 0.0689375251531601 + 1.0 * 6.099084854125977
Epoch 750, val loss: 0.8746942281723022
Epoch 760, training loss: 6.166021347045898 = 0.06583376228809357 + 1.0 * 6.1001877784729
Epoch 760, val loss: 0.88202965259552
Epoch 770, training loss: 6.163471221923828 = 0.06291905790567398 + 1.0 * 6.100552082061768
Epoch 770, val loss: 0.8892353773117065
Epoch 780, training loss: 6.156339168548584 = 0.06018819659948349 + 1.0 * 6.096150875091553
Epoch 780, val loss: 0.8964914083480835
Epoch 790, training loss: 6.160164833068848 = 0.05761950463056564 + 1.0 * 6.102545261383057
Epoch 790, val loss: 0.9037954807281494
Epoch 800, training loss: 6.1499810218811035 = 0.05520820617675781 + 1.0 * 6.094772815704346
Epoch 800, val loss: 0.9109768867492676
Epoch 810, training loss: 6.145127773284912 = 0.052929025143384933 + 1.0 * 6.092198848724365
Epoch 810, val loss: 0.9182599186897278
Epoch 820, training loss: 6.144595146179199 = 0.05077281966805458 + 1.0 * 6.093822479248047
Epoch 820, val loss: 0.925609290599823
Epoch 830, training loss: 6.141170501708984 = 0.0487484373152256 + 1.0 * 6.092422008514404
Epoch 830, val loss: 0.9327459931373596
Epoch 840, training loss: 6.136294364929199 = 0.04684404283761978 + 1.0 * 6.089450359344482
Epoch 840, val loss: 0.93979412317276
Epoch 850, training loss: 6.132934093475342 = 0.045042287558317184 + 1.0 * 6.087891578674316
Epoch 850, val loss: 0.9470003247261047
Epoch 860, training loss: 6.132422924041748 = 0.04333321377635002 + 1.0 * 6.089089870452881
Epoch 860, val loss: 0.9541919827461243
Epoch 870, training loss: 6.129012107849121 = 0.04171212762594223 + 1.0 * 6.087299823760986
Epoch 870, val loss: 0.9611688852310181
Epoch 880, training loss: 6.125823974609375 = 0.04018155857920647 + 1.0 * 6.085642337799072
Epoch 880, val loss: 0.968200147151947
Epoch 890, training loss: 6.1219162940979 = 0.038725290447473526 + 1.0 * 6.08319091796875
Epoch 890, val loss: 0.9752432107925415
Epoch 900, training loss: 6.125616550445557 = 0.037341196089982986 + 1.0 * 6.08827543258667
Epoch 900, val loss: 0.9822558164596558
Epoch 910, training loss: 6.118099689483643 = 0.036030035465955734 + 1.0 * 6.0820698738098145
Epoch 910, val loss: 0.9890665411949158
Epoch 920, training loss: 6.116407871246338 = 0.034779466688632965 + 1.0 * 6.081628322601318
Epoch 920, val loss: 0.995917558670044
Epoch 930, training loss: 6.115440845489502 = 0.03359375149011612 + 1.0 * 6.081847190856934
Epoch 930, val loss: 1.0026519298553467
Epoch 940, training loss: 6.111673355102539 = 0.032468970865011215 + 1.0 * 6.079204559326172
Epoch 940, val loss: 1.0092982053756714
Epoch 950, training loss: 6.1092424392700195 = 0.03139201179146767 + 1.0 * 6.077850341796875
Epoch 950, val loss: 1.0160290002822876
Epoch 960, training loss: 6.109740257263184 = 0.030370328575372696 + 1.0 * 6.079370021820068
Epoch 960, val loss: 1.022735357284546
Epoch 970, training loss: 6.107572555541992 = 0.029395032674074173 + 1.0 * 6.078177452087402
Epoch 970, val loss: 1.0292109251022339
Epoch 980, training loss: 6.105464935302734 = 0.028469907119870186 + 1.0 * 6.076994895935059
Epoch 980, val loss: 1.0356371402740479
Epoch 990, training loss: 6.102205276489258 = 0.027583230286836624 + 1.0 * 6.07462215423584
Epoch 990, val loss: 1.0421475172042847
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.8044
Flip ASR: 0.7689/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32296085357666 = 1.949082612991333 + 1.0 * 8.373878479003906
Epoch 0, val loss: 1.9555962085723877
Epoch 10, training loss: 10.312664985656738 = 1.9392025470733643 + 1.0 * 8.373462677001953
Epoch 10, val loss: 1.9449853897094727
Epoch 20, training loss: 10.297645568847656 = 1.9269026517868042 + 1.0 * 8.370742797851562
Epoch 20, val loss: 1.9315016269683838
Epoch 30, training loss: 10.266277313232422 = 1.9096636772155762 + 1.0 * 8.356614112854004
Epoch 30, val loss: 1.9123300313949585
Epoch 40, training loss: 10.169078826904297 = 1.886650800704956 + 1.0 * 8.282427787780762
Epoch 40, val loss: 1.8874179124832153
Epoch 50, training loss: 9.708108901977539 = 1.862048864364624 + 1.0 * 7.846059799194336
Epoch 50, val loss: 1.8619130849838257
Epoch 60, training loss: 9.136343002319336 = 1.8409953117370605 + 1.0 * 7.295348167419434
Epoch 60, val loss: 1.8401392698287964
Epoch 70, training loss: 8.753047943115234 = 1.823474645614624 + 1.0 * 6.9295735359191895
Epoch 70, val loss: 1.82119882106781
Epoch 80, training loss: 8.553735733032227 = 1.8046249151229858 + 1.0 * 6.749111175537109
Epoch 80, val loss: 1.8015130758285522
Epoch 90, training loss: 8.41023063659668 = 1.7839818000793457 + 1.0 * 6.626248359680176
Epoch 90, val loss: 1.780671238899231
Epoch 100, training loss: 8.31945514678955 = 1.7631360292434692 + 1.0 * 6.556318759918213
Epoch 100, val loss: 1.7604731321334839
Epoch 110, training loss: 8.24383544921875 = 1.7414973974227905 + 1.0 * 6.50233793258667
Epoch 110, val loss: 1.7405766248703003
Epoch 120, training loss: 8.180182456970215 = 1.7189422845840454 + 1.0 * 6.461240291595459
Epoch 120, val loss: 1.7201788425445557
Epoch 130, training loss: 8.121479034423828 = 1.6948710680007935 + 1.0 * 6.426608085632324
Epoch 130, val loss: 1.6988518238067627
Epoch 140, training loss: 8.069348335266113 = 1.6680512428283691 + 1.0 * 6.401297092437744
Epoch 140, val loss: 1.6760742664337158
Epoch 150, training loss: 8.013425827026367 = 1.637574553489685 + 1.0 * 6.375851631164551
Epoch 150, val loss: 1.6509768962860107
Epoch 160, training loss: 7.956844806671143 = 1.6019834280014038 + 1.0 * 6.354861259460449
Epoch 160, val loss: 1.6222879886627197
Epoch 170, training loss: 7.897734642028809 = 1.5602418184280396 + 1.0 * 6.337492942810059
Epoch 170, val loss: 1.5888805389404297
Epoch 180, training loss: 7.833183288574219 = 1.5117344856262207 + 1.0 * 6.321448802947998
Epoch 180, val loss: 1.5505214929580688
Epoch 190, training loss: 7.763766288757324 = 1.4563517570495605 + 1.0 * 6.307414531707764
Epoch 190, val loss: 1.5066564083099365
Epoch 200, training loss: 7.689646244049072 = 1.3947101831436157 + 1.0 * 6.294936180114746
Epoch 200, val loss: 1.4582334756851196
Epoch 210, training loss: 7.6146745681762695 = 1.329167127609253 + 1.0 * 6.2855072021484375
Epoch 210, val loss: 1.4074090719223022
Epoch 220, training loss: 7.539366245269775 = 1.2630434036254883 + 1.0 * 6.276322841644287
Epoch 220, val loss: 1.3569328784942627
Epoch 230, training loss: 7.4646315574646 = 1.1978654861450195 + 1.0 * 6.26676607131958
Epoch 230, val loss: 1.3079625368118286
Epoch 240, training loss: 7.3991289138793945 = 1.1358568668365479 + 1.0 * 6.263271808624268
Epoch 240, val loss: 1.262710690498352
Epoch 250, training loss: 7.330188751220703 = 1.078747034072876 + 1.0 * 6.251441955566406
Epoch 250, val loss: 1.2216154336929321
Epoch 260, training loss: 7.270089149475098 = 1.025496006011963 + 1.0 * 6.244593143463135
Epoch 260, val loss: 1.1843675374984741
Epoch 270, training loss: 7.212770938873291 = 0.9755495190620422 + 1.0 * 6.2372212409973145
Epoch 270, val loss: 1.1504517793655396
Epoch 280, training loss: 7.162651062011719 = 0.9287773370742798 + 1.0 * 6.2338738441467285
Epoch 280, val loss: 1.1198407411575317
Epoch 290, training loss: 7.111849308013916 = 0.8854624629020691 + 1.0 * 6.226387023925781
Epoch 290, val loss: 1.092232584953308
Epoch 300, training loss: 7.064021110534668 = 0.844207227230072 + 1.0 * 6.219813823699951
Epoch 300, val loss: 1.0668038129806519
Epoch 310, training loss: 7.017637729644775 = 0.8040884137153625 + 1.0 * 6.2135491371154785
Epoch 310, val loss: 1.0429531335830688
Epoch 320, training loss: 6.9846296310424805 = 0.7649391889572144 + 1.0 * 6.219690322875977
Epoch 320, val loss: 1.0204482078552246
Epoch 330, training loss: 6.931715488433838 = 0.7272175550460815 + 1.0 * 6.204497814178467
Epoch 330, val loss: 0.999677300453186
Epoch 340, training loss: 6.890103340148926 = 0.690734326839447 + 1.0 * 6.199368953704834
Epoch 340, val loss: 0.9804456830024719
Epoch 350, training loss: 6.8494110107421875 = 0.6551511883735657 + 1.0 * 6.1942596435546875
Epoch 350, val loss: 0.9625054001808167
Epoch 360, training loss: 6.824305534362793 = 0.6205211877822876 + 1.0 * 6.203784465789795
Epoch 360, val loss: 0.945821225643158
Epoch 370, training loss: 6.7747039794921875 = 0.5877501368522644 + 1.0 * 6.186954021453857
Epoch 370, val loss: 0.9305723905563354
Epoch 380, training loss: 6.739552974700928 = 0.5562201142311096 + 1.0 * 6.183332920074463
Epoch 380, val loss: 0.9166575074195862
Epoch 390, training loss: 6.704452037811279 = 0.5257420539855957 + 1.0 * 6.178709983825684
Epoch 390, val loss: 0.9038777947425842
Epoch 400, training loss: 6.680487632751465 = 0.4964815378189087 + 1.0 * 6.184006214141846
Epoch 400, val loss: 0.8923773765563965
Epoch 410, training loss: 6.641942977905273 = 0.4689604938030243 + 1.0 * 6.172982692718506
Epoch 410, val loss: 0.8825737237930298
Epoch 420, training loss: 6.611752510070801 = 0.44282349944114685 + 1.0 * 6.168929100036621
Epoch 420, val loss: 0.8742654323577881
Epoch 430, training loss: 6.583510398864746 = 0.41796353459358215 + 1.0 * 6.165546894073486
Epoch 430, val loss: 0.8672882914543152
Epoch 440, training loss: 6.560619831085205 = 0.3944329023361206 + 1.0 * 6.166186809539795
Epoch 440, val loss: 0.8615640997886658
Epoch 450, training loss: 6.535244464874268 = 0.3723541498184204 + 1.0 * 6.162890434265137
Epoch 450, val loss: 0.8571516275405884
Epoch 460, training loss: 6.510347366333008 = 0.3515160083770752 + 1.0 * 6.1588311195373535
Epoch 460, val loss: 0.8538888096809387
Epoch 470, training loss: 6.487848281860352 = 0.33183586597442627 + 1.0 * 6.156012535095215
Epoch 470, val loss: 0.8515987992286682
Epoch 480, training loss: 6.466095447540283 = 0.3133024275302887 + 1.0 * 6.152792930603027
Epoch 480, val loss: 0.8502696752548218
Epoch 490, training loss: 6.44689416885376 = 0.29569530487060547 + 1.0 * 6.151198863983154
Epoch 490, val loss: 0.8497292399406433
Epoch 500, training loss: 6.429208755493164 = 0.2790740132331848 + 1.0 * 6.150134563446045
Epoch 500, val loss: 0.8499106764793396
Epoch 510, training loss: 6.41013240814209 = 0.26339906454086304 + 1.0 * 6.146733283996582
Epoch 510, val loss: 0.8507446050643921
Epoch 520, training loss: 6.397977352142334 = 0.24853959679603577 + 1.0 * 6.14943790435791
Epoch 520, val loss: 0.8522207140922546
Epoch 530, training loss: 6.378012657165527 = 0.23449131846427917 + 1.0 * 6.143521308898926
Epoch 530, val loss: 0.8541240692138672
Epoch 540, training loss: 6.3602752685546875 = 0.2212325930595398 + 1.0 * 6.139042854309082
Epoch 540, val loss: 0.8565484285354614
Epoch 550, training loss: 6.354196071624756 = 0.20866696536540985 + 1.0 * 6.145529270172119
Epoch 550, val loss: 0.859281063079834
Epoch 560, training loss: 6.334289073944092 = 0.1969464272260666 + 1.0 * 6.13734245300293
Epoch 560, val loss: 0.8624505996704102
Epoch 570, training loss: 6.318944454193115 = 0.18586547672748566 + 1.0 * 6.1330790519714355
Epoch 570, val loss: 0.866034746170044
Epoch 580, training loss: 6.307142734527588 = 0.17537333071231842 + 1.0 * 6.131769180297852
Epoch 580, val loss: 0.8699389100074768
Epoch 590, training loss: 6.295907020568848 = 0.16549739241600037 + 1.0 * 6.1304097175598145
Epoch 590, val loss: 0.8741801977157593
Epoch 600, training loss: 6.283907413482666 = 0.15623316168785095 + 1.0 * 6.127674102783203
Epoch 600, val loss: 0.878780722618103
Epoch 610, training loss: 6.274074554443359 = 0.14748594164848328 + 1.0 * 6.126588821411133
Epoch 610, val loss: 0.883668839931488
Epoch 620, training loss: 6.270580768585205 = 0.1392432302236557 + 1.0 * 6.131337642669678
Epoch 620, val loss: 0.8888056874275208
Epoch 630, training loss: 6.254962921142578 = 0.1315779834985733 + 1.0 * 6.123384952545166
Epoch 630, val loss: 0.8942499160766602
Epoch 640, training loss: 6.245629787445068 = 0.12437263131141663 + 1.0 * 6.121257305145264
Epoch 640, val loss: 0.9000612497329712
Epoch 650, training loss: 6.238947868347168 = 0.11760389059782028 + 1.0 * 6.121344089508057
Epoch 650, val loss: 0.9061398506164551
Epoch 660, training loss: 6.2319440841674805 = 0.11129354685544968 + 1.0 * 6.120650768280029
Epoch 660, val loss: 0.9125041365623474
Epoch 670, training loss: 6.228183746337891 = 0.1054125726222992 + 1.0 * 6.122771263122559
Epoch 670, val loss: 0.9192265272140503
Epoch 680, training loss: 6.21574068069458 = 0.09991157799959183 + 1.0 * 6.115828990936279
Epoch 680, val loss: 0.9261947870254517
Epoch 690, training loss: 6.2071638107299805 = 0.09478159993886948 + 1.0 * 6.112382411956787
Epoch 690, val loss: 0.9334994554519653
Epoch 700, training loss: 6.204097270965576 = 0.08996759355068207 + 1.0 * 6.114129543304443
Epoch 700, val loss: 0.9409111738204956
Epoch 710, training loss: 6.198657035827637 = 0.08547864854335785 + 1.0 * 6.113178253173828
Epoch 710, val loss: 0.9484684467315674
Epoch 720, training loss: 6.191140651702881 = 0.0813087448477745 + 1.0 * 6.109831809997559
Epoch 720, val loss: 0.9562197327613831
Epoch 730, training loss: 6.1905694007873535 = 0.07739822566509247 + 1.0 * 6.113171100616455
Epoch 730, val loss: 0.9641547203063965
Epoch 740, training loss: 6.180072784423828 = 0.07373299449682236 + 1.0 * 6.106339931488037
Epoch 740, val loss: 0.972078800201416
Epoch 750, training loss: 6.174274444580078 = 0.07029405981302261 + 1.0 * 6.103980541229248
Epoch 750, val loss: 0.9801701307296753
Epoch 760, training loss: 6.170745849609375 = 0.06706050783395767 + 1.0 * 6.10368537902832
Epoch 760, val loss: 0.9883379936218262
Epoch 770, training loss: 6.167912006378174 = 0.06402246654033661 + 1.0 * 6.103889465332031
Epoch 770, val loss: 0.9964962601661682
Epoch 780, training loss: 6.166683673858643 = 0.06118125468492508 + 1.0 * 6.105502605438232
Epoch 780, val loss: 1.0046861171722412
Epoch 790, training loss: 6.163218021392822 = 0.05851532891392708 + 1.0 * 6.104702472686768
Epoch 790, val loss: 1.0128000974655151
Epoch 800, training loss: 6.153675556182861 = 0.05600487068295479 + 1.0 * 6.097670555114746
Epoch 800, val loss: 1.0209290981292725
Epoch 810, training loss: 6.150243759155273 = 0.05363675206899643 + 1.0 * 6.096607208251953
Epoch 810, val loss: 1.0291132926940918
Epoch 820, training loss: 6.153059005737305 = 0.0514017790555954 + 1.0 * 6.101657390594482
Epoch 820, val loss: 1.0372310876846313
Epoch 830, training loss: 6.145068645477295 = 0.04931047931313515 + 1.0 * 6.095757961273193
Epoch 830, val loss: 1.0453922748565674
Epoch 840, training loss: 6.139815330505371 = 0.047319527715444565 + 1.0 * 6.092495918273926
Epoch 840, val loss: 1.0535082817077637
Epoch 850, training loss: 6.148232936859131 = 0.04543829709291458 + 1.0 * 6.102794647216797
Epoch 850, val loss: 1.0615103244781494
Epoch 860, training loss: 6.134862899780273 = 0.04367829114198685 + 1.0 * 6.091184616088867
Epoch 860, val loss: 1.069463849067688
Epoch 870, training loss: 6.132140636444092 = 0.042010724544525146 + 1.0 * 6.090129852294922
Epoch 870, val loss: 1.0774362087249756
Epoch 880, training loss: 6.133857727050781 = 0.04042145982384682 + 1.0 * 6.093436241149902
Epoch 880, val loss: 1.085300087928772
Epoch 890, training loss: 6.1272501945495605 = 0.038933165371418 + 1.0 * 6.088316917419434
Epoch 890, val loss: 1.0930263996124268
Epoch 900, training loss: 6.123884201049805 = 0.037516262382268906 + 1.0 * 6.086368083953857
Epoch 900, val loss: 1.1007767915725708
Epoch 910, training loss: 6.12064790725708 = 0.03617233410477638 + 1.0 * 6.084475517272949
Epoch 910, val loss: 1.1084630489349365
Epoch 920, training loss: 6.1224541664123535 = 0.03489231318235397 + 1.0 * 6.087562084197998
Epoch 920, val loss: 1.116046667098999
Epoch 930, training loss: 6.12143087387085 = 0.03367668017745018 + 1.0 * 6.087754249572754
Epoch 930, val loss: 1.123442530632019
Epoch 940, training loss: 6.116837978363037 = 0.032534316182136536 + 1.0 * 6.084303855895996
Epoch 940, val loss: 1.1308685541152954
Epoch 950, training loss: 6.112072944641113 = 0.031442880630493164 + 1.0 * 6.080630302429199
Epoch 950, val loss: 1.1382086277008057
Epoch 960, training loss: 6.120266914367676 = 0.03040565550327301 + 1.0 * 6.0898613929748535
Epoch 960, val loss: 1.1454665660858154
Epoch 970, training loss: 6.115029811859131 = 0.029414016753435135 + 1.0 * 6.085615634918213
Epoch 970, val loss: 1.1523919105529785
Epoch 980, training loss: 6.109036445617676 = 0.028481725603342056 + 1.0 * 6.080554485321045
Epoch 980, val loss: 1.1594573259353638
Epoch 990, training loss: 6.105597496032715 = 0.027582576498389244 + 1.0 * 6.078014850616455
Epoch 990, val loss: 1.1664314270019531
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.84256, 0.09288, Accuracy:0.79383, 0.01492
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9526])
updated graph: torch.Size([2, 10554])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98032, 0.00870, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.310117721557617 = 1.936327576637268 + 1.0 * 8.37378978729248
Epoch 0, val loss: 1.9310431480407715
Epoch 10, training loss: 10.299759864807129 = 1.926411747932434 + 1.0 * 8.373348236083984
Epoch 10, val loss: 1.92176353931427
Epoch 20, training loss: 10.285008430480957 = 1.914890170097351 + 1.0 * 8.370118141174316
Epoch 20, val loss: 1.9108057022094727
Epoch 30, training loss: 10.242263793945312 = 1.8998098373413086 + 1.0 * 8.342453956604004
Epoch 30, val loss: 1.896484613418579
Epoch 40, training loss: 9.94869327545166 = 1.881945013999939 + 1.0 * 8.06674861907959
Epoch 40, val loss: 1.8798599243164062
Epoch 50, training loss: 8.999048233032227 = 1.8644988536834717 + 1.0 * 7.134549140930176
Epoch 50, val loss: 1.8638168573379517
Epoch 60, training loss: 8.6161527633667 = 1.8525806665420532 + 1.0 * 6.763571739196777
Epoch 60, val loss: 1.8532849550247192
Epoch 70, training loss: 8.423866271972656 = 1.8420438766479492 + 1.0 * 6.581821918487549
Epoch 70, val loss: 1.8438479900360107
Epoch 80, training loss: 8.298065185546875 = 1.8316268920898438 + 1.0 * 6.4664387702941895
Epoch 80, val loss: 1.8345472812652588
Epoch 90, training loss: 8.215484619140625 = 1.8212954998016357 + 1.0 * 6.394189357757568
Epoch 90, val loss: 1.8252886533737183
Epoch 100, training loss: 8.153387069702148 = 1.8111631870269775 + 1.0 * 6.34222412109375
Epoch 100, val loss: 1.816503643989563
Epoch 110, training loss: 8.102045059204102 = 1.80131995677948 + 1.0 * 6.300724983215332
Epoch 110, val loss: 1.8079590797424316
Epoch 120, training loss: 8.060020446777344 = 1.7914543151855469 + 1.0 * 6.268565654754639
Epoch 120, val loss: 1.7993760108947754
Epoch 130, training loss: 8.02334213256836 = 1.7809637784957886 + 1.0 * 6.242378234863281
Epoch 130, val loss: 1.79030179977417
Epoch 140, training loss: 7.9913530349731445 = 1.7693655490875244 + 1.0 * 6.221987247467041
Epoch 140, val loss: 1.7804560661315918
Epoch 150, training loss: 7.959282398223877 = 1.7562909126281738 + 1.0 * 6.202991485595703
Epoch 150, val loss: 1.7695649862289429
Epoch 160, training loss: 7.928831100463867 = 1.7410457134246826 + 1.0 * 6.187785625457764
Epoch 160, val loss: 1.7571007013320923
Epoch 170, training loss: 7.898705959320068 = 1.7228509187698364 + 1.0 * 6.1758551597595215
Epoch 170, val loss: 1.7425156831741333
Epoch 180, training loss: 7.866203308105469 = 1.700751543045044 + 1.0 * 6.165452003479004
Epoch 180, val loss: 1.7250181436538696
Epoch 190, training loss: 7.831106185913086 = 1.6736886501312256 + 1.0 * 6.1574177742004395
Epoch 190, val loss: 1.7035199403762817
Epoch 200, training loss: 7.790033340454102 = 1.6403920650482178 + 1.0 * 6.149641513824463
Epoch 200, val loss: 1.6769896745681763
Epoch 210, training loss: 7.74240779876709 = 1.5991755723953247 + 1.0 * 6.143232345581055
Epoch 210, val loss: 1.6439733505249023
Epoch 220, training loss: 7.688657760620117 = 1.5485889911651611 + 1.0 * 6.140068531036377
Epoch 220, val loss: 1.6035001277923584
Epoch 230, training loss: 7.62159538269043 = 1.4887726306915283 + 1.0 * 6.1328229904174805
Epoch 230, val loss: 1.5558339357376099
Epoch 240, training loss: 7.548558235168457 = 1.4198501110076904 + 1.0 * 6.1287078857421875
Epoch 240, val loss: 1.5008329153060913
Epoch 250, training loss: 7.468409538269043 = 1.3442299365997314 + 1.0 * 6.124179840087891
Epoch 250, val loss: 1.4407583475112915
Epoch 260, training loss: 7.388343334197998 = 1.2666290998458862 + 1.0 * 6.121714115142822
Epoch 260, val loss: 1.3795994520187378
Epoch 270, training loss: 7.307656288146973 = 1.1905688047409058 + 1.0 * 6.117087364196777
Epoch 270, val loss: 1.320137619972229
Epoch 280, training loss: 7.2350263595581055 = 1.118778944015503 + 1.0 * 6.116247177124023
Epoch 280, val loss: 1.2648686170578003
Epoch 290, training loss: 7.163292407989502 = 1.0541495084762573 + 1.0 * 6.109142780303955
Epoch 290, val loss: 1.2157474756240845
Epoch 300, training loss: 7.099429130554199 = 0.995307207107544 + 1.0 * 6.104122161865234
Epoch 300, val loss: 1.1718838214874268
Epoch 310, training loss: 7.042220592498779 = 0.9411125779151917 + 1.0 * 6.101108074188232
Epoch 310, val loss: 1.1321040391921997
Epoch 320, training loss: 6.996682167053223 = 0.8911085724830627 + 1.0 * 6.105573654174805
Epoch 320, val loss: 1.0959620475769043
Epoch 330, training loss: 6.939321517944336 = 0.8447622656822205 + 1.0 * 6.094559192657471
Epoch 330, val loss: 1.0629771947860718
Epoch 340, training loss: 6.89224910736084 = 0.8006049394607544 + 1.0 * 6.091644287109375
Epoch 340, val loss: 1.0320838689804077
Epoch 350, training loss: 6.857174396514893 = 0.7581328749656677 + 1.0 * 6.09904146194458
Epoch 350, val loss: 1.0027393102645874
Epoch 360, training loss: 6.807586193084717 = 0.7178767919540405 + 1.0 * 6.089709281921387
Epoch 360, val loss: 0.9753757119178772
Epoch 370, training loss: 6.763847351074219 = 0.6793162226676941 + 1.0 * 6.084531307220459
Epoch 370, val loss: 0.9498581290245056
Epoch 380, training loss: 6.724569797515869 = 0.6421971321105957 + 1.0 * 6.082372665405273
Epoch 380, val loss: 0.9257888197898865
Epoch 390, training loss: 6.688869476318359 = 0.6067432761192322 + 1.0 * 6.082126140594482
Epoch 390, val loss: 0.9034243822097778
Epoch 400, training loss: 6.651587009429932 = 0.573189377784729 + 1.0 * 6.078397750854492
Epoch 400, val loss: 0.8831006288528442
Epoch 410, training loss: 6.617661476135254 = 0.5410935878753662 + 1.0 * 6.076568126678467
Epoch 410, val loss: 0.8642470240592957
Epoch 420, training loss: 6.585371017456055 = 0.5101004242897034 + 1.0 * 6.075270652770996
Epoch 420, val loss: 0.8466257452964783
Epoch 430, training loss: 6.555083274841309 = 0.48050999641418457 + 1.0 * 6.074573516845703
Epoch 430, val loss: 0.8304125070571899
Epoch 440, training loss: 6.524322032928467 = 0.4523250162601471 + 1.0 * 6.071997165679932
Epoch 440, val loss: 0.8155993223190308
Epoch 450, training loss: 6.507225036621094 = 0.42539674043655396 + 1.0 * 6.0818281173706055
Epoch 450, val loss: 0.8018642663955688
Epoch 460, training loss: 6.469177722930908 = 0.400066077709198 + 1.0 * 6.0691118240356445
Epoch 460, val loss: 0.7893911600112915
Epoch 470, training loss: 6.442470073699951 = 0.37597766518592834 + 1.0 * 6.066492557525635
Epoch 470, val loss: 0.7780786156654358
Epoch 480, training loss: 6.421288967132568 = 0.352981299161911 + 1.0 * 6.068307876586914
Epoch 480, val loss: 0.7676716446876526
Epoch 490, training loss: 6.399763584136963 = 0.3311649560928345 + 1.0 * 6.068598747253418
Epoch 490, val loss: 0.7582486271858215
Epoch 500, training loss: 6.3724188804626465 = 0.31052908301353455 + 1.0 * 6.0618896484375
Epoch 500, val loss: 0.7498394250869751
Epoch 510, training loss: 6.3557820320129395 = 0.2909036874771118 + 1.0 * 6.064878463745117
Epoch 510, val loss: 0.7422685027122498
Epoch 520, training loss: 6.334432125091553 = 0.2723957300186157 + 1.0 * 6.062036514282227
Epoch 520, val loss: 0.7355852127075195
Epoch 530, training loss: 6.312380790710449 = 0.2549917697906494 + 1.0 * 6.057389259338379
Epoch 530, val loss: 0.7297608256340027
Epoch 540, training loss: 6.303819179534912 = 0.2385854721069336 + 1.0 * 6.0652337074279785
Epoch 540, val loss: 0.7247535586357117
Epoch 550, training loss: 6.280086994171143 = 0.22339439392089844 + 1.0 * 6.056692600250244
Epoch 550, val loss: 0.7205803990364075
Epoch 560, training loss: 6.26304292678833 = 0.20928673446178436 + 1.0 * 6.053756237030029
Epoch 560, val loss: 0.7172885537147522
Epoch 570, training loss: 6.249661445617676 = 0.19615569710731506 + 1.0 * 6.053505897521973
Epoch 570, val loss: 0.7147747278213501
Epoch 580, training loss: 6.235239028930664 = 0.18399761617183685 + 1.0 * 6.051241397857666
Epoch 580, val loss: 0.712984561920166
Epoch 590, training loss: 6.228213787078857 = 0.17278024554252625 + 1.0 * 6.055433750152588
Epoch 590, val loss: 0.711894690990448
Epoch 600, training loss: 6.214588642120361 = 0.16250135004520416 + 1.0 * 6.052087306976318
Epoch 600, val loss: 0.7114693522453308
Epoch 610, training loss: 6.200924396514893 = 0.15305310487747192 + 1.0 * 6.047871112823486
Epoch 610, val loss: 0.7117086052894592
Epoch 620, training loss: 6.189774513244629 = 0.1443345546722412 + 1.0 * 6.045440196990967
Epoch 620, val loss: 0.712443470954895
Epoch 630, training loss: 6.180708885192871 = 0.13625241816043854 + 1.0 * 6.044456481933594
Epoch 630, val loss: 0.7137122750282288
Epoch 640, training loss: 6.183780670166016 = 0.12878215312957764 + 1.0 * 6.054998397827148
Epoch 640, val loss: 0.7154102325439453
Epoch 650, training loss: 6.170856952667236 = 0.12194962799549103 + 1.0 * 6.048907279968262
Epoch 650, val loss: 0.7174288034439087
Epoch 660, training loss: 6.15868616104126 = 0.11563415080308914 + 1.0 * 6.0430521965026855
Epoch 660, val loss: 0.7198659181594849
Epoch 670, training loss: 6.151027202606201 = 0.109737329185009 + 1.0 * 6.041289806365967
Epoch 670, val loss: 0.722620964050293
Epoch 680, training loss: 6.144115924835205 = 0.10425626486539841 + 1.0 * 6.039859771728516
Epoch 680, val loss: 0.7256185412406921
Epoch 690, training loss: 6.140118598937988 = 0.09917062520980835 + 1.0 * 6.040947914123535
Epoch 690, val loss: 0.7288880944252014
Epoch 700, training loss: 6.131035804748535 = 0.09440965950489044 + 1.0 * 6.03662633895874
Epoch 700, val loss: 0.7324355840682983
Epoch 710, training loss: 6.126684665679932 = 0.08992879837751389 + 1.0 * 6.0367560386657715
Epoch 710, val loss: 0.7361806631088257
Epoch 720, training loss: 6.1242523193359375 = 0.08571826666593552 + 1.0 * 6.038534164428711
Epoch 720, val loss: 0.7400831580162048
Epoch 730, training loss: 6.11775541305542 = 0.08178529143333435 + 1.0 * 6.035970211029053
Epoch 730, val loss: 0.7441259026527405
Epoch 740, training loss: 6.111781120300293 = 0.07807862013578415 + 1.0 * 6.033702373504639
Epoch 740, val loss: 0.7483083605766296
Epoch 750, training loss: 6.1214985847473145 = 0.07458541542291641 + 1.0 * 6.046913146972656
Epoch 750, val loss: 0.7526033520698547
Epoch 760, training loss: 6.103858470916748 = 0.07131028920412064 + 1.0 * 6.032547950744629
Epoch 760, val loss: 0.7570034265518188
Epoch 770, training loss: 6.10021448135376 = 0.06822404265403748 + 1.0 * 6.0319905281066895
Epoch 770, val loss: 0.761538565158844
Epoch 780, training loss: 6.096257209777832 = 0.06530066579580307 + 1.0 * 6.030956745147705
Epoch 780, val loss: 0.7661055326461792
Epoch 790, training loss: 6.095647811889648 = 0.06253688782453537 + 1.0 * 6.033111095428467
Epoch 790, val loss: 0.7707296013832092
Epoch 800, training loss: 6.092446327209473 = 0.05994521453976631 + 1.0 * 6.032501220703125
Epoch 800, val loss: 0.7753493189811707
Epoch 810, training loss: 6.0861124992370605 = 0.05749904736876488 + 1.0 * 6.028613567352295
Epoch 810, val loss: 0.7800441384315491
Epoch 820, training loss: 6.083553314208984 = 0.055175524204969406 + 1.0 * 6.028378009796143
Epoch 820, val loss: 0.7847772836685181
Epoch 830, training loss: 6.08404016494751 = 0.052969176322221756 + 1.0 * 6.031071186065674
Epoch 830, val loss: 0.7895429134368896
Epoch 840, training loss: 6.086522102355957 = 0.050877783447504044 + 1.0 * 6.03564453125
Epoch 840, val loss: 0.7942753434181213
Epoch 850, training loss: 6.078464508056641 = 0.048911936581134796 + 1.0 * 6.029552459716797
Epoch 850, val loss: 0.7990567684173584
Epoch 860, training loss: 6.071804046630859 = 0.047048598527908325 + 1.0 * 6.024755477905273
Epoch 860, val loss: 0.8038219809532166
Epoch 870, training loss: 6.069934844970703 = 0.04527335986495018 + 1.0 * 6.024661540985107
Epoch 870, val loss: 0.8086103796958923
Epoch 880, training loss: 6.075378894805908 = 0.043586503714323044 + 1.0 * 6.031792163848877
Epoch 880, val loss: 0.8133618831634521
Epoch 890, training loss: 6.065715789794922 = 0.041991546750068665 + 1.0 * 6.02372407913208
Epoch 890, val loss: 0.8181003928184509
Epoch 900, training loss: 6.064883232116699 = 0.04047536104917526 + 1.0 * 6.024407863616943
Epoch 900, val loss: 0.8228457570075989
Epoch 910, training loss: 6.0660929679870605 = 0.039033498615026474 + 1.0 * 6.027059555053711
Epoch 910, val loss: 0.8275466561317444
Epoch 920, training loss: 6.061062812805176 = 0.03766274452209473 + 1.0 * 6.02340030670166
Epoch 920, val loss: 0.832225501537323
Epoch 930, training loss: 6.0625433921813965 = 0.03636109083890915 + 1.0 * 6.026182174682617
Epoch 930, val loss: 0.8368870615959167
Epoch 940, training loss: 6.055628776550293 = 0.03512050583958626 + 1.0 * 6.020508289337158
Epoch 940, val loss: 0.8415412902832031
Epoch 950, training loss: 6.053771495819092 = 0.03394070640206337 + 1.0 * 6.019830703735352
Epoch 950, val loss: 0.8461453318595886
Epoch 960, training loss: 6.053750991821289 = 0.03281245008111 + 1.0 * 6.020938396453857
Epoch 960, val loss: 0.8507400155067444
Epoch 970, training loss: 6.053081035614014 = 0.031735531985759735 + 1.0 * 6.021345615386963
Epoch 970, val loss: 0.8552894592285156
Epoch 980, training loss: 6.048887729644775 = 0.030713440850377083 + 1.0 * 6.018174171447754
Epoch 980, val loss: 0.8598005771636963
Epoch 990, training loss: 6.050328731536865 = 0.029739512130618095 + 1.0 * 6.020589351654053
Epoch 990, val loss: 0.8642491698265076
Epoch 1000, training loss: 6.046806335449219 = 0.028813056647777557 + 1.0 * 6.017993450164795
Epoch 1000, val loss: 0.8686181902885437
Epoch 1010, training loss: 6.044500827789307 = 0.02793094888329506 + 1.0 * 6.016570091247559
Epoch 1010, val loss: 0.8729535937309265
Epoch 1020, training loss: 6.042365074157715 = 0.02708621323108673 + 1.0 * 6.0152788162231445
Epoch 1020, val loss: 0.8773109316825867
Epoch 1030, training loss: 6.041579723358154 = 0.026273882016539574 + 1.0 * 6.015305995941162
Epoch 1030, val loss: 0.8816285729408264
Epoch 1040, training loss: 6.053685188293457 = 0.025494486093521118 + 1.0 * 6.028190612792969
Epoch 1040, val loss: 0.8858690857887268
Epoch 1050, training loss: 6.039665699005127 = 0.024760106578469276 + 1.0 * 6.0149054527282715
Epoch 1050, val loss: 0.8899692893028259
Epoch 1060, training loss: 6.0390520095825195 = 0.024057995527982712 + 1.0 * 6.014994144439697
Epoch 1060, val loss: 0.8941160440444946
Epoch 1070, training loss: 6.037563323974609 = 0.02338082529604435 + 1.0 * 6.0141825675964355
Epoch 1070, val loss: 0.8982461094856262
Epoch 1080, training loss: 6.039952278137207 = 0.02273048274219036 + 1.0 * 6.017221927642822
Epoch 1080, val loss: 0.9022659063339233
Epoch 1090, training loss: 6.034135818481445 = 0.022111106663942337 + 1.0 * 6.012024879455566
Epoch 1090, val loss: 0.9062627553939819
Epoch 1100, training loss: 6.03326940536499 = 0.021515384316444397 + 1.0 * 6.011754035949707
Epoch 1100, val loss: 0.9102720022201538
Epoch 1110, training loss: 6.032440185546875 = 0.02094077505171299 + 1.0 * 6.011499404907227
Epoch 1110, val loss: 0.9142665266990662
Epoch 1120, training loss: 6.042215824127197 = 0.020386746153235435 + 1.0 * 6.021829128265381
Epoch 1120, val loss: 0.9182260632514954
Epoch 1130, training loss: 6.0331878662109375 = 0.019858192652463913 + 1.0 * 6.01332950592041
Epoch 1130, val loss: 0.9220280647277832
Epoch 1140, training loss: 6.034908771514893 = 0.019350340589880943 + 1.0 * 6.015558242797852
Epoch 1140, val loss: 0.9258630871772766
Epoch 1150, training loss: 6.029226303100586 = 0.0188628938049078 + 1.0 * 6.010363578796387
Epoch 1150, val loss: 0.9296887516975403
Epoch 1160, training loss: 6.028406620025635 = 0.01839395985007286 + 1.0 * 6.010012626647949
Epoch 1160, val loss: 0.9334548711776733
Epoch 1170, training loss: 6.026580810546875 = 0.017939303070306778 + 1.0 * 6.008641719818115
Epoch 1170, val loss: 0.937221109867096
Epoch 1180, training loss: 6.029934406280518 = 0.01750030927360058 + 1.0 * 6.012434005737305
Epoch 1180, val loss: 0.9409967660903931
Epoch 1190, training loss: 6.0281982421875 = 0.017077283933758736 + 1.0 * 6.011120796203613
Epoch 1190, val loss: 0.9446691870689392
Epoch 1200, training loss: 6.032109260559082 = 0.016672734171152115 + 1.0 * 6.01543664932251
Epoch 1200, val loss: 0.9482319951057434
Epoch 1210, training loss: 6.029561519622803 = 0.016285602003335953 + 1.0 * 6.013276100158691
Epoch 1210, val loss: 0.9517858624458313
Epoch 1220, training loss: 6.024747371673584 = 0.01591256447136402 + 1.0 * 6.0088348388671875
Epoch 1220, val loss: 0.9553008079528809
Epoch 1230, training loss: 6.022066116333008 = 0.015551642514765263 + 1.0 * 6.006514549255371
Epoch 1230, val loss: 0.9588428735733032
Epoch 1240, training loss: 6.021057605743408 = 0.015200948342680931 + 1.0 * 6.005856513977051
Epoch 1240, val loss: 0.962409257888794
Epoch 1250, training loss: 6.030035495758057 = 0.014860416762530804 + 1.0 * 6.015174865722656
Epoch 1250, val loss: 0.9659371972084045
Epoch 1260, training loss: 6.030182361602783 = 0.01453382521867752 + 1.0 * 6.015648365020752
Epoch 1260, val loss: 0.9692598581314087
Epoch 1270, training loss: 6.020724296569824 = 0.014219694770872593 + 1.0 * 6.006504535675049
Epoch 1270, val loss: 0.972556471824646
Epoch 1280, training loss: 6.018157482147217 = 0.013918829150497913 + 1.0 * 6.004238605499268
Epoch 1280, val loss: 0.9758644104003906
Epoch 1290, training loss: 6.018075942993164 = 0.013626623898744583 + 1.0 * 6.004449367523193
Epoch 1290, val loss: 0.9792036414146423
Epoch 1300, training loss: 6.017141342163086 = 0.013340632431209087 + 1.0 * 6.003800868988037
Epoch 1300, val loss: 0.9825541377067566
Epoch 1310, training loss: 6.019381999969482 = 0.013062234036624432 + 1.0 * 6.006319999694824
Epoch 1310, val loss: 0.9858672022819519
Epoch 1320, training loss: 6.020500659942627 = 0.012793416157364845 + 1.0 * 6.007707118988037
Epoch 1320, val loss: 0.9890859127044678
Epoch 1330, training loss: 6.01601505279541 = 0.012535064481198788 + 1.0 * 6.003479957580566
Epoch 1330, val loss: 0.9921867251396179
Epoch 1340, training loss: 6.0166144371032715 = 0.012285743840038776 + 1.0 * 6.004328727722168
Epoch 1340, val loss: 0.9953434467315674
Epoch 1350, training loss: 6.015890121459961 = 0.012043310329318047 + 1.0 * 6.003846645355225
Epoch 1350, val loss: 0.9985228180885315
Epoch 1360, training loss: 6.020726203918457 = 0.011807452887296677 + 1.0 * 6.008918762207031
Epoch 1360, val loss: 1.0016459226608276
Epoch 1370, training loss: 6.016444206237793 = 0.01157884206622839 + 1.0 * 6.0048651695251465
Epoch 1370, val loss: 1.0046857595443726
Epoch 1380, training loss: 6.014270782470703 = 0.011357254348695278 + 1.0 * 6.002913475036621
Epoch 1380, val loss: 1.0077720880508423
Epoch 1390, training loss: 6.016220569610596 = 0.011141415685415268 + 1.0 * 6.00507926940918
Epoch 1390, val loss: 1.0108392238616943
Epoch 1400, training loss: 6.016512393951416 = 0.010931695811450481 + 1.0 * 6.005580902099609
Epoch 1400, val loss: 1.0138673782348633
Epoch 1410, training loss: 6.012962818145752 = 0.010729925706982613 + 1.0 * 6.002233028411865
Epoch 1410, val loss: 1.0167877674102783
Epoch 1420, training loss: 6.013224124908447 = 0.010533847846090794 + 1.0 * 6.002690315246582
Epoch 1420, val loss: 1.0197738409042358
Epoch 1430, training loss: 6.011064529418945 = 0.010342312976717949 + 1.0 * 6.000722408294678
Epoch 1430, val loss: 1.0227417945861816
Epoch 1440, training loss: 6.014222621917725 = 0.010155821219086647 + 1.0 * 6.0040669441223145
Epoch 1440, val loss: 1.0256582498550415
Epoch 1450, training loss: 6.010657787322998 = 0.009974823333323002 + 1.0 * 6.000682830810547
Epoch 1450, val loss: 1.028523564338684
Epoch 1460, training loss: 6.012467861175537 = 0.009798724204301834 + 1.0 * 6.002669334411621
Epoch 1460, val loss: 1.0313951969146729
Epoch 1470, training loss: 6.012296199798584 = 0.009627425111830235 + 1.0 * 6.002668857574463
Epoch 1470, val loss: 1.0342035293579102
Epoch 1480, training loss: 6.0090556144714355 = 0.00946220476180315 + 1.0 * 5.999593257904053
Epoch 1480, val loss: 1.0370234251022339
Epoch 1490, training loss: 6.00855016708374 = 0.009301208890974522 + 1.0 * 5.99924898147583
Epoch 1490, val loss: 1.0398398637771606
Epoch 1500, training loss: 6.015493869781494 = 0.00914379395544529 + 1.0 * 6.006350040435791
Epoch 1500, val loss: 1.0426145792007446
Epoch 1510, training loss: 6.009106636047363 = 0.00899009220302105 + 1.0 * 6.000116348266602
Epoch 1510, val loss: 1.045328140258789
Epoch 1520, training loss: 6.016079425811768 = 0.008841054514050484 + 1.0 * 6.007238388061523
Epoch 1520, val loss: 1.0480475425720215
Epoch 1530, training loss: 6.008241176605225 = 0.008696570061147213 + 1.0 * 5.999544620513916
Epoch 1530, val loss: 1.0507054328918457
Epoch 1540, training loss: 6.006752014160156 = 0.008556162938475609 + 1.0 * 5.998195648193359
Epoch 1540, val loss: 1.0533971786499023
Epoch 1550, training loss: 6.005306243896484 = 0.008417900651693344 + 1.0 * 5.996888160705566
Epoch 1550, val loss: 1.0560954809188843
Epoch 1560, training loss: 6.014071941375732 = 0.008282351307570934 + 1.0 * 6.005789756774902
Epoch 1560, val loss: 1.0588140487670898
Epoch 1570, training loss: 6.008194446563721 = 0.008150944486260414 + 1.0 * 6.0000433921813965
Epoch 1570, val loss: 1.0613582134246826
Epoch 1580, training loss: 6.005864143371582 = 0.00802378449589014 + 1.0 * 5.997840404510498
Epoch 1580, val loss: 1.0639234781265259
Epoch 1590, training loss: 6.0138044357299805 = 0.007900247350335121 + 1.0 * 6.005904197692871
Epoch 1590, val loss: 1.0665013790130615
Epoch 1600, training loss: 6.00681209564209 = 0.007779187988489866 + 1.0 * 5.999032974243164
Epoch 1600, val loss: 1.0689843893051147
Epoch 1610, training loss: 6.003969192504883 = 0.007661723531782627 + 1.0 * 5.996307373046875
Epoch 1610, val loss: 1.0714792013168335
Epoch 1620, training loss: 6.0020575523376465 = 0.0075468579307198524 + 1.0 * 5.994510650634766
Epoch 1620, val loss: 1.0740087032318115
Epoch 1630, training loss: 6.002743721008301 = 0.007433399558067322 + 1.0 * 5.995310306549072
Epoch 1630, val loss: 1.0765687227249146
Epoch 1640, training loss: 6.010111331939697 = 0.007321981713175774 + 1.0 * 6.002789497375488
Epoch 1640, val loss: 1.0791035890579224
Epoch 1650, training loss: 6.008849143981934 = 0.007213701028376818 + 1.0 * 6.001635551452637
Epoch 1650, val loss: 1.081435203552246
Epoch 1660, training loss: 6.002753734588623 = 0.007109245751053095 + 1.0 * 5.995644569396973
Epoch 1660, val loss: 1.0838841199874878
Epoch 1670, training loss: 6.001903057098389 = 0.0070068552158772945 + 1.0 * 5.994896411895752
Epoch 1670, val loss: 1.0863077640533447
Epoch 1680, training loss: 6.011800765991211 = 0.006906022317707539 + 1.0 * 6.004894733428955
Epoch 1680, val loss: 1.0887560844421387
Epoch 1690, training loss: 6.003830909729004 = 0.006808866281062365 + 1.0 * 5.9970221519470215
Epoch 1690, val loss: 1.0910077095031738
Epoch 1700, training loss: 6.000695705413818 = 0.006713672541081905 + 1.0 * 5.993981838226318
Epoch 1700, val loss: 1.093381404876709
Epoch 1710, training loss: 6.000150203704834 = 0.00662060035392642 + 1.0 * 5.993529796600342
Epoch 1710, val loss: 1.0957602262496948
Epoch 1720, training loss: 6.00248384475708 = 0.0065284171141684055 + 1.0 * 5.995955467224121
Epoch 1720, val loss: 1.098122239112854
Epoch 1730, training loss: 6.00053071975708 = 0.00643796706572175 + 1.0 * 5.99409294128418
Epoch 1730, val loss: 1.1004414558410645
Epoch 1740, training loss: 6.003129005432129 = 0.006349886767566204 + 1.0 * 5.996778964996338
Epoch 1740, val loss: 1.1027499437332153
Epoch 1750, training loss: 6.0032572746276855 = 0.0062637790106236935 + 1.0 * 5.996993541717529
Epoch 1750, val loss: 1.1050128936767578
Epoch 1760, training loss: 5.998466491699219 = 0.006180195603519678 + 1.0 * 5.992286205291748
Epoch 1760, val loss: 1.1072392463684082
Epoch 1770, training loss: 5.997442722320557 = 0.006098288577049971 + 1.0 * 5.991344451904297
Epoch 1770, val loss: 1.1095023155212402
Epoch 1780, training loss: 5.999404430389404 = 0.00601744232699275 + 1.0 * 5.993387222290039
Epoch 1780, val loss: 1.1118055582046509
Epoch 1790, training loss: 6.011078834533691 = 0.0059380605816841125 + 1.0 * 6.005140781402588
Epoch 1790, val loss: 1.113931655883789
Epoch 1800, training loss: 5.998810768127441 = 0.0058615365996956825 + 1.0 * 5.99294900894165
Epoch 1800, val loss: 1.1160422563552856
Epoch 1810, training loss: 5.9965739250183105 = 0.005787341855466366 + 1.0 * 5.990786552429199
Epoch 1810, val loss: 1.1181596517562866
Epoch 1820, training loss: 5.997504711151123 = 0.005714086350053549 + 1.0 * 5.991790771484375
Epoch 1820, val loss: 1.1203855276107788
Epoch 1830, training loss: 6.005420684814453 = 0.005641513504087925 + 1.0 * 5.999779224395752
Epoch 1830, val loss: 1.122522234916687
Epoch 1840, training loss: 5.998899459838867 = 0.005570105742663145 + 1.0 * 5.9933295249938965
Epoch 1840, val loss: 1.1246201992034912
Epoch 1850, training loss: 5.995718955993652 = 0.005501075182110071 + 1.0 * 5.990217685699463
Epoch 1850, val loss: 1.126733422279358
Epoch 1860, training loss: 5.99472713470459 = 0.005432453937828541 + 1.0 * 5.989294528961182
Epoch 1860, val loss: 1.1289269924163818
Epoch 1870, training loss: 6.003781795501709 = 0.0053646438755095005 + 1.0 * 5.998417377471924
Epoch 1870, val loss: 1.131037950515747
Epoch 1880, training loss: 5.998528480529785 = 0.005298306699842215 + 1.0 * 5.99323034286499
Epoch 1880, val loss: 1.133116602897644
Epoch 1890, training loss: 5.999073028564453 = 0.005234825890511274 + 1.0 * 5.993838310241699
Epoch 1890, val loss: 1.1351025104522705
Epoch 1900, training loss: 5.994515895843506 = 0.005172455217689276 + 1.0 * 5.989343643188477
Epoch 1900, val loss: 1.1371636390686035
Epoch 1910, training loss: 5.994510173797607 = 0.005110968369990587 + 1.0 * 5.989399433135986
Epoch 1910, val loss: 1.1392515897750854
Epoch 1920, training loss: 5.997859954833984 = 0.005049745086580515 + 1.0 * 5.992810249328613
Epoch 1920, val loss: 1.1413393020629883
Epoch 1930, training loss: 5.999650955200195 = 0.004989494103938341 + 1.0 * 5.994661331176758
Epoch 1930, val loss: 1.1433414220809937
Epoch 1940, training loss: 5.994073867797852 = 0.004931272007524967 + 1.0 * 5.989142417907715
Epoch 1940, val loss: 1.1452451944351196
Epoch 1950, training loss: 5.995064735412598 = 0.0048741670325398445 + 1.0 * 5.990190505981445
Epoch 1950, val loss: 1.1472615003585815
Epoch 1960, training loss: 5.995491981506348 = 0.004818195942789316 + 1.0 * 5.990674018859863
Epoch 1960, val loss: 1.1492412090301514
Epoch 1970, training loss: 5.991905212402344 = 0.004763199016451836 + 1.0 * 5.987142086029053
Epoch 1970, val loss: 1.151222586631775
Epoch 1980, training loss: 5.992758274078369 = 0.004708658903837204 + 1.0 * 5.988049507141113
Epoch 1980, val loss: 1.1532334089279175
Epoch 1990, training loss: 5.999640464782715 = 0.004654819145798683 + 1.0 * 5.994985580444336
Epoch 1990, val loss: 1.1552035808563232
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7269
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.323341369628906 = 1.9495198726654053 + 1.0 * 8.373821258544922
Epoch 0, val loss: 1.9545936584472656
Epoch 10, training loss: 10.312431335449219 = 1.9392664432525635 + 1.0 * 8.373165130615234
Epoch 10, val loss: 1.9447414875030518
Epoch 20, training loss: 10.29481315612793 = 1.926377773284912 + 1.0 * 8.36843490600586
Epoch 20, val loss: 1.931802749633789
Epoch 30, training loss: 10.248443603515625 = 1.9088197946548462 + 1.0 * 8.33962345123291
Epoch 30, val loss: 1.9137762784957886
Epoch 40, training loss: 10.05409049987793 = 1.8866333961486816 + 1.0 * 8.167457580566406
Epoch 40, val loss: 1.89150071144104
Epoch 50, training loss: 9.385849952697754 = 1.8620930910110474 + 1.0 * 7.523756980895996
Epoch 50, val loss: 1.866706132888794
Epoch 60, training loss: 8.971083641052246 = 1.842028021812439 + 1.0 * 7.129055500030518
Epoch 60, val loss: 1.8458952903747559
Epoch 70, training loss: 8.694059371948242 = 1.8256069421768188 + 1.0 * 6.868452072143555
Epoch 70, val loss: 1.8294621706008911
Epoch 80, training loss: 8.44301700592041 = 1.813144326210022 + 1.0 * 6.629872798919678
Epoch 80, val loss: 1.8175039291381836
Epoch 90, training loss: 8.31624984741211 = 1.8014171123504639 + 1.0 * 6.514832973480225
Epoch 90, val loss: 1.8056082725524902
Epoch 100, training loss: 8.225361824035645 = 1.7866809368133545 + 1.0 * 6.438681125640869
Epoch 100, val loss: 1.7898361682891846
Epoch 110, training loss: 8.161544799804688 = 1.7707172632217407 + 1.0 * 6.390827655792236
Epoch 110, val loss: 1.773206114768982
Epoch 120, training loss: 8.112090110778809 = 1.7543398141860962 + 1.0 * 6.357750415802002
Epoch 120, val loss: 1.7564719915390015
Epoch 130, training loss: 8.066828727722168 = 1.7362643480300903 + 1.0 * 6.330564498901367
Epoch 130, val loss: 1.7388442754745483
Epoch 140, training loss: 8.020191192626953 = 1.7153701782226562 + 1.0 * 6.304821491241455
Epoch 140, val loss: 1.7193031311035156
Epoch 150, training loss: 7.972747325897217 = 1.6906661987304688 + 1.0 * 6.282081127166748
Epoch 150, val loss: 1.6977624893188477
Epoch 160, training loss: 7.921811103820801 = 1.6608316898345947 + 1.0 * 6.260979652404785
Epoch 160, val loss: 1.6725391149520874
Epoch 170, training loss: 7.865530014038086 = 1.6238420009613037 + 1.0 * 6.241688251495361
Epoch 170, val loss: 1.6417847871780396
Epoch 180, training loss: 7.805261135101318 = 1.5791500806808472 + 1.0 * 6.226110935211182
Epoch 180, val loss: 1.605180025100708
Epoch 190, training loss: 7.7391815185546875 = 1.5277050733566284 + 1.0 * 6.2114763259887695
Epoch 190, val loss: 1.5632436275482178
Epoch 200, training loss: 7.671850681304932 = 1.4714956283569336 + 1.0 * 6.200355052947998
Epoch 200, val loss: 1.5178147554397583
Epoch 210, training loss: 7.604106903076172 = 1.414672613143921 + 1.0 * 6.189434051513672
Epoch 210, val loss: 1.4720981121063232
Epoch 220, training loss: 7.54001522064209 = 1.3602969646453857 + 1.0 * 6.179718494415283
Epoch 220, val loss: 1.4293146133422852
Epoch 230, training loss: 7.487687110900879 = 1.311372995376587 + 1.0 * 6.176314353942871
Epoch 230, val loss: 1.391724705696106
Epoch 240, training loss: 7.434597015380859 = 1.2702481746673584 + 1.0 * 6.164348602294922
Epoch 240, val loss: 1.3612253665924072
Epoch 250, training loss: 7.39014196395874 = 1.2348989248275757 + 1.0 * 6.155242919921875
Epoch 250, val loss: 1.3358491659164429
Epoch 260, training loss: 7.35259485244751 = 1.2035518884658813 + 1.0 * 6.149043083190918
Epoch 260, val loss: 1.3138748407363892
Epoch 270, training loss: 7.3174872398376465 = 1.1751049757003784 + 1.0 * 6.1423821449279785
Epoch 270, val loss: 1.294897437095642
Epoch 280, training loss: 7.283111095428467 = 1.1483603715896606 + 1.0 * 6.134750843048096
Epoch 280, val loss: 1.2772969007492065
Epoch 290, training loss: 7.250758171081543 = 1.121777057647705 + 1.0 * 6.128981113433838
Epoch 290, val loss: 1.2600436210632324
Epoch 300, training loss: 7.218300819396973 = 1.0944011211395264 + 1.0 * 6.123899459838867
Epoch 300, val loss: 1.2422326803207397
Epoch 310, training loss: 7.188114643096924 = 1.0656061172485352 + 1.0 * 6.122508525848389
Epoch 310, val loss: 1.2233996391296387
Epoch 320, training loss: 7.152271270751953 = 1.0352883338928223 + 1.0 * 6.116982936859131
Epoch 320, val loss: 1.2031043767929077
Epoch 330, training loss: 7.115045547485352 = 1.0026743412017822 + 1.0 * 6.11237096786499
Epoch 330, val loss: 1.1809425354003906
Epoch 340, training loss: 7.076318740844727 = 0.9673658609390259 + 1.0 * 6.10895299911499
Epoch 340, val loss: 1.1567209959030151
Epoch 350, training loss: 7.0367045402526855 = 0.9295285940170288 + 1.0 * 6.107175827026367
Epoch 350, val loss: 1.1304657459259033
Epoch 360, training loss: 6.993049621582031 = 0.8898837566375732 + 1.0 * 6.103165626525879
Epoch 360, val loss: 1.1027734279632568
Epoch 370, training loss: 6.957505226135254 = 0.8488669991493225 + 1.0 * 6.108638286590576
Epoch 370, val loss: 1.073927640914917
Epoch 380, training loss: 6.906089782714844 = 0.8078516721725464 + 1.0 * 6.098237991333008
Epoch 380, val loss: 1.0447649955749512
Epoch 390, training loss: 6.862349033355713 = 0.7670417428016663 + 1.0 * 6.095307350158691
Epoch 390, val loss: 1.0158209800720215
Epoch 400, training loss: 6.8198933601379395 = 0.7268303632736206 + 1.0 * 6.093062877655029
Epoch 400, val loss: 0.9872466921806335
Epoch 410, training loss: 6.788051128387451 = 0.6878020167350769 + 1.0 * 6.100249290466309
Epoch 410, val loss: 0.9596899151802063
Epoch 420, training loss: 6.740775108337402 = 0.6511816382408142 + 1.0 * 6.089593410491943
Epoch 420, val loss: 0.9340469241142273
Epoch 430, training loss: 6.703550815582275 = 0.6166066527366638 + 1.0 * 6.086944103240967
Epoch 430, val loss: 0.9104903340339661
Epoch 440, training loss: 6.66851806640625 = 0.583966851234436 + 1.0 * 6.0845513343811035
Epoch 440, val loss: 0.888882577419281
Epoch 450, training loss: 6.6416473388671875 = 0.5534436106681824 + 1.0 * 6.0882039070129395
Epoch 450, val loss: 0.8694520592689514
Epoch 460, training loss: 6.606571674346924 = 0.5252032279968262 + 1.0 * 6.081368446350098
Epoch 460, val loss: 0.8523660898208618
Epoch 470, training loss: 6.5770487785339355 = 0.49868902564048767 + 1.0 * 6.078359603881836
Epoch 470, val loss: 0.8372564315795898
Epoch 480, training loss: 6.553956985473633 = 0.47367528080940247 + 1.0 * 6.080281734466553
Epoch 480, val loss: 0.8238201141357422
Epoch 490, training loss: 6.527839183807373 = 0.4500572979450226 + 1.0 * 6.077781677246094
Epoch 490, val loss: 0.8119259476661682
Epoch 500, training loss: 6.500849723815918 = 0.4276546835899353 + 1.0 * 6.073194980621338
Epoch 500, val loss: 0.801520586013794
Epoch 510, training loss: 6.478213787078857 = 0.40615206956863403 + 1.0 * 6.072061538696289
Epoch 510, val loss: 0.792147159576416
Epoch 520, training loss: 6.4608964920043945 = 0.3854382038116455 + 1.0 * 6.07545804977417
Epoch 520, val loss: 0.7837298512458801
Epoch 530, training loss: 6.4360198974609375 = 0.36561867594718933 + 1.0 * 6.070401191711426
Epoch 530, val loss: 0.7760921120643616
Epoch 540, training loss: 6.41429328918457 = 0.346687912940979 + 1.0 * 6.067605495452881
Epoch 540, val loss: 0.769392192363739
Epoch 550, training loss: 6.401495456695557 = 0.32843053340911865 + 1.0 * 6.073064804077148
Epoch 550, val loss: 0.763218104839325
Epoch 560, training loss: 6.376249313354492 = 0.3109171390533447 + 1.0 * 6.065331935882568
Epoch 560, val loss: 0.7577164769172668
Epoch 570, training loss: 6.357131004333496 = 0.29405340552330017 + 1.0 * 6.063077449798584
Epoch 570, val loss: 0.7528294324874878
Epoch 580, training loss: 6.347247123718262 = 0.27785295248031616 + 1.0 * 6.069394111633301
Epoch 580, val loss: 0.7484491467475891
Epoch 590, training loss: 6.326791763305664 = 0.26254263520240784 + 1.0 * 6.064249038696289
Epoch 590, val loss: 0.744543731212616
Epoch 600, training loss: 6.308867454528809 = 0.24807879328727722 + 1.0 * 6.060788631439209
Epoch 600, val loss: 0.7413978576660156
Epoch 610, training loss: 6.2926106452941895 = 0.23435670137405396 + 1.0 * 6.058253765106201
Epoch 610, val loss: 0.7387417554855347
Epoch 620, training loss: 6.281052112579346 = 0.22138802707195282 + 1.0 * 6.059664249420166
Epoch 620, val loss: 0.7366097569465637
Epoch 630, training loss: 6.275956153869629 = 0.20929548144340515 + 1.0 * 6.0666608810424805
Epoch 630, val loss: 0.734900712966919
Epoch 640, training loss: 6.255409240722656 = 0.19803838431835175 + 1.0 * 6.057370662689209
Epoch 640, val loss: 0.7338747978210449
Epoch 650, training loss: 6.241707801818848 = 0.1874704509973526 + 1.0 * 6.054237365722656
Epoch 650, val loss: 0.7333576679229736
Epoch 660, training loss: 6.237025737762451 = 0.17753629386425018 + 1.0 * 6.0594892501831055
Epoch 660, val loss: 0.7332188487052917
Epoch 670, training loss: 6.223300457000732 = 0.168257936835289 + 1.0 * 6.055042743682861
Epoch 670, val loss: 0.7334948778152466
Epoch 680, training loss: 6.210450172424316 = 0.15957526862621307 + 1.0 * 6.050874710083008
Epoch 680, val loss: 0.7342007160186768
Epoch 690, training loss: 6.201098442077637 = 0.15140579640865326 + 1.0 * 6.049692630767822
Epoch 690, val loss: 0.7352778315544128
Epoch 700, training loss: 6.205494403839111 = 0.14372920989990234 + 1.0 * 6.061765193939209
Epoch 700, val loss: 0.7366194725036621
Epoch 710, training loss: 6.186373233795166 = 0.1366073340177536 + 1.0 * 6.0497660636901855
Epoch 710, val loss: 0.7382314205169678
Epoch 720, training loss: 6.17710018157959 = 0.1299363225698471 + 1.0 * 6.047163963317871
Epoch 720, val loss: 0.7402627468109131
Epoch 730, training loss: 6.169888973236084 = 0.1236179918050766 + 1.0 * 6.046270847320557
Epoch 730, val loss: 0.7424818873405457
Epoch 740, training loss: 6.162754535675049 = 0.11761920154094696 + 1.0 * 6.045135498046875
Epoch 740, val loss: 0.7450157403945923
Epoch 750, training loss: 6.163168430328369 = 0.11193389445543289 + 1.0 * 6.051234722137451
Epoch 750, val loss: 0.7477885484695435
Epoch 760, training loss: 6.150623798370361 = 0.10656612366437912 + 1.0 * 6.044057846069336
Epoch 760, val loss: 0.7506760358810425
Epoch 770, training loss: 6.160759449005127 = 0.10148275643587112 + 1.0 * 6.059276580810547
Epoch 770, val loss: 0.7537187933921814
Epoch 780, training loss: 6.140752792358398 = 0.09670809656381607 + 1.0 * 6.044044494628906
Epoch 780, val loss: 0.7568598985671997
Epoch 790, training loss: 6.1337480545043945 = 0.09219502657651901 + 1.0 * 6.041553020477295
Epoch 790, val loss: 0.7602754235267639
Epoch 800, training loss: 6.127455234527588 = 0.08790253847837448 + 1.0 * 6.039552688598633
Epoch 800, val loss: 0.7638903856277466
Epoch 810, training loss: 6.122443199157715 = 0.0838160365819931 + 1.0 * 6.0386271476745605
Epoch 810, val loss: 0.767699658870697
Epoch 820, training loss: 6.128687381744385 = 0.07992898672819138 + 1.0 * 6.048758506774902
Epoch 820, val loss: 0.7716922760009766
Epoch 830, training loss: 6.118801593780518 = 0.07626853138208389 + 1.0 * 6.042532920837402
Epoch 830, val loss: 0.7756329774856567
Epoch 840, training loss: 6.1111674308776855 = 0.07283339649438858 + 1.0 * 6.038333892822266
Epoch 840, val loss: 0.7797586917877197
Epoch 850, training loss: 6.107231616973877 = 0.06958844512701035 + 1.0 * 6.037642955780029
Epoch 850, val loss: 0.7839542627334595
Epoch 860, training loss: 6.101866245269775 = 0.06652364879846573 + 1.0 * 6.035342693328857
Epoch 860, val loss: 0.7881680130958557
Epoch 870, training loss: 6.109040260314941 = 0.06363969296216965 + 1.0 * 6.045400619506836
Epoch 870, val loss: 0.792495846748352
Epoch 880, training loss: 6.097278118133545 = 0.06092769652605057 + 1.0 * 6.036350250244141
Epoch 880, val loss: 0.7968662977218628
Epoch 890, training loss: 6.092304229736328 = 0.05836515128612518 + 1.0 * 6.033938884735107
Epoch 890, val loss: 0.8013579845428467
Epoch 900, training loss: 6.088011264801025 = 0.05592576414346695 + 1.0 * 6.032085418701172
Epoch 900, val loss: 0.805939793586731
Epoch 910, training loss: 6.086236000061035 = 0.0536087341606617 + 1.0 * 6.032627105712891
Epoch 910, val loss: 0.8105911612510681
Epoch 920, training loss: 6.084291458129883 = 0.05140671879053116 + 1.0 * 6.03288459777832
Epoch 920, val loss: 0.815279483795166
Epoch 930, training loss: 6.085824012756348 = 0.04931012541055679 + 1.0 * 6.036513805389404
Epoch 930, val loss: 0.8200125098228455
Epoch 940, training loss: 6.084865570068359 = 0.047320347279310226 + 1.0 * 6.037545204162598
Epoch 940, val loss: 0.8245931267738342
Epoch 950, training loss: 6.07403039932251 = 0.045454125851392746 + 1.0 * 6.028576374053955
Epoch 950, val loss: 0.8292699456214905
Epoch 960, training loss: 6.072726726531982 = 0.04369107633829117 + 1.0 * 6.029035568237305
Epoch 960, val loss: 0.8340432643890381
Epoch 970, training loss: 6.0691046714782715 = 0.04199865460395813 + 1.0 * 6.027105808258057
Epoch 970, val loss: 0.838850736618042
Epoch 980, training loss: 6.067256927490234 = 0.040381815284490585 + 1.0 * 6.026875019073486
Epoch 980, val loss: 0.8437462449073792
Epoch 990, training loss: 6.084916591644287 = 0.03884223476052284 + 1.0 * 6.046074390411377
Epoch 990, val loss: 0.8485562801361084
Epoch 1000, training loss: 6.063673973083496 = 0.03739052265882492 + 1.0 * 6.026283264160156
Epoch 1000, val loss: 0.85330730676651
Epoch 1010, training loss: 6.061179161071777 = 0.03601544722914696 + 1.0 * 6.025163650512695
Epoch 1010, val loss: 0.8581777215003967
Epoch 1020, training loss: 6.059712886810303 = 0.03469683229923248 + 1.0 * 6.025015830993652
Epoch 1020, val loss: 0.8630314469337463
Epoch 1030, training loss: 6.061223030090332 = 0.03343497961759567 + 1.0 * 6.027788162231445
Epoch 1030, val loss: 0.867919385433197
Epoch 1040, training loss: 6.0592451095581055 = 0.03223905712366104 + 1.0 * 6.027006149291992
Epoch 1040, val loss: 0.8726745247840881
Epoch 1050, training loss: 6.055727958679199 = 0.031111640855669975 + 1.0 * 6.024616241455078
Epoch 1050, val loss: 0.8775393962860107
Epoch 1060, training loss: 6.052596569061279 = 0.030038096010684967 + 1.0 * 6.022558689117432
Epoch 1060, val loss: 0.8824067711830139
Epoch 1070, training loss: 6.055304527282715 = 0.029009444639086723 + 1.0 * 6.026295185089111
Epoch 1070, val loss: 0.8871932625770569
Epoch 1080, training loss: 6.051543235778809 = 0.028031105175614357 + 1.0 * 6.023512363433838
Epoch 1080, val loss: 0.891901433467865
Epoch 1090, training loss: 6.048000812530518 = 0.02710149437189102 + 1.0 * 6.020899295806885
Epoch 1090, val loss: 0.896616518497467
Epoch 1100, training loss: 6.047021865844727 = 0.02621064893901348 + 1.0 * 6.020811080932617
Epoch 1100, val loss: 0.9013739824295044
Epoch 1110, training loss: 6.055211544036865 = 0.025351909920573235 + 1.0 * 6.02985954284668
Epoch 1110, val loss: 0.9060284495353699
Epoch 1120, training loss: 6.048530101776123 = 0.024532349780201912 + 1.0 * 6.023997783660889
Epoch 1120, val loss: 0.9104859232902527
Epoch 1130, training loss: 6.043694972991943 = 0.02375650964677334 + 1.0 * 6.0199384689331055
Epoch 1130, val loss: 0.9150217771530151
Epoch 1140, training loss: 6.0422282218933105 = 0.023013491183519363 + 1.0 * 6.019214630126953
Epoch 1140, val loss: 0.9195705652236938
Epoch 1150, training loss: 6.048495769500732 = 0.02229980379343033 + 1.0 * 6.026196002960205
Epoch 1150, val loss: 0.9239806532859802
Epoch 1160, training loss: 6.040469169616699 = 0.021617980673909187 + 1.0 * 6.018851280212402
Epoch 1160, val loss: 0.9284160733222961
Epoch 1170, training loss: 6.037786483764648 = 0.02096833847463131 + 1.0 * 6.016818046569824
Epoch 1170, val loss: 0.9328269362449646
Epoch 1180, training loss: 6.039602756500244 = 0.02034379355609417 + 1.0 * 6.019258975982666
Epoch 1180, val loss: 0.9371662735939026
Epoch 1190, training loss: 6.04279088973999 = 0.01974603161215782 + 1.0 * 6.023045063018799
Epoch 1190, val loss: 0.9414081573486328
Epoch 1200, training loss: 6.036508083343506 = 0.01917756162583828 + 1.0 * 6.017330646514893
Epoch 1200, val loss: 0.945563793182373
Epoch 1210, training loss: 6.036409854888916 = 0.01863298937678337 + 1.0 * 6.017776966094971
Epoch 1210, val loss: 0.9497451186180115
Epoch 1220, training loss: 6.035941123962402 = 0.018108846619725227 + 1.0 * 6.017832279205322
Epoch 1220, val loss: 0.9538403749465942
Epoch 1230, training loss: 6.035390853881836 = 0.017606837674975395 + 1.0 * 6.017784118652344
Epoch 1230, val loss: 0.9579047560691833
Epoch 1240, training loss: 6.033061504364014 = 0.017126185819506645 + 1.0 * 6.01593542098999
Epoch 1240, val loss: 0.9618456959724426
Epoch 1250, training loss: 6.030879020690918 = 0.016669070348143578 + 1.0 * 6.014209747314453
Epoch 1250, val loss: 0.9657863974571228
Epoch 1260, training loss: 6.029841423034668 = 0.016229305416345596 + 1.0 * 6.013612270355225
Epoch 1260, val loss: 0.9697229266166687
Epoch 1270, training loss: 6.031128406524658 = 0.015805792063474655 + 1.0 * 6.015322685241699
Epoch 1270, val loss: 0.9736454486846924
Epoch 1280, training loss: 6.029227256774902 = 0.01539821270853281 + 1.0 * 6.013829231262207
Epoch 1280, val loss: 0.9774699807167053
Epoch 1290, training loss: 6.032647132873535 = 0.015008124522864819 + 1.0 * 6.01763916015625
Epoch 1290, val loss: 0.9813140034675598
Epoch 1300, training loss: 6.032018661499023 = 0.01463411282747984 + 1.0 * 6.0173845291137695
Epoch 1300, val loss: 0.9850755333900452
Epoch 1310, training loss: 6.027706146240234 = 0.014277984388172626 + 1.0 * 6.013428211212158
Epoch 1310, val loss: 0.988726794719696
Epoch 1320, training loss: 6.026247024536133 = 0.01393590122461319 + 1.0 * 6.012310981750488
Epoch 1320, val loss: 0.9924408197402954
Epoch 1330, training loss: 6.024255275726318 = 0.013603935949504375 + 1.0 * 6.010651111602783
Epoch 1330, val loss: 0.9961285591125488
Epoch 1340, training loss: 6.024128437042236 = 0.013281569816172123 + 1.0 * 6.010847091674805
Epoch 1340, val loss: 0.9997780323028564
Epoch 1350, training loss: 6.03046989440918 = 0.01296994648873806 + 1.0 * 6.017499923706055
Epoch 1350, val loss: 1.0033411979675293
Epoch 1360, training loss: 6.031216621398926 = 0.012671916745603085 + 1.0 * 6.018544673919678
Epoch 1360, val loss: 1.0068234205245972
Epoch 1370, training loss: 6.023569107055664 = 0.01238669827580452 + 1.0 * 6.0111823081970215
Epoch 1370, val loss: 1.010310173034668
Epoch 1380, training loss: 6.021478176116943 = 0.012112829834222794 + 1.0 * 6.009365558624268
Epoch 1380, val loss: 1.0138072967529297
Epoch 1390, training loss: 6.020044803619385 = 0.011847549118101597 + 1.0 * 6.00819730758667
Epoch 1390, val loss: 1.0172590017318726
Epoch 1400, training loss: 6.024096488952637 = 0.011589265428483486 + 1.0 * 6.012507438659668
Epoch 1400, val loss: 1.0206797122955322
Epoch 1410, training loss: 6.022050380706787 = 0.01134132593870163 + 1.0 * 6.010709285736084
Epoch 1410, val loss: 1.0240166187286377
Epoch 1420, training loss: 6.024489402770996 = 0.011103495955467224 + 1.0 * 6.013385772705078
Epoch 1420, val loss: 1.0273138284683228
Epoch 1430, training loss: 6.018161773681641 = 0.01087665744125843 + 1.0 * 6.007285118103027
Epoch 1430, val loss: 1.0306100845336914
Epoch 1440, training loss: 6.02058744430542 = 0.01065637357532978 + 1.0 * 6.0099310874938965
Epoch 1440, val loss: 1.0338921546936035
Epoch 1450, training loss: 6.020287036895752 = 0.01044143084436655 + 1.0 * 6.009845733642578
Epoch 1450, val loss: 1.037095546722412
Epoch 1460, training loss: 6.0185723304748535 = 0.010234499350190163 + 1.0 * 6.00833797454834
Epoch 1460, val loss: 1.0402235984802246
Epoch 1470, training loss: 6.0174150466918945 = 0.010035280138254166 + 1.0 * 6.007379531860352
Epoch 1470, val loss: 1.043372631072998
Epoch 1480, training loss: 6.016429901123047 = 0.00984029658138752 + 1.0 * 6.006589412689209
Epoch 1480, val loss: 1.0465342998504639
Epoch 1490, training loss: 6.018103122711182 = 0.009650619700551033 + 1.0 * 6.008452415466309
Epoch 1490, val loss: 1.049614667892456
Epoch 1500, training loss: 6.015895843505859 = 0.00946798175573349 + 1.0 * 6.006427764892578
Epoch 1500, val loss: 1.052682876586914
Epoch 1510, training loss: 6.023396968841553 = 0.009290926158428192 + 1.0 * 6.014106273651123
Epoch 1510, val loss: 1.05569326877594
Epoch 1520, training loss: 6.017504692077637 = 0.009119182825088501 + 1.0 * 6.00838565826416
Epoch 1520, val loss: 1.0585944652557373
Epoch 1530, training loss: 6.014669895172119 = 0.008956008590757847 + 1.0 * 6.005713939666748
Epoch 1530, val loss: 1.0616050958633423
Epoch 1540, training loss: 6.012592792510986 = 0.008797165006399155 + 1.0 * 6.003795623779297
Epoch 1540, val loss: 1.0645900964736938
Epoch 1550, training loss: 6.011946201324463 = 0.008641085587441921 + 1.0 * 6.003304958343506
Epoch 1550, val loss: 1.0675259828567505
Epoch 1560, training loss: 6.017247676849365 = 0.008488446474075317 + 1.0 * 6.008759021759033
Epoch 1560, val loss: 1.0704058408737183
Epoch 1570, training loss: 6.0149078369140625 = 0.008340485394001007 + 1.0 * 6.006567478179932
Epoch 1570, val loss: 1.0732486248016357
Epoch 1580, training loss: 6.01283073425293 = 0.008198363706469536 + 1.0 * 6.004632472991943
Epoch 1580, val loss: 1.0760496854782104
Epoch 1590, training loss: 6.012758731842041 = 0.008061135187745094 + 1.0 * 6.004697799682617
Epoch 1590, val loss: 1.0789486169815063
Epoch 1600, training loss: 6.011784076690674 = 0.007926294580101967 + 1.0 * 6.003857612609863
Epoch 1600, val loss: 1.0817010402679443
Epoch 1610, training loss: 6.009957313537598 = 0.007794966921210289 + 1.0 * 6.002162456512451
Epoch 1610, val loss: 1.08450448513031
Epoch 1620, training loss: 6.013401031494141 = 0.007667004596441984 + 1.0 * 6.005733966827393
Epoch 1620, val loss: 1.0872766971588135
Epoch 1630, training loss: 6.010332107543945 = 0.007541737053543329 + 1.0 * 6.002790451049805
Epoch 1630, val loss: 1.0898668766021729
Epoch 1640, training loss: 6.008523464202881 = 0.007421540562063456 + 1.0 * 6.001101970672607
Epoch 1640, val loss: 1.0926284790039062
Epoch 1650, training loss: 6.008732795715332 = 0.0073051308281719685 + 1.0 * 6.00142765045166
Epoch 1650, val loss: 1.095357894897461
Epoch 1660, training loss: 6.0147905349731445 = 0.007190006319433451 + 1.0 * 6.0076003074646
Epoch 1660, val loss: 1.098008155822754
Epoch 1670, training loss: 6.008026599884033 = 0.0070785763673484325 + 1.0 * 6.000947952270508
Epoch 1670, val loss: 1.1005804538726807
Epoch 1680, training loss: 6.007544994354248 = 0.0069700670428574085 + 1.0 * 6.000575065612793
Epoch 1680, val loss: 1.1032644510269165
Epoch 1690, training loss: 6.015827655792236 = 0.006864169612526894 + 1.0 * 6.008963584899902
Epoch 1690, val loss: 1.1058069467544556
Epoch 1700, training loss: 6.0089311599731445 = 0.006760555785149336 + 1.0 * 6.002170562744141
Epoch 1700, val loss: 1.1083074808120728
Epoch 1710, training loss: 6.006954669952393 = 0.006660693325102329 + 1.0 * 6.000294208526611
Epoch 1710, val loss: 1.1108996868133545
Epoch 1720, training loss: 6.00922155380249 = 0.006562604568898678 + 1.0 * 6.002658843994141
Epoch 1720, val loss: 1.1134206056594849
Epoch 1730, training loss: 6.0060930252075195 = 0.006466197315603495 + 1.0 * 5.999626636505127
Epoch 1730, val loss: 1.1159065961837769
Epoch 1740, training loss: 6.006403923034668 = 0.006372283678501844 + 1.0 * 6.000031471252441
Epoch 1740, val loss: 1.1184378862380981
Epoch 1750, training loss: 6.005801677703857 = 0.006280276924371719 + 1.0 * 5.999521255493164
Epoch 1750, val loss: 1.1208866834640503
Epoch 1760, training loss: 6.007901668548584 = 0.006190226413309574 + 1.0 * 6.001711368560791
Epoch 1760, val loss: 1.1232988834381104
Epoch 1770, training loss: 6.007023811340332 = 0.006102977320551872 + 1.0 * 6.00092077255249
Epoch 1770, val loss: 1.1257330179214478
Epoch 1780, training loss: 6.008236408233643 = 0.006017858162522316 + 1.0 * 6.002218723297119
Epoch 1780, val loss: 1.1281183958053589
Epoch 1790, training loss: 6.005453109741211 = 0.005934777203947306 + 1.0 * 5.999518394470215
Epoch 1790, val loss: 1.1305228471755981
Epoch 1800, training loss: 6.003559589385986 = 0.005854284856468439 + 1.0 * 5.997705459594727
Epoch 1800, val loss: 1.1329635381698608
Epoch 1810, training loss: 6.005980968475342 = 0.005774692166596651 + 1.0 * 6.000206470489502
Epoch 1810, val loss: 1.1353031396865845
Epoch 1820, training loss: 6.004117965698242 = 0.005696489475667477 + 1.0 * 5.998421669006348
Epoch 1820, val loss: 1.1376228332519531
Epoch 1830, training loss: 6.00419282913208 = 0.005620500538498163 + 1.0 * 5.99857234954834
Epoch 1830, val loss: 1.1399433612823486
Epoch 1840, training loss: 6.004937648773193 = 0.005546246189624071 + 1.0 * 5.999391555786133
Epoch 1840, val loss: 1.1422182321548462
Epoch 1850, training loss: 6.0032453536987305 = 0.005473555997014046 + 1.0 * 5.997771739959717
Epoch 1850, val loss: 1.144514799118042
Epoch 1860, training loss: 6.005960941314697 = 0.005402771290391684 + 1.0 * 6.000558376312256
Epoch 1860, val loss: 1.146829605102539
Epoch 1870, training loss: 6.005637168884277 = 0.005333441309630871 + 1.0 * 6.000303745269775
Epoch 1870, val loss: 1.1490137577056885
Epoch 1880, training loss: 6.00158166885376 = 0.005265423096716404 + 1.0 * 5.996316432952881
Epoch 1880, val loss: 1.1511856317520142
Epoch 1890, training loss: 6.0027689933776855 = 0.0051992810331285 + 1.0 * 5.997569561004639
Epoch 1890, val loss: 1.1534509658813477
Epoch 1900, training loss: 6.006017208099365 = 0.005134147126227617 + 1.0 * 6.000883102416992
Epoch 1900, val loss: 1.1555758714675903
Epoch 1910, training loss: 6.001291751861572 = 0.005070218816399574 + 1.0 * 5.996221542358398
Epoch 1910, val loss: 1.157751202583313
Epoch 1920, training loss: 6.000112056732178 = 0.005007875617593527 + 1.0 * 5.9951043128967285
Epoch 1920, val loss: 1.1599146127700806
Epoch 1930, training loss: 6.003074645996094 = 0.004946577362716198 + 1.0 * 5.9981279373168945
Epoch 1930, val loss: 1.1620463132858276
Epoch 1940, training loss: 6.002734184265137 = 0.004886524751782417 + 1.0 * 5.997847557067871
Epoch 1940, val loss: 1.1641627550125122
Epoch 1950, training loss: 5.999614238739014 = 0.004827618598937988 + 1.0 * 5.994786739349365
Epoch 1950, val loss: 1.1663002967834473
Epoch 1960, training loss: 5.999771595001221 = 0.004770274274051189 + 1.0 * 5.995001316070557
Epoch 1960, val loss: 1.168466329574585
Epoch 1970, training loss: 6.006857872009277 = 0.004713786765933037 + 1.0 * 6.002143859863281
Epoch 1970, val loss: 1.1705584526062012
Epoch 1980, training loss: 6.001477241516113 = 0.0046580880880355835 + 1.0 * 5.996819019317627
Epoch 1980, val loss: 1.1724786758422852
Epoch 1990, training loss: 5.998741626739502 = 0.004604879301041365 + 1.0 * 5.994136810302734
Epoch 1990, val loss: 1.174629807472229
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.1402
Flip ASR: 0.1378/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.327835083007812 = 1.9540047645568848 + 1.0 * 8.37382984161377
Epoch 0, val loss: 1.9528969526290894
Epoch 10, training loss: 10.316938400268555 = 1.9436249732971191 + 1.0 * 8.373312950134277
Epoch 10, val loss: 1.9423775672912598
Epoch 20, training loss: 10.300337791442871 = 1.9304745197296143 + 1.0 * 8.369863510131836
Epoch 20, val loss: 1.9284032583236694
Epoch 30, training loss: 10.257621765136719 = 1.9118813276290894 + 1.0 * 8.34574031829834
Epoch 30, val loss: 1.908228874206543
Epoch 40, training loss: 10.078174591064453 = 1.8880228996276855 + 1.0 * 8.19015121459961
Epoch 40, val loss: 1.8836628198623657
Epoch 50, training loss: 9.480819702148438 = 1.861053228378296 + 1.0 * 7.619766712188721
Epoch 50, val loss: 1.85655677318573
Epoch 60, training loss: 9.074410438537598 = 1.8386365175247192 + 1.0 * 7.23577356338501
Epoch 60, val loss: 1.8356804847717285
Epoch 70, training loss: 8.673140525817871 = 1.8246670961380005 + 1.0 * 6.84847354888916
Epoch 70, val loss: 1.8222719430923462
Epoch 80, training loss: 8.491573333740234 = 1.8123672008514404 + 1.0 * 6.679206371307373
Epoch 80, val loss: 1.810166597366333
Epoch 90, training loss: 8.403127670288086 = 1.7993135452270508 + 1.0 * 6.603813648223877
Epoch 90, val loss: 1.7970912456512451
Epoch 100, training loss: 8.309749603271484 = 1.786091685295105 + 1.0 * 6.52365779876709
Epoch 100, val loss: 1.7844064235687256
Epoch 110, training loss: 8.228934288024902 = 1.774924397468567 + 1.0 * 6.454010009765625
Epoch 110, val loss: 1.7740659713745117
Epoch 120, training loss: 8.169540405273438 = 1.7642674446105957 + 1.0 * 6.405272960662842
Epoch 120, val loss: 1.7647205591201782
Epoch 130, training loss: 8.118345260620117 = 1.7521313428878784 + 1.0 * 6.366214275360107
Epoch 130, val loss: 1.7545907497406006
Epoch 140, training loss: 8.07158088684082 = 1.7380365133285522 + 1.0 * 6.3335442543029785
Epoch 140, val loss: 1.7431323528289795
Epoch 150, training loss: 8.026177406311035 = 1.7218835353851318 + 1.0 * 6.304293632507324
Epoch 150, val loss: 1.7301877737045288
Epoch 160, training loss: 7.981158256530762 = 1.7031582593917847 + 1.0 * 6.2779998779296875
Epoch 160, val loss: 1.7152786254882812
Epoch 170, training loss: 7.936280727386475 = 1.6806387901306152 + 1.0 * 6.255641937255859
Epoch 170, val loss: 1.697480320930481
Epoch 180, training loss: 7.891978740692139 = 1.6533522605895996 + 1.0 * 6.238626480102539
Epoch 180, val loss: 1.6760129928588867
Epoch 190, training loss: 7.843575954437256 = 1.620466709136963 + 1.0 * 6.223109245300293
Epoch 190, val loss: 1.650146484375
Epoch 200, training loss: 7.791268348693848 = 1.581063151359558 + 1.0 * 6.210205078125
Epoch 200, val loss: 1.6190769672393799
Epoch 210, training loss: 7.735163688659668 = 1.534740686416626 + 1.0 * 6.200422763824463
Epoch 210, val loss: 1.5825251340866089
Epoch 220, training loss: 7.67301082611084 = 1.4830656051635742 + 1.0 * 6.189945220947266
Epoch 220, val loss: 1.5418367385864258
Epoch 230, training loss: 7.60734224319458 = 1.4263781309127808 + 1.0 * 6.18096399307251
Epoch 230, val loss: 1.4973465204238892
Epoch 240, training loss: 7.5376667976379395 = 1.3646937608718872 + 1.0 * 6.172973155975342
Epoch 240, val loss: 1.4491946697235107
Epoch 250, training loss: 7.470762252807617 = 1.2989450693130493 + 1.0 * 6.171817302703857
Epoch 250, val loss: 1.3980597257614136
Epoch 260, training loss: 7.394708156585693 = 1.2325325012207031 + 1.0 * 6.16217565536499
Epoch 260, val loss: 1.346917986869812
Epoch 270, training loss: 7.32090950012207 = 1.1657296419143677 + 1.0 * 6.155179977416992
Epoch 270, val loss: 1.2957327365875244
Epoch 280, training loss: 7.252292633056641 = 1.0990477800369263 + 1.0 * 6.153244972229004
Epoch 280, val loss: 1.2445380687713623
Epoch 290, training loss: 7.1815104484558105 = 1.034706950187683 + 1.0 * 6.146803379058838
Epoch 290, val loss: 1.1950023174285889
Epoch 300, training loss: 7.1138834953308105 = 0.973181426525116 + 1.0 * 6.140702247619629
Epoch 300, val loss: 1.1477848291397095
Epoch 310, training loss: 7.051438808441162 = 0.9151671528816223 + 1.0 * 6.1362714767456055
Epoch 310, val loss: 1.1035712957382202
Epoch 320, training loss: 6.994026184082031 = 0.8618307709693909 + 1.0 * 6.132195472717285
Epoch 320, val loss: 1.0631619691848755
Epoch 330, training loss: 6.94194221496582 = 0.8124606609344482 + 1.0 * 6.129481792449951
Epoch 330, val loss: 1.026265025138855
Epoch 340, training loss: 6.893831729888916 = 0.7674552798271179 + 1.0 * 6.126376628875732
Epoch 340, val loss: 0.9931681156158447
Epoch 350, training loss: 6.84636926651001 = 0.7264482378959656 + 1.0 * 6.1199212074279785
Epoch 350, val loss: 0.9638069272041321
Epoch 360, training loss: 6.8076887130737305 = 0.6884677410125732 + 1.0 * 6.119220733642578
Epoch 360, val loss: 0.9374000430107117
Epoch 370, training loss: 6.767765045166016 = 0.6536229252815247 + 1.0 * 6.114141941070557
Epoch 370, val loss: 0.9138902425765991
Epoch 380, training loss: 6.730422019958496 = 0.6211662292480469 + 1.0 * 6.109255790710449
Epoch 380, val loss: 0.8930942416191101
Epoch 390, training loss: 6.6957926750183105 = 0.5903730988502502 + 1.0 * 6.105419635772705
Epoch 390, val loss: 0.874280571937561
Epoch 400, training loss: 6.6654438972473145 = 0.5610695481300354 + 1.0 * 6.104374408721924
Epoch 400, val loss: 0.8572671413421631
Epoch 410, training loss: 6.6371846199035645 = 0.5335050225257874 + 1.0 * 6.103679656982422
Epoch 410, val loss: 0.8421700596809387
Epoch 420, training loss: 6.605184555053711 = 0.5074286460876465 + 1.0 * 6.0977559089660645
Epoch 420, val loss: 0.8288989067077637
Epoch 430, training loss: 6.576830863952637 = 0.48216986656188965 + 1.0 * 6.094661235809326
Epoch 430, val loss: 0.8168044090270996
Epoch 440, training loss: 6.549263000488281 = 0.4574534296989441 + 1.0 * 6.0918097496032715
Epoch 440, val loss: 0.8056656122207642
Epoch 450, training loss: 6.531895637512207 = 0.433154821395874 + 1.0 * 6.098741054534912
Epoch 450, val loss: 0.7953352332115173
Epoch 460, training loss: 6.498391151428223 = 0.40943270921707153 + 1.0 * 6.088958263397217
Epoch 460, val loss: 0.7857856154441833
Epoch 470, training loss: 6.472103118896484 = 0.3861561715602875 + 1.0 * 6.085947036743164
Epoch 470, val loss: 0.7770881056785583
Epoch 480, training loss: 6.447083473205566 = 0.3630550801753998 + 1.0 * 6.084028244018555
Epoch 480, val loss: 0.7689379453659058
Epoch 490, training loss: 6.4238409996032715 = 0.34030216932296753 + 1.0 * 6.083539009094238
Epoch 490, val loss: 0.7612758874893188
Epoch 500, training loss: 6.399936199188232 = 0.31811949610710144 + 1.0 * 6.081816673278809
Epoch 500, val loss: 0.7545028924942017
Epoch 510, training loss: 6.3748979568481445 = 0.2965398132801056 + 1.0 * 6.078358173370361
Epoch 510, val loss: 0.7485153675079346
Epoch 520, training loss: 6.362226486206055 = 0.2757105529308319 + 1.0 * 6.0865159034729
Epoch 520, val loss: 0.743396520614624
Epoch 530, training loss: 6.3346123695373535 = 0.256090372800827 + 1.0 * 6.078522205352783
Epoch 530, val loss: 0.7393083572387695
Epoch 540, training loss: 6.3117523193359375 = 0.23768386244773865 + 1.0 * 6.074068546295166
Epoch 540, val loss: 0.736405611038208
Epoch 550, training loss: 6.292912483215332 = 0.2204551100730896 + 1.0 * 6.072457313537598
Epoch 550, val loss: 0.7345163822174072
Epoch 560, training loss: 6.276930809020996 = 0.20453299582004547 + 1.0 * 6.072397708892822
Epoch 560, val loss: 0.733559250831604
Epoch 570, training loss: 6.258572101593018 = 0.18998050689697266 + 1.0 * 6.068591594696045
Epoch 570, val loss: 0.7336154580116272
Epoch 580, training loss: 6.252132415771484 = 0.17669042944908142 + 1.0 * 6.075441837310791
Epoch 580, val loss: 0.7344648241996765
Epoch 590, training loss: 6.232513427734375 = 0.16468098759651184 + 1.0 * 6.0678324699401855
Epoch 590, val loss: 0.7360333800315857
Epoch 600, training loss: 6.219031810760498 = 0.153781458735466 + 1.0 * 6.065250396728516
Epoch 600, val loss: 0.7383788228034973
Epoch 610, training loss: 6.223055839538574 = 0.14382299780845642 + 1.0 * 6.079232692718506
Epoch 610, val loss: 0.7411994338035583
Epoch 620, training loss: 6.199612617492676 = 0.134860098361969 + 1.0 * 6.064752578735352
Epoch 620, val loss: 0.7444459795951843
Epoch 630, training loss: 6.187705993652344 = 0.12670689821243286 + 1.0 * 6.060998916625977
Epoch 630, val loss: 0.7482670545578003
Epoch 640, training loss: 6.177988529205322 = 0.11921827495098114 + 1.0 * 6.058770179748535
Epoch 640, val loss: 0.7524052262306213
Epoch 650, training loss: 6.17157506942749 = 0.11231574416160583 + 1.0 * 6.059259414672852
Epoch 650, val loss: 0.7568598389625549
Epoch 660, training loss: 6.172072410583496 = 0.10599561780691147 + 1.0 * 6.066076755523682
Epoch 660, val loss: 0.7614947557449341
Epoch 670, training loss: 6.157658100128174 = 0.1002282127737999 + 1.0 * 6.057429790496826
Epoch 670, val loss: 0.7664006948471069
Epoch 680, training loss: 6.1499433517456055 = 0.09489117562770844 + 1.0 * 6.055052280426025
Epoch 680, val loss: 0.7715181708335876
Epoch 690, training loss: 6.146538734436035 = 0.0899243950843811 + 1.0 * 6.056614398956299
Epoch 690, val loss: 0.7768002152442932
Epoch 700, training loss: 6.138897895812988 = 0.08531645685434341 + 1.0 * 6.053581237792969
Epoch 700, val loss: 0.7822384238243103
Epoch 710, training loss: 6.136772632598877 = 0.08104587346315384 + 1.0 * 6.055726528167725
Epoch 710, val loss: 0.7878553867340088
Epoch 720, training loss: 6.127062797546387 = 0.07706141471862793 + 1.0 * 6.050001621246338
Epoch 720, val loss: 0.793462872505188
Epoch 730, training loss: 6.1230034828186035 = 0.07334328442811966 + 1.0 * 6.0496602058410645
Epoch 730, val loss: 0.7992556095123291
Epoch 740, training loss: 6.122563362121582 = 0.06985388696193695 + 1.0 * 6.052709579467773
Epoch 740, val loss: 0.8050978779792786
Epoch 750, training loss: 6.116001129150391 = 0.06658747792243958 + 1.0 * 6.049413681030273
Epoch 750, val loss: 0.8109995126724243
Epoch 760, training loss: 6.110982418060303 = 0.06352396309375763 + 1.0 * 6.047458648681641
Epoch 760, val loss: 0.8170183897018433
Epoch 770, training loss: 6.111813545227051 = 0.060638684779405594 + 1.0 * 6.051174640655518
Epoch 770, val loss: 0.8230326771736145
Epoch 780, training loss: 6.109987735748291 = 0.05793154984712601 + 1.0 * 6.052056312561035
Epoch 780, val loss: 0.8290005326271057
Epoch 790, training loss: 6.100100040435791 = 0.05540400370955467 + 1.0 * 6.044695854187012
Epoch 790, val loss: 0.8350693583488464
Epoch 800, training loss: 6.096923828125 = 0.05301688238978386 + 1.0 * 6.043907165527344
Epoch 800, val loss: 0.8411471247673035
Epoch 810, training loss: 6.0939531326293945 = 0.050757184624671936 + 1.0 * 6.043195724487305
Epoch 810, val loss: 0.8472520709037781
Epoch 820, training loss: 6.1005635261535645 = 0.04862669110298157 + 1.0 * 6.051936626434326
Epoch 820, val loss: 0.8532533049583435
Epoch 830, training loss: 6.090803146362305 = 0.046642154455184937 + 1.0 * 6.044160842895508
Epoch 830, val loss: 0.8593264818191528
Epoch 840, training loss: 6.085077285766602 = 0.04477725923061371 + 1.0 * 6.040299892425537
Epoch 840, val loss: 0.8654127717018127
Epoch 850, training loss: 6.083479404449463 = 0.04300496727228165 + 1.0 * 6.0404744148254395
Epoch 850, val loss: 0.8714063167572021
Epoch 860, training loss: 6.0844855308532715 = 0.041328102350234985 + 1.0 * 6.043157577514648
Epoch 860, val loss: 0.8773024678230286
Epoch 870, training loss: 6.0771918296813965 = 0.039750467985868454 + 1.0 * 6.037441253662109
Epoch 870, val loss: 0.8832622766494751
Epoch 880, training loss: 6.075052261352539 = 0.03825816139578819 + 1.0 * 6.036794185638428
Epoch 880, val loss: 0.889249861240387
Epoch 890, training loss: 6.074460506439209 = 0.03683469817042351 + 1.0 * 6.037625789642334
Epoch 890, val loss: 0.8951725363731384
Epoch 900, training loss: 6.070859909057617 = 0.035486504435539246 + 1.0 * 6.035373210906982
Epoch 900, val loss: 0.9009655714035034
Epoch 910, training loss: 6.070497512817383 = 0.03421664983034134 + 1.0 * 6.036280632019043
Epoch 910, val loss: 0.9068705439567566
Epoch 920, training loss: 6.06791353225708 = 0.03301031142473221 + 1.0 * 6.034903049468994
Epoch 920, val loss: 0.9127399921417236
Epoch 930, training loss: 6.071990013122559 = 0.031857289373874664 + 1.0 * 6.040132522583008
Epoch 930, val loss: 0.9185384511947632
Epoch 940, training loss: 6.073095321655273 = 0.030765429139137268 + 1.0 * 6.042329788208008
Epoch 940, val loss: 0.9241415858268738
Epoch 950, training loss: 6.063050746917725 = 0.029742369428277016 + 1.0 * 6.033308506011963
Epoch 950, val loss: 0.9297656416893005
Epoch 960, training loss: 6.061875343322754 = 0.02876829355955124 + 1.0 * 6.033107280731201
Epoch 960, val loss: 0.935343325138092
Epoch 970, training loss: 6.061227798461914 = 0.02783462591469288 + 1.0 * 6.033393383026123
Epoch 970, val loss: 0.9407985210418701
Epoch 980, training loss: 6.063784122467041 = 0.026943495497107506 + 1.0 * 6.036840438842773
Epoch 980, val loss: 0.9461636543273926
Epoch 990, training loss: 6.056814670562744 = 0.026101049035787582 + 1.0 * 6.0307135581970215
Epoch 990, val loss: 0.9515368342399597
Epoch 1000, training loss: 6.055457592010498 = 0.025297487154603004 + 1.0 * 6.030159950256348
Epoch 1000, val loss: 0.956906259059906
Epoch 1010, training loss: 6.053305149078369 = 0.024526679888367653 + 1.0 * 6.028778553009033
Epoch 1010, val loss: 0.962188720703125
Epoch 1020, training loss: 6.05353307723999 = 0.02378728985786438 + 1.0 * 6.029745578765869
Epoch 1020, val loss: 0.9674126505851746
Epoch 1030, training loss: 6.054990291595459 = 0.023079844191670418 + 1.0 * 6.031910419464111
Epoch 1030, val loss: 0.9724199175834656
Epoch 1040, training loss: 6.055168151855469 = 0.022415025159716606 + 1.0 * 6.032752990722656
Epoch 1040, val loss: 0.9773905277252197
Epoch 1050, training loss: 6.048599720001221 = 0.02178170345723629 + 1.0 * 6.026817798614502
Epoch 1050, val loss: 0.9824260473251343
Epoch 1060, training loss: 6.047698497772217 = 0.021171357482671738 + 1.0 * 6.026526927947998
Epoch 1060, val loss: 0.9873552322387695
Epoch 1070, training loss: 6.050973892211914 = 0.02058297023177147 + 1.0 * 6.030390739440918
Epoch 1070, val loss: 0.9921835660934448
Epoch 1080, training loss: 6.047526836395264 = 0.020018091425299644 + 1.0 * 6.027508735656738
Epoch 1080, val loss: 0.9968847036361694
Epoch 1090, training loss: 6.05020809173584 = 0.019483568146824837 + 1.0 * 6.03072452545166
Epoch 1090, val loss: 1.001562237739563
Epoch 1100, training loss: 6.045816898345947 = 0.01897096075117588 + 1.0 * 6.026845932006836
Epoch 1100, val loss: 1.006211280822754
Epoch 1110, training loss: 6.042891502380371 = 0.018478967249393463 + 1.0 * 6.024412631988525
Epoch 1110, val loss: 1.0108118057250977
Epoch 1120, training loss: 6.042473316192627 = 0.01800369843840599 + 1.0 * 6.02446985244751
Epoch 1120, val loss: 1.0153772830963135
Epoch 1130, training loss: 6.048713684082031 = 0.017544178292155266 + 1.0 * 6.031169414520264
Epoch 1130, val loss: 1.0198559761047363
Epoch 1140, training loss: 6.041992664337158 = 0.017105961218476295 + 1.0 * 6.024886608123779
Epoch 1140, val loss: 1.0242923498153687
Epoch 1150, training loss: 6.038769721984863 = 0.016684751957654953 + 1.0 * 6.022085189819336
Epoch 1150, val loss: 1.0287442207336426
Epoch 1160, training loss: 6.042150974273682 = 0.016277359798550606 + 1.0 * 6.02587366104126
Epoch 1160, val loss: 1.033152461051941
Epoch 1170, training loss: 6.037690162658691 = 0.015884919092059135 + 1.0 * 6.021805286407471
Epoch 1170, val loss: 1.0373531579971313
Epoch 1180, training loss: 6.037169456481934 = 0.015510017983615398 + 1.0 * 6.0216593742370605
Epoch 1180, val loss: 1.0416755676269531
Epoch 1190, training loss: 6.0377326011657715 = 0.015147320926189423 + 1.0 * 6.022585391998291
Epoch 1190, val loss: 1.0459562540054321
Epoch 1200, training loss: 6.036232948303223 = 0.014795704744756222 + 1.0 * 6.021437168121338
Epoch 1200, val loss: 1.0499544143676758
Epoch 1210, training loss: 6.038721561431885 = 0.014461471699178219 + 1.0 * 6.0242600440979
Epoch 1210, val loss: 1.054068922996521
Epoch 1220, training loss: 6.033844947814941 = 0.014140098355710506 + 1.0 * 6.019704818725586
Epoch 1220, val loss: 1.0581247806549072
Epoch 1230, training loss: 6.032881259918213 = 0.01382889412343502 + 1.0 * 6.019052505493164
Epoch 1230, val loss: 1.0621417760849
Epoch 1240, training loss: 6.032285213470459 = 0.013525843620300293 + 1.0 * 6.018759250640869
Epoch 1240, val loss: 1.0661638975143433
Epoch 1250, training loss: 6.040125370025635 = 0.013230754993855953 + 1.0 * 6.026894569396973
Epoch 1250, val loss: 1.0700594186782837
Epoch 1260, training loss: 6.0369157791137695 = 0.01294846460223198 + 1.0 * 6.023967266082764
Epoch 1260, val loss: 1.0738226175308228
Epoch 1270, training loss: 6.030086040496826 = 0.012681021355092525 + 1.0 * 6.017405033111572
Epoch 1270, val loss: 1.0776796340942383
Epoch 1280, training loss: 6.0290703773498535 = 0.01242054719477892 + 1.0 * 6.0166497230529785
Epoch 1280, val loss: 1.0815333127975464
Epoch 1290, training loss: 6.028326511383057 = 0.0121647659689188 + 1.0 * 6.016161918640137
Epoch 1290, val loss: 1.0853652954101562
Epoch 1300, training loss: 6.031042098999023 = 0.011915371753275394 + 1.0 * 6.019126892089844
Epoch 1300, val loss: 1.0891408920288086
Epoch 1310, training loss: 6.027782917022705 = 0.011674032546579838 + 1.0 * 6.016108989715576
Epoch 1310, val loss: 1.0927492380142212
Epoch 1320, training loss: 6.03072452545166 = 0.011443795636296272 + 1.0 * 6.019280910491943
Epoch 1320, val loss: 1.0964072942733765
Epoch 1330, training loss: 6.0268378257751465 = 0.01122116670012474 + 1.0 * 6.0156168937683105
Epoch 1330, val loss: 1.1000254154205322
Epoch 1340, training loss: 6.027502536773682 = 0.011004673317074776 + 1.0 * 6.01649808883667
Epoch 1340, val loss: 1.103684425354004
Epoch 1350, training loss: 6.031139373779297 = 0.010792877525091171 + 1.0 * 6.020346641540527
Epoch 1350, val loss: 1.1071046590805054
Epoch 1360, training loss: 6.025064468383789 = 0.010590482503175735 + 1.0 * 6.014473915100098
Epoch 1360, val loss: 1.1105886697769165
Epoch 1370, training loss: 6.023688316345215 = 0.010394159704446793 + 1.0 * 6.013294219970703
Epoch 1370, val loss: 1.1141606569290161
Epoch 1380, training loss: 6.024312973022461 = 0.0102013498544693 + 1.0 * 6.014111518859863
Epoch 1380, val loss: 1.1176416873931885
Epoch 1390, training loss: 6.028330326080322 = 0.010012517683207989 + 1.0 * 6.018317699432373
Epoch 1390, val loss: 1.120992660522461
Epoch 1400, training loss: 6.024437427520752 = 0.00983123667538166 + 1.0 * 6.01460599899292
Epoch 1400, val loss: 1.1244065761566162
Epoch 1410, training loss: 6.026895046234131 = 0.009656010195612907 + 1.0 * 6.017239093780518
Epoch 1410, val loss: 1.127803087234497
Epoch 1420, training loss: 6.021742820739746 = 0.009484423324465752 + 1.0 * 6.012258529663086
Epoch 1420, val loss: 1.131090521812439
Epoch 1430, training loss: 6.022430419921875 = 0.00931923184543848 + 1.0 * 6.013111114501953
Epoch 1430, val loss: 1.1344200372695923
Epoch 1440, training loss: 6.036363124847412 = 0.009157789871096611 + 1.0 * 6.027205467224121
Epoch 1440, val loss: 1.1376264095306396
Epoch 1450, training loss: 6.021656513214111 = 0.009000953286886215 + 1.0 * 6.012655735015869
Epoch 1450, val loss: 1.1406362056732178
Epoch 1460, training loss: 6.018935680389404 = 0.008852208033204079 + 1.0 * 6.0100836753845215
Epoch 1460, val loss: 1.1438963413238525
Epoch 1470, training loss: 6.019034385681152 = 0.00870469305664301 + 1.0 * 6.010329723358154
Epoch 1470, val loss: 1.1471751928329468
Epoch 1480, training loss: 6.024674892425537 = 0.008558609522879124 + 1.0 * 6.016116142272949
Epoch 1480, val loss: 1.1502501964569092
Epoch 1490, training loss: 6.018435001373291 = 0.008416960947215557 + 1.0 * 6.0100178718566895
Epoch 1490, val loss: 1.153353214263916
Epoch 1500, training loss: 6.018410682678223 = 0.008279862813651562 + 1.0 * 6.010130882263184
Epoch 1500, val loss: 1.1564931869506836
Epoch 1510, training loss: 6.019888877868652 = 0.008145247586071491 + 1.0 * 6.011743545532227
Epoch 1510, val loss: 1.1595321893692017
Epoch 1520, training loss: 6.020143985748291 = 0.008015092462301254 + 1.0 * 6.012128829956055
Epoch 1520, val loss: 1.162562370300293
Epoch 1530, training loss: 6.018842697143555 = 0.00788865890353918 + 1.0 * 6.010953903198242
Epoch 1530, val loss: 1.1655563116073608
Epoch 1540, training loss: 6.017994403839111 = 0.0077661131508648396 + 1.0 * 6.010228157043457
Epoch 1540, val loss: 1.1685377359390259
Epoch 1550, training loss: 6.015238285064697 = 0.0076463064178824425 + 1.0 * 6.00759220123291
Epoch 1550, val loss: 1.171517014503479
Epoch 1560, training loss: 6.0274481773376465 = 0.007528604473918676 + 1.0 * 6.019919395446777
Epoch 1560, val loss: 1.1744047403335571
Epoch 1570, training loss: 6.016170978546143 = 0.007413987070322037 + 1.0 * 6.0087571144104
Epoch 1570, val loss: 1.1772193908691406
Epoch 1580, training loss: 6.013589382171631 = 0.007304325234144926 + 1.0 * 6.006285190582275
Epoch 1580, val loss: 1.1801267862319946
Epoch 1590, training loss: 6.013138294219971 = 0.0071956925094127655 + 1.0 * 6.0059428215026855
Epoch 1590, val loss: 1.1830933094024658
Epoch 1600, training loss: 6.02043342590332 = 0.00708812614902854 + 1.0 * 6.013345241546631
Epoch 1600, val loss: 1.185922384262085
Epoch 1610, training loss: 6.016149044036865 = 0.006982979830354452 + 1.0 * 6.009166240692139
Epoch 1610, val loss: 1.1885652542114258
Epoch 1620, training loss: 6.015959739685059 = 0.006883449386805296 + 1.0 * 6.009076118469238
Epoch 1620, val loss: 1.1913663148880005
Epoch 1630, training loss: 6.019214630126953 = 0.0067857056856155396 + 1.0 * 6.0124287605285645
Epoch 1630, val loss: 1.1940873861312866
Epoch 1640, training loss: 6.014111518859863 = 0.006689941510558128 + 1.0 * 6.007421493530273
Epoch 1640, val loss: 1.196761965751648
Epoch 1650, training loss: 6.011641502380371 = 0.006597374100238085 + 1.0 * 6.005043983459473
Epoch 1650, val loss: 1.1996012926101685
Epoch 1660, training loss: 6.009976863861084 = 0.006504798773676157 + 1.0 * 6.003471851348877
Epoch 1660, val loss: 1.202376127243042
Epoch 1670, training loss: 6.014998435974121 = 0.0064133089035749435 + 1.0 * 6.008584976196289
Epoch 1670, val loss: 1.205142855644226
Epoch 1680, training loss: 6.012277126312256 = 0.006323189940303564 + 1.0 * 6.005953788757324
Epoch 1680, val loss: 1.2075504064559937
Epoch 1690, training loss: 6.0133137702941895 = 0.006237800233066082 + 1.0 * 6.007075786590576
Epoch 1690, val loss: 1.2101930379867554
Epoch 1700, training loss: 6.0131306648254395 = 0.00615479564294219 + 1.0 * 6.0069756507873535
Epoch 1700, val loss: 1.2128422260284424
Epoch 1710, training loss: 6.008284568786621 = 0.006072631571441889 + 1.0 * 6.002212047576904
Epoch 1710, val loss: 1.215505599975586
Epoch 1720, training loss: 6.009013652801514 = 0.0059919110499322414 + 1.0 * 6.003021717071533
Epoch 1720, val loss: 1.2181817293167114
Epoch 1730, training loss: 6.012701511383057 = 0.005911574233323336 + 1.0 * 6.0067901611328125
Epoch 1730, val loss: 1.2207798957824707
Epoch 1740, training loss: 6.01550817489624 = 0.005832773633301258 + 1.0 * 6.0096755027771
Epoch 1740, val loss: 1.2231520414352417
Epoch 1750, training loss: 6.009761333465576 = 0.005758508574217558 + 1.0 * 6.004003047943115
Epoch 1750, val loss: 1.2256031036376953
Epoch 1760, training loss: 6.007996082305908 = 0.00568648986518383 + 1.0 * 6.002309799194336
Epoch 1760, val loss: 1.2282217741012573
Epoch 1770, training loss: 6.00648307800293 = 0.005614102818071842 + 1.0 * 6.000868797302246
Epoch 1770, val loss: 1.230806589126587
Epoch 1780, training loss: 6.008644104003906 = 0.005541798658668995 + 1.0 * 6.0031023025512695
Epoch 1780, val loss: 1.233325481414795
Epoch 1790, training loss: 6.012059211730957 = 0.005470657255500555 + 1.0 * 6.006588459014893
Epoch 1790, val loss: 1.2356772422790527
Epoch 1800, training loss: 6.007242202758789 = 0.005402625538408756 + 1.0 * 6.001839637756348
Epoch 1800, val loss: 1.2379505634307861
Epoch 1810, training loss: 6.006164073944092 = 0.005337539128959179 + 1.0 * 6.000826358795166
Epoch 1810, val loss: 1.2404944896697998
Epoch 1820, training loss: 6.006185531616211 = 0.005272634793072939 + 1.0 * 6.000912666320801
Epoch 1820, val loss: 1.2429746389389038
Epoch 1830, training loss: 6.009178638458252 = 0.005207185633480549 + 1.0 * 6.003971576690674
Epoch 1830, val loss: 1.245396614074707
Epoch 1840, training loss: 6.004415035247803 = 0.0051435972563922405 + 1.0 * 5.999271392822266
Epoch 1840, val loss: 1.2477256059646606
Epoch 1850, training loss: 6.006658554077148 = 0.005081971175968647 + 1.0 * 6.0015764236450195
Epoch 1850, val loss: 1.2501776218414307
Epoch 1860, training loss: 6.01017427444458 = 0.005020187236368656 + 1.0 * 6.005154132843018
Epoch 1860, val loss: 1.2524491548538208
Epoch 1870, training loss: 6.005885124206543 = 0.0049607413820922375 + 1.0 * 6.000924587249756
Epoch 1870, val loss: 1.254709005355835
Epoch 1880, training loss: 6.006968975067139 = 0.004902772139757872 + 1.0 * 6.002066135406494
Epoch 1880, val loss: 1.2570194005966187
Epoch 1890, training loss: 6.003737926483154 = 0.004846001975238323 + 1.0 * 5.998891830444336
Epoch 1890, val loss: 1.2593317031860352
Epoch 1900, training loss: 6.006749153137207 = 0.004790033679455519 + 1.0 * 6.001959323883057
Epoch 1900, val loss: 1.2616506814956665
Epoch 1910, training loss: 6.003302097320557 = 0.0047345710918307304 + 1.0 * 5.998567581176758
Epoch 1910, val loss: 1.2638808488845825
Epoch 1920, training loss: 6.00232458114624 = 0.004680980462580919 + 1.0 * 5.99764347076416
Epoch 1920, val loss: 1.266202449798584
Epoch 1930, training loss: 6.003594875335693 = 0.004627329297363758 + 1.0 * 5.99896764755249
Epoch 1930, val loss: 1.2684897184371948
Epoch 1940, training loss: 6.0085930824279785 = 0.004574147053062916 + 1.0 * 6.004018783569336
Epoch 1940, val loss: 1.2706577777862549
Epoch 1950, training loss: 6.0047125816345215 = 0.004522454924881458 + 1.0 * 6.000190258026123
Epoch 1950, val loss: 1.2727077007293701
Epoch 1960, training loss: 6.003512382507324 = 0.0044733383692801 + 1.0 * 5.999039173126221
Epoch 1960, val loss: 1.274887204170227
Epoch 1970, training loss: 6.002192974090576 = 0.00442464230582118 + 1.0 * 5.997768402099609
Epoch 1970, val loss: 1.2771352529525757
Epoch 1980, training loss: 6.0021820068359375 = 0.004376057535409927 + 1.0 * 5.997806072235107
Epoch 1980, val loss: 1.2793179750442505
Epoch 1990, training loss: 6.000429630279541 = 0.004328055307269096 + 1.0 * 5.996101379394531
Epoch 1990, val loss: 1.281516432762146
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9483
Flip ASR: 0.9378/225 nodes
The final ASR:0.60517, 0.34096, Accuracy:0.81605, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9536])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.82716, 0.00462
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.329870223999023 = 1.956087350845337 + 1.0 * 8.373783111572266
Epoch 0, val loss: 1.9641366004943848
Epoch 10, training loss: 10.318185806274414 = 1.9450944662094116 + 1.0 * 8.373091697692871
Epoch 10, val loss: 1.9528995752334595
Epoch 20, training loss: 10.299458503723145 = 1.9308249950408936 + 1.0 * 8.368633270263672
Epoch 20, val loss: 1.9382470846176147
Epoch 30, training loss: 10.250019073486328 = 1.9104098081588745 + 1.0 * 8.339609146118164
Epoch 30, val loss: 1.917375087738037
Epoch 40, training loss: 10.035009384155273 = 1.8854533433914185 + 1.0 * 8.149556159973145
Epoch 40, val loss: 1.8927624225616455
Epoch 50, training loss: 9.33790397644043 = 1.8591488599777222 + 1.0 * 7.478754997253418
Epoch 50, val loss: 1.8666654825210571
Epoch 60, training loss: 8.90500259399414 = 1.837989330291748 + 1.0 * 7.067012786865234
Epoch 60, val loss: 1.8476881980895996
Epoch 70, training loss: 8.57473373413086 = 1.8258814811706543 + 1.0 * 6.748852252960205
Epoch 70, val loss: 1.836737036705017
Epoch 80, training loss: 8.439017295837402 = 1.8126962184906006 + 1.0 * 6.626321315765381
Epoch 80, val loss: 1.8245006799697876
Epoch 90, training loss: 8.345965385437012 = 1.798531413078308 + 1.0 * 6.547434329986572
Epoch 90, val loss: 1.8113051652908325
Epoch 100, training loss: 8.257373809814453 = 1.7850565910339355 + 1.0 * 6.472317695617676
Epoch 100, val loss: 1.7988529205322266
Epoch 110, training loss: 8.173471450805664 = 1.773612380027771 + 1.0 * 6.399859428405762
Epoch 110, val loss: 1.7880452871322632
Epoch 120, training loss: 8.106623649597168 = 1.7617052793502808 + 1.0 * 6.344918251037598
Epoch 120, val loss: 1.7770919799804688
Epoch 130, training loss: 8.05468463897705 = 1.7474281787872314 + 1.0 * 6.307256698608398
Epoch 130, val loss: 1.7650126218795776
Epoch 140, training loss: 8.008479118347168 = 1.7303041219711304 + 1.0 * 6.278174877166748
Epoch 140, val loss: 1.7511399984359741
Epoch 150, training loss: 7.965839385986328 = 1.709434986114502 + 1.0 * 6.256404399871826
Epoch 150, val loss: 1.7344517707824707
Epoch 160, training loss: 7.923983097076416 = 1.683822751045227 + 1.0 * 6.2401604652404785
Epoch 160, val loss: 1.7140593528747559
Epoch 170, training loss: 7.877079963684082 = 1.6531107425689697 + 1.0 * 6.223969459533691
Epoch 170, val loss: 1.6896430253982544
Epoch 180, training loss: 7.826961994171143 = 1.615805745124817 + 1.0 * 6.211156368255615
Epoch 180, val loss: 1.6596965789794922
Epoch 190, training loss: 7.770455837249756 = 1.570612907409668 + 1.0 * 6.199842929840088
Epoch 190, val loss: 1.6232060194015503
Epoch 200, training loss: 7.710229873657227 = 1.5174816846847534 + 1.0 * 6.192748069763184
Epoch 200, val loss: 1.5804040431976318
Epoch 210, training loss: 7.639500141143799 = 1.4592007398605347 + 1.0 * 6.180299282073975
Epoch 210, val loss: 1.5338857173919678
Epoch 220, training loss: 7.569209098815918 = 1.3970423936843872 + 1.0 * 6.17216682434082
Epoch 220, val loss: 1.484284520149231
Epoch 230, training loss: 7.499864101409912 = 1.3325505256652832 + 1.0 * 6.167313575744629
Epoch 230, val loss: 1.4329060316085815
Epoch 240, training loss: 7.428950309753418 = 1.269081711769104 + 1.0 * 6.1598687171936035
Epoch 240, val loss: 1.3831068277359009
Epoch 250, training loss: 7.359137535095215 = 1.2071672677993774 + 1.0 * 6.151970386505127
Epoch 250, val loss: 1.33452308177948
Epoch 260, training loss: 7.294301509857178 = 1.1469792127609253 + 1.0 * 6.147322177886963
Epoch 260, val loss: 1.2876893281936646
Epoch 270, training loss: 7.231784343719482 = 1.0898770093917847 + 1.0 * 6.141907215118408
Epoch 270, val loss: 1.2435020208358765
Epoch 280, training loss: 7.172730445861816 = 1.036470651626587 + 1.0 * 6.136260032653809
Epoch 280, val loss: 1.2025467157363892
Epoch 290, training loss: 7.118771553039551 = 0.9865229725837708 + 1.0 * 6.132248401641846
Epoch 290, val loss: 1.1642532348632812
Epoch 300, training loss: 7.066112041473389 = 0.9393046498298645 + 1.0 * 6.12680721282959
Epoch 300, val loss: 1.1282435655593872
Epoch 310, training loss: 7.017994403839111 = 0.8942410349845886 + 1.0 * 6.123753547668457
Epoch 310, val loss: 1.094082236289978
Epoch 320, training loss: 6.976240158081055 = 0.8502478003501892 + 1.0 * 6.125992298126221
Epoch 320, val loss: 1.0605894327163696
Epoch 330, training loss: 6.925667762756348 = 0.8076865077018738 + 1.0 * 6.117981433868408
Epoch 330, val loss: 1.0283324718475342
Epoch 340, training loss: 6.879083633422852 = 0.7660305500030518 + 1.0 * 6.113052845001221
Epoch 340, val loss: 0.9972092509269714
Epoch 350, training loss: 6.834910869598389 = 0.7251964807510376 + 1.0 * 6.109714508056641
Epoch 350, val loss: 0.9667792320251465
Epoch 360, training loss: 6.792716979980469 = 0.6856873035430908 + 1.0 * 6.107029438018799
Epoch 360, val loss: 0.9376921057701111
Epoch 370, training loss: 6.751859664916992 = 0.6481716632843018 + 1.0 * 6.1036882400512695
Epoch 370, val loss: 0.9106379747390747
Epoch 380, training loss: 6.715731143951416 = 0.6128796935081482 + 1.0 * 6.102851390838623
Epoch 380, val loss: 0.8858417272567749
Epoch 390, training loss: 6.680045127868652 = 0.5803579092025757 + 1.0 * 6.099687099456787
Epoch 390, val loss: 0.8638778328895569
Epoch 400, training loss: 6.645652770996094 = 0.5505256056785583 + 1.0 * 6.095127105712891
Epoch 400, val loss: 0.8448729515075684
Epoch 410, training loss: 6.61561918258667 = 0.5230217576026917 + 1.0 * 6.092597484588623
Epoch 410, val loss: 0.8283398747444153
Epoch 420, training loss: 6.595123291015625 = 0.49777165055274963 + 1.0 * 6.097351551055908
Epoch 420, val loss: 0.8142340779304504
Epoch 430, training loss: 6.562455177307129 = 0.4746684432029724 + 1.0 * 6.087786674499512
Epoch 430, val loss: 0.8024943470954895
Epoch 440, training loss: 6.537017345428467 = 0.45314109325408936 + 1.0 * 6.083876132965088
Epoch 440, val loss: 0.7925613522529602
Epoch 450, training loss: 6.514928817749023 = 0.43289172649383545 + 1.0 * 6.082036972045898
Epoch 450, val loss: 0.7839426398277283
Epoch 460, training loss: 6.49227237701416 = 0.41384008526802063 + 1.0 * 6.078432083129883
Epoch 460, val loss: 0.7767131924629211
Epoch 470, training loss: 6.471243858337402 = 0.3955838680267334 + 1.0 * 6.075660228729248
Epoch 470, val loss: 0.7704400420188904
Epoch 480, training loss: 6.457536697387695 = 0.37790676951408386 + 1.0 * 6.079629898071289
Epoch 480, val loss: 0.7648571729660034
Epoch 490, training loss: 6.4371337890625 = 0.36092904210090637 + 1.0 * 6.076204776763916
Epoch 490, val loss: 0.7599784135818481
Epoch 500, training loss: 6.419789791107178 = 0.34468910098075867 + 1.0 * 6.075100898742676
Epoch 500, val loss: 0.7558839321136475
Epoch 510, training loss: 6.397954940795898 = 0.3289567530155182 + 1.0 * 6.068998336791992
Epoch 510, val loss: 0.7524171471595764
Epoch 520, training loss: 6.380540370941162 = 0.3136398196220398 + 1.0 * 6.066900730133057
Epoch 520, val loss: 0.7494461536407471
Epoch 530, training loss: 6.364294528961182 = 0.2987121343612671 + 1.0 * 6.065582275390625
Epoch 530, val loss: 0.7469997406005859
Epoch 540, training loss: 6.348843097686768 = 0.2842490077018738 + 1.0 * 6.064594268798828
Epoch 540, val loss: 0.7450348138809204
Epoch 550, training loss: 6.332071781158447 = 0.27033424377441406 + 1.0 * 6.061737537384033
Epoch 550, val loss: 0.7436507344245911
Epoch 560, training loss: 6.3177642822265625 = 0.2568109929561615 + 1.0 * 6.060953140258789
Epoch 560, val loss: 0.7427642941474915
Epoch 570, training loss: 6.3102569580078125 = 0.2437078207731247 + 1.0 * 6.066549301147461
Epoch 570, val loss: 0.74215167760849
Epoch 580, training loss: 6.2917962074279785 = 0.23114095628261566 + 1.0 * 6.060655117034912
Epoch 580, val loss: 0.7419322729110718
Epoch 590, training loss: 6.276360034942627 = 0.21899503469467163 + 1.0 * 6.0573649406433105
Epoch 590, val loss: 0.7422087788581848
Epoch 600, training loss: 6.26255989074707 = 0.20723043382167816 + 1.0 * 6.055329322814941
Epoch 600, val loss: 0.7427783012390137
Epoch 610, training loss: 6.256000995635986 = 0.19585159420967102 + 1.0 * 6.060149192810059
Epoch 610, val loss: 0.7436463236808777
Epoch 620, training loss: 6.241572856903076 = 0.18501721322536469 + 1.0 * 6.05655574798584
Epoch 620, val loss: 0.7446944117546082
Epoch 630, training loss: 6.2279953956604 = 0.17473141849040985 + 1.0 * 6.053264141082764
Epoch 630, val loss: 0.7461409568786621
Epoch 640, training loss: 6.216368675231934 = 0.16494418680667877 + 1.0 * 6.051424503326416
Epoch 640, val loss: 0.7479826807975769
Epoch 650, training loss: 6.20769739151001 = 0.15563717484474182 + 1.0 * 6.052060127258301
Epoch 650, val loss: 0.7500735521316528
Epoch 660, training loss: 6.197903156280518 = 0.14685538411140442 + 1.0 * 6.0510478019714355
Epoch 660, val loss: 0.7523102760314941
Epoch 670, training loss: 6.187595367431641 = 0.13859635591506958 + 1.0 * 6.048998832702637
Epoch 670, val loss: 0.7549235820770264
Epoch 680, training loss: 6.178231239318848 = 0.13079486787319183 + 1.0 * 6.047436237335205
Epoch 680, val loss: 0.7578694820404053
Epoch 690, training loss: 6.172293663024902 = 0.12345504760742188 + 1.0 * 6.0488386154174805
Epoch 690, val loss: 0.7608764171600342
Epoch 700, training loss: 6.168978214263916 = 0.11659538000822067 + 1.0 * 6.052382946014404
Epoch 700, val loss: 0.7640618681907654
Epoch 710, training loss: 6.155474662780762 = 0.11018087714910507 + 1.0 * 6.045293807983398
Epoch 710, val loss: 0.7675300240516663
Epoch 720, training loss: 6.147370338439941 = 0.10415396094322205 + 1.0 * 6.043216228485107
Epoch 720, val loss: 0.7712332606315613
Epoch 730, training loss: 6.140082359313965 = 0.09847034513950348 + 1.0 * 6.041612148284912
Epoch 730, val loss: 0.775092601776123
Epoch 740, training loss: 6.141663551330566 = 0.09312531352043152 + 1.0 * 6.0485382080078125
Epoch 740, val loss: 0.7790223956108093
Epoch 750, training loss: 6.132462978363037 = 0.08814765512943268 + 1.0 * 6.044315338134766
Epoch 750, val loss: 0.7830005288124084
Epoch 760, training loss: 6.122914791107178 = 0.08348750323057175 + 1.0 * 6.039427280426025
Epoch 760, val loss: 0.7872616052627563
Epoch 770, training loss: 6.118000030517578 = 0.07911592721939087 + 1.0 * 6.038884162902832
Epoch 770, val loss: 0.7915807962417603
Epoch 780, training loss: 6.117186546325684 = 0.075019471347332 + 1.0 * 6.0421671867370605
Epoch 780, val loss: 0.7958483099937439
Epoch 790, training loss: 6.110827922821045 = 0.07120320200920105 + 1.0 * 6.0396246910095215
Epoch 790, val loss: 0.8001866936683655
Epoch 800, training loss: 6.1049981117248535 = 0.06764204055070877 + 1.0 * 6.037355899810791
Epoch 800, val loss: 0.8047015070915222
Epoch 810, training loss: 6.0994954109191895 = 0.06430799514055252 + 1.0 * 6.035187244415283
Epoch 810, val loss: 0.8092951774597168
Epoch 820, training loss: 6.096627235412598 = 0.06117800995707512 + 1.0 * 6.035449028015137
Epoch 820, val loss: 0.8139153718948364
Epoch 830, training loss: 6.09737491607666 = 0.0582476407289505 + 1.0 * 6.039127349853516
Epoch 830, val loss: 0.8184292316436768
Epoch 840, training loss: 6.089995384216309 = 0.05551958829164505 + 1.0 * 6.034475803375244
Epoch 840, val loss: 0.8229603171348572
Epoch 850, training loss: 6.084387302398682 = 0.05296522006392479 + 1.0 * 6.031422138214111
Epoch 850, val loss: 0.8277589678764343
Epoch 860, training loss: 6.0818772315979 = 0.050557028502225876 + 1.0 * 6.031320095062256
Epoch 860, val loss: 0.8325123190879822
Epoch 870, training loss: 6.0826873779296875 = 0.04829367250204086 + 1.0 * 6.034393787384033
Epoch 870, val loss: 0.8370993137359619
Epoch 880, training loss: 6.082487106323242 = 0.04617705196142197 + 1.0 * 6.036310195922852
Epoch 880, val loss: 0.8417478799819946
Epoch 890, training loss: 6.075588703155518 = 0.04420223459601402 + 1.0 * 6.031386375427246
Epoch 890, val loss: 0.8464310765266418
Epoch 900, training loss: 6.069962024688721 = 0.042339809238910675 + 1.0 * 6.027622222900391
Epoch 900, val loss: 0.8512545228004456
Epoch 910, training loss: 6.067728519439697 = 0.040579769760370255 + 1.0 * 6.027148723602295
Epoch 910, val loss: 0.855977475643158
Epoch 920, training loss: 6.073784351348877 = 0.03891751170158386 + 1.0 * 6.034866809844971
Epoch 920, val loss: 0.8606392741203308
Epoch 930, training loss: 6.066133499145508 = 0.03735300153493881 + 1.0 * 6.028780460357666
Epoch 930, val loss: 0.8652396202087402
Epoch 940, training loss: 6.0622429847717285 = 0.035882897675037384 + 1.0 * 6.026360034942627
Epoch 940, val loss: 0.8700141906738281
Epoch 950, training loss: 6.061179161071777 = 0.034490786492824554 + 1.0 * 6.026688575744629
Epoch 950, val loss: 0.8747125864028931
Epoch 960, training loss: 6.061985969543457 = 0.033175770193338394 + 1.0 * 6.028810024261475
Epoch 960, val loss: 0.8793355822563171
Epoch 970, training loss: 6.05768346786499 = 0.031936001032590866 + 1.0 * 6.025747299194336
Epoch 970, val loss: 0.8838310241699219
Epoch 980, training loss: 6.053540229797363 = 0.030767006799578667 + 1.0 * 6.022773265838623
Epoch 980, val loss: 0.8885062336921692
Epoch 990, training loss: 6.052151203155518 = 0.029656626284122467 + 1.0 * 6.022494792938232
Epoch 990, val loss: 0.8931862115859985
Epoch 1000, training loss: 6.0544328689575195 = 0.02860090881586075 + 1.0 * 6.025832176208496
Epoch 1000, val loss: 0.8977343440055847
Epoch 1010, training loss: 6.050839424133301 = 0.027598511427640915 + 1.0 * 6.02324104309082
Epoch 1010, val loss: 0.9021174907684326
Epoch 1020, training loss: 6.053600311279297 = 0.026652641594409943 + 1.0 * 6.026947498321533
Epoch 1020, val loss: 0.9065205454826355
Epoch 1030, training loss: 6.047616958618164 = 0.02575697749853134 + 1.0 * 6.021860122680664
Epoch 1030, val loss: 0.9109290838241577
Epoch 1040, training loss: 6.043581008911133 = 0.024907240644097328 + 1.0 * 6.018673896789551
Epoch 1040, val loss: 0.9154195785522461
Epoch 1050, training loss: 6.0423665046691895 = 0.02409450151026249 + 1.0 * 6.0182719230651855
Epoch 1050, val loss: 0.9198510050773621
Epoch 1060, training loss: 6.046855926513672 = 0.023316988721489906 + 1.0 * 6.023539066314697
Epoch 1060, val loss: 0.9241575002670288
Epoch 1070, training loss: 6.04676628112793 = 0.022577084600925446 + 1.0 * 6.024188995361328
Epoch 1070, val loss: 0.9281089305877686
Epoch 1080, training loss: 6.039410591125488 = 0.021879535168409348 + 1.0 * 6.017530918121338
Epoch 1080, val loss: 0.9323105216026306
Epoch 1090, training loss: 6.038926601409912 = 0.021215710788965225 + 1.0 * 6.0177106857299805
Epoch 1090, val loss: 0.9367361664772034
Epoch 1100, training loss: 6.036075592041016 = 0.020578134804964066 + 1.0 * 6.01549768447876
Epoch 1100, val loss: 0.9409639239311218
Epoch 1110, training loss: 6.0371527671813965 = 0.019965311512351036 + 1.0 * 6.017187595367432
Epoch 1110, val loss: 0.9450716376304626
Epoch 1120, training loss: 6.037744998931885 = 0.01937835104763508 + 1.0 * 6.018366813659668
Epoch 1120, val loss: 0.9488734602928162
Epoch 1130, training loss: 6.035556316375732 = 0.0188228078186512 + 1.0 * 6.016733646392822
Epoch 1130, val loss: 0.9527981281280518
Epoch 1140, training loss: 6.036045551300049 = 0.018293989822268486 + 1.0 * 6.017751693725586
Epoch 1140, val loss: 0.9568803310394287
Epoch 1150, training loss: 6.031711101531982 = 0.017785869538784027 + 1.0 * 6.013925075531006
Epoch 1150, val loss: 0.9608713984489441
Epoch 1160, training loss: 6.036287784576416 = 0.017298070713877678 + 1.0 * 6.018989562988281
Epoch 1160, val loss: 0.9647987484931946
Epoch 1170, training loss: 6.030588150024414 = 0.016831055283546448 + 1.0 * 6.013757228851318
Epoch 1170, val loss: 0.9684476256370544
Epoch 1180, training loss: 6.0289716720581055 = 0.016384072601795197 + 1.0 * 6.012587547302246
Epoch 1180, val loss: 0.9723290205001831
Epoch 1190, training loss: 6.0277180671691895 = 0.015953993424773216 + 1.0 * 6.011764049530029
Epoch 1190, val loss: 0.9762164354324341
Epoch 1200, training loss: 6.028916358947754 = 0.015538590960204601 + 1.0 * 6.013377666473389
Epoch 1200, val loss: 0.9800019264221191
Epoch 1210, training loss: 6.030198097229004 = 0.015138491988182068 + 1.0 * 6.015059471130371
Epoch 1210, val loss: 0.983540415763855
Epoch 1220, training loss: 6.027546405792236 = 0.014758064411580563 + 1.0 * 6.01278829574585
Epoch 1220, val loss: 0.9871021509170532
Epoch 1230, training loss: 6.024995803833008 = 0.014393248595297337 + 1.0 * 6.0106024742126465
Epoch 1230, val loss: 0.9908616542816162
Epoch 1240, training loss: 6.02423095703125 = 0.014041191898286343 + 1.0 * 6.010189533233643
Epoch 1240, val loss: 0.9946057200431824
Epoch 1250, training loss: 6.025394439697266 = 0.013699895702302456 + 1.0 * 6.011694431304932
Epoch 1250, val loss: 0.9982185363769531
Epoch 1260, training loss: 6.026001930236816 = 0.013370504602789879 + 1.0 * 6.012631416320801
Epoch 1260, val loss: 1.0015639066696167
Epoch 1270, training loss: 6.022648334503174 = 0.013056169264018536 + 1.0 * 6.009592056274414
Epoch 1270, val loss: 1.004936933517456
Epoch 1280, training loss: 6.024447917938232 = 0.012754449620842934 + 1.0 * 6.011693477630615
Epoch 1280, val loss: 1.0084673166275024
Epoch 1290, training loss: 6.022851467132568 = 0.012462119571864605 + 1.0 * 6.01038932800293
Epoch 1290, val loss: 1.0117895603179932
Epoch 1300, training loss: 6.0219197273254395 = 0.012182089500129223 + 1.0 * 6.009737491607666
Epoch 1300, val loss: 1.015162706375122
Epoch 1310, training loss: 6.020849227905273 = 0.01191137358546257 + 1.0 * 6.008937835693359
Epoch 1310, val loss: 1.0185463428497314
Epoch 1320, training loss: 6.018427848815918 = 0.01164934504777193 + 1.0 * 6.006778717041016
Epoch 1320, val loss: 1.0219130516052246
Epoch 1330, training loss: 6.018025875091553 = 0.011395018547773361 + 1.0 * 6.006630897521973
Epoch 1330, val loss: 1.0252783298492432
Epoch 1340, training loss: 6.027985572814941 = 0.011147651821374893 + 1.0 * 6.016838073730469
Epoch 1340, val loss: 1.0284347534179688
Epoch 1350, training loss: 6.019587993621826 = 0.01091031264513731 + 1.0 * 6.0086774826049805
Epoch 1350, val loss: 1.0313972234725952
Epoch 1360, training loss: 6.01672887802124 = 0.01068328320980072 + 1.0 * 6.006045818328857
Epoch 1360, val loss: 1.0346274375915527
Epoch 1370, training loss: 6.016773700714111 = 0.010463092476129532 + 1.0 * 6.00631046295166
Epoch 1370, val loss: 1.0378913879394531
Epoch 1380, training loss: 6.021018981933594 = 0.010248481296002865 + 1.0 * 6.010770320892334
Epoch 1380, val loss: 1.0409579277038574
Epoch 1390, training loss: 6.017242431640625 = 0.010040477849543095 + 1.0 * 6.0072021484375
Epoch 1390, val loss: 1.0438674688339233
Epoch 1400, training loss: 6.01720666885376 = 0.009840395301580429 + 1.0 * 6.007366180419922
Epoch 1400, val loss: 1.0468941926956177
Epoch 1410, training loss: 6.0166120529174805 = 0.009646437130868435 + 1.0 * 6.006965637207031
Epoch 1410, val loss: 1.0499334335327148
Epoch 1420, training loss: 6.014355659484863 = 0.009458567947149277 + 1.0 * 6.004897117614746
Epoch 1420, val loss: 1.0528677701950073
Epoch 1430, training loss: 6.0131330490112305 = 0.009276732802391052 + 1.0 * 6.003856182098389
Epoch 1430, val loss: 1.055875539779663
Epoch 1440, training loss: 6.011702060699463 = 0.009099336341023445 + 1.0 * 6.002602577209473
Epoch 1440, val loss: 1.0588817596435547
Epoch 1450, training loss: 6.015807628631592 = 0.008926323615014553 + 1.0 * 6.006881237030029
Epoch 1450, val loss: 1.0617873668670654
Epoch 1460, training loss: 6.013544082641602 = 0.008757694624364376 + 1.0 * 6.004786491394043
Epoch 1460, val loss: 1.0642443895339966
Epoch 1470, training loss: 6.013429164886475 = 0.008597595617175102 + 1.0 * 6.004831790924072
Epoch 1470, val loss: 1.0670452117919922
Epoch 1480, training loss: 6.011337757110596 = 0.008442547172307968 + 1.0 * 6.002895355224609
Epoch 1480, val loss: 1.070111870765686
Epoch 1490, training loss: 6.017782688140869 = 0.008290105499327183 + 1.0 * 6.00949239730835
Epoch 1490, val loss: 1.0729082822799683
Epoch 1500, training loss: 6.011737823486328 = 0.008141866885125637 + 1.0 * 6.00359582901001
Epoch 1500, val loss: 1.0754951238632202
Epoch 1510, training loss: 6.009526252746582 = 0.0079986322671175 + 1.0 * 6.001527786254883
Epoch 1510, val loss: 1.0783636569976807
Epoch 1520, training loss: 6.009030818939209 = 0.007858160883188248 + 1.0 * 6.0011725425720215
Epoch 1520, val loss: 1.0811817646026611
Epoch 1530, training loss: 6.015514373779297 = 0.007720871362835169 + 1.0 * 6.007793426513672
Epoch 1530, val loss: 1.0837935209274292
Epoch 1540, training loss: 6.014386177062988 = 0.007587785832583904 + 1.0 * 6.006798267364502
Epoch 1540, val loss: 1.0862784385681152
Epoch 1550, training loss: 6.009119987487793 = 0.007459618616849184 + 1.0 * 6.001660346984863
Epoch 1550, val loss: 1.0887701511383057
Epoch 1560, training loss: 6.0068488121032715 = 0.007335952017456293 + 1.0 * 5.999512672424316
Epoch 1560, val loss: 1.091522216796875
Epoch 1570, training loss: 6.006473064422607 = 0.007215011399239302 + 1.0 * 5.999258041381836
Epoch 1570, val loss: 1.0943058729171753
Epoch 1580, training loss: 6.006412982940674 = 0.0070954845286905766 + 1.0 * 5.999317646026611
Epoch 1580, val loss: 1.096925973892212
Epoch 1590, training loss: 6.016348838806152 = 0.006978177931159735 + 1.0 * 6.009370803833008
Epoch 1590, val loss: 1.0992910861968994
Epoch 1600, training loss: 6.007315635681152 = 0.00686574075371027 + 1.0 * 6.0004496574401855
Epoch 1600, val loss: 1.1016111373901367
Epoch 1610, training loss: 6.005085468292236 = 0.006756864488124847 + 1.0 * 5.998328685760498
Epoch 1610, val loss: 1.1042864322662354
Epoch 1620, training loss: 6.0062432289123535 = 0.006649982184171677 + 1.0 * 5.999593257904053
Epoch 1620, val loss: 1.106872797012329
Epoch 1630, training loss: 6.009342670440674 = 0.006544632837176323 + 1.0 * 6.002798080444336
Epoch 1630, val loss: 1.1090993881225586
Epoch 1640, training loss: 6.006461143493652 = 0.006443310063332319 + 1.0 * 6.0000176429748535
Epoch 1640, val loss: 1.1114240884780884
Epoch 1650, training loss: 6.005298614501953 = 0.006345425266772509 + 1.0 * 5.998953342437744
Epoch 1650, val loss: 1.1139086484909058
Epoch 1660, training loss: 6.004745006561279 = 0.006249226629734039 + 1.0 * 5.998495578765869
Epoch 1660, val loss: 1.1163434982299805
Epoch 1670, training loss: 6.0072150230407715 = 0.006154804956167936 + 1.0 * 6.0010600090026855
Epoch 1670, val loss: 1.118606686592102
Epoch 1680, training loss: 6.004545211791992 = 0.006063114386051893 + 1.0 * 5.9984822273254395
Epoch 1680, val loss: 1.1209152936935425
Epoch 1690, training loss: 6.00449800491333 = 0.005973785184323788 + 1.0 * 5.998524188995361
Epoch 1690, val loss: 1.1233094930648804
Epoch 1700, training loss: 6.0061540603637695 = 0.0058858213014900684 + 1.0 * 6.000268459320068
Epoch 1700, val loss: 1.1254868507385254
Epoch 1710, training loss: 6.002827167510986 = 0.005800346843898296 + 1.0 * 5.9970269203186035
Epoch 1710, val loss: 1.1276936531066895
Epoch 1720, training loss: 6.0011773109436035 = 0.005717501975595951 + 1.0 * 5.995460033416748
Epoch 1720, val loss: 1.130063772201538
Epoch 1730, training loss: 6.002903938293457 = 0.005635533947497606 + 1.0 * 5.997268199920654
Epoch 1730, val loss: 1.1323894262313843
Epoch 1740, training loss: 6.006287097930908 = 0.005554751493036747 + 1.0 * 6.000732421875
Epoch 1740, val loss: 1.134376049041748
Epoch 1750, training loss: 6.00336217880249 = 0.005477013532072306 + 1.0 * 5.997885227203369
Epoch 1750, val loss: 1.1363972425460815
Epoch 1760, training loss: 6.0012640953063965 = 0.005402145907282829 + 1.0 * 5.995862007141113
Epoch 1760, val loss: 1.1386691331863403
Epoch 1770, training loss: 6.000354290008545 = 0.005328553728759289 + 1.0 * 5.995025634765625
Epoch 1770, val loss: 1.1410465240478516
Epoch 1780, training loss: 6.006429195404053 = 0.005255625583231449 + 1.0 * 6.001173496246338
Epoch 1780, val loss: 1.1432353258132935
Epoch 1790, training loss: 6.003643035888672 = 0.005183759611099958 + 1.0 * 5.998459339141846
Epoch 1790, val loss: 1.1450891494750977
Epoch 1800, training loss: 6.000658988952637 = 0.005114519037306309 + 1.0 * 5.99554443359375
Epoch 1800, val loss: 1.1471949815750122
Epoch 1810, training loss: 5.998960494995117 = 0.005047090817242861 + 1.0 * 5.993913173675537
Epoch 1810, val loss: 1.1494532823562622
Epoch 1820, training loss: 6.000717639923096 = 0.004980168770998716 + 1.0 * 5.995737552642822
Epoch 1820, val loss: 1.1516532897949219
Epoch 1830, training loss: 6.001598834991455 = 0.004913909826427698 + 1.0 * 5.996685028076172
Epoch 1830, val loss: 1.153558611869812
Epoch 1840, training loss: 6.003883361816406 = 0.004849887453019619 + 1.0 * 5.999033451080322
Epoch 1840, val loss: 1.155524492263794
Epoch 1850, training loss: 6.005673885345459 = 0.004787903279066086 + 1.0 * 6.000885963439941
Epoch 1850, val loss: 1.1575031280517578
Epoch 1860, training loss: 5.998802185058594 = 0.004727164749056101 + 1.0 * 5.994074821472168
Epoch 1860, val loss: 1.1595443487167358
Epoch 1870, training loss: 5.998291969299316 = 0.004667971283197403 + 1.0 * 5.993624210357666
Epoch 1870, val loss: 1.1617074012756348
Epoch 1880, training loss: 6.0039520263671875 = 0.00460911076515913 + 1.0 * 5.999342918395996
Epoch 1880, val loss: 1.1636911630630493
Epoch 1890, training loss: 5.998334884643555 = 0.004551292397081852 + 1.0 * 5.993783473968506
Epoch 1890, val loss: 1.165568470954895
Epoch 1900, training loss: 5.996514797210693 = 0.004495004191994667 + 1.0 * 5.9920196533203125
Epoch 1900, val loss: 1.1676450967788696
Epoch 1910, training loss: 5.997154712677002 = 0.004439315292984247 + 1.0 * 5.992715358734131
Epoch 1910, val loss: 1.1696827411651611
Epoch 1920, training loss: 6.001898765563965 = 0.004384429659694433 + 1.0 * 5.997514247894287
Epoch 1920, val loss: 1.1714285612106323
Epoch 1930, training loss: 5.9962310791015625 = 0.004331178497523069 + 1.0 * 5.9918999671936035
Epoch 1930, val loss: 1.1732381582260132
Epoch 1940, training loss: 5.995892524719238 = 0.00427997438237071 + 1.0 * 5.991612434387207
Epoch 1940, val loss: 1.1753355264663696
Epoch 1950, training loss: 6.002222537994385 = 0.004229047801345587 + 1.0 * 5.997993469238281
Epoch 1950, val loss: 1.1771986484527588
Epoch 1960, training loss: 5.9970269203186035 = 0.0041789710521698 + 1.0 * 5.992847919464111
Epoch 1960, val loss: 1.1790305376052856
Epoch 1970, training loss: 5.998305320739746 = 0.004130383487790823 + 1.0 * 5.994174957275391
Epoch 1970, val loss: 1.1809946298599243
Epoch 1980, training loss: 5.995595455169678 = 0.004082106985151768 + 1.0 * 5.991513252258301
Epoch 1980, val loss: 1.182724118232727
Epoch 1990, training loss: 5.99497127532959 = 0.004035341087728739 + 1.0 * 5.990935802459717
Epoch 1990, val loss: 1.1846028566360474
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6679
Flip ASR: 0.6044/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.331353187561035 = 1.957576036453247 + 1.0 * 8.373777389526367
Epoch 0, val loss: 1.953989028930664
Epoch 10, training loss: 10.319863319396973 = 1.9467198848724365 + 1.0 * 8.373143196105957
Epoch 10, val loss: 1.9434504508972168
Epoch 20, training loss: 10.3021240234375 = 1.9333220720291138 + 1.0 * 8.368802070617676
Epoch 20, val loss: 1.9298920631408691
Epoch 30, training loss: 10.253188133239746 = 1.914794921875 + 1.0 * 8.338393211364746
Epoch 30, val loss: 1.9106791019439697
Epoch 40, training loss: 10.034366607666016 = 1.892368197441101 + 1.0 * 8.141998291015625
Epoch 40, val loss: 1.8879454135894775
Epoch 50, training loss: 9.44017505645752 = 1.8688586950302124 + 1.0 * 7.571316242218018
Epoch 50, val loss: 1.8639968633651733
Epoch 60, training loss: 9.040489196777344 = 1.8503769636154175 + 1.0 * 7.190112590789795
Epoch 60, val loss: 1.8465712070465088
Epoch 70, training loss: 8.63710880279541 = 1.838921308517456 + 1.0 * 6.798187255859375
Epoch 70, val loss: 1.8358210325241089
Epoch 80, training loss: 8.396771430969238 = 1.8292115926742554 + 1.0 * 6.567559719085693
Epoch 80, val loss: 1.8266521692276
Epoch 90, training loss: 8.250008583068848 = 1.8178861141204834 + 1.0 * 6.432122707366943
Epoch 90, val loss: 1.8155498504638672
Epoch 100, training loss: 8.145469665527344 = 1.8067857027053833 + 1.0 * 6.338683605194092
Epoch 100, val loss: 1.8048975467681885
Epoch 110, training loss: 8.085336685180664 = 1.7968374490737915 + 1.0 * 6.288499355316162
Epoch 110, val loss: 1.795626163482666
Epoch 120, training loss: 8.045055389404297 = 1.786720871925354 + 1.0 * 6.258334159851074
Epoch 120, val loss: 1.7863988876342773
Epoch 130, training loss: 8.0104341506958 = 1.7759603261947632 + 1.0 * 6.234474182128906
Epoch 130, val loss: 1.7767351865768433
Epoch 140, training loss: 7.9805450439453125 = 1.764461874961853 + 1.0 * 6.21608304977417
Epoch 140, val loss: 1.7666640281677246
Epoch 150, training loss: 7.948178291320801 = 1.751819372177124 + 1.0 * 6.196359157562256
Epoch 150, val loss: 1.7558661699295044
Epoch 160, training loss: 7.918725967407227 = 1.737207293510437 + 1.0 * 6.1815185546875
Epoch 160, val loss: 1.743739366531372
Epoch 170, training loss: 7.89027214050293 = 1.719807744026184 + 1.0 * 6.170464515686035
Epoch 170, val loss: 1.7294864654541016
Epoch 180, training loss: 7.858492374420166 = 1.6992429494857788 + 1.0 * 6.159249305725098
Epoch 180, val loss: 1.7127928733825684
Epoch 190, training loss: 7.825360298156738 = 1.6748310327529907 + 1.0 * 6.150529384613037
Epoch 190, val loss: 1.6929649114608765
Epoch 200, training loss: 7.788412094116211 = 1.6456385850906372 + 1.0 * 6.142773628234863
Epoch 200, val loss: 1.6692744493484497
Epoch 210, training loss: 7.748457431793213 = 1.6113214492797852 + 1.0 * 6.137135982513428
Epoch 210, val loss: 1.6414493322372437
Epoch 220, training loss: 7.702532768249512 = 1.57197904586792 + 1.0 * 6.130553722381592
Epoch 220, val loss: 1.60955810546875
Epoch 230, training loss: 7.653195381164551 = 1.5275511741638184 + 1.0 * 6.125644207000732
Epoch 230, val loss: 1.5738056898117065
Epoch 240, training loss: 7.5999860763549805 = 1.478621244430542 + 1.0 * 6.121364593505859
Epoch 240, val loss: 1.5348491668701172
Epoch 250, training loss: 7.545284748077393 = 1.4265170097351074 + 1.0 * 6.118767738342285
Epoch 250, val loss: 1.4938396215438843
Epoch 260, training loss: 7.486462116241455 = 1.372179388999939 + 1.0 * 6.114282608032227
Epoch 260, val loss: 1.4517055749893188
Epoch 270, training loss: 7.428706169128418 = 1.316420555114746 + 1.0 * 6.112285614013672
Epoch 270, val loss: 1.4090245962142944
Epoch 280, training loss: 7.367580413818359 = 1.260056972503662 + 1.0 * 6.107523441314697
Epoch 280, val loss: 1.3664988279342651
Epoch 290, training loss: 7.308781147003174 = 1.2034317255020142 + 1.0 * 6.105349540710449
Epoch 290, val loss: 1.3242560625076294
Epoch 300, training loss: 7.247129440307617 = 1.146907091140747 + 1.0 * 6.100222587585449
Epoch 300, val loss: 1.2822500467300415
Epoch 310, training loss: 7.189655780792236 = 1.0901976823806763 + 1.0 * 6.09945821762085
Epoch 310, val loss: 1.2402948141098022
Epoch 320, training loss: 7.12974214553833 = 1.0344053506851196 + 1.0 * 6.0953369140625
Epoch 320, val loss: 1.199102759361267
Epoch 330, training loss: 7.072010517120361 = 0.9800443053245544 + 1.0 * 6.091966152191162
Epoch 330, val loss: 1.1591722965240479
Epoch 340, training loss: 7.0197062492370605 = 0.9267721772193909 + 1.0 * 6.0929341316223145
Epoch 340, val loss: 1.1201921701431274
Epoch 350, training loss: 6.963685989379883 = 0.8755953907966614 + 1.0 * 6.088090419769287
Epoch 350, val loss: 1.0827853679656982
Epoch 360, training loss: 6.910673141479492 = 0.8265668153762817 + 1.0 * 6.0841064453125
Epoch 360, val loss: 1.0472921133041382
Epoch 370, training loss: 6.867361545562744 = 0.7793432474136353 + 1.0 * 6.088018417358398
Epoch 370, val loss: 1.0132277011871338
Epoch 380, training loss: 6.814929008483887 = 0.7345290184020996 + 1.0 * 6.080399990081787
Epoch 380, val loss: 0.9810636043548584
Epoch 390, training loss: 6.769833087921143 = 0.6917847394943237 + 1.0 * 6.078048229217529
Epoch 390, val loss: 0.9506834745407104
Epoch 400, training loss: 6.726860523223877 = 0.650957465171814 + 1.0 * 6.075902938842773
Epoch 400, val loss: 0.9219335317611694
Epoch 410, training loss: 6.693321228027344 = 0.6124891638755798 + 1.0 * 6.080832004547119
Epoch 410, val loss: 0.8949446082115173
Epoch 420, training loss: 6.652848243713379 = 0.5771253108978271 + 1.0 * 6.075722694396973
Epoch 420, val loss: 0.8705624938011169
Epoch 430, training loss: 6.614078998565674 = 0.5441378355026245 + 1.0 * 6.06994104385376
Epoch 430, val loss: 0.8483135104179382
Epoch 440, training loss: 6.581541538238525 = 0.513199508190155 + 1.0 * 6.068342208862305
Epoch 440, val loss: 0.8278189301490784
Epoch 450, training loss: 6.556216716766357 = 0.4843299090862274 + 1.0 * 6.071887016296387
Epoch 450, val loss: 0.809121310710907
Epoch 460, training loss: 6.524231433868408 = 0.45759809017181396 + 1.0 * 6.066633224487305
Epoch 460, val loss: 0.7923596501350403
Epoch 470, training loss: 6.495511531829834 = 0.43243181705474854 + 1.0 * 6.063079833984375
Epoch 470, val loss: 0.7770765423774719
Epoch 480, training loss: 6.474173069000244 = 0.4086032211780548 + 1.0 * 6.065569877624512
Epoch 480, val loss: 0.7629985809326172
Epoch 490, training loss: 6.4468302726745605 = 0.386138916015625 + 1.0 * 6.0606913566589355
Epoch 490, val loss: 0.7502287030220032
Epoch 500, training loss: 6.422654628753662 = 0.3648768663406372 + 1.0 * 6.0577778816223145
Epoch 500, val loss: 0.7386528253555298
Epoch 510, training loss: 6.400717258453369 = 0.34444695711135864 + 1.0 * 6.056270122528076
Epoch 510, val loss: 0.728029727935791
Epoch 520, training loss: 6.384601593017578 = 0.32467418909072876 + 1.0 * 6.059927463531494
Epoch 520, val loss: 0.7182067632675171
Epoch 530, training loss: 6.363604545593262 = 0.30570030212402344 + 1.0 * 6.057904243469238
Epoch 530, val loss: 0.7092997431755066
Epoch 540, training loss: 6.340436935424805 = 0.2874026298522949 + 1.0 * 6.05303430557251
Epoch 540, val loss: 0.7013630867004395
Epoch 550, training loss: 6.321445465087891 = 0.26974231004714966 + 1.0 * 6.051702976226807
Epoch 550, val loss: 0.6943377256393433
Epoch 560, training loss: 6.302914142608643 = 0.2526651918888092 + 1.0 * 6.050249099731445
Epoch 560, val loss: 0.6881526112556458
Epoch 570, training loss: 6.286710262298584 = 0.23631730675697327 + 1.0 * 6.050393104553223
Epoch 570, val loss: 0.6827574968338013
Epoch 580, training loss: 6.269041061401367 = 0.22081220149993896 + 1.0 * 6.048228740692139
Epoch 580, val loss: 0.6782771348953247
Epoch 590, training loss: 6.2522053718566895 = 0.2061273753643036 + 1.0 * 6.046078205108643
Epoch 590, val loss: 0.6747910976409912
Epoch 600, training loss: 6.247442722320557 = 0.1923026591539383 + 1.0 * 6.055140018463135
Epoch 600, val loss: 0.6722275614738464
Epoch 610, training loss: 6.2250847816467285 = 0.17950665950775146 + 1.0 * 6.0455780029296875
Epoch 610, val loss: 0.6705396771430969
Epoch 620, training loss: 6.214120388031006 = 0.16768354177474976 + 1.0 * 6.046436786651611
Epoch 620, val loss: 0.6697795987129211
Epoch 630, training loss: 6.199209213256836 = 0.15676209330558777 + 1.0 * 6.042447090148926
Epoch 630, val loss: 0.6698060035705566
Epoch 640, training loss: 6.187798976898193 = 0.14671164751052856 + 1.0 * 6.0410871505737305
Epoch 640, val loss: 0.6705358624458313
Epoch 650, training loss: 6.186757564544678 = 0.13742130994796753 + 1.0 * 6.0493364334106445
Epoch 650, val loss: 0.6719512939453125
Epoch 660, training loss: 6.170848846435547 = 0.12894749641418457 + 1.0 * 6.041901588439941
Epoch 660, val loss: 0.6738410592079163
Epoch 670, training loss: 6.159040451049805 = 0.12113992124795914 + 1.0 * 6.037900447845459
Epoch 670, val loss: 0.6761448979377747
Epoch 680, training loss: 6.158181190490723 = 0.11392652243375778 + 1.0 * 6.044254779815674
Epoch 680, val loss: 0.6789354681968689
Epoch 690, training loss: 6.14615535736084 = 0.10732726752758026 + 1.0 * 6.038827896118164
Epoch 690, val loss: 0.6820850968360901
Epoch 700, training loss: 6.135973930358887 = 0.10124421119689941 + 1.0 * 6.034729957580566
Epoch 700, val loss: 0.6854705214500427
Epoch 710, training loss: 6.132787227630615 = 0.09561093896627426 + 1.0 * 6.037176132202148
Epoch 710, val loss: 0.6891531348228455
Epoch 720, training loss: 6.124766826629639 = 0.09041544049978256 + 1.0 * 6.034351348876953
Epoch 720, val loss: 0.6929768919944763
Epoch 730, training loss: 6.118340015411377 = 0.08562739938497543 + 1.0 * 6.032712459564209
Epoch 730, val loss: 0.6969276666641235
Epoch 740, training loss: 6.119568347930908 = 0.08118399977684021 + 1.0 * 6.038384437561035
Epoch 740, val loss: 0.7010372877120972
Epoch 750, training loss: 6.110325336456299 = 0.07708186656236649 + 1.0 * 6.033243656158447
Epoch 750, val loss: 0.7052050828933716
Epoch 760, training loss: 6.103458404541016 = 0.07327508181333542 + 1.0 * 6.0301833152771
Epoch 760, val loss: 0.7094264030456543
Epoch 770, training loss: 6.09931755065918 = 0.06972569227218628 + 1.0 * 6.029592037200928
Epoch 770, val loss: 0.7137529253959656
Epoch 780, training loss: 6.0976128578186035 = 0.0664275512099266 + 1.0 * 6.031185150146484
Epoch 780, val loss: 0.7181665301322937
Epoch 790, training loss: 6.092525959014893 = 0.06337115913629532 + 1.0 * 6.0291547775268555
Epoch 790, val loss: 0.722469687461853
Epoch 800, training loss: 6.086546421051025 = 0.06051746755838394 + 1.0 * 6.026029109954834
Epoch 800, val loss: 0.7268244624137878
Epoch 810, training loss: 6.0852251052856445 = 0.05783412232995033 + 1.0 * 6.027390956878662
Epoch 810, val loss: 0.7312751412391663
Epoch 820, training loss: 6.081927299499512 = 0.05532107874751091 + 1.0 * 6.02660608291626
Epoch 820, val loss: 0.7357723116874695
Epoch 830, training loss: 6.080665588378906 = 0.052980344742536545 + 1.0 * 6.027685165405273
Epoch 830, val loss: 0.7401944398880005
Epoch 840, training loss: 6.075619220733643 = 0.050782185047864914 + 1.0 * 6.024837017059326
Epoch 840, val loss: 0.7445753812789917
Epoch 850, training loss: 6.071901798248291 = 0.04871416091918945 + 1.0 * 6.023187637329102
Epoch 850, val loss: 0.7489740252494812
Epoch 860, training loss: 6.069014072418213 = 0.046761829406023026 + 1.0 * 6.022252082824707
Epoch 860, val loss: 0.7533844709396362
Epoch 870, training loss: 6.071903705596924 = 0.04491659998893738 + 1.0 * 6.026987075805664
Epoch 870, val loss: 0.7578169107437134
Epoch 880, training loss: 6.06851863861084 = 0.04317738115787506 + 1.0 * 6.025341033935547
Epoch 880, val loss: 0.7621628046035767
Epoch 890, training loss: 6.070910453796387 = 0.04154692962765694 + 1.0 * 6.029363632202148
Epoch 890, val loss: 0.7665035128593445
Epoch 900, training loss: 6.061415195465088 = 0.04001280292868614 + 1.0 * 6.021402359008789
Epoch 900, val loss: 0.7706751823425293
Epoch 910, training loss: 6.057863235473633 = 0.038563139736652374 + 1.0 * 6.0192999839782715
Epoch 910, val loss: 0.7748557329177856
Epoch 920, training loss: 6.055200576782227 = 0.03717679902911186 + 1.0 * 6.01802396774292
Epoch 920, val loss: 0.7790794372558594
Epoch 930, training loss: 6.068177700042725 = 0.03585882857441902 + 1.0 * 6.032319068908691
Epoch 930, val loss: 0.7833443284034729
Epoch 940, training loss: 6.052331924438477 = 0.03461359068751335 + 1.0 * 6.017718315124512
Epoch 940, val loss: 0.7874974012374878
Epoch 950, training loss: 6.052759170532227 = 0.03343798965215683 + 1.0 * 6.019320964813232
Epoch 950, val loss: 0.791523277759552
Epoch 960, training loss: 6.0530195236206055 = 0.03231770545244217 + 1.0 * 6.020701885223389
Epoch 960, val loss: 0.7955899238586426
Epoch 970, training loss: 6.048036098480225 = 0.03124738670885563 + 1.0 * 6.016788482666016
Epoch 970, val loss: 0.7995925545692444
Epoch 980, training loss: 6.046132564544678 = 0.030228950083255768 + 1.0 * 6.015903472900391
Epoch 980, val loss: 0.8035959005355835
Epoch 990, training loss: 6.048420429229736 = 0.029256360605359077 + 1.0 * 6.019164085388184
Epoch 990, val loss: 0.8075720071792603
Epoch 1000, training loss: 6.043981552124023 = 0.028333205729722977 + 1.0 * 6.015648365020752
Epoch 1000, val loss: 0.8114989399909973
Epoch 1010, training loss: 6.042267799377441 = 0.027453729882836342 + 1.0 * 6.0148138999938965
Epoch 1010, val loss: 0.8153732419013977
Epoch 1020, training loss: 6.041816711425781 = 0.02661319635808468 + 1.0 * 6.015203475952148
Epoch 1020, val loss: 0.8192250728607178
Epoch 1030, training loss: 6.040552616119385 = 0.025809142738580704 + 1.0 * 6.014743328094482
Epoch 1030, val loss: 0.8230602145195007
Epoch 1040, training loss: 6.042973041534424 = 0.025041723623871803 + 1.0 * 6.0179314613342285
Epoch 1040, val loss: 0.8268100023269653
Epoch 1050, training loss: 6.0364990234375 = 0.024310428649187088 + 1.0 * 6.01218843460083
Epoch 1050, val loss: 0.8305337429046631
Epoch 1060, training loss: 6.035048007965088 = 0.02361253835260868 + 1.0 * 6.011435508728027
Epoch 1060, val loss: 0.8341833353042603
Epoch 1070, training loss: 6.037073612213135 = 0.022939613088965416 + 1.0 * 6.014133930206299
Epoch 1070, val loss: 0.8378600478172302
Epoch 1080, training loss: 6.035529136657715 = 0.02229471504688263 + 1.0 * 6.013234615325928
Epoch 1080, val loss: 0.8414809107780457
Epoch 1090, training loss: 6.0310258865356445 = 0.021679600700736046 + 1.0 * 6.0093464851379395
Epoch 1090, val loss: 0.8450984954833984
Epoch 1100, training loss: 6.033761978149414 = 0.02108907327055931 + 1.0 * 6.0126729011535645
Epoch 1100, val loss: 0.848637044429779
Epoch 1110, training loss: 6.030213832855225 = 0.020519457757472992 + 1.0 * 6.009694576263428
Epoch 1110, val loss: 0.8522133827209473
Epoch 1120, training loss: 6.033324718475342 = 0.019975129514932632 + 1.0 * 6.013349533081055
Epoch 1120, val loss: 0.8557272553443909
Epoch 1130, training loss: 6.028682231903076 = 0.019452378153800964 + 1.0 * 6.00922966003418
Epoch 1130, val loss: 0.859240710735321
Epoch 1140, training loss: 6.02743673324585 = 0.0189528726041317 + 1.0 * 6.00848388671875
Epoch 1140, val loss: 0.8626574873924255
Epoch 1150, training loss: 6.025173664093018 = 0.01846879906952381 + 1.0 * 6.006704807281494
Epoch 1150, val loss: 0.8660462498664856
Epoch 1160, training loss: 6.035549163818359 = 0.018001005053520203 + 1.0 * 6.017548084259033
Epoch 1160, val loss: 0.8694879412651062
Epoch 1170, training loss: 6.027395725250244 = 0.017552785575389862 + 1.0 * 6.009842872619629
Epoch 1170, val loss: 0.8728941082954407
Epoch 1180, training loss: 6.02306604385376 = 0.017125235870480537 + 1.0 * 6.005940914154053
Epoch 1180, val loss: 0.8761420249938965
Epoch 1190, training loss: 6.031611442565918 = 0.016712818294763565 + 1.0 * 6.014898777008057
Epoch 1190, val loss: 0.8794556856155396
Epoch 1200, training loss: 6.026665687561035 = 0.016312452033162117 + 1.0 * 6.010353088378906
Epoch 1200, val loss: 0.882718026638031
Epoch 1210, training loss: 6.021268367767334 = 0.015932941809296608 + 1.0 * 6.005335330963135
Epoch 1210, val loss: 0.8858588933944702
Epoch 1220, training loss: 6.0201005935668945 = 0.015563687309622765 + 1.0 * 6.004537105560303
Epoch 1220, val loss: 0.8889825344085693
Epoch 1230, training loss: 6.033336639404297 = 0.015204469673335552 + 1.0 * 6.018132209777832
Epoch 1230, val loss: 0.8922139406204224
Epoch 1240, training loss: 6.020440101623535 = 0.01486229244619608 + 1.0 * 6.00557804107666
Epoch 1240, val loss: 0.8953611850738525
Epoch 1250, training loss: 6.017556667327881 = 0.014533099718391895 + 1.0 * 6.003023624420166
Epoch 1250, val loss: 0.8982992768287659
Epoch 1260, training loss: 6.016803741455078 = 0.014214348047971725 + 1.0 * 6.002589225769043
Epoch 1260, val loss: 0.9013036489486694
Epoch 1270, training loss: 6.021595478057861 = 0.01390131190419197 + 1.0 * 6.007694244384766
Epoch 1270, val loss: 0.9044233560562134
Epoch 1280, training loss: 6.01768159866333 = 0.013598097488284111 + 1.0 * 6.004083633422852
Epoch 1280, val loss: 0.9074362516403198
Epoch 1290, training loss: 6.015007495880127 = 0.013309989124536514 + 1.0 * 6.001697540283203
Epoch 1290, val loss: 0.9103845953941345
Epoch 1300, training loss: 6.015401363372803 = 0.013029183261096478 + 1.0 * 6.0023722648620605
Epoch 1300, val loss: 0.9132863283157349
Epoch 1310, training loss: 6.018754959106445 = 0.01275576651096344 + 1.0 * 6.0059990882873535
Epoch 1310, val loss: 0.9162912964820862
Epoch 1320, training loss: 6.014328956604004 = 0.012491179630160332 + 1.0 * 6.001837730407715
Epoch 1320, val loss: 0.9191755056381226
Epoch 1330, training loss: 6.016112804412842 = 0.012238004244863987 + 1.0 * 6.003874778747559
Epoch 1330, val loss: 0.9219622611999512
Epoch 1340, training loss: 6.01364278793335 = 0.011992374435067177 + 1.0 * 6.001650333404541
Epoch 1340, val loss: 0.924805760383606
Epoch 1350, training loss: 6.011703968048096 = 0.011754583567380905 + 1.0 * 5.9999494552612305
Epoch 1350, val loss: 0.9275615215301514
Epoch 1360, training loss: 6.011740684509277 = 0.011522598564624786 + 1.0 * 6.000217914581299
Epoch 1360, val loss: 0.9303223490715027
Epoch 1370, training loss: 6.012150287628174 = 0.011295702308416367 + 1.0 * 6.0008544921875
Epoch 1370, val loss: 0.9331332445144653
Epoch 1380, training loss: 6.012546062469482 = 0.011075932532548904 + 1.0 * 6.00147008895874
Epoch 1380, val loss: 0.935913622379303
Epoch 1390, training loss: 6.010369777679443 = 0.01086292415857315 + 1.0 * 5.999506950378418
Epoch 1390, val loss: 0.938596785068512
Epoch 1400, training loss: 6.011590480804443 = 0.010657502338290215 + 1.0 * 6.0009331703186035
Epoch 1400, val loss: 0.9412714242935181
Epoch 1410, training loss: 6.011632442474365 = 0.01045780349522829 + 1.0 * 6.001174449920654
Epoch 1410, val loss: 0.9439322352409363
Epoch 1420, training loss: 6.008009433746338 = 0.010264414362609386 + 1.0 * 5.997745037078857
Epoch 1420, val loss: 0.9465593695640564
Epoch 1430, training loss: 6.007187843322754 = 0.010077261365950108 + 1.0 * 5.997110366821289
Epoch 1430, val loss: 0.9490647912025452
Epoch 1440, training loss: 6.0064520835876465 = 0.009894518181681633 + 1.0 * 5.996557712554932
Epoch 1440, val loss: 0.9516498446464539
Epoch 1450, training loss: 6.010905742645264 = 0.009714400395751 + 1.0 * 6.001191139221191
Epoch 1450, val loss: 0.9542953968048096
Epoch 1460, training loss: 6.006011486053467 = 0.009540305472910404 + 1.0 * 5.996471405029297
Epoch 1460, val loss: 0.956912100315094
Epoch 1470, training loss: 6.00692081451416 = 0.009373152628540993 + 1.0 * 5.997547626495361
Epoch 1470, val loss: 0.9593809843063354
Epoch 1480, training loss: 6.0093607902526855 = 0.009211212396621704 + 1.0 * 6.000149726867676
Epoch 1480, val loss: 0.9618452191352844
Epoch 1490, training loss: 6.005940914154053 = 0.009052511304616928 + 1.0 * 5.996888637542725
Epoch 1490, val loss: 0.9643504023551941
Epoch 1500, training loss: 6.006984233856201 = 0.00889885239303112 + 1.0 * 5.9980854988098145
Epoch 1500, val loss: 0.9667583107948303
Epoch 1510, training loss: 6.004395961761475 = 0.008748650550842285 + 1.0 * 5.995647430419922
Epoch 1510, val loss: 0.969192624092102
Epoch 1520, training loss: 6.003037452697754 = 0.008601602166891098 + 1.0 * 5.994435787200928
Epoch 1520, val loss: 0.9715959429740906
Epoch 1530, training loss: 6.006026744842529 = 0.008457996882498264 + 1.0 * 5.997568607330322
Epoch 1530, val loss: 0.9740507006645203
Epoch 1540, training loss: 6.009136199951172 = 0.00831796508282423 + 1.0 * 6.000818252563477
Epoch 1540, val loss: 0.9765276908874512
Epoch 1550, training loss: 6.004279613494873 = 0.008182371966540813 + 1.0 * 5.996097087860107
Epoch 1550, val loss: 0.9788186550140381
Epoch 1560, training loss: 6.002023220062256 = 0.008052723482251167 + 1.0 * 5.9939703941345215
Epoch 1560, val loss: 0.9810255169868469
Epoch 1570, training loss: 6.0046210289001465 = 0.007923820056021214 + 1.0 * 5.996697425842285
Epoch 1570, val loss: 0.9833521246910095
Epoch 1580, training loss: 6.002036094665527 = 0.007797195576131344 + 1.0 * 5.99423885345459
Epoch 1580, val loss: 0.9857451319694519
Epoch 1590, training loss: 6.001400470733643 = 0.007675581146031618 + 1.0 * 5.993724822998047
Epoch 1590, val loss: 0.9879793524742126
Epoch 1600, training loss: 6.004156589508057 = 0.007556883618235588 + 1.0 * 5.9965996742248535
Epoch 1600, val loss: 0.9901975393295288
Epoch 1610, training loss: 6.0018229484558105 = 0.007440447341650724 + 1.0 * 5.994382381439209
Epoch 1610, val loss: 0.9924429655075073
Epoch 1620, training loss: 6.001360893249512 = 0.007327413186430931 + 1.0 * 5.994033336639404
Epoch 1620, val loss: 0.9946656227111816
Epoch 1630, training loss: 5.999098777770996 = 0.007217037491500378 + 1.0 * 5.991881847381592
Epoch 1630, val loss: 0.9968494772911072
Epoch 1640, training loss: 6.004398822784424 = 0.0071091121062636375 + 1.0 * 5.997289657592773
Epoch 1640, val loss: 0.9990519285202026
Epoch 1650, training loss: 6.00018310546875 = 0.007003033999353647 + 1.0 * 5.993180274963379
Epoch 1650, val loss: 1.0012582540512085
Epoch 1660, training loss: 5.9983110427856445 = 0.0069000013172626495 + 1.0 * 5.991411209106445
Epoch 1660, val loss: 1.003374457359314
Epoch 1670, training loss: 5.997682571411133 = 0.006799203343689442 + 1.0 * 5.9908833503723145
Epoch 1670, val loss: 1.005502700805664
Epoch 1680, training loss: 6.007261753082275 = 0.006699872203171253 + 1.0 * 6.000561714172363
Epoch 1680, val loss: 1.007728099822998
Epoch 1690, training loss: 6.001837253570557 = 0.006602914538234472 + 1.0 * 5.995234489440918
Epoch 1690, val loss: 1.0099271535873413
Epoch 1700, training loss: 6.000326156616211 = 0.006510287057608366 + 1.0 * 5.993815898895264
Epoch 1700, val loss: 1.0119210481643677
Epoch 1710, training loss: 5.997189998626709 = 0.00641877856105566 + 1.0 * 5.990771293640137
Epoch 1710, val loss: 1.0139306783676147
Epoch 1720, training loss: 6.00028133392334 = 0.006329095922410488 + 1.0 * 5.99395227432251
Epoch 1720, val loss: 1.0159705877304077
Epoch 1730, training loss: 5.998757362365723 = 0.006241419818252325 + 1.0 * 5.992516040802002
Epoch 1730, val loss: 1.0181066989898682
Epoch 1740, training loss: 5.996079921722412 = 0.006155910901725292 + 1.0 * 5.98992395401001
Epoch 1740, val loss: 1.0200449228286743
Epoch 1750, training loss: 5.995211124420166 = 0.00607279222458601 + 1.0 * 5.989138126373291
Epoch 1750, val loss: 1.0219850540161133
Epoch 1760, training loss: 5.998549938201904 = 0.005989505909383297 + 1.0 * 5.992560386657715
Epoch 1760, val loss: 1.0240453481674194
Epoch 1770, training loss: 5.997196674346924 = 0.005908150691539049 + 1.0 * 5.991288661956787
Epoch 1770, val loss: 1.0261870622634888
Epoch 1780, training loss: 5.994802474975586 = 0.005830167327076197 + 1.0 * 5.988972187042236
Epoch 1780, val loss: 1.02812922000885
Epoch 1790, training loss: 5.994619369506836 = 0.0057539367116987705 + 1.0 * 5.988865375518799
Epoch 1790, val loss: 1.0299650430679321
Epoch 1800, training loss: 5.998341083526611 = 0.005677829030901194 + 1.0 * 5.992663383483887
Epoch 1800, val loss: 1.03192138671875
Epoch 1810, training loss: 5.993924140930176 = 0.0056028119288384914 + 1.0 * 5.988321304321289
Epoch 1810, val loss: 1.033955693244934
Epoch 1820, training loss: 5.997950553894043 = 0.005530218593776226 + 1.0 * 5.992420196533203
Epoch 1820, val loss: 1.0358728170394897
Epoch 1830, training loss: 5.994961261749268 = 0.0054588112980127335 + 1.0 * 5.989502429962158
Epoch 1830, val loss: 1.0378472805023193
Epoch 1840, training loss: 5.994575500488281 = 0.00539002800360322 + 1.0 * 5.989185333251953
Epoch 1840, val loss: 1.0396579504013062
Epoch 1850, training loss: 5.996264934539795 = 0.005322498269379139 + 1.0 * 5.990942478179932
Epoch 1850, val loss: 1.0415006875991821
Epoch 1860, training loss: 5.994375705718994 = 0.005255797877907753 + 1.0 * 5.989120006561279
Epoch 1860, val loss: 1.0434046983718872
Epoch 1870, training loss: 5.997918128967285 = 0.005190711468458176 + 1.0 * 5.992727279663086
Epoch 1870, val loss: 1.0452934503555298
Epoch 1880, training loss: 5.992428779602051 = 0.005126527510583401 + 1.0 * 5.987302303314209
Epoch 1880, val loss: 1.047065019607544
Epoch 1890, training loss: 5.991829872131348 = 0.005064043682068586 + 1.0 * 5.9867658615112305
Epoch 1890, val loss: 1.0488152503967285
Epoch 1900, training loss: 5.99467658996582 = 0.005001906305551529 + 1.0 * 5.9896745681762695
Epoch 1900, val loss: 1.050657868385315
Epoch 1910, training loss: 5.993461608886719 = 0.004940775688737631 + 1.0 * 5.988520622253418
Epoch 1910, val loss: 1.0525767803192139
Epoch 1920, training loss: 5.993153095245361 = 0.004881848115473986 + 1.0 * 5.988271236419678
Epoch 1920, val loss: 1.0543408393859863
Epoch 1930, training loss: 5.991302490234375 = 0.004823995288461447 + 1.0 * 5.986478328704834
Epoch 1930, val loss: 1.0560561418533325
Epoch 1940, training loss: 5.99627161026001 = 0.004766921047121286 + 1.0 * 5.991504669189453
Epoch 1940, val loss: 1.0578562021255493
Epoch 1950, training loss: 5.992362976074219 = 0.004710379987955093 + 1.0 * 5.987652778625488
Epoch 1950, val loss: 1.059645652770996
Epoch 1960, training loss: 5.991469860076904 = 0.0046562254428863525 + 1.0 * 5.986813545227051
Epoch 1960, val loss: 1.0612996816635132
Epoch 1970, training loss: 5.99096155166626 = 0.004602790344506502 + 1.0 * 5.986358642578125
Epoch 1970, val loss: 1.0629467964172363
Epoch 1980, training loss: 5.998239994049072 = 0.004549873992800713 + 1.0 * 5.993690013885498
Epoch 1980, val loss: 1.064728021621704
Epoch 1990, training loss: 5.990839004516602 = 0.004498408176004887 + 1.0 * 5.986340522766113
Epoch 1990, val loss: 1.0664794445037842
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9373
Flip ASR: 0.9244/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.330602645874023 = 1.9568537473678589 + 1.0 * 8.373748779296875
Epoch 0, val loss: 1.9537585973739624
Epoch 10, training loss: 10.319201469421387 = 1.9462445974349976 + 1.0 * 8.372957229614258
Epoch 10, val loss: 1.943486213684082
Epoch 20, training loss: 10.300618171691895 = 1.9329925775527954 + 1.0 * 8.36762523651123
Epoch 20, val loss: 1.9301470518112183
Epoch 30, training loss: 10.250940322875977 = 1.9148163795471191 + 1.0 * 8.336124420166016
Epoch 30, val loss: 1.9113714694976807
Epoch 40, training loss: 10.022907257080078 = 1.8926774263381958 + 1.0 * 8.130229949951172
Epoch 40, val loss: 1.889021635055542
Epoch 50, training loss: 9.076656341552734 = 1.870102882385254 + 1.0 * 7.2065534591674805
Epoch 50, val loss: 1.8662559986114502
Epoch 60, training loss: 8.729148864746094 = 1.8502730131149292 + 1.0 * 6.878876209259033
Epoch 60, val loss: 1.8463127613067627
Epoch 70, training loss: 8.530583381652832 = 1.831855297088623 + 1.0 * 6.698728084564209
Epoch 70, val loss: 1.8276877403259277
Epoch 80, training loss: 8.396759986877441 = 1.815513014793396 + 1.0 * 6.581247329711914
Epoch 80, val loss: 1.8112797737121582
Epoch 90, training loss: 8.292010307312012 = 1.8003875017166138 + 1.0 * 6.4916229248046875
Epoch 90, val loss: 1.7959538698196411
Epoch 100, training loss: 8.210264205932617 = 1.7856265306472778 + 1.0 * 6.424637794494629
Epoch 100, val loss: 1.781263828277588
Epoch 110, training loss: 8.142569541931152 = 1.7709141969680786 + 1.0 * 6.371654987335205
Epoch 110, val loss: 1.76669442653656
Epoch 120, training loss: 8.085165023803711 = 1.7553362846374512 + 1.0 * 6.329829216003418
Epoch 120, val loss: 1.751574158668518
Epoch 130, training loss: 8.035956382751465 = 1.7383641004562378 + 1.0 * 6.2975921630859375
Epoch 130, val loss: 1.735790729522705
Epoch 140, training loss: 7.991117000579834 = 1.7191108465194702 + 1.0 * 6.272006034851074
Epoch 140, val loss: 1.718984842300415
Epoch 150, training loss: 7.947380065917969 = 1.696771502494812 + 1.0 * 6.250608444213867
Epoch 150, val loss: 1.7005854845046997
Epoch 160, training loss: 7.900818824768066 = 1.6701467037200928 + 1.0 * 6.230672359466553
Epoch 160, val loss: 1.6796232461929321
Epoch 170, training loss: 7.850964546203613 = 1.6374876499176025 + 1.0 * 6.21347713470459
Epoch 170, val loss: 1.6543554067611694
Epoch 180, training loss: 7.798246383666992 = 1.5969722270965576 + 1.0 * 6.201274394989014
Epoch 180, val loss: 1.6232562065124512
Epoch 190, training loss: 7.735528945922852 = 1.5485198497772217 + 1.0 * 6.187008857727051
Epoch 190, val loss: 1.5859315395355225
Epoch 200, training loss: 7.666651725769043 = 1.4911119937896729 + 1.0 * 6.175539493560791
Epoch 200, val loss: 1.5415323972702026
Epoch 210, training loss: 7.591289520263672 = 1.425228238105774 + 1.0 * 6.1660614013671875
Epoch 210, val loss: 1.4904717206954956
Epoch 220, training loss: 7.515337944030762 = 1.3535034656524658 + 1.0 * 6.161834239959717
Epoch 220, val loss: 1.4353704452514648
Epoch 230, training loss: 7.434688568115234 = 1.282051920890808 + 1.0 * 6.152636528015137
Epoch 230, val loss: 1.380340337753296
Epoch 240, training loss: 7.356056213378906 = 1.2118879556655884 + 1.0 * 6.144168376922607
Epoch 240, val loss: 1.326799750328064
Epoch 250, training loss: 7.283053874969482 = 1.1445521116256714 + 1.0 * 6.1385016441345215
Epoch 250, val loss: 1.275916576385498
Epoch 260, training loss: 7.214298248291016 = 1.081762671470642 + 1.0 * 6.132535457611084
Epoch 260, val loss: 1.2290277481079102
Epoch 270, training loss: 7.151006698608398 = 1.022842526435852 + 1.0 * 6.128164291381836
Epoch 270, val loss: 1.1855685710906982
Epoch 280, training loss: 7.090229511260986 = 0.9679955840110779 + 1.0 * 6.122233867645264
Epoch 280, val loss: 1.1454174518585205
Epoch 290, training loss: 7.033243179321289 = 0.9163099527359009 + 1.0 * 6.116933345794678
Epoch 290, val loss: 1.1084060668945312
Epoch 300, training loss: 6.982750415802002 = 0.8678569197654724 + 1.0 * 6.114893436431885
Epoch 300, val loss: 1.0741009712219238
Epoch 310, training loss: 6.932216167449951 = 0.8227505087852478 + 1.0 * 6.109465599060059
Epoch 310, val loss: 1.0426653623580933
Epoch 320, training loss: 6.885521411895752 = 0.7801023721694946 + 1.0 * 6.105419158935547
Epoch 320, val loss: 1.0134375095367432
Epoch 330, training loss: 6.842467308044434 = 0.739709734916687 + 1.0 * 6.102757453918457
Epoch 330, val loss: 0.9863635897636414
Epoch 340, training loss: 6.799748420715332 = 0.7015981078147888 + 1.0 * 6.098150253295898
Epoch 340, val loss: 0.9611603617668152
Epoch 350, training loss: 6.763334274291992 = 0.6653690338134766 + 1.0 * 6.097965240478516
Epoch 350, val loss: 0.9377536177635193
Epoch 360, training loss: 6.729219436645508 = 0.6312764883041382 + 1.0 * 6.09794282913208
Epoch 360, val loss: 0.9163249135017395
Epoch 370, training loss: 6.690671920776367 = 0.5995293855667114 + 1.0 * 6.091142654418945
Epoch 370, val loss: 0.8968106508255005
Epoch 380, training loss: 6.656379222869873 = 0.5694195628166199 + 1.0 * 6.0869598388671875
Epoch 380, val loss: 0.879149317741394
Epoch 390, training loss: 6.624756813049316 = 0.5406749844551086 + 1.0 * 6.084081649780273
Epoch 390, val loss: 0.86296147108078
Epoch 400, training loss: 6.600557327270508 = 0.5132758617401123 + 1.0 * 6.087281227111816
Epoch 400, val loss: 0.8484032154083252
Epoch 410, training loss: 6.56891393661499 = 0.4874749481678009 + 1.0 * 6.081439018249512
Epoch 410, val loss: 0.8357173204421997
Epoch 420, training loss: 6.540836811065674 = 0.4627687335014343 + 1.0 * 6.078068256378174
Epoch 420, val loss: 0.8245931267738342
Epoch 430, training loss: 6.515035629272461 = 0.4388844966888428 + 1.0 * 6.076150894165039
Epoch 430, val loss: 0.8147528171539307
Epoch 440, training loss: 6.497976303100586 = 0.41601768136024475 + 1.0 * 6.081958770751953
Epoch 440, val loss: 0.8064188957214355
Epoch 450, training loss: 6.468057632446289 = 0.3942805826663971 + 1.0 * 6.073777198791504
Epoch 450, val loss: 0.7996498942375183
Epoch 460, training loss: 6.444198131561279 = 0.3733717203140259 + 1.0 * 6.070826530456543
Epoch 460, val loss: 0.7942103743553162
Epoch 470, training loss: 6.422460556030273 = 0.35338303446769714 + 1.0 * 6.069077491760254
Epoch 470, val loss: 0.7898951768875122
Epoch 480, training loss: 6.403021335601807 = 0.3345065414905548 + 1.0 * 6.068514823913574
Epoch 480, val loss: 0.7867861986160278
Epoch 490, training loss: 6.382515907287598 = 0.31653285026550293 + 1.0 * 6.065983295440674
Epoch 490, val loss: 0.7847545742988586
Epoch 500, training loss: 6.364445209503174 = 0.29946988821029663 + 1.0 * 6.064975261688232
Epoch 500, val loss: 0.7834323644638062
Epoch 510, training loss: 6.352338790893555 = 0.28349751234054565 + 1.0 * 6.068841457366943
Epoch 510, val loss: 0.7828115820884705
Epoch 520, training loss: 6.33185338973999 = 0.268655002117157 + 1.0 * 6.063198566436768
Epoch 520, val loss: 0.7830190062522888
Epoch 530, training loss: 6.315328121185303 = 0.25466325879096985 + 1.0 * 6.060664653778076
Epoch 530, val loss: 0.7837674021720886
Epoch 540, training loss: 6.304003715515137 = 0.24151065945625305 + 1.0 * 6.062492847442627
Epoch 540, val loss: 0.7848187685012817
Epoch 550, training loss: 6.286242961883545 = 0.22920028865337372 + 1.0 * 6.057042598724365
Epoch 550, val loss: 0.7864149212837219
Epoch 560, training loss: 6.2733917236328125 = 0.21763025224208832 + 1.0 * 6.055761337280273
Epoch 560, val loss: 0.7884083390235901
Epoch 570, training loss: 6.275618076324463 = 0.20671753585338593 + 1.0 * 6.0689005851745605
Epoch 570, val loss: 0.7905318737030029
Epoch 580, training loss: 6.250216484069824 = 0.19653740525245667 + 1.0 * 6.0536789894104
Epoch 580, val loss: 0.7928829789161682
Epoch 590, training loss: 6.242138385772705 = 0.18692439794540405 + 1.0 * 6.055213928222656
Epoch 590, val loss: 0.7955105900764465
Epoch 600, training loss: 6.229153633117676 = 0.1778169572353363 + 1.0 * 6.051336765289307
Epoch 600, val loss: 0.7981362342834473
Epoch 610, training loss: 6.219477653503418 = 0.1691446155309677 + 1.0 * 6.050333023071289
Epoch 610, val loss: 0.8008706569671631
Epoch 620, training loss: 6.2162652015686035 = 0.16087846457958221 + 1.0 * 6.055386543273926
Epoch 620, val loss: 0.8036506772041321
Epoch 630, training loss: 6.204861164093018 = 0.15303395688533783 + 1.0 * 6.051827430725098
Epoch 630, val loss: 0.8063395023345947
Epoch 640, training loss: 6.198948383331299 = 0.14556896686553955 + 1.0 * 6.053379535675049
Epoch 640, val loss: 0.8091512322425842
Epoch 650, training loss: 6.184288501739502 = 0.1384599357843399 + 1.0 * 6.045828342437744
Epoch 650, val loss: 0.8118592500686646
Epoch 660, training loss: 6.177127838134766 = 0.13164108991622925 + 1.0 * 6.045486927032471
Epoch 660, val loss: 0.8145361542701721
Epoch 670, training loss: 6.1739606857299805 = 0.12508134543895721 + 1.0 * 6.048879146575928
Epoch 670, val loss: 0.817200779914856
Epoch 680, training loss: 6.166126251220703 = 0.11877202242612839 + 1.0 * 6.047354221343994
Epoch 680, val loss: 0.8197584748268127
Epoch 690, training loss: 6.155850410461426 = 0.11267732083797455 + 1.0 * 6.043173313140869
Epoch 690, val loss: 0.8222399950027466
Epoch 700, training loss: 6.14748477935791 = 0.10669679939746857 + 1.0 * 6.040788173675537
Epoch 700, val loss: 0.8246480822563171
Epoch 710, training loss: 6.150129318237305 = 0.1008407324552536 + 1.0 * 6.049288749694824
Epoch 710, val loss: 0.8268272280693054
Epoch 720, training loss: 6.140356063842773 = 0.09516865015029907 + 1.0 * 6.045187473297119
Epoch 720, val loss: 0.82892906665802
Epoch 730, training loss: 6.127493858337402 = 0.08974480628967285 + 1.0 * 6.037749290466309
Epoch 730, val loss: 0.8312458395957947
Epoch 740, training loss: 6.123058319091797 = 0.08457109332084656 + 1.0 * 6.038487434387207
Epoch 740, val loss: 0.8338425159454346
Epoch 750, training loss: 6.120570182800293 = 0.07970129698514938 + 1.0 * 6.040868759155273
Epoch 750, val loss: 0.8366265892982483
Epoch 760, training loss: 6.121572971343994 = 0.07517741620540619 + 1.0 * 6.046395778656006
Epoch 760, val loss: 0.8398902416229248
Epoch 770, training loss: 6.10861349105835 = 0.07105328887701035 + 1.0 * 6.037559986114502
Epoch 770, val loss: 0.8435570597648621
Epoch 780, training loss: 6.101060390472412 = 0.06726901978254318 + 1.0 * 6.033791542053223
Epoch 780, val loss: 0.8476735949516296
Epoch 790, training loss: 6.098031044006348 = 0.06379272043704987 + 1.0 * 6.034238338470459
Epoch 790, val loss: 0.8521608114242554
Epoch 800, training loss: 6.095230579376221 = 0.060594480484724045 + 1.0 * 6.0346360206604
Epoch 800, val loss: 0.8568817973136902
Epoch 810, training loss: 6.089365005493164 = 0.05764582008123398 + 1.0 * 6.031719207763672
Epoch 810, val loss: 0.8619608283042908
Epoch 820, training loss: 6.086658477783203 = 0.05490731820464134 + 1.0 * 6.0317511558532715
Epoch 820, val loss: 0.8671630620956421
Epoch 830, training loss: 6.0903449058532715 = 0.052358683198690414 + 1.0 * 6.0379862785339355
Epoch 830, val loss: 0.8723119497299194
Epoch 840, training loss: 6.079906940460205 = 0.04999343305826187 + 1.0 * 6.029913425445557
Epoch 840, val loss: 0.8775371313095093
Epoch 850, training loss: 6.076731204986572 = 0.04778636246919632 + 1.0 * 6.028944969177246
Epoch 850, val loss: 0.8828837275505066
Epoch 860, training loss: 6.0732421875 = 0.04571216180920601 + 1.0 * 6.027530193328857
Epoch 860, val loss: 0.8882328867912292
Epoch 870, training loss: 6.090759754180908 = 0.04376792535185814 + 1.0 * 6.04699182510376
Epoch 870, val loss: 0.8934557437896729
Epoch 880, training loss: 6.068459510803223 = 0.04194808751344681 + 1.0 * 6.026511192321777
Epoch 880, val loss: 0.8986556529998779
Epoch 890, training loss: 6.066948890686035 = 0.0402507409453392 + 1.0 * 6.026698112487793
Epoch 890, val loss: 0.9040077924728394
Epoch 900, training loss: 6.062572479248047 = 0.038645386695861816 + 1.0 * 6.023927211761475
Epoch 900, val loss: 0.9092565774917603
Epoch 910, training loss: 6.061385154724121 = 0.03712432086467743 + 1.0 * 6.024260997772217
Epoch 910, val loss: 0.9144303798675537
Epoch 920, training loss: 6.069475173950195 = 0.03568701073527336 + 1.0 * 6.033788204193115
Epoch 920, val loss: 0.9195206761360168
Epoch 930, training loss: 6.062915325164795 = 0.034344110637903214 + 1.0 * 6.028571128845215
Epoch 930, val loss: 0.9245941638946533
Epoch 940, training loss: 6.056203365325928 = 0.03308618441224098 + 1.0 * 6.0231170654296875
Epoch 940, val loss: 0.9297599792480469
Epoch 950, training loss: 6.053262233734131 = 0.03189249336719513 + 1.0 * 6.021369934082031
Epoch 950, val loss: 0.9348735213279724
Epoch 960, training loss: 6.051182270050049 = 0.030754540115594864 + 1.0 * 6.020427703857422
Epoch 960, val loss: 0.9398495554924011
Epoch 970, training loss: 6.0505051612854 = 0.029670579358935356 + 1.0 * 6.020834445953369
Epoch 970, val loss: 0.944841742515564
Epoch 980, training loss: 6.0533599853515625 = 0.028641492128372192 + 1.0 * 6.024718284606934
Epoch 980, val loss: 0.9497460126876831
Epoch 990, training loss: 6.0522637367248535 = 0.027678783982992172 + 1.0 * 6.024584770202637
Epoch 990, val loss: 0.9545846581459045
Epoch 1000, training loss: 6.048060894012451 = 0.026772141456604004 + 1.0 * 6.021288871765137
Epoch 1000, val loss: 0.9595117568969727
Epoch 1010, training loss: 6.044092655181885 = 0.025904200971126556 + 1.0 * 6.0181884765625
Epoch 1010, val loss: 0.9644041657447815
Epoch 1020, training loss: 6.043788433074951 = 0.025072362273931503 + 1.0 * 6.018715858459473
Epoch 1020, val loss: 0.9691483378410339
Epoch 1030, training loss: 6.047482013702393 = 0.024279030039906502 + 1.0 * 6.023202896118164
Epoch 1030, val loss: 0.973775327205658
Epoch 1040, training loss: 6.0492706298828125 = 0.023530686274170876 + 1.0 * 6.025740146636963
Epoch 1040, val loss: 0.9783320426940918
Epoch 1050, training loss: 6.039383888244629 = 0.02281981334090233 + 1.0 * 6.016563892364502
Epoch 1050, val loss: 0.9829366207122803
Epoch 1060, training loss: 6.038372993469238 = 0.022143715992569923 + 1.0 * 6.016229152679443
Epoch 1060, val loss: 0.9876140356063843
Epoch 1070, training loss: 6.03655481338501 = 0.02149384468793869 + 1.0 * 6.015060901641846
Epoch 1070, val loss: 0.992127001285553
Epoch 1080, training loss: 6.035046577453613 = 0.020867273211479187 + 1.0 * 6.014179229736328
Epoch 1080, val loss: 0.9965925216674805
Epoch 1090, training loss: 6.045199394226074 = 0.020267263054847717 + 1.0 * 6.024931907653809
Epoch 1090, val loss: 1.0010149478912354
Epoch 1100, training loss: 6.04183292388916 = 0.01969539187848568 + 1.0 * 6.022137641906738
Epoch 1100, val loss: 1.005260944366455
Epoch 1110, training loss: 6.034049987792969 = 0.019154224544763565 + 1.0 * 6.014895915985107
Epoch 1110, val loss: 1.0096228122711182
Epoch 1120, training loss: 6.0312581062316895 = 0.018636219203472137 + 1.0 * 6.012621879577637
Epoch 1120, val loss: 1.0140336751937866
Epoch 1130, training loss: 6.037452697753906 = 0.018135998398065567 + 1.0 * 6.019316673278809
Epoch 1130, val loss: 1.0182549953460693
Epoch 1140, training loss: 6.033563137054443 = 0.017655514180660248 + 1.0 * 6.0159077644348145
Epoch 1140, val loss: 1.0223326683044434
Epoch 1150, training loss: 6.031272888183594 = 0.017197150737047195 + 1.0 * 6.014075756072998
Epoch 1150, val loss: 1.026431918144226
Epoch 1160, training loss: 6.029979228973389 = 0.016757041215896606 + 1.0 * 6.0132222175598145
Epoch 1160, val loss: 1.0306109189987183
Epoch 1170, training loss: 6.029066562652588 = 0.016332104802131653 + 1.0 * 6.012734413146973
Epoch 1170, val loss: 1.0346578359603882
Epoch 1180, training loss: 6.0273356437683105 = 0.015923425555229187 + 1.0 * 6.011412143707275
Epoch 1180, val loss: 1.038673758506775
Epoch 1190, training loss: 6.029200553894043 = 0.015530813485383987 + 1.0 * 6.013669967651367
Epoch 1190, val loss: 1.0425597429275513
Epoch 1200, training loss: 6.024395942687988 = 0.015157638117671013 + 1.0 * 6.009238243103027
Epoch 1200, val loss: 1.0464422702789307
Epoch 1210, training loss: 6.023303031921387 = 0.014797811396420002 + 1.0 * 6.008505344390869
Epoch 1210, val loss: 1.050437092781067
Epoch 1220, training loss: 6.022801876068115 = 0.01444847509264946 + 1.0 * 6.008353233337402
Epoch 1220, val loss: 1.0543313026428223
Epoch 1230, training loss: 6.037325859069824 = 0.01410977728664875 + 1.0 * 6.023216247558594
Epoch 1230, val loss: 1.0581300258636475
Epoch 1240, training loss: 6.0287580490112305 = 0.01378465536981821 + 1.0 * 6.014973163604736
Epoch 1240, val loss: 1.0617260932922363
Epoch 1250, training loss: 6.021790981292725 = 0.013475130312144756 + 1.0 * 6.0083160400390625
Epoch 1250, val loss: 1.065525770187378
Epoch 1260, training loss: 6.021191596984863 = 0.013176155276596546 + 1.0 * 6.0080156326293945
Epoch 1260, val loss: 1.069312572479248
Epoch 1270, training loss: 6.021686553955078 = 0.012884801253676414 + 1.0 * 6.0088019371032715
Epoch 1270, val loss: 1.0729517936706543
Epoch 1280, training loss: 6.021233081817627 = 0.012602590955793858 + 1.0 * 6.008630275726318
Epoch 1280, val loss: 1.076506495475769
Epoch 1290, training loss: 6.022628307342529 = 0.012330272234976292 + 1.0 * 6.010298252105713
Epoch 1290, val loss: 1.0800726413726807
Epoch 1300, training loss: 6.02175760269165 = 0.012067661620676517 + 1.0 * 6.009689807891846
Epoch 1300, val loss: 1.0835769176483154
Epoch 1310, training loss: 6.017770290374756 = 0.011815777979791164 + 1.0 * 6.005954742431641
Epoch 1310, val loss: 1.0870105028152466
Epoch 1320, training loss: 6.017341613769531 = 0.011572496965527534 + 1.0 * 6.0057692527771
Epoch 1320, val loss: 1.0905412435531616
Epoch 1330, training loss: 6.019296169281006 = 0.011335025541484356 + 1.0 * 6.007961273193359
Epoch 1330, val loss: 1.0940049886703491
Epoch 1340, training loss: 6.014662265777588 = 0.011104603298008442 + 1.0 * 6.0035576820373535
Epoch 1340, val loss: 1.0973390340805054
Epoch 1350, training loss: 6.015166282653809 = 0.010882220230996609 + 1.0 * 6.004283905029297
Epoch 1350, val loss: 1.1007225513458252
Epoch 1360, training loss: 6.031397342681885 = 0.010667051188647747 + 1.0 * 6.020730495452881
Epoch 1360, val loss: 1.104061484336853
Epoch 1370, training loss: 6.019248008728027 = 0.010456847958266735 + 1.0 * 6.008790969848633
Epoch 1370, val loss: 1.1071439981460571
Epoch 1380, training loss: 6.013435363769531 = 0.010259863920509815 + 1.0 * 6.003175735473633
Epoch 1380, val loss: 1.1104484796524048
Epoch 1390, training loss: 6.012827396392822 = 0.010068532079458237 + 1.0 * 6.002758979797363
Epoch 1390, val loss: 1.1138298511505127
Epoch 1400, training loss: 6.011319160461426 = 0.009878769516944885 + 1.0 * 6.001440525054932
Epoch 1400, val loss: 1.1170775890350342
Epoch 1410, training loss: 6.010937213897705 = 0.009692873805761337 + 1.0 * 6.00124454498291
Epoch 1410, val loss: 1.1202572584152222
Epoch 1420, training loss: 6.034990310668945 = 0.009512487798929214 + 1.0 * 6.025477886199951
Epoch 1420, val loss: 1.1233432292938232
Epoch 1430, training loss: 6.011946678161621 = 0.009338651783764362 + 1.0 * 6.002607822418213
Epoch 1430, val loss: 1.126271367073059
Epoch 1440, training loss: 6.012187957763672 = 0.009174242615699768 + 1.0 * 6.003013610839844
Epoch 1440, val loss: 1.129503607749939
Epoch 1450, training loss: 6.008901596069336 = 0.009013842791318893 + 1.0 * 5.999887943267822
Epoch 1450, val loss: 1.1327427625656128
Epoch 1460, training loss: 6.011452674865723 = 0.008854929357767105 + 1.0 * 6.002597808837891
Epoch 1460, val loss: 1.1358147859573364
Epoch 1470, training loss: 6.01025390625 = 0.008699254132807255 + 1.0 * 6.001554489135742
Epoch 1470, val loss: 1.1386511325836182
Epoch 1480, training loss: 6.0077996253967285 = 0.008549731224775314 + 1.0 * 5.9992499351501465
Epoch 1480, val loss: 1.1416192054748535
Epoch 1490, training loss: 6.00711727142334 = 0.008405291475355625 + 1.0 * 5.998712062835693
Epoch 1490, val loss: 1.1447293758392334
Epoch 1500, training loss: 6.0088582038879395 = 0.008262581191956997 + 1.0 * 6.000595569610596
Epoch 1500, val loss: 1.1477338075637817
Epoch 1510, training loss: 6.010313987731934 = 0.008123047649860382 + 1.0 * 6.002191066741943
Epoch 1510, val loss: 1.150523066520691
Epoch 1520, training loss: 6.007851600646973 = 0.007989528588950634 + 1.0 * 5.999862194061279
Epoch 1520, val loss: 1.1533889770507812
Epoch 1530, training loss: 6.008044242858887 = 0.007859701290726662 + 1.0 * 6.000184535980225
Epoch 1530, val loss: 1.1563466787338257
Epoch 1540, training loss: 6.007259845733643 = 0.007732251659035683 + 1.0 * 5.999527454376221
Epoch 1540, val loss: 1.159224271774292
Epoch 1550, training loss: 6.005733489990234 = 0.007607589475810528 + 1.0 * 5.998126029968262
Epoch 1550, val loss: 1.1620779037475586
Epoch 1560, training loss: 6.007327079772949 = 0.007485765963792801 + 1.0 * 5.999841213226318
Epoch 1560, val loss: 1.164907693862915
Epoch 1570, training loss: 6.007727146148682 = 0.007366448640823364 + 1.0 * 6.000360488891602
Epoch 1570, val loss: 1.1676498651504517
Epoch 1580, training loss: 6.0088114738464355 = 0.007250889670103788 + 1.0 * 6.001560688018799
Epoch 1580, val loss: 1.1703249216079712
Epoch 1590, training loss: 6.0059309005737305 = 0.007140751928091049 + 1.0 * 5.998790264129639
Epoch 1590, val loss: 1.1730188131332397
Epoch 1600, training loss: 6.007108688354492 = 0.007033560425043106 + 1.0 * 6.000075340270996
Epoch 1600, val loss: 1.1758489608764648
Epoch 1610, training loss: 6.003982067108154 = 0.006927674636244774 + 1.0 * 5.997054576873779
Epoch 1610, val loss: 1.1785354614257812
Epoch 1620, training loss: 6.002057075500488 = 0.006823709234595299 + 1.0 * 5.995233535766602
Epoch 1620, val loss: 1.181257963180542
Epoch 1630, training loss: 6.002169609069824 = 0.006721617188304663 + 1.0 * 5.995448112487793
Epoch 1630, val loss: 1.1839871406555176
Epoch 1640, training loss: 6.014196395874023 = 0.006620937492698431 + 1.0 * 6.007575511932373
Epoch 1640, val loss: 1.1866055727005005
Epoch 1650, training loss: 6.009097099304199 = 0.006524145603179932 + 1.0 * 6.002573013305664
Epoch 1650, val loss: 1.1890711784362793
Epoch 1660, training loss: 6.004302024841309 = 0.006430214270949364 + 1.0 * 5.9978718757629395
Epoch 1660, val loss: 1.191712498664856
Epoch 1670, training loss: 6.001810550689697 = 0.006339604035019875 + 1.0 * 5.995471000671387
Epoch 1670, val loss: 1.1943755149841309
Epoch 1680, training loss: 6.001823902130127 = 0.006249617785215378 + 1.0 * 5.995574474334717
Epoch 1680, val loss: 1.196974515914917
Epoch 1690, training loss: 6.002954959869385 = 0.006161111406981945 + 1.0 * 5.996793746948242
Epoch 1690, val loss: 1.1994742155075073
Epoch 1700, training loss: 6.006525039672852 = 0.006074375938624144 + 1.0 * 6.000450611114502
Epoch 1700, val loss: 1.2019654512405396
Epoch 1710, training loss: 6.00114631652832 = 0.005989682860672474 + 1.0 * 5.995156764984131
Epoch 1710, val loss: 1.204362392425537
Epoch 1720, training loss: 6.000828742980957 = 0.005908596329391003 + 1.0 * 5.994920253753662
Epoch 1720, val loss: 1.2068935632705688
Epoch 1730, training loss: 6.002252101898193 = 0.005828998517245054 + 1.0 * 5.996423244476318
Epoch 1730, val loss: 1.2093936204910278
Epoch 1740, training loss: 5.998579025268555 = 0.005750176031142473 + 1.0 * 5.992828845977783
Epoch 1740, val loss: 1.2118364572525024
Epoch 1750, training loss: 6.0025153160095215 = 0.005673177074640989 + 1.0 * 5.996841907501221
Epoch 1750, val loss: 1.2142764329910278
Epoch 1760, training loss: 5.999233245849609 = 0.005597516428679228 + 1.0 * 5.993635654449463
Epoch 1760, val loss: 1.2166478633880615
Epoch 1770, training loss: 5.999565124511719 = 0.00552361598238349 + 1.0 * 5.994041442871094
Epoch 1770, val loss: 1.2190510034561157
Epoch 1780, training loss: 6.001430988311768 = 0.00545096630230546 + 1.0 * 5.9959797859191895
Epoch 1780, val loss: 1.2214186191558838
Epoch 1790, training loss: 5.997962474822998 = 0.005379809997975826 + 1.0 * 5.99258279800415
Epoch 1790, val loss: 1.2237718105316162
Epoch 1800, training loss: 5.99742317199707 = 0.005310629960149527 + 1.0 * 5.992112636566162
Epoch 1800, val loss: 1.2261377573013306
Epoch 1810, training loss: 6.006476879119873 = 0.005242600571364164 + 1.0 * 6.00123405456543
Epoch 1810, val loss: 1.228437066078186
Epoch 1820, training loss: 6.0042243003845215 = 0.0051759881898760796 + 1.0 * 5.999048233032227
Epoch 1820, val loss: 1.2306329011917114
Epoch 1830, training loss: 5.997663974761963 = 0.005111691076308489 + 1.0 * 5.992552280426025
Epoch 1830, val loss: 1.232935905456543
Epoch 1840, training loss: 5.995361804962158 = 0.005049451254308224 + 1.0 * 5.990312576293945
Epoch 1840, val loss: 1.2353088855743408
Epoch 1850, training loss: 5.9956183433532715 = 0.004986871499568224 + 1.0 * 5.990631580352783
Epoch 1850, val loss: 1.2375810146331787
Epoch 1860, training loss: 6.0048933029174805 = 0.004924618639051914 + 1.0 * 5.999968528747559
Epoch 1860, val loss: 1.2397494316101074
Epoch 1870, training loss: 5.9989399909973145 = 0.00486410828307271 + 1.0 * 5.994075775146484
Epoch 1870, val loss: 1.2418479919433594
Epoch 1880, training loss: 5.99590539932251 = 0.004805837757885456 + 1.0 * 5.9910993576049805
Epoch 1880, val loss: 1.2441068887710571
Epoch 1890, training loss: 6.007741928100586 = 0.004748542793095112 + 1.0 * 6.002993583679199
Epoch 1890, val loss: 1.2463299036026
Epoch 1900, training loss: 5.997620582580566 = 0.004691723734140396 + 1.0 * 5.992928981781006
Epoch 1900, val loss: 1.2483835220336914
Epoch 1910, training loss: 5.995098114013672 = 0.004637626465409994 + 1.0 * 5.990460395812988
Epoch 1910, val loss: 1.250557780265808
Epoch 1920, training loss: 5.995098114013672 = 0.004583702422678471 + 1.0 * 5.990514278411865
Epoch 1920, val loss: 1.252790927886963
Epoch 1930, training loss: 6.002072811126709 = 0.004529855214059353 + 1.0 * 5.997542858123779
Epoch 1930, val loss: 1.2548458576202393
Epoch 1940, training loss: 5.99446439743042 = 0.004477940034121275 + 1.0 * 5.989986419677734
Epoch 1940, val loss: 1.2568879127502441
Epoch 1950, training loss: 5.992372035980225 = 0.004426856990903616 + 1.0 * 5.987945079803467
Epoch 1950, val loss: 1.2590504884719849
Epoch 1960, training loss: 5.99547004699707 = 0.004376021679490805 + 1.0 * 5.99109411239624
Epoch 1960, val loss: 1.2611815929412842
Epoch 1970, training loss: 6.0000786781311035 = 0.004325986374169588 + 1.0 * 5.995752811431885
Epoch 1970, val loss: 1.2631105184555054
Epoch 1980, training loss: 5.993712425231934 = 0.004277474712580442 + 1.0 * 5.989434719085693
Epoch 1980, val loss: 1.2651140689849854
Epoch 1990, training loss: 5.9922027587890625 = 0.0042305453680455685 + 1.0 * 5.987972259521484
Epoch 1990, val loss: 1.2672775983810425
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7380
Flip ASR: 0.7067/225 nodes
The final ASR:0.78106, 0.11411, Accuracy:0.80370, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9326])
updated graph: torch.Size([2, 10358])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00627, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.321718215942383 = 1.9478106498718262 + 1.0 * 8.373907089233398
Epoch 0, val loss: 1.954391598701477
Epoch 10, training loss: 10.311641693115234 = 1.9379686117172241 + 1.0 * 8.373673439025879
Epoch 10, val loss: 1.9447697401046753
Epoch 20, training loss: 10.297797203063965 = 1.9256459474563599 + 1.0 * 8.372151374816895
Epoch 20, val loss: 1.9320522546768188
Epoch 30, training loss: 10.268343925476074 = 1.9082046747207642 + 1.0 * 8.360138893127441
Epoch 30, val loss: 1.9135161638259888
Epoch 40, training loss: 10.148273468017578 = 1.8844515085220337 + 1.0 * 8.263821601867676
Epoch 40, val loss: 1.8885917663574219
Epoch 50, training loss: 9.54344654083252 = 1.8579025268554688 + 1.0 * 7.685544013977051
Epoch 50, val loss: 1.8612333536148071
Epoch 60, training loss: 9.0775785446167 = 1.8364745378494263 + 1.0 * 7.241103649139404
Epoch 60, val loss: 1.841125726699829
Epoch 70, training loss: 8.738826751708984 = 1.823759913444519 + 1.0 * 6.915066719055176
Epoch 70, val loss: 1.8289679288864136
Epoch 80, training loss: 8.499223709106445 = 1.8118081092834473 + 1.0 * 6.68741512298584
Epoch 80, val loss: 1.817825436592102
Epoch 90, training loss: 8.356002807617188 = 1.7991548776626587 + 1.0 * 6.55684757232666
Epoch 90, val loss: 1.805898666381836
Epoch 100, training loss: 8.253058433532715 = 1.7863726615905762 + 1.0 * 6.466685771942139
Epoch 100, val loss: 1.7938035726547241
Epoch 110, training loss: 8.17870044708252 = 1.774231195449829 + 1.0 * 6.404469013214111
Epoch 110, val loss: 1.7825117111206055
Epoch 120, training loss: 8.122912406921387 = 1.7619694471359253 + 1.0 * 6.36094331741333
Epoch 120, val loss: 1.7715572118759155
Epoch 130, training loss: 8.074027061462402 = 1.7484740018844604 + 1.0 * 6.325552940368652
Epoch 130, val loss: 1.7599655389785767
Epoch 140, training loss: 8.029561996459961 = 1.7329002618789673 + 1.0 * 6.296661376953125
Epoch 140, val loss: 1.7470786571502686
Epoch 150, training loss: 7.987123489379883 = 1.7142528295516968 + 1.0 * 6.2728705406188965
Epoch 150, val loss: 1.731964349746704
Epoch 160, training loss: 7.946723937988281 = 1.691913366317749 + 1.0 * 6.254810333251953
Epoch 160, val loss: 1.7140212059020996
Epoch 170, training loss: 7.902731418609619 = 1.664940357208252 + 1.0 * 6.237791061401367
Epoch 170, val loss: 1.6923365592956543
Epoch 180, training loss: 7.856005668640137 = 1.6320092678070068 + 1.0 * 6.223996162414551
Epoch 180, val loss: 1.6659770011901855
Epoch 190, training loss: 7.804677486419678 = 1.592401385307312 + 1.0 * 6.212275981903076
Epoch 190, val loss: 1.63421630859375
Epoch 200, training loss: 7.746469974517822 = 1.5452747344970703 + 1.0 * 6.201195240020752
Epoch 200, val loss: 1.5961772203445435
Epoch 210, training loss: 7.682519912719727 = 1.4897631406784058 + 1.0 * 6.192756652832031
Epoch 210, val loss: 1.551304817199707
Epoch 220, training loss: 7.613527774810791 = 1.427006721496582 + 1.0 * 6.186521053314209
Epoch 220, val loss: 1.5006712675094604
Epoch 230, training loss: 7.537634372711182 = 1.358519196510315 + 1.0 * 6.179115295410156
Epoch 230, val loss: 1.4454962015151978
Epoch 240, training loss: 7.464970588684082 = 1.2872233390808105 + 1.0 * 6.1777472496032715
Epoch 240, val loss: 1.3884117603302002
Epoch 250, training loss: 7.388754367828369 = 1.218772292137146 + 1.0 * 6.169981956481934
Epoch 250, val loss: 1.3340418338775635
Epoch 260, training loss: 7.317954063415527 = 1.154207468032837 + 1.0 * 6.163746356964111
Epoch 260, val loss: 1.283092975616455
Epoch 270, training loss: 7.253244400024414 = 1.0944139957427979 + 1.0 * 6.158830642700195
Epoch 270, val loss: 1.2363073825836182
Epoch 280, training loss: 7.194294452667236 = 1.040270447731018 + 1.0 * 6.154024124145508
Epoch 280, val loss: 1.1946780681610107
Epoch 290, training loss: 7.143551349639893 = 0.9919376373291016 + 1.0 * 6.151613712310791
Epoch 290, val loss: 1.1584091186523438
Epoch 300, training loss: 7.0943379402160645 = 0.9478986859321594 + 1.0 * 6.146439075469971
Epoch 300, val loss: 1.125950574874878
Epoch 310, training loss: 7.047966480255127 = 0.906790554523468 + 1.0 * 6.141175746917725
Epoch 310, val loss: 1.0963289737701416
Epoch 320, training loss: 7.006381034851074 = 0.8678203821182251 + 1.0 * 6.138560771942139
Epoch 320, val loss: 1.0687050819396973
Epoch 330, training loss: 6.963804721832275 = 0.830449104309082 + 1.0 * 6.133355617523193
Epoch 330, val loss: 1.0427440404891968
Epoch 340, training loss: 6.924224853515625 = 0.7938934564590454 + 1.0 * 6.130331516265869
Epoch 340, val loss: 1.017645239830017
Epoch 350, training loss: 6.8879570960998535 = 0.7583337426185608 + 1.0 * 6.1296234130859375
Epoch 350, val loss: 0.9933244585990906
Epoch 360, training loss: 6.847597122192383 = 0.7238631844520569 + 1.0 * 6.123733997344971
Epoch 360, val loss: 0.9700493812561035
Epoch 370, training loss: 6.810206890106201 = 0.6903017163276672 + 1.0 * 6.1199049949646
Epoch 370, val loss: 0.947580099105835
Epoch 380, training loss: 6.776210784912109 = 0.6576328277587891 + 1.0 * 6.11857795715332
Epoch 380, val loss: 0.926000714302063
Epoch 390, training loss: 6.743433952331543 = 0.626254141330719 + 1.0 * 6.117179870605469
Epoch 390, val loss: 0.9056630730628967
Epoch 400, training loss: 6.708339691162109 = 0.5965483784675598 + 1.0 * 6.111791133880615
Epoch 400, val loss: 0.8869668245315552
Epoch 410, training loss: 6.677063941955566 = 0.5681918859481812 + 1.0 * 6.108871936798096
Epoch 410, val loss: 0.8696437478065491
Epoch 420, training loss: 6.6600341796875 = 0.5411568880081177 + 1.0 * 6.118877410888672
Epoch 420, val loss: 0.8537466526031494
Epoch 430, training loss: 6.621325969696045 = 0.5159339308738708 + 1.0 * 6.105391979217529
Epoch 430, val loss: 0.8396329283714294
Epoch 440, training loss: 6.592534065246582 = 0.49198466539382935 + 1.0 * 6.100549221038818
Epoch 440, val loss: 0.8269653916358948
Epoch 450, training loss: 6.566601276397705 = 0.4689449965953827 + 1.0 * 6.09765625
Epoch 450, val loss: 0.8154484629631042
Epoch 460, training loss: 6.5444254875183105 = 0.4466288983821869 + 1.0 * 6.097796440124512
Epoch 460, val loss: 0.8049934506416321
Epoch 470, training loss: 6.523243427276611 = 0.42505279183387756 + 1.0 * 6.098190784454346
Epoch 470, val loss: 0.7954023480415344
Epoch 480, training loss: 6.4975056648254395 = 0.40411657094955444 + 1.0 * 6.09338903427124
Epoch 480, val loss: 0.7866855263710022
Epoch 490, training loss: 6.48006534576416 = 0.3837882876396179 + 1.0 * 6.096277236938477
Epoch 490, val loss: 0.7785174250602722
Epoch 500, training loss: 6.451655864715576 = 0.36403876543045044 + 1.0 * 6.087616920471191
Epoch 500, val loss: 0.7710592746734619
Epoch 510, training loss: 6.428882122039795 = 0.34469687938690186 + 1.0 * 6.0841851234436035
Epoch 510, val loss: 0.7639389038085938
Epoch 520, training loss: 6.411131381988525 = 0.32574671506881714 + 1.0 * 6.085384845733643
Epoch 520, val loss: 0.7570633292198181
Epoch 530, training loss: 6.393764972686768 = 0.3073825240135193 + 1.0 * 6.0863823890686035
Epoch 530, val loss: 0.7505230903625488
Epoch 540, training loss: 6.3698649406433105 = 0.2896851599216461 + 1.0 * 6.080179691314697
Epoch 540, val loss: 0.7444231510162354
Epoch 550, training loss: 6.35986328125 = 0.27257847785949707 + 1.0 * 6.087284564971924
Epoch 550, val loss: 0.7385762333869934
Epoch 560, training loss: 6.333106994628906 = 0.2562497556209564 + 1.0 * 6.076857089996338
Epoch 560, val loss: 0.7330628037452698
Epoch 570, training loss: 6.315019130706787 = 0.24068087339401245 + 1.0 * 6.074338436126709
Epoch 570, val loss: 0.7281061410903931
Epoch 580, training loss: 6.303547382354736 = 0.22582973539829254 + 1.0 * 6.0777177810668945
Epoch 580, val loss: 0.7235973477363586
Epoch 590, training loss: 6.288334369659424 = 0.21183627843856812 + 1.0 * 6.076498031616211
Epoch 590, val loss: 0.7195111513137817
Epoch 600, training loss: 6.268242359161377 = 0.19870726764202118 + 1.0 * 6.069535255432129
Epoch 600, val loss: 0.716215193271637
Epoch 610, training loss: 6.255346298217773 = 0.1863865852355957 + 1.0 * 6.068959712982178
Epoch 610, val loss: 0.7135736346244812
Epoch 620, training loss: 6.242462158203125 = 0.17496250569820404 + 1.0 * 6.06749963760376
Epoch 620, val loss: 0.7115442156791687
Epoch 630, training loss: 6.229443550109863 = 0.164456307888031 + 1.0 * 6.0649871826171875
Epoch 630, val loss: 0.7102507948875427
Epoch 640, training loss: 6.218504428863525 = 0.15473446249961853 + 1.0 * 6.063769817352295
Epoch 640, val loss: 0.7096403241157532
Epoch 650, training loss: 6.217901229858398 = 0.1457471400499344 + 1.0 * 6.0721540451049805
Epoch 650, val loss: 0.7096442580223083
Epoch 660, training loss: 6.200337886810303 = 0.1374954879283905 + 1.0 * 6.06284236907959
Epoch 660, val loss: 0.7101345658302307
Epoch 670, training loss: 6.189908504486084 = 0.1298789083957672 + 1.0 * 6.06002950668335
Epoch 670, val loss: 0.711268424987793
Epoch 680, training loss: 6.186701774597168 = 0.12281979620456696 + 1.0 * 6.063881874084473
Epoch 680, val loss: 0.7128749489784241
Epoch 690, training loss: 6.1810503005981445 = 0.11630252003669739 + 1.0 * 6.0647478103637695
Epoch 690, val loss: 0.7149182558059692
Epoch 700, training loss: 6.168830394744873 = 0.11027172952890396 + 1.0 * 6.058558464050293
Epoch 700, val loss: 0.7174419164657593
Epoch 710, training loss: 6.163402080535889 = 0.1046626940369606 + 1.0 * 6.058739185333252
Epoch 710, val loss: 0.7203429341316223
Epoch 720, training loss: 6.155344009399414 = 0.09944156557321548 + 1.0 * 6.055902481079102
Epoch 720, val loss: 0.7235254645347595
Epoch 730, training loss: 6.1499552726745605 = 0.09456606954336166 + 1.0 * 6.055389404296875
Epoch 730, val loss: 0.727042555809021
Epoch 740, training loss: 6.142538070678711 = 0.09000309556722641 + 1.0 * 6.052535057067871
Epoch 740, val loss: 0.7308856844902039
Epoch 750, training loss: 6.14901876449585 = 0.08572877198457718 + 1.0 * 6.063290119171143
Epoch 750, val loss: 0.7349599003791809
Epoch 760, training loss: 6.134861946105957 = 0.0817466601729393 + 1.0 * 6.053115367889404
Epoch 760, val loss: 0.7391840219497681
Epoch 770, training loss: 6.129390716552734 = 0.07802516967058182 + 1.0 * 6.051365375518799
Epoch 770, val loss: 0.7435856461524963
Epoch 780, training loss: 6.12302303314209 = 0.0745188370347023 + 1.0 * 6.04850435256958
Epoch 780, val loss: 0.7482211589813232
Epoch 790, training loss: 6.125319480895996 = 0.07121007144451141 + 1.0 * 6.054109573364258
Epoch 790, val loss: 0.7530439496040344
Epoch 800, training loss: 6.119666576385498 = 0.06810831278562546 + 1.0 * 6.051558494567871
Epoch 800, val loss: 0.757788360118866
Epoch 810, training loss: 6.111867904663086 = 0.06520206481218338 + 1.0 * 6.046665668487549
Epoch 810, val loss: 0.76279217004776
Epoch 820, training loss: 6.111367702484131 = 0.06245631352066994 + 1.0 * 6.0489115715026855
Epoch 820, val loss: 0.7678920030593872
Epoch 830, training loss: 6.10400390625 = 0.059861473739147186 + 1.0 * 6.044142246246338
Epoch 830, val loss: 0.7730584740638733
Epoch 840, training loss: 6.102730751037598 = 0.057409849017858505 + 1.0 * 6.045320987701416
Epoch 840, val loss: 0.7783408761024475
Epoch 850, training loss: 6.105738639831543 = 0.055085331201553345 + 1.0 * 6.050653457641602
Epoch 850, val loss: 0.7837633490562439
Epoch 860, training loss: 6.097301006317139 = 0.05289466679096222 + 1.0 * 6.044406414031982
Epoch 860, val loss: 0.7891488671302795
Epoch 870, training loss: 6.092519760131836 = 0.050822313874959946 + 1.0 * 6.0416975021362305
Epoch 870, val loss: 0.7946539521217346
Epoch 880, training loss: 6.093456268310547 = 0.04885327070951462 + 1.0 * 6.044602870941162
Epoch 880, val loss: 0.8002451658248901
Epoch 890, training loss: 6.091300964355469 = 0.04698807746171951 + 1.0 * 6.044312953948975
Epoch 890, val loss: 0.8057563304901123
Epoch 900, training loss: 6.085944175720215 = 0.045223500579595566 + 1.0 * 6.040720462799072
Epoch 900, val loss: 0.8113512992858887
Epoch 910, training loss: 6.083217620849609 = 0.04354775324463844 + 1.0 * 6.039669990539551
Epoch 910, val loss: 0.8169680833816528
Epoch 920, training loss: 6.083512783050537 = 0.04195290803909302 + 1.0 * 6.04155969619751
Epoch 920, val loss: 0.822609007358551
Epoch 930, training loss: 6.083025932312012 = 0.040440723299980164 + 1.0 * 6.042585372924805
Epoch 930, val loss: 0.8282361030578613
Epoch 940, training loss: 6.077131748199463 = 0.03900754451751709 + 1.0 * 6.038124084472656
Epoch 940, val loss: 0.833854079246521
Epoch 950, training loss: 6.075835704803467 = 0.037643879652023315 + 1.0 * 6.038191795349121
Epoch 950, val loss: 0.8394545912742615
Epoch 960, training loss: 6.074482440948486 = 0.03634410724043846 + 1.0 * 6.038138389587402
Epoch 960, val loss: 0.8450960516929626
Epoch 970, training loss: 6.072497844696045 = 0.03510962054133415 + 1.0 * 6.037388324737549
Epoch 970, val loss: 0.8506800532341003
Epoch 980, training loss: 6.073861122131348 = 0.03393447399139404 + 1.0 * 6.039926528930664
Epoch 980, val loss: 0.8562623262405396
Epoch 990, training loss: 6.066636562347412 = 0.03281443938612938 + 1.0 * 6.033822059631348
Epoch 990, val loss: 0.8617007732391357
Epoch 1000, training loss: 6.065669059753418 = 0.03174853324890137 + 1.0 * 6.0339202880859375
Epoch 1000, val loss: 0.8672614097595215
Epoch 1010, training loss: 6.063912868499756 = 0.030729003250598907 + 1.0 * 6.033184051513672
Epoch 1010, val loss: 0.8727476596832275
Epoch 1020, training loss: 6.064239978790283 = 0.029757443815469742 + 1.0 * 6.034482479095459
Epoch 1020, val loss: 0.8782913088798523
Epoch 1030, training loss: 6.061402320861816 = 0.02883112244307995 + 1.0 * 6.032571315765381
Epoch 1030, val loss: 0.8836738467216492
Epoch 1040, training loss: 6.062666893005371 = 0.027949435636401176 + 1.0 * 6.034717559814453
Epoch 1040, val loss: 0.8890455365180969
Epoch 1050, training loss: 6.058470249176025 = 0.027109667658805847 + 1.0 * 6.031360626220703
Epoch 1050, val loss: 0.8943515419960022
Epoch 1060, training loss: 6.05543327331543 = 0.02630593441426754 + 1.0 * 6.02912712097168
Epoch 1060, val loss: 0.899717390537262
Epoch 1070, training loss: 6.059010028839111 = 0.02553493157029152 + 1.0 * 6.033474922180176
Epoch 1070, val loss: 0.905017614364624
Epoch 1080, training loss: 6.056040287017822 = 0.024797601625323296 + 1.0 * 6.031242847442627
Epoch 1080, val loss: 0.9101841449737549
Epoch 1090, training loss: 6.053812026977539 = 0.02409391477704048 + 1.0 * 6.029717922210693
Epoch 1090, val loss: 0.9154118299484253
Epoch 1100, training loss: 6.053013324737549 = 0.023419881239533424 + 1.0 * 6.029593467712402
Epoch 1100, val loss: 0.9205015301704407
Epoch 1110, training loss: 6.0489373207092285 = 0.022773979231715202 + 1.0 * 6.026163578033447
Epoch 1110, val loss: 0.9256770014762878
Epoch 1120, training loss: 6.051411151885986 = 0.02215234935283661 + 1.0 * 6.029258728027344
Epoch 1120, val loss: 0.930742621421814
Epoch 1130, training loss: 6.047219276428223 = 0.021555161103606224 + 1.0 * 6.025664329528809
Epoch 1130, val loss: 0.935828685760498
Epoch 1140, training loss: 6.050093173980713 = 0.020983558148145676 + 1.0 * 6.029109477996826
Epoch 1140, val loss: 0.9408138990402222
Epoch 1150, training loss: 6.045675277709961 = 0.020437005907297134 + 1.0 * 6.025238037109375
Epoch 1150, val loss: 0.9457609057426453
Epoch 1160, training loss: 6.052844047546387 = 0.019911274313926697 + 1.0 * 6.032932758331299
Epoch 1160, val loss: 0.9506031274795532
Epoch 1170, training loss: 6.049139022827148 = 0.019408034160733223 + 1.0 * 6.029730796813965
Epoch 1170, val loss: 0.9555163383483887
Epoch 1180, training loss: 6.043231010437012 = 0.018927384167909622 + 1.0 * 6.024303436279297
Epoch 1180, val loss: 0.960253119468689
Epoch 1190, training loss: 6.040423393249512 = 0.01846311427652836 + 1.0 * 6.021960258483887
Epoch 1190, val loss: 0.9650287628173828
Epoch 1200, training loss: 6.038919925689697 = 0.018012147396802902 + 1.0 * 6.020907878875732
Epoch 1200, val loss: 0.9697194695472717
Epoch 1210, training loss: 6.050665855407715 = 0.017576158046722412 + 1.0 * 6.033089637756348
Epoch 1210, val loss: 0.9744236469268799
Epoch 1220, training loss: 6.043677806854248 = 0.017156990244984627 + 1.0 * 6.026520729064941
Epoch 1220, val loss: 0.978998601436615
Epoch 1230, training loss: 6.039978981018066 = 0.016758115962147713 + 1.0 * 6.023221015930176
Epoch 1230, val loss: 0.9835940003395081
Epoch 1240, training loss: 6.037632942199707 = 0.01637239009141922 + 1.0 * 6.021260738372803
Epoch 1240, val loss: 0.9881344437599182
Epoch 1250, training loss: 6.036238193511963 = 0.015998946502804756 + 1.0 * 6.020239353179932
Epoch 1250, val loss: 0.9925786852836609
Epoch 1260, training loss: 6.03924036026001 = 0.015638066455721855 + 1.0 * 6.023602485656738
Epoch 1260, val loss: 0.9969753623008728
Epoch 1270, training loss: 6.036989212036133 = 0.015290379524230957 + 1.0 * 6.021698951721191
Epoch 1270, val loss: 1.0013717412948608
Epoch 1280, training loss: 6.033183574676514 = 0.014956341125071049 + 1.0 * 6.0182271003723145
Epoch 1280, val loss: 1.0057804584503174
Epoch 1290, training loss: 6.032026290893555 = 0.014632470905780792 + 1.0 * 6.017393589019775
Epoch 1290, val loss: 1.010148525238037
Epoch 1300, training loss: 6.0374908447265625 = 0.014316597022116184 + 1.0 * 6.023174285888672
Epoch 1300, val loss: 1.0145080089569092
Epoch 1310, training loss: 6.032389163970947 = 0.014010884799063206 + 1.0 * 6.018378257751465
Epoch 1310, val loss: 1.0186225175857544
Epoch 1320, training loss: 6.032954692840576 = 0.013719022274017334 + 1.0 * 6.019235610961914
Epoch 1320, val loss: 1.0228885412216187
Epoch 1330, training loss: 6.03135871887207 = 0.013436551205813885 + 1.0 * 6.017922401428223
Epoch 1330, val loss: 1.0270884037017822
Epoch 1340, training loss: 6.0337114334106445 = 0.013161380775272846 + 1.0 * 6.02055025100708
Epoch 1340, val loss: 1.031267762184143
Epoch 1350, training loss: 6.028397083282471 = 0.012894794344902039 + 1.0 * 6.015502452850342
Epoch 1350, val loss: 1.0352591276168823
Epoch 1360, training loss: 6.02773904800415 = 0.012637263163924217 + 1.0 * 6.015101909637451
Epoch 1360, val loss: 1.0393744707107544
Epoch 1370, training loss: 6.028956890106201 = 0.01238658744841814 + 1.0 * 6.016570091247559
Epoch 1370, val loss: 1.043503761291504
Epoch 1380, training loss: 6.028360843658447 = 0.012142782099545002 + 1.0 * 6.016218185424805
Epoch 1380, val loss: 1.0475049018859863
Epoch 1390, training loss: 6.0307745933532715 = 0.011907745152711868 + 1.0 * 6.018867015838623
Epoch 1390, val loss: 1.0514609813690186
Epoch 1400, training loss: 6.025014877319336 = 0.011682040989398956 + 1.0 * 6.013332843780518
Epoch 1400, val loss: 1.0552884340286255
Epoch 1410, training loss: 6.024295806884766 = 0.011464225128293037 + 1.0 * 6.012831687927246
Epoch 1410, val loss: 1.059211254119873
Epoch 1420, training loss: 6.023679733276367 = 0.01125077810138464 + 1.0 * 6.0124287605285645
Epoch 1420, val loss: 1.0631119012832642
Epoch 1430, training loss: 6.03085994720459 = 0.011040941812098026 + 1.0 * 6.0198187828063965
Epoch 1430, val loss: 1.0669697523117065
Epoch 1440, training loss: 6.0266432762146 = 0.010837901383638382 + 1.0 * 6.015805244445801
Epoch 1440, val loss: 1.0707355737686157
Epoch 1450, training loss: 6.022917747497559 = 0.010642630979418755 + 1.0 * 6.012275218963623
Epoch 1450, val loss: 1.0744664669036865
Epoch 1460, training loss: 6.0215301513671875 = 0.010453369468450546 + 1.0 * 6.011076927185059
Epoch 1460, val loss: 1.0782489776611328
Epoch 1470, training loss: 6.028811931610107 = 0.010267027653753757 + 1.0 * 6.018544673919678
Epoch 1470, val loss: 1.0819958448410034
Epoch 1480, training loss: 6.036704063415527 = 0.010086295194923878 + 1.0 * 6.026618003845215
Epoch 1480, val loss: 1.0856437683105469
Epoch 1490, training loss: 6.021297454833984 = 0.009913984686136246 + 1.0 * 6.011383533477783
Epoch 1490, val loss: 1.0890792608261108
Epoch 1500, training loss: 6.020596504211426 = 0.009748819284141064 + 1.0 * 6.010847568511963
Epoch 1500, val loss: 1.0926469564437866
Epoch 1510, training loss: 6.0184502601623535 = 0.0095853665843606 + 1.0 * 6.008864879608154
Epoch 1510, val loss: 1.096253752708435
Epoch 1520, training loss: 6.018894672393799 = 0.009423425421118736 + 1.0 * 6.009471416473389
Epoch 1520, val loss: 1.099884033203125
Epoch 1530, training loss: 6.029288291931152 = 0.00926516018807888 + 1.0 * 6.020023345947266
Epoch 1530, val loss: 1.103367805480957
Epoch 1540, training loss: 6.019184589385986 = 0.009112624451518059 + 1.0 * 6.010071754455566
Epoch 1540, val loss: 1.1066945791244507
Epoch 1550, training loss: 6.017730712890625 = 0.008966445922851562 + 1.0 * 6.008764266967773
Epoch 1550, val loss: 1.1101651191711426
Epoch 1560, training loss: 6.020681858062744 = 0.008822547271847725 + 1.0 * 6.01185941696167
Epoch 1560, val loss: 1.1136462688446045
Epoch 1570, training loss: 6.017185211181641 = 0.008681039325892925 + 1.0 * 6.008504390716553
Epoch 1570, val loss: 1.1170234680175781
Epoch 1580, training loss: 6.018548488616943 = 0.008543696254491806 + 1.0 * 6.010004997253418
Epoch 1580, val loss: 1.120448112487793
Epoch 1590, training loss: 6.021788120269775 = 0.008409098722040653 + 1.0 * 6.013379096984863
Epoch 1590, val loss: 1.1237647533416748
Epoch 1600, training loss: 6.017236709594727 = 0.008278687484562397 + 1.0 * 6.008957862854004
Epoch 1600, val loss: 1.1271107196807861
Epoch 1610, training loss: 6.01585054397583 = 0.008151614107191563 + 1.0 * 6.007699012756348
Epoch 1610, val loss: 1.1303960084915161
Epoch 1620, training loss: 6.015229225158691 = 0.008027039468288422 + 1.0 * 6.0072021484375
Epoch 1620, val loss: 1.1337097883224487
Epoch 1630, training loss: 6.014771938323975 = 0.007905177772045135 + 1.0 * 6.006866931915283
Epoch 1630, val loss: 1.1369900703430176
Epoch 1640, training loss: 6.015539169311523 = 0.007785566616803408 + 1.0 * 6.007753372192383
Epoch 1640, val loss: 1.140260100364685
Epoch 1650, training loss: 6.017696857452393 = 0.007668712642043829 + 1.0 * 6.01002836227417
Epoch 1650, val loss: 1.1434770822525024
Epoch 1660, training loss: 6.0166192054748535 = 0.007555385120213032 + 1.0 * 6.009063720703125
Epoch 1660, val loss: 1.146602988243103
Epoch 1670, training loss: 6.0137739181518555 = 0.007446014788001776 + 1.0 * 6.006328105926514
Epoch 1670, val loss: 1.1497204303741455
Epoch 1680, training loss: 6.013911247253418 = 0.00733919208869338 + 1.0 * 6.006572246551514
Epoch 1680, val loss: 1.1528851985931396
Epoch 1690, training loss: 6.016446113586426 = 0.007233253680169582 + 1.0 * 6.009212970733643
Epoch 1690, val loss: 1.1559854745864868
Epoch 1700, training loss: 6.015162467956543 = 0.007130460347980261 + 1.0 * 6.008031845092773
Epoch 1700, val loss: 1.1590535640716553
Epoch 1710, training loss: 6.012380123138428 = 0.007030037231743336 + 1.0 * 6.005350112915039
Epoch 1710, val loss: 1.1620903015136719
Epoch 1720, training loss: 6.01426887512207 = 0.006931846030056477 + 1.0 * 6.0073370933532715
Epoch 1720, val loss: 1.1650928258895874
Epoch 1730, training loss: 6.01094913482666 = 0.006835572421550751 + 1.0 * 6.004113674163818
Epoch 1730, val loss: 1.1681485176086426
Epoch 1740, training loss: 6.011679172515869 = 0.006742109078913927 + 1.0 * 6.004937171936035
Epoch 1740, val loss: 1.1711363792419434
Epoch 1750, training loss: 6.0146870613098145 = 0.006650273688137531 + 1.0 * 6.0080366134643555
Epoch 1750, val loss: 1.1740301847457886
Epoch 1760, training loss: 6.0098748207092285 = 0.0065602147951722145 + 1.0 * 6.00331449508667
Epoch 1760, val loss: 1.1770159006118774
Epoch 1770, training loss: 6.008599281311035 = 0.006472479552030563 + 1.0 * 6.002126693725586
Epoch 1770, val loss: 1.1799322366714478
Epoch 1780, training loss: 6.0094990730285645 = 0.006385919637978077 + 1.0 * 6.003113269805908
Epoch 1780, val loss: 1.1828598976135254
Epoch 1790, training loss: 6.014307498931885 = 0.0063002873212099075 + 1.0 * 6.008007049560547
Epoch 1790, val loss: 1.1856645345687866
Epoch 1800, training loss: 6.010017395019531 = 0.006217480171471834 + 1.0 * 6.003799915313721
Epoch 1800, val loss: 1.1885615587234497
Epoch 1810, training loss: 6.009981632232666 = 0.006138036958873272 + 1.0 * 6.003843784332275
Epoch 1810, val loss: 1.191362738609314
Epoch 1820, training loss: 6.008799076080322 = 0.00605985801666975 + 1.0 * 6.002739429473877
Epoch 1820, val loss: 1.1941806077957153
Epoch 1830, training loss: 6.0068793296813965 = 0.005982727278023958 + 1.0 * 6.000896453857422
Epoch 1830, val loss: 1.1969501972198486
Epoch 1840, training loss: 6.010642051696777 = 0.005906578153371811 + 1.0 * 6.004735469818115
Epoch 1840, val loss: 1.1997909545898438
Epoch 1850, training loss: 6.011686325073242 = 0.005831365939229727 + 1.0 * 6.005855083465576
Epoch 1850, val loss: 1.2025336027145386
Epoch 1860, training loss: 6.007017612457275 = 0.005758934188634157 + 1.0 * 6.001258850097656
Epoch 1860, val loss: 1.2051082849502563
Epoch 1870, training loss: 6.0048627853393555 = 0.0056892638094723225 + 1.0 * 5.999173641204834
Epoch 1870, val loss: 1.2078454494476318
Epoch 1880, training loss: 6.0046281814575195 = 0.0056199789978563786 + 1.0 * 5.9990081787109375
Epoch 1880, val loss: 1.210570216178894
Epoch 1890, training loss: 6.006997585296631 = 0.005550536792725325 + 1.0 * 6.0014472007751465
Epoch 1890, val loss: 1.21331787109375
Epoch 1900, training loss: 6.00770902633667 = 0.005481804721057415 + 1.0 * 6.002227306365967
Epoch 1900, val loss: 1.2159793376922607
Epoch 1910, training loss: 6.009225368499756 = 0.0054154060781002045 + 1.0 * 6.003809928894043
Epoch 1910, val loss: 1.2185707092285156
Epoch 1920, training loss: 6.0078887939453125 = 0.0053511266596615314 + 1.0 * 6.002537727355957
Epoch 1920, val loss: 1.2210938930511475
Epoch 1930, training loss: 6.006599426269531 = 0.005288120359182358 + 1.0 * 6.001311302185059
Epoch 1930, val loss: 1.2236557006835938
Epoch 1940, training loss: 6.005640983581543 = 0.00522649334743619 + 1.0 * 6.0004143714904785
Epoch 1940, val loss: 1.2262201309204102
Epoch 1950, training loss: 6.00391960144043 = 0.005165753420442343 + 1.0 * 5.998754024505615
Epoch 1950, val loss: 1.2288131713867188
Epoch 1960, training loss: 6.002046585083008 = 0.005105683580040932 + 1.0 * 5.996941089630127
Epoch 1960, val loss: 1.2314319610595703
Epoch 1970, training loss: 6.0076117515563965 = 0.005045893602073193 + 1.0 * 6.002565860748291
Epoch 1970, val loss: 1.2339626550674438
Epoch 1980, training loss: 6.002010345458984 = 0.0049868798814713955 + 1.0 * 5.997023582458496
Epoch 1980, val loss: 1.236501932144165
Epoch 1990, training loss: 6.002411365509033 = 0.004930458031594753 + 1.0 * 5.997480869293213
Epoch 1990, val loss: 1.2389732599258423
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7159
Flip ASR: 0.6578/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.331899642944336 = 1.9580022096633911 + 1.0 * 8.373897552490234
Epoch 0, val loss: 1.963388204574585
Epoch 10, training loss: 10.321152687072754 = 1.947535753250122 + 1.0 * 8.373617172241211
Epoch 10, val loss: 1.9521710872650146
Epoch 20, training loss: 10.306416511535645 = 1.9346463680267334 + 1.0 * 8.371769905090332
Epoch 20, val loss: 1.938175916671753
Epoch 30, training loss: 10.274975776672363 = 1.9168540239334106 + 1.0 * 8.358121871948242
Epoch 30, val loss: 1.9189292192459106
Epoch 40, training loss: 10.153223037719727 = 1.8935061693191528 + 1.0 * 8.259716987609863
Epoch 40, val loss: 1.8945915699005127
Epoch 50, training loss: 9.504307746887207 = 1.8677880764007568 + 1.0 * 7.636519432067871
Epoch 50, val loss: 1.8684509992599487
Epoch 60, training loss: 8.953636169433594 = 1.8485718965530396 + 1.0 * 7.1050639152526855
Epoch 60, val loss: 1.8501408100128174
Epoch 70, training loss: 8.666027069091797 = 1.8347177505493164 + 1.0 * 6.831308841705322
Epoch 70, val loss: 1.8349355459213257
Epoch 80, training loss: 8.520111083984375 = 1.8189876079559326 + 1.0 * 6.7011237144470215
Epoch 80, val loss: 1.8177851438522339
Epoch 90, training loss: 8.42007827758789 = 1.802540898323059 + 1.0 * 6.617537021636963
Epoch 90, val loss: 1.8005598783493042
Epoch 100, training loss: 8.340959548950195 = 1.7869566679000854 + 1.0 * 6.5540032386779785
Epoch 100, val loss: 1.7846770286560059
Epoch 110, training loss: 8.263631820678711 = 1.773162841796875 + 1.0 * 6.490468978881836
Epoch 110, val loss: 1.7710309028625488
Epoch 120, training loss: 8.200569152832031 = 1.7597671747207642 + 1.0 * 6.440802097320557
Epoch 120, val loss: 1.7583340406417847
Epoch 130, training loss: 8.148605346679688 = 1.745131492614746 + 1.0 * 6.403473854064941
Epoch 130, val loss: 1.7449971437454224
Epoch 140, training loss: 8.097375869750977 = 1.7283060550689697 + 1.0 * 6.369070053100586
Epoch 140, val loss: 1.7299696207046509
Epoch 150, training loss: 8.042097091674805 = 1.708838701248169 + 1.0 * 6.333258628845215
Epoch 150, val loss: 1.7131723165512085
Epoch 160, training loss: 7.986072063446045 = 1.6858705282211304 + 1.0 * 6.300201416015625
Epoch 160, val loss: 1.6938308477401733
Epoch 170, training loss: 7.930968284606934 = 1.6581971645355225 + 1.0 * 6.272770881652832
Epoch 170, val loss: 1.6707687377929688
Epoch 180, training loss: 7.876368522644043 = 1.624407410621643 + 1.0 * 6.2519612312316895
Epoch 180, val loss: 1.642736792564392
Epoch 190, training loss: 7.819056987762451 = 1.5834542512893677 + 1.0 * 6.235602855682373
Epoch 190, val loss: 1.608772873878479
Epoch 200, training loss: 7.756008148193359 = 1.5346384048461914 + 1.0 * 6.221369743347168
Epoch 200, val loss: 1.5683062076568604
Epoch 210, training loss: 7.689510345458984 = 1.4785706996917725 + 1.0 * 6.210939884185791
Epoch 210, val loss: 1.5222232341766357
Epoch 220, training loss: 7.616034030914307 = 1.4166592359542847 + 1.0 * 6.199374675750732
Epoch 220, val loss: 1.4715790748596191
Epoch 230, training loss: 7.53959846496582 = 1.3496546745300293 + 1.0 * 6.189943790435791
Epoch 230, val loss: 1.4173145294189453
Epoch 240, training loss: 7.463691234588623 = 1.2796630859375 + 1.0 * 6.184028148651123
Epoch 240, val loss: 1.361204981803894
Epoch 250, training loss: 7.3867034912109375 = 1.21016263961792 + 1.0 * 6.176540851593018
Epoch 250, val loss: 1.3065561056137085
Epoch 260, training loss: 7.311413764953613 = 1.1410363912582397 + 1.0 * 6.170377254486084
Epoch 260, val loss: 1.252495527267456
Epoch 270, training loss: 7.238442420959473 = 1.0729800462722778 + 1.0 * 6.165462493896484
Epoch 270, val loss: 1.1996121406555176
Epoch 280, training loss: 7.174403190612793 = 1.0091906785964966 + 1.0 * 6.165212631225586
Epoch 280, val loss: 1.1503381729125977
Epoch 290, training loss: 7.1087775230407715 = 0.9518401026725769 + 1.0 * 6.156937599182129
Epoch 290, val loss: 1.1064285039901733
Epoch 300, training loss: 7.0512614250183105 = 0.8992026448249817 + 1.0 * 6.1520586013793945
Epoch 300, val loss: 1.0664806365966797
Epoch 310, training loss: 6.997315406799316 = 0.8509940505027771 + 1.0 * 6.1463212966918945
Epoch 310, val loss: 1.0305753946304321
Epoch 320, training loss: 6.94890022277832 = 0.8072164058685303 + 1.0 * 6.141684055328369
Epoch 320, val loss: 0.9986553192138672
Epoch 330, training loss: 6.912341117858887 = 0.767768383026123 + 1.0 * 6.144572734832764
Epoch 330, val loss: 0.9708960652351379
Epoch 340, training loss: 6.8685784339904785 = 0.7328730821609497 + 1.0 * 6.135705471038818
Epoch 340, val loss: 0.9471818208694458
Epoch 350, training loss: 6.83130407333374 = 0.7012671828269958 + 1.0 * 6.1300368309021
Epoch 350, val loss: 0.9267297387123108
Epoch 360, training loss: 6.801607131958008 = 0.6720676422119141 + 1.0 * 6.129539489746094
Epoch 360, val loss: 0.9087010025978088
Epoch 370, training loss: 6.76854133605957 = 0.6450933814048767 + 1.0 * 6.123447895050049
Epoch 370, val loss: 0.8928210139274597
Epoch 380, training loss: 6.73835563659668 = 0.6196569204330444 + 1.0 * 6.118698596954346
Epoch 380, val loss: 0.8785524964332581
Epoch 390, training loss: 6.71024751663208 = 0.5950039029121399 + 1.0 * 6.115243434906006
Epoch 390, val loss: 0.8652210831642151
Epoch 400, training loss: 6.691860198974609 = 0.5708296298980713 + 1.0 * 6.121030807495117
Epoch 400, val loss: 0.8527026176452637
Epoch 410, training loss: 6.657947540283203 = 0.5473172664642334 + 1.0 * 6.110630512237549
Epoch 410, val loss: 0.8409393429756165
Epoch 420, training loss: 6.631762981414795 = 0.5239171981811523 + 1.0 * 6.107845783233643
Epoch 420, val loss: 0.8296712636947632
Epoch 430, training loss: 6.617281913757324 = 0.5004379749298096 + 1.0 * 6.1168437004089355
Epoch 430, val loss: 0.8186902403831482
Epoch 440, training loss: 6.581899642944336 = 0.47709527611732483 + 1.0 * 6.104804515838623
Epoch 440, val loss: 0.8081761598587036
Epoch 450, training loss: 6.55405330657959 = 0.45372891426086426 + 1.0 * 6.100324630737305
Epoch 450, val loss: 0.7980285286903381
Epoch 460, training loss: 6.533086776733398 = 0.4302734136581421 + 1.0 * 6.102813243865967
Epoch 460, val loss: 0.7882604002952576
Epoch 470, training loss: 6.503452301025391 = 0.4071439504623413 + 1.0 * 6.09630823135376
Epoch 470, val loss: 0.7793296575546265
Epoch 480, training loss: 6.479349613189697 = 0.38441798090934753 + 1.0 * 6.094931602478027
Epoch 480, val loss: 0.7711052298545837
Epoch 490, training loss: 6.453845500946045 = 0.3620333969593048 + 1.0 * 6.0918121337890625
Epoch 490, val loss: 0.7636297345161438
Epoch 500, training loss: 6.432621479034424 = 0.3402668535709381 + 1.0 * 6.092354774475098
Epoch 500, val loss: 0.7571235299110413
Epoch 510, training loss: 6.412642955780029 = 0.31949082016944885 + 1.0 * 6.093152046203613
Epoch 510, val loss: 0.7515555024147034
Epoch 520, training loss: 6.38795804977417 = 0.2998110055923462 + 1.0 * 6.088147163391113
Epoch 520, val loss: 0.7469540238380432
Epoch 530, training loss: 6.365740776062012 = 0.28112053871154785 + 1.0 * 6.084620475769043
Epoch 530, val loss: 0.7432150840759277
Epoch 540, training loss: 6.347962379455566 = 0.2635992169380188 + 1.0 * 6.084362983703613
Epoch 540, val loss: 0.740264356136322
Epoch 550, training loss: 6.3278489112854 = 0.2473076581954956 + 1.0 * 6.080541133880615
Epoch 550, val loss: 0.7383639812469482
Epoch 560, training loss: 6.322587013244629 = 0.2321186661720276 + 1.0 * 6.090468406677246
Epoch 560, val loss: 0.7372534871101379
Epoch 570, training loss: 6.296746730804443 = 0.21817906200885773 + 1.0 * 6.0785675048828125
Epoch 570, val loss: 0.7368171811103821
Epoch 580, training loss: 6.282134056091309 = 0.2053099125623703 + 1.0 * 6.076824188232422
Epoch 580, val loss: 0.7371923923492432
Epoch 590, training loss: 6.2672038078308105 = 0.19333019852638245 + 1.0 * 6.073873519897461
Epoch 590, val loss: 0.7381480932235718
Epoch 600, training loss: 6.271523475646973 = 0.182216078042984 + 1.0 * 6.0893073081970215
Epoch 600, val loss: 0.7397270202636719
Epoch 610, training loss: 6.243154525756836 = 0.17199023067951202 + 1.0 * 6.071164131164551
Epoch 610, val loss: 0.7418102025985718
Epoch 620, training loss: 6.232476234436035 = 0.16251805424690247 + 1.0 * 6.069958209991455
Epoch 620, val loss: 0.7443885803222656
Epoch 630, training loss: 6.220991134643555 = 0.15367725491523743 + 1.0 * 6.0673136711120605
Epoch 630, val loss: 0.7473902106285095
Epoch 640, training loss: 6.233067512512207 = 0.14541281759738922 + 1.0 * 6.0876545906066895
Epoch 640, val loss: 0.7507177591323853
Epoch 650, training loss: 6.209129333496094 = 0.13782715797424316 + 1.0 * 6.0713019371032715
Epoch 650, val loss: 0.7542948126792908
Epoch 660, training loss: 6.195755958557129 = 0.13078664243221283 + 1.0 * 6.064969539642334
Epoch 660, val loss: 0.7582522630691528
Epoch 670, training loss: 6.185664176940918 = 0.12418012320995331 + 1.0 * 6.061483860015869
Epoch 670, val loss: 0.7623976469039917
Epoch 680, training loss: 6.19278621673584 = 0.11798379570245743 + 1.0 * 6.074802398681641
Epoch 680, val loss: 0.7667641639709473
Epoch 690, training loss: 6.177047252655029 = 0.11225004494190216 + 1.0 * 6.064797401428223
Epoch 690, val loss: 0.7711767554283142
Epoch 700, training loss: 6.1660847663879395 = 0.1068902239203453 + 1.0 * 6.059194564819336
Epoch 700, val loss: 0.7757777571678162
Epoch 710, training loss: 6.166818618774414 = 0.10184171050786972 + 1.0 * 6.064976692199707
Epoch 710, val loss: 0.7805162072181702
Epoch 720, training loss: 6.158486843109131 = 0.09711474180221558 + 1.0 * 6.06137228012085
Epoch 720, val loss: 0.7853205800056458
Epoch 730, training loss: 6.149252414703369 = 0.09269128739833832 + 1.0 * 6.05656099319458
Epoch 730, val loss: 0.7901777029037476
Epoch 740, training loss: 6.143036842346191 = 0.08850542455911636 + 1.0 * 6.054531574249268
Epoch 740, val loss: 0.7951464653015137
Epoch 750, training loss: 6.14484977722168 = 0.0845561996102333 + 1.0 * 6.060293674468994
Epoch 750, val loss: 0.8001509308815002
Epoch 760, training loss: 6.133099555969238 = 0.08084940910339355 + 1.0 * 6.052249908447266
Epoch 760, val loss: 0.8052220940589905
Epoch 770, training loss: 6.128387451171875 = 0.07734636962413788 + 1.0 * 6.051041126251221
Epoch 770, val loss: 0.8103569149971008
Epoch 780, training loss: 6.128170490264893 = 0.07402213662862778 + 1.0 * 6.054148197174072
Epoch 780, val loss: 0.8154494762420654
Epoch 790, training loss: 6.130041122436523 = 0.07089602202177048 + 1.0 * 6.059144973754883
Epoch 790, val loss: 0.8204567432403564
Epoch 800, training loss: 6.1160197257995605 = 0.06795984506607056 + 1.0 * 6.048059940338135
Epoch 800, val loss: 0.8254787921905518
Epoch 810, training loss: 6.113088130950928 = 0.06517736613750458 + 1.0 * 6.047910690307617
Epoch 810, val loss: 0.8305206894874573
Epoch 820, training loss: 6.113344192504883 = 0.06252947449684143 + 1.0 * 6.050814628601074
Epoch 820, val loss: 0.8355568051338196
Epoch 830, training loss: 6.1064348220825195 = 0.06002454832196236 + 1.0 * 6.046410083770752
Epoch 830, val loss: 0.8405494689941406
Epoch 840, training loss: 6.103272914886475 = 0.05764951929450035 + 1.0 * 6.045623302459717
Epoch 840, val loss: 0.845611035823822
Epoch 850, training loss: 6.100705146789551 = 0.05538719892501831 + 1.0 * 6.045318126678467
Epoch 850, val loss: 0.8506513237953186
Epoch 860, training loss: 6.1012773513793945 = 0.05323517322540283 + 1.0 * 6.048042297363281
Epoch 860, val loss: 0.8556798696517944
Epoch 870, training loss: 6.095056056976318 = 0.05119173973798752 + 1.0 * 6.0438642501831055
Epoch 870, val loss: 0.860552966594696
Epoch 880, training loss: 6.099804401397705 = 0.04925743490457535 + 1.0 * 6.050547122955322
Epoch 880, val loss: 0.8654806613922119
Epoch 890, training loss: 6.089571952819824 = 0.047421034425497055 + 1.0 * 6.042150974273682
Epoch 890, val loss: 0.870327353477478
Epoch 900, training loss: 6.086352348327637 = 0.045677777379751205 + 1.0 * 6.040674686431885
Epoch 900, val loss: 0.8751816749572754
Epoch 910, training loss: 6.095511436462402 = 0.04401730000972748 + 1.0 * 6.051494121551514
Epoch 910, val loss: 0.8798916935920715
Epoch 920, training loss: 6.085350513458252 = 0.042453065514564514 + 1.0 * 6.0428972244262695
Epoch 920, val loss: 0.884505033493042
Epoch 930, training loss: 6.078463077545166 = 0.04096692427992821 + 1.0 * 6.037496089935303
Epoch 930, val loss: 0.8892273902893066
Epoch 940, training loss: 6.075998306274414 = 0.03954065591096878 + 1.0 * 6.036457538604736
Epoch 940, val loss: 0.8939472436904907
Epoch 950, training loss: 6.074278354644775 = 0.03817143663764 + 1.0 * 6.036107063293457
Epoch 950, val loss: 0.898656964302063
Epoch 960, training loss: 6.086600303649902 = 0.03686092048883438 + 1.0 * 6.049739360809326
Epoch 960, val loss: 0.9032339453697205
Epoch 970, training loss: 6.070040225982666 = 0.03562320023775101 + 1.0 * 6.034417152404785
Epoch 970, val loss: 0.9077606797218323
Epoch 980, training loss: 6.069478988647461 = 0.03444282338023186 + 1.0 * 6.035036087036133
Epoch 980, val loss: 0.9123550057411194
Epoch 990, training loss: 6.066479206085205 = 0.03330694139003754 + 1.0 * 6.033172130584717
Epoch 990, val loss: 0.9169107675552368
Epoch 1000, training loss: 6.064477443695068 = 0.03221556544303894 + 1.0 * 6.032261848449707
Epoch 1000, val loss: 0.9215101599693298
Epoch 1010, training loss: 6.0825514793396 = 0.03117099590599537 + 1.0 * 6.051380634307861
Epoch 1010, val loss: 0.9260569214820862
Epoch 1020, training loss: 6.063328266143799 = 0.030173812061548233 + 1.0 * 6.033154487609863
Epoch 1020, val loss: 0.9303975701332092
Epoch 1030, training loss: 6.073663234710693 = 0.029231514781713486 + 1.0 * 6.044431686401367
Epoch 1030, val loss: 0.9348095059394836
Epoch 1040, training loss: 6.062864303588867 = 0.028332261368632317 + 1.0 * 6.034532070159912
Epoch 1040, val loss: 0.9391304850578308
Epoch 1050, training loss: 6.058391571044922 = 0.02747301571071148 + 1.0 * 6.030918598175049
Epoch 1050, val loss: 0.9435356855392456
Epoch 1060, training loss: 6.055795669555664 = 0.026644373312592506 + 1.0 * 6.029151439666748
Epoch 1060, val loss: 0.9479626417160034
Epoch 1070, training loss: 6.054642677307129 = 0.025845980271697044 + 1.0 * 6.028796672821045
Epoch 1070, val loss: 0.9524021744728088
Epoch 1080, training loss: 6.06222677230835 = 0.025079360231757164 + 1.0 * 6.037147521972656
Epoch 1080, val loss: 0.9568088054656982
Epoch 1090, training loss: 6.052465915679932 = 0.024349788203835487 + 1.0 * 6.028116226196289
Epoch 1090, val loss: 0.9610013365745544
Epoch 1100, training loss: 6.052085876464844 = 0.023653950542211533 + 1.0 * 6.0284318923950195
Epoch 1100, val loss: 0.9652459025382996
Epoch 1110, training loss: 6.060970306396484 = 0.022985003888607025 + 1.0 * 6.037985324859619
Epoch 1110, val loss: 0.9695093035697937
Epoch 1120, training loss: 6.051092624664307 = 0.022343939170241356 + 1.0 * 6.028748512268066
Epoch 1120, val loss: 0.9736889004707336
Epoch 1130, training loss: 6.0481133460998535 = 0.021731387823820114 + 1.0 * 6.026381969451904
Epoch 1130, val loss: 0.9778717160224915
Epoch 1140, training loss: 6.045909404754639 = 0.021138321608304977 + 1.0 * 6.024771213531494
Epoch 1140, val loss: 0.982060432434082
Epoch 1150, training loss: 6.047163486480713 = 0.020565956830978394 + 1.0 * 6.026597499847412
Epoch 1150, val loss: 0.9862026572227478
Epoch 1160, training loss: 6.053256988525391 = 0.02001606486737728 + 1.0 * 6.033240795135498
Epoch 1160, val loss: 0.9902556538581848
Epoch 1170, training loss: 6.048085689544678 = 0.019497303292155266 + 1.0 * 6.02858829498291
Epoch 1170, val loss: 0.9942342042922974
Epoch 1180, training loss: 6.042845726013184 = 0.01899993047118187 + 1.0 * 6.023845672607422
Epoch 1180, val loss: 0.9982695579528809
Epoch 1190, training loss: 6.040771961212158 = 0.01851741410791874 + 1.0 * 6.022254467010498
Epoch 1190, val loss: 1.0023105144500732
Epoch 1200, training loss: 6.052335739135742 = 0.01804913394153118 + 1.0 * 6.0342864990234375
Epoch 1200, val loss: 1.0062613487243652
Epoch 1210, training loss: 6.042950630187988 = 0.017600297927856445 + 1.0 * 6.025350093841553
Epoch 1210, val loss: 1.010180950164795
Epoch 1220, training loss: 6.0396857261657715 = 0.01717093400657177 + 1.0 * 6.022514820098877
Epoch 1220, val loss: 1.01405668258667
Epoch 1230, training loss: 6.038778781890869 = 0.0167548768222332 + 1.0 * 6.022023677825928
Epoch 1230, val loss: 1.0180273056030273
Epoch 1240, training loss: 6.0434346199035645 = 0.016350900754332542 + 1.0 * 6.027083873748779
Epoch 1240, val loss: 1.0218560695648193
Epoch 1250, training loss: 6.039352893829346 = 0.015963997691869736 + 1.0 * 6.023388862609863
Epoch 1250, val loss: 1.02570641040802
Epoch 1260, training loss: 6.0356879234313965 = 0.015591392293572426 + 1.0 * 6.020096302032471
Epoch 1260, val loss: 1.029478907585144
Epoch 1270, training loss: 6.034976482391357 = 0.01523072924464941 + 1.0 * 6.019745826721191
Epoch 1270, val loss: 1.033305048942566
Epoch 1280, training loss: 6.036794185638428 = 0.014880295842885971 + 1.0 * 6.021914005279541
Epoch 1280, val loss: 1.0370687246322632
Epoch 1290, training loss: 6.034470081329346 = 0.014542585238814354 + 1.0 * 6.019927501678467
Epoch 1290, val loss: 1.0407921075820923
Epoch 1300, training loss: 6.037460803985596 = 0.014218519441783428 + 1.0 * 6.023242473602295
Epoch 1300, val loss: 1.0444477796554565
Epoch 1310, training loss: 6.034725666046143 = 0.013907755725085735 + 1.0 * 6.020817756652832
Epoch 1310, val loss: 1.0481252670288086
Epoch 1320, training loss: 6.034581661224365 = 0.013605700805783272 + 1.0 * 6.0209760665893555
Epoch 1320, val loss: 1.0517061948776245
Epoch 1330, training loss: 6.030159950256348 = 0.01331318262964487 + 1.0 * 6.016846656799316
Epoch 1330, val loss: 1.0552839040756226
Epoch 1340, training loss: 6.029183864593506 = 0.013028796762228012 + 1.0 * 6.016155242919922
Epoch 1340, val loss: 1.0588772296905518
Epoch 1350, training loss: 6.030139446258545 = 0.01275185402482748 + 1.0 * 6.017387390136719
Epoch 1350, val loss: 1.0624475479125977
Epoch 1360, training loss: 6.038244247436523 = 0.012484046630561352 + 1.0 * 6.025760173797607
Epoch 1360, val loss: 1.0658717155456543
Epoch 1370, training loss: 6.029075622558594 = 0.012228449806571007 + 1.0 * 6.016847133636475
Epoch 1370, val loss: 1.0692847967147827
Epoch 1380, training loss: 6.028499126434326 = 0.011984026990830898 + 1.0 * 6.016515254974365
Epoch 1380, val loss: 1.0727579593658447
Epoch 1390, training loss: 6.026329517364502 = 0.011744147166609764 + 1.0 * 6.014585494995117
Epoch 1390, val loss: 1.0762276649475098
Epoch 1400, training loss: 6.028188228607178 = 0.011507966555655003 + 1.0 * 6.0166802406311035
Epoch 1400, val loss: 1.0795937776565552
Epoch 1410, training loss: 6.027796268463135 = 0.0112794553861022 + 1.0 * 6.01651668548584
Epoch 1410, val loss: 1.0828999280929565
Epoch 1420, training loss: 6.025171756744385 = 0.01106084045022726 + 1.0 * 6.014111042022705
Epoch 1420, val loss: 1.0861968994140625
Epoch 1430, training loss: 6.027848720550537 = 0.010849188081920147 + 1.0 * 6.0169997215271
Epoch 1430, val loss: 1.0895487070083618
Epoch 1440, training loss: 6.024655342102051 = 0.010642198845744133 + 1.0 * 6.014013290405273
Epoch 1440, val loss: 1.0928009748458862
Epoch 1450, training loss: 6.025205612182617 = 0.010441157035529613 + 1.0 * 6.014764308929443
Epoch 1450, val loss: 1.096161127090454
Epoch 1460, training loss: 6.030077934265137 = 0.010245712473988533 + 1.0 * 6.019832134246826
Epoch 1460, val loss: 1.099382996559143
Epoch 1470, training loss: 6.022579193115234 = 0.010055343620479107 + 1.0 * 6.012523651123047
Epoch 1470, val loss: 1.1025004386901855
Epoch 1480, training loss: 6.022130966186523 = 0.009871522895991802 + 1.0 * 6.012259483337402
Epoch 1480, val loss: 1.1057159900665283
Epoch 1490, training loss: 6.026810646057129 = 0.009692194871604443 + 1.0 * 6.017118453979492
Epoch 1490, val loss: 1.1089059114456177
Epoch 1500, training loss: 6.02304744720459 = 0.009517001919448376 + 1.0 * 6.013530254364014
Epoch 1500, val loss: 1.1120891571044922
Epoch 1510, training loss: 6.02016544342041 = 0.009348542429506779 + 1.0 * 6.010817050933838
Epoch 1510, val loss: 1.1152297258377075
Epoch 1520, training loss: 6.019867897033691 = 0.009184454567730427 + 1.0 * 6.010683536529541
Epoch 1520, val loss: 1.118375539779663
Epoch 1530, training loss: 6.023007392883301 = 0.00902306567877531 + 1.0 * 6.013984203338623
Epoch 1530, val loss: 1.1215202808380127
Epoch 1540, training loss: 6.021819591522217 = 0.008865933865308762 + 1.0 * 6.012953758239746
Epoch 1540, val loss: 1.1244474649429321
Epoch 1550, training loss: 6.022454738616943 = 0.008714689873158932 + 1.0 * 6.013740062713623
Epoch 1550, val loss: 1.127429485321045
Epoch 1560, training loss: 6.022362232208252 = 0.00856834091246128 + 1.0 * 6.0137939453125
Epoch 1560, val loss: 1.1304233074188232
Epoch 1570, training loss: 6.020935535430908 = 0.008425810374319553 + 1.0 * 6.012509822845459
Epoch 1570, val loss: 1.1335101127624512
Epoch 1580, training loss: 6.0192999839782715 = 0.008286078460514545 + 1.0 * 6.011013984680176
Epoch 1580, val loss: 1.1364127397537231
Epoch 1590, training loss: 6.022012233734131 = 0.008149664849042892 + 1.0 * 6.013862609863281
Epoch 1590, val loss: 1.1393954753875732
Epoch 1600, training loss: 6.017984390258789 = 0.00801753904670477 + 1.0 * 6.009966850280762
Epoch 1600, val loss: 1.1423512697219849
Epoch 1610, training loss: 6.0170416831970215 = 0.007887895219027996 + 1.0 * 6.009153842926025
Epoch 1610, val loss: 1.1452438831329346
Epoch 1620, training loss: 6.021677494049072 = 0.007760779932141304 + 1.0 * 6.013916492462158
Epoch 1620, val loss: 1.14815092086792
Epoch 1630, training loss: 6.019237041473389 = 0.007637412752956152 + 1.0 * 6.011599540710449
Epoch 1630, val loss: 1.1511059999465942
Epoch 1640, training loss: 6.017892360687256 = 0.0075170425698161125 + 1.0 * 6.010375499725342
Epoch 1640, val loss: 1.1538200378417969
Epoch 1650, training loss: 6.021051406860352 = 0.007400445640087128 + 1.0 * 6.013650894165039
Epoch 1650, val loss: 1.1566282510757446
Epoch 1660, training loss: 6.016468048095703 = 0.007286714389920235 + 1.0 * 6.009181499481201
Epoch 1660, val loss: 1.159388542175293
Epoch 1670, training loss: 6.01495885848999 = 0.007175908423960209 + 1.0 * 6.007782936096191
Epoch 1670, val loss: 1.162214756011963
Epoch 1680, training loss: 6.022884368896484 = 0.007067417725920677 + 1.0 * 6.015817165374756
Epoch 1680, val loss: 1.164955735206604
Epoch 1690, training loss: 6.0165934562683105 = 0.006960644852370024 + 1.0 * 6.009632587432861
Epoch 1690, val loss: 1.16766357421875
Epoch 1700, training loss: 6.01844596862793 = 0.006857959553599358 + 1.0 * 6.011588096618652
Epoch 1700, val loss: 1.1703392267227173
Epoch 1710, training loss: 6.013968467712402 = 0.0067566498182713985 + 1.0 * 6.007211685180664
Epoch 1710, val loss: 1.1729673147201538
Epoch 1720, training loss: 6.01418399810791 = 0.00665782717987895 + 1.0 * 6.007526397705078
Epoch 1720, val loss: 1.175660252571106
Epoch 1730, training loss: 6.016356468200684 = 0.006560429930686951 + 1.0 * 6.009796142578125
Epoch 1730, val loss: 1.1783219575881958
Epoch 1740, training loss: 6.013862133026123 = 0.006465425249189138 + 1.0 * 6.007396697998047
Epoch 1740, val loss: 1.1810247898101807
Epoch 1750, training loss: 6.016106128692627 = 0.0063730026595294476 + 1.0 * 6.009733200073242
Epoch 1750, val loss: 1.1835992336273193
Epoch 1760, training loss: 6.015172004699707 = 0.006282785441726446 + 1.0 * 6.008889198303223
Epoch 1760, val loss: 1.1860896348953247
Epoch 1770, training loss: 6.017004489898682 = 0.006195034831762314 + 1.0 * 6.010809421539307
Epoch 1770, val loss: 1.1887328624725342
Epoch 1780, training loss: 6.011441707611084 = 0.00610949844121933 + 1.0 * 6.005331993103027
Epoch 1780, val loss: 1.1912156343460083
Epoch 1790, training loss: 6.010354042053223 = 0.006025318522006273 + 1.0 * 6.004328727722168
Epoch 1790, val loss: 1.193891167640686
Epoch 1800, training loss: 6.012217044830322 = 0.005941817536950111 + 1.0 * 6.006275177001953
Epoch 1800, val loss: 1.1964482069015503
Epoch 1810, training loss: 6.017917156219482 = 0.005859698168933392 + 1.0 * 6.012057304382324
Epoch 1810, val loss: 1.198828101158142
Epoch 1820, training loss: 6.013062953948975 = 0.00578115601092577 + 1.0 * 6.00728178024292
Epoch 1820, val loss: 1.2012858390808105
Epoch 1830, training loss: 6.011414527893066 = 0.0057045575231313705 + 1.0 * 6.005710124969482
Epoch 1830, val loss: 1.2038018703460693
Epoch 1840, training loss: 6.016147613525391 = 0.005628952290862799 + 1.0 * 6.010518550872803
Epoch 1840, val loss: 1.206255316734314
Epoch 1850, training loss: 6.01027774810791 = 0.005554995499551296 + 1.0 * 6.004722595214844
Epoch 1850, val loss: 1.2087217569351196
Epoch 1860, training loss: 6.011093616485596 = 0.005482296459376812 + 1.0 * 6.005611419677734
Epoch 1860, val loss: 1.2111459970474243
Epoch 1870, training loss: 6.010268211364746 = 0.005411060526967049 + 1.0 * 6.004857063293457
Epoch 1870, val loss: 1.213526725769043
Epoch 1880, training loss: 6.009433269500732 = 0.005341380834579468 + 1.0 * 6.004091739654541
Epoch 1880, val loss: 1.2160354852676392
Epoch 1890, training loss: 6.012752532958984 = 0.005272651556879282 + 1.0 * 6.007479667663574
Epoch 1890, val loss: 1.2183278799057007
Epoch 1900, training loss: 6.008291244506836 = 0.0052055614069104195 + 1.0 * 6.003085613250732
Epoch 1900, val loss: 1.2207493782043457
Epoch 1910, training loss: 6.020341873168945 = 0.005140644498169422 + 1.0 * 6.015201091766357
Epoch 1910, val loss: 1.223096489906311
Epoch 1920, training loss: 6.010256767272949 = 0.0050766379572451115 + 1.0 * 6.005180358886719
Epoch 1920, val loss: 1.2253100872039795
Epoch 1930, training loss: 6.007697582244873 = 0.005015427712351084 + 1.0 * 6.002682209014893
Epoch 1930, val loss: 1.2275938987731934
Epoch 1940, training loss: 6.006133556365967 = 0.0049542151391506195 + 1.0 * 6.001179218292236
Epoch 1940, val loss: 1.2300058603286743
Epoch 1950, training loss: 6.006159782409668 = 0.004893118515610695 + 1.0 * 6.0012664794921875
Epoch 1950, val loss: 1.2323740720748901
Epoch 1960, training loss: 6.013087272644043 = 0.004832748789340258 + 1.0 * 6.008254528045654
Epoch 1960, val loss: 1.2346705198287964
Epoch 1970, training loss: 6.009247303009033 = 0.004773760214447975 + 1.0 * 6.004473686218262
Epoch 1970, val loss: 1.2369179725646973
Epoch 1980, training loss: 6.010429859161377 = 0.004717017058283091 + 1.0 * 6.005712985992432
Epoch 1980, val loss: 1.2390258312225342
Epoch 1990, training loss: 6.008184909820557 = 0.004662072751671076 + 1.0 * 6.003522872924805
Epoch 1990, val loss: 1.2412550449371338
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6125
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320413589477539 = 1.9465786218643188 + 1.0 * 8.373834609985352
Epoch 0, val loss: 1.9388163089752197
Epoch 10, training loss: 10.309457778930664 = 1.936182975769043 + 1.0 * 8.373274803161621
Epoch 10, val loss: 1.9287769794464111
Epoch 20, training loss: 10.292943954467773 = 1.9234049320220947 + 1.0 * 8.369539260864258
Epoch 20, val loss: 1.9159283638000488
Epoch 30, training loss: 10.249974250793457 = 1.9060980081558228 + 1.0 * 8.343875885009766
Epoch 30, val loss: 1.898152470588684
Epoch 40, training loss: 10.040553092956543 = 1.8846062421798706 + 1.0 * 8.155946731567383
Epoch 40, val loss: 1.876511812210083
Epoch 50, training loss: 9.261621475219727 = 1.8615061044692993 + 1.0 * 7.400115489959717
Epoch 50, val loss: 1.8533118963241577
Epoch 60, training loss: 8.928136825561523 = 1.8426228761672974 + 1.0 * 7.085513591766357
Epoch 60, val loss: 1.8346941471099854
Epoch 70, training loss: 8.666162490844727 = 1.8290683031082153 + 1.0 * 6.837093830108643
Epoch 70, val loss: 1.8205701112747192
Epoch 80, training loss: 8.488693237304688 = 1.8144826889038086 + 1.0 * 6.674211025238037
Epoch 80, val loss: 1.8058215379714966
Epoch 90, training loss: 8.374259948730469 = 1.8006569147109985 + 1.0 * 6.573602676391602
Epoch 90, val loss: 1.7919120788574219
Epoch 100, training loss: 8.297003746032715 = 1.7863789796829224 + 1.0 * 6.510624885559082
Epoch 100, val loss: 1.777747392654419
Epoch 110, training loss: 8.231592178344727 = 1.772458553314209 + 1.0 * 6.459133148193359
Epoch 110, val loss: 1.7638778686523438
Epoch 120, training loss: 8.170123100280762 = 1.7584580183029175 + 1.0 * 6.411665439605713
Epoch 120, val loss: 1.7499947547912598
Epoch 130, training loss: 8.1157808303833 = 1.7438231706619263 + 1.0 * 6.371957778930664
Epoch 130, val loss: 1.7356425523757935
Epoch 140, training loss: 8.063495635986328 = 1.7277193069458008 + 1.0 * 6.3357768058776855
Epoch 140, val loss: 1.7204296588897705
Epoch 150, training loss: 8.012873649597168 = 1.7092761993408203 + 1.0 * 6.303597450256348
Epoch 150, val loss: 1.7039762735366821
Epoch 160, training loss: 7.964165687561035 = 1.6874403953552246 + 1.0 * 6.2767252922058105
Epoch 160, val loss: 1.6852468252182007
Epoch 170, training loss: 7.91679048538208 = 1.660679817199707 + 1.0 * 6.256110668182373
Epoch 170, val loss: 1.6629467010498047
Epoch 180, training loss: 7.866395950317383 = 1.6277824640274048 + 1.0 * 6.238613605499268
Epoch 180, val loss: 1.6359034776687622
Epoch 190, training loss: 7.812601089477539 = 1.588503360748291 + 1.0 * 6.224097728729248
Epoch 190, val loss: 1.6039564609527588
Epoch 200, training loss: 7.752536773681641 = 1.5413362979888916 + 1.0 * 6.211200714111328
Epoch 200, val loss: 1.565838098526001
Epoch 210, training loss: 7.686073303222656 = 1.4858299493789673 + 1.0 * 6.2002434730529785
Epoch 210, val loss: 1.5212019681930542
Epoch 220, training loss: 7.616798400878906 = 1.4241546392440796 + 1.0 * 6.192643642425537
Epoch 220, val loss: 1.472469687461853
Epoch 230, training loss: 7.544034004211426 = 1.359184980392456 + 1.0 * 6.184849262237549
Epoch 230, val loss: 1.4218063354492188
Epoch 240, training loss: 7.469932556152344 = 1.292036771774292 + 1.0 * 6.177895545959473
Epoch 240, val loss: 1.3705012798309326
Epoch 250, training loss: 7.39694881439209 = 1.2250300645828247 + 1.0 * 6.171918869018555
Epoch 250, val loss: 1.3208434581756592
Epoch 260, training loss: 7.3269758224487305 = 1.1604328155517578 + 1.0 * 6.166543006896973
Epoch 260, val loss: 1.2746492624282837
Epoch 270, training loss: 7.25853157043457 = 1.0977599620819092 + 1.0 * 6.16077184677124
Epoch 270, val loss: 1.2309585809707642
Epoch 280, training loss: 7.197142124176025 = 1.0375466346740723 + 1.0 * 6.159595489501953
Epoch 280, val loss: 1.189471960067749
Epoch 290, training loss: 7.130155086517334 = 0.9802164435386658 + 1.0 * 6.149938583374023
Epoch 290, val loss: 1.1503040790557861
Epoch 300, training loss: 7.0691680908203125 = 0.9248242974281311 + 1.0 * 6.144343852996826
Epoch 300, val loss: 1.1124908924102783
Epoch 310, training loss: 7.014072418212891 = 0.8713142275810242 + 1.0 * 6.142758369445801
Epoch 310, val loss: 1.0758752822875977
Epoch 320, training loss: 6.956565856933594 = 0.820161759853363 + 1.0 * 6.136404037475586
Epoch 320, val loss: 1.0409044027328491
Epoch 330, training loss: 6.903075218200684 = 0.7708566188812256 + 1.0 * 6.132218837738037
Epoch 330, val loss: 1.007118582725525
Epoch 340, training loss: 6.856376647949219 = 0.7235650420188904 + 1.0 * 6.132811546325684
Epoch 340, val loss: 0.9748919606208801
Epoch 350, training loss: 6.804754734039307 = 0.6786616444587708 + 1.0 * 6.126092910766602
Epoch 350, val loss: 0.9445691704750061
Epoch 360, training loss: 6.7565155029296875 = 0.635413646697998 + 1.0 * 6.1211018562316895
Epoch 360, val loss: 0.915742039680481
Epoch 370, training loss: 6.713142395019531 = 0.5939595699310303 + 1.0 * 6.11918306350708
Epoch 370, val loss: 0.8886822462081909
Epoch 380, training loss: 6.670931816101074 = 0.5546404123306274 + 1.0 * 6.116291522979736
Epoch 380, val loss: 0.86366868019104
Epoch 390, training loss: 6.629669666290283 = 0.5172428488731384 + 1.0 * 6.1124267578125
Epoch 390, val loss: 0.8405555486679077
Epoch 400, training loss: 6.59384298324585 = 0.4820597469806671 + 1.0 * 6.111783027648926
Epoch 400, val loss: 0.8195971250534058
Epoch 410, training loss: 6.556402206420898 = 0.44921839237213135 + 1.0 * 6.107183933258057
Epoch 410, val loss: 0.8010682463645935
Epoch 420, training loss: 6.522818088531494 = 0.41840022802352905 + 1.0 * 6.10441780090332
Epoch 420, val loss: 0.7846718430519104
Epoch 430, training loss: 6.494082450866699 = 0.3897375166416168 + 1.0 * 6.104344844818115
Epoch 430, val loss: 0.7704618573188782
Epoch 440, training loss: 6.464399337768555 = 0.36322009563446045 + 1.0 * 6.101179122924805
Epoch 440, val loss: 0.758343517780304
Epoch 450, training loss: 6.437784671783447 = 0.3386869728565216 + 1.0 * 6.099097728729248
Epoch 450, val loss: 0.7479786276817322
Epoch 460, training loss: 6.409294128417969 = 0.3159073293209076 + 1.0 * 6.093386650085449
Epoch 460, val loss: 0.739133894443512
Epoch 470, training loss: 6.386298179626465 = 0.29467424750328064 + 1.0 * 6.091623783111572
Epoch 470, val loss: 0.7315926551818848
Epoch 480, training loss: 6.37005615234375 = 0.27484971284866333 + 1.0 * 6.095206260681152
Epoch 480, val loss: 0.7252138257026672
Epoch 490, training loss: 6.348020076751709 = 0.2565619647502899 + 1.0 * 6.091458320617676
Epoch 490, val loss: 0.7198194265365601
Epoch 500, training loss: 6.326656818389893 = 0.23972518742084503 + 1.0 * 6.0869317054748535
Epoch 500, val loss: 0.7155855894088745
Epoch 510, training loss: 6.30768346786499 = 0.22405849397182465 + 1.0 * 6.083624839782715
Epoch 510, val loss: 0.7122017741203308
Epoch 520, training loss: 6.290500640869141 = 0.20942701399326324 + 1.0 * 6.081073760986328
Epoch 520, val loss: 0.7095750570297241
Epoch 530, training loss: 6.296764373779297 = 0.19581615924835205 + 1.0 * 6.100948333740234
Epoch 530, val loss: 0.7076878547668457
Epoch 540, training loss: 6.265594005584717 = 0.1833897978067398 + 1.0 * 6.082204341888428
Epoch 540, val loss: 0.7065474987030029
Epoch 550, training loss: 6.24832010269165 = 0.1719365417957306 + 1.0 * 6.076383590698242
Epoch 550, val loss: 0.7061807513237
Epoch 560, training loss: 6.2366862297058105 = 0.16129715740680695 + 1.0 * 6.0753889083862305
Epoch 560, val loss: 0.7063683271408081
Epoch 570, training loss: 6.225348472595215 = 0.15146757662296295 + 1.0 * 6.073880672454834
Epoch 570, val loss: 0.7070720195770264
Epoch 580, training loss: 6.213815689086914 = 0.14247038960456848 + 1.0 * 6.071345329284668
Epoch 580, val loss: 0.7083826661109924
Epoch 590, training loss: 6.20410680770874 = 0.13414199650287628 + 1.0 * 6.06996488571167
Epoch 590, val loss: 0.7102388739585876
Epoch 600, training loss: 6.199833393096924 = 0.12639771401882172 + 1.0 * 6.0734357833862305
Epoch 600, val loss: 0.7125537991523743
Epoch 610, training loss: 6.1911396980285645 = 0.11925029754638672 + 1.0 * 6.071889400482178
Epoch 610, val loss: 0.7151444554328918
Epoch 620, training loss: 6.182963848114014 = 0.1126449778676033 + 1.0 * 6.070318698883057
Epoch 620, val loss: 0.718206524848938
Epoch 630, training loss: 6.172117233276367 = 0.1065295934677124 + 1.0 * 6.065587520599365
Epoch 630, val loss: 0.7216235995292664
Epoch 640, training loss: 6.164482116699219 = 0.10082849115133286 + 1.0 * 6.063653469085693
Epoch 640, val loss: 0.7253587245941162
Epoch 650, training loss: 6.167283535003662 = 0.0955108106136322 + 1.0 * 6.071772575378418
Epoch 650, val loss: 0.7293605208396912
Epoch 660, training loss: 6.152438640594482 = 0.0905851498246193 + 1.0 * 6.061853408813477
Epoch 660, val loss: 0.7335118055343628
Epoch 670, training loss: 6.146668910980225 = 0.08600936084985733 + 1.0 * 6.060659408569336
Epoch 670, val loss: 0.737946629524231
Epoch 680, training loss: 6.151236057281494 = 0.08172459155321121 + 1.0 * 6.069511413574219
Epoch 680, val loss: 0.7425412535667419
Epoch 690, training loss: 6.138185977935791 = 0.07775259763002396 + 1.0 * 6.060433387756348
Epoch 690, val loss: 0.7471485137939453
Epoch 700, training loss: 6.1325860023498535 = 0.07405728846788406 + 1.0 * 6.058528900146484
Epoch 700, val loss: 0.7519608736038208
Epoch 710, training loss: 6.1258955001831055 = 0.07058516144752502 + 1.0 * 6.055310249328613
Epoch 710, val loss: 0.7568374276161194
Epoch 720, training loss: 6.134702682495117 = 0.0673220157623291 + 1.0 * 6.067380428314209
Epoch 720, val loss: 0.7617738842964172
Epoch 730, training loss: 6.118539810180664 = 0.06427975744009018 + 1.0 * 6.05426025390625
Epoch 730, val loss: 0.7667067646980286
Epoch 740, training loss: 6.1141767501831055 = 0.06143692880868912 + 1.0 * 6.05273962020874
Epoch 740, val loss: 0.7718083262443542
Epoch 750, training loss: 6.11094331741333 = 0.058755628764629364 + 1.0 * 6.052187919616699
Epoch 750, val loss: 0.7769147753715515
Epoch 760, training loss: 6.110515117645264 = 0.05623066425323486 + 1.0 * 6.054284572601318
Epoch 760, val loss: 0.7819259166717529
Epoch 770, training loss: 6.1061201095581055 = 0.053874559700489044 + 1.0 * 6.052245616912842
Epoch 770, val loss: 0.7870110869407654
Epoch 780, training loss: 6.103580474853516 = 0.051655638962984085 + 1.0 * 6.051924705505371
Epoch 780, val loss: 0.7920822501182556
Epoch 790, training loss: 6.097420692443848 = 0.04956310614943504 + 1.0 * 6.047857761383057
Epoch 790, val loss: 0.7971633076667786
Epoch 800, training loss: 6.094158172607422 = 0.04758891835808754 + 1.0 * 6.046569347381592
Epoch 800, val loss: 0.8022379279136658
Epoch 810, training loss: 6.091909408569336 = 0.045716531574726105 + 1.0 * 6.046192646026611
Epoch 810, val loss: 0.8073341846466064
Epoch 820, training loss: 6.100955486297607 = 0.043940093368291855 + 1.0 * 6.057015419006348
Epoch 820, val loss: 0.8124144673347473
Epoch 830, training loss: 6.090910911560059 = 0.04227543622255325 + 1.0 * 6.048635482788086
Epoch 830, val loss: 0.8174306154251099
Epoch 840, training loss: 6.086430549621582 = 0.040710315108299255 + 1.0 * 6.045720100402832
Epoch 840, val loss: 0.8225146532058716
Epoch 850, training loss: 6.082199573516846 = 0.03922343626618385 + 1.0 * 6.042975902557373
Epoch 850, val loss: 0.8275526762008667
Epoch 860, training loss: 6.086790561676025 = 0.03780873492360115 + 1.0 * 6.048981666564941
Epoch 860, val loss: 0.8325139880180359
Epoch 870, training loss: 6.08253812789917 = 0.03646930679678917 + 1.0 * 6.0460686683654785
Epoch 870, val loss: 0.8373547196388245
Epoch 880, training loss: 6.0766825675964355 = 0.03520410880446434 + 1.0 * 6.041478633880615
Epoch 880, val loss: 0.8422965407371521
Epoch 890, training loss: 6.080051422119141 = 0.03400326147675514 + 1.0 * 6.046048164367676
Epoch 890, val loss: 0.8471845388412476
Epoch 900, training loss: 6.073655605316162 = 0.03286488726735115 + 1.0 * 6.040790557861328
Epoch 900, val loss: 0.8519656658172607
Epoch 910, training loss: 6.0708136558532715 = 0.03178005293011665 + 1.0 * 6.03903341293335
Epoch 910, val loss: 0.8567502498626709
Epoch 920, training loss: 6.072059631347656 = 0.03074314445257187 + 1.0 * 6.041316509246826
Epoch 920, val loss: 0.8614808917045593
Epoch 930, training loss: 6.068760395050049 = 0.029754258692264557 + 1.0 * 6.039006233215332
Epoch 930, val loss: 0.8661236763000488
Epoch 940, training loss: 6.070275783538818 = 0.02881592884659767 + 1.0 * 6.041460037231445
Epoch 940, val loss: 0.8708294034004211
Epoch 950, training loss: 6.067809104919434 = 0.027924498543143272 + 1.0 * 6.039884567260742
Epoch 950, val loss: 0.8753687739372253
Epoch 960, training loss: 6.063453197479248 = 0.027077069506049156 + 1.0 * 6.036375999450684
Epoch 960, val loss: 0.8798666596412659
Epoch 970, training loss: 6.06147575378418 = 0.026268595829606056 + 1.0 * 6.035207271575928
Epoch 970, val loss: 0.8844030499458313
Epoch 980, training loss: 6.058832168579102 = 0.02549130842089653 + 1.0 * 6.033340930938721
Epoch 980, val loss: 0.8888721466064453
Epoch 990, training loss: 6.062133312225342 = 0.024744253605604172 + 1.0 * 6.037389278411865
Epoch 990, val loss: 0.8932859301567078
Epoch 1000, training loss: 6.058365345001221 = 0.024030106142163277 + 1.0 * 6.034335136413574
Epoch 1000, val loss: 0.8977285623550415
Epoch 1010, training loss: 6.057925701141357 = 0.023350728675723076 + 1.0 * 6.03457498550415
Epoch 1010, val loss: 0.9019938111305237
Epoch 1020, training loss: 6.054263591766357 = 0.02270037680864334 + 1.0 * 6.0315632820129395
Epoch 1020, val loss: 0.9063838124275208
Epoch 1030, training loss: 6.061106204986572 = 0.022074740380048752 + 1.0 * 6.039031505584717
Epoch 1030, val loss: 0.9106523990631104
Epoch 1040, training loss: 6.057886123657227 = 0.0214761421084404 + 1.0 * 6.036409854888916
Epoch 1040, val loss: 0.9147335290908813
Epoch 1050, training loss: 6.0508527755737305 = 0.020907077938318253 + 1.0 * 6.0299458503723145
Epoch 1050, val loss: 0.9189325571060181
Epoch 1060, training loss: 6.049181938171387 = 0.020359547808766365 + 1.0 * 6.028822422027588
Epoch 1060, val loss: 0.9230937361717224
Epoch 1070, training loss: 6.057980537414551 = 0.01982976496219635 + 1.0 * 6.038150787353516
Epoch 1070, val loss: 0.9271934032440186
Epoch 1080, training loss: 6.052667617797852 = 0.019322359934449196 + 1.0 * 6.0333452224731445
Epoch 1080, val loss: 0.9311716556549072
Epoch 1090, training loss: 6.0467376708984375 = 0.01883745566010475 + 1.0 * 6.027900218963623
Epoch 1090, val loss: 0.9351633191108704
Epoch 1100, training loss: 6.051687240600586 = 0.01836986094713211 + 1.0 * 6.033317565917969
Epoch 1100, val loss: 0.9391277432441711
Epoch 1110, training loss: 6.045825004577637 = 0.017919810488820076 + 1.0 * 6.027904987335205
Epoch 1110, val loss: 0.9430249333381653
Epoch 1120, training loss: 6.0440850257873535 = 0.017487939447164536 + 1.0 * 6.026597023010254
Epoch 1120, val loss: 0.9468808770179749
Epoch 1130, training loss: 6.042751312255859 = 0.017070507630705833 + 1.0 * 6.025681018829346
Epoch 1130, val loss: 0.9507498145103455
Epoch 1140, training loss: 6.049193859100342 = 0.016665225848555565 + 1.0 * 6.032528400421143
Epoch 1140, val loss: 0.9545642137527466
Epoch 1150, training loss: 6.0469970703125 = 0.016276495531201363 + 1.0 * 6.0307207107543945
Epoch 1150, val loss: 0.9583073258399963
Epoch 1160, training loss: 6.040488243103027 = 0.015903668478131294 + 1.0 * 6.024584770202637
Epoch 1160, val loss: 0.9619843363761902
Epoch 1170, training loss: 6.038340091705322 = 0.015543228946626186 + 1.0 * 6.022796630859375
Epoch 1170, val loss: 0.965728759765625
Epoch 1180, training loss: 6.0486555099487305 = 0.015192500315606594 + 1.0 * 6.033463001251221
Epoch 1180, val loss: 0.9693523049354553
Epoch 1190, training loss: 6.041342258453369 = 0.01485570427030325 + 1.0 * 6.026486396789551
Epoch 1190, val loss: 0.9730242490768433
Epoch 1200, training loss: 6.037819862365723 = 0.014531106688082218 + 1.0 * 6.023288726806641
Epoch 1200, val loss: 0.9765608310699463
Epoch 1210, training loss: 6.039058685302734 = 0.014216683804988861 + 1.0 * 6.024841785430908
Epoch 1210, val loss: 0.9801502227783203
Epoch 1220, training loss: 6.036011695861816 = 0.013912179507315159 + 1.0 * 6.022099494934082
Epoch 1220, val loss: 0.9836791157722473
Epoch 1230, training loss: 6.037099361419678 = 0.013617567718029022 + 1.0 * 6.023481845855713
Epoch 1230, val loss: 0.9871616959571838
Epoch 1240, training loss: 6.03952693939209 = 0.013333126902580261 + 1.0 * 6.026193618774414
Epoch 1240, val loss: 0.9906288385391235
Epoch 1250, training loss: 6.035585880279541 = 0.01305869035422802 + 1.0 * 6.02252721786499
Epoch 1250, val loss: 0.9939584136009216
Epoch 1260, training loss: 6.032629013061523 = 0.012793569825589657 + 1.0 * 6.019835472106934
Epoch 1260, val loss: 0.9973391890525818
Epoch 1270, training loss: 6.032065391540527 = 0.012535699643194675 + 1.0 * 6.019529819488525
Epoch 1270, val loss: 1.0006879568099976
Epoch 1280, training loss: 6.040075778961182 = 0.012284337542951107 + 1.0 * 6.027791500091553
Epoch 1280, val loss: 1.0040059089660645
Epoch 1290, training loss: 6.032138347625732 = 0.012041651643812656 + 1.0 * 6.020096778869629
Epoch 1290, val loss: 1.0072413682937622
Epoch 1300, training loss: 6.029666900634766 = 0.011806858703494072 + 1.0 * 6.017859935760498
Epoch 1300, val loss: 1.0104960203170776
Epoch 1310, training loss: 6.036567211151123 = 0.011578358709812164 + 1.0 * 6.024988651275635
Epoch 1310, val loss: 1.0137991905212402
Epoch 1320, training loss: 6.029769420623779 = 0.011355997063219547 + 1.0 * 6.018413543701172
Epoch 1320, val loss: 1.0168991088867188
Epoch 1330, training loss: 6.037322044372559 = 0.011142587289214134 + 1.0 * 6.026179313659668
Epoch 1330, val loss: 1.0200306177139282
Epoch 1340, training loss: 6.029730319976807 = 0.010935774073004723 + 1.0 * 6.018794536590576
Epoch 1340, val loss: 1.0231727361679077
Epoch 1350, training loss: 6.026932239532471 = 0.01073531899601221 + 1.0 * 6.0161967277526855
Epoch 1350, val loss: 1.026292324066162
Epoch 1360, training loss: 6.0303239822387695 = 0.010539345443248749 + 1.0 * 6.019784450531006
Epoch 1360, val loss: 1.0294032096862793
Epoch 1370, training loss: 6.025853157043457 = 0.010347558185458183 + 1.0 * 6.015505790710449
Epoch 1370, val loss: 1.0323933362960815
Epoch 1380, training loss: 6.026247024536133 = 0.010162546299397945 + 1.0 * 6.016084671020508
Epoch 1380, val loss: 1.0354427099227905
Epoch 1390, training loss: 6.02679443359375 = 0.009982381947338581 + 1.0 * 6.016811847686768
Epoch 1390, val loss: 1.0384379625320435
Epoch 1400, training loss: 6.025152206420898 = 0.009806538932025433 + 1.0 * 6.015345573425293
Epoch 1400, val loss: 1.0414224863052368
Epoch 1410, training loss: 6.027089595794678 = 0.009635561145842075 + 1.0 * 6.017454147338867
Epoch 1410, val loss: 1.0444124937057495
Epoch 1420, training loss: 6.026278018951416 = 0.009469762444496155 + 1.0 * 6.016808032989502
Epoch 1420, val loss: 1.0473287105560303
Epoch 1430, training loss: 6.023616313934326 = 0.009308326058089733 + 1.0 * 6.014307975769043
Epoch 1430, val loss: 1.0502153635025024
Epoch 1440, training loss: 6.027199745178223 = 0.009151624515652657 + 1.0 * 6.018048286437988
Epoch 1440, val loss: 1.0531522035598755
Epoch 1450, training loss: 6.022388458251953 = 0.008998407982289791 + 1.0 * 6.013390064239502
Epoch 1450, val loss: 1.055924892425537
Epoch 1460, training loss: 6.026790618896484 = 0.00885021686553955 + 1.0 * 6.017940521240234
Epoch 1460, val loss: 1.0587841272354126
Epoch 1470, training loss: 6.0208282470703125 = 0.008705456741154194 + 1.0 * 6.012122631072998
Epoch 1470, val loss: 1.0615596771240234
Epoch 1480, training loss: 6.02182149887085 = 0.00856503751128912 + 1.0 * 6.013256549835205
Epoch 1480, val loss: 1.0644205808639526
Epoch 1490, training loss: 6.0233235359191895 = 0.008427192457020283 + 1.0 * 6.014896392822266
Epoch 1490, val loss: 1.0671485662460327
Epoch 1500, training loss: 6.020323753356934 = 0.008293308317661285 + 1.0 * 6.012030601501465
Epoch 1500, val loss: 1.0698785781860352
Epoch 1510, training loss: 6.023345947265625 = 0.008162747137248516 + 1.0 * 6.015182971954346
Epoch 1510, val loss: 1.0726525783538818
Epoch 1520, training loss: 6.018399238586426 = 0.008035058155655861 + 1.0 * 6.010364055633545
Epoch 1520, val loss: 1.0753072500228882
Epoch 1530, training loss: 6.019571781158447 = 0.00791085697710514 + 1.0 * 6.011661052703857
Epoch 1530, val loss: 1.0780218839645386
Epoch 1540, training loss: 6.022315979003906 = 0.007789195980876684 + 1.0 * 6.014526844024658
Epoch 1540, val loss: 1.0806692838668823
Epoch 1550, training loss: 6.01920747756958 = 0.007670827675610781 + 1.0 * 6.011536598205566
Epoch 1550, val loss: 1.083319902420044
Epoch 1560, training loss: 6.017943859100342 = 0.007555863820016384 + 1.0 * 6.010387897491455
Epoch 1560, val loss: 1.0859639644622803
Epoch 1570, training loss: 6.021071434020996 = 0.007442737929522991 + 1.0 * 6.0136284828186035
Epoch 1570, val loss: 1.0885818004608154
Epoch 1580, training loss: 6.016993999481201 = 0.0073322514072060585 + 1.0 * 6.009661674499512
Epoch 1580, val loss: 1.091192603111267
Epoch 1590, training loss: 6.017103672027588 = 0.007224628236144781 + 1.0 * 6.009879112243652
Epoch 1590, val loss: 1.093772053718567
Epoch 1600, training loss: 6.018587112426758 = 0.007118996698409319 + 1.0 * 6.011467933654785
Epoch 1600, val loss: 1.0963205099105835
Epoch 1610, training loss: 6.017932891845703 = 0.0070154983550310135 + 1.0 * 6.0109171867370605
Epoch 1610, val loss: 1.0988389253616333
Epoch 1620, training loss: 6.015003681182861 = 0.006914845202118158 + 1.0 * 6.008089065551758
Epoch 1620, val loss: 1.1013576984405518
Epoch 1630, training loss: 6.017352104187012 = 0.006816746201366186 + 1.0 * 6.01053524017334
Epoch 1630, val loss: 1.103929877281189
Epoch 1640, training loss: 6.01580810546875 = 0.006720562465488911 + 1.0 * 6.009087562561035
Epoch 1640, val loss: 1.106368899345398
Epoch 1650, training loss: 6.016806125640869 = 0.006626972928643227 + 1.0 * 6.010179042816162
Epoch 1650, val loss: 1.108831763267517
Epoch 1660, training loss: 6.012557506561279 = 0.006535961292684078 + 1.0 * 6.006021499633789
Epoch 1660, val loss: 1.1112514734268188
Epoch 1670, training loss: 6.012208461761475 = 0.006446552462875843 + 1.0 * 6.005762100219727
Epoch 1670, val loss: 1.1137592792510986
Epoch 1680, training loss: 6.01774787902832 = 0.006357955746352673 + 1.0 * 6.01138973236084
Epoch 1680, val loss: 1.116188645362854
Epoch 1690, training loss: 6.015048980712891 = 0.006271363701671362 + 1.0 * 6.008777618408203
Epoch 1690, val loss: 1.1185452938079834
Epoch 1700, training loss: 6.012258052825928 = 0.006187858060002327 + 1.0 * 6.006070137023926
Epoch 1700, val loss: 1.1209114789962769
Epoch 1710, training loss: 6.012433052062988 = 0.006106339395046234 + 1.0 * 6.006326675415039
Epoch 1710, val loss: 1.1233100891113281
Epoch 1720, training loss: 6.013885498046875 = 0.006025738548487425 + 1.0 * 6.007859706878662
Epoch 1720, val loss: 1.1256985664367676
Epoch 1730, training loss: 6.010981559753418 = 0.0059469398111104965 + 1.0 * 6.005034446716309
Epoch 1730, val loss: 1.1281096935272217
Epoch 1740, training loss: 6.013171672821045 = 0.00586942583322525 + 1.0 * 6.007302284240723
Epoch 1740, val loss: 1.1304140090942383
Epoch 1750, training loss: 6.013364315032959 = 0.0057931747287511826 + 1.0 * 6.007571220397949
Epoch 1750, val loss: 1.1327452659606934
Epoch 1760, training loss: 6.011792182922363 = 0.005719063337892294 + 1.0 * 6.006072998046875
Epoch 1760, val loss: 1.1351083517074585
Epoch 1770, training loss: 6.015283107757568 = 0.005646382924169302 + 1.0 * 6.009636878967285
Epoch 1770, val loss: 1.1373854875564575
Epoch 1780, training loss: 6.0098347663879395 = 0.005575037561357021 + 1.0 * 6.0042595863342285
Epoch 1780, val loss: 1.1396063566207886
Epoch 1790, training loss: 6.010303974151611 = 0.005505702458322048 + 1.0 * 6.004798412322998
Epoch 1790, val loss: 1.1419041156768799
Epoch 1800, training loss: 6.012643814086914 = 0.005437086336314678 + 1.0 * 6.007206916809082
Epoch 1800, val loss: 1.1441420316696167
Epoch 1810, training loss: 6.010417461395264 = 0.005370308179408312 + 1.0 * 6.00504732131958
Epoch 1810, val loss: 1.1463972330093384
Epoch 1820, training loss: 6.00937032699585 = 0.005304685328155756 + 1.0 * 6.00406551361084
Epoch 1820, val loss: 1.1486049890518188
Epoch 1830, training loss: 6.008925437927246 = 0.005240002181380987 + 1.0 * 6.003685474395752
Epoch 1830, val loss: 1.1508063077926636
Epoch 1840, training loss: 6.008607864379883 = 0.005176414269953966 + 1.0 * 6.00343132019043
Epoch 1840, val loss: 1.1529912948608398
Epoch 1850, training loss: 6.010966777801514 = 0.005114075727760792 + 1.0 * 6.005852699279785
Epoch 1850, val loss: 1.155192494392395
Epoch 1860, training loss: 6.012757778167725 = 0.005053475499153137 + 1.0 * 6.007704257965088
Epoch 1860, val loss: 1.157415509223938
Epoch 1870, training loss: 6.008955478668213 = 0.004993957467377186 + 1.0 * 6.003961563110352
Epoch 1870, val loss: 1.1594699621200562
Epoch 1880, training loss: 6.007901191711426 = 0.004936032462865114 + 1.0 * 6.002964973449707
Epoch 1880, val loss: 1.1616698503494263
Epoch 1890, training loss: 6.009847164154053 = 0.004878488834947348 + 1.0 * 6.004968643188477
Epoch 1890, val loss: 1.1637519598007202
Epoch 1900, training loss: 6.005770683288574 = 0.004821892827749252 + 1.0 * 6.000948905944824
Epoch 1900, val loss: 1.1659237146377563
Epoch 1910, training loss: 6.011419296264648 = 0.004766341298818588 + 1.0 * 6.00665283203125
Epoch 1910, val loss: 1.1680082082748413
Epoch 1920, training loss: 6.007435321807861 = 0.004711863584816456 + 1.0 * 6.002723693847656
Epoch 1920, val loss: 1.1701006889343262
Epoch 1930, training loss: 6.005429267883301 = 0.0046587008982896805 + 1.0 * 6.000770568847656
Epoch 1930, val loss: 1.1722100973129272
Epoch 1940, training loss: 6.00738000869751 = 0.004606523085385561 + 1.0 * 6.002773284912109
Epoch 1940, val loss: 1.1743038892745972
Epoch 1950, training loss: 6.005552768707275 = 0.0045547871850430965 + 1.0 * 6.000998020172119
Epoch 1950, val loss: 1.176318883895874
Epoch 1960, training loss: 6.007598876953125 = 0.00450412230566144 + 1.0 * 6.003094673156738
Epoch 1960, val loss: 1.1784188747406006
Epoch 1970, training loss: 6.004390716552734 = 0.004454309120774269 + 1.0 * 5.999936580657959
Epoch 1970, val loss: 1.1804310083389282
Epoch 1980, training loss: 6.003551006317139 = 0.0044057779014110565 + 1.0 * 5.999145030975342
Epoch 1980, val loss: 1.1824736595153809
Epoch 1990, training loss: 6.0107645988464355 = 0.0043578376062214375 + 1.0 * 6.006406784057617
Epoch 1990, val loss: 1.1844608783721924
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.8081
Flip ASR: 0.7733/225 nodes
The final ASR:0.71218, 0.07988, Accuracy:0.82469, 0.01429
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11566])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10486])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.325884819030762 = 1.951979637145996 + 1.0 * 8.373905181884766
Epoch 0, val loss: 1.9540753364562988
Epoch 10, training loss: 10.315254211425781 = 1.9416314363479614 + 1.0 * 8.37362289428711
Epoch 10, val loss: 1.9447342157363892
Epoch 20, training loss: 10.300284385681152 = 1.928922414779663 + 1.0 * 8.37136173248291
Epoch 20, val loss: 1.9328391551971436
Epoch 30, training loss: 10.26334285736084 = 1.9112050533294678 + 1.0 * 8.352137565612793
Epoch 30, val loss: 1.9160531759262085
Epoch 40, training loss: 10.100172996520996 = 1.8877872228622437 + 1.0 * 8.212386131286621
Epoch 40, val loss: 1.8945800065994263
Epoch 50, training loss: 9.550704002380371 = 1.8630586862564087 + 1.0 * 7.687644958496094
Epoch 50, val loss: 1.872455358505249
Epoch 60, training loss: 9.234021186828613 = 1.841612696647644 + 1.0 * 7.39240837097168
Epoch 60, val loss: 1.8530464172363281
Epoch 70, training loss: 8.789847373962402 = 1.82610285282135 + 1.0 * 6.963744163513184
Epoch 70, val loss: 1.8391962051391602
Epoch 80, training loss: 8.503881454467773 = 1.8131088018417358 + 1.0 * 6.690772533416748
Epoch 80, val loss: 1.8272414207458496
Epoch 90, training loss: 8.340743064880371 = 1.7988396883010864 + 1.0 * 6.541903018951416
Epoch 90, val loss: 1.8138586282730103
Epoch 100, training loss: 8.242485046386719 = 1.7816100120544434 + 1.0 * 6.460875511169434
Epoch 100, val loss: 1.7983391284942627
Epoch 110, training loss: 8.167709350585938 = 1.7622100114822388 + 1.0 * 6.40549898147583
Epoch 110, val loss: 1.7818124294281006
Epoch 120, training loss: 8.106019973754883 = 1.7416486740112305 + 1.0 * 6.3643717765808105
Epoch 120, val loss: 1.7646549940109253
Epoch 130, training loss: 8.052148818969727 = 1.719192385673523 + 1.0 * 6.332956790924072
Epoch 130, val loss: 1.745760202407837
Epoch 140, training loss: 7.9990129470825195 = 1.6932573318481445 + 1.0 * 6.305755615234375
Epoch 140, val loss: 1.724156379699707
Epoch 150, training loss: 7.946047306060791 = 1.6627821922302246 + 1.0 * 6.283265113830566
Epoch 150, val loss: 1.6993268728256226
Epoch 160, training loss: 7.889191627502441 = 1.6273059844970703 + 1.0 * 6.261885643005371
Epoch 160, val loss: 1.670920968055725
Epoch 170, training loss: 7.831052303314209 = 1.5857869386672974 + 1.0 * 6.245265483856201
Epoch 170, val loss: 1.6378017663955688
Epoch 180, training loss: 7.768858909606934 = 1.537911057472229 + 1.0 * 6.230947971343994
Epoch 180, val loss: 1.5995508432388306
Epoch 190, training loss: 7.702992916107178 = 1.4839520454406738 + 1.0 * 6.219040870666504
Epoch 190, val loss: 1.55617356300354
Epoch 200, training loss: 7.638479232788086 = 1.4250820875167847 + 1.0 * 6.213397026062012
Epoch 200, val loss: 1.5090502500534058
Epoch 210, training loss: 7.565792560577393 = 1.3639129400253296 + 1.0 * 6.201879501342773
Epoch 210, val loss: 1.460124135017395
Epoch 220, training loss: 7.495362758636475 = 1.3012040853500366 + 1.0 * 6.194158554077148
Epoch 220, val loss: 1.4098963737487793
Epoch 230, training loss: 7.425196170806885 = 1.2378720045089722 + 1.0 * 6.187324047088623
Epoch 230, val loss: 1.359495997428894
Epoch 240, training loss: 7.356855392456055 = 1.176294207572937 + 1.0 * 6.180561065673828
Epoch 240, val loss: 1.3112086057662964
Epoch 250, training loss: 7.291952133178711 = 1.1171787977218628 + 1.0 * 6.174773216247559
Epoch 250, val loss: 1.2653108835220337
Epoch 260, training loss: 7.2281317710876465 = 1.0597052574157715 + 1.0 * 6.168426513671875
Epoch 260, val loss: 1.2214690446853638
Epoch 270, training loss: 7.174407958984375 = 1.0037060976028442 + 1.0 * 6.17070198059082
Epoch 270, val loss: 1.1792242527008057
Epoch 280, training loss: 7.110053062438965 = 0.9505348205566406 + 1.0 * 6.159518241882324
Epoch 280, val loss: 1.1394755840301514
Epoch 290, training loss: 7.053436279296875 = 0.8995901942253113 + 1.0 * 6.153846263885498
Epoch 290, val loss: 1.1019450426101685
Epoch 300, training loss: 7.000391960144043 = 0.8508791327476501 + 1.0 * 6.149512767791748
Epoch 300, val loss: 1.0665558576583862
Epoch 310, training loss: 6.953492164611816 = 0.8051297068595886 + 1.0 * 6.148362636566162
Epoch 310, val loss: 1.0338971614837646
Epoch 320, training loss: 6.9032440185546875 = 0.7625207304954529 + 1.0 * 6.14072322845459
Epoch 320, val loss: 1.0043559074401855
Epoch 330, training loss: 6.865240573883057 = 0.7226499319076538 + 1.0 * 6.142590522766113
Epoch 330, val loss: 0.9775365591049194
Epoch 340, training loss: 6.819834232330322 = 0.6856996417045593 + 1.0 * 6.134134769439697
Epoch 340, val loss: 0.953460156917572
Epoch 350, training loss: 6.782314300537109 = 0.6510717868804932 + 1.0 * 6.131242275238037
Epoch 350, val loss: 0.9318130612373352
Epoch 360, training loss: 6.751012802124023 = 0.6183822751045227 + 1.0 * 6.132630348205566
Epoch 360, val loss: 0.9122307896614075
Epoch 370, training loss: 6.719249725341797 = 0.5877287983894348 + 1.0 * 6.131520748138428
Epoch 370, val loss: 0.8947353959083557
Epoch 380, training loss: 6.681451797485352 = 0.5589512586593628 + 1.0 * 6.122500419616699
Epoch 380, val loss: 0.8789767622947693
Epoch 390, training loss: 6.6499342918396 = 0.5313068628311157 + 1.0 * 6.118627548217773
Epoch 390, val loss: 0.8645444512367249
Epoch 400, training loss: 6.620127201080322 = 0.5045450329780579 + 1.0 * 6.11558198928833
Epoch 400, val loss: 0.8511252999305725
Epoch 410, training loss: 6.59110164642334 = 0.47844770550727844 + 1.0 * 6.112653732299805
Epoch 410, val loss: 0.8386646509170532
Epoch 420, training loss: 6.578367710113525 = 0.45286139845848083 + 1.0 * 6.125506401062012
Epoch 420, val loss: 0.8270286321640015
Epoch 430, training loss: 6.541454792022705 = 0.42819884419441223 + 1.0 * 6.113255977630615
Epoch 430, val loss: 0.816302478313446
Epoch 440, training loss: 6.51043701171875 = 0.40401938557624817 + 1.0 * 6.106417655944824
Epoch 440, val loss: 0.8065208196640015
Epoch 450, training loss: 6.48361349105835 = 0.3801613748073578 + 1.0 * 6.103452205657959
Epoch 450, val loss: 0.7974022626876831
Epoch 460, training loss: 6.461884021759033 = 0.3566686511039734 + 1.0 * 6.105215549468994
Epoch 460, val loss: 0.7891399264335632
Epoch 470, training loss: 6.434386253356934 = 0.33383607864379883 + 1.0 * 6.100550174713135
Epoch 470, val loss: 0.7818021178245544
Epoch 480, training loss: 6.4122796058654785 = 0.31178128719329834 + 1.0 * 6.100498199462891
Epoch 480, val loss: 0.7754489183425903
Epoch 490, training loss: 6.388120651245117 = 0.2907346189022064 + 1.0 * 6.097385883331299
Epoch 490, val loss: 0.7702122330665588
Epoch 500, training loss: 6.363622665405273 = 0.2707148492336273 + 1.0 * 6.092907905578613
Epoch 500, val loss: 0.766079306602478
Epoch 510, training loss: 6.3476762771606445 = 0.2518148124217987 + 1.0 * 6.095861434936523
Epoch 510, val loss: 0.7629730701446533
Epoch 520, training loss: 6.325857639312744 = 0.23422522842884064 + 1.0 * 6.09163236618042
Epoch 520, val loss: 0.7609352469444275
Epoch 530, training loss: 6.307122707366943 = 0.21799336373806 + 1.0 * 6.089129447937012
Epoch 530, val loss: 0.759825587272644
Epoch 540, training loss: 6.289566516876221 = 0.20304986834526062 + 1.0 * 6.086516857147217
Epoch 540, val loss: 0.7596111297607422
Epoch 550, training loss: 6.273481369018555 = 0.18935726583003998 + 1.0 * 6.0841240882873535
Epoch 550, val loss: 0.7603110074996948
Epoch 560, training loss: 6.265412330627441 = 0.17681774497032166 + 1.0 * 6.088594436645508
Epoch 560, val loss: 0.7616440653800964
Epoch 570, training loss: 6.249522686004639 = 0.1654018759727478 + 1.0 * 6.084120750427246
Epoch 570, val loss: 0.7636538147926331
Epoch 580, training loss: 6.23697566986084 = 0.15493999421596527 + 1.0 * 6.082035541534424
Epoch 580, val loss: 0.7663372755050659
Epoch 590, training loss: 6.223247528076172 = 0.14533571898937225 + 1.0 * 6.077911853790283
Epoch 590, val loss: 0.7695371508598328
Epoch 600, training loss: 6.212991237640381 = 0.13648447394371033 + 1.0 * 6.076506614685059
Epoch 600, val loss: 0.7732810378074646
Epoch 610, training loss: 6.2047882080078125 = 0.12833809852600098 + 1.0 * 6.076450347900391
Epoch 610, val loss: 0.7773573994636536
Epoch 620, training loss: 6.195692539215088 = 0.12085849791765213 + 1.0 * 6.074833869934082
Epoch 620, val loss: 0.7817874550819397
Epoch 630, training loss: 6.189553260803223 = 0.11398059874773026 + 1.0 * 6.075572490692139
Epoch 630, val loss: 0.7865211963653564
Epoch 640, training loss: 6.177509307861328 = 0.10760855674743652 + 1.0 * 6.0699005126953125
Epoch 640, val loss: 0.791459321975708
Epoch 650, training loss: 6.172977447509766 = 0.10170140117406845 + 1.0 * 6.0712761878967285
Epoch 650, val loss: 0.796690046787262
Epoch 660, training loss: 6.168972969055176 = 0.0962303876876831 + 1.0 * 6.072742462158203
Epoch 660, val loss: 0.8021253943443298
Epoch 670, training loss: 6.161075592041016 = 0.09114809334278107 + 1.0 * 6.06992769241333
Epoch 670, val loss: 0.8075847625732422
Epoch 680, training loss: 6.152390480041504 = 0.08644265681505203 + 1.0 * 6.065948009490967
Epoch 680, val loss: 0.813286542892456
Epoch 690, training loss: 6.146064281463623 = 0.08204399794340134 + 1.0 * 6.064020156860352
Epoch 690, val loss: 0.819137454032898
Epoch 700, training loss: 6.153456211090088 = 0.07793503999710083 + 1.0 * 6.075520992279053
Epoch 700, val loss: 0.8250881433486938
Epoch 710, training loss: 6.138315200805664 = 0.07413806021213531 + 1.0 * 6.0641770362854
Epoch 710, val loss: 0.8309639096260071
Epoch 720, training loss: 6.132126808166504 = 0.07058043032884598 + 1.0 * 6.061546325683594
Epoch 720, val loss: 0.8369866609573364
Epoch 730, training loss: 6.127751350402832 = 0.06725277006626129 + 1.0 * 6.0604987144470215
Epoch 730, val loss: 0.8431764841079712
Epoch 740, training loss: 6.126554489135742 = 0.06413505226373672 + 1.0 * 6.062419414520264
Epoch 740, val loss: 0.849297046661377
Epoch 750, training loss: 6.12110710144043 = 0.06122659146785736 + 1.0 * 6.05988073348999
Epoch 750, val loss: 0.8553369641304016
Epoch 760, training loss: 6.114638805389404 = 0.05849926546216011 + 1.0 * 6.0561394691467285
Epoch 760, val loss: 0.8614528179168701
Epoch 770, training loss: 6.112885475158691 = 0.05593319237232208 + 1.0 * 6.056952476501465
Epoch 770, val loss: 0.867698073387146
Epoch 780, training loss: 6.109180927276611 = 0.05352224409580231 + 1.0 * 6.05565881729126
Epoch 780, val loss: 0.8737897276878357
Epoch 790, training loss: 6.105652332305908 = 0.05126417800784111 + 1.0 * 6.054388046264648
Epoch 790, val loss: 0.879913330078125
Epoch 800, training loss: 6.107083797454834 = 0.04913533106446266 + 1.0 * 6.057948589324951
Epoch 800, val loss: 0.886030912399292
Epoch 810, training loss: 6.101250648498535 = 0.047127433121204376 + 1.0 * 6.054123401641846
Epoch 810, val loss: 0.8921008110046387
Epoch 820, training loss: 6.096485137939453 = 0.045239996165037155 + 1.0 * 6.051245212554932
Epoch 820, val loss: 0.8981817364692688
Epoch 830, training loss: 6.093770980834961 = 0.0434572696685791 + 1.0 * 6.050313472747803
Epoch 830, val loss: 0.9042969942092896
Epoch 840, training loss: 6.097663402557373 = 0.04176962375640869 + 1.0 * 6.055893898010254
Epoch 840, val loss: 0.9104070663452148
Epoch 850, training loss: 6.096411228179932 = 0.04017107933759689 + 1.0 * 6.056240081787109
Epoch 850, val loss: 0.9162268042564392
Epoch 860, training loss: 6.087371826171875 = 0.0386694110929966 + 1.0 * 6.048702239990234
Epoch 860, val loss: 0.9221305847167969
Epoch 870, training loss: 6.08433198928833 = 0.03724260255694389 + 1.0 * 6.047089576721191
Epoch 870, val loss: 0.9280694723129272
Epoch 880, training loss: 6.086930751800537 = 0.035886164754629135 + 1.0 * 6.051044464111328
Epoch 880, val loss: 0.9339948296546936
Epoch 890, training loss: 6.083625793457031 = 0.034605178982019424 + 1.0 * 6.049020767211914
Epoch 890, val loss: 0.9397703409194946
Epoch 900, training loss: 6.078843593597412 = 0.033391136676073074 + 1.0 * 6.04545259475708
Epoch 900, val loss: 0.9454677700996399
Epoch 910, training loss: 6.075973033905029 = 0.03223995119333267 + 1.0 * 6.0437331199646
Epoch 910, val loss: 0.9512190222740173
Epoch 920, training loss: 6.073828220367432 = 0.031140977516770363 + 1.0 * 6.04268741607666
Epoch 920, val loss: 0.9570095539093018
Epoch 930, training loss: 6.078439235687256 = 0.030094876885414124 + 1.0 * 6.048344135284424
Epoch 930, val loss: 0.9627640247344971
Epoch 940, training loss: 6.076315402984619 = 0.029098710045218468 + 1.0 * 6.047216892242432
Epoch 940, val loss: 0.9682601094245911
Epoch 950, training loss: 6.0716729164123535 = 0.02815517596900463 + 1.0 * 6.043517589569092
Epoch 950, val loss: 0.9736959934234619
Epoch 960, training loss: 6.06766939163208 = 0.027254631742835045 + 1.0 * 6.040414810180664
Epoch 960, val loss: 0.9792255759239197
Epoch 970, training loss: 6.069342613220215 = 0.02639211155474186 + 1.0 * 6.042950630187988
Epoch 970, val loss: 0.9848136901855469
Epoch 980, training loss: 6.066085338592529 = 0.02557171694934368 + 1.0 * 6.040513515472412
Epoch 980, val loss: 0.9902358651161194
Epoch 990, training loss: 6.067439556121826 = 0.024790894240140915 + 1.0 * 6.042648792266846
Epoch 990, val loss: 0.995543897151947
Epoch 1000, training loss: 6.0644941329956055 = 0.024045024067163467 + 1.0 * 6.040449142456055
Epoch 1000, val loss: 1.000754475593567
Epoch 1010, training loss: 6.060795307159424 = 0.02333008497953415 + 1.0 * 6.0374650955200195
Epoch 1010, val loss: 1.0060697793960571
Epoch 1020, training loss: 6.060159683227539 = 0.02264651469886303 + 1.0 * 6.037513256072998
Epoch 1020, val loss: 1.011384129524231
Epoch 1030, training loss: 6.065186023712158 = 0.021989231929183006 + 1.0 * 6.043196678161621
Epoch 1030, val loss: 1.0165612697601318
Epoch 1040, training loss: 6.059528827667236 = 0.021366097033023834 + 1.0 * 6.038162708282471
Epoch 1040, val loss: 1.021590232849121
Epoch 1050, training loss: 6.055281639099121 = 0.020769262686371803 + 1.0 * 6.034512519836426
Epoch 1050, val loss: 1.0266497135162354
Epoch 1060, training loss: 6.05631685256958 = 0.020194036886096 + 1.0 * 6.036122798919678
Epoch 1060, val loss: 1.0317347049713135
Epoch 1070, training loss: 6.059665679931641 = 0.019641906023025513 + 1.0 * 6.0400238037109375
Epoch 1070, val loss: 1.0366861820220947
Epoch 1080, training loss: 6.053752899169922 = 0.019115887582302094 + 1.0 * 6.034636974334717
Epoch 1080, val loss: 1.0415308475494385
Epoch 1090, training loss: 6.050959587097168 = 0.01861034706234932 + 1.0 * 6.032349109649658
Epoch 1090, val loss: 1.0464102029800415
Epoch 1100, training loss: 6.049673080444336 = 0.018122294917702675 + 1.0 * 6.031550884246826
Epoch 1100, val loss: 1.0513278245925903
Epoch 1110, training loss: 6.049485206604004 = 0.01765115186572075 + 1.0 * 6.031834125518799
Epoch 1110, val loss: 1.0561951398849487
Epoch 1120, training loss: 6.0663909912109375 = 0.01719711720943451 + 1.0 * 6.049193859100342
Epoch 1120, val loss: 1.060886025428772
Epoch 1130, training loss: 6.052388668060303 = 0.01677108369767666 + 1.0 * 6.035617351531982
Epoch 1130, val loss: 1.0652244091033936
Epoch 1140, training loss: 6.0465521812438965 = 0.016358325257897377 + 1.0 * 6.03019380569458
Epoch 1140, val loss: 1.069762110710144
Epoch 1150, training loss: 6.045267105102539 = 0.015960726886987686 + 1.0 * 6.029306411743164
Epoch 1150, val loss: 1.0744818449020386
Epoch 1160, training loss: 6.045292377471924 = 0.015573966316878796 + 1.0 * 6.029718399047852
Epoch 1160, val loss: 1.0791642665863037
Epoch 1170, training loss: 6.046540260314941 = 0.015202466398477554 + 1.0 * 6.031337738037109
Epoch 1170, val loss: 1.083651065826416
Epoch 1180, training loss: 6.042134761810303 = 0.01484448928385973 + 1.0 * 6.027290344238281
Epoch 1180, val loss: 1.0880813598632812
Epoch 1190, training loss: 6.041184902191162 = 0.01449865847826004 + 1.0 * 6.026686191558838
Epoch 1190, val loss: 1.0925867557525635
Epoch 1200, training loss: 6.043317794799805 = 0.01416393555700779 + 1.0 * 6.029153823852539
Epoch 1200, val loss: 1.097118616104126
Epoch 1210, training loss: 6.041776657104492 = 0.013840647414326668 + 1.0 * 6.027935981750488
Epoch 1210, val loss: 1.1014533042907715
Epoch 1220, training loss: 6.0408101081848145 = 0.013531181961297989 + 1.0 * 6.027278900146484
Epoch 1220, val loss: 1.1057690382003784
Epoch 1230, training loss: 6.039079666137695 = 0.013229027390480042 + 1.0 * 6.025850772857666
Epoch 1230, val loss: 1.1101168394088745
Epoch 1240, training loss: 6.046425819396973 = 0.01293749175965786 + 1.0 * 6.0334882736206055
Epoch 1240, val loss: 1.1144129037857056
Epoch 1250, training loss: 6.0443854331970215 = 0.01265646331012249 + 1.0 * 6.031728744506836
Epoch 1250, val loss: 1.118454933166504
Epoch 1260, training loss: 6.038447380065918 = 0.012389374896883965 + 1.0 * 6.026058197021484
Epoch 1260, val loss: 1.1225165128707886
Epoch 1270, training loss: 6.035369873046875 = 0.012127554975450039 + 1.0 * 6.023242473602295
Epoch 1270, val loss: 1.1267098188400269
Epoch 1280, training loss: 6.034543037414551 = 0.011872967705130577 + 1.0 * 6.022670269012451
Epoch 1280, val loss: 1.1309088468551636
Epoch 1290, training loss: 6.038725852966309 = 0.011625880375504494 + 1.0 * 6.027100086212158
Epoch 1290, val loss: 1.1350525617599487
Epoch 1300, training loss: 6.03825569152832 = 0.011388626880943775 + 1.0 * 6.026866912841797
Epoch 1300, val loss: 1.1390196084976196
Epoch 1310, training loss: 6.033900260925293 = 0.011159014888107777 + 1.0 * 6.022741317749023
Epoch 1310, val loss: 1.1428903341293335
Epoch 1320, training loss: 6.032619953155518 = 0.010936613194644451 + 1.0 * 6.021683216094971
Epoch 1320, val loss: 1.1469078063964844
Epoch 1330, training loss: 6.034791946411133 = 0.010720333084464073 + 1.0 * 6.02407169342041
Epoch 1330, val loss: 1.1509031057357788
Epoch 1340, training loss: 6.030989646911621 = 0.010510889813303947 + 1.0 * 6.02047872543335
Epoch 1340, val loss: 1.154792308807373
Epoch 1350, training loss: 6.033446311950684 = 0.01030781865119934 + 1.0 * 6.023138523101807
Epoch 1350, val loss: 1.1586748361587524
Epoch 1360, training loss: 6.037418365478516 = 0.010110927745699883 + 1.0 * 6.027307510375977
Epoch 1360, val loss: 1.1625287532806396
Epoch 1370, training loss: 6.031375885009766 = 0.00991878192871809 + 1.0 * 6.021457195281982
Epoch 1370, val loss: 1.166199803352356
Epoch 1380, training loss: 6.028405666351318 = 0.009734396822750568 + 1.0 * 6.018671035766602
Epoch 1380, val loss: 1.1699832677841187
Epoch 1390, training loss: 6.028171062469482 = 0.00955385621637106 + 1.0 * 6.018617153167725
Epoch 1390, val loss: 1.1738312244415283
Epoch 1400, training loss: 6.0402703285217285 = 0.009379163384437561 + 1.0 * 6.030890941619873
Epoch 1400, val loss: 1.1775741577148438
Epoch 1410, training loss: 6.033207893371582 = 0.009207982569932938 + 1.0 * 6.0239996910095215
Epoch 1410, val loss: 1.1810702085494995
Epoch 1420, training loss: 6.028561592102051 = 0.009045297279953957 + 1.0 * 6.019516468048096
Epoch 1420, val loss: 1.1846115589141846
Epoch 1430, training loss: 6.026012897491455 = 0.008884899318218231 + 1.0 * 6.017127990722656
Epoch 1430, val loss: 1.1883249282836914
Epoch 1440, training loss: 6.027034282684326 = 0.008728929795324802 + 1.0 * 6.01830530166626
Epoch 1440, val loss: 1.1920617818832397
Epoch 1450, training loss: 6.029937267303467 = 0.008577166125178337 + 1.0 * 6.021359920501709
Epoch 1450, val loss: 1.1955480575561523
Epoch 1460, training loss: 6.028611660003662 = 0.008428946137428284 + 1.0 * 6.0201826095581055
Epoch 1460, val loss: 1.198954463005066
Epoch 1470, training loss: 6.023735046386719 = 0.008286581374704838 + 1.0 * 6.015448570251465
Epoch 1470, val loss: 1.2024245262145996
Epoch 1480, training loss: 6.023630619049072 = 0.008146612904965878 + 1.0 * 6.015483856201172
Epoch 1480, val loss: 1.2060246467590332
Epoch 1490, training loss: 6.027617931365967 = 0.008010451681911945 + 1.0 * 6.0196075439453125
Epoch 1490, val loss: 1.2095608711242676
Epoch 1500, training loss: 6.023231029510498 = 0.007877626456320286 + 1.0 * 6.015353202819824
Epoch 1500, val loss: 1.2129355669021606
Epoch 1510, training loss: 6.027587413787842 = 0.007749015931040049 + 1.0 * 6.019838333129883
Epoch 1510, val loss: 1.2162939310073853
Epoch 1520, training loss: 6.023397445678711 = 0.007623673416674137 + 1.0 * 6.015773773193359
Epoch 1520, val loss: 1.2195947170257568
Epoch 1530, training loss: 6.021101474761963 = 0.007502017077058554 + 1.0 * 6.013599395751953
Epoch 1530, val loss: 1.2229043245315552
Epoch 1540, training loss: 6.021147727966309 = 0.007382669020444155 + 1.0 * 6.01376485824585
Epoch 1540, val loss: 1.2263295650482178
Epoch 1550, training loss: 6.026219844818115 = 0.007265736348927021 + 1.0 * 6.018954277038574
Epoch 1550, val loss: 1.2296785116195679
Epoch 1560, training loss: 6.02084493637085 = 0.007152687292546034 + 1.0 * 6.013692378997803
Epoch 1560, val loss: 1.232832908630371
Epoch 1570, training loss: 6.019828796386719 = 0.007043043617159128 + 1.0 * 6.012785911560059
Epoch 1570, val loss: 1.2359843254089355
Epoch 1580, training loss: 6.02111291885376 = 0.00693474430590868 + 1.0 * 6.014178276062012
Epoch 1580, val loss: 1.2392632961273193
Epoch 1590, training loss: 6.021975040435791 = 0.006829557940363884 + 1.0 * 6.015145301818848
Epoch 1590, val loss: 1.2424497604370117
Epoch 1600, training loss: 6.018902778625488 = 0.006726923398673534 + 1.0 * 6.012176036834717
Epoch 1600, val loss: 1.245548963546753
Epoch 1610, training loss: 6.020809173583984 = 0.006627153605222702 + 1.0 * 6.014182090759277
Epoch 1610, val loss: 1.248740792274475
Epoch 1620, training loss: 6.02188777923584 = 0.006529666017740965 + 1.0 * 6.015357971191406
Epoch 1620, val loss: 1.2517861127853394
Epoch 1630, training loss: 6.017009258270264 = 0.00643448019400239 + 1.0 * 6.010574817657471
Epoch 1630, val loss: 1.2546547651290894
Epoch 1640, training loss: 6.016268730163574 = 0.006341947242617607 + 1.0 * 6.009926795959473
Epoch 1640, val loss: 1.2577285766601562
Epoch 1650, training loss: 6.016261100769043 = 0.006250787992030382 + 1.0 * 6.010010242462158
Epoch 1650, val loss: 1.2608835697174072
Epoch 1660, training loss: 6.0283427238464355 = 0.006161857862025499 + 1.0 * 6.022181034088135
Epoch 1660, val loss: 1.2639601230621338
Epoch 1670, training loss: 6.0186076164245605 = 0.006074392702430487 + 1.0 * 6.012533187866211
Epoch 1670, val loss: 1.266814112663269
Epoch 1680, training loss: 6.016639709472656 = 0.005990150850266218 + 1.0 * 6.010649681091309
Epoch 1680, val loss: 1.2697597742080688
Epoch 1690, training loss: 6.023316383361816 = 0.005907556973397732 + 1.0 * 6.017408847808838
Epoch 1690, val loss: 1.2727792263031006
Epoch 1700, training loss: 6.018396854400635 = 0.005825710482895374 + 1.0 * 6.012571334838867
Epoch 1700, val loss: 1.275564432144165
Epoch 1710, training loss: 6.014455318450928 = 0.005747288465499878 + 1.0 * 6.0087080001831055
Epoch 1710, val loss: 1.2783641815185547
Epoch 1720, training loss: 6.014103889465332 = 0.005670195911079645 + 1.0 * 6.008433818817139
Epoch 1720, val loss: 1.2813336849212646
Epoch 1730, training loss: 6.016602039337158 = 0.005593763664364815 + 1.0 * 6.011008262634277
Epoch 1730, val loss: 1.2842777967453003
Epoch 1740, training loss: 6.016810894012451 = 0.005519420839846134 + 1.0 * 6.01129150390625
Epoch 1740, val loss: 1.287125825881958
Epoch 1750, training loss: 6.016809940338135 = 0.005446592345833778 + 1.0 * 6.011363506317139
Epoch 1750, val loss: 1.2898614406585693
Epoch 1760, training loss: 6.014218330383301 = 0.005376025568693876 + 1.0 * 6.008842468261719
Epoch 1760, val loss: 1.2926651239395142
Epoch 1770, training loss: 6.0128631591796875 = 0.00530645065009594 + 1.0 * 6.007556915283203
Epoch 1770, val loss: 1.2955543994903564
Epoch 1780, training loss: 6.019547462463379 = 0.005238357000052929 + 1.0 * 6.014308929443359
Epoch 1780, val loss: 1.2983790636062622
Epoch 1790, training loss: 6.013092041015625 = 0.005170947872102261 + 1.0 * 6.00792121887207
Epoch 1790, val loss: 1.3010143041610718
Epoch 1800, training loss: 6.014732837677002 = 0.005106041673570871 + 1.0 * 6.009626865386963
Epoch 1800, val loss: 1.3036830425262451
Epoch 1810, training loss: 6.010744571685791 = 0.005042296834290028 + 1.0 * 6.005702495574951
Epoch 1810, val loss: 1.3064241409301758
Epoch 1820, training loss: 6.011073112487793 = 0.0049798451364040375 + 1.0 * 6.006093502044678
Epoch 1820, val loss: 1.3092372417449951
Epoch 1830, training loss: 6.0224175453186035 = 0.0049181836657226086 + 1.0 * 6.0174994468688965
Epoch 1830, val loss: 1.3119641542434692
Epoch 1840, training loss: 6.015818119049072 = 0.00485795084387064 + 1.0 * 6.010960102081299
Epoch 1840, val loss: 1.314352035522461
Epoch 1850, training loss: 6.0112199783325195 = 0.004800011403858662 + 1.0 * 6.006420135498047
Epoch 1850, val loss: 1.3169825077056885
Epoch 1860, training loss: 6.010300636291504 = 0.004742404446005821 + 1.0 * 6.005558013916016
Epoch 1860, val loss: 1.3197014331817627
Epoch 1870, training loss: 6.016539096832275 = 0.004686280153691769 + 1.0 * 6.011852741241455
Epoch 1870, val loss: 1.3224127292633057
Epoch 1880, training loss: 6.0106306076049805 = 0.004629983566701412 + 1.0 * 6.006000518798828
Epoch 1880, val loss: 1.324901819229126
Epoch 1890, training loss: 6.010075569152832 = 0.004576006438583136 + 1.0 * 6.005499362945557
Epoch 1890, val loss: 1.3274390697479248
Epoch 1900, training loss: 6.009185314178467 = 0.0045225233770906925 + 1.0 * 6.004662990570068
Epoch 1900, val loss: 1.3300753831863403
Epoch 1910, training loss: 6.020869731903076 = 0.0044698165729641914 + 1.0 * 6.01639986038208
Epoch 1910, val loss: 1.3325989246368408
Epoch 1920, training loss: 6.011223316192627 = 0.00441925460472703 + 1.0 * 6.0068039894104
Epoch 1920, val loss: 1.3349863290786743
Epoch 1930, training loss: 6.007231712341309 = 0.004369436297565699 + 1.0 * 6.002862453460693
Epoch 1930, val loss: 1.337448000907898
Epoch 1940, training loss: 6.0074286460876465 = 0.004320234060287476 + 1.0 * 6.003108501434326
Epoch 1940, val loss: 1.3400708436965942
Epoch 1950, training loss: 6.013324737548828 = 0.0042717657051980495 + 1.0 * 6.009052753448486
Epoch 1950, val loss: 1.3426276445388794
Epoch 1960, training loss: 6.007688999176025 = 0.004224136937409639 + 1.0 * 6.003464698791504
Epoch 1960, val loss: 1.3450342416763306
Epoch 1970, training loss: 6.0074687004089355 = 0.004177469294518232 + 1.0 * 6.003291130065918
Epoch 1970, val loss: 1.3474676609039307
Epoch 1980, training loss: 6.007023811340332 = 0.004131631460040808 + 1.0 * 6.002892017364502
Epoch 1980, val loss: 1.3499876260757446
Epoch 1990, training loss: 6.015550136566162 = 0.004086351953446865 + 1.0 * 6.011463642120361
Epoch 1990, val loss: 1.3524290323257446
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.6236
Flip ASR: 0.5600/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.306702613830566 = 1.9328423738479614 + 1.0 * 8.373860359191895
Epoch 0, val loss: 1.9370594024658203
Epoch 10, training loss: 10.296618461608887 = 1.9232292175292969 + 1.0 * 8.37338924407959
Epoch 10, val loss: 1.9269413948059082
Epoch 20, training loss: 10.281145095825195 = 1.911405324935913 + 1.0 * 8.369739532470703
Epoch 20, val loss: 1.9141759872436523
Epoch 30, training loss: 10.23685359954834 = 1.8947391510009766 + 1.0 * 8.342114448547363
Epoch 30, val loss: 1.8962006568908691
Epoch 40, training loss: 10.03439998626709 = 1.8735568523406982 + 1.0 * 8.160842895507812
Epoch 40, val loss: 1.8744930028915405
Epoch 50, training loss: 9.490981101989746 = 1.8502331972122192 + 1.0 * 7.640747547149658
Epoch 50, val loss: 1.8514792919158936
Epoch 60, training loss: 9.21986198425293 = 1.829160451889038 + 1.0 * 7.3907012939453125
Epoch 60, val loss: 1.8316019773483276
Epoch 70, training loss: 8.863171577453613 = 1.8130817413330078 + 1.0 * 7.0500898361206055
Epoch 70, val loss: 1.816294550895691
Epoch 80, training loss: 8.58296012878418 = 1.8011078834533691 + 1.0 * 6.781852722167969
Epoch 80, val loss: 1.8041679859161377
Epoch 90, training loss: 8.417080879211426 = 1.7882570028305054 + 1.0 * 6.628824234008789
Epoch 90, val loss: 1.7908893823623657
Epoch 100, training loss: 8.315790176391602 = 1.7723685503005981 + 1.0 * 6.543421745300293
Epoch 100, val loss: 1.7755744457244873
Epoch 110, training loss: 8.213434219360352 = 1.7554433345794678 + 1.0 * 6.457991123199463
Epoch 110, val loss: 1.7600195407867432
Epoch 120, training loss: 8.133310317993164 = 1.7383933067321777 + 1.0 * 6.394917011260986
Epoch 120, val loss: 1.7450613975524902
Epoch 130, training loss: 8.066308975219727 = 1.7189480066299438 + 1.0 * 6.347360610961914
Epoch 130, val loss: 1.7285276651382446
Epoch 140, training loss: 8.006233215332031 = 1.6954200267791748 + 1.0 * 6.310812950134277
Epoch 140, val loss: 1.7091162204742432
Epoch 150, training loss: 7.947690963745117 = 1.6675219535827637 + 1.0 * 6.2801690101623535
Epoch 150, val loss: 1.6862354278564453
Epoch 160, training loss: 7.888912677764893 = 1.6345306634902954 + 1.0 * 6.254382133483887
Epoch 160, val loss: 1.6594229936599731
Epoch 170, training loss: 7.831248760223389 = 1.5957374572753906 + 1.0 * 6.235511302947998
Epoch 170, val loss: 1.6282000541687012
Epoch 180, training loss: 7.769530296325684 = 1.5509498119354248 + 1.0 * 6.21858024597168
Epoch 180, val loss: 1.5923501253128052
Epoch 190, training loss: 7.704185962677002 = 1.4998115301132202 + 1.0 * 6.204374313354492
Epoch 190, val loss: 1.5514929294586182
Epoch 200, training loss: 7.638821125030518 = 1.442639708518982 + 1.0 * 6.196181297302246
Epoch 200, val loss: 1.5061548948287964
Epoch 210, training loss: 7.565583229064941 = 1.3818395137786865 + 1.0 * 6.183743953704834
Epoch 210, val loss: 1.4586129188537598
Epoch 220, training loss: 7.493494510650635 = 1.3186777830123901 + 1.0 * 6.174816608428955
Epoch 220, val loss: 1.4099347591400146
Epoch 230, training loss: 7.423947334289551 = 1.2552534341812134 + 1.0 * 6.168694019317627
Epoch 230, val loss: 1.361924648284912
Epoch 240, training loss: 7.355469226837158 = 1.1943954229354858 + 1.0 * 6.161073684692383
Epoch 240, val loss: 1.3163255453109741
Epoch 250, training loss: 7.292078971862793 = 1.1367883682250977 + 1.0 * 6.155290603637695
Epoch 250, val loss: 1.2735587358474731
Epoch 260, training loss: 7.232896327972412 = 1.08318030834198 + 1.0 * 6.149715900421143
Epoch 260, val loss: 1.234002947807312
Epoch 270, training loss: 7.179542541503906 = 1.0342103242874146 + 1.0 * 6.145332336425781
Epoch 270, val loss: 1.1978956460952759
Epoch 280, training loss: 7.127712726593018 = 0.989135205745697 + 1.0 * 6.138577461242676
Epoch 280, val loss: 1.164718747138977
Epoch 290, training loss: 7.081084251403809 = 0.9469156265258789 + 1.0 * 6.13416862487793
Epoch 290, val loss: 1.133641242980957
Epoch 300, training loss: 7.0401787757873535 = 0.9070726633071899 + 1.0 * 6.133106231689453
Epoch 300, val loss: 1.104189395904541
Epoch 310, training loss: 6.993838310241699 = 0.8688763976097107 + 1.0 * 6.124961853027344
Epoch 310, val loss: 1.0759310722351074
Epoch 320, training loss: 6.95257568359375 = 0.831243097782135 + 1.0 * 6.12133264541626
Epoch 320, val loss: 1.0480562448501587
Epoch 330, training loss: 6.914305686950684 = 0.7937995195388794 + 1.0 * 6.120506286621094
Epoch 330, val loss: 1.020330786705017
Epoch 340, training loss: 6.872741222381592 = 0.7568255662918091 + 1.0 * 6.115915775299072
Epoch 340, val loss: 0.9928833842277527
Epoch 350, training loss: 6.832076072692871 = 0.720129668712616 + 1.0 * 6.1119465827941895
Epoch 350, val loss: 0.9658464193344116
Epoch 360, training loss: 6.792593479156494 = 0.6837143301963806 + 1.0 * 6.108879089355469
Epoch 360, val loss: 0.9392409324645996
Epoch 370, training loss: 6.759527206420898 = 0.6481459140777588 + 1.0 * 6.1113810539245605
Epoch 370, val loss: 0.9136554002761841
Epoch 380, training loss: 6.718955039978027 = 0.6138484477996826 + 1.0 * 6.105106830596924
Epoch 380, val loss: 0.8896433115005493
Epoch 390, training loss: 6.682260513305664 = 0.5804702639579773 + 1.0 * 6.101790428161621
Epoch 390, val loss: 0.8670196533203125
Epoch 400, training loss: 6.646518230438232 = 0.5480276346206665 + 1.0 * 6.0984907150268555
Epoch 400, val loss: 0.8458501696586609
Epoch 410, training loss: 6.61422872543335 = 0.5164768099784851 + 1.0 * 6.097752094268799
Epoch 410, val loss: 0.8261913657188416
Epoch 420, training loss: 6.582714557647705 = 0.48565253615379333 + 1.0 * 6.097062110900879
Epoch 420, val loss: 0.8078634738922119
Epoch 430, training loss: 6.548263072967529 = 0.4555693566799164 + 1.0 * 6.09269380569458
Epoch 430, val loss: 0.7908487319946289
Epoch 440, training loss: 6.518880844116211 = 0.42617109417915344 + 1.0 * 6.092709541320801
Epoch 440, val loss: 0.7751191854476929
Epoch 450, training loss: 6.487885475158691 = 0.39763322472572327 + 1.0 * 6.09025239944458
Epoch 450, val loss: 0.7607877254486084
Epoch 460, training loss: 6.471526145935059 = 0.37014684081077576 + 1.0 * 6.10137939453125
Epoch 460, val loss: 0.7480554580688477
Epoch 470, training loss: 6.430655002593994 = 0.34420302510261536 + 1.0 * 6.086452007293701
Epoch 470, val loss: 0.7370632886886597
Epoch 480, training loss: 6.402941703796387 = 0.3196945786476135 + 1.0 * 6.083247184753418
Epoch 480, val loss: 0.7278246879577637
Epoch 490, training loss: 6.378332614898682 = 0.2966415584087372 + 1.0 * 6.081691265106201
Epoch 490, val loss: 0.7201367020606995
Epoch 500, training loss: 6.3759541511535645 = 0.2751588225364685 + 1.0 * 6.100795269012451
Epoch 500, val loss: 0.7139323949813843
Epoch 510, training loss: 6.34069299697876 = 0.25557419657707214 + 1.0 * 6.085118770599365
Epoch 510, val loss: 0.7092969417572021
Epoch 520, training loss: 6.315176010131836 = 0.23760583996772766 + 1.0 * 6.077569961547852
Epoch 520, val loss: 0.7063509821891785
Epoch 530, training loss: 6.296341896057129 = 0.22104895114898682 + 1.0 * 6.075293064117432
Epoch 530, val loss: 0.7046495676040649
Epoch 540, training loss: 6.282976150512695 = 0.2057708203792572 + 1.0 * 6.077205181121826
Epoch 540, val loss: 0.7041165828704834
Epoch 550, training loss: 6.27439546585083 = 0.19183948636054993 + 1.0 * 6.082555770874023
Epoch 550, val loss: 0.7046404480934143
Epoch 560, training loss: 6.252291679382324 = 0.17915493249893188 + 1.0 * 6.073136806488037
Epoch 560, val loss: 0.7063507437705994
Epoch 570, training loss: 6.23761510848999 = 0.1675092726945877 + 1.0 * 6.070106029510498
Epoch 570, val loss: 0.708953857421875
Epoch 580, training loss: 6.229035377502441 = 0.15682485699653625 + 1.0 * 6.072210311889648
Epoch 580, val loss: 0.7123420238494873
Epoch 590, training loss: 6.217803478240967 = 0.14708182215690613 + 1.0 * 6.070721626281738
Epoch 590, val loss: 0.7164052724838257
Epoch 600, training loss: 6.205179214477539 = 0.13818824291229248 + 1.0 * 6.066990852355957
Epoch 600, val loss: 0.7211112380027771
Epoch 610, training loss: 6.196080684661865 = 0.13006651401519775 + 1.0 * 6.066014289855957
Epoch 610, val loss: 0.7264329195022583
Epoch 620, training loss: 6.187409400939941 = 0.12260632961988449 + 1.0 * 6.064803123474121
Epoch 620, val loss: 0.7322559952735901
Epoch 630, training loss: 6.178674697875977 = 0.11575666069984436 + 1.0 * 6.062918186187744
Epoch 630, val loss: 0.7384018301963806
Epoch 640, training loss: 6.170460224151611 = 0.10944340378046036 + 1.0 * 6.061017036437988
Epoch 640, val loss: 0.744991660118103
Epoch 650, training loss: 6.16935396194458 = 0.10362637788057327 + 1.0 * 6.065727710723877
Epoch 650, val loss: 0.751794695854187
Epoch 660, training loss: 6.163856506347656 = 0.0983109101653099 + 1.0 * 6.065545558929443
Epoch 660, val loss: 0.7588637471199036
Epoch 670, training loss: 6.151885032653809 = 0.09339617937803268 + 1.0 * 6.058488845825195
Epoch 670, val loss: 0.765950620174408
Epoch 680, training loss: 6.146413803100586 = 0.08885692059993744 + 1.0 * 6.057557106018066
Epoch 680, val loss: 0.7733120918273926
Epoch 690, training loss: 6.14436674118042 = 0.08463913947343826 + 1.0 * 6.059727668762207
Epoch 690, val loss: 0.7806499004364014
Epoch 700, training loss: 6.136654376983643 = 0.08073524385690689 + 1.0 * 6.055919170379639
Epoch 700, val loss: 0.7880932092666626
Epoch 710, training loss: 6.1382646560668945 = 0.07709751278162003 + 1.0 * 6.061167240142822
Epoch 710, val loss: 0.7955723404884338
Epoch 720, training loss: 6.127877235412598 = 0.0737127959728241 + 1.0 * 6.054164409637451
Epoch 720, val loss: 0.8029441237449646
Epoch 730, training loss: 6.123845100402832 = 0.07055483013391495 + 1.0 * 6.053290367126465
Epoch 730, val loss: 0.8104604482650757
Epoch 740, training loss: 6.119867324829102 = 0.06759020686149597 + 1.0 * 6.052277088165283
Epoch 740, val loss: 0.8177714943885803
Epoch 750, training loss: 6.114585876464844 = 0.06481431424617767 + 1.0 * 6.049771785736084
Epoch 750, val loss: 0.8252118229866028
Epoch 760, training loss: 6.1199259757995605 = 0.062201134860515594 + 1.0 * 6.057724952697754
Epoch 760, val loss: 0.8324974179267883
Epoch 770, training loss: 6.111541748046875 = 0.059753671288490295 + 1.0 * 6.051787853240967
Epoch 770, val loss: 0.8396977782249451
Epoch 780, training loss: 6.105376243591309 = 0.05744702368974686 + 1.0 * 6.047929286956787
Epoch 780, val loss: 0.8470038771629333
Epoch 790, training loss: 6.102880477905273 = 0.05525842681527138 + 1.0 * 6.047622203826904
Epoch 790, val loss: 0.8541741371154785
Epoch 800, training loss: 6.106327056884766 = 0.05318509787321091 + 1.0 * 6.053142070770264
Epoch 800, val loss: 0.8611350655555725
Epoch 810, training loss: 6.096257209777832 = 0.05123981460928917 + 1.0 * 6.045017242431641
Epoch 810, val loss: 0.8680770397186279
Epoch 820, training loss: 6.095890045166016 = 0.049395643174648285 + 1.0 * 6.046494483947754
Epoch 820, val loss: 0.8750854134559631
Epoch 830, training loss: 6.090928077697754 = 0.04764141887426376 + 1.0 * 6.0432868003845215
Epoch 830, val loss: 0.8819707036018372
Epoch 840, training loss: 6.091513156890869 = 0.045970674604177475 + 1.0 * 6.0455427169799805
Epoch 840, val loss: 0.888738214969635
Epoch 850, training loss: 6.086360454559326 = 0.04438413307070732 + 1.0 * 6.041976451873779
Epoch 850, val loss: 0.8954930305480957
Epoch 860, training loss: 6.087149620056152 = 0.042871080338954926 + 1.0 * 6.044278621673584
Epoch 860, val loss: 0.9022380113601685
Epoch 870, training loss: 6.082834720611572 = 0.04143308103084564 + 1.0 * 6.0414018630981445
Epoch 870, val loss: 0.9087030291557312
Epoch 880, training loss: 6.079484939575195 = 0.040059491991996765 + 1.0 * 6.039425373077393
Epoch 880, val loss: 0.9152777791023254
Epoch 890, training loss: 6.077294826507568 = 0.038745779544115067 + 1.0 * 6.038548946380615
Epoch 890, val loss: 0.9218087196350098
Epoch 900, training loss: 6.090949058532715 = 0.03748863935470581 + 1.0 * 6.053460597991943
Epoch 900, val loss: 0.9282001852989197
Epoch 910, training loss: 6.077345371246338 = 0.036288708448410034 + 1.0 * 6.0410566329956055
Epoch 910, val loss: 0.9341180324554443
Epoch 920, training loss: 6.072998523712158 = 0.03514853119850159 + 1.0 * 6.0378499031066895
Epoch 920, val loss: 0.9404556751251221
Epoch 930, training loss: 6.06935453414917 = 0.03405168280005455 + 1.0 * 6.035302639007568
Epoch 930, val loss: 0.9466474652290344
Epoch 940, training loss: 6.070249080657959 = 0.032995693385601044 + 1.0 * 6.037253379821777
Epoch 940, val loss: 0.9527379870414734
Epoch 950, training loss: 6.068131446838379 = 0.03197979927062988 + 1.0 * 6.036151885986328
Epoch 950, val loss: 0.958439826965332
Epoch 960, training loss: 6.067583084106445 = 0.03101292997598648 + 1.0 * 6.036570072174072
Epoch 960, val loss: 0.9645008444786072
Epoch 970, training loss: 6.063009262084961 = 0.030075620859861374 + 1.0 * 6.032933712005615
Epoch 970, val loss: 0.9704882502555847
Epoch 980, training loss: 6.062351226806641 = 0.029174210503697395 + 1.0 * 6.033176898956299
Epoch 980, val loss: 0.9763048887252808
Epoch 990, training loss: 6.066096782684326 = 0.028308093547821045 + 1.0 * 6.0377888679504395
Epoch 990, val loss: 0.9818177223205566
Epoch 1000, training loss: 6.060193061828613 = 0.027480876073241234 + 1.0 * 6.032711982727051
Epoch 1000, val loss: 0.987467885017395
Epoch 1010, training loss: 6.058163166046143 = 0.026682494208216667 + 1.0 * 6.03148078918457
Epoch 1010, val loss: 0.9932798147201538
Epoch 1020, training loss: 6.062647342681885 = 0.02590790018439293 + 1.0 * 6.036739349365234
Epoch 1020, val loss: 0.9987772703170776
Epoch 1030, training loss: 6.056346416473389 = 0.02516677975654602 + 1.0 * 6.031179428100586
Epoch 1030, val loss: 1.0041797161102295
Epoch 1040, training loss: 6.054110527038574 = 0.02444596216082573 + 1.0 * 6.029664516448975
Epoch 1040, val loss: 1.0097371339797974
Epoch 1050, training loss: 6.053555965423584 = 0.02375159226357937 + 1.0 * 6.029804229736328
Epoch 1050, val loss: 1.0152158737182617
Epoch 1060, training loss: 6.055452823638916 = 0.02307916432619095 + 1.0 * 6.032373428344727
Epoch 1060, val loss: 1.020260214805603
Epoch 1070, training loss: 6.051766395568848 = 0.022436948493123055 + 1.0 * 6.029329299926758
Epoch 1070, val loss: 1.0255731344223022
Epoch 1080, training loss: 6.050716876983643 = 0.021816560998558998 + 1.0 * 6.028900146484375
Epoch 1080, val loss: 1.030956745147705
Epoch 1090, training loss: 6.048477649688721 = 0.02121700532734394 + 1.0 * 6.027260780334473
Epoch 1090, val loss: 1.0361456871032715
Epoch 1100, training loss: 6.053401947021484 = 0.020634979009628296 + 1.0 * 6.032766819000244
Epoch 1100, val loss: 1.0411335229873657
Epoch 1110, training loss: 6.04832124710083 = 0.020080219954252243 + 1.0 * 6.028241157531738
Epoch 1110, val loss: 1.0461894273757935
Epoch 1120, training loss: 6.046118259429932 = 0.01954161375761032 + 1.0 * 6.026576519012451
Epoch 1120, val loss: 1.0514084100723267
Epoch 1130, training loss: 6.045456409454346 = 0.01902039162814617 + 1.0 * 6.026435852050781
Epoch 1130, val loss: 1.0564262866973877
Epoch 1140, training loss: 6.0469651222229 = 0.01851784624159336 + 1.0 * 6.028447151184082
Epoch 1140, val loss: 1.0611072778701782
Epoch 1150, training loss: 6.041873931884766 = 0.018035221844911575 + 1.0 * 6.023838520050049
Epoch 1150, val loss: 1.0659995079040527
Epoch 1160, training loss: 6.040661811828613 = 0.01756943017244339 + 1.0 * 6.023092269897461
Epoch 1160, val loss: 1.070949673652649
Epoch 1170, training loss: 6.039926052093506 = 0.017117256298661232 + 1.0 * 6.022809028625488
Epoch 1170, val loss: 1.0757564306259155
Epoch 1180, training loss: 6.049704074859619 = 0.016681866720318794 + 1.0 * 6.033022403717041
Epoch 1180, val loss: 1.080588459968567
Epoch 1190, training loss: 6.047341823577881 = 0.016262922435998917 + 1.0 * 6.031078815460205
Epoch 1190, val loss: 1.0848181247711182
Epoch 1200, training loss: 6.037691593170166 = 0.015863513574004173 + 1.0 * 6.0218281745910645
Epoch 1200, val loss: 1.089663028717041
Epoch 1210, training loss: 6.037707805633545 = 0.01547717209905386 + 1.0 * 6.022230625152588
Epoch 1210, val loss: 1.0944417715072632
Epoch 1220, training loss: 6.037696361541748 = 0.015100900083780289 + 1.0 * 6.022595405578613
Epoch 1220, val loss: 1.0989031791687012
Epoch 1230, training loss: 6.038958549499512 = 0.014738625846803188 + 1.0 * 6.024219989776611
Epoch 1230, val loss: 1.103305459022522
Epoch 1240, training loss: 6.039961814880371 = 0.014391940087080002 + 1.0 * 6.025569915771484
Epoch 1240, val loss: 1.1079477071762085
Epoch 1250, training loss: 6.038060188293457 = 0.014055484905838966 + 1.0 * 6.024004936218262
Epoch 1250, val loss: 1.1122751235961914
Epoch 1260, training loss: 6.035247325897217 = 0.013735270127654076 + 1.0 * 6.021512031555176
Epoch 1260, val loss: 1.1166397333145142
Epoch 1270, training loss: 6.033764839172363 = 0.013422712683677673 + 1.0 * 6.0203423500061035
Epoch 1270, val loss: 1.1210556030273438
Epoch 1280, training loss: 6.032280921936035 = 0.013120956718921661 + 1.0 * 6.01915979385376
Epoch 1280, val loss: 1.125527024269104
Epoch 1290, training loss: 6.031425476074219 = 0.01282529253512621 + 1.0 * 6.018599987030029
Epoch 1290, val loss: 1.1297985315322876
Epoch 1300, training loss: 6.041782379150391 = 0.01254182867705822 + 1.0 * 6.029240608215332
Epoch 1300, val loss: 1.1338428258895874
Epoch 1310, training loss: 6.030147075653076 = 0.012269165366888046 + 1.0 * 6.01787805557251
Epoch 1310, val loss: 1.1379157304763794
Epoch 1320, training loss: 6.030289649963379 = 0.012007063254714012 + 1.0 * 6.018282413482666
Epoch 1320, val loss: 1.1424285173416138
Epoch 1330, training loss: 6.028628826141357 = 0.01175002846866846 + 1.0 * 6.016878604888916
Epoch 1330, val loss: 1.1465891599655151
Epoch 1340, training loss: 6.034046649932861 = 0.011501522734761238 + 1.0 * 6.022545337677002
Epoch 1340, val loss: 1.1506197452545166
Epoch 1350, training loss: 6.031492710113525 = 0.011259871535003185 + 1.0 * 6.020232677459717
Epoch 1350, val loss: 1.154466152191162
Epoch 1360, training loss: 6.029935836791992 = 0.011030538007616997 + 1.0 * 6.018905162811279
Epoch 1360, val loss: 1.1585514545440674
Epoch 1370, training loss: 6.027707576751709 = 0.010806689970195293 + 1.0 * 6.016901016235352
Epoch 1370, val loss: 1.16259765625
Epoch 1380, training loss: 6.028957366943359 = 0.01058903243392706 + 1.0 * 6.018368244171143
Epoch 1380, val loss: 1.1664718389511108
Epoch 1390, training loss: 6.027217864990234 = 0.010379180312156677 + 1.0 * 6.016838550567627
Epoch 1390, val loss: 1.170271873474121
Epoch 1400, training loss: 6.026036739349365 = 0.010174752213060856 + 1.0 * 6.015861988067627
Epoch 1400, val loss: 1.1741217374801636
Epoch 1410, training loss: 6.028515815734863 = 0.009976610541343689 + 1.0 * 6.0185394287109375
Epoch 1410, val loss: 1.1779245138168335
Epoch 1420, training loss: 6.024885177612305 = 0.009786061011254787 + 1.0 * 6.015099048614502
Epoch 1420, val loss: 1.1817985773086548
Epoch 1430, training loss: 6.024234294891357 = 0.009598733857274055 + 1.0 * 6.0146355628967285
Epoch 1430, val loss: 1.1856380701065063
Epoch 1440, training loss: 6.027895927429199 = 0.009416773915290833 + 1.0 * 6.018479347229004
Epoch 1440, val loss: 1.1892998218536377
Epoch 1450, training loss: 6.0237555503845215 = 0.009240599349141121 + 1.0 * 6.014514923095703
Epoch 1450, val loss: 1.192828893661499
Epoch 1460, training loss: 6.022712707519531 = 0.009069821797311306 + 1.0 * 6.01364278793335
Epoch 1460, val loss: 1.1964977979660034
Epoch 1470, training loss: 6.026106357574463 = 0.008904349990189075 + 1.0 * 6.017201900482178
Epoch 1470, val loss: 1.2001298666000366
Epoch 1480, training loss: 6.022308349609375 = 0.008744723163545132 + 1.0 * 6.013563632965088
Epoch 1480, val loss: 1.2035995721817017
Epoch 1490, training loss: 6.02841854095459 = 0.008589552715420723 + 1.0 * 6.019828796386719
Epoch 1490, val loss: 1.2071740627288818
Epoch 1500, training loss: 6.021462917327881 = 0.008438998833298683 + 1.0 * 6.013023853302002
Epoch 1500, val loss: 1.2103761434555054
Epoch 1510, training loss: 6.019721508026123 = 0.008292763493955135 + 1.0 * 6.0114288330078125
Epoch 1510, val loss: 1.2140504121780396
Epoch 1520, training loss: 6.018899440765381 = 0.008149790577590466 + 1.0 * 6.010749816894531
Epoch 1520, val loss: 1.2174937725067139
Epoch 1530, training loss: 6.022171974182129 = 0.008009025827050209 + 1.0 * 6.014163017272949
Epoch 1530, val loss: 1.220791220664978
Epoch 1540, training loss: 6.019073486328125 = 0.00787393655627966 + 1.0 * 6.011199474334717
Epoch 1540, val loss: 1.2240352630615234
Epoch 1550, training loss: 6.019937992095947 = 0.007742907851934433 + 1.0 * 6.012195110321045
Epoch 1550, val loss: 1.2274914979934692
Epoch 1560, training loss: 6.021020412445068 = 0.007614551577717066 + 1.0 * 6.013405799865723
Epoch 1560, val loss: 1.2307658195495605
Epoch 1570, training loss: 6.017788887023926 = 0.0074905590154230595 + 1.0 * 6.010298252105713
Epoch 1570, val loss: 1.2340439558029175
Epoch 1580, training loss: 6.01882266998291 = 0.007368803024291992 + 1.0 * 6.011453628540039
Epoch 1580, val loss: 1.2373278141021729
Epoch 1590, training loss: 6.018338680267334 = 0.007250306662172079 + 1.0 * 6.0110883712768555
Epoch 1590, val loss: 1.240538239479065
Epoch 1600, training loss: 6.021690368652344 = 0.007134458515793085 + 1.0 * 6.014555931091309
Epoch 1600, val loss: 1.2436532974243164
Epoch 1610, training loss: 6.022538661956787 = 0.007023205980658531 + 1.0 * 6.015515327453613
Epoch 1610, val loss: 1.246680736541748
Epoch 1620, training loss: 6.01705265045166 = 0.006913980934768915 + 1.0 * 6.010138511657715
Epoch 1620, val loss: 1.2498499155044556
Epoch 1630, training loss: 6.016348838806152 = 0.006807822268456221 + 1.0 * 6.009541034698486
Epoch 1630, val loss: 1.25309157371521
Epoch 1640, training loss: 6.0227861404418945 = 0.006704169325530529 + 1.0 * 6.016081809997559
Epoch 1640, val loss: 1.2560268640518188
Epoch 1650, training loss: 6.015961647033691 = 0.00660273851826787 + 1.0 * 6.009358882904053
Epoch 1650, val loss: 1.2589223384857178
Epoch 1660, training loss: 6.016829013824463 = 0.006504099350422621 + 1.0 * 6.010324954986572
Epoch 1660, val loss: 1.2620251178741455
Epoch 1670, training loss: 6.014416217803955 = 0.0064081051386892796 + 1.0 * 6.008008003234863
Epoch 1670, val loss: 1.2648565769195557
Epoch 1680, training loss: 6.0134806632995605 = 0.006314711645245552 + 1.0 * 6.007165908813477
Epoch 1680, val loss: 1.267997145652771
Epoch 1690, training loss: 6.0149245262146 = 0.006222906522452831 + 1.0 * 6.008701801300049
Epoch 1690, val loss: 1.2709436416625977
Epoch 1700, training loss: 6.015745162963867 = 0.006133045069873333 + 1.0 * 6.009612083435059
Epoch 1700, val loss: 1.2736176252365112
Epoch 1710, training loss: 6.013513088226318 = 0.00604508351534605 + 1.0 * 6.007468223571777
Epoch 1710, val loss: 1.2765675783157349
Epoch 1720, training loss: 6.017362594604492 = 0.005960550624877214 + 1.0 * 6.011402130126953
Epoch 1720, val loss: 1.2795381546020508
Epoch 1730, training loss: 6.011111736297607 = 0.005876835435628891 + 1.0 * 6.005234718322754
Epoch 1730, val loss: 1.2821176052093506
Epoch 1740, training loss: 6.011533260345459 = 0.0057956017553806305 + 1.0 * 6.005737781524658
Epoch 1740, val loss: 1.2850419282913208
Epoch 1750, training loss: 6.0178141593933105 = 0.005716264247894287 + 1.0 * 6.0120978355407715
Epoch 1750, val loss: 1.2878751754760742
Epoch 1760, training loss: 6.010532855987549 = 0.005638182163238525 + 1.0 * 6.004894733428955
Epoch 1760, val loss: 1.2902566194534302
Epoch 1770, training loss: 6.010355472564697 = 0.0055624633096158504 + 1.0 * 6.004793167114258
Epoch 1770, val loss: 1.2932175397872925
Epoch 1780, training loss: 6.015021800994873 = 0.005487779155373573 + 1.0 * 6.009533882141113
Epoch 1780, val loss: 1.2958711385726929
Epoch 1790, training loss: 6.010663986206055 = 0.005415597464889288 + 1.0 * 6.005248546600342
Epoch 1790, val loss: 1.298385739326477
Epoch 1800, training loss: 6.009515285491943 = 0.005344523582607508 + 1.0 * 6.004170894622803
Epoch 1800, val loss: 1.301125168800354
Epoch 1810, training loss: 6.0094828605651855 = 0.00527463061735034 + 1.0 * 6.004208087921143
Epoch 1810, val loss: 1.3038146495819092
Epoch 1820, training loss: 6.017042636871338 = 0.005205892492085695 + 1.0 * 6.011836528778076
Epoch 1820, val loss: 1.306268334388733
Epoch 1830, training loss: 6.013990879058838 = 0.005139521788805723 + 1.0 * 6.008851528167725
Epoch 1830, val loss: 1.3085570335388184
Epoch 1840, training loss: 6.00944185256958 = 0.005075871478766203 + 1.0 * 6.004365921020508
Epoch 1840, val loss: 1.3113609552383423
Epoch 1850, training loss: 6.007371425628662 = 0.005011929664760828 + 1.0 * 6.002359390258789
Epoch 1850, val loss: 1.3139945268630981
Epoch 1860, training loss: 6.007457733154297 = 0.0049486965872347355 + 1.0 * 6.002509117126465
Epoch 1860, val loss: 1.3164764642715454
Epoch 1870, training loss: 6.0132575035095215 = 0.004886074457317591 + 1.0 * 6.008371353149414
Epoch 1870, val loss: 1.3188166618347168
Epoch 1880, training loss: 6.012063980102539 = 0.004827424418181181 + 1.0 * 6.007236480712891
Epoch 1880, val loss: 1.321163535118103
Epoch 1890, training loss: 6.01043701171875 = 0.004768941085785627 + 1.0 * 6.0056681632995605
Epoch 1890, val loss: 1.323513388633728
Epoch 1900, training loss: 6.008423805236816 = 0.004712687339633703 + 1.0 * 6.003711223602295
Epoch 1900, val loss: 1.3261733055114746
Epoch 1910, training loss: 6.008638858795166 = 0.0046564387157559395 + 1.0 * 6.0039825439453125
Epoch 1910, val loss: 1.3285331726074219
Epoch 1920, training loss: 6.008106708526611 = 0.004601386375725269 + 1.0 * 6.003505229949951
Epoch 1920, val loss: 1.330850601196289
Epoch 1930, training loss: 6.008646488189697 = 0.004546917509287596 + 1.0 * 6.004099369049072
Epoch 1930, val loss: 1.3331537246704102
Epoch 1940, training loss: 6.006503582000732 = 0.004493615124374628 + 1.0 * 6.002009868621826
Epoch 1940, val loss: 1.3354114294052124
Epoch 1950, training loss: 6.005519866943359 = 0.004441634751856327 + 1.0 * 6.001078128814697
Epoch 1950, val loss: 1.3378199338912964
Epoch 1960, training loss: 6.0059919357299805 = 0.004390348214656115 + 1.0 * 6.001601696014404
Epoch 1960, val loss: 1.3401975631713867
Epoch 1970, training loss: 6.012058734893799 = 0.004340198822319508 + 1.0 * 6.007718563079834
Epoch 1970, val loss: 1.34230375289917
Epoch 1980, training loss: 6.007201194763184 = 0.004290993791073561 + 1.0 * 6.002910137176514
Epoch 1980, val loss: 1.3444509506225586
Epoch 1990, training loss: 6.006124496459961 = 0.0042434209026396275 + 1.0 * 6.001881122589111
Epoch 1990, val loss: 1.3468507528305054
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.8745
Flip ASR: 0.8533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320499420166016 = 1.9466315507888794 + 1.0 * 8.373867988586426
Epoch 0, val loss: 1.9397608041763306
Epoch 10, training loss: 10.30966854095459 = 1.9361952543258667 + 1.0 * 8.373473167419434
Epoch 10, val loss: 1.9297081232070923
Epoch 20, training loss: 10.293785095214844 = 1.9235129356384277 + 1.0 * 8.370271682739258
Epoch 20, val loss: 1.9169437885284424
Epoch 30, training loss: 10.250393867492676 = 1.9060781002044678 + 1.0 * 8.344315528869629
Epoch 30, val loss: 1.8990139961242676
Epoch 40, training loss: 10.054871559143066 = 1.8842295408248901 + 1.0 * 8.170641899108887
Epoch 40, val loss: 1.8771255016326904
Epoch 50, training loss: 9.480155944824219 = 1.8608468770980835 + 1.0 * 7.619308948516846
Epoch 50, val loss: 1.854024052619934
Epoch 60, training loss: 9.019855499267578 = 1.840725302696228 + 1.0 * 7.1791300773620605
Epoch 60, val loss: 1.8355607986450195
Epoch 70, training loss: 8.6185302734375 = 1.824009895324707 + 1.0 * 6.794519901275635
Epoch 70, val loss: 1.8193459510803223
Epoch 80, training loss: 8.468891143798828 = 1.8073511123657227 + 1.0 * 6.661540508270264
Epoch 80, val loss: 1.8031598329544067
Epoch 90, training loss: 8.367531776428223 = 1.7893576622009277 + 1.0 * 6.578174114227295
Epoch 90, val loss: 1.7863543033599854
Epoch 100, training loss: 8.25770378112793 = 1.7723876237869263 + 1.0 * 6.485315799713135
Epoch 100, val loss: 1.77121102809906
Epoch 110, training loss: 8.180035591125488 = 1.7556618452072144 + 1.0 * 6.424373626708984
Epoch 110, val loss: 1.7565581798553467
Epoch 120, training loss: 8.11594295501709 = 1.7370307445526123 + 1.0 * 6.378912448883057
Epoch 120, val loss: 1.7403444051742554
Epoch 130, training loss: 8.05429744720459 = 1.7156223058700562 + 1.0 * 6.338675498962402
Epoch 130, val loss: 1.7220336198806763
Epoch 140, training loss: 7.999910354614258 = 1.691127061843872 + 1.0 * 6.308783531188965
Epoch 140, val loss: 1.7016175985336304
Epoch 150, training loss: 7.942889213562012 = 1.6627804040908813 + 1.0 * 6.28010892868042
Epoch 150, val loss: 1.6784892082214355
Epoch 160, training loss: 7.88864278793335 = 1.629010796546936 + 1.0 * 6.259632110595703
Epoch 160, val loss: 1.6512991189956665
Epoch 170, training loss: 7.8311285972595215 = 1.5887194871902466 + 1.0 * 6.2424092292785645
Epoch 170, val loss: 1.6189697980880737
Epoch 180, training loss: 7.770516395568848 = 1.5411888360977173 + 1.0 * 6.22932767868042
Epoch 180, val loss: 1.5813283920288086
Epoch 190, training loss: 7.704324722290039 = 1.4871941804885864 + 1.0 * 6.217130661010742
Epoch 190, val loss: 1.5390305519104004
Epoch 200, training loss: 7.634174346923828 = 1.4270026683807373 + 1.0 * 6.20717191696167
Epoch 200, val loss: 1.4924393892288208
Epoch 210, training loss: 7.560539722442627 = 1.36106538772583 + 1.0 * 6.199474334716797
Epoch 210, val loss: 1.4422458410263062
Epoch 220, training loss: 7.484068870544434 = 1.2921268939971924 + 1.0 * 6.19194221496582
Epoch 220, val loss: 1.3904095888137817
Epoch 230, training loss: 7.405829429626465 = 1.2219228744506836 + 1.0 * 6.183906555175781
Epoch 230, val loss: 1.338792324066162
Epoch 240, training loss: 7.332115650177002 = 1.1521626710891724 + 1.0 * 6.179953098297119
Epoch 240, val loss: 1.2887388467788696
Epoch 250, training loss: 7.256207466125488 = 1.0849592685699463 + 1.0 * 6.171248435974121
Epoch 250, val loss: 1.241319179534912
Epoch 260, training loss: 7.187759876251221 = 1.0203570127487183 + 1.0 * 6.167402744293213
Epoch 260, val loss: 1.1964250802993774
Epoch 270, training loss: 7.120725154876709 = 0.9598172903060913 + 1.0 * 6.160907745361328
Epoch 270, val loss: 1.154571771621704
Epoch 280, training loss: 7.060144424438477 = 0.9032374024391174 + 1.0 * 6.156907081604004
Epoch 280, val loss: 1.1157433986663818
Epoch 290, training loss: 7.001303672790527 = 0.8507140874862671 + 1.0 * 6.150589466094971
Epoch 290, val loss: 1.0800323486328125
Epoch 300, training loss: 6.948299407958984 = 0.8021658658981323 + 1.0 * 6.1461334228515625
Epoch 300, val loss: 1.0471242666244507
Epoch 310, training loss: 6.899563312530518 = 0.7572302222251892 + 1.0 * 6.142333030700684
Epoch 310, val loss: 1.0171329975128174
Epoch 320, training loss: 6.853715896606445 = 0.715868353843689 + 1.0 * 6.137847423553467
Epoch 320, val loss: 0.9900105595588684
Epoch 330, training loss: 6.810361385345459 = 0.6775038838386536 + 1.0 * 6.132857322692871
Epoch 330, val loss: 0.965408980846405
Epoch 340, training loss: 6.770566940307617 = 0.641351580619812 + 1.0 * 6.129215240478516
Epoch 340, val loss: 0.9426645636558533
Epoch 350, training loss: 6.740629196166992 = 0.6071645617485046 + 1.0 * 6.133464813232422
Epoch 350, val loss: 0.9214405417442322
Epoch 360, training loss: 6.70086669921875 = 0.5752217769622803 + 1.0 * 6.125645160675049
Epoch 360, val loss: 0.9018200635910034
Epoch 370, training loss: 6.664453983306885 = 0.5449740290641785 + 1.0 * 6.119480133056641
Epoch 370, val loss: 0.8834565281867981
Epoch 380, training loss: 6.632803440093994 = 0.5159252285957336 + 1.0 * 6.116878032684326
Epoch 380, val loss: 0.8660544753074646
Epoch 390, training loss: 6.610434532165527 = 0.48800626397132874 + 1.0 * 6.1224284172058105
Epoch 390, val loss: 0.8495630621910095
Epoch 400, training loss: 6.574728012084961 = 0.461362361907959 + 1.0 * 6.113365650177002
Epoch 400, val loss: 0.8342748880386353
Epoch 410, training loss: 6.544101715087891 = 0.43579697608947754 + 1.0 * 6.108304977416992
Epoch 410, val loss: 0.8201711773872375
Epoch 420, training loss: 6.52548885345459 = 0.4112183153629303 + 1.0 * 6.1142706871032715
Epoch 420, val loss: 0.8071253895759583
Epoch 430, training loss: 6.4946208000183105 = 0.38790377974510193 + 1.0 * 6.106717109680176
Epoch 430, val loss: 0.7951701879501343
Epoch 440, training loss: 6.4676642417907715 = 0.36552757024765015 + 1.0 * 6.102136611938477
Epoch 440, val loss: 0.7843612432479858
Epoch 450, training loss: 6.443315505981445 = 0.34403228759765625 + 1.0 * 6.099283218383789
Epoch 450, val loss: 0.7745391726493835
Epoch 460, training loss: 6.4267659187316895 = 0.32348382472991943 + 1.0 * 6.1032819747924805
Epoch 460, val loss: 0.7656301856040955
Epoch 470, training loss: 6.40041446685791 = 0.30411872267723083 + 1.0 * 6.0962958335876465
Epoch 470, val loss: 0.7576586008071899
Epoch 480, training loss: 6.379842758178711 = 0.28568845987319946 + 1.0 * 6.094154357910156
Epoch 480, val loss: 0.7506099939346313
Epoch 490, training loss: 6.35950231552124 = 0.26806408166885376 + 1.0 * 6.091438293457031
Epoch 490, val loss: 0.7443327307701111
Epoch 500, training loss: 6.359647750854492 = 0.25122010707855225 + 1.0 * 6.10842752456665
Epoch 500, val loss: 0.7387112379074097
Epoch 510, training loss: 6.329579830169678 = 0.2354467213153839 + 1.0 * 6.094132900238037
Epoch 510, val loss: 0.7337202429771423
Epoch 520, training loss: 6.307650566101074 = 0.22056704759597778 + 1.0 * 6.087083339691162
Epoch 520, val loss: 0.7295360565185547
Epoch 530, training loss: 6.290327072143555 = 0.20648923516273499 + 1.0 * 6.083837985992432
Epoch 530, val loss: 0.7260804176330566
Epoch 540, training loss: 6.285611629486084 = 0.19320763647556305 + 1.0 * 6.092403888702393
Epoch 540, val loss: 0.7231763005256653
Epoch 550, training loss: 6.263358116149902 = 0.18081055581569672 + 1.0 * 6.082547664642334
Epoch 550, val loss: 0.7209286093711853
Epoch 560, training loss: 6.2491021156311035 = 0.1692771464586258 + 1.0 * 6.079824924468994
Epoch 560, val loss: 0.7193357944488525
Epoch 570, training loss: 6.237612724304199 = 0.15847532451152802 + 1.0 * 6.079137325286865
Epoch 570, val loss: 0.7183690667152405
Epoch 580, training loss: 6.226746559143066 = 0.14841607213020325 + 1.0 * 6.0783305168151855
Epoch 580, val loss: 0.7178813219070435
Epoch 590, training loss: 6.217360019683838 = 0.13909442722797394 + 1.0 * 6.07826566696167
Epoch 590, val loss: 0.7179740071296692
Epoch 600, training loss: 6.206243515014648 = 0.1304323524236679 + 1.0 * 6.075811386108398
Epoch 600, val loss: 0.7186036705970764
Epoch 610, training loss: 6.19658088684082 = 0.1223854348063469 + 1.0 * 6.074195384979248
Epoch 610, val loss: 0.7197296619415283
Epoch 620, training loss: 6.191433906555176 = 0.11492301523685455 + 1.0 * 6.076510906219482
Epoch 620, val loss: 0.7213279008865356
Epoch 630, training loss: 6.1803083419799805 = 0.10801418870687485 + 1.0 * 6.072294235229492
Epoch 630, val loss: 0.7233486771583557
Epoch 640, training loss: 6.1707963943481445 = 0.10160767287015915 + 1.0 * 6.069188594818115
Epoch 640, val loss: 0.7258403301239014
Epoch 650, training loss: 6.164710521697998 = 0.0956592932343483 + 1.0 * 6.069051265716553
Epoch 650, val loss: 0.7287222743034363
Epoch 660, training loss: 6.160236358642578 = 0.0901440680027008 + 1.0 * 6.07009220123291
Epoch 660, val loss: 0.7320024371147156
Epoch 670, training loss: 6.153716564178467 = 0.08508267998695374 + 1.0 * 6.068634033203125
Epoch 670, val loss: 0.7354836463928223
Epoch 680, training loss: 6.145204544067383 = 0.08037889003753662 + 1.0 * 6.064825534820557
Epoch 680, val loss: 0.7393819689750671
Epoch 690, training loss: 6.141168594360352 = 0.07601059973239899 + 1.0 * 6.065157890319824
Epoch 690, val loss: 0.7435688376426697
Epoch 700, training loss: 6.1353678703308105 = 0.07196193933486938 + 1.0 * 6.063405990600586
Epoch 700, val loss: 0.7478747963905334
Epoch 710, training loss: 6.131922245025635 = 0.06820861250162125 + 1.0 * 6.063713550567627
Epoch 710, val loss: 0.7523713707923889
Epoch 720, training loss: 6.125744819641113 = 0.06472402811050415 + 1.0 * 6.061020851135254
Epoch 720, val loss: 0.7570244073867798
Epoch 730, training loss: 6.126560211181641 = 0.061477407813072205 + 1.0 * 6.065083026885986
Epoch 730, val loss: 0.7618906497955322
Epoch 740, training loss: 6.119966983795166 = 0.05846525728702545 + 1.0 * 6.061501502990723
Epoch 740, val loss: 0.7668253183364868
Epoch 750, training loss: 6.1149492263793945 = 0.055656976997852325 + 1.0 * 6.059292316436768
Epoch 750, val loss: 0.7718399167060852
Epoch 760, training loss: 6.111349582672119 = 0.0530330166220665 + 1.0 * 6.058316707611084
Epoch 760, val loss: 0.7769649028778076
Epoch 770, training loss: 6.109065532684326 = 0.050586096942424774 + 1.0 * 6.058479309082031
Epoch 770, val loss: 0.7821522355079651
Epoch 780, training loss: 6.106383800506592 = 0.048294760286808014 + 1.0 * 6.058089256286621
Epoch 780, val loss: 0.7873784303665161
Epoch 790, training loss: 6.1055908203125 = 0.04615088924765587 + 1.0 * 6.0594401359558105
Epoch 790, val loss: 0.7926294803619385
Epoch 800, training loss: 6.098587512969971 = 0.04415787383913994 + 1.0 * 6.054429531097412
Epoch 800, val loss: 0.797867476940155
Epoch 810, training loss: 6.094191074371338 = 0.04227864742279053 + 1.0 * 6.051912307739258
Epoch 810, val loss: 0.8031744360923767
Epoch 820, training loss: 6.0926995277404785 = 0.040511444211006165 + 1.0 * 6.052187919616699
Epoch 820, val loss: 0.8084843158721924
Epoch 830, training loss: 6.091932773590088 = 0.038848649710416794 + 1.0 * 6.053083896636963
Epoch 830, val loss: 0.8137779235839844
Epoch 840, training loss: 6.095526218414307 = 0.03728444129228592 + 1.0 * 6.058241844177246
Epoch 840, val loss: 0.8190422058105469
Epoch 850, training loss: 6.088191986083984 = 0.03583022579550743 + 1.0 * 6.052361965179443
Epoch 850, val loss: 0.8242018818855286
Epoch 860, training loss: 6.084210395812988 = 0.03445497527718544 + 1.0 * 6.049755573272705
Epoch 860, val loss: 0.8294727802276611
Epoch 870, training loss: 6.084904670715332 = 0.03315725177526474 + 1.0 * 6.0517473220825195
Epoch 870, val loss: 0.8346891403198242
Epoch 880, training loss: 6.079039573669434 = 0.031930312514305115 + 1.0 * 6.047109127044678
Epoch 880, val loss: 0.8398317098617554
Epoch 890, training loss: 6.079674243927002 = 0.030770905315876007 + 1.0 * 6.048903465270996
Epoch 890, val loss: 0.8449689149856567
Epoch 900, training loss: 6.075270652770996 = 0.02967657707631588 + 1.0 * 6.045594215393066
Epoch 900, val loss: 0.8500249981880188
Epoch 910, training loss: 6.073235511779785 = 0.028638087213039398 + 1.0 * 6.044597625732422
Epoch 910, val loss: 0.8550646305084229
Epoch 920, training loss: 6.076573371887207 = 0.027650678530335426 + 1.0 * 6.048922538757324
Epoch 920, val loss: 0.8600843548774719
Epoch 930, training loss: 6.071540832519531 = 0.026718148961663246 + 1.0 * 6.044822692871094
Epoch 930, val loss: 0.8650311231613159
Epoch 940, training loss: 6.069216251373291 = 0.025830158963799477 + 1.0 * 6.043385982513428
Epoch 940, val loss: 0.8699507117271423
Epoch 950, training loss: 6.0710015296936035 = 0.024985095486044884 + 1.0 * 6.046016216278076
Epoch 950, val loss: 0.8748707175254822
Epoch 960, training loss: 6.066234588623047 = 0.02418268658220768 + 1.0 * 6.042051792144775
Epoch 960, val loss: 0.8797054886817932
Epoch 970, training loss: 6.068726539611816 = 0.023420291021466255 + 1.0 * 6.045306205749512
Epoch 970, val loss: 0.8844780921936035
Epoch 980, training loss: 6.064167022705078 = 0.022702185437083244 + 1.0 * 6.041464805603027
Epoch 980, val loss: 0.889135479927063
Epoch 990, training loss: 6.060932159423828 = 0.022013334557414055 + 1.0 * 6.038918972015381
Epoch 990, val loss: 0.8938301801681519
Epoch 1000, training loss: 6.0605950355529785 = 0.02135341428220272 + 1.0 * 6.039241790771484
Epoch 1000, val loss: 0.8985331654548645
Epoch 1010, training loss: 6.062779426574707 = 0.02072221226990223 + 1.0 * 6.042057037353516
Epoch 1010, val loss: 0.9031355977058411
Epoch 1020, training loss: 6.060102462768555 = 0.02011786960065365 + 1.0 * 6.039984703063965
Epoch 1020, val loss: 0.9076915979385376
Epoch 1030, training loss: 6.060346603393555 = 0.019544312730431557 + 1.0 * 6.040802478790283
Epoch 1030, val loss: 0.9121803641319275
Epoch 1040, training loss: 6.055541515350342 = 0.018993018195033073 + 1.0 * 6.036548614501953
Epoch 1040, val loss: 0.9166679978370667
Epoch 1050, training loss: 6.0557074546813965 = 0.01846657134592533 + 1.0 * 6.037240982055664
Epoch 1050, val loss: 0.921123743057251
Epoch 1060, training loss: 6.0553765296936035 = 0.017961552366614342 + 1.0 * 6.037415027618408
Epoch 1060, val loss: 0.9255237579345703
Epoch 1070, training loss: 6.053465843200684 = 0.017479727044701576 + 1.0 * 6.035985946655273
Epoch 1070, val loss: 0.9298809766769409
Epoch 1080, training loss: 6.057014465332031 = 0.0170163344591856 + 1.0 * 6.0399980545043945
Epoch 1080, val loss: 0.9341756105422974
Epoch 1090, training loss: 6.052587985992432 = 0.01657223328948021 + 1.0 * 6.03601598739624
Epoch 1090, val loss: 0.9384380578994751
Epoch 1100, training loss: 6.048749923706055 = 0.016148436814546585 + 1.0 * 6.032601356506348
Epoch 1100, val loss: 0.9426515102386475
Epoch 1110, training loss: 6.048552989959717 = 0.015737727284431458 + 1.0 * 6.032815456390381
Epoch 1110, val loss: 0.9468563795089722
Epoch 1120, training loss: 6.0537872314453125 = 0.01534220203757286 + 1.0 * 6.038444995880127
Epoch 1120, val loss: 0.9510526061058044
Epoch 1130, training loss: 6.052011966705322 = 0.01496320590376854 + 1.0 * 6.037048816680908
Epoch 1130, val loss: 0.9550803899765015
Epoch 1140, training loss: 6.048467636108398 = 0.014601186849176884 + 1.0 * 6.0338664054870605
Epoch 1140, val loss: 0.9591296315193176
Epoch 1150, training loss: 6.050286293029785 = 0.014251286163926125 + 1.0 * 6.036035060882568
Epoch 1150, val loss: 0.9631341099739075
Epoch 1160, training loss: 6.044716835021973 = 0.013914207927882671 + 1.0 * 6.0308027267456055
Epoch 1160, val loss: 0.9671155214309692
Epoch 1170, training loss: 6.043120384216309 = 0.013589398004114628 + 1.0 * 6.029531002044678
Epoch 1170, val loss: 0.9710857272148132
Epoch 1180, training loss: 6.046934127807617 = 0.013274973258376122 + 1.0 * 6.033658981323242
Epoch 1180, val loss: 0.97501140832901
Epoch 1190, training loss: 6.041937351226807 = 0.012972251512110233 + 1.0 * 6.028964996337891
Epoch 1190, val loss: 0.9788278937339783
Epoch 1200, training loss: 6.042360782623291 = 0.012682158499956131 + 1.0 * 6.029678821563721
Epoch 1200, val loss: 0.9825993180274963
Epoch 1210, training loss: 6.0408244132995605 = 0.012401239015161991 + 1.0 * 6.028423309326172
Epoch 1210, val loss: 0.9864020347595215
Epoch 1220, training loss: 6.039604663848877 = 0.012128859758377075 + 1.0 * 6.027475833892822
Epoch 1220, val loss: 0.9901940226554871
Epoch 1230, training loss: 6.043499946594238 = 0.011866211891174316 + 1.0 * 6.0316338539123535
Epoch 1230, val loss: 0.9939119815826416
Epoch 1240, training loss: 6.040021896362305 = 0.011612693779170513 + 1.0 * 6.028409004211426
Epoch 1240, val loss: 0.9975699186325073
Epoch 1250, training loss: 6.0379133224487305 = 0.011367592960596085 + 1.0 * 6.026545524597168
Epoch 1250, val loss: 1.0012280941009521
Epoch 1260, training loss: 6.04110050201416 = 0.011129043065011501 + 1.0 * 6.029971599578857
Epoch 1260, val loss: 1.0048812627792358
Epoch 1270, training loss: 6.037312030792236 = 0.0108995595946908 + 1.0 * 6.026412487030029
Epoch 1270, val loss: 1.0084686279296875
Epoch 1280, training loss: 6.041113376617432 = 0.010677349753677845 + 1.0 * 6.030436038970947
Epoch 1280, val loss: 1.0119866132736206
Epoch 1290, training loss: 6.035080432891846 = 0.010465873405337334 + 1.0 * 6.024614334106445
Epoch 1290, val loss: 1.0154622793197632
Epoch 1300, training loss: 6.03358793258667 = 0.010257680900394917 + 1.0 * 6.023330211639404
Epoch 1300, val loss: 1.0189757347106934
Epoch 1310, training loss: 6.037181377410889 = 0.01005469635128975 + 1.0 * 6.027126789093018
Epoch 1310, val loss: 1.022505283355713
Epoch 1320, training loss: 6.032862186431885 = 0.009858934208750725 + 1.0 * 6.023003101348877
Epoch 1320, val loss: 1.0258584022521973
Epoch 1330, training loss: 6.033934116363525 = 0.009670279920101166 + 1.0 * 6.024263858795166
Epoch 1330, val loss: 1.02922785282135
Epoch 1340, training loss: 6.0377583503723145 = 0.009487099014222622 + 1.0 * 6.028271198272705
Epoch 1340, val loss: 1.0325735807418823
Epoch 1350, training loss: 6.030514240264893 = 0.009309448301792145 + 1.0 * 6.021204948425293
Epoch 1350, val loss: 1.0358892679214478
Epoch 1360, training loss: 6.030453681945801 = 0.009136196225881577 + 1.0 * 6.021317481994629
Epoch 1360, val loss: 1.0392045974731445
Epoch 1370, training loss: 6.0356292724609375 = 0.008967926725745201 + 1.0 * 6.026661396026611
Epoch 1370, val loss: 1.0424799919128418
Epoch 1380, training loss: 6.030767917633057 = 0.008804118260741234 + 1.0 * 6.021963596343994
Epoch 1380, val loss: 1.045695424079895
Epoch 1390, training loss: 6.034704685211182 = 0.008647543378174305 + 1.0 * 6.026057243347168
Epoch 1390, val loss: 1.0488072633743286
Epoch 1400, training loss: 6.0299882888793945 = 0.00849324744194746 + 1.0 * 6.0214948654174805
Epoch 1400, val loss: 1.0519846677780151
Epoch 1410, training loss: 6.031971454620361 = 0.008344619534909725 + 1.0 * 6.023626804351807
Epoch 1410, val loss: 1.0551292896270752
Epoch 1420, training loss: 6.027058124542236 = 0.008198868483304977 + 1.0 * 6.018859386444092
Epoch 1420, val loss: 1.058254361152649
Epoch 1430, training loss: 6.027177810668945 = 0.008056429214775562 + 1.0 * 6.019121170043945
Epoch 1430, val loss: 1.0613658428192139
Epoch 1440, training loss: 6.03169584274292 = 0.007917513139545918 + 1.0 * 6.023778438568115
Epoch 1440, val loss: 1.0644586086273193
Epoch 1450, training loss: 6.030672073364258 = 0.0077833253890275955 + 1.0 * 6.022888660430908
Epoch 1450, val loss: 1.067495346069336
Epoch 1460, training loss: 6.027309894561768 = 0.007654384709894657 + 1.0 * 6.019655704498291
Epoch 1460, val loss: 1.0704134702682495
Epoch 1470, training loss: 6.025790691375732 = 0.007528337184339762 + 1.0 * 6.0182623863220215
Epoch 1470, val loss: 1.0733839273452759
Epoch 1480, training loss: 6.025871276855469 = 0.007404835894703865 + 1.0 * 6.018466472625732
Epoch 1480, val loss: 1.076385259628296
Epoch 1490, training loss: 6.028902530670166 = 0.00728415185585618 + 1.0 * 6.021618366241455
Epoch 1490, val loss: 1.079332709312439
Epoch 1500, training loss: 6.027124404907227 = 0.007165880408138037 + 1.0 * 6.01995849609375
Epoch 1500, val loss: 1.0821973085403442
Epoch 1510, training loss: 6.029259204864502 = 0.007053004577755928 + 1.0 * 6.0222063064575195
Epoch 1510, val loss: 1.084999918937683
Epoch 1520, training loss: 6.02304220199585 = 0.006942578591406345 + 1.0 * 6.016099452972412
Epoch 1520, val loss: 1.0878095626831055
Epoch 1530, training loss: 6.022243022918701 = 0.0068350899964571 + 1.0 * 6.015408039093018
Epoch 1530, val loss: 1.0906267166137695
Epoch 1540, training loss: 6.021035194396973 = 0.00672914506867528 + 1.0 * 6.01430606842041
Epoch 1540, val loss: 1.0934683084487915
Epoch 1550, training loss: 6.025672435760498 = 0.006625398062169552 + 1.0 * 6.019047260284424
Epoch 1550, val loss: 1.0962761640548706
Epoch 1560, training loss: 6.022589683532715 = 0.006524380762130022 + 1.0 * 6.0160651206970215
Epoch 1560, val loss: 1.099019169807434
Epoch 1570, training loss: 6.027668476104736 = 0.006426646374166012 + 1.0 * 6.021241664886475
Epoch 1570, val loss: 1.1016660928726196
Epoch 1580, training loss: 6.024658203125 = 0.006331801414489746 + 1.0 * 6.018326282501221
Epoch 1580, val loss: 1.1043020486831665
Epoch 1590, training loss: 6.0224456787109375 = 0.00623923959210515 + 1.0 * 6.01620626449585
Epoch 1590, val loss: 1.106945276260376
Epoch 1600, training loss: 6.020033836364746 = 0.00614858977496624 + 1.0 * 6.013885021209717
Epoch 1600, val loss: 1.1096006631851196
Epoch 1610, training loss: 6.022360801696777 = 0.006059827748686075 + 1.0 * 6.016301155090332
Epoch 1610, val loss: 1.1122469902038574
Epoch 1620, training loss: 6.021249771118164 = 0.005973165854811668 + 1.0 * 6.0152764320373535
Epoch 1620, val loss: 1.1148298978805542
Epoch 1630, training loss: 6.018529415130615 = 0.005888182669878006 + 1.0 * 6.012641429901123
Epoch 1630, val loss: 1.117375373840332
Epoch 1640, training loss: 6.024035453796387 = 0.005804650951176882 + 1.0 * 6.01823091506958
Epoch 1640, val loss: 1.1199222803115845
Epoch 1650, training loss: 6.019102096557617 = 0.005724651739001274 + 1.0 * 6.013377666473389
Epoch 1650, val loss: 1.122427225112915
Epoch 1660, training loss: 6.017745018005371 = 0.00564564997330308 + 1.0 * 6.012099266052246
Epoch 1660, val loss: 1.1249144077301025
Epoch 1670, training loss: 6.023028373718262 = 0.00556857418268919 + 1.0 * 6.017459869384766
Epoch 1670, val loss: 1.127388596534729
Epoch 1680, training loss: 6.018003940582275 = 0.0054929135367274284 + 1.0 * 6.012511253356934
Epoch 1680, val loss: 1.1297993659973145
Epoch 1690, training loss: 6.017596244812012 = 0.00541967386379838 + 1.0 * 6.012176513671875
Epoch 1690, val loss: 1.132196307182312
Epoch 1700, training loss: 6.020053863525391 = 0.0053473361767828465 + 1.0 * 6.014706611633301
Epoch 1700, val loss: 1.1346184015274048
Epoch 1710, training loss: 6.017458438873291 = 0.005276794545352459 + 1.0 * 6.012181758880615
Epoch 1710, val loss: 1.1370075941085815
Epoch 1720, training loss: 6.014178276062012 = 0.005208439193665981 + 1.0 * 6.008969783782959
Epoch 1720, val loss: 1.1393508911132812
Epoch 1730, training loss: 6.017274379730225 = 0.005140745081007481 + 1.0 * 6.012133598327637
Epoch 1730, val loss: 1.141750693321228
Epoch 1740, training loss: 6.0200581550598145 = 0.005074456334114075 + 1.0 * 6.014983654022217
Epoch 1740, val loss: 1.1441071033477783
Epoch 1750, training loss: 6.016200065612793 = 0.005009673070162535 + 1.0 * 6.011190414428711
Epoch 1750, val loss: 1.1463334560394287
Epoch 1760, training loss: 6.013955593109131 = 0.004947036039084196 + 1.0 * 6.009008407592773
Epoch 1760, val loss: 1.1486016511917114
Epoch 1770, training loss: 6.020131587982178 = 0.004885212983936071 + 1.0 * 6.015246391296387
Epoch 1770, val loss: 1.1509230136871338
Epoch 1780, training loss: 6.014891624450684 = 0.004824113100767136 + 1.0 * 6.010067462921143
Epoch 1780, val loss: 1.1531267166137695
Epoch 1790, training loss: 6.012392997741699 = 0.004765723366290331 + 1.0 * 6.007627487182617
Epoch 1790, val loss: 1.1553070545196533
Epoch 1800, training loss: 6.011452674865723 = 0.004707446787506342 + 1.0 * 6.006745338439941
Epoch 1800, val loss: 1.1575385332107544
Epoch 1810, training loss: 6.013882160186768 = 0.004650109447538853 + 1.0 * 6.009232044219971
Epoch 1810, val loss: 1.1597965955734253
Epoch 1820, training loss: 6.015033721923828 = 0.004593844059854746 + 1.0 * 6.010439872741699
Epoch 1820, val loss: 1.1619588136672974
Epoch 1830, training loss: 6.012392044067383 = 0.004539185203611851 + 1.0 * 6.007853031158447
Epoch 1830, val loss: 1.1640524864196777
Epoch 1840, training loss: 6.016716480255127 = 0.004485884215682745 + 1.0 * 6.012230396270752
Epoch 1840, val loss: 1.166154384613037
Epoch 1850, training loss: 6.012490272521973 = 0.004433661699295044 + 1.0 * 6.008056640625
Epoch 1850, val loss: 1.1682921648025513
Epoch 1860, training loss: 6.010380744934082 = 0.004382158163934946 + 1.0 * 6.005998611450195
Epoch 1860, val loss: 1.1704254150390625
Epoch 1870, training loss: 6.012277603149414 = 0.004331599455326796 + 1.0 * 6.007946014404297
Epoch 1870, val loss: 1.1725419759750366
Epoch 1880, training loss: 6.015083312988281 = 0.004281651694327593 + 1.0 * 6.010801792144775
Epoch 1880, val loss: 1.1746083498001099
Epoch 1890, training loss: 6.012017250061035 = 0.004232773557305336 + 1.0 * 6.007784366607666
Epoch 1890, val loss: 1.1766420602798462
Epoch 1900, training loss: 6.016801834106445 = 0.004184930119663477 + 1.0 * 6.012617111206055
Epoch 1900, val loss: 1.1786893606185913
Epoch 1910, training loss: 6.009451389312744 = 0.004138697870075703 + 1.0 * 6.005312919616699
Epoch 1910, val loss: 1.1806848049163818
Epoch 1920, training loss: 6.00864839553833 = 0.004092886112630367 + 1.0 * 6.004555702209473
Epoch 1920, val loss: 1.1826934814453125
Epoch 1930, training loss: 6.009491920471191 = 0.004047339782118797 + 1.0 * 6.005444526672363
Epoch 1930, val loss: 1.1847355365753174
Epoch 1940, training loss: 6.015235900878906 = 0.004002987872809172 + 1.0 * 6.011232852935791
Epoch 1940, val loss: 1.1867313385009766
Epoch 1950, training loss: 6.014166831970215 = 0.0039594280533492565 + 1.0 * 6.010207176208496
Epoch 1950, val loss: 1.1886011362075806
Epoch 1960, training loss: 6.008411884307861 = 0.0039173923432827 + 1.0 * 6.004494667053223
Epoch 1960, val loss: 1.1905103921890259
Epoch 1970, training loss: 6.007717132568359 = 0.0038757240399718285 + 1.0 * 6.003841400146484
Epoch 1970, val loss: 1.1924636363983154
Epoch 1980, training loss: 6.010220527648926 = 0.003834380302578211 + 1.0 * 6.0063862800598145
Epoch 1980, val loss: 1.1944602727890015
Epoch 1990, training loss: 6.012099266052246 = 0.003793856129050255 + 1.0 * 6.008305549621582
Epoch 1990, val loss: 1.1963390111923218
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.8192
Flip ASR: 0.7867/225 nodes
The final ASR:0.77245, 0.10764, Accuracy:0.80741, 0.02688
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10524])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.316638946533203 = 1.9427495002746582 + 1.0 * 8.373889923095703
Epoch 0, val loss: 1.9495266675949097
Epoch 10, training loss: 10.306612014770508 = 1.9331157207489014 + 1.0 * 8.373496055603027
Epoch 10, val loss: 1.9404940605163574
Epoch 20, training loss: 10.29123306274414 = 1.9207289218902588 + 1.0 * 8.370504379272461
Epoch 20, val loss: 1.9284543991088867
Epoch 30, training loss: 10.249300956726074 = 1.9029796123504639 + 1.0 * 8.346321105957031
Epoch 30, val loss: 1.9109506607055664
Epoch 40, training loss: 10.058175086975098 = 1.879734992980957 + 1.0 * 8.17844009399414
Epoch 40, val loss: 1.8891088962554932
Epoch 50, training loss: 9.452648162841797 = 1.8547929525375366 + 1.0 * 7.597855091094971
Epoch 50, val loss: 1.8666915893554688
Epoch 60, training loss: 9.106452941894531 = 1.8345082998275757 + 1.0 * 7.271944522857666
Epoch 60, val loss: 1.8494513034820557
Epoch 70, training loss: 8.673477172851562 = 1.8202717304229736 + 1.0 * 6.85320520401001
Epoch 70, val loss: 1.8367328643798828
Epoch 80, training loss: 8.434691429138184 = 1.808650255203247 + 1.0 * 6.626040935516357
Epoch 80, val loss: 1.8256137371063232
Epoch 90, training loss: 8.341997146606445 = 1.79358971118927 + 1.0 * 6.548407077789307
Epoch 90, val loss: 1.8112136125564575
Epoch 100, training loss: 8.272753715515137 = 1.7761662006378174 + 1.0 * 6.49658727645874
Epoch 100, val loss: 1.7951889038085938
Epoch 110, training loss: 8.200787544250488 = 1.7586233615875244 + 1.0 * 6.442164421081543
Epoch 110, val loss: 1.7791717052459717
Epoch 120, training loss: 8.129045486450195 = 1.7411255836486816 + 1.0 * 6.387919902801514
Epoch 120, val loss: 1.7630666494369507
Epoch 130, training loss: 8.065688133239746 = 1.7217689752578735 + 1.0 * 6.343919277191162
Epoch 130, val loss: 1.7458202838897705
Epoch 140, training loss: 8.008788108825684 = 1.6989264488220215 + 1.0 * 6.309861660003662
Epoch 140, val loss: 1.7262861728668213
Epoch 150, training loss: 7.954676628112793 = 1.6714667081832886 + 1.0 * 6.283209800720215
Epoch 150, val loss: 1.7031158208847046
Epoch 160, training loss: 7.902432918548584 = 1.638476014137268 + 1.0 * 6.2639570236206055
Epoch 160, val loss: 1.675445795059204
Epoch 170, training loss: 7.846551895141602 = 1.6001472473144531 + 1.0 * 6.246404647827148
Epoch 170, val loss: 1.6430556774139404
Epoch 180, training loss: 7.78676700592041 = 1.5554492473602295 + 1.0 * 6.23131799697876
Epoch 180, val loss: 1.60524320602417
Epoch 190, training loss: 7.722373962402344 = 1.5040948390960693 + 1.0 * 6.2182793617248535
Epoch 190, val loss: 1.5619456768035889
Epoch 200, training loss: 7.658897399902344 = 1.4464107751846313 + 1.0 * 6.212486743927002
Epoch 200, val loss: 1.5136401653289795
Epoch 210, training loss: 7.583468437194824 = 1.385507345199585 + 1.0 * 6.197961330413818
Epoch 210, val loss: 1.4632859230041504
Epoch 220, training loss: 7.511953353881836 = 1.322951078414917 + 1.0 * 6.18900203704834
Epoch 220, val loss: 1.4120898246765137
Epoch 230, training loss: 7.441890716552734 = 1.2606157064437866 + 1.0 * 6.181274890899658
Epoch 230, val loss: 1.361905574798584
Epoch 240, training loss: 7.3752241134643555 = 1.200899600982666 + 1.0 * 6.1743245124816895
Epoch 240, val loss: 1.3145352602005005
Epoch 250, training loss: 7.311602592468262 = 1.1442644596099854 + 1.0 * 6.167337894439697
Epoch 250, val loss: 1.2703948020935059
Epoch 260, training loss: 7.252745628356934 = 1.091045618057251 + 1.0 * 6.161700248718262
Epoch 260, val loss: 1.2298399209976196
Epoch 270, training loss: 7.198246002197266 = 1.0417389869689941 + 1.0 * 6.1565070152282715
Epoch 270, val loss: 1.1931898593902588
Epoch 280, training loss: 7.145832538604736 = 0.9957674741744995 + 1.0 * 6.150064945220947
Epoch 280, val loss: 1.1596800088882446
Epoch 290, training loss: 7.098881244659424 = 0.952311635017395 + 1.0 * 6.146569728851318
Epoch 290, val loss: 1.1284953355789185
Epoch 300, training loss: 7.053321361541748 = 0.9113555550575256 + 1.0 * 6.141965866088867
Epoch 300, val loss: 1.099621057510376
Epoch 310, training loss: 7.0088114738464355 = 0.872447669506073 + 1.0 * 6.136363983154297
Epoch 310, val loss: 1.0726947784423828
Epoch 320, training loss: 6.967144012451172 = 0.8348639607429504 + 1.0 * 6.132279872894287
Epoch 320, val loss: 1.0470143556594849
Epoch 330, training loss: 6.931478977203369 = 0.7987380623817444 + 1.0 * 6.1327409744262695
Epoch 330, val loss: 1.0226613283157349
Epoch 340, training loss: 6.888808727264404 = 0.763946533203125 + 1.0 * 6.124862194061279
Epoch 340, val loss: 0.9997416138648987
Epoch 350, training loss: 6.849980354309082 = 0.7302067279815674 + 1.0 * 6.119773864746094
Epoch 350, val loss: 0.9777182340621948
Epoch 360, training loss: 6.813015937805176 = 0.6971682906150818 + 1.0 * 6.115847587585449
Epoch 360, val loss: 0.9565149545669556
Epoch 370, training loss: 6.778419494628906 = 0.6649144887924194 + 1.0 * 6.113504886627197
Epoch 370, val loss: 0.9360840320587158
Epoch 380, training loss: 6.746702194213867 = 0.6336591839790344 + 1.0 * 6.113042831420898
Epoch 380, val loss: 0.9165564179420471
Epoch 390, training loss: 6.713127136230469 = 0.603222668170929 + 1.0 * 6.1099042892456055
Epoch 390, val loss: 0.8978713154792786
Epoch 400, training loss: 6.6788153648376465 = 0.5736215710639954 + 1.0 * 6.105193614959717
Epoch 400, val loss: 0.8798822164535522
Epoch 410, training loss: 6.6468939781188965 = 0.5446252226829529 + 1.0 * 6.102268695831299
Epoch 410, val loss: 0.8626224398612976
Epoch 420, training loss: 6.616655349731445 = 0.5164260268211365 + 1.0 * 6.100229263305664
Epoch 420, val loss: 0.8459962606430054
Epoch 430, training loss: 6.591489791870117 = 0.4891951084136963 + 1.0 * 6.102294921875
Epoch 430, val loss: 0.8302575945854187
Epoch 440, training loss: 6.55921745300293 = 0.46287423372268677 + 1.0 * 6.096343040466309
Epoch 440, val loss: 0.8154100179672241
Epoch 450, training loss: 6.531917095184326 = 0.43744373321533203 + 1.0 * 6.094473361968994
Epoch 450, val loss: 0.8014216423034668
Epoch 460, training loss: 6.512008190155029 = 0.4130181074142456 + 1.0 * 6.098989963531494
Epoch 460, val loss: 0.788441002368927
Epoch 470, training loss: 6.481192588806152 = 0.38991713523864746 + 1.0 * 6.091275691986084
Epoch 470, val loss: 0.7765302658081055
Epoch 480, training loss: 6.457683086395264 = 0.36797478795051575 + 1.0 * 6.08970832824707
Epoch 480, val loss: 0.7656797170639038
Epoch 490, training loss: 6.434150695800781 = 0.3471497893333435 + 1.0 * 6.087000846862793
Epoch 490, val loss: 0.7559231519699097
Epoch 500, training loss: 6.411835670471191 = 0.32740044593811035 + 1.0 * 6.084434986114502
Epoch 500, val loss: 0.7471882700920105
Epoch 510, training loss: 6.391261100769043 = 0.30879443883895874 + 1.0 * 6.0824666023254395
Epoch 510, val loss: 0.7394510507583618
Epoch 520, training loss: 6.372328281402588 = 0.2913456857204437 + 1.0 * 6.080982685089111
Epoch 520, val loss: 0.732751190662384
Epoch 530, training loss: 6.357885360717773 = 0.27484560012817383 + 1.0 * 6.0830397605896
Epoch 530, val loss: 0.7270318269729614
Epoch 540, training loss: 6.344208717346191 = 0.25932803750038147 + 1.0 * 6.084880828857422
Epoch 540, val loss: 0.7221193313598633
Epoch 550, training loss: 6.322689533233643 = 0.24466370046138763 + 1.0 * 6.078025817871094
Epoch 550, val loss: 0.7181147933006287
Epoch 560, training loss: 6.311416149139404 = 0.2307652235031128 + 1.0 * 6.080650806427002
Epoch 560, val loss: 0.7149052619934082
Epoch 570, training loss: 6.292061805725098 = 0.2175927311182022 + 1.0 * 6.074469089508057
Epoch 570, val loss: 0.7124819159507751
Epoch 580, training loss: 6.285572528839111 = 0.2051287144422531 + 1.0 * 6.080443859100342
Epoch 580, val loss: 0.7107913494110107
Epoch 590, training loss: 6.266691207885742 = 0.1933722048997879 + 1.0 * 6.073318958282471
Epoch 590, val loss: 0.7097591161727905
Epoch 600, training loss: 6.25720739364624 = 0.18226194381713867 + 1.0 * 6.074945449829102
Epoch 600, val loss: 0.7094303369522095
Epoch 610, training loss: 6.24197244644165 = 0.1718309074640274 + 1.0 * 6.070141315460205
Epoch 610, val loss: 0.7096894979476929
Epoch 620, training loss: 6.22904109954834 = 0.16197364032268524 + 1.0 * 6.067067623138428
Epoch 620, val loss: 0.7105881571769714
Epoch 630, training loss: 6.226539134979248 = 0.1526971161365509 + 1.0 * 6.0738420486450195
Epoch 630, val loss: 0.7121151685714722
Epoch 640, training loss: 6.212554931640625 = 0.14401936531066895 + 1.0 * 6.068535804748535
Epoch 640, val loss: 0.7141252160072327
Epoch 650, training loss: 6.201114654541016 = 0.1359000951051712 + 1.0 * 6.06521463394165
Epoch 650, val loss: 0.7165963649749756
Epoch 660, training loss: 6.194972038269043 = 0.12829415500164032 + 1.0 * 6.066678047180176
Epoch 660, val loss: 0.7195031046867371
Epoch 670, training loss: 6.1865153312683105 = 0.12119219452142715 + 1.0 * 6.065323352813721
Epoch 670, val loss: 0.7228990197181702
Epoch 680, training loss: 6.178268909454346 = 0.11453687399625778 + 1.0 * 6.063732147216797
Epoch 680, val loss: 0.7266899347305298
Epoch 690, training loss: 6.170769214630127 = 0.10831388831138611 + 1.0 * 6.062455177307129
Epoch 690, val loss: 0.7307932376861572
Epoch 700, training loss: 6.16072416305542 = 0.10249031335115433 + 1.0 * 6.058233737945557
Epoch 700, val loss: 0.7352648973464966
Epoch 710, training loss: 6.1590895652771 = 0.09703686088323593 + 1.0 * 6.0620527267456055
Epoch 710, val loss: 0.7400559186935425
Epoch 720, training loss: 6.1492509841918945 = 0.09193447232246399 + 1.0 * 6.057316303253174
Epoch 720, val loss: 0.74512779712677
Epoch 730, training loss: 6.145674705505371 = 0.08714444190263748 + 1.0 * 6.058530330657959
Epoch 730, val loss: 0.7504406571388245
Epoch 740, training loss: 6.138101100921631 = 0.0826629251241684 + 1.0 * 6.055438041687012
Epoch 740, val loss: 0.7559633255004883
Epoch 750, training loss: 6.134572982788086 = 0.07845064252614975 + 1.0 * 6.056122303009033
Epoch 750, val loss: 0.7617258429527283
Epoch 760, training loss: 6.126898765563965 = 0.07451354712247849 + 1.0 * 6.052385330200195
Epoch 760, val loss: 0.7676486372947693
Epoch 770, training loss: 6.123163223266602 = 0.0708012729883194 + 1.0 * 6.052361965179443
Epoch 770, val loss: 0.7737221121788025
Epoch 780, training loss: 6.12146520614624 = 0.06730513274669647 + 1.0 * 6.054160118103027
Epoch 780, val loss: 0.7799866795539856
Epoch 790, training loss: 6.117942810058594 = 0.06402587890625 + 1.0 * 6.053916931152344
Epoch 790, val loss: 0.7863970994949341
Epoch 800, training loss: 6.11548376083374 = 0.060939397662878036 + 1.0 * 6.054544448852539
Epoch 800, val loss: 0.7928245067596436
Epoch 810, training loss: 6.108883857727051 = 0.058049675077199936 + 1.0 * 6.0508341789245605
Epoch 810, val loss: 0.7994258999824524
Epoch 820, training loss: 6.101230144500732 = 0.055341050028800964 + 1.0 * 6.045888900756836
Epoch 820, val loss: 0.8060135245323181
Epoch 830, training loss: 6.097456932067871 = 0.05278656631708145 + 1.0 * 6.044670581817627
Epoch 830, val loss: 0.8127197027206421
Epoch 840, training loss: 6.1044416427612305 = 0.0503837876021862 + 1.0 * 6.054058074951172
Epoch 840, val loss: 0.8195383548736572
Epoch 850, training loss: 6.094990253448486 = 0.04812515527009964 + 1.0 * 6.046864986419678
Epoch 850, val loss: 0.8263567090034485
Epoch 860, training loss: 6.089846611022949 = 0.04601598531007767 + 1.0 * 6.043830394744873
Epoch 860, val loss: 0.8331254720687866
Epoch 870, training loss: 6.093375205993652 = 0.04402672126889229 + 1.0 * 6.0493483543396
Epoch 870, val loss: 0.8398987054824829
Epoch 880, training loss: 6.087702751159668 = 0.04216308891773224 + 1.0 * 6.045539855957031
Epoch 880, val loss: 0.8467061519622803
Epoch 890, training loss: 6.08407735824585 = 0.04041207209229469 + 1.0 * 6.043665409088135
Epoch 890, val loss: 0.8534670472145081
Epoch 900, training loss: 6.079169750213623 = 0.03876414895057678 + 1.0 * 6.040405750274658
Epoch 900, val loss: 0.8600771427154541
Epoch 910, training loss: 6.079362869262695 = 0.037217166274785995 + 1.0 * 6.042145729064941
Epoch 910, val loss: 0.8667541146278381
Epoch 920, training loss: 6.074004650115967 = 0.035752806812524796 + 1.0 * 6.038251876831055
Epoch 920, val loss: 0.8733041882514954
Epoch 930, training loss: 6.073275089263916 = 0.03438040614128113 + 1.0 * 6.0388946533203125
Epoch 930, val loss: 0.8798072338104248
Epoch 940, training loss: 6.070472240447998 = 0.03307894989848137 + 1.0 * 6.037393093109131
Epoch 940, val loss: 0.8862238526344299
Epoch 950, training loss: 6.071478366851807 = 0.031848713755607605 + 1.0 * 6.0396294593811035
Epoch 950, val loss: 0.8926787972450256
Epoch 960, training loss: 6.066210746765137 = 0.030692083761096 + 1.0 * 6.035518646240234
Epoch 960, val loss: 0.8989830613136292
Epoch 970, training loss: 6.070401191711426 = 0.02959231473505497 + 1.0 * 6.04080867767334
Epoch 970, val loss: 0.9051321148872375
Epoch 980, training loss: 6.0649566650390625 = 0.028556838631629944 + 1.0 * 6.036399841308594
Epoch 980, val loss: 0.9113082885742188
Epoch 990, training loss: 6.062988758087158 = 0.02757016196846962 + 1.0 * 6.035418510437012
Epoch 990, val loss: 0.9173871874809265
Epoch 1000, training loss: 6.061087608337402 = 0.026638062670826912 + 1.0 * 6.034449577331543
Epoch 1000, val loss: 0.9233682751655579
Epoch 1010, training loss: 6.058152198791504 = 0.02574964240193367 + 1.0 * 6.032402515411377
Epoch 1010, val loss: 0.9293496608734131
Epoch 1020, training loss: 6.056093215942383 = 0.024905849248170853 + 1.0 * 6.031187534332275
Epoch 1020, val loss: 0.9351546764373779
Epoch 1030, training loss: 6.056107521057129 = 0.024102086201310158 + 1.0 * 6.032005310058594
Epoch 1030, val loss: 0.9409675598144531
Epoch 1040, training loss: 6.0565643310546875 = 0.023333696648478508 + 1.0 * 6.033230781555176
Epoch 1040, val loss: 0.9467718601226807
Epoch 1050, training loss: 6.051951885223389 = 0.022606533020734787 + 1.0 * 6.029345512390137
Epoch 1050, val loss: 0.9524142742156982
Epoch 1060, training loss: 6.055219650268555 = 0.02191210724413395 + 1.0 * 6.0333075523376465
Epoch 1060, val loss: 0.9578891396522522
Epoch 1070, training loss: 6.051213264465332 = 0.021250005811452866 + 1.0 * 6.029963493347168
Epoch 1070, val loss: 0.9635004997253418
Epoch 1080, training loss: 6.04833459854126 = 0.020620007067918777 + 1.0 * 6.027714729309082
Epoch 1080, val loss: 0.9689222574234009
Epoch 1090, training loss: 6.049594402313232 = 0.020015627145767212 + 1.0 * 6.029578685760498
Epoch 1090, val loss: 0.9743463397026062
Epoch 1100, training loss: 6.049010753631592 = 0.01943613588809967 + 1.0 * 6.029574394226074
Epoch 1100, val loss: 0.9797288775444031
Epoch 1110, training loss: 6.046278953552246 = 0.018885426223278046 + 1.0 * 6.027393341064453
Epoch 1110, val loss: 0.9849888682365417
Epoch 1120, training loss: 6.046475410461426 = 0.018358903005719185 + 1.0 * 6.028116703033447
Epoch 1120, val loss: 0.9900828003883362
Epoch 1130, training loss: 6.044519901275635 = 0.01785425841808319 + 1.0 * 6.026665687561035
Epoch 1130, val loss: 0.9952247738838196
Epoch 1140, training loss: 6.043038845062256 = 0.017369987443089485 + 1.0 * 6.025668621063232
Epoch 1140, val loss: 1.0002937316894531
Epoch 1150, training loss: 6.05719518661499 = 0.016903480514883995 + 1.0 * 6.040291786193848
Epoch 1150, val loss: 1.0052326917648315
Epoch 1160, training loss: 6.044501781463623 = 0.016463402658700943 + 1.0 * 6.028038501739502
Epoch 1160, val loss: 1.0102699995040894
Epoch 1170, training loss: 6.039804458618164 = 0.016039729118347168 + 1.0 * 6.023764610290527
Epoch 1170, val loss: 1.0149563550949097
Epoch 1180, training loss: 6.0380330085754395 = 0.015632102265954018 + 1.0 * 6.022400856018066
Epoch 1180, val loss: 1.019639015197754
Epoch 1190, training loss: 6.050621509552002 = 0.015240589156746864 + 1.0 * 6.035380840301514
Epoch 1190, val loss: 1.0244388580322266
Epoch 1200, training loss: 6.040045738220215 = 0.014860551804304123 + 1.0 * 6.0251851081848145
Epoch 1200, val loss: 1.0292646884918213
Epoch 1210, training loss: 6.036489009857178 = 0.01449939887970686 + 1.0 * 6.021989822387695
Epoch 1210, val loss: 1.0337250232696533
Epoch 1220, training loss: 6.035006523132324 = 0.014148383401334286 + 1.0 * 6.020858287811279
Epoch 1220, val loss: 1.0383037328720093
Epoch 1230, training loss: 6.0533952713012695 = 0.013811861164867878 + 1.0 * 6.039583206176758
Epoch 1230, val loss: 1.0429000854492188
Epoch 1240, training loss: 6.0344696044921875 = 0.013486293144524097 + 1.0 * 6.0209832191467285
Epoch 1240, val loss: 1.0474801063537598
Epoch 1250, training loss: 6.0320539474487305 = 0.013175570406019688 + 1.0 * 6.01887845993042
Epoch 1250, val loss: 1.051697850227356
Epoch 1260, training loss: 6.032247066497803 = 0.012874914333224297 + 1.0 * 6.01937198638916
Epoch 1260, val loss: 1.0559768676757812
Epoch 1270, training loss: 6.042403697967529 = 0.01258239708840847 + 1.0 * 6.029821395874023
Epoch 1270, val loss: 1.0603474378585815
Epoch 1280, training loss: 6.033228397369385 = 0.012302618473768234 + 1.0 * 6.020925998687744
Epoch 1280, val loss: 1.0647319555282593
Epoch 1290, training loss: 6.03481912612915 = 0.01203172281384468 + 1.0 * 6.022787570953369
Epoch 1290, val loss: 1.068839430809021
Epoch 1300, training loss: 6.02866792678833 = 0.011771132238209248 + 1.0 * 6.016896724700928
Epoch 1300, val loss: 1.0729900598526
Epoch 1310, training loss: 6.029851913452148 = 0.011519440449774265 + 1.0 * 6.018332481384277
Epoch 1310, val loss: 1.0771071910858154
Epoch 1320, training loss: 6.031371116638184 = 0.011274849995970726 + 1.0 * 6.020096302032471
Epoch 1320, val loss: 1.081130862236023
Epoch 1330, training loss: 6.028100967407227 = 0.011040394194424152 + 1.0 * 6.01706075668335
Epoch 1330, val loss: 1.0852069854736328
Epoch 1340, training loss: 6.02742862701416 = 0.010810788720846176 + 1.0 * 6.016617774963379
Epoch 1340, val loss: 1.0891401767730713
Epoch 1350, training loss: 6.040614604949951 = 0.01058956515043974 + 1.0 * 6.030025005340576
Epoch 1350, val loss: 1.0932408571243286
Epoch 1360, training loss: 6.026364326477051 = 0.01037866435945034 + 1.0 * 6.015985488891602
Epoch 1360, val loss: 1.0970542430877686
Epoch 1370, training loss: 6.025990009307861 = 0.010175304487347603 + 1.0 * 6.015814781188965
Epoch 1370, val loss: 1.1006954908370972
Epoch 1380, training loss: 6.023678302764893 = 0.009975826367735863 + 1.0 * 6.013702392578125
Epoch 1380, val loss: 1.104390263557434
Epoch 1390, training loss: 6.023728370666504 = 0.0097816726192832 + 1.0 * 6.013946533203125
Epoch 1390, val loss: 1.1082013845443726
Epoch 1400, training loss: 6.032202243804932 = 0.009592531248927116 + 1.0 * 6.022609710693359
Epoch 1400, val loss: 1.1120628118515015
Epoch 1410, training loss: 6.025119781494141 = 0.009410299360752106 + 1.0 * 6.015709400177002
Epoch 1410, val loss: 1.1157405376434326
Epoch 1420, training loss: 6.023812294006348 = 0.009233715012669563 + 1.0 * 6.014578342437744
Epoch 1420, val loss: 1.1193292140960693
Epoch 1430, training loss: 6.029865741729736 = 0.009061873890459538 + 1.0 * 6.020803928375244
Epoch 1430, val loss: 1.1229331493377686
Epoch 1440, training loss: 6.0245137214660645 = 0.008897742256522179 + 1.0 * 6.015615940093994
Epoch 1440, val loss: 1.1265491247177124
Epoch 1450, training loss: 6.020533084869385 = 0.008736505173146725 + 1.0 * 6.011796474456787
Epoch 1450, val loss: 1.129948377609253
Epoch 1460, training loss: 6.023439884185791 = 0.00858070608228445 + 1.0 * 6.014859199523926
Epoch 1460, val loss: 1.1334244012832642
Epoch 1470, training loss: 6.020077705383301 = 0.008427857421338558 + 1.0 * 6.011650085449219
Epoch 1470, val loss: 1.1369552612304688
Epoch 1480, training loss: 6.0192766189575195 = 0.008280172944068909 + 1.0 * 6.010996341705322
Epoch 1480, val loss: 1.1403545141220093
Epoch 1490, training loss: 6.019263744354248 = 0.008136915042996407 + 1.0 * 6.01112699508667
Epoch 1490, val loss: 1.1436808109283447
Epoch 1500, training loss: 6.021640777587891 = 0.007996875792741776 + 1.0 * 6.013643741607666
Epoch 1500, val loss: 1.1470857858657837
Epoch 1510, training loss: 6.017859935760498 = 0.007860003970563412 + 1.0 * 6.009999752044678
Epoch 1510, val loss: 1.150428295135498
Epoch 1520, training loss: 6.028007507324219 = 0.007727191783487797 + 1.0 * 6.020280361175537
Epoch 1520, val loss: 1.1537572145462036
Epoch 1530, training loss: 6.020791053771973 = 0.00759986974298954 + 1.0 * 6.013191223144531
Epoch 1530, val loss: 1.1570416688919067
Epoch 1540, training loss: 6.016778945922852 = 0.007476171012967825 + 1.0 * 6.009302616119385
Epoch 1540, val loss: 1.1601808071136475
Epoch 1550, training loss: 6.015941143035889 = 0.007355646230280399 + 1.0 * 6.008585453033447
Epoch 1550, val loss: 1.1631888151168823
Epoch 1560, training loss: 6.016827583312988 = 0.007237096782773733 + 1.0 * 6.0095906257629395
Epoch 1560, val loss: 1.166377305984497
Epoch 1570, training loss: 6.020724773406982 = 0.007121258415281773 + 1.0 * 6.013603687286377
Epoch 1570, val loss: 1.1695880889892578
Epoch 1580, training loss: 6.017563819885254 = 0.007009180262684822 + 1.0 * 6.010554790496826
Epoch 1580, val loss: 1.1726813316345215
Epoch 1590, training loss: 6.020412445068359 = 0.0068998513743281364 + 1.0 * 6.01351261138916
Epoch 1590, val loss: 1.1756422519683838
Epoch 1600, training loss: 6.015283107757568 = 0.006792877800762653 + 1.0 * 6.008490085601807
Epoch 1600, val loss: 1.1787152290344238
Epoch 1610, training loss: 6.015661716461182 = 0.006688482128083706 + 1.0 * 6.008973121643066
Epoch 1610, val loss: 1.1816699504852295
Epoch 1620, training loss: 6.0170512199401855 = 0.006587071344256401 + 1.0 * 6.010464191436768
Epoch 1620, val loss: 1.184586524963379
Epoch 1630, training loss: 6.015651702880859 = 0.006488319952040911 + 1.0 * 6.0091633796691895
Epoch 1630, val loss: 1.187656283378601
Epoch 1640, training loss: 6.014170169830322 = 0.0063918158411979675 + 1.0 * 6.007778167724609
Epoch 1640, val loss: 1.1905272006988525
Epoch 1650, training loss: 6.012103080749512 = 0.0062980749644339085 + 1.0 * 6.005805015563965
Epoch 1650, val loss: 1.1932841539382935
Epoch 1660, training loss: 6.012074947357178 = 0.006206475663930178 + 1.0 * 6.005868434906006
Epoch 1660, val loss: 1.1960906982421875
Epoch 1670, training loss: 6.023149490356445 = 0.006116366013884544 + 1.0 * 6.017033100128174
Epoch 1670, val loss: 1.198932409286499
Epoch 1680, training loss: 6.0152268409729 = 0.0060281772166490555 + 1.0 * 6.0091986656188965
Epoch 1680, val loss: 1.2019240856170654
Epoch 1690, training loss: 6.012099742889404 = 0.005943405907601118 + 1.0 * 6.0061564445495605
Epoch 1690, val loss: 1.2045003175735474
Epoch 1700, training loss: 6.011674404144287 = 0.0058599659241735935 + 1.0 * 6.005814552307129
Epoch 1700, val loss: 1.207235336303711
Epoch 1710, training loss: 6.015220642089844 = 0.005777665879577398 + 1.0 * 6.0094428062438965
Epoch 1710, val loss: 1.209962248802185
Epoch 1720, training loss: 6.015460968017578 = 0.005698371212929487 + 1.0 * 6.009762763977051
Epoch 1720, val loss: 1.2127233743667603
Epoch 1730, training loss: 6.011405944824219 = 0.005620437674224377 + 1.0 * 6.0057854652404785
Epoch 1730, val loss: 1.2153509855270386
Epoch 1740, training loss: 6.008887767791748 = 0.005544775631278753 + 1.0 * 6.003343105316162
Epoch 1740, val loss: 1.2179495096206665
Epoch 1750, training loss: 6.009662628173828 = 0.005470580887049437 + 1.0 * 6.004191875457764
Epoch 1750, val loss: 1.2205239534378052
Epoch 1760, training loss: 6.0116682052612305 = 0.005397352389991283 + 1.0 * 6.006270885467529
Epoch 1760, val loss: 1.2231404781341553
Epoch 1770, training loss: 6.010787487030029 = 0.00532611133530736 + 1.0 * 6.0054612159729
Epoch 1770, val loss: 1.225752830505371
Epoch 1780, training loss: 6.011724472045898 = 0.005256214644759893 + 1.0 * 6.006468296051025
Epoch 1780, val loss: 1.228362798690796
Epoch 1790, training loss: 6.012158393859863 = 0.005187939386814833 + 1.0 * 6.006970405578613
Epoch 1790, val loss: 1.2308717966079712
Epoch 1800, training loss: 6.008502006530762 = 0.005121641326695681 + 1.0 * 6.003380298614502
Epoch 1800, val loss: 1.2332819700241089
Epoch 1810, training loss: 6.009712219238281 = 0.005056989844888449 + 1.0 * 6.004655361175537
Epoch 1810, val loss: 1.2357476949691772
Epoch 1820, training loss: 6.010587215423584 = 0.004992911592125893 + 1.0 * 6.005594253540039
Epoch 1820, val loss: 1.2381725311279297
Epoch 1830, training loss: 6.010612964630127 = 0.004930426366627216 + 1.0 * 6.005682468414307
Epoch 1830, val loss: 1.2405617237091064
Epoch 1840, training loss: 6.00697660446167 = 0.004870031028985977 + 1.0 * 6.002106666564941
Epoch 1840, val loss: 1.2429556846618652
Epoch 1850, training loss: 6.006080627441406 = 0.004809957463294268 + 1.0 * 6.001270771026611
Epoch 1850, val loss: 1.2452702522277832
Epoch 1860, training loss: 6.008884906768799 = 0.004750614520162344 + 1.0 * 6.004134178161621
Epoch 1860, val loss: 1.2476412057876587
Epoch 1870, training loss: 6.008029937744141 = 0.0046925852075219154 + 1.0 * 6.003337383270264
Epoch 1870, val loss: 1.2500250339508057
Epoch 1880, training loss: 6.0080180168151855 = 0.004635779187083244 + 1.0 * 6.003382205963135
Epoch 1880, val loss: 1.252307415008545
Epoch 1890, training loss: 6.010892391204834 = 0.004580340348184109 + 1.0 * 6.006311893463135
Epoch 1890, val loss: 1.2546402215957642
Epoch 1900, training loss: 6.009181022644043 = 0.004526828415691853 + 1.0 * 6.004654407501221
Epoch 1900, val loss: 1.256961464881897
Epoch 1910, training loss: 6.011048793792725 = 0.0044740899465978146 + 1.0 * 6.006574630737305
Epoch 1910, val loss: 1.2590630054473877
Epoch 1920, training loss: 6.003843784332275 = 0.004422480706125498 + 1.0 * 5.999421119689941
Epoch 1920, val loss: 1.261335015296936
Epoch 1930, training loss: 6.005226135253906 = 0.004371412564069033 + 1.0 * 6.0008544921875
Epoch 1930, val loss: 1.2634518146514893
Epoch 1940, training loss: 6.011838436126709 = 0.004321024753153324 + 1.0 * 6.007517337799072
Epoch 1940, val loss: 1.2657045125961304
Epoch 1950, training loss: 6.005842208862305 = 0.004272495396435261 + 1.0 * 6.001569747924805
Epoch 1950, val loss: 1.2679351568222046
Epoch 1960, training loss: 6.0039753913879395 = 0.004224090371280909 + 1.0 * 5.999751091003418
Epoch 1960, val loss: 1.2700200080871582
Epoch 1970, training loss: 6.007468223571777 = 0.004176647402346134 + 1.0 * 6.003291606903076
Epoch 1970, val loss: 1.2721542119979858
Epoch 1980, training loss: 6.002230167388916 = 0.004130322486162186 + 1.0 * 5.9980998039245605
Epoch 1980, val loss: 1.2742887735366821
Epoch 1990, training loss: 6.008378028869629 = 0.004084741696715355 + 1.0 * 6.004293441772461
Epoch 1990, val loss: 1.2763217687606812
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.4871
Flip ASR: 0.3867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.323280334472656 = 1.9493622779846191 + 1.0 * 8.373918533325195
Epoch 0, val loss: 1.9444948434829712
Epoch 10, training loss: 10.313175201416016 = 1.9395251274108887 + 1.0 * 8.373649597167969
Epoch 10, val loss: 1.9351463317871094
Epoch 20, training loss: 10.299025535583496 = 1.9274835586547852 + 1.0 * 8.371541976928711
Epoch 20, val loss: 1.9230167865753174
Epoch 30, training loss: 10.264961242675781 = 1.910799264907837 + 1.0 * 8.354162216186523
Epoch 30, val loss: 1.9057886600494385
Epoch 40, training loss: 10.130088806152344 = 1.888319969177246 + 1.0 * 8.241768836975098
Epoch 40, val loss: 1.883315086364746
Epoch 50, training loss: 9.636268615722656 = 1.8629857301712036 + 1.0 * 7.773283004760742
Epoch 50, val loss: 1.859263300895691
Epoch 60, training loss: 9.180780410766602 = 1.8418470621109009 + 1.0 * 7.338932991027832
Epoch 60, val loss: 1.8401256799697876
Epoch 70, training loss: 8.76185417175293 = 1.8260917663574219 + 1.0 * 6.93576192855835
Epoch 70, val loss: 1.824750304222107
Epoch 80, training loss: 8.525093078613281 = 1.8089642524719238 + 1.0 * 6.716128826141357
Epoch 80, val loss: 1.8077844381332397
Epoch 90, training loss: 8.416641235351562 = 1.790081262588501 + 1.0 * 6.626560211181641
Epoch 90, val loss: 1.7893083095550537
Epoch 100, training loss: 8.326909065246582 = 1.7703040838241577 + 1.0 * 6.556605339050293
Epoch 100, val loss: 1.7706830501556396
Epoch 110, training loss: 8.242331504821777 = 1.7509469985961914 + 1.0 * 6.491384506225586
Epoch 110, val loss: 1.7528772354125977
Epoch 120, training loss: 8.176774024963379 = 1.730698823928833 + 1.0 * 6.446074962615967
Epoch 120, val loss: 1.7342356443405151
Epoch 130, training loss: 8.10847282409668 = 1.7083284854888916 + 1.0 * 6.400144100189209
Epoch 130, val loss: 1.71387779712677
Epoch 140, training loss: 8.042478561401367 = 1.6828954219818115 + 1.0 * 6.359582901000977
Epoch 140, val loss: 1.6909584999084473
Epoch 150, training loss: 7.984036922454834 = 1.6532100439071655 + 1.0 * 6.330826759338379
Epoch 150, val loss: 1.6648645401000977
Epoch 160, training loss: 7.922102451324463 = 1.619179606437683 + 1.0 * 6.30292272567749
Epoch 160, val loss: 1.6353868246078491
Epoch 170, training loss: 7.859199047088623 = 1.5795856714248657 + 1.0 * 6.279613494873047
Epoch 170, val loss: 1.6011896133422852
Epoch 180, training loss: 7.794224739074707 = 1.5334099531173706 + 1.0 * 6.260814666748047
Epoch 180, val loss: 1.5616422891616821
Epoch 190, training loss: 7.725623607635498 = 1.4799789190292358 + 1.0 * 6.245644569396973
Epoch 190, val loss: 1.5163732767105103
Epoch 200, training loss: 7.653892517089844 = 1.4198639392852783 + 1.0 * 6.2340288162231445
Epoch 200, val loss: 1.466224193572998
Epoch 210, training loss: 7.579373836517334 = 1.355617880821228 + 1.0 * 6.223755836486816
Epoch 210, val loss: 1.4131593704223633
Epoch 220, training loss: 7.501051425933838 = 1.287873387336731 + 1.0 * 6.2131781578063965
Epoch 220, val loss: 1.3581422567367554
Epoch 230, training loss: 7.425699710845947 = 1.2193547487258911 + 1.0 * 6.206345081329346
Epoch 230, val loss: 1.3033554553985596
Epoch 240, training loss: 7.34834623336792 = 1.1520376205444336 + 1.0 * 6.196308612823486
Epoch 240, val loss: 1.250408411026001
Epoch 250, training loss: 7.275807857513428 = 1.0867804288864136 + 1.0 * 6.189027309417725
Epoch 250, val loss: 1.1996381282806396
Epoch 260, training loss: 7.206798553466797 = 1.0249526500701904 + 1.0 * 6.1818461418151855
Epoch 260, val loss: 1.152089238166809
Epoch 270, training loss: 7.1418867111206055 = 0.9677283763885498 + 1.0 * 6.174158573150635
Epoch 270, val loss: 1.108676791191101
Epoch 280, training loss: 7.092311859130859 = 0.9146076440811157 + 1.0 * 6.177704334259033
Epoch 280, val loss: 1.068763017654419
Epoch 290, training loss: 7.033352375030518 = 0.8665222525596619 + 1.0 * 6.166830062866211
Epoch 290, val loss: 1.0336261987686157
Epoch 300, training loss: 6.978766441345215 = 0.8223011493682861 + 1.0 * 6.15646505355835
Epoch 300, val loss: 1.0020124912261963
Epoch 310, training loss: 6.931208610534668 = 0.7812182903289795 + 1.0 * 6.149990081787109
Epoch 310, val loss: 0.9733546376228333
Epoch 320, training loss: 6.899759292602539 = 0.7427841424942017 + 1.0 * 6.156975269317627
Epoch 320, val loss: 0.9474040865898132
Epoch 330, training loss: 6.849027633666992 = 0.707360565662384 + 1.0 * 6.141666889190674
Epoch 330, val loss: 0.9245432615280151
Epoch 340, training loss: 6.811991214752197 = 0.6742783188819885 + 1.0 * 6.1377129554748535
Epoch 340, val loss: 0.904116153717041
Epoch 350, training loss: 6.7769317626953125 = 0.6433241963386536 + 1.0 * 6.133607387542725
Epoch 350, val loss: 0.8859128952026367
Epoch 360, training loss: 6.742774963378906 = 0.6143016219139099 + 1.0 * 6.128473281860352
Epoch 360, val loss: 0.8696504831314087
Epoch 370, training loss: 6.7247700691223145 = 0.5868402123451233 + 1.0 * 6.137929916381836
Epoch 370, val loss: 0.8552175760269165
Epoch 380, training loss: 6.685477256774902 = 0.5613703727722168 + 1.0 * 6.1241068840026855
Epoch 380, val loss: 0.8422918915748596
Epoch 390, training loss: 6.6566314697265625 = 0.5372335910797119 + 1.0 * 6.11939811706543
Epoch 390, val loss: 0.8308731913566589
Epoch 400, training loss: 6.629570007324219 = 0.5141834020614624 + 1.0 * 6.115386486053467
Epoch 400, val loss: 0.820760190486908
Epoch 410, training loss: 6.609879016876221 = 0.4921708405017853 + 1.0 * 6.117708206176758
Epoch 410, val loss: 0.8118331432342529
Epoch 420, training loss: 6.583512783050537 = 0.4714326858520508 + 1.0 * 6.112080097198486
Epoch 420, val loss: 0.8039878010749817
Epoch 430, training loss: 6.559316158294678 = 0.45150572061538696 + 1.0 * 6.1078104972839355
Epoch 430, val loss: 0.7971444129943848
Epoch 440, training loss: 6.5437164306640625 = 0.43226659297943115 + 1.0 * 6.111449718475342
Epoch 440, val loss: 0.7911834716796875
Epoch 450, training loss: 6.519560813903809 = 0.4136849343776703 + 1.0 * 6.1058759689331055
Epoch 450, val loss: 0.7862931489944458
Epoch 460, training loss: 6.496406555175781 = 0.39576420187950134 + 1.0 * 6.100642204284668
Epoch 460, val loss: 0.781783938407898
Epoch 470, training loss: 6.480533599853516 = 0.3782915472984314 + 1.0 * 6.1022419929504395
Epoch 470, val loss: 0.7780752778053284
Epoch 480, training loss: 6.4612507820129395 = 0.3614785075187683 + 1.0 * 6.0997724533081055
Epoch 480, val loss: 0.7749396562576294
Epoch 490, training loss: 6.439985752105713 = 0.34521210193634033 + 1.0 * 6.094773769378662
Epoch 490, val loss: 0.7722794413566589
Epoch 500, training loss: 6.433286666870117 = 0.32943758368492126 + 1.0 * 6.103848934173584
Epoch 500, val loss: 0.7701329588890076
Epoch 510, training loss: 6.4056596755981445 = 0.31419065594673157 + 1.0 * 6.091468811035156
Epoch 510, val loss: 0.7687123417854309
Epoch 520, training loss: 6.390076160430908 = 0.29950153827667236 + 1.0 * 6.090574741363525
Epoch 520, val loss: 0.7677029371261597
Epoch 530, training loss: 6.373260498046875 = 0.28530487418174744 + 1.0 * 6.087955474853516
Epoch 530, val loss: 0.7671181559562683
Epoch 540, training loss: 6.358138561248779 = 0.2715598940849304 + 1.0 * 6.086578845977783
Epoch 540, val loss: 0.7670959234237671
Epoch 550, training loss: 6.347882270812988 = 0.25831684470176697 + 1.0 * 6.089565277099609
Epoch 550, val loss: 0.7675533890724182
Epoch 560, training loss: 6.331349849700928 = 0.24564699828624725 + 1.0 * 6.085702896118164
Epoch 560, val loss: 0.7684289813041687
Epoch 570, training loss: 6.320781707763672 = 0.2335044890642166 + 1.0 * 6.087277412414551
Epoch 570, val loss: 0.7697410583496094
Epoch 580, training loss: 6.301124095916748 = 0.22190672159194946 + 1.0 * 6.079217433929443
Epoch 580, val loss: 0.7715619802474976
Epoch 590, training loss: 6.288575172424316 = 0.21082653105258942 + 1.0 * 6.077748775482178
Epoch 590, val loss: 0.7737798094749451
Epoch 600, training loss: 6.289133548736572 = 0.2002514898777008 + 1.0 * 6.088881969451904
Epoch 600, val loss: 0.7764927744865417
Epoch 610, training loss: 6.269887447357178 = 0.1903410255908966 + 1.0 * 6.0795464515686035
Epoch 610, val loss: 0.7794208526611328
Epoch 620, training loss: 6.254736423492432 = 0.18089817464351654 + 1.0 * 6.073838233947754
Epoch 620, val loss: 0.7827857732772827
Epoch 630, training loss: 6.244360446929932 = 0.1719328612089157 + 1.0 * 6.072427749633789
Epoch 630, val loss: 0.7866155505180359
Epoch 640, training loss: 6.244748115539551 = 0.16342800855636597 + 1.0 * 6.081320285797119
Epoch 640, val loss: 0.7908899188041687
Epoch 650, training loss: 6.227357387542725 = 0.15546607971191406 + 1.0 * 6.0718913078308105
Epoch 650, val loss: 0.7953711152076721
Epoch 660, training loss: 6.217043399810791 = 0.14795878529548645 + 1.0 * 6.069084644317627
Epoch 660, val loss: 0.8001180291175842
Epoch 670, training loss: 6.216974258422852 = 0.14085516333580017 + 1.0 * 6.0761189460754395
Epoch 670, val loss: 0.8052449822425842
Epoch 680, training loss: 6.203185081481934 = 0.1341473013162613 + 1.0 * 6.069037914276123
Epoch 680, val loss: 0.8106858134269714
Epoch 690, training loss: 6.196048736572266 = 0.12784986197948456 + 1.0 * 6.0681986808776855
Epoch 690, val loss: 0.8162310719490051
Epoch 700, training loss: 6.187385559082031 = 0.1219005212187767 + 1.0 * 6.065485000610352
Epoch 700, val loss: 0.8221223950386047
Epoch 710, training loss: 6.184657096862793 = 0.1163228377699852 + 1.0 * 6.068334102630615
Epoch 710, val loss: 0.8280108571052551
Epoch 720, training loss: 6.174638748168945 = 0.11104300618171692 + 1.0 * 6.063595771789551
Epoch 720, val loss: 0.834201455116272
Epoch 730, training loss: 6.167868137359619 = 0.10605896264314651 + 1.0 * 6.061809062957764
Epoch 730, val loss: 0.8404877185821533
Epoch 740, training loss: 6.169672966003418 = 0.1013270765542984 + 1.0 * 6.06834602355957
Epoch 740, val loss: 0.847012996673584
Epoch 750, training loss: 6.16294002532959 = 0.09689909219741821 + 1.0 * 6.066040992736816
Epoch 750, val loss: 0.8534611463546753
Epoch 760, training loss: 6.1517229080200195 = 0.09271617233753204 + 1.0 * 6.059006690979004
Epoch 760, val loss: 0.859899640083313
Epoch 770, training loss: 6.146751403808594 = 0.0887485221028328 + 1.0 * 6.058002948760986
Epoch 770, val loss: 0.8664721846580505
Epoch 780, training loss: 6.144232749938965 = 0.08497707545757294 + 1.0 * 6.059255599975586
Epoch 780, val loss: 0.8731693029403687
Epoch 790, training loss: 6.145897388458252 = 0.08141522854566574 + 1.0 * 6.06448221206665
Epoch 790, val loss: 0.8798388242721558
Epoch 800, training loss: 6.137429714202881 = 0.07804838567972183 + 1.0 * 6.059381484985352
Epoch 800, val loss: 0.8863940238952637
Epoch 810, training loss: 6.129331588745117 = 0.07485842704772949 + 1.0 * 6.054472923278809
Epoch 810, val loss: 0.8930042386054993
Epoch 820, training loss: 6.1260576248168945 = 0.07181756943464279 + 1.0 * 6.0542402267456055
Epoch 820, val loss: 0.8997481465339661
Epoch 830, training loss: 6.12633752822876 = 0.068930983543396 + 1.0 * 6.057406425476074
Epoch 830, val loss: 0.906462550163269
Epoch 840, training loss: 6.118074417114258 = 0.06620527803897858 + 1.0 * 6.051868915557861
Epoch 840, val loss: 0.9129842519760132
Epoch 850, training loss: 6.11520528793335 = 0.06360089033842087 + 1.0 * 6.051604270935059
Epoch 850, val loss: 0.9195879697799683
Epoch 860, training loss: 6.120060920715332 = 0.061129916459321976 + 1.0 * 6.05893087387085
Epoch 860, val loss: 0.9262277483940125
Epoch 870, training loss: 6.1114821434021 = 0.05878285691142082 + 1.0 * 6.052699089050293
Epoch 870, val loss: 0.9327045679092407
Epoch 880, training loss: 6.106059551239014 = 0.05655801296234131 + 1.0 * 6.049501419067383
Epoch 880, val loss: 0.9391000270843506
Epoch 890, training loss: 6.103094100952148 = 0.05443449690937996 + 1.0 * 6.048659801483154
Epoch 890, val loss: 0.9455094933509827
Epoch 900, training loss: 6.102936744689941 = 0.05240435525774956 + 1.0 * 6.050532341003418
Epoch 900, val loss: 0.9519451856613159
Epoch 910, training loss: 6.098150253295898 = 0.05047023668885231 + 1.0 * 6.047679901123047
Epoch 910, val loss: 0.9582479000091553
Epoch 920, training loss: 6.094536781311035 = 0.04862738773226738 + 1.0 * 6.045909404754639
Epoch 920, val loss: 0.9645193815231323
Epoch 930, training loss: 6.091311454772949 = 0.04687102884054184 + 1.0 * 6.044440269470215
Epoch 930, val loss: 0.9706669449806213
Epoch 940, training loss: 6.089776515960693 = 0.045193810015916824 + 1.0 * 6.044582843780518
Epoch 940, val loss: 0.9768211245536804
Epoch 950, training loss: 6.092620372772217 = 0.04360177740454674 + 1.0 * 6.049018383026123
Epoch 950, val loss: 0.9829335808753967
Epoch 960, training loss: 6.086584091186523 = 0.042098913341760635 + 1.0 * 6.044485092163086
Epoch 960, val loss: 0.988614022731781
Epoch 970, training loss: 6.0828537940979 = 0.04066471382975578 + 1.0 * 6.042189121246338
Epoch 970, val loss: 0.9942795634269714
Epoch 980, training loss: 6.080475807189941 = 0.03928690403699875 + 1.0 * 6.041188716888428
Epoch 980, val loss: 1.0001516342163086
Epoch 990, training loss: 6.07869815826416 = 0.03796437755227089 + 1.0 * 6.040733814239502
Epoch 990, val loss: 1.0059986114501953
Epoch 1000, training loss: 6.086493968963623 = 0.03669479489326477 + 1.0 * 6.049798965454102
Epoch 1000, val loss: 1.011804223060608
Epoch 1010, training loss: 6.080018043518066 = 0.03549297899007797 + 1.0 * 6.044525146484375
Epoch 1010, val loss: 1.01737380027771
Epoch 1020, training loss: 6.07536506652832 = 0.03433898836374283 + 1.0 * 6.0410261154174805
Epoch 1020, val loss: 1.0228298902511597
Epoch 1030, training loss: 6.071667671203613 = 0.033235352486371994 + 1.0 * 6.0384321212768555
Epoch 1030, val loss: 1.028326153755188
Epoch 1040, training loss: 6.070863723754883 = 0.032173577696084976 + 1.0 * 6.038690090179443
Epoch 1040, val loss: 1.0338478088378906
Epoch 1050, training loss: 6.080173492431641 = 0.031158020719885826 + 1.0 * 6.049015522003174
Epoch 1050, val loss: 1.0393059253692627
Epoch 1060, training loss: 6.071542739868164 = 0.03019670769572258 + 1.0 * 6.041346073150635
Epoch 1060, val loss: 1.0444705486297607
Epoch 1070, training loss: 6.066157817840576 = 0.029276886954903603 + 1.0 * 6.036880970001221
Epoch 1070, val loss: 1.0494976043701172
Epoch 1080, training loss: 6.0647687911987305 = 0.02839263714849949 + 1.0 * 6.036375999450684
Epoch 1080, val loss: 1.0546461343765259
Epoch 1090, training loss: 6.062734127044678 = 0.027540937066078186 + 1.0 * 6.035192966461182
Epoch 1090, val loss: 1.0598349571228027
Epoch 1100, training loss: 6.069220542907715 = 0.026721376925706863 + 1.0 * 6.04249906539917
Epoch 1100, val loss: 1.0649924278259277
Epoch 1110, training loss: 6.064005374908447 = 0.025941871106624603 + 1.0 * 6.0380635261535645
Epoch 1110, val loss: 1.0699129104614258
Epoch 1120, training loss: 6.061572074890137 = 0.025193722918629646 + 1.0 * 6.036378383636475
Epoch 1120, val loss: 1.0747387409210205
Epoch 1130, training loss: 6.059429168701172 = 0.02447524480521679 + 1.0 * 6.034954071044922
Epoch 1130, val loss: 1.0795893669128418
Epoch 1140, training loss: 6.062821865081787 = 0.023783627897500992 + 1.0 * 6.039038181304932
Epoch 1140, val loss: 1.0844405889511108
Epoch 1150, training loss: 6.062117099761963 = 0.023118946701288223 + 1.0 * 6.038998126983643
Epoch 1150, val loss: 1.089203953742981
Epoch 1160, training loss: 6.0566725730896 = 0.022484127432107925 + 1.0 * 6.034188270568848
Epoch 1160, val loss: 1.0937769412994385
Epoch 1170, training loss: 6.055828094482422 = 0.02187316305935383 + 1.0 * 6.033955097198486
Epoch 1170, val loss: 1.098362922668457
Epoch 1180, training loss: 6.053752422332764 = 0.021286316215991974 + 1.0 * 6.032465934753418
Epoch 1180, val loss: 1.1028931140899658
Epoch 1190, training loss: 6.051093101501465 = 0.02071995474398136 + 1.0 * 6.0303730964660645
Epoch 1190, val loss: 1.1074117422103882
Epoch 1200, training loss: 6.058516025543213 = 0.020173242315649986 + 1.0 * 6.0383429527282715
Epoch 1200, val loss: 1.1119155883789062
Epoch 1210, training loss: 6.05361795425415 = 0.01965106464922428 + 1.0 * 6.033967018127441
Epoch 1210, val loss: 1.1162837743759155
Epoch 1220, training loss: 6.051156520843506 = 0.019148802384734154 + 1.0 * 6.032007694244385
Epoch 1220, val loss: 1.120535969734192
Epoch 1230, training loss: 6.052097797393799 = 0.01866445504128933 + 1.0 * 6.033433437347412
Epoch 1230, val loss: 1.124817967414856
Epoch 1240, training loss: 6.052331924438477 = 0.01819983497262001 + 1.0 * 6.03413200378418
Epoch 1240, val loss: 1.1290092468261719
Epoch 1250, training loss: 6.047163486480713 = 0.01775304786860943 + 1.0 * 6.029410362243652
Epoch 1250, val loss: 1.1330876350402832
Epoch 1260, training loss: 6.044156074523926 = 0.01731950230896473 + 1.0 * 6.026836395263672
Epoch 1260, val loss: 1.1372030973434448
Epoch 1270, training loss: 6.048736095428467 = 0.01689983159303665 + 1.0 * 6.031836032867432
Epoch 1270, val loss: 1.141339659690857
Epoch 1280, training loss: 6.0436482429504395 = 0.01649663969874382 + 1.0 * 6.027151584625244
Epoch 1280, val loss: 1.1453787088394165
Epoch 1290, training loss: 6.045034408569336 = 0.016107900068163872 + 1.0 * 6.028926372528076
Epoch 1290, val loss: 1.1493141651153564
Epoch 1300, training loss: 6.045744895935059 = 0.015732932835817337 + 1.0 * 6.030012130737305
Epoch 1300, val loss: 1.153227686882019
Epoch 1310, training loss: 6.044623851776123 = 0.015373273752629757 + 1.0 * 6.029250621795654
Epoch 1310, val loss: 1.1570161581039429
Epoch 1320, training loss: 6.04182767868042 = 0.015025737695395947 + 1.0 * 6.026802062988281
Epoch 1320, val loss: 1.1607502698898315
Epoch 1330, training loss: 6.038918495178223 = 0.01468921173363924 + 1.0 * 6.024229049682617
Epoch 1330, val loss: 1.1644957065582275
Epoch 1340, training loss: 6.040545463562012 = 0.014362185262143612 + 1.0 * 6.026183128356934
Epoch 1340, val loss: 1.1682896614074707
Epoch 1350, training loss: 6.043778419494629 = 0.01404541078954935 + 1.0 * 6.029733180999756
Epoch 1350, val loss: 1.1720529794692993
Epoch 1360, training loss: 6.038003444671631 = 0.013741713017225266 + 1.0 * 6.024261951446533
Epoch 1360, val loss: 1.1756401062011719
Epoch 1370, training loss: 6.036797046661377 = 0.013448571786284447 + 1.0 * 6.023348331451416
Epoch 1370, val loss: 1.1791561841964722
Epoch 1380, training loss: 6.035599231719971 = 0.013163045980036259 + 1.0 * 6.022436141967773
Epoch 1380, val loss: 1.1827497482299805
Epoch 1390, training loss: 6.040135860443115 = 0.01288588996976614 + 1.0 * 6.027249813079834
Epoch 1390, val loss: 1.1863727569580078
Epoch 1400, training loss: 6.03971529006958 = 0.012618137523531914 + 1.0 * 6.027097225189209
Epoch 1400, val loss: 1.1898561716079712
Epoch 1410, training loss: 6.035580158233643 = 0.012359961867332458 + 1.0 * 6.023220062255859
Epoch 1410, val loss: 1.1932342052459717
Epoch 1420, training loss: 6.033669471740723 = 0.012109693139791489 + 1.0 * 6.021559715270996
Epoch 1420, val loss: 1.1965856552124023
Epoch 1430, training loss: 6.03942346572876 = 0.011866019107401371 + 1.0 * 6.027557373046875
Epoch 1430, val loss: 1.200041651725769
Epoch 1440, training loss: 6.033328533172607 = 0.011630009859800339 + 1.0 * 6.021698474884033
Epoch 1440, val loss: 1.2033727169036865
Epoch 1450, training loss: 6.034701824188232 = 0.011402435600757599 + 1.0 * 6.023299217224121
Epoch 1450, val loss: 1.2066372632980347
Epoch 1460, training loss: 6.030840873718262 = 0.01118115708231926 + 1.0 * 6.019659519195557
Epoch 1460, val loss: 1.2098711729049683
Epoch 1470, training loss: 6.031713485717773 = 0.010966470465064049 + 1.0 * 6.020747184753418
Epoch 1470, val loss: 1.2131330966949463
Epoch 1480, training loss: 6.037276268005371 = 0.01075757946819067 + 1.0 * 6.026518821716309
Epoch 1480, val loss: 1.2163467407226562
Epoch 1490, training loss: 6.032227993011475 = 0.010554289445281029 + 1.0 * 6.021673679351807
Epoch 1490, val loss: 1.2194788455963135
Epoch 1500, training loss: 6.029521942138672 = 0.010358437895774841 + 1.0 * 6.019163608551025
Epoch 1500, val loss: 1.2225788831710815
Epoch 1510, training loss: 6.030332088470459 = 0.01016668789088726 + 1.0 * 6.02016544342041
Epoch 1510, val loss: 1.2257068157196045
Epoch 1520, training loss: 6.0305495262146 = 0.009980439208447933 + 1.0 * 6.020569324493408
Epoch 1520, val loss: 1.228814959526062
Epoch 1530, training loss: 6.028268337249756 = 0.00979972817003727 + 1.0 * 6.018468379974365
Epoch 1530, val loss: 1.231873631477356
Epoch 1540, training loss: 6.032466411590576 = 0.0096239959821105 + 1.0 * 6.0228424072265625
Epoch 1540, val loss: 1.2349225282669067
Epoch 1550, training loss: 6.0389885902404785 = 0.00945474673062563 + 1.0 * 6.029533863067627
Epoch 1550, val loss: 1.237826943397522
Epoch 1560, training loss: 6.026724815368652 = 0.009289326146245003 + 1.0 * 6.017435550689697
Epoch 1560, val loss: 1.2406517267227173
Epoch 1570, training loss: 6.025362014770508 = 0.009130503982305527 + 1.0 * 6.016231536865234
Epoch 1570, val loss: 1.2434091567993164
Epoch 1580, training loss: 6.0239458084106445 = 0.00897444412112236 + 1.0 * 6.0149712562561035
Epoch 1580, val loss: 1.2462797164916992
Epoch 1590, training loss: 6.023824214935303 = 0.008821208029985428 + 1.0 * 6.015003204345703
Epoch 1590, val loss: 1.2492153644561768
Epoch 1600, training loss: 6.03444766998291 = 0.008672123774886131 + 1.0 * 6.02577543258667
Epoch 1600, val loss: 1.252121090888977
Epoch 1610, training loss: 6.025856971740723 = 0.008527074940502644 + 1.0 * 6.017329692840576
Epoch 1610, val loss: 1.2549525499343872
Epoch 1620, training loss: 6.023937702178955 = 0.008386587724089622 + 1.0 * 6.0155510902404785
Epoch 1620, val loss: 1.2576727867126465
Epoch 1630, training loss: 6.030235767364502 = 0.008250013925135136 + 1.0 * 6.0219855308532715
Epoch 1630, val loss: 1.2604109048843384
Epoch 1640, training loss: 6.026571273803711 = 0.008115426637232304 + 1.0 * 6.018455982208252
Epoch 1640, val loss: 1.2630982398986816
Epoch 1650, training loss: 6.023456573486328 = 0.00798652321100235 + 1.0 * 6.015470027923584
Epoch 1650, val loss: 1.2657577991485596
Epoch 1660, training loss: 6.022082328796387 = 0.007859311066567898 + 1.0 * 6.014223098754883
Epoch 1660, val loss: 1.268418788909912
Epoch 1670, training loss: 6.023327827453613 = 0.007735501974821091 + 1.0 * 6.015592098236084
Epoch 1670, val loss: 1.2711057662963867
Epoch 1680, training loss: 6.020409107208252 = 0.0076141743920743465 + 1.0 * 6.0127949714660645
Epoch 1680, val loss: 1.2737663984298706
Epoch 1690, training loss: 6.023952960968018 = 0.007495700381696224 + 1.0 * 6.0164570808410645
Epoch 1690, val loss: 1.2764198780059814
Epoch 1700, training loss: 6.020292282104492 = 0.007380420807749033 + 1.0 * 6.012911796569824
Epoch 1700, val loss: 1.2790095806121826
Epoch 1710, training loss: 6.02181339263916 = 0.007268570363521576 + 1.0 * 6.01454496383667
Epoch 1710, val loss: 1.2815409898757935
Epoch 1720, training loss: 6.023282051086426 = 0.007159563712775707 + 1.0 * 6.016122341156006
Epoch 1720, val loss: 1.2840605974197388
Epoch 1730, training loss: 6.019660472869873 = 0.007053349167108536 + 1.0 * 6.012607097625732
Epoch 1730, val loss: 1.2864888906478882
Epoch 1740, training loss: 6.024676322937012 = 0.006950079929083586 + 1.0 * 6.017726421356201
Epoch 1740, val loss: 1.288914442062378
Epoch 1750, training loss: 6.01771354675293 = 0.006848498713225126 + 1.0 * 6.010865211486816
Epoch 1750, val loss: 1.29135000705719
Epoch 1760, training loss: 6.016475200653076 = 0.0067490581423044205 + 1.0 * 6.009726047515869
Epoch 1760, val loss: 1.2937958240509033
Epoch 1770, training loss: 6.021940231323242 = 0.006651460193097591 + 1.0 * 6.015288829803467
Epoch 1770, val loss: 1.2963000535964966
Epoch 1780, training loss: 6.017175197601318 = 0.00655625993385911 + 1.0 * 6.010619163513184
Epoch 1780, val loss: 1.2987384796142578
Epoch 1790, training loss: 6.015999794006348 = 0.006463991478085518 + 1.0 * 6.009535789489746
Epoch 1790, val loss: 1.3011009693145752
Epoch 1800, training loss: 6.017209053039551 = 0.006372956093400717 + 1.0 * 6.010836124420166
Epoch 1800, val loss: 1.3034844398498535
Epoch 1810, training loss: 6.021066188812256 = 0.006284067872911692 + 1.0 * 6.014781951904297
Epoch 1810, val loss: 1.3058624267578125
Epoch 1820, training loss: 6.016941547393799 = 0.0061972299590706825 + 1.0 * 6.010744094848633
Epoch 1820, val loss: 1.308113694190979
Epoch 1830, training loss: 6.015256404876709 = 0.006112835835665464 + 1.0 * 6.009143352508545
Epoch 1830, val loss: 1.3103845119476318
Epoch 1840, training loss: 6.026909828186035 = 0.0060296691954135895 + 1.0 * 6.020880222320557
Epoch 1840, val loss: 1.3126598596572876
Epoch 1850, training loss: 6.0176801681518555 = 0.005949857644736767 + 1.0 * 6.011730194091797
Epoch 1850, val loss: 1.3148564100265503
Epoch 1860, training loss: 6.0146355628967285 = 0.005870950408279896 + 1.0 * 6.008764743804932
Epoch 1860, val loss: 1.3170291185379028
Epoch 1870, training loss: 6.015263557434082 = 0.005793346557766199 + 1.0 * 6.009469985961914
Epoch 1870, val loss: 1.319262981414795
Epoch 1880, training loss: 6.017042636871338 = 0.005717073101550341 + 1.0 * 6.011325359344482
Epoch 1880, val loss: 1.3215082883834839
Epoch 1890, training loss: 6.0136613845825195 = 0.005643240641802549 + 1.0 * 6.0080180168151855
Epoch 1890, val loss: 1.3236665725708008
Epoch 1900, training loss: 6.011814117431641 = 0.005570149980485439 + 1.0 * 6.00624418258667
Epoch 1900, val loss: 1.3258402347564697
Epoch 1910, training loss: 6.016870498657227 = 0.005498230457305908 + 1.0 * 6.011372089385986
Epoch 1910, val loss: 1.3280463218688965
Epoch 1920, training loss: 6.015443801879883 = 0.005428415257483721 + 1.0 * 6.010015487670898
Epoch 1920, val loss: 1.3301490545272827
Epoch 1930, training loss: 6.012369155883789 = 0.005360396113246679 + 1.0 * 6.0070085525512695
Epoch 1930, val loss: 1.3321834802627563
Epoch 1940, training loss: 6.010319232940674 = 0.005293780472129583 + 1.0 * 6.005025386810303
Epoch 1940, val loss: 1.3342314958572388
Epoch 1950, training loss: 6.017496109008789 = 0.005228440742939711 + 1.0 * 6.012267589569092
Epoch 1950, val loss: 1.3363460302352905
Epoch 1960, training loss: 6.010195255279541 = 0.005163779016584158 + 1.0 * 6.005031585693359
Epoch 1960, val loss: 1.3383737802505493
Epoch 1970, training loss: 6.0096917152404785 = 0.005101498682051897 + 1.0 * 6.004590034484863
Epoch 1970, val loss: 1.3403338193893433
Epoch 1980, training loss: 6.009578227996826 = 0.00504016038030386 + 1.0 * 6.004538059234619
Epoch 1980, val loss: 1.342330813407898
Epoch 1990, training loss: 6.014150142669678 = 0.004979284480214119 + 1.0 * 6.009171009063721
Epoch 1990, val loss: 1.3444126844406128
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.8229
Flip ASR: 0.7867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.318262100219727 = 1.9444698095321655 + 1.0 * 8.37379264831543
Epoch 0, val loss: 1.9439231157302856
Epoch 10, training loss: 10.307798385620117 = 1.934842824935913 + 1.0 * 8.372955322265625
Epoch 10, val loss: 1.9336217641830444
Epoch 20, training loss: 10.28982162475586 = 1.9231830835342407 + 1.0 * 8.36663818359375
Epoch 20, val loss: 1.9206727743148804
Epoch 30, training loss: 10.231456756591797 = 1.9077143669128418 + 1.0 * 8.323742866516113
Epoch 30, val loss: 1.9032409191131592
Epoch 40, training loss: 9.8902006149292 = 1.8899449110031128 + 1.0 * 8.000255584716797
Epoch 40, val loss: 1.8838680982589722
Epoch 50, training loss: 9.238178253173828 = 1.871211051940918 + 1.0 * 7.36696720123291
Epoch 50, val loss: 1.8645941019058228
Epoch 60, training loss: 8.790140151977539 = 1.855912685394287 + 1.0 * 6.93422794342041
Epoch 60, val loss: 1.8496389389038086
Epoch 70, training loss: 8.559479713439941 = 1.8411883115768433 + 1.0 * 6.718291282653809
Epoch 70, val loss: 1.8342913389205933
Epoch 80, training loss: 8.423685073852539 = 1.8270286321640015 + 1.0 * 6.596656322479248
Epoch 80, val loss: 1.8195070028305054
Epoch 90, training loss: 8.323017120361328 = 1.811633825302124 + 1.0 * 6.511383056640625
Epoch 90, val loss: 1.803595781326294
Epoch 100, training loss: 8.24413013458252 = 1.7959506511688232 + 1.0 * 6.448179244995117
Epoch 100, val loss: 1.7874287366867065
Epoch 110, training loss: 8.180362701416016 = 1.7803736925125122 + 1.0 * 6.399988651275635
Epoch 110, val loss: 1.7717787027359009
Epoch 120, training loss: 8.125567436218262 = 1.7643029689788818 + 1.0 * 6.361264705657959
Epoch 120, val loss: 1.7560635805130005
Epoch 130, training loss: 8.075104713439941 = 1.746660590171814 + 1.0 * 6.328444004058838
Epoch 130, val loss: 1.7396233081817627
Epoch 140, training loss: 8.027063369750977 = 1.7272093296051025 + 1.0 * 6.299854278564453
Epoch 140, val loss: 1.721929907798767
Epoch 150, training loss: 7.9804792404174805 = 1.705437421798706 + 1.0 * 6.2750420570373535
Epoch 150, val loss: 1.7028604745864868
Epoch 160, training loss: 7.933435916900635 = 1.6807808876037598 + 1.0 * 6.252655029296875
Epoch 160, val loss: 1.6819555759429932
Epoch 170, training loss: 7.88514518737793 = 1.652239441871643 + 1.0 * 6.232905864715576
Epoch 170, val loss: 1.6583921909332275
Epoch 180, training loss: 7.836213111877441 = 1.6186108589172363 + 1.0 * 6.217602252960205
Epoch 180, val loss: 1.6313029527664185
Epoch 190, training loss: 7.781999588012695 = 1.5796921253204346 + 1.0 * 6.202307224273682
Epoch 190, val loss: 1.6003217697143555
Epoch 200, training loss: 7.723537921905518 = 1.534507155418396 + 1.0 * 6.189030647277832
Epoch 200, val loss: 1.564772129058838
Epoch 210, training loss: 7.661009788513184 = 1.4821453094482422 + 1.0 * 6.178864479064941
Epoch 210, val loss: 1.523911476135254
Epoch 220, training loss: 7.592142105102539 = 1.4225983619689941 + 1.0 * 6.169543743133545
Epoch 220, val loss: 1.4775694608688354
Epoch 230, training loss: 7.52647590637207 = 1.3569743633270264 + 1.0 * 6.169501781463623
Epoch 230, val loss: 1.4269423484802246
Epoch 240, training loss: 7.44492769241333 = 1.288008689880371 + 1.0 * 6.156919002532959
Epoch 240, val loss: 1.37419855594635
Epoch 250, training loss: 7.366165637969971 = 1.2160725593566895 + 1.0 * 6.150093078613281
Epoch 250, val loss: 1.3197484016418457
Epoch 260, training loss: 7.286931037902832 = 1.1426023244857788 + 1.0 * 6.144328594207764
Epoch 260, val loss: 1.2645009756088257
Epoch 270, training loss: 7.2164812088012695 = 1.0693343877792358 + 1.0 * 6.147146701812744
Epoch 270, val loss: 1.2099519968032837
Epoch 280, training loss: 7.139787197113037 = 1.00111722946167 + 1.0 * 6.138669967651367
Epoch 280, val loss: 1.158949851989746
Epoch 290, training loss: 7.069516658782959 = 0.937883198261261 + 1.0 * 6.131633281707764
Epoch 290, val loss: 1.1118522882461548
Epoch 300, training loss: 7.006659507751465 = 0.8793670535087585 + 1.0 * 6.127292633056641
Epoch 300, val loss: 1.068498969078064
Epoch 310, training loss: 6.959660053253174 = 0.826099693775177 + 1.0 * 6.1335601806640625
Epoch 310, val loss: 1.0291473865509033
Epoch 320, training loss: 6.901514530181885 = 0.7786696553230286 + 1.0 * 6.122844696044922
Epoch 320, val loss: 0.9944370985031128
Epoch 330, training loss: 6.854666709899902 = 0.7362889051437378 + 1.0 * 6.118377685546875
Epoch 330, val loss: 0.9636934399604797
Epoch 340, training loss: 6.814964771270752 = 0.6981067657470703 + 1.0 * 6.116858005523682
Epoch 340, val loss: 0.9363383650779724
Epoch 350, training loss: 6.775100231170654 = 0.6636109352111816 + 1.0 * 6.111489295959473
Epoch 350, val loss: 0.9119667410850525
Epoch 360, training loss: 6.740914344787598 = 0.6319345831871033 + 1.0 * 6.10897970199585
Epoch 360, val loss: 0.8900545239448547
Epoch 370, training loss: 6.706445693969727 = 0.6024528741836548 + 1.0 * 6.103992938995361
Epoch 370, val loss: 0.8701838850975037
Epoch 380, training loss: 6.675649166107178 = 0.5744697451591492 + 1.0 * 6.101179599761963
Epoch 380, val loss: 0.8518471717834473
Epoch 390, training loss: 6.656824111938477 = 0.5475782752037048 + 1.0 * 6.109245777130127
Epoch 390, val loss: 0.8347083926200867
Epoch 400, training loss: 6.621137619018555 = 0.5216769576072693 + 1.0 * 6.099460601806641
Epoch 400, val loss: 0.8186594247817993
Epoch 410, training loss: 6.5905609130859375 = 0.4964534640312195 + 1.0 * 6.094107627868652
Epoch 410, val loss: 0.8034527897834778
Epoch 420, training loss: 6.562448024749756 = 0.47153112292289734 + 1.0 * 6.090917110443115
Epoch 420, val loss: 0.7889273166656494
Epoch 430, training loss: 6.548628330230713 = 0.4468519985675812 + 1.0 * 6.101776123046875
Epoch 430, val loss: 0.7749722599983215
Epoch 440, training loss: 6.513375282287598 = 0.42262861132621765 + 1.0 * 6.090746879577637
Epoch 440, val loss: 0.7616395354270935
Epoch 450, training loss: 6.488785743713379 = 0.39874744415283203 + 1.0 * 6.090038299560547
Epoch 450, val loss: 0.7489209771156311
Epoch 460, training loss: 6.458561897277832 = 0.3752717971801758 + 1.0 * 6.083290100097656
Epoch 460, val loss: 0.7369259595870972
Epoch 470, training loss: 6.433509826660156 = 0.3520888388156891 + 1.0 * 6.0814208984375
Epoch 470, val loss: 0.7256089448928833
Epoch 480, training loss: 6.4153666496276855 = 0.3294360935688019 + 1.0 * 6.085930347442627
Epoch 480, val loss: 0.7151426672935486
Epoch 490, training loss: 6.38747501373291 = 0.3075922727584839 + 1.0 * 6.079882621765137
Epoch 490, val loss: 0.7056678533554077
Epoch 500, training loss: 6.3629231452941895 = 0.2865089774131775 + 1.0 * 6.076414108276367
Epoch 500, val loss: 0.6972601413726807
Epoch 510, training loss: 6.34061861038208 = 0.26622170209884644 + 1.0 * 6.074397087097168
Epoch 510, val loss: 0.6899139881134033
Epoch 520, training loss: 6.32444429397583 = 0.24697451293468475 + 1.0 * 6.077469825744629
Epoch 520, val loss: 0.6836106777191162
Epoch 530, training loss: 6.302208423614502 = 0.22906674444675446 + 1.0 * 6.073141574859619
Epoch 530, val loss: 0.6784201860427856
Epoch 540, training loss: 6.281569004058838 = 0.21234780550003052 + 1.0 * 6.069221019744873
Epoch 540, val loss: 0.6743414402008057
Epoch 550, training loss: 6.265555381774902 = 0.19680090248584747 + 1.0 * 6.06875467300415
Epoch 550, val loss: 0.6713070869445801
Epoch 560, training loss: 6.254199504852295 = 0.18247009813785553 + 1.0 * 6.0717291831970215
Epoch 560, val loss: 0.6692052483558655
Epoch 570, training loss: 6.237113952636719 = 0.16935478150844574 + 1.0 * 6.067759037017822
Epoch 570, val loss: 0.6680132746696472
Epoch 580, training loss: 6.230134963989258 = 0.15735070407390594 + 1.0 * 6.072784423828125
Epoch 580, val loss: 0.6676828861236572
Epoch 590, training loss: 6.21143913269043 = 0.1464623063802719 + 1.0 * 6.064976692199707
Epoch 590, val loss: 0.668175458908081
Epoch 600, training loss: 6.201037406921387 = 0.13654837012290955 + 1.0 * 6.064488887786865
Epoch 600, val loss: 0.6693288683891296
Epoch 610, training loss: 6.188246250152588 = 0.12751886248588562 + 1.0 * 6.060727596282959
Epoch 610, val loss: 0.6710737943649292
Epoch 620, training loss: 6.18093204498291 = 0.11924673616886139 + 1.0 * 6.061685085296631
Epoch 620, val loss: 0.6734156608581543
Epoch 630, training loss: 6.178859233856201 = 0.11170222610235214 + 1.0 * 6.067156791687012
Epoch 630, val loss: 0.6762406229972839
Epoch 640, training loss: 6.162947654724121 = 0.10485518723726273 + 1.0 * 6.0580925941467285
Epoch 640, val loss: 0.6794208884239197
Epoch 650, training loss: 6.153973579406738 = 0.09854759275913239 + 1.0 * 6.055426120758057
Epoch 650, val loss: 0.683020293712616
Epoch 660, training loss: 6.1526336669921875 = 0.09275567531585693 + 1.0 * 6.059877872467041
Epoch 660, val loss: 0.68693608045578
Epoch 670, training loss: 6.1455488204956055 = 0.0874364823102951 + 1.0 * 6.058112144470215
Epoch 670, val loss: 0.6910896897315979
Epoch 680, training loss: 6.1347198486328125 = 0.08256127685308456 + 1.0 * 6.052158355712891
Epoch 680, val loss: 0.6954711079597473
Epoch 690, training loss: 6.132026195526123 = 0.07804729044437408 + 1.0 * 6.05397891998291
Epoch 690, val loss: 0.7000477910041809
Epoch 700, training loss: 6.124682426452637 = 0.07386711984872818 + 1.0 * 6.050815105438232
Epoch 700, val loss: 0.7047997117042542
Epoch 710, training loss: 6.12022066116333 = 0.07000526040792465 + 1.0 * 6.050215244293213
Epoch 710, val loss: 0.7095836997032166
Epoch 720, training loss: 6.116171836853027 = 0.06643572449684143 + 1.0 * 6.049736022949219
Epoch 720, val loss: 0.7144933342933655
Epoch 730, training loss: 6.109457492828369 = 0.06311527639627457 + 1.0 * 6.046342372894287
Epoch 730, val loss: 0.7194380164146423
Epoch 740, training loss: 6.105396270751953 = 0.060020700097084045 + 1.0 * 6.045375347137451
Epoch 740, val loss: 0.7244961857795715
Epoch 750, training loss: 6.111043930053711 = 0.05713604390621185 + 1.0 * 6.053907871246338
Epoch 750, val loss: 0.7295710444450378
Epoch 760, training loss: 6.0975565910339355 = 0.054472483694553375 + 1.0 * 6.043084144592285
Epoch 760, val loss: 0.7346253395080566
Epoch 770, training loss: 6.09794282913208 = 0.05197841301560402 + 1.0 * 6.045964241027832
Epoch 770, val loss: 0.7396928071975708
Epoch 780, training loss: 6.091181755065918 = 0.049647800624370575 + 1.0 * 6.041533946990967
Epoch 780, val loss: 0.7447625994682312
Epoch 790, training loss: 6.088878154754639 = 0.04747079685330391 + 1.0 * 6.041407585144043
Epoch 790, val loss: 0.7498121857643127
Epoch 800, training loss: 6.0881733894348145 = 0.04542633518576622 + 1.0 * 6.0427470207214355
Epoch 800, val loss: 0.7548413872718811
Epoch 810, training loss: 6.082900524139404 = 0.04351465776562691 + 1.0 * 6.039385795593262
Epoch 810, val loss: 0.7598936557769775
Epoch 820, training loss: 6.085717678070068 = 0.04171251878142357 + 1.0 * 6.044005393981934
Epoch 820, val loss: 0.7649374008178711
Epoch 830, training loss: 6.078754901885986 = 0.040026403963565826 + 1.0 * 6.038728713989258
Epoch 830, val loss: 0.7698948979377747
Epoch 840, training loss: 6.075751304626465 = 0.038439467549324036 + 1.0 * 6.037312030792236
Epoch 840, val loss: 0.7748825550079346
Epoch 850, training loss: 6.080759525299072 = 0.03694181889295578 + 1.0 * 6.043817520141602
Epoch 850, val loss: 0.7798002362251282
Epoch 860, training loss: 6.074896812438965 = 0.03552974388003349 + 1.0 * 6.039367198944092
Epoch 860, val loss: 0.7846758365631104
Epoch 870, training loss: 6.068718433380127 = 0.03420574590563774 + 1.0 * 6.034512519836426
Epoch 870, val loss: 0.7895116806030273
Epoch 880, training loss: 6.068863391876221 = 0.03294648230075836 + 1.0 * 6.035916805267334
Epoch 880, val loss: 0.7943139672279358
Epoch 890, training loss: 6.067987442016602 = 0.031759656965732574 + 1.0 * 6.036227703094482
Epoch 890, val loss: 0.7990825176239014
Epoch 900, training loss: 6.062897205352783 = 0.030639536678791046 + 1.0 * 6.032257556915283
Epoch 900, val loss: 0.8037811517715454
Epoch 910, training loss: 6.060440540313721 = 0.02957494556903839 + 1.0 * 6.030865669250488
Epoch 910, val loss: 0.8084427118301392
Epoch 920, training loss: 6.065492630004883 = 0.028563957661390305 + 1.0 * 6.036928653717041
Epoch 920, val loss: 0.8131025433540344
Epoch 930, training loss: 6.059159278869629 = 0.027602361515164375 + 1.0 * 6.031557083129883
Epoch 930, val loss: 0.8177089691162109
Epoch 940, training loss: 6.064487457275391 = 0.02669445611536503 + 1.0 * 6.037793159484863
Epoch 940, val loss: 0.822223961353302
Epoch 950, training loss: 6.0619306564331055 = 0.02582843229174614 + 1.0 * 6.036102294921875
Epoch 950, val loss: 0.8266794085502625
Epoch 960, training loss: 6.05412483215332 = 0.025013796985149384 + 1.0 * 6.029110908508301
Epoch 960, val loss: 0.8311158418655396
Epoch 970, training loss: 6.052206039428711 = 0.02423224039375782 + 1.0 * 6.027973651885986
Epoch 970, val loss: 0.8354923725128174
Epoch 980, training loss: 6.053069114685059 = 0.023485524579882622 + 1.0 * 6.02958345413208
Epoch 980, val loss: 0.8398747444152832
Epoch 990, training loss: 6.049624443054199 = 0.022772876545786858 + 1.0 * 6.026851654052734
Epoch 990, val loss: 0.8441827297210693
Epoch 1000, training loss: 6.048896312713623 = 0.022091789171099663 + 1.0 * 6.026804447174072
Epoch 1000, val loss: 0.8484535813331604
Epoch 1010, training loss: 6.053518295288086 = 0.021443918347358704 + 1.0 * 6.032074451446533
Epoch 1010, val loss: 0.8526260852813721
Epoch 1020, training loss: 6.048393249511719 = 0.020822370424866676 + 1.0 * 6.027570724487305
Epoch 1020, val loss: 0.8568119406700134
Epoch 1030, training loss: 6.0570292472839355 = 0.020236533135175705 + 1.0 * 6.036792755126953
Epoch 1030, val loss: 0.8608360290527344
Epoch 1040, training loss: 6.044386386871338 = 0.019675465300679207 + 1.0 * 6.0247111320495605
Epoch 1040, val loss: 0.864862322807312
Epoch 1050, training loss: 6.042636394500732 = 0.019135555252432823 + 1.0 * 6.023500919342041
Epoch 1050, val loss: 0.8688861131668091
Epoch 1060, training loss: 6.04095983505249 = 0.018616050481796265 + 1.0 * 6.022343635559082
Epoch 1060, val loss: 0.8728665709495544
Epoch 1070, training loss: 6.054238319396973 = 0.018116101622581482 + 1.0 * 6.0361223220825195
Epoch 1070, val loss: 0.8768078684806824
Epoch 1080, training loss: 6.044278144836426 = 0.01764298416674137 + 1.0 * 6.02663516998291
Epoch 1080, val loss: 0.8807013034820557
Epoch 1090, training loss: 6.0374040603637695 = 0.01718607172369957 + 1.0 * 6.0202178955078125
Epoch 1090, val loss: 0.8845187425613403
Epoch 1100, training loss: 6.037520408630371 = 0.0167457964271307 + 1.0 * 6.020774841308594
Epoch 1100, val loss: 0.8883148431777954
Epoch 1110, training loss: 6.047575950622559 = 0.016321299597620964 + 1.0 * 6.031254768371582
Epoch 1110, val loss: 0.8921133875846863
Epoch 1120, training loss: 6.038675785064697 = 0.01591828465461731 + 1.0 * 6.022757530212402
Epoch 1120, val loss: 0.8958072662353516
Epoch 1130, training loss: 6.0353779792785645 = 0.01552651822566986 + 1.0 * 6.0198516845703125
Epoch 1130, val loss: 0.8994741439819336
Epoch 1140, training loss: 6.04217529296875 = 0.01515005063265562 + 1.0 * 6.02702522277832
Epoch 1140, val loss: 0.9031416773796082
Epoch 1150, training loss: 6.037112712860107 = 0.014788346365094185 + 1.0 * 6.022324562072754
Epoch 1150, val loss: 0.9067565202713013
Epoch 1160, training loss: 6.032763957977295 = 0.014442541636526585 + 1.0 * 6.018321514129639
Epoch 1160, val loss: 0.9103001356124878
Epoch 1170, training loss: 6.031243801116943 = 0.014106141403317451 + 1.0 * 6.01713752746582
Epoch 1170, val loss: 0.913847029209137
Epoch 1180, training loss: 6.036602020263672 = 0.013782043009996414 + 1.0 * 6.022819995880127
Epoch 1180, val loss: 0.9173728823661804
Epoch 1190, training loss: 6.036581039428711 = 0.013468125835061073 + 1.0 * 6.023112773895264
Epoch 1190, val loss: 0.9208857417106628
Epoch 1200, training loss: 6.03150749206543 = 0.013169296085834503 + 1.0 * 6.018338203430176
Epoch 1200, val loss: 0.9242525100708008
Epoch 1210, training loss: 6.02921199798584 = 0.012878469191491604 + 1.0 * 6.01633358001709
Epoch 1210, val loss: 0.9276516437530518
Epoch 1220, training loss: 6.034860134124756 = 0.012597249820828438 + 1.0 * 6.022263050079346
Epoch 1220, val loss: 0.9310136437416077
Epoch 1230, training loss: 6.027706146240234 = 0.012325534597039223 + 1.0 * 6.015380382537842
Epoch 1230, val loss: 0.9343070387840271
Epoch 1240, training loss: 6.0260701179504395 = 0.012063001282513142 + 1.0 * 6.014007091522217
Epoch 1240, val loss: 0.9375863671302795
Epoch 1250, training loss: 6.0260186195373535 = 0.011807730421423912 + 1.0 * 6.0142107009887695
Epoch 1250, val loss: 0.940868079662323
Epoch 1260, training loss: 6.03179931640625 = 0.01155998557806015 + 1.0 * 6.020239353179932
Epoch 1260, val loss: 0.9441313743591309
Epoch 1270, training loss: 6.0259904861450195 = 0.011321933940052986 + 1.0 * 6.0146684646606445
Epoch 1270, val loss: 0.947280764579773
Epoch 1280, training loss: 6.036753177642822 = 0.011092249304056168 + 1.0 * 6.025660991668701
Epoch 1280, val loss: 0.9504677057266235
Epoch 1290, training loss: 6.029064655303955 = 0.010871014557778835 + 1.0 * 6.01819372177124
Epoch 1290, val loss: 0.9534457921981812
Epoch 1300, training loss: 6.023362159729004 = 0.01065710186958313 + 1.0 * 6.012704849243164
Epoch 1300, val loss: 0.9565445780754089
Epoch 1310, training loss: 6.022762298583984 = 0.010448232293128967 + 1.0 * 6.0123138427734375
Epoch 1310, val loss: 0.9596115946769714
Epoch 1320, training loss: 6.029056549072266 = 0.010245772078633308 + 1.0 * 6.018810749053955
Epoch 1320, val loss: 0.962638795375824
Epoch 1330, training loss: 6.021115303039551 = 0.010048123076558113 + 1.0 * 6.0110673904418945
Epoch 1330, val loss: 0.9656631946563721
Epoch 1340, training loss: 6.021037578582764 = 0.009857336059212685 + 1.0 * 6.011180400848389
Epoch 1340, val loss: 0.9686189889907837
Epoch 1350, training loss: 6.0305585861206055 = 0.009673009626567364 + 1.0 * 6.020885467529297
Epoch 1350, val loss: 0.9715518355369568
Epoch 1360, training loss: 6.022368907928467 = 0.009493490681052208 + 1.0 * 6.012875556945801
Epoch 1360, val loss: 0.9744020700454712
Epoch 1370, training loss: 6.020937442779541 = 0.00932067446410656 + 1.0 * 6.0116167068481445
Epoch 1370, val loss: 0.9772640466690063
Epoch 1380, training loss: 6.024873733520508 = 0.00915153231471777 + 1.0 * 6.015722274780273
Epoch 1380, val loss: 0.980107843875885
Epoch 1390, training loss: 6.019359111785889 = 0.008986886590719223 + 1.0 * 6.010372161865234
Epoch 1390, val loss: 0.9829501509666443
Epoch 1400, training loss: 6.019824028015137 = 0.008827230893075466 + 1.0 * 6.0109968185424805
Epoch 1400, val loss: 0.9857630729675293
Epoch 1410, training loss: 6.02213716506958 = 0.008672384545207024 + 1.0 * 6.01346492767334
Epoch 1410, val loss: 0.9885014295578003
Epoch 1420, training loss: 6.018194198608398 = 0.008520657196640968 + 1.0 * 6.009673595428467
Epoch 1420, val loss: 0.9912846088409424
Epoch 1430, training loss: 6.018703937530518 = 0.008374353870749474 + 1.0 * 6.010329723358154
Epoch 1430, val loss: 0.9939877986907959
Epoch 1440, training loss: 6.019706726074219 = 0.00823168084025383 + 1.0 * 6.011475086212158
Epoch 1440, val loss: 0.9966776371002197
Epoch 1450, training loss: 6.016439437866211 = 0.008092056028544903 + 1.0 * 6.008347511291504
Epoch 1450, val loss: 0.9993530511856079
Epoch 1460, training loss: 6.014925479888916 = 0.007956885732710361 + 1.0 * 6.0069684982299805
Epoch 1460, val loss: 1.002013087272644
Epoch 1470, training loss: 6.026129722595215 = 0.007824169471859932 + 1.0 * 6.018305778503418
Epoch 1470, val loss: 1.0046594142913818
Epoch 1480, training loss: 6.021160125732422 = 0.007696888875216246 + 1.0 * 6.013463020324707
Epoch 1480, val loss: 1.0072243213653564
Epoch 1490, training loss: 6.015096187591553 = 0.007574106566607952 + 1.0 * 6.007522106170654
Epoch 1490, val loss: 1.0097036361694336
Epoch 1500, training loss: 6.013125419616699 = 0.007453230209648609 + 1.0 * 6.005671977996826
Epoch 1500, val loss: 1.0122413635253906
Epoch 1510, training loss: 6.013499736785889 = 0.007334295194596052 + 1.0 * 6.006165504455566
Epoch 1510, val loss: 1.0148115158081055
Epoch 1520, training loss: 6.019822597503662 = 0.007218659855425358 + 1.0 * 6.012603759765625
Epoch 1520, val loss: 1.0173254013061523
Epoch 1530, training loss: 6.018434047698975 = 0.007106427568942308 + 1.0 * 6.011327743530273
Epoch 1530, val loss: 1.019789457321167
Epoch 1540, training loss: 6.016634941101074 = 0.006997362710535526 + 1.0 * 6.009637355804443
Epoch 1540, val loss: 1.0222012996673584
Epoch 1550, training loss: 6.012078762054443 = 0.006890484597533941 + 1.0 * 6.005188465118408
Epoch 1550, val loss: 1.0246306657791138
Epoch 1560, training loss: 6.011063098907471 = 0.006786458659917116 + 1.0 * 6.004276752471924
Epoch 1560, val loss: 1.0270788669586182
Epoch 1570, training loss: 6.012989044189453 = 0.006683940999209881 + 1.0 * 6.00630521774292
Epoch 1570, val loss: 1.0295308828353882
Epoch 1580, training loss: 6.012551784515381 = 0.006583639420568943 + 1.0 * 6.00596809387207
Epoch 1580, val loss: 1.031942367553711
Epoch 1590, training loss: 6.013531684875488 = 0.006486230064183474 + 1.0 * 6.007045269012451
Epoch 1590, val loss: 1.0342575311660767
Epoch 1600, training loss: 6.011727809906006 = 0.006392369046807289 + 1.0 * 6.005335330963135
Epoch 1600, val loss: 1.0365294218063354
Epoch 1610, training loss: 6.025160789489746 = 0.006300360430032015 + 1.0 * 6.018860340118408
Epoch 1610, val loss: 1.0388579368591309
Epoch 1620, training loss: 6.0121378898620605 = 0.006210404448211193 + 1.0 * 6.005927562713623
Epoch 1620, val loss: 1.041121006011963
Epoch 1630, training loss: 6.008826732635498 = 0.0061234100721776485 + 1.0 * 6.0027031898498535
Epoch 1630, val loss: 1.043305516242981
Epoch 1640, training loss: 6.007896900177002 = 0.0060371761210262775 + 1.0 * 6.001859664916992
Epoch 1640, val loss: 1.0456191301345825
Epoch 1650, training loss: 6.011694431304932 = 0.00595213333144784 + 1.0 * 6.005742073059082
Epoch 1650, val loss: 1.0479069948196411
Epoch 1660, training loss: 6.008708953857422 = 0.005869815591722727 + 1.0 * 6.002839088439941
Epoch 1660, val loss: 1.0501461029052734
Epoch 1670, training loss: 6.007640361785889 = 0.0057901968248188496 + 1.0 * 6.001850128173828
Epoch 1670, val loss: 1.0522441864013672
Epoch 1680, training loss: 6.007354736328125 = 0.005711826030164957 + 1.0 * 6.00164270401001
Epoch 1680, val loss: 1.0544524192810059
Epoch 1690, training loss: 6.012980937957764 = 0.00563465291634202 + 1.0 * 6.007346153259277
Epoch 1690, val loss: 1.0566121339797974
Epoch 1700, training loss: 6.008950710296631 = 0.00555940717458725 + 1.0 * 6.003391265869141
Epoch 1700, val loss: 1.0588433742523193
Epoch 1710, training loss: 6.009938716888428 = 0.005487013608217239 + 1.0 * 6.004451751708984
Epoch 1710, val loss: 1.0608301162719727
Epoch 1720, training loss: 6.005978584289551 = 0.005415558349341154 + 1.0 * 6.000563144683838
Epoch 1720, val loss: 1.0629061460494995
Epoch 1730, training loss: 6.005429267883301 = 0.005345052108168602 + 1.0 * 6.000084400177002
Epoch 1730, val loss: 1.0650044679641724
Epoch 1740, training loss: 6.006769180297852 = 0.005275583826005459 + 1.0 * 6.001493453979492
Epoch 1740, val loss: 1.0671404600143433
Epoch 1750, training loss: 6.007960796356201 = 0.005207546055316925 + 1.0 * 6.002753257751465
Epoch 1750, val loss: 1.0692250728607178
Epoch 1760, training loss: 6.012744903564453 = 0.005141444969922304 + 1.0 * 6.007603645324707
Epoch 1760, val loss: 1.071255087852478
Epoch 1770, training loss: 6.006825923919678 = 0.00507735600695014 + 1.0 * 6.001748561859131
Epoch 1770, val loss: 1.0731920003890991
Epoch 1780, training loss: 6.004433631896973 = 0.005014978814870119 + 1.0 * 5.99941873550415
Epoch 1780, val loss: 1.0751678943634033
Epoch 1790, training loss: 6.004608631134033 = 0.004952885676175356 + 1.0 * 5.999655723571777
Epoch 1790, val loss: 1.0771875381469727
Epoch 1800, training loss: 6.014387130737305 = 0.004891860298812389 + 1.0 * 6.009495258331299
Epoch 1800, val loss: 1.079219937324524
Epoch 1810, training loss: 6.004692554473877 = 0.004832870792597532 + 1.0 * 5.999859809875488
Epoch 1810, val loss: 1.0810929536819458
Epoch 1820, training loss: 6.002142906188965 = 0.00477526243776083 + 1.0 * 5.997367858886719
Epoch 1820, val loss: 1.0829963684082031
Epoch 1830, training loss: 6.001864910125732 = 0.004717906471341848 + 1.0 * 5.997147083282471
Epoch 1830, val loss: 1.0849602222442627
Epoch 1840, training loss: 6.010672569274902 = 0.00466184364631772 + 1.0 * 6.00601053237915
Epoch 1840, val loss: 1.0869108438491821
Epoch 1850, training loss: 6.002655506134033 = 0.004606279078871012 + 1.0 * 5.998049259185791
Epoch 1850, val loss: 1.088861107826233
Epoch 1860, training loss: 6.002562999725342 = 0.004553097300231457 + 1.0 * 5.99800968170166
Epoch 1860, val loss: 1.090631127357483
Epoch 1870, training loss: 6.005266189575195 = 0.0045004477724432945 + 1.0 * 6.000765800476074
Epoch 1870, val loss: 1.092517614364624
Epoch 1880, training loss: 6.001370429992676 = 0.00444879150018096 + 1.0 * 5.996921539306641
Epoch 1880, val loss: 1.0943613052368164
Epoch 1890, training loss: 6.011719703674316 = 0.004397973883897066 + 1.0 * 6.007321834564209
Epoch 1890, val loss: 1.0962128639221191
Epoch 1900, training loss: 6.002588272094727 = 0.004348798654973507 + 1.0 * 5.998239517211914
Epoch 1900, val loss: 1.0980650186538696
Epoch 1910, training loss: 6.0001983642578125 = 0.004300213884562254 + 1.0 * 5.995898246765137
Epoch 1910, val loss: 1.0998460054397583
Epoch 1920, training loss: 6.0007548332214355 = 0.00425191642716527 + 1.0 * 5.996502876281738
Epoch 1920, val loss: 1.1017297506332397
Epoch 1930, training loss: 6.00986385345459 = 0.004204658325761557 + 1.0 * 6.005659103393555
Epoch 1930, val loss: 1.1035510301589966
Epoch 1940, training loss: 6.003201007843018 = 0.004158536903560162 + 1.0 * 5.999042510986328
Epoch 1940, val loss: 1.1052855253219604
Epoch 1950, training loss: 6.002730846405029 = 0.004113529343158007 + 1.0 * 5.998617172241211
Epoch 1950, val loss: 1.1070475578308105
Epoch 1960, training loss: 6.003052711486816 = 0.004069240763783455 + 1.0 * 5.998983383178711
Epoch 1960, val loss: 1.1087920665740967
Epoch 1970, training loss: 5.999713897705078 = 0.004025756847113371 + 1.0 * 5.995687961578369
Epoch 1970, val loss: 1.1105692386627197
Epoch 1980, training loss: 5.998697280883789 = 0.003982871305197477 + 1.0 * 5.994714260101318
Epoch 1980, val loss: 1.1123203039169312
Epoch 1990, training loss: 6.000362396240234 = 0.003940338268876076 + 1.0 * 5.996422290802002
Epoch 1990, val loss: 1.1141257286071777
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7749
Flip ASR: 0.7644/225 nodes
The final ASR:0.69496, 0.14829, Accuracy:0.82840, 0.00761
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10538])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.325496673583984 = 1.9515963792800903 + 1.0 * 8.373900413513184
Epoch 0, val loss: 1.946615219116211
Epoch 10, training loss: 10.314468383789062 = 1.940964698791504 + 1.0 * 8.373503684997559
Epoch 10, val loss: 1.9367458820343018
Epoch 20, training loss: 10.298379898071289 = 1.927998423576355 + 1.0 * 8.370381355285645
Epoch 20, val loss: 1.9242372512817383
Epoch 30, training loss: 10.257671356201172 = 1.9102767705917358 + 1.0 * 8.347394943237305
Epoch 30, val loss: 1.906915307044983
Epoch 40, training loss: 10.087719917297363 = 1.8878672122955322 + 1.0 * 8.19985294342041
Epoch 40, val loss: 1.8858802318572998
Epoch 50, training loss: 9.562095642089844 = 1.86408269405365 + 1.0 * 7.6980133056640625
Epoch 50, val loss: 1.8644907474517822
Epoch 60, training loss: 9.178482055664062 = 1.8435558080673218 + 1.0 * 7.334926128387451
Epoch 60, val loss: 1.8469971418380737
Epoch 70, training loss: 8.770562171936035 = 1.827527642250061 + 1.0 * 6.9430341720581055
Epoch 70, val loss: 1.8333848714828491
Epoch 80, training loss: 8.509160995483398 = 1.8123993873596191 + 1.0 * 6.6967620849609375
Epoch 80, val loss: 1.819860577583313
Epoch 90, training loss: 8.367119789123535 = 1.7953182458877563 + 1.0 * 6.57180118560791
Epoch 90, val loss: 1.804898738861084
Epoch 100, training loss: 8.292662620544434 = 1.776476502418518 + 1.0 * 6.516186237335205
Epoch 100, val loss: 1.7892831563949585
Epoch 110, training loss: 8.218888282775879 = 1.7571362257003784 + 1.0 * 6.461751937866211
Epoch 110, val loss: 1.7736151218414307
Epoch 120, training loss: 8.158915519714355 = 1.737053632736206 + 1.0 * 6.4218621253967285
Epoch 120, val loss: 1.7570269107818604
Epoch 130, training loss: 8.100387573242188 = 1.7151257991790771 + 1.0 * 6.3852620124816895
Epoch 130, val loss: 1.7386462688446045
Epoch 140, training loss: 8.043554306030273 = 1.6899614334106445 + 1.0 * 6.353592872619629
Epoch 140, val loss: 1.7177270650863647
Epoch 150, training loss: 7.990306854248047 = 1.6605907678604126 + 1.0 * 6.329716205596924
Epoch 150, val loss: 1.6938073635101318
Epoch 160, training loss: 7.931769847869873 = 1.6272072792053223 + 1.0 * 6.304562568664551
Epoch 160, val loss: 1.6668237447738647
Epoch 170, training loss: 7.873180389404297 = 1.5886892080307007 + 1.0 * 6.284491062164307
Epoch 170, val loss: 1.6355432271957397
Epoch 180, training loss: 7.814654350280762 = 1.5445142984390259 + 1.0 * 6.270140171051025
Epoch 180, val loss: 1.5995466709136963
Epoch 190, training loss: 7.748616695404053 = 1.495237946510315 + 1.0 * 6.253378868103027
Epoch 190, val loss: 1.559216856956482
Epoch 200, training loss: 7.681583404541016 = 1.441075325012207 + 1.0 * 6.240508079528809
Epoch 200, val loss: 1.5149976015090942
Epoch 210, training loss: 7.614551067352295 = 1.3826605081558228 + 1.0 * 6.231890678405762
Epoch 210, val loss: 1.467315435409546
Epoch 220, training loss: 7.542879581451416 = 1.3218446969985962 + 1.0 * 6.221035003662109
Epoch 220, val loss: 1.4177882671356201
Epoch 230, training loss: 7.471713542938232 = 1.2590765953063965 + 1.0 * 6.212636947631836
Epoch 230, val loss: 1.3669365644454956
Epoch 240, training loss: 7.406089782714844 = 1.1948747634887695 + 1.0 * 6.211215019226074
Epoch 240, val loss: 1.315211296081543
Epoch 250, training loss: 7.3314714431762695 = 1.1307145357131958 + 1.0 * 6.200757026672363
Epoch 250, val loss: 1.2638022899627686
Epoch 260, training loss: 7.261322021484375 = 1.0666404962539673 + 1.0 * 6.194681644439697
Epoch 260, val loss: 1.2129216194152832
Epoch 270, training loss: 7.192757606506348 = 1.0031416416168213 + 1.0 * 6.1896162033081055
Epoch 270, val loss: 1.1627742052078247
Epoch 280, training loss: 7.130404472351074 = 0.9411417841911316 + 1.0 * 6.189262866973877
Epoch 280, val loss: 1.11415433883667
Epoch 290, training loss: 7.064016342163086 = 0.8826367259025574 + 1.0 * 6.181379795074463
Epoch 290, val loss: 1.06839919090271
Epoch 300, training loss: 7.002636909484863 = 0.8270776867866516 + 1.0 * 6.175559043884277
Epoch 300, val loss: 1.0253024101257324
Epoch 310, training loss: 6.953973293304443 = 0.7747178673744202 + 1.0 * 6.179255485534668
Epoch 310, val loss: 0.9849292039871216
Epoch 320, training loss: 6.894207954406738 = 0.7265760898590088 + 1.0 * 6.167632102966309
Epoch 320, val loss: 0.9481244683265686
Epoch 330, training loss: 6.845211029052734 = 0.6823633313179016 + 1.0 * 6.162847518920898
Epoch 330, val loss: 0.9150699377059937
Epoch 340, training loss: 6.802889347076416 = 0.641706645488739 + 1.0 * 6.161182880401611
Epoch 340, val loss: 0.8853545784950256
Epoch 350, training loss: 6.758821487426758 = 0.6046793460845947 + 1.0 * 6.154142379760742
Epoch 350, val loss: 0.8592569828033447
Epoch 360, training loss: 6.721259117126465 = 0.570706307888031 + 1.0 * 6.150552749633789
Epoch 360, val loss: 0.8363713026046753
Epoch 370, training loss: 6.696455955505371 = 0.5394154787063599 + 1.0 * 6.157040596008301
Epoch 370, val loss: 0.8162040114402771
Epoch 380, training loss: 6.6562066078186035 = 0.5107851624488831 + 1.0 * 6.145421504974365
Epoch 380, val loss: 0.7986007332801819
Epoch 390, training loss: 6.625161647796631 = 0.48423537611961365 + 1.0 * 6.140926361083984
Epoch 390, val loss: 0.7832605838775635
Epoch 400, training loss: 6.598236560821533 = 0.4593261480331421 + 1.0 * 6.138910293579102
Epoch 400, val loss: 0.769550085067749
Epoch 410, training loss: 6.568515300750732 = 0.4358731508255005 + 1.0 * 6.1326422691345215
Epoch 410, val loss: 0.7572159767150879
Epoch 420, training loss: 6.543633460998535 = 0.41352659463882446 + 1.0 * 6.1301069259643555
Epoch 420, val loss: 0.7460272312164307
Epoch 430, training loss: 6.52001428604126 = 0.3921889662742615 + 1.0 * 6.1278252601623535
Epoch 430, val loss: 0.7358300089836121
Epoch 440, training loss: 6.496987819671631 = 0.3718235194683075 + 1.0 * 6.12516450881958
Epoch 440, val loss: 0.7265428900718689
Epoch 450, training loss: 6.481728553771973 = 0.3522782623767853 + 1.0 * 6.12945032119751
Epoch 450, val loss: 0.7180758714675903
Epoch 460, training loss: 6.454248905181885 = 0.3335858881473541 + 1.0 * 6.120663166046143
Epoch 460, val loss: 0.7102331519126892
Epoch 470, training loss: 6.432737827301025 = 0.3157247304916382 + 1.0 * 6.117012977600098
Epoch 470, val loss: 0.7034088969230652
Epoch 480, training loss: 6.411619663238525 = 0.29850462079048157 + 1.0 * 6.113114833831787
Epoch 480, val loss: 0.6972693800926208
Epoch 490, training loss: 6.400693893432617 = 0.2818868160247803 + 1.0 * 6.118807315826416
Epoch 490, val loss: 0.6917325854301453
Epoch 500, training loss: 6.377233028411865 = 0.2659856081008911 + 1.0 * 6.111247539520264
Epoch 500, val loss: 0.6870318651199341
Epoch 510, training loss: 6.357399940490723 = 0.2505837082862854 + 1.0 * 6.106816291809082
Epoch 510, val loss: 0.6831193566322327
Epoch 520, training loss: 6.346276760101318 = 0.2356206625699997 + 1.0 * 6.110656261444092
Epoch 520, val loss: 0.6798650026321411
Epoch 530, training loss: 6.326035499572754 = 0.22115711867809296 + 1.0 * 6.1048784255981445
Epoch 530, val loss: 0.6771893501281738
Epoch 540, training loss: 6.3075270652771 = 0.20718125998973846 + 1.0 * 6.100345611572266
Epoch 540, val loss: 0.6754428148269653
Epoch 550, training loss: 6.293774127960205 = 0.19366787374019623 + 1.0 * 6.100106239318848
Epoch 550, val loss: 0.6743921041488647
Epoch 560, training loss: 6.285138130187988 = 0.18074676394462585 + 1.0 * 6.104391574859619
Epoch 560, val loss: 0.6737830638885498
Epoch 570, training loss: 6.266671657562256 = 0.16858232021331787 + 1.0 * 6.098089218139648
Epoch 570, val loss: 0.6742745041847229
Epoch 580, training loss: 6.251425266265869 = 0.15709492564201355 + 1.0 * 6.094330310821533
Epoch 580, val loss: 0.6755713224411011
Epoch 590, training loss: 6.238199710845947 = 0.1463153213262558 + 1.0 * 6.091884613037109
Epoch 590, val loss: 0.6774677038192749
Epoch 600, training loss: 6.238885879516602 = 0.13627728819847107 + 1.0 * 6.102608680725098
Epoch 600, val loss: 0.6800734996795654
Epoch 610, training loss: 6.216277122497559 = 0.1270536631345749 + 1.0 * 6.089223384857178
Epoch 610, val loss: 0.6833893656730652
Epoch 620, training loss: 6.205688953399658 = 0.1185673326253891 + 1.0 * 6.087121486663818
Epoch 620, val loss: 0.687473714351654
Epoch 630, training loss: 6.213622093200684 = 0.11078421026468277 + 1.0 * 6.102838039398193
Epoch 630, val loss: 0.6920833587646484
Epoch 640, training loss: 6.188999176025391 = 0.10368978977203369 + 1.0 * 6.0853095054626465
Epoch 640, val loss: 0.6968923211097717
Epoch 650, training loss: 6.181568622589111 = 0.09722565114498138 + 1.0 * 6.084342956542969
Epoch 650, val loss: 0.7025043964385986
Epoch 660, training loss: 6.17359733581543 = 0.0912899598479271 + 1.0 * 6.0823073387146
Epoch 660, val loss: 0.7083966732025146
Epoch 670, training loss: 6.177896976470947 = 0.08584294468164444 + 1.0 * 6.0920538902282715
Epoch 670, val loss: 0.7144990563392639
Epoch 680, training loss: 6.161824703216553 = 0.08087355643510818 + 1.0 * 6.08095121383667
Epoch 680, val loss: 0.7208718657493591
Epoch 690, training loss: 6.1566972732543945 = 0.07630886137485504 + 1.0 * 6.08038854598999
Epoch 690, val loss: 0.7275745868682861
Epoch 700, training loss: 6.149371147155762 = 0.07210052013397217 + 1.0 * 6.0772705078125
Epoch 700, val loss: 0.7343766689300537
Epoch 710, training loss: 6.147937774658203 = 0.06821439415216446 + 1.0 * 6.079723358154297
Epoch 710, val loss: 0.7413732409477234
Epoch 720, training loss: 6.141338348388672 = 0.06462599337100983 + 1.0 * 6.076712131500244
Epoch 720, val loss: 0.7480999827384949
Epoch 730, training loss: 6.1372270584106445 = 0.061328329145908356 + 1.0 * 6.07589864730835
Epoch 730, val loss: 0.7552455067634583
Epoch 740, training loss: 6.129868507385254 = 0.05825798213481903 + 1.0 * 6.071610450744629
Epoch 740, val loss: 0.7623493075370789
Epoch 750, training loss: 6.130946636199951 = 0.05539876967668533 + 1.0 * 6.075547695159912
Epoch 750, val loss: 0.7693422436714172
Epoch 760, training loss: 6.1253557205200195 = 0.05275886878371239 + 1.0 * 6.072597026824951
Epoch 760, val loss: 0.7763751745223999
Epoch 770, training loss: 6.120545387268066 = 0.05029144883155823 + 1.0 * 6.070253849029541
Epoch 770, val loss: 0.7834183573722839
Epoch 780, training loss: 6.124020576477051 = 0.047997407615184784 + 1.0 * 6.076023101806641
Epoch 780, val loss: 0.7904939651489258
Epoch 790, training loss: 6.11343240737915 = 0.04585545137524605 + 1.0 * 6.067576885223389
Epoch 790, val loss: 0.7972991466522217
Epoch 800, training loss: 6.109332084655762 = 0.043845709413290024 + 1.0 * 6.065486431121826
Epoch 800, val loss: 0.804169774055481
Epoch 810, training loss: 6.106757640838623 = 0.04195815324783325 + 1.0 * 6.0647993087768555
Epoch 810, val loss: 0.8110705614089966
Epoch 820, training loss: 6.11195707321167 = 0.04018745571374893 + 1.0 * 6.071769714355469
Epoch 820, val loss: 0.8176285028457642
Epoch 830, training loss: 6.1050310134887695 = 0.03853752464056015 + 1.0 * 6.066493511199951
Epoch 830, val loss: 0.8242560029029846
Epoch 840, training loss: 6.099649906158447 = 0.03698724880814552 + 1.0 * 6.062662601470947
Epoch 840, val loss: 0.8309491276741028
Epoch 850, training loss: 6.102718830108643 = 0.03552568331360817 + 1.0 * 6.067193031311035
Epoch 850, val loss: 0.8374893665313721
Epoch 860, training loss: 6.098939418792725 = 0.034145575016736984 + 1.0 * 6.064794063568115
Epoch 860, val loss: 0.843553900718689
Epoch 870, training loss: 6.094015598297119 = 0.032855235040187836 + 1.0 * 6.061160564422607
Epoch 870, val loss: 0.8499990701675415
Epoch 880, training loss: 6.089986324310303 = 0.03162956237792969 + 1.0 * 6.058356761932373
Epoch 880, val loss: 0.8562585711479187
Epoch 890, training loss: 6.087397575378418 = 0.030468584969639778 + 1.0 * 6.056929111480713
Epoch 890, val loss: 0.8624392151832581
Epoch 900, training loss: 6.092649936676025 = 0.029363926500082016 + 1.0 * 6.063285827636719
Epoch 900, val loss: 0.8684576153755188
Epoch 910, training loss: 6.092981815338135 = 0.028323588892817497 + 1.0 * 6.064658164978027
Epoch 910, val loss: 0.874216616153717
Epoch 920, training loss: 6.082529067993164 = 0.027344832196831703 + 1.0 * 6.055184364318848
Epoch 920, val loss: 0.8802074193954468
Epoch 930, training loss: 6.080080509185791 = 0.026415076106786728 + 1.0 * 6.053665637969971
Epoch 930, val loss: 0.886125922203064
Epoch 940, training loss: 6.0836381912231445 = 0.025528592988848686 + 1.0 * 6.058109760284424
Epoch 940, val loss: 0.8918813467025757
Epoch 950, training loss: 6.0802903175354 = 0.024685312062501907 + 1.0 * 6.055604934692383
Epoch 950, val loss: 0.8972434997558594
Epoch 960, training loss: 6.077807903289795 = 0.023890549317002296 + 1.0 * 6.053917407989502
Epoch 960, val loss: 0.9028877019882202
Epoch 970, training loss: 6.0734076499938965 = 0.023131104186177254 + 1.0 * 6.050276756286621
Epoch 970, val loss: 0.9084962010383606
Epoch 980, training loss: 6.073764324188232 = 0.02240370586514473 + 1.0 * 6.051360607147217
Epoch 980, val loss: 0.9138848781585693
Epoch 990, training loss: 6.075125694274902 = 0.02171061746776104 + 1.0 * 6.053415298461914
Epoch 990, val loss: 0.9191059470176697
Epoch 1000, training loss: 6.070487022399902 = 0.021054469048976898 + 1.0 * 6.049432754516602
Epoch 1000, val loss: 0.9243696331977844
Epoch 1010, training loss: 6.069073677062988 = 0.020428074523806572 + 1.0 * 6.048645496368408
Epoch 1010, val loss: 0.9297663569450378
Epoch 1020, training loss: 6.067371845245361 = 0.019826257601380348 + 1.0 * 6.047545433044434
Epoch 1020, val loss: 0.934934139251709
Epoch 1030, training loss: 6.07395076751709 = 0.019252028316259384 + 1.0 * 6.054698944091797
Epoch 1030, val loss: 0.9398923516273499
Epoch 1040, training loss: 6.0664167404174805 = 0.018701978027820587 + 1.0 * 6.047714710235596
Epoch 1040, val loss: 0.9448307156562805
Epoch 1050, training loss: 6.063839435577393 = 0.01817803457379341 + 1.0 * 6.045661449432373
Epoch 1050, val loss: 0.9498845934867859
Epoch 1060, training loss: 6.064075946807861 = 0.01767408475279808 + 1.0 * 6.0464019775390625
Epoch 1060, val loss: 0.9547283053398132
Epoch 1070, training loss: 6.066666126251221 = 0.01719013601541519 + 1.0 * 6.049476146697998
Epoch 1070, val loss: 0.9594108462333679
Epoch 1080, training loss: 6.063647270202637 = 0.01673169806599617 + 1.0 * 6.046915531158447
Epoch 1080, val loss: 0.9641326069831848
Epoch 1090, training loss: 6.059846878051758 = 0.016291355714201927 + 1.0 * 6.043555736541748
Epoch 1090, val loss: 0.9689274430274963
Epoch 1100, training loss: 6.059092998504639 = 0.01586766727268696 + 1.0 * 6.043225288391113
Epoch 1100, val loss: 0.9735267758369446
Epoch 1110, training loss: 6.05718469619751 = 0.01545911468565464 + 1.0 * 6.0417256355285645
Epoch 1110, val loss: 0.9780795574188232
Epoch 1120, training loss: 6.0662102699279785 = 0.015064245089888573 + 1.0 * 6.051146030426025
Epoch 1120, val loss: 0.9823981523513794
Epoch 1130, training loss: 6.060446262359619 = 0.014690912328660488 + 1.0 * 6.045755386352539
Epoch 1130, val loss: 0.986885130405426
Epoch 1140, training loss: 6.056506633758545 = 0.014329544268548489 + 1.0 * 6.042177200317383
Epoch 1140, val loss: 0.9912635087966919
Epoch 1150, training loss: 6.055411338806152 = 0.013984110206365585 + 1.0 * 6.041427135467529
Epoch 1150, val loss: 0.9957594275474548
Epoch 1160, training loss: 6.059165000915527 = 0.013648579828441143 + 1.0 * 6.045516490936279
Epoch 1160, val loss: 0.9998871088027954
Epoch 1170, training loss: 6.053909778594971 = 0.01332581415772438 + 1.0 * 6.040584087371826
Epoch 1170, val loss: 1.0040143728256226
Epoch 1180, training loss: 6.0512285232543945 = 0.013015558011829853 + 1.0 * 6.038212776184082
Epoch 1180, val loss: 1.0082896947860718
Epoch 1190, training loss: 6.052355766296387 = 0.012715809047222137 + 1.0 * 6.039639949798584
Epoch 1190, val loss: 1.0124340057373047
Epoch 1200, training loss: 6.051193714141846 = 0.012426435947418213 + 1.0 * 6.038767337799072
Epoch 1200, val loss: 1.0164390802383423
Epoch 1210, training loss: 6.052835941314697 = 0.012147639878094196 + 1.0 * 6.040688514709473
Epoch 1210, val loss: 1.0204038619995117
Epoch 1220, training loss: 6.048916816711426 = 0.011879120953381062 + 1.0 * 6.0370378494262695
Epoch 1220, val loss: 1.0243983268737793
Epoch 1230, training loss: 6.04970121383667 = 0.011620384640991688 + 1.0 * 6.03808069229126
Epoch 1230, val loss: 1.028388261795044
Epoch 1240, training loss: 6.056097984313965 = 0.011371306143701077 + 1.0 * 6.044726848602295
Epoch 1240, val loss: 1.0321933031082153
Epoch 1250, training loss: 6.045403003692627 = 0.01112914178520441 + 1.0 * 6.034273624420166
Epoch 1250, val loss: 1.0358514785766602
Epoch 1260, training loss: 6.043913841247559 = 0.010897242464125156 + 1.0 * 6.033016681671143
Epoch 1260, val loss: 1.03973388671875
Epoch 1270, training loss: 6.043322563171387 = 0.010671117343008518 + 1.0 * 6.032651424407959
Epoch 1270, val loss: 1.0435388088226318
Epoch 1280, training loss: 6.049768924713135 = 0.010451151058077812 + 1.0 * 6.039317607879639
Epoch 1280, val loss: 1.0471625328063965
Epoch 1290, training loss: 6.044766426086426 = 0.010238587856292725 + 1.0 * 6.034527778625488
Epoch 1290, val loss: 1.0507241487503052
Epoch 1300, training loss: 6.043265342712402 = 0.010033460333943367 + 1.0 * 6.033231735229492
Epoch 1300, val loss: 1.0542978048324585
Epoch 1310, training loss: 6.046523571014404 = 0.009835279546678066 + 1.0 * 6.036688327789307
Epoch 1310, val loss: 1.0580402612686157
Epoch 1320, training loss: 6.040102005004883 = 0.009643254801630974 + 1.0 * 6.030458927154541
Epoch 1320, val loss: 1.0613514184951782
Epoch 1330, training loss: 6.040210247039795 = 0.009457455016672611 + 1.0 * 6.030752658843994
Epoch 1330, val loss: 1.0648831129074097
Epoch 1340, training loss: 6.039315223693848 = 0.009277054108679295 + 1.0 * 6.030038356781006
Epoch 1340, val loss: 1.0683969259262085
Epoch 1350, training loss: 6.054623126983643 = 0.009100268594920635 + 1.0 * 6.045522689819336
Epoch 1350, val loss: 1.0716279745101929
Epoch 1360, training loss: 6.044432640075684 = 0.008933120407164097 + 1.0 * 6.035499572753906
Epoch 1360, val loss: 1.0749698877334595
Epoch 1370, training loss: 6.038257122039795 = 0.008768517524003983 + 1.0 * 6.029488563537598
Epoch 1370, val loss: 1.0783528089523315
Epoch 1380, training loss: 6.036690711975098 = 0.008609684184193611 + 1.0 * 6.028080940246582
Epoch 1380, val loss: 1.0817828178405762
Epoch 1390, training loss: 6.047212600708008 = 0.008454709313809872 + 1.0 * 6.038757801055908
Epoch 1390, val loss: 1.0850532054901123
Epoch 1400, training loss: 6.0391764640808105 = 0.008303080685436726 + 1.0 * 6.0308732986450195
Epoch 1400, val loss: 1.0879613161087036
Epoch 1410, training loss: 6.037940502166748 = 0.008157163858413696 + 1.0 * 6.029783248901367
Epoch 1410, val loss: 1.091249942779541
Epoch 1420, training loss: 6.035576343536377 = 0.00801492016762495 + 1.0 * 6.027561187744141
Epoch 1420, val loss: 1.0944395065307617
Epoch 1430, training loss: 6.034156322479248 = 0.007876373827457428 + 1.0 * 6.026279926300049
Epoch 1430, val loss: 1.0976293087005615
Epoch 1440, training loss: 6.040372848510742 = 0.007741464301943779 + 1.0 * 6.0326313972473145
Epoch 1440, val loss: 1.100716233253479
Epoch 1450, training loss: 6.033448219299316 = 0.007609050255268812 + 1.0 * 6.025839328765869
Epoch 1450, val loss: 1.103622317314148
Epoch 1460, training loss: 6.035852909088135 = 0.007481277920305729 + 1.0 * 6.028371810913086
Epoch 1460, val loss: 1.1066919565200806
Epoch 1470, training loss: 6.034039497375488 = 0.007357545662671328 + 1.0 * 6.026681900024414
Epoch 1470, val loss: 1.1097452640533447
Epoch 1480, training loss: 6.039993762969971 = 0.00723640713840723 + 1.0 * 6.03275728225708
Epoch 1480, val loss: 1.1125694513320923
Epoch 1490, training loss: 6.033031940460205 = 0.007119216024875641 + 1.0 * 6.025912761688232
Epoch 1490, val loss: 1.1154099702835083
Epoch 1500, training loss: 6.030694961547852 = 0.007005635183304548 + 1.0 * 6.023689270019531
Epoch 1500, val loss: 1.1183990240097046
Epoch 1510, training loss: 6.03100061416626 = 0.006893988698720932 + 1.0 * 6.024106502532959
Epoch 1510, val loss: 1.1213161945343018
Epoch 1520, training loss: 6.03420352935791 = 0.0067848144099116325 + 1.0 * 6.027418613433838
Epoch 1520, val loss: 1.1239674091339111
Epoch 1530, training loss: 6.031454563140869 = 0.006679392419755459 + 1.0 * 6.02477502822876
Epoch 1530, val loss: 1.1266999244689941
Epoch 1540, training loss: 6.0284576416015625 = 0.006577000021934509 + 1.0 * 6.021880626678467
Epoch 1540, val loss: 1.1295685768127441
Epoch 1550, training loss: 6.028621673583984 = 0.006476685870438814 + 1.0 * 6.022144794464111
Epoch 1550, val loss: 1.1324396133422852
Epoch 1560, training loss: 6.03401517868042 = 0.00637868931517005 + 1.0 * 6.027636528015137
Epoch 1560, val loss: 1.1351239681243896
Epoch 1570, training loss: 6.0336012840271 = 0.006283091846853495 + 1.0 * 6.027318000793457
Epoch 1570, val loss: 1.1376917362213135
Epoch 1580, training loss: 6.029365062713623 = 0.006189705803990364 + 1.0 * 6.023175239562988
Epoch 1580, val loss: 1.1403203010559082
Epoch 1590, training loss: 6.027168273925781 = 0.006098938174545765 + 1.0 * 6.021069526672363
Epoch 1590, val loss: 1.1429911851882935
Epoch 1600, training loss: 6.025180816650391 = 0.006010073237121105 + 1.0 * 6.019170761108398
Epoch 1600, val loss: 1.145665168762207
Epoch 1610, training loss: 6.033146858215332 = 0.005923351272940636 + 1.0 * 6.027223587036133
Epoch 1610, val loss: 1.1483383178710938
Epoch 1620, training loss: 6.028328895568848 = 0.005837981589138508 + 1.0 * 6.022490978240967
Epoch 1620, val loss: 1.150499939918518
Epoch 1630, training loss: 6.024569511413574 = 0.005755752325057983 + 1.0 * 6.018813610076904
Epoch 1630, val loss: 1.1531877517700195
Epoch 1640, training loss: 6.023691177368164 = 0.005675280001014471 + 1.0 * 6.0180158615112305
Epoch 1640, val loss: 1.1557294130325317
Epoch 1650, training loss: 6.0253520011901855 = 0.005595972761511803 + 1.0 * 6.019755840301514
Epoch 1650, val loss: 1.1582540273666382
Epoch 1660, training loss: 6.02669095993042 = 0.005518646910786629 + 1.0 * 6.021172523498535
Epoch 1660, val loss: 1.160568356513977
Epoch 1670, training loss: 6.025321960449219 = 0.005443650297820568 + 1.0 * 6.019878387451172
Epoch 1670, val loss: 1.1629700660705566
Epoch 1680, training loss: 6.024967193603516 = 0.005370082799345255 + 1.0 * 6.019597053527832
Epoch 1680, val loss: 1.1653777360916138
Epoch 1690, training loss: 6.02410888671875 = 0.005298417527228594 + 1.0 * 6.018810272216797
Epoch 1690, val loss: 1.167934536933899
Epoch 1700, training loss: 6.026582717895508 = 0.005228468682616949 + 1.0 * 6.0213541984558105
Epoch 1700, val loss: 1.1702309846878052
Epoch 1710, training loss: 6.022357940673828 = 0.005159171763807535 + 1.0 * 6.01719856262207
Epoch 1710, val loss: 1.1724965572357178
Epoch 1720, training loss: 6.020817279815674 = 0.005091811064630747 + 1.0 * 6.015725612640381
Epoch 1720, val loss: 1.1749632358551025
Epoch 1730, training loss: 6.0239949226379395 = 0.0050256578251719475 + 1.0 * 6.0189690589904785
Epoch 1730, val loss: 1.1773148775100708
Epoch 1740, training loss: 6.025792598724365 = 0.0049610501155257225 + 1.0 * 6.02083158493042
Epoch 1740, val loss: 1.1794350147247314
Epoch 1750, training loss: 6.021015644073486 = 0.004897965118288994 + 1.0 * 6.016117572784424
Epoch 1750, val loss: 1.1816213130950928
Epoch 1760, training loss: 6.020264625549316 = 0.0048367176204919815 + 1.0 * 6.015428066253662
Epoch 1760, val loss: 1.1839617490768433
Epoch 1770, training loss: 6.021190643310547 = 0.004776339512318373 + 1.0 * 6.016414165496826
Epoch 1770, val loss: 1.1862022876739502
Epoch 1780, training loss: 6.0231852531433105 = 0.004717031493782997 + 1.0 * 6.018468379974365
Epoch 1780, val loss: 1.1883816719055176
Epoch 1790, training loss: 6.022183895111084 = 0.004659175407141447 + 1.0 * 6.017524719238281
Epoch 1790, val loss: 1.1905487775802612
Epoch 1800, training loss: 6.021300792694092 = 0.004602515604346991 + 1.0 * 6.016698360443115
Epoch 1800, val loss: 1.1927285194396973
Epoch 1810, training loss: 6.020811080932617 = 0.004547187592834234 + 1.0 * 6.016263961791992
Epoch 1810, val loss: 1.1948012113571167
Epoch 1820, training loss: 6.018194675445557 = 0.004492511507123709 + 1.0 * 6.013702392578125
Epoch 1820, val loss: 1.1968798637390137
Epoch 1830, training loss: 6.017198085784912 = 0.004439199343323708 + 1.0 * 6.012758731842041
Epoch 1830, val loss: 1.1990599632263184
Epoch 1840, training loss: 6.019554138183594 = 0.004386205691844225 + 1.0 * 6.015167713165283
Epoch 1840, val loss: 1.2011253833770752
Epoch 1850, training loss: 6.01918888092041 = 0.004334527999162674 + 1.0 * 6.014854431152344
Epoch 1850, val loss: 1.2030284404754639
Epoch 1860, training loss: 6.018448829650879 = 0.004284936003386974 + 1.0 * 6.014163970947266
Epoch 1860, val loss: 1.2050957679748535
Epoch 1870, training loss: 6.016350269317627 = 0.0042359852232038975 + 1.0 * 6.01211404800415
Epoch 1870, val loss: 1.2072027921676636
Epoch 1880, training loss: 6.015501976013184 = 0.004187853541225195 + 1.0 * 6.0113139152526855
Epoch 1880, val loss: 1.209254503250122
Epoch 1890, training loss: 6.019956588745117 = 0.004139768425375223 + 1.0 * 6.015816688537598
Epoch 1890, val loss: 1.211138129234314
Epoch 1900, training loss: 6.017294406890869 = 0.004093321040272713 + 1.0 * 6.0132012367248535
Epoch 1900, val loss: 1.2129679918289185
Epoch 1910, training loss: 6.01444149017334 = 0.004048111382871866 + 1.0 * 6.010393142700195
Epoch 1910, val loss: 1.2149831056594849
Epoch 1920, training loss: 6.014116287231445 = 0.004003824666142464 + 1.0 * 6.010112285614014
Epoch 1920, val loss: 1.2170857191085815
Epoch 1930, training loss: 6.015076637268066 = 0.003959959838539362 + 1.0 * 6.0111165046691895
Epoch 1930, val loss: 1.2190706729888916
Epoch 1940, training loss: 6.019383907318115 = 0.003916375804692507 + 1.0 * 6.015467643737793
Epoch 1940, val loss: 1.220895528793335
Epoch 1950, training loss: 6.019000053405762 = 0.003873735899105668 + 1.0 * 6.0151262283325195
Epoch 1950, val loss: 1.2225552797317505
Epoch 1960, training loss: 6.018311977386475 = 0.00383243290707469 + 1.0 * 6.014479637145996
Epoch 1960, val loss: 1.2244805097579956
Epoch 1970, training loss: 6.017854690551758 = 0.0037920435424894094 + 1.0 * 6.014062881469727
Epoch 1970, val loss: 1.2262526750564575
Epoch 1980, training loss: 6.013302803039551 = 0.003752534743398428 + 1.0 * 6.009550094604492
Epoch 1980, val loss: 1.2281601428985596
Epoch 1990, training loss: 6.012533664703369 = 0.003713448764756322 + 1.0 * 6.008820056915283
Epoch 1990, val loss: 1.230017900466919
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5203
Flip ASR: 0.4356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.308777809143066 = 1.934841513633728 + 1.0 * 8.373936653137207
Epoch 0, val loss: 1.9246066808700562
Epoch 10, training loss: 10.298408508300781 = 1.9247186183929443 + 1.0 * 8.373689651489258
Epoch 10, val loss: 1.9146544933319092
Epoch 20, training loss: 10.28408432006836 = 1.9121218919754028 + 1.0 * 8.371962547302246
Epoch 20, val loss: 1.9020577669143677
Epoch 30, training loss: 10.253267288208008 = 1.8947539329528809 + 1.0 * 8.358512878417969
Epoch 30, val loss: 1.8843473196029663
Epoch 40, training loss: 10.147897720336914 = 1.8715341091156006 + 1.0 * 8.276363372802734
Epoch 40, val loss: 1.8611339330673218
Epoch 50, training loss: 9.662245750427246 = 1.8466449975967407 + 1.0 * 7.815600395202637
Epoch 50, val loss: 1.837502360343933
Epoch 60, training loss: 9.137661933898926 = 1.8234775066375732 + 1.0 * 7.314184188842773
Epoch 60, val loss: 1.8160334825515747
Epoch 70, training loss: 8.704696655273438 = 1.802567720413208 + 1.0 * 6.902129173278809
Epoch 70, val loss: 1.7955288887023926
Epoch 80, training loss: 8.510573387145996 = 1.7804712057113647 + 1.0 * 6.7301025390625
Epoch 80, val loss: 1.7743335962295532
Epoch 90, training loss: 8.396405220031738 = 1.7573468685150146 + 1.0 * 6.639058589935303
Epoch 90, val loss: 1.7532516717910767
Epoch 100, training loss: 8.304043769836426 = 1.732688307762146 + 1.0 * 6.57135534286499
Epoch 100, val loss: 1.731408953666687
Epoch 110, training loss: 8.222238540649414 = 1.7065908908843994 + 1.0 * 6.5156474113464355
Epoch 110, val loss: 1.7086914777755737
Epoch 120, training loss: 8.157246589660645 = 1.677186131477356 + 1.0 * 6.48006010055542
Epoch 120, val loss: 1.6836215257644653
Epoch 130, training loss: 8.089900970458984 = 1.6433185338974 + 1.0 * 6.446582794189453
Epoch 130, val loss: 1.6550438404083252
Epoch 140, training loss: 8.025336265563965 = 1.6037296056747437 + 1.0 * 6.42160701751709
Epoch 140, val loss: 1.6224706172943115
Epoch 150, training loss: 7.956368446350098 = 1.5574982166290283 + 1.0 * 6.39886999130249
Epoch 150, val loss: 1.5852354764938354
Epoch 160, training loss: 7.880113124847412 = 1.504490852355957 + 1.0 * 6.375622272491455
Epoch 160, val loss: 1.543509840965271
Epoch 170, training loss: 7.8039398193359375 = 1.445598840713501 + 1.0 * 6.358341217041016
Epoch 170, val loss: 1.497975468635559
Epoch 180, training loss: 7.7191572189331055 = 1.3850282430648804 + 1.0 * 6.3341288566589355
Epoch 180, val loss: 1.4527090787887573
Epoch 190, training loss: 7.637025833129883 = 1.324275255203247 + 1.0 * 6.312750339508057
Epoch 190, val loss: 1.408278465270996
Epoch 200, training loss: 7.558399677276611 = 1.2642055749893188 + 1.0 * 6.294194221496582
Epoch 200, val loss: 1.3656432628631592
Epoch 210, training loss: 7.490148544311523 = 1.2063486576080322 + 1.0 * 6.28380012512207
Epoch 210, val loss: 1.325635313987732
Epoch 220, training loss: 7.419936180114746 = 1.152949333190918 + 1.0 * 6.266986846923828
Epoch 220, val loss: 1.289917230606079
Epoch 230, training loss: 7.356410026550293 = 1.102921724319458 + 1.0 * 6.253488063812256
Epoch 230, val loss: 1.2567514181137085
Epoch 240, training loss: 7.2974700927734375 = 1.0554713010787964 + 1.0 * 6.241998672485352
Epoch 240, val loss: 1.2252076864242554
Epoch 250, training loss: 7.241274356842041 = 1.009766697883606 + 1.0 * 6.231507778167725
Epoch 250, val loss: 1.194682002067566
Epoch 260, training loss: 7.193181991577148 = 0.9655624032020569 + 1.0 * 6.227619647979736
Epoch 260, val loss: 1.1648720502853394
Epoch 270, training loss: 7.1415534019470215 = 0.9236702919006348 + 1.0 * 6.217883110046387
Epoch 270, val loss: 1.1358438730239868
Epoch 280, training loss: 7.090312480926514 = 0.8829649686813354 + 1.0 * 6.207347393035889
Epoch 280, val loss: 1.1074622869491577
Epoch 290, training loss: 7.04268741607666 = 0.8429303765296936 + 1.0 * 6.199757099151611
Epoch 290, val loss: 1.0792362689971924
Epoch 300, training loss: 7.006546974182129 = 0.8033015727996826 + 1.0 * 6.203245639801025
Epoch 300, val loss: 1.0510529279708862
Epoch 310, training loss: 6.954133033752441 = 0.7645906209945679 + 1.0 * 6.189542293548584
Epoch 310, val loss: 1.02349853515625
Epoch 320, training loss: 6.9098687171936035 = 0.726773738861084 + 1.0 * 6.1830949783325195
Epoch 320, val loss: 0.9968129396438599
Epoch 330, training loss: 6.866636753082275 = 0.6898411512374878 + 1.0 * 6.176795482635498
Epoch 330, val loss: 0.9710374474525452
Epoch 340, training loss: 6.831042766571045 = 0.6542650461196899 + 1.0 * 6.1767778396606445
Epoch 340, val loss: 0.9467669129371643
Epoch 350, training loss: 6.787136077880859 = 0.6205341219902039 + 1.0 * 6.16660213470459
Epoch 350, val loss: 0.924358069896698
Epoch 360, training loss: 6.751669406890869 = 0.5881837010383606 + 1.0 * 6.163485527038574
Epoch 360, val loss: 0.9039121866226196
Epoch 370, training loss: 6.718476295471191 = 0.5576494336128235 + 1.0 * 6.160826683044434
Epoch 370, val loss: 0.8852980732917786
Epoch 380, training loss: 6.683858394622803 = 0.5288742780685425 + 1.0 * 6.154983997344971
Epoch 380, val loss: 0.8690211772918701
Epoch 390, training loss: 6.651484966278076 = 0.50153648853302 + 1.0 * 6.149948596954346
Epoch 390, val loss: 0.8545883297920227
Epoch 400, training loss: 6.621156215667725 = 0.4755161702632904 + 1.0 * 6.145639896392822
Epoch 400, val loss: 0.8417819738388062
Epoch 410, training loss: 6.599242210388184 = 0.4510318338871002 + 1.0 * 6.148210525512695
Epoch 410, val loss: 0.8304782509803772
Epoch 420, training loss: 6.56896448135376 = 0.4282958209514618 + 1.0 * 6.140668869018555
Epoch 420, val loss: 0.8207581043243408
Epoch 430, training loss: 6.543196678161621 = 0.40689894556999207 + 1.0 * 6.136297702789307
Epoch 430, val loss: 0.8125866651535034
Epoch 440, training loss: 6.519244194030762 = 0.38670259714126587 + 1.0 * 6.132541656494141
Epoch 440, val loss: 0.8055877089500427
Epoch 450, training loss: 6.496427059173584 = 0.36751073598861694 + 1.0 * 6.128916263580322
Epoch 450, val loss: 0.7996909022331238
Epoch 460, training loss: 6.475116729736328 = 0.34926486015319824 + 1.0 * 6.125851631164551
Epoch 460, val loss: 0.7947921752929688
Epoch 470, training loss: 6.475516319274902 = 0.3319224715232849 + 1.0 * 6.143593788146973
Epoch 470, val loss: 0.790720522403717
Epoch 480, training loss: 6.4426069259643555 = 0.31554344296455383 + 1.0 * 6.127063274383545
Epoch 480, val loss: 0.7873232960700989
Epoch 490, training loss: 6.419064044952393 = 0.2999981641769409 + 1.0 * 6.119065761566162
Epoch 490, val loss: 0.7849375605583191
Epoch 500, training loss: 6.401932239532471 = 0.28509941697120667 + 1.0 * 6.116832733154297
Epoch 500, val loss: 0.7831472754478455
Epoch 510, training loss: 6.387997150421143 = 0.27075761556625366 + 1.0 * 6.117239475250244
Epoch 510, val loss: 0.7819268703460693
Epoch 520, training loss: 6.372861862182617 = 0.257046639919281 + 1.0 * 6.115815162658691
Epoch 520, val loss: 0.7811428308486938
Epoch 530, training loss: 6.354323387145996 = 0.24390316009521484 + 1.0 * 6.110420227050781
Epoch 530, val loss: 0.7809349298477173
Epoch 540, training loss: 6.339052677154541 = 0.23127637803554535 + 1.0 * 6.107776165008545
Epoch 540, val loss: 0.7812743782997131
Epoch 550, training loss: 6.326275825500488 = 0.219151571393013 + 1.0 * 6.107124328613281
Epoch 550, val loss: 0.7820967435836792
Epoch 560, training loss: 6.314958095550537 = 0.2075781524181366 + 1.0 * 6.107379913330078
Epoch 560, val loss: 0.7832205295562744
Epoch 570, training loss: 6.300480365753174 = 0.19662721455097198 + 1.0 * 6.103853225708008
Epoch 570, val loss: 0.7848526239395142
Epoch 580, training loss: 6.287504196166992 = 0.18625573813915253 + 1.0 * 6.101248264312744
Epoch 580, val loss: 0.7869434356689453
Epoch 590, training loss: 6.275756359100342 = 0.17646758258342743 + 1.0 * 6.0992889404296875
Epoch 590, val loss: 0.7894691824913025
Epoch 600, training loss: 6.269000053405762 = 0.1672510802745819 + 1.0 * 6.101748943328857
Epoch 600, val loss: 0.7923411130905151
Epoch 610, training loss: 6.259453773498535 = 0.15865685045719147 + 1.0 * 6.100796699523926
Epoch 610, val loss: 0.7954574823379517
Epoch 620, training loss: 6.246161937713623 = 0.15062010288238525 + 1.0 * 6.095541954040527
Epoch 620, val loss: 0.7989657521247864
Epoch 630, training loss: 6.239364147186279 = 0.14309793710708618 + 1.0 * 6.096266269683838
Epoch 630, val loss: 0.80275958776474
Epoch 640, training loss: 6.22825288772583 = 0.1360739916563034 + 1.0 * 6.092178821563721
Epoch 640, val loss: 0.8067499995231628
Epoch 650, training loss: 6.221429824829102 = 0.12949682772159576 + 1.0 * 6.091932773590088
Epoch 650, val loss: 0.8111039996147156
Epoch 660, training loss: 6.215978145599365 = 0.1233288124203682 + 1.0 * 6.092649459838867
Epoch 660, val loss: 0.8154957890510559
Epoch 670, training loss: 6.2055511474609375 = 0.11755924671888351 + 1.0 * 6.087991714477539
Epoch 670, val loss: 0.8200132846832275
Epoch 680, training loss: 6.198631763458252 = 0.11215755343437195 + 1.0 * 6.086474418640137
Epoch 680, val loss: 0.8248001933097839
Epoch 690, training loss: 6.1973066329956055 = 0.10707204788923264 + 1.0 * 6.090234756469727
Epoch 690, val loss: 0.8296652436256409
Epoch 700, training loss: 6.192783832550049 = 0.10230152308940887 + 1.0 * 6.090482234954834
Epoch 700, val loss: 0.8343265056610107
Epoch 710, training loss: 6.182623863220215 = 0.09784144163131714 + 1.0 * 6.084782600402832
Epoch 710, val loss: 0.8392930626869202
Epoch 720, training loss: 6.174874305725098 = 0.09362141788005829 + 1.0 * 6.0812530517578125
Epoch 720, val loss: 0.8444436192512512
Epoch 730, training loss: 6.180230140686035 = 0.08962655067443848 + 1.0 * 6.090603351593018
Epoch 730, val loss: 0.849631130695343
Epoch 740, training loss: 6.165660858154297 = 0.08584154397249222 + 1.0 * 6.079819202423096
Epoch 740, val loss: 0.8547787666320801
Epoch 750, training loss: 6.161520957946777 = 0.08225943148136139 + 1.0 * 6.079261302947998
Epoch 750, val loss: 0.8601770997047424
Epoch 760, training loss: 6.156373023986816 = 0.07885033637285233 + 1.0 * 6.0775227546691895
Epoch 760, val loss: 0.8653134107589722
Epoch 770, training loss: 6.151638507843018 = 0.07560927420854568 + 1.0 * 6.076029300689697
Epoch 770, val loss: 0.8706889152526855
Epoch 780, training loss: 6.146543979644775 = 0.07250356674194336 + 1.0 * 6.074040412902832
Epoch 780, val loss: 0.8761677145957947
Epoch 790, training loss: 6.1413350105285645 = 0.0695236399769783 + 1.0 * 6.071811199188232
Epoch 790, val loss: 0.8816981315612793
Epoch 800, training loss: 6.143068313598633 = 0.06665512919425964 + 1.0 * 6.076413154602051
Epoch 800, val loss: 0.8872761130332947
Epoch 810, training loss: 6.141215801239014 = 0.06390270590782166 + 1.0 * 6.07731294631958
Epoch 810, val loss: 0.8926604986190796
Epoch 820, training loss: 6.132472038269043 = 0.061273735016584396 + 1.0 * 6.071198463439941
Epoch 820, val loss: 0.8982808589935303
Epoch 830, training loss: 6.1343183517456055 = 0.05875058472156525 + 1.0 * 6.075567722320557
Epoch 830, val loss: 0.9038882851600647
Epoch 840, training loss: 6.124175071716309 = 0.056344565004110336 + 1.0 * 6.067830562591553
Epoch 840, val loss: 0.9095848202705383
Epoch 850, training loss: 6.120909214019775 = 0.05404176935553551 + 1.0 * 6.066867351531982
Epoch 850, val loss: 0.9154129028320312
Epoch 860, training loss: 6.11683988571167 = 0.05184385925531387 + 1.0 * 6.064996242523193
Epoch 860, val loss: 0.9213567972183228
Epoch 870, training loss: 6.1237473487854 = 0.04975588619709015 + 1.0 * 6.073991298675537
Epoch 870, val loss: 0.927298367023468
Epoch 880, training loss: 6.111599922180176 = 0.04778442159295082 + 1.0 * 6.063815593719482
Epoch 880, val loss: 0.9333010315895081
Epoch 890, training loss: 6.12842321395874 = 0.045921698212623596 + 1.0 * 6.082501411437988
Epoch 890, val loss: 0.9394280910491943
Epoch 900, training loss: 6.111278533935547 = 0.044154342263936996 + 1.0 * 6.067124366760254
Epoch 900, val loss: 0.9452946782112122
Epoch 910, training loss: 6.103500843048096 = 0.04248879477381706 + 1.0 * 6.061012268066406
Epoch 910, val loss: 0.9515418410301208
Epoch 920, training loss: 6.100344657897949 = 0.04089498519897461 + 1.0 * 6.059449672698975
Epoch 920, val loss: 0.9577707052230835
Epoch 930, training loss: 6.098148345947266 = 0.03937774524092674 + 1.0 * 6.058770656585693
Epoch 930, val loss: 0.9639875888824463
Epoch 940, training loss: 6.106988430023193 = 0.03793085366487503 + 1.0 * 6.069057464599609
Epoch 940, val loss: 0.970140814781189
Epoch 950, training loss: 6.100297927856445 = 0.03656351566314697 + 1.0 * 6.063734531402588
Epoch 950, val loss: 0.976166307926178
Epoch 960, training loss: 6.0947394371032715 = 0.03526394069194794 + 1.0 * 6.059475421905518
Epoch 960, val loss: 0.9821456074714661
Epoch 970, training loss: 6.089603900909424 = 0.03402434661984444 + 1.0 * 6.055579662322998
Epoch 970, val loss: 0.988197386264801
Epoch 980, training loss: 6.0910964012146 = 0.032839033752679825 + 1.0 * 6.058257579803467
Epoch 980, val loss: 0.9942548871040344
Epoch 990, training loss: 6.088803291320801 = 0.031706832349300385 + 1.0 * 6.057096481323242
Epoch 990, val loss: 0.9999545812606812
Epoch 1000, training loss: 6.085175037384033 = 0.0306283887475729 + 1.0 * 6.05454683303833
Epoch 1000, val loss: 1.0058238506317139
Epoch 1010, training loss: 6.083070278167725 = 0.029603583738207817 + 1.0 * 6.053466796875
Epoch 1010, val loss: 1.0117534399032593
Epoch 1020, training loss: 6.079908847808838 = 0.02861928753554821 + 1.0 * 6.0512895584106445
Epoch 1020, val loss: 1.0175527334213257
Epoch 1030, training loss: 6.084108352661133 = 0.027679193764925003 + 1.0 * 6.056429386138916
Epoch 1030, val loss: 1.023314118385315
Epoch 1040, training loss: 6.082845687866211 = 0.02678641863167286 + 1.0 * 6.05605936050415
Epoch 1040, val loss: 1.028714656829834
Epoch 1050, training loss: 6.077815055847168 = 0.025934718549251556 + 1.0 * 6.051880359649658
Epoch 1050, val loss: 1.0341848134994507
Epoch 1060, training loss: 6.074591159820557 = 0.025123553350567818 + 1.0 * 6.04946756362915
Epoch 1060, val loss: 1.0397953987121582
Epoch 1070, training loss: 6.078362464904785 = 0.024349147453904152 + 1.0 * 6.054013252258301
Epoch 1070, val loss: 1.0451750755310059
Epoch 1080, training loss: 6.072417259216309 = 0.023612402379512787 + 1.0 * 6.048804759979248
Epoch 1080, val loss: 1.050435185432434
Epoch 1090, training loss: 6.077856540679932 = 0.02290724776685238 + 1.0 * 6.0549492835998535
Epoch 1090, val loss: 1.0556493997573853
Epoch 1100, training loss: 6.0688862800598145 = 0.022232700139284134 + 1.0 * 6.046653747558594
Epoch 1100, val loss: 1.0608618259429932
Epoch 1110, training loss: 6.066308975219727 = 0.02158379554748535 + 1.0 * 6.04472541809082
Epoch 1110, val loss: 1.066125750541687
Epoch 1120, training loss: 6.068054676055908 = 0.020958079025149345 + 1.0 * 6.0470967292785645
Epoch 1120, val loss: 1.0713392496109009
Epoch 1130, training loss: 6.065267086029053 = 0.020351922139525414 + 1.0 * 6.044915199279785
Epoch 1130, val loss: 1.0761311054229736
Epoch 1140, training loss: 6.062306880950928 = 0.01976696215569973 + 1.0 * 6.042540073394775
Epoch 1140, val loss: 1.081088662147522
Epoch 1150, training loss: 6.060817241668701 = 0.01920037716627121 + 1.0 * 6.041616916656494
Epoch 1150, val loss: 1.0861624479293823
Epoch 1160, training loss: 6.060089111328125 = 0.01864589750766754 + 1.0 * 6.041443347930908
Epoch 1160, val loss: 1.0911649465560913
Epoch 1170, training loss: 6.065189838409424 = 0.018109256401658058 + 1.0 * 6.0470805168151855
Epoch 1170, val loss: 1.0961005687713623
Epoch 1180, training loss: 6.0606865882873535 = 0.017590949311852455 + 1.0 * 6.043095588684082
Epoch 1180, val loss: 1.1007633209228516
Epoch 1190, training loss: 6.061784744262695 = 0.017095543444156647 + 1.0 * 6.044689178466797
Epoch 1190, val loss: 1.105519413948059
Epoch 1200, training loss: 6.056884765625 = 0.016620006412267685 + 1.0 * 6.04026460647583
Epoch 1200, val loss: 1.1103088855743408
Epoch 1210, training loss: 6.060868263244629 = 0.016162650659680367 + 1.0 * 6.044705390930176
Epoch 1210, val loss: 1.1149624586105347
Epoch 1220, training loss: 6.055724620819092 = 0.015722155570983887 + 1.0 * 6.040002346038818
Epoch 1220, val loss: 1.119426965713501
Epoch 1230, training loss: 6.054903984069824 = 0.015298137441277504 + 1.0 * 6.039605617523193
Epoch 1230, val loss: 1.1240267753601074
Epoch 1240, training loss: 6.062185287475586 = 0.014888805337250233 + 1.0 * 6.047296524047852
Epoch 1240, val loss: 1.1282386779785156
Epoch 1250, training loss: 6.051902770996094 = 0.014505635015666485 + 1.0 * 6.0373969078063965
Epoch 1250, val loss: 1.132456660270691
Epoch 1260, training loss: 6.051229000091553 = 0.014133861288428307 + 1.0 * 6.037095069885254
Epoch 1260, val loss: 1.1369813680648804
Epoch 1270, training loss: 6.04848051071167 = 0.013775614090263844 + 1.0 * 6.034704685211182
Epoch 1270, val loss: 1.141343593597412
Epoch 1280, training loss: 6.047311782836914 = 0.013429342769086361 + 1.0 * 6.0338826179504395
Epoch 1280, val loss: 1.1456876993179321
Epoch 1290, training loss: 6.055970191955566 = 0.013095094822347164 + 1.0 * 6.042875289916992
Epoch 1290, val loss: 1.149956464767456
Epoch 1300, training loss: 6.050563812255859 = 0.012771587818861008 + 1.0 * 6.037792205810547
Epoch 1300, val loss: 1.1538166999816895
Epoch 1310, training loss: 6.0490570068359375 = 0.01246502622961998 + 1.0 * 6.03659200668335
Epoch 1310, val loss: 1.1579821109771729
Epoch 1320, training loss: 6.044871807098389 = 0.012167382054030895 + 1.0 * 6.0327043533325195
Epoch 1320, val loss: 1.1620677709579468
Epoch 1330, training loss: 6.043822765350342 = 0.01188083365559578 + 1.0 * 6.031941890716553
Epoch 1330, val loss: 1.1661639213562012
Epoch 1340, training loss: 6.0442304611206055 = 0.011601941660046577 + 1.0 * 6.032628536224365
Epoch 1340, val loss: 1.1701619625091553
Epoch 1350, training loss: 6.044521808624268 = 0.011334140785038471 + 1.0 * 6.0331878662109375
Epoch 1350, val loss: 1.1740835905075073
Epoch 1360, training loss: 6.052905082702637 = 0.011074843816459179 + 1.0 * 6.041830062866211
Epoch 1360, val loss: 1.1779048442840576
Epoch 1370, training loss: 6.049060821533203 = 0.010825013741850853 + 1.0 * 6.038235664367676
Epoch 1370, val loss: 1.1813446283340454
Epoch 1380, training loss: 6.042215824127197 = 0.010595249943435192 + 1.0 * 6.031620502471924
Epoch 1380, val loss: 1.1848818063735962
Epoch 1390, training loss: 6.040046691894531 = 0.01036863960325718 + 1.0 * 6.029677867889404
Epoch 1390, val loss: 1.188777208328247
Epoch 1400, training loss: 6.03804874420166 = 0.010147850029170513 + 1.0 * 6.027900695800781
Epoch 1400, val loss: 1.1925958395004272
Epoch 1410, training loss: 6.03740930557251 = 0.00993327610194683 + 1.0 * 6.027475833892822
Epoch 1410, val loss: 1.1962960958480835
Epoch 1420, training loss: 6.037225246429443 = 0.009724199771881104 + 1.0 * 6.027501106262207
Epoch 1420, val loss: 1.199961543083191
Epoch 1430, training loss: 6.04970121383667 = 0.009520533494651318 + 1.0 * 6.040180683135986
Epoch 1430, val loss: 1.2034295797348022
Epoch 1440, training loss: 6.042028903961182 = 0.009328914806246758 + 1.0 * 6.032700061798096
Epoch 1440, val loss: 1.206788182258606
Epoch 1450, training loss: 6.035835266113281 = 0.00914094876497984 + 1.0 * 6.026694297790527
Epoch 1450, val loss: 1.210314154624939
Epoch 1460, training loss: 6.036238670349121 = 0.008960510604083538 + 1.0 * 6.027277946472168
Epoch 1460, val loss: 1.213916540145874
Epoch 1470, training loss: 6.0414347648620605 = 0.008783127181231976 + 1.0 * 6.032651424407959
Epoch 1470, val loss: 1.2172225713729858
Epoch 1480, training loss: 6.036535739898682 = 0.008613505400717258 + 1.0 * 6.0279221534729
Epoch 1480, val loss: 1.2204805612564087
Epoch 1490, training loss: 6.035369873046875 = 0.008447994478046894 + 1.0 * 6.02692174911499
Epoch 1490, val loss: 1.2239042520523071
Epoch 1500, training loss: 6.034141540527344 = 0.008287971839308739 + 1.0 * 6.025853633880615
Epoch 1500, val loss: 1.2271369695663452
Epoch 1510, training loss: 6.039855480194092 = 0.008134298026561737 + 1.0 * 6.031721115112305
Epoch 1510, val loss: 1.2303144931793213
Epoch 1520, training loss: 6.031044006347656 = 0.007982663810253143 + 1.0 * 6.023061275482178
Epoch 1520, val loss: 1.233383297920227
Epoch 1530, training loss: 6.030683517456055 = 0.007837868295609951 + 1.0 * 6.02284574508667
Epoch 1530, val loss: 1.2366663217544556
Epoch 1540, training loss: 6.032073974609375 = 0.00769560132175684 + 1.0 * 6.024378299713135
Epoch 1540, val loss: 1.2399123907089233
Epoch 1550, training loss: 6.038081645965576 = 0.007556939031928778 + 1.0 * 6.030524730682373
Epoch 1550, val loss: 1.2429105043411255
Epoch 1560, training loss: 6.030793190002441 = 0.007425237447023392 + 1.0 * 6.023367881774902
Epoch 1560, val loss: 1.2459020614624023
Epoch 1570, training loss: 6.028913497924805 = 0.007295303046703339 + 1.0 * 6.021618366241455
Epoch 1570, val loss: 1.2490307092666626
Epoch 1580, training loss: 6.032905578613281 = 0.007169220596551895 + 1.0 * 6.025736331939697
Epoch 1580, val loss: 1.2521002292633057
Epoch 1590, training loss: 6.035302639007568 = 0.007046055980026722 + 1.0 * 6.028256416320801
Epoch 1590, val loss: 1.2549073696136475
Epoch 1600, training loss: 6.031663417816162 = 0.0069276755675673485 + 1.0 * 6.024735927581787
Epoch 1600, val loss: 1.2578123807907104
Epoch 1610, training loss: 6.032268524169922 = 0.006812151987105608 + 1.0 * 6.025456428527832
Epoch 1610, val loss: 1.2606617212295532
Epoch 1620, training loss: 6.02683687210083 = 0.006700484082102776 + 1.0 * 6.02013635635376
Epoch 1620, val loss: 1.2636170387268066
Epoch 1630, training loss: 6.027959823608398 = 0.006590964738279581 + 1.0 * 6.021368980407715
Epoch 1630, val loss: 1.2665276527404785
Epoch 1640, training loss: 6.032932281494141 = 0.006483652163296938 + 1.0 * 6.026448726654053
Epoch 1640, val loss: 1.2692837715148926
Epoch 1650, training loss: 6.026678562164307 = 0.006380339153110981 + 1.0 * 6.020298004150391
Epoch 1650, val loss: 1.2719650268554688
Epoch 1660, training loss: 6.026340007781982 = 0.006280031055212021 + 1.0 * 6.020060062408447
Epoch 1660, val loss: 1.2748281955718994
Epoch 1670, training loss: 6.024299144744873 = 0.0061815171502530575 + 1.0 * 6.018117427825928
Epoch 1670, val loss: 1.2776693105697632
Epoch 1680, training loss: 6.02452278137207 = 0.006084982771426439 + 1.0 * 6.01843786239624
Epoch 1680, val loss: 1.280489206314087
Epoch 1690, training loss: 6.034262180328369 = 0.005989393685013056 + 1.0 * 6.02827262878418
Epoch 1690, val loss: 1.283034324645996
Epoch 1700, training loss: 6.02379846572876 = 0.0058992826379835606 + 1.0 * 6.017899036407471
Epoch 1700, val loss: 1.2855228185653687
Epoch 1710, training loss: 6.023446083068848 = 0.005810540169477463 + 1.0 * 6.017635345458984
Epoch 1710, val loss: 1.2882425785064697
Epoch 1720, training loss: 6.024512767791748 = 0.005724606104195118 + 1.0 * 6.0187883377075195
Epoch 1720, val loss: 1.291009545326233
Epoch 1730, training loss: 6.027427673339844 = 0.005640091840177774 + 1.0 * 6.021787643432617
Epoch 1730, val loss: 1.2935372591018677
Epoch 1740, training loss: 6.0301947593688965 = 0.005558423697948456 + 1.0 * 6.024636268615723
Epoch 1740, val loss: 1.2958766222000122
Epoch 1750, training loss: 6.022465705871582 = 0.005479674786329269 + 1.0 * 6.016985893249512
Epoch 1750, val loss: 1.298263430595398
Epoch 1760, training loss: 6.021200180053711 = 0.005401962902396917 + 1.0 * 6.015798091888428
Epoch 1760, val loss: 1.3009408712387085
Epoch 1770, training loss: 6.021299839019775 = 0.005326236132532358 + 1.0 * 6.0159735679626465
Epoch 1770, val loss: 1.3035413026809692
Epoch 1780, training loss: 6.031461238861084 = 0.005251207388937473 + 1.0 * 6.026209831237793
Epoch 1780, val loss: 1.305993914604187
Epoch 1790, training loss: 6.021336078643799 = 0.005179394967854023 + 1.0 * 6.0161566734313965
Epoch 1790, val loss: 1.3081276416778564
Epoch 1800, training loss: 6.01914644241333 = 0.005108990706503391 + 1.0 * 6.014037609100342
Epoch 1800, val loss: 1.3106400966644287
Epoch 1810, training loss: 6.018794059753418 = 0.005040058400481939 + 1.0 * 6.013753890991211
Epoch 1810, val loss: 1.3131896257400513
Epoch 1820, training loss: 6.020846843719482 = 0.004972012247890234 + 1.0 * 6.015874862670898
Epoch 1820, val loss: 1.3156338930130005
Epoch 1830, training loss: 6.021013259887695 = 0.004902982618659735 + 1.0 * 6.016110420227051
Epoch 1830, val loss: 1.3176918029785156
Epoch 1840, training loss: 6.019076347351074 = 0.004843334201723337 + 1.0 * 6.014233112335205
Epoch 1840, val loss: 1.3197380304336548
Epoch 1850, training loss: 6.019002437591553 = 0.004779573529958725 + 1.0 * 6.014223098754883
Epoch 1850, val loss: 1.3223086595535278
Epoch 1860, training loss: 6.017480850219727 = 0.0047188107855618 + 1.0 * 6.012762069702148
Epoch 1860, val loss: 1.3247309923171997
Epoch 1870, training loss: 6.016910076141357 = 0.004657474812120199 + 1.0 * 6.0122528076171875
Epoch 1870, val loss: 1.3271552324295044
Epoch 1880, training loss: 6.023448944091797 = 0.004597457591444254 + 1.0 * 6.018851280212402
Epoch 1880, val loss: 1.3294240236282349
Epoch 1890, training loss: 6.017465591430664 = 0.004539381712675095 + 1.0 * 6.01292610168457
Epoch 1890, val loss: 1.3314683437347412
Epoch 1900, training loss: 6.024292469024658 = 0.0044823638163506985 + 1.0 * 6.019810199737549
Epoch 1900, val loss: 1.3336361646652222
Epoch 1910, training loss: 6.017810344696045 = 0.004427977837622166 + 1.0 * 6.013382434844971
Epoch 1910, val loss: 1.3358012437820435
Epoch 1920, training loss: 6.018252849578857 = 0.004373239818960428 + 1.0 * 6.013879776000977
Epoch 1920, val loss: 1.3380573987960815
Epoch 1930, training loss: 6.020272731781006 = 0.004320723470300436 + 1.0 * 6.015952110290527
Epoch 1930, val loss: 1.34023118019104
Epoch 1940, training loss: 6.014980792999268 = 0.004268691875040531 + 1.0 * 6.010712146759033
Epoch 1940, val loss: 1.3423421382904053
Epoch 1950, training loss: 6.018433094024658 = 0.004217265173792839 + 1.0 * 6.01421594619751
Epoch 1950, val loss: 1.3445721864700317
Epoch 1960, training loss: 6.015491008758545 = 0.0041674361564219 + 1.0 * 6.01132345199585
Epoch 1960, val loss: 1.346676230430603
Epoch 1970, training loss: 6.016276836395264 = 0.0041184090077877045 + 1.0 * 6.012158393859863
Epoch 1970, val loss: 1.3487690687179565
Epoch 1980, training loss: 6.015740394592285 = 0.004070256371051073 + 1.0 * 6.011670112609863
Epoch 1980, val loss: 1.3508710861206055
Epoch 1990, training loss: 6.028663158416748 = 0.004022432491183281 + 1.0 * 6.0246405601501465
Epoch 1990, val loss: 1.3528896570205688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.3063
Flip ASR: 0.2756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320582389831543 = 1.946702241897583 + 1.0 * 8.373880386352539
Epoch 0, val loss: 1.9516549110412598
Epoch 10, training loss: 10.309771537780762 = 1.9365614652633667 + 1.0 * 8.373209953308105
Epoch 10, val loss: 1.941053867340088
Epoch 20, training loss: 10.293375015258789 = 1.923856496810913 + 1.0 * 8.369518280029297
Epoch 20, val loss: 1.9268982410430908
Epoch 30, training loss: 10.255070686340332 = 1.9062998294830322 + 1.0 * 8.348771095275879
Epoch 30, val loss: 1.906678318977356
Epoch 40, training loss: 10.118836402893066 = 1.8834706544876099 + 1.0 * 8.235365867614746
Epoch 40, val loss: 1.881009817123413
Epoch 50, training loss: 9.67792797088623 = 1.8599846363067627 + 1.0 * 7.817943572998047
Epoch 50, val loss: 1.8554197549819946
Epoch 60, training loss: 9.34982967376709 = 1.836478352546692 + 1.0 * 7.513350963592529
Epoch 60, val loss: 1.831796646118164
Epoch 70, training loss: 8.961614608764648 = 1.8136992454528809 + 1.0 * 7.147915363311768
Epoch 70, val loss: 1.8095906972885132
Epoch 80, training loss: 8.716282844543457 = 1.7914775609970093 + 1.0 * 6.924805641174316
Epoch 80, val loss: 1.7883942127227783
Epoch 90, training loss: 8.515650749206543 = 1.7728761434555054 + 1.0 * 6.742774963378906
Epoch 90, val loss: 1.7707065343856812
Epoch 100, training loss: 8.411750793457031 = 1.753462791442871 + 1.0 * 6.658288478851318
Epoch 100, val loss: 1.752010703086853
Epoch 110, training loss: 8.325763702392578 = 1.729723334312439 + 1.0 * 6.59604024887085
Epoch 110, val loss: 1.7301937341690063
Epoch 120, training loss: 8.235701560974121 = 1.7047898769378662 + 1.0 * 6.530911922454834
Epoch 120, val loss: 1.708558201789856
Epoch 130, training loss: 8.137887954711914 = 1.6801035404205322 + 1.0 * 6.457784652709961
Epoch 130, val loss: 1.6874909400939941
Epoch 140, training loss: 8.050374031066895 = 1.6534502506256104 + 1.0 * 6.396924018859863
Epoch 140, val loss: 1.664945125579834
Epoch 150, training loss: 7.978021621704102 = 1.62222158908844 + 1.0 * 6.355800151824951
Epoch 150, val loss: 1.6390666961669922
Epoch 160, training loss: 7.909921646118164 = 1.5848230123519897 + 1.0 * 6.325098514556885
Epoch 160, val loss: 1.6090408563613892
Epoch 170, training loss: 7.848222255706787 = 1.5413435697555542 + 1.0 * 6.306878566741943
Epoch 170, val loss: 1.574945092201233
Epoch 180, training loss: 7.779542446136475 = 1.4933295249938965 + 1.0 * 6.286212921142578
Epoch 180, val loss: 1.5375510454177856
Epoch 190, training loss: 7.71142578125 = 1.4407418966293335 + 1.0 * 6.270683765411377
Epoch 190, val loss: 1.4971039295196533
Epoch 200, training loss: 7.641556739807129 = 1.384336233139038 + 1.0 * 6.257220268249512
Epoch 200, val loss: 1.4544340372085571
Epoch 210, training loss: 7.573153495788574 = 1.3262388706207275 + 1.0 * 6.246914863586426
Epoch 210, val loss: 1.4115755558013916
Epoch 220, training loss: 7.50200080871582 = 1.2682998180389404 + 1.0 * 6.233700752258301
Epoch 220, val loss: 1.3694945573806763
Epoch 230, training loss: 7.433788299560547 = 1.2099088430404663 + 1.0 * 6.223879337310791
Epoch 230, val loss: 1.3278682231903076
Epoch 240, training loss: 7.366336345672607 = 1.1508325338363647 + 1.0 * 6.215503692626953
Epoch 240, val loss: 1.286210298538208
Epoch 250, training loss: 7.302553653717041 = 1.0916937589645386 + 1.0 * 6.210859775543213
Epoch 250, val loss: 1.2448703050613403
Epoch 260, training loss: 7.233786106109619 = 1.0330524444580078 + 1.0 * 6.200733661651611
Epoch 260, val loss: 1.2036654949188232
Epoch 270, training loss: 7.167883396148682 = 0.9741742014884949 + 1.0 * 6.193709373474121
Epoch 270, val loss: 1.1625308990478516
Epoch 280, training loss: 7.10539436340332 = 0.9156813621520996 + 1.0 * 6.189713001251221
Epoch 280, val loss: 1.1220693588256836
Epoch 290, training loss: 7.046695709228516 = 0.8602492809295654 + 1.0 * 6.186446189880371
Epoch 290, val loss: 1.0841426849365234
Epoch 300, training loss: 6.98794412612915 = 0.8088709712028503 + 1.0 * 6.179073333740234
Epoch 300, val loss: 1.0500352382659912
Epoch 310, training loss: 6.933795928955078 = 0.7613720297813416 + 1.0 * 6.172423839569092
Epoch 310, val loss: 1.0196852684020996
Epoch 320, training loss: 6.8845696449279785 = 0.7179556488990784 + 1.0 * 6.166614055633545
Epoch 320, val loss: 0.9931215643882751
Epoch 330, training loss: 6.8434929847717285 = 0.678689181804657 + 1.0 * 6.164803981781006
Epoch 330, val loss: 0.9702960848808289
Epoch 340, training loss: 6.806187152862549 = 0.6437795758247375 + 1.0 * 6.162407398223877
Epoch 340, val loss: 0.9514515995979309
Epoch 350, training loss: 6.7669291496276855 = 0.6123988032341003 + 1.0 * 6.1545305252075195
Epoch 350, val loss: 0.9359973073005676
Epoch 360, training loss: 6.733799934387207 = 0.5835995078086853 + 1.0 * 6.150200366973877
Epoch 360, val loss: 0.9230846762657166
Epoch 370, training loss: 6.704887390136719 = 0.5571412444114685 + 1.0 * 6.1477460861206055
Epoch 370, val loss: 0.9124209880828857
Epoch 380, training loss: 6.674901485443115 = 0.5327674150466919 + 1.0 * 6.142134189605713
Epoch 380, val loss: 0.9037061333656311
Epoch 390, training loss: 6.651104927062988 = 0.5099102258682251 + 1.0 * 6.141194820404053
Epoch 390, val loss: 0.8966413140296936
Epoch 400, training loss: 6.622622489929199 = 0.4885013997554779 + 1.0 * 6.134120941162109
Epoch 400, val loss: 0.8910351395606995
Epoch 410, training loss: 6.6175336837768555 = 0.4682069420814514 + 1.0 * 6.149326801300049
Epoch 410, val loss: 0.8866240978240967
Epoch 420, training loss: 6.5847907066345215 = 0.44908607006073 + 1.0 * 6.135704517364502
Epoch 420, val loss: 0.8832347989082336
Epoch 430, training loss: 6.556393623352051 = 0.4308801591396332 + 1.0 * 6.125513553619385
Epoch 430, val loss: 0.8806938529014587
Epoch 440, training loss: 6.537771701812744 = 0.41327109932899475 + 1.0 * 6.124500751495361
Epoch 440, val loss: 0.8789923787117004
Epoch 450, training loss: 6.51654052734375 = 0.39619502425193787 + 1.0 * 6.120345592498779
Epoch 450, val loss: 0.8778708577156067
Epoch 460, training loss: 6.5003743171691895 = 0.3797345757484436 + 1.0 * 6.120639801025391
Epoch 460, val loss: 0.8772200345993042
Epoch 470, training loss: 6.479320049285889 = 0.363619863986969 + 1.0 * 6.1157002449035645
Epoch 470, val loss: 0.8771217465400696
Epoch 480, training loss: 6.464694976806641 = 0.3478575348854065 + 1.0 * 6.116837501525879
Epoch 480, val loss: 0.8773877024650574
Epoch 490, training loss: 6.447947025299072 = 0.33250731229782104 + 1.0 * 6.1154398918151855
Epoch 490, val loss: 0.878002941608429
Epoch 500, training loss: 6.427448749542236 = 0.3174864649772644 + 1.0 * 6.109962463378906
Epoch 500, val loss: 0.8790273070335388
Epoch 510, training loss: 6.416701316833496 = 0.3026636242866516 + 1.0 * 6.11403751373291
Epoch 510, val loss: 0.8803313374519348
Epoch 520, training loss: 6.3986334800720215 = 0.28830111026763916 + 1.0 * 6.110332489013672
Epoch 520, val loss: 0.8819178342819214
Epoch 530, training loss: 6.3793253898620605 = 0.27420690655708313 + 1.0 * 6.105118274688721
Epoch 530, val loss: 0.883430540561676
Epoch 540, training loss: 6.361721038818359 = 0.2603193521499634 + 1.0 * 6.1014018058776855
Epoch 540, val loss: 0.8854138255119324
Epoch 550, training loss: 6.345901012420654 = 0.2466181367635727 + 1.0 * 6.099282741546631
Epoch 550, val loss: 0.8877716660499573
Epoch 560, training loss: 6.345168590545654 = 0.23289605975151062 + 1.0 * 6.1122727394104
Epoch 560, val loss: 0.890285313129425
Epoch 570, training loss: 6.322853088378906 = 0.21977129578590393 + 1.0 * 6.103081703186035
Epoch 570, val loss: 0.8935530185699463
Epoch 580, training loss: 6.303494453430176 = 0.20717853307724 + 1.0 * 6.096315860748291
Epoch 580, val loss: 0.8974963426589966
Epoch 590, training loss: 6.28889274597168 = 0.1952974498271942 + 1.0 * 6.093595504760742
Epoch 590, val loss: 0.9026414752006531
Epoch 600, training loss: 6.276525497436523 = 0.1842186450958252 + 1.0 * 6.092306613922119
Epoch 600, val loss: 0.9085869789123535
Epoch 610, training loss: 6.2758049964904785 = 0.17385432124137878 + 1.0 * 6.101950645446777
Epoch 610, val loss: 0.9152465462684631
Epoch 620, training loss: 6.253586769104004 = 0.16422529518604279 + 1.0 * 6.089361667633057
Epoch 620, val loss: 0.9221970438957214
Epoch 630, training loss: 6.252695560455322 = 0.1552017629146576 + 1.0 * 6.097493648529053
Epoch 630, val loss: 0.929443895816803
Epoch 640, training loss: 6.242459774017334 = 0.1467713713645935 + 1.0 * 6.095688343048096
Epoch 640, val loss: 0.9369578957557678
Epoch 650, training loss: 6.225825309753418 = 0.13893577456474304 + 1.0 * 6.086889743804932
Epoch 650, val loss: 0.9444915652275085
Epoch 660, training loss: 6.215810775756836 = 0.13154879212379456 + 1.0 * 6.084261894226074
Epoch 660, val loss: 0.9525609612464905
Epoch 670, training loss: 6.207021236419678 = 0.12458887696266174 + 1.0 * 6.082432270050049
Epoch 670, val loss: 0.9608286619186401
Epoch 680, training loss: 6.199763298034668 = 0.11803177744150162 + 1.0 * 6.08173131942749
Epoch 680, val loss: 0.9693759679794312
Epoch 690, training loss: 6.199729919433594 = 0.11189605295658112 + 1.0 * 6.087833881378174
Epoch 690, val loss: 0.9781503081321716
Epoch 700, training loss: 6.190659999847412 = 0.10617654025554657 + 1.0 * 6.084483623504639
Epoch 700, val loss: 0.9867612719535828
Epoch 710, training loss: 6.180782318115234 = 0.10081346333026886 + 1.0 * 6.0799689292907715
Epoch 710, val loss: 0.9956098794937134
Epoch 720, training loss: 6.175373554229736 = 0.0957581102848053 + 1.0 * 6.079615592956543
Epoch 720, val loss: 1.004636526107788
Epoch 730, training loss: 6.171639919281006 = 0.09101124852895737 + 1.0 * 6.080628871917725
Epoch 730, val loss: 1.0137661695480347
Epoch 740, training loss: 6.16300630569458 = 0.0865471214056015 + 1.0 * 6.0764594078063965
Epoch 740, val loss: 1.0228934288024902
Epoch 750, training loss: 6.158024787902832 = 0.08235781639814377 + 1.0 * 6.075666904449463
Epoch 750, val loss: 1.0321426391601562
Epoch 760, training loss: 6.159820556640625 = 0.0783865824341774 + 1.0 * 6.0814337730407715
Epoch 760, val loss: 1.0414090156555176
Epoch 770, training loss: 6.152153968811035 = 0.07469318062067032 + 1.0 * 6.077460765838623
Epoch 770, val loss: 1.0505437850952148
Epoch 780, training loss: 6.142962455749512 = 0.07119300961494446 + 1.0 * 6.0717692375183105
Epoch 780, val loss: 1.0597079992294312
Epoch 790, training loss: 6.142242431640625 = 0.06789786368608475 + 1.0 * 6.074344635009766
Epoch 790, val loss: 1.0688854455947876
Epoch 800, training loss: 6.137456893920898 = 0.06479137390851974 + 1.0 * 6.072665691375732
Epoch 800, val loss: 1.0779383182525635
Epoch 810, training loss: 6.1423749923706055 = 0.061873599886894226 + 1.0 * 6.080501556396484
Epoch 810, val loss: 1.0868815183639526
Epoch 820, training loss: 6.129937648773193 = 0.05914213880896568 + 1.0 * 6.07079553604126
Epoch 820, val loss: 1.0956950187683105
Epoch 830, training loss: 6.124986171722412 = 0.05654403194785118 + 1.0 * 6.068442344665527
Epoch 830, val loss: 1.1045347452163696
Epoch 840, training loss: 6.120668411254883 = 0.05409127101302147 + 1.0 * 6.066576957702637
Epoch 840, val loss: 1.1133559942245483
Epoch 850, training loss: 6.120342254638672 = 0.0517633892595768 + 1.0 * 6.068578720092773
Epoch 850, val loss: 1.1221171617507935
Epoch 860, training loss: 6.116912841796875 = 0.04957190901041031 + 1.0 * 6.067340850830078
Epoch 860, val loss: 1.1307294368743896
Epoch 870, training loss: 6.112624168395996 = 0.04751253500580788 + 1.0 * 6.0651116371154785
Epoch 870, val loss: 1.1391724348068237
Epoch 880, training loss: 6.1138458251953125 = 0.04554925113916397 + 1.0 * 6.068296432495117
Epoch 880, val loss: 1.1476091146469116
Epoch 890, training loss: 6.107475280761719 = 0.04370443522930145 + 1.0 * 6.063770771026611
Epoch 890, val loss: 1.1558996438980103
Epoch 900, training loss: 6.1053786277771 = 0.04194358363747597 + 1.0 * 6.063435077667236
Epoch 900, val loss: 1.1640832424163818
Epoch 910, training loss: 6.109349250793457 = 0.04028283432126045 + 1.0 * 6.069066524505615
Epoch 910, val loss: 1.1721457242965698
Epoch 920, training loss: 6.099997043609619 = 0.03871466591954231 + 1.0 * 6.061282157897949
Epoch 920, val loss: 1.1799923181533813
Epoch 930, training loss: 6.097135543823242 = 0.037220701575279236 + 1.0 * 6.059915065765381
Epoch 930, val loss: 1.1878139972686768
Epoch 940, training loss: 6.103207111358643 = 0.035807572305202484 + 1.0 * 6.067399501800537
Epoch 940, val loss: 1.1955047845840454
Epoch 950, training loss: 6.093391418457031 = 0.034458354115486145 + 1.0 * 6.058933258056641
Epoch 950, val loss: 1.202971339225769
Epoch 960, training loss: 6.091842174530029 = 0.03318333253264427 + 1.0 * 6.058659076690674
Epoch 960, val loss: 1.2104182243347168
Epoch 970, training loss: 6.090970516204834 = 0.03196578845381737 + 1.0 * 6.059004783630371
Epoch 970, val loss: 1.2178020477294922
Epoch 980, training loss: 6.0884881019592285 = 0.030812028795480728 + 1.0 * 6.057675838470459
Epoch 980, val loss: 1.2250014543533325
Epoch 990, training loss: 6.084593296051025 = 0.029715171083807945 + 1.0 * 6.054878234863281
Epoch 990, val loss: 1.2321003675460815
Epoch 1000, training loss: 6.087519645690918 = 0.028671834617853165 + 1.0 * 6.058847904205322
Epoch 1000, val loss: 1.2390835285186768
Epoch 1010, training loss: 6.085788249969482 = 0.027682075276970863 + 1.0 * 6.058105945587158
Epoch 1010, val loss: 1.2458291053771973
Epoch 1020, training loss: 6.079500675201416 = 0.026738718152046204 + 1.0 * 6.052762031555176
Epoch 1020, val loss: 1.2525544166564941
Epoch 1030, training loss: 6.080729961395264 = 0.025837482884526253 + 1.0 * 6.054892539978027
Epoch 1030, val loss: 1.2592447996139526
Epoch 1040, training loss: 6.083926677703857 = 0.024977384135127068 + 1.0 * 6.0589494705200195
Epoch 1040, val loss: 1.2657541036605835
Epoch 1050, training loss: 6.076177597045898 = 0.024168873205780983 + 1.0 * 6.052008628845215
Epoch 1050, val loss: 1.272076964378357
Epoch 1060, training loss: 6.073302268981934 = 0.02338944748044014 + 1.0 * 6.049912929534912
Epoch 1060, val loss: 1.2783944606781006
Epoch 1070, training loss: 6.072934627532959 = 0.02264357917010784 + 1.0 * 6.050291061401367
Epoch 1070, val loss: 1.2846723794937134
Epoch 1080, training loss: 6.085485458374023 = 0.021934181451797485 + 1.0 * 6.063551425933838
Epoch 1080, val loss: 1.2907640933990479
Epoch 1090, training loss: 6.071675777435303 = 0.0212534312158823 + 1.0 * 6.050422191619873
Epoch 1090, val loss: 1.296610713005066
Epoch 1100, training loss: 6.068328380584717 = 0.020607976242899895 + 1.0 * 6.047720432281494
Epoch 1100, val loss: 1.3025072813034058
Epoch 1110, training loss: 6.067440032958984 = 0.019988305866718292 + 1.0 * 6.047451496124268
Epoch 1110, val loss: 1.3083770275115967
Epoch 1120, training loss: 6.070207595825195 = 0.01939353533089161 + 1.0 * 6.050814151763916
Epoch 1120, val loss: 1.3141071796417236
Epoch 1130, training loss: 6.074309349060059 = 0.01882283389568329 + 1.0 * 6.055486679077148
Epoch 1130, val loss: 1.3196748495101929
Epoch 1140, training loss: 6.063913345336914 = 0.018284127116203308 + 1.0 * 6.045629024505615
Epoch 1140, val loss: 1.3250876665115356
Epoch 1150, training loss: 6.063422203063965 = 0.017765140160918236 + 1.0 * 6.045657157897949
Epoch 1150, val loss: 1.330445647239685
Epoch 1160, training loss: 6.0625104904174805 = 0.017265135422348976 + 1.0 * 6.045245170593262
Epoch 1160, val loss: 1.3358113765716553
Epoch 1170, training loss: 6.064506530761719 = 0.016787156462669373 + 1.0 * 6.047719478607178
Epoch 1170, val loss: 1.341017723083496
Epoch 1180, training loss: 6.059382915496826 = 0.016332954168319702 + 1.0 * 6.0430498123168945
Epoch 1180, val loss: 1.3461023569107056
Epoch 1190, training loss: 6.059055805206299 = 0.015894992277026176 + 1.0 * 6.043160915374756
Epoch 1190, val loss: 1.351189374923706
Epoch 1200, training loss: 6.065591812133789 = 0.015471029095351696 + 1.0 * 6.050120830535889
Epoch 1200, val loss: 1.356210708618164
Epoch 1210, training loss: 6.05807638168335 = 0.015072139911353588 + 1.0 * 6.043004035949707
Epoch 1210, val loss: 1.360945701599121
Epoch 1220, training loss: 6.056328296661377 = 0.0146851921454072 + 1.0 * 6.041643142700195
Epoch 1220, val loss: 1.3656620979309082
Epoch 1230, training loss: 6.055207252502441 = 0.014312674291431904 + 1.0 * 6.040894508361816
Epoch 1230, val loss: 1.3703795671463013
Epoch 1240, training loss: 6.054698467254639 = 0.013952414505183697 + 1.0 * 6.040746212005615
Epoch 1240, val loss: 1.3750699758529663
Epoch 1250, training loss: 6.060684680938721 = 0.013607037253677845 + 1.0 * 6.047077655792236
Epoch 1250, val loss: 1.3796145915985107
Epoch 1260, training loss: 6.056759357452393 = 0.013277026824653149 + 1.0 * 6.043482303619385
Epoch 1260, val loss: 1.3840147256851196
Epoch 1270, training loss: 6.052515506744385 = 0.012954876758158207 + 1.0 * 6.039560794830322
Epoch 1270, val loss: 1.388384461402893
Epoch 1280, training loss: 6.0539422035217285 = 0.012646044604480267 + 1.0 * 6.041296005249023
Epoch 1280, val loss: 1.3927514553070068
Epoch 1290, training loss: 6.055872917175293 = 0.012347398325800896 + 1.0 * 6.043525695800781
Epoch 1290, val loss: 1.3969881534576416
Epoch 1300, training loss: 6.051824569702148 = 0.01206404622644186 + 1.0 * 6.039760589599609
Epoch 1300, val loss: 1.4010854959487915
Epoch 1310, training loss: 6.050361156463623 = 0.011788279749453068 + 1.0 * 6.038572788238525
Epoch 1310, val loss: 1.4051439762115479
Epoch 1320, training loss: 6.053755283355713 = 0.011523068882524967 + 1.0 * 6.042232036590576
Epoch 1320, val loss: 1.4091782569885254
Epoch 1330, training loss: 6.047732353210449 = 0.011267588473856449 + 1.0 * 6.036464691162109
Epoch 1330, val loss: 1.4131190776824951
Epoch 1340, training loss: 6.047679901123047 = 0.01101987436413765 + 1.0 * 6.036660194396973
Epoch 1340, val loss: 1.4170584678649902
Epoch 1350, training loss: 6.052227973937988 = 0.010780693963170052 + 1.0 * 6.041447162628174
Epoch 1350, val loss: 1.4209315776824951
Epoch 1360, training loss: 6.048099040985107 = 0.010549264959990978 + 1.0 * 6.03754997253418
Epoch 1360, val loss: 1.4245924949645996
Epoch 1370, training loss: 6.045690059661865 = 0.010325770825147629 + 1.0 * 6.035364151000977
Epoch 1370, val loss: 1.428263783454895
Epoch 1380, training loss: 6.043868541717529 = 0.010110283270478249 + 1.0 * 6.033758163452148
Epoch 1380, val loss: 1.431976556777954
Epoch 1390, training loss: 6.049267768859863 = 0.009899941273033619 + 1.0 * 6.03936767578125
Epoch 1390, val loss: 1.4356085062026978
Epoch 1400, training loss: 6.0450029373168945 = 0.00969783402979374 + 1.0 * 6.035305023193359
Epoch 1400, val loss: 1.4390231370925903
Epoch 1410, training loss: 6.0424723625183105 = 0.009503941051661968 + 1.0 * 6.032968521118164
Epoch 1410, val loss: 1.442474603652954
Epoch 1420, training loss: 6.045072555541992 = 0.00931375939399004 + 1.0 * 6.035758972167969
Epoch 1420, val loss: 1.4459072351455688
Epoch 1430, training loss: 6.047481536865234 = 0.009129699319601059 + 1.0 * 6.038352012634277
Epoch 1430, val loss: 1.4491982460021973
Epoch 1440, training loss: 6.040584087371826 = 0.008954931050539017 + 1.0 * 6.0316290855407715
Epoch 1440, val loss: 1.4524003267288208
Epoch 1450, training loss: 6.042200565338135 = 0.008783052675426006 + 1.0 * 6.033417701721191
Epoch 1450, val loss: 1.4556643962860107
Epoch 1460, training loss: 6.042079925537109 = 0.008615879341959953 + 1.0 * 6.033463954925537
Epoch 1460, val loss: 1.458858847618103
Epoch 1470, training loss: 6.042013168334961 = 0.00845345202833414 + 1.0 * 6.033559799194336
Epoch 1470, val loss: 1.4619953632354736
Epoch 1480, training loss: 6.044082164764404 = 0.008295261301100254 + 1.0 * 6.035787105560303
Epoch 1480, val loss: 1.4651110172271729
Epoch 1490, training loss: 6.041327476501465 = 0.00814254954457283 + 1.0 * 6.033185005187988
Epoch 1490, val loss: 1.468087911605835
Epoch 1500, training loss: 6.040365695953369 = 0.00799573864787817 + 1.0 * 6.032370090484619
Epoch 1500, val loss: 1.4710804224014282
Epoch 1510, training loss: 6.039804458618164 = 0.00785156525671482 + 1.0 * 6.031952857971191
Epoch 1510, val loss: 1.4739716053009033
Epoch 1520, training loss: 6.03977108001709 = 0.0077125439420342445 + 1.0 * 6.0320587158203125
Epoch 1520, val loss: 1.4768528938293457
Epoch 1530, training loss: 6.046360969543457 = 0.007578876800835133 + 1.0 * 6.038782119750977
Epoch 1530, val loss: 1.4795840978622437
Epoch 1540, training loss: 6.038433074951172 = 0.007448364049196243 + 1.0 * 6.030984878540039
Epoch 1540, val loss: 1.4822916984558105
Epoch 1550, training loss: 6.035643577575684 = 0.007320549804717302 + 1.0 * 6.028323173522949
Epoch 1550, val loss: 1.4850434064865112
Epoch 1560, training loss: 6.03633451461792 = 0.007195549085736275 + 1.0 * 6.029139041900635
Epoch 1560, val loss: 1.487766981124878
Epoch 1570, training loss: 6.04233980178833 = 0.007073657121509314 + 1.0 * 6.035265922546387
Epoch 1570, val loss: 1.4903734922409058
Epoch 1580, training loss: 6.0345659255981445 = 0.0069583021104335785 + 1.0 * 6.027607440948486
Epoch 1580, val loss: 1.4929578304290771
Epoch 1590, training loss: 6.039312839508057 = 0.0068442849442362785 + 1.0 * 6.032468318939209
Epoch 1590, val loss: 1.495586633682251
Epoch 1600, training loss: 6.0359954833984375 = 0.006732081528753042 + 1.0 * 6.029263496398926
Epoch 1600, val loss: 1.4980394840240479
Epoch 1610, training loss: 6.033448696136475 = 0.006626168731600046 + 1.0 * 6.026822566986084
Epoch 1610, val loss: 1.5005186796188354
Epoch 1620, training loss: 6.032474517822266 = 0.006520411930978298 + 1.0 * 6.025954246520996
Epoch 1620, val loss: 1.5029596090316772
Epoch 1630, training loss: 6.032294273376465 = 0.006416752468794584 + 1.0 * 6.025877475738525
Epoch 1630, val loss: 1.5054140090942383
Epoch 1640, training loss: 6.036104679107666 = 0.006316537968814373 + 1.0 * 6.029788017272949
Epoch 1640, val loss: 1.507810354232788
Epoch 1650, training loss: 6.038276672363281 = 0.00621922267600894 + 1.0 * 6.032057285308838
Epoch 1650, val loss: 1.510109782218933
Epoch 1660, training loss: 6.032351970672607 = 0.006124407518655062 + 1.0 * 6.0262274742126465
Epoch 1660, val loss: 1.5123454332351685
Epoch 1670, training loss: 6.032559394836426 = 0.006032010540366173 + 1.0 * 6.026527404785156
Epoch 1670, val loss: 1.5145776271820068
Epoch 1680, training loss: 6.03476619720459 = 0.005943151190876961 + 1.0 * 6.028822898864746
Epoch 1680, val loss: 1.5168359279632568
Epoch 1690, training loss: 6.031903266906738 = 0.0058546969667077065 + 1.0 * 6.02604866027832
Epoch 1690, val loss: 1.5189344882965088
Epoch 1700, training loss: 6.029052257537842 = 0.0057686022482812405 + 1.0 * 6.0232834815979
Epoch 1700, val loss: 1.521100640296936
Epoch 1710, training loss: 6.031115531921387 = 0.005684464238584042 + 1.0 * 6.025431156158447
Epoch 1710, val loss: 1.523263692855835
Epoch 1720, training loss: 6.030463218688965 = 0.0056029981933534145 + 1.0 * 6.024860382080078
Epoch 1720, val loss: 1.525302529335022
Epoch 1730, training loss: 6.032778739929199 = 0.005524127744138241 + 1.0 * 6.027254581451416
Epoch 1730, val loss: 1.527321457862854
Epoch 1740, training loss: 6.031339168548584 = 0.005446808412671089 + 1.0 * 6.02589225769043
Epoch 1740, val loss: 1.5292667150497437
Epoch 1750, training loss: 6.028103351593018 = 0.005370339844375849 + 1.0 * 6.022733211517334
Epoch 1750, val loss: 1.53121817111969
Epoch 1760, training loss: 6.026750564575195 = 0.005296260584145784 + 1.0 * 6.021454334259033
Epoch 1760, val loss: 1.5331865549087524
Epoch 1770, training loss: 6.030788421630859 = 0.005223077721893787 + 1.0 * 6.025565147399902
Epoch 1770, val loss: 1.5351250171661377
Epoch 1780, training loss: 6.0291748046875 = 0.0051527367904782295 + 1.0 * 6.024022102355957
Epoch 1780, val loss: 1.5369205474853516
Epoch 1790, training loss: 6.027291297912598 = 0.00508450111374259 + 1.0 * 6.022206783294678
Epoch 1790, val loss: 1.5387414693832397
Epoch 1800, training loss: 6.029532432556152 = 0.005016546230763197 + 1.0 * 6.0245161056518555
Epoch 1800, val loss: 1.5405771732330322
Epoch 1810, training loss: 6.029937744140625 = 0.004950454458594322 + 1.0 * 6.02498722076416
Epoch 1810, val loss: 1.5423005819320679
Epoch 1820, training loss: 6.029759883880615 = 0.004886038601398468 + 1.0 * 6.024873733520508
Epoch 1820, val loss: 1.543985366821289
Epoch 1830, training loss: 6.025012016296387 = 0.004823649302124977 + 1.0 * 6.020188331604004
Epoch 1830, val loss: 1.5456256866455078
Epoch 1840, training loss: 6.025097370147705 = 0.004762173630297184 + 1.0 * 6.0203351974487305
Epoch 1840, val loss: 1.5473724603652954
Epoch 1850, training loss: 6.023457050323486 = 0.004701113793998957 + 1.0 * 6.018755912780762
Epoch 1850, val loss: 1.5490697622299194
Epoch 1860, training loss: 6.03240966796875 = 0.004641392733901739 + 1.0 * 6.027768135070801
Epoch 1860, val loss: 1.5507192611694336
Epoch 1870, training loss: 6.030938625335693 = 0.004583217669278383 + 1.0 * 6.026355266571045
Epoch 1870, val loss: 1.5522607564926147
Epoch 1880, training loss: 6.0281829833984375 = 0.004527560900896788 + 1.0 * 6.023655414581299
Epoch 1880, val loss: 1.553712248802185
Epoch 1890, training loss: 6.0231828689575195 = 0.004473818000406027 + 1.0 * 6.018709182739258
Epoch 1890, val loss: 1.5552138090133667
Epoch 1900, training loss: 6.022829532623291 = 0.004419674631208181 + 1.0 * 6.018409729003906
Epoch 1900, val loss: 1.5567574501037598
Epoch 1910, training loss: 6.027501583099365 = 0.004366386216133833 + 1.0 * 6.023135185241699
Epoch 1910, val loss: 1.5582275390625
Epoch 1920, training loss: 6.021788120269775 = 0.004314086399972439 + 1.0 * 6.017474174499512
Epoch 1920, val loss: 1.5596733093261719
Epoch 1930, training loss: 6.023451328277588 = 0.004262498114258051 + 1.0 * 6.01918888092041
Epoch 1930, val loss: 1.5611366033554077
Epoch 1940, training loss: 6.023902893066406 = 0.004212390631437302 + 1.0 * 6.01969051361084
Epoch 1940, val loss: 1.5625485181808472
Epoch 1950, training loss: 6.024509429931641 = 0.0041637783870100975 + 1.0 * 6.020345687866211
Epoch 1950, val loss: 1.5638995170593262
Epoch 1960, training loss: 6.027284622192383 = 0.004116720985621214 + 1.0 * 6.023168087005615
Epoch 1960, val loss: 1.565164566040039
Epoch 1970, training loss: 6.021106719970703 = 0.00406920351088047 + 1.0 * 6.017037391662598
Epoch 1970, val loss: 1.5664016008377075
Epoch 1980, training loss: 6.019857883453369 = 0.004023905843496323 + 1.0 * 6.015833854675293
Epoch 1980, val loss: 1.567741870880127
Epoch 1990, training loss: 6.0191521644592285 = 0.003978676628321409 + 1.0 * 6.015173435211182
Epoch 1990, val loss: 1.569087266921997
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7306
Flip ASR: 0.6889/225 nodes
The final ASR:0.51907, 0.17324, Accuracy:0.80000, 0.00524
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10610])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.327275276184082 = 1.9534595012664795 + 1.0 * 8.373815536499023
Epoch 0, val loss: 1.9613755941390991
Epoch 10, training loss: 10.315902709960938 = 1.942689061164856 + 1.0 * 8.373213768005371
Epoch 10, val loss: 1.9500505924224854
Epoch 20, training loss: 10.298650741577148 = 1.9293051958084106 + 1.0 * 8.369345664978027
Epoch 20, val loss: 1.9356257915496826
Epoch 30, training loss: 10.250923156738281 = 1.9108442068099976 + 1.0 * 8.340079307556152
Epoch 30, val loss: 1.9157520532608032
Epoch 40, training loss: 9.995755195617676 = 1.888362169265747 + 1.0 * 8.107393264770508
Epoch 40, val loss: 1.8931504487991333
Epoch 50, training loss: 9.206564903259277 = 1.8644068241119385 + 1.0 * 7.34215784072876
Epoch 50, val loss: 1.8698468208312988
Epoch 60, training loss: 8.814234733581543 = 1.8484684228897095 + 1.0 * 6.965766429901123
Epoch 60, val loss: 1.8554840087890625
Epoch 70, training loss: 8.541275024414062 = 1.8358087539672852 + 1.0 * 6.705465793609619
Epoch 70, val loss: 1.843308448791504
Epoch 80, training loss: 8.410640716552734 = 1.8241924047470093 + 1.0 * 6.5864481925964355
Epoch 80, val loss: 1.8322124481201172
Epoch 90, training loss: 8.324174880981445 = 1.8108208179473877 + 1.0 * 6.513354301452637
Epoch 90, val loss: 1.8196539878845215
Epoch 100, training loss: 8.25534725189209 = 1.7975437641143799 + 1.0 * 6.457803726196289
Epoch 100, val loss: 1.8073810338974
Epoch 110, training loss: 8.189451217651367 = 1.7857046127319336 + 1.0 * 6.403747081756592
Epoch 110, val loss: 1.796614646911621
Epoch 120, training loss: 8.133790969848633 = 1.7751524448394775 + 1.0 * 6.358638763427734
Epoch 120, val loss: 1.7871661186218262
Epoch 130, training loss: 8.083492279052734 = 1.7643378973007202 + 1.0 * 6.319154739379883
Epoch 130, val loss: 1.7777214050292969
Epoch 140, training loss: 8.04018497467041 = 1.7518714666366577 + 1.0 * 6.288313865661621
Epoch 140, val loss: 1.7671219110488892
Epoch 150, training loss: 8.00136661529541 = 1.737352967262268 + 1.0 * 6.264013290405273
Epoch 150, val loss: 1.7551521062850952
Epoch 160, training loss: 7.963767051696777 = 1.7200943231582642 + 1.0 * 6.243672847747803
Epoch 160, val loss: 1.7412053346633911
Epoch 170, training loss: 7.926061630249023 = 1.699317216873169 + 1.0 * 6.226744174957275
Epoch 170, val loss: 1.7245328426361084
Epoch 180, training loss: 7.886558532714844 = 1.6741738319396973 + 1.0 * 6.2123847007751465
Epoch 180, val loss: 1.7044293880462646
Epoch 190, training loss: 7.8443121910095215 = 1.643922209739685 + 1.0 * 6.200389862060547
Epoch 190, val loss: 1.6802483797073364
Epoch 200, training loss: 7.7965240478515625 = 1.6080495119094849 + 1.0 * 6.188474655151367
Epoch 200, val loss: 1.6514089107513428
Epoch 210, training loss: 7.744753837585449 = 1.5660587549209595 + 1.0 * 6.178695201873779
Epoch 210, val loss: 1.6175787448883057
Epoch 220, training loss: 7.691550254821777 = 1.518701195716858 + 1.0 * 6.172849178314209
Epoch 220, val loss: 1.5795341730117798
Epoch 230, training loss: 7.632115364074707 = 1.4678936004638672 + 1.0 * 6.16422176361084
Epoch 230, val loss: 1.5389394760131836
Epoch 240, training loss: 7.5717244148254395 = 1.4142837524414062 + 1.0 * 6.157440662384033
Epoch 240, val loss: 1.4965683221817017
Epoch 250, training loss: 7.511106967926025 = 1.3587075471878052 + 1.0 * 6.15239953994751
Epoch 250, val loss: 1.4533690214157104
Epoch 260, training loss: 7.4503068923950195 = 1.3033350706100464 + 1.0 * 6.146971702575684
Epoch 260, val loss: 1.4116780757904053
Epoch 270, training loss: 7.391201972961426 = 1.248664140701294 + 1.0 * 6.142537593841553
Epoch 270, val loss: 1.3712611198425293
Epoch 280, training loss: 7.3373823165893555 = 1.194074273109436 + 1.0 * 6.143308162689209
Epoch 280, val loss: 1.3315026760101318
Epoch 290, training loss: 7.276496887207031 = 1.1409027576446533 + 1.0 * 6.135593891143799
Epoch 290, val loss: 1.2933317422866821
Epoch 300, training loss: 7.21773624420166 = 1.0886777639389038 + 1.0 * 6.129058361053467
Epoch 300, val loss: 1.2558314800262451
Epoch 310, training loss: 7.167500972747803 = 1.0369879007339478 + 1.0 * 6.1305131912231445
Epoch 310, val loss: 1.2186763286590576
Epoch 320, training loss: 7.111067771911621 = 0.9866843819618225 + 1.0 * 6.124383449554443
Epoch 320, val loss: 1.182274580001831
Epoch 330, training loss: 7.05634880065918 = 0.9374995231628418 + 1.0 * 6.118849277496338
Epoch 330, val loss: 1.1465587615966797
Epoch 340, training loss: 7.008310794830322 = 0.8893950581550598 + 1.0 * 6.118915557861328
Epoch 340, val loss: 1.1113841533660889
Epoch 350, training loss: 6.956538200378418 = 0.8428813815116882 + 1.0 * 6.113656997680664
Epoch 350, val loss: 1.0774154663085938
Epoch 360, training loss: 6.911811828613281 = 0.7982325553894043 + 1.0 * 6.113579273223877
Epoch 360, val loss: 1.04500150680542
Epoch 370, training loss: 6.864871978759766 = 0.7557446956634521 + 1.0 * 6.109127044677734
Epoch 370, val loss: 1.0144312381744385
Epoch 380, training loss: 6.821366310119629 = 0.7151904106140137 + 1.0 * 6.106175899505615
Epoch 380, val loss: 0.9854745268821716
Epoch 390, training loss: 6.7922563552856445 = 0.6768144369125366 + 1.0 * 6.115441799163818
Epoch 390, val loss: 0.9583646655082703
Epoch 400, training loss: 6.743783950805664 = 0.6415355205535889 + 1.0 * 6.102248191833496
Epoch 400, val loss: 0.9338882565498352
Epoch 410, training loss: 6.708987236022949 = 0.6087300777435303 + 1.0 * 6.100257396697998
Epoch 410, val loss: 0.911871075630188
Epoch 420, training loss: 6.6753644943237305 = 0.577976405620575 + 1.0 * 6.09738826751709
Epoch 420, val loss: 0.8917605876922607
Epoch 430, training loss: 6.64625358581543 = 0.5492445230484009 + 1.0 * 6.097009181976318
Epoch 430, val loss: 0.8735449314117432
Epoch 440, training loss: 6.617308616638184 = 0.5225105881690979 + 1.0 * 6.0947980880737305
Epoch 440, val loss: 0.857440710067749
Epoch 450, training loss: 6.588680267333984 = 0.4972883462905884 + 1.0 * 6.0913920402526855
Epoch 450, val loss: 0.8428600430488586
Epoch 460, training loss: 6.564089298248291 = 0.47338253259658813 + 1.0 * 6.090706825256348
Epoch 460, val loss: 0.8295795321464539
Epoch 470, training loss: 6.538522720336914 = 0.4507540166378021 + 1.0 * 6.0877685546875
Epoch 470, val loss: 0.8176920413970947
Epoch 480, training loss: 6.515507698059082 = 0.42892003059387207 + 1.0 * 6.086587905883789
Epoch 480, val loss: 0.8068311214447021
Epoch 490, training loss: 6.49137020111084 = 0.4076724350452423 + 1.0 * 6.08369779586792
Epoch 490, val loss: 0.7966801524162292
Epoch 500, training loss: 6.472784996032715 = 0.38686123490333557 + 1.0 * 6.085923671722412
Epoch 500, val loss: 0.787397563457489
Epoch 510, training loss: 6.448853015899658 = 0.36639395356178284 + 1.0 * 6.082458972930908
Epoch 510, val loss: 0.778670608997345
Epoch 520, training loss: 6.425775527954102 = 0.34612226486206055 + 1.0 * 6.079653263092041
Epoch 520, val loss: 0.7706301212310791
Epoch 530, training loss: 6.407941818237305 = 0.32590705156326294 + 1.0 * 6.082034587860107
Epoch 530, val loss: 0.7630025744438171
Epoch 540, training loss: 6.387515068054199 = 0.3060454726219177 + 1.0 * 6.081469535827637
Epoch 540, val loss: 0.7560672760009766
Epoch 550, training loss: 6.365914344787598 = 0.2865997850894928 + 1.0 * 6.079314708709717
Epoch 550, val loss: 0.749981164932251
Epoch 560, training loss: 6.34176778793335 = 0.26766449213027954 + 1.0 * 6.074103355407715
Epoch 560, val loss: 0.7444169521331787
Epoch 570, training loss: 6.321779251098633 = 0.24928992986679077 + 1.0 * 6.072489261627197
Epoch 570, val loss: 0.7397878766059875
Epoch 580, training loss: 6.3025970458984375 = 0.23163527250289917 + 1.0 * 6.070961952209473
Epoch 580, val loss: 0.73607337474823
Epoch 590, training loss: 6.29902458190918 = 0.21500535309314728 + 1.0 * 6.084019184112549
Epoch 590, val loss: 0.7331562638282776
Epoch 600, training loss: 6.270020484924316 = 0.19965960085391998 + 1.0 * 6.0703606605529785
Epoch 600, val loss: 0.7314883470535278
Epoch 610, training loss: 6.252547264099121 = 0.1854030191898346 + 1.0 * 6.067144393920898
Epoch 610, val loss: 0.7305946350097656
Epoch 620, training loss: 6.2412872314453125 = 0.17219288647174835 + 1.0 * 6.069094181060791
Epoch 620, val loss: 0.730437695980072
Epoch 630, training loss: 6.228983402252197 = 0.1601451337337494 + 1.0 * 6.068838119506836
Epoch 630, val loss: 0.7311648726463318
Epoch 640, training loss: 6.2144951820373535 = 0.14918620884418488 + 1.0 * 6.065309047698975
Epoch 640, val loss: 0.7326985001564026
Epoch 650, training loss: 6.201171875 = 0.13919349014759064 + 1.0 * 6.061978340148926
Epoch 650, val loss: 0.7347899079322815
Epoch 660, training loss: 6.196225643157959 = 0.13004069030284882 + 1.0 * 6.066184997558594
Epoch 660, val loss: 0.7374423146247864
Epoch 670, training loss: 6.190234184265137 = 0.12174385786056519 + 1.0 * 6.068490505218506
Epoch 670, val loss: 0.7405017018318176
Epoch 680, training loss: 6.175093650817871 = 0.11423157155513763 + 1.0 * 6.060862064361572
Epoch 680, val loss: 0.7440832853317261
Epoch 690, training loss: 6.165577411651611 = 0.10735702514648438 + 1.0 * 6.058220386505127
Epoch 690, val loss: 0.747925341129303
Epoch 700, training loss: 6.158656597137451 = 0.10103759169578552 + 1.0 * 6.057619094848633
Epoch 700, val loss: 0.7520090937614441
Epoch 710, training loss: 6.15674352645874 = 0.0952482521533966 + 1.0 * 6.061495304107666
Epoch 710, val loss: 0.7563971877098083
Epoch 720, training loss: 6.145272254943848 = 0.08995532989501953 + 1.0 * 6.055316925048828
Epoch 720, val loss: 0.761069655418396
Epoch 730, training loss: 6.137151718139648 = 0.08507362008094788 + 1.0 * 6.0520782470703125
Epoch 730, val loss: 0.7659293413162231
Epoch 740, training loss: 6.141523361206055 = 0.08055731654167175 + 1.0 * 6.0609660148620605
Epoch 740, val loss: 0.7709263563156128
Epoch 750, training loss: 6.133170127868652 = 0.07641371339559555 + 1.0 * 6.056756496429443
Epoch 750, val loss: 0.7759038209915161
Epoch 760, training loss: 6.123987674713135 = 0.07259764522314072 + 1.0 * 6.051390171051025
Epoch 760, val loss: 0.7811528444290161
Epoch 770, training loss: 6.116776943206787 = 0.0690469965338707 + 1.0 * 6.047729969024658
Epoch 770, val loss: 0.786412239074707
Epoch 780, training loss: 6.129334449768066 = 0.06572820246219635 + 1.0 * 6.063606262207031
Epoch 780, val loss: 0.7916528582572937
Epoch 790, training loss: 6.110048770904541 = 0.06266730278730392 + 1.0 * 6.047381401062012
Epoch 790, val loss: 0.7969188690185547
Epoch 800, training loss: 6.107276916503906 = 0.059826262295246124 + 1.0 * 6.047450542449951
Epoch 800, val loss: 0.8024085164070129
Epoch 810, training loss: 6.101407051086426 = 0.05715619772672653 + 1.0 * 6.044250965118408
Epoch 810, val loss: 0.8078101277351379
Epoch 820, training loss: 6.104104042053223 = 0.054645001888275146 + 1.0 * 6.049458980560303
Epoch 820, val loss: 0.8132359981536865
Epoch 830, training loss: 6.097995758056641 = 0.052298348397016525 + 1.0 * 6.045697212219238
Epoch 830, val loss: 0.8185452222824097
Epoch 840, training loss: 6.0939741134643555 = 0.05011314898729324 + 1.0 * 6.043860912322998
Epoch 840, val loss: 0.8240953683853149
Epoch 850, training loss: 6.090077877044678 = 0.04804809391498566 + 1.0 * 6.042029857635498
Epoch 850, val loss: 0.8295369148254395
Epoch 860, training loss: 6.089850902557373 = 0.046106353402137756 + 1.0 * 6.0437445640563965
Epoch 860, val loss: 0.8348345756530762
Epoch 870, training loss: 6.087138652801514 = 0.044296037405729294 + 1.0 * 6.042842388153076
Epoch 870, val loss: 0.8402628302574158
Epoch 880, training loss: 6.080621719360352 = 0.04258786886930466 + 1.0 * 6.038033962249756
Epoch 880, val loss: 0.8457114100456238
Epoch 890, training loss: 6.0804548263549805 = 0.04096467047929764 + 1.0 * 6.039490222930908
Epoch 890, val loss: 0.8510449528694153
Epoch 900, training loss: 6.076164245605469 = 0.039432574063539505 + 1.0 * 6.036731719970703
Epoch 900, val loss: 0.8562431335449219
Epoch 910, training loss: 6.077117919921875 = 0.03799787908792496 + 1.0 * 6.039120197296143
Epoch 910, val loss: 0.8615543842315674
Epoch 920, training loss: 6.072115898132324 = 0.036637261509895325 + 1.0 * 6.035478591918945
Epoch 920, val loss: 0.866874098777771
Epoch 930, training loss: 6.087641716003418 = 0.03534376993775368 + 1.0 * 6.052298069000244
Epoch 930, val loss: 0.8720434308052063
Epoch 940, training loss: 6.070926189422607 = 0.034121207892894745 + 1.0 * 6.036805152893066
Epoch 940, val loss: 0.8770886063575745
Epoch 950, training loss: 6.067812919616699 = 0.032969847321510315 + 1.0 * 6.0348429679870605
Epoch 950, val loss: 0.8822813034057617
Epoch 960, training loss: 6.0713605880737305 = 0.03186957910656929 + 1.0 * 6.039491176605225
Epoch 960, val loss: 0.8872537612915039
Epoch 970, training loss: 6.064087867736816 = 0.03082716278731823 + 1.0 * 6.033260822296143
Epoch 970, val loss: 0.8922256231307983
Epoch 980, training loss: 6.061707019805908 = 0.029835864901542664 + 1.0 * 6.031871318817139
Epoch 980, val loss: 0.8972358703613281
Epoch 990, training loss: 6.071022033691406 = 0.028886331245303154 + 1.0 * 6.042135715484619
Epoch 990, val loss: 0.9020997881889343
Epoch 1000, training loss: 6.061506271362305 = 0.027983592823147774 + 1.0 * 6.033522605895996
Epoch 1000, val loss: 0.9069026708602905
Epoch 1010, training loss: 6.057437896728516 = 0.027130112051963806 + 1.0 * 6.030307769775391
Epoch 1010, val loss: 0.9117721915245056
Epoch 1020, training loss: 6.05621337890625 = 0.02631077915430069 + 1.0 * 6.029902458190918
Epoch 1020, val loss: 0.9165512919425964
Epoch 1030, training loss: 6.0673441886901855 = 0.025522753596305847 + 1.0 * 6.041821479797363
Epoch 1030, val loss: 0.9212583303451538
Epoch 1040, training loss: 6.057901382446289 = 0.024778032675385475 + 1.0 * 6.03312349319458
Epoch 1040, val loss: 0.9258471727371216
Epoch 1050, training loss: 6.051793575286865 = 0.0240684412419796 + 1.0 * 6.0277252197265625
Epoch 1050, val loss: 0.930547833442688
Epoch 1060, training loss: 6.0622758865356445 = 0.023387616500258446 + 1.0 * 6.038888454437256
Epoch 1060, val loss: 0.9351556301116943
Epoch 1070, training loss: 6.055514335632324 = 0.02273525483906269 + 1.0 * 6.032779216766357
Epoch 1070, val loss: 0.9394860863685608
Epoch 1080, training loss: 6.050155162811279 = 0.02211744152009487 + 1.0 * 6.0280375480651855
Epoch 1080, val loss: 0.9440181255340576
Epoch 1090, training loss: 6.047113418579102 = 0.021520357578992844 + 1.0 * 6.025593280792236
Epoch 1090, val loss: 0.948509156703949
Epoch 1100, training loss: 6.046159744262695 = 0.020942145958542824 + 1.0 * 6.025217533111572
Epoch 1100, val loss: 0.9529058337211609
Epoch 1110, training loss: 6.057687282562256 = 0.02038523554801941 + 1.0 * 6.037302017211914
Epoch 1110, val loss: 0.9572818279266357
Epoch 1120, training loss: 6.0497050285339355 = 0.019853346049785614 + 1.0 * 6.029851913452148
Epoch 1120, val loss: 0.9614329934120178
Epoch 1130, training loss: 6.048519134521484 = 0.019347775727510452 + 1.0 * 6.029171466827393
Epoch 1130, val loss: 0.9657649397850037
Epoch 1140, training loss: 6.042882919311523 = 0.018861331045627594 + 1.0 * 6.024021625518799
Epoch 1140, val loss: 0.9699979424476624
Epoch 1150, training loss: 6.042494297027588 = 0.018391674384474754 + 1.0 * 6.024102687835693
Epoch 1150, val loss: 0.9741749167442322
Epoch 1160, training loss: 6.0434184074401855 = 0.017936531454324722 + 1.0 * 6.025481700897217
Epoch 1160, val loss: 0.9782899022102356
Epoch 1170, training loss: 6.049235820770264 = 0.017498644068837166 + 1.0 * 6.031737327575684
Epoch 1170, val loss: 0.9823932647705078
Epoch 1180, training loss: 6.042662143707275 = 0.017081933096051216 + 1.0 * 6.025580406188965
Epoch 1180, val loss: 0.9864165186882019
Epoch 1190, training loss: 6.040012359619141 = 0.016681170091032982 + 1.0 * 6.023331165313721
Epoch 1190, val loss: 0.990542471408844
Epoch 1200, training loss: 6.041995048522949 = 0.016292743384838104 + 1.0 * 6.025702476501465
Epoch 1200, val loss: 0.9945164322853088
Epoch 1210, training loss: 6.03719425201416 = 0.015915775671601295 + 1.0 * 6.021278381347656
Epoch 1210, val loss: 0.99839848279953
Epoch 1220, training loss: 6.03671407699585 = 0.01555541716516018 + 1.0 * 6.021158695220947
Epoch 1220, val loss: 1.0023269653320312
Epoch 1230, training loss: 6.036840438842773 = 0.015206861309707165 + 1.0 * 6.021633625030518
Epoch 1230, val loss: 1.006242275238037
Epoch 1240, training loss: 6.036317825317383 = 0.014868716709315777 + 1.0 * 6.021449089050293
Epoch 1240, val loss: 1.0100882053375244
Epoch 1250, training loss: 6.041921138763428 = 0.01454242691397667 + 1.0 * 6.027378559112549
Epoch 1250, val loss: 1.0138671398162842
Epoch 1260, training loss: 6.036077499389648 = 0.014228267595171928 + 1.0 * 6.021849155426025
Epoch 1260, val loss: 1.0175613164901733
Epoch 1270, training loss: 6.033382415771484 = 0.013925207778811455 + 1.0 * 6.0194573402404785
Epoch 1270, val loss: 1.0213861465454102
Epoch 1280, training loss: 6.034689426422119 = 0.013628870248794556 + 1.0 * 6.021060466766357
Epoch 1280, val loss: 1.0250520706176758
Epoch 1290, training loss: 6.037685394287109 = 0.013341760262846947 + 1.0 * 6.024343490600586
Epoch 1290, val loss: 1.0286084413528442
Epoch 1300, training loss: 6.0341057777404785 = 0.013067612424492836 + 1.0 * 6.021038055419922
Epoch 1300, val loss: 1.032248854637146
Epoch 1310, training loss: 6.032419204711914 = 0.01280373614281416 + 1.0 * 6.019615650177002
Epoch 1310, val loss: 1.0359219312667847
Epoch 1320, training loss: 6.033312797546387 = 0.012545530684292316 + 1.0 * 6.0207672119140625
Epoch 1320, val loss: 1.0394582748413086
Epoch 1330, training loss: 6.030681133270264 = 0.012295644730329514 + 1.0 * 6.018385410308838
Epoch 1330, val loss: 1.0429189205169678
Epoch 1340, training loss: 6.028867244720459 = 0.012053624726831913 + 1.0 * 6.0168137550354
Epoch 1340, val loss: 1.046452283859253
Epoch 1350, training loss: 6.03369140625 = 0.01181713119149208 + 1.0 * 6.02187442779541
Epoch 1350, val loss: 1.0498987436294556
Epoch 1360, training loss: 6.030144214630127 = 0.011589070782065392 + 1.0 * 6.018555164337158
Epoch 1360, val loss: 1.0532543659210205
Epoch 1370, training loss: 6.030028820037842 = 0.011368097737431526 + 1.0 * 6.018660545349121
Epoch 1370, val loss: 1.0566643476486206
Epoch 1380, training loss: 6.028234958648682 = 0.011154506355524063 + 1.0 * 6.017080307006836
Epoch 1380, val loss: 1.060010552406311
Epoch 1390, training loss: 6.028932094573975 = 0.010946238413453102 + 1.0 * 6.017985820770264
Epoch 1390, val loss: 1.0633245706558228
Epoch 1400, training loss: 6.029524803161621 = 0.010744296945631504 + 1.0 * 6.018780708312988
Epoch 1400, val loss: 1.0666192770004272
Epoch 1410, training loss: 6.026820182800293 = 0.010548900812864304 + 1.0 * 6.016271114349365
Epoch 1410, val loss: 1.0698903799057007
Epoch 1420, training loss: 6.025061130523682 = 0.010359492152929306 + 1.0 * 6.014701843261719
Epoch 1420, val loss: 1.0731362104415894
Epoch 1430, training loss: 6.025222301483154 = 0.010174066759645939 + 1.0 * 6.015048027038574
Epoch 1430, val loss: 1.0763484239578247
Epoch 1440, training loss: 6.025877475738525 = 0.009992915205657482 + 1.0 * 6.0158843994140625
Epoch 1440, val loss: 1.079545021057129
Epoch 1450, training loss: 6.023949146270752 = 0.009816423058509827 + 1.0 * 6.014132499694824
Epoch 1450, val loss: 1.0826679468154907
Epoch 1460, training loss: 6.029474258422852 = 0.009645899757742882 + 1.0 * 6.0198283195495605
Epoch 1460, val loss: 1.0857460498809814
Epoch 1470, training loss: 6.025702476501465 = 0.009481513872742653 + 1.0 * 6.016221046447754
Epoch 1470, val loss: 1.0888310670852661
Epoch 1480, training loss: 6.021903038024902 = 0.009321245364844799 + 1.0 * 6.012581825256348
Epoch 1480, val loss: 1.091927170753479
Epoch 1490, training loss: 6.022947311401367 = 0.009164673276245594 + 1.0 * 6.013782501220703
Epoch 1490, val loss: 1.0950336456298828
Epoch 1500, training loss: 6.0254058837890625 = 0.009010473266243935 + 1.0 * 6.016395568847656
Epoch 1500, val loss: 1.0979546308517456
Epoch 1510, training loss: 6.020695686340332 = 0.008862573653459549 + 1.0 * 6.011833190917969
Epoch 1510, val loss: 1.1009632349014282
Epoch 1520, training loss: 6.019890785217285 = 0.008720245212316513 + 1.0 * 6.011170387268066
Epoch 1520, val loss: 1.1039897203445435
Epoch 1530, training loss: 6.02077054977417 = 0.008579667657613754 + 1.0 * 6.012190818786621
Epoch 1530, val loss: 1.1069966554641724
Epoch 1540, training loss: 6.023351669311523 = 0.008440770208835602 + 1.0 * 6.014910697937012
Epoch 1540, val loss: 1.1098757982254028
Epoch 1550, training loss: 6.029423236846924 = 0.008305996656417847 + 1.0 * 6.021117210388184
Epoch 1550, val loss: 1.112743854522705
Epoch 1560, training loss: 6.021841526031494 = 0.008176254108548164 + 1.0 * 6.013665199279785
Epoch 1560, val loss: 1.1155502796173096
Epoch 1570, training loss: 6.022024631500244 = 0.00805070623755455 + 1.0 * 6.013973712921143
Epoch 1570, val loss: 1.1185059547424316
Epoch 1580, training loss: 6.017710208892822 = 0.007927561178803444 + 1.0 * 6.009782791137695
Epoch 1580, val loss: 1.1213079690933228
Epoch 1590, training loss: 6.016561031341553 = 0.007806791923940182 + 1.0 * 6.008754253387451
Epoch 1590, val loss: 1.1241141557693481
Epoch 1600, training loss: 6.019252777099609 = 0.007687743287533522 + 1.0 * 6.011565208435059
Epoch 1600, val loss: 1.1268795728683472
Epoch 1610, training loss: 6.017566204071045 = 0.007570750545710325 + 1.0 * 6.009995460510254
Epoch 1610, val loss: 1.129591464996338
Epoch 1620, training loss: 6.0253005027771 = 0.007457548752427101 + 1.0 * 6.017842769622803
Epoch 1620, val loss: 1.1323645114898682
Epoch 1630, training loss: 6.019086837768555 = 0.007348379585891962 + 1.0 * 6.011738300323486
Epoch 1630, val loss: 1.1350616216659546
Epoch 1640, training loss: 6.017178535461426 = 0.007242490537464619 + 1.0 * 6.0099358558654785
Epoch 1640, val loss: 1.1377909183502197
Epoch 1650, training loss: 6.015781879425049 = 0.007138533983379602 + 1.0 * 6.00864315032959
Epoch 1650, val loss: 1.1404914855957031
Epoch 1660, training loss: 6.015393257141113 = 0.007035787217319012 + 1.0 * 6.008357524871826
Epoch 1660, val loss: 1.1431466341018677
Epoch 1670, training loss: 6.017566204071045 = 0.006935001350939274 + 1.0 * 6.010631084442139
Epoch 1670, val loss: 1.1457663774490356
Epoch 1680, training loss: 6.014120578765869 = 0.006836310960352421 + 1.0 * 6.007284164428711
Epoch 1680, val loss: 1.148362159729004
Epoch 1690, training loss: 6.019101619720459 = 0.006740426644682884 + 1.0 * 6.0123610496521
Epoch 1690, val loss: 1.1509637832641602
Epoch 1700, training loss: 6.016361713409424 = 0.006646610330790281 + 1.0 * 6.0097150802612305
Epoch 1700, val loss: 1.1535155773162842
Epoch 1710, training loss: 6.013613224029541 = 0.006555778905749321 + 1.0 * 6.0070576667785645
Epoch 1710, val loss: 1.1561205387115479
Epoch 1720, training loss: 6.019067764282227 = 0.006466537248343229 + 1.0 * 6.012601375579834
Epoch 1720, val loss: 1.158685326576233
Epoch 1730, training loss: 6.015048027038574 = 0.006379139143973589 + 1.0 * 6.008668899536133
Epoch 1730, val loss: 1.1611754894256592
Epoch 1740, training loss: 6.016868591308594 = 0.006293971091508865 + 1.0 * 6.010574817657471
Epoch 1740, val loss: 1.1636273860931396
Epoch 1750, training loss: 6.011960983276367 = 0.006211553700268269 + 1.0 * 6.005749225616455
Epoch 1750, val loss: 1.1661474704742432
Epoch 1760, training loss: 6.012156963348389 = 0.006129938643425703 + 1.0 * 6.0060272216796875
Epoch 1760, val loss: 1.1686244010925293
Epoch 1770, training loss: 6.014100074768066 = 0.0060487971641123295 + 1.0 * 6.00805139541626
Epoch 1770, val loss: 1.1710515022277832
Epoch 1780, training loss: 6.014747142791748 = 0.0059692650102078915 + 1.0 * 6.008778095245361
Epoch 1780, val loss: 1.173419713973999
Epoch 1790, training loss: 6.010544300079346 = 0.0058929831720888615 + 1.0 * 6.0046515464782715
Epoch 1790, val loss: 1.1758862733840942
Epoch 1800, training loss: 6.010900497436523 = 0.005818511825054884 + 1.0 * 6.005082130432129
Epoch 1800, val loss: 1.1783490180969238
Epoch 1810, training loss: 6.017723560333252 = 0.005744303110986948 + 1.0 * 6.011979103088379
Epoch 1810, val loss: 1.180750846862793
Epoch 1820, training loss: 6.010560035705566 = 0.005671112798154354 + 1.0 * 6.004889011383057
Epoch 1820, val loss: 1.1829867362976074
Epoch 1830, training loss: 6.008989334106445 = 0.005601564887911081 + 1.0 * 6.003387928009033
Epoch 1830, val loss: 1.1854077577590942
Epoch 1840, training loss: 6.010526180267334 = 0.005532858427613974 + 1.0 * 6.004993438720703
Epoch 1840, val loss: 1.1877968311309814
Epoch 1850, training loss: 6.012267112731934 = 0.005464260466396809 + 1.0 * 6.006803035736084
Epoch 1850, val loss: 1.190019965171814
Epoch 1860, training loss: 6.011127471923828 = 0.005397511180490255 + 1.0 * 6.005730152130127
Epoch 1860, val loss: 1.1922885179519653
Epoch 1870, training loss: 6.013983726501465 = 0.00533282570540905 + 1.0 * 6.008650779724121
Epoch 1870, val loss: 1.1945916414260864
Epoch 1880, training loss: 6.0086822509765625 = 0.005268987268209457 + 1.0 * 6.003413200378418
Epoch 1880, val loss: 1.196898341178894
Epoch 1890, training loss: 6.008296489715576 = 0.005206536967307329 + 1.0 * 6.003089904785156
Epoch 1890, val loss: 1.1991522312164307
Epoch 1900, training loss: 6.009283542633057 = 0.005144375376403332 + 1.0 * 6.004138946533203
Epoch 1900, val loss: 1.2013604640960693
Epoch 1910, training loss: 6.010370254516602 = 0.0050833625718951225 + 1.0 * 6.005286693572998
Epoch 1910, val loss: 1.2035475969314575
Epoch 1920, training loss: 6.007801532745361 = 0.005024014040827751 + 1.0 * 6.002777576446533
Epoch 1920, val loss: 1.2057653665542603
Epoch 1930, training loss: 6.012180328369141 = 0.004965662956237793 + 1.0 * 6.007214546203613
Epoch 1930, val loss: 1.2080031633377075
Epoch 1940, training loss: 6.0081586837768555 = 0.004908445756882429 + 1.0 * 6.0032501220703125
Epoch 1940, val loss: 1.2100794315338135
Epoch 1950, training loss: 6.007409572601318 = 0.004852927289903164 + 1.0 * 6.002556800842285
Epoch 1950, val loss: 1.2123137712478638
Epoch 1960, training loss: 6.006227016448975 = 0.004798191599547863 + 1.0 * 6.001428604125977
Epoch 1960, val loss: 1.2144699096679688
Epoch 1970, training loss: 6.008590221405029 = 0.004743777681142092 + 1.0 * 6.003846645355225
Epoch 1970, val loss: 1.2166019678115845
Epoch 1980, training loss: 6.006584644317627 = 0.004690391011536121 + 1.0 * 6.001894474029541
Epoch 1980, val loss: 1.2187345027923584
Epoch 1990, training loss: 6.008508205413818 = 0.004638219252228737 + 1.0 * 6.003870010375977
Epoch 1990, val loss: 1.2208417654037476
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.6863
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.329442024230957 = 1.9556502103805542 + 1.0 * 8.373791694641113
Epoch 0, val loss: 1.959219217300415
Epoch 10, training loss: 10.317864418029785 = 1.9446150064468384 + 1.0 * 8.373249053955078
Epoch 10, val loss: 1.9481273889541626
Epoch 20, training loss: 10.30065631866455 = 1.9308860301971436 + 1.0 * 8.369770050048828
Epoch 20, val loss: 1.9339219331741333
Epoch 30, training loss: 10.256768226623535 = 1.9116111993789673 + 1.0 * 8.3451566696167
Epoch 30, val loss: 1.9136967658996582
Epoch 40, training loss: 10.064595222473145 = 1.886861801147461 + 1.0 * 8.177733421325684
Epoch 40, val loss: 1.8885283470153809
Epoch 50, training loss: 9.37242317199707 = 1.860694408416748 + 1.0 * 7.5117292404174805
Epoch 50, val loss: 1.8624051809310913
Epoch 60, training loss: 8.937676429748535 = 1.8420608043670654 + 1.0 * 7.095615386962891
Epoch 60, val loss: 1.8450015783309937
Epoch 70, training loss: 8.612629890441895 = 1.8301934003829956 + 1.0 * 6.782436847686768
Epoch 70, val loss: 1.83289635181427
Epoch 80, training loss: 8.42764949798584 = 1.817274808883667 + 1.0 * 6.610374450683594
Epoch 80, val loss: 1.819865345954895
Epoch 90, training loss: 8.310864448547363 = 1.8037444353103638 + 1.0 * 6.507119655609131
Epoch 90, val loss: 1.8064651489257812
Epoch 100, training loss: 8.233474731445312 = 1.7903441190719604 + 1.0 * 6.443130970001221
Epoch 100, val loss: 1.7935971021652222
Epoch 110, training loss: 8.172428131103516 = 1.7778836488723755 + 1.0 * 6.39454460144043
Epoch 110, val loss: 1.7816234827041626
Epoch 120, training loss: 8.123528480529785 = 1.7658915519714355 + 1.0 * 6.35763692855835
Epoch 120, val loss: 1.7700458765029907
Epoch 130, training loss: 8.079351425170898 = 1.7531863451004028 + 1.0 * 6.326164722442627
Epoch 130, val loss: 1.7580468654632568
Epoch 140, training loss: 8.038355827331543 = 1.7385623455047607 + 1.0 * 6.299793243408203
Epoch 140, val loss: 1.7447959184646606
Epoch 150, training loss: 8.000910758972168 = 1.720659852027893 + 1.0 * 6.2802510261535645
Epoch 150, val loss: 1.7292826175689697
Epoch 160, training loss: 7.959451675415039 = 1.699542760848999 + 1.0 * 6.259909152984619
Epoch 160, val loss: 1.711384654045105
Epoch 170, training loss: 7.9173583984375 = 1.674666166305542 + 1.0 * 6.242692470550537
Epoch 170, val loss: 1.6909801959991455
Epoch 180, training loss: 7.870582580566406 = 1.6449512243270874 + 1.0 * 6.225631237030029
Epoch 180, val loss: 1.6666771173477173
Epoch 190, training loss: 7.8199310302734375 = 1.6089117527008057 + 1.0 * 6.211019039154053
Epoch 190, val loss: 1.6373170614242554
Epoch 200, training loss: 7.767590522766113 = 1.566576600074768 + 1.0 * 6.201014041900635
Epoch 200, val loss: 1.6030150651931763
Epoch 210, training loss: 7.708240985870361 = 1.5186891555786133 + 1.0 * 6.189551830291748
Epoch 210, val loss: 1.5640915632247925
Epoch 220, training loss: 7.647132873535156 = 1.4655582904815674 + 1.0 * 6.181574821472168
Epoch 220, val loss: 1.5212135314941406
Epoch 230, training loss: 7.584301948547363 = 1.409944772720337 + 1.0 * 6.174356937408447
Epoch 230, val loss: 1.4769890308380127
Epoch 240, training loss: 7.520084381103516 = 1.3534595966339111 + 1.0 * 6.166624546051025
Epoch 240, val loss: 1.4326406717300415
Epoch 250, training loss: 7.456799030303955 = 1.2966898679733276 + 1.0 * 6.160109043121338
Epoch 250, val loss: 1.3887027502059937
Epoch 260, training loss: 7.398404121398926 = 1.2403498888015747 + 1.0 * 6.158054351806641
Epoch 260, val loss: 1.345969557762146
Epoch 270, training loss: 7.337218284606934 = 1.1863737106323242 + 1.0 * 6.150844573974609
Epoch 270, val loss: 1.305399775505066
Epoch 280, training loss: 7.277225017547607 = 1.1336923837661743 + 1.0 * 6.143532752990723
Epoch 280, val loss: 1.2661466598510742
Epoch 290, training loss: 7.219574451446533 = 1.0810829401016235 + 1.0 * 6.138491630554199
Epoch 290, val loss: 1.22683584690094
Epoch 300, training loss: 7.166990280151367 = 1.0287657976150513 + 1.0 * 6.1382246017456055
Epoch 300, val loss: 1.1877048015594482
Epoch 310, training loss: 7.107982635498047 = 0.9776921272277832 + 1.0 * 6.130290508270264
Epoch 310, val loss: 1.1494708061218262
Epoch 320, training loss: 7.053730010986328 = 0.9272744655609131 + 1.0 * 6.126455307006836
Epoch 320, val loss: 1.1114319562911987
Epoch 330, training loss: 7.000917434692383 = 0.877832293510437 + 1.0 * 6.123085021972656
Epoch 330, val loss: 1.0738011598587036
Epoch 340, training loss: 6.955589771270752 = 0.8305601477622986 + 1.0 * 6.125029563903809
Epoch 340, val loss: 1.0376032590866089
Epoch 350, training loss: 6.9045538902282715 = 0.7863984704017639 + 1.0 * 6.118155479431152
Epoch 350, val loss: 1.0038117170333862
Epoch 360, training loss: 6.857617378234863 = 0.7447357177734375 + 1.0 * 6.112881660461426
Epoch 360, val loss: 0.9719040393829346
Epoch 370, training loss: 6.818719387054443 = 0.7056478261947632 + 1.0 * 6.113071441650391
Epoch 370, val loss: 0.9422372579574585
Epoch 380, training loss: 6.779590606689453 = 0.6697941422462463 + 1.0 * 6.109796524047852
Epoch 380, val loss: 0.915372371673584
Epoch 390, training loss: 6.742367267608643 = 0.6367724537849426 + 1.0 * 6.105594635009766
Epoch 390, val loss: 0.8917538523674011
Epoch 400, training loss: 6.708138942718506 = 0.6057052612304688 + 1.0 * 6.102433681488037
Epoch 400, val loss: 0.8699531555175781
Epoch 410, training loss: 6.675788879394531 = 0.5761608481407166 + 1.0 * 6.09962797164917
Epoch 410, val loss: 0.8498806953430176
Epoch 420, training loss: 6.659757137298584 = 0.5478215217590332 + 1.0 * 6.111935615539551
Epoch 420, val loss: 0.8312305212020874
Epoch 430, training loss: 6.619856357574463 = 0.5209031105041504 + 1.0 * 6.0989532470703125
Epoch 430, val loss: 0.8141237497329712
Epoch 440, training loss: 6.58843469619751 = 0.4948706328868866 + 1.0 * 6.093564033508301
Epoch 440, val loss: 0.7984232306480408
Epoch 450, training loss: 6.56207275390625 = 0.46935564279556274 + 1.0 * 6.092717170715332
Epoch 450, val loss: 0.7834433317184448
Epoch 460, training loss: 6.538227558135986 = 0.4443976581096649 + 1.0 * 6.093830108642578
Epoch 460, val loss: 0.7693665027618408
Epoch 470, training loss: 6.508010387420654 = 0.4199630618095398 + 1.0 * 6.088047504425049
Epoch 470, val loss: 0.7563784718513489
Epoch 480, training loss: 6.484272480010986 = 0.3958471119403839 + 1.0 * 6.088425159454346
Epoch 480, val loss: 0.7442160844802856
Epoch 490, training loss: 6.459534645080566 = 0.37214523553848267 + 1.0 * 6.0873894691467285
Epoch 490, val loss: 0.7328872680664062
Epoch 500, training loss: 6.4350972175598145 = 0.34899869561195374 + 1.0 * 6.086098670959473
Epoch 500, val loss: 0.7227096557617188
Epoch 510, training loss: 6.408557415008545 = 0.3265139162540436 + 1.0 * 6.082043647766113
Epoch 510, val loss: 0.713549017906189
Epoch 520, training loss: 6.385300636291504 = 0.30480384826660156 + 1.0 * 6.080496788024902
Epoch 520, val loss: 0.7054680585861206
Epoch 530, training loss: 6.375258922576904 = 0.2840326726436615 + 1.0 * 6.091226100921631
Epoch 530, val loss: 0.6984396576881409
Epoch 540, training loss: 6.34283447265625 = 0.2645420730113983 + 1.0 * 6.078292369842529
Epoch 540, val loss: 0.6924703121185303
Epoch 550, training loss: 6.322777271270752 = 0.2463497668504715 + 1.0 * 6.076427459716797
Epoch 550, val loss: 0.6878967881202698
Epoch 560, training loss: 6.308168888092041 = 0.22937647998332977 + 1.0 * 6.078792572021484
Epoch 560, val loss: 0.6842939257621765
Epoch 570, training loss: 6.291219711303711 = 0.21370339393615723 + 1.0 * 6.077516078948975
Epoch 570, val loss: 0.6816508173942566
Epoch 580, training loss: 6.272171497344971 = 0.19937032461166382 + 1.0 * 6.072801113128662
Epoch 580, val loss: 0.6801941394805908
Epoch 590, training loss: 6.261408805847168 = 0.18619824945926666 + 1.0 * 6.0752105712890625
Epoch 590, val loss: 0.6796435117721558
Epoch 600, training loss: 6.245792865753174 = 0.17419400811195374 + 1.0 * 6.071599006652832
Epoch 600, val loss: 0.6798784136772156
Epoch 610, training loss: 6.232698917388916 = 0.16325461864471436 + 1.0 * 6.069444179534912
Epoch 610, val loss: 0.6809431314468384
Epoch 620, training loss: 6.220974445343018 = 0.1532461941242218 + 1.0 * 6.067728042602539
Epoch 620, val loss: 0.6826603412628174
Epoch 630, training loss: 6.2112956047058105 = 0.14412584900856018 + 1.0 * 6.067169666290283
Epoch 630, val loss: 0.6850438714027405
Epoch 640, training loss: 6.203278064727783 = 0.13572248816490173 + 1.0 * 6.0675554275512695
Epoch 640, val loss: 0.6880479454994202
Epoch 650, training loss: 6.193739414215088 = 0.1280004382133484 + 1.0 * 6.065739154815674
Epoch 650, val loss: 0.6914237141609192
Epoch 660, training loss: 6.18268346786499 = 0.12088768929243088 + 1.0 * 6.061795711517334
Epoch 660, val loss: 0.6952754259109497
Epoch 670, training loss: 6.174781322479248 = 0.1142793595790863 + 1.0 * 6.060502052307129
Epoch 670, val loss: 0.6995952725410461
Epoch 680, training loss: 6.175277233123779 = 0.10810864716768265 + 1.0 * 6.067168712615967
Epoch 680, val loss: 0.7042431831359863
Epoch 690, training loss: 6.165185451507568 = 0.10242284834384918 + 1.0 * 6.06276273727417
Epoch 690, val loss: 0.7089822888374329
Epoch 700, training loss: 6.15529727935791 = 0.0971456989645958 + 1.0 * 6.058151721954346
Epoch 700, val loss: 0.7140688300132751
Epoch 710, training loss: 6.148987770080566 = 0.09220325946807861 + 1.0 * 6.056784629821777
Epoch 710, val loss: 0.7193697690963745
Epoch 720, training loss: 6.150565147399902 = 0.08756992220878601 + 1.0 * 6.062995433807373
Epoch 720, val loss: 0.7247653007507324
Epoch 730, training loss: 6.140636920928955 = 0.08326466381549835 + 1.0 * 6.057372093200684
Epoch 730, val loss: 0.7302578091621399
Epoch 740, training loss: 6.133891582489014 = 0.07925278693437576 + 1.0 * 6.054638862609863
Epoch 740, val loss: 0.7359577417373657
Epoch 750, training loss: 6.128801345825195 = 0.07549165189266205 + 1.0 * 6.053309917449951
Epoch 750, val loss: 0.7418158650398254
Epoch 760, training loss: 6.123281955718994 = 0.0719490796327591 + 1.0 * 6.051332950592041
Epoch 760, val loss: 0.7478063106536865
Epoch 770, training loss: 6.121854782104492 = 0.0686119794845581 + 1.0 * 6.0532426834106445
Epoch 770, val loss: 0.7538555264472961
Epoch 780, training loss: 6.117471694946289 = 0.0654832273721695 + 1.0 * 6.05198860168457
Epoch 780, val loss: 0.7600541114807129
Epoch 790, training loss: 6.12177038192749 = 0.06255478411912918 + 1.0 * 6.059215545654297
Epoch 790, val loss: 0.7662175297737122
Epoch 800, training loss: 6.1083807945251465 = 0.059823062270879745 + 1.0 * 6.048557758331299
Epoch 800, val loss: 0.7724327445030212
Epoch 810, training loss: 6.10467004776001 = 0.05725213512778282 + 1.0 * 6.047418117523193
Epoch 810, val loss: 0.7787468433380127
Epoch 820, training loss: 6.10032320022583 = 0.054818060249090195 + 1.0 * 6.045505046844482
Epoch 820, val loss: 0.7850350141525269
Epoch 830, training loss: 6.1078338623046875 = 0.052519507706165314 + 1.0 * 6.055314540863037
Epoch 830, val loss: 0.7912817597389221
Epoch 840, training loss: 6.095737457275391 = 0.05037211999297142 + 1.0 * 6.045365333557129
Epoch 840, val loss: 0.7975349426269531
Epoch 850, training loss: 6.09302282333374 = 0.048353828489780426 + 1.0 * 6.044669151306152
Epoch 850, val loss: 0.8039252758026123
Epoch 860, training loss: 6.091602325439453 = 0.04644543677568436 + 1.0 * 6.045156955718994
Epoch 860, val loss: 0.8101709485054016
Epoch 870, training loss: 6.0868754386901855 = 0.04463867098093033 + 1.0 * 6.042236804962158
Epoch 870, val loss: 0.8164024353027344
Epoch 880, training loss: 6.085210800170898 = 0.042930569499731064 + 1.0 * 6.042280197143555
Epoch 880, val loss: 0.8226744532585144
Epoch 890, training loss: 6.086857795715332 = 0.041315264999866486 + 1.0 * 6.0455427169799805
Epoch 890, val loss: 0.8289101719856262
Epoch 900, training loss: 6.086751937866211 = 0.039789680391550064 + 1.0 * 6.046962261199951
Epoch 900, val loss: 0.8349241614341736
Epoch 910, training loss: 6.077927112579346 = 0.038361433893442154 + 1.0 * 6.039565563201904
Epoch 910, val loss: 0.841070830821991
Epoch 920, training loss: 6.075447082519531 = 0.037004780024290085 + 1.0 * 6.038442134857178
Epoch 920, val loss: 0.8471822142601013
Epoch 930, training loss: 6.07280158996582 = 0.03570855036377907 + 1.0 * 6.037093162536621
Epoch 930, val loss: 0.8531627655029297
Epoch 940, training loss: 6.077204704284668 = 0.03447414189577103 + 1.0 * 6.042730331420898
Epoch 940, val loss: 0.859162449836731
Epoch 950, training loss: 6.073005199432373 = 0.03330365568399429 + 1.0 * 6.039701461791992
Epoch 950, val loss: 0.8649654388427734
Epoch 960, training loss: 6.068051815032959 = 0.03220272809267044 + 1.0 * 6.035849094390869
Epoch 960, val loss: 0.8709467053413391
Epoch 970, training loss: 6.077329635620117 = 0.031150978058576584 + 1.0 * 6.046178817749023
Epoch 970, val loss: 0.876708447933197
Epoch 980, training loss: 6.064717769622803 = 0.030154885724186897 + 1.0 * 6.034563064575195
Epoch 980, val loss: 0.8823757171630859
Epoch 990, training loss: 6.063003063201904 = 0.02921094000339508 + 1.0 * 6.033792018890381
Epoch 990, val loss: 0.888183057308197
Epoch 1000, training loss: 6.06130313873291 = 0.02830391377210617 + 1.0 * 6.032999038696289
Epoch 1000, val loss: 0.8938440680503845
Epoch 1010, training loss: 6.068775653839111 = 0.027438120916485786 + 1.0 * 6.041337490081787
Epoch 1010, val loss: 0.8993434906005859
Epoch 1020, training loss: 6.058743953704834 = 0.02661816217005253 + 1.0 * 6.032125949859619
Epoch 1020, val loss: 0.9048029184341431
Epoch 1030, training loss: 6.057473182678223 = 0.02584080398082733 + 1.0 * 6.031632423400879
Epoch 1030, val loss: 0.9103901982307434
Epoch 1040, training loss: 6.0559821128845215 = 0.025092333555221558 + 1.0 * 6.030889987945557
Epoch 1040, val loss: 0.9157983660697937
Epoch 1050, training loss: 6.058828830718994 = 0.024373671039938927 + 1.0 * 6.034455299377441
Epoch 1050, val loss: 0.9210995435714722
Epoch 1060, training loss: 6.0535783767700195 = 0.023691678419709206 + 1.0 * 6.029886722564697
Epoch 1060, val loss: 0.9263697862625122
Epoch 1070, training loss: 6.0522003173828125 = 0.023037025704979897 + 1.0 * 6.029163360595703
Epoch 1070, val loss: 0.9316504001617432
Epoch 1080, training loss: 6.058821201324463 = 0.02240717224776745 + 1.0 * 6.03641414642334
Epoch 1080, val loss: 0.9367249011993408
Epoch 1090, training loss: 6.05034065246582 = 0.021810628473758698 + 1.0 * 6.028530120849609
Epoch 1090, val loss: 0.9418234825134277
Epoch 1100, training loss: 6.049261093139648 = 0.02124112658202648 + 1.0 * 6.028019905090332
Epoch 1100, val loss: 0.9470105171203613
Epoch 1110, training loss: 6.048079967498779 = 0.02068873681128025 + 1.0 * 6.02739143371582
Epoch 1110, val loss: 0.9519996047019958
Epoch 1120, training loss: 6.049709320068359 = 0.020155807957053185 + 1.0 * 6.029553413391113
Epoch 1120, val loss: 0.9569362998008728
Epoch 1130, training loss: 6.050525665283203 = 0.019647127017378807 + 1.0 * 6.03087854385376
Epoch 1130, val loss: 0.9618363976478577
Epoch 1140, training loss: 6.050716876983643 = 0.019158639013767242 + 1.0 * 6.031558036804199
Epoch 1140, val loss: 0.9666299223899841
Epoch 1150, training loss: 6.04419469833374 = 0.018690545111894608 + 1.0 * 6.025504112243652
Epoch 1150, val loss: 0.9714018702507019
Epoch 1160, training loss: 6.043386936187744 = 0.018239766359329224 + 1.0 * 6.025146961212158
Epoch 1160, val loss: 0.9761837720870972
Epoch 1170, training loss: 6.042076587677002 = 0.017802758142352104 + 1.0 * 6.024273872375488
Epoch 1170, val loss: 0.9808638691902161
Epoch 1180, training loss: 6.0509748458862305 = 0.017379768192768097 + 1.0 * 6.033595085144043
Epoch 1180, val loss: 0.9854475259780884
Epoch 1190, training loss: 6.045394420623779 = 0.01697850041091442 + 1.0 * 6.028416156768799
Epoch 1190, val loss: 0.9899617433547974
Epoch 1200, training loss: 6.039883613586426 = 0.016592029482126236 + 1.0 * 6.02329158782959
Epoch 1200, val loss: 0.9945238828659058
Epoch 1210, training loss: 6.044012546539307 = 0.01621907204389572 + 1.0 * 6.0277934074401855
Epoch 1210, val loss: 0.9989445209503174
Epoch 1220, training loss: 6.039717197418213 = 0.015857098624110222 + 1.0 * 6.023859977722168
Epoch 1220, val loss: 1.0032869577407837
Epoch 1230, training loss: 6.037189483642578 = 0.01551054697483778 + 1.0 * 6.021678924560547
Epoch 1230, val loss: 1.0077149868011475
Epoch 1240, training loss: 6.037029266357422 = 0.01517401821911335 + 1.0 * 6.021855354309082
Epoch 1240, val loss: 1.0121150016784668
Epoch 1250, training loss: 6.041819095611572 = 0.014846773818135262 + 1.0 * 6.02697229385376
Epoch 1250, val loss: 1.0162931680679321
Epoch 1260, training loss: 6.036584854125977 = 0.014533127658069134 + 1.0 * 6.022051811218262
Epoch 1260, val loss: 1.0204721689224243
Epoch 1270, training loss: 6.034394264221191 = 0.014231408014893532 + 1.0 * 6.020163059234619
Epoch 1270, val loss: 1.0247502326965332
Epoch 1280, training loss: 6.034345626831055 = 0.013936546631157398 + 1.0 * 6.020409107208252
Epoch 1280, val loss: 1.0288467407226562
Epoch 1290, training loss: 6.036997318267822 = 0.013649117201566696 + 1.0 * 6.023348331451416
Epoch 1290, val loss: 1.032843828201294
Epoch 1300, training loss: 6.0347065925598145 = 0.013374698348343372 + 1.0 * 6.021331787109375
Epoch 1300, val loss: 1.0368540287017822
Epoch 1310, training loss: 6.03087043762207 = 0.013109410181641579 + 1.0 * 6.01776123046875
Epoch 1310, val loss: 1.0409566164016724
Epoch 1320, training loss: 6.032664775848389 = 0.01285054162144661 + 1.0 * 6.0198140144348145
Epoch 1320, val loss: 1.0449450016021729
Epoch 1330, training loss: 6.032806873321533 = 0.012598708271980286 + 1.0 * 6.020208358764648
Epoch 1330, val loss: 1.0487879514694214
Epoch 1340, training loss: 6.033435821533203 = 0.012357029132544994 + 1.0 * 6.021078586578369
Epoch 1340, val loss: 1.052756667137146
Epoch 1350, training loss: 6.0305986404418945 = 0.012122889049351215 + 1.0 * 6.018475532531738
Epoch 1350, val loss: 1.0565500259399414
Epoch 1360, training loss: 6.027940273284912 = 0.011897135525941849 + 1.0 * 6.016043186187744
Epoch 1360, val loss: 1.060392141342163
Epoch 1370, training loss: 6.0271077156066895 = 0.011677712202072144 + 1.0 * 6.015429973602295
Epoch 1370, val loss: 1.0642681121826172
Epoch 1380, training loss: 6.027730941772461 = 0.011461236514151096 + 1.0 * 6.016269683837891
Epoch 1380, val loss: 1.0680164098739624
Epoch 1390, training loss: 6.031444072723389 = 0.011250008828938007 + 1.0 * 6.020194053649902
Epoch 1390, val loss: 1.0716584920883179
Epoch 1400, training loss: 6.029387474060059 = 0.011048915795981884 + 1.0 * 6.018338680267334
Epoch 1400, val loss: 1.0752884149551392
Epoch 1410, training loss: 6.025569438934326 = 0.010853368788957596 + 1.0 * 6.014716148376465
Epoch 1410, val loss: 1.0789587497711182
Epoch 1420, training loss: 6.026899814605713 = 0.010663686320185661 + 1.0 * 6.016236305236816
Epoch 1420, val loss: 1.082625389099121
Epoch 1430, training loss: 6.02647590637207 = 0.010477248579263687 + 1.0 * 6.015998840332031
Epoch 1430, val loss: 1.0861477851867676
Epoch 1440, training loss: 6.024521827697754 = 0.01029698085039854 + 1.0 * 6.014225006103516
Epoch 1440, val loss: 1.0896416902542114
Epoch 1450, training loss: 6.0271477699279785 = 0.010122033767402172 + 1.0 * 6.017025947570801
Epoch 1450, val loss: 1.0932282209396362
Epoch 1460, training loss: 6.024255752563477 = 0.009949678555130959 + 1.0 * 6.01430606842041
Epoch 1460, val loss: 1.096537470817566
Epoch 1470, training loss: 6.0219197273254395 = 0.0097844572737813 + 1.0 * 6.0121355056762695
Epoch 1470, val loss: 1.1000020503997803
Epoch 1480, training loss: 6.021656513214111 = 0.009622073732316494 + 1.0 * 6.0120344161987305
Epoch 1480, val loss: 1.1034692525863647
Epoch 1490, training loss: 6.0235066413879395 = 0.009462500922381878 + 1.0 * 6.014044284820557
Epoch 1490, val loss: 1.1068153381347656
Epoch 1500, training loss: 6.030882835388184 = 0.009306823834776878 + 1.0 * 6.021575927734375
Epoch 1500, val loss: 1.110075831413269
Epoch 1510, training loss: 6.02740478515625 = 0.00915857870131731 + 1.0 * 6.018246173858643
Epoch 1510, val loss: 1.1132320165634155
Epoch 1520, training loss: 6.020089626312256 = 0.009016635827720165 + 1.0 * 6.011073112487793
Epoch 1520, val loss: 1.1165943145751953
Epoch 1530, training loss: 6.01963996887207 = 0.00887672696262598 + 1.0 * 6.010763168334961
Epoch 1530, val loss: 1.1199480295181274
Epoch 1540, training loss: 6.019277572631836 = 0.008737118914723396 + 1.0 * 6.01054048538208
Epoch 1540, val loss: 1.1231422424316406
Epoch 1550, training loss: 6.022489070892334 = 0.008600210770964622 + 1.0 * 6.013888835906982
Epoch 1550, val loss: 1.1262837648391724
Epoch 1560, training loss: 6.019252777099609 = 0.008467272855341434 + 1.0 * 6.0107855796813965
Epoch 1560, val loss: 1.129528522491455
Epoch 1570, training loss: 6.022367000579834 = 0.008338600397109985 + 1.0 * 6.014028549194336
Epoch 1570, val loss: 1.1326698064804077
Epoch 1580, training loss: 6.018654823303223 = 0.008212700486183167 + 1.0 * 6.01044225692749
Epoch 1580, val loss: 1.1356852054595947
Epoch 1590, training loss: 6.017138957977295 = 0.00809152889996767 + 1.0 * 6.009047508239746
Epoch 1590, val loss: 1.1388014554977417
Epoch 1600, training loss: 6.015866279602051 = 0.00797198060899973 + 1.0 * 6.007894515991211
Epoch 1600, val loss: 1.1419708728790283
Epoch 1610, training loss: 6.018588066101074 = 0.007853305898606777 + 1.0 * 6.010734558105469
Epoch 1610, val loss: 1.144999623298645
Epoch 1620, training loss: 6.015690803527832 = 0.007737157866358757 + 1.0 * 6.007953643798828
Epoch 1620, val loss: 1.1479357481002808
Epoch 1630, training loss: 6.0173563957214355 = 0.007625249680131674 + 1.0 * 6.009731292724609
Epoch 1630, val loss: 1.151009202003479
Epoch 1640, training loss: 6.016504764556885 = 0.0075157624669373035 + 1.0 * 6.008988857269287
Epoch 1640, val loss: 1.1539881229400635
Epoch 1650, training loss: 6.015759468078613 = 0.007409135811030865 + 1.0 * 6.008350372314453
Epoch 1650, val loss: 1.1569815874099731
Epoch 1660, training loss: 6.0195536613464355 = 0.0073040253482759 + 1.0 * 6.01224946975708
Epoch 1660, val loss: 1.1598258018493652
Epoch 1670, training loss: 6.014474868774414 = 0.0072026788257062435 + 1.0 * 6.007272243499756
Epoch 1670, val loss: 1.162737250328064
Epoch 1680, training loss: 6.013840198516846 = 0.007103847339749336 + 1.0 * 6.0067362785339355
Epoch 1680, val loss: 1.1657112836837769
Epoch 1690, training loss: 6.019027233123779 = 0.0070049576461315155 + 1.0 * 6.012022495269775
Epoch 1690, val loss: 1.1684519052505493
Epoch 1700, training loss: 6.013087272644043 = 0.006909807678312063 + 1.0 * 6.0061774253845215
Epoch 1700, val loss: 1.171280026435852
Epoch 1710, training loss: 6.011597156524658 = 0.006817413028329611 + 1.0 * 6.004779815673828
Epoch 1710, val loss: 1.1742452383041382
Epoch 1720, training loss: 6.011237144470215 = 0.006725166458636522 + 1.0 * 6.004511833190918
Epoch 1720, val loss: 1.177097201347351
Epoch 1730, training loss: 6.024264812469482 = 0.006634577177464962 + 1.0 * 6.017630100250244
Epoch 1730, val loss: 1.1798465251922607
Epoch 1740, training loss: 6.015558242797852 = 0.006544556934386492 + 1.0 * 6.009013652801514
Epoch 1740, val loss: 1.1823315620422363
Epoch 1750, training loss: 6.011279106140137 = 0.006460849661380053 + 1.0 * 6.004818439483643
Epoch 1750, val loss: 1.185210108757019
Epoch 1760, training loss: 6.010103225708008 = 0.006376510486006737 + 1.0 * 6.003726482391357
Epoch 1760, val loss: 1.1879801750183105
Epoch 1770, training loss: 6.015316486358643 = 0.006292993668466806 + 1.0 * 6.009023666381836
Epoch 1770, val loss: 1.1906449794769287
Epoch 1780, training loss: 6.013368129730225 = 0.00621078023687005 + 1.0 * 6.007157325744629
Epoch 1780, val loss: 1.1932188272476196
Epoch 1790, training loss: 6.010313510894775 = 0.0061325752176344395 + 1.0 * 6.004180908203125
Epoch 1790, val loss: 1.195873498916626
Epoch 1800, training loss: 6.011704444885254 = 0.006056035868823528 + 1.0 * 6.005648612976074
Epoch 1800, val loss: 1.1986510753631592
Epoch 1810, training loss: 6.010764122009277 = 0.005979903042316437 + 1.0 * 6.004784107208252
Epoch 1810, val loss: 1.2012120485305786
Epoch 1820, training loss: 6.008765697479248 = 0.0059051415883004665 + 1.0 * 6.0028605461120605
Epoch 1820, val loss: 1.2038365602493286
Epoch 1830, training loss: 6.011963844299316 = 0.005831796210259199 + 1.0 * 6.006132125854492
Epoch 1830, val loss: 1.206400990486145
Epoch 1840, training loss: 6.009410858154297 = 0.0057593705132603645 + 1.0 * 6.0036516189575195
Epoch 1840, val loss: 1.208848476409912
Epoch 1850, training loss: 6.007627487182617 = 0.00568990595638752 + 1.0 * 6.001937389373779
Epoch 1850, val loss: 1.2114934921264648
Epoch 1860, training loss: 6.008125305175781 = 0.005621373187750578 + 1.0 * 6.002503871917725
Epoch 1860, val loss: 1.2141468524932861
Epoch 1870, training loss: 6.011404991149902 = 0.005552859976887703 + 1.0 * 6.005852222442627
Epoch 1870, val loss: 1.2165628671646118
Epoch 1880, training loss: 6.009370803833008 = 0.005485669244080782 + 1.0 * 6.003885269165039
Epoch 1880, val loss: 1.2189838886260986
Epoch 1890, training loss: 6.012999057769775 = 0.00542113883420825 + 1.0 * 6.007577896118164
Epoch 1890, val loss: 1.221474528312683
Epoch 1900, training loss: 6.007929801940918 = 0.005357890389859676 + 1.0 * 6.002572059631348
Epoch 1900, val loss: 1.2239974737167358
Epoch 1910, training loss: 6.005939960479736 = 0.0052960654720664024 + 1.0 * 6.000643730163574
Epoch 1910, val loss: 1.226489543914795
Epoch 1920, training loss: 6.007388114929199 = 0.005234135314822197 + 1.0 * 6.0021538734436035
Epoch 1920, val loss: 1.2289365530014038
Epoch 1930, training loss: 6.007421016693115 = 0.005172950215637684 + 1.0 * 6.002248287200928
Epoch 1930, val loss: 1.2312666177749634
Epoch 1940, training loss: 6.008293151855469 = 0.005113763269037008 + 1.0 * 6.003179550170898
Epoch 1940, val loss: 1.2336918115615845
Epoch 1950, training loss: 6.007965564727783 = 0.0050557078793644905 + 1.0 * 6.0029096603393555
Epoch 1950, val loss: 1.2360496520996094
Epoch 1960, training loss: 6.00510311126709 = 0.004998354706913233 + 1.0 * 6.000104904174805
Epoch 1960, val loss: 1.2383625507354736
Epoch 1970, training loss: 6.004114627838135 = 0.004943129140883684 + 1.0 * 5.999171733856201
Epoch 1970, val loss: 1.2407642602920532
Epoch 1980, training loss: 6.003662586212158 = 0.004888005554676056 + 1.0 * 5.998774528503418
Epoch 1980, val loss: 1.2431857585906982
Epoch 1990, training loss: 6.012074947357178 = 0.004833008162677288 + 1.0 * 6.007241725921631
Epoch 1990, val loss: 1.2454997301101685
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5867
Flip ASR: 0.5200/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.317497253417969 = 1.9437072277069092 + 1.0 * 8.37378978729248
Epoch 0, val loss: 1.9529674053192139
Epoch 10, training loss: 10.306511878967285 = 1.9333784580230713 + 1.0 * 8.373133659362793
Epoch 10, val loss: 1.9416929483413696
Epoch 20, training loss: 10.28929615020752 = 1.9201637506484985 + 1.0 * 8.369132041931152
Epoch 20, val loss: 1.927037000656128
Epoch 30, training loss: 10.24639892578125 = 1.9018805027008057 + 1.0 * 8.344518661499023
Epoch 30, val loss: 1.9067798852920532
Epoch 40, training loss: 10.081316947937012 = 1.8795101642608643 + 1.0 * 8.201807022094727
Epoch 40, val loss: 1.8832669258117676
Epoch 50, training loss: 9.461759567260742 = 1.8549389839172363 + 1.0 * 7.606820583343506
Epoch 50, val loss: 1.8583755493164062
Epoch 60, training loss: 8.992682456970215 = 1.8344860076904297 + 1.0 * 7.158196449279785
Epoch 60, val loss: 1.8397091627120972
Epoch 70, training loss: 8.67041015625 = 1.8219183683395386 + 1.0 * 6.848491668701172
Epoch 70, val loss: 1.8279250860214233
Epoch 80, training loss: 8.477276802062988 = 1.8085200786590576 + 1.0 * 6.668756484985352
Epoch 80, val loss: 1.814190149307251
Epoch 90, training loss: 8.360603332519531 = 1.796688199043274 + 1.0 * 6.563914775848389
Epoch 90, val loss: 1.8025151491165161
Epoch 100, training loss: 8.278665542602539 = 1.7849527597427368 + 1.0 * 6.493712425231934
Epoch 100, val loss: 1.7907490730285645
Epoch 110, training loss: 8.201455116271973 = 1.774179458618164 + 1.0 * 6.427275657653809
Epoch 110, val loss: 1.780224323272705
Epoch 120, training loss: 8.1334228515625 = 1.7637758255004883 + 1.0 * 6.3696465492248535
Epoch 120, val loss: 1.770630955696106
Epoch 130, training loss: 8.076942443847656 = 1.7520502805709839 + 1.0 * 6.324892044067383
Epoch 130, val loss: 1.760313630104065
Epoch 140, training loss: 8.03021240234375 = 1.7374229431152344 + 1.0 * 6.292789459228516
Epoch 140, val loss: 1.7478175163269043
Epoch 150, training loss: 7.988907814025879 = 1.7196292877197266 + 1.0 * 6.269278526306152
Epoch 150, val loss: 1.7331019639968872
Epoch 160, training loss: 7.948686122894287 = 1.6982961893081665 + 1.0 * 6.25039005279541
Epoch 160, val loss: 1.7156976461410522
Epoch 170, training loss: 7.907378196716309 = 1.6721678972244263 + 1.0 * 6.235210418701172
Epoch 170, val loss: 1.6947274208068848
Epoch 180, training loss: 7.862194061279297 = 1.640058994293213 + 1.0 * 6.222135066986084
Epoch 180, val loss: 1.6693038940429688
Epoch 190, training loss: 7.812751293182373 = 1.601298451423645 + 1.0 * 6.211452960968018
Epoch 190, val loss: 1.6387735605239868
Epoch 200, training loss: 7.7554030418396 = 1.5553975105285645 + 1.0 * 6.200005531311035
Epoch 200, val loss: 1.6028540134429932
Epoch 210, training loss: 7.692912578582764 = 1.5017757415771484 + 1.0 * 6.191136837005615
Epoch 210, val loss: 1.5610631704330444
Epoch 220, training loss: 7.625324249267578 = 1.4418694972991943 + 1.0 * 6.183454990386963
Epoch 220, val loss: 1.5145394802093506
Epoch 230, training loss: 7.552435398101807 = 1.377203106880188 + 1.0 * 6.175232410430908
Epoch 230, val loss: 1.4646739959716797
Epoch 240, training loss: 7.478630065917969 = 1.3094946146011353 + 1.0 * 6.169135570526123
Epoch 240, val loss: 1.412716031074524
Epoch 250, training loss: 7.405317306518555 = 1.2425086498260498 + 1.0 * 6.162808895111084
Epoch 250, val loss: 1.361716628074646
Epoch 260, training loss: 7.334290504455566 = 1.1780084371566772 + 1.0 * 6.1562819480896
Epoch 260, val loss: 1.312846302986145
Epoch 270, training loss: 7.269476890563965 = 1.1166911125183105 + 1.0 * 6.152785778045654
Epoch 270, val loss: 1.2665226459503174
Epoch 280, training loss: 7.206022262573242 = 1.0597469806671143 + 1.0 * 6.146275043487549
Epoch 280, val loss: 1.223598599433899
Epoch 290, training loss: 7.148062229156494 = 1.006849765777588 + 1.0 * 6.141212463378906
Epoch 290, val loss: 1.1838759183883667
Epoch 300, training loss: 7.093571186065674 = 0.9575942158699036 + 1.0 * 6.135976791381836
Epoch 300, val loss: 1.147074818611145
Epoch 310, training loss: 7.044304370880127 = 0.9113499522209167 + 1.0 * 6.1329545974731445
Epoch 310, val loss: 1.1130346059799194
Epoch 320, training loss: 6.997594833374023 = 0.8676394820213318 + 1.0 * 6.129955291748047
Epoch 320, val loss: 1.081168532371521
Epoch 330, training loss: 6.9493727684021 = 0.8255237936973572 + 1.0 * 6.123848915100098
Epoch 330, val loss: 1.0507746934890747
Epoch 340, training loss: 6.904968738555908 = 0.7841254472732544 + 1.0 * 6.120843410491943
Epoch 340, val loss: 1.0214191675186157
Epoch 350, training loss: 6.864029884338379 = 0.7434554696083069 + 1.0 * 6.120574474334717
Epoch 350, val loss: 0.9928892850875854
Epoch 360, training loss: 6.819240570068359 = 0.7037562131881714 + 1.0 * 6.115484237670898
Epoch 360, val loss: 0.9652822017669678
Epoch 370, training loss: 6.775323867797852 = 0.6645314693450928 + 1.0 * 6.11079216003418
Epoch 370, val loss: 0.9384174942970276
Epoch 380, training loss: 6.734477996826172 = 0.6257679462432861 + 1.0 * 6.108710289001465
Epoch 380, val loss: 0.9122284650802612
Epoch 390, training loss: 6.700065612792969 = 0.5880985856056213 + 1.0 * 6.111967086791992
Epoch 390, val loss: 0.8872121572494507
Epoch 400, training loss: 6.655649662017822 = 0.5519179701805115 + 1.0 * 6.103731632232666
Epoch 400, val loss: 0.8638823628425598
Epoch 410, training loss: 6.620334148406982 = 0.5172316431999207 + 1.0 * 6.103102684020996
Epoch 410, val loss: 0.8421347141265869
Epoch 420, training loss: 6.584691047668457 = 0.4843730330467224 + 1.0 * 6.10031795501709
Epoch 420, val loss: 0.8221048712730408
Epoch 430, training loss: 6.549806118011475 = 0.4532146751880646 + 1.0 * 6.096591472625732
Epoch 430, val loss: 0.8039515018463135
Epoch 440, training loss: 6.520206451416016 = 0.42384567856788635 + 1.0 * 6.096360683441162
Epoch 440, val loss: 0.7876073718070984
Epoch 450, training loss: 6.488793849945068 = 0.39644020795822144 + 1.0 * 6.092353820800781
Epoch 450, val loss: 0.7731298208236694
Epoch 460, training loss: 6.466681957244873 = 0.37084153294563293 + 1.0 * 6.0958404541015625
Epoch 460, val loss: 0.7603816390037537
Epoch 470, training loss: 6.436257362365723 = 0.347115695476532 + 1.0 * 6.089141845703125
Epoch 470, val loss: 0.7492435574531555
Epoch 480, training loss: 6.4155144691467285 = 0.3249659836292267 + 1.0 * 6.090548515319824
Epoch 480, val loss: 0.7395766973495483
Epoch 490, training loss: 6.389235973358154 = 0.3043625056743622 + 1.0 * 6.084873676300049
Epoch 490, val loss: 0.7312638163566589
Epoch 500, training loss: 6.366769313812256 = 0.28502514958381653 + 1.0 * 6.081744194030762
Epoch 500, val loss: 0.7241294384002686
Epoch 510, training loss: 6.358902454376221 = 0.2667718529701233 + 1.0 * 6.092130661010742
Epoch 510, val loss: 0.717986524105072
Epoch 520, training loss: 6.333644390106201 = 0.24980252981185913 + 1.0 * 6.083841800689697
Epoch 520, val loss: 0.712661862373352
Epoch 530, training loss: 6.312821865081787 = 0.2339615523815155 + 1.0 * 6.078860282897949
Epoch 530, val loss: 0.708306610584259
Epoch 540, training loss: 6.29373025894165 = 0.21892935037612915 + 1.0 * 6.074800968170166
Epoch 540, val loss: 0.7046432495117188
Epoch 550, training loss: 6.277640342712402 = 0.20461420714855194 + 1.0 * 6.073026180267334
Epoch 550, val loss: 0.7015306949615479
Epoch 560, training loss: 6.267468452453613 = 0.19100899994373322 + 1.0 * 6.0764594078063965
Epoch 560, val loss: 0.6989847421646118
Epoch 570, training loss: 6.253328800201416 = 0.17827115952968597 + 1.0 * 6.075057506561279
Epoch 570, val loss: 0.6969822645187378
Epoch 580, training loss: 6.235051155090332 = 0.16641150414943695 + 1.0 * 6.068639755249023
Epoch 580, val loss: 0.6956039667129517
Epoch 590, training loss: 6.2234787940979 = 0.15527334809303284 + 1.0 * 6.0682053565979
Epoch 590, val loss: 0.6948198080062866
Epoch 600, training loss: 6.212015628814697 = 0.14481288194656372 + 1.0 * 6.067202568054199
Epoch 600, val loss: 0.6945478916168213
Epoch 610, training loss: 6.200092792510986 = 0.135090172290802 + 1.0 * 6.06500244140625
Epoch 610, val loss: 0.694783091545105
Epoch 620, training loss: 6.192389488220215 = 0.1261238306760788 + 1.0 * 6.06626558303833
Epoch 620, val loss: 0.6955742835998535
Epoch 630, training loss: 6.18269681930542 = 0.1178688332438469 + 1.0 * 6.064827919006348
Epoch 630, val loss: 0.6967998147010803
Epoch 640, training loss: 6.175143241882324 = 0.11033854633569717 + 1.0 * 6.064804553985596
Epoch 640, val loss: 0.6984972357749939
Epoch 650, training loss: 6.165199279785156 = 0.10341574251651764 + 1.0 * 6.061783313751221
Epoch 650, val loss: 0.7007327079772949
Epoch 660, training loss: 6.155796527862549 = 0.09702721983194351 + 1.0 * 6.058769226074219
Epoch 660, val loss: 0.7033672332763672
Epoch 670, training loss: 6.149440288543701 = 0.09113822132349014 + 1.0 * 6.05830192565918
Epoch 670, val loss: 0.7063940167427063
Epoch 680, training loss: 6.14634895324707 = 0.0857383981347084 + 1.0 * 6.060610771179199
Epoch 680, val loss: 0.709712028503418
Epoch 690, training loss: 6.136752128601074 = 0.0808374360203743 + 1.0 * 6.055914878845215
Epoch 690, val loss: 0.7132602334022522
Epoch 700, training loss: 6.1309075355529785 = 0.0763494223356247 + 1.0 * 6.054558277130127
Epoch 700, val loss: 0.7171184420585632
Epoch 710, training loss: 6.125765323638916 = 0.07220833003520966 + 1.0 * 6.0535569190979
Epoch 710, val loss: 0.7212052345275879
Epoch 720, training loss: 6.124341011047363 = 0.06838548928499222 + 1.0 * 6.055955410003662
Epoch 720, val loss: 0.7254825830459595
Epoch 730, training loss: 6.126091957092285 = 0.06488069891929626 + 1.0 * 6.061211109161377
Epoch 730, val loss: 0.7298086881637573
Epoch 740, training loss: 6.113470554351807 = 0.0616891123354435 + 1.0 * 6.05178165435791
Epoch 740, val loss: 0.7342251539230347
Epoch 750, training loss: 6.108506679534912 = 0.05874070152640343 + 1.0 * 6.0497660636901855
Epoch 750, val loss: 0.7388023734092712
Epoch 760, training loss: 6.104991436004639 = 0.05598828196525574 + 1.0 * 6.0490031242370605
Epoch 760, val loss: 0.7434306144714355
Epoch 770, training loss: 6.101398468017578 = 0.05341750755906105 + 1.0 * 6.047980785369873
Epoch 770, val loss: 0.748148500919342
Epoch 780, training loss: 6.117887496948242 = 0.05102001503109932 + 1.0 * 6.066867351531982
Epoch 780, val loss: 0.7529054880142212
Epoch 790, training loss: 6.097935199737549 = 0.04881608486175537 + 1.0 * 6.049118995666504
Epoch 790, val loss: 0.757654070854187
Epoch 800, training loss: 6.093081474304199 = 0.04677802324295044 + 1.0 * 6.0463032722473145
Epoch 800, val loss: 0.7624169588088989
Epoch 810, training loss: 6.090887069702148 = 0.04486190155148506 + 1.0 * 6.046025276184082
Epoch 810, val loss: 0.7671970725059509
Epoch 820, training loss: 6.093169689178467 = 0.043058108538389206 + 1.0 * 6.050111770629883
Epoch 820, val loss: 0.7719577550888062
Epoch 830, training loss: 6.087812900543213 = 0.04136700555682182 + 1.0 * 6.046445846557617
Epoch 830, val loss: 0.7767105102539062
Epoch 840, training loss: 6.089608192443848 = 0.03978469967842102 + 1.0 * 6.04982328414917
Epoch 840, val loss: 0.7814446091651917
Epoch 850, training loss: 6.083407402038574 = 0.03829483315348625 + 1.0 * 6.045112609863281
Epoch 850, val loss: 0.7861166000366211
Epoch 860, training loss: 6.078614711761475 = 0.036894917488098145 + 1.0 * 6.041719913482666
Epoch 860, val loss: 0.7907673716545105
Epoch 870, training loss: 6.077071189880371 = 0.03556640073657036 + 1.0 * 6.041504859924316
Epoch 870, val loss: 0.7954331636428833
Epoch 880, training loss: 6.07775354385376 = 0.03430738300085068 + 1.0 * 6.043446063995361
Epoch 880, val loss: 0.8000648021697998
Epoch 890, training loss: 6.081642150878906 = 0.03311975300312042 + 1.0 * 6.048522472381592
Epoch 890, val loss: 0.8046403527259827
Epoch 900, training loss: 6.073111057281494 = 0.03200717642903328 + 1.0 * 6.041103839874268
Epoch 900, val loss: 0.809124767780304
Epoch 910, training loss: 6.069485664367676 = 0.03095916472375393 + 1.0 * 6.03852653503418
Epoch 910, val loss: 0.813581645488739
Epoch 920, training loss: 6.067324161529541 = 0.029961541295051575 + 1.0 * 6.037362575531006
Epoch 920, val loss: 0.8180036544799805
Epoch 930, training loss: 6.0659050941467285 = 0.029004527255892754 + 1.0 * 6.036900520324707
Epoch 930, val loss: 0.8224039077758789
Epoch 940, training loss: 6.073686599731445 = 0.028088927268981934 + 1.0 * 6.045597553253174
Epoch 940, val loss: 0.8267935514450073
Epoch 950, training loss: 6.066985130310059 = 0.02722265012562275 + 1.0 * 6.039762496948242
Epoch 950, val loss: 0.8311261534690857
Epoch 960, training loss: 6.061742782592773 = 0.026405401527881622 + 1.0 * 6.035337448120117
Epoch 960, val loss: 0.8353981971740723
Epoch 970, training loss: 6.059630870819092 = 0.02562042884528637 + 1.0 * 6.034010410308838
Epoch 970, val loss: 0.8396701216697693
Epoch 980, training loss: 6.066688060760498 = 0.02486727572977543 + 1.0 * 6.041821002960205
Epoch 980, val loss: 0.843889057636261
Epoch 990, training loss: 6.064513683319092 = 0.024149663746356964 + 1.0 * 6.040363788604736
Epoch 990, val loss: 0.8480099439620972
Epoch 1000, training loss: 6.056799411773682 = 0.023477686569094658 + 1.0 * 6.033321857452393
Epoch 1000, val loss: 0.8520443439483643
Epoch 1010, training loss: 6.054826259613037 = 0.022831980139017105 + 1.0 * 6.031994342803955
Epoch 1010, val loss: 0.8561003804206848
Epoch 1020, training loss: 6.0533366203308105 = 0.022206993773579597 + 1.0 * 6.031129837036133
Epoch 1020, val loss: 0.8601479530334473
Epoch 1030, training loss: 6.062450408935547 = 0.02160465158522129 + 1.0 * 6.04084587097168
Epoch 1030, val loss: 0.86417156457901
Epoch 1040, training loss: 6.057931423187256 = 0.021027332171797752 + 1.0 * 6.0369038581848145
Epoch 1040, val loss: 0.8680931329727173
Epoch 1050, training loss: 6.049772262573242 = 0.020483264699578285 + 1.0 * 6.0292887687683105
Epoch 1050, val loss: 0.8720131516456604
Epoch 1060, training loss: 6.049072742462158 = 0.019958436489105225 + 1.0 * 6.029114246368408
Epoch 1060, val loss: 0.8759000897407532
Epoch 1070, training loss: 6.055361270904541 = 0.019448498263955116 + 1.0 * 6.035912990570068
Epoch 1070, val loss: 0.8797610402107239
Epoch 1080, training loss: 6.048721790313721 = 0.018961802124977112 + 1.0 * 6.029759883880615
Epoch 1080, val loss: 0.8835251331329346
Epoch 1090, training loss: 6.050003528594971 = 0.018496695905923843 + 1.0 * 6.0315070152282715
Epoch 1090, val loss: 0.8872883915901184
Epoch 1100, training loss: 6.046338081359863 = 0.018049459904432297 + 1.0 * 6.028288841247559
Epoch 1100, val loss: 0.890986442565918
Epoch 1110, training loss: 6.048302173614502 = 0.017618343234062195 + 1.0 * 6.030683994293213
Epoch 1110, val loss: 0.8946887254714966
Epoch 1120, training loss: 6.044426918029785 = 0.017202354967594147 + 1.0 * 6.027224540710449
Epoch 1120, val loss: 0.8982994556427002
Epoch 1130, training loss: 6.044158935546875 = 0.01680113933980465 + 1.0 * 6.027357578277588
Epoch 1130, val loss: 0.9019222855567932
Epoch 1140, training loss: 6.047399520874023 = 0.01641351357102394 + 1.0 * 6.0309858322143555
Epoch 1140, val loss: 0.9055152535438538
Epoch 1150, training loss: 6.040982246398926 = 0.016043193638324738 + 1.0 * 6.024939060211182
Epoch 1150, val loss: 0.909031331539154
Epoch 1160, training loss: 6.039566993713379 = 0.01568610407412052 + 1.0 * 6.023880958557129
Epoch 1160, val loss: 0.9125251173973083
Epoch 1170, training loss: 6.040938854217529 = 0.015338340774178505 + 1.0 * 6.025600433349609
Epoch 1170, val loss: 0.916013777256012
Epoch 1180, training loss: 6.042009353637695 = 0.0150009635835886 + 1.0 * 6.027008533477783
Epoch 1180, val loss: 0.9194501638412476
Epoch 1190, training loss: 6.044557094573975 = 0.01467722374945879 + 1.0 * 6.029880046844482
Epoch 1190, val loss: 0.9228354096412659
Epoch 1200, training loss: 6.037973880767822 = 0.014369089156389236 + 1.0 * 6.023604869842529
Epoch 1200, val loss: 0.9261833429336548
Epoch 1210, training loss: 6.035408973693848 = 0.014070668257772923 + 1.0 * 6.02133846282959
Epoch 1210, val loss: 0.9294753074645996
Epoch 1220, training loss: 6.03456974029541 = 0.01377873308956623 + 1.0 * 6.020791053771973
Epoch 1220, val loss: 0.932781457901001
Epoch 1230, training loss: 6.035802841186523 = 0.013493383303284645 + 1.0 * 6.022309303283691
Epoch 1230, val loss: 0.9360716342926025
Epoch 1240, training loss: 6.03911018371582 = 0.013217189349234104 + 1.0 * 6.025893211364746
Epoch 1240, val loss: 0.9393627047538757
Epoch 1250, training loss: 6.035441875457764 = 0.01295266393572092 + 1.0 * 6.022489070892334
Epoch 1250, val loss: 0.9425513744354248
Epoch 1260, training loss: 6.033932685852051 = 0.012700098566710949 + 1.0 * 6.021232604980469
Epoch 1260, val loss: 0.9456725120544434
Epoch 1270, training loss: 6.031507968902588 = 0.012452788650989532 + 1.0 * 6.019055366516113
Epoch 1270, val loss: 0.9488239288330078
Epoch 1280, training loss: 6.040468692779541 = 0.012209671549499035 + 1.0 * 6.028258800506592
Epoch 1280, val loss: 0.9519784450531006
Epoch 1290, training loss: 6.033014297485352 = 0.0119745172560215 + 1.0 * 6.021039962768555
Epoch 1290, val loss: 0.9550269246101379
Epoch 1300, training loss: 6.0306501388549805 = 0.011749602854251862 + 1.0 * 6.018900394439697
Epoch 1300, val loss: 0.9580889940261841
Epoch 1310, training loss: 6.03129768371582 = 0.01152994204312563 + 1.0 * 6.019767761230469
Epoch 1310, val loss: 0.9611478447914124
Epoch 1320, training loss: 6.0314435958862305 = 0.011314013041555882 + 1.0 * 6.020129680633545
Epoch 1320, val loss: 0.964200496673584
Epoch 1330, training loss: 6.027956008911133 = 0.011106843128800392 + 1.0 * 6.016849040985107
Epoch 1330, val loss: 0.9671579003334045
Epoch 1340, training loss: 6.027487277984619 = 0.010907979682087898 + 1.0 * 6.0165791511535645
Epoch 1340, val loss: 0.9700626134872437
Epoch 1350, training loss: 6.031033039093018 = 0.010711511597037315 + 1.0 * 6.020321369171143
Epoch 1350, val loss: 0.9730210900306702
Epoch 1360, training loss: 6.028991222381592 = 0.010518824681639671 + 1.0 * 6.018472194671631
Epoch 1360, val loss: 0.9759657382965088
Epoch 1370, training loss: 6.026932716369629 = 0.010334249585866928 + 1.0 * 6.016598701477051
Epoch 1370, val loss: 0.9788647890090942
Epoch 1380, training loss: 6.026909828186035 = 0.01015494391322136 + 1.0 * 6.016755104064941
Epoch 1380, val loss: 0.981713056564331
Epoch 1390, training loss: 6.026117324829102 = 0.009979278780519962 + 1.0 * 6.016138076782227
Epoch 1390, val loss: 0.9845672845840454
Epoch 1400, training loss: 6.025060653686523 = 0.009807340800762177 + 1.0 * 6.01525354385376
Epoch 1400, val loss: 0.9874109625816345
Epoch 1410, training loss: 6.026592254638672 = 0.009639824740588665 + 1.0 * 6.0169525146484375
Epoch 1410, val loss: 0.9902390241622925
Epoch 1420, training loss: 6.032390117645264 = 0.009476891718804836 + 1.0 * 6.022913455963135
Epoch 1420, val loss: 0.9930856227874756
Epoch 1430, training loss: 6.026557922363281 = 0.009320521727204323 + 1.0 * 6.017237186431885
Epoch 1430, val loss: 0.9958021640777588
Epoch 1440, training loss: 6.025659084320068 = 0.009169966913759708 + 1.0 * 6.016489028930664
Epoch 1440, val loss: 0.9984985589981079
Epoch 1450, training loss: 6.023784160614014 = 0.00902269035577774 + 1.0 * 6.014761447906494
Epoch 1450, val loss: 1.0011818408966064
Epoch 1460, training loss: 6.0216569900512695 = 0.00887736864387989 + 1.0 * 6.012779712677002
Epoch 1460, val loss: 1.003886342048645
Epoch 1470, training loss: 6.02594518661499 = 0.008734630420804024 + 1.0 * 6.017210483551025
Epoch 1470, val loss: 1.0065741539001465
Epoch 1480, training loss: 6.021488666534424 = 0.008595995604991913 + 1.0 * 6.012892723083496
Epoch 1480, val loss: 1.0092666149139404
Epoch 1490, training loss: 6.022938251495361 = 0.008461897261440754 + 1.0 * 6.014476299285889
Epoch 1490, val loss: 1.0118939876556396
Epoch 1500, training loss: 6.022036552429199 = 0.008330889977514744 + 1.0 * 6.013705730438232
Epoch 1500, val loss: 1.0145070552825928
Epoch 1510, training loss: 6.019433498382568 = 0.008203104138374329 + 1.0 * 6.01123046875
Epoch 1510, val loss: 1.0170953273773193
Epoch 1520, training loss: 6.020105361938477 = 0.00807823333889246 + 1.0 * 6.012027263641357
Epoch 1520, val loss: 1.0196958780288696
Epoch 1530, training loss: 6.0207200050354 = 0.007955163717269897 + 1.0 * 6.012764930725098
Epoch 1530, val loss: 1.0222866535186768
Epoch 1540, training loss: 6.021215438842773 = 0.0078348433598876 + 1.0 * 6.013380527496338
Epoch 1540, val loss: 1.0248439311981201
Epoch 1550, training loss: 6.019820690155029 = 0.0077187176793813705 + 1.0 * 6.012102127075195
Epoch 1550, val loss: 1.0273874998092651
Epoch 1560, training loss: 6.02012825012207 = 0.007606076542288065 + 1.0 * 6.012522220611572
Epoch 1560, val loss: 1.0298579931259155
Epoch 1570, training loss: 6.016427040100098 = 0.0074963998049497604 + 1.0 * 6.008930683135986
Epoch 1570, val loss: 1.032300591468811
Epoch 1580, training loss: 6.016408920288086 = 0.00738845719024539 + 1.0 * 6.0090203285217285
Epoch 1580, val loss: 1.0347458124160767
Epoch 1590, training loss: 6.020577430725098 = 0.007281700149178505 + 1.0 * 6.013295650482178
Epoch 1590, val loss: 1.037246823310852
Epoch 1600, training loss: 6.019189357757568 = 0.007177980616688728 + 1.0 * 6.012011528015137
Epoch 1600, val loss: 1.0396807193756104
Epoch 1610, training loss: 6.0162553787231445 = 0.007077796384692192 + 1.0 * 6.0091776847839355
Epoch 1610, val loss: 1.0420411825180054
Epoch 1620, training loss: 6.015019416809082 = 0.0069807288236916065 + 1.0 * 6.008038520812988
Epoch 1620, val loss: 1.0443791151046753
Epoch 1630, training loss: 6.015635967254639 = 0.0068839192390441895 + 1.0 * 6.00875186920166
Epoch 1630, val loss: 1.0467926263809204
Epoch 1640, training loss: 6.01945686340332 = 0.006788045167922974 + 1.0 * 6.012668609619141
Epoch 1640, val loss: 1.0491881370544434
Epoch 1650, training loss: 6.018099784851074 = 0.006695756688714027 + 1.0 * 6.011404037475586
Epoch 1650, val loss: 1.0515267848968506
Epoch 1660, training loss: 6.014781951904297 = 0.006605965085327625 + 1.0 * 6.008175849914551
Epoch 1660, val loss: 1.0538055896759033
Epoch 1670, training loss: 6.0125837326049805 = 0.0065180049277842045 + 1.0 * 6.006065845489502
Epoch 1670, val loss: 1.056117057800293
Epoch 1680, training loss: 6.017712593078613 = 0.006430195644497871 + 1.0 * 6.011282444000244
Epoch 1680, val loss: 1.0584238767623901
Epoch 1690, training loss: 6.014102458953857 = 0.006344379857182503 + 1.0 * 6.007758140563965
Epoch 1690, val loss: 1.0607184171676636
Epoch 1700, training loss: 6.014552593231201 = 0.006261794362217188 + 1.0 * 6.008290767669678
Epoch 1700, val loss: 1.0629435777664185
Epoch 1710, training loss: 6.011364936828613 = 0.006181616801768541 + 1.0 * 6.005183219909668
Epoch 1710, val loss: 1.0651389360427856
Epoch 1720, training loss: 6.010810852050781 = 0.006101670674979687 + 1.0 * 6.004709243774414
Epoch 1720, val loss: 1.0673803091049194
Epoch 1730, training loss: 6.017962455749512 = 0.006022173445671797 + 1.0 * 6.0119404792785645
Epoch 1730, val loss: 1.0696629285812378
Epoch 1740, training loss: 6.012818813323975 = 0.0059452420100569725 + 1.0 * 6.006873607635498
Epoch 1740, val loss: 1.0719196796417236
Epoch 1750, training loss: 6.017096996307373 = 0.005870702210813761 + 1.0 * 6.011226177215576
Epoch 1750, val loss: 1.074064016342163
Epoch 1760, training loss: 6.010993003845215 = 0.00579792819917202 + 1.0 * 6.005195140838623
Epoch 1760, val loss: 1.0762240886688232
Epoch 1770, training loss: 6.011941909790039 = 0.0057268645614385605 + 1.0 * 6.0062150955200195
Epoch 1770, val loss: 1.0783405303955078
Epoch 1780, training loss: 6.010792255401611 = 0.005656805355101824 + 1.0 * 6.005135536193848
Epoch 1780, val loss: 1.0804774761199951
Epoch 1790, training loss: 6.010064601898193 = 0.005587835796177387 + 1.0 * 6.004476547241211
Epoch 1790, val loss: 1.0825870037078857
Epoch 1800, training loss: 6.011518478393555 = 0.005520033650100231 + 1.0 * 6.005998611450195
Epoch 1800, val loss: 1.08470618724823
Epoch 1810, training loss: 6.010460376739502 = 0.005453094374388456 + 1.0 * 6.005007266998291
Epoch 1810, val loss: 1.0868291854858398
Epoch 1820, training loss: 6.008045196533203 = 0.0053878785111010075 + 1.0 * 6.002657413482666
Epoch 1820, val loss: 1.0889174938201904
Epoch 1830, training loss: 6.008903503417969 = 0.005323763936758041 + 1.0 * 6.003579616546631
Epoch 1830, val loss: 1.0910158157348633
Epoch 1840, training loss: 6.009577751159668 = 0.005260067526251078 + 1.0 * 6.004317760467529
Epoch 1840, val loss: 1.0931336879730225
Epoch 1850, training loss: 6.015008926391602 = 0.005197727587074041 + 1.0 * 6.0098114013671875
Epoch 1850, val loss: 1.0952597856521606
Epoch 1860, training loss: 6.00683069229126 = 0.005137584637850523 + 1.0 * 6.001693248748779
Epoch 1860, val loss: 1.0972473621368408
Epoch 1870, training loss: 6.005764961242676 = 0.005079454742372036 + 1.0 * 6.000685691833496
Epoch 1870, val loss: 1.099249005317688
Epoch 1880, training loss: 6.008556842803955 = 0.005020866170525551 + 1.0 * 6.003535747528076
Epoch 1880, val loss: 1.101285457611084
Epoch 1890, training loss: 6.0074334144592285 = 0.004962468985468149 + 1.0 * 6.002470970153809
Epoch 1890, val loss: 1.1033604145050049
Epoch 1900, training loss: 6.007813453674316 = 0.004906967282295227 + 1.0 * 6.002906322479248
Epoch 1900, val loss: 1.1053048372268677
Epoch 1910, training loss: 6.007887363433838 = 0.004852186422795057 + 1.0 * 6.003035068511963
Epoch 1910, val loss: 1.1072880029678345
Epoch 1920, training loss: 6.005932331085205 = 0.004798136651515961 + 1.0 * 6.001134395599365
Epoch 1920, val loss: 1.1092798709869385
Epoch 1930, training loss: 6.0092668533325195 = 0.004744675476104021 + 1.0 * 6.004522323608398
Epoch 1930, val loss: 1.1112631559371948
Epoch 1940, training loss: 6.005107879638672 = 0.004692411981523037 + 1.0 * 6.000415325164795
Epoch 1940, val loss: 1.113197922706604
Epoch 1950, training loss: 6.0042643547058105 = 0.004640985280275345 + 1.0 * 5.9996232986450195
Epoch 1950, val loss: 1.1151466369628906
Epoch 1960, training loss: 6.0087456703186035 = 0.004589756950736046 + 1.0 * 6.004156112670898
Epoch 1960, val loss: 1.1171244382858276
Epoch 1970, training loss: 6.0047430992126465 = 0.004539327695965767 + 1.0 * 6.000203609466553
Epoch 1970, val loss: 1.119141697883606
Epoch 1980, training loss: 6.006404876708984 = 0.004490373190492392 + 1.0 * 6.0019145011901855
Epoch 1980, val loss: 1.121049404144287
Epoch 1990, training loss: 6.004261016845703 = 0.004442650359123945 + 1.0 * 5.999818325042725
Epoch 1990, val loss: 1.122955560684204
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9041
Flip ASR: 0.8844/225 nodes
The final ASR:0.72571, 0.13251, Accuracy:0.80864, 0.01145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9556])
updated graph: torch.Size([2, 10590])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.97417, 0.00904, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.334528923034668 = 1.9607630968093872 + 1.0 * 8.37376594543457
Epoch 0, val loss: 1.9511922597885132
Epoch 10, training loss: 10.322999954223633 = 1.94989013671875 + 1.0 * 8.373109817504883
Epoch 10, val loss: 1.9409774541854858
Epoch 20, training loss: 10.30543327331543 = 1.9364445209503174 + 1.0 * 8.368988990783691
Epoch 20, val loss: 1.9279098510742188
Epoch 30, training loss: 10.255671501159668 = 1.9177981615066528 + 1.0 * 8.337873458862305
Epoch 30, val loss: 1.9095854759216309
Epoch 40, training loss: 10.00366497039795 = 1.8949638605117798 + 1.0 * 8.1087007522583
Epoch 40, val loss: 1.887695550918579
Epoch 50, training loss: 9.29001522064209 = 1.87002432346344 + 1.0 * 7.4199910163879395
Epoch 50, val loss: 1.863524317741394
Epoch 60, training loss: 8.892438888549805 = 1.8496114015579224 + 1.0 * 7.042827129364014
Epoch 60, val loss: 1.8452396392822266
Epoch 70, training loss: 8.564417839050293 = 1.8353887796401978 + 1.0 * 6.729029178619385
Epoch 70, val loss: 1.832208514213562
Epoch 80, training loss: 8.39945125579834 = 1.8223456144332886 + 1.0 * 6.577105522155762
Epoch 80, val loss: 1.8202743530273438
Epoch 90, training loss: 8.310443878173828 = 1.8087778091430664 + 1.0 * 6.50166654586792
Epoch 90, val loss: 1.8086187839508057
Epoch 100, training loss: 8.235228538513184 = 1.7952970266342163 + 1.0 * 6.439931392669678
Epoch 100, val loss: 1.797635555267334
Epoch 110, training loss: 8.173373222351074 = 1.7835270166397095 + 1.0 * 6.389846324920654
Epoch 110, val loss: 1.7880858182907104
Epoch 120, training loss: 8.125069618225098 = 1.7721844911575317 + 1.0 * 6.352884769439697
Epoch 120, val loss: 1.778596043586731
Epoch 130, training loss: 8.0789794921875 = 1.7598646879196167 + 1.0 * 6.319114685058594
Epoch 130, val loss: 1.768333911895752
Epoch 140, training loss: 8.036291122436523 = 1.7458746433258057 + 1.0 * 6.290416240692139
Epoch 140, val loss: 1.7569944858551025
Epoch 150, training loss: 7.998615264892578 = 1.7295228242874146 + 1.0 * 6.269092559814453
Epoch 150, val loss: 1.7439533472061157
Epoch 160, training loss: 7.961415767669678 = 1.7096995115280151 + 1.0 * 6.251716136932373
Epoch 160, val loss: 1.72832453250885
Epoch 170, training loss: 7.9181647300720215 = 1.6860036849975586 + 1.0 * 6.232161045074463
Epoch 170, val loss: 1.7096474170684814
Epoch 180, training loss: 7.8739776611328125 = 1.6573054790496826 + 1.0 * 6.216672420501709
Epoch 180, val loss: 1.6869577169418335
Epoch 190, training loss: 7.8236918449401855 = 1.6229987144470215 + 1.0 * 6.200693130493164
Epoch 190, val loss: 1.6596640348434448
Epoch 200, training loss: 7.7699079513549805 = 1.5815895795822144 + 1.0 * 6.188318252563477
Epoch 200, val loss: 1.6266679763793945
Epoch 210, training loss: 7.709929943084717 = 1.532132625579834 + 1.0 * 6.177797317504883
Epoch 210, val loss: 1.5871752500534058
Epoch 220, training loss: 7.644681453704834 = 1.4751754999160767 + 1.0 * 6.169506072998047
Epoch 220, val loss: 1.542091727256775
Epoch 230, training loss: 7.576313018798828 = 1.4132426977157593 + 1.0 * 6.163070201873779
Epoch 230, val loss: 1.4936045408248901
Epoch 240, training loss: 7.503320693969727 = 1.3473820686340332 + 1.0 * 6.155938625335693
Epoch 240, val loss: 1.4421404600143433
Epoch 250, training loss: 7.431449890136719 = 1.2795536518096924 + 1.0 * 6.1518964767456055
Epoch 250, val loss: 1.3892611265182495
Epoch 260, training loss: 7.358372211456299 = 1.2125180959701538 + 1.0 * 6.1458539962768555
Epoch 260, val loss: 1.337166428565979
Epoch 270, training loss: 7.2899088859558105 = 1.147212028503418 + 1.0 * 6.142696857452393
Epoch 270, val loss: 1.2867101430892944
Epoch 280, training loss: 7.221027374267578 = 1.0854648351669312 + 1.0 * 6.135562419891357
Epoch 280, val loss: 1.2393357753753662
Epoch 290, training loss: 7.160830020904541 = 1.027114987373352 + 1.0 * 6.1337151527404785
Epoch 290, val loss: 1.1950039863586426
Epoch 300, training loss: 7.099826812744141 = 0.9731897115707397 + 1.0 * 6.126636981964111
Epoch 300, val loss: 1.154517412185669
Epoch 310, training loss: 7.04550313949585 = 0.9233231544494629 + 1.0 * 6.122179985046387
Epoch 310, val loss: 1.1175085306167603
Epoch 320, training loss: 6.998493194580078 = 0.8770265579223633 + 1.0 * 6.121466636657715
Epoch 320, val loss: 1.083426594734192
Epoch 330, training loss: 6.9508562088012695 = 0.8345797657966614 + 1.0 * 6.116276264190674
Epoch 330, val loss: 1.052585244178772
Epoch 340, training loss: 6.906002998352051 = 0.7959586977958679 + 1.0 * 6.110044479370117
Epoch 340, val loss: 1.024954080581665
Epoch 350, training loss: 6.867187023162842 = 0.760500967502594 + 1.0 * 6.106686115264893
Epoch 350, val loss: 1.0000114440917969
Epoch 360, training loss: 6.841665744781494 = 0.7279314994812012 + 1.0 * 6.113734245300293
Epoch 360, val loss: 0.9776614904403687
Epoch 370, training loss: 6.800990581512451 = 0.6983408331871033 + 1.0 * 6.102649688720703
Epoch 370, val loss: 0.9579430222511292
Epoch 380, training loss: 6.7691874504089355 = 0.6710924506187439 + 1.0 * 6.098094940185547
Epoch 380, val loss: 0.940396785736084
Epoch 390, training loss: 6.743724822998047 = 0.6457613706588745 + 1.0 * 6.097963333129883
Epoch 390, val loss: 0.9246399402618408
Epoch 400, training loss: 6.71427583694458 = 0.6220202445983887 + 1.0 * 6.092255592346191
Epoch 400, val loss: 0.9105314612388611
Epoch 410, training loss: 6.690303802490234 = 0.5992678999900818 + 1.0 * 6.091035842895508
Epoch 410, val loss: 0.8975995779037476
Epoch 420, training loss: 6.665067672729492 = 0.5772416591644287 + 1.0 * 6.087825775146484
Epoch 420, val loss: 0.8855183720588684
Epoch 430, training loss: 6.6493988037109375 = 0.5556656718254089 + 1.0 * 6.093733310699463
Epoch 430, val loss: 0.8741635680198669
Epoch 440, training loss: 6.619196891784668 = 0.5344152450561523 + 1.0 * 6.084781646728516
Epoch 440, val loss: 0.8633365631103516
Epoch 450, training loss: 6.594976902008057 = 0.513192355632782 + 1.0 * 6.081784725189209
Epoch 450, val loss: 0.8529700636863708
Epoch 460, training loss: 6.571107864379883 = 0.4917323589324951 + 1.0 * 6.079375267028809
Epoch 460, val loss: 0.8427126407623291
Epoch 470, training loss: 6.548466205596924 = 0.4700978100299835 + 1.0 * 6.078368186950684
Epoch 470, val loss: 0.8325249552726746
Epoch 480, training loss: 6.525640487670898 = 0.4485127031803131 + 1.0 * 6.077127933502197
Epoch 480, val loss: 0.8227685689926147
Epoch 490, training loss: 6.502042770385742 = 0.42679572105407715 + 1.0 * 6.075246810913086
Epoch 490, val loss: 0.813288688659668
Epoch 500, training loss: 6.488802909851074 = 0.40501904487609863 + 1.0 * 6.083784103393555
Epoch 500, val loss: 0.8040004968643188
Epoch 510, training loss: 6.456954002380371 = 0.3835761845111847 + 1.0 * 6.07337760925293
Epoch 510, val loss: 0.7953166961669922
Epoch 520, training loss: 6.43449068069458 = 0.3624417185783386 + 1.0 * 6.072049140930176
Epoch 520, val loss: 0.7872983813285828
Epoch 530, training loss: 6.4138360023498535 = 0.3417980968952179 + 1.0 * 6.072037696838379
Epoch 530, val loss: 0.7798787951469421
Epoch 540, training loss: 6.390475749969482 = 0.3218894302845001 + 1.0 * 6.068586349487305
Epoch 540, val loss: 0.7733364105224609
Epoch 550, training loss: 6.37406587600708 = 0.30274075269699097 + 1.0 * 6.071325302124023
Epoch 550, val loss: 0.7676726579666138
Epoch 560, training loss: 6.3492960929870605 = 0.28454655408859253 + 1.0 * 6.064749717712402
Epoch 560, val loss: 0.7629887461662292
Epoch 570, training loss: 6.330589294433594 = 0.26720982789993286 + 1.0 * 6.063379287719727
Epoch 570, val loss: 0.7592480778694153
Epoch 580, training loss: 6.318073272705078 = 0.25075435638427734 + 1.0 * 6.067318916320801
Epoch 580, val loss: 0.7563994526863098
Epoch 590, training loss: 6.299461364746094 = 0.23527386784553528 + 1.0 * 6.064187526702881
Epoch 590, val loss: 0.7543972134590149
Epoch 600, training loss: 6.28303337097168 = 0.22078527510166168 + 1.0 * 6.062248229980469
Epoch 600, val loss: 0.7532703876495361
Epoch 610, training loss: 6.264704704284668 = 0.20722061395645142 + 1.0 * 6.057484149932861
Epoch 610, val loss: 0.7529845833778381
Epoch 620, training loss: 6.252658367156982 = 0.1944737732410431 + 1.0 * 6.058184623718262
Epoch 620, val loss: 0.7534386515617371
Epoch 630, training loss: 6.242728233337402 = 0.182571679353714 + 1.0 * 6.060156345367432
Epoch 630, val loss: 0.7544991970062256
Epoch 640, training loss: 6.227555274963379 = 0.17155764997005463 + 1.0 * 6.055997848510742
Epoch 640, val loss: 0.7562710046768188
Epoch 650, training loss: 6.215258598327637 = 0.16128510236740112 + 1.0 * 6.05397367477417
Epoch 650, val loss: 0.7586455941200256
Epoch 660, training loss: 6.205896377563477 = 0.15165328979492188 + 1.0 * 6.054243087768555
Epoch 660, val loss: 0.7615278363227844
Epoch 670, training loss: 6.197005271911621 = 0.14271874725818634 + 1.0 * 6.054286479949951
Epoch 670, val loss: 0.7647684216499329
Epoch 680, training loss: 6.186172008514404 = 0.13444790244102478 + 1.0 * 6.051723957061768
Epoch 680, val loss: 0.7686138153076172
Epoch 690, training loss: 6.177164077758789 = 0.12675145268440247 + 1.0 * 6.050412654876709
Epoch 690, val loss: 0.7727906107902527
Epoch 700, training loss: 6.16945743560791 = 0.11957143247127533 + 1.0 * 6.049886226654053
Epoch 700, val loss: 0.7772729992866516
Epoch 710, training loss: 6.164077281951904 = 0.11289265751838684 + 1.0 * 6.05118465423584
Epoch 710, val loss: 0.7821173071861267
Epoch 720, training loss: 6.153656005859375 = 0.1066908985376358 + 1.0 * 6.0469651222229
Epoch 720, val loss: 0.7871780395507812
Epoch 730, training loss: 6.146235466003418 = 0.10091713815927505 + 1.0 * 6.045318126678467
Epoch 730, val loss: 0.7925751209259033
Epoch 740, training loss: 6.147154808044434 = 0.0955435186624527 + 1.0 * 6.051611423492432
Epoch 740, val loss: 0.7980721592903137
Epoch 750, training loss: 6.133919715881348 = 0.09054820239543915 + 1.0 * 6.043371677398682
Epoch 750, val loss: 0.8037647604942322
Epoch 760, training loss: 6.130522727966309 = 0.08589506149291992 + 1.0 * 6.044627666473389
Epoch 760, val loss: 0.8096249103546143
Epoch 770, training loss: 6.1240997314453125 = 0.08157519996166229 + 1.0 * 6.042524337768555
Epoch 770, val loss: 0.8156047463417053
Epoch 780, training loss: 6.120427131652832 = 0.0775427520275116 + 1.0 * 6.042884349822998
Epoch 780, val loss: 0.8216941356658936
Epoch 790, training loss: 6.115446090698242 = 0.07378362864255905 + 1.0 * 6.041662693023682
Epoch 790, val loss: 0.8278579115867615
Epoch 800, training loss: 6.112197399139404 = 0.0702752098441124 + 1.0 * 6.041922092437744
Epoch 800, val loss: 0.8340641260147095
Epoch 810, training loss: 6.1051716804504395 = 0.06700238585472107 + 1.0 * 6.0381693840026855
Epoch 810, val loss: 0.8403095006942749
Epoch 820, training loss: 6.101696968078613 = 0.0639410987496376 + 1.0 * 6.037755966186523
Epoch 820, val loss: 0.8465994596481323
Epoch 830, training loss: 6.100484371185303 = 0.06106149032711983 + 1.0 * 6.039422988891602
Epoch 830, val loss: 0.8529276251792908
Epoch 840, training loss: 6.094756603240967 = 0.05836085230112076 + 1.0 * 6.03639554977417
Epoch 840, val loss: 0.859199047088623
Epoch 850, training loss: 6.09925651550293 = 0.05583481118083 + 1.0 * 6.043421745300293
Epoch 850, val loss: 0.8654952645301819
Epoch 860, training loss: 6.089879035949707 = 0.053476233035326004 + 1.0 * 6.036402702331543
Epoch 860, val loss: 0.871873140335083
Epoch 870, training loss: 6.084716796875 = 0.05125182494521141 + 1.0 * 6.0334649085998535
Epoch 870, val loss: 0.8781501054763794
Epoch 880, training loss: 6.090091705322266 = 0.04915464296936989 + 1.0 * 6.0409369468688965
Epoch 880, val loss: 0.8845152854919434
Epoch 890, training loss: 6.082513332366943 = 0.04717814922332764 + 1.0 * 6.035335063934326
Epoch 890, val loss: 0.890689492225647
Epoch 900, training loss: 6.078512191772461 = 0.045324355363845825 + 1.0 * 6.0331878662109375
Epoch 900, val loss: 0.8969778418540955
Epoch 910, training loss: 6.074734687805176 = 0.043571893125772476 + 1.0 * 6.031162738800049
Epoch 910, val loss: 0.903154194355011
Epoch 920, training loss: 6.0720295906066895 = 0.04191816225647926 + 1.0 * 6.030111312866211
Epoch 920, val loss: 0.9093069434165955
Epoch 930, training loss: 6.074903964996338 = 0.040355127304792404 + 1.0 * 6.034548759460449
Epoch 930, val loss: 0.9154331088066101
Epoch 940, training loss: 6.0750203132629395 = 0.038885150104761124 + 1.0 * 6.036135196685791
Epoch 940, val loss: 0.9215103983879089
Epoch 950, training loss: 6.0668463706970215 = 0.037495583295822144 + 1.0 * 6.029350757598877
Epoch 950, val loss: 0.9275223612785339
Epoch 960, training loss: 6.062847137451172 = 0.03617759793996811 + 1.0 * 6.026669502258301
Epoch 960, val loss: 0.9335397481918335
Epoch 970, training loss: 6.063819885253906 = 0.03491885960102081 + 1.0 * 6.028901100158691
Epoch 970, val loss: 0.9395132064819336
Epoch 980, training loss: 6.0633320808410645 = 0.03372936323285103 + 1.0 * 6.029602527618408
Epoch 980, val loss: 0.9453932642936707
Epoch 990, training loss: 6.059143543243408 = 0.03260418772697449 + 1.0 * 6.026539325714111
Epoch 990, val loss: 0.9512007832527161
Epoch 1000, training loss: 6.056856632232666 = 0.031532760709524155 + 1.0 * 6.025323867797852
Epoch 1000, val loss: 0.9569531679153442
Epoch 1010, training loss: 6.061047554016113 = 0.030507318675518036 + 1.0 * 6.030540466308594
Epoch 1010, val loss: 0.9625958204269409
Epoch 1020, training loss: 6.059600830078125 = 0.029538877308368683 + 1.0 * 6.030061721801758
Epoch 1020, val loss: 0.96825110912323
Epoch 1030, training loss: 6.052498817443848 = 0.028621964156627655 + 1.0 * 6.023876667022705
Epoch 1030, val loss: 0.9737721681594849
Epoch 1040, training loss: 6.050289630889893 = 0.027740811929106712 + 1.0 * 6.022548675537109
Epoch 1040, val loss: 0.9792172908782959
Epoch 1050, training loss: 6.050861835479736 = 0.026896875351667404 + 1.0 * 6.023964881896973
Epoch 1050, val loss: 0.9846731424331665
Epoch 1060, training loss: 6.052006244659424 = 0.02609221450984478 + 1.0 * 6.025914192199707
Epoch 1060, val loss: 0.9900742173194885
Epoch 1070, training loss: 6.047572135925293 = 0.02532724104821682 + 1.0 * 6.022244930267334
Epoch 1070, val loss: 0.9953084588050842
Epoch 1080, training loss: 6.045897960662842 = 0.024602118879556656 + 1.0 * 6.02129602432251
Epoch 1080, val loss: 1.0006048679351807
Epoch 1090, training loss: 6.043993949890137 = 0.023902850225567818 + 1.0 * 6.0200910568237305
Epoch 1090, val loss: 1.005776286125183
Epoch 1100, training loss: 6.046433925628662 = 0.023229539394378662 + 1.0 * 6.023204326629639
Epoch 1100, val loss: 1.0110468864440918
Epoch 1110, training loss: 6.043036937713623 = 0.022585880011320114 + 1.0 * 6.020451068878174
Epoch 1110, val loss: 1.0160331726074219
Epoch 1120, training loss: 6.044044494628906 = 0.02197764627635479 + 1.0 * 6.022067070007324
Epoch 1120, val loss: 1.0211042165756226
Epoch 1130, training loss: 6.043705463409424 = 0.021390769630670547 + 1.0 * 6.022314548492432
Epoch 1130, val loss: 1.0260287523269653
Epoch 1140, training loss: 6.038869857788086 = 0.020824799314141273 + 1.0 * 6.018044948577881
Epoch 1140, val loss: 1.0308563709259033
Epoch 1150, training loss: 6.038634777069092 = 0.020281625911593437 + 1.0 * 6.01835298538208
Epoch 1150, val loss: 1.0357353687286377
Epoch 1160, training loss: 6.041720390319824 = 0.019758542999625206 + 1.0 * 6.021961688995361
Epoch 1160, val loss: 1.040604591369629
Epoch 1170, training loss: 6.035909175872803 = 0.01925799250602722 + 1.0 * 6.016651153564453
Epoch 1170, val loss: 1.045311689376831
Epoch 1180, training loss: 6.038140773773193 = 0.018777286633849144 + 1.0 * 6.0193634033203125
Epoch 1180, val loss: 1.0500012636184692
Epoch 1190, training loss: 6.035497665405273 = 0.018314730376005173 + 1.0 * 6.01718282699585
Epoch 1190, val loss: 1.0546257495880127
Epoch 1200, training loss: 6.033033847808838 = 0.01787020079791546 + 1.0 * 6.015163421630859
Epoch 1200, val loss: 1.0592646598815918
Epoch 1210, training loss: 6.034492492675781 = 0.017438536509871483 + 1.0 * 6.017054080963135
Epoch 1210, val loss: 1.0638163089752197
Epoch 1220, training loss: 6.035199165344238 = 0.017025483772158623 + 1.0 * 6.018173694610596
Epoch 1220, val loss: 1.0684083700180054
Epoch 1230, training loss: 6.032866477966309 = 0.016629936173558235 + 1.0 * 6.016236305236816
Epoch 1230, val loss: 1.0728490352630615
Epoch 1240, training loss: 6.031774520874023 = 0.016248632222414017 + 1.0 * 6.015525817871094
Epoch 1240, val loss: 1.077242374420166
Epoch 1250, training loss: 6.032716751098633 = 0.015879441052675247 + 1.0 * 6.016837120056152
Epoch 1250, val loss: 1.0816229581832886
Epoch 1260, training loss: 6.030507564544678 = 0.015523559413850307 + 1.0 * 6.014984130859375
Epoch 1260, val loss: 1.085919737815857
Epoch 1270, training loss: 6.0310211181640625 = 0.015180095098912716 + 1.0 * 6.015841007232666
Epoch 1270, val loss: 1.090205192565918
Epoch 1280, training loss: 6.027578353881836 = 0.014847795478999615 + 1.0 * 6.012730598449707
Epoch 1280, val loss: 1.0943564176559448
Epoch 1290, training loss: 6.026437759399414 = 0.014526686631143093 + 1.0 * 6.011910915374756
Epoch 1290, val loss: 1.098580002784729
Epoch 1300, training loss: 6.03724479675293 = 0.014214539900422096 + 1.0 * 6.0230302810668945
Epoch 1300, val loss: 1.1028358936309814
Epoch 1310, training loss: 6.030553817749023 = 0.013915179297327995 + 1.0 * 6.01663875579834
Epoch 1310, val loss: 1.106844425201416
Epoch 1320, training loss: 6.024677753448486 = 0.013627946376800537 + 1.0 * 6.011049747467041
Epoch 1320, val loss: 1.1108969449996948
Epoch 1330, training loss: 6.023332595825195 = 0.013347114436328411 + 1.0 * 6.009985446929932
Epoch 1330, val loss: 1.1148896217346191
Epoch 1340, training loss: 6.0273051261901855 = 0.01307260524481535 + 1.0 * 6.014232635498047
Epoch 1340, val loss: 1.1188679933547974
Epoch 1350, training loss: 6.029125690460205 = 0.012808511033654213 + 1.0 * 6.016317367553711
Epoch 1350, val loss: 1.1228878498077393
Epoch 1360, training loss: 6.026785850524902 = 0.012556921690702438 + 1.0 * 6.014228820800781
Epoch 1360, val loss: 1.1267942190170288
Epoch 1370, training loss: 6.022646427154541 = 0.012314031831920147 + 1.0 * 6.0103325843811035
Epoch 1370, val loss: 1.130594253540039
Epoch 1380, training loss: 6.020378589630127 = 0.012075923383235931 + 1.0 * 6.008302688598633
Epoch 1380, val loss: 1.1344144344329834
Epoch 1390, training loss: 6.026832103729248 = 0.01184276957064867 + 1.0 * 6.014989376068115
Epoch 1390, val loss: 1.1382719278335571
Epoch 1400, training loss: 6.020688533782959 = 0.011616343632340431 + 1.0 * 6.009072303771973
Epoch 1400, val loss: 1.141994595527649
Epoch 1410, training loss: 6.022587299346924 = 0.01139911450445652 + 1.0 * 6.01118803024292
Epoch 1410, val loss: 1.1458014249801636
Epoch 1420, training loss: 6.018869876861572 = 0.011187432333827019 + 1.0 * 6.0076823234558105
Epoch 1420, val loss: 1.1494972705841064
Epoch 1430, training loss: 6.018289566040039 = 0.010982324369251728 + 1.0 * 6.007307052612305
Epoch 1430, val loss: 1.1532403230667114
Epoch 1440, training loss: 6.0246901512146 = 0.010782227851450443 + 1.0 * 6.0139079093933105
Epoch 1440, val loss: 1.1569792032241821
Epoch 1450, training loss: 6.0197577476501465 = 0.010586405172944069 + 1.0 * 6.009171485900879
Epoch 1450, val loss: 1.1604912281036377
Epoch 1460, training loss: 6.0245890617370605 = 0.010400569066405296 + 1.0 * 6.014188289642334
Epoch 1460, val loss: 1.1640853881835938
Epoch 1470, training loss: 6.016870498657227 = 0.010219006799161434 + 1.0 * 6.006651401519775
Epoch 1470, val loss: 1.1676123142242432
Epoch 1480, training loss: 6.01489782333374 = 0.010041819885373116 + 1.0 * 6.004856109619141
Epoch 1480, val loss: 1.1711044311523438
Epoch 1490, training loss: 6.016625881195068 = 0.009866444393992424 + 1.0 * 6.0067596435546875
Epoch 1490, val loss: 1.174567461013794
Epoch 1500, training loss: 6.016517639160156 = 0.009696988388895988 + 1.0 * 6.0068206787109375
Epoch 1500, val loss: 1.1781331300735474
Epoch 1510, training loss: 6.015047073364258 = 0.009533789940178394 + 1.0 * 6.0055131912231445
Epoch 1510, val loss: 1.1815391778945923
Epoch 1520, training loss: 6.018272876739502 = 0.009374598041176796 + 1.0 * 6.0088982582092285
Epoch 1520, val loss: 1.1849387884140015
Epoch 1530, training loss: 6.013154983520508 = 0.009218945167958736 + 1.0 * 6.003935813903809
Epoch 1530, val loss: 1.1882433891296387
Epoch 1540, training loss: 6.012246131896973 = 0.009068625047802925 + 1.0 * 6.003177642822266
Epoch 1540, val loss: 1.1915802955627441
Epoch 1550, training loss: 6.014602184295654 = 0.008920788764953613 + 1.0 * 6.00568151473999
Epoch 1550, val loss: 1.1948922872543335
Epoch 1560, training loss: 6.014869213104248 = 0.008776125498116016 + 1.0 * 6.0060930252075195
Epoch 1560, val loss: 1.198175072669983
Epoch 1570, training loss: 6.0146613121032715 = 0.00863734632730484 + 1.0 * 6.00602388381958
Epoch 1570, val loss: 1.201451063156128
Epoch 1580, training loss: 6.018030166625977 = 0.008502067066729069 + 1.0 * 6.009528160095215
Epoch 1580, val loss: 1.204606056213379
Epoch 1590, training loss: 6.01162052154541 = 0.008369464427232742 + 1.0 * 6.003251075744629
Epoch 1590, val loss: 1.2077399492263794
Epoch 1600, training loss: 6.0109710693359375 = 0.008240205235779285 + 1.0 * 6.002730846405029
Epoch 1600, val loss: 1.2108649015426636
Epoch 1610, training loss: 6.014605522155762 = 0.008112817071378231 + 1.0 * 6.006492614746094
Epoch 1610, val loss: 1.2140491008758545
Epoch 1620, training loss: 6.010453224182129 = 0.007989458739757538 + 1.0 * 6.0024638175964355
Epoch 1620, val loss: 1.2172428369522095
Epoch 1630, training loss: 6.010514736175537 = 0.007868795655667782 + 1.0 * 6.002645969390869
Epoch 1630, val loss: 1.2203352451324463
Epoch 1640, training loss: 6.010513782501221 = 0.007750649005174637 + 1.0 * 6.002763271331787
Epoch 1640, val loss: 1.223429560661316
Epoch 1650, training loss: 6.009056568145752 = 0.007635402027517557 + 1.0 * 6.001420974731445
Epoch 1650, val loss: 1.2265549898147583
Epoch 1660, training loss: 6.013309478759766 = 0.0075226579792797565 + 1.0 * 6.005786895751953
Epoch 1660, val loss: 1.2296141386032104
Epoch 1670, training loss: 6.009496688842773 = 0.007412803824990988 + 1.0 * 6.002083778381348
Epoch 1670, val loss: 1.232566237449646
Epoch 1680, training loss: 6.010484218597412 = 0.007306178566068411 + 1.0 * 6.003178119659424
Epoch 1680, val loss: 1.235497236251831
Epoch 1690, training loss: 6.0087056159973145 = 0.007202716078609228 + 1.0 * 6.001502990722656
Epoch 1690, val loss: 1.2384634017944336
Epoch 1700, training loss: 6.007434844970703 = 0.00710256677120924 + 1.0 * 6.000332355499268
Epoch 1700, val loss: 1.241423487663269
Epoch 1710, training loss: 6.011047840118408 = 0.007002626080065966 + 1.0 * 6.004045009613037
Epoch 1710, val loss: 1.244257926940918
Epoch 1720, training loss: 6.006197929382324 = 0.00690549286082387 + 1.0 * 5.999292373657227
Epoch 1720, val loss: 1.24721097946167
Epoch 1730, training loss: 6.005947589874268 = 0.006810058373957872 + 1.0 * 5.9991374015808105
Epoch 1730, val loss: 1.2500202655792236
Epoch 1740, training loss: 6.0056633949279785 = 0.006715534254908562 + 1.0 * 5.998948097229004
Epoch 1740, val loss: 1.2528806924819946
Epoch 1750, training loss: 6.010866641998291 = 0.006622906308621168 + 1.0 * 6.004243850708008
Epoch 1750, val loss: 1.2557851076126099
Epoch 1760, training loss: 6.007413864135742 = 0.006534144282341003 + 1.0 * 6.000879764556885
Epoch 1760, val loss: 1.2586954832077026
Epoch 1770, training loss: 6.007166862487793 = 0.006446182727813721 + 1.0 * 6.000720500946045
Epoch 1770, val loss: 1.2613589763641357
Epoch 1780, training loss: 6.004115581512451 = 0.006361206062138081 + 1.0 * 5.9977545738220215
Epoch 1780, val loss: 1.2641576528549194
Epoch 1790, training loss: 6.006016254425049 = 0.0062766307964921 + 1.0 * 5.999739646911621
Epoch 1790, val loss: 1.2669001817703247
Epoch 1800, training loss: 6.013947486877441 = 0.00619465671479702 + 1.0 * 6.007752895355225
Epoch 1800, val loss: 1.2697644233703613
Epoch 1810, training loss: 6.006312370300293 = 0.006115603726357222 + 1.0 * 6.000196933746338
Epoch 1810, val loss: 1.2723323106765747
Epoch 1820, training loss: 6.003210067749023 = 0.00603810278698802 + 1.0 * 5.997171878814697
Epoch 1820, val loss: 1.2750011682510376
Epoch 1830, training loss: 6.005099296569824 = 0.005960446782410145 + 1.0 * 5.999138832092285
Epoch 1830, val loss: 1.2776654958724976
Epoch 1840, training loss: 6.004646301269531 = 0.0058845882304012775 + 1.0 * 5.9987616539001465
Epoch 1840, val loss: 1.2804147005081177
Epoch 1850, training loss: 6.0014448165893555 = 0.00581177556887269 + 1.0 * 5.995633125305176
Epoch 1850, val loss: 1.2830296754837036
Epoch 1860, training loss: 6.001408576965332 = 0.005740068852901459 + 1.0 * 5.995668411254883
Epoch 1860, val loss: 1.2855842113494873
Epoch 1870, training loss: 6.004456520080566 = 0.005668386816978455 + 1.0 * 5.998788356781006
Epoch 1870, val loss: 1.2882379293441772
Epoch 1880, training loss: 6.00262451171875 = 0.005597875919193029 + 1.0 * 5.997026443481445
Epoch 1880, val loss: 1.2908207178115845
Epoch 1890, training loss: 6.001298427581787 = 0.005529982969164848 + 1.0 * 5.9957685470581055
Epoch 1890, val loss: 1.2934428453445435
Epoch 1900, training loss: 5.999837398529053 = 0.005462750792503357 + 1.0 * 5.994374752044678
Epoch 1900, val loss: 1.2960323095321655
Epoch 1910, training loss: 6.007623672485352 = 0.005396257154643536 + 1.0 * 6.002227306365967
Epoch 1910, val loss: 1.2986620664596558
Epoch 1920, training loss: 6.000614643096924 = 0.005331342574208975 + 1.0 * 5.995283126831055
Epoch 1920, val loss: 1.301212191581726
Epoch 1930, training loss: 6.00080680847168 = 0.005268448498100042 + 1.0 * 5.995538234710693
Epoch 1930, val loss: 1.3036717176437378
Epoch 1940, training loss: 6.005439281463623 = 0.005206357222050428 + 1.0 * 6.000232696533203
Epoch 1940, val loss: 1.3061368465423584
Epoch 1950, training loss: 6.001901626586914 = 0.0051469155587255955 + 1.0 * 5.9967546463012695
Epoch 1950, val loss: 1.308628797531128
Epoch 1960, training loss: 6.001182556152344 = 0.005087265279144049 + 1.0 * 5.996095180511475
Epoch 1960, val loss: 1.3109766244888306
Epoch 1970, training loss: 6.000157356262207 = 0.005028729792684317 + 1.0 * 5.995128631591797
Epoch 1970, val loss: 1.3134769201278687
Epoch 1980, training loss: 5.999407768249512 = 0.004970593843609095 + 1.0 * 5.994437217712402
Epoch 1980, val loss: 1.3158965110778809
Epoch 1990, training loss: 6.00398063659668 = 0.004913903307169676 + 1.0 * 5.9990668296813965
Epoch 1990, val loss: 1.3183507919311523
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.6974
Flip ASR: 0.6400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.314422607421875 = 1.9406919479370117 + 1.0 * 8.373730659484863
Epoch 0, val loss: 1.9283668994903564
Epoch 10, training loss: 10.303138732910156 = 1.9302278757095337 + 1.0 * 8.372910499572754
Epoch 10, val loss: 1.9182058572769165
Epoch 20, training loss: 10.284810066223145 = 1.9172409772872925 + 1.0 * 8.367568969726562
Epoch 20, val loss: 1.9051917791366577
Epoch 30, training loss: 10.228524208068848 = 1.8996365070343018 + 1.0 * 8.328887939453125
Epoch 30, val loss: 1.8872231245040894
Epoch 40, training loss: 9.909622192382812 = 1.879464030265808 + 1.0 * 8.030158042907715
Epoch 40, val loss: 1.8675750494003296
Epoch 50, training loss: 9.112345695495605 = 1.858951449394226 + 1.0 * 7.25339412689209
Epoch 50, val loss: 1.8479514122009277
Epoch 60, training loss: 8.695219039916992 = 1.8435156345367432 + 1.0 * 6.851703643798828
Epoch 60, val loss: 1.8331362009048462
Epoch 70, training loss: 8.471983909606934 = 1.8305895328521729 + 1.0 * 6.64139461517334
Epoch 70, val loss: 1.8204319477081299
Epoch 80, training loss: 8.356520652770996 = 1.8180460929870605 + 1.0 * 6.5384745597839355
Epoch 80, val loss: 1.8084290027618408
Epoch 90, training loss: 8.278912544250488 = 1.8052915334701538 + 1.0 * 6.473620891571045
Epoch 90, val loss: 1.7965974807739258
Epoch 100, training loss: 8.206698417663574 = 1.7929350137710571 + 1.0 * 6.413763046264648
Epoch 100, val loss: 1.785653829574585
Epoch 110, training loss: 8.138097763061523 = 1.7820286750793457 + 1.0 * 6.356069087982178
Epoch 110, val loss: 1.7763475179672241
Epoch 120, training loss: 8.082966804504395 = 1.7717229127883911 + 1.0 * 6.311243534088135
Epoch 120, val loss: 1.767648458480835
Epoch 130, training loss: 8.038061141967773 = 1.7602373361587524 + 1.0 * 6.277823448181152
Epoch 130, val loss: 1.7579574584960938
Epoch 140, training loss: 7.997060298919678 = 1.7469226121902466 + 1.0 * 6.250137805938721
Epoch 140, val loss: 1.7467292547225952
Epoch 150, training loss: 7.958367824554443 = 1.7313097715377808 + 1.0 * 6.227057933807373
Epoch 150, val loss: 1.7337602376937866
Epoch 160, training loss: 7.922140598297119 = 1.7126363515853882 + 1.0 * 6.209504127502441
Epoch 160, val loss: 1.7183712720870972
Epoch 170, training loss: 7.882542133331299 = 1.690081000328064 + 1.0 * 6.192461013793945
Epoch 170, val loss: 1.7001858949661255
Epoch 180, training loss: 7.842248916625977 = 1.6625444889068604 + 1.0 * 6.179704189300537
Epoch 180, val loss: 1.6781617403030396
Epoch 190, training loss: 7.796911716461182 = 1.628616213798523 + 1.0 * 6.168295383453369
Epoch 190, val loss: 1.6512335538864136
Epoch 200, training loss: 7.746715545654297 = 1.5873013734817505 + 1.0 * 6.159414291381836
Epoch 200, val loss: 1.6185765266418457
Epoch 210, training loss: 7.6894211769104 = 1.5387078523635864 + 1.0 * 6.1507134437561035
Epoch 210, val loss: 1.5803700685501099
Epoch 220, training loss: 7.628522872924805 = 1.483236312866211 + 1.0 * 6.145286560058594
Epoch 220, val loss: 1.5370405912399292
Epoch 230, training loss: 7.561785697937012 = 1.422911286354065 + 1.0 * 6.138874530792236
Epoch 230, val loss: 1.49027419090271
Epoch 240, training loss: 7.493465423583984 = 1.360101580619812 + 1.0 * 6.133363723754883
Epoch 240, val loss: 1.4419358968734741
Epoch 250, training loss: 7.426229953765869 = 1.2962478399276733 + 1.0 * 6.129981994628906
Epoch 250, val loss: 1.3930983543395996
Epoch 260, training loss: 7.359878063201904 = 1.2338613271713257 + 1.0 * 6.126016616821289
Epoch 260, val loss: 1.3454718589782715
Epoch 270, training loss: 7.293352127075195 = 1.1728668212890625 + 1.0 * 6.120485305786133
Epoch 270, val loss: 1.2990238666534424
Epoch 280, training loss: 7.229105472564697 = 1.1127508878707886 + 1.0 * 6.116354465484619
Epoch 280, val loss: 1.2531201839447021
Epoch 290, training loss: 7.168881416320801 = 1.0542937517166138 + 1.0 * 6.114587783813477
Epoch 290, val loss: 1.2086549997329712
Epoch 300, training loss: 7.107019424438477 = 0.9978638887405396 + 1.0 * 6.109155654907227
Epoch 300, val loss: 1.1657006740570068
Epoch 310, training loss: 7.049347877502441 = 0.9435474872589111 + 1.0 * 6.105800151824951
Epoch 310, val loss: 1.124502182006836
Epoch 320, training loss: 6.995774745941162 = 0.8920345902442932 + 1.0 * 6.103740215301514
Epoch 320, val loss: 1.0857832431793213
Epoch 330, training loss: 6.9425764083862305 = 0.8434995412826538 + 1.0 * 6.099076747894287
Epoch 330, val loss: 1.0496970415115356
Epoch 340, training loss: 6.894527435302734 = 0.7979897260665894 + 1.0 * 6.0965375900268555
Epoch 340, val loss: 1.0163865089416504
Epoch 350, training loss: 6.850894451141357 = 0.7557096481323242 + 1.0 * 6.095184803009033
Epoch 350, val loss: 0.9860321283340454
Epoch 360, training loss: 6.809475421905518 = 0.7169553637504578 + 1.0 * 6.092520236968994
Epoch 360, val loss: 0.9588063359260559
Epoch 370, training loss: 6.768984317779541 = 0.6815702319145203 + 1.0 * 6.087414264678955
Epoch 370, val loss: 0.9347844123840332
Epoch 380, training loss: 6.733522415161133 = 0.6489648222923279 + 1.0 * 6.08455753326416
Epoch 380, val loss: 0.9133338332176208
Epoch 390, training loss: 6.705010890960693 = 0.6190630793571472 + 1.0 * 6.0859479904174805
Epoch 390, val loss: 0.8942769765853882
Epoch 400, training loss: 6.672489643096924 = 0.5918892025947571 + 1.0 * 6.080600261688232
Epoch 400, val loss: 0.8778690695762634
Epoch 410, training loss: 6.64395809173584 = 0.5667207837104797 + 1.0 * 6.077237129211426
Epoch 410, val loss: 0.863305389881134
Epoch 420, training loss: 6.624578475952148 = 0.5432116389274597 + 1.0 * 6.081367015838623
Epoch 420, val loss: 0.8503357768058777
Epoch 430, training loss: 6.594391822814941 = 0.5214686393737793 + 1.0 * 6.072923183441162
Epoch 430, val loss: 0.8390265703201294
Epoch 440, training loss: 6.571627140045166 = 0.5010595917701721 + 1.0 * 6.070567607879639
Epoch 440, val loss: 0.8291338086128235
Epoch 450, training loss: 6.553617477416992 = 0.48169007897377014 + 1.0 * 6.071927547454834
Epoch 450, val loss: 0.820326566696167
Epoch 460, training loss: 6.532581329345703 = 0.46339696645736694 + 1.0 * 6.069184303283691
Epoch 460, val loss: 0.8126739859580994
Epoch 470, training loss: 6.514283657073975 = 0.4460344910621643 + 1.0 * 6.068249225616455
Epoch 470, val loss: 0.8059948682785034
Epoch 480, training loss: 6.494398593902588 = 0.4294007122516632 + 1.0 * 6.064997673034668
Epoch 480, val loss: 0.8000823855400085
Epoch 490, training loss: 6.478052616119385 = 0.4133368134498596 + 1.0 * 6.06471586227417
Epoch 490, val loss: 0.7949439287185669
Epoch 500, training loss: 6.460636615753174 = 0.39779070019721985 + 1.0 * 6.062845706939697
Epoch 500, val loss: 0.7905042171478271
Epoch 510, training loss: 6.4471049308776855 = 0.3828350305557251 + 1.0 * 6.06427001953125
Epoch 510, val loss: 0.7866944670677185
Epoch 520, training loss: 6.428714275360107 = 0.36837342381477356 + 1.0 * 6.060340881347656
Epoch 520, val loss: 0.7834233045578003
Epoch 530, training loss: 6.410865306854248 = 0.3543812036514282 + 1.0 * 6.056484222412109
Epoch 530, val loss: 0.7806646227836609
Epoch 540, training loss: 6.397593975067139 = 0.3407808840274811 + 1.0 * 6.0568132400512695
Epoch 540, val loss: 0.7783558368682861
Epoch 550, training loss: 6.392056465148926 = 0.3277195394039154 + 1.0 * 6.064336776733398
Epoch 550, val loss: 0.7764227390289307
Epoch 560, training loss: 6.369302272796631 = 0.31520530581474304 + 1.0 * 6.0540971755981445
Epoch 560, val loss: 0.7748543620109558
Epoch 570, training loss: 6.35667085647583 = 0.30308422446250916 + 1.0 * 6.053586483001709
Epoch 570, val loss: 0.7736480832099915
Epoch 580, training loss: 6.342355728149414 = 0.29128727316856384 + 1.0 * 6.051068305969238
Epoch 580, val loss: 0.7727558016777039
Epoch 590, training loss: 6.3298163414001465 = 0.27981868386268616 + 1.0 * 6.049997806549072
Epoch 590, val loss: 0.7721046805381775
Epoch 600, training loss: 6.340165615081787 = 0.2686801254749298 + 1.0 * 6.07148551940918
Epoch 600, val loss: 0.7717018127441406
Epoch 610, training loss: 6.313185691833496 = 0.25806185603141785 + 1.0 * 6.055123805999756
Epoch 610, val loss: 0.7715616822242737
Epoch 620, training loss: 6.295156478881836 = 0.24781811237335205 + 1.0 * 6.047338485717773
Epoch 620, val loss: 0.7715792655944824
Epoch 630, training loss: 6.284492015838623 = 0.23788821697235107 + 1.0 * 6.046603679656982
Epoch 630, val loss: 0.7718533277511597
Epoch 640, training loss: 6.274087429046631 = 0.22823967039585114 + 1.0 * 6.0458478927612305
Epoch 640, val loss: 0.7722902297973633
Epoch 650, training loss: 6.273800849914551 = 0.2189089059829712 + 1.0 * 6.054892063140869
Epoch 650, val loss: 0.7728748917579651
Epoch 660, training loss: 6.255659103393555 = 0.20994529128074646 + 1.0 * 6.045713901519775
Epoch 660, val loss: 0.7736167907714844
Epoch 670, training loss: 6.244884490966797 = 0.20132000744342804 + 1.0 * 6.043564319610596
Epoch 670, val loss: 0.7745132446289062
Epoch 680, training loss: 6.235688209533691 = 0.19299724698066711 + 1.0 * 6.042690753936768
Epoch 680, val loss: 0.7756398916244507
Epoch 690, training loss: 6.230656147003174 = 0.1849737912416458 + 1.0 * 6.045682430267334
Epoch 690, val loss: 0.7769582271575928
Epoch 700, training loss: 6.223614692687988 = 0.17724919319152832 + 1.0 * 6.046365261077881
Epoch 700, val loss: 0.7784343957901001
Epoch 710, training loss: 6.210251808166504 = 0.1698104590177536 + 1.0 * 6.040441513061523
Epoch 710, val loss: 0.7800521850585938
Epoch 720, training loss: 6.202117443084717 = 0.16262267529964447 + 1.0 * 6.03949499130249
Epoch 720, val loss: 0.7818255424499512
Epoch 730, training loss: 6.194697380065918 = 0.155690997838974 + 1.0 * 6.039006233215332
Epoch 730, val loss: 0.783832848072052
Epoch 740, training loss: 6.194057941436768 = 0.1489885002374649 + 1.0 * 6.045069217681885
Epoch 740, val loss: 0.7859567999839783
Epoch 750, training loss: 6.181630611419678 = 0.1424890011548996 + 1.0 * 6.039141654968262
Epoch 750, val loss: 0.7880933284759521
Epoch 760, training loss: 6.172379493713379 = 0.13615520298480988 + 1.0 * 6.036224365234375
Epoch 760, val loss: 0.7903544902801514
Epoch 770, training loss: 6.16580057144165 = 0.12998805940151215 + 1.0 * 6.0358123779296875
Epoch 770, val loss: 0.792779266834259
Epoch 780, training loss: 6.177196025848389 = 0.12391817569732666 + 1.0 * 6.053277969360352
Epoch 780, val loss: 0.7952826023101807
Epoch 790, training loss: 6.152331352233887 = 0.11798840761184692 + 1.0 * 6.0343427658081055
Epoch 790, val loss: 0.7976130843162537
Epoch 800, training loss: 6.146957874298096 = 0.11220277845859528 + 1.0 * 6.034755229949951
Epoch 800, val loss: 0.8000267744064331
Epoch 810, training loss: 6.139292240142822 = 0.1064450591802597 + 1.0 * 6.0328474044799805
Epoch 810, val loss: 0.8027008175849915
Epoch 820, training loss: 6.133882522583008 = 0.10075511783361435 + 1.0 * 6.033127307891846
Epoch 820, val loss: 0.8053955435752869
Epoch 830, training loss: 6.128528594970703 = 0.09513172507286072 + 1.0 * 6.0333967208862305
Epoch 830, val loss: 0.8080934286117554
Epoch 840, training loss: 6.122176170349121 = 0.08975103497505188 + 1.0 * 6.0324249267578125
Epoch 840, val loss: 0.8108415007591248
Epoch 850, training loss: 6.116340160369873 = 0.08480250090360641 + 1.0 * 6.0315375328063965
Epoch 850, val loss: 0.8136967420578003
Epoch 860, training loss: 6.112074851989746 = 0.08034805208444595 + 1.0 * 6.031726837158203
Epoch 860, val loss: 0.8168932795524597
Epoch 870, training loss: 6.1085991859436035 = 0.07629738748073578 + 1.0 * 6.032301902770996
Epoch 870, val loss: 0.820581316947937
Epoch 880, training loss: 6.102720260620117 = 0.0725923404097557 + 1.0 * 6.030128002166748
Epoch 880, val loss: 0.8246518969535828
Epoch 890, training loss: 6.0998945236206055 = 0.06918604671955109 + 1.0 * 6.030708312988281
Epoch 890, val loss: 0.8288417458534241
Epoch 900, training loss: 6.096618175506592 = 0.06602884829044342 + 1.0 * 6.0305891036987305
Epoch 900, val loss: 0.8331120610237122
Epoch 910, training loss: 6.0895609855651855 = 0.06308005005121231 + 1.0 * 6.0264811515808105
Epoch 910, val loss: 0.8374384641647339
Epoch 920, training loss: 6.0869550704956055 = 0.060307618230581284 + 1.0 * 6.026647567749023
Epoch 920, val loss: 0.841779351234436
Epoch 930, training loss: 6.089443683624268 = 0.057698749005794525 + 1.0 * 6.031744956970215
Epoch 930, val loss: 0.8461798429489136
Epoch 940, training loss: 6.083499908447266 = 0.055240530520677567 + 1.0 * 6.02825927734375
Epoch 940, val loss: 0.8505408763885498
Epoch 950, training loss: 6.085186004638672 = 0.05293310806155205 + 1.0 * 6.032252788543701
Epoch 950, val loss: 0.8549307584762573
Epoch 960, training loss: 6.075681686401367 = 0.050749581307172775 + 1.0 * 6.024931907653809
Epoch 960, val loss: 0.8592147827148438
Epoch 970, training loss: 6.071812629699707 = 0.048657145351171494 + 1.0 * 6.023155689239502
Epoch 970, val loss: 0.8635411858558655
Epoch 980, training loss: 6.0704426765441895 = 0.04666405916213989 + 1.0 * 6.023778438568115
Epoch 980, val loss: 0.8678828477859497
Epoch 990, training loss: 6.067346096038818 = 0.04476669058203697 + 1.0 * 6.022579193115234
Epoch 990, val loss: 0.8720692992210388
Epoch 1000, training loss: 6.064330101013184 = 0.042960237711668015 + 1.0 * 6.021369934082031
Epoch 1000, val loss: 0.8762551546096802
Epoch 1010, training loss: 6.069147109985352 = 0.04124639928340912 + 1.0 * 6.027900695800781
Epoch 1010, val loss: 0.880499005317688
Epoch 1020, training loss: 6.062950134277344 = 0.03961551934480667 + 1.0 * 6.023334503173828
Epoch 1020, val loss: 0.8845915794372559
Epoch 1030, training loss: 6.058010578155518 = 0.038071177899837494 + 1.0 * 6.019939422607422
Epoch 1030, val loss: 0.8886370658874512
Epoch 1040, training loss: 6.056716442108154 = 0.036594491451978683 + 1.0 * 6.020122051239014
Epoch 1040, val loss: 0.8927026391029358
Epoch 1050, training loss: 6.064997673034668 = 0.035177040845155716 + 1.0 * 6.029820442199707
Epoch 1050, val loss: 0.8967649340629578
Epoch 1060, training loss: 6.055294990539551 = 0.033826105296611786 + 1.0 * 6.0214691162109375
Epoch 1060, val loss: 0.9006301760673523
Epoch 1070, training loss: 6.051074028015137 = 0.032555099576711655 + 1.0 * 6.018518924713135
Epoch 1070, val loss: 0.9043387174606323
Epoch 1080, training loss: 6.048280239105225 = 0.03134220093488693 + 1.0 * 6.016938209533691
Epoch 1080, val loss: 0.9080953598022461
Epoch 1090, training loss: 6.046360015869141 = 0.030180148780345917 + 1.0 * 6.016180038452148
Epoch 1090, val loss: 0.9119054675102234
Epoch 1100, training loss: 6.045536518096924 = 0.029060350731015205 + 1.0 * 6.016476154327393
Epoch 1100, val loss: 0.9157063364982605
Epoch 1110, training loss: 6.049342155456543 = 0.027994150295853615 + 1.0 * 6.021347999572754
Epoch 1110, val loss: 0.9194802641868591
Epoch 1120, training loss: 6.043201446533203 = 0.026991626247763634 + 1.0 * 6.016209602355957
Epoch 1120, val loss: 0.9231184124946594
Epoch 1130, training loss: 6.041805267333984 = 0.026041699573397636 + 1.0 * 6.015763759613037
Epoch 1130, val loss: 0.9267438650131226
Epoch 1140, training loss: 6.041396141052246 = 0.025138717144727707 + 1.0 * 6.016257286071777
Epoch 1140, val loss: 0.9303882718086243
Epoch 1150, training loss: 6.042133331298828 = 0.02428371272981167 + 1.0 * 6.017849445343018
Epoch 1150, val loss: 0.9339653253555298
Epoch 1160, training loss: 6.037805080413818 = 0.02347560040652752 + 1.0 * 6.014329433441162
Epoch 1160, val loss: 0.9374834895133972
Epoch 1170, training loss: 6.035996437072754 = 0.022712618112564087 + 1.0 * 6.013283729553223
Epoch 1170, val loss: 0.940956175327301
Epoch 1180, training loss: 6.03699254989624 = 0.021985867992043495 + 1.0 * 6.0150065422058105
Epoch 1180, val loss: 0.9444781541824341
Epoch 1190, training loss: 6.036165237426758 = 0.02129114232957363 + 1.0 * 6.01487398147583
Epoch 1190, val loss: 0.9479258060455322
Epoch 1200, training loss: 6.032979488372803 = 0.020633241161704063 + 1.0 * 6.012346267700195
Epoch 1200, val loss: 0.951293408870697
Epoch 1210, training loss: 6.034115314483643 = 0.020008858293294907 + 1.0 * 6.014106273651123
Epoch 1210, val loss: 0.9547500014305115
Epoch 1220, training loss: 6.031872272491455 = 0.019413968548178673 + 1.0 * 6.012458324432373
Epoch 1220, val loss: 0.9580971002578735
Epoch 1230, training loss: 6.029993534088135 = 0.018849201500415802 + 1.0 * 6.011144161224365
Epoch 1230, val loss: 0.9614643454551697
Epoch 1240, training loss: 6.028648853302002 = 0.018307145684957504 + 1.0 * 6.010341644287109
Epoch 1240, val loss: 0.9648324251174927
Epoch 1250, training loss: 6.035026550292969 = 0.01778758317232132 + 1.0 * 6.017239093780518
Epoch 1250, val loss: 0.9681883454322815
Epoch 1260, training loss: 6.028141021728516 = 0.017296230420470238 + 1.0 * 6.010844707489014
Epoch 1260, val loss: 0.971358597278595
Epoch 1270, training loss: 6.0284929275512695 = 0.01682700775563717 + 1.0 * 6.0116658210754395
Epoch 1270, val loss: 0.9745966196060181
Epoch 1280, training loss: 6.026261806488037 = 0.016378482803702354 + 1.0 * 6.009883403778076
Epoch 1280, val loss: 0.9778949022293091
Epoch 1290, training loss: 6.02509880065918 = 0.0159507654607296 + 1.0 * 6.009148120880127
Epoch 1290, val loss: 0.9810222387313843
Epoch 1300, training loss: 6.025284767150879 = 0.015540260821580887 + 1.0 * 6.009744644165039
Epoch 1300, val loss: 0.98423832654953
Epoch 1310, training loss: 6.0261006355285645 = 0.015143953263759613 + 1.0 * 6.010956764221191
Epoch 1310, val loss: 0.9874221086502075
Epoch 1320, training loss: 6.0226640701293945 = 0.014764254912734032 + 1.0 * 6.007899761199951
Epoch 1320, val loss: 0.9904925227165222
Epoch 1330, training loss: 6.023133277893066 = 0.01440077181905508 + 1.0 * 6.008732318878174
Epoch 1330, val loss: 0.9936452507972717
Epoch 1340, training loss: 6.022857189178467 = 0.01405060850083828 + 1.0 * 6.0088067054748535
Epoch 1340, val loss: 0.9967459440231323
Epoch 1350, training loss: 6.019692420959473 = 0.013716635294258595 + 1.0 * 6.005975723266602
Epoch 1350, val loss: 0.9997414946556091
Epoch 1360, training loss: 6.019989490509033 = 0.013394029811024666 + 1.0 * 6.006595611572266
Epoch 1360, val loss: 1.0027779340744019
Epoch 1370, training loss: 6.0212931632995605 = 0.013083277270197868 + 1.0 * 6.008209705352783
Epoch 1370, val loss: 1.0058417320251465
Epoch 1380, training loss: 6.024012565612793 = 0.012786018662154675 + 1.0 * 6.011226654052734
Epoch 1380, val loss: 1.008771538734436
Epoch 1390, training loss: 6.017739295959473 = 0.012500414624810219 + 1.0 * 6.005239009857178
Epoch 1390, val loss: 1.0116748809814453
Epoch 1400, training loss: 6.016448974609375 = 0.012225129641592503 + 1.0 * 6.004223823547363
Epoch 1400, val loss: 1.0146119594573975
Epoch 1410, training loss: 6.0170207023620605 = 0.01195773109793663 + 1.0 * 6.005063056945801
Epoch 1410, val loss: 1.0175689458847046
Epoch 1420, training loss: 6.019369125366211 = 0.01169969979673624 + 1.0 * 6.007669448852539
Epoch 1420, val loss: 1.020501971244812
Epoch 1430, training loss: 6.018759250640869 = 0.011451694183051586 + 1.0 * 6.007307529449463
Epoch 1430, val loss: 1.0233194828033447
Epoch 1440, training loss: 6.015190124511719 = 0.011212975718080997 + 1.0 * 6.003977298736572
Epoch 1440, val loss: 1.0261238813400269
Epoch 1450, training loss: 6.014493942260742 = 0.010982460342347622 + 1.0 * 6.003511428833008
Epoch 1450, val loss: 1.0289063453674316
Epoch 1460, training loss: 6.02307653427124 = 0.010759735479950905 + 1.0 * 6.012316703796387
Epoch 1460, val loss: 1.031753659248352
Epoch 1470, training loss: 6.014883995056152 = 0.010544517077505589 + 1.0 * 6.004339694976807
Epoch 1470, val loss: 1.0344889163970947
Epoch 1480, training loss: 6.012542247772217 = 0.010337739251554012 + 1.0 * 6.002204418182373
Epoch 1480, val loss: 1.0371464490890503
Epoch 1490, training loss: 6.0115861892700195 = 0.010134917683899403 + 1.0 * 6.00145149230957
Epoch 1490, val loss: 1.0400290489196777
Epoch 1500, training loss: 6.018123626708984 = 0.009937846101820469 + 1.0 * 6.008185863494873
Epoch 1500, val loss: 1.0427716970443726
Epoch 1510, training loss: 6.013455390930176 = 0.0097478237003088 + 1.0 * 6.003707408905029
Epoch 1510, val loss: 1.0454175472259521
Epoch 1520, training loss: 6.014324188232422 = 0.009565209969878197 + 1.0 * 6.004758834838867
Epoch 1520, val loss: 1.0479512214660645
Epoch 1530, training loss: 6.010457992553711 = 0.009387296624481678 + 1.0 * 6.001070499420166
Epoch 1530, val loss: 1.050622582435608
Epoch 1540, training loss: 6.010012149810791 = 0.009214312769472599 + 1.0 * 6.000797748565674
Epoch 1540, val loss: 1.053284764289856
Epoch 1550, training loss: 6.012481689453125 = 0.009045951999723911 + 1.0 * 6.0034356117248535
Epoch 1550, val loss: 1.0559202432632446
Epoch 1560, training loss: 6.009596824645996 = 0.008882974274456501 + 1.0 * 6.00071382522583
Epoch 1560, val loss: 1.0584897994995117
Epoch 1570, training loss: 6.008260250091553 = 0.0087254224345088 + 1.0 * 5.999534606933594
Epoch 1570, val loss: 1.0610111951828003
Epoch 1580, training loss: 6.012624740600586 = 0.008571954444050789 + 1.0 * 6.004052639007568
Epoch 1580, val loss: 1.063636302947998
Epoch 1590, training loss: 6.009267330169678 = 0.008423211053013802 + 1.0 * 6.0008440017700195
Epoch 1590, val loss: 1.0661287307739258
Epoch 1600, training loss: 6.008403778076172 = 0.008279961533844471 + 1.0 * 6.000123977661133
Epoch 1600, val loss: 1.0685322284698486
Epoch 1610, training loss: 6.007352352142334 = 0.008140361867845058 + 1.0 * 5.99921178817749
Epoch 1610, val loss: 1.071048378944397
Epoch 1620, training loss: 6.01097297668457 = 0.008004162460565567 + 1.0 * 6.002968788146973
Epoch 1620, val loss: 1.0735399723052979
Epoch 1630, training loss: 6.005657196044922 = 0.007872490212321281 + 1.0 * 5.997784614562988
Epoch 1630, val loss: 1.0758999586105347
Epoch 1640, training loss: 6.0052809715271 = 0.007744663394987583 + 1.0 * 5.9975361824035645
Epoch 1640, val loss: 1.0782771110534668
Epoch 1650, training loss: 6.004411697387695 = 0.007618715520948172 + 1.0 * 5.996792793273926
Epoch 1650, val loss: 1.0807342529296875
Epoch 1660, training loss: 6.009387969970703 = 0.0074957795441150665 + 1.0 * 6.00189208984375
Epoch 1660, val loss: 1.0830986499786377
Epoch 1670, training loss: 6.011163234710693 = 0.007377276197075844 + 1.0 * 6.003786087036133
Epoch 1670, val loss: 1.0855060815811157
Epoch 1680, training loss: 6.007019996643066 = 0.007263436447829008 + 1.0 * 5.999756336212158
Epoch 1680, val loss: 1.087615728378296
Epoch 1690, training loss: 6.002825736999512 = 0.007153191138058901 + 1.0 * 5.995672702789307
Epoch 1690, val loss: 1.0899009704589844
Epoch 1700, training loss: 6.002734661102295 = 0.007043834775686264 + 1.0 * 5.995690822601318
Epoch 1700, val loss: 1.0923124551773071
Epoch 1710, training loss: 6.00532341003418 = 0.006936303805559874 + 1.0 * 5.998387336730957
Epoch 1710, val loss: 1.0946390628814697
Epoch 1720, training loss: 6.004048824310303 = 0.00683187460526824 + 1.0 * 5.997217178344727
Epoch 1720, val loss: 1.096867561340332
Epoch 1730, training loss: 6.008129119873047 = 0.006730691995471716 + 1.0 * 6.00139856338501
Epoch 1730, val loss: 1.0990320444107056
Epoch 1740, training loss: 6.002666473388672 = 0.006632823031395674 + 1.0 * 5.996033668518066
Epoch 1740, val loss: 1.1011439561843872
Epoch 1750, training loss: 6.002706050872803 = 0.006536796223372221 + 1.0 * 5.996169090270996
Epoch 1750, val loss: 1.1033676862716675
Epoch 1760, training loss: 6.00469970703125 = 0.006442198064178228 + 1.0 * 5.998257637023926
Epoch 1760, val loss: 1.1056045293807983
Epoch 1770, training loss: 6.000463962554932 = 0.006350406911224127 + 1.0 * 5.994113445281982
Epoch 1770, val loss: 1.1076836585998535
Epoch 1780, training loss: 6.003340244293213 = 0.006260550115257502 + 1.0 * 5.997079849243164
Epoch 1780, val loss: 1.1098062992095947
Epoch 1790, training loss: 6.0036444664001465 = 0.006172104738652706 + 1.0 * 5.997472286224365
Epoch 1790, val loss: 1.112023115158081
Epoch 1800, training loss: 5.999133110046387 = 0.0060867853462696075 + 1.0 * 5.993046283721924
Epoch 1800, val loss: 1.1140782833099365
Epoch 1810, training loss: 6.000425338745117 = 0.0060027288272976875 + 1.0 * 5.994422435760498
Epoch 1810, val loss: 1.1161986589431763
Epoch 1820, training loss: 6.004902362823486 = 0.005920020863413811 + 1.0 * 5.9989824295043945
Epoch 1820, val loss: 1.1183679103851318
Epoch 1830, training loss: 6.003233432769775 = 0.00584007753059268 + 1.0 * 5.9973931312561035
Epoch 1830, val loss: 1.1203622817993164
Epoch 1840, training loss: 6.000849723815918 = 0.005763642955571413 + 1.0 * 5.995086193084717
Epoch 1840, val loss: 1.1222697496414185
Epoch 1850, training loss: 5.999235153198242 = 0.0056878793984651566 + 1.0 * 5.993547439575195
Epoch 1850, val loss: 1.1243609189987183
Epoch 1860, training loss: 6.0028157234191895 = 0.005612896755337715 + 1.0 * 5.9972028732299805
Epoch 1860, val loss: 1.1265156269073486
Epoch 1870, training loss: 5.997513771057129 = 0.005540018901228905 + 1.0 * 5.991973876953125
Epoch 1870, val loss: 1.1283994913101196
Epoch 1880, training loss: 5.9968037605285645 = 0.005468526389449835 + 1.0 * 5.991335391998291
Epoch 1880, val loss: 1.1304033994674683
Epoch 1890, training loss: 6.000749111175537 = 0.0053976597264409065 + 1.0 * 5.995351314544678
Epoch 1890, val loss: 1.132485032081604
Epoch 1900, training loss: 5.9995317459106445 = 0.0053291707299649715 + 1.0 * 5.994202613830566
Epoch 1900, val loss: 1.1344877481460571
Epoch 1910, training loss: 5.9986162185668945 = 0.005263368133455515 + 1.0 * 5.993352890014648
Epoch 1910, val loss: 1.1362131834030151
Epoch 1920, training loss: 5.996337413787842 = 0.005199078470468521 + 1.0 * 5.991138458251953
Epoch 1920, val loss: 1.138142704963684
Epoch 1930, training loss: 5.9955153465271 = 0.005134257022291422 + 1.0 * 5.990381240844727
Epoch 1930, val loss: 1.1401705741882324
Epoch 1940, training loss: 6.004517555236816 = 0.0050709364004433155 + 1.0 * 5.999446392059326
Epoch 1940, val loss: 1.1421105861663818
Epoch 1950, training loss: 5.998767375946045 = 0.005009135231375694 + 1.0 * 5.993758201599121
Epoch 1950, val loss: 1.1439398527145386
Epoch 1960, training loss: 5.997523784637451 = 0.0049500116147100925 + 1.0 * 5.9925737380981445
Epoch 1960, val loss: 1.1456571817398071
Epoch 1970, training loss: 6.001489639282227 = 0.004891291260719299 + 1.0 * 5.996598243713379
Epoch 1970, val loss: 1.1475915908813477
Epoch 1980, training loss: 5.995666027069092 = 0.004833884071558714 + 1.0 * 5.990832328796387
Epoch 1980, val loss: 1.1494052410125732
Epoch 1990, training loss: 5.994449615478516 = 0.00477801775559783 + 1.0 * 5.98967170715332
Epoch 1990, val loss: 1.151213526725769
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5646
Flip ASR: 0.5067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.318957328796387 = 1.945164680480957 + 1.0 * 8.37379264831543
Epoch 0, val loss: 1.9539986848831177
Epoch 10, training loss: 10.309099197387695 = 1.9358062744140625 + 1.0 * 8.373292922973633
Epoch 10, val loss: 1.9446989297866821
Epoch 20, training loss: 10.294559478759766 = 1.924275517463684 + 1.0 * 8.370284080505371
Epoch 20, val loss: 1.9326239824295044
Epoch 30, training loss: 10.258559226989746 = 1.9081686735153198 + 1.0 * 8.350390434265137
Epoch 30, val loss: 1.9150723218917847
Epoch 40, training loss: 10.10483169555664 = 1.8871493339538574 + 1.0 * 8.217681884765625
Epoch 40, val loss: 1.8926295042037964
Epoch 50, training loss: 9.321403503417969 = 1.8649256229400635 + 1.0 * 7.456477642059326
Epoch 50, val loss: 1.8694838285446167
Epoch 60, training loss: 8.842253684997559 = 1.8463493585586548 + 1.0 * 6.995904445648193
Epoch 60, val loss: 1.85163152217865
Epoch 70, training loss: 8.53495979309082 = 1.8326748609542847 + 1.0 * 6.702284812927246
Epoch 70, val loss: 1.8378382921218872
Epoch 80, training loss: 8.380330085754395 = 1.8185938596725464 + 1.0 * 6.561736583709717
Epoch 80, val loss: 1.8234503269195557
Epoch 90, training loss: 8.29637622833252 = 1.8051317930221558 + 1.0 * 6.491244316101074
Epoch 90, val loss: 1.8096191883087158
Epoch 100, training loss: 8.22464656829834 = 1.7915335893630981 + 1.0 * 6.433112621307373
Epoch 100, val loss: 1.795896053314209
Epoch 110, training loss: 8.161153793334961 = 1.7788947820663452 + 1.0 * 6.382258892059326
Epoch 110, val loss: 1.7832744121551514
Epoch 120, training loss: 8.10952091217041 = 1.7661501169204712 + 1.0 * 6.3433709144592285
Epoch 120, val loss: 1.7708766460418701
Epoch 130, training loss: 8.06129264831543 = 1.7523220777511597 + 1.0 * 6.3089704513549805
Epoch 130, val loss: 1.7578901052474976
Epoch 140, training loss: 8.013397216796875 = 1.7367734909057617 + 1.0 * 6.276623725891113
Epoch 140, val loss: 1.7437832355499268
Epoch 150, training loss: 7.970302581787109 = 1.7186279296875 + 1.0 * 6.251674652099609
Epoch 150, val loss: 1.7278116941452026
Epoch 160, training loss: 7.930095672607422 = 1.69663405418396 + 1.0 * 6.233461856842041
Epoch 160, val loss: 1.7091155052185059
Epoch 170, training loss: 7.888829231262207 = 1.6699495315551758 + 1.0 * 6.218879699707031
Epoch 170, val loss: 1.6868830919265747
Epoch 180, training loss: 7.8435468673706055 = 1.6374812126159668 + 1.0 * 6.206065654754639
Epoch 180, val loss: 1.6601672172546387
Epoch 190, training loss: 7.794012546539307 = 1.5978797674179077 + 1.0 * 6.196132659912109
Epoch 190, val loss: 1.6278070211410522
Epoch 200, training loss: 7.736526012420654 = 1.5502285957336426 + 1.0 * 6.186297416687012
Epoch 200, val loss: 1.5887244939804077
Epoch 210, training loss: 7.671566009521484 = 1.493054986000061 + 1.0 * 6.178511142730713
Epoch 210, val loss: 1.5417143106460571
Epoch 220, training loss: 7.601812839508057 = 1.4262803792953491 + 1.0 * 6.175532341003418
Epoch 220, val loss: 1.486696720123291
Epoch 230, training loss: 7.520242691040039 = 1.3531837463378906 + 1.0 * 6.167058944702148
Epoch 230, val loss: 1.4261879920959473
Epoch 240, training loss: 7.436710834503174 = 1.2747937440872192 + 1.0 * 6.161917209625244
Epoch 240, val loss: 1.3611416816711426
Epoch 250, training loss: 7.353071212768555 = 1.193578839302063 + 1.0 * 6.159492492675781
Epoch 250, val loss: 1.2936599254608154
Epoch 260, training loss: 7.270601272583008 = 1.1145838499069214 + 1.0 * 6.156017303466797
Epoch 260, val loss: 1.2285197973251343
Epoch 270, training loss: 7.189567565917969 = 1.0397346019744873 + 1.0 * 6.149832725524902
Epoch 270, val loss: 1.167052149772644
Epoch 280, training loss: 7.122713088989258 = 0.9691123962402344 + 1.0 * 6.153600692749023
Epoch 280, val loss: 1.109470009803772
Epoch 290, training loss: 7.047986030578613 = 0.9050067067146301 + 1.0 * 6.142979145050049
Epoch 290, val loss: 1.0578900575637817
Epoch 300, training loss: 6.983842372894287 = 0.8461422920227051 + 1.0 * 6.137700080871582
Epoch 300, val loss: 1.0110528469085693
Epoch 310, training loss: 6.924910068511963 = 0.7915380597114563 + 1.0 * 6.133371829986572
Epoch 310, val loss: 0.9679111242294312
Epoch 320, training loss: 6.874425411224365 = 0.7414335012435913 + 1.0 * 6.132991790771484
Epoch 320, val loss: 0.9291590452194214
Epoch 330, training loss: 6.8220367431640625 = 0.6960920691490173 + 1.0 * 6.1259446144104
Epoch 330, val loss: 0.8945000171661377
Epoch 340, training loss: 6.776315689086914 = 0.6541496515274048 + 1.0 * 6.122166156768799
Epoch 340, val loss: 0.8630731105804443
Epoch 350, training loss: 6.735424041748047 = 0.6150001287460327 + 1.0 * 6.120423793792725
Epoch 350, val loss: 0.8343243598937988
Epoch 360, training loss: 6.693778991699219 = 0.5788317918777466 + 1.0 * 6.114947319030762
Epoch 360, val loss: 0.8084348440170288
Epoch 370, training loss: 6.66102409362793 = 0.5453145503997803 + 1.0 * 6.11570930480957
Epoch 370, val loss: 0.7851115465164185
Epoch 380, training loss: 6.623636245727539 = 0.5138360261917114 + 1.0 * 6.109800338745117
Epoch 380, val loss: 0.7638061046600342
Epoch 390, training loss: 6.589563369750977 = 0.4839562475681305 + 1.0 * 6.105607032775879
Epoch 390, val loss: 0.7441335916519165
Epoch 400, training loss: 6.557901859283447 = 0.4552345275878906 + 1.0 * 6.102667331695557
Epoch 400, val loss: 0.7257419228553772
Epoch 410, training loss: 6.532336235046387 = 0.42749810218811035 + 1.0 * 6.104837894439697
Epoch 410, val loss: 0.7084543704986572
Epoch 420, training loss: 6.516531467437744 = 0.4008258581161499 + 1.0 * 6.115705490112305
Epoch 420, val loss: 0.6924080848693848
Epoch 430, training loss: 6.475095748901367 = 0.3756071627140045 + 1.0 * 6.099488735198975
Epoch 430, val loss: 0.6774781942367554
Epoch 440, training loss: 6.445977687835693 = 0.35134875774383545 + 1.0 * 6.094628810882568
Epoch 440, val loss: 0.6635743975639343
Epoch 450, training loss: 6.419249534606934 = 0.32778623700141907 + 1.0 * 6.091463088989258
Epoch 450, val loss: 0.6502870917320251
Epoch 460, training loss: 6.393412113189697 = 0.3049268424510956 + 1.0 * 6.088485240936279
Epoch 460, val loss: 0.6377590894699097
Epoch 470, training loss: 6.369171619415283 = 0.28290316462516785 + 1.0 * 6.086268424987793
Epoch 470, val loss: 0.6260592937469482
Epoch 480, training loss: 6.347418785095215 = 0.26196926832199097 + 1.0 * 6.085449695587158
Epoch 480, val loss: 0.615400493144989
Epoch 490, training loss: 6.329037189483643 = 0.24247783422470093 + 1.0 * 6.086559295654297
Epoch 490, val loss: 0.6059207320213318
Epoch 500, training loss: 6.305198669433594 = 0.22426697611808777 + 1.0 * 6.080931663513184
Epoch 500, val loss: 0.5974257588386536
Epoch 510, training loss: 6.291941165924072 = 0.20734764635562897 + 1.0 * 6.084593296051025
Epoch 510, val loss: 0.5900025367736816
Epoch 520, training loss: 6.271668434143066 = 0.19189438223838806 + 1.0 * 6.079773902893066
Epoch 520, val loss: 0.5836971402168274
Epoch 530, training loss: 6.254457473754883 = 0.17771436274051666 + 1.0 * 6.076743125915527
Epoch 530, val loss: 0.5784271359443665
Epoch 540, training loss: 6.247915744781494 = 0.16474345326423645 + 1.0 * 6.08317232131958
Epoch 540, val loss: 0.574196994304657
Epoch 550, training loss: 6.227167129516602 = 0.15299369394779205 + 1.0 * 6.074173450469971
Epoch 550, val loss: 0.5708943009376526
Epoch 560, training loss: 6.213440418243408 = 0.14230875670909882 + 1.0 * 6.071131706237793
Epoch 560, val loss: 0.5684136152267456
Epoch 570, training loss: 6.206943035125732 = 0.13257482647895813 + 1.0 * 6.074368000030518
Epoch 570, val loss: 0.5667731165885925
Epoch 580, training loss: 6.195094108581543 = 0.12376914173364639 + 1.0 * 6.071324825286865
Epoch 580, val loss: 0.5658425688743591
Epoch 590, training loss: 6.185708999633789 = 0.1158246099948883 + 1.0 * 6.069884300231934
Epoch 590, val loss: 0.565549910068512
Epoch 600, training loss: 6.174984455108643 = 0.10865335911512375 + 1.0 * 6.066330909729004
Epoch 600, val loss: 0.5657910108566284
Epoch 610, training loss: 6.165971755981445 = 0.1021006852388382 + 1.0 * 6.063870906829834
Epoch 610, val loss: 0.5664941668510437
Epoch 620, training loss: 6.159046649932861 = 0.09608064591884613 + 1.0 * 6.0629658699035645
Epoch 620, val loss: 0.5676842927932739
Epoch 630, training loss: 6.159921646118164 = 0.09054679423570633 + 1.0 * 6.069375038146973
Epoch 630, val loss: 0.5692751407623291
Epoch 640, training loss: 6.145785808563232 = 0.08552204072475433 + 1.0 * 6.060263633728027
Epoch 640, val loss: 0.5712029337882996
Epoch 650, training loss: 6.140589714050293 = 0.08090728521347046 + 1.0 * 6.059682369232178
Epoch 650, val loss: 0.573330283164978
Epoch 660, training loss: 6.13332986831665 = 0.0766354650259018 + 1.0 * 6.056694507598877
Epoch 660, val loss: 0.5757842063903809
Epoch 670, training loss: 6.1281514167785645 = 0.07266274839639664 + 1.0 * 6.055488586425781
Epoch 670, val loss: 0.5785117745399475
Epoch 680, training loss: 6.135570526123047 = 0.06898697465658188 + 1.0 * 6.066583633422852
Epoch 680, val loss: 0.5814181566238403
Epoch 690, training loss: 6.12474250793457 = 0.06561993807554245 + 1.0 * 6.059122562408447
Epoch 690, val loss: 0.5843725204467773
Epoch 700, training loss: 6.122333526611328 = 0.0625070184469223 + 1.0 * 6.059826374053955
Epoch 700, val loss: 0.5874661207199097
Epoch 710, training loss: 6.113109111785889 = 0.05960801988840103 + 1.0 * 6.053501129150391
Epoch 710, val loss: 0.5907381176948547
Epoch 720, training loss: 6.107855796813965 = 0.05689876154065132 + 1.0 * 6.050957202911377
Epoch 720, val loss: 0.5941163301467896
Epoch 730, training loss: 6.105740070343018 = 0.05436057597398758 + 1.0 * 6.051379680633545
Epoch 730, val loss: 0.5975964665412903
Epoch 740, training loss: 6.110084056854248 = 0.05198924243450165 + 1.0 * 6.0580949783325195
Epoch 740, val loss: 0.6011791229248047
Epoch 750, training loss: 6.103283405303955 = 0.04978644475340843 + 1.0 * 6.053496837615967
Epoch 750, val loss: 0.6047996282577515
Epoch 760, training loss: 6.097173690795898 = 0.04773067310452461 + 1.0 * 6.049443244934082
Epoch 760, val loss: 0.6084180474281311
Epoch 770, training loss: 6.091359615325928 = 0.045802392065525055 + 1.0 * 6.045557022094727
Epoch 770, val loss: 0.6120511889457703
Epoch 780, training loss: 6.09003210067749 = 0.043976400047540665 + 1.0 * 6.046055793762207
Epoch 780, val loss: 0.6157947778701782
Epoch 790, training loss: 6.090665340423584 = 0.0422535240650177 + 1.0 * 6.048411846160889
Epoch 790, val loss: 0.6196185946464539
Epoch 800, training loss: 6.084339141845703 = 0.0406443290412426 + 1.0 * 6.043694972991943
Epoch 800, val loss: 0.6233354806900024
Epoch 810, training loss: 6.081686019897461 = 0.03912265971302986 + 1.0 * 6.042563438415527
Epoch 810, val loss: 0.6270831823348999
Epoch 820, training loss: 6.080802917480469 = 0.03767620772123337 + 1.0 * 6.043126583099365
Epoch 820, val loss: 0.6309091448783875
Epoch 830, training loss: 6.079301357269287 = 0.036311306059360504 + 1.0 * 6.042990207672119
Epoch 830, val loss: 0.6347419619560242
Epoch 840, training loss: 6.077723979949951 = 0.035027939826250076 + 1.0 * 6.042695999145508
Epoch 840, val loss: 0.6385135054588318
Epoch 850, training loss: 6.0747389793396 = 0.03381732106208801 + 1.0 * 6.040921688079834
Epoch 850, val loss: 0.6421874761581421
Epoch 860, training loss: 6.072029113769531 = 0.032668158411979675 + 1.0 * 6.039361000061035
Epoch 860, val loss: 0.6458908319473267
Epoch 870, training loss: 6.071382522583008 = 0.031571581959724426 + 1.0 * 6.039811134338379
Epoch 870, val loss: 0.6496737003326416
Epoch 880, training loss: 6.070908546447754 = 0.030528340488672256 + 1.0 * 6.040380001068115
Epoch 880, val loss: 0.6534498333930969
Epoch 890, training loss: 6.068990230560303 = 0.029543660581111908 + 1.0 * 6.0394463539123535
Epoch 890, val loss: 0.6571398973464966
Epoch 900, training loss: 6.0648627281188965 = 0.028609812259674072 + 1.0 * 6.036252975463867
Epoch 900, val loss: 0.6607502698898315
Epoch 910, training loss: 6.061991214752197 = 0.027715547010302544 + 1.0 * 6.034275531768799
Epoch 910, val loss: 0.664397656917572
Epoch 920, training loss: 6.062082290649414 = 0.026856165379285812 + 1.0 * 6.035226345062256
Epoch 920, val loss: 0.6681016683578491
Epoch 930, training loss: 6.06093168258667 = 0.026037003844976425 + 1.0 * 6.0348944664001465
Epoch 930, val loss: 0.6718533039093018
Epoch 940, training loss: 6.061371803283691 = 0.025267530232667923 + 1.0 * 6.036104202270508
Epoch 940, val loss: 0.6753774881362915
Epoch 950, training loss: 6.0572614669799805 = 0.024532590061426163 + 1.0 * 6.032728672027588
Epoch 950, val loss: 0.678840696811676
Epoch 960, training loss: 6.055809020996094 = 0.02382107637822628 + 1.0 * 6.031988143920898
Epoch 960, val loss: 0.6824451088905334
Epoch 970, training loss: 6.062455654144287 = 0.023138118907809258 + 1.0 * 6.039317607879639
Epoch 970, val loss: 0.6861234903335571
Epoch 980, training loss: 6.0559492111206055 = 0.022495508193969727 + 1.0 * 6.033453464508057
Epoch 980, val loss: 0.6896275281906128
Epoch 990, training loss: 6.051694869995117 = 0.02188272587954998 + 1.0 * 6.029812335968018
Epoch 990, val loss: 0.6929474472999573
Epoch 1000, training loss: 6.0506591796875 = 0.02128775604069233 + 1.0 * 6.02937126159668
Epoch 1000, val loss: 0.6964442133903503
Epoch 1010, training loss: 6.049491882324219 = 0.020710762590169907 + 1.0 * 6.028780937194824
Epoch 1010, val loss: 0.7000259160995483
Epoch 1020, training loss: 6.064024925231934 = 0.020155690610408783 + 1.0 * 6.0438690185546875
Epoch 1020, val loss: 0.7035530805587769
Epoch 1030, training loss: 6.04865837097168 = 0.01963047683238983 + 1.0 * 6.029027938842773
Epoch 1030, val loss: 0.7070199847221375
Epoch 1040, training loss: 6.047289848327637 = 0.019129710271954536 + 1.0 * 6.028160095214844
Epoch 1040, val loss: 0.7102994918823242
Epoch 1050, training loss: 6.045118808746338 = 0.01864449493587017 + 1.0 * 6.026474475860596
Epoch 1050, val loss: 0.7136464715003967
Epoch 1060, training loss: 6.048123359680176 = 0.01817326806485653 + 1.0 * 6.029950141906738
Epoch 1060, val loss: 0.7170922160148621
Epoch 1070, training loss: 6.0468878746032715 = 0.01772041618824005 + 1.0 * 6.029167652130127
Epoch 1070, val loss: 0.7204911112785339
Epoch 1080, training loss: 6.046360969543457 = 0.01728898473083973 + 1.0 * 6.029071807861328
Epoch 1080, val loss: 0.7237394452095032
Epoch 1090, training loss: 6.041810989379883 = 0.016875525936484337 + 1.0 * 6.024935245513916
Epoch 1090, val loss: 0.7269390821456909
Epoch 1100, training loss: 6.040602684020996 = 0.01647433638572693 + 1.0 * 6.024128437042236
Epoch 1100, val loss: 0.7301738858222961
Epoch 1110, training loss: 6.040768623352051 = 0.016083989292383194 + 1.0 * 6.024684429168701
Epoch 1110, val loss: 0.7334368824958801
Epoch 1120, training loss: 6.0447258949279785 = 0.015707029029726982 + 1.0 * 6.029018878936768
Epoch 1120, val loss: 0.7367344498634338
Epoch 1130, training loss: 6.041074752807617 = 0.01534679438918829 + 1.0 * 6.02572774887085
Epoch 1130, val loss: 0.7399187088012695
Epoch 1140, training loss: 6.039804935455322 = 0.015002671629190445 + 1.0 * 6.024802207946777
Epoch 1140, val loss: 0.7429991960525513
Epoch 1150, training loss: 6.042431831359863 = 0.014669987373054028 + 1.0 * 6.027761936187744
Epoch 1150, val loss: 0.7460388541221619
Epoch 1160, training loss: 6.036596775054932 = 0.014347338117659092 + 1.0 * 6.022249221801758
Epoch 1160, val loss: 0.7491546869277954
Epoch 1170, training loss: 6.035516262054443 = 0.014035598374903202 + 1.0 * 6.021480560302734
Epoch 1170, val loss: 0.7522074580192566
Epoch 1180, training loss: 6.036230087280273 = 0.013732300139963627 + 1.0 * 6.022497653961182
Epoch 1180, val loss: 0.7552700638771057
Epoch 1190, training loss: 6.036426544189453 = 0.01343807764351368 + 1.0 * 6.022988319396973
Epoch 1190, val loss: 0.7584109902381897
Epoch 1200, training loss: 6.032951354980469 = 0.013156650587916374 + 1.0 * 6.019794940948486
Epoch 1200, val loss: 0.7613553404808044
Epoch 1210, training loss: 6.0322370529174805 = 0.012884901836514473 + 1.0 * 6.019351959228516
Epoch 1210, val loss: 0.7642348408699036
Epoch 1220, training loss: 6.033459663391113 = 0.012618467211723328 + 1.0 * 6.020841121673584
Epoch 1220, val loss: 0.7672386169433594
Epoch 1230, training loss: 6.034867286682129 = 0.012359663844108582 + 1.0 * 6.022507667541504
Epoch 1230, val loss: 0.7703138589859009
Epoch 1240, training loss: 6.030697822570801 = 0.012112455442547798 + 1.0 * 6.018585205078125
Epoch 1240, val loss: 0.7731829285621643
Epoch 1250, training loss: 6.032499313354492 = 0.011874143034219742 + 1.0 * 6.020625114440918
Epoch 1250, val loss: 0.7759859561920166
Epoch 1260, training loss: 6.031569480895996 = 0.011641379445791245 + 1.0 * 6.019927978515625
Epoch 1260, val loss: 0.7788631916046143
Epoch 1270, training loss: 6.029295921325684 = 0.011414921842515469 + 1.0 * 6.017880916595459
Epoch 1270, val loss: 0.7816754579544067
Epoch 1280, training loss: 6.035153865814209 = 0.01119473110884428 + 1.0 * 6.023959159851074
Epoch 1280, val loss: 0.7845277786254883
Epoch 1290, training loss: 6.0300211906433105 = 0.010981129482388496 + 1.0 * 6.019040107727051
Epoch 1290, val loss: 0.787362813949585
Epoch 1300, training loss: 6.028219699859619 = 0.010777028277516365 + 1.0 * 6.01744270324707
Epoch 1300, val loss: 0.7900631427764893
Epoch 1310, training loss: 6.026198863983154 = 0.01057690940797329 + 1.0 * 6.015622138977051
Epoch 1310, val loss: 0.792739748954773
Epoch 1320, training loss: 6.026933193206787 = 0.010380307212471962 + 1.0 * 6.016552925109863
Epoch 1320, val loss: 0.7955232858657837
Epoch 1330, training loss: 6.028642654418945 = 0.01018879096955061 + 1.0 * 6.018454074859619
Epoch 1330, val loss: 0.7984177470207214
Epoch 1340, training loss: 6.028920650482178 = 0.010007752105593681 + 1.0 * 6.0189127922058105
Epoch 1340, val loss: 0.801030158996582
Epoch 1350, training loss: 6.0240654945373535 = 0.009833290241658688 + 1.0 * 6.014232158660889
Epoch 1350, val loss: 0.8034425973892212
Epoch 1360, training loss: 6.024115562438965 = 0.009660725481808186 + 1.0 * 6.0144548416137695
Epoch 1360, val loss: 0.8060270547866821
Epoch 1370, training loss: 6.030742645263672 = 0.009490108117461205 + 1.0 * 6.021252632141113
Epoch 1370, val loss: 0.8087876439094543
Epoch 1380, training loss: 6.024712085723877 = 0.009324304759502411 + 1.0 * 6.015388011932373
Epoch 1380, val loss: 0.8114032745361328
Epoch 1390, training loss: 6.02272367477417 = 0.009164745919406414 + 1.0 * 6.013558864593506
Epoch 1390, val loss: 0.8139714598655701
Epoch 1400, training loss: 6.025874614715576 = 0.009008132852613926 + 1.0 * 6.016866683959961
Epoch 1400, val loss: 0.8164712190628052
Epoch 1410, training loss: 6.021820545196533 = 0.008855361491441727 + 1.0 * 6.012965202331543
Epoch 1410, val loss: 0.8191849589347839
Epoch 1420, training loss: 6.0224761962890625 = 0.008707918226718903 + 1.0 * 6.013768196105957
Epoch 1420, val loss: 0.8216602206230164
Epoch 1430, training loss: 6.0230865478515625 = 0.008564022369682789 + 1.0 * 6.014522552490234
Epoch 1430, val loss: 0.8241721391677856
Epoch 1440, training loss: 6.025426387786865 = 0.008423550985753536 + 1.0 * 6.017003059387207
Epoch 1440, val loss: 0.8266589045524597
Epoch 1450, training loss: 6.020020484924316 = 0.00828725378960371 + 1.0 * 6.011733055114746
Epoch 1450, val loss: 0.8291848301887512
Epoch 1460, training loss: 6.01917839050293 = 0.008154713548719883 + 1.0 * 6.01102352142334
Epoch 1460, val loss: 0.8315607905387878
Epoch 1470, training loss: 6.0194621086120605 = 0.008024175651371479 + 1.0 * 6.011437892913818
Epoch 1470, val loss: 0.8339996337890625
Epoch 1480, training loss: 6.023727893829346 = 0.00789604615420103 + 1.0 * 6.01583194732666
Epoch 1480, val loss: 0.836540162563324
Epoch 1490, training loss: 6.0193986892700195 = 0.0077728088945150375 + 1.0 * 6.01162576675415
Epoch 1490, val loss: 0.8389397263526917
Epoch 1500, training loss: 6.025051593780518 = 0.007653625216335058 + 1.0 * 6.017397880554199
Epoch 1500, val loss: 0.8412725925445557
Epoch 1510, training loss: 6.0193610191345215 = 0.007537286262959242 + 1.0 * 6.011823654174805
Epoch 1510, val loss: 0.8435576558113098
Epoch 1520, training loss: 6.016557693481445 = 0.0074237678200006485 + 1.0 * 6.009133815765381
Epoch 1520, val loss: 0.8458316922187805
Epoch 1530, training loss: 6.01822566986084 = 0.007311339024454355 + 1.0 * 6.010914325714111
Epoch 1530, val loss: 0.8481945395469666
Epoch 1540, training loss: 6.018880367279053 = 0.007200752850621939 + 1.0 * 6.011679649353027
Epoch 1540, val loss: 0.8506181240081787
Epoch 1550, training loss: 6.017542362213135 = 0.007094102445989847 + 1.0 * 6.010448455810547
Epoch 1550, val loss: 0.8529396653175354
Epoch 1560, training loss: 6.019692420959473 = 0.006989608984440565 + 1.0 * 6.012702941894531
Epoch 1560, val loss: 0.8552232384681702
Epoch 1570, training loss: 6.015518665313721 = 0.006887550465762615 + 1.0 * 6.008631229400635
Epoch 1570, val loss: 0.8574643135070801
Epoch 1580, training loss: 6.015044689178467 = 0.006788219790905714 + 1.0 * 6.008256435394287
Epoch 1580, val loss: 0.8597226142883301
Epoch 1590, training loss: 6.018633842468262 = 0.006690381094813347 + 1.0 * 6.011943340301514
Epoch 1590, val loss: 0.8619650602340698
Epoch 1600, training loss: 6.015829563140869 = 0.006594989914447069 + 1.0 * 6.009234428405762
Epoch 1600, val loss: 0.864266574382782
Epoch 1610, training loss: 6.014033317565918 = 0.0065020546317100525 + 1.0 * 6.00753116607666
Epoch 1610, val loss: 0.8664541840553284
Epoch 1620, training loss: 6.0203328132629395 = 0.006411160342395306 + 1.0 * 6.013921737670898
Epoch 1620, val loss: 0.8687220215797424
Epoch 1630, training loss: 6.014050483703613 = 0.006321735680103302 + 1.0 * 6.007728576660156
Epoch 1630, val loss: 0.8709526658058167
Epoch 1640, training loss: 6.0124897956848145 = 0.0062359231524169445 + 1.0 * 6.006253719329834
Epoch 1640, val loss: 0.873024582862854
Epoch 1650, training loss: 6.012166976928711 = 0.006150740198791027 + 1.0 * 6.006016254425049
Epoch 1650, val loss: 0.8750978112220764
Epoch 1660, training loss: 6.015865802764893 = 0.006066407077014446 + 1.0 * 6.009799480438232
Epoch 1660, val loss: 0.8773035407066345
Epoch 1670, training loss: 6.0120134353637695 = 0.005983856040984392 + 1.0 * 6.0060296058654785
Epoch 1670, val loss: 0.8795743584632874
Epoch 1680, training loss: 6.012351989746094 = 0.005904641933739185 + 1.0 * 6.0064473152160645
Epoch 1680, val loss: 0.8816651105880737
Epoch 1690, training loss: 6.01530122756958 = 0.005826923064887524 + 1.0 * 6.009474277496338
Epoch 1690, val loss: 0.8836898803710938
Epoch 1700, training loss: 6.011101245880127 = 0.005750670097768307 + 1.0 * 6.005350589752197
Epoch 1700, val loss: 0.8858129382133484
Epoch 1710, training loss: 6.009944438934326 = 0.005676565691828728 + 1.0 * 6.004267692565918
Epoch 1710, val loss: 0.8878320455551147
Epoch 1720, training loss: 6.010055065155029 = 0.0056036231108009815 + 1.0 * 6.004451274871826
Epoch 1720, val loss: 0.8898308277130127
Epoch 1730, training loss: 6.0144829750061035 = 0.005531175993382931 + 1.0 * 6.008951663970947
Epoch 1730, val loss: 0.8919381499290466
Epoch 1740, training loss: 6.014455795288086 = 0.005460292100906372 + 1.0 * 6.008995532989502
Epoch 1740, val loss: 0.89399653673172
Epoch 1750, training loss: 6.008887767791748 = 0.005391779821366072 + 1.0 * 6.003496170043945
Epoch 1750, val loss: 0.8960332274436951
Epoch 1760, training loss: 6.00888729095459 = 0.005324949976056814 + 1.0 * 6.0035624504089355
Epoch 1760, val loss: 0.8979688882827759
Epoch 1770, training loss: 6.007702827453613 = 0.005258374847471714 + 1.0 * 6.002444267272949
Epoch 1770, val loss: 0.8999616503715515
Epoch 1780, training loss: 6.008278846740723 = 0.005192283540964127 + 1.0 * 6.003086566925049
Epoch 1780, val loss: 0.9020293951034546
Epoch 1790, training loss: 6.016149997711182 = 0.00512734055519104 + 1.0 * 6.011022567749023
Epoch 1790, val loss: 0.9041266441345215
Epoch 1800, training loss: 6.007749557495117 = 0.005065202713012695 + 1.0 * 6.002684116363525
Epoch 1800, val loss: 0.9061486124992371
Epoch 1810, training loss: 6.007480621337891 = 0.005005655810236931 + 1.0 * 6.002474784851074
Epoch 1810, val loss: 0.9078288078308105
Epoch 1820, training loss: 6.006228446960449 = 0.0049461666494607925 + 1.0 * 6.001282215118408
Epoch 1820, val loss: 0.9096704721450806
Epoch 1830, training loss: 6.0141167640686035 = 0.004886389710009098 + 1.0 * 6.009230136871338
Epoch 1830, val loss: 0.9117140173912048
Epoch 1840, training loss: 6.0064897537231445 = 0.004827830940485001 + 1.0 * 6.001661777496338
Epoch 1840, val loss: 0.9137876033782959
Epoch 1850, training loss: 6.007130146026611 = 0.00477159908041358 + 1.0 * 6.002358436584473
Epoch 1850, val loss: 0.915587842464447
Epoch 1860, training loss: 6.007593154907227 = 0.00471620075404644 + 1.0 * 6.0028767585754395
Epoch 1860, val loss: 0.9174044132232666
Epoch 1870, training loss: 6.012679100036621 = 0.004661647602915764 + 1.0 * 6.008017539978027
Epoch 1870, val loss: 0.9192793965339661
Epoch 1880, training loss: 6.005955219268799 = 0.004608262330293655 + 1.0 * 6.001347064971924
Epoch 1880, val loss: 0.9211564064025879
Epoch 1890, training loss: 6.004319667816162 = 0.004556124098598957 + 1.0 * 5.999763488769531
Epoch 1890, val loss: 0.9229571223258972
Epoch 1900, training loss: 6.007880210876465 = 0.004504036623984575 + 1.0 * 6.003376007080078
Epoch 1900, val loss: 0.924837052822113
Epoch 1910, training loss: 6.004075527191162 = 0.004452480003237724 + 1.0 * 5.999622821807861
Epoch 1910, val loss: 0.9267900586128235
Epoch 1920, training loss: 6.003945827484131 = 0.004402706399559975 + 1.0 * 5.999543190002441
Epoch 1920, val loss: 0.9285740256309509
Epoch 1930, training loss: 6.007452964782715 = 0.004353645257651806 + 1.0 * 6.00309944152832
Epoch 1930, val loss: 0.930372953414917
Epoch 1940, training loss: 6.004113674163818 = 0.004304939415305853 + 1.0 * 5.9998087882995605
Epoch 1940, val loss: 0.9322295784950256
Epoch 1950, training loss: 6.007674217224121 = 0.00425766222178936 + 1.0 * 6.003416538238525
Epoch 1950, val loss: 0.9340574145317078
Epoch 1960, training loss: 6.002670764923096 = 0.004211682826280594 + 1.0 * 5.9984588623046875
Epoch 1960, val loss: 0.9358159899711609
Epoch 1970, training loss: 6.003418922424316 = 0.004166342783719301 + 1.0 * 5.999252796173096
Epoch 1970, val loss: 0.937554657459259
Epoch 1980, training loss: 6.0064568519592285 = 0.00412078108638525 + 1.0 * 6.002336025238037
Epoch 1980, val loss: 0.9393482804298401
Epoch 1990, training loss: 6.004242420196533 = 0.004076051991432905 + 1.0 * 6.000166416168213
Epoch 1990, val loss: 0.9412247538566589
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8856
Flip ASR: 0.8622/225 nodes
The final ASR:0.71587, 0.13171, Accuracy:0.80494, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11652])
remove edge: torch.Size([2, 9558])
updated graph: torch.Size([2, 10654])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83333, 0.00302
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.328277587890625 = 1.9544215202331543 + 1.0 * 8.373856544494629
Epoch 0, val loss: 1.9474303722381592
Epoch 10, training loss: 10.31748104095459 = 1.9440057277679443 + 1.0 * 8.373475074768066
Epoch 10, val loss: 1.9378349781036377
Epoch 20, training loss: 10.301840782165527 = 1.9310542345046997 + 1.0 * 8.370786666870117
Epoch 20, val loss: 1.9256031513214111
Epoch 30, training loss: 10.263113021850586 = 1.913253664970398 + 1.0 * 8.349859237670898
Epoch 30, val loss: 1.90874183177948
Epoch 40, training loss: 10.096024513244629 = 1.890713095664978 + 1.0 * 8.20531177520752
Epoch 40, val loss: 1.8880146741867065
Epoch 50, training loss: 9.517635345458984 = 1.8674023151397705 + 1.0 * 7.650232791900635
Epoch 50, val loss: 1.8669670820236206
Epoch 60, training loss: 9.100484848022461 = 1.849996566772461 + 1.0 * 7.25048828125
Epoch 60, val loss: 1.8519210815429688
Epoch 70, training loss: 8.685352325439453 = 1.8388689756393433 + 1.0 * 6.8464837074279785
Epoch 70, val loss: 1.8421694040298462
Epoch 80, training loss: 8.483301162719727 = 1.8278465270996094 + 1.0 * 6.655454158782959
Epoch 80, val loss: 1.8325060606002808
Epoch 90, training loss: 8.354187965393066 = 1.814703345298767 + 1.0 * 6.539484977722168
Epoch 90, val loss: 1.821290373802185
Epoch 100, training loss: 8.253715515136719 = 1.8022388219833374 + 1.0 * 6.451476573944092
Epoch 100, val loss: 1.8113763332366943
Epoch 110, training loss: 8.187759399414062 = 1.791661024093628 + 1.0 * 6.396098613739014
Epoch 110, val loss: 1.8030253648757935
Epoch 120, training loss: 8.140397071838379 = 1.7812669277191162 + 1.0 * 6.359130382537842
Epoch 120, val loss: 1.7942960262298584
Epoch 130, training loss: 8.09744930267334 = 1.7703194618225098 + 1.0 * 6.32712984085083
Epoch 130, val loss: 1.7847418785095215
Epoch 140, training loss: 8.060026168823242 = 1.7583380937576294 + 1.0 * 6.301687717437744
Epoch 140, val loss: 1.7744711637496948
Epoch 150, training loss: 8.027568817138672 = 1.744743824005127 + 1.0 * 6.282824516296387
Epoch 150, val loss: 1.763256311416626
Epoch 160, training loss: 7.99459981918335 = 1.7289358377456665 + 1.0 * 6.265664100646973
Epoch 160, val loss: 1.750588297843933
Epoch 170, training loss: 7.961037635803223 = 1.710183024406433 + 1.0 * 6.2508544921875
Epoch 170, val loss: 1.7358077764511108
Epoch 180, training loss: 7.9258575439453125 = 1.6874902248382568 + 1.0 * 6.238367557525635
Epoch 180, val loss: 1.717981219291687
Epoch 190, training loss: 7.887823104858398 = 1.659877061843872 + 1.0 * 6.227945804595947
Epoch 190, val loss: 1.6962264776229858
Epoch 200, training loss: 7.846642971038818 = 1.627217173576355 + 1.0 * 6.219425678253174
Epoch 200, val loss: 1.6705611944198608
Epoch 210, training loss: 7.8000898361206055 = 1.5893253087997437 + 1.0 * 6.210764408111572
Epoch 210, val loss: 1.6407893896102905
Epoch 220, training loss: 7.748190402984619 = 1.5460416078567505 + 1.0 * 6.202148914337158
Epoch 220, val loss: 1.6066759824752808
Epoch 230, training loss: 7.693007469177246 = 1.497833490371704 + 1.0 * 6.195174217224121
Epoch 230, val loss: 1.568733811378479
Epoch 240, training loss: 7.648031234741211 = 1.4463485479354858 + 1.0 * 6.2016825675964355
Epoch 240, val loss: 1.528651475906372
Epoch 250, training loss: 7.582433223724365 = 1.3955696821212769 + 1.0 * 6.186863422393799
Epoch 250, val loss: 1.4895226955413818
Epoch 260, training loss: 7.524435043334961 = 1.3453798294067383 + 1.0 * 6.179055213928223
Epoch 260, val loss: 1.450971007347107
Epoch 270, training loss: 7.468881607055664 = 1.2960050106048584 + 1.0 * 6.172876358032227
Epoch 270, val loss: 1.4134773015975952
Epoch 280, training loss: 7.414797782897949 = 1.247164249420166 + 1.0 * 6.167633533477783
Epoch 280, val loss: 1.3766391277313232
Epoch 290, training loss: 7.375141620635986 = 1.199156641960144 + 1.0 * 6.175984859466553
Epoch 290, val loss: 1.3409297466278076
Epoch 300, training loss: 7.315052032470703 = 1.154227375984192 + 1.0 * 6.160824775695801
Epoch 300, val loss: 1.3071180582046509
Epoch 310, training loss: 7.2657060623168945 = 1.111043930053711 + 1.0 * 6.154662132263184
Epoch 310, val loss: 1.2749801874160767
Epoch 320, training loss: 7.219490051269531 = 1.0697367191314697 + 1.0 * 6.149753093719482
Epoch 320, val loss: 1.2439870834350586
Epoch 330, training loss: 7.179459095001221 = 1.0306992530822754 + 1.0 * 6.148759841918945
Epoch 330, val loss: 1.2149224281311035
Epoch 340, training loss: 7.137935638427734 = 0.9945030212402344 + 1.0 * 6.1434326171875
Epoch 340, val loss: 1.1883193254470825
Epoch 350, training loss: 7.099215507507324 = 0.9604995250701904 + 1.0 * 6.138715744018555
Epoch 350, val loss: 1.1635477542877197
Epoch 360, training loss: 7.0725202560424805 = 0.928259551525116 + 1.0 * 6.144260883331299
Epoch 360, val loss: 1.140424132347107
Epoch 370, training loss: 7.0301289558410645 = 0.8978995680809021 + 1.0 * 6.132229328155518
Epoch 370, val loss: 1.1188162565231323
Epoch 380, training loss: 6.997501850128174 = 0.8684091567993164 + 1.0 * 6.129092693328857
Epoch 380, val loss: 1.0981149673461914
Epoch 390, training loss: 6.971149921417236 = 0.8391692042350769 + 1.0 * 6.131980895996094
Epoch 390, val loss: 1.077781081199646
Epoch 400, training loss: 6.9339447021484375 = 0.8099939227104187 + 1.0 * 6.123950958251953
Epoch 400, val loss: 1.0578315258026123
Epoch 410, training loss: 6.900620937347412 = 0.7805705666542053 + 1.0 * 6.120050430297852
Epoch 410, val loss: 1.0376750230789185
Epoch 420, training loss: 6.874630451202393 = 0.7505660057067871 + 1.0 * 6.1240644454956055
Epoch 420, val loss: 1.0171414613723755
Epoch 430, training loss: 6.834907054901123 = 0.7202457785606384 + 1.0 * 6.11466121673584
Epoch 430, val loss: 0.9963567852973938
Epoch 440, training loss: 6.802189826965332 = 0.6893637180328369 + 1.0 * 6.112825870513916
Epoch 440, val loss: 0.9752729535102844
Epoch 450, training loss: 6.7810258865356445 = 0.6577358841896057 + 1.0 * 6.123290061950684
Epoch 450, val loss: 0.9537909626960754
Epoch 460, training loss: 6.737354278564453 = 0.6260170340538025 + 1.0 * 6.111337184906006
Epoch 460, val loss: 0.9320635795593262
Epoch 470, training loss: 6.700558662414551 = 0.5939165949821472 + 1.0 * 6.106642246246338
Epoch 470, val loss: 0.9101885557174683
Epoch 480, training loss: 6.665400505065918 = 0.5614694356918335 + 1.0 * 6.103930950164795
Epoch 480, val loss: 0.8880686163902283
Epoch 490, training loss: 6.630805969238281 = 0.5290608406066895 + 1.0 * 6.101745128631592
Epoch 490, val loss: 0.8663467764854431
Epoch 500, training loss: 6.6016526222229 = 0.4970060884952545 + 1.0 * 6.104646682739258
Epoch 500, val loss: 0.8449029922485352
Epoch 510, training loss: 6.564743518829346 = 0.466101735830307 + 1.0 * 6.098641872406006
Epoch 510, val loss: 0.8246544599533081
Epoch 520, training loss: 6.532751083374023 = 0.4362810254096985 + 1.0 * 6.096469879150391
Epoch 520, val loss: 0.8054379224777222
Epoch 530, training loss: 6.501458644866943 = 0.40759941935539246 + 1.0 * 6.0938591957092285
Epoch 530, val loss: 0.787316083908081
Epoch 540, training loss: 6.4909257888793945 = 0.38029420375823975 + 1.0 * 6.110631465911865
Epoch 540, val loss: 0.7705259919166565
Epoch 550, training loss: 6.451108932495117 = 0.35493671894073486 + 1.0 * 6.096172332763672
Epoch 550, val loss: 0.7553883194923401
Epoch 560, training loss: 6.422121524810791 = 0.33116886019706726 + 1.0 * 6.0909528732299805
Epoch 560, val loss: 0.7419025897979736
Epoch 570, training loss: 6.405106544494629 = 0.3087197542190552 + 1.0 * 6.096386909484863
Epoch 570, val loss: 0.7294945120811462
Epoch 580, training loss: 6.376595973968506 = 0.2877192199230194 + 1.0 * 6.088876724243164
Epoch 580, val loss: 0.7182807922363281
Epoch 590, training loss: 6.353137969970703 = 0.2679901421070099 + 1.0 * 6.085147857666016
Epoch 590, val loss: 0.7083826661109924
Epoch 600, training loss: 6.340022087097168 = 0.24942319095134735 + 1.0 * 6.090599060058594
Epoch 600, val loss: 0.6994217038154602
Epoch 610, training loss: 6.317045211791992 = 0.23222357034683228 + 1.0 * 6.084821701049805
Epoch 610, val loss: 0.6915480494499207
Epoch 620, training loss: 6.2959418296813965 = 0.21624013781547546 + 1.0 * 6.079701900482178
Epoch 620, val loss: 0.6848040819168091
Epoch 630, training loss: 6.282021999359131 = 0.201396182179451 + 1.0 * 6.080626010894775
Epoch 630, val loss: 0.6790636777877808
Epoch 640, training loss: 6.265702247619629 = 0.18773168325424194 + 1.0 * 6.077970504760742
Epoch 640, val loss: 0.6741832494735718
Epoch 650, training loss: 6.268961429595947 = 0.17519953846931458 + 1.0 * 6.093761920928955
Epoch 650, val loss: 0.6703624725341797
Epoch 660, training loss: 6.240354537963867 = 0.1638374924659729 + 1.0 * 6.076517105102539
Epoch 660, val loss: 0.6674537062644958
Epoch 670, training loss: 6.227376461029053 = 0.15349307656288147 + 1.0 * 6.073883533477783
Epoch 670, val loss: 0.6655703783035278
Epoch 680, training loss: 6.215597152709961 = 0.1439598798751831 + 1.0 * 6.071637153625488
Epoch 680, val loss: 0.6642801761627197
Epoch 690, training loss: 6.209290504455566 = 0.135196253657341 + 1.0 * 6.074094295501709
Epoch 690, val loss: 0.6637107133865356
Epoch 700, training loss: 6.208686351776123 = 0.12718985974788666 + 1.0 * 6.081496715545654
Epoch 700, val loss: 0.6636840105056763
Epoch 710, training loss: 6.192385196685791 = 0.11989562958478928 + 1.0 * 6.0724897384643555
Epoch 710, val loss: 0.6643096804618835
Epoch 720, training loss: 6.180886745452881 = 0.1132064014673233 + 1.0 * 6.067680358886719
Epoch 720, val loss: 0.6653670072555542
Epoch 730, training loss: 6.173022270202637 = 0.10699045658111572 + 1.0 * 6.0660319328308105
Epoch 730, val loss: 0.6666958332061768
Epoch 740, training loss: 6.1720685958862305 = 0.1012294590473175 + 1.0 * 6.070838928222656
Epoch 740, val loss: 0.6684376001358032
Epoch 750, training loss: 6.16634464263916 = 0.09590258449316025 + 1.0 * 6.070442199707031
Epoch 750, val loss: 0.670432984828949
Epoch 760, training loss: 6.155917167663574 = 0.09097672998905182 + 1.0 * 6.064940452575684
Epoch 760, val loss: 0.6727522015571594
Epoch 770, training loss: 6.149492263793945 = 0.08643578737974167 + 1.0 * 6.063056468963623
Epoch 770, val loss: 0.6754456758499146
Epoch 780, training loss: 6.1435227394104 = 0.08218022435903549 + 1.0 * 6.061342716217041
Epoch 780, val loss: 0.6782000064849854
Epoch 790, training loss: 6.144556045532227 = 0.07820434123277664 + 1.0 * 6.066351890563965
Epoch 790, val loss: 0.6810563802719116
Epoch 800, training loss: 6.133605003356934 = 0.0745096206665039 + 1.0 * 6.05909538269043
Epoch 800, val loss: 0.6842089891433716
Epoch 810, training loss: 6.1293768882751465 = 0.07104168832302094 + 1.0 * 6.058335304260254
Epoch 810, val loss: 0.687496542930603
Epoch 820, training loss: 6.1374735832214355 = 0.06778062134981155 + 1.0 * 6.069693088531494
Epoch 820, val loss: 0.6907519698143005
Epoch 830, training loss: 6.124503135681152 = 0.06475723534822464 + 1.0 * 6.059745788574219
Epoch 830, val loss: 0.6941759586334229
Epoch 840, training loss: 6.118537425994873 = 0.061913494020700455 + 1.0 * 6.056623935699463
Epoch 840, val loss: 0.6977682709693909
Epoch 850, training loss: 6.114258289337158 = 0.05923372507095337 + 1.0 * 6.05502462387085
Epoch 850, val loss: 0.7013681530952454
Epoch 860, training loss: 6.115823268890381 = 0.05670700594782829 + 1.0 * 6.059116363525391
Epoch 860, val loss: 0.7050361037254333
Epoch 870, training loss: 6.1077728271484375 = 0.054323360323905945 + 1.0 * 6.053449630737305
Epoch 870, val loss: 0.7087106704711914
Epoch 880, training loss: 6.106762886047363 = 0.052091334015131 + 1.0 * 6.054671764373779
Epoch 880, val loss: 0.712611973285675
Epoch 890, training loss: 6.103849411010742 = 0.0499805249273777 + 1.0 * 6.053868770599365
Epoch 890, val loss: 0.716306209564209
Epoch 900, training loss: 6.097961902618408 = 0.04800183326005936 + 1.0 * 6.049960136413574
Epoch 900, val loss: 0.7202664613723755
Epoch 910, training loss: 6.095014572143555 = 0.04612842574715614 + 1.0 * 6.048886299133301
Epoch 910, val loss: 0.7242322564125061
Epoch 920, training loss: 6.094942569732666 = 0.0443444550037384 + 1.0 * 6.05059814453125
Epoch 920, val loss: 0.7281779646873474
Epoch 930, training loss: 6.092294692993164 = 0.042662348598241806 + 1.0 * 6.049632549285889
Epoch 930, val loss: 0.7319610714912415
Epoch 940, training loss: 6.090858459472656 = 0.041087571531534195 + 1.0 * 6.049770832061768
Epoch 940, val loss: 0.7360126972198486
Epoch 950, training loss: 6.088200569152832 = 0.03959459066390991 + 1.0 * 6.048605918884277
Epoch 950, val loss: 0.7399978637695312
Epoch 960, training loss: 6.085108757019043 = 0.038173601031303406 + 1.0 * 6.046935081481934
Epoch 960, val loss: 0.7438546419143677
Epoch 970, training loss: 6.082920551300049 = 0.036822739988565445 + 1.0 * 6.046097755432129
Epoch 970, val loss: 0.7478612661361694
Epoch 980, training loss: 6.081010818481445 = 0.035541824996471405 + 1.0 * 6.045468807220459
Epoch 980, val loss: 0.751862645149231
Epoch 990, training loss: 6.079433441162109 = 0.03432450443506241 + 1.0 * 6.045108795166016
Epoch 990, val loss: 0.755649745464325
Epoch 1000, training loss: 6.075952053070068 = 0.03317876160144806 + 1.0 * 6.042773246765137
Epoch 1000, val loss: 0.7597030401229858
Epoch 1010, training loss: 6.074589729309082 = 0.03208647668361664 + 1.0 * 6.042503356933594
Epoch 1010, val loss: 0.7636827826499939
Epoch 1020, training loss: 6.07260799407959 = 0.031038695946335793 + 1.0 * 6.041569232940674
Epoch 1020, val loss: 0.7676059007644653
Epoch 1030, training loss: 6.083772659301758 = 0.030034635215997696 + 1.0 * 6.053738117218018
Epoch 1030, val loss: 0.7714242935180664
Epoch 1040, training loss: 6.070268154144287 = 0.029095852747559547 + 1.0 * 6.041172504425049
Epoch 1040, val loss: 0.775306224822998
Epoch 1050, training loss: 6.068294525146484 = 0.02820291370153427 + 1.0 * 6.040091514587402
Epoch 1050, val loss: 0.7793348431587219
Epoch 1060, training loss: 6.067571640014648 = 0.02734246291220188 + 1.0 * 6.040229320526123
Epoch 1060, val loss: 0.78319251537323
Epoch 1070, training loss: 6.0665693283081055 = 0.026520125567913055 + 1.0 * 6.040049076080322
Epoch 1070, val loss: 0.7869206666946411
Epoch 1080, training loss: 6.067718982696533 = 0.02573613077402115 + 1.0 * 6.041982650756836
Epoch 1080, val loss: 0.7907195687294006
Epoch 1090, training loss: 6.061232566833496 = 0.024990124627947807 + 1.0 * 6.036242485046387
Epoch 1090, val loss: 0.7944678068161011
Epoch 1100, training loss: 6.06269645690918 = 0.02427823282778263 + 1.0 * 6.038418292999268
Epoch 1100, val loss: 0.7983120679855347
Epoch 1110, training loss: 6.064626216888428 = 0.02359119802713394 + 1.0 * 6.041035175323486
Epoch 1110, val loss: 0.8019092082977295
Epoch 1120, training loss: 6.059185981750488 = 0.022940143942832947 + 1.0 * 6.036245822906494
Epoch 1120, val loss: 0.8056751489639282
Epoch 1130, training loss: 6.056667804718018 = 0.022313283756375313 + 1.0 * 6.0343546867370605
Epoch 1130, val loss: 0.8094122409820557
Epoch 1140, training loss: 6.064560413360596 = 0.02170763723552227 + 1.0 * 6.042852878570557
Epoch 1140, val loss: 0.8130372762680054
Epoch 1150, training loss: 6.059755802154541 = 0.02112877182662487 + 1.0 * 6.0386271476745605
Epoch 1150, val loss: 0.8164874911308289
Epoch 1160, training loss: 6.054697036743164 = 0.020580748096108437 + 1.0 * 6.034116268157959
Epoch 1160, val loss: 0.8202499151229858
Epoch 1170, training loss: 6.052189826965332 = 0.020047659054398537 + 1.0 * 6.032142162322998
Epoch 1170, val loss: 0.8238007426261902
Epoch 1180, training loss: 6.066585063934326 = 0.019531970843672752 + 1.0 * 6.04705286026001
Epoch 1180, val loss: 0.8272053599357605
Epoch 1190, training loss: 6.0545172691345215 = 0.019046016037464142 + 1.0 * 6.035471439361572
Epoch 1190, val loss: 0.8306646943092346
Epoch 1200, training loss: 6.049500465393066 = 0.018577514216303825 + 1.0 * 6.030922889709473
Epoch 1200, val loss: 0.8342630863189697
Epoch 1210, training loss: 6.047601699829102 = 0.018123509362339973 + 1.0 * 6.029478073120117
Epoch 1210, val loss: 0.8377119302749634
Epoch 1220, training loss: 6.054530620574951 = 0.01768220216035843 + 1.0 * 6.036848545074463
Epoch 1220, val loss: 0.8410036563873291
Epoch 1230, training loss: 6.054332256317139 = 0.01726122386753559 + 1.0 * 6.037071228027344
Epoch 1230, val loss: 0.8443409204483032
Epoch 1240, training loss: 6.046085834503174 = 0.01685824617743492 + 1.0 * 6.0292277336120605
Epoch 1240, val loss: 0.8477312922477722
Epoch 1250, training loss: 6.044069766998291 = 0.01647064834833145 + 1.0 * 6.027599334716797
Epoch 1250, val loss: 0.8511191010475159
Epoch 1260, training loss: 6.0437211990356445 = 0.01609228551387787 + 1.0 * 6.0276288986206055
Epoch 1260, val loss: 0.8543633222579956
Epoch 1270, training loss: 6.047279357910156 = 0.015724966302514076 + 1.0 * 6.031554222106934
Epoch 1270, val loss: 0.8575317859649658
Epoch 1280, training loss: 6.043827533721924 = 0.015374766662716866 + 1.0 * 6.0284528732299805
Epoch 1280, val loss: 0.8608960509300232
Epoch 1290, training loss: 6.056829929351807 = 0.015035700984299183 + 1.0 * 6.041794300079346
Epoch 1290, val loss: 0.8640187978744507
Epoch 1300, training loss: 6.0409345626831055 = 0.014707724563777447 + 1.0 * 6.026226997375488
Epoch 1300, val loss: 0.8670594096183777
Epoch 1310, training loss: 6.03999662399292 = 0.014398058876395226 + 1.0 * 6.025598526000977
Epoch 1310, val loss: 0.8704032897949219
Epoch 1320, training loss: 6.039189338684082 = 0.014092820696532726 + 1.0 * 6.025096416473389
Epoch 1320, val loss: 0.8735170960426331
Epoch 1330, training loss: 6.046423435211182 = 0.013794546946883202 + 1.0 * 6.032629013061523
Epoch 1330, val loss: 0.8764901757240295
Epoch 1340, training loss: 6.042384624481201 = 0.013509448617696762 + 1.0 * 6.028875350952148
Epoch 1340, val loss: 0.8794575929641724
Epoch 1350, training loss: 6.041641712188721 = 0.01323514711111784 + 1.0 * 6.028406620025635
Epoch 1350, val loss: 0.8825611472129822
Epoch 1360, training loss: 6.035421848297119 = 0.012968185357749462 + 1.0 * 6.022453784942627
Epoch 1360, val loss: 0.8855499029159546
Epoch 1370, training loss: 6.036258697509766 = 0.012708637863397598 + 1.0 * 6.023550033569336
Epoch 1370, val loss: 0.8885817527770996
Epoch 1380, training loss: 6.0366291999816895 = 0.01245533674955368 + 1.0 * 6.024173736572266
Epoch 1380, val loss: 0.8914636373519897
Epoch 1390, training loss: 6.033986568450928 = 0.012209783308207989 + 1.0 * 6.0217766761779785
Epoch 1390, val loss: 0.8943924307823181
Epoch 1400, training loss: 6.045608043670654 = 0.011970958672463894 + 1.0 * 6.033637046813965
Epoch 1400, val loss: 0.8971083760261536
Epoch 1410, training loss: 6.033173084259033 = 0.011745624244213104 + 1.0 * 6.021427631378174
Epoch 1410, val loss: 0.8999766707420349
Epoch 1420, training loss: 6.031900405883789 = 0.01152772642672062 + 1.0 * 6.0203728675842285
Epoch 1420, val loss: 0.9030150771141052
Epoch 1430, training loss: 6.030943870544434 = 0.011311257258057594 + 1.0 * 6.019632816314697
Epoch 1430, val loss: 0.9058189392089844
Epoch 1440, training loss: 6.03851318359375 = 0.011100530624389648 + 1.0 * 6.0274128913879395
Epoch 1440, val loss: 0.9086178541183472
Epoch 1450, training loss: 6.034853458404541 = 0.010894265957176685 + 1.0 * 6.023959159851074
Epoch 1450, val loss: 0.9110581278800964
Epoch 1460, training loss: 6.031715393066406 = 0.010700573213398457 + 1.0 * 6.02101469039917
Epoch 1460, val loss: 0.9140180945396423
Epoch 1470, training loss: 6.028466701507568 = 0.010509572923183441 + 1.0 * 6.0179572105407715
Epoch 1470, val loss: 0.9168033003807068
Epoch 1480, training loss: 6.032331943511963 = 0.010321217589080334 + 1.0 * 6.022010803222656
Epoch 1480, val loss: 0.9194559454917908
Epoch 1490, training loss: 6.02891206741333 = 0.010139061138033867 + 1.0 * 6.018773078918457
Epoch 1490, val loss: 0.9220101237297058
Epoch 1500, training loss: 6.031509876251221 = 0.00996367447078228 + 1.0 * 6.021546363830566
Epoch 1500, val loss: 0.9246771931648254
Epoch 1510, training loss: 6.026783466339111 = 0.009793338365852833 + 1.0 * 6.0169901847839355
Epoch 1510, val loss: 0.9273807406425476
Epoch 1520, training loss: 6.026620864868164 = 0.009626016952097416 + 1.0 * 6.016994953155518
Epoch 1520, val loss: 0.9300358295440674
Epoch 1530, training loss: 6.0289483070373535 = 0.009461759589612484 + 1.0 * 6.019486427307129
Epoch 1530, val loss: 0.9325933456420898
Epoch 1540, training loss: 6.0259833335876465 = 0.009301253594458103 + 1.0 * 6.016682147979736
Epoch 1540, val loss: 0.9350013732910156
Epoch 1550, training loss: 6.031283855438232 = 0.009147947654128075 + 1.0 * 6.0221357345581055
Epoch 1550, val loss: 0.9377043843269348
Epoch 1560, training loss: 6.0247883796691895 = 0.008999035693705082 + 1.0 * 6.01578950881958
Epoch 1560, val loss: 0.9401205778121948
Epoch 1570, training loss: 6.024267673492432 = 0.008855420164763927 + 1.0 * 6.015412330627441
Epoch 1570, val loss: 0.9427799582481384
Epoch 1580, training loss: 6.023666858673096 = 0.008712674491107464 + 1.0 * 6.014954090118408
Epoch 1580, val loss: 0.9452859163284302
Epoch 1590, training loss: 6.026241779327393 = 0.008572258986532688 + 1.0 * 6.017669677734375
Epoch 1590, val loss: 0.9477009773254395
Epoch 1600, training loss: 6.023612022399902 = 0.008436048403382301 + 1.0 * 6.015175819396973
Epoch 1600, val loss: 0.9500802755355835
Epoch 1610, training loss: 6.0220489501953125 = 0.008304308168590069 + 1.0 * 6.013744831085205
Epoch 1610, val loss: 0.9526214599609375
Epoch 1620, training loss: 6.025047302246094 = 0.00817498005926609 + 1.0 * 6.016872406005859
Epoch 1620, val loss: 0.9550584554672241
Epoch 1630, training loss: 6.022477149963379 = 0.00804818607866764 + 1.0 * 6.014429092407227
Epoch 1630, val loss: 0.9573376774787903
Epoch 1640, training loss: 6.022006511688232 = 0.007926643826067448 + 1.0 * 6.014080047607422
Epoch 1640, val loss: 0.9597588181495667
Epoch 1650, training loss: 6.020874500274658 = 0.007807551883161068 + 1.0 * 6.01306676864624
Epoch 1650, val loss: 0.9622274041175842
Epoch 1660, training loss: 6.031041622161865 = 0.0076902033761143684 + 1.0 * 6.023351192474365
Epoch 1660, val loss: 0.9644566178321838
Epoch 1670, training loss: 6.022943019866943 = 0.007576040457934141 + 1.0 * 6.015367031097412
Epoch 1670, val loss: 0.9666498303413391
Epoch 1680, training loss: 6.020375728607178 = 0.007466490846127272 + 1.0 * 6.012909412384033
Epoch 1680, val loss: 0.9691612124443054
Epoch 1690, training loss: 6.025779724121094 = 0.007357963360846043 + 1.0 * 6.018421649932861
Epoch 1690, val loss: 0.9714159965515137
Epoch 1700, training loss: 6.019686698913574 = 0.007250883150845766 + 1.0 * 6.0124359130859375
Epoch 1700, val loss: 0.9735240936279297
Epoch 1710, training loss: 6.019660472869873 = 0.0071478113532066345 + 1.0 * 6.012512683868408
Epoch 1710, val loss: 0.9758849143981934
Epoch 1720, training loss: 6.021546840667725 = 0.007045653648674488 + 1.0 * 6.014501094818115
Epoch 1720, val loss: 0.9780789017677307
Epoch 1730, training loss: 6.02405309677124 = 0.006945725530385971 + 1.0 * 6.0171074867248535
Epoch 1730, val loss: 0.9802126288414001
Epoch 1740, training loss: 6.01655912399292 = 0.006849972996860743 + 1.0 * 6.009709358215332
Epoch 1740, val loss: 0.9823895692825317
Epoch 1750, training loss: 6.016632556915283 = 0.006756471004337072 + 1.0 * 6.009876251220703
Epoch 1750, val loss: 0.9847211837768555
Epoch 1760, training loss: 6.016030788421631 = 0.006662794854491949 + 1.0 * 6.009367942810059
Epoch 1760, val loss: 0.9868876338005066
Epoch 1770, training loss: 6.020333290100098 = 0.006570887751877308 + 1.0 * 6.013762474060059
Epoch 1770, val loss: 0.9890536665916443
Epoch 1780, training loss: 6.016331195831299 = 0.006480797193944454 + 1.0 * 6.00985050201416
Epoch 1780, val loss: 0.9909834265708923
Epoch 1790, training loss: 6.016899108886719 = 0.006393979769200087 + 1.0 * 6.010505199432373
Epoch 1790, val loss: 0.9932259917259216
Epoch 1800, training loss: 6.0200395584106445 = 0.006308882497251034 + 1.0 * 6.013730525970459
Epoch 1800, val loss: 0.9953004121780396
Epoch 1810, training loss: 6.018601894378662 = 0.006225437391549349 + 1.0 * 6.012376308441162
Epoch 1810, val loss: 0.9973313808441162
Epoch 1820, training loss: 6.016193866729736 = 0.006144743878394365 + 1.0 * 6.010049343109131
Epoch 1820, val loss: 0.9993992447853088
Epoch 1830, training loss: 6.015761852264404 = 0.006065278314054012 + 1.0 * 6.0096964836120605
Epoch 1830, val loss: 1.0015132427215576
Epoch 1840, training loss: 6.0127339363098145 = 0.005987176671624184 + 1.0 * 6.006746768951416
Epoch 1840, val loss: 1.0035700798034668
Epoch 1850, training loss: 6.014453887939453 = 0.005910095293074846 + 1.0 * 6.008543968200684
Epoch 1850, val loss: 1.0056440830230713
Epoch 1860, training loss: 6.019129753112793 = 0.005833999253809452 + 1.0 * 6.013295650482178
Epoch 1860, val loss: 1.0075379610061646
Epoch 1870, training loss: 6.018807888031006 = 0.005760395899415016 + 1.0 * 6.013047695159912
Epoch 1870, val loss: 1.0094324350357056
Epoch 1880, training loss: 6.014429569244385 = 0.005689272657036781 + 1.0 * 6.008740425109863
Epoch 1880, val loss: 1.0115280151367188
Epoch 1890, training loss: 6.014843463897705 = 0.005619552917778492 + 1.0 * 6.009223937988281
Epoch 1890, val loss: 1.0136350393295288
Epoch 1900, training loss: 6.013225078582764 = 0.0055498345755040646 + 1.0 * 6.0076751708984375
Epoch 1900, val loss: 1.015454888343811
Epoch 1910, training loss: 6.011713027954102 = 0.005481326021254063 + 1.0 * 6.006231784820557
Epoch 1910, val loss: 1.0173835754394531
Epoch 1920, training loss: 6.012550354003906 = 0.00541413389146328 + 1.0 * 6.007136344909668
Epoch 1920, val loss: 1.019340991973877
Epoch 1930, training loss: 6.020937919616699 = 0.005348443053662777 + 1.0 * 6.015589714050293
Epoch 1930, val loss: 1.0211762189865112
Epoch 1940, training loss: 6.013227462768555 = 0.005283854901790619 + 1.0 * 6.007943630218506
Epoch 1940, val loss: 1.0230131149291992
Epoch 1950, training loss: 6.013120174407959 = 0.005222338251769543 + 1.0 * 6.007897853851318
Epoch 1950, val loss: 1.0250425338745117
Epoch 1960, training loss: 6.011965274810791 = 0.005160650238394737 + 1.0 * 6.006804466247559
Epoch 1960, val loss: 1.0268793106079102
Epoch 1970, training loss: 6.016101360321045 = 0.005100564565509558 + 1.0 * 6.011000633239746
Epoch 1970, val loss: 1.028748631477356
Epoch 1980, training loss: 6.009059429168701 = 0.005040946416556835 + 1.0 * 6.004018306732178
Epoch 1980, val loss: 1.0304577350616455
Epoch 1990, training loss: 6.009809494018555 = 0.004983129911124706 + 1.0 * 6.004826545715332
Epoch 1990, val loss: 1.0324630737304688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.7528
Flip ASR: 0.7022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.311002731323242 = 1.937162160873413 + 1.0 * 8.37384033203125
Epoch 0, val loss: 1.9290728569030762
Epoch 10, training loss: 10.300888061523438 = 1.9275412559509277 + 1.0 * 8.373346328735352
Epoch 10, val loss: 1.9187703132629395
Epoch 20, training loss: 10.285725593566895 = 1.9157435894012451 + 1.0 * 8.36998176574707
Epoch 20, val loss: 1.9060192108154297
Epoch 30, training loss: 10.246115684509277 = 1.8996801376342773 + 1.0 * 8.346435546875
Epoch 30, val loss: 1.8887711763381958
Epoch 40, training loss: 10.028569221496582 = 1.8798459768295288 + 1.0 * 8.148723602294922
Epoch 40, val loss: 1.8680946826934814
Epoch 50, training loss: 9.132079124450684 = 1.859448790550232 + 1.0 * 7.272630214691162
Epoch 50, val loss: 1.8465871810913086
Epoch 60, training loss: 8.9270658493042 = 1.84187912940979 + 1.0 * 7.085186958312988
Epoch 60, val loss: 1.829540491104126
Epoch 70, training loss: 8.680953025817871 = 1.8272846937179565 + 1.0 * 6.853668689727783
Epoch 70, val loss: 1.8156131505966187
Epoch 80, training loss: 8.503482818603516 = 1.812612533569336 + 1.0 * 6.690870761871338
Epoch 80, val loss: 1.8013607263565063
Epoch 90, training loss: 8.369608879089355 = 1.7992265224456787 + 1.0 * 6.570382118225098
Epoch 90, val loss: 1.7879903316497803
Epoch 100, training loss: 8.279500961303711 = 1.7861008644104004 + 1.0 * 6.493399620056152
Epoch 100, val loss: 1.7746638059616089
Epoch 110, training loss: 8.206419944763184 = 1.773106575012207 + 1.0 * 6.433313369750977
Epoch 110, val loss: 1.761801838874817
Epoch 120, training loss: 8.139382362365723 = 1.759859323501587 + 1.0 * 6.379523277282715
Epoch 120, val loss: 1.7491676807403564
Epoch 130, training loss: 8.081808090209961 = 1.7460718154907227 + 1.0 * 6.335736274719238
Epoch 130, val loss: 1.7364617586135864
Epoch 140, training loss: 8.03421401977539 = 1.7305831909179688 + 1.0 * 6.303630352020264
Epoch 140, val loss: 1.7226221561431885
Epoch 150, training loss: 7.993221282958984 = 1.7123401165008545 + 1.0 * 6.280880928039551
Epoch 150, val loss: 1.7066508531570435
Epoch 160, training loss: 7.952364921569824 = 1.6903836727142334 + 1.0 * 6.261981010437012
Epoch 160, val loss: 1.6879544258117676
Epoch 170, training loss: 7.910274982452393 = 1.6638407707214355 + 1.0 * 6.246434211730957
Epoch 170, val loss: 1.6658233404159546
Epoch 180, training loss: 7.8638529777526855 = 1.6314724683761597 + 1.0 * 6.232380390167236
Epoch 180, val loss: 1.6391180753707886
Epoch 190, training loss: 7.812560081481934 = 1.5918065309524536 + 1.0 * 6.2207536697387695
Epoch 190, val loss: 1.6067785024642944
Epoch 200, training loss: 7.759528636932373 = 1.5456773042678833 + 1.0 * 6.213851451873779
Epoch 200, val loss: 1.5699018239974976
Epoch 210, training loss: 7.6979660987854 = 1.4936646223068237 + 1.0 * 6.204301357269287
Epoch 210, val loss: 1.5286591053009033
Epoch 220, training loss: 7.633257865905762 = 1.436059832572937 + 1.0 * 6.197197914123535
Epoch 220, val loss: 1.4834282398223877
Epoch 230, training loss: 7.565817832946777 = 1.3744977712631226 + 1.0 * 6.191319942474365
Epoch 230, val loss: 1.4357789754867554
Epoch 240, training loss: 7.501607894897461 = 1.3110487461090088 + 1.0 * 6.190558910369873
Epoch 240, val loss: 1.3876173496246338
Epoch 250, training loss: 7.432875633239746 = 1.2493131160736084 + 1.0 * 6.183562755584717
Epoch 250, val loss: 1.342189908027649
Epoch 260, training loss: 7.365199089050293 = 1.189682126045227 + 1.0 * 6.1755170822143555
Epoch 260, val loss: 1.2987538576126099
Epoch 270, training loss: 7.301109313964844 = 1.1315255165100098 + 1.0 * 6.169583797454834
Epoch 270, val loss: 1.2562676668167114
Epoch 280, training loss: 7.24221658706665 = 1.07563054561615 + 1.0 * 6.166585922241211
Epoch 280, val loss: 1.2156028747558594
Epoch 290, training loss: 7.184102535247803 = 1.0232410430908203 + 1.0 * 6.160861492156982
Epoch 290, val loss: 1.1771208047866821
Epoch 300, training loss: 7.128199577331543 = 0.9730442762374878 + 1.0 * 6.155155181884766
Epoch 300, val loss: 1.1398236751556396
Epoch 310, training loss: 7.075003623962402 = 0.9245190024375916 + 1.0 * 6.150484561920166
Epoch 310, val loss: 1.103358507156372
Epoch 320, training loss: 7.031092643737793 = 0.8783116936683655 + 1.0 * 6.152781009674072
Epoch 320, val loss: 1.0683199167251587
Epoch 330, training loss: 6.979795932769775 = 0.8353279232978821 + 1.0 * 6.144467830657959
Epoch 330, val loss: 1.035638451576233
Epoch 340, training loss: 6.933117389678955 = 0.7946183085441589 + 1.0 * 6.1384992599487305
Epoch 340, val loss: 1.004744529724121
Epoch 350, training loss: 6.891140937805176 = 0.7562422156333923 + 1.0 * 6.134898662567139
Epoch 350, val loss: 0.9761589765548706
Epoch 360, training loss: 6.853455543518066 = 0.720556914806366 + 1.0 * 6.132898807525635
Epoch 360, val loss: 0.9505636692047119
Epoch 370, training loss: 6.816760063171387 = 0.6879518628120422 + 1.0 * 6.12880802154541
Epoch 370, val loss: 0.9283007979393005
Epoch 380, training loss: 6.782910346984863 = 0.657592236995697 + 1.0 * 6.1253180503845215
Epoch 380, val loss: 0.9089600443840027
Epoch 390, training loss: 6.752150058746338 = 0.6291505694389343 + 1.0 * 6.122999668121338
Epoch 390, val loss: 0.8922839164733887
Epoch 400, training loss: 6.7228899002075195 = 0.6025216579437256 + 1.0 * 6.120368480682373
Epoch 400, val loss: 0.878212034702301
Epoch 410, training loss: 6.694495677947998 = 0.577113926410675 + 1.0 * 6.117381572723389
Epoch 410, val loss: 0.8661637306213379
Epoch 420, training loss: 6.674485683441162 = 0.552635133266449 + 1.0 * 6.121850490570068
Epoch 420, val loss: 0.8557224273681641
Epoch 430, training loss: 6.643648624420166 = 0.5291866660118103 + 1.0 * 6.114461898803711
Epoch 430, val loss: 0.8467777967453003
Epoch 440, training loss: 6.630611419677734 = 0.5065624713897705 + 1.0 * 6.124049186706543
Epoch 440, val loss: 0.8391658067703247
Epoch 450, training loss: 6.596560955047607 = 0.4848315417766571 + 1.0 * 6.111729621887207
Epoch 450, val loss: 0.8325362801551819
Epoch 460, training loss: 6.570619106292725 = 0.4636252224445343 + 1.0 * 6.106993675231934
Epoch 460, val loss: 0.8269498944282532
Epoch 470, training loss: 6.54762077331543 = 0.4427819848060608 + 1.0 * 6.104838848114014
Epoch 470, val loss: 0.8221299648284912
Epoch 480, training loss: 6.527924537658691 = 0.4223768711090088 + 1.0 * 6.105547904968262
Epoch 480, val loss: 0.8179929256439209
Epoch 490, training loss: 6.506754398345947 = 0.40258586406707764 + 1.0 * 6.10416841506958
Epoch 490, val loss: 0.8146400451660156
Epoch 500, training loss: 6.4867262840271 = 0.38326650857925415 + 1.0 * 6.10345983505249
Epoch 500, val loss: 0.8119632005691528
Epoch 510, training loss: 6.465146541595459 = 0.36451205611228943 + 1.0 * 6.100634574890137
Epoch 510, val loss: 0.8097716569900513
Epoch 520, training loss: 6.443489074707031 = 0.3463655114173889 + 1.0 * 6.097123622894287
Epoch 520, val loss: 0.8081836700439453
Epoch 530, training loss: 6.4229631423950195 = 0.3287455141544342 + 1.0 * 6.094217777252197
Epoch 530, val loss: 0.8071041703224182
Epoch 540, training loss: 6.4141669273376465 = 0.31166279315948486 + 1.0 * 6.102504253387451
Epoch 540, val loss: 0.8064939379692078
Epoch 550, training loss: 6.390427589416504 = 0.2954288721084595 + 1.0 * 6.094998836517334
Epoch 550, val loss: 0.806129515171051
Epoch 560, training loss: 6.370604515075684 = 0.27995070815086365 + 1.0 * 6.090653896331787
Epoch 560, val loss: 0.8063527345657349
Epoch 570, training loss: 6.353553771972656 = 0.26508310437202454 + 1.0 * 6.088470458984375
Epoch 570, val loss: 0.8070549368858337
Epoch 580, training loss: 6.339481830596924 = 0.25082677602767944 + 1.0 * 6.0886549949646
Epoch 580, val loss: 0.8082669973373413
Epoch 590, training loss: 6.326706886291504 = 0.2372814416885376 + 1.0 * 6.089425563812256
Epoch 590, val loss: 0.8098134398460388
Epoch 600, training loss: 6.308884143829346 = 0.22450317442417145 + 1.0 * 6.084381103515625
Epoch 600, val loss: 0.8119081854820251
Epoch 610, training loss: 6.296796798706055 = 0.2124050259590149 + 1.0 * 6.0843915939331055
Epoch 610, val loss: 0.8145354986190796
Epoch 620, training loss: 6.285374164581299 = 0.20101919770240784 + 1.0 * 6.084354877471924
Epoch 620, val loss: 0.8175336122512817
Epoch 630, training loss: 6.273380279541016 = 0.1903882473707199 + 1.0 * 6.082992076873779
Epoch 630, val loss: 0.8209800720214844
Epoch 640, training loss: 6.260656833648682 = 0.1804763525724411 + 1.0 * 6.080180644989014
Epoch 640, val loss: 0.8249443769454956
Epoch 650, training loss: 6.254538536071777 = 0.1712227463722229 + 1.0 * 6.083315849304199
Epoch 650, val loss: 0.8293259739875793
Epoch 660, training loss: 6.240135669708252 = 0.16262032091617584 + 1.0 * 6.077515125274658
Epoch 660, val loss: 0.8341046571731567
Epoch 670, training loss: 6.2333455085754395 = 0.1546412706375122 + 1.0 * 6.078704357147217
Epoch 670, val loss: 0.8392930030822754
Epoch 680, training loss: 6.225664138793945 = 0.14724676311016083 + 1.0 * 6.0784173011779785
Epoch 680, val loss: 0.8447664380073547
Epoch 690, training loss: 6.214537143707275 = 0.140391543507576 + 1.0 * 6.074145793914795
Epoch 690, val loss: 0.8504367470741272
Epoch 700, training loss: 6.207891941070557 = 0.1340109407901764 + 1.0 * 6.073881149291992
Epoch 700, val loss: 0.8563829064369202
Epoch 710, training loss: 6.198768138885498 = 0.12808313965797424 + 1.0 * 6.070684909820557
Epoch 710, val loss: 0.862400472164154
Epoch 720, training loss: 6.19745397567749 = 0.12260515242815018 + 1.0 * 6.074848651885986
Epoch 720, val loss: 0.8686591982841492
Epoch 730, training loss: 6.1877946853637695 = 0.11749928444623947 + 1.0 * 6.070295333862305
Epoch 730, val loss: 0.8750045895576477
Epoch 740, training loss: 6.180919170379639 = 0.11273619532585144 + 1.0 * 6.068182945251465
Epoch 740, val loss: 0.8815191984176636
Epoch 750, training loss: 6.17570686340332 = 0.10825412720441818 + 1.0 * 6.067452907562256
Epoch 750, val loss: 0.8881486654281616
Epoch 760, training loss: 6.1716461181640625 = 0.10404682159423828 + 1.0 * 6.067599296569824
Epoch 760, val loss: 0.8948354125022888
Epoch 770, training loss: 6.170943737030029 = 0.10009507834911346 + 1.0 * 6.07084846496582
Epoch 770, val loss: 0.9015189409255981
Epoch 780, training loss: 6.161895275115967 = 0.09639409184455872 + 1.0 * 6.0655012130737305
Epoch 780, val loss: 0.9082010984420776
Epoch 790, training loss: 6.159459590911865 = 0.09292663633823395 + 1.0 * 6.066533088684082
Epoch 790, val loss: 0.9148547053337097
Epoch 800, training loss: 6.151137828826904 = 0.08965980261564255 + 1.0 * 6.061478137969971
Epoch 800, val loss: 0.9214528799057007
Epoch 810, training loss: 6.147382736206055 = 0.08658551424741745 + 1.0 * 6.060797214508057
Epoch 810, val loss: 0.9280755519866943
Epoch 820, training loss: 6.145303249359131 = 0.08366416394710541 + 1.0 * 6.061639308929443
Epoch 820, val loss: 0.9346950054168701
Epoch 830, training loss: 6.142385959625244 = 0.08090010285377502 + 1.0 * 6.061485767364502
Epoch 830, val loss: 0.9412716031074524
Epoch 840, training loss: 6.136347770690918 = 0.07828535884618759 + 1.0 * 6.058062553405762
Epoch 840, val loss: 0.9477872252464294
Epoch 850, training loss: 6.133641242980957 = 0.07580889761447906 + 1.0 * 6.05783224105835
Epoch 850, val loss: 0.9542585611343384
Epoch 860, training loss: 6.130468845367432 = 0.07346130162477493 + 1.0 * 6.057007312774658
Epoch 860, val loss: 0.9607089757919312
Epoch 870, training loss: 6.130224227905273 = 0.07122541218996048 + 1.0 * 6.0589985847473145
Epoch 870, val loss: 0.9670904278755188
Epoch 880, training loss: 6.125923156738281 = 0.06910169124603271 + 1.0 * 6.056821346282959
Epoch 880, val loss: 0.9734330773353577
Epoch 890, training loss: 6.12008810043335 = 0.06708001345396042 + 1.0 * 6.053008079528809
Epoch 890, val loss: 0.9797194004058838
Epoch 900, training loss: 6.118379592895508 = 0.06515202671289444 + 1.0 * 6.053227424621582
Epoch 900, val loss: 0.9859700798988342
Epoch 910, training loss: 6.124386310577393 = 0.06330450624227524 + 1.0 * 6.061081886291504
Epoch 910, val loss: 0.9921974539756775
Epoch 920, training loss: 6.116186141967773 = 0.06154199317097664 + 1.0 * 6.0546441078186035
Epoch 920, val loss: 0.998271107673645
Epoch 930, training loss: 6.111288070678711 = 0.05986388400197029 + 1.0 * 6.051424026489258
Epoch 930, val loss: 1.0043927431106567
Epoch 940, training loss: 6.109730243682861 = 0.058251313865184784 + 1.0 * 6.051478862762451
Epoch 940, val loss: 1.0104416608810425
Epoch 950, training loss: 6.108790397644043 = 0.05670381709933281 + 1.0 * 6.052086353302002
Epoch 950, val loss: 1.0163934230804443
Epoch 960, training loss: 6.102477550506592 = 0.055227600038051605 + 1.0 * 6.047249794006348
Epoch 960, val loss: 1.0223528146743774
Epoch 970, training loss: 6.101414680480957 = 0.05381632223725319 + 1.0 * 6.047598361968994
Epoch 970, val loss: 1.0282825231552124
Epoch 980, training loss: 6.109760284423828 = 0.05245325341820717 + 1.0 * 6.057307243347168
Epoch 980, val loss: 1.0341370105743408
Epoch 990, training loss: 6.099964618682861 = 0.05114363878965378 + 1.0 * 6.048820972442627
Epoch 990, val loss: 1.039829969406128
Epoch 1000, training loss: 6.0941877365112305 = 0.04989241808652878 + 1.0 * 6.044295310974121
Epoch 1000, val loss: 1.0456162691116333
Epoch 1010, training loss: 6.093271255493164 = 0.04867808520793915 + 1.0 * 6.044593334197998
Epoch 1010, val loss: 1.0513241291046143
Epoch 1020, training loss: 6.101665019989014 = 0.0475008450448513 + 1.0 * 6.054164409637451
Epoch 1020, val loss: 1.056944727897644
Epoch 1030, training loss: 6.092576026916504 = 0.04636809974908829 + 1.0 * 6.046207904815674
Epoch 1030, val loss: 1.0625046491622925
Epoch 1040, training loss: 6.087286949157715 = 0.04528471454977989 + 1.0 * 6.042002201080322
Epoch 1040, val loss: 1.068055272102356
Epoch 1050, training loss: 6.085565567016602 = 0.044229816645383835 + 1.0 * 6.041335582733154
Epoch 1050, val loss: 1.0735584497451782
Epoch 1060, training loss: 6.089710235595703 = 0.043205346912145615 + 1.0 * 6.046504974365234
Epoch 1060, val loss: 1.0790050029754639
Epoch 1070, training loss: 6.086338996887207 = 0.0422137975692749 + 1.0 * 6.044125080108643
Epoch 1070, val loss: 1.0844142436981201
Epoch 1080, training loss: 6.084167957305908 = 0.04125620424747467 + 1.0 * 6.042911529541016
Epoch 1080, val loss: 1.089763879776001
Epoch 1090, training loss: 6.079974174499512 = 0.04033061861991882 + 1.0 * 6.03964376449585
Epoch 1090, val loss: 1.0951379537582397
Epoch 1100, training loss: 6.080723285675049 = 0.039429355412721634 + 1.0 * 6.041294097900391
Epoch 1100, val loss: 1.100447177886963
Epoch 1110, training loss: 6.078322887420654 = 0.03854936361312866 + 1.0 * 6.039773464202881
Epoch 1110, val loss: 1.1056808233261108
Epoch 1120, training loss: 6.082927227020264 = 0.03769547492265701 + 1.0 * 6.045231819152832
Epoch 1120, val loss: 1.1108644008636475
Epoch 1130, training loss: 6.0756096839904785 = 0.0368649922311306 + 1.0 * 6.038744926452637
Epoch 1130, val loss: 1.116021752357483
Epoch 1140, training loss: 6.077816009521484 = 0.03605830669403076 + 1.0 * 6.041757583618164
Epoch 1140, val loss: 1.1211717128753662
Epoch 1150, training loss: 6.073812484741211 = 0.03526721149682999 + 1.0 * 6.03854513168335
Epoch 1150, val loss: 1.1260441541671753
Epoch 1160, training loss: 6.072096347808838 = 0.03450854495167732 + 1.0 * 6.037587642669678
Epoch 1160, val loss: 1.1310657262802124
Epoch 1170, training loss: 6.067775726318359 = 0.0337643027305603 + 1.0 * 6.034011363983154
Epoch 1170, val loss: 1.13605797290802
Epoch 1180, training loss: 6.066242694854736 = 0.03303006291389465 + 1.0 * 6.033212661743164
Epoch 1180, val loss: 1.1410090923309326
Epoch 1190, training loss: 6.069370269775391 = 0.032307080924510956 + 1.0 * 6.037063121795654
Epoch 1190, val loss: 1.1459499597549438
Epoch 1200, training loss: 6.064277648925781 = 0.03159593418240547 + 1.0 * 6.032681941986084
Epoch 1200, val loss: 1.1508203744888306
Epoch 1210, training loss: 6.065216064453125 = 0.030908197164535522 + 1.0 * 6.034307956695557
Epoch 1210, val loss: 1.155707597732544
Epoch 1220, training loss: 6.070151329040527 = 0.030231762677431107 + 1.0 * 6.039919376373291
Epoch 1220, val loss: 1.1605980396270752
Epoch 1230, training loss: 6.0622124671936035 = 0.029568130150437355 + 1.0 * 6.032644271850586
Epoch 1230, val loss: 1.1653871536254883
Epoch 1240, training loss: 6.059701442718506 = 0.02891778014600277 + 1.0 * 6.030783653259277
Epoch 1240, val loss: 1.1702642440795898
Epoch 1250, training loss: 6.059961318969727 = 0.02827886864542961 + 1.0 * 6.03168249130249
Epoch 1250, val loss: 1.1751540899276733
Epoch 1260, training loss: 6.061215877532959 = 0.02765110321342945 + 1.0 * 6.033564567565918
Epoch 1260, val loss: 1.179997444152832
Epoch 1270, training loss: 6.060594081878662 = 0.027041207998991013 + 1.0 * 6.033552646636963
Epoch 1270, val loss: 1.1849098205566406
Epoch 1280, training loss: 6.055450439453125 = 0.026448799297213554 + 1.0 * 6.029001712799072
Epoch 1280, val loss: 1.1898202896118164
Epoch 1290, training loss: 6.057092189788818 = 0.02587311342358589 + 1.0 * 6.031219005584717
Epoch 1290, val loss: 1.1947294473648071
Epoch 1300, training loss: 6.064153671264648 = 0.025310438126325607 + 1.0 * 6.038843154907227
Epoch 1300, val loss: 1.1995368003845215
Epoch 1310, training loss: 6.05496883392334 = 0.024771466851234436 + 1.0 * 6.0301971435546875
Epoch 1310, val loss: 1.204435110092163
Epoch 1320, training loss: 6.052321910858154 = 0.024254899471998215 + 1.0 * 6.028067111968994
Epoch 1320, val loss: 1.2094427347183228
Epoch 1330, training loss: 6.050465106964111 = 0.023752151057124138 + 1.0 * 6.026712894439697
Epoch 1330, val loss: 1.214460849761963
Epoch 1340, training loss: 6.0588908195495605 = 0.02326357364654541 + 1.0 * 6.035627365112305
Epoch 1340, val loss: 1.2194575071334839
Epoch 1350, training loss: 6.051746368408203 = 0.022794542834162712 + 1.0 * 6.028951644897461
Epoch 1350, val loss: 1.2244404554367065
Epoch 1360, training loss: 6.049861431121826 = 0.02234679087996483 + 1.0 * 6.027514457702637
Epoch 1360, val loss: 1.2294905185699463
Epoch 1370, training loss: 6.04786491394043 = 0.02191532775759697 + 1.0 * 6.025949478149414
Epoch 1370, val loss: 1.2345887422561646
Epoch 1380, training loss: 6.053862571716309 = 0.021495159715414047 + 1.0 * 6.03236722946167
Epoch 1380, val loss: 1.239622950553894
Epoch 1390, training loss: 6.047464370727539 = 0.021087920293211937 + 1.0 * 6.026376247406006
Epoch 1390, val loss: 1.2444239854812622
Epoch 1400, training loss: 6.048241138458252 = 0.02070075273513794 + 1.0 * 6.02754020690918
Epoch 1400, val loss: 1.2494415044784546
Epoch 1410, training loss: 6.044457912445068 = 0.020325854420661926 + 1.0 * 6.024132251739502
Epoch 1410, val loss: 1.2543673515319824
Epoch 1420, training loss: 6.04360294342041 = 0.019961636513471603 + 1.0 * 6.023641109466553
Epoch 1420, val loss: 1.2593120336532593
Epoch 1430, training loss: 6.058841228485107 = 0.0196077860891819 + 1.0 * 6.039233207702637
Epoch 1430, val loss: 1.2641701698303223
Epoch 1440, training loss: 6.044734477996826 = 0.019265132024884224 + 1.0 * 6.0254693031311035
Epoch 1440, val loss: 1.2688688039779663
Epoch 1450, training loss: 6.041152000427246 = 0.018939368426799774 + 1.0 * 6.022212505340576
Epoch 1450, val loss: 1.2737722396850586
Epoch 1460, training loss: 6.040811538696289 = 0.018621332943439484 + 1.0 * 6.022190093994141
Epoch 1460, val loss: 1.278581976890564
Epoch 1470, training loss: 6.041649341583252 = 0.018310319632291794 + 1.0 * 6.023338794708252
Epoch 1470, val loss: 1.283335566520691
Epoch 1480, training loss: 6.04373025894165 = 0.018006255850195885 + 1.0 * 6.025723934173584
Epoch 1480, val loss: 1.2879266738891602
Epoch 1490, training loss: 6.040375232696533 = 0.01771598309278488 + 1.0 * 6.0226593017578125
Epoch 1490, val loss: 1.292619228363037
Epoch 1500, training loss: 6.0412797927856445 = 0.017436008900403976 + 1.0 * 6.023843765258789
Epoch 1500, val loss: 1.297267198562622
Epoch 1510, training loss: 6.039689540863037 = 0.017161905765533447 + 1.0 * 6.022527694702148
Epoch 1510, val loss: 1.3018449544906616
Epoch 1520, training loss: 6.036583423614502 = 0.016894718632102013 + 1.0 * 6.019688606262207
Epoch 1520, val loss: 1.3064279556274414
Epoch 1530, training loss: 6.043487071990967 = 0.016635308042168617 + 1.0 * 6.026851654052734
Epoch 1530, val loss: 1.3110430240631104
Epoch 1540, training loss: 6.036798000335693 = 0.016379360109567642 + 1.0 * 6.020418643951416
Epoch 1540, val loss: 1.3153347969055176
Epoch 1550, training loss: 6.036159038543701 = 0.016135703772306442 + 1.0 * 6.020023345947266
Epoch 1550, val loss: 1.3198446035385132
Epoch 1560, training loss: 6.035794258117676 = 0.01589721441268921 + 1.0 * 6.019896984100342
Epoch 1560, val loss: 1.3243166208267212
Epoch 1570, training loss: 6.0469160079956055 = 0.015662752091884613 + 1.0 * 6.031253337860107
Epoch 1570, val loss: 1.3286235332489014
Epoch 1580, training loss: 6.037375450134277 = 0.015434863977134228 + 1.0 * 6.0219407081604
Epoch 1580, val loss: 1.3329648971557617
Epoch 1590, training loss: 6.033904075622559 = 0.015215983614325523 + 1.0 * 6.018688201904297
Epoch 1590, val loss: 1.3372992277145386
Epoch 1600, training loss: 6.033095836639404 = 0.014999409206211567 + 1.0 * 6.018096446990967
Epoch 1600, val loss: 1.3416028022766113
Epoch 1610, training loss: 6.0394287109375 = 0.014786065556108952 + 1.0 * 6.024642467498779
Epoch 1610, val loss: 1.3457889556884766
Epoch 1620, training loss: 6.031752586364746 = 0.014578384347259998 + 1.0 * 6.017174243927002
Epoch 1620, val loss: 1.3500123023986816
Epoch 1630, training loss: 6.032422065734863 = 0.014377124607563019 + 1.0 * 6.018044948577881
Epoch 1630, val loss: 1.3542778491973877
Epoch 1640, training loss: 6.033183574676514 = 0.014178566634654999 + 1.0 * 6.019004821777344
Epoch 1640, val loss: 1.3584753274917603
Epoch 1650, training loss: 6.034709930419922 = 0.013984373770654202 + 1.0 * 6.020725727081299
Epoch 1650, val loss: 1.3625487089157104
Epoch 1660, training loss: 6.030904293060303 = 0.013796009123325348 + 1.0 * 6.01710844039917
Epoch 1660, val loss: 1.3665872812271118
Epoch 1670, training loss: 6.028532028198242 = 0.013612618669867516 + 1.0 * 6.014919281005859
Epoch 1670, val loss: 1.3707250356674194
Epoch 1680, training loss: 6.027892112731934 = 0.013431644067168236 + 1.0 * 6.014460563659668
Epoch 1680, val loss: 1.374814748764038
Epoch 1690, training loss: 6.02952241897583 = 0.013252804055809975 + 1.0 * 6.016269683837891
Epoch 1690, val loss: 1.378861665725708
Epoch 1700, training loss: 6.033032417297363 = 0.013076887466013432 + 1.0 * 6.019955635070801
Epoch 1700, val loss: 1.382753610610962
Epoch 1710, training loss: 6.031282424926758 = 0.012907070107758045 + 1.0 * 6.018375396728516
Epoch 1710, val loss: 1.386687159538269
Epoch 1720, training loss: 6.029160499572754 = 0.012742992490530014 + 1.0 * 6.016417503356934
Epoch 1720, val loss: 1.3907310962677002
Epoch 1730, training loss: 6.032637119293213 = 0.012581334449350834 + 1.0 * 6.020055770874023
Epoch 1730, val loss: 1.3946152925491333
Epoch 1740, training loss: 6.02644681930542 = 0.012421863153576851 + 1.0 * 6.01402473449707
Epoch 1740, val loss: 1.3983778953552246
Epoch 1750, training loss: 6.0268330574035645 = 0.01226793136447668 + 1.0 * 6.0145649909973145
Epoch 1750, val loss: 1.4023096561431885
Epoch 1760, training loss: 6.024694442749023 = 0.012115719728171825 + 1.0 * 6.01257848739624
Epoch 1760, val loss: 1.4062013626098633
Epoch 1770, training loss: 6.025856018066406 = 0.011964945122599602 + 1.0 * 6.013891220092773
Epoch 1770, val loss: 1.4100899696350098
Epoch 1780, training loss: 6.0319437980651855 = 0.011816207319498062 + 1.0 * 6.020127773284912
Epoch 1780, val loss: 1.4138717651367188
Epoch 1790, training loss: 6.029079437255859 = 0.011671158485114574 + 1.0 * 6.01740837097168
Epoch 1790, val loss: 1.417609691619873
Epoch 1800, training loss: 6.026770114898682 = 0.01153003703802824 + 1.0 * 6.01524019241333
Epoch 1800, val loss: 1.421378493309021
Epoch 1810, training loss: 6.0262556076049805 = 0.011391021311283112 + 1.0 * 6.014864444732666
Epoch 1810, val loss: 1.425100564956665
Epoch 1820, training loss: 6.023151874542236 = 0.011254864744842052 + 1.0 * 6.011897087097168
Epoch 1820, val loss: 1.4289084672927856
Epoch 1830, training loss: 6.022974967956543 = 0.011119809933006763 + 1.0 * 6.011855125427246
Epoch 1830, val loss: 1.4326171875
Epoch 1840, training loss: 6.036027431488037 = 0.010986757464706898 + 1.0 * 6.025040626525879
Epoch 1840, val loss: 1.436252236366272
Epoch 1850, training loss: 6.02715539932251 = 0.010854675434529781 + 1.0 * 6.016300678253174
Epoch 1850, val loss: 1.4397225379943848
Epoch 1860, training loss: 6.02287483215332 = 0.010728714056313038 + 1.0 * 6.01214599609375
Epoch 1860, val loss: 1.4434646368026733
Epoch 1870, training loss: 6.021265029907227 = 0.010602578520774841 + 1.0 * 6.01066255569458
Epoch 1870, val loss: 1.447084903717041
Epoch 1880, training loss: 6.027659893035889 = 0.010477215051651001 + 1.0 * 6.01718282699585
Epoch 1880, val loss: 1.4506111145019531
Epoch 1890, training loss: 6.023056983947754 = 0.01035381481051445 + 1.0 * 6.012702941894531
Epoch 1890, val loss: 1.4541376829147339
Epoch 1900, training loss: 6.0223388671875 = 0.010233026929199696 + 1.0 * 6.012105941772461
Epoch 1900, val loss: 1.45769464969635
Epoch 1910, training loss: 6.02032995223999 = 0.01011379063129425 + 1.0 * 6.010216236114502
Epoch 1910, val loss: 1.4612106084823608
Epoch 1920, training loss: 6.024137496948242 = 0.009996356442570686 + 1.0 * 6.014141082763672
Epoch 1920, val loss: 1.4647514820098877
Epoch 1930, training loss: 6.019587993621826 = 0.009879671968519688 + 1.0 * 6.009708404541016
Epoch 1930, val loss: 1.4681494235992432
Epoch 1940, training loss: 6.019361972808838 = 0.009765205904841423 + 1.0 * 6.009596824645996
Epoch 1940, val loss: 1.4716438055038452
Epoch 1950, training loss: 6.020815372467041 = 0.009652065113186836 + 1.0 * 6.011163234710693
Epoch 1950, val loss: 1.4751033782958984
Epoch 1960, training loss: 6.0217061042785645 = 0.009539585560560226 + 1.0 * 6.012166500091553
Epoch 1960, val loss: 1.4783639907836914
Epoch 1970, training loss: 6.017000198364258 = 0.009431458078324795 + 1.0 * 6.007568836212158
Epoch 1970, val loss: 1.4817771911621094
Epoch 1980, training loss: 6.016580581665039 = 0.009324896149337292 + 1.0 * 6.007255554199219
Epoch 1980, val loss: 1.4852631092071533
Epoch 1990, training loss: 6.016892433166504 = 0.00921703316271305 + 1.0 * 6.0076751708984375
Epoch 1990, val loss: 1.488591194152832
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.4428
Flip ASR: 0.4089/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.319247245788574 = 1.9456889629364014 + 1.0 * 8.373558044433594
Epoch 0, val loss: 1.9479119777679443
Epoch 10, training loss: 10.30367374420166 = 1.9339178800582886 + 1.0 * 8.369755744934082
Epoch 10, val loss: 1.9330459833145142
Epoch 20, training loss: 10.284826278686523 = 1.9190666675567627 + 1.0 * 8.36575984954834
Epoch 20, val loss: 1.9128291606903076
Epoch 30, training loss: 10.253652572631836 = 1.900356411933899 + 1.0 * 8.353296279907227
Epoch 30, val loss: 1.888046145439148
Epoch 40, training loss: 10.124313354492188 = 1.8801754713058472 + 1.0 * 8.24413776397705
Epoch 40, val loss: 1.8641897439956665
Epoch 50, training loss: 9.473125457763672 = 1.862691879272461 + 1.0 * 7.610433101654053
Epoch 50, val loss: 1.8454560041427612
Epoch 60, training loss: 8.973553657531738 = 1.8452236652374268 + 1.0 * 7.128330230712891
Epoch 60, val loss: 1.8284534215927124
Epoch 70, training loss: 8.661614418029785 = 1.8283308744430542 + 1.0 * 6.8332839012146
Epoch 70, val loss: 1.8124116659164429
Epoch 80, training loss: 8.503769874572754 = 1.8142822980880737 + 1.0 * 6.689487934112549
Epoch 80, val loss: 1.7989250421524048
Epoch 90, training loss: 8.390933990478516 = 1.802187442779541 + 1.0 * 6.588746547698975
Epoch 90, val loss: 1.7867711782455444
Epoch 100, training loss: 8.310614585876465 = 1.790222406387329 + 1.0 * 6.520392417907715
Epoch 100, val loss: 1.7746665477752686
Epoch 110, training loss: 8.243179321289062 = 1.7782329320907593 + 1.0 * 6.464946269989014
Epoch 110, val loss: 1.7630844116210938
Epoch 120, training loss: 8.180807113647461 = 1.7663809061050415 + 1.0 * 6.414426326751709
Epoch 120, val loss: 1.7521001100540161
Epoch 130, training loss: 8.127508163452148 = 1.7546361684799194 + 1.0 * 6.3728718757629395
Epoch 130, val loss: 1.7412476539611816
Epoch 140, training loss: 8.077205657958984 = 1.7424554824829102 + 1.0 * 6.334750175476074
Epoch 140, val loss: 1.7303658723831177
Epoch 150, training loss: 8.032896995544434 = 1.7289005517959595 + 1.0 * 6.303996562957764
Epoch 150, val loss: 1.7186726331710815
Epoch 160, training loss: 7.992051124572754 = 1.7129690647125244 + 1.0 * 6.27908182144165
Epoch 160, val loss: 1.7051825523376465
Epoch 170, training loss: 7.953122138977051 = 1.693518042564392 + 1.0 * 6.259603977203369
Epoch 170, val loss: 1.689156413078308
Epoch 180, training loss: 7.91557502746582 = 1.6700074672698975 + 1.0 * 6.245567321777344
Epoch 180, val loss: 1.6701250076293945
Epoch 190, training loss: 7.871565341949463 = 1.6416856050491333 + 1.0 * 6.229879856109619
Epoch 190, val loss: 1.647778868675232
Epoch 200, training loss: 7.82457160949707 = 1.6074299812316895 + 1.0 * 6.217141628265381
Epoch 200, val loss: 1.6210110187530518
Epoch 210, training loss: 7.771587371826172 = 1.5659375190734863 + 1.0 * 6.2056498527526855
Epoch 210, val loss: 1.588593602180481
Epoch 220, training loss: 7.721699237823486 = 1.5165318250656128 + 1.0 * 6.205167293548584
Epoch 220, val loss: 1.5504087209701538
Epoch 230, training loss: 7.652133464813232 = 1.4619712829589844 + 1.0 * 6.190162181854248
Epoch 230, val loss: 1.5079350471496582
Epoch 240, training loss: 7.583168983459473 = 1.4023122787475586 + 1.0 * 6.180856704711914
Epoch 240, val loss: 1.4619771242141724
Epoch 250, training loss: 7.51328706741333 = 1.338793396949768 + 1.0 * 6.174493789672852
Epoch 250, val loss: 1.4138838052749634
Epoch 260, training loss: 7.452289581298828 = 1.273768663406372 + 1.0 * 6.178521156311035
Epoch 260, val loss: 1.365818977355957
Epoch 270, training loss: 7.376016616821289 = 1.2109874486923218 + 1.0 * 6.165029048919678
Epoch 270, val loss: 1.31993567943573
Epoch 280, training loss: 7.3084001541137695 = 1.1497910022735596 + 1.0 * 6.158609390258789
Epoch 280, val loss: 1.2760586738586426
Epoch 290, training loss: 7.244049549102783 = 1.0899354219436646 + 1.0 * 6.154114246368408
Epoch 290, val loss: 1.233616828918457
Epoch 300, training loss: 7.1836137771606445 = 1.031998872756958 + 1.0 * 6.151614665985107
Epoch 300, val loss: 1.1926605701446533
Epoch 310, training loss: 7.125255584716797 = 0.9772241711616516 + 1.0 * 6.148031234741211
Epoch 310, val loss: 1.153671145439148
Epoch 320, training loss: 7.065428733825684 = 0.9241954684257507 + 1.0 * 6.141233444213867
Epoch 320, val loss: 1.1156843900680542
Epoch 330, training loss: 7.010654926300049 = 0.872601330280304 + 1.0 * 6.1380534172058105
Epoch 330, val loss: 1.0785458087921143
Epoch 340, training loss: 6.960978031158447 = 0.8229137063026428 + 1.0 * 6.138064384460449
Epoch 340, val loss: 1.0428946018218994
Epoch 350, training loss: 6.907350540161133 = 0.7757397294044495 + 1.0 * 6.131610870361328
Epoch 350, val loss: 1.008912205696106
Epoch 360, training loss: 6.857625961303711 = 0.7304427623748779 + 1.0 * 6.127182960510254
Epoch 360, val loss: 0.9764599800109863
Epoch 370, training loss: 6.810040473937988 = 0.6871235370635986 + 1.0 * 6.1229166984558105
Epoch 370, val loss: 0.9455283284187317
Epoch 380, training loss: 6.790485382080078 = 0.6462361812591553 + 1.0 * 6.144248962402344
Epoch 380, val loss: 0.916329026222229
Epoch 390, training loss: 6.7283148765563965 = 0.6087204813957214 + 1.0 * 6.119594573974609
Epoch 390, val loss: 0.8900235891342163
Epoch 400, training loss: 6.688211917877197 = 0.5741257667541504 + 1.0 * 6.114086151123047
Epoch 400, val loss: 0.8661673069000244
Epoch 410, training loss: 6.653217792510986 = 0.5418127179145813 + 1.0 * 6.111404895782471
Epoch 410, val loss: 0.8445514440536499
Epoch 420, training loss: 6.619230270385742 = 0.5114712119102478 + 1.0 * 6.10775899887085
Epoch 420, val loss: 0.8251332640647888
Epoch 430, training loss: 6.5897698402404785 = 0.48296716809272766 + 1.0 * 6.106802463531494
Epoch 430, val loss: 0.8079856038093567
Epoch 440, training loss: 6.566847801208496 = 0.4564417600631714 + 1.0 * 6.110405921936035
Epoch 440, val loss: 0.7932388186454773
Epoch 450, training loss: 6.5339579582214355 = 0.43175551295280457 + 1.0 * 6.102202415466309
Epoch 450, val loss: 0.7806708812713623
Epoch 460, training loss: 6.50633430480957 = 0.4084174633026123 + 1.0 * 6.097917079925537
Epoch 460, val loss: 0.7697570323944092
Epoch 470, training loss: 6.484627723693848 = 0.3861161172389984 + 1.0 * 6.098511695861816
Epoch 470, val loss: 0.7601568102836609
Epoch 480, training loss: 6.459598541259766 = 0.3648390769958496 + 1.0 * 6.094759464263916
Epoch 480, val loss: 0.7518235445022583
Epoch 490, training loss: 6.436997890472412 = 0.3446056544780731 + 1.0 * 6.092392444610596
Epoch 490, val loss: 0.7446426153182983
Epoch 500, training loss: 6.414104461669922 = 0.325207382440567 + 1.0 * 6.088897228240967
Epoch 500, val loss: 0.7385944724082947
Epoch 510, training loss: 6.397575855255127 = 0.3066559135913849 + 1.0 * 6.0909199714660645
Epoch 510, val loss: 0.733389675617218
Epoch 520, training loss: 6.391551971435547 = 0.28909832239151 + 1.0 * 6.102453708648682
Epoch 520, val loss: 0.7290237545967102
Epoch 530, training loss: 6.356733322143555 = 0.2725260853767395 + 1.0 * 6.084207057952881
Epoch 530, val loss: 0.725586473941803
Epoch 540, training loss: 6.338204860687256 = 0.25691065192222595 + 1.0 * 6.081294059753418
Epoch 540, val loss: 0.7230330109596252
Epoch 550, training loss: 6.321398735046387 = 0.24216072261333466 + 1.0 * 6.079237937927246
Epoch 550, val loss: 0.72124844789505
Epoch 560, training loss: 6.306145668029785 = 0.22816696763038635 + 1.0 * 6.077978610992432
Epoch 560, val loss: 0.72016441822052
Epoch 570, training loss: 6.305082321166992 = 0.21502158045768738 + 1.0 * 6.090060710906982
Epoch 570, val loss: 0.7197503447532654
Epoch 580, training loss: 6.280319690704346 = 0.20270946621894836 + 1.0 * 6.077610015869141
Epoch 580, val loss: 0.7199456095695496
Epoch 590, training loss: 6.2654876708984375 = 0.19127367436885834 + 1.0 * 6.074213981628418
Epoch 590, val loss: 0.7207510471343994
Epoch 600, training loss: 6.252691268920898 = 0.18056288361549377 + 1.0 * 6.0721282958984375
Epoch 600, val loss: 0.7221054434776306
Epoch 610, training loss: 6.245687484741211 = 0.17053009569644928 + 1.0 * 6.075157165527344
Epoch 610, val loss: 0.7239294648170471
Epoch 620, training loss: 6.234665393829346 = 0.16115693747997284 + 1.0 * 6.073508262634277
Epoch 620, val loss: 0.7262241840362549
Epoch 630, training loss: 6.222832202911377 = 0.15247872471809387 + 1.0 * 6.0703535079956055
Epoch 630, val loss: 0.7289608716964722
Epoch 640, training loss: 6.211993217468262 = 0.1444234400987625 + 1.0 * 6.067569732666016
Epoch 640, val loss: 0.7321563363075256
Epoch 650, training loss: 6.204396724700928 = 0.13689395785331726 + 1.0 * 6.067502975463867
Epoch 650, val loss: 0.7357209324836731
Epoch 660, training loss: 6.194774627685547 = 0.12986069917678833 + 1.0 * 6.064913749694824
Epoch 660, val loss: 0.7395944595336914
Epoch 670, training loss: 6.1881256103515625 = 0.12330195307731628 + 1.0 * 6.064823627471924
Epoch 670, val loss: 0.743796706199646
Epoch 680, training loss: 6.180245876312256 = 0.11718191206455231 + 1.0 * 6.063064098358154
Epoch 680, val loss: 0.748261034488678
Epoch 690, training loss: 6.1718950271606445 = 0.11145195364952087 + 1.0 * 6.060442924499512
Epoch 690, val loss: 0.7529702186584473
Epoch 700, training loss: 6.177583694458008 = 0.10607261955738068 + 1.0 * 6.071511268615723
Epoch 700, val loss: 0.7579156756401062
Epoch 710, training loss: 6.163164138793945 = 0.10102248191833496 + 1.0 * 6.062141418457031
Epoch 710, val loss: 0.7629648447036743
Epoch 720, training loss: 6.156891822814941 = 0.0963369682431221 + 1.0 * 6.0605549812316895
Epoch 720, val loss: 0.7682691216468811
Epoch 730, training loss: 6.148447513580322 = 0.09194043278694153 + 1.0 * 6.056507110595703
Epoch 730, val loss: 0.7736981511116028
Epoch 740, training loss: 6.142569541931152 = 0.08781088888645172 + 1.0 * 6.054758548736572
Epoch 740, val loss: 0.7793217301368713
Epoch 750, training loss: 6.140102386474609 = 0.08392422646284103 + 1.0 * 6.056178092956543
Epoch 750, val loss: 0.7850844264030457
Epoch 760, training loss: 6.139344692230225 = 0.08027420192956924 + 1.0 * 6.059070587158203
Epoch 760, val loss: 0.7908794283866882
Epoch 770, training loss: 6.136754989624023 = 0.07686375826597214 + 1.0 * 6.059891223907471
Epoch 770, val loss: 0.7967178225517273
Epoch 780, training loss: 6.126194953918457 = 0.07366637140512466 + 1.0 * 6.052528381347656
Epoch 780, val loss: 0.8026290535926819
Epoch 790, training loss: 6.120636940002441 = 0.07065755873918533 + 1.0 * 6.049979209899902
Epoch 790, val loss: 0.8086705803871155
Epoch 800, training loss: 6.117361068725586 = 0.06780079007148743 + 1.0 * 6.049560070037842
Epoch 800, val loss: 0.8148036003112793
Epoch 810, training loss: 6.123819351196289 = 0.06510067731142044 + 1.0 * 6.058718681335449
Epoch 810, val loss: 0.8209051489830017
Epoch 820, training loss: 6.109310626983643 = 0.06256216764450073 + 1.0 * 6.046748638153076
Epoch 820, val loss: 0.8269715309143066
Epoch 830, training loss: 6.1066060066223145 = 0.06017322465777397 + 1.0 * 6.046432971954346
Epoch 830, val loss: 0.8331179618835449
Epoch 840, training loss: 6.104294300079346 = 0.05790224298834801 + 1.0 * 6.04639196395874
Epoch 840, val loss: 0.8393603563308716
Epoch 850, training loss: 6.103735446929932 = 0.05574345961213112 + 1.0 * 6.047991752624512
Epoch 850, val loss: 0.8455172181129456
Epoch 860, training loss: 6.097593307495117 = 0.05371217802166939 + 1.0 * 6.043880939483643
Epoch 860, val loss: 0.8517079949378967
Epoch 870, training loss: 6.095836162567139 = 0.051779791712760925 + 1.0 * 6.044056415557861
Epoch 870, val loss: 0.8579254150390625
Epoch 880, training loss: 6.104103088378906 = 0.049938835203647614 + 1.0 * 6.054164409637451
Epoch 880, val loss: 0.8640618920326233
Epoch 890, training loss: 6.093692779541016 = 0.04819322004914284 + 1.0 * 6.045499324798584
Epoch 890, val loss: 0.8700997829437256
Epoch 900, training loss: 6.0904107093811035 = 0.046545978635549545 + 1.0 * 6.043864727020264
Epoch 900, val loss: 0.8761606812477112
Epoch 910, training loss: 6.087069511413574 = 0.04497527331113815 + 1.0 * 6.0420942306518555
Epoch 910, val loss: 0.8821744322776794
Epoch 920, training loss: 6.086962699890137 = 0.043475329875946045 + 1.0 * 6.043487548828125
Epoch 920, val loss: 0.8881492018699646
Epoch 930, training loss: 6.0829176902771 = 0.042049020528793335 + 1.0 * 6.040868759155273
Epoch 930, val loss: 0.8940678834915161
Epoch 940, training loss: 6.080824851989746 = 0.040692608803510666 + 1.0 * 6.04013204574585
Epoch 940, val loss: 0.8999519348144531
Epoch 950, training loss: 6.080644607543945 = 0.03939369320869446 + 1.0 * 6.041250705718994
Epoch 950, val loss: 0.9058459401130676
Epoch 960, training loss: 6.077643871307373 = 0.03814902901649475 + 1.0 * 6.03949499130249
Epoch 960, val loss: 0.9116159081459045
Epoch 970, training loss: 6.073923110961914 = 0.03696000948548317 + 1.0 * 6.036962985992432
Epoch 970, val loss: 0.9173581600189209
Epoch 980, training loss: 6.080023765563965 = 0.03582138568162918 + 1.0 * 6.0442023277282715
Epoch 980, val loss: 0.923089861869812
Epoch 990, training loss: 6.0711894035339355 = 0.03473948314785957 + 1.0 * 6.036449909210205
Epoch 990, val loss: 0.9287828207015991
Epoch 1000, training loss: 6.0687761306762695 = 0.03370223194360733 + 1.0 * 6.035073757171631
Epoch 1000, val loss: 0.934405505657196
Epoch 1010, training loss: 6.072911262512207 = 0.03270548954606056 + 1.0 * 6.040205955505371
Epoch 1010, val loss: 0.9400591254234314
Epoch 1020, training loss: 6.0650248527526855 = 0.031746119260787964 + 1.0 * 6.033278942108154
Epoch 1020, val loss: 0.9455259442329407
Epoch 1030, training loss: 6.065888404846191 = 0.030824929475784302 + 1.0 * 6.03506326675415
Epoch 1030, val loss: 0.9510093331336975
Epoch 1040, training loss: 6.066061019897461 = 0.029942432418465614 + 1.0 * 6.036118507385254
Epoch 1040, val loss: 0.956407904624939
Epoch 1050, training loss: 6.061542987823486 = 0.029103772714734077 + 1.0 * 6.032439231872559
Epoch 1050, val loss: 0.961722731590271
Epoch 1060, training loss: 6.059638977050781 = 0.028294255957007408 + 1.0 * 6.031344890594482
Epoch 1060, val loss: 0.9669837951660156
Epoch 1070, training loss: 6.059696674346924 = 0.027516106143593788 + 1.0 * 6.0321807861328125
Epoch 1070, val loss: 0.9722840189933777
Epoch 1080, training loss: 6.058219909667969 = 0.02676544152200222 + 1.0 * 6.031454563140869
Epoch 1080, val loss: 0.9774829745292664
Epoch 1090, training loss: 6.0652618408203125 = 0.026049144566059113 + 1.0 * 6.039212703704834
Epoch 1090, val loss: 0.9825629591941833
Epoch 1100, training loss: 6.05549955368042 = 0.025359150022268295 + 1.0 * 6.030140399932861
Epoch 1100, val loss: 0.9875566363334656
Epoch 1110, training loss: 6.054080963134766 = 0.024697648361325264 + 1.0 * 6.029383182525635
Epoch 1110, val loss: 0.99260413646698
Epoch 1120, training loss: 6.053142547607422 = 0.024059243500232697 + 1.0 * 6.029083251953125
Epoch 1120, val loss: 0.9976089000701904
Epoch 1130, training loss: 6.0548095703125 = 0.023441065102815628 + 1.0 * 6.031368732452393
Epoch 1130, val loss: 1.00254487991333
Epoch 1140, training loss: 6.05859899520874 = 0.022847052663564682 + 1.0 * 6.035751819610596
Epoch 1140, val loss: 1.007400393486023
Epoch 1150, training loss: 6.050657272338867 = 0.022279679775238037 + 1.0 * 6.028377532958984
Epoch 1150, val loss: 1.0122246742248535
Epoch 1160, training loss: 6.049240589141846 = 0.021732596680521965 + 1.0 * 6.027507781982422
Epoch 1160, val loss: 1.0169333219528198
Epoch 1170, training loss: 6.048225402832031 = 0.021202728152275085 + 1.0 * 6.027022838592529
Epoch 1170, val loss: 1.021668791770935
Epoch 1180, training loss: 6.051483631134033 = 0.020688872784376144 + 1.0 * 6.030794620513916
Epoch 1180, val loss: 1.0263092517852783
Epoch 1190, training loss: 6.04541540145874 = 0.020195316523313522 + 1.0 * 6.025219917297363
Epoch 1190, val loss: 1.030953049659729
Epoch 1200, training loss: 6.044342041015625 = 0.019721202552318573 + 1.0 * 6.02462100982666
Epoch 1200, val loss: 1.035439372062683
Epoch 1210, training loss: 6.042755603790283 = 0.01926124095916748 + 1.0 * 6.023494243621826
Epoch 1210, val loss: 1.0399574041366577
Epoch 1220, training loss: 6.0447282791137695 = 0.0188156608492136 + 1.0 * 6.025912761688232
Epoch 1220, val loss: 1.0444903373718262
Epoch 1230, training loss: 6.044101238250732 = 0.01838347129523754 + 1.0 * 6.025717735290527
Epoch 1230, val loss: 1.0488357543945312
Epoch 1240, training loss: 6.041283130645752 = 0.01796688884496689 + 1.0 * 6.023316383361816
Epoch 1240, val loss: 1.0531178712844849
Epoch 1250, training loss: 6.042155742645264 = 0.017565740272402763 + 1.0 * 6.024590015411377
Epoch 1250, val loss: 1.057443618774414
Epoch 1260, training loss: 6.0447001457214355 = 0.017176460474729538 + 1.0 * 6.027523517608643
Epoch 1260, val loss: 1.0616962909698486
Epoch 1270, training loss: 6.039602756500244 = 0.016803665086627007 + 1.0 * 6.022799015045166
Epoch 1270, val loss: 1.0658971071243286
Epoch 1280, training loss: 6.037332534790039 = 0.016440559178590775 + 1.0 * 6.020892143249512
Epoch 1280, val loss: 1.0700373649597168
Epoch 1290, training loss: 6.040881633758545 = 0.01608896255493164 + 1.0 * 6.024792671203613
Epoch 1290, val loss: 1.074273705482483
Epoch 1300, training loss: 6.038463592529297 = 0.015747809782624245 + 1.0 * 6.0227155685424805
Epoch 1300, val loss: 1.0783158540725708
Epoch 1310, training loss: 6.035552501678467 = 0.015418716706335545 + 1.0 * 6.020133972167969
Epoch 1310, val loss: 1.0822561979293823
Epoch 1320, training loss: 6.035067558288574 = 0.015102140605449677 + 1.0 * 6.019965648651123
Epoch 1320, val loss: 1.0862482786178589
Epoch 1330, training loss: 6.0357346534729 = 0.014791406691074371 + 1.0 * 6.0209431648254395
Epoch 1330, val loss: 1.0902315378189087
Epoch 1340, training loss: 6.03521203994751 = 0.014490176923573017 + 1.0 * 6.020721912384033
Epoch 1340, val loss: 1.0941883325576782
Epoch 1350, training loss: 6.034099102020264 = 0.014201020821928978 + 1.0 * 6.019897937774658
Epoch 1350, val loss: 1.0981042385101318
Epoch 1360, training loss: 6.034640312194824 = 0.01391696184873581 + 1.0 * 6.020723342895508
Epoch 1360, val loss: 1.101938009262085
Epoch 1370, training loss: 6.03378438949585 = 0.013642176985740662 + 1.0 * 6.020142078399658
Epoch 1370, val loss: 1.105806827545166
Epoch 1380, training loss: 6.035097122192383 = 0.013375918380916119 + 1.0 * 6.021721363067627
Epoch 1380, val loss: 1.1095377206802368
Epoch 1390, training loss: 6.037914276123047 = 0.013118809089064598 + 1.0 * 6.0247955322265625
Epoch 1390, val loss: 1.1132231950759888
Epoch 1400, training loss: 6.030046463012695 = 0.012871982529759407 + 1.0 * 6.017174243927002
Epoch 1400, val loss: 1.1168770790100098
Epoch 1410, training loss: 6.028151512145996 = 0.0126308249309659 + 1.0 * 6.0155205726623535
Epoch 1410, val loss: 1.1205275058746338
Epoch 1420, training loss: 6.027781009674072 = 0.01239476352930069 + 1.0 * 6.01538610458374
Epoch 1420, val loss: 1.1241847276687622
Epoch 1430, training loss: 6.029062747955322 = 0.012164626270532608 + 1.0 * 6.016898155212402
Epoch 1430, val loss: 1.127862572669983
Epoch 1440, training loss: 6.0355544090271 = 0.01194093283265829 + 1.0 * 6.023613452911377
Epoch 1440, val loss: 1.1314319372177124
Epoch 1450, training loss: 6.026570796966553 = 0.011724047362804413 + 1.0 * 6.0148468017578125
Epoch 1450, val loss: 1.1347721815109253
Epoch 1460, training loss: 6.026583671569824 = 0.01151543203741312 + 1.0 * 6.015068054199219
Epoch 1460, val loss: 1.1381760835647583
Epoch 1470, training loss: 6.02564001083374 = 0.01131180115044117 + 1.0 * 6.0143280029296875
Epoch 1470, val loss: 1.1416746377944946
Epoch 1480, training loss: 6.028137683868408 = 0.011110321618616581 + 1.0 * 6.017027378082275
Epoch 1480, val loss: 1.1451964378356934
Epoch 1490, training loss: 6.028810501098633 = 0.010915323160588741 + 1.0 * 6.017895221710205
Epoch 1490, val loss: 1.1486021280288696
Epoch 1500, training loss: 6.023858070373535 = 0.010726287961006165 + 1.0 * 6.013131618499756
Epoch 1500, val loss: 1.1518992185592651
Epoch 1510, training loss: 6.0290632247924805 = 0.010544387623667717 + 1.0 * 6.018518924713135
Epoch 1510, val loss: 1.1551849842071533
Epoch 1520, training loss: 6.023780345916748 = 0.010366423986852169 + 1.0 * 6.013413906097412
Epoch 1520, val loss: 1.1584186553955078
Epoch 1530, training loss: 6.023515224456787 = 0.010192763060331345 + 1.0 * 6.013322353363037
Epoch 1530, val loss: 1.1616305112838745
Epoch 1540, training loss: 6.02389669418335 = 0.010023177601397038 + 1.0 * 6.01387357711792
Epoch 1540, val loss: 1.1648674011230469
Epoch 1550, training loss: 6.023292064666748 = 0.009857538156211376 + 1.0 * 6.013434410095215
Epoch 1550, val loss: 1.168121576309204
Epoch 1560, training loss: 6.0215229988098145 = 0.009696315042674541 + 1.0 * 6.011826515197754
Epoch 1560, val loss: 1.1713321208953857
Epoch 1570, training loss: 6.027561187744141 = 0.009537961333990097 + 1.0 * 6.0180230140686035
Epoch 1570, val loss: 1.1744831800460815
Epoch 1580, training loss: 6.022357940673828 = 0.009384679608047009 + 1.0 * 6.012973308563232
Epoch 1580, val loss: 1.1776143312454224
Epoch 1590, training loss: 6.021965503692627 = 0.009237139485776424 + 1.0 * 6.012728214263916
Epoch 1590, val loss: 1.180720329284668
Epoch 1600, training loss: 6.021206378936768 = 0.009091569110751152 + 1.0 * 6.012115001678467
Epoch 1600, val loss: 1.1837964057922363
Epoch 1610, training loss: 6.02281379699707 = 0.008949861861765385 + 1.0 * 6.013864040374756
Epoch 1610, val loss: 1.1869505643844604
Epoch 1620, training loss: 6.02127742767334 = 0.008810595609247684 + 1.0 * 6.012466907501221
Epoch 1620, val loss: 1.189951777458191
Epoch 1630, training loss: 6.022581100463867 = 0.008675867691636086 + 1.0 * 6.013905048370361
Epoch 1630, val loss: 1.1929415464401245
Epoch 1640, training loss: 6.019355773925781 = 0.008543940261006355 + 1.0 * 6.010811805725098
Epoch 1640, val loss: 1.1959228515625
Epoch 1650, training loss: 6.017245292663574 = 0.00841553881764412 + 1.0 * 6.008829593658447
Epoch 1650, val loss: 1.1988701820373535
Epoch 1660, training loss: 6.025241851806641 = 0.008289591409265995 + 1.0 * 6.016952037811279
Epoch 1660, val loss: 1.2019304037094116
Epoch 1670, training loss: 6.018768310546875 = 0.008165695704519749 + 1.0 * 6.0106024742126465
Epoch 1670, val loss: 1.2048064470291138
Epoch 1680, training loss: 6.016545295715332 = 0.008046222850680351 + 1.0 * 6.0084991455078125
Epoch 1680, val loss: 1.207605242729187
Epoch 1690, training loss: 6.017089366912842 = 0.007929752580821514 + 1.0 * 6.009159564971924
Epoch 1690, val loss: 1.2104839086532593
Epoch 1700, training loss: 6.022945880889893 = 0.007815340533852577 + 1.0 * 6.015130519866943
Epoch 1700, val loss: 1.2133567333221436
Epoch 1710, training loss: 6.016025543212891 = 0.007702828384935856 + 1.0 * 6.008322715759277
Epoch 1710, val loss: 1.2161078453063965
Epoch 1720, training loss: 6.0155415534973145 = 0.007593882270157337 + 1.0 * 6.0079474449157715
Epoch 1720, val loss: 1.2189253568649292
Epoch 1730, training loss: 6.01470947265625 = 0.00748650124296546 + 1.0 * 6.007223129272461
Epoch 1730, val loss: 1.2217546701431274
Epoch 1740, training loss: 6.022851943969727 = 0.007380689028650522 + 1.0 * 6.015471458435059
Epoch 1740, val loss: 1.224574327468872
Epoch 1750, training loss: 6.0189290046691895 = 0.007279734127223492 + 1.0 * 6.011649131774902
Epoch 1750, val loss: 1.2273434400558472
Epoch 1760, training loss: 6.01564359664917 = 0.007179186679422855 + 1.0 * 6.008464336395264
Epoch 1760, val loss: 1.2299907207489014
Epoch 1770, training loss: 6.013138294219971 = 0.007081621326506138 + 1.0 * 6.006056785583496
Epoch 1770, val loss: 1.2327007055282593
Epoch 1780, training loss: 6.01459264755249 = 0.00698520103469491 + 1.0 * 6.007607460021973
Epoch 1780, val loss: 1.2354247570037842
Epoch 1790, training loss: 6.017764568328857 = 0.006890781223773956 + 1.0 * 6.010873794555664
Epoch 1790, val loss: 1.2381525039672852
Epoch 1800, training loss: 6.016467094421387 = 0.006798175163567066 + 1.0 * 6.009668827056885
Epoch 1800, val loss: 1.2407655715942383
Epoch 1810, training loss: 6.013621807098389 = 0.0067085870541632175 + 1.0 * 6.006913185119629
Epoch 1810, val loss: 1.2432607412338257
Epoch 1820, training loss: 6.015574932098389 = 0.006620629224926233 + 1.0 * 6.0089545249938965
Epoch 1820, val loss: 1.2459349632263184
Epoch 1830, training loss: 6.014052391052246 = 0.006534301210194826 + 1.0 * 6.007518291473389
Epoch 1830, val loss: 1.2485380172729492
Epoch 1840, training loss: 6.013510227203369 = 0.006449609994888306 + 1.0 * 6.007060527801514
Epoch 1840, val loss: 1.2511405944824219
Epoch 1850, training loss: 6.010141372680664 = 0.006366319954395294 + 1.0 * 6.003775119781494
Epoch 1850, val loss: 1.2536216974258423
Epoch 1860, training loss: 6.009897708892822 = 0.006284778006374836 + 1.0 * 6.003612995147705
Epoch 1860, val loss: 1.25620436668396
Epoch 1870, training loss: 6.019909858703613 = 0.006204681470990181 + 1.0 * 6.013705253601074
Epoch 1870, val loss: 1.2587800025939941
Epoch 1880, training loss: 6.014718532562256 = 0.00612636236473918 + 1.0 * 6.008592128753662
Epoch 1880, val loss: 1.261229395866394
Epoch 1890, training loss: 6.011559009552002 = 0.006050555966794491 + 1.0 * 6.0055084228515625
Epoch 1890, val loss: 1.263663649559021
Epoch 1900, training loss: 6.008294582366943 = 0.005975883919745684 + 1.0 * 6.002318859100342
Epoch 1900, val loss: 1.2660757303237915
Epoch 1910, training loss: 6.008210182189941 = 0.005902331322431564 + 1.0 * 6.002307891845703
Epoch 1910, val loss: 1.268545150756836
Epoch 1920, training loss: 6.01548957824707 = 0.0058298660442233086 + 1.0 * 6.009659767150879
Epoch 1920, val loss: 1.2710492610931396
Epoch 1930, training loss: 6.010878562927246 = 0.005758439656347036 + 1.0 * 6.005120277404785
Epoch 1930, val loss: 1.2734025716781616
Epoch 1940, training loss: 6.013214111328125 = 0.005688582081347704 + 1.0 * 6.007525444030762
Epoch 1940, val loss: 1.2757599353790283
Epoch 1950, training loss: 6.0131988525390625 = 0.005620873998850584 + 1.0 * 6.007577896118164
Epoch 1950, val loss: 1.2780671119689941
Epoch 1960, training loss: 6.008344650268555 = 0.005555876065045595 + 1.0 * 6.002788543701172
Epoch 1960, val loss: 1.2803841829299927
Epoch 1970, training loss: 6.006535053253174 = 0.005490878131240606 + 1.0 * 6.001044273376465
Epoch 1970, val loss: 1.282676100730896
Epoch 1980, training loss: 6.0053229331970215 = 0.005426375195384026 + 1.0 * 5.99989652633667
Epoch 1980, val loss: 1.285056710243225
Epoch 1990, training loss: 6.007014274597168 = 0.0053621805272996426 + 1.0 * 6.001652240753174
Epoch 1990, val loss: 1.28745436668396
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.3616
Flip ASR: 0.4044/225 nodes
The final ASR:0.51907, 0.16854, Accuracy:0.81235, 0.02444
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9420])
updated graph: torch.Size([2, 10450])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00460, Accuracy:0.83086, 0.00972
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.316305160522461 = 1.9424463510513306 + 1.0 * 8.373858451843262
Epoch 0, val loss: 1.9337693452835083
Epoch 10, training loss: 10.306450843811035 = 1.9329898357391357 + 1.0 * 8.37346076965332
Epoch 10, val loss: 1.9242761135101318
Epoch 20, training loss: 10.292216300964355 = 1.9215013980865479 + 1.0 * 8.370715141296387
Epoch 20, val loss: 1.912715196609497
Epoch 30, training loss: 10.254850387573242 = 1.9057326316833496 + 1.0 * 8.34911823272705
Epoch 30, val loss: 1.8969471454620361
Epoch 40, training loss: 10.081283569335938 = 1.8854495286941528 + 1.0 * 8.195834159851074
Epoch 40, val loss: 1.8772354125976562
Epoch 50, training loss: 9.489977836608887 = 1.8629792928695679 + 1.0 * 7.6269989013671875
Epoch 50, val loss: 1.8556944131851196
Epoch 60, training loss: 9.08715534210205 = 1.8404161930084229 + 1.0 * 7.246738910675049
Epoch 60, val loss: 1.8357765674591064
Epoch 70, training loss: 8.792051315307617 = 1.8225868940353394 + 1.0 * 6.969464302062988
Epoch 70, val loss: 1.8208662271499634
Epoch 80, training loss: 8.534924507141113 = 1.8060933351516724 + 1.0 * 6.728830814361572
Epoch 80, val loss: 1.8062119483947754
Epoch 90, training loss: 8.40876579284668 = 1.7880785465240479 + 1.0 * 6.620687007904053
Epoch 90, val loss: 1.7906221151351929
Epoch 100, training loss: 8.334022521972656 = 1.7677621841430664 + 1.0 * 6.566259860992432
Epoch 100, val loss: 1.7739770412445068
Epoch 110, training loss: 8.256203651428223 = 1.7477701902389526 + 1.0 * 6.5084333419799805
Epoch 110, val loss: 1.7589869499206543
Epoch 120, training loss: 8.18984603881836 = 1.7272173166275024 + 1.0 * 6.462628364562988
Epoch 120, val loss: 1.743238925933838
Epoch 130, training loss: 8.128843307495117 = 1.7037224769592285 + 1.0 * 6.4251203536987305
Epoch 130, val loss: 1.7245917320251465
Epoch 140, training loss: 8.06532096862793 = 1.6770964860916138 + 1.0 * 6.3882246017456055
Epoch 140, val loss: 1.7033931016921997
Epoch 150, training loss: 7.993811130523682 = 1.647387146949768 + 1.0 * 6.346424102783203
Epoch 150, val loss: 1.6797932386398315
Epoch 160, training loss: 7.928997993469238 = 1.6129095554351807 + 1.0 * 6.3160881996154785
Epoch 160, val loss: 1.6527037620544434
Epoch 170, training loss: 7.863304615020752 = 1.5723341703414917 + 1.0 * 6.290970325469971
Epoch 170, val loss: 1.620991826057434
Epoch 180, training loss: 7.796358585357666 = 1.5250846147537231 + 1.0 * 6.271274089813232
Epoch 180, val loss: 1.5840040445327759
Epoch 190, training loss: 7.726161003112793 = 1.471036434173584 + 1.0 * 6.255124568939209
Epoch 190, val loss: 1.5415494441986084
Epoch 200, training loss: 7.655062198638916 = 1.411016583442688 + 1.0 * 6.244045734405518
Epoch 200, val loss: 1.4942320585250854
Epoch 210, training loss: 7.579591751098633 = 1.3474812507629395 + 1.0 * 6.232110500335693
Epoch 210, val loss: 1.4441884756088257
Epoch 220, training loss: 7.504578590393066 = 1.2820515632629395 + 1.0 * 6.222527027130127
Epoch 220, val loss: 1.3923566341400146
Epoch 230, training loss: 7.43179178237915 = 1.2171012163162231 + 1.0 * 6.214690685272217
Epoch 230, val loss: 1.3404127359390259
Epoch 240, training loss: 7.360533714294434 = 1.1542468070983887 + 1.0 * 6.206286907196045
Epoch 240, val loss: 1.2899290323257446
Epoch 250, training loss: 7.292657852172852 = 1.0940799713134766 + 1.0 * 6.198577880859375
Epoch 250, val loss: 1.2416828870773315
Epoch 260, training loss: 7.228423118591309 = 1.0366805791854858 + 1.0 * 6.191742420196533
Epoch 260, val loss: 1.1956655979156494
Epoch 270, training loss: 7.168984413146973 = 0.9813730120658875 + 1.0 * 6.1876115798950195
Epoch 270, val loss: 1.1514692306518555
Epoch 280, training loss: 7.109491348266602 = 0.9284359812736511 + 1.0 * 6.181055545806885
Epoch 280, val loss: 1.1095795631408691
Epoch 290, training loss: 7.051743030548096 = 0.8772806525230408 + 1.0 * 6.17446231842041
Epoch 290, val loss: 1.0696028470993042
Epoch 300, training loss: 6.99648380279541 = 0.8270818591117859 + 1.0 * 6.169402122497559
Epoch 300, val loss: 1.0307798385620117
Epoch 310, training loss: 6.946931838989258 = 0.7781646251678467 + 1.0 * 6.168766975402832
Epoch 310, val loss: 0.9934617877006531
Epoch 320, training loss: 6.89216423034668 = 0.7316614985466003 + 1.0 * 6.160502910614014
Epoch 320, val loss: 0.9585311412811279
Epoch 330, training loss: 6.843284606933594 = 0.6875277757644653 + 1.0 * 6.155756950378418
Epoch 330, val loss: 0.9259477257728577
Epoch 340, training loss: 6.799689292907715 = 0.6458649039268494 + 1.0 * 6.153824329376221
Epoch 340, val loss: 0.8959392309188843
Epoch 350, training loss: 6.758656024932861 = 0.6070343852043152 + 1.0 * 6.1516218185424805
Epoch 350, val loss: 0.8687702417373657
Epoch 360, training loss: 6.716846466064453 = 0.5712038278579712 + 1.0 * 6.1456427574157715
Epoch 360, val loss: 0.8446472883224487
Epoch 370, training loss: 6.677521228790283 = 0.5380972027778625 + 1.0 * 6.139423847198486
Epoch 370, val loss: 0.8231629133224487
Epoch 380, training loss: 6.648138046264648 = 0.507178544998169 + 1.0 * 6.140959739685059
Epoch 380, val loss: 0.8040570020675659
Epoch 390, training loss: 6.611232757568359 = 0.4784029424190521 + 1.0 * 6.132829666137695
Epoch 390, val loss: 0.7872269153594971
Epoch 400, training loss: 6.580678462982178 = 0.4512867033481598 + 1.0 * 6.129391670227051
Epoch 400, val loss: 0.7721715569496155
Epoch 410, training loss: 6.552540302276611 = 0.42535167932510376 + 1.0 * 6.127188682556152
Epoch 410, val loss: 0.7585981488227844
Epoch 420, training loss: 6.530209541320801 = 0.4005558490753174 + 1.0 * 6.129653453826904
Epoch 420, val loss: 0.7463179230690002
Epoch 430, training loss: 6.49795389175415 = 0.3767196536064148 + 1.0 * 6.12123441696167
Epoch 430, val loss: 0.7351795434951782
Epoch 440, training loss: 6.472976207733154 = 0.3534986674785614 + 1.0 * 6.11947774887085
Epoch 440, val loss: 0.7248932123184204
Epoch 450, training loss: 6.454683780670166 = 0.33109429478645325 + 1.0 * 6.123589515686035
Epoch 450, val loss: 0.715519905090332
Epoch 460, training loss: 6.423987865447998 = 0.3094326853752136 + 1.0 * 6.114555358886719
Epoch 460, val loss: 0.7071329355239868
Epoch 470, training loss: 6.399686336517334 = 0.2884276807308197 + 1.0 * 6.111258506774902
Epoch 470, val loss: 0.699560821056366
Epoch 480, training loss: 6.3833112716674805 = 0.2682233452796936 + 1.0 * 6.115087985992432
Epoch 480, val loss: 0.6929453611373901
Epoch 490, training loss: 6.356849670410156 = 0.24915827810764313 + 1.0 * 6.107691287994385
Epoch 490, val loss: 0.6873725056648254
Epoch 500, training loss: 6.335537910461426 = 0.2311265468597412 + 1.0 * 6.104411602020264
Epoch 500, val loss: 0.6828368306159973
Epoch 510, training loss: 6.317850112915039 = 0.2141839563846588 + 1.0 * 6.103666305541992
Epoch 510, val loss: 0.6792747378349304
Epoch 520, training loss: 6.301666736602783 = 0.19852425158023834 + 1.0 * 6.103142261505127
Epoch 520, val loss: 0.6767120361328125
Epoch 530, training loss: 6.285218715667725 = 0.1841474324464798 + 1.0 * 6.101071357727051
Epoch 530, val loss: 0.6753019690513611
Epoch 540, training loss: 6.268385410308838 = 0.17099358141422272 + 1.0 * 6.097391605377197
Epoch 540, val loss: 0.6748127937316895
Epoch 550, training loss: 6.257793426513672 = 0.15895774960517883 + 1.0 * 6.098835468292236
Epoch 550, val loss: 0.6752542853355408
Epoch 560, training loss: 6.243492603302002 = 0.14803235232830048 + 1.0 * 6.095460414886475
Epoch 560, val loss: 0.676482617855072
Epoch 570, training loss: 6.231692314147949 = 0.13811391592025757 + 1.0 * 6.093578338623047
Epoch 570, val loss: 0.6784759759902954
Epoch 580, training loss: 6.220094680786133 = 0.12912048399448395 + 1.0 * 6.0909743309021
Epoch 580, val loss: 0.6810609102249146
Epoch 590, training loss: 6.209011077880859 = 0.12090325355529785 + 1.0 * 6.088108062744141
Epoch 590, val loss: 0.6842634081840515
Epoch 600, training loss: 6.2038960456848145 = 0.11339510232210159 + 1.0 * 6.090500831604004
Epoch 600, val loss: 0.6880697011947632
Epoch 610, training loss: 6.1990251541137695 = 0.10661536455154419 + 1.0 * 6.092409610748291
Epoch 610, val loss: 0.6921705007553101
Epoch 620, training loss: 6.185178279876709 = 0.10043022036552429 + 1.0 * 6.084748268127441
Epoch 620, val loss: 0.6967430114746094
Epoch 630, training loss: 6.176568508148193 = 0.09475656598806381 + 1.0 * 6.081811904907227
Epoch 630, val loss: 0.7016832232475281
Epoch 640, training loss: 6.171753883361816 = 0.08952277898788452 + 1.0 * 6.082231044769287
Epoch 640, val loss: 0.7068809270858765
Epoch 650, training loss: 6.166008472442627 = 0.08470902591943741 + 1.0 * 6.081299304962158
Epoch 650, val loss: 0.7122600078582764
Epoch 660, training loss: 6.15959358215332 = 0.08028192818164825 + 1.0 * 6.079311847686768
Epoch 660, val loss: 0.7179628610610962
Epoch 670, training loss: 6.152699947357178 = 0.07617732882499695 + 1.0 * 6.0765228271484375
Epoch 670, val loss: 0.7237656712532043
Epoch 680, training loss: 6.154374122619629 = 0.07236979901790619 + 1.0 * 6.082004547119141
Epoch 680, val loss: 0.7296997904777527
Epoch 690, training loss: 6.145323276519775 = 0.0688476637005806 + 1.0 * 6.076475620269775
Epoch 690, val loss: 0.7357416152954102
Epoch 700, training loss: 6.143429756164551 = 0.06557108461856842 + 1.0 * 6.0778584480285645
Epoch 700, val loss: 0.7419083714485168
Epoch 710, training loss: 6.136105060577393 = 0.06251383572816849 + 1.0 * 6.073591232299805
Epoch 710, val loss: 0.7480330467224121
Epoch 720, training loss: 6.131829738616943 = 0.05966582149267197 + 1.0 * 6.072164058685303
Epoch 720, val loss: 0.7542818188667297
Epoch 730, training loss: 6.128245830535889 = 0.05700061097741127 + 1.0 * 6.071245193481445
Epoch 730, val loss: 0.7605680227279663
Epoch 740, training loss: 6.122729778289795 = 0.05451589077711105 + 1.0 * 6.068213939666748
Epoch 740, val loss: 0.7668275833129883
Epoch 750, training loss: 6.11956262588501 = 0.05218237638473511 + 1.0 * 6.067380428314209
Epoch 750, val loss: 0.7731509208679199
Epoch 760, training loss: 6.115772247314453 = 0.04998154565691948 + 1.0 * 6.06579065322876
Epoch 760, val loss: 0.7794406414031982
Epoch 770, training loss: 6.12401008605957 = 0.0479079969227314 + 1.0 * 6.076102256774902
Epoch 770, val loss: 0.7857704758644104
Epoch 780, training loss: 6.110794544219971 = 0.045968666672706604 + 1.0 * 6.064826011657715
Epoch 780, val loss: 0.7919746041297913
Epoch 790, training loss: 6.107307434082031 = 0.044141292572021484 + 1.0 * 6.06316614151001
Epoch 790, val loss: 0.7982070446014404
Epoch 800, training loss: 6.104098320007324 = 0.0424104668200016 + 1.0 * 6.06168794631958
Epoch 800, val loss: 0.804427444934845
Epoch 810, training loss: 6.106019020080566 = 0.04077669233083725 + 1.0 * 6.065242290496826
Epoch 810, val loss: 0.81058269739151
Epoch 820, training loss: 6.0977559089660645 = 0.03923049569129944 + 1.0 * 6.058525562286377
Epoch 820, val loss: 0.8167568445205688
Epoch 830, training loss: 6.097206115722656 = 0.03776869177818298 + 1.0 * 6.059437274932861
Epoch 830, val loss: 0.8228574991226196
Epoch 840, training loss: 6.094411373138428 = 0.03638778254389763 + 1.0 * 6.058023452758789
Epoch 840, val loss: 0.8288750648498535
Epoch 850, training loss: 6.094223499298096 = 0.035082511603832245 + 1.0 * 6.059141159057617
Epoch 850, val loss: 0.834889829158783
Epoch 860, training loss: 6.088873863220215 = 0.03384379670023918 + 1.0 * 6.05502986907959
Epoch 860, val loss: 0.8408114314079285
Epoch 870, training loss: 6.086792469024658 = 0.032661013305187225 + 1.0 * 6.054131507873535
Epoch 870, val loss: 0.846717119216919
Epoch 880, training loss: 6.0863118171691895 = 0.03153472766280174 + 1.0 * 6.054777145385742
Epoch 880, val loss: 0.8525924682617188
Epoch 890, training loss: 6.085297107696533 = 0.030470017343759537 + 1.0 * 6.0548272132873535
Epoch 890, val loss: 0.8585105538368225
Epoch 900, training loss: 6.081602573394775 = 0.029464850202202797 + 1.0 * 6.052137851715088
Epoch 900, val loss: 0.8641740679740906
Epoch 910, training loss: 6.080329418182373 = 0.028508339077234268 + 1.0 * 6.051821231842041
Epoch 910, val loss: 0.8697559833526611
Epoch 920, training loss: 6.0819220542907715 = 0.02759287878870964 + 1.0 * 6.0543293952941895
Epoch 920, val loss: 0.8753124475479126
Epoch 930, training loss: 6.076674461364746 = 0.026720058172941208 + 1.0 * 6.049954414367676
Epoch 930, val loss: 0.880935788154602
Epoch 940, training loss: 6.074123382568359 = 0.025891389697790146 + 1.0 * 6.048232078552246
Epoch 940, val loss: 0.8864076733589172
Epoch 950, training loss: 6.074400424957275 = 0.025098470970988274 + 1.0 * 6.049302101135254
Epoch 950, val loss: 0.891859769821167
Epoch 960, training loss: 6.07319974899292 = 0.024342695251107216 + 1.0 * 6.04885721206665
Epoch 960, val loss: 0.8972235918045044
Epoch 970, training loss: 6.0731987953186035 = 0.02362087555229664 + 1.0 * 6.049577713012695
Epoch 970, val loss: 0.9025511741638184
Epoch 980, training loss: 6.0690460205078125 = 0.0229331161826849 + 1.0 * 6.046113014221191
Epoch 980, val loss: 0.9077820777893066
Epoch 990, training loss: 6.068835735321045 = 0.022272275760769844 + 1.0 * 6.046563625335693
Epoch 990, val loss: 0.9129685163497925
Epoch 1000, training loss: 6.06898832321167 = 0.021639065816998482 + 1.0 * 6.047349452972412
Epoch 1000, val loss: 0.9181404113769531
Epoch 1010, training loss: 6.067697048187256 = 0.02103428915143013 + 1.0 * 6.0466628074646
Epoch 1010, val loss: 0.9232297539710999
Epoch 1020, training loss: 6.065390586853027 = 0.020456094294786453 + 1.0 * 6.044934272766113
Epoch 1020, val loss: 0.9282458424568176
Epoch 1030, training loss: 6.063514709472656 = 0.019901785999536514 + 1.0 * 6.043612957000732
Epoch 1030, val loss: 0.9331879019737244
Epoch 1040, training loss: 6.064931869506836 = 0.01937101222574711 + 1.0 * 6.045560836791992
Epoch 1040, val loss: 0.9381815791130066
Epoch 1050, training loss: 6.059639930725098 = 0.018859408795833588 + 1.0 * 6.040780544281006
Epoch 1050, val loss: 0.9430519938468933
Epoch 1060, training loss: 6.059208869934082 = 0.018369264900684357 + 1.0 * 6.040839672088623
Epoch 1060, val loss: 0.9479015469551086
Epoch 1070, training loss: 6.0578389167785645 = 0.017896579578518867 + 1.0 * 6.039942264556885
Epoch 1070, val loss: 0.9527179002761841
Epoch 1080, training loss: 6.064756870269775 = 0.017442043870687485 + 1.0 * 6.047314643859863
Epoch 1080, val loss: 0.9575167298316956
Epoch 1090, training loss: 6.060778617858887 = 0.017009157687425613 + 1.0 * 6.043769359588623
Epoch 1090, val loss: 0.962069571018219
Epoch 1100, training loss: 6.056027889251709 = 0.01659066416323185 + 1.0 * 6.039437294006348
Epoch 1100, val loss: 0.9666683673858643
Epoch 1110, training loss: 6.0572638511657715 = 0.016189416870474815 + 1.0 * 6.041074275970459
Epoch 1110, val loss: 0.9711820483207703
Epoch 1120, training loss: 6.052982807159424 = 0.01580497995018959 + 1.0 * 6.037178039550781
Epoch 1120, val loss: 0.9755784869194031
Epoch 1130, training loss: 6.051153182983398 = 0.015430508181452751 + 1.0 * 6.035722732543945
Epoch 1130, val loss: 0.9800290465354919
Epoch 1140, training loss: 6.050520420074463 = 0.015068409033119678 + 1.0 * 6.035451889038086
Epoch 1140, val loss: 0.9844319820404053
Epoch 1150, training loss: 6.062445163726807 = 0.014719615690410137 + 1.0 * 6.047725677490234
Epoch 1150, val loss: 0.9888468384742737
Epoch 1160, training loss: 6.054491996765137 = 0.014385108835995197 + 1.0 * 6.040106773376465
Epoch 1160, val loss: 0.9930727481842041
Epoch 1170, training loss: 6.049941062927246 = 0.014064385555684566 + 1.0 * 6.035876750946045
Epoch 1170, val loss: 0.9972144365310669
Epoch 1180, training loss: 6.0470452308654785 = 0.013754680752754211 + 1.0 * 6.033290386199951
Epoch 1180, val loss: 1.0013507604599
Epoch 1190, training loss: 6.049992084503174 = 0.013452015817165375 + 1.0 * 6.0365400314331055
Epoch 1190, val loss: 1.005489468574524
Epoch 1200, training loss: 6.04574728012085 = 0.013159913010895252 + 1.0 * 6.03258752822876
Epoch 1200, val loss: 1.0096420049667358
Epoch 1210, training loss: 6.045536994934082 = 0.012878269888460636 + 1.0 * 6.032658576965332
Epoch 1210, val loss: 1.0136935710906982
Epoch 1220, training loss: 6.044553279876709 = 0.012604638002812862 + 1.0 * 6.031948566436768
Epoch 1220, val loss: 1.0177228450775146
Epoch 1230, training loss: 6.045880317687988 = 0.012338503263890743 + 1.0 * 6.033541679382324
Epoch 1230, val loss: 1.021735429763794
Epoch 1240, training loss: 6.045860290527344 = 0.012083303183317184 + 1.0 * 6.033776760101318
Epoch 1240, val loss: 1.025782585144043
Epoch 1250, training loss: 6.043966770172119 = 0.011838053353130817 + 1.0 * 6.032128810882568
Epoch 1250, val loss: 1.0296069383621216
Epoch 1260, training loss: 6.042239665985107 = 0.01160176657140255 + 1.0 * 6.030637741088867
Epoch 1260, val loss: 1.033440113067627
Epoch 1270, training loss: 6.042218208312988 = 0.011368809267878532 + 1.0 * 6.030849456787109
Epoch 1270, val loss: 1.0372512340545654
Epoch 1280, training loss: 6.043951034545898 = 0.011144068092107773 + 1.0 * 6.032806873321533
Epoch 1280, val loss: 1.0411138534545898
Epoch 1290, training loss: 6.0411057472229 = 0.010925647802650928 + 1.0 * 6.030179977416992
Epoch 1290, val loss: 1.044952630996704
Epoch 1300, training loss: 6.0434346199035645 = 0.010714170522987843 + 1.0 * 6.032720565795898
Epoch 1300, val loss: 1.0486502647399902
Epoch 1310, training loss: 6.040301322937012 = 0.010508972220122814 + 1.0 * 6.029792308807373
Epoch 1310, val loss: 1.0523236989974976
Epoch 1320, training loss: 6.039093971252441 = 0.010310888290405273 + 1.0 * 6.028783321380615
Epoch 1320, val loss: 1.0559676885604858
Epoch 1330, training loss: 6.037203788757324 = 0.010117671452462673 + 1.0 * 6.02708625793457
Epoch 1330, val loss: 1.0595985651016235
Epoch 1340, training loss: 6.036036014556885 = 0.009928414598107338 + 1.0 * 6.0261077880859375
Epoch 1340, val loss: 1.0631835460662842
Epoch 1350, training loss: 6.036839962005615 = 0.009744353592395782 + 1.0 * 6.027095794677734
Epoch 1350, val loss: 1.0667837858200073
Epoch 1360, training loss: 6.040426731109619 = 0.009566228836774826 + 1.0 * 6.030860424041748
Epoch 1360, val loss: 1.0703123807907104
Epoch 1370, training loss: 6.035467624664307 = 0.00939479935914278 + 1.0 * 6.026072978973389
Epoch 1370, val loss: 1.0737495422363281
Epoch 1380, training loss: 6.03532075881958 = 0.009228713810443878 + 1.0 * 6.026092052459717
Epoch 1380, val loss: 1.0771946907043457
Epoch 1390, training loss: 6.041574478149414 = 0.00906567182391882 + 1.0 * 6.032508850097656
Epoch 1390, val loss: 1.0806232690811157
Epoch 1400, training loss: 6.035348892211914 = 0.008909139782190323 + 1.0 * 6.026439666748047
Epoch 1400, val loss: 1.0838948488235474
Epoch 1410, training loss: 6.034040927886963 = 0.008756395429372787 + 1.0 * 6.025284767150879
Epoch 1410, val loss: 1.087281346321106
Epoch 1420, training loss: 6.0316853523254395 = 0.008607307448983192 + 1.0 * 6.023077964782715
Epoch 1420, val loss: 1.090541958808899
Epoch 1430, training loss: 6.0326924324035645 = 0.008461172692477703 + 1.0 * 6.024231433868408
Epoch 1430, val loss: 1.093843698501587
Epoch 1440, training loss: 6.0384440422058105 = 0.008319739252328873 + 1.0 * 6.030124187469482
Epoch 1440, val loss: 1.097169041633606
Epoch 1450, training loss: 6.031884670257568 = 0.008181169629096985 + 1.0 * 6.023703575134277
Epoch 1450, val loss: 1.1002631187438965
Epoch 1460, training loss: 6.030365943908691 = 0.008047886192798615 + 1.0 * 6.022317886352539
Epoch 1460, val loss: 1.1034152507781982
Epoch 1470, training loss: 6.03054141998291 = 0.007916499860584736 + 1.0 * 6.022624969482422
Epoch 1470, val loss: 1.1065672636032104
Epoch 1480, training loss: 6.033109664916992 = 0.007788166403770447 + 1.0 * 6.0253214836120605
Epoch 1480, val loss: 1.1097233295440674
Epoch 1490, training loss: 6.0286865234375 = 0.007663806434720755 + 1.0 * 6.021022796630859
Epoch 1490, val loss: 1.1128625869750977
Epoch 1500, training loss: 6.0314459800720215 = 0.007542688865214586 + 1.0 * 6.0239033699035645
Epoch 1500, val loss: 1.115957260131836
Epoch 1510, training loss: 6.033966064453125 = 0.007424234878271818 + 1.0 * 6.026541709899902
Epoch 1510, val loss: 1.1189533472061157
Epoch 1520, training loss: 6.029468536376953 = 0.007309580221772194 + 1.0 * 6.022159099578857
Epoch 1520, val loss: 1.122016429901123
Epoch 1530, training loss: 6.029046535491943 = 0.007197390776127577 + 1.0 * 6.021849155426025
Epoch 1530, val loss: 1.1249620914459229
Epoch 1540, training loss: 6.032527923583984 = 0.007087895181030035 + 1.0 * 6.025440216064453
Epoch 1540, val loss: 1.1279650926589966
Epoch 1550, training loss: 6.027769565582275 = 0.006981417071074247 + 1.0 * 6.020788192749023
Epoch 1550, val loss: 1.1308261156082153
Epoch 1560, training loss: 6.026235580444336 = 0.006877170410007238 + 1.0 * 6.0193586349487305
Epoch 1560, val loss: 1.133785605430603
Epoch 1570, training loss: 6.027737140655518 = 0.006774274166673422 + 1.0 * 6.020962715148926
Epoch 1570, val loss: 1.1366734504699707
Epoch 1580, training loss: 6.025915622711182 = 0.006675056181848049 + 1.0 * 6.019240379333496
Epoch 1580, val loss: 1.1395357847213745
Epoch 1590, training loss: 6.024438381195068 = 0.00657893531024456 + 1.0 * 6.01785945892334
Epoch 1590, val loss: 1.1423860788345337
Epoch 1600, training loss: 6.024191379547119 = 0.006484617479145527 + 1.0 * 6.017706871032715
Epoch 1600, val loss: 1.1452035903930664
Epoch 1610, training loss: 6.024413585662842 = 0.006391212344169617 + 1.0 * 6.018022537231445
Epoch 1610, val loss: 1.1479655504226685
Epoch 1620, training loss: 6.030694961547852 = 0.006300571374595165 + 1.0 * 6.024394512176514
Epoch 1620, val loss: 1.150771141052246
Epoch 1630, training loss: 6.02756404876709 = 0.006211696658283472 + 1.0 * 6.021352291107178
Epoch 1630, val loss: 1.1535634994506836
Epoch 1640, training loss: 6.026963710784912 = 0.006125974003225565 + 1.0 * 6.020837783813477
Epoch 1640, val loss: 1.1562227010726929
Epoch 1650, training loss: 6.023569583892822 = 0.006041805259883404 + 1.0 * 6.0175275802612305
Epoch 1650, val loss: 1.1589574813842773
Epoch 1660, training loss: 6.024710178375244 = 0.005958928260952234 + 1.0 * 6.01875114440918
Epoch 1660, val loss: 1.161618709564209
Epoch 1670, training loss: 6.02224063873291 = 0.005878273397684097 + 1.0 * 6.016362190246582
Epoch 1670, val loss: 1.1642920970916748
Epoch 1680, training loss: 6.021311283111572 = 0.005799575243145227 + 1.0 * 6.015511512756348
Epoch 1680, val loss: 1.1669061183929443
Epoch 1690, training loss: 6.022372722625732 = 0.005722637288272381 + 1.0 * 6.016650199890137
Epoch 1690, val loss: 1.1695077419281006
Epoch 1700, training loss: 6.024795055389404 = 0.005646782927215099 + 1.0 * 6.019148349761963
Epoch 1700, val loss: 1.1721298694610596
Epoch 1710, training loss: 6.026648998260498 = 0.0055725048296153545 + 1.0 * 6.021076679229736
Epoch 1710, val loss: 1.1746768951416016
Epoch 1720, training loss: 6.021821975708008 = 0.00550135737285018 + 1.0 * 6.016320705413818
Epoch 1720, val loss: 1.177202820777893
Epoch 1730, training loss: 6.019355773925781 = 0.0054304953664541245 + 1.0 * 6.013925075531006
Epoch 1730, val loss: 1.1797302961349487
Epoch 1740, training loss: 6.019144535064697 = 0.0053604235872626305 + 1.0 * 6.013783931732178
Epoch 1740, val loss: 1.1822706460952759
Epoch 1750, training loss: 6.0229716300964355 = 0.0052917771972715855 + 1.0 * 6.017679691314697
Epoch 1750, val loss: 1.1847575902938843
Epoch 1760, training loss: 6.019412517547607 = 0.005225549917668104 + 1.0 * 6.014186859130859
Epoch 1760, val loss: 1.187334418296814
Epoch 1770, training loss: 6.018937110900879 = 0.005160787142813206 + 1.0 * 6.0137763023376465
Epoch 1770, val loss: 1.1896599531173706
Epoch 1780, training loss: 6.021408557891846 = 0.00509784696623683 + 1.0 * 6.016310691833496
Epoch 1780, val loss: 1.1921179294586182
Epoch 1790, training loss: 6.0204339027404785 = 0.0050353920087218285 + 1.0 * 6.0153985023498535
Epoch 1790, val loss: 1.1944698095321655
Epoch 1800, training loss: 6.018228530883789 = 0.00497449841350317 + 1.0 * 6.013254165649414
Epoch 1800, val loss: 1.1968648433685303
Epoch 1810, training loss: 6.017345905303955 = 0.004915101919323206 + 1.0 * 6.012430667877197
Epoch 1810, val loss: 1.19921875
Epoch 1820, training loss: 6.016345024108887 = 0.004855490289628506 + 1.0 * 6.011489391326904
Epoch 1820, val loss: 1.201587200164795
Epoch 1830, training loss: 6.0169878005981445 = 0.00479701766744256 + 1.0 * 6.012190818786621
Epoch 1830, val loss: 1.203983187675476
Epoch 1840, training loss: 6.031301975250244 = 0.004739953204989433 + 1.0 * 6.026562213897705
Epoch 1840, val loss: 1.2063738107681274
Epoch 1850, training loss: 6.019863605499268 = 0.0046857744455337524 + 1.0 * 6.0151777267456055
Epoch 1850, val loss: 1.208661675453186
Epoch 1860, training loss: 6.016707897186279 = 0.004632708616554737 + 1.0 * 6.012075424194336
Epoch 1860, val loss: 1.210777997970581
Epoch 1870, training loss: 6.015128135681152 = 0.004579800181090832 + 1.0 * 6.010548114776611
Epoch 1870, val loss: 1.21304452419281
Epoch 1880, training loss: 6.015197277069092 = 0.004527050070464611 + 1.0 * 6.010670185089111
Epoch 1880, val loss: 1.2153527736663818
Epoch 1890, training loss: 6.021947383880615 = 0.004475627094507217 + 1.0 * 6.017471790313721
Epoch 1890, val loss: 1.2176579236984253
Epoch 1900, training loss: 6.018686294555664 = 0.004424595274031162 + 1.0 * 6.014261722564697
Epoch 1900, val loss: 1.2198675870895386
Epoch 1910, training loss: 6.016924858093262 = 0.00437607616186142 + 1.0 * 6.012548923492432
Epoch 1910, val loss: 1.2220574617385864
Epoch 1920, training loss: 6.017752170562744 = 0.004327529575675726 + 1.0 * 6.013424873352051
Epoch 1920, val loss: 1.2242566347122192
Epoch 1930, training loss: 6.014443397521973 = 0.004280441906303167 + 1.0 * 6.010162830352783
Epoch 1930, val loss: 1.2263630628585815
Epoch 1940, training loss: 6.014220237731934 = 0.004233531653881073 + 1.0 * 6.009986877441406
Epoch 1940, val loss: 1.2285033464431763
Epoch 1950, training loss: 6.01290225982666 = 0.0041874973103404045 + 1.0 * 6.00871467590332
Epoch 1950, val loss: 1.2307063341140747
Epoch 1960, training loss: 6.021709442138672 = 0.004142449703067541 + 1.0 * 6.017567157745361
Epoch 1960, val loss: 1.2328497171401978
Epoch 1970, training loss: 6.013874053955078 = 0.004098074976354837 + 1.0 * 6.0097761154174805
Epoch 1970, val loss: 1.2349110841751099
Epoch 1980, training loss: 6.0125203132629395 = 0.004055181052535772 + 1.0 * 6.00846529006958
Epoch 1980, val loss: 1.2370253801345825
Epoch 1990, training loss: 6.012991428375244 = 0.0040126279927790165 + 1.0 * 6.008978843688965
Epoch 1990, val loss: 1.2390786409378052
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7454
Flip ASR: 0.6978/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.325882911682129 = 1.9520100355148315 + 1.0 * 8.373872756958008
Epoch 0, val loss: 1.9516680240631104
Epoch 10, training loss: 10.314043045043945 = 1.9409613609313965 + 1.0 * 8.373082160949707
Epoch 10, val loss: 1.9397330284118652
Epoch 20, training loss: 10.296749114990234 = 1.9277698993682861 + 1.0 * 8.368979454040527
Epoch 20, val loss: 1.9250766038894653
Epoch 30, training loss: 10.262556076049805 = 1.9098926782608032 + 1.0 * 8.352663040161133
Epoch 30, val loss: 1.9047528505325317
Epoch 40, training loss: 10.144129753112793 = 1.8861750364303589 + 1.0 * 8.257954597473145
Epoch 40, val loss: 1.8777847290039062
Epoch 50, training loss: 9.728297233581543 = 1.862259864807129 + 1.0 * 7.866037368774414
Epoch 50, val loss: 1.8507025241851807
Epoch 60, training loss: 9.409686088562012 = 1.838295578956604 + 1.0 * 7.571390151977539
Epoch 60, val loss: 1.8241077661514282
Epoch 70, training loss: 8.988286972045898 = 1.8148205280303955 + 1.0 * 7.173466682434082
Epoch 70, val loss: 1.8000364303588867
Epoch 80, training loss: 8.656707763671875 = 1.7928813695907593 + 1.0 * 6.863826274871826
Epoch 80, val loss: 1.779545783996582
Epoch 90, training loss: 8.480609893798828 = 1.7733370065689087 + 1.0 * 6.707273006439209
Epoch 90, val loss: 1.7609821557998657
Epoch 100, training loss: 8.351774215698242 = 1.7537546157836914 + 1.0 * 6.598019599914551
Epoch 100, val loss: 1.7425223588943481
Epoch 110, training loss: 8.237204551696777 = 1.7337647676467896 + 1.0 * 6.503439903259277
Epoch 110, val loss: 1.724539875984192
Epoch 120, training loss: 8.150321960449219 = 1.712547779083252 + 1.0 * 6.437774658203125
Epoch 120, val loss: 1.7059444189071655
Epoch 130, training loss: 8.080574035644531 = 1.6883938312530518 + 1.0 * 6.392180442810059
Epoch 130, val loss: 1.6852220296859741
Epoch 140, training loss: 8.016793251037598 = 1.6607171297073364 + 1.0 * 6.356075763702393
Epoch 140, val loss: 1.6620234251022339
Epoch 150, training loss: 7.956892013549805 = 1.6286574602127075 + 1.0 * 6.328234672546387
Epoch 150, val loss: 1.63554048538208
Epoch 160, training loss: 7.898039817810059 = 1.5916451215744019 + 1.0 * 6.306394577026367
Epoch 160, val loss: 1.6055026054382324
Epoch 170, training loss: 7.838003158569336 = 1.5496388673782349 + 1.0 * 6.288364410400391
Epoch 170, val loss: 1.571556806564331
Epoch 180, training loss: 7.7731757164001465 = 1.5023384094238281 + 1.0 * 6.270837306976318
Epoch 180, val loss: 1.5337755680084229
Epoch 190, training loss: 7.7076497077941895 = 1.4507412910461426 + 1.0 * 6.256908416748047
Epoch 190, val loss: 1.4933079481124878
Epoch 200, training loss: 7.640769958496094 = 1.3963897228240967 + 1.0 * 6.244379997253418
Epoch 200, val loss: 1.4516031742095947
Epoch 210, training loss: 7.575006008148193 = 1.341334342956543 + 1.0 * 6.23367166519165
Epoch 210, val loss: 1.4103021621704102
Epoch 220, training loss: 7.5109405517578125 = 1.286945104598999 + 1.0 * 6.223995208740234
Epoch 220, val loss: 1.3712931871414185
Epoch 230, training loss: 7.448625087738037 = 1.2347081899642944 + 1.0 * 6.213916778564453
Epoch 230, val loss: 1.3346538543701172
Epoch 240, training loss: 7.390956401824951 = 1.184662938117981 + 1.0 * 6.20629358291626
Epoch 240, val loss: 1.3005238771438599
Epoch 250, training loss: 7.337769031524658 = 1.1376155614852905 + 1.0 * 6.200153350830078
Epoch 250, val loss: 1.2688288688659668
Epoch 260, training loss: 7.2873945236206055 = 1.0935065746307373 + 1.0 * 6.193888187408447
Epoch 260, val loss: 1.2397066354751587
Epoch 270, training loss: 7.239133358001709 = 1.0519083738327026 + 1.0 * 6.187224864959717
Epoch 270, val loss: 1.212323546409607
Epoch 280, training loss: 7.193556785583496 = 1.0121238231658936 + 1.0 * 6.181433200836182
Epoch 280, val loss: 1.1861940622329712
Epoch 290, training loss: 7.160205364227295 = 0.9739858508110046 + 1.0 * 6.186219692230225
Epoch 290, val loss: 1.1613051891326904
Epoch 300, training loss: 7.112318992614746 = 0.9381253123283386 + 1.0 * 6.174193859100342
Epoch 300, val loss: 1.137686848640442
Epoch 310, training loss: 7.070677280426025 = 0.903518795967102 + 1.0 * 6.167158603668213
Epoch 310, val loss: 1.1151081323623657
Epoch 320, training loss: 7.0326032638549805 = 0.8696577548980713 + 1.0 * 6.16294527053833
Epoch 320, val loss: 1.0930790901184082
Epoch 330, training loss: 6.996757507324219 = 0.8362149596214294 + 1.0 * 6.1605424880981445
Epoch 330, val loss: 1.0713471174240112
Epoch 340, training loss: 6.958608150482178 = 0.8030511736869812 + 1.0 * 6.155557155609131
Epoch 340, val loss: 1.049821376800537
Epoch 350, training loss: 6.923076629638672 = 0.7698379755020142 + 1.0 * 6.153238773345947
Epoch 350, val loss: 1.028282642364502
Epoch 360, training loss: 6.887588977813721 = 0.7369308471679688 + 1.0 * 6.150658130645752
Epoch 360, val loss: 1.0065124034881592
Epoch 370, training loss: 6.850252628326416 = 0.7043089866638184 + 1.0 * 6.145943641662598
Epoch 370, val loss: 0.985213577747345
Epoch 380, training loss: 6.813838005065918 = 0.6722103357315063 + 1.0 * 6.141627788543701
Epoch 380, val loss: 0.9642199277877808
Epoch 390, training loss: 6.793190002441406 = 0.6407944560050964 + 1.0 * 6.152395725250244
Epoch 390, val loss: 0.9435316920280457
Epoch 400, training loss: 6.752103328704834 = 0.6105950474739075 + 1.0 * 6.141508102416992
Epoch 400, val loss: 0.9238711595535278
Epoch 410, training loss: 6.715913772583008 = 0.5816729068756104 + 1.0 * 6.134240627288818
Epoch 410, val loss: 0.9054611325263977
Epoch 420, training loss: 6.684014320373535 = 0.5537387132644653 + 1.0 * 6.130275726318359
Epoch 420, val loss: 0.8881325125694275
Epoch 430, training loss: 6.656265735626221 = 0.526863157749176 + 1.0 * 6.1294026374816895
Epoch 430, val loss: 0.8717689514160156
Epoch 440, training loss: 6.627575874328613 = 0.5011410117149353 + 1.0 * 6.126434803009033
Epoch 440, val loss: 0.8566504120826721
Epoch 450, training loss: 6.600347518920898 = 0.4766993224620819 + 1.0 * 6.123648166656494
Epoch 450, val loss: 0.8427276015281677
Epoch 460, training loss: 6.574079990386963 = 0.45316779613494873 + 1.0 * 6.120912075042725
Epoch 460, val loss: 0.8301377892494202
Epoch 470, training loss: 6.552289009094238 = 0.4304860234260559 + 1.0 * 6.121802806854248
Epoch 470, val loss: 0.8187857270240784
Epoch 480, training loss: 6.544382095336914 = 0.4087379276752472 + 1.0 * 6.13564395904541
Epoch 480, val loss: 0.8084185719490051
Epoch 490, training loss: 6.504173278808594 = 0.3878287076950073 + 1.0 * 6.116344451904297
Epoch 490, val loss: 0.7994059324264526
Epoch 500, training loss: 6.481198310852051 = 0.36773574352264404 + 1.0 * 6.113462448120117
Epoch 500, val loss: 0.7915254831314087
Epoch 510, training loss: 6.458269119262695 = 0.34833264350891113 + 1.0 * 6.109936714172363
Epoch 510, val loss: 0.7846401929855347
Epoch 520, training loss: 6.437785625457764 = 0.32956647872924805 + 1.0 * 6.108219146728516
Epoch 520, val loss: 0.7787296175956726
Epoch 530, training loss: 6.4249372482299805 = 0.3115154504776001 + 1.0 * 6.11342191696167
Epoch 530, val loss: 0.7737259268760681
Epoch 540, training loss: 6.400967597961426 = 0.29432767629623413 + 1.0 * 6.106639862060547
Epoch 540, val loss: 0.7698327898979187
Epoch 550, training loss: 6.381425857543945 = 0.27797621488571167 + 1.0 * 6.103449821472168
Epoch 550, val loss: 0.7669001817703247
Epoch 560, training loss: 6.376591682434082 = 0.26249513030052185 + 1.0 * 6.114096641540527
Epoch 560, val loss: 0.7648162841796875
Epoch 570, training loss: 6.34966516494751 = 0.24783146381378174 + 1.0 * 6.101833820343018
Epoch 570, val loss: 0.763655960559845
Epoch 580, training loss: 6.332243919372559 = 0.2340630739927292 + 1.0 * 6.098180770874023
Epoch 580, val loss: 0.763401210308075
Epoch 590, training loss: 6.3170576095581055 = 0.22104224562644958 + 1.0 * 6.096015453338623
Epoch 590, val loss: 0.7637887001037598
Epoch 600, training loss: 6.3070902824401855 = 0.20871730148792267 + 1.0 * 6.098372936248779
Epoch 600, val loss: 0.7649128437042236
Epoch 610, training loss: 6.299054145812988 = 0.19714950025081635 + 1.0 * 6.10190486907959
Epoch 610, val loss: 0.7666353583335876
Epoch 620, training loss: 6.278005599975586 = 0.18639715015888214 + 1.0 * 6.09160852432251
Epoch 620, val loss: 0.768899142742157
Epoch 630, training loss: 6.267369270324707 = 0.176273912191391 + 1.0 * 6.091095447540283
Epoch 630, val loss: 0.7717872858047485
Epoch 640, training loss: 6.256663799285889 = 0.1667737513780594 + 1.0 * 6.089890003204346
Epoch 640, val loss: 0.7751520872116089
Epoch 650, training loss: 6.249946594238281 = 0.15787836909294128 + 1.0 * 6.092068195343018
Epoch 650, val loss: 0.7789453864097595
Epoch 660, training loss: 6.235944747924805 = 0.1496197134256363 + 1.0 * 6.086325168609619
Epoch 660, val loss: 0.7831037044525146
Epoch 670, training loss: 6.227678298950195 = 0.14187581837177277 + 1.0 * 6.0858025550842285
Epoch 670, val loss: 0.7877107858657837
Epoch 680, training loss: 6.217659950256348 = 0.13460281491279602 + 1.0 * 6.083056926727295
Epoch 680, val loss: 0.7926446795463562
Epoch 690, training loss: 6.215176582336426 = 0.1277541220188141 + 1.0 * 6.0874223709106445
Epoch 690, val loss: 0.7978894710540771
Epoch 700, training loss: 6.210047245025635 = 0.12131045758724213 + 1.0 * 6.0887370109558105
Epoch 700, val loss: 0.8033864498138428
Epoch 710, training loss: 6.197388648986816 = 0.1153387501835823 + 1.0 * 6.08204984664917
Epoch 710, val loss: 0.8090623021125793
Epoch 720, training loss: 6.189172267913818 = 0.10969525575637817 + 1.0 * 6.079476833343506
Epoch 720, val loss: 0.8151435852050781
Epoch 730, training loss: 6.181591033935547 = 0.1044006273150444 + 1.0 * 6.077190399169922
Epoch 730, val loss: 0.8213598132133484
Epoch 740, training loss: 6.1747965812683105 = 0.09937863796949387 + 1.0 * 6.075417995452881
Epoch 740, val loss: 0.8277807235717773
Epoch 750, training loss: 6.182633399963379 = 0.09463519603013992 + 1.0 * 6.087998390197754
Epoch 750, val loss: 0.8343552350997925
Epoch 760, training loss: 6.168117523193359 = 0.09017195552587509 + 1.0 * 6.077945709228516
Epoch 760, val loss: 0.8409463763237
Epoch 770, training loss: 6.159769535064697 = 0.08596926927566528 + 1.0 * 6.073800086975098
Epoch 770, val loss: 0.847722589969635
Epoch 780, training loss: 6.159900665283203 = 0.08198779821395874 + 1.0 * 6.0779128074646
Epoch 780, val loss: 0.8545611500740051
Epoch 790, training loss: 6.148531436920166 = 0.07825629413127899 + 1.0 * 6.07027530670166
Epoch 790, val loss: 0.861393928527832
Epoch 800, training loss: 6.145062446594238 = 0.07471536099910736 + 1.0 * 6.070347309112549
Epoch 800, val loss: 0.8683459758758545
Epoch 810, training loss: 6.1412787437438965 = 0.07136384397745132 + 1.0 * 6.069914817810059
Epoch 810, val loss: 0.8753448724746704
Epoch 820, training loss: 6.1443305015563965 = 0.06820814311504364 + 1.0 * 6.076122283935547
Epoch 820, val loss: 0.8823992013931274
Epoch 830, training loss: 6.132865905761719 = 0.06523621827363968 + 1.0 * 6.067629814147949
Epoch 830, val loss: 0.889427661895752
Epoch 840, training loss: 6.129400253295898 = 0.06243376061320305 + 1.0 * 6.066966533660889
Epoch 840, val loss: 0.896575391292572
Epoch 850, training loss: 6.1391825675964355 = 0.05979672074317932 + 1.0 * 6.079385757446289
Epoch 850, val loss: 0.9035667181015015
Epoch 860, training loss: 6.121864318847656 = 0.05729909613728523 + 1.0 * 6.064565181732178
Epoch 860, val loss: 0.910540759563446
Epoch 870, training loss: 6.119133472442627 = 0.05494353920221329 + 1.0 * 6.064189910888672
Epoch 870, val loss: 0.9175595641136169
Epoch 880, training loss: 6.113916873931885 = 0.0527179129421711 + 1.0 * 6.061199188232422
Epoch 880, val loss: 0.9245631694793701
Epoch 890, training loss: 6.110989093780518 = 0.050600022077560425 + 1.0 * 6.060389041900635
Epoch 890, val loss: 0.9315407276153564
Epoch 900, training loss: 6.113320350646973 = 0.04857689514756203 + 1.0 * 6.064743518829346
Epoch 900, val loss: 0.9384694695472717
Epoch 910, training loss: 6.111383438110352 = 0.046665266156196594 + 1.0 * 6.064718246459961
Epoch 910, val loss: 0.9452922344207764
Epoch 920, training loss: 6.103157997131348 = 0.04486933723092079 + 1.0 * 6.05828857421875
Epoch 920, val loss: 0.9521633386611938
Epoch 930, training loss: 6.101576328277588 = 0.043160580098629 + 1.0 * 6.05841588973999
Epoch 930, val loss: 0.9590005278587341
Epoch 940, training loss: 6.105926036834717 = 0.04154141992330551 + 1.0 * 6.064384460449219
Epoch 940, val loss: 0.9657573699951172
Epoch 950, training loss: 6.102171897888184 = 0.04001178592443466 + 1.0 * 6.062160015106201
Epoch 950, val loss: 0.9722638130187988
Epoch 960, training loss: 6.1035237312316895 = 0.038565948605537415 + 1.0 * 6.064957618713379
Epoch 960, val loss: 0.9788467884063721
Epoch 970, training loss: 6.09401798248291 = 0.03718165308237076 + 1.0 * 6.056836128234863
Epoch 970, val loss: 0.9853441119194031
Epoch 980, training loss: 6.090692043304443 = 0.03588106855750084 + 1.0 * 6.054811000823975
Epoch 980, val loss: 0.9918274879455566
Epoch 990, training loss: 6.087334156036377 = 0.03463978320360184 + 1.0 * 6.052694320678711
Epoch 990, val loss: 0.9982165098190308
Epoch 1000, training loss: 6.087373733520508 = 0.03345324844121933 + 1.0 * 6.053920269012451
Epoch 1000, val loss: 1.004554033279419
Epoch 1010, training loss: 6.093387126922607 = 0.032323673367500305 + 1.0 * 6.061063289642334
Epoch 1010, val loss: 1.0106562376022339
Epoch 1020, training loss: 6.084945201873779 = 0.031265970319509506 + 1.0 * 6.053679466247559
Epoch 1020, val loss: 1.0167204141616821
Epoch 1030, training loss: 6.080606460571289 = 0.03025190904736519 + 1.0 * 6.050354480743408
Epoch 1030, val loss: 1.0228021144866943
Epoch 1040, training loss: 6.07877779006958 = 0.029281433671712875 + 1.0 * 6.049496173858643
Epoch 1040, val loss: 1.0288549661636353
Epoch 1050, training loss: 6.077917098999023 = 0.02835301123559475 + 1.0 * 6.049563884735107
Epoch 1050, val loss: 1.0347777605056763
Epoch 1060, training loss: 6.081889629364014 = 0.027463003993034363 + 1.0 * 6.054426670074463
Epoch 1060, val loss: 1.0405267477035522
Epoch 1070, training loss: 6.075905799865723 = 0.02662513218820095 + 1.0 * 6.049280643463135
Epoch 1070, val loss: 1.0462450981140137
Epoch 1080, training loss: 6.076208114624023 = 0.02581760659813881 + 1.0 * 6.050390720367432
Epoch 1080, val loss: 1.0519667863845825
Epoch 1090, training loss: 6.074155330657959 = 0.02504638582468033 + 1.0 * 6.049108982086182
Epoch 1090, val loss: 1.0575796365737915
Epoch 1100, training loss: 6.0702056884765625 = 0.024312064051628113 + 1.0 * 6.045893669128418
Epoch 1100, val loss: 1.0631568431854248
Epoch 1110, training loss: 6.0687174797058105 = 0.023607352748513222 + 1.0 * 6.04511022567749
Epoch 1110, val loss: 1.0686649084091187
Epoch 1120, training loss: 6.073795795440674 = 0.022931182757019997 + 1.0 * 6.0508646965026855
Epoch 1120, val loss: 1.0740821361541748
Epoch 1130, training loss: 6.070369720458984 = 0.02228253148496151 + 1.0 * 6.048087120056152
Epoch 1130, val loss: 1.0794368982315063
Epoch 1140, training loss: 6.082902431488037 = 0.021670065820217133 + 1.0 * 6.061232566833496
Epoch 1140, val loss: 1.0846054553985596
Epoch 1150, training loss: 6.069943904876709 = 0.021069122478365898 + 1.0 * 6.048874855041504
Epoch 1150, val loss: 1.089747667312622
Epoch 1160, training loss: 6.064332008361816 = 0.020509829744696617 + 1.0 * 6.043822288513184
Epoch 1160, val loss: 1.0949122905731201
Epoch 1170, training loss: 6.061595916748047 = 0.019962206482887268 + 1.0 * 6.041633605957031
Epoch 1170, val loss: 1.1000263690948486
Epoch 1180, training loss: 6.060128211975098 = 0.0194388460367918 + 1.0 * 6.040689468383789
Epoch 1180, val loss: 1.1050835847854614
Epoch 1190, training loss: 6.059098243713379 = 0.018932469189167023 + 1.0 * 6.040165901184082
Epoch 1190, val loss: 1.1100772619247437
Epoch 1200, training loss: 6.071680545806885 = 0.018447427079081535 + 1.0 * 6.0532331466674805
Epoch 1200, val loss: 1.1149522066116333
Epoch 1210, training loss: 6.062921047210693 = 0.017973855137825012 + 1.0 * 6.044947147369385
Epoch 1210, val loss: 1.1196362972259521
Epoch 1220, training loss: 6.05634880065918 = 0.017528757452964783 + 1.0 * 6.038820266723633
Epoch 1220, val loss: 1.1244601011276245
Epoch 1230, training loss: 6.05649995803833 = 0.017098892480134964 + 1.0 * 6.039401054382324
Epoch 1230, val loss: 1.1292858123779297
Epoch 1240, training loss: 6.057601451873779 = 0.01668144203722477 + 1.0 * 6.040919780731201
Epoch 1240, val loss: 1.1339725255966187
Epoch 1250, training loss: 6.053706645965576 = 0.016277536749839783 + 1.0 * 6.037429332733154
Epoch 1250, val loss: 1.1385201215744019
Epoch 1260, training loss: 6.055070877075195 = 0.015888504683971405 + 1.0 * 6.039182186126709
Epoch 1260, val loss: 1.143084168434143
Epoch 1270, training loss: 6.058373928070068 = 0.015516095794737339 + 1.0 * 6.042857646942139
Epoch 1270, val loss: 1.147538661956787
Epoch 1280, training loss: 6.052016735076904 = 0.015152853913605213 + 1.0 * 6.036863803863525
Epoch 1280, val loss: 1.151900291442871
Epoch 1290, training loss: 6.050519943237305 = 0.014805729500949383 + 1.0 * 6.035714149475098
Epoch 1290, val loss: 1.156341314315796
Epoch 1300, training loss: 6.049543380737305 = 0.014470461755990982 + 1.0 * 6.0350728034973145
Epoch 1300, val loss: 1.1607221364974976
Epoch 1310, training loss: 6.052453517913818 = 0.014145348221063614 + 1.0 * 6.038308143615723
Epoch 1310, val loss: 1.1650452613830566
Epoch 1320, training loss: 6.049010753631592 = 0.013832811266183853 + 1.0 * 6.035177707672119
Epoch 1320, val loss: 1.1692274808883667
Epoch 1330, training loss: 6.050246715545654 = 0.013531160540878773 + 1.0 * 6.036715507507324
Epoch 1330, val loss: 1.1734346151351929
Epoch 1340, training loss: 6.048436164855957 = 0.013239460997283459 + 1.0 * 6.035196781158447
Epoch 1340, val loss: 1.177520751953125
Epoch 1350, training loss: 6.047019004821777 = 0.012959406711161137 + 1.0 * 6.034059524536133
Epoch 1350, val loss: 1.1816802024841309
Epoch 1360, training loss: 6.045028209686279 = 0.012685682624578476 + 1.0 * 6.032342433929443
Epoch 1360, val loss: 1.185744285583496
Epoch 1370, training loss: 6.052340507507324 = 0.012420482002198696 + 1.0 * 6.039919853210449
Epoch 1370, val loss: 1.1897695064544678
Epoch 1380, training loss: 6.05236291885376 = 0.012167653068900108 + 1.0 * 6.040195465087891
Epoch 1380, val loss: 1.1935429573059082
Epoch 1390, training loss: 6.044495105743408 = 0.01192244328558445 + 1.0 * 6.0325727462768555
Epoch 1390, val loss: 1.19745671749115
Epoch 1400, training loss: 6.0417022705078125 = 0.01168430969119072 + 1.0 * 6.030017852783203
Epoch 1400, val loss: 1.2014083862304688
Epoch 1410, training loss: 6.04122257232666 = 0.01145139615982771 + 1.0 * 6.029771327972412
Epoch 1410, val loss: 1.2053029537200928
Epoch 1420, training loss: 6.045097827911377 = 0.011225107125937939 + 1.0 * 6.033872604370117
Epoch 1420, val loss: 1.2091014385223389
Epoch 1430, training loss: 6.040397644042969 = 0.011006686836481094 + 1.0 * 6.029390811920166
Epoch 1430, val loss: 1.2127271890640259
Epoch 1440, training loss: 6.046431064605713 = 0.010794471018016338 + 1.0 * 6.0356364250183105
Epoch 1440, val loss: 1.2164448499679565
Epoch 1450, training loss: 6.042426586151123 = 0.010591247119009495 + 1.0 * 6.031835556030273
Epoch 1450, val loss: 1.220025658607483
Epoch 1460, training loss: 6.038978576660156 = 0.010394101031124592 + 1.0 * 6.0285844802856445
Epoch 1460, val loss: 1.2236411571502686
Epoch 1470, training loss: 6.037783145904541 = 0.010200005024671555 + 1.0 * 6.027583122253418
Epoch 1470, val loss: 1.2273130416870117
Epoch 1480, training loss: 6.038292407989502 = 0.010012071579694748 + 1.0 * 6.028280258178711
Epoch 1480, val loss: 1.2309112548828125
Epoch 1490, training loss: 6.040215969085693 = 0.009829932823777199 + 1.0 * 6.030385971069336
Epoch 1490, val loss: 1.2342725992202759
Epoch 1500, training loss: 6.0364789962768555 = 0.00965376477688551 + 1.0 * 6.026825428009033
Epoch 1500, val loss: 1.2377046346664429
Epoch 1510, training loss: 6.0366363525390625 = 0.009481831453740597 + 1.0 * 6.027154445648193
Epoch 1510, val loss: 1.2411813735961914
Epoch 1520, training loss: 6.03981876373291 = 0.00931533146649599 + 1.0 * 6.030503273010254
Epoch 1520, val loss: 1.2446284294128418
Epoch 1530, training loss: 6.035501003265381 = 0.009151667356491089 + 1.0 * 6.0263495445251465
Epoch 1530, val loss: 1.24789559841156
Epoch 1540, training loss: 6.036283493041992 = 0.00899206381291151 + 1.0 * 6.027291297912598
Epoch 1540, val loss: 1.2512006759643555
Epoch 1550, training loss: 6.034956932067871 = 0.008838153444230556 + 1.0 * 6.026118755340576
Epoch 1550, val loss: 1.2545547485351562
Epoch 1560, training loss: 6.035698890686035 = 0.008687752299010754 + 1.0 * 6.027010917663574
Epoch 1560, val loss: 1.2577126026153564
Epoch 1570, training loss: 6.033388614654541 = 0.008543717674911022 + 1.0 * 6.024845123291016
Epoch 1570, val loss: 1.260937213897705
Epoch 1580, training loss: 6.0320329666137695 = 0.00840186420828104 + 1.0 * 6.0236310958862305
Epoch 1580, val loss: 1.2641804218292236
Epoch 1590, training loss: 6.031907558441162 = 0.008263057097792625 + 1.0 * 6.02364444732666
Epoch 1590, val loss: 1.2673743963241577
Epoch 1600, training loss: 6.0485520362854 = 0.008126933127641678 + 1.0 * 6.0404253005981445
Epoch 1600, val loss: 1.2703642845153809
Epoch 1610, training loss: 6.031110763549805 = 0.007999350316822529 + 1.0 * 6.023111343383789
Epoch 1610, val loss: 1.2732064723968506
Epoch 1620, training loss: 6.031279563903809 = 0.00787291768938303 + 1.0 * 6.023406505584717
Epoch 1620, val loss: 1.2763664722442627
Epoch 1630, training loss: 6.029478549957275 = 0.007748502306640148 + 1.0 * 6.021729946136475
Epoch 1630, val loss: 1.2794439792633057
Epoch 1640, training loss: 6.0289201736450195 = 0.0076272329315543175 + 1.0 * 6.0212931632995605
Epoch 1640, val loss: 1.2824870347976685
Epoch 1650, training loss: 6.037457466125488 = 0.007507469970732927 + 1.0 * 6.029950141906738
Epoch 1650, val loss: 1.2853950262069702
Epoch 1660, training loss: 6.032324314117432 = 0.007392847444862127 + 1.0 * 6.02493143081665
Epoch 1660, val loss: 1.2881568670272827
Epoch 1670, training loss: 6.030391216278076 = 0.0072808535769581795 + 1.0 * 6.023110389709473
Epoch 1670, val loss: 1.2910488843917847
Epoch 1680, training loss: 6.031167984008789 = 0.007172053679823875 + 1.0 * 6.023995876312256
Epoch 1680, val loss: 1.2939331531524658
Epoch 1690, training loss: 6.028665542602539 = 0.007064698729664087 + 1.0 * 6.021600723266602
Epoch 1690, val loss: 1.2968146800994873
Epoch 1700, training loss: 6.027899742126465 = 0.006960459519177675 + 1.0 * 6.020939350128174
Epoch 1700, val loss: 1.2996182441711426
Epoch 1710, training loss: 6.030731201171875 = 0.00685883266851306 + 1.0 * 6.023872375488281
Epoch 1710, val loss: 1.3024131059646606
Epoch 1720, training loss: 6.029252529144287 = 0.0067594959400594234 + 1.0 * 6.0224928855896
Epoch 1720, val loss: 1.305083155632019
Epoch 1730, training loss: 6.027151107788086 = 0.006660541985183954 + 1.0 * 6.020490646362305
Epoch 1730, val loss: 1.3077905178070068
Epoch 1740, training loss: 6.025579929351807 = 0.006565949879586697 + 1.0 * 6.01901388168335
Epoch 1740, val loss: 1.3105378150939941
Epoch 1750, training loss: 6.030971527099609 = 0.00647337781265378 + 1.0 * 6.024497985839844
Epoch 1750, val loss: 1.313192367553711
Epoch 1760, training loss: 6.025460243225098 = 0.006382239516824484 + 1.0 * 6.019077777862549
Epoch 1760, val loss: 1.3157933950424194
Epoch 1770, training loss: 6.0295538902282715 = 0.006294106598943472 + 1.0 * 6.02325963973999
Epoch 1770, val loss: 1.318377137184143
Epoch 1780, training loss: 6.024045944213867 = 0.006207441445440054 + 1.0 * 6.017838478088379
Epoch 1780, val loss: 1.3210327625274658
Epoch 1790, training loss: 6.02414083480835 = 0.0061226231046020985 + 1.0 * 6.0180182456970215
Epoch 1790, val loss: 1.3236016035079956
Epoch 1800, training loss: 6.027835845947266 = 0.0060404641553759575 + 1.0 * 6.021795272827148
Epoch 1800, val loss: 1.3261771202087402
Epoch 1810, training loss: 6.024720191955566 = 0.0059593175537884235 + 1.0 * 6.018760681152344
Epoch 1810, val loss: 1.3286470174789429
Epoch 1820, training loss: 6.0232014656066895 = 0.005878603085875511 + 1.0 * 6.017323017120361
Epoch 1820, val loss: 1.3311641216278076
Epoch 1830, training loss: 6.024354934692383 = 0.005800866521894932 + 1.0 * 6.018554210662842
Epoch 1830, val loss: 1.3336775302886963
Epoch 1840, training loss: 6.027335166931152 = 0.0057256887666881084 + 1.0 * 6.021609306335449
Epoch 1840, val loss: 1.3360800743103027
Epoch 1850, training loss: 6.022943496704102 = 0.005651588086038828 + 1.0 * 6.017292022705078
Epoch 1850, val loss: 1.3384943008422852
Epoch 1860, training loss: 6.022099018096924 = 0.005578803829848766 + 1.0 * 6.016520023345947
Epoch 1860, val loss: 1.3409334421157837
Epoch 1870, training loss: 6.02408504486084 = 0.005508281756192446 + 1.0 * 6.018576622009277
Epoch 1870, val loss: 1.3433589935302734
Epoch 1880, training loss: 6.024016380310059 = 0.005438425112515688 + 1.0 * 6.018578052520752
Epoch 1880, val loss: 1.3456778526306152
Epoch 1890, training loss: 6.02324104309082 = 0.0053699081763625145 + 1.0 * 6.017870903015137
Epoch 1890, val loss: 1.3479528427124023
Epoch 1900, training loss: 6.0207719802856445 = 0.005303011275827885 + 1.0 * 6.015469074249268
Epoch 1900, val loss: 1.3502936363220215
Epoch 1910, training loss: 6.020604133605957 = 0.005237954203039408 + 1.0 * 6.015366077423096
Epoch 1910, val loss: 1.3526679277420044
Epoch 1920, training loss: 6.026671886444092 = 0.005174171179533005 + 1.0 * 6.02149772644043
Epoch 1920, val loss: 1.354949951171875
Epoch 1930, training loss: 6.024578094482422 = 0.005111825652420521 + 1.0 * 6.019466400146484
Epoch 1930, val loss: 1.3570892810821533
Epoch 1940, training loss: 6.022558212280273 = 0.005050669889897108 + 1.0 * 6.017507553100586
Epoch 1940, val loss: 1.3592103719711304
Epoch 1950, training loss: 6.019757270812988 = 0.004989685025066137 + 1.0 * 6.014767646789551
Epoch 1950, val loss: 1.3613497018814087
Epoch 1960, training loss: 6.0179338455200195 = 0.004932051058858633 + 1.0 * 6.013001918792725
Epoch 1960, val loss: 1.3636468648910522
Epoch 1970, training loss: 6.017152309417725 = 0.004874085541814566 + 1.0 * 6.012278079986572
Epoch 1970, val loss: 1.3658883571624756
Epoch 1980, training loss: 6.022968769073486 = 0.004817171487957239 + 1.0 * 6.018151760101318
Epoch 1980, val loss: 1.3679953813552856
Epoch 1990, training loss: 6.017317295074463 = 0.004761068616062403 + 1.0 * 6.012556076049805
Epoch 1990, val loss: 1.369954228401184
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8007
Flip ASR: 0.7644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.320416450500488 = 1.9465360641479492 + 1.0 * 8.373880386352539
Epoch 0, val loss: 1.9454675912857056
Epoch 10, training loss: 10.30876636505127 = 1.9353382587432861 + 1.0 * 8.373428344726562
Epoch 10, val loss: 1.9344326257705688
Epoch 20, training loss: 10.291101455688477 = 1.9209367036819458 + 1.0 * 8.37016487121582
Epoch 20, val loss: 1.919661283493042
Epoch 30, training loss: 10.247379302978516 = 1.9006595611572266 + 1.0 * 8.346719741821289
Epoch 30, val loss: 1.8987277746200562
Epoch 40, training loss: 10.060142517089844 = 1.8752979040145874 + 1.0 * 8.184844970703125
Epoch 40, val loss: 1.873952031135559
Epoch 50, training loss: 9.518465995788574 = 1.8482835292816162 + 1.0 * 7.670182228088379
Epoch 50, val loss: 1.8490290641784668
Epoch 60, training loss: 9.139662742614746 = 1.827795147895813 + 1.0 * 7.3118672370910645
Epoch 60, val loss: 1.8314067125320435
Epoch 70, training loss: 8.749738693237305 = 1.8140604496002197 + 1.0 * 6.935678005218506
Epoch 70, val loss: 1.8190181255340576
Epoch 80, training loss: 8.503740310668945 = 1.8021519184112549 + 1.0 * 6.701588153839111
Epoch 80, val loss: 1.8079640865325928
Epoch 90, training loss: 8.35440731048584 = 1.7870644330978394 + 1.0 * 6.567342758178711
Epoch 90, val loss: 1.7941093444824219
Epoch 100, training loss: 8.262073516845703 = 1.768782615661621 + 1.0 * 6.493290901184082
Epoch 100, val loss: 1.7781847715377808
Epoch 110, training loss: 8.188875198364258 = 1.7498455047607422 + 1.0 * 6.439029216766357
Epoch 110, val loss: 1.7622380256652832
Epoch 120, training loss: 8.127249717712402 = 1.730366826057434 + 1.0 * 6.396883010864258
Epoch 120, val loss: 1.7457212209701538
Epoch 130, training loss: 8.075105667114258 = 1.7086070775985718 + 1.0 * 6.366498947143555
Epoch 130, val loss: 1.7273155450820923
Epoch 140, training loss: 8.0216064453125 = 1.6833381652832031 + 1.0 * 6.338268756866455
Epoch 140, val loss: 1.7064272165298462
Epoch 150, training loss: 7.968604564666748 = 1.653855323791504 + 1.0 * 6.314749240875244
Epoch 150, val loss: 1.6824315786361694
Epoch 160, training loss: 7.913557529449463 = 1.61951744556427 + 1.0 * 6.294040203094482
Epoch 160, val loss: 1.6547352075576782
Epoch 170, training loss: 7.855480194091797 = 1.5805104970932007 + 1.0 * 6.274969577789307
Epoch 170, val loss: 1.6233909130096436
Epoch 180, training loss: 7.7947678565979 = 1.537052035331726 + 1.0 * 6.257715702056885
Epoch 180, val loss: 1.5883572101593018
Epoch 190, training loss: 7.735960483551025 = 1.4894863367080688 + 1.0 * 6.246474266052246
Epoch 190, val loss: 1.5500901937484741
Epoch 200, training loss: 7.670703887939453 = 1.4395991563796997 + 1.0 * 6.231104850769043
Epoch 200, val loss: 1.5099188089370728
Epoch 210, training loss: 7.606538772583008 = 1.3878329992294312 + 1.0 * 6.218705654144287
Epoch 210, val loss: 1.4683144092559814
Epoch 220, training loss: 7.543011665344238 = 1.3349006175994873 + 1.0 * 6.208110809326172
Epoch 220, val loss: 1.4261934757232666
Epoch 230, training loss: 7.4859514236450195 = 1.2817243337631226 + 1.0 * 6.204226970672607
Epoch 230, val loss: 1.38446843624115
Epoch 240, training loss: 7.421015739440918 = 1.2299118041992188 + 1.0 * 6.191103935241699
Epoch 240, val loss: 1.3443702459335327
Epoch 250, training loss: 7.363351821899414 = 1.1792185306549072 + 1.0 * 6.184133052825928
Epoch 250, val loss: 1.3056987524032593
Epoch 260, training loss: 7.307882308959961 = 1.1293362379074097 + 1.0 * 6.178545951843262
Epoch 260, val loss: 1.2680649757385254
Epoch 270, training loss: 7.255368709564209 = 1.0807313919067383 + 1.0 * 6.174637317657471
Epoch 270, val loss: 1.231833815574646
Epoch 280, training loss: 7.200376510620117 = 1.0339370965957642 + 1.0 * 6.166439533233643
Epoch 280, val loss: 1.1973956823349
Epoch 290, training loss: 7.150041580200195 = 0.9891355037689209 + 1.0 * 6.160905838012695
Epoch 290, val loss: 1.1647502183914185
Epoch 300, training loss: 7.101818561553955 = 0.9462060928344727 + 1.0 * 6.155612468719482
Epoch 300, val loss: 1.1337134838104248
Epoch 310, training loss: 7.05967903137207 = 0.9054844379425049 + 1.0 * 6.1541948318481445
Epoch 310, val loss: 1.1045721769332886
Epoch 320, training loss: 7.01569128036499 = 0.8673020005226135 + 1.0 * 6.1483893394470215
Epoch 320, val loss: 1.077646017074585
Epoch 330, training loss: 6.981385231018066 = 0.83139568567276 + 1.0 * 6.149989604949951
Epoch 330, val loss: 1.0527194738388062
Epoch 340, training loss: 6.939109802246094 = 0.7978729009628296 + 1.0 * 6.141236782073975
Epoch 340, val loss: 1.0298603773117065
Epoch 350, training loss: 6.903075695037842 = 0.7663348913192749 + 1.0 * 6.136740684509277
Epoch 350, val loss: 1.008907675743103
Epoch 360, training loss: 6.870321273803711 = 0.7361405491828918 + 1.0 * 6.134180545806885
Epoch 360, val loss: 0.9893200993537903
Epoch 370, training loss: 6.83912467956543 = 0.7071337103843689 + 1.0 * 6.131990909576416
Epoch 370, val loss: 0.9707875847816467
Epoch 380, training loss: 6.80920934677124 = 0.6790896058082581 + 1.0 * 6.130119800567627
Epoch 380, val loss: 0.9533981680870056
Epoch 390, training loss: 6.776825428009033 = 0.6516889929771423 + 1.0 * 6.125136375427246
Epoch 390, val loss: 0.9368273615837097
Epoch 400, training loss: 6.7458672523498535 = 0.6247372031211853 + 1.0 * 6.121129989624023
Epoch 400, val loss: 0.9207954406738281
Epoch 410, training loss: 6.730849266052246 = 0.5980821251869202 + 1.0 * 6.132767200469971
Epoch 410, val loss: 0.9051097631454468
Epoch 420, training loss: 6.691870212554932 = 0.5719742178916931 + 1.0 * 6.119895935058594
Epoch 420, val loss: 0.8899917602539062
Epoch 430, training loss: 6.661052703857422 = 0.5463762283325195 + 1.0 * 6.114676475524902
Epoch 430, val loss: 0.8755103349685669
Epoch 440, training loss: 6.633078098297119 = 0.5211919546127319 + 1.0 * 6.111886024475098
Epoch 440, val loss: 0.8615210652351379
Epoch 450, training loss: 6.6173200607299805 = 0.4965536892414093 + 1.0 * 6.1207661628723145
Epoch 450, val loss: 0.8480252623558044
Epoch 460, training loss: 6.5846991539001465 = 0.4729214608669281 + 1.0 * 6.1117777824401855
Epoch 460, val loss: 0.8353083729743958
Epoch 470, training loss: 6.556260108947754 = 0.4502969980239868 + 1.0 * 6.105963230133057
Epoch 470, val loss: 0.8236192464828491
Epoch 480, training loss: 6.531960487365723 = 0.42857539653778076 + 1.0 * 6.103384971618652
Epoch 480, val loss: 0.8128191232681274
Epoch 490, training loss: 6.510798454284668 = 0.4078366756439209 + 1.0 * 6.102962017059326
Epoch 490, val loss: 0.8030377626419067
Epoch 500, training loss: 6.489598751068115 = 0.38815832138061523 + 1.0 * 6.1014404296875
Epoch 500, val loss: 0.7943969964981079
Epoch 510, training loss: 6.4679036140441895 = 0.3693702518939972 + 1.0 * 6.0985331535339355
Epoch 510, val loss: 0.7869049906730652
Epoch 520, training loss: 6.451422691345215 = 0.35129547119140625 + 1.0 * 6.100127220153809
Epoch 520, val loss: 0.780318558216095
Epoch 530, training loss: 6.427684783935547 = 0.33392563462257385 + 1.0 * 6.093759059906006
Epoch 530, val loss: 0.7746926546096802
Epoch 540, training loss: 6.409889221191406 = 0.31698328256607056 + 1.0 * 6.0929059982299805
Epoch 540, val loss: 0.7697755694389343
Epoch 550, training loss: 6.397406578063965 = 0.3003842830657959 + 1.0 * 6.09702205657959
Epoch 550, val loss: 0.7653812170028687
Epoch 560, training loss: 6.375631332397461 = 0.28416141867637634 + 1.0 * 6.091469764709473
Epoch 560, val loss: 0.7613520622253418
Epoch 570, training loss: 6.354629039764404 = 0.2680990993976593 + 1.0 * 6.086529731750488
Epoch 570, val loss: 0.7577436566352844
Epoch 580, training loss: 6.345857620239258 = 0.2521562874317169 + 1.0 * 6.093701362609863
Epoch 580, val loss: 0.7543313503265381
Epoch 590, training loss: 6.32405948638916 = 0.23636026680469513 + 1.0 * 6.0876994132995605
Epoch 590, val loss: 0.7510227560997009
Epoch 600, training loss: 6.303199291229248 = 0.22088856995105743 + 1.0 * 6.082310676574707
Epoch 600, val loss: 0.7481199502944946
Epoch 610, training loss: 6.29006290435791 = 0.20570755004882812 + 1.0 * 6.084355354309082
Epoch 610, val loss: 0.7456521391868591
Epoch 620, training loss: 6.279757976531982 = 0.19116973876953125 + 1.0 * 6.088588237762451
Epoch 620, val loss: 0.7434234023094177
Epoch 630, training loss: 6.258490085601807 = 0.17737407982349396 + 1.0 * 6.081116199493408
Epoch 630, val loss: 0.7419625520706177
Epoch 640, training loss: 6.242002964019775 = 0.16437263786792755 + 1.0 * 6.077630519866943
Epoch 640, val loss: 0.7412027716636658
Epoch 650, training loss: 6.228630542755127 = 0.15223659574985504 + 1.0 * 6.076394081115723
Epoch 650, val loss: 0.7412039041519165
Epoch 660, training loss: 6.221786975860596 = 0.14105655252933502 + 1.0 * 6.080730438232422
Epoch 660, val loss: 0.7419341206550598
Epoch 670, training loss: 6.2055277824401855 = 0.13099870085716248 + 1.0 * 6.07452917098999
Epoch 670, val loss: 0.7434748411178589
Epoch 680, training loss: 6.194303035736084 = 0.12180645763874054 + 1.0 * 6.07249641418457
Epoch 680, val loss: 0.7458744049072266
Epoch 690, training loss: 6.183906555175781 = 0.11344429105520248 + 1.0 * 6.070462226867676
Epoch 690, val loss: 0.7490665912628174
Epoch 700, training loss: 6.178022861480713 = 0.10581944137811661 + 1.0 * 6.072203636169434
Epoch 700, val loss: 0.752953052520752
Epoch 710, training loss: 6.172438144683838 = 0.09887949377298355 + 1.0 * 6.073558807373047
Epoch 710, val loss: 0.7573279738426208
Epoch 720, training loss: 6.161048412322998 = 0.09264705330133438 + 1.0 * 6.068401336669922
Epoch 720, val loss: 0.7622649669647217
Epoch 730, training loss: 6.154021739959717 = 0.08693936467170715 + 1.0 * 6.067082405090332
Epoch 730, val loss: 0.7677858471870422
Epoch 740, training loss: 6.1464152336120605 = 0.08170290291309357 + 1.0 * 6.0647125244140625
Epoch 740, val loss: 0.7736899256706238
Epoch 750, training loss: 6.155457496643066 = 0.07691013813018799 + 1.0 * 6.078547477722168
Epoch 750, val loss: 0.7799334526062012
Epoch 760, training loss: 6.136141300201416 = 0.07254167646169662 + 1.0 * 6.063599586486816
Epoch 760, val loss: 0.7862728834152222
Epoch 770, training loss: 6.1304731369018555 = 0.06856357306241989 + 1.0 * 6.0619096755981445
Epoch 770, val loss: 0.792917788028717
Epoch 780, training loss: 6.125880241394043 = 0.06488059461116791 + 1.0 * 6.060999870300293
Epoch 780, val loss: 0.7997910380363464
Epoch 790, training loss: 6.130146026611328 = 0.061486031860113144 + 1.0 * 6.068659782409668
Epoch 790, val loss: 0.8067324757575989
Epoch 800, training loss: 6.117236137390137 = 0.0583636574447155 + 1.0 * 6.058872699737549
Epoch 800, val loss: 0.8137078881263733
Epoch 810, training loss: 6.113723278045654 = 0.05546935647726059 + 1.0 * 6.058253765106201
Epoch 810, val loss: 0.8208510875701904
Epoch 820, training loss: 6.117998123168945 = 0.05278192460536957 + 1.0 * 6.065216064453125
Epoch 820, val loss: 0.8280242085456848
Epoch 830, training loss: 6.110240936279297 = 0.05030146613717079 + 1.0 * 6.059939384460449
Epoch 830, val loss: 0.8351768255233765
Epoch 840, training loss: 6.107726097106934 = 0.047989387065172195 + 1.0 * 6.059736728668213
Epoch 840, val loss: 0.8422813415527344
Epoch 850, training loss: 6.1001458168029785 = 0.04584885761141777 + 1.0 * 6.054296970367432
Epoch 850, val loss: 0.8494405746459961
Epoch 860, training loss: 6.097536087036133 = 0.04384082928299904 + 1.0 * 6.053695201873779
Epoch 860, val loss: 0.8565630316734314
Epoch 870, training loss: 6.096308708190918 = 0.04196027293801308 + 1.0 * 6.054348468780518
Epoch 870, val loss: 0.8636402487754822
Epoch 880, training loss: 6.100891590118408 = 0.04020204395055771 + 1.0 * 6.060689449310303
Epoch 880, val loss: 0.8706189393997192
Epoch 890, training loss: 6.092175006866455 = 0.038560524582862854 + 1.0 * 6.053614616394043
Epoch 890, val loss: 0.8775338530540466
Epoch 900, training loss: 6.088415145874023 = 0.03702554479241371 + 1.0 * 6.051389694213867
Epoch 900, val loss: 0.8844196796417236
Epoch 910, training loss: 6.085610389709473 = 0.035582274198532104 + 1.0 * 6.050028324127197
Epoch 910, val loss: 0.8912646174430847
Epoch 920, training loss: 6.091846942901611 = 0.034215010702610016 + 1.0 * 6.057631969451904
Epoch 920, val loss: 0.898036003112793
Epoch 930, training loss: 6.086044788360596 = 0.032937247306108475 + 1.0 * 6.053107738494873
Epoch 930, val loss: 0.9046251177787781
Epoch 940, training loss: 6.080368995666504 = 0.03173566982150078 + 1.0 * 6.048633098602295
Epoch 940, val loss: 0.9112303853034973
Epoch 950, training loss: 6.087764739990234 = 0.03060038387775421 + 1.0 * 6.057164192199707
Epoch 950, val loss: 0.9177331924438477
Epoch 960, training loss: 6.0765581130981445 = 0.02952422946691513 + 1.0 * 6.047033786773682
Epoch 960, val loss: 0.9240018129348755
Epoch 970, training loss: 6.073278903961182 = 0.028511367738246918 + 1.0 * 6.044767379760742
Epoch 970, val loss: 0.9303651452064514
Epoch 980, training loss: 6.072657585144043 = 0.027545172721147537 + 1.0 * 6.045112609863281
Epoch 980, val loss: 0.9365912079811096
Epoch 990, training loss: 6.074502468109131 = 0.02662699669599533 + 1.0 * 6.04787540435791
Epoch 990, val loss: 0.942692756652832
Epoch 1000, training loss: 6.070040225982666 = 0.025758003816008568 + 1.0 * 6.04428243637085
Epoch 1000, val loss: 0.94877690076828
Epoch 1010, training loss: 6.067923545837402 = 0.024930806830525398 + 1.0 * 6.04299259185791
Epoch 1010, val loss: 0.9547793865203857
Epoch 1020, training loss: 6.067464351654053 = 0.024140020832419395 + 1.0 * 6.0433244705200195
Epoch 1020, val loss: 0.9607478380203247
Epoch 1030, training loss: 6.066125392913818 = 0.023388400673866272 + 1.0 * 6.042737007141113
Epoch 1030, val loss: 0.9665509462356567
Epoch 1040, training loss: 6.0679168701171875 = 0.02267739735543728 + 1.0 * 6.045239448547363
Epoch 1040, val loss: 0.9722523093223572
Epoch 1050, training loss: 6.06497049331665 = 0.021998371928930283 + 1.0 * 6.042972087860107
Epoch 1050, val loss: 0.9778277277946472
Epoch 1060, training loss: 6.060826301574707 = 0.021351465955376625 + 1.0 * 6.039474964141846
Epoch 1060, val loss: 0.9834460020065308
Epoch 1070, training loss: 6.060075283050537 = 0.02073231339454651 + 1.0 * 6.039342880249023
Epoch 1070, val loss: 0.9890003204345703
Epoch 1080, training loss: 6.064993858337402 = 0.020139651373028755 + 1.0 * 6.044854164123535
Epoch 1080, val loss: 0.9944136142730713
Epoch 1090, training loss: 6.06099271774292 = 0.019574996083974838 + 1.0 * 6.041417598724365
Epoch 1090, val loss: 0.9996891617774963
Epoch 1100, training loss: 6.056704521179199 = 0.01903627999126911 + 1.0 * 6.037668228149414
Epoch 1100, val loss: 1.0049805641174316
Epoch 1110, training loss: 6.0585856437683105 = 0.01852167397737503 + 1.0 * 6.040063858032227
Epoch 1110, val loss: 1.0102115869522095
Epoch 1120, training loss: 6.0547308921813965 = 0.018026866018772125 + 1.0 * 6.036704063415527
Epoch 1120, val loss: 1.0153100490570068
Epoch 1130, training loss: 6.055318355560303 = 0.01755170337855816 + 1.0 * 6.037766456604004
Epoch 1130, val loss: 1.0204472541809082
Epoch 1140, training loss: 6.0558366775512695 = 0.017095014452934265 + 1.0 * 6.038741588592529
Epoch 1140, val loss: 1.0254807472229004
Epoch 1150, training loss: 6.054135322570801 = 0.0166547242552042 + 1.0 * 6.03748083114624
Epoch 1150, val loss: 1.0304107666015625
Epoch 1160, training loss: 6.05116605758667 = 0.016235526651144028 + 1.0 * 6.03493070602417
Epoch 1160, val loss: 1.0353013277053833
Epoch 1170, training loss: 6.058247089385986 = 0.015832453966140747 + 1.0 * 6.042414665222168
Epoch 1170, val loss: 1.0401101112365723
Epoch 1180, training loss: 6.051058292388916 = 0.015447361394762993 + 1.0 * 6.035611152648926
Epoch 1180, val loss: 1.0448193550109863
Epoch 1190, training loss: 6.048831939697266 = 0.015077413059771061 + 1.0 * 6.033754348754883
Epoch 1190, val loss: 1.0495656728744507
Epoch 1200, training loss: 6.046843528747559 = 0.014718909747898579 + 1.0 * 6.0321245193481445
Epoch 1200, val loss: 1.0542749166488647
Epoch 1210, training loss: 6.051727771759033 = 0.014371250756084919 + 1.0 * 6.037356376647949
Epoch 1210, val loss: 1.0589382648468018
Epoch 1220, training loss: 6.046128273010254 = 0.014037548564374447 + 1.0 * 6.032090663909912
Epoch 1220, val loss: 1.0633320808410645
Epoch 1230, training loss: 6.045894145965576 = 0.01371731050312519 + 1.0 * 6.032176971435547
Epoch 1230, val loss: 1.0678752660751343
Epoch 1240, training loss: 6.045594215393066 = 0.013408632948994637 + 1.0 * 6.0321855545043945
Epoch 1240, val loss: 1.0723286867141724
Epoch 1250, training loss: 6.051151275634766 = 0.013109048828482628 + 1.0 * 6.038042068481445
Epoch 1250, val loss: 1.0767040252685547
Epoch 1260, training loss: 6.043951511383057 = 0.012821479700505733 + 1.0 * 6.031129837036133
Epoch 1260, val loss: 1.0810123682022095
Epoch 1270, training loss: 6.040926933288574 = 0.01254542451351881 + 1.0 * 6.02838134765625
Epoch 1270, val loss: 1.0853854417800903
Epoch 1280, training loss: 6.04264497756958 = 0.012275929562747478 + 1.0 * 6.030369281768799
Epoch 1280, val loss: 1.0896921157836914
Epoch 1290, training loss: 6.0450439453125 = 0.012013972736895084 + 1.0 * 6.033030033111572
Epoch 1290, val loss: 1.093828558921814
Epoch 1300, training loss: 6.038859844207764 = 0.011765144765377045 + 1.0 * 6.027094841003418
Epoch 1300, val loss: 1.0979282855987549
Epoch 1310, training loss: 6.037744522094727 = 0.011524385772645473 + 1.0 * 6.026220321655273
Epoch 1310, val loss: 1.1020750999450684
Epoch 1320, training loss: 6.038052558898926 = 0.011288782581686974 + 1.0 * 6.026763916015625
Epoch 1320, val loss: 1.1062086820602417
Epoch 1330, training loss: 6.048389434814453 = 0.011059057898819447 + 1.0 * 6.037330150604248
Epoch 1330, val loss: 1.1102139949798584
Epoch 1340, training loss: 6.039567947387695 = 0.010841619223356247 + 1.0 * 6.028726100921631
Epoch 1340, val loss: 1.114072322845459
Epoch 1350, training loss: 6.03483247756958 = 0.010630653239786625 + 1.0 * 6.0242018699646
Epoch 1350, val loss: 1.118101954460144
Epoch 1360, training loss: 6.035375118255615 = 0.010424336418509483 + 1.0 * 6.024950981140137
Epoch 1360, val loss: 1.1220327615737915
Epoch 1370, training loss: 6.0435309410095215 = 0.010223563760519028 + 1.0 * 6.0333075523376465
Epoch 1370, val loss: 1.1258385181427002
Epoch 1380, training loss: 6.036242961883545 = 0.010029667988419533 + 1.0 * 6.0262131690979
Epoch 1380, val loss: 1.1295183897018433
Epoch 1390, training loss: 6.033091068267822 = 0.009843835607171059 + 1.0 * 6.023247241973877
Epoch 1390, val loss: 1.133367657661438
Epoch 1400, training loss: 6.033677101135254 = 0.009661795571446419 + 1.0 * 6.024015426635742
Epoch 1400, val loss: 1.1371147632598877
Epoch 1410, training loss: 6.034739017486572 = 0.009483621455729008 + 1.0 * 6.02525520324707
Epoch 1410, val loss: 1.1407802104949951
Epoch 1420, training loss: 6.03585958480835 = 0.009312333539128304 + 1.0 * 6.026547431945801
Epoch 1420, val loss: 1.1443787813186646
Epoch 1430, training loss: 6.034125804901123 = 0.009146436117589474 + 1.0 * 6.024979591369629
Epoch 1430, val loss: 1.1480013132095337
Epoch 1440, training loss: 6.032098293304443 = 0.008984364569187164 + 1.0 * 6.02311372756958
Epoch 1440, val loss: 1.1515990495681763
Epoch 1450, training loss: 6.030940055847168 = 0.00882643647491932 + 1.0 * 6.022113800048828
Epoch 1450, val loss: 1.1551389694213867
Epoch 1460, training loss: 6.030128002166748 = 0.008673765696585178 + 1.0 * 6.021454334259033
Epoch 1460, val loss: 1.158698320388794
Epoch 1470, training loss: 6.032233715057373 = 0.008524489589035511 + 1.0 * 6.023709297180176
Epoch 1470, val loss: 1.16217839717865
Epoch 1480, training loss: 6.0356340408325195 = 0.008378962986171246 + 1.0 * 6.027255058288574
Epoch 1480, val loss: 1.1655398607254028
Epoch 1490, training loss: 6.029313087463379 = 0.00823962688446045 + 1.0 * 6.021073341369629
Epoch 1490, val loss: 1.168891191482544
Epoch 1500, training loss: 6.027421951293945 = 0.00810294784605503 + 1.0 * 6.0193190574646
Epoch 1500, val loss: 1.1723335981369019
Epoch 1510, training loss: 6.036898612976074 = 0.007969989441335201 + 1.0 * 6.028928756713867
Epoch 1510, val loss: 1.1757004261016846
Epoch 1520, training loss: 6.03060245513916 = 0.007839406840503216 + 1.0 * 6.022763252258301
Epoch 1520, val loss: 1.1788325309753418
Epoch 1530, training loss: 6.027006149291992 = 0.007715323939919472 + 1.0 * 6.019290924072266
Epoch 1530, val loss: 1.1820822954177856
Epoch 1540, training loss: 6.024580955505371 = 0.007592611014842987 + 1.0 * 6.016988277435303
Epoch 1540, val loss: 1.1853928565979004
Epoch 1550, training loss: 6.0251383781433105 = 0.007471614517271519 + 1.0 * 6.017666816711426
Epoch 1550, val loss: 1.188607096672058
Epoch 1560, training loss: 6.0329084396362305 = 0.007353425491601229 + 1.0 * 6.02555513381958
Epoch 1560, val loss: 1.1917742490768433
Epoch 1570, training loss: 6.027843475341797 = 0.00724123977124691 + 1.0 * 6.020602226257324
Epoch 1570, val loss: 1.1947741508483887
Epoch 1580, training loss: 6.023715496063232 = 0.007131543010473251 + 1.0 * 6.0165839195251465
Epoch 1580, val loss: 1.1978758573532104
Epoch 1590, training loss: 6.021968841552734 = 0.007023706566542387 + 1.0 * 6.014945030212402
Epoch 1590, val loss: 1.2009623050689697
Epoch 1600, training loss: 6.02202844619751 = 0.006917325779795647 + 1.0 * 6.015110969543457
Epoch 1600, val loss: 1.2040505409240723
Epoch 1610, training loss: 6.030009746551514 = 0.006813505198806524 + 1.0 * 6.023196220397949
Epoch 1610, val loss: 1.2070560455322266
Epoch 1620, training loss: 6.025251865386963 = 0.006712226662784815 + 1.0 * 6.0185394287109375
Epoch 1620, val loss: 1.2099674940109253
Epoch 1630, training loss: 6.02575159072876 = 0.006615349557250738 + 1.0 * 6.019136428833008
Epoch 1630, val loss: 1.2129606008529663
Epoch 1640, training loss: 6.02989387512207 = 0.006520984694361687 + 1.0 * 6.023373126983643
Epoch 1640, val loss: 1.215848684310913
Epoch 1650, training loss: 6.021815299987793 = 0.006427887827157974 + 1.0 * 6.015387535095215
Epoch 1650, val loss: 1.2185677289962769
Epoch 1660, training loss: 6.019476890563965 = 0.006337905302643776 + 1.0 * 6.013138771057129
Epoch 1660, val loss: 1.221583604812622
Epoch 1670, training loss: 6.018615245819092 = 0.00624838238582015 + 1.0 * 6.012366771697998
Epoch 1670, val loss: 1.2244740724563599
Epoch 1680, training loss: 6.019530296325684 = 0.006160043179988861 + 1.0 * 6.013370037078857
Epoch 1680, val loss: 1.2273521423339844
Epoch 1690, training loss: 6.028683662414551 = 0.006074362900108099 + 1.0 * 6.022609233856201
Epoch 1690, val loss: 1.2300912141799927
Epoch 1700, training loss: 6.021373748779297 = 0.005990717560052872 + 1.0 * 6.015383243560791
Epoch 1700, val loss: 1.2327048778533936
Epoch 1710, training loss: 6.019828796386719 = 0.005910740699619055 + 1.0 * 6.013917922973633
Epoch 1710, val loss: 1.2355109453201294
Epoch 1720, training loss: 6.024963855743408 = 0.005831399001181126 + 1.0 * 6.019132614135742
Epoch 1720, val loss: 1.2381651401519775
Epoch 1730, training loss: 6.017587661743164 = 0.005755306221544743 + 1.0 * 6.011832237243652
Epoch 1730, val loss: 1.2408156394958496
Epoch 1740, training loss: 6.016630172729492 = 0.005679418332874775 + 1.0 * 6.010950565338135
Epoch 1740, val loss: 1.2435314655303955
Epoch 1750, training loss: 6.017001152038574 = 0.005604287143796682 + 1.0 * 6.011396884918213
Epoch 1750, val loss: 1.2462035417556763
Epoch 1760, training loss: 6.020735740661621 = 0.005530605558305979 + 1.0 * 6.015204906463623
Epoch 1760, val loss: 1.2487826347351074
Epoch 1770, training loss: 6.02653694152832 = 0.005459590349346399 + 1.0 * 6.0210771560668945
Epoch 1770, val loss: 1.2512743473052979
Epoch 1780, training loss: 6.019857883453369 = 0.0053909532725811005 + 1.0 * 6.014466762542725
Epoch 1780, val loss: 1.2537858486175537
Epoch 1790, training loss: 6.019279479980469 = 0.005323811434209347 + 1.0 * 6.013955593109131
Epoch 1790, val loss: 1.2563389539718628
Epoch 1800, training loss: 6.01424503326416 = 0.005257551092654467 + 1.0 * 6.0089874267578125
Epoch 1800, val loss: 1.2587525844573975
Epoch 1810, training loss: 6.02042293548584 = 0.0051924725994467735 + 1.0 * 6.015230655670166
Epoch 1810, val loss: 1.2613096237182617
Epoch 1820, training loss: 6.01524543762207 = 0.005128341261297464 + 1.0 * 6.010117053985596
Epoch 1820, val loss: 1.263684868812561
Epoch 1830, training loss: 6.013598442077637 = 0.005065703298896551 + 1.0 * 6.008532524108887
Epoch 1830, val loss: 1.2661226987838745
Epoch 1840, training loss: 6.0142364501953125 = 0.005004308186471462 + 1.0 * 6.009232044219971
Epoch 1840, val loss: 1.2685891389846802
Epoch 1850, training loss: 6.018467426300049 = 0.0049433112144470215 + 1.0 * 6.013524055480957
Epoch 1850, val loss: 1.2709475755691528
Epoch 1860, training loss: 6.013200759887695 = 0.004884282127022743 + 1.0 * 6.008316516876221
Epoch 1860, val loss: 1.2733322381973267
Epoch 1870, training loss: 6.016412258148193 = 0.004826928488910198 + 1.0 * 6.011585235595703
Epoch 1870, val loss: 1.2757424116134644
Epoch 1880, training loss: 6.0147175788879395 = 0.004770278465002775 + 1.0 * 6.009947299957275
Epoch 1880, val loss: 1.2779507637023926
Epoch 1890, training loss: 6.015162467956543 = 0.004715611692517996 + 1.0 * 6.010447025299072
Epoch 1890, val loss: 1.2802951335906982
Epoch 1900, training loss: 6.014135360717773 = 0.0046612112782895565 + 1.0 * 6.009474277496338
Epoch 1900, val loss: 1.2825541496276855
Epoch 1910, training loss: 6.011500835418701 = 0.0046076118014752865 + 1.0 * 6.006893157958984
Epoch 1910, val loss: 1.2847665548324585
Epoch 1920, training loss: 6.017948150634766 = 0.004555302672088146 + 1.0 * 6.013392925262451
Epoch 1920, val loss: 1.2869980335235596
Epoch 1930, training loss: 6.012622833251953 = 0.00450420007109642 + 1.0 * 6.008118629455566
Epoch 1930, val loss: 1.2891464233398438
Epoch 1940, training loss: 6.0112080574035645 = 0.004454050678759813 + 1.0 * 6.006753921508789
Epoch 1940, val loss: 1.2914201021194458
Epoch 1950, training loss: 6.01166296005249 = 0.004404520615935326 + 1.0 * 6.007258415222168
Epoch 1950, val loss: 1.2936065196990967
Epoch 1960, training loss: 6.017528533935547 = 0.004355715587735176 + 1.0 * 6.013172626495361
Epoch 1960, val loss: 1.2957355976104736
Epoch 1970, training loss: 6.0118303298950195 = 0.004309188574552536 + 1.0 * 6.007521152496338
Epoch 1970, val loss: 1.297766089439392
Epoch 1980, training loss: 6.009330749511719 = 0.00426269369199872 + 1.0 * 6.005067825317383
Epoch 1980, val loss: 1.2999708652496338
Epoch 1990, training loss: 6.010739326477051 = 0.0042167711071670055 + 1.0 * 6.0065226554870605
Epoch 1990, val loss: 1.3021273612976074
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.9114
Flip ASR: 0.8933/225 nodes
The final ASR:0.81919, 0.06903, Accuracy:0.81605, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10580])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00603, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.330764770507812 = 1.956913709640503 + 1.0 * 8.37385082244873
Epoch 0, val loss: 1.954666256904602
Epoch 10, training loss: 10.31939697265625 = 1.9459869861602783 + 1.0 * 8.37341022491455
Epoch 10, val loss: 1.944096565246582
Epoch 20, training loss: 10.30278491973877 = 1.932551622390747 + 1.0 * 8.370233535766602
Epoch 20, val loss: 1.9305311441421509
Epoch 30, training loss: 10.259743690490723 = 1.9137780666351318 + 1.0 * 8.345965385437012
Epoch 30, val loss: 1.9112417697906494
Epoch 40, training loss: 10.07308578491211 = 1.889796495437622 + 1.0 * 8.183289527893066
Epoch 40, val loss: 1.8875588178634644
Epoch 50, training loss: 9.459986686706543 = 1.8640307188034058 + 1.0 * 7.595956325531006
Epoch 50, val loss: 1.8634296655654907
Epoch 60, training loss: 9.092123031616211 = 1.842336893081665 + 1.0 * 7.249786376953125
Epoch 60, val loss: 1.8450369834899902
Epoch 70, training loss: 8.778671264648438 = 1.8268226385116577 + 1.0 * 6.95184850692749
Epoch 70, val loss: 1.8315101861953735
Epoch 80, training loss: 8.582956314086914 = 1.8102320432662964 + 1.0 * 6.772724628448486
Epoch 80, val loss: 1.8168257474899292
Epoch 90, training loss: 8.427156448364258 = 1.7940669059753418 + 1.0 * 6.633090019226074
Epoch 90, val loss: 1.8028438091278076
Epoch 100, training loss: 8.316123962402344 = 1.7793056964874268 + 1.0 * 6.536818027496338
Epoch 100, val loss: 1.7904300689697266
Epoch 110, training loss: 8.228890419006348 = 1.7641288042068481 + 1.0 * 6.464761734008789
Epoch 110, val loss: 1.777602195739746
Epoch 120, training loss: 8.155997276306152 = 1.7478159666061401 + 1.0 * 6.408181190490723
Epoch 120, val loss: 1.7635704278945923
Epoch 130, training loss: 8.096054077148438 = 1.7297219038009644 + 1.0 * 6.366332054138184
Epoch 130, val loss: 1.7480623722076416
Epoch 140, training loss: 8.04281234741211 = 1.708737850189209 + 1.0 * 6.334074020385742
Epoch 140, val loss: 1.7302449941635132
Epoch 150, training loss: 7.99330472946167 = 1.6840745210647583 + 1.0 * 6.309230327606201
Epoch 150, val loss: 1.7096617221832275
Epoch 160, training loss: 7.9430999755859375 = 1.6552512645721436 + 1.0 * 6.287848949432373
Epoch 160, val loss: 1.6855816841125488
Epoch 170, training loss: 7.892041206359863 = 1.6211278438568115 + 1.0 * 6.270913600921631
Epoch 170, val loss: 1.657214879989624
Epoch 180, training loss: 7.839964389801025 = 1.5809402465820312 + 1.0 * 6.259024143218994
Epoch 180, val loss: 1.624023199081421
Epoch 190, training loss: 7.782533645629883 = 1.5357747077941895 + 1.0 * 6.246758937835693
Epoch 190, val loss: 1.5871213674545288
Epoch 200, training loss: 7.7210235595703125 = 1.4862433671951294 + 1.0 * 6.234780311584473
Epoch 200, val loss: 1.5471566915512085
Epoch 210, training loss: 7.663151741027832 = 1.4331575632095337 + 1.0 * 6.229994297027588
Epoch 210, val loss: 1.5051473379135132
Epoch 220, training loss: 7.597596168518066 = 1.3794528245925903 + 1.0 * 6.218143463134766
Epoch 220, val loss: 1.4643564224243164
Epoch 230, training loss: 7.533763885498047 = 1.3259140253067017 + 1.0 * 6.207849979400635
Epoch 230, val loss: 1.4252514839172363
Epoch 240, training loss: 7.477592945098877 = 1.2731610536575317 + 1.0 * 6.204432010650635
Epoch 240, val loss: 1.388411283493042
Epoch 250, training loss: 7.417989730834961 = 1.222745656967163 + 1.0 * 6.195244312286377
Epoch 250, val loss: 1.354409098625183
Epoch 260, training loss: 7.363019943237305 = 1.1741172075271606 + 1.0 * 6.188902854919434
Epoch 260, val loss: 1.3226509094238281
Epoch 270, training loss: 7.308328151702881 = 1.126904845237732 + 1.0 * 6.181423187255859
Epoch 270, val loss: 1.2926182746887207
Epoch 280, training loss: 7.256649494171143 = 1.0808006525039673 + 1.0 * 6.175848960876465
Epoch 280, val loss: 1.2635706663131714
Epoch 290, training loss: 7.211956024169922 = 1.0358706712722778 + 1.0 * 6.176085472106934
Epoch 290, val loss: 1.23545241355896
Epoch 300, training loss: 7.158637046813965 = 0.9925137758255005 + 1.0 * 6.166123390197754
Epoch 300, val loss: 1.2083287239074707
Epoch 310, training loss: 7.11228609085083 = 0.9500487446784973 + 1.0 * 6.162237167358398
Epoch 310, val loss: 1.1818315982818604
Epoch 320, training loss: 7.075901508331299 = 0.9084535241127014 + 1.0 * 6.167448043823242
Epoch 320, val loss: 1.155921459197998
Epoch 330, training loss: 7.025498390197754 = 0.8684991598129272 + 1.0 * 6.156999111175537
Epoch 330, val loss: 1.1311328411102295
Epoch 340, training loss: 6.980792045593262 = 0.8298425674438477 + 1.0 * 6.150949478149414
Epoch 340, val loss: 1.1073490381240845
Epoch 350, training loss: 6.947628974914551 = 0.7924094796180725 + 1.0 * 6.155219554901123
Epoch 350, val loss: 1.0842647552490234
Epoch 360, training loss: 6.9021735191345215 = 0.7566022276878357 + 1.0 * 6.145571231842041
Epoch 360, val loss: 1.0622358322143555
Epoch 370, training loss: 6.863256931304932 = 0.7222414016723633 + 1.0 * 6.141015529632568
Epoch 370, val loss: 1.0413044691085815
Epoch 380, training loss: 6.841754913330078 = 0.6894228458404541 + 1.0 * 6.152331829071045
Epoch 380, val loss: 1.021246314048767
Epoch 390, training loss: 6.794637203216553 = 0.6584731340408325 + 1.0 * 6.13616418838501
Epoch 390, val loss: 1.0026400089263916
Epoch 400, training loss: 6.762448787689209 = 0.6290326118469238 + 1.0 * 6.133416175842285
Epoch 400, val loss: 0.9854735136032104
Epoch 410, training loss: 6.731863975524902 = 0.6007981896400452 + 1.0 * 6.131065845489502
Epoch 410, val loss: 0.9692864418029785
Epoch 420, training loss: 6.699690341949463 = 0.573551595211029 + 1.0 * 6.126138687133789
Epoch 420, val loss: 0.9541488289833069
Epoch 430, training loss: 6.677492618560791 = 0.5470041632652283 + 1.0 * 6.130488395690918
Epoch 430, val loss: 0.9399028420448303
Epoch 440, training loss: 6.651806831359863 = 0.52125483751297 + 1.0 * 6.130551815032959
Epoch 440, val loss: 0.9264423251152039
Epoch 450, training loss: 6.6171183586120605 = 0.49627652764320374 + 1.0 * 6.120841979980469
Epoch 450, val loss: 0.9140607118606567
Epoch 460, training loss: 6.588203430175781 = 0.47156062722206116 + 1.0 * 6.116642951965332
Epoch 460, val loss: 0.9022669792175293
Epoch 470, training loss: 6.560884475708008 = 0.44695380330085754 + 1.0 * 6.113930702209473
Epoch 470, val loss: 0.8909509778022766
Epoch 480, training loss: 6.540657043457031 = 0.4225656986236572 + 1.0 * 6.118091583251953
Epoch 480, val loss: 0.8800104856491089
Epoch 490, training loss: 6.512060165405273 = 0.3987165093421936 + 1.0 * 6.113343715667725
Epoch 490, val loss: 0.8698473572731018
Epoch 500, training loss: 6.483763694763184 = 0.3752768039703369 + 1.0 * 6.108487129211426
Epoch 500, val loss: 0.8604022264480591
Epoch 510, training loss: 6.47517204284668 = 0.3522914946079254 + 1.0 * 6.122880458831787
Epoch 510, val loss: 0.8514297604560852
Epoch 520, training loss: 6.437392234802246 = 0.330338716506958 + 1.0 * 6.107053279876709
Epoch 520, val loss: 0.8433195948600769
Epoch 530, training loss: 6.411908149719238 = 0.3092261254787445 + 1.0 * 6.102682113647461
Epoch 530, val loss: 0.8362918496131897
Epoch 540, training loss: 6.388167858123779 = 0.289002001285553 + 1.0 * 6.099165916442871
Epoch 540, val loss: 0.8300095796585083
Epoch 550, training loss: 6.38043212890625 = 0.26976317167282104 + 1.0 * 6.110669136047363
Epoch 550, val loss: 0.8245628476142883
Epoch 560, training loss: 6.350649833679199 = 0.25189077854156494 + 1.0 * 6.098759174346924
Epoch 560, val loss: 0.8200352787971497
Epoch 570, training loss: 6.332313060760498 = 0.23525084555149078 + 1.0 * 6.097062110900879
Epoch 570, val loss: 0.8167523145675659
Epoch 580, training loss: 6.312236785888672 = 0.2197084128856659 + 1.0 * 6.092528343200684
Epoch 580, val loss: 0.8143308162689209
Epoch 590, training loss: 6.302346706390381 = 0.20520837604999542 + 1.0 * 6.097138404846191
Epoch 590, val loss: 0.8127620816230774
Epoch 600, training loss: 6.2848920822143555 = 0.19179649651050568 + 1.0 * 6.093095779418945
Epoch 600, val loss: 0.8121535778045654
Epoch 610, training loss: 6.271090507507324 = 0.179417222738266 + 1.0 * 6.091673374176025
Epoch 610, val loss: 0.8125
Epoch 620, training loss: 6.256363391876221 = 0.16797485947608948 + 1.0 * 6.088388442993164
Epoch 620, val loss: 0.8134583830833435
Epoch 630, training loss: 6.243121147155762 = 0.1574110984802246 + 1.0 * 6.085710048675537
Epoch 630, val loss: 0.8152617812156677
Epoch 640, training loss: 6.232274055480957 = 0.1476241648197174 + 1.0 * 6.084650039672852
Epoch 640, val loss: 0.817753255367279
Epoch 650, training loss: 6.222934722900391 = 0.1385689675807953 + 1.0 * 6.0843658447265625
Epoch 650, val loss: 0.8207003474235535
Epoch 660, training loss: 6.213522911071777 = 0.13024012744426727 + 1.0 * 6.083282947540283
Epoch 660, val loss: 0.8242673873901367
Epoch 670, training loss: 6.20393180847168 = 0.12251350283622742 + 1.0 * 6.081418514251709
Epoch 670, val loss: 0.8283237814903259
Epoch 680, training loss: 6.1984124183654785 = 0.11536206305027008 + 1.0 * 6.08305025100708
Epoch 680, val loss: 0.8325929045677185
Epoch 690, training loss: 6.193410396575928 = 0.10875292867422104 + 1.0 * 6.084657669067383
Epoch 690, val loss: 0.8372353911399841
Epoch 700, training loss: 6.180369853973389 = 0.10264700651168823 + 1.0 * 6.077723026275635
Epoch 700, val loss: 0.8422975540161133
Epoch 710, training loss: 6.172816753387451 = 0.09698538482189178 + 1.0 * 6.075831413269043
Epoch 710, val loss: 0.8475608825683594
Epoch 720, training loss: 6.173268795013428 = 0.09171402454376221 + 1.0 * 6.081554889678955
Epoch 720, val loss: 0.8529706001281738
Epoch 730, training loss: 6.168362140655518 = 0.08687984198331833 + 1.0 * 6.081482410430908
Epoch 730, val loss: 0.8586646914482117
Epoch 740, training loss: 6.15558385848999 = 0.0823986679315567 + 1.0 * 6.073184967041016
Epoch 740, val loss: 0.8645302653312683
Epoch 750, training loss: 6.149522304534912 = 0.07823602110147476 + 1.0 * 6.071286201477051
Epoch 750, val loss: 0.8705379366874695
Epoch 760, training loss: 6.146834850311279 = 0.07434838265180588 + 1.0 * 6.072486400604248
Epoch 760, val loss: 0.8765460848808289
Epoch 770, training loss: 6.139692783355713 = 0.07071968168020248 + 1.0 * 6.068973064422607
Epoch 770, val loss: 0.8826709389686584
Epoch 780, training loss: 6.146559715270996 = 0.06732741743326187 + 1.0 * 6.079232215881348
Epoch 780, val loss: 0.8888242244720459
Epoch 790, training loss: 6.130743980407715 = 0.06418021768331528 + 1.0 * 6.066563606262207
Epoch 790, val loss: 0.8949524164199829
Epoch 800, training loss: 6.127322196960449 = 0.06123275309801102 + 1.0 * 6.066089630126953
Epoch 800, val loss: 0.9012712240219116
Epoch 810, training loss: 6.127135753631592 = 0.05845482274889946 + 1.0 * 6.068680763244629
Epoch 810, val loss: 0.9074544310569763
Epoch 820, training loss: 6.123650074005127 = 0.055876653641462326 + 1.0 * 6.067773342132568
Epoch 820, val loss: 0.9137110114097595
Epoch 830, training loss: 6.11754846572876 = 0.05346043035387993 + 1.0 * 6.064087867736816
Epoch 830, val loss: 0.9198890328407288
Epoch 840, training loss: 6.114926815032959 = 0.05119131878018379 + 1.0 * 6.063735485076904
Epoch 840, val loss: 0.9262200593948364
Epoch 850, training loss: 6.1115570068359375 = 0.049057766795158386 + 1.0 * 6.062499046325684
Epoch 850, val loss: 0.9323387742042542
Epoch 860, training loss: 6.107980251312256 = 0.047045160084962845 + 1.0 * 6.060935020446777
Epoch 860, val loss: 0.938540518283844
Epoch 870, training loss: 6.113874912261963 = 0.045144934207201004 + 1.0 * 6.068729877471924
Epoch 870, val loss: 0.9447068572044373
Epoch 880, training loss: 6.109683513641357 = 0.04338135942816734 + 1.0 * 6.066302299499512
Epoch 880, val loss: 0.9507632851600647
Epoch 890, training loss: 6.100693702697754 = 0.04171014577150345 + 1.0 * 6.058983325958252
Epoch 890, val loss: 0.9568344950675964
Epoch 900, training loss: 6.09622859954834 = 0.040129244327545166 + 1.0 * 6.0560994148254395
Epoch 900, val loss: 0.9629926681518555
Epoch 910, training loss: 6.094724178314209 = 0.03862810507416725 + 1.0 * 6.056096076965332
Epoch 910, val loss: 0.968958854675293
Epoch 920, training loss: 6.109384059906006 = 0.037207718938589096 + 1.0 * 6.072176456451416
Epoch 920, val loss: 0.9748325943946838
Epoch 930, training loss: 6.0900421142578125 = 0.035873111337423325 + 1.0 * 6.054169178009033
Epoch 930, val loss: 0.9806447625160217
Epoch 940, training loss: 6.090209007263184 = 0.03461121395230293 + 1.0 * 6.05559778213501
Epoch 940, val loss: 0.9867436289787292
Epoch 950, training loss: 6.092522144317627 = 0.03341206908226013 + 1.0 * 6.059110164642334
Epoch 950, val loss: 0.9926592707633972
Epoch 960, training loss: 6.089922904968262 = 0.03226847946643829 + 1.0 * 6.05765438079834
Epoch 960, val loss: 0.9980419874191284
Epoch 970, training loss: 6.084203243255615 = 0.03119724430143833 + 1.0 * 6.053006172180176
Epoch 970, val loss: 1.0039268732070923
Epoch 980, training loss: 6.081376075744629 = 0.030168386176228523 + 1.0 * 6.051207542419434
Epoch 980, val loss: 1.0096653699874878
Epoch 990, training loss: 6.088079452514648 = 0.02919178269803524 + 1.0 * 6.058887481689453
Epoch 990, val loss: 1.0152770280838013
Epoch 1000, training loss: 6.082857131958008 = 0.028258677572011948 + 1.0 * 6.054598331451416
Epoch 1000, val loss: 1.0206058025360107
Epoch 1010, training loss: 6.077324390411377 = 0.02737545035779476 + 1.0 * 6.0499491691589355
Epoch 1010, val loss: 1.026221752166748
Epoch 1020, training loss: 6.074971675872803 = 0.026528041809797287 + 1.0 * 6.048443794250488
Epoch 1020, val loss: 1.0316598415374756
Epoch 1030, training loss: 6.081859588623047 = 0.02571691945195198 + 1.0 * 6.056142807006836
Epoch 1030, val loss: 1.036954641342163
Epoch 1040, training loss: 6.074054718017578 = 0.02495122142136097 + 1.0 * 6.049103260040283
Epoch 1040, val loss: 1.0422569513320923
Epoch 1050, training loss: 6.071702480316162 = 0.02421300858259201 + 1.0 * 6.047489643096924
Epoch 1050, val loss: 1.0475342273712158
Epoch 1060, training loss: 6.075702667236328 = 0.023507902398705482 + 1.0 * 6.052194595336914
Epoch 1060, val loss: 1.0525890588760376
Epoch 1070, training loss: 6.069449424743652 = 0.022842613980174065 + 1.0 * 6.04660701751709
Epoch 1070, val loss: 1.0576471090316772
Epoch 1080, training loss: 6.066550254821777 = 0.022203478962183 + 1.0 * 6.044346809387207
Epoch 1080, val loss: 1.0629068613052368
Epoch 1090, training loss: 6.065654277801514 = 0.021588681265711784 + 1.0 * 6.044065475463867
Epoch 1090, val loss: 1.0679889917373657
Epoch 1100, training loss: 6.076753616333008 = 0.021000156179070473 + 1.0 * 6.055753231048584
Epoch 1100, val loss: 1.0728514194488525
Epoch 1110, training loss: 6.065881252288818 = 0.020435582846403122 + 1.0 * 6.045445442199707
Epoch 1110, val loss: 1.0775221586227417
Epoch 1120, training loss: 6.062194347381592 = 0.0198946725577116 + 1.0 * 6.042299747467041
Epoch 1120, val loss: 1.0825241804122925
Epoch 1130, training loss: 6.06376314163208 = 0.019371509552001953 + 1.0 * 6.044391632080078
Epoch 1130, val loss: 1.087245225906372
Epoch 1140, training loss: 6.062230587005615 = 0.018870562314987183 + 1.0 * 6.043360233306885
Epoch 1140, val loss: 1.0918861627578735
Epoch 1150, training loss: 6.061243534088135 = 0.01839304156601429 + 1.0 * 6.042850494384766
Epoch 1150, val loss: 1.0965216159820557
Epoch 1160, training loss: 6.0576372146606445 = 0.017933178693056107 + 1.0 * 6.039703845977783
Epoch 1160, val loss: 1.1012815237045288
Epoch 1170, training loss: 6.059770584106445 = 0.017489828169345856 + 1.0 * 6.042280673980713
Epoch 1170, val loss: 1.1058743000030518
Epoch 1180, training loss: 6.055452823638916 = 0.0170609038323164 + 1.0 * 6.038392066955566
Epoch 1180, val loss: 1.1103512048721313
Epoch 1190, training loss: 6.060304164886475 = 0.016647618263959885 + 1.0 * 6.043656349182129
Epoch 1190, val loss: 1.1149078607559204
Epoch 1200, training loss: 6.058065414428711 = 0.016251597553491592 + 1.0 * 6.041813850402832
Epoch 1200, val loss: 1.1190754175186157
Epoch 1210, training loss: 6.054589748382568 = 0.015870723873376846 + 1.0 * 6.038719177246094
Epoch 1210, val loss: 1.1234313249588013
Epoch 1220, training loss: 6.05172872543335 = 0.015503350645303726 + 1.0 * 6.036225318908691
Epoch 1220, val loss: 1.127889633178711
Epoch 1230, training loss: 6.0676493644714355 = 0.015145640820264816 + 1.0 * 6.05250358581543
Epoch 1230, val loss: 1.1319174766540527
Epoch 1240, training loss: 6.054574012756348 = 0.014809289015829563 + 1.0 * 6.039764881134033
Epoch 1240, val loss: 1.136052131652832
Epoch 1250, training loss: 6.048863887786865 = 0.014481295831501484 + 1.0 * 6.0343828201293945
Epoch 1250, val loss: 1.140278935432434
Epoch 1260, training loss: 6.0482048988342285 = 0.014162587001919746 + 1.0 * 6.0340423583984375
Epoch 1260, val loss: 1.1444464921951294
Epoch 1270, training loss: 6.048160076141357 = 0.013851972296833992 + 1.0 * 6.034307956695557
Epoch 1270, val loss: 1.1485003232955933
Epoch 1280, training loss: 6.056436538696289 = 0.013550618663430214 + 1.0 * 6.042885780334473
Epoch 1280, val loss: 1.152310848236084
Epoch 1290, training loss: 6.049469470977783 = 0.013266283087432384 + 1.0 * 6.036203384399414
Epoch 1290, val loss: 1.156465768814087
Epoch 1300, training loss: 6.056307792663574 = 0.012986376881599426 + 1.0 * 6.04332160949707
Epoch 1300, val loss: 1.1603577136993408
Epoch 1310, training loss: 6.051005840301514 = 0.012718857266008854 + 1.0 * 6.038287162780762
Epoch 1310, val loss: 1.1639645099639893
Epoch 1320, training loss: 6.045565605163574 = 0.012461160309612751 + 1.0 * 6.033104419708252
Epoch 1320, val loss: 1.168014407157898
Epoch 1330, training loss: 6.043028831481934 = 0.012209078297019005 + 1.0 * 6.030819892883301
Epoch 1330, val loss: 1.1718406677246094
Epoch 1340, training loss: 6.042811393737793 = 0.011963441036641598 + 1.0 * 6.030848026275635
Epoch 1340, val loss: 1.1756317615509033
Epoch 1350, training loss: 6.04840087890625 = 0.011724049225449562 + 1.0 * 6.03667688369751
Epoch 1350, val loss: 1.1793180704116821
Epoch 1360, training loss: 6.044308662414551 = 0.01149206142872572 + 1.0 * 6.032816410064697
Epoch 1360, val loss: 1.182844877243042
Epoch 1370, training loss: 6.0429911613464355 = 0.01126911211758852 + 1.0 * 6.031722068786621
Epoch 1370, val loss: 1.1865736246109009
Epoch 1380, training loss: 6.045647621154785 = 0.011052408255636692 + 1.0 * 6.034595012664795
Epoch 1380, val loss: 1.1900867223739624
Epoch 1390, training loss: 6.040607452392578 = 0.010844176635146141 + 1.0 * 6.029763221740723
Epoch 1390, val loss: 1.1936975717544556
Epoch 1400, training loss: 6.041294097900391 = 0.010641464032232761 + 1.0 * 6.0306525230407715
Epoch 1400, val loss: 1.197418212890625
Epoch 1410, training loss: 6.040131568908691 = 0.010442318394780159 + 1.0 * 6.029689311981201
Epoch 1410, val loss: 1.200835108757019
Epoch 1420, training loss: 6.039546966552734 = 0.010249187238514423 + 1.0 * 6.029297828674316
Epoch 1420, val loss: 1.204270601272583
Epoch 1430, training loss: 6.043259143829346 = 0.010061566717922688 + 1.0 * 6.033197402954102
Epoch 1430, val loss: 1.2076383829116821
Epoch 1440, training loss: 6.041703701019287 = 0.00988053623586893 + 1.0 * 6.03182315826416
Epoch 1440, val loss: 1.210877537727356
Epoch 1450, training loss: 6.037814617156982 = 0.009707419201731682 + 1.0 * 6.028107166290283
Epoch 1450, val loss: 1.2143833637237549
Epoch 1460, training loss: 6.037919044494629 = 0.009536265395581722 + 1.0 * 6.028382778167725
Epoch 1460, val loss: 1.2177906036376953
Epoch 1470, training loss: 6.039815425872803 = 0.009370433166623116 + 1.0 * 6.030445098876953
Epoch 1470, val loss: 1.2210590839385986
Epoch 1480, training loss: 6.033973217010498 = 0.00920743215829134 + 1.0 * 6.024765968322754
Epoch 1480, val loss: 1.2241835594177246
Epoch 1490, training loss: 6.036565780639648 = 0.009049049578607082 + 1.0 * 6.027516841888428
Epoch 1490, val loss: 1.2274998426437378
Epoch 1500, training loss: 6.041320323944092 = 0.00889527890831232 + 1.0 * 6.0324249267578125
Epoch 1500, val loss: 1.230674386024475
Epoch 1510, training loss: 6.035494327545166 = 0.008748696185648441 + 1.0 * 6.026745796203613
Epoch 1510, val loss: 1.233709454536438
Epoch 1520, training loss: 6.0325093269348145 = 0.008603882044553757 + 1.0 * 6.023905277252197
Epoch 1520, val loss: 1.2367826700210571
Epoch 1530, training loss: 6.031338214874268 = 0.008463221602141857 + 1.0 * 6.02287483215332
Epoch 1530, val loss: 1.240047574043274
Epoch 1540, training loss: 6.033967971801758 = 0.008324195630848408 + 1.0 * 6.025643825531006
Epoch 1540, val loss: 1.2431172132492065
Epoch 1550, training loss: 6.035482883453369 = 0.00818859413266182 + 1.0 * 6.027294158935547
Epoch 1550, val loss: 1.2459189891815186
Epoch 1560, training loss: 6.031548023223877 = 0.008059198036789894 + 1.0 * 6.023488998413086
Epoch 1560, val loss: 1.2490143775939941
Epoch 1570, training loss: 6.030198097229004 = 0.007931772619485855 + 1.0 * 6.022266387939453
Epoch 1570, val loss: 1.2521610260009766
Epoch 1580, training loss: 6.03792667388916 = 0.0078073046170175076 + 1.0 * 6.0301194190979
Epoch 1580, val loss: 1.2552155256271362
Epoch 1590, training loss: 6.0312275886535645 = 0.007685304153710604 + 1.0 * 6.023542404174805
Epoch 1590, val loss: 1.25779390335083
Epoch 1600, training loss: 6.03130578994751 = 0.007568059954792261 + 1.0 * 6.023737907409668
Epoch 1600, val loss: 1.2609058618545532
Epoch 1610, training loss: 6.032992839813232 = 0.007453394588083029 + 1.0 * 6.025539398193359
Epoch 1610, val loss: 1.2637097835540771
Epoch 1620, training loss: 6.031339645385742 = 0.007340162992477417 + 1.0 * 6.0239996910095215
Epoch 1620, val loss: 1.26646089553833
Epoch 1630, training loss: 6.028240203857422 = 0.007230774965137243 + 1.0 * 6.02100944519043
Epoch 1630, val loss: 1.2693239450454712
Epoch 1640, training loss: 6.029962062835693 = 0.007122681010514498 + 1.0 * 6.022839546203613
Epoch 1640, val loss: 1.272126317024231
Epoch 1650, training loss: 6.029333114624023 = 0.007017281837761402 + 1.0 * 6.022315979003906
Epoch 1650, val loss: 1.2747215032577515
Epoch 1660, training loss: 6.027708530426025 = 0.0069158789701759815 + 1.0 * 6.020792484283447
Epoch 1660, val loss: 1.2775651216506958
Epoch 1670, training loss: 6.026877403259277 = 0.006815201137214899 + 1.0 * 6.02006196975708
Epoch 1670, val loss: 1.2803244590759277
Epoch 1680, training loss: 6.02971887588501 = 0.006716638803482056 + 1.0 * 6.0230021476745605
Epoch 1680, val loss: 1.2829720973968506
Epoch 1690, training loss: 6.0264787673950195 = 0.006621492560952902 + 1.0 * 6.019857406616211
Epoch 1690, val loss: 1.2855521440505981
Epoch 1700, training loss: 6.027049541473389 = 0.006528780795633793 + 1.0 * 6.0205206871032715
Epoch 1700, val loss: 1.2883797883987427
Epoch 1710, training loss: 6.028400421142578 = 0.006437739823013544 + 1.0 * 6.021962642669678
Epoch 1710, val loss: 1.2909272909164429
Epoch 1720, training loss: 6.025191783905029 = 0.006348465569317341 + 1.0 * 6.018843173980713
Epoch 1720, val loss: 1.293399691581726
Epoch 1730, training loss: 6.024057865142822 = 0.0062612127512693405 + 1.0 * 6.017796516418457
Epoch 1730, val loss: 1.2961252927780151
Epoch 1740, training loss: 6.027428150177002 = 0.006175692658871412 + 1.0 * 6.021252632141113
Epoch 1740, val loss: 1.29856276512146
Epoch 1750, training loss: 6.024509429931641 = 0.006091879680752754 + 1.0 * 6.0184173583984375
Epoch 1750, val loss: 1.3010567426681519
Epoch 1760, training loss: 6.024900436401367 = 0.0060102916322648525 + 1.0 * 6.018890380859375
Epoch 1760, val loss: 1.3034967184066772
Epoch 1770, training loss: 6.024558067321777 = 0.005930788349360228 + 1.0 * 6.018627166748047
Epoch 1770, val loss: 1.305947184562683
Epoch 1780, training loss: 6.0236592292785645 = 0.005852806381881237 + 1.0 * 6.017806529998779
Epoch 1780, val loss: 1.3083758354187012
Epoch 1790, training loss: 6.021513938903809 = 0.005777247715741396 + 1.0 * 6.0157365798950195
Epoch 1790, val loss: 1.310943841934204
Epoch 1800, training loss: 6.022518157958984 = 0.005701770074665546 + 1.0 * 6.01681661605835
Epoch 1800, val loss: 1.3134238719940186
Epoch 1810, training loss: 6.026956558227539 = 0.005628108512610197 + 1.0 * 6.021328449249268
Epoch 1810, val loss: 1.3156285285949707
Epoch 1820, training loss: 6.022313117980957 = 0.005557622294872999 + 1.0 * 6.0167555809021
Epoch 1820, val loss: 1.318008542060852
Epoch 1830, training loss: 6.023660659790039 = 0.005487772170454264 + 1.0 * 6.018172740936279
Epoch 1830, val loss: 1.320494532585144
Epoch 1840, training loss: 6.019805908203125 = 0.0054189166985452175 + 1.0 * 6.014387130737305
Epoch 1840, val loss: 1.3227181434631348
Epoch 1850, training loss: 6.018720626831055 = 0.005350965075194836 + 1.0 * 6.013369560241699
Epoch 1850, val loss: 1.3250516653060913
Epoch 1860, training loss: 6.024033069610596 = 0.005284378305077553 + 1.0 * 6.018748760223389
Epoch 1860, val loss: 1.327346682548523
Epoch 1870, training loss: 6.0213541984558105 = 0.0052199289202690125 + 1.0 * 6.016134262084961
Epoch 1870, val loss: 1.3295964002609253
Epoch 1880, training loss: 6.020365238189697 = 0.0051560960710048676 + 1.0 * 6.015209197998047
Epoch 1880, val loss: 1.3316956758499146
Epoch 1890, training loss: 6.020629405975342 = 0.005093835294246674 + 1.0 * 6.015535354614258
Epoch 1890, val loss: 1.333926796913147
Epoch 1900, training loss: 6.021656036376953 = 0.00503350468352437 + 1.0 * 6.016622543334961
Epoch 1900, val loss: 1.336208462715149
Epoch 1910, training loss: 6.0174713134765625 = 0.004973877687007189 + 1.0 * 6.012497425079346
Epoch 1910, val loss: 1.3384867906570435
Epoch 1920, training loss: 6.024463176727295 = 0.004915652796626091 + 1.0 * 6.019547462463379
Epoch 1920, val loss: 1.3408398628234863
Epoch 1930, training loss: 6.018966197967529 = 0.004857403691858053 + 1.0 * 6.014108657836914
Epoch 1930, val loss: 1.3425761461257935
Epoch 1940, training loss: 6.015680313110352 = 0.004801723174750805 + 1.0 * 6.010878562927246
Epoch 1940, val loss: 1.3450120687484741
Epoch 1950, training loss: 6.017024517059326 = 0.004746594000607729 + 1.0 * 6.012278079986572
Epoch 1950, val loss: 1.3472440242767334
Epoch 1960, training loss: 6.018611907958984 = 0.00469166599214077 + 1.0 * 6.013920307159424
Epoch 1960, val loss: 1.3492923974990845
Epoch 1970, training loss: 6.016035556793213 = 0.004637700505554676 + 1.0 * 6.011397838592529
Epoch 1970, val loss: 1.3514091968536377
Epoch 1980, training loss: 6.024061679840088 = 0.004584847018122673 + 1.0 * 6.019476890563965
Epoch 1980, val loss: 1.3534879684448242
Epoch 1990, training loss: 6.0186638832092285 = 0.004534119740128517 + 1.0 * 6.014129638671875
Epoch 1990, val loss: 1.3552708625793457
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7601
Flip ASR: 0.7111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.311280250549316 = 1.9374874830245972 + 1.0 * 8.37379264831543
Epoch 0, val loss: 1.938324213027954
Epoch 10, training loss: 10.300039291381836 = 1.9272342920303345 + 1.0 * 8.372804641723633
Epoch 10, val loss: 1.9274269342422485
Epoch 20, training loss: 10.280034065246582 = 1.9144134521484375 + 1.0 * 8.365620613098145
Epoch 20, val loss: 1.9137475490570068
Epoch 30, training loss: 10.216913223266602 = 1.8972272872924805 + 1.0 * 8.319685935974121
Epoch 30, val loss: 1.8954094648361206
Epoch 40, training loss: 9.883039474487305 = 1.8780826330184937 + 1.0 * 8.00495719909668
Epoch 40, val loss: 1.8758571147918701
Epoch 50, training loss: 9.334440231323242 = 1.8610172271728516 + 1.0 * 7.473423004150391
Epoch 50, val loss: 1.858839511871338
Epoch 60, training loss: 8.83398151397705 = 1.845388412475586 + 1.0 * 6.988593101501465
Epoch 60, val loss: 1.8428189754486084
Epoch 70, training loss: 8.562390327453613 = 1.8298542499542236 + 1.0 * 6.732536315917969
Epoch 70, val loss: 1.8271738290786743
Epoch 80, training loss: 8.421883583068848 = 1.8142175674438477 + 1.0 * 6.607666015625
Epoch 80, val loss: 1.8117650747299194
Epoch 90, training loss: 8.314866065979004 = 1.7989197969436646 + 1.0 * 6.515945911407471
Epoch 90, val loss: 1.7970333099365234
Epoch 100, training loss: 8.228128433227539 = 1.7839975357055664 + 1.0 * 6.4441304206848145
Epoch 100, val loss: 1.7825900316238403
Epoch 110, training loss: 8.155567169189453 = 1.7685216665267944 + 1.0 * 6.387045860290527
Epoch 110, val loss: 1.7674856185913086
Epoch 120, training loss: 8.09477424621582 = 1.7516838312149048 + 1.0 * 6.343090534210205
Epoch 120, val loss: 1.7514467239379883
Epoch 130, training loss: 8.039678573608398 = 1.7331756353378296 + 1.0 * 6.306502819061279
Epoch 130, val loss: 1.73426353931427
Epoch 140, training loss: 7.988813400268555 = 1.7118258476257324 + 1.0 * 6.276987552642822
Epoch 140, val loss: 1.7149473428726196
Epoch 150, training loss: 7.941377639770508 = 1.6866140365600586 + 1.0 * 6.254763603210449
Epoch 150, val loss: 1.6929044723510742
Epoch 160, training loss: 7.893585205078125 = 1.6572397947311401 + 1.0 * 6.236345291137695
Epoch 160, val loss: 1.6679339408874512
Epoch 170, training loss: 7.840844631195068 = 1.6234036684036255 + 1.0 * 6.217441082000732
Epoch 170, val loss: 1.6395150423049927
Epoch 180, training loss: 7.788167953491211 = 1.5842845439910889 + 1.0 * 6.203883647918701
Epoch 180, val loss: 1.6071088314056396
Epoch 190, training loss: 7.73203182220459 = 1.5397396087646484 + 1.0 * 6.192292213439941
Epoch 190, val loss: 1.5705976486206055
Epoch 200, training loss: 7.674753665924072 = 1.4902793169021606 + 1.0 * 6.184474468231201
Epoch 200, val loss: 1.5305383205413818
Epoch 210, training loss: 7.613009452819824 = 1.4384745359420776 + 1.0 * 6.174534797668457
Epoch 210, val loss: 1.4894382953643799
Epoch 220, training loss: 7.555103778839111 = 1.3863099813461304 + 1.0 * 6.168793678283691
Epoch 220, val loss: 1.449089527130127
Epoch 230, training loss: 7.497851371765137 = 1.3354618549346924 + 1.0 * 6.162389278411865
Epoch 230, val loss: 1.410947322845459
Epoch 240, training loss: 7.448173999786377 = 1.2871026992797852 + 1.0 * 6.161071300506592
Epoch 240, val loss: 1.3757367134094238
Epoch 250, training loss: 7.392512321472168 = 1.2418867349624634 + 1.0 * 6.150625705718994
Epoch 250, val loss: 1.3441892862319946
Epoch 260, training loss: 7.344222068786621 = 1.1992684602737427 + 1.0 * 6.144953727722168
Epoch 260, val loss: 1.315367579460144
Epoch 270, training loss: 7.299926280975342 = 1.1583118438720703 + 1.0 * 6.1416144371032715
Epoch 270, val loss: 1.2883050441741943
Epoch 280, training loss: 7.257706642150879 = 1.1188616752624512 + 1.0 * 6.138844966888428
Epoch 280, val loss: 1.2626302242279053
Epoch 290, training loss: 7.212494373321533 = 1.080220103263855 + 1.0 * 6.132274150848389
Epoch 290, val loss: 1.2378017902374268
Epoch 300, training loss: 7.173312664031982 = 1.0418089628219604 + 1.0 * 6.131503582000732
Epoch 300, val loss: 1.2133315801620483
Epoch 310, training loss: 7.130496025085449 = 1.0041370391845703 + 1.0 * 6.126358985900879
Epoch 310, val loss: 1.1891109943389893
Epoch 320, training loss: 7.089214324951172 = 0.9669176936149597 + 1.0 * 6.1222968101501465
Epoch 320, val loss: 1.1652456521987915
Epoch 330, training loss: 7.04758358001709 = 0.9303411841392517 + 1.0 * 6.117242336273193
Epoch 330, val loss: 1.1417322158813477
Epoch 340, training loss: 7.013123989105225 = 0.8944134712219238 + 1.0 * 6.118710517883301
Epoch 340, val loss: 1.1185530424118042
Epoch 350, training loss: 6.9737114906311035 = 0.8595804572105408 + 1.0 * 6.114130973815918
Epoch 350, val loss: 1.0958832502365112
Epoch 360, training loss: 6.935419082641602 = 0.8256824612617493 + 1.0 * 6.109736442565918
Epoch 360, val loss: 1.0737544298171997
Epoch 370, training loss: 6.901467323303223 = 0.7925074100494385 + 1.0 * 6.108960151672363
Epoch 370, val loss: 1.0520178079605103
Epoch 380, training loss: 6.863894939422607 = 0.7599812746047974 + 1.0 * 6.1039137840271
Epoch 380, val loss: 1.0307661294937134
Epoch 390, training loss: 6.83107852935791 = 0.7280074954032898 + 1.0 * 6.103071212768555
Epoch 390, val loss: 1.009860634803772
Epoch 400, training loss: 6.797066688537598 = 0.6966859698295593 + 1.0 * 6.100380897521973
Epoch 400, val loss: 0.9893409013748169
Epoch 410, training loss: 6.763951301574707 = 0.6659318804740906 + 1.0 * 6.098019599914551
Epoch 410, val loss: 0.969362735748291
Epoch 420, training loss: 6.736443519592285 = 0.6357889175415039 + 1.0 * 6.100654602050781
Epoch 420, val loss: 0.9498608708381653
Epoch 430, training loss: 6.70003080368042 = 0.6066160798072815 + 1.0 * 6.093414783477783
Epoch 430, val loss: 0.9310722351074219
Epoch 440, training loss: 6.670529842376709 = 0.578438401222229 + 1.0 * 6.0920915603637695
Epoch 440, val loss: 0.9132148623466492
Epoch 450, training loss: 6.641892433166504 = 0.5514223575592041 + 1.0 * 6.090469837188721
Epoch 450, val loss: 0.8964199423789978
Epoch 460, training loss: 6.616274833679199 = 0.5257319808006287 + 1.0 * 6.090542793273926
Epoch 460, val loss: 0.8809033632278442
Epoch 470, training loss: 6.587380409240723 = 0.501577615737915 + 1.0 * 6.085803031921387
Epoch 470, val loss: 0.8668618202209473
Epoch 480, training loss: 6.563108444213867 = 0.47875815629959106 + 1.0 * 6.084350109100342
Epoch 480, val loss: 0.8543528318405151
Epoch 490, training loss: 6.544552803039551 = 0.45722222328186035 + 1.0 * 6.0873308181762695
Epoch 490, val loss: 0.8434086441993713
Epoch 500, training loss: 6.520459175109863 = 0.43687042593955994 + 1.0 * 6.083588600158691
Epoch 500, val loss: 0.8340938091278076
Epoch 510, training loss: 6.496926784515381 = 0.41737207770347595 + 1.0 * 6.079554557800293
Epoch 510, val loss: 0.8262708187103271
Epoch 520, training loss: 6.483667850494385 = 0.39848124980926514 + 1.0 * 6.08518648147583
Epoch 520, val loss: 0.8196734189987183
Epoch 530, training loss: 6.457192897796631 = 0.38003936409950256 + 1.0 * 6.07715368270874
Epoch 530, val loss: 0.8141571879386902
Epoch 540, training loss: 6.436591625213623 = 0.36191970109939575 + 1.0 * 6.074671745300293
Epoch 540, val loss: 0.8096551299095154
Epoch 550, training loss: 6.426782608032227 = 0.34392866492271423 + 1.0 * 6.0828537940979
Epoch 550, val loss: 0.8058837056159973
Epoch 560, training loss: 6.401011943817139 = 0.3260318636894226 + 1.0 * 6.07498025894165
Epoch 560, val loss: 0.8029224872589111
Epoch 570, training loss: 6.379716873168945 = 0.3083529770374298 + 1.0 * 6.071363925933838
Epoch 570, val loss: 0.8008062839508057
Epoch 580, training loss: 6.360517501831055 = 0.290912389755249 + 1.0 * 6.069605350494385
Epoch 580, val loss: 0.7994030714035034
Epoch 590, training loss: 6.34183931350708 = 0.27380070090293884 + 1.0 * 6.068038463592529
Epoch 590, val loss: 0.7989342212677002
Epoch 600, training loss: 6.325769901275635 = 0.25718602538108826 + 1.0 * 6.068583965301514
Epoch 600, val loss: 0.7993425130844116
Epoch 610, training loss: 6.307878494262695 = 0.2411855310201645 + 1.0 * 6.06669282913208
Epoch 610, val loss: 0.8007211089134216
Epoch 620, training loss: 6.29456901550293 = 0.22585834562778473 + 1.0 * 6.068710803985596
Epoch 620, val loss: 0.8029993176460266
Epoch 630, training loss: 6.2756876945495605 = 0.2113201767206192 + 1.0 * 6.064367294311523
Epoch 630, val loss: 0.8060421943664551
Epoch 640, training loss: 6.265231132507324 = 0.19761982560157776 + 1.0 * 6.067611217498779
Epoch 640, val loss: 0.8099424242973328
Epoch 650, training loss: 6.2500529289245605 = 0.18481051921844482 + 1.0 * 6.065242290496826
Epoch 650, val loss: 0.8146073818206787
Epoch 660, training loss: 6.232873439788818 = 0.17294324934482574 + 1.0 * 6.059930324554443
Epoch 660, val loss: 0.8199626207351685
Epoch 670, training loss: 6.220449447631836 = 0.1618817001581192 + 1.0 * 6.058567523956299
Epoch 670, val loss: 0.8258309364318848
Epoch 680, training loss: 6.216352462768555 = 0.15160739421844482 + 1.0 * 6.06474494934082
Epoch 680, val loss: 0.8320925235748291
Epoch 690, training loss: 6.205969333648682 = 0.14212293922901154 + 1.0 * 6.063846588134766
Epoch 690, val loss: 0.8386951088905334
Epoch 700, training loss: 6.190588474273682 = 0.13342560827732086 + 1.0 * 6.057162761688232
Epoch 700, val loss: 0.8457719683647156
Epoch 710, training loss: 6.179354667663574 = 0.12538295984268188 + 1.0 * 6.053971767425537
Epoch 710, val loss: 0.8531035780906677
Epoch 720, training loss: 6.176056861877441 = 0.11791620403528214 + 1.0 * 6.058140754699707
Epoch 720, val loss: 0.8605718016624451
Epoch 730, training loss: 6.171534538269043 = 0.11102175712585449 + 1.0 * 6.060512542724609
Epoch 730, val loss: 0.8681275248527527
Epoch 740, training loss: 6.1565728187561035 = 0.1046898141503334 + 1.0 * 6.051883220672607
Epoch 740, val loss: 0.8759734034538269
Epoch 750, training loss: 6.150313377380371 = 0.09883279353380203 + 1.0 * 6.051480770111084
Epoch 750, val loss: 0.8839532732963562
Epoch 760, training loss: 6.147175312042236 = 0.09338472038507462 + 1.0 * 6.05379056930542
Epoch 760, val loss: 0.8919873237609863
Epoch 770, training loss: 6.1388630867004395 = 0.08835109323263168 + 1.0 * 6.050511837005615
Epoch 770, val loss: 0.9000829458236694
Epoch 780, training loss: 6.132724285125732 = 0.08368939161300659 + 1.0 * 6.04903507232666
Epoch 780, val loss: 0.9082362651824951
Epoch 790, training loss: 6.1268744468688965 = 0.0793878585100174 + 1.0 * 6.047486782073975
Epoch 790, val loss: 0.9164299368858337
Epoch 800, training loss: 6.125201225280762 = 0.07538799196481705 + 1.0 * 6.049813270568848
Epoch 800, val loss: 0.9245448112487793
Epoch 810, training loss: 6.116735458374023 = 0.07168158143758774 + 1.0 * 6.045053958892822
Epoch 810, val loss: 0.9327331185340881
Epoch 820, training loss: 6.13015604019165 = 0.06824120879173279 + 1.0 * 6.061914920806885
Epoch 820, val loss: 0.9409356713294983
Epoch 830, training loss: 6.1084394454956055 = 0.06503918021917343 + 1.0 * 6.043400287628174
Epoch 830, val loss: 0.9490114450454712
Epoch 840, training loss: 6.105276107788086 = 0.06207030266523361 + 1.0 * 6.043205738067627
Epoch 840, val loss: 0.9572510719299316
Epoch 850, training loss: 6.100839614868164 = 0.05929155275225639 + 1.0 * 6.041548252105713
Epoch 850, val loss: 0.96530681848526
Epoch 860, training loss: 6.106369495391846 = 0.05668819323182106 + 1.0 * 6.049681186676025
Epoch 860, val loss: 0.9731873869895935
Epoch 870, training loss: 6.094920635223389 = 0.054251529276371 + 1.0 * 6.040668964385986
Epoch 870, val loss: 0.9811570048332214
Epoch 880, training loss: 6.09173583984375 = 0.05197017267346382 + 1.0 * 6.03976583480835
Epoch 880, val loss: 0.9891242980957031
Epoch 890, training loss: 6.093599319458008 = 0.049824684858322144 + 1.0 * 6.043774604797363
Epoch 890, val loss: 0.9968223571777344
Epoch 900, training loss: 6.087106227874756 = 0.04781068488955498 + 1.0 * 6.039295673370361
Epoch 900, val loss: 1.0045912265777588
Epoch 910, training loss: 6.0869832038879395 = 0.04591900110244751 + 1.0 * 6.041064262390137
Epoch 910, val loss: 1.0123565196990967
Epoch 920, training loss: 6.081945419311523 = 0.044130805879831314 + 1.0 * 6.037814617156982
Epoch 920, val loss: 1.0199264287948608
Epoch 930, training loss: 6.078118324279785 = 0.04244194179773331 + 1.0 * 6.0356764793396
Epoch 930, val loss: 1.0274808406829834
Epoch 940, training loss: 6.076539993286133 = 0.04084046185016632 + 1.0 * 6.035699367523193
Epoch 940, val loss: 1.035016417503357
Epoch 950, training loss: 6.078279495239258 = 0.03931383788585663 + 1.0 * 6.038965702056885
Epoch 950, val loss: 1.0424168109893799
Epoch 960, training loss: 6.076094627380371 = 0.037880897521972656 + 1.0 * 6.038213729858398
Epoch 960, val loss: 1.049704670906067
Epoch 970, training loss: 6.0742411613464355 = 0.03652498871088028 + 1.0 * 6.037716388702393
Epoch 970, val loss: 1.0569541454315186
Epoch 980, training loss: 6.067531585693359 = 0.035241756588220596 + 1.0 * 6.032289981842041
Epoch 980, val loss: 1.0641465187072754
Epoch 990, training loss: 6.066225528717041 = 0.03401835262775421 + 1.0 * 6.032207012176514
Epoch 990, val loss: 1.0712473392486572
Epoch 1000, training loss: 6.066198825836182 = 0.03284824639558792 + 1.0 * 6.033350467681885
Epoch 1000, val loss: 1.0782077312469482
Epoch 1010, training loss: 6.063584804534912 = 0.0317336805164814 + 1.0 * 6.031851291656494
Epoch 1010, val loss: 1.0851306915283203
Epoch 1020, training loss: 6.0663018226623535 = 0.030673397704958916 + 1.0 * 6.035628318786621
Epoch 1020, val loss: 1.0920580625534058
Epoch 1030, training loss: 6.0603227615356445 = 0.02966412715613842 + 1.0 * 6.030658721923828
Epoch 1030, val loss: 1.0988048315048218
Epoch 1040, training loss: 6.057713985443115 = 0.02870694175362587 + 1.0 * 6.0290069580078125
Epoch 1040, val loss: 1.1055700778961182
Epoch 1050, training loss: 6.0646796226501465 = 0.02779066190123558 + 1.0 * 6.03688907623291
Epoch 1050, val loss: 1.1122055053710938
Epoch 1060, training loss: 6.0549726486206055 = 0.026908712461590767 + 1.0 * 6.028063774108887
Epoch 1060, val loss: 1.1187280416488647
Epoch 1070, training loss: 6.052517414093018 = 0.0260673388838768 + 1.0 * 6.026450157165527
Epoch 1070, val loss: 1.125264286994934
Epoch 1080, training loss: 6.052464485168457 = 0.025259368121623993 + 1.0 * 6.027204990386963
Epoch 1080, val loss: 1.1317205429077148
Epoch 1090, training loss: 6.060163974761963 = 0.024482030421495438 + 1.0 * 6.03568172454834
Epoch 1090, val loss: 1.1380192041397095
Epoch 1100, training loss: 6.050296783447266 = 0.023749040439724922 + 1.0 * 6.026547908782959
Epoch 1100, val loss: 1.14432954788208
Epoch 1110, training loss: 6.047983169555664 = 0.023042922839522362 + 1.0 * 6.024940013885498
Epoch 1110, val loss: 1.1506743431091309
Epoch 1120, training loss: 6.046664237976074 = 0.022362619638442993 + 1.0 * 6.024301528930664
Epoch 1120, val loss: 1.1569068431854248
Epoch 1130, training loss: 6.055508136749268 = 0.02171144261956215 + 1.0 * 6.033796787261963
Epoch 1130, val loss: 1.1630041599273682
Epoch 1140, training loss: 6.049017429351807 = 0.021083122119307518 + 1.0 * 6.0279340744018555
Epoch 1140, val loss: 1.1690059900283813
Epoch 1150, training loss: 6.04609489440918 = 0.02048446796834469 + 1.0 * 6.025610446929932
Epoch 1150, val loss: 1.1751232147216797
Epoch 1160, training loss: 6.044653415679932 = 0.01990911178290844 + 1.0 * 6.024744510650635
Epoch 1160, val loss: 1.181099534034729
Epoch 1170, training loss: 6.040933609008789 = 0.01935572363436222 + 1.0 * 6.021577835083008
Epoch 1170, val loss: 1.1870498657226562
Epoch 1180, training loss: 6.0459370613098145 = 0.018824473023414612 + 1.0 * 6.0271124839782715
Epoch 1180, val loss: 1.1929651498794556
Epoch 1190, training loss: 6.041050434112549 = 0.018309026956558228 + 1.0 * 6.022741317749023
Epoch 1190, val loss: 1.1987224817276
Epoch 1200, training loss: 6.0389838218688965 = 0.017815740779042244 + 1.0 * 6.021168231964111
Epoch 1200, val loss: 1.2045265436172485
Epoch 1210, training loss: 6.0392889976501465 = 0.017341136932373047 + 1.0 * 6.021947860717773
Epoch 1210, val loss: 1.2102596759796143
Epoch 1220, training loss: 6.037796497344971 = 0.016885196790099144 + 1.0 * 6.02091121673584
Epoch 1220, val loss: 1.2159521579742432
Epoch 1230, training loss: 6.040164947509766 = 0.016445456072688103 + 1.0 * 6.023719310760498
Epoch 1230, val loss: 1.221562147140503
Epoch 1240, training loss: 6.0349321365356445 = 0.016022801399230957 + 1.0 * 6.018909454345703
Epoch 1240, val loss: 1.2270450592041016
Epoch 1250, training loss: 6.039134502410889 = 0.015615192241966724 + 1.0 * 6.023519515991211
Epoch 1250, val loss: 1.2325799465179443
Epoch 1260, training loss: 6.038279056549072 = 0.015223780646920204 + 1.0 * 6.023055076599121
Epoch 1260, val loss: 1.2380049228668213
Epoch 1270, training loss: 6.03265905380249 = 0.014845631085336208 + 1.0 * 6.017813205718994
Epoch 1270, val loss: 1.2433661222457886
Epoch 1280, training loss: 6.032830715179443 = 0.014481364749372005 + 1.0 * 6.0183491706848145
Epoch 1280, val loss: 1.248779296875
Epoch 1290, training loss: 6.042731285095215 = 0.014128896407783031 + 1.0 * 6.028602600097656
Epoch 1290, val loss: 1.2539341449737549
Epoch 1300, training loss: 6.032389163970947 = 0.01379474252462387 + 1.0 * 6.018594264984131
Epoch 1300, val loss: 1.2590669393539429
Epoch 1310, training loss: 6.028947830200195 = 0.013467086479067802 + 1.0 * 6.0154805183410645
Epoch 1310, val loss: 1.2643622159957886
Epoch 1320, training loss: 6.0283589363098145 = 0.013151092454791069 + 1.0 * 6.015207767486572
Epoch 1320, val loss: 1.2694542407989502
Epoch 1330, training loss: 6.0373454093933105 = 0.01284403819590807 + 1.0 * 6.024501323699951
Epoch 1330, val loss: 1.2744171619415283
Epoch 1340, training loss: 6.031524181365967 = 0.012550748884677887 + 1.0 * 6.018973350524902
Epoch 1340, val loss: 1.2794233560562134
Epoch 1350, training loss: 6.029735565185547 = 0.012266603298485279 + 1.0 * 6.0174689292907715
Epoch 1350, val loss: 1.2843941450119019
Epoch 1360, training loss: 6.025940895080566 = 0.011991789564490318 + 1.0 * 6.013948917388916
Epoch 1360, val loss: 1.2893644571304321
Epoch 1370, training loss: 6.030817031860352 = 0.011724826879799366 + 1.0 * 6.019092082977295
Epoch 1370, val loss: 1.2942112684249878
Epoch 1380, training loss: 6.0249457359313965 = 0.011469096876680851 + 1.0 * 6.013476848602295
Epoch 1380, val loss: 1.2988225221633911
Epoch 1390, training loss: 6.024831295013428 = 0.01122210267931223 + 1.0 * 6.013609409332275
Epoch 1390, val loss: 1.303674340248108
Epoch 1400, training loss: 6.0262227058410645 = 0.010982610285282135 + 1.0 * 6.01524019241333
Epoch 1400, val loss: 1.3084039688110352
Epoch 1410, training loss: 6.023164749145508 = 0.010749372653663158 + 1.0 * 6.012415409088135
Epoch 1410, val loss: 1.3129732608795166
Epoch 1420, training loss: 6.023229122161865 = 0.010523688048124313 + 1.0 * 6.012705326080322
Epoch 1420, val loss: 1.317601203918457
Epoch 1430, training loss: 6.031590938568115 = 0.010306327603757381 + 1.0 * 6.021284580230713
Epoch 1430, val loss: 1.322078824043274
Epoch 1440, training loss: 6.0234832763671875 = 0.01009550504386425 + 1.0 * 6.013387680053711
Epoch 1440, val loss: 1.3264719247817993
Epoch 1450, training loss: 6.0205278396606445 = 0.009892103262245655 + 1.0 * 6.010635852813721
Epoch 1450, val loss: 1.3310885429382324
Epoch 1460, training loss: 6.02207612991333 = 0.009694327600300312 + 1.0 * 6.0123820304870605
Epoch 1460, val loss: 1.3355072736740112
Epoch 1470, training loss: 6.020288944244385 = 0.009502183645963669 + 1.0 * 6.010786533355713
Epoch 1470, val loss: 1.3398324251174927
Epoch 1480, training loss: 6.020617961883545 = 0.009316320531070232 + 1.0 * 6.011301517486572
Epoch 1480, val loss: 1.344185471534729
Epoch 1490, training loss: 6.02390193939209 = 0.009135949425399303 + 1.0 * 6.014766216278076
Epoch 1490, val loss: 1.3484200239181519
Epoch 1500, training loss: 6.018520832061768 = 0.008960277773439884 + 1.0 * 6.009560585021973
Epoch 1500, val loss: 1.3526535034179688
Epoch 1510, training loss: 6.017246246337891 = 0.008792441338300705 + 1.0 * 6.008453845977783
Epoch 1510, val loss: 1.3570095300674438
Epoch 1520, training loss: 6.017066478729248 = 0.008627915754914284 + 1.0 * 6.008438587188721
Epoch 1520, val loss: 1.3611695766448975
Epoch 1530, training loss: 6.029479026794434 = 0.008467464707791805 + 1.0 * 6.0210113525390625
Epoch 1530, val loss: 1.3651666641235352
Epoch 1540, training loss: 6.018598556518555 = 0.008312374353408813 + 1.0 * 6.010286331176758
Epoch 1540, val loss: 1.369053602218628
Epoch 1550, training loss: 6.015329837799072 = 0.008163170889019966 + 1.0 * 6.007166862487793
Epoch 1550, val loss: 1.3732891082763672
Epoch 1560, training loss: 6.014795303344727 = 0.008017780259251595 + 1.0 * 6.006777286529541
Epoch 1560, val loss: 1.3773455619812012
Epoch 1570, training loss: 6.015096664428711 = 0.007874422706663609 + 1.0 * 6.0072221755981445
Epoch 1570, val loss: 1.3812452554702759
Epoch 1580, training loss: 6.024472713470459 = 0.007735568564385176 + 1.0 * 6.01673698425293
Epoch 1580, val loss: 1.3850512504577637
Epoch 1590, training loss: 6.017883777618408 = 0.007599875330924988 + 1.0 * 6.010283946990967
Epoch 1590, val loss: 1.388877272605896
Epoch 1600, training loss: 6.018812656402588 = 0.007469675969332457 + 1.0 * 6.011343002319336
Epoch 1600, val loss: 1.3927863836288452
Epoch 1610, training loss: 6.019903182983398 = 0.007344966288655996 + 1.0 * 6.0125579833984375
Epoch 1610, val loss: 1.396429181098938
Epoch 1620, training loss: 6.014636516571045 = 0.007222786545753479 + 1.0 * 6.007413864135742
Epoch 1620, val loss: 1.400236964225769
Epoch 1630, training loss: 6.012168884277344 = 0.007104319985955954 + 1.0 * 6.005064487457275
Epoch 1630, val loss: 1.4040809869766235
Epoch 1640, training loss: 6.011722564697266 = 0.0069879163056612015 + 1.0 * 6.004734516143799
Epoch 1640, val loss: 1.407700538635254
Epoch 1650, training loss: 6.01704216003418 = 0.00687355175614357 + 1.0 * 6.010168552398682
Epoch 1650, val loss: 1.41129732131958
Epoch 1660, training loss: 6.012387752532959 = 0.006761271972209215 + 1.0 * 6.005626678466797
Epoch 1660, val loss: 1.4147958755493164
Epoch 1670, training loss: 6.011562347412109 = 0.006654244847595692 + 1.0 * 6.004908084869385
Epoch 1670, val loss: 1.4184709787368774
Epoch 1680, training loss: 6.01047945022583 = 0.006550007965415716 + 1.0 * 6.003929615020752
Epoch 1680, val loss: 1.4222183227539062
Epoch 1690, training loss: 6.01155424118042 = 0.006447254680097103 + 1.0 * 6.0051069259643555
Epoch 1690, val loss: 1.4256994724273682
Epoch 1700, training loss: 6.015655517578125 = 0.006347115617245436 + 1.0 * 6.009308338165283
Epoch 1700, val loss: 1.4290300607681274
Epoch 1710, training loss: 6.012172698974609 = 0.0062486557289958 + 1.0 * 6.005924224853516
Epoch 1710, val loss: 1.432397723197937
Epoch 1720, training loss: 6.021284580230713 = 0.006155458278954029 + 1.0 * 6.015129089355469
Epoch 1720, val loss: 1.4358917474746704
Epoch 1730, training loss: 6.012197494506836 = 0.006062469445168972 + 1.0 * 6.006134986877441
Epoch 1730, val loss: 1.4392486810684204
Epoch 1740, training loss: 6.008933067321777 = 0.005973358638584614 + 1.0 * 6.002959728240967
Epoch 1740, val loss: 1.4426214694976807
Epoch 1750, training loss: 6.008291721343994 = 0.005885533522814512 + 1.0 * 6.002406120300293
Epoch 1750, val loss: 1.446077585220337
Epoch 1760, training loss: 6.012784957885742 = 0.005799146369099617 + 1.0 * 6.006985664367676
Epoch 1760, val loss: 1.4492241144180298
Epoch 1770, training loss: 6.008081912994385 = 0.005715155974030495 + 1.0 * 6.002366542816162
Epoch 1770, val loss: 1.4524346590042114
Epoch 1780, training loss: 6.015782833099365 = 0.005634407512843609 + 1.0 * 6.010148525238037
Epoch 1780, val loss: 1.455669641494751
Epoch 1790, training loss: 6.00717830657959 = 0.00555370282381773 + 1.0 * 6.001624584197998
Epoch 1790, val loss: 1.4589030742645264
Epoch 1800, training loss: 6.0061492919921875 = 0.005476753227412701 + 1.0 * 6.000672340393066
Epoch 1800, val loss: 1.4621107578277588
Epoch 1810, training loss: 6.005972385406494 = 0.005400765687227249 + 1.0 * 6.0005717277526855
Epoch 1810, val loss: 1.465378761291504
Epoch 1820, training loss: 6.00531005859375 = 0.005325336009263992 + 1.0 * 5.9999847412109375
Epoch 1820, val loss: 1.468484878540039
Epoch 1830, training loss: 6.016683578491211 = 0.005251217167824507 + 1.0 * 6.01143217086792
Epoch 1830, val loss: 1.471572995185852
Epoch 1840, training loss: 6.0134453773498535 = 0.0051802461966872215 + 1.0 * 6.008265018463135
Epoch 1840, val loss: 1.4744305610656738
Epoch 1850, training loss: 6.008510112762451 = 0.005111368373036385 + 1.0 * 6.003398895263672
Epoch 1850, val loss: 1.477489948272705
Epoch 1860, training loss: 6.004579067230225 = 0.005044594407081604 + 1.0 * 5.999534606933594
Epoch 1860, val loss: 1.4807144403457642
Epoch 1870, training loss: 6.005927085876465 = 0.004978302866220474 + 1.0 * 6.000948905944824
Epoch 1870, val loss: 1.4836938381195068
Epoch 1880, training loss: 6.010443687438965 = 0.004913214594125748 + 1.0 * 6.00553035736084
Epoch 1880, val loss: 1.4864622354507446
Epoch 1890, training loss: 6.00712251663208 = 0.004849523771554232 + 1.0 * 6.002273082733154
Epoch 1890, val loss: 1.4893180131912231
Epoch 1900, training loss: 6.006498336791992 = 0.004788022954016924 + 1.0 * 6.001710414886475
Epoch 1900, val loss: 1.492357850074768
Epoch 1910, training loss: 6.004776477813721 = 0.004727226682007313 + 1.0 * 6.000049114227295
Epoch 1910, val loss: 1.4952529668807983
Epoch 1920, training loss: 6.004356384277344 = 0.004668087232857943 + 1.0 * 5.999688148498535
Epoch 1920, val loss: 1.498165488243103
Epoch 1930, training loss: 6.010159969329834 = 0.0046100979670882225 + 1.0 * 6.005549907684326
Epoch 1930, val loss: 1.5009115934371948
Epoch 1940, training loss: 6.006715297698975 = 0.004552292637526989 + 1.0 * 6.002162933349609
Epoch 1940, val loss: 1.5036791563034058
Epoch 1950, training loss: 6.003365993499756 = 0.00449680769816041 + 1.0 * 5.9988694190979
Epoch 1950, val loss: 1.5065332651138306
Epoch 1960, training loss: 6.003561496734619 = 0.004442026372998953 + 1.0 * 5.999119281768799
Epoch 1960, val loss: 1.5093709230422974
Epoch 1970, training loss: 6.007789134979248 = 0.004388048313558102 + 1.0 * 6.003401279449463
Epoch 1970, val loss: 1.5120147466659546
Epoch 1980, training loss: 6.003119468688965 = 0.004335325676947832 + 1.0 * 5.998784065246582
Epoch 1980, val loss: 1.514733910560608
Epoch 1990, training loss: 6.0026373863220215 = 0.004283561371266842 + 1.0 * 5.998353958129883
Epoch 1990, val loss: 1.5175659656524658
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.6863
Flip ASR: 0.6400/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.318144798278809 = 1.9442646503448486 + 1.0 * 8.373880386352539
Epoch 0, val loss: 1.9346716403961182
Epoch 10, training loss: 10.306819915771484 = 1.9333381652832031 + 1.0 * 8.373481750488281
Epoch 10, val loss: 1.9238808155059814
Epoch 20, training loss: 10.289992332458496 = 1.919655203819275 + 1.0 * 8.37033748626709
Epoch 20, val loss: 1.9098021984100342
Epoch 30, training loss: 10.248866081237793 = 1.9005855321884155 + 1.0 * 8.348280906677246
Epoch 30, val loss: 1.8899221420288086
Epoch 40, training loss: 10.105063438415527 = 1.8760567903518677 + 1.0 * 8.22900676727295
Epoch 40, val loss: 1.8659489154815674
Epoch 50, training loss: 9.676804542541504 = 1.8491216897964478 + 1.0 * 7.827682971954346
Epoch 50, val loss: 1.841010570526123
Epoch 60, training loss: 9.410639762878418 = 1.8234304189682007 + 1.0 * 7.587209224700928
Epoch 60, val loss: 1.8189146518707275
Epoch 70, training loss: 9.011482238769531 = 1.803731083869934 + 1.0 * 7.207751274108887
Epoch 70, val loss: 1.8028970956802368
Epoch 80, training loss: 8.616162300109863 = 1.789997935295105 + 1.0 * 6.826164722442627
Epoch 80, val loss: 1.792175054550171
Epoch 90, training loss: 8.442378044128418 = 1.7763701677322388 + 1.0 * 6.6660075187683105
Epoch 90, val loss: 1.7802191972732544
Epoch 100, training loss: 8.333850860595703 = 1.7581396102905273 + 1.0 * 6.575711250305176
Epoch 100, val loss: 1.7642604112625122
Epoch 110, training loss: 8.242920875549316 = 1.7383784055709839 + 1.0 * 6.504542350769043
Epoch 110, val loss: 1.7473512887954712
Epoch 120, training loss: 8.16156005859375 = 1.7187083959579468 + 1.0 * 6.442851543426514
Epoch 120, val loss: 1.730728030204773
Epoch 130, training loss: 8.096169471740723 = 1.696989893913269 + 1.0 * 6.399179458618164
Epoch 130, val loss: 1.7122950553894043
Epoch 140, training loss: 8.034106254577637 = 1.671177625656128 + 1.0 * 6.36292839050293
Epoch 140, val loss: 1.6909806728363037
Epoch 150, training loss: 7.973821640014648 = 1.640798807144165 + 1.0 * 6.3330230712890625
Epoch 150, val loss: 1.6665061712265015
Epoch 160, training loss: 7.914852619171143 = 1.6056876182556152 + 1.0 * 6.309165000915527
Epoch 160, val loss: 1.6387088298797607
Epoch 170, training loss: 7.853598117828369 = 1.5662771463394165 + 1.0 * 6.287321090698242
Epoch 170, val loss: 1.6077409982681274
Epoch 180, training loss: 7.791932106018066 = 1.5223610401153564 + 1.0 * 6.269570827484131
Epoch 180, val loss: 1.5736066102981567
Epoch 190, training loss: 7.728569030761719 = 1.4741698503494263 + 1.0 * 6.254399299621582
Epoch 190, val loss: 1.5367155075073242
Epoch 200, training loss: 7.667721271514893 = 1.4225398302078247 + 1.0 * 6.245181560516357
Epoch 200, val loss: 1.4980639219284058
Epoch 210, training loss: 7.599095344543457 = 1.3698267936706543 + 1.0 * 6.229268550872803
Epoch 210, val loss: 1.4597963094711304
Epoch 220, training loss: 7.534972667694092 = 1.3165947198867798 + 1.0 * 6.218378067016602
Epoch 220, val loss: 1.421797275543213
Epoch 230, training loss: 7.4717631340026855 = 1.2629910707473755 + 1.0 * 6.2087721824646
Epoch 230, val loss: 1.384164810180664
Epoch 240, training loss: 7.410975933074951 = 1.2100683450698853 + 1.0 * 6.2009077072143555
Epoch 240, val loss: 1.3473881483078003
Epoch 250, training loss: 7.350404739379883 = 1.1587940454483032 + 1.0 * 6.191610813140869
Epoch 250, val loss: 1.3119398355484009
Epoch 260, training loss: 7.29347562789917 = 1.1091119050979614 + 1.0 * 6.184363842010498
Epoch 260, val loss: 1.277439832687378
Epoch 270, training loss: 7.239500999450684 = 1.06154465675354 + 1.0 * 6.177956581115723
Epoch 270, val loss: 1.2441810369491577
Epoch 280, training loss: 7.189548969268799 = 1.0167012214660645 + 1.0 * 6.172847747802734
Epoch 280, val loss: 1.212530255317688
Epoch 290, training loss: 7.141909122467041 = 0.9739716649055481 + 1.0 * 6.167937278747559
Epoch 290, val loss: 1.182126760482788
Epoch 300, training loss: 7.093911170959473 = 0.9330330491065979 + 1.0 * 6.1608781814575195
Epoch 300, val loss: 1.1525789499282837
Epoch 310, training loss: 7.048436164855957 = 0.8930548429489136 + 1.0 * 6.155381202697754
Epoch 310, val loss: 1.1236765384674072
Epoch 320, training loss: 7.014858245849609 = 0.8534473180770874 + 1.0 * 6.161410808563232
Epoch 320, val loss: 1.0947269201278687
Epoch 330, training loss: 6.962250232696533 = 0.8142927885055542 + 1.0 * 6.1479573249816895
Epoch 330, val loss: 1.0660403966903687
Epoch 340, training loss: 6.917673587799072 = 0.774941623210907 + 1.0 * 6.1427321434021
Epoch 340, val loss: 1.0375773906707764
Epoch 350, training loss: 6.874251365661621 = 0.7351301312446594 + 1.0 * 6.139121055603027
Epoch 350, val loss: 1.00877845287323
Epoch 360, training loss: 6.838888645172119 = 0.6950100064277649 + 1.0 * 6.14387845993042
Epoch 360, val loss: 0.9799243807792664
Epoch 370, training loss: 6.791388511657715 = 0.6553720831871033 + 1.0 * 6.136016368865967
Epoch 370, val loss: 0.9517609477043152
Epoch 380, training loss: 6.746471881866455 = 0.616456925868988 + 1.0 * 6.130014896392822
Epoch 380, val loss: 0.9244793653488159
Epoch 390, training loss: 6.705963611602783 = 0.5785778164863586 + 1.0 * 6.12738561630249
Epoch 390, val loss: 0.8982083797454834
Epoch 400, training loss: 6.670021057128906 = 0.5420209169387817 + 1.0 * 6.128000259399414
Epoch 400, val loss: 0.8731984496116638
Epoch 410, training loss: 6.628807544708252 = 0.5072348713874817 + 1.0 * 6.121572494506836
Epoch 410, val loss: 0.8500384092330933
Epoch 420, training loss: 6.592972278594971 = 0.47414451837539673 + 1.0 * 6.118827819824219
Epoch 420, val loss: 0.828604519367218
Epoch 430, training loss: 6.572721481323242 = 0.44275784492492676 + 1.0 * 6.129963397979736
Epoch 430, val loss: 0.8088564276695251
Epoch 440, training loss: 6.527522563934326 = 0.41348010301589966 + 1.0 * 6.114042282104492
Epoch 440, val loss: 0.7911022305488586
Epoch 450, training loss: 6.49746561050415 = 0.3858984112739563 + 1.0 * 6.11156702041626
Epoch 450, val loss: 0.7754070162773132
Epoch 460, training loss: 6.4744954109191895 = 0.3598190248012543 + 1.0 * 6.114676475524902
Epoch 460, val loss: 0.7612016201019287
Epoch 470, training loss: 6.444936752319336 = 0.3353823125362396 + 1.0 * 6.109554290771484
Epoch 470, val loss: 0.7486670613288879
Epoch 480, training loss: 6.417603969573975 = 0.3125064969062805 + 1.0 * 6.10509729385376
Epoch 480, val loss: 0.7377051711082458
Epoch 490, training loss: 6.394298553466797 = 0.29101988673210144 + 1.0 * 6.103278636932373
Epoch 490, val loss: 0.7280282974243164
Epoch 500, training loss: 6.373959541320801 = 0.27092376351356506 + 1.0 * 6.103035926818848
Epoch 500, val loss: 0.7194844484329224
Epoch 510, training loss: 6.353607177734375 = 0.2523173689842224 + 1.0 * 6.101289749145508
Epoch 510, val loss: 0.7122271060943604
Epoch 520, training loss: 6.331159591674805 = 0.23514288663864136 + 1.0 * 6.096016883850098
Epoch 520, val loss: 0.706265389919281
Epoch 530, training loss: 6.312593460083008 = 0.21916937828063965 + 1.0 * 6.093424320220947
Epoch 530, val loss: 0.7011662721633911
Epoch 540, training loss: 6.296976089477539 = 0.20441915094852448 + 1.0 * 6.092556953430176
Epoch 540, val loss: 0.6968362331390381
Epoch 550, training loss: 6.281002521514893 = 0.19094310700893402 + 1.0 * 6.090059280395508
Epoch 550, val loss: 0.6937005519866943
Epoch 560, training loss: 6.26675271987915 = 0.1785086989402771 + 1.0 * 6.0882439613342285
Epoch 560, val loss: 0.6914343237876892
Epoch 570, training loss: 6.261415958404541 = 0.16700556874275208 + 1.0 * 6.094410419464111
Epoch 570, val loss: 0.6898148655891418
Epoch 580, training loss: 6.244306564331055 = 0.15646308660507202 + 1.0 * 6.087843418121338
Epoch 580, val loss: 0.6889018416404724
Epoch 590, training loss: 6.230255126953125 = 0.14672936499118805 + 1.0 * 6.083525657653809
Epoch 590, val loss: 0.6888158321380615
Epoch 600, training loss: 6.219081401824951 = 0.13773737847805023 + 1.0 * 6.081344127655029
Epoch 600, val loss: 0.689264178276062
Epoch 610, training loss: 6.2141265869140625 = 0.12945294380187988 + 1.0 * 6.084673881530762
Epoch 610, val loss: 0.6901060342788696
Epoch 620, training loss: 6.201416969299316 = 0.12186695635318756 + 1.0 * 6.079549789428711
Epoch 620, val loss: 0.6915914416313171
Epoch 630, training loss: 6.191849231719971 = 0.11486958712339401 + 1.0 * 6.076979637145996
Epoch 630, val loss: 0.693651556968689
Epoch 640, training loss: 6.19437837600708 = 0.10839333385229111 + 1.0 * 6.08598518371582
Epoch 640, val loss: 0.6959438323974609
Epoch 650, training loss: 6.177853107452393 = 0.1024295836687088 + 1.0 * 6.075423717498779
Epoch 650, val loss: 0.6986435651779175
Epoch 660, training loss: 6.170528888702393 = 0.09689527750015259 + 1.0 * 6.073633670806885
Epoch 660, val loss: 0.7017751336097717
Epoch 670, training loss: 6.163501739501953 = 0.09173549711704254 + 1.0 * 6.071766376495361
Epoch 670, val loss: 0.7051157355308533
Epoch 680, training loss: 6.163549900054932 = 0.08694478124380112 + 1.0 * 6.076605319976807
Epoch 680, val loss: 0.7085834741592407
Epoch 690, training loss: 6.154906749725342 = 0.08251672983169556 + 1.0 * 6.072390079498291
Epoch 690, val loss: 0.7123838663101196
Epoch 700, training loss: 6.148106098175049 = 0.07839754223823547 + 1.0 * 6.069708347320557
Epoch 700, val loss: 0.7165630459785461
Epoch 710, training loss: 6.142258644104004 = 0.07453756779432297 + 1.0 * 6.067720890045166
Epoch 710, val loss: 0.7207667827606201
Epoch 720, training loss: 6.143446445465088 = 0.07092681527137756 + 1.0 * 6.072519779205322
Epoch 720, val loss: 0.7250420451164246
Epoch 730, training loss: 6.137438774108887 = 0.06757274270057678 + 1.0 * 6.069866180419922
Epoch 730, val loss: 0.7295258641242981
Epoch 740, training loss: 6.129701137542725 = 0.06443601846694946 + 1.0 * 6.06526517868042
Epoch 740, val loss: 0.734085738658905
Epoch 750, training loss: 6.125278472900391 = 0.061497580260038376 + 1.0 * 6.063780784606934
Epoch 750, val loss: 0.7387731075286865
Epoch 760, training loss: 6.122659683227539 = 0.05874132737517357 + 1.0 * 6.063918590545654
Epoch 760, val loss: 0.7435191869735718
Epoch 770, training loss: 6.12186861038208 = 0.05615108087658882 + 1.0 * 6.065717697143555
Epoch 770, val loss: 0.748287558555603
Epoch 780, training loss: 6.115704536437988 = 0.053736988455057144 + 1.0 * 6.061967372894287
Epoch 780, val loss: 0.7531422972679138
Epoch 790, training loss: 6.112277030944824 = 0.05146530643105507 + 1.0 * 6.060811519622803
Epoch 790, val loss: 0.7581269145011902
Epoch 800, training loss: 6.113120079040527 = 0.04932241514325142 + 1.0 * 6.063797473907471
Epoch 800, val loss: 0.7630312442779541
Epoch 810, training loss: 6.106700897216797 = 0.04731127247214317 + 1.0 * 6.059389591217041
Epoch 810, val loss: 0.7679983973503113
Epoch 820, training loss: 6.101607322692871 = 0.04541404917836189 + 1.0 * 6.0561933517456055
Epoch 820, val loss: 0.7730593085289001
Epoch 830, training loss: 6.1083269119262695 = 0.04361695051193237 + 1.0 * 6.0647101402282715
Epoch 830, val loss: 0.7780449390411377
Epoch 840, training loss: 6.101439476013184 = 0.04192381724715233 + 1.0 * 6.059515476226807
Epoch 840, val loss: 0.7829601764678955
Epoch 850, training loss: 6.0987772941589355 = 0.04032956063747406 + 1.0 * 6.05844783782959
Epoch 850, val loss: 0.7880406379699707
Epoch 860, training loss: 6.09267520904541 = 0.0388239361345768 + 1.0 * 6.053851127624512
Epoch 860, val loss: 0.7930852770805359
Epoch 870, training loss: 6.091532230377197 = 0.037397269159555435 + 1.0 * 6.054134845733643
Epoch 870, val loss: 0.7981172204017639
Epoch 880, training loss: 6.093698978424072 = 0.03603649511933327 + 1.0 * 6.057662487030029
Epoch 880, val loss: 0.8030722737312317
Epoch 890, training loss: 6.0882110595703125 = 0.03475665673613548 + 1.0 * 6.053454399108887
Epoch 890, val loss: 0.8079895973205566
Epoch 900, training loss: 6.0863165855407715 = 0.03353644907474518 + 1.0 * 6.0527801513671875
Epoch 900, val loss: 0.8130205273628235
Epoch 910, training loss: 6.083081245422363 = 0.032378245145082474 + 1.0 * 6.050703048706055
Epoch 910, val loss: 0.8179530501365662
Epoch 920, training loss: 6.085529804229736 = 0.031273454427719116 + 1.0 * 6.054256439208984
Epoch 920, val loss: 0.8228667378425598
Epoch 930, training loss: 6.087000846862793 = 0.030232524499297142 + 1.0 * 6.056768417358398
Epoch 930, val loss: 0.8277978301048279
Epoch 940, training loss: 6.078742504119873 = 0.029239492490887642 + 1.0 * 6.049502849578857
Epoch 940, val loss: 0.8326017260551453
Epoch 950, training loss: 6.074915885925293 = 0.02829732745885849 + 1.0 * 6.046618461608887
Epoch 950, val loss: 0.837533175945282
Epoch 960, training loss: 6.073263645172119 = 0.027393942698836327 + 1.0 * 6.045869827270508
Epoch 960, val loss: 0.8423464894294739
Epoch 970, training loss: 6.08088493347168 = 0.026527641341090202 + 1.0 * 6.054357528686523
Epoch 970, val loss: 0.8470105528831482
Epoch 980, training loss: 6.073122024536133 = 0.02571088820695877 + 1.0 * 6.04741096496582
Epoch 980, val loss: 0.8517633676528931
Epoch 990, training loss: 6.069846153259277 = 0.02492510713636875 + 1.0 * 6.044920921325684
Epoch 990, val loss: 0.8565149903297424
Epoch 1000, training loss: 6.071412563323975 = 0.02417517639696598 + 1.0 * 6.047237396240234
Epoch 1000, val loss: 0.8611487150192261
Epoch 1010, training loss: 6.070638179779053 = 0.023462491109967232 + 1.0 * 6.047175884246826
Epoch 1010, val loss: 0.8658076524734497
Epoch 1020, training loss: 6.06571102142334 = 0.02277742139995098 + 1.0 * 6.042933464050293
Epoch 1020, val loss: 0.8704134225845337
Epoch 1030, training loss: 6.068424224853516 = 0.022122453898191452 + 1.0 * 6.04630184173584
Epoch 1030, val loss: 0.8750027418136597
Epoch 1040, training loss: 6.062345504760742 = 0.021492887288331985 + 1.0 * 6.0408525466918945
Epoch 1040, val loss: 0.8795081377029419
Epoch 1050, training loss: 6.063962459564209 = 0.02089095488190651 + 1.0 * 6.043071269989014
Epoch 1050, val loss: 0.8839977979660034
Epoch 1060, training loss: 6.061525821685791 = 0.020314689725637436 + 1.0 * 6.041211128234863
Epoch 1060, val loss: 0.8884028792381287
Epoch 1070, training loss: 6.059100151062012 = 0.019763797521591187 + 1.0 * 6.039336204528809
Epoch 1070, val loss: 0.8928447961807251
Epoch 1080, training loss: 6.059507846832275 = 0.01923343911767006 + 1.0 * 6.040274620056152
Epoch 1080, val loss: 0.8972112536430359
Epoch 1090, training loss: 6.062605857849121 = 0.01871970295906067 + 1.0 * 6.043886184692383
Epoch 1090, val loss: 0.9014692902565002
Epoch 1100, training loss: 6.0560832023620605 = 0.018231606110930443 + 1.0 * 6.037851810455322
Epoch 1100, val loss: 0.9057356119155884
Epoch 1110, training loss: 6.055680751800537 = 0.01776360534131527 + 1.0 * 6.037917137145996
Epoch 1110, val loss: 0.9100303649902344
Epoch 1120, training loss: 6.058986663818359 = 0.01731037348508835 + 1.0 * 6.0416765213012695
Epoch 1120, val loss: 0.9141932725906372
Epoch 1130, training loss: 6.0527167320251465 = 0.01687406748533249 + 1.0 * 6.0358428955078125
Epoch 1130, val loss: 0.9183309674263
Epoch 1140, training loss: 6.055799961090088 = 0.016456108540296555 + 1.0 * 6.03934383392334
Epoch 1140, val loss: 0.9224763512611389
Epoch 1150, training loss: 6.051031112670898 = 0.016054216772317886 + 1.0 * 6.034976959228516
Epoch 1150, val loss: 0.9265816807746887
Epoch 1160, training loss: 6.051799774169922 = 0.01566668599843979 + 1.0 * 6.036133289337158
Epoch 1160, val loss: 0.9306502342224121
Epoch 1170, training loss: 6.050528049468994 = 0.015292766503989697 + 1.0 * 6.035235404968262
Epoch 1170, val loss: 0.9346480965614319
Epoch 1180, training loss: 6.049583435058594 = 0.014931740239262581 + 1.0 * 6.034651756286621
Epoch 1180, val loss: 0.9386162757873535
Epoch 1190, training loss: 6.047398567199707 = 0.014583081007003784 + 1.0 * 6.032815456390381
Epoch 1190, val loss: 0.9425773620605469
Epoch 1200, training loss: 6.046987533569336 = 0.014245875179767609 + 1.0 * 6.032741546630859
Epoch 1200, val loss: 0.9465261101722717
Epoch 1210, training loss: 6.055497169494629 = 0.013921329751610756 + 1.0 * 6.041575908660889
Epoch 1210, val loss: 0.9503868818283081
Epoch 1220, training loss: 6.04701566696167 = 0.01361010130494833 + 1.0 * 6.033405780792236
Epoch 1220, val loss: 0.9541453123092651
Epoch 1230, training loss: 6.043630599975586 = 0.013310542330145836 + 1.0 * 6.030320167541504
Epoch 1230, val loss: 0.9580259919166565
Epoch 1240, training loss: 6.0464396476745605 = 0.013019208796322346 + 1.0 * 6.033420562744141
Epoch 1240, val loss: 0.9618252515792847
Epoch 1250, training loss: 6.042640686035156 = 0.012738336808979511 + 1.0 * 6.029902458190918
Epoch 1250, val loss: 0.9655570983886719
Epoch 1260, training loss: 6.042599201202393 = 0.012465848587453365 + 1.0 * 6.030133247375488
Epoch 1260, val loss: 0.9693127870559692
Epoch 1270, training loss: 6.048191547393799 = 0.012200606986880302 + 1.0 * 6.0359907150268555
Epoch 1270, val loss: 0.972905158996582
Epoch 1280, training loss: 6.047213077545166 = 0.01194769423455 + 1.0 * 6.0352654457092285
Epoch 1280, val loss: 0.9765509963035583
Epoch 1290, training loss: 6.039144992828369 = 0.011702806688845158 + 1.0 * 6.02744197845459
Epoch 1290, val loss: 0.9801547527313232
Epoch 1300, training loss: 6.038857460021973 = 0.011466730386018753 + 1.0 * 6.027390956878662
Epoch 1300, val loss: 0.9838132262229919
Epoch 1310, training loss: 6.037194728851318 = 0.011234809644520283 + 1.0 * 6.0259599685668945
Epoch 1310, val loss: 0.9873793125152588
Epoch 1320, training loss: 6.040277481079102 = 0.011008602567017078 + 1.0 * 6.029268741607666
Epoch 1320, val loss: 0.9908885955810547
Epoch 1330, training loss: 6.039328098297119 = 0.010789511725306511 + 1.0 * 6.028538703918457
Epoch 1330, val loss: 0.994293749332428
Epoch 1340, training loss: 6.037842273712158 = 0.010579001158475876 + 1.0 * 6.027263164520264
Epoch 1340, val loss: 0.9977797865867615
Epoch 1350, training loss: 6.038435459136963 = 0.010377049446105957 + 1.0 * 6.0280585289001465
Epoch 1350, val loss: 1.0012246370315552
Epoch 1360, training loss: 6.040642261505127 = 0.010180017910897732 + 1.0 * 6.030462265014648
Epoch 1360, val loss: 1.0045664310455322
Epoch 1370, training loss: 6.035477638244629 = 0.00998726300895214 + 1.0 * 6.0254902839660645
Epoch 1370, val loss: 1.0079728364944458
Epoch 1380, training loss: 6.032820224761963 = 0.009801147505640984 + 1.0 * 6.023019313812256
Epoch 1380, val loss: 1.0113558769226074
Epoch 1390, training loss: 6.034871578216553 = 0.009619715623557568 + 1.0 * 6.025251865386963
Epoch 1390, val loss: 1.0147004127502441
Epoch 1400, training loss: 6.0389509201049805 = 0.009441290982067585 + 1.0 * 6.029509544372559
Epoch 1400, val loss: 1.0179156064987183
Epoch 1410, training loss: 6.033451080322266 = 0.009271361865103245 + 1.0 * 6.024179935455322
Epoch 1410, val loss: 1.0211430788040161
Epoch 1420, training loss: 6.033438682556152 = 0.009106820449233055 + 1.0 * 6.024332046508789
Epoch 1420, val loss: 1.0244306325912476
Epoch 1430, training loss: 6.034456253051758 = 0.008945099078118801 + 1.0 * 6.025511264801025
Epoch 1430, val loss: 1.0275981426239014
Epoch 1440, training loss: 6.031619548797607 = 0.00878965388983488 + 1.0 * 6.022830009460449
Epoch 1440, val loss: 1.0307948589324951
Epoch 1450, training loss: 6.03013801574707 = 0.008636350743472576 + 1.0 * 6.021501541137695
Epoch 1450, val loss: 1.0339549779891968
Epoch 1460, training loss: 6.033951282501221 = 0.008487989194691181 + 1.0 * 6.025463104248047
Epoch 1460, val loss: 1.0370845794677734
Epoch 1470, training loss: 6.030132293701172 = 0.008342279121279716 + 1.0 * 6.021790027618408
Epoch 1470, val loss: 1.040088176727295
Epoch 1480, training loss: 6.032902240753174 = 0.00820197630673647 + 1.0 * 6.024700164794922
Epoch 1480, val loss: 1.0431662797927856
Epoch 1490, training loss: 6.030002593994141 = 0.008065488189458847 + 1.0 * 6.021936893463135
Epoch 1490, val loss: 1.0461657047271729
Epoch 1500, training loss: 6.029139518737793 = 0.007933295331895351 + 1.0 * 6.021206378936768
Epoch 1500, val loss: 1.049208164215088
Epoch 1510, training loss: 6.031128406524658 = 0.007802996318787336 + 1.0 * 6.023325443267822
Epoch 1510, val loss: 1.05220365524292
Epoch 1520, training loss: 6.027982711791992 = 0.007676411885768175 + 1.0 * 6.02030611038208
Epoch 1520, val loss: 1.0551066398620605
Epoch 1530, training loss: 6.028465270996094 = 0.007554118987172842 + 1.0 * 6.02091121673584
Epoch 1530, val loss: 1.058043360710144
Epoch 1540, training loss: 6.027176380157471 = 0.00743449293076992 + 1.0 * 6.019742012023926
Epoch 1540, val loss: 1.0609440803527832
Epoch 1550, training loss: 6.024966716766357 = 0.007317354436963797 + 1.0 * 6.017649173736572
Epoch 1550, val loss: 1.0638203620910645
Epoch 1560, training loss: 6.028586387634277 = 0.00720306346192956 + 1.0 * 6.021383285522461
Epoch 1560, val loss: 1.0666931867599487
Epoch 1570, training loss: 6.026907444000244 = 0.0070912218652665615 + 1.0 * 6.0198163986206055
Epoch 1570, val loss: 1.0694749355316162
Epoch 1580, training loss: 6.027362823486328 = 0.0069830226711928844 + 1.0 * 6.020380020141602
Epoch 1580, val loss: 1.0722448825836182
Epoch 1590, training loss: 6.025317192077637 = 0.006877818610519171 + 1.0 * 6.018439292907715
Epoch 1590, val loss: 1.074977159500122
Epoch 1600, training loss: 6.028434753417969 = 0.00677537964656949 + 1.0 * 6.0216593742370605
Epoch 1600, val loss: 1.077746868133545
Epoch 1610, training loss: 6.021860122680664 = 0.006674526259303093 + 1.0 * 6.015185832977295
Epoch 1610, val loss: 1.080422043800354
Epoch 1620, training loss: 6.021639823913574 = 0.00657674390822649 + 1.0 * 6.015063285827637
Epoch 1620, val loss: 1.083139419555664
Epoch 1630, training loss: 6.025446891784668 = 0.00648032920435071 + 1.0 * 6.0189666748046875
Epoch 1630, val loss: 1.085784673690796
Epoch 1640, training loss: 6.024651050567627 = 0.00638548843562603 + 1.0 * 6.018265724182129
Epoch 1640, val loss: 1.0883760452270508
Epoch 1650, training loss: 6.022128582000732 = 0.006294870749115944 + 1.0 * 6.015833854675293
Epoch 1650, val loss: 1.0910303592681885
Epoch 1660, training loss: 6.0203633308410645 = 0.006205465644598007 + 1.0 * 6.014157772064209
Epoch 1660, val loss: 1.0936270952224731
Epoch 1670, training loss: 6.026394844055176 = 0.0061180912889540195 + 1.0 * 6.0202765464782715
Epoch 1670, val loss: 1.0962339639663696
Epoch 1680, training loss: 6.020055294036865 = 0.006031883880496025 + 1.0 * 6.014023303985596
Epoch 1680, val loss: 1.098711371421814
Epoch 1690, training loss: 6.021800994873047 = 0.005948198959231377 + 1.0 * 6.015852928161621
Epoch 1690, val loss: 1.1012492179870605
Epoch 1700, training loss: 6.021689414978027 = 0.005866638384759426 + 1.0 * 6.015822887420654
Epoch 1700, val loss: 1.103763222694397
Epoch 1710, training loss: 6.02064847946167 = 0.005787236616015434 + 1.0 * 6.014861106872559
Epoch 1710, val loss: 1.1062544584274292
Epoch 1720, training loss: 6.021840572357178 = 0.005709981545805931 + 1.0 * 6.016130447387695
Epoch 1720, val loss: 1.1086857318878174
Epoch 1730, training loss: 6.020780563354492 = 0.0056328484788537025 + 1.0 * 6.015147686004639
Epoch 1730, val loss: 1.1110987663269043
Epoch 1740, training loss: 6.017885208129883 = 0.005558923818171024 + 1.0 * 6.012326240539551
Epoch 1740, val loss: 1.1135060787200928
Epoch 1750, training loss: 6.017165660858154 = 0.0054861255921423435 + 1.0 * 6.011679649353027
Epoch 1750, val loss: 1.1159294843673706
Epoch 1760, training loss: 6.017064094543457 = 0.005414141342043877 + 1.0 * 6.011650085449219
Epoch 1760, val loss: 1.118313193321228
Epoch 1770, training loss: 6.021517276763916 = 0.005343177355825901 + 1.0 * 6.01617431640625
Epoch 1770, val loss: 1.1206426620483398
Epoch 1780, training loss: 6.019351959228516 = 0.005274368915706873 + 1.0 * 6.014077663421631
Epoch 1780, val loss: 1.1229817867279053
Epoch 1790, training loss: 6.017266750335693 = 0.005207288544625044 + 1.0 * 6.012059688568115
Epoch 1790, val loss: 1.1252549886703491
Epoch 1800, training loss: 6.017319202423096 = 0.005141946952790022 + 1.0 * 6.012177467346191
Epoch 1800, val loss: 1.127605676651001
Epoch 1810, training loss: 6.0181403160095215 = 0.005077922251075506 + 1.0 * 6.013062477111816
Epoch 1810, val loss: 1.1299036741256714
Epoch 1820, training loss: 6.016246795654297 = 0.005015004891902208 + 1.0 * 6.011231899261475
Epoch 1820, val loss: 1.1321876049041748
Epoch 1830, training loss: 6.015535831451416 = 0.004953625146299601 + 1.0 * 6.010581970214844
Epoch 1830, val loss: 1.1344412565231323
Epoch 1840, training loss: 6.013869762420654 = 0.0048927734605968 + 1.0 * 6.008976936340332
Epoch 1840, val loss: 1.136702060699463
Epoch 1850, training loss: 6.018172264099121 = 0.004832628648728132 + 1.0 * 6.013339519500732
Epoch 1850, val loss: 1.1389062404632568
Epoch 1860, training loss: 6.016878128051758 = 0.00477380957454443 + 1.0 * 6.012104511260986
Epoch 1860, val loss: 1.1410436630249023
Epoch 1870, training loss: 6.015804767608643 = 0.004717088304460049 + 1.0 * 6.011087894439697
Epoch 1870, val loss: 1.1432600021362305
Epoch 1880, training loss: 6.0128865242004395 = 0.004661727696657181 + 1.0 * 6.008224964141846
Epoch 1880, val loss: 1.145443081855774
Epoch 1890, training loss: 6.0189433097839355 = 0.004607248120009899 + 1.0 * 6.014336109161377
Epoch 1890, val loss: 1.1476202011108398
Epoch 1900, training loss: 6.012754917144775 = 0.004553013015538454 + 1.0 * 6.008202075958252
Epoch 1900, val loss: 1.1496742963790894
Epoch 1910, training loss: 6.011322975158691 = 0.004500590730458498 + 1.0 * 6.00682258605957
Epoch 1910, val loss: 1.1518499851226807
Epoch 1920, training loss: 6.011624813079834 = 0.004448333755135536 + 1.0 * 6.007176399230957
Epoch 1920, val loss: 1.1539719104766846
Epoch 1930, training loss: 6.0187458992004395 = 0.004396429751068354 + 1.0 * 6.014349460601807
Epoch 1930, val loss: 1.156025767326355
Epoch 1940, training loss: 6.017551422119141 = 0.004346669185906649 + 1.0 * 6.013204574584961
Epoch 1940, val loss: 1.1580641269683838
Epoch 1950, training loss: 6.013535499572754 = 0.004298278130590916 + 1.0 * 6.009237289428711
Epoch 1950, val loss: 1.1601598262786865
Epoch 1960, training loss: 6.017889499664307 = 0.0042511047795414925 + 1.0 * 6.013638496398926
Epoch 1960, val loss: 1.1621791124343872
Epoch 1970, training loss: 6.011135578155518 = 0.004204324912279844 + 1.0 * 6.006931304931641
Epoch 1970, val loss: 1.164196491241455
Epoch 1980, training loss: 6.0101776123046875 = 0.004158547148108482 + 1.0 * 6.006019115447998
Epoch 1980, val loss: 1.1662489175796509
Epoch 1990, training loss: 6.012624740600586 = 0.00411296496167779 + 1.0 * 6.008511543273926
Epoch 1990, val loss: 1.1682512760162354
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8487
Flip ASR: 0.8267/225 nodes
The final ASR:0.76507, 0.06638, Accuracy:0.80247, 0.01397
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11568])
remove edge: torch.Size([2, 9538])
updated graph: torch.Size([2, 10550])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98032, 0.00920, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 1 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.33936882019043 = 1.9654614925384521 + 1.0 * 8.373907089233398
Epoch 0, val loss: 1.962340235710144
Epoch 10, training loss: 10.328570365905762 = 1.9550448656082153 + 1.0 * 8.373525619506836
Epoch 10, val loss: 1.9529950618743896
Epoch 20, training loss: 10.312982559204102 = 1.9423906803131104 + 1.0 * 8.37059211730957
Epoch 20, val loss: 1.9412990808486938
Epoch 30, training loss: 10.273871421813965 = 1.9252283573150635 + 1.0 * 8.34864330291748
Epoch 30, val loss: 1.9251688718795776
Epoch 40, training loss: 10.09841251373291 = 1.9030444622039795 + 1.0 * 8.195367813110352
Epoch 40, val loss: 1.904988408088684
Epoch 50, training loss: 9.536601066589355 = 1.8793145418167114 + 1.0 * 7.657286167144775
Epoch 50, val loss: 1.8838409185409546
Epoch 60, training loss: 9.131491661071777 = 1.858731985092163 + 1.0 * 7.272759437561035
Epoch 60, val loss: 1.865932822227478
Epoch 70, training loss: 8.794931411743164 = 1.8405803442001343 + 1.0 * 6.954351425170898
Epoch 70, val loss: 1.8500921726226807
Epoch 80, training loss: 8.562276840209961 = 1.8221924304962158 + 1.0 * 6.740084171295166
Epoch 80, val loss: 1.8339941501617432
Epoch 90, training loss: 8.426697731018066 = 1.8016208410263062 + 1.0 * 6.625077247619629
Epoch 90, val loss: 1.8164211511611938
Epoch 100, training loss: 8.321368217468262 = 1.779489517211914 + 1.0 * 6.541878700256348
Epoch 100, val loss: 1.7982475757598877
Epoch 110, training loss: 8.234894752502441 = 1.7571927309036255 + 1.0 * 6.477701663970947
Epoch 110, val loss: 1.779807209968567
Epoch 120, training loss: 8.164417266845703 = 1.7338030338287354 + 1.0 * 6.430613994598389
Epoch 120, val loss: 1.7599022388458252
Epoch 130, training loss: 8.09787654876709 = 1.7086350917816162 + 1.0 * 6.3892412185668945
Epoch 130, val loss: 1.738284707069397
Epoch 140, training loss: 8.036008834838867 = 1.6805225610733032 + 1.0 * 6.3554863929748535
Epoch 140, val loss: 1.7142648696899414
Epoch 150, training loss: 7.980108261108398 = 1.6479620933532715 + 1.0 * 6.332146167755127
Epoch 150, val loss: 1.6868385076522827
Epoch 160, training loss: 7.919406890869141 = 1.611241340637207 + 1.0 * 6.308165550231934
Epoch 160, val loss: 1.6561092138290405
Epoch 170, training loss: 7.85890007019043 = 1.5699626207351685 + 1.0 * 6.288937568664551
Epoch 170, val loss: 1.6217896938323975
Epoch 180, training loss: 7.796369552612305 = 1.5235949754714966 + 1.0 * 6.272774696350098
Epoch 180, val loss: 1.583189845085144
Epoch 190, training loss: 7.73115873336792 = 1.471945881843567 + 1.0 * 6.259212970733643
Epoch 190, val loss: 1.5403430461883545
Epoch 200, training loss: 7.666270732879639 = 1.415977120399475 + 1.0 * 6.250293731689453
Epoch 200, val loss: 1.494212031364441
Epoch 210, training loss: 7.596218109130859 = 1.3577938079833984 + 1.0 * 6.238424301147461
Epoch 210, val loss: 1.446868658065796
Epoch 220, training loss: 7.5274858474731445 = 1.2983520030975342 + 1.0 * 6.2291340827941895
Epoch 220, val loss: 1.3992433547973633
Epoch 230, training loss: 7.467998027801514 = 1.2389698028564453 + 1.0 * 6.229028224945068
Epoch 230, val loss: 1.3526382446289062
Epoch 240, training loss: 7.396139621734619 = 1.182193636894226 + 1.0 * 6.2139458656311035
Epoch 240, val loss: 1.3088756799697876
Epoch 250, training loss: 7.335699081420898 = 1.1276580095291138 + 1.0 * 6.208041191101074
Epoch 250, val loss: 1.267612338066101
Epoch 260, training loss: 7.276824474334717 = 1.0754047632217407 + 1.0 * 6.201419830322266
Epoch 260, val loss: 1.2287991046905518
Epoch 270, training loss: 7.225854873657227 = 1.0261790752410889 + 1.0 * 6.199675559997559
Epoch 270, val loss: 1.1928292512893677
Epoch 280, training loss: 7.170001983642578 = 0.9799104928970337 + 1.0 * 6.190091609954834
Epoch 280, val loss: 1.1597431898117065
Epoch 290, training loss: 7.126744747161865 = 0.936226487159729 + 1.0 * 6.190518379211426
Epoch 290, val loss: 1.128965973854065
Epoch 300, training loss: 7.076820373535156 = 0.8952386379241943 + 1.0 * 6.181581497192383
Epoch 300, val loss: 1.100340723991394
Epoch 310, training loss: 7.0326247215271 = 0.8563252687454224 + 1.0 * 6.176299571990967
Epoch 310, val loss: 1.07351553440094
Epoch 320, training loss: 6.993888854980469 = 0.8193185329437256 + 1.0 * 6.174570560455322
Epoch 320, val loss: 1.0482679605484009
Epoch 330, training loss: 6.9531755447387695 = 0.784355878829956 + 1.0 * 6.168819904327393
Epoch 330, val loss: 1.0245417356491089
Epoch 340, training loss: 6.914758205413818 = 0.750934898853302 + 1.0 * 6.163823127746582
Epoch 340, val loss: 1.0021308660507202
Epoch 350, training loss: 6.885982513427734 = 0.7188510298728943 + 1.0 * 6.167131423950195
Epoch 350, val loss: 0.9807888269424438
Epoch 360, training loss: 6.84618616104126 = 0.6882776618003845 + 1.0 * 6.1579084396362305
Epoch 360, val loss: 0.9607136845588684
Epoch 370, training loss: 6.812580108642578 = 0.6586582660675049 + 1.0 * 6.153921604156494
Epoch 370, val loss: 0.9417100548744202
Epoch 380, training loss: 6.781920433044434 = 0.6298577785491943 + 1.0 * 6.15206241607666
Epoch 380, val loss: 0.9235728979110718
Epoch 390, training loss: 6.749464511871338 = 0.6019225716590881 + 1.0 * 6.1475419998168945
Epoch 390, val loss: 0.9063558578491211
Epoch 400, training loss: 6.71878719329834 = 0.5744737982749939 + 1.0 * 6.144313335418701
Epoch 400, val loss: 0.8900479078292847
Epoch 410, training loss: 6.690426349639893 = 0.5475448369979858 + 1.0 * 6.142881393432617
Epoch 410, val loss: 0.8745610117912292
Epoch 420, training loss: 6.6598381996154785 = 0.5212896466255188 + 1.0 * 6.138548374176025
Epoch 420, val loss: 0.8602191209793091
Epoch 430, training loss: 6.631176948547363 = 0.495398610830307 + 1.0 * 6.135778427124023
Epoch 430, val loss: 0.8469773530960083
Epoch 440, training loss: 6.605705738067627 = 0.4698179364204407 + 1.0 * 6.135887622833252
Epoch 440, val loss: 0.8346912264823914
Epoch 450, training loss: 6.577286243438721 = 0.44463077187538147 + 1.0 * 6.132655620574951
Epoch 450, val loss: 0.8237108588218689
Epoch 460, training loss: 6.553572654724121 = 0.41990530490875244 + 1.0 * 6.133667469024658
Epoch 460, val loss: 0.8139486312866211
Epoch 470, training loss: 6.5232110023498535 = 0.3957199454307556 + 1.0 * 6.127490997314453
Epoch 470, val loss: 0.8057430386543274
Epoch 480, training loss: 6.503764629364014 = 0.3719930946826935 + 1.0 * 6.131771564483643
Epoch 480, val loss: 0.7988940477371216
Epoch 490, training loss: 6.4754252433776855 = 0.349082350730896 + 1.0 * 6.1263427734375
Epoch 490, val loss: 0.7934411764144897
Epoch 500, training loss: 6.450746059417725 = 0.326994389295578 + 1.0 * 6.123751640319824
Epoch 500, val loss: 0.7895358800888062
Epoch 510, training loss: 6.425693035125732 = 0.30579128861427307 + 1.0 * 6.119901657104492
Epoch 510, val loss: 0.7868455052375793
Epoch 520, training loss: 6.40648078918457 = 0.28554654121398926 + 1.0 * 6.120934009552002
Epoch 520, val loss: 0.785518229007721
Epoch 530, training loss: 6.382869720458984 = 0.2663973271846771 + 1.0 * 6.116472244262695
Epoch 530, val loss: 0.7854939699172974
Epoch 540, training loss: 6.366255760192871 = 0.24841713905334473 + 1.0 * 6.1178388595581055
Epoch 540, val loss: 0.7864794135093689
Epoch 550, training loss: 6.344529151916504 = 0.23172233998775482 + 1.0 * 6.112806797027588
Epoch 550, val loss: 0.7887542247772217
Epoch 560, training loss: 6.326415538787842 = 0.2161535620689392 + 1.0 * 6.110261917114258
Epoch 560, val loss: 0.7919535636901855
Epoch 570, training loss: 6.329089641571045 = 0.20171190798282623 + 1.0 * 6.127377510070801
Epoch 570, val loss: 0.7959587574005127
Epoch 580, training loss: 6.297311305999756 = 0.18844322860240936 + 1.0 * 6.10886812210083
Epoch 580, val loss: 0.8007161617279053
Epoch 590, training loss: 6.281915664672852 = 0.17624318599700928 + 1.0 * 6.105672359466553
Epoch 590, val loss: 0.806358814239502
Epoch 600, training loss: 6.270208358764648 = 0.16498485207557678 + 1.0 * 6.105223655700684
Epoch 600, val loss: 0.8124093413352966
Epoch 610, training loss: 6.25846529006958 = 0.15465404093265533 + 1.0 * 6.103811264038086
Epoch 610, val loss: 0.8189447522163391
Epoch 620, training loss: 6.245519638061523 = 0.1452125906944275 + 1.0 * 6.100306987762451
Epoch 620, val loss: 0.8260275721549988
Epoch 630, training loss: 6.236362934112549 = 0.1365240514278412 + 1.0 * 6.099838733673096
Epoch 630, val loss: 0.8334311246871948
Epoch 640, training loss: 6.2387919425964355 = 0.12850454449653625 + 1.0 * 6.110287189483643
Epoch 640, val loss: 0.8410130143165588
Epoch 650, training loss: 6.217874526977539 = 0.12115689367055893 + 1.0 * 6.096717834472656
Epoch 650, val loss: 0.8488385677337646
Epoch 660, training loss: 6.210532188415527 = 0.11439557373523712 + 1.0 * 6.096136569976807
Epoch 660, val loss: 0.8570401668548584
Epoch 670, training loss: 6.207190990447998 = 0.10813067108392715 + 1.0 * 6.099060535430908
Epoch 670, val loss: 0.8653202056884766
Epoch 680, training loss: 6.1997456550598145 = 0.10233091562986374 + 1.0 * 6.097414970397949
Epoch 680, val loss: 0.8735784292221069
Epoch 690, training loss: 6.189483642578125 = 0.09699519723653793 + 1.0 * 6.0924882888793945
Epoch 690, val loss: 0.8822444081306458
Epoch 700, training loss: 6.1830010414123535 = 0.09202169626951218 + 1.0 * 6.09097957611084
Epoch 700, val loss: 0.890877366065979
Epoch 710, training loss: 6.1835432052612305 = 0.08739268779754639 + 1.0 * 6.0961503982543945
Epoch 710, val loss: 0.8994461894035339
Epoch 720, training loss: 6.174957275390625 = 0.08309914916753769 + 1.0 * 6.09185791015625
Epoch 720, val loss: 0.9081772565841675
Epoch 730, training loss: 6.165948390960693 = 0.07909361273050308 + 1.0 * 6.086854934692383
Epoch 730, val loss: 0.9169148206710815
Epoch 740, training loss: 6.165947437286377 = 0.07534552365541458 + 1.0 * 6.090601921081543
Epoch 740, val loss: 0.9255759119987488
Epoch 750, training loss: 6.158705711364746 = 0.07184448838233948 + 1.0 * 6.0868611335754395
Epoch 750, val loss: 0.9341998100280762
Epoch 760, training loss: 6.1556291580200195 = 0.06857351958751678 + 1.0 * 6.087055683135986
Epoch 760, val loss: 0.9429552555084229
Epoch 770, training loss: 6.148146629333496 = 0.06550467759370804 + 1.0 * 6.082642078399658
Epoch 770, val loss: 0.9512676000595093
Epoch 780, training loss: 6.143746852874756 = 0.06264200806617737 + 1.0 * 6.081104755401611
Epoch 780, val loss: 0.9599264860153198
Epoch 790, training loss: 6.139705181121826 = 0.059937410056591034 + 1.0 * 6.07976770401001
Epoch 790, val loss: 0.9684293270111084
Epoch 800, training loss: 6.136048793792725 = 0.05738689377903938 + 1.0 * 6.078661918640137
Epoch 800, val loss: 0.9769455790519714
Epoch 810, training loss: 6.13723611831665 = 0.054980091750621796 + 1.0 * 6.082255840301514
Epoch 810, val loss: 0.9851743578910828
Epoch 820, training loss: 6.135025501251221 = 0.05272653326392174 + 1.0 * 6.082298755645752
Epoch 820, val loss: 0.9934620261192322
Epoch 830, training loss: 6.126669406890869 = 0.05060061067342758 + 1.0 * 6.076068878173828
Epoch 830, val loss: 1.001755952835083
Epoch 840, training loss: 6.125803470611572 = 0.04858817160129547 + 1.0 * 6.077215194702148
Epoch 840, val loss: 1.009912371635437
Epoch 850, training loss: 6.1226115226745605 = 0.0466805063188076 + 1.0 * 6.075931072235107
Epoch 850, val loss: 1.0181162357330322
Epoch 860, training loss: 6.119629859924316 = 0.044879067689180374 + 1.0 * 6.074750900268555
Epoch 860, val loss: 1.0261099338531494
Epoch 870, training loss: 6.115452289581299 = 0.04317054897546768 + 1.0 * 6.072281837463379
Epoch 870, val loss: 1.034140944480896
Epoch 880, training loss: 6.112076282501221 = 0.04154941812157631 + 1.0 * 6.070527076721191
Epoch 880, val loss: 1.0421432256698608
Epoch 890, training loss: 6.1143693923950195 = 0.04000371694564819 + 1.0 * 6.074365615844727
Epoch 890, val loss: 1.049988031387329
Epoch 900, training loss: 6.11332368850708 = 0.03854481875896454 + 1.0 * 6.074779033660889
Epoch 900, val loss: 1.0577263832092285
Epoch 910, training loss: 6.107427597045898 = 0.03716372326016426 + 1.0 * 6.070263862609863
Epoch 910, val loss: 1.0654327869415283
Epoch 920, training loss: 6.103401184082031 = 0.03584897145628929 + 1.0 * 6.067552089691162
Epoch 920, val loss: 1.0731801986694336
Epoch 930, training loss: 6.104769706726074 = 0.0345950685441494 + 1.0 * 6.070174694061279
Epoch 930, val loss: 1.080811619758606
Epoch 940, training loss: 6.099660873413086 = 0.03340175375342369 + 1.0 * 6.066258907318115
Epoch 940, val loss: 1.0883355140686035
Epoch 950, training loss: 6.098843574523926 = 0.03226698189973831 + 1.0 * 6.0665764808654785
Epoch 950, val loss: 1.0957746505737305
Epoch 960, training loss: 6.100793361663818 = 0.031188391149044037 + 1.0 * 6.069604873657227
Epoch 960, val loss: 1.1033052206039429
Epoch 970, training loss: 6.0943989753723145 = 0.030156083405017853 + 1.0 * 6.064242839813232
Epoch 970, val loss: 1.110564112663269
Epoch 980, training loss: 6.096460342407227 = 0.029174059629440308 + 1.0 * 6.067286491394043
Epoch 980, val loss: 1.1178845167160034
Epoch 990, training loss: 6.09374475479126 = 0.028239335864782333 + 1.0 * 6.065505504608154
Epoch 990, val loss: 1.125085711479187
Epoch 1000, training loss: 6.089153289794922 = 0.02734370343387127 + 1.0 * 6.061809539794922
Epoch 1000, val loss: 1.132237195968628
Epoch 1010, training loss: 6.097603797912598 = 0.026487506926059723 + 1.0 * 6.0711164474487305
Epoch 1010, val loss: 1.13918137550354
Epoch 1020, training loss: 6.089056968688965 = 0.02567792311310768 + 1.0 * 6.063378810882568
Epoch 1020, val loss: 1.1461268663406372
Epoch 1030, training loss: 6.0848493576049805 = 0.0249005313962698 + 1.0 * 6.059948921203613
Epoch 1030, val loss: 1.1530441045761108
Epoch 1040, training loss: 6.085097789764404 = 0.024157870560884476 + 1.0 * 6.060939788818359
Epoch 1040, val loss: 1.1599040031433105
Epoch 1050, training loss: 6.082670211791992 = 0.02344527468085289 + 1.0 * 6.059225082397461
Epoch 1050, val loss: 1.1665157079696655
Epoch 1060, training loss: 6.086258411407471 = 0.022765381261706352 + 1.0 * 6.063493251800537
Epoch 1060, val loss: 1.1731507778167725
Epoch 1070, training loss: 6.078549385070801 = 0.02211451157927513 + 1.0 * 6.0564351081848145
Epoch 1070, val loss: 1.17966890335083
Epoch 1080, training loss: 6.079005241394043 = 0.02148960530757904 + 1.0 * 6.057515621185303
Epoch 1080, val loss: 1.1862601041793823
Epoch 1090, training loss: 6.0850300788879395 = 0.020891645923256874 + 1.0 * 6.064138412475586
Epoch 1090, val loss: 1.192781925201416
Epoch 1100, training loss: 6.080097198486328 = 0.02031894028186798 + 1.0 * 6.059778213500977
Epoch 1100, val loss: 1.198927640914917
Epoch 1110, training loss: 6.073641777038574 = 0.01976933889091015 + 1.0 * 6.053872585296631
Epoch 1110, val loss: 1.205217957496643
Epoch 1120, training loss: 6.073212623596191 = 0.01924268901348114 + 1.0 * 6.053969860076904
Epoch 1120, val loss: 1.2116031646728516
Epoch 1130, training loss: 6.075839042663574 = 0.01873343624174595 + 1.0 * 6.057105541229248
Epoch 1130, val loss: 1.217706561088562
Epoch 1140, training loss: 6.074967861175537 = 0.018244944512844086 + 1.0 * 6.056723117828369
Epoch 1140, val loss: 1.2237472534179688
Epoch 1150, training loss: 6.069657802581787 = 0.01777799241244793 + 1.0 * 6.0518798828125
Epoch 1150, val loss: 1.229743480682373
Epoch 1160, training loss: 6.0724687576293945 = 0.017329029738903046 + 1.0 * 6.055139541625977
Epoch 1160, val loss: 1.2357330322265625
Epoch 1170, training loss: 6.068004608154297 = 0.016896294429898262 + 1.0 * 6.051108360290527
Epoch 1170, val loss: 1.2416220903396606
Epoch 1180, training loss: 6.067179203033447 = 0.01647992804646492 + 1.0 * 6.050699234008789
Epoch 1180, val loss: 1.247552752494812
Epoch 1190, training loss: 6.066100120544434 = 0.016077762469649315 + 1.0 * 6.050022125244141
Epoch 1190, val loss: 1.253382682800293
Epoch 1200, training loss: 6.073483943939209 = 0.015690259635448456 + 1.0 * 6.057793617248535
Epoch 1200, val loss: 1.2589571475982666
Epoch 1210, training loss: 6.0665082931518555 = 0.015320802107453346 + 1.0 * 6.051187515258789
Epoch 1210, val loss: 1.2646187543869019
Epoch 1220, training loss: 6.066197395324707 = 0.014963549561798573 + 1.0 * 6.051233768463135
Epoch 1220, val loss: 1.2701693773269653
Epoch 1230, training loss: 6.06294059753418 = 0.01461835578083992 + 1.0 * 6.0483222007751465
Epoch 1230, val loss: 1.2756731510162354
Epoch 1240, training loss: 6.06173849105835 = 0.014284930191934109 + 1.0 * 6.0474534034729
Epoch 1240, val loss: 1.281211018562317
Epoch 1250, training loss: 6.067946910858154 = 0.013960887677967548 + 1.0 * 6.053986072540283
Epoch 1250, val loss: 1.2865322828292847
Epoch 1260, training loss: 6.061662197113037 = 0.013652055524289608 + 1.0 * 6.048010349273682
Epoch 1260, val loss: 1.2919247150421143
Epoch 1270, training loss: 6.063567161560059 = 0.013351892121136189 + 1.0 * 6.050215244293213
Epoch 1270, val loss: 1.2971652746200562
Epoch 1280, training loss: 6.060274600982666 = 0.013063822872936726 + 1.0 * 6.047210693359375
Epoch 1280, val loss: 1.3023990392684937
Epoch 1290, training loss: 6.059046268463135 = 0.012783994898200035 + 1.0 * 6.046262264251709
Epoch 1290, val loss: 1.3075577020645142
Epoch 1300, training loss: 6.057294845581055 = 0.012513220310211182 + 1.0 * 6.044781684875488
Epoch 1300, val loss: 1.312775731086731
Epoch 1310, training loss: 6.060290813446045 = 0.012250407598912716 + 1.0 * 6.048040390014648
Epoch 1310, val loss: 1.3178530931472778
Epoch 1320, training loss: 6.060848712921143 = 0.011997969821095467 + 1.0 * 6.0488505363464355
Epoch 1320, val loss: 1.3228555917739868
Epoch 1330, training loss: 6.055799961090088 = 0.011753477156162262 + 1.0 * 6.044046401977539
Epoch 1330, val loss: 1.3276875019073486
Epoch 1340, training loss: 6.053278923034668 = 0.011517609469592571 + 1.0 * 6.04176139831543
Epoch 1340, val loss: 1.3327316045761108
Epoch 1350, training loss: 6.052407741546631 = 0.01128784567117691 + 1.0 * 6.0411200523376465
Epoch 1350, val loss: 1.337708592414856
Epoch 1360, training loss: 6.060018539428711 = 0.011065041646361351 + 1.0 * 6.048953533172607
Epoch 1360, val loss: 1.342472791671753
Epoch 1370, training loss: 6.0531907081604 = 0.010848668403923512 + 1.0 * 6.042342185974121
Epoch 1370, val loss: 1.3471603393554688
Epoch 1380, training loss: 6.056180477142334 = 0.010640204884111881 + 1.0 * 6.0455403327941895
Epoch 1380, val loss: 1.3520315885543823
Epoch 1390, training loss: 6.050912857055664 = 0.010436906479299068 + 1.0 * 6.040475845336914
Epoch 1390, val loss: 1.3562735319137573
Epoch 1400, training loss: 6.050239086151123 = 0.01024388987571001 + 1.0 * 6.039995193481445
Epoch 1400, val loss: 1.3610814809799194
Epoch 1410, training loss: 6.048977375030518 = 0.010053860023617744 + 1.0 * 6.038923740386963
Epoch 1410, val loss: 1.3656978607177734
Epoch 1420, training loss: 6.0504937171936035 = 0.009869011119008064 + 1.0 * 6.040624618530273
Epoch 1420, val loss: 1.3702722787857056
Epoch 1430, training loss: 6.047537326812744 = 0.00968841277062893 + 1.0 * 6.037848949432373
Epoch 1430, val loss: 1.3745543956756592
Epoch 1440, training loss: 6.046624183654785 = 0.009515739046037197 + 1.0 * 6.037108421325684
Epoch 1440, val loss: 1.3790005445480347
Epoch 1450, training loss: 6.046572208404541 = 0.009346499107778072 + 1.0 * 6.037225723266602
Epoch 1450, val loss: 1.383516550064087
Epoch 1460, training loss: 6.057664394378662 = 0.009181461296975613 + 1.0 * 6.048482894897461
Epoch 1460, val loss: 1.387744426727295
Epoch 1470, training loss: 6.047529220581055 = 0.009023059159517288 + 1.0 * 6.038506031036377
Epoch 1470, val loss: 1.3920950889587402
Epoch 1480, training loss: 6.044134140014648 = 0.008867529220879078 + 1.0 * 6.035266399383545
Epoch 1480, val loss: 1.3964303731918335
Epoch 1490, training loss: 6.044000148773193 = 0.008716082200407982 + 1.0 * 6.035284042358398
Epoch 1490, val loss: 1.4007519483566284
Epoch 1500, training loss: 6.052463531494141 = 0.008569066412746906 + 1.0 * 6.043894290924072
Epoch 1500, val loss: 1.404862880706787
Epoch 1510, training loss: 6.048592567443848 = 0.008426054380834103 + 1.0 * 6.04016637802124
Epoch 1510, val loss: 1.4088155031204224
Epoch 1520, training loss: 6.042210102081299 = 0.008288496173918247 + 1.0 * 6.033921718597412
Epoch 1520, val loss: 1.4129769802093506
Epoch 1530, training loss: 6.0417094230651855 = 0.00815389771014452 + 1.0 * 6.033555507659912
Epoch 1530, val loss: 1.4171539545059204
Epoch 1540, training loss: 6.047987937927246 = 0.00802193395793438 + 1.0 * 6.039966106414795
Epoch 1540, val loss: 1.4211283922195435
Epoch 1550, training loss: 6.041014194488525 = 0.00789294857531786 + 1.0 * 6.033121109008789
Epoch 1550, val loss: 1.4250497817993164
Epoch 1560, training loss: 6.040881156921387 = 0.007767944596707821 + 1.0 * 6.0331130027771
Epoch 1560, val loss: 1.429076075553894
Epoch 1570, training loss: 6.043188571929932 = 0.007645502220839262 + 1.0 * 6.035542964935303
Epoch 1570, val loss: 1.4329721927642822
Epoch 1580, training loss: 6.039980411529541 = 0.007526292931288481 + 1.0 * 6.032454013824463
Epoch 1580, val loss: 1.4368823766708374
Epoch 1590, training loss: 6.042596817016602 = 0.007410854566842318 + 1.0 * 6.035185813903809
Epoch 1590, val loss: 1.440840244293213
Epoch 1600, training loss: 6.045022010803223 = 0.0072988467290997505 + 1.0 * 6.037723064422607
Epoch 1600, val loss: 1.444469690322876
Epoch 1610, training loss: 6.0406317710876465 = 0.007188846822828054 + 1.0 * 6.033442974090576
Epoch 1610, val loss: 1.4482272863388062
Epoch 1620, training loss: 6.04096794128418 = 0.007082535419613123 + 1.0 * 6.033885478973389
Epoch 1620, val loss: 1.4520008563995361
Epoch 1630, training loss: 6.039024829864502 = 0.006977831944823265 + 1.0 * 6.032046794891357
Epoch 1630, val loss: 1.4557137489318848
Epoch 1640, training loss: 6.037939071655273 = 0.006875528953969479 + 1.0 * 6.031063556671143
Epoch 1640, val loss: 1.4593971967697144
Epoch 1650, training loss: 6.0409955978393555 = 0.006775642745196819 + 1.0 * 6.034219741821289
Epoch 1650, val loss: 1.462979793548584
Epoch 1660, training loss: 6.039506435394287 = 0.0066779665648937225 + 1.0 * 6.032828330993652
Epoch 1660, val loss: 1.4664932489395142
Epoch 1670, training loss: 6.038047790527344 = 0.006583421025425196 + 1.0 * 6.031464576721191
Epoch 1670, val loss: 1.4701101779937744
Epoch 1680, training loss: 6.035124778747559 = 0.006491364911198616 + 1.0 * 6.0286335945129395
Epoch 1680, val loss: 1.4737473726272583
Epoch 1690, training loss: 6.036312103271484 = 0.0064006089232862 + 1.0 * 6.029911518096924
Epoch 1690, val loss: 1.4773330688476562
Epoch 1700, training loss: 6.036598205566406 = 0.006311440374702215 + 1.0 * 6.03028678894043
Epoch 1700, val loss: 1.4807242155075073
Epoch 1710, training loss: 6.036531448364258 = 0.006225074175745249 + 1.0 * 6.030306339263916
Epoch 1710, val loss: 1.4841978549957275
Epoch 1720, training loss: 6.038089275360107 = 0.0061406283639371395 + 1.0 * 6.031948566436768
Epoch 1720, val loss: 1.4876290559768677
Epoch 1730, training loss: 6.039077281951904 = 0.0060578384436666965 + 1.0 * 6.033019542694092
Epoch 1730, val loss: 1.49090576171875
Epoch 1740, training loss: 6.033156871795654 = 0.005978022702038288 + 1.0 * 6.027178764343262
Epoch 1740, val loss: 1.4943255186080933
Epoch 1750, training loss: 6.032364845275879 = 0.005899498239159584 + 1.0 * 6.02646541595459
Epoch 1750, val loss: 1.4977288246154785
Epoch 1760, training loss: 6.033559799194336 = 0.005822035949677229 + 1.0 * 6.027737617492676
Epoch 1760, val loss: 1.5010626316070557
Epoch 1770, training loss: 6.034684181213379 = 0.005746194627135992 + 1.0 * 6.028937816619873
Epoch 1770, val loss: 1.5042941570281982
Epoch 1780, training loss: 6.0339531898498535 = 0.005672266241163015 + 1.0 * 6.028280735015869
Epoch 1780, val loss: 1.5076431035995483
Epoch 1790, training loss: 6.034775733947754 = 0.005599912256002426 + 1.0 * 6.029175758361816
Epoch 1790, val loss: 1.5107955932617188
Epoch 1800, training loss: 6.03208589553833 = 0.005529115442186594 + 1.0 * 6.026556968688965
Epoch 1800, val loss: 1.5139955282211304
Epoch 1810, training loss: 6.037519931793213 = 0.005459845066070557 + 1.0 * 6.032060146331787
Epoch 1810, val loss: 1.5170832872390747
Epoch 1820, training loss: 6.031691551208496 = 0.005393338855355978 + 1.0 * 6.0262980461120605
Epoch 1820, val loss: 1.5201939344406128
Epoch 1830, training loss: 6.02928352355957 = 0.005327105056494474 + 1.0 * 6.023956298828125
Epoch 1830, val loss: 1.5234671831130981
Epoch 1840, training loss: 6.028672695159912 = 0.005261923186480999 + 1.0 * 6.023410797119141
Epoch 1840, val loss: 1.5266062021255493
Epoch 1850, training loss: 6.037200927734375 = 0.005197556689381599 + 1.0 * 6.032003402709961
Epoch 1850, val loss: 1.5296006202697754
Epoch 1860, training loss: 6.032855987548828 = 0.005135508719831705 + 1.0 * 6.0277204513549805
Epoch 1860, val loss: 1.5325350761413574
Epoch 1870, training loss: 6.031890392303467 = 0.0050749159418046474 + 1.0 * 6.026815414428711
Epoch 1870, val loss: 1.5354928970336914
Epoch 1880, training loss: 6.033756256103516 = 0.005016264505684376 + 1.0 * 6.028739929199219
Epoch 1880, val loss: 1.5387017726898193
Epoch 1890, training loss: 6.027203559875488 = 0.004957546014338732 + 1.0 * 6.02224588394165
Epoch 1890, val loss: 1.541448712348938
Epoch 1900, training loss: 6.027690887451172 = 0.004900018218904734 + 1.0 * 6.022790908813477
Epoch 1900, val loss: 1.5444560050964355
Epoch 1910, training loss: 6.027092456817627 = 0.004843608476221561 + 1.0 * 6.0222487449646
Epoch 1910, val loss: 1.5474907159805298
Epoch 1920, training loss: 6.034458160400391 = 0.004787973128259182 + 1.0 * 6.029670238494873
Epoch 1920, val loss: 1.5503507852554321
Epoch 1930, training loss: 6.029132843017578 = 0.00473407469689846 + 1.0 * 6.0243988037109375
Epoch 1930, val loss: 1.5531675815582275
Epoch 1940, training loss: 6.03064489364624 = 0.004681146237999201 + 1.0 * 6.02596378326416
Epoch 1940, val loss: 1.5559335947036743
Epoch 1950, training loss: 6.028934478759766 = 0.0046294513158500195 + 1.0 * 6.0243048667907715
Epoch 1950, val loss: 1.5588154792785645
Epoch 1960, training loss: 6.026996612548828 = 0.004578567575663328 + 1.0 * 6.022418022155762
Epoch 1960, val loss: 1.5615968704223633
Epoch 1970, training loss: 6.0263895988464355 = 0.004528521094471216 + 1.0 * 6.0218610763549805
Epoch 1970, val loss: 1.564355731010437
Epoch 1980, training loss: 6.027811527252197 = 0.004479455295950174 + 1.0 * 6.023332118988037
Epoch 1980, val loss: 1.567068338394165
Epoch 1990, training loss: 6.024040699005127 = 0.004431579727679491 + 1.0 * 6.019608974456787
Epoch 1990, val loss: 1.569979190826416
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6494
Flip ASR: 0.5822/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.322402000427246 = 1.948478102684021 + 1.0 * 8.373924255371094
Epoch 0, val loss: 1.937591314315796
Epoch 10, training loss: 10.310944557189941 = 1.9373005628585815 + 1.0 * 8.37364387512207
Epoch 10, val loss: 1.9261610507965088
Epoch 20, training loss: 10.295026779174805 = 1.9234915971755981 + 1.0 * 8.371535301208496
Epoch 20, val loss: 1.9119235277175903
Epoch 30, training loss: 10.261323928833008 = 1.9042762517929077 + 1.0 * 8.357048034667969
Epoch 30, val loss: 1.8918248414993286
Epoch 40, training loss: 10.160600662231445 = 1.878507375717163 + 1.0 * 8.282093048095703
Epoch 40, val loss: 1.8655908107757568
Epoch 50, training loss: 9.766009330749512 = 1.8510738611221313 + 1.0 * 7.91493558883667
Epoch 50, val loss: 1.8385378122329712
Epoch 60, training loss: 9.318964958190918 = 1.8245270252227783 + 1.0 * 7.4944376945495605
Epoch 60, val loss: 1.8135780096054077
Epoch 70, training loss: 8.97887134552002 = 1.8034255504608154 + 1.0 * 7.175445556640625
Epoch 70, val loss: 1.7943629026412964
Epoch 80, training loss: 8.755085945129395 = 1.7811850309371948 + 1.0 * 6.97390079498291
Epoch 80, val loss: 1.7739285230636597
Epoch 90, training loss: 8.560688018798828 = 1.7595949172973633 + 1.0 * 6.801092624664307
Epoch 90, val loss: 1.754744291305542
Epoch 100, training loss: 8.44186019897461 = 1.7385419607162476 + 1.0 * 6.703318119049072
Epoch 100, val loss: 1.7363277673721313
Epoch 110, training loss: 8.340291023254395 = 1.7137987613677979 + 1.0 * 6.626492023468018
Epoch 110, val loss: 1.7147746086120605
Epoch 120, training loss: 8.236515045166016 = 1.686073660850525 + 1.0 * 6.550441741943359
Epoch 120, val loss: 1.6910929679870605
Epoch 130, training loss: 8.153953552246094 = 1.6567012071609497 + 1.0 * 6.497251987457275
Epoch 130, val loss: 1.666157841682434
Epoch 140, training loss: 8.078198432922363 = 1.6222063302993774 + 1.0 * 6.455992221832275
Epoch 140, val loss: 1.637181282043457
Epoch 150, training loss: 8.006970405578613 = 1.582040548324585 + 1.0 * 6.424929618835449
Epoch 150, val loss: 1.6039079427719116
Epoch 160, training loss: 7.931337833404541 = 1.5374029874801636 + 1.0 * 6.393934726715088
Epoch 160, val loss: 1.5673224925994873
Epoch 170, training loss: 7.856118679046631 = 1.4885435104370117 + 1.0 * 6.367575168609619
Epoch 170, val loss: 1.5278223752975464
Epoch 180, training loss: 7.783044815063477 = 1.4362432956695557 + 1.0 * 6.3468017578125
Epoch 180, val loss: 1.48615562915802
Epoch 190, training loss: 7.711408615112305 = 1.3830288648605347 + 1.0 * 6.3283796310424805
Epoch 190, val loss: 1.4444373846054077
Epoch 200, training loss: 7.641347885131836 = 1.3302371501922607 + 1.0 * 6.311110973358154
Epoch 200, val loss: 1.4039703607559204
Epoch 210, training loss: 7.576418399810791 = 1.2785882949829102 + 1.0 * 6.297830104827881
Epoch 210, val loss: 1.3650498390197754
Epoch 220, training loss: 7.513993740081787 = 1.2296743392944336 + 1.0 * 6.2843194007873535
Epoch 220, val loss: 1.3288383483886719
Epoch 230, training loss: 7.45318603515625 = 1.182396650314331 + 1.0 * 6.27078914642334
Epoch 230, val loss: 1.2944257259368896
Epoch 240, training loss: 7.395089149475098 = 1.1357367038726807 + 1.0 * 6.259352684020996
Epoch 240, val loss: 1.260466456413269
Epoch 250, training loss: 7.338533878326416 = 1.0887341499328613 + 1.0 * 6.249799728393555
Epoch 250, val loss: 1.2264949083328247
Epoch 260, training loss: 7.283689498901367 = 1.0412373542785645 + 1.0 * 6.242452144622803
Epoch 260, val loss: 1.1923882961273193
Epoch 270, training loss: 7.229815483093262 = 0.9937776327133179 + 1.0 * 6.236037731170654
Epoch 270, val loss: 1.1585748195648193
Epoch 280, training loss: 7.17668342590332 = 0.9458227157592773 + 1.0 * 6.230860710144043
Epoch 280, val loss: 1.1247034072875977
Epoch 290, training loss: 7.122119903564453 = 0.8984589576721191 + 1.0 * 6.223660945892334
Epoch 290, val loss: 1.0911328792572021
Epoch 300, training loss: 7.068354606628418 = 0.8519207239151001 + 1.0 * 6.216434001922607
Epoch 300, val loss: 1.0584765672683716
Epoch 310, training loss: 7.017249584197998 = 0.8067094683647156 + 1.0 * 6.210540294647217
Epoch 310, val loss: 1.0268940925598145
Epoch 320, training loss: 6.968991279602051 = 0.7634264230728149 + 1.0 * 6.205564975738525
Epoch 320, val loss: 0.9969139099121094
Epoch 330, training loss: 6.930331230163574 = 0.7228734493255615 + 1.0 * 6.207458019256592
Epoch 330, val loss: 0.9693263173103333
Epoch 340, training loss: 6.882201671600342 = 0.685717761516571 + 1.0 * 6.196484088897705
Epoch 340, val loss: 0.9446498155593872
Epoch 350, training loss: 6.842058181762695 = 0.6514217853546143 + 1.0 * 6.190636157989502
Epoch 350, val loss: 0.9224516749382019
Epoch 360, training loss: 6.80819845199585 = 0.6197123527526855 + 1.0 * 6.188486099243164
Epoch 360, val loss: 0.9024311304092407
Epoch 370, training loss: 6.779559135437012 = 0.5905263423919678 + 1.0 * 6.189032554626465
Epoch 370, val loss: 0.8845857977867126
Epoch 380, training loss: 6.743283271789551 = 0.5638275742530823 + 1.0 * 6.179455757141113
Epoch 380, val loss: 0.8688872456550598
Epoch 390, training loss: 6.713322162628174 = 0.5389279723167419 + 1.0 * 6.174394130706787
Epoch 390, val loss: 0.8548714518547058
Epoch 400, training loss: 6.685779571533203 = 0.515411376953125 + 1.0 * 6.170368194580078
Epoch 400, val loss: 0.8421843647956848
Epoch 410, training loss: 6.676530838012695 = 0.4930158257484436 + 1.0 * 6.1835150718688965
Epoch 410, val loss: 0.8306565880775452
Epoch 420, training loss: 6.641130447387695 = 0.4718612730503082 + 1.0 * 6.16926908493042
Epoch 420, val loss: 0.8202193379402161
Epoch 430, training loss: 6.612846851348877 = 0.4514865279197693 + 1.0 * 6.161360263824463
Epoch 430, val loss: 0.8109267950057983
Epoch 440, training loss: 6.59458589553833 = 0.4315601587295532 + 1.0 * 6.163025856018066
Epoch 440, val loss: 0.8024243712425232
Epoch 450, training loss: 6.570539951324463 = 0.41201460361480713 + 1.0 * 6.158525466918945
Epoch 450, val loss: 0.7944864630699158
Epoch 460, training loss: 6.544987201690674 = 0.3925715386867523 + 1.0 * 6.152415752410889
Epoch 460, val loss: 0.7868700623512268
Epoch 470, training loss: 6.528228282928467 = 0.37313470244407654 + 1.0 * 6.155093669891357
Epoch 470, val loss: 0.7794835567474365
Epoch 480, training loss: 6.500041484832764 = 0.35369154810905457 + 1.0 * 6.146349906921387
Epoch 480, val loss: 0.7723221778869629
Epoch 490, training loss: 6.487779140472412 = 0.3341869115829468 + 1.0 * 6.153592109680176
Epoch 490, val loss: 0.7653524279594421
Epoch 500, training loss: 6.45884895324707 = 0.3147503733634949 + 1.0 * 6.14409875869751
Epoch 500, val loss: 0.7584956884384155
Epoch 510, training loss: 6.435546875 = 0.29532167315483093 + 1.0 * 6.140225410461426
Epoch 510, val loss: 0.7517719864845276
Epoch 520, training loss: 6.419674396514893 = 0.27600911259651184 + 1.0 * 6.143665313720703
Epoch 520, val loss: 0.7452142834663391
Epoch 530, training loss: 6.395073890686035 = 0.25724661350250244 + 1.0 * 6.137827396392822
Epoch 530, val loss: 0.7388135194778442
Epoch 540, training loss: 6.372952461242676 = 0.23918366432189941 + 1.0 * 6.133768558502197
Epoch 540, val loss: 0.7329327464103699
Epoch 550, training loss: 6.353351593017578 = 0.22177059948444366 + 1.0 * 6.131580829620361
Epoch 550, val loss: 0.727594256401062
Epoch 560, training loss: 6.333916664123535 = 0.20513896644115448 + 1.0 * 6.128777503967285
Epoch 560, val loss: 0.7228373289108276
Epoch 570, training loss: 6.316103935241699 = 0.18945516645908356 + 1.0 * 6.126648902893066
Epoch 570, val loss: 0.7187969088554382
Epoch 580, training loss: 6.308714389801025 = 0.1749359667301178 + 1.0 * 6.1337785720825195
Epoch 580, val loss: 0.715549886226654
Epoch 590, training loss: 6.288178443908691 = 0.16173328459262848 + 1.0 * 6.126445293426514
Epoch 590, val loss: 0.713371217250824
Epoch 600, training loss: 6.272640228271484 = 0.14964933693408966 + 1.0 * 6.12299108505249
Epoch 600, val loss: 0.7122200727462769
Epoch 610, training loss: 6.262059688568115 = 0.1386336088180542 + 1.0 * 6.1234259605407715
Epoch 610, val loss: 0.7119945287704468
Epoch 620, training loss: 6.247056484222412 = 0.1286313682794571 + 1.0 * 6.118424892425537
Epoch 620, val loss: 0.7126061916351318
Epoch 630, training loss: 6.238944053649902 = 0.11956538259983063 + 1.0 * 6.119378566741943
Epoch 630, val loss: 0.7139928936958313
Epoch 640, training loss: 6.229800224304199 = 0.11137758940458298 + 1.0 * 6.118422508239746
Epoch 640, val loss: 0.7160331010818481
Epoch 650, training loss: 6.21793270111084 = 0.10396849364042282 + 1.0 * 6.113964080810547
Epoch 650, val loss: 0.7186106443405151
Epoch 660, training loss: 6.210475444793701 = 0.09722357243299484 + 1.0 * 6.113251686096191
Epoch 660, val loss: 0.7217791080474854
Epoch 670, training loss: 6.211370468139648 = 0.09110093116760254 + 1.0 * 6.120269775390625
Epoch 670, val loss: 0.7254458665847778
Epoch 680, training loss: 6.1991496086120605 = 0.08555501699447632 + 1.0 * 6.1135945320129395
Epoch 680, val loss: 0.7294226288795471
Epoch 690, training loss: 6.189411163330078 = 0.08053538203239441 + 1.0 * 6.108875751495361
Epoch 690, val loss: 0.7337493896484375
Epoch 700, training loss: 6.1814398765563965 = 0.07593097537755966 + 1.0 * 6.105508804321289
Epoch 700, val loss: 0.7384014129638672
Epoch 710, training loss: 6.176002025604248 = 0.07169705629348755 + 1.0 * 6.104304790496826
Epoch 710, val loss: 0.7432835698127747
Epoch 720, training loss: 6.176224708557129 = 0.06780973076820374 + 1.0 * 6.108415126800537
Epoch 720, val loss: 0.7483460903167725
Epoch 730, training loss: 6.17145299911499 = 0.06425581872463226 + 1.0 * 6.107197284698486
Epoch 730, val loss: 0.7535044550895691
Epoch 740, training loss: 6.163525104522705 = 0.0609835721552372 + 1.0 * 6.102541446685791
Epoch 740, val loss: 0.7587535977363586
Epoch 750, training loss: 6.162496566772461 = 0.05795426666736603 + 1.0 * 6.104542255401611
Epoch 750, val loss: 0.7641221880912781
Epoch 760, training loss: 6.154033184051514 = 0.055154040455818176 + 1.0 * 6.098879337310791
Epoch 760, val loss: 0.7695215344429016
Epoch 770, training loss: 6.150806903839111 = 0.052548836916685104 + 1.0 * 6.098258018493652
Epoch 770, val loss: 0.7749918103218079
Epoch 780, training loss: 6.15010929107666 = 0.05012126639485359 + 1.0 * 6.099987983703613
Epoch 780, val loss: 0.780543327331543
Epoch 790, training loss: 6.150138854980469 = 0.04786882549524307 + 1.0 * 6.102270126342773
Epoch 790, val loss: 0.7860643267631531
Epoch 800, training loss: 6.1388092041015625 = 0.045776963233947754 + 1.0 * 6.093032360076904
Epoch 800, val loss: 0.7915439605712891
Epoch 810, training loss: 6.136930465698242 = 0.043815936893224716 + 1.0 * 6.093114376068115
Epoch 810, val loss: 0.7970755696296692
Epoch 820, training loss: 6.139759540557861 = 0.041975513100624084 + 1.0 * 6.097784042358398
Epoch 820, val loss: 0.8025910258293152
Epoch 830, training loss: 6.13205099105835 = 0.04025615379214287 + 1.0 * 6.091794967651367
Epoch 830, val loss: 0.8080107569694519
Epoch 840, training loss: 6.128479480743408 = 0.038633592426776886 + 1.0 * 6.089845657348633
Epoch 840, val loss: 0.8134435415267944
Epoch 850, training loss: 6.131365776062012 = 0.03710973635315895 + 1.0 * 6.0942559242248535
Epoch 850, val loss: 0.818885326385498
Epoch 860, training loss: 6.123995780944824 = 0.03567704185843468 + 1.0 * 6.088318824768066
Epoch 860, val loss: 0.8242431879043579
Epoch 870, training loss: 6.1349711418151855 = 0.03433269262313843 + 1.0 * 6.100638389587402
Epoch 870, val loss: 0.8295223712921143
Epoch 880, training loss: 6.117894172668457 = 0.03306831791996956 + 1.0 * 6.0848259925842285
Epoch 880, val loss: 0.8347638845443726
Epoch 890, training loss: 6.116252899169922 = 0.03187694773077965 + 1.0 * 6.084375858306885
Epoch 890, val loss: 0.8399066925048828
Epoch 900, training loss: 6.113040447235107 = 0.03074309602379799 + 1.0 * 6.082297325134277
Epoch 900, val loss: 0.8450472354888916
Epoch 910, training loss: 6.1200056076049805 = 0.02966686151921749 + 1.0 * 6.090338706970215
Epoch 910, val loss: 0.8501591086387634
Epoch 920, training loss: 6.112185478210449 = 0.028647294268012047 + 1.0 * 6.083538055419922
Epoch 920, val loss: 0.8551788330078125
Epoch 930, training loss: 6.108639717102051 = 0.027682486921548843 + 1.0 * 6.080957412719727
Epoch 930, val loss: 0.8601474761962891
Epoch 940, training loss: 6.107089042663574 = 0.02676270343363285 + 1.0 * 6.080326557159424
Epoch 940, val loss: 0.865079402923584
Epoch 950, training loss: 6.106880187988281 = 0.025887705385684967 + 1.0 * 6.080992698669434
Epoch 950, val loss: 0.8699513673782349
Epoch 960, training loss: 6.101872444152832 = 0.025059429928660393 + 1.0 * 6.076813220977783
Epoch 960, val loss: 0.8747338652610779
Epoch 970, training loss: 6.100584506988525 = 0.02427074871957302 + 1.0 * 6.0763139724731445
Epoch 970, val loss: 0.8794671297073364
Epoch 980, training loss: 6.108055591583252 = 0.02351812832057476 + 1.0 * 6.084537506103516
Epoch 980, val loss: 0.8842312693595886
Epoch 990, training loss: 6.096843242645264 = 0.02280365116894245 + 1.0 * 6.074039459228516
Epoch 990, val loss: 0.8889220952987671
Epoch 1000, training loss: 6.101560115814209 = 0.02212267927825451 + 1.0 * 6.079437255859375
Epoch 1000, val loss: 0.8934823870658875
Epoch 1010, training loss: 6.093045711517334 = 0.02147470787167549 + 1.0 * 6.071570873260498
Epoch 1010, val loss: 0.8980227708816528
Epoch 1020, training loss: 6.094051837921143 = 0.020853731781244278 + 1.0 * 6.073198318481445
Epoch 1020, val loss: 0.9025091528892517
Epoch 1030, training loss: 6.0903425216674805 = 0.020256001502275467 + 1.0 * 6.070086479187012
Epoch 1030, val loss: 0.9069637656211853
Epoch 1040, training loss: 6.097466945648193 = 0.01968398503959179 + 1.0 * 6.077783107757568
Epoch 1040, val loss: 0.9113770723342896
Epoch 1050, training loss: 6.09088659286499 = 0.01913624070584774 + 1.0 * 6.071750164031982
Epoch 1050, val loss: 0.9157244563102722
Epoch 1060, training loss: 6.097224712371826 = 0.01861659809947014 + 1.0 * 6.07860803604126
Epoch 1060, val loss: 0.9200472831726074
Epoch 1070, training loss: 6.091811180114746 = 0.018118659034371376 + 1.0 * 6.073692321777344
Epoch 1070, val loss: 0.9243011474609375
Epoch 1080, training loss: 6.084415912628174 = 0.01764511689543724 + 1.0 * 6.066771030426025
Epoch 1080, val loss: 0.9284260272979736
Epoch 1090, training loss: 6.082873344421387 = 0.017189091071486473 + 1.0 * 6.0656843185424805
Epoch 1090, val loss: 0.9325367212295532
Epoch 1100, training loss: 6.083573818206787 = 0.016748277470469475 + 1.0 * 6.0668253898620605
Epoch 1100, val loss: 0.9366042613983154
Epoch 1110, training loss: 6.082757472991943 = 0.01632351242005825 + 1.0 * 6.066433906555176
Epoch 1110, val loss: 0.9406402111053467
Epoch 1120, training loss: 6.081223011016846 = 0.015914911404252052 + 1.0 * 6.065308094024658
Epoch 1120, val loss: 0.9445921182632446
Epoch 1130, training loss: 6.077478408813477 = 0.015522785484790802 + 1.0 * 6.061955451965332
Epoch 1130, val loss: 0.9485534429550171
Epoch 1140, training loss: 6.086886405944824 = 0.015145507641136646 + 1.0 * 6.071741104125977
Epoch 1140, val loss: 0.9524819850921631
Epoch 1150, training loss: 6.080471038818359 = 0.014781530946493149 + 1.0 * 6.065689563751221
Epoch 1150, val loss: 0.9563124179840088
Epoch 1160, training loss: 6.080432891845703 = 0.014436161145567894 + 1.0 * 6.0659966468811035
Epoch 1160, val loss: 0.9601050615310669
Epoch 1170, training loss: 6.073632717132568 = 0.014101936481893063 + 1.0 * 6.059530735015869
Epoch 1170, val loss: 0.9638369083404541
Epoch 1180, training loss: 6.073309421539307 = 0.01377827301621437 + 1.0 * 6.059531211853027
Epoch 1180, val loss: 0.9675125479698181
Epoch 1190, training loss: 6.076601505279541 = 0.01346540916711092 + 1.0 * 6.063136100769043
Epoch 1190, val loss: 0.9711699485778809
Epoch 1200, training loss: 6.07012414932251 = 0.01316357683390379 + 1.0 * 6.056960582733154
Epoch 1200, val loss: 0.974822461605072
Epoch 1210, training loss: 6.070950508117676 = 0.012873164378106594 + 1.0 * 6.058077335357666
Epoch 1210, val loss: 0.9784082770347595
Epoch 1220, training loss: 6.081971645355225 = 0.012591569684445858 + 1.0 * 6.069380283355713
Epoch 1220, val loss: 0.9819657206535339
Epoch 1230, training loss: 6.074260711669922 = 0.012323026545345783 + 1.0 * 6.0619378089904785
Epoch 1230, val loss: 0.9855185151100159
Epoch 1240, training loss: 6.068869113922119 = 0.012066139839589596 + 1.0 * 6.056802749633789
Epoch 1240, val loss: 0.9889300465583801
Epoch 1250, training loss: 6.06601095199585 = 0.011814615689218044 + 1.0 * 6.054196357727051
Epoch 1250, val loss: 0.9922969341278076
Epoch 1260, training loss: 6.0660271644592285 = 0.011569780297577381 + 1.0 * 6.054457187652588
Epoch 1260, val loss: 0.9956024289131165
Epoch 1270, training loss: 6.073103427886963 = 0.011333625763654709 + 1.0 * 6.061769962310791
Epoch 1270, val loss: 0.9989291429519653
Epoch 1280, training loss: 6.063666343688965 = 0.011105367913842201 + 1.0 * 6.052560806274414
Epoch 1280, val loss: 1.0022028684616089
Epoch 1290, training loss: 6.061849117279053 = 0.010884825140237808 + 1.0 * 6.05096435546875
Epoch 1290, val loss: 1.0054441690444946
Epoch 1300, training loss: 6.061280250549316 = 0.010669167153537273 + 1.0 * 6.0506110191345215
Epoch 1300, val loss: 1.0086263418197632
Epoch 1310, training loss: 6.075061321258545 = 0.01045942958444357 + 1.0 * 6.064601898193359
Epoch 1310, val loss: 1.0117889642715454
Epoch 1320, training loss: 6.065128326416016 = 0.010259019210934639 + 1.0 * 6.054869174957275
Epoch 1320, val loss: 1.0149400234222412
Epoch 1330, training loss: 6.060173511505127 = 0.010064520873129368 + 1.0 * 6.050108909606934
Epoch 1330, val loss: 1.018012285232544
Epoch 1340, training loss: 6.058807849884033 = 0.009875297546386719 + 1.0 * 6.0489325523376465
Epoch 1340, val loss: 1.0210707187652588
Epoch 1350, training loss: 6.070931911468506 = 0.00969146192073822 + 1.0 * 6.0612406730651855
Epoch 1350, val loss: 1.02413010597229
Epoch 1360, training loss: 6.062182426452637 = 0.009511947631835938 + 1.0 * 6.052670478820801
Epoch 1360, val loss: 1.0271209478378296
Epoch 1370, training loss: 6.057239532470703 = 0.00934054609388113 + 1.0 * 6.047898769378662
Epoch 1370, val loss: 1.0300568342208862
Epoch 1380, training loss: 6.06551456451416 = 0.009172558784484863 + 1.0 * 6.056342124938965
Epoch 1380, val loss: 1.0329928398132324
Epoch 1390, training loss: 6.057010173797607 = 0.009009862318634987 + 1.0 * 6.048000335693359
Epoch 1390, val loss: 1.0359079837799072
Epoch 1400, training loss: 6.054684162139893 = 0.008851728402078152 + 1.0 * 6.045832633972168
Epoch 1400, val loss: 1.038725733757019
Epoch 1410, training loss: 6.055078029632568 = 0.008697014302015305 + 1.0 * 6.046380996704102
Epoch 1410, val loss: 1.0415642261505127
Epoch 1420, training loss: 6.056393146514893 = 0.008547543548047543 + 1.0 * 6.047845840454102
Epoch 1420, val loss: 1.0444272756576538
Epoch 1430, training loss: 6.052962779998779 = 0.008404088206589222 + 1.0 * 6.044558525085449
Epoch 1430, val loss: 1.0471765995025635
Epoch 1440, training loss: 6.05264139175415 = 0.008263959549367428 + 1.0 * 6.044377326965332
Epoch 1440, val loss: 1.0499053001403809
Epoch 1450, training loss: 6.05156946182251 = 0.008125878870487213 + 1.0 * 6.04344367980957
Epoch 1450, val loss: 1.0525950193405151
Epoch 1460, training loss: 6.063904285430908 = 0.007991543971002102 + 1.0 * 6.055912971496582
Epoch 1460, val loss: 1.0552425384521484
Epoch 1470, training loss: 6.053915023803711 = 0.007860562764108181 + 1.0 * 6.046054363250732
Epoch 1470, val loss: 1.0579286813735962
Epoch 1480, training loss: 6.050171375274658 = 0.0077345590107142925 + 1.0 * 6.042436599731445
Epoch 1480, val loss: 1.0605533123016357
Epoch 1490, training loss: 6.048467636108398 = 0.007609865162521601 + 1.0 * 6.040857791900635
Epoch 1490, val loss: 1.0631616115570068
Epoch 1500, training loss: 6.05822229385376 = 0.0074882726185023785 + 1.0 * 6.050734043121338
Epoch 1500, val loss: 1.0657333135604858
Epoch 1510, training loss: 6.057132720947266 = 0.007371001411229372 + 1.0 * 6.049761772155762
Epoch 1510, val loss: 1.0683025121688843
Epoch 1520, training loss: 6.047085762023926 = 0.007258383557200432 + 1.0 * 6.039827346801758
Epoch 1520, val loss: 1.0708374977111816
Epoch 1530, training loss: 6.046804904937744 = 0.007147610187530518 + 1.0 * 6.039657115936279
Epoch 1530, val loss: 1.073319673538208
Epoch 1540, training loss: 6.0484619140625 = 0.0070384349673986435 + 1.0 * 6.041423320770264
Epoch 1540, val loss: 1.0757651329040527
Epoch 1550, training loss: 6.046965599060059 = 0.006931821815669537 + 1.0 * 6.04003381729126
Epoch 1550, val loss: 1.078169822692871
Epoch 1560, training loss: 6.051436424255371 = 0.006828358396887779 + 1.0 * 6.044608116149902
Epoch 1560, val loss: 1.0805672407150269
Epoch 1570, training loss: 6.047044277191162 = 0.006727683357894421 + 1.0 * 6.040316581726074
Epoch 1570, val loss: 1.082958459854126
Epoch 1580, training loss: 6.054403305053711 = 0.00662968447431922 + 1.0 * 6.047773838043213
Epoch 1580, val loss: 1.0852992534637451
Epoch 1590, training loss: 6.044654369354248 = 0.00653440086171031 + 1.0 * 6.038119792938232
Epoch 1590, val loss: 1.087617039680481
Epoch 1600, training loss: 6.042018890380859 = 0.006441047415137291 + 1.0 * 6.035577774047852
Epoch 1600, val loss: 1.0899231433868408
Epoch 1610, training loss: 6.042279243469238 = 0.006349130533635616 + 1.0 * 6.035930156707764
Epoch 1610, val loss: 1.0921716690063477
Epoch 1620, training loss: 6.04892110824585 = 0.006258936133235693 + 1.0 * 6.042662143707275
Epoch 1620, val loss: 1.0944228172302246
Epoch 1630, training loss: 6.046694755554199 = 0.006171415094286203 + 1.0 * 6.040523529052734
Epoch 1630, val loss: 1.0966657400131226
Epoch 1640, training loss: 6.045116901397705 = 0.006086430978029966 + 1.0 * 6.0390305519104
Epoch 1640, val loss: 1.0988707542419434
Epoch 1650, training loss: 6.041136741638184 = 0.006003634538501501 + 1.0 * 6.035132884979248
Epoch 1650, val loss: 1.1011062860488892
Epoch 1660, training loss: 6.040240287780762 = 0.005922503303736448 + 1.0 * 6.034317970275879
Epoch 1660, val loss: 1.1032179594039917
Epoch 1670, training loss: 6.04241418838501 = 0.0058425115421414375 + 1.0 * 6.036571502685547
Epoch 1670, val loss: 1.1053287982940674
Epoch 1680, training loss: 6.041778564453125 = 0.005764390341937542 + 1.0 * 6.036014080047607
Epoch 1680, val loss: 1.1074347496032715
Epoch 1690, training loss: 6.040128707885742 = 0.005687863565981388 + 1.0 * 6.034440994262695
Epoch 1690, val loss: 1.1095002889633179
Epoch 1700, training loss: 6.042407035827637 = 0.005613467190414667 + 1.0 * 6.0367937088012695
Epoch 1700, val loss: 1.1115543842315674
Epoch 1710, training loss: 6.042027950286865 = 0.005540774203836918 + 1.0 * 6.036487102508545
Epoch 1710, val loss: 1.1135717630386353
Epoch 1720, training loss: 6.0388922691345215 = 0.005468954797834158 + 1.0 * 6.03342342376709
Epoch 1720, val loss: 1.1155686378479004
Epoch 1730, training loss: 6.042074203491211 = 0.005399185698479414 + 1.0 * 6.036674976348877
Epoch 1730, val loss: 1.1175658702850342
Epoch 1740, training loss: 6.035552501678467 = 0.005331084597855806 + 1.0 * 6.030221462249756
Epoch 1740, val loss: 1.119529366493225
Epoch 1750, training loss: 6.036880016326904 = 0.005264067556709051 + 1.0 * 6.031615734100342
Epoch 1750, val loss: 1.1214823722839355
Epoch 1760, training loss: 6.037598133087158 = 0.005198047962039709 + 1.0 * 6.032400131225586
Epoch 1760, val loss: 1.1234023571014404
Epoch 1770, training loss: 6.035737991333008 = 0.005133345723152161 + 1.0 * 6.030604839324951
Epoch 1770, val loss: 1.125295877456665
Epoch 1780, training loss: 6.050172328948975 = 0.005070805549621582 + 1.0 * 6.045101642608643
Epoch 1780, val loss: 1.1271617412567139
Epoch 1790, training loss: 6.037696361541748 = 0.005008983891457319 + 1.0 * 6.032687187194824
Epoch 1790, val loss: 1.129059910774231
Epoch 1800, training loss: 6.0340046882629395 = 0.004949656780809164 + 1.0 * 6.029055118560791
Epoch 1800, val loss: 1.1309274435043335
Epoch 1810, training loss: 6.038662910461426 = 0.004890589974820614 + 1.0 * 6.0337724685668945
Epoch 1810, val loss: 1.132723331451416
Epoch 1820, training loss: 6.032692909240723 = 0.004832586273550987 + 1.0 * 6.027860164642334
Epoch 1820, val loss: 1.134518027305603
Epoch 1830, training loss: 6.032556533813477 = 0.004776235669851303 + 1.0 * 6.027780532836914
Epoch 1830, val loss: 1.1362953186035156
Epoch 1840, training loss: 6.037076473236084 = 0.004720464814454317 + 1.0 * 6.032355785369873
Epoch 1840, val loss: 1.1380329132080078
Epoch 1850, training loss: 6.031349182128906 = 0.004666019231081009 + 1.0 * 6.026683330535889
Epoch 1850, val loss: 1.1397799253463745
Epoch 1860, training loss: 6.031630516052246 = 0.00461241789162159 + 1.0 * 6.027018070220947
Epoch 1860, val loss: 1.1415377855300903
Epoch 1870, training loss: 6.039265155792236 = 0.004559530410915613 + 1.0 * 6.034705638885498
Epoch 1870, val loss: 1.1432477235794067
Epoch 1880, training loss: 6.033167839050293 = 0.004508340265601873 + 1.0 * 6.028659343719482
Epoch 1880, val loss: 1.1449153423309326
Epoch 1890, training loss: 6.031591892242432 = 0.004458319395780563 + 1.0 * 6.027133464813232
Epoch 1890, val loss: 1.1466138362884521
Epoch 1900, training loss: 6.0324273109436035 = 0.004408943932503462 + 1.0 * 6.028018474578857
Epoch 1900, val loss: 1.1482962369918823
Epoch 1910, training loss: 6.031988143920898 = 0.004360395483672619 + 1.0 * 6.027627944946289
Epoch 1910, val loss: 1.1499242782592773
Epoch 1920, training loss: 6.031317710876465 = 0.0043128132820129395 + 1.0 * 6.027004718780518
Epoch 1920, val loss: 1.1515008211135864
Epoch 1930, training loss: 6.032463550567627 = 0.004266026429831982 + 1.0 * 6.028197288513184
Epoch 1930, val loss: 1.1531271934509277
Epoch 1940, training loss: 6.029996395111084 = 0.004219878930598497 + 1.0 * 6.025776386260986
Epoch 1940, val loss: 1.1547306776046753
Epoch 1950, training loss: 6.033233165740967 = 0.004174516536295414 + 1.0 * 6.029058456420898
Epoch 1950, val loss: 1.1563078165054321
Epoch 1960, training loss: 6.029186248779297 = 0.004130245186388493 + 1.0 * 6.025055885314941
Epoch 1960, val loss: 1.1579097509384155
Epoch 1970, training loss: 6.029863357543945 = 0.004086528904736042 + 1.0 * 6.0257768630981445
Epoch 1970, val loss: 1.1594237089157104
Epoch 1980, training loss: 6.0320329666137695 = 0.004043814726173878 + 1.0 * 6.027989387512207
Epoch 1980, val loss: 1.1609959602355957
Epoch 1990, training loss: 6.027152061462402 = 0.004001989029347897 + 1.0 * 6.0231499671936035
Epoch 1990, val loss: 1.1625174283981323
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7048
Flip ASR: 0.6578/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 10.32185173034668 = 1.947935700416565 + 1.0 * 8.373915672302246
Epoch 0, val loss: 1.9415868520736694
Epoch 10, training loss: 10.311147689819336 = 1.9374711513519287 + 1.0 * 8.373676300048828
Epoch 10, val loss: 1.9315176010131836
Epoch 20, training loss: 10.296788215637207 = 1.9249402284622192 + 1.0 * 8.371848106384277
Epoch 20, val loss: 1.9189523458480835
Epoch 30, training loss: 10.264058113098145 = 1.9077638387680054 + 1.0 * 8.356294631958008
Epoch 30, val loss: 1.9012905359268188
Epoch 40, training loss: 10.136149406433105 = 1.8845769166946411 + 1.0 * 8.251572608947754
Epoch 40, val loss: 1.8777797222137451
Epoch 50, training loss: 9.641341209411621 = 1.8590304851531982 + 1.0 * 7.782310485839844
Epoch 50, val loss: 1.853004813194275
Epoch 60, training loss: 9.16883373260498 = 1.838058352470398 + 1.0 * 7.330775737762451
Epoch 60, val loss: 1.8335835933685303
Epoch 70, training loss: 8.758326530456543 = 1.8225082159042358 + 1.0 * 6.935818672180176
Epoch 70, val loss: 1.818577527999878
Epoch 80, training loss: 8.509661674499512 = 1.8074793815612793 + 1.0 * 6.702182292938232
Epoch 80, val loss: 1.804753065109253
Epoch 90, training loss: 8.374520301818848 = 1.7901933193206787 + 1.0 * 6.58432674407959
Epoch 90, val loss: 1.7892102003097534
Epoch 100, training loss: 8.27283763885498 = 1.771064281463623 + 1.0 * 6.501773357391357
Epoch 100, val loss: 1.7723758220672607
Epoch 110, training loss: 8.198549270629883 = 1.7517346143722534 + 1.0 * 6.446815013885498
Epoch 110, val loss: 1.7555334568023682
Epoch 120, training loss: 8.129490852355957 = 1.7322098016738892 + 1.0 * 6.397281169891357
Epoch 120, val loss: 1.7391287088394165
Epoch 130, training loss: 8.074069023132324 = 1.7113053798675537 + 1.0 * 6.362763404846191
Epoch 130, val loss: 1.7212281227111816
Epoch 140, training loss: 8.021913528442383 = 1.6873080730438232 + 1.0 * 6.334605693817139
Epoch 140, val loss: 1.7008053064346313
Epoch 150, training loss: 7.971004009246826 = 1.6592501401901245 + 1.0 * 6.311753749847412
Epoch 150, val loss: 1.6771953105926514
Epoch 160, training loss: 7.921604156494141 = 1.6263103485107422 + 1.0 * 6.295293807983398
Epoch 160, val loss: 1.6497381925582886
Epoch 170, training loss: 7.867454528808594 = 1.588236689567566 + 1.0 * 6.279217720031738
Epoch 170, val loss: 1.6180427074432373
Epoch 180, training loss: 7.811047554016113 = 1.5444362163543701 + 1.0 * 6.266611099243164
Epoch 180, val loss: 1.5818932056427002
Epoch 190, training loss: 7.751692771911621 = 1.4948465824127197 + 1.0 * 6.2568464279174805
Epoch 190, val loss: 1.5412451028823853
Epoch 200, training loss: 7.685585975646973 = 1.4403529167175293 + 1.0 * 6.245233058929443
Epoch 200, val loss: 1.496893286705017
Epoch 210, training loss: 7.6171464920043945 = 1.381445050239563 + 1.0 * 6.235701560974121
Epoch 210, val loss: 1.4496068954467773
Epoch 220, training loss: 7.556829929351807 = 1.3194810152053833 + 1.0 * 6.237349033355713
Epoch 220, val loss: 1.4006571769714355
Epoch 230, training loss: 7.479552745819092 = 1.2574005126953125 + 1.0 * 6.222152233123779
Epoch 230, val loss: 1.3528424501419067
Epoch 240, training loss: 7.4083638191223145 = 1.194821834564209 + 1.0 * 6.2135419845581055
Epoch 240, val loss: 1.3048946857452393
Epoch 250, training loss: 7.337787628173828 = 1.1317901611328125 + 1.0 * 6.205997467041016
Epoch 250, val loss: 1.2571572065353394
Epoch 260, training loss: 7.268200874328613 = 1.0689280033111572 + 1.0 * 6.199272632598877
Epoch 260, val loss: 1.2100601196289062
Epoch 270, training loss: 7.202130317687988 = 1.0076606273651123 + 1.0 * 6.194469451904297
Epoch 270, val loss: 1.1646143198013306
Epoch 280, training loss: 7.1401777267456055 = 0.9498423933982849 + 1.0 * 6.190335273742676
Epoch 280, val loss: 1.122079610824585
Epoch 290, training loss: 7.078057289123535 = 0.8948745131492615 + 1.0 * 6.183182716369629
Epoch 290, val loss: 1.0820971727371216
Epoch 300, training loss: 7.022593975067139 = 0.8434574007987976 + 1.0 * 6.179136753082275
Epoch 300, val loss: 1.045197606086731
Epoch 310, training loss: 6.970476150512695 = 0.796104907989502 + 1.0 * 6.174371242523193
Epoch 310, val loss: 1.0116115808486938
Epoch 320, training loss: 6.921971797943115 = 0.7520956993103027 + 1.0 * 6.1698760986328125
Epoch 320, val loss: 0.9810547828674316
Epoch 330, training loss: 6.878275394439697 = 0.7109917998313904 + 1.0 * 6.167283535003662
Epoch 330, val loss: 0.9532133936882019
Epoch 340, training loss: 6.836055755615234 = 0.6730825901031494 + 1.0 * 6.162972927093506
Epoch 340, val loss: 0.9280271530151367
Epoch 350, training loss: 6.795589447021484 = 0.6378839015960693 + 1.0 * 6.157705307006836
Epoch 350, val loss: 0.9052751660346985
Epoch 360, training loss: 6.769492149353027 = 0.6048858165740967 + 1.0 * 6.16460657119751
Epoch 360, val loss: 0.8844969272613525
Epoch 370, training loss: 6.726414680480957 = 0.5743023157119751 + 1.0 * 6.1521124839782715
Epoch 370, val loss: 0.865398645401001
Epoch 380, training loss: 6.692260265350342 = 0.5454081296920776 + 1.0 * 6.146852016448975
Epoch 380, val loss: 0.8477845191955566
Epoch 390, training loss: 6.662499904632568 = 0.5179978609085083 + 1.0 * 6.14450216293335
Epoch 390, val loss: 0.8313491940498352
Epoch 400, training loss: 6.645036220550537 = 0.4921312630176544 + 1.0 * 6.152904987335205
Epoch 400, val loss: 0.8161653280258179
Epoch 410, training loss: 6.606044292449951 = 0.4679458439350128 + 1.0 * 6.138098239898682
Epoch 410, val loss: 0.8023112416267395
Epoch 420, training loss: 6.580277442932129 = 0.4448794722557068 + 1.0 * 6.135397911071777
Epoch 420, val loss: 0.7896764874458313
Epoch 430, training loss: 6.5553412437438965 = 0.42287951707839966 + 1.0 * 6.1324615478515625
Epoch 430, val loss: 0.7780370712280273
Epoch 440, training loss: 6.531897068023682 = 0.40190553665161133 + 1.0 * 6.12999153137207
Epoch 440, val loss: 0.7674989104270935
Epoch 450, training loss: 6.511974334716797 = 0.3820353150367737 + 1.0 * 6.129939079284668
Epoch 450, val loss: 0.7578583359718323
Epoch 460, training loss: 6.4872355461120605 = 0.3629656434059143 + 1.0 * 6.124269962310791
Epoch 460, val loss: 0.7491315007209778
Epoch 470, training loss: 6.476505279541016 = 0.3446347713470459 + 1.0 * 6.131870269775391
Epoch 470, val loss: 0.7411962151527405
Epoch 480, training loss: 6.447751522064209 = 0.32721441984176636 + 1.0 * 6.120537281036377
Epoch 480, val loss: 0.7339290976524353
Epoch 490, training loss: 6.4347944259643555 = 0.31047502160072327 + 1.0 * 6.124319553375244
Epoch 490, val loss: 0.7273234128952026
Epoch 500, training loss: 6.4111714363098145 = 0.29447251558303833 + 1.0 * 6.116698741912842
Epoch 500, val loss: 0.7213717699050903
Epoch 510, training loss: 6.394045829772949 = 0.2790776789188385 + 1.0 * 6.114968299865723
Epoch 510, val loss: 0.7161023020744324
Epoch 520, training loss: 6.382909297943115 = 0.26423946022987366 + 1.0 * 6.1186699867248535
Epoch 520, val loss: 0.7114895582199097
Epoch 530, training loss: 6.367787837982178 = 0.250221312046051 + 1.0 * 6.1175665855407715
Epoch 530, val loss: 0.7074000239372253
Epoch 540, training loss: 6.34777307510376 = 0.2367769181728363 + 1.0 * 6.110996246337891
Epoch 540, val loss: 0.7040635347366333
Epoch 550, training loss: 6.330967426300049 = 0.2238956093788147 + 1.0 * 6.107071876525879
Epoch 550, val loss: 0.7013944387435913
Epoch 560, training loss: 6.316674709320068 = 0.2115386575460434 + 1.0 * 6.105135917663574
Epoch 560, val loss: 0.6993302702903748
Epoch 570, training loss: 6.303844451904297 = 0.19979147613048553 + 1.0 * 6.104053020477295
Epoch 570, val loss: 0.6979852914810181
Epoch 580, training loss: 6.289650917053223 = 0.1887180209159851 + 1.0 * 6.100933074951172
Epoch 580, val loss: 0.6972475051879883
Epoch 590, training loss: 6.277540683746338 = 0.17819055914878845 + 1.0 * 6.0993499755859375
Epoch 590, val loss: 0.6972458362579346
Epoch 600, training loss: 6.274362564086914 = 0.16817550361156464 + 1.0 * 6.106186866760254
Epoch 600, val loss: 0.697910487651825
Epoch 610, training loss: 6.2627153396606445 = 0.15883290767669678 + 1.0 * 6.103882312774658
Epoch 610, val loss: 0.6990419626235962
Epoch 620, training loss: 6.246105670928955 = 0.14997784793376923 + 1.0 * 6.096127986907959
Epoch 620, val loss: 0.700735867023468
Epoch 630, training loss: 6.239381313323975 = 0.1416197270154953 + 1.0 * 6.097761631011963
Epoch 630, val loss: 0.7030590772628784
Epoch 640, training loss: 6.2324137687683105 = 0.13379043340682983 + 1.0 * 6.098623275756836
Epoch 640, val loss: 0.7058207988739014
Epoch 650, training loss: 6.218631744384766 = 0.1264316588640213 + 1.0 * 6.09220027923584
Epoch 650, val loss: 0.7089315056800842
Epoch 660, training loss: 6.209475994110107 = 0.11949269473552704 + 1.0 * 6.0899834632873535
Epoch 660, val loss: 0.7125319838523865
Epoch 670, training loss: 6.2116522789001465 = 0.1129659116268158 + 1.0 * 6.098686218261719
Epoch 670, val loss: 0.7165586948394775
Epoch 680, training loss: 6.194785118103027 = 0.10682055354118347 + 1.0 * 6.0879645347595215
Epoch 680, val loss: 0.7209479808807373
Epoch 690, training loss: 6.188465118408203 = 0.10105440020561218 + 1.0 * 6.087410926818848
Epoch 690, val loss: 0.7256115674972534
Epoch 700, training loss: 6.187775611877441 = 0.09562421590089798 + 1.0 * 6.092151165008545
Epoch 700, val loss: 0.730633556842804
Epoch 710, training loss: 6.175138473510742 = 0.0905555859208107 + 1.0 * 6.084582805633545
Epoch 710, val loss: 0.7358090281486511
Epoch 720, training loss: 6.168163299560547 = 0.08576836436986923 + 1.0 * 6.082395076751709
Epoch 720, val loss: 0.7412360906600952
Epoch 730, training loss: 6.170525074005127 = 0.0812574177980423 + 1.0 * 6.089267730712891
Epoch 730, val loss: 0.7468780279159546
Epoch 740, training loss: 6.162171840667725 = 0.07705039530992508 + 1.0 * 6.0851216316223145
Epoch 740, val loss: 0.7526755332946777
Epoch 750, training loss: 6.154287815093994 = 0.0730968788266182 + 1.0 * 6.081191062927246
Epoch 750, val loss: 0.7585408687591553
Epoch 760, training loss: 6.155126094818115 = 0.0693911463022232 + 1.0 * 6.085734844207764
Epoch 760, val loss: 0.7646071910858154
Epoch 770, training loss: 6.145148754119873 = 0.06589905172586441 + 1.0 * 6.079249858856201
Epoch 770, val loss: 0.7708051800727844
Epoch 780, training loss: 6.139900207519531 = 0.06264081597328186 + 1.0 * 6.077259540557861
Epoch 780, val loss: 0.7770062685012817
Epoch 790, training loss: 6.137511730194092 = 0.05957711860537529 + 1.0 * 6.077934741973877
Epoch 790, val loss: 0.783324658870697
Epoch 800, training loss: 6.132837772369385 = 0.05671952664852142 + 1.0 * 6.076118469238281
Epoch 800, val loss: 0.7895635366439819
Epoch 810, training loss: 6.127386569976807 = 0.05402853339910507 + 1.0 * 6.073358058929443
Epoch 810, val loss: 0.7958500385284424
Epoch 820, training loss: 6.132284164428711 = 0.05149699002504349 + 1.0 * 6.080787181854248
Epoch 820, val loss: 0.8022456169128418
Epoch 830, training loss: 6.1291608810424805 = 0.04915155470371246 + 1.0 * 6.080009460449219
Epoch 830, val loss: 0.808530867099762
Epoch 840, training loss: 6.117924690246582 = 0.04695366322994232 + 1.0 * 6.0709710121154785
Epoch 840, val loss: 0.8145991563796997
Epoch 850, training loss: 6.1138014793396 = 0.044878989458084106 + 1.0 * 6.068922519683838
Epoch 850, val loss: 0.8207379579544067
Epoch 860, training loss: 6.111086368560791 = 0.04292328283190727 + 1.0 * 6.06816291809082
Epoch 860, val loss: 0.8269049525260925
Epoch 870, training loss: 6.114687919616699 = 0.04107503592967987 + 1.0 * 6.073612689971924
Epoch 870, val loss: 0.8330341577529907
Epoch 880, training loss: 6.114501953125 = 0.03935261070728302 + 1.0 * 6.0751495361328125
Epoch 880, val loss: 0.8390089869499207
Epoch 890, training loss: 6.10367488861084 = 0.03773905336856842 + 1.0 * 6.0659356117248535
Epoch 890, val loss: 0.844776451587677
Epoch 900, training loss: 6.10150146484375 = 0.03621828928589821 + 1.0 * 6.065283298492432
Epoch 900, val loss: 0.850504994392395
Epoch 910, training loss: 6.10648250579834 = 0.03477801755070686 + 1.0 * 6.071704387664795
Epoch 910, val loss: 0.8563186526298523
Epoch 920, training loss: 6.098677158355713 = 0.03341703861951828 + 1.0 * 6.06525993347168
Epoch 920, val loss: 0.8620535135269165
Epoch 930, training loss: 6.096126556396484 = 0.03213990852236748 + 1.0 * 6.063986778259277
Epoch 930, val loss: 0.8675203919410706
Epoch 940, training loss: 6.093616962432861 = 0.030927279964089394 + 1.0 * 6.062689781188965
Epoch 940, val loss: 0.8730029463768005
Epoch 950, training loss: 6.0919270515441895 = 0.02977403625845909 + 1.0 * 6.062152862548828
Epoch 950, val loss: 0.8785066604614258
Epoch 960, training loss: 6.095219135284424 = 0.02868444286286831 + 1.0 * 6.066534519195557
Epoch 960, val loss: 0.8839306831359863
Epoch 970, training loss: 6.091548919677734 = 0.027652360498905182 + 1.0 * 6.063896656036377
Epoch 970, val loss: 0.8892191648483276
Epoch 980, training loss: 6.085390090942383 = 0.02668084390461445 + 1.0 * 6.058709144592285
Epoch 980, val loss: 0.8944644331932068
Epoch 990, training loss: 6.084683895111084 = 0.025758683681488037 + 1.0 * 6.058925151824951
Epoch 990, val loss: 0.8995940089225769
Epoch 1000, training loss: 6.100014686584473 = 0.024876775220036507 + 1.0 * 6.075138092041016
Epoch 1000, val loss: 0.9047859907150269
Epoch 1010, training loss: 6.0845232009887695 = 0.024057108908891678 + 1.0 * 6.060466289520264
Epoch 1010, val loss: 0.9097320437431335
Epoch 1020, training loss: 6.079662799835205 = 0.023267371580004692 + 1.0 * 6.056395530700684
Epoch 1020, val loss: 0.9146248698234558
Epoch 1030, training loss: 6.078127861022949 = 0.02251408062875271 + 1.0 * 6.055613994598389
Epoch 1030, val loss: 0.9195259809494019
Epoch 1040, training loss: 6.089232444763184 = 0.021796608343720436 + 1.0 * 6.0674357414245605
Epoch 1040, val loss: 0.9244368672370911
Epoch 1050, training loss: 6.078930377960205 = 0.02111235074698925 + 1.0 * 6.0578179359436035
Epoch 1050, val loss: 0.9292500615119934
Epoch 1060, training loss: 6.07570219039917 = 0.02046389877796173 + 1.0 * 6.055238246917725
Epoch 1060, val loss: 0.9338885545730591
Epoch 1070, training loss: 6.077013969421387 = 0.019845619797706604 + 1.0 * 6.057168483734131
Epoch 1070, val loss: 0.9385706782341003
Epoch 1080, training loss: 6.074949264526367 = 0.01925470493733883 + 1.0 * 6.055694580078125
Epoch 1080, val loss: 0.9431265592575073
Epoch 1090, training loss: 6.070868015289307 = 0.018687931820750237 + 1.0 * 6.052180290222168
Epoch 1090, val loss: 0.9476580023765564
Epoch 1100, training loss: 6.0755181312561035 = 0.018147684633731842 + 1.0 * 6.057370662689209
Epoch 1100, val loss: 0.9521349668502808
Epoch 1110, training loss: 6.068911075592041 = 0.01762973703444004 + 1.0 * 6.051281452178955
Epoch 1110, val loss: 0.956646740436554
Epoch 1120, training loss: 6.067378044128418 = 0.01713556982576847 + 1.0 * 6.0502424240112305
Epoch 1120, val loss: 0.9609110355377197
Epoch 1130, training loss: 6.068746566772461 = 0.01666194759309292 + 1.0 * 6.052084445953369
Epoch 1130, val loss: 0.9651992321014404
Epoch 1140, training loss: 6.073174476623535 = 0.016207221895456314 + 1.0 * 6.056967258453369
Epoch 1140, val loss: 0.9695143103599548
Epoch 1150, training loss: 6.065595626831055 = 0.015776416286826134 + 1.0 * 6.049818992614746
Epoch 1150, val loss: 0.973638117313385
Epoch 1160, training loss: 6.0634965896606445 = 0.015360643155872822 + 1.0 * 6.048135757446289
Epoch 1160, val loss: 0.9777063727378845
Epoch 1170, training loss: 6.068957805633545 = 0.014962024986743927 + 1.0 * 6.053995609283447
Epoch 1170, val loss: 0.9818024635314941
Epoch 1180, training loss: 6.066864490509033 = 0.014580235816538334 + 1.0 * 6.052284240722656
Epoch 1180, val loss: 0.9858807325363159
Epoch 1190, training loss: 6.063124656677246 = 0.014212025329470634 + 1.0 * 6.048912525177002
Epoch 1190, val loss: 0.9898211359977722
Epoch 1200, training loss: 6.0639262199401855 = 0.013860154896974564 + 1.0 * 6.050065994262695
Epoch 1200, val loss: 0.9936888813972473
Epoch 1210, training loss: 6.061964511871338 = 0.013522233814001083 + 1.0 * 6.048442363739014
Epoch 1210, val loss: 0.9975714683532715
Epoch 1220, training loss: 6.058140754699707 = 0.013194714672863483 + 1.0 * 6.044946193695068
Epoch 1220, val loss: 1.001389741897583
Epoch 1230, training loss: 6.062242031097412 = 0.012880446389317513 + 1.0 * 6.049361705780029
Epoch 1230, val loss: 1.0051600933074951
Epoch 1240, training loss: 6.061952590942383 = 0.012578659690916538 + 1.0 * 6.049374103546143
Epoch 1240, val loss: 1.0089770555496216
Epoch 1250, training loss: 6.058537483215332 = 0.01228656992316246 + 1.0 * 6.046250820159912
Epoch 1250, val loss: 1.0126047134399414
Epoch 1260, training loss: 6.055078506469727 = 0.012007253244519234 + 1.0 * 6.043071269989014
Epoch 1260, val loss: 1.016165018081665
Epoch 1270, training loss: 6.05757999420166 = 0.01173551194369793 + 1.0 * 6.045844554901123
Epoch 1270, val loss: 1.019792914390564
Epoch 1280, training loss: 6.05866813659668 = 0.011472271755337715 + 1.0 * 6.047195911407471
Epoch 1280, val loss: 1.0234334468841553
Epoch 1290, training loss: 6.0529069900512695 = 0.011221639811992645 + 1.0 * 6.041685581207275
Epoch 1290, val loss: 1.0269232988357544
Epoch 1300, training loss: 6.057597637176514 = 0.01098038163036108 + 1.0 * 6.046617031097412
Epoch 1300, val loss: 1.0302842855453491
Epoch 1310, training loss: 6.052812099456787 = 0.01074547041207552 + 1.0 * 6.04206657409668
Epoch 1310, val loss: 1.0337786674499512
Epoch 1320, training loss: 6.052461624145508 = 0.01051841676235199 + 1.0 * 6.041943073272705
Epoch 1320, val loss: 1.0371174812316895
Epoch 1330, training loss: 6.051896572113037 = 0.010300085879862309 + 1.0 * 6.041596412658691
Epoch 1330, val loss: 1.0404244661331177
Epoch 1340, training loss: 6.0536208152771 = 0.010087123140692711 + 1.0 * 6.043533802032471
Epoch 1340, val loss: 1.0437860488891602
Epoch 1350, training loss: 6.052198886871338 = 0.009881033562123775 + 1.0 * 6.042317867279053
Epoch 1350, val loss: 1.0470613241195679
Epoch 1360, training loss: 6.050199508666992 = 0.009682570584118366 + 1.0 * 6.0405168533325195
Epoch 1360, val loss: 1.0502713918685913
Epoch 1370, training loss: 6.053635120391846 = 0.00949058961123228 + 1.0 * 6.044144630432129
Epoch 1370, val loss: 1.0534744262695312
Epoch 1380, training loss: 6.049652576446533 = 0.009303843602538109 + 1.0 * 6.040348529815674
Epoch 1380, val loss: 1.0567530393600464
Epoch 1390, training loss: 6.04697322845459 = 0.009126567281782627 + 1.0 * 6.037846565246582
Epoch 1390, val loss: 1.0597271919250488
Epoch 1400, training loss: 6.046428203582764 = 0.00895320437848568 + 1.0 * 6.037475109100342
Epoch 1400, val loss: 1.0627268552780151
Epoch 1410, training loss: 6.048439979553223 = 0.008782965131103992 + 1.0 * 6.039657115936279
Epoch 1410, val loss: 1.065824270248413
Epoch 1420, training loss: 6.046874046325684 = 0.008618054911494255 + 1.0 * 6.0382561683654785
Epoch 1420, val loss: 1.0689457654953003
Epoch 1430, training loss: 6.045632839202881 = 0.008458424359560013 + 1.0 * 6.037174224853516
Epoch 1430, val loss: 1.0719257593154907
Epoch 1440, training loss: 6.048669338226318 = 0.008303540758788586 + 1.0 * 6.040365695953369
Epoch 1440, val loss: 1.0748591423034668
Epoch 1450, training loss: 6.045864105224609 = 0.008153368718922138 + 1.0 * 6.037710666656494
Epoch 1450, val loss: 1.0779091119766235
Epoch 1460, training loss: 6.044187545776367 = 0.008009816519916058 + 1.0 * 6.036177635192871
Epoch 1460, val loss: 1.080676555633545
Epoch 1470, training loss: 6.0431108474731445 = 0.007867840118706226 + 1.0 * 6.035243034362793
Epoch 1470, val loss: 1.0835225582122803
Epoch 1480, training loss: 6.045370101928711 = 0.007729175966233015 + 1.0 * 6.0376410484313965
Epoch 1480, val loss: 1.0863821506500244
Epoch 1490, training loss: 6.046844959259033 = 0.0075953188352286816 + 1.0 * 6.039249420166016
Epoch 1490, val loss: 1.0893360376358032
Epoch 1500, training loss: 6.042348861694336 = 0.00746686477214098 + 1.0 * 6.034882068634033
Epoch 1500, val loss: 1.0920703411102295
Epoch 1510, training loss: 6.041354179382324 = 0.007342299912124872 + 1.0 * 6.0340118408203125
Epoch 1510, val loss: 1.0947327613830566
Epoch 1520, training loss: 6.0426154136657715 = 0.007218717131763697 + 1.0 * 6.035396575927734
Epoch 1520, val loss: 1.0974526405334473
Epoch 1530, training loss: 6.044143199920654 = 0.00709939980879426 + 1.0 * 6.037043571472168
Epoch 1530, val loss: 1.100322961807251
Epoch 1540, training loss: 6.040029048919678 = 0.006985702086240053 + 1.0 * 6.033043384552002
Epoch 1540, val loss: 1.102890133857727
Epoch 1550, training loss: 6.039272785186768 = 0.00687276991084218 + 1.0 * 6.032400131225586
Epoch 1550, val loss: 1.1054768562316895
Epoch 1560, training loss: 6.043118953704834 = 0.006762477103620768 + 1.0 * 6.036356449127197
Epoch 1560, val loss: 1.1081557273864746
Epoch 1570, training loss: 6.040268421173096 = 0.006655964534729719 + 1.0 * 6.033612251281738
Epoch 1570, val loss: 1.1107590198516846
Epoch 1580, training loss: 6.039088726043701 = 0.006552569102495909 + 1.0 * 6.032536029815674
Epoch 1580, val loss: 1.1132793426513672
Epoch 1590, training loss: 6.042243480682373 = 0.006451049353927374 + 1.0 * 6.035792350769043
Epoch 1590, val loss: 1.1158301830291748
Epoch 1600, training loss: 6.036683082580566 = 0.006351878866553307 + 1.0 * 6.030331134796143
Epoch 1600, val loss: 1.118414044380188
Epoch 1610, training loss: 6.038825988769531 = 0.0062553114257752895 + 1.0 * 6.032570838928223
Epoch 1610, val loss: 1.1208593845367432
Epoch 1620, training loss: 6.040496826171875 = 0.0061615221202373505 + 1.0 * 6.034335136413574
Epoch 1620, val loss: 1.1234029531478882
Epoch 1630, training loss: 6.037685871124268 = 0.006072062999010086 + 1.0 * 6.031613826751709
Epoch 1630, val loss: 1.1257461309432983
Epoch 1640, training loss: 6.03502893447876 = 0.00598213542252779 + 1.0 * 6.029047012329102
Epoch 1640, val loss: 1.128102421760559
Epoch 1650, training loss: 6.036002159118652 = 0.005894382018595934 + 1.0 * 6.0301079750061035
Epoch 1650, val loss: 1.1305184364318848
Epoch 1660, training loss: 6.04189395904541 = 0.005808866582810879 + 1.0 * 6.03608512878418
Epoch 1660, val loss: 1.1329823732376099
Epoch 1670, training loss: 6.036705017089844 = 0.005726302042603493 + 1.0 * 6.030978679656982
Epoch 1670, val loss: 1.1353315114974976
Epoch 1680, training loss: 6.034265041351318 = 0.005646131467074156 + 1.0 * 6.028618812561035
Epoch 1680, val loss: 1.1375060081481934
Epoch 1690, training loss: 6.03534460067749 = 0.005566797684878111 + 1.0 * 6.029778003692627
Epoch 1690, val loss: 1.1397801637649536
Epoch 1700, training loss: 6.036690711975098 = 0.005489069037139416 + 1.0 * 6.0312018394470215
Epoch 1700, val loss: 1.1421231031417847
Epoch 1710, training loss: 6.036282539367676 = 0.005414193961769342 + 1.0 * 6.0308685302734375
Epoch 1710, val loss: 1.1443254947662354
Epoch 1720, training loss: 6.033273696899414 = 0.005340404808521271 + 1.0 * 6.027933120727539
Epoch 1720, val loss: 1.146528959274292
Epoch 1730, training loss: 6.03399133682251 = 0.005268455017358065 + 1.0 * 6.028722763061523
Epoch 1730, val loss: 1.148690104484558
Epoch 1740, training loss: 6.037359714508057 = 0.005198387894779444 + 1.0 * 6.032161235809326
Epoch 1740, val loss: 1.1509422063827515
Epoch 1750, training loss: 6.033264636993408 = 0.005130086559802294 + 1.0 * 6.028134346008301
Epoch 1750, val loss: 1.1530643701553345
Epoch 1760, training loss: 6.033086776733398 = 0.005063445307314396 + 1.0 * 6.0280232429504395
Epoch 1760, val loss: 1.155123233795166
Epoch 1770, training loss: 6.034401893615723 = 0.004997502081096172 + 1.0 * 6.029404163360596
Epoch 1770, val loss: 1.1572680473327637
Epoch 1780, training loss: 6.0293121337890625 = 0.0049338508397340775 + 1.0 * 6.024378299713135
Epoch 1780, val loss: 1.1592999696731567
Epoch 1790, training loss: 6.0337066650390625 = 0.004870903678238392 + 1.0 * 6.028835773468018
Epoch 1790, val loss: 1.1613327264785767
Epoch 1800, training loss: 6.030708312988281 = 0.004808448255062103 + 1.0 * 6.025899887084961
Epoch 1800, val loss: 1.1634753942489624
Epoch 1810, training loss: 6.033699989318848 = 0.004748329054564238 + 1.0 * 6.028951644897461
Epoch 1810, val loss: 1.165492296218872
Epoch 1820, training loss: 6.028502464294434 = 0.0046900068409740925 + 1.0 * 6.023812294006348
Epoch 1820, val loss: 1.1674357652664185
Epoch 1830, training loss: 6.029923915863037 = 0.004632297437638044 + 1.0 * 6.025291442871094
Epoch 1830, val loss: 1.1693480014801025
Epoch 1840, training loss: 6.0363383293151855 = 0.004574942868202925 + 1.0 * 6.031763553619385
Epoch 1840, val loss: 1.1714364290237427
Epoch 1850, training loss: 6.030216217041016 = 0.004520331043750048 + 1.0 * 6.02569580078125
Epoch 1850, val loss: 1.1733921766281128
Epoch 1860, training loss: 6.028078079223633 = 0.0044670323841273785 + 1.0 * 6.023611068725586
Epoch 1860, val loss: 1.175167441368103
Epoch 1870, training loss: 6.027127742767334 = 0.004413632210344076 + 1.0 * 6.022714138031006
Epoch 1870, val loss: 1.1770360469818115
